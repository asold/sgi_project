{
    "title": "Transformer-Transducer: End-to-End Speech Recognition with Self-Attention",
    "url": "https://openalex.org/W2982413405",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4214337936",
            "name": "Yeh, Ching-Feng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4214337930",
            "name": "Mahadeokar, Jay",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4302753862",
            "name": "Kalgaonkar, Kaustubh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1928402660",
            "name": "Wang Yong-qiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100217444",
            "name": "Le Duc",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2182642077",
            "name": "Jain Mahaveer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4302753866",
            "name": "Schubert, Kjell",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4214337937",
            "name": "Fuegen, Christian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4214337938",
            "name": "Seltzer, Michael L.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2885185669",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2963414781",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2173629880",
        "https://openalex.org/W1828163288",
        "https://openalex.org/W2102113734",
        "https://openalex.org/W2941814890",
        "https://openalex.org/W2147768505",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2515439472",
        "https://openalex.org/W2533523411",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W2913718171",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1686810756"
    ],
    "abstract": "We explore options to use Transformer networks in neural transducer for end-to-end speech recognition. Transformer networks use self-attention for sequence modeling and comes with advantages in parallel computation and capturing contexts. We propose 1) using VGGNet with causal convolution to incorporate positional information and reduce frame rate for efficient inference 2) using truncated self-attention to enable streaming for Transformer and reduce computational complexity. All experiments are conducted on the public LibriSpeech corpus. The proposed Transformer-Transducer outperforms neural transducer with LSTM/BLSTM networks and achieved word error rates of 6.37 % on the test-clean set and 15.30 % on the test-other set, while remaining streamable, compact with 45.7M parameters for the entire system, and computationally efficient with complexity of O(T), where T is input sequence length.",
    "full_text": "TRANSFORMER-TRANSDUCER:\nEND-TO-END SPEECH RECOGNITION WITH SELF-ATTENTION\nChing-Feng Yeh⋆, Jay Mahadeokar⋆, Kaustubh Kalgaonkar, Yongqiang Wang,\nDuc Le, Mahaveer Jain, Kjell Schubert, Christian Fuegen, Michael L. Seltzer\nFacebook AI, USA\nABSTRACT\nWe explore options to use Transformer networks in neural trans-\nducer for end-to-end speech recognition. Transformer networks use\nself-attention for sequence modeling and comes with advantages in\nparallel computation and capturing contexts. We propose 1) using\nVGGNet with causal convolution to incorporate positional informa-\ntion and reduce frame rate for efﬁcient inference 2) using truncated\nself-attention to enable streaming for Transformer and reduce com-\nputational complexity. All experiments are conducted on the public\nLibriSpeech corpus. The proposed Transformer-Transducer outper-\nforms neural transducer with LSTM/BLSTM networks and achieved\nword error rates of 6.37 % on thetest-clean set and 15.30 % on\nthe test-other set, while remaining streamable, compact with\n45.7M parameters for the entire system, and computationally efﬁ-\ncient with complexity of O(T), where T is input sequence length.\nIndex Terms— transformer, transducer, end-to-end, self-\nattention, speech recognition\n1. INTRODUCTION\nThere has been signiﬁcant progress on automatic speech recognition\n(ASR) technologies over the past few years due to the adoption of\ndeep neural networks [1]. Conventionally, speech recognition sys-\ntems involve individual components for explicit modeling on dif-\nferent levels of signal transformation: acoustic models for audio to\nacoustic units, pronunciation model for acoustic units to words and\nlanguage model for words to sentences. This framework is often\nreferred to as the “traditional” hybrid system. Conventionally, indi-\nvidual components in the hybrid system can be optimized separately.\nFor example, CD-DNN-HMM [1] focuses on maximizing the likeli-\nhood between acoustic signals and acoustic models with frame-level\nalignments. For language modeling, both statistical n-gram mod-\nels [2] and more recently, neural-network-based models [3] aim to\nmodel purely the connection between word tokens.\nHybrid systems achieved signiﬁcant success [4] but also present\nchallenges. For example, hybrid system requires more human in-\ntervention in the building process, including the design of acoustic\nunits, the vocabulary, the pronunciation model and more. In addi-\ntion, an accurate hybrid system often comes with the cost of higher\ncomputational complexity and memory consumption, thus increas-\ning the difﬁculty of deploying hybrid systems in resource-limited\nscenarios such as on-device speech recognition. Given the chal-\nlenges, the interests in end-to-end approaches for speech recognition\nhave surged recently [5–12]. Different from hybrid systems, end-to-\nend approaches aim to model the transformation from audio signal\nto word tokens directly, therefore the model becomes simpler and\n⋆ Equal contribution.\nrequires less human intervention. In addition to the simplicity of\ntraining process, end-to-end systems also demonstrated promising\nrecognition accuracy [11]. Among many end-to-end approaches, re-\ncurrent neural network transducer (RNN-T) [5, 6] provides promis-\ning potential on footprint, accuracy and efﬁciency. In this work, we\nexplore options for further improvements based on RNN-T.\nRecurrent neural networks (RNNs) such as long-short term\nmemory (LSTM) [13] networks are good at sequence modeling and\nwidely adopted for speech recognition. RNNs rely on the recurrent\nconnection from the previous state ht−1 to the current state ht to\npropagate contextual information. This recurrent connection is ef-\nfective but also presents challenges. For example, since ht depends\non ht−1, RNNs are difﬁcult to compute in parallel. In addition, ht\nis usually of ﬁxed dimensions, which means all historical informa-\ntion is condensed into a ﬁxed-length vector and makes capturing\nlong contexts also difﬁcult. The attention mechanism [14, 15] was\nintroduced recently as an alternative for sequence modeling. Com-\npared with RNNs, the attention mechanism is non-recurrent and can\ncompute in parallel easily. In addition, the attention mechanism can\n”attend” to longer contexts explicitly. With the attention mechanism,\nthe Transformer model [14] achieved state-of-the-art performance\nin many sequence-to-sequence tasks [15, 16].\nIn this paper, we explore options to apply Transformer networks\nin the neural transducer framework. VGG networks [17] with causal\nconvolution are adopted to incorporate contextual information into\nthe Transformer networks and reduce the frame rate for efﬁcient\ninference. In addition, we use truncated self-attention to enable\nstreaming inference and reduce computational complexity .\n2. NEURAL TRANSDUCER (RNN-T)\nIn nature, speech recognition is a sequence-to-sequence (audio-to-\ntext) task in which the lengths of input and output sequences can\nvary. As an end-to-end approach, connectionist temporal classi-\nﬁcation (CTC) [9] was introduced before RNN-T to model such\nsequence-to-sequence transformation. Given input sequence x =\n(x1,x2,··· ,xT ), where xt ∈ Rd and T is the input sequence\nlength, output sequence y = (y1,y2,··· ,yu), where yu ∈Z rep-\nresent output symbols and U is the output sequence length, CTC\nintroduces an additional ”blank” label b and models the posterior\nprobability of y given x by:\nP(y|x) =\n∑\nˆy∈Hctc(x,y)\nT∏\nt=1\nP(ˆyt|x1 ···xT ) (1)\nwhere ˆy = (ˆy1,ˆy2,··· ˆyT ) ∈Hctc(x,y) ⊂{Z∪ b}T correspond\nto any possible paths such that after removing band repeated con-\nsecutive symbols of ˆy yields y.\narXiv:1910.12977v1  [eess.AS]  28 Oct 2019\nThe formulation of CTC assumes that symbols in the output se-\nquence are conditionally independent of one another given the input\nsequence. The RNN-T model improves upon CTC by making the\noutput symbol distribution at each step dependent on the input se-\nquence and previous non-blank output symbols in the history:\nP(y|x) =\n∑\nˆy∈Hrnnt(x,y)\nT+U∏\ni=1\nP(ˆyi|x1 ···xti,y1 ···yui−1 ) (2)\nwhere ˆy = (ˆy1,ˆy2,··· ˆyT+U ) ∈Hrnnt(x,y) ⊂{Z∪ b}T+U cor-\nrespond to any possible paths such that after removingband repeated\nconsecutive symbols of ˆy yields y. By explicitly conditioning the\ncurrent output on the history, RNN-T outperforms CTC when no ex-\nternal language model is present [6, 7]. RNN-T can be implemented\nin the encoder-decoder framework, as illustrated in Fig. 1. The en-\ncoder encodes the input acoustic sequencex = (x1,x2,··· ,xT ) to\nh = (h1,h2,··· ,hT′) with potential subsampling T′ ≤T. And\nthe decoder contains a predictor to encode the previous non-blank\noutput symbol yu−1 for the logits zt,u to condition on. It’s worth\nnoting that only when the most probable symbol ˆyu is non-blank\nthe input to predictor yu−1 will be updated, so that the condition-\ning encoding pu only changes when non-blank output symbols are\nobserved. From the illustration, we see that RNN-T incorporates a\nlanguage model of output symbols internally in the decoder.\nFig. 1: Neural Transducer.\nThere are many architectures that can be used as encoders and\npredictors. The functionality of these blocks is to take a sequence\nand ﬁnd a higher-order representations. Recurrent neural networks\n(RNNs) such as LSTM [13] have been successfully used for such\nfunctionality. In this paper, we explore Transformer [14, 15] as an\nalternative for sequence encoding in RNN-T. Since Transformer is\nnot recurrent in nature, we refer to the architecture illustrated in Fig.\n1 as simply ”neural transducer” [18] for the rest of the paper.\n3. TRANSFORMER\nThe attention mechanism [19] is one of the core ideas of Trans-\nformer [15]. It was proposed to model correlation between contex-\ntual signals and produced state-of-the-art performance in many do-\nmains including machine translation [15] and natural language pro-\ncessing [14]. Similar to RNNs, attention mechanism aims to encode\nthe input sequence to a higher-level representation by formulating\nthe encoding function into the relationship between queries Q, keys\nKand values V and describing the similarities between them with:\nAttention(Q,K,V ) =Softmax(QKT\n√dk\n)V (3)\nwhere Q ∈ RTq×dk , K ∈ RTk×dk and V ∈ RTk×dv . This\nmechanism becomes ”self-attention” when Q = K = V =\n(x1,··· ,xT ). A self-attention block encodes the input x to a\nhigher-level representationh, just like RNNs but without recurrence.\nCompared with RNNs where ht depends on ht−1, self-attention has\nno recurrent connections between time steps in the encoding h,\ntherefore it can generate encoding efﬁciently in parallel. In addition,\ncompared with RNNs where contexts are condensed into ﬁxed-\nlength states for the next time step to condition on, self-attention\n”pays attention” to all available contexts to better model the context\nwithin the input sequence.\n3.1. Multi-Head Self-Attention\nThe attention mechanism can be further extended to multi-head at-\ntention, in which 1) dimensions of input sequences are split into mul-\ntiple chunks with multiple projections 2) each chunk goes through\nindependent attention mechanisms 3) encodings from each chunks\nare concatenated then projected to produce the output encodings, as\ndescribed with:\nMultiHeadAttention(Q,K,V ) = [e1,··· ,eH]Wo\nwhere ei = Attention(QWQ\ni ,KW K\ni ,VW V\ni ) (4)\nwhere H is the number of heads, din is the dimension of input\nsequence, dk = din/H, ei is the encoding generated by head i,\nWo ∈Rdin×din, WQ\ni ∈Rdin×dk , WK\ni ∈Rdin×dk and WV\ni ∈\nRdin×dv . Multi-head attention integrates encodings generated from\nmultiple subspaces to higher-dimensional representations [15].\n3.2. Transformer Encoder\nThe Transformer [14] is also a sequence-to-sequence model. The\narchitecture of the Transformer encoder contains three main blocks:\n1) attention block, 2) feed-forward block and 3) layer norm [20] as\nshown in Fig. 2(a). The attention block contains the core multi-\nhead self-attention component. The feed-forward block projects the\ninput dimension din to another feature space dff and then back to\ndin (usually dff ≥din) for learning feature representation. The\nﬁnal layer normalization and other additional components including\nlayer norm and dropout in the ﬁrst two blocks are added to stabilize\nthe model training and prevent overﬁtting. Furthermore, we use VG-\nGNets to incorporate positional information into the Transformer as\nillustrated in Fig. 2(b). More details are given in section 4.1.\nFig. 2: (a) Transformer Encoder (b) VGG-Transformer.\n4. TRANSFORMER-TRANSDUCER\nGiven the success of the Transformer, we explore the options of ap-\nplying Transformer in neural transducer. For further improvement,\nwe propose 1) using causal convolution for context modeling and\nframe rate reduction and 2) using truncated self-attention to reduce\nthe computational complexity and enable streaming for Transformer.\n4.1. Context Modeling with Causal Convolution\nTransformer relies on multi-head self-attention to model the contex-\ntual information. However, attention mechanism is non-recurrent\nand non-convolutive, therefore risks losing the order or positional\ninformation in the input sequence [15, 21], which could harm the\nperformance especially for the case of language modeling. To in-\ncorporate the positional information into Transformer, a simple way\nis adding positional encoding [15] but convolutional approaches [8]\ndemonstrated superior performance. In this paper we adopt the con-\nvolutional approach in [8] with modiﬁcation.\nFig. 3: (a) Causal Convolution (b) VGGNet\nConvolution networks model contexts by using kernels to con-\nvolve blocks of features. If we treat the input sequence (for example:\nacoustic features) as a two-dimensional image X ∈RT×D, in com-\nmon practice for a N×Kkernel the convolution would cover from\nX(i−N−1\n2 ,j−K−1\n2 ) to X(i+ N−1\n2 ,j+ K−1\n2 ) to produce the con-\nvolved output Y(i,j). Therefore the convolution would need ”fu-\nture” information to generate the encoding for the current time step.\nFor acoustic modeling this introduces additional look ahead and la-\ntency, but introducing future information is impractical for language\nmodeling since the next symbol is unknown during inference.\nTo prevent future information from leaking into the computation\nat the current time step, we use causal convolution in which all con-\ntexts required are pushed to the history, as illustrated in Fig. 3(a).\nWith causal convolution, for aN×Kkernel the convolution covers\nfrom X(i−N+1,j −K−1\n2 ) to X(i,j + K−1\n2 ) to produce the con-\nvolved output Y(i,j), therefore ensuring the convolution is purely\n”causal”. Similar to [8], we also adopt the VGGNet [17] structure,\nas illustrated in Fig. 3(b), where two two-dimensional convolution\nlayers are stacked sequentially followed by a two-dimensional max-\npooling layer. We use layers of the causal VGGNet to incorporate\npositional information and propagate to the succeeding Transformer\nencoder layers. We refer to this network as ”VGG-Transformer” and\nillustrate the architecture used for the encoder in neural transducer\nin Fig. 3(b), where the ﬁrst two VGGNet layers are used to incor-\nporate positional information and reduce the frame rate for efﬁcient\ninference, followed by a linear layer for dimension reduction and\nmultiple Transformer encoder layers for generating higher-level rep-\nresentations.\n4.2. Truncated Self-Attention\nUnlimited self-attention attends to the whole input sequence and\nposes two issues: 1) streaming inference is disabled and 2) compu-\ntational complexity is high. As illustrated in Fig. 4(a), for unlimited\nself-attention, the output ht at time step tdepends on the entire in-\nput sequence x = (x1,··· ,xT ), meaning the inference can only\nbegin after the ﬁnal length T is known. In addition, ht depends on\nthe similarity pairs (xt,x1),(xt,x2) ···(xt,xT ), giving complex-\nity O(T2) for computing (h1,··· ,hT ). These issues are critical\nfor self-attention to work in scenarios demanding low-latency and\nlow-computation such as on-device speech recognition [6].\nFig. 4: Self-Attention: (a) Unlimited (b) Truncated.\nTo reduce both the latency and computational cost, we replace\nthe unlimited self-attention by truncated self-attention, as illustrated\nin Fig. 4(b). Similar to time-delayed neural network (TDNN) [22,\n23], we limit the contexts available for self-attention so that output\nht at time tonly depends on (xt−L ···xt+R). Compared with un-\nlimited self-attention, truncated self-attention is both streamable and\ncomputationally efﬁcient. The look-ahead is the right context Rand\nthe computational complexity reduces from O(T2 to O(T). How-\never, it also comes with potential performance degradation and is\ninvestigated further in experiments.\n5. EXPERIMENTS\n5.1. Corpus and Setup\nWe use the publicly-available, widely-used LibriSpeech corpus [24]\nfor experiments. LibriSpeech comes with 960 hours of read speech\ndata for training, and 4 sets {dev, test}-{clean,other}for\nﬁne-tuning and evaluations. The clean sets contain high quality\nutterances where as the other sets are more acoustically challeng-\ning. We use dev-{clean,other}sets to ﬁne-tune parameters\nfor beam search and report results on test-{clean,other}re-\nsults. We extract 80-dimensional log Mel-ﬁlter bank features ev-\nery 10ms as acoustic features and normalize them with global mean\ncomputed from the training set. We also apply SpecAugment [25]\nwith policy ”LD” for data distortion. A sentence piece model [26]\nwith 256 symbols is trained from transcriptions of the training set\nand serves as the output symbols. For each model, we use a learnable\nembedding layer to convert symbols to 128-dimensional vectors just\nbefore the predictor. The experiments are done using PyTorch [27]\nand Fairseq [28] All models are trained on 32 GPUs with distributed\ndata parallel (DDP) mechanism. We use standard beam search with\nbeam size of 10 for decoding. The decoded sentence pieces are then\nconcatenated into hypotheses to be compared with ground truth tran-\nscription for word error rate (WER) evaluation.\n5.2. Model Architectures and Details\nWe compare architectures with roughly the same number of param-\neters in total. For the encoder in neural transducer, we evaluate\noptions including 1) BLSTM 4x640: bidirectional LSTM with 4\nlayers of 640 hidden units in each direction, 2) LSTM 5x1024:\nLSTM with 5 layers of 1024 hidden units and 3) Transformer\n12x: VGG-Transformer with 2 layers of VGGNets and 12 Trans-\nformer encoder layers. Each VGGNet layer contain 2 layers of two-\ndimension convolution of 64 kernels of size 3x3. Each Transformer\nencoder layer takes 512-dimensional inputs, with 8 heads for multi-\nhead self-attention and 2048 as the feed-forward dimension. For efﬁ-\ncient inference, all encoders generate output encodings every 60ms.\nFor LSTM/BLSTM this is achieved with low frame rate [29] in\nwhich every three consecutive frames are stacked and subsampled\nto form the new frame, and apply subsampling of factor 2 to the out-\nput of the second LSTM/BLSTM layer [6]. For VGG-Transformer\nwe set the max-pooling on time dimension to 3 for the 1st VGGNet\nand 2 for the 2nd VGGNet, as illustrated in Fig. 2(b).\nFor the predictor in neural transducer, we evaluate options in-\ncluding 1) LSTM 2x700: LSTM with 2 layers of 700 hidden units\nand 2) Transformer 6x: VGG-Transformer with 1 layer of VG-\nGNet and 6 Transformer encoder layers. Both the VGGNet layer and\nthe Transformer encoder layers share the same conﬁguration with the\nthe encoder case, with the exception that max-pooling is removed in\nthe VGGNet. In addition, the right context R for these the Trans-\nformer encoders is 0 for preventing future information leakage.\nFor the joiner in neural transducer, outputs from the encoder ht\nand the predictor pu are joined with:\nzt,u = φ(htWh + puWp)Wo (5)\nwhere Wh ∈Rdh×dJ and Wp ∈Rdp×dJ project ht and pu to a\ncommon feature space of dimensiondJ, φ() is an activation function\nand Wo ∈RdJ×do generates the logits zt,u. We use dJ = 640,\nφ= ReLU and do = 256consistently for all experiments.\n5.3. Results on Transformer/LSTM Combinations\nWe experimented with combinations of Transformer and LSTM\nnetworks for neural transducer. The results are summarized in\nTable 1. For the encoder, we use LSTM 5x1024 as the stream-\nable baseline, BLSTM 5x640 as the non-streamable baseline and\nTransformer 12x as the novel replacement for the two. For the\npredictor, we useLSTM 2x700 and Transformer 6x described\nin section 5.2 as the two options.\nTable 1: Neural Transducer with (B)LSTM / Transformer.\nencoder predictor # params test-\nclean\ntest-\nother\n(1) LSTM 5x1024 LSTM 2x700 50.5 M 12.31 23.16\n(2) BLSTM 4x640LSTM 2x700 48.3 M 6.85 16.90\n(3) Transformer 12xLSTM 2x700 45.7 M 6.08 13.89\n(4) LSTM 5x1024 Transformer 6x 67.1 M 15.76 26.67\n(5) BLSTM 4x640Transformer 6x 64.9 M 7.20 16.67\n(6) Transformer 12xTransformer 6x 62.3 M 7.11 15.62\nFrom Table 1, given the same conﬁguration for the predictor we\nsee that it is difﬁcult for the LSTM network as encoder to perform\nwell given the constraint on number of parameters. The bidirectional\nLSTM (BLSTM) network however can compensate the performance\nand remain compact in size at the cost of being non-streamable.\nThe VGG-Transformer with unlimited self-attention outperforms\nBLSTM signiﬁcantly as the encoder and is also non-streamable.\nFor the predictor, for all encoder conﬁgurations we see the LSTM\nnetwork still gives better results than the VGG-Transformer and is\nsmaller in size. As a result we keep LSTM 2x700 as the predic-\ntor for the experiments in section 5.4. It is worth noting that the\nVGG-Transformer loses the advantage of parallel computation as\nthe predictor, as during beam search the hypothesis also extends a\ntoken at one search step.\n5.4. Results on Truncated Self-Attention\nWe evaluated the impact of the contexts (L,R) in truncated self-\nattention on recognition accuracy for the VGG-Transformer. As\nsummarized in section 5.3, we ﬁnd the VGG-Transformer performs\nwell as the encoder but not as the predictor. Therefore we keep\nLSTM 2x700 as the predictor for the experiments in truncated self-\nattention. The results are summarized in Table 2, where (L,R) are\nused for truncated self-attention in the VGG-Transformer per layer\nand aggregate through layers.\nTable 2: Transformer with Truncated Self-Attention.\nModel Architecture L R test-\nclean\ntest-\nother\n(1) LSTM 5x1024 + LSTM 2x700inf 0 12.31 23.16\n(2) BLSTM 4x640 + LSTM 2x700inf inf 6.85 16.90\n(3) Transformer 12x + LSTM 2x700inf inf 6.08 13.89\n(4) Transformer 12x + LSTM 2x700inf 0 12.32 23.08\n(5) Transformer 12x + LSTM 2x700inf 1 6.99 16.88\n(6) Transformer 12x + LSTM 2x700inf 2 6.47 15.79\n(7) Transformer 12x + LSTM 2x700inf 4 6.14 14.86\n(8) Transformer 12x + LSTM 2x700inf 8 5.99 14.17\n(9) Transformer 12x + LSTM 2x7004 4 6.84 17.38\n(10) Transformer 12x + LSTM 2x7008 4 6.69 16.79\n(11) Transformer 12x + LSTM 2x70016 4 6.57 15.92\n(12) Transformer 12x + LSTM 2x70032 4 6.37 15.30\nSince the right context Rintroduces algorithmic latency and has\nmajor impact on the recognition accuracy, to ﬁnd optimal parame-\nters for truncated self-attention, we search for the right context R\nﬁrst while keeping the left context Lunlimited and then reduce the\nleft context Lgiven the selected right context R. From Table 2 we\nsee both L and R have signiﬁcant impact on the performance, es-\npecially when R = 0when the VGG-Transformer becomes purely\ncausal. However, as Rincreases, the WERs gradually recover and\ncome close to the case of unlimited self-attention whenR= 8. With\nlimited right context R, the VGG-Transformer becomes streamable\nbut still is O(T2) in computational complexity due to the unlimited\nleft context L. To keep reasonable performance while minimizing\nlatency at the same time, we selected right context R= 4and eval-\nuate different left contexts L. Similar to right context R, we see the\nWER is also sensitive to left context L. With (L,R) = (16,4) we\nsee the VGG-Transformer with truncated self-attention gives better\nWER than both LSTM/BLSTM baselines. With (L,R) = (32,4)\nwe only lose 4.7 % on test-clean and 10.1 % on test-other\nrelatively compared with the case of umlimited self-attention, but the\nsystem becomes streamable and efﬁcient with computational com-\nplexity O(T).\n6. CONCLUSION\nIn this paper, we explore options for using the Transformer networks\nin neural transducer for end-to-end speech recognition. The Trans-\nformer network uses self-attention for sequence modeling and can\ncompute in parallel. With causal convolution and truncated self-\nattention, the neural transducer with the proposed VGG-Transformer\nas the encoder achieved 6.37 % on the test-clean set and 15.30\n% on the test-other set of the public corpus LibriSpeech with a\nsmall footprint of 45.7 M parameters for the entire system. The pro-\nposed Transformer-Transducer is accurate, streamable, compact and\nefﬁcient, therefore a promising option for resource-limited scenarios\nsuch as on-device speech recognition.\n7. REFERENCES\n[1] George E Dahl, Dong Yu, Li Deng, and Alex Acero,\n“Context-dependent pre-trained deep neural networks for\nlarge-vocabulary speech recognition,” IEEE Transactions on\naudio, speech, and language processing, vol. 20, no. 1, pp. 30–\n42, 2011.\n[2] Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent\nJ Della Pietra, and Jenifer C Lai, “Class-based n-gram models\nof natural language,” Computational linguistics, vol. 18, no. 4,\npp. 467–479, 1992.\n[3] Tom ´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan ˇCernock`y,\nand Sanjeev Khudanpur, “Recurrent neural network based lan-\nguage model,” in Eleventh annual conference of the interna-\ntional speech communication association, 2010.\n[4] Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide,\nMike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig,\n“Achieving human parity in conversational speech recogni-\ntion,” arXiv preprint arXiv:1610.05256, 2016.\n[5] Alex Graves, “Sequence transduction with recurrent neural\nnetworks,” arXiv preprint arXiv:1211.3711, 2012.\n[6] Yanzhang He, Tara N Sainath, Rohit Prabhavalkar, Ian Mc-\nGraw, Raziel Alvarez, Ding Zhao, David Rybach, Anjuli Kan-\nnan, Yonghui Wu, Ruoming Pang, et al., “Streaming end-to-\nend speech recognition for mobile devices,” in ICASSP 2019-\n2019 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2019, pp. 6381–6385.\n[7] Kanishka Rao, Has ¸im Sak, and Rohit Prabhavalkar, “Exploring\narchitectures, data and units for streaming end-to-end speech\nrecognition with rnn-transducer,” in 2017 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU).\nIEEE, 2017, pp. 193–199.\n[8] Abdelrahman Mohamed, Dmytro Okhonko, and Luke Zettle-\nmoyer, “Transformers with convolutional context for asr,”\narXiv preprint arXiv:1904.11660, 2019.\n[9] Alex Graves, Santiago Fern ´andez, Faustino Gomez, and J¨urgen\nSchmidhuber, “Connectionist temporal classiﬁcation: la-\nbelling unsegmented sequence data with recurrent neural net-\nworks,” in Proceedings of the 23rd international conference\non Machine learning. ACM, 2006, pp. 369–376.\n[10] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech\nrecognition with recurrent neural networks,” in International\nconference on machine learning, 2014, pp. 1764–1772.\n[11] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals,\n“Listen, attend and spell: A neural network for large vocabu-\nlary conversational speech recognition,” in2016 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2016, pp. 4960–4964.\n[12] Linhao Dong, Feng Wang, and Bo Xu, “Self-attention aligner:\nA latency-control end-to-end model for asr using self-attention\nnetwork and chunk-hopping,” in ICASSP 2019-2019 IEEE In-\nternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2019, pp. 5656–5660.\n[13] Sepp Hochreiter and J ¨urgen Schmidhuber, “Long short-term\nmemory,” Neural computation, vol. 9, no. 8, pp. 1735–1780,\n1997.\n[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin, “Attention is all you need,” in Advances in neural\ninformation processing systems, 2017, pp. 5998–6008.\n[15] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,\nKyunghyun Cho, and Yoshua Bengio, “Attention-based mod-\nels for speech recognition,” in Advances in neural information\nprocessing systems, 2015, pp. 577–585.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova, “Bert: Pre-training of deep bidirectional\ntransformers for language understanding,” arXiv preprint\narXiv:1810.04805, 2018.\n[17] Karen Simonyan and Andrew Zisserman, “Very deep convo-\nlutional networks for large-scale image recognition,” arXiv\npreprint arXiv:1409.1556, 2014.\n[18] Eric Battenberg, Jitong Chen, Rewon Child, Adam Coates,\nYashesh Gaur Yi Li, Hairong Liu, Sanjeev Satheesh, Anuroop\nSriram, and Zhenyao Zhu, “Exploring neural transducers\nfor end-to-end speech recognition,” in 2017 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU).\nIEEE, 2017, pp. 206–213.\n[19] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio,\n“Neural machine translation by jointly learning to align and\ntranslate,” arXiv preprint arXiv:1409.0473, 2014.\n[20] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton,\n“Layer normalization,” arXiv preprint arXiv:1607.06450,\n2016.\n[21] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,\nand Yann N Dauphin, “Convolutional sequence to sequence\nlearning,” in Proceedings of the 34th International Conference\non Machine Learning-Volume 70. JMLR. org, 2017, pp. 1243–\n1252.\n[22] Alexander Waibel, Toshiyuki Hanazawa, Geoffrey Hinton,\nKiyohiro Shikano, and Kevin J Lang, “Phoneme recognition\nusing time-delay neural networks,” Backpropagation: Theory,\nArchitectures and Applications, pp. 35–61, 1995.\n[23] Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur,\n“A time delay neural network architecture for efﬁcient model-\ning of long temporal contexts,” inSixteenth Annual Conference\nof the International Speech Communication Association, 2015.\n[24] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev\nKhudanpur, “Librispeech: an asr corpus based on public do-\nmain audio books,” in 2015 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP). IEEE,\n2015, pp. 5206–5210.\n[25] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D Cubuk, and Quoc V Le, “Specaugment: A\nsimple data augmentation method for automatic speech recog-\nnition,” arXiv preprint arXiv:1904.08779, 2019.\n[26] Taku Kudo and John Richardson, “Sentencepiece: A simple\nand language independent subword tokenizer and detokenizer\nfor neural text processing,” arXiv preprint arXiv:1808.06226,\n2018.\n[27] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan,\nEdward Yang, Zachary DeVito, Zeming Lin, Alban Desmai-\nson, Luca Antiga, and Adam Lerer, “Automatic differentiation\nin pytorch,” in NIPS-W, 2017.\n[28] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam\nGross, Nathan Ng, David Grangier, and Michael Auli, “fairseq:\nA fast, extensible toolkit for sequence modeling,” in Proceed-\nings of NAACL-HLT 2019: Demonstrations, 2019.\n[29] Golan Pundak and Tara N Sainath, “Lower frame rate neural\nnetwork acoustic models,” Interspeech 2016, pp. 22–26, 2016."
}