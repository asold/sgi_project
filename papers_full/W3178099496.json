{
    "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers",
    "url": "https://openalex.org/W3178099496",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221559792",
            "name": "Yang, Ruihan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2348781631",
            "name": "Zhang Ming-hao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221940329",
            "name": "Hansen, Nicklas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222266827",
            "name": "Xu, Huazhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1806612777",
            "name": "Wang Xiaolong",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3099126917",
        "https://openalex.org/W3205321526",
        "https://openalex.org/W3154596443",
        "https://openalex.org/W2969111820",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2937521543",
        "https://openalex.org/W2964051877",
        "https://openalex.org/W2899496413",
        "https://openalex.org/W124445416",
        "https://openalex.org/W1985875375",
        "https://openalex.org/W1191599655",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W3104876774",
        "https://openalex.org/W2070038508",
        "https://openalex.org/W2798036083",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2786036274",
        "https://openalex.org/W2726187156",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W2551887912",
        "https://openalex.org/W2145339207",
        "https://openalex.org/W3155758913",
        "https://openalex.org/W2135131592",
        "https://openalex.org/W3204973825",
        "https://openalex.org/W2804941773",
        "https://openalex.org/W3105609823",
        "https://openalex.org/W2977481643",
        "https://openalex.org/W3085605093",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2909553221",
        "https://openalex.org/W3172863135",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3143320354",
        "https://openalex.org/W3012366945",
        "https://openalex.org/W2968883249",
        "https://openalex.org/W1981271452",
        "https://openalex.org/W3098053103",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W3090207068",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W2962736495",
        "https://openalex.org/W3196685263",
        "https://openalex.org/W2801567109",
        "https://openalex.org/W2950739196",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2991420892",
        "https://openalex.org/W2967936796",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2002853107",
        "https://openalex.org/W3048659003",
        "https://openalex.org/W2978854499",
        "https://openalex.org/W2895915466",
        "https://openalex.org/W2992977009",
        "https://openalex.org/W3003817470",
        "https://openalex.org/W3043840704",
        "https://openalex.org/W2911087563",
        "https://openalex.org/W2964161785",
        "https://openalex.org/W1981932940",
        "https://openalex.org/W2001165022",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3041890730",
        "https://openalex.org/W2970786335",
        "https://openalex.org/W2736601468",
        "https://openalex.org/W2909331752",
        "https://openalex.org/W2567015638",
        "https://openalex.org/W2739330054",
        "https://openalex.org/W3205250329",
        "https://openalex.org/W3108883557",
        "https://openalex.org/W3127756416",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2963184939",
        "https://openalex.org/W3126442315",
        "https://openalex.org/W2969862959",
        "https://openalex.org/W2076337359",
        "https://openalex.org/W3115293622",
        "https://openalex.org/W2751973545",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W3178351738",
        "https://openalex.org/W3039737909",
        "https://openalex.org/W2951805468",
        "https://openalex.org/W2139053308"
    ],
    "abstract": "We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method that leverages both proprioceptive states and visual observations for locomotion control. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We transfer our learned policy from simulation to a real robot by running it indoors and in the wild with unseen obstacles and terrain. Our method not only significantly improves over baselines, but also achieves far better generalization performance, especially when transferred to the real robot. Our project page with videos is at https://rchalyang.github.io/LocoTransformer/ .",
    "full_text": "Published as a conference paper at ICLR 2022\nLEARNING VISION -GUIDED QUADRUPEDAL LOCO -\nMOTION END-TO-END WITH CROSS -MODAL TRANS -\nFORMERS\nRuihan Yang∗\nUC San Diego\nMinghao Zhang∗\nTsinghua University\nNicklas Hansen\nUC San Diego\nHuazhe Xu\nUC Berkeley\nXiaolong Wang\nUC San Diego\nABSTRACT\nWe propose to address quadrupedal locomotion tasks using Reinforcement Learn-\ning (RL) with a Transformer-based model that learns to combine proprioceptive\ninformation and high-dimensional depth sensor inputs. While learning-based lo-\ncomotion has made great advances using RL, most methods still rely on domain\nrandomization for training blind agents that generalize to challenging terrains.\nOur key insight is that proprioceptive states only offer contact measurements for\nimmediate reaction, whereas an agent equipped with visual sensory observations\ncan learn to proactively maneuver environments with obstacles and uneven terrain\nby anticipating changes in the environment many steps ahead. In this paper, we\nintroduce LocoTransformer, an end-to-end RL method that leverages both propri-\noceptive states and visual observations for locomotion control. We evaluate our\nmethod in challenging simulated environments with different obstacles and uneven\nterrain. We transfer our learned policy from simulation to a real robot by running it\nindoors and in the wild with unseen obstacles and terrain. Our method not only\nsigniﬁcantly improves over baselines, but also achieves far better generalization\nperformance, especially when transferred to the real robot. Our project page with\nvideos is at https://rchalyang.github.io/LocoTransformer/.\n1 I NTRODUCTION\nLegged locomotion is one of the core problems in robotics research. It expands the reach of robots\nand enables them to solve a wide range of tasks, from daily life delivery to planetary exploration in\nchallenging, uneven terrain (Delcomyn & Nelson, 2000; Arena et al., 2006). Recently, besides the\nsuccess of Deep Reinforcement Learning (RL) in navigation (Mirowski et al., 2017; Gupta et al.,\n2019; Yang et al., 2019; Kahn et al., 2021) and robotic manipulation (Levine et al., 2018; 2016; Tian\net al., 2019; Jain et al., 2019b), we have also witnessed the tremendous improvement of locomotion\nskills for quadruped robots, allowing them to walk on uneven terrain (Xie et al., 2020; 2021), and\neven generalize to real-world with mud, snow, and running water (Lee et al., 2020a).\nWhile these results are encouraging, most RL approaches focus on learning a robust controller\nfor blind quadrupedal locomotion, using only the proprioceptive state. For example, Lee et al.\n(2020a) utilize RL with domain randomization and large-scale training in simulation to learn a robust\nquadrupedal locomotion policy, which can be applied to challenging terrains. However, is domain\nrandomization with blind agents really sufﬁcient for general legged locomotion?\nBy studying eye movement during human locomotion, Matthis et al. (2018) show that humans\nrely heavily on eye-body coordination when walking and that the gaze changes depending on\ncharacteristics of the environment, e.g., whether humans walk in ﬂat or rough terrain. This ﬁnding\nmotivates the use of visual sensory input to improve quadrupedal locomotion on uneven terrain. While\nhandling uneven terrain is still possible without the vision, a blind agent is unable to consistently\navoid large obstacles as shown in Figure 1. To maneuver around such obstacles, the agent needs to\nperceive the obstacles at a distance and dynamically make adjustments to its trajectory to avoid any\ncollision. Likewise, an agent navigating rough terrain (mountain and forest in Figure 1) may also\n∗Equal Contribution\n1\narXiv:2107.03996v3  [cs.LG]  26 May 2022\nPublished as a conference paper at ICLR 2022\nWideObstacle&Sphere\n Mountain\nThinObstacle&Sphere\n MovingObstacle\nFigure 1: Overview of simulated environments & real robot trajectories.Top rowshows the simulated\nenvironments. For each sample, the left image is the environment and the right image is the corresponding\nobservation. Agents are tasked to move forward while avoiding black obstacles and collecting red spheres.\nFollowing two rowsshow the deployment of the RL policy to a real robot in an indoor hallway with boxes and a\nforest with trees. Our robot successfully utilizes the visual information to traverse the complex environments.\nbeneﬁt from vision by anticipating changes in the terrain before contact, and visual observations can\ntherefore play an important role in improving locomotion skills.\nIn this paper, we propose to combine proprioceptive states and ﬁrst-person-view visual inputs with a\ncross-modal Transformer for learning locomotion RL policies. Our key insight is that proprioceptive\nstates (i.e. robot pose, Inertial Measurement Unit (IMU) readings, and local joint rotations) provide\na precise measurement of the current robot status for immediate reaction, while visual inputs from\na depth camera help the agent plan to maneuver uneven terrain and large obstacles in the path. We\nfuse the proprioceptive state and depth image inputs using Transformers (Vaswani et al., 2017; Tsai\net al., 2019) for RL, which enables the model to reason with complementary information from both\nmodalities. Additionally, Transformers also offer a mechanism for agents to attend to speciﬁc visual\nregions (e.g. objects or uneven ground) that are critical for their long-term and short-term decision\nmaking, which may in turn lead to a more generalizable and interpretable policy.\nOur Transformer-based model for locomotion, LocoTransformer, consists of two encoders for inputs\n(an MLP for proprioceptive states, a ConvNet for depth image inputs) and Transformer encoders for\nmulti-modal fusion. We obtain a feature embedding from the proprioceptive states and multiple image\npatch embeddings from the depth images, which are used jointly as token inputs for the Transformer\nencoders. Feature embeddings for both modalities are then updated with information propagation\namong all the tokens using self-attention. We combine both features for action prediction. The model\nis trained end-to-end without hierarchical RL (Peng et al., 2017; Jiang et al., 2019; Jain et al., 2019a)\nnor pre-deﬁned controllers (Da et al., 2020; Escontrela et al., 2020).\nWe experiment on both simulated and real environments as shown in Figure 1. Our tasks in simulation\ninclude maneuvering around obstacles of different sizes, dynamically moving obstacles, and rough\nmountainous terrain. With simulation-to-real (sim2real) transfer, we deploy the policies to the robot\non indoor hallways with box obstacles and outdoor forests with trees and uneven terrain. We show\nthat learning policies with both proprioceptive states and vision signiﬁcantly improve locomotion\ncontrol, and the policies further beneﬁt from adopting cross-modal Transformer. We also show that\nLocoTransformer generalizes much better to unseen environments, especially for sim2real transfer.\nWe highlight our main contributions as follows:\n• Going beyond blind robots, we introduce visual information into end-to-end RL policies for\nquadrupedal locomotion to traverse complex terrain with different kinds of obstacles.\n• We propose LocoTransformer, which fuses proprioceptive states and visual inputs for better\nmulti-modal reasoning in sequential decision making.\n• To the best of our knowledge, this is the ﬁrst work which deploys vision-based RL policy on\nrunning real quadrupedal robot avoiding obstacles and trees in the wild.\n2 R ELATED WORK\nLearning Legged Locomotion.Developing legged locomotion controllers has been a long-standing\nproblem in robotics (Miura & Shimoyama, 1984; Raibert, 1984; Torkos & van de Panne, 1998; Geyer\n2\nPublished as a conference paper at ICLR 2022\net al., 2003; Yin et al., 2007; Bledt et al., 2018). While encouraging results have been achieved\nusing Model Predictive Control (MPC) and trajectory optimization (Gehring et al., 2013; Carlo et al.,\n2018; Di Carlo et al., 2018; Carius et al., 2019; Ding et al., 2019; Grandia et al., 2019; Bledt & Kim,\n2020; Sun et al., 2021), these methods require in-depth knowledge of the environment and substantial\nmanual parameter tuning, which makes these methods challenging to apply to complex environments.\nAlternatively, model-free RL can learn general policies on challenging terrain (Kohl & Stone, 2004;\nZhang et al., 2018; Luo et al., 2020; Peng et al., 2018; Tan et al., 2018; Hwangbo et al., 2019; Lee\net al., 2020a; Iscen et al., 2018; Jain et al., 2019a; Xie et al., 2021; Kumar et al., 2021). Xie et al.\n(2020) use dynamics randomization to generalize RL locomotion policy in different environments,\nand Peng et al. (2020) use animal videos to provide demonstrations for imitation learning. However,\nmost approaches currently rely only on proprioceptive states without other visual inputs. In this work,\nwe propose to incorporate both visual and proprioceptive inputs using a Transformer for RL policy,\nwhich allows the quadruped robot to simultaneously move and plan its trajectory.\nVision-based Reinforcement Learning.To generalize RL to real-world applications beyond state\ninputs, a lot of effort has been made in RL with visual inputs (Sax et al., 2018; Jaderberg et al.,\n2017; Levine et al., 2016; 2018; Pathak et al., 2017; Jain et al., 2019b; Mnih et al., 2015a; Lin\net al., 2019; Yarats et al., 2019; Laskin et al., 2020; Stooke et al., 2020; Schwarzer et al., 2020). For\nexample, Srinivas et al. (2020) propose to apply contrastive self-supervised representation learning (He\net al., 2020) with the RL objective in vision-based RL. Hansen & Wang (2021) further extend the\njoint representation learning and RL for better generalization to out-of-distribution environments.\nResearchers have also looked into combining multi-modalities with RL for manipulation tasks (Lee\net al., 2020b; Calandra et al., 2018) and locomotion control (Heess et al., 2017; Merel et al., 2020).\nEscontrela et al. (2020) propose to combine proprioceptive states and LiDAR inputs for learning\nquadrupedal locomotion with RL using MLPs. Jain et al. (2020) propose to use Hierarchical RL\n(HRL) for locomotion, which learns high-level policies under visual guidance and low-level motor\ncontrol policies with IMU inputs. Different from previous work, we provide a simple approach to\ncombine proprioceptive states and visual inputs with a Transformer model in an end-to-end manner\nwithout HRL. Our LocoTransformer not only performs better in challenging environments but also\nachieves better generalization results in unseen environments and with the real robot.\nTransformers and Multi-modal Learning.The Transformer model has been widely applied in the\nﬁelds of language processing (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020) and visual\nrecognition and synthesis (Wang et al., 2018; Parmar et al., 2018; Child et al., 2019; Dosovitskiy\net al., 2020; Carion et al., 2020; Chen et al., 2020a). Besides achieving impressive performance in\na variety of language and vision tasks, the Transformer also provides an effective mechanism for\nmulti-modal reasoning by taking different modality inputs as tokens for self-attention (Su et al., 2019;\nTan & Bansal, 2019; Li et al., 2019; Sun et al., 2019; Chen et al., 2020b; Li et al., 2020; Prakash\net al., 2021; Huang et al., 2021; Hu & Singh, 2021; Akbari et al., 2021; Hendricks et al., 2021). For\nexample, Sun et al. (2019) propose to use a Transformer to jointly model video frames and their\ncorresponding captions from instructional videos for representation learning. Going beyond language\nand vision, we propose to utilize cross-modal Transformers to fuse proprioceptive states and visual\ninputs. To our knowledge, this is the ﬁrst work using cross-modal Transformers for locomotion.\n3 R EINFORCEMENT LEARNING BACKGROUND\nWe model the interaction between the robot and the environment as an MDP (Bellman, 1957)\n(S,A,P, R,H,γ ), where s ∈S are states, a ∈Aare actions, P(s′|s,a) is transition function, R\nis reward function, H is ﬁnite episode horizon, and γis discount factor. The Agent learn a policy\nπθ parameterized by θ to output actions distribution conditioned on current state. The goal of\nagent is to learn θthat maximizes the discounted episode return: R= Eτ∼pθ(τ)[∑H\nt=0 γtrt], where\nrt ∼R(st,at) is the reward for time step t, τ ∼pθ(τ) is the trajectory.\n4 M ETHOD\nWe propose to incorporate both proprioceptive and visual information for locomotion tasks using a\nnovel Transformer model, LocoTransformer. Figure 2 provides an overview of our architecture. Our\nmodel consists of the following two components: (i) Separate modality encoders for proprioceptive\nand visual inputs that project both modalities into a latent feature space; (ii) A shared Transformer\nencoder that performs cross-modality attention over proprioceptive features and visual features, as\nwell as spatial attention over visual tokens to predict actions and predict values.\n3\nPublished as a conference paper at ICLR 2022\n4.1 S EPARATE MODALITY ENCODERS\nSharedTransformerEncoder\nProprioceptiveState\n Conv\n……\nProprioceptiveFeaturesVisualFeatures\nProjectionHead\nOutput\nLx SelfAttentionAdd&Norm\nFeedForwardAdd&Norm\nDepthImageLinearLinear\nFigure 2: Network Architecture. We process proprioceptive\nstates with a MLP and depth images with a ConvNet. We take\nproprioceptive embedding as a single token, split the spatial visual\nfeature representation into N ×N tokens and feed all tokens into\nthe Transformer encoder. The output tokens are further processed\nby the projection head to predict value or action distribution.\nIn our setting, the agent utilizes both\nproprioceptive states and visual ob-\nservations for decision-making. Pro-\nprioceptive state and visual observa-\ntion are distinctively different modal-\nities: the proprioceptive input is a 93-\nD vector, and we use stacked ﬁrst-\nperson view depth images to encode\nthe visual observations. To facili-\ntate domain-speciﬁc characteristics of\nboth modalities, we use two separate,\ndomain-speciﬁc encoders for propri-\noceptive and visual data respectively,\nand unify the representation in a latent\nspace. We now introduce the architec-\ntural design of each encoder, and how\nfeatures are converted into tokens for\nthe Transformer encoder.\nWe use an MLP to encode the propri-\noceptive input into proprioceptive fea-\ntures Eprop ∈RCprop\n, where Cprop is\nthe proprioceptive feature dimension.\nWe encode additionally provided visual information using a ConvNet. The ConvNet encoder forwards\nthe stacked depth image inputs into a spatial representation Evisual with shape C×N ×N, where C\nis the channel number, and N is the width and height of the representation. The depth images are\nfrom the ﬁrst-person view from the frontal of the robot, which captures the obstacles and terrain from\nthe perspective of the acting robot. However, for ﬁrst-person view, the moving camera and limited\nﬁeld-of-view make learning visual policies signiﬁcantly more challenging. For instance, changes in\nrobot pose can result in changes in visual observations. This makes it essential to leverage propri-\noceptive information to improve visual understanding. In the following, we present our proposed\nmethod for fusing the two modalities and improving their joint representation using a Transformer.\n4.2 T RANSFORMER ENCODER\nWe introduce the Transformer encoder to fuse the visual observations and the proprioceptive states for\ndecision making. Given a spatial visual features with shape C×N ×N from the ConvNet encoder,\nwe split the spatial features into N ×N different C-dimensional token embeddings tvisual ∈RC\n(illustrated as yellow tokens in Figure 2), each corresponding to a local visual region. We use a\nlinear layer to project the proprioceptive features into a C-dimensional token embedding tprop ∈RC\n(illustrated as a green token in Figure 2). Formally, we have N ×N + 1tokens in total obtained by:\ntprop = Wprop(Eprop) +bprop tprop ∈RC (1)\nT0 = [tprop,tvisual\n0,0 ,tvisual\n0,1 ,...,t visual\nN−1,N−1] tvisual\ni,j ∈RC (2)\nwhere tvisual\ni,j is the token at spatial position (i,j) of the visual features Evisual, and Wprop,bprop are\nthe weights and biases, respectively, of the linear projection for proprioceptive token embedding. In\nthe following, we denote Tm ∈R(N2+1)×C as the sequence of tokens after mTransformer encoder\nlayers, and T0 as the input token sequence from Eq. 2.\nWe adopt a stack of Transformer encoder layers (Vaswani et al., 2017) to fuse information from\nproprioceptive and visual tokens. Speciﬁcally, we formulate the Self-Attention (SA) mechanism of\nthe Transformer encoder as a scaled dot-product attention mechanism, omitting subscripts for brevity:\nTq,Tk,Tv = TUq,TU k,TU v Uq,Uk,Uv ∈RC×C (3)\nWsum = Softmax(TqTk⊤\n/\n√\nD) Wsum ∈R(N2+1)×(N2+1) (4)\nSA(T) =WsumTvUSA USA ∈RC×C (5)\n4\nPublished as a conference paper at ICLR 2022\nwhere Dis the dimension of the self-attention layer. The SA mechanism ﬁrst applies separate linear\ntransformations on each input token T to produce embeddings Tq,Tk,Tv as deﬁned in Eq. 3. We\nthen compute a weighted sum over input tokens, where the weight Wsum\ni,j for each token pair (ti,tj)\nis computed as the dot-product of elements ti and tj scaled by 1/\n√\nDand normalized by a Softmax\noperation. After a matrix multiplication between weights Wsum and values Tv, we forward the result\nto a linear layer with parameters USA as in Eq. 5, and denote this as the output SA(T).\nEach Transformer encoder layer consists of a self-attention layer, two LayerNorm (LN) layers with\nresidual connections, and a 2-layer MLP as shown in Figure 2 (right). This is formally expressed as,\nT′\nm = LN(SA(Tm) +Tm), T m+1 = LN(MLP(T′\nm) +T′\nm), T m,Tm+1 ∈R(N2+1)×C (6)\nwhere T′\nm is the normalized SA. Because SA is computed across visual tokens and single propriocep-\ntive token, proprioceptive information may gradually vanish in multi-layer Transformers; the residual\nconnections make the propagation of proprioceptive information through the network easier.\nWe stack LTransformer encoder layers. Performing multi-layer self-attention on proprioceptive and\nvisual features enables our model to fuse tokens from both modalities at multiple levels of abstraction.\nFurther, we emphasize that a Transformer-based fusion allows spatial reasoning, as each visual token\nhas a separate regional receptive ﬁeld, and self-attention, therefore, enables the agent to explicitly\nattend to relevant visual regions. For modality-level fusion, direct application of a pooling operation\nacross all tokens would easily dilute proprioceptive information since the number of visual tokens far\nexceed that of the proprioceptive token. To balance information from both modalities, we ﬁrst pool\ninformation separately for each modality, compute the mean of all tokens from the same modality to\nget a single feature vector. We then concatenate the feature vectors of both modalities and project the\nconcatenated vector into a ﬁnal output vector using an MLP, which we denote the projection head.\nObservation Space.In all environments, the agent receives both proprioceptive states and visual\ninput as follows: (i) proprioceptive data: a 93-D vector consists of IMU readings, local joint\nrotations, and actions taken by the agent for the last three time steps; and (ii) visual data: stacked the\nmost recent 4 dense depth image of shape 64 ×64 from a depth camera mounted on the head of the\nrobot, which provides the agent with both spatial and temporal visual information.\nImplementation Details.For the proprioceptive encoder and the projection head, we use a 2-layer\nMLP with hidden dimensions (256,256). Our visual encoder encode visual inputs into 4 ×4 spatial\nfeature maps with 128 channels, following the architecture in Mnih et al. (2015b). Our shared\nTransformer consists of 2 Transformer encoder layers, each with a hidden feature dimension of 256.\n5 E XPERIMENTS\nWe evaluate our method in simulation and the real world. In the simulation, we simulate a quadruped\nrobot in a set of challenging and diverse environments. In the real world, we conduct experiments in\nindoor scenarios with obstacles and in-the-wild with complex terrain and novel obstacles.\n5.1 E NVIRONMENTS IN SIMULATION\nWe design 6 simulated environments with varying terrain, obstacles to avoid, and spheres to collect\nfor reward bonuses. Spheres are added to see whether agents are able to distinguish objects and their\nassociated functions based on their appearance. All obstacles and spheres are randomly initialized and\nremain static throughout the episode unless stated otherwise. Speciﬁcally, our environments include:\nWide Obstacle(Wide Obs.): wide cuboid obstacles on ﬂat terrain, without spheres; Thin Obstacle\n(Thin Obs.): numerous thin cuboid obstacles on ﬂat terrain, without spheres; Wide Obstacle &\nSphere (Wide Obs.& Sph.): wide cuboid obstacles on ﬂat terrain, including spheres that give a\nreward bonus when collected; Thin Obstacle & Sphere(Thin Obs.& Sph.): numerous thin cuboid\nobstacles and spheres on a ﬂat terrain; Moving Obstacle: similar to the Thin Obs.environment, but\nobstacles are now dynamically moving in random directions updated at a low frequency. Mountain:\na rugged mountain range with a goal on the top of the mountain. We show 4 environments above\nin Figure 1, omitting Wide Obs. and Thin Obs. for simplicity. We provide further details on the\nobservation and action space, speciﬁc reward function, and relevant hyper-parameters in Appendix A.\nReward Function.For all environments, we adopt the same reward function containing the following\nterms: (i) Forward rewardincentivizing the robot to move forward along a task-speciﬁc direction, i.e.\ntowards the goal position in the Mountain environment (visualized as the red sphere in Figure 1), or\n5\nPublished as a conference paper at ICLR 2022\n1.0\n0.5\n0.0\n(a) In the environment with obstacles, the agent learns to automatically attend to obstacles.\n1.0\n0.5\n0.0\n(b) On challenging terrain, the agent attends to the goal destination and the local terrain in an alternative manner.\nFigure 3: Self-attention from our shared Transformer module.We visualize the self-attention\nbetween the proprioceptive token and all visual tokens in the last layer of our Transformer model. We\nplot the attention weight over raw visual input where warmer color represents larger attention weight.\nthe move along the axis in all other environments (i.e. moving forward); (ii) Sphere rewardfor each\nsphere collected; (iii) Alive rewardencouraging the agent to avoid unsafe situations, e.g. falling; and\n(iv) Energy usage penaltyencouraging the agent to use motor torque of small magnitude.\n5.2 B ASELINE AND EXPERIMENTAL SETTING\nTo demonstrate the importance of visual information for locomotion in complex environments, as\nwell as the effectiveness of our Transformer model, we compare our method to: State-Only baseline\nthat only uses proprioceptive states; Depth-Only baseline that only uses visual observations; State-\nDepth-Concat that uses both proprioceptive states and vision, but without our proposed Transformer.\nThe State-Depth-Concat baseline uses a linear projection to project visual features into a feature\nvector that has the same dimensions as the proprioceptive features. The State-Depth-Concat baseline\nthen concatenates both features and feeds it into the value and policy networks. We also introduce a\nHierarchical Reinforcement Learning (HRL)baseline as described in Jain et al. (2020), but without\nthe use of the trajectory generator for a fair comparison (We follow Jain et al. (2020) faithfully and\nour results indicate that it works as expected). We train all agents using PPO (Schulman et al., 2017)\nand share the same proprioceptive and visual encoder for the value and policy network.\nEvaluation Metric and Training Samples.We evaluate policies by their mean episode return, and\ntwo domain-speciﬁc evaluation metrics: (i) the distance (in meters) an agent moved along its target\ndirection; and (ii) the number of collisions with obstacles per episode (with length of 1k steps). The\ncollision is examined at every time step. , and we only compute the collision when the robot pass by\nat least one obstacle. We train all methods for 15M samples with 5 different random seeds and report\nthe mean and standard deviation of the ﬁnal policies.\n5.3 A TTENTION MAPS\nTo gain insight into how our Transformer model leverages spatial information and recognizes\ndominant visual regions for decision-making at different time steps, we visualize the attention map of\nour policy on the simulated environment in Figure 3. Speciﬁcally, we compute the attention weight\nWi,j between the proprioceptive token and all other visual tokens and visualize the attention weights\non the corresponding visual region of each token. In the top row, we observe that the agent pays most\nattention to nearby obstacles in the front, i.e. objects that the agent needs to avoid to move forward.\nThe attention also evolves when new obstacles appear or get closer. In the Mountain environment\n(bottom row), the agent attends alternatively to two different types of regions: the close terrain\nimmediately inﬂuencing the locomotion of the robot, and regions corresponding to the task-speciﬁc\ndirection towards the target. The robot ﬁrst attends to the terrain in front to step on the ground (1st &\n3rd frame), once the agent is in a relatively stable state, it attends to the goal far away to perform\nlonger-term planning (2nd & 4th frame). The regions attended by the agent are highly task-related\nand this indicates that our model learns to recognize important visual regions for decision-making.\n5.4 N AVIGATION ON FLAT TERRAIN WITH OBSTACLES\nStatic Obstacles without Spheres.We train all methods on navigation tasks with obstacles and ﬂat\nterrain to evaluate the effectiveness of modal fusion and stability of locomotion. Results are shown\nin Figure 4 (a). Our method, the HRL baseline, and the State-Depth-Concat baseline signiﬁcantly\noutperform the State-Only baseline in both the Thin Obstacle and Wide Obstacle environment,\ndemonstrating a clear beneﬁt of vision for locomotion in complex environments. Interestingly, when\n6\nPublished as a conference paper at ICLR 2022\n(a) (b) (c) (d)\nFigure 4: Training and evaluation curveson simulated environments (Concrete lines and shaded areas shows\nthe mean and the std over 5 seeds, respectively). For environment without sphere (in (a)), our method achieve\ncomparable training performance but much better evaluation performance on unseen environments (in (b)). For\nmore challenging environment (in (c) and (d)) our method achieve better performance and sample efﬁciency.\nTable 1: Generalization. We evaluate the generalization ability of all methods by evaluating on unseen\nenvironments. Our method signiﬁcantly outperform baselines on both metrics (longer distance & less collision).\nDistance Moved (m)↑ Collision Happened↓\nThin Obs.(Train\non Wide Obs.)\nWide Obs.(Train\non Thin Obs.)\nThin Obs.(Train\non Wide Obs.)\nWide Obs.(Train\non Thin Obs.)\nState-Only 3.6±1.3 5.9±0.9 456.3±262.2 545.1±57.7\nDepth-Only 1.1±1.1 0.1±0.0 - -\nState-Depth-Concat 5.6±2.1 7.1±2.0 406.8±89.5 331.1±192.8\nHRL 5.8 ±2.2 11.5±1.8 527.9±94.6 238.8±59.5\nOurs 8.2±2.5 14.2±2.8 310.4±131.3 82.2±103.8\nthe environment appearance is relatively simple (e.g., theWide Obstacleenvironment), the Depth-\nOnly baseline can learn a reasonable policy without using proprioceptive states. We surmise that the\nagent can infer part of the proprioceptive state from visual observations for policy learning. This\nphenomenon suggests that modeling the correlation between different modalities and better fusion\ntechniques are essential for a good policy. We also observe that the simpler State-Depth-Concat\nbaseline performs as well as our Transformer-based model in these environments. We conjecture that\nthis is because differentiating obstacles from ﬂat terrain is not a perceptually complex task, and a\nsimple concatenation, therefore proves sufﬁcient for policy learning.\nWe further evaluate the generalization ability of methods by transferring methods trained with thin\nobstacles to environments with wide obstacles, and vice versa. Figure 4 (b) shows generalization\nmeasured by episode return, and Table 1 shows average the quantitative evaluation results. While the\nState-Depth-Concat baseline is sufﬁcient for training, we ﬁnd that our Transformer-based method\nimproves episode return in transfer by as much as 69% and 56% in the Wide and Thin obstacle\nenvironments, respectively, over the State-Depth-Concat baseline. Compared with the HRL baseline,\nthe improvements of our method are 257.6% and 118.2%, respectively. We observe that our method\nmoves signiﬁcantly farther on average, and reduces the number of collisions by 290.5%, 402% and\n663% over the HRL baseline, the State-Depth-Concat and State-Only baselines when trained on\nthin obstacles and evaluated on wide obstacles. The Depth-Only baseline fails to generalize across\nenvironments and no collision occurs as the robot moves too little to even collide with obstacles.\nInterestingly, we observe that the generalization ability of the State-Depth-Concat decreases as\ntraining progresses, whereas for our method it either plateaus or increases over time. This indicates\nthat our method is more effective at capturing essential information in the visual and proprioceptive\ninformation during training, and is less prone to overﬁt to training environments.\nStatic Obstacles with Spheres.We now consider a perceptually more challenging setting with the\naddition of spheres in the environment; results are shown in Figure 4 (c). We observe that with\nadditional spheres, the sample efﬁciency of all methods decreases. While spheres with positive reward\nprovide the possibility for higher episode return, spheres increase complexity in two ways: (i) spheres\nmay lure agents into areas where it is prone to get stuck; and (ii) although spheres do not block the\n7\nPublished as a conference paper at ICLR 2022\nTable 2: Evaluation on environments with spheres.We evaluate the ﬁnal policy of all methods. Our method\nachieved the best performance on almost all environment for all metrics.\nDistance Moved (m)↑ Sphere Reward↑ Collision Happened↓\nThin Obs.\n& Sph.\nWide Obs.\n& Sph.\nThin Obs.\n& Sph.\nWide Obs.\n& Sph.\nThin Obs.\n& Sph.\nWide Obs.\n& Sph.\nState-Only 5.6±1.6 7.4±2.8 80.0±43.2 80.0±32.7 450.2±59.7 556.5±173.1\nDepth-Only 0.0±0.1 5.2±3.9 0.0±0.0 33.3±47.1 - -\nState-Depth-Concat 13.1±2.3 11.4±3.3 206.0±41.1 193.3±24.9 229.2±65.3 87.2±40.7\nHRL 10.8±0.8 11.3±2.9 166.7±54.4 288.9±154.8 256.8±87.4 423.3±170.0\nOurs 15.2±1.8 14.5±0.7 233.3±47.1 220.0±33.2 256.2±70.0 54.6±20.8\nagent physically, they may occlude the agent’s vision and can be visually difﬁcult to distinguish from\nobstacles in a depth map. We observe that with increased environmental complexity, our method\nconsistently outperforms both the HRL baseline and the State-Depth-Concat baseline in the ﬁnal\nperformance and sample efﬁciency. We report the average distance moved, number of collisions, and\nthe reward obtained from collecting spheres, in Table 2. Our method obtains a comparable sphere\nreward but a longer moved distance, which indicates that our LocoTransformer method is more\ncapable of modeling complex environments using spatial and cross-modal attention.\nTable 3: Evaluation resultson the Moving Obsta-\ncle Environment.\nMethod Distance\nMoved (m) ↑\nCollision\nHappened ↓\nState-Only 6.0±1.3 129.4±25.4\nDepth-Only 1.1±1.1 -\nState-Depth-Concat 16.3±1.7 88.4±34.0\nHRL 7.1±2.6 75.8±11.0\nOurs 11.3±2.9 67.9±18.1\nMoving Obstacles.When the positions of obstacles\nare ﬁxed within an episode, the agent may learn\nto only attend to the closest obstacle, instead of\nlearning to plan long-term. To evaluate the ability of\nlong-term planning, we conduct a comparison in an\nenvironment with moving obstacles to simulate real-\nworld scenarios with moving objects like navigating\nin the human crowd. The top row of Figure 4 (d)\nand Table 3 shows that the State-Only baseline and\nthe Depth-Only baseline both perform poorly , and\nthe HRL baseline performs worse than the State-Depth-Concat baseline. These results indicate that\nthe State-Only baseline lacks planning skills, which can be provided by visual observations, and\nthe hierarchical policy can not fuse the information from different modalities effectively when the\nenvironment is sufﬁciently complex. While the State-Depth-Concat baseline performs better in terms\nof distance, it collides more frequently than our method. This indicates that the baseline fails to\nrecognize the moving obstacles, while our method predicts the movement of obstacles and takes a\ndetour to avoid potential collisions. In this case, the conservative policy obtained by our method\nachieved better performance in terms of episode return though it did not move farther. We deduce\nthat with only a compact visual feature vector, it is very hard for the State-Depth-Concat baseline to\nkeep track of the movement of obstacles in the environment. On the other hand, it is easier to learn\nand predict the movement of multiple obstacles with our method since the Transformer provides an\nattention mechanism to model the visual region relations.\nTable 4: Ablation study on Thin Obs. & Sph.: We perform\nablations on Thin Obs. & Sph. environment and adopt the best\nsetting (N = 4, L= 2) for all environments, which includes 16\nvisual tokens and 2 Transformer encoder layers.\n(a) On Number of Visual Tokens\nMethod Episode Return ↑\nOurs (N=1) 1204.8±243.6\nOurs (N=2) 1418.1±167.8\nOurs (N=4) 1551.5±120.4\n(b) On Number of Layers\nMethod Episode Return ↑\nOurs (L=1) 1509.7±244.8\nOurs (L=2) 1551.5±120.4\nOurs (L=3) 1423.5±100.7\nAblations. We evaluate the impor-\ntance of two components of our Trans-\nformer model on the Thin Obs. &\nSph. environment: (1) the number\nof Transformer encoder layers; and\n(2) the number of visual tokens ( N2\nvisual tokens). Results are shown in\nTable 4. From Table 4b, we observe\nthat the performance of our model is\nrelatively insensitive to the number of\nTransformer encoder layers. For ablation on the number of visual tokens, we change the kernel size\nand the stride of the last convolutional layer in our visual encoder to get visual features with different\nshapes and different numbers of visual tokens. From Table 4a, we can see that the performance of our\nmethod is positively correlated with the number of the visual tokens. With a ﬁxed size of the visual\nfeature map, a higher number of tokens directly results in a smaller receptive ﬁeld for each visual\ntoken. Because our method performs spatial cross- modality attention across all tokens, our model\nbeneﬁts from richer low-level visual information. This indicates a potential for our model to work\nwith high-resolution visual input and in more complicated environments and complex tasks.\n8\nPublished as a conference paper at ICLR 2022\n5.5 N AVIGATION ON SIMULATED UNEVEN TERRAIN\nTable 5: Evaluation Result on the\nMountain environment.\nMethod 3D Distance\nMoved (m) ↑\nState-Only 3.7±1.6\nDepth-Only 3.0±0.5\nState-Depth-Concat 4.7 ±0.8\nHRL 6.3±0.3\nOurs 6.8 ±1.1\nWe also evaluate all methods on uneven, mountainous terrain.\nThe bottom row of Figure 4 (d) and Table 5 shows training\ncurves and the mean distance moved for each method, and\nour method improves over all baselines by a large margin in\nepisode return. Despite having access to depth images, the State-\nDepth-Concat baseline does not show any improvement over\nthe State-Only baseline in episode return. We conjecture that\nnaively projecting spatial-visual feature into a vector and fusing\nmulti-modality information with a simple concatenation can\neasily lose the spatial structure of visual information. Although\nthe HRL baseline moves farther among baselines, it does not obtain a higher episode return, indicating\nthe HRL baseline is not able to utilize the visual guidance towards the target. Our Transformer-based\nmethod better captures spatial information such as both global and local characteristics of the terrain\nand more successfully fuses spatial and proprioceptive information than a simple concatenation.\n5.6 R EAL -WORLD EXPERIMENTS\n(a) Indoor & Obs.\n (b) Forest\nFigure 5: Real World SamplesWe evaluate our\nmethod in real-world scenarios with different ob-\nstacles on complex terrain.\nFigure 6: Experiment results in the real-world:\nWe perform real-world experiment on Indoor &\nObs. and Forest environments.\nMethod Distance\nMoved (m) ↑\nCollision\nTimes ↓\nState-Depth-Concat 5.0±2.6 0.4±0.5\nOurs 9.6±2.2 0.3±0.5\n(a) Indoor & Obs.\nMethod Distance\nMoved (m) ↑\nCollision\nCount ↓\nState-Depth-Concat 5.1±0.9 0.3±0.5\nOurs 9.6±2.0 0.0±0.0\n(b) Forest\nTo validate our method in different real-world scenes\nbeyond the simulation, we conduct real-world ex-\nperiments in both indoor scenarios with obstacles\n(referred as Indoor & Obs.) and in-the-wild forest\nwith complex terrain and trees (referred to as For-\nest)) as shown in Figure 5. As the HRL baseline\nis found to not generalize well to unseen environ-\nments as shown in Figure 4 (b) and Table 1, we only\ndeploy policies learned in simulation using our Lo-\ncoTransformer and the State-Depth-Concat baseline\non a Unitree A1 Robot (Unitree, 2018). The poli-\ncies are trained with the Thin Obstacle environment\nrandomized with uneven terrains. All the real-world\ndeployment experiments are repeated 15 times across\ndifferent seeds. Details about robot setup are pro-\nvided in Appendix A. Since it is challenging to mea-\nsure the exact duration of collision with obstacles in\nthe real world, we instead report the number of times\nthat robot collides with obstacles (Collision Count)\nas a measure of performance.\nAs shown in Table 6, our method outperforms the\nbaseline by a large margin in both scenarios. In the\nIndoor & Obs environment, our method moves 92%\nfarther than the baseline and collides less. When facing complex terrain and unseen obstacles in the\nForest environment, our method greatly improves over the baseline; our policy moved approximately\n90% farther without colliding into any obstacles, while the baseline frequently collides into trees and\ngets stuck in potholes. We generally observe that our method is more robust than the baseline when\ndeployed in the real world, indicating that our method better captures the object structure from visual\nobservations, rather than overﬁtting the appearance of objects during training.\n6 C ONCLUSION\nWe propose to incorporate the proprioceptive and visual information with the proposed LocoTrans-\nformer model for locomotion control. By borrowing the visual inputs, we show that the robot can\nplan to walk through different sizes of obstacles and even moving obstacles. The visual inputs also\nhelps the locomotion in challenging terrain such as mountain. Beyond the training environment,\nwe also show that our method with the cross-modality Transformer achieves better generalization\nresults when testing on unseen environments and in the real world. This shows our Transformer\nmodel provides an effective fusion mechanism between proprioceptive and visual information and\nnew possibilities on reinforcement learning with information from multi-modality.\n9\nPublished as a conference paper at ICLR 2022\n7 R EPRODUCIBILITY STATEMENT\nTo ensure the reproducibility of our work, we provide the following illustrations in our paper and\nappendix:\n• Environment: We provide the detailed description of the environment in Section 5.1, as\nwell as the speciﬁc observation space, action space and reward function in Appendix A.2.\n• Implementation Details: We provide all implementation details and related hyperparame-\nters for both our methods and baselines in Section 4.2 and Appendix B.\n• Real Robot Setup: We provide all relavant details about setting up the real robot and\nconduct real-world experiment in Appendix A.3.\nWe have released the code, environment and videos on our project page: https://rchalyang.\ngithub.io/LocoTransformer/. We believe the open source of our code and environment\nwill be an important contribution to the community.\nAcknowledgement: This work was supported, in part, by gifts from Meta, Qualcomm, and TuSim-\nple.\nREFERENCES\nHassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing\nGong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text,\n2021. 3\nP. Arena, L. Fortuna, M. Frasca, L. Patané, and M. Pavone. Realization of a cnn-driven cockroach-\ninspired robot. 2006 IEEE International Symposium on Circuits and Systems, pp. 4 pp.–, 2006.\n1\nRichard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, 6(5):\n679–684, 1957. 3\nGerardo Bledt and Sangbae Kim. Extracting legged locomotion heuristics with regularized predictive\ncontrol. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 406–412.\nIEEE, 2020. 3\nGerardo Bledt, Matthew J Powell, Benjamin Katz, Jared Di Carlo, Patrick M Wensing, and Sangbae\nKim. Mit cheetah 3: Design and control of a robust, dynamic quadruped robot. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pp. 2245–2252. IEEE, 2018. 3\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020. 3\nR. Calandra, Andrew Owens, Dinesh Jayaraman, Justin Lin, Wenzhen Yuan, Jitendra Malik, E. Adel-\nson, and Sergey Levine. More than a feeling: Learning to grasp and regrasp using vision and touch.\nIEEE Robotics and Automation Letters, 3:3300–3307, 2018. 3\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer\nVision, pp. 213–229. Springer, 2020. 3\nJan Carius, René Ranftl, Vladlen Koltun, and Marco Hutter. Trajectory optimization for legged\nrobots with slipping motions. IEEE Robotics and Automation Letters, 4(3):3013–3020, 2019. doi:\n10.1109/LRA.2019.2923967. 3\nJared Di Carlo, Patrick M. Wensing, Benjamin Katz, Gerardo Bledt, and Sangbae Kim. Dynamic\nlocomotion in the MIT cheetah 3 through convex model-predictive control. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems, IROS 2018, Madrid, Spain, October\n1-5, 2018, pp. 1–9. IEEE, 2018. doi: 10.1109/IROS.2018.8594448. URL https://doi.org/\n10.1109/IROS.2018.8594448. 3, 19, 20\n10\nPublished as a conference paper at ICLR 2022\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International Conference on Machine Learning, pp. 1691–\n1703. PMLR, 2020a. 3\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Universal image-text representation learning. In European Conference on\nComputer Vision, pp. 104–120. Springer, 2020b. 3\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019. 3\nXingye Da, Zhaoming Xie, David Hoeller, Byron Boots, Animashree Anandkumar, Yuke Zhu, Buck\nBabich, and Animesh Garg. Learning a contact-adaptive controller for robust, efﬁcient legged\nlocomotion. ArXiv, abs/2009.10019, 2020. 2\nF. Delcomyn and M. Nelson. Architectures for a biomimetic hexapod robot. Robotics Auton. Syst.,\n30:5–15, 2000. 1\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 3\nJared Di Carlo, Patrick M Wensing, Benjamin Katz, Gerardo Bledt, and Sangbae Kim. Dynamic\nlocomotion in the mit cheetah 3 through convex model-predictive control. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pp. 1–9. IEEE, 2018. 3\nYanran Ding, Abhishek Pandala, and Hae-Won Park. Real-time model predictive control for versatile\ndynamic motions in quadrupedal robots. In 2019 International Conference on Robotics and\nAutomation (ICRA), pp. 8484–8490. IEEE, 2019. 3\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 3\nAlejandro Escontrela, George Yu, Peng Xu, Atil Iscen, and Jie Tan. Zero-shot terrain generalization\nfor visual locomotion policies, 2020. 2, 3\nChristian Gehring, Stelian Coros, Marco Hutter, Michael Blösch, Mark A. Hoepﬂinger, and Roland\nSiegwart. Control of dynamic gaits for a quadrupedal robot. In2013 IEEE International Conference\non Robotics and Automation, Karlsruhe, Germany, May 6-10, 2013, pp. 3287–3292. IEEE,\n2013. doi: 10.1109/ICRA.2013.6631035. URL https://doi.org/10.1109/ICRA.2013.\n6631035. 3\nHartmut Geyer, Andre Seyfarth, and Reinhard Blickhan. Positive force feedback in bouncing gaits?\nProceedings of the Royal Society of London. Series B: Biological Sciences, 270(1529):2173–2183,\n2003. 2\nRuben Grandia, Farbod Farshidian, Alexey Dosovitskiy, René Ranftl, and Marco Hutter. Frequency-\naware model predictive control. IEEE Robotics and Automation Letters, 4(2):1517–1524, 2019.\n3\nSaurabh Gupta, Varun Tolani, James Davidson, Sergey Levine, R. Sukthankar, and J. Malik. Cognitive\nmapping and planning for visual navigation. International Journal of Computer Vision, 128:1311–\n1330, 2019. 1\nNicklas Hansen and Xiaolong Wang. Generalization in reinforcement learning by soft data augmen-\ntation. In International Conference on Robotics and Automation, 2021. 3\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for\nunsupervised visual representation learning. 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 9726–9735, 2020. 3\n11\nPublished as a conference paper at ICLR 2022\nNicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa,\nTom Erez, Ziyu Wang, S. M. Ali Eslami, Martin A. Riedmiller, and David Silver. Emergence\nof locomotion behaviours in rich environments. CoRR, abs/1707.02286, 2017. URL http:\n//arxiv.org/abs/1707.02286. 3\nLisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, and Aida Nematzadeh.\nDecoupling the role of data, attention, and losses in multimodal transformers. arXiv preprint\narXiv:2102.00529, 2021. 3\nRonghang Hu and Amanpreet Singh. Unit: Multimodal multitask learning with a uniﬁed transformer,\n2021. 3\nZhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. Seeing out\nof the box: End-to-end pre-training for vision-language representation learning, 2021. 3\nJemin Hwangbo, J. Lee, A. Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, V . Koltun, and M. Hutter.\nLearning agile and dynamic motor skills for legged robots. Science Robotics, 4, 2019. 3\nAtil Iscen, Ken Caluwaerts, Jie Tan, Tingnan Zhang, Erwin Coumans, Vikas Sindhwani, and\nVincent Vanhoucke. Policies modulating trajectory generators. In 2nd Annual Conference\non Robot Learning, CoRL 2018, Zürich, Switzerland, 29-31 October 2018, Proceedings, vol-\nume 87 of Proceedings of Machine Learning Research, pp. 916–926. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v87/iscen18a.html. 3\nMax Jaderberg, V olodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David\nSilver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In 5th\nInternational Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,\n2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.\nnet/forum?id=SJ6yPD5xg. 3\nDeepali Jain, Atil Iscen, and Ken Caluwaerts. Hierarchical reinforcement learning for quadruped\nlocomotion. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems,\nIROS 2019, Macau, SAR, China, November 3-8, 2019, pp. 7551–7557. IEEE, 2019a. doi: 10.\n1109/IROS40897.2019.8967913. URL https://doi.org/10.1109/IROS40897.2019.\n8967913. 2, 3\nDeepali Jain, Atil Iscen, and Ken Caluwaerts. From pixels to legs: Hierarchical learning of quadruped\nlocomotion, 2020. 3, 6\nDivye Jain, Andrew Li, Shivam Singhal, Aravind Rajeswaran, Vikash Kumar, and Emanuel Todorov.\nLearning deep visuomotor policies for dexterous hand manipulation. In 2019 International\nConference on Robotics and Automation (ICRA), pp. 3636–3643, 2019b. doi: 10.1109/ICRA.2019.\n8794033. 1, 3\nYiding Jiang, Shixiang Gu, Kevin Murphy, and Chelsea Finn. Language as an abstraction for\nhierarchical deep reinforcement learning. In Hanna M. Wallach, Hugo Larochelle, Alina\nBeygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in\nNeural Information Processing Systems 32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.\n9414–9426, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/\n0af787945872196b42c9f73ead2565c8-Abstract.html. 2\nG. Kahn, P. Abbeel, and Sergey Levine. Badgr: An autonomous self-supervised learning-based\nnavigation system. IEEE Robotics and Automation Letters, 6:1312–1319, 2021. 1\nNate Kohl and Peter Stone. Policy gradient reinforcement learning for fast quadrupedal locomotion.\nIn Proceedings of the 2004 IEEE International Conference on Robotics and Automation, ICRA\n2004, April 26 - May 1, 2004, New Orleans, LA, USA, pp. 2619–2624. IEEE, 2004. doi: 10.1109/\nROBOT.2004.1307456. URL https://doi.org/10.1109/ROBOT.2004.1307456. 3\nAshish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for\nlegged robot. Robotics: Science and Systems, 2021. 3\n12\nPublished as a conference paper at ICLR 2022\nMichael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Rein-\nforcement learning with augmented data. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,\nMaria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/\n2020/hash/e615c82aba461681ade82da2da38004a-Abstract.html. 3\nJoonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning\nquadrupedal locomotion over challenging terrain. Science robotics, 5(47), 2020a. 1, 3\nMichelle A Lee, Yuke Zhu, Peter Zachares, Matthew Tan, Krishnan Srinivasan, Silvio Savarese,\nLi Fei-Fei, Animesh Garg, and Jeannette Bohg. Making sense of vision and touch: Learning\nmultimodal representations for contact-rich tasks. IEEE Transactions on Robotics, 36(3):582–596,\n2020b. 3\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep\nvisuomotor policies. J. Mach. Learn. Res., 17:39:1–39:40, 2016. URL http://jmlr.org/\npapers/v17/15-522.html. 1, 3\nSergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-\neye coordination for robotic grasping with deep learning and large-scale data collection. Int.\nJ. Robotics Res., 37(4-5):421–436, 2018. doi: 10.1177/0278364917710318. URL https:\n//doi.org/10.1177/0278364917710318. 1, 3\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple\nand performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. 3\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision, pp. 121–137. Springer, 2020. 3\nXingyu Lin, Harjatin Singh Baweja, George Kantor, and David Held. Adaptive auxiliary task\nweighting for reinforcement learning. Advances in neural information processing systems, 32,\n2019. 3\nY . Luo, Jonathan Hans Soeseno, T. Chen, and Wei-Chao Chen. Carl: Controllable agent with\nreinforcement learning for quadruped locomotion. ArXiv, abs/2005.03288, 2020. 3\nJonathan Samir Matthis, Jacob L Yates, and Mary M Hayhoe. Gaze and the control of foot placement\nwhen walking in natural terrain. Current Biology, 28(8):1224–1233, 2018. 1\nJosh Merel, Saran Tunyasuvunakool, Arun Ahuja, Yuval Tassa, Leonard Hasenclever, Vu Pham, Tom\nErez, Greg Wayne, and Nicolas Heess. Catch & carry: reusable neural controllers for vision-guided\nwhole-body tasks. ACM Trans. Graph., 39(4):39, 2020. doi: 10.1145/3386569.3392474. URL\nhttps://doi.org/10.1145/3386569.3392474. 3\nP. Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha\nDenil, R. Goroshin, L. Sifre, K. Kavukcuoglu, D. Kumaran, and R. Hadsell. Learning to navigate\nin complex environments. ArXiv, abs/1611.03673, 2017. 1\nHirofumi Miura and Isao Shimoyama. Dynamic walk of a biped. The International Journal of\nRobotics Research, 3(2):60–74, 1984. 2\nV olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-\ntersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,\nDaan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-\nforcement learning. Nature, 518(7540):529–533, February 2015a. ISSN 00280836. URL\nhttp://dx.doi.org/10.1038/nature14236. 3\n13\nPublished as a conference paper at ICLR 2022\nV olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-\ntersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,\nDaan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-\nforcement learning. Nature, 518(7540):529–533, February 2015b. ISSN 00280836. URL\nhttp://dx.doi.org/10.1038/nature14236. 5\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. InInternational Conference on Machine Learning, pp. 4055–4064.\nPMLR, 2018. 3\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by\nself-supervised prediction. In ICML, 2017. 3\nX. Peng, G. Berseth, KangKang Yin, and M. V . D. Panne. Deeploco: dynamic locomotion skills\nusing hierarchical deep reinforcement learning. ACM Trans. Graph., 36:41:1–41:13, 2017. 2\nXue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-\nguided deep reinforcement learning of physics-based character skills. ACM Trans. Graph., 37\n(4):143:1–143:14, July 2018. ISSN 0730-0301. doi: 10.1145/3197517.3201311. URL http:\n//doi.acm.org/10.1145/3197517.3201311. 3\nXue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Edward Lee, Jie Tan, and Sergey Levine.\nLearning agile robotic locomotion skills by imitating animals. In Robotics: Science and Systems,\n07 2020. doi: 10.15607/RSS.2020.XVI.064. 3, 19\nAditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for end-to-end\nautonomous driving. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3\nMarc H Raibert. Hopping in legged systems—modeling and simulation for the two-dimensional\none-legged case. IEEE Transactions on Systems, Man, and Cybernetics, SMC-14(3):451–463,\n1984. 2\nAlexander Sax, Bradley Emi, Amir R. Zamir, Leonidas J. Guibas, Silvio Savarese, and Jitendra\nMalik. Mid-level visual representations improve generalization and sample efﬁciency for learning\nvisuomotor policies. 2018. 3\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 6\nMax Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bach-\nman. Data-efﬁcient reinforcement learning with self-predictive representations. arXiv preprint\narXiv:2007.05929, 2020. 3\nAravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representations\nfor reinforcement learning. arXiv preprint arXiv:2004.04136, 2020. 3\nAdam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning\nfrom reinforcement learning. CoRR, abs/2009.08319, 2020. URL https://arxiv.org/\nabs/2009.08319. 3\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training\nof generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019. 3\nChen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A\njoint model for video and language representation learning. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 7464–7473, 2019. 3\nYu Sun, Wyatt L. Ubellacker, Wen-Loong Ma, Xiang Zhang, Changhao Wang, Noel V . Csomay-\nShanklin, Masayoshi Tomizuka, Koushil Sreenath, and Aaron D. Ames. Online learning of\nunknown dynamics for model-based controllers in legged locomotion. IEEE Robotics and Automa-\ntion Letters (RA-L), 2021. 3, 20\n14\nPublished as a conference paper at ICLR 2022\nHao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from trans-\nformers. arXiv preprint arXiv:1908.07490, 2019. 3\nJie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and\nVincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. In Hadas Kress-\nGazit, Siddhartha S. Srinivasa, Tom Howard, and Nikolay Atanasov (eds.), Robotics: Science\nand Systems XIV , Carnegie Mellon University, Pittsburgh, Pennsylvania, USA, June 26-30, 2018,\n2018. doi: 10.15607/RSS.2018.XIV .010. URL http://www.roboticsproceedings.\norg/rss14/p10.html. 3\nStephen Tian, Frederik Ebert, Dinesh Jayaraman, Mayur Mudigonda, Chelsea Finn, Roberto Calandra,\nand Sergey Levine. Manipulation by feel: Touch-based control with deep predictive models. In\n2019 International Conference on Robotics and Automation (ICRA), pp. 818–824. IEEE, 2019. 1\nNick Torkos and Michiel van de Panne. Footprint–based quadruped motion synthesis. InProceedings\nof the Graphics Interface 1998 Conference, June 18-20, 1998, Vancouver, BC, Canada, pp. 151–\n160, June 1998. URL http://graphicsinterface.org/wp-content/uploads/\ngi1998-19.pdf. 2\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Z. Kolter, Louis-Philippe Morency, and\nR. Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. Pro-\nceedings of the conference. Association for Computational Linguistics. Meeting, 2019:6558–6569,\n2019. 2\nUnitree. A1: More dexterity, more posibility, 2018. URL https://www.unitree.com/\nproducts/a1/. 9, 16\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need, 2017. 2, 3, 4\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794–7803,\n2018. 3\nZhaoming Xie, Xingye Da, Michiel van de Panne, Buck Babich, and Animesh Garg. Dynamics\nrandomization revisited: A case study for quadrupedal locomotion. CoRR, abs/2011.02404, 2020.\nURL https://arxiv.org/abs/2011.02404. 1, 3\nZhaoming Xie, Xingye Da, Buck Babich, Animesh Garg, and Michiel van de Panne. Glide: Gen-\neralizable quadrupedal locomotion in diverse environments with a centroidal model. CoRR,\nabs/2104.09771, 2021. URL https://arxiv.org/abs/2104.09771. 1, 3\nWei Yang, X. Wang, Ali Farhadi, A. Gupta, and R. Mottaghi. Visual semantic navigation using scene\npriors. ArXiv, abs/1810.06543, 2019. 1\nYuxiang Yang, Tingnan Zhang, Erwin Coumans, Jie Tan, and Byron Boots. Fast and efﬁcient\nlocomotion via learned gait transitions. arXiv preprint arXiv:2104.04644, 2021. 20\nDenis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving\nsample efﬁciency in model-free reinforcement learning from images, 2019. 3\nKangKang Yin, Kevin Loken, and Michiel Van de Panne. Simbicon: Simple biped locomotion\ncontrol. ACM Transactions on Graphics (TOG), 26(3):105–es, 2007. 3\nWenhao Yu, Greg Turk, and C. Karen Liu. Learning symmetric and low-energy locomotion.ACM\nTrans. Graph., 37(4):144:1–144:12, 2018. doi: 10.1145/3197517.3201397. URL https://doi.\norg/10.1145/3197517.3201397. 16\nH. Zhang, S. Starke, T. Komura, and Jun Saito. Mode-adaptive neural networks for quadruped motion\ncontrol. ACM Transactions on Graphics (TOG), 37:1 – 11, 2018. 3\n15\nPublished as a conference paper at ICLR 2022\nA D ETAILED EXPERIMENT SETUP\nA.1 D ETAILS ON PROPRIOCEPTION AND ACTION\nOur Unitree A1 robot has 12 Degrees of Freedom (DoF), and we use position control to set actions\nfor the robot. Speciﬁcally, the proprioceptive input contains the following components:\n• Joint angle: a 12-dimensional vector records the angle of each joint.\n• IMU information: a 4-dimensional vector records orientations and angular velocities.\n• Base displacement: a 3-dimensional vector records the absolute base position of robot.\n• Last action: a 12-dimensional vector records the angle change in the last step.\nThe full proprioceptive vector consists of all these vectors over the last three steps to retain historical\nstate information. The action is also a 12-dimensional vector that controls the change of all the joint\nangles. We use 0.5 as the upper bound of action for locomotion stability. We use all default settings\nof A1 robot in the ofﬁcial repository.\nA.2 R EWARD DEFINITION\nIn all our experiments, we use the same reward function as follow:\nR= αforwardRforward + αenergyRenergy + αaliveRalive + K·Rsphere, (7)\nwhere we set αforward = 1,αenergy = 0.005,αalive = 0.1 for all tasks.\nRforward stands for moving forward reward. In ﬂat environments, it’s deﬁned by the moving speed\nof robot along the x-axis; in mountain environment, it’s deﬁned by that along the direction to the\nmountain top (red sphere in Figure 1 Mountain in paper).\nRenergy ensures the robot is using minimal energy, which has been shown to improve the naturalness\nof motion, similar to Yu et al. (2018). Speciﬁcally, we penalize the actions resulting motor torques\nwith large euclidean norm.:\nRenergy = −∥τ∥2, τ is the motor torques.\nRalive encourages the agent to live longer. It gives a positive reward of 1.0 at each time step until\ntermination. Dangerous behaviors like falling down and crashing into obstacles will call termination.\nRsphere stands for sphere collection reward (whenever applicable) for each sphere collected, and Kis\nthe number of spheres collected at the current time step.\nA.3 R EAL ROBOT SETUP\nWe use the Unitree A1 Robot (Unitree, 2018), which has 18 links and 12 degrees of freedom (3 for\neach leg). We mount an Intel RealSense camera at the head of the robot to capture the depth map, and\nuse the robot sensors to get the joint states and IMU for the proprioceptive input. All computations\nare running with on-board resources. We set the control frequency to be 25 Hz and set the action\nrepeat to be 16, so that the PD controller converts the target position commands to motor torques at\n400 Hz. We set the KP and KD of PD controller to be 40 and 0.6 respectively. The base displacement\nis removed from the observation for the real-world experiment. For the real-world experiment, we\nexecute the policy on the real-robot for 20 seconds and measure the Euclidean distance between the\nstart and end point of the trajectory for evaluating the performance.\nB H YPERPARAMETERS\nIn this section, we detail the hyperparameters for each method used in our experiments.\n16\nPublished as a conference paper at ICLR 2022\nParameters Range\nKP [40, 90]\nKD [0.4, 0.8]\nInertia (×default value) [0.5, 1.5]\nLateral Friction (Ns / m) [0.5, 1.25]\nMass (×default value) [0.8, 1.2]\nMotor Friction (Nms / rad) [0.0, 0.05]\nMotor Strength (×default value) [0.8, 1.2]\nSensor Latency (s) [0, 0.04]\nTable 6: Variation of Environment and Robot Parameters.\nB.1 D OMAIN RANDOMIZATION\nTo narrow the reality gap, we leverage domain randomization during training phase. All methods\nconducted in real world experiments use a same group of randomization setting to be fair. Speciﬁcally,\nwe set the range of parameters as follow:\nDuring training, besides the domain randomization for the proprioceptive state, we perform domain\nrandomization for visual input. Speciﬁcally, at each time-step, we randomly choose 3 to 30 values in\n(64,64) depth input and set the depth reading to the maximum reading. In this case, we simulate the\nnoisy visual observation in the real-world.\nB.2 H YPERPARAMETERS SHARED BY ALL METHODS\nHere we give the details of all hyperparameters that are related to reinforcement learning and shared\nby all tested methods. “Horizon” denotes the episode length in both training and testing, and “Clip\nparameter” denotes the max norm of gradients in all trained networks.\nHyperparameter Value\nHorizon 1000\nNon-linearity ReLU\nPolicy initialization Standard Gaussian\n# of samples per iteration 8192\nDiscount factor .99\nBatch size 256\nOptimization epochs 3\nClip parameter 0.2\nPolicy network learning rate 1e-4\nValue network learning rate 1e-4\nOptimizer Adam\nB.3 S TATE-ONLY BASELINE\nTo keep the comparison fair, we use 4 fully-connected layers for state-only baseline to keep the\nnetwork size large enough for higher learning capacity. We also try more layers but observe minor\ndifference in performance.\nHyperparameter Value\nNetwork size 4 FC layers with 256 units\nB.4 S TATE-DEPTH -CONCAT BASELINE\nApart from the perception encoder, we keep all other parts same for State-Depth-Concat baseline and\nour LocoTransformer.\nHyperparameter Value\nProprioceptive encoder 2 FC layers with 256 units\nProjection Head 2 FC layers with 256 units\n17\nPublished as a conference paper at ICLR 2022\nB.5 O URS - LOCO TRANSFORMER\nHyperparameter Value\nToken dimension 128\nProprioceptive encoder 2 FC layers with 256 units\nProjection Head 2 FC layers with 256 units\n# of transoformer encoder layers 2\nC M ORE ATTENTION VISUALIZATION RESULTS\nWe offer more visualization of attention maps here to show that LocoTransformer consistently attends\nto reasonable parts of environment during an episode. In the Thin Obstacle, robot will be aware of\nthe new appearing obstacles and give emergent reaction to escape the threat. For example, in the\nlast row, the robot attends to the closed obstacle and then turns left. Suddenly it attends to the wall,\nso it reorientates its body to avoid risky. In the Mountain, robot not only pays attention to the ﬁnal\ngoal position, but also attend to rugged terrains that are extremely hard to step on. This also shows\nthe planning ability according to different visual regions learned by Transformer architecture. In the\nsecond case, the robot ﬁrst attends to goal position to ensure the forward direction, but the uneven\nrocks also draw its attention.\nFigure 7: Additional Attention VisualizationWe visualize more attention map visualization for better\nunderstanding of how our LocoTransformer works. Each row shows a sequence of attention map to present how\nthe attention of agent evolves.\nD C OMPARISON WITH MODEL PREDICTIVE CONTROL\nTo understand the advantages of learning-based locomotion, we offer a detailed comparison with the\nclassical quadrupedal robotic control pipeline, which is commonly used in both classicalal approaches\nand recent visual locomotion learning works. With this comparison, we can also help the community\nunderstand the main difference between learning-based academic works and industrial solutions, like\nthe famous Boston Dynamics Spot.\n18\nPublished as a conference paper at ICLR 2022\nD.1 V ISION -GUIDED WHOLE -BODY CONTROLLER\nWe mainly follow Carlo et al. (2018) and the code for (Peng et al., 2020) to reproduce the vision-\nguided whole-body controller for A1. Our high-level vision controller is\ntrained with RL perceiviing the visual information same as other approaches and outputing the target\nlinear and angular velocity for low-level controller. Our low-level motion controller provide the\nactual motor commands to the robot according to current target velocity.\nHigh-level Visual Policy. High-level visual policy outputs the target linear velocity and angular\nvelocity given the stacked depth maps and (optionally) CoM velocity and IMU information. If\nonly given the depth maps, we remove the state encoder part in the original models and keep the\nTransformer and CNN encoders; if given both the depth maps and the body state (CoM velocity\nand IMU information), we just keep the two architectures same as the main paper. We train the\nhigh-level controller with PPO to provide fair and consistent comparison with our method. The\ncontrol frequency of the high-level controller is 20Hz, with the action repeat set as 10 to guide the\nlow-level controller. We use the Unicycle Model to control the CoM velocity, i.e., specifying the\nabsolute linear velocity and angular (rotating) velocity. In our experiments, the target linear velocity\nis clipped to ±0.4 and the target angular velocity is clipped to ±0.3.\nLow-level Controller. The low-level controller uses position control for swing actions and torque\ncontrol for stance actions. Speciﬁcally, we use a ﬁnite state machine (FSM) based gait scheduler to\ndecide when to swing each leg and how long stance each leg needs in a complete control cycle. The\nswing action is determined by a ﬁxed foot clearance height and controlled by a PD-controller with the\ntarget foot position. The stance force (torques of each joint in a leg) is computed by model predictive\ncontrol to track the desired CoM velocity. The whole-body controller outputs a new command in\n200HZ and use another action repeat 5 to control the body.\nTraining. We use PPO to train the control policy, similar to other methods in this paper. Here we\nonly demonstrate some main modiﬁcations that might inﬂuence reproducing the experiments: 1) we\nremove the energy reward term, since the low-level motion control is not learnable; 2) Because of the\nchange of the control frequency (all other methods in our paper directly provide low-level command\nwhich requires a higher control frequency), we tune the following hyperparameters:\nHyperparameter Value\nHorizon 500\n# of samples per iteration 4096\nαforward 0.5\nWe remove domain randomization terms which are only related to motion robustness.\nD.2 R ESULTS IN SIMULATION\nWe perform the training in simulation and answer the following two questions:\n• Can our Transformer-based architecture still outperform CNN-based one while combining\nwith classical controller?\n• Is our RL-based framework better than visual policy + classical controller?\nOur results show the advantage of our network design and that of our End-to-end RL pipeline.\nComparison between Network Architectures.We train the vision-guided whole body controller\nin two settings. The two settings are: (i) Stacked depth maps; (ii) Stacked depth maps with CoM\nvelocity and IMU sensor input. The training curves are in Figure 9. Due to the difference of the\nreward settings and episode length, it’s meaningless and unfair to compare the curves with that of\nRL-based methods. The training results show that for multi-modal setting, our LocoTransformer still\noutperforms the baseline. For vision-only setting, we observe minor difference between transformer\nand CNN, and speculate that attention mechanism indeed improves the modal fusion rather than\nimage representation learning.\n19\nPublished as a conference paper at ICLR 2022\nNote that it costs several times of samples to train the controller. We deduce the phenomenon is\ndue to low-level controller is not agile enough to the varying high-level command to gain explicit\ninformation for RL optimization.\nFor customized traditional controllers, such as the MPC controller used by MIT cheetah 3 (Carlo et al.,\n2018), the inference speed is at around 120Hz on the Unitree A1 robot. To overcome the limitation\nof computation, an external laptop is used in (Sun et al., 2021; Yang et al., 2021). In contrast, our\nend-2-end learned policy is able to utilize the on-board GPU to perform real-time inference.\n0 10 20 30 40 50\nMillion Controller Steps\n50\n100\n150\n200\n250\n300\n350\n400Episode Return\nMulti Modal\n0 10 20 30 40 50\nMillion Controller Steps\n50\n100\n150\n200\n250\n300 Vision Only\nBaseline Locotransformer\nFigure 8: Training curves of vision-guided whole body controller.For multi-modal input setting, Loco-\nTransformer still outperforms the baseline.\nD.3 R ESULTS IN REAL WORLD\nTo further illustrate the advantage of end-to-end RL for locomotion, we deploy the vision-guided\nwhole-body controller using only visual observation and baseline architecture in the real world. The\nvision-guided whole-body controller controller generates natural and stable gaits, including larger\namplitude of feet swing, lower frequency of stepping, and consistent body height. However, when\nfacing obstacles like trees, the vision-guided whole-body controller cannot plan well with the MPC\ncontroller for avoidance. It tries to avoid the tree, but since the motion is not very ﬂexible and\nthe high-level transitions are not smooth, it collides with the side of the tree, and bounces away to\ncome across the obstacle. While the agent is still moving forward, having more collisions makes the\nrobot unsafe. On the other hand, when trained end-to-end with both vision and legged motion, our\napproach can ﬂexibly change from a moving forward motion to a turning motion and adjust the speed\nat the same time. We generally observe much more diverse motions emerge and our policy is able\nto transition smoothly between these motions according to vision. For in-door environments where\nthe obstacles are larger, there are less chances for the vision-guided whole-body controller to come\nacross the obstacle when the robot is about to collide into it.\nFigure 9: Some failure cases of vision-guided whole-body controller due to limitation of agility.The left\nﬁgure shows that when walking through a narrow path, the robot can not quickly turn around and collide into the\nwall. The right ﬁgure denotes that it may collide to the tree in the wild, due to the shape of the obstacles are out\nof the training distribution and the robot can’t adjust quickly enough to avoid.\n20\nPublished as a conference paper at ICLR 2022\nD.4 Q UANTITATIVE COMPARISON IN REAL WORLD\nWe also provide quantitative comparisons between our method and the vision-guided whole-body\ncontroller. The vision-guided whole-body controller create more collisions even though this helps\nwalking longer distances.\nFigure 10: Experiment results in the real-world: We perform real-world experiment on Indoor & Obs. and\nForest environments to compare our method and the vision-guided whole-body controller\n(a) Indoor & Obs.\nMethod Distance\nMoved (m) ↑\nCollision\nTimes ↓\nVision-Guided Whole-Body Controller 7.5±1.5 3.4±0.8\nOurs 9.6±2.2 0.3±0.5\n(b) Forest\nMethod Distance\nMoved (m) ↑\nCollision\nCount ↓\nVision-Guided Whole-Body Controller 11.6±3.5 1.9±0.8\nOurs 9.6±2.0 0.0±0.0\nFor in-door environments where the obstacles are larger, there is less chance for the vision-guided\nwhole-body controller to avoid the obstacle when the robot is about to collide into it. Thus the\nvision-guided whole-body controller performs worse in both collision times and distance moved\ncompared to our approach.\n21"
}