{
  "title": "Frozen CLIP Transformer Is an Efficient Point Cloud Encoder",
  "url": "https://openalex.org/W4393149778",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2242310582",
      "name": "Xiaoshui Huang",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2105853307",
      "name": "Zhou Huang",
      "affiliations": [
        "Jiangxi University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A1978672188",
      "name": "Sheng Li",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2061456863",
      "name": "Wentao Qu",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107178496",
      "name": "Tong He",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2899966645",
      "name": "Yuenan Hou",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2137349215",
      "name": "Yifan Zuo",
      "affiliations": [
        "Jiangxi University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A2138640236",
      "name": "Wanli Ouyang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6718544293",
    "https://openalex.org/W2965803762",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W6687484953",
    "https://openalex.org/W3159344844",
    "https://openalex.org/W2939201152",
    "https://openalex.org/W2594519801",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6793697131",
    "https://openalex.org/W4302013134",
    "https://openalex.org/W3211559226",
    "https://openalex.org/W6845288475",
    "https://openalex.org/W4308067897",
    "https://openalex.org/W6795475546",
    "https://openalex.org/W6633640470",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W2556802233",
    "https://openalex.org/W2938428612",
    "https://openalex.org/W6639657675",
    "https://openalex.org/W6810213675",
    "https://openalex.org/W3156629209",
    "https://openalex.org/W6747904511",
    "https://openalex.org/W4385374062",
    "https://openalex.org/W6640300118",
    "https://openalex.org/W3045125647",
    "https://openalex.org/W4287123058",
    "https://openalex.org/W3137210930",
    "https://openalex.org/W4285069859",
    "https://openalex.org/W3217247671",
    "https://openalex.org/W6810884384",
    "https://openalex.org/W6788305448",
    "https://openalex.org/W4362679702",
    "https://openalex.org/W3202611145",
    "https://openalex.org/W2460657278",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2953937638",
    "https://openalex.org/W3047223011",
    "https://openalex.org/W2961193895",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W4230405732",
    "https://openalex.org/W4312274934",
    "https://openalex.org/W4243744369",
    "https://openalex.org/W3186632151",
    "https://openalex.org/W2963125977",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4225871896",
    "https://openalex.org/W4380558379",
    "https://openalex.org/W4312558481",
    "https://openalex.org/W4312367717",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W4304701453",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W4312270234",
    "https://openalex.org/W4312788538",
    "https://openalex.org/W4308480154",
    "https://openalex.org/W4312818263",
    "https://openalex.org/W4297847122",
    "https://openalex.org/W2962714319",
    "https://openalex.org/W4287255572",
    "https://openalex.org/W4290055897",
    "https://openalex.org/W4293339448",
    "https://openalex.org/W4307106676",
    "https://openalex.org/W4394671432",
    "https://openalex.org/W4323927254",
    "https://openalex.org/W4389115497",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W4312414163",
    "https://openalex.org/W2804935296",
    "https://openalex.org/W2991216808",
    "https://openalex.org/W3116959466",
    "https://openalex.org/W4311992366",
    "https://openalex.org/W3181158454",
    "https://openalex.org/W2990613095"
  ],
  "abstract": "The pretrain-finetune paradigm has achieved great success in NLP and 2D image fields because of the high-quality representation ability and transferability of their pretrained models. However, pretraining such a strong model is difficult in the 3D point cloud field due to the limited amount of point cloud sequences. This paper introduces Efficient Point Cloud Learning (EPCL), an effective and efficient point cloud learner for directly training high-quality point cloud models with a frozen CLIP transformer. Our EPCL connects the 2D and 3D modalities by semantically aligning the image features and point cloud features without paired 2D-3D data. Specifically, the input point cloud is divided into a series of local patches, which are converted to token embeddings by the designed point cloud tokenizer. These token embeddings are concatenated with a task token and fed into the frozen CLIP transformer to learn point cloud representation. The intuition is that the proposed point cloud tokenizer projects the input point cloud into a unified token space that is similar to the 2D images. Comprehensive experiments on 3D detection, semantic segmentation, classification and few-shot learning demonstrate that the CLIP transformer can serve as an efficient point cloud encoder and our method achieves promising performance on both indoor and outdoor benchmarks. In particular, performance gains brought by our EPCL are 19.7 AP50 on ScanNet V2 detection, 4.4 mIoU on S3DIS segmentation and 1.2 mIoU on SemanticKITTI segmentation compared to contemporary pretrained models. Code is available at \\url{https://github.com/XiaoshuiHuang/EPCL}.",
  "full_text": "Frozen CLIP Transformer Is an Efficient Point Cloud Encoder\nXiaoshui Huang1*, Zhou Huang2*, Sheng Li3*, Wentao Qu4\nTong He1†, Yuenan Hou1, Yifan Zuo2†, Wanli Ouyang1\n1Shanghai AI Laboratory\n2Jiangxi University of Finance and Economics\n3University of Electronic Science and Technology of China\n4Nanjing University of Science and Technology\nAbstract\nThe pretrain-finetune paradigm has achieved great success\nin NLP and 2D image fields because of the high-quality\nrepresentation ability and transferability of their pretrained\nmodels. However, pretraining such a strong model is diffi-\ncult in the 3D point cloud field due to the limited amount\nof point cloud sequences. This paper introduces Efficient\nPoint Cloud Learning (EPCL), an effective and efficient point\ncloud learner for directly training high-quality point cloud\nmodels with a frozen CLIP transformer. Our EPCL connects\nthe 2D and 3D modalities by semantically aligning the image\nfeatures and point cloud features without paired 2D-3D data.\nSpecifically, the input point cloud is divided into a series of\nlocal patches, which are converted to token embeddings by\nthe designed point cloud tokenizer. These token embeddings\nare concatenated with a task token and fed into the frozen\nCLIP transformer to learn point cloud representation. The in-\ntuition is that the proposed point cloud tokenizer projects the\ninput point cloud into a unified token space that is similar to\nthe 2D images. Comprehensive experiments on 3D detection,\nsemantic segmentation, classification and few-shot learning\ndemonstrate that the CLIP transformer can serve as an effi-\ncient point cloud encoder and our method achieves promis-\ning performance on both indoor and outdoor benchmarks. In\nparticular, performance gains brought by our EPCL are 19.7\nAP50 on ScanNet V2 detection, 4.4 mIoU on S3DIS segmen-\ntation and 1.2 mIoU on SemanticKITTI segmentation com-\npared to contemporary pretrained models. Code is available\nat https://github.com/XiaoshuiHuang/EPCL.\nIntroduction\nRecently, the pretrain-finetune paradigm has achieved great\nsuccess in natural language processing (NLP) (Chowdhery\net al. 2022; Gu et al. 2021) and 2D image fields (Alayrac\net al. 2022; Dosovitskiy et al. 2021; Radford et al. 2021).\nIn the pretrain-finetune paradigm, a backbone is first pre-\ntrained on a large-scale dataset to learn general and trans-\nferable representations. Then, the pretrained model is fine-\ntuned on training samples of the downstream task to learn\ntask-specific knowledge.\n*These authors contributed equally.\n†Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: (a) Traditional paradigm fine-tunes the whole\nmodel, while our method only fine-tunes the tokenizer (T)\nand head (H). The CLIP transformer, which is initialized\nfrom the original CLIP weight, is kept frozen during train-\ning. (b) Our EPCL brings accuracy gains with higher train-\ning efficiency compared to SOTA pre-training methods.\nThe CLIP models (Radford et al. 2021) are strong pre-\ntrained models, which are trained in the contrastive man-\nner by leveraging more than 400 million image-text pairs.\nThe impressive performance of CLIP models on few-shot\nand zero-shot tasks is attributed to the powerful representa-\ntion learned from the large quantity of pretraining data and\nthe inherent alignment between the image and language do-\nmains.\nHowever, directly applying the pretrain-finetune\nparadigm to the point cloud field will confront great\ndifficulties due to the scarcity of training samples as well\nas the inherent domain gap between point cloud and image\ndomains. For instance, the majority of pretraining methods\n(Pang et al. 2022; Qian et al. 2022; Yu et al. 2022; Xie et al.\n2020; Huang et al. 2023; Zheng et al. 2023) are trained\nwith limited data, ShapeNet (Chang et al. 2015) or ScanNet\ndatasets (Dai et al. 2017). ShapeNet contains about 50,\n000 objects and ScanNet contains 1, 513 room scans (Xie\net al. 2020). Compared to the pretraining data of CLIP,\nthe number of training samples in the point cloud field is\nmerely ten thousandth. The prior knowledge learned from\nthe limited training samples is also limited.\nInspired by the great success of CLIP, we ask a question:\ncan we apply the CLIP transformer to point cloud tasks as\na pretrained encoder? If the answer is yes, the 2D and 3D\nmodalities are bridged and we can leverage the pretrained\nCLIP transformer for learning effective representations in\nthe point cloud field. In this condition, the heavy reliance on\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2382\nFigure 2: Using the frozen CLIP image transformer as an en-\ncoder for 2D and 3D classification, the saliency maps show\nthe frozen CLIP model can attend to similar regions at dif-\nferent modalities.\n3D pre-training data can also be relieved.\nTo mitigate the domain gap between point cloud and\nimage domains, we design an extremely efficient module,\ni.e., the point tokenizer, to map the point cloud and im-\nage information into the same embedding space. Since the\npoint cloud and images all describe the surface informa-\ntion, we can consider them as a unified 2D-manifold that\nevery point/pixel has a neighbourhood homeomorphic to a\ncertain region of space R2 (Pressley 2010). We hypothe-\nsise that the frozen CLIP can extract meaningful representa-\ntion from the 2D-manifold input. The tokenizer embeds the\npoint/pixel neighbourhoods into the unified token space and\nweakly aligns the features of 2D images and 3D point cloud.\nThen, the transformer encoder extracts meaningful represen-\ntations by semantically aligning them further. We validate\nthis hypothesis in the experimental results (Figure 4).\nBased on this finding, we propose the Efficient Point\nCloud Learning (EPCL) framework to directly leverage the\nfrozen CLIP transformer as the encoder for point cloud\ntasks. The difference between our method and other 3D pre-\ntraining methods is illustrated in Figure 1. Our EPCL merely\nrequires the training of the lightweight task token, tokenizer\nand task head while previous 3D pre-training algorithms\nneed to the train the whole model. Take S3DIS segmentation\nas an example. The trainable parameters of our EPCL barely\naccount for 9.1% of all trainable model parameters, which\nstrongly demonstrates the superior efficiency of EPCL.\nTo sum up, our EPCL framework possesses the following\nmerits:\nHigh efficiency. Our EPCL fully leverages the rich and\nbroad knowledge hidden in CLIP models and only requires\nthe training of a small portion of trainable network parame-\nters. Our EPCL is apparently much more efficient compared\nto contemporary 3D pretraining algorithms that train all net-\nwork parameters.\nAligning 2D and 3D models without paired data.\nAligning multi-modal models has become mainstream to\ntrain a strong pretrained model while the existing methods\n(Radford et al. 2021; Alayrac et al. 2022) usually require\npaired data, e.g., image-text pairs (Radford et al. 2021) and\nvideo-text pairs (Alayrac et al. 2022). Our EPCL does not\nneed 3D-2D paired data to train the model when adapting\nthe CLIP image transformer for 3D tasks. Figure 2 reveals\nthat our method can semantically align similar regions in the\n3D point cloud compared to CLIP in the 2D image on the\nrecognition task.\nFree from 3D pre-training. The strong 2D pre-trained\nCLIP transformer is directly used for 3D point clouds in\nEPCL without requiring 3D pre-training, which helps cir-\ncumvent the barrier from the scarcity of 3D data.\nFacilitating few-shot learning in downstream tasks.\nLeveraging the rich knowledge learned in CLIP, EPCL is\neffective when downstream tasks have scarce training sam-\nples.\nWe perform comprehensive experiments on mainstream\npoint cloud tasks including detection, segmentation, recog-\nnition, classification and few-shot learning. Experimental re-\nsults show that EPCL achieves better performance than the\nstate-of-the-art 3D pre-training methods. Notably, our EPCL\nbrings the gains of 19.7 AP50 on ScanNet V2 detection,\n4.4 mIoU on S3DIS segmentation and 1.2 mIoU on Se-\nmanticKITTI segmentation compared to contemporary pre-\ntrained encoders.\nDifference to prior works.While there have been several\nexisting works proposing the utilization of 2D pretrained\nmodels, our method differs from them. For instance, Im-\nage2Point (Xu et al. 2022) expands 2D kernels of a CNN\ninto 3D kernels for point cloud feature extraction. Pix4Point\n(Qian et al. 2022) initializes from 2D pretrained backbones\nand finetunes the entire neural network, resulting in low\ntraining efficiency. PPKT (Liu et al. 2021b) pretrains 3D\nbackbones by distilling from 2D pretrained models, but it\nexhibits relatively low performance. ACT (Dong et al. 2022)\nadopts a two-stage strategy, training the teacher from a 2D\npretrained model and distilling the teacher to a 3D point\ncloud Transformer student through masked modeling. How-\never, prior works utilizing 2D pretrained models often suffer\nfrom either low performance or low efficiency.\nIn contrast to these prior works, our method, EPCL, di-\nrectly applies the frozen CLIP model to extract point cloud\nfeatures for various tasks, achieving better performance\ncompared to recent pretrained methods. In this GPT era, our\napproach provides an efficient encoder for point cloud fea-\nture extraction. Additionally, since CLIP is a general vision\npretrained model without task-specific information, we have\ndesigned a task token to further embed task-related biases.\nRelated Work\nCLIP-Based Methods\nCLIP (Radford et al. 2021), which aims to learn transfer-\nable visual representation from natural languages, has at-\ntracted increasing attention due to its promising results on\nvarious downstream tasks (Lei Ba et al. 2015; Kornblith,\nShlens, and Le 2019; Recht et al. 2019). It consists of two\nencoders for visual and text representations, respectively.\nThe method is jointly trained to align the two modalities\nwith over 400 million image-text pairs. The rich seman-\ntic representation shared by both domains inspires many\nworks and has been demonstrated effective in tasks like\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2383\nimage caption (Vinyals et al. 2015) and video (Carreira\net al. 2019). CLIPCAP (Mokady, Hertz, and Bermano 2021)\ntrained a lightweight mapping network to generate meaning-\nful captions, while the CLIP and language model is frozen.\nEVL (Lin et al. 2022) addressed the task of zero-shot video\nunderstanding via contrastive learning between video and\ntext representations, which is free from the annotation of the\ndownstream tasks. CLIP-ViL (Shen et al. 2021) uses CLIPs\nas pretrained backbones and finetunes the CLIP model on\nspecific vision-language tasks. LAMM (Yin et al. 2023) uses\nfrozen CLIP to embed multiple modalities into tokens and\ninput these tokens into large language model to conduct\nmulti-modal understanding tasks. Although promising, di-\nrectly applying CLIP to 3D tasks is non-trivial due to the\nsignificant domain gap.\nPoint Cloud Representation Learning\nLearning discriminative point cloud representation plays a\ncritical role in downstream tasks. The existing methods can\nbe divided into two categories, i.e., point-based and voxel-\nbased methods.\nPoint-based methods extract discriminative representa-\ntion from raw points using either multi-layer perception (Qi\net al. 2017), graph convolution (Wang et al. 2019) or kernel-\nbased convolution (Thomas et al. 2019) or multimodal fu-\nsion (Huang et al. 2022). The objective of these methods is\nto leverage the global structure information or local prop-\nerty of point neighbours to describe the 3D point cloud. The\nadvantages of these methods are that features can directly\nextract from analyzing point neighbours, the memory con-\nsumption is relatively small and no preprocessing steps are\nrequired.\nVoxel-based methods require to pre-process the given\npoint clouds into voxels. Then, voxel-based convolution\nneural networks are applied to extract the representation.\nTypical examples are V oxelNet (Riegler, Osman Ulusoy,\nand Geiger 2017) and Minkowski Engine (Choy, Gwak, and\nSavarese 2019). They design octree-based convolution and\nsparse convolution to effectively extract the local represen-\ntation of the point cloud without large GPU memory con-\nsumption. The advantage of these methods is that the repre-\nsentation can easily overcome the density variation.\n3D Pre-Training\nPre-training aims to learn prior knowledge from the training\ndata. The existing pre-training methods can be divided into\nthree categories,i.e., global contrastive, local contrastive and\nMasking AutoEncoder (MAE). The global contrastive learn-\ning methods (Wang et al. 2021; Mei et al. 2022; Huang et al.\n2023) compare the global feature difference of point clouds.\nIn contrast, local contrastive learning methods (Xie et al.\n2020; Wang et al. 2023) compare the local point feature dif-\nferences or local view pixel differences. Recently, several\nMAE-based pre-training methods (Yu et al. 2022; Pang et al.\n2022) are proposed to learn pretrained transformer back-\nbones. These methods leverage the knowledge of pretrained\ndatasets so that the downstream task models initialize from\na better starting point.\nRecently, several methods are proposed to use the 2D pre-\ntrained models on point cloud tasks. PointCLIP (Zhang et al.\n2022) projects the point cloud into 2D views and directly\nuses the frozen 2D pre-trained models for 3D recognition.\nP2P(Wang et al. 2022c) designs a projector to project the\n3D objects into the 2D plane and designs several prompts to\nuse the frozen 2D pre-trained backbones. PPKT (Liu et al.\n2021b) transfers the knowledge of 2D pretraiend model to\n3D backbones by using point-to-pixel loss. However, these\nmethods require projecting the 3D objects into several 2D\nviews and are sensitive to view projection. Hence, they are\nused for object-level point clouds but face great difficulty\nin handling scene-level point cloud perception. Image2Point\n(Xu et al. 2022) expands 2D kernels of a 2D CNN into 3D\nkernels and applied them to voxel-based point cloud tasks,\nwhich suffers from relatively low accuracy as the parame-\nter domain gap. Pix4Point (Qian et al. 2022) initializes from\n2D pretrained backbones and finetunes the whole neural net-\nwork, which is not efficient. ACT (Dong et al. 2022) requires\ntwo stage training to transfer the knowledge of 2D pretrained\nmodel to 3D point cloud transformer. However, the training\nprocess is not efficient and the performance has a large gap\nto task-specific model.\nDifferent to previous approaches, our EPCL directly uti-\nlizes the pre-trained 2D CLIP transformer as an efficient en-\ncoder to extract point cloud features. And our method is ap-\nplicable to both real-world and synthetic point cloud tasks.\nIn this GPT era, our work provides the insights that frozen\nCLIP can achieve comparable or better performance to re-\ncent SOTA pretrained methods with higher efficiency.\nMethod\nThis section first introduces the Vision Transformer\n(ViT) (Dosovitskiy et al. 2021) in the 2D image field and the\ntransformer in the point cloud field. Then, we present our\nEPCL and the rationale behind the workability of EPCL.\nPreliminary\n2D Vision Transformer. Given an image I ∈ RH×W×C,\nthe ViT (Dosovitskiy et al. 2021) divides the image into a\nsequence of flattened local image patches xp ∈ RN×(P2·C)\nand uses a tokenizer to convert these patches into a 1D se-\nquence of visual token embeddings EI(I) ∈ RN×D, where\nN is the number of tokens, P × P is the image patch size,\nD is the dimension of each image token. H and W are the\nheight and width of the given image, respectively. The total\nnumber of patches is N = HW/(P2). The position embed-\nding is concatenated to the visual token embeddings. Visual\ntokens and class tokens are fed into the transformer for fea-\nture extraction. Afterwards, the feature is fed into the clas-\nsification head to yield the classification results. Mathemat-\nically, the 2D ViT can be formulated as follows:\nz0 = [xcls, EI(I1,1), ..., EI(IH\nP ,W\nP\n)] + Epos, (1)\nezl = MSA(LN(zl−1) + zl−1), (2)\nzl = MLP(LN(ezl)) + ezl, (3)\ny = Hcls(LN(z0\nL)), (4)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2384\nFigure 3: Schematic overview of EPCL. The Point Tokenizer contains two successive steps, that are Farthest Point Sampling\n(FPS) for downsampling the input point cloud and Multi-Layer Perceptron (MLP) for extracting features from the downsampled\npoint cloud. The Task Token is task-specific and learnable. Tokens from the point tokenizer and task token are fed into the frozen\nCLIP Transformer. The Head uses the tokens from the Transformer to yield the predictions for each specific downstream task.\nThe CLIP transformer, which is initialized from the original CLIP weight, is kept frozen during the training stage, while the\npoint cloud tokenizer, task token and head are trainable.\nwhere EI(.) is the image tokenizer that extracts the token\nembedding for each image patch, and xcls is the class to-\nken. The transformer consists of L layers of layer normal-\nization LN(.), multi-head self-attention MSA(.) and multi-\nlayer perceptron MLP(.). The residual connection is applied\nafter every block.Hcls represents the classification head and\ntakes the feature of the class token at the last layer as input.\nTake the 1000-class image classification task as an example.\nHcls refers to a single MLP that maps the input feature into\na 1000-dimension classification output.\nTransformer in point cloud. Before the standard trans-\nformer is applied to the point cloud field, there are some\ntransformer layers (Zhao et al. 2021; Guo et al. 2021) specif-\nically designed for point cloud processing. Pioneered by\nPointBERT (Yu et al. 2022), the standard transformer has\nbeen applied to point cloud tasks. Similar to the ViT, the\npoint cloud P ∈ RA×3 is divided into a sequence of point\ncloud patches Pp ∈ RM×(3K), where A and M denote the\nnumber of points and patches, respectively, and K denotes\nthe number of points in each patch. These patches are sent to\na tokenizer to extract point token embeddings. Then, these\npoint token embeddings, position embedding and class to-\nken are fed into the standard transformer for feature extrac-\ntion. Afterwards, these features are fed into a task head for\ndownstream tasks. The Transformer for point cloud can be\nformulated as follows:\nf0 = [xcls, Ep(P1), ..., Ep(PM )] + Epos, (5)\nefl = MSA(LN(fl−1) + fl−1), (6)\nfl = MLP(LN( efl)) + efl, (7)\ny = Hcls\np (LN(f0\nL)), (8)\nwhere the Ep is the point cloud tokenizer, the three-layer\nMLP is usually applied for obtaining point cloud token em-\nbeddings. Hcls\np refers to three-layer MLPs that map the in-\nput feature into the C-dimension classification predictions\nfor C-class point cloud classification tasks.\nComparison between 2D and 3D transformers. By\ncomparing the equation (1)-(3) and (5)-(8), the standard\ntransformer module is the same, which consists of a series of\nthe LN, MSA and MLP. The only difference lies in the tok-\nenizer during the feature extraction. Then, the deep features\nare fed into different 2D/3D task heads for 2D/3D down-\nstream tasks. Here, we want to investigate whether the same\nstandard transformer module pretrained on 2D could be di-\nrectly applied to 3D point cloud tasks.\nThe Proposed Algorithm: EPCL\nThe motivation of our method is to leverage the frozen\n2D CLIP model for downstream point cloud understand-\ning tasks. To this end, we propose the Efficient Point Cloud\nLearning (EPCL) framework to use the 2D frozen CLIP\ntransformer and only finetune the tokenizer, task token and\ntask head. The overall framework is shown in Figure 3. This\nsection introduce the details of tokenizer, task token and\nFrozen CLIP transformer. The details of task head are at-\ntached in the supplement.\nPoint cloud tokenizer. Given a point cloud P ∈ RA×3,\nsimilar to the objective of the 2D tokenizer, the point cloud\ntokenizer aims to convert the input point cloud into a se-\nquence of token embeddings. Specifically, we first sample\nM points as centers of point patches by the Farthest Point\nSampling (FPS) algorithm, and then group K points from\neach center by the K-Nearest Neighbourhood (KNN) al-\ngorithm and thus obtain M patches. These patches are fur-\nther fed into several MLPs to obtain the token embeddings\nEp(P) ∈ RM×Dp, i∈ [1...M], where Dp is the dimension\nof each point token, i.e., 768.\nTask token. Since the CLIP is trained by a large-scale\ntext-image pair dataset, it is lacked of task information. To\nfurther embed the given point clouds into a shared token\nspace that benefit for the task, we design a task token to\nlearn a global task-related bias. Our task token module is\nimplemented by a fully connected layer with learnable pa-\nrameters. Following (Liu et al. 2021a), we initialize the task\ntoken as enumerated numbers.\nFrozen CLIP transformer. After the input point cloud is\nconverted into a sequence of visual tokens, we feed visual to-\nkens and task tokens into the CLIP image transformer, which\nis initialized from the original CLIP weight and kept frozen\nduring training. The frozen CLIP transformer serves as the\nfeature extractor for downstream point cloud tasks.\nAnalysis on 2D-3D Semantic Alignment of CLIP Trans-\nformer To analyse the workability of frozen 2D CLIP\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2385\nLayer-1 Layer-9 Layer-12\nFigure 4: The cross-correlation between CLIP image fea-\ntures and point cloud features at layers 1, 9, and 12 for dif-\nferent object categories.\ntransformer for point cloud representation learning, we cal-\nculate the semantic similarity between image features and\npoint cloud features. Specifically, we first calculate their fea-\nture cross-correlation at different layers of the same CLIP\ntransformer. Then, we use the transformer explanation tool\nin (Chefer, Gur, and Wolf 2021) to obtain the significance\nmap. For the 2D image, we crop a view from the ShapeNet\nmodel to keep the texture and apply the CLIP model to clas-\nsify the image view.\nStatistical results. Figure 4 shows that the tokenizer can\nweakly align the 2D and 3D features. At shallow layers, the\nfeatures from the point cloud and image for the same cate-\ngory have lower cross-correlation in the left sub-figure. As\nthe layers go deeper, the 2D-3D features are matched with\nhigh cross-correlation in the same category (see the right\nsub-figure).\nVisual results. Figure 5 shows the roughly similar signif-\nicance maps at a 2D image and 3D point cloud. This figure\nshows that the frozen CLIP model can capture similar se-\nmantic regions from 2D and 3D modalities.\n3D Point Cloud 2D Image\nFigure 5: The semantic similarity between 2D image and 3D\npoint cloud from significance maps.\nThe Rationale Behind EPCL To better understand why\nthe frozen CLIP transformer is workable for the point cloud,\nwe provide an intuitive explanation from the manifold as-\npect. We define the input token space asΩI and output token\nspace as ΩO. The CLIP image transformer learns a func-\ntion f to map the input tokens X ∈ ΩI into semantically\nmeaningful tokens Y ∈ ΩO: Y = f(X). Since the CLIP\nhas been trained in a large-scale dataset that contains diverse\nweb image-text pairs, the input token space ΩI is large and\ndiverse.\nThe image tokenizer uses convolution to aggregate lo-\ncal information into the image token space, denoted by ΩI\nI.\nSimilarly, our point cloud tokenizer uses the FPS + KNN\n+ MLP to aggregate local neighbourhood information to-\nken space, denoted by ΩP\nI . Since a point cloud frame only\nrecords points on the surface, according to the manifold def-\ninition (Pressley 2010), a small local point cloud patch is\napproximately a plane and the 3D point cloud lies in the 2D\nmanifold. Since the given point cloud consists of many lo-\ncal planes, our tokenizer and the task token learn to project\nthe 2D-manifold point cloud into the token space ΩP\nI that is\nsimilar to the CLIP image token spaceΩI\nI projected from 2D\nimage plane. Since the images and local point cloud patches\nare both 2D-manifold planes, the above tokenizer learning is\nachievable. Previous research shows that the transformer in-\nherently extracts shape-biased features (Park and Kim 2022;\nNaseer et al. 2021) for 2D images. Therefore, the CLIP im-\nage transformer can extract shape-based features from the\ntoken space ΩP\nI for the given point cloud.\nExperiments\nWe introduce the details of used datasets and baselines in\nsection . Then, experiments of the downstream tasks are de-\nscribed in section , section and section , respectively. After-\nwards, the ablation studies are presented in section .\nDatasets and Baselines\nDatasets. We conduct real-world detection on ScanNet (Dai\net al. 2017), indoor semantic segmentation on S3DIS (Ar-\nmeni et al. 2016) and outdoor semantic segmentation on Se-\nmanticKITTI Behley et al. (2019). Also, we evaluate the ac-\ncuracy of few-shot learning and classification on synthetic\nModelNet40 (Wu et al. 2015).\nBaselines. Our EPCL focuses on leveraging pre-training\nfor downstream tasks. For a fair comparison, the state-of-\nthe-art (SOTA) point cloud pre-training methods with the\ntransformer-based architectures are selected as the baselines.\n• Detection: Following MaskPoint (Yu et al. 2022),\nwe compare with MaskPoint (Yu et al. 2022), Point-\nBERT(Pang et al. 2022), TAP (Wang et al. 2023),\nSimple3D-Former (Wang et al. 2022b), SoftGroup (Vu\net al. 2022) and CAGroup3D (Wang et al. 2022a).\n• Segmentation: Following Simple3D-former (Wang et al.\n2022b), we compare with MaskPoint (Pang et al. 2022)\nand other transformer-based method (Zhao et al. 2021).\n• Classification: Following MaskPoint (Yu et al. 2022),\nwe compare with MaskPoint (Yu et al. 2022), Point-\nBERT(Pang et al. 2022), Simple3D-Former (Wang et al.\n2022b) and P2P (Wang et al. 2022c).\nDetection\nFor many contemporary 3D pre-training works, they report\nthe performance mainly on object-level classification and\npart segmentation tasks, which is insufficient in real-world\npoint cloud tasks. Moreover, the most recent P2P (Wang\net al. 2022c) needs to project the point cloud into 2D views,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2386\nwhich is confronted with great challenges in solving real-\nworld point cloud tasks. Our EPCL does not require any pro-\njections and thus can be widely applied to real-world point\ncloud scenarios. In this section, the detection on ScanNet V2\nis evaluated and compared with the state-of-the-art 3D pre-\ntraining approaches (Pang et al. 2022; Yu et al. 2022) and\nobject detection methods(Vu et al. 2022; Wang et al. 2022a).\nMethod 3D Pretrain AP 50 AP25\nPointBERT ✓ 38.3 61.0\nMaskPoint ✓ 42.1 64.2\nTAP ✓ 41.4 63.0\nSimple3D-Former 40.7 59.4\nCLIP frozen + 3DETR 43.0 62.6\nSoftGroup 59.4 71.6\nCAGroup3D 60.8 73.6\nCLIP froz.+CAGoup3D 61.1 73.7\nTable 1: Detection on ScanNet V2.\nTable 1 shows that the frozen CLIP model achieves better\naccuracy than baseline methods. This observation shows that\nthe CLIP transformer can effectively learn 3D representa-\ntion to solve real-world 3D detection and achieve better per-\nformance than state-of-the-art 3D pre-training method, TAP\n(Wang et al. 2023). Note that the CLIP model is frozen and\nhas not seen any 3D point cloud in their learned parame-\nters. Our method only fine-tunes the same training dataset\nwith other baselines. These results demonstrate that the\nCLIP transformer achieves better accuracy. Notably, EPCL\nachieves better performance than the state-of-the-art object\ndetection method CAGroup3D when using the head of CA-\nGroup3D. The impressive performance is attributed to the\nstrong ability of the CLIP model to align the features in dif-\nferent modalities.\nSemantic Segmentation\nIndoor semantic segmentation. This section introduces the\nexperiments on indoor segmentation dataset S3DIS(Armeni\net al. 2016). To ensure fair comparison, we put all these en-\ncoders on the same code base, which shares the same hier-\narchical tokenizer and semantic task head. The MaskPoint\ninitializes the encoder with the pre-trained model of Mask-\nPoint, and the Simple3D-Former initializes the encoder with\n2D ViT. Then, the MaskPoint and Simple3D-Former meth-\nods finetune the whole model on the S3DIS training sam-\nples. In contrast, our method keeps the CLIP model frozen\nand only finetunes the tokenizer and task head.\nMethod OA mAcc. mIoU.\nPoint Transformer 90.8 76.5 70.4\nMaskPoint 89.0 73.8 67.1\nSimple3D-Former - 72.5 67.0\nOurs 90.8 77.8 71.5\nTable 2: Indoor semantic segmentation on S3DIS (Area5).\nTable 2 shows that the frozen CLIP model obtains ob-\nviously better accuracy (i.e., mAcc. and mIoU) than other\nstate-of-the-art 3D pre-training methods as well as Point\nTransformer on S3DIS (Area5) dataset. This observation il-\nlustrates that the frozen CLIP model is an efficient point\ncloud learner in the real-world semantic segmentation task.\nOutdoor semantic segmentation. We also evaluate our\nEPCL on outdoor segmentation task and compare with re-\ncent task-specific methods (Cylinder3D (Zhou et al. 2020),\nPVKD (Hou et al. 2022), 2DPASS (Yan et al. 2022),\nRPVNet (Xu et al. 2021)) and pretrained method (Range-\nFormer (Kong et al. 2023)). Quantitative comparison on\nSemanticKITTI validation set is shown in Table 3, which\ndemonstrates better accuracy than these compared meth-\nods. It is worth noting that the frozen CLIP achieves\nthe highly competitive performance compared to the task-\nspecific model, even without any engineering tricks such as\ntest time augmentation and model ensemble.\nMethod mIoU. (val.)\nCylinder3D 65.2\nPVKD 66.4\n2DPASS 69.3\nRPVNet 69.6\nRangeFormer 69.6\nOurs 72.4\nTable 3: Outdoor semantic segmentation on SemanticKITTI.\nMethod 3D Pretrain OA\nPointBERT ✓ 93.2\nMaskPoint ✓ 93.8\nP2P 92.7\nSimple3D-Former 92.0\nOurs + w/o CLIP frozen 92.3\nOurs 92.9\nTable 4: Classification on ModelNet40.\nClassification\nSupervised classification. Table 4 summarizes the clas-\nsification results on the synthetic ModelNet40 dataset. Our\nmethod achieves comparable performance to the state-of-\nthe-art 3D pre-training methods although our method does\nnot pre-train the model on the object dataset, i.e., ShapeNet.\nCompared to the P2P, which is the recent state-of-the-\nart method that directly used a 2D pre-trained model, our\nmethod achieves better accuracy. P2P designs a projection\nmodule to render images from 3D objects which is tailored\nfor 3D classification. Our method does not need to project\n3D to 2D images and directly processes the 3D tokens,\nwhich shows potential in other applications except classifi-\ncation. Compared to Simple3D-Former, which finetunes the\nwhole model from a 2D pre-trained model, our method still\nachieves better accuracy. These positive results verify our\nargument that the frozen CLIP transformer is an effective\nencoder to learn 3D representation for point cloud under-\nstanding.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2387\nTuning Method 5-w,10-s 5-w,20-s 10-w,10-s 10-w,20-s 30-w,10-s\nPointBERT 94.6 ± 3.1 96.3 ± 2.7 91.0 ± 5.4 92.7 ± 5.1 81.4 ± 2.4\nMaskPoint 95.0 ± 3.7 97.2 ± 1.7 91.4 ± 4.0 93.4 ± 3.5 80.7 ± 4.9\nOurs 95.1 ± 2.7 97.3 ± 1.6 91.1 ± 4.2 93.5 ± 3.8 81.7 ± 0.7\nTable 5: Few-shot learning accuracy of 3D pre-training methods and EPCL on ModelNet40.\nFew-shot learning. One important advantage of pre-\ntrained models is that fewer training samples are required\nin downstream tasks. This is usually evaluated by the few-\nshot learning task. To evaluate the few-shot learning abil-\nity, we follow MaskPoint (Pang et al. 2022) to conduct ex-\nperiments with the setting of ”K -way N-shot”, i.e., 5way-\n10shot, 5way-20shot,10way-10shot and 10way-20shot.\nTable 5 summarizes the comparison experiments on ”K -\nway N-shot” few-shot learning. Our EPCL obtains more ac-\ncurate classification results than the state-of-the-art 3D pre-\ntraining methods. This observation shows that the frozen\nCLIP transformer is an effective representation learning en-\ncoder in the challenging few-shot learning setting.\nAblation Studies and Discussion\nIn this section, we introduce several key ablation studies to\nexamine the effect of each component of our EPCL. More\nablation studies and discussions can be found in the supple-\nment.\nTask token. The task token module aims to learn task\nembedding for a specific task. As the task embedding mod-\nule is only trainable in the training stage and the parameters\nand initialization are frozen during the inference stage. To\ndemonstrate its effectiveness, we conduct an ablation study\nby removing the task token module on ScanNet V2 at the\ndetection task. Table 6 shows that the detection accuracy de-\ncreases (1.9%) when the task embedding is discarded. This\nexperiment illustrates that learning additional task-related\nfeature bias is beneficial to the CLIP model in point cloud\ntasks.\ntask token CLIP Frozen AP 50 Train Para. (%)\n✓ 59.2 55.23\n✓ 60.1 100\n✓ ✓ 61.1 55.51\nTable 6: Ablation studies of the task embedding and the\nfrozen strategy on ScanNet V2 detection task.\nTask token transferability. To demonstrate the transfer-\nability of our task token, we use the detection task token\nto replace the classification task token. The classification\nresult improves by 5.7% by using the detection task token\ncompared to the random one. The improved performance of\nusing the detection task token over the random task token\ndemonstrates the transferability. Moreover, the accuracy will\ndrop >10% when directly using other task tokens. This re-\nsult shows that the task token needs to be fine-tuned together\nwith the tokenizer. Simply replacing the task token will lead\nto inferior performance.\nCLIP Frozen or not? Recall that we freeze the CLIP\nmodel during the entire training process. It is natural to won-\nder what the performance will be if turning the parameters of\nthe CLIP model during training. To answer this question, we\nturn the whole neural network on during the training stage\nand conduct an ablation study on real-world detection task.\nTable 6 shows that the accuracy drops 1.0% when the CLIP\nmodel is turned on. The reason is that the relatively small-\nscale 3D training dataset fine-tunes the CLIP model to a\nworse parameter space compared to the one trained on the\nlarge-scale dataset. Also, our method shows that the frozen\nCLIP transformer achieves better training efficiency than the\nversion without freezing.\nMethod AP50 AP25\nSAM 59.5 73.7\nDINO 60.0 72.7\nOurs 61.1 73.7\nTable 7: Detection results of other 2D pre-trained models.\nIs CLIP better than other 2D pretrained models? The\nCLIP model is trained on a large-scale dataset that pairs\ninternet images with text, containing a wide range of real-\nworld multimodal knowledge. To demonstrate its effective-\nness in 3D representation learning, we replace the frozen\nCLIP transformer with other 2D pretrained models (SAM\n(Kirillov et al. 2023), DINO (Caron et al. 2021)) that are\nsolely trained on images (single modality). We then freeze\nthe model during the training stage. Table 7 illustrates\nthat CLIP achieves higher detection accuracy on ScanNet-\nV2 compared to other 2D pretrained models. This result\ndemonstrates that the CLIP transformer, with its multimodal\nknowledge, outperforms 2D pretrained models that are only\ntrained on images.\nConclusion\nThis paper proposes an efficient yet effective method to\nconstruct point cloud understanding models by using the\nfrozen CLIP transformer. Our method converts the input\npoint cloud into sequential tokens with a point tokenizer.\nThese tokens and the learnable task token input into the\nfrozen CLIP transformer can generate robust 3D represen-\ntation. We conduct thorough analyses of the inner mecha-\nnism and find the tokenizer can weakly align the 3D and 2D\nfeatures at different modalities. Then, the CLIP transformer\ncan align them further. Our method achieves appealing per-\nformance on a wide range of downstream tasks, including\nboth real-world detection and segmentation tasks as well as\nsynthetic object-level classification tasks.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2388\nAcknowledgements\nThis work is partially supported by the National Key\nR&D Program of China(NO.2022ZD0160101), National\nNatural Science Foundation of China General Program\n(NO. 62271237) and Jiangxi Provincial Natural Science\nFoundation Outstanding Young Scientist Program (NO.\n20224ACB212005).\nReferences\nAlayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Hasson,\nY .; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.; et al. 2022.\nFlamingo: a visual language model for few-shot learning. arXiv\npreprint arXiv:2204.14198.\nArmeni, I.; Sener, O.; Zamir, A. R.; Jiang, H.; Brilakis, I.; Fischer,\nM.; and Savarese, S. 2016. 3d semantic parsing of large-scale in-\ndoor spaces. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 1534–1543.\nBehley, J.; Garbade, M.; Milioto, A.; Quenzel, J.; Behnke, S.;\nStachniss, C.; and Gall, J. 2019. Semantickitti: A dataset for se-\nmantic scene understanding of lidar sequences. In Proceedings of\nthe IEEE/CVF international conference on computer vision, 9297–\n9307.\nCaron, M.; Touvron, H.; Misra, I.; J ´egou, H.; Mairal, J.; Bo-\njanowski, P.; and Joulin, A. 2021. Emerging Properties in Self-\nSupervised Vision Transformers. In Proceedings of the Interna-\ntional Conference on Computer Vision (ICCV).\nCarreira, J.; Noland, E.; Hillier, C.; and Zisserman, A. 2019. A\nshort note on the kinetics-700 human action dataset.arXiv preprint\narXiv:1907.06987.\nChang, A. X.; Funkhouser, T.; Guibas, L.; Hanrahan, P.; Huang,\nQ.; Li, Z.; Savarese, S.; Savva, M.; Song, S.; Su, H.; Xiao, J.; Yi,\nL.; and Yu, F. 2015. ShapeNet: An Information-Rich 3D Model\nRepository. Technical Report arXiv:1512.03012 [cs.GR], Stanford\nUniversity — Princeton University — Toyota Technological Insti-\ntute at Chicago.\nChefer, H.; Gur, S.; and Wolf, L. 2021. Generic attention-model\nexplainability for interpreting bi-modal and encoder-decoder trans-\nformers. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, 397–406.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.;\nRoberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.;\net al. 2022. Palm: Scaling language modeling with pathways.arXiv\npreprint arXiv:2204.02311.\nChoy, C.; Gwak, J.; and Savarese, S. 2019. 4d spatio-temporal\nconvnets: Minkowski convolutional neural networks. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 3075–3084.\nDai, A.; Chang, A. X.; Savva, M.; Halber, M.; Funkhouser, T.; and\nNießner, M. 2017. Scannet: Richly-annotated 3d reconstructions of\nindoor scenes. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 5828–5839.\nDong, R.; Qi, Z.; Zhang, L.; Zhang, J.; Sun, J.; Ge, Z.; Yi, L.; and\nMa, K. 2022. Autoencoders as Cross-Modal Teachers: Can Pre-\ntrained 2D Image Transformers Help 3D Representation Learning?\nIn The Eleventh International Conference on Learning Represen-\ntations.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai,\nX.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.;\nGelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth\n16x16 Words: Transformers for Image Recognition at Scale.ICLR.\nGu, Y .; Han, X.; Liu, Z.; and Huang, M. 2021. Ppt: Pre-\ntrained prompt tuning for few-shot learning. arXiv preprint\narXiv:2109.04332.\nGuo, M.-H.; Cai, J.-X.; Liu, Z.-N.; Mu, T.-J.; Martin, R. R.; and Hu,\nS.-M. 2021. Pct: Point cloud transformer. Computational Visual\nMedia, 7(2): 187–199.\nHou, Y .; Zhu, X.; Ma, Y .; Loy, C. C.; and Li, Y . 2022. Point-to-\nvoxel knowledge distillation for lidar semantic segmentation. In\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 8479–8488.\nHuang, T.; Dong, B.; Yang, Y .; Huang, X.; Lau, R. W.; Ouyang,\nW.; and Zuo, W. 2023. Clip2point: Transfer clip to point cloud\nclassification with image-depth pre-training. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 22157–\n22167.\nHuang, X.; Qu, W.; Zuo, Y .; Fang, Y .; and Zhao, X. 2022. IMFNet:\nInterpretable multimodal fusion for point cloud registration. IEEE\nRobotics and Automation Letters, 7(4): 12323–12330.\nKirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.; Gustafson,\nL.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.-Y .; Doll´ar, P.; and\nGirshick, R. 2023. Segment Anything. arXiv:2304.02643.\nKong, L.; Liu, Y .; Chen, R.; Ma, Y .; Zhu, X.; Li, Y .; Hou, Y .; Qiao,\nY .; and Liu, Z. 2023. Rethinking range view representation for lidar\nsegmentation. arXiv preprint arXiv:2303.05367.\nKornblith, S.; Shlens, J.; and Le, Q. V . 2019. Do better imagenet\nmodels transfer better? In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 2661–2671.\nLei Ba, J.; Swersky, K.; Fidler, S.; et al. 2015. Predicting deep\nzero-shot convolutional neural networks using textual descriptions.\nIn Proceedings of the IEEE international conference on computer\nvision, 4247–4255.\nLin, Z.; Geng, S.; Zhang, R.; Gao, P.; de Melo, G.; Wang, X.; Dai,\nJ.; Qiao, Y .; and Li, H. 2022. Frozen CLIP Models are Efficient\nVideo Learners. In European Conference on Computer Vision,\n388–404. Springer.\nLiu, X.; Ji, K.; Fu, Y .; Du, Z.; Yang, Z.; and Tang, J. 2021a. P-\ntuning v2: Prompt tuning can be comparable to fine-tuning univer-\nsally across scales and tasks. arXiv preprint arXiv:2110.07602.\nLiu, Y .-C.; Huang, Y .-K.; Chiang, H.-Y .; Su, H.-T.; Liu, Z.-Y .;\nChen, C.-T.; Tseng, C.-Y .; and Hsu, W. H. 2021b. Learning from\n2d: Contrastive pixel-to-point knowledge transfer for 3d pretrain-\ning. arXiv preprint arXiv:2104.04687.\nMei, G.; Huang, X.; Liu, J.; Zhang, J.; and Wu, Q. 2022. Unsuper-\nvised Point Cloud Pre-Training Via Contrasting and Clustering. In\n2022 IEEE International Conference on Image Processing (ICIP),\n66–70. IEEE.\nMokady, R.; Hertz, A.; and Bermano, A. H. 2021. Clipcap: Clip\nprefix for image captioning. arXiv preprint arXiv:2111.09734.\nNaseer, M. M.; Ranasinghe, K.; Khan, S. H.; Hayat, M.; Shah-\nbaz Khan, F.; and Yang, M.-H. 2021. Intriguing properties of vision\ntransformers. Advances in Neural Information Processing Systems,\n34: 23296–23308.\nPang, Y .; Wang, W.; Tay, F. E.; Liu, W.; Tian, Y .; and Yuan, L.\n2022. Masked autoencoders for point cloud self-supervised learn-\ning. arXiv preprint arXiv:2203.06604.\nPark, N.; and Kim, S. 2022. How do vision transformers work?\narXiv preprint arXiv:2202.06709.\nPressley, A. N. 2010. Elementary differential geometry. Springer\nScience & Business Media.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2389\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017. Pointnet: Deep\nlearning on point sets for 3d classification and segmentation. In\nProceedings of the IEEE conference on computer vision and pat-\ntern recognition, 652–660.\nQian, G.; Zhang, X.; Hamdi, A.; and Ghanem, B. 2022. Pix4Point:\nImage Pretrained Transformers for 3D Point Cloud Understanding.\narXiv preprint arXiv:2208.12259.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agar-\nwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021.\nLearning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning, 8748–\n8763. PMLR.\nRecht, B.; Roelofs, R.; Schmidt, L.; and Shankar, V . 2019. Do\nimagenet classifiers generalize to imagenet? In International Con-\nference on Machine Learning, 5389–5400. PMLR.\nRiegler, G.; Osman Ulusoy, A.; and Geiger, A. 2017. Octnet:\nLearning deep 3d representations at high resolutions. In Proceed-\nings of the IEEE conference on computer vision and pattern recog-\nnition, 3577–3586.\nShen, S.; Li, L. H.; Tan, H.; Bansal, M.; Rohrbach, A.; Chang, K.-\nW.; Yao, Z.; and Keutzer, K. 2021. How Much Can CLIP Ben-\nefit Vision-and-Language Tasks? In International Conference on\nLearning Representations.\nThomas, H.; Qi, C. R.; Deschaud, J.-E.; Marcotegui, B.; Goulette,\nF.; and Guibas, L. J. 2019. Kpconv: Flexible and deformable con-\nvolution for point clouds. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, 6411–6420.\nVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015. Show\nand tell: A neural image caption generator. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition,\n3156–3164.\nVu, T.; Kim, K.; Luu, T. M.; Nguyen, T.; and Yoo, C. D. 2022. Soft-\ngroup for 3d instance segmentation on point clouds. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2708–2717.\nWang, H.; Ding, L.; Dong, S.; Shi, S.; Li, A.; Li, J.; Li, Z.; and\nWang, L. 2022a. CAGroup3D: Class-Aware Grouping for 3D Ob-\nject Detection on Point Clouds. arXiv preprint arXiv:2210.04264.\nWang, H.; Liu, Q.; Yue, X.; Lasenby, J.; and Kusner, M. J. 2021.\nUnsupervised point cloud pre-training via occlusion completion.\nIn Proceedings of the IEEE/CVF international conference on com-\nputer vision, 9782–9792.\nWang, Y .; Fan, Z.; Chen, T.; Fan, H.; and Wang, Z. 2022b. Can We\nSolve 3D Vision Tasks Starting from A 2D Vision Transformer?\narXiv preprint arXiv:2209.07026.\nWang, Y .; Sun, Y .; Liu, Z.; Sarma, S. E.; Bronstein, M. M.; and\nSolomon, J. M. 2019. Dynamic graph cnn for learning on point\nclouds. Acm Transactions On Graphics (tog), 38(5): 1–12.\nWang, Z.; Yu, X.; Rao, Y .; Zhou, J.; and Lu, J. 2022c. P2p: Tuning\npre-trained image models for point cloud analysis with point-to-\npixel prompting. arXiv preprint arXiv:2208.02812.\nWang, Z.; Yu, X.; Rao, Y .; Zhou, J.; and Lu, J. 2023. Take-A-Photo:\n3D-to-2D Generative Pre-training of Point Cloud Models.\nWu, Z.; Song, S.; Khosla, A.; Yu, F.; Zhang, L.; Tang, X.; and Xiao,\nJ. 2015. 3d shapenets: A deep representation for volumetric shapes.\nIn Proceedings of the IEEE conference on computer vision and pat-\ntern recognition, 1912–1920.\nXie, S.; Gu, J.; Guo, D.; Qi, C. R.; Guibas, L.; and Litany, O. 2020.\nPointcontrast: Unsupervised pre-training for 3d point cloud under-\nstanding. In European conference on computer vision, 574–591.\nSpringer.\nXu, C.; Yang, S.; Galanti, T.; Wu, B.; Yue, X.; Zhai, B.; Zhan, W.;\nVajda, P.; Keutzer, K.; and Tomizuka, M. 2022. Image2point: 3d\npoint-cloud understanding with 2d image pretrained models. In\nEuropean Conference on Computer Vision, 638–656. Springer.\nXu, J.; Zhang, R.; Dou, J.; Zhu, Y .; Sun, J.; and Pu, S. 2021.\nRpvnet: A deep and efficient range-point-voxel fusion network for\nlidar point cloud segmentation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 16024–16033.\nYan, X.; Gao, J.; Zheng, C.; Zheng, C.; Zhang, R.; Cui, S.; and\nLi, Z. 2022. 2dpass: 2d priors assisted semantic segmentation on\nlidar point clouds. In European Conference on Computer Vision,\n677–695. Springer.\nYin, Z.; Wang, J.; Cao, J.; Shi, Z.; Liu, D.; Li, M.; Sheng, L.; Bai,\nL.; Huang, X.; Wang, Z.; et al. 2023. LAMM: Language-Assisted\nMulti-Modal Instruction-Tuning Dataset, Framework, and Bench-\nmark. arXiv preprint arXiv:2306.06687.\nYu, X.; Tang, L.; Rao, Y .; Huang, T.; Zhou, J.; and Lu, J. 2022.\nPoint-bert: Pre-training 3d point cloud transformers with masked\npoint modeling. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 19313–19322.\nZhang, R.; Guo, Z.; Zhang, W.; Li, K.; Miao, X.; Cui, B.; Qiao,\nY .; Gao, P.; and Li, H. 2022. Pointclip: Point cloud understanding\nby clip. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 8552–8562.\nZhao, H.; Jiang, L.; Jia, J.; Torr, P. H.; and Koltun, V . 2021. Point\ntransformer. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 16259–16268.\nZheng, X.; Huang, X.; Mei, G.; Hou, Y .; Lyu, Z.; Dai, B.; Ouyang,\nW.; and Gong, Y . 2023. Point Cloud Pre-training with Diffusion\nModels. arXiv preprint arXiv:2311.14960.\nZhou, H.; Zhu, X.; Song, X.; Ma, Y .; Wang, Z.; Li, H.; and Lin,\nD. 2020. Cylinder3d: An effective 3d framework for driving-scene\nlidar semantic segmentation. arXiv preprint arXiv:2008.01550.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n2390",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6249250173568726
    },
    {
      "name": "Encoder",
      "score": 0.598422646522522
    },
    {
      "name": "Computer science",
      "score": 0.5517610311508179
    },
    {
      "name": "Cloud computing",
      "score": 0.42051708698272705
    },
    {
      "name": "Electrical engineering",
      "score": 0.2962256371974945
    },
    {
      "name": "Engineering",
      "score": 0.18880784511566162
    },
    {
      "name": "Operating system",
      "score": 0.1414562165737152
    },
    {
      "name": "Voltage",
      "score": 0.057933688163757324
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210122302",
      "name": "ShangHai JiAi Genetics & IVF Institute",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I59649739",
      "name": "Jiangxi University of Finance and Economics",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I150229711",
      "name": "University of Electronic Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    }
  ],
  "cited_by": 19
}