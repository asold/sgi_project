{
  "title": "SGFormer: Semantic Graph Transformer for Point Cloud-Based 3D Scene Graph Generation",
  "url": "https://openalex.org/W4393149385",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2155348223",
      "name": "Changsheng Lv",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2765271593",
      "name": "Mengshi Qi",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2096615312",
      "name": "Xia Li",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2294975087",
      "name": "Zhengyuan Yang",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2184795702",
      "name": "Huadong Ma",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2155348223",
      "name": "Changsheng Lv",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2765271593",
      "name": "Mengshi Qi",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2184795702",
      "name": "Huadong Ma",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4328029385",
    "https://openalex.org/W6756344431",
    "https://openalex.org/W64813323",
    "https://openalex.org/W3195833508",
    "https://openalex.org/W6787995345",
    "https://openalex.org/W3103830808",
    "https://openalex.org/W3090459043",
    "https://openalex.org/W2786947658",
    "https://openalex.org/W6757204575",
    "https://openalex.org/W6669605930",
    "https://openalex.org/W2784814091",
    "https://openalex.org/W2756246624",
    "https://openalex.org/W2789177853",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W6721769334",
    "https://openalex.org/W3025468529",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W2902620067",
    "https://openalex.org/W3205377310",
    "https://openalex.org/W2186222003",
    "https://openalex.org/W3128401049",
    "https://openalex.org/W3022778813",
    "https://openalex.org/W3014195143",
    "https://openalex.org/W2971974407",
    "https://openalex.org/W2888754481",
    "https://openalex.org/W3054401859",
    "https://openalex.org/W6774617867",
    "https://openalex.org/W6810213675",
    "https://openalex.org/W2969056421",
    "https://openalex.org/W3015373178",
    "https://openalex.org/W6747904511",
    "https://openalex.org/W6732298944",
    "https://openalex.org/W6753998590",
    "https://openalex.org/W2740693737",
    "https://openalex.org/W2769277182",
    "https://openalex.org/W3135541551",
    "https://openalex.org/W6757488640",
    "https://openalex.org/W4223930281",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3213320466",
    "https://openalex.org/W4388093614",
    "https://openalex.org/W2964051675",
    "https://openalex.org/W2962737704",
    "https://openalex.org/W2479423890",
    "https://openalex.org/W2886970679",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963902384",
    "https://openalex.org/W3035017890",
    "https://openalex.org/W3211394146",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4312274934",
    "https://openalex.org/W2964228567",
    "https://openalex.org/W2579549467",
    "https://openalex.org/W2963536419",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2904332125",
    "https://openalex.org/W3173271937",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4313136902",
    "https://openalex.org/W2963649796",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W2775221064",
    "https://openalex.org/W3114942877",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W2077069816",
    "https://openalex.org/W2894669491",
    "https://openalex.org/W2953007748",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2981422075",
    "https://openalex.org/W2963319519",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3204757729",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2884561390"
  ],
  "abstract": "In this paper, we propose a novel model called SGFormer, Semantic Graph TransFormer for point cloud-based 3D scene graph generation. The task aims to parse a point cloud-based scene into a semantic structural graph, with the core challenge of modeling the complex global structure. Existing methods based on graph convolutional networks (GCNs) suffer from the over-smoothing dilemma and can only propagate information from limited neighboring nodes. In contrast, SGFormer uses Transformer layers as the base building block to allow global information passing, with two types of newly-designed layers tailored for the 3D scene graph generation task. Specifically, we introduce the graph embedding layer to best utilize the global information in graph edges while maintaining comparable computation costs. Furthermore, we propose the semantic injection layer to leverage linguistic knowledge from large-scale language model (i.e., ChatGPT), to enhance objects' visual features. We benchmark our SGFormer on the established 3DSSG dataset and achieve a 40.94% absolute improvement in relationship prediction's R@50 and an 88.36% boost on the subset with complex scenes over the state-of-the-art. Our analyses further show SGFormer's superiority in the long-tail and zero-shot scenarios. Our source code is available at https://github.com/Andy20178/SGFormer.",
  "full_text": "SGFormer: Semantic Graph Transformer for\nPoint Cloud-Based 3D Scene Graph Generation\nChangsheng Lv1,2, Mengshi Qi1,2*, Xia Li2, Zhengyuan Yang3, Huadong Ma1,2\n1Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia\n2Beijing University of Posts and Telecommunications\n3University of Rochester\n{lvchangsheng, qms, mhd}@bupt.edu.cn; lixia@bupt.cn; zhengyuan.yang13@gmail.com\nAbstract\nIn this paper, we propose a novel model called SGFormer,\nSemantic Graph TransFormerfor point cloud-based 3D scene\ngraph generation. The task aims to parse a point cloud-based\nscene into a semantic structural graph, with the core chal-\nlenge of modeling the complex global structure. Existing\nmethods based on graph convolutional networks (GCNs) suf-\nfer from the over-smoothing dilemma and can only prop-\nagate information from limited neighboring nodes. In con-\ntrast, SGFormer uses Transformer layers as the base build-\ning block to allow global information passing, with two types\nof newly-designed layers tailored for the 3D scene graph\ngeneration task. Specifically, we introduce the graph embed-\nding layer to best utilize the global information in graph\nedges while maintaining comparable computation costs. Fur-\nthermore, we propose the semantic injection layer to lever-\nage linguistic knowledge from large-scale language model\n(i.e., ChatGPT), to enhance objects‚Äô visual features. We\nbenchmark our SGFormer on the established 3DSSG dataset\nand achieve a 40.94% absolute improvement in relation-\nship prediction‚Äôs R@50 and an 88.36% boost on the sub-\nset with complex scenes over the state-of-the-art. Our anal-\nyses further show SGFormer‚Äôs superiority in the long-tail\nand zero-shot scenarios. Our source code is available at\nhttps://github.com/Andy20178/SGFormer.\nIntroduction\nUnderstanding a 3D scene is the essence of human vision,\nrequiring accurate recognition of each object‚Äôs category and\nlocalization, as well as the complex intrinsic structural and\nsemantic relationship. Conventional 3D scene understand-\ning tasks, such as 3D semantic segmentation (Qi et al. 2017;\nEngelmann et al. 2017; Rethage et al. 2018; Hou, Dai, and\nNie√üner 2019; Vu et al. 2022), object detection and classifi-\ncation (Qi et al. 2017; Zhao et al. 2019; Zheng et al. 2022),\nfocus primarily on the single object localization and recog-\nnition but miss higher-order object relationship informa-\ntion, making it challenging to deploy such 3D understanding\nmodels in practice. To close this gap, the 3D Scene Graph\nGeneration task (Wald et al. 2020) is recently proposed, as a\nvisually-grounded graph over the detected object instances\nwith edges depicting their pairwise relationships (Yu et al.\n*Corresponding author.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nSemantic Graph \nTransformer\n(a) Input 3D Scene (b) Local and Global aggregation\nvs\nwallsupported by\ncabinet\ncabinetattached to\nplant box\nclose by\nlamp\nhanging on\nsink\nbuild in\n(c) GCN Result\nclose by\n(d) Ours Result\nwallsupported by\ncabinet\ncabinetsupported by\nkettle heater\nclose by\ntowel\nsupported by\nsink\nbuild in\nclose by\nFigure 1: An example of 3D scene graph generation (SGG).\n(a) an input 3D scene, (b) global aggregation using Trans-\nformer on the right versus local aggregation using GCN on\nthe left, (c) result using EdgeGCN (Zhang et al. 2021a),\nand (d) result using our SGFormer. The incorrect results are\nhighlighted in red font, whereas the correct ones are shown\nin black.\n2017) (as shown in Figure 1). The significant importance of\n3D scene graph is evident as it has already been applied in a\nwide range of 3D vision tasks, such as VR/AR (Tahara et al.\n2020), 3D scene synthesis (Dhamo et al. 2021), and robot\nnavigation (Gomez et al. 2020).\nOne core challenge for 3D scene graph generation is to\naccurately predict complex 3D scene structures from sparse\nand noisy point cloud inputs. Most existing methods (Wald\net al. 2020; Zhang et al. 2021a,b; Chen et al. 2022) are built\nbased on Graph Convolution Networks (GCNs) (Kipf and\nWelling 2016), where nodes represent objects in the scene\nand edges indicate relationships between objects. Despite\nthe recent success, GCN suffers from its inherent limitation\nof the over-smoothing dilemma (Li, Han, and Wu 2018).\nThat is, GCN is effective in modeling neighboring nodes\nwith a controlled number of layers but could struggle with\nlearning the global structure and high-order relationships\nof the entire scene. As shown in Figure 1, the GCN-based\nmethod deteriorates in relationship accuracy when the scene\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n4035\nbecomes more complicated.\nIncreasing the GCN layers or node radius could be one\nattempt to improve the global modeling capability, but of-\nten makes the network difficult to converge and results in\nworse performance, as discussed in previous studies (Ying\net al. 2021) and our later analyses. In contrast, we explore\na more fundamental change, i.e., replacing the base build-\ning block from GCN to Transformer layers (Vaswani et al.\n2017), which has shown a strong global context modeling\ncapability and therefore could overcome the limitations of\nGCNs.\nWith the above analysis, we propose a Transformer-based\nmodel called Semantic Graph TransFormer (SGFormer), to\ngenerate a scene graph for a 3D point clouds scene. SG-\nFormer takes both node and edge proposal features obtained\nfrom PointNet (Qi et al. 2017) as inputs and stacks Trans-\nformer layers to model the higher-order inter-object relation-\nships. The node and edge classifiers then predict each node\nand edge‚Äôs categories. However, one technical challenge is\nthe number of edges will grow quadratically to the num-\nber of nodes, leading to a quadratically growing input se-\nquence length to the Transformer. To address this challenge,\nwe propose the Graph Embedding Layer that injects the edge\ninformation only to the relevant nodes via edge-aware self-\nattention. In this way, SGFormer preserves the global con-\ntext with a comparable computation cost.\nFurthermore, we propose to exploit external semantic\nknowledge via the Semantic Injection Layer. Previous GCN-\nbased methods (Zhang et al. 2021b; Chen et al. 2022) have\nshown the benefit of learning categorical objects and rela-\ntionships semantic knowledge embeddings. However, these\nworks need to train an extra module to obtain such knowl-\nedge. Moreover, the prior knowledge learned from the train-\ning set may only work well for common categories in train-\ning set distribution but may fail to help with the long-tail\nobject and relationship categories. Alternatively, we design\na Semantic Injection Layer to enhance the visual features of\nobject nodes. Specifically, we expand the objects‚Äô category\nlabels to detailed descriptions generated from the pre-trained\nlarge-scale language model (LLM) (Brown et al. 2020; Tou-\nvron et al. 2023), and extract the text features as semantic\nknowledge embeddings. Note that our approach requires no\nextra module and significantly improves SGFormer‚Äôs perfor-\nmance in long-tail and zero-shot scenarios.\nOur main contributions can be summarized as follows:\n(1) We propose a Semantic Graph Transformer (SG-\nFormer) for 3D scene graph generation, which captures\nglobal dependencies between objects and models inter-\nobject relationships with the Graph Embedding Layer.\nTo the best of our knowledge, SGFormer is the first\nTransformer-based framework for this specific task.\n(2) We introduce a Semantic Injection Layer to enhance\nobject features with natural language knowledge and show\nits effectiveness in the long-tail and zero-shot scenarios.\n(3) We benchmark our SGFormer on the established\n3DSSG dataset (Wald et al. 2020), with 40.94% absolute\nimprovements over the state-of-the-art GCN-based method.\nMore importantly, we achieved an 88.36% improvement on\nthe subset with complex scenes.\nRelated Work\nScene Graph Generation. Scene graph was first intro-\nduced into image retrieval (Johnson et al. 2015) to capture\nmore semantic information about objects and their inter-\nrelationships. Afterward, the first large-scale dataset, Visual\nGenome (Krishna et al. 2017), with scene graph annota-\ntions on 2D images gave rise to a line of deep learning-\nbased advances (Xu et al. 2017; Li et al. 2017; Herzig et al.\n2018; Yang et al. 2018; Zellers et al. 2018; Qi et al. 2019a),\nand also contributed to the research of other downstream\ntasks (Qi et al. 2018, 2019a,b, 2020, 2021a,b). Nowadays,\nto understand the complex 3D indoor structure, 3DSSG\ndataset (Wald et al. 2020) was first presented to tackle 3D\nscene graph (Zhang et al. 2021a,b) from point clouds. The\nEdgeGCN (Zhang et al. 2021a) introduced an edge-oriented\nGCN to learn a pair of twinning interactions between nodes\nand edges, and (Zhang et al. 2021b) divide the generation\ntask into two stages, the prior knowledge learning stage and\nthe scene graph generation with a prior knowledge inter-\nvention. However, the aforementioned methods fall short in\nmodeling the global-level structure of scenes, and increasing\nthe number of GCN layers easily results in over-smoothing\nproblems. We address the task differently by utilizing the\nedge-aware self-attention in the Graph Embedding Layer to\ncapture adaptable global representation among nodes.\nKnowledge Representation. There has been growing inter-\nest in improving data-driven models with external seman-\ntic knowledge in natural language processing (Yang et al.\n2018; Hinton et al. 2015) and computer vision (Deng et al.\n2014; Li, Su, and Zhu 2017; Qi et al. 2019c; Lv et al. 2023),\nby incorporating semantic cues, (e.g., language priors of\nobject class names) into visual contents (e.g., object pro-\nposals) could significantly improve the generation capabil-\nity (Pennington, Socher, and Manning 2014; Lu et al. 2016;\nSpeer, Chin, and Havasi 2017; Liang et al. 2018). Early\nmethod (Zellers et al. 2018) uses statistical co-occurrence as\nextra knowledge for scene graph generation by introducing\na pre-computed bias into the final prediction. In this work,\nwe propose a simple yet effective Semantic Injection Layer\nto enhance visual features with semantic knowledge getting\nfrom LLMs-expanded description by cross-attention mech-\nanism.\nProposed Approach\nOverview\nProblem Definition. We define a 3D scene graph as G =\n(V, E), which describes the category of each object and\nthe corresponding semantic inter-object relationships. In the\ngraph, nodes V refer to the object set, while edges E mean\ninter-object relationships. Meanwhile, we define the output\nobject class labels as O = {o1, ..., oN }, oi ‚àà Cnode, where\nCnode represents all possible object categories,N is the num-\nber of nodes. And the set of inter-object relationships can be\ndefined as R = {r1, ..., rM }, rj ‚àà Cedge, where Cedge is the\nset of all pre-defined relationships classes, M denotes the\nnumber of valid edges.\nAs illustrated in Figure 2, the overall framework of our\nproposed method follows a typical Transformer architecture\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n4036\nPointNet\nInput 3D scene Semantic Graph Transformer\nGraph\nEmbeddingLayer\nSemantic\nInjectionLayer\nGraph\nEmbeddingLayer\nPrediction\nHead\nL-2√ó\nText\nEncoder\nSemantic Knowledge\nEmbeddings\nPrompt: Describe the [object] in the scene\nChatGPT\nIt is standing on the floor, positioned against the \nwall. The bed consists of a mattress and a frame, \nand it may have a blanket, pillows, and clothes\nlying on top of it. It is in spatial proximity to the \nnightstand, where a lamp or other objects might \nbe placed. \nPrompt\nTemplate \nEdge-aware Self-Attention\ncurtain ceiling\nattached to\nwall\nattached to\nfloor\nbed\nstanding\non\nblanket\npillow\npillow armchair\nright of\nstanding on\nbehind \nlying on\nEdge\nFeature\nNode\nFeature\nOutput semantic 3D scene graph\nEùê∏!\n\"\nNode Feature\nEdge Feature\nQùëâ!\n\"\nùëâ#\n\"\nV\nK\nScaling ‚Ä¢ Softmax\nLinear\nMatrix multiplication\n‚Ä¢ Hadamard product\nùëâ\"$%\nùê∏\"$%\nLLMs-expanded Description\n√óùêª\nHeads\nFigure 2: Overview pipeline of our proposed SGFormer. We leverage PointNet to initialize the node and edge features in the 3D\nscene and use LLMs ( i.e., ChatGPT) to enrich the object description text of the dataset as semantic knowledge. The SGFormer\nmain consists of two carefully designed components: a Graph Embedding Layer and a Semantic Injection Layer.\nbut consists of two carefully designed components: Graph\nEmbedding Layer (GEL) encodes the global-level features\nof nodes simultaneously with edge-aware self-attention; Se-\nmantic Injection Layer (SIL) extracts the objects‚Äô semantic\nknowledge from the LLMs-expanded descriptions and en-\nhances the node features with knowledge via cross-attention\nmechanism. For our SGFormer, we empirically set theL lay-\ners, i.e. a GEL layer and a SIL layer followed by another\nL ‚àí 2 layers of GEL. We detail the analyses on L and layer\nsetup in ablation studies.\nScene Graph Initialization\nDifferent from 3D instance segmentation (Vu et al. 2022),\nwe study the higher-order problem of object relationship\nprediction in the scene. Note that we use point cloud data\nwith real instance indexes but without category labels. We\nfollow (Zhang et al. 2021a) adopt the PointNet (Qi et al.\n2017) backbone to capture the point-wise feature XP ‚àà\nRP√óCpoint from the input point cloud P ‚àà RP√óCinput that\nforms 3D scene S, where Cpoint and Cinput denote the chan-\nnel numbers of point clouds and their extracted point-wise\nfeature, respectively, and P denote the number of sampling\npoints 1.\nNode and edge feature generation. Following (Zhang\net al. 2021a), for the input sceneS, we use the average pool-\ning function (Qi et al. 2017) to aggregate the points in XP\nwith the same instance index to obtain the corresponding\n1In our experiments, we set P = 4096.\nnode visual features XV ‚àà RN√ódnode , where N indicates the\nnumber of instances in scene S, and dnode means node fea-\nture dimensions.\nUnlike (Wald et al. 2020) uses independent PointNet to\nextract inter-object relationships, we assume that all objects\nare connected to each other, and thus we can obtain multi-\ndimensional edge features based on node features. For each\nXE(i,j) ‚àà Rdedge , it denotes the feature for the edge E(i,j)\nthat connects two points from subject Vi toward object Vj,\nand the features can be initialized as the following formula\nusing the concatenation scheme introduced in (Wang et al.\n2019):\nXE(i,j) = (XVi ‚à• (XVj ‚àí XVi)), (1)\nwhere ‚à• denotes the concatenation operation.\nGraph Embedding Layer\nFollowing (Dwivedi and Bresson 2021), we feed the input\nnode and edge features into the Graph Embedding Layer.\nThe input node features XVi ‚àà Rdnode and edge features\nXE(i,j) ‚àà Rdedge are passed via linear projections to embed\ninto d-dimensional hidden features V0\ni and E0\n(i,j), respec-\ntively.\nMulti-Head Edge-aware Self-Attention is proposed in\nthe layer for the message passing in the graph, which\nis different from the conventional self-attention described\nin (Vaswani et al. 2017), as shown in Figure 2. For the l-th\nlayer, we use the node feature Vl\ni as query and the neigh-\nboring node features Vl\nj (j = 1, 2, ¬∑¬∑¬∑ , N) as keys and val-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n4037\nThe objects included in the \ndataset are:wall, ..., and\nrelationships among them are:\nsupported by...\nDescribe the [sofa] in the scene\nText\nEncoder\nQKV\nCross Attention\nSemantic Injection Layer\nLearnable Embeddings\nSemantic Embeddings\nNode Features\nEnhanced Features\nUsing these objects and \nrelationships to respond to the \nfollowing questions.\nOf course !\nIt is standing on the floor,\nagainst a wall or near a table...\nFigure 3: Illustration of the prompt templates to LLMs and\nthe proposed Semantic Injection Layer SIL. The enhanced\nfeatures obtained through SIL will serve as the subsequent\nlayer‚Äôs node features.\nues. The node features Vl+1\ni in the l + 1layer are calcu-\nlated as the concatenation of the self-attention results from\nH heads. Besides, the updated edge featureEl+1\n(i,j) can be cal-\nculated by concatenating the edge-aware self-attention maps\nMl,h\nij ‚àà Rdh, 1 ‚â§ h ‚â§ H, h denotes the number of atten-\ntion heads and dh denotes the dimension corresponding to\neach head. Note that, Ml,h\nij in our method is a vector instead\nof a scalar as in the standard Transformer. The information\npropagating from node Vj to Vi can be formulated as the\nfollows in the layer l:\nÀÜVl+1\ni = Ol\nv ¬∑\nÔ£Æ\nÔ£∞‚à•H\nh=1\nÔ£´\nÔ£≠\nNX\nj\nMl,h\nij ‚ó¶ Wl,h\nV Vl,h\nj\nÔ£∂\nÔ£∏\nÔ£π\nÔ£ª, (2)\nÀÜEl+1\n(i,j) = Ol\ne ¬∑\nh\n‚à•H\nh=1 Ml,h\nij\ni\n, (3)\nwhere\nMl,h\nij = softmaxj\n\u0010\nÀÜMl,h\nij\n\u0011\n, (4)\nÀÜMl,h\nij =\n \n(Wl,h\nQ Vl,h\ni )T ¬∑ Wl,h\nK Vl,h\nj\n‚àödh\n!\n¬∑ Wl,h\nE El,h\ni,j , (5)\nwhere Wl,h\nQ , Wl,h\nK , Wl,h\nV , Wl,h\nE ‚àà Rdh√ódh, Ol\nv, Ol\ne ‚àà\nRd√ód, Ol\nv, Ol\ne are the weights of linear layers,‚ó¶ denotes the\nHadamard product, ¬∑ denotes the matrix multiplication,and\n‚à• denotes the concatenation operation.\nFollowing (Dwivedi and Bresson 2021), in order to keep\nthe numerical stability, the outputs after taking exponents\nof the terms inside softmax will be clamped to a value in\n[‚àí5, 5]. The outputs ÀÜVl+1\ni and ÀÜEl+1\nij are then separately\npassed into feed-forward network (FFN) preceded and suc-\nceeded by residual connections and normalization layers.\nMore detail about FFN please refer to (Vaswani et al. 2017).\nSemantic Injection Layer\nTo fully exploit semantic knowledge within the text modal-\nity, we design a new prompt template to obtain a de-\ntailed description of the object using the LLMs, i.e., Chat-\nGPT (Brown et al. 2020). After extracting textual features\nfrom the descriptions using a text encoder, i.e., CLIP (Rad-\nford et al. 2021), we fuse them together with visual features\nby the Cross-Attention mechanism.\nPrompt Template. The output of LLMs provides fine-\ngrained descriptions of each object. To align such descrip-\ntions with the content of the given dataset, we design\nprompt templates that include objects and relationships in\nthe dataset. The prompt template is formulated as ‚ÄúWithin\nthe 3D indoor scene dataset, the objects included in it are:\n[object], and the relationships among them include [rela-\ntionship]. ‚Äù, where [object] and [relationship] represent all\nobject and relationship categories in the dataset. For differ-\nent objects, we design a prompt as follows: ‚ÄúDescribe the\n[object] in the scene‚Äù. For example, we input‚ÄúDescribe the\n[floor] in the scene‚Äù, obtaining ChatGPT response as:‚ÄúThe\nfloor in this 3D scene supporting all the other objects in the\nroom. It lies beneath the bed, chair, sofa, table, and other\nfurniture. The floor is in spatial proximity to the wall, door,\nand window. It is covered by a rug or carpet, and there might\nbe a towel or clothes lying on it. ‚Äù\nAs shown in Figure 3, we obtain descriptions of all objects\nin the dataset using the aforementioned prompts and extract\nthe semantic embeddings ZK ‚àà RK√ódemb using a frozen\npre-trained text encoder, where K is the number of object\nclasses. And for the unknown object categories that don‚Äôt\nhave corresponding descriptions, we set learnable embed-\ndings ZU ‚àà RU√ódemb to represent their text features, where\nK + U = |Cnode| is the category number and demb denotes\nthe text features dimension. The semantic knowledge em-\nbeddings set can be formed as follows:\nZ = Concat(ZK, ZU ). (6)\nwhere Concat refers to concatenation in the column dimen-\nsion, with the Z ‚àà R|Cnode|√ódemb means the semantic knowl-\nedge embeddings of all object categories.\nCross-Attention is designed in the layer to aggregate the\nvisual features and the semantic knowledge embeddings fol-\nlowed by two feed-forward networks. Since the node visual\nfeatures and the semantic knowledge embeddings are in dif-\nferent latent spaces, we transform node features through a\nlayer of a feed-forward network before fusing them. Then\nfor the node i in l-th layer, we use the node feature Vl\ni as\nquery and semantic knowledge embeddings Zi as keys and\nvalues. We calculate the cross-attention scores Si between\nVl\ni and each semantic knowledge embedding Zj as follows:\nHl\ni = Norm\n\u0000\nFFNH\n\u0000\nVl\ni\n\u0001\n+ Vl\ni\n\u0001\n, (7)\nSl\ni,j = softmaxj\n\u0012(WhHl\ni)T ¬∑ WzZj\n‚àö\nd\n\u0013\n, (8)\nwhere Wh ‚àà Rd√ód, and Wz ‚àà Rd√ódemb . Therefore the en-\nhanced node features with semantic knowledge injection can\nbe calculated as follows:\nÀÜUl\ni =\nX\nj\nSl\ni,j ¬∑ WuZl\nj, (9)\nUl\ni = Norm\n\u0010\nFFNU\n\u0010\nÀÜUl\ni\n\u0011\n+ ÀÜUl\ni\n\u0011\n, (10)\nwhere Wu ‚àà Rd√ódemb .\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n4038\nTraining and Inference\nAs shown in Figure 2, we first initialize the nodes and edges\nfeatures and extract embeddings from the LLMs-expanded\ndescription of each object to get semantic knowledge em-\nbeddings. Then Graph Embedding Layer is used to encode\nand propagate the global node-edge information, while the\nSemantic Injection Layer is utilized after the first Graph Em-\nbedding Layer to inject the semantic knowledge into the\nnode features. The final result is output from the last Graph\nEmbedding Layer, by the prediction head consisting of two\nMLPs named NodeMLP and EdgeMLP to recognize objects\nand their structural relationships, respectively.\nSpecifically, we adopt the focal loss (Lin et al. 2017) for\nobjects and predicates classification due to the data distri-\nbution imbalance problem (Zhang et al. 2021b; Wald et al.\n2020). We convert object labels into one-hot label and de-\nnote prediction logits p as the model‚Äôs estimated probability\nfor the class where the label y = 1. Here we define pt:\npt =\n\u001ap, if y = 1\n1 ‚àí p, otherwise. (11)\nHence we formulate the object classification focal loss as\nthe following:\nLobj\nfocal = Œ± (1 ‚àí pt)Œ≥ log (pt) , (12)\nwhere Œ± is the normalized inverse frequency of objects, and\nŒ≥ is a hyper-parameter. Similarly, we can formulateLedge\nfocal as\npredicate classification focal loss.\nFurthermore, to align the injected semantic knowledge\nembeddings closely with the corresponding object‚Äôs visual\nfeatures, we employ the same ground truth in object classi-\nfication and denote the score S in Eq. (8) as the estimated\nprobability. Upon deriving St from Eq. (11), we can define\nthe semantic similarity focal loss for training the Semantic\nInjection Layer as the following:\nLSIL = Œ± (1 ‚àí St)Œ≥ log (St) . (13)\nTherefore, the final loss LSG can be calculated as the sum\nof the aforementioned three loss functions:\nLSG = Lobj\nfocal + Ledge\nfocal + LSIL. (14)\nExperiments\nExperimental Settings\n3DSSG Dataset (Wald et al. 2020). 3DSSG provides an-\nnotated 3D semantic scene graphs for 3RScan (Wald et al.\n2019). We follow their RIO27 annotation to evaluate 27\nclass objects and 16 class relationships in our experiments.\nFor the fair comparison, we follow the same experimental\nsettings in (Zhang et al. 2021a) and split the dataset into\n1084/113/113 scenes as train/validation/test sets, respec-\ntively.\nMetrics. We evaluate our model in terms of object,\npredicate, and relationship classification. We adopt Re-\ncall@K (Lu et al. 2016) for object classification, computing\nthe macro-F1 score and Recall@K for predicate classifica-\ntion due to the imbalanced distribution. Besides, we multiply\nthe classification scores of each subject, predicate, and ob-\nject, and then useRecall@K to evaluate the obtained ordered\nlist of relationship classification. Regarding the long-tailed\nand zero-shot tasks, we use mean Recall@K(mR@K) (Tang\net al. 2020) for object and predicate classification, andZero-\nShot Recall@K (Lu et al. 2016) for never been observed re-\nlationships evaluation.\nCompared Methods. We compare our approach with the\nfollowing methods on 3DSSG benchmarks: only using\nPointNet (Qi et al. 2017), SGPN (Wald et al. 2020), GloRePC\nwith the point cloud (Ma et al. 2020), GloRe SG with the\nscene graph (Chen et al. 2019), and Edge-GCN (Zhang et al.\n2021a) 2.\nImplementation Details\nWe implement our model based on Pytorch (Paszke et al.\n2019) on a single NVIDIA RTX 3090 GPU. Similar to\nprior works in 3D scene graph generation (Wald et al. 2020;\nZhang et al. 2021a), we choose PointNet (Qi et al. 2017)\nas the backbone. In the Semantic Injection Layer, we use\nthe aforementioned command template to obtain a descrip-\ntion of each object, with the category ‚Äúobjects‚Äù employing a\nlearnable embedding. For the text encoder, we use CLIP and\nset the demb = 512. We set the default layer numberL to be\n12, consisting of a pair of GEL and SIL layers followed by\nL ‚àí 2 = 10extra GEL layers.\nResults\nQuantitative Results. We report the quantitative perfor-\nmance of our proposed model compared with other ex-\nisting methods in Table 1. For PointNet alone, we only\nuse PointNet without any reasoning model for evaluation.\nSPGN (Wald et al. 2020) adopts the edges between nodes\nas a kind of node to conduct message propagation with\nGCN. As shown in Table 1, using GCN for scene graph\ngeneration could improve the object classification but harm\nthe predicate or relationship classification, which confirms\nthe empirical findings reported in (Wald et al. 2020; Zhang\net al. 2021b) about the over-smoothing issue caused by\nmulti-layer GCNs. Instead, our model captures the global\nscene structure with the Transformer architecture, alleviat-\ning the over-smoothing in multi-layer GCN and surpassing\nthe state-of-the-art Edge-GCN (Zhang et al. 2021a). Further-\nmore, compared with the global-level model GloRe SG, our\nproposed model obtains better results by utilizing the infor-\nmation passing between inter-object relationships. Attribut-\ning to the introduced Semantic Injection Layer, our model\nachieves the best result in terms of all evaluation metrics.\nQualitative Results. Figures 4 (a) and (b) illustrate qualita-\ntive results comparing the state-of-the-art method with our\nSGFormer, demonstrating significant advancements in both\nnode and edge prediction. In Figure 4 (b), a scenario is pre-\nsented where ‚Äúsink‚Äù and ‚Äúobject‚Äù have only one neighbor,\nleading to a performance drop for EdgeGCN. In contrast,\nour SGFormer leverages global information effectively, ac-\ncurately predicting the labels of these nodes. Furthermore,\n2In all experiments, the settings of the above-mentioned meth-\nods are adopted from the corresponding papers.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n4039\nObject Class Prediction Predicate Class Prediction Relationship Prediction\nGraph Reasoning Approach\nR@5 R@10 F1@3 F1@5 R@50 R@100\n+ SGPN (Wald et al. 2020) 89.61 96.98 63.38 77.79 32.45 41.65\n+ GloRePC (Ma et al. 2020) 84.06 95.17 69.23 80.01 31.87 42.21\n+ GloReSG (Chen et al. 2019) 85.27 96.62 72.57 83.42 29.58 38.64\n+ EdgeGCN‚àó (Zhang et al. 2021a) 64.46 84.88 33.34 35.90 12.19 19.69\n+ EdgeGCN (Zhang et al. 2021a) 90.70 97.58 78.88 90.86 39.91 48.68\n+ Ours w/o SIL 92.71 97.67 76.64 77.82 50.67 57.50\n+ Ours Full 96.41 98.48 85.12 91.58 56.25 60.67\nTable 1: Comparisons of our model and existing state-of-the-art methods on 3DSSG (Wald et al. 2020). EdgeGCN ‚àó denotes\naggregates information with all nodes, while EdgeGCN aggregates information only with nodes that have a known relationship.\nOurs w/o SIL and Ours Full denote our model without the Semantic Injection Layer and our full model, respectively. The best\nperformances are shown in bold.\nGCN Result\nobjectceiling\nchair\nwall ceiling\nwall standing \non\nattached \nto\nsupported\nby\nstanding \nonstanding \non\ncounter\nclose\nby\nlying on\nsupported by\n(a) (b)\nspatial \nproximity\nwall\nstanding on\nOurs Result\nstanding \non\nsofa\nstanding \non\nchair\nsofa\nsupported \nby\ntable\nstanding on\nclose by\nfloor\nfloor\nconnected \nto\nwall\nstanding on\nstanding \non\nchair\nstanding \non\nbox\nchair\nsupported \nby\nchair\nstanding on\nclose by\nfloor\nGCN Result\nsinkwall\nobject\nwall ceiling\ncabinetsupported \nby\nsupported\nby\nstanding \nonstanding \non\ncounter\nclose by\nbuild in\nsupported by\nfloor\nOurs ResultGCN Result\nsupported \nby\nFigure 4: The qualitative results of our model. Given a 3D scene with class-agnostic instance segmentation labels, our SGFormer\ninfers a semantic graph G from the point cloud. For visualization purposes, misclassified object or relationship predictions are\nindicated in red, while the correct ones are shown in black with the GT value omitted.\nModel Edge Layer Node R@1 Edge R@1\nOurs full A √ó 3 68.34 88.54\nOurs full B ‚úì 3 71.07 91.58\nOurs full C √ó 6 69.78 89.44\nOurs full D ‚úì 6 70.67 90.89\nOurs full E √ó 9 71.20 89.85\nOurs full F ‚úì 9 72.50 91.86\nOurs full G √ó 12 72.53 93.67\nOurs full H ‚úì 12 75.33 96.59\nTable 2: Ablation study on edge feature and layer numbers.\nEdgeGCN exhibits a cascade of errors when reaching an er-\nror threshold, while SGFormer maintains robust results in\nsimilar situations. These findings emphasize the importance\nof global information aggregation in SGFormer for enhanc-\ning object and relationship recognition in complex scenes.\nAdditionally, when dealing with objects with similar appear-\nances like ‚Äúcabinet‚Äù and ‚Äúwall‚Äù, EdgeGCN struggles with\nclassification based solely on visual cues. However, our ap-\nproach excels in avoiding such errors by incorporating se-\nmantic knowledge, showcasing the effectiveness of our SIL\nin providing crucial semantic assistance.\nNode EdgeModel\nR@1 mR@5 R@1 mR@3\nw/o SIL 68.80 84.16 91.07 53.77\nSILText 65.51 83.67 87.21 53.75\nSILBasic prompt 71.62 88.59 91.70 58.50\nSILGPT w/o template 73.56 90.50 93.47 60.33\nSILGPT w/ template 75.33 92.75 96.59 65.67\nTable 3: Ablation study for different semantic knowledge.\nAblation Studies\nGraph Embedding Layer (GEL). In Table 2, we design\nvariants of our models to examine the impact of utilizing\nedge features and the varying layer numbers. Specifically,\nby comparing the performance of G and H, which have\nthe same depth but differ in node updates, we can observe\nthat incorporating edge features can enhance the model‚Äôs\npredictive capabilities. Additionally, we find that increasing\nthe layer number effectively improves performance, as ev-\nidenced by comparing B, D, F, and H, demonstrating the\nGEL can avoid the over-smoothing problem commonly en-\ncountered by GCN when attempting to stack deeper.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n4040\nLatent Features\nSemantic Knowledge Embeddings\n(a)The latent space \nof w/o SIL\n(b) The latent space \nof SIL\t\"#$\t%/\t'()*+,'(\nFigure 5: The t-SNE visualization of objects latent space of\nthe w/o SIL(a) and SILGPT w/ template(b).\n36.55 35.80 30.09 25.78\n52.09 53.73 46.99 48.56\n42.52 50.08 56.16\n88.36\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n1-53 53-89 89-170 >170\nRelative Gain (%)\nRelationship Triplet \nAccuracy(%)\nScene Size\nRelationship Accuracy Analysis\nEdegGCN SGFormer Relative Gain\nFigure 6: Performance comparison of SGFormer and\nEdgeGCN w.r.t relationship classification.\nSemantic Injection Layer (SIL). We explore various ways\nto acquire semantic knowledge. To be specific, we directly\nused object category labels, utilized the basic prompt ‚ÄúA\nphoto of [object]‚Äù, and obtained descriptions without adding\nthe designed prompt template, denoted as SILText, SILPrompt,\nand SILGPT w/o template, respectively. The results of our exper-\niments are shown in Table 3. Notably, directly extracting\nword embeddings from category labels resulted in inferior\nperformance compared to SGFormer without SIL and SIL\nwith the basic prompt. Furthermore, we also find that the in-\njected knowledge from the descriptions with adding the pro-\nposed prompt templates can help SGFormer achieve the best\nperformance than a simple prompt and descriptions with-\nout adding the prompt templates. Besides, Figure 5 demon-\nstrates the latent features and we can see SGFormer with\nSIL using LLMs-expanded description (SIL GPT w/ template)\nachieves a more distinguishable latent space compared with\nthe SGFormer without SIL (w/o SIL), demonstrating that the\nSIL can enlarge the distance between different object cate-\ngories, thereby improving object classification accuracy.\nHow Does SGFormer Help?\nScene size analysis. Defining scene complexity as the sum-\nmation of the number of nodes and relationships in the\nwhole scene, we compare the performance of our approach\nwith EdgeGCN in varying scene scales. As shown in Fig-\nure 6, our SGFormer exhibits a growing relative gain along\nwith the heightened scene complexity.\nLong-tail issue. The long-tail phenomenon is very com-\nmon yet challenging in scene graph generation, where the\nmodel usually misleads an uncommon or rare category in\n-0.1\n0.2\n0.5\n0\n10\n20\nwallchaircabi‚Ä¶tablefloorwin‚Ä¶plantlampclot‚Ä¶sink cou‚Ä¶tv nigh‚Ä¶bath‚Ä¶\nImprovement(%)\nOccurrence\nFrequency( %)\nObject Accuracy Improvement Analysis\nOccurrence Frequency Improvement Trendline\nwallchairbox\npillowtabledoorplantsofasink bed tv ‚Ä¶\nFigure 7: Illustration of objects‚Äô per-label accuracy improve-\nment with occurrence frequency in the training set.\n010203040\n50\nclothes\nlving on\ntable\ncabinet\nsupported\nby table\ncabinet\nsupported\nby table\ntoilet\nspatial\nproximity\ncurtain\nwindow\nclose by\nbox\nplant\nstanding\nin cabinet\nR@50\nZero-shot Triplets Accuracy Analysis\nEdgeGCN SGFormer\nFigure 8: The R@50 results of EdgeGCN and our SGFormer\nin terms of the zero-shot relationships\nthe dataset as a common label. As shown in Figure 7, the\noccurrence frequency of each label in the train set is unbal-\nanced. In the Table 3, SIL GPT w/ template achieves the best re-\nsult, which improves 10.21% and 22.13% than baseline w.r.t\nNode mR@5 and Edge mR@3, indicating that our proposed\nSIL can exploit the emergent ability of LLM to improve the\npredictions performance in marginally sampled object and\npredicate categories, and successfully alleviate the long-tail\nchallenge. Figure 7 shows the improvement is increasingly\ngreater for very ‚Äútail‚Äù items. The analysis of predicates can\nyield the same conclusions.\nZero-shot scenario. In Figure 8. We can observe that our\nproposed model can improve by a large margin compared\nwith EdgeGCN (Zhang et al. 2021a), such as <cabinet-\nsupported by-table> and <window-close by-box>. These\nfindings consistently demonstrate the efficacy and superior-\nity of the proposed GEL and SIL in our SGFormer. How-\never, both methods perform poorly in <plant-standing in-\ncabinet> due to this relationship being uncommon in real\nlife, and more training data can resolve this case.\nConclusion\nIn this paper, we proposed a novel Semantic Graph Trans-\nformer model (i.e., SGFormer) for 3D scene graph gen-\neration, including Graph Embedding Layer to capture the\nglobal-level structure and Semantic Injection Layer to en-\nhance the objects‚Äô visual features with LLMs‚Äô powerful se-\nmantic knowledge. Extensive experiments on the 3DSSG\nbenchmark demonstrate the effectiveness and state-of-the-\nart performance of our proposed model. Notably, our SG-\nFormer performs exceptionally well on complicated scenes,\nand under challenging long-tail and zero-shot scenarios.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n4041\nAcknowledgements\nThis work was partly supported by the Funds for Creative\nResearch Groups of China under Grant 61921003, the Na-\ntional Natural Science Foundation of China under Grant\n62202063, the Young Elite Scientists Sponsorship Program\nby China Association for Science and Technology (CAST)\nunder Grant 2021QNRC001, the 111 Project under Grant\nB18008, the Open Project Program of State Key Laboratory\nof Virtual Reality Technology and Systems, Beihang Uni-\nversity (No.VRLAB2022C01).\nReferences\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nAdvances in Neural Information Processing Systems, 33:\n1877‚Äì1901.\nChen, L.; Lu, J.; Cai, Y .; Wang, C.; and He, G. 2022. Ex-\nploring Contextual Relationships in 3D Cloud Points by Se-\nmantic Knowledge Mining. In Computer Graphics Forum,\nvolume 41, 75‚Äì86. Wiley Online Library.\nChen, Y .; Rohrbach, M.; Yan, Z.; Shuicheng, Y .; Feng, J.;\nand Kalantidis, Y . 2019. Graph-based global reasoning net-\nworks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 433‚Äì442.\nDeng, J.; Ding, N.; Jia, Y .; Frome, A.; Murphy, K.; Bengio,\nS.; Li, Y .; Neven, H.; and Adam, H. 2014. Large-scale object\nclassification using label relation graphs. In Proceedings of\nthe European Conference on Computer Vision, 48‚Äì64.\nDhamo, H.; Manhardt, F.; Navab, N.; and Tombari, F.\n2021. Graph-to-3d: End-to-end generation and manipula-\ntion of 3d scenes using scene graphs. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 16352‚Äì16361.\nDwivedi, V . P.; and Bresson, X. 2021. A Generalization of\nTransformer Networks to Graphs. AAAI Workshop on Deep\nLearning on Graphs: Methods and Applications.\nEngelmann, F.; Kontogianni, T.; Hermans, A.; and Leibe, B.\n2017. Exploring spatial context for 3D semantic segmenta-\ntion of point clouds. In Proceedings of the IEEE interna-\ntional conference on computer vision workshops, 716‚Äì724.\nGomez, C.; Fehr, M.; Millane, A.; Hernandez, A. C.; Nieto,\nJ.; Barber, R.; and Siegwart, R. 2020. Hybrid topological\nand 3d dense mapping through autonomous exploration for\nlarge indoor environments. In2020 IEEE International Con-\nference on Robotics and Automation, 9673‚Äì9679. IEEE.\nHerzig, R.; Raboh, M.; Chechik, G.; Berant, J.; and Glober-\nson, A. 2018. Mapping images to scene graphs with\npermutation-invariant structured prediction. Advances in\nNeural Information Processing Systems, 31.\nHinton, G.; Vinyals, O.; Dean, J.; et al. 2015. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2(7).\nHou, J.; Dai, A.; and Nie√üner, M. 2019. 3d-sis: 3d seman-\ntic instance segmentation of rgb-d scans. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 4421‚Äì4430.\nJohnson, J.; Krishna, R.; Stark, M.; Li, L.-J.; Shamma, D.;\nBernstein, M.; and Fei-Fei, L. 2015. Image retrieval using\nscene graphs. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 3668‚Äì3678.\nKipf, T. N.; and Welling, M. 2016. Semi-supervised classi-\nfication with graph convolutional networks. arXiv preprint\narXiv:1609.02907.\nKrishna, R.; Zhu, Y .; Groth, O.; Johnson, J.; Hata, K.;\nKravitz, J.; Chen, S.; Kalantidis, Y .; Li, L.-J.; Shamma,\nD. A.; et al. 2017. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. Inter-\nnational journal of computer vision, 123(1): 32‚Äì73.\nLi, G.; Su, H.; and Zhu, W. 2017. Incorporating external\nknowledge to answer open-domain visual questions with dy-\nnamic memory networks. arXiv preprint arXiv:1712.00733.\nLi, Q.; Han, Z.; and Wu, X.-M. 2018. Deeper insights into\ngraph convolutional networks for semi-supervised learning.\nIn Thirty-Second AAAI conference on Artificial Intelligence.\nLi, Y .; Ouyang, W.; Zhou, B.; Wang, K.; and Wang, X. 2017.\nScene graph generation from objects, phrases and region\ncaptions. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 1261‚Äì1270.\nLiang, K.; Guo, Y .; Chang, H.; and Chen, X. 2018. Visual re-\nlationship detection with deep structural ranking. In Thirty-\nSecond AAAI Conference on Artificial Intelligence.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll ¬¥ar, P.\n2017. Focal loss for dense object detection. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 2980‚Äì2988.\nLu, C.; Krishna, R.; Bernstein, M.; and Fei-Fei, L. 2016.\nVisual relationship detection with language priors. In Pro-\nceedings of the European Conference on Computer Vision ,\n852‚Äì869.\nLv, C.; Zhang, S.; Tian, Y .; Qi, M.; and Ma, H.\n2023. Disentangled Counterfactual Learning for Physi-\ncal Audiovisual Commonsense Reasoning. arXiv preprint\narXiv:2310.19559.\nMa, Y .; Guo, Y .; Liu, H.; Lei, Y .; and Wen, G. 2020. Global\ncontext reasoning for semantic segmentation of 3D point\nclouds. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision, 2931‚Äì2940.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in Neural Information Pro-\ncessing Systems, 32.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing, 1532‚Äì1543.\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017. Pointnet:\nDeep learning on point sets for 3d classification and seg-\nmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 652‚Äì660.\nQi, M.; Li, W.; Yang, Z.; Wang, Y .; and Luo, J. 2019a. Atten-\ntive relational networks for mapping images to scene graphs.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n4042\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 3957‚Äì3966.\nQi, M.; Qin, J.; Huang, D.; Shen, Z.; Yang, Y .; and Luo,\nJ. 2021a. Latent memory-augmented graph transformer for\nvisual storytelling. In Proceedings of the 29th ACM Inter-\nnational Conference on Multimedia, 4892‚Äì4901.\nQi, M.; Qin, J.; Li, A.; Wang, Y .; Luo, J.; and Van Gool, L.\n2018. stagnet: An attentive semantic rnn for group activity\nrecognition. In Proceedings of the European Conference on\nComputer Vision, 101‚Äì117.\nQi, M.; Qin, J.; Yang, Y .; Wang, Y .; and Luo, J. 2021b.\nSemantics-aware spatial-temporal binaries for cross-modal\nvideo retrieval. IEEE Transactions on Image Processing,\n30: 2989‚Äì3004.\nQi, M.; Wang, Y .; Li, A.; and Luo, J. 2019b. Sports video\ncaptioning via attentive motion representation and group re-\nlationship modeling. IEEE Transactions on Circuits and\nSystems for Video Technology, 30(8): 2617‚Äì2633.\nQi, M.; Wang, Y .; Li, A.; and Luo, J. 2020. STC-GAN:\nSpatio-temporally coupled generative adversarial networks\nfor predictive scene parsing. IEEE Transactions on Image\nProcessing, 29: 5420‚Äì5430.\nQi, M.; Wang, Y .; Qin, J.; and Li, A. 2019c. KE-GAN:\nKnowledge embedded generative adversarial networks for\nsemi-supervised scene parsing. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 5237‚Äì5246.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748‚Äì8763.\nRethage, D.; Wald, J.; Sturm, J.; Navab, N.; and Tombari,\nF. 2018. Fully-convolutional point networks for large-scale\npoint clouds. In Proceedings of the European Conference on\nComputer Vision, 596‚Äì611.\nSpeer, R.; Chin, J.; and Havasi, C. 2017. Conceptnet 5.5: An\nopen multilingual graph of general knowledge. In Thirty-\nfirst AAAI conference on Artificial Intelligence.\nTahara, T.; Seno, T.; Narita, G.; and Ishikawa, T. 2020. Re-\ntargetable AR: Context-aware augmented reality in indoor\nscenes based on 3D scene graph. In 2020 IEEE Interna-\ntional Symposium on Mixed and Augmented Reality Adjunct,\n249‚Äì255. IEEE.\nTang, K.; Niu, Y .; Huang, J.; Shi, J.; and Zhang, H. 2020.\nUnbiased scene graph generation from biased training. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, 3716‚Äì3725.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need.Advances in Neural Information Pro-\ncessing Systems, 30.\nVu, T.; Kim, K.; Luu, T. M.; Nguyen, T.; and Yoo, C. D.\n2022. Softgroup for 3d instance segmentation on point\nclouds. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2708‚Äì2717.\nWald, J.; Avetisyan, A.; Navab, N.; Tombari, F.; and\nNie√üner, M. 2019. RIO: 3D object instance re-localization\nin changing indoor environments. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n7658‚Äì7667.\nWald, J.; Dhamo, H.; Navab, N.; and Tombari, F. 2020.\nLearning 3d semantic scene graphs from 3d indoor recon-\nstructions. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 3961‚Äì3970.\nWang, Y .; Sun, Y .; Liu, Z.; Sarma, S. E.; Bronstein, M. M.;\nand Solomon, J. M. 2019. Dynamic graph cnn for learning\non point clouds. Acm Transactions On Graphics, 38(5): 1‚Äì\n12.\nXu, D.; Zhu, Y .; Choy, C. B.; and Fei-Fei, L. 2017. Scene\ngraph generation by iterative message passing. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, 5410‚Äì5419.\nYang, J.; Lu, J.; Lee, S.; Batra, D.; and Parikh, D. 2018.\nGraph r-cnn for scene graph generation. In Proceedings of\nthe European Conference on Computer Vision, 670‚Äì685.\nYing, C.; Cai, T.; Luo, S.; Zheng, S.; Ke, G.; He, D.; Shen,\nY .; and Liu, T.-Y . 2021. Do transformers really perform\nbadly for graph representation? Advances in Neural Infor-\nmation Processing Systems, 34: 28877‚Äì28888.\nYu, R.; Li, A.; Morariu, V . I.; and Davis, L. S. 2017. Visual\nrelationship detection with internal and external linguistic\nknowledge distillation. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, 1974‚Äì1982.\nZellers, R.; Yatskar, M.; Thomson, S.; and Choi, Y . 2018.\nNeural motifs: Scene graph parsing with global context. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 5831‚Äì5840.\nZhang, C.; Yu, J.; Song, Y .; and Cai, W. 2021a. Exploit-\ning edge-oriented reasoning for 3d point-based scene graph\nanalysis. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 9705‚Äì9715.\nZhang, S.; Hao, A.; Qin, H.; et al. 2021b. Knowledge-\ninspired 3D Scene Graph Prediction in Point Cloud. Ad-\nvances in Neural Information Processing Systems, 34:\n18620‚Äì18632.\nZhao, Y .; Birdal, T.; Deng, H.; and Tombari, F. 2019. 3D\npoint capsule networks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n1009‚Äì1018.\nZheng, Y .; Duan, Y .; Lu, J.; Zhou, J.; and Tian, Q. 2022. Hy-\nperDet3D: Learning a Scene-conditioned 3D Object Detec-\ntor. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 5585‚Äì5594.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n4043",
  "topic": "Point cloud",
  "concepts": [
    {
      "name": "Point cloud",
      "score": 0.6695809364318848
    },
    {
      "name": "Computer science",
      "score": 0.6131705045700073
    },
    {
      "name": "Graph",
      "score": 0.4927046597003937
    },
    {
      "name": "Scene graph",
      "score": 0.43832215666770935
    },
    {
      "name": "Transformer",
      "score": 0.4225195348262787
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3187170624732971
    },
    {
      "name": "Theoretical computer science",
      "score": 0.28376781940460205
    },
    {
      "name": "Engineering",
      "score": 0.09705173969268799
    },
    {
      "name": "Electrical engineering",
      "score": 0.08037349581718445
    },
    {
      "name": "Rendering (computer graphics)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I5388228",
      "name": "University of Rochester",
      "country": "US"
    }
  ]
}