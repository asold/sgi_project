{
    "title": "A Framework for Adapting Pre-Trained Language Models to Knowledge Graph Completion",
    "url": "https://openalex.org/W4385573038",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2982393567",
            "name": "Justin Lovelace",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2182727196",
            "name": "Carolyn Rose",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1533230146",
        "https://openalex.org/W2587690726",
        "https://openalex.org/W1936750108",
        "https://openalex.org/W3092984062",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2963157366",
        "https://openalex.org/W2972167903",
        "https://openalex.org/W4289761690",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2250184916",
        "https://openalex.org/W4221154018",
        "https://openalex.org/W3150807214",
        "https://openalex.org/W3179013701",
        "https://openalex.org/W3174931845",
        "https://openalex.org/W3202794254",
        "https://openalex.org/W2728059831",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W2971155257",
        "https://openalex.org/W3105816068",
        "https://openalex.org/W3155001903",
        "https://openalex.org/W2432356473",
        "https://openalex.org/W2995448904",
        "https://openalex.org/W3118062200",
        "https://openalex.org/W759515131",
        "https://openalex.org/W2995925258",
        "https://openalex.org/W3082429057",
        "https://openalex.org/W2970723181",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W205829674",
        "https://openalex.org/W3186468809",
        "https://openalex.org/W3154735894",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2997545008",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3037530970",
        "https://openalex.org/W3173551127",
        "https://openalex.org/W3125516434"
    ],
    "abstract": "Recent work has demonstrated that entity representations can be extracted from pre-trained language models to develop knowledge graph completion models that are more robust to the naturally occurring sparsity found in knowledge graphs. In this work, we conduct a comprehensive exploration of how to best extract and incorporate those embeddings into knowledge graph completion models. We explore the suitability of the extracted embeddings for direct use in entity ranking and introduce both unsupervised and supervised processing methods that can lead to improved downstream performance. We then introduce supervised embedding extraction methods that can extract more informative representations. We then synthesize our findings and develop a knowledge graph completion model that significantly outperforms recent neural models.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5937–5955\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nA Framework for Adapting Pre-Trained Language Models to Knowledge\nGraph Completion\nJustin Lovelace∗\nComputer Science Department\nCornell University\njl3353@cornell.edu\nCarolyn Penstein Rosé\nLanguage Technologies Institute\nCarnegie Mellon University\ncp3a@andrew.cmu.edu\nAbstract\nRecent work has demonstrated that entity rep-\nresentations can be extracted from pre-trained\nlanguage models to develop knowledge graph\ncompletion models that are more robust to the\nnaturally occurring sparsity found in knowl-\nedge graphs. In this work, we conduct a com-\nprehensive exploration of how to best extract\nand incorporate those embeddings into knowl-\nedge graph completion models. We explore\nthe suitability of the extracted embeddings for\ndirect use in entity ranking and introduce both\nunsupervised and supervised processing meth-\nods that can lead to improved downstream per-\nformance. We then introduce supervised em-\nbedding extraction methods that can extract\nmore informative representations. We then syn-\nthesize our findings and develop a knowledge\ngraph completion model that significantly out-\nperforms recent neural models. 1\n1 Introduction\nKnowledge graphs (KG) are structured represen-\ntations of knowledge that contain a collection of\nfactual relations between entities. KGs are valu-\nable resources with applications in different areas\nsuch as representation learning (Liu et al., 2018),\nquestion answering (Sun et al., 2019; Shen et al.,\n2019; Thirukovalluru et al., 2021), and entity link-\ning (Thai et al., 2021).\nHowever, the difficulty of curating knowledge\nat scale means that existing KGs are highly in-\ncomplete. This has led to the widespread study of\nknowledge graph completion (KGC) which aims to\ndevelop automated solutions that can suggest new\nfacts to add to the KG (Yang et al., 2015; Trouil-\nlon et al., 2016; Dettmers et al., 2018). KGC is\ntypically formulated as ranking problem where an\nincomplete fact is used as a query to retrieve enti-\nties that complete the fact.\n∗Work conducted while at Carnegie Mellon University.\n1https://github.com/justinlovelace/\nLM-KG-Completion\nRecent work has utilized pre-trained language\nmodels to develop approaches that are more robust\nto the naturally occurring sparsity within knowl-\nedge graphs. These approaches utilize textual en-\ntity descriptions to develop entity representations\nthat are less reliant on graph connectivity.\nSuch work either fine-tunes the language model\ndirectly during training to encode the entities (e.g.\nYao et al. (2019)) or extracts a set of entity embed-\ndings prior to training which can then be used to\ntrain a KGC model using standard training proce-\ndures (e.g. Lovelace et al. (2021)).\nWhile fine-tuning language models often im-\nproves downstream performance (Rogers et al.,\n2020), it increases the computational overhead of\ncomputing entity representations. As a result, stan-\ndard KGC training procedures that involve evaluat-\ning a large number of negative candidates for each\npositive instance are typically infeasible. Sampling\nonly a small set of negative candidates enables\ntraining, but can harm performance.\nApproaches that extract entity embeddings prior\nto training (Lovelace et al., 2021; Wang et al.,\n2021a) do not introduce any overhead for com-\nputing entity representations and are able to take\nadvantage of standard training protocols. However,\nsuch approaches do not utilize any supervision to\nadapt the pre-trained language model to KGC.\nWhile both lines of previous work have demon-\nstrated their approaches effectiveness at retrieving\nsparsely connected entities, they still lag behind\nKGC models that do not incorporate any textual\ninformation on standard benchmark datasets.\nIn this work, we develop a framework for adapt-\ning pre-trained language models to KGC that takes\nadvantage of the strengths of both prior lines of\nwork. We accomplish this by decoupling the entity\nrepresentations used for computing the query rep-\nresentation and the entity representations used for\nretrieval (see Figure 1).\nFor candidate ranking, we extract and cache en-\n5937\ntity representations from a pre-trained language\nmodel prior to training. We then introduce\nlightweight unsupervised and supervised embed-\nding processing techniques that improve the suit-\nability of the space for candidate retrieval without\nsacrificing the scalability necessary to use standard\nKGC training procedures. The embedding process-\ning techniques introduced in this work lead to sig-\nnificant performance improvements across datasets\nfrom diverse domains.\nThis decoupling also enables us to scalably fine-\ntune pre-trained language models to extract more\ninformative entity representations for the query.\nHowever, naively fine-tuning the language model\noverfits the knowledge graph and actually degrades\nperformance. We find that parameter-efficient fine-\ntuning methods such as prompt-tuning mitigate this\nand improve downstream performance.\nWe synthesize our findings and utilize the most\neffective candidate representation processing and\nentity extraction techniques with a recently pro-\nposed neural ranking architecture. Although we do\nnot make any modifications to the ranking architec-\nture, our representation extraction and processing\ntechniques lead to significant improvements across\nfour diverse datasets. The findings and analysis\nfrom this work provide useful guidelines for devel-\noping and utilizing effective textual entity represen-\ntations for KGC.\nThe rest of our paper is organized as follows. We\ndiscuss related work in Section 2, present a formal\ndescription of our task in Section 3, and describe\nthe datasets used in this work in Section 4. We\nintroduce unsupervised and supervised techniques\nto improve the suitability of entity embeddings for\ncandidate ranking in Section 5. We then introduce\nsupervised methods to extract more informative\nrepresentations for the query entity in Section 6\nand explore the effect of language model selection\nin Section 7. Finally, we synthesize our findings in\nSection 8 and compare against recent work on our\ndatasets. Our contributions are as follows.\n• We develop a novel framework for adapting\npre-trained language models for KGC that\nsignificantly improves performance for both\nsparsely connected and widely studied bench-\nmark datasets.\n• We demonstrate that the embeddings extracted\nfrom pre-trained language models are subopti-\nmal for entity ranking and introduce unsuper-\nvised and supervised processing techniques\nthat transform the textual embedding space to\nbe more suitable for candidate retrieval.\n• We demonstrate that parameter-efficient fine-\ntuning methods can be applied scalably to ex-\ntract more informative query entity represen-\ntations.\n2 Related Work\nYao et al. (2019) adapted a pre-trained language\nmodel to KGC by fine-tuning it for triplet clas-\nsification, i.e. predicting whether a given fact is\ntrue. However, such an approach scales poorly to\nthe widely studied ranking formulation and is not\ncompetitive with simpler approaches.\nFollow-up work has developed more scalable\nframeworks utilizing siamese encoders to inde-\npendently encode the query and candidate entities\n(Wang et al., 2021b; Li et al., 2022; Daza et al.,\n2021). While this is an improvement, it still cannot\nscale to the tens of thousands of negative candi-\ndates typically considered during training. Clouatre\net al. (2021) take a different approach and adapt the\nMLM objective to perform candidate retrieval by\naggregating the logits for a number of mask tokens,\neliminating the need to directly encode negative\ncandidate entities. Although these approaches gen-\nerally improve upon Yao et al. (2019), they still lag\nbehind simpler models on standard benchmarks.\nMalaviya et al. (2020); Lovelace et al. (2021);\nWang et al. (2021a) have taken a different approach\nand extracted entity embeddings from pre-trained\nlanguage models prior to training. This eliminates\nthe overhead of computing entity representations\nduring training, enabling the use of standard train-\ning procedures. The focus of this line of work\nhas been on developing neural ranking architec-\ntures that can effectively utilize the extracted tex-\ntual embeddings. We focus on the complementary\nquestions of how to best extract and use entity rep-\nresentations with existing neural architectures.\n3 Task Formulation\nGiven a set of entities Eand relations R, a KG can\nbe defined as a collection of entity-relation-entity\ntriplets K= {(ei,rj,ek)}⊂E×R×E where\nei,ek ∈E and rj ∈R. The aim of KGC is to\ndevelop a model that accepts a query consisting of\na head entity and a relation, (ei,rj,?), and ranks\nall candidate entities ek ∈E to resolve the query.\nAn effective KGC model should rank correct can-\ndidates more highly than incorrect candidates.\n5938\nFigure 1: Overview of our proposed framework.\nNeural KGC models embed the head entity and\nrelation and compute a query vector fθ(ei,rj) =q\nwhere fθ(·) is a neural network and ei,rj,q ∈Rd.\nScores for each candidate, ek ∈E, are computed\nas the inner product between the query vector and\nthe candidate entity embedding yk = qek⊺ where\nek ∈Rd. We follow Lovelace et al. (2021) and use\ntextual descriptors to extract the entity embeddings\nfrom pre-trained language models while learning\nrelation embeddings during training.\nWe evaluate the KGC models with standard rank-\ning metrics: Mean Reciprocal Rank (MRR), Hits at\n1 (H@1), Hits at 3 (H@3), and Hits at 10 (H@10).\nWe follow standard procedure and consider both\nforward and reverse relations and use the filtered\nevaluation setting (Dettmers et al., 2018). We vali-\ndate the significance of improvements in the MRR\nwith the paired bootstrap significance testing (Berg-\nKirkpatrick et al., 2012) and correct for multiple\nhypothesis testing with the Benjamini/Hochberg\nmethod (Benjamini and Hochberg, 1995).\n4 Datasets\nWe work with KGC datasets that cover diverse\ndomains such as commonsense, biomedical, and\nencyclopedic knowledge. For the commonsense\nKG dataset, we work with the CN-82K dataset in-\ntroduced by (Wang et al., 2021a) which is derived\nfrom ConceptNet. For the biomedical KGC dataset,\nwe work with the SNOMED-CT Core dataset intro-\nduced by Lovelace et al. (2021). For the encyclope-\ndic dataset, we utilize the widely used benchmark\nKGC dataset, FB15k-237 (Toutanova and Chen,\n2015). We additionally utilize the widely studied\nWN18RR (Dettmers et al., 2018) dataset which\nis derived from WordNet. Dataset statistics are\nreported in the appendix in Table 7.\n5 Candidate Retrieval\nMu and Viswanath (2018); Ethayarajh (2019); Li\net al. (2020) have observed that textual embedding\nspaces tend to be highly anisotropic, i.e. most\nvectors occupy a narrow cone within the space,\nwhich limits their expressiveness. Furthermore,\napproaches that improve the isotropy, i.e. the uni-\nformity with respect to direction, of the embedding\nspace lead to significant improvements on semantic\nsimilarity benchmarks (Mu and Viswanath, 2018;\nLi et al., 2020; Gao et al., 2021). Given that entity\nranking relies upon a similar scoring mechanism,\nthe existing embedding space may be similarly sub-\noptimal for candidate retrieval.\n5.1 Embedding Quality Metrics\nWe measure two primary aspects of the embedding\nspace to analyze the effect of different processing\ntechniques: the anisotropy of the space and the\nalignment of the space with the knowledge con-\ntained within the graph. We note that these aspects\ncorrespond to the notions of uniformity and align-\nment from work in constrastive learning (Wang and\nIsola, 2020; Gao et al., 2021).\n5.1.1 Effective Dimension\nWe utilize a measure of anisotropy introduced by\nCai et al. (2021) called the ϵ-effective-dimension.\nWe first apply PCA to the matrix of entity em-\nbeddings. The ratio of the variance explained by\nk principal components can then be calculated\nas rk = ∑k−1\ni=0 σi/∑m−1\nj=0 σj, where σi is the i-th\nlargest eigenvalue of the covariance matrix of the\nembeddings. The ϵ-effective-dimension is then\nd(ϵ) =argminkrk ≥ϵ. We set ϵ = 0.8, which\nmeans that we measure the minimum number of\nPCA components necessary to explain 80% of the\nvariance in the embedding space.\n5939\n5.1.2 Knowledge Alignment\nFor some set of facts {(ei,rj,ek)}n\nk=1, we\nwould expect {ek}n\nk=1 to be similar in some\nway. For example, all entities that satisfy the\nquery (abdomen,finding_site_of,?) are abdom-\ninal conditions. The inner product scoring means\nthat this similarity should be encoded within the\nentity embedding space to enable retrieving the set\nof correct entities with a single query vector.\nTo evaluate the alignment of the embedding\nspace and the KG, we define the similarity between\ntwo entities as\nSim(ei,ej) =∑\nek∈E,rl∈R1(ek,rl,ei) ×1(ek,rl,ej)\nwhere Eis the set of entities, Ris the set of rela-\ntions, and 1(ek,rl,ei) evaluates to one if the fact\nis contained within the KG and zero otherwise. We\nreport the knowledge aligment as the Spearman’s\nrank correlation, ρ, between our KG-induced mea-\nsure of similarity and the inner product between\ncentered entity embeddings.\n5.1.3 Lexical Alignment\nAs a complementary measure to knowledge align-\nment, we also measure the lexical alignment of the\nembedding space by calculating the Spearman’s\nrank correlation, ρ, between the Jaccard Similar-\nity of the entity descriptions and the inner product\nbetween centered entity embeddings.\n5.2 Embedding Processing Techniques\n5.2.1 Unsupervised Techniques\nNormalization As a simple baseline, we normalize\neach entity embedding, ei ∈Rd, by centering the\nembedding space and scaling each vector to unit\nnorm as ˜ ei = ei−c\n∥ei−c∥2\nwhere c ∈Rd is the mean\nof the entity embeddings.\nNormalizing Flow We learn a normalizing flow\nto transform the anisotropic embedding space to\nan isotropic space, similar to Li et al. (2020). We\nbriefly introduce normalizing flows, but we refer\nthe reader to Papamakarios et al. (2021) for a com-\nprehensive overview.\nNormalizing flows can be used to transform a\ndistribution into a known probability distribution.\nGiven x ∈ Rd with an unknown true distribu-\ntion x ∼ p∗\nx(x), we can define a joint distribu-\ntion over x following the generative process of\nx = T(u),u ∼pu(u) where pu(u) is the base\nprobability distribution of the flow model.\nFigure 2: Intrinsic evaluation of embedding processing tech-\nniques. We note the MRR for each approach in parenthesis.\nNormalizing flows constrain the transforma-\ntion, T, to be a diffeomorphism which al-\nlows us to write the density of x in terms of\npu(u) and the Jacobian determinant of T−1 as\npx(x) =pu(T−1(x))|det(JT−1 (x))|. We can then\nfit the flow by minimizing the negative log-\nlikelihood of observed samples {xn}N\nn=1 as\n−log(px(xi)) =\n−log(pu(T−1(xi))) −log|det(JT−1 (xi))|\nWe define T−1(x) =Wx + b where W ∈\nRd×d and x,b ∈Rd. To ensure the invertibility\nof W and to simplify the computation of the Ja-\ncobian determinant, we parameterize W using its\nLU decomposition (Kingma and Dhariwal, 2018).\nWe select a multivariate Guassian centered on the\norigin with identity convariance for the base distri-\nbution. Thus, the normalizing flow learns to map\nthe embedding space to an isotropic Gaussian.\n5.2.2 Supervised Techniques\nWe explore two inexpensive supervised techniques\nthat learn to transform the embedding space. For\nboth techniques, we preprocess the set of entity\nembeddings by centering and scaling them to have\nunit norm prior to the transformation.\nMLP We consider an MLP with one hidden\nlayer followed by normalization. Thus, a pro-\ncessed entity embedding, ei, is transformed as\n˜ ei = MLP(ei)\n∥MLP(ei)∥2\n.\nResidual MLP We consider an MLP that uses\na residual connection with the original embedding.\nA processed entity embedding, ei, would then be\ntransformed as ˜ ei = (ei+MLP(ei))\n∥(ei+MLP(ei))∥2\n.\n5940\nSNOMED CT Core CN-82K FB15k-237 WN18RR\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10\nDefault Embeddings.488 .383 .543 .689 .190 .127 .208 .314 .339 .259 .370 .500 .575 .503 .606 .716\nNormalization .487 .381 .544 .692 .192 .128 .211 .317 .348∗∗∗ .264 .381 .514 .576 .501 .608 .726Normalizing Flow.508∗∗∗ .401 .566 .713 .194∗∗ .129 .213 .320 .352∗∗∗ .265 .385 .527 .580∗ .509 .607 .721\nMLP .539∗∗∗† .431 .598 .749 .200∗∗∗† .132 .222 .339 .374∗∗∗† .282 .407 .561 .583∗∗ .510 .613 .730\nResidual MLP .549∗∗∗† .445 .507 .752 .209∗∗∗† .138 .230 .350 .375∗∗∗† .283 .408 .564 .591∗∗∗† .518 .616 .735\nTable 1: Comparison of candidate transformation techniques. The highest metrics for unsupervised and supervised techniques\nare bolded. We indicate a significant improvement over the default embeddings with ∗, ∗∗, ∗∗∗(p <0.05, 0.005, 5e−5) and\nover the normalizing flow with †(p <5e−5).\nSNOMED CT Core CN-82K FB15k-237 WN18RR\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10\nCLS Token .472 .371 .521 .671 .157 .104 .171 .259 .351 .266 .383 .525 .549 .488 .567 .675\n+ Pretraining.489∗ .385 .540 .695 .189∗ .126 .207 .314 .356∗ .270 .388 .530 .587∗ .515 .618 .732\nMean Pooling.503 .397 .559 .705 .184 .124 .202 .303 .352 .266 .385 .525 .577 .508 .603 .719\n+ Pretraining.509∗ .403 .566 .713 .195∗ .130 .216 .323 .352 .265 .385 .527 .580 .509 .607 .721\nTable 2: Ablation of embedding extraction techniques. We indicate significant improvements from the pretraining procedure\nwith ∗(p <5e−5).\n5.3 Experiments\nWe evaluated the different embedding processing\nmethods using the textual entity embeddings re-\nleased by Lovelace et al. (2021)2. We also utilize\nBERT-ResNet with the default hyperparameters\nfrom Lovelace et al. (2021) as our neural ranking\narchitecture, fθ(·,·). We only apply the transforma-\ntion, gθ(ek) =˜ ek where ˜ ek ∈Rd, to the embed-\nding matrix used for candidate ranking. Therefore,\nwe compute the score as yk = fθ(ei,rj)gθ(ek)⊺.\n5.4 Impact Of Embedding Space\nTransformations\nWe report the effect of the different transformations\non downstream performance in Table 1 and display\nthe intrinsic embedding metrics for WN18RR in\nFigure 2. Figures for the other datasets are pre-\nsented in the appendix and show similar findings.\nThe normalization baseline is generally ineffec-\ntive, which is consistent with its limited effect\non the embedding metrics. The normalizing flow\ngreatly increases the effective dimensionality but\ndecreases the knowledge alignment of the space.\nThis suggests that there may be a trade-off between\nisotropy and alignment of the space, which is con-\nsistent with observations from work in contrastive\nlearning (Gao et al., 2021). Despite that trade-\noff, optimizing solely for isotropy significantly im-\nproves performance across all datasets, confirming\nthat the anisotropy of the original space hurts per-\nformance.\nFor the supervised techniques, the MLP and\n2Lovelace et al. (2021) did not work with WN18RR, so we\ndeveloped embeddings following their procedure. We examine\nembedding extraction methods in detail in Section 5.5\nFigure 3: Effect Of Residual MLP on knowledge and lexical\nalignment.\nResidual MLP lead to significantly improved per-\nformance, with the Residual MLP consistently out-\nperforming the MLP. Both transformations consis-\ntently improve the knowledge alignment of the em-\nbedding spaces. Compared to the MLP, the Resid-\nual MLP produces a more isotropic space. Given\nits strong performance, the Residual MLP seems to\nbest balance the trade-off between the knowledge\nalignment and isotropy of the embeddings.\nWe contrast the effect of the Residual MLP on\nknowledge and lexical alignment in Figure 3. The\nResidual MLP strengthens the KG alignment while\nreducing the lexical alignment across all datasets,\ndemonstrating that it learns to emphasize relevant\ninformation while discarding spurious information.\n5.5 Embedding Extraction Ablation\nFor this ablation, we used the most effective un-\nsupervised processing technique, the normalizing\nflow, for candidate ranking. We ablate the efficacy\nof the following embedding extraction choices.\n[CLS] Token: We extract the embedding of the\n5941\nSNOMED CT Core CN-82K FB15k-237 WN18RR\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10\nUnsupervised Extraction.509 .403 .566 .713 .195 .130 .216 .323 .356 .270 .388 .530 .587 .515 .618 .732\nFinetuning .496 .386 .555 .709 .186 .124 .203 .307 .347 .260 .379 .522 .579 .509 .606 .721\nLinear Probe .516††† .408 .575 .722 .195 .130 .215 .324 .358† .272 .392 .530 .598†† .524 .630 .746\nPrompt-tuning .515††† .410 .573 .719 .201††† .136 .222 .333 .357 .271 .392 .528 .597†† .523 .630 .744\nTable 3: Comparison of query entity extraction techniques. We indicate significant improvements over the best unsupervised\napproach with †, ††, †††(p < .05, 5e−4, 5e−5).\n[CLS] token from the final layer following prior\nwork (Malaviya et al., 2020; Wang et al., 2021a).\nMean Pooling: We mean pool across all tokens\nand layers following Lovelace et al. (2021).\nMLM Pretraining: Recent work (Malaviya et al.,\n2020; Wang et al., 2021a; Lovelace et al., 2021)\nhas pretrained the language model using the MLM\nobjective on the set of entity names. We ablate the\nimpact of this choice.\nWe report the KGC metrics in Table 2. The\nMLM pretraining often results in significant im-\nprovements in downstream performance. The opti-\nmal unsupervised extraction technique varies based\non the dataset, with mean-pooling being most effec-\ntive for the SNOMED CT Core dataset and the CN-\n82K dataset while the [CLS] embedding is most\neffective for the other two datasets. However, we\nobserve that mean pooling after MLM pre-training\nis reasonably effective across all datasets.\n6 Query Entity Extraction\nWe explore supervised techniques to extract more\ninformative representations from pre-trained lan-\nguage models for the query entity.\nFine-tuning: We fine-tune the language model\nduring training and extract the entity representation\nby mean pooling across the intermediate states in\neach layer and aggregating across layers with a\nlearned linear combination.\nLinear Probe: We freeze the language model\nand apply a learned linear projection (Toshniwal\net al., 2020) to every hidden state of the model.\nWe then max-pool across the tokens in each layer\nto produce a single feature vector for every layer.\nWe aggregate these features using a learned linear\ncombination across layers.\nPrompt-tuning We learn continuous prompts\nthat we prepend to the language model inputs at\nevery layer to prompt the frozen model (Li and\nLiang, 2021). We extract entity representations by\nmean pooling across intermediate states in each\nlayer and aggregate across layers with a learned\nlinear combination.\nFigure 4: Effect of supervised extraction techniques com-\npared to the unsupervised baseline. Error bars indicate 95%\nconfidence intervals.\n6.1 Experiments\nTo isolate the effect of the query embedding ex-\ntraction technique, we use the normalizing flow for\ncandidate ranking with the most effective embed-\ndings from our prior ablation for each dataset.\nThe supervised extraction techniques introduce\nan additional function, hθ(ei) = ˆ ei where ˆ ei ∈\nRd, to extract entity representations for computing\nthe query fθ(ˆ ei,rj) = ˆ q. Therefore, the score is\ncomputed as yk = fθ(hθ(ei),rj)gθ(ek)⊺.\n6.1.1 Impact of Embedding Extraction\nTechniques\nWe report the KGC metrics in Table 3. Fine-tuning\nthe language model during training actually de-\ngrades performance across all datasets, although\nit does minimize the training loss more effectively\nthan other approaches. We break down the effect\nof different techniques in Figure 4 by the connec-\ntivity of the query entity for the WN18RR dataset.\nWe observe that the performance degradation is\nmore pronounced for queries with lower connectiv-\nity although this degradation doesn’t extend to un-\nseen query entities. This suggests that fine-tuning\nthe language model leads to overfitting for entities\nwith limited information. The figures for the other\ndatasets show similar trends and are presented in\nthe appendix.\n5942\nSNOMED CT Core CN-82K FB15k-237 WN18RR\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10\nUnsupervised Embedding Extraction & Residual MLP\nBERT-base .531 .425 .588 .736 .210 .139 .232 .352 .373 .282 .406 .559 .590 .518 .616 .735\nBERT-large .545∗∗ .441 .601 .749 .212 .139 .234 .356 .375 .282 .410 .563 .597∗ .524 .624 .743\nPubMedBERT.549‡ .444 .606 .754 − − − − − − − − − − − −\nPrompt-tuning & Residual MLP\nBERT-base .530 .423 .587 .736 .214†† .142 .237 .361 .376† .284 .410 .562 .599† .525 .632 .749\nBERT-large .541∗∗ .434 .599 .749 .216†† .144 .238 .361 .373 .280 .409 .561 .608∗∗†† .538 .636 .751\nPubMedBERT.550‡ .443 .611 .755 − − − − − − − − − − − −\nTable 4: Effect of language model selection. We indicate significant improvements from the larger language model with\n∗, ∗∗(p < .05, 5e−5); from prompting with †, ††(p <0.05, .005); and from specialization with ‡(p <5e−5).\nThe parameter-efficient supervised techniques\ndo, however, lead to significantly improved perfor-\nmance across all datasets, although there is not a\nclear winner between them. These techniques miti-\ngate the overfitting problem while enabling bene-\nficial adaptation to the downstream task. Figure 4\nshows that the benefits of supervision are greatest\nfor sparsely connected query entities. For densely\nconnected query entities, the impact is generally\nnegligible, potentially because the graph already\ncontains sufficient information about the entity.\nWe note that sparsely connected entities were\nfiltered out of the FB15k-237 KG during the cura-\ntion of the dataset, producing an artificially dense\nKG (Lovelace et al., 2021). This artificial den-\nsity limits the benefit of techniques which improve\nperformance for sparsely connected entities. There-\nfore, our analysis also explains the limited topline\nimprovements for the FB15k-237 dataset.\n7 Effect of Language Model Selection\nFurther performance improvements can often be\ngained by scaling up the size of the language model\nDevlin et al. (2019) or from using specialized,\ndomain-specific language models Gu et al. (2020).\nIn this section, we examine the effect of these two\naspects on downstream KGC performance.\nWe conduct experiments with both unsupervised\nand supervised query entity extraction techniques\nwhile using our best candidate ranking approach,\nthe Residual MLP. We conduct experiments with\nBERT-base-uncased and BERT-large-uncased for\nall three KGs. To evaluate the effect of specializa-\ntion, we use PubMedBERT, which is the same size\nas BERT-base, for SNOMED-CT Core.\nWe report the results of these experiments in\nTable 4. When using unsupervised extraction tech-\nniques, the larger language model consistently im-\nproves performance, but the differences can be mi-\nnor. For the supervised extraction techniques, the\nlarger language model actually degrades perfor-\nmance over the unsupervised extraction techniques\nin some cases. The effect of using supervision for\nextracting the query entity is dataset-dependent and\nis helpful for CN82K and WN18RR.\nThe supervised extraction and larger language\nmodels do lead to lower training loss, but that im-\nprovement does not consistently translate to stonger\ntest performance. Thus, the mixed results likely\narise from overfitting which could potentially be\nmitigated with careful regularization. Domain-\nspecific pretraining is particularly effective, with\nPubMedBERT consistently outperforming other\nmodels.\n8 Comparison Against Recent Work\nWe synthesize our findings to develop a KGC\nmodel and compare against recent work. We again\nsimply repurpose the BERT-ResNet ranking ar-\nchitecture with the default hyperparameters from\nLovelace et al. (2021) to demonstrate the impact of\nthe decisions explored in this work.\nWe report results across the two sparser datasets\nin Table 5. Our embedding extraction and process-\ning techniques outperform recent work, with the su-\npervised techniques being particularly effective. In\nTable 5 we also compare against a selection of base-\nlines on the FB15K-237 and WN18RR datasets.\nWe also denote whether the models utilize addi-\ntional graph information or textual information.\nOur KGC model is very effective and outper-\nforms the models that do not incorporate any ad-\nditional information. Although this seems natural,\nthis was actually not the case with previous work.\nTherefore, our method integrates textual informa-\ntion in a way that leads to competitive performance\neven for these widely studied benchmark datasets.\n5943\nSNOMED CT Core CN-82K Additional Information\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10 Text\nDistMult (Yang et al., 2015) .293 .226 .318 .426 .0280 − .0290 .0560 ✗\nComplEx (Trouillon et al., 2016) .302 .224 .332 .456 .0260 − .0270 .0500 ✗\nConvE (Dettmers et al., 2018) .271 .191 .303 .429 .0801 − ..0867 .1313 ✗\nBERT-ConvTransE (Malaviya et al., 2020) − − − − .1626 − .1795 .2751 ✓\nInductivE (Wang et al., 2021a) − − − − .2035 − .2265 .3386 ✓\nBERT-DeepConv (Lovelace et al., 2021) .479 .374 .532 .685 − − − − ✓\nBERT-ResNet (Lovelace et al., 2021) .492 .389 .544 .694 .190 .127 .208 .318 ✓\nBERT-ResNet + Normalizing Flow .509 .403 .566 .713 .195 .130 .216 .323 ✓\nBERT-ResNet + Prompt-tuning + Normalizing Flow.515 .410 .573 .719 .201 .136 .222 .333 ✓\nBERT-ResNet + Residual MLP .549 .444 .606 .754 .212 .139 .234 .356 ✓\nBERT-ResNet + Prompt-tuning + Residual MLP.550 .443 .611 .755 .216 .144 .238 .361 ✓\nFB15K-237 WN18RR Additional Information\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10 Graph Structure Text\nRESCAL†(Nickel et al., 2011) .357 − − .541 .467 − − .517 ✗ ✗\nTransE†(Bordes et al., 2013) .313 − − .497 .228 − − .520 ✗ ✗\nDistMult†(Yang et al., 2015) .343 − − .531 .452 − − .531 ✗ ✗\nComplEx†(Trouillon et al., 2016) .348 − − .536 .475 − − .547 ✗ ✗\nConvE†(Dettmers et al., 2018) .339 − − .521 .442 − − .504 ✗ ✗\nCompGCN (Vashishth et al., 2020) .355 .264 .390 .535 .479 .443 .494 .546 ✓ ✗\nHittER (Chen et al., 2021) .373 .279 .409 .558 .503 .462 .516 .584 ✓ ✗\nKG-BERT‡(Yao et al., 2019) .236 .145 .258 .420 .242 .110 .280 .524 ✗ ✓\nBERT-TransE (Daza et al., 2021) .235 .150 .253 .411 .325 .144 .431 .679 ✗ ✓\nMLMLM (Clouatre et al., 2021) .259 .187 .282 .403 .502 .439 .542 .611 ✗ ✓\nStAR (Wang et al., 2021b) .296 .205 .322 .482 .401 .243 .491 .709 ✗ ✓\nLP-BERT (Li et al., 2022) .310 .223 .336 .490 .482 .343 .563 .752 ✗ ✓\nBERT-ResNet (Lovelace et al., 2021) .346 .262 .379 .514 .575 .503 .606 .716 ✗ ✓\nBERT-ResNet + Normalizing Flow .356 .270 .388 .530 .587 .515 .618 .732 ✗ ✓\nBERT-ResNet + Prompt-tuning + Normalizing Flow.357 .271 .392 .528 .599 .527 .630 .743 ✗ ✓\nBERT-ResNet + Residual MLP .375 .282 .410 .563 .597 .524 .624 .743 ✗ ✓\nBERT-ResNet + Prompt-tuning + Residual MLP.376 .284 .410 .562 .608 .538 .636 .751 ✗ ✓\nTable 5: Comparison against baselines and recent work. We indicate that the results are from Ruffinelli et al. (2020) with a†and\nfrom the work by Daza et al. (2021) with a ‡. The baselines for SNOMED CT Core and CN82K are taken from Lovelace et al.\n(2021) and Wang et al. (2021a) respectively, except for the BERT-ResNet result for CN82K which is from our implementation.\nThe WN18RR result for BERT-ResNet is also from our implementation. Other results are taken from the original work. Dashes\nindicate that the metric was not reported by the prior work.\n8.1 Complementarity of Textual Approach\nTo evaluate the complementarity of textual and non-\ntextual approaches, we train a transformer model\nsimilarly to Chen et al. (2021). We refer the reader\nto the appendix for full details regarding this model.\nWe then ensemble this model with our most ef-\nfective model from Table 5, computing candidate\nscores as a convex combination of the two sets of\nscores. We tune the ensemble weight with the vali-\ndation set. We also explore using an independent\nweight for each relation. As a baseline compari-\nson, we ensemble our best configuration across two\nrandom seeds.\nWe report the results of this experiment in Ta-\nble 6. We observe that ensembling is consistently\neffective, particularly the relation-specific ensem-\nbling. On the WN18RR dataset where the tex-\ntual approach is already highly effective, ensem-\nbling the different approaches does not outpeform\nthe self-ensemble. However, for the FB15k-237\ndataset where the performance of the different ap-\nproaches is closer, ensembling the textual and non-\nFB15K-237 WN18RR\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10\nTransformer .367 .272 .404 .554 .486 .446 .503 .564Our Framework .376 .284 .410 .562 .608 .538 .636 .751Alt. Seed .377 .285 .412 .564 .605 .533 .634 .749\nSimple Ensemble\nSelf-Ensemble .384∗∗∗ .292 .420 .570 .613∗∗∗ .540 .641 .760Transformer Ensemble.388∗∗∗†‡‡‡.295 .425 .576 .609∗ .539 .638 .755\nRelation-Specific Ensemble\nSelf-Ensemble .391∗∗∗ .303 .424 .571 .616∗∗∗‡‡.544.642.758Transformer Ensemble.400∗∗∗†‡‡‡.310.435.582 .612∗‡ .543 .640 .756\nTable 6: Ensembling Results. We indicate sig-\nnificant improvements over our framework with\n∗, ∗∗, ∗∗∗(p < .05, 5e−4, 5e−5); from the transformer\nensemble with †(p <5e−5); and from relation-specific\nensembling with ‡, ‡‡, ‡‡‡(p < .005, 5e−4, 5e−5).\ntextual models does meaningfully improve perfor-\nmance over the self-ensemble. This demonstrates\nthat textual approaches can complement existing\nmethods.\n9 Conclusion\nWe present a framework for adapting pre-trained\nlanguage models for KGC. The key insight driving\nthe development of our framework was that decou-\npling the entity representations used for computing\n5944\nthe query representation and the entity represen-\ntations used for candidate retrieval enabled us to\nbetter integrate the information from pre-trained\nlanguage models while maintaining the scalability\nnecessary to train performant KGC models.\nWe introduced unsupervised and supervised tech-\nniques to improve the suitability of entity embed-\ndings for candidate ranking (Section 5), introduced\nmethods to extract entity embeddings from lan-\nguage models (Section 6), and explored the effect\nof language model selection (Section 7).\nBy synthesizing the insights from our explo-\nration, we developed a KGC model that signifi-\ncantly outperforms recent work while simply repur-\nposing an existing ranking architecture. While in-\nnovations in neural ranking architecture have been\nvaluable, our work demonstrates the importance of\ndeveloping more informative entity representations.\nThe findings and analysis from this work provide a\nuseful framework for adapting pre-trained language\nmodels for knowledge graph completion.\n10 Limitations\n10.1 Training Overhead\nWe report and discuss the number of trainable pa-\nrameters and training times across our different con-\nfigurations in detail in the appendix 3. We present\nthe main takeaways in this section.\nThe supervised techniques like the Residual\nMLP and prompt-tuning introduce additional pa-\nrameters and can increase the training time com-\npared to the BERT-ResNet baseline. However,\nboth the Residual MLP and prompt-tuning are very\nparameter-efficient. When utilizing BERT-base, the\nResidual MLP increases the number of trainable\nparameters by 3.6% and prompt-tuning increases\nit by 1.2%. The increases are similar when uti-\nlizing BERT-large (3.6% and 1.1% respectively).\nDirectly fine-tuning BERT-base, for comparison,\nincreases the number of trainable parameters by\n331.2%.\nThe residual MLP, while lightweight, does in-\ncrease the training time per iteration. For the can-\ndidate transformation experiment on the WN18RR\ndataset (Section 5), the baseline completes one\nepoch in 3m56s while the Residual MLP increases\nthis to 5m44s. However, the Residual MLP also ac-\ncelerates convergence, offsetting the per-iteration\nslowdown. Although it takes a similar amount of\n3All ranking models reported in this work were trained on\na single NVIDIA GeForce GTX 1080 Ti.\ntime to train the baseline for 6 epochs as it does\nto train the Residual MLP model for 4 epochs, the\nResidual MLP actually outperforms the baseline at\nthat time despite training for fewer iterations.\nTherefore, the baseline is only more effective in\nthe earliest stage of training before being surpassed\nby the Residual MLP model. For the WN18RR\ndataset, this breakeven point occurs within only\n29m of training. This trend holds across all datasets,\nwith the worst breakeven point being only 1h43m.\nTherefore the accelerated convergence offsets the\nincreased per-iteration cost for all but the shortest\nof training times.\nTechniques such as prompt-tuning require the ap-\nplication of a language model, which increases the\ntime per iteration. For the query extraction experi-\nment on the WN18RR dataset (Section 6), the base-\nline completes one epoch in 3m54s, while prompt-\ntuning increases this to 8m47s. When controlling\nfor wall clock time, we observe a similar trend\nwhere the baseline is more effective early in train-\ning before being surpassed by prompt-tuning. How-\never, the breakeven point occurs much later (e.g.\nat 14h1m for WN18RR). Therefore, in settings\nwith limited training budgets, the performance im-\nprovement from prompt-tuning may not justify the\nadditional training cost.\nWe note that none of the techniques explored in\nour work introduce any overhead at inference time.\nAfter training, the improved entity representations\nfrom the Residual MLP or prompt-tuning can be\ncomputed and cached for inference, reducing the\ncost of computing entity embeddings to a simple\nlookup like the original BERT-ResNet model.\n10.2 Availability of Textual Descriptions\nThe integration of pre-trained langauge models to\nimprove KG entity representations is predicated\nupon the existence of informative textual descrip-\ntions for the entities within the graph. Although\nthis assumption holds in many scenarios, it does\nnot hold universally. For instance, clinical data\nfrom the Electronic Health Record can naturally be\nrepresented as a knowledge graph for applications\nsuch as question answering (Park et al., 2021).\nEntities like medications and procedures would\nhave well-defined names, but others such as those\nrepresenting specific admissions events or hospital\nstays would be represented with a numerical ID\nand would not have natural textual representations.\nAlthough a hybrid approach that adaptively inte-\n5945\ngrates textual information when available would\nlikely be beneficial, the extension of our framework\nto such settings is left for future work.\n11 Ethical Considerations\nKnowledge graphs are valuable resources utilized\nby applications such as search engines (Sullivan,\n2020) and automated voice assistants (Flint, 2021)\nto present information to users. While KGC mod-\nels have the potential to improve the coverage of\nsuch resources, they also risk introducing inaccu-\nrate facts that could mislead users. The cost of\nsuch inaccuracies can vary significantly based on\nthe information domain (e.g. film trivia vs. medical\ninformation).\nTherefore, such tools should not be deployed\nwithout careful consideration of the potential harms\nor the development of appropriate mitigation strate-\ngies. One way to minimize such risks is to use\nKGC methods to accelerate the curation of likely\ncandidate facts that must undergo further verifica-\ntion before their inclusion in the knowledge graph.\nAcknowledgments\nThis research was funded in part by NSF grant IIS\n1917955.\nReferences\nYoav Benjamini and Yosef Hochberg. 1995. Controlling\nthe false discovery rate - a practical and powerful\napproach to multiple testing. J. Royal Statist. Soc.,\nSeries B, 57:289 – 300.\nTaylor Berg-Kirkpatrick, David Burkett, and Dan Klein.\n2012. An empirical investigation of statistical sig-\nnificance in NLP. In Proceedings of the 2012 Joint\nConference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Lan-\nguage Learning, pages 995–1005, Jeju Island, Korea.\nAssociation for Computational Linguistics.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in neural information\nprocessing systems, pages 2787–2795.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2021. Isotropy in the contextual embedding\nspace: Clusters and manifolds. In International Con-\nference on Learning Representations.\nSanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nLouis Clouatre, Philippe Trempe, Amal Zouaq, and\nSarath Chandar. 2021. MLMLM: Link prediction\nwith mean likelihood masked language model. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 4321–4331, On-\nline. Association for Computational Linguistics.\nDaniel Daza, Michael Cochez, and Paul Groth. 2021.\nInductive entity representations from text via link\nprediction. In Proceedings of the Web Conference\n2021, WWW ’21, page 798–808, New York, NY ,\nUSA. Association for Computing Machinery.\nTim Dettmers, Minervini Pasquale, Stenetorp Pontus,\nand Sebastian Riedel. 2018. Convolutional 2d knowl-\nedge graph embeddings. In Proceedings of the 32th\nAAAI Conference on Artificial Intelligence , pages\n1811–1818.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nEmma Flint. 2021. Alexa entities launches to general\navailability.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2020. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\n5946\nDurk P Kingma and Prafulla Dhariwal. 2018. Glow:\nGenerative flow with invertible 1x1 convolutions. In\nAdvances in Neural Information Processing Systems,\nvolume 31. Curran Associates, Inc.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nDa Li, Ming Yi, and Yukai He. 2022. LP-BERT: multi-\ntask pre-training knowledge graph BERT for link\nprediction. CoRR, abs/2201.04843.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation.\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and\nZhiyuan Liu. 2018. Entity-duet neural ranking: Un-\nderstanding the role of knowledge graph semantics\nin neural information retrieval. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2395–2405, Melbourne, Australia. Association for\nComputational Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nJustin Lovelace, Denis Newman-Griffis, Shikhar\nVashishth, Jill Fain Lehman, and Carolyn Rosé. 2021.\nRobust knowledge graph completion with stacked\nconvolutions and a student re-ranking network. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1016–\n1029, Online. Association for Computational Linguis-\ntics.\nChaitanya Malaviya, Chandra Bhagavatula, Antoine\nBosselut, and Yejin Choi. 2020. Commonsense\nknowledge base completion with structural and se-\nmantic context. Proceedings of the 34th AAAI Con-\nference on Artificial Intelligence.\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-top:\nSimple and effective postprocessing for word repre-\nsentations. In International Conference on Learning\nRepresentations.\nMaximilian Nickel, V olker Tresp, and Hans-Peter\nKriegel. 2011. A three-way model for collective\nlearning on multi-relational data. In Proceedings of\nthe 28th International Conference on International\nConference on Machine Learning , ICML’11, page\n809–816, Madison, WI, USA. Omnipress.\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez\nRezende, Shakir Mohamed, and Balaji Lakshmi-\nnarayanan. 2021. Normalizing flows for probabilistic\nmodeling and inference. Journal of Machine Learn-\ning Research, 22(57):1–64.\nJunwoo Park, Youngwoo Cho, Haneol Lee, Jaegul Choo,\nand E. Choi. 2021. Knowledge graph-based question\nanswering with electronic health records. In MLHC.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nDaniel Ruffinelli, Samuel Broscheit, and Rainer\nGemulla. 2020. You can teach an old dog new tricks!\non training knowledge graph embeddings. In Inter-\nnational Conference on Learning Representations.\nTao Shen, Xiubo Geng, Tao Qin, Daya Guo, Duyu\nTang, Nan Duan, Guodong Long, and Daxin Jiang.\n2019. Multi-task learning for conversational ques-\ntion answering over a large-scale knowledge base. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2442–\n2451, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDanny Sullivan. 2020. A reintroduction to our knowl-\nedge graph and knowledge panels.\nHaitian Sun, Tania Bedrax-Weiss, and William Cohen.\n2019. PullNet: Open domain question answering\nwith iterative retrieval on knowledge bases and text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2380–\n2390, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDung Thai, Raghuveer Thirukovalluru, Trapit Bansal,\nand Andrew McCallum. 2021. Simultaneously\nself-attending to text and entities for knowledge-\ninformed text representations. In Proceedings of the\n6th Workshop on Representation Learning for NLP\n(RepL4NLP-2021), pages 241–247, Online. Associa-\ntion for Computational Linguistics.\nRaghuveer Thirukovalluru, Mukund Sridhar, Dung\nThai, Shruti Chanumolu, Nicholas Monath, Sankara-\nnarayanan Ananthakrishnan, and Andrew McCallum.\n2021. Knowledge informed semantic parsing for con-\nversational question answering. In Proceedings of\nthe 6th Workshop on Representation Learning for\nNLP (RepL4NLP-2021), pages 231–240, Online. As-\nsociation for Computational Linguistics.\nJ. Tompson, R. Goroshin, A. Jain, Y . LeCun, and C. Bre-\ngler. 2015. Efficient object localization using con-\nvolutional networks. In 2015 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR),\npages 648–656.\nShubham Toshniwal, Haoyue Shi, Bowen Shi, Lingyu\nGao, Karen Livescu, and Kevin Gimpel. 2020. A\ncross-task analysis of text span representations. In\nProceedings of the 5th Workshop on Representation\n5947\nLearning for NLP, pages 166–176, Online. Associa-\ntion for Computational Linguistics.\nKristina Toutanova and Danqi Chen. 2015. Observed\nversus latent features for knowledge base and text\ninference. In Proceedings of the 3rd Workshop on\nContinuous Vector Space Models and their Composi-\ntionality, pages 57–66, Beijing, China. Association\nfor Computational Linguistics.\nThéo Trouillon, Johannes Welbl, Sebastian Riedel, Éric\nGaussier, and Guillaume Bouchard. 2016. Complex\nembeddings for simple link prediction. In Proceed-\nings of the 33rd International Conference on Interna-\ntional Conference on Machine Learning - Volume 48,\nICML’16, pages 2071–2080. JMLR.org.\nShikhar Vashishth, Soumya Sanyal, Vikram Nitin, and\nPartha Talukdar. 2020. Composition-based multi-\nrelational graph convolutional networks. In Interna-\ntional Conference on Learning Representations.\nBin Wang, Guangtao Wang, Jing Huang, Jiaxuan You,\nJure Leskovec, and C-C Jay Kuo. 2021a. Inductive\nlearning on commonsense knowledge graph com-\npletion. International Joint Conference on Neural\nNetworks (IJCNN).\nBo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying\nWang, and Yi Chang. 2021b. Structure-augmented\ntext representation learning for efficient knowledge\ngraph completion. In Proceedings of the Web Confer-\nence 2021, WWW ’21, page 1737–1748, New York,\nNY , USA. Association for Computing Machinery.\nTongzhou Wang and Phillip Isola. 2020. Understanding\ncontrastive representation learning through alignment\nand uniformity on the hypersphere. In International\nConference on Machine Learning, pages 9929–9939.\nPMLR.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nBishan Yang, Scott Wen-tau Yih, Xiaodong He, Jian-\nfeng Gao, and Li Deng. 2015. Embedding entities\nand relations for learning and inference in knowledge\nbases. In Proceedings of the International Confer-\nence on Learning Representations (ICLR) 2015.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nKG-BERT: BERT for knowledge graph completion.\nCoRR, abs/1909.03193.\n5948\nA Dataset Information\nWe report the details for the datasets used in this\nwork in Table 7. For SNOMED CT Core, CN82k,\nand FB15k-237 we utilize the textual descriptions\nused by Lovelace et al. (2021). For SNOMED\nCT Core and CN82k, these consist of short en-\ntity names. For FB15k-237, the descriptions are\nshort paragraphs that describe the entity. For the\nWN18RR dataset, we utilize the entity descriptions\nreleased by Yao et al. (2019), which consist of\nthe word and a short definition. Unless otherwise\nstated, we utilize PubmedBERT to extract embed-\ndings for the SNOMED CT Core dataset and uti-\nlize the uncased version of BERT-base for the other\nthree datasets.\nB Evaluation Metrics\nWe present a mathematical formulation of our eval-\nuation metrics. We consider both forward and in-\nverse relations for the datasets examined in this\nwork. For the CN82k and FB15k-237 datasets, we\nfollow standard procedure and introduce an inverse\nfact, (el,r−1\nj ,ei), for every fact, (ei,rj,el), in the\ndataset. The SNOMED CT Core dataset already\ncontains inverse relations so manually adding in-\nverse facts in unecessary. We letT denote the set\nof all facts in the test set.\nThe Mean Reciprocal Rank (MRR) is defined as\nMRR = 1\n|T|\n∑\n(ei,rj ,el)∈T\n1\nrank(el)\nThe Hits at k (H@k) is defined as\nH@k = 1\n|T|\n∑\n(ei,rj ,el)∈T\nI[rank(el) ≤k]\nwhere I[P] is 1 if the condition P is true and is\n0 otherwise. When computing rank(xi), we first\nfilter out all positive samples other than the tar-\nget entity xi. This is commonly referred to as the\nfiltered setting. If the correct entity is tied with\nsome other entity, then we compute its rank as the\naverage rank of all entities with that score.\nC Model Configuration Details\nC.1 Trainable Parameters\nWe report parameter counts for the WN18RR\ndataset across all the different configurations con-\nsidered in this work in in Table 8. The parameter\ncounts are identical across datasets with the ex-\nception of the relation parameters which depends\nupon the number of relations within each KG. The\nrelation parameters make up a small portion of\nthe overall parameters and are unaffected by the\nmethods introduced in this work, so we simply re-\nport parameter counts for the WN18RR dataset for\nbrevity.\nThe unsupervised Normalizing Flow technique\ncan be applied prior to training and thus introduces\nzero additional trainable parameters for the ranking\nmodel. The supervised MLP and Residual MLP\ntechniques introduce only 3.6% additional trainable\nparameters compared to the baseline model.\nDirectly fine-tuning the language model during\ntraining increases the number of trainable parame-\nters by 331.2% because even the BERT-base model\nis over 3 times the size of the ranking model. The\nparameter-efficient methods, on the other hand,\nhave a much more modest effect with the Lin-\near Probe increasing the parameters by 3.0% and\nPrompt Tuning increasing the model size by 1.2%.\nC.2 Training Time\nWe compare the training times across our different\nconfigurations. We report details for the candi-\ndate processing methods explored in Section 5 in\nTable 9. The normalizing flow technique has a neg-\nligible impact on training time because the unsu-\npervised technique can be applied prior to training.\nThe Residual MLP does increase the time per iter-\nation as observed by the increased time needed to\ncomplete one epoch. However, the Residual MLP\nalso accelerates convergence which largely offsets\nthe aforementioned slowdown. Across all datasets,\nthe Residual MLP outperforms the baseline even\nwhen controlling for wall clock time for all but the\nshortest of training times.\nWe report the training times for the query en-\ntity extraction methods explored in Section 6 in\nTable 10. The supervised methods introduce the ap-\nplication of a language model which also increases\nthe time per iterations as seen by the time needed\nto complete one epoch. The effect on accelerating\nthe convergence of the model is not as pronounced\nwhich means that in some cases, the supervised\nquery extraction techniques do meaningfully in-\ncrease the training time compared to the baseline.\n5949\nDataset # Nodes # Rels # Train # Valid # Test\nSNOMED-CT Core 77,316 140 502,224 71,778 143,486\nCN82K 78,334 34 81,920 10,240 10,240\nFB15K-237 14,451 237 272,115 17,535 20,466\nWN18RR 40,943 11 86,835 3,034 3,134\nTable 7: Dataset statistics\nConfiguration Trainable Params Delta (%)\nBERT-base\nBERT-ResNet 33.2M -\n+Normalizing Flow 33.2M 0%\n+Fine-tuning 143.1M 331.2%\n+Linear Probe 34.2M 3.0%\n+Prompt Tuning 33.6M 1.2%\n+MLP 34.4M 3.6%\n+Residual MLP 34.4M 3.6%\n+Prompt Tuning 34.8M 4.8%\nBERT-large\nBERT-ResNet 58.9M -\n+Residual MLP 61.0M 3.6%\n+Prompt Tuning 61.6M 4.7%\nTable 8: Parameter Counts for WN18RR Models\nD Additional Figures\nD.1 Effect Of Embedding Processing\nTechniques\nWe report the embedding metrics across all datasets\nin Figure 5.\nD.2 Effect Of Query Extraction Techniques\nWe report the performance of different query entity\nextraction techniques broken down by the connec-\ntivity of the query entity in Figure 6.\nE Implementation Details\nWe outline our implementation details below. We\nbegin by outlining the details shared across all ex-\nperiments and then outline the details specific to\nthe experiments performed for each of the experi-\nments.\nE.1 Training Procedure\nWe train all ranking models for a maximum of\n200 epochs and terminate training if the validation\nMRR has not improved for 20 epochs. We evaluate\nthe model with the highest validation MRR upon\nthe test set.\nWe use a batch size of 64 with the 1vsAll train-\ning strategy (Ruffinelli et al., 2020) with the binary\ncross entropy loss function. We use the Adam op-\ntimizer (Kingma and Ba, 2015) with decoupled\nweight decay regularization (Loshchilov and Hut-\nter, 2019). We set the learning rate to 1e-3 and set\nthe weight decay coefficient to 1e-4. We reduce\nthe learning rate by a factor of 0.5 if the validation\nMRR has plateaued for 3 epochs. We use label\nsmoothing with a value of 0.1, clip gradients to a\nmax value of 1.\nE.2 BERT-ResNet\nWe reuse the reported hyperparameters from\nLovelace et al. (2021) for the BERT-ResNet rank-\ning architecture which we redescribe here. We set\nf = 5where f is the hyperparameter that controls\nthe side length of the spatial feature map produced\nby the initial 1D convolution. We setN = 2where\nN controls the depth of the convolutional network.\nOur BERT-ResNet model then consists of3N = 6\nbottleneck convolutional blocks. The dimensional-\nity of the model is simply determined by the dimen-\nsionality of the language model, e.g. d= 768for\nexperiments with BERT-base and PubmedBERT\nand d = 1024 for experiments with BERT-large.\nWe apply dropout with drop probability 0.2 after\nthe embedding layer and apply 2D dropout (Tomp-\nson et al., 2015) with the same probability before\nthe convolutions. We apply dropout with probabil-\nity 0.3 after the final fully connected layer. These\nhyperparameter values are simply the default val-\nues reported by Lovelace et al. (2021).\nE.3 Candidate Retrieval\nWe describe implementation details pertinent to\nthe experiments conducted in Section 5. To isolate\nthe impact of the structure of the entity embedding\nspace, we utilize a single shared bias term across all\nentities instead of the per-entity bias term utilized\nby Lovelace et al. (2021). Thus the entity ranking\nis determined entirely by the query vector and the\nentity embeddings. All future experiments also use\nthis shared bias term.\nFor all of our embedding processing techniques,\n5950\nFigure 5: Intrinsic evaluation of embedding processing techniques. We note the MRR for each approach in parenthesis.\nFigure 6: Performance delta of different extraction techniques across queries of varying connectivity. Error bars indicate 95%\nconfidence intervals.\n5951\nConfiguration SNOMED CT Core CN-82K\nWall Clock Time Wall Clock Time\nPer Epoch Best Validation MRR Breakeven Point Per Epoch Best Validation MRR Breakeven Point\nBERT-ResNet 12m31s 22h57m49s - 4m6s 5h33m38s -\n+Normalizing Flow 12m38s 28h38m39s 1h2m52s 4m7s 5h21m56s 1h6m55s\n+Residual MLP 22m9s 52h5m26s 1h6m46s 7m20s 5h38m22s 1h43m9s\nFB15k-237 WN18RR\nConfiguration Wall Clock Time Wall Clock Time\nPer Epoch Best Validation MRR Breakeven Point Per Epoch Best Validation MRR Breakeven Point\nBERT-ResNet 11m52s 25h7m37s - 3m56s 11h40m37s -\n+Normalizing Flow 11m50s 18h10m8s 35m32s 3m56s 10h10m7s 43m55s\n+Residual MLP 14m31s 15h0m48s 14m31s 5m44s 11h5m2s 28m37s\nTable 9: Run time for best supervised and unsupervised processing techniques from Section 5. We report the\naverage wall clock time per epoch, the total time until the peak validation MRR, and the breakeven point where the\nconfiguration begins to outperform the baseline.\nConfiguration SNOMED CT Core CN-82K\nWall Clock Time Wall Clock Time\nPer Epoch Best Validation MRR Breakeven Point Per Epoch Best Validation MRR Breakeven Point\nBERT-ResNet 12m32s 34h17m41s - 4m1s 5h25m20s -\n+Linear Probe 18m51s 42h8m20s 20h29m38s 6m25s 4h52m17s 4h52m17s\n+Prompt-tuning 26m10s 60h48m38s 41h37m57s 8m42s 11h2m26s 5h40m58s\nFB15k-237 WN18RR\nConfiguration Wall Clock Time Wall Clock Time\nPer Epoch Best Validation MRR Breakeven Point Per Epoch Best Validation MRR Breakeven Point\nBERT-ResNet 11m52s 15h2m11s - 3m54s 10h42m4s -\n+Linear Probe 23m19s 23h42m57s N/A 6m3s 10h8m40s 6h36m41s\n+Prompt-tuning 32m43s 47h22m32s N/A 8m47s 20h36m0s 14h1m5s\nTable 10: Run time for query entity extraction techniques from Section 6. We report the average wall clock time\nper epoch, the total time until the peak validation MRR, and the breakeven point where the configuration begins to\noutperform the baseline.\nwe decouple the entity embeddings fed to the con-\nvolutional model and the entity embeddings used\nfor candidate ranking. All of our transformations\nare only applied to the entity embeddings used for\ncandidate ranking.\nE.3.1 Normalizing Flow\nWe define the normalizing flow with the transforma-\ntion T−1(x) =Wx + b where W ∈Rd×d and\nx,b ∈Rd4. To ensure the invertibility of W and\nto simplify the computation of the Jacobian deter-\nminant, we follow Kingma and Dhariwal (2018)\nand parameterize W using its LU decomposition.\nso W = PL(U + diag(s)) where P ∈Rd×d is a\npermutation matrix, L ∈Rd×d is a lower triangular\n4This transformation consistently outperformed more ex-\npressive nonlinear flows (e.g. GLOW (Kingma and Dhariwal,\n2018)) in our preliminary experiments. It’s possible that a\nmore comprehensive exploration of flow architectures and\nhyperparameter choices would lead to improvements over our\ndesign, but we leave such an exploration to future work.\nmatrix with ones on the diagonal, U ∈Rd×d is a\nstrictly upper triangular matrix, and s ∈Rd is a\nvector. During the training process, we fix P and\nlearn the parameters for L, U, and s.\nWe train the Normalizing Flow on the set of\nentity embeddings with a batch size of 64 for a\nmaximum of 500 epochs using a learning rate of\n1e-3 with the Adam optimizer (Kingma and Ba,\n2015). We clip gradients to a max value of 1 and\nuse the checkpoint that acheived the lowest train-\ning loss to transform the embeddings for candidate\nranking. We normalize the transformed embed-\ndings to have unit norm before use in candidate\nranking so an entity embedding, ei, is transformed\nas ˜ ei = T−1(ei)\n∥T−1(ei)∥2\n.\nE.3.2 MLP and Residual MLP\nFor the supervised transformations, we set the di-\nmensionality of the hidden layer to match the di-\nmensionality of the entity embeddings. We use a\n5952\nReLU nonlinearity and apply dropout with drop\nprobability 0.1 after the first projection. We found\nit necessary to reduce the learning rate for the MLP\nto stabilize training so we set the learning rate to\n1e-4 for the MLP parameters. For the residual MLP,\nwe also initialized the final linear layer to zeros so\nthat the candidate embeddings were equivalent to\nthe original embeddings at the start of training. All\nother hyperparameters remained fixed.\nE.4 Embedding Extraction Ablation\nWe describe implementation details pertinent to\nthe experiments conducted in Section 5.5. We use\nthe HuggingFace Transformers library (Wolf et al.,\n2020) to work with pretrained language models.\nFor this set of experiments, we utilize the normaliz-\ning flow technique for candidate ranking to isolate\nthe effect of the extraction techniques. For the\nsupervised extraction experiments, we utilize the\nmost effective unsupervised embeddings with the\nnormalizing flow for candidate ranking.\nE.4.1 MLM Pre-training\nWe fine-tune the language models using the MLM\npretraining objective over the set of textual entity\nidentifiers. We fine-tune the language models for 3\nepochs with a batch size of 32 and a learning rate of\n3e-5. We use a linear learning rate warmup for first\n10% of the total training steps. For SNOMED-CT\nCore, CN82K, and WN18RR we set the maximum\nsequence length to 64. For FB15k-237, we set the\nmaximum sequence length to 256 to account for the\nlonger entity descriptions. All other hyperparame-\nters follow the default values from Huggingface.\nE.5 Query Entity Extraction\nE.5.1 Linear Projection\nWe learn a linear projection that is applied to every\nhidden state of the frozen model as˜hl,j = hl,jW⊺+\nb where hl,j ∈Rd, W ∈Rd×d, and b ∈Rd. We\nthen max-pool across every token in each layer\nto produce a single feature vector for each layer ,\n˜hl. and aggregate these features using a learned\nlinear combination across layers ˜ ei = ∑L\nl=1 λl·˜hl\nwhere λl = softmax(a)l and a ∈RL is a learned\nvector of scalars. We set the learning rate for the\nparameters for embedding extraction to 5e-5.\nE.5.2 Prompting\nWe learn continuous prompts that we prepend to\nthe language model inputs at every layer to prompt\nthe frozen model (Li and Liang, 2021). We param-\neterize the prompt embeddings, pi,j ∈Rd′\n, in a\nlow-dimensional space where d′ < d, and learn\nan MLP with one hidden layer to project them to\nthe dimensionality of the language model. We set\nd′= 256in this work and apply dropout with drop\nprobability 0.1 before the MLP and after the first\nprojection. The dimensionality of the hidden layer\nis set to d/2. We also apply a shared layer normal-\nization layer to the output of the MLP.\nTherefore the input to the ith\nlayer of the language model is\nsi= [LN(MLP(pi,0)),..., LN(MLP(pi,k)),xi,0,..., xi,n]\nwhere LN(MLP(pi,j)) ∈Rd and xi,j ∈Rd are\nthe transformed prompt token and tokenized entity\nembedding respectively for the jth position at the\nith layer. We use k = 3 prompt tokens across\nall experiments in this work. We extract the\nentity representation by mean pooling across all\nintermediate states in each layer and aggregate\nacross layers with a learned linear combination.\nWe set the learning rate for the parameters for\nembedding extraction to 5e-5.\nE.6 Effect of Language Model Selection\nWe describe implementation details pertinent to the\nexperiments conducted in Section 7. For the unsu-\npervised embedding extraction, we utilize mean-\npooled embeddings from language models with\nadditional MLM pretraining upon the set of entity\nnames. All other hyperparameters are kept constant\nfrom earlier sections.\nE.7 Ensembling\nFor our ensembling experiment, we train a trans-\nformer model that accepts a [CLS] token, the em-\nbedded query entity, and the embedded relation\nentity. This can be viewed as a simplified version\nof the HittER model from Chen et al. (2021) that\ndoesn’t utilize any additional graph context. The\n[CLS] embedding output from the final layer is\nused for candidate scoring.\nWe tune hyperparameters by running 20 trials of\na random search over the grid of hyperparameters\ndefined in Table 11. All models are trained for a\nmaximum of 200 epochs with the AdamW opti-\nmizer. We linearly warm up the learning rate for\nthe first 4000 steps before annealing it with a cosine\ndecay schedule over the rest of training. We clip all\ngradient norms to 1 and apply early stopping with\na patience of 50 epochs.\n5953\nHyperparameter Search Range Selected Value\nFB15k-237 WN18RR\nLearning Rate [3e-3, 1e-3, 5e-4, 3e-4, 1e-4] 3e-4 3e-3\nWeight Decay [.3, .1, .03, .01, .001, 1e-4, 1e-5] .01 0.1\nOutput Dropout [.1, .2, .3, .4, .5, .6, .7] .7 .5\nInput Dropout [.1, .2, .3, .4, .5, .6, .7] .6 .5\nLabel Smoothing [.1, .2, .3, .4, .5, .6] .2 .2\nNumber Layers [4,5,6] 6 5\nAttention Heads 8 8 8\nEmbedding Dim 320 320 320\nFeedforward Dim 1280 1280 1280\nTable 11: Hyperparameter Search Space for Transformer Model\nF Validation Results\nWe report the validation results corresponding to\nour final results reported in Table 5 in Table 12\n5954\nSNOMED CT Core CN-82K FB15K-237 WN18RR\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10\nBERT-ResNet + Normalizing Flow.510 .403 .568 .714 .196 .133 .216 .323 .362 .279 .393 .529 .582 .511 .610 .729BERT-ResNet + Prompt-tuning + Normalizing Flow.517 .411 .574 .719 .202 .137 .223 .329 .361 .278 .394 .530 .591 .521 .618 .736BERT-ResNet + Residual MLP .551 .445 .608 .754 .213 .142 .235 .356 .378 .286 .414 .564 .592 .521 .621 .737BERT-ResNet + Prompt-tuning + Residual MLP.551 .444 .612 .757 .218 .146 .240 .363 .377 .287 .410 .564 .600 .531 .626 .742\nTable 12: Validation results corresponding to results reported in Table 5.\n5955"
}