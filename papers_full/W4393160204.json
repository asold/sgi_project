{
    "title": "Detecting and Preventing Hallucinations in Large Vision Language Models",
    "url": "https://openalex.org/W4393160204",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5088137864",
            "name": "Anisha Gunjal",
            "affiliations": [
                "Meso Scale Discovery (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A3015065345",
            "name": "Jihan Yin",
            "affiliations": [
                "Meso Scale Discovery (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2164275388",
            "name": "Erhan Bas",
            "affiliations": [
                "Meso Scale Discovery (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5088137864",
            "name": "Anisha Gunjal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3015065345",
            "name": "Jihan Yin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2164275388",
            "name": "Erhan Bas",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4367367040",
        "https://openalex.org/W4379089709",
        "https://openalex.org/W4309088836",
        "https://openalex.org/W2793429163",
        "https://openalex.org/W4384111857",
        "https://openalex.org/W4225323055",
        "https://openalex.org/W4389523832",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W3032398409",
        "https://openalex.org/W4287674181",
        "https://openalex.org/W4319793302",
        "https://openalex.org/W2736601468",
        "https://openalex.org/W4379474731",
        "https://openalex.org/W4378771755",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4366330503",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4382334342",
        "https://openalex.org/W2973379954",
        "https://openalex.org/W4382323090",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4378770815",
        "https://openalex.org/W4223908421",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4376312115",
        "https://openalex.org/W4226278401"
    ],
    "abstract": "Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling (RS). We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores. The dataset is available at https://github.com/hendryx-scale/mhal-detect.",
    "full_text": "Detecting and Preventing Hallucinations in\nLarge Vision Language Models\nAnisha Gunjal*, Jihan Yin*, Erhan Bas†\nScale AI\nanishagunjal@utexas.edu, jihan yin@berkeley.edu, erhan.bas@gehealthcare.com\nAbstract\nInstruction tuned Large Vision Language Models (LVLMs)\nhave significantly advanced in generalizing across a diverse\nset of multi-modal tasks, especially for Visual Question An-\nswering (VQA). However, generating detailed responses that\nare visually grounded is still a challenging task for these mod-\nels. We find that even the current state-of-the-art LVLMs (In-\nstructBLIP) still contain a staggering 30 percent of the hal-\nlucinatory text in the form of non-existent objects, unfaith-\nful descriptions, and inaccurate relationships. To address this,\nwe introduce M-HalDetect, a Multimodal Hallucination De-\ntection Dataset that can be used to train and benchmark mod-\nels for hallucination detection and prevention. M-HalDetect\nconsists of 16k fine-grained annotations on VQA examples,\nmaking it the first comprehensive multi-modal hallucina-\ntion detection dataset for detailed image descriptions. Un-\nlike previous work that only consider object hallucination,\nwe additionally annotate both entity descriptions and rela-\ntionships that are unfaithful. To demonstrate the potential\nof this dataset for hallucination prevention, we optimize In-\nstructBLIP through our novel Fine-grained Direct Prefer-\nence Optimization (FDPO). We also train fine-grained multi-\nmodal reward models from InstructBLIP and evaluate their\neffectiveness with best-of-n rejection sampling (RS). We per-\nform human evaluation on both FDPO and rejection sam-\npling, and find that they reduce hallucination rates in Instruct-\nBLIP by 41% and 55% respectively. We also find that our\nreward model generalizes to other multi-modal models, re-\nducing hallucinations in LLaV A and mPLUG-OWL by 15%\nand 57% respectively, and has strong correlation with hu-\nman evaluated accuracy scores. The dataset is available at\nhttps://github.com/hendryx-scale/mhal-detect.\nIntroduction\nLarge language models (LLMs) have transformed the AI\nlandscape in recent years, scaling their training data to tril-\nlions of tokens and their parameter count to hundreds of\nbillions (Brown et al. 2020; Achiam et al. 2023; Touvron\net al. 2023). This has unlocked powerful emergent behav-\niors, and seen widespread adoption through the use of chat\nagents such as ChatGPT. Recently, advances in multi-modal\n*These authors contributed equally.\n†Work done at ScaleAI\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nmodels have seen adoption around grafting visual backbones\nonto pre-trained large language models, resulting in LVLMs\n(Liu et al. 2023b; Dai et al. 2023; Ye et al. 2023). While\nthis has led to strides in overall VQA performance, it brings\nalong the same challenges that plague these LLMs - a sig-\nnificant one being the propensity to generate hallucinations.\nIn language models, hallucinations occur when the model\nproduces inaccurate or misleading factual information that\ncannot be supported by existing knowledge stores (Ji et al.\n2023; Bang et al. 2023). In the context of VQA for LVLMs,\nhallucinations can manifest as responses containing refer-\nences or descriptions of the input image that are incorrect (Li\net al. 2023). It is essential to address and mitigate these hal-\nlucinations to enhance the reliability and accuracy of multi-\nmodal models in real-life usecases. However, these multi-\nmodal hallucinations are hard to programatically detect and\noften requires human supervision, which can be costly.\nTo facilitate automatic hallucination detection, we build a\ndiverse human-labeled dataset using VQA responses from\nInstructBLIP, as seen in Figure 1. We train multiple re-\nward models of various densities (sentence and sub-sentence\nlevel) on this dataset for hallucination detection. An effective\nway to use these reward models to reduce hallucinations is\nto use them to generate rewards in a reinforcement learning\nsetup (Ziegler et al. 2019; Stiennon et al. 2020; Nakano et al.\n2021), although the resulting final model can only be as ef-\nfective as the original reward model used (Bai et al. 2022).\nTherefore, in this paper, we focus on measuring the quality\nof these reward models, exploring classification metrics, and\nusing best-of-n rejection sampling as an approximation of\nthe system’s performance. Similar to (Rafailov et al. 2023),\nwe also directly optimize InstructBLIP with fine-grained Di-\nrect Preference Optimization (FDPO), a novel variation of\nDPO in which we leverage fine-grained annotation informa-\ntion from individual examples, rather than collecting rela-\ntive preference signals from pairs of texts. Both methods\nshow significant success in reducing hallucination rates from\nInstructBLIP, and furthermore, rejection sampling with our\nreward models reduces hallucination rates in other multi-\nmodal models as well - LLaV A (Liu et al. 2023b) and\nmPLUG-OWL (Ye et al. 2023).\nOur main contributions are as follows:\n1. We create and release M-HalDetect, a new hallucination\ndetection dataset focused on fine-grained annotations at\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18135\n Analysis          \n   Accurate           \nInaccurate\nQuestion: \n<image> Give a detailed description of the image.\nLVLM Response:\nThe image depicts a busy city street ﬁlled with cars, traﬃc lights, and pedestrians.\nThe street is lined with buildings on both sides of the road, creating a scenic urban \nenvironment.\nThere are also trees visible in the background, adding to the natural beauty of the area.\nNo people are speciﬁcally mentioned in the image, but they can be seen in the crowd of \npedestrians crossing the street.\nOverall, the image captures the hustle and bustle of a busy city street, showcasing the \ndiversity and vibrancy of urban life.\nFigure 1: Example Annotation from the M-HalDetect Dataset. The sub-sentences of text generated by multi-modal LM are\ntagged into categories: Accurate, Inaccurate, andAnalysis.\na sub-sentence level over detailed image descriptions.\n2. We show that InstructBLIP can be optimized using Fine-\ngrained DPO (FDPO) using the M-HalDetect dataset to\nreduce hallucination rates by 41%.\n3. We show that reward models trained on this dataset can\nreduce hallucination rates by 55% in InstructBLIP with\nbest-of-64 rejection sampling. The reward model gener-\nalizes to other LVLMs, reducing hallucination rates in\nLLaV A and mPLUG-OWL by 15% and 57% respectively\nwith best-of-16 sampling.\n4. We show that our reward model is an effective evaluator\nof hallucination rates, giving scores aligned with human\nratings.\nRelated Work\nLarge Vision Language Models (LVLMs) have seen per-\nformative advancements in tasks such as generating text\nfrom images(Li 2023) and multi-modal in-context learn-\ning(Alayrac et al. 2022). Recent work has focused on uti-\nlizing instruction tuning techniques to enhance the zero-\nshot performance of instruction-aware LVLMs across dif-\nferent vision-language tasks(Liu et al. 2023b; Dai et al.\n2023). These approaches utilize GPT-4 to generate multi-\nmodal instruction tuning datasets(Liu et al. 2023b) where\nthe image context is provided to GPT-4 through symbolic\nrepresentations of the image such as captions and object\nbounding boxes. Others combine datasets across various\nmulti-modal tasks(Dai et al. 2023) with hand-crafted in-\nstructions, a method that has found success in training tradi-\ntional LLMs(Wei et al. 2021). This achieves state-of-the-art\nperformance in a variety of multi-modal tasks such as visual\nand video QA, image captioning and classification.\nNevertheless, a significant challenge associated with\nLVLMs has emerged: preventing hallucinations when gen-\nerating textual output. It is essential to address and mitigate\nthese hallucinations to enhance the reliability and accuracy\nof LVLMs in production use cases.\nHallucination Analysis in LVLMs In (Li et al. 2023), the\nevaluation metric ”POPE” is proposed to evaluate hallucina-\ntions in LVLMs by polling questions about generated text.\nThey observed that current state-of-the-art LVLM (Instruct-\nBLIP) has the lowest object hallucination rates among re-\ncent LVLMs. Another relevant contribution by Liu et al. (Liu\net al. 2023a) is the introduction of the LRV dataset. This\ndataset contains positive and negative instructions specifi-\ncally designed to enhance the robustness of LVLMs against\nhallucination and inconsistent text generation. Furthermore,\nthey proposed a method called GA VIE, which leverages\nGPT-4 to assist in evaluating preferred answer generations.\nThese studies collectively contribute to the understand-\ning and mitigation of hallucination-related challenges in\nLVLMs, by providing evaluation metrics, datasets, and eval-\nuation methods that enhance the reliability and consistency\nof text generation in multi-modal models. Our work extends\nthe scope of the previous works by not only considering hal-\nlucinations on the presence of objects, but also on descrip-\ntions of objects such as relative positioning or attributes. We\nalso consider hallucinations on complex object reasoning.\nAligning to Human Preferences Despite having strong\nzero-shot performance on classical language benchmark\ndatasets, pre-trained LLMs still struggle to produce detailed\ngenerations on par with those written by real humans. Super-\nvised fine-tuning on demonstration data written by humans\nis not enough, where recent works have focused on using\nReinforcement Learning with Human Feedback (RLHF) to\naddress this problem (Stiennon et al. 2020; Touvron et al.\n2023; Ouyang et al. 2022; Achiam et al. 2023).\nRLHF typically uses Proximal Policy Optimization\n(Schulman et al. 2017), to optimize a policy model with re-\nwards from a reward model. This reward model is typically\ntrained on preference pairs of same-prompt generations, of-\nten sourced from the base policy model. This preference is\nusually given by humans, though attempts have been made\nto use more traditional metrics such as BLEU (Papineni et al.\n2002) and ROUGE (Ganesan 2018) as proxies. Using hu-\nman preferences is more effective in aligning LLMs to hu-\nman preferences (Stiennon et al. 2020), though sees mixed\nresults in hallucination prevention. Ouyang et al. (Ouyang\net al. 2022) found that RLHF helps smaller (6B) language\nmodels reduce their hallucination rate, while having the op-\nposite effect on larger models (175B). In this paper, we will\nfocus on relatively smaller multi-modal models (7B) that can\nbe more accessible to end users.\nDPO has emerged recently as a viable alternative to RLHF\nfor preference alignment, optimizing the policy model di-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18136\nrectly without needing to train a reward model and sam-\nple rewards through reinforcement learning (Rafailov et al.\n2023). It has shown comparable performances with RLHF\nin summarization and chatbot usecases on language mod-\nels, and maintains strong performance in higher temperature\nsampling. At the same time, it avoids the unstable and brittle\nprocess of training models with RL (Engstrom et al. 2020).\nFine-grained Preferences A limitation of both RLHF and\nDPO is their lack of fine-grained interpretability regarding\nwhat makes one generation more preferred than the other.\nRecent research has made significant progress in leveraging\nfine-grained user preferences to improve the performance\nand interpretability of reward models. For example, Wu et\nal. (Wu et al. 2023) utilize fine-grained human feedback\nto train multiple reward models at different density levels.\nThese reward models covered passage level preferences as\nin the traditional RLHF setting, but also sentence level and\nsub-sentence level preferences in the form of error identifi-\ncation. (Lightman et al. 2023) employs process supervision,\nproviding human feedback on individual steps for more ro-\nbust rewards.\nTo extend this fine-grained feedback mechanism into the\nmulti-modal domain, we introduce a new dataset for multi-\nmodal hallucination detection. Our dataset comprises of\n4,000 images with 4 detailed descriptions each, for a to-\ntal of 16,000 image description pairs, annotated at the sub-\nsentence level to indicate the accuracy of the generated\ndescriptions. Similarly to (Wu et al. 2023), we train sub-\nsentence and sentence level reward models on this dataset.\nWe also modify the DPO loss to utilize fine-grained annota-\ntions.\nM-HalDetect : Multi-Modal Hallucination\nDetection Dataset\nDataset Description In this section, we introduce the M-\nHalDetect dataset that incorporates fine-grained annotations\nfor identifying hallucinations in detailed image descriptions\ngenerated by LVLMs. The dataset comprises of image-\ndescription pairs sampled from 4,000 images taken from the\nval2014 split of the Common Objects in Context (COCO)\ndataset (Lin et al. 2014). The dataset is divided into a train-\ning set with 3,200 images and a development set with 800\nimages.\nWe choose to utilize the validation set of COCO to avoid\npotential training data regurgitation from LVLMs trained on\nthe COCO training set. This is roughly 10% of the original\nCOCO validation set, leaving enough data untouched to not\nimpact further validation too heavily.\nTo generate responses, we prompt InstructBLIP (Dai et al.\n2023) with each image and a randomly selected question\nfrom a pool of instructions for describing an image. We ini-\ntially reuse instructions from ones used in InstructBLIP’s de-\ntailed image description training data, which were sourced\nfrom the LLaV A-150k (Liu et al. 2023b) dataset. During ini-\ntial analysis, we observed that doing so led to less diverse re-\nsponses, potentially due to the influence of this dataset dur-\ning training. To address this, we added in our own prompts\nto improve generation diversity. Refer to the appendix??for\ndetails on dataset and diverse prompt generation, training,\nand inference analysis.\nWe sample four responses using nucleus sampling from\nInstructBLIP with a temperature value set to 1.0. This\ncreates 16k image-prompt-response triplets, split between\n12800 samples in the train split and 3200 samples in the\nval split.\nDataset Categories The annotation process involves cate-\ngorizing different segments of each response into three cat-\negories: (i) Accurate, (ii) Inaccurate, and (iii) Analysis. We\nalso include an Unsure category for ambiguous cases. We\ndefine the classes as follows:\n• Accurate Objects exist in the image, their descriptions\nare accurate according the image, and any described re-\nlationships can be accurately inferred from the image.\n• Inaccurate Objects do not exist in the image or their\ndescriptions are inaccurate. Furthermore, if the analysis\nabout the image is not plausible, it is also marked as In-\naccurate.\n• Analysis Scene or object analysis including complex\nreasoning or interpretations about the image. These are\nportions of the data that are more subjective and not\ngrounded visually within the image.\n• Unsure This category is reserved as a last resort if an-\nnotators cannot make a judgment about the sentence seg-\nment into one of the above three categories.\nWe provide fine-grained annotations for these 3 categories\non the detailed descriptions of images generated by the\nLVLM. The annotations are provided at sub-sentence level\n- i.e. one sentence can comprise of multiple segments from\ndifferent classes, as seen in Figure 1.\nTo make the annotation process user-friendly, we allow a\nleeway to the annotators to miss a few words in the anno-\ntations if there are too many segments in a sentence to be\nannotated. The unmarked words in a sentence are by default\nconsidered as ”Accurate”. In our analysis, we noticed that\nsometime annotators skip annotating punctuation, connec-\ntor words, or introductory sub-sentences such as ”The image\nfeatures” (illustrated in Figure 1).\nDataset Collection To collect the annotations, we em-\nployed Scale AI’s RAPID(ScaleAI 2023) labeling tool and\ninvolved 10 randomly selected human annotators. These an-\nnotators had to qualify by passing a training course with a\nminimum accuracy of 85% on the example tasks to be se-\nlected for the final tagging task. The annotators are presented\nwith an image and four responses about the image gener-\nated by InstructBLIP. Their task is to annotate segments of\nthe sentence into one the categories. An example annotation\ntask is illustrated in Figure 1.\nMethod\nMulti-Modal Reward Model\nWe implement a multi-modal reward model for detecting\nthe presence of hallucinations generated by LVLMs. Specif-\nically, we reuse the InstructBLIP weights and architecture,\nswapping the final embedding layer with a classification\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18137\n20000\n15000\n10000\n5000\n0\n0.0 0.2 0.4 0.6 0.8 1.0\nFigure 2: Label density histogram for the Inaccurate class.\nThe x-axis represents the percentage of a sentence that is an-\nnotated as Inaccurate and the y-axis represents the frequency\nof such sentences in the dataset.\nhead. We do this as initializing the reward model from the\ngenerative model weights improves training robustness and\nreward generalization in later RL (Zheng et al. 2023). In-\nstructBLIP consists of an image encoder that extracts im-\nage features and a linear mapping layer that projects these\nfeatures. These image feature are passed to an instruction-\naware attention layer, the QFormer, that attends instructions\nover the projected image features. The QFormer outputs are\npassed to a frozen pretrained decoder as soft prompts, pre-\nfixed to the instruction. For this paper, we choose to use Vi-\ncuna (Chiang et al. 2023) as the frozen decoder following\nthe original InstructBLIP.\nWe train reward models at sentence level and sub-\nsentence level densities. For each image-text pair, we run\none forward pass similar to (Lightman et al. 2023), and set\ntarget class labels at the token concluding each segment,\nmasking out all other indices in the segment. We optimize\nwith cross-entropy loss. We fine-tune the entire decoder and\nreward model head, while freezing the rest of the model. Ab-\nlations on model freezing, hyperparameters as well as details\non training can be found in the extended version.\nSentence-level Reward Prediction\nWe condense the labeled sub-sentence segments in M-\nHalDetect into sentence-level segments for a more struc-\ntured reward format - this makes it more straightforward\nto run rejection sampling and train with RL, without wor-\nrying about localizing proper segments. We identify these\nsentences using the Natural Language Toolkit(Bird, Klein,\nand Loper 2009). For each sentence, if there is any segment\nthat is inaccurate, we label the entire sentence as inaccu-\nrate. While this may introduce some noise when converting\npartially inaccurate sentences, we see in Figure 2 that the\nfrequency of such sentences is low. Furthermore, if a sen-\ntence has a segment with the ”unsure” category, we merge\nthat sentence into the inaccurate class. We experiment with\ntwo levels of label granularity with this dataset:\n• Binary Classification: Condense Analysis and Accurate\nclasses into the Accurate class. In this setting we have\ntwo classes: Accurate and Inaccurate\n• Ternary Classification: In this setting, we have three\nclasses: Accurate, Inaccurate and Analysis.\nTrue Label\nINACCURATE\nPredicted Label\nBINARY CLASSIFIER\nINACCURATEACCURATE\nACCURATE\nACC INACCANALYSIS\nPredicted Label\nACCINACC ANALYSIS\nTERNARY CLASSIFIER\n8379\n1947 2492\n908\n3971 635\n243418\n566\n23757131351 \n3454\n8478 809 \n2064\nFigure 3: Confusion Matrix comparison between Binary and\nTernary Classifiers. The right plot represents the binary clas-\nsifier labels derived from the ternary classifier by merging\nthe Accurate and Analysis classes.\nSegment-level Reward Prediction\nWe also train a finer-grained reward model that make hallu-\ncination judgments on segments of sentences as opposed to\nentire sentences. This can provide less noisy signal when\ntraining on annotations, especially with longer compound\nsentences and hallucinations isolated to small portions of a\nsentence. We train on this data in a similar fashion to the sen-\ntence level rewards, by labeling the end token index of each\nspan or segment of annotated text into its corresponding la-\nbel. We then mask out every other index in the sequence.\nAs a baseline, we assume perfect localization of the anno-\ntation segments as an upper bound for the performance of\nthis method. Future works can consider training a segment\nlocalization model in parallel with the reward model, to de-\ntect when hallucinations start and end. Since we do not do\nthis, we cannot use this reward model for rejection sampling,\nand evaluate purely on classification metrics over the test\nset. Similar to sentence-level reward prediction baselines,\nwe also experiment with the binary and ternary variants of\nthe segment-level reward prediction models.\nRejection Sampling (RS)\nWe use the trained reward models to perform rejection sam-\npling on the generations of InstructBLIP to promote selec-\ntion of less hallucinatory responses. We do this on the pas-\nsage level, computing reward scores for the whole gener-\nation at once. We calculate the reward score by averaging\nthe non-hallucination negative log probabilities of each sen-\ntence. This represents the normalized negative log probabil-\nity of the entire passage containing no hallucinations. We\ncompute rejection sampling in a best-of-n and worst-of-n\nsetting, for n = 16 , 64, to study the ability of the reward\nmodel in selecting the best generations from InstructBLIP,\nand the variance in quality between generations.\nAs we train two types of sentence level reward models\n(binary and ternary, including the analysis class), we exper-\niment with using both models for reward scoring. We found\nin our initial experiments that although the binary reward\nmodel is able to penalize hallucinations with low scores, it\ntends to give very high scores towards the analysis class. We\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18138\nThe image features a person riding a green motorcycle on a race track.\nThe motorcycle is leaning to the left, and the person is wearing a helmet, gloves, and a backpack.\nThe motorcycle is towards the right side of the image and the person appears to be in the middle of a turn. \nThere are two other motorcycles visible in the scene, one closer to the left side and other closer to \nthe right side of the image.\nIn addition to the motorcycle, there are several chairs scattered throughout the scene, possibly \nbelonging to spectators or screw members.\nPrompt: <image>\\nDescribe the\n following image.\nReward Model Fine-grained \n Predictions\nMost Preferred Least Preferred \nThe motorcycle is leaning sharply as the rider takes a turn, making for an exciting scene.\nThe image depicts a person riding a green motorcycle on a race track.\nThe rider is wearing a helmet and gloves, ensuring their safety during the high speed maneuver.\nIn the background, there are several other motorcycles visible, some of which are closer to \nthe viewer while others are further away.\nA grassy area can also be seen in the background providing a natural setting for the race.\nACC\nACC\nINACC\nINACC\nINACC\nACC\nACC\nACC\nINACC\nINACC\nANLY Overall, the image captures the excitement and adrenaline of a competitive motorcycle race.\nAverage NLL Score: -1.041 \nAverage NLL Score: -2.771 \nFigure 4: Rejection sampling examples with ternary reward model labels per sentence. Score for each response is computed\nusing the average negative log-probability per sentence of a hallucination.\ntheorize that it is much easier to detect non-hallucinogenic\nanalysis over factual descriptions, and as a result the bi-\nnary reward model scores are biased towards generations\nthat contain more subjective analysis rather than objective\ndescriptions. This is less of a problem with the ternary re-\nward model, as analysis has been split into its own class. As\nwe will discuss in the results, the ternary model’s function-\naltiy is a superset of the binary model. For these reasons, we\nchoose to use the ternary reward model for rejection sam-\npling moving forward.\nTo study our the robustness of our reward model and our\ndataset, we conduct rejection sampling on generations from\nother LVLMs, namely LLaV A and mPLUG-OWL. For these\nexperiments, we reuse the reward model initialized from In-\nstructBLIP.\nFine-grained Direct Preference Optimization\nWhile we train a reward model to show the potential of opti-\nmizing against hallucinations with RL, we also directly op-\ntimize InstructBLIP using FDPO to reduce hallucinations.\nSince M-HalDetect does not contain the traditional pref-\nerence pairs used in DPO and RLHF, we explicitly segment\neach generation into sequences of preferred, dispreferred,\nand neutral chunks. We then reuse the DPO loss in increas-\ning the likelihoods of preferred chunks while decreasing the\nlikelihood of dispreferred chunks, each regularized by the\noriginal likelihood from the base model for the correspond-\ning chunk, while neutral chunks are ignored. Similar to (Wu\net al. 2023), this should give stronger signal during training\nin reducing hallucinatory generations as compared to using\npairs of likelihoods over entire generations.\nRecall the loss used in DPO, with πref as the reference\nmodel, πθ as the policy model, x being the input, yw being\nthe preferred generation, and yl being the dispreferred gen-\neration.\nLDPO (πθπref) = −E(x,yw,yl)∼D [log σ (∆r)]\n∆r = β log πθ (yw | x)\nπref (yw | x) − β log πθ (yl | x)\nπref (yl | x)\nSince we don’t have preferences over pairs of generations,\nbut spans of fine-grained preferences throughout each gen-\neration, our FDPO loss can be modeled as\nLFDPO (πθ; πref) = −E(x,y,c)∼D [log σ (βk)]\nk =\n\n\n\n−r c = 0\nr c = 1\n−∞ c >1\n, r = log πθ (y | x)\nπref (y | x)\nwith sample segments x, y, cbeing drawn from the\ndataset. Here, x is the entire input up until the start of the\ncurrent segment, y is the generated segment, and c is the\nclass of the current segment, with c = 1 being the preferred\nclass, c = 0 being the dispreferred class, and all other classes\nbeing ignored. Since segments are non-overlapping, we can\nrun a single forward pass for each sample to calculate the\nloss of all segments within the sample all at once.\nThis formulation allows us to categorize each class into\npositive, negative, or neutral signal, the latter of which will\nbe ignored during training. We run ablations on including\nthe analysis class as either a negative or neutral class when\noptimizing InstructBLIP with FDPO. We fine-tune only the\nQFormer and language head, keeping the rest of the model\nfrozen. We use β = 0.5 for all our FDPO experiments, and\ntrain for a maximum of 5 epochs with lr = 10−6, warmup\nratio of .03, and a cosine scheduler.\nEvaluation\nRecent works in multi-modal LLMs(Liu et al. 2023b,a)\nsometimes use GPT-4 as a human proxy to qualitatively\nevaluate LM outputs. Specifically, GPT-4 is prompted to\ngive a preference score to a LM generation, either as a stand-\nalone or compared against GPT-4’s own generation. This\nmetric enables automatic evaluation without depending on\nhuman evaluators.\nHowever, this is plagued with systematic bias such as sen-\nstitivity to the ordering of responses (Wang et al. 2023). Fur-\nthermore, GPT-4’s public API does not yet support image in-\nputs. Recent multi-modal works instead pass image context\nin the form of captions and object bounding boxes. In sev-\neral cases, this symbolic input cannot represent the image\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18139\nModel Type Method RM Score ↓ Human Eval ↑\nInstructBLIP Baseline Baseline (T=0) 0.97 0.71\nInstructBLIP DPO IA Finetune Qformer (T=0) 0.48 0. 83\nInstructBLIP DPO IA Finetune Qformer (T=1) 0.72 0.75\nInstructBLIP DPO DA Finetune Qformer (T=0) 0.85 0.70\nInstructBLIP DPO DA Finetune Qformer (T=1) 1.03 0.58\nInstructBLIP RS Best of 64 0.26 0. 87\nInstructBLIP RS Worst of 64 1.76 0.53\nInstructBLIP RS Best of 16 0.36 0.82\nLLaV A Baseline Baseline (T=0) 0.383 0.805\nLLaV A RS Best of 16 0.159 0.834\nmPLUG-OWL Baseline Baseline (T=0) 1.26 0.476\nmPLUG-OWL RS Best of 16 0.595 0.707\nTable 1: Results of reward model and human evaluation scores. The RM Score is the average negative log probability of the\npassage not containing hallucinations, while the human evaluation score is the percentage of content that was truthful. A perfect\nRM score would be 0, and a perfect human evaluation score would be 1.\nrobustly and leads to incorrect evaluations. We performed\na qualitative analysis on GPT-4’s performance on LLaV A-\n150k’s detail subset and noted that GPT-4 gave frequent in-\naccurate scores and explanations, failing to detect halluci-\nnations while incorrectly penalizing correct generations. For\nthis reason, we do not use GPT-4 for automatic evaluation of\ngeneration quality.\nTo combat these limitations, we use human evaluation to\nevaluate the hallucination rates of our rejection sampling and\nDPO generations. Following the same labeling instructions\nas the M-HalDetect, we annotate the generations into accu-\nrate, inaccurate, and analysis spans. For generations from\nour DPO model, we use temperature=1 and nucleus sam-\npling. We apply this across 50 different images sourced from\nCOCO’s validation set, separate from the ones used in M-\nHalDetect, though we reuse instructions from the dataset.\nA common trade-off between reducing hallucinations is\na reduction in helpfulness. Consider, for example, a model\nthat outputs nothing - it does not hallucinate, yet it is not\nhelpful either. To avoid this potential bias in our evaluation,\nwe choose to measure the hallucination rate as the number\nof inaccurate words divided by the number of total words,\nexcluding analysis segments, to calculate what percentage\nof descriptive objective content contained hallucinations.\nResults\nReward Model Classification Metrics\nWe evaluate the multi-modal reward models (sentence-level\nand segment-level) using the development split of the M-\nHalDetect Dataset. We report Accuracy and F-1 Score for\neach of the training strategies. All models are initialized\nwith pre-trained InstructBLIP weights, and the results are\nreported in Table 2.\nAlthough the binary version has higher accuracy and F1\nthan the ternary in both sentence and segment level applica-\ntions, we see in Figure 3 that the ternary reward model ac-\nType Density Accuracy F1 Score\nBinary Sentence Level 79.2 78.37\nTernary Sentence Level 71.4 70.8\nBinary Segment Level 83.92 83.22\nTernary Segment Level 77.2 76.93\nTable 2: Baseline Reward Model Results\ntually performs about the same as the binary reward model,\nif we were to reduce from a ternary to a binary setting. The\nternary model additionally learns to separate the Accurate\nand Analysis classes, and we use it for rejection sampling\nand reward scoring experiments moving forward.\nHuman Evaluation\nFigure 4 illustrates an example of rejection sampling using\nfine-grained feedback from the reward model. The reward\nmodel can accurately flag hallucinatory sentences which\nincorrectly claims the presence of other motorcycles and\nchairs. Furthermore, it is also able to flag sentences that gen-\nerate analysis about non-existent objects.\nWe observe in Table 1 that rejection sampling signifi-\ncantly improves the factual rate of InstructBLIP’s outputs.\nOn the other hand, the worst generations of InstructBLIP\ncan be extremely poor, with an almost 50% hallucination\nrate! We can see from both the human eval results and our\nreward model scores in Figure 6 that we get exponentially\ndiminishing returns as the sample size increases.\nRejection Sampling We also see that rejection sampling\nwith InstructBLIP manages to reduce hallucination rates\nfor LLaV A and significantly for mPLUG-OWL. This shows\nthat although M-HalDetect’s image descriptions are sourced\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18140\nFigure 5: Human evaluation scores against reward scores for\nall human evaluated results.\nReward Model Score\nN = number of generations per sample\n-0.2\n-0.4\n-0.6\n-0.8\n-1.0\n-1.2\n-1.4\n-1.6\n2  4   8 16 32 64 1\nFigure 6: Reward model score means and variances as n in-\ncreases in best-of-n rejection sampling. We see diminishing\nreturns as we increase n.\nfrom InstructBLIP, they can still be used successfully in\nevaluating and improving on other LVLMs. It is interest-\ning to see LLaV A’s baseline model performing so strongly -\nwe suspect this is because LLaV A is trained specifically for\ngenerating detailed descriptions, whereas InstructBLIP and\nmPLUG-OWL are more general models with a wide range\nof task applicability.\nAdditionally, we study the correlation between reward\nmodel and human evaluation scores. In Figure 5, we see\nthat across all human evaluated results, there is a clear and\nstrong correlation between our reward model scores and hu-\nman accuracy scores. Although this is by no means a robust\nreplacement for human annotations, this shows the potential\nof training models as specific evaluators for hallucinations.\nDespite the noisiness, such a model could be used for early\nhyper-parameter selection, being much more cost-effective\nthan humans evaluation.\nFine-Grained DPO We evaluate two variations of FDPO\nacross the three classes - one that ignores analysis (IA), and\none that disprefers analysis (DA), merging it with the inac-\ncurate class. We see in Table 1 that marking analysis as a\nnegative class does not impact hallucination rates in a sig-\nnificant way when training with FDPO, and may actually\nworsen rates at higher temperatures. We suspect that this\nmay be because InstructBLIP’s generations often have the\nlast sentence being subjective analysis of the image, fol-\nlowed by an end of sequence token. Pushing down the likeli-\nhoods of generating this sentence increases the likelihood of\nthe generation being lengthened, potentially inducing addi-\ntional hallucinations as the model runs out of accurate con-\ntent to describe.\nOn the other hand, we see that ignoring analysis in FDPO\ntraining almost cuts hallucination rates in half. Even sam-\npling at high temperature, generations still on average con-\ntain less hallucinations than the baseline InstructBLIP model\nsampled at 0 temperature, where it would have the least\npropensity to hallucinate. This is slightly better than best-of-\n16 rejection sampling, and almost as good as best-of-64 re-\njection sampling. This performance gap is to be expected as\nrejection sampling can generalize over the entire set of pos-\nsible model generations, whereas FDPO is more limited in\noptimizing only over the data that it sees in the training data.\nThough, there is a trade-off in this performance, as best-of-n\nrejection sampling is slower in inference by a factor of n.\nConclusion\nWe introduce M-HalDetect, a novel multi-modal fine-\ngrained hallucination detection dataset for benchmarking\nand training LVLMs to produce more truthful generations.\nWe train fine-grained multi-modal reward models to per-\nform rejection sampling against InstructBLIP. We innovate\nFDPO to optimize InstructBLIP directly on M-HalDetect,\navoiding the need for preference pairs. Both methods signif-\nicantly reduce InstructBLIP’s hallucination rate, extending\ntheir effectiveness to the multi-modal domain, and demon-\nstrating the usefulness of M-HalDetect in catching and re-\nducing hallucinations. We show this dataset is generalizable\nacross multiple LVLMs, successfully reducing the halluci-\nnation rates of LLaV A and mPLUG-OWL.\nWhile we show strong performance with rejection sam-\npling, it is prohibitively slow for inference in real-world\nuse-cases. The next step would be to optimize a generative\nmodel, perhaps InstructBLIP, using reinforcement learning\nwith our trained reward models to create a higher quality\nLVLM for instruction aware VQA.\nA limitation of modern day applications towards train-\ning large models with fine-grained feedback is that training\ntypically takes place over multiple iterations of model train-\ning and feedback collection. This ensures the final model is\nmore robustly aligned with the high level training objective.\nIn this paper, we only perform one cycle of collecting re-\nsponse feedback and training. Indeed, when analyzing some\nof the responses, we can see hints of overfitting to our train-\ning objective - image descriptions are slightly more generic\nthan before, and the preciseness of descriptions may have\ngone down. Future work can extend our dataset and meth-\nods to also account for descriptiveness and informativeness,\ntraining multiple reward models for optimizing a more ro-\nbust final model.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18141\nAcknowledgements\nWe thank Sean Hendryx and Utsav Garg for their feedback\nand support through internal development of the paper.\nReferences\nAchiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;\nAleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;\nAnadkat, S.; et al. 2023. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774.\nAlayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Has-\nson, Y .; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.;\net al. 2022. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing Sys-\ntems, 35: 23716–23736.\nBai, Y .; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; Das-\nSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.;\net al. 2022. Training a helpful and harmless assistant with re-\ninforcement learning from human feedback. arXiv preprint\narXiv:2204.05862.\nBang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; Do, Q. V .; Xu,\nY .; and Fung, P. 2023. A Multitask, Multilingual, Multi-\nmodal Evaluation of ChatGPT on Reasoning, Hallucination,\nand Interactivity. arXiv:2302.04023.\nBird, S.; Klein, E.; and Loper, E. 2009. Natural language\nprocessing with Python: analyzing text with the natural lan-\nguage toolkit. ” O’Reilly Media, Inc.”.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. Advances in Neural Information\nProcessing Systems, 33: 1877–1901.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;\nLi, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards\nGeneral-purpose Vision-Language Models with Instruction\nTuning. arXiv:2305.06500.\nEngstrom, L.; Ilyas, A.; Santurkar, S.; Tsipras, D.; Janoos,\nF.; Rudolph, L.; and Madry, A. 2020. Implementation Mat-\nters in Deep Policy Gradients: A Case Study on PPO and\nTRPO. CoRR, abs/2005.12729.\nGanesan, K. 2018. ROUGE 2.0: Updated and Im-\nproved Measures for Evaluation of Summarization Tasks.\narXiv:1803.01937.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\nBang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of Hal-\nlucination in Natural Language Generation. ACM Comput-\ning Surveys, 55(12): 1–38.\nLi, C. 2023. Large Multimodal Models: Notes on CVPR\n2023 Tutorial. arXiv preprint arXiv:2306.14895.\nLi, Y .; Du, Y .; Zhou, K.; Wang, J.; Zhao, W. X.; and Wen,\nJ.-R. 2023. Evaluating object hallucination in large vision-\nlanguage models. arXiv preprint arXiv:2305.10355.\nLightman, H.; Kosaraju, V .; Burda, Y .; Edwards, H.; Baker,\nB.; Lee, T.; Leike, J.; Schulman, J.; Sutskever, I.; and\nCobbe, K. 2023. Let’s Verify Step by Step. arXiv preprint\narXiv:2305.20050.\nLin, T.; Maire, M.; Belongie, S. J.; Bourdev, L. D.; Girshick,\nR. B.; Hays, J.; Perona, P.; Ramanan, D.; Doll´ar, P.; and Zit-\nnick, C. L. 2014. Microsoft COCO: Common Objects in\nContext. CoRR, abs/1405.0312.\nLiu, F.; Lin, K.; Li, L.; Wang, J.; Yacoob, Y .; and Wang,\nL. 2023a. Aligning Large Multi-Modal Model with Robust\nInstruction Tuning. arXiv preprint arXiv:2306.14565.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023b. Visual instruc-\ntion tuning. arXiv preprint arXiv:2304.08485.\nNakano, R.; Hilton, J.; Balaji, S.; Wu, J.; Ouyang, L.; Kim,\nC.; Hesse, C.; Jain, S.; Kosaraju, V .; Saunders, W.; Jiang, X.;\nCobbe, K.; Eloundou, T.; Krueger, G.; Button, K.; Knight,\nM.; Chess, B.; and Schulman, J. 2021. WebGPT: Browser-\nassisted question-answering with human feedback. CoRR,\nabs/2112.09332.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Pro-\ncessing Systems, 35: 27730–27744.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a Method for Automatic Evaluation of Machine Trans-\nlation. In Proceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics, 311–318. Asso-\nciation for Computational Linguistics.\nRafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning,\nC. D.; and Finn, C. 2023. Direct preference optimization:\nYour language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290.\nScaleAI. 2023. Scale AI Rapid Portal. https://scale.com/\ndocs/how-rapid-works. Accessed: 2023-08-01.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal Policy Optimization Algorithms.\nCoRR, abs/1707.06347.\nStiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D. M.; Lowe,\nR.; V oss, C.; Radford, A.; Amodei, D.; and Christiano, P. F.\n2020. Learning to summarize from human feedback. CoRR,\nabs/2009.01325.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nWang, P.; Li, L.; Chen, L.; Zhu, D.; Lin, B.; Cao, Y .; Liu,\nQ.; Liu, T.; and Sui, Z. 2023. Large language models are not\nfair evaluators. arXiv preprint arXiv:2305.17926.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18142\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.;\nLester, B.; Du, N.; Dai, A. M.; and Le, Q. V . 2021. Fine-\ntuned Language Models Are Zero-Shot Learners. CoRR,\nabs/2109.01652.\nWu, Z.; Hu, Y .; Shi, W.; Dziri, N.; Suhr, A.; Am-\nmanabrolu, P.; Smith, N. A.; Ostendorf, M.; and Hajishirzi,\nH. 2023. Fine-Grained Human Feedback Gives Better\nRewards for Language Model Training. arXiv preprint\narXiv:2306.01693.\nYe, Q.; Xu, H.; Xu, G.; Ye, J.; Yan, M.; Zhou, Y .; Wang, J.;\nHu, A.; Shi, P.; Shi, Y .; et al. 2023. mplug-owl: Modulariza-\ntion empowers large language models with multimodality.\narXiv preprint arXiv:2304.14178.\nZheng, R.; Dou, S.; Gao, S.; Hua, Y .; Shen, W.; Wang, B.;\nLiu, Y .; Jin, S.; Liu, Q.; Zhou, Y .; Xiong, L.; Chen, L.; Xi,\nZ.; Xu, N.; Lai, W.; Zhu, M.; Chang, C.; Yin, Z.; Weng, R.;\nCheng, W.; Huang, H.; Sun, T.; Yan, H.; Gui, T.; Zhang, Q.;\nQiu, X.; and Huang, X. 2023. Secrets of RLHF in Large\nLanguage Models Part I: PPO. arXiv:2307.04964.\nZiegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,\nA.; Amodei, D.; Christiano, P. F.; and Irving, G. 2019. Fine-\nTuning Language Models from Human Preferences. CoRR,\nabs/1909.08593.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18143"
}