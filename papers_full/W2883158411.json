{
  "title": "Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction",
  "url": "https://openalex.org/W2883158411",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2515171796",
      "name": "Daniela Gerz",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A1969142033",
      "name": "Ivan Vulić",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A4229069644",
      "name": "Edoardo Ponti",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A267471188",
      "name": "Jason Naradowsky",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A1972118651",
      "name": "Roi Reichart",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1927037681",
      "name": "Anna Korhonen",
      "affiliations": [
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2111305191",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2041258965",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2251654079",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2949563612",
    "https://openalex.org/W1517590677",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2741986357",
    "https://openalex.org/W2251044566",
    "https://openalex.org/W2963173190",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2129250947",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W4205167092",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W3038058348",
    "https://openalex.org/W2508661145",
    "https://openalex.org/W1503259811",
    "https://openalex.org/W932413789",
    "https://openalex.org/W2620558438",
    "https://openalex.org/W2159746348",
    "https://openalex.org/W2963901637",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2401969231",
    "https://openalex.org/W1814992895",
    "https://openalex.org/W2321470647",
    "https://openalex.org/W2963216505",
    "https://openalex.org/W2154579312",
    "https://openalex.org/W2507731975",
    "https://openalex.org/W2168664213",
    "https://openalex.org/W1899794420",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2963324947",
    "https://openalex.org/W2963582782",
    "https://openalex.org/W1505680913",
    "https://openalex.org/W2963899393",
    "https://openalex.org/W2270190199",
    "https://openalex.org/W2591264712",
    "https://openalex.org/W2964318358",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W2963099225",
    "https://openalex.org/W630532510",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2962883855"
  ],
  "abstract": "Neural architectures are prominent in the construction of language models (LMs). However, word-level prediction is typically agnostic of subword-level information (characters and character sequences) and operates over a closed vocabulary, consisting of a limited word set. Indeed, while subword-aware models boost performance across a variety of NLP tasks, previous work did not evaluate the ability of these models to assist next-word prediction in language modeling tasks. Such subword-level informed models should be particularly effective for morphologically-rich languages (MRLs) that exhibit high type-to-token ratios. In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of morphological systems, and offer new LM benchmarks to the community, while considering subword-level information. The main technical contribution of our work is a novel method for injecting subword-level information into semantic word vectors, integrated into the neural language modeling training, to facilitate word-level prediction. We conduct experiments in the LM setting where the number of infrequent words is large, and demonstrate strong perplexity gains across our 50 languages, especially for morphologically-rich languages. Our code and data sets are publicly available.",
  "full_text": "Language Modeling for Morphologically Rich Languages:\nCharacter-Aware Modeling for Word-Level Prediction\nDaniela Gerz1, Ivan Vuli´c1, Edoardo Ponti1\nJason Naradowsky3, Roi Reichart2 Anna Korhonen1\n1Language Technology Lab, DTAL, University of Cambridge\n2Faculty of Industrial Engineering and Management, Technion, IIT\n3Johns Hopkins University\n1{dsg40,iv250,ep490,alk23}@cam.ac.uk\n2roiri@ie.technion.ac.il\n3narad@jhu.edu\nAbstract\nNeural architectures are prominent in the con-\nstruction of language models (LMs). How-\never, word-level prediction is typically agnos-\ntic of subword-level information (characters\nand character sequences) and operates over\na closed vocabulary, consisting of a limited\nword set. Indeed, while subword-aware mod-\nels boost performance across a variety of NLP\ntasks, previous work did not evaluate the abil-\nity of these models to assist next-word predic-\ntion in language modeling tasks. Such subword-\nlevel informed models should be particularly\neffective for morphologically-rich languages\n(MRLs) that exhibit high type-to-token ratios.\nIn this work, we present a large-scale LM study\non 50 typologically diverse languages cover-\ning a wide variety of morphological systems,\nand offer new LM benchmarks to the commu-\nnity, while considering subword-level informa-\ntion. The main technical contribution of our\nwork is a novel method for injecting subword-\nlevel information into semantic word vectors,\nintegrated into the neural language modeling\ntraining, to facilitate word-level prediction. We\nconduct experiments in the LM setting where\nthe number of infrequent words is large, and\ndemonstrate strong perplexity gains across our\n50 languages, especially for morphologically-\nrich languages. Our code and data sets are pub-\nlicly available.\n1 Introduction\nLanguage Modeling (LM) is a key NLP task, serving\nas an important component for applications that re-\nquire some form of text generation, such as machine\ntranslation (Vaswani et al., 2013), speech recognition\n(Mikolov et al., 2010), dialogue generation (Serban et\nal., 2016), or summarisation (Filippova et al., 2015).\nA traditional recurrent neural network (RNN) LM\nsetup operates on a limited closed vocabulary of\nwords (Bengio et al., 2003; Mikolov et al., 2010).\nThe limitation arises due to the model learning pa-\nrameters exclusive to single words. A standard train-\ning procedure for neural LMs gradually modiﬁes the\nparameters based on contextual/distributional infor-\nmation: each occurrence of a word token in train-\ning data contributes to the estimate of a word vector\n(i.e., model parameters) assigned to this word type.\nLow-frequency words therefore often have incorrect\nestimates, not having moved far from their random\ninitialisation. A common strategy for dealing with\nthis issue is to simply exclude the low-quality param-\neters from the model (i.e., to replace them with the\n<unk> placeholder), leading to only a subset of the\nvocabulary being represented by the model.\nThis limited vocabulary assumption enables the\nmodel to bypass the problem of unreliable word es-\ntimates for low-frequency and unseen words, but it\ndoes not resolve it. The assumption is far from ideal,\npartly due to the Zipﬁan nature of each language\n(Zipf, 1949), and its limitation is even more pro-\nnounced for morphologically-rich languages (MRLs):\nthese languages inherently generate a plethora of\nwords by their morphological systems. As a conse-\nquence, there will be a large number of words for\nwhich a standard RNN LM cannot guarantee a reli-\nable word estimate.\nSince gradual parameter estimation based on con-\ntextual information is not feasible for rare phenomena\n451\nTransactions of the Association for Computational Linguistics, vol. 6, pp. 451–465, 2018. Action Editor: Brian Roark.\nSubmission batch: 12/2017; Revision batch: 5/2018; Published 7/2018.\nc⃝2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\nin the full vocabulary setup (Adams et al., 2017), it\nis of crucial importance to construct and enable tech-\nniques that can obtain these parameters in alternative\nways. One solution is to draw information from ad-\nditional sources, such as characters and character\nsequences. As a consequence, such character-aware\nmodels should facilitate LM word-level prediction in\na real-life LM setup which deals with a large amount\nof low-frequency or unseen words.\nEfforts into this direction have yielded exciting\nresults, primarily on the input side of neural LMs. A\nstandard RNN LM architecture relies on two word\nrepresentation matrices learned during training for\nits input and next-word prediction. This effectively\nmeans that there are two sets of per-word speciﬁc pa-\nrameters that need to be trained. Recent work shows\nthat it is possible to generate a word representation\non-the-ﬂy based on its constituent characters, thereby\neffectively solving the problem for the parameter set\non the input side of the model (Kim et al., 2016; Lu-\nong and Manning, 2016; Miyamoto and Cho, 2016;\nLing et al., 2015). However, it is not straightforward\nhow to advance these ideas to the output side of the\nmodel, as this second set of word-speciﬁc parameters\nis directly responsible for the next-word prediction:\nit has to encode a much wider range of information,\nsuch as topical and semantic knowledge about words,\nwhich cannot be easily obtained from its characters\nalone (Jozefowicz et al., 2016).\nWhile one solution is to directly output characters\ninstead of words (Graves, 2013; Miyamoto and Cho,\n2016), a recent work from Jozefowicz et al. (2016)\nsuggests that such purely character-based architec-\ntures, which do not reserve parameters for informa-\ntion speciﬁc to single words, cannot attain state-of-\nthe-art LM performance on word-level prediction.\nIn this work, we combine the two worlds and pro-\npose a novel LM approach which relies on both word-\nlevel (i.e., contextual) and subword-level knowledge.\nIn addition to training word-speciﬁc parameters for\nword-level prediction using a regular LM objective,\nour method encourages the parameters to also re-\nﬂect subword-level patterns by injecting knowledge\nabout morphology. This information is extracted in\nan unsupervised manner based on already available\ninformation in convolutional ﬁlters from earlier net-\nwork layers. The proposed method leads to large\nimprovements in perplexity across a wide spectrum\nof languages: 22 in English, 144 in Hebrew, 378\nin Finnish, 957 in Korean on our LM benchmarks.\nWe also show that the gains extend to another mul-\ntilingual LM evaluation set, compiled recently for 7\nlanguages by Kawakami et al. (2017).\nWe conduct a systematic LM study on 50 typo-\nlogically diverse languages, sampled to represent a\nvariety of morphological systems. We discuss the im-\nplications of typological diversity on the LM task,\nboth theoretically in Section 2, and empirically in\nSection 7; we ﬁnd a clear correspondence between\nperformance of state-of-the art LMs and structural\nlinguistic properties. Further, the consistent perplex-\nity gains across the large sample of languages suggest\nwide applicability of our novel method.\nFinally, this article can also be read as a com-\nprehensive multilingual analysis of current LM ar-\nchitectures on a set of languages which is much\nlarger than the ones used in recent LM work\n(Botha and Blunsom, 2014; Vania and Lopez, 2017;\nKawakami et al., 2017). We hope that this article\nwith its new datasets, methodology and models,\nall available online at http://people.ds.cam.\nac.uk/dsg40/lmmrl.html, will pave the way\nfor true multilingual research in language modeling.\n2 LM Data and Typological Diversity\nA language model deﬁnes a probability distribution\nover sequences of tokens, and is typically trained to\nmaximise the likelihood of token input sequences.\nFormally, the LM objective is expressed as follows:\nP(t1,...tn) =\n∏\ni\nP(ti|t1,...ti−1). (1)\nti is a token with the index i in the sequence. For\nword-level prediction a token corresponds to one\nword, whereas for character-level (also termed char-\nlevel) prediction it is one character.\nLMs are most commonly tested on Western Eu-\nropean languages. Standard LM benchmarks in En-\nglish include the Penn Treebank (PTB) (Marcus et\nal., 1993), the 1 Billion Word Benchmark (BWB)\n(Chelba et al., 2014), and the Hutter Prize data (Hut-\nter, 2012). English datasets extracted from BBC\nNews (Greene and Cunningham, 2006) and IMDB\nMovie Reviews (Maas et al., 2011) are also used for\nLM evaluation (Wang and Cho, 2016; Miyamoto and\n452\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\nCho, 2016; Press and Wolf, 2017).\nRegarding multilingual LM evaluation, Botha and\nBlunsom (2014) extract datasets for other languages\nfrom the sets provided by the 2013 Workshop on\nStatistical Machine Translation (WMT) (Bojar et al.,\n2013): they experiment with Czech, French, Span-\nish, German and Russian. A recent work of Kim\net al. (2016) reuses these datasets and adds Arabic.\nLing et al. (2015) evaluate on English, Portuguese,\nCatalan, German and Turkish datasets extracted from\nWikipedia. Verwimp et al. (2017) use a subset of the\nCorpus of Spoken Dutch (Oostdijk, 2000) for Dutch\nLM. Kawakami et al. (2017) evaluate on 7 European\nlanguages using Wikipedia data, including Finnish.\nPerhaps the largest and most diverse set of lan-\nguages used for multilingual LM evaluation so far is\nthe one of Vania and Lopez (2017). Their study in-\ncludes 10 languages in total representing several mor-\nphological types (fusional, e.g., Russian, and aggluti-\nnative, e.g., Finnish), as well as languages with par-\nticular morphological phenomena (root-and-pattern\nin Hebrew and reduplication in Malay). In this work,\nwe provide LM evaluation datasets for 50 typologi-\ncally diverse languages, with their selection guided\nby structural properties.\nLanguage Selection Aiming for a comprehensive\nmultilingual LM evaluation, we include languages\nfor all possible types of morphological systems. Our\nstarting point is the Polyglot Wikipedia (PW) (Al-\nRfou et al., 2013). While at ﬁrst PW seems com-\nprehensive and quite large already (covering 40 lan-\nguages), the majority of the PW languages are simi-\nlar from both a genealogical perspective (26/40 are\nIndo-European) and a geographic perspective (28/40\nWestern European). As a consequence, they share\nmany patterns and are not a representative sample of\nthe world’s languages.\nIn order to quantitatively analyse global trends and\ncross-linguistic generalisations across a large set of\nlanguages, we propose to test on all PW languages\nand source additional data from the same domain,\nWikipedia1, considering candidates in descending\norder of corpus size and morphological type. Tradi-\ntionally, languages have been grouped into the four\n1Chinese, Japanese, and Thai are sourced from Wikipedia\nand processed with the Polyglot tokeniser since we found their\npreprocessing in the PW is not adequate for language modeling.\n is outside  listening  to\nlistening to music\nChar-CNN-LSTM LM Fine-tuning\nCw\nMw\nLSTM\nCNN\nyw\nMc\n( x   , x   ) w\n        \np\n( x   , x   ) w\n        \nn\nsampling\nhw\nFigure 1: An illustration of the Char-CNN-LSTM\nLM and our ﬁne-tuning post-processing method. Af-\nter each epoch we adapt word-level vectors in the\nsoftmax embedding Mw using samples based on fea-\ntures from the char-level convolutional ﬁlters. The\nﬁgure follows the model ﬂow bottom to the top.\nmain types: isolating, fusional, introﬂexive and ag-\nglutinative, based on their position along a spectrum\nmeasuring their preference on breaking up concepts\nin many words (on one extreme) or rather compose\nthem into single words (on the other extreme).\nHowever, even languages belonging to the same\ntype display different out-of-vocabulary rates and\ntype-token ratios. This happens because languages\nspecify different subsets of grammatical categories\n(such as tense for verbs, or number for nouns) and\nvalues (such as future for tense, plural for number).\nThe amount of grammatical categories expressed in a\nlanguage determines its inﬂectional synthesis (Bickel\nand Nichols, 2013).\nIn our ﬁnal sample of languages, we select lan-\nguages belonging to morphological types different\nfrom the fusional one, which is over-represented in\nthe PW. In particular, we include new isolating (Min\nNan, Burmese, Khmer), agglutinative (Basque, Geor-\ngian, Kannada, Tamil, Mongolian, Javanese ), and\nintroﬂexive languages (Amharic).\n453\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\n3 Underlying LM: Char-CNN-LSTM\nAs the underlying model we opt for the state-of-the-\nart neural LM architecture of Kim et al. (2016): it has\nbeen shown to work across a number of languages\nand in a large-scale setup (Jozefowicz et al., 2016). It\nalready provides a solution for the input side param-\neters of the model by building word vectors based\non the word’s constituent character sequences. How-\never, its output side still operates with a standard\nword-level matrix within the closed and limited vo-\ncabulary assumption. We refer to this model asChar-\nCNN-LSTM and describe its details in the following.\nFigure 1 (left) illustrates the model architecture.\nChar-CNN-LSTM constructs input word vectors\nbased on the characters in each word using a convo-\nlutional neural network (CNN) (LeCun et al., 1989),\nthen processes the input word-level using a LSTM\n(Hochreiter and Schmidhuber, 1997). The next word\nis predicted using word embeddings, a large number\nof parameters which have to be trained speciﬁcally\nto represent the semantics of single words. We refer\nto this space of word representations as Mw.\nFormally, for the input layer the model trains a\nlook-up matrix C ∈R|V c|×dc, corresponding to one\ndc-dimensional vector per character c in the char\nvocabulary Vc. For each input, it takes a sequence\nof characters of a ﬁxed length m, [c1,...cm], where\nmis the maximum length of all words in the word\nvocabulary Vw, and the length of each word isl≤m.\nLooking up all characters of a word yields a se-\nquence of char representations in Rdc×l, which is\nzero-padded to ﬁt the ﬁxed length m. For each\nword one gets a sequence of char representations\nCw ∈Rdc×m, passed through a 1D convolution:\nfw\ni = tanh(⟨Cw,Hi⟩+ b). (2)\nHi ∈Rdf,i×si is a ﬁlter or kernel of size/width si and\n⟨A,B⟩= Tr(ABT ) is the Frobenius inner product.\nThe model has multiple ﬁlters, Hi, with kernels of\ndifferent width, si, and dimensionality df,i, iis used\nto index ﬁlters. Since the model performs a convo-\nlution over char embeddings, si corresponds to the\nchar window the convolution is operating on: e.g., a\nﬁlter of width si = 3 and d3,i = 150 could be seen\nas learning 150 features for detecting 3-grams.\nBy learning kernels of different width, si, the\nmodel can learn subword-level features for charac-\nter sequences of different lengths. fw\ni is the output\nof taking the convolution with ﬁlter Hi for word\nw. Since fw\ni can get quite large, its dimensional-\nity is reduced using max-over-time (1D) pooling:\nyw\ni = maxj fw\ni [j]. Here, j indexes the dimensions\ndf,i of the ﬁlter fw\ni , and yw\ni ∈ Rdf,i . This corre-\nsponds to taking the maximum value for each feature\nof Hi, with the intuition that the most informative\nfeature would have the highest activation. The out-\nput of all max-pooling operations yw\ni is concatenated\nto form a word vector yw ∈Rdp, where dp is the\nnumber of all features for all Hi:\nyw = concat([yw\n1 ,...,y w\ni ]). (3)\nThis vector is passed through a highway network\n(Srivastava et al., 2015) to give the network the pos-\nsibility to reweigh or transform the features: hw =\nHighway(yw). So far all transformations were done\nper word; after the highway transformation word rep-\nresentations are processed in a sequence by an LSTM\n(Hochreiter and Schmidhuber, 1997):\now\nt = LSTM([hw1 ,...hwt−1 ]). (4)\nThe LSTM yields one output vector ow\nt per word\nin the sequence, given all previous time steps\n[yw1 ,...ywt−1 ]. To predict the next word wt+1, one\ntakes the dot product of the vector ow\nt ∈R1×dl with\na lookup matrix Mw ∈Rdl×|V w|, where dl corre-\nsponds to the LSTM hidden state size. The vector\npt+1 ∈R1×|V w|is normalised to contain values be-\ntween 0 and 1, representing a probability distribution\nover the next word. This corresponds to calculating\nthe softmax function for every word kin Vw:\np(wt+1 = k|ot) = e(ot·mk)\n∑\nk′∈Vw e(ot·mk′) (5)\nwhere P(wt+1 = k|ot) is the probability of the next\nword wt+1 being k given ot, and mk is the output\nembedding vector taken from Mw.\nWord-Level Vector Space: Mw The model pa-\nrameters in Mw can be seen as the bottleneck of\nthe model, as they need to be trained speciﬁcally\nfor single words, leading to unreliable estimates for\ninfrequent words. As an analysis of the corpus statis-\ntics later in Section 7 reveals, the Zipﬁan effect and\nits inﬂuence on word vector estimation cannot be\nfully resolved even with a large corpus, especially\n454\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\ntaking into account how ﬂexible MRLs are in terms\nof word formation and combination. Yet, having a\ngood estimate for the parameters in Mw is essential\nfor the ﬁnal LM performance, as they are directly\nresponsible for the next-word prediction.\nTherefore, our aim is to improve the quality of rep-\nresentations in Mw, focusing on infrequent words. To\nachieve this, we turn to another source of information:\ncharacter patterns. In other words, since Mw does\nnot have any information about character patterns\nfrom lower layers, we seek a way to: a) detect words\nwith similar subword structures (i.e., “morpheme”-\nlevel information), and b) let these words share their\nsemantic information.\n4 Character-Aware Vector Space\nThe CNN part of Char-CNN-LSTM, see Eq. (3), in\nfact provides information about such subword-level\npatterns: the model constructs a word vector yw on-\nthe-ﬂy based on the word’s constituent characters.\nWe let the model constructyw for all words in the vo-\ncabulary, resulting in a character-aware word vector\nspace Mc ∈R|V w|×dp. The construction of the space\nis completely unsupervised and independent of the\nword’s context; only the ﬁrst (CNN) network layers\nare activated. Our core idea is to leverage this in-\nformation obtained from Mc to inﬂuence the output\nmatrix Mw, and consequently the network prediction,\nand extend the model to handle unseen words.\nWe ﬁrst take a closer look at the character-aware\nspace Mc, and then describe how to improve and\nexpand the semantic space Mw based on the infor-\nmation contained in Mc (Section 5). Each vocabulary\nentry in Mc encodes character n-gram patterns about\nthe represented word, for 1 ≥n ≤7. The n-gram\npatterns arise through ﬁlters of different lengths, and\ntheir maximum activation is concatenated to form\neach individual vector yw. The matrix Mc is of di-\nmensionality |Vw|×1100, where each of the 1,100\ndimensions corresponds to the activation of one ker-\nnel feature. In practice, dimensions [0,1,.. : 50]\ncorrespond to single-character features, [50 : 150]\nto character 2-grams, [150 : 300] to 3-grams. The\nhigher-order n-grams get assigned 200 dimensions\neach, up to dimensions [900 : 1100] for 7-grams.\nDrawing an analogy to work in computer vision\n(Zeiler and Fergus, 2014; Chatﬁeld et al., 2014), we\nsi Pattern Max Activations\nZH 1 更, 不 更 更 更为, 更 更 更改, 更 更 更名, ..,不 不 不满, 不 不 不明, 不 不 不易\n1 今, 代 今 今 今日, 今 今 今人, 至少, ..,如何, 现代 代 代, 当代 代 代\n1 Caps In,Ebru,VIC,..,FAT, MW, MIT\nTR 3 mu- .., mutfa˘gının,muharebe,muhtelif\n6 Üniversite .., Üniversitesi’nin,üniversitelerde\nTable 1: Each CNN ﬁlter tends to have high acti-\nvations for a small number of subword patterns. si\ndenotes the ﬁlter size.\nWord Nearest Neighbours\nUrsprünglichkeitursprüngliche,Urstoff,ursprünglichen\nDE Mittelwert Mittelwerten,Regelwerkes,Mittelweser\neffektiv Effekt,Perfekt,Effekte,perfekten,Respekt\n大学 大 大 大金,大 大 大石,大 大 大震災,大 大 大空,大 大 大野\nJA ハイク ハ ハ ハイ イ イム,バイ イ イク ク ク,メイ イ イク ク ク,ハ ハ ハッサク ク ク\n1725 1825,1625,1524mm,1728\nMagenta Maplet,Maya,Management\nTable 2: Nearest neighbours for vocabulary words,\nbased on the character-aware vector space Mc.\ndelve deeper into the ﬁlter activations and analyse the\nkey properties of the vector spaceMc. The qualitative\nanalysis reveals that many features are interpretable\nby humans, and indeed correspond to frequent sub-\nword patterns, as illustrated in Table 1. For instance,\ntokenised Chinese data favours short words: conse-\nquently short ﬁlters activate strongly for one or two\ncharacters. The ﬁrst two ﬁlters (width 1) are highly\nactive for two common single characters each: one\nﬁlter is active for 更(again, more), 不(not), and the\nother for 今 (now), 代 (time period). Larger ﬁlters\n(width 5-7) do not show interpretable patterns in Chi-\nnese, since the vocabulary largely consists of short\nwords (length 1-4).\nAgglutinative languages show a tendency towards\nlong words. We ﬁnd that medium-sized ﬁlters (width\n3-5) are active for morphemes or short common sub-\nword units, and the long ﬁlters are activated for dif-\nferent surface realisations of the same root word. In\nTurkish, one ﬁlter is highly active on various forms\nof the word üniversite (university). Further, in MRLs\nwith the Latin alphabet short ﬁlters are typically ac-\ntive on capitalisation or special chars.\nTable 2 shows examples of nearest neighbours\nbased on the activations in Mc. The space seems\nto be arranged according to shared subword patterns\n455\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\nbased on the CNN features. It does not rely only on\na simple character overlap, but also captures shared\nmorphemes. This property is exploited to inﬂuence\nthe LM output word embedding matrixMw in a com-\npletely unsupervised way, as illustrated on the right\nside of Figure 1.\n5 Fine-Tuning the LM Prediction\nWhile the output vector space Mw captures word-\nlevel semantics, Mc arranges words by subword\nfeatures. A model which relies solely on character-\nlevel knowledge (similar to the information stored in\nMc) for word-level prediction cannot fully capture\nword-level semantics and even hurts LM performance\n(Jozefowicz et al., 2016). However, shared subword\nunits still provide useful evidence of shared semantics\n(Cotterell et al., 2016; Vuli´c et al., 2017): injecting\nthis into the space Mw to additionally reﬂect shared\nsubword-level information should lead to improved\nword vector estimates, especially for MRLs.\n5.1 Fine-Tuning and Constraints\nWe inject this information into Mw by adapting re-\ncent ﬁne-tuning (often termed retroﬁtting or special-\nisation) methods for vector space post-processing\n(Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et\nal., 2017; Vuli´c et al., 2017, i.a.). These models enrich\ninitial vector spaces by encoding external knowledge\nprovided in the form of simple linguistic constraints\n(i.e., word pairs) into the initial vector space.\nThere are two fundamental differences between\nour work and previous work on specialisation. First,\nprevious models typically use rich hand-crafted lexi-\ncal resources such as WordNet (Fellbaum, 1998) or\nthe Paraphrase Database (Ganitkevitch et al., 2013),\nor manually deﬁned rules (Vuli´c et al., 2017) to ex-\ntract the constraints, while we generate them directly\nusing the implicit knowledge coded in Mc. Second,\nour method isintegrated into a language model: it per-\nforms updates after each epoch of the LM training.2\nIn Section 5.2, we describe our model for ﬁne-tuning\nMw based on the information provided in Mc.\nOur ﬁne-tuning approach relies on constraints:\npositive and negative word pairs (xi,xj), where\n2We have also experimented with a variant which performs\nonly a post-hoc single update of the Mw matrix after the LM\ntraining, but a variant which performs continuous per-epoch\nupdates is more beneﬁcial for the ﬁnal LM performance.\nxi,xj ∈Vw. Iterating over each cue word xw ∈Vw\nwe ﬁnd a set of positive word pairs Pw and nega-\ntive word pairs Nw: their extraction is based on their\n(dis)similarity with xw in Mc. Positive pairs (xw,xp)\ncontain words xp yielding the highest cosine similar-\nity to the xw (=nearest neighbors) in Mc. Negative\npairs (xw,xn) are constructed by randomly sampling\nwords xn from the vocabulary. Since Mc gets up-\ndated during the LM training, we (re)generate the\nsets Pw and Nw after each epoch.\n5.2 Attract-Preserve\nWe now present a method for ﬁne-tuning the out-\nput matrix Mw within the Char-CNN-LSTM LM\nframework. As said, the ﬁne-tuning procedure runs\nafter each epoch of the standard log-likelihood LM\ntraining (see Figure 1). We adapt a variant of a state-\nof-the-art post-processing specialisation procedure\n(Wieting et al., 2015; Mrkši´c et al., 2017). The idea\nof the ﬁne-tuning method, which we label Attract-\nPreserve (AP), is to pull the positive pairs closer\ntogether in the output word-level space, while push-\ning the negative pairs further away.\nLet vi denote the word vector of the word xi. The\nAP cost function has two parts: attract and preserve.\nIn the attract term, using the extracted sets Pw and\nNw, we push the vector of xw to be closer to xp by a\nsimilarity margin δthan to its negative sample xn:\nattr(Pw,Nw) =\n∑\n(xw,xp)∈Pw,\n(xw,xn)∈Nw\nReLU(δ+vwvn−vwvp).\nReLU(x) is the standard rectiﬁed linear unit (Nair\nand Hinton, 2010). The δmargin is set to 0.6 in all\nexperiments as in prior work (Mrkši ´c et al., 2017)\nwithout any subsequent ﬁne-tuning.\nThe preserve cost acts as a regularisation pulling\nthe “ﬁne-tuned” vector back to its initial value:\npres(Pw,Nw) =\n∑\nxw∈V w\nλreg||ˆvw −vw||2. (6)\nλreg = 10 −9 is the L2-regularisation constant\n(Mrkši´c et al., 2017); ˆvw is the original word vec-\ntor before the procedure. This term tries to preserve\nthe semantic content present in the original vector\nspace, as long as this information does not contradict\nthe knowledge injected by the constraints. The ﬁnal\ncost function adds the two costs: cost= attr+ pres.\n456\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\n6 Experiments\nDatasets We use the Polyglot Wikipedia (Al-Rfou\net al., 2013) for all available languages except for\nJapanese, Chinese, and Thai, and add these and fur-\nther languages using Wikipedia dumps. The Wiki\ndumps were cleaned and preprocessed by the Poly-\nglot tokeniser. We construct similarly-sized datasets\nby extracting 46K sentences for each language from\nthe beginning of each dump, ﬁltered to contain only\nfull sentences, and split into train (40K), validation\n(3K), and test (3K). The ﬁnal list of languages along\nwith standard language codes (ISO 639-1 standard,\nused throughout the paper) and statistics on vocabu-\nlary and token counts are provided in Table 4.\nEvaluation Setup We report perplexity scores (Ju-\nrafsky and Martin, 2017, Chapter 4.2.1) using the\nfull vocabulary of the respective LM dataset. This\nmeans that we explicitly decide to retain also in-\nfrequent words in the modeled data. Replacing in-\nfrequent words by a placeholder token <unk> is a\nstandard technique in LM to obtain equal vocabu-\nlary sizes across different datasets. Motivated by the\nobservation that infrequent words constitute a sig-\nniﬁcant part of the vocabulary in MRLs, and that\nvocabulary sizes naturally differ between languages,\nwe have decided to avoid the <unk> placeholder for\nlow-frequency words, and run all models on the full\nvocabulary (Adams et al., 2017; Grave et al., 2017).\nWe believe that this full-vocabulary setup offers\nadditional insight into the standard LM techniques,\nleading to evaluation which pinpoints crucial limita-\ntions of current word-based models with regard to\nmorphologically-rich languages. In our setup the vo-\ncabulary contains all words occurring at least once in\nthe training set. To ensure a fair comparison between\nall neural models, words occurring only in the test\nset are mapped to a random vector with the same\ntechnique for all neural models, as described next.\nSampling Vectors of Unseen Words Since zero-\nshot semantic vector estimation at test time is an\nunresolved problem, we seek an alternative way to\ncompare model predictions at test time. We report\nall results with unseen test words being mapped to\none randomly sampled <unk> vector. The <unk>\nvector is part of the vocabulary at training time, but\nremains untrained and at its random initialization\nCharacter embedding size 15\nWord embedding size 650\nNumber of RNN layers 2\nNumber of highway layers 2\nDropout value 0.5\nOptimizer SGD\nLearning rate 1.0\nLearning rate decay 0.5\nParameter init: rand uniform [-0.05, 0.05]\nBatch size 20\nRNN sequence length 35\nMax grad norm 5.0\nMax word length dynamic\nMax epochs 15 or 30\nAP margin (δ) 0.6\nAP optimizer Adagrad\nAP learning rate 0.05\nAP gradient clip 2\nAP regularization constant 10−9\nAP rare words frequency threshold 5\nTable 3: Hyper-parameters.\nsince it never occurs in the training data. Therefore,\nwe sample a random <unk> vector at test time from\nthe same part of the space as the trained vectors,\nusing a normal distribution with the mean and the\nvariance of Mw and the same ﬁxed random seed\nfor all models. We employ this methodology for all\nneural LM models, and thereby ensure that results\nare comparable.\nTraining Setup and Parameters We reproduce\nthe standard LM setup of Zaremba et al. (2015) and\nparameter choices of Kim et al. (2016), with batches\nof 20 and a sequence length of 35, where one step\ncorresponds to one token. The maximum word length\nis chosen dynamically based on the longest word in\nthe corpus. The corpus is processed continuously, and\nthe RNN hidden state resets occur at the beginning of\neach epoch. Parameters are optimised with stochastic\ngradient descent. The gradient is averaged over the\nbatch size and sequence length. We then scale the\naveraged gradient by the sequence length (=35) and\nclip to 5.0 for more stable training. The learning rate\nis 1.0, decayed by 0.5 after each epoch if the valida-\ntion perplexity does not improve. We train all models\nfor 15 epochs on the smaller corpora, and for 30 on\nthe large ones, which is typically sufﬁcient for model\nconvergence.\nOur AP ﬁne-tuning method operates on the whole\n457\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\nMw space, but we only allow words more frequent\nthan 5 as cue words xw (see Section 5 again), while\nthere are no restrictions on xp and xn.3 Our prelimi-\nnary analysis on the inﬂuence of the number of near-\nest neighbours in Mc shows that this parameter has\nonly a moderate effect on the ﬁnal LM scores. We\nthus ﬁx it to 3 positive and 3 negative samples for\neach xw without any tuning. AP is optimised with\nAdagrad (Duchi et al., 2011) and a learning rate of\n0.05, the gradients are clipped to ±2.4 A full sum-\nmary of all hyper-parameters and their values is pro-\nvided in Table 3.\n(Baseline) Language Models The availability of\nLM evaluation sets in a large number of diverse lan-\nguages, described in Section 2, now provides an op-\nportunity to perform a full-ﬂedged multilingual anal-\nysis of representative LM architectures. At the same\ntime, these different architectures serve as the base-\nlines for our novel model which ﬁne-tunes the output\nmatrix Mw.\nAs mentioned, the traditional LM setup is to\nuse words both on the input and on the output\nside (Goodman, 2001; Bengio et al., 2003; De-\nschacht and Moens, 2009) relying on n-gram word\nsequences. We evaluate a strong model from the\nn-gram family of models from the KenLM pack-\nage (https://github.com/kpu/kenlm): it is based on 5-\ngrams with extended Kneser-Ney smoothing (KN5)\n(Kneser and Ney, 1995; Heaﬁeld et al., 2013) 5. The\nrationale behind including this non-neural model is\nto also probe the limitations of such n-gram-based\nLM architectures on a diverse set of languages.\nRecurrent neural networks (RNNs), especially\nLong-Short-Term Memory networks (LSTMs), have\ntaken over the LM universe recently (Mikolov et al.,\n2010; Sundermeyer et al., 2015; Chen et al., 2016,\ni.a.). These LMs map a sequence of input words\nto embedding vectors using a look-up matrix. The\nembeddings are passed to the LSTM as input, and\n3This choice has been motivated by the observation that rare\nwords tend to have other rare words as their nearest neighbours.\nNote that vectors of words from positive and negative examples,\nand not only cue words, also get updated by the AP method.\n4All scores with neural models are produced with our own\nimplementations in TensorFlow (Abadi et al., 2016).\n5We evaluate the default setup for this model using the option\n-interpolate_unigrams=1 which avoids assigning zero-\nprobability to unseen words.\nthe model is trained in an autoregressive fashion to\npredict the next word from the pre-deﬁned vocabu-\nlary given the current context. As a strong baseline\nfrom this LM family, we train a standard LSTM LM\n(LSTM-Word) relying on the setup from Zaremba\net al. (2015) (see Table 3).\nFinally, a recent strand of LM work uses characters\non the input side while retaining word-level predic-\ntion on the output side. A representative architecture\nfrom this group, also serving as the basis in our work\n(Section 3), is Char CNN-LSTM (Kim et al., 2016).\nAll neural models operate on exactly the same vo-\ncabulary and treat out-of-vocabulary (OOV) words\nin exactly the same way. As mentioned, we include\nKN5 as a strong (non-neural) baseline to give per-\nspective on how this more traditional model performs\nacross 50 typologically diverse languages. We have\nselected the setup for the KN5 model to be as close as\npossible to that of neural LMs, However, due to the\ndifferent nature of the models, we note that the results\nbetween KN5 and other models are not comparable.\nIn KN5 discounts are added for low-frequency\nwords, and unseen words at test time are regarded\nas outliers and assigned low probability estimates.\nIn contrast, for all neural models we sample unseen\nword vectors to lie in the space of trained vectors\n(see before). We ﬁnd the latter setup to better reﬂect\nour intuition that especially in MRLs unseen words\nare not outliers but often arise due to morphological\ncomplexity.\n7 Results and Discussion\nIn this section, we present the main empirical ﬁnd-\nings of our work. The focus is on:a) the results of our\nnovel language model with the AP ﬁne-tuning proce-\ndure, and its comparison to other language models in\nour comparison; b) the analysis of the LM results in\nrelation to typological features and corpus statistics.\nTable 4 that lists all 50 test languages along with\ntheir language codes and provides the key statistics\nof our 50 LM evaluation benchmarks. The statistics\ninclude the number of word types in training data,\nthe number of word types occurring in test data but\nunseen in training, as well as the total number of\nword tokens in both training and test data, and type-\nto-token ratios.\nTable 4 also shows the results for KN5, LSTM-\n458\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\nData Stats Baseline Models Ours: Fine-Tuning Mw\nLanguage (code) V ocab\nSize\n(Train)\nNew\nTest\nV ocab\nNumber\nTokens\n(Train)\nNumber\nTokens\n(Test)\nType /\nToken\n(Train)\nKN5 LSTM Char-\nCNN-\nLSTM\n+AP ∆ +AP\n×Amharic (am) 89749 4805 511K 39.2K 0.18 1252 1535 981 817 164\n×Arabic (ar) 89089 5032 722K 54.7K 0.12 2156 2587 1659 1604 55\n□Bulgarian (bg) 71360 3896 670K 49K 0.11 610 651 415 409 6\n□Catalan (ca) 61033 2562 788K 59.4K 0.08 358 318 241 238 3\n□Czech (cs) 86783 4300 641K 49.6K 0.14 1658 2200 1252 1131 121\n□Danish (da) 72468 3618 663K 50.3K 0.11 668 710 466 442 24\n□German (de) 80741 4045 682K 51.3K 0.12 930 903 602 551 51\n□Greek (el) 76264 3767 744K 56.5K 0.10 607 538 405 389 16\n□English (en) 55521 2480 783K 59.5K 0.07 533 494 371 349 22\n□Spanish (es) 60196 2721 781K 57.2K 0.08 415 366 275 270 5\n⋆Estonian (et) 94184 3907 556K 38.6K 0.17 1609 2564 1478 1388 90\n⋆Basque (eu) 81177 3365 647K 47.3K 0.13 560 533 347 309 38\n□Farsi (fa) 52306 2041 738K 54.2K 0.07 355 263 208 205 3\n⋆Finnish (ﬁ) 115579 6489 585K 44.8K 0.20 2611 4263 2236 1858 378\n□French (fr) 58539 2575 769K 57.1K 0.08 350 294 231 220 11\n×Hebrew (he) 83217 3862 717K 54.6K 0.12 1797 2189 1519 1375 144\n□Hindi (hi) 50384 2629 666K 49.1K 0.08 473 426 326 299 27\n□Croatian (hr) 86357 4371 620K 48.1K 0.14 1294 1665 1014 906 108\n⋆Hungarian (hu) 101874 5015 672K 48.7K 0.15 1151 1595 929 819 110\n\u0003 Indonesian (id) 49125 2235 702K 52.2K 0.07 454 359 286 263 23\n□Italian (it) 70194 2923 787K 59.3K 0.09 567 493 349 350 -1\n⋆Japanese (ja) 44863 1768 729K 54.6K 0.06 169 156 136 125 11\n⋆Javanese (jv) 65141 4292 622K 52K 0.10 1387 1443 1158 1003 155\n⋆Georgian (ka) 80211 3738 580K 41.1K 0.14 1370 1827 1097 939 158\n\u0003 Khmer (km) 37851 1303 579K 37.4K 0.07 586 637 522 535 -13\n⋆Kannada (kn) 94660 4604 434K 29.4K 0.22 2315 5310 2558 2265 293\n⋆Korean (ko) 143794 8275 648K 50.6K 0.22 5146 10063 4778 3821 957\n□Lithuanian (lt) 81501 3791 554K 41.7K 0.15 1155 1415 854 827 27\n□Latvian (lv) 75294 4564 587K 45K 0.13 1452 1967 1129 969 160\n\u0003 Malay (ms) 49385 2824 702K 54.1K 0.07 776 725 525 513 12\n⋆Mongolian (mng) 73884 4171 629K 50K 0.12 1392 1716 1165 1091 74\n\u0003 Burmese (my) 20574 755 576K 46.1K 0.04 209 212 182 180 2\n\u0003 Min-Nan (nan) 33238 1404 1.2M 65.6K 0.03 61 43 39 38 1\n□Dutch (nl) 60206 2626 708K 53.8K 0.08 397 340 267 248 19\n□Norwegian (no) 69761 3352 674K 47.8K 0.10 534 513 379 346 33\n□Polish (pl) 97325 4526 634K 47.7K 0.15 1741 2641 1491 1328 163\n□Portuguese (pt) 56167 2394 780K 59.3K 0.07 342 272 214 202 12\n□Romanian (ro) 68913 3079 743K 52.5K 0.09 384 359 256 247 9\n□Russian (ru) 98097 3987 666K 48.4K 0.15 1128 1309 812 715 97\n□Slovak (sk) 88726 4521 618K 45K 0.14 1560 2062 1275 1151 124\n□Slovene (sl) 83997 4343 659K 49.2K 0.13 1114 1308 776 733 43\n□Serbian (sr) 81617 3641 628K 46.7K 0.13 790 961 582 547 35\n□Swedish (sv) 77499 4109 688K 50.4K 0.11 843 832 583 543 40\n⋆Tamil (ta) 106403 6017 507K 39.6K 0.21 3342 6234 3496 2768 728\n\u0003 Thai (th) 30056 1300 628K 49K 0.05 233 241 206 199 7\n\u0003 Tagalog (tl) 72416 3791 972K 66.3K 0.07 379 298 219 211 8\n⋆Turkish (tr) 90840 4608 627K 45K 0.14 1724 2267 1350 1290 60\n□Ukranian (uk) 89724 4983 635K 47K 0.14 1639 1893 1283 1091 192\n\u0003 Vietnamese (vi) 32055 1160 754K 61.9K 0.04 197 190 158 165 -7\n\u0003 Chinese (zh) 43672 1653 746K 56.8K 0.06 1064 826 797 762 35\n\u0003 Isolating (avg) 40930 1825 759K 54K 0.05 440 392 326 318 8\n□Fusional (avg) 73499 3532 689K 51.3K 0.11 842 969 618 566 52\n×Introﬂexive (avg) 87352 4566 650K 49.5K 0.14 1735 2104 1386 1265 121\n⋆Agglutinative (avg) 91051 4687 603K 45K 0.16 1898 3164 1727 1473 254\nTable 4: Test perplexities for 50 languages (ISO 639-1 codes sorted alphabetically) in the full-vocabulary\nprediction LM setup; Left: Basic statistics of our evaluation data. Middle: Results with the Baseline LMs.\nNote that the absolute scores in the KN5 column are not comparable to the scores obtained with neural models\n(see Section 6). Right: Results with Char-CNN-LSTM and our AP ﬁne-tuning strategy. ∆ is indicating the\ndifference in performance over the original Char-CNN-LSTM model. The best scoring neural baseline is\nunderlined. The overall best performing neural model for each language is in bold.\nWord, Char-CNN-LSTM, and our model with the AP\nﬁne-tuning. Furthermore, a visualisation of the Char-\nCNN-LSTM+AP model as a function of type/token\nratio is shown in Figure 2.\n7.1 Fine-Tuning the Output Matrix\nFirst, we test the impact of our AP ﬁne-tuning\nmethod. As the main ﬁnding, the inclusion of ﬁne-\ntuning into Char-CNN-LSTM (this model is termed\n459\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\n+AP) yields improvements on a large number of test\nlanguages. The model is better than both strong neu-\nral baseline language models for 47/50 languages,\nand it improves over the original Char-CNN-LSTM\nLM for 47/50 languages. The largest gains are indi-\ncated for the subset of agglutinative MRLs (e.g., 950\nperplexity points in Korean, large gains also marked\nfor FI, HE, KA, HU, TA, ET). We also observe large\ngains for the three introﬂexive languages included in\nour study (Amharic, Arabic, Hebrew).\nWhile these large absolute gains may be partially\nattributed to the exponential nature of the perplexity\nmeasure, one cannot ignore the substantial relative\ngains achieved by our models: e.g., EU (∆PPL=38)\nimproves more than a fusional language like DA\n(∆PPL=24) even with a lower baseline perplex-\nity. This suggests that injecting subword-level infor-\nmation is more straightforward for the former: in\nagglutinative languages, the mapping between mor-\nphemes and meanings is less ambiguous. Moreover,\nthe number of words that beneﬁt from the injection\nof character-based information is larger for aggluti-\nnative languages, because they also tend to display\nthe highest inﬂectional synthesis.\nFor the opposite reasons, we do not surpass Char-\nCNN-LSTM in a few fusional (IT) and isolating lan-\nguages (KM, VI). We also observe improvements\nfor Slavic languages with rich morphology (RU, HR,\nPL). The gains are also achieved for some isolating\nand fusional languages with smaller vocabularies and\na smaller number of rare words, e.g., in Tagalog, En-\nglish, Catalan, and Swedish. This suggests that our\nmethod for ﬁne-tuning the LM prediction is not re-\nstricted to MRLs only, and has the ability to improve\nthe estimation for rare words in multiple typologi-\ncally diverse languages.\n7.2 Language Models, Typological Features,\nand Corpus Statistics\nIn the next experiment, we estimate correlation\nstrength of all perplexity scores with a series of inde-\npendent variables. The variables are 1) type-token ra-\ntio in the train data; 2) new word types in the test data;\n3) the morphological type of the language among iso-\nlating, fusional, introﬂexive, and agglutinative, cap-\nturing different aspects related to the morphological\nrichness of a language.\nResults with Pearson’s ρ (numerical) and η2 in\n40000 60000 80000 100000 120000 140000\nvocabulary size\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000perplexity ar\nam\nko\nro\nhe\nen eues\nms\nmng\nhuru\nnan\nlt\nja\nka\nkn\njv\nzh\nvi\nfi\nta\npl\ncs\ntl\ntr\nkn\nlt\nlv\nsl\nIsolating\nFusional\nIntroflexive\nAgglutinative\nFigure 2: Perplexity results with Char-CNN-\nLSTM+AP (y-axis) in relation to type/token ratio\n(x-axis). For language codes, see Table 4.\none-way ANOVA (categorical) are shown in Table 5.\nSigniﬁcance tests show p-values <1−3 for all combi-\nnations of models and independent variables, demon-\nstrating all of them are good performance predictors.\nOur main ﬁnding indicates that linguistic categories\nand data statistics both correlate well ( ≈0.35 and\n≈0.82, respectively) with the performance of lan-\nguage models.\nFor the categorical variables we compare the mean\nvalues per category with the numerical dependent\nvariable. As such,η2 can be interpreted as the amount\nof variation explained by the model - the resulting\nhigh correlations suggest that perplexities tend to be\nhomogeneous for languages of a same morphological\ntype, especially so for state-of-the-art models.\nThis is intuitively evident in Figure 2, where per-\nplexity scores of Char-CNN-LSTM+AP are plot-\nted against type/token ratio. Isolating languages are\nplaced on the left side of the spectrum as expected,\nwith low type/token ratio and good performance (e.g.,\nVI, ZH). As for fusional languages, sub-groups be-\nhave differently. We ﬁnd that Romance and Germanic\nlanguages display roughly the same level of perfor-\nmance as isolating languages, despite their overall\nlarger type/token ratio. Balto-Slavic languages (e.g.\nCS, LV) instead show both higher perplexities and\nhigher type/token ratio. These differences may be\nexplained in terms of different inﬂectional synthesis.\nIntroﬂexive and agglutinative languages can be\n460\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\nVariables\nIndependent Dependent Statistical Test Models\nKN5 LSTM +Char-CNN ++AP\nTrain type/token PPL Pearson’s ρ0.833 0.813 0.823 0.831\nTest new types PPL Pearson’s ρ 0.860 0.803 0.818 0.819\nMorphology PPL one-way ANOVA η2 0.354 0.338 0.369 0.374\nLSTM vs +CharCNN +CharCNN vs ++AP\nTrain type/token ∆ PPL Pearson’s ρ 0.729 0.778\nMorphology ∆ PPL one-way ANOVA η2 0.308 0.284\nTable 5: Correlations between model performance and language typology as well as with corpus statistics\n(type/token ratio and new word types in test data). All variables are good performance predictions.\nfound mostly on the right side of the spectrum in\nterms of performance (see Figure 2). Although the\nlanguages with highest absolute perplexity scores are\ncertainly classiﬁed as agglutinative (e.g., Dravidian\nlanguages such as KN and TA), we also ﬁnd some\noutliers in the agglutinative languages (EU) with re-\nmarkably low perplexity scores.\n7.3 Corpus Size and Type/Token Ratio\nBuilding on the strong correlation between\ntype/token ratio and model performance from\nSection 7.2, we now further analyse the results\nin light of corpus size and type/token statistics.\nThe LM datasets for our 50 languages are similar\nin size to the widely used English PTB dataset\n(Marcus et al., 1993). As such, we hope that these\nevaluation datasets can help guide multilingual\nlanguage modeling research across a wide spectrum\nof languages.\nHowever, our goal now is to verify that type/token\nratio and not absolute corpus size is the deciding\nfactor when unraveling the limitations of standard\nLM architectures across different languages. To this\nend, we conduct additional experiments on all lan-\nguages of the recent Multilingual Wikipedia Corpus\n(MWC) (Kawakami et al., 2017) for language mod-\neling, using the same setup as before (see Table 3).\nThe corpus provides datasets for 7 languages from\nthe same domain as our benchmarks (Wikipedia),\nand comes in two sizes. We choose the larger corpus\nvariant for each language, which provides about 3-5\ntimes as many tokens as contained in our data sets\nfrom Table 4.\nThe results on the MWC evaluation data along\nwith corpus statistics are summarised in Table 6. As\none important ﬁnding, we observe that the gains in\nperplexity using our ﬁne-tuning AP method extend\nalso to these larger evaluation datasets. In particular,\nwe ﬁnd improvements of the same magnitude as in\nthe PTB-sized data sets over the strongest baseline\nmodel (Char-CNN-LSTM) for all MWC languages.\nFor instance, perplexity is reduced from 1781 to 1578\nfor Russian, and from 365 to 352 for English. We also\nobserve a gain for French and Spanish with perplexity\nreduced from 282 to 272 and 255 to 243 respectively.\nIn addition, we test on samples of the Europarl\ncorpus (Koehn, 2005; Tiedemann, 2012) which con-\ntains approximately 10 times more tokens than our\nPTB-sized evaluation data: we use 400K sentences\nfrom Europarl for training and testing. However, this\ndata comes from a much narrower domain of parlia-\nmentary proceedings: this property yields a very low\ntype/token ratio as visible from Table 6. In fact, we\nﬁnd the type/token ratio in this corpus to be on the\nsame level or even smaller than isolating languages\n(compare with the scores in Table 4): 0.02 for Dutch\nand 0.03 for Czech. This leads to similar perplexi-\nties with and without +AP for these two selected test\nlanguages. The third EP test language, Finnish, has a\nslightly higher type/token ratio. Consequently, we do\nobserve an improvement of 10 points in perplexity. A\nmore detailed analysis of this phenomenon follows.\nTable 7 displays the overall type/token ratio in\nthe training set of these copora. We observe that the\nMWC has comparable or even higher type/token\nratios than the smaller sets despite its increased\nsize. The corpus has been constructed by sampling\nthe data from a variety of different Wikipedia cate-\ngories (Kawakami et al., 2017): it can therefore be\nregarded as more diverse and challenging to model.\n461\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\nLang Corpus # V ocab # Tokens Type/Token Char-CNN-LSTM +AP\ntrain test train test train\nnl EP 197K 200K 10M 255K 0.02 62 63\ncs EP 265K 268K 7.9M 193K 0.03 180 186\nen MWC 310K 330K 5.0M 0.5M 0.06 365 352\nes MWC 258K 277K 3.7M 0.4M 0.07 255 243\nfr MWC 260K 278K 4.0M 0.5M 0.07 282 272\nﬁ EP 459K 465K 6.8M 163K 0.07 515 505\nde MWC 394K 420K 3.8M 0.3M 0.10 710 665\nru MWC 372K 399K 2.5M 0.3M 0.15 1781 1578\ncs MWC 241K 258K 1.5M 0.2M 0.16 2396 2159\nﬁ MWC 320K 343K 1.5M 0.1M 0.21 5300 4911\nTable 6: Results on the larger MWC data set (Kawakami et al., 2017) and on a subset of the Europarl (EP)\ncorpus. Improvements with +AP are not dependent on corpus size, but rather they strongly correlate with the\ntype/token ratio of the corpus.\nType/Token Ratio\nLanguage Our Data MWC Europarl\nCzech 0.13 0.16 0.03\nGerman 0.12 0.10 -\nEnglish 0.06 0.06 -\nSpanish 0.07 0.07 -\nFinnish 0.20 0.21 0.07\nFrench 0.07 0.07 -\nRussian 0.14 0.15 -\nDutch 0.09 - 0.02\nTable 7: Comparison of type/token ratios in the cor-\npora used for evaluation. The ratio is not dependent\nonly on the corpus size but also on the language and\ndomain of the corpus.\nEuroparl on the other hand shows substantially lower\ntype/token ratios, presumably due to its narrower do-\nmain and more repetitive nature.\nIn general, we ﬁnd that although the type/token\nratio decreases with increasing corpus size, the de-\ncreasing rate slows down dramatically at a certain\npoint (Herdan, 1960; Heaps, 1978). This depends\non the typology of the language and domain of the\ncorpus. Figure 3 shows the empirical proof of this\nintuition. We show the variation of type/token ratios\nin Wikipedia and Europarl with increasing corpus\nsize. We can see that in a very large corpus of 800K\nsentences, the type/token ratio in MRLs such as Ko-\nrean or Finnish stays close to 0.1, a level where we\nstill expect an improvement in perplexity with the\nproposed AP ﬁne-tuning method applied on top of\n0K 100K 200K 300K 400K 500K 600K 700K 800K\nnumber of sentences\n0.0\n0.1\n0.2\n0.3\n0.4type/token ratio\nFormatStrFormatter('%dK')\nfi europarl\nnl europarl\nfi wiki\nde wiki\nnl wiki\nko wiki\nFigure 3: Type/token ratio values vs. corpus\nsize. A domain-specifc corpus (Europarl) has a\nlower type/token ratio than a more general corpus\n(Wikipedia), regardless of the absolute corpus size.\nChar-CNN-LSTM.\nIn order to isolate and verify the effect of the\ntype/token ratio, we now present results on synthet-\nically created data sets where the ratio is controlled\nexplicitly. We experiment with subsets of the German\nWikipedia with equal number of sentences (25K) 6,\ncomparable number of tokens, but varying type/token\nratio. We generate these controlled data sets by clus-\ntering sparse bag-of-words sentence vectors with the\nk-means algorithm, sampling from different clusters,\n6We split the data into 20K training, 2.5K validation and\n2.5K test sentences\n462\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\nClusters # V ocab # Tokens Type/Token Char-CNN-LSTM +AP\ntrain test train test train\n2 48K 52K 382K 47K 0.13 225 217\n2,4 69K 75K 495K 62K 0.14 454 420\n2,4,5,9 78K 84K 494K 62K 0.16 605 547\n5,9 84K 91K 492K 62K 0.17 671 612\n5 66K 72K 372K 46K 0.18 681 598\nTable 8: Results on German with data sets of comparable size and increasing type/token ratio.\n0.13 0.14 0.15 0.16 0.17 0.18\ntype/token ratio\n200\n300\n400\n500\n600\n700\n800perplexity\nCharCNN-LSTM\nCharCNN-LSTM+AP\nFigure 4: Visualisation of results from Table 8. The\nAP method is especially helpful for corpora with high\ntype/token ratios.\nand then selecting the ﬁnal combinations according\nto their type/token ratio and the number of tokens.\nCorpora statistics along with corresponding perplex-\nity scores are shown in Table 8, and plotted in Fig-\nure 4. These results clearly demonstrate and verify\nthat the effectiveness of the AP method increases for\ncorpora with higher type/token ratios. This ﬁnding\nalso further supports the usefulness of the proposed\nmethod for morphologically-rich languages in partic-\nular, where such high type/token ratios are expected.\n8 Conclusion\nWe have presented a comprehensive language mod-\neling study over a set of 50 typologically diverse\nlanguages. The languages were carefully selected to\nrepresent a wide spectrum of different morphological\nsystems that are found among the world’s languages.\nOur comprehensive study provides new benchmarks\nand language modeling baselines which should guide\nthe development of next-generation language models\nfocused on the challenging multilingual setting.\nOne particular LM challenge is an effective learn-\ning of parameters for infrequent words, especially\nfor morphologically-rich languages (MRLs). The\nmethodological contribution of this work is a new\nneural approach which enriches word vectors at the\nLM output with subword-level information to cap-\nture similar character sequences and consequently to\nfacilitate word-level LM prediction. Our method has\nbeen implemented as a ﬁne-tuning step which grad-\nually reﬁnes word vectors during the LM training,\nbased on subword-level knowledge extracted in an\nunsupervised manner from character-aware CNN lay-\ners. Our approach yields gains for 47/50 languages\nin the challenging full-vocabulary setup, with largest\ngains reported for MRLs such as Korean or Finnish.\nWe have also demonstrated that the gains extend to\nlarger training corpora, and are well correlated with\nthe type-to-token ratio in the training data.\nIn future work we plan to deal with the open vocab-\nulary LM setup and extend our framework to also han-\ndle unseen words at test time. One interesting avenue\nmight be to further ﬁne-tune the LM prediction based\non additional evidence beyond purely contextual in-\nformation. In summary, we hope that this article will\nencourage further research into learning semantic\nrepresentations for rare and unseen words, and steer\nfurther developments in multilingual language model-\ning across a large number of diverse languages. Code\nand data are available online: http://people.\nds.cam.ac.uk/dsg40/lmmrl.html.\nAcknowledgments\nThis work is supported by the ERC Consolidator\nGrant LEXICAL (648909). We thank all editors and\nreviewers for their helpful feedback and suggestions.\n463\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\nReferences\nMartin Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg Corrado,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-\nmawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving,\nMichael Isard, Yangqing Jia, Lukasz Kaiser, Manju-\nnath Kudlur, Josh Levenberg, Dan Man, Rajat Monga,\nSherry Moore, Derek Murray, Jon Shlens, Benoit\nSteiner, Ilya Sutskever, Paul Tucker, Vincent Van-\nhoucke, Vijay Vasudevan, Oriol Vinyals, Pete Warden,\nMartin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016.\nTensorFlow: Large-scale machine learning on hetero-\ngeneous distributed systems. CoRR, abs/1603.04467.\nOliver Adams, Adam Makarucha, Graham Neubig, Steven\nBird, and Trevor Cohn. 2017. Cross-lingual word\nembeddings for low-resource language modeling. In\nProceedings of EACL, pages 937–947.\nRami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013.\nPolyglot: Distributed word representations for multilin-\ngual NLP. In Proceedings of CoNLL, pages 183–192.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Research,\n3:1137–1155.\nBalthasar Bickel and Johanna Nichols, 2013. Inﬂectional\nSynthesis of the Verb. Max Planck Institute for Evolu-\ntionary Anthropology, Leipzig.\nOndˇrej Bojar, Christian Buck, Chris Callison-Burch,\nChristian Federmann, Barry Haddow, Philipp Koehn,\nChristof Monz, Matt Post, Radu Soricut, and Lucia\nSpecia. 2013. Findings of the 2013 Workshop on Sta-\ntistical Machine Translation. In Proceedings of the 8th\nWorkshop on Statistical Machine Translation , pages\n1–44.\nJan A. Botha and Phil Blunsom. 2014. Compositional\nmorphology for word representations and language\nmodelling. In Proceedings of ICML, pages 1899–1907.\nKen Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and An-\ndrew Zisserman. 2014. Return of the devil in the\ndetails: Delving deep into convolutional nets. In Pro-\nceedings of BMVC.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, and Phillipp Koehn. 2014. One bil-\nlion word benchmark for measuring progress in statis-\ntical language modeling. In Proceedings of INTER-\nSPEECH, pages 2635–2639.\nXie Chen, Xunying Liu, Yanmin Qian, M.J.F. Gales, and\nPhilip C Woodland. 2016. CUED-RNNLM: An open-\nsource toolkit for efﬁcient training and evaluation of\nrecurrent neural network language models. In Proceed-\nings of ICASSP, pages 6000 –6004.\nRyan Cotterell, Hinrich Schütze, and Jason Eisner. 2016.\nMorphological smoothing and extrapolation of word\nembeddings. In Proceedings of ACL, pages 1651–1660.\nKoen Deschacht and Marie-Francine Moens. 2009. Semi-\nsupervised semantic role labeling using the latent words\nlanguage model. In Proceedings of EMNLP, pages 21–\n29.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning\nResearch, 12:2121–2159.\nManaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris\nDyer, Eduard H Hovy, and Noah A Smith. 2015.\nRetroﬁtting Word Vectors to Semantic Lexicons. In\nProceedings of NAACL-HLT, pages 1606–1615.\nChristiane Fellbaum. 1998. WordNet: An Electronic Lexi-\ncal Database. Bradford Books.\nKatja Filippova, Enrique Alfonseca, Carlos A. Col-\nmenares, Lukasz Kaiser, and Oriol Vinyals. 2015.\nSentence compression by deletion with LSTMs. In\nProceedings of EMNLP, pages 360–368.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. PPDB: The Paraphrase\nDatabase. In Proceedings of NAACL-HLT, pages 758–\n764.\nJoshua T. Goodman. 2001. A bit of progress in language\nmodeling. Computer Speech & Language, 15(4):403–\n434.\nEdouard Grave, Moustapha Cissé, and Armand Joulin.\n2017. Unbounded cache model for online language\nmodeling with open vocabulary. In Proceedings of\nNIPS, pages 6044–6054.\nAlex Graves. 2013. Generating sequences with recurrent\nneural networks. CoRR, abs/1308.0850.\nDerek Greene and Padraig Cunningham. 2006. Practi-\ncal solutions to the problem of diagonal dominance in\nkernel document clustering. In Proceedings of ICML,\npages 377–384.\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H. Clark,\nand Philipp Koehn. 2013. Scalable modiﬁed Kneser-\nNey language model estimation. In Proceedings of\nACL, pages 690–696.\nHarold Stanley Heaps. 1978. Information retrieval, com-\nputational and theoretical aspects. Academic Press.\nGustav Herdan. 1960. Type-token mathematics, volume 4.\nMouton.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nShort-Term Memory. Neural Computation, 9(8):1735–\n1780.\nMarcus Hutter. 2012. The human knowledge compression\ncontest.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the limits\nof language modeling. In Proceedings of ICML.\nDan Jurafsky and James H. Martin. 2017. Speech and\nLanguage Processing, volume 3. Pearson.\n464\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\nKazuya Kawakami, Chris Dyer, and Phil Blunsom. 2017.\nLearning to create and reuse words in open-vocabulary\nneural language modeling. In Proceedings of ACL ,\npages 1492–1502.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M. Rush. 2016. Character-aware neural language\nmodels. In Proceedings of AAAI, pages 2741–2749.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for M-gram language modeling. In Pro-\nceedings of ICASSP, pages 181–184.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of the\n10th Machine Translation Summit, pages 79–86.\nYann LeCun, Bernhard E. Boser, John S. Denker, Donnie\nHenderson, Richard E. Howard, Wayne E. Hubbard,\nand Lawrence D. Jackel. 1989. Handwritten digit\nrecognition with a back-propagation network. In Pro-\nceedings of NIPS, pages 396–404.\nWang Ling, Tiago Luís, Luís Marujo, Ramón Fernández\nAstudillo, Silvio Amir, Chris Dyer, Alan W. Black,\nand Isabel Trancoso. 2015. Finding function in form:\nCompositional character models for open vocabulary\nword representation. In Proceedings of EMNLP, pages\n1520–1530.\nMinh-Thang Luong and Christopher D. Manning. 2016.\nAchieving open vocabulary neural machine translation\nwith hybrid word-character models. In Proceedings of\nACL, pages 1054–1063.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan\nHuang, Andrew Y . Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In Pro-\nceedings of ACL, pages 142–150.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-\nrice Santorini. 1993. Building a large annotated corpus\nof English: The Penn Treebank. Computational Lin-\nguistics, 19(2):313–330.\nTomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan Cer-\nnock`y, and Sanjeev Khudanpur. 2010. Recurrent neu-\nral network based language model. In Proceedings of\nINTERSPEECH, pages 1045–1048.\nYasumasa Miyamoto and Kyunghyun Cho. 2016. Gated\nword-character recurrent language model. In Proceed-\nings of EMNLP, pages 1992–1997.\nNikola Mrkši´c, Ivan Vuli ´c, Diarmuid Ó Séaghdha, Ira\nLeviant, Roi Reichart, Milica Gaši´c, Anna Korhonen,\nand Steve Young. 2017. Semantic specialisation of\ndistributional word vector spaces using monolingual\nand cross-lingual constraints. Transactions of the ACL,\n5:309–324.\nVinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed\nlinear units improve restricted Boltzmann machines. In\nProceedings of ICML, pages 807–814.\nNelleke Oostdijk. 2000. The spoken Dutch corpus.\nOverview and ﬁrst evaluation. In Proceedings of LREC,\npages 887–894.\nOﬁr Press and Lior Wolf. 2017. Using the output embed-\nding to improve language models. In Proceedings of\nEACL, pages 157–163.\nIulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron C. Courville, and Joelle Pineau. 2016. Building\nend-to-end dialogue systems using generative hierar-\nchical neural network models. In Proceedings of AAAI,\npages 3776–3784.\nRupesh Kumar Srivastava, Klaus Greff, and Jürgen\nSchmidhuber. 2015. Highway networks. In Proceed-\nings of the ICML Deep Learning Workshop.\nMartin Sundermeyer, Hermann Ney, and Ralf Schluter.\n2015. From feedforward to recurrent LSTM neural\nnetworks for language modeling. IEEE Transactions\non Audio, Speech and Language Processing, 23(3):517–\n529.\nJörg Tiedemann. 2012. Parallel data, tools and interfaces\nin OPUS. In Proceedings of LREC, pages 2214–2218.\nClara Vania and Adam Lopez. 2017. From characters to\nwords to in between: Do we capture morphology? In\nProceedings of ACL, pages 2016–2027.\nAshish Vaswani, Yinggong Zhao, Victoria Fossum, and\nDavid Chiang. 2013. Decoding with large-scale neural\nlanguage models improves translation. In Proceedings\nof EMNLP, pages 1387–1392.\nLyan Verwimp, Joris Pelemans, Hugo Van hamme, and\nPatrick Wambacq. 2017. Character-word LSTM lan-\nguage models. In Proceedings of EACL, pages 417–\n427.\nIvan Vuli´c, Nikola Mrkši ´c, Roi Reichart, Diarmuid Ó\nSéaghdha, Steve Young, and Anna Korhonen. 2017.\nMorph-ﬁtting: Fine-tuning word vector spaces with\nsimple language-speciﬁc rules. In Proceedings of ACL,\npages 56–68.\nTian Wang and Kyunghyun Cho. 2016. Larger-context\nlanguage modelling with recurrent neural network. In\nProceedings of ACL, pages 1319–1329.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2015. From paraphrase database to composi-\ntional paraphrase model and back. Transactions of the\nACL, 3:345–358.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2015. Recurrent neural network regularization. In\nProceedings of ICLR.\nMatthew D. Zeiler and Rob Fergus. 2014. Visualizing and\nunderstanding convolutional networks. In Proceedings\nof ECCV, pages 818–833.\nGeorge Kingsley Zipf. 1949. Human behavior and the\nprinciple of least effort: An introduction to human ecol-\nogy. Martino Fine Books.\n465\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025\n466\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00032 by guest on 05 November 2025",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9116212129592896
    },
    {
      "name": "Computer science",
      "score": 0.8904123306274414
    },
    {
      "name": "Word (group theory)",
      "score": 0.7141534686088562
    },
    {
      "name": "Language model",
      "score": 0.6990431547164917
    },
    {
      "name": "Natural language processing",
      "score": 0.6945463418960571
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6330778002738953
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.6268523335456848
    },
    {
      "name": "Vocabulary",
      "score": 0.5576488971710205
    },
    {
      "name": "Character (mathematics)",
      "score": 0.5249474048614502
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4445386230945587
    },
    {
      "name": "Linguistics",
      "score": 0.15126639604568481
    },
    {
      "name": "Programming language",
      "score": 0.10884362459182739
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I174306211",
      "name": "Technion – Israel Institute of Technology",
      "country": "IL"
    }
  ],
  "cited_by": 48
}