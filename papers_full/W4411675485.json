{
  "title": "Navigating the Labyrinth: A Review of Explainability and Trustworthiness in Large Language Model-Powered Systems for Sensitive Decision-Making",
  "url": "https://openalex.org/W4411675485",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2519087253",
      "name": "Ayman Asaad",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A5118644989",
      "name": "A. M. Azizul Hassan Chy",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A5118644990",
      "name": "Anzam Shahriar Kabir",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A5118644991",
      "name": "Amrun Nakib",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2785441887",
      "name": "Nazifa Tabassum",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2519087253",
      "name": "Ayman Asaad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5118644989",
      "name": "A. M. Azizul Hassan Chy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5118644990",
      "name": "Anzam Shahriar Kabir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5118644991",
      "name": "Amrun Nakib",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2785441887",
      "name": "Nazifa Tabassum",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4317937895",
    "https://openalex.org/W3135998240",
    "https://openalex.org/W4405876245",
    "https://openalex.org/W3173672122",
    "https://openalex.org/W2981731882",
    "https://openalex.org/W3101981467",
    "https://openalex.org/W4410000132",
    "https://openalex.org/W4396832909",
    "https://openalex.org/W4405107520",
    "https://openalex.org/W4408242673",
    "https://openalex.org/W4313185449",
    "https://openalex.org/W3035371891",
    "https://openalex.org/W4399750638",
    "https://openalex.org/W4401023735",
    "https://openalex.org/W3163885883",
    "https://openalex.org/W3182221256",
    "https://openalex.org/W3187467055",
    "https://openalex.org/W3046238651",
    "https://openalex.org/W3190013839",
    "https://openalex.org/W2952602496",
    "https://openalex.org/W4399449888",
    "https://openalex.org/W4396668454",
    "https://openalex.org/W4393156965",
    "https://openalex.org/W4319655558",
    "https://openalex.org/W3175426713",
    "https://openalex.org/W3003166972",
    "https://openalex.org/W4309289152",
    "https://openalex.org/W3093583920",
    "https://openalex.org/W4409016605",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W3124373176"
  ],
  "abstract": "Large Language Models (LLMs) have proven to be game-changers in sectors that require high-stakes decision-making, such as healthcare, finance, and law. Despite this, LLMs are opaque, black boxes that present significant challenges in terms of explainability and trust, particularly when ethical, legal, and regulatory requirements necessitate transparency. This review examines the state-of-the-art in the intersection of Large Language Models (LLMs), Explainable Artificial Intelligence (XAI), and trustworthy AI. It addresses four research questions: the faithfulness of LLMs' explanations, the current state of evaluation, user-centered and ethical design in XAI systems, and the broader challenge of making LLMs themselves explainable. Using a systematic literature review approach, we reviewed over 500 sources published between 2018 and 2025 and narrowed our selection to draw insights from 40 significant contributions. We categorized the XAI methods into post-hoc, intrinsic, and human-centered approaches, and then mapped these methods to the categories of trustworthiness, including robustness, fairness, privacy, and accountability. The review revealed recurrent themes, including the hallucination of LLM explanations, limited evaluation metrics, the urgent need to create interactive explanation interfaces, and the meta-challenge of explaining complicated LLMs. As a consequence of the review process, we propose a research agenda with short-, medium-, and long-term priorities, including faithfulness verification, user modeling, symbolic-LLM hybrid architectures, and LLMs that are interpretable by design. This review synthesis contributes to the corpus of technical, ethical, and context-specific knowledge, offering foundational insights into the design of transparent, reliable, and human-aligned LLM-powered systems for sensitive domains.",
  "full_text": null,
  "topic": "Trustworthiness",
  "concepts": [
    {
      "name": "Trustworthiness",
      "score": 0.6704117059707642
    },
    {
      "name": "Decision-making models",
      "score": 0.5513177514076233
    },
    {
      "name": "Computer science",
      "score": 0.47893795371055603
    },
    {
      "name": "Psychology",
      "score": 0.38418877124786377
    },
    {
      "name": "Computer security",
      "score": 0.3017549514770508
    },
    {
      "name": "Artificial intelligence",
      "score": 0.21562564373016357
    }
  ],
  "institutions": [],
  "cited_by": 1
}