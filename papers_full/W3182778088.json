{
  "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills",
  "url": "https://openalex.org/W3182778088",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3123211793",
      "name": "Ori Yoran",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2609247393",
      "name": "Alon Talmor",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A86633810",
      "name": "Jonathan Berant",
      "affiliations": [
        "Tel Aviv University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3127307050",
    "https://openalex.org/W3176229980",
    "https://openalex.org/W3185011771",
    "https://openalex.org/W3035275890",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W3155396033",
    "https://openalex.org/W2949849869",
    "https://openalex.org/W3034655581",
    "https://openalex.org/W2951873305",
    "https://openalex.org/W2995638926",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W2911681509",
    "https://openalex.org/W2995649781",
    "https://openalex.org/W3172267148",
    "https://openalex.org/W2971822538",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2333897677",
    "https://openalex.org/W2913178646",
    "https://openalex.org/W4287659415",
    "https://openalex.org/W2951036431",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W3213159541",
    "https://openalex.org/W2995793065",
    "https://openalex.org/W3101007570",
    "https://openalex.org/W3035019713",
    "https://openalex.org/W2964327384",
    "https://openalex.org/W3103667349",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W2995628494",
    "https://openalex.org/W2958892036",
    "https://openalex.org/W4293579911",
    "https://openalex.org/W2996251235",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W4287214436",
    "https://openalex.org/W3148237218",
    "https://openalex.org/W3034284249",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3034407524",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W4298018319",
    "https://openalex.org/W3099524945",
    "https://openalex.org/W3087148478",
    "https://openalex.org/W3035438620",
    "https://openalex.org/W2971044268",
    "https://openalex.org/W3100618740",
    "https://openalex.org/W3176119108",
    "https://openalex.org/W3126918322",
    "https://openalex.org/W3174828871",
    "https://openalex.org/W3153924949",
    "https://openalex.org/W3088874071",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2971124188",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4210451781",
    "https://openalex.org/W3147136710",
    "https://openalex.org/W2970900584",
    "https://openalex.org/W2996657743",
    "https://openalex.org/W2964120615",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4229907684",
    "https://openalex.org/W3166298099",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2970789589",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W3162734203",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2983984338",
    "https://openalex.org/W2963995027"
  ],
  "abstract": "Models pre-trained with a language modeling objective possess ample world knowledge and language skills, but are known to struggle in tasks that require reasoning. In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we sample examples from reasoning skills where the model currently errs. We evaluate our approach on three reasoning-focused reading comprehension datasets, and show that our model, PReasM, substantially outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling examples based on model errors leads to faster training and higher performance.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6016 - 6031\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nTurning Tables: Generating Examples from Semi-structured Tables for\nEndowing Language Models with Reasoning Skills\nOri Yoran† Alon Talmor Jonathan Berant\nTel-Aviv University\n{ori.yoran, alon.talmor, joberant}@cs.tau.ac.il\nAbstract\nModels pre-trained with a language model-\ning objective possess ample world knowledge\nand language skills, but are known to struggle\nin tasks that require reasoning. In this work,\nwe propose to leverage semi-structured tables,\nand automatically generate at scale question-\nparagraph pairs, where answering the question\nrequires reasoning over multiple facts in the\nparagraph. We add a pre-training step over this\nsynthetic data, which includes examples that\nrequire 16 different reasoning skills such as\nnumber comparison, conjunction, and fact com-\nposition. To improve data efficiency, we sam-\nple examples from reasoning skills where the\nmodel currently errs. We evaluate our approach\non three reasoning-focused reading compre-\nhension datasets, and show that our model,\nPReasM, substantially outperforms T5, a popu-\nlar pre-trained encoder-decoder model. More-\nover, sampling examples based on model errors\nleads to faster training and higher performance.\n1 Introduction\nLarge pre-trained language models (LMs) (Devlin\net al., 2019; Liu et al., 2019; Brown et al., 2020)\nhave become the backbone of natural language pro-\ncessing in recent years. However, recent work has\nshown that they struggle in performing symbolic\nreasoning operations, such as composition or con-\njunction of facts (Talmor et al., 2019, 2020), numer-\nical operations (Wallace et al., 2019; Hidey et al.,\n2020), and quantification (Warstadt et al., 2019),\nwithout substantial amounts of additional data.\nPast work on improving reasoning in pre-trained\nmodels has taken two flavors: (a) adding special-\nized components for specific skills, like numerical\nand temporal reasoning (Ran et al., 2019; Gupta\net al., 2020a; Khot et al., 2021; Chen et al., 2020a),\nor (b) generating synthetic examples at scale, for\nexample, by using grammars or templates (Rozen\n†Work done while working at the Allen Institute for Arti-\nficial Intelligence.\nFigure 1: An example table and question-context-\nanswer triplets generated from the table as synthetic\ndata. Each color corresponds to a different reasoning\nskill and colored cells are necessary to answer the ques-\ntion. The contexts shown are partial, such that the actual\ncontext contains the necessary information to answer\nthe question and additional distractors. Answers are not\nnecessarily extractive (e.g., date difference).\net al., 2019; Zhao et al., 2019; Andreas, 2020; Asai\nand Hajishirzi, 2020; Campagna et al., 2020), and\nquestion generation models (Alberti et al., 2019;\nPuri et al., 2020; Bartolo et al., 2021).\nIn this work, we take the latter approach and\nargue that semi-structured tables are a valuable re-\nsource for automatic generation of training data\nthat can endow LMs with reasoning skills. Tables\ncan be crawled from the web at scale, and cover\na wide range of domains and topics. Moreover,\ntheir structured nature makes them amenable to au-\ntomatic processes of data generation. Specifically,\ngiven a table, we use templates to generate reading\ncomprehension (RC) examples, that is, question-\ncontext-answer triplets, where answering the ques-\ntion requires diverse types of reasoning over facts\nmentioned in the context. Fig. 1 shows an example\ntable, and three generated question-context-answer\nexamples, which require fact composition, num-\nber comparison, and computing a date difference.\n6016\nFigure 2: Approach overview. First, we use tables to generate large amounts of data from 16 different example\ngenerators (EGs), each corresponding to a different reasoning skill. Then, a pre-trained LM is trained over this data\nto obtain our model, PReasM, where we sample examples based on current model errors (arrow width corresponds\nto number of examples). Last, our model is fine-tuned and evaluated on target tasks that require reasoning.\nUnlike prior work where semi-structured data was\nused for reasoning over tables or knowledge-bases\n(Eisenschlos et al., 2020; Yin et al., 2020; Herzig\net al., 2020; Yu et al., 2021), here we harness tables\nto allow LMs to reason over text directly.\nFig. 2 provides an overview of our approach. We\ngenerate data by crawling tables from Wikipedia,\nand applying 16 different example generators (EGs)\non each table. Each EG corresponds to a particular\nreasoning skill (composition, numerical compari-\nson, see Table 1 for full list), and comprises a small\nset of question templates. Variables in the tem-\nplates are filled with content from the table, and the\nstructure of the table allows to compute the answer\nautomatically. The context is a list of facts gener-\nated from the table that contain facts required for\nanswering the question as well as distractor facts.\nWe add a pre-training step over this generated\ndata, where we perform multi-task training over the\n16 task corresponding to the EGs. Since each EG\ncan generate vast numbers of examples, it is impor-\ntant to focus training on reasoning skills that the\nmodel lacks. Thus, we use error-driven sampling\n(Gottumukkala et al., 2020) to construct training\nbatches, where most examples are sampled from\nEGs that the model currently struggles with.\nWe fine tune our Pre-traind for Reasoning\nModel, PReasM, on three RC datasets that require\nreasoning: DROP (Dua et al., 2019), IIRC (Fergu-\nson et al., 2020), and MMQA (Talmor et al., 2021).\nPReasM outperforms the original pre-trained T5\n(Raffel et al., 2020) model by significant margins:\n7.6, 4.1, and 1.2 F1 points, respectively. Our results\nset a new state-of-the-art on MMQA and are the\nbest results on IIRC for models where the retriever\nand reader are trained separately. Our analysis\nshows that PReasM leads to improvements of up\nto 40 F1 points on specific question types, such as\ncomputing the difference between two dates, with-\nout causing a drop in other question types.\nIn conclusion, our results suggest that tables are\na viable and untapped source of information for\nautomatically generating large amounts of data that\ncan be used to endow LMs with skills that are\nnot captured using current pre-training approaches.\nOur code, data, and models are publicly available\nand can be downloaded fromhttps://github.\ncom/oriyor/turning_tables.\n2 Data Generation\nOur goal is to train a RC model that given a ques-\ntion q and textual context c returns an answer a,\ngiven a training set D = {(qi, ci, ai)}N\ni=1. We fo-\ncus on questions that require reasoning over the\ncontext, e.g., composing two facts. To endow LMs\nwith reasoning skills, we want to generate a large\nsynthetic training set Dsyn = {(qj, cj, aj)}M\nj=1\n(M ≫ N) from semi-structured tables, before fine-\ntuning on a target dataset.\n2.1 Generating Examples from Tables\nWe use tables from English Wikipedia1 to gener-\nate Dsyn. English Wikipedia includes millions of\ntables with high lexical and domain diversity (Fe-\ntahu et al., 2019; Chen et al., 2020b; Gupta et al.,\n2020b; Talmor et al., 2021; Nan et al., 2021; Neer-\naja et al., 2021a). We extract from Wikipedia all\ntables T that have at least two columns and 10-25\nrows, resulting in more than 700K tables. Then, we\nannotate all table columns with their semantic type\n(STRING, NUMBER, or DATE), which allows us to\ngenerate questions that involve manipulating num-\nbers and dates. Details on the process of column\nannotation are in §A.1.\n1We use the 01-01-2020 Wikipedia dump.\n6017\nEG Template Question\n2/3-hop What was the col:1(s) when the col:2 was val:2 in “What was the Play(s) when the Author was William Shakespeare in Notable works\nComposition table-title of page-title? of Lidia Zamkow?”\nConjunction What was the col:1 when the col:2 was val:2 and the col:3 “What was the Common name when the Family was Picidae and the Distribution was\nwas val:3 in table-title of page-title? Okinawa in List of species of List of endemic birds of Japan?”\nQuantifiers Is val:1 the only col:1 that has col:2 val:2in table-title “Is Jean Philippe the only Artist that has Language French in Results of Eurovision\nOnly of page-title? Song Contest 1959?”\nQuantifiers In table-title of page-title, does [OPERATOR]col:1 “In Coal of List of Mines in South Africa, does every Mine have Owner Exxaro?”\nEvery/Most have col:2 val:2?\nNum. In table-title of page-title, which col:1 had “In Administration of Mueang Nonthaburi District, which Name had a higher\nComparison [OPERATOR] col:2: val:1 or val:1? population: Suan Yai or Bang Khen?”\nTemp. In table-title of page-title, what happened [OPERATOR]: “In Awards and nominations of Alexandre Pires, what happened earlier: the\nComparison the col:1 was val:1 or the col:2 was val:2? Category was Pop New Artist or the Category was Album of the Year?”\nNum. Yes/No In table-title of page-title did val:1 have [OPERATOR] “In Top employers of Chula Vista, California, did Walmart have more Employees\nComparison col:2 than val:1? than Target?”\nTemp. Yes/No The col:1 was val:1 [OPERATOR] thecol:2 was val:2 in “The Referee was Salim Oussassi more recently than when the Referee was\nComparison table-title of page-title? Rachid Medjiba in 1980 to 1999 of Algerian Cup Final referees?”\nTemp./Num. In table-title of page-title, which col:1 has the “In List of graphic novels of Minx (comics), which Title has the earliest Release\nSuperlatives [OPERATOR] col:2? date?”\nArithmetic In table-title of page-title, what was the [OPERATOR] “In By rocket of 1961 in spaceflight, what was the highest Successes when the\nSuperlatives col:1 when the col:2 was val:2? Remarks was Maiden flight?”\nCounting How many col:1 have col:2 val:2in table-title of “How many Elections have Candidate John Kufuor in Presidential elections of New\npage-title? Patriotic Party?”\nArithmetic In table-title of page-title, what was the total number of “In Assists table of 2010-11 La Liga, what was the total number of Assists when the\nAddition col:1 when the col:2 was val2? Club was Villarreal?”\nDate In table-title of page-title, how much time had passed bet- “In Notable events | Concerts of Candlestick Park, how much time had passed\nDifference ween when the col:1 was val:1 and when the col:2 was val:2? between when the Artist was Paul McCartney and when the Artist was The Beatles?”\nTable 1: Question templates with examples for all EGs. Variable names specify permissible instantiations, where\ncol is a column name, val is a value, and indices denote that a value must originate from a particular column.\n2/3-hop composition examples are generated by generating 2/3-long fact chains between the answer and the value\nin the question. For example, above, the chain will include the facts “The Role when the Author was Shakespeare\nwas Lady Macbeth. The Play when the Role was Lady Macbeth was Macbeth”. ‘[OPERATOR]’ corresponds to\nEG-specific operators that we instantiate, e.g., in the EG ‘Temp. comparison’ [OPERATOR] is replaced with the\noperators ‘earlier’ or ‘later’. Some EGs are collapsed into a single row (e.g., Quantifiers Every/Most).\nThe core of the generation process are the ex-\nample generators (EGs), each corresponding to a\nreasoning skill (Table 1). Each example genera-\ntor g ∈ Gis a function that takes a table t ∈ T\nand randomly samples at most ten (q, c, a) triplets\nfrom the set of all possible triplets, where (i) q is\na question is pseudo-language, (ii) c is the context,\ni.e., a list of facts extracted from t that includes the\ngold facts necessary for answering q and distractor\nfacts, all phrased in pseudo-language, and (iii) a\nis the answer. Overall, the synthetic training set is\nDsyn = S\nt∈T\nS\ng∈G g(t).\nEGs generate examples in the following way.\nEach EG is associated with one or more question\ntemplates, which differ in their surface phrasing.2\nTemplates contain typed variables that are instanti-\nated with content from the table (see all variables\nin Table 1). Column and value variables are in-\ndexed to specify that the variable val:i must be\ninstantiated by a value from the column col:i.\nInstantiating all variables results in the question\n2We also experimented with using just one question tem-\nplate per EG and observed very similar downstream results.\nq and the template allows us to programmatically\ncompute the answer a. E.g., in the question from\nFig. 1: “In League Cup of 1990–91 Chelsea F .C.\nseason, Which Round had a higher Attendance: QF\nor QFR?” the answer a can be found by finding\nthe rows with the values “QF” and “QFR” in the\ncolumn “Round”, and returning the value that has\na higher number in the column “Attendance”.\nThe context c is generated from the content nec-\nessary for answering the question, which can be\nidentified using the instantiated question template.\nFacts generally have the form “The col:1 when\nthe col:2 was val:2 was val:1”. E.g., to\nanswer the question above, we generate the gold\nfacts “The Attendance when the Round was QF was\n34,178”, and “The Attendance when the Round was\nQFR was 33,861”. We also generate distractors by\ngenerating facts from rows or columns that are not\nrelevant for the question, e.g., “The Attendance\nwhen the Round was R4 was 9,789”.\nOverall, our process results in a large set Dsyn,\nwhich includes examples from 16 EGs (all shown\nin Table 1).\n6018\nEG Question Context Answer\n3-hop What was the Result(s) when the In League Cup of 1990-91 Chelsea F.C. season: The attendance when the round was R2 1st Leg was 5,666. 2-1\nComposition Round was R4 in League Cup of The result when the date was 6 November 1990 was 3-2. The date when the attendance was 34,669 was 27\n1990-91 Chelsea F.C. season? February 1991. The attendance when the round was QF was 34,178. The date when the attendance was\n34,074 was 24 February 1991. The date when the attendance was 16,085 was 6 November 1990. The\nattendance when the round was R3 was 16,699. The date when the attendance was 9,789 was 28 November\n1990. The result when the date was 28 November 1990 was 2-1. The result when the date was 31 October\n1990 was 0-0. The attendance when the round was QFR was 33,861. The result when the date was 16\nJanuary 1991 was 0-0. The attendance when the round was R4 was 9,789. The result when the date was\n10 October 1990 was 4-1 (won 9-1 on agg). The date when the attendance was 5,666 was 26 September 1990.\nCounting In Presidential elections of New In Presidential elections of New Patriotic Party: The candidate when the election was 1992 was Albert Adu 4\nPatriotic Party, how many Boahen. The candidate when the election was 2008 (1) was Nana Akufo-Addo. The candidate when\nElections have Candidate John the election was 2000 (2nd) was John Kufuor. The candidate when the election was 2000 (1st) was John\nKufuor? Kufuor. The candidate when the election was 1992 was Albert Adu Boahen. The candidate when the election\nwas 2004 was John Kufuor. The candidate when the election was 1996 was John Kufuor.The candidate\nwhen the election was 2008 (2) was Nana Akufo-Addo.\nDate In Notable concerts of In Notable concerts of Comiskey Park: The artist was Rush in August 19, 1979. The artist was The Police 17 years,\nDifference Comiskey Park, how much in July 23, 1983. The dates when the artist was The Jacksons were October 12, 1984, October 13, 1984, and 11 months,\ntime had passed between when October 14, 1984. The artist was Simon and Garfunkel in July 24, 1983. The artist was The Beatles in and 3 days\nthe Artist was The Beatles and August 20, 1965. The date when the artist was Aerosmith was July 10, 1976.\nwhen the Artist was The Police?\nTable 2: Examples for generated (q, c, a) triplets. The first example is from the table in Fig. 1. Gold facts are bolded.\nEG # Questions EG # Questions\n2-Hop composition 277,069 3-Hop composition 364,772\nConjunction 353,738 Only quantifier 522,071\nMost quantifier 94,180 Every quantifier 16,693\nNumber comparison 410,749 Number Y/N comparison 410,749\nTemporal comparison 453,499 Temporal Y/N comparison 470,694\nNumber superlatives 125,144 Temporal superlatives 80,884\nArithmetic superlatives 183,892 Arithmetic addition 86,969\nCounting 484,471 Date difference 452,061\nTotal 4,787,635\nTable 3: Number of examples generated by each EG.\nDuring data generation, we randomly generate at most\n10 examples from each EG and table.\n2.2 Data Analysis\nData generation yields 4.8M questions from over\n176K tables and 130K pages. Table 2 contains\nexamples for generated (q, c, a) triplets, including\nthe full context c. Table 3 shows the number of\ngenerated examples for each EG. The number of\ndistinct words is large (850K), illustrating the wide\ncoverage and high lexical diversity of our approach.\nMoreover, generated examples have diverse answer\ntypes, which include text spans ( 43.2%), yes/no\nquestions (31.6%), numeric (15.8%), and date an-\nswers (9.4%). In addition, our questions cover a\nwide range of domains including popular culture,\npolitics and science. Tables cover more than 2,500\ndifferent Wikipedia categories, with 150 categories\ncovering 80% of the data. Fig. 3 presents the most\ncommon categories of the Wikipedia pages from\nwhich we scraped our tables.\n3 Training\nSince our EGs generate large quantities of exam-\nples, one can think of each EG as providing an\ninfinite stream of examples. In this setup, a natural\nquestion is how to construct training batches such\nthat the model learns the required skills as quickly\nFigure 3: The most frequent categories of our Wikipedia\npages and their frequency. Colors represent domains.\nas possible. After briefly describing our model, we\nwill detail our training framework, where we sam-\nple examples from EGs in an error-driven manner.\nModel We use a standard encoder-decoder ar-\nchitecture (Raffel et al., 2020; Lewis et al., 2020).\nGiven a training example (q, c, a), the model takes\nas input the sequence of tokens ‘ q [SEP] c’, and\nthe task is to autoregressively decode the answer\na token-by-token. We train to maximize the maxi-\nmum likelihood objective log P(a | q, c).\n3.1 Multi-task Training over EGs\nGiven a pre-trained LM, we add another pre-\ntraining step, where we multi-task over a set of\ntasks S, each task corresponding to examples gener-\nated from one EG. Similar to past work (Yogatama\net al., 2019; Geva et al., 2020), to avoid “catas-\ntrophic forgetting” (Kirkpatrick et al., 2016) of the\n6019\nlanguage skills, we sample batches from the origi-\nnal pre-training task with probability λ = 0.5.\nPast work (Gottumukkala et al., 2020) has shown\nthat heterogeneous batching, i.e., having examples\nfrom all tasks in each batch, leads to better perfor-\nmance compared to having entire batches from a\nsingle task. We follow this practice, and in every\nbatch sample examples from every task according\nto a probability distribution Ptasks ∈ R|S|. The\nmain question is how to determine the distribution\nPtasks, which we turn to next.\n3.2 Sampling Strategies\nWe describe strategies for computing Ptasks, start-\ning with the commonly-used uniform sampling ap-\nproach, and then turn to error-driven approaches.\nUniform sampling Past work (Khashabi et al.,\n2020; Raffel et al., 2020; Wang et al., 2020) used\nuniform sampling, where the probability to sam-\nple from a task s is Ptasks(s) = 1\n|S| , as a-priori\nall tasks are equally important. Some approaches\nalso sample examples in proportion to the size of\nthe training set (Raffel et al., 2020; Wang et al.,\n2020). This is not applicable in our case, where\nwe assume an infinite stream of examples for every\ntask, and make no assumptions on the distribution\nover reasoning skills in the downstream test set.\nError sampling Recent work (Sharma et al.,\n2018; Gottumukkala et al., 2020) proposed to con-\nstruct Ptasks based on model errors, where one\nover-samples tasks with higher errors. More for-\nmally, let Ceil(s) be an estimate of the maximum\naccuracy achievable on a task s, and Acc(s) be\nthe current model accuracy for task s on an held-\nout set. We define ∆(s) = Ceil(s) − Acc(s) and\nPtasks(s) = ∆(s)P\ns′∈S ∆(s′). The distribution Ptasks\nof a task is updated every time we evaluate the cur-\nrent model on the held-out data. In our setup, since\nthe data is synthetic and abundant, we assume that\nthe ceiling accuracy for all tasks is 1.0, and hence:\n∆(s) = 1 .0 − Acc(s). Similar to Gottumukkala\net al. (2020), we use accuracy over a held-out set\nrather than the training loss, as this corresponds\ndirectly to our target metric.\nMomentum sampling A potential issue with er-\nror sampling, is that if the error rate on a task is\nhigh, the model will spend most of its time on that\ntask at the expense of other tasks, which may lead\nto low data efficiency. To remedy this, we intro-\nduce momentum sampling, a sampling strategy that\nAlgorithm 1 Momentum Sampling(w, t, ϵ, k)\nInput: windows size w, training time t, minimum share of\nexamples per task ϵ, smoothing factor k.\n1: for s ∈ Sdo\n2: if t ≥ w then\n3: Acchead ← 1\nk\nPt\ni=t−k Accs(i)\n4: Acctail ← 1\nk\nPt−w+k\ni=t−w Accs(i)\n5: Ptasks[s] ← max(|Acchead − Acctail|, ϵ)\n6: else\n7: Ptasks[s] ← 1/|S|\n8: Ptasks ← Ptasks/∥Ptasks∥1\nsamples from a task in proportion to its rate of im-\nprovement, putting most probability mass on skills\nthat are improving rapidly.\nAlg. 1 provides the details of momentum sam-\npling. Let t denote the index of a checkpoint eval-\nuated on the held-out set, let w be a window size,\nand Accs(i) be the held-out accuracy of checkpoint\ni on task s. We estimate model accuracy on a task\ns at the beginning and end of the window, and\nsample examples in proportion to the difference3\nin accuracy during that window. To smooth out\naccuracy fluctuations in adjacent checkpoints, we\nestimate accuracy as an average of k model check-\npoints. During the first w checkpoint evaluations,\nwe simply use uniform sampling.\nMomentum sampling has several theoretical ben-\nefits over error sampling. First, it does not assume\nanything on the ceiling accuracy of a task. Sec-\nond, when all tasks converge to their ceiling accu-\nracy, momentum sampling converges to uniform\nsampling, unlike error sampling, which will over-\nsample from tasks for which Ceil(s) is low. This\nis useful in cases where the variance of Ceil(s) is\nhigh across tasks. On the other hand, momentum\nsampling requires a warm-up of w steps, and might\nunder-sample from tasks that train slowly. In §A.2.,\nwe describe two synthetic experiments where mo-\nmentum sampling clearly outperforms error sam-\npling. However, we do not observe an advantage\nfor momentum sampling in our experiments in §5,\nand leave further investigation of momentum sam-\npling to future work.\n4 Experimental Setup\n4.1 Models\nBaselines Our main baseline is T5 (Raffel et al.,\n2020), a popular pre-trained encoder-decoder\nmodel, which we fine-tune on the downstream\n3We use the difference in performance and not the gain to\naccount for cases of sudden drops in performance.\n6020\ndatasets. We experiment with Base and Large size\nmodels. For each dataset, we compare to the rele-\nvant state-of-the-art model.\nOur pre-trained for reasoning model, PReasM, is\na T5 model with another pre-training step on Dsyn.\nWe experiment with uniform sampling (PReasM-\nUni), error sampling (PReasM-Err), and momen-\ntum sampling (PReasM-Moment) strategies.\n4.2 Datasets\nDROP (Dua et al., 2019) is a RC dataset with\nquestions that require numeric reasoning. As an\nadditional baseline, we also compare to GenBERT\n(Geva et al., 2020), which similar to our approach\ninjects numerical skills by automatically generating\nsynthetic data from a grammar.\nIIRC (Ferguson et al., 2020) is a QA dataset,\nwhere annotators were given a single Wikipedia\nparagraph, and were asked to author questions that\ndepend on that paragraph, but also on other para-\ngraphs linked from the input paragraph. This re-\nsulted in questions that require discrete temporal\n(28%) or numeric ( 11%) reasoning. In addition,\n30% of the questions are unanswerable.\nWe experiment with IIRC in two settings: (a)\nOracle, where the model is given the gold context,\nreducing the problem to RC, where we can apply\nour models. (b) Retrieval, where we use the “im-\nproved pipeline”introduced by Ni et al. (2021) to re-\ntrieve the context, and replace the NumNet+ (Base)\nreader (Ran et al., 2019) used by the authors (which\nhas specialized architecture) with T5/PReasM.\nMMQA (Talmor et al., 2021) is a QA dataset,\nwhere the input is a question and a context that\nconsists of a table, multiple text paragraphs, and\nmultiple images, and the model must reason over a\nsubset of the input modalities to answer the ques-\ntion.4 We chose to use MMQA as it has many\nquestions that involve a conjunction of facts, an op-\neration that is largely missing from other datasets.\nMoreover, a large fractions of the questions can be\nanswered by reasoning over the text and table only.\nSince T5/PReasM cannot handle images or very\nlong contexts, we construct a pipeline that au-\ntomatically directs some MMQA questions to\nT5/PReasM, and uses the original Implicit-Decomp\nbaseline from Talmor et al. (2021) elsewhere. Our\npipeline includes two classifiers, the first deter-\nmines whether a question requires reasoning over\n4We removed tables that appear in the MMQA develop-\nment and test sets from Dsyn.\nan image, and the second classifies whether a text\nparagraph is necessary to answer a question. Again,\nwe experiment with an oracle and retrieval setting,\nsuch that in the oracle setting our model is pre-\nsented with the gold paragraphs. We provide the\nfull description of this pipeline in §A.4.\nEvaluation metrics For all datasets, we use the\nofficial scripts for computing F 1 and EM, which\ncompare the gold and predicted list of answers.\n5 Experimental Results\nWe present results on the downstream RC datasets\n(§5.1) and on the synthetic data (§5.2).\n5.1 Performance on RC Datasets\nTable 4 presents the results of our large models\nover all datasets, also in comparison to current\nstate-of-the-art. We observe that PReasM substan-\ntially improves performance compared to T5 in\nall conditions, improving on the test set by 7.6,\n7.9, 4.1, and 1.2 F1 points on DROP, IIRCoracle,\nIIRC, and MMQA respectively.5 We obtain new\nstate-of-the-art results on MMQA and IIRCoracle.\nOn IIRC, we improve performance when using the\nsame retriever (Pipeline) and replacing the Num-\nNet+ reader with PReasM.6 On DROP, specialized\narchitectures for handling numbers still substan-\ntially outperform both T5 and PReasM.\nTable 5 shows the effect of different sampling\nstrategies. Error sampling and momentum sam-\npling generally outperform uniform sampling, but\nthere is no clear advantage to momentum sampling\ncompared to error sampling. We further analyze\nthe effect of sampling methods in §5.2.\nWe now look at performance on different an-\nswer types across datasets, where PReasM leads\nto dramatic improvements on some types, while\nmaintaining similar performance on other types.\nDROP Table 6 breaks down performance based\non answer types: PReasM outperforms T5 across\n5To verify that the gains of PReasM over T5 are not due to\nknowledge memorized fromDsyn, we trained T5 and PReasM\nto generate the answer given the question only (without con-\ntext). We found that the performance of T5 and PReasM is\nnearly identical in this setup.\n6We report the numbers from Ni et al. (2021) (45.8/44.3\nF1 on the development/test sets). To fairly compare with\nthe NumNet+ reader, we got the retrieved paragraphs for the\nPipeline model through personal communication. However,\nresults on these paragraphs was lower than reported in the\npaper: 45.5/42.8 F1. The reported results of our models\nare with this slightly worse retriever, but still outperform the\nperformance of NumNet+ (Pipeline) from the original paper.\n6021\nDataset Model Development Test\nDROP\nT5-Large 64.6/61.8 65.0/61.8\nPReasM-Large 72.3/69.4 72.6/69.5\nGenBERT 72.3/68.8 72.4/68.6\nQDGAT-ALBERT 90.1/87.0\nIIRC oracle\nT5-Large 69.9/64.9 67.1/62.7\nPReasM-Large 77.4/72.7 75.0/70.6\nNumNet+ 69.2/63.9 70.3/65.6\nIIRC\nT5-Large (Pipeline) 47.4/44.2 41.0/37.8\nPReasM-Large (Pipeline) 50.0/46.5 45.1/42.0\nNumNet+ (Pipeline) 45.8/41.7 44.3/41.3\nNumNet+ (Joint) 50.6/46.9 50.5/47.4\nMMQA\nT5-Large 64.3/57.9 63.4/57.0\nPReasM-Large 65.5/59.0 64.6/58.3\nImplicit-Decomp 55.5/48.8 55.9/49.3\nTable 4: Development and test results. The two values\nin each cell indicate F 1/EM. Improvement over T5 is\nstatistically significant in all cases(p <0.05) according\nto the paired bootstrap test (Efron and Tibshirani, 1993).\nModel DROP IIRC oracle IIRC MMQA\nT5-Large 64.6 ±0.1 69.6±0.3 46.7±0.5 64.2±0.2\nPReasM-Uni-Large 71.4 ±0.1 75.1±0.2 48.9±0.3 64.9±0.4\nPReasM-Moment-Large 71.7 ±0.1 76.8±0.4 49.7±0.1 64.9±0.2\nPReasM-Err-Large 72.2±0.1 76.5±0.5 49.3±0.4 65.3±0.1\nTable 5: F1 on the development set with different sam-\npling strategies. Results show the average and standard\ndeviation over 3 seeds for DROP and MMQA, and 5\nseeds for IIRC and IIRCoracle.\nthe board for all model sizes and answer types.\nPReasM-Base outperforms GenBERT on 3 of 4\nanswer types. The high performance of GenBERT\non Number questions can be explained by: (a)\nGenBERT uses digit tokenization which improves\narithmetic reasoning (Thawani et al., 2021), and (b)\ntraining on multiple numerical reasoning templates.\nIIRC Table 7 breaks down performance based\non answer types. PReasM outperforms T5 in the\noracle setup by roughly 8 points for both Base and\nLarge models, and by 2.6-4 points in the retrieval\nsetup. Improvements are mostly due to cases when\nthe answer is a numerical Value, where PReasM\noutperforms T5 by 39.1 and 40.3 F1 points in Base\nand Large models (oracle setup).\nComparing PReasM-Base to NumNet+, PReasM\noutperforms NumNet+ on None, Span and Bi-\nnary questions, but lags behind on Value questions,\nwhere NumNet+ uses specialized architecture.\nOverall, PReasM-Large improves state-of-the-\nart in the oracle setup by 4.7 F1 points. In the\nretrieval setting, PReasM outperforms NumNet+\n(Pipeline) by 4.2 and 0.8 F1 points on the develop-\nment and test sets, respectively (see Table 4).\nModel Span Spans Date Number Total\nT5-Base 77.5 65.8 57.1 43.7 55.8\nPReasM-Base 81.1 69.4 64.6 61.5 68.1\nT5-Large 86.1 78.4 75.7 52.2 64.6\nPReasM-Large 86.6 78.4 77.7 64.4 72.3\nGenBERT 74.5 24.2 56.4 75.2 72.3\nTable 6: Drop development F1 across answer types.\nModel Oracle None Span Binary Value Total\nT5-Base ✓ 91.4 72.0 76.6 8.7 66.3\nPReasM-Base ✓ 92.5 74.9 71.9 47.8 74.5\nT5-Large ✓ 92.2 77.7 81.3 10.9 69.9\nPReasM-Large ✓ 92.2 78.4 80.5 51.2 77.4\nT5-Base ✗ 57.1 47.6 54.7 6.7 43.5\nPReasM-Base ✗ 53.9 49.1 64.8 24.3 47.5\nT5-Large ✗ 56.2 49.9 77.3 11.5 47.4\nPReasM-Large ✗ 55.9 50.8 69.5 28.6 50.0\nNumNet+ (Pipeline) ✗ 49.6 48.4 52.3 30.0 45.8\nTable 7: IIRC Development F1 across answer types.\nMMQA Table 8 breaks down performance based\non reasoning skills (annotated per example in\nMMQA). PReasM outperforms T5 in both the ora-\ncle and retrieval setting, and for both model sizes.\nThe main cause for improvement are compari-\nson questions, where PReasM outperforms T5 by\n19 and 11.7 F1 points on Base and Large models.\nPReasM outperforms T5 on conjunction questions\nin Base models, and yes/no questions in all settings.\nInterestingly, T5 is equipped with decent composi-\ntion skills, without any specialized pre-training.\nCompared to Implicit-Decomp, although\nImplicit-Decomp outperforms our models on\nquestions that require hopping between two table\ncolumns and aggregations, PReasM outperforms\nImplicit-Decomp in all other cases. When consider-\ning only questions that require reasoning over text\nand tables, PReasM-Large improves F 1 by 16.1\npoints, from 62.3 to 78.4.\n5.2 Performance on Dsyn\nFig. 4 shows statistics on the performance of\nPReasM on different tasks in Dsyn during train-\ning. The average accuracy across all tasks at the\nend of training is high – almost 98.0 F1. PReasM\nreaches high performance on all tasks, where the\nlowest-performing tasks are ‘arithmetic addition’\n(91.1) and ‘date difference’ (94.7). On those tasks,\nthe advantage of error-driven sampling is evident,\nand it outperforms uniform sampling by as much\nas 4 points.\nZooming-in on the learning curve, momentum\n6022\nModel Oracle ColumnHop Text Composition Comparison Conjunction Yes/No Aggregate Total\nT5-Base ✗ 81.7 75.2 67.0 61.8 74.1 76.9 27.3 71.9\nPReasM-Base ✗ 80.8 75.7 66.3 80.8 80.8 83.1 36.4 74.3\nT5-Large ✗ 82.6 79.8 71.8 69.3 83.0 83.1 27.3 76.8\nPReasM-Large ✗ 84.0 79.7 71.9 81.0 82.3 93.8 36.4 78.4\nT5-Base ✓ 85.2 82.1 74.6 63.3 77.4 80.0 27.3 77.9\nPReasM-Base ✓ 86.9 80.0 75.4 84.1 82.6 89.2 36.4 79.9\nT5-Large ✓ 88.2 85.9 79.4 74.1 83.2 83.1 36.4 82.7\nPReasM-Large ✓ 87.8 85.6 79.8 83.6 82.3 90.8 45.5 83.8\nImplicit-Decomp ✓ 96.6 57.1 53.2 78.4 68.1 76.9 59.1 62.3\nTable 8: Development F1 on MMQA with reasoning type breakdown on the development set. The column ‘Total’\nrefers to all questions that do not require reasoning over the image modality.\nFigure 4: Minimum (left) and average (center) task accuracy on 1,000 held-out examples per task from Dsyn, and\nthe entropy of Ptasks (right) as a function of the number of training steps for all sampling strategies (Large models).\nand error sampling learn reasoning skills a lot faster\nthan uniform sampling. Looking at the entropy\nof Ptasks sheds light on the difference between\nerror sampling and momentum sampling. Error\nsampling puts most probability mass on the lowest-\nperforming task (arithmetic addition), and thus its\nentropy over tasks is roughly constant from a cer-\ntain point. Conversely, momentum sampling puts\na lot of probability mass on tasks that are improv-\ning quickly at the beginning, but as improvements\nplateau, it converges towards uniform sampling.\nFig. 5 and Table 11 (in the Appendix) show the\nresults for T5 and PReasM onDsyn. The results for\nT5 were obtained by training in a few-shot manner\non 32 examples for 200 steps, as suggested in Ram\net al. (2021). T5-Large outperforms T5-Base on\nmost tasks, suggesting that larger models are able\nto learn reasoning skills faster. On tasks such as\ndate difference and arithmetic addition, the results\nfor T5-Large are low, at around10 F1. Our PReasM\nmodels significantly outperform T5 on all tasks.\n6 Analysis\nReasoning skills in DROP To check which rea-\nsoning skills PReasM has, we use a proposed split\nof a subset of DROP to reasoning skills (Gupta\net al., 2020a). Table 9 presents the F1 for our best\nPReasM and T5 models, as well as the F 1 from\nQuestion Type NMN T5- PReasM- T5- PReasM-\nBase Base Large Large\nDate-Compare 82.6 86.4 87.5 87.6 89.9\nDate-Difference 75.4 19.6 78.9 45.4 80.4\nNumber-Compare 92.7 91.3 95.2 97.3 98.5\nExtract-Number 86.1 91.8 94.9 92.1 95.1\nCount 55.7 80.1 86.7 86.7 89.2\nExtract-Argument 69.7 87.6 86.2 90.5 92.1\nTable 9: F1 on a previously-proposed split of a subset\nof the development set of DROP to reasoning skills.\nthe neural module network (NMN) used in Gupta\net al. (2020a). NMN was trained only on a subset\nof the original DROP dataset. When comparing to\nT5, PReasM dramatically improves performance\non Date-Difference, and also leads to sizable gains\nin Number-Compare, Extract-Number and Count.\nAccuracy vs. training cost trade-off We\nevaluate PReasM-Base models on DROP and\nIIRCoracle as we vary the number of pre-training\nsteps on Dsyn (Fig. 6). Most of the improvement\nhappens in the first 100K steps, and error-driven\nsampling outperforms uniform sampling through-\nout training. Error sampling outperforms momen-\ntum sampling in the latter part of training. A possi-\nble reason is that the reasoning skills in the down-\nstream tasks are correlated with the harder tasks\nduring pre-training (arithmetic addition and date\ndifference). This provides an advantage for error\n6023\nFigure 5: F1 for each task in Dsyn, for T5 and PReasM on the held-out evaluation set.\nFigure 6: Development F1 on DROP and IIRCoracle as\na function of the number of training steps (Base models).\nThe light lines mark confidence intervals over 5 seeds.\nThe first point shows the performance of T5-Base.\nsampling, since it will focus on these tasks even if\nthe improvement during pre-training is small.\n7 Related Work\nTemplate-based data generation has been previ-\nously used for data augmentation, for example to\ninject numerical skills (Geva et al., 2020), and to\nimprove consistency (Asai and Hajishirzi, 2020),\nand zero-shot accuracy (Zhao et al., 2019). In ad-\ndition, templates were used for dataset construc-\ntion (Talmor and Berant, 2018; Clark et al., 2020;\nThorne et al., 2021), and to analyse model gener-\nalization (Rozen et al., 2019). In this work, we\nautomatically generate examples by instantiating\ntemplates using structured data. Since our method\nrelies solely on tables as input, it is highly scal-\nable, has rich lexical diversity, and can be easily\nextended to new skills and domains.\nRecently, Thorne et al. (2021) introduced the\nWIKINLDB dataset, which includes queries that re-\nquire reasoning over a set of textual facts. Queries\nare instantiated with values from a knowledge\ngraph (KG), and facts are generated by a LM. Un-\nlike this work, WIKINLDB is focused on evaluat-\ning reasoning skills. We, on the other hand, show\nthat generated examples can be used to endow a pre-\ntrained LM with new reasoning skills. Moreover,\ntables are much easier to collect at scale compared\nto KGs, which tend to have limited coverage.\nData augmentation techniques have been exten-\nsively explored in RC, QA, and dialogue (Feng\net al., 2021; Talmor and Berant, 2019; Khashabi\net al., 2020; Alberti et al., 2019; Puri et al., 2020;\nBartolo et al., 2021). Here, we focus on tables as a\nvaluable source for data generation.\nPre-training over tableshas focused in the past on\nreasoning over tables and knowledge-bases (Eisen-\nschlos et al., 2020; Yin et al., 2020; Herzig et al.,\n2020; Müller et al., 2021; Yu et al., 2021; Neer-\naja et al., 2021b). Here, we use pre-training over\ntables to improve reasoning over text. We leave\nevaluation on tasks beyond RC to future work.\nError-driven sampling has been considered in the\npast in the context of active learning (Sharma et al.,\n2018), reinforcement learning (Graves et al., 2017;\nGlover and Hokamp, 2019; Xu et al., 2019), trans-\nfer learning (Zhang et al., 2020; Pilault et al., 2021),\nand distributionally robust optimization (Oren et al.,\n2019; Sagawa et al., 2020), where the goal is to per-\nform well over a family of distributions. Similar to\nGottumukkala et al. (2020), we compute heteroge-\nneous batches based on error rates, and show that\nthis improves efficiency and performance.\n8 Conclusion\nWe propose semi-structured tables as a valuable re-\nsource for generating examples that can endow pre-\ntrained language models with reasoning skills. We\ngenerate 5M examples that correspond to 16 rea-\nsoning skills from Wikipedia tables and add a pre-\ntraining step over this data. To improve data effi-\nciency we use error-driven sampling, which focuses\ntraining on reasoning skills that the model currently\nlacks. We evaluate our model, PReasM, on three\nreasoning-focused RC datasets and show that it\nleads to substantial improvements in all cases.\n6024\nAcknowledgments\nWe thank Elad Segal, Uri Shaham, Tomer Wolf-\nson, and Ankit Gupta for their useful comments\nand James Ferguson, Ansong Ni and Matt Gard-\nner for their help with the IIRC dataset. This re-\nsearch was partially supported by The Yandex Ini-\ntiative for Machine Learning, and the European Re-\nsearch Council (ERC) under the European Union\nHorizons 2020 research and innovation programme\n(grant ERC DELPHI 802800).\nReferences\nChris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin,\nand Michael Collins. 2019. Synthetic QA corpora\ngeneration with roundtrip consistency. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 6168–6173, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nJacob Andreas. 2020. Good-enough compositional data\naugmentation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7556–7566, Online. Association for\nComputational Linguistics.\nAkari Asai and Hannaneh Hajishirzi. 2020. Logic-\nguided data augmentation and regularization for con-\nsistent question answering. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5642–5650, Online. Asso-\nciation for Computational Linguistics.\nMax Bartolo, Tristan Thrush, Robin Jia, Sebastian\nRiedel, Pontus Stenetorp, and Douwe Kiela. 2021.\nImproving question answering model robustness with\nsynthetic adversarial data generation. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 8830–8848, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nGiovanni Campagna, Agata Foryciarz, Mehrad Morad-\nshahi, and Monica Lam. 2020. Zero-shot transfer\nlearning with synthesized data for multi-domain dia-\nlogue state tracking. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 122–132, Online. Association for\nComputational Linguistics.\nKunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xi-\naochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan\nQi, and Wei Chu. 2020a. Question directed graph\nattention network for numerical reasoning over text.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6759–6768, Online. Association for Computa-\ntional Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020b. Tabfact: A large-scale\ndataset for table-based fact verification. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In\nProceedings of the Twenty-Ninth International Joint\nConference on Artificial Intelligence, IJCAI 2020 ,\npages 3882–3890. ijcai.org.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nBradley Efron and Robert J. Tibshirani. 1993. An Intro-\nduction to the Bootstrap. Number 57 in Monographs\non Statistics and Applied Probability. Chapman &\nHall/CRC, Boca Raton, Florida, USA.\nJulian Eisenschlos, Syrine Krichene, and Thomas\nMüller. 2020. Understanding tables with interme-\ndiate pre-training. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n281–296, Online. Association for Computational Lin-\nguistics.\nSteven Y . Feng, Varun Gangal, Jason Wei, Sarath Chan-\ndar, Soroush V osoughi, Teruko Mitamura, and Ed-\nuard Hovy. 2021. A survey of data augmentation\n6025\napproaches for NLP. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 968–988, Online. Association for Computa-\ntional Linguistics.\nJames Ferguson, Matt Gardner, Hannaneh Hajishirzi,\nTushar Khot, and Pradeep Dasigi. 2020. IIRC: A\ndataset of incomplete information reading compre-\nhension questions. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1137–1147, Online. As-\nsociation for Computational Linguistics.\nBesnik Fetahu, Avishek Anand, and Maria Koutraki.\n2019. Tablenet: An approach for determining fine-\ngrained relations for wikipedia tables. In The World\nWide Web Conference, WWW 2019, San Francisco,\nCA, USA, May 13-17, 2019, pages 2736–2742. ACM.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into language\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 946–958, Online. Association for Computa-\ntional Linguistics.\nJohn Glover and Chris Hokamp. 2019. Task selection\npolicies for multitask learning.\nAnanth Gottumukkala, Dheeru Dua, Sameer Singh, and\nMatt Gardner. 2020. Dynamic sampling strategies for\nmulti-task reading comprehension. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 920–924, Online.\nAssociation for Computational Linguistics.\nAlex Graves, Marc G. Bellemare, Jacob Menick, Rémi\nMunos, and Koray Kavukcuoglu. 2017. Automated\ncurriculum learning for neural networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning, ICML 2017, Sydney, NSW, Australia, 6-11\nAugust 2017, volume 70 of Proceedings of Machine\nLearning Research, pages 1311–1320. PMLR.\nNitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and\nMatt Gardner. 2020a. Neural module networks for\nreasoning over text. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek\nSrikumar. 2020b. INFOTABS: Inference on tables\nas semi-structured data. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2309–2324, Online. Association\nfor Computational Linguistics.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nChristopher Hidey, Tuhin Chakrabarty, Tariq Alhindi,\nSiddharth Varia, Kriste Krstovski, Mona Diab, and\nSmaranda Muresan. 2020. DeSePtion: Dual se-\nquence prediction and adversarial examples for im-\nproved fact-checking. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 8593–8606, Online. Association\nfor Computational Linguistics.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1896–1907, Online. Association\nfor Computational Linguistics.\nTushar Khot, Daniel Khashabi, Kyle Richardson, Peter\nClark, and Ashish Sabharwal. 2021. Text modular\nnetworks: Learning to decompose tasks in the lan-\nguage of existing models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1264–1279, Online.\nAssociation for Computational Linguistics.\nJames Kirkpatrick, Razvan Pascanu, Neil C. Rabi-\nnowitz, Joel Veness, Guillaume Desjardins, Andrei A.\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2016. Overcoming catastrophic forgetting in neural\nnetworks. ArXiv preprint, abs/1612.00796.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nThomas Müller, Julian Eisenschlos, and Syrine Krich-\nene. 2021. TAPAS at SemEval-2021 task 9: Reason-\ning over tables with intermediate pre-training. In Pro-\nceedings of the 15th International Workshop on Se-\nmantic Evaluation (SemEval-2021), pages 423–430,\nOnline. Association for Computational Linguistics.\nLinyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria\nLin, Neha Verma, Rui Zhang, Wojciech Kryscinski,\nNick Schoelkopf, Riley Kong, Xiangru Tang, Murori\nMutuma, Ben Rosand, Isabel Trindade, Renusree\nBandaru, Jacob Cunningham, Caiming Xiong, and\nDragomir R. Radev. 2021. Fetaqa: Free-form table\nquestion answering. ArXiv preprint, abs/2104.00369.\n6026\nJ. Neeraja, Vivek Gupta, and Vivek Srikumar. 2021a.\nIncorporating external knowledge to enhance tabular\nreasoning. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2799–2809, Online. Association\nfor Computational Linguistics.\nJ. Neeraja, Vivek Gupta, and Vivek Srikumar. 2021b.\nIncorporating external knowledge to enhance tabular\nreasoning. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2799–2809, Online. Association\nfor Computational Linguistics.\nAnsong Ni, Matt Gardner, and Pradeep Dasigi. 2021.\nMitigating false-negative contexts in multi-document\nquestion answering with retrieval marginalization.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6149–6161, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nYonatan Oren, Shiori Sagawa, Tatsunori B. Hashimoto,\nand Percy Liang. 2019. Distributionally robust lan-\nguage modeling. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 4227–4237, Hong Kong, China. Association\nfor Computational Linguistics.\nJonathan Pilault, Amine Elhattami, and Christopher J.\nPal. 2021. Conditionally adaptive multi-task learn-\ning: Improving transfer learning in NLP using fewer\nparameters & less data. In 9th International Confer-\nence on Learning Representations, ICLR 2021, Vir-\ntual Event, Austria, May 3-7, 2021. OpenReview.net.\nRaul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa\nPatwary, and Bryan Catanzaro. 2020. Training\nquestion answering models from synthetic data. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5811–5826, Online. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nOri Ram, Yuval Kirstain, Jonathan Berant, Amir Glober-\nson, and Omer Levy. 2021. Few-shot question an-\nswering by pretraining span selection. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3066–3079, Online.\nAssociation for Computational Linguistics.\nQiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan\nLiu. 2019. NumNet: Machine reading comprehen-\nsion with numerical reasoning. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2474–2484, Hong Kong,\nChina. Association for Computational Linguistics.\nOhad Rozen, Vered Shwartz, Roee Aharoni, and Ido\nDagan. 2019. Diversify your datasets: Analyzing\ngeneralization via controlled variance in adversarial\ndatasets. In Proceedings of the 23rd Conference on\nComputational Natural Language Learning (CoNLL),\npages 196–205, Hong Kong, China. Association for\nComputational Linguistics.\nShiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto,\nand Percy Liang. 2020. Distributionally robust neu-\nral networks. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nSahil Sharma, Ashutosh Kumar Jha, Parikshit Hegde,\nand Balaraman Ravindran. 2018. Learning to multi-\ntask by active sampling. In 6th International Con-\nference on Learning Representations, ICLR 2018,\nVancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings. OpenReview.net.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 641–651, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nAlon Talmor and Jonathan Berant. 2019. MultiQA: An\nempirical investigation of generalization and trans-\nfer in reading comprehension. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4911–4921, Florence, Italy.\nAssociation for Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\nnaneh Hajishirzi, and Jonathan Berant. 2021. Mul-\ntimodalqa: complex question answering over text,\n6027\ntables and images. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021. Representing numbers in NLP: a\nsurvey and a vision. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–656, Online. As-\nsociation for Computational Linguistics.\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fab-\nrizio Silvestri, Sebastian Riedel, and Alon Halevy.\n2021. Database reasoning over text. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3091–3104, Online.\nAssociation for Computational Linguistics.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know num-\nbers? probing numeracy in embeddings. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5307–5315, Hong\nKong, China. Association for Computational Linguis-\ntics.\nXinyi Wang, Yulia Tsvetkov, and Graham Neubig. 2020.\nBalancing training for multilingual neural machine\ntranslation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 8526–8537, Online. Association for Computa-\ntional Linguistics.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019. Investigating\nBERT’s knowledge of language: Five analysis meth-\nods with NPIs. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2877–2887, Hong Kong, China. Association\nfor Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020. Hug-\ngingface’s transformers: State-of-the-art natural lan-\nguage processing.\nYichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu,\nand Jianfeng Gao. 2019. Multi-task learning with\nsample re-weighting for machine reading compre-\nhension. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2644–2655, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. TaBERT: Pretraining for joint\nunderstanding of textual and tabular data. InProceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nDani Yogatama, Cyprien de Masson d’Autume, Jerome\nConnor, Tomas Kocisky, Mike Chrzanowski, Ling-\npeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu,\nChris Dyer, and Phil Blunsom. 2019. Learning and\nevaluating general linguistic intelligence.\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang,\nYi Chern Tan, Xinyi Yang, Dragomir R. Radev,\nRichard Socher, and Caiming Xiong. 2021. Grappa:\nGrammar-augmented pre-training for table semantic\nparsing. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\nSheng Zhang, Xin Zhang, Weiming Zhang, and An-\nders Søgaard. 2020. Worst-case-aware curriculum\nlearning for zero and few shot transfer.\nZijian Zhao, Su Zhu, and Kai Yu. 2019. Data augmen-\ntation with atomic templates for spoken language\nunderstanding. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 3637–3643, Hong Kong, China. Association\nfor Computational Linguistics.\nA Supplemental Material\nA.1 Data Generation\nIn this section, we provide details about how we\nclassify table columns.\nClassifying table columns When annotating the\nsemantic types of columns, a column will be of type\nNUMBER or DATE if all values in the column can\nbe parsed with standard tools for parsing numbers\nand dates,7 accordingly. Otherwise, we annotate\nthe column as type STRING.\nInformation in tables is usually aggregated such\nthat certain columns serve as the semantic index of\nthe table. For example, the table in Fig. 1 provides\ninformation about each round in a tournament. In\norder for our examples to be semantically mean-\ningful, we generate questions about columns that\nserve as the semantic index of their table.\n7https://pypi.org/project/\npython-dateutil/\n6028\nSince the semantic index is not provided, we\nuse a linear decision rule to find such columns.\nThe features to our classifier include the column’s\ndistance from the leftmost column, the percentage\nof unique cells in the column, the percentage of\ncells whose values are links to Wikipedia articles,\nthe percentage of cells with short text (at most 2\ncharacters), the percentage of cells with numbers,\nand the column’s header. We allow more than one\nsemantic index per table, such that both the Round\nand Opponent columns can serve as a semantic\nindex in the table in Fig. 1.\nA.2 Advantages of Momentum Sampling\nTo highlight the theoretical benefits of momen-\ntum sampling, we construct synthetic experiments\nwhere there is high variance in the ceiling accuracy\nbetween different tasks. As we show in §5.2, our\nmodels are able to achieve near perfect accuracy\non our tasks when provided with enough training\nexamples. Hence, we create settings where the\nceiling accuracy for a task is lower than 1.0, either\nby adding noise or by down-sampling the num-\nber of training examples. More specifically, we\ntrain on two tasks: an arithmetic addition task that\ntrains slowly and has a high ceiling accuracy, and a\nsecond task that trains quickly, and evaluate the per-\nformance on a held-out set of arithmetic addition\nexamples.\nFirst, we train on arithmetic addition and 2-hop\ncomposition, which is faster to train. We conduct\ntwo experiments, in which we add noise to the 2-\nhop composition task by randomly sampling the\nlabel from the vocabulary in order to force the ceil-\ning accuracy to be lower than 1.0. To check the\nperformance of sampling strategies in varying lev-\nels of noise, we conduct two experiments where\nwe add noise to 30% or 100% of the examples (in\nthe latter case the label of 2-hop composition is\nrandom). We expect that this will lead to slower\nlearning of arithmetic addition for error sampling,\nsince more probability mass will be allocated to\nthe noisy task (since its ceiling accuracy is low),\ndespite the fact that it is easier.\nNext, we train on arithmetic addition and date\ndifference, both of which train slowly. To force\nthe ceiling accuracy of the date difference task to\nbe lower than 1.0, our training set contains only\n1,000 examples. This emulates settings where the\ndata is not generated automatically and the cost of\ngenerating examples is higher. Again, we expect\nFigure 7: Motivation for momentum sampling.\nAD=Arithmetic Addition, 2hC=2-hop Composition,\nDD=Date Difference. When one task has high ceil-\ning accuracy and trains slowly, and the other task has\na lower ceiling accuracy and trains fast, momentum\nsampling outperforms error sampling.\nerror sampling to over-sample from the date differ-\nence task even when this would not lead to gains\nin performance, due to the small training set.\nFig. 7 illustrates the advantage of momentum\nsampling in these settings. Without noise (top left),\nboth momentum sampling and error sampling learn\nfaster than uniform sampling. Momentum sam-\npling learns more slowly than error sampling, due\nto the warm-start period in the first w evaluation\ncheckpoints. As we add noise to 30% of the exam-\nples (top right), error sampling focuses on the noisy\ntask once accuracy approaches a certain level (0.7\nF1). When we add noise to all of the 2-hop com-\nposition examples (bottom left), uniform sampling\noutperforms error sampling, while momentum sam-\npling still performs well. This phenomenon repeats\nwhen we switch the 2-hop composition task with\nthe date difference task and down-sample the num-\nber of training examples (bottom right).\nA.3 Implementation Details\nThe following section includes implementation\ndetails for our experiments, including: hyper-\nparameters for the momentum sampling algorithm,\nthe original pre-training task, and technical details.\n6029\nExperiment Size LR Batch Size GAS Epochs\nPReasM Base 1e-4 64 1 50\nPReasM Large 1e-4 18 4 36\nDROP Base 1e-4 20 1 20\nIIRC Base 1e-4 20 1 60\nIIRC oracle Base 1e-4 20 1 60\nMMQA Base 1e-4 6 3 20\nDROP Large 5e-5 16 2 20\nIIRC Large 5e-5 16 2 60\nIIRC oracle Large 5e-5 16 2 60\nMMQA Large 1e-4 2 16 10\nTable 10: Hyper-parameters used in all experiments, LR\nand GAS refer to learning-rate and gradient accumula-\ntion steps. In our PReasM experiments, epochs refer to\nthe number of steps between evaluations, which is set\nto 5, 000 and 2, 500 for our base and large experiments,\nwhich leads to 250, 000 and 90, 000 optimization steps,\nrespectively.\nMomentum sampling For momentum sampling\nwe use a window size ofw = 4, a smoothing factor\nof k = 2, and sample at least ϵ = 0.002 examples\nfrom every task in Dsyn.\nOriginal pre-training task In order to avoid\ncatastrophic forgetting (Kirkpatrick et al., 2016),\nwe continue training with the span-corruption ob-\njective introduced in (Raffel et al., 2020), over se-\nquences of length 256 from the English Wikipedia.\nTechnical details We train all our experiments\non one RTX8000 (48GB) or RTX3090 (24GB)\nGPUs. Our PReasM-Base and PReasM-Large\nmodels training time was 5-6 and 8-9 days on\none RTX8000 GPU, respectively. We use the\nT5 model from https://huggingface.\nco/transformers/model_doc/t5.html\n(Wolf et al., 2020). Table 10 contains the\nhyper-parameters used in our experiments.\nA.4 MMQA Pipeline\nThe first classifier in our pipeline is a T5-large\nmodel fine-tuned on the MMQA training set to\ndetermine if a question is likely to require an im-\nage or not. When the classifier determines a ques-\ntion requires an image, the example is directed to\nImplicit-Decomp. The accuracy of this classifier on\nthe MMQA development set is 99.2%.\nThe second classifier in the pipeline is a T5-3B\nmodel, fine-tuned on the MMQA training set to\ndetermine given a question and one of the textual\nparagraphs if that paragraph is required for answer-\ning the question. Then, for every question that does\nnot require an image, we classify each of the tex-\ntual paragraphs and only use the ones classified as\nrelevant. This process identifies all gold paragraphs\nin 95.8% of the examples.\nLast, we convert the table into text by lineariz-\ning the table as described in Talmor et al. (2021).\nThe model is presented with multiple paragraphs\nand the linearized table, and can answer questions\nthat require any reasoning across them. Since the\ncontext is long, we present the model with contexts\nof size 1,536 word-pieces (without any change to\nthe original T5 model).\n6030\nT5- PReasM- PReasM- PReasM- T5- PReasM- PReasM- PReasM-\nBase Uni- Moment- Err- Large Uni- Moment- Err-\nBase Base Base Large Large Large\n2-hop Composition 72.8 98.6 98.4 98.6 82.6 98.5 98.5 98.5\n3-hop Composition 40.7 97.5 97.9 97.3 50.8 97.6 97.5 97.6\nConjunction 59.6 96.1 95.9 95.9 63.2 96.5 96 96.7\nQuantifiers Only 65.8 99.8 99.9 99.5 69.6 99.7 100 99.7\nQuantifiers Most 74.7 99.6 99.2 99 82.5 99.6 99.7 99.4\nQuantifiers Every 67 100 100 100 86.6 100 100 100\nNumerical Comparison 53.6 96.3 96.6 96.6 57.1 96.6 96.5 96.5\nTemporal Comparison 71.1 99.3 99.2 99.2 72.3 99.3 99.2 99.4\nNumerical Comparison Yes/No 57 99.9 99.9 99.7 62.5 99.9 99.9 99.9\nTemporal Comparison Yes/No 52.2 100 99.9 100 59.4 99.7 100 99.9\nNumerical Superlatives 37.3 96.3 96.2 95.9 67.8 96 96.6 96.4\nTemporal Superlatives 33.6 96.6 97.5 97 59.6 97.5 97.8 97.5\nArithmetic Superlatives 42.4 98.2 98.4 97.9 56.6 98.4 98.9 97.6\nArithmetic Addition 7.1 90.4 90.9 91.8 11.8 89.7 91.3 91.1\nCounting 46.5 96.8 97.7 98.6 56.1 95.1 97.6 97.7\nDate Difference 11.1 92.1 94.3 95.0 11.2 90.7 93.7 94.7\nTable 11: F1 for every task in Dsyn for T5 and PReasM on the held-out evaluation set.\n6031",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.826138973236084
    },
    {
      "name": "Paragraph",
      "score": 0.804262101650238
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7061427235603333
    },
    {
      "name": "Language model",
      "score": 0.636923611164093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6014120578765869
    },
    {
      "name": "Natural language processing",
      "score": 0.5394137501716614
    },
    {
      "name": "Qualitative reasoning",
      "score": 0.4787925183773041
    },
    {
      "name": "Reading comprehension",
      "score": 0.46056538820266724
    },
    {
      "name": "Machine learning",
      "score": 0.4529988169670105
    },
    {
      "name": "Encoder",
      "score": 0.43737685680389404
    },
    {
      "name": "Reading (process)",
      "score": 0.2950771450996399
    },
    {
      "name": "Linguistics",
      "score": 0.0803740918636322
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    }
  ]
}