{
  "title": "Grammar Induction with Neural Language Models: An Unusual Replication",
  "url": "https://openalex.org/W2889260178",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2526928059",
      "name": "Phu Mon Htut",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2115080942",
      "name": "Kyunghyun Cho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2145255766",
      "name": "Samuel Bowman",
      "affiliations": [
        "New York University",
        "Canadian Institute for Advanced Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2144916786",
    "https://openalex.org/W2104917081",
    "https://openalex.org/W1423339008",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W1495446613",
    "https://openalex.org/W2104518905",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W145849181",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2129882630",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2157762871",
    "https://openalex.org/W2963580443",
    "https://openalex.org/W2949847915",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2619818172"
  ],
  "abstract": "A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4998–5003\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n4998\nGrammar Induction with Neural Language Models:\nAn Unusual Replication\nPhu Mon Htut1\nAdeptMind Scholar\npmh330@nyu.edu\nKyunghyun Cho1,2\nCIFAR Global Scholar\nkyunghyun.cho@nyu.edu\nSamuel R. Bowman1,2,3\nbowman@nyu.edu\n1Center for Data Science\nNew York University\n60 Fifth Avenue\nNew York, NY 10011\n2Dept. of Computer Science\nNew York University\n60 Fifth Avenue\nNew York, NY 10011\n3Dept. of Linguistics\nNew York University\n10 Washington Place\nNew York, NY 10003\nAbstract\nA substantial thread of recent work onlatent\ntree learninghas attempted to develop neural\nnetwork models with parse-valued latent vari-\nables and train them on non-parsing tasks, in\nthe hope of having them discover interpretable\ntree structure. In a recent paper,Shen et al.\n(2018) introduce such a model and report near-\nstate-of-the-art results on the target task of lan-\nguage modeling, and the ﬁrst strong latent tree\nlearning result on constituency parsing. In an\nattempt to reproduce these results, we discover\nissues that make the original results hard to\ntrust, including tuning and even training on\nwhat is effectively the test set. Here, we at-\ntempt to reproduce these results in a fair exper-\niment and to extend them to two new datasets.\nWe ﬁnd that the results of this work are robust:\nAll variants of the model under study outper-\nform all latent tree learning baselines, and per-\nform competitively with symbolic grammar\ninduction systems. We ﬁnd that this model\nrepresents the ﬁrst empirical success for la-\ntent tree learning, and that neural network lan-\nguage modeling warrants further study as a\nsetting for grammar induction.\n1 Introduction and Background\nWork on grammar induction attempts to ﬁnd\nmethods for syntactic parsing that do not re-\nquire expensive and difﬁcult-to-design expert-\nlabeled treebanks for training (Charniak and Car-\nroll, 1992; Klein and Manning, 2002; Smith and\nEisner, 2005). Recent work onlatent tree learning\noffers a new family of approaches to the problem\n(Yogatama et al., 2017; Maillard et al., 2017; Choi\net al., 2018). Latent tree learning models attempt\nto induce syntactic structure using the supervision\nfrom a downstream NLP task such as textual en-\ntailment. Though these models tend to show good\ntask performance, they are often not evaluated us-\ning standard parsing metrics, andWilliams et al.\n(2018a) report that the parses they produce tend to\nbe no better than random trees in a standard evalu-\nation on the full Wall Street Journal section of the\nPenn Treebank (WSJ;Marcus et al., 1993).\nThis paper addresses the Parsing-Reading-\nPredict Network (PRPN;Shen et al., 2018), which\nwas recently published at ICLR, and which reports\nnear-state-of-the-art results on language modeling\nand strong results on grammar induction, a ﬁrst\nfor latent tree models (though they do not use that\nterm). PRPN is built around a substantially novel\narchitecture, and uses convolutional networks with\na form of structured attention (Kim et al., 2017)\nrather than recursive neural networks (Goller and\nKuchler, 1996; Socher et al., 2011) to evaluate and\nlearn trees while performing straightforward back-\npropagation training on a language modeling ob-\njective. In this work, we aim to understand what\nthe PRPN model learns that allows it to succeed,\nand to identify the conditions under which this\nsuccess is possible.\nTheir experiments on language modeling and\nparsing are carried out using different conﬁgura-\ntions of the PRPN model, which were claimed to\nbe optimized for the corresponding tasks. PRPN-\nLM is tuned for language modeling performance,\nand PRPN-UP for (unsupervised) parsing perfor-\nmance. In the parsing experiments, we also ob-\nserve that the WSJ data is not split, such that\nthe test data is used without parse information\nfor training. This approach follows the previ-\nous works on grammar induction using non-neural\nmodels where the entire dataset is used for train-\ning (Klein and Manning, 2002). However, this\nimplies that the parsing results of PRPN-UP may\nnot be generalizable in the way usually expected\nof machine learning evaluation results. Addition-\nally, it is not obvious that the model should be able\nto learn to parse reliably: (1) Since the parser is\ntrained as part of a language model, it makes pars-\n4999\nThere’snothingworthseeinginthetouristofﬁces. There’snothingworthseeinginthetouristofﬁces.\nTheentireMinoancivilizationwasdestroyedbyavolcaniceruption. TheentireMinoancivilizationwasdestroyedbyavolcaniceruption.\nFigure 1: Left Parses from PRPN-LM trained on AllNLI.Right Parses from PRPN-UP trained on AllNLI\n(stopping criterion: parsing). We can observe that both sets of parses tend to have roughly reasonable\nhigh-level structure and tend to identify noun phrases correctly.\ning decisions greedily and withno access to any\nwords to the right of the point where each parsing\ndecision must be made (Collins and Roark, 2004);\n(2) As RNN language models are known to be in-\nsufﬁcient for capturing syntax-sensitive dependen-\ncies (Linzen et al., 2016), language modeling as\nthe downstream task may not be well-suited to la-\ntent tree learning.\nIn this replication we train PRPN on two cor-\npora: The full WSJ, a staple in work on gram-\nmar induction, and AllNLI, the concatenation of\nthe Stanford Natural Language Inference Corpus\n(SNLI; Bowman et al., 2015) and the Multi-Genre\nNLI Corpus (MultiNLI; Williams et al., 2018b),\nwhich is used in other latent tree learning work for\nits non-syntactic classiﬁcation labels for the task\nof textual entailment, and which we include for\ncomparison. We then evaluate the constituency\ntrees produced by these models on the WSJ test\nset, full WSJ10,1 and the MultiNLI development\nset.\nOur results indicate that PRPN-LM achieves\nbetter parsing performance than PRPN-UP on\nboth WSJ and WSJ10 even though PRPN-UP was\ntuned—at least to some extent—for parsing. Sur-\nprisingly, a PRPN-LM model trained on the large\nout-of-domain AllNLI dataset achieves the best\nparsing performance on WSJ despite not being\ntuned for parsing. We also notice that vocabulary\nsize affects the language modeling signiﬁcantly—\nthe perplexity gets higher as the vocabulary size\nincreases.\nOverall, despite the relatively uninformative ex-\nperimental design used inShen et al.(2018), we\nﬁnd that PRPN is an effective model. It outper-\nforms all latent tree learning baselines by large\n1A standard processed subset of WSJ used in grammar\ninduction in which the sentences contain no punctuation and\nno more than 10 words.\nmargins on both WSJ and MultiNLI, and performs\ncompetitively with symbolic grammar induction\nsystems on WSJ10, suggesting that PRPN in par-\nticular and language modeling in general are a vi-\nable setting for latent tree learning.\n2 Methods\nPRPN consists of three components: (i) aparsing\nnetwork that uses a two-layer convolution kernel\nto calculate the syntactic distance between suc-\ncessive pairs of words, which can form an indi-\nrect representation of the constituency structure of\nthe sentence, (ii) a recurrentreading networkthat\nsummarizes the current memory state based on\nall previous memory states and the implicit con-\nstituent structure, and (iii) apredict network that\nuses the memory state to predict the next token.\nWe refer readers to the appendix and the original\nwork for details.\nWe do not re-implement or re-tune PRPN, but\nrather attempt to replicate and understand the re-\nsults of the work using the author’s publicly avail-\nable code.2 The experiments on language model-\ning and parsing are carried out using different con-\nﬁgurations of the model, with substantially differ-\nent hyperparameter values including the size of the\nword embeddings, the maximum sentence length,\nthe vocabulary size, and the sizes of hidden layers.\nPRPN-LM is larger than PRPN-UP, with embed-\nding layer that is 4 times larger and the number of\nunits per layer that is 3 times larger. We use both\nversions of the model in all our experiments.\nWe use the 49k-sentence WSJ corpus in two set-\ntings. To replicate the original results, we re-run\nan experiment with no train/test split, and for a\nclearer picture of the model’s performance, we run\nit again with the train (Section 0-21 of WSJ), val-\nidation (Section 22 of WSJ), and test (Section 23\n2https://github.com/yikangshen/PRPN\n5000\nTraining\nData\nStopping\nCriterion\nVocab\nSize\nParsing F1 Depth\nWSJ\nAccuracy on WSJ by TagModel WSJ10 WSJ ADJP NP PP INTJµ (\u0000) max µ (\u0000) max\nPRPN-UP AllNLI Train UP 76k 67.5 (0.6) 68.6 36.9 (0.6) 38.0 5.8 29.3 62.0 31.6 0.0\nPRPN-UP AllNLI Train LM 76k 66.3 (0.8) 68.5 38.3 (0.5) 39.8 5.8 28.7 65.5 32.7 0.0\nPRPN-LM AllNLI Train LM 76k 52.4 (4.9) 58.1 35.0 (5.4) 42.8 6.1 37.8 59.7 61.5 100.0\nPRPN-UP WSJ Full UP 15.8k 64.7 (3.2) 70.9 26.4 (1.7) 31.1 5.8 22.5 47.2 17.9 0.0\nPRPN-UP WSJ Full LM 15.8k 64.3 (3.3) 70.8 26.3 (1.8) 30.8 5.8 22.7 46.6 17.8 0.0\nPRPN-UP WSJ Train UP 15.8k 63.5 (3.5) 70.7 26.2 (2.3) 33.0 5.8 24.8 55.2 18.0 0.0\nPRPN-UP WSJ Train LM 15.8k 62.2 (3.9) 70.3 26.0 (2.3) 32.8 5.8 24.8 54.4 17.8 0.0\nPRPN-LM WSJ Train LM 10k 70.5 (0.4) 71.3 37.4 (0.3) 38.1 5.9 26.2 63.9 24.4 0.0\nPRPN-LM WSJ Train UP 10k 66.1 (0.5) 67.2 33.4 (0.8) 35.6 5.9 33.0 57.1 18.3 0.0\n300D ST-Gumbel AllNLI Train NLI – – – 19.0 (1.0) 20.1 – 15.6 18.8 9.9 59.4\nw/o Leaf GRU AllNLI Train NLI – – – 22.8 (1.6) 25.0 – 18.9 24.1 14.2 51.8\n300D RL-SPINN AllNLI Train NLI – – – 13.2 (0.0) 13.2 – 1.7 10.8 4.6 50.6\nw/o Leaf GRU AllNLI Train NLI – – – 13.1 (0.1) 13.2 – 1.6 10.9 4.6 50.0\nCCM WSJ10 Full – – – 71.9 – – – – – – –\nDMV+CCM WSJ10 Full – – – 77.6 – – – – – – –\nUML-DOP WSJ10 Full – – – 82.9 –– – – – – –\nRandom Trees – – – – 34.7 21.3 (0.0) 21.4 5.3 17.4 22.3 16.0 40.4\nBalanced Trees – – – – – 21.3 (0.0) 21.3 4.6 22.1 20.2 9.3 55.9\nLeft Branching – – – 28.7 28.7 13.1 (0.0) 13.1 12.4 – – – –\nRight Branching – – – 61.7 61.7 16.5 (0.0) 16.5 12.4 – – – –\nTable 1: Unlabeled parsing F1 results evaluated on full WSJ10 and WSJ test set broken down by train-\ning data and by early stopping criterion. TheAccuracy columns represent the fraction of ground truth\nconstituents of a given type that correspond to constituents in the model parses. Italics mark results that\nare worse than the random baseline. Underlining marks the best results from our runs. Results with\nRL-SPINN and ST-Gumbel are fromWilliams et al.(2018a), and are evaluated on the full WSJ. We\nrun the model with 5 different random seeds to calculate the average F1. We use the model with the\nbest F1 score to report ADJP, NP, PP, and INTJ. WSJ10 baselines are fromKlein and Manning(2002,\nCCM), Klein and Manning(2005, DMV+CCM), andBod (2006, UML-DOP). As the WSJ10 baselines\nare trained using additional information such as POS tags and dependency parser, they are not strictly\ncomparable with the latent tree learning results.\nof WSJ) splits. To compare PRPN to the models\nstudied inWilliams et al.(2018a), we also retrain\nit on AllNLI. As the MultiNLI test set is not pub-\nlicly available, we followWilliams et al.(2018a)\nand use the development set for testing. The pars-\ning evaluation code in the original codebase does\nnot support PRPN-LM, and we modify it in our\nexperiments only to add this support.\nFor early stopping, we remove 10k random sen-\ntences from the MultiNLI training set and combine\nthem with the SNLI development set to create a\nvalidation set. Our AllNLI training set contains\n280.5K unique sentences (1.8M sentences in total\nincluding duplicate premise sentences), and cov-\ners six distinct genres of spoken and written En-\nglish. We do not remove the duplicate sentences.\nWe train the model for 100 epochs for WSJ and 15\nepochs for AllNLI. We run the model ﬁve times\nwith random initializations and average the results\nfrom the ﬁve runs. The generated parses from the\ntrained models with the best F1 scores and the\npre-trained model that provides the highest F1 are\navailable online.3\n3 Experimental Results\nTable 2 shows our results for language modeling.\nPRPN-UP, conﬁgured as-is with parsing criterion\nand language modeling criterion, performs dra-\nmatically worse than the standard PRPN-LM (a\nvs. d and e). However, this is not a fair comparison\nas the larger vocabulary gives PRPN-UP a harder\ntask to solve. Adjusting the vocabulary of PRPN-\nUP down to 10k to make a fairer comparison pos-\nsible, the PPL of PRPN-UP improves signiﬁcantly\n(c vs. d), but not enough to match PRPN-LM (a\nvs. c). We also observe that early stopping on\n3https://github.com/nyu-mll/\nPRPN-Analysis\n5001\nTraining Stopping Vocab PPL\nModel Data Criterion Size Median\n(a) PRPN-LM WSJ Train LM 10k 61.4\n(b) PRPN-LM WSJ Train UP 10k 81.6\n(c) PRPN-UP WSJ Train LM 10k 92.8\n(d) PRPN-UP WSJ Train LM 15.8k 112.1\n(e) PRPN-UP WSJ Train UP 15.8k 112.8\n(f) PRPN-UP AllNLI Train LM 76k 797.5\n(g) PRPN-UP AllNLI Train UP 76k 848.9\nTable 2: Language modeling performance (per-\nplexity) on the WSJ test set, broken down by train-\ning data used and by whether early stopping is\ndone using the parsing objective (UP) or the lan-\nguage modeling objective (LM).\nparsing leads to incomplete training and a substan-\ntial decrease in perplexity (a vs. b and d vs. e).\nThe models stop training at around the 13th epoch\nwhen we early-stop on parsing objective, while\nthey stop training around the 65th epoch when we\nearly-stop on language modeling objective. Both\nPRPN models trained on AllNLI do even worse\n(f and g), though the mismatch in vocabulary and\ndomain may explain this effect. In addition, since\nit takes much longer to train PRPN on the larger\nAllNLI dataset, we train PRPN on AllNLI for only\n15 epochs while we train the PRPN on WSJ for\n100 epochs. Although the parsing objective con-\nverges within 15 epochs, we notice that language\nmodeling perplexity is still improving. We expect\nthat the perplexity of the PRPN models trained on\nAllNLI could be lower if we increase the number\nof training epochs.\nTurning toward parsing performance, Table1\nshows results with all the models under study, plus\nseveral baselines, on WSJ test set and full WSJ10.\nOn full WSJ10, we reproduce the main parsing\nresult of Shen et al.(2018) with their UP model\ntrained on WSJ without a data split. We also ﬁnd\nthe choice of parse quality as an early stopping\ncriterion does not have a substantial effect and\nthat training on the (unlabeled) test set does not\ngive a signiﬁcant improvement in performance.\nIn addition and unexpectedly, we observe that\nPRPN-LM models achievehigher parsing perfor-\nmance than PRPN-UP. This shows that any tuning\ndone to separate PRPN-UP from PRPN-LM was\nnot necessary, and more importantly, that the re-\nsults described in the paper can be largely repro-\nduced by a uniﬁed model in a fair setting. More-\nover, the PRPN models trained on WSJ achieves\nStopping F1 wrt.\nModel Criterion LB RB SP Depth\n300D SPINN NLI 19.3 36.9 70.2 6.2\nw/o Leaf GRU NLI 21.2 39.0 63.5 6.4\n300D SPINN-NC NLI 19.2 36.2 70.5 6.1\nw/o Leaf GRU NLI 20.6 38.9 64.1 6.3\n300D ST-Gumbel NLI 32.6 37.5 23.7 4.1\nw/o Leaf GRU NLI 30.8 35.6 27.5 4.6\n300D RL-SPINN NLI 95.0 13.5 18.8 8.6\nw/o Leaf GRU NLI 99.1 10.7 18.1 8.6\nPRPN-LM LM 25.6 26.9 45.7 4.9\nPRPN-UP UP 19.4 41.0 46.3 4.9\nPRPN-UP LM 19.9 37.4 48.6 4.9\nRandom Trees – 27.9 28.0 27.0 4.4\nBalanced Trees – 21.7 36.8 21.3 3.9\nTable 3: Unlabeled parsing F1 on the MultiNLI\ndevelopment set for models trained on AllNLI.F1\nwrt. shows F1 with respect to strictly right- and\nleft-branching (LB/RB) trees and with respect to\nthe Stanford Parser (SP) trees supplied with the\ncorpus; The evaluations of SPINN, RL-SPINN,\nand ST-Gumbel are fromWilliams et al.(2018a).\nSPINN is a supervised parsing model, and the oth-\ners are latent tree models. Median F1 of each\nmodel trained with 5 different random seeds is re-\nported.\ncomparable results with CCM (Klein and Man-\nning, 2002). The PRPN models are outperformed\nby DMV+CCM(Klein and Manning, 2005), and\nUML-DOP(Bod, 2006). However, these models\nuse additional information such as POS and de-\npendency parser so they are not strictly compara-\nble with the PRPN models.\nTurning to the WSJ test set, the results look\nsomewhat different: Although the differences in\nWSJ10 performance across models are small, the\nsame is not true for the WSJ in terms of average\nF1. PRPN-LM outperforms all the other mod-\nels on WSJ test set, even the potentially-overﬁt\nPRPN-UP model. Moreover, the PRPN models\ntrained on the larger, out-of-domain AllNLI per-\nform better than those trained on WSJ. Surpris-\ningly, PRPN-LM tained on out-of-domain AllNLI\nachieves the best F1 score on WSJ test set among\nall the models we experimented, even though its\nperformance on WSJ10 is the lowest of all. This\nmean that PRPN-LM trained on AllNLI is strik-\ningly good at parsing longer sentences though its\nperformance on shorter sentences is worse than\nother models. Under all the conﬁgurations we\ntested, the PRPN model yields much better per-\n5002\nformance than the baselines fromYogatama et al.\n(2017, called RL-SPINN) andChoi et al.(2018,\ncalled ST-Gumbel), despite the fact that the model\nwas tuned exclusively for WSJ10 parsing. This\nsuggests that PRPN is consistently effective at la-\ntent tree learning.\nWe also show detailed results for several spe-\nciﬁc constituent types, followingWilliams et al.\n(2018a). We observe that the accuracy for NP\n(noun phrases) on the WSJ test set is above 46%\n(Table 1) for all PRPN models, much higher than\nany of the baseline models. These runs also per-\nform substantially better than the random baseline\nin the two other categoriesWilliams et al.(2018a)\nreport: ADJP (adjective phrases) and PP (preposi-\ntional phrases). However, as WSJ test set contains\nonly one INTJ (interjection phrases), the results on\nINTJ are either 0.0% or 100%.\nIn addition, Table3 shows that the PRPN-UP\nmodels achieve the median parsing F1 scores of\n46.3 and 48.6 respectively on the MultiNLI dev\nset while PRPN-LM performs the median F1 of\n45.7; setting the state of the art in parsing perfor-\nmance on this dataset among latent tree models by\na large margin. We conclude that PRPN does ac-\nquire some substantial knowledge of syntax, and\nthat this knowledge agrees with Penn Treebank\n(PTB) grammar signiﬁcantly better than chance.\nQualitatively, the parses produced by most of\nthe best performing PRPN models are relatively\nbalanced (F1 score of 36.5 w.r.t balanced trees)\nand tend toward right branching (F1 score of 42.0\nwith respect to balanced trees). They are also shal-\nlower than average ground truth PTB parsed trees.\nThese models can parse short sentences relatively\nwell, as shown by their high WSJ10 performance.\nFor a large proportion of long sentences, most\nof the best performing models can produce rea-\nsonable constituents (Table1). The best perform-\ning model, PRPN-LM trained on AllNLI, achieves\nthe best accuracy at identifying ADJP (adjective\nphrases), PP (prepositional phrases), and INTJ (in-\nterjection phrases) constituents, and a high accu-\nracy on NP (noun phrases). In a more informal\ninspection, we also observe that our best PRPN-\nLM and PRPN-UP runs are fairly good at pairing\ndeterminers with NPs as we can observe in Fig-\nure 1). Although lower level tree constituents ap-\npear random in many cases for both PRPN-LM\nand PRPN-UP, the intermediate and higher-level\nconstituents are generally reasonable. For exam-\nple, in Figure1, although the parse for lower level\nconstituents likeThe entire Minoanseem random,\nthe higher-level constituents, such asThe entire\nMinoan civilization and nothing worth seeing in\nthe tourist ofﬁces, are reasonable.\n4 Conclusion\nIn our attempt to replicate the grammar induction\nresults reported inShen et al.(2018), we ﬁnd sev-\neral experimental design problems that make the\nresults difﬁcult to interpret. However, in exper-\niments and analyses going well beyond the scope\nof the original paper, we ﬁnd that the PRPN model\npresented in that work is nonetheless robust. It\nrepresents a viable method for grammar induction\nand the ﬁrst clear success for latent tree learning\nwith neural networks, and we expect that it her-\nalds further work on language modeling as a tool\nfor grammar induction research.\nAcknowledgments\nThis project has beneﬁted from ﬁnancial support\nto SB by Google and Tencent Holdings, and was\npartly supported by Samsung Electronics (Improv-\ning Deep Learning using Latent Structure). We\nthank Adina Williams, Katharina Kann, Ryan Cot-\nterell, and the anonymous reviewers for their help-\nful comments and suggestions, and NVIDIA for\ntheir support.\nReferences\nRens Bod. 2006. An All-Subtrees Approach to Un-\nsupervised Parsing. Proceedings of the 21st Inter-\nnational Conference on Computational Linguistics\nand the 44th annual meeting of the Association for\nComputational Linguistics, pages 865–872.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP). Association for Computational Linguis-\ntics.\nEugene Charniak and Glen Carroll. 1992. Two exper-\niments on learning probabilistic dependency gram-\nmars from corpora. In Proceedings of the AAAI\nWorkshop on Statistically-Based NLP Techniques,\npage 113.\nJihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018.\nLearning to compose task-speciﬁc tree structures.\nIn Proceedings of the Thirty-Second Association for\nthe Advancement of Artiﬁcial Intelligence Confer-\nence on Artiﬁcial Intelligence (AAAI-18), volume 2.\n5003\nMichael Collins and Brian Roark. 2004. Incremen-\ntal parsing with the perceptron algorithm. InPro-\nceedings of the 42nd Annual Meeting of the Asso-\nciation for Computational Linguistics, 21-26 July,\n2004, Barcelona, Spain., pages 111–118.\nChristoph Goller and Andreas Kuchler. 1996. Learn-\ning task-dependent distributed representations by\nbackpropagation through structure. InProceedings\nof International Conference on Neural Networks\n(ICNN’96).\nSepp Hochreiter and J¨urgen Schmidhuber. 1996. Long\nShort Term Memory.Memory, (1993):1–28.\nYoon Kim, Carl Denton, Luong Hoang, and Alexan-\nder M. Rush. 2017. Structured attention networks.\nDan Klein and Christopher D. Manning. 2002. A\ngenerative constituent-context model for improved\ngrammar induction. InProceedings of the 40th An-\nnual Meeting on Association for Computational Lin-\nguistics - ACL ’02, page 128.\nDan Klein and Christopher D. Manning. 2005. Nat-\nural language grammar induction with a genera-\ntive constituent-context model.Pattern Recognition,\n38(9):1407–1419.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies.TACL, 4:521–535.\nJean Maillard, Stephen Clark, and Dani Yogatama.\n2017. Jointly learning sentence embeddings and\nsyntax with unsupervised Tree-LSTMs. arXiv\npreprint 1705.09189.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of english: The penn treebank. Computa-\ntional Linguistics, 19(2):313–330.\nYikang Shen, Zhouhan Lin, Chin wei Huang, and\nAaron Courville. 2018. Neural language modeling\nby jointly learning syntax and lexicon. InInterna-\ntional Conference on Learning Representations.\nNoah A. Smith and Jason Eisner. 2005. Guiding un-\nsupervised grammar induction using contrastive es-\ntimation. In Proceedings of IJCAI Workshop on\nGrammatical Inference Applications, pages 73–82.\nRichard Socher, Cliff Chiung-Yu Lin, Andrew Ng, and\nChris Manning. 2011. Parsing Natural Scenes and\nNatural Language with Recursive Neural Networks.\nIn Proceedings of the 28th International Conference\non Machine Learning, pages 129–136.\nAdina Williams, Andrew Drozdov, and Samuel R.\nBowman. 2018a. Do latent tree learning models\nidentify meaningful structure in sentences?Trans-\nactions of the Association for Computational Lin-\nguistics (TACL).\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018b. A broad-coverage challenge corpus for\nsentence understanding through inference. InPro-\nceedings of the North American Chapter of the As-\nsociation for Computational Linguistics (NAACL).\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward\nGrefenstette, and Wang Ling. 2017. Learning to\nCompose Words into Setences with Reinforcement\nLearning. Proceedings of the International Confer-\nence on Learning Representations, pages 1–17.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8552280068397522
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7144315242767334
    },
    {
      "name": "Parsing",
      "score": 0.6629277467727661
    },
    {
      "name": "Grammar",
      "score": 0.6189174056053162
    },
    {
      "name": "Machine learning",
      "score": 0.5633193254470825
    },
    {
      "name": "Tree (set theory)",
      "score": 0.5515449643135071
    },
    {
      "name": "Natural language processing",
      "score": 0.5339950919151306
    },
    {
      "name": "Parse tree",
      "score": 0.49380382895469666
    },
    {
      "name": "Artificial neural network",
      "score": 0.4457128345966339
    },
    {
      "name": "Grammar induction",
      "score": 0.423724889755249
    },
    {
      "name": "Language model",
      "score": 0.4208429455757141
    },
    {
      "name": "Tree structure",
      "score": 0.41769129037857056
    },
    {
      "name": "Rule-based machine translation",
      "score": 0.2507975101470947
    },
    {
      "name": "Data structure",
      "score": 0.16145333647727966
    },
    {
      "name": "Linguistics",
      "score": 0.119190514087677
    },
    {
      "name": "Programming language",
      "score": 0.0967683494091034
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I109736498",
      "name": "Canadian Institute for Advanced Research",
      "country": "CA"
    }
  ],
  "cited_by": 46
}