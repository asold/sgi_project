{
  "title": "Human Emotion and Sentiment in Natural Language Understanding and Generation using Large Language Models with Limited to No Labeled Data",
  "url": "https://openalex.org/W4384698103",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2706196786",
      "name": "Md Moinuddin Sharif Riyadh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1975675278",
    "https://openalex.org/W2102998034",
    "https://openalex.org/W6661118852",
    "https://openalex.org/W2182096631",
    "https://openalex.org/W2156516654",
    "https://openalex.org/W2011722134",
    "https://openalex.org/W4205184193",
    "https://openalex.org/W2783382819",
    "https://openalex.org/W4211186029",
    "https://openalex.org/W2148506018",
    "https://openalex.org/W6633918527",
    "https://openalex.org/W2019759670",
    "https://openalex.org/W3102944297",
    "https://openalex.org/W2920568733",
    "https://openalex.org/W2908802621",
    "https://openalex.org/W3001254509",
    "https://openalex.org/W2810665353",
    "https://openalex.org/W2949998441",
    "https://openalex.org/W6683895063",
    "https://openalex.org/W38739846",
    "https://openalex.org/W6647096421",
    "https://openalex.org/W6723383672",
    "https://openalex.org/W6718766952",
    "https://openalex.org/W6665993671",
    "https://openalex.org/W2180724871",
    "https://openalex.org/W2588144628",
    "https://openalex.org/W2144378002",
    "https://openalex.org/W2951278869",
    "https://openalex.org/W40549020",
    "https://openalex.org/W2124156373",
    "https://openalex.org/W2113125055",
    "https://openalex.org/W2925425818",
    "https://openalex.org/W2889086200",
    "https://openalex.org/W3110418807",
    "https://openalex.org/W2744125130",
    "https://openalex.org/W2743246365",
    "https://openalex.org/W2996027940",
    "https://openalex.org/W2146338426",
    "https://openalex.org/W2609916062",
    "https://openalex.org/W2785939461",
    "https://openalex.org/W2807038642",
    "https://openalex.org/W2626561952",
    "https://openalex.org/W2733520819",
    "https://openalex.org/W2549957643",
    "https://openalex.org/W2738554243",
    "https://openalex.org/W2250879510",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2901737885",
    "https://openalex.org/W2937872991",
    "https://openalex.org/W2584429674",
    "https://openalex.org/W3081987387",
    "https://openalex.org/W2562539671",
    "https://openalex.org/W2948738230",
    "https://openalex.org/W3009222422",
    "https://openalex.org/W101809282",
    "https://openalex.org/W2020111801",
    "https://openalex.org/W1992605069",
    "https://openalex.org/W25301398",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2773167282",
    "https://openalex.org/W2771971003",
    "https://openalex.org/W2766095568",
    "https://openalex.org/W2963711448",
    "https://openalex.org/W2950868217",
    "https://openalex.org/W2948211076",
    "https://openalex.org/W3035327313",
    "https://openalex.org/W3100743427",
    "https://openalex.org/W3190697745",
    "https://openalex.org/W3154679754",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3172474223",
    "https://openalex.org/W2612780102",
    "https://openalex.org/W4301144586",
    "https://openalex.org/W3093592644",
    "https://openalex.org/W1980089755",
    "https://openalex.org/W3099880460",
    "https://openalex.org/W2981046779",
    "https://openalex.org/W4297734170",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4292358656",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6811340617",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W3146079624",
    "https://openalex.org/W2144656844",
    "https://openalex.org/W6747888264",
    "https://openalex.org/W7030290986",
    "https://openalex.org/W2186250423",
    "https://openalex.org/W3035454789",
    "https://openalex.org/W1973116500",
    "https://openalex.org/W3101980583",
    "https://openalex.org/W3115279077",
    "https://openalex.org/W2779594574",
    "https://openalex.org/W6679390333",
    "https://openalex.org/W3042491947",
    "https://openalex.org/W6742630102",
    "https://openalex.org/W2947298857",
    "https://openalex.org/W3159075545",
    "https://openalex.org/W4221154033",
    "https://openalex.org/W3199849862",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W6754111937",
    "https://openalex.org/W3092556601",
    "https://openalex.org/W3212218375",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2101210369",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4205611454",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W3154461582",
    "https://openalex.org/W4287689466",
    "https://openalex.org/W2096166399",
    "https://openalex.org/W1565746575",
    "https://openalex.org/W1527143998",
    "https://openalex.org/W2775349221",
    "https://openalex.org/W3005135893",
    "https://openalex.org/W2738581557",
    "https://openalex.org/W3209701493",
    "https://openalex.org/W6739651123",
    "https://openalex.org/W4283574889",
    "https://openalex.org/W3199446690",
    "https://openalex.org/W3132892498",
    "https://openalex.org/W3207046702",
    "https://openalex.org/W2963847257",
    "https://openalex.org/W6770211150",
    "https://openalex.org/W3101415412",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W6777273528",
    "https://openalex.org/W1967339229",
    "https://openalex.org/W2913957986",
    "https://openalex.org/W3035101819",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W6767278183",
    "https://openalex.org/W6763240421",
    "https://openalex.org/W2975003218",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W1967390364",
    "https://openalex.org/W3092180782",
    "https://openalex.org/W6741814560",
    "https://openalex.org/W3118043957",
    "https://openalex.org/W2969740599",
    "https://openalex.org/W3166728183",
    "https://openalex.org/W6796963063",
    "https://openalex.org/W6791241415",
    "https://openalex.org/W3195809814",
    "https://openalex.org/W6757380498",
    "https://openalex.org/W2725671465",
    "https://openalex.org/W3203324590",
    "https://openalex.org/W6741121127",
    "https://openalex.org/W3187269764",
    "https://openalex.org/W3098902347",
    "https://openalex.org/W3111711122",
    "https://openalex.org/W3158172841",
    "https://openalex.org/W4225141070",
    "https://openalex.org/W4287208373",
    "https://openalex.org/W4308291381",
    "https://openalex.org/W3089889634",
    "https://openalex.org/W3094002940",
    "https://openalex.org/W4214571532",
    "https://openalex.org/W2013946207",
    "https://openalex.org/W4313679560",
    "https://openalex.org/W2970785793",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W2152921782",
    "https://openalex.org/W2081035377",
    "https://openalex.org/W6667515630",
    "https://openalex.org/W1493882948",
    "https://openalex.org/W63818876",
    "https://openalex.org/W1763968285",
    "https://openalex.org/W3214667981",
    "https://openalex.org/W3017138996",
    "https://openalex.org/W3168264548",
    "https://openalex.org/W4221165966",
    "https://openalex.org/W4287111386",
    "https://openalex.org/W2885227423",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W2974422315",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W3116170960",
    "https://openalex.org/W2148598618",
    "https://openalex.org/W1565863475",
    "https://openalex.org/W2951124019",
    "https://openalex.org/W2068390867",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3167369375",
    "https://openalex.org/W3101638716",
    "https://openalex.org/W2200988052",
    "https://openalex.org/W4312773554",
    "https://openalex.org/W4384209868",
    "https://openalex.org/W2964236337",
    "https://openalex.org/W4288365890",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4245973528",
    "https://openalex.org/W3169017236",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3213795336",
    "https://openalex.org/W1589554437",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3175039711",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2042026400",
    "https://openalex.org/W4231458041",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W3113473253",
    "https://openalex.org/W2491699931",
    "https://openalex.org/W2062913298",
    "https://openalex.org/W3107250304",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2985831993",
    "https://openalex.org/W1987425720",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W2132984949",
    "https://openalex.org/W4287028715",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W2964321678",
    "https://openalex.org/W2963672599",
    "https://openalex.org/W3176489198",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2966024403",
    "https://openalex.org/W2963499153",
    "https://openalex.org/W4287019656",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2112422413",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2964010806",
    "https://openalex.org/W4206232983",
    "https://openalex.org/W2250243742",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2971071159",
    "https://openalex.org/W3024131638",
    "https://openalex.org/W4252684946",
    "https://openalex.org/W4287213042",
    "https://openalex.org/W4321153836",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3112887748",
    "https://openalex.org/W4385567267",
    "https://openalex.org/W3134250911"
  ],
  "abstract": "Natural Language Processing (NLP) aims to utilize computational resources to comprehend and generate human language. Emotion and sentiment are integral parts of human beings, and they are often reflected in human language. Consequently, these two closely related ideas are of paramount importance to NLP. In this thesis, we focus on several NLP tasks related to human emotion and sentiment. Particularly, we focus on the domains of Sentiment Analysis and Emotion-Cause Analysis (ECA). Like most other NLP tasks, machine learning technologies are frequently leveraged to perform various NLP tasks in these two domains. A common challenge in applying machine learning technology to context-dependent tasks like Sentiment Analysis is that they require a large amount of labeled data to develop a performant model. In this thesis, we develop several techniques leveraging Transformer-based large language models (LLMs) to perform various NLP tasks within these two domains in a limited to no labeled data setting. Specifically, we devise two technical architectures to perform multi-class Sentiment Analysis with limited labeled data. We introduce two new NLP tasks within the domain of ECA, which are also the first Natural Language Generation (NLG) tasks in this domain. We devise technical solutions to perform these NLG tasks, one with limited labeled data, and the other with no labeled data. We publish a new dataset for one of these novel NLG tasks. Lastly, we propose leveraging conversational LLMs for the automatic evaluation of open-ended NLG tasks, which also does not require any new training or labeled data.",
  "full_text": " 1 \n \n \nHuman Emotion and Sentiment in Natural Language \nUnderstanding and Generation using Large Language Models \nwith Limited to No Labeled Data \n \nby \n \nMd Riyadh \n \n \nA thesis submitted to the Faculty of Graduate and Postdoctoral Affairs in \npartial fulfillment of the requirements for the degree of \n \nDoctor of Philosophy \nin \nInformation Technology \n \nCarleton University \nOttawa, Ontario \n \n \n© 2023, Md Riyadh \n\n 2 \nAcknowledgments \nI would like to express my heartfelt gratitude to everyone who has supported me throughout my \ndoctoral journey. \nFirst and foremost, I am deeply grateful to my supervisor, Professor Omair Shafiq , for his \nguidance, expertise, and unwavering support. His insightful and constructive feedback have been \ninvaluable in shaping my research and helping me grow as a researcher . With his excellent \nmentorship, I was able to publish several conference and journal pape rs in reputed venues. In \naddition to academic guidance, he has been exceptionally supportive in helping me successfully \nnavigate many unique challenges I encountered, including the tough time shared by all of us during \nthe COVID -19 pandemic. Without his ph enomenal support, I would not have been able to \ncomplete this endeavor. \nI would like to thank the members of my thesis committee  for their time and effort in reviewing \nmy work and providing valuable feedback.  I would also like to thank Professor Ali Arya f or his \nkind support during the initial phase of my Ph.D., without which I would not have embarked on \nthis journey. \nMy sincere thanks also go to my friends who have offered encouragement, assistance, and \ncamaraderie throughout this challenging but rewarding journey. Particularly, I would like to thank \nmy dear friend Dr. Gerry Chan for his consistent encouragement and support throughout my Ph.D. \njourney. His inspiration has been a key factor behind my decision to undertake this challenge and \nthen persist in it. I would also like to thank my colleagues and mentors who have supported me in \nvarious capacities during this journey.  \nSpecial thanks are due to my family, who have been my pillars of strength and unwavering support \nthrough thick and thin. Their love, p atience, and understanding have made all the difference in \nhelping me pursue my academic goals.  Specifically, I am grateful to my parents for their \ncontinuous prayers and inspiration in all my academic pursuits, including this one. It is them who \nnurtured my curiosity at an early age and turned me into a lifelong seeker of knowledge. I am \nthankful to my wife who has made remarkable sacrifices and supported me to stay motivated \nduring the entire journey. I am also grateful to my son who is just about to turn 7 and has seen me \nas a graduate student almost all his life. Though imperceptible, the sacrifices he made by skipping \n 3 \nthat play session or that outing is monumental, a lot of which are alleviated by his mother’s \nextraordinary sacrifice and affection.  \nThank you all for your support, encouragement, and inspiration. I am deeply grateful and humbled \nby your kindness and generosity. \n  \n 4 \nAuthor Declaration \nI am the first author of the publications that contribute to this thesis. I used the word ‘we’ in this \ndocument to indicate the collaboration with my supervisor and mentorship from him in completing \nthis research . I want to thank my supervisor, Professor Omair Shafiq, for his support in \naccomplishing this work. Publications directly related to this thesis include: \n• Riyadh, M., & Shafiq, M. O. (2021). Towards Multi -class Sentiment Analysis with Limited \nLabeled Data. In 2021 IEEE International Conference on Big Data (Big Data) (pp. 4955-4964) \n• Riyadh, M., & Shafiq, M. O. (2022). GAN -BElectra: Enhanced Multi -class Sentiment \nAnalysis with Limited Labeled Data. Taylor and Francis Journal of Applied Artificial \nIntelligence, 36(1), 2083794, impact factor: 2.777. \n• Riyadh, M., & Shafiq, M. O. (2022). Towards Emotion Cause Generation in Natural Language \nProcessing using Deep Learning. In 21st IEEE International Conference on Machine Learning \nand Applications (ICMLA), 2022.  \n• [In review] Riyadh, M., & Shafiq, M. O. (2023). ECSGen and iZen: A New NLP Task and A \nZero-shot Framework to Perform It. \n• [Accepted] Riyadh, M., & Shafiq, M. O. (2023). Towards Automatic Evaluation of NLG \nTasks using Conversational Large Language Models. In 19th IFIP International Conference on \nArtificial Intelligence Applications and Innovations. \n  \n 5 \nAbstract \nNatural Language Processing (NLP) aims  to utilize computational resources to comprehend and \ngenerate human language. Emotion and sentiment are integral parts of human beings, and they are \noften reflected in human language. Consequently, these two closely related ideas are of paramount \nimportance to NLP. In this thesis, we focus on several NLP tasks related to human emotion and \nsentiment. Particularly, we focus on the domains of Sentiment Analysis  and Emotion-Cause \nAnalysis (ECA). Like most other NLP tasks, machine learning technologies are frequently \nleveraged to perform various NLP tasks in these two domains. A common challenge in applying \nmachine learning technology to context -dependent tasks like Sentiment Analysis  is that they \nrequire a large amount of labeled data to develop a performant model. In this thesis, we develop \nseveral techniques leveraging Transformer -based large language models (LLMs) to perform \nvarious NLP tasks within these two domai ns in a limited to no labeled data setting. Specifically, \nwe devise two technical architectures to perform multi -class Sentiment Analysis with limited \nlabeled data. We introduce two new NLP tasks within the domain of ECA, which are also the first \nNatural L anguage Generation (NLG) tasks in this domain. We devise technical solutions to \nperform these NLG tasks, one with limited labeled data, and the other with no labeled data. We \npublish a new dataset for one of these novel NLG tasks. Lastly, we propose levera ging \nconversational LLMs for the automatic evaluation of open-ended NLG tasks, which also does not \nrequire any new training or labeled data.  \nKeywords: Natural Language Processing, Natural Language Generation, Sentiment Analysis , \nEmotion-Cause Analysis, Deep Learning, Transformer, Large Language Model \n \n 6 \nTable of Contents \nAcknowledgments ......................................................................................................................... 2 \nAuthor Declaration ....................................................................................................................... 4 \nAbstract .......................................................................................................................................... 5 \nTable of Contents .......................................................................................................................... 6 \nList of Tables ................................................................................................................................. 8 \nList of Figures ................................................................................................................................ 9 \nList of Abbreviations .................................................................................................................. 11 \nChapter  1: Introduction .......................................................................................................................... 12 \n1.1 Overview ............................................................................................................................... 12 \n1.2 Research Problems ................................................................................................................ 13 \n1.3 Contributions ......................................................................................................................... 15 \n1.4 Overall Document Structure.................................................................................................. 16 \nChapter  2: Background .......................................................................................................................... 18 \n2.1 Emotion and Sentiment ......................................................................................................... 18 \n2.2 Emotion Cause Analysis (ECA) ............................................................................................ 34 \n2.3 An Overview of Related Machine Learning Technologies ................................................... 39 \nChapter  3: Multiclass Sentiment Analysis with Limited Labeled Data ................................................. 51 \n3.1 Introduction ........................................................................................................................... 51 \n3.2 Related Studies ...................................................................................................................... 53 \n3.3 SG-Elect ................................................................................................................................ 55 \n3.4 GAN-BElectra ....................................................................................................................... 62 \n3.5 Experiments and Evaluation.................................................................................................. 66 \n3.6 Results ................................................................................................................................... 71 \n3.7 Discussion ............................................................................................................................. 84 \n3.8 Conclusion............................................................................................................................. 88 \nChapter  4: Emotion Cause Generation: A New NLP Task ................................................................... 89 \n4.1 Introduction ........................................................................................................................... 89 \n4.2 Contributions ......................................................................................................................... 91 \n4.3 Related Studies ...................................................................................................................... 92 \n4.4 ECG: The Proposed Task ...................................................................................................... 94 \n4.5 Demonstration of the Proposed Task .................................................................................... 94 \n4.6 Dataset ................................................................................................................................... 96 \n4.7 Results ................................................................................................................................... 98 \n 7 \n4.8 Observation and Analysis...................................................................................................... 99 \n4.9 Discussion ........................................................................................................................... 103 \n4.10 Limitation and Future Work ................................................................................................ 105 \n4.11 Conclusion........................................................................................................................... 106 \nChapter  5: ECSGen and iZen: A New NLP Task and a Zero-shot Framework to Perform It ............. 107 \n5.1 Introduction ......................................................................................................................... 107 \n5.2 Contributions ....................................................................................................................... 109 \n5.3 Related Studies .................................................................................................................... 109 \n5.4 ECSGen: The Proposed Task .............................................................................................. 111 \n5.5 iZen: A Framework to Perform ECSGen ............................................................................ 112 \n5.6 Experiments and Evaluation................................................................................................ 122 \n5.7 Results ................................................................................................................................. 129 \n5.8 Discussion ........................................................................................................................... 136 \n5.9 Risks, Limitations and Future Work ................................................................................... 139 \n5.10 Conclusion........................................................................................................................... 142 \nChapter  6: Towards Automatic Evaluation of NLG Tasks using Conversational Large Language Models\n .............................................................................................................................................................. 143 \n6.1 Introduction ......................................................................................................................... 143 \n6.2 Contributions ....................................................................................................................... 144 \n6.3 Related Studies .................................................................................................................... 145 \n6.4 Conversational LLMs as Automatic Evaluators for NLG Tasks ........................................ 146 \n6.5 Discussion ........................................................................................................................... 154 \n6.6 Limitations and Future Studies ........................................................................................... 156 \n6.7 Conclusions ......................................................................................................................... 158 \nChapter  7: Overall Conclusions ........................................................................................................... 159 \n7.1 Implications of the Contributions ........................................................................................ 160 \n7.2 Future Work ........................................................................................................................ 161 \nAppendices ................................................................................................................................. 163 \nReferences .................................................................................................................................. 165 \n 8 \nList of Tables \nTable  1: Vectorization Example ................................................................................................................ 40 \nTable  2: Percentage of labeled data in total training data .......................................................................... 67 \nTable  3: Summary of Results (F1 Macro, F1 Weighted Avg., Accuracy, Standard Deviation) ................ 74 \nTable  4: Summary of Results (Standard Error, Confidence Interval of Standard Error) ........................... 74 \nTable  5: GAN-BERT's accuracy in pseudo label generation across three datasets ................................... 74 \nTable  6: Percentage contribution to final selected pseudo labels for each class ........................................ 74 \nTable  7: Percentage contribution to final selected pseudo labels .............................................................. 75 \nTable  8: Detailed Results for the SST5 Dataset ......................................................................................... 76 \nTable  9: Detailed Results for the US Airline Dataset ................................................................................ 76 \nTable  10: Detailed Results for the SemEval Dataset ................................................................................. 77 \nTable  11: Results (p -values) from significance testing to compare SG -Elect’s performance with GAN -\nBERT, Electra, and SS-Trainer using Wilcoxon Signed Ranks Test [180] for accuracy, F1 macro, and F1 \nweighted average scores. Bold indicates statistical significance (p-value < 0.05). .................................... 83 \nTable  12: Results (p-values) from significance testing to compare GAN-BElectra’s performance with SG-\nElect, GAN-BERT, Electra, and SS -Trainer using Wilcoxon Signed Ranks Test [180] for accuracy, F1 \nmacro, and F1 weighted average scores. Bold indicates statistical significance (p-value < 0.05). ............ 83 \nTable  13: Texts with original causes and texts with generated causes ...................................................... 99 \nTable  14: Evaluation of the generated causes with methods that use original text as a reference ........... 102 \nTable  15: Evaluating the linguistic quality of the generated causes ........................................................ 103 \nTable  16: Sentiment and emotion analysis of the test data point with original and generated causes. The \nrows follow the same order as Table  13, and the serial numbers in left most column correspond to the serial \nnumbers in the left most column of Table  13. Asterisk (*) indicates synonymous emotion label added for \nthe ease of manual comparison. Circumflex (^) indicates mismatch. ...................................................... 103 \nTable  17: Automatic metrics score of the generated suggestions ............................................................ 130 \nTable  18: Relevancy of the generated suggestion per input statements ................................................... 132 \nTable  19: ChatGPT Prompts used in this study for different NLG task .................................................. 150 \nTable  20: Summary of ChatGPT’s performance as an evaluator for various NLG tasks ........................ 154 \n 9 \nList of Figures \nFigure 1: Robert Plutchik's “Wheel of emotion” ........................................................................................ 21 \nFigure 2: Sentiment Analysis within the NLP taxonomy ........................................................................... 23 \nFigure 3: Tokenization of a sentence .......................................................................................................... 25 \nFigure 4: Sentiment Analysis Process Overview ........................................................................................ 26 \nFigure 5: Sentiment Analysis using a lexicon ............................................................................................. 27 \nFigure 6: Different ECA Tasks ................................................................................................................... 36 \nFigure 7: CBOW and Skip-gram ................................................................................................................ 42 \nFigure 8: Machine learning and deep learning process ............................................................................... 44 \nFigure 9: Deep Neural Network .................................................................................................................. 45 \nFigure 10: A typical GAN architecture ....................................................................................................... 46 \nFigure 11: Conventional learning vs transfer learning ................................................................................ 47 \nFigure 12: Transformer Architecture .......................................................................................................... 48 \nFigure 13: Semi-supervised stacked classifier subcomponent .................................................................... 57 \nFigure 14: GAN-BERT Architecture .......................................................................................................... 58 \nFigure 15: Electra-based pretrained component ......................................................................................... 59 \nFigure 16: SG-Elect Architecture. U, L, P denote unlabeled, labeled, and pseudo-labeled data respectively\n .................................................................................................................................................................... 60 \nFigure 17: GAN-BElectra architecture. ...................................................................................................... 63 \nFigure 18: SST5 dataset composition ......................................................................................................... 69 \nFigure 19: US Airline dataset composition ................................................................................................. 70 \nFigure 20: SemEval dataset composition .................................................................................................... 70 \nFigure 21: Confusion matrices for SST5 dataset ........................................................................................ 80 \nFigure 22: Confusion matrices(with red shades) for the US Airline dataset .............................................. 81 \nFigure 23: ECG and its relationship with the existing ECA tasks. ............................................................. 90 \nFigure 24: A sentence expressing an emotion (\"happy\") with the cause indicated in bold. ....................... 94 \nFigure 25: The top sentence represents an input to the model .................................................................... 96 \nFigure 26: An example of the ECSGen task ............................................................................................. 111 \nFigure 27: ECSGen and other ECA tasks ................................................................................................. 112 \nFigure 28: iZen Framework’s Architecture .............................................................................................. 114 \nFigure 29: An overview of how 800 input statements .............................................................................. 123 \nFigure 30: Sentiment of the generated suggestions................................................................................... 130 \nFigure 31: Overall relevancy of all the generated suggestions. ................................................................ 131 \nFigure 32: Relevancy of the generated suggestion per input statements. ................................................. 132 \n 10 \nFigure 33: Variation of the relevancy of the generated suggestion .......................................................... 133 \nFigure 34: Boxplot to show the variation of the relevancy of the generated suggestion .......................... 134 \nFigure 35: Variation of the relevancy of the generated suggestion based on the data source................... 135 \nFigure 36: Boxplot to show the variation of the relevancy of the generated suggestion .......................... 135 \nFigure 37: Variation of the relevancy of the generated suggestion based on the LM ............................... 136 \nFigure 38: Boxplot to show the variation .................................................................................................. 136 \nFigure 39: ECSGen confusion matrices .................................................................................................... 153 \n \n  \n 11 \nList of Abbreviations \nAbbreviation Meaning \nAI Artificial Intelligence \nANN Artificial Neural Networks \nBERT Bidirectional Encoder Representations from Transformers \nBOW Bag of Words \nCBOW Continuous Bag of Words \nCNN Convolution Neural Networks \nCPU Central Processing Unit \nCRF Conditional Random Field \nDNN Deep Neural Network \nECE Emotion cause extraction \nECPE Emotion-Cause Pair Extraction \nESCP Emotion-Cause Span-Pair extraction and classification \nGAN Generative Adversarial Network \nGPT Generative Pretrained Transformer \nGPU Graphics Processing Unit \nGRU Gated Recurrent Unit \nIDF Inverse Document Frequency \nLLM Large Language Model \nLSTM Long Short-Term Memory \nNB Naive Bayes \nNLG Natural Language Generation \nNLP Natural Language Processing \nNLU Natural Language Understanding \nOPT Open Pretrained Transformer \nPOS Parts of Speech \nRF Random Forest \nRNN Recurrent Neural Networks \nSGD Stochastic Gradient Descent \nSOTA State of the Art \nSRN Simple Recurrent Network \nSS Semi Supervised \nSVM Support Vector Machine \nTF Term Frequency \nTPU Tensor Processing Unit \n \n  \n 12 \nChapter  1: Introduction \nIn this chapter, we provide an overview of this thesis, including the main research problems we \ninvestigated along with our associated contributions. We discuss the common premises and \ndistinctiveness of these contributions. We also outline the overall structure of this document. \n1.1 Overview  \nWith the internet and social media rapidly evolving in recent years, a vast amount of text data is \ninundating every aspect of our lives. Much of this textual data is unstructured and raw, making it \ndifficult to properly utilize. One of the most significant challenges in Artificial Intelligence (AI) is \ndeveloping systems that can comprehend unstructured text data and reasonably deduce various \nrich information from them. This is an important driving factor for the rapid advancement of the \nNatural Language Pro cessing (NLP) research field that we currently witness. NLP is an \ninterdisciplinary subfield of linguistics and computer science, which involves understanding and \ngeneration of human language using AI. Natural Language Understanding (NLU) [1], a subset of \nNLP, concentrates on assessing and comprehending human language, including named entities, \nsentiment, and intent. On the other hand, the generation of natural language, like summaries, \ninquiries, or descriptions  of an entity , falls under the category of Natural Language Generation \n(NLG) from [1], which is another subfield of NLP. Sophisticated machine learning systems are \nbeing developed and enhanced every day at an unprecedented pace to perform various NLP tasks \nsuch as Sentiment Analysis , machine tran sition, question -answering, summary generation, and \nemotion analysis among other applications.  \nIn this thesis, we refer to NLP as it relates to human language in the textual form. One of the \nunderlying themes of the research problems we investigate in this thesis is the application domains \nwe explore, which revolve around human sentiment and emotion. Another common premise is the \nquest of performing specific NLP tasks with limited to no training data. All the technical solutions \nwe propose to perform various NLP tasks in this thesis intersect at the involvement of Transformer-\nbased Large Language Mod els (LLMs). We explore both NLU and NLG as part of our \ninvestigation. Specifically, for NLU, we investigate multi -class Sentiment Analysis with limited \nlabel data, and for NLG, we explore the domain of Emotion-Cause Analysis  (ECA). While \nexploring these different avenues of NLP with an emphasis on the underlying themes mentioned \n 13 \nabove, we develop new techniques to perform an existing task (i.e., Sentiment Analysis ), we \npropose novel NLP tasks, particularly two new generative NLP tasks (i.e., NLG tasks) with in the \nECA domain, we curate our own dataset and devise technical solutions to perform these novel \ntasks. Additionally, we suggest a novel method for the automatic evaluation of open -ended NLG \ntasks, such as the ones we propose in this thesis. \n1.2 Research Problems \nIn this thesis, we aim to address several research problems which encompass some common \npremises as well as some distinct elements. The common grounds include the basis of the \napplication domains we explore which are human emotion and sentiment. It a lso includes our \nconsistent focus on solving NLP tasks with limited to no labeled data with techniques involving \nTransformer-based large language models (LLMs). With these common premises, each constituent \nresearch work of this thesis builds upon the learn ing and inspiration from the previous one and \ncontributes distinctively to the respective areas. Our thesis commences with Sentiment Analysis \nwhich subsequently extends toward analyzing the cause of the emotion. Further investigation into \nthe ECA research area results in the proposal of two novel NLG tasks and the development of \nassociated technical solutions to perform them. Lastly, inspired by the limitations of the existing \nautomatic evaluators for open -ended NLG tasks that we observe while evaluating th ese newly \nproposed NLG tasks, we suggest a new automatic evaluation technique for such NLG tasks. We \noutline these successive research problems in the following paragraphs: \nIn Sentiment Analysis , a given piece of text is categorized with respect to the sen timent it \nexpresses, often in terms of positive, neutral, or negative. Training machine learning models to \nperform such classification of text is a common phenomenon . This type of machine learning \ntechnique, commonly referred to as supervised learning, hea vily relies on training data. \nComplexity in Sentiment Analysis can increase due to the fact that  words used in one context to \nexpress certain sentiments may exert different sentiments in different contexts. As a result, training \na machine learning model fo r Sentiment Analysis tasks typically requires training with domain -\nspecific labeled data. Having a large quantity of labeled training data for specific domains can be \ntedious and expensive. This problem is one of the focus areas of this thesis. Specificall y, we \nattempt to tackle the following research question: \n 14 \n• RQ1: How to develop techniques that can perform multi-class Sentiment Analysis with limited \ntraining data (i.e., 50 datapoints per class) while still achieving superior classification accuracy \ncompared to the state-of-the-art (SOTA) baseline technique? \nIn the ECA domain, we observe that researchers have recently proposed several new tasks, all of \nwhich can be broadly labeled as classification tasks. Though we have recently witnessed the rapid \ngrowth of NLG tasks (i.e., generative NLP tasks) within the broader domain of NLP, such as poetry \ngeneration, machine translation, and question -answering, the application of generative machine \nlearning has largely remained  unexplored in the ECA domain. We, therefo re, investigate this \nunderexplored area; specifically, we attempt to answer the following research questions: \n• RQ2(a): Can we formulate a viable novel NLG task within the domain of ECA that aims to \ngenerate a meaningful cause of an emotion expressed in a given text? \n• RQ2(b): How to perform the novel task mentioned in RQ2(a) by building upon existing \nmachine learning techniques and leveraging the existing ECA datasets? \n• RQ3(a): Can we construct a new and feasible NLG task within the ECA domain that aims to \ngenerate relevant suggestions to mitigate the cause of a negative emotion stated in a given text? \n• RQ3(b): How to devise a technical solution to perform the novel task mentioned in RQ3(a) \nwithout requiring any new training or fine-tuning steps? \nEvaluating a mac hine learning model on its ability to perform the NLG tasks we refer to in the \nabove research questions typically requires human participants. This is equally applicable to \nalmost all open -ended NLG tasks such as summary generation, poetry generation, and \nparaphrasing. This usually impacts the velocity of these research studies as conducting human \nevaluation can be time-consuming. While there are many attempts to develop automatic evaluation \ntechniques for NLG tasks, none of them have been regarded as a use ful alternative to human \nevaluation. Largely motivated by the challenges we encountered in evaluating the NLG tasks \nmentioned in RQ2 and RQ3, we conduct a brief investigation into this area and attempt to answer \nthe following research question: \n• RQ4: Can we leverage Conversational Large Language Models as an automatic evaluator for \nopen-ended NLG tasks? \n 15 \n1.3 Contributions \nThe overall contributions of this thesis are listed below: \n• We propose two successive techniques for multi-class Sentiment Analysis named SG-Elect \nand GAN-BElectra. Our experiments demonstrate that SG-Elect achieves significantly higher \nperformance (i.e., F1 macro, and F1 weighted average) in multi -class Sentiment Analysis \ncompared to its baseline. Building upon SG -Elect, we develop G AN-BElectra, which \nsignificantly reduces the architecture complexity and required training steps compared to SG-\nElect, without having any adverse effect in the classification accuracy in a similar limited \nlabeled data setting. This is related to RQ1. We publish one conference paper and one journal \npaper based on this work:  \no Riyadh, M., & Shafiq, M. O. (2021). Towards Multi -class Sentiment Analysis with \nLimited Labeled Data. In 2021 IEEE International Conference on Big Data (Big Data) \n(pp. 4955-4964)  \no Riyadh, M., & Shafiq, M. O. (2022). GAN-BElectra: Enhanced Multi-class Sentiment \nAnalysis with Limited Labeled Data. Taylor and Francis Journal of Applied Artificial \nIntelligence, 36(1), 2083794, impact factor: 2.777. \n• We propose a new generative NLP task within the domain of ECA named Emotion -Cause \nGeneration (ECG). We enhance an existing infilling architecture to perform this task and \nestablish the viability of the proposed task through a technical demonstration. This is related \nto RQ2(a) and RQ2(b). We published one conference paper related to this contribution:  \no Riyadh, M., & Shafiq, M. O. (2022). Towards Emotion Cause Generation in Natural \nLanguage Processing using Deep Learning. In  21st IEEE International Conference on \nMachine Learning and Applications (ICMLA), 2022. \n• We propose Emotion-Cause mitigating Suggestion Generation (ECSGen), a novel generative \nNLP task within the domain of ECA. We curate and publish a dataset to perform this new task \nand for potential future research in this area. This is related to RQ3(a). We also propose iZen, \na zero-shot framework to perform the ECSGen task. We demonstrate its promising ability to \nperform this task without any training or fine-tuning through experiments and evaluations. This \nis related to RQ3(b). This contribution is reported in the following journal paper:  \n 16 \no [In Review] Riyadh, M., & Shafiq, M. O. (2023). ECSGen and iZen: A New NLP Task \nand A Zero-shot Framework to Perform It. \n• We hypothesize that the conversational Large Language Models can be used as automatic \nevaluators for open-ended NLG tasks such as the ones we introduce in this thesis. We perform \nseveral experiments to investigate our hypothesis. This contribution is related to RQ4 and is \nincluded in the following conference paper:  \no [Accepted] Riyadh, M., & Shafiq, M. O. (2023). Towards Automatic Evaluation of \nNLG Tasks using Conversational Large Language Model s. In 19th IFIP International \nConference on Artificial Intelligence Applications and Innovations. \n1.4 Overall Document Structure \nThis thesis is structured as a series of research investigations intended to answer the research \nquestions outlined in section 1.2. The organization of this thesis follows an integrated article \nformat [2] consisting of two introductory chapters (Introduction and Background) and a \nconcluding one along with several self-contained chapters which are based on our own published \njournals, conference papers, or papers awaiting publication. In addition to the expe riment, \nevaluation, and discussion sections, these self -contained chapters introduce each of the research \nquestions in more detail supported by the discussion on the specific related studies. We describe \nthe overall structure of this thesis below: \n• Chapter 1 introduces the overall research problems we investigate in this thesis along with \ntheir common premises and distinctiveness.  \n• Chapter 2 provides an overview of the technology and application domains explored in this \nthesis. We discuss two main applicatio n domains of this thesis: Sentiment Analysis , and \nEmotion-Cause Analysis. We present a literature review to show the overall research trends in \nthese two domains. We also introduce the machine learning technologies that we leverage \nthroughout different parts of this thesis.  \n• Chapter 3 presents our study on multi-class Sentiment Analysis with limited labeled data and \nthe two successive techniques that we devise to perform this task. We discuss both techniques \nin tandem to account for the overlaps between the m and to demonstrate how the latter \ntechnique builds upon the initial one.  \n 17 \n• Chapter 4 focuses on our first investigation related to the ECA domain where we introduce \nthe first-ever NLG task in this domain named Emotion-Cause Generation (ECG) and illustrate \nthe viability of this new task through a technical demonstration. \n• Chapter 5 delves into the details of our subsequent investigation related to the ECA domain \nwhere we introduce a second novel NLG task named Emotion -Cause mitigating Suggestion \nGeneration (ECSGen). We also discuss iZen, a zero-shot technical framework that we develop \nto perform this novel task. Additionally, this chapter introduces the dataset we curated for this \nstudy.  \n• Chapter 6 presents our proposal and associated experiments to leverage Conversational Large \nLanguage Models (LLMs) to evaluate open -ended NLG tasks. In addition to some common \nNLG tasks, we also evaluate the two novel ECA -related NLG tasks that we introduce in this \nthesis (ECG, ECSGen) using this method.  \n• Chapter 7  summarizes the key findings of this thesis, their implications in the respective \ndomains along with some potential future work.   \n  \n 18 \nChapter  2: Background \nNatural Language Processing (NLP) is a branch of Artificial Intelligence (AI) concerned with the \nutilization of computational resources to understand and generate human language. In this thesis, \nwe refer to NLP as it relates to human language in the textual form. In broader terms, NLP is an \ninterdisciplinary subfield of linguistics, and computer science , which entails handling , \ncomprehending, and generating human language. This thesis focuses on some distinct NLP tasks \nrelated to Sentiment Analysis and Emotion-Cause Analysis, which we discuss in the following \nsections. \nIn this chapter, we introduce Sentiment Analysis and the Emotion-Cause Analysis domain and \nsummarize the general research trend in these domains. The subsequent chapters present more \nfocused literature reviews specific to the individual research questions. In addition, this chapter \nalso introduces the machine lear ning technologies that are relevant to this thesis. The literature \nreview presented in this chapter and subsequent chapters is the culmination of our thorough search \nof references throughout the successive investigations we report in this thesis. Our literature search \nstrategy included snowballing method [3], both forward and backward. We leveraged several tools \nfor our search including Google Scholar [4], and Carleton University Omni Library [5]. We \neventually narrowed down our focus to the specific gaps we have identified in the literature which \noffered us insight to formulate our research questions and the associated contributions.  \n2.1 Emotion and Sentiment \nThere have been many attempts in the literature to formulate a general definition of emotion. For \nexample, according to Scherer [6], emotions refer to coordinated and interconnected changes in \nvarious bodily systems in response to the evaluation of an internal or external stimulus that is \nrelevant to the organism's primary concerns. Scherer highlights several key features of emotions, \nincluding their event focus, appraisal-driven nature, response synchronization, rapidity of change, \nbehavioral impact, intensity, and duration. Emotions are typically elicited by a stimulus event that \nhas significance to the individual. Such appraisal of significance can be intrinsic or extrinsic and \nis often based on the individual's goals, desires, or needs. Emotions also involve a coordinated \nresponse across various bodily systems that correspond to the individual's appraisal of the event. \nEmotional states are usually intense and have a significant impact on an individual's behavior and \n 19 \nsocial interactions. They also undergo constant modification and readjustment  in response to \nchanging circumstances or evaluations. However, emotions typically have a relatively short \nduration to avoid taxing the individual's resources and maintain behavioral flexibility. \nFillmore et al. developed a linguistic resource named FrameN et [7], where they suggest that an \nindividual who experiences emotions has a specific emotional state, which may be described in \nterms of a particular stimulus that elicits it or a category that classifies the type of stimulus. \nAdditionally, there may be  circumstances that influence the emotional response or reasons why \nthe stimulus provokes a particular response in the experiencer.  Ortony et al.  [8] offer another \nperspective on emotions, defining them as “valenced” (positive-negative) reactions to even ts, \nagents, or objects, with their specific nature being determined by the construction of the eliciting \nsituation. Using this theory as a basis, Ghazi et al. [9] put forth a few propositions to explain the \ndistinctions between some basic emotions . For e xample, they define “j oy” as the feeling of \npleasure in response to a desirable event , and “distress” as the feeling of displeasure in response \nto an undesirable event. Similarly, “hope” is the feeling of pleasure in anticipation of a desirable \nevent or response to an uncertain desirable event  while “f ear” is the feeling of displeasure in \nanticipation of an undesirable event or response to an uncertain undesirable event. They also define \n“anger” as the feeling of displeasure in response to an undesirable event caused by someone else's \naction and “disgust” as the feeling of disliking an unappealing object or situation.  In contrast, \n“shame and guilt ” are defined as feelings of displeasure in  response to one's own blameworthy \nactions or undesirable events caused by oneself. \n \n \nAlthough Ortony et al.'s [8] theory explains some basic emotions, there is still no agreement among \ndifferent theories regarding the number of basic emotions. For insta nce, Ortony et al. [8] reject \n“surprise” as an emotion because they believe it is a cognitive state related to unexpectedness \nrather than a valenced reaction. Other theorists, including Ekman  and Plutchik, take a \ncombinatorial approach, suggesting that primary emotions can combine to produce other emotions. \nEkman [10] proposed six basic emotions with distinct facial expressions that are universally \nrecognized: joy, sadness, anger, fear, disgust, and surprise. Plutchik [11] added “trust” and \n“anticipation” to Ekman's set based on their relationship to adaptive biological processes. This \n 20 \nformulation of human emotions is presented in the famous “Wheels of Emotion” [11] as depicted \nin the Figure 1. In addition to the eight basic emotions (2nd layer from the center in Figure 1), this \ndiagram shows various other compound emotions and relationships between them. For instance, \neach of these basic emotions has its polar opposite, located on the opposite end of the diagram \n(e.g., joy vs. sadness). The emotions in the outer boundary without any background color indicate \na combination of two primary emotions. For instance, “anticipation” and “joy” in conjunction form \n“optimism”. \n \n 21 \n \nFigure 1: Robert Plutchik's “Wheel of emotion” [11] \nAlthough emotion and sentiment are related ideas and there are meaningful overlaps between \nthem, they are not completely identical or interchangeable as they are often depicted in day-to-day \nhuman conversations. Like emotion, sentiment has also received se veral different definitions in \nthe literature. For instance, Pang et al. [12] suggest that sentiment refers to the attitudes, emotions, \nand opinions expressed in written or spoken language. In this definition, we observe that sentiment \nis one of the ways emotions are manifested. It is as if sentiment is a response generated by one or \n 22 \nmore emotions. Gross et al. [13] define sentiment as a type of affective state that involves the \nappraisal of an object, event, or situation in terms of its positive or negative valence. This definition \nin essence is closer to emotion as defined by Ortony et al. [8] where they propose that emotion is \nvalenced (positive-negative) reactions to events, agents, or objects. Osgood et al.’s [14] definition \nof sentiment also cl osely resembles this where they describe sentiment as affective reactions \npeople have towards objects, people, events, and experiences. Some researchers also use the term \nsentiment and emotion interchangeably though with the acknowledgment that they are no t \nidentical [15]. Sentiment usually connects the experienced emotion with an action [7]. Emotions \nare typically intuitive whereas a person’s sentiment towards an entity is more likely to be organized \nand targeted. On this front, the word “sentiment” is closer to the word “opinion”, which means “a \nthought or belief about something or someone ” [16]. As opposed to emotion, which can have \nmultiple dimensions (e.g., eight distinct basic emotions), sentiment is typically expressed as \nbinary: negative or posit ive, and the polar continuum that exists between them. If we express \nnegative sentiment as “ -1” and positive as “1”, the whole range of real numbers between them \nrepresents different sentiment intensity in either direction. At the center, when the value is “0”, the \nsentiment is neither positive nor negative, or in other word: neutral.  \nIn summary, emotion is a complex psychological state that encompasses subjective feelings, \nphysiological changes, and expressive behaviors. It can be triggered by internal or external stimuli \nand can vary in intensity and duration. In contrast, sentiment refers to a positive or negative \nattitude, opinion, or feeling directed toward a specific entity, such as a person, product, or event.  \nIt is often depicted as a manifestation of emotion. As discussed above, there are overlaps between \nemotion and sentiment as they both involve affective states  and human feelings, however, they \ndiffer in their scope and specificity. \n \n \n2.1.1 Sentiment Analysis \nSentiment Analysis, also commonly referred to as “Sentiment Classification”, is a special type of \nNLP task ( Figure 2). Sentiment Classification  (or text classification in gene ral) often involves \nunderstanding the meaning of the sentence rather than its syntactical structure. This is the reason \nwhy it fits well under the “semantic” sub-category within NLP’s NLU branch. \n 23 \n \nFigure 2: Sentiment Analysis within the NLP taxonomy. Adapted from [1] \nAlthough NLP has a long history of research, little research has been done on Sentiment Analysis \nbefore the year 2000. The term “ Sentiment Analysis ” first appeared in 2003 [17] though the \nresearch on the analysis of sentiment appeared earlier in 2000 [18]. Since then, it has been an active \nand growing research area. The Sentiment Analysis process identifies the sentiment expressed in \na given text [19], and then classifies it often in terms of “positive” or negative” sentiment.  It is a \ntype of text classification task, where the given text inputs are classified into different sentiment \npolarity. Dang et al. [20] defined Sentiment Analysis as “a process of extracting information about \nan entity and automatically identifying any of the subjectivities of that entity”. According to Dang \net al. [20], the objective  of Sentiment Analysis is to determine whether text generated by users \nconveys their positive, negative, or neutral opinions. Pang et al. [21] define Sentiment Analysis as \nthe “computational study of opinions, feelings, and subjectivity in text”. According to Alsaeedi et \nal. [22], “Sentiment Analysis is a means of assessing written or spoken languages to decide whether \narticulation is positive, negative or neutral and to what degree ”. They elaborate by stating that it \nalludes “to the utilization of natural language processing, text mining, computational linguistics, \nand bio measurements to methodically recognize, extr icate, evaluate, and examine emotional \nstates and subjective information”. Sentiments can be denoted as positive, negative, or neutral or \n\n 24 \nan arithmetical score conveying the strength of the sentiment, often termed as “sentiment polarity \nscore” [23]. \nSentiment Analysis  can be performed at many different levels, which researchers tend to \ncategorize into three levels:  \n• Document level, where the overall sentiment of an entire document (e.g., a news article) is \ntaken into consideration. \n• Sentence level, which concerns about the sentiment expressed in a single sentence (e.g., typical \ntextual content from social media). In this thesis, we primarily focus on this level of Sentiment \nAnalysis. \n• Entity and aspect level , which, in addition to analyzing the sentiment  expressed in a given \ntext (typically a sentence), also attempts to discover what entity the sentiment is associated \nwith [15]. \nSentiment Analysis also has various subtasks: depending on the number of sentiment classes it can \nbe binary or multi-class; it can also be a single or multi-label classification task depending on the \nnumber of sentiments expressed in any given input. There are also further variations of Sentiment \nAnalysis where the aspect of sentiment is also identified (i.e., entity and aspect l evel Sentiment \nAnalysis). For example, in a product review, aspect -oriented Sentiment Analysis  [24] can \ndetermine the sentiment as well as the “aspect” of the product that the sentiment is related to, based \non the context provided in the given review.  \nIn summary, Sentiment Analysis  is a process that often leverages computational power and \ntechniques in order to determine the sentiment expressed in a given piece of text.  \n2.1.2 Overview of Sentiment Analysis Literature \nA typical Sentiment Analysis process starts with data preprocessing. There are various \npreprocessing techniques that can be applied to the input data before the actual classification \ntask. For example, stop-words removal is a common preprocessing technique. Stop-words, such \nas, “is”, “are”, “the”, “of”, are those that are typically present in abundance in any given text but \ncarry little weight in terms of the overall meaning or sentiment of the sentence. These words are \nconsidered noise and are typically removed before performing a Sentiment Classification task. \nAnother common preprocessing technique is the tokenization of sentences into words.  \n 25 \nFigure 3 demonstrates an example of the tokenization process. Each word in a given sentence is \nseparated, which are called tokens. These individual tokens are then further processed in different \nways (e.g., lemmatization) based on the Sentiment Classification technique in use.  \n \nFigure 3: Tokenization of a sentence \nAfter the data preprocessing is done, the actual Sentiment Classification process begins. There are \ntwo primary ways to perform the Sentiment Classification task - (i) lexicon-based and (ii) machine \nlearning-based approaches [25]. Machine learning-based approaches can be further grouped into \ntwo categories: traditional machine learning and deep learning. Figure 4 represents an overview of \nthe Sentiment Analysis process encompassing various approaches. \n\n 26 \n \nFigure 4: Sentiment Analysis Process Overview \n \n2.1.2.1 Lexicon-based Approach \nIn the lexicon-based approach, the sentiment of a given text is calculated from the polarity of the \nwords or phrases in that text [26]. A lexicon (i.e. , a dictionary) of words with sentiment polarity \n\n 27 \nassigned to them is used [27]–[29]. Typically, the overall sentiment score of the text is the average \nof the computed sum of the polarities for each word found in the text. Figure 5 represents a basic \nexample of a lexicon-based Sentiment Analysis process. There is a total of nine words. Only two \nwords among them contribute to the sentiment of the overall sentence: “exciting” and “expensive” \nas they are ones present in the lexicon being used. Figure 5 shows a part of the lexicon where we \nhave these “sentiment words” along with their sentiment polarity score. By adding up the polarity \nscore of each sentiment word for a given sentence, we can find the overall sentiment of the \nsentence. In this case, for example, the sum is: +5 + ( -3) = +2. This means the overall sentiment \nof this sentence is positive.  \n \nFigure 5: Sentiment Analysis using a lexicon \nLexicon-based methods use lists of words (i.e., dictionary) marked by polarity score to determine \nthe overall opinion score of a given text input. Their main advantage compared to machine learning \nmethods is that they do not require training data. There are two common ways to build a sentiment \nlexicon: existing lexicon -based construction method that artificially creates a lexicon, and \nautomatic or semiautomatic construction method [25]. General Inquirer (GI) is considered the \nearliest emotional thesaurus and affective analysis program. It derives emotional words from \nHarvard IV-4 Dictionary [30] and Lasswell’s Dictionary [31]. GI labels each word with polarity, \nstrength, and parts of speech. These labels make it useful for Sentiment Analysis tasks. Typically, \nGI takes a pre-set list of positive and negative words and analyzes the polarity of documents based \non the frequency of words from each category. This type of lexicon -based method is time -\nconsuming and contains less vocabulary. Automatic or se miautomatic construction of sentiment \nlexicon later became more dominant. For instance, SentiWordNet [28] is a lexical resource for \n\n 28 \nSentiment Analysis, which assigns each entry of WordNet [32] three sentiment scores: positivity, \nnegativity, and objectivity. SentiStrength  [33], a lexicon -based algorithm subdivides human \nemotion into more dimensions. It estimates the strength of positive and negative sentiment in short \ntexts, even for informal language. This makes it well-suited for social media text. S entiStrength \nreports two sentiment strengths: −1 (not negative) to −5 (extremely negative), 1 (not positive) to 5 \n(extremely positive).  \nOrtega et al. [34] proposed a three-step technique for Sentiment Analysis using Twitter data. The \nfirst step was pre-processing and the second and third steps were polarity detection and rule-based \nclassification consecutively. The latter two steps leveraged WordNet and SentiWordNet. Their \napproach demonstrated promising results when evaluated on the SemEval -2013 Task-2 dataset \n[35]. Saif et al. [36] developed another lexicon -based approach for social media Sentiment \nAnalysis, particularly Twitter, named SentiCircles. By considering the patterns of words that co -\noccur in different contexts, it updated the pre -assigned scores and polarity of words in sentiment \nlexicons. Their evaluation demonstrated that the methods based on SenitCircles outperformed \nmethods based on SentiWordNet.  \nA technique to expand the list of sentiment words in a lexicon is to leverage semantic relationships \nsuch as synonyms and antonyms. A typical scenario is to define a small number of sentiment \nwords, often annotated manually, and then to enlarge this initial list by inserting words with similar \nsemantics [37]. The limitation of this approach is that the extension of the opinion information is \nrestricted and dependent on the initial list of seed words. To address this, Feng et al. [38] suggested \nconnotation lexicons enclose subtle dimensions of a word’s sentiment. After defining a list of seed \nwords, they used graph -based algorithms (e.g., PageRank [39]) to learn the connotation lexicon \ntogether with the connotative predicates. They evaluated their approach in SemEval -2007 Task-\n14 [40] demonstrating promising results.  \nIn an aspect-based Sentiment Analysis study, Salas-Zárate et al. [41] used semantic annotation to \nidentify aspects concerning diabetes in tweets. Subsequently, they used these identified aspects to \nperform aspect-based Sentiment Analysis using SentiWordNet. Their results revealed that for this \naspect-based Sentiment Analysis  task, the “N -gram around” method  (i.e., N -gram [42] words \nbefore the aspect and after the aspect)  obtained promising results.  The author noted that their \nresults could be improved by using domain-specific sentiment lexicons and by using an automatic \n 29 \nor semi-automatic approach to create ontologies in order to obtain more domain-related knowledge \nin the ontology.  \nSome researchers attempted to generate dynamic lexicons for a particular domain based on th e \ndomain data and existing static lexicons. For instance, Mowlaei et al. [24] developed two dynamic \nsentiment lexicon generation methods: one is based on the frequency and some data pre-processing \ntechniques such as negation, and the other one is based o n a genetic algorithm. Their evaluation \nsuggests that these dynamically generated lexicons, which are essentially a product of the fusion \nbetween existing static lexicons and domain-specific information from the given dataset, \nperformed significantly better than static lexicons.  \nAlthough recent  lexicon-based approaches have  seen improved efficiency in Sentiment \nClassification, it still falls short in several areas. For instance, it cannot detect linguistic nuances \nsuch as irony or sarcasm. It typically fai ls or underperforms in capturing the context of the \nlanguage and it is significantly outperformed by machine learning approaches [25].  \n2.1.2.2 Machine Learning-based Approach \nMachine learning algorithms are used for many NLP tasks including Sentiment Analysis. The most \ncommon machine learning approach used for Sentiment Analysis on textual data is supervised \nlearning. By using  a large number of  labeled input data for training, supervised learning -based \nmodels can classify the sentiment of a given text. Go et al. [43] conducted one of the first studies \nto utilize a supervised machine learning-based approach to analyze sentiment in social media texts. \nThey classified the tweets as either positive or negative. To avoid manual tagging of sentiment, \nthey used distant supervision (i.e., using existing labeled data or knowledge to automatically label \nlarge amounts of unlabeled data for a given task ) to build a machine -learning classifier as \ndemonstrated in [44]. They differentiated negative and positive tweets with e moticons within the \nTweets. They developed a large training data set consisting of 1,600,000 tweets. They inspected \nNB, MaxEnt, and SVM classifiers on this training data. Their choice of classifier algorithms was \ninspired by Pang et al.’s [45] work who performed Sentiment Analysis of movie reviews. Bigrams \n[42], unigrams [42],  and POS tags were used as features. Their evaluations suggest that NB with \nbigrams as features outperformed other classifiers and achieved an accuracy of 82.7%. They also \nconcluded that adding negation as an explicit feature with unigrams and using POS ta gs are not \nuseful for polarity classification.  \n 30 \nPak and Paroubek [46] also used emoticons as labels to annotate about 300,000 tweets but as a \nnon-binary multiclass classification task by classifying the tweets as positive, negative, or neutral. \nThey evaluated the performance of SVM, MNB (i.e., Multinominal NB), and Conditional Random \nField (CRF) [47] using various features including unigrams, bigrams, n-grams, and the position of \nn-grams. MNB with n-grams and POS tags demonstrated the best result, with an observed increase \nin performance with more training data. Another contribution of the work included the \nidentification of the co -occurrence of the pronoun for the first person and the adjectives in the \nopinionated messages. Though they included a neutral category, which is an improvement over \nGo et al’s [43] work, labeling their training dataset followed a similar procedure as Go et al. which \nheavily relied on emoticons. Since emoticons may not always represent the actual sentiment \nexpressed in a tweet, it is likely to generate a considerable amount of inappropriately labeled data.  \nBarbosa and Feng [48] presented a two -step classifier. The first step determined whether the \nmessage was opinionated or not. The second step aimed to further classify the sentiment of the \ntweet as positive or negative. They created a training dataset with 200,000 tweets annotated \nautomatically with multiple sentiment detection tools. In their experiment, the SVM classifier \ndemonstrated the best result with an accuracy of 81 .9% for subjectivity detection and 81.3% for \nsentiment polarity detection. One of the limitations of this study, as acknowledged by the authors, \nwas that the classifier was not robust when analyzing sentences with antagonistic sentiments. \nJiang et al. [49] experimented with including target -dependent features along with usual target -\nindependent features in their Sentiment Analysis work. The author provided an example sentence \nto explain the importance of target dependency in the following tweet – “Windows 7 is much better \nthan Vista!”. In this tweet, though the sentiment is positive towards Windows 7, it is negative \ntowards Vista; and a typical target-independent classifier ignores this fact and is likely to classify \nthe tweet as positive. The authors manu ally defined rules to detect the syntactic patterns that \ndetermined a term’s relation to a specific object. They used many features from Twitter such as \nretweets, replies, and mentions to create a graph that reflects the similarities of tweets. They used \nSVM for subjectivity and polarity classification. Their experiment concluded that utilizing target \nidentification improved the accuracy of Sentiment Classification, achieving an accuracy of 85%. \nMore recently, a group of researchers proposed a method to improve SVM classification accuracy \nby sub -setting training data using clustering [50]. The clustering -based approach involved an \n 31 \ninstance selection method using data points with maximum, minimum, and average distances to \neach cluster center. Their results showed the higher accuracy of the applied method compared to \nearlier work [51] by the same authors which did not utilize this clustering method to subset training \ndata.  \nBania et al. [52] evaluated the performance of four traditional non -parametric machine learning \nmodels [53] while analyzing public sentiment expressed in tweets regarding the Covid-19 \npandemic [54]. The learning algorithms included Naïve Bayes (NB), Random Forest (RF), and \nSupport Vector Machine (SVM). Two variations of the NB: Gaus sian-NB (G -NB), and \nBernoulli’s-NB (B-NB). They leveraged the TF-IDF feature extraction scheme with unigram, bi-\ngram, and tri-gram techniques. Their result suggests that RF and B -NB models outperform the \nother two models. They also revealed that linear-SVM based models had higher computation costs. \nThough this study did not propose a new technique, it indicates the generalizability of Sentiment \nAnalysis techniques across domains, especially when dealing with similar data sources. This \ngeneralizability, while desirable, may not always contribute to the expected accuracy. The authors \nalso considered the fact that many of the tweets used in the experiment might be coming from bots \nor fake accounts, especially since the study analyzed tweets regarding a globally trending topic on \nTwitter. \nSome researchers attempted to apply unsupervised learning algorithms in classifying sentiment. \nThese algorithms are used t o analyze and cluster un labeled datasets, which discover hidden \npatterns or data groupings without the need for human intervention or data labels. Some \nresearchers have attempted to apply it to Sentiment Analysis . For instance, Riaz et al. [55] \nperformed Sentiment Analysis on the customer review data using unsupervised learning. Their \nanalysis was at the phrase level which aimed to understand customer preference by analyzing \nsubjective expressions. The researchers calculated the strength of sentiment word s to determine \nthe intensity of each expression. They applied k -means clustering for categorizing the words in \nvarious clusters based on their intensity. Fully unsupervised methods like this typically suffer from \nlow accuracy in classification tasks simila r to Sentiment Analysis . However, using only a few \nlabeled data, the semi-supervised learning method, which is a sub-class of supervised learning can \nachieve reasonably better accuracy in Sentiment Classification . This often includes generating \n“pseudo labels” for the un labeled training data in iterations, primarily based on the few initial \nlabeled data. For instance, Iosifidis et al. [56] leveraged a semi-supervised approach to annotate \n 32 \nlarge datasets with sentiment labels using techniques such as self -learning and co-training. They \nobserved that, unlike co -training, self -learning propagates the original class imbalance to \nsuccessive iterations. They noticed that in initial iterations with a very low number of labels, co -\ntraining performed better than sel f-learning; however, in later iterations, self -learning improves \nfaster than the co-training technique. \nOne study demonstrated how lexicon and semi-supervised methods can be combined to offer better \nSentiment Classification [57]. They integrated several sentiment lexicons to generate a unified \nsolution that provides a high-quality lexicon. For semi-supervised classification, they generated a \nhigh-quality data set that is smaller than the one used in traditional supervised learning. They \nleveraged confidence levels to fine-tune their classifier. Eventually, they developed an architecture \nthat combines their high -quality lexicons and high -quality training dataset in semi-supervised \nalgorithms, which demonstrated comparable performance with fully supervised methods.  \nAn issue with supervised machine learning techniques to perform Sentiment Analysis is that they \nrequire a large amount of training data. We observe in the literature that some researchers have \nattempted to mitigate this issue by incorporating semi -supervised methods. We also notice that \nthere is a growing number of studies that combine various algorithms to form one solution, as well \nas solutions that combine machine learning -based approaches with lexicon -based ones. More \nrecently, deep learning -based methods to perform Sentiment Analysis have garnered popularity \namong researchers.  \nDeep learning is one of the fastest -growing sub-domains of machine learning. It has been seeing \nextensive usage in solving perceptual problems such as image recognition and understanding \nnatural languages. Deep learning uses neural networks to learn many levels of abstraction. In text-\nrelated tasks, deep -learning approaches typically include two steps. First, they learn word \nembeddings from the text collection, and these are then applied to produce the representations of \nthe documents.  Tang et al. [58] and Zhang and Zheng [59] are among the first research groups to \napply deep learning approaches for Sentiment Analysis. Both groups experimented with part of \nspeech (POS) as a text feature and TF-IDF to calculate the weight of words for the analysis. Zhang \net al. also compared Extreme Learning Machine (ELM) (with kernels) [60] with SVM and found \nthat ELM significantly outperformed SVM in terms of classification accuracy. Since then, several \nstudies applied deep -learning-based Sentiment Analysis in different domains, including finance \n 33 \n[61], weather-related tweets [62], trip advisors [63], and movie reviews [64], [65]. In many of \nthese studies [61], [62] , input data was transformed into word embedding using tools such as \nWord2Vec [66] in order to classify the sentiment of text using deep learning algorithms. \nA group of researchers [67] proposed combining sentiment and semantic features in a Long Short-\nTerm Memory (LSTM) model (SS-LSTM) based on emotion detection. They detected emotions \nsuch as happy, sad , and angry in conversational pairs on Twitter. They applied semi -automated \ntechniques to gather a dataset containing over 17 million tweets. Their approach involved \ncombining sentiment and semantic features from user utterances using Sentiment Specific Word \nEmbedding (SSWE) [68] and GloVe embeddings [69] respectively without requiring any hand -\ncrafted features. Their results suggest that the proposed SS -LSTM model outperforms traditional \nmachine learning baselines as well as other off-the-shelf deep learning models.  \nLi et al. [70] studied the impact of data quali ty on Sentiment Classification  performance by \nconsidering three criteria: informativeness, readability, and subjectivity. Their dataset consisted of \nonline product reviews, and they applied three deep learning techniques to this data: Simple \nRecurrent Network (SRN), LSTM, and CNN. Their evaluation suggested that two factors affect \nthe level of accuracy of Sentiment Analysis : readability and length of the reviews. Higher \nreadability and shorter text datasets yielded higher-quality Sentiment Classification.  \nDang et al. [20] applied deep learning models (DNN, RNN, CNN) with TF -IDF and word \nembedding to Twitter datasets and implemented a Sentiment Analysis technique with a special \nfocus on the problem of sentiment polarity analysis. The results suggest that combining deep \nlearning techniques with word embedding than with TF -IDF when performing a Sentiment \nAnalysis can yield better results. Their experiments revealed that CNN outperforms other models \nconsidering both accuracy and processing time. Though RNN ’s reliability is slightly higher than \nCNN with most datasets, it requires significantly more processing time. This is because RNNs are \ndesigned to handle sequential data by processing one element at a time while maintaining a hidden \nstate. This sequential processing can result in higher accuracy for certain text classification tasks, \nas RNNs are able to capture contextual information and long -term dependencies. However, this \nsequential processing also makes RNNs more computationally expensive, as each element in the \nsequence must be processed sequentially. This can result in longer processing times, especially for \n 34 \nlonger sequences or larger datasets. On the other hand , CNNs can process multiple elements in \nparallel, making them faster and more efficient for certain types of text classification tasks. \nJebbara et al. [71] leveraged transfer learning while experimenting with cross -lingual Sentiment \nAnalysis where the fo cus was on supporting Sentiment Analysis  in languages that are under -\nresourced in terms of training data. For this approach, they leveraged zero -shot transfer learning \nusing pretrained multilingual word embeddings. While their best achieved F1-score remained low \n(under 0.60), they have demonstrated performance gains over other approaches attempting to \naddress the same problem.  \nSimilar to other applications of machine learning, researchers have experimented with combining \nmachine learning and deep learning approaches within one architecture in order to build a more \nperformant Sentiment Classification  method. For instance, Araque et al. [72] created a deep \nlearning-based sentiment classifier using a word-embedding model and a linear machine learning \nalgorithm which outperformed both individual classifiers.  \nAnother study produced a performant deep learning model for Sentiment Analysis which is based \non an attention-based bi -directional CNN -RNN deep network  that leverages two independent \nLSTM and (Gated Recurrent Unit) GRU layers [73]. They used GloVe word embedding vectors \nas the initial weights of the embedding layer. On top of that, the bidirectional LSTM and GRU \nnetworks were used to extract both past and future contexts. The final layer included a typic al \ndense fully connected layer with a Sigmoid activation function to transform the vector into \nsentiment representation in order to perform the binary sentiment polarity classification.  \nDeep learning-based methods for Sentiment Analysis have seen increasing popularity in the last \nfew years. There is also the trend of combining multiple algorithms and approaches to achieve \nbetter classification accuracy, including a combination of traditional machine learning and deep \nlearning-based techniques. More recentl y, there has been a number of Sentiment Analysis work \nthat leveraged transfer learning, especially in scenarios where the number of labeled training data \nwas limited. \n2.2 Emotion Cause Analysis (ECA) \nSentiment Analysis, which we discuss in the previous section, primarily focuses on categorizing a \ngiven text based on sentiment polarity . In contrast, ECA tasks focus on the cause behind the \n 35 \nemotion – it concerns with the question of why a person is experiencing a particular emotion in a \ngiven context. One of the earliest works in this area refers to the event, person, or state that elicits \nan emotional response in the experiencer as the \"emotion stimulus\" [7]. According to this study, \nan “experiencer” has a particular emotional state which is often described by the stimulus that \nelicits it. There can also be a circumstance under which the emotional response occurs or a reason \nwhy the stimulus elicits the particular emotional response in the experiencer. For example, in the \nsentence, “Rizvee is very happy to get the latest smartphone as a gift”, \"Rizvee\" is the experiencer, \n\"happy\" is the emotion, and \"latest smartphone as a gift\" is the emotion stimulus. Ghazi et al. [9] \nobserve that among the elements of emotion [7], the emotion stimulus can be characterized as the \n“cause” behind the emotion felt by the experiencer. Learning about the “cause” behind ce rtain \nexpressed emotions is of vital value in many real -life scenarios such as getting a deeper \nunderstanding of consumer behavior, and the root cause of public opinion on certain matters. \nRecognition of this inspired many research works that focus on dete rmining the cause of the \nemotion in various ways, which form a particular domain within NLP, known as Emotion-Cause \nAnalysis (ECA).  \nThere are various subtasks within ECA. The primary one is Emotion cause extraction (ECE), \nwhich is the earliest and most popular ECA task investigated by researchers so far.  \nThe ECE task involves finding the potential causes that lead to expressed emotions in text. A \nsimple representation of this task is to formulate it as a binary classification problem at the clause \nlevel [74]. The aim is to determine, for each clause in a document, whether it is the cause of the \nannotated emotion. In the ECE task, the input is a document and the annotated emotion, and the \ngoal is to extract the cause clause that corresponds to the given emotion. \nEmotion-Cause Pair Extraction (ECPE) is a newer variation of the ECE task, which focuses on \nfinding all possible pairs of emotions and their corresponding causes within a document [75]. \nUnlike ECE, the output of the ECPE task is a pair of emotion -cause, without the need for prior \nannotated emotions. Emotion-Cause Span-Pair extraction and classification (ECSP),  is another \nvariation of the ECE task that builds upon ECPE [76]. The goal of this task  is to extract the \npotential span-pair of emotions and their corresponding causes in a document and to classify the \nemotions for each pair. Thus, Emotion Cause Extraction (ECE) and Emotion -Cause Pair \nExtraction (ECPE) can be seen as two specific cases of ECSP at the clause level [76]. \n 36 \nFigure 6 shows an intuitive example of these ECA tasks. For a given set of clauses contained \nwithin one single datapoint, the task of ECE is to identify the clause that contains the emotion -\ncause. In Figure 6, the ECE task thus aims to identify clause 3 while the emotion “happy” is given. \nIn contrast, the ECPE task identifies the clause that contains the expression of emotion as well as \nthe cause of the expressed emotion, without any given annotated emotion. In this case, it is the \nidentification of the pair (clause 4, clause 3). For the ECSP task, the aim is to identify the emotion \nexpression and emotion cause at a more granular span level instead of the entire clause. As a result, \nthe expected output would be to extract the specific spans within clause 3 and clause 4 (highlighted \nin bold in Figure 6 for clarity), as well as the classification of the emotion (i.e., “happy” in this \nexample).  \n \nFigure 6: Different ECA Tasks \n2.2.1 Overview of Emotion-Cause Analysis Literature \nECA is relatively a newer research area compared to Sentiment Analysis . With a few initial \nexceptions, most of the ECA-related tasks are performed with machine learning techniques. There \nare a few rule -based techniques for th e ECE task. Apart from these, most other ECA subtasks \nleverage traditional machine learning technologies, and more recently, deep learning techniques.  \n2.2.1.1 Rule-based Approach \nThe first ECA task was proposed by Lee et al. [77] where they manually created a small corpus of \nemotion causes, using the Academia Sinica Balanced Chinese Corpus. They introduced the \nemotion cause extraction (ECE) task, which was defined as a word -level sequence labeling \n\n 37 \nproblem, with both emotion expression and em otion cause annotated. Some other early studies \nalso explored different rule-based approaches to extract emotion causes from a given text. For \nexample, Li and Xu [78] created an emotion cause corpus leveraging Chinese microblog posts and \nproposed a rule-based method to extract emotion causes by incorporating knowledge from other \nfields such as sociology. Leveraging the same corpus, Gao et al. [79], [80] then designed a set of \ncomplex rules considering a cognitive emotion model and emotions categories to extract emotion \ncauses (some other rule-based examples can be found at [81], [82]). In its most basic form, these \nrule-based methods for the ECE tasks concentrate on detecting explicit emotions expressed \nthrough emotion keywords. They annotated each emotion keyword with its corresponding cause. \nThese studies primarily considered that most emotion causes can be found within the same clause \nas the expression of emotion. Based on this, they conclude that a clause may be the most suitable \nunit for detecting a cause. The clauses were identified through punctuation such as commas, \nperiods, question marks, and exclamation marks. The authors utilized linguistic cues such as \ncausative verbs and perception verbs to create patterns for extracting general cause expressions or \nspecific constructions for emotion causes. They formalized the emotion cause detection as a multi-\nlabel classification problem, where each instance could have more than one label, such as \"left -1, \nleft-0\", to indicate the location of the clauses that are part of the cause. The method also involved \nkeyword spotting to identify the emotion word in the sentence and find its cause.  \nIn later years, more advanced ECA systems have been developed, employing Conditional Random \nFields (CRFs) as a probabili stic framework for identifying the cause of emotions. These systems \nhave been trained on annotated datasets containing cause-spans, resulting in accurate identification \nof emotional causes  [9]. Recent research studies in the ECA domain leverage machine l earning \ntechniques, primarily using supervised learning. The outcome of the task can vary based on the \ntype of ECA subtasks. This difference in outcome is also reflected in the type of machine learning \ntechniques and structure of the training dataset used in the study. We discuss them below. \n2.2.1.2 Machine-learning based Approach \nGui et al.  [9], [83] built a microblog emotion cause corpus based on the NLPCC 2013 emotion \nanalysis task and proposed a machine learning method using SVMs and CRFs to extract emotion \ncauses. These studies regarded the ECE task as a sequence labeling problem. Gui et al. later \nreleased a Chinese emotion cause corpus from a public SINA city news and proposed a multi -\n 38 \nkernel-based method for emotion cause extraction [84]. Unlike previous corpora, the ECE task in \nthis corpus was defined as a clause classification problem, where the goal is to predict if a clause \nin a document is an emotion cause. It was evaluated using clause -level precision, recall, and F1 \nscore metrics, and has  since become a benchmark dataset for ECE research.  Several other \ntechniques for ECE were approached based on the traditional machine learning approach (e.g. \n[85], [74]).  \nIn recent years, deep learning techniques have also been applied to perform the ECE task, including \nLSTM [86], deep memory network [87] etc. Most of these studies treated the ECE task as a set of \nindependent clause classification tasks, ignoring the relationships between clauses in a document. \nTo address this, Ding et al. [88] converted the task to a reordered clause classification problem, \nwhere predictions of previous clauses were used as features for predicting subsequent clauses. In \ncontrast, another study [89] proposed a joint emotion cause extraction framework that models and \nclassifies multiple clauses in a document simultaneously  leveraging Transformer as the clause -\nlevel encoder to model the relationships between multiple clauses. \nRecently, researchers suggested that the ECE task has two limitations : (1) it needs the emotion \nannotation, and (2) it does not take into account the mutual connection between the emotion clause \nand cause clause [90]. In order to address these limitations, emotion–cause pair extraction (ECPE) \ntask was proposed , which extracts both the emotion clause and cause clause and devises an \nemotion–cause pair [75]. This, however, conducts the entire process in a two -step mechanism. \nFirst, it extracts all clauses – potential clauses containing the emotion expression and clauses \ncontaining th e cause of the expressed emotion. Second, it matches  these clauses to form the \nemotion–cause pair. This type of two -step method generally causes error propagation from the \nfirst step to the second. To resolve this, a group of researchers proposed a unified architecture that \nextracts the emotion-cause clause pair using a ranking system [91]. Some subsequent work treats \nthe ECPE task as a sequence labeling problem. For instance, one study treated the relative distance \nbetween the emotion clause and cause clause as a part of the label [92]. Another study reformulated \nthe ECPE task as a unified sequence labeling task which extracts all possible pairs of emotion -\ncause [93]. An interesting recent approach tackled the ECPE task as a dual question -answer task \n[94], leveraging the attention network. It considers the emotions and causes as two separate \nquestions and extracts answers from the given context separately leveraging the attention network \n[95]. Another study proposed a novel technique by incorporati ng the distances between the \n 39 \nemotion and cause clause into a novel tagging scheme [96]. They perform the ECPE task by \nadjusting the paired tagging distribution based on the estimated distribution of the auxiliary tasks.  \nInspired by recent success in the  span-based models for co-reference resolution  and syntactic \nparsing [97], [98], Bi et al. [76] proposed a new task within ECA, named Emotion-Cause Span-\nPair extraction and classification (ECSP) . This is also a variation of the original ECE task with \nmore advanced requirements. The objective of this task  is to extract the potential span -pair of \nemotion and related causes in a given document and classify emotion for each pair . This work \nfocuses on finer granularity in terms of detecting the actual cause of the emotion, compared to the \nprevious works where the emotion clauses were detected, and these clauses often contained \nunrelated information. The main idea is to annotate each emotion and cause with its span boundary \nand emotion categorization. This annotation leads to the creation of a span -based extract-then-\nclassify model, which directly pairs emotions and causes from the document while following the \ntarget span boundaries . The categorization is then determined through the use of pair \nrepresentations and localized context. This method is advantageous as it allows for a consistent \ninterpretation of clause-based and span-based tasks. Furthermore, the polarity is determined based \non the targeted span representation, ensuring that all target words are taken into account before \nmaking predictions and avoiding sentiment inconsistencies. \nCompared to Sentiment Analysis, ECA is a relatively new research area that initially relied on \nrule-based techniques and slowly evolved into an NLP domain that primarily leverages supervised \nmachine learning techniques. A variety of subtasks have been included in this domain over the \nyears, which all primarily focus on emotion classification and ext raction of the emotion-cause in \na given text.  \n2.3 An Overview of Related Machine Learning Technologies \nIn this section, we offer a brief overview of machine learning techniques and concepts that are \nrelevant to this thesis.  \n2.3.1 Representation of Text in Machine Learning \nMachine learning algorithms typically process data in the form of vectors. Translating textual data \nto vectors may not be always straightforward and how it is done impacts the overall outcome of \n 40 \nthe machine learning algorithm. We discuss some of the common techniques for representing \ntextual data in machine learning algorithms below: \nOne-hot vector: A common technique to represent text is using a sequence of discrete tokens \nwhere each of these tokens is denoted by a one-hot vector. One-hot vector is a large sparse vector \nwith a single dimension for each word in the overall data. This representati on method is popular \nfor categorical data such as the ones used for Sentiment Analysis (i.e., different sentiment polarity \nrepresenting each category).  \nBag-of-Words: Bag-of-Words (BoW) is a similar concept to the one-hot vector representation of \ntexts but with the addition of frequency (of words) in the representation whereas one -hot vector \nonly contains information about the presence (1) and absence (0) of words. Let us consider the \nfollowing three texts as an example: \n• Text 1: The weather is bad. \n• Text 2: The weather is good. \n• Text 3: The weather is unpredictable today, but the afternoon is sunny. \nWe first develop a vocabulary consisting of all the unique words in the above texts: ‘The’, \n‘weather’, ‘is’, ‘bad’, ‘good’, ‘unpredictable’, ‘today’, ‘but’, ‘after noon’, ‘sunny’. This is our \nBoW. Now we can represent our three texts using BoW in the following manner: \n The weathe\nr \nis bad good unpredictabl\ne \ntoday but afternoon sunny Word \ncount \nText 1 1 1 1 1 0 0 0 0 0 0 4 \nText 2 1 1 1 0 1 0 0 0 0 0 4 \nText 3 2 1 2 0 0 1 1 1 1 1 10 \nTable  1: Vectorization Example \nBased on the above table, the following are the vector representations for our texts using the BoW \napproach: \n• Text 1: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0] \n• Text 2: [1, 1, 1, 0, 1, 0, 0, 0, 0, 0] \n• Text 3: [2, 1, 2, 0, 0, 1, 1, 1, 1, 1] \nThese vector representations are now ready to be used by machine learning algorithms. However, \nsince this method requires having a separate dimension for every single word in BoW, the one-hot \n 41 \nvector representation can become extremely large. This may lead to a computationally unplausible \nsituation when the text in question contains hundreds or thousands of words. Some researchers try \nto put a “fixed” size to the BoW in order to reduce the vector size for each text. However, this \ncreates the risk of losing important information that may be of significance for Sentiment Analysis. \nAnother issue with one-hot vectors, in general, is that any two words are orthogonal regardless of \nthe similarity of words, lacking important information regarding the meaning of associated words.  \nTF-IDF: Some of the limitations of BoW can be avoided by using Term Frequency -Inverse \nDocument Frequency (TF-IDF). It is a numerical statistic that is intended to reflect how important \na word is to a document in a collection or corpus  [99].  Term Frequency (TF) measures how \nfrequently a term occurs in a document. TF is often divided by the document length (i.e., the total \nnumber of terms in the document) in order to perform normalization to account for the varying \ndocument sizes. On the other hand, Inverse Document Frequency (IDF) measures how important \na term is. While computing TF, all terms are considered equally important. However, many words \nsuch as “is”, “of”, “this” contribute little weight to the meaning while appearing frequently in any \ngiven text. IDF measure weighs down the frequent terms and scales up the rare ones. \nConsider Text 3 in our above example which contains 10 words. The term “is” appears 2 times \nthere. So, the TF measure for the term “is” is: 2/10 = 0.2. We have a total of 3 texts in the above \nexample (Text 1, Text 2, and Text 3), and “is” appears in all of them. This means our IDF count \nwould be: 3 / 3 = 1. Consequently, the overall TF-IDF measure if the product of TF and IDF: 0.2 \n* 1 = 0.2 \nWord Embeddings: Word embeddings (also known as “word vectors”) is another technique for \ntextual representations, typically leveraged by deep learning approaches . Words in word \nembeddings are represented with dense vectors in a latent space that better captures the meaning \nof words [71].  In word embeddings, similar words have a similar encoding. Importantly, we do \nnot need to specify this encoding by hand. It i s typically a dense vector of floating -point values \nwhere the length of the vector is a parameter that can be specified [100]. The values for embeddings \nare trainable parameters, meaning they are learned by the model during training. A higher \ndimension w ord embedding can contain a fine -grained relationship between words. There are \nvarious techniques to generate word embeddings. One example is Continuous Bag of Word \n(CBOW). In CBOW, the context of a word is represented by multiple words for a given target \n 42 \nword. For example, let us consider text 3 again: “Text 3: The weather is unpredictable today, but \nthe afternoon is sunny”. Here, we can use “unpredictable” and “sunny” as context words for \n“weather” as the target word. These learned relationships of target  words and context words are \nused by CBOW [101] to create word embeddings that are rich in the syntactical meaning of words \nin a given context. Another technique is Skip-gram [101], which is the opposite of CBOW. Skip-\ngram technique attempts to predict the “context words” (w(t-2)…..w(t+2) in Figure 7) for a given \n“word” (depicted as w(t) in Figure 7). For this algorithm, the input is one word, and the outputs \nare the associated context words of that one word. For our Text 3 example above, Skip-gram will \noutput “unpredictable”, and “sunny” as context words for the term “weather” as input. Google \nWord2vec [66] algorithm uses either CBOW or Skip-gram in order to produce word embeddings.  \n \nFigure 7: CBOW and Skip-gram [101] \nCommon word embeddings such as Word2vec are created with shallow neural networks (i.e., \nneural networks containing only 1 or 2 hidden layers. See [102] for more details.). Typically, this \nneural network will take one-hot vector representations of words (i.e., sparse matrix) and then use \nit for unsupervised training of a neural network. Instead of using the output of the neural network, \nthe weight matrix of the fully connected dense hidden layer is stripped out of the model. This layer \nrepresents the one-hot vector encoding of words as a dense vector of floating points, which is what \nis used as word embeddings for the word that one-hot vector input represents.  \nWord embeddings overcome many common problems of one -hot vector and similar frequency -\nbased vecto r encodings such as high computational cost and lack of semantic relationship \n\n 43 \ninformation of words within a context. By preserving rich contextual information, word \nembeddings can boost the performance and accuracy of many NLP tasks such as Sentiment \nAnalysis, especially for deep neural network algorithms-based solutions.  \n2.3.2 Machine Learning Techniques \nArtificial Intelligence (AI) enables computers to mimic human intelligence. Machine learning is a \nsubset of AI that enables machines to use the experience to i mprove at specific tasks [103]. The \nlearning process generally includes feeding data into an algorithm, using the data to train a model, \nevaluating and iteratively improving the model, and then finally using the model for the desired \ntask. Data used for machine learning is typically divided into training and test dataset. A common \ntraining and test data ratio is 80:20. Specific portion of training datasets sometimes includes a third \nset, called a validation set, which is typically used for cross-validating the machine learning models \nand adjusting model parameters to find their best values. This step is also known as hyperparameter \ntuning [104]. \nMachine learning approaches are commonly divided into two groups: “supervised” and \n“unsupervised”. In a supervised learning model, the algorithm learns on a large amount of labeled \ntraining datasets. On the other hand, in an unsupervised model, unlabeled data is provided that \nthe algorithm attempts to understand by extracting features and patterns on its own [105]. For \nclassification tasks like Sentiment Analysis  and Emotion-Cause Analysis , supervised learning \nmethods are more commonly used. Traditional  supervised machine learning approach typically \ninvolves analyzing previously labeled data, extracting features that model the differences between \ndifferent classes, and inferring a function that  can be used for classifying unlabeled data. \nSupervised learning also has a subset called “ semi-supervised learning”. It combines a small \namount of labeled data with a large amount of unlabeled data during the training process. Semi -\nsupervised learning falls between unsupervised learning (with no labeled training data) and \nsupervised learning (with only labeled training data) [106]. Unlabeled data , when used in \nconjunction with a small amount of labeled data, can produce considerable improvement in \nlearning accuracy. Labeled training data scarcity is a common issue for machine learning projects \nas labeling a large amount of data can be expensive and tedious. This is especially true for domain-\nspecific tasks such as Sentiment Analysis . Semi -supervised learning algorithms have been \nhistorically proven useful in such scenarios (e.g., [107]).  \n 44 \nRecently, deep learning, which is a subset of machine learning, is gaining rapid adaption for many \nNLP tasks. There are several fundamental differences in how a typical machine learning versus a \ndeep learning mechanism work. For instance, machine learning algorithms typically divide the \nlearning processes into smaller steps and finally combine the results from each step into one output. \nWhereas in deep learning, the learning process resolves the problem on an end -to-end basis. For \ntypical machine learning algorithms, external processes identify and create the requ ired features, \nwhereas deep learning algorithms learn high -level features from data and create new features by \nthemselves [103]. Figure 8 shows the differences betwee n the two approaches  using an example \nNLP task . Both can go through some common preprocessing techniques such as tokenization, \nstemming, and stop -word removal. However, the steps after that are different for traditional \nmachine learning and deep learning. As we have discussed in section 2.3.1, various vectorization \ntechniques are commonly used for machine learning while deep learning techniques leverage word \nembeddings.  \n \nFigure 8: Machine learning and deep learning process [20] \nAfter the data preprocessing is complete, the preprocessed training data  is fed into the learning \nalgorithms for training. For traditional machine learning, popular learning algorithms include  \nLogistic Regression  (LR) [108], Support Vector Machines (SVMs) [109], Maximum Entropy \n(MaxEnt) [110] and Naive Bayes (NB) [111]. In the case of deep learning, it is typically a variation \n\n 45 \nof different Artificial N eural Networks (ANN). There are various types of neural networks such \nas Deep Neural Networks (DNN), Convolutional Neural Networks (CNN), Recurrent Neural \nNetworks (RNN), and Long -Short Term Memory (LSTM) Neural Networks [112]. The neural  \nnetwork attempts to mimic the way the biological neurons in the human brain signal to one another \n[20]. These networks are made up of layers, containing an input layer, one or more hidden layers, \nand an output layer  (see Figure 9). Each node (i.e. , artificial neuron, depicted as hn in Figure 9) \nconnects to another and has an associated weight and threshold. The node is activated  based on \nthe activation function [113]. For instance, it can be activated  when the output of any individual \nnode is above the specified threshold value. Then it sends data to the next layer of the network. \nOtherwise, no data is passed along to the next layer of the network.  At a high level, in a typical \nsupervised learning setting, a neural network processes the inputs and compares its resulting \noutputs against the desired outputs. Errors are then propagated back through t he layers, causing \nthe nodes to adjust the weights which control the network. This process occurs over and over as \nthe weights are continually tweaked  [114]. At the end of this process, we get the trained model \nthat can be used for various NLP tasks. \n \n \nFigure 9: Deep Neural Network [20] \nThe landscape of deep learning technologies continues to evolve, with the introduction of \nincreasingly sophisticated models occurring on a regular basis . We provide a few relevant \nexamples of some recent deep learning technologies below: \n\n 46 \nGenerative Adversarial Network (GAN): GAN consists of two components: generator and \ndiscriminator. During training, the generator uses random noise to create new synthetic data that \nclosely resembles real data [103]. The discriminator uses this output from the generator as input \nand leverages real data to determine whether the generated data is real or synthetic. Here the \ngenerator and the discriminator are competing with each other in an adversarial setting, hence the \nnaming. The output from the discriminator is fed back to the network to improve the respective \ngoals of both components.  \n \nFigure 10: A typical GAN architecture [115] \nTransfer learning: Transfer learning aims to store knowledge gained while solving one problem \nand then apply it to a different but related problem [116]. Deep learning typically requires a large \namount of training data, high -end computing resources, and a long training duration. Transfer \nlearning is hence beneficial as it can alleviate all these requirements when used appropriately. The \nfirst set of layers in deep neural networks usually contains lower-level features which are more \ngeneric, whereas the final set of layers contains higher-level features that are closer to the specific \ndomain or task in question. This final layer can be repurposed to fit a new domain or task. This \nallows for saving significant resources while training the new model [103].  \n\n 47 \n \nFigure 11: Conventional learning vs transfer learning [117] \nFor instance, when a model is trained to perform Emotion-Cause Analysis  spanning multiple \ndomains of topics such as sports and movies; in a conventional learning process, a new machine \nlearning model is trained on both. In the transfer learning par adigm, if there is an existing model \nthat has been trained on Emotion-Cause Analysis  for the sports domain, the new model can \nleverage the knowledge (see Figure 11) of that model without retraining itself on that domain, and \nonly train on the “movies” domain. Thus, transfer learning can significantly reduce training time \nand cost. \nTransformers: Transformer is an encoder-decoder based neural network that introduced the usage \nof self -attention to perform tasks involving sequence [95]. Encoders consist of two major \ncomponents: a self -attention mechanism and a feed -forward neural network. In deep learning, \nattention refers to attending to different parts of another sequence  (i.e., sequence of texts \nrepresented as vectors) typically, output sequence and connecting it back to the input sequence \n[118]. In contrast, self-attention refers to attending to different parts of the same input sequence, \nfacilitating learning about itself [95]. The self-attention mechanism within the encoder takes in a \nset of input encodings from the previous encoder and weighs their relevance to each other to \ngenerate a set of enriched encodings [119]. In simpler terms, self-attention can help understand the \nrelevance of words to other words in a sentence, creating an overall understanding of the likelihood \nof different words that may appear together. The feed -forward neural network then further \nprocesses each enriched encoding individually. On the  contrary, each decoder consists of three \n 48 \nmajor components: a self-attention mechanism, an attention mechanism over the encodings, and a \nfeed-forward neural network. The decoder functions in a similar fashion to the encoder, but an \nadditional attention mec hanism is inserted which instead draws relevant information from the \nencodings generated by the encoders. Figure 12 demonstrates the Transformer architecture.  \n \nFigure 12: Transformer Architecture [95] \n \nIn contrast with any encoder-decoder architecture (which existed before Transformer, i.e., [118]), \nthe main novelty of Transformers is that it introduces self -attention to perform tasks involving \nsequences, which were typically done by using an RNN -based system with a costly training \nprocess [95]. Though the original Transformer used the encoder -decoder combination, there are \nmany models that are based on either the encoder or the decoder but are still referred to as \nTransformer-based models as they leverage the encoder or decoder in a very similar manner \nproposed in the original Transformer [95]. For example, Bidirectional Encoder Representations \nfrom Transformers (BERT) [120] is an encoder-only Transformer based model while Generative \nPre-trained Transformer (GPT) [121] is a decoder-only Transformer-based model.  \n\n 49 \nLanguage Models : Large-scale pre-trained models (e.g. , [122]) have been a key factor in the \nrecent advancements in computer vision (CV). This encouraged the NLP community to adopt the \nsame approach. Starting with the acceleration of convergence speed in NLP models by initializing \nthe word embeddings with pre-trained ones, such as Word2Vec [123] and GloVe [69]. However, \nthese pre -trained word embeddings  lack the understanding of the  context-dependent nature of \nlanguages. To address this, some researchers attempted to use pre-trained RNN models as the \nbackbone for various text classification tasks [124]–[126]. These efforts laid the foundation for the \ndevelopment of large pre -trained language models (LLMs). Building upon these , Devlin et al. \n[120] and Radford et al. [127] proposed self-supervised pre-training large Transformer models on \nlarge text corpora. This demonstrated a significant improvement in the performance of downstream \nNLP tasks compared to previous state -of-the-art (SOTA) systems. Following this phenomenon , \nthe pre-training and fine-tuning paradigm has become the dominant approach in NLP.  Continued \ninnovation in this space has resulted in many popular pretrained language models with increasingly \npowerful capabilities such as GPT-2 [128], GPT-3 [129], OPT [130], and more recently, ChatGPT \n[131].  \nThis thesis has a focus on using machine learning technologies with as little training as possible. \nConsequently, as we shall see in the subsequent chapters, we heavily leverage pre -trained LLMs \nin our proposed technical solutions for various tasks.  \nPretraining and Fine-tuning: Pretraining refers to the process of training a model on a vast set \nof unlabeled data, usual ly with an unsupervised learning technique, in order to acquire general \nfeatures or representations of the input data [132]. Fine-tuning has its roots in the Transfer learning \nparadigm [116]. Fine-tuning model refers to training it again using a new dataset that is tailored to \na specific task after the model has already been pre -trained on a larger dataset. The new dataset \nincludes labeled data, and the aim of fine -tuning is to modify the parameters of the pre -trained \nmodel so th at it can fit the new data more effectively.  The practice of fine -tuning pre -trained \nlanguage models for downstream tasks gained significant popularity with the emergence of large-\nscale pre-trained languages models like BERT [120], and GPT [121]. \nFew-shot, One-shot, Zero-shot learning: Few-shot learning (also known as: low -shot learning) \ninvolves training a model on a small number of examples from a new task not known to the model \n[133]. This approach aims to teach the model to learn and generalize fr om a limited amount of \n 50 \ndata, making it more adaptable to new tasks with limited labeled data. In contrast, one-shot learning \ninvolves learning from a single example, although sometimes learning from a few examples is \ninterchangeably referred to as one-shot learning [133], [134]. \nZero-shot learning refers to the process of training a model to identify new classes that are not \nincluded in the training dataset. During training, the model is exposed to a labeled dataset that \nincludes certain classes. Subseque ntly, the model is tested on a set of classes that it has never \nencountered before. The expectation is that the model can recognize the new classes based on their \nattributes or descriptions. \nIn this chapter, we discuss ed the machine learning technologies t hat we leverage throughout our \nresearch. We introduce d the two main application domains explored in this thesis: Sentiment \nAnalysis and Emotion-Cause Analysis. While in this chapter we summarized the general research \ntrend in these two areas, we discuss the specific gaps in the literature in the subsequent chapters: \nChapter 3, 4, 5, and 6. Each of these  chapters focus on specific  gaps in the literature and the \nassociated research questions that we  listed in Chapter 1. Following an integrated article format \n[2], each chapter provides contextual information to clarify the research questions supported by \nbrief and focused discussions on the literature  related to the specific research question. \nSubsequently, after this introduction to the specific research question, each chapter describes the \nproposed solutions, associated experiments, discussions, and concluding remarks.  \n  \n 51 \nChapter  3: Multiclass Sentiment Analysis with Limited \nLabeled Data \nPerforming Sentiment Analysis with high accuracy using machine learning techniques requires a \nlarge quantity of training data. However, such extensive training and access to such a large quantity \nof labeled data for specific domains can be expensive and time -consuming. This warrants \ndeveloping more efficient techniques that can perform Sentiment Analysis with high accuracy with \na few labeled training data. In this chapter, we aim to address this problem  with two proposed \nsolutions, SG-Elect and GAN-BElectra. With rigorous exper iments, we investigate the \nperformance of these two architectures in comparison to their respective selected baseline machine \nlearning techniques in terms of performance in multiclass Sentiment Analysis with limited labeled \ndata.  \nThe contributions present ed in this chapter have been published in one conference 1 and one \njournal2. \n3.1 Introduction \nIn recent times, high -performance computing along with cloud technologies facilitated the rapid \ngrowth of many research domains. One such domain is machine learning and applied research \nbased on machine learning techniques such as self -driving cars and language translation services \n[135]. The dramatic rise in internet usage and social media applications has made most people \nwith connected devices content generators of some sort. These user -generated contents are \nessential in the growth of many machine learning -based applications such as virtual personal \nassistants, and recommender systems [136]. The influx of user-generated content elevated the \ndemand for efficient techniques to analyze them to generate valuable insights for government and \ncompanies alike. This also caught the attention of researchers who are designing increasingly \nefficient and accurate data analysis techniques in a frequent manner. Natural Language Processing \n(NLP) of textual data is one such data analysis domain that saw rapid innovation in recent times. \n \n1 Riyadh, M., & Shafiq, M. O. (2021). Towards Multi-class Sentiment Analysis with Limited Labeled Data. In 2021 IEEE \nInternational Conference on Big Data (Big Data) (pp. 4955-4964). \n2 Riyadh, M., & Shafiq, M. O. (2022). GAN-BElectra: Enhanced Multi-class Sentiment Analysis with Limited Labeled Data. \nTaylor and Francis Journal of Applied Artificial Intelligence, 36(1), 2083794. \n 52 \nNLP’s general goal is to create an understanding of language in computers [137]. Ide ntifying \nsentiment expressed in a text is a task that contributes to that overall understanding. This task is \ngenerally known as “Sentiment Analysis”, a research area that has seen rapid progress in recent \nyears [135], [138].  \nIn Sentiment Analysis , a gi ven piece of text is categorized with respect to the sentiment it \nexpresses, often in terms of positive, neutral, or negative. It commonly uses machine learning \ntechniques in order to identify such patterns and then perform these categorizations of textual data. \nThis type of machine learning technique, commonly referred to as supervised learning, heavily \nrelies on training data. Sentiment Analysis is a domain -specific task. This means words used in \none context to express certain sentiments may exert different sentiments in different contexts. As \na result, training a machine learning model for Sentiment Analysis tasks typically requires training \nwith domain-specific labeled data. Having a large quantity of labeled training data for specific \ndomains can be ted ious and expensive. In this research, we focus on the problem of the limited  \nlabeled training data in Sentiment Analysis. \nMachine learning-based Sentiment Analysis models trained with a low number of training data \ncan perform poorly on this text classification task. However, since a large number of training data \nmay not be always available, researchers have attempted to develop techniques to have higher \nSentiment Classification accuracy with a lower number of labeled training data. These techniques, \nhowever, still fall short of reaching the peak performance of machine learning models trained on \nlarge, labeled datasets . As a result, achieving higher accuracy in Sentiment Classification tasks \nwith a low amount of training data is still a worthwhile research problem to address. \nIn this chapter, we propose two successive technical solutions, named SG -Elect, and GAN -\nBElectra to achieve high accuracy in Sentiment Analysis  with limited labeled data (i.e., 50 \ndatapoints per class). Both solutions focus on multi -class Sentiment Analysis . In this type of \nSentiment Analysis, there are more than two sentiment classes to choose from. The typical class \nlabels are positive, neutral, and negative. This is in contrast with binary classification where only \ntwo sentiment classes  are used for categorization: positive and negative. Multi -class Sentiment \nAnalysis, especially the ones that consider neutral as one of the classes, seems to possess more \npractical value compared to binary classification since not every text expresses a s entiment, and \nsome texts naturally fall into the neutral category.  \n 53 \nWe first develop SG -Elect and evaluate its performance against its baseline technique GAN - \nBERT [139]. Building upon the learning from this work, we develop GAN -BElectra, for which \nwe use its predecessor, SG-Elect, as the baseline. \nIn the subsequent sections, we describe the architectures of both techniques in detail. After that, \nwe discuss the experiments and evaluations related to both techniques in concert to facilitate a \nconvenient understanding and comparison of both techniques. This is also to account for the \noverlaps between the two studies incurred by the fact that GAN-BElectra builds upon SG-Elect. \n3.2 Related Studies \nThe lack of labeled data is a common problem in text classificati on tasks. There are several \nproposals in the literature to address this general problem [139]–[141]. Using lexicons to classify \ntexts is a typical solution offered for this issue. [142]. However, the lack of accuracy of lexicon -\nbased techniques in text  classification, especially in comparison to fully supervised machine \nlearning-based methods is well known [143].  \nSemi-supervised learning is another common technique that attempts to resolve the issue of the \nlack of labeled training data. This method is specially engineered to train machine learning models \nwith a few labeled data [144], [145] . Researchers have designed several variations of semi -\nsupervised learning. These include using the teacher-student method where teacher confidence is \nutilized to  identify easy samples during training (e.g., self -paced co -training [146], self -paced \nlearning [144]). Other examples include utilizing meta -learning [147] and active learning [145] \nfor sample selection based on teacher confidence. A recent applic ation of semi -supervised \nlearning, specifically for Sentiment Analysis, is a neural network-based semi-supervised learning \nframework proposed by Li et al. [107]. It performs training with a few labeled data along with \nmany unlabeled data. To address the labeled data scarcity issue in short text classification tasks \nsuch as Sentiment Analysis of tweets, Yang et al. [148] proposed a heterogeneous graph attention \nnetwork that embedded a flexible heterogeneous information network framework that modeled \nshort text with the functionality to include additional information while appropriately detecting \ntheir semantic relations. By extending their technique with semi -supervised inductive learning, \nthey demonstrate that the proposed solution outperforms their sele cted SOTA baselines for both \nsingle and multi-label classification tasks. Kim et al. [149] proposed a novel self-training method \n 54 \nthat leveraged a lexicon to guide its mechanism in generating pseudo-labels in order to address the \nlack of labeled data in text classification, particularly Sentiment Analysis. They demonstrated that \nthe guidance from the lexicon in their experimental setup enhanced the reliability of pseudo labels \nby performing manipulation on the loss term.  \nAbonizio et al. [150] conducted an in-depth study of the usage of text augmentation (i.e., creation \nof synthetic textual data by modifying existing text with the goal of increasing the diversity and \nsize of training data for NLP tasks ) in addressing labeled data scarcity issues in Sentiment \nAnalysis. They offered a taxonomy for these techniques, first categorizing all techniques into \n“sentence” manipulation and “embedding” manipulation (i.e., manipulating representative vectors \nof text instead of the actual text data), and then further di viding the “sentence” manipulation \ncategory into three subcategories: transformation (e.g., synonym replacement), paraphrasing (e.g., \nadapting translation models to generate rephrased text in the same language), and generation (e.g., \nusing autoregressive l anguage models such as GPT2 [128] to generate text). They evaluated \nvarious text augmentation techniques and observed how they influenced the Sentiment \nClassification accuracy of different techniques in scenarios such as a low number of labeled \ntraining data. For instance, they found that BERT  [120] and ERNIE [151] achieved superior \nclassification performance with a low number of available training samples when boosted with the \nback-translation [152] augmentation technique. Some researchers also ap plied text augmentation \ntechniques to mitigate the labeled data scarcity in non -English languages. For example, Barriere \net al. [153] used automatic translation of English tweets to French, Spanish, German, and Italian \nto apply data augmentation which improved the Sentiment Analysis performance over non-English \ntweets using different transformer-based techniques [119]. Edwards et al. [154] demonstrated how \nleveraging GPT-2 [128] driven text augmentation in a few-shot learning setup could enhance text \nclassification accuracy.  \nRecent advances in the pre-trained language model made their landfall in the Sentiment Analysis \nresearch area inevitable. Examples of such pre-trained models include BERT [120], Electra [155] \netc. These models, which are typically based on Transformers [95], are trained on a vast amount \nof data , typically  following a n unsupervised or self-supervised appro ach. This provides these \nmodels with a general capability to comprehend language. At this stage, these pre-trained models \nare capable of many tasks that require an understanding of general -purpose language \nrepresentation. For leveraging them in downstream tasks that require domain-specific knowledge, \n 55 \nfor example, Sentiment Analysis, they go through another lightweight training process known as \nfine-tuning. This is essentially training the pretrained model with the task/domain-specific labeled \ndata. Since the pretrained model already has an understanding of language in general, fine-tuning \non a very small set of domain-specific labeled data can make them significantly more accurate as \nclassifiers compared to typical training of machine learning models. As a r esult, there has been \nsome interest in the researcher community in using these pretrained models to address the issue \nlabeled data scarcity in text -classification tasks in general. For example, Croce et al. [139] \nexperimented with using a semi-supervised generative adversarial network (GAN) to improve \nBERT’s fine-tuning stage. This architecture, named GAN -BERT [139], was mainly focused on \nachieving higher accuracy compared to the original BERT network in text classification tasks with \na low amount of labeled training data.  \nResearchers have proposed various approaches that attained decent accuracy in Sentiment \nClassification tasks with limited labeled data. While approaches that rely on a lot of training data \ncan achieve considerably higher accuracy in t his task, achieving similar accuracy with limited \nlabeled data remains a challenge. As a result, achieving higher accuracy in Sentiment \nClassification tasks using a few labeled data remains a worthwhile investigation.  \nIn the subsequent sections, we first introduce the two successive techniques we developed to \nperform multi-class Sentiment Analysis with limited labeled data (i.e., 50 datapoints per class), \nnamed SG-Elect, and GAN-BElectra. We begin with outlining the contributions of each technique, \nfollowed by in-depth discussions on the proposed architectures.  \n3.3 SG-Elect \n3.3.1 Contributions \nSG-Elect outperforms a recently published technique (i.e., GAN -BERT [139]) in multiclass \nSentiment Classification tasks with limited labeled data in terms of various classification metrics \n(i.e., F1 macro and F1 weighted average scores) . We employ cutting -edge deep learning \ntechnologies such as transformer-based pre-trained models together with more traditional machine \nlearning algorithms to devise our solution. With SG-Elect, we make the following contributions:  \n 56 \n• We propose a multi-class Sentiment Analysis technique named SG-Elect that improves upon \nour SOTA baseline GAN-BERT in terms of sentiment classification performance (F2 macro, \nF1 weighted average score) with limited labeled training data. \n• We evaluate our proposed technique with three publicly available datasets to demonstrate the \nefficacy of our technique in terms of classification accuracy compared to our SOTA baseline \n[139].  \n• We evaluate the performance of individual components used in SG -Elect for an in-depth \nunderstanding of its architecture. \n3.3.2 Architecture \nAs discussed in our literature review, the usage of data with pseudo labels is a well -established \ntechnique to mitigate the effect of limited labeled data. Drawing upon the principles of leveraging \ndata with pseudo labels and the paradigm of semi -supervised learning in limited labeled data \nsetting [144], [145] , w e propose SG -Elect, a Sentiment Analysis  technique for multi-class \nSentiment Analysis with limited number of labeled training data , which consists of two primary \ncomponents: a pseudo-label generator and two-step fine-tuning of the final classifier. We discuss \nthis architecture in detail below. \n3.3.2.1 Pseudo label generator \nThis part of the architecture is responsible for generating pseudo labels based on the few original \nlabeled data and a large amount of unlabeled data. This component combines techniques from both \nthe traditional machine learning and deep learning  paradigms. It includes the following \nsubcomponents: \n3.3.2.1.1 SS-Trainer \nSS-Trainer (i.e., Semi-Supervised Self Trainer) subcomponent leverages the Self-Training \nmechanism [156], [157], which allows the supervised classifier to function as a semi-supervised \nclassifier, which iteratively produces pseudo labels and include them in the training data . The \niteration continues until a maximum number of iterations is reached, which we selected as 1000 \nbased on our experiments. For the probability thr eshold criterion, we used 0.80, meaning, the \nalgorithm will only add a pseudo label to the training set if the probability meets this threshold. As \nthe base estimator of the Self Training Classifier, we used a stacked classifier (i.e., ensemble). This \n 57 \nstacked classifier consists of an SGD Classifier (i.e., machine learning algorithm that uses the SGD \nor Stochastic Gradient Descent  optimization algorithm to train a linear classifier ) [158] and an \nXGB Classifier [159] (Figure 13). \n \nFigure 13: Semi-supervised stacked classifier subcomponent \nSGD Classifier employs an efficient method to perform classification tasks such as Sentiment \nClassification. We use logarithmic loss which produces logistic regression, eventually making it a \nprobabilistic classifier. For the regularization term (i.e., penalty), we use L2, whic h is a standard \nregularizer for this linear classifier. To multiply the regularization term, we use a constant value \n(i.e., named alpha) of 0.00001, which is used within the algorithm to automatically determine the \noptimal learning rate. The maximum number  of passing over the training data (i.e., iterations) is \n1000, while the tolerance is 0.001, both are standard choices that have been shown to provide the \nbest outcome in our experiments.  \nXGB Classifier comes from the family of XGBoost which is an optimiz ed distributed boosting \nmechanism often used to improve the efficiency of learning algorithms [159]. In our experimental \nsetting, the combination of the XGB Classifier with its standard configuration with the SGD \nClassifier provided the best outcome in terms of pseudo-label generation compared to some other \ncombinations involving classifiers of SVM and NB variants. Figure 13 represents this Self \nTraining Classifier subcomponent where it begins training with the labeled (L) and unlabeled data \n(U) and, the stack classifier incrementally generates some pseudo labels which are then fed back \n\n 58 \n(data with pseudo labels, P)  to the model as training data. This process is continued until all the \ndata that is unlabeled gets a pseudo-label counterpart.  \n3.3.2.1.2 GAN-BERT \nGAN-BERT is a deep learning network that contains BERT (Bidirectional Encoder \nRepresentations from Transformers) [146], and SS -GAN (Semi -supervised Generative \nAdversarial Network) (Croce, Castellucci, and Basili 2020). BERT is a pretrained model based on \nTransformer technology [95]. It learns contextual relations between words using an attention -\nbased mechanism. The model was pretrained with large training data consisting of raw texts. The \npre-trained model is then fine-tuned on training data for a specific task.  \n \n \n \nFigure 14: GAN-BERT Architecture [139]. U, L, F, G, D denote unlabeled data, labeled data, \nfake labels, generator, and discriminator respectively  \nGAN-BERT’s novelty mainly stems from the fact that it extends BERT’s fine -tuning phase with \nan SS-GAN. SS-GAN consists of two main parts: a) a Generator and b) a Discriminator as shown \nin Figure 14. The Generator generates synthetic labels, and the Discriminator classifies those labels \ninto real and fake using adversarial learning. In GAN -BERT, a pre-trained BERT model is fine -\ntuned with task-specific layers first, which is part of BERT’s typical fine-tuning process. Second, \nduring this fine-tuning stage, SS-GAN layers are used to enable semi-supervised learning.  \n 59 \nWe use GAN -BERT in its original configuration from the study [139]. At first, we use GAN -\nBERT’s fine-tuning process to fine -tune the model with the original few labeled data. Next, the \nfine-tuned GAN-BERT model is used to generate pseudo labels for the unlabeled data.  \n3.3.2.1.3 Combinator \nThe typical output s of both Semi -Supervised Self Training Classifier  (i.e., traditional machine \nlearning) and GAN -BERT (i.e., deep learning) are predicted labels. Along with the predicted \nlabels, the next subcomponent in our architecture requires the probability of each label in order to \nchoose the best pseudo label . As a result, we also derive the probability for each prediction for \neach classifier and pass it to the next subcomponent “Combinator” (see Figure 16). Combinator \nuses this probability score to select the best pseudo label from the pseudo labels generated by the \ntwo pseudo label generators for each unlabeled d atapoints. For example, for a given sentence, if \nGAN-BERT’s predicted pseudo label is “Negative” with a probability score of 0.75 and SS -\nTrainer’s predicted pseudo label is “Neutral” with a probability score of 0.65, then the combinator \nwill choose GAN-BERT’s predicted pseudo label “Negative” since it has a higher probability \nscore. This selected pseudo label along with the associated text then becomes the input for the next \ncomponent within the architecture which we discuss below.  \n3.3.2.2 Electra \nThis final component involves a deep learning model ( Figure 15). This model includes multiple \ndeep-learning layers. Immediately after the input layer, we use the Transformer based pre-trained \nmodel which is known as Electra (Efficiently Learning an Encoder that Classifies Token \nReplacements Accurately) [155].  \n \nFigure 15: Electra-based pretrained component \nElectra implements a more efficient training mechanism compared to the BERT-like masked \nlanguage models (MLM). BERT-like MLMs utilize subsets of unlabeled input in the pre-training \nstage, where the network learns to identify the masked tokens and then redeem the original input. \n\n 60 \nIn Electra, the improvement is primarily achieved through corrupting the input w ith replaced \ntokens instead of using a masked subset of input and then pre -training the network as a \ndiscriminator that identifies original tokens vs. replaced ones. In our architecture, the “pooled \noutput” from this Electra layer goes to a Dropout Layer which prevents the model from overfitting. \nThe next layer is a deeply connected neural network layer (i.e., Dense layer). The output of this \nlayer is passed to an Argmax function in order to calculate the final predicted labels , as shown in \nFigure 15.  \nThis final classifier model involves AdamW [160] as the optimizer, which is a stochastic \noptimization method that carries out modifications in the typical implementation of weight decay \nin Adam optimizer [161]. It achieves this by separating weight decay from the gradient  update. \nFor calculating loss, we use Sparse Categorical Cross-entropy since the classification task we are \nperforming involves more than two mutually exclusive classes. We observed this classifier to reach \nthe peak performance at 10 epochs, which is what we set as the number of epochs. The classifier \nis fine-tuned twice, once with the data with pseudo labels generated in the previous component,  \nand the next using the original few labeled data.   \n \nFigure 16: SG-Elect Architecture. U, L, P denote unlabeled, labeled, and pseudo-labeled data \nrespectively \n  \n\n 61 \nAlgorithm 1: Overall Training Process of SG-Elect. Meanings of symbols used: CSS: SS-Trainer, \nCGB: GAN-BERT, CEL: Electra, YSSL: Predicted label by CSS, YSSC: Probability score of each \nprediction by CSS, YSS: Collection of all predicted labels and associated probability scores by \nCSS, YGBL: Predicted label by CGB, YGBC: Probability score of each prediction by CGB, YGB: \nCollection of all predicted labels and associated probability scores by CGB, YP: Collection of \nfinal selected pseudo labels, XP: Collection of data with pseudo labels \ninputs: X = XL + XU  \nXL = xL1, xL2, …, xLn   \nXU= xU1, xU2, …, xUm  \noutputs: Trained model \n1 \n2 \n3 \n4 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n35 \n36 \n37 \n38 \n39 \nBEGIN \n for all x  X do { \n  CSS  Train (SGD + XGB, x)   \n } end for \n return CSS  \n for all xU  XU do { \n  YSSL, YSSC  CSS  \n } end for \n return YSS  \n for all x  X do { \n  CGB  Train (GAN-BERT, x)  \n } end for \n return CGB  \n for all xU  XU do { \n  YGBL, YGBC  CGB  \n } end for \n return YGB  \n for all xU  XU do { \n  YP \n  if YGBC > YSSC  \n   add YGBL to YP \n  else  \n   add YSSL to YP \n } end for \n return YP  \n      Init XP //to store data with pseudo labels \n      for all xU  XU do { \n  Append (xU,yP) to XP \n } end for \n      for all xP  XP do { \n  CEL  Train (ElectraTransformer, xP) \n } end for \n return CEL \n for all xL  XL do { \n  CEL  Train (ElectraTransformer, xL) \n } end for \n return CEL  \nEND \n  \nFigure 16 shows the overall architecture of our solution.   \n 62 \nAlgorithm 1 captures its overall training process. The algorithm for training our solution includes \nthe following primary steps: \n• We start with a few labeled (XL) and many unlabeled data (XU).  \n• Using this input data for the training process, the Semi-supervised Self Training Classifier \n(CSS) and GAN-BERT component (CGB) generate pseudo labels (YSS, YGB respectively) for \nthe unlabeled data.  \n• Based on the confidence of prediction, the next subcomponent, ‘Com binator’ selects the \nbest predictions (Y P) for each datapoint. We form dataset X P using these pseudo labels \nalong with the associated text from X U, to fine-tune an Electra Transformer-based pre-\ntrained model (CEL).  \n• Finally, another fine-tuning process is done by using the original few labeled data.  \n• In the end, we run our test data on the fine-tuned Electra Transformer which generates the \nfinal predicted labels for our architecture.  \n3.4 GAN-BElectra \n3.4.1 Contributions \nGAN-BElectra builds upon the technique discussed in the previous section, SG-Elect, [162] with \nthe aim to reduce architecture complexity and training steps without sacrificing performance gain \nachieved by its predecessor. SG-Elect employs three machine learning components. Two of them \nare deep learn ing components (GAN -BERT [139] and Electra [155]) and the other one is a \ntraditional machine learning component (Semi -Supervised Self Trainer [156]. In GAN-BElectra, \nwhich is an evolution of SG -Elect, we employ only two deep learning components (GAN-BERT \nand Electra), which significantly reduces the complexity of the architecture  and training steps,  \nwhile still achieving a small performance gain (arithmetically) in  Sentiment Classification over \nSG-Elect.  \nAs part of devising GAN-BElectra, we make the following contributions: \n• We propose a novel technique for multi -class Sentiment Analysis named GAN-BElectra \nwhich builds upon SG-Elect. It possesses a significantly more lightweight architecture and \nrequires less training step compared to SG-Elect, without sacrificing any performance gain \n 63 \nachieved by its predecessor in multi-class Sentiment Analysis with limited labeled training \ndata (i.e., 50 labeled datapoints per class). \n• We evaluate GAN-BElectra on three datasets that are available in the public domain. These \nexperiments illustrate that GAN-BElectra outperforms its state-of-the-art (SOTA) baseline \n(i.e., SG-Elect) in the multi -class Sentiment Analysis  task with limited labeled training \ndata. \n• We dissect GAN -BElectra’s architecture and analyze the individual components that \nconstitute it to facilitate a thorough understanding of the proposed solution.  \n3.4.2 Architecture \nGAN-BElectra consists of two primary components: GAN-BERT [139] and Electra [155]. These \ntwo components have been described above in greater detail in the SG -Elect’s architecture and \nthey are leveraged in GAN-BElectra with similar configuration and use-case.  \nGAN-BElectra’s architectural design is built upon the same principles tha t guided the design of \nSG-Elect’s architecture: the utility of data with pseudo labels and the semi -supervised learning \nmethods in mitigating the effect of limited labeled training data in machine learning models [144], \n[145]. In GAN -BElectra, GAN-BERT a cts as the sole  pseudo label generator whereas Electra \nconsumes the data with those pseudo labels to fine-tune itself.  \n \nFigure 17: GAN-BElectra architecture. U, L, P denote unlabeled, labeled, and pseudo-labeled \ndata respectively \n\n 64 \nFigure 17 shows the overall architecture of GAN -BElectra. This architecture evolved from SG -\nElect where there was an additional Sem i-Supervised Self Training Classifier  (SS-Trainer) that \nacted as another pseudo-label generator in addition to GAN-BERT.  \nGAN-BElectra streamlines SG-Elect architecture by stripping away the process-heavy SS-Trainer, \nleaving only GAN-BERT as the sole gener ator of pseudo labels. This also abolishes the need of \nhaving the “Combinator” component which further simplifies the architecture.  \n  \n 65 \nAlgorithm 2: Overall training process of GAN-BElectra. Meanings of symbols used: CGB: GAN-\nBERT, CEL: Electra, YGB: Collection of pseudo labels generated by CGB, XP: Collection of data \nwith pseudo labels \ninputs: X = XL + XU  \nXL = xL1, xL2, …, xLn   \nXU= xU1, xU2, …, xUm  \noutputs: Trained model \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \nBEGIN \n for all x  X do { \n  CGB  Train (GAN-BERT, X)  \n } end for \n return CGB  \n for all xU  XU do { \n  YGB  Predict (CGB, XU) \n } end for \n return YGB  \n  Init XP //to store data with pseudo labels \n  for all xU  XU do { \n     Append (xU,yGB) to XP \n  } end for \n  for all xP  XP do { \n CEL  Train (ElectraTransformer, xP) \n  } end for \n  return CEL \n for all xL  XL do { \n  CEL  Train (ElectraTransformer, XL) \n } end for \n return CEL  \nEND \n \n  \n 66 \nAlgorithm 2 demonstrates GAN -BElectra’s high -level training process. The training process \nbegins with a large amount of unlabeled data (XU) and a few labeled data (XL). After being trained \non the origina l few labeled data (X L) in a semi -supervised manner, the GAN -BERT component \n(CGB) generates pseudo labels (YGB) for the unlabeled data. We form dataset XP using these pseudo \nlabels along with the associated text from X U to fine -tune our Electra -based pretr ained model \n(CEL). This final Electra -based classifier goes through another fine -tuning using our few labeled \ndata. At this point, GAN-BElectra is fully trained, and we evaluate it using our test data.  \n3.5 Experiments and Evaluation \n3.5.1 Procedure \nWe perform several experiments in order to evaluate GAN -BElectra, SG-Elect (GAN-BElectra’s \nbaseline), GAN-BERT (SG-Elect’s baseline), and two other constituting techniques, Electra and \nSS-Trainer. Although individual experiments have some specificities, they all share some common \nsteps. These include:  \n3.5.1.1 Data splitting  \nOne of the initial steps in our experiments involves splitting the dataset into training and test sets. \nFor the train-test split, we opted for an 80:20 ratio where 80% of the total data accounted for the \ntraining dataset and 20% is allocated for testing. We also split the training dataset into labeled and \nunlabeled sets. In all our data-splitting exercises, we use specific seed numbers to generate \ncontrolled randomness. This makes our experiments reproducible. It is noteworthy that during the \ndevelopment phase,  particularly for hyperparameter tuning,  to mitigate the possibility of \noverfitting to the test data from each dataset, we leveraged random samples from one of our \nselected datasets. These selected hyperparameters were later used in all out experiments across all \ndatasets.  \nOnce we split our training dataset between labeled and unlabeled sets, we remove the labels from \nthe unlabeled dataset to serve its intended purpose. The labeled dataset contains 50 datapoints for \neach sentiment class , and the remaining training dataset contain s unlabeled data. With only 50 \ndatapoints per class, the labeled dataset represents a minute fraction of the total training datapoints \nused across our experiments with various datasets.  \n 67 \nDataset Percentage of Labeled data in Training Data \nSST5 2.91% \nUS Airline 1.28% \nSemEval 0.31% \nTable  2: Percentage of labeled data in total training data \n \n3.5.1.2 Data preprocessing \nAfter training (labeled, and unlabeled ) and test datasets are created, we perform additional data \npre-processing such as the removal of stop words as necessary, and performing vectorization of \nthe datasets so that they are consumable by the machine learning algorithms. We  describe this in \ngreater detail in the section that follows. \n3.5.1.3 Defining model \nThe next step is to define the machine learning model. This involves defining the correct \nparameters and configuration for the model, in addition to defining the layers that event ually \nconstruct the deep learning networks used in this study. \n3.5.1.4 Training and testing \nAfter the model definition step, we begin training the model with our training data which is \ndifferent for each experiment. Following the completion of the training, we test our trained model \nwith the test datasets. We report various metrics including F1 -score, and accuracy based on our \nevaluations of the models. \nTo sum up, each experiment mainly consists of dataset splitting, data preprocessing, defining the \nmodel, training the model, and evaluating the trained model. To make our experimental findings \nmore reliable, we run each of our experiments three times using three different random seeds, and \nwe report the average results from these experiments. It is noteworthy that si nce GAN-BElectra \nbuilds upon SG -Elect [162] and the evaluation criteria, datasets, and experimental settings are \nintentionally identical in these two consecutive studies and they share some of the reported results, \nespecially as it relates to the two common individual sub-components, namely GAN-BERT [139] \nand Electra [155]. Consequently, we report results from both studies in a consolidated manner. \n3.5.2 Tools \n 68 \nOur experiments required leveraging several existing tools. The following are some noteworthy \nexamples: \n3.5.2.1 Software: \nProgramming language: We use Python in all our experiments. Python provides many useful \nlibraries for NLP tasks out of the box as well as for machine learning experiments in general. \nLibraries: We use many publicly available software librar ies in our experiments as required. \nSome of the noteworthy ones include Numpy [163] and Matplotlib [164]. For deep learning \nalgorithms, we heavily leverage TensorFlow [165]. \nMachine learning models: We use GAN-BERT in its original configuration [166]. This serves \nas the common pseudo -label generator in both GAN -BElectra and SG -Elect. For SG-Elect, we \nalso use SS-Trainer in addition to GAN-BERT for pseudo-label generation. As the final classifier, \nboth solutions leverage the large variant of the pre-trained Electra model [167]. \n3.5.2.2 Hardware \nInstead of using our on -premise hardware resources, we utilized cloud hardware resources \nprovided as part of the Pro -tier subscription of Google Colab [168]. This platfo rm provides a \nbrowser-based user interface to perform coding, which is specially designed for data science tasks. \nThis is supported by backend resources such as Tensor Processing Unit (TPU) [169] and Graphics \nProcessing Unit (GPU) [170]. The hardware s pecification provided by Google Colab Pro varied \nfor different experiments and the runtime engines are dynamically allocated. However, the \nfollowing specification represents the typical maximum hardware resource we received  from \nGoogle Colab Pro: \n• Compute: Intel(R) Xeon(R) CPU @ 2.30GHz (Max. available cores: 40) \n• Memory: 36 GB  \n• Disk: 226 GB \n \n3.5.3 Datasets \nIn order to perform a robust evaluation of our solution, we chose these three datasets: SST5 dataset \n[171], US Airline dataset [172], and SemEval 2017 dataset [173] (we refer to this as SemEval in \n 69 \nthis thesis ). The reasons for choosing these three datasets include, they are all multi -class \nSentiment Analysis datasets, publicly available for use for research, and commonly used in \nSentiment Analysis research. We describe these datasets below: \n3.5.3.1 SST5 \nThe SST5 is a 5 -class Sentiment Analysis dataset. SST stands for Stanford Sentiment Treebank \n[171]. Datasets with 5 or more sentiment classes are typically called fine -grained sentiment \ndatasets. The SST5 dataset contains short texts, and each short text is labeled with any of the \nfollowing 5 sentiment classes:  \n• Very Positive \n• Positive \n• Neutral \n• Negative \n• Very Negative \nFigure 18 demonstrates the composition of this dataset.  \n \nFigure 18: SST5 dataset composition \n3.5.3.2 US Airline \nThe US Airline dataset [172] is a 3-class Sentiment Analysis dataset. Each datapoint consists of \nshort text which is a real Twitter post about US Airline carriers, and a sentiment label which is \neither Positive, Neutral, or Negative. Figure 19 shows the composition of this dataset. \n\n 70 \n \nFigure 19: US Airline dataset composition \n3.5.3.3 SemEval \nSemEval [173] is another 3 -class Sentiment Analysis dataset we have used in our experiments. \nThis dataset is composed of short text from real Twitter posts and each short text is labeled as \neither Positive, Neutral, or Negative. Figure 20 shows the composition of this dataset.  \n \nFigure 20: SemEval dataset composition \n3.5.4 Data preprocessing \nWe use the BERT tokenizer [165] to transform our input texts into a numerical format. The deep \nlearning models incorporated in our experiments anticipate all the input sentences concatenated to \none another. The input begins with a “[CLS]” token (this indicates that the task is a \"classification \nproblem\"). The end of each input is indicated with a separator token “[SEP]”. Finally, t he \n\n 71 \ndatapoints were represented as Tensors [165], and the deep learning experiments were executed \non TPUs [169] \nIt is noteworthy that since our usage of 50 datapoints per class as the original few labeled data \nmakes the composition of the primary traini ng data organically balanced, we kept the datasets in \ntheir original composition without applying any additional balancing technique. Also of note is \nthat the original SemEval dataset contains a total of 61473 datapoints, the majority of which would \nhave been used in our setup as unlabeled data. Since our original labeled data per class is only 50 \ndatapoints, an extensively large amount of unlabeled data provides little value in our setup while \nsignificantly increasing the training time. Consequently, we op ted to use a representative sample \nfrom this dataset (i.e., taking a total of datapoints 20000 out of 61473, resembling the other 3-class \ndataset used in our study: US Airline dataset) while preserving the original composition (i.e., class \ndistribution) of the dataset by leveraging stratified random sampling [174].  \n3.5.5 Evaluated Techniques \nUsing the three selected datasets, we evaluated our proposed techniques: GAN-BElectra, and SG-\nElect (GAN-BElectra’s baseline). We also evaluate GAN -BERT (SG-Elect’s baseline) using the \nsame datasets. In addition, we inspect how other constituting components (Electra, SS-Trainer) of \nour proposed solutions perform on the same datasets. \n3.6 Results  \nIn order to make our findings more reliable, we evaluated each t echnique (GAN-BElectra, SG-\nElect, GAN-BERT, Electra, SS-Trainer) three times on each of the three selected datasets using \nseed numbers to achieve controlled randomized datapoint allocation for train and test split for each \nevaluation (for an example of a s imilar study with three runs per model, see [175]). In summary, \nwe ran a total of 45 experiments representing 9 experiments per technique. We report the average \nresults from the three randomized runs for each evaluation.  \nTable  3 and Table  4 provide a summary of all our experimental results. In Table  3, we report the \nmean accuracy (i.e., an average of three runs. Accuracy is calculated using true positives and true \nnegatives as shown below) for each technique on each dataset.  \n \n 72 \n𝑎𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =  𝑡𝑝 + 𝑡𝑛\n𝑡𝑝 + 𝑓𝑝 + 𝑡𝑛 + 𝑓𝑛   \nwhere, tp, fp, tn, fn consecutively represent true positive, false positive, true negative, and false negative.  \n \nIn addition, we report the standard deviation of this mean, which represents the variations of the \naccuracy scores across the three evaluations for each technique on each dataset. Table  3 shows \nthat the standard deviation scores for all our experiments were between 0.01 11 and 0.0514 \nindicating high reliability of the reported mean accuracy scores based on our randomized \nexperiments.  \nTable  3 also contains F1 macro (defined below) which, in contrast with accuracy, provides a more \nreliable assessment of techniques in the presence of class imbalance [176].  \n𝐹1 𝑆𝑐𝑜𝑟𝑒 =  2 ∗  𝑝𝑟 ∗ 𝑟𝑒\n𝑝𝑟 + 𝑟𝑒    \nwhere,  \n𝑝𝑟 (𝑝𝑟𝑒𝑐𝑖𝑠𝑜𝑛) =  𝑡𝑝\n𝑡𝑝 + 𝑓𝑝   \n𝑟𝑒 (𝑟𝑒𝑐𝑎𝑙𝑙) =  𝑡𝑝\n𝑡𝑝 + 𝑓𝑛    \n𝐹1 (𝑚𝑎𝑐𝑟𝑜) =  \n∑ 𝐹1 𝑆𝑐𝑜𝑟𝑒𝑖\n𝑁\n𝑖=1\n𝑁     \nwhere,  \n𝑁 =  𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑐𝑙𝑎𝑠𝑠𝑒𝑠 𝑖𝑛 𝑡ℎ𝑒 𝑑𝑎𝑡𝑎𝑠𝑒𝑡 \nIn addition to accuracy and F1 macro average, we additionally report F1 weighted average in Table  \n3, which is typically used when more importance is given to classes with larger support [176].  \n𝐹1 (𝑤𝑒𝑖𝑔ℎ𝑡𝑒𝑑) =   ∑ 𝑤𝑖\n𝑁\n𝑖=1\n 2 ∗ 𝐹1 𝑆𝑐𝑜𝑟𝑒𝑖  \nwhere, \n𝑤𝑖 =  𝑁𝑜. 𝑜𝑓 𝑠𝑎𝑚𝑝𝑙𝑒𝑠 𝑖𝑛 𝑐𝑙𝑎𝑠𝑠 𝑖\n𝑇𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑠𝑎𝑚𝑝𝑙𝑒𝑠   \n𝑁 =  𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑐𝑙𝑎𝑠𝑠𝑒𝑠 𝑖𝑛 𝑡ℎ𝑒 𝑑𝑎𝑡𝑎𝑠𝑒𝑡 \n \n 73 \nWe report the mean standard error (i.e., the inverse of accuracy) in Table  4. Inspired by Tom \nMitchell’s suggestion on comparing machine -learning models [177], we additionally report \nestimation statistics based on the reported standard error at the commonly used significance level \nof 95%. The following formula was used for the confidence interval (ci) calculation: \n𝑐𝑖 = 𝑧 ×  √  𝑒 ×  (1 − 𝑒)\n𝑛    \nwhere, \n- z is a critical value from the Gaussian distribution which has a value of 1.96 for 95% significance level [178],  \n- n is the size of the test sample,  \n- e is the standard error reported in Table  4.  \nThe equations reported in this section can be found in [178], [179]. \nOur analysis suggests that the confidence interval  (half width)  for the reported standard error \nremained between the range of 0.0154 and 0.0203 (see Table  4). This arguably further indicates \nthe robustness of our results across multiple experimental runs for each of the evaluated techniques \nfor the selected datasets. We also conduct statistical significance testing on our overall findings on \nthe models’ classification performance based on accuracy, F1 macro, and F1 weighted average \nscores (see Table  11 and Table  12). \nTable  6 contains the accuracy score of pseudo-label generation by GAN-BERT and SS-Trainer \n(only used in SG-Elect). Table  8, Table  9, and Table  10 contain detailed results for all evaluated \ntechniques for SST5, US Airline, and SemEval datasets respectively. Following the tables, we also \ninclude confusion matrices visualizing the classification accuracy and true positives and negatives \nfor each evaluated technique for the three selected datasets.  \nWe describe our results for each dataset in the following sections. \nDataset Technique Mean  \nF1 Macro \nMean  \nF1 Weighted Avg. \nMean \nAccuracy \nStandard Deviation \nof Mean Accuracy \nSST5 GAN-BElectra 0.3796 0.3764 0.3825 0.0350 \nSG-Elect 0.3763 0.3715 0.373 0.0249 \nElectra 0.3138 0.3179 0.3533 0.0225 \nGAN-BERT 0.3185 0.327 0.3406 0.0337 \nSS-Trainer 0.2176 0.2136 0.2333 0.0111 \nUS \nAirline \nGAN-BElectra 0.6722 0.7239 0.7108 0.0195 \nSG-Elect 0.6659 0.7203 0.7072 0.0195 \nElectra 0.5493 0.6318 0.623 0.0331 \n 74 \nGAN-BERT 0.6413 0.7054 0.7032 0.0514 \nSS-Trainer 0.4178 0.446 0.4307 0.0480 \nSemEval GAN-BElectra 0.5663 0.5769 0.5741 0.0145 \nSG-Elect 0.558 0.5658 0.5635 0.0228 \nElectra 0.3992 0.4309 0.4536 0.0184 \nGAN-BERT 0.473 0.478 0.5208 0.0448 \nSS-Trainer 0.4072 0.4367 0.4472 0.0305 \nTable  3: Summary of Results (F1 Macro, F1 Weighted Avg., Accuracy, Standard Deviation) \nDataset Technique Mean Standard Error Confidence Interval (at 95%) of \nStandard Error (Half Width) \nSST5 GAN-BElectra 0.6175 0.0203 \nSG-Elect 0.627 0.0202 \nElectra 0.6467 0.0199 \nGAN-BERT 0.6594 0.0198 \nSS-Trainer 0.7667 0.176 \nUS Airline GAN-BElectra 0.2892 0.0164 \nSG-Elect 0.2928 0.0165 \nElectra 0.377 0.0176 \nGAN-BERT 0.2968 0.0165 \nSS-Trainer 0.5693 0.0179 \nSemEval GAN-BElectra 0.4259 0.0153 \nSG-Elect 0.4365 0.0154 \nElectra 0.5464 0.0154 \nGAN-BERT 0.4792 0.0155 \nSS-Trainer 0.5528 0.0154 \nTable  4: Summary of Results (Standard Error, Confidence Interval of Standard Error) \nDataset GAN-BERT Pseudo Label Accuracy SS-Trainer Pseudo Label Accuracy \nSST5 0.3328 0.2255 \nUS Airline 0.7087 0.4229 \nSemEval 0.5209 0.4403 \nTable  5: GAN-BERT's accuracy in pseudo label generation across three datasets \nDataset Technique Very Negative Negative Neutral Positive Very Positive \nSST5 SS-Trainer 71.36 78.66 76.58 40.53 52.90 \nGAN-BERT 28.64 21.34 23.42 59.47 47.10 \nUS Airline SS-Trainer \n \n4.30 4.62 10.41 \n \nGAN-BERT \n \n95.70 95.38 89.59 \n \nSem-Eval SS-Trainer \n \n9.99 10.39 9.38 \n \nGAN-BERT \n \n90.01 89.61 90.62 \n \nTable  6: Percentage contribution to final selected pseudo labels for each class by GAN-BERT \nand SS-Trainer (SG-Elect only) \n \nSST5 US Airline SemEval \nGAN-BERT 31.50 94.55 90.02 \nSS-Trainer 68.50 5.45 9.98 \n 75 \nTable  7: Percentage contribution to final selected pseudo labels by GAN-BERT and SS-Trainer \n(SG-Elect only) \n3.6.1 SST5 Dataset \nBoth GAN-BElectra and SG-Elect outperformed GAN-BERT, Electra, and SS -Trainer for the \nSST5 dataset in mean accuracy. GAN-BElectra also outperformed its predecessor, SG-Elect. Table  \n3 shows that GAN -BElectra achieved an average accuracy of 0.3825 while SG -Elect, Electra, \nGAN-BERT, and SS-Trainer scored 0.373, 0.3533, 0.3406, and 0.2333 respectively. Table  3 also \ncontains mean F1 macro and F1 weighted average scores for all techniques and shows that GAN-\nBElectra and SG-Elect outperform all other evaluated techniques in these two other metrics as well \nfor the SST5 dataset, while SG-Elect was outperformed by its successor GAN-BElectra. Table  8 \nshows that our proposed solutions also performed better than the other evaluated techniques in F1-\nscore for individual classes. GAN-BElectra outperformed SG -Elect in F1 -score for the “Very \nNegative”, “Positive”, and “Very Positive” classes, whereas SG-Elect achieved a higher F1-score \nfor the “Negative” and “Neutral” classes.  GAN -BElectra also outperformed GAN -BERT and \nElectra for F1 -score for all individual classes except the “Neutral” class. Confusion matrices in \nFigure 21 visualize the correct and incorrect predictions by all evaluated techniques for the SST5 \ndataset.  \nTechnique Class Precision Recall F1-score Accuracy \nGAN-\nBElectra \nVery Negative 0.3246 0.5253 0.397 0.3825 \nNegative 0.4493 0.2461 0.3129 \nNeutral 0.2481 0.2601 0.2537 \nPositive 0.4232 0.485 0.4499 \nVery Positive 0.4969 0.4773 0.4847 \nSG-Elect Very Negative 0.3357 0.4854 0.3912 0.373 \nNegative 0.4604 0.2755 0.3301 \nNeutral 0.2617 0.3488 0.2944 \nPositive 0.4434 0.3452 0.3849 \nVery Positive 0.4695 0.5072 0.4807 \nElectra Very Negative 0.4121 0.3831 0.2701 0.3533 \nNegative 0.3828 0.2062 0.2363 \nNeutral 0.2385 0.3119 0.2663 \nPositive 0.3813 0.4459 0.4063 \nVery Positive 0.5194 0.4514 0.3901 \nGAN-BERT Very Negative 0.3114 0.4908 0.3725 0.3406 \nNegative 0.4753 0.2027 0.2476 \nNeutral 0.2358 0.3524 0.27 \nPositive 0.3932 0.3564 0.3539 \n 76 \nTable  8: Detailed Results for the SST5 Dataset \n3.6.2 US Airline Dataset \nTable  9 shows the detailed comparative classification result for GAN-BElectra, SG-Elect, GAN-\nBERT, Electra, and SS-Trainer for the US Airline dataset. GAN-BElectra and SG-Elect achieved \nan overall accuracy of 0.7108 and 0.7072 respectively, while Electra, GAN-BERT, and SS-Trainer \nscored, 0.623, 0.7032, and 0.4307 respectively. This finding is further strengthened by the fact that \nGAN-BElectra and SG-Elect outperform all other evaluated techniques in the mean F1-macro and \nF1 weighted average scores as well for the US Airline dataset (see Table  3). GAN-BElectra also \noutperforms SC-Elect in this metric.  As demonstrated in Table V, GAN -BElectra and SG-Elect \nachieved higher F1 -score for all three individual classes compared to GAN -BERT, Electra, and \nSS-Trainer, while GAN-BElectra also outperforms SG-Elect in the same metric. Figure 22 shows \nthe confusion matrices that visualize the correct and incorrect predictions made by all four \nevaluated techniques for the US Airline dataset. \nTechnique Class Precision Recall F1-score Accuracy \nGAN-BElectra Negative 0.8896 0.7231 0.7975 0.7108 \nNeutral 0.463 0.6516 0.5404 \nPositive 0.6277 0.7408 0.6788 \nSG-Elect Negative 0.8921 0.7202 0.7967 0.7072 \nNeutral 0.4686 0.6328 0.5372 \nPositive 0.5932 0.7542 0.6637 \nElectra Negative 0.8282 0.6759 0.7375 0.623 \nNeutral 0.4369 0.4882 0.4468 \nPositive 0.398 0.5939 0.4636 \nGAN-BERT Negative 0.835 0.7752 0.7938 0.7032 \nNeutral 0.5015 0.5484 0.5048 \nPositive 0.6342 0.6264 0.6253 \nSS-Trainer Negative 0.834 0.3477 0.4876 0.4307 \nNeutral 0.2892 0.428 0.333 \nPositive 0.3106 0.7571 0.4328 \nTable  9: Detailed Results for the US Airline Dataset \n3.6.3 SemEval Dataset \nVery Positive 0.4881 0.4159 0.4396 \nSS-Trainer Very Negative 0.1673 0.3432 0.2102 0.2333 \nNegative 0.3477 0.2176 0.2357 \nNeutral 0.1932 0.42 0.2633 \nPositive 0.3636 0.0892 0.1429 \nVery Positive 0.3119 0.1959 0.2361 \n 77 \nTable  10 demonstrates that GAN -BElectra and SG-Elect outperform GAN-BERT, Electra, and \nSS-Trainer in terms of classification accuracy for the SemEval dataset. They achieved an accuracy \nscore of 0.5741  and 0.5635 respectively while Electra, GAN -BERT, and SS -Trainer achieved, \n0.4536, 0.5208, and 0.4472 respectively. We also observe in Table  3 that GAN-BElectra and SG-\nElect outperform all other evaluated techniques in terms of mean F1  macro and F1 weighted \naverage scores as well for the SemvEval dataset.  In line with other datasets , GAN-BElectra also \noutperforms SG-Elect in this metric. As can be seen in Table  10, GAN-BElectra performed better \nthan SG-Elect in 2 out of 3 individual classes, ac hieving a higher F1-score for the “Neutral” and \n“Positive” classes while SG -Elect achieved a higher F1 -score for the “Negative” class. Both \nElectra and GAN-BERT underperformed in terms of F1-scores for all individual classes compared \nto SG -Elect and GAN -BElectra. Confusion matrices in Figure 22 visualize the correct and \nincorrect predictions made by GAN-BElectra, SG-Elect, GAN-BERT, Electra, and SS-Trainer for \nthe SemEval dataset.  \nTechnique Class Precision Recall F1-score Accuracy \nGAN-BElectra Negative 0.4373 0.6046 0.507 0.5741 \nNeutral 0.5992 0.5524 0.5727 \nPositive 0.6665 0.5857 0.6191 \nSG-Elect Negative 0.4374 0.6219 0.5116 0.5635 \nNeutral 0.5977 0.5244 0.557 \nPositive 0.639 0.5826 0.6055 \nElectra Negative 0.2835 0.3515 0.3006 0.4536 \nNeutral 0.5288 0.596 0.5526 \nPositive 0.5434 0.3259 0.3445 \nGAN-BERT Negative 0.4542 0.4787 0.44 0.5208 \nNeutral 0.5736 0.5394 0.468 \nPositive 0.634 0.5191 0.5109 \nSS-Trainer Negative 0.2891 0.3573 0.2839 0.4472 \nNeutral 0.5116 0.5231 0.4958 \nPositive 0.5199 0.3978 0.4418 \nTable  10: Detailed Results for the SemEval Dataset \n3.6.4 Ablation Study \nTwo major components within GAN-BElectra are GAN-BERT and Electra. Additionally, in SG-\nElect, another important component is SG-Trainer. As part of our ablation study, we investigated \nthe individual performance of these components on the same test data used for GAN-BElectra and \nSG-Elect. This analysis provided insight into the effect of individual components that constitute \nour proposed solution. Table  3 and Table  4 contain the summary of these results for GAN-BERT, \n 78 \nElectra, and SS -Trainer, while Table  8, Table  9, and Table  10 provide detailed results with \nmetrics for each class. Below we discuss the performance of these individual components on the \nthree datasets used with our proposed solutions: SG-Elect and GAN-BElectra.  \n3.6.4.1 US Airline and SemEval Dataset \nGAN-BERT outperformed Electra  and SS-Trainer for our two 3 -class Sentiment Classification \ndatasets (US Airline and SemEval). For the US Airline dataset, GAN-BERT achieved an overall \naccuracy of 0.7032 whereas Electra  and SS -Trainer achieved 0.623  and 0.4307 respectively . \nSimilarly, for the SemEval dataset, GAN -BERT achieved an overall accur acy score of 0.5208 \nwhile Electra and SS-Trainer achieved 0.4536 and 0.4472 respectively. \n3.6.4.2 SST5 Dataset \nFor the SST5 dataset, Electra outperformed GAN-BERT, achieving an overall accuracy of 0.3533 \nwhile GAN-BERT scored 0.3406. However, SS-Trainer consistently scored the lowest (0.2333) \nfor this dataset like the other dataset. \n3.6.4.3 Pseudo Label Generation \nWe also investigated the accuracy of pseudo labels generated by GAN -BERT which were \neventually utilized to train Electra along with the few original labeled data . For three different \ndatasets we have used (SST5, US Airline, and SemEval), the average accuracy of GAN -BERT’s \ngenerated pseudo labels were 0.3328, 0.7087, and 0.5209 respectively, as demonstrated in Table  \n5.  \nIn SG-Elect, SS-Trainer was also used in addition to GAN-BERT for pseudo-label generation. The \naverage accuracy of SS -Trainer’s generated pseudo labels were 0.2255, 0.4229, and 0.4403 \nrespectively for SST5, US Airline, and SemEval datasets. We also inspect ed how each pseudo \nlabel generator (GAN-BERT and SS-Trainer) contributed to the final select pseudo labels for SG-\nElect. It shows how the contribution differed for 3 -class (SST5 dataset) and 5 -class (US Airline \nand SemEval datasets) Sentiment Analysis . For  3-class Sentiment Analysis , GAN -BERT \nconsistently contributed significantly more pseudo labels to the final Electra-based classifier in our \narchitecture, with 94.55% and 90.02% for US Airline and SemEval datasets respectively. In \ncontrast, for 5-class Sentiment Analysis, SS-Trainer contributed 68.50% pseudo labels to the final \n 79 \nclassifier. Table  6 shows the percentage contribution to selected pseudo labels by GAN -BERT \nand SS-Trainer for individual classes across three datasets used in this evaluating SG-Elect.  \nIn the subsequent section, we discuss the results we achieved with our proposed solution along \nwith the insights gained from the ablation study.  \n 80 \n \nFigure 21: Confusion matrices for SST5 dataset for (a) GAN-BElectra, (b) SG-Elect, (c) Electra, \n(d) GAN-BERT, (e) SS-Tainer \n\n 81 \n \nFigure 22: Confusion matrices(with red shades) for the US Airline dataset for (a) GAN-BElectra \n(b) SG-Elect (c) Electra (d) GAN-BART (e) SS-Trainer and confusion matrices (with green \nshades) for the SemEval dataset (f) GAN-BElectra (g) SG-Elect (h) Electra and (i) GAN-BART \n(j) SS-Trainer   \n\n 82 \n3.6.5 Statistical Significance Test \nWe perf orm statistical significance test in order to understand if the differences in model \nperformance (in terms of various classification metrics: accuracy, F1 macro, F1 weighted average) \nare statistically significant. For significance testing, we use the value  of these three metrics from \neach experiment run across all three datasets. Our experimental setup includes three experiment \nruns (see Section 3.5.1.4) per evaluated model. Each run consists of different train and test split, \nwhich we control using seed number. For example, for the SST5 datasets, we had three train and \ntest splits: Train1:Test1, Train2:Test2, Train3:Test3. Each technique was evaluated on these three \nsplits for this dataset, and similarly, for the other two datasets. C onsequently, each model was \nevaluated on a total of nine sets of train and test split. As a result, we have nine related (or paired) \ndata points per metric for the significance testing for model comparison. This means, for example, \nto test the statistical significance of the difference in F1 macro score achieved by SG -Elect and \nGAN-BERT, we use nine paired datapoints for each of these models. For each of the three datasets \n(SST5, US Airline, SemEval), there are three F1 macro scores obtained from three experiment \nruns for each model, resulting in a total of nine data points for F1 macro per model. \nWe use Wilcoxon Signed Ranks Test [180] for the significance test, which is often leveraged for \ncomparing two classifiers over multiple different datasets (for relevant guidelines, see [181], [182]; \nfor example usages, see [183]–[185]). Since we had three randomized experiment runs per \ntechnique per dataset due to the limited scope of the study, statistical significance test for models’ \nperformance comparison for each individual dataset is not feasible due to the small sample size \n(i.e., 3 datapoints per model per statistical test). Instead, w e utilize the Wilcoxon Signed Ranks \nTest [180] across all datasets together, which avail us with nine data points per model per statistical \ntest. This helps  us understand the statistical significance of the differences in the overall \nperformance of the models we evaluated  across all datasets, which satisfies our goal of \nunderstanding models’ overall performance in our selected task.  \nWe use a significance threshold of 0.05 in the Wilcoxon Signed Ranks Test that we conduct. Our \nnull hypothesis (H0) for these tests is that “there is no statistically significant difference in the \nperformance of the models in comparison ”. If the p -value is less than 0.05, we reject the null \nhypothesis, and accept the alternative hypothesis  (Ha) which states that “there is statistically \nsignificant difference in the performance of the models in comparison”.  \n 83 \nWe compare the performance of SG-Elect and GAN-BElectra with GAN-BERT, Electra, and SS-\nTrainer. We also compare SG-Elect’s performance with GAN-BElectra. Table  11 and Table  12 \nrepresent the results from these statistical tests, where we report the asymptotic p-value (2-tailed) \nfrom the Wilcoxon Signed Ranks Tests [180] for three classification metrics: accuracy, F1 macro, \nF1 weighted average. \nBased on the accuracy score, SG-Elect’s superior performance is statistically significant (p-value \n< 0.05; i.e., rejects H0) in comparison with all other evaluated models except GAN-BElectra and \nGAN-BERT (see Table  11). For the F1 macro score and F1 weighted average, SG -Elect’s \nperformance gain is statistically significant (p-value < 0.05; i.e., rejects H0) compared to all models \nexcept GAN-BElectra.  \nFrom Table  12, we observe that GAN -BElectra’s performance is significantl y superior (p-value \n< 0.05 ; i.e., rejects H0) than all the other models except SG -Elect based on F1 macro and F1 \nweighted average. For the accuracy score, GAN-BElectra’s performance is significantly higher (p-\nvalue < 0.05; i.e., rejects H0) than all other evaluated models except SG-Elect and GAN-BERT.  \nModels in \nComparison \np-value \n(Accuracy) \np-value \n(F1 Macro) \np-value \n(F1 Weighted Average) \nSG-Elect \nvs. \nGAN-BERT 0.086 0.015 0.038 \nElectra 0.012 0.015 0.008 \nSS-Trainer 0.008 0.008 0.008 \nTable  11: Results (p-values) from significance testing to compare SG-Elect’s performance with \nGAN-BERT, Electra, and SS-Trainer using Wilcoxon Signed Ranks Test [180] for accuracy, F1 \nmacro, and F1 weighted average scores. Bold indicates statistical significance (p-value < 0.05). \nModels in \nComparison \np-value \n(Accuracy) \np-value \n(F1 Macro) \np-value \n(F1 Weighted Average) \nGAN-\nBElectra \nvs. \nSG-Elect 0.214 0.26 0.314 \nGAN-BERT 0.051 0.008 0.015 \nElectra 0.011 0.008 0.008 \nSS-Trainer 0.008 0.008 0.008 \nTable  12: Results (p-values) from significance testing to compare GAN-BElectra’s performance \nwith SG-Elect, GAN-BERT, Electra, and SS-Trainer using Wilcoxon Signed Ranks Test [180] \nfor accuracy, F1 macro, and F1 weighted average scores. Bold indicates statistical significance \n(p-value < 0.05). \n 84 \n3.7 Discussion \n3.7.1 Impact of double fine-tuning \nFor both SG -Elect and GAN -BElectra, w e observe that double fine -tuning Electra, once with \npseudo labels and next with a few original labels boosts the accuracy of this transformer network \ncompared to training it only once with either pseudo labels or the original few labels. This confirms \nthe finding in previous studies [186] where researchers noticed a similar improvement in \nclassification accuracy by fine-tuning pretrained models twice.  \n3.7.2 Performance gain with fewer training steps and reduced network \ncomplexity  \nOur experiments suggest that the semi-supervised self-training sub-component of SG-Elect [162] \nthat included a stacked classifier consisting of a Stochastic Gradient Descent (SGD) Classifier \n[187] and an XGBClassifier [159] negatively contributed to the overall classification a ccuracy \nachieved by SG -Elect. SG-Elect’s successor, GAN-BElectra, removes that component from the \narchitecture along with SG -Elect’s “Combinator” component. We observe that this did not \nadversely impact the performance. Moreover, a s demonstrated in in the reported results, GAN -\nBElectra achieved a small gain in performance (arithmetically) over SG-Elect for all the evaluated \ndatasets for all three evaluation metrics (see Table  3). While GAN-BElectra’s performance gain \nover SG-Elect is small and statistically insignificant (see  Table  12), the gain is consistent across \nall evaluated datasets , and we consider this  as an additional improvement over its predecessor  \nwhile acknowledging its lack of statistical significance  (some examples of similar considerations \nin the literature where significance test was not carried out or reported: [155], [175], [188], [189]). \nIt is also important to emphasize that compared to SG -Elect, GAN -BElectra reduces the \ncomplexity of the archit ecture as it relieves the need for training an additional machine learning \ncomponent that consumes resources and incurs more training cycles.  \n3.7.3 Impact of pseudo labels on the final result \nFor SG-Elect, where we used SS-Trainer as one of the pseudo label gen erators along with GAN-\nBERT, we noticed that despite having low accuracy, SS-Trainer still had a high confidence score \nfor its prediction for the SST5 dataset, resulting in more of its generated pseudo labels to be filtered \n 85 \nin by SG -Elect’s Combinator comp onent which selected the best pseudo labels based on \nconfidence score. We argue that if SS-Trainer was well calibrated (i.e., SS-Trainer’s confidence \nscore was in sync with its classification accuracy for SST5), GAN-BERT would have had a larger \nshare in the final selected pseudo labels by the Combinator.  This indicates that SS -Trainer had a \npositive bias in its probability score. It is particularly more apparent for the SST5 dataset. A \nworthwhile future investigation is to introduce the model calibration process [190] for SG-Elect’s \npseudo label generators, which can potentially mitigate such unanticipated positive bias.  \nWe also observed that for US Airline and SemEval datasets, the overall accuracy scores of SG -\nElect were 0.7072 and 0.58875 respectively for these two datasets and SS-Trainer contributed only \n5.45% and 9.98% to final pseudo labels for them respectively. From these observations, we deduce \nthat the higher the contribution of SS-Trainer in the selected pseudo labels, the lower the accuracy \nour architecture achieved. We evaluated this hypothesis with GAN -BElectra, where GAN-BERT \nis the sole pseudo label generator, resulting in higher overall accuracy for all three datasets \ncompared to SG -Elect. As reported in the results, we notice that the accu racy of the generated \npseudo labels  by GAN -BERT (0.3328, 0.7087, 0.5209 for SST5, US Airline, and SemEval \nrespectively) resembles the final results of GAN -BElectra for all three datasets (0.3825, 0.7108, \n0.5741 in the same order). This highlights the impac t of the GAN -BERT as a pseudo -label \ngenerator in GAN-BElectra’s overall  architecture, making it an important component of the \nproposed solution.  \nWithout the contribution of GAN -BERT as a pseudo -label generator, the performance of GAN-\nBElectra would have degraded significantly, as can be inferred from our ablation study where we \ntested the performance of the stand-alone Electra pre-trained model fine-tuned only on the original \nfew labels. In terms of average accuracy, this fine -tuned stand -alone Electra mod el achieves \n0.3533, 0.6233, and 0.4536 for SST5, US Airline, and SemEval dataset respectively, whereas our \nproposed model GAN -BElectra demonstrates a boost in performance contributed by the pseudo \nlabel generator GAN-BERT, and achieved 0.3825, 0.7108, 0.57 41 in terms of average accuracy \nfor these three datasets in the same order. Although slightly lower than GAN -BElectra, a similar \nboost in performance was observed for SG-Elect as well, which utilized both GAN-BERT and SS-\nTrainer as pseudo -label generators.  It achieved average accuracy scores of 0.373, 0.7072, and \n0.5635 for SST5, US Airline, and SemEval datasets respectively. \n 86 \nFor SG-Elect, we also notice that, with SST5, GAN -BERT had a significantly higher share of \nselected pseudo labels for the Positive and Very Positive classes compared to the other three \nclasses. The opposite is observed for the US Airline dataset where SS -Trainer had a larger share \nof pseudo labels for the Positive class compared to the other two classes. These differences do not \nseem to have a noticeable impact on the overall accuracy achieved by SG-Elect. \n3.7.4 Prediction trends for individual class \n3.7.4.1 Incorrect predictions more likely to fall into the adjacent categories  \nWe observe that the majority of incorrect predictions typicall y fall into the adjacent sentiment \ncategories. This means that a “positive” datapoint is more likely to be predicted as “positive” and \n“neutral” (also “very positive” for the 5-class dataset) compared to contrasting labels such as \n“negative”. For instance, for the SST5 dataset, out of a total of 556 datapoints with the “positive” \nlabel, GAN -BElectra correctly predicted the label for 269 datapoints as “positive”, and then \nincorrectly predicted 138 datapoints as “very positive”, 95 datapoints as “neutral”, on ly 26 \ndatapoints as “very negative” and 28 as negative. This similar phenomenon has been observed for \nalmost techniques including SG-Elect, with all three datasets (see confusion matrices in Figure 21 \nand Figure 22).  \n3.7.4.2 Variation in pseudo label accuracy \nThe accuracy of pseudo label generation by GAN-BERT and SS-Trainer (only applicable for SG-\nElect) varied significantly across our three selected datasets . GAN -BERT achieved 0.3328, \n0.7087, 0.5209 in pseudo label generation accuracy for SST5, US Airline, and SemEval datasets \nrespectively, and SS-Trainer achieved 0.2255, 0.4229, and 0.4403 (in the same order). Since SST5 \nis a finer-grained sentiment dataset (i.e., 5 sentiment classes whereas the other two datasets have \n3 classes), we expected it to  contribute to lower classification accuracy compared to US Airline \nand SemEval dataset. This is indeed consistent in all other experiments performed in this study.  \nWe also observed that between US Airline and SemEval datasets, though they are both 3 -class \nsentiment datasets, GAN-BERT scored 0.7087 for the accuracy for the former compared to 0.5209 \nfor the latter. We believe this is due to the different compositions of the two datasets. For example, \nthe US Airline dataset has a significantly higher number of “negative” datapoints compared to the \nother two classes. On the other hand, SemEval contains much fewer “negative” datapoints \n 87 \ncompared to the other two classes. However, Figure 19 and Figure 20 show that this c lass \nimbalance in the SemEval dataset is visibly less significant compared to the US Airline dataset. \nConfusion matrices in Figure 22 for test data show that all evalu ated techniques performed more \naccurate predictions for the neutral class for the SemEval dataset compared to the other two classes.  \nFor the US Airline dataset, it is the negative sentiment class where more accurate predictions \noccurred across all techniq ues. Based on these observations, we argue that the different \ncompositional attributes of these two datasets (e.g., a significantly higher number of “negative” \ndatapoints in the US Airline dataset compared to the SemEval dataset which was visibly less \nimbalanced) might had an impact on the overall differences in the accuracy of pseudo label \ngeneration for these two datasets  for GAN-BERT. As mentioned in earlier, our rationale for not \nartificially balancing the composition of the datasets was due to the fact that the primary training \ndata used in our experiments were organically balanced since we used 50 datapoints for each class \nfor each dataset in each of our experiments as the “few original labeled training data”. In addition, \nalong with accuracy, we also reported the F1-macro score which provided a more reliable \ncomparison of techniques when class imbalance is present in the selected dataset. However, we \nbelieve that this may still had an impact on the pseudo label generation accuracy as well as the \nvariations in the number of correctly predicted labels for our test data (see Confusion matrices in \nFigure 21 and Figure 22). In contrast, in SG -Elect, where SS-Trainer was also used as a pseudo \nlabel generator along with GAN -BERT, this phenomenon was not observed. Instead, SS -Trainer \nhad similar performance in pseudo label accuracy for both 3 -class datasets, scoring 0.4229 and \n0.4403 for US Airline and SemEval respectively. This provides an opportunity for future \ninvestigation to inspect the  underlying reason for this difference in performance, particularly \nconsidering the fact that SS -Trainer is a traditional machine learning technique whereas GAN -\nBERT is based on deep learning technologies. \nIt is also noteworthy that between GAN -BERT and Electra, the two main components of GAN -\nBElectra, GAN -BERT outperforms Electra for two 3 -class sentiment datasets, while Electra \noutperforms GAN-BERT for the SST5 dataset which contains 5-class sentiment data. Fine-tuning \nstand-alone Electra is significantly faster than fine-tuning GAN-BERT since GAN-BERT uses a \nsemi-supervised training method, which essentially performs many iterations of supervised \ntraining with varying training data. Despite this, Electra achieving higher accuracy for SST5 than \nGAN-BERT is an interesting observation that warrants further investigation. \n 88 \n3.8 Conclusion \nIn this chapter, we discuss SG-Elect and GAN-BElectra, two successive techniques for multi-class \nSentiment Analysis with limited labeled data. Our experiments suggest that SG -Elect achieves \nsignificantly higher performance (i.e., F1 macro, and F1 weighted average) in multi -class \nSentiment Analysis compared to its baseline (GAN -BERT). We subsequently develop GAN -\nBElectra, that builds upon SG -Elect and significantly reduces the archit ecture complexity and \nrequired training steps compared to SG -Elect, without having any adverse effect on the \nperformance. It is noteworthy that GAN -BElectra achieved a small gain in performance \n(arithmetically) consistently across all datasets we evaluated  though the gain in performance did \nnot reach statistical significance (see section 3.6.5 and 3.7.2 for more details).  \nThough GAN-BElectra demonstrated better performance compared to its predecessor (i.e., SG -\nElect), the margin of improvement is not statistically significant. This suggests that there is still \nroom for further improvement in this area.  \nWe performed statistical significance testing across all datasets to compare the performance of our \nproposed architectures over other evaluated techniques. It can be a worthwhile investigation to \nconduct similar statistical significance testing for each individual dat aset which would require \nconducting more experiment runs (i.e., 10 or more) per models for each dataset than we performed \nin our study (i.e., three).  \nAnother noteworthy limitation of both the proposed solution s is that they require a multi -step \ntraining process involving training the pseudo label generator(s) first and then training the Electra \ncomponent. Achieving an end-to-end solution without requiring a multi-step training process is a \nworthwhile future exploration.   \n 89 \nChapter  4: Emotion Cause Generation: A New NLP Task  \nFrom Sentiment Analysis  discussed in the previous chapter, this chapter extends our focus on \nhuman emotion towards the more complicated tasks related to the cause of emotion. This is the \nfocal point of a relatively new NLP research area, named Emotion-Cause Analysis (ECA). ECA \nhas recently garnered substantial attention from the researcher community. In addition to devising \nvarious techniques to solve ECA-related tasks, researchers also introduced different variants of the \nECA tasks such as Emotion Cause Extraction (ECE), Emotion Cause Pair Extraction (ECPE), \nEmotion Cause-Pair Span Extraction (ECS P). These are primarily classification tasks where the \ncause of the emotion and/or type of emotion expressed in the text are identified. In this study, we \npropose the first -ever generative NLP task within the  ECA domain, named Emotion Cause \nGeneration (ECG). The objective of this task is  to generate a meaningful cause for an emotion \nexpressed in a given text. We demonstrate the viability of this newly proposed task with promising \nearly observation. \nThe work presented in this chapter has been published in one conference3. \n4.1 Introduction \nRecent advances in high -performance computing coupled with omnipresent cloud technologies \nhave been a catalyst in amplifying t he volume of research work performed many in domains. \nMachine learning and its various applications are prime examples of such domains. A plethora of \nrecent research now focuses on leveraging machine learning to perform various tasks, such as \nSentiment Analysis (e.g., [191]–[193]), Emotion-Cause Analysis [90], [194], [195] , persuasion \n(e.g., [196]–[198]), predicting future events, (e.g., [199]) image generation from the text (e.g., \n[200]). Machine learning is also being utilized in many industries on a routine basis and it is \ncontributing to novel consumer products such as autonomous cars, personalized virtual assistants, \netc. [135]. Natural Language Processing (NLP) is an interdisciplin ary research area that has seen \nextraordinary growth in recent years influenced by the advancement in machine learning \ntechnologies along with the availability of a large amount of data. NLP is typically broken down \n \n3 Riyadh, M., & Shafiq, M. O. (2022). Towards Emotion Cause Generation in Natural Language Processing using Deep Learning. \nIn 21st IEEE International Conference on Machine Learning and Applications (ICMLA), 2022. \n 90 \ninto two branches: NLU (Natural Language  Understanding) and NLG (Natural Language \nGeneration) [1]. These two subdomains of NLP complement each other which eventually \nconstitute the overall domain of NLP. \nWithin NLP, there are a plethora of “tasks” that researchers focus on such as Sentiment Analysis, \nEmotion Cause Extraction (ECA) related tasks. These are primarily some examples of NLU tasks, \nwhich have historically been the dominant subarea within NLP. Recent advances in “language \nmodels” such as GPT-2 [128] have garnered remarkable focus from the researcher community on \nthe NLG tasks. In this study, we attempt to leverage the advances in general -purpose language \nmodels in order to perform the new NLG task we introduce within the domain of ECA, named \nEmotion Cause Generation (ECG). Figure 23 shows how ECG fits within the existing ECA tasks. \n \nFigure 23: ECG and its relationship with the existing ECA tasks. \nThere are several tasks within the domain of ECA. The most common among them is ECE \n(Emotion Cause Extraction). Introduced in 2010 [77], ECE aims to identify the cause of a certain \nemotion expressed in a given text. Subsequently, many other researchers p roposed various rule-\nbased techniques and traditional machine learning techniques to perform this task (e.g., [9], [74], \n[201]). More recently, some studies proposed more variants of the ECE tasks leveraging deep \nlearning technologies. For instance, Xia et al. introduced the ECPE (Emotion Cause Pair \nExtraction) task which not only extracts the emotion -cause but also identifies the emotion \nexpressed [90]. Following this, Wei et al. proposed the first end-to-end model for the ECPE task  \n[91]. Li et al. proposed the ECSP (Emotion Cause -Pair Span Extraction) task which aims to \n\n 91 \nidentify the cause of the emotion in the given text at a more granular span-level instead of typical \nclause-level identification [194].  \nThough we have witnessed the rapid growth of generative tasks within the broader domain of NLP, \nsuch as open -ended text generation, poetry generation, machine translation, and question -\nanswering, the application of generative machine learning has largely remained unexplored in the \nECA research area. We propose leveraging text generation methods to generate meaningful causes \nof an emotion expressed in a given text  where the “cause of the expressed emotion” is missing . \nWe name this task – Emotion Cause Generation (ECG). We believe that this new ta sk and its \npotential extensions can be useful in various applications. For instance, an automated mechanism \nto generate meaningful causes for an emotion expressed in a given context can be useful in \nspecialized artificial intelligence systems where it need s to offer suggestions as to why someone \nmay be experiencing certain emotional states. In addition, this task initiates the exploration of \ngenerative machine learning in the ECA research area. This initiative is expected to encourage \nfurther similar explor ation in this area, planting the seed for many impactful future research \nactivities. \nIn this study, we particularly demonstrate the ECG task a text -infilling NLP task. We build upon \nan existing infilling machine learning solution [202] to perform the ECG  task to demonstrate its \nviability as a new NLP task in the domain of ECA. \n4.2 Contributions \nThe specific contributions of the study discussed in this chapter include the following: \n• We propose a new ECA -related NLP task named Emotion Cause Generation (ECG). Th is is \nthe first ever generative NLG task (i.e., NLG task) in the ECA domain. \n• We establish the viability of the proposed ECG task as an NLP task through technical \ndemonstration that includes: \no Identifying and adapting an existing ECA -related dataset [9] for the proposed ECG \ntask. \no Building upon an existing text generation infilling solution [202] to perform the ECG \ntask. \n 92 \no Leveraging various existing quantitative methods to evaluate the generated causes \ncoupled with qualitative analysis performed by the authors.  \n4.3 Related Studies \nLee et al. [77] first proposed the ECE task . The objective was to find the underlying cause of  a \nspecific emotion that is conveyed in a given text. In contingency to their work, many researchers \nhave subsequently designed various techniques based on traditional machine learning  and rule-\nbased based methods to tackle this problem [9], [78], [203]. Despite their good performance, these \nmethods were still highly dependent on the complex feature design and suffered from poor learning \nability compared to their successors such as deep learning methods.  \nThe recent advance in deep learning has inspired some researchers to put their focus on discerning \nthe cause of emotions by leveraging neural networks along with the attention mechanism in order \nto discern the underlying cause of an emotion. Gui et al. [74] issued a new dataset in the Chinese \nlanguage with texts containing various expressed emotion and their causes and devised a technique \nto identify the causes of emotions. Leveraging this corpus, Gui et al. [87] employed a machine \nlearning model that utilizes a deep memory network to identify the cause of emotion by leveraging \na question-answering model. Chen et al. [204] developed a technique for detecting emotion-cause \nin microblogs with a hierarchical convolution neural network . It uses an encoder at the clause -\nlevel and another at the subtweet-level to integrate contextual features. Xu et al.  [205] adopted a \nlesson to re-categorize methods based on a set of features dependent on emotion along with a set \nof emotion-independent features. \nXia et al. [206] offered a new ECPE task within ECA where the goal was to also determine the \nemotion expressed in the sentences in ad dition to identifying the associated cause -clause of the \nemotion. To evaluate their proposed task, they  enhanced the datasets released by Gui et al. [74]. \nFan et al. [207] reconstructed the ECPE task into a technique of directed graph construction  by \nleveraging this newly reconstructed dataset along with BERT [120]. These studies however \nconsidered ECA as a  clause-level classification task, largely disregarding the reality that cause-\nclause itself is not the actual cause. Instead, the clause is a set of text that contains the emotion -\ncause along with other texts. To address this, Li et al. [194] proposed a new ECSP task where they \nreleased a novel span -based joint learning approach incorporating BERT [120] and attention \n 93 \nmechanism [95] as the backbone of their technical solution. However, these new tasks still \nconfined ECA under NLU, without any indication of how NLG can be useful for ECA. \nWith the introduction of the Transformer [95], particularly BERT [120], the NLP research area, \nspecifically NLG, has seen exponential growth, primarily towards large pre -trained language \nmodels (LLMs). These pretrained LLMs have demonstrated an outstanding ability to generate text \nwith few-shot and zero-shot learning [129], [208]. LMs typically consist o f a large number of \nparameters with some having several hundred billion parameters [208]. The highest increase in the \nmodel size (117M to 500B) came within Auto -regressive LLMs [128], [208]. GPT-2 [128] first \nbrought vast enhancement to the quality a nd articulacy of the general -purpose LLMs. This trend \ncontinued with GPT-3 [129] and newer LLMs . Since the introduction of these large LLMs, \nresearchers have proposed many new NLP tasks [128], [129], [209]. \nOne such task is text infilling which attempts to “fill in the blank” by predicting missing pieces of \na given text. This type of infilling task requires the predicted text to be semantically appropriate \nfor both the preceding and succeeding text. The existing off-the-shelf open-source LLMs such as \nGPT-2 can effectively produce coherent text [210], [211], but cannot properly perform the infilling \noperation as they only leverage the text in a single direction as context, typically the preceding \nones. In contrast, bidirectional attention -based mechanisms such as BERT [120] and SpanBERT \n[212] are capable of performing the infilling task by taking both the preceding and succeeding \ntexts into consideration. Nevertheless, this very bidirectional att ention capability bounds their \nability to infill only to spans with fixed lengths.  \nDonahue et al. [202] provided a general and flexible infilling framework that can be utilized for \nvarious domains. Simply put, their technique takes “text with blanks” as  input and generates \ncompleted text as output. To achieve this, they first constructed training samples by masking \nrandom spans in the input text to generate two input components: text with blanks and the \nassociated answer for each blank. They trained unid irectional LLMs such as GPT-2 on these two \ninput components, joined together. In inference time, the trained model can predict the “answer \nfor the blanks” when the “text with blanks” is given as input. Due to the various relevant features \nof this framework, such as the ability to infill for variable length spans, the capability to leverage \npretrained LLMs and their standard fine -tuning processes, we build upon this framework to \n 94 \ndemonstrate the feasibility of the proposed ECG task as an NLP task, particularly, an infilling task \nunder the NLG branch.  \n4.4 ECG: The Proposed Task \nIn this study, we propose Emotion-Cause Generation ( ECG), the first ever generative NLP task \nwithin the ECA domain that aims to generate a meaningful cause for an emotion expressed in a \ngiven text. Compared to the existing ECA tasks which primarily focus on extracting the cause of \nthe emotion from a set of given texts or clauses, ECG focuses on generating the text span or n -\ngram that represents a meaningful cause for the given emotion (Figure 24). \n \nFigure 24: A sentence expressing an emotion (\"happy\") with the cause indicated in bold. \nWe regard ECG as an infilling assignment [202]. This means for a given emotion-cause containing \ntext, the cause of the emotion is the “blank” that needs to be “filled” or “generated”. Figure 25 \nshows an example of  such a text with an expressed emotion and a meaningful cause. ECG task \naims to generate the text span (indicated in bold in Figure 24) that forms the cause of the emotion \nfor a given input text.   \n4.5 Demonstration of the Proposed Task  \n4.5.1 Cause-generation model \nTo generate a meaningful cause for given emotion -containing text, we treat the emotion -cause \ngeneration task as an infilling task. We leverage Ilm [202], which offers a convenient mechanism \nto fine-tune GPT2 [128] for such infilling tasks. One can mask any part of a sentence, regardless \nof the role of that part within the sentence (i.e., it can be a random part of a sentence, the verb \nclause or subject clause etc.), and use Ilm to “infill” that part. In order to use Ilm to generate the \nspecifically masked “cause” clause for the proposed ECG task, we make some modifications to its \nalgorithm and fine-tune it with our selected ECA dataset [9]. We describe these below. \n\n 95 \n4.5.2 Customization of Ilm for the ECG task \nIlm includes a mechanism called “masking”, with wh ich, the full or a portion of the given text \ninput is omitted and eventually replaced with the generated texts. Ilm offers several masking \nmechanisms such as the masking of words, entire document, or randomly selected n -gram spans. \nThe masked texts are treated as the “blanks” in the infilling task where Ilm generates appropriate \ntext in place of the masked text.  \nIn order to perform our proposed ECG task, we have created a custom masking step leveraging \nIlm’s existing random n -gram masking capability. In Il m’s existing random n -gram masking \nfunctionality, it randomly selects a starting and ending point for the texts to be masked. We build \nupon this random n-gram masking mechanism to define a new custom masking mechanism where \nthe n-gram’s starting and ending point can be defined based on the identifier within the input text. \nThe specified n-grams represent the cause-spans in our study. We discuss this further in the dataset \nsubsection below. \n4.5.3 Fine-tuning \nThe author of Ilm fine -tuned GPT2 on three datasets and made those trained models publicly \navailable (i.e., one model trained on story excerpts, one on song lyrics, and another on research \nabstracts). Instead of fine -tuning GPT2 from its original checkpoint, we start fine -tuning from \nIlm’s checkpoints as it comes with the understanding of the masking provided within Ilm for the \ninfilling task. During the development phase, we used some random samples from our selected \ndataset [9] to observe how these different Ilm models may potentially perform the ECG task. W e \nnoticed that the Ilm model that was trained on a dataset of story excerpts  functioned closer to our \nuse-case compared to the other two datasets: research paper abstracts and song lyrics [202]. We \nbelieve this is due to the fact that story excerpts can contain more content related to human emotion \nin a regular prosaic format compared to the other two types of datasets used to train the other two \nvariations of the Ilm model. We utilize most of Ilm’s original hyperparameters as our analysis \nduring the development phase demonstrated that these default settings are sufficient for our \nintended task. However, we adjusted a few of these hyperparameters to suit the goal and the scope \nof the study, which includes capping the max-training step at 1000 and epochs at 500 to optimize \nthe training. After loading the selected Ilm checkpoint, we further fine-tune it with our dataset [9] \nleveraging our custom masking step. \n 96 \n4.5.4 Inference  \nAfter the fine-tuning process as described in the prior sections, the model can be used for inference \n– to generate a “cause” for a given emotion expressed in a text. The expected input text expresses \nan emotion with the cause of the emotion “masked” with “an underscore” (i.e., “_”). The fine -\ntuned model generates the “cause” of the emotion expressed in the text by replacing the mask (i.e., \nthe “underscore”). Figure 25 demonstrates an example of the expected input and output of the fine-\ntuned model.  \n \nFigure 25: The top sentence represents an input to the model which “masks” the cause of the \nemotion with an “underscore”. The bottom text represents the output text with a generated cause \nwhich replaced the masked portion of the input sentence.  \n4.5.5 Hardware \nWe leveraged cloud computing resources with Google Colab Pro, which provides elastic \ncomputing resources based on processing and storage needs. Typically, the following \nspecifications were the maximum available resources available for us to use at Google Colab: \n• CPU: Intel Xeon 2.30GHz with max. 40 cores \n• Memory: 36 GB  \n• Storage: 226 GB \n4.6 Dataset \nWe use the emotion-cause dataset offered by Ghazi et al. [9]. It properly fits our use -case for the \ndemonstration of the ECG task as the causes within the data points are separated as spans using \nspecific tags in the XML format.  \n4.6.1 Sample Datapoint \n\n 97 \nBelow is an example of a given datapoint of this dataset:  \n<happy> These days he is quite happy <cause> traveling by trolley <\\cause>.<\\happy> \nThe tag “<cause>” indicates the start of the cause -span while “<\\cause>” indicates the end of it. \nThe emotion expressed in the datapoint wraps the text with tags of a similar format.  In this case, \nthe emotion expressed is “happiness” and the sentence is wrapped with starting (i.e., <happy>) and \nclosing (i.e., </happy>) tags indicating the emotion.  \nWe transform each datapoint to adhere to the expected format in the model used in this study. For \nexample, the above sample datapoint was transformed into the following format: \nhappy \nThese days he is quite happy <cause> traveling by trolley <\\cause> \nIn this transformed format, each datapoint comprises of two lines. The first line represents the \nemotion expressed in the given sentence, and the second line contains the original datapoint with \nthe “cause” tag while removing the “emotion” tags that wrapped the sentence in the original \ndataset. Our custom masking function leverages the “cause” tags to programmatically identify the \n“cause span” (i.e., masked n-gram) of the input sentences. \n4.6.2 Overall Composition: \nThe dataset contains 2414 sentences. Among them, there are 820 sentences with both a cause and \nan emotion tag. In this study, we leverage the datapoints that include both cause and emotion tags. \nEach sentence in this subset belongs to one of the following seven emotions:  \n• Happiness \n• Fear \n• Anger \n• Disgust \n• Surprise \n• Shame \n• Sadness  \n4.6.3 Data Splits: \nThe datapoints are split into training and vali dation sets in an 80:20 ratio with 10 reserved \ndatapoints for inference to test the generation capability of the fine -tuned model. The training \n 98 \ndataset was used for fine -tuning the pretrained model. We then used this fine -tuned model to \ngenerate the causes for 10 test datapoints, which we discuss below. \n4.7 Results \nWe use our fine -tuned model (i.e., Ilm story excerpt model with custom masking function) to \ngenerate cause for each of the 10 test datapoints in our study. As discussed earlier, we mask the \n“cause” s pan from these datapoints with underscore (“_”) and tasked the fine -tuned model to \nreplace the masks with relevant causes. Table  13 demonstrates the generated text spans (in bold) \nthat represent causes for the emotion expressed in the example inputs. We also include the \nassociated texts with the original causes along with the emotion expressed for ease of inspection \nfor the readers. \n# Emotion Text with Original Cause  Text with Generated Cause \n1 Anger “I don't feel resentful that God has \ndecided to take me before my \nallotted span.” \nI don't feel resentful of what people are up to. \n2 Fear “In 1986 the local authority became \nconcerned about injuries which D \nhad sustained and they obtained a \nplace of safety order.” \nIn 1986 the local authority became concerned at \nsuspected dog attacks  and they obtained a \nplace of safety order. \n3 Sad “It's possible to be sad about your \nbabies becoming children, and yet \naccept the fact that you won't have \nany more.” \nIt's possible to be sad at the dead body of \nTamar, and yet accept the fact that you won't \nhave any more. \n4 Fear “To see him fumbling with our rich \nand delicate language is to \nexperience all the horror of seeing a \nSvres vase in the hands of a \nchimpanzee.” \nTo see him fumbling with our rich and delicate \nlanguage is to experience all the horror of being \nswallowed, and t o be swallowed by two \npeople, and to feel the sting of it all. \n5 Anger “They were cross with each other  \nbecause of him, and now Nanny was \nsaying bad things about Smallfry \nthat would make Buddie very, very \nangry.” \nThey were cross at him  because of him, and \nnow Nanny was saying bad things about \nSmallfry that would make Buddie very, very \nangry. \n6 Surprise “His last illness was the most \nviolent, and his doctors were \nastounded that he survived it.” \nHis last illness was the most violent, and  his \ndoctors were astounded to see him. \n7 Sad “I was quite sad to hear my right \nhon. Friend the Minister of \nAgriculture, Fisheries and Food \nsay that he was prepared to sit in \nthe negotiations for months.” \nI was quite sad at being unable to come to \nterms with my father. \n 99 \n8 Fear “France expressed horror at the \nassassination and appealed for unity \nto assure peace in the country it once \nruled.” \nFrance expressed horror at the barbarism of \nyesterday's attack  and appealed for unity to \nassure peace in the country it once ruled. \n9 Happy “I'm really happy in the group \nnow.” \nI'm really happy that my mum's gone now. \n10 Happy “Looking anything but delighted at \nthe prospect, he stood up.” \nLooking anything but delighted at having his \ncake and eat it, he stood up. \nTable  13: Texts with original causes and texts with generated causes based on the test data from \nGhazi et el. [9] Original texts are within quotation marks, and cause spans are indicated in bold \nfor both original and generated ones. \nWe inspect these generated causes in the following section. \n4.8 Observation and Analysis \n4.8.1 Evaluation Procedure \nIn order to evaluate the gener ate causes for their meaningfulness in related to the emotion \nexpressed in the sentence, we follow two main approaches:  \nQualitative:  \nWe manually inspect each of the generated causes in order to understand its meaningfulness within \nthe context of a given sentence. We report this analysis in section 4.8.2. \nQuantitative:  \nWe use different types of quantitative metrics to assess the meaningfulness of the generated causes \nin relation the emotion expressed in the given text. We report this analysis in section 4.8.3. We \nleverage some common quantitative metrics that are used for evaluating machine generated texts \nwith reference to the original text . We use them as complementary metrics as  they are more \nsuitable for more structured generative tasks (i.e., such as machine translation which can have an \napproximate verifiable answer) as opposed to the open-ended tasks such as the proposed ECG task \nwhere a single or an approximate “correct” answer does not exist. We briefly introduce these \nmetrics below: \n• BLEU [213] is a common machine translation evaluation metric that compares the n -gram \nmatches in machine-generated text to those in reference text. BLEU-1 [213] and BLEU-2 [213] \nmeasure similarity at the unigram and bigram level respectively.  \n 100 \n• METEOR [214] is another metric that combines precision, recall, and synonymy to evaluate \ntranslation quality.  \n• ROUGE [215] is a set of metrics used for text summarization, with ROUGE -1 [215] and \nROUGE-2 [215] measuring similarity at the unigram and bigram level respectively, and \nROUGE-L [215] using the longest common subsequence.  \n• BERTScore [216] evaluates text quality by comparing contextualized word embeddings. \nAs an additional quantitative analysis, we analyze the general linguistic quality of the generated \ncauses and compared them with their original counterparts in the dataset. The following metrics \nwere used for assessing the linguistic quality: \n• For readability, we use Flesch Reading Ease metric [217] and Coleman Liau Index [218], both \nof whi ch deem a text easily readable when it is easy to read for 13 -15 years old who are \nproficient in English. For Flesch Reading Ease metric, this threshold is a score of 60 or above. \nFor Coleman Liau Index, this threshold is a score 8 or below. We report the average score from \n10 test datapoints.  \n• We also report the overall count of grammatical errors within the original and generated \nsentences. \n• Additionally, we report GRUEN score [219] for evaluating grammaticality, non -redundancy, \nfocus, structure and coherence of generated text. It is a value between 0 to 1 (higher is better). \nLastly, we perform sentiment (using  “twitter-roberta-base-sentiment”  [220]) and emotion \nclassification (using “ emotion-english-distilroberta-base” model  [221]) of the original and \ngenerated causes in order to check their agreement with each other and with the original emotion \nlabels from the dataset. \nWe discuss our analysis on the generated causes for the 10 test datapoints using these various \nmethods in the following subsections. \n4.8.2 Manual Inspection of the Generated Causes \nBased on our observation, the generated cause of item# 1 in Table  13 seems to be grammatically \nand semantically correct. We believe it represents an appropriate cause for the emotion expressed \nin the given text. We share the same remarks for item# 2 as well. For item# 3 and 4, while the \n 101 \ngenerated causes sound somewhat bizarre, it still seems to represent the emotions being expressed \n– “sad” and “fear”. They also demonstrate grammatical and semantical correctness.  \nIn our opinion, the generated cause for item #5 seems to suffer from redundancy and lack \nmeaningfulness as a representative cause for the expressed emotion. The generated cause for item# \n6 seems to be an appropriate cause for the expressed emotion, while also being grammatically and \nsemantically accurate. For item# 7, we also believe that the generated cause represents the emotion \nexpressed in the gi ven text. It also shares a similar grammatical and semantical quality to most \nother items. Similar remarks can be made about item# 8 as well.  \nFor item# 9, although the generated cause sounds peculiar, similar to items 2 and 3, it still seems \nto be a meani ngful cause for the expressed emotion while also being grammatically and \nsemantically correct. We believe that the generated cause for item# 10 is also a meaningful cause \nfor the expressed emotion in the text and that it is grammatically and semantically correct.  \nOverall, with a few exceptions, we opine that the generated causes seem to be meaningful for the \nassociated emotions in the given texts. The consistent grammatical and semantical correctness of \nthe generated texts is also something noteworthy. \n4.8.3 Quantitative Analysis \n4.8.3.1 Methods that use original text as reference: \nWe have leveraged several computational methods to understand the general quality of sentences \nwith generated causes in contrast with sentences with original causes. These methods evaluate the \ngenerated text by using the original text as a reference. While some of these methods were \noriginally meant for evaluating the quality of machine translation, they have been used to evaluate \nother natural language generation tasks due to their generalizability [222].  \nOur test sentences with generated causes achieved a BLEU -1 score [213] of 0.6647. The mean \nprecision for BLEU-1 was 0.6825. BLE U-1 brevity penalty was 0.9739 and the length ratio was \n0.9742. Resembling BLEU-1 scores, for BLEU -2, the score was 0.6191, precisions were 0.6825 \nand 0.5921 (respectively for 1 -gram and 2 -gram) while brevity penalty and length ratio are \nexpectedly the same with BLEU-1.  \nWe also calculated the METEOR score [214], where our test sentences with generated causes \nachieved a score of 0.6550. We calculated 3 different variations of ROUGE scores [215]: ROUGE-\n 102 \n1, ROUGE-2, and ROUGE-L, where our test sentences with the generated causes achieved 0.6617, \n0.5650, and 0.6580 respectively. Finally, we also evaluated our test s entences with the generated \ncauses for BERTSCORE [216], achieving a score of 0.7905. These results are reported in Table  \n14. \nMetric Score \nBLEU-1 [213] 0.6647 \nBLEU-2 [213] 0.6192 \nMETEOR [214] 0.6550 \nROUGE-1 [215] 0.6617 \nROUGE-2 [215] 0.5650 \nROUGE-L [215] 0.6579 \nBERTSCORE [216] 0.7905 \nTable  14: Evaluation of the generated causes with methods that use original text as a reference \n4.8.3.2 Methods to evaluate linguistic quality independent of the reference or original \ntext: \nWe have also performed quantitative analysis of the generated causes using methods that evaluate \nthe linguistic quality of texts independently considering various factors such as grammatical \ncorrectness, readability, etc. We performed two comparative readability tests between the \nsentences with the original causes and sentences with the generated causes namely: Flesch Reading \nEase [217] and Coleman Liau Index [218].  \nFor Flesch Reading Ease, the sentences with the original causes score 63.19 whereas the sentences \nwith the generated causes scored 70.84, indicating relatively similar readability quality of both. As \nseen in Table  15, for Coleman Liau Index, sentences with generated causes achieved 8.06, while \nthe sentences with the original causes scored 9.63. This also indicates how comparable sentences \nwith both the original and generated causes are in terms of readability.  \nWe leveraged a standard Python library [223] to analyze the grammatical correctness of sentences \nwith both the original and generated causes, and it identified a total of 6 and 7 grammatical issues \nrespectively for a total of 10 test data -points. We have also measured the GRUEN score [219] \nwhich represents the general linguistical quality of texts. Our test sentences with generated causes \nachieved an average GRUEN score of 0.7421 with a standard deviation of 0.1432, while the \noriginal text scored 0.7473 and 0.1368 for the same metrics respectively. \nMetric Text with the original cause Text with generated cause \nFlesch Reading Ease [217] 63.19 70.84 \n 103 \nColeman Liau Index [218] 9.63 8.06 \nGrammatical Issues Count [223] 6 7 \nGRUEN [219] 0.7473 0.7421 \nTable  15: Evaluating the linguistic quality of the generated causes using methods that do not use \noriginal text as a reference \n4.8.3.3 Sentiment and Emotion Analysis \nWe perform sentiment and emotion analysis of the test data points with original and the generated \ncauses. Findings from these analysis is presented in Table  16. We observe that for all the 10 data \npoints, emotion labels from the dataset, emotion labels from the emotion classification algorithm \nfor the test data points with origin al and generated causes agree with each other except only for \ndata point #6, where the emotion label from the dataset is “Surprise” but classified by the algorithm \nused (i.e., “emotion-english-distilroberta-base” model [221]) as “Fear” for both the test data points \nwith the original and the generated causes. From the sentiment analysis results (using “ twitter-\nroberta-base-sentiment”  [220]), we notice that all the test data points with original and generated \ncauses agree with each other except for the data point #7, which for the test data with original is \nclassified as “Neutral”, whereas for the generated one, it is classified as “Negative”.  \n# Emotion  \n(from dataset) \nEmotion  \n(Original) \nEmotion  \n(Generated) \nSentiment  \n(Original) \nSentiment \n(Generated) \n1 Anger Anger Anger Neutral Neutral \n2 Fear Fear Fear Neutral Neutral \n3 Sad / Sadness* Sadness Sadness Negative Negative \n4 Fear Fear Fear Negative Negative \n5 Anger Anger Anger Negative Negative \n6 Surprise^ Fear^ Fear^ Negative Negative \n7 Sad / Sadness* Sadness Sadness Negative Negative \n8 Fear Fear Fear Neutral^ Negative^ \n9 Happy / Joy* Joy Joy Positive Positive \n10 Happy / Joy* Joy Joy Neutral Neutral \nTable  16: Sentiment and emotion analysis of the test data point with original and generated \ncauses. The rows follow the same order as Table  13, and the serial numbers in left most column \ncorrespond to the serial numbers in the left most column of Table  13. Asterisk (*) indicates \nsynonymous emotion label added for the ease of manual comparison. Circumflex (^) indicates \nmismatch. \n4.9 Discussion \nBased on our observation, we opine that the generated causes for our test data are in general \nrepresentative of the emotions expressed in the respective sentences . We obse rve that a few \n 104 \ngenerated causes seem somewhat “bizarre”, however, they still seem to be relevant for the emotion \nexpressed in the sentence.  \nIt is also noteworthy that the generated causes generally maintained grammatical correctness. It is \nalso confirmed by the comparative GRUEN score [219], followed by other quantitative analyses \nsuch as Coleman Liau Index [218], and Grammatical Issues Count [223], that the grammatical and \nsemantical correctness of the sentences with the generated causes is similar to the sentences with \nthe original causes. The GRUEN score achieved in the text with generated causes also indicates \nthe low redundancy and high focus and coherence of the generated causes [219].  \nWe also evaluated our generated causes using BLEU [213], METEOR [214], ROUGE [214], and \nBERTSCORE [216]. These techniques typically attempt to predict the “similarity” of the \ngenerated text with some reference text. These techniques, while typically used for machine \nlearning, have also been used to evalu ate other text generation tasks due to their generalizability \n[222]. Our test sentences with generated causes achieved a decent score for these metrics. For \ninstance, a BLEU score of more than 0.50 is interpreted as “very high -quality translation” for the \nmachine translation task [213]. Our achieved scores for these metrics, which are all well above \n0.50, indicate the decent quality of the generated causes with reference to the original causes.  \nOverall, based on our observation and linguistic quality e valuations by techniques like GRUEN \n[219], and other scores such as BLEU that indicate semantic similarity of the generated causes \nwith the original ones, we assert that the generated causes using the “infilling” method \nmeaningfully represent the emotion expressed in the given text while acknowledging that there is \nroom for improvement.  \nBased on the sentiment and emotion analysis  results presented in Table  16, we observe that the \nemotion labels of the test data with the original and generated causes, as well as the emotion labels \nfrom the dataset, agree with each other for 9 out of 10 data points. Furthermore, the emotion labels \nfor the test data with the original causes and generated causes agree with each other 100% of the \ntime. In terms of sentiment analysis, the sentiment of the test data with the original causes matches \nthe sentiment of the te st data with the generated causes for 9 out of 10 data points.  These results \nsuggests that the generated causes are meaningful and relevant to the emotion expressed in the \nsentence. \n 105 \nWe also highlight the need for large -scale human evaluation of the generat ed causes when \nevaluating the performance of techniques to perform the proposed ECG task.  \nIn summary, we believe that this research work demonstrates the viability of the proposed ECG \ntask, particularly as an “infilling” text generation task. We anticipat e that this work will serve as \nthe foundation for further research focusing on developing novel techniques to perform the ECG \ntask.  \n4.10 Limitation and Future Work \nThe focus of this study is to introduce the new task of “Emotion Cause Generation” (ECG) and \ndemonstrate the viability of the task through technical implementation. The technical \nimplementation itself is not the primary focus of the study. There is a vast opportunity to develop \nnovel technical solutions with regard to the proposed ECG task. We plan t o navigate these areas \nin our future studies. \nApart of individual human differences, there can be many common collective factors that can have \nimpact on the cause of an expressed emotion , such as culture, geography, political and economic \nstatus of a regio n etc. For example, people living in regions where snowfall is common can \nassociate many emotions with snow, which may not be the case for people living regions where \nsnowfall never happens. In this study, we present ECG in a generic and simplistic manner, and we \nmodified an existing infilling mechanism to demonstrate how ECG can be performed in general \nas an infilling task. There are opportunities to introduce more sophistications to the ECG task by \nbringing in factors such as cultural context in the cause  generation process. For example, we can \nfurther train a machine learning model that can perform the ECG task with awareness of a specific \nculture by infusing it with specific cultural information. We believe these are interesting avenues \nto explore in future studies related to the ECG task. \nThe evaluation of the technical solution used in this study to perform the proposed ECG task was \ndone on a limited dataset of 10 generated causes containing texts. This was due to the scope of the \nstudy, which was primarily to introduce the ECG task along with assessing its viability as an NLP \ntask and not to propose a novel machine learning architecture to perform the proposed task. \nWe perform  a manual inspection of the generated causes to understand their meaningfulnes s \ncoupled with several quantitative analyses. This manual inspection by the researchers would not \n 106 \nhave been possible for a large number of datapoints. In future, particularly for studies where novel \ntechnical solutions for the ECG task are proposed, we recommend performing analysis on more \ntest datapoints and conducting human-evaluations of the generated texts through distributed user \nsurveys.  \nIn addition to human -evaluations, which can be tedious and difficult to scale, a worthwhile \ninvestigation can be attempting to find and / or develop automatic evaluation metrics for the ECG \ntask. While the metrics for general linguistic quality is useful, they are not capable for evaluating \nthe relevancy or meaningfulness of the generated causes. The other automatic m etrics we used \nsuch as BLEU are typically used for more structured generative tasks such as machine translation \nwhere a finite set of correct answers exists. However, for open -ended tasks such as the proposed \nECG task, the correct answers for a single data  point can be numerous. As a result, metrics such \nas BLEU has less utility for the ECG task. We believe these indicate the need for more automatic \nevaluation methods for open-ended generative tasks such as the one proposed in this chapter.  \n4.11 Conclusion \nIn this study, we introduce  the ECG task, the first generative NLP task in the ECA domain that \naims to generate meaningful cause-spans for the emotion expressed in a given text. We customize \nan existing text-infilling mechanism and fine -tune it further to demonstrate the viability of the \nECG task. We observe that the fine-tuned model is able to generate reasonably meaningful causes \nfor most of the example inputs. We emphasize that the focus of this work is to introduce the new \nECG task as a viable NLP task while acknowledging the limitations in our technical demonstration \nand evaluation. We hop e that this work will inspire researcher communities to develop novel \ntechniques to perform the proposed ECG task. \n  \n 107 \nChapter  5: ECSGen and iZen: A New NLP Task and a \nZero-shot Framework to Perform It \nBuilding upon the previous chapter where we introduced the first-ever generative NLP task in the \nEmotion-Cause Analyses (ECA) domain, in this chapter, we introduce another novel generative \ntask within the ECA domain named, Emotion-Cause mitigating Suggestion Generation (ECSGen). \nThe objective of this task is to generate pertinent suggestions to alleviate the underlying reason for \na negative emotion conveyed in a given text. We also propose iZen, a technical framework to \nperform this task in a zero -shot manner, without requiring any new training or fine -tuning steps. \nWe cur ate new datasets to evaluate iZen’s ability to perform the ECSGen task. Our rigorous \nexperiments and analysis, which included human evaluations and some complementary automatic \nmetrics, suggest that iZen is capable of generating relevant suggestions as par t of the ECSGen \ntask.  \nThe work presented in this chapter has been submitted to one journal4. \n5.1 Introduction \nMachine learning algorithms process a large amount of data to generate machine learning models. \nThese models then can be used to perform the intende d tasks, such as Sentiment Classification, \ntranslation, and face recognition. Generally, machine learning algorithms learn by optimizing loss \nfunction (i.e., objective function) [224]. This optimization takes place by iterating through a vast \namount of r elevant data (i.e., training data), and adjusting the internal model parameters \naccordingly. [1] \nThere are many different algorithms to train machine learning models. Some common among them \ninclude, but are not limited to, Linear Regression, Logistic Regression, Random Forest Algorithm, \nK-Nearest Neighbor (K -NN) Algorithm, and different variants of Artificial Neural Network \n(Convolution Neural Network, Recurrent Neural Network, Long Short-Term Memory, etc.) [225], \nand Transformer (which introduces the notion of self -attention [95]). Transformer enabled the \n \n4 [IN REVIEW] Riyadh, M., & Shafiq, M. O. (2023). ECSGen and iZen: A New NLP Task and A Zero-shot Framework to \nPerform It. \n 108 \nrapid development of large pre-trained language models (LM). Over the past few years, many such \nlarge-scale pretrained models have been released, typically containi ng billions of parameters. \nSome notable examples include Bidirectional Encoder Representations from Transformers \n(BERT) [120], Generative Pre-trained Transformer (GPT) [127] etc. Facebook recently released \none of the most advanced open -source pretraine d LMs, named Open Pretrained Transformer \n(OPT), which has 175 billion parameters in its largest variant [130]. These LMs can generate \nmeaningful texts based on the prompt provided without requiring any further training. A process \nnamed fine-tuning [120] is typically performed to use these models in specific downstream tasks \nsuch as question answering, summarization, and sentiment analysis [226]–[230]. This fine-tuning \nprocess provides a way to control text generation for different outcomes [128]. Although fine-\ntuning process leverages the advantages of pre -trained models, it is still a training process that \nrequires curated datasets and compute-resources.  \nApart from fine-tuning, another method to control text generation from LMs is tuning the prompt \nprovided to it in order to elicit the desired response. Recently, many researchers have focused on \ndeveloping prompt engineering techniques [209], [231]. We believe that the prompt engineering \ntechniques and their possible applications within NLP can be nefit from further investigation, \nparticularly in designing plausible novel NLP tasks and related techniques to perform those tasks. \nAs part of this study, we are interested in exploring this area further, particularly within Emotion-\nCause Analysis (ECA) [232], which is a specific domain within the NLP field.  \nIn recent years, the ECA research area has flourished significantly [90], [194], [207], [233], [234]. \nHowever, these studies are largely focused on classification tasks, with little to no emphasis on the \nutility of LMs and their text generation capability. We introduced the first generative ECA task, \nnamed Emotion Cause Generation (ECG) [see Chapter 4] . In ECG, we attempted to generate a \nrelevant cause-span for the given emotion expressed in a sentence. We positioned the ECG task as \nan “infilling” task [202], [235], where the cause -span was masked. We customized an existing \nLM-based infilling technique [202] to generate a relevant cause-span in place of the mask.   \nBuilding upon these studies, p articularly the introduction of ECG [see Chapter 4] , the potential \nutility of LMs in ECA related research, and various techniques to use these LMs without any new \ntraining steps, we propose a new ECA -related generative NLP task, named ECSGen ( Emotion-\n 109 \nCause Mitigating Suggestion Generation) and a zero -shot [236] framework, named iZen to \nperform this task.  \n5.2 Contributions \nThe specific contributions of the research work described in this chapter include the following: \n• We propose a novel generative NLP task wi thin the ECA domain named  Emotion-Cause \nmitigating Suggestion Generation (ECSGen) that aims to generate suggestions to alleviate the \nunderlying reason for a negative emotion expressed in a given text. \n• We devise a framework named iZen that can perform this task in a zero -shot manner, i.e., \nrequires no specific training or labeled data to perform this new task. \n• We create our own datasets containing input statements for iZen to perform the ECSGen task. \nWe make one of our curated datasets publicly available, which contains 200 input statements, \n6 generated suggestions by iZen for each of these input statements (a total of 1200 suggestions), \nalong with the relevancy score (provided by human participants) for each suggestion and \ninformation on contextual richness of the input statements. We expect this dataset to encourage \nfurther research in this area. \n• We perform rigorous experiments along with human evaluation in order to understand iZen’s \ncapability to perform the proposed ECSGen task. \n \n5.3 Related Studies \nRapid advancement in machine learning technologies is having a significant impact on numerous \nareas. NLP is one of the prime examples of such areas. A relatively recent addition to machine \nlearning technology is Transformer [95], which acted as a cat alyst to develop more sophisticated \nmachine learning models, such as large language models (LM) [128]. These LMs and their unique \ncapabilities inspire a plethora of new research studies in NLP. For example, Dathathri et al. [237] \nintroduced Plug and Play Language Model (PPLM) which incorporates an LM with one or more \nclassifiers to guide the text generation without requiring any additional training or fine -tuning. \nFollowing this work, several other Attribute -based Controlled Text Generation (CTG) [238] \nrelated techniques emerged [239]–[241], such as Future Discriminators for Generation (FUDGE) \n 110 \n[242], which only requires access to the logits of the language model output for imposing a variety \nof controls (e.g., formality) on the text generation.  \nLMs such as OPT [130] have significant reliance on the “prompt” (i.e., an initial snippet of text \nbased on which LMs generate more text) in terms of deciding what to generate. This led many \nresearchers to investigate designing prompts in various ways in or der to get different desired \noutcomes from the LMs. For instance, Yang et al. [243] introduced Text -attribute Controller \n(Tailor) which is a prompt -based approach to control text generation. They propose mechanisms \nto control text generation with single and multiple attributes. However, their multi -attribute text \ngeneration required training a separate component that converted two single attributes to a multi -\nattribute controller. In addition, their method was also limited to two attributes, leaving room for \nfurther exploration in this area. These approaches are also sometimes referred to as “prompt \ntuning”. Like Tailor [243], these approaches may involve some sort of training in order to generate \nthe appropriate prompt. Many other examples of studies wi th a similar goal exist in the literature \n[209], [231], [244], [245] . The main intuition behind this type of prompt tuning is to trigger a \ndesired response from the LM with task relevant prompt parameters without needing to update any \nparameters in the original LM.  \nOne of the goals of this study is to explore the concept of prompt tuning in the domain of ECA \n[232], which is a specific research area within NLP. Within ECA, several different tasks exist \ntoday. For example, Emotion-Cause Extraction (ECE),  Emotion-Cause Pair Extraction (ECPE) , \nand Emotion Cause-Pair Span Extraction (ECSP) [[76], [92], [194], [206], [246]]. \nLee et al.  [77] proposed the ECE task, which was the first task within the ECA domain . The \nchallenge was to identify the reason for a particular emotion that was represented in a paragraph. \nFurther investigations, including those utilizing rule -based and conventional machine learning \ntechnologies, were conducted after th is introductory work ([9], [203], [205] ). Soon after, more \nsophisticated deep-learning based methods for performing ECA tasks appeared. These included a \ntechnique developed by Gui et al. [87], which leverages a question-answering model and a deep \nmemory network to de termine the source of emotion . Another example was the usage of a \nConvolutional Neural Network in accomplishing the ECA task [204]. \nThe design of more ECA related innovative tasks followed in the subsequent years. For example, \nXia et al.  [90] introduced ECPE, which classifies emotion in a text and detects the cause of that \n 111 \nemotion. Leveraging BERT, Fan et al. [207] reconstructed the ECPE problem as a method of \ndirected graph creation. These ECA task variations identified emotion-cause at a clause-level. An \nissue with this was that these clauses often contained information irrelevant to the cause of the \nemotion. Li et al. [194] attempted to address this issue with their proposed ECSP task that extracts \nthe span of the cause from a given sentence inst ead of detecting the cause -clause from multiple \noptions. We introduced the first generative task within ECA, named ECG [see Chapter 4], which \nwe discussed in the previous chapter . For ECG, the task is to generate a relevant cause of the \nemotion expressed in an input statement, where the “cause-span” is masked in that statement. This \nstudy is largely inspired by the ECG task and builds upon it to propose another novel generative \nNLP task within the ECA domain, named ECSGen. \n5.4 ECSGen: The Proposed Task \nIn this study, we propose a new ECA -related generative NLP task, named ECSGen ( Emotion-\nCause Mitigating Suggestion Generation). It takes a given sentence that expresses a negative \nemotion (e.g., sadness, frustration) and an underlying cause of that emotion as inpu t and aims to \ngenerate relevant suggestion(s) (e.g., an activity or idea) to mitigate that emotion. For instance, if \nthe input statement is - “I am bored because I don’t enjoy this game anymore ”, then an expected \nrelevant suggestion could be - “Maybe try a new game!”. Figure 26 demonstrates this example. In \nthis instance, the emotion expressed is “bored”, the causal conjunction used is “because”, and the \ncause of the emotion is, “I don’t enjoy this game anymore”.  \n \nFigure 26: An example of the ECSGen task. The input statement expresses an emotion with an \nassociated cause and the ECSGen task requires generating relevant suggestion(s) to mitigate the \ncause behind the emotion \n\n 112 \n \nFigure 27: ECSGen and other ECA tasks \nFigure 27 demonstrates how ECSGen fits into the ECA domain among other NLP tasks. ECSGen \nis the second NLG task in this domain. \n5.5 iZen: A Framework to Perform ECSGen \nIn this study, we propose a framework, named iZen  (i.e., the name iZen does not possess any \nspecific meaning), to perform the proposed ECSGen task . We first describe the scope of iZen in \nrelation to the proposed task, and then we describe its technical architecture. \n5.5.1 Scope of the Proposed Framework: \niZen expects input with certain constraints. The desired input text is a simple sentence where a \nperson expresses his/her emotion and states a cause behind that emotion. iZen requires the input \nto be formulated in the following specific pattern: \nI am <express emotion> <causal conjunction> <cause> \nThe following example demonstrates the above pattern: \nI am <feeling sad> <because> <I failed the exam> \nThe pattern is broken down below with further details outlining the required input constraints: \n1. The input text starts with “singular first person” (i.e., “ I”). In the input text, an emotion is \nexpected to be desc ribed in “singular first person”. For example, “ I am feeling sad ”, “I am \nbored”, “I feel frustrated” etc. \n\n 113 \n2. Immediately after the emotion is expressed, it is followed by a causal conjunction such as \n“because”, “because of the fact that” etc. \n3. After the causal conjunction, the cause of the emotion is stated. In the above example, “I failed \nthe exam” is the cause behind the emotion expressed in that sentence. \nPoints 1 and 2 from the above list together form the “beginning part” (expression of emotion and \ncausal conjunction) of the input text. In this study, the following “beginning parts” were used: \n• “I feel disgusted because”,  \n• “I feel down because”,  \n• “I feel bored because”,  \n• “I am bored because”,  \n• “I am feeling sad because”,  \n• “I am annoyed because”,  \n• “I am frustrated because”,  \n• “I feel sad because of”,  \n• “I am angry because of the fact that”,  \n• “I feel angry because” \nThe negative emotions used in the “beginning parts” included, “ sadness”, “ anger”, “ disgust”, \n“annoyance”, “frustration” etc. We selected these emotions from the collection of various types \nof emotions in Plutchik’s Wheel of Emotion [11].  \nThe zero -shot nature of iZen obviates the need for additional training, fine -tuning, and \nhyperparameter tuning. The selection of off -the-shelf (OTS) components within the iZen \narchitecture was based on a variety of criteria, including their domain usage, reported superior \nperformance in specific tasks, and availability in a suitable Python package. For instance, the \nwidely used \"twitter -roberta-base-sentiment\" model [220] from the Cardiff NLP research group \n[247], which satisfies most of these criteria, was utilized. Furthermore, before final integration \nwithin the architecture, we evaluated the suitability of the selected OTS components for the \nspecific task segment in iZen through experimentation during the development phase.  \n5.5.2 iZen’s Architecture: \niZen is comprised of 4 main components:  \n 114 \n• PromptGenie (P) \n• Zen (Z) \n• Eliminator (E) \n• Guardrail (G) \nThe adjacent letters represent the symbols used to identify  these components in this study. Each \nof these components is described below. \n \n \nFigure 28: iZen Framework’s Architecture: PromptGenie, Zen, Eliminator, and Guardrail are \ndenoted with P, Z, E, G (respectively). PC refers to PromptGenie's Cause-span Extractor, PP \nrefers to Prompt Prefix Generator, PS refers to Cause-span Simplifier, and PG refers to Grammar \nCorrector. EG, ET, EO, EP refer to Eliminator’s calculators for Grammatical Correctness, \nToxicity, Optimism, Positivity respectively. Outputs from PromptGenie, Zen, and Eliminator are \nindicated with p, z, and y respectively. Guardrail outputs are denoted with y or ∅ (null or no \nsuitable suggestion generated). \n\n 115 \n5.5.2.1 PromptGenie (P) \nPromptGenie is predominantly a rule-based prompt generator for ECSGen. Given an input in the \nexpected format described in the previous section, PromptGenie generates a prompt that gets \nsupplied to the next component in the pipeline in iZen: Zen. The generated prompt is significantly \nimportant for the eventual outcome of iZen since it is the input used by Zen that generates the \ncandidates for desired suggestions.  \nAs described in the previous section, the expected input (x) has the following format: \nI am <express emotion> <causal conjunction> <cause> \nThe purpose of PromptGenie is to take the “cause” part of the original input and leverage it to form \na text in a manner that catalyzes a reversal of the cause. Given an original input statement (x), \nPromptGenie generates a prompt (p) in the following pattern: \nIf you <want | don’t want> <text span crafted based on the cause-span in x> \nTo generate the desired prompt (p) from the given input statement (x), PromptGenie goes through \nthese four main steps: \n1. Extract cause-span  \n2. Generate prompt prefix \n3. Simplify the identified cause-span \n4. Grammar correction \nThese four operations are supported by PromptGenie’s four different sub-components (in the same \norder as the steps mentioned above): \n1. Cause-span Extractor (PC) \n2. Prompt Prefix Generator (PP) \n3. Cause-span Simplifier (PS) \n4. Grammar Corrector (PG) \nTo elaborate on PromptGenie’s process, we can consider the same example in the previous section:  \nI am <feeling sad> <because> <I failed the exam> \n 116 \nHere, the cause part is the following: “ I failed the exam ”. PromptGenie’s Cause -span Extractor \n(PC) first extracts this cause part (pc) based on the rules developed around the “causal conjunction” \nset used in this study (i.e., as mentioned in the previous section: “because”, “because of” etc.).  \nAfter the cause is identified and separated, PromptGenie’s  Prompt Prefix Generator (PP) generates \na prefix (pp) to be added to the cause. The prefix is determined based on the cause. For example, \nfor the cause “I failed the exam”, the generated prefix would be, “If you don’t want to”.  \nThe extracted cause (p c) is further simplified (p s) by the Cause -span Simplifier ( PS) before the \nprefix is added to it. For instance, it is transformed to present tense, and the subject is stripped out \n(e.g., “I”). The example cause then becomes: “fail the exam”. After this, PromptGenie adds a prefix \nto this simplified version of the cause: “ If you don’t want to fail the exam ”. PromptGenie also \nincludes logic to handle cases where the original cause contains a negation. For example, if the \ninput is the following: \n“I am sad because I did not pass the exam.” \nThen, PromptGenie generates the following prompt: \n“If you want to pass the exam”. \nFinally, PromptGenie’s Grammar Corrector ( PG) leverages an off -the-shelf (OTS) pretrained \ngrammar corrector model [248] in order to rectify grammatical errors that can be caused during \ndifferent steps of rule-based transformations in the prior steps mentioned. \n5.5.2.2 Zen (Z) \nZen is the main generator component of iZen that generates the candidates (zi, where i ∈ {1...11}) \nfor the desired suggestions based on the prompt (p) supplied by PromptGenie. Zen is primarily a \nwrapper around the pretrained language model (LM) of choice. In this study, we included two LM \noptions for Zen, – OPT [130] and GPT2 [128]. We chose OPT as it is one of the latest and most \nadvanced general purpose open -source LM available publicly. Particularly, the “opt -1.3b” [249] \n(i.e., contains 1.3 b illion parameters) variant of OPT was used as larger variants with more \nparameters required significantly more computing resources than the resources that were available \nto us (i.e., Google’s Colab Pro [250]). GPT2 is chosen due to its significant adoption across NLP \nresearch as well as the fact that it is open source and researchers have unrestricted access to it, \n 117 \nunlike some other LMs such as GPT3 [129]. We used the “gpt2-xl” variant [251] as it is the closest \nto our OPT variant of choice in terms of the number of parameters (i.e., 1.5 billion). \nWithin Zen, we leverage many of the default configurations [252], [253]  for OPT and GPT2 \nmodels with some customizations to suit the specific needs of this study. For instance, we include \na custom “Stopping Criteria” function within Zen that dictates when it must stop generation. We \nuse some characters to signal Zen to stop text generation, which includes, full stop or period (“.”), \nnew line character (“ \\n”), exclamation mark (“!”), and semi -colon (“; ”). Zen sets a few other \nconfigurations for the LMs in order to suit the purpose of the study. This included setting the \nmaximum number of new tokens generated by the LMs to 100, and setting the minimum generation \nlength to 30 characters. Since our goal wa s to generate short and simple suggestions, we did not \nwant our suggestions to be more than one sentence long, which is supported by our custom \n“Stopping Criteria” functions along with these LM configurations. The minimum length condition \nwas used to ensur e there is sufficient room for meaningful text. In addition, we leveraged \n“multinomial sampling” as our decoding strategy [254], which allows for generating different \noutputs for the same input in each inference. For each input, Zen generated 11 candidat e \nsuggestions (the choice of this number is explained in the next section), which are then fed into \nthe next component in iZen’s pipeline: Eliminator.  \n5.5.2.3 Eliminator (E) \nEliminator sits next to Zen in iZen’s architecture where it takes in 11 generated suggest ions (zi, \nwhere i ∈ {1...11}) from Zen against one input statement (x). This component is a hybrid of rule -\nbased and machine-learning based mechanisms. It is built on top of some OTS pretrained models \nand libraries. The goal of this component is to find the top 3 suggestions (y1, y2, y3) out of the 11. \nThe elimination of the rest of the suggestions occurs based on certain elimination criteria: \ngrammatical correctness, optimism, positivity (i.e., positive sentiment), and toxicity. The selection \nof these criteria is mainly inspired by a relevant prior study [255]. In addition, we use “toxicity” \nas a criterion as LMs can generate toxic content depending on the prompt [129]. Also, noteworthy \nthat with the intent to make Eliminator more robust, we used optimi sm and positivity as two \nseparate criteria, filtered using two separate OTS pretrained models, though they may sound \nsimilar.  \n 118 \nEliminator operates on two primary steps: (1) Calculate candidate suggestions’ scores for different \nelimination criteria, (2) Ide ntify top-k (for this study, k is 3) suggestions among the generated \ncandidate suggestions based on these scores. Step 1 accommodates different sub -steps. As \nmentioned above, for this study, these sub-steps include the following: \na. Calculate grammatical correctness score (g_s) \nb. Calculate toxicity score (t_s) \nc. Calculate optimism score (o_s) \nd. Calculate positivity score (p_s) \nThese four steps are supported by four different sub -components of Eliminator, which are based \non three separate OTS pretrained models and a Python library. These are listed below in the same \norder as the steps: \na. Grammatical Correctness Calculator (EG): It is based on the “ language-tool-python” package \n[256] \nb. Toxicity Calculator (ET): This component is based on the “toxic-bert” pretrained model [256] \nc. Optimism Calculator (E O): This component leverages the “ twitter-roberta-base-emotion” \npretrained model [257] \nd. Positivity Calculator (EP): It is based on the “distilbert-base-uncased-finetuned-sst-2-english” \npretrained model [258] \nAfter these subcomponents calculate the elimination criteria scores (e scores), the second main step \nof the Eliminator takes place. This step is carried out by the fifth and final sub component of the \nEliminator, named Selective Deduction Unit (E SD). E SD algorithm eliminates the two least \nfavorable suggestions for each criterion in the following order:  \n• First, it eliminates the two suggestions out of 11 that contain the most grammatical  errors. If \nthere are suggestions with the same number of grammatical errors, then two among them are \nchosen randomly. At the end of this step, nine suggestions are remaining. \n• In the second step, Eliminator removes the two most toxic suggestions from the remaining nine \nsuggestions, leaving seven suggestions for further filtering. \n• Third, Eliminator removes the two least optimistic suggestions from the seven. At the end of \nthis step, five more suggestions are remaining. \n 119 \n• Lastly, Eliminator discards the two lea st positive suggestions from the five supplied in the \nprevious step. Finally, this leaves us with the top 3 suggestions. \nThe ESD algorithm was designed to function on a “deduction unit” at each step. For this study, this \ndeduction unit was two and we had four steps (or criteria). As a result, 11 outputs get reduced by \ntwo at each step and eventually resulting in the top 3 outputs.  \nAs described above, E SD follows a greedy algorithm  [259] approach (i.e., makes locally optimal \nchoices at each step with the hope of finding a global optimum ) in a specific order: grammatical \ncorrectness, toxicity, optimism, positivity. The reason for t his is to remove the grammatically \nincorrect and potentially toxic suggestions first as they are critical to remove. The last two criteria \nfurther filter the rest of the suggestions to eventually select the final three suggestions. However, \nit is worth men tioning that this can be achieved in a few other apparent ways instead of using \ngreedy algorithm approach. For example, we  can sum all the scores from the Eliminator \nsubcomponents using appropriate weights (e.g., if higher weight indicates  more likely candidate \nfor removal, then toxicity and grammatical correctness should have more weights than the other \ncriteria) in order to select the top three suggestions.  \nThe top 3 suggestions  selected by E SD then can either be chosen as the final output or can be \nforwarded to the next but optional component of iZen: Guardrail, which we describe below. \n5.5.2.4 Guardrail (G) \nGuardrail is an optional safeguard component that aims to automatically discard an entire output \nif there is any serious concern identified in any part of t he top 3 suggestions. iZen’s Guardrail \ncomponent checks for profanity in the top 3 suggestions supplied by Eliminator, leveraging a \nPython library named “profanity-filter” [260]. If there is any profanity identified by Guardrail in \nany of the top 3 suggestions for an input statement, then the entire output is discarded. This means, \nGuardrail either outputs the same top 3 suggestions ( y1, y2, y3) supplied by the Eliminator if they \nqualify as the final output of iZen for a given input statement (x), otherwise it outputs ∅ (null or \nno suggestions). \nIn this study, we opted to use this component in order to further safeguard against unintended \noutputs. Eliminator, with its filtering based on toxicity, positivity, and optimism, provides a robust \nmechanism to filter out unintended texts. However, based on our observation, even with such \n 120 \nrobust filtering, there were still some suggestions among the top 3 that included profanity. Though \nthis represents a negligible amount of the generated suggestions in our experiment (about 1.6%), \nnonetheless, we opted to include this final Guardrail component as an additional safety net in order \nto ensure, as much as pos sible, that iZen’s outputs do not include any profanity. For this first \nrelease of iZen, we chose to discard all top 3 suggestions when profanity was identified by the \nGuardrail component in any of the suggestions. This is to maintain iZen’s input/output consistency \n(i.e., three suggestions for one input statement). \nThe reason we chose to have the Guardrail as a separate component and not as a part of the \nEliminator is because of the distinct goals of these two components. Eliminator essentially ranks \nthe generated suggestions and selects the top 3 among them based on some elimination criteria. \nWhereas Guardrail’s criteria (in our case, it is profanity; however, we expect this to be extensible \nto other factors in the future) are such that even if there is a little profanity for example, it may \nneed to be excluded – it is intentionally more binary and stricter in nature. As a result, we believe \nGuardrail serves best as a separate last optional checkpoint instead of being combined with the \nEliminator component which is more akin to ranking systems.  \n5.5.2.5 iZen’s Overall Algorithm \nAs discussed in detail in the previous section, iZen takes in an input statement, x, and attempts to \ngenerate three relevant suggestions, y 1, y2, y3. Algorithm 3 summarizes iZen’s overall process to \ngenerate suggestions for each input as described in the previous section:  \n• First, PromptGenie’s (P) Cause-span Extractor (P C) subcomponent takes in the input \nstatement (x), and then it goes through the other three subcomponents of PromptGenie in \nseries. p is the final output from PromptGenie. \n• Zen (Z) generates 11 candidate suggestions: {z1…, z11} \n• Next, the four calculators of the Eliminator component (EG, ET, EO, EP refer to Eliminator’s \ncalculators for Grammatical Correctness, Toxicity, Optimism, P ositivity respectively)  \ncalculates the respective scores for each of the generated suggestions and appends them in \nescores \n• The fifth and the final subcomponent of the Eliminator, named Selective Deduction Unit \n(ESD) takes in {z1…, z 11} and the associated  escores and selects the top three generated \nsuggestions: {y1, y2, y3} \n 121 \n• The last safeguard component, Guardrail (G) checks for profanity in {y1, y2, y3}  and passes \nthem as the final output of the iZen  architecture if no profanity if found. In case any \nprofanity is found, it outputs null (∅) \n \n  \n 122 \nAlgorithm 3: iZen’s Overall Algorithm. Meaning of symbols used: PromptGenie, Zen, \nEliminator, and Guardrail are denoted with P, Z, E, G (respectively). PC refers to PromptGenie's \nCause-span Extractor, PP refers to Prompt Prefix Generator, PS refers to Cause-span Simplifier, \nand PG refers to Grammar Corrector. EG, ET, EO, EP refer to Eliminator’s calculators for \nGrammatical Correctness, Toxicity, Optimism, Positivity respectively. Outputs from \nPromptGenie, Zen, and Eliminator are indicated with p, z, and y respectively. Guardrail (G) \noutputs are denoted with y or ∅ (null or no suitable suggestion generated). \nInput(s): x \nOutput(s): {y1, y2, y3} or ∅ \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n11 \n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n// PromptGenie steps \npc  PC(x) \npp  PP(pc) \nps  PS(pc) \npd  pp + ps \np  PG(pd) \n \n// Zen step \n{z1…, z11} Z(p)  \n \n// Eliminator steps \ndeclare escores: collection of scores from Eliminator subcomponents \n \nfor z ∈ {z1…, z11} do \n      g_s  EG(z)  \n      t_s   ET(z)  \n      o_s  EO(z)  \n      p_s  EP(z)  \n      append {g_s, t_s, o_s, p_s} in escores \nend for \n \nESD(z1:11, escores)  {y1, y2, y3} \n \n// Optional Guardrail step \n{y1, y2, y3} or ∅  G(y1:3)  \n \nreturn: {y1, y2, y3} or ∅ \n5.6 Experiments and Evaluation \n5.6.1 Procedure \n5.6.1.1 Suggestion Generation \nThe initial steps in setting up our experiments involved developing iZen’s pipeline (see section 5.5 \nfor more details). This pipeline was then used to generate suggestions for inputs contained within \nour curated datasets (see sub -section 5.6.3 below). Our experiments included four variations in \nterms of input dataset and LM used in iZen’s Zen component. We used two  input datasets, each \n 123 \ncontaining 200 input datapoints for iZen. This sums up to 400 uniq ue input datapoints. As LMs, \nwe used OPT and GPT2 (see section 5.5.2.2 for more detail). Therefore, iZen with OPT generated \nthree suggestions for each of the 400 datapoints, and iZen with GPT2 generated three suggestions \nfor each of the 400 datapoints. This sums up to 800 input datapoints for iZen, and for each of them, \nthree suggestions were generated, summing up to 2400 suggestions. Figure 29 visually summarizes \nthis for further clarity. \n \nFigure 29: An overview of how 800 input statements (400 unique) resulted in the final 2400 \nsuggestions in this study. \nAs mentioned in the earlier sections, we opted to use iZen’s optional Guardrail component in order \nto further ensure that iZen’s outputs are free of profanity. In our experimental setup, after the \nEliminator component selects the top 3 suggestions, it goes through the final “Guardrail test” (i.e., \nfiltered based on Guardrail’s configuration). After the top 3 suggestions “pass” the “Guardrail test” \n(representing about 98.4% of the suggestions in our experiments), they are accepted as the final \noutputs of iZen. Since the suggestions that “failed” the “Guardrail test” are negligible (only 1.6% \nof the generated suggestions), we excluded them from our analysis for simplification.  \nConsequently, the results reported in the subsequent sections are based on 98.6% of the datapoints \n(i.e., this represents the 800 input statements mentioned above), and the remaining 1.6% is \n\n 124 \nconsidered as the overall error margin. We opted to accept this error margin in our reported result \ndue to its insignificance and in exchange for the additional safeguard it provided for our generated \noutputs which we present to our human evaluators.  \n5.6.1.2 Evaluation of the suggestions \nOur goal with iZen is to generate simple and relevant suggestions to mitigate the emotion -causes \nexpressed in the input texts. iZen’s internal mechanisms ensure that the generated texts are simple \n(i.e., keeping the text short, one sentence long). However, in order to “measure” relevancy (i.e., \nhow relevant generated suggestions are in relation to the input statement containing an emotion \nwith an associated cause), the suggestions are required to be evaluated by humans. We consider \nthis as the primary method for evaluating the generated suggestions for their expected quality in \nthis study. In addition, we also leverage some automatic metrics as complementary measurements \nto evaluate the generated suggestions. The following sections include details regarding both types \nof evaluation. \n5.6.1.3 Human Evaluation \nData Collection: We conducted a survey with 20 participants, aged between 18-50 years. All our \nparticipants were required to be able to read and understand English. We designed the survey using \nQualtrics [261] and distributed it online using Amazon MTurk [262]. We leveraged some of the \noptions provided by these tools to ensure the integrity of the survey responses. For instance, we \nrequired our survey participants to have certain MTurk System Qualification Scores (e.g., \n“Number of HITs Approved ”: above 10,000 and HIT “Approval Rate”: above 90%) in order to \nensure their re liability. We also examined the survey responses manually in order to identify \noutliers based on criteria such as completion time (i.e., if a participant completed the survey \nextremely faster compared to the other participants), and sanity check of the res ponses (i.e., if \nmany of the responses seemed nonsensical based on our observation. For example, if many of the \nobvious completely irrelevant suggestions were marked as “ Very relevant” and vice versa). We \nreleased our survey in several batches through MTurk until we received 20 valid sets of responses \n(i.e., a total of 26 participants completed our survey, and among them, 6 were discarded based on \nthe criteria mentioned above). \n 125 \nThe survey contained the 800 final datapoints used in iZen, including 800 input texts and three \noutput suggestions for each. As mentioned above, these datapoints can be categorized into four \ngroups based on the input dataset used for generation and LM used within iZen. Consequently, \neach of these four groups contained 200 input datapoints.  \nEach participant in our study was presented with an equal number of questions (i.e., 10) from each \nof the four categories, which summed up to 40 datapoints per participant. Each of these 40 \ndatapoints constituted a question set. A question set inclu ded the original input statement that \ncontained an emotion and an associated cause and the three generated suggestions from iZen. We \nasked the participants to rank the relevancy of each suggestion in relation to the original input \nstatement using a 5-point Likert scale. In order of relevancy (high to low), our Likert scale labels \nwere: “Very relevant”, “Moderately relevant”, “Somewhat relevant”, “Slightly relevant”, and “Not \nat all relevant ” (label names are inspired by prior studies and guidelines [263], [264]). We also \nasked them to share their opinion on whether the original text contained sufficient contextual \ninformation in order to provide a relevant suggestion. At the end of the survey, the participants \nwere also presented with two open-ended questions to share their opinion on their overall thought \nabout the suggestions (i.e., what they liked and disliked about the suggestions). We included some \nsample questions in the Appendix to demonstrate the overall structure of the survey.  \nWe have attained et hics clearance from Carleton University Research Ethics Board -B (CUREB-\nB)5 to perform this evaluation with human participants. \nData Analysis: To analyze our collected data, primarily Likert scale responses which represent \nour intended measurements, we report the percentages of different labels of our Likert scale. For \nexample, we report how many of our survey participants (in percentage) found at least one of the \ntop three suggestions “somewhat or more relevant” for each question -set, or how many of the \nparticipants thought all of the three generated suggestions were “Not relevant at all”. In addition, \nwe report median relevancy scores of all the generated suggestions. We also observe the effect of \ndifferent factors, for example, across four different categories of the data inputs as discussed above \n(i.e., in terms of the dataset we used, and LM utilized in iZen) on the relevancy scores using \nstatistical methods. \n \n5 CUREB-B Ethics Clearance ID: 118094 \n 126 \n5.6.1.4 Automatic Metrics \nTo the best of our knowledge, there are no standard automatic metrics that can be leveraged to \nevaluate the “relevancy” of the generated suggestions against the original input statement as \nrequired by the proposed task ECSGen. Furthermore, since this is a newly proposed task, there is \nno precedence exists in conducting such evaluations. However, based on our experience in \nresearching this area and developing ECSGen along with iZen, and some related literature from \nother domains (e.g., [255]), we selected a few metrics with available technical solutions to \ncalculate them automatically. These metrics do not necessarily relate to the “relevancy” of the \nsuggestions; instead, we expect them to represent the general quality of the generated suggestions. \nThese include readability, grammaticality, and positivity. We consider these automatic metrics as \ncomplementary to the human evaluation discussed above. \n \n5.6.2 Tools \n5.6.2.1 Software tools for iZen: \nFor iZen we utilized several software tools and machine learning models. These included: \n• Python and standard libraries [265]  \n• Language Tool Python [266] \n• Profanity Filter [260] \n• HuggingFace library [267] \n• Grammar correction model: T5 Grammar Correction model [248] \n• Toxicity model: Detoxify [256] \n• Cardiff NLP’s “Twitter-roBERTa-base” pretrained model  for Emotion Recognition [220], \n[257] \n• “DistilBERT-base-uncased-finetuned-SST-2” pretrained model for Sentiment Analysis [258] \n• Large generative language models: GPT2 [128], OPT [130] \n5.6.2.2 Survey Tools: \nWe used Qualtrics [261] to design and host our survey. The survey was distributed using Amazon \nMechanical Turk (MTurk) [262].  \n 127 \nWe utilized several tools to analyze the survey results. These included:  \n• IBM SPSS [268] \n• Microsoft Excel [269] \n• Python and standard libraries [265] \n• Pandas [270] \n5.6.2.3 Automatic Metrics Tools: \nFor readability scores, we used a Python package named TextDesc riptive [271]. For grammar \ncheck, we used another Python package name Language Tool Python [266].  \nFor Sentiment Analysis, we used Cardiff NLP’s Twitter-roBERTa-base for the Sentiment Analysis \nmodel [257] along with the HuggingFace library [267]. We also used Python and Excel for \nanalyzing and visualizing some of these results. \n5.6.2.4 Hardware Specification: \nWe used cloud hardware resources for our machine learning related workloads. These cloud \nhardware resour ces were available as part of our Google Colab Pro subscription [250]. This \nsubscription provided us with the following hardware specification (maximum available): \n• Processor: Intel Xeon CPU, 2.20 GHz (12 Cores) \n• RAM: 87 GB \n• GPU: NVIDIA A100 SXM4 40GB \nFor o ther compute workloads, we used a Macbook Pro (2019, 16 -inch) with the following \nconfiguration: \n• Processor: Intel Core i7, 2.6 GHz (6 Cores) \n• RAM: 16 GB \n• GPUs: \no AMD Radeon Pro 5300M 4 GB \no Intel UHD Graphics 630 1536 MB \n5.6.3 Datasets \n 128 \nSince this study introduces a new  NLP task - ECSGen, we created new datasets as part of this \nstudy to perform the task. We created two datasets: (1) Dataset generated using OPT (we name it: \nECSGen-Data-OPT), and (2) Dataset retrieved from Twitter (we name it ECSGen -Data-TWT). \nWe describe these datasets below: \n5.6.3.1 ECSGen-Data-OPT \nECSGen-Data-OPT dataset is generated using OPT [130]. We chose OPT as it is one of the most \nadvanced and latest general-purpose LMs available publicly. Specifically, we leveraged the “opt-\n1.3b” [249] variant due to its compatibility with our available hardware resources. As prompts for \nOPT, we used the specific “beginning part” texts mentioned in section 5.5.1 in order to generate \ninput sequences with the desired format mentioned in the same section. Using random seeds, we \ngenerated multiple outputs for the same “beginning part” text. After the generation, we inspected \nevery output in order to make sure the data is clean (e.g., does not include vulgarity, or obscenity).  \nThis dataset contains 200 texts that begin with one of the “beginning part” texts mentioned in \nsection 5.5.1. Each text follows the following pattern (as described in section 5.5.1): “ I am \n<express emotion> <causal conjunction> <cause> ”. The static prompts given to OPT already \nincluded the initial parts: “I am <express emotion> <causal conjunction> ” and OPT was tasked \nwith generating the “<cause>” part. OPT was configured in a manner to make sure generated texts \nare simple, one sentence long etc. (similar criteria us ed for the “Zen” component of “iZen” as \ndescribed in section 5.5.2.2. In summary, each datapoint in this dataset has “I” as the main subject, \nand the main subject expresses a negative emotion (see section 5.5.1 for more information) along \nwith a cause associated with the expressed emotion. To benefit researchers with similar i nterests, \nwe make this dataset publicly available 6 along with the top 3 generated suggestions and their \nrelevancy scores based on the responses from our human evaluators. \n5.6.3.2 ECSGen-Data-TWT \nECSGen-Data-TWT dataset is developed using data retrieved from Twitter [272] using existing \nlibraries [273]. This dataset has similar qualities as ECSGen-Data-OPT. For instance, each text in \n \n6 https://github.com/riyadhctg/ECSGen-and-iZen \n 129 \nthis dataset also begins with one of the “beginning part” texts mentioned in section 5.5.1. This was \nachieved by using the “beginning part” texts as query strings.  \nSimilar to ECSGen -Data-OPT, we performed data preprocessing in order to ensure this dataset \nadheres to the data quality and format required to perform the proposed task with the iZen. For \nexample, final datapoints in ECSGen-Data-TWT are one sentence long and grammatically correct, \nthey start with “ I”, and each of them expresses a negative emotion with an associated cause. In \nsummary, ECSGen -Data-TWT is similar to ECSGen -Data-OPT in terms of the content and \nquantity (i.e., 200 total datapoints); the only dis tinction being the source of the content. For \nECSGen-Data-OPT, the content source is OPT, and for ECSGen-Data-TWT, the content source is \nTwitter. \n5.7 Results  \n5.7.1 Automatic Metrics Results: \nAs discussed in the prior sections, we leveraged some automatic metrics in order to understand the \ngeneral linguistic quality of iZen’s generated suggestions. We discuss them below. \nWe found that about 80% of the generated suggestions had no grammatical errors. Approximately \n15% contained 1 grammatical error and another 3% conta ined 2 grammatical errors. Only the \nremaining 2% had 3 or more grammatical errors.  \nWe have used several standard metrics to analyze the readability of the generated suggestions (e.g., \n[217], [218] ). Most of these metrics evaluate the readability of a te xt by age of the reader. \nTypically, a text is deemed easily readable by these metrics when it is easy to read for 13-15 years \nold who are proficient in English [274]. We use this threshold in reporting our findings on the \nreadability of our generated suggestions. For the Flesch Reading Ease metric [217], this threshold \nmeans a score of 60 or above. Approximately 97% of the generated suggestions scored 60 or above \nin this metric. For Flesch Kincaid Grade [274], a score of 8 or below means it is easily readable. \nAbout 92% of generated suggestions had a score of 8 or below. For Gunning Fog metric [274], a \ngood readable score is 13 or below, and approximately 94% of our generated suggestions had a \nscore of 13 or below in this metric. Similarly, according to Automated Readability Index (threshold \n9 or below), Coleman Liau Index (threshold 8 or below), and Lix metric (threshold 40 or below) \n 130 \n[275], about (in the same order) 91%, 93%, and 92% of iZen’s generated suggestions were easily \nreadable. Table  17 summarizes these scores. \nReadability Metrics Easily readable (% of all generated suggestions) \nFlesch Reading Ease 97% \nFlesch Kincaid Grade 92% \nGunning Fog 94% \nAutomated Readability \nIndex \n91% \nColeman Liau Index 93% \nLix 92% \nTable  17: Automatic metrics score of the generated suggestions \n \nFigure 30: Sentiment of the generated suggestions \nIn addition to basic linguistic quality, we also evaluated the generated suggestion on their \nsentiment. Since iZen generates suggestions to address a negative emotion expressed in a simple \nstatement, we expect that the majority of the generated suggestions will be either neutral or positive \ninstead of having negative sentiment. As shown in Figure 30, we found that more than 82% of the \ngenerated suggestions were either neutral (about 69%) or positive (about 14%). \n5.7.2 Human Evaluation Results: \n5.7.2.1 Relevancy scores of the generated suggestions \nAs discussed earlier, our study involved 800 input statements, and for each of them, iZen generated \nthree suggestions, summing up to a total of 2400 suggestions. Our survey participants evaluated \nthese suggestions using a 5-point Likert scale (numeric labels were 1-5, mentioned in the bracket \nbeside each text label): “Not at all relevant” (1), “Slightly relevant” (2), “Somewhat relevant” (3), \n“Moderately relevant” (4) and “Very relevant” (5).   \n\n 131 \nAs shown in Figure 31, among 2400 suggestions, our participants found 17.5% of them “ Very \nrelevant”, 17.8% “Moderately relevant”, 19.4% “Somewhat relevant”, 18.5% “Slightly relevant” \nand 26.8% “Not relevant at all ”.  In order wor ds, about 73% of the generated suggestions were \nslightly or more relevant; approximately 55% were somewhat or more relevant; about 35% were \nmoderately or more relevant.  \n \nFigure 31: Overall relevancy of all the generated suggestions. \nFor each of the 800 input statements, iZen generated three suggestions. We wanted to understand \nhow many of the 800 input statements had at least 1 or more generated suggestions that achieve a \nminimum threshold in terms of the relevancy score. We found  that for 800 input statements, \n91.25% had a t least 1 slightly or more relevant suggestion, 80% at least 1 somewhat or more \nrelevant suggestion, 68.25% at least 1 moderately or more relevant suggestion, and 38% at least 1 \nvery relevant suggestion. We also observed that 76.75% of the input statements had at least  2 \nslightly or more relevant suggestions, 59.25% at least 2 somewhat or more relevant  suggestions, \n28% at least 2 moderately or more relevant suggestions, and 10.875%  at least 2 very relevant \nsuggestions. We found that all three suggestions for each input statement were slightly or more \nrelevant for 51.75% of the input statements, somewhat or more relevant for 25.125% of the input \nstatements, moderately or more relevant  for 9.875%, and very relevant for 3.75% of the input \nstatements. Lastly, we found that for 8.75% of the input statements, all three suggestions generated \nby iZen were irrelevant. We summarize and illustrate these findings in Table  18 and Figure 32. \nRelevancy Percentage \nAt least 1 slightly or more relevant 91.25 \nAt least 1 somewhat or more relevant 80 \nAt least 1 moderately or more relevant 68.25 \nAt least 1 very relevant 38 \n\n 132 \nAt least 2 slightly or more relevant 76.75 \nAt least 2 somewhat or more relevant 59.25 \nAt least 2 moderately or more relevant 28 \nAt least 2 very relevant 10.875 \nAll slightly or more relevant 51.75 \nAll somewhat or more relevant 25.125 \nAll moderately or more relevant 9.875 \nAll very relevant 3.75 \nNone is relevant 8.75 \nTable  18: Relevancy of the generated suggestion per input statements \n \nFigure 32: Relevancy of the generated suggestion per input statements. \n5.7.2.2 Comparison of relevancy scores across different factors \n5.7.2.2.1 Contextual richness of the input statement \nFor each input statement, along with evaluating suggestions using the Likert scale, we also asked \nour participants to evaluate whether or not the original input statement contained enough context \n\n 133 \nor information in order to provide a relevant suggestion. Participants answered this question using \n“Yes” (contains enough context), “ No” (does not contain enough context), or “ Maybe” (unsure \nwhether or not the statement contains enough information). Among 800 input statements (used to \ngenerate 2400 suggestions), 446 received a “ Yes” response, 139 statements a “No” response, and \n215 received a “ Maybe” response from our participants. We performed further analysis to \nunderstand how this score relates to the relevancy score for the generated suggestions. We \nobserved that overall when the input statement contained enough context, the generated \nsuggestions were likely to be more relevant. A p-value of less than 0.001 in a Chi-Square test [77] \nconfirms this impact of context contained within the input statement on the generated suggestion. \nWe illustrate these results in Figure 33 and Figure 34. \n \nFigure 33: Variation of the relevancy of the generated suggestion based on the contextual \nrichness of the input statement. (a) Grouped bar chart (where the y-axis represents the “count” of \nthe suggestions), (b) Stacked bar chart (where the y-axis represents the “percentage” of the \nsuggestions) \n\n 134 \n \nFigure 34: Boxplot to show the variation of the relevancy of the generated suggestion based on \nthe contextual richness of the input statement. \n5.7.2.2.2 Input Source: OPT vs. Twitter \nWe used the language model (OPT) [272] and Twitter [272] as the source for our input statements. \nWe performed a Chi-Square test to understand if there is any significant impact of the input source \non the relevancy of the generated suggestions. A Chi-Square test, achieving a p-value of less than \n0.05 revealed that the input source had a significant impact on the relevancy score achieved by the \ngenerated suggestions. O verall, when the data source for the input statements was Twitter, \nparticipants found iZen’s generated suggestions to be less relevant compared to when the data \nsource of the input statements was OPT language model (LM). We visualize these results in Figure \n35 and Figure 36. \n \n\n 135 \n \nFigure 35: Variation of the relevancy of the generated suggestion based on the data source of the \ninput statement. (a) Grouped bar chart (where the y-axis represents the “count” of the \nsuggestions), (b) Stacked bar chart (where the y-axis represents the “percentage” of the \nsuggestions) \n \nFigure 36: Boxplot to show the variation of the relevancy of the generated suggestion based on \nthe data source of the input statement. \n5.7.2.2.3 LM used in Zen: OPT vs. GPT2 \niZen leveraged either GPT2 or OPT language model (LM) for its Zen component. We perform a \nChi-Square test to understand if there was any significant impact of the choice of LM for Zen on \nthe relevancy score of the generated suggestions. The Chi -Square test result (p-value of 0.478) \nsuggests that there was no significant impact of this choice  on the Likert score of the generated \nsuggestions. We illustrate these findings in Figure 37 and Figure 38. \n\n 136 \n \nFigure 37: Variation of the relevancy of the generated suggestion based on the LM used in Zen. \n(a) Grouped bar chart (where the y-axis represents the “count” of the suggestions), (b) Stacked \nbar chart (where the y-axis represents the “percentage” of the suggestions) \n \nFigure 38: Boxplot to show the variation of the relevancy of the generated suggestion based on \nthe LM used in Zen. \n5.8 Discussion \n5.8.1 Generated suggestions are easily readable and contain minimal \ngrammatical errors \nBased on the automatic metrics, namely the count of grammatical errors and vari ous readability \nmetrics, we observed that iZen’s generated suggestions had sound linguistic quality. About 95% \nof the generated suggestions had 1 (15%) or 0 grammatical (80%) errors which demonstrate the \n\n 137 \ngrammatical correctness of iZen’s generated suggesti ons. More than 90% of iZen’s generated \nsuggestions were deemed easily readable by all the readability metrics used in this study.  \n5.8.2 Generated suggestions contain minimal negative sentiment \nWe observed that the generated suggestions contained minimal negativ ity. Most of iZen’s \ngenerated suggestions were neutral (69.29%), while 13.58% were positive and 17.13% were \nnegative. While we expect that most of the generated suggestions to not have negative sentiment, \nwe also believe that a negative sentiment (as determined by the Sentiment Analysis technique used \nin the study) may not necessarily mean a bad suggestion and vice versa. This is because a typical \nSentiment Analysis technique may consider something negative based on the constituent words, \nwhile the suggestion may actually be meaningful in the context of the ECSGen task. For example, \nin our analysis, for the statement, “I feel angry because of the information”, one of iZen’s generated \nsuggestions was, “ Do not click on those links ”. While the sentiment of this  suggestion was \ndetermined to be negative, the suggestion seems meaningful based on the original statement. While \nnon-negativity of the suggestions may be an important factor, based on our observation, we opine \nthat it is not a primary indicator of a suggestion’s relevancy.  \n5.8.3 Generated suggestions are mostly relevant with varying degrees of \nrelevancy \nResults from the human evaluation of iZen’s generated suggestions suggest that 26.8% of the \ngenerated suggestions were completely irrelevant, indicating that th e remaining 73.2% of the \nsuggestions were relevant to the emotion expressed in the input statement. The relevancy of these \n73.2% generated suggestions was of varying degrees ( 17.5% very relevant, 17.8% moderately \nrelevant, 19.4% somewhat relevant, 18.5% slightly relevant). The median relevancy score for all \ngenerated suggestions was “Somewhat relevant”.  \nWe also observed that for 91.25% of input statements, iZen generated at least 1 slightly or more \nrelevant suggestion. For 76.75% of input statements, iZen generated at least 2 slightly or more \nrelevant suggestions. At least 1 suggestion was somewhat or more relevant for 80% of the input \nstatements, and at least 2 suggestions had a similar level of relevancy for 59.25% of the input \nstatements. 68.25% of input statements yielded at least 1 moderately or more relevant suggestion, \nand 28% of the input statements had at least 2 moderately or more relevant suggestions. For \n 138 \n51.75% of the input statements, all three generated suggestions were slightly or more relevan t. \nOnly 8.75% of the input statements received no relevant suggestions at all.  \nIn summary, we believe that the following achievements by iZen’s generated suggestions indicate \nthat iZen demonstrated promising results in performing the ECSGen task: \n• A median relevancy score of “Somewhat relevant”,  \n• 73.2% of all the generated suggestions are slightly or more relevant,  \n• 91.25% of the input statements received at least 1 slightly or more relevant suggestions,  \n• 80% of the input statements received at least 1 somewhat or more relevant suggestions. \n5.8.4 Contextually rich input statements resulted in more relevant \nsuggestions \nOur results suggest that iZen can generate more relevant suggestions when the input statement has \nrich contextual information. We see in Section 5.7.2.2.1 that less contextually rich input statements \ngenerated most of the “ Not at all relevant ” suggestions, while most of the “ Very relevant ” \nsuggestions were generated from input statements with rich contextual information. As supported \nby the statistical test, we conclude that the richness of contextual information in the input statement \nis an important factor for iZen to generate relevant suggestions. \n5.8.5 OPT and Twitter as data sources for input statements impacted the \nrelevancy of the generated suggestions \nWe observe that the two data sources (OPT [130], Twitter [272]) used in this study for input \nstatements impacted the relevancy score of the suggestion s generated from them. In our \nexperimental settings, iZen generated more relevant suggestions for input statements generated \nfrom OPT than compared to input statements extracted from Twitter. We believe that this outcome \nwarrants further investigation in order to identify what exact factor(s) contributed to the difference \nin relevancy score for input statements from these two sources.  \n5.8.6 GPT2 and OPT as iZen's core generative LM performed on par in \nterms of relevancy score \nOur results suggest that both OPT [130] and GPT2 [128] performed similarly to iZen’s core \ngenerative language model. While OPT is more recent and arguably more advanced compared to \n 139 \nGPT2, for the ECSGen task, their performance within iZen’s architecture was found to be \ncomparable. We beli eve that these findings warrant experimenting with more language models \nwithin the Zen component in order to observe how they impact iZen’s generated suggestions’ \nrelevancy.  \n5.9 Risks, Limitations and Future Work \n5.9.1 Potential risks posed by iZen \nIn the last safeguard component, Guardrail, we only used the “profanity filter”. It can be argued \nthat this single filter may not be sufficient as the last guardrail. There can be suggestions generated \nby LLMs that may not have any profanity in them but still may be i nappropriate. For example, \nLLMs can generate insensitive suggestions that may potentially contradict the objectives of the \nECSGen task that aims to generate suggestions that “mitigates” negative emotions and not \naggravates them. This warrants further inves tigation in future and can be an interesting research \narea to explore.  \nWith all the filters in place, the possibility of iZen and similar systems generating unacceptable \nsuggestions cannot be fully ruled out. As a standard practice, these types of systems , if released \npublicly, should come with substantial warnings about the potential dangers it can pose for their \nusers. These types of systems often benefit from user feedback, from which they can continuously \nlearn and become better at avoiding undesirable  situations such as generating inappropriate \nsuggestions. An intuitive way to report unacceptable issues may help these systems (when publicly \nreleased) learn better and faster.  \nAnother measure that can be taken to improve the resilience of iZen from generating inappropriate \nsuggestions is input moderations. In iZen we included Guardrail which is a safeguard component \nthat comes at the end of the pipeline. In addition to this, we can include another safeguard \ncomponent at the beginning of the pipeline to m oderate the inputs to iZen. Input moderation can \nhave multiple benefits since it can be a decoupled and a more lightweight component which can \nrestrict entry to the main iZen pipeline, thereby avoiding  potentially unnecessary suggestion \ngeneration and detecting inappositeness early. \n 140 \nThe above points only serve as some examples of the potential risks posed by iZen and it is not an \nexhaustive list. When developing such AI systems for public release, we must diligently consider \nthe potential risks they pose and take necessary steps to mitigate them.  \n5.9.2 Input constraints \niZen’s development was initiated with defining several input constraints as mentioned in the paper. \nAs a result, iZen can only perform the ECSGen task under certain conditions. This restricts iZen’s \ncapability. While more rules can be included in iZen’s PromptGenie component to reduce the \nconstraints, it may not be scalable. A major task of this component is to extract the cause of the \nemotion. A significant portion of the input constraints were ad ded to enable the rule -based \nPromptGenie component to perform this extraction. One potential alternative to this can be \nleveraging the existing Emotion -Cause Extraction (ECE) techniques within the ECA domain. \nIntroduction of such components within iZen’s P romptGenie component may allow significantly \nreducing the number of input constraints that currently exist. We believe there is ample opportunity \nfor further research in this area in order to reduce the number of input constraints in iZen to enable \nit to support a wide range of input statements.  \n5.9.3 Relevancy score \nIn our experimental setting, iZen generated 3 suggestions for 1 input statement. Based on iZen’s \nperformance (e.g., at least 1 of the 3 suggestions was somewhat or more relevant for 80% of the \ninput statements), we can state that this is a promising achievement by a framework that did not \nreceive any specific training to perform this task. However, we believe this performance can be \nfurther enhanced by devising a more innovative and sophisticated fra mework. In addition to \ndesigning completely new frameworks to perform ECSGen, one can explore customizing iZen due \nto its conceptual simplicity and modularity. For example, the Zen component can effortlessly \nleverage other similar language models to genera te candidate suggestions, which is a worthwhile \ninvestigation. The same is true for the Eliminator and Guardrail components where the models and \nlibraries used can be easily replaced and extended.  \n5.9.4 Leaner system \niZen’s superiority comes from the fact that it can perform the ECSGen task without receiving \nspecific training to perform this task. However, this comes with a few drawbacks. For example, \n 141 \niZen includes a reasonably complicated post-generation filtering mechanism that is comprised of \nmultiple pre -trained machine learning models. This increases the size and complexity of the \narchitecture. There is ample opportunity to innovate in this area in order to devise a leaner and \nlightweight solution that can perform ECSGen with similar or better performance.  \n5.9.5 Alternative design for iZen \nOur proposed design for iZen in this chapter assumes that 1 or more suggestions out of the 11 \ngenerated by the Zen component for each input statement are plausible that meet the desired \ncriteria. While this is not an unrealistic  assumption for the advanced large LMs given an \nappropriate prompt exists, particularly with a stochastic decoding strategy that is used in this study \n[237], and that we have observed encouraging performance by iZen, however, it is still constrained \nby a fixed number (i.e., 11) of candidate suggestions generated by Zen. We believe this offers an \nopportunity to explore alternative designs for iZen, particularly the one where Zen can be invoked \nfor iterative suggestion generation until a (or more) suitable output is found. Needless to say, such \na design still requires a threshold that offers an exit path in case where no suitable output is found \nin order to avoid an infinite loop. This alternative design also provides an opportunity to include \nGuardrail with Eliminator to streamline the framework.  \nAnother potential modification to iZen’s current design can be inside the Eliminator component. \nEliminator’s Selective Deduction Unit, E SD follows a greedy algorithm approach which was \ndesigned to address the critical issues first (i.e., toxicity, grammatical errors). There is opportunity \nto explore alternative algorithm s for this selection process. For instance, sum all the scores from \nthe Eliminator subcomponents can be considered together using appropriate weight s in order to \nselect the top three suggestions. We believe a worthwhile future work would be to explore such \nalternative designs for iZen. \n5.9.6 Relevant automatic metrics \nFor ECSGen, we primarily relied on human evaluation to understand the relevance of the generated \nsuggestions. Human evaluation introduces several complicated and time -consuming steps within \nthe entire research project. In our observation, this is not only an issue for ECSGen but a common \nissue for generative tasks in general. An aspirational  future work can be developing relevant \nautomatic metrics for the ECSGen task to measure the relevancy of the generated suggestions. \n 142 \n5.10 Conclusion \nIn this study, we propose ECSGen, a new NLP task within the ECA domain. The purpose of this \ntask is to generate relevant suggestions to mitigate the cause of a negative emotion expressed in a \ngiven input statement. We also devise a technical framework named iZen to perform this task in a \nzero-shot manner within certain constraints. We curate two new datasets to conduct our experiment \nwith iZen. iZen generates three suggestions for each input datapoint. We perform human \nevaluations to understand the relevancy of these generated suggestions. The median relevancy \nscore of the suggestions generated by iZen was “Somewhat relevant”. More than 90% of the input \nstatements in our experiments received at least 1 slightly or more relevant suggestion, and about \n80% received at least 1 somewhat and more relevant suggestion. More than 70% of all the \ngenerated suggestions were slightly or more relevant . These are promising results suggesting \niZen’s decent capability to perform the proposed ECSGen task. However, we acknowledge that \nthere is room for improvement for iZen on several fronts. These include but are not limited to, \ngenerating suggestions that are more relevant, reducing iZen’s input constraints so that a large \nvariety of input statements can be supported, and making iZen more lightweight, especially in \nterms of the number of pretrained models required to perform the task. We  plan to tackle these \nchallenges in our future studies. \n \n  \n 143 \nChapter  6: Towards Automatic Evaluation of NLG Tasks \nusing Conversational Large Language Models \nIn the previous two chapters, we introduced two new open -ended Natural Language Generation \n(NLG) tasks in the do main of Emotion-Cause Analysis  (ECA). A common challenge we \nencountered in both studies was the automatic evaluation of the techniques that performed these \ntasks. Due to the open -ended nature of these tasks, these are difficult to assess in a rule -based \nmanner, often requiring tedious evaluation by human participants. Consequently, for the first NLG \ntask we proposed, named Emotion-Cause Generation (ECG), due to the limited scope of the study, \nwe primarily restricted ourselves to the  available automatic metrics, all of which only supported \nassessing the general linguistic quality of text. For the  second NLG task we proposed, named, \nEmotion Cause mitigating Suggestion Generation (ECSGen), we relied on human participants as \nour primary mode of evaluation. Conducting such tedious evaluations by human participants often \nreduces the pace of research, which is particularly apparent for a fast -advancing research field s \nlike NLG. We observe that this is a challenge experienced by many researchers working on NLG \nproblems. In fact, evaluating the quality of machine generated open-ended texts is a long-standing \nchallenge in Natural Language Processing (NLP). Although the NLG field has seen rapid \nadvancements in recent years, a promising and widely adopted automatic evaluation technique for \nNLG tasks is yet to be developed. In this chapter, we propose leveraging Conversational Large \nLanguage Models (LLMs) as automatic evaluators for open-ended NLG tasks. To demonstrate the \nviability of our proposal, we perform several exper iments with three common NLG tasks as well \nas the two NLG tasks that we introduced in this thesis.  \nThe work presented in this chapter has been accepted into one conference7. \n6.1 Introduction \nWith the advent of Transformers [95] and large pretrained language models (LLM) (e.g., [129]) in \nrecent years, there has been a plethora of research work in Natural Language Processing (NLP) \n \n7 [Accepted] Riyadh, M., & Shafiq, M. O. (2023). Towards Automatic Evaluation of NLG Tasks using Conversational Large \nLanguage Models. In 19th IFIP International Conference on Artificial Intelligence Applications and Innovations. \n 144 \nthat contributed significantly to the advancement of this field , particularly in the area of  Natural \nLanguage Generation (NLG) [277]. While there have been rapid advancements in the NLG field, \nthe difficulty in evaluating them has become more prominent. Unlike many NLP tasks such as \nclassification tasks like Sentiment Analysis, NLG tasks cannot be easily evaluated with automatic \nevaluation techniques. This is due to the nature of the NLG tasks, many of which can be open \nended. Some researchers have attempted to develop automatic evaluation techniques for NLG \ntasks [216], [278]. However, none of them have received widespread adoption primarily due to \ntheir poor generalizability to evaluate open ended and diverse NLG tasks. As a result, a vast \nmajority of the NLG tasks still require performing tedious and lengthy evaluations by human \nparticipants.  \nA re cent addition to the LLM technology is ChatGPT [131], which is closely related to the \nInstructGPT [279] model and fine -tuned from a GPT -3.5 variant [280]. As the name suggests, \nChatGPT is a conversational LLM that can provide a detailed answer to a v ariety of questions. It \nis trained to understand many simple and complex instructions and execute those instructions \neffectively. Guo et al. [281] experimented with ChatGPT’s capability to perform a variety of NLP \ntasks. In this study, we tap into the si milar capabilities of ChatGPT but with the objective of \nunderstanding its ability to be an automatic evaluator for NLG tasks. In addition to some common \nNLG tasks, we focus on the domain of Emotion-Cause Analysis (ECS) [232], which consists of \nseveral tasks related to the cause of an expressed emotion in a given text. Our experiments cover \nall the NLG tasks within the ECA domain that are currently available: Emotion-Cause Generation \n(ECG) [see Chapter 4]  and Emotion -Cause mitigating Suggestion Generation (ECSGen) [see \nChapter 5]. Through a series of experiments, we attempt to understand if ChatGPT can be an \neffective automatic evaluator for various NLG tasks, particularly the ones that do not require \nspecific domain knowledge and that typically need to be evaluated by human participants.  \n6.2 Contributions \nThe main contributions of this study include:  \n• We propose leveraging Conversational LLMs as automatic evaluators for the NLG tasks, \nespecially for open-ended tasks that do not require domain specific knowledge.   \n 145 \n• We perform a series of experiments to understand ChatGPT’s (a recently released advanced \nconversational LLM) ability as an automatic evaluator for three common NLG tasks: \nParaphrasing, Question -Answer, Story -cloze test (i.e., evaluates models’ common -sense \nreasoning capability).  \n• We also experiment with two ECA related NLG tasks: ECG, and ECSGen. These are currently \nthe only two NLG tasks available within the domain of ECA. Consequently, our experiments \non the usage of conversational LLM as an automati c evaluator cover all ECA related NLG \ntasks that exist today.  \n6.3 Related Studies \nThe evaluation of NLG systems has conventionally been conducted by human evaluators [282]. In \nsuch procedures, human evaluators are presented with generated texts from the NLG systems and \nhuman-written texts. The performance of the NLG system is determined by comparing the ratings \nassigned to each. This evaluation approach, which was first introduced in the mid -90s [283], \ncontinues to be utilized in the present day in various forms for the majority of the NLG tasks. \nResearchers later suggested that evaluating  NLG systems by comparing the generated texts to a \ncorpus of human -written texts  offers the potential for faster, m ore cost -effective and scalable \nevaluation [284]–[286]. Despite its advantages, this method has faced criticism, with objections \nsuch as that the generated texts may differ from corpus texts while still meeting the criteria of \nsuccessfully completing an NLG task [287]. \nSome other  researchers developed a group of alternative  automatic metrics  called word-based \nmetrics (WBM). WBMs typically evaluate the similarity between the output text produced by the \nsystems and human -generated reference texts  based on  various criteria [288]. The closer the \nsimilarity between the output and reference texts, the higher the score obtained from the metric. \nBLEU [213], ROUGE [215] etc. are some common examples of such metrics. However, none of \nthese metrics can perform the tasks typically done by human evaluators due to their limited scope.   \nIn addition to WBM, automatic metrics related to NLG also include readability metrics such as the \nFlesch Reading Ease score [217] that quantifies the “reading ease” of a text based on the number \nof characters per sentence, words per sentence, and syllables per word. Grammatical correctness \nis also another automatic metric that some NLG studies tend to use to understand if an output by \n 146 \na machine learning model is grammatically correct [222]. More recently, there has been a growing \ninterest in the use of Transformer-based metrics for evaluating NLG tasks. They can be broadly \ndivided into two categories: reference -based models that require human references  (e.g., [289], \n[290]) and reference-free models that do not (e.g., [291], [292]). The latter category is more cost-\neffective to implement. However, som e researchers pointed out that these metrics are still \ninadequate for evaluating specific NLG tasks without infusing task specific knowledge into them \n– which can be burdensome and increase the scope and complexity of any given study [293]. This \ncould be  one of the reasons why older and less accurate metrics such as BLEU continue to \ndominate the field of automatic evaluation of NLG tasks despite their limitations [294], [295].  \nDue to all these limitations of the existing automatic evaluation techniques  for NLG tasks, \nparticularly for tasks that are more open ended in nature such as paraphrasing, most researchers \nstill opt for human evaluation as the only available but tedious and time-consuming alternative.   \n6.4 Conversational LLMs as Automatic Evaluators for NLG Tasks \nWe propose leveraging conversational LLM as an alternative automatic evaluator for many NLG \ntasks, especially the ones that do not require specific domain knowledge, and that typically requires \nhuman evaluators. Evaluating NLG tasks with huma n evaluators require the evaluators to \nunderstand the tasks with simple instructions and execute those tasks accordingly. For example, to \nevaluate the meaningfulness of a summary of a given news article, human evaluators are typically \ngiven instructions on how to evaluate the relevance of such summaries in relation to the original \nnews article. Based on that, they may be tasked with ranking the relevancy of those summaries. \nWe propose that we can utilize conversational LLM to conduct such evaluations. Parti cularly, in \nthis study, we focus on ChatGPT [131], a recently released conversational LLM to understand its \nability as an automatic evaluator for open-ended NLG tasks. \nAs we demonstrate in the upcoming sections, ChatGPT is similarly capable of understand ing the \ninstructions typically given to human participants and then completing the given task. In the \ncontext of this study, “ability of understanding the instructions” refers to the fact that ChatGPT is \nable to follow the textual instructions given to it in order to carry out the tasks we evaluated. For \nexample, we can input the following textual instructions (i.e., similar to natural language humans \nuse to communicate with each other)  into ChatGPT: “paraphrase this sentence: <I could not go \n 147 \nto the office today because of sudden illness>”, then, it can follow these instructions and carry out \nthe anticipated task.  This capability, in our opinion, makes conversational LLMs like ChatGPT \nsuitable candidates to become automatic evaluators for many NLG tasks. \nTo use ChatGPT for such evaluations, the instruction set of each task needs to be designed \ncarefully, ideally through experimenting with ChatGPT and by observing its response. These \ninstruction sets are often called “prompt” in the context of LLMs [243]. After an effective prompt \ntemplate is determined for a given NLG task, prompts for each datapoint can be generated \nautomatically by developing simple custom logic in programming languages such as Python [265]. \nThese prompts then can be provided to ChatGPT using the available interface and then its response \ncan be collected subsequently. In the following section, we conduct a series of experiments to \nunderstand ChatGPT’s capability as an automatic evaluator for some NLG tasks. \n6.4.1 Evaluation \nIn this study, we ev aluate ChatGPT as an automatic evaluator for the following five NLG tasks: \n(a) Paraphrasing, (b) Question -Answering, (c) Story-cloze Test [296], (d) ECSGen [see Chapter \n5], (e) ECG [see Chapter 4].  \n6.4.1.1 Procedure \nWe perform five small scale experiments to evaluate ChatGPT for the five selected NLG tasks. \nEach experiment has some specificities, but they all share some common procedures, which we \ndiscuss below: \n• We take a small sample of data (i.e., about 100 datapoints) from an existing dataset  for each \ntask. This is because we leveraged ChatGPT’s Research Preview [131] for this study which (at \nthe time of this study) is accessible through a browser interface requiring tedious manual input. \nFor sampling, we leverage different random sampling t echniques including balanced and \nstratified sampling for different tasks to present diverse data compositions to ChatGPT. \n• To use the dataset for our intended evaluation, we preprocess each of them differently \ndepending on their original format and evaluati on process. We auto -generate ChatGPT \nprompts for each datapoint for all five tasks using Python [265]. We use different prompt \nformats for each task, which we designed by iteratively experimenting with ChatGPT for the \n 148 \nbest outcome in terms of aiding ChatGPT to understand the task as well as receiving an answer \nfrom it in a format that is easy to process for analysis.  \n• We enter these prompts into ChatGPT’s interface and manually collect its response for each of \nthe datapoint. This interaction with ChatGPT Research Preview occurred between Jan 26, \n2023, and Feb 16, 2023. \n• After that we compare ChatGPT’s responses with the ones included in the datasets. To observe \nhow ChatGPT performs as an automatic evaluator for different NLG tasks that can be assessed \ndifferently, we leverage diverse techniques such as classification accuracy, multiple choice \ntype questions (MCQ), correlating Likert responses etc. We use Python [265], IBM SPSS \n[268], and Google Sheet [297] for our analysis.  \nExperiments for each task included some distinct procedures, particularly as it relates to the dataset \nused for each, how the task is conducted as well as the analysis methods we used for them. We \ndiscuss them below. \nFor the paraphrasing task, we take a sample from the GLUE dataset’s Quora Question Pairs [298]. \nThe dataset contains two one -sentence long questions. The task is to determine whether the \nquestions are paraphrases of each other. To understand if ChatGPT can automatically evaluate if \na paraphrase is correct, we fra me this as a binary classification problem for analysis where \nChatGPT attempts to determine the correct and incorrect paraphrases. ChatGPT’s performance on \nthis would indicate how it can perform as an evaluator for the NLG task when machine learning \nmodels generate a paraphrase for a given text (in this case, short text).  \nFor the question -answering task, we utilize the WikiQA dataset [299] where each question has \nseveral correct and incorrect answers. The goal is to determine which answers are correct an d \nwhich ones are not. For ease of analysis, we randomly sample some questions with one incorrect \nanswer and some with one correct answer. We then frame this task as a binary classification \nproblem for our analysis where ChatGPT’s task is to determine which  answers are correct for a \ngiven question and which are not. We expect this to indicate how ChatGPT can perform as an \nevaluator to understand a model’s performance in generating answers for a given question. \nSimilarly, we leverage an existing dataset for t he Story-cloze test [296] which also evaluates the \ncommon-sense reasoning of machine learning models. Each datapoint in this dataset contains a \nshort story consisting of five sentences. For each, the first four sentences are provided in the correct \n 149 \norder. For the fifth sentence, there are two options, only one of them being correct. The task is to \ndetermine which option is the appropriate choice for the fifth or last sentence of the story. \nChatGPT’s performance in determining the correct fifth sentence in  this task would indicate how \nwell it can evaluate the models that are tasked with completing unfinished stories using common-\nsense reasoning.  \nThe ECSGen dataset [see Chapter 5]  contains 5 -point Likert responses [264] from human \nevaluators for each “emo tion-cause mitigating suggestion”, representing the degree of relevance \nof the suggestion in relation to the cause of the emotion. The five levels of the Likert scale are: 1: \nNot at all relevant, 2: Slightly relevant, 3: Somewhat relevant, 4: Moderately relevant, and 5: Very \nrelevant. The task for ChatGPT is to rank the sampled suggestions similarly. The dataset contains \n3 suggestions for each statement. We sampled 120 datapoints; consequently, our sample contains \n360 suggestions. For analysis, we frame this as a multi-category classification problem. Since the \nhuman response for each suggestion can be extensively subjective for this type of question, \nparticularly with 5 intensity levels to choose from, we compare ChatGPT’s response with the ones \nfrom human evaluators using various techniques such as correlation testing, classification accuracy \nat 5-levels, and classifications accuracy at 3 and 2-levels by recoding the Likert labels accordingly. \nWe believe ChatGPT’s performance in this experiment can indicate  how well it can rank the \nrelevancy of a suggestion that aims to mitigate the cause of an emotion expressed in a short text.  \nFor the ECG task [see Chapter 4], we preprocess a sample from an existing Emotion-stimuli dataset \n[9] for our study. We replace the “cause-span” for each datapoint with an underscore (“_”). We \nconvert the dataset to a comma -separated values format where the separated cause -span, and the \noriginal texts (without the cause-span) are placed into separate columns. Then we create a duplicate \ncolumn from the “cause-span” column and randomize the order of this new column. Thus, for each \ndatapoint, we have a correct cause and an incorrect cause for the expressed emotion. We remove \nthe datapoints where we found duplicates between these two cause columns and where both causes \nseemed equally appropriate. We frame it as an MCQ-style binary classification task for ChatGPT \nwhere it attempts to pick the correct cause for each statement with an emotional expression. We \nexpect this to indicate ChatGPT ’s ability to evaluate the meaningfulness or relevance of a \ngenerated cause in relation to the emotion expressed in a text. Please refer to the following table \nfor examples of ChatGPT prompts used in this study for each NLG task. \n 150 \nNLG Task ChatGPT prompts used in this study \nStory-cloze \ntest \nConsider the following sentences from a short story:  \nBetsy was about to celebrate her sixteenth birthday. Her parents surprised her with a \nnice dinner out and a small present. Happily, her parents gave her a monogram necklace. \nOn the way home, Betsy misplaced the necklace in the car.  \nWhich one among the following two is a more appropriate next sentence that fits the \nstory above?  \n1. Betsy felt really bad for the rest of the night.  \n2. Betsy wore the monogrammed necklace when she turned 14.  \nAnswer using only numbers 1 or 2. Do not write any other additional notes or explain \nyour answer \nQuestion \nAnswering \nConsider the following question: - what are garnishments?  \nIs the following a correct answer to the above question?  \n- A garnishment is a means of collecting a monetary judgment against a defendant by \nordering a third party (the garnishee) to pay money, otherwise owed to the defendant, \ndirectly to the plaintiff.  \nAnswer with Yes or No. Do not write any additional notes. \nParaphrasing \nConsider this statement: How do I start up a new cafe?  \nIs the following a correct paraphrasing of the above statement?  \n- What are requirements I would need to start my own cafe?  \nAnswer with Yes or No. Do not write any additional notes. \nECG \nConsider the following statement which expresses the emotion surprise. The cause of \nthis emotion is missing which is denoted by '_':  \n- Martha replied and left her twin sister blinking in astonishment _ but which was now \nenticingly attractive  \nWhich of the  following causes is contextually more meaningful for the emotion \nexpressed in the above statement? Please consider the context of the statement above as \nthe cause should not only be relevant to the emotion expressed, but also the context of \nthe emotion:  \nA. at a possibility that had not occurred to her before.  \nB. at the lack of progress Corbett was making.  \nIf you are not sure, then just pick your best guess. If you find one is slightly more \nappropriate than the other, then answer accordingly. Do not write any additional notes. \nAnswer with letter: A or B. No need to explain your answer or rewrite the cause. \nECSGen \nConsider this statement: I feel bored because I have been here for so long. \nThese are three suggestions to potentially address the emotion expr essed in the \nstatement above:  \n1) At least please let me know  \n2) Just turn off the tv and close the window  \n3) You will always be gone.  \nNow rank the relevancy of each of these three suggestions in relation to the statement \nabove. Use a 5 -point Likert scale for ranking where: 1 means \"Not at all relevant\" 2 \nmeans \"Slightly relevant\" 3 means \"Somewhat relevant\" 4 means \"Moderately relevant\" \n5 means \"Very relevant\".  \nPut your ranking in an array in the same order as the suggestion above. Don't write any \nadditional notes. \nTable  19: ChatGPT Prompts used in this study for different NLG task \n 151 \nIn this study, we evaluate ChatGPT, a recently released conversational LLM, as an automatic \nevaluator for a set of NLG tasks and report our findings. We do not compare ChatGPT’s \nperformance for this with any other existing LLMs. This is due to the fact our evaluation settin g \ninvolves providing instructions to ChatGPT in a conversational manner, similar to the ones \ntypically given to human evaluators. To the best of our knowledge, other than ChatGPT, there is \nno model that is currently publicly available for inference that ca n aptly understand these \ninstructions to describe complex tasks and execute the tasks as instructed. Consequently, we only \nexperiment with ChatGPT and report our findings accordingly. \n6.4.1.2 Results \nParaphrasing: ChatGPT classified the correct and incorrect parap hrases with an accuracy score \nof 0.81. For the correct ones, precision and recall scores are 0.84 and 0.77 with an F1 score of 0.80 \nfor 60 datapoints, and for the incorrect ones, precision and recall scores are 0.78 and 0.85 with an \nF1 score of 0.82 for the same amount of datapoints.  \nQuestion-answering: ChatGPT classified the correct and incorrect answers with an accuracy \nscore of 0.89. For the correct answers, precision and recall scores are 0.81 and 0.88 with an F1 \nscore of 0.85 for 34 datapoints, and for the incorrect answers, precision and recall scores are 0.94 \nand 0.89 with an F1 score of 0.91 for 65 datapoints.  \nStory-cloze test: For this task, ChatGPT was able to choose the correct fifth sentence with an \naccuracy score of 0.99 for a total datapoints of 157 short stories. Since it is an MCQ style question \nthat requires identifying only the correct answer between two options, the individual scores (e.g., \nprecision, recall) for each option are not relevant, hence we do not report them.  \nECSGen: We conduct a series of analyses to understand ChatGPT’s performance on the ECSGen \ntask as the only dataset available for this task is relatively more complicated compared to the other \nevaluated tasks, which uses a 5 -point Likert scale representing the degree of re levance of a \nsuggestion to an emotion-cause. As explained in the previous section, 5-point Likert responses can \nbe extremely subjective, particularly between the adjacent levels. Consequently, we analyze this \nusing several techniques to understand ChatGPT’ s suitability as an automatic evaluator for the \nECSGen task. The mean score of 5 -point Likert responses from human evaluators and ChatGPT \nare 2.83 and 2.47 respectively, with a median of 3 and 2, and a standard deviation of 1.451 and \n1.117 respectively. For the correlation test, we primarily use the Spearman coefficient (0.334) as \n 152 \nit fits the data type and distribution used in the study. However, we also report its parametric \ncounterpart: the Pearson coefficient (0.33), as an additional correlation measure. Both correlation \ntests indicate a positive correlation between ChatGPT and human responses and both tests were \nsignificant with a p-value < 0.001.  \nWe also report the distance between the Likert responses for each question to understand the \npercentages of the answers that varied differently. About 26.11% of suggestions had the exact \nsame ranking by human evaluators and ChatGPT. ChatGPT and human evaluators’ responses \nvaried by only 1 Likert point for 40% of the suggestions. This indicates that more than 65% of the \nresponses are an exact match or only varied by 1 Likert-point. For the remainder of the suggestions, \n22.50% varied by 2 Likert points, 10.28% varied by 3, and only 1.11% varied by 4 Likert-point.   \nAdditionally, we measure ChatGPT’s effectiveness as an automatic evaluator for the ECSGen task \nby framing this analysis as a classification problem. However, since the responses are in a 5-point \nLikert scale that uses typical labels for intensity measurement, the accuracy score is expectedly \nlow (0.26) f or classification with five categories (i.e., 5 -point Likert scale). Consequently, we \nperform further analysis by recoding the Likert scores. First, we recode this as a 3 -category \nclassification problem, where Likert score 1 is recoded as 1, 5 is recoded a s 3, and the more \nambiguous scores in between are recoded to 2. This expectedly increases the classification \naccuracy (0.55) compared to the 5 -category approach. Next, we recode the original data again in \n3-categories. However, this time, we recode 1 and 2  as 1, 4 and 5 as 3, and 3 as 2. This yields a \nsimilar classification accuracy (0.48) as the previous 3-category approach although slightly lower. \nWe also analyze this from a binary classification perspective, for which we use two different \napproaches. First, we remove the datapoints with a Likert score of 3 from the original 5 -point \nLikert data (i.e., 3 representing the mathematical mid -point in a 5-point Likert scale). We recode \n1 and 2 as 1, and 4 and 5 as 2. This increases the classification accuracy further to 0.67. Lastly, we \nlook at the agreement of human evaluators and ChatGPT for only the lowest and the highest scores: \n1 and 5. This dramatically reduces the datapoints for analysis, but it provides a closer \nrepresentation of a binary classification problem. The classification accuracy with this approach is \nsignificantly higher with a score of 0.91. The confusion matrices in Figure 1 provide a more \ndetailed view of these various classification results.  \n 153 \n \nFigure 39: ECSGen confusion matrices: a) 5-class. b) binary with only labels 1 and 5. c) binary \nwith label 3 removed and 1, 2 recoded to 1 and 4, 5 recoded to 2. d) 3-class with label 1 and 2 \nrecoded as 1; 4 and 5 as 3; and 3 as 2. c) 3-class recoded 1 and 2 as 1; 4 and 5 as 3; and 3 as 2. \nECG: Similar to the Story -cloze test, for the ECG task, ChatGPT is tasked with MCQ -style \nquestions where the requirement is to choose an emotion -cause from the two given options that \n\n 154 \nmore appropriately explains the emotion expressed in a given statement. Chat GPT achieved an \naccuracy score of 0.98 for a total datapoints of 120.  \nTable  20 represents the summary of ChatGPT’s performance in evaluating all the NLG tasks \nconsidered in this study. \nTask Analysis Method(s) Score(s) \nParaphrasing Classification Accuracy  0.81 \nQuestion Answering Classification Accuracy  0.89 \nStory-cloze Classification Accuracy Score (MCQ) 0.99 \nECSGen \nSpearman Coefficient, Pearson \nCoefficient \n0.334, 0.33 \nClassification Accuracy - \nNumber of classes: 5, 3, 3*, 2, 2* \n0.26, 0.55, 0.48*, 0.67, 0.91* \nECG Classification Accuracy (MCQ) 0.98 \nTable  20: Summary of ChatGPT’s performance as an evaluator for various NLG tasks. Asterisk \n(*) indicates an alternative recoding technique used (more details above). \n6.5 Discussion \nWe have experimented with five NLG tasks to understand if ChatGPT, a recently released \nconversational LLM, can be an automatic evaluator for these tasks. Three of them are among the \ncommon NLG tasks and the remaining two have been recently introduced within the ECA domain. \nWe observe that ChatGPT is able to understand all these tasks and conduct the evaluation for them \nwith minimal instructions akin to human evaluators. In our opinion, this is a significant strength \nof Conversational LLMs like ChatGPT that makes them a good candidate to be automatic \nevaluators for many NLG tasks where researchers can quickly evaluate their models’ performance \nfor a given NLG task by providing simple instructions to ChatGPT. In addition, we leverage \nvarious techniques to eval uate ChatGPT’s strength as an automatic evaluator for the given NLG \ntasks. These include framing the analysis as a classification problem with two or more categories, \nMCQ style questions, and Likert -scale ranking. We observe that ChatGPT is capable of \nunderstanding these diverse framings of problems with minimal instructions, which, we believe, \nreinforces its potential as an automatic evaluator for many NLG tasks.  \nWe observe that ChatGPT achieves almost 100% accuracy for two tasks: ECG and Story -cloze \ntest. For question-answering and paraphrasing it scored 0.89 and 0.81 respectively. We opine that \n 155 \nthese consistent high scores represent ChatGPT’s ability to be an effective evaluator for these NLG \ntasks. In our analysis, the ECSGen task’s evaluation is different than the others experimented with \nin this study given the fact that its dataset comes with a 5 -point Likert response from human \nevaluators which is prone to subjectivity. As explained earlier, we perform more analysis for this \ntask to understand ChatGP T’s suitability to be an effective evaluator for this task. Based on the \nclassification accuracy that ChatGPT achieved in our analysis of the ECSGen task, particularly \nwhen framed as a binary problem, we suggest that ChatGPT can be considered a useful evaluator \nfor this task as well. This is also reinforced by the positive score in the correlation tests between \nChatGPT and human evaluators’ ranking. In addition, we observe that the distance between the \nLikert responses from ChatGPT and human evaluators indicates a high correlation between them. \nParticularly, more than 65% of the responses had a Likert distance of 1 or 0, while only about 12% \nof the responses varied widely with a Likert distance of 3 or more. However, we believe that for \nECSGen, a further study may be beneficial to confirm the findings of our study, especially with a \nbinary ranking from human evaluators, which can be then compared against ChatGPT’s response \nwith more accuracy. \nBased on our overall results, we suggest that conversational LLMs like ChatGPT can be effective \nautomatic evaluators for the NLG tasks used in this study. We believe that this statement can \npotentially be generalized to many other similar NLG tasks. This is because, in our observation, \nChatGPT is able to understand instructions for evaluating a variety of NLG tasks, some of which \nhave been introduced very recently. In our opinion, this ability to understand how to evaluate a \nnew NLG task from simple instructions, and then execute those instructions with decent \nperformance are the two most important features of ChatGPT that can make it an effective \nautomatic evaluator for many NLG tasks in the future. However, to use ChatGPT as an automatic \nevaluator for NLG tasks other than the ones used in this study, we recommend performing a small-\nscale evaluation with ChatGPT for that task. We expect that the evaluations we performed in this \nstudy can be an inspiration for such future experiments.  \nWe observe that designing an appropriate prompt is crucial to help ChatGPT understand the task \nproperly and respond accordingly. We recommend careful consideration while designing the \nprompts to use ChatGPT as an automatic evaluator for a given NLG task. \n 156 \nLastly, since this is a newly suggested automatic evaluation technique, we expect this to m ature \nover time with more successive contributions from the researcher community, especially with \nlarger-scale studies. We expect this to initially complement existing evaluation techniques such as \nhuman evaluations. This may pave the way for this techniqu e to gradually become one of the \nmainstream automatic evaluation techniques in the future for many NLG tasks, especially the ones \nthat do not require specific domain knowledge. There is ongoing research to develop more \nconversational LLMs like ChatGPT [300]. When more such LLMs are available, preferably with \nenhanced performance, we believe researchers can leverage multiple conversational LLMs as \nautomatic evaluators and report their findings accordingly for increasing the robustness of their \nanalysis.  \n6.5.1 Guidelines for Researchers \nWe recommend transparency while using Conversational LLMs such as ChatGPT as an automatic \nevaluator for NLG tasks. Following are our general guidelines for this use-case:  \n• After the NLG task is complete, researchers can pick a small sample from the output (e.g., 10-\n20 datapoints, depending on the type of task). \n• Researchers can use the Conversational LLM of choice to evaluate these sampled outputs. \n• Optionally, researchers can then provide their own evaluation of these sampled output s (or \nthey can choose to ask human evaluators to do this). \n• Then, these two evaluations of the sampled outputs, one by conversational LLM and another \nby the human can be presented in the research publication together in complete detail (i.e., \nevaluation of each datapoint in the sampled outputs, along with the input provided, and output \ngenerated). Alternatively, if human evaluation is not performed, the researchers can choose to \nonly report the detailed evaluation from the Conversational LLM. \n• This will provide the readers an understanding of the performance of the automatic evaluator \nwhich they can consider when interpreting the findings in a given study that includes automatic \nevaluation of NLG tasks using conversational LLM.  \n6.6 Limitations and Future Studies \nThere are several limitations of this study . The chief among them is the fact that we used a small \nnumber of samples in our experiments. As described above, this is because ChatGPT Research \n 157 \nPreview currently only allows access through a web browser and man ually inputting a large \namount of datapoint can be tedious and erroneous. It is also because we believe that such a sample \nis sufficient to get an indication of the general efficacy of ChatGPT as an evaluator of the NLG \ntasks discussed in this study. It is however a noteworthy limitation, and we acknowledge the need \nto investigate this further with larger samples.  \nWe investigated ChatGPT’s effectiveness as an automatic evaluator for several NLG tasks. \nHowever, there are many other NLG tasks that we have no t experimented with. We have \nconstrained this study to a small subset of NLG tasks to keep the experiments within the scope of \nthis study. We acknowledge that our results cannot be generalized to every other NLG task, \nparticularly the likes of machine translation, which we believe Conversational LLMs like ChatGPT \nare currently not able to effectively evaluate due to their limited knowledge of languages other \nthan English. We encourage researchers to investigate the efficacy of ChatGPT and possible \nsimilar conversational LLMs as automatic evaluators for more NLG tasks. \nWhile we encourage the usage of conversational LLMs such as ChatGPT for evaluating open -\nended NLG tasks, evaluation of many tasks may require awareness of different additional factors \nsuch as cultural and geographical context. This may not be an issue for tasks such as paraphrasing \nor summarization, however, this can be an important factor for tasks such as ECG and ECSGen. \nFor example, for the ECG task, which requires generation of a meaningful cause of an emotion \nexpressed in a given text, there can be many answers, some can be more relevant than others based \non different cultural context. For instance, different parts of the world have different public \nholidays that are reasons for celebration and joy. A cause for emotion related to these celebrations \nmay be meaningful in one country but not the others. The same is true for the ECSGen task – the \nsame suggestion to mitigate a particular negative emotion may not be appropriate all over the \nworld. In the context of geographical and cultural differences, this is akin to the well -established \nconcept of localization (i.e., the process of adapting a product or service to suit the language, \nculture, and specific requirements of a particular region, inclu ding the translation of content, \nmodification of graphics, and compliance with local laws ) [301]. We opine that  there is \nopportunity to borrow from  the existing concepts such as this one  in order to enhance the \nconversational LLMs to have such awareness such as cultural and geographical context. This type \nof awareness within these LLMs can make them better evaluator s of the open-ended NLG tasks \n 158 \nsuch as ECG and ECSGen. We believe t his provides interesting future research opportunit ies \nregarding more effective usages of conversational LLMs in general. \nThe current public access of ChatGPT only allows interaction with the model without granting full \naccess to it. In addition, the model is regularly updated with more training data. As a result, its \nresponse to one question may change over time. While we expect the response of ChatGPT to \nbecome more enhanced over time, we, however, suggest that the apparent varying nature of \nChatGPT needs to be considered while interpreting the results we publish in this study. \n6.7 Conclusions \nWe experimented with five NLG tasks to understand the performance of a recently released \nconversational LLM named ChatGPT as an automatic evaluator for these tasks. There are two \nECA related tasks within these five tasks representing all currently available NLG tasks within the \nECA domain. We observe that ChatGPT is able to understand how to execute these tasks with \nsimple instructions akin to the ones typically given to the human participants who evaluate these \ntasks. We believe that this capability to understand NLG tasks from simple instructions along with \nthe results from our experiments suggest that ChatGPT and possible future and more advanced \nvariants of such conversational LLM can be effective automatic evaluators for the NLG tas ks \nexplored in this study. We opine that with more experiments, this finding can be potentially \ngeneralized to many other similar NLG tasks, especially the tasks that do not require domain \nspecific knowledge.  \n  \n 159 \nChapter  7: Overall Conclusions \nIn this thesis, we have conducted multiple investigations on several NLP tasks related to human \nemotion and sentiment. We devise d two successive techniques to perform multi -class Sentiment \nAnalysis using only 50 labeled training data per class . Extending the  focus beyond the  \nclassification of sentiment towards the cause of the emotion, we introduced two novel generative \nNLP tasks (i.e., NLG tasks) in the domain of Emotion-Cause Analysis  (ECA) and develop ed \ntechnical solutions to perform these tasks. Additionally, building upon our  experience with the \nNLG task evaluation, we proposed a new automatic evaluation method for open-ended NLG tasks \nand conducted experiments to demonstrate its viability. For all our investigations on these various \nNLP tasks, primarily related to human emotion and sentiment, we devised technical solutions that \ncould operate with little to no labeled data. This is largely facilitated by leveraging Transformer -\nbased large language models (LLMs) as a core element in our proposed architectures. We provide \na summary of each investigation we conducted in this thesis below: \n• We developed SG-Elect technical architecture to perform multi-class Sentiment Analysis. Our \nexperiments showed that SG-Elect achieved significantly higher performance (i.e., F1 macro, \nand F1 weighted average) in multi-class Sentiment Analysis compared to its baseline. Building \nupon SG -Elect, we developed GAN -BElectra, which significantly reduced the architecture \ncomplexity and required training steps compared to SG -Elect, without sacrificing the \nperformance gain achieved by its predecessor. It is noteworthy that GAN -BElectra in fact \nachieved a small gain in performance (arithmetically) consistently across all datasets we \nevaluated though the gain did not reach statistical significance.  \n• We propose d the first ever NLG task within the ECA domain named Emotion -Cause \nGeneration (ECG). The goal of this task is to generate a meaningful cause for an emotion \nexpressed in a given text. We position ed this task as an infilling task and establish ed its \nviability as an NLP task through technical demonstration. We customized an existing infilling \narchitecture and repurpose d an available ECA dataset (containing a total of 820 labeled \ndatapoints for training, validation, and inference) to perform the demonstration.   \n• We designed a second novel NLG task in the ECA domain named Emotion -Cause mitigating \nSuggestion Generation (ECSGen). The objective of this task is to generate relevant suggestions \nto mitigate a negative emotion expressed in a given text statement. We proposed iZen, a zero-\n 160 \nshot framework to perform this without requiring any labeled data or new training steps. Our \nexperiments suggested that iZen could generate suggestions with decent relevancy as part of \nthe ECSGen task. We also curated our own dataset to perform this task. \n• Lastly, we propose d a novel automatic evaluation method for open -ended NLG tasks \nleveraging Conversational Large Language Models. We perform ed several experiments with \nChatGPT which was the most advanced conversational LLM available at the time of this \ninvestigation. These experiments suggest ed the promising ability of conversational LLMs as \neffective automatic evaluators for open -ended NLG tasks. Additionally, we provide d \nguidelines for the researchers on how to use conversational LLMs in their stu dy for such use-\ncase. \n7.1 Implications of the Contributions \nThe implications of the contributions of this research are manifold. We discuss them below: \n• Sentiment Analysis is a highly context-dependent task where the same set of words can express \ndifferent sent iments in different settings and domains. Consequently, it warrants domain -\nspecific labeled data to train machine learning models that can perform the task with decent \naccuracy, which can be expensive. We proposed two performant techniques for the multi-class \nSentiment Analysis task which only required 50 labeled datapoints per class. Thus, we opine \nthat these two techniques are important contributions in the specific problem space of multi -\nclass Sentiment Analysis with limited labeled data.  \n• Despite sheer advancements in generative machine learning technologies in recent years, the \nECA domain only had NLU tasks to date. This thesis introduced two novel NLG tasks (ECG, \nECSGen) in this domain with accompanying technical solutions. The solution for the ECG \ntask utilized a small existing dataset containing only 820 labeled datapoints for training, \nvalidation, and inference combined. The technical architecture to perform ECSGen, named \niZen, did not require any labeled data or new training steps. Additionally, we published a new \ndataset for the ECSGen task. We believe all these contributions significantly enrich the ECA \nresearch domain and pave the way for more similar and innovative investigations in this area.  \n• Evaluation of NLG tasks typically requires human par ticipants in the evaluation process, \nparticularly for open-ended generative tasks such as summarizing, and poetry generation. This \nis a time-consuming process for this rapidly advancing field. We proposed that Conversational \n 161 \nLLMs like ChatGPT can be effect ive automatic evaluators for open -ended NLG tasks. We \nperformed several experiments that demonstrate d the viability of this proposal. We provide d \nguidelines for the researchers to effectively use conversational LLM in their study which we \nexpect to create more transparency in reporting of results generated through such \nconversational LLMs. We believe that the proper utilization of this proposed automatic \nevaluation method for open-ended NLG tasks can significantly benefit this vast research area, \npotentially accelerating the velocity of innovation in this area.  \nIn summary, this thesis makes important contributions to the NLP field on several fronts, \nparticularly NLP tasks related to human emotion: by introducing generative NLP tasks in the ECA \ndomain, by devising technical solutions to perform an existing classification task (i.e., multi-class \nSentiment Analysis) and two new generative tasks (i.e., ECG, ECSGen) in limited to no labeled \ndata settings, and by proposing a novel method for automatic evaluation of open-ended NLG tasks. \nOur investigations also demonstrate d the general capability of Transformer -based LLMs in \nperforming various tasks in limited to no labeled data situations, spanning both NLU and NLG \nsubdivisions of NLP, and for both existing and novel tasks. This reinforces the ongoing utilization \nof LLMs in a variety of NLP tasks in different domains and invites further investigations into the \npotential utility of these LLMs in more NLP tasks.  \n7.2 Future Work \nResearch presented in this thesis activates  several avenues for potential future work. We discuss \nthem below: \n• Both technical solutions (SG -Elect and GAN -BElectra) that we propose d for multi -class \nSentiment Analysis  with limited labeled data require d multi-step training. This makes the \noverall training more tedious, time-consuming, and error prone. A worthwhile future work in \nthis area is to devise architecture with unified training which can achieve similar or higher \nclassification accuracy. Another useful research in this area could be taking the z ero-shot \napproach where no labeled training data is required for multi-class Sentiment Analysis, while \nstill attaining equivalent or greater classification accuracy. Lastly, similar inquiries within the \nlimited or no labeled data circumstances can be exten ded towards various subtasks of \nSentiment Analysis such as aspect-oriented Sentiment Analysis.  \n 162 \n• We introduce d generative NLP tasks within the ECA domain. This is expected to create \npotential future research opportunities on various fronts. One direction could be devising more \nsophisticated and performant technical solutions to perform the novel tasks proposed in this \nthesis. We expect the ECSGen dataset that we publish as part of our research to be beneficial \nin accelerating more research around this task. Secondly, we believe that our research can be \na catalyst for creating more new generative tasks within the ECA domain. Extending on this \nfront, there is an opportunity to combine the existing ECA related NLU tasks with the proposed \nNLG tasks. For example, a solution can be devised that can perform ECSP (Emotion Cause -\nSpan Pair Extraction) on a given text to identify the emotion expressed in a given sentence \nalong with extracting the cause -span related to that emotion. Subsequently, it can perform \nESCGen to generate a relevant suggestion to mitigate that emotion. In our thesis, the ECSGen \ntask only took input statements with negative emotions in order to generate a suggestion to \nmitigate that. A worthwhile future work can be to extend the ECSGen task so that it can take \na statement with any kind of emotion, and based on the emotion, whether given or classified \nduring the inference time, a solution generates a suggestion if the emotion is negative or \ngenerates a relevant comment if the emotion is positive.  \n• Lastly, we proposed using conversational LLMs for the automatic evaluation of open -ended \nNLG tasks. We performed a few small-scale experiments to prove our hypothesis. An expected \nfuture work can be performing more rigorous experiments with more NLG tasks. In  addition, \nwe only evaluated the performance of one conversational LLM: ChatGPT. As more such \ntechnologies become available, the outcome of this research can be more firmly established by \nexperimenting with more of these technologies. We also believe  that our proposal creates an \nopportunity to develop more structured guidelines for using these automatic evaluators so that \nit can be widely adopted following a common procedure.  \n \n \n \n  \n 163 \nAppendices \nAppendix A   \nA.1 Guidelines for participants on how to respond to the survey questions for the \nECSGen Task \n \n  \n\n 164 \nA.2 A Sample Questionnaire from the Survey for the ECSGen Task \n \n\n 165 \nReferences \n[1] D. Khurana, A. Koli, K. Khatter, and S. Singh, “Natural language processing: state of the \nart, current trends and challenges,” Multimed Tools Appl , vol. 82, no. 3, pp. 3713 –3744, \nJan. 2022, doi: 10.1007/S11042-022-13428-4/FIGURES/3. \n[2] “Guidelines for PhD Integrated Thesis Format | Recreation and Leisure Studies | University \nof Waterloo.” https://uwaterloo.ca/recreation -and-leisure-studies/current-graduate-\nstudents/thesis-information/doctoral-thesis/guidelines-phd-integrated-thesis-format \n(accessed Mar. 08, 2023). \n[3] C. Wohlin, “Guidelines for Snowballing in Systematic Literature Studies and a Replication \nin Software Engineering,” 2014, doi: 10.1145/2601248.2601268. \n[4] “Google Scholar.” https://scholar.google.ca/ (accessed Jan. 24, 2021). \n[5] “Omni - Carleton University Library.” https://ocul -\ncrl.primo.exlibrisgroup.com/discovery/search?vid=01OCUL_CRL:CRL_DEFAULT \n(accessed Mar. 08, 2023). \n[6] K. R. Scherer, “What are emotions? And how can they be measured?,” \nhttp://dx.doi.org/10.1177/0539018405058216, vol. 44, no. 4, pp. 695–729, Dec. 2005, doi: \n10.1177/0539018405058216. \n[7] C. J. Fillmore, J. Ruppenhofer, and A. Wright, “FRAMENET IN ACTION: THE CASE OF \nATTACHING”. \n[8] A. Ortony, G. L. Clore, and A. Collins, “The Cognitive structure of emotions cambridge,” \nUK: Cambridge University Press9, 1988. \n[9] D. Ghazi, D. Inkpen, and S. Szpakowicz, “Detecting Emotion Stimuli in Emotion -Bearing \nSentences,” in Computational Linguistics and Intelligent Text Processing , Springer \nInternational Publishing, 2015, pp. 152–165. \n[10] W. V. F. and P. E. PAUL EKMAN, Emotion in the Human Face . Elsevier, 1972. doi: \n10.1016/c2013-0-02458-9. \n[11] R. Plutchik, “A psychoevolutionary theory of emotions,” Social Science Information, vol. \n21, no. 4–5, pp. 529–553, Feb. 1982, doi: 10.1177/053901882021004003. \n 166 \n[12] B. Pang and L. Lee, “Opinion Mining and Sentiment Analysis,” Foundations and Trends® \nin Information Retrieval, vol. 2, no. 1–2, pp. 1–135, 2008, doi: 10.1561/1500000011. \n[13] J. J. Gross, “Emotion regulation: Conceptual and empirical foundations.,” in Handbook of \nemotion regulation , 2014. Accessed: Mar. 12, 2023. [Online]. Available: \nhttps://psycnet.apa.org/record/2013-44085-001 \n[14] C. E. Osgood, G. J. Suci, and P. H. Tannenbaum, The measurement of meaning , no. 47. \nUniversity of Illinois press, 1957. \n[15] B. Liu, “Sentiment analysis and opinion mining,” Synthesis Lectures on Human Language \nTechnologies, vol. 5, no. 1, pp. 1 –184, May 2012, doi: \n10.2200/S00416ED1V01Y201204HLT016. \n[16] “Cambridge Dictionary | English Dictionary, Translations & Thesaurus.” \nhttps://dictionary.cambridge.org/ (accessed Apr. 25, 2021). \n[17] T. Nasukawa and J. Yi, “Sentiment analysis: Capturing favorability using natural language \nprocessing,” in Proceedings of the 2nd International Conference on Knowledge Capture, \nK-CAP 2003, New York, New York, USA: Association for Computing Machinery, Inc, Oct. \n2003, pp. 70–77. doi: 10.1145/945645.945658. \n[18] J. M. Wiebe, “Learning Subjective Adjectives from Corpora,” 2000. \n[19] W. Medhat, A. Hassan, and H. Korashy, “Sentiment analysis algorithms and applications: \nA survey,” Ain Shams Engineering Journal, vol. 5, no. 4, pp. 1093 –1113, Dec. 2014, doi: \n10.1016/j.asej.2014.04.011. \n[20] N. C. Dang, M. N. Moreno-García, and F. De la Prieta, “Sentiment analysis based on deep \nlearning: A comparative study,” Electronics (Switzerland), vol. 9, no. 3, Mar. 2020, doi: \n10.3390/electronics9030483. \n[21] B. Pang and L. Lee, “Opinion mining and sentiment analysis,” Foundations and Trends in \nInformation Retrieval, vol. 2, no. 1–2, pp. 1–135, 2008, doi: 10.1561/1500000011. \n[22] A. Alsaeedi and M. Z. Khan, “A Study on Sentiment Analysis Techniques of Twitter Data,” \nArticle in International Journal of Advanced Computer Science and Applications , vol. 10, \nno. 2, 2019, doi: 10.14569/IJACSA.2019.0100248. \n 167 \n[23] K. Chakraborty, S. Bhattacharyya, R. Bag, and A. A. Hassanien, “Sentiment Analysis on a \nSet of Movie Reviews Using Deep Learning Techniques,” in Social Network Analytics , \nElsevier, 2019, pp. 127–147. doi: 10.1016/b978-0-12-815458-8.00007-4. \n[24] M. E. Mowlaei, M. Saniee Abadeh, and H. Keshavarz, “Aspect -based sentiment analysis \nusing adaptive aspect -based lexicons,” Expert Syst Appl , vol. 148, p. 113234, 2020, doi: \n10.1016/j.eswa.2020.113234. \n[25] L. Yue, W. Chen, X. Li, W. Zuo, and M. Yin, “A survey of sentiment analysis in social \nmedia,” Knowl Inf Syst, vol. 60, pp. 617–663, 2019, doi: 10.1007/s10115-018-1236-4. \n[26] P. D. Turney, “Thumbs Up or Thumbs Down? Semantic Orientation Applied to \nUnsupervised Classification of Reviews.” \n[27] M. Hu and B. Liu, “Mining and Summarizing Customer Reviews,” 2004. \n[28] A. Esuli and F. Sebastiani, “SENTIWORDNET: A Publicly Available Lexical Resource for \nOpinion Mining.” \n[29] S. M. Mohammad, S. Kiritchenko, and X. Zhu, “NRC-Canada: Building the State -of-the-\nArt in Sentiment Analysis of Tweets,” 2013. \n[30] “General Inquirer Categories.” http://www.wjh.harvard.edu/~inquirer/homecat.htm \n(accessed May 24, 2021). \n[31] “Lasswell Dictionary Description.” http://www.wjh.harvard.edu/~inquirer/lasswell.htm \n(accessed May 24, 2021). \n[32] “WordNet | A Lexical Database for English.” https://wordnet.princeton.edu/ (accessed May \n02, 2021). \n[33] M. Thelwall, K. Buckley, and G. Paltoglou, “Sentiment Strength Detectio n for the Social \nWeb 1,” 2011. \n[34] R. Ortega, A. Fonseca, A. P. Lumumba S/, N. Santiago De Cuba, and C. Yoan Gutiérrez, \n“SSA-UO: Unsupervised Twitter Sentiment Analysis.” \n[35] P. Nakov, Z. Kozareva, A. Ritter, S. Rosenthal, V. Stoyanov, and T. Wilson, “Se mEval-\n2013 Task 2: Sentiment Analysis in Twitter,” 2013. \n 168 \n[36] H. Saif, Y. He, M. Fernandez, and H. Alani, “Contextual Semantics for Sentiment Analysis \nof Twitter,” 2015. \n[37] S.-M. Kim and E. Hovy, “Determining the Sentiment of Opinions.” \n[38] S. Feng, R. Bose, and Y. Choi, “Learning General Connotation of Words using Graph-based \nAlgorithms.” \n[39] “PageRank - Wikipedia.” https://en.wikipedia.org/wiki/PageRank (accessed May 24, \n2021). \n[40] C. Strapparava and R. Mihalcea, “SemEval -2007 Task 14: Affective Text ,” 2007. doi: \n10.5555/1621474.1621487. \n[41] M. D. P. Salas -Zárate, J. Medina -Moreira, K. Lagos -Ortiz, H. Luna -Aveiga, M. Á. \nRodríguez-García, and R. Valencia-García, “Sentiment Analysis on Tweets about Diabetes: \nAn Aspect -Level Approach,” Comput Math Metho ds Med , vol. 2017, 2017, doi: \n10.1155/2017/5140631. \n[42] “n-gram - Wikipedia.” https://en.wikipedia.org/wiki/N-gram (accessed May 24, 2021). \n[43] A. Go, R. Bhayani, and L. Huang, “Twitter Sentiment Classification using Distant \nSupervision.” \n[44] J. Read, “Using Emoticons to reduce Dependency in Machine Learning Techniques for \nSentiment Classification,” 2005. \n[45] B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up? Sentiment Classification using \nMachine Learning Techniques,” EMNLP, 2002. \n[46] A. Pak and P. Paroubek, “Twitter as a Corpus for Sentiment Analysis and Opinion Mining.” \n[47] “Conditional random field - Wikipedia.” \nhttps://en.wikipedia.org/wiki/Conditional_random_field (accessed May 25, 2021). \n[48] L. Barbosa and J. Feng, “Robust Sentiment Dete ction on Twitter from Biased and Noisy \nData,” 2010. \n[49] L. Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao, “Target -dependent Twitter Sentiment \nClassification.” \n 169 \n[50] K. Korovkinas, P. Danėnas, G. Garšva, P. Dan, and G. Gař Sva, “SVM and k-Means Hybrid \nMethod for Textual Data Sentiment Analysis,” Baltic J. Modern Computing, vol. 7, no. 1, \npp. 47–60, 2019, doi: 10.22364/bjmc.2018.7.1.04. \n[51] K. Korovkinas, P. Danėnas, and G. Garšva, “SVM Accuracy and Training Speed Trade-Off \nin Sentiment Analysis Tasks,” in Communications in Computer and Information Science , \nSpringer Verlag, Oct. 2018, pp. 227–239. doi: 10.1007/978-3-319-99972-2_18. \n[52] R. K. BANIA, “COVID-19 Public Tweets Sentiment Analysis using TF-IDF and Inductive \nLearning Models,” no. 2, 2020. \n[53] “Parametric and Nonparametric Machine Learning Algorithms.” \nhttps://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-\nalgorithms/ (accessed May 25, 2021). \n[54] “COVID-19 pandemic - Wikipedia.” https://en.wikipedia.org/wiki/COVID-19_pandemic \n(accessed May 25, 2021). \n[55] S. Riaz, M. Fatima, M. Kamran, and M. W. Nisar, “Opinion mining on large scale data \nusing sentiment analysis and k-means clustering,” Cluster Comput, vol. 22, no. 3, pp. 7149–\n7164, May 2019, doi: 10.1007/s10586-017-1077-z. \n[56] V. Iosifidis and E. Ntoutsi, “Large Scale Sentiment Learning with Limited Labels,” vol. 10, \n2017, doi: 10.1145/3097983.3098159. \n[57] J. Khan and Y. K. Lee, “LeSSA: A unified framework based on lexicons and semi-\nsupervised learning approaches for textual sentiment classification,” Applied Sciences \n(Switzerland), vol. 9, no. 24, Dec. 2019, doi: 10.3390/app9245562. \n[58] D. Tang, B. Qin, and T. Liu, “Deep learning for sentiment analysis: successful approaches \nand future challenges,” WIREs Data Mining Knowl Discov, vol. 5, pp. 292–303, 2015, doi: \n10.1002/widm.1171. \n[59] X. Zhang and X. Zheng, “Comparison of text sentiment analysis based on machine \nlearning,” in Proceedings - 15th International Symposium on Paralle l and Distributed \nComputing, ISPDC 2016, Institute of Electrical and Electronics Engineers Inc., Apr. 2017, \npp. 230–233. doi: 10.1109/ISPDC.2016.39. \n 170 \n[60] “Extreme learning machine - Wikipedia.” \nhttps://en.wikipedia.org/wiki/Extreme_learning_machine (accessed May 25, 2021). \n[61] S. Sohangir, D. Wang, A. Pomeranets, and T. M. Khoshgoftaar, “Big Data: Deep Learning \nfor financial sentiment analysis,” J Big Data, doi: 10.1186/s40537-017-0111-6. \n[62] J. Qian, Z. Niu, and C. Shi, “Sentiment analysis model on weather related tweets with deep \nneural network,” in ACM International Conference Proceeding Series , New York, New \nYork, USA: Association for Computing Machinery, Feb. 2018, pp. 31 –35. doi: \n10.1145/3195106.3195111. \n[63] D.-H. Pham and A. -C. Le, “Learning Multip le Layers of Knowledge Representation for \nAspect Based Sentiment Analysis.” \n[64] Q. T. Ain et al., “Sentiment Analysis Using Deep Learning Techniques: A Review,” 2017. \n[65] Y. Gao, W. Rong, Y. Shen, and Z. Xiong, “Convolutional Neural Network based sentiment \nanalysis using Adaboost combination,” in Proceedings of the International Joint \nConference on Neural Networks, Institute of Electrical and Electronics Engineers Inc., Oct. \n2016, pp. 1333–1338. doi: 10.1109/IJCNN.2016.7727352. \n[66] “Word2vec - Wikipedia.” https://en.wikipedia.org/wiki/Word2vec (accessed Dec. 15, \n2020). \n[67] U. Gupta, A. Chatterjee, R. Srikanth, and P. Agrawal, “A Sentiment-and-Semantics-Based \nApproach for Emotion Detection in Textual Conversations,” Neu, 2017. \n[68] D. Tang, F. W ei, N. Yang, M. Zhou, T. Liu, and B. Qin, “Learning Sentiment -Specific \nWord Embedding for Twitter Sentiment Classification *,” Association for Computational \nLinguistics. \n[69] J. Pennington, R. Socher, and C. D. Manning, “GloVe: Global Vectors for Word \nRepresentation.” Accessed: May 25, 2021. [Online]. Available: http://nlp. \n[70] L. Li, T. T. Goh, and D. Jin, “How textual quality of online reviews affect classification \nperformance: a case of deep learning sentiment analysis,” Neural Comput Appl, vol. 32, no. \n9, pp. 4387–4415, May 2020, doi: 10.1007/s00521-018-3865-7. \n[71] S. Jebbara and P. Cimiano, “Zero-Shot Cross-Lingual Opinion Target Extraction.” \n 171 \n[72] O. Araque, I. Corcuera -Platas, J. F. Sánchez -Rada, and C. A. Iglesias, “Enhancing deep \nlearning sentiment analysis with ensemble techniques in social applications,” Expert Syst \nAppl, vol. 77, pp. 236–246, Jul. 2017, doi: 10.1016/j.eswa.2017.02.002. \n[73] M. E. Basiri, S. Nemati, M. Abdar, E. Cambria, and U. R. Acharya, “ABCDM: An \nAttention-based Bidirectional CNN-RNN Deep Model for sentiment analysis,” Future \nGeneration Computer Systems , vol. 115, pp. 279 –294, Feb. 2021, doi: \n10.1016/j.future.2020.08.005. \n[74] L. Gui, D. Wu, R. Xu, Q. Lu, and Y. Zhou, “Event-Driven Emotion Cause Extraction with \nCorpus Construct ion,” pp. 1639 –1649, Accessed: Jan. 23, 2022. [Online]. Available: \nhttp://hlt.hitsz.edu.cn/?page \n[75] R. Xia and Z. Ding, “Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in \nTexts,” Jun. 2019, [Online]. Available: http://arxiv.org/abs/1906.01267 \n[76] H. Bi and P. Liu, “ECSP: A New Task for Emotion -Cause Span -Pair Extraction and \nClassification,” Mar. 2020, [Online]. Available: http://arxiv.org/abs/2003.03507 \n[77] S. Yat, M. Lee, Y. Chen, and C.-R. Huang, “A Text-driven Rule-based System for Emotion \nCause Detection,” in NAACL HLT , 2010, pp. 45 –53. [Online]. Available: \nhttp://dbo.sinica.edu.tw/SinicaCorpus/ \n[78] W. Li and H. Xu, “Text -based emotion classification using emotion cause extraction,” \nExpert Syst Appl , vol. 41, no. 4 PART 2, pp. 1742 –1749, 2014, doi: \n10.1016/J.ESWA.2013.08.073. \n[79] K. Gao, H. Xu, and J. Wang, “A rule-based approach to emotion cause detection for Chinese \nmicro-blogs,” Expert Syst Appl , vol. 42, no. 9, pp. 4517 –4528, Jun. 2015, doi: \n10.1016/J.ESWA.2015.01.064. \n[80] K. Gao, H. Xu, and J. Wang, “Emotion cause detection for Chinese micro -blogs based on \nECOCC model,” Lecture Notes in Computer Science (including subseries Lecture Notes in \nArtificial Intelligence and Lecture Notes in Bioinformatics), vol. 9078, pp. 3–14, 2015, doi: \n10.1007/978-3-319-18032-8_1/COVER. \n[81] Y. Chen, S. Yat Mei Lee, S. Li, and C.-R. Huang, “Emotion Cause Detection with Linguistic \nConstructions,” pp. 179–187, 2010. \n 172 \n[82] J. Lafferty, A. McCallum, and F. Pereira, “Conditional Random Fields: Probabilistic \nModels for Segmenting and Labeling Sequence Data,” Departmental Papers (CIS) , Jun. \n2001, Accessed: Feb. 26, 2023. [On line]. Available: \nhttps://repository.upenn.edu/cis_papers/159 \n[83] S. Yada, K. Ikeda, K. Hoashi, and K. Kageura, “A Bootstrap Method for Automatic Rule \nAcquisition on Emotion Cause Extraction,” IEEE International Conference on Data Mining \nWorkshops, ICDMW , vol. 2017 -November, pp. 414 –421, Dec. 2017, doi: \n10.1109/ICDMW.2017.60. \n[84] L. Gui, R. Xu, Q. Lu, D. Wu, and Y. Zhou, “Emotion cause extraction, a challenging task \nwith corpus construction,” Communications in Computer and Information Science , vol. \n669, pp. 98–109, 2016, doi: 10.1007/978-981-10-2993-6_8/COVER. \n[85] R. Xu, J. Hu, Q. Lu, D. Wu, and L. Gui, “An ensemble approach for emotion cause detection \nwith event extraction and multi -kernel SVMs,” Tsinghua Sci Technol , vol. 22, no. 6, pp. \n646–659, Dec. 2017, doi: 10.23919/TST.2017.8195347. \n[86] X. Cheng, Y. Chen, B. Cheng, S. Li, and G. Zhou, “An Emotion Cause Corpus for Chinese \nMicroblogs with Multiple -User Structures,” ACM Transactions on Asian and Low -\nResource Language Information Processing (TALLIP) , v ol. 17, no. 1, Nov. 2017, doi: \n10.1145/3132684. \n[87] L. Gui, J. Hu, Y. He, R. Xu, Q. Lu, and J. Du, “A Question Answering Approach for \nEmotion Cause Extraction,” EMNLP 2017 - Conference on Empirical Methods in Natural \nLanguage Processing, Proceedings, pp. 1593–1602, 2017, doi: 10.18653/V1/D17-1167. \n[88] Z. Ding, H. He, M. Zhang, and R. Xia, “From Independent Prediction to Reordered \nPrediction: Integrating Relative Position and Global Label Information to Emotion Cause \nIdentification,” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. \n01, pp. 6343–6350, Jul. 2019, doi: 10.1609/AAAI.V33I01.33016343. \n[89] R. Xia, M. Zhang, and Z. Ding, “RTHN: A RNN -Transformer Hierarchical Network for \nEmotion Cause Extraction,” Jun. 2019, [Online]. Ava ilable: \nhttp://arxiv.org/abs/1906.01236 \n 173 \n[90] R. Xia and Z. Ding, “Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in \nTexts,” Jun. 2019, [Online]. Available: http://arxiv.org/abs/1906.01267 \n[91] P. Wei, J. Zhao, and W. Mao, “Effective Inter-Clause Modeling for End-to-End Emotion-\nCause Pair Extraction.” \n[92] C. Yuan, C. Fan, J. Bao, and R. Xu, “Emotion-Cause Pair Extraction as Sequence Labeling \nBased on A Novel Tagging Scheme.” Accessed: Feb. 27, 2021. [Online]. Available: \nhttps://github.com/huggingface/ \n[93] Z. Cheng, Z. Jiang, Y. Yin, N. Li, and Q. Gu, “A Unified Target -Oriented Sequence-to-\nSequence Model for Emotion -Cause Pair Extraction,” IEEE/ACM Trans Audio Speech \nLang Process, vol. 29, pp. 2779–2791, 2021, doi: 10.1109/TASLP.2021.3102194. \n[94] Q. Sun, Y. Yin, and H. Yu, “A Dual -Questioning Attention Network for Emotion -Cause \nPair Extraction with Context Awareness,” Apr. 2021, [Online]. Available: \nhttp://arxiv.org/abs/2104.07221 \n[95] A. Vaswani et al. , “Attention is all yo u need,” in Advances in Neural Information \nProcessing Systems , Neural information processing systems foundation, Jun. 2017, pp. \n5999–6009. Accessed: May 02, 2021. [Online]. Available: \nhttps://arxiv.org/abs/1706.03762v5 \n[96] C. Fan, C. Yuan, L. Gui, Y. Zhang, and R. Xu, “Multi-Task Sequence Tagging for Emotion-\nCause Pair Extraction Via Tag Distribution Refinement,” IEEE/ACM Trans Audio Speech \nLang Process, vol. 29, pp. 2339–2350, 2021, doi: 10.1109/TASLP.2021.3089837. \n[97] M. Stern, J. Andreas, and D. Klein, “A Minimal Span-Based Neural Constituency Parser,” \nACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, \nProceedings of the Conference (Long Papers) , vol. 1, pp. 818 –827, May 2017, doi: \n10.48550/arxiv.1705.03919. \n[98] K. Lee, L. He, M. Lewis, and L. Zettlemoyer, “End-to-end Neural Coreference Resolution,” \nEMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, \nProceedings, pp. 188–197, Jul. 2017, doi: 10.48550/arxiv.1707.07045. \n[99] “tf–idf - Wikipedia.” https://en.wikipedia.org/wiki/Tf–idf (accessed Dec. 15, 2020). \n 174 \n[100] “Word embeddings   |  TensorFlow Core.” \nhttps://www.tensorflow.org/tutorials/text/word_embeddings (accessed May 01, 2021). \n[101] J. Landthaler et al. , “Extending Thesauri Using  Word Embeddings and the Intersection \nMethod SaToS -Software aided analysis of Terms of Services View project Extending \nThesauri Using Word Embeddings and the Intersection Method,” 2017. \n[102] “Shallow Neural Networks. In this post, I have explained what… | by Rochak Agrawal | \nTowards Data Science.” https://towardsdatascience.com/shallow -neural-networks-\n23594aa97a5 (accessed May 24, 2021). \n[103] “Deep learning vs. machine learning - Azure Machine Learning | Microsoft Docs.” \nhttps://docs.microsoft.com/en-us/azure/machine-learning/concept-deep-learning-vs-\nmachine-learning (accessed May 02, 2021). \n[104] “Hyperparameter optimization - Wikipedia.” \nhttps://en.wikipedia.org/wiki/Hyperparameter_optimization (accessed May 24, 2021). \n[105] “Difference Between Supervised, Unsupervised, & Reinforcement Learning | NVIDIA \nBlog.” https://blogs.nvidia.com/blog/2018/08/02/supervised -unsupervised-learning/ \n(accessed Dec. 15, 2020). \n[106] O. Chapelle, B. Schölkopf, and A. Zien, Semi-supervised learning. Cambridge, Mass.: MIT \nPress, 2006. \n[107] N. Li, C. -Y. Chow, and J. -D. Zhang, “SEML: A Semi -Supervised Multi -Task Learning \nFramework for Aspect -Based Sentiment Analysis,” IEEE Access , vol. 8, pp. 189287 –\n189297, Oct. 2020, doi: 10.1109/access.2020.3031665. \n[108] A. Nöu, “Logistic Regression versus Support Vector Machines Matematiska institutionen,” \np. 42, 2018. \n[109] V. V. Corinna Cortes, “Support -Vector Networks,” Machine Leaming, vol. 20, pp. 273 –\n297, 1995, doi: 10.1109/64.163674. \n[110] E. W. Weisstein, “Maximum Entropy Method”. \n[111] V. Narayanan, I. Arora, and A. Bhatia, “Fast and accurate sentiment classification using an \nenhanced Naive Bayes model.” \n 175 \n[112] “Understanding Deep Learning: DNN, RNN, LSTM, CNN and R-CNN | by SPRH LABS | \nMedium.” https://medium.com/@sprhlabs/understanding-deep-learning-dnn-rnn-lstm-cnn-\nand-r-cnn-6602ed94dbff (accessed May 22, 2021). \n[113] “Activation function - Wikipedia.” https://en.wikipedia.org/wiki/Activation_function \n(accessed May 22, 2021). \n[114] “Training an Artificial Neural Network.” \nhttp://www2.psych.utoronto.ca/users/reingold/courses/ai/cache/neural3.html (accessed \nMay 20, 2021). \n[115] “Overview of GAN Structure   |  Generative Adversarial Networks.” \nhttps://developers.google.com/machine-learning/gan/gan_structure (ac cessed May 20, \n2021). \n[116] “Transfer learning - Wikipedia.” https://en.wikipedia.org/wiki/Transfer_learning (accessed \nMay 02, 2021). \n[117] D. Wang et al., “Comprehensive eye diagram analysis: A transfer learning approach,” IEEE \nPhotonics J, vol. 11, no. 6, Dec. 2019, doi: 10.1109/JPHOT.2019.2947705. \n[118] D. Bahdanau, K. H. Cho, and Y. Bengio, “Neural Machine Translation by Jointly Learning \nto Align and Translate,” 3rd International Conference on Learning Representations, ICLR \n2015 - Conference Track Proceedings, Sep. 2014, doi: 10.48550/arxiv.1409.0473. \n[119] T. Wolf et al., “Transformers: State-of-the-art natural language processing,” arXiv. arXiv, \npp. 38–45, Oct. 08, 2019. doi: 10.18653/v1/2020.emnlp-demos.6. \n[120] J. Devlin, M. -W. Chang, K. Lee, and K. Toutanova, “BERT: Pre -training of Deep \nBidirectional Transformers for Language Understanding,” NAACL HLT 2019 - 2019 \nConference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies - Proceedings of the  Conference, vol. 1, pp. \n4171–4186, Oct. 2018, Accessed: May 02, 2021. [Online]. Available: \nhttp://arxiv.org/abs/1810.04805 \n[121] “GPT-2 - Wikipedia.” https://en.wikipedia.org/wiki/GPT-2 (accessed May 02, 2021). \n 176 \n[122] O. Russakovsky et al. , “ImageNet Large  Scale Visual Recognition Challenge,” Int J \nComput Vis , vol. 115, no. 3, pp. 211 –252, Dec. 2015, doi: 10.1007/S11263 -015-0816-\nY/FIGURES/16. \n[123] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient Estimation of Word \nRepresentations in Vector Space,” 1st International Conference on Learning \nRepresentations, ICLR 2013 - Workshop Track Proceedings , Jan. 2013, doi: \n10.48550/arxiv.1301.3781. \n[124] A. M. Dai and Q. v. Le, “Semi -supervised Sequence Learning,” Adv Neural Inf Process \nSyst, vol. 28, 2015, Acces sed: Feb. 26, 2023. [Online]. Available: \nhttp://ai.Stanford.edu/amaas/data/sentiment/index.html \n[125] J. Howard and S. Ruder, “Universal Language Model Fine-tuning for Text Classification,” \nACL 2018 - 56th Annual Meeting of the Association for Computationa l Linguistics, \nProceedings of the Conference (Long Papers) , vol. 1, pp. 328 –339, Jan. 2018, doi: \n10.48550/arxiv.1801.06146. \n[126] M. E. Peters et al., “Deep Contextualized Word Representations,” NAACL HLT 2018 - 2018 \nConference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies - Proceedings of the Conference , vol. 1, pp. \n2227–2237, 2018, doi: 10.18653/V1/N18-1202. \n[127] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improvi ng Language \nUnderstanding by Generative Pre -Training,” OpenAI, 2018, Accessed: Jan. 08, 2023. \n[Online]. Available: https://gluebenchmark.com/leaderboard \n[128] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language Models are \nUnsupervised Multitask Learners,” OpenAI blog, vol. 1(8), no. 9, 2019, Accessed: May 03, \n2021. [Online]. Available: https://github.com/codelucas/newspaper \n[129] T. B. Brown et al., “Language Models are Few -Shot Learners,” 2020, Accessed: Jun. 12, \n2022. [Online]. Available: https://commoncrawl.org/the-data/ \n[130] S. Zhang et al., “OPT: Open Pre -trained Transformer Language Models”, Accessed: Jun. \n05, 2022. [Online]. Available: https://bigscience. \n 177 \n[131] “ChatGPT: Optimizing Language Models for Dialogue.” https://opena i.com/blog/chatgpt/ \n(accessed Jan. 11, 2023). \n[132] Y. Bengio et al., “A neural probabilistic language model,” The Journal of Machine Learning \nResearch, vol. 3, pp. 1137–1155, Mar. 2003, doi: 10.5555/944919.944966. \n[133] L. Fei -Fei, R. Fergus, and P. Peron a, “A Bayesian approach to unsupervised one -shot \nlearning of object categories,” Proceedings of the IEEE International Conference on \nComputer Vision, vol. 2, pp. 1134–1141, 2003, doi: 10.1109/ICCV.2003.1238476. \n[134] M. Fink, “Object Classification from a Single Example Utilizing Class Relevance Metrics,” \nin Advances in neural information processing systems, 2004. \n[135] L. Zhang, S. Wang, and B. Liu, “Deep Learning for Sentiment Analysis: A Survey,” Wiley \nInterdiscip Rev Data Min Knowl Discov, vol. 8(4), no. e1253, 2018. \n[136] F. Ricci, B. Shapira, and L. Rokach, “Recommender systems: Introduction and challenges,” \nin Recommender Systems Handbook, Second Edition, 2015. doi: 10.1007/978-1-4899-7637-\n6_1. \n[137] E. D. Liddy, “Natural Language Processing,” 2001, A ccessed: May 02, 2022. [Online]. \nAvailable: https://surface.syr.edu/istpub \n[138] D. Patel, “Approaches for Sentiment Analysis on Twitter: A State-of-Art study,” 2015. \n[139] D. Croce, G. Castellucci, and R. Basili, “GAN-BERT: Generative Adversarial Learning for \nRobust Text Classification with a Bunch of Labeled Examples,” in Proceedings of the 58th \nAnnual Meeting of the Association for Computational Linguistics , Association fo r \nComputational Linguistics, 2020, pp. 2114–2119. \n[140] S. Liu, X. Cheng, F. Li, and F. Li, “TASC: Topic -adaptive sentiment classification on \ndynamic tweets,” IEEE Trans Knowl Data Eng , vol. 27, no. 6, pp. 1696 –1709, Jun. 2015, \ndoi: 10.1109/TKDE.2014.2382600. \n[141] Z. Miao, Y. Li, Y. Ai, M. Labs, X. Wang, and X. Ai, “Snippext: Semi -supervised Opinion \nMining with Augmented Data,” 2020, doi: 10.1145/3366423.3380144. \n[142] S. Mazharul Islam, X. Dong, and G. de Melo, “Domain -Specific Sentiment Lexicons \nInduced from Labeled Documents,” in Proceedings of the 28th International Conference \n 178 \non Computational Linguistics , Online, 2020, pp. 6576 –6587. Accessed: Feb. 27, 2021. \n[Online]. Available: http://sentimentanalysis.org \n[143] F. Hemmatian and M. K. Sohrabi, “A survey on classification techniques for opinion mining \nand sentiment analysis,” Artif Intell Rev , vol. 52, no. 3, pp. 1495 –1545, Oct. 2019, doi: \n10.1007/s10462-017-9599-6. \n[144] M. P. Kumar, B. Packer, and D. Koller, “Self-Paced Learning for Latent Variable Models,” \nNIPS, vol. 1, p. 2, 2010. \n[145] E.-P. Mastoropoulou, “Enhancing Deep Active Learning Using Selective Self-Training For \nImage Classification,” 2019. \n[146] F. Ma, D. Meng, Q. Xie, Z. Li, and X. Dong, “Self-Paced Co-training,” PMLR, Jul. 2017. \n[147] X. Li et al., “Learning to Self -Train for Semi -Supervised Few-Shot Classification,” Adv \nNeural Inf Process Syst, vol. 32, pp. 10276–10286, 2019. \n[148] T. Yang, L. Hu, C. Shi, H. Ji, X. Li, and L. Nie, “HGAT: Heterogeneous Graph Attention \nNetworks for Semi-supervised Short Text Classification,” ACM Trans Inf Syst, vol. 39, no. \n3, May 2021, doi: 10.1145/3450352. \n[149] H. Kim, J. Son, and Y. -S. Han, “LST: Lexicon -Guided Self-Training for Few -Shot Text \nClassification,” Feb. 2022, [Online]. Available: http://arxiv.org/abs/2202.02566 \n[150] H. Q. Abonizio, E. C. Paraiso, and S. Barbon Junior, “Toward Text Data Augmentation for \nSentiment Analysis,” IEEE Transactions on Artificial Intelligence, pp. 1–1, Sep. 2021, doi: \n10.1109/tai.2021.3114390. \n[151] Y. Sun et al., “ERNIE: Enhanced Representation through Knowledge Integration,” in arXiv \ne-prints, 2019. Accessed: May 02, 2022. [Online]. Available: \nhttps://github.com/PaddlePaddle/ \n[152] S. Edunov, M. Ott, M. Auli, and D. Grangier, “Understanding Back -Translation at Scale,” \n2018. \n[153] V. Barriere and A. Balahur, “Improving Sentiment Analysis over non-English Tweets using \nMultilingual Transformers and Automatic Translation for Data-Augmentation,” Oct. 2020, \n[Online]. Available: http://arxiv.org/abs/2010.03486 \n 179 \n[154] A. Edwards, A. Ushio, J. Camacho -Collados, H. de Ribaupierre, and A. Preece, “Guiding \nGenerative Language Models for Data Augmentation in Few -Shot Text Classification,” \nNov. 2021, [Online]. Available: http://arxiv.org/abs/2111.09064 \n[155] K. Clark, M.-T. Luong, G. Brain, Q. V Le Google Brain, and C. D. Manning, “ELECTRA: \nPre-training Text Encoders as Discriminators Rather than Generators,” in ICLR, 2020. \n[156] D. Yarowsky, “Unsupervised word sense disambiguation rivaling supervised methods,” \nAssociation for Co mputational Linguistics (ACL), 1995, pp. 189 –196. doi: \n10.3115/981658.981684. \n[157] “Self Training in Semi -supervised learning — Scikit-learn documentation.” https://scikit -\nlearn.org/stable/modules/semi_supervised.html#self-training (accessed Nov. 15, 2021). \n[158] “Stochastic gradient descent - Wikipedia.” \nhttps://en.wikipedia.org/wiki/Stochastic_gradient_descent (accessed May 26, 2021). \n[159] “XGBoost Documentation — xgboost 1.5.0 -SNAPSHOT documentation.” \nhttps://xgboost.readthedocs.io/en/latest/ (accessed May 16, 2021). \n[160] I. Loshchilov and F. Hutter, “Decoupled Weight Decay Regularization,” in ICLR, 2019. \n[161] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimization,” in 3rd \nInternational Conference on Learning Representations, ICLR 2015  - Conference Track \nProceedings, International Conference on Learning Representations, ICLR, Dec. 2015. \n[162] M. Riyadh and M. O. Shafiq, “Towards Multiclass Sentiment Analysis With Limited \nLabeled Data,” in IEEE International Conference on Big Data , 2021, pp. 4955–4964. \n[163] “NumPy.” https://numpy.org/ (accessed May 15, 2021). \n[164] “Matplotlib: Python plotting  — Matplotlib 3.4.2 documentation.” https://matplotlib.org/ \n(accessed May 15, 2021). \n[165] “TensorFlow.” https://www.tensorflow.org/ (accessed May 15, 2021). \n[166] “GitHub - crux82/ganbert: Enhancing the BERT training with Semi-supervised Generative \nAdversarial Networks.” https://github.com/crux82/ganbert (accessed May 26, 2021). \n 180 \n[167] “TensorFlow Hub - Electra Large.” https://tfhub.dev/google/electra_large/2 (accessed May \n26, 2021). \n[168] “Colaboratory – Google.” https://research.google.com/colaboratory/fa q.html (accessed \nMay 15, 2021). \n[169] “Cloud Tensor Processing Units (TPUs)   |  Google Cloud.” \nhttps://cloud.google.com/tpu/docs/tpus (accessed May 02, 2022). \n[170] “What Is a GPU? Graphics Processing Units Defined.” \nhttps://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html \n(accessed May 15, 2021). \n[171] R. Socher et al., “Recursive Deep Models for Semantic Compositionality Over a Sentiment \nTreebank,” in Proceedings of the 2013 Conference on Empirical Methods in Natural \nLanguage Processing, Association for Computational Linguistics, 2013, pp. 1631 –1642. \nAccessed: May 22, 2021. [Online]. Available: http://nlp.stanford.edu/ \n[172] “Twitter US Airline Sentiment | Kaggle.” https://www.kaggle.com/crowdflower/twitter -\nairline-sentiment (accessed May 22, 2021). \n[173] S. Rosenthal, N. Farra, and P. Nakov, “SemEval -2017 Task 4: Sentiment Analysis in \nTwitter,” in Proceedings of the 11th International Workshop on Semantic Evaluation \n({S}em{E}val-2017), Vancouver, Canada: Association for Computational Linguistics, Aug. \n2017, pp. 502–518. doi: 10.18653/v1/S17-2088. \n[174] P. G. de Vries, “Stratified Random Sampling,” in Sampling Theory for Forest Inventory , \nSpringer, 1986, pp. 31–55. \n[175] J. Dai, H. Yan, T. Sun, P. Liu, and X. Qiu, “Does syntax matter? A strong baseline for \nAspect-based Sentiment Analysis with RoBERTa,” in Conference of the North American \nChapter of the Association for Computational Linguistics: Human Language Technologies \n, 2021, pp. 1816–1829. Accessed: May 05, 2023. [Online]. Available: https://github.com/ \n[176] “sklearn.metrics.f1_score — scikit-learn 1.2.2 documentation.” https://scikit -\nlearn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_scor\ne (accessed May 05, 2023). \n 181 \n[177] T. Mitchell, Machine Learning. McGraw-Hill International, 1997. \n[178] J. Brownlee, “Statistical Methods for Machine Learning Discover how to Transform Data \ninto Knowledge with Python,” 2019. \n[179] M. Grandini, E. Bagli, G. Visani, M. Grandini, E. Bagli, and G. Visani, “Metrics for Multi-\nClass Classification: an Overview,” ArXiv, p. arXiv:2008.05756, Aug. 2020, doi: \n10.48550/ARXIV.2008.05756. \n[180] F. Wilcoxon, “Individual comparisons by ranking meth ods, Biometrics Bulletin, 1, 80 –\n83.(1947) Probability tables for individual comparisons by ranking methods.” Biomet, \n1945. \n[181] J. Derrac, S. García, D. Molina, and F. Herrera, “A practical tutorial on the use of \nnonparametric statistical tests as a metho dology for comparing evolutionary and swarm \nintelligence algorithms,” Swarm Evol Comput , vol. 1, pp. 3 –18, 2011, doi: \n10.1016/j.swevo.2011.02.002. \n[182] J. Demšar, “Statistical Comparisons of Classifiers over Multiple Data Sets,” Journal of \nMachine Learning Research, vol. 7, pp. 1–30, 2006. \n[183] Y. Kim and O. Zhang, “Credibility Adjusted Term Frequency: A Supervised Term \nWeighting Scheme for Sentiment Analysis and Text Classification,” pp. 79 –83, 2014, \nAccessed: May 05, 2023. [Online]. Available: \nhttps://www.cs.cornell.edu/people/pabo/movie-review- \n[184] Y. Guo, Q. Hu, M. Cordy, M. Papadakis, and Y. Le Traon, “DRE: density -based data \nselection with entropy for adversarial-robust deep learning models,” Neural Comput Appl, \nvol. 35, no. 5, pp. 4009–4026, Feb. 2023, doi: 10.1007/S00521-022-07812-2/TABLES/6. \n[185] S. Zhang, X. Zhang, and J. Chan, “A word -character convolutional neural network for \nlanguage-agnostic twitter sentiment analysis,” ACM International Conference Proceeding \nSeries, vol. 2017-December, Dec. 2017, doi: 10.1145/3166072.3166082. \n[186] B. Ko and H. J. Choi, “Twice fine -tuning deep neural networks for paraphrase \nidentification,” Electron Lett , vol. 56, no. 9, pp. 449 –450, Apr. 2020, doi: \n10.1049/el.2019.4183. \n 182 \n[187] “Optimization: Stochastic Gradient Descent.” \nhttp://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDesce\nnt/ (accessed May 02, 2022). \n[188] A. Zadeh, M. Chen, S. Poria, E. Cambria, and L.-P. Morency, “Tensor Fusion Network for \nMultimodal Senti ment Analysis,” in Conference on Empirical Methods in Natural \nLanguage Processing (EMNLP), 2017, pp. 1103–1114. \n[189] Z. Li, Y. Zou, C. Zhang, Q. Zhang, and Z. Wei, “Learning Implicit Sentiment in Aspect -\nbased Sentiment Analysis with Supervised Contrastive  Pre-Training,” in Conference on \nEmpirical Methods in Natural Language Processing (EMNLP) , 2021, pp. 246 –256. \nAccessed: May 05, 2023. [Online]. Available: https://github.com/Tribleave/SCAPT-ABSA \n[190] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On Ca libration of Modern Neural \nNetworks,” 2017. \n[191] M. Riyadh and M. O. Shafiq, “GAN -BElectra: Enhanced Multi -class Sentiment Analysis \nwith Limited Labeled Data,” https://doi.org/10.1080/08839514.2022.2083794, vol. 36, no. \n1, 2022, doi: 10.1080/08839514.2022.2083794. \n[192] M. Riyadh and M. Omair Shafiq, “Towards Multi -class Sentiment Analysis with Limited \nLabeled Data,” Proceedings - 2021 IEEE International Conference on Big Data, Big Data \n2021, pp. 4955–4964, 2021, doi: 10.1109/BIGDATA52589.2021.9671692. \n[193] M. Hardalov, A. Arora, P. Nakov, and I. Augenstein, “Few -Shot Cross -Lingual Stance \nDetection with Sentiment -Based Pre -Training,” Sep. 2021, [Online]. Available: \nhttp://arxiv.org/abs/2109.06050 \n[194] M. Li, H. Zhao, H. Su, Y. R. Qian, and P. Li, “Emotion -cause span extraction: a new task \nto emotion cause identification in texts,” Applied Intelligence, vol. 51, no. 10, pp. 7109 –\n7121, Oct. 2021, doi: 10.1007/s10489-021-02188-7. \n[195] F. Wang, Z. Ding, R. Xia, and J. Li Zhaoyu and Yu, “Multimodal Emotion -Cause Pair \nExtraction in Conversations,” Oct. 2021. \n[196] S. Dutta, D. Das, and T. Chakraborty, “Changing views: Persuasion modeling and argument \nextraction from online discussions,” Inf Process Manag , vol. 57, no. 2, p. 102085, Mar. \n2020, doi: 10.1016/J.IPM.2019.102085. \n 183 \n[197] S. Yu, G. D. S. Martino, and P. Nakov, “Experiments in Detecting Persuasion Techniques \nin the News,” Nov. 2019, [Online]. Available: http://arxiv.org/abs/1911.06815 \n[198] H. Chen, D. Ghosal, N. Majumder, A. Hussain, and S. Poria, “Persuasive Dialogue \nUnderstanding: the Baselines and Negative Results,” Nov. 2020, [Online]. Available: \nhttp://arxiv.org/abs/2011.09954 \n[199] S. Das and A. K. Kolya, “Predicting the pandemic: sentiment evaluation and predictive \nanalysis from large-scale tweets on Covid-19 by deep convolutional neural network,” Evol \nIntell, vol. 15, no. 3, pp. 1913 –1934, Sep. 2022, doi: 10.1007/S12065 -021-00598-\n7/FIGURES/7. \n[200] A. Ramesh et al. , “Zero -Shot Text -to-Image Generation,” Feb. 2021, doi: \n10.48550/arxiv.2102.12092. \n[201] S. Yada, K. Ikeda, and K. Hoashi Keiichiro and Kageura, “A bootstrap method for \nautomatic rule acquisition on emotion cause extraction,” in 2017 IEEE International \nConference on Data Mining Workshops (ICDMW), IEEE, Nov. 2017. \n[202] C. Donahue, M. Lee, and P. Liang, “Enabling Language Models to Fill in the Blanks,” May \n2020, [Online]. Available: http://arxiv.org/abs/2005.05339 \n[203] S. Y. M. Lee, Y. Chen, C. R. Huang, and S. Li, “Detecting Emotion Causes With A \nLinguistic Rule-based Approach,” Comput Intell, vol. 29, no. 3, pp. 390 –416, Aug. 2013, \ndoi: 10.1111/J.1467-8640.2012.00459.X. \n[204] Y. Chen, W. Hou, and X. Cheng, “Hierarchical convolution neural network for emotion \ncause detection on microblogs,” Lecture Notes in Computer Science (including  subseries \nLecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) , vol. 11139 \nLNCS, pp. 115–122, 2018, doi: 10.1007/978-3-030-01418-6_12/COVER/. \n[205] B. Xu, H. Lin, Y. Lin, Y. Diao, L. Yang, and K. Xu, “Extracting Emotion Causes Usi ng \nLearning to Rank Methods from an Information Retrieval Perspective,” IEEE Access, vol. \n7, pp. 15573–15583, 2019, doi: 10.1109/ACCESS.2019.2894701. \n[206] R. Xia and Z. Ding, “Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in \nTexts.” \n 184 \n[207] C. Fan, C. Yuan, J. Du, L. Gui, M. Yang, and R. Xu, “Transition -based Directed Graph \nConstruction for Emotion -Cause Pair Extraction,” pp. 3707 –3717, Jul. 2020, doi: \n10.18653/V1/2020.ACL-MAIN.342. \n[208] S. Smith et al., “Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A \nLarge-Scale Generative Language Model”. \n[209] F. Petroni et al. , “Language Models as Knowledge Bases?”, Accessed: Jun. 12, 2022. \n[Online]. Available: https://github.com/pytorch/fairseq \n[210] R. Zellers et al. , “Defending Against Neural Fake News”, Accessed: Jun. 12, 2022. \n[Online]. Available: https://rowanzellers.com/grover \n[211] A. See, A. Pappu, R. Saxena, A. Yerukola, and C. D. Manning, “Do Massively Pretrained \nLanguage Models Make Better Storytellers?”, Accessed: Jun. 12, 2022. [Online]. Available: \nhttps://github.com/huggingface/ \n[212] M. Joshi et al. , “SpanBERT: Improving Pre -training by Representing and Predicting \nSpans”, Accessed: Feb. 19, 2022. [Online]. Available: \nhttps://github.com/facebookresearch/ \n[213] K. Papineni, S. Roukos, T. Ward, and W. -J. Zhu, “BLEU: a Method for Automatic \nEvaluation of Machine Translation”. \n[214] S. Banerjee and A. Lavie, “METEOR: An Automatic Metric for MT Evaluation with \nImproved Correlation with Human Judgments”. \n[215] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of Summaries”. \n[216] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, “BERTSCORE: \nEVALUATING TEXT GENERATION WITH BERT”, Accessed: Jun. 12, 2022. [Online]. \nAvailable: https://github.com/Tiiiger/bert_score. \n[217] R. Flesch, “A new readability yardstick,” Journal of Applied Psychology, vol. 32, no. 3, pp. \n221–233, Jun. 1948, doi: 10.1037/H0057532. \n[218] “Coleman–Liau index - Wikipedia.” \nhttps://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index (accessed Jun. 12, 2022). \n 185 \n[219] W. Zhu and S. Bhat, “GRUEN for Evaluating Linguistic Quality of Generated Text”, \nAccessed: May 16, 2022. [Online]. Available: http://tac.nist.gov/ \n[220] “cardiffnlp/twitter-roberta-base-sentiment · Hugging Face.” \nhttps://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment (accessed Jan. 08, 2023). \n[221] “j-hartmann/emotion-english-distilroberta-base · Hugging Face.” https://huggingface.co/j -\nhartmann/emotion-english-distilroberta-base (accessed May 06, 2023). \n[222] J. Novikova, O. Dušek, A. C. Curry, and V. Rieser, “Why We Need New Evaluation Metrics \nfor NLG”, Accessed: Jun. 12, 2022. [Online]. Available: \nhttps://github.com/glampouras/JLOLS_ \n[223] “language-check · PyPI.” https://pypi.org/project/language-check/ (accessed Jun. 12, 2022). \n[224] D. Ping, “The Machine Learning Solutions Architect Handbook,” p. 442, 2022, Accessed: \nJan. 08, 2023. [Online]. Available: https://www.oreilly.com/library/view/the -machine-\nlearning/9781801072168/ \n[225] G. Bonaccorso, “Machine Learni ng Algorithms: Popular algorithms for data science and \nmachine ... - Giuseppe Bonaccorso - Google Libros,” Packt Publishing, vol. 2nd ed, pp. 0–\n332, 2018, Accessed: Jan. 08, 2023. [Online]. Available: \nhttps://www.oreilly.com/library/view/machine-learning-algorithms/9781789347999/ \n[226] M. Riyadh and M. O. Shafiq, “GAN -BElectra: Enhanced Multi -class Sentiment Analysis \nwith Limited Labeled Data,” https://doi.org/10.1080/08839514.2022.2083794, vol. 36, no. \n1, 2022, doi: 10.1080/08839514.2022.2083794. \n[227] M. Riyadh and M. Omair Shafiq, “Towards Multi -class Sentiment Analysis with Limited \nLabeled Data,” Proceedings - 2021 IEEE International Conference on Big Data, Big Data \n2021, pp. 4955–4964, 2021, doi: 10.1109/BIGDATA52589.2021.9671692. \n[228] J. Zhou, J. Tian, R. Wang, Y. Wu, W. Xiao, and L. He, “SentiX: A Sentiment -Aware Pre-\nTrained Model for Cross -Domain Sentiment Analysis,” pp. 568 –579, Jan. 2020, doi: \n10.18653/V1/2020.COLING-MAIN.49. \n[229] Y. Liu and M. Lapata, “Text Summarization with Pretrained Encoders, ” EMNLP-IJCNLP \n2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th \n 186 \nInternational Joint Conference on Natural Language Processing, Proceedings of the \nConference, pp. 3730–3740, Aug. 2019, doi: 10.48550/arxiv.1908.08345. \n[230] Y. Niu, F. Huang, J. Liang, W. Chen, X. Zhu, and M. Huang, “A Semantic -based Method \nfor Unsupervised Commonsense Question Answering,” ACL-IJCNLP 2021 - 59th Annual \nMeeting of the Association for Computational Linguistics and the 11th International Joint \nConference on Natural Language Processing, Proceedings of the Conference , pp. 3037 –\n3049, May 2021, doi: 10.48550/arxiv.2105.14781. \n[231] L. Cui et al., “Template-Based Named Entity Recognition Using BART,” ACL-IJCNLP, \n2021. \n[232] Q. Gao et al., “Overview of NTCIR-13 ECA Task.” \n[233] A. Singh, S. Hingane, S. Wani, and A. Modi, “An End-to-End Network for Emotion-Cause \nPair Extraction,” Mar. 2021, [Online]. Available: http://arxiv.org/abs/2103.01544 \n[234] W. Fan, Y. Zhu, Z. Wei, T. Yang, W. H. Ip, and Y. Zhang, “Order -guided deep neural \nnetwork for emotion -cause pair prediction,” Appl Soft Comput , vol. 112, Nov. 2021, doi: \n10.1016/j.asoc.2021.107818. \n[235] W. Zhu, Z. Hu, and E. P. Xing, “Text Infilling,” 20 19, Accessed: Feb. 12, 2022. [Online]. \nAvailable: https://github.com/VegB/Text_Infilling \n[236] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata, “Zero -Shot Learning-A Comprehensive \nEvaluation of the Good, the Bad and the Ugly”. \n[237] D. Pascual, B. Egressy , C. Meister, R. Cotterell, and R. Wattenhofer, “A Plug -and-Play \nMethod for Controlled Text Generation,” Sep. 2021, [Online]. Available: \nhttp://arxiv.org/abs/2109.09707 \n[238] Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing, “Toward Controlled \nGeneration of Text,” 2017. \n[239] S. Kumar, E. Malmi, A. Severyn, and Y. Tsvetkov, “Controlled Text Generation as \nContinuous Optimization with Multiple Constraints”, Accessed: Jun. 25, 2022. [Online]. \nAvailable: https://github.com/Sachin19/mucoco \n 187 \n[240] I. Sin gh, A. Barkati, T. Goswamy, and A. Modi, “Adapting a Language Model for \nControlled Affective Text Generation,” Nov. 2020, [Online]. Available: \nhttp://arxiv.org/abs/2011.04000 \n[241] N. Madaan, I. Padhi, N. Panwar, and D. Saha, “Generate Your Counterfactuals : Towards \nControlled Counterfactual Generation for Text,” 2021. [Online]. Available: www.aaai.org \n[242] K. Yang and D. Klein, “FUDGE: Controlled Text Generation With Future Discriminators”, \nAccessed: Jun. 25, 2022. [Online]. Available: https://github.com/yangkevin2/ \n[243] K. Yang et al. , “Tailor: A Prompt -Based Approach to Attribute -Based Controlled Text \nGeneration.” \n[244] B. Lester, R. Al -Rfou, and N. Constant, “The Power of Scale for Parameter -Efficient \nPrompt Tuning,” EMNLP 2021 - 2021 Conference on Empi rical Methods in Natural \nLanguage Processing, Proceedings , pp. 3045 –3059, Apr. 2021, doi: \n10.48550/arxiv.2104.08691. \n[245] M. Ghazvininejad, V. Karpukhin, V. Gor, and A. Celikyilmaz, “Discourse -Aware Soft \nPrompting for Text Generation.” \n[246] S. Wu, F. Chen, F. Wu, Y. Huang, and X. Li, “A Multi-Task Learning Neural Network for \nEmotion-Cause Pair Extraction.” Accessed: Feb. 21, 2021. [Online]. Available: \nhttps://github.com/wusx00/MTNECP \n[247] “Natural language processing - Research - Cardiff University.” \nhttps://www.cardiff.ac.uk/research/explore/research-units/natural-language-processing \n(accessed May 06, 2023). \n[248] “vennify/t5-base-grammar-correction · Hugging Face.” https://huggingface.co/vennify/t5 -\nbase-grammar-correction (accessed Jan. 08, 2023). \n[249] “facebook/opt-1.3b · Hugging Face.” https://huggingface.co/facebook/opt -1.3b (accessed \nJan. 08, 2023). \n[250] “Google Colab.” https://research.google.com/colaboratory/faq.html (accessed Jan. 08, \n2023). \n[251] “gpt2-xl · Hugging Face.” https://huggingface.co/gpt2-xl (accessed Jan. 08, 2023). \n 188 \n[252] “OPTConfig.” \nhttps://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTConfig \n(accessed Jan. 11, 2023). \n[253] “GPT2 Config.” \nhttps://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Config \n(accessed Jan. 11, 2023). \n[254] “Generation.” \nhttps://huggingface.co/docs/transformers/v4.24.0/en/main_classes/text_generation#transfo\nrmers.GenerationConfig (accessed Jan. 11, 2023). \n[255] Z. Yang, S. Ram, and F. Currim, “What Makes an Online Suggest ion A Good One for \nOnline Health Communities Communities,” 2000, Accessed: Nov. 10, 2022. [Online]. \nAvailable: https://aisel.aisnet.org/amcis2020/healthcare_it/healthcare_it/1 \n[256] L. Hanu, “Detoxify,” Github, 2020. https://github.com/unitaryai/detoxify (accessed Jan. 08, \n2023). \n[257] F. Barbieri, J. Camacho -Collados, L. Neves, and L. Espinosa -Anke, “TWEETEVAL: \nUnified Benchmark and Comparative Evaluation for Tweet Classification”, Accessed: Jan. \n08, 2023. [Online]. Available: https://github.com/cardiffnlp/tweeteval \n[258] HF Canonical Model Maintainers, “distilbert -base-uncased-finetuned-sst-2-english,” \nHuggingFace, 2022. https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-\nenglish (accessed Jan. 08, 2023). \n[259] M. Moshkov, “Greedy Algorithms,” Wiley Encyclopedia of Electrical and Electronics \nEngineering, pp. 1–11, Jun. 2015, doi: 10.1002/047134608X.W8262. \n[260] “rominf/profanity-filter: A Python library for detecting and filtering profanity.” \nhttps://github.com/rominf/profanity-filter (accessed Jan. 08, 2023). \n[261] “Qualtrics XM: The Leading Experience Management Software.” \nhttps://www.qualtrics.com/ (accessed Jan. 08, 2023). \n[262] “Amazon Mechanical Turk.” https://www.mturk.com/ (accessed Jan. 08, 2023). \n 189 \n[263] “How to Label Response Scale Points in Your Survey | Qualtrics.” \nhttps://www.qualtrics.com/blog/how-to-label-response-scale-points-in-your-survey-to-\navoid-misdirecting-respondents/ (accessed Jan. 08, 2023). \n[264] K. S. Dobson A N D and K. J. Mothersill, “Equidistant Categorical Labels For Construction \nOf Likert-type Scales,” 1979. \n[265] “About PythonTM | Python.org.” https://www.python.org/about/ (accessed Jan. 08, 2023). \n[266] “language-tool-python · PyPI.” https://pypi.org/project/language -tool-python/ (accessed \nJan. 08, 2023). \n[267] “huggingface (Hugging Face).” https://huggingface.co/huggingface (accessed Jan. 08, \n2023). \n[268] “SPSS Software | IBM.” https://www.ibm.com/spss (accessed Jan. 08, 2023). \n[269] “Microsoft Excel Spreadsheet Software | Microsoft 365.” https://www.microsoft.com/en -\nus/microsoft-365/excel (accessed Jan. 08, 2023). \n[270] “pandas - Python Data Analysis Library.” https://pandas.pydata.org/ (accessed Jan. 08, \n2023). \n[271] L. Hansen and K. Enevoldsen, “TextDescriptives: A Python package for calculating a large \nvariety of statistics from text,” Jan. 2023, doi: 10.48550/arxiv.2301.02057. \n[272] “Explore / Twitter.” https://twitter.com/ (accessed Jan. 08, 2023). \n[273] “Snscrape: A social networking service scraper in Python.” \nhttps://github.com/JustAnotherArchivist/snscrape (accessed Jan. 08, 2023). \n[274] “Measure reading levels with readability indexes.” \nhttps://www.wyliecomm.com/2021/11/measure-reading-levels-with-readability-indexes/ \n(accessed Jan. 08, 2023). \n[275] “Readability formulas – Readable.” https://readable.com/features/readability -formulas/ \n(accessed Jan. 08, 2023). \n 190 \n[276] “The Chi -Square Test | Introduction to  Statistics | JMP.” \nhttps://www.jmp.com/en_be/statistics-knowledge-portal/chi-square-test.html (accessed \nJan. 08, 2023). \n[277] “NLP vs. NLU vs. NLG: the differences between three natural language processing \nconcepts - Watson Blog.” https://www.ibm.com/blog s/watson/2020/11/nlp-vs-nlu-vs-nlg-\nthe-differences-between-three-natural-language-processing-concepts/ (accessed Jan. 20, \n2023). \n[278] W. Zhao, M. Peyrard, F. Liu, Y. Gao, C. M. Meyer, and S. Eger, “MoverScore: Text \nGeneration Evaluating with Contextualize d Embeddings and Earth Mover Distance”, \nAccessed: Feb. 18, 2023. [Online]. Available: http://tiny.cc/vsqtbz \n[279] L. Ouyang et al., “Training language models to follow instructions with human feedback,” \nMar. 2022, doi: 10.48550/arxiv.2203.02155. \n[280] “Model index for researchers - OpenAI API.” https://platform.openai.com/docs/model -\nindex-for-researchers (accessed Feb. 18, 2023). \n[281] B. Guo et al., “How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, \nand Detection”, Accessed: Feb. 11, 2023. [Online]. Available: https://chat.openai.com/chat \n[282] C. Mellish and R. Dale, “Evaluation in the context of natural language generation,” Comput \nSpeech Lang, vol. 12, no. 4, pp. 349–373, Oct. 1998, doi: 10.1006/CSLA.1998.0106. \n[283] J. Coch, “Evaluating and comparing three text-production techniques,” in COLING, 1996. \n[284] S. Bangalore, O. Rambow, and S. Whittaker, “Evaluation Metrics for Generation”. \n[285] T. Marciniak and M. Strube, “Classification -based generation using TAG,” Lecture Notes \nin Computer Science (including subseries Lecture Notes in Artificial Intelligence and \nLecture Notes in Bioinformatics) , vol. 3123, pp. 100 –109, 2004, doi: 10.1007/978 -3-540-\n27823-8_11/COVER. \n[286] I. Langkilde-Geary, “An Empirical Verification of Coverage and Correctness for a General-\nPurpose Sentence Generator”. \n[287] E. Reiter and S. Sripada, “Should Corpora Texts Be Gold Standards for NLG?” pp. 97–104, \n2002. Accessed: Feb. 18, 2023. [Online]. Available: https://aclanthology.org/W02-2113 \n 191 \n[288] A. Belz and E. Reiter, “Comparing Automatic and Human Evaluation of NLG Systems”. \n[289] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, “BERTSCORE: \nEVALUATING TEXT GENERATION WITH BERT”, Accessed: May 16, 2022. [Online]. \nAvailable: https://github.com/Tiiiger/bert \n[290] P. Colombo, G. Staerman, C. Clavel, and P. Piantanida, “Automatic Text Evaluation \nthrough the Lens of Wasserstein Barycenters,” EMNLP 2021 - 2021 Conference on \nEmpirical Methods in Natural Language Processing, Proceedings, pp. 10450–10466, Aug. \n2021, doi: 10.48550/arxiv.2108.12463. \n[291] W. Zhao, Y. Gao, and S. Eger, “Evaluating Machine Translation without Human References \nUsing Cross-lingual Encoders”. \n[292] Y. Song, J. Zhao, and L. Specia, “SentSim: Crosslingual Semantic Evaluati on of Machine \nTranslation,” NAACL-HLT 2021 - 2021 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technologies, Proceedings \nof the Conference, pp. 3143–3156, 2021, doi: 10.18653/V1/2021.NAACL-MAIN.252. \n[293] C. Leiter, P. Lertvittayakumjorn, M. Fomicheva, W. Zhao, Y. Gao, and S. Eger, “Towards \nExplainable Evaluation Metrics for Natural Language Generation”, Accessed: Feb. 18, \n2023. [Online]. Available: https://github.com/Gringham/explainable -metrics-machine-\ntranslation. \n[294] B. Marie, A. Fujita, and R. Rubino, “Scientific Credibility of Machine Translation \nResearch: A Meta-Evaluation of 769 Papers,” ACL-IJCNLP 2021 - 59th Annual Meeting of \nthe Association for Computational Linguistics and the 11th International Joint Conference \non Natural Language Processing, Proceedings of the Conference , pp. 7297 –7306, Jun. \n2021, doi: 10.48550/arxiv.2106.15195. \n[295] J. Grünwald, C. Leiter, and S. Eger, “Can we do that simpler? Simple, Efficient, High -\nQuality Evaluation Metrics for NLG”. \n[296] R. Sharma, J. F. Allen, O. Bakhshandeh, and N. Mostafazadeh, “Tackling the Story Ending \nBiases in The Story Cloze Test,” ACL 2018 - 56th Annual Meeting of the Association for \nComputational Linguistics, Proceedings of the Conference (Long Papers), vol. 2, pp. 752–\n757, 2018, doi: 10.18653/V1/P18-2119. \n 192 \n[297] “Google Sheets: Online Spreadsheet Editor | Google Workspace.” \nhttps://www.google.com/sheets/about/ (accessed Feb. 18, 2023). \n[298] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bow man, “GLUE: A MULTI-\nTASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE \nUNDERSTAND-ING”. \n[299] Y. Yang, W. -T. Yih, and C. Meek, “WIKIQA: A Challenge Dataset for Open -Domain \nQuestion Answering,” pp. 17 –21, 2015, Accessed: Feb. 18, 2023. [Online]. Avai lable: \nhttp://aka.ms/WikiQA. \n[300] “Google AI updates: Bard and new AI features in Search.” \nhttps://blog.google/technology/ai/bard-google-ai-search-updates/ (accessed Feb. 18, 2023). \n[301] “Localization vs. Internationalization,” W3C, Accessed: May 06, 2023. [Online]. Available: \nhttp://www.w3.org/International/questions/qa-i18n \n  \n \n \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8049126863479614
    },
    {
      "name": "Sentiment analysis",
      "score": 0.7959141731262207
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6955945491790771
    },
    {
      "name": "Natural language processing",
      "score": 0.6659443378448486
    },
    {
      "name": "Natural language generation",
      "score": 0.5160843729972839
    },
    {
      "name": "Focus (optics)",
      "score": 0.5064219236373901
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5003633499145508
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4797062277793884
    },
    {
      "name": "Language model",
      "score": 0.41727519035339355
    },
    {
      "name": "Natural language",
      "score": 0.408115029335022
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I67031392",
      "name": "Carleton University",
      "country": "CA"
    }
  ],
  "cited_by": 2
}