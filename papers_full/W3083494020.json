{
  "title": "E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce",
  "url": "https://openalex.org/W3083494020",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2361727444",
      "name": "Zhang, Denghui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202042112",
      "name": "Yuan Zixuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2637544180",
      "name": "Liu, Yanchi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749414701",
      "name": "Zhuang, Fuzhen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1690476359",
      "name": "Chen Hai-feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099001205",
      "name": "Xiong Hui",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2465978385",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W3030236966",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W1991418309",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2593560537",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2987154291",
    "https://openalex.org/W2946345909",
    "https://openalex.org/W2293491541",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W3022116759",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2983102021",
    "https://openalex.org/W2938830017"
  ],
  "abstract": "Pre-trained language models such as BERT have achieved great success in a broad range of natural language processing tasks. However, BERT cannot well support E-commerce related tasks due to the lack of two levels of domain knowledge, i.e., phrase-level and product-level. On one hand, many E-commerce tasks require an accurate understanding of domain phrases, whereas such fine-grained phrase-level knowledge is not explicitly modeled by BERT's training objective. On the other hand, product-level knowledge like product associations can enhance the language modeling of E-commerce, but they are not factual knowledge thus using them indiscriminately may introduce noise. To tackle the problem, we propose a unified pre-training framework, namely, E-BERT. Specifically, to preserve phrase-level knowledge, we introduce Adaptive Hybrid Masking, which allows the model to adaptively switch from learning preliminary word knowledge to learning complex phrases, based on the fitting progress of two modes. To utilize product-level knowledge, we introduce Neighbor Product Reconstruction, which trains E-BERT to predict a product's associated neighbors with a denoising cross attention layer. Our investigation reveals promising results in four downstream tasks, i.e., review-based question answering, aspect extraction, aspect sentiment classification, and product classification.",
  "full_text": "E-BERT: Adapting BERT to E-commerce with Adaptive Hybrid Masking\nand Neighbor Product Reconstruction\nDenghui Zhang1, Zixuan Yuan1, Yanchi Liu2, Fuzhen Zhuang3,\nHaifeng Chen2, Hui Xiong1\n1Rutgers University, USA, {denghui.zhang, zy101, hxiong}@rutgers.edu\n2NEC Laboratories America, Inc., USA, yanchi@nec-labs.com\n3Institute of Computing Technology, Chinese Academy of Sciences, China\nAbstract\nPre-trained language models such as BERT have achieved\ngreat success in a broad range of natural language process-\ning tasks. However, BERT cannot well support E-commerce\nrelated tasks due to the lack of two levels of domain knowl-\nedge, i.e., phrase-level and product-level. On one hand, many\nE-commerce tasks require accurate understanding of domain\nphrases, whereas such ﬁne-grained phrase-level knowledge\nis not explicitly modeled by BERT’s training objective. On\nthe other hand, product-level knowledge like product associ-\nations can enhance the language modeling of E-commerce,\nbut they are not factual knowledge thus using them indis-\ncriminately may introduce noise. To tackle the problem, we\npropose a uniﬁed pre-training framework, namely, E-BERT.\nSpeciﬁcally, to preserve phrase-level knowledge, we intro-\nduce Adaptive Hybrid Masking, which allows the model to\nadaptively switch from learning preliminary word knowledge\nto learning complex phrases, based on the ﬁtting progress of\ntwo modes. To utilize product-level knowledge, we introduce\nNeighbor Product Reconstruction, which trains E-BERT to\npredict a product’s associated neighbors with a denoising\ncross attention layer. Our investigation reveals promising re-\nsults in four downstream tasks, i.e., review-based question\nanswering, aspect extraction, aspect sentiment classiﬁcation,\nand product classiﬁcation.\nIntroduction\nUnsupervised pre-trained language models like BERT (De-\nvlin et al. 2019) have greatly advanced the natural language\nprocessing research in recent years. However, these mod-\nels are pre-trained on open-domain corpus and then ﬁne-\ntuned for generic tasks, thus cannot well support domain-\nspeciﬁc tasks. To this end, several domain-adaptive BERTs\nhave been proposed recently, such as BioBERT (Lee et al.\n2020), SciBERT (Beltagy, Lo, and Cohan 2019). They em-\nploy large-scale domain corpora to obtain language knowl-\nedge of speciﬁc domains, e.g., BioBERT uses 1M PubMed\narticles for pre-training. Despite this, they adopt the same\narchitecture and pre-training approach as BERT does, ne-\nglecting the crucial domain knowledge which is beneﬁcial\nfor downstream tasks. Speciﬁcally, we ﬁnd two levels of do-\nmain knowledge may not be effectively captured by BERT,\ni.e., phrase-level and product-level knowledge. Along this\nReview 1: We love the size of the screen, although it is still light-\nweight and very easy to tote around.\nReview 2: That included the extra Sony Sonic Stage software, the \nspeakers and the subwoofer I got (that WAS worth the money), the \nbluetooth mouse for my supposedly bluetooth enabled computer, \nthe extended life battery and the docking port. [… ]\nBERT prediction: 1: screen 2: software, bluetooth mouse, battery\nE-BERT prediction: 1: size of the screen 2: Sony Sonic Stage\nsoftware, bluetooth mouse, battery, docking port\nFigure 1: An example of review aspect extraction, with an-\nswers marked in color. The vanilla BERT tends to make in-\ncomplete/wrong predictions, and miss aspects sometimes.\nBut it gets improved after integrating phrase-level and\nproduct-level domain knowledge.\nline, we incorporate these two-levels of domain knowledge\nto BERT for the E-commerce domain and perform evalua-\ntions on several related tasks.\nFirst, many tasks in E-commerce involve understanding\nvarious domain phrases. However, such ﬁne-grainedphrase-\nlevel knowledge is not explicitly captured by BERT’s train-\ning objective, i.e., Masked Language Model (MLM). Specif-\nically, MLM aims to predict individual masked words from\nincomplete input, thus being a word-oriented rather than\nphrase-oriented task. Although some subsequent work (Sun\net al. 2020) proposes to mask phrases so that BERT can learn\nphrase-level knowledge, there are two major limitations: (i)\nthey mask phrases that are simply obtained by chunking,\nmay not be domain-speciﬁc; (ii) they discard word masking\nafter using phrase masking, yet, we argue word-level knowl-\nedge is preliminary to phrase understanding. Figure 1 gives\na motivating example from review Aspect Extraction. The\ntask aims to extract entity aspects on which opinions have\nbeen expressed. Without phrase modeling, BERT tends to\nmiss aspects or output incomplete aspects. It gets improved\nafter effective phrase knowledge encoding.\nOn the other hand, there is rich semantic knowledge hid-\nden in product associations, and we consider it as product-\nlevel knowledge. Existing models like BERT rely on co-\noccurrence to capture the semantics of words and phrases,\nwhich is inefﬁcient and expensive. For instance, to teach\narXiv:2009.02835v3  [cs.CL]  17 Dec 2021\nE-commerce\nPhrase Pool\nE-commerce\nCorpus Product Association Graph\nPre-training Resources\nAHM\n NPR\nPre-training Tasks Downstream Tasks\nReview QA\nReview AE\nReview ASC\nProduct\nClassification\nFine-tuning\nInferencePre-training\nTransformer\nTransformer\nFigure 2: Overview of E-BERT.\nProduct Description\nSamsung Galaxy S10 OS: Andriod; 5G network; Dynamic AMOLED; …\niPhone XS iOS; 4G signal; T-Mobile service; OLED screen; …\nFigure 3: Two associated products and their descriptions.\nthe model that Android and iOS are semantically simi-\nlar and correlated, a large number of co-occurrences of them\nare required in the corpus. Leveraging product associations\nto bridge the contents ofSamsung galaxy and iPhone,\nwe can easily enhance such semantic learning. However, it\nis challenging in practice because different fragments in two\nconnected contents have different association conﬁdence,\nwithout differentiating, it may introduce extra noise.\nTo enable pre-trained language models with the above\ntwo levels of domain knowledge, we propose a uniﬁed pre-\ntraining framework, E-BERT. As shown in Figure 2, we\ncontinue to use Transformer as the underlying architecture,\nand leverage a massive domain corpus, a high-quality E-\ncommerce phrase pool, and a product association graph as\nour pre-training resources. To train E-BERT on them, we in-\ntroduce two novel improvements as the pre-training tasks,\ni.e., Adaptive Hybrid Masking (AHM) and Neighbor Prod-\nuct Reconstruction (NPR):\n(1) AHM extends MLM by introducing a new masking\nstrategy. Speciﬁcally, it sets two different modes, i.e., word\nmasking mode and phrase masking mode. The former ran-\ndomly masks separate words while the latter masks domain\nphrases. Moreover, it can adaptively switch between the two\nmodes based on feedback losses, enabling the model to cap-\nture word-level and phrase-level knowledge progressively.\n(2) In NPR, we train E-BERT to reconstruct the neighbor\nproducts in the association graph given a central product, us-\ning its own content representation and a de-noising cross at-\ntention layer. The cross attention layer enables the model to\npay more/less attention to different positions of the content\naccording to their relevance. As a result, NPR transforms\nproduct-level knowledge into semantic knowledge without\nintroducing too much noise.\nTo validate the effectiveness of the proposed approach, we\nﬁne-tune E-BERT on four downstream tasks, i.e., Review-\nbased Question Answering (RQA), Aspect Extraction (AE),\nAspect Sentiment Classiﬁcation (ASC), and Product Clas-\nsiﬁcation. The experimental results show that E-BERT sig-\nniﬁcantly outperforms BERT and several following work on\nthese domain-speciﬁc tasks, by taking full advantage of the\nphrase-level and product-level knowledge.\nTable 1: High-quality phrases of 6 product categories.\nCategory Top-rated phrases\nAutomotive jumper cables, cometic gasket, angel\neyes, drink holder, static cling\nClothing, Shoes\nand Jewelry\nhigh waisted jean, nike classic, remov-\nable tie, elegant victorian, vintage grey\nElectronics ipads tablets, SDHC memory card,\nmemory bandwidth, auto switching\nOfﬁce Products decorative paper, heavy duty rubber,\nmailing labels, hybrid notebinder\nSports and\nOutdoors\nbasketball backboard, table tennis pad-\ndle, string oscillation, ﬁshing tackles\nToys and Games hulk hogan, augmented reality, teacup\npiggies, beam sabers, naruto uzumaki\nMethodology\nIn this section, we ﬁrst present the pre-training resources\nused in E-BERT. Then, we provide an in-depth introduction\nabout our improvements in pre-training, i.e., Adaptive Hy-\nbrid Masking and Neighbor Product Reconstruction.\nPre-training Resources\nE-commerce Corpus We extract millions of product ti-\ntles, descriptions, and reviews from the Amazon Dataset 1\n(Ni, Li, and McAuley 2019) to build this corpus. We divide\nthe corpus into two sub-corpus, i.e., product corpus and re-\nview corpus. In the ﬁrst corpus, each line corresponds to a\nproduct title and its description, while in the second, it corre-\nsponds to a user comment on a speciﬁc product. The corpus\nserves as the foundation for E-BERT to learn preliminary\nlanguage knowledge.\nE-commerce Phrase Pool To incorporate domain phrase\nknowledge into E-BERT, we extract plenty of domain\nphrases from the above corpus and build an E-commerce\nphrase pool in advance. Considering phrase quality, we\nadopt AutoPhrase2, a high efﬁcient phrase mining method\n(Shang et al. 2018), can generate a quality score for each\nphrase based on corpus-level statistics like popularity, con-\ncordance, informativeness, and completeness. In total, we\nextract more than one million initial phrases. Then, we ﬁl-\nter out phrases where score <0.5 to get quality phrases\nand store them in the pool. We also attach the score of each\nphrase in the pool, which is used for phrase sampling in\nAHM. Table 1 shows the top-ranked phrases from six prod-\nuct categories. Compared with existing work using chunk-\ning to select noun phrases for masking, our phrase pool is\nmore diversiﬁed, ﬁne-grained, and domain-speciﬁc. Discus-\nsion about the effects of choosing different phrase sets is\nshown in the experiment section.\nProduct Association Graph To enable E-BERT with\nproduct-level knowledge, we build Product Association\nGraph in advance. Speciﬁcally, products in our corpus are\nrepresented as nodes, and associations among products are\nrepresented as undirect edges. To obtain product associa-\ntions, we extract product pairs such as substitutable and\n1https://nijianmo.github.io/amazon/index.html\n2https://github.com/shangjingbo1226/AutoPhrase\nInput sequence: Baby Monitor with Remote Pan-Tilt-Zoom Camera and 3.2'' LCD Screen\nAdaptive Switch\n! < # ?\nWord masking mode\n# = %(ℒ(, ℒ* )\n,-\n ,.\nToken\nEmbedding\nPosition\nEmbedding\n[M]\nTransformer\nTransformer\nPhrase masking mode\n1\n 2\n[M]\n3\nT3\n4\nT4\n5\n[M]\n6\n[M]\n7\n[M]\n8\nT5\n9\nT6\n…\nT78\n…\nT77\n…\nT79\nE-commerce\nPhrase Pool\nBaby Monitor\n Pan Tilt Zoom\nBaby Monitor with Remote Pan-Tilt-Zoom Camera and 3.2'' LCD Screen\nPhrase\nExtractor\nTemporary\nPhrase Pool\nSampling\nRemote\n1\nT7\n2\n[M]\n3\nT3\n4\n[M]\n5\nT:\n6\nT;\n7\nT<\n8\n[M]\n9\nT6\n…\n[M]\n…\nT77\n…\n[M]\nTransformer\nTransformer\nMonitor\n Camera\n 3.2''\n Screen\nBaby Monitor with Remote Pan-Tilt-Zoom Camera and 3.2'' LCD Screen\nWord Sampling\nBaby Monitor,\nPan-Tilt-Zoom,\nLCD Screen ...\nFigure 4: The illustration of Adaptive Hybrid Masking. Based on the feedback losses, it adaptively switches between two\nmasking modes, enabling the model to learn word-level and phrase-level knowledge in a progressive manner.\ncomplementary from the Amazon dataset, using a heuristic\nmethod based on consumer shopping statistics (McAuley,\nPandey, and Leskovec 2015).\nAdaptive Hybrid Masking\nTo encode phrase-level knowledge effectively, we introduce\na new masking strategy, namely, Adaptive Hybrid Mask-\ning (AHM), which is easy to implement by extending MLM.\nIn AHM, we set two masking modes, i.e., word masking\nand phrase masking respectively. The former masks word\nunits while the latter masks domain phrase units, resulting\nin inconsistent difﬁculty levels of reconstructing the masked\ntokens. To this end, we adaptively switch from predicting\nmasked words to predicting masked phrases, enabling E-\nBERT to capture word-level and phrase-level knowledge in\na progressive manner.\nWord Masking Mode In this mode, we select random\nwords from input sequence iteratively until obtain 15% to-\nkens for masking. This scheme learns preliminary word-\nlevel semantics, which is essential for phrase understanding.\nPhrase Masking Mode In phrase masking mode, we ran-\ndomly mask consecutive tokens that can form quality do-\nmain phrases. Speciﬁcally, given an input sequence of to-\nkens X = {xi}n\ni=1, we ﬁrst detect all the E-commerce\nphrases {pi}m\ni=1 in X, leveraging the E-commerce phrase\npool PE along with a rule-based phrase matcher3. Then, we\ncreate a temporary phrase poolPX consisting of the detected\nphrases {pi}m\ni=1. Some input sequences may contain too few\ndomain phrases, therefore, to ensure we have enough and di-\nverse phrases, we extend PX with noun phrases. That is, we\nextract all the noun phrases{ni}l\ni=1 in Xusing constituency\n3https://spacy.io/usage/examples#phrase-matcher\nparsing4, and abandon the ones that have an intersection with\nthe domain phrases {pi}m\ni=1. Then, we add the rest “clean”\nnoun phrases in to PX. Based on the extended PX, we sam-\nple phrases iteratively until obtain approximately 15% to-\nkens for masking. The probability of selecting each phrase\nis set as the softmax of its quality score:\np(pi) = exp\n(\ns[pi]\n)\n∑\npj∈PX\nexp\n(\ns[pj]\n), (1)\nwhere s[pi] denotes the score of phrase pi. For the supple-\nmental phrases, it is assigned with the lowest score in PE.\nPhrases with higher scores are usually more E-commerce re-\nlated, thus the quality-based sampling impels our model to\npay more attention to unique domain phrases.\nAdaptive Switching When learning a new language, peo-\nple usually start with individual words (the vocabulary), and\ngradually turn to study more complex phrases and expres-\nsions. Inspired by this, we start pre-training with word mask-\ning mode, and set a time-varying parameter αto adaptively\nswitch to phrase mode.\nIn detail, at each iteration (tth), we calculate a “ﬁtting in-\ndex” for both modes to track their ﬁtting progress, i.e., ηt\nw\nand ηt\np. The larger ηt\nw (ηt\np) is, the less sufﬁcient the model\nis trained on the word (phrase) mode. Next, we calculate γt,\nrepresenting the relative importance of the word mode, and\nrescale it to [0,1] via a non-linear unit (tanh) to get the prob-\nability of choosing word mode at the next iteration (t+ 1th),\ni.e., αt+1:\nηt\nw = ∆t,t−1\nw\n∆t,1\nw\n=\n[\nLt−1\nw −Lt\nw\n]\n+\nL1w −Ltw\n, (2)\n4https://spacy.io/universe/project/self-attentive-parser\nηt\np = ∆t,t−1\np\n∆t,1\np\n=\n[\nLt−1\np −Lt\np\n]\n+\nL1p −Ltp\n, (3)\nγt = ηt+1\nw\nηt+1p\n, (4)\nαt+1 = tanh(γt). (5)\nwhere ∆t,t−1\nw denotes the loss reduction of word mode be-\ntween current and last iteration. ∆t,1\nw denotes the total loss\nreduction. Lt\nw denotes the loss and will only be updated if\nword mode is selected at the t-th iteration. ∆t,t−1\np , ∆t,1\np , de-\nnotes the counterparts in phrase mode. [x]+ is equivalent to\nmax(x,0). When ηt+1\nw ≫ηt+1\np , αt+1 ≈1, and the word\nmode becomes dominating, vice versa. In other words, α\ncontrols the model to switch to the weaker mode adaptively.\nFigure 4 presents an overall illustration of AHM. At each\niteration, we ﬁrst generate a random number r∈[0,1], then\nwe select word masking mode if r < αt and select phrase\nmode otherwise. To be noted, for the ﬁrst t≤T1 iterations,\nwe set αt = α0 (α0 >0.5) to make word mode more likely\nto be selected. After this initial stage, ηp gets larger than ηw\nand α decreases, consequently, the probability of selecting\nphrase mode gets larger. Until the end, it will switch between\nthe two modes adaptively based on their losses, balancing\nword-level and phrase-level learning.\nReconstructing Masked Tokens For both modes after se-\nlecting tokens to be masked, following BERT to mitigate the\nmismatch between pre-training and ﬁne-tuning, we replace\nthe selected tokens with (1) the [MASK] token 80% of the\ntime, (2) a random token 10% of the time, (3) the original\ntoken 10% of the time. Next, we predict each masked token\nby feeding their output embedding to a shared softmax layer\n(take word masking mode as example), i.e.,\np\n(\nXt\nm\n⏐⏐Xt\n\\WXt\n)\n=\nexp\n(\nW⊤\nm\n[\nE-BERT\n(\nXt\n\\WXt\n)]\nm\n)\n∑\nk∈V\nexp\n(\nW⊤\nk\n[\nE-BERT\n(\nXt\n\\WXt\n)]\nk\n), (6)\nwhere W represents the parameters of softmax layer. Vde-\nnotes the vocabulary. Xt denotes the input sequence. WXt\ndenotes the set of masked tokens in word masking mode, \\\ndenotes set minus, Xt\n\\WXt denotes the modiﬁed input where\nWXt are masked. Xt\nm denotes the masked token to be pre-\ndicted, m∈WXt . The overall loss function of AHM is the\ncombined cross entropy of the two masking modes, i.e.,\nLAHM = − 1\n|D|\n∑\nXt∈D\nαt log\n∏\nm∈WXt\np\n(\nXt\nm\n⏐⏐Xt\n\\WXt\n)\n+\n(1 −αt) log\n∏\nm∈PXt\np\n(\nXt\nm\n⏐⏐Xt\n\\PXt\n)\n,\n(7)\nwhere Drepresents the training corpus. p\n(\nXt\nm\n⏐⏐Xt\n\\PXt\n)\nde-\nnotes the prediction in phrase masking mode, calculated the\nsame way as word masking mode. PXt denotes the set of\nmasked tokens in phrase mode.\nCross-Attention\nX\n–Loss\n–Loss\nReconstructed Content Embeddings\nReconstructed Content Embeddings\nX\nDell LED Monitor 32 inch\nPlugable Display Docking Station\nproduct a\nproduct b\nFigure 5: Illustration of Neighbor Product Reconstruction.\nNeighbor Product Reconstruction\nIn this task, we train E-BERT to reconstruct the neighbor\n(associated) product’s content using the central product’s\ncontent, and thereby, transform the hidden semantic knowl-\nedge into the weights of the model.\nAs illustrated in Figure 5, we ﬁrst sample associated prod-\nucts from the product association graph and put their content\nembeddings into a pair (shown in the the middle of the ﬁg-\nure). The cross attention layer is then used to learn a cor-\nrelation matrix, indicating word-level correlations between\ntwo products. Next, we multiply the correlation matrix with\nthe central product’s content embeddings to generate a set of\nreconstructed content embeddings for the neighbor product.\nThey are optimized to resemble the real ones via the con-\ntent reconstructing loss. Considering the central product can\nalso be viewed as neighbor to the reconstructed product, we\nperform the reconstructing task in both directions.\nContent Embeddings Given the product pair (a,b), we\nfeed their contents (title and description) into E-BERT re-\nspectively to get their content embeddings, i.e.,\n{wi}n\ni=1 = E-BERT\n(\n{ai}n\ni=1\n)\n,\n{oi}n\ni=1 = E-BERT\n(\n{bi}n\ni=1\n)\n.\n(8)\nCross Attention Layer We adopt the cross attention layer\nto generate two correlation matrices, i.e.,\nαij = exp (wioj )∑\nj′ exp (wioj′ ), βji = exp (wioj )∑\ni′ exp (wi′ oj ), (9)\nwhere wi indicates the ith word in the product a’s content\nand oj represents the jth word in b. The cross attention\nweight αij and βji both indicates the correlation between\nwi and oj, but using different normalizers.\nReconstructed Embeddings Using the attention weights,\nwe compute a weighted average of the real content embed-\ndings, i.e.,\nw′\ni =\n∑\nj\nαijoj , o′\nj =\n∑\ni\nβjiwi. (10)\nwhere w′\ni and o′\nj are the reconstructed embeddings, they\nare subsequently optimized to resemble the original con-\ntent embeddings wi and oj. The negative effect of noise is\nminimized by the cross attention as it automatically assigns\nsmaller weights to irrelevant contents.\nReconstructing Loss We deﬁne the content reconstruct-\ning loss of a product pair as the Euclidean distance between\ntheir real content embeddings to the reconstructed embed-\ndings, i.e.,\n⟨a,b⟩=\n∑\ni\n||wi −w′\ni||2\n2 +\n∑\nj\n||oj −o′\nj||2\n2 (11)\nWe use a triplet loss as the ﬁnal loss to pull relevant product-\nproduct pairs close while pushing irrelevant ones apart:\nLNPR = max(0,1 + ⟨a,b⟩−⟨a,b−⟩), (12)\nwhere b−is a randomly sampled negative product that is not\nrelated to a.\nTo be noted, we only train NPR on the product\ncorpus where the input is formated as content pairs\n⟨content(a),content(b)⟩. We do not train NPR on the review\ncorpus since it consists of user feedbacks, can not reﬂect\nproduct semantics accurately.\nExperiments\nIn this section, we conduct extensive experiments to answer\nthe following research questions:\n• What is the performance gain of the E-commerce corpus\nfor each downstream task, with respect to the state-of-the-\nart performance?\n• What is the overall performance gain of our pre-training\nframework incorporating two levels domain knowledge?\n• What is the performance gain of each component (i.e.,\nAHM and NPR) in E-BERT?\nBaselines\nIn this paper, we compare E-BERT to the following baseline\nmethods:\n• BERT-Raw: The vanilla BERT which is pre-trained on\nlarge-scale open-domain corpus5. We use this baseline to\nanswer the ﬁrst question.\n• BERT:The vanilla BERT which is further post-trained on\nour E-commerce corpus. We compare with this baseline to\nanswer the second question.\n• BERT-NP: The vanilla BERT which is post-trained on\nour E-commerce corpus, but uses a different masking\nstrategy, i.e., masks noun phrases instead of words. We\nuse this to validate the effect of our domain phrase pool.\n• SpanBERT: An variant of BERT which masks spans of\ntokens instead of seperate tokens. We compare with it to\nfurther validate the effect of the phrase masking scheme.\nFor ablation studies, we further compare with the following\ninternal baselines:\n• E-BERT-DP: The reduced E-BERT which only uses the\nphrase masking mode, without word-level masking.\n5We use the pre-trained model released by Huggingface.\n• E-BERT-AHM: The reduced E-BERT which adopts\nAHM to adaptively change the masking mode, but not uti-\nlizes NPR to encode product-level knowledge.\n• E-BERT: The full E-BERT, utilizing both AHM and\nNPR to encode two levels of domain knowledge.\nPre-training Dataset\nThe dataset contains four pre-training resources:\n• Product Corpus It contains a total of5,436,547 product\ntitles and descriptions with a size of 1.4 GB.\n• Review Corpus It contains a total of 9,636,112 million\nproduct reviews with a size of 2.3 GB.\n• E-commerce Phrase Pool It consists of 536,332 high\nquality E-commerce phrases.\n• Product Association Graph It consists of 2,125,352\nproducts and 3,484,325 product associations.\nFigure 6: The overlap ra-\ntio (%) between the E-\ncommerce phrases and\nordinary noun phrases.\nPhrase Overlap Figure 6\npresents the overlap be-\ntween the E-commerce do-\nmain phrases and the ordi-\nnary noun phrases extracted\nfrom our corpus, divided by\n5 product categories. Each\nentry indicates the propor-\ntion of domain phrases that\nalso occurred in the noun\nphrase set. We can observe\nthat the overlap ratio is low\neven for the same cate-\ngory, indicating our phrase\npool contains more unique\nphrase-level knowledge.\nPre-training Details\nAll the baselines and E-BERT is initialized with the weights\nof the pre-trained BERT (the bert-base-uncased ver-\nsion by Huggingface, with 12 layers, 768 hidden dimen-\nsions, 12 heads, 110M parameters). We post-train all the\nbaselines except BERT-Raw on the E-commerce corpus for\n10 epochs, with batch size 32 and learning rate 1e-5. For E-\nBERT, we adopt Continual Multi-task Learning (Sun et al.\n2020) to combine AHM and NPR. To be speciﬁc, we ﬁrst\ntrain AHM alone on the entire corpus for 5 epochs with the\nsame batch size and learning rate. Then, we train AHM and\nNPR jointly on the product corpus for another5 epochs. The\nonly hyperparameter in AHM, T1, is set to be 1 epoch.\nDownstream Tasks\nReview-based Question Answering Given a question\nq = {qi}m\ni=1 and a related review r = {ri}n\ni=1, it aims to\nﬁnd the span s= {ri}e\ni=s from rthat can answer q. To ﬁne-\ntune this task, we adopt the standard BERT approach (Devlin\net al. 2019) for span-based QA, which maximizes the sum of\nthe log-likelihoods of the correct start and end positions.\nTable 2: Results of baselines and our model on E-commerce downstream tasks (%).\nModels\\Tasks Review QA Product Classiﬁcation Review AE Review ASC\nP. R. F 1 EM Acc. MiF 1 MaF 1 P. R. F 1 Acc. MaF 1\nPre-trained on Wikipedia + BookCorpus by Huggingface.\nBERT-Raw 58.91 62.58 60.69 40.22 66.54 81.90 78.82 83.15 84.66 83.90 86.01 62.87\nFurther post-trained on E-commerce corpus by us.\nBERT 60.28 62.25 61.25 41.23 69.12 82.38 80.66 84.33 84.09 84.81 86.40 64.96\nBERT-NP 61.39 64.57 62.94 43.35 70.28 81.72 81.34 85.23 85.71 86.11 85.79 63.21\nSpanBERT 62.52 64.77 63.63 43.94 71.59 81.51 81.50 85.67 86.22 86.23 86.76 65.13\nE-BERT-DP 63.76 67.02 65.77 44.63 75.07 85.84 86.28 86.80 89.47 88.11 87.84 69.02\nE-BERT-AHM 65.18 68.30 66.18 45.56 76.61 86.35 87.32 87.42 90.55 88.96 89.17 70.35\nE-BERT 66.71 70.13 68.77 45.40 78.74 90.37 90.94 87.35 89.61 88.42 88.43 69.32\nReview Aspect Extraction Given a review r = {ri}n\ni=1,\nthe task aims to ﬁnd aspects that reviewers have expressed\nopinions on. It is typically formalized as a sequence labeling\ntask (Xu et al. 2019), in which each token is classiﬁed as one\nof {B,I,O }, and tokens betweenBand Iare considered the\ncorrect aspect. We apply a dense layer and softmax layer on\ntop of each output embedding to ﬁne-tune.\nReview Aspect Sentiment Classiﬁcation Given an aspect\na = {ai}l\ni=1 and the review sentence r = {ri}n\ni=1 where a\nextracted from, this task aims to classify the sentiment polar-\nity (positive, negative, or neutral) expressed on aspecta. For\nﬁne-tuning, both aand rare input into E-BERT, and we use\nthe [CLS] token along with a dense layer and softmax layer\nto predict the polarity. Training loss is the cross entropy on\nthe polarities.\nProduct Classiﬁcation Given a product title x =\n{xi}n\ni=1, it aims to classifyxusing a predeﬁned category hi-\nerarchy H. Each product may belong to multiple categories,\nthus making it a multi-lable classiﬁcation problem. We use\nthe [CLS] token along with a dense layer and softmax layer\nto perform prediction.\nEvaluation Datasets For review QA, we evaluate on a\nnewly released Amazon QA dataset (Miller et al. 2020),\nwhich consists of 8,967 samples. We use the laptop dataset\nof SemEval 2014 Task 4 (Pontiki et al. 2016) for both review\nAE and review ASC tasks, which contains3,845 review sen-\ntences, 3,012 annotated aspects and the sentiment polarities\non them. For product classiﬁcation, we create an evalua-\ntion dataset by extracting Amazon product metadata, con-\nsisting of 10,039 product titles and 133 categories. For all\nthe datasets, we divide them into training/validation/testing\nset with the ratio of 7:1:2.\nFine-tuning Details In each task, we adopt the standard\narchitecture for each BERT variant. We choose the learning\nrate and epochs from {5e-6, 1e-5, 2e-5, 5e-5}and {2,3,4,5}\nrespectively. For each task and BERT variant, we pick the\nbest learning rate and number of epochs on the development\nset and report the corresponding test results. We found the\nsetting that works best across most tasks and models is 2 or\n4 epochs and a learning rate of 2e-5.\n(a) Loss\n (b) Accuracy\nFigure 7: The convergence of different masking schemes.\nEvaluation Metrics For review QA, we adopt the stan-\ndard evaluation script from SQuAD 1.1 to report Precision,\nRecall, F1 scores, and Exact Match (EM). To evaluate re-\nview AE, we report Precision (P), Recall (R), and F1 score.\nFor review ASC, we report Macro-F1 and Accuracy. Lastly,\nwe adopt Accuracy (Acc), Micro-F1 (MiF1), and Macro-F1\n(MaF1) to evaluate product classiﬁcation.\nResult Analysis\nTable 2 presents the results of all the baselines and E-\nBERT on the four tasks. First, we can see that BERT out-\nperforms BERT-Raw on all the tasks, verifying that the\nE-commerce corpus can largely improve the performance\non related tasks. Compared with BERT, BERT-NP and\nSpanBERT achieves further improvements in review QA\nand review AE, indicating that phrase-level knowledge is\nquite helpful in these extractive tasks. Comparing E-BERT-\nDP with BERT-NP and SpanBERT, we prove that our E-\ncommerce phrase pool can provide more quality phrase\nknowledge for the downstream tasks. E-BERT outperforms\nall the baselines by a large margin in terms of all metrics,\nverifying the overall superiority of our pre-training frame-\nwork in E-commerce tasks. To examine the effectiveness of\nproduct-level domain knowledge and each component of E-\nBERT, we present more discussions in ablation studies.\nAblation Studies\nAs shown in the bottom of Table 2, E-BERT-AHM consis-\ntently outperforms E-BERT-DP in four tasks, proving that\nour adaptive hybrid masking strategy can utilize phrase-\nBest\nUtensils\nPremium\nStainless\nSteel\nChopper\nAnd\nDicer\nChop\nVeggies\nHerbs\nAnd\nNuts\nWith\nEase\nMetabao\n8-in-1\nMandoline\nSlicer\nVegetable\nChopper\nEgg\nSlicer\nFruit\nDicer\nwith\nContainer\nWhite\n0.06\n0.07\n0.08\n0.09\n0.10\nDell\nSE2419\n23.8\ninch\nIPS\nFull\nHD\nMonitor\nBlack\nDell\nUSB\n3.0\nUltra\nHD/4K\nTriple\nDisplay\nDocking\nStation\nBlack\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nFigure 8: Visualizing the cross attention.\nlevel knowledge in a more sufﬁcient way. Besides, as shown\nin Figure 7, our masking method has a better conver-\ngence rate in terms of loss and phrase reconstruction ac-\ncuracy. Compared with E-BERT-DP and E-BERT-AHM, E-\nBERT further encodes product-level knowledge via NPR,\nand it achieves signiﬁcant improvements in product classi-\nﬁcation. We assume this is because there are strong category\ncorrelations between associated products, by utilizing prod-\nuct association knowledge, E-BERT enhances feature shar-\ning among different instances. While the performance of re-\nview QA is also boosted, review AE and review ASC even\ndeteriorates slightly after using NPR, indicating product-\nlevel knowledge has no signiﬁcant effect on the task of re-\nview aspect analysis.\nCross Attention Probing\nFigure 8 presents the cross attention visualization of two\npairs of product titles. In the ﬁrst example, two similar\nproducts Mandoline Slicer and Steel Chopper\nare connected using the learned attention weights. The\ndarker color indicates stronger correlations. It can be\nseen the cross attention automatically learn to align cor-\nrelated words in two product contents, e.g., (Slicer,\nChopper), (Slicer, Dicer) and (Vegetables,\nVeggies). In the second example, two complemen-\ntary products Docking station and Dell Monitor\nare connected. Similarly, correlated contents such as\n(Display, Monitor) and (Docking station,\nMonitor) are aligned automatically.\nRelated Work\nPre-trained Language Models Recent years have wit-\nnessed the great success of Pre-trained Language Models\n(PLMs) (Devlin et al. 2019; Peters et al. 2018; Radford et al.\n2018) on a broad range of NLP tasks. Compared with tradi-\ntional word embedding models (Gupta and Manning 2015),\nPLMs learn to represent words based on the entire input\ncontext to deal with word polysemy, thus captures seman-\ntics more accurately. Following PLMs, many endeavors have\nbeen made for further optimization. SpanBERT (Joshi et al.\n2020) proposes to reconstruct randomly masked spans in-\nstead of single words. However, the span consists of random\ncontinuous words and may not form phrases, thus fails to\ncapture phrase-level knowledge accurately. ERNIE-1.0 (Sun\net al. 2019) integrates phrase-level masking and entity-level\nmasking into BERT, which is closely related to our masking\nstrategy. Unlike them using simple chunking tools to get or-\ndinary phrases, we build a high-quality E-commerce phrase\npool and only mask domain phrases. Besides, we combine\nword-masking and phrase-masking coherently with Adap-\ntive Hybrid Masking, accelerating the convergence without\naffecting performance. Due to space limit, we refer readers\nto the references for more work along this line (Liu et al.\n2019; Lan et al. 2019; Yang et al. 2019; Brown et al. 2020;\nSun et al. 2020).\nDomain-adaptive PLMs To adapt PLMs to speciﬁc do-\nmains, several domain-adaptive BERTs have been proposed\nrecently. BioBERT (Lee et al. 2020) and SciBERT (Beltagy,\nLo, and Cohan 2019) train BERT on large-scale biomedical\nand scientiﬁc corpus respectively to get a pre-trained lan-\nguge model for biomedical and scientiﬁc NLP tasks. BERT-\nPT (Xu et al. 2019) propose to post-train BERT on a re-\nview corpus and obtains better performance on the task of\nreview reading comprehension. Gururangan et al. propose\nto continue pre-training on domain corpus as well as task\ncorpus and obtains more performance gains. More work\nalong this line can be referred to (Rietzler et al. 2020; Ma\net al. 2019; Jin et al. 2019; Huang, Altosaar, and Ranganath\n2019). These work only leverages domain corpus for pre-\ntraining, without considering special domain knowledge like\nthe product association graph.\nKnowledge Enhanced PLMs Recently, to enable PLMs\nwith world knowledge, several attempts (Wang et al. 2019;\nPeters et al. 2019; Zhang et al. 2019; Liu et al. 2020; Wang\net al. 2020) have been made to inject knowledge into BERT\nleveraging Knowledge Graphs (KGs). Most of these work\nadopts the “BERT+entity linking” paradigm, whereas, it\nis not suitable for E-commerce corpus due to the lack of\nquality entity linkers as well as KGs in this domain. In-\nstead, we consider utilizing the product association knowl-\nedge which is coarse-grained and may introduce noise. In\nE-BERT, through Neighbor Product Reconstruction and the\nde-noising cross attention layer, the meaning of each word\nin a product content is expanded to those of associated prod-\nucts, greatly enriches the semantic learning.\nConclusions\nIn this paper, we proposed a domain-enhanced BERT for\nE-commerce, namely, E-BERT. We leveraged two levels of\ndomain knowledge, i.e., phrase-level and product-level, to\nboost performance on related tasks. Despite the challenge of\nmodeling phrase knowledge and reducing noise in product\nknowledge, we provided two technical improvements, i.e.,\nAHM and NPR. Our investigation revealed promising re-\nsults on four downstream tasks. Incorporating phrase knowl-\nedge via AHM can improve the performance signiﬁcantly on\nall the investigated tasks. Utilizing the product-level knowl-\nedge via NPR further boosts the performance on product\nclassiﬁcation and review QA.\nReferences\nBeltagy, I.; Lo, K.; and Cohan, A. 2019. SciBERT: A Pre-\ntrained Language Model for Scientiﬁc Text. In Proceed-\nings of the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-\nIJCNLP), 3606–3611.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186.\nGupta, S.; and Manning, C. D. 2015. Distributed represen-\ntations of words to guide bootstrapped entity classiﬁers. In\nProceedings of the 2015 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 1215–1220.\nGururangan, S.; Marasovi ´c, A.; Swayamdipta, S.; Lo, K.;\nBeltagy, I.; Downey, D.; and Smith, N. A. 2020. Don’t Stop\nPretraining: Adapt Language Models to Domains and Tasks.\narXiv preprint arXiv:2004.10964.\nHuang, K.; Altosaar, J.; and Ranganath, R. 2019. Clinical-\nbert: Modeling clinical notes and predicting hospital read-\nmission. arXiv preprint arXiv:1904.05342.\nJin, Q.; Dhingra, B.; Liu, Z.; Cohen, W.; and Lu, X. 2019.\nPubMedQA: A Dataset for Biomedical Research Question\nAnswering. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), 2567–2577.\nJoshi, M.; Chen, D.; Liu, Y .; Weld, D. S.; Zettlemoyer, L.;\nand Levy, O. 2020. Spanbert: Improving pre-training by rep-\nresenting and predicting spans. Transactions of the Associ-\nation for Computational Linguistics8: 64–77.\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma,\nP.; and Soricut, R. 2019. Albert: A lite bert for self-\nsupervised learning of language representations. arXiv\npreprint arXiv:1909.11942.\nLee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and\nKang, J. 2020. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinfor-\nmatics 36(4): 1234–1240.\nLiu, W.; Zhou, P.; Zhao, Z.; Wang, Z.; Ju, Q.; Deng, H.; and\nWang, P. 2020. K-BERT: Enabling Language Representa-\ntion with Knowledge Graph. In AAAI, 2901–2908.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMa, X.; Xu, P.; Wang, Z.; Nallapati, R.; and Xiang, B. 2019.\nDomain Adaptation with BERT-based Domain Classiﬁca-\ntion and Data Selection. In Proceedings of the 2nd Work-\nshop on Deep Learning Approaches for Low-Resource NLP\n(DeepLo 2019), 76–83.\nMcAuley, J.; Pandey, R.; and Leskovec, J. 2015. Inferring\nnetworks of substitutable and complementary products. In\nProceedings of the 21th ACM SIGKDD international con-\nference on knowledge discovery and data mining, 785–794.\nMiller, J.; Krauth, K.; Recht, B.; and Schmidt, L. 2020. The\nEffect of Natural Distribution Shift on Question Answering\nModels. arXiv preprint arXiv:2004.14444.\nNi, J.; Li, J.; and McAuley, J. 2019. Justifying recommen-\ndations using distantly-labeled reviews and ﬁne-grained as-\npects. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 188–197.\nPeters, M.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,\nC.; Lee, K.; and Zettlemoyer, L. 2018. Deep Contextualized\nWord Representations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), 2227–2237.\nPeters, M. E.; Neumann, M.; Logan, R.; Schwartz, R.; Joshi,\nV .; Singh, S.; and Smith, N. A. 2019. Knowledge Enhanced\nContextual Word Representations. In Proceedings of the\n2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP),\n43–54.\nPontiki, M.; Galanis, D.; Papageorgiou, H.; Androutsopou-\nlos, I.; Manandhar, S.; Al-Smadi, M.; Al-Ayyoub, M.; Zhao,\nY .; Qin, B.; De Clercq, O.; et al. 2016. Semeval-2016 task\n5: Aspect based sentiment analysis. In 10th International\nWorkshop on Semantic Evaluation (SemEval 2016).\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training.\nRietzler, A.; Stabinger, S.; Opitz, P.; and Engl, S. 2020.\nAdapt or Get Left Behind: Domain Adaptation through\nBERT Language Model Finetuning for Aspect-Target Senti-\nment Classiﬁcation. In Proceedings of The 12th Language\nResources and Evaluation Conference, 4933–4941.\nShang, J.; Liu, J.; Jiang, M.; Ren, X.; V oss, C. R.; and Han,\nJ. 2018. Automated phrase mining from massive text cor-\npora. IEEE Transactions on Knowledge and Data Engineer-\ning 30(10): 1825–1837.\nSun, Y .; Wang, S.; Li, Y .; Feng, S.; Chen, X.; Zhang, H.;\nTian, X.; Zhu, D.; Tian, H.; and Wu, H. 2019. Ernie: En-\nhanced representation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nSun, Y .; Wang, S.; Li, Y .-K.; Feng, S.; Tian, H.; Wu, H.;\nand Wang, H. 2020. ERNIE 2.0: A Continual Pre-Training\nFramework for Language Understanding. In AAAI, 8968–\n8975.\nWang, R.; Tang, D.; Duan, N.; Wei, Z.; Huang, X.; Cao, C.;\nJiang, D.; Zhou, M.; et al. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. arXiv preprint\narXiv:2002.01808 .\nWang, X.; Gao, T.; Zhu, Z.; Liu, Z.; Li, J.; and Tang, J.\n2019. KEPLER: A uniﬁed model for knowledge embed-\nding and pre-trained language representation. arXiv preprint\narXiv:1911.06136 .\nXu, H.; Liu, B.; Shu, L.; and Philip, S. Y . 2019. BERT Post-\nTraining for Review Reading Comprehension and Aspect-\nbased Sentiment Analysis. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 2324–2335.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR. R.; and Le, Q. V . 2019. Xlnet: Generalized autoregres-\nsive pretraining for language understanding. In Advances in\nneural information processing systems, 5753–5763.\nZhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu,\nQ. 2019. ERNIE: Enhanced language representation with\ninformative entities. arXiv preprint arXiv:1905.07129.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8081640005111694
    },
    {
      "name": "Phrase",
      "score": 0.7282048463821411
    },
    {
      "name": "Natural language processing",
      "score": 0.5713418126106262
    },
    {
      "name": "Domain knowledge",
      "score": 0.5659693479537964
    },
    {
      "name": "Product (mathematics)",
      "score": 0.5391595363616943
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5065754652023315
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4727754592895508
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": []
}