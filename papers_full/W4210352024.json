{
  "title": "Transformer-Driven Semantic Relation Inference for Multilabel Classification of High-Resolution Remote Sensing Images",
  "url": "https://openalex.org/W4210352024",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2112197870",
      "name": "Xiao-Wei Tan",
      "affiliations": [
        "State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A1991537926",
      "name": "Zhifeng Xiao",
      "affiliations": [
        "State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2001859568",
      "name": "Jian-Jun Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122362957",
      "name": "Qiao Wan",
      "affiliations": [
        "State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A1983718584",
      "name": "Kai Wang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2119587491",
      "name": "Deren Li",
      "affiliations": [
        "Wuhan University",
        "State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6637373629",
    "https://openalex.org/W4246193833",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2897086142",
    "https://openalex.org/W3113728373",
    "https://openalex.org/W3011990881",
    "https://openalex.org/W3127571973",
    "https://openalex.org/W2585836593",
    "https://openalex.org/W2592226848",
    "https://openalex.org/W3162100949",
    "https://openalex.org/W3155649272",
    "https://openalex.org/W2998002262",
    "https://openalex.org/W2592628593",
    "https://openalex.org/W1567302070",
    "https://openalex.org/W2801287733",
    "https://openalex.org/W2958781163",
    "https://openalex.org/W3018169007",
    "https://openalex.org/W3174905907",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W2156935079",
    "https://openalex.org/W2012995268",
    "https://openalex.org/W1524416683",
    "https://openalex.org/W2166912588",
    "https://openalex.org/W1953606363",
    "https://openalex.org/W2052684427",
    "https://openalex.org/W2118712128",
    "https://openalex.org/W1753402186",
    "https://openalex.org/W4232706428",
    "https://openalex.org/W2010181071",
    "https://openalex.org/W7746136",
    "https://openalex.org/W2410641892",
    "https://openalex.org/W2963745697",
    "https://openalex.org/W2963300078",
    "https://openalex.org/W2463598282",
    "https://openalex.org/W2932399282",
    "https://openalex.org/W2982112268",
    "https://openalex.org/W2998420437",
    "https://openalex.org/W2884821995",
    "https://openalex.org/W3027532550",
    "https://openalex.org/W3014247046",
    "https://openalex.org/W6713494006",
    "https://openalex.org/W2140302574",
    "https://openalex.org/W1905051261",
    "https://openalex.org/W6713507025",
    "https://openalex.org/W3011296786",
    "https://openalex.org/W3111813289",
    "https://openalex.org/W2946373483",
    "https://openalex.org/W6682137061",
    "https://openalex.org/W2105482032",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W3201623325",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W2766938848",
    "https://openalex.org/W3091842132",
    "https://openalex.org/W2602837914",
    "https://openalex.org/W3102503200",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3102112912"
  ],
  "abstract": "It is hard to use a single label to describe an image for the complexity of remote sensing scenes. Thus, it is a more general and practical choice to use multilabel image classification for high-resolution remote sensing (HRS) images. How to construct the relation between categories is a vital problem for multilabel classification. Some researchers use the recurrent neural network (RNN) or long short-term memory (LSTM) to exploit label relations over the last years. However, the RNN or LSTM could model such category dependence in a chain propagation manner. The performance of the RNN/LSTM might be questioned when a specific category is improperly inferred. To address this, we propose a novel HRS image multilabel classification network, transformer-driven semantic relation inference network. The network comprises two modules: semantic sensitive module (SSM) and semantic relation-building module (SRBM). The SSM locates the semantic attentional regions in the features extracted by a deep convolutional neural network and generates a discriminative content-aware category representation (CACR). The SRBM uses label relation inference from outputs of the SSM to predict final results. The characteristic of the proposed method is that it can extract semantic attentional regions relevant to the category and generate a discriminative CACR and natural and interpretable reasoning about label relations. Experiments were performed on the public UCM multilabel and MLRSNet datasets. Quantitative and qualitative analyses on state-of-the-art multilabel benchmarks proved that the proposed method could effectively locate semantic regions and build relationships between categories with better robustness.",
  "full_text": "1884 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nTransformer-Driven Semantic Relation Inference for\nMultilabel Classiﬁcation of High-Resolution Remote\nSensing Images\nXiaowei Tan , Zhifeng Xiao , Member , IEEE, Jianjun Zhu, Qiao Wan, Kai Wang, and Deren Li, Member , IEEE\nAbstract—It is hard to use a single label to describe an image\nfor the complexity of remote sensing scenes. Thus, it is a more\ngeneral and practical choice to use multilabel image classiﬁcation\nfor high-resolution remote sensing (HRS) images. How to construct\nthe relation between categories is a vital problem for multilabel\nclassiﬁcation. Some researchers use the recurrent neural network\n(RNN) or long short-term memory (LSTM) to exploit label rela-\ntions over the last years. However, the RNN or LSTM could model\nsuch category dependence in a chain propagation manner. The per-\nformance of the RNN/LSTM might be questioned when a speciﬁc\ncategory is improperly inferred. To address this, we propose a novel\nHRS image multilabel classiﬁcation network, transformer-driven\nsemantic relation inference network. The network comprises two\nmodules: semantic sensitive module (SSM) and semantic relation-\nbuilding module (SRBM). The SSM locates the semantic attentional\nregions in the features extracted by a deep convolutional neural\nnetwork and generates a discriminative content-aware category\nrepresentation (CACR). The SRBM uses label relation inference\nfrom outputs of the SSM to predict ﬁnal results. The characteristic\nof the proposed method is that it can extract semantic attentional\nregions relevant to the category and generate a discriminative\nCACR and natural and interpretable reasoning about label rela-\ntions. Experiments were performed on the public UCM multilabel\nand MLRSNet datasets. Quantitative and qualitative analyses on\nstate-of-the-art multilabel benchmarks proved that the proposed\nmethod could effectively locate semantic regions and build rela-\ntionships between categories with better robustness.\nIndex Terms—Deep convolutional neural network (DCNN),\nlabel dependence, multilabel scene classiﬁcation, remote sensing,\nsemantic relation learning.\nI. INTRODUCTION\nO\nWING to the complexity of remote sensing scenes, using\na single label to describe an image for the complexity\nof remote sensing scenes is hard. Thus, multilabel image clas-\nsiﬁcation for high-resolution remote sensing (HRS) images is\nManuscript received August 22, 2021; revised October 25, 2021 and Decem-\nber 10, 2021; accepted January 12, 2022. Date of publication January 25, 2022;\ndate of current version February 28, 2022. This work was supported in part by\nthe Science and Technology Project of State Grid Corporation of China under\nGrant 52789921001M.(Corresponding author: Zhifeng Xiao.)\nXiaowei Tan, Zhifeng Xiao, Qiao Wan, and Deren Li are with the State Key\nLaboratory of Information Engineering in Surveying, Mapping, and Remote\nSensing, Wuhan University, Wuhan 430079, China (e-mail: cug_txw@163.com;\nxzf@whu.edu.cn; wanqiao@whu.edu.cn; drli@whu.edu.cn).\nJianjun Zhu is with the National Bio Energy Company Ltd., Beijing 100052,\nChina (e-mail: zhujianjun@sgecs.sgcc.com.cn).\nKai Wang is with the School of Remote Sensing and Information Engineering,\nWuhan University, Wuhan 430079, China (e-mail: whu-wk@whu.edu.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2022.3145042\nmore general and practical than single-label image classiﬁcation.\nHRS scenes comprise various categories with correlations and\ndifferences between them. For example, for correlations between\ncategories, as shown in Fig. 1(a) and (b), “road” and “car” often\nappear simultaneously in the remote sensing image, “grass”\nand “water” accompany “golf course,” and “airport” usually\ncontains “aircraft.” For differences between categories, as shown\nin Fig. 1(c) and (d), they both have two categories of road\nand water; however, the relationship between road and water\nis different, and the road shapes in the two pictures differ too,\nso their categories are different. In picture (c), both roads are\nclose to the water body, and one of them is in a spiral and\noverlapping position, so its category is an overpass. In picture\n(d), the road is above the water body, so this type of road is a\nbridge. From Fig. 1, there are not only semantic associations\nbut also spatial location associations between categories. Even\nif they have the same feature type, their spatial relationships\nare different, and they may belong to different categories. How\nto construct the relationship between categories is a complex\nproblem for multilabel image classiﬁcation.\nWith the rapid progress of artiﬁcial intelligence and ma-\nchine learning (ML), many deep convolutional neural networks\n(DCNNs) have been proposed to extract high-level semantic\nfeatures, e.g., VGG-Net [1], GoogLeNet [2], ResNet [3], and\nDenseNet [4]. These networks have been successfully used in\nmany computer vision tasks such as image classiﬁcation [5],\nobject detection [6], semantic segmentation [7], and video track-\ning [8], and achieved satisfactory performances. Nevertheless,\nmost applications are for single-label remote sensing image\nclassiﬁcation [9]–[13]. Although these networks achieve some\nimprovements for remote sensing image classiﬁcation, they only\nconsider the case that each image contains only one label; they\ndo not consider that an image may be associated with multiple\nsemantic labels.\nIn ML and other ﬁelds, the multilabel issue has attracted much\nattention. Many researchers have shown that it is helpful to use\nlabel correlation to classify multilabel images [14]–[19]. How-\never, how to efﬁciently make use of label correlations is still an\nopen issue. Most scholars use recurrent neural network (RNN)\nor long short-term memory (LSTM), to ﬁnd the dependence be-\ntween categories. However, due to its chain propagation fashion,\nRNN/LSTM’s performance heavily depends on its long-term\nmemorization learning effectiveness. In addition, the categories\nrelationship is implicitly modeled in this way, resulting in a\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTAN et al.: TRANSFORMER-DRIVEN SEMANTIC RELATION INFERENCE FOR MULTILABEL CLASSIFICATION 1885\nFig. 1. Examples for semantic relation of categories in remote sensing image scenes. For an input image, we can get a correlation graph between categories. The\nsolid line indicates higher relation of the categories, and the dashed line indicates lower relation of the categories. We can obtain a relation matrix of categories\nfrom their correlation. The darker color in the matrix indicates the stronger correlation of the categories, and the lighter color indicates the weaker correlation of\nthe categories.\nlack of interpretability. Moreover, the RNN only considers the\ncorrelation between adjacent labels, while nonadjacent labels\nare ignored.\nThe transformer solves the problems of the RNN and LSTM.\nTransformer processes a sentence as a whole and does not rely\non the hidden state of the past to capture the dependence on\nprevious words, so there is no risk of losing (or “forgetting”) past\ninformation. In addition, both multihead attention and position\nembedding provide information about the relationship between\ndifferent words. Some scholars have currently tried to apply\nthe transformer in image classiﬁcation tasks [20], [21]. One\nof the main problems in applying the transformer to image\nclassiﬁcation is how to convert the image into the sequence\nfashion in a semantic manner. In [20], the author proposes iGPT\nand directly converts the image into a sequence as input. Notably,\nthe general input for image classiﬁcation is 384×384 ×tex3 or\n224 ×224 ×3, which is too large for iGPT to directly reshape\ninto sequence length, so the image is reshaped to 32×32 ×3,\n48 ×48 ×3, or 64×64 ×3. In [21], images have been divided\ninto 16×16 image patches, and then, reshaped into a sequence.\nThe aforementioned methods are not good enough to process\npictures and cannot model the local and semantic information.\nSome objects are relatively small in remote sensing scenes and\nare ignored when reducing the image resolution, which affects\nthe ﬁnal classiﬁcation result. In addition, it is more suitable to\nunderstand images from the global scene for multilabel remote\nsensing image classiﬁcation. Cutting an image into several small\npatches is unconducive to understanding the image as a whole,\nlet alone constructing the relationship between categories. More-\nover, the transformer does not have a local receptive ﬁeld. How\nto transform the image into a sequence in a semantic manner\nand make the transformer have a local receptive ﬁeld remains\nan open issue.\nTo address the aforementioned problems, we propose a novel\nend-to-end transformer-driven semantic relation inference net-\nwork (SR-Net) for the multilabel classiﬁcation of HRS images.\nIts characteristic is locating the category-speciﬁc semantic re-\ngions without bounding box and segmentation and modeling\nthe semantic category relation autonomously for the task. The\nnetwork comprises two modules: semantic sensitive module\n(SSM) and semantic relation-building module (SRBM). We used\nthe DCNN as the feature extractor to extract high-level semantic\nfeatures. Then, used the SSM to locate the category-speciﬁc\nsemantic region and obtain a discriminative content-aware cat-\negory representation (CACR). Finally, the SRBM uses label\nrelation inference from outputs of the SSM to predict ﬁnal\nresults. The main contributions of our work can be summarized\nas follows.\n1) The signiﬁcant contribution of this article is that we\nintroduce a novel transformer-driven relation reasoning\nfrom CACRs for multilabel HRS image classiﬁcation.\nThe transformer-driven relation reasoning is competent\nto model category relations for a speciﬁc HRS image in\nan adaptive way, further enhancing its representative and\ndiscriminative ability. For all we know, this is the ﬁrst\nmethod of using the transformer to build the relationship\nbetween categories for multilabel classiﬁcation of HRS\nimages.\n2) The features extracted by the DCNN contain rich contexts\nand semantic information. We design the SSM to further\nutilize this information to locate the semantic attentional\nregions in the feature and generate a discriminative CACR.\nIt makes up for the lack of local receptive ﬁeld in the\ntransformer while enhancing features’ representative and\ndiscriminative ability.\n3) We propose a novel end-to-end HRS image multilabel\nclassiﬁcation network, transformer-driven SR-Net. The\nnetwork comprises two modules: sSSM and SRBM). We\ndesign the SSM to locate semantic attentional regions from\nthe features extracted by the DCNN without bounding box\nand segmentation and generate a discriminative CACR.\nWe design the SRBM to use label relation inference from\noutputs of the SSM, that is, CACR, to predict ﬁnal results.\nThe rest of this article is organized as follows. Section II\nintroduces related multilabel classiﬁcation and transformer\nmethods. Section III introduces an overview of the proposed\nmethod. Section IV presents the employed dataset, model\ntraining parameters, evaluation indicators, experimental re-\nsults, and analyses. Finally, Section V concludes this arti-\ncle.\n1886 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nII. RELA TEDWORK\nA. Traditional ML Methods for Multilabel Classiﬁcation\nThe traditional ML methods for solving the multilabel clas-\nsiﬁcation problem include mainly two solutions: problem trans-\nformation and algorithm adaptation.\nThe main idea of the problem transformation method is\nconverting a multilabel dataset into a single-label dataset in\nsome way and proceeding with the single-label classiﬁcation\nmethod. This method can include transforming to binary clas-\nsiﬁcation [22]–[24], transforming to label ranking [25], and\ntransforming to multiclass classiﬁcation [26].\nThe adaptive algorithm is based on the particularity of multi-\nlabel classiﬁcation, improving the existing single-label classiﬁ-\ncation algorithm, mainly including multilabel k-nearest neigh-\nbor (ML-KNN) [27], ranking support vector machine (Rank-\nSVM) [28], multilabel decision tree (ML-DT) [29], and collec-\ntive multilabel classiﬁer (CML) [30].\nB. Region-Based Methods for Multilabel Classiﬁcation\nRegion-based methods aim to ﬁrst roughly locate multiple\nregions, and then, use the DCNN to recognize each region.\nWei et al. [15] propose a hypotheses–convolutional-neural-\nnetwork (CNN)–pooling (HCP) framework that generates many\nproposals through an object detection method [31], [32] and\nconsiders each proposal as a single-label image classiﬁcation\nissue. Yang et al. [33] regarded the task as a multiclass and\nmulti-instance learning problem. Speciﬁcally, they generated an\ninstance package for each image and used label information to\nenhance discriminant features to incorporate local information.\nThese methods are complex and computationally expensive\nbecause they lead to many categories of unknown regions. In\naddition, these methods generally ignore label dependence and\nregional relations, which are crucial for the multilabel image\nclassiﬁcation.\nC. Relation-Based Methods for Multilabel Classiﬁcation\nRelation-based methods are designed to exploit dependen-\ncies or region relationships between objects [34]–[39]. Wang\net al. [34] proposed the CNN–RNN framework, which used\nRNN to predict ﬁnal scores and formulate label relations. Wang\net al. [35] employed LSTM and a spatial transformer to locate\nthe attention area iteratively to ﬁnd the relation. Huaet al.[40]\nused the class attention learning layer to capture discriminative\nclass-speciﬁc features, and used the bidirectional LSTM-based\nsubnetwork to classify class dependence in both directions and\nproduce structured multiple object labels. Sumbulet al. [41]\nused the DCNN to develop the spatial and spectral features of\nlocal areas in images, used LSTM to characterize the importance\nscores of different local areas in each image, and then, deﬁned\na global descriptor for each image based on those scores.\nJi et al. [42] introduced the attention module to separate the\nfeatures extracted from the DCNN by channel, and then, sent the\nseparated features to the LSTM network for a prediction label.\nThese relation-based methods explore the relationship between\ncategories or semantic regions using the RNN or LSTM; they\ncannot fully explore the direct relationship between categories\nor semantic regions. Different from those methods, some re-\nsearchers have tried to solve this problem through graphical\narchitectures. Before the age of neural networks, people used\nto use graphical models [36], [43]–[45]. Guoet al. [43] used\nthe circular directed graph model to construct the dependence\nrelationship between labels. Micusiket al.[44] constructed the\nMarkov random ﬁeld on super superpixels. Liet al.[36] handle\nsuch relations by image-dependent conditional label structures\nwith a graphical Lasso framework. Liet al.[46] use a maximum\nspanning tree algorithm to create a tree structure graph in the\nlabel space. Recently, with the concept of graph convolution and\nthe outstanding performance of the graph convolution network\n(GCN) in multiple visual tasks, people used GCNs to model the\ncorrelation between nodes [37], [38], [47]–[49]. Particularly,\nChen et al. [37] use the GCN to propagate prior label repre-\nsentations (e.g., word embeddings) and generate a classiﬁer that\nreplaces the last linear layer in a standard deep convolutional\nneural network such as ResNet [3]. Chenet al. [38] compute\na probabilistic matrix as the relation edge between each label\nin a graph with the help of label annotations. Khanet al. [50]\nproposed a new multilabel deep GCN by modeling the subse-\nquent supervised learning problem. Liet al.[49] extracted image\nfeatures through the DCNN and inferred spatiotopological re-\nlationships between images and features using the graph neural\nnetwork (GNN).\nD. Transformer-Based Methods\nConcerning transformers, the attention mechanism is\nparamount. The attention mechanism was ﬁrst proposed in the\nvisual ﬁeld. Mnih et al. [51] incorporated the attention mech-\nanism into the RNN to classify images, thereby making the\nattention mechanism popular. Bahdanauet al. [52] applied the\nattention mechanism to the natural language processing (NLP)\nﬁeld, using the Seq2Seq and attention model for machine trans-\nlation, thereby improving the performance. Vaswaniet al.[53]\nproposed the transformer structure, entirely abandoning net-\nwork structures such as RNN and CNN. They only used the\nattention mechanism for machine translation tasks and achieved\ngood results. The transformer is a deep learning model entirely\nbased on the self-attention mechanism. Self-attention was ﬁrst\napplied to machine translation, with a focus on the contextual\nrelationship between words. Self-attention uses the attention\nmechanism to calculate the relationship between each word and\nall other words. In other words, the encoder reads the input data,\nusing the self-attention mechanism of superimposed layers to\nobtain a new representation of each word that considers the\ncontext information. In the past two years, several scholars have\napplied transformer to visual tasks, such as target detection [54],\nimage classiﬁcation [20], [21], and semantic segmentation [55].\nSpeciﬁcally, in the ﬁeld of remote sensing scene classiﬁcation,\nDeng et al. [56] proposed a joint framework of the CNN and\ntransformer, which has two branches (CNN and Transformer)\nand combines the features extracted from the two branches to\nmake predictions.\nTAN et al.: TRANSFORMER-DRIVEN SEMANTIC RELATION INFERENCE FOR MULTILABEL CLASSIFICATION 1887\nFig. 2. Overall framework of our approach.\nIII. METHODOLOGY\nOur method focuses on the semantic attentional regions loca-\ntion and semantic relationship modeling between categories. It\ncomprises two modules: SSM and SRBM, as shown in Fig. 2.\nHigh-level semantic features extraction is vital for visual\nclassiﬁcation tasks. Many recent studies adopt DCNNs for vi-\nsual classiﬁcation tasks owing to their remarkable performance\nin learning such features. Hence, we take a standard DCNN,\nsuch as VGG, ResNet, and DenseNet, as the backbone of our\nnetwork. In Fig. 2, given an image, we use a DCNN as the\nfeature extractor to obtain the feature mapF, which contains\nrich semantic and location information. We designed the SSM\nto locate the semantic attentional regions inF and obtain the\ncategory-speciﬁc activation map (CSAM), thereby obtaining the\nCACR. We use the SRBM to ratiocinate the relation matrix\nbetween categories to generate the ﬁnal robust semantic rela-\ntionship category representation (SRCR), which contains rich\nrelationship information with other categories. Finally, we use\na binary classiﬁer to classify SRCR and obtain the ﬁnal result.\nThis section introduces our proposed network framework. The\nimplementation details of the SSM and SRBM will be described\nin detail in Sections III-A and III-B, respectively. The classiﬁ-\ncation and loss function will be introduced in Section III-C.\nA. Semantic Sensitive Module (SSM)\nSemantic relations are essential for multilabel classiﬁcation,\nbut obtaining the semantic information of DCNN extracted\nFig. 3. Example images of CSAMs. (a) and (c) Original images, and (b) and\n(d) activation maps (AM) for a certain category. (a) Images. (b) AM. (c) Images.\n(d) AM.\nfeatures is complex. Although the features extracted by the\nDCNN can be directly applied to explore label dependencies,\nsome regions of features that are less relevant to the category\n(such as the blue area in Fig. 3) may bring noise and further\nreduce the effectiveness of feature representations. Fig. 3 is a\nvisual example of images of CSAMs. Fig. 3(a) and (c) are the\noriginal images, and Fig. 3(b) and (d) are the activation maps of\n1888 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\na speciﬁc category. In Fig. 3, the weakly activated area indicates\nthat the correlation with the corresponding category is weak, and\nthe highlighted area indicates that the correlation between the\nregion and the category is vital. In order to strengthen the feature\nrepresentations ability related to the category, we designed the\nSSM to locate the regions related to the category of the features\nand enhance the semantic feature representation ability.\nEach convolution in the DCNN plays the role of an object\ndetector, and it can locate objects. However, this ability is lost\nwhen using a fully connected (FC) layer for classiﬁcation. We\ncan avoid using an FC layer and instead use a global average\npooling (GAP) to establish the relationship between the feature\nmap and the categories [57]. We generate a CSAMA based\non class activation mapping (CAM). The CAM technology is\nto extract implicit attention region on the image in a proposal-\nfree fashion. Speciﬁcally, we can perform GAP or global max\npooling (GMP) on the feature mapF, and use the FC classiﬁer to\nclassify these pooled features, and then, perform the process by\ncombining the weight of the FC classiﬁer with the feature map\nF convolution, using these classiﬁers to identify the CSAM.\nOwing to GMP only recognizes one discriminative part, much\ninformation will be lost, and it is disadvantageous for multilabel\nregion extraction. Thus, we use a classiﬁer before GMP, which\nwill make up for GMP’s disadvantages. Moreover, unlike the\nFC classiﬁer, we use a convolution layer and sigmoid activation\nfunction as a classiﬁer.\nWe use A =[ a1,a 2,... ,aC] ∈ RH×W×C to represent the\nCSAM, where C is the number of categories.aC can be cal-\nculated using the following formula.\naC = Sigmoid(Conv(fC)) (1)\nwhere Sigmoid(·) is sigmoid activation function; Conv(·) is a\nconvolution with a 1*1 convolution kernel; andfC is the cth\nfeature vector of the feature mapF.\nThen, multiplyA and F′∈ RH×W×D′\nto obtain the CACR,\nF′is obtained by reshaping the feature mapF after convolution,\nand we use 1 ∗1 convolution kernel for convolution. We use\nS =[ s1,s 2,... ,sC] ∈ RC×D for the CACR. Speciﬁcally, each\nclass representation sC as a weighted sum onF′ so that the\ngenerated sC can selectively highlight features related to its\nspeciﬁc category c. sC can be calculated using the following\nformula.\nsC = aT\nCF′=\nH∑\ni=1\nW∑\nj=1\naC\ni,jf′\ni,j (2)\nwhere aC\ni,j is the weight ofcth activation map at (i,j); andf′\ni,j ∈\nRD′\nis the feature vector of the feature mapF′at (i, j).\nB. Semantic Relation Building Module (SRBM)\nIn the SSM, we obtain the CACR, which was represented\nby S. As mentioned in the previous section, each dimension in\nS contains the feature code of its corresponding category, and\nthe code contains weighted location and semantic information.\nUsing the information to construct the relationship between\ncategories is a problem that the SRBM needs to solve. We\nFig. 4. Semantic relation learning. In the transformer encoder, MSA is fol-\nlowed by an FFN, this FFN contains two FC layers, and the nonlinear activation\nfunction in the middle uses GeLU. Both MSA and FFN contain the same skip\nconnection as ResNet, and both MSA and FFN contain the layer norm layer.\nintroduced the transformer encoder to model the relationship\nbetween the categories further and get the relation matrix.\nOne of the main problems in applying the transformer to\nimage classiﬁcation is how to transform the image into a se-\nquence in a semantic manner and make the transformer have a\nlocal receptive ﬁeld. In this work, we use the SSM’s output,\nCACR, as the transformer encoder’s input. The CACR is a\nC-dimensional sequence (C is the number of categories anal-\nogous to the sentence length) whose each dimension represents\nthe semantically activated feature encoding of its corresponding\ncategory. The length of each dimension isX, andX is the feature\ncode length of each category analogous to the word code length.\nTherefore, we can use the self-attention mechanism to explore\nthe relationship between categories.\nWe use the transformer encoder architecture to learn relations\nin the CACR. The framework is shown in Fig. 4. In the trans-\nformer encoder, multihead self-attention (MSA) is followed by\na feed-forward network (FFN), containing two FC layers, and\nthe nonlinear activation function in the middle uses GeLU. Both\nMSA and FFN contain the same skip connection and number of\nlayer as ResNet, and norm layer, respectively.\nMSA is to deﬁne h attention heads, i.e.,h self-attention is\napplied to the input sequence; the sequence can be split intoh\nsequences Xss of sizeN ×d, whereD = hd. Then, the outputs\nobtained fromh different heads are concatenated. For theith h\nsequence, the attention head will learn three weight matrices,\nnamely WQ\ni , WK\ni , andWV\ni , and through (5), three vectorsQ′\ni,\nK′\ni, andV ′\ni can be obtained. The formulas are as follows:\nMSA(X)= Concat(head1,..., headh)WO (3)\nheadi = Attention(Q′\ni,K ′\ni,V ′\ni ) (4)\nQ′\ni = XW Q\ni ,K ′\ni = XW K\ni ,V ′\ni = XW V\ni (5)\nAttention(Q, K, V)= Softmax\n(QKT\n√dk\n)\nV (6)\nwhere √dk is the scaling factor to avoid the variance effect\ncaused by the dot product.\nAs shown in Figs. 2 and 4, the SRBM takes theS ∈ RC×D as\ninput node features and sequentially feeds them into the trans-\nformer encoder. Speciﬁcally, the transformer encoder’s output\nis deﬁned as H, where H =[ h1,h 2,... ,hC] ∈ RC×D′\n.N e x t ,\nwe introduce a relation matrixR ∈ RC×C, adaptively estimated\nTAN et al.: TRANSFORMER-DRIVEN SEMANTIC RELATION INFERENCE FOR MULTILABEL CLASSIFICATION 1889\nFig. 5. Example images of the UCM dataset are shown, with the corresponding categories.\nFig. 6. Example images of the AID dataset are shown, with the corresponding categories.\nfrom the feature H. Since the category in each picture has a\ndifferent R, the model is highly representative. Formally, the\noutput T ∈ RC×D′′\nof the SRBM can be deﬁned as follows:\nT = f(RH) (7)\nwhere R = S (Conv(H)); f(·) LeakyReLU activation function;\nS(·) sigmoid activation function.\nNotably, the relation matrix R is speciﬁc for each image,\nwhich may capture content-dependent category dependencies.\nOverall, the module can autonomously reason about the category\nrelationship and does not require speciﬁc prior knowledge about\nthe relationship between all categories. All relationships are\nautomatically learned in a data-driven way and proved to be\nrealistic in our experiments.\nC. Classiﬁcation and Loss Function\nAs shown in Fig. 2, the ﬁnal SRCR is represented byT,\nT =[ t1; t2; ... ; tC] is used for the ﬁnal classiﬁcation. Since\neach vector is aligned with its speciﬁc class and contains rich\nrelationship information with other vectors, we put each class\nvector into a binary classiﬁer to predict its class score. We\nsupervise the ﬁnal score O =[ O1; O2; ... ; OC] and use the\nmultilabel classiﬁcation loss function to train the entire network.\nThe loss function comprises two parts: sigmoid function and\nbinary cross-entropy loss (BCEloss).\nIn multilabel image classiﬁcation, we assume that there are\nﬁve categories in total, and the label form is [1, 0, 0, 1, 1], i.e.,\nimages have the 0th, 3rd, and 4th categories. We judge whether\nan image has each category, which is a two-class classiﬁcation\nproblem. We can use BCEloss for the classiﬁcation. BCELoss is\n1890 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 7. Example images of the MLRSNet dataset are shown, with the corresponding multilabel image on the right side of the image.\nFig. 8. Example images and predicted labels by the ResNet50-SR-Net on the UCM multilabel dataset. Multilabel of each image are reported below the related\nimage, and the red font indicates the incorrect classiﬁcation result, while the green font indicates the correct labels, but the model is not tagged.\nto create a standard to measure the binary cross-entropy between\nlabel and output; it is expressed as follows:\nL(O,y )= − 1\nC ∗\nC∑\ni=1\nyi∗log(Oi)+(1 −yi)∗log(1 −Oi) (8)\nwhere C the number of labels; O the label predicted by the\nmodel, the shape of O is(N,C ), N is the batch size;y the real\nlabel.\nBCEloss requires the input value between(0, 1), so we need\nto use the sigmoid function to convertO. The sigmoid function\nis a differentiable and bounded function, often used in binary\nclassiﬁcation problems; it is expressed as follows:\nS(x)= − 1\n1+ e−x . (9)\nTAN et al.: TRANSFORMER-DRIVEN SEMANTIC RELATION INFERENCE FOR MULTILABEL CLASSIFICATION 1891\nFig. 9. Visualization results of CSAMs on UCM multilabel dataset.\nTherefore, the ﬁnal loss function is represented by the follow-\ning formula:\nL(O,y )= − 1\nC ∗\nC∑\ni=1\nyi ∗log(1 +exp(−Oi)−1)\n+(1 −yi) ∗log\n( exp(−Oi)\n1+ exp(−Oi)\n)\n. (10)\nIV . EXPERIMENTAL RESULTS ANDDISCUSSION\nExtensive experiments and analyses are presented in this sec-\ntion. There are six subsections. The ﬁrst subsection demonstrates\nexperimental setups, including the datasets and training details.\nThe second subsection describes the evaluation metrics. The\nthird section introduces the DCNN feature extraction algorithm.\nThe fourth to sixth subsections present the experimental results\nand qualitative and quantitative analyses of the tested methods\non the UCM multilabel, AID multilabel, and MLRSNet datasets.\nA. Datasets and Training Details\nTo verify the effectiveness of the proposed method, we se-\nlected three challenging datasets, UCM multi-label [58], AID\nmulti-label [17], and MLRSNet [59].\n1) UCM Multilabel Dataset: The UCM multilabel dataset\nreproduces all the aerial images collected in the UCM dataset by\nassigning them to newly deﬁned object tags. The UCM dataset\nis extracted from aerial imagery provided by the National Map\nof the U.S. Geological Survey. It contains 2100 images and\nTABLE I\nNUMBER OF IMAGES PRESENT IN THEUCM DATA S E T F O REACH CLASS LABEL\nThere are 17 predeﬁned class labels in total.\nis divided into 21 categories. These categories correspond to\ndifferent land cover and land use types. Each category has 100\nimages with a size of256 ×256 ×3 and a spatial resolution\nof 0.3 m. In the UCM multilabel dataset, there are a total of\n17 object-level labels, including airplane, bare soil, buildings,\ncars, chaparral, court, dock, ﬁeld, grass, mobile home, pavement,\nsand, sea, ship, tanks, trees, and water. Each image is labeled\nwith one or more (up to seven) tags. Fig. 5 shows some UCM\nmultilabel dataset examples, and Table I shows the details of the\ndatasets. In order to compare with other methods, we follow the\nprinciple of dataset division in [17] and [40]. We select80% of\nthe image samples in each scene category to train our model,\nand the other20% of the images are used to test our model.\n2) AID Multilabel Dataset: AID multilabel dataset selects\n3000 aerial images from 30 scenes in the AID dataset and assigns\n1892 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nTABLE II\nNUMBER OF IMAGES AREPRESENT IN THEAID DATA S E T F O REACH CLASS\nLABEL\nThere are 17 predeﬁned class labels in total.\nmultiple object labels. The AID dataset contains 10 000 high-\nresolution aerial images collected from Google Earth around\nthe world, including scenes from China, the United States, the\nUnited Kingdom, France, Italy, Japan, and Germany, with spatial\nresolutions ranging from 0.5 to 8 m and sizes of600 ×600 ×3.\nIn the AID multilabel dataset, there are a total of 17 object-level\nlabels, consistent with the UCM multilabel dataset. Fig. 6 shows\nsome examples of AID multilabel datasets, and Table II shows\ndetails of the datasets. In order to compare with other methods,\nwe follow the principle of dataset division in [18] and [40]. We\nselect 80% of the image samples in each scene category to train\nour model, and the other20% of the images are used to test our\nmodel.\n3) MLRSNet Dataset: MLRSNet [59] comprises 109 161\nlabeled RGB images globally, labeled into 46 broad categories.\nThe number of sample images varies in the range of 1500–\n3000 for each category. In addition, this dataset contains 60\npredeﬁned class categories, and the number of categories as-\nsociated with each image ranges from 1 to 13. Table III lists\nthe number of images present in the dataset associated with\neach predeﬁned label, and Fig. 7 shows some examples of\nimages with corresponding multilabel. Besides, MLRSNet has\nvarious resolutions, approximately 10–0.1 m. Each multilabel\nimage has a size of 256 ×256 pixels to cover a scene with\nvarious resolutions. In the DCNN training, training data are\noften related to experimental results. The more training data,\nthe better the testing effect of the model. In order to verify that\nour model has better generalization ability and robustness, we\nrandomly select 10% of the dataset for training and90% for\ntesting.\n4) Training Details: For the entire framework, we, respec-\ntively, use VGG-Net [1], ResNet [3], and DenseNet [4] as our\nbackbone. During training, we adopt the data augmentation\nsuggested in [37] to avoid overﬁtting: for the UCM multilabel\nand MLRSNet datasets, the input image is randomly cropped\nand resized to224 ×224 with random horizontal ﬂips for data\naugmentation; for the AID multilabel dataset, the input image\nis randomly cropped and resized to 512 ×512 with random\nhorizontal ﬂips for data augmentation. To make the proposed\nmodel converge quickly, we follow [38] to choose the model\ntrained on VOC2012 as the pretrain model. We chose SGD as\nthe optimizer, with a 0.9 momentum and10−4 weight decay.\nTABLE III\nNUMBER OF IMAGES PRESENT IN THEMLRSNET DATA S E T F O REACH CLASS\nLABEL\nThere are 60 predeﬁned class labels in total.\nThe initial learning rate was set to 0.01 for the SSM and SRBM\nand 0.001 for the backbone DCNN. We trained our model for\n100 epochs on UCM multilabel and AID multilabel datasets and\n50 epochs on the MLRSNet dataset, and the learning rate was\nreduced by a factor of 0.1 at 25, 50, and 75 epochs, respectively.\nThe batch size was 32 for the UCM multilabel dataset and 16\nfor the AID multilabel and MLRSNet datasets. All experiments\nare implemented based on PyTorch.\nB. Evaluation Metrics\nTo fairly compare with existing methods, we follow previous\nworks [18], [19], [40] to adopt the average of example-based\nprecision and recall (Pe/Re), label-based precision and recall\n(Pl/Rl), and the mean F1 (mF1) as evaluation metrics. Suppose\nthat there is a test set D =( xi,y i) | 1 ≤ i ≤ N, where the\nbinary vector yi =[ yi1,y i2,... ,yC]T ∈ 0, 1C is the ground-\ntruth label of theith test sample. Letˆyi =[ ˆyi1, ˆyi2,... , ˆyC\nT ∈\n0, 1C] denote the predicted label vector for the sample\nxi, Y =[ y1,y 2,... ,yN ]T =[ I1,I 2,... ,IC] ∈ RN×C denote\nthe ground truth label matrix, and ˆY =[ˆy1, ˆy2,... , ˆyN ]T =\n[ ˆI1, ˆI2,... , ˆIC] ∈ RN×C denote the predicted label matrix. The\nTAN et al.: TRANSFORMER-DRIVEN SEMANTIC RELATION INFERENCE FOR MULTILABEL CLASSIFICATION 1893\nFig. 10. Visualization of an example and what its relation matrixR looks like on the UCM multilabel dataset. (a) Input Image (b) Relation Matrix\nformulas are deﬁned as follows:\nPe = 1\nN\nN∑\ni\n| ˆyi\n⋂yi |\n| ˆyi | ,P l = 1\nC\nC∑\ni\n| ˆIj\n⋂Ij |\nˆ| Ij |\n(11)\nRe = 1\nN\nN∑\ni\n| ˆyi\n⋂yi |\n| yi | ,R l = 1\nC\nC∑\ni\n| ˆIj\n⋂Ij |\n| Ij | (12)\nmF1= 2PeRe\nPe + Re\n. (13)\nWhen measuring Pe/Re/Pl/Rl/mF1, the label is positive if\nits conﬁdent score is greater than 0.5. To fairly compare with\nstate-of-the-arts, generally speaking, the mF1 is more vital than\nother indicators.\nC. Compare Methods\nFive popular CNN architectures, VGGNet16 [1], VG-\nGNet19 [1], ResNet50 [3], ResNet101 [3], and DenseNet201 [4]\nare chosen as our backbone to verify the effectiveness of the\nproposed method.\n1) VGGNet16 and VGGNet19 [1]: The Visual Geometry\nGroup of Oxford proposed VGG at ILSVRC 2014, which mainly\nproved that increasing the depth of a network can affect the ﬁnal\nperformance of the network to a certain extent. Simonyan and\nZisserman used a continuous3 ×3 convolution kernel instead\nof a larger convolution kernel to increase the depth of networks,\nwhich showed a signiﬁcant improvement in accuracies. We\nuse two models that show corresponding performance in scene\nclassiﬁcation, namely VGGNet16 and VGGNet19.\n2) ResNet50 and ResNet101 [3]:The proposal of the deep\nresidual network (ResNet) is a milestone event in the history\nof CNN imaging. This model won ﬁrst place in the ILSVRC\n2015 classiﬁcation task. It proposes residual learning, which\nsolves the problem of decreasing accuracy as the network struc-\nture deepens. ResNet50 and ResNet101 are 50- and 101-layer\nResNet, respectively.\n3) DenseNet201 [4]: Dense convolutional network (Dense\nNet) uses a feed-forward method to connect each layer to other\nlayers. In the traditional DCNN, if the network hasL layers,\nthen there will beL connections, but in DenseNet, there will\nbe L(L+1)\n2 connections. Simply, the input of each layer comes\nfrom the output of all previous layers. DenseNets are widely\nused because they have several compelling advantages, such\nas alleviating the problem of vanishing gradients, enhancing\nfeature propagation, promoting feature reuse, and signiﬁcantly\nreducing the number of parameters. DensesNet201 is the 201-\nlayer DenseNet.\n4) DCNN + SSM: This method uses the DCNN (such as\nVGG16, ResNet101, and DenseNet201) as a feature extractor\nand feeds features into the SSM. Use a binary classiﬁer upon the\noutput of the SSM directly.\n5) DCNN + SRBM: This method uses the DCNN (such as\nVGG16, ResNet101, and DenseNet201) as a feature extractor\nand feeds features into the SRBM. Use a binary classiﬁer upon\nthe output of the SRBM directly.\n6) DCNN-RBFNN [60]: This method uses the DCNN\nfor feature extraction and the RBFNN [60] for classiﬁca-\ntion.\n7) CA-DCNN-BiLSTM [40]: This method uses the DCNN\nfor feature extraction, uses an attention learning layer to cap-\nture class-speciﬁc features, and uses the bidirectional LSTM\nnetwork to model the class dependence for ﬁnal classiﬁca-\ntion.\n8) AL-RN-DCNN [17]: This method uses the DCNN for\nfeature extraction. Then, localize discriminative regions in these\nfeatures and uses a1 ×1 convolution to explore spatial infor-\nmation. Finally, use an MLP layer to produce the label relation\nfor ﬁnal classiﬁcation.\n9) MLRSSC-CNN-GNN [49]: This method combines a CNN\nand a GNN to generate high-level appearance features using\nCNN’s perception of visual elements in learning scenes. Based\non the trained CNN, a scene graph of each scene was further\nconstructed, and the superpixel region of the scene represented\nthe nodes in the graph.\n1894 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nTABLE IV\nCOMPARISON OF THEPROPOSED METHOD AND THESTATE-OF-THE-ART\nMETHOD ON THEUCM MULTILABEL DATA S E T\nD. Experiments on UCM Multilabel Dataset\n1) Quantitative Analysis: Generally, mF1 is more vital than\nother indicators. Table IV shows the experimental results of the\nUCM multilabel dataset. Speciﬁcally, compared with VGG16,\nVGG16-SR-Net improves the mF1 score by3.25%. Compared\nwith CA-VGG16-BiLSTM, our method improves the mF1 score\nby 3.65%. Compared with VGG16-RBFNN, our method im-\nproves the mF1 score by4.63%.\nIn contrast, VGG16-SR-Net is superior to VGG16, VGG16-\nRBFNN, and CA-VGG16-BiLSTM in mF1 scores and mean\nexample- and label-based precisions and recalls. For another\nbackbone, ResNet50, our network got the best mF1 score. As\nshown in Table IV, ResNet50-SR-Net increases the mF1 scores\nof ResNet50, ResNet50-RBFNN, CA-ResNet50-BILSTM, AL-\nRN-ResNet50, and MLRSSC-CNN-RNN by11.99%, 8.09%,\n7.2%, 1.91%, and 2.28%, respectively. Our method also sur-\npasses other competitors for other indicators, which proves the\neffectiveness and robustness of our method. Compared with all\nother methods, the mF1 score of ResNet50-SR-Net is 0.8867.\nIn addition, it achieved the best mean example-based recall rate\nof 0.8940, mean label-based accuracy of 0.9352, and recall rate\nof 0.9151. Fig. 8 shows an example of classiﬁcation results on\nResNet50-SR-Net. Although our mean example-based precision\nis slightly lower than AL-RN-ResNet50, we have achieved a\nsigniﬁcant improvement in label-based precision and recall. The\nclassiﬁcation results are shown below the picture. Here, the black\nfont is the correctly classiﬁed category, the red font is the wrong\ncategory, and the green is the correct label, but the model does\nnot recognize its correct label.\nAll in all, the comparison between DCNN-SR-Net and other\nmodels proves the effectiveness of our method. In addition,\nwhen using VGG16 as our backbone network, our method is\nlower than AL-RN-VGG16Net in various accuracy, indicat-\ning that our method is more suitable for using the DCNN\nwith a deeper network structure as a feature extractor. At the\nsame time, VGG16 is a shallower DCNN network, and the\nexperimental results are easily affected by the division of the\ndataset, while the UCM multilabel dataset is relatively small, so\nwe added a more extensive dataset for supplementary experi-\nments.\n2) Qualitative Analysis: In order to ﬁgure out what is learned\ninside our method, we further visualize each module to verify\nTABLE V\nCOMPARISON OF THEPROPOSED METHOD AND THESTATE-OF-THE-ART\nMETHOD ON THEAID MULTILABEL DATA S E T\nthe effectiveness of the proposed method in a qualitative way. In\nFig. 9, we visualized the original image and its corresponding\nCSAM to illustrate the SSM’s ability to capture the semantic\nregions of each category appearing in the image. We used\nthe standard ResNet50 as a backbone. Each row shows the\noriginal image and activation map of a speciﬁc category. The\ncategory and score are displayed in the upper left corner of\nthe corresponding category activation map. The proposed model\ncould locate discriminative semantic regions related to positive\ncategories are highlighted in these feature maps, whereas less\ninformative regions are weakly activated. We can observe that\nin these activation maps, the discriminative regions related to\nthe positive categories are highlighted, while the regions with\nless information are weakly activated. As an exception, there is\nan activation map in Fig. 9(a) that is incorrectly identiﬁed as a\npavement, which may lead to mispredictions.\nFurthermore, in Fig. 10, we visualized an original image with\nits corresponding relation matrixR to illustrate what relations\nthe SRBM had learned. For the input image in Fig. 10(a), its\nlabels are “buildings,” “bridge,” “court,” “grass,” “pavement,”\nand “trees.” Fig. 10(b) is the visualization of theR of the input\nimage. Rcourt,trees ranked top (about top 10%)i nt h er o wo f\n“court.” It means that “trees” were more relevant for “court”\nin the image. From the relation matrix visualization, the SRBM\ncan build such semantic relations for a speciﬁc input image.\nE. Experiments on AID Multilabel Dataset\n1) Quantitative Analysis: Generally, mF1 is more vital than\nother indicators. Table V shows the experimental results of the\nAID multilabel dataset. Speciﬁcally, compared with VGG16,\nVGG16-SR-Net improves the mF1 score by1.63%. Compared\nwith CA-VGG16-BiLSTM, our method improves the mF1 score\nby 0.47%. Compared with VGG16-RBFNN, our method im-\nproves the mF1 score by2.57%.\nIn contrast, VGG16-SR-Net is superior to VGG16, VGG16-\nRBFNN, and CA-VGG16-BiLSTM in mF1 scores and mean\nexample- and label-based precisions and recalls. For another\nbackbone, ResNet50, our network got the best mF1 score. As\nshown in Table V, ResNet50-SR-Net increases the mF1 scores\nof ResNet50, ResNet50-RBFNN, CA-ResNet50-BILSTM AL-\nRN-ResNet50, and MLRSSC-CNN-RNN by 3.74%, 6.20%,\n2.34%, 1.25% and 3.58%, respectively. Our method also\nTAN et al.: TRANSFORMER-DRIVEN SEMANTIC RELATION INFERENCE FOR MULTILABEL CLASSIFICATION 1895\nFig. 11. Example images and predicted labels by the ResNet50-SR-Net on the AID multilabel dataset. Multilabel of each image are reported below the related\nimage, and the red font indicates the incorrect classiﬁcation result, while the green font indicates the correct labels, but the model is not tagged.\nsurpasses other competitors for other indicators, which proves\nthe effectiveness and robustness of our method. Compared with\nall other methods, the mF1 score of ResNet50-SR-Net is 0.8997.\nIn addition, it achieved the best mean example-based recall rate\nof 0.9052, mean label-based accuracy of 0.8724, and recall rate\nof 0.8225. Fig. 11 shows an example of classiﬁcation results on\nResNet50-SR-Net. Although our mean example-based precision\nis slightly lower than AL-RN-ResNet50, we have achieved a\nsigniﬁcant improvement in label-based precision and recall. The\nclassiﬁcation results are shown below the picture. Here, the black\nfont is the correctly classiﬁed category, the red font is the wrong\ncategory, and the green is the correct label, but the model does\nnot recognize its correct label.\nAll in all, the comparison between DCNN-SR-Net and other\nmodels proves the effectiveness of our method. In addition, when\nusing VGG16 as our backbone network, our method is lower\nthan AL-RN-VGG16Net in various accuracy, indicating that our\nmethod is more suitable for using the DCNN with a deeper\nnetwork structure as a feature extractor.\n2) Qualitative Analysis: In order to ﬁgure out what is learned\ninside our method, we further visualize each module to verify\nthe effectiveness of the proposed method in a qualitative way. In\nFig. 12, we visualized the original image and its corresponding\nCSAM to illustrate the SSM’s ability to capture the semantic\nregions of each category appearing in the image. We used\nthe standard ResNet50 as a backbone. Each row shows the\noriginal image and activation map of a speciﬁc category. The\ncategory and score are displayed in the upper left corner of\nthe corresponding category activation map. The proposed model\ncould locate discriminative semantic regions related to positive\ncategories are highlighted in these feature maps, whereas less\ninformative regions are weakly activated. We can observe that in\nthese activation maps, the discriminative regions related to the\npositive categories are highlighted, while the regions with less\ninformation are weakly activated.\nFurthermore, in Fig. 13, we visualized an original image with\nits corresponding relation matrixR to illustrate what relations\nTABLE VI\nCOMPONENT EFFECTIVENESS EVA L UAT I O N O F T H EPROPOSED METHOD ON\nMLRSNET DATA S E T\nthe SRBM had learned. For the input image in Fig. 13(a),\nits labels are “buildings,” “cars,” “dock,” “grass,” “pavement,”\n“sea,” “ship,” and “trees.” Fig. 10(b) is the visualization of the\nR of the input image.Rship,dock ranked top (about top10%)i n\nthe row of “ship.” It means that “dock” were more relevant for\n“ship” in the image. From the relation matrix visualization, the\nSRBM can build such semantic relations for a speciﬁc input\nimage.\nF . Experiments on MLRSNet Dataset\n1) Quantitative Analysis: In order to explore whether our\nmethod can achieve a better performance with less training data\non a large dataset and to verify the inﬂuence of different modules\nin our method, we conducted the following experiments with the\nMLRSNet dataset. Table VI shows in detail the evaluation of\ndifferent modules in SR-Net methods. Generally, mF1 is more\n1896 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 12. Visualization results of CSAMs on AID multilabel dataset.\nFig. 13. Visualization of an example and what its relation matrixR looks like on the AID multilabel dataset.\nvital than other indicators. In Table VI, the performance of each\nDCNN model improves after our method is added. Speciﬁcally,\ncompared with VGG16, VGG19, ResNet50, ResNet101, and\nDenseNet201, our method improves the mF1 score by5.79%,\n6.23%, 1.53%, 1.5%, and1.19%, respectively.\nThe improvement of the SSM showed that the decomposed\nrepresentation had a more vital discriminative ability. We also\nfound that the SRBM could improve the feature recognition\nability of the results compared with the baseline. However,\nif the features learned from the DCNN were directly input\nto the SRBM, the result would not be as good as combining\nwith the SSM and using the CACR as the input. That means\nincreasing the semantic information in the feature can further\nimprove the ability of the SRBM to learn relationships, and\nenhancing the semantic information in the features can further\nenhance the ability of the SRBM to learn relationships. Although\nfor VGG19, ResNet50, and ResNet101 models, our method\ndoes not perform the best on mean example- and label-based\nprecisions, but our method sacriﬁces a little bit of the accuracy\nof mean example- and label-based precisions, but the mean\nTAN et al.: TRANSFORMER-DRIVEN SEMANTIC RELATION INFERENCE FOR MULTILABEL CLASSIFICATION 1897\nFig. 14. Example images and predicted labels by the ResNet101-SR-Net on MLRSNet dataset. Multilabel of each image are reported below the related image,\nand the red font indicates the incorrect classiﬁcation result, while the green font indicates the correct labels, but the model is not tagged.\nexample- and label-based recalls have been signiﬁcantly im-\nproved. Overall, combining SSM and SRBM would further im-\nprove performance, as expected, because it focuses on different\naspects.\nNotably, VGG16 and VGG19 had the most signiﬁcant\nimprovement. The proposed method signiﬁcantly improved on\nthe VGG model because the VGG model is not as deep as the\nnetwork structure of ResNet and DenseNet, so the high-level\nsemantic information in the features extracted by VGG is less\nthan those extracted by ResNet and DenseNet. The proposed\nSSM could capture the semantic regions in the DCNN extracted\nfeatures and strengthen the representation ability of DCNN\nfeatures. In addition, the proposed SRBM could construct the\nrelationship between category representations and further en-\nhance the representation ability of features. Experiments have\nproven the effectiveness of the proposed method. The results\nof the proposed model were signiﬁcantly superior to those of\nmodels with shallower network structures, such as VGG. There\nwere also speciﬁc improvements on more complex models with\ndeeper network structures, such as ResNet and DendeNet.\nFig. 14 shows an example of classiﬁcation results using\nResNet101 as the backbone. The classiﬁcation results are shown\nbelow the picture. Here, the black font is the correctly classiﬁed\ncategory, the red font is the wrong category, and the green is the\ncorrect label, but the model does not recognize its correct label.\nTable VII shows the models’ number of parameters, GFLOPs,\nand inference speed.\nTABLE VII\nMODELS’NUMBER OF PARAMETERS(PARAMS), GFLOPS, AND INFERENCE\nSPEED\nAll FLOPs are measured with a size of 224 over the ﬁrst 20 000 images of\nthe MLRSNet dataset. Moreover, the FPS in the table is calculated with batch\nsize one on 1080Ti from the total inference pure compute time reported in the\nPytorch.\n2) Qualitative Analysis: In order to ﬁgure out what is learned\ninside our method, we further visualize each module to verify\nthe effectiveness of the proposed method in a qualitative way. In\nFig. 15, we visualized the original image and its corresponding\nCSAM to illustrate the SSM’s ability to capture the semantic\nregions of each category appearing in the image. We used the\nstandard ResNet101 as a backbone, and the training–testing\nratio was 10 −90%. Each row shows the original image and\nactivation map of a speciﬁc category. The category and score are\ndisplayed in the upper left corner of the corresponding category\n1898 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 15. Visualization results of CSAMs on MLRSNet dataset.\nactivation map. The proposed model could locate discriminative\nsemantic regions related to positive categories are highlighted in\nthese feature maps, whereas less informative regions are weakly\nactivated. For example, in Fig. 15(e), the activated area of the\nroad is the road itself, whereas the activated area of the bridge\nincludes the road and water around the bridge, which considers\nthe semantic information of the bridge and water. The SR-Net\ncould accurately highlight relevant semantic regions. In addition,\nthe ﬁnal score indicated that the CACR was sufﬁciently discrim-\ninative and could be accurately identiﬁed using the proposed\nmethod.\nFurthermore, in Fig. 16, we visualized an original image with\nits corresponding relation matrixR to illustrate what relations\nthe SRBM had learned. For the input image in Fig. 16(a), its\nlabels are “buildings,” “bridge,” “cars,” “grass,” “pavement,”\n“road,” “ships,” “trees,” and “water.” Fig. 14(b) is the visual-\nization of theR of the input image.Rwater,bridge and Rwater,ships\nranked top (about top 10%) in the row of “water.” It means\nthat “bridge” and “ships” were more relevant for “water” in\nthe image. From the relation matrix visualization, the SRBM\ncan build such semantic relations for a speciﬁc input im-\nage.\nTAN et al.: TRANSFORMER-DRIVEN SEMANTIC RELATION INFERENCE FOR MULTILABEL CLASSIFICATION 1899\nFig. 16. Visualization of an example and what its relation matrixR looks like on MLRSNet dataset. (a) Input Image (b) Relation Matrix (c) Partial Graph of\nRelation Matrix\nV. CONCLUSION\nIn this study, we propose a novel HRS image multilabel clas-\nsiﬁcation network, transformer-driven SR-Net. The proposed\nnetwork contains two modules: SSM and SRBM. The SSM\ncaptures the semantic regions of features extracted by the DCNN\nand generates a discriminative CACR. The SRBM further uses\nlabel relation inference from outputs of the SSM, that is, CACR,\nto obtain the relation matrix of categories for ﬁnal classiﬁca-\ntion. We conduct extensive experiments on the public UCM\nmultilabel, AID multilabel, and MLRSNet datasets to evaluate\nthe proposed method. The experimental results showed that\nour network using the deep network (e.g., ResNet) could offer\nbetter classiﬁcation results. In addition, we visualize extracted\nsemantic attentional regions and relation matrix for qualitatively\ndemonstrating each module’s effectiveness.\nACKNOWLEDGMENT\nThe numerical calculations in this article have been done on\nthe supercomputing system in the Supercomputing Center of\nWuhan University.\nREFERENCES\n[1] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” inProc. 3rd Int. Conf. Learn. Representa-\ntions, 2015, pp. 19–36.\n[2] C. Szegedy, L. Wei, Y . Jia, P. Sermanet, and A. Rabinovich, “Going deeper\nwith convolutions,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2015, pp. 1–9.\n[3] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\npp. 770–778.\n[4] G. Huang, Z. Liu, V . Laurens, and K. Q. Weinberger, “Densely connected\nconvolutional networks,” inProc. IEEE Conf. Comput. Vis. Pattern Recog-\nnit., 2017, pp. 4700–4708.\n[5] Y . Yuan, J. Fang, X. Lu, and Y . Feng, “Remote sensing image scene clas-\nsiﬁcation using rearranged local features,”IEEE Trans. Geosci. Remote\nSens., vol. 57, no. 3, pp. 1779–1792, Mar. 2019.\n[6] Z. Xiao, K. Wang, Q. Wan, X. Tan, and F. Xia, “A2S-Det: Efﬁciency\nanchor matching in aerial image oriented object detection,”Remote Sens.,\nvol. 13, no. 1, p. 73, 2020.\n[7] X. Tan, Z. Xiao, Q. Wan, and W. Shao, “Scale sensitive neural network\nfor road segmentation in high-resolution remote sensing images,”IEEE\nGeosci. Remote Sens. Lett., vol. 18, no. 3, pp. 533–537, Mar. 2021.\n[8] J. Shao, B. Du, C. Wu, M. Gong, and T. Liu, “HRSiam: High-resolution\nsiamese network, towards space-borne satellite video tracking,” IEEE\nTrans. Image Process., vol. 30, pp. 3056–3068, 2021.\n[9] L. Wan, N. Liu, Y . Guo, H. Huo, and T. Fang, “Local feature representation\nbased on linear ﬁltering with feature pooling and divisive normalization\nfor remote sensing image classiﬁcation,”J. Appl. Remote Sens., vol. 11,\nno. 1, 2017, Art. no. 0 16017.\n1900 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\n[10] H. Li-jun, H. Bin, and Z. Da-biao, “A destriping method with multi-scale\nvariational model for remote sensing images,”Opt. Precis. Eng., vol. 25,\npp. 198–207, Jan. 2017.\n[11] K. Xu, H. Huang, and P. Deng, “Remote sensing image scene classiﬁcation\nbased on global-local dual-branch structure model,”IEEE Geosci. Remote\nSens. Lett., vol. 30, pp. 3056–3068, 2021.\n[12] K. Xu, H. Huang, P. Deng, and Y . Li, “Deep feature aggregation frame-\nwork driven by graph convolutional network for scene classiﬁcation in\nremote sensing,”IEEE Trans. Neural Netw. Learn. Syst., to be published,\ndoi: 10.1109/TNNLS.2021.3071369.\n[13] K. Xu, H. Huang, Y . Li, and G. Shi, “Multilayer feature fusion network for\nscene classiﬁcation in remote sensing,”IEEE Geosci. Remote Sens. Lett.,\nvol. 17, no. 11, pp. 1894–1898, Nov. 2020.\n[14] Z. Feng, H. Li, W. Ouyang, N. Yu, and X. Wang, “Learning spatial regular-\nization with image-level supervisions for multi-label image classiﬁcation,”\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 2027–2036.\n[15] Y . Weiet al., “HCP: A ﬂexible CNN framework for multi-label image\nclassiﬁcation,” IEEE Trans. Softw. Eng., vol. 38, no. 9, pp. 1901–1907,\nSep. 2016.\n[16] P. F. Zhang, H. Y . Wu, and X. S. Xu,A Dual-CNN Model for Multi-Label\nClassiﬁcation by Leveraging Co-Occurrence Dependencies Between La-\nbels. Cham, Switzerland: Springer, 2017.\n[17] Y . Hua, L. Mou, and X. X. Zhu, “Relation network for multilabel aerial\nimage classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 7,\npp. 4558–4572, Jul. 2020.\n[18] L. Mou, Y . Hua, and X. X. Zhu, “Relation matters: Relational context-\naware fully convolutional network for semantic segmentation of high-\nresolution aerial images,” IEEE Trans. Geosci. Remote Sens., vol. 58,\nno. 11, pp. 7557–7569, Nov. 2020.\n[19] R. Huang, F. Zheng, and W. Huang, “Multilabel remote sensing image\nannotation with multiscale attention and label correlation,”IEEE J. Sel.\nTopics Appl. Earth Observ. Remote Sens., vol. 14, pp. 6951–6961, 2021.\n[20] M. Chen et al., “Generative pretraining from pixels,” inProc. Int. Conf.\nMach. Learn., Jul. 2020, vol. 119, pp. 1691–1703.\n[21] A. Dosovitskiyet al., “An image is worth 16×16 words: Transformers for\nimage recognition at scale,”Int. Conf. Learn. Representations, 2021.\n[22] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown, “Learning multi-label\nscene classiﬁcation,” Pattern Recognit., vol. 37, no. 9, pp. 1757–1771,\n2004.\n[23] E. Tanaka et al., “A multi-label approach using binary relevance and\ndecision trees applied to functional genomics,” J. Biomed. Informat.,\nvol. 54, pp. 85–95, 2015.\n[24] J. Read, B. Pfahringer, G. Holmes, and E. Frank, “Classiﬁer chains for\nmulti-label classiﬁcation,” inProc. Eur . Conf. Mach. Learn. Knowl. Discov.\nDatabases, II, 2009, pp. 254–269.\n[25] J. Fürnkranz, E. Hüllermeier, E. L. Mencía, and K. Brinker, “Multilabel\nclassiﬁcation via calibrated label ranking,”Mach. Learn., vol. 73, no. 2,\npp. 133–153, 2008.\n[26] G. Tsoumakas and I. Vlahavas, “Random K-labelsets: An ensemble\nmethod for multilabel classiﬁcation,” inProc. Eur . Conf. Mach. Learn.,\n2007, pp. 406–417.\n[27] M. L. Zhang and Z. H. Zhou, “Ml-KNN: A lazy learning approach to\nmulti-label learning,” Pattern Recognit., vol. 40, no. 7, pp. 2038–2048,\n2007.\n[28] A. E. Elisseeff and J. Weston, “A Kernel method for multi-labelled\nclassiﬁcation,” inProc. Adv. Neural Inf. Process. Syst., 2001, pp. 681–687.\n[29] A. Clare and R. D. King, “Knowledge discovery in multi-label phenotype\ndata” inProc. Eur . Conf. Principles Data Mining Knowl. Discov., 2001,\npp. 42–53.\n[30] N. Ghamrawi and A. McCallum, “Collective multi-label classiﬁcation,” in\nProc. 14th ACM Int. Conf. Inf. Knowl. Manage., 2005, pp. 195–200.\n[31] M.-M. Cheng, Z. Zhang, W.-Y. Lin, and P. Torr, “Bing: Binarized normed\ngradients for objectness estimation at 300FPS,” in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2014, pp. 3286–3293.\n[32] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals from\nedges,” inProc. Eur . Conf. Comput. Vis., 2014, pp. 391–405.\n[33] H. Yang, J. T. Zhou, Y . Zhang, B.-B. Gao, J. Wu, and J. Cai, “Exploit\nbounding box annotations for multi-label object recognition,” inProc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 280–288.\n[34] J. Wang, Y . Yang, J. Mao, Z. Huang, C. Huang, and W. Xu, “CNN-RNN:\nA uniﬁed framework for multi-label image classiﬁcation,” inProc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2016, pp. 2285–2294.\n[35] Z. Wang, T. Chen, G. Li, R. Xu, and L. Lin, “Multi-label image recognition\nby recurrently discovering attentional regions,” inIEEE Int. Conf. Comput.\nVis., 2017, pp. 464-472, doi:10.1109/ICCV .2017.58.\n[36] Q. Li, M. Qiao, W. Bian, and D. Tao, “Conditional graphical lasso for\nmulti-label image classiﬁcation,” inProc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2016, pp. 2977–2986.\n[37] Z. Chen, X. Wei, P. Wang, and Y . Guo, “Multi-label image recognition\nwith graph convolutional networks,” inIEEE/CVF Conf. Comput. Vision\nPattern Recognit., 2019, pp. 5172–5181, doi:10.1109/CVPR.2019.00532.\n[38] T. Chen, M. Xu, X. Hui, H. Wu, and L. Lin, “Learning semantic-\nspeciﬁc graph representation for multi-label image recognition,” inProc.\nIEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 522–531.\n[39] Y . Wanget al., “Multi-label classiﬁcation with label graph superimposing,”\n2019, arXiv:1911.09243.\n[40] Y . Hua, L. Mou, and X. Zhu, “Recurrently exploring class-wise attention\nin a hybrid convolutional and bidirectional LSTM network for multi-label\naerial image classiﬁcation,”ISPRS J. Photogramm. Remote Sens., vol. 149,\npp. 188–199, Feb. 2019.\n[41] G. Sumbul and B. Dem˙Ir, “A deep multi-attention driven approach for\nmulti-label remote sensing image classiﬁcation,” IEEE Access,v o l .8 ,\npp. 95934–95946, 2020.\n[42] J. Ji, W. Jing, G. Chen, J. Lin, and H. Song, “Multi-label remote sensing\nimage classiﬁcation with latent semantic dependencies,”Remote Sens.,\nvol. 12, no. 7, 2020, Art. no. 1110. [Online]. Available: https://www.mdpi.\ncom/2072-4292/12/7/1110\n[43] Y . Guo and S. Gu, “Multi-label classiﬁcation using conditional dependency\nnetworks,” inProc. 22nd Int. Joint Conf. Artif. Intell., 2011, pp. 1300–1305.\n[44] B. Micusik and T. Pajdla, “Multi-label image segmentation via max-\nsum solver,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2007,\npp. 1–6.\n[45] M. Tanet al., “Learning graph structure for multi-label image classiﬁcation\nvia clique generation,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2015, pp. 4100–4109.\n[46] X. Li, F. Zhao, and Y . Guo, “Multi-label image classiﬁcation with a\nprobabilistic label enhancement model,” inProc. 30th Conf. Uncertainty\nArtif. Intell., Jan. 2014, pp. 430–439.\n[47] A. Pal, M. Selvakumar, and M. Sankarasubbu, “Magnet: Multi-label text\nclassiﬁcation using attention-based graph neural network,” inProc. 12th\nInt. Conf. Agents Artiﬁ. Intell., vol. 2, 2020, pp. 494–505.\n[48] Z.-M. Chen, X.-S. Wei, P. Wang, and Y . Guo, “Multi-label image recog-\nnition with graph convolutional networks,” in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit., 2019, pp. 5172–5181.\n[49] Y . Li, R. Chen, Y . Zhang, M. Zhang, and L. Chen, “Multi-label remote\nsensing image scene classiﬁcation by combining a convolutional neural\nnetwork and a graph neural network,”Remote Sens., vol. 12, no. 23, 2020,\nArt. no. 4003.\n[50] N. Khan, U. Chaudhuri, B. Banerjee, and S. Chaudhuri, “Graph convo-\nlutional network for multi-label VHR remote sensing scene recognition,”\nNeurocomputing, vol. 357, pp. 36–46, May 2019.\n[51] V . Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, “Recurrent models of\nvisual attention,” inProc. Adv. Neural Inf. Process. Syst., 2014, pp. 2204–\n2212.\n[52] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by\njointly learning to align and translate,” inProc. 3rd Int. Conf. Learn.\nRepresentations, Jan. 2015.\n[53] A. Vaswani et al., “Attention is all you need,” inProc. Adv. Neural Inf.\nProcess. Syst., 2017, pp. 6000–6010.\n[54] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S.\nZagoruyko, “End-to-end object detection with transformers,” inProc. Eur .\nConf. Comput. Vis., 2020, pp. 213–229.\n[55] J. Chenet al., “Transunet: Transformers make strong encoders for medical\nimage segmentation,” 2021,arXiv:2102.04306.\n[56] P. Deng, K. Xu, and H. Huang, “When CNNs meet vision transformer:\nA joint framework for remote sensing scene classiﬁcation,”IEEE Geosci.\nRemote Sens. Lett., pp. 1–5, vol. 19, 2022.\n[57] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning\ndeep features for discriminative localization,” inProc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2016, pp. 2921–2929.\n[58] B. Chaudhuri, B. Demir, S. Chaudhuri, and L. Bruzzone, “Multilabel\nremote sensing image retrieval using a semisupervised graph-theoretic\nmethod,”IEEE Trans. Geosci. Remote Sens., vol. 56, no. 2, pp. 1144–1158,\nFeb. 2018.\n[59] A. Xq et al., “MLRSNet: A multi-label high spatial resolution remote\nsensing dataset for semantic scene understanding,”ISPRS J. Photogramm.\nRemote Sens., vol. 169, pp. 337–350, 2020.\n[60] A. Zeggada, F. Melgani, and Y . Bazi, “A deep learning approach to UA V\nimage multilabeling,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 5,\npp. 694–698, May 2017.\nTAN et al.: TRANSFORMER-DRIVEN SEMANTIC RELATION INFERENCE FOR MULTILABEL CLASSIFICATION 1901\nXiaowei Tanreceived the B.S. degree in remote sens-\ning science and technology in 2017 from the China\nUniversity of Geosciences, Wuhan, China, where\nshe is currently working toward the Ph.D. degree in\nphotogrammetry and remote sensing with the State\nKey Laboratory of Surveying, Mapping and Remote\nSensing Information Engineering.\nHer research interests include the application of\nclassiﬁcation and semantic segmentation in remote\nsensing.\nZhifeng Xiao (Member, IEEE) received the Ph.D.\ndegree in photogrammetry and remote sensing from\nWuhan University, Wuhan, China, in 2008.\nFrom 2014 to 2015, he was a Visiting Scholar with\nthe Computational Biomedicine Imaging and Model-\ning Center, Rutgers University, New Brunswick, NJ,\nUSA. He is currently an Associate Professor with\nthe State Key Laboratory of Information Engineering\nin Surveying Mapping and Remote Sensing, Wuhan\nUniversity. His work consists of object detection in\nremote sensing images, large-scale content-based re-\nmote sensing image retrieval, and scene analysis on remote sensing images. His\nresearch interests include remote sensing image processing, computer vision,\nand machine learning.\nJianjun Zhu received the M.S. degree in electrical\nengineering from North China Electric Power Univer-\nsity, Beijing, China, in 2005. He is a Deputy General\nManager and a Senior Engineer with National Bio\nEnergy Company, Ltd., Beijing, China.\nHe has in-depth research on biomass power gen-\neration and integrated energy development based on\nbiomass energy utilization at the county level.\nQiao Wan received the B.S. degree in geographic in-\nformation system and M.S. degree in cartography and\ngeographic information systems in 2012 and 2015,\nrespectively, from the Wuhan University, Wuhan,\nChina, where she is currently working toward the\nPh.D. degree in photogrammetry and remote sensing\nwith the State Key Laboratory of Surveying, Mapping\nand Remote Sensing Information Engineering.\nShe is currently a Teacher with the College of\nComputer Science and Technology, Guizhou Univer-\nsity, Guiyang, China. Her research interests include\ndomain generalization in remote sensing images.\nKai Wang received the B.S. degree in geographic\ninformation science and the M.S. degree in cartogra-\nphy and geographic information systems in 2018 and\n2021, respectively, from Wuhan University, Wuhan,\nChina, where he is currently working toward the Ph.D.\ndegree in remote sensing science and technology with\nSchool of Remote Sensing and Information Engineer-\ning.\nHis research interests include object detection in\nremote sensing.\nDeren Li (Member, IEEE) received the Ph.D. degree\nin photogrammetry from the University of Stuttgart,\nStuttgart, Germany, in 1986 and the Honorary Doc-\ntorate degree from ETH Zürich, Zürich, Switzerland.\nHe is a Scientist in surveying, mapping and remote\nsensing with Wuhan University, Wuhan, China.\nDr. Li is a member of the Chinese Academy of Sci-\nences and the Chinese Academy of Engineering. He\nis also a member of International Eurasia Academy of\nSciences and International Academy of Astronautics.\nHe was the recipient of the Honorary Member and the\nBrock Gold Medal in recognition of outstanding contributions to photogram-\nmetry from the International Society for Photogrammetry and Remote Sensing.",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.8387272357940674
    },
    {
      "name": "Computer science",
      "score": 0.7359397411346436
    },
    {
      "name": "Inference",
      "score": 0.7221662998199463
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7101114392280579
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.565460205078125
    },
    {
      "name": "Transformer",
      "score": 0.5543760061264038
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5358238220214844
    },
    {
      "name": "Relation (database)",
      "score": 0.5186678171157837
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.49965357780456543
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4945785701274872
    },
    {
      "name": "Artificial neural network",
      "score": 0.4194062054157257
    },
    {
      "name": "Machine learning",
      "score": 0.41771399974823
    },
    {
      "name": "Natural language processing",
      "score": 0.36753273010253906
    },
    {
      "name": "Data mining",
      "score": 0.3096107244491577
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}