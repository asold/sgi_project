{
  "title": "Foundation models for metallurgy?",
  "url": "https://openalex.org/W4410956524",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5042612762",
      "name": "Daniel Marchand",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4310135808",
    "https://openalex.org/W4386740800",
    "https://openalex.org/W4391631778",
    "https://openalex.org/W4389132751",
    "https://openalex.org/W6602780416",
    "https://openalex.org/W6902240183",
    "https://openalex.org/W1992985800",
    "https://openalex.org/W4402827043",
    "https://openalex.org/W4406237260",
    "https://openalex.org/W4400526657",
    "https://openalex.org/W4408168640",
    "https://openalex.org/W6699574212",
    "https://openalex.org/W4405456044",
    "https://openalex.org/W4406693067",
    "https://openalex.org/W4401335746",
    "https://openalex.org/W3091277511",
    "https://openalex.org/W4230126906",
    "https://openalex.org/W6929642656",
    "https://openalex.org/W4207013141",
    "https://openalex.org/W3176386153",
    "https://openalex.org/W2910857709",
    "https://openalex.org/W3111002019",
    "https://openalex.org/W4389477389",
    "https://openalex.org/W6604066465",
    "https://openalex.org/W4388761260",
    "https://openalex.org/W3109685026",
    "https://openalex.org/W4281610525",
    "https://openalex.org/W2485827177",
    "https://openalex.org/W2765259952",
    "https://openalex.org/W2007850701",
    "https://openalex.org/W1973663039",
    "https://openalex.org/W3000291311",
    "https://openalex.org/W4392301541",
    "https://openalex.org/W4384206875",
    "https://openalex.org/W4406270750",
    "https://openalex.org/W6944992465",
    "https://openalex.org/W3013990447",
    "https://openalex.org/W2120145199",
    "https://openalex.org/W3013285202",
    "https://openalex.org/W3043317903",
    "https://openalex.org/W1952152691",
    "https://openalex.org/W2601081289",
    "https://openalex.org/W2015197254",
    "https://openalex.org/W1976492731",
    "https://openalex.org/W2025444507",
    "https://openalex.org/W2914218087",
    "https://openalex.org/W2939169979",
    "https://openalex.org/W3201073812",
    "https://openalex.org/W3195293013",
    "https://openalex.org/W3084164197",
    "https://openalex.org/W3083787461"
  ],
  "abstract": "Abstract Foundation models appear to promise precision atomic modeling across the periodic table, requiring little more than “fine-tuning” with a few density functional theory calculations. However, it is not clear whether they are sufficiently accurate, even with fine-tuning, for materials modeling to justify their high compute cost. Here, we compare state-of-the art foundation models against a collection of “bespoke” neural network potentials for the Al–Cu–Mg–Zn system. While we find many foundation models to give poor or very poor results, some such as GRACE2L-OAM offer extremely good accuracy in most cases. We find that models trained on the defect-containing and higher k-point density “Alexandria” data set had much better performance than those trained on Materials Project data alone. Our results also indicate that thermal conductivity scores are a much better indicator of metallurgical performance than energy errors on the convex hull. Impact statement GPT4 established to the broad community that foundation models can achieve results that are impossible with hand-crafted or “bespoke” language models. There has now been a rush to see if this is also true for atomic modeling. Indeed even large corporations such as Microsoft, Meta, and Google have begun to develop large training sets and publishing results for universal atomic potentials. The promise is that these models should be accurate enough out of the box for most applications or at most, require a small amount of “fine-tuning” on targeted data. But how accurate are these models and can they be used in a production context for metallurgy? Here, we carefully validate a variety of top-performing foundation models on a comprehensive set of benchmarks applicable to aluminum metallurgy. While we find many are completely unsuitable, some, including the very recently released GRACE2L-OAM model, do indeed have sufficient accuracy for most applications. Our results show that the GRACE2L-OAM model offers similar and sometimes somewhat better accuracy than our previously developed neural network potentials. While foundation models do indeed seem to have found their place in the atomic modeling toolkit, they remain much more computationally expensive. Thus proper model selection is a tradeoff between desired accuracy, compute time, and user setup time. Graphical abstract",
  "full_text": "MRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin              805\nFoundation models for metallurgy?\nDaniel Marchand* \nReceived: 14 December 2024 / Revised: 3 March 2025 / Accepted: 1 April 2025 /  \nPublished online: 2 June 2025\nFoundation models appear to promise precision atomic modeling across the periodic \ntable, requiring little more than “fine-tuning” with a few density functional theory  calcula-\ntions. However, it is not clear whether they are sufficiently accurate, even with fine-tuning, \nfor materials modeling to justify their high compute cost. Here, we compare state-of-the \nart foundation models against a collection of “bespoke” neural network potentials for the \nAl–Cu–Mg–Zn system. While we find many foundation models to give poor or very poor \nresults, some such as GRACE2L-OAM offer extremely good accuracy in most cases. We \nfind that models trained on the defect-containing and higher k-point density “Alexandria” \ndata set had much better performance than those trained on Materials Project data alone. \nOur results also indicate that thermal conductivity scores are a much better indicator of \nmetallurgical performance than energy errors on the convex hull.\nIntroduction\nRecently, there has been substantial work \nand interest in the development of “foun-\ndation” interatomic potentials. Many such \npotentials have been released, includ-\ning M3GNET, 1 CHGNET, 2 SevenNet, 3 \nMACE, 4 GNoME, 5 MatterSim, 6 eqV2, 7 \nand GRACE. 8 These potentials generally \nuse a graph neural network with a message \npassing architecture and are trained on vast \ntraining data  sets. The first generation of \nuniversal interatomic potentials were typi-\ncally trained on MPtraj,9 from the Materi-\nals Project,10 which contains DFT calcula -\ntions from more than 1.5 million structures. \nMore recently, foundation models have been \ntrained on much larger training sets such as \nOMat247 consisting of 110 million struc-\ntures and Alexandria 11 consisting of \n10.4 million structures.\nThese potentials can often compute \nproperties that are not directly represented \nin their training data. For example, MACE \ncan predict the generalized stacking-fault \nenergy profiles for W, Mo, and Nb with \nqualitative accuracy. However, it does not \ndo so with sufficient numeric precision to \npredict mechanical properties, with the \nunstable stacking-fault energies of W and \nMo being half that of density functional \ntheory (DFT). 4 An emerging strategy is \nto use a baseline foundation model and \nthen further train or “fine-tune” the model \non a targeted data set. Fine-tuning, there-\nfore, may be able to overcome systemic \nand structure-specific weaknesses in the \nunderlying foundation model. For exam -\nple, universal potentials based on MPtraj \nare oversampled on near-equilibrium struc-\ntures, causing systematic softening that can \nbe mitigated via fine-tuning. 12 Fine-tuning \nhas also proven to be quite effective in \nsurface energy, 13 phonon, 14 and thermal \nconductivity  calculations15 and can boost \nperformance with only a few additional \nstructures.16\nThe field is moving very rapidly. A \npopular leaderboard for foundation mod-\nels, Matbench Discovery, 17 a benchmark \nof accuracy on convex hull and, more \nrecently, thermal conductivity[?], typi-\ncally shows a new record-breaking poten-\ntial every month. At the time of writing the \ntop scoring model for energy predictions is \neqV2_86M_omat_mp_salex model the \nmodel trained on MPtraj, OMat24, and \nAlexandria.7 The top scoring model for \nthermal conductivity is GRACE-2 L-OAM \n© The Author(s) 2025\ndoi:10.1557/s43577-025-00911-0\nImpact Article\nImpact statement\nGPT4 established to the broad community that \nfoundation models can achieve results that are \nimpossible with hand-crafted or “bespoke” lan-\nguage models. There has now been a rush to see if \nthis is also true for atomic modeling. Indeed even \nlarge corporations such as Microsoft, Meta, and \nGoogle have begun to develop large training sets \nand publishing results for universal atomic poten-\ntials. The promise is that these models should be \naccurate enough out of the box for most applica-\ntions or at most, require a small amount of “fine-\ntuning” on targeted data. But how accurate are \nthese models and can they be used in a production \ncontext for metallurgy? Here, we carefully validate \na variety of top-performing foundation models on \na comprehensive set of benchmarks applicable to \naluminum metallurgy. While we find many are \ncompletely unsuitable, some, including the very \nrecently released GRACE2L-OAM model, do indeed \nhave sufficient accuracy for most applications. Our \nresults show that the GRACE2L-OAM model offers \nsimilar and sometimes somewhat better accuracy \nthan our previously developed neural network \npotentials. While foundation models do indeed \nseem to have found their place in the atomic mod-\neling toolkit, they remain much more computation-\nally expensive. Thus proper model selection is a \ntradeoff between desired accuracy, compute time, \nand user setup time.\nDaniel Marchand , SINTEF Industry, Oslo, Norway; daniel.marchand@sintef.no\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\n806        MRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin\ntrained on the same data sets. Other high scoring models are \nMACE-MPA-0, MACE-trained MPtraj and Alexandria, \nGRACE-1L-OAM (a lower cost purely local version of the \nGRACE2L-OAM model), and MatterSim v1 5 M.\nImmense computational resources are required to create \nthese DFT data sets and train large models. For example, the \neqV2 model from  Meta7 was trained on 64 NVIDIA A100 \nGPUs for more than 400 million compute hours. It is therefore \nessential that careful validation is undertaken to verify that \nthe quality of these models are indeed improving when given \nthese additional data and as the models grow in size and com-\nplexity. In particular, it is important to check the computational \ncost of running large foundation models against less expensive \nalternatives. For example, in one benchmark computing Cux Al \nstructures MACE was found to be 60 times more computation-\nally expensive than computing with atomic cluster expansion \n(ACE).17 Kabylda et al. found tht “bespoke” potentials could \noutperform foundation models in predicting tensile behavior \nof graphene nanosheets.18\nIn this work, we focus on assessing whether the top foun-\ndation models are currently ready for production scientific \nuse for metals and if fine-tuning can boost their performance. \nThere has been limited work here, particularly for foundations \nthat go beyond MPtraj. The ORB foundation model trained \non Alexandria was found to be able to reproduce a DFT-\ncomputed Au–Cu phase diagram with qualitative accuracy, 19 \ngetting the broad phase transition feature correct but not with \nsufficient accuracy for applied use. Wang et al. tested fine-\ntuning several different foundation models on high-entropy \nalloy structures and found that the variation performance was \nmore correlated with training data set selection than to model \narchitecture.20 For real metallurgical applications, one needs to \nconsider (often simultaneously) the thermodynamics, kinetics, \nand mechanics of the system. This means carefully validating \nthe bulk properties of the main matrix element of the alloy, \nthe formation energy of its intermetallics, the clustering of \nsolutes, the structure of dislocations, and the response of its \nprecipitates to shearing.\nIn prior work, we created a collection of custom, \n“bespoke,” Behler–Parrinello neural network potentials \n(BP-NNPs) for aluminum alloys. 21–23 Here, we compare the \nperformance of top foundation models “out of the box” and \nthen examine whether fine-tuning a foundation model can \nsubstantially boost performance on a robust set of bench-\nmarks for aluminum alloys. In the main article, we focus on \nMACE-trained MPtraj referred to afterward as MACE-MPt \nand GRACE2l-OAM referred afterward as grace2l. In Supple-\nmentary 2, we also include results for GRACE1l-OAM (gra-\nce1l), MatterSim v1 5 M (mattersim), MACE-MPA-0  \n(MACE-MPA), and eqV2_86M_omat_mp_salex (eqV2-\n86 M_OMS). For the main article, we fine-tune MACE-MPt \non the exact same large training data set used to create the \nAlCuMgZn-NNPs (MACE-FtL), the expectation being \nthat MACE-FtL should outperform AlCuMgZn-NNPs. In \nSupplementary 3, we include other training and fine-tuning \nvariants of MACE, including using a multi-head training pro-\ncedure (MACE-MH), directly training MACE on only the \nAlCuMgZn data (MACE-DT). Finally, we fine-tune MACE-\nMPt on a small data set (MACE-FtS) consisting of 867 \n32-atom fcc bulk structures with random elements and vacan-\ncies. The ambition of MACE-FtS is to see whether it could \nequal the accuracy of AlCuMgZn-NNPs given very limited \ndata. We show a trial performant atomic cluster expansion \n(PACE)  potential24–26 to evaluate the speed of future bespoke \npotentials on a graphics processing unit (GPU). We were not \nable to develop a PACE potential that was as accurate as our \nAlCuMgZn-NNPs (Supplementary 4), but we assume this \nshould be possible in future.\nWe find that the grace2l model offers extremely good per-\nformance beating the AlCuMgZn-NNPs on many challeng-\ning properties they were specifically designed to handle. In \ngeneral, we see that thermal conductivity is a much better \npredictor of model performance than convex hull energies, \nas the eqV2 model, which scores slightly higher than gra-\nce2l but the eqV2 model has awful performance and is the \nworst model tested here. The mattersim model performs little \nbetter. MACE-MPt performs well but not well enough for \nproduction use and the MACE-MPA variant performs much \nbetter in all categories, indicating the value of the Alex-\nandria training data. The grace1l is on average the second \nbest foundation model indicating the value of the training \ndata used by both models while hinting that the nonlocal \neffects included by 2 l do indeed boost accuracy overall. In \nthe main paper, we show that fine-tuning the MACE-MPt \nmodel on the AlCuMgZn data does boost performance to \nnearly the same level as the AlCuMgZn-NNPs. In Supple-\nmentary 3, we show that all versions of MACE that were \ntrained or fine-tuned on the AlCuMgZn data (MACE-FtL, \nMACE-MH, MACE-DT) performed similarly well, indicat-\ning that the MPtraj data pretraining did not substantially \nboost performance. MACE-FtS shows that limited data can \nsignificantly boost performance.\nResults and discussion\nPure bulk structures\nFigure 1 shows the percentage deviation from pure elemen-\ntal DFT results in lattice and elastic constants, surfaces ener -\ngies, and, for Al and Cu, the stable and unstable stacking-fault \nenergies.\nFor pure Al, MACE-MPt performs quite badly, with poor \nresults across all properties. MACE-FtL shows substantial \nimprovement in performance, although perhaps insufficient \nto justify its higher computational cost. The AlCuMgZn-\nNNPs perform well for Al and Cu, with some error in elastic \nconstants for Al and a notable error in stable stacking-fault \nenergies for Cu. The grace2l model, however, shows superb \nperformance for all elements except for Zn.\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\nMRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin              807\nFigure 1.  Percentage difference of AlCuMgZn-NNPs, MACE-MPt, MACE-FtL, and grace2l relative to density functional \ntheory (DFT) for lattice constants, elastic constants, and surfaces energies and, for Al and Cu, stable and unstable \nstacking-fault energies. The relevant structures are included in the AlCuMgZn-neural network potentials (NNPs) and \nMACE-FtL training data. The grace2l model performs very well for all elements except Zn, while the AlCuMgZn-NNPs \ntend to outperform the MACE models.\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\n808        MRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin\nFigure 2.  Formation energy, atomic volume, and elastic constants C11 , C21 , and C44 with respect to Al–Cu Open Quantum Materi-\nals Database structures for density functonal theory (DFT), AlCuMgZn-neural network potentials (NNPs), MACE-MPt, MACE-FtL, \nand grace2l. The relevant structures are included in the AlCuMgZn-NNPs and MACE-FtL training data. While the AlCuMgZn-NNPs \nare the best predictors of formation energy, we note that grace2l was trained using different DFT settings which may account for \nsome of the error.\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\nMRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin              809\nMACE-MPt and all other foundation models based only \non MPtraj fail for Al because Al is  very sensitive to k-point \ndensity and electronic smearing for accurate mechanical prop-\nerties. In our prior work for bulk Al, we used a k-point density \nof 2000 per reciprocal atom, with a high Methfessel–Paxton \nsmearing of 0.6 eV needed to ensure high-quality elastic con-\nstants in aluminum. 23 The Materials Project uses a k-point \ndensity of 1000 per reciprocal atom and a small Gaussian \nsmearing of 0.03  eV . This is far too low for aluminum, and \nthe current entry mp-134 shows their settings would have \naluminum be elastically unstable at –28 GPa for C 44 . Some of \nthe Alexandria data set does use much denser k-point meshes \nof 2000–8000 k-points per reciprocal angstrom,11 which would \nbe sufficient. Indeed, we see that MACE-MPA (see Supple -\nmentary 3) also performs much better for pure aluminum, \nindicating the importance of the Alexandria training data. \nIndeed, it seems that even fine-tuning over a small number \nof structures is enough to correct for the insufficient k-points \nto compute elastic constants, as shown by MACE-FtS (see \nSupplementary 3).\nFor the purposes of Al metallurgy the stable stacking-fault \nenergy, γSSF , is a critical parameter for mechanical perfor -\nmance. As will be discussed later, the partial dislocation sepa-\nration distance is largely a function of the stable stacking-fault \nenergy. As predicted here, we will later see that the MACE-\nMPt and MACE-FtL do not perform well in dislocation sepa-\nration distance prediction. We find that only the AlCuMgZn-\nNNPs and grace2l performs well for stable stacking-fault \nenergies in Al. All other foundation models, including the less \ncomputational expensive grace1l fail for stable stacking-fault \nenergy (Supplementary 2), and all variants of MACE fail as \nwell (Supplementary 3).\nZn is a struggle for all models and is known to be particu-\nlarly difficult to model correctly. 27 For example, it has been \nvery challenging to get both the lattice and elastic constants \ncorrect simultaneously and only very recently has a potential \nthat can reasonably model lattice and elastic constants for pure \nzinc been developed.28\nWe note that the AlCuMgZn-NNPs are only well suited \nfor aluminum alloys and only do a reasonable job for Cu, Mg, \nand Zn. For simulations on these elements there are other \npotentials that target them much more precisely (see Refer -\nence 29 for Cu, Reference  30 for Mg, and Reference 28  for \nZn). Although it may not always be computationally efficient \nto use the grace2l model, it should provide fairly good results \nin most cases, at least as good as the AlCuMgZn-NNPs.\nBulk alloy structures\nWhen evaluating whether a potential will be good at predict-\ning the stability of unknown compounds, it is reasonable to \nexpect it to at least predict the stability of all known com-\npounds. Here, we show an evaluation for the atomic volume \nand formation energy for all OQMD structures and for many \nof them the elastic constants as well. All formation energies \n/Delta1E structure\nf  are computed relative to a reference of dilute solid \nsolution in aluminum matrix:\nwhere E structure is the energy of the structure computed via \nthe potential, n i is the number of elements of type i  in that \nstructure, and E ref\ni  is the reference energy for element i , for \naluminum, it is the energy of a single atom in bulk fcc, while \nfor all other elements it is\nwhere E Al255i refers to the reference cell used in this study \nconsisting of a 4 × 4 × 4 fcc supercell of 256 atoms, where \none of them has been substituted with solute element i. When \nthe space group of the structure prior to relaxation does not \nmatch the space group after relaxation, that structure is omitted \nfrom the graph. For example in Figure 2, the high-energy Cu2 \nCmcm and Al2 Fmmm structures are unstable for all potentials \n(usually relaxing into the Fm-3 m fcc configuration) and thus \nare not plotted.\nThe bulk formation energies, atomic volumes, and elastic \nconstants for all Al–Cu OQMD structures are shown in Fig-\nure 2. The supplementary material includes similar results for \nAl–Cu–Mg–Zn, Al–Cu–Mg, Al–Mg–Zn, Cu–Mg–Zn, Al–Mg, \nMg–Cu, Cu–Zn, and Mg–Zn for all potentials. In general, \nMACE-MPt would be unusable for this system without fine-\ntuning and cannot even predict general trends in formation \nenergy. We find that only Alexandria-based models (except \neqV2) have reasonable performance for these structures. All \nMACE variants that were directly trained on the entire Al–Cu \nsystem (MACE-FtL, MACE-MH, MACE-DT) did fairly well \nin performance, similar to AlCuMgZn-NNPs. It is important \nto note that we used different settings than in the Alexandria \ndata set and so some of the error could be purely differences \nin DFT settings.\nAtomic volumes and elastic constants can be very impor -\ntant in predicting the stability and growth of strengthening \nphases in aluminum alloys. For the 2xxx series, these are the \nθ′′ , θ′′ , and θ phases for which the fine-tuned MACE models, \ngrace2l, and the AlCuMgZn-NNPs all manage fairly well. But, \nin general none of the models are very good predictors of elas-\ntic constants and so caution should be used here if modeling \nthe precipitation of an unknown phase. The grace2l model has \npossibly the best elastic constants, which one would expect \nfrom its good ability to model phonons.\nBulk structures and precipitates often develop with dilute \ndeviations in their composition (i.e., with substitutional [antisite] \ndefects). In some engineering applications, such as for crossover \n(1)�E structure\nf =\n(\nE structure−\n∑\ni\nniE ref\ni\n)\n/N ,\n(2)E ref\ni =Al = E Al 255 i − 255E ref\ni=Al ,\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\n810        MRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin\n7xxx-based alloys, discussed below, substitutional defects are \nimportant stabilizers of desirable precipitate phases. Substitu-\ntion defect energies, /Delta1E Defect\nf  , were computed via the following:\n where E Defect is the energy of the structure with the defect, \nE pristine is the energy without the defect, and E ref\nX  and E ref\nY  are the \nreference energies of atoms Y and X. For Al–Cu and Al–Cu–Mg, \nthe relevant structures are not included in the training data set, \nwhile for η′ and T-phase, the relevant structures are included \n(3)/Delta1E Defect\nX →Y = E Defect\nX →Y − E pristine− E ref\nY + E ref\nX ,\nin the training data set \nof MACE-FtL and the \nAlCuMgZn-NNPs.\nIn Figures S1–22 and \nS1–23 in Supplementary \n1 (see the same figures in \nSupplementary 2–4 for \nother models), we show \nthe substitution ener -\ngies for structures in the \nAl–Cu and Al–Cu–Mg \nsystems. In Figure S1–21, \nwe show substitutional \nenergy results for the T \nand η′ phases, which are \nimportant for the 7xxx \nseries alloys as well as for \nnovel crossover alloys.31 \nWhile MACE-MPt has \ninsufficient accuracy \nfor most antisites, gra-\nce2l performs very well. \nIndeed on most structures, \ngrace2l substantially out-\nperforms the AlCuMgZn-\nNNPs, even for the η′ and \nT-phase results that were \nexplicitly included in the \nAlCuMgZn-NNp train-\ning set. It seems likely \nthat this is due to the \nAlexandria training set, \nas MACE-MPA also has \nvery good performance \non antisite energy predic-\ntion. Surprisingly, MACE-\nMPA outperforms the \nMACE variants that are \ndirectly trained on the η′ \nand T-phase antisite data.\nOverall, it appears \nthat predicting antisite \nformation energy is a \ntask that is very well \nsuited for foundation models, particularly if they include \nthe Alexandria training data.\nSolute clusters and solute–stacking‑fault interactions\nFor age-hardened alloys, such as the 2xxx, 6xxx, and 7xxx \nalloys in aluminum, the kinetics of early-stage precipitate for-\nmation is dominated by the clustering of dilute solute atoms. \nIf the energetics of solute pairs and triplets is wrong then any \nkinetic study of precipitate development will be wrong. Sol-\nute–solute interaction energies for Zn–Mg–Cu-Vacancy bina-\nries are shown in Figure 3.\nFigure 3.  Solute–solute binding energy computed with density functional theory (DFT), AlCuMgZn-neural \nnetwork potentials (NNPs), MACE-MPt, MACE-FtL, and MACE-FtS for two-atom Cu, Mg, Zn, Vacancy–Cu, \nMg, Zn, Vacancy pairs in aluminum matrix for the first five nearest-neighbor positions. The relevant struc-\ntures are included in the AlCuMgZn-NNPs and MACE-FtL training data. Despite not directly including any \nrelevant structures, grace2l outperforms all models.\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\nMRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin              811\nWhile MACE-MPt can perform well more so than most clas-\nsical potentials,21,32 it still shows some odd artifacts. For exam-\nple, the fourth nearest neighbor in Cu–Cu has the same binding \nenergy as the first nearest neighbor. This means a kinetic study \nof Cu clustering would likely get stuck with Cu atoms resting \nat fourth nearest neighbors longer than expected thus inhibiting \ncluster formation. Overall, for binary solutes, the AlCuMgZn-\nNNPs and the MACE fine-tuned models perform roughly \nequally well. All of these potentials are broadly within kT (25-\nmeV room temperature) accuracy, with the AlCuMgZn-NNPs \nperforming better on solute–solute configurations and the MACE \nmodels performing better on vacancy-containing configurations. \nThe grace2l model performs very well on binary solute energies, \nequal or perhaps somewhat better than the AlCuMgZn-NNPs.\nTriplet formation energies for Cu–Mg–Zn solutes in 111 \n(all nearest neighbor) and 112 (two nearest neighbors and \none second nearest neighbor) configurations are shown in \nFigure S1-18. MACE-MPt performs much worse on tri-\nplet binding energies than the custom AlCuMgZn-NNPs. \nThe grace2l model performs roughly equally as well as the \nAlCuMgZn-NNPs without any training. None of the other \nfoundation models, including Alexandria-trained models \n(grace1l and MACE-MPA) perform as well as grace2l.\nFigure S1-17 shows some larger four-atom clusters of Cu \nin an Al matrix. MACE-MPt predicts relative stability among \nthe clusters correctly but does so with sign errors, indicating \nthat some stable clusters should decompose. Again, we see that \ngrace2l is the only other model that matches the performance \nof the AlCuMgZn-NNPs.\nSolute strengthening is an important mechanism for many \naluminum alloys, especially the 5xxx series used in the auto-\nmotive industry. It involves the impediment of dislocation \nmovement by solute atoms. The amount of solute strengthen-\ning is dependent on the solute–stacking-fault interaction ener-\ngies which we examine here. We computed solute–stacking-\nfault interaction energies, EX −SF , with the following:\nwhere ESol−SF\nN −1,X  is the energy of the structure with a stacking fault \nwith N-1 aluminum atoms and one solute, X, ESF\nN  is the energy \nof the stacking fault without the solute, and E Ref is the reference \nenergy. The results of these calculations are shown in (Figure \n4). Note that when calculating the stacking-fault energy with \nsolutes we relax the cell in the direction normal to the stacking \nfault so as to reach zero stress on that axis. MACE-MPt per -\nforms poorly here, com-\nputing a near-constant \n≈−5 meV first nearest-\nneighbor interaction \nenergy for all elements \n(Figure 4). MACE-FtL \nperforms worse or equal \nto MACE-MPt in all \ncases, despite actually \ncontaining the underly-\ning structures in its train-\ning set.\nThe misfit volume is \nanother important contri-\nbution to solute strength-\nening. We compute the \nmisfit volumes /Delta1vm of \nsolutes using relative \nchanges in pressure /Delta1p \nproportional to the bulk \nmodulus, B, and change \nin concentration /Delta1c:\nIn this work, we used the \ndifference in pressure \nfrom swapping one Al \nmatrix atom to a solute \nin a 4 × 4 × 4 supercell \nof 256 atoms. Figure \nS18 shows the misfit \n(4)EX −SF = E Sol−SF\nN −1,X − E SF\nN − E Ref\nX − (N − 1)E Ref\nAl ,\n(5)�vm = ( 1\nB\n�p\n�c).\nFigure 4.  Solute–stacking-fault interaction for the first nearest neighbors to the stacking fault, for Cu, \nMg, and Zn. Computed with density functional theory (DFT), AlCuMgZn-neural network potentials (NNPs), \nMACE-MPt, MACE-FtL, and grace2l. The relevant structures are included in the AlCuMgZn-NNPs and \nMACE-FtL training data. The MACE models perform poorly here, particularly for Cu-stacking-fault ener-\ngies. The grace2l model performs very well here again, despite not being trained directly on the relevant \nstructures.\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\n812        MRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin\nvolume for Vac, Cu, Mg, and Zn. In general, all potentials \nperform fairly well, MACE-MPt does the best job for Vac, \nwhile predicting an overly negative Cu misfit volume.\nOverall, the MACE fine-tuned models performed some-\nwhat worse than the AlCuMgZn-NNPs. While they performed \nslightly better for vacancy-solute binary formation energies, \nthey did worse on solute triplets and solute–stacking-fault inter-\naction energies. Again, the conclusion appears to be that MACE \nmodels could serve well in a screening context, as MACE-FtS \nappears to be efficient at learning solute energies, so long as \ncalculations are later verified with DFT, so that major failures \nsuch as the Cu-stacking-fault energy error are controlled for.\nOverall, only grace2l, MACE-MPA, and the AlCuMgZn-\nNNPs have good performance for solute-stacking-fault ener -\ngies. Indeed, it is somewhat surprising that MACE-MPA out-\nperforms MACE models that have been fine-tuned specifically \nto perform well on this task, indicating that the Alexandria \ndata set does cover relevant structures. However, the grace1l \nmodel does have some substantial errors despite having the \nsame training data as the grace2l model.\nPartial dislocation separation\nAn edge dislocation in a fcc metal is inherently unstable and \nwill dissociate into two partial dislocations separated by a stack-\ning fault. The dissociation distance is an important property for \na potential to compute correctly when simulating the mechanics \nof fcc metals. This distance can be computed from basic proper-\nties of the metal using linear elasticity theory as follows:33\nwhere d is the distance between the two partial dislocations, G \nis the elastic shear modulus, b is the partial dislocation Burgers \nvector, in fcc this is a0\n6 [112] , and γSSF is the stable stacking-\nfault energy.33\nTo simulate the partial dislocation dissociation, we first \ninserted an edge dislocation, preseparated into partials, into \na large rectangular slab of atoms. Atoms away from the dis-\nlocation were deformed according to linear elasticity using \nthe material constants of each potential. The insertion of the \nedge dislocation and the application of the elastic field were \n(6)d = Gb2\n4πγSSF ,\nperformed using the atomman package, which has been devel-\noped under the Interatomic Potentials Repository project. 34,35 \nWe then carved out a cylinder with a 60 Å radius around the \ncenter of the dislocation holding the outer 10 Å radius of \natoms fixed and allowing the rest to relax until forces were \nconverged within 0.0005 eV/Å for all atoms. The initial partial \nseparation distance was taken from an initial trial run with a \n50 Å radius cylinder, which had no separation distance. It was \nnot feasible to run all 20 AlCuMgZn-NNPs so we ran only \nAlCuMgZn-NNP1, which was selected arbitrarily. The results \nof this relaxation can be seen in Figure 5.\nA summary of the key material parameters used to predict \nthe partial dislocation separation distance, d, as well as the pre-\ndicted versus simulated values can be seen in Table I. All poten-\ntials ran in this study showed excellent agreement between the \npredicted value from linear elasticity and the simulated result, \nexcept MACE-MPt. This is likely because of MACE-MPt hav-\ning a very small γSSF and therefore a very large partial disso-\nciation length, too large for the limited simulation cell size. In \nany case, the simulation shows that MACE-MPt deviations are \nstrongly from DFT. The best-performing model was grace2l \nlikely due to its highly accurate stable stacking-fault energy.\nGeneralized stacking faults\nThe generalized stacking-fault (GSF) energy surface is an \nimportant generalization of the stable stacking-fault energy \nfor understanding how precipitates strengthen age-hardening \nalloys. When a dislocation encounters a precipitate, such as \nthe θ′′ and θ phases in Al–Cu-containing alloys, the resistance \nof the precipitate to shearing is determined by the points in \nthe GSF energy surface corresponding to the slip imposed by \nthe incoming dislocation. Here, GSF energies, /Delta1γGSF\n⃗t  , at a set \nof predetermined critical locations, ⃗t  were computed using \nthe following:\nwhere EGSF\n⃗t  is the energy of the structure with the lattice \nvector going through the slip plane shifted by vector, ⃗t  , and \nE pristine is the energy of the unshifted structure and A  is the \narea of the periodic cell normal to the GSF.\nIn Figure 6, we show the GSF energies for the θ′′ and θ pre-\ncipitates. In both cases, MACE-MPt performs somewhat well, \n(7)�γGSF\n�t = (E GSF\n�t − E pristine)/A ,\nTable I.  Predicted and simulated partial dislocation dissociation distances along with underlying material parameters.\n1AlCuMgZn-NNP1.\n2DFT results from Reference 21 except for d simulated.\n3DFT simulation taken from Reference 36.\nPotential MACE-MPt MACE-FtL grace2l NNP1 DFT2\nb2 (Å2) 2.784 2.718 2.724 2.719 2.720\nG (GPa) 17.0 31.1 30.8 41.8 30.7\nγSSF (mJ/m2) 12.3 56.8 128.5 108.7 122\ndpredicted  (Å) 63.6 23.7 10.4 16.6 10.8\ndsimulated (Å) 45.2 23.7 13.6 16.4 9.53\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\nMRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin              813\nwith a notable error at site index 7 for θ′′, Figure 6 (bottom), \nresults being comparable to a classical potential.21 While most \nmodels perform well on this task, we once again see the best \nperformance being for the AlCuMgZn-NNPs and for grace2l. \nIn Supplementary 3 S19–S20, we see hints that foundation \nmodels might be highly effective at learning generalized stack-\ning fault energies. MACE-FtS, which had been trained using \na very small training set, performs nearly as well as models \ndirectly trained using the relevant structures. In general, pre-\ndicting stacking-fault energies appears to be a task that is well \nsuited for foundation models.\nConclusion\nIn this work, we have evaluated several leading foundation \nmodels, along with multiple MACE variants, on a compre -\nhensive set of aluminum alloy benchmarks. In general, we \nfound that the grace2l \nmodel performed very \nwell on all our bench -\nmarks. The single excep-\ntion would be formation \nenergies of compounds, \nalthough some of this \ndifference could simply \nbe on account of the dif-\nference in DFT codes \nand settings. The MACE \nfoundation model trained \non the MPtraj, MACE-\nMPt, did not perform \nsufficiently well for \nmost applications. Some \nfoundation models, \neqV2 and mattersim \nperformed very badly on \nmost tests despite having \nmuch larger training sets \nand being much more \ncomputationally expen-\nsive than MACE-MPt. \nThe grace1l performed \nwell, but as well, as the \ngrace2l model, indicat-\ning that the nonlocal \nterms used in grace2l \ndo indeed boost per -\nformance. We find that \nthermal conductivity \naccuracy from Matbench \nDiscovery appears to be \na much better indica -\ntor of applied material \nperformance than the \nroot mean squared error \n(RMSE) on convex hull energies. The eqV2 model is best \nperforming for RMSE convex hull energy, but was one of the \nworst models in this study, while grace2l had the best thermal \nconductivity and the best overall results.\nFine-tuning MACE-MPt on the entire AlCuMgZn training \ndata did not result in a superior potential (MACE-FtL), although \nperformance was significantly improved. We did not find sig-\nnificant differences in quality by directly training the MACE \narchitecture on the AlCuMgZn data (MACE-DT) or using multi-\nhead fine-tuning (MACE-MH). Performance of the MACE-MPA \nfoundation model was often quite good. We were not able to \nfine-tune MACE-MPA using the multi-head configuration, nor \nis the grace2l model used here available for fine-tuning, but we \nexpect good performance in these cases. We believe that part of \nthe success of the grace2l for aluminum alloys was the use of the \nAlexandria training data, part of which was trained with a \nmuch higher k-point density. As mentioned the k-points used for \nFigure 5.  Edge dislocation dissociation into partials for MACE-MPt, MACE-FtL, AlcuMgZn-NNP1, and \ngrace2l. Green atoms are fcc, red are hcp, blue are bcc, and gray are others. An approximate scale of the \ndensity functional theory (DFT) partial dislocation separation distance is added to clarify performance. The \nrelevant structures are not included in the training data set. The grace2l model is the best performing likely \ndue to its accurate estimation of the stable stacking-fault energy for Al. acc, face-centered cubic; hcp, \nhexagonal close-packed; bcc, body-centered cubic.\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\n814        MRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin\nMPtraj and OMat24 are insufficient for aluminum and result \nin a mechanically unstable fcc phase. We see, for example, that \nMACE-MPA has very good performance, in most cases similar \nto grace2l, and we suspect this is due to the denser k-points in \nthe Alexandria training data.\nWe should emphasize that several improvements could be \nmade to our AlCuMgZn-NNPs. Porting the NNPs to a GPU \nor including stress terms in training (neither of which is pos -\nsible with the N2P2 library used here) would likely improve \ntheir speed and accuracy. Recent benchmarks indicate that a \nwell-crafted moment tensor potential (MPT) or ACE potential \nmay have better accuracy and/or faster performance. 25,37,38 \nAll of these models BP-NNP, MPT and ACE have poor scal -\ning with the number of elements. ACE is effectively limited to \nsimulations with six elements or  less39 and BP-NNPs also suffer \nfrom combinatorial scaling with the number of elements.40 The \nMACE input descriptors scale very efficiently with the number \nof elements, but that does necessarily mean that users should \nstart with a foundation model, such as MACE-MPt. It may be \nthe case that even for simulations involving large numbers of \nelements that better results could be obtained by carefully work-\ning with descriptor hyperparameters than fine-tuning a premade \nmodel. Recent work has also shown that one can analyze neural \nnetworks to see what aspects are important in their fitting thus \nguiding future bespoke and foundation models.41 For example, \nby developing functions that naturally follow the correct power \nlaw decay of interaction strength as a function of distance.\nFoundation models have shown substantial improve-\nment, but cannot totally replace “bespoke” potentials in all \ncases. The main problem with foundation models is that their \nFigure 6.  Generalized stacking-fault energies for key points for θ′′ (top) and θ (bottom) as computed by density functional \ntheory (DFT), AlCuMgZn-neural network potentials (NNPs), MACE-MPt, MACE-FtL, and grace2l. The large standard deviation \nat site index 4 for θ with the AlCuMgZn-NNPs is due to a single potential having a system collapse; this artifact can be avoided \nby adding a repulsive Lennard-Jones term to the potential. The relevant structures are not included in the training data set. The \ngrace2l model performs almost equally as well as AlCuMgZn-NNPs without direct training.\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\nMRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin              815\nlarge size means they need substantially more computational \nresources than custom potentials. Also, while we found that \nmodels such as grace2l do have broad accuracy they can still \nfail similar to any other potential. For example, grace2l would \nnot be ideal for dislocation-related work in Cu as it does not \noptimally predict the stable stacking-fault energy.\nFor developers of foundation models who are interested in \ntargeting metals this work offers the following recommenda-\ntions: (1) Training data should be performed with appropriate \nDFT settings that are often quite different from other materi-\nals, for example, high smearing and high k-point density. (2) \nTraining data should incorporate more material defects, for \nexample, stacking faults and solute–solute interactions. (3) \nBenchmarks should go beyond RMSE and target real material \nproperties such as those shown here.\nIt appears that foundation models either are, or soon will \nbe, ready for serious use in metallurgical atomic simulations. \nGiven recent developments, it appears that the greater focus on \nincluding more defects, and emphasizing benchmarks such as \nthermal conductivity are creating much higher-quality poten-\ntials than were possible as little as a year ago. That said, they \nremain massively more expensive than standard potentials and \nthey are not yet accurate enough to bypass DFT entirely. They \ndo seem poised as being complimentary tools within the field. \nFor example, as an initial screening step in a high-throughput \npipeline searching for novel materials across many elements, so \nlong as results are confirmed later with DFT. Their ease of fine-\ntuning over limited data sets could make them useful as quick \ntrial tools, again only if it is understood these models frequently \nfail and if promising results are later confirmed with DFT.\nMethods\nThe AlCuMgZn-NNP potentials, their training data, and the \ntesting framework used to evaluate the potentials have been \ndescribed in prior work.21,23,32 Here, we provide a short outline \nof the methodology relevant to this study.\nThe training data set used in this study was taken from a \npublicly available  database 42 stored on Materials  Cloud43 \nconsisting of 10119 structures spanning the Al–Cu–Mg–Zn \nalloy system. The data set includes a comprehensive set of \nstored material properties (e.g., solute-stacking-fault energies, \nmisfit volume, lattice constants, elastic constants, and solute \ndefect [antisite] energies). This database was computed in a \nprior study using Quantum  ESPRESSO44 via AiiDA.45–47 The \ninput structures were either created directly using the  ASE48 \nand  pymatgen49 packages or imported from the Open Quan-\ntum Materials Database (OQMD)50 and transformed (e.g., via \nelastic deformations, small atomic displacements, and elemen-\ntal substitutions).\nThe AlCuMgZn-NNPs used are those described in Ref-\nerence 32 and are of the Behler–Parrinello  formulation 51 as \nimplemented by N2P2 package.52,53 These AlCuMgZn-NNPs \nconsist of two hidden layers and 24 nodes per layer and \n96 symmetry functions (SFs) per element plus three hand-\nchosen SFs for aluminum, for a total of 389 SFs. With this \narchitecture, each NNP has 9984 learnable parameters. In this \nwork, we show the average and the standard deviation across \n20 AlCuMgZn-NNPs.\nHere, we describe the MACE variants used in this study; all \ntraining scripts are included in the Supplementary materials. \nMACE-MPt is the “out-of-the-box” version trained only on \nMPtraj and no further training or modification. MACE-FtL \nwas MACE-MPt fine-tuned over exactly the same data used \nto create the AlCuMgZn-NNPs. The goal of MACE-FtL was \nto test if fine-tuning could result in greater performance than \nwhat would be possible using AlCuMgZn-NNPs. MACE-FtS \nwas fine-tuned on a small subset of the AlCuMgZn-NNP data \nconsisting of fcc 2 × 2 × 2 cubic supercells with Al, Cu, Mg, \nZn, and 0–5 vacancies, randomly distributed over 32 atomic \nlattice sites. The goal of MACE-FtS was to evaluate how well \nfoundation models are able to perform using very limited data. \nMACE-DT was the MACE architecture directly trained on the \nAlCuMgZn-NNP data. MACE-MH was similar to MACE-FtL \nexcept it used the “multi-head” training procedure. Fine-tuning \nfor both MACE-FtL and MACE-FtS was conducted using a \nlearning rate of 0.001 over a period of 225 epochs with a force \nweighting of 1 and an energy weighting of 1. No stresses were \nused in the fine-tuning. We briefly examined alternate training \nprocedures, such as increasing the number of training epochs \nto 700 and increasing the learning rate to 0.01, but did not \nfind a substantially different RMSE for forces or energies. The \ndirectly trained and multi-head fine-tuned versions were run \nfor 200 epochs (see the training scripts in the Supplementary \ninformation for complete details). MACE-FtL had an RMSE \nof 12.4 meV/atom for energies and 24.0 meV/Å for forces. \nThis is a similar RMSE for energies as the average RMSE \nacross all 20 AlCuMgZn-NNPs, 11.0 meV/atom, and half the \nforce RMSE average across all 20 AlCuMgZn-NNPs, 41.3 \nmeV/Å. For the directly trained model, we had a RMSE of \n17.7 meV/atom and 27.2 meV/Å for the energies and forces. \nFor the multi-head, we obtained a result of 22.1 meV/atom \nand 32.1 meV/Å. For the PACE model, we obtained a RMSE \nof 12.5 meV/atom for energies and 33.54 meV/Å for forces.\nThe other foundation models were ones that scored high \non the Matbench Discovery leaderboard. We used the GRA-\nCE1l-OAM, MatterSim v1 5 M, MACE-MPA-0, and \neqV2_86M_omat_mp_salex foundation models.\nIt is somewhat challenging to directly compare computa-\ntional performance between the AlCuMgZn-NNPs and the \nfoundation models. The N2P2 library used only supports CPU \ninference, while all the tested foundation models require a \nGPU for reasonable compute times. It is also critical that speed \nbenchmarks be conducted via LAMMPS and not with the ASE \ncalculator as the ASE calculator involves substantial overhead. \nWe decided to focus on the grace series of models because gra-\nce2l was the best-performing model in this study; however, we \nestimate that the other foundation models should follow simi-\nlar trends. We also emphasize that performance can depend \non many subtle details on compilation and implementation on \nsource hardware. Because the BP-NNPs cannot be run on a \nFOUndatiOn MOdELs FOr MEtaLLUrgY?\n816        MRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin\nGPU, we used a PACE model fitted on the AlCuMgZn-NNPs to \nestimate the performance of a bespoke ACE model. The PACE \nmodel had an error of 12.56 meV/atom and 33.54 meV/Å for \nenergies/forces; its evaluation could be seen in Supplementary \n4. To test the speed of these models we created large super -\ncells of Al with randomly substituted Mg, Zn, and Cu solutes. \nThe structures were created in advance and the same structures \nwere used for all models to ensure equal conditions. The PACE \nmodel was run with  LAMMPS54 the  kokkos55 backend, while \nall other models used a  TensorFlow56 backend. The results of \nthese benchmarks can be seen in Figure 7. We see that all mod-\nels offer good linear scaling with the number of atoms. The \ngrace2l can be used effectively on this hardware up to 10,000 \natoms, the grace1l model is  5× faster and can be used up to \n100,000 atoms; finally the PACE model is 10× faster than the \ngrace1l and 50× faster than the grace2l model and can be used \nup to 1,000,000 atoms.\nAcknowledgments \nThe author thanks  I.  Svenum for organizing the project, \nT. Haugland for help in running the MACE code, and O. Løv-\nvik for useful discussions.  M. Liyanage provided input scripts \nand guidance for running the dislocation section. The author \nwould also like to thank Prof. Curtin for very helpful feedback \nand for encouraging \nthe publication of this \nwork. D.M. acknowl-\nedges funding form \nthe UNIPOT project at \nSINTEF.\nFunding \nOpen access funding pro-\nvided by SINTEF. This \nwork received funding \nsupport from the UNI-\nPOT project at SINTEF. \nCode and data \navailability \nThe code and data used \nin this study can be found \non Zenodo.57 It is not in a \nproduction state, but inter-\nested readers are highly \nencouraged to contact the \nauthor. The benchmark \ncode should in practice \nbe able to work with any \natomic model that has \nan ASE interface via the \nASE.calculator. 48 The \nAlCuMgZn-NNPs and the \nDFT data could be found \non Materials Cloud.42\nConflict of interest \nThe author has no conflict of interest in relation to this work.\nSupplementary information\nThe online version contains supplementary material available \nat https:// doi. org/ 10. 1557/ s43577- 025- 00911-0\nOpen Access\nThis article is licensed under a Creative Commons Attribu -\ntion 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images \nor other third party material in this article are included in the \narticle’s Creative Commons licence, unless indicated other -\nwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, \nvisit http://creativecommons.org/licenses/by/4.0/.\nFigure 7.  Speed benchmark for the grace2l and grace1l foundation models and a “bespoke” AlCuMgZn-\nperformant atomic cluster expansion (PACE) potential. Compute time is the average over 10 molecular \ndynamics (MD) timesteps. The PACE potential is not production quality, but should serve as a rough \nestimate of compute cost for a “bespoke” atomic cluster expansion potential. Runs were conducted on a \nsingle NVIDIA A100 80GB graphics processing unit.\nFOUndatiOn MOdELs FOr MEtaLLUrgY?\nMRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin              817\nReferences\n1. C. Chen, S.P . Ong, Nat. Comput. Sci. 2, 718 (2022). https:// doi. org/ 10. 1038/ \ns43588- 022- 00349-3\n2. B. Deng, P . Zhong, K. Jun, J. Riebesell, K. Han, C.J. Bartel, G. Ceder, Nat. Mach. \nIntell. 5, 1031 (2023). https:// doi. org/ 10. 1038/ s42256- 023- 00716-3\n3. Y. Park, J. Kim, S. Hwang, S. Han, J. Chem. Theory Comput.  20, 4857 (2024). \nhttps:// doi. org/ 10. 1021/ acs. jctc. 4c001 90\n4. I. Batatia, P . Benner, Y. Chiang, A.M. Elena, D.P . Kovács, J. Riebesell, X.R. Advincula, \nM. Asta, M. Avaylon, W.J. Baldwin, F. Berger, N. Bernstein, A. Bhowmik, S.M. Blau, V. \nCărare, J.P . Darby, S. De, F. Della Pia, V.L. Deringer, R. Elijošius, Z. El-Machachi, F. \nFalcioni, E. Fako, A.C. Ferrari, A. Genreith-Schriever, J. George, R.E.A. Goodall, C.P . \nGrey, P . Grigorev, S. Han, W. Handley, H.H. Heenen, K. Hermansson, C. Holm, J. Jaafar, \nS. Hofmann, K.S. Jakob, H. Jung, V. Kapil, A.D. Kaplan, N. Karimitari, J.R. Kermode, N. \nKroupa, J. Kullgren, M.C. Kuner, D. Kuryla, G. Liepuoniute, J.T. Margraf, I.-B. Magdău, \nA. Michaelides, J.H. Moore, A.A. Naik, S.P . Niblett, S.W. Norwood, N. O’Neill, C. Ortner, \nK.A. Persson, K. Reuter, A.S. Rosen, L.L. Schaaf, C. Schran, B.X. Shi, E. Sivonxay, \nT.K. Stenczel, V. Svahn, C. Sutton, T.D. Swinburne, J. Tilly, C. van der Oord, E. Varga \nUmbrich, T. Vegge, M. Vondrák, Y. Wang, W.C. Witt, F. Zills, G. Csányi, A foundation \nmodel for atomistic materials chemistry, Preprint (2023). http:// arxiv. org/ abs/ 2401. \n00096\n5. A. Merchant, S. Batzner, S.S. Schoenholz, M. Aykol, G. Cheon, E.D. Cubuk, Nature \n624, 80 (2023). https:// doi. org/ 10. 1038/ s41586- 023- 06735-9\n6. H. Yang, C. Hu, Y. Zhou, X. Liu, Y. Shi, J. Li, G. Li, Z. Chen, S. Chen, C. Zeni, M. Horton, \nR. Pinsler, A. Fowler, D. Zügner, T. Xie, J. Smith, L. Sun, Q. Wang, L. Kong, C. Liu, H. Hao, \nZ. Lu, MatterSim: A deep learning atomistic model across elements, temperatures and \npressures, Preprint (2024). http:// arxiv. org/ abs/ 2405. 04967\n7. L. Barroso-Luque, M. Shuaibi, X. Fu, B. M. Wood, M. Dzamba, M. Gao, A. Rizvi, C. L. \nZitnick, Z. W. Ulissi, Open materials 2024 (OMat24) inorganic materials dataset and \nmodels, Preprint (2024). http:// arxiv. org/ abs/ 2410. 12771\n8. A. Bochkarev, Y. Lysogorskiy, R. Drautz, Phys. Rev. X 14, 021036 (2024)\n9. B. Deng, Materials Project Trajectory (MPtrj) Dataset (2023). https:// doi.  org/ 10. \n6084/ m9. figsh are. 23713 842. v2\n10. A. Jain, S.P . Ong, G. Hautier, W. Chen, W.D. Richards, S. Dacek, S. Cholia, D. Gunter, \nD. Skinner, G. Ceder, K.A. Persson,  APL Mater. 1, 011002 (2013). https:// doi. org/ 10. \n1063/1. 48123 23\n11. J. Schmidt, T.F.T. Cerqueira, A.H. Romero, A. Loew, F. Jäger, H.-C. Wang, S. \nBotti, M.A.L. Marques, Mater. Today Phys.  48, 101560 (2024). Accessed 22 Feb \n2025. https:// doi. org/ 10. 1016/j. mtphys. 2024. 101560\n12. B. Deng, Y. Choi, P . Zhong, J. Riebesell, S. Anand, Z. Li, K. Jun, K.A. Persson, G. \nCeder, Overcoming systematic softening in universal machine learning interatomic \npotentials by fine-tuning, Preprint (2024). http:// arxiv. org/ abs/ 2405. 07105\n13. B. Focassio, M. Freitas, L.P ., G.R. Schleder, ACS Appl. Mater. Interfaces 17(9), \n13111 (2024). https:// doi. org/ 10. 1021/ acsami. 4c038 15\n14. H. Lee, V.I. Hegde, C. Wolverton, Y. Xia, Accelerating high-throughput phonon \ncalculations via machine learning universal potentials, Preprint (2024). http:// arxiv. \norg/ abs/ 2407. 09674\n15. B. Póta, P . Ahlawat, G. Csányi, M. Simoncelli, Thermal conductivity predictions \nwith foundation atomistic models, Preprint (2024). http:// arxiv. org/ abs/ 2408. 00755\n16. H. Kaur, F. Della Pia, I. Batatia, X.R. Advincula, B.X. Shi, J. Lan, G. Csányi, A. \nMichaelides, V. Kapil, Data-efficient fine-tuning of foundational models for first-\nprinciples quality sublimation enthalpies, Preprint (2024). http:// arXiv. org/ abs/  \n2405. 20217. Accessed 7 Sept 2024\n17. J. Riebesell, R.E.A. Goodall, P . Benner, Y. Chiang, B. Deng, A.A. Lee, A. Jain, K.A. Pers-\nson, Matbench Discovery—A framework to evaluate machine learning crystal stability \npredictions, Preprint (2024). http:// arxiv. org/ abs/ 2308. 14920. Accessed 8 Sept 2024\n18. A. Kabylda, B. Mortazavi, X. Zhuang, A. Tkatchenko, Adv. Funct. Mater. 35 \n(13), 2417891 (2024). https:// doi. org/ 10. 1002/ adfm. 20241 7891\n19. S. Zhu, D. Saritürk, R. Arróyave,  Acta Mater. 286, 120747 (2025). https:// doi. \norg/ 10. 1016/j. actam at. 2025. 120747\n20. L. Wang, T. He, B. Ouyang, The impact of domain knowledge on universal \nmachine learning models, Preprint (2024). https:// doi. org/ 10. 26434/ chemr xiv- \n2024- fmq8p. Accessed 7 Sept 2024\n21. D. Marchand, A. Jain, A. Glensk, W.A. Curtin, Phys. Rev. Mater. 4, 103601 \n(2020). https:// doi. org/ 10. 1103/ PhysR evMat erials. 4. 103601\n22. A.C.P . Jain, D. Marchand, A. Glensk, M. Ceriotti, W.A. Curtin, Phys. Rev. Mater. \n5(5), 053805 (2021). https:// doi. org/ 10. 1103/ PhysR evMat erials. 5. 053805\n23. D.J.G. Marchand, “Neural Network Potentials for Age Hardening Aluminum \nAlloys,” PhD thesis, École Polytechnique Fédérale de Lausanne (EPFL) (2022). \nhttps:// doi. org/ 10. 5075/ epfl- thesis- 9535\n24. A. Bochkarev, Y. Lysogorskiy, S. Menon, M. Qamar, M. Mrovec, R. Drautz, Phys. \nRev. Mater. 6(1), 013804 (2022)\n25. Y. Lysogorskiy, C.V.D. Oord, A. Bochkarev, S. Menon, M. Rinaldi, T. Hammer -\nschmidt, M. Mrovec, A. Thompson, G. Csányi, C. Ortner, R. Drautz,  Comput. Mater. \n7(1), 97 (2021). https:// doi. org/ 10. 1038/ s41524- 021- 00559-9\n26. R. Drautz, Phys. Rev. B 99(1), 014104 (2019)\n27. M.S. Nitol, D.E. Dickel, C.D. Barrett, Comput. Mater. Sci.  188, 110207 (2021). \nhttps:// doi. org/ 10. 1016/j. comma tsci. 2020. 110207\n28. H. Mei, L. Cheng, L. Chen, F. Wang, J. Li, L. Kong, Comput. Mater. Sci. 233, \n112723 (2024). https:// doi. org/ 10. 1016/j. comma tsci. 2023. 112723\n29. C.J. Owen, A.D. Naghdi, A. Johansson, D. Massa, S. Papanikolaou, B. Kozinsky, \nUnbiased atomistic predictions of crystal dislocation dynamics using Bayesian force \nfields, Preprint (2024). http:// arxiv. org/ abs/ 2401. 04359. Accessed 7 Sept 2024\n30. E. Ibrahim, Y. Lysogorskiy, M. Mrovec, R. Drautz, Phys. Rev. Mater. 7(11), 113801 \n(2023)\n31. L. Stemper, M.A. Tunes, P . Dumitraschkewitz, F. Mendez-Martin, R. Tosone, D. \nMarchand, W.A. Curtin, P .J. Uggowitzer, S. Pogatscher, Acta Mater. 206, 116617 \n(2021). https:// doi. org/ 10. 1016/j. actam at. 2020. 116617\n32. D. Marchand, W.A. Curtin, Phys. Rev. Mater. 6(5), 053803 (2022)\n33. D. Hull, D.J. Bacon, “Dislocations in Face-Centered Cubic Metals,” in Introduction \nto Dislocations, 5th edn., ed. by D. Hull, D.J. Bacon  (Butterworth-Heinemann, Oxford, \n2011), chap. 5, pp. 85–107. https:// doi. org/ 10. 1016/ B978-0- 08- 096672- 4. 00005-0\n34. L.M. Hale, Z.T. Trautt, C.A. Becker, Model. Simul. Mater. Sci. Eng. 26(5), 055003 \n(2018). https:// doi. org/ 10. 1088/ 1361- 651X/ aabc05\n35. C.A. Becker, F. Tavazza, Z.T. Trautt, R.A. Buarque de Macedo, Curr. Opin. Solid \nState Mater. Sci. 17(6), 277 (2013). https:// doi. org/ 10. 1016/j. cossms. 2013. 10. 001\n36. C. Woodward, D. Trinkle, L. Hector Jr., D. Olmsted, Phys. Rev. Lett.  100(4), \n045507 (2008)\n37. Y. Zuo, C. Chen, X. Li, Z. Deng, Y. Chen, J. Behler, G. Csányi, A.V. Shapeev, A.P . \nThompson, M.A. Wood, S.P . Ong,  J. Phys. Chem. A 124(4), 731 (2020), https:// doi. \norg/ 10. 1021/ acs. jpca. 9b087 23. arXiv: 1906. 08888. Accessed 10 Oct 2024\n38. L. Zhang, G. Csányi, E. van der Giessen, F. Maresca,  Acta Mater. 270, 119788 \n(2024). https:// doi. org/ 10. 1016/j. actam at. 2024. 119788\n39. J.P . Darby, D.P . Kovács, I. Batatia, M.A. Caro, G.L.W. Hart, C. Ortner, G. Csányi, Phys. \nRev. Lett.131(2), 028001 (2023). https:// doi. org/ 10. 1103/ PhysR evLett. 131. 028001\n40. E. Kocer, T.W. Ko, J. Behler, Neural network potentials: A concise overview of \nmethods, Preprint (2021). http:// arxiv. org/ abs/ 2107. 03727. Accessed 11 Oct 2024\n41. M. Esders, T. Schnake, J. Lederer, A. Kabylda, G. Montavon, A. Tkatchenko, K.-R. Muller, \nJ. Chem. Theory Comput. 21(2), 714 (2025). https:// doi. org/ 10. 1021/ acs. jctc. 4c014 24\n42. D. Marchand, W.A. Curtin, Machine learning for metallurgy: A neural network \npotential for Al-Cu-Mg, Materials Cloud Archive 2021.106 (2021).  https:// doi. org/ \n10. 24435/ mater ialsc loud: ea- y9\n43. L. Talirz, S. Kumbhar, E. Passaro, A.V. Yakutovich, V. Granata, F. Gargiulo, M. Borelli, \nM. Uhrin, S.P . Huber, S. Zoupanos, C.S. Adorf, C.W. Andersen, O. Schütt, C.A. Pignedoli, \nD. Passerone, J. VandeVondele, T.C. Schulthess, B. Smit, G. Pizzi, N. Marzari,  Sci. Data \n7(1), 299 (2020). https:// doi. org/ 10. 1038/ s41597- 020- 00637-5\n44. P . Giannozzi, S. Baroni, N. Bonini, M. Calandra, R. Car, C. Cavazzoni, D. Ceresoli, G.L. \nChiarotti, M. Cococcioni, I. Dabo, A. Dal Corso, S. De Gironcoli, S. Fabris, G. Fratesi, R. \nGebauer, U. Gerstmann, C. Gougoussis, A. Kokalj, M. Lazzeri, L. Martin-Samos, N. Marzari, \nF. Mauri, R. Mazzarello, S. Paolini, A. Pasquarello, L. Paulatto, C. Sbraccia, S. Scandolo, G. \nSclauzero, A.P . Seitsonen, A. Smogunov, P . Umari, R.M. Wentzcovitch, J. Phys. Condens. \nMatter. 21(39), 395502 (2009). https:// doi. org/ 10. 1088/ 0953- 8984/ 21/ 39/ 395502\n45. S.P . Huber, S. Zoupanos, M. Uhrin, L. Talirz, L. Kahle, R. Häuselmann, D. Gresch, T. \nMüller, A.V. Yakutovich, C.W. Andersen, F.F. Ramirez, C.S. Adorf, F. Gargiulo, S. Kumbhar, \nE. Passaro, C. Johnston, A. Merkys, A. Cepellotti, N. Mounet, N. Marzari, B. Kozinsky, \nG. Pizzi, Sci. Data 7(1), 300 (2020). https:// doi. org/ 10. 1038/ s41597- 020- 00638-4\n46. M. Uhrin, S.P . Huber, J. Yu, N. Marzari, G. Pizzi, Comput. Mater. Sci. 187, 110086 \n(2020). https:// doi. org/ 10. 1016/j. comma tsci. 2020. 110086\n47. G. Pizzi, A. Cepellotti, R. Sabatini, N. Marzari, B. Kozinsky, Comput. Mater. Sci. \n111, 218 (2016). https:// doi. org/ 10. 1016/j. comma tsci. 2015. 09. 013\n48. A. Hjorth Larsen, J. Jørgen Mortensen, J. Blomqvist, I.E. Castelli, R. Christensen, \nM. Dulak, J. Friis, M.N. Groves, B. Hammer, C. Hargus, E.D. Hermes, P .C. Jennings, P . \nBjerre Jensen, J. Kermode, J.R. Kitchin, E. Leonhard Kolsbjerg, J. Kubal, K. Kaasbjerg, \nS. Lysgaard, J. Bergmann Maronsson, T. Maxson, T. Olsen, L. Pastewka, A. Peterson, \nC. Rostgaard, J. Schiøtz, O. Schütt, M. Strange, K.S. Thygesen, T. Vegge, L. Vilhelmsen, \nM. Walter, Z. Zeng, K.W. Jacobsen, J. Phys. Condens. Matter 29(27), 273002 (2017).  \nhttps:// doi. org/ 10. 1088/ 1361- 648X/ aa680e. Accessed 25 Apr 2018\n49. S.P . Ong, W.D. Richards, A. Jain, G. Hautier, M. Kocher, S. Cholia, D. Gunter, V.L. \nChevrier, K.A. Persson, G. Ceder, Comput. Mater Sci. 68, 314 (2013). https:// doi. org/ \n10. 1016/J. COMMA TSCI. 2012. 10. 028\n50. J.E. Saal, S. Kirklin, M. Aykol, B. Meredig, C. Wolverton, JOM 65(11), 1501 (2013). \nhttps:// doi. org/ 10. 1007/ s11837- 013- 0755-4\n51. J. Behler, M. Parrinello, Phys. Rev. Lett. 98, 146401 (2007). https:// doi. org/ 10. \n1103/ PhysR evLett. 98. 146401\n52. A. Singraber, J. Behler, C. Dellago, J. Chem. Theory Comput. 15(3), 1827 (2019). \nhttps:// doi. org/ 10. 1021/ acs. jctc. 8b007 70\n53. A. Singraber, T. Morawietz, J. Behler, C. Dellago, J. Chem. Theory Comput. 15(5), \n3075 (2019). https:// doi. org/ 10. 1021/ acs. jctc. 8b010 92\n54. A.P . Thompson, H.M. Aktulga, R. Berger, D.S. Bolintineanu, W.M. Brown, P .S. \nCrozier, P .J. Veld, A. Kohlmeyer, S.G. Moore, T.D. Nguyen, R. Shan, M.J. Stevens, J. \nTranchida, C. Trott, S.J. Plimpton, Comput. Phys. Commun. 271, 108171 (2022). \nhttps:// doi. org/ 10. 1016/j. cpc. 2021. 108171\n55. C.R. Trott, D. Lebrun-Grandié, D. Arndt, J. Ciesko, V. Dang, N. Ellingwood, R. Gay-\natri, E. Harvey, D.S. Hollman, D. Ibanez, N. Liber, J. Madsen, J. Miles, D. Poliakoff, A. \nPowell, S. Rajamanickam, M. Simberg, D. Sunderland, B. Turcksin, J. Wilke. IEEE \nFOUndatiOn MOdELs FOr MEtaLLUrgY?\n818        MRS BULLETIN • VOLUME 50 • JULY 2025 • mrs.org/bulletin\nTrans. Parallel Distrib. Syst. 33(4), 805 (2022). https:// doi. org/ 10. 1109/ TPDS. 2021. \n30972 83\n56. M. Abadi, A. Agarwal, P . Barham, E. Brevdo, Z. Chen, C. Citro, G.S. Corrado, \nA. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. \nIsard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, \nS. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. \nTalwar, P . Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P . Warden, M. \nWattenberg, M. Wicke, Y. Yu, X. Zheng, TensorFlow: Large-Scale Machine Learning \non Heterogeneous Systems. Software available from tensorflow.org (2015). https:// \nwww. tenso rflow. org\n57. D. Marchand, “Supplementary Data and Code for ‘Comprehensive Evaluation of \nMACE on Aluminium Alloys’” Zenodo (October 29, 2024)\b⃞\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.",
  "topic": "Bespoke",
  "concepts": [
    {
      "name": "Bespoke",
      "score": 0.8326339721679688
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.7082504034042358
    },
    {
      "name": "Statement (logic)",
      "score": 0.6343741416931152
    },
    {
      "name": "Computer science",
      "score": 0.5936980843544006
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.516339898109436
    },
    {
      "name": "Point (geometry)",
      "score": 0.4962983727455139
    },
    {
      "name": "Industrial engineering",
      "score": 0.3334296941757202
    },
    {
      "name": "Programming language",
      "score": 0.18123093247413635
    },
    {
      "name": "Engineering",
      "score": 0.16060245037078857
    },
    {
      "name": "Mathematics",
      "score": 0.10541251301765442
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}