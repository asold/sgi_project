{
  "title": "Low Rank Fusion based Transformers for Multimodal Sequences",
  "url": "https://openalex.org/W3039263585",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3188379679",
      "name": "Sahay, Saurav",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3189468127",
      "name": "Okur, Eda",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4288672032",
      "name": "Kumar, Shachi H",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3191891187",
      "name": "Nachman, Lama",
      "affiliations": [
        "Intel (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2165857685",
    "https://openalex.org/W1790719788",
    "https://openalex.org/W2964010806",
    "https://openalex.org/W2964346351",
    "https://openalex.org/W2556418146",
    "https://openalex.org/W1809872410",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970972665",
    "https://openalex.org/W2963063161",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2883409523",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3131553854"
  ],
  "abstract": "Our senses individually work in a coordinated fashion to express our\\nemotional intentions. In this work, we experiment with modeling\\nmodality-specific sensory signals to attend to our latent multimodal emotional\\nintentions and vice versa expressed via low-rank multimodal fusion and\\nmultimodal transformers. The low-rank factorization of multimodal fusion\\namongst the modalities helps represent approximate multiplicative latent signal\\ninteractions. Motivated by the work of~\\\\cite{tsai2019MULT} and~\\\\cite{Liu_2018},\\nwe present our transformer-based cross-fusion architecture without any\\nover-parameterization of the model. The low-rank fusion helps represent the\\nlatent signal interactions while the modality-specific attention helps focus on\\nrelevant parts of the signal. We present two methods for the Multimodal\\nSentiment and Emotion Recognition results on CMU-MOSEI, CMU-MOSI, and IEMOCAP\\ndatasets and show that our models have lesser parameters, train faster and\\nperform comparably to many larger fusion-based architectures.\\n",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 29–34\nSeattle, USA, July5 - 10, 2020.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n29\nLow Rank Fusion based Transformers for Multimodal Sequences\nSaurav Sahay Eda Okur Shachi H Kumar Lama Nachman\nIntel Labs, Anticipatory Computing Lab, USA\n{saurav.sahay, eda.okur, shachi.h.kumar, lama.nachman}\n@intel.com\nAbstract\nOur senses individually work in a coordi-\nnated fashion to express our emotional inten-\ntions. In this work, we experiment with mod-\neling modality-speciﬁc sensory signals to at-\ntend to our latent multimodal emotional in-\ntentions and vice versa expressed via low-\nrank multimodal fusion and multimodal trans-\nformers. The low-rank factorization of multi-\nmodal fusion amongst the modalities helps rep-\nresent approximate multiplicative latent signal\ninteractions. Motivated by the work of (Tsai\net al., 2019) and (Liu et al., 2018), we present\nour transformer-based cross-fusion architec-\nture without any over-parameterization of the\nmodel. The low-rank fusion helps repre-\nsent the latent signal interactions while the\nmodality-speciﬁc attention helps focus on rel-\nevant parts of the signal. We present two meth-\nods for the Multimodal Sentiment and Emo-\ntion Recognition results on CMU-MOSEI,\nCMU-MOSI, and IEMOCAP datasets and\nshow that our models have lesser parameters,\ntrain faster and perform comparably to many\nlarger fusion-based architectures.\n1 Introduction\nThe ﬁeld of Emotion Understanding involves com-\nputational study of subjective elements such as sen-\ntiments, opinions, attitudes, and emotions towards\nother objects or persons. Subjectivity is an inher-\nent part of emotion understanding that comes from\nthe contextual nature of the natural phenomenon.\nDeﬁning the metrics and disentangling the objec-\ntive assessment of the metrics from the subjective\nsignal makes the ﬁeld quite challenging and excit-\ning. Sentiments and Emotions are attached to the\nlanguage, audio and visual modalities at different\nrates of expression and granularity and are use-\nful in deriving social, psychological and behavioral\ninsights about various entities such as movies, prod-\nucts, people or organizations. Emotions are deﬁned\nas brief organically synchronized evaluations of\nmajor events whereas sentiments are considered as\nmore enduring beliefs and dispositions towards ob-\njects or persons (Scherer, 1984). The ﬁeld of Emo-\ntion Understanding has rich literature with many in-\nteresting models of understanding (Plutchik, 2001;\nEkman, 2009; Posner et al., 2005). Recent studies\non tensor-based multimodal fusion explore regu-\nlarizing tensor representations (Liang et al., 2019)\nand polynomial tensor pooling (Hou et al., 2019).\nIn this work, we combine ideas from (Tsai et al.,\n2019) and (Liu et al., 2018) and explore the use\nof Transformer (Vaswani et al., 2017) based mod-\nels for both aligned and unaligned signals with-\nout extensive over-parameterization of the models\nby using multiple modality-speciﬁc transformers.\nWe utilize Low Rank Matrix Factorization (LMF)\nbased fusion method for representing multimodal\nfusion of the modality-speciﬁc information. Our\nmain contributions can be summarized as follows:\n•Recently proposed Multimodal Transformer\n(MulT) architecture (Tsai et al., 2019) uses at\nleast 9 Transformer based models for cross-\nmodal representation of language, audio and\nvisual modalities (3 parallel modality-speciﬁc\nstandard Transformers with self-attention and\n6 parallel bimodal Transformers with cross-\nmodal attention). These models utilize several\nparallel unimodal and bimodal transformers\nand do not capture the full trimodal signal\ninterplay in any single transformer model in\nthe architecture. In contrast, our method uses\nfewer Transformer based models and fewer\nparallel models for the same multimodal rep-\nresentation.\n•We look at two methods for leveraging the\nmultimodal fusion into the transformer ar-\nchitecture. In one method (LMF-MulT), the\nfused multimodal signal is reinforced using\n30\nFigure 1: Modality-speciﬁc Fused Attention\nattention from the 3 modalities. In the other\nmethod (Fusion-Based-CM-Attn), the individ-\nual modalities are reinforced in parallel via\nthe fused signal.\nThe ability to use unaligned sequences for mod-\neling is advantageous since we rely on learning\nbased methods instead of using methods that force\nthe signal synchronization (requiring extra timing\ninformation) to mimic the coordinated nature of\nhuman multimodal language expression. The LMF\nmethod aims to capture all unimodal, bimodal and\ntrimodal interactions amongst the modalities via\napproximate Tensor Fusion method.\nWe develop and test our approaches on the CMU-\nMOSI, CMU-MOSEI, and IEMOCAP datasets\nas reported in (Tsai et al., 2019). CMU Multi-\nmodal Opinion Sentiment and Emotion Intensity\n(CMU-MOSEI) (Zadeh et al., 2018) is a large\ndataset of multimodal sentiment analysis and emo-\ntion recognition on YouTube video segments. The\ndataset contains more than 23,500 sentence utter-\nance videos from more than 1000 online YouTube\nspeakers. The dataset has several interesting prop-\nerties such as being gender balanced, containing\nvarious topics and monologue videos from peo-\nple with different personality traits. The videos\nare manually transcribed and properly punctuated.\nSince the dataset comprises of natural audio-visual\nopinionated expressions of the speakers, it provides\nan excellent test-bed for research in emotion and\nsentiment understanding. The videos are cut into\ncontinuous segments and the segments are anno-\ntated with 7 point scale sentiment labels and 4\npoint scale emotion categories corresponding to the\nEkman’s 6 basic emotion classes (Ekman, 2002).\nThe opinionated expressions in the segments con-\ntain visual cues, audio variations in signal as well\nas textual expressions showing various subtle and\nnon-obvious interactions across the modalities for\nboth sentiment and emotion classiﬁcation. CMU-\nMOSI (Zadeh et al., 2016) is a smaller dataset\n(2199 clips) of YouTube videos with sentiment an-\nnotations. IEMOCAP (Busso et al., 2008) dataset\nconsists of 10K videos with sentiment and emotion\nlabels. We use the same setup as (Tsai et al., 2019)\nwith 4 emotions (happy, sad, angry, neutral).\nIn Fig 1, we illustrate our ideas by showing the\nfused signal representation attending to different\nparts of the unimodal sequences. There’s no need\nto align the signals since the attention computation\nto different parts of the modalities acts as proxy\nto the multimodal sequence alignment. The fused\nsignal is computed via Low Rank Matrix Factor-\nization (LMF). The other model we propose uses a\nswapped conﬁguration where the individual modal-\nities attend to the fused signal in parallel.\n2 Model Description\nIn this section, we describe our models and meth-\nods for Low Rank Fusion of the modalities for use\nwith Multimodal Transformers with cross-modal\nattention.\n2.1 Low Rank Fusion\nLMF is a Tensor Fusion method that models the uni-\nmodal, bimodal and trimodal interactions without\nusing an expensive 3-fold Cartesian product (Zadeh\net al., 2017) from modality-speciﬁc embeddings.\n31\nFigure 2: Low Rank Matrix Factorization\nInstead, the method leverages unimodal features\nand weights directly to approximate the full multi-\ntensor outer product operation. This low-rank ma-\ntrix factorization operation easily extends to prob-\nlems where the interaction space (feature space or\nnumber of modalities) is very large. We utilize the\nmethod as described in (Liu et al., 2018). Simi-\nlar to the prior work, we compress the time-series\ninformation of the individual modalities using an\nLSTM (Hochreiter and Schmidhuber, 1997) and\nextract the hidden state context vector for modality-\nspeciﬁc fusion. We depict the LMF method in Fig 2\nsimilar to the illustration in (Liu et al., 2018). This\nshows how the unimodal tensor sequences are ap-\npended with 1s before taking the outer product to\nFigure 3: Fused Cross-modal Transformer\nbe equivalent to the tensor representation that cap-\ntures the unimodal and multimodal interaction in-\nformation explicitly (top right of Fig 2). As shown,\nthe compressed representation (h) is computed us-\ning batch matrix multiplications of the low-rank\nmodality-speciﬁc factors and the appended modal-\nity representations. All the low-rank products are\nfurther multiplied together to get the fused vector.\n2.2 Multimodal Transformer\nWe build up on the Transformers (Vaswani et al.,\n2017) based sequence encoding and utilize the\nideas from (Tsai et al., 2019) for multiple cross-\nmodal attention blocks followed by self-attention\nfor encoding multimodal sequences for classiﬁ-\nFigure 4: Low Rank Fusion Transformer\n32\nMetric Acch\n7 Acch\n2 F1h MAEl Corrh\n(Aligned) CMU-MOSI Sentiment\nLF-LSTM (pub) 35.3 76.8 76.7 1.015 0.625\nMulT (Tsai et al., 2019) (pub) 40.0 83.0 82.8 0.871 0.698\nMulT (Tsai et al., 2019) (our run) 33.1 78.5 78.4 0.991 0.676\nFusion-Based-CM-Attn-MulT (ours) 32.9 77.0 76.9 1.017 0.636\nLMF-MulT (ours) 32.4 77.9 77.9 1.016 0.647\n(Unaligned) CMU-MOSI Sentiment\nLF-LSTM (pub) 33.7 77.6 77.8 0.988 0.624\nMulT (Tsai et al., 2019) (pub) 39.1 81.1 81.0 0.889 0.686\nMulT (Tsai et al., 2019) (our run) 34.3 80.3 80.4 1.008 0.645\nFusion-Based-CM-Attn-MulT (ours) 34.4 76.8 76.8 1.003 0.640\nLMF-MulT (ours) 34.0 78.5 78.5 0.957 0.681\nTable 1: Performance Results for Multimodal Sentiment Analysis on CMU-MOSI dataset with aligned and un-\naligned multimodal sequences.\nMetric Acch\n7 Acch\n2 F1h MAEl Corrh\n(Aligned) CMU-MOSEI Sentiment\nLF-LSTM (pub) 48.8 80.6 80.6 0.619 0.659\nMulT (Tsai et al., 2019) (pub) 51.8 82.5 82.3 0.580 0.703\nMulT (Tsai et al., 2019) (our run) 49.3 80.5 81.1 0.625 0.663\nFusion-Based-CM-Attn-MulT (ours) 49.6 79.9 80.7 0.616 0.673\nLMF-MulT (ours) 50.2 80.3 80.3 0.616 0.662\n(Unaligned) CMU-MOSEI Sentiment\nLF-LSTM (pub) 48.8 77.5 78.2 0.624 0.656\nMulT (Tsai et al., 2019) (pub) 50.7 81.6 81.6 0.591 0.694\nMulT (Tsai et al., 2019) (our run) 50.4 80.7 80.6 0.617 0.677\nFusion-Based-CM-Attn-MulT (ours) 49.3 79.4 79.2 0.613 0.674\nLMF-MulT (ours) 49.3 80.8 81.3 0.620 0.668\nTable 2: Performance Results for Multimodal Sentiment Analysis on larger-scale CMU-MOSEI dataset with\naligned and unaligned multimodal sequences.\ncation. While the earlier work focuses on latent\nadaptation of one modality to another, we focus on\nadaptation of the latent multimodal signal itself us-\ning single-head cross-modal attention to individual\nmodalities. This helps us reduce the excessive pa-\nrameterization of the models by using all combina-\ntions of modality to modality cross-modal attention\nfor each modality. Instead, we only utilize a linear\nnumber of cross-modal attention for each modality\nand the fused signal representation. We add Tempo-\nral Convolutions after the LMF operation to ensure\nthat the input sequences have a sufﬁcient awareness\nof the neighboring elements. We show the overall\narchitecture of our two proposed models in Fig 3\nand Fig 4. In Fig 3, we show the fused multimodal\nsignal representation after a temporal convolution\nto enrich the individual modalities via cross-modal\ntransformer attention. In Fig 4, we show the archi-\ntecture with the least number of Transformer layers\nwhere the individual modalities attend to the fused\nconvoluted multimodal signal.\n3 Experiments\nWe present our early experiments to evaluate the\nperformance of proposed models on the standard\nmultimodal datasets used by (Tsai et al., 2019) 1.\nWe run our models on CMU-MOSI, CMU-MOSEI,\nand IEMOCAP datasets and present the results for\nthe proposed LMF-MulT and Fusion-Based-CM-\nAttn-MulT models. Late Fusion (LF) LSTM is\na common baseline for all datasets with reported\nresults (pub) together with MulT in (Tsai et al.,\n2019). We include the results we obtain (our run)\nfor the MulT model for a direct comparison2. Ta-\nble 1, Table 2, and Table 3 show the performance of\nvarious models on the sentiment analysis and emo-\ntion classiﬁcation datasets. We do not observe any\ntrend suggesting that our methods can achieve bet-\nter accuracies or F1-scores than the original MulT\nmethod (Tsai et al., 2019). However, we do note\n1We have built this work up on the code-base released\nfor MulT (Tsai et al., 2019) at https://github.com/\nyaohungt/Multimodal-Transformer\n2In this work, we have not focused on the further hyper-\nparameter tuning of our models.\n33\nEmotion Happy Sad Angry Neutral\nMetric Acch F1h Acch F1h Acch F1h Acch F1h\n(Aligned) IEMOCAP Emotions\nLF-LSTM (pub) 85.1 86.3 78.9 81.7 84.7 83.0 67.1 67.6\nMulT (Tsai et al., 2019) (pub) 90.7 88.6 86.7 86.0 87.4 87.0 72.4 70.7\nMulT (Tsai et al., 2019) (our run) 86.4 82.9 82.3 82.4 85.3 85.8 71.2 70.0\nFusion-Based-CM-Attn-MulT (ours) 85.6 83.7 83.6 83.7 84.6 85.0 70.4 69.9\nLMF-MulT (ours) 85.3 84.1 84.1 83.4 85.7 86.2 71.2 70.8\n(Unaligned) IEMOCAP Emotions\nLF-LSTM (pub) 72.5 71.8 72.9 70.4 68.6 67.9 59.6 56.2\nMulT (Tsai et al., 2019) (pub) 84.8 81.9 77.7 74.1 73.9 70.2 62.5 59.7\nMulT (Tsai et al., 2019) (our run) 85.6 79.0 79.4 70.3 75.8 65.4 59.2 44.0\nFusion-Based-CM-Attn-MulT (ours) 85.6 79.0 79.4 70.3 75.8 65.4 59.3 44.2\nLMF-MulT (ours) 85.6 79.0 79.4 70.3 75.8 65.4 59.2 44.0\nTable 3: Performance Results for Multimodal Emotion Recognition on IEMOCAP dataset with aligned and un-\naligned multimodal sequences.\nDataset CMU-MOSI CMU-MOSEI IEMOCAP\nModel Aligned Unaligned Aligned Unaligned Aligned Unaligned\nMulT (Tsai et al., 2019) 18.87 19.25 191.40 216.32 36.20 37.93\nFusion-Based-CM-Attn (ours) 14.53 15.80 140.95 175.68 26.10 29.16\nLMF-MulT (ours) 11.01 12.03 106.15 137.35 20.57 23.53\nTable 4: Average Time/Epoch (sec)\nDataset CMU-MOSI CMU-MOSEI IEMOCAP\nMulT (Tsai et al., 2019) 1071211 1073731 1074998\nFusion-Based-CM-Attn (ours) 512121 531441 532078\nLMF-MulT (ours) 836121 855441 856078\nTable 5: Number of Model Parameters\nthat on some occasions, our methods can achieve\nhigher results than the MulT model, in both aligned\n(see LMF-MulT results for IEMOCAP in Table 3)\nand unaligned (see LMF-MulT results for CMU-\nMOSEI in Table 2) case. We plan to do an ex-\nhaustive grid search over the hyper-parameters to\nunderstand if our methods can learn to classify the\nmultimodal signal better than the original competi-\ntive method. Although the results are comparable,\nbelow are the advantages of using our methods:\n•Our LMF-MulT model does not use multiple\nparallel self-attention transformers for the dif-\nferent modalities and it uses least number of\ntransformers compared to the other two mod-\nels. Given the same training infrastructure and\nresources, we observe a consistent speedup in\ntraining with this method. See Table 4 for\naverage time per epoch in seconds measured\nwith ﬁxed batch sizes for all three models.\n•As summarized in Table 5, we observe that\nour models use lesser number of trainable pa-\nrameters compared to the MulT model, and\nyet achieve similar performance.\n4 Conclusion\nIn this paper, we present our early investigations\ntowards utilizing Low Rank representations of the\nmultimodal sequences for usage in multimodal\ntransformers with cross-modal attention to the\nfused signal or the modalities. Our methods\nbuild up on the (Tsai et al., 2019) work and apply\ntransformers to fused multimodal signal that aim\nto capture all inter-modal signals via the Low\nRank Matrix Factorization (Liu et al., 2018). This\nmethod is applicable to both aligned and unaligned\nsequences. Our methods train faster and use\nfewer parameters to learn classiﬁers with similar\nSOTA performance. We are exploring methods to\ncompress the temporal sequences without using\nthe hidden state context vectors from LSTMs that\nlose the temporal information. We recover the\ntemporal information with a Convolution layer.\nWe believe these models can be deployed in low\nresource settings with further optimizations. We\nare also interested in using richer features for\nthe audio, text, and the vision pipeline for other\nuse-cases where we can utilize more resources.\n34\nReferences\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jean-\nnette N. Chang, Sungbok Lee, and Shrikanth S.\nNarayanan. 2008. Iemocap: interactive emotional\ndyadic motion capture database. Language Re-\nsources and Evaluation, 42(4):335.\nPaul Ekman. 2002. Facial action coding system (facs).\nA Human Face.\nPaul Ekman. 2009. Telling Lies: Clues to Deceit in the\nMarketplace, Politics, and Marriage (Revised Edi-\ntion). WW Norton & Company.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nMing Hou, Jiajia Tang, Jianhai Zhang, Wanzeng Kong,\nand Qibin Zhao. 2019. Deep multimodal mul-\ntilinear fusion with high-order polynomial pool-\ning. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. dAlch´e-Buc, E. Fox, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems\n32, pages 12136–12145. Curran Associates, Inc.\nPaul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin\nZhao, Ruslan Salakhutdinov, and Louis-Philippe\nMorency. 2019. Learning representations from im-\nperfect time series data via tensor rank regulariza-\ntion. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics,\npages 1569–1576, Florence, Italy. Association for\nComputational Linguistics.\nZhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-\nnarasimhan, Paul Pu Liang, AmirAli Bagher Zadeh,\nand Louis-Philippe Morency. 2018. Efﬁcient low-\nrank multimodal fusion with modality-speciﬁc fac-\ntors. Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers).\nRobert Plutchik. 2001. The nature of emotions human\nemotions have deep evolutionary roots, a fact that\nmay explain their complexity and provide tools for\nclinical practice. American Scientist.\nJonathan Posner, James A Russell, and Bradley S Pe-\nterson. 2005. The circumplex model of affect: An\nintegrative approach to affective neuroscience, cog-\nnitive development, and psychopathology. Develop-\nment and psychopathology, 17(03):715–734.\nKlaus R Scherer. 1984. Emotion as a multicomponent\nprocess: A model and some cross-cultural data. Re-\nview of Personality & Social Psychology.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ. Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. 2019. Multimodal transformer for\nunaligned multimodal language sequences. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), Florence, Italy. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAmir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-\nbria, and Louis-Philippe Morency. 2017. Tensor\nfusion network for multimodal sentiment analysis.\nCoRR, abs/1707.07250.\nAmir Zadeh, Paul Pu Liang, Jon Vanbriesen, Soujanya\nPoria, Erik Cambria, Minghai Chen, and Louis-\nPhilippe Morency. 2018. Multimodal language anal-\nysis in the wild: Cmu-mosei dataset and inter-\npretable dynamic fusion graph. In Association for\nComputational Linguistics (ACL).\nAmir Zadeh, Rowan Zellers, Eli Pincus, and Louis-\nPhilippe Morency. 2016. Multimodal sentiment in-\ntensity analysis in videos: Facial gestures and verbal\nmessages. IEEE Intelligent Systems, 31(6):82–88.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6714842319488525
    },
    {
      "name": "Modalities",
      "score": 0.5753597021102905
    },
    {
      "name": "Transformer",
      "score": 0.5208255052566528
    },
    {
      "name": "Fusion",
      "score": 0.5017743110656738
    },
    {
      "name": "Sensor fusion",
      "score": 0.4972417652606964
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4791944921016693
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.4559732675552368
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36280229687690735
    },
    {
      "name": "Machine learning",
      "score": 0.3600316643714905
    },
    {
      "name": "Engineering",
      "score": 0.07943958044052124
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ]
}