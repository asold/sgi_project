{
  "title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
  "url": "https://openalex.org/W4387144848",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4208650056",
      "name": "Eddie Guo",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2897392305",
      "name": "Mehul Gupta",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2469496886",
      "name": "Deng Jiawen",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A4220085055",
      "name": "Ye-Jean Park",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2634773987",
      "name": "Michael Paget",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A260375131",
      "name": "Christopher Naugler",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A4208650056",
      "name": "Eddie Guo",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2897392305",
      "name": "Mehul Gupta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2469496886",
      "name": "Deng Jiawen",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A4220085055",
      "name": "Ye-Jean Park",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2634773987",
      "name": "Michael Paget",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A260375131",
      "name": "Christopher Naugler",
      "affiliations": [
        "University of Calgary"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3000482183",
    "https://openalex.org/W2912011336",
    "https://openalex.org/W3087544668",
    "https://openalex.org/W3002050776",
    "https://openalex.org/W1530466595",
    "https://openalex.org/W3143985904",
    "https://openalex.org/W3164863290",
    "https://openalex.org/W1917732024",
    "https://openalex.org/W1561556558",
    "https://openalex.org/W2944878516",
    "https://openalex.org/W2961191798",
    "https://openalex.org/W4297860676",
    "https://openalex.org/W4213127247",
    "https://openalex.org/W2600107025",
    "https://openalex.org/W3087185831",
    "https://openalex.org/W3093336514",
    "https://openalex.org/W2807522649",
    "https://openalex.org/W2184378182",
    "https://openalex.org/W3037933615",
    "https://openalex.org/W4315641584",
    "https://openalex.org/W2043566294",
    "https://openalex.org/W2560438049",
    "https://openalex.org/W2604351697",
    "https://openalex.org/W4313371821",
    "https://openalex.org/W3202911911",
    "https://openalex.org/W4316496279",
    "https://openalex.org/W4210366877",
    "https://openalex.org/W4281396895",
    "https://openalex.org/W4327720432",
    "https://openalex.org/W4205276654",
    "https://openalex.org/W3111278950",
    "https://openalex.org/W2808847453",
    "https://openalex.org/W4320164613",
    "https://openalex.org/W4387144848",
    "https://openalex.org/W2147469877",
    "https://openalex.org/W4206902451"
  ],
  "abstract": "Background The systematic review of clinical research papers is a labor-intensive and time-consuming process that often involves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality of the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a significant investment of time and resources. Objective This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4 application programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers. Methods We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title and abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against human-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts. Results Our results show an accuracy of 0.91, a macro F1-score of 0.60, a sensitivity of excluded papers of 0.91, and a sensitivity of included papers of 0.76. The interrater variability between 2 independent human screeners was κ=0.46, and the prevalence and bias-adjusted κ between our proposed methods and the consensus-based human decisions was κ=0.96. On a randomly selected subset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial decisions upon being asked to explain their reasoning for incorrect classifications. Conclusions Large language models have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and reliable conclusions in medical research.",
  "full_text": "Original Paper\nAutomated Paper Screening for Clinical Reviews Using Large\nLanguage Models: Data Analysis Study\nEddie Guo1; Mehul Gupta1, MD; Jiawen Deng2, BHSc; Ye-Jean Park2; Michael Paget1, BFA; Christopher Naugler1,\nMD\n1Cumming School of Medicine, University of Calgary, Calgary, AB, Canada\n2Temerty Faculty of Medicine, University of Toronto, Toronto, AB, Canada\nCorresponding Author:\nEddie Guo\nCumming School of Medicine\nUniversity of Calgary\n3330 University Dr NW\nCalgary, AB, T2N 1N4\nCanada\nPhone: 1 5879880292\nEmail: eddie.guo@ucalgary.ca\nAbstract\nBackground: The systematic review of clinical research papers is a labor-intensive and time-consuming process that often\ninvolves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality\nof the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a\nsignificant investment of time and resources.\nObjective: This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4\napplication programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world\nclinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers.\nMethods: We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical\nreviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title\nand abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against\nhuman-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts.\nResults: Our results show an accuracy of 0.91, a macro F1-score of 0.60, a sensitivity of excluded papers of 0.91, and a sensitivity\nof included papers of 0.76. The interrater variability between 2 independent human screeners was κ=0.46, and the prevalence\nand bias-adjusted κ between our proposed methods and the consensus-based human decisions was κ=0.96. On a randomly selected\nsubset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial\ndecisions upon being asked to explain their reasoning for incorrect classifications.\nConclusions: Large language models have the potential to streamline the clinical review process, save valuable time and effort\nfor researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather\nthan a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and\nreliable conclusions in medical research.\n(J Med Internet Res 2024;26:e48996) doi: 10.2196/48996\nKEYWORDS\nabstract screening; Chat GPT; classification; extract; extraction; free text; GPT; GPT-4; language model; large language models;\nLLM; natural language processing; NLP; nonopiod analgesia; review methodology; review methods; screening; systematic review;\nsystematic; unstructured data\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 1https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nIntroduction\nKnowledge synthesis, the process of integrating and\nsummarizing relevant studies in the literature to gain an\nimproved understanding of a topic, is a key component in\nidentifying knowledge gaps and informing future research\nendeavors on a topic of interest [1,2]. Systematic and scoping\nreviews are among the most commonly used and rigorous forms\nof knowledge synthesis across multiple disciplines [1,2]. Given\nthat the results from systematic and scoping reviews can inform\nguidelines, protocols, and decision-making processes,\nparticularly for stakeholders in the realms of health care, the\nquality of the evidence presented by such reviews can\nsignificantly impact generated recommendations [3].\nThe quality of systematic and scoping reviews is highly\ndependent on the comprehensiveness of the database searches\nand the subsequent article screening processes. Overlooking\nrelevant articles during these critical steps can lead to bias [4],\nwhile including discrepant studies can yield misleading\nconclusions and increase discordant heterogeneity [5]. Thus,\nguidelines surrounding the conduct of clinical reviews, such as\nthe Cochrane Handbook [6], recommend that article screening\nbe completed in duplicate by at least 2 independent reviewers.\nHowever, duplicate screening effectively doubles the financial\nand human resources needed to complete systematic reviews\ncompared to single screening. This is especially problematic\nfor small research groups, review projects with broad inclusion\ncriteria (such as network meta-analyses), or time-constrained\nreview projects (such as reviews relating to COVID-19 during\nthe early stages of the pandemic) [7,8]. Additionally, there is\noften substantial interrater variability in screening decisions,\nleading to additional time spent on discussions to resolve\ndisagreements [9]. Due to the time constraints and wasted\nresources that are often features of duplicate screening, research\nstudies may also include a more tailored, sensitive search\nstrategy that can lead to missing several articles during the\nretrieval process [10]. Furthermore, although the nuances of\neach study differ, many systematic reviews may contain\nthousands of retrieved articles, only to exclude the majority (ie,\nup to 90%) from the title and abstract screening [10,11].\nRecent developments in artificial intelligence and machine\nlearning have made it possible to semiautomate or fully automate\nrepetitive steps within the systematic review workflow [12-14].\nProminent examples of such applications include\nRobotReviewer [15], TrialStreamer [16], Research Screener\n[7], DistillerSR [17], and Abstrackr [18], which are artificial\nintelligence models developed to extract information from\nscientific articles or abstracts to judge study quality and infer\ntreatment effects. More specifically, RobotReviewer (2016)\nwas shown to have similar capabilities to assess the risk of bias\nassessment as a human reviewer, only differing by around 7%\nin accuracy [19]. Similarly, TrialStreamer was a system\ndeveloped to extract key elements of information from full texts,\nsuch as inferring which interventions in a clinical paper worked\nbest, along with comparisons in study outcomes between all\nrelevant extracted full texts of a topic indexed on MEDLINE\n[20].\nWhile there have been previous attempts at automating the title\nand abstract screening process, they often involved labor- or\ncomputationally-intensive labeling, pretraining, or vectorizations\n[21]. For instance, Rayyan and Abstrackr are 2 free web tools\nthat provide a semiautomated approach to article filtering by\nusing natural language processing algorithms to learn when and\nwhere a reviewer includes or excludes an article and\nsubsequently mimics a similar approach [22,23]. Rayyan also\ndemonstrated high specificity, wherein 98% of all relevant\narticles were included after the tool had screened 75% of all\narticles to be analyzed in a study [24]. While automation using\nthese tools was found to save time, there was still minimal to\nsubstantive risk that there would be missing studies if the tool\nwere fully independent or automated [22,23]. Furthermore,\ncurrent programs may use previously standard methods,\nincluding n-grams, in comparison to more updated techniques,\nsuch as the generative pretrained transformer (GPT) model,\nwhich is trained with data from a general domain and does not\nrequire additional training to learn embeddings that can represent\nthe semantics and contexts of words in relation to other words\n[25,26].\nIn this paper, we introduce a novel workflow to screen titles\nand abstracts for clinical reviews by providing plain language\nprompts to the publicly available OpenAI GPT application\nprogramming interface (API). We aimed to assess GPT models’\nability to accurately and efficiently identify relevant titles and\nabstracts from real-world clinical review data sets, as well as\ntheir ability to explain their decisions and reflect on incorrect\nclassifications. We compare the performance of our model\nagainst ground truth labeling by 2 independent human reviewers\nacross 6 review papers in the screening of over 24,000 titles\nand abstracts.\nMethods\nOverview\nIn our study, we obtained a corpus of title and abstract data sets\nthat have already been filtered by a minimum of 2 human\nreviewers to train our model (Figure 1). Subsequently, we\ncreated a Python script that provides the screening criteria for\neach paper to the OpenAI Chat GPT or GPT-4 API, depending\non the input token length. We then passed each paper to the API\nusing a consistent instruction prompt to determine whether a\npaper should be included or excluded based on the contents of\nits title and abstract. The overall accuracy (computed by dividing\npapers selected by both GPT and human reviewers by the total\nnumber of papers), sensitivity of both included and excluded\npapers, and interrater reliability through Cohen κ and\nprevalence-adjusted and bias-adjusted κ (PABAK) were\ncomputed against the human-reviewed papers:\nWhere k is the number of categories and pobs is the proportion\nof included papers. All data and code are available in Mendeley\ndata sets [27].\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 2https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nFigure 1. Overview of the Python script to automate screening with the generative pretrained transformer (GPT) application programming interface\n(API). LLM: large language model.\nData Collection\nTo validate our proposed inclusion and exclusion methodology,\nwe obtained 6 title and abstract screening data sets from different\nsystematic and scoping reviews previously published by the\nauthors of this study, each screened by 2 independent reviewers\nwith conflicts resolved through consensus. These projects cover\nvarious medical science topics and vary in size, methodology,\nand complexity of screening criteria (Table 1 and Table S1 in\nMultimedia Appendix 1 [28-33]). We obtained the inclusion\nand exclusion decision from expert reviewers for each title and\nabstract entry, as well as the criteria provided to the expert\nreviewers during the screening process. A summary of the\nreview characteristics is presented in Table 2.\nTable 1. Included studies and their characteristics. The first 5 data sets are systematic reviews with meta-analyses. The last study is a scoping review.\nStudy topicStudy typeIncluded studies\n(538/24,307), n/N\nData set nameStudy title\nCOVID-19 treatment and\nantimalarials\nSystematic review and meta-anal-\nysis of randomized and nonrandom-\nized trials\n35/279IVMaEfficacy and Safety of Ivermectin for the\nTreatment of COVID-19: A Systematic Review\nand Meta-Analysis [29]\nCOVID-19 treatment and\nantidepressants\nSystematic review and meta-anal-\nysis of randomized and nonrandom-\nized trials\n29/3989SSRIbEfficacy and Safety of Selective Serotonin Re-\nuptake Inhibitors in COVID-19 Management:\nA Systematic Review and Meta-Analysis [30]\nCOVID-19 treatment and\nantiretrovirals\nSystematic review and meta-anal-\nysis of randomized and nonrandom-\nized trials\n91/1456LPVRcEfficacy of Lopinavir-Ritonavir Combination\nTherapy for the Treatment of Hospitalized\nCOVID-19 Patients: A Meta-Analysis [31]\nRaynaud syndrome and\nacupuncture\nSystematic review and meta-anal-\nysis of randomized and nonrandom-\nized trials\n6/942RAYNAUDSdThe Use of Acupuncture in Patients With Ray-\nnaud’s Syndrome: A Systematic Re-View and\nMeta-Analysis of Randomized Controlled Tri-\nals [32]\nPostoperative pain and\nanalgesics\nSystematic review and meta-anal-\nysis of randomized and nonrandom-\nized trials\n354/14,771NOAeComparative Efficacy of Adjuvant Non-Opioid\nAnalgesia in Adult Cardiac Surgical Patients:\nA Network Meta-Analysis [33]\nMachine learning in clin-\nical medicine\nScoping review23/2870LLMAssessing the Research Landscape and Utility\nof LLMsf in the Clinical Setting: Protocol for\na Scoping Review g\naIVM: ivermectin.\nbSSRI: selective serotonin reuptake inhibitor.\ncLPVR: lopinavir-ritonavir.\ndRAYNAUDS: Raynaud syndrome.\neNOA: nonopioid analgesia.\nfLLM: large language model.\ngRegistered with Open Science Framework [28].\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 3https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTable 2. Data formatting for the Python script automating screening with the generative pretrained transformer application programming interface. All\nnon-English characters were removed before analysis.\nColumnsData\ndf_info • Dataset Name (str): name of data set\n• Inclusion Criteria (str): screening inclusion criteria\n• Exclusion Criteria (str): screening exclusion criteria\nDataseta • Title (str): paper title\n• Abstract (str): paper abstract\naThe name of the data set must match Dataset Name in df_info.\nApp Creation\nGiven a data set, df_info, containing information about inclusion\nand exclusion criteria of the data sets containing titles and\nabstracts to be reviewed, the app calls the OpenAI GPT API to\nclassify each paper to be screened as either included or excluded.\nThe app was coded in Python. The prompt given to the GPT\nAPI is provided in Textbox 1.\nTextbox 1. Prompt given to the generative pretrained transformer application programming interface.\nInstructions: You are a researcher rigorously screening titles and abstracts of scientific papers for inclusion or exclusion in a review paper. Use the\ncriteria below to inform your decision. If any exclusion criteria are met or not all inclusion criteria are met, exclude the article. If all inclusion criteria\nare met, include the article. Only type “included” or “excluded” to indicate your decision. Do not type anything else.\nAbstract: {abstract}\nInclusion criteria: {inclusion_criteria}\nExclusion criteria: {exclusion_criteria}\nDecision:\nWhere “Decision:” is whether GPT API includes or excludes the article. Thus, the algorithm is as follows:\ndata_df <- load(df_info)\nfor each dataset in data_df: for each row in dataset:\nprompt <- instructions + title + abstract + inclusion criteria \\\n+ exclusion criteria decision <- GPT(prompt) row[‘decision’] <- decision\nsave(dataset)\nAssessment and Data Analysis\nAfter the app was run on all data sets included in our analysis,\nthe following metrics were computed: accuracy, macro F1-score,\nsensitivity for decision tags, κ, and PABAK. A subset of the\nresults was selected for the GPT models to explain their\nreasoning. The following prompt was appended to the beginning\nof the original prompt given to the API: “Explain your reasoning\nfor the decision given with the information below.” The human\nand GPT decisions were appended to the end of the prompt. A\nsubset of incorrect results was selected for GPT to reflect on its\nincorrect answers. The following prompt was appended to the\nbeginning of the original prompt given to the API: “Explain\nyour reasoning for why the decision given was incorrect with\nthe information below.” The human and GPT decisions were\nappended to the end of the prompt.\nResults\nThe overall accuracy of the GPT models was 0.91, the sensitivity\nof included papers was 0.76, and the sensitivity of excluded\npapers was 0.91 (Table 3 and Figure 2). On the nonopioid\nanalgesia (NOA) data set (354/14,771 included abstracts), the\nmodel ran for 643 minutes and 50.8 seconds, with an\napproximate cost of US $25. The data set characteristics are\ndetailed in Table 1, the model performance is in Table 3 and\nvisualized in Figure 2, and the reasoning from GPT is tabulated\nin Table 4.\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 4https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTable 3. Performance of generative pretrained transformer (GPT) in screening titles and abstracts against a human reviewer’s ground truth. κ (human)\nis the agreement between 2 independent human reviewers. κ (screen) is the agreement between GPT and the final papers included and excluded in each\ndata set.\nPABAKaκ (screen)κ (human)Sensitivity (excluded)Sensitivity (included)Macro F1-scoreAccuracyData set\n0.780.260.720.7560.6860.6100.748IVMb\n0.990.210.580.9490.9660.5950.846SSRIc\n0.880.250.510.8620.5930.6130.949LPVRd\n0.990.220.910.9660.8330.6070.965RAYNAUDSe\n0.950.230.350.8980.7820.6010.895NOAf\n0.980.210.690.9421.0000.5940.943LLMg\n0.960.220.460.9100.7640.6000.907Total (weighted)\n0.930.230.630.9000.8100.6640.891Total (macro)\naPABAK: prevalence-adjusted and bias-adjusted κ.\nbIVM: ivermectin.\ncSSRI: selective serotonin reuptake inhibitor.\ndLPVR: lopinavir-ritonavir.\neRAYNAUDS: Raynaud syndrome.\nfNOA: nonopioid analgesia.\ngLLM: large language model.\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 5https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nFigure 2. (A-F) Confusion matrices for the included and excluded papers for all data sets and (G) the overall performance. IVM: ivermectin; LLM:\nlarge language model; LPVR: lopinavir-ritonavir; NOA: nonopioid analgesia; RAYNAUDS: Raynaud’s syndrome; SSRI: selective serotonin reuptake\ninhibitor.\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 6https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTable 4. Decision and reasoning of the generative pretrained transformer (GPT) application programming interface for a subset of 4 example titles and\nabstracts. Full abstracts are in ivermectin (IVM) data set [27].\nDecision and reasoningPrompt\n•• Human: IncludedTitle: Use of Ivermectin in the Treatment of COVID-19:\nA Pilot Trial • GPT: Included\n• Abstract: Objectives: In this randomized open-label trial\npilot study we assessed the antiviral effects and safety of\nvarious doses of ivermectin in patients with mild clinical\nsymptoms of COVID-19. Method(s): Patients were ran-\ndomly assigned to receive standard of care (SOC) treatment\nat hospital admission...\n• Reasoning: The study meets all of the inclusion criteria. It is a randomized\ncontrolled trial that compares ivermectin against standard of care treatment in\nadult COVID-19 patients. The study reports outcomes related to time to viral\nclearance and adverse events. Additionally, the study is published in a peer-re-\nviewed journal. Therefore, the article should be included in the review paper.\n•• Human: ExcludedTitle: What Does the COVID-19 Pandemic Mean for the\nNext Decade of Onchocerciasis Control and Elimination? • GPT: Excluded\n• Abstract: Background: Mass drug administration (MDA)\nof ivermectin for onchocerciasis has been disrupted by the\ncoronavirus disease 2019 (COVID-19) pandemic. Mathe-\nmatical modelling can help predict how missed/delayed\nMDA will affect short-term epidemiological trends and\nelimination prospects by 2030...\n• Reasoning: The article does not meet any of the inclusion criteria or exclusion\ncriteria listed. It is not a randomized controlled trial or comparative cohort study,\nit does not compare ivermectin against standard of care or control group/placebo,\nit does not include adult COVID-19 inpatients and/or outpatients, and it does\nnot report any of the specified outcomes. Additionally, there is no indication\nthat the article is non-peer-reviewed. Therefore, the article is excluded.\n•• Human: IncludedTitle: Use of Ivermectin Is Associated With Lower Mortal-\nity in Hospitalized Patients With Coronavirus Disease\n2019: The Ivermectin in COVID Nineteen Study\n• GPT: Excluded\n• Reasoning: The article meets all of the inclusion criteria and none of the exclu-\nsion criteria, therefore it should be included in the review paper.• Abstract: Background: Ivermectin was shown to inhibit\nsevere acute respiratory syndrome coronavirus 2 replication\nin vitro, which has led to off-label use, but clinical efficacy\nhas not been described previously. Research Question:\nDoes ivermectin benefit hospitalized coronavirus disease\n2019 (COVID-19) patients?...\n• Reflection: The given decision of excluding the article is incorrect. The article\nmeets all the inclusion criteria as it is a non-randomized comparative cohort\nstudy that compares ivermectin against standard of care or control\ngroup/placebo, includes adult COVID-19 inpatients, and reports mortality inci-\ndence. The article does not use ivermectin for prophylaxis of COVID-19 and\nis a peer-reviewed article. Therefore, the correct decision should be “Included.”\n•• Human: ExcludedTitle: What to Expect From Different Drugs Used in the\nTreatment of COVID-19: A Study on Applications and in\nVivo and in Vitro Results\n• GPT: Included\n• Reasoning: The study is not a randomized controlled trial or a non-randomized\ncomparative cohort study, which is an inclusion criteria. Therefore, the article\ndoes not meet all inclusion criteria and must be excluded.\n• Abstract: The end of 2019 was marked by the emergence\nof a new type of coronavirus (SARS-CoV-2), which has\nkilled more than 240,000 people... • Reflection: As per the given inclusion and exclusion criteria, the decision given\nis incorrect. The abstract does not mention the use of ivermectin in any of the\nstudies. Therefore, the article cannot be included based on the inclusion criteria.\nDiscussion\nOverview\nIn this study, we assessed the performance of the OpenAI GPT\nAPI in the context of clinical review paper inclusion and\nexclusion criteria selection. We report an overall accuracy of\n0.91 and a PABAK of 0.96, indicating a high level of agreement\nbetween the app’s decisions and the reference standard.\nHowever, the κ was low, ranging from 0.21 to 0.26, largely due\nto the imbalanced nature of the data sets in this study. The\nsensitivity of the included papers was 0.76, suggesting that the\napp needs improvement to correctly identify relevant papers\n(Table 3 and Figure 2). The sensitivity of excluded papers was\n0.91, showing promise in excluding irrelevant papers. These\nresults highlight the potential of large language models (LLMs)\nto support the clinical review process.\nImplications of GPT API’s Performance in the Review\nProcess\nGPT’s performance has several implications for the efficiency\nand consistency of clinical review paper inclusion and exclusion\ncriteria selection. By prioritizing the workflow and acting as an\naid rather than a replacement for researchers and reviewers, the\nGPT and other large language models have the potential to\nstreamline the review process. This enhanced efficiency could\nsave valuable time and effort for researchers and clinicians,\nallowing them to focus on more complex tasks and in-depth\nanalysis. Further, the API does not require pretraining or seed\narticles and can provide reasoning for its decision to either\ninclude or exclude papers, an aspect traditional natural language\nprocessing algorithms lack in automated or semiautomated\npaper screening (Table 4). Interestingly, upon being asked to\nexplain its reasoning for a subset of incorrect classifications,\nGPT corrected its initial decision. Ultimately, this increased\nefficiency, paired with reasoning capabilities, could contribute\nto the overall quality of clinical reviews, leading to more\naccurate and reliable conclusions in medical research.\nThe use of LLMs in the review process could also promote\nconsistency in the selection of relevant papers. By automating\ncertain aspects of the process and acting as an aid to researchers\nand clinicians, the model can streamline the review process and\nhelp reduce the potential for human error and bias, leading to\nmore objective and reliable results [34]. This increased\nconsistency could, in turn, improve the overall quality of the\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 7https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nevidence synthesized in clinical reviews, providing a more\nrobust foundation for medical decision-making and the\ndevelopment of clinical guidelines.\nThe potential of LLMs as a decision tool becomes particularly\nvaluable when resources are limited. In such situations, LLMs\ncan be used as a first-pass decision aid, streamlining the review\nprocess, and allowing human screeners to focus on a smaller,\nmore relevant subset of papers. By automating the initial\nscreening process, LLMs can help reduce the workload for\nresearchers and clinicians, enabling them to allocate their time\nand effort more efficiently.\nIn particular, using the GPT API as a first-pass decision aid can\nalso help mitigate the risk of human error and bias in the initial\nscreening phase, promoting a more objective and consistent\nselection of papers. While the API’s sensitivity for including\nrelevant papers may not be perfect, its high specificity for\nexcluding irrelevant papers can still provide valuable support\nin narrowing down the pool of potentially relevant studies [10].\nThis can be particularly beneficial in situations where a large\nnumber of papers need to be screened and human resources are\nscarce [35].\nComparison to Other Tools\nThe comparison of our proposed machine learning method to\nother tools, such as Abstrackr [18], DistillerSR [17], and\nRobotAnalyst [36], provides evidence of its efficacy and\nreliability in the context of systematic review processes. On a\ndata set of 24,307 abstracts and titles, our model achieved an\naccuracy of 0.91 and comparable sensitivity of 0.91 and 0.76\nfor excluded and included papers, respectively. The significant\ninterrater agreement (κ=0.96) between our proposed method\nand consensus-based human decisions, juxtaposed to the lower\ninterrater variability between 2 independent human screeners\n(κ=0.46), emphasizes the model’s robustness. In comparison,\nAbstrackr reported overall sensitivities of 0.96, 0.79, 0.92, and\n0.82 on data sets ranging from 5243 to 47,385 records. When\ncomparing the proportion of missed records across Abstrackr,\nDistillerSR, and RobotAnalyst on nonpublic medical title and\nabstract screening data sets, Abstrackr exhibited the lowest\nproportions of missed records, namely 28%, 5%, and 0%,\nrespectively [37]. Conversely, DistillerSR showed a high\nproportion of missed records, reaching up to 100% in the last\ndata set. RobotAnalyst’s performance fell between the 2, with\nmissed proportions of 70%, 23%, and 100%, respectively. Future\nwork will explore comparative analyses in greater depth and on\na broader array of data sets to compare state-of-the-art screening\ntools.\nLimitations and Challenges in Implementing GPT API\nin the Review Process\nWhile the GPT API shows promise in streamlining the review\nprocess, it is important to acknowledge its limitations and\nchallenges. One notable limitation is the disparity between the\nhigh specificity of 0.91 for excluding papers and the lower\nsensitivity of 0.76 for including papers. This discrepancy\nsuggests that while the API effectively excludes irrelevant\npapers, it may not be as proficient in identifying relevant papers\nfor inclusion. This could lead to the omission of important\nstudies in the review process, potentially affecting the\ncomprehensiveness and quality of the final review. Therefore,\nthe GPT API should not be considered a replacement for human\nexpertise. Instead, it should be viewed as a complementary tool\nthat can enhance the efficiency and consistency of the review\nprocess. Human screeners should still be involved in the final\ndecision-making process, particularly in cases where the API’s\nsensitivity for including relevant papers may be insufficient [7].\nAnother limitation arises in the selection of data sets for\nscreening; 3 of the 6 data sets focused on the efficacy of various\ndrugs for COVID-19, potentially limiting the generalizability\nof the results from other types of studies. Further work will\nassess GPT on a greater diversity of studies. By combining the\nstrengths of the GPT API with human expertise, researchers\ncan optimize the review process and ensure the accuracy and\ncomprehensiveness of the final review.\nFuture Research and Development\nSeveral avenues for future research and development include\nrefining the GPT API’s performance in the clinical review paper\ncontext, incorporating metadata such as study type and year,\nand exploring few-shot learning approaches. Additionally,\ntraining a generator-discriminator model through fine-tuning\ncould improve the API’s performance [38]. Expanding the\napplication of the GPT API to other areas of medical research\nor literature review could also be explored. This would involve\nlarge language models for tasks such as identifying and\nextracting study design information, patient characteristics, and\nadverse events. As the maximum token length increases with\nfuture iterations of the GPT model, screening entire papers may\nbecome feasible [39]. Furthermore, exploring the use of LLMs\nto generate clinical review papers could be a promising research\ndirection.\nConclusions\nThe GPT API shows potential as a valuable tool for improving\nthe efficiency and consistency of clinical review paper inclusion\nand exclusion criteria selection. While there are limitations and\nchallenges to its implementation, its performance in this study\nsuggests that it could have a broader impact on clinical review\npaper writing and medical research. Future research and\ndevelopment should focus on refining the API’s performance,\nexpanding its applications, and exploring its potential in other\naspects of clinical research.\nAcknowledgments\nWe would like to acknowledge the following expert reviewers for providing the screening decisions in the review data sets used\nin this study and for agreeing to make the data sets publicly available: Abhinav Pillai, Mike Paget, Christopher Naugler, Kiyan\nHeybati, Fangwen Zhou, Myron Moskalyk, Saif Ali, Chi Yi Wong, Wenteng Hou, Umaima Abbas, Qi Kang Zuo, Emma Huang,\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 8https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nDaniel Rayner, Cristian Garcia, Harikrishnaa Ba Ramaraju, Oswin Chang, Zachary Silver, Thanansayan Dhivagaran, Elena\nZheng, and Shayan Heybati.\nAuthors' Contributions\nEG contributed to conceptualization, methodology, software, formal analysis, investigation, writing the original draft, reviewing,\nediting, visualization, supervision, and project administration. MG was responsible for conceptualization, methodology,\ninvestigation, writing the original draft, reviewing, editing, supervision, and project administration. JD and YJP were involved\nin methodology, software, formal analysis, investigation, data curation, writing the original draft, and visualization. MP and CN\ncontributed to writing, reviewing, and editing.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nIncluded studies and their inclusion and exclusion criteria.\n[DOCX File , 20 KB-Multimedia Appendix 1]\nReferences\n1. Sargeant JM, O'Connor AM. Scoping reviews, systematic reviews, and meta-analysis: applications in veterinary medicine.\nFront Vet Sci. 2020;7:11. [FREE Full text] [doi: 10.3389/fvets.2020.00011] [Medline: 32047759]\n2. Garritty C, Stevens A, Hamel C, Golfam M, Hutton B, Wolfe D. Knowledge synthesis in evidence-based medicine. Semin\nNucl Med. 2019;49(2):136-144. [doi: 10.1053/j.semnuclmed.2018.11.006] [Medline: 30819393]\n3. Luchini C, Veronese N, Nottegar A, Shin JI, Gentile G, Granziol U, et al. Assessing the quality of studies in meta-research:\nreview/guidelines on the most important quality assessment tools. Pharm Stat. 2021;20(1):185-195. [doi: 10.1002/pst.2068]\n[Medline: 32935459]\n4. Gartlehner G, Affengruber L, Titscher V, Noel-Storr A, Dooley G, Ballarini N, et al. Single-reviewer abstract screening\nmissed 13 percent of relevant studies: a crowd-based, randomized controlled trial. J Clin Epidemiol. 2020;121:20-28. [FREE\nFull text] [doi: 10.1016/j.jclinepi.2020.01.005] [Medline: 31972274]\n5. Fletcher J. What is heterogeneity and is it important? BMJ. 2007;334(7584):94-96. [FREE Full text] [doi:\n10.1136/bmj.39057.406644.68] [Medline: 17218716]\n6. The Cochrane Collaboration; Higgins JPT, Thomas J. Cochrane Handbook for Systematic Reviews of Interventions. London.\nThe Cochrane Collaboration; 2021.\n7. Chai KEK, Lines RLJ, Gucciardi DF, Ng L. Research screener: a machine learning tool to semi-automate abstract screening\nfor systematic reviews. Syst Rev. 2021;10(1):93. [FREE Full text] [doi: 10.1186/s13643-021-01635-3] [Medline: 33795003]\n8. Clark J, McFarlane C, Cleo G, Ramos CI, Marshall S. The impact of systematic review automation tools on methodological\nquality and time taken to complete systematic review tasks: case study. JMIR Med Educ. 2021;7(2):e24418. [FREE Full\ntext] [doi: 10.2196/24418] [Medline: 34057072]\n9. Tuijn S, Janssens F, Robben P, van den Bergh H. Reducing interrater variability and improving health care: a meta-analytical\nreview. J Eval Clin Pract. 2012;18(4):887-895. [doi: 10.1111/j.1365-2753.2011.01705.x] [Medline: 21726359]\n10. Rathbone J, Carter M, Hoffmann T, Glasziou P. Better duplicate detection for systematic reviewers: evaluation of systematic\nreview assistant-deduplication module. Syst Rev. 2015;4(1):6. [FREE Full text] [doi: 10.1186/2046-4053-4-6] [Medline:\n25588387]\n11. Polanin JR, Pigott TD, Espelage DL, Grotpeter JK. Best practice guidelines for abstract screening large‐evidence systematic\nreviews and meta‐analyses. Res Synth Methods. 2019;10(3):330-342. [FREE Full text] [doi: 10.1002/jrsm.1354]\n12. Marshall IJ, Wallace BC. Toward systematic review automation: a practical guide to using machine learning tools in research\nsynthesis. Syst Rev. 2019;8(1):163. [FREE Full text] [doi: 10.1186/s13643-019-1074-9] [Medline: 31296265]\n13. Marshall IJ, Trikalinos TA, Soboczenski F, Yun HS, Kell G, Marshall R, et al. In a pilot study, automated real-time systematic\nreview updates were feasible, accurate, and work-saving. J Clin Epidemiol. 2023;153:26-33. [FREE Full text] [doi:\n10.1016/j.jclinepi.2022.08.013] [Medline: 36150548]\n14. Blaizot A, Veettil SK, Saidoung P, Moreno-Garcia CF, Wiratunga N, Aceves-Martins M, et al. Using artificial intelligence\nmethods for systematic review in health sciences: a systematic review. Res Synth Methods. 2022;13(3):353-362. [doi:\n10.1002/jrsm.1553] [Medline: 35174972]\n15. Marshall I, Kuiper J, Banner E, Wallace BC. Automating biomedical evidence synthesis: RobotReviewer. Association for\nComputational Linguistics; Presented at: Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics-System Demonstrations; July 30-August 4, 2017, 2017;7-12; Vancouver, Canada. URL: https://aclanthology.\norg/P17-4002.pdf [doi: 10.18653/v1/p17-4002]\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 9https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n16. Marshall IJ, Nye B, Kuiper J, Noel-Storr A, Marshall R, Maclean R, et al. Trialstreamer: a living, automatically updated\ndatabase of clinical trial reports. J Am Med Inform Assoc. 2020;27(12):1903-1912. [FREE Full text] [doi:\n10.1093/jamia/ocaa163] [Medline: 32940710]\n17. Hamel C, Kelly SE, Thavorn K, Rice DB, Wells GA, Hutton B. An evaluation of DistillerSR's machine learning-based\nprioritization tool for title/abstract screening—impact on reviewer-relevant outcomes. BMC Med Res Methodol.\n2020;20(1):256. [FREE Full text] [doi: 10.1186/s12874-020-01129-1] [Medline: 33059590]\n18. Gates A, Johnson C, Hartling L. Technology-assisted title and abstract screening for systematic reviews: a retrospective\nevaluation of the Abstrackr machine learning tool. Syst Rev. 2018;7(1):45. [FREE Full text] [doi: 10.1186/s13643-018-0707-8]\n[Medline: 29530097]\n19. Marshall IJ, Kuiper J, Wallace BC. RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials.\nJ Am Med Inform Assoc. 2016;23(1):193-201. [FREE Full text] [doi: 10.1093/jamia/ocv044] [Medline: 26104742]\n20. Nye BE, Nenkova A, Marshall IJ, Wallace BC. Trialstreamer: mapping and browsing medical evidence in real-time. Proc\nConf. 2020;2020:63-69. [FREE Full text] [doi: 10.18653/v1/2020.acl-demos.9] [Medline: 34136886]\n21. Moreno-Garcia CF, Jayne C, Elyan E, Aceves-Martins M. A novel application of machine learning and zero-shot classification\nmethods for automated abstract screening in systematic reviews. Decis Anal J. 2023;6:100162. [FREE Full text] [doi:\n10.1016/j.dajour.2023.100162]\n22. Wallace BC, Trikalinos TA, Lau J, Brodley C, Schmid CH. Semi-automated screening of biomedical citations for systematic\nreviews. BMC Bioinformatics. 2010;11:55. [FREE Full text] [doi: 10.1186/1471-2105-11-55] [Medline: 20102628]\n23. Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyan-a web and mobile app for systematic reviews. Syst Rev.\n2016;5(1):210. [FREE Full text] [doi: 10.1186/s13643-016-0384-4] [Medline: 27919275]\n24. Olofsson H, Brolund A, Hellberg C, Silverstein R, Stenström K, Österberg M, et al. Can abstract screening workload be\nreduced using text mining? User experiences of the tool Rayyan. Res Synth Methods. 2017;8(3):275-280. [doi:\n10.1002/jrsm.1237] [Medline: 28374510]\n25. Shree P. The journey of Open AI GPT models. Medium. 2020. URL: https://medium.com/walmartglobaltech/\nthe-journey-of-open-ai-gpt-models-32d95b7b7fb2 [accessed 2023-04-19]\n26. O'Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S. Using text mining for study identification in systematic\nreviews: a systematic review of current approaches. Syst Rev. 2015;4(1):5. [FREE Full text] [doi: 10.1186/2046-4053-4-5]\n[Medline: 25588314]\n27. Guo E, Gupta M, Deng J, Park YJ, Paget M, Naugler C. Automated paper screening for clinical reviews using large language\nmodels. Mendeley Data. 2023. URL: https://data.mendeley.com/datasets/np79tmhkh5/1 [accessed 2023-12-15]\n28. Assessing the research landscape and utility of LLMs in the clinical setting: protocol for a scoping review. OSF Registries.\nURL: https://osf.io/498k6 [accessed 2023-12-15]\n29. Deng J, Zhou F, Ali S, Heybati K, Hou W, Huang E, et al. Efficacy and safety of ivermectin for the treatment of COVID-19:\na systematic review and meta-analysis. QJM. 2021;114(10):721-732. [FREE Full text] [doi: 10.1093/qjmed/hcab247]\n[Medline: 34570241]\n30. Deng J, Rayner D, Ramaraju HB, Abbas U, Garcia C, Heybati K, et al. Efficacy and safety of selective serotonin reuptake\ninhibitors in COVID-19 management: a systematic review and meta-analysis. Clin Microbiol Infect. 2023;29(5):578-586.\n[FREE Full text] [doi: 10.1016/j.cmi.2023.01.010] [Medline: 36657488]\n31. Deng J, Zhou F, Hou W, Heybati K, Ali S, Chang O, et al. Efficacy of lopinavir-ritonavir combination therapy for the\ntreatment of hospitalized COVID-19 patients: a meta-analysis. Future Virol. 2021 [FREE Full text] [doi:\n10.2217/fvl-2021-0066] [Medline: 35145560]\n32. Zhou F, Huang E, Zheng E, Deng J. The use of acupuncture in patients with Raynaud's syndrome: a systematic review and\nmeta-analysis of randomized controlled trials. Acupunct Med. 2023;41(2):63-72. [FREE Full text] [doi:\n10.1177/09645284221076504] [Medline: 35608095]\n33. Heybati K, Zhou F, Lynn MJ, Deng J, Ali S, Hou W, et al. Comparative efficacy of adjuvant nonopioid analgesia in adult\ncardiac surgical patients: a network meta-analysis. J Cardiothorac Vasc Anesth. 2023;37(7):1169-1178. [doi:\n10.1053/j.jvca.2023.03.018] [Medline: 37088644]\n34. Zhang Y, Liang S, Feng Y, Wang Q, Sun F, Chen S, et al. Automation of literature screening using machine learning in\nmedical evidence synthesis: a diagnostic test accuracy systematic review protocol. Syst Rev. 2022;11(1):11. [FREE Full\ntext] [doi: 10.1186/s13643-021-01881-5] [Medline: 35031074]\n35. van de Schoot R, de Bruin J, Schram R, Zahedi P, de Boer J, Weijdema F, et al. An open source machine learning framework\nfor efficient and transparent systematic reviews. Nat Mach Intell. 2021;3(2):125-133. [FREE Full text] [doi:\n10.1038/s42256-020-00287-7]\n36. Przybyła P, Brockmeier AJ, Kontonatsios G, Le Pogam M, McNaught J, von Elm E, et al. Prioritising references for\nsystematic reviews with RobotAnalyst: a user study. Res Synth Methods. 2018;9(3):470-488. [FREE Full text] [doi:\n10.1002/jrsm.1311] [Medline: 29956486]\n37. Gates A, Guitard S, Pillay J, Elliott SA, Dyson MP, Newton AS, et al. Performance and usability of machine learning for\nscreening in systematic reviews: a comparative evaluation of three tools. In: AHRQ Methods for Effective Health Care.\nRockville, MD. Agency for Healthcare Research and Quality (US); 2019.\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 10https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n38. Schade M. Fine-tuning a classifier to improve truthfulness. OpenAI. URL: https://help.openai.com/en/articles/\n5528730-fine-tuning-a-classifier-to-improve-truthfulness [accessed 2023-04-20]\n39. Joshua J. What is the difference between the GPT-4 models? OpenAI. URL: https://help.openai.com/en/articles/\n7127966-what-is-the-difference-between-the-gpt-4-models [accessed 2023-04-19]\nAbbreviations\nAPI: application programming interface\nGPT: generative pretrained transformer\nLLM: large language model\nNOA: nonopioid analgesia\nPABAK: prevalence and bias-adjusted kappa\nEdited by T de Azevedo Cardoso, G Eysenbach; submitted 14.05.23; peer-reviewed by T Kang, M Chatzimina, I Bojic; comments to\nauthor 30.08.23; revised version received 30.08.23; accepted 28.09.23; published 12.01.24\nPlease cite as:\nGuo E, Gupta M, Deng J, Park YJ, Paget M, Naugler C\nAutomated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study\nJ Med Internet Res 2024;26:e48996\nURL: https://www.jmir.org/2024/1/e48996\ndoi: 10.2196/48996\nPMID: 38214966\n©Eddie Guo, Mehul Gupta, Jiawen Deng, Ye-Jean Park, Michael Paget, Christopher Naugler. Originally published in the Journal\nof Medical Internet Research (https://www.jmir.org), 12.01.2024. This is an open-access article distributed under the terms of\nthe Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use,\ndistribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet\nResearch, is properly cited. The complete bibliographic information, a link to the original publication on https://www.jmir.org/,\nas well as this copyright and license information must be included.\nJ Med Internet Res 2024 | vol. 26 | e48996 | p. 11https://www.jmir.org/2024/1/e48996\n(page number not for citation purposes)\nGuo et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6769943237304688
    },
    {
      "name": "Workflow",
      "score": 0.5420932769775391
    },
    {
      "name": "Machine learning",
      "score": 0.5140119791030884
    },
    {
      "name": "Python (programming language)",
      "score": 0.48816972970962524
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4499928057193756
    },
    {
      "name": "Natural language processing",
      "score": 0.4294120669364929
    },
    {
      "name": "Information retrieval",
      "score": 0.3956989049911499
    },
    {
      "name": "Data science",
      "score": 0.3635343313217163
    },
    {
      "name": "Data mining",
      "score": 0.3521561026573181
    },
    {
      "name": "Database",
      "score": 0.14752578735351562
    },
    {
      "name": "Programming language",
      "score": 0.11606806516647339
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I168635309",
      "name": "University of Calgary",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    }
  ],
  "cited_by": 122
}