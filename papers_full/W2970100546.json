{
    "title": "Transformer Dissection: An Unified Understanding for Transformer’s Attention via the Lens of Kernel",
    "url": "https://openalex.org/W2970100546",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A5012188239",
            "name": "Yao-Hung Hubert Tsai",
            "affiliations": [
                "Machine Science"
            ]
        },
        {
            "id": "https://openalex.org/A2785652606",
            "name": "Shaojie Bai",
            "affiliations": [
                "Machine Science"
            ]
        },
        {
            "id": "https://openalex.org/A2099049790",
            "name": "Makoto Yamada",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4212702429",
            "name": "Louis-Philippe Morency",
            "affiliations": [
                "Language Science (South Korea)",
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2031945151",
            "name": "Ruslan Salakhutdinov",
            "affiliations": [
                "Machine Science"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2964051877",
        "https://openalex.org/W2891815651",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2789541106",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W3038029863",
        "https://openalex.org/W2962892300",
        "https://openalex.org/W2964190861",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W1560724230",
        "https://openalex.org/W2090923791",
        "https://openalex.org/W2097073572",
        "https://openalex.org/W1571975558",
        "https://openalex.org/W2964135722",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2151416941",
        "https://openalex.org/W2894384847",
        "https://openalex.org/W2908802752",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2982515679",
        "https://openalex.org/W2792764867"
    ],
    "abstract": "Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer's attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer's attention. As an example, we propose a new variant of Transformer's attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.",
    "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 4344–4353,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n4344\nTransformer Dissection: An Uniﬁed Understanding for Transformer’s\nAttention via the Lens of Kernel\nYao-Hung Hubert Tsai1 Shaojie Bai1 Makoto Yamada34\nLouis-Philippe Morency2 Ruslan Salakhutdinov1\n{1Machine Learning Department,2Language Technology Institute}, Carnegie Mellon University\n3Kyoto University 4RIKEN AIP\n{yaohungt, shaojieb, morency, rsalakhu}@cs.cmu.edu, myamada@i.kyoto-u.ac.jp\nAbstract\nTransformer is a powerful architecture that\nachieves superior performance on various se-\nquence learning tasks, including neural ma-\nchine translation, language understanding, and\nsequence prediction. At the core of the Trans-\nformer is the attention mechanism, which con-\ncurrently processes all inputs in the streams.\nIn this paper, we present a new formulation\nof attention via the lens of the kernel. To be\nmore precise, we realize that the attention can\nbe seen as applying kernel smoother over the\ninputs with the kernel scores being the simi-\nlarities between inputs. This new formulation\ngives us a better way to understand individ-\nual components of the Transformer’s attention,\nsuch as the better way to integrate the posi-\ntional embedding. Another important advan-\ntage of our kernel-based formulation is that it\npaves the way to a larger space of compos-\ning Transformer’s attention. As an example,\nwe propose a new variant of Transformer’s at-\ntention which models the input as a product\nof symmetric kernels. This approach achieves\ncompetitive performance to the current state of\nthe art model with less computation. In our\nexperiments, we empirically study different\nkernel construction strategies on two widely\nused tasks: neural machine translation and se-\nquence prediction.\n1 Introduction\nTransformer (Vaswani et al., 2017) is a relative\nnew architecture which outperforms traditional\ndeep learning models such as Recurrent Neural\nNetworks (RNNs) (Sutskever et al., 2014) and\nTemporal Convolutional Networks (TCNs) (Bai\net al., 2018) for sequence modeling tasks across\nneural machine translations (Vaswani et al., 2017),\nlanguage understanding (Devlin et al., 2018), se-\nquence prediction (Dai et al., 2019), image gener-\nation (Child et al., 2019), video activity classiﬁca-\ntion (Wang et al., 2018), music generation (Huang\net al., 2018a), and multimodal sentiment analy-\nsis (Tsai et al., 2019a). Instead of performing re-\ncurrence (e.g., RNN) or convolution (e.g., TCN)\nover the sequences, Transformer is a feed-forward\nmodel that concurrently processes the entire se-\nquence. At the core of the Transformer is its at-\ntention mechanism, which is proposed to integrate\nthe dependencies between the inputs. There are\nup to three types of attention within the full Trans-\nformer model as exempliﬁed with neural machine\ntranslation application (Vaswani et al., 2017): 1)\nEncoder self-attention considers the source sen-\ntence as input, generating a sequence of encoded\nrepresentations, where each encoded token has a\nglobal dependency with other tokens in the in-\nput sequence. 2) Decoder self-attention consid-\ners the target sentence (e.g., predicted target se-\nquence for translation) as input, generating a se-\nquence of decoded representations 1, where each\ndecoded token depends on previous decoded to-\nkens. 3) Decoder-encoder attention considers both\nencoded and decoded sequences, generating a se-\nquence with the same length as the decoded se-\nquence. It should be noted that some applica-\ntions has only the decoder self-attention such as\nsequence prediction (Dai et al., 2019). In all cases,\nthe Transformer’s attentions follow the same gen-\neral mechanism.\nAt the high level, the attention can be seen as\na weighted combination of the input sequence,\nwhere the weights are determined by the similari-\nties between elements of the input sequence. We\nnote that this operation is order-agnostic to the per-\nmutation in the input sequence (order is encoded\nwith extra positional embedding (Vaswani et al.,\n2017; Shaw et al., 2018; Dai et al., 2019)). The\n1The generated sequence can be regarded as a translated\nsequence (i.e., translating from the encoded sequence), where\neach generated token depends on all tokens in the encoded\nsequence.\n4345\nabove observation inspires us to connect Trans-\nformer’s attention to kernel learning (Scholkopf\nand Smola, 2001): they both concurrently and\norder-agnostically process all inputs by calculat-\ning the similarity between the inputs. Therefore,\nin the paper, we present a new formulation for\nTransformer’s attention via the lens of kernel. To\nbe more precise, the new formulation can be in-\nterpreted as a kernel smoother (Wasserman, 2006)\nover the inputs in a sequence, where the kernel\nmeasures how similar two different inputs are. The\nmain advantage of connecting attention to kernel\nis that it opens up a new family of attention mech-\nanisms that can relate to the well-established lit-\nerature in kernel learning (Scholkopf and Smola,\n2001). As a result, we develop a new variant of at-\ntention which simply considers a product of sym-\nmetric kernels when modeling non-positional and\npositional embedding.\nFurthermore, our proposed formulation high-\nlights naturally the main components of Trans-\nformer’s attention, enabling a better understand-\ning of this mechanism: recent variants of Trans-\nformers (Shaw et al., 2018; Huang et al., 2018b;\nDai et al., 2019; Child et al., 2019; Lee et al.,\n2018; Wang et al., 2018; Tsai et al., 2019a)\ncan be expressed through these individual com-\nponents. Among all the components, we argue\nthat the most important one is the construction of\nthe kernel function. We empirically study mul-\ntiple kernel forms and the ways to integrate po-\nsitional embedding in neural machine translation\n(NMT) using IWSLT’14 German-English (De-En)\ndataset (Edunov et al., 2017) and sequence pre-\ndiction (SP) using WikiText-103 dataset (Merity\net al., 2016).\n2 Attention\nThis section aims at providing an understand-\ning of attention in Transformer via the lens of\nkernel. The inspiration for connecting the ker-\nnel (Scholkopf and Smola, 2001) and attention in-\nstantiates from the observation: both operations\nconcurrently processes all inputs and calculate the\nsimilarity between the inputs. We ﬁrst introduce\nthe background (i.e., the original formulation) of\nattention and then provide a new reformulation\nwithin the class of kernel smoothers (Wasserman,\n2006). Next, we show that this new formulation\nallows us to explore new family of attention while\nat the same time offering a framework to cate-\ngorize previous attention variants (Vaswani et al.,\n2017; Shaw et al., 2018; Huang et al., 2018b; Dai\net al., 2019; Child et al., 2019; Lee et al., 2018;\nWang et al., 2018; Tsai et al., 2019a). Last, we\npresent a new form of attention, which requires\nfewer parameters and empirically reaches compet-\nitive performance as the state-of-the-art models.\nFor notation, we use lowercase representing a\nvector (e.g., x), bold lowercase representing a ma-\ntrix (e.g., x), calligraphy letter denoting a space\n(e.g., X), and S denoting a set. To relate the no-\ntations in sequence to sequence learning (Vaswani\net al., 2017), xrepresents a speciﬁc element of a\nsequence, x = [x1,x2,/uni22EF,xT]denotes a sequence\nof features, Sx = {xexp,x2,/uni22EF,xT}represents the\nset with its elements being the features in sequence\nx, and we refer the space of set Sx as S.\n2.1 Technical Background\nUnlike recurrent computation (Sutskever et al.,\n2014) (i.e., RNNs) and temporal convolutional\ncomputation (Bai et al., 2018) (i.e., TCNs), Trans-\nformer’s attention is an order-agnostic operation\ngiven the order in the inputs (Vaswani et al., 2017).\nHence, in the presentation of the paper, we con-\nsider the inputs as a set instead of a sequence.\nWhen viewing sequence as a set, we lose the tem-\nporal (positional) information in inputs which is\noften crucial for sequence modeling (Sutskever\net al., 2014). As a result, Transformer (Vaswani\net al., 2017) introduced positional embedding to\nindicate the positional relation for the inputs. For-\nmally, a sequencex =[x1,x2,/uni22EF,xT]deﬁnes each\nelement as xi =(fi,ti) with fi ∈F being the non-\ntemporal feature at time i and ti ∈T as an tem-\nporal feature (or we called it positional embed-\nding). Note that fi can be the word representa-\ntion (in neural machine translation (Vaswani et al.,\n2017)), a pixel in a frame (in video activity recog-\nnition (Wang et al., 2018)), or a music unit (in mu-\nsic generation (Huang et al., 2018b)). ti can be\na mixture of sine and cosine functions (Vaswani\net al., 2017) or parameters that can be learned dur-\ning back-propagation (Dai et al., 2019; Ott et al.,\n2019). The feature vector are deﬁned over a joint\nspace X ∶= (F ×T ). The resulting permutation-\ninvariant set is: Sx = {x1,x2,/uni22EF,xT} =\n{(f1,t1),(f2,t2),/uni22EF,(fT,tT)}.\nFollowed the deﬁnition by Vaswani et al.\n(2017), we use queries(q)/keys(k)/values(v) to\nrepresent the inputs for the attention. To be\n4346\nmore precise, x{q/slash.leftk/slash.leftv} is used for denoting a\nquery/key/value data in the query/key/value se-\nquence x{q/slash.leftk/slash.leftv} (x{q/slash.leftk/slash.leftv} ∈ Sx{q/slash.leftk/slash.leftv}) with\nSx{q/slash.leftk/slash.leftv}being its set representation. We note that\nthe input sequences are the same ( xq = xk) for\nself-attention and are different ( xq from decoder\nand xk from encoder) for encoder-decoder atten-\ntion.\nGiven the introduced notation, the attention\nmechanism in original Transformer (Vaswani\net al., 2017) can be presented as:\nAttention(xq ; Sxk)\n=softmax /parenleft.alt4xqWq(xkWk)/uni22BA\n√ dk\n/parenright.alt4xkWv\n(1)\nwith xq = fq +tq, xk = fk +tk, Wq/slash.leftk/slash.leftv being\nthe weight, and dk being the feature dimension of\nxkWk. Decoder self-attention further introduces a\nmask to block the visibility of elements in Sxk to\nxq. Particularly, decoder self-attention considers\nthe decoded sequence as inputs ( xk = xq), where\nthe decoded token at timetis not allowed to access\nthe future decoded tokens (i.e., tokens decoded at\ntime greater than t). On the contrary, encoder self-\nattention and decoder-encoder attention consider\nno additional mask to Eq. (1).\nRecent work (Shaw et al., 2018; Dai et al., 2019;\nHuang et al., 2018b; Child et al., 2019; Lee et al.,\n2018; Parmar et al., 2018; Tsai et al., 2019a) pro-\nposed modiﬁcations to the Transformer for the\npurpose of better modeling inputs positional re-\nlation (Shaw et al., 2018; Huang et al., 2018b;\nDai et al., 2019), appending additional keys in\nSxk (Dai et al., 2019), modifying the mask ap-\nplied to Eq. (1) (Child et al., 2019), or applying\nto distinct feature types (Lee et al., 2018; Parmar\net al., 2018; Tsai et al., 2019a). These works adopt\ndifferent designs of attention as comparing to the\noriginal form (Eq. (1)). In our paper, we aim at\nproviding an uniﬁed view via the lens of kernel.\n2.2 Reformulation via the Lens of Kernel\nWe now provide the intuition to reformulate Eq.\n(1) via the lens of kernel. First, the softmax func-\ntion can be realized as a probability function for\nxq observing the keys {xk}s in Sxk (Sxk is the set\nrepresentation of sequence xk). The probability\nis determined by the dot product between xq and\nxk with additional mappings Wq/slash.leftWk and scaling\nby dk, which we note the dot-product operation is\nan instance of kernel function. We also introduce\na set ﬁltering function M(xq,Sxk) ∶X ×S → S\nwhich returns a set with its elements that operate\nwith (or are connected/visible to) xq. The ﬁlter-\ning function M(⋅,⋅) plays as the role of the mask\nin decoder self-attention (Vaswani et al., 2017).\nPutting these altogether, we re-represent Eq. (1)\ninto the following deﬁnition.\nDeﬁnition 1. Given a non-negative kernel func-\ntion k(⋅,⋅) ∶X ×X → R+, a set ﬁltering func-\ntion M(⋅,⋅) ∶X ×S → S, and a value function\nv(⋅) ∶X → Y, the Attention function taking the\ninput of a query feature xq ∈X is deﬁned as\nAttention/parenleft.alt2xq ; M(xq,Sxk)/parenright.alt2\n= /summation.disp\nxk∈M(xq,Sxk)\nk(xq,xk)\n∑xk′∈M(xq,Sxk) k(xq,xk′)v(xk).\n(2)\nThe Deﬁnition 1 is a class of linear\nsmoothers (Wasserman, 2006) with kernel\nsmoothing:\n/summation.disp\nxk∈M(xq,Sxk)\nk(xq,xk)\n∑xk′∈M(xq,Sxk) k(xq,xk′)v(xk)\n= Ep(xk/divides.alt0xq)/bracketleft.alt1v(xk)/bracketright.alt,\nwhere v(xk) outputs the “values” and p(xk/divides.alt0xq) =\nk(xq,xk)\n∑xk′∈M(xq,Sxk) k(xq,xk′) is a probability function\ndepends on k and N when k(⋅,⋅) is always\npositive. In the prior work (Vaswani et al.,\n2017), k(xq,xk) = exp /parenleft.alt1/uni27E8xqWq,xkWk/uni27E9/slash.left√ dk/parenright.alt1\nand v(xk) = xkWv. Note that the kernel form\nk(xq,xk) in the original Transformer (Vaswani\net al., 2017) is a asymmetric exponential ker-\nnel with additional mapping Wq and Wk (Wilson\net al., 2016; Li et al., 2017)2.\nThe new formulation deﬁnes a larger space for\ncomposing attention by manipulating its individ-\nual components, and at the same time it is able to\ncategorize different variants of attention in prior\nwork (Shaw et al., 2018; Huang et al., 2018b; Dai\net al., 2019; Child et al., 2019; Lee et al., 2018;\nWang et al., 2018; Tsai et al., 2019a). In the fol-\nlowing, we study these components by dissect-\ning Eq. (2) into: 1) kernel feature space X, 2)\n2We note that rigorous deﬁnition of kernel func-\ntion (Scholkopf and Smola, 2001) requires the kernel to be\nsemi-positive deﬁnite and symmetric. While in the paper, the\ndiscussion on kernel allows it to be non-semi-positive deﬁnite\nand asymmetric. In Section 3, we will examine the kernels\nwhich are semi-positive and symmetric.\n4347\nkernel construction k(⋅,⋅), 3) value function v(⋅),\nand 4) set ﬁltering function M(⋅,⋅).\n2.2.1 Kernel Feature Space X\nIn Eq. (2), to construct a kernel on X, the\nﬁrst thing is to identify the kernel feature space\nX. In addition to modeling sequences like\nword sentences (Vaswani et al., 2017) or music\nsignals (Huang et al., 2018b), the Transformer\ncan also be applied to images (Parmar et al.,\n2018), sets (Lee et al., 2018), and multimodal se-\nquences (Tsai et al., 2019a). Due to distinct data\ntypes, these applications admit various kernel fea-\nture space:\n(i) Sequence Transformer (Vaswani et al., 2017;\nDai et al., 2019):\nX ∶=(F ×T )\nwith F being non-positional feature space and T\nbeing the positional embedding space of the posi-\ntion in the sequence.\n(ii) Image Transformer (Parmar et al., 2018):\nX ∶=(F ×H ×W)\nwith F being non-positional feature space, H be-\ning the positional space of the height in an image,\nand W being the positional space of the width in\nan image.\n(iii) Set Transformer (Lee et al., 2018) and Non-\nLocal Neural Networks (Wang et al., 2018):\nX ∶=(F)\nwith no any positional information present.\n(iv) Multimodal Transformer (Tsai et al., 2019a):\nX ∶=(Fℓ×Fv ×Fa×T )\nwith Fℓ representing the language feature space,\nFv representing the vision feature space, Fa rep-\nresenting the audio feature space, andT represent-\ning the temporal indicator space.\nFor the rest of the paper, we will focus on the\nsetting for sequence Transformer X = (F ×T )\nand discuss the kernel construction on it.\n2.2.2 Kernel Construction and the Role of\nPositional Embedding k(⋅,⋅)\nThe kernel construction on X = (F ×T ) has dis-\ntinct design in variants of Transformers (Vaswani\net al., 2017; Dai et al., 2019; Huang et al., 2018b;\nShaw et al., 2018; Child et al., 2019). Since now\nthe kernel feature space considers a joint space,\nwe will ﬁrst discuss the kernel construction on F\n(the non-positional feature space) and then discuss\nhow different variants integrate the positional em-\nbedding (with the positional feature space T ) into\nthe kernel.\nKernel construction on F. All the work con-\nsidered the scaled asymmetric exponential kernel\nwith the mapping Wq and Wk (Wilson et al., 2016;\nLi et al., 2017) for non-positional features fq and\nfk:\nkexp(fq,fk) =exp /parenleft.alt4/uni27E8fqWq,fkWk/uni27E9√ dk\n/parenright.alt4. (3)\nNote that the usage of asymmetric kernel is\nalso commonly used in various machine learning\ntasks (Yilmaz, 2007; Tsuda, 1999; Kulis et al.,\n2011), where they observed the kernel form can\nbe ﬂexible and even non-valid (i.e., a kernel that is\nnot symmetric and positive semi-deﬁnite). In Sec-\ntion 3, we show that symmetric design of the ker-\nnel has similar performance for various sequence\nlearning tasks, and we also examine different ker-\nnel choices (i.e., linear, polynomial, and rbf ker-\nnel).\nKernel construction on X = (F ×T ). The de-\nsigns for integrating the positional embedding tq\nand tk are listed in the following.\n(i) Absolute Positional Embedding (Vaswani et al.,\n2017; Dai et al., 2019; Ott et al., 2019): For\nthe original Transformer (Vaswani et al., 2017),\neach ti is represented by a vector with each di-\nmension being sine or cosine functions. For\nlearned positional embedding (Dai et al., 2019; Ott\net al., 2019), each ti is a learned parameter and is\nﬁxed for the same position for different sequences.\nThese works deﬁnes the feature space as the di-\nrect sum of its temporal and non-temporal space:\nX = F /uni2295.bigT . Via the lens of kernel, the kernel\nsimilarity is deﬁned as\nk/parenleft.alt2xq,xk/parenright.alt2∶=kexp/parenleft.alt2fq +tq,fk +tk/parenright.alt2. (4)\n(ii) Relative Positional Embedding in\nTransformer-XL (Dai et al., 2019): t repre-\nsents the indicator of the position in the sequence,\nand the kernel is chosen to be asymmetric of\nmixing sine and cosine functions:\nk/parenleft.alt2xq,xk/parenright.alt2∶=kexp/parenleft.alt2fq,fk/parenright.alt2⋅kfq /parenleft.alt2tq,tk/parenright.alt2(5)\n4348\nwith kfq /parenleft.alt2tq,tk/parenright.alt2being an asymmetric kernel with\ncoefﬁcients inferred by fq: log kfq /parenleft.alt2tq,tk/parenright.alt2=\n∑\n/uni230Adk/slash.left2/uni230B−1\np=0 c2p sin( tq−tk\n10000\n2p\n512\n)+c2p+1 cos( tq−tk\n10000\n2p\n512\n)\nwith [c0,/uni22EF,cdk−1] = fqWqWR where WR is an\nlearned weight matrix. We refer readers to Dai\net al. (2019) for more details.\n(iii) Relative Positional Embedding of Shaw et al.\n(2018) and Music Transformer (Huang et al.,\n2018b): t⋅represents the indicator of the position\nin the sequence, and the kernel is modiﬁed to be\nindexed by a look-up table:\nk/parenleft.alt2xq,xk/parenright.alt2∶=Ltq−tk,fq ⋅kexp/parenleft.alt2fq,fk/parenright.alt2, (6)\nwhere Ltq−tk,fq = exp(fqWqatq−tk) with a⋅being\na learnable matrix having matrix width to be the\nlength of the sequence. We refer readers to Shaw\net al. (2018) for more details.\nDai et al. (2019) showed that the way to inte-\ngrate positional embedding is better through Eq.\n(5) than through Eq. (6) and is better through Eq.\n(6) than through Eq. (4). We argue the reason\nis that if viewing fi and ti as two distinct spaces\n/parenleft.alt2X ∶= (F ×T )/parenright.alt2, the direct sum xi = fi +ti may\nnot be optimal when considering the kernel score\nbetween xq and xk. In contrast, Eq. (5) represents\nthe kernel as a product of two kernels (one for fi\nand another for ti), which is able to capture the\nsimilarities for both temporal and non-temporal\ncomponents.\n2.2.3 Value Function v(⋅)\nThe current Transformers consider two different\nvalue function construction:\n(i) Original Transformer (Vaswani et al., 2017)\nand Sparse Transformer (Child et al., 2019):\nv(xk) =v((fk,tk)) ∶=(fk +tk)Wv. (7)\n(ii) Transformer-XL (Dai et al., 2019), Music\nTransformer (Huang et al., 2018b), Self-Attention\nwith Relative Positional Embedding (Shaw et al.,\n2018):\nv(xk) =v((fk,tk)) ∶=fkWv. (8)\nCompared Eq. (7) to Eq. (8), Eq. (7) takes\nthe positional embedding into account for con-\nstructing the value function. In Section 3, we em-\npirically observe that constructing value function\nwith Eq. (8) constantly outperforms the construc-\ntion with Eq. (7), which suggests that we do not\nneed positional embedding for value function.\n2.2.4 Set Filtering Function M(⋅,⋅)\nIn Eq. (2), the returned set by the set ﬁltering\nfunction M(xq,Sxk) deﬁnes how many keys and\nwhich keys are operating with xq. In the follow-\ning, we itemize the corresponding designs for the\nvariants in Transformers:\n(i) Encoder Self-Attention in original Trans-\nformer (Vaswani et al., 2017): For each query xq\nin the encoded sequence, M(xq,Sxk) = Sxk con-\ntains the keys being all the tokens in the encoded\nsequence. Note that encoder self-attention consid-\ners xq =xk with xq being the encoded sequence.\n(ii) Encoder-Decoder Attention in original Trans-\nformer (Vaswani et al., 2017): For each query xq\nin decoded sequence, M(xq,Sxk) = Sxk contains\nthe keys being all the tokens in the encoded se-\nquence. Note that encode-decoder attention con-\nsiders xq ≠ xk with xq being the decoded se-\nquence and xk being the encoded sequence.\n(iii) Decoder Self-Attention in original Trans-\nformer (Vaswani et al., 2017): For each query\nxq in the decoded sequence, M(xq,Sxk) returns\na subset of Sxk (M(xq,Sxk) ⊂ Sxk). Note that\ndecoder self-attention considers xq = xk with xq\nbeing the decoded sequence. Since the decoded\nsequence is the output for previous timestep, the\nquery at position ican only observe the keys being\nthe tokens that are decoded with position < i. For\nconvenience, let us deﬁneS1 as the set returned by\noriginal Transformer (Vaswani et al., 2017) from\nM(xq,Sxk), which we will use it later.\n(iv) Decoder Self-Attention in Transformer-\nXL (Dai et al., 2019): For each query xq in the de-\ncoded sequence, M(xq,Sxk) returns a set contain-\ning S1 and additional memories ( M(xq,Sxk) =\nS1 +Smem,M(xq,Sxk) ⊃ S1). Smem refers to\nadditional memories.\n(v) Decoder Self-Attention in Sparse Trans-\nformer (Child et al., 2019): For each query xq in\nthe decoded sentence, M(xq,Sxk) returns a sub-\nset of S1 (M(xq,Sxk) ⊂S1).\nTo compare the differences for various designs,\nwe see the computation time is inversely propor-\ntional to the number of elements in M(xq,Sxk).\nFor performance-wise comparisons, Transformer-\nXL (Dai et al., 2019) showed that, the addi-\ntional memories in M(xq,Sxk) are able to capture\nlonger-term dependency than the original Trans-\nformer (Vaswani et al., 2017) and hence results\nin better performance. Sparse Transformer (Child\n4349\net al., 2019) showed that although having much\nfewer elements in M(xq,Sxk), if the elements are\ncarefully chosen, the attention can still reach the\nsame performance as Transformer-XL (Dai et al.,\n2019).\n2.3 Exploring the Design of Attention\nSo far, we see how Eq. (2) connects to the vari-\nants of Transformers. By changing the kernel con-\nstruction in Section 2.2.2, we can deﬁne a larger\nspace for composing attention. In this paper, we\npresent a new form of attention with a kernel that\nis 1) valid (i.e., a kernel that is symmetric and pos-\nitive semi-deﬁnite) and 2) delicate in the sense of\nconstructing a kernel on a joint space (i.e., X =\n(F ×T )):\nk(xq,xk) ∶=kF/parenleft.alt2fq,fk/parenright.alt2⋅kT/parenleft.alt2tq,tk/parenright.alt2\nwith kF(fq,fk) =exp/parenleft.alt2/uni27E8fqWF,fkWF/uni27E9√ dk\n/parenright.alt2\nand kT(tq,tk) =exp/parenleft.alt2/uni27E8tqWT,tkWT/uni27E9√ dk\n/parenright.alt2,\n(9)\nwhere WF and WT are weight matrices. The new\nform considers product of kernels with the ﬁrst\nkernel measuring similarity between non-temporal\nfeatures and the second kernel measuring similar-\nity between temporal features. Both kernels are\nsymmetric exponential kernel. Note that ti here\nis chosen as the mixture of sine and cosine func-\ntions as in the prior work (Vaswani et al., 2017; Ott\net al., 2019). In our experiment, we ﬁnd it reaching\ncompetitive performance as comparing to the cur-\nrent state-of-the-art designs (Eq. (5) by Dai et al.\n(2019)). We ﬁx the size of the weight matrices W⋅\nin Eq. (9) and Eq. (5) which means we save 33%\nof the parameters in attention from Eq. (9) to Eq.\n(5) (Eq. (5) has weights WQ/slash.leftWK/slash.leftWR and Eq. (9)\nhas weights WF/slash.leftWT).\n3 Experiments\nBy viewing the attention mechanism with Eq. (2),\nwe aims at answering the following questions re-\ngarding the Transformer’s designs:\nQ1. What is the suggested way for incorporating\npositional embedding in the kernel function?\nQ2. What forms of kernel are recommended to\nchoose in the attention mechanism? Can we re-\nplace the asymmetric kernel with the symmetric\nversion?\nQ3. Is there any exception that the attention mech-\nanism is not order-agnostic with respect to inputs?\nIf so, can we downplay the role of positional em-\nbedding?\nQ4. Is positional embedding required in value\nfunction?\nWe conduct experiments on neural machine\ntranslation (NMT) and sequence prediction (SP)\ntasks since these two tasks are commonly cho-\nsen for studying Transformers (Vaswani et al.,\n2017; Dai et al., 2019). Note that NMT has\nthree different types of attentions (e.g., encoder\nself-attention, decoder-encoder attention, decoder\nself-attention) and SP has only one type of atten-\ntion (e.g., decoder self-attention). For the choice\nof datasets, we pick IWSLT’14 German-English\n(De-En) dataset (Edunov et al., 2017) for NMT\nand WikiText-103 dataset (Merity et al., 2016) for\nSP as suggested by Edunov et al. (Edunov et al.,\n2017) and Dai et al. (Dai et al., 2019). For fair-\nness of comparisons, we train ﬁve random initial-\nizations and report test accuracy with the highest\nvalidation score. We ﬁx the position-wise opera-\ntions in Transformer 3 and only change the atten-\ntion mechanism. Similar to prior work (Vaswani\net al., 2017; Dai et al., 2019), we report BLEU\nscore for NMT and perplexity for SP.\n3.1 Incorporating Positional Embedding\nIn order to ﬁnd the best way to integrate positional\nembedding (PE), we study different PE incorpora-\ntion in the kernel function k(⋅,⋅) in Eq. (2). Refer-\nring to Sections 2.2.2 and 2.3, we consider four\ncases: 1) PE as direct sum in the feature space\n(see Eq. (4)), 2) PE as a look-up table (see Eq.\n(6)), 3) PE in product kernel with asymmetric ker-\nnel (see Eq. (5)), and 4) PE in product kernel with\nsymmetric kernel (see Eq. (9)). We present the\nresults in Table 1.\nFirst, we see that by having PE as a look-up\ntable, it outperforms the case with having PE as\ndirect-sum in feature space, especially for SP task.\nNote that the look-up table is indexed by the rela-\ntive position (i.e., tq−tk) instead of absolute posi-\ntion. Second, we see that PE in the product kernel\nproposed by Dai et al. (Dai et al., 2019) may not\n3The computation of Transformer can be categorized into\nposition-wise and inter-positions (i.e., the attention mecha-\nnism) operations. Position-wise operations include layer nor-\nmalization, residual connection, and feed-forward mapping.\nWe refer the readers to Vaswani et al. (Vaswani et al., 2017)\nfor more details.\n4350\nTable 1: Incorporating Positional Embedding (PE). NMT stands for neural machine translation on IWSLT’14 De-\nEn dataset (Edunov et al., 2017) and SP stands for sequence prediction on WikiText-103 dataset (Merity et al.,\n2016). ↑means the upper the better and ↓means the lower the better.\nApproach PE Incorporation Kernel Form NMT (BLEU ↑) SP (Perplexity↓)\nVaswani et al. (2017) (Eq. (4)) Direct-Sumkexp/parenleft.alt2fq+tq,fk+tk/parenright.alt233.98 30.97\nShaw et al. (2018) (Eq. (6)) Look-up TableLtq−tk,fq ⋅kexp/parenleft.alt2fq,fk/parenright.alt234.12 27.56\nDai et al. (2019) (Eq. (5)) Product Kernelkexp/parenleft.alt2fq,fk/parenright.alt2⋅kfq/parenleft.alt2tq,tk/parenright.alt233.62 24.10\nOurs (Eq. (9)) Product Kernel kF/parenleft.alt2fq,fk/parenright.alt2⋅kT/parenleft.alt2tq,tk/parenright.alt234.71 24.28\nTable 2: Kernel Types. Other than manipulating the kernel choice of the non-positional features, we ﬁx the\nconﬁguration by Vaswani et al. (2017) for NMT and the conﬁguration by Dai et al. (2019) for SP.\nType Kernel Form NMT (BLEU↑) SP (Perplexity ↓)\nAsym. (Wq≠Wk) Sym. ( Wq=Wk) Asym. ( Wq≠Wk) Sym. ( Wq=Wk)\nLinear /uni27E8faWq,fbWk/uni27E9 not converge not converge not converge not converge\nPolynomial /parenleft.alt2/uni27E8faWq,fbWk/uni27E9/parenright.alt2\n2\n32.72 32.43 25.91 26.25\nExponential exp/parenleft.alt2/uni27E8faWq,fbWk/uni27E9√dk /parenright.alt233.98 33.78 24.10 24.01\nRBF exp/parenleft.alt2−/parallel.alt1faWq−fbWk/parallel.alt12\n√dk /parenright.alt234.26 34.14 24.13 24.21\nconstantly outperform the other integration types\n(it has lower BLEU score for NMT). Our proposed\nproduct kernel reaches the best result in NMT and\nis competitive to the best result in SP.\n3.2 Kernel Types\nTo ﬁnd the best kernel form in the attention mecha-\nnism, in addition to the exponential kernel (see Eq.\n(3)), we compare different kernel forms (i.e., lin-\near, polynomial, and rbf kernel) for the non-\npositional features. We also provide the results\nfor changing asymmetric to the symmetric kernel,\nwhen forcing Wq = Wk, so that the resulting ker-\nnel is a valid kernel (Scholkopf and Smola, 2001).\nThe numbers are shown in Table 2. Note that, for\nfairness, other than manipulating the kernel choice\nof the non-positional features, we ﬁx the conﬁg-\nuration by Vaswani et al. (Vaswani et al., 2017)\nfor NMT and the conﬁguration by Dai et al. (Dai\net al., 2019) for SP.\nWe ﬁrst observe that the linear kernel does not\nconverge for both NMT and SP. We argue the rea-\nson is that the linear kernel may have negative\nvalue and thus it violates the assumption in ker-\nnel smoother that the kernel score must be pos-\nitive (Wasserman, 2006). Next, we observe the\nkernel with inﬁnite feature space (i.e., exponen-\ntial and rbf kernel) outperforms the kernel with ﬁ-\nnite feature space (i.e., polynomial kernel). And\nwe see rbf kernel performs the best for NMT and\nexponential kernel performs the best for SP. We\nconclude that the choice of kernel matters for the\ndesign of attention in Transformer. Also, we see\nno much performance difference when comparing\nasymmetric to symmetric kernel. In the experi-\nment, we ﬁx the size of W⋅in the kernel, and thus\nadopting the symmetric kernel beneﬁts us from\nsaving parameters.\n3.3 Order-Invariance in Attention\nThe need of the positional embedding (PE) in the\nattention mechanism is based on the argument that\nthe attention mechanism is an order-agnostic (or,\npermutation equivariant) operation (Vaswani et al.,\n2017; Shaw et al., 2018; Huang et al., 2018b;\nDai et al., 2019; Child et al., 2019). However,\nwe show that, for decoder self-attention, the op-\neration is not order-agnostic. For clariﬁcation,\nwe are not attacking the claim made by the prior\nwork (Vaswani et al., 2017; Shaw et al., 2018;\nHuang et al., 2018b; Dai et al., 2019; Child et al.,\n2019), but we aim at providing a new look at the\norder-invariance problem when considering the at-\ntention mechanism with masks (masks refer to the\nset ﬁltering function in our kernel formulation). In\nother words, previous work did not consider the\n4351\nTable 3: Order-Invariance in Attention. To save the space, we denote Encoder Self-Attention / Encoder-Decoder\nAttention / Decoder Self-Attention as A/B/C. Note that SP only has decoder self-attention.\nApproach Positional Embedding NMT (BLEU↑)\nOurs (Eq. (9)) In A/B/C 34.71\nOurs (Eq. (9)) In A/B 34.49\nNo Positional Embedding none 14.47\nApproach Positional Embedding SP (Perplexity↓)\nVaswani et al. (2017) (Eq.\n(4))\nIn C 30.97\nOurs (Eq. (9) In C 24.28\nNo Positional Embedding none 30.92\nTable 4: Positional Embedding in Value Function.\nI: Value Function Considering Positional Embedding (Eq. (7)) / II: Value Function Considering no Positional Embedding (Eq. (8))\nApproach NMT (BLEU↑) SP (Perplexity ↓)\nI/parenleft.alt2v(xk)∶=(fk+tk)WV/parenright.alt2II/parenleft.alt2v(xk)∶=fkWV/parenright.alt2I/parenleft.alt2v(xk)∶=(fk+tk)WV/parenright.alt2II/parenleft.alt2v(xk)∶=fkWV/parenright.alt2\nVaswani et al. (2017) (Eq. (4)) 33.98 34.02 30.97 30.50\nShaw et al. (2018) (Eq. (6)) 34.04 34.12 27.56 27.45\nDai et al. (2019) (Eq. (5)) 33.32 33.62 24.18 24.10\nOurs (Eq. (9)) 34.60 34.71 24.42 24.28\nmask between queries and keys when discussing\nthe order-invariance problem (Pérez et al., 2019).\nTo put it formally, we ﬁrst present the deﬁnition\nby Lee et al. (2018) for a permutation equivariance\nfunction:\nDeﬁnition 2. Denote Π as the set of all permu-\ntations over [n] = {1,/uni22EF,n}. A function func ∶\nXn → Yn is permutation equivariant iff for any\npermutation π∈Π, func(πx) =πfunc(x).\nLee et al. (2018) showed that the standard atten-\ntion (encoder self-attention (Vaswani et al., 2017;\nDai et al., 2019) ) is permutation equivariant.\nHere, we present the non-permutation-equivariant\nproblem on the decoder self-attention:\nProposition 1. Decoder self-attention (Vaswani\net al., 2017; Dai et al., 2019) is not permutation\nequivariant.\nTo proceed the proof, we need the following\ndeﬁnition and propositions.\nDeﬁnition 3. Denote Π as the set of all permuta-\ntions over [n]= {1,/uni22EF,n}and Sπ\nxk as performing\npermutation π over Sxk. Attention(xq; Sxk) is\nsaid to be permutation equivariant w.r.t. Sxk if\nand only if for any π ∈Π, Attention(xq; Sπ\nxk) =\nAttention(xq; Sxk).\nProposition 2. Attention with the set ﬁltering\nfunction M(xq,Sxk) = Sxk is permutation equiv-\nariant w.r.t.Sxk.\nProof. It is easy to show that if M(xq,Sxk) =\nSxk, Eq. (2) remains unchanged for any permu-\ntation πperformed on Sxk. /uni220E\nProposition 3. Attention with the set difference\nSxk /uni2216M(xq,Sxk) ≠ φis not permutation equiv-\nariant w.r.t.Sxk.\nProof. First, suppose that ˆx∈Sxk /uni2216M(xq,Sxk).\nThen, we construct a permutation π such that\nˆx ∈ M(xq,Sπ\nxk). It is obvious that Eq.\n(2) changes after this permutation and thus\nAttention/parenleft.alt2xq ; M(xq,Sxk)/parenright.alt2is not permutation\nequivariant w.r.t. Sxk. /uni220E\nProof. [Proof for Proposition 1] First, we have\nxq ∼ Sxk. Hence, showing Attention(xq; Sxk)\nnot permutation equivariant w.r.t. Sxk equals to\nshowing Attention not permutation equivariant.\nThen, since the decoder self-attention considers\nmasking (i.e., M(xq,Sxk) returns a subset of\nSxk), by Proposition 3, the decoder self-attention\nis not permutation equivariant. /uni220E\nIn fact, not only being a permutation inequivari-\nant process, the decoding process in the decoder\nself-attention already implies the order informa-\ntion from the data. To show this, take the decoded\nsequence y = [init,y1,y2,y3,y4]as an example.\ninit stands for the initial token. When determin-\ning the output y1 from init, the set ﬁltering func-\ntion is M(init,Sy) = {init}. Similarly, we will\nhave M(y1,Sy),M(y2,Sy),M(y3,Sy) to be\n{init,y1},{init,y1,y2},{init,y1,y2,y3}. Then,\nit raises a concern: do we require PE in decoder\nself-attention? By removing PE in decoder self-\nattention, we present the results in Table 3. From\nthe table, we can see that, for NMT, removing PE\n4352\nonly in decoder self-attention results in slight per-\nformance drop (from 34.71 to 34.49). However,\nremoving PE in the entire model greatly degrades\nthe performance (from 34.71 to 14.47). On the\nother hand, for SP, removing PE from our pro-\nposed attention variant dramatically degrades the\nperformance (from 24.28 to 30.92). Nonetheless,\nthe performance is slightly better than considering\nPE from the original Transformer (Vaswani et al.,\n2017).\n3.4 Positional Embedding in Value Function\nTo determine the need of positional embedding\n(PE) in value function, we conduct the experi-\nments by adopting Eq. (7) or Eq. (8) in the at-\ntention mechanism. The results are presented in\nTable 4. From the table, we ﬁnd that considering\nPE in value function (Eq. (7)) does not gain per-\nformance as compared to not considering PE in\nvalue function (Eq. (8)).\n3.5 Take-Home Messages\nBased on the results and discussions, we can now\nanswer the questions given at the beginning of\nthis section. The answers are summarized into the\ntake-home messages in the following.\nA1. We show that integrating the positional em-\nbedding in the form of product kernel (Eq. (5)\nor Eq. (9)) gives us best performance.\nA2. The kernel form does matter. Adopting ker-\nnel form with inﬁnite feature dimension (i.e., ex-\nponential kernel or rbf kernel) gives us best results.\nThe symmetric design of the kernel may beneﬁt\nus from saving parameters and barely sacriﬁce the\nperformance as compared to the non-symmetric\none.\nA3. The decoder self-attention is not an order-\nagnostic operation with respect to the order of in-\nputs. However, incorporating positional embed-\nding into the attention mechanism may still im-\nprove performance.\nA4. We ﬁnd that there is no much performance\ndifference by considering or not considering the\npositional embedding in value function.\n4 Related Work\nOther than relating Transformer’s attention mech-\nanism with kernel methods, the prior work (Wang\net al., 2018; Shaw et al., 2018; Tsai et al.,\n2019b) related the attention mechanism with\ngraph-structured learning. For example, Non-\nLocal Neural Networks (Wang et al., 2018) made\na connection between the attention and the non-\nlocal operation in image processing (Buades et al.,\n2005). Others (Shaw et al., 2018; Tsai et al.,\n2019b) linked the attention to the message passing\nin graphical models. In addition to the fundamen-\ntal difference between graph-structured learning\nand kernel learning, the prior work (Wang et al.,\n2018; Shaw et al., 2018; Tsai et al., 2019b) fo-\ncused on presenting Transformer for its particular\napplication (e.g., video classiﬁcation (Wang et al.,\n2018) and neural machine translation (Shaw et al.,\n2018)). Alternatively, our work focuses on pre-\nsenting a new formulation of Transformer’s atten-\ntion mechanism that gains us the possibility for\nunderstanding the attention mechanism better.\n5 Conclusions\nIn this paper, we presented a kernel formulation\nfor the attention mechanism in Transformer, which\nallows us to deﬁne a larger space for designing at-\ntention. As an example, we proposed a new vari-\nant of attention which reaches competitive perfor-\nmance when compared to previous state-of-the-art\nmodels. Via the lens of the kernel, we were able\nto better understand the role of individual com-\nponents in Transformer’s attention and categorize\nprevious attention variants in a uniﬁed formula-\ntion. Among these components, we found the con-\nstruction of the kernel function acts the most im-\nportant role, and we studied different kernel forms\nand the ways to integrate positional embedding on\nneural machine translation and sequence predic-\ntion. We hope our empirical study may potentially\nallow others to design better attention mechanisms\ngiven their particular applications.\nAcknowledgments\nWe thank Zhilin Yang for helpful discussion on the\npositional encoding in Transformer’s Attention.\nThis work was supported in part by the DARPA\ngrant FA875018C0150, Ofﬁce of Naval Research\ngrant N000141812861, AFRL CogDeCON, NSF\nAwards #1734868 #1722822, National Institutes\nof Health, JST PRESTO program JPMJPR165A,\nand Apple. We would also like to acknowledge\nNVIDIA’s GPU support.\n4353\nReferences\nShaojie Bai, J Zico Kolter, and Vladlen Koltun.\n2018. An empirical evaluation of generic convolu-\ntional and recurrent networks for sequence model-\ning. arXiv preprint arXiv:1803.01271.\nAntoni Buades, Bartomeu Coll, and J-M Morel. 2005.\nA non-local algorithm for image denoising. In\n2005 IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition (CVPR’05) ,\nvolume 2, pages 60–65. IEEE.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nSergey Edunov, Myle Ott, Michael Auli, David Grang-\nier, and Marc’Aurelio Ranzato. 2017. Classical\nstructured prediction losses for sequence to se-\nquence learning. arXiv preprint arXiv:1711.04956.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Noam Shazeer, Curtis Hawthorne, An-\ndrew M Dai, Matthew D Hoffman, and Douglas Eck.\n2018a. An improved relative self-attention mecha-\nnism for transformer with application to music gen-\neration. arXiv preprint arXiv:1809.04281.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Ian Simon, Curtis Hawthorne, Noam\nShazeer, Andrew M Dai, Matthew D Hoffman,\nMonica Dinculescu, and Douglas Eck. 2018b. Mu-\nsic transformer: Generating music with long-term\nstructure.\nBrian Kulis, Kate Saenko, and Trevor Darrell. 2011.\nWhat you saw is not what you get: Domain adapta-\ntion using asymmetric kernel transforms. In CVPR\n2011, pages 1785–1792. IEEE.\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam R Ko-\nsiorek, Seungjin Choi, and Yee Whye Teh. 2018. Set\ntransformer. arXiv preprint arXiv:1810.00825.\nChun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yim-\ning Yang, and Barnabás Póczos. 2017. Mmd gan:\nTowards deeper understanding of moment matching\nnetwork. In Advances in Neural Information Pro-\ncessing Systems, pages 2203–2213.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018. Image transformer. arXiv preprint\narXiv:1802.05751.\nJorge Pérez, Javier Marinkovi ´c, and Pablo Barceló.\n2019. On the turing completeness of mod-\nern neural network architectures. arXiv preprint\narXiv:1901.03429.\nBernhard Scholkopf and Alexander J Smola. 2001.\nLearning with kernels: support vector machines,\nregularization, optimization, and beyond . MIT\npress.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. arXiv preprint arXiv:1803.02155.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nLouis-Philippe Morency, and Ruslan Salakhutdinov.\n2019a. Multimodal transformer for unaligned mul-\ntimodal language sequences. ACL.\nYao-Hung Hubert Tsai, Santosh Divvala, Louis-\nPhilippe Morency, Ruslan Salakhutdinov, and Ali\nFarhadi. 2019b. Video relationship reasoning using\ngated spatio-temporal energy graph. CVPR.\nKoji Tsuda. 1999. Support vector classiﬁer with asym-\nmetric kernel functions. In in European Symposium\non Artiﬁcial Neural Networks (ESANN. Citeseer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and\nKaiming He. 2018. Non-local neural networks. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7794–7803.\nLarry Wasserman. 2006. All of nonparametric statis-\ntics. Springer Science & Business Media.\nAndrew Gordon Wilson, Zhiting Hu, Ruslan Salakhut-\ndinov, and Eric P Xing. 2016. Deep kernel learning.\nIn Artiﬁcial Intelligence and Statistics , pages 370–\n378.\nAlper Yilmaz. 2007. Object tracking by asymmetric\nkernel mean shift with automatic scale and orienta-\ntion selection. In 2007 IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 1–6.\nIEEE."
}