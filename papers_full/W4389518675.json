{
    "title": "Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?",
    "url": "https://openalex.org/W4389518675",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5100444201",
            "name": "Yichi Zhang",
            "affiliations": [
                null,
                "New School",
                "University of California, Berkeley",
                "University of Michigan"
            ]
        },
        {
            "id": "https://openalex.org/A5112313376",
            "name": "Jiayi Pan",
            "affiliations": [
                null,
                "New School",
                "University of California, Berkeley",
                "University of Michigan"
            ]
        },
        {
            "id": "https://openalex.org/A5084438004",
            "name": "Yuchen Zhou",
            "affiliations": [
                null,
                "New School",
                "University of California, Berkeley",
                "University of Michigan"
            ]
        },
        {
            "id": "https://openalex.org/A5030097622",
            "name": "Rui Pan",
            "affiliations": [
                null,
                "New School",
                "University of California, Berkeley",
                "University of Michigan"
            ]
        },
        {
            "id": "https://openalex.org/A5026638047",
            "name": "Joyce Chai",
            "affiliations": [
                null,
                "New School",
                "University of California, Berkeley",
                "University of Michigan"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1574145485",
        "https://openalex.org/W3200213953",
        "https://openalex.org/W4225323055",
        "https://openalex.org/W2079549534",
        "https://openalex.org/W3103072285",
        "https://openalex.org/W4246356407",
        "https://openalex.org/W2145936547",
        "https://openalex.org/W4366330503",
        "https://openalex.org/W2970633943",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2107176174",
        "https://openalex.org/W4285387508",
        "https://openalex.org/W3016211260",
        "https://openalex.org/W1787226621",
        "https://openalex.org/W4248486038",
        "https://openalex.org/W3083092310",
        "https://openalex.org/W4319452268",
        "https://openalex.org/W4283208789",
        "https://openalex.org/W2982143932",
        "https://openalex.org/W3217211865",
        "https://openalex.org/W2156727763",
        "https://openalex.org/W4322627058",
        "https://openalex.org/W4281633595",
        "https://openalex.org/W2970365306",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2059243251",
        "https://openalex.org/W4287777324",
        "https://openalex.org/W4376312115",
        "https://openalex.org/W3205187079",
        "https://openalex.org/W4312791030"
    ],
    "abstract": "Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human’s perception of reality isn’t always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are available at [github.com/vl-illusion/dataset](https://github.com/vl-illusion/dataset).",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5718–5728\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nGrounding Visual Illusions in Language:\nDo Vision-Language Models Perceive Illusions Like Humans?\nYichi Zhang\nUniversity of Michigan\nzhangyic@umich.edu\nJiayi Pan∗\nUniversity of California, Berkeley\njiayipan@berkeley.edu\nYuchen Zhou∗\nThe New School\nzhoua926@newschool.edu\nRui Pan\nNorthwestern University\nrui.pan@kellogg.northwestern.edu\nJoyce Chai\nUniversity of Michigan\nchaijy@umich.edu\nAbstract\nVision-Language Models (VLMs) are trained\non vast amounts of data captured by humans\nemulating our understanding of the world.\nHowever, known as visual illusions, human’s\nperception of reality isn’t always faithful to\nthe physical world. This raises a key question:\ndo VLMs have the similar kind of illusions as\nhumans do, or do they faithfully learn to rep-\nresent reality? To investigate this question, we\nbuild a dataset containing five types of visual\nillusions and formulate four tasks to examine\nvisual illusions in state-of-the-art VLMs. Our\nfindings have shown that although the overall\nalignment is low, larger models are closer to\nhuman perception and more susceptible to vi-\nsual illusions. Our dataset and initial findings\nwill promote a better understanding of visual\nillusions in humans and machines and provide\na stepping stone for future computational mod-\nels that can better align humans and machines\nin perceiving and communicating about the\nshared visual world. The code and data are\navailable at github.com/vl-illusion/dataset.\n1 Introduction\nIt’s well established that human perceptual systems\nare susceptible to visual illusions, which are de-\nfined as “consistent and persistent discrepancies\nbetween a physical state of affairs and its represen-\ntation in consciousness” (Day, 1984). Figure 1\nshows a well-known example - the checker shadow\nillusion (Adelson, 1995). Here, a cylinder on the\nchecker board creates a shadow on the board. Hu-\nman viewers are directed to look at the two squares\nA and B as shown in Figure 1(a). To most normal-\nsighted people, they will perceive that square A is\n∗Work done while the author was an undergraduate re-\nsearch assistant at the University of Michigan.\n(a)\n (b)\nFigure 1: The checker shadow illusion (Adelson, 1995).\ndarker than square B. However, the reality is, the\ncolor pixels of A and B are exactly the same, as\nshown in Figure 1(b). This example demonstrates\nthat while the physical attributes of A and B are the\nsame, from humans’ eyes, they may look different,\nwhich may further influence how language is used\nto describe these objects.\nMotivated by human visual illusion phenomena,\nrecent years have seen an increasing amount of\nwork in machine visual illusions (Gomez-Villa\net al., 2019, 2020; Hirsch and Tal, 2020; Sun and\nDekel, 2021; Lonnqvist et al., 2021). These pre-\nvious works were solely based on vision, for ex-\nample, evaluating how the internal representation\nfrom a computer vision model can be used as a\nproxy of stimulus compared to human’s stimulus\nshift under illusions. Most previous experiments\nwere conducted in a case-by-case manner, without\naddressing general behaviors through a systematic\ninvestigation.\nDifferent from these previous works, this paper\nstudies visual illusion from a new angle, in the\ncontext of language communication. Language\ncomprehension and language production are tightly\nlinked to how we perceive the visual world. Back to\nFigure 1(a), when two people are observing the fig-\nure together, due to their likely shared illusion, the\n5718\nexpression “the darker square” will lead to the same\nreference of square A. But when a human com-\nmunicates with a machine, will the machine also\ninterpret “the darker square” as square A? Given\nthe rise of large vision-language models (VLM),\nit’s important to understand whether these VLM\nmodels have a similar tendency to visual illusions,\nand to what degree they may align with human vi-\nsion. The answers to these questions will further\nimpact the alignment of the grounded language\ncommunication between humans and machines.\nTo address these questions, we created a new\nvisual illusion dataset covering five different cate-\ngories from the cognitive literature. Based on the\ndataset, we created a benchmark, Grounding Vi-\nsual Illusion in Language ( GVIL), which consists\nof four subtasks: Same-Difference Question An-\nswering (SameDiffQA), Referential Question An-\nswering (RefQA), Attribute Question Answering\n(AttrQA), and Referential Localization (RefLoc) to\nassess machines’ alignment with the human un-\nder visual illusions. We specifically evaluated four\nstate-of-the-art vision-language models: Unified-\nIO (Lu et al., 2022), OFA (Wang et al., 2022),\nLLaV A (Liu et al., 2023) and InstructBLIP (Dai\net al., 2023). Our results have shown that these\nfour models mostly do not align with human vision\nillusions, especially for QA-based tasks. However,\nfor the RefLoc task, these models (especially ones\nwith larger parameters) have demonstrated an im-\npressive alignment with humans.\nTo our knowledge, this is the first work that takes\nlanguage into consideration to study machine vi-\nsual illusion. There are two main contributions of\nthis work. First, this investigation provides an ini-\ntial understanding of the alignment between human\nand machine visual illusions. Such understanding\nwill enable techniques for a better communicative\ngrounding in situated language communication and\nhelp develop more reliable embodied agents in the\nfuture. Second, unlike using internal representa-\ntions to explain illusions, language can be used\nas a proxy to demonstrate whether and how ma-\nchine illusions match or mis-match with the hu-\nman illusion. This benchmark will not only facili-\ntate computational models for better human agent\nalignment, but also provide tools for scientific un-\nderstanding of visual illusion in both humans and\nmachines.\n2 Related Work\nHuman Visual Illusion Visual illusions in hu-\nmans are instances where human subjective per-\nceived properties, such as color or size, deviates\nfrom their true physical characteristics (Carbon,\n2014). This underscores the fact that the human\nbrain doesn’t perfectly replicate physical features;\nrather, it integrates contextual information and prior\nknowledge to form the perceptual experiences (Car-\nbon, 2014).\nVisual illusions can affect human behavior in\nmultiple ways. Research has shown that human ac-\ntion cannot resist visual illusions (Gentilucci et al.,\n1996; Franz, 2001; Carey, 2001), so is language\ncomprehension and language production. Such\nfindings catalyze inquiries regarding the capabil-\nity of models to comprehend language instructions\nbased on human perceptions and align them with\nhuman intent.\nMachine Visual Illusion. Previous studies have\nsignificantly advanced our ability to examine visual\nillusions by providing systematic data and tools.\nThese efforts include the introduction of tools for\ncalculating and generating illusory images system-\natically (Hirsch and Tal, 2020; Fan and Zeng, 2023),\nthe development of open-source software with a\nparametric framework for controlled illusion gen-\neration (Makowski et al., 2021), and the proposal\nof a framework synthesizing new visual illusions\nusing automatic differentiation techniques (Gomez-\nVilla et al., 2022). With the goal of evaluating\nmachine visual illusions, prior research (Gomez-\nVilla et al., 2019, 2020; Afifi and Brown, 2019;\nBenjamin et al., 2019) has also demonstrated that\nconvolutional neural networks trained on ImageNet\nor low-level vision tasks can be misled by certain\nvisual illusions, similar to human responses. These\nworks have formed a foundation for scalable and\nreproducible research on machine illusions.\nUnlike prior research focusing exclusively on\nvision models, our study introduces a novel and\nunique angle by presenting the first dataset offering\nnatural language annotations for the evaluation of\nmachine-processed visual illusions. This work in-\ntends to bridge the current gap and facilitate future\nevaluations of vision-language models concerning\ntheir alignment with human visual illusions. This\nnovel perspective illuminates future improvements\nin human-machine alignment and promotes the cru-\ncial role of human language as the interaction inter-\n5719\nface with machines.\nFoundation Vision-Language Models. Recent\nadvancements in foundational vision-language\nmodels (VLMs) have shown impressive results\nacross a broad spectrum of tasks (OpenAI, 2023;\nWang et al., 2022; Lu et al., 2022; Alayrac et al.,\n2022; Radford et al., 2021). These models, acting\nas user interfaces, interact with users through both\nlanguage and visuals, necessitating a deep under-\nstanding of human intent and an alignment with hu-\nman values to make them more useful. While pre-\nvious research has primarily focused on language-\nbased uni-modal alignment problems (Ouyang\net al., 2022; Kosinski, 2023), our work offers a\nfresh perspective. Centered on the intersection of\nVLM’s perception capability and human cognitive\nbiases, we investigate to what degree they can un-\nderstand humans and align with human intentions\nunder visual illusions.\n3 The Grounding Visual Illusion in\nLanguage (GVIL) Benchmark\nTo facilitate our investigation, we created a bench-\nmark for evaluating machine visual illusions in the\ncontext of grounded communication. This bench-\nmark is built upon a set of images with visual illu-\nsions. Each image consists of two objects which\nmay look different to humans but are actually iden-\ntical in their pixels. This setup has two advantages.\nFirst, the definition of illusion is clear and non-\nambiguous, thus it is easy to measure whether the\nmachine has a similar illusion as humans. Sec-\nondly, the multi-object setup naturally supports the\nevaluation of language grounding, such as evalu-\nating whether the machine can select the object\nan expression grounds to under the illusion (i.e.,\nsquare A is what \"the darker square\" grounds to in\nFigure1(a)).\n3.1 Data Collection\nOur dataset encapsulates five distinct types of illu-\nsions, each reflecting different elements of human\nphysiological and cognitive processes (Gregory,\n1997; Kitaoka, 2010; Robinson, 2013). Table 1 dis-\nplays a sample of each illusion type, along with a\ndetailed description.\nThese illusions can be categorized into two broad\nareas: color and geometric illusions. For color illu-\nsions, we adopt the classifications of color con-\nstancy, assimilation, and simultaneous contrast\n(MacEvoy, 2005). In terms of geometric illusions,\nColor Constancy\nThe red ship on the left still looks red\nafter applying a blue filter, the blue\nship on the right still looks blue af-\nter applying a red filter, even though\nthe RGB colors of both ships are the\nsame.\nColor Assimilation\nThe two circles have the same\ncolor, while the one on the\nleft looks red (due to its neigh-\nbor/foreground) and the one\non the right looks orange.\nColor Contrast\nThe two grey circles have the\nsame color, while the one on\nthe left looks lighter and the\none on the right looks darker.\nGeometrical Relativity\nThe two orange circles have\nthe same size, while the one on\nthe left looks smaller and the\none on the right looks bigger.\nGeometrical Perspective\nThe two people have the same\nheight, while the one on the\nleft looks shorter and the one\non the right looks taller.\nTable 1: Example illusion from each category and the\ncorresponding explanations.\nwe only included distortions among the four cat-\negories in Robinson’s illusion classification in or-\nder to fix the need for a comparative assessment.\nThe illusions we used to generate our data include\nDelboeuf (Delboeuf, 1865), Ebbinghaus, and Jas-\ntrow illusions (Jastrow, 1892) for relativity, and\nMüller-Lyer (Müller-Lyer, 1889) and Ponzo illu-\nsions (Ponzo, 1911) for perspective distortion. The\nfollowing explanations give an overview of the hu-\nman perception phenomena underlying each cate-\ngory:\n• Color Constancyrefers to phenomenon where\nthe color of an object remains constant perceptu-\nally, despite changes in lighting conditions.\n• Color Assimilation shows how two adjacent\ncolor patches influence each other’s perceptual\nappearance, reducing the perceived color differ-\nence.\n• Color Contrast.The perceived color of an ob-\nject shifts opposite to the color of its surround-\nings\n5720\nFigure 2: Data augmentation examples for the Ebbing-\nhaus illusion.\n• Geometrical Relativityrefers to the distortion\nin the perceived shape or size of an object due to\nthe influence of surrounding oobjects.\n• Geometrical Perspectivereflects the tendency\nof our visual system to perceive perceptually dis-\ntant objects as larger than nearby ones of the\nsame size.\nFor each illusion type, we first collected several\nroot images from the literature (Todorovi´c, 2020)\nand online resources 1. We manually identify at-\ntributes that can be changed without impacting the\neffect of illusion (e.g., the color of objects in geo-\nmetric illusions, or the position of objects in color\nillusions), and edit them to create more illusion\ninstances of the same type, to enrich the number of\nimages in our dataset. We show some augmentation\nexamples in Figure 2.\nThe statistics of our dataset is shown in Table 2.\nNote that since this dataset is only used for the eval-\nuation purpose, i.e., to assess machine’s alignment\nwith human in visual illusion, we chose quality\nover quantity. The dataset is modest in size as each\ninstance is carefully selected (or augmented) based\non cognitive literature. Nonetheless, our infras-\ntructure is also designed to foster the continuous\ndevelopment and augmentation of this dataset, al-\nlowing for community contributions, for instance.\nIt will become an important resource to not only\nsupport a better understanding of machine/human\nvisual illusion, but also facilitate the adaptation of\ncomputational models to visual illusions.\n3.2 Benchmark Tasks\nWe define four vision-language tasks targeting dif-\nferent model capabilities.\nSame-Different Question Answering (SameDif-\nfQA) aims at evaluating the ability of recognizing\n1https://michaelbach.de/ot/\nCategory #Root #Image #Instance\nColor Constancy 3 6 96\nColor Assimilation 5 34 544\nColor Contrast 3 30 480\nGeometrical Relativity 3 20 320\nGeometrical Perspective 2 10 160\nTotal 16 100 1600\nTable 2: Dataset statistics.\nIMG1 (illusion-free)IMG2 (illusion-induced)\nQ: Are the two balls the same color or different?\nDifferent\nSame\nDifferent\nSame\nHumanlikeSame\nSame\nNo-Illusion···\nDifferent\nN/A\nIMG2:\nIMG1:\nFigure 3: Illustration of the SameDiffQA setup. For each\ninstance, the model is asked about its perception of an\nobject property across two images, one illusion-free and\none illusion-induced. For valid illusion evaluation, the\nmodel must initially identify identical properties in the\nillusion-free image.\nillusions. As shown in Figure 3, each question\nconcerns a pair of images ( IMG1 and IMG2). One\nimage (IMG1) is illusion-free where two objects\n(blue balls) are identical in color. The other image\n(IMG2) is induced with an effect of illusion where\ntwo balls appear in different colors (blue on the left\nand green on the right) although their pixels are the\nsame as in IMG1. The model is tasked to answer\nwhether two objects are the same color for each of\nthe images. From a human’s perspective, the an-\nswer would be “Same” to IMG1 and “Different” to\nIMG2 due to the visual illusion. If the model gives\nthe answer ‘Same” toIMG1 and “Different” toIMG2,\nthen the answers align with human’s answers and\ntherefore the model is considered “human-like\". If\nthe model gives “Same” to both images, it implies\nthat the model is faithful to reality and does not per-\nceive the same illusion as humans do. If the model\nanswers “Different” to IMG1, it means it lacks ba-\nsic ability to correctly perceive reality and these\ncases are considered not applicable to our illusion\nevaluation.\n5721\nWhile SameDiffQA focuses on the detection of\nthe presence of illusions, we design three tasks to\nexamine how well do machines align with humans\nwhen communication happens under the influence\nof illusions. Since it is reported that models tend\nto take shortcut by giving an answer purely based\non the text question without looking at the image\n(Goyal et al., 2017), we propose a paired test to\nreduce the evaluation bias. As shown in Figure 4,\neach instance comes with two images: one original\nillusion image (IMG1) and one imageIMG2 that flips\nthe objects from the original image (IMG1) in a way\nthat will also invert the answer to the question.\nSpecifically, we evaluate the following three as-\npects:\nReferential Question Answering (RefQA) tests\nthe human-likeness of referring to objects under\nthe illusion. In the question, the object of interest\nis referred to by a property affected by the illusion,\nand the machine is asked to select the object from\ntwo options, e.g., select either left or right for the\nball that looks blue, in IMG1 and IMG2.\nAttribute Question Answering (AttrQA) tests the\nhuman-likeness to describe the attribute of objects\nunder the illusion. The question is about describing\na visual attribute (e.g. color) influenced by the\nillusion of a selected object, and we provide two\nanswer options to select from.\nReferential Localization ( RefLoc) tests the\nhuman-likeness of localizing the referred object un-\nder the illusion. Given a referential expression that\nmakes sense to humans under the effect of illusion\n(but may not be faithful to reality), the model needs\nto predict a bounding box for the object the expres-\nsion is referring to. For each referential query, we\nconsider the machine’s response to be humanlike\nonly when the pair of responses from the two im-\nages both match with those from human’s. This\nenforces that a humanlike response from machines\nhas to be grounded in the illusion image.\nTo create this benchmark, we annotate the col-\nlected illusion images with natural language ques-\ntions and the corresponding answers that humans\nwill give under illusions. To support the study of\nlanguage grounding, we also annotate the referring\nexpressions for each of the objects with the cor-\nresponding bounding box, where the expressions\nare formed under illusions. We provide several\nparaphrases for all the language annotations to help\nthe evaluation be more robust to models that are\nsensitive to language forms. All the images and\nIMG1:             | IMG2:           .\nIMG1 (original)\nIMG1: Left | IMG2: Right\n··· | ···\nHumanlike\nWhich ball looks blue, left or right?\nIs the ball on the left blue or green?\nLocalize the blue ball in the image.  \n··· | ···\nIMG2 (flipped)\nIMG1: Blue  | IMG2: Green\nRefQA \nAttrQA\nRefLoc\nUnlike··· | ···\n ··· | ···\nFigure 4: Illustration of the RefQA, AttrQA and RefLoc\nsetups. We flip the illusion image wherein the grounding\noutcome should also be inverted, to create a pair of\nimages for each test. Model success requires accurate\nidentification in both original and flipped versions to\nalign with human responses. Matching human answers\nsignals the model’s capability to interpret illusions in a\nhumanlike way, while a mismatch indicates otherwise.\ncorresponding annotations are verified by at least\nthree human annotators from our team.\n4 Experimental Setup\nVision-Language Models. To be evaluated on\nall of the four tasks in GVIL, the model has to be\nequipped with the visual question-answering skill\nand the object localization skill simultaneously.\nAmong a few candidates, we choose two state-of-\nthe-art models, the Unified-IO (Lu et al., 2022)\nand OFA (Wang et al., 2022), both of which are\ntrained on a wide range of vision-language tasks,\nand achieve impressive performance with a strong\nzero-shot inference capability on unseen data. Ad-\nditionally, we select two recent works that adapt\nlarge language models to understand visual images:\nthe LLaV A (Liu et al., 2023) and InstructBLIP (Dai\net al., 2023). These models are interesting to in-\nspect as they have shown a highly conceptual under-\nstanding of the nuance in images, such as the capa-\nbility of interpreting jokes, which may also be use-\nful in interpreting illusions. For each of the afore-\n5722\nTiny (33M) Base (93M) Large (470M) Huge (930M)\n0\n20\n40\n60\n80OFA\n4.2\n43.0\n52.8\n19.5\n36.544.0\n14.518.2\n67.2\n15.0\n34.0\n51.0\nVicuna-7B Vicuna-13B\n0\n20\n40\n60\n80LLaVA\n3.2\n50.546.2\n1.5\n48.849.8\nSmall (71M) Base (241M) Large (776M) XL (2925M)\n0\n20\n40\n60\n80Unified-IO8.2\n55.2\n36.5\n8.2 15.5\n76.2\n28.825.8\n45.5\n35.5\n8.5\n56.0\nVicuna-7B Vicuna-13B\n0\n20\n40\n60\n80InstructBLIP\n22.8\n55.8\n21.5 26.830.2\n43.0\nHumanlike No-Illusion N/A\nFigure 5: Results of SameDiffQA. The number shows the percentage of the answers. Each cluster represents the\ndistribution over humanlike, no-illusion and N/A answers from a model. The green and red line correspond to the\nlinear regression of humanlike rate and no-illusion rate across all the model sizes. Except for OFA-Large, Unified-\nIO-Large, InstructBLIP-13B, the differences between the humanlike rate and the no-illusion rate are statistically\nsignificant P <0.005. Details are in Table 4 Appendix A.\nmentioned models, there exists a range of variants\nin different sizes: OFA-{Tiny, Base, Large, Huge},\nUnified-IO-{Small, Base, Large, XL}, LLaV A-\n{Vicuna-7B, Vicuna-13B}, InstructBLIP-{Vicuna-\n7B, Vicuna-13B}. This allows us to study the im-\npact of size variations on model’s understanding of\nvisual illusions.\nMetrics. Through the experiments, we keep track\nof the Humanlike rate to measure the alignment be-\ntween humans and VLMs, which is the percentage\nof examples where the machine gives exactly the\nsame answers as humans. For theSameDiffQA task,\nwe also compute the No-Illusion rate, which cor-\nresponds to the percentage of examples where the\nmachine consistently considers the objects as the\nsame under both illusion and illusion-free settings.\nFor examples where the model fails to identify the\nobjects as the same in the illusion-free image or\nproduces nonsense answers to the questions, we\nmark them as Not Applicable (N/A)and exclude\nthem from the illusion recognition assessment.\n5 Results Analysis\nFrom our experiments, we are interested in investi-\ngating the following research questions:\n• RQ1: to what extent do VLMs recognize the\npresence of illusions similar to humans?\n• RQ2: how much do VLMs align with humans\nwhen communication happens under the influ-\nence of illusions?\n• RQ3: does the degree of alignment between\nVLMs and human responses vary across dif-\nferent categories of illusions?\nWe highlight several of our findings across this\nthree questions in below.\n5.1 Illusion Recognition\nThe results of SameDiffQA are shown in Figure 5.\nRelative proportions of \"humanlike,\" \"no-illusion,\"\nand \"not applicable (N/A)\" responses are repre-\nsented as green, orange, and grey bars respectively\nfor each model, which all together account for\n100%. First of all, we notice a large percentage\nof responses, across all models, fall under the N/A\ncategory. This suggests that these models often\ncannot even tell that the objects are identical in the\nillusion-free image, underscoring the need for im-\nprovement in standard vision-language reasoning\ncapabilities beyond the scope of illusion contexts.\nGiven the high proportion of N/A responses, one\nmight question the benchmark’s adequacy in reli-\nably reflecting a model’s tendency towards either\n\"humanlike\" or \"no-illusion\". Excluding the N/A\nresponses, we employed a χ2-test and found that 9\nout of 12 models would reject the null hypothesis\nwhich posits that the \"humanlike\" or \"no-illusion\"\nresponses are uniformly distributed. In other words,\nthese models do not behave randomly. Refer to Ap-\npendix A for more details. Such findings indicate\nthat, despite certain limitations in their capabili-\nties, our dataset and experimental design effectively\n5723\nOFA Unified-IO LLaVA I-BLIP\n0\n3\n6\n9\n12\n15RefQA\n0.5\n7.5\n14.0\n12.0\n2.8 3.8\n7.8\n9.8\n5.2 5.2\n2.0 1.2\nOFA Unified-IO LLaVA I-BLIP\n0\n3\n6\n9\n12\n15AttrQA\n0.5\n3.0\n9.0\n11.0\n2.2 3.5\n11.2\n9.5\n2.2 2.0 2.0 1.8\nOFA Unified-IO\n0\n10\n20\n30\n40\n50RefLoc 6.8\n18.2\n39.2\n34.8\n16.0\n30.8 32.0\n44.5\nFigure 6: Humanlike rate on RefQA, AttrQA and RefLoc.\nEach bar represents a different model size, arranged in\nascending order from left to right. Note that LLaV A and\nInstructBLIP cannot predict object bounding boxes thus\ndo not have the RefLoc results.\nTask Model Pearson coeff. p-value\nSameDiffQA OFA 0.689 0.311\nUnifiedIO 0.940 0.059*\nRefQA OFA 0.946 0.054*\nUnifiedIO 0.977 0.022**\nAttrQA OFA 0.957 0.043**\nUnifiedIO 0.853 0.146\nRefLoc OFA 0.933 0.066*\nUnifiedIO 0.960 0.039**\nTable 3: Pearson’s correlation analysis between the hu-\nmanlike rate and model size. Statistically significant\nresults with p < 0.05 and p < 0.1 are marked with **\nand *, respectively.\ngauge illusion recognition in the assessed VLMs.\nWhen examining cases where responses are ap-\nplicable for testing illusion recognition, we observe\nthat the majority of models are more likely to fail\nin recognizing illusions (35.4% on average) than\nproducing humanlike responses (15.6% on aver-\nage). This discrepancy is most pronounced in the\ncase of InstructBLIP, where the model predomi-\nnantly offers ’no-illusion’ answers. Conversely, the\nUnified-IO XL model stands out as the only model\nexhibiting a significant propensity towards human-\nlike illusion recognition. A further investigation of\nthe underlying reason that causes this discrepancy\nwould be interesting further work.\nTo illustrate how the scores evolve with model\nsize, we plot regression lines of “humanlike\"\n(green) and “no-illusion\" (red) rates, respectively.\n0\n20\n40\n60\n80OFA Huge8.3 4.2 0.0 0.0\n9.6 12.5 8.1\n36.8\n23.3\n12.5\n20.8 21.7\n7.5 5.0 3.8\n41.2\n27.5 27.5\n12.5\n75.0\nSameDiffQA RefQA AttrQA RefLoc\nConstancy\nAssimilation\nContrast\nRelativity\nPerspective\n0\n20\n40\n60\n80Unified-IO XL\n20.8\n8.3 4.2\n33.329.4\n8.8 9.6\n44.9\n35.0\n5.8\n11.7\n42.5\n50.0\n2.5\n8.8\n35.037.5 40.0\n7.5\n75.0\nSameDiffQA RefQA AttrQA RefLoc\n0\n20\n40\n60\n80LLaVA 13B\n0.0 4.2 0.01.5 2.9 1.51.7\n10.0\n2.52.5 5.0 1.20.0 0.0\n5.0\nSameDiffQA RefQA AttrQA\n0\n20\n40\n60\n80InstructBLIP 13B\n33.3\n8.3\n0.0\n19.9\n0.0 2.9\n25.8\n2.5 1.7\n31.2\n0.0 0.0\n40.0\n0.0 2.5\nSameDiffQA RefQA AttrQA\nFigure 7: Humanlike rates of the largest model of each\nfamily, with finer-grained human-likeness scores on\neach illusion category.\nAn emerging trend reveals that “humanlike\" scores\ntend to increase as the model scales, whereas \"no-\nillusion\" responses tend to decline. This finding\nsuggests a positive correlation between model scale\nand human-machine alignment under illusions. We\nhypothesize that this observed trend could be at-\ntributed to the enhanced pattern-recognition capa-\nbilities of larger models. These models, arguably,\nare better suited to discern and assimilate patterns\npresent in data generated by humans, which may\nhave been shaped by the influence of illusions. Con-\nsequently, it’s plausible to assume that these models\nare progressing towards a more humanlike compre-\nhension of illusions.\n5.2 Communication Under Illusion\nThe results of RefQA, AttrQA, and RefLoc exper-\niments are shown in Figure 6, offering insights\ninto the alignment between machine and human\nresponses under the influence of visual illusions.\nWe find that all the VLMs encounter significant\nchallenges in responding to questions presented un-\nder the influence of visual illusions in both RefQA\nand AttrQA. As a result, the models obtain a maxi-\nmum humanlike response rate of only 14.0% and\n11.2% for RefQA and AttrQA, respectively. Inter-\nestingly, models exhibit much stronger alignment\nin the localization task, with the highest alignment\nof 44.5% achieved by Unified-IO XL. This indi-\ncates that the learned object localization skill aligns\n5724\nTiny Base Large Huge\n0\n20\n40\n60\n19.9\n36.0\n53.5\n47.4\n19.9\n33.4\n39.4 43.3\n19.9\n35.2\n48.7 46.2\n20.0\n34.3\n45.1 44.8\nAttn. on the humanlike choice under illusion\nAttn. on the counter-humanlike choice under illusion\nAttn. on the humanlike choice w/o illusion\nAttn. on the counter-humanlike choice w/o illusion\nFigure 8: Attention weight distribution of OFA models for the RefLoc task.\nFigure 9: Visualization of the attention maps generated\nby the OFA-Large model for the RefLoc task. In each\nrow, the input image is shown on the left, and the atten-\ntion map for the referential expression \"smaller orange\nball\" is shown on the right. The attention maps surround-\ning the object of interest are highlighted for enhanced\nvisibility.\nbetter with humans under illusions compared to the\nvisual question answering skills. Research into the\nunderlying reason behind this difference might be\nan interesting future direction.\nNotably, we find a positive correlation between\nscaling up models and the increase of human-\nlike rate across different models and tasks, which\nechoes our earlier observations from the SameD-\niffQA experiment. To verify the statistical signifi-\ncance, we conducted Pearson’s correlation analysis\nfor OFA and UnifiedIO models2. As shown in Ta-\nble 3, 6 of the 8 tested scenarios showed significant\nor moderately significant positive correlations, with\nPearson coefficients exceeding 0.9. Such results un-\nderscore the potential of larger models to enhance\nthe human-machine alignment of responses across\ndifferent tasks and illusion contexts.\n2InstructBLIP and LLaV A were excluded since at least\nthree data points are needed for the test.\n5.3 Delving into Illusion Categories\nWe provide a more granular analysis by examin-\ning each type of illusion, presenting the human-\nlike rates for each category in Figure 7. The re-\nsults depicted here are sourced from the largest\nmodel within each model family, namely Unified-\nIO Huge, OFA Huge, LLaV A Vicuna-13B, and\nInstructBLIP Vicuna-13B. Our observation reveals\nthat the perspective category demonstrates the high-\nest degree of alignment between machines and hu-\nmans. On the contrary, color constancy illusions\nemerge as the category with the least congruity in\nrelation to human responses.\n5.4 Understanding the Cause of Illusions\nTo gain insight into model predictions under the in-\nfluence of illusions, we analyze the attention distri-\nbutions of OFA models in the RefLoc task. Specif-\nically, we compute the attention weight from the\nlocalization query (e.g., \"the smaller orange ball\")\nto the object representation of either a \"humanlike\"\nor \"counter-humanlike\" perception under illusions.\nAs depicted by the dark blue and light blue bars\nin Figure 8, as the model size increases, attention\nweights lean more towards the humanlike selec-\ntion. This trend is consistent with the humanlike\nrate observed for the RefLoc task in Figure 6. To\ndetermine if this bias stems from the illusion, we\nalso calculate attention weights for images without\nthe illusion inducer (represented by orange bars).\nThese weights are nearly equally distributed across\nboth objects, suggesting that the illusion does in-\ndeed influence the model’s attention and biases its\npredictions similarly to human perceptions.\nFigure 9 shows an example using the attention\nvisualization tool (Aflalo et al., 2022). The first\nimage displays the original illusion image, with\ntwo orange balls of identical size while the left\nball seems smaller. The second image is devoid\n5725\nof the illusion inducer, while the third image arti-\nficially enlarges the right orange ball. Attention\nmaps corresponding to the \"smaller orange ball\"\nquery3 are shown adjacent to each image. In the\noriginal illusion, the model predominantly focuses\non the left ball, aligning with human observations.\nWithout the illusion inducer, the query becomes\nambiguous, leading to a dispersed attention map.\nHowever, when an actual size difference is present,\nthe model’s attention decisively shifts to the cor-\nrectly perceived smaller ball on the left. A compar-\nison of these attention maps highlights that while\nillusions can steer the model’s attention similarly\nto humans, its effect is less pronounced than when\na real disparity exists.\n6 Discussion and Conclusion\nWe introduce GVIL, the first dataset facilitating a\nsystematic evaluation of machine visual illusion via\nnatural language. Evaluating four distinct series\nof state-of-the-art vision-language model families\nacross varying scales, we observe a notable align-\nment between these models and human perceptions\nduring object localization in the context of illusions.\nInterestingly, this alignment tends to strengthen\nwith increased model size. Conversely, many mod-\nels face challenges in mirroring human perspec-\ntives within visual question-answering tasks. Our\npreliminary observations underscore the need for\nfurther discussions in two directions:\nAssessment of Vision-Language Models in the\nRealm of Visual Illusions. Vision-language\nmodels have demonstrated commendable prowess\nin both visual and language understanding. Yet, a\nnotable gap persists in assessing their performance\nin the presence of visual illusions. Given that\nsuch illusions are intrinsic to human perception,\noverlooking this facet may contribute to misalign-\nment between human and AI interpretations during\nreal-world engagements. While our study unveils\ncertain trends, like the correlation between model\nsize and human-model alignment, making defini-\ntive assertions is non-trivial due to the discrepancy\nin model architectures and their training datasets.\nThrough GVIL, we aspire to catalyze further re-\nsearch that addresses visual illusion in VLMs.\n3We use the second last layer of the OFA large model, as\nthe overall attention score of this layer is the highest. Atten-\ntions from all the heads are averaged.\nGaining Insights from Illusions.Exploring the\neffects of visual illusions can offer fresh per-\nspectives to comprehend the intricacies of vision-\nlanguage models. Visual illusion, in some way, is\nsimilar to various types of values shared by our\nhuman society, but not shared among today’s AI\nmodels. Given the rapidly growing applications\nof large AI models, it’s important to identify and\nunderstand various aspects of alignment between\nthese models and humans. Vision illusion is only\nan example among many possibilities for future\nstudies.\nLimitations\nThis work is only the initial attempt to the question\nand there are many limitations which we think of as\nexciting future research directions. First of all, al-\nthough our experiments yields some interesting em-\npirical findings, it is not clear why different forms\nof tasks (e.g., QA-based tasks vs. RefLoc) lead to a\nstark contrast in the results. As these findings may\nhave implications in future technology that adapt\nto visual illusions, more in-depth understanding of\nthese behaviors will be needed in the future. Sec-\nond, our benchmark is currently small in size. It\nlays an infrastructure for this work. Future efforts\nto collect more data to form a centralized reposi-\ntory will be desired for studying visual illusions in\nboth humans and machines. Third, our investiga-\ntion is only based on a manually collected dataset\nfor our intellectual curiosity. The construction of\nthis dataset has the limitations that the effect of\nvisual illusions are not validated by a wider range\nof human subjects other than the authors. While\nit has motivation in improving situated language\ncommunication with embodied agents in the physi-\ncal world, how visual illusions play in perceiving\nand communicating about the real physical world\nremains an interesting question.\nEthics Statement\nThe data are collected and annotated by the au-\nthors without the involvement of any other human\nsubject. Data samples are selected from a wide\nliterature search on the subject of visual illusions.\nAcknowledgements\nThis work was supported by NSF IIS-1949634 and\nthe DARPA PTG program HR00112220003. The\nauthors would like to thank the anonymous review-\ners for their valuable comments and suggestions.\n5726\nReferences\nEdward H Adelson. 1995. Checkershadow illusion.\nMahmoud Afifi and Michael S Brown. 2019. What else\ncan fool deep learning? addressing color constancy\nerrors on deep neural network performance. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 243–252.\nEstelle Aflalo, Meng Du, Shao-Yen Tseng, Yongfei\nLiu, Chenfei Wu, Nan Duan, and Vasudev Lal. 2022.\nVl-interpret: An interactive visualization tool for\ninterpreting vision-language transformers. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 21406–21415.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: a visual language model for\nfew-shot learning. arXiv preprint arXiv:2204.14198.\nA Benjamin, Cheng Qiu, Ling-Qi Zhang, K Kording,\nand A Stocker. 2019. Shared visual illusions between\nhumans and artificial neural networks. In 2019 Con-\nference on Cognitive Computational Neuroscience,\nvolume 10, pages 2019–1299.\nClaus-Christian Carbon. 2014. Understanding human\nperception by human-made illusions. Frontiers in\nhuman neuroscience, 8:566.\nDavid P Carey. 2001. Do action systems resist visual\nillusions? Trends in cognitive sciences, 5(3):109–\n113.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint\narXiv:2305.06500.\nRH Day. 1984. The nature of, perceptual illusions. In-\nterdisciplinary Science Reviews, 9(1):47–58.\nFranz Joseph Delboeuf. 1865. Note sur certaines illu-\nsions d’optique: Essai d’une théorie psychophysique\nde la maniere dont l’oeil apprécie les distances et les\nangles. Bulletins de l’Académie Royale des Sciences,\n19:195–216.\nJinyu Fan and Yi Zeng. 2023. Challenging deep learn-\ning models with image distortion based on the abut-\nting grating illusion. Patterns, 4(3).\nV olker H Franz. 2001. Action does not resist visual\nillusions. Trends in cognitive sciences, 5(11):457–\n459.\nMaurizio Gentilucci, Sergio Chieffi, Elena Daprati,\nM Cristina Saetti, and Ivan Toni. 1996. Visual illu-\nsion and action. Neuropsychologia, 34(5):369–376.\nAlex Gomez-Villa, Adrián Martín, Javier Vazquez-\nCorral, Marcelo Bertalmío, and Jesús Malo. 2022.\nOn the synthesis of visual illusions using deep gener-\native models. Journal of Vision, 22(8):2–2.\nAlexander Gomez-Villa, Adrian Martin, Javier Vazquez-\nCorral, and Marcelo Bertalmío. 2019. Convolutional\nneural networks can be deceived by visual illusions.\nIn Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 12309–\n12317.\nAlexander Gomez-Villa, Adrian Martín, Javier Vazquez-\nCorral, Marcelo Bertalmío, and Jesús Malo. 2020.\nColor illusions also deceive cnns for low-level vision\ntasks: Analysis and implications. Vision Research,\n176:156–174.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 6904–6913.\nRichard L Gregory. 1997. Visual illusions classified.\nTrends in cognitive sciences, 1(5):190–194.\nElad Hirsch and Ayellet Tal. 2020. Color visual illu-\nsions: A statistics-based computational model. Ad-\nvances in Neural Information Processing Systems,\n33:9447–9458.\nJoseph Jastrow. 1892. Studies from the laboratory of ex-\nperimental psychology of the university of wisconsin.\nii. The American Journal of Psychology, 4(3):381–\n428.\nAkiyoshi Kitaoka. 2010. A brief classification of colour\nillusions. Colour: Design & Creativity, 5(3):1–9.\nMichal Kosinski. 2023. Theory of mind may have spon-\ntaneously emerged in large language models. arXiv\npreprint arXiv:2302.02083.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nBen Lonnqvist, Alban Bornet, Adrien Doerig, and\nMichael H Herzog. 2021. A comparative biology ap-\nproach to dnn modeling of vision: A focus on differ-\nences, not similarities. Journal of Vision, 21(10):17–\n17.\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh\nMottaghi, and Aniruddha Kembhavi. 2022. Unified-\nio: A unified model for vision, language, and multi-\nmodal tasks. arXiv preprint arXiv:2206.08916.\nBruce MacEvoy. 2005. Handprint : Color vision.\nDominique Makowski, Zen J Lau, Tam Pham,\nW Paul Boyce, and SH Annabel Chen. 2021. A para-\nmetric framework to generate visual illusions using\npython. Perception, 50(11):950–965.\n5727\nFC Müller-Lyer. 1889. Optische urteilstäuschungen.\nArchiv für Physiologie Suppl, pages 263–270.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nMario Ponzo. 1911. Intorno ad alcune illusioni nel\ncampo delle sensazioni tattili sull. Archives Itali-\nennes de Biologie.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nJames Outram Robinson. 2013. The psychology of vi-\nsual illusion. Courier Corporation.\nEric D Sun and Ron Dekel. 2021. Imagenet-trained\ndeep neural networks exhibit illusion-like response\nto the scintillating grid. Journal of Vision, 21(11):15–\n15.\nDejan Todorovi´c. 2020. What are visual illusions? Per-\nception, 49(11):1128–1199.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022. Ofa: Unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning, pages\n23318–23340. PMLR.\nA Statistical Analysis for Illusion\nRecognition\nWe conducted a statistical analysis to rigorously\nvalidate that our experimental setup is able to reveal\na model’s inclination towards either “humanlike\" or\n“no-illusion\", notwithstanding the high prevalence\nof N/A samples. Specifically, we applied a χ2-test\nto the model predictions omitting the N/A samples.\nThe null hypothesis posits that the humanlike and\nno-illusion samples are uniformly distributed, i.e.,\nthe model behaviors randomly. As shown in Table\n4, a significant proportion of the models reject the\nnull hypothesis. Out of the 12 models tested, 8\nmodels rejected the null hypothesis with a p-value\n<0.001, and 9 models rejected the null hypothe-\nsis with a p-value <0.01. These figures strongly\nsuggest that most models perform better than what\nwould be expected by chance alone, which is a\npiece of strong evidence that our dataset and exper-\nimental setup can support the evaluation of illusion\nrecognition for the tested VLMs.\nModel #HL #NI χ2 p-value\nOFA-Tiny 17 172 129.45 <0.001***\nOFA-Base 78 146 20.64 <0.001***\nOFA-Large 58 73 1.72 0.190\nOFA-Huge 60 136 29.47 <0.001***\nUnifiedIO-Small 33 221 139.15 <0.001***\nUnifiedIO-Base 33 62 9.38 0.002**\nUnifiedIO-Large 115 103 0.66 0.416\nUnifiedIO-XL 142 34 66.27 <0.001***\nLLaV A-7B 13 202 12.89 <0.001***\nLLaV A-13B 6 195 177.72 <0.001***\nInstructBLIP-7B 91 223 55.49 <0.001***\nInstructBLIP-13B 107 121 0.86 0.354\nTable 4: Chi-square test for the SameDiff Task (Figure\n5). #HL and #NI denote the number of humanlike illu-\nsionary answers and no-illusion answers, respectively.\nStatistically significant results with p < 0.001 and p <\n0.05 are marked with *** and **. respectively.\n5728"
}