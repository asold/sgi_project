{
  "title": "TransC-GD-CD: Transformer-Based Conditional Generative Diffusion Change Detection Model",
  "url": "https://openalex.org/W4392450086",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2167993061",
      "name": "Yi-Han Wen",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2101859101",
      "name": "Zhuo Zhang",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2097673345",
      "name": "Qi Cao",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2883300863",
      "name": "Guanchong Niu",
      "affiliations": [
        "Xidian University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4313413542",
    "https://openalex.org/W3009755915",
    "https://openalex.org/W4290755378",
    "https://openalex.org/W2042141552",
    "https://openalex.org/W2113308243",
    "https://openalex.org/W2144552105",
    "https://openalex.org/W6629944422",
    "https://openalex.org/W2751993439",
    "https://openalex.org/W2891248708",
    "https://openalex.org/W3130754787",
    "https://openalex.org/W4380369869",
    "https://openalex.org/W4321609022",
    "https://openalex.org/W4226228401",
    "https://openalex.org/W4323064994",
    "https://openalex.org/W4390591071",
    "https://openalex.org/W4312950730",
    "https://openalex.org/W4226014430",
    "https://openalex.org/W4298005622",
    "https://openalex.org/W4386076368",
    "https://openalex.org/W4389610328",
    "https://openalex.org/W6805335523",
    "https://openalex.org/W4292829030",
    "https://openalex.org/W4296123057",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4312497550",
    "https://openalex.org/W4386075573",
    "https://openalex.org/W6797359156",
    "https://openalex.org/W4390873054",
    "https://openalex.org/W4386083096",
    "https://openalex.org/W4312911498",
    "https://openalex.org/W4393207086",
    "https://openalex.org/W2896092083",
    "https://openalex.org/W3000451586",
    "https://openalex.org/W2564140372",
    "https://openalex.org/W2792827505",
    "https://openalex.org/W3216244838",
    "https://openalex.org/W3036453075",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W3217456364",
    "https://openalex.org/W3180045188",
    "https://openalex.org/W4312549298",
    "https://openalex.org/W4319866415",
    "https://openalex.org/W4383372117",
    "https://openalex.org/W4316035304",
    "https://openalex.org/W2766049824",
    "https://openalex.org/W3183600011",
    "https://openalex.org/W2908048833",
    "https://openalex.org/W6809890315",
    "https://openalex.org/W2908320224",
    "https://openalex.org/W2805152403",
    "https://openalex.org/W3119131609",
    "https://openalex.org/W3102127038",
    "https://openalex.org/W3175528029",
    "https://openalex.org/W4224301858",
    "https://openalex.org/W1686810756"
  ],
  "abstract": "Change detection (CD) methodologies have garnered substantial attention owing to their capability to monitor alterations in geographic spaces across temporal intervals, especially with the acquisition of high-resolution remote sensing images. However, challenges persist due to dissimilar imaging conditions and temporal windows. Although deep-learning architectures have shown promise in addressing challenges in CD, many existing methods struggle to capture long-range dependencies and local spatial information effectively. The current CD methods rely heavily on pure CNNs and transformers, which employ only single-pass forward propagation. This approach leads to inadequate utilization of feature information, resulting in inaccurate CD maps, particularly when discerning edges. To overcome these limitations, we propose a transformer-based conditional generative diffusion method for CD, named TransC-GD-CD, tailored for RS data. This approach leverages the numerous sampling iterations of the DDPM, contributing to the generation of high-quality CD maps. In addition, the frequency cross transformer mechanism seamlessly amalgamates CD condition with the noise feature within the DDPM. The innovative mechanism effectively bridges diffusion noise and conditional semantic terrains. Moreover, a novel multitype difference extraction module, named appear&#x2013;disappear&#x2013;concat, is devised to partition the CD task to optimize both segmentation extraction and CD classification, overcoming the persistent challenge of information loss endemic to conventional CD algorithms, such as simple subtraction. We demonstrate the superiority of TransC-GD-CD by comparing the experiment results against various algorithms across three widely used CD datasets, namely CDD, WHU, and LEVIR.",
  "full_text": "1\nTransC-GD-CD: Transformer-based Conditional\nGenerative Diffusion Change Detection Model\nYihan Wen\n∗\n, Zhuo Zhang\n∗\n, Qi Cao, and Guanchong Niu †\nAbstract—Change Detection (CD) methodologies have gar-\nnered substantial attention owing to their capability to monitor\nalterations in geographic spaces across temporal intervals, es-\npecially with the acquisition of high-resolution Remote Sensing\nimages. However, challenges persist due to dissimilar imaging\nconditions and temporal windows. Although deep-learning (DL)\narchitectures have shown promise in addressing challenges in CD,\nmany existing methods struggle to capture long-range dependen-\ncies and local spatial information effectively. The current CD\nmethods rely heavily on pure CNNs and Transformers, which\nemploy only single-pass forward propagation. This approach\nleads to inadequate utilization of feature information, resulting\nin inaccurate CD maps, particularly when discerning edges. To\novercome these limitations, we propose a Transformer-based con-\nditional generative diffusion method for CD, named TransC-GD-\nCD, tailored for RS data. This approach leverages the numerous\nsampling iterations of the DDPM, contributing to the generation\nof high-quality CD maps. In addition, the Frequency Cross\nTransformer (FCT) mechanism seamlessly amalgamates CD\ncondition with the noise feature within the DDPM. The innovative\nmechanism effectively bridges diffusion noise and conditional\nsemantic terrains. Moreover, a novel multi-type difference extrac-\ntion module, named Appear-Disappear-Concat (ADC), is devised\nto partition the CD task to optimize both segmentation extraction\nand CD classification, overcoming the persistent challenge of\ninformation loss endemic to conventional CD algorithms like\nsimple subtraction. We demonstrate the superiority of TransC-\nGD-CD by comparing the experiment results against various\nalgorithms across three widely-used CD datasets, namely CDD,\nWHU, and LEVIR. The code for this work will be available on\nhttps://github.com/YihanWen/DDPM-based-Change-Detection.\nIndex Terms—Denoising diffusion probabilistic model, Change\ndetection, generative models\nI. I NTRODUCTION\nCD focuses on identifying the differences in a specific\ngeographical area over various periods. Due to advancements\nin sensor technology, a vast amount of high-quality RS images\nhave been taken at different times, which are crucial for tasks\nThis work was supported by the Fundamental Research Funds for the\nCentral Universities of China under grant 20103237888, National Natural\nScience Foundation of China under Grant 62301406, Guangzhou Basic and\nApplied Basic Research Foundation under Grant 2023A04J0394, and Tianjin\nResearch Innovation Project for Postgraduate Students of China under grand\nof 2022SKY126. (†Corresponding authors: Guanchong Niu)\nYihan Wen is with Guangzhou Institute of Technology, Xidian University,\nand also with The Hong Kong Polytechnice University, HongKong, China\n(e-mail: wenyihan4396@gmail.com;).\nZhuo Zhang is with School of Computer Science, National University of\nDefense Technology, Changsha 410073, China (e-mail: 2130070829@tian-\ngong.edu.cn).\nGuanchong Niu and Qi Cao are both with Guangzhou Institute of\nTechnology, Xidian University, Guangzhou, China (e-mail: {niuguanchong,\ncaoqi}@xidian.edu.cn).\n∗The authors have contributed equally to this work.\nlike land management [1], and studying changes in urban\nand rural areas [2, 3]. However, the challenge arises as these\nimages are captured under varying conditions and at different\ntimes, which introduces significant discrepancies due to non-\nessential alterations. To mitigate these disruptions and strive\nfor a more precise CD map, various techniques have been\ndeveloped, including image algebra, image transformation, and\nDL approaches.\nAlgebra-based techniques involve mathematical processes\nlike image differencing [4], image ratioing [5], and image\nquantification [6] to generate change maps. On the other\nhand, transformation methods, like principal component anal-\nysis (PCA) [7], and change vector analysis (CV A) [8] adapt\nthe spatial mapping to emphasize the change information.\nHowever, the aforementioned methods substantially rely on\nprior observations and experiences. This dependence often\nresults in accuracy and efficiency limitations, especially when\ntackling complicated scenarios, such as identifying irregularly\npatterned land degradation or discerning transient urban de-\nvelopments.\nRecently, deep-learning (DL) based methods have overshad-\nowed traditional methods due to their extraordinary abilities to\nextract nonlinear features. Deep neural networks (DNNs) were\ntypically utilized to extract distinguishable features between\nbitemporal images. For instance, Zhan et al. [9] employed\nSiamese neural networks (SNNs) to extract deep features that\ndemonstrate greater abstraction and robustness as compared to\nhandcrafted features. Moreover, Daudt et al. [10] introduced\nan improved approach called the fully convolutional neural\nnetwork (FCNN) with Unet architecture, which enhances the\naccuracy of CD by extracting multi-scale feature information.\nNonetheless, these methods always focus on extracting deep\nchange semantic features, often neglecting the importance\nof shallow-layer information containing high-resolution and\nfine-grained features. To address this issue, Fang et al. [11]\nproposed a densely connected Siamese network to aggregate\nand refine features of multiple semantic levels, which partially\nmitigates the semantic gaps and localization errors. In addition\nto these advancements, the Onboard Change Detection System\n(OCDS) [12] utilizes CNNs for real-time natural disaster\nresponse in space environments, balancing DL capabilities\nwithin the unique constraints of space hardware. Furthermore,\nthe Multilevel Feature Enhancement Network (MFENet) [13]\nintroduces a novel method for landslide mapping, leveraging\nCNNs and attention mechanisms to enhance postevent features\nand generate robust CD features. Nonetheless, the DNN-\nbased methods, despite their broader application, encounter\na substantial limitation when it comes to utilizing spatial-\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2\ntemporal information due to the reduced size of the image\npatches.\nConsidering the lack of long-range dependencies in DNNs,\nthe difference discrimination network within the Transformer\nstructure is introduced in pursuit of boosting the global rep-\nresentation capability. For instance, Zhang et al. [14] intro-\nduced a pure Transformer network with a Siamese U-shaped\nstructure called SwinsUnet, leveraging the Swin Transformer\nblock for the encoder, fusion, and decoder components. In\naddition, STCD-Former [15] employs a dual-branch spectral\ntransformer, adeptly guiding patch tokens to discern changes,\nwhile Trans-MAD [16] utilizes a pre-detection strategy with\ncompressed change vector analysis for unsupervised CD,\nproving effective in various RS applications. Despite their\nability to capture global contexts, these models are proficient\nat capturing global contextual information, while they may not\nbe as effective in capturing intricate local variations and edge\ndetails [17]. This restriction impedes their ability to precisely\ndepict change boundaries, potentially resulting in less refined\npredictions.\nConcurrently, the DDPM has emerged as a potent gener-\native framework, demonstrating its capability in synthesizing\nhigh-quality and diverse imagery. Research indicates that the\nDDPM outperforms traditional Generative Adversarial Net-\nworks (GANs) and the Transformer in a range of applica-\ntions including super-resolution [18–21], segmentation [22–\n24], inpainting [25–27], and conditional image generation\n[28–31]. Recently, a variant model called DDPM-CD has\nbeen introduced for semantic segmentation and CD tasks.\nThis model employs a pre-trained DDPM architecture as a\nfeature extractor, capturing semantic nuances via a singular\nforward propagation. However, this approach insufficiently\nleverages the intrinsic capacities of DDPM for progressive\nlearning and iterative refinement. To mitigate this limitation,\nan end-to-end generative CD (GCD) model, termed as GCD-\nDDPM [32], has been constructed to synthesize CD maps\nthrough an iterative sampling process during the inference\nstage comprising thousands of steps, initiated from Gaussian\nnoise, successfully outperforming the DDPM-CD.\nIn addition, it is noteworthy that existing DDPM methods\nfor CD predominantly employ a U-Net backbone consisting\nof pure convolution layers [32]. However, the dynamism and\nglobal receptive fields inherent to the Transformer architecture\nmake it exceptionally adept, surpassing CNNs, especially in\ntasks related to vision representation learning. Recognizing\nthis potential, hybrid Transformer-convolutional architectures\nhave garnered significant attention, delivering superior per-\nformance metrics [17]. Inspired by these advancements, we\npropose a novel Transformer-based Conditional Generative\nDiffusion U-Net architecture, designated as TransC-GD-CD,\nfor diffusion process optimization. Our approach introduces\nsignificant methodological advancements that distinguish it\nfrom existing models such as our previous work in [32]:\n• Frequency Cross Transformer (FCT) and Change Feature\nNormalization (CFN) Modules: Contrary to the GCD-\nDDPM method, which merges CD guidance information\ninto the Noise Predictor through direct addition, our\nmodel incorporates the FCT and CFN modules. These\nmodules enhance high-dimensional and low-dimensional\nfeature maps with CD information, effectively bridging\nthe gap between change features and diffusion noise do-\nmains. This innovation allows for the generation of high-\nprecision CD maps, with the high-dimensional condition\ninfluencing the map’s contour and positional information,\nand the low-dimensional details improving edge fidelity\nduring the iterative sampling process.\n• Appear-Disappear-Concat (ADC) Module for Difference\nComputation: Unlike GCD-DDPM, which relies on di-\nrect subtraction of post-change image features from pre-\nchange images, our model mitigates information loss\nthrough the ADC module. This multi-type difference\ncomputation module extracts abundant differential infor-\nmation, significantly enhancing the quality of the result-\ning CD map.\nIn the TransC-GD-CD model, we have innovatively in-\ntegrated both high-dimensional and low-dimensional change\nfeatures to guide the Noise Predictor in generating high-\nquality CD maps. Additionally, traditional CD methodologies\npredominantly employ a rudimentary subtraction process to\nderive feature differences between pre- and post-change im-\nagery, inherently resulting in substantial information loss. To\ncircumvent this issue, we introduce a multi-type difference\nextraction module named ADC. The ADC divides the CD\ntask into two specialized sub-tasks: segmentation extraction\nand CD classification. The ADC comprises three distinctive\nbranches designed to capture various types of change infor-\nmation, followed by an element-wise addition operation to\naccumulate these features.\nThen, an element-wise multiplication is employed to inte-\ngrate the low-dimension CD condition into the foundational\ndiffusion features.\nIn summary, our contributions are elaborated as follows:\n1) Drawing inspiration from recent breakthroughs in DL, we\npioneer the marriage of the Transformer architecture with\na diffusion-based framework for CD tasks.\n2) We design and implement the FCT module, strategically\nbridging the gap between change feature and diffusion\nnoise domains, integrating the high-dimension CD con-\ndition into noise predictor effectively. In the meantime,\nthe low-dimension CD condition with precise edge details\nenhanced by CFN is seamlessly conveyed into DDPM\nframework. The high-dimensional condition steers the\ncontour and positional information of the CD map, while\nthe low-dimensional details augment edge fidelity during\nthe iterative sampling process.\n3) We mitigate the information loss caused by element-wise\nsubtraction process utilized in traditional CD methodolo-\ngies by introducing a multi-type difference computation\nmodule, namely ADC. This module helps to extract\nabundant differential information and ultimately improves\nthe quality of the resulting CD map.\nThe remainder of this paper is structured as follows: Sec-\ntion II surveys the existing literature on DL-based CD methods\nand recently proposed DDPM architectures. Section III delin-\neates the architectural and operational details of TransC-GD-\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3\nCD. Extensive experimental results and evaluations are sub-\nsequently presented in Section IV, culminating in concluding\nremarks in Section VI.\nII. P RELIMINARY KNOWLEDGE\nA. Discriminative Models\n1) CNN-based models: Traditional CNN models have at-\ntracted considerable interest in CD due to their proficiency in\ndecoding intricate local spatial patterns [33–35]. For instance,\nthe symmetric convolutional coupling network (SCNN) in [36]\nutilized unsupervised techniques to understand the relation-\nships between different images, while novel recurrent convolu-\ntional neural network (ReCNN) [37] employs skip-connections\nto address temporal gaps in multi-temporal datasets, enhancing\nspectral-spatial feature extraction. In addition, a series of\ninnovative models have emerged to further address various\nchallenges in this domain. Liu et al. introduced a local–global\npyramid network (LGPNet) [38], which employs global and\nlocal feature pyramid modules to capture building objects from\ndiverse angles, ensuring a comprehensive representation of\nstructures irrespective of their size or shape. Subsequently,\na notable advancement proposed in Zhang et al.’s deeply\nsupervised image fusion network (DSIFN) [39], leveraging the\nVGG net [40] to extract deep features from bi-temporal im-\nages, further refined by spatial and channel attention modules.\nChen et al.’s double attentive siamese network (DASNet) [41]\nincorporated attention mechanisms, enhancing the extraction\nof change map features. Despite these advancements, the\nCNN-based models remains a fundamental limitation that\nrestricts the ability to map global dependencies, confining their\neffectiveness to specific local regions. This issue becomes\nespecially intractable when the models are challenged by\nintricate patterns and widespread changes spanning extensive\nspatial dimensions.\n2) Transformer-based models: In the domain of computer\nvision, the Transformer architecture has emerged as a front-\nrunner, exhibiting remarkable performance in CD tasks. A\nnotable strength of the Transformer lies in its adeptness at\nmanaging long-range dependencies, a feat primarily attributed\nto its incorporation of self-attention modules [42–45]. This\nallows Transformers to harness global contextual relationships,\nthereby addressing the limited issue of receptive field com-\nmonly associated with CNN-based models. Among these, the\nSiamixFormer [46] model stands out by using bi-temporal\nimages for building and CD. It combines a fully-transformer\narchitecture with temporal transformers for feature fusion,\neffectively maintaining large receptive fields and capturing\ntemporal feature dynamics. Additionally, a novel approach in\nthis domain is the MultiTask Bitemporal Images Transformer\n(MTBIT) [47], designed to simultaneously generate 2D and 3D\nCD maps using optical images, innovatively inferring elevation\nchanges without direct elevation data.\nHowever, it is imperative to note that while Transformers\nexcel in discerning global patterns, they occasionally falter in\ncapturing finer nuances. Their limited proficiency in capturing\nintricate local variations and precise edge details [48]. This\nshortcoming often compromises their ability to accurately\ndemarcate the peripheries of change regions.\nLocked and \nPretrained\nModule\nAlive \nModule\nWeight-Sharing\n Semantic \nExtractor\n(DDPM) \nSemantic \nExtractor\n(DDPM)\nSemantic \nExtractor\nSemantic \nExtractor\nCD \nModule\nGenerator\n(DDPM)\nCD \nModule\n\n\n\n\n(a)\n(b)\nFig. 1. A comparative illustration of DDPM-based CD Architectures: (a)\nDDPM-CD and (b) the proposed Trans-GD-CD. Modules in blue signify\ntheir internal parameters have been locked, while those in orange indicate\ntheir internal parameters are available for optimization. While DDPM-CD\nincorporates two locked DDPM modules pre-trained\non RS images for semantic information extraction, only its\nCD module is optimized during the CD task. In contrast,\nTrans-GD-CD emphasizes end-to-end training, with its\nDDPM modules as a generator, colored in orange,\nprogressively optimized throughout the CD task.\nB. Generative Models\n1) GAN-based models: Regarding CD, GANs have proven\nto be effective in modeling the intricate dynamics between\nbi-temporal images. A notable example is [49], where GANs\nutilize initial difference images to refine change maps, thereby\nenhancing detection accuracy through the principles of adver-\nsarial learning. Expanding the utility of GANs, the DTCDN\napproach [50] successfully fuses optical and Synthetic Aper-\nture Radar (SAR) images. This method employs deep im-\nage translation to unify different feature domains, processed\nthrough a supervised network. It is distinguished by its ad-\nvanced UNet++ architecture and a multi-scale loss function,\noutperforming traditional techniques in optimizing both global\nand local effects. Furthermore, the GDCN [51] introduces an\ninnovative approach to multispectral image CD by integrating\na mix of labeled, unlabeled, and GAN-generated data. GDCN\nincorporates a discriminatory classified network that accurately\ncategorizes changes, complemented by a generator that creates\nadditional training samples. This design significantly boosts\nthe performance of the CD process, particularly in scenarios\nwith limited labeled data availability.\nDespite these advancements, GAN-based models are not\nwithout limitations, especially when compared to DDPMs.\nGANs frequently encounter challenges in training stability\nand convergence, where maintaining a balance between the\ngenerator and discriminator can be intricate. Such issues\nmay result in mode collapse, limiting the variety of outputs\nproduced by the generator. Additionally, GANs can demand\nconsiderable computational resources and exhibit sensitivity\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4\n\n\nElement-Wise Multiple\nTransC-GD-CD Framework\n\n−1 0\nADC\nFCT\nConditional Encoder \nand Decoder together \nform the CDC module\n − \n\n\n\n\n()\nIN\nConv\nElement-Wise Addition\nConditional Encoder\nConditional Decoder\nDPM Decoder\nDPM Encoder\nNEUNet\nConv\nTokenization\nTokenization\n\n\nReconstruction\nCFN\n\n\n\n\n\nShared-weight\nPost-Change Image\nPre-Change Image\nFig. 2. Architecture of the proposed TransC-GD-CD network. The CD Conditioner CDC module extracts multilevel features from pre- and post-change\nRGB images Ia and Ib. These features are then passed to the ADC module to compute informative change features. The Noise Estimation UNet (NEUNet)\nintegrates these features, employing the DPM -Encoder, FCT module, and DPM -Decoder to generate high-quality CD-related noise. The entire process\nis conditioned on temporal RS images and follows the DDPM framework.\nto hyperparameter configurations, potentially restricting their\napplication in environments with limited resources. In contrast,\nDDPMs offer a more stable training trajectory, characterized\nby gradual image refinement, which can lead to higher-quality\ngeneration with reduced training complexity. Addressing these\nchallenges is vital for enhancing the reliability and efficiency\nof GAN-based CD methods, particularly in comparison to the\nmore stable and consistent performance of DDPMs.\n2) DDPM-based models: Within the enormous models for\nCD, DDPM-based architectures emerge with distinguished\nadvantages over traditional CNNs and Transformers. These\nmodels, leveraging their intrinsic diffusion mechanisms, pos-\nsess an adeptness at handling intricate data distributions. This\nability leads to enhanced precision in depicting fine-grained\ndetails and edges within CD maps. The core of DDPM is\na diffusion model enhanced by probabilistic modeling, which\nhas the capacity to represent and adapt to a vast range of scenes\nand image attributes, from the shapes and sizes to the textures\nof terrestrial objects. This is achieved by training the model\nto morph a standard normal distribution into the empirical\ndistribution of the data. Nevertheless, a critical observation\nis that many extant DDPM models do not tap into the full\npotential of this architecture. For instance, the DDPM-CD\nmodel [52] first leans on a pre-trained DDPM for extracting\nsegmentation features from RS images. This segmented data\nis then funneled into a lightweight module tailored for the CD\ntask. However, this method potentially neglects the inherent\nadvantages of iterative sampling process during the inference\nstage intrinsic to DDPM. In essence, during this process,\nthe current-step output serves as prior information for the\nsubsequent step input. To cope with this challenge, we present\nTransG-GD-CD, which integrates variational inference, offer-\ning a more refined probabilistic framework as compared to\nDDPM-CD. A salient feature of TransG-GD-CD is its end-\nto-end training design, which progressively generates precise\nCD maps, employing iterative refinements during its inference\nphase. This novel model is adept at adaptively discerning the\ninherent complexities and nuances typical of CD contexts,\npositioning it a cut above contemporaneous solutions in terms\nof accuracy and adaptability. Fig. 1 illustrates the distinctions\nbetween DDPM-CD and the proposed TransC-GD-CD where\nIa and Ib denote pre- and post-change images respectively.\nIII. METHODOLOGY\nA. Diffusion Model\nDiffusion probabilistic models have gained a lot of attention\nin recent years due to their ability to generate extraordinary\nhigh-quality images compared to GANs, Variational Autoen-\ncoders (V AEs), autoregressive models, and flows. Owing to\ntheir superior ability to extract key semantics from training\nimages, diffusion models are also being utilized for image\nsegmentation [22, 23] and CD [52]. In this section, we present\na brief overview of the proposed diffusion model framework.\nDDPM consists of two stages, namely the forward diffusion\nstage and the reverse diffusion stage. In the forward process,\nthe segmentation label x0 is gradually added with Gaussian\nnoise through a series of steps T. During the reverse diffusion\nstage, a neural network is trained as a noise predictor to reverse\nthe noising process and subsequently, recover the original data.\n1) Forward Process: In the forward diffusion process, we\ntransform an initial data point x0 through a Markov chain\nof conditional Gaussian distributions to obtain a sequence\nx1, x2, . . . , xT . This process is formulated as:\nq(x1, . . . , xT |x0) =\nTY\nt=1\nN(xt;\np\n1 − βtxt−1, βtIn×n), (1)\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n5\nwhere βt controls the noise variance and αt = 1 − βt defines\nthe transition between states.\n2) Reverse Denoising Process: The reverse diffusion pro-\ncess aims to reconstruct the original data distribution from the\nnoisy state xT . It is described by a reverse Markov chain:\npθ(x0, . . . , xT−1|xT ) =\n1Y\nt=T\nN(xt−1; µθ(xt, t), σ2\nθ(xt, t)In×n),\n(2)\nwhere µθ(xt, t) and σ2\nθ(xt, t) are parameterized by the neural\nnetwork ϵθ. The training involves minimizing the MSE loss\nbetween the actual and predicted noise:\nL(θ) =\n\r\rϵtar − ϵθ(√αtx0 +\n√\n1 − αtϵ; θ)\n\r\r2\n, (3)\nwhere ϵtar ∼ N(0, In×n). During the inference phase, the\ndenoising process is iteratively applied to reconstruct the noisy\nimage, progressively denoising the image at each step to\nultimately yield a denoised image during the inference phase.\nIn applications such as image segmentation, this denoised\nimage can then be used to obtain more accurate segmentation\nresults.\nB. Network Details\nFig. 2 illustrates the architecture of the proposed network.\nConsistent with the standard implementation of the DDPM, we\nemploy a hybrid Transformer-convolutional framework as the\nnoise predictor, which is conditioned on temporal RS images,\nas expressed by\nϵθ(xt, Ia, Ib, t) = D\n\u0000\nFCT(E(mLC + P(xt), t), mHC ), t\n\u0001\n,\n(4)\nwhere mHC and mLC denote the high- and low-dimension CD\ncondition extracted from RS images Ia and Ib, respectively.\nP is a convolution layer employed for encoding the single-\nchannel current-step noise xt ∈ RH×W×1 and deriving a CD\nnoise feature, denoted as P(xt) ∈ R64×H×W . The feature\nP(xt) is initially conditioned by mLC, further encoded by\nthe DP M-encoder E, and then processed by the FCT, which\nintegrates the mHC into the encoded conditioned CD noise\nfeature. The attention model subsequently yields an Attention\nChange feature map called mAC, which is fed into the DP M-\nDecoder Dto obtain the CD-related noise.\nSpecifically, given RGB pre- and post-change images Ia ∈\nRH×W×3 and Ib ∈ RH×W×3, a Conditional Encoder within\nthe CDC extracts multilevel features from each image. The\ndown-sampled condition feature maps ma\ni and mb\ni produced by\nthe i-th Conditional Encoder layer are of size Ci× H\n2i−1 × W\n2i−1 ,\nwhere i is the layer index with i = 1, 2, ...,5, and Ci = 64×i.\nThese semantic features (i = 2 , ...,5) of Ia, and Ib are\nconveyed into ADC Module to compute informative change\nfeatures mCD\ni ∈ RCi× H\n2i−1 × W\n2i−1 , i = 2 , ...,5, where mCD\n5\ndenotes the highest dimension change feature map mHC ,\nwhich will be integrated into FCT as the high-dimension CD\ncondition. Extracted change features are fused and upscaled by\na Conditional Decoder in CDC to obtain a lowest dimension\nchange feature map with precise edge details, followed by\nCFN to obtain a CD-information-refined low-dimension CD\ncondition, denotes as mLC ∈ R64×H×W . Element-wise addi-\ntion is employed to fuse the mLC and P(xt).\nUpon incorporating mLC into DDPM, the conditioned CD\nnoise feature is encoded by the DPM-Encoder in DDPM,\nrepresented as E(mLC + P(xt), t). This encoded feature is\nfurther fused with mHC by the proposed FCT module, and\nseamlessly conveyed into DPM Decoder within DDPM, to\nfinally obtain the CD-related noise ϵθ(xt, Ia, Ib, t).\nThe estimated noise is then leveraged to facilitate the\nsampling process each step iteratively, based on Equation (1)\n- (4), obtaining\nxt−1 = 1√at\n\u0012\nxt − 1 − at√1 − ¯at\nϵ(xt, Ia, Ib, t)\n\u0013\n+ ˜β\n1\n2 z, (5)\nwhere z ∼ N(0, In×n) is a random vector, each of whose\nelements follows an independent and identically distributed\n(i.i.d) standard normal distribution.\nThrough 1000 iterations sampling [53], the single-channel\nCD map is ultimately generated, sampled from 2D Gaussian\nnoise.\n1) Change Detection Conditioner (CDC): To address the\nchallenge of accurately identifying and delineating changes in\ncomplex, high-dimensional image data, we introduce a spe-\ncialized conditional module termed the CDC. Given pre- and\npost-change RGB images Ia ∈ RH×W×3 and Ib ∈ RH×W×3,\nthe CDC employs a Conditional Encoder to derive multilevel\nfeatures from each image respectively. Mathematically, the\ni(= 1, 2, ...,5)-th level conditional feature maps derived from\npre- and post-change images are denoted byma\ni and mb\ni, where\nma\ni = conv(Ia), (6)\nmb\ni = conv(Ib). (7)\nUpon extracting semantic features from both Ia and Ib, the\ninformation is funneled into the ADC Module to compute\na set of highly informative change features mCD\ni , which are\nformulated as follows:\nmCD\ni = conv(A) + conv(D) + conv2t1(Co), (8)\nA = Relu(mb\ni − ma\ni ), (9)\nD = Relu(ma\ni − mb\ni), (10)\nCo = Relu\n\u0000\nConcat(ma\ni , mb\ni)\n\u0001\n, (11)\nwhere A, D, and Co represent the Appear, Disappear, and\nConcatenated features between the pre- and post-change im-\nages, respectively.\nNotably, the highest dimension change feature map is de-\nnoted by mHC ∈ RC5× H\n16 ×W\n16 , serving as the high-dimension\nCD condition. Moreover, upon the extraction of change\nfeatures mCD\ni , an intricately designed Conditional Decoder\nwithin CDC performs feature fusion and upscaling to yield a\nfeature map mout ∈ R64×H×W , which is the lowest dimension\nchange feature map with well-defined edge details. To further\nenhance the fidelity of CD, the CFN module is integrated,\nresulting in a refined low-dimension CD condition denoted as\nmLC ∈ R64×H×W , where\nmLC = conv(IN(mout)) ⊗ Soft-MaskCD , (12)\nSoft-MaskCD = conv(mout), (13)\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n6\nInput1 Input2 NC\n256×320 256×320\n320×256\n256×256\nAC\n256×320 256×320\n256×320\nIFT\nOutput\n256×320\nFT FT\nTranspose\nLinear TransformationLinear Transformation\nMatrix Multiplication\nMatrix Multiplication\nQ K V\nFig. 3. Illustration of the FCT module within the TransC-GD-CD network.\nThe FCT is meticulously designed to bridge the gap between the noise\nfeature and CD condition, ensuring a seamless and coherent integration. It\nutilizes Fourier transformations and an attention mechanism to achieve this\nintegration, resulting in the Attention Change feature map mAC.\nwith IN being the instance normalization. Specifically, mout\nis processed by a convolution layer to generate a CD-related\nSoft-MaskCD ∈ R1×H×W . In the meantime, Layer Nor-\nmalization (LN) is applied to mout to stabilize the feature\ndistribution, followed by a convolution layer to produce a CD\nfeature map conv(LN(mout)) ∈ R64×H×W . The Soft-Mask is\nthen combined with this CD feature map through element-wise\nmultiplication, further enhancing the CD details and resulting\nin the refined low-dimension CD condition mLC.\nIn the final computational step of CDC, an element-wise\naddition operation fuses mLC with the pre-computed CD noise\nfeature P(xt), enriching the feature map conveyed into Noise\nEstimation UNet (NEUNet) [53] with both noise-distributed\nand change-oriented elements\nmk\nn = P(xt) + mLC, k= 1, (14)\nwhere mk\nn denotes the conditioned CD noise feature.\n2) Noise Estimation UNet (NEUNet): NEUNet Consists of\nDP M-Encoder, FCT module, and DP M-Decoder, integrat-\ning the low- and high-dimension CD conditions to generate\nhigh-quality CD-related noise.\nSpecifically, DP M-Encoder aims to learn enriched con-\nditioned CD noise features of mk\nn, which consists of four\nResidual blocks, indexed by the k(= 2 , 3, 4, 5)-th level re-\nspectively. Each level of block comprises downsample layers\nthat perform spatial downsampling using a convolutional layer\nwith a 2 ×2 kernel size and stride of 2. Each of these stages\nis equipped with the LN preceding the convolution operation.\nThe dimension is changed to 64 × H × W after the stem.\nAs the image progresses through the encoder, each of the\nsubsequent stages performs 2× downsampling. Thus, the\nfeature dimensions of the output after the respective stages\nare 128× H\n2 × W\n2 , 192× H\n4 × W\n4 , and 256× H\n8 × W\n8 . The fifth\nstage further reduces it to 320 × H\n16 × W\n16 denoted as m5\nn. A\nfeature vector with a dimension of 320 × H\n16 × W\n16 is obtained\nafter processing through the ResNet-based encoder.\nMoreover, to address the inherent challenge of effectively\nfusing high-level CD conditions with noise features, we intro-\nduce FCT, meticulously designed to bridge the gap between\nthe noise feature and CD condition by ensuring a seamless\nand coherent integration of the high-dimension CD condition\nmHC with the encoded conditioned CD noise feature m5\nn.\nSpecifically, the mHC and m5\nn are first transformed into\ntwo tokens denoted as C ∈ R256×320 and N ∈ R256×320\nrespectively. After tokenization, the tokens are processed by\nFourier transformation F T, and query, key, and value weight\nmatrix Wq, Wk, Wv, obtaining corresponding tokens Q, K,\nV, denoted by\nQ = F T(C)Wq, K = F T(N)Wk, V = F T(N)Wv, (15)\nMoreover, the attention mechanism is invoked to derive the\ncross attention between C and N. The attention computation\nis captured by\nAC = IF T(SV) = IF T\n\u0012\nΦ(QKT\n√\nC\n)V\n\u0013\n, (16)\nwhere IF T(·) is the inverse Fourier transformation process,\nand Φ(·) represents the SoftMax function. In addition, KT\ndenotes the transposed token of K, and the dimension C =\n320. Note that the S represents the weighting parameters of\nall channels, while the output Attention Change Token AC ∈\nR256×320 is derived with channel weighting.\nFinally, AC is reconstructed by the reconstruction module\nto generate Attention Change feature map mAC whose size\nis 320 × H\n16 × W\n16 . In summary, after performing the fusion at\nFCT, we obtain attention change feature map mAC.\nFinally, the Decoder begins with mk\nn ∈ R320× H\n16 ×W\n16 from\nthe Encoder’s final stage. The stages in the Decoder are\naccomplished by employing an upsample layer equipped with\na transposed convolution operation, having a 2 ×2 kernel size\nand a stride of 2. After these convolution processes with the\nDPM-Decoder, the dimensions are expanded to 1 × H × W.\nThe NEUNet’s comprehensive architecture ensures a seam-\nless flow of operations from encoding to fusion, and ultimately\nto decoding. This capability makes NEUNet an indispensable\ncomponent for achieving high-precision noise estimation in\ncomplex CD tasks. By explicitly modeling and conditioning\nthe noise, NEUNet contributes to the generation of a more\naccurate and reliable CD map.\nIV. E XPERIMENT\nA. Experimental Dataset\nIn this section, extensive experiments are performed on\nthree CD datasets, namely the WHU-CD [54], the LEVIR-\nCD dataset [55], and the CD Dataset (CDD) [56].\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n7\n• CD Dataset (CDD) : A publicly available dataset designed\nfor CD across four seasons, encompassing image pairs with\nvarying spatial resolutions between 3 and 100 centimeters\nper pixel. This resolution range facilitates accurate char-\nacterization of objects of diverse sizes, from automobiles\nto substantial construction structures. Notably, the CDD\ncaptures seasonal variations of natural elements, including\nindividual trees and extensive forest regions. The dataset is\norganized into a training set of 10, 000 images, a test set of\n3, 000 images, and a validation set of 3, 000 images, with\neach image dimensioned at 256 × 256 pixels.\n• WHU-CD : A public CD dataset featuring high-resolution\naerial images with a spatial resolution of 7.5 centimeters.\nThe dataset was divided into non-overlapping patches of\nsize 256 ×256, and subsequently split into a training set of\n6096 patches, a test set of 762 patches, and a validation set\nof 762 patches.\n• LEVIR-CD: A publicly available large-scale CD dataset for\nbuildings, comprising 637 pairs of high-resolution RS im-\nages with a spatial resolution of 0.5 meters. Upon cropping\nthese images into non-overlapping patches of size 256×256,\nwe acquired 7120, 1024, and 2048 pairs of patches for\ntraining, validation, and testing, respectively.\n• OSCD (Onera Satellite Change Detection) : The OSCD\ndataset features 24 pairs of multispectral images from\nSentinel-2 satellites, covering 2015 to 2018. It includes\ndiverse global locations, focusing on urban changes like new\nbuildings and roads. The dataset provides 13 spectral bands\nwith resolutions from 10 to 60 meters. It’s divided into 14\ntraining and 10 test pairs, ideal for developing and testing\nCD algorithms in RS.\nB. Implementation Details\nOur experiments were conducted using the PyTorch frame-\nwork on a single NVIDIA GeForce RTX 3090 Ti GPU. In the\nexperiments, we performed 1000 diffusion steps for inference.\nAll images were uniformly resized to a dimension of 256×256\npixels. We employed the Stochastic Gradient Descent (SGD)\nalgorithm with momentum for optimization, training the model\nwith a batch size of 32. The momentum and weight decay\nparameters were set to 0.99 and 0.0005, respectively. The\nlearning rate was initially set to 0.0001, and linearly decayed\nto zero over 200 epochs.\nC. Evaluation Metrics\nIn CD analysis, evaluating model efficacy is often accom-\nplished through key performance metrics such as the F1-\nscore, Intersection over Union (IoU), and Overall Accuracy\n(OA). These metrics elucidate different facets of a CD model’s\nperformance.\nThe F1-score represents a harmonic mean of precision and\nrecall, encapsulating the model’s ability to detect changes\naccurately and comprehensively. It is calculated based on the\nrelationship between true positives (TP), false positives (FP),\nand false negatives (FN) as outlined below\nF1 = 2\n1\nRecall + 1\nPrecision\n, (17)\nRecall = TP\nTP + FN, (18)\nPrecision = TP\nTP + FP, (19)\nIoU, on the other hand, measures the overlap between two\ndatasets, typically between the model’s predicted change map\nand the ground truth. In the context of CD, IoU is defined as\nIoU = TP\nTP + FN + FP. (20)\nLastly, OA provides an overarching view of the model’s\nperformance, indicating the proportion of correctly identified\ninstances. The formula for OA, which includes both true\npositive (TP) and true negative (TN) categories, is given by\nOA = (TP + TN)\n(TP + TN + FN + FP), (21)\nwhere TN denotes the number of true negatives.\nD. Comparison\nA comprehensive comparison was conducted to evaluate\nthe proposed method, TransC-GD-CD, against five state-of-\nthe-art models reported in the literature: convolution-based\ntechniques FC-SC [9] and SNUNet [11], transformer-based\nmodels BIT [43] and ChangeFormer [44], and DDPM-based\nmodels DDPM-CD [52] and GCD-DDPM [32]. Specifically,\nFC-SC employs a Siamese Fully Convolutional Network to\nextract multi-level features, utilizing feature concatenation as\na fusion strategy for bitemporal information. SNUNet extracts\nhigh-resolution high-level features by jointly leveraging the\nSiamese network and NestedUNet architectures [57]. BIT\nfacilitates the identification of changes of interest while ex-\ncluding irrelevant alterations by incorporating the transformer\narchitecture into the CD task [43]. ChangeFormer [44] utilizes\na hierarchical transformer encoder in a Siamese architecture\nwith a simple Multi-Layer Perceptron decoder. DDPM-CD\nemploys the DDPM as a feature extractor for CD, while GCD-\nDDPM is optimized end-to-end by leveraging the inference\nvariation with DDPM, and can generate CD maps directly.\nE. Experiment on WHU Building CD Data Set\nWe extended the validation of the Trans-GD-CD method’s\neffectiveness by undertaking experiments on the WHU build-\ning CD dataset. The WHU dataset encapsulates a spectrum\nof urban transitions, including the emergence of newly con-\nstructed buildings, demolition of existing structures, and evolv-\ning land cover patterns. Given the complex urban landscapes\nand diverse land cover attributes encapsulated in the dataset,\nseveral challenges manifest. These challenges include the\noccurrence of pseudo-changes and the potential for misclas-\nsifications in the resultant CD maps. A closer inspection of\nFig. 4(c)-(h) illuminates these challenges, as it showcases clear\ninaccuracies, such as missed detections and false positives\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8\n1-a 1-b 1-c 1-d 1-e\n1-f 1-g 1-i 1-j1-h\n2-a 2-b 2-c 2-d 2-e\n2-f 2-g 2-i 2-j2-h\n3-a 3-b 3-c 3-d 3-e\n3-f 3-g 3-h 3-i 3-j\nFig. 4. Comparision of different state-of-the-art CD methods on WHU dataset: (a) Pre-change image, (b) Post-change image, (c) Ground-truth, (d) FC-SC,\n(e) SNUNet, (f) BIT, (g) ChangeFormer, (h) DDPM-CD, (i) GCD-DDPM, and (j) the proposed Trans-GD-CD.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n9\n1-a 1-b 1-c 1-d 1-e\n1-f 1-g 1-h 1-i\n2-a 2-b 2-c 2-d 2-e\n2-f 2-g 2-i 2-j\n1-j\n2-h\n3-a 3-b 3-d 3-e3-c\n3-f 3-g 3-i 3-j3-h\nFig. 5. Comparision of different state-of-the-art CD methods on Levir dataset: (a) Pre-change image, (b) Post-change image, (c) Ground-truth, (d) FC-SC,\n(e) SNUNet, (f) BIT, (g) ChangeFormer, (h) DDPM-CD, (i) GCD-DDPM, and (j) the proposed Trans-GD-CD.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n10\n1-a 1-b 1-c 1-d 1-e\n1-f 1-g 1-h 1-i\n 1-j\n2-a 2-b 2-c 2-d 2-e\n2-f 2-g 2-h 2-i 2-j\n3-a 3-b 3-c 3-d 3-e\n3-f 3-g 3-h 3-i 3-j\nFig. 6. Comparision of different state-of-the-art CD methods on CDD dataset: (a) Pre-change image, (b) Post-change image, (c) Ground-truth, (d) FC-SC,\n(e) SNUNet, (f) BIT, (g) ChangeFormer, (h) DDPM-CD, (i) GCD-DDPM, and (j) the proposed Trans-GD-CD.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n11\n1-a 1-b 1-c 1-d 1-e\n1-f 1-g 1-h 1-i 1-j\n2-a 2-b 2-c 2-d 2-e\n2-f 2-g 2-h 2-i 2-j\n3-a 3-c3-b 3-d 3-e\n3-f 3-g 3-i 3-j3-h\nFig. 7. Comparision of different state-of-the-art CD methods on OSCD dataset: (a) Pre-change image, (b) Post-change image, (c) Ground-truth, (d) FC-SC,\n(e) SNUNet, (f) BIT, (g) ChangeFormer, (h) DDPM-CD, (i) GCD-DDPM, and (j) the proposed Trans-GD-CD.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n12\namong various urban entities, in several established models. In\nstark contrast, the Trans-GD-CD method exhibited remarkable\nresilience against these challenges, proficiently suppressing\nnoise and pseudo-changes. This ensures a high degree of\nauthenticity in delineating urban modifications, while concur-\nrently preserving the structural integrity of urban entities.\nThis qualitative analysis is robustly substantiated by the\nquantitative results showcased in Table I. The Trans-GD-\nCD method consistently eclipsed its counterparts, registering\ndefinitive improvements in pivotal metrics such as the F1\nscore, OA, and IoU. These advancements underscore the\nmodel’s prowess in accurately discerning urban transforma-\ntions compared to other prevalent techniques.\nF . Evaluation on LEVIR-CD Dataset\nSubsequently, we executed analogous experiments on the\nLEVIR-CD dataset, which is tailored for the detection of\nbuilding changes across various scales. As depicted in Fig. 5,\nthe proposed TransC-GD-CD model exhibits superior ca-\npability in preserving the internal compactness of building\nobjects while effectively suppressing noise, in contrast to other\nexisting models. Notably, seasonal and illumination variations\nin the bitemporal images often engender significant spectral\nvariability, leading to the manifestation of pseudo-changes in\nthe CD maps. This issue is evident from the small building\ntargets in Fig. 5, where pseudo-changes were induced on the\nbuilding roofs, while numerous small building targets were\noverlooked. Conversely, the proposed TransC-GD-CD model\nadeptly detected the majority of the small targets, concurrently\nsuppressing the pseudo-changes.\nTable II underscores that the proposed TransC-GD-CD\nmodel significantly outstrips the existing models on the\nLEVIR-CD dataset across all evaluated performance met-\nrics. In particular, the conventional GCD-DDPM generated a\nslightly higher recall, i.e. 91.24, at the cost of worse FP and\nsubsequently precision performance. In contrast, GCD-DDPM\nachieved a comparable recall performance of 90.81 as well as\nimpressive precision performance.\nG. Experiment on CD DataSet (CDD)\nThe CD Dataset is meticulously curated to focus on\nseasonal-variance in RS image changes, thereby posing a\nchallenge to CD algorithms by incorporating images acquired\nduring different seasons, each exhibiting distinct spectral char-\nacteristics. As depicted in Fig. 6, panels (a) − (c) represent\npre- and post-change images alongside the ground truth. The\nCD images encompass a variety of land cover and land use\ntypes, including urban areas and agricultural fields, captured\nacross various seasons.\nPanels (d) − (i) in Fig. 6 elucidate the predictive capabil-\nities of TransG-CD-GD in comparison with other methods.\nSeasonal transitions in vegetation and other land cover types\nmay manifest as areas of pseudo-changes on the CD maps, as\nillustrated in Fig. 6. This intricacy intensifies the challenge\nfor algorithms in distinguishing actual changes from those\ninduced by seasonal variability. The proposed TransG-CD-\nGD method demonstrates notable advantages over competing\nTABLE I\nTHE QUANTITATIVE EXPERIMENTAL RESULTS (%) ON THE WHU. THE\nVALUES IN BOLD ARE THE BEST\nMethod Recall Precision OA F1 Iou\nFC-SC 86.54 72.03 98.42 78.62 64.37\nSNUNet 81.33 85.66 98.68 83.44 71.39\nBIT 87.94 89.98 99.30 88.95 81.53\nChangeFormer 86.43 89.69 98.95 88.03 78.46\nDDPM-CD 92.05 92.71 99.37 92.38 85.84\nGCD-DDPM 92.29 92.79 99.39 92.54 86.52\nTransC-GD-CD 92.86 93.44 99.43 93.15 87.42\nalgorithms, notably in retaining the details of the predicted\nchange map and accurately delineating the actual boundaries\nof the changed objects.\nTABLE II\nTHE QUANTITATIVE EXPERIMENTAL RESULTS (%) ON THE LEVIR. THE\nVALUES IN BOLD ARE THE BEST\nMethod Recall Precision OA F1 Iou\nFC-SC 77.29 89.04 98.25 82.75 69.95\nSNUNet 84.33 88.55 98.70 86.39 76.11\nBIT 87.85 90.26 98.83 89.04 80.12\nChangeFormer 87.73 89.39 98.81 88.56 79.34\nDDPM-CD 89.67 91.39 99.06 90.52 82.73\nGCD-DDPM 91.24 90.68 99.14 90.96 83.56\nTransC-GD-CD 90.81 91.49 99.17 91.15 83.95\nTable III delineates the performance of the six models\nunder examination, evaluated based on OA, F1-score, Recall,\nPrecision, and IoU. A perusal of Table III indicates that the\nproposed TransC-GD-CD model outperforms the others in the\nmajority of the performance metrics.\nH. Evaluation on OSCD Dataset\nThe experiments on the OSCD dataset underscored the effi-\ncacy of the TransC-GD-CD model in handling low-resolution\nCD scenarios. The OSCD dataset, with its diversity of ur-\nban and natural landscapes captured in multispectral images,\npresents unique challenges in identifying subtle changes,\nparticularly in low-resolution settings. These challenges are\nfurther compounded by the presence of pseudo-changes and\nvarying spectral characteristics across different regions.\nIn our evaluation, as illustrated in Fig. 7 (to be inserted), the\nTransC-GD-CD model displayed a remarkable capability to\ndistinguish actual changes from background noise and pseudo-\nchanges, particularly in urban areas where the complexity of\nchanges is higher. This proficiency is attributed to the model’s\nrobust feature extraction and fusion mechanisms, enabling it\nto discern subtle variations in multispectral data effectively.\nThe quantitative results, presented in Table IV, reveal the\nsuperiority of TransC-GD-CD over other models in key perfor-\nmance metrics such as IoU and F1 score. Notably, while GCD-\nDDPM exhibited a higher recall, TransC-GD-CD achieved the\nbest balance between precision and recall, as evidenced by\nits highest F1 score and IoU. This balance is crucial in low-\nresolution datasets like OSCD, where accurately identifying\nchanges without overestimating them is essential.\nThese findings confirm the adaptability of the TransC-GD-\nCD model to various resolution scales, affirming its potential\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n13\n1-a 1-b 1-c 1-d 1-e 1-f\n2-a 2-b 2-c 2-d 2-e 2-f\n3-a 3-b 3-c 3-d 3-e 3-f\nFig. 8. A comparison of heatmaps generated by state-of-the-art methods on Levir dataset: (a), (b), and (c) represent the pre-change image, post-change image,\nand change map respectively. For the heatmaps, (d), (e), and (f) correspond to the feature maps in the decoder for the CNN method FC-EF, Transformer\nmethod BIT, and the proposed DDPM method TransG-GD-CD methods, respectively.\nTABLE III\nTHE QUANTITATIVE EXPERIMENTAL RESULTS (%) ON THE CDD. THE\nVALUES IN BOLD ARE THE BEST\nMethod Recall Precision OA F1 Iou\nFC-SC 71.10 78.62 94.55 74.67 58.87\nSNUNet 80.29 84.52 95.73 82.35 69.91\nBIT 90.75 86.38 97.13 88.51 79.30\nChangeFormer 93.64 94.54 98.45 94.09 88.94\nDDPM-CD 94.43 95.05 98.81 94.74 90.05\nGCD-DDPM 95.10 94.76 98.87 94.93 90.56\nTransG-GD-CD 96.33 97.13 99.23 96.73 93.58\nTABLE IV\nTHE QUANTITATIVE EXPERIMENTAL RESULTS (%) ON THE OSCD. THE\nVALUES IN BOLD ARE THE BEST\nMethod Recall Precision OA F1 Iou\nFC-SC 54.83 47.97 94.55 51.17 34.33\nSNUNet 60.49 48.62 94.63 53.91 36.13\nBIT 50.09 65.64 94.63 56.82 40.26\nChangeFormer 49.37 62.90 95.20 55.32 38.10\nDDPM-CD 56.61 60.74 95.92 58.60 41.95\nGCD-DDPM 73.94 50.60 95.84 60.08 43.29\nTransG-GD-CD 56.85 66.29 96.01 61.21 44.86\nfor broad applicability in CD tasks. The model’s advanced\narchitecture effectively hones in on significant changes, setting\na new benchmark in low-resolution CD accuracy.\nI. Discerning Superiority through Heatmap Comparisons\nTo better demonstrate the performance improvement\narchived, the Gradient-weighted Class Activation Mapping\n(Grad-CAM) was employed to visualize the feature map\noutput from the Decoder layer.\nThe images on the first three columns of Fig. 8 depict\nthe pre-change image, post-change image, and the ground\ntruth (GT) representation respectively. The heatmaps derived\nfrom the decoder layers using FC-EF, BIT, and the proposed\nTransC-GD-CD is shown in the next three columns. For Grad-\nCAM visualization, a red dot was marked in the change region.\nIt is worth noting that brighter pixels have higher scores and\nare more likely to be classified as change regions. Upon a thor-\nough analysis of Fig. 8, it becomes evident that methodologies\nsuch as FC-EF and BIT, while competent, demonstrate certain\nlimitations in their heatmap representations, particularly in\ncapturing the subtleties of change dynamics. While acknowl-\nedging the clear advantage of DDPM in reconstructing edge\ndetails, we acknowledge that its internal structure perception\nmight be slightly weaker than that of Transformer (BIT). We\nattribute this to the special mechanism of DDPM, simulating\nimage generation through noise diffusion, which may intro-\nduce subtle structures or textures perceived as visual noise\nor irregularities in the heatmap. Taking into account both\ninternal padding details and edge details, we argue that the\nperformance of TransG-GD-CD is superior. We emphasize\nthe importance of quantitative evaluation. TransC-GD-CD\nexhibits superior quantitative performance, with IoU values\nof 87.42%, 83.95%, and 93.58% on the WHU, LEVIR, and\nCDD datasets, respectively. These results outperform BIT,\nwhich achieved IoU values of 81.53%, 80.12%, and 79.30% on\nthe same datasets. Consequently, TransC-GD-CD, built upon\nthe DDPM, exhibits the superior performance compared to\ntraditional CNN and Transformer methodologies. It’s evident\nthat by leveraging the variational inference technique within\nDDPM, TransC-GD-CD has managed to achieve a level of\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n14\nTABLE V\nABLATION EXPERIMENT RESULTS (%) ON THE WHU DATASET.THE\nVALUES IN BOLD ARE THE BEST\nADC high-dimension Low-dimension OA F1 IoU\n✓ ✓ 99.37 92.49 86.31\n✓ ✓ 99.36 92.13 85.46\n✓ ✓ ✓ 99.43 93.15 87.42\n✓ ✓ 99.39 92.81 86.92\nprecision and efficiency that surpasses these conventional\napproaches.\nJ. Ablation Study\nNext, we conducted on two series of ablation experiments\ncentered around the proposed TransC-GD-CD. First, we in-\ntended to demonstrate the effectiveness of High-dimension and\nLow-dimension Conditions in CD tasks. Two separate ablation\nexperiments were undertaken: in the first ablation experiment,\nonly the high-dimension CD Condition was employed, while\nin the second experiment, only the low-dimension CD condi-\ntion was utilized.\nThe results from the first set of ablation experiments, as\ndepicted in the first three rows of Table V, indicate that\nthe High- and Low-level Conditions played a crucial role\nin enhancing the CD results. Specifically, the large improve-\nment was derived from incorporating both High- and Low-\ndimension CD Conditions (the third row). As compared to\nthe case with Low-dimension CD Condition (the second row)\nor High-dimension CD Condition (the first row) only, which\nconfirmed the importance of the Low-dimension or High-\ndimension Condition.\nIn the second set of ablation experiments, we removed\nthe proposed ADC module from the TransC-GD-CD model.\nAs shown in the last row of Table V, the removal of the\nADC module resulted in noticeable performance degradation\nacross all three performance metrics compared to the complete\nTransC-GD-CD model. In particular, the F1-score experienced\na degradation of 0.34%, and the IoU decreased by 0.5%.\nThis ablation experiment substantiated that the ADC module\nis instrumental in extracting abundant multi-type difference\nfeatures, thereby addressing the information loss issue to some\nextent.\nK. Efficiency\nTABLE VI\nCOMPARISON ON COMPUTATION COMPLEXITY\nMethod GFLOPs (G) Para (M)\nFC-SC 4.73 1.35\nDT-SCN 13.22 31.26\nSNUNet 13.79 3.01\nBit 8.75 3.04\nChangeFormer 202.7 41.02\nDDPM-CD 838.60 434.90\nGCD-DDPM 269.50 130.80\nTrans-GD-CD 307.21 149.54\nV. E FFICIENCY ANALYSIS OF TRANS -GD-CD\nTo comprehensively evaluate the efficiency of our newly\nproposed Trans-GD-CD model, we conducted a comparative\nanalysis of its computational and space complexity against\nvarious leading CD models. This assessment included FC-SC,\nDT-SCN, SNUNet, Bit, ChangeFormer, DDPM-CD, GCD-\nDDPM, and Trans-GD-CD itself. We focused on two crucial\nmetrics: floating-point operations per second (FLOPs) and\nthe number of trainable parameters, which together offer\ninsights into both the computational demands and memory\nrequirements of these models. As indicated in Table VI, Trans-\nGD-CD demonstrates a computational complexity of 307.21\nGFLOPs. This figure positions it above most of the non-\nDDPM-based models in terms of computational load, but still\nconsiderably more efficient than DDPM-CD, which has a\ncomplexity of 838.60 GFLOPs. In terms of space complexity,\nTrans-GD-CD requires approximately 149.54 million trainable\nparameters. While this is higher than most other models,\nincluding non-DDPM-based ones, it’s important to note that\nTrans-GD-CD’s architectural sophistication and the inclusion\nof advanced modules such as FCT and CFN contribute to this\nincreased parameter count.\nDespite its higher computational and space complexity,\nthe performance advantages of Trans-GD-CD in CD tasks\nare evident. The model successfully leverages its elaborate\narchitecture, incorporating the efficiency of the Transformer\nand the generative prowess of DDPM, to deliver superior\nCD results. This is particularly notable when considering the\ncomplex nature of CD tasks, where accuracy and the ability\nto handle intricate image dynamics are paramount.\nIn conclusion, while Trans-GD-CD exhibits a relatively\nhigher computational load compared to some of its counter-\nparts, this is offset by its enhanced performance and accuracy\nin CD. The model represents a significant step forward in\nleveraging advanced neural network architectures for effective\nCD in RS images.\nVI. CONCLUSION\nThis paper presents a novel framework, namely TransC-GD-\nCD, devised to tackle persistent challenges in CD utilizing\nhigh-resolution RS data. The challenge lies in the simultaneous\ncapture of long-range dependencies and local spatial informa-\ntion from pre- and post-change images with diverse imaging\nconditions and temporal intervals. Our solution synergizes the\nprowess of Transformer architectures with generative diffusion\nframeworks based on the innovative FCT mechanism. This\nmechanism seamlessly fuses CD conditions with noise features\nwithin a DDPM. Furthermore, we introduce a multi-type\ndifference extraction module named ADC aimed at optimizing\nboth segmentation extraction and CD classification, thereby\nmitigating the information loss inherent to traditional CD\nmethodologies. Rigorous comparative experiments on CD\ndatasets including CDD, WHU, and LEVIR underscore the\nsuperior performance of TransC-GD-CD in constructing accu-\nrate CD maps. This work not only addresses obstacles in the\nCD domain but also paves a promising trail by combining\nTransformer architectures with diffusion-based frameworks.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n15\nThis significant advancement moves us closer to precise and\nefficient monitoring of geographic alterations over time, thus\nlaying a solid groundwork for future explorations aimed at\nelevating CD performance.\nREFERENCES\n[1] X. Yang, Z. Lv, J. Atli Benediktsson, and F. Chen, “Novel\nspatial–spectral channel attention neural network for land cover\nchange detection with remote sensed images,” Remote Sensing,\nvol. 15, no. 1, p. 87, 2022.\n[2] A. Song and J. Choi, “Fully convolutional networks with\nmultiscale 3d filters and transfer learning for change detection\nin high spatial resolution satellite images,” Remote Sensing ,\nvol. 12, no. 5, p. 799, 2020.\n[3] J. Zheng, Y . Tian, C. Yuan, K. Yin, F. Zhang, F. Chen, and\nQ. Chen, “Mdesnet: Multitask difference-enhanced siamese\nnetwork for building change detection in high-resolution remote\nsensing images,” Remote Sensing, vol. 14, no. 15, p. 3775, 2022.\n[4] A. Singh, “Change detection in the tropical forest environment\nof northeastern india using landsat,” Remote sensing and tropi-\ncal land management , vol. 44, pp. 273–254, 1986.\n[5] J. Ma, M. Gong, and Z. Zhou, “Wavelet fusion on ratio images\nfor change detection in sar images,” IEEE Geoscience and\nRemote Sensing Letters , vol. 9, no. 6, pp. 1122–1126, 2012.\n[6] X. Dai and S. Khorram, “Quantification of the impact of\nmisregistration on the accuracy of remotely sensed change\ndetection,” in IGARSS’97. 1997 IEEE International Geoscience\nand Remote Sensing Symposium Proceedings. Remote Sensing-\nA Scientific Vision for Sustainable Development , vol. 4. IEEE,\n1997, pp. 1763–1765.\n[7] T. Celik, “Unsupervised change detection in satellite images\nusing principal component analysis and k-means clustering,”\nIEEE geoscience and remote sensing letters , vol. 6, no. 4, pp.\n772–776, 2009.\n[8] W. A. Malila, “Change vector analysis: An approach for detect-\ning forest changes with landsat,” in LARS symposia , 1980, p.\n385.\n[9] Y . Zhan, K. Fu, M. Yan, X. Sun, H. Wang, and X. Qiu, “Change\ndetection based on deep siamese convolutional network for\noptical aerial images,” IEEE Geoscience and Remote Sensing\nLetters, vol. 14, no. 10, pp. 1845–1849, 2017.\n[10] R. C. Daudt, B. Le Saux, and A. Boulch, “Fully convolutional\nsiamese networks for change detection,” in 2018 25th IEEE\nInternational Conference on Image Processing (ICIP) . IEEE,\n2018, pp. 4063–4067.\n[11] S. Fang, K. Li, J. Shao, and Z. Li, “Snunet-cd: A densely\nconnected siamese network for change detection of vhr images,”\nIEEE Geoscience and Remote Sensing Letters , vol. 19, pp. 1–5,\n2021.\n[12] C. Serief, Y . Ghelamallah, and Y . Bentoutou, “Deep learning-\nbased system for change detection on-board earth observation\nsmall satellites,” IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing , 2023.\n[13] L. Wang, M. Zhang, X. Shen, and W. Shi, “Landslide map-\nping using multilevel-feature-enhancement change detection\nnetwork,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , vol. 16, pp. 3599–3610,\n2023.\n[14] C. Zhang, L. Wang, S. Cheng, and Y . Li, “Swinsunet: Pure\ntransformer network for remote sensing image change detec-\ntion,” IEEE Transactions on Geoscience and Remote Sensing ,\nvol. 60, pp. 1–13, 2022.\n[15] B. Sun, Q. Liu, N. Yuan, J. Tan, X. Gao, and T. Yu, “Spectral\ntoken guidance transformer for multisource images change\ndetection,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , vol. 16, pp. 2559–2572,\n2023.\n[16] Y . Lin, S. Liu, Y . Zheng, X. Tong, H. Xie, H. Zhu, K. Du,\nH. Zhao, and J. Zhang, “An unsupervised transformer-based\nmultivariate alteration detection approach for change detection\nin vhr remote sensing images,” IEEE Journal of Selected Topics\nin Applied Earth Observations and Remote Sensing , 2024.\n[17] J. Guo, K. Han, H. Wu, Y . Tang, X. Chen, Y . Wang, and C. Xu,\n“Cmt: Convolutional neural networks meet vision transformers,”\nin Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2022, pp. 12 175–12 185.\n[18] H. Li, Y . Yang, M. Chang, S. Chen, H. Feng, Z. Xu, Q. Li, and\nY . Chen, “Srdiff: Single image super-resolution with diffusion\nprobabilistic models,” Neurocomputing, vol. 479, pp. 47–59,\n2022.\n[19] J. Liu, Z. Yuan, Z. Pan, Y . Fu, L. Liu, and B. Lu, “Diffusion\nmodel with detail complement for super-resolution of remote\nsensing,” Remote Sensing, vol. 14, no. 19, p. 4834, 2022.\n[20] S. Gao, X. Liu, B. Zeng, S. Xu, Y . Li, X. Luo, J. Liu, X. Zhen,\nand B. Zhang, “Implicit diffusion models for continuous super-\nresolution,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2023, pp. 10 021–\n10 030.\n[21] S. Shang, Z. Shan, G. Liu, and J. Zhang, “Resdiff: Combining\ncnn and diffusion model for image super-resolution,” arXiv\npreprint arXiv:2303.08714, 2023.\n[22] J. Wolleb, R. Sandk ¨uhler, F. Bieder, P. Valmaggia, and P. C.\nCattin, “Diffusion models for implicit image segmentation en-\nsembles,” in International Conference on Medical Imaging with\nDeep Learning. PMLR, 2022, pp. 1336–1348.\n[23] E. A. Brempong, S. Kornblith, T. Chen, N. Parmar, M. Min-\nderer, and M. Norouzi, “Denoising pretraining for semantic\nsegmentation,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , 2022, pp. 4175–4186.\n[24] W. H. Pinaya, M. S. Graham, R. Gray, P. F. Da Costa, P.-\nD. Tudosiu, P. Wright, Y . H. Mah, A. D. MacKinnon, J. T.\nTeo, R. Jager et al., “Fast unsupervised brain anomaly detection\nand segmentation with diffusion models,” in International Con-\nference on Medical Image Computing and Computer-Assisted\nIntervention. Springer, 2022, pp. 705–714.\n[25] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n“High-resolution image synthesis with latent diffusion models,”\nin Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2022, pp. 10 684–10 695.\n[26] A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Timofte, and\nL. Van Gool, “Repaint: Inpainting using denoising diffusion\nprobabilistic models,” in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2022, pp.\n11 461–11 471.\n[27] J. Lei, J. Tang, and K. Jia, “Rgbd2: Generative scene synthesis\nvia incremental view inpainting using rgbd diffusion models,” in\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2023, pp. 8422–8434.\n[28] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and\nT. Salimans, “Cascaded diffusion models for high fidelity image\ngeneration.” J. Mach. Learn. Res. , vol. 23, no. 47, pp. 1–33,\n2022.\n[29] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hi-\nerarchical text-conditional image generation with clip latents,”\narXiv preprint arXiv:2204.06125 , 2022.\n[30] M. Kim, F. Liu, A. Jain, and X. Liu, “Dcface: Synthetic face\ngeneration with dual condition diffusion model,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023, pp. 12 715–12 725.\n[31] Z. Dorjsembe, H.-K. Pao, S. Odonchimed, and F. Xiao, “Con-\nditional diffusion models for semantic 3d medical image syn-\nthesis,” arXiv preprint arXiv:2305.18453 , 2023.\n[32] Y . Wen, J. Sui, X. Ma, W. Liang, X. Zhang, and M. O.\nPun, “Change diffusion: Change detection map generation\nbased on difference-feature guided DDPM,” arXiv preprint\narXiv:2306.03424, 2023.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n16\n[33] M. Zhang, G. Xu, K. Chen, M. Yan, and X. Sun, “Triplet-\nbased semantic relation learning for aerial remote sensing\nimage change detection,” IEEE Geoscience and Remote Sensing\nLetters, vol. 16, no. 2, pp. 266–270, 2018.\n[34] M. Wang, K. Tan, X. Jia, X. Wang, and Y . Chen, “A deep\nsiamese network with hybrid convolutional feature extraction\nmodule for change detection based on multi-sensor remote\nsensing images,” Remote Sensing, vol. 12, no. 2, p. 205, 2020.\n[35] R. C. Daudt, B. Le Saux, and A. Boulch, “Fully convolutional\nsiamese networks for change detection,” in 2018 25th IEEE\nInternational Conference on Image Processing (ICIP) . IEEE,\n2018, pp. 4063–4067.\n[36] J. Liu, M. Gong, K. Qin, and P. Zhang, “A deep convolutional\ncoupling network for change detection based on heterogeneous\noptical and radar images,” IEEE transactions on neural net-\nworks and learning systems , vol. 29, no. 3, pp. 545–559, 2016.\n[37] L. Mou, L. Bruzzone, and X. X. Zhu, “Learning spectral-\nspatial-temporal features via a recurrent convolutional neural\nnetwork for change detection in multispectral imagery,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 57, no. 2,\npp. 924–935, 2018.\n[38] T. Liu, M. Gong, D. Lu, Q. Zhang, H. Zheng, F. Jiang,\nand M. Zhang, “Building change detection for vhr remote\nsensing images via local–global pyramid network and cross-task\ntransfer learning strategy,” IEEE Transactions on Geoscience\nand Remote Sensing , vol. 60, pp. 1–17, 2021.\n[39] C. Zhang, P. Yue, D. Tapete, L. Jiang, B. Shangguan, L. Huang,\nand G. Liu, “A deeply supervised image fusion network for\nchange detection in high resolution bi-temporal remote sensing\nimages,” ISPRS Journal of Photogrammetry and Remote Sens-\ning, vol. 166, pp. 183–200, 2020.\n[40] K. Simonyan and A. Zisserman, “Very deep convolutional\nnetworks for large-scale image recognition,” arXiv preprint\narXiv:1409.1556, 2014.\n[41] J. Chen, Z. Yuan, J. Peng, L. Chen, H. Huang, J. Zhu, Y . Liu,\nand H. Li, “Dasnet: Dual attentive fully convolutional siamese\nnetworks for change detection in high-resolution satellite im-\nages,” IEEE Journal of Selected Topics in Applied Earth Ob-\nservations and Remote Sensing , vol. 14, pp. 1194–1206, 2020.\n[42] Z. Zheng, Y . Zhong, S. Tian, A. Ma, and L. Zhang, “Change-\nMask: Deep multi-task encoder-transformer-decoder architec-\nture for semantic change detection,” ISPRS Journal of pho-\ntogrammetry and remote sensing , vol. 183, pp. 228–239, Jan-\nuary 2022.\n[43] H. Chen, Z. Qi, and Z. Shi, “Remote sensing image change\ndetection with transformers,” IEEE Transactions on Geoscience\nand Remote Sensing , vol. 60, pp. 1–14, 2021.\n[44] W. G. C. Bandara and V . M. Patel, “A transformer-based\nsiamese network for change detection,” in IGARSS 2022-2022\nIEEE International Geoscience and Remote Sensing Sympo-\nsium. IEEE, 2022, pp. 207–210.\n[45] X. Xu, J. Li, and Z. Chen, “Tcianet: Transformer-based context\ninformation aggregation network for remote sensing image\nchange detection,” IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing , vol. 16, pp. 1951–\n1971, 2023.\n[46] A. Mohammadian and F. Ghaderi, “Siamixformer: A fully-\ntransformer siamese network with temporal fusion for accurate\nbuilding detection and change detection in bi-temporal remote\nsensing images,” International Journal of Remote Sensing ,\nvol. 44, no. 12, pp. 3660–3678, 2023.\n[47] V . Marsocci, V . Coletta, R. Ravanelli, S. Scardapane, and\nM. Crespi, “Inferring 3d change detection from bitemporal\noptical images,” ISPRS Journal of Photogrammetry and Remote\nSensing, vol. 196, pp. 325–339, 2023.\n[48] J. Guo, K. Han, H. Wu, Y . Tang, X. Chen, Y . Wang, and\nC. Xu, “CMT: Convolutional neural networks meet vision\ntransformers,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2022, pp. 12 175–\n12 185.\n[49] M. Gong, X. Niu, P. Zhang, and Z. Li, “Generative adversarial\nnetworks for change detection in multispectral imagery,” IEEE\nGeoscience and Remote Sensing Letters , vol. 14, no. 12, pp.\n2310–2314, 2017.\n[50] X. Li, Z. Du, Y . Huang, and Z. Tan, “A deep translation (gan)\nbased change detection network for optical and sar remote\nsensing images,” ISPRS Journal of Photogrammetry and Remote\nSensing, vol. 179, pp. 14–34, 2021.\n[51] M. Gong, Y . Yang, T. Zhan, X. Niu, and S. Li, “A generative\ndiscriminatory classified network for change detection in mul-\ntispectral imagery,” IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing , vol. 12, no. 1, pp.\n321–333, 2019.\n[52] W. G. C. Bandara, N. G. Nair, and V . M. Patel, “DDPM-\nCD: Remote sensing change detection using denoising diffusion\nprobabilistic models,” arXiv preprint arXiv:2206.11892 , 2022.\n[53] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic\nmodels,” Advances in neural information processing systems ,\nvol. 33, pp. 6840–6851, 2020.\n[54] W. G. C. Bandara and V . M. Patel, “Revisiting consistency\nregularization for semi-supervised change detection in remote\nsensing images,” arXiv preprint arXiv:2204.08454 , 2022.\n[55] H. Chen and Z. Shi, “A spatial-temporal attention-based method\nand a new dataset for remote sensing image change detection,”\nRemote Sensing, vol. 12, no. 10, p. 1662, 2020.\n[56] M. Lebedev, Y . V . Vizilter, O. Vygolov, V . Knyaz, and A. Y .\nRubis, “Change detection in remote sensing images using\nconditional adversarial networks.” International Archives of\nthe Photogrammetry, Remote Sensing & Spatial Information\nSciences, vol. 42, no. 2, 2018.\n[57] K. Li, Z. Li, and S. Fang, “Siamese nestedunet networks for\nchange detection of high resolution satellite image,” in Pro-\nceedings of the 2020 1st International Conference on Control,\nRobotics and Intelligent System , 2020, pp. 42–48.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3373201\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.4546736180782318
    },
    {
      "name": "Change detection",
      "score": 0.451323926448822
    },
    {
      "name": "Computer science",
      "score": 0.43581217527389526
    },
    {
      "name": "Generative model",
      "score": 0.43102651834487915
    },
    {
      "name": "Generative grammar",
      "score": 0.3497233986854553
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3338255286216736
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2759353518486023
    },
    {
      "name": "Electrical engineering",
      "score": 0.0682988166809082
    },
    {
      "name": "Engineering",
      "score": 0.0657537579536438
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I149594827",
      "name": "Xidian University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I170215575",
      "name": "National University of Defense Technology",
      "country": "CN"
    }
  ],
  "cited_by": 16
}