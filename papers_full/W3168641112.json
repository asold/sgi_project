{
  "title": "Specializing Multilingual Language Models: An Empirical Study",
  "url": "https://openalex.org/W3168641112",
  "year": 2021,
  "authors": [
    {
      "id": null,
      "name": "Chau, Ethan C.",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4221392677",
      "name": "Smith, Noah A.",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W3166790124",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2971863715",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3169244955",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W4230579319",
    "https://openalex.org/W2995118574",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W3093721400",
    "https://openalex.org/W3214161538",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3098466758",
    "https://openalex.org/W3173954987",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2970529259",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3105190698",
    "https://openalex.org/W3112302586",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3103727211"
  ],
  "abstract": "Pretrained multilingual language models have become a common tool in transferring NLP capabilities to low-resource languages, often with adaptations. In this work, we study the performance, extensibility, and interaction of two such adaptations: vocabulary augmentation and script transliteration. Our evaluations on part-of-speech tagging, universal dependency parsing, and named entity recognition in nine diverse low-resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low-resource settings.",
  "full_text": "Specializing Multilingual Language Models: An Empirical Study\nEthan C. Chau† Noah A. Smith†⋆\n†Paul G. Allen School of Computer Science & Engineering, University of Washington\n⋆Allen Institute for Artificial Intelligence\n{echau18,nasmith}@cs.washington.edu\nAbstract\nPretrained multilingual language models have\nbecome a common tool in transferring NLP\ncapabilities to low-resource languages, often\nwith adaptations. In this work, we study the\nperformance, extensibility, and interaction of\ntwo such adaptations: vocabulary augmentation\nand script transliteration. Our evaluations on\npart-of-speech tagging, universal dependency\nparsing, and named entity recognition in nine\ndiverse low-resource languages uphold the vi-\nability of these approaches while raising new\nquestions around how to optimally adapt multi-\nlingual models to low-resource settings.\n1 Introduction\nResearch in natural language processing is increas-\ningly carried out in languages beyond English. This\nincludes high-resource languages with abundant\ndata, as well as low-resource languages, for which\nlabeled (and unlabeled) data is scarce. In fact,\nmany of the world’s languages fall into the latter\ncategory, even some with a high number of speak-\ners. This presents unique challenges compared\nto high-resource languages: effectively modeling\nlow-resource languages involves both accurately\ntokenizing text in such languages and maximally\nleveraging the limited available data.\nOne common approach to low-resource NLP is\nthe multilingual paradigm, in which methods that\nhave shown success in English are applied to the\nunion of many languages’ data, 1 enabling trans-\nfer between languages. For instance, multilingual\ncontextual word representations (CWRs) from lan-\nguage models (Devlin et al., 2019; Huang et al.,\n2019; Lample and Conneau, 2019, inter alia) are\nconventionally “pretrained” on large multilingual\n1Within the multilingual paradigm, a distinction is some-\ntimes made between massively multilingualmethods, which\nconsider tens or hundreds of languages; and polyglot meth-\nods, which use only a handful. In this paper, all mentions of\n“multilingual” refer to the former.\ncorpora before being “finetuned” directly on super-\nvised tasks; this pretraining-finetuning approach is\nderived from analogous monolingual models (De-\nvlin et al., 2019; Liu et al., 2019; Peters et al., 2018).\nHowever, considering the diversity of the world’s\nlanguages and the great data imbalance among\nthem, it is natural to question whether the current\nmultilingual paradigm can be improved upon for\nlow-resource languages.\nIndeed, past work has demonstrated that it can.\nFor instance, Wu and Dredze (2020) find that multi-\nlingual models often lag behind non-contextualized\nbaselines for the lowest-resource languages in their\ntraining data, drawing into question their utility in\nsuch settings. Conneau et al. (2020a) posit that\nthis phenomenon is a result of limited model capac-\nity, which proves to be a bottleneck for sufficient\ntransfer to low-resource languages. In fact, with\nmultilingual models only being pretrained on a lim-\nited set of languages, most of the world’s languages\nare unseen by the model. For such languages, the\nperformance of such models is even worse (Chau\net al., 2020), due in part to the diversity of scripts\nacross the world’s languages (Muller et al., 2021;\nPfeiffer et al., 2021b; Rust et al., 2021) as compared\nto the models’ Latin-centricity (Ács, 2019).\nNonetheless, there have been multiple attempts\nto remedy this discrepancy by specializing2 a mul-\ntilingual model to a given target low-resource lan-\nguage, from which we take inspiration. Among\nthem, Chau et al. (2020) augment the model’s vo-\ncabulary to more effectively tokenize text, then\npretrain on a small amount of data in the target\nlanguage; they report significant performance im-\nprovements on a small set of low-resource lan-\nguages. In a similar vein, Muller et al. (2021)\npropose to transliterate text in the target language\n2We use specialization to denote preparing a model for use\non a specific target language, to the exclusion of others. This\nis a subset of adaptation, which includes all techniques that\nadjust a model for use on target languages, regardless of their\nresulting universality.\nto Latin script to be better tokenized by the ex-\nisting model, followed by additional pretraining;\nthey observe mixed results and note that translit-\neration quality may be a confounding factor. We\nhypothesize that these two methods can serve as\nthe basis for improvements in modeling a broad set\nof low-resource languages.\nIn this work, we study the effectiveness, exten-\nsibility, and interaction of these two approaches to\nspecialization: the vocabulary augmentation tech-\nnique of Chau et al. (2020) and the script translit-\neration method of Muller et al. (2021). We verify\nthe performance of vocabulary augmentation on\nthree tasks in a diverse set of nine low-resource\nlanguages across three different scripts, especially\non non-Latin scripts (§2) and find that these gains\nare associated with improved vocabulary coverage\nof the target language. We further observe a nega-\ntive interaction between vocabulary augmentation\nand transliteration in light of a broader framework\nfor specializing multilingual models, while noting\nthat vocabulary augmentation offers an appealing\nbalance of performance and cost (§3). Overall, our\nresults highlight several possible directions for fu-\nture study in the low-resource setting. Our code,\ndata, and hyperparameters are publicly available.3\n2 Revisiting Vocabulary Augmentation\nWe begin by revisiting the V ocabulary Augmenta-\ntion method of Chau et al. (2020), which we recast\nmore generally in light of recent work (§2.1). We\nevaluate their claims on three different tasks, using\na diverse set of languages in multiple scripts (§2.2),\nand find that the results hold to an even more pro-\nnounced degree in unseen low-resource languages\nwith non-Latin scripts (§2.3).\n2.1 Method Overview\nFollowing Chau et al. (2020), we consider how\nto apply the pretrained multilingual BERT model\n(MBERT; Devlin et al., 2019) to a target low-\nresource language, for which both labeled and un-\nlabeled data is scarce. This model has produced\nstrong CWRs for many languages (Kondratyuk and\nStraka, 2019, inter alia) and has been the starting\nmodel for many studies on low-resource languages\n(Muller et al., 2021; Pfeiffer et al., 2020; Wang\net al., 2020). MBERT covers the languages with the\n104 largest Wikipedias, and it uses this data to con-\n3https://github.com/ethch18/\nspecializing-multilingual\nstruct a wordpiece vocabulary (Wu et al., 2016) and\ntrain its transformer-based architecture (Vaswani\net al., 2017). Although low-resource languages are\nslightly oversampled, high-resource languages still\ndominate both the final pretraining data and the\nvocabulary (Ács, 2019; Devlin et al., 2019).\nChau et al. (2020) note that target low-resource\nlanguages fall into three categories with respect\nto MBERT’s pretraining data: the lowest-resource\nlanguages in the data (Type 1), completely unseen\nlow-resource languages (Type 2), and low-resouce\nlanguages with more representation (Type 0).4 Due\nto their poor representation in the vocabulary, Type\n1 and Type 2 languages achieve suboptimal tok-\nenization and higher rates of the “unknown” word-\npiece5 when using MBERT out of the box. This\nhinders the model’s ability to capture meaningful\npatterns in the data, resulting in reduced data effi-\nciency and degraded performance.\nWe note that this challenge is exacerbated when\nmodeling languages written in non-Latin scripts.\nMBERT’s vocabulary is heavily Latin-centric (Ács,\n2019; Muller et al., 2021), resulting in a signif-\nicantly larger portion of non-Latin scripts being\nrepresented with “unknown” tokens (Pfeiffer et al.,\n2021b) and further limiting the model’s ability to\ngeneralize. In effect, MBERT’s low initial perfor-\nmance on such languages can be attributed to its\ninability to represent the script itself.\nTo alleviate the problem of poor tokenization,\nChau et al. (2020) propose to specialize MBERT\nusing V ocabulary Augmentation (VA). Given un-\nlabeled data in the target language, they train a\nnew wordpiece vocabulary on the data, then se-\nlect the 99 most common wordpieces in the new\nvocabulary that replace “unknown” tokens under\nthe original vocabulary. They then add these 99\nwordpieces to the original vocabulary and continue\npretraining MBERT on the unlabeled data for addi-\ntional steps. They further describe a tiered variant\n(TVA), in which a larger learning rate is used for\nthe embeddings of these 99 new wordpieces. VA\nyields strong gains over unadapted multilingual\nlanguage models on dependency parsing in four\nlow-resource languages with Latin scripts. How-\n4Muller et al. (2021) further subdivide Type 2 into Easy,\nMedium, and Hard languages, based on the performance of\nMBERT after being exposed to these languages. However,\nthis categorization cannot be determined a priorifor a given\nlanguage.\n5The “unknown” wordpiece is inserted when the word-\npiece algorithm is unable to segment a word-level token with\nthe current vocabulary.\never, no evaluation has been performed on other\ntasks or on languages with non-Latin scripts, which\nraises our first research question:\nRQ1: Do the conclusions of Chau et al. (2020)\nhold for other tasks and for languages with non-\nLatin scripts?\nWe can view VA and TVA as an instantation of a\nmore general framework of vocabulary augmenta-\ntion, shared by other approaches to using MBERT\nin low-resource settings. Given a new vocabulary\nV , number of wordpieces n, and learning rate mul-\ntiplier a, the n most common wordpieces in V are\nadded to the original vocabulary. Additional pre-\ntraining is then performed, with the embeddings\nof the n wordpieces taking on a learning rate a\ntimes greater than the overall learning rate. For VA,\nwe set n = 99 and a = 1, while we treat a as a\nhyperparameter for TVA. The related E-MBERT\nmethod of Wang et al. (2020) sets n = |V | and\na = 1. Investigating various other instantiations of\nthis framework is an interesting research direction,\nthough it is out of the scope of this work.\n2.2 Experiments\nWe expand on the dependency parsing evaluations\nof Chau et al. (2020) by additionally considering\nnamed entity recognition and part-of-speech tag-\nging. We follow Kondratyuk and Straka (2019) and\ncompute the CWR for each token as a weighted\nsum of the activations at each MBERT layer. For\ndependency parsing, we follow the setup of Chau\net al. (2020) and Muller et al. (2021) and use the\nCWRs as input to the graph-based dependency\nparser of Dozat and Manning (2017). For named\nentity recognition, the CWRs are used as input to\na CRF layer, while part-of-speech tagging uses a\nlinear projection atop the representations. In all\ncases, the underlying CWRs are finetuned during\ndownstream task training, and we do not add an\nadditional encoder layer above the transformer out-\nputs. We train models on five different random\nseeds and report average scores and standard er-\nrors.\n2.2.1 Languages and Datasets\nWe select a set of nine typologically diverse low-\nresource languages for evaluation, including three\nof the original four used by Chau et al. (2020).\nThese languages use three different scripts and are\nchosen based on the availability of labeled datasets\nand their exemplification of the three language\ntypes identified by Chau et al. (2020). Of the lan-\nguages seen by MBERT, all selected Type 0 lan-\nguages are within the 45 largest Wikipedias, while\nthe remaining Type 1 languages are within the top\n100. The Type 2 languages, which are excluded\nfrom MBERT, are all outside of the top 150.6 Addi-\ntional information about the evaluation languages\nis given in Tab. 1.\nUnlabeled Datasets Following Chau et al.\n(2020), we use articles from Wikipedia as unla-\nbeled data for additional pretraining in order to\nreflect the original pretraining data. We downsam-\nple full articles from the largest Wikipedias to be on\nthe order of millions of tokens in order to simulate\na low-resource unlabeled setting, and we remove\nsentences that appear in the labeled validation or\ntest sets.\nLabeled Datasets For dependency parsing and\npart-of-speech tagging, we use datasets and\ntrain/test splits from Universal Dependencies\n(Nivre et al., 2020), version 2.5 (Zeman et al.,\n2019). POS tagging uses language-specific part-\nof-speech tags (XPOS) to evaluate understanding\nof language-specific syntactic phenomena. The\nBelarusian treebank lacks XPOS tags for certain\nexamples, so we use universal part-of-speech tags\ninstead. Dependency parsers are trained with gold\nword segmentation and no part-of-speech features.\nExperiments with named entity recognition use the\nWikiAnn dataset (Pan et al., 2017), following past\nwork (Muller et al., 2021; Pfeiffer et al., 2020; Wu\nand Dredze, 2020). Specifically, we use the bal-\nanced train/test splits of (Rahimi et al., 2019). We\nnote that UD datasets were unavailable for Meadow\nMari, and partitioned WikiAnn datasets were miss-\ning for Wolof.\n2.2.2 Baselines\nTo measure the effectiveness of VA, we benchmark\nit against unadapted MBERT, as well as directly\npretraining MBERT on the unlabeled data with-\nout modifying the vocabulary (Chau et al., 2020;\nMuller et al., 2021; Pfeiffer et al., 2020). Follow-\ning Chau et al. (2020), we refer to the latter ap-\nproach as language-adaptive pretraining(LAPT ).\nWe also evaluate two monolingual baselines that\nare trained on our unlabeled data: fastText embed-\ndings ( FAST T; Bojanowski et al., 2017), which\nrepresent a static word vector approach; and a\nBERT model trained from scratch ( BERT ). For\n6Based on https://meta.wikimedia.org/\nwiki/List_of_Wikipedias.\nLanguage Type Script Family # Sentences # Tokens Downsample % # WP/Token\nBulgarian (BG) 0 Cyrillic Slavic 357k 5.6M 10% 1.81\nBelarusian (BE) 0 Cyrillic Slavic 187k 2.7M 10% 2.25\nMeadow Mari (MHR) 2 Cyrillic Uralic 52k 512k – 2.37\nVietnamese (VI) 0 Latin Viet-Muong 338k 6.9M 5% 1.17\nIrish (GA) 1 Latin Celtic 274k 5.8M – 1.83\nMaltese (MT) 2 Latin Semitic 75k 1.4M – 2.39\nWolof (WO) 2 Latin Niger-Congo 15k 396k – 1.78\nUrdu (UR) 0 Perso-Arabic Indic 201k 3.6M 20% 1.58\nUyghur (UG) 2 Perso-Arabic Turkic 136k 2.3M – 2.54\nTable 1: Language overview and unlabeled dataset statistics: number of sentences, number of tokens, and average\nwordpieces per token under the original MBERT vocabulary.\nBERT , we follow Muller et al. (2021) and train a\nsix-layer RoBERTa model (Liu et al., 2019) with a\nlanguage-specific SentencePiece tokenizer (Kudo\nand Richardson, 2018). For a fair comparison to\nVA, we use the same task-specific architectures and\nmodify only the input representations.\n2.2.3 Implementation Details\nTo pretrain LAPT and VA models, we use the code\nof Chau et al. (2020), who modify the pretraining\ncode of Devlin et al. (2019) to only use the masked\nlanguage modeling (MLM) loss. To generate VA\nvocabularies, we train a new vocabulary of size\n5000 and select the 99 wordpieces that replace the\nmost unknown tokens. We train with a fixed linear\nwarmup of 1000 steps. To pretrain BERT mod-\nels, we use the HuggingFace Transformers library\n(Wolf et al., 2020). Following Muller et al. (2021),\nwe train a half-sized RoBERTa model with six lay-\ners and 12 attention heads. We use a byte-pair\nvocabulary of size 52000 and a linear warmup of 1\nepoch. For LAPT , VA, and BERT , we train for up to\n20 epochs total, selecting the highest-performing\nepoch based on validation masked language mod-\neling loss. FAST T models are trained with the skip-\ngram model for five epochs, with the default hyper-\nparameters of Bojanowski et al. (2017).\nTraining of downstream parsers and taggers fol-\nlows Chau et al. (2020) and Kondratyuk and Straka\n(2019), with an inverse square-root learning rate\ndecay and linear warmup, and layer-wise gradual\nunfreezing and discriminative finetuning. Models\nare trained with AllenNLP, version 2.1.0 (Gardner\net al., 2018), for up to 200 epochs with early stop-\nping based on validation performance. We choose\nbatch sizes to be the maximum that allows for suc-\ncessful training on one GPU.\n2.3 Results\nTab. 2 presents performance of the different input\nrepresentations on POS tagging, dependency pars-\ning, and named entity recognition. VA achieves\nstrong results across all languages and tasks and\nis the top performer in the majority of them, sug-\ngesting that augmenting the vocabulary addresses\nMBERT’s limited vocabulary coverage of the tar-\nget language and is beneficial during continued\npretraining.\nThe relative gains that VA provides appear to cor-\nrelate not only with language type, as in the find-\nings of Chau et al. (2020), but also with each lan-\nguage’s script. For instance, in Vietnamese, which\nis a Type 0 Latin script language, the improvements\nfrom VA are marginal at best, reflecting the Latin-\ndominated pretraining data of MBERT. Irish, the\nType 1 Latin script language, is only slightly more\nreceptive. However, Type 0 languages in Cyrillic\nand Arabic scripts, which are less represented in\nMBERT’s pretraining data, are more receptive to\nVA, with VA even outperforming all other methods\nfor Urdu. This trend is amplified in the Type 2\nlanguages, as the improvements for Maltese and\nWolof are small but significant. However, they are\ndwarfed in magnitude by those of Uyghur, where\nVA achieves up to a 57% relative error reduction\nover LAPT .\nThis result corroborates the findings of both\nChau et al. (2020) and Muller et al. (2021) and\nanswers RQ1. Prior to specialization, MBERT is\nespecially poorly equipped to handle unseen low-\nresource languages and languages in non-Latin\nscripts due to its inability to model the script itself.\nIn such cases, specialization via VA is beneficial,\nproviding MBERT with explicit signal about the\ntarget language and script while maintaining its\nlanguage-agnostic insights. On the other hand, this\nalso motivates additional investigation into reme-\nRep. BE* (0) BG(0) GA(1) MT(2) UG(2) UR(0) VI(0) WO(2) Avg.\nFASTT 68.84 ±7.16 88.86±0.37 86.87±2.55 89.68±2.15 89.45±1.37 90.81±0.31 81.84±1.15 87.48±0.55 85.48\nBERT 91.00±0.30 94.48±0.10 90.36±0.20 92.61±0.10 90.87±0.13 89.88±0.13 84.73±0.13 87.71±0.31 90.20\nMBERT 94.57±0.45 96.98±0.08 91.91±0.25 94.01±0.17 78.07±0.22 91.77±0.18 88.97±0.10 93.04±0.20 91.16\nLAPT 95.74±0.44 97.15±0.04 93.28±0.19 95.76±0.09 79.88±0.27 92.18±0.16 89.64±0.20 94.58±0.13 92.28\nVA 95.28±0.51 97.20±0.06 93.33±0.16 96.33±0.09 91.49±0.13 92.24±0.16 89.49±0.22 94.48±0.20 93.73\n(a) POS tagging (accuracy). *Belarusian uses universal POS tags.\nRep. BE(0) BG(0) GA(1) MT(2) UG(2) UR(0) VI(0) WO(2) Avg.\nFASTT 35.81 ±2.24 84.03±0.41 65.58±1.21 68.45±1.40 54.52±1.02 79.33±0.25 54.91±0.79 70.39±1.39 64.13\nBERT 45.77±1.35 84.61±0.27 64.02±0.49 65.92±0.45 60.34±0.27 78.07±0.22 54.70±0.27 60.12±0.39 64.19\nMBERT 71.83±0.90 91.62±0.23 71.68±0.62 76.63±0.35 47.70±0.44 81.45±0.26 64.58±0.42 76.24±0.83 72.72\nLAPT 72.77±1.12 92.08±0.31 74.79±0.12 81.53±0.37 50.67±0.34 81.78±0.44 66.15±0.41 80.34±0.14 75.01\nVA 73.22±1.23 91.90±0.20 74.35±0.22 82.00±0.31 67.55±0.17 81.88±0.25 65.64±0.12 80.22±0.41 77.09\n(b) UD parsing (LAS).\nRep. BE(0) BG(0) GA(1) MT(2) UG(2) UR(0) VI(0) MHR(2) Avg.\nFASTT 84.26 ±0.86 87.98±0.76 67.21±4.30 33.53±17.89 – 92.85±2.04 85.57±1.98 35.28±13.81 60.84\nBERT 88.08±0.62 90.31±0.20 76.58±0.98 54.64±3.51 61.54±3.70 94.04±0.55 88.08±0.15 54.17±2.88 75.93\nMBERT 91.13±0.07 92.56±0.09 82.82±0.57 61.86±2.60 50.76±1.86 94.60±0.34 92.13±0.27 61.85±3.25 78.46\nLAPT 91.61±0.74 92.96±0.13 84.13±0.78 81.53±2.33 56.76±4.91 95.17±0.29 92.41±0.15 59.17±5.15 81.72\nVA 91.38±0.56 92.70±0.11 84.82±1.00 80.00±2.77 68.93±3.30 95.43±0.22 92.43±0.16 64.23±3.07 83.74\n(c) NER (macro F1). – indicates that a model did not converge.\nTable 2: Results on POS tagging, UD parsing, and NER, with standard deviations from five random initializations.\nBolded results are the maximum for each language, and scores in gray are not significantly worse than the best\nmodel (1-sided paired t-test, p = 0.05 with Bonferonni correction).\ndies for the script imbalance at a larger scale, e.g.,\nmore diverse pretraining data.\n2.4 Analysis\nWe perform further analysis to investigate VA’s pat-\nterns of success. Concretely, we hypothesize that\nVA significantly improves the tokenizer’s coverage\nof target languages where it is most successful. In-\nspired by Ács (2019), Chau et al. (2020), and Rust\net al. (2021), we quantify tokenizer coverage using\nthe percentage of tokens in the raw text that yield\nunknown wordpieces when tokenized with a given\nvocabulary (“UNK token percentage”). These are\ntokens whose representations contain at least par-\ntial ambiguity due to the inclusion of the unknown\nwordpiece.\nTab. 3 presents the UNK token percentage for\neach dataset using the MBERT vocabulary, aver-\naged over each script and language type. This vo-\ncabulary is used in LAPT and represents the base-\nline level of vocabulary coverage. We also include\nthe change in the UNK token percentage between\nthe MBERT and VA vocabularies, which quanti-\nfies the coverage improvement. Both sets of val-\nues are juxtaposed against the average change in\ntask-specific performance from LAPT to VA, repre-\nsenting the effect of augmenting the vocabulary on\ntask-specific performance.\nWe observe that off-the-shelf MBERT already at-\ntains relatively high vocabulary coverage for Type\n0 and 1 languages, as well as languages written\nin Latin and Cyrillic scripts. On the other hand,\nup to one-fifth of the tokens in Arabic languages\nand one-sixth of those in Type 2 languages yield\nan unknown wordpiece. For these languages, there\nis great room for increasing tokenizer coverage,\nand VA indeed addresses this more tangible need.\nThis aligns with the task-specific performance im-\nprovements for each group and helps to explain our\nresults in §2.3.\nIt is notable that VA does not always eliminate\nthe issue of unknown wordpieces, even in lan-\nguages for which MBERT attains high vocabulary\ncoverage. This suggests that the remaining un-\nknown wordpieces in these languages are more\nsparsely distributed (i.e., they represent low fre-\nquency sequences), while the unknown wordpieces\nin languages with lower vocabulary coverage repre-\nsent sequences that occur more commonly. As a re-\nsult, augmenting the vocabulary in such languages\nquickly improves coverage while associating these\ncommonly occurring sequences with each other,\nwhich benefits the overall tokenization quality.\nWe further explore the association between the\nimprovements in vocabulary coverage and task-\nspecific performance in Fig. 1. Although we do not\nfind that languages from the same types or scripts\nform clear clusters, we nonetheless observe a loose\nLang. Group Avg. UNK Token % (MBERT) Avg. UNK Token % (∆) Avg. Task Performance (∆)\n(# of Langs.)Unlabeled UD WikiAnn Unlabeled UD WikiAnn POS UD NER\nAll (9) 5.9 % (–) 5.2 % (–) 6.2 % (–) –5.3 % (–) 4.7 % (–) –5.8 % (–) +1.45 (–) +2.08 (–) +2.02 (–)\nType 0 (4) 1.0 % ( ↓) 0.3 % ( ↓) 1.2 % ( ↓) –0.9 % (↑) –0.3 % (↑) –1.2 % (↑) –0.13 (↓) –0.04 (↓) –0.05 (↓)\nType 1 (1) 0.3 % ( ↓) 0.0 % ( ↓) 0.4 % ( ↓) –0.3 % (↑) –0.00 % (↑) –0.4 % (↑) +0.05 (↓) –0.44 (↓) +0.69 (↓)\nType 2 (4) 12.3 % (↑) 13.5 % (↑) 14.8 % (↑) –10.8 % (↓) –12.1 % (↓) –13.7 % (↓) +4.03 (↑) +5.74 (↑) +5.23 (↑)\nLatin (4) 1.2 % ( ↓) 0.6 % ( ↓) 2.4 % ( ↓) –1.2 % (↑) –0.6 % (↑) –2.3 % (↑) +0.09 (↓) –0.15 (↓) –0.27 (↓)\nCyrillic (3) 3.6 % (↓) 0.6 % ( ↓) 2.8 % ( ↓) –3.6 % (↑) –0.6 % (↑) –2.7 % (↑) –0.21 (↓) +0.14 (↓) +1.52 (↓)\nArabic (2) 19.0 % (↑) 19.2 % (↑) 16.9 % (↑) –16.1 % (↓) –17.0 % (↓) –15.5 % (↓) +5.84 (↑) +8.49 (↑) +6.22 (↑)\nTable 3: Average UNK token percentage under the MBERT vocabulary (left); change in UNK token percentage\nfrom MBERT to VA vocabularies (center); and average task performance change from LAPT to VA (right). Averages\nare computed overall and within each script and language type, with comparisons to the overall average; all UNK\ntoken percentages are computed on the respective training sets for illustration. Note that Uyghur accounts for a\nlarge portion of the behavior of the Type 2/Arabic rows.\ncorrelation between the two factors in question and\nsee that VA delivers greater performance gains on\nType 2 and Arabic-script languages compared to\ntheir Type 0/1 and Latin-script counterparts, respec-\ntively. To quantify the strength of this association,\nwe also compute the language-level Spearman cor-\nrelation between the change in UNK token percent-\nage on the unlabeled dataset7 from the MBERT to\nVA vocabulary and the task-specific performance\nimprovements from LAPT to VA. The resulting\nρ-values – 0.29 for NER, 0.56 for POS tagging,\nand 0.81 for UD parsing – suggest that this set of\nfactors is meaningful for some tasks, though ad-\nditional and more fine-grained analysis in future\nwork should give a more complete explanation.\n3 Mix-in Specialization: VA and\nTransliteration\nWe now expand on the observation made in §2.3\nregarding the difficulties that MBERT encounters\nwhen faced with unseen low-resource languages in\nnon-Latin scripts because of its inability to model\nthe script. Having observed that VA is benefi-\ncial in such cases, we now investigate the inter-\naction between this method and another special-\nization approach that targets this problem. Specif-\nically, we consider the transliteration methods of\nMuller et al. (2021), in which unseen low-resource\nlanguages in non-Latin scripts are transliterated\ninto the Latin script, often using transliteration\nschemes inspired by the Latin orthographies of lan-\nguages related to the target language. They hypoth-\nesize that the increased similarity in the languages’\nwriting systems, combined with MBERT’s overall\nLatin-centricity, provides increased opportunity for\ncrosslingual transfer.\n7We benchmark against the unlabeled dataset instead of\ntask-specific ones for comparability.\nWe can view transliteration as a inverted form of\nvocabulary augmentation: instead of adapting the\nmodel to the needs of the data, the data is adjusted\nto meet the assumptions of the model. Furthermore,\nthe transliteration step is performed prior to pre-\ntraining MBERT on additional unlabeled data in the\ntarget language, the same stage at which VA is per-\nformed. In both cases, the ultimate goal is identical:\nimproving tokenization and more effectively using\navailable data. We can thus view transliteration and\nVA as two instantiations of a more general mix-in\nparadigm for model specialization, whereby var-\nious transformations (mix-ins) are applied to the\ndata and/or model prior to performing additional\npretraining. These mix-ins target different compo-\nnents of the experimental pipeline, which naturally\nraises our second research question:\nRQ2: How do the VA and transliteration mix-ins\nfor MBERT compare and interact?\n3.1 Method and Experiments\nTo test this research question, we apply translit-\neration and VA in succession and evaluate their\ncompatibility. Given unlabeled data in the target\nlanguage, we first transliterate it into Latin script,\nwhich decreases but does not fully eliminate the\nissue of unseen wordpieces. We then perform VA,\ngenerating the vocabulary for augmentation based\non the transliterated data.\nWe evaluate on Meadow Mari and Uyghur,\nwhich are Type 2 languages where transliteration\nwas successfully applied by Muller et al. (2021).\nTo transliterate the data, we use the same methods\nas Muller et al. (2021): Meadow Mari uses the\ntransliterate8 package, while Uyghur uses\n8https://pypi.org/project/\ntransliterate\n(a) POS tagging.\n (b) UD parsing.\n (c) NER.\nFigure 1: Relationship between the change in UNK token percentage on task data and the change in task performance,\nfrom (MBERT/LAPT to VA), with a 1-degree line of best fit. All vocabulary values are computed on the respective\ntraining sets.\na linguistically-motivated transliteration scheme9\naimed at associating Uyghur with Turkish. We use\nthe same training scheme, model architectures, and\nbaselines as in §2.2, the only difference being the\nuse of transliterated data. This includes directly\npretraining on the unlabeled data ( LAPT ), which\nis comparable to the highest-performing translit-\neration models of Muller et al. (2021). Although\nour initial investigation of VA in §2 also included\nnon-Type 2 languages of other scripts, we omit\nthem from our investigation based on the finding\nof Muller et al. (2021) that transliterating higher-\nresource languages into Latin scripts is not benefi-\ncial.\n3.2 Results\nTab. 4 gives the results of our transliteration mix-in\nexperiments. For the MBERT-based models, both\nVA and transliteration provide strong improvements\nover their respective baselines. Specifically, the im-\nprovements from LAPT to VA and LAPT to LAPT\nwith transliteration are most pronounced. This ver-\nifies the independent results of Chau et al. (2020)\nand Muller et al. (2021) and suggests that in the\nnon-Latin low-resource setting, unadapted addi-\ntional pretraining is insufficient, but that the mix-in\nstage between initial and additional pretraining is\namenable to performance-improving modifications.\nUnsurprisingly, transliteration provides no consis-\ntent improvement to the monolingual baselines,\nsince the noisy transliteration process removes in-\nformation without improving crosslingual align-\nment.\nHowever, VA and transliteration appear to inter-\nact negatively. Although VA with transliteration im-\n9https://github.com/benjamin-mlr/\nmbert-unseen-languages\nproves over plain VA for Uyghur POS tagging and\ndependency parsing, it still slightly underperforms\nLAPT with transliteration for the latter. For the\ntwo NER experiments, VA with transliteration lags\nboth methods independently. One possible expla-\nnation is that transliteration into Latin script serves\nas implicit vocabulary augmentation, with embed-\ndings that have already been updated during the\ninitial pretraining stage; as a result, the two sources\nof augmentation conflict. Alternatively, since the\ntransliteration process merges certain characters\nthat are distinct in the original script, VA may aug-\nment the vocabulary with misleading character clus-\nters. Either way, additional vocabulary augmenta-\ntion is generally not as useful when combined with\ntransliteration, answering RQ2.\nNonetheless, additional investigation into the op-\ntimal amount of vocabulary augmentation might\nyield a configuration that is consistently comple-\nmentary to transliteration and is an interesting di-\nrection for future work. Furthermore, designing\nlinguistically-informed transliteration schemes like\nthose devised by Muller et al. (2021) for Uyghur\nrequires large amounts of time and domain knowl-\nedge. VA’s fully data-driven nature and relatively\ncomparable performance suggest that it achieves\nan appealing balance between performance gain\nand implementation difficulty.\n4 Related Work\nOur work follows a long line of studies investi-\ngating the performance of multilingual language\nmodels like MBERT in various settings. The exact\nsource of such models’ crosslingual ability is con-\ntested: early studies attributed MBERT’s success to\nvocabulary overlap between languages (Cao et al.,\n2020; Pires et al., 2019; Wu and Dredze, 2019),\nRep. MHR(NER) UG (NER) UG (POS) UG (UD)\nFASTT 35.28 →41.32 (+6.04) – 89.45 →89.03 (–0.42) 54.52→54.45 (–0.07)\nBERT 54.17→48.45 (–5.72) 61.54→63.05 (+1.51) 90.87→90.76 (–0.09) 60.34→60.08 (–0.26)\nMBERT 61.85→63.84 (+1.99) 50.76→56.80 (+6.04) 78.07→91.34 (+13.27) 47.70→65.85 (+18.15)\nLAPT 59.17→63.68 (+4.51) 56.76→67.57 (+10.81) 79.88→92.59 (+12.71) 50.67→69.39 (+18.72)\nVA 64.23→63.19 (–1.04) 68.93→67.10 (–1.83) 91.49→92.64 (+1.15) 67.55→68.58 (+1.03)\nTable 4: Comparison of model performance before and after transliteration. Bolded results are the maximum for\neach language-task pair. – indicates that a model did not converge.\nbut subsequent studies find typological similarity\nand parameter sharing to be better explanations\n(Conneau et al., 2020b; K et al., 2020). Nonethe-\nless, past work has consistently highlighted the\nlimitations of multilingual models in the context\nof low-resource languages. Conneau et al. (2020a)\nhighlight the tension between crosslingual transfer\nand per-language model capacity, which poses a\nchallenge for low-resource languages that require\nboth. Indeed, Wu and Dredze (2020) find that\nMBERT is unable to outperform baselines in the\nlowest-resource seen languages. Our experiments\nbuild off these insights, which motivate the devel-\nopment of methods for adapting MBERT to target\nlow-resource languages.\nAdapting Language Models Several prior stud-\nies have proposed methods for adapting pretrained\nmodels to a downstream task. The simplest of\nthese is to perform additional pretraining on unla-\nbeled data in the target language (Chau et al., 2020;\nMuller et al., 2021; Pfeiffer et al., 2020), which in\nturn builds off similar approaches for domain adap-\ntation (Gururangan et al., 2020; Han and Eisenstein,\n2019). Recent work uses one or more of these ad-\nditional pretraining stages to specifically train mod-\nular adapter layers for specific tasks or languages,\nwith the goal of maintaining a language-agnostic\nmodel while improving performance on individ-\nual languages (Pfeiffer et al., 2020, 2021a; Vidoni\net al., 2020). However, as Muller et al. (2021) note,\nthe typological diversity of the world’s languages\nultimately limits the viability of this approach.\nOn the other hand, many adaptation techniques\nhave focused on improving representation of the\ntarget language by modifying the model’s vocabu-\nlary or tokenization schemes (Chung et al., 2020;\nClark et al., 2021; Wang et al., 2021). This is well-\nmotivated: Artetxe et al. (2020) emphasize repre-\nsentation in the vocabulary as a key factor for effec-\ntive crosslingual transfer, while Rust et al. (2021)\nfind that MBERT’s tokenization scheme for many\nlanguages is subpar. Pfeiffer et al. (2021b) further\nobserve that for languages with unseen scripts, a\nlarge proportion of the language is mapped to the\ngeneric “unknown” wordpiece, and they propose\na matrix factorization-based approach to improve\nscript representation. Wang et al. (2020) extend\nMBERT’s vocabulary with an entire new vocabu-\nlary in the target language to facilitate zero-shot\ntransfer to low-resource languages from English.\nThe present study most closely derives from Chau\net al. (2020), who select 99 wordpieces with the\ngreatest amount of coverage to augment MBERT’s\nvocabulary while preserving the remainder; and\nMuller et al. (2021), who transliterate target lan-\nguage data into Latin script to improve vocabulary\ncoverage. We deliver new insights on the effective-\nness and applicability of these methods.\n5 Conclusion\nWe explore the interactions between vocabulary\naugmentation and script transliteration for spe-\ncializing multilingual contextual word represen-\ntations in low-resource settings. We confirm vocab-\nulary augmentation’s effectiveness on multiple lan-\nguages, scripts, and tasks; identify the mix-in stage\nas amenable to specialization; and observe a nega-\ntive interaction between vocabulary augmentation\nand script transliteration. Our findings highlight\nseveral open questions in model specialization and\nlow-resource natural language processing at large,\nmotivating further study in this area.\nFuture directions for investigation are manifold.\nIn particular, our results in this work unify the sep-\narate findings of past works, which use MBERT as\na case study; a natural continuation would extend\nthese methods to a broader set of multilingual mod-\nels, such as mT5 (Xue et al., 2021) and XLM-R\n(Conneau et al., 2020a), in order to obtain a clearer\nunderstanding of the factors behind specialization\nmethods’ patterns of success. While we intention-\nally choose a set of small unlabeled datasets to\nevaluate on a setting applicable to the vast majority\nof the world’s low-resource languages, we acknowl-\nedge great variation in the amount of unlabeled data\navailable in different languages. Continued study\non the applicability of these methods to datasets\nof different sizes is an important future step. An\ninteresting direction of work is to train multilingual\nmodels on data where script respresentation is more\nbalanced, which might also allow for different out-\nput scripts for transliteration. Given that the mix-in\nstage is an effective opportunity to specialize mod-\nels to target languages, constructing mix-ins at both\nthe data and model level that are complementary by\ndesign has potential to be beneficial. Finally, future\nwork might shed light on the interaction between\ndifferent configurations of the adaptations studied\nhere (e.g., the number of wordpiece types used in\nvocabulary augmentation).\nAcknowledgments\nWe thank Jungo Kasai, Phoebe Mulcaire, and mem-\nbers of UW NLP for their helpful comments on\npreliminary versions of this paper. We also thank\nBenjamin Muller for insightful discussions and pro-\nviding details about transliteration methods and\nbaselines. Finally, we thank the anonymous review-\ners for their helpful remarks.\nReferences\nJudit Ács. 2019. Exploring BERT’s vocabulary.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proc. of ACL.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. TACL, 5:135–146.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Multi-\nlingual alignment of contextual word representations.\nIn Proc. of ICLR.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual BERT, a small corpus, and\na small treebank. In Findings of ACL: EMNLP.\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and\nJason Riesa. 2020. Improving multilingual models\nwith language-clustered vocabularies. In Proc. of\nEMNLP.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2021. Canine: Pre-training an efficient\ntokenization-free encoder for language representa-\ntion.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\ncross-lingual representation learning at scale. InProc.\nof ACL.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerging\ncross-lingual structure in pretrained language models.\nIn Proc. of ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proc. of NAACL.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biaffine attention for neural dependency pars-\ning. In Proc. of ICLR.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In Proc. of NLP-OSS.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProc. of ACL.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsuper-\nvised domain adaptation of contextualized embed-\ndings for sequence labeling. In Proc. of EMNLP-\nIJCNLP.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Proc. of\nEMNLP-IJCNLP.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and Dan\nRoth. 2020. Cross-lingual ability of multilingual\nBERT: An empirical study. In Proc. of ICLR.\nDan Kondratyuk and Milan Straka. 2019. 75 languages,\n1 model: Parsing universal dependencies universally.\nIn Proc. of EMNLP-IJCNLP.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProc. of EMNLP.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. In Proc. of\nNeurIPS.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nBenjamin Muller, Antonios Anastasopoulos, Benoît\nSagot, and Djamé Seddah. 2021. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn Proc. of NAACL-HLT.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProc. of LREC.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Proc.\nof ACL.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proc. of NAACL-HLT.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021a.\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proc. of EACL.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proc. of EMNLP.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2021b. Unks everywhere: Adapting mul-\ntilingual language models to new scripts.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proc. of\nACL.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proc. of\nACL.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli´c, Sebastian Ruder,\nand Iryna Gurevych. 2021. How good is your tok-\nenizer? on the monolingual performance of multilin-\ngual language models. In Proc. of ACL-IJCNLP.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. of NeurIPS.\nMarko Vidoni, Ivan Vuli ´c, and Goran Glavaš. 2020.\nOrthogonal language and task adapters in zero-shot\ncross-lingual transfer.\nXinyi Wang, Sebastian Ruder, and Graham Neubig.\n2021. Multi-view subword regularization. In Proc.\nof NAACL-HLT.\nZihan Wang, Karthikeyan K, Stephen Mayhew, and Dan\nRoth. 2020. Extending multilingual BERT to low-\nresource languages. In Findings of ACL: EMNLP.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Proc.\nof EMNLP.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proc. of EMNLP-IJCNLP.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? In Proc. of\nRepL4NLP.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing\nLiu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGreg Corrado, Macduff Hughes, and Jeffrey Dean.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilin-\ngual pre-trained text-to-text transformer. In Proc.\nof NAACL-HLT.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Noëmi\nAepli, Željko Agi´c, Lars Ahrenberg, Gabriel˙e Alek-\nsandraviˇci¯ut˙e, Lene Antonsen, Katya Aplonova,\nMaria Jesus Aranzabe, Gashaw Arutie, Masayuki\nAsahara, Luma Ateyah, Mohammed Attia, Aitz-\niber Atutxa, Liesbeth Augustinus, Elena Badmaeva,\nMiguel Ballesteros, Esha Banerjee, Sebastian Bank,\nVerginica Barbu Mititelu, Victoria Basmov, Colin\nBatchelor, John Bauer, Sandra Bellato, Kepa Ben-\ngoetxea, Yevgeni Berzak, Irshad Ahmad Bhat,\nRiyaz Ahmad Bhat, Erica Biagetti, Eckhard Bick,\nAgn˙e Bielinskien ˙e, Rogier Blokland, Victoria Bo-\nbicev, Loïc Boizou, Emanuel Borges Völker, Carl\nBörstell, Cristina Bosco, Gosse Bouma, Sam Bow-\nman, Adriane Boyd, Kristina Brokait ˙e, Aljoscha\nBurchardt, Marie Candito, Bernard Caron, Gauthier\nCaron, Tatiana Cavalcanti, Gül¸ sen Cebiro˘glu Ery-\ni˘git, Flavio Massimiliano Cecchini, Giuseppe G. A.\nCelano, Slavomír ˇCéplö, Savas Cetin, Fabricio\nChalub, Jinho Choi, Yongseok Cho, Jayeol Chun,\nAlessandra T. Cignarella, Silvie Cinková, Aurélie\nCollomb, Ça ˘grı Çöltekin, Miriam Connor, Ma-\nrine Courtin, Elizabeth Davidson, Marie-Catherine\nde Marneffe, Valeria de Paiva, Elvis de Souza,\nArantza Diaz de Ilarraza, Carly Dickerson, Bamba\nDione, Peter Dirix, Kaja Dobrovoljc, Timothy Dozat,\nKira Droganova, Puneet Dwivedi, Hanne Eckhoff,\nMarhaba Eli, Ali Elkahky, Binyam Ephrem, Olga\nErina, Tomaž Erjavec, Aline Etienne, Wograine\nEvelyn, Richárd Farkas, Hector Fernandez Al-\ncalde, Jennifer Foster, Cláudia Freitas, Kazunori\nFujita, Katarína Gajdošová, Daniel Galbraith, Mar-\ncos Garcia, Moa Gärdenfors, Sebastian Garza,\nKim Gerdes, Filip Ginter, Iakes Goenaga, Koldo\nGojenola, Memduh Gökırmak, Yoav Goldberg,\nXavier Gómez Guinovart, Berta González Saave-\ndra, Bernadeta Grici ¯ut˙e, Matias Grioni, Normunds\nGr¯uz¯ıtis, Bruno Guillaume, Céline Guillot-Barbance,\nNizar Habash, Jan Hajiˇc, Jan Hajiˇc jr., Mika Hämäläi-\nnen, Linh Hà M ˜y, Na-Rae Han, Kim Harris, Dag\nHaug, Johannes Heinecke, Felix Hennig, Barbora\nHladká, Jaroslava Hlaváˇcová, Florinel Hociung, Pet-\nter Hohle, Jena Hwang, Takumi Ikeda, Radu Ion,\nElena Irimia, O. lájídé Ishola, Tomáš Jelínek, An-\nders Johannsen, Fredrik Jørgensen, Markus Juutinen,\nHüner Ka¸ sıkara, Andre Kaasen, Nadezhda Kabaeva,\nSylvain Kahane, Hiroshi Kanayama, Jenna Kanerva,\nBoris Katz, Tolga Kayadelen, Jessica Kenney, Vá-\nclava Kettnerová, Jesse Kirchner, Elena Klemen-\ntieva, Arne Köhn, Kamil Kopacewicz, Natalia Kot-\nsyba, Jolanta Kovalevskait ˙e, Simon Krek, Sooky-\noung Kwak, Veronika Laippala, Lorenzo Lambertino,\nLucia Lam, Tatiana Lando, Septina Dian Larasati,\nAlexei Lavrentiev, John Lee, Phuong Lê H `ông,\nAlessandro Lenci, Saran Lertpradit, Herman Leung,\nCheuk Ying Li, Josie Li, Keying Li, KyungTae Lim,\nMaria Liovina, Yuan Li, Nikola Ljubeši´c, Olga Logi-\nnova, Olga Lyashevskaya, Teresa Lynn, Vivien Mack-\netanz, Aibek Makazhanov, Michael Mandl, Christo-\npher Manning, Ruli Manurung, C ˘at˘alina M ˘ar˘an-\nduc, David Mare ˇcek, Katrin Marheinecke, Héctor\nMartínez Alonso, André Martins, Jan Mašek, Yuji\nMatsumoto, Ryan McDonald, Sarah McGuinness,\nGustavo Mendonça, Niko Miekka, Margarita Misir-\npashayeva, Anna Missilä, C ˘at˘alin Mititelu, Maria\nMitrofan, Yusuke Miyao, Simonetta Montemagni,\nAmir More, Laura Moreno Romero, Keiko Sophie\nMori, Tomohiko Morioka, Shinsuke Mori, Shigeki\nMoro, Bjartur Mortensen, Bohdan Moskalevskyi,\nKadri Muischnek, Robert Munro, Yugo Murawaki,\nKaili Müürisep, Pinkey Nainwani, Juan Igna-\ncio Navarro Horñiacek, Anna Nedoluzhko, Gunta\nNešpore-B¯erzkalne, Luong Nguy ˜ên Th i., Huy `ên\nNguy˜ên Thi. Minh, Yoshihiro Nikaido, Vitaly Niko-\nlaev, Rattima Nitisaroj, Hanna Nurmi, Stina Ojala,\nAtul Kr. Ojha, Adéday`o. Olúòkun, Mai Omura, Petya\nOsenova, Robert Östling, Lilja Øvrelid, Niko Par-\ntanen, Elena Pascual, Marco Passarotti, Agnieszka\nPatejuk, Guilherme Paulino-Passos, Angelika Peljak-\nŁapi´nska, Siyao Peng, Cenel-Augusto Perez, Guy\nPerrier, Daria Petrova, Slav Petrov, Jason Phelan,\nJussi Piitulainen, Tommi A Pirinen, Emily Pitler,\nBarbara Plank, Thierry Poibeau, Larisa Ponomareva,\nMartin Popel, Lauma Pretkalni n, a, Sophie Prévost,\nProkopis Prokopidis, Adam Przepiórkowski, Ti-\nina Puolakainen, Sampo Pyysalo, Peng Qi, An-\ndriela Rääbis, Alexandre Rademaker, Loganathan\nRamasamy, Taraka Rama, Carlos Ramisch, Vinit Rav-\nishankar, Livy Real, Siva Reddy, Georg Rehm, Ivan\nRiabov, Michael Rießler, Erika Rimkut˙e, Larissa Ri-\nnaldi, Laura Rituma, Luisa Rocha, Mykhailo Ro-\nmanenko, Rudolf Rosa, Davide Rovati, Valentin\nRos, ca, Olga Rudina, Jack Rueter, Shoval Sadde,\nBenoît Sagot, Shadi Saleh, Alessio Salomoni, Tanja\nSamardži´c, Stephanie Samson, Manuela Sanguinetti,\nDage Särg, Baiba Saul ¯ıte, Yanin Sawanakunanon,\nNathan Schneider, Sebastian Schuster, Djamé Sed-\ndah, Wolfgang Seeker, Mojgan Seraji, Mo Shen,\nAtsuko Shimada, Hiroyuki Shirasu, Muh Shohibus-\nsirri, Dmitry Sichinava, Aline Silveira, Natalia Sil-\nveira, Maria Simi, Radu Simionescu, Katalin Simkó,\nMária Šimková, Kiril Simov, Aaron Smith, Isabela\nSoares-Bastos, Carolyn Spadine, Antonio Stella, Mi-\nlan Straka, Jana Strnadová, Alane Suhr, Umut Su-\nlubacak, Shingo Suzuki, Zsolt Szántó, Dima Taji,\nYuta Takahashi, Fabio Tamburini, Takaaki Tanaka, Is-\nabelle Tellier, Guillaume Thomas, Liisi Torga, Trond\nTrosterud, Anna Trukhina, Reut Tsarfaty, Francis\nTyers, Sumire Uematsu, Zde ˇnka Urešová, Larraitz\nUria, Hans Uszkoreit, Andrius Utka, Sowmya Vaj-\njala, Daniel van Niekerk, Gertjan van Noord, Vik-\ntor Varga, Eric Villemonte de la Clergerie, Veronika\nVincze, Lars Wallin, Abigail Walsh, Jing Xian Wang,\nJonathan North Washington, Maximilan Wendt, Seyi\nWilliams, Mats Wirén, Christian Wittern, Tsegay\nWoldemariam, Tak-sum Wong, Alina Wróblewska,\nMary Yako, Naoki Yamazaki, Chunxiao Yan, Koichi\nYasuoka, Marat M. Yavrumyan, Zhuoran Yu, Zdenˇek\nŽabokrtský, Amir Zeldes, Manying Zhang, and\nHanzhi Zhu. 2019. Universal dependencies 2.5.\nLINDAT/CLARIAH-CZ digital library at the Insti-\ntute of Formal and Applied Linguistics (ÚFAL), Fac-\nulty of Mathematics and Physics, Charles University.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8633327484130859
    },
    {
      "name": "Natural language processing",
      "score": 0.7555785179138184
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6442348957061768
    },
    {
      "name": "Parsing",
      "score": 0.6391518115997314
    },
    {
      "name": "Dependency grammar",
      "score": 0.6238993406295776
    },
    {
      "name": "Transliteration",
      "score": 0.5599603652954102
    },
    {
      "name": "Dependency (UML)",
      "score": 0.5498329997062683
    },
    {
      "name": "Vocabulary",
      "score": 0.49834418296813965
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.46787214279174805
    },
    {
      "name": "Machine translation",
      "score": 0.4477991461753845
    },
    {
      "name": "Language model",
      "score": 0.42626795172691345
    },
    {
      "name": "Linguistics",
      "score": 0.24861416220664978
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ]
}