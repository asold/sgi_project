{
    "title": "Can Unconditional Language Models Recover Arbitrary Sentences?",
    "url": "https://openalex.org/W2956352737",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5109481132",
            "name": "Nishant Subramani",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A5112713734",
            "name": "Samuel R. Bowman",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5091175785",
            "name": "Kyunghyun Cho",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963563735",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W2963773505",
        "https://openalex.org/W2963336685",
        "https://openalex.org/W1815076433",
        "https://openalex.org/W2124807415",
        "https://openalex.org/W2963376432",
        "https://openalex.org/W2105464873",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2963277143",
        "https://openalex.org/W3029645440",
        "https://openalex.org/W2785533664",
        "https://openalex.org/W3005347330",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2143017621",
        "https://openalex.org/W2964268978",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W2922709902",
        "https://openalex.org/W2170973209",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W1828163288",
        "https://openalex.org/W2888779557",
        "https://openalex.org/W2963088995",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2963223306",
        "https://openalex.org/W1915251500",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W2963506925",
        "https://openalex.org/W2353655624",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2970597249"
    ],
    "abstract": "Neural network-based generative language models like ELMo and BERT can work effectively as general purpose sentence encoders in text classification without further fine-tuning. Is it possible to adapt them in a similar way for use as general-purpose decoders? For this to be possible, it would need to be the case that for any target sentence of interest, there is some continuous representation that can be passed to the language model to cause it to reproduce that sentence. We set aside the difficult problem of designing an encoder that can produce such representations and, instead, ask directly whether such representations exist at all. To do this, we introduce a pair of effective, complementary methods for feeding representations into pretrained unconditional language models and a corresponding set of methods to map sentences into and out of this representation space, the reparametrized sentence space. We then investigate the conditions under which a language model can be made to generate a sentence through the identification of a point in such a space and find that it is possible to recover arbitrary sentences nearly perfectly with language models and representations of moderate size without modifying any model parameters.",
    "full_text": "Can Unconditional Language Models Recover\nArbitrary Sentences?\nNishant Subramani\nNew York University\nnishant@nyu.edu\nSamuel R. Bowman\nNew York University\nKyunghyun Cho\nNew York Univeristy\nFacebook AI Research\nCIFAR Azrieli Global Scholar\nAbstract\nNeural network-based generative language models like ELMo and BERT can work\neffectively as general purpose sentence encoders in text classiﬁcation without\nfurther ﬁne-tuning. Is it possible to adapt them in a similar way for use as general-\npurpose decoders? For this to be possible, it would need to be the case that for\nany target sentence of interest, there is some continuous representation that can be\npassed to the language model to cause it to reproduce that sentence. We set aside\nthe difﬁcult problem of designing an encoder that can produce such representations\nand, instead, ask directly whether such representations exist at all. To do this, we\nintroduce a pair of effective, complementary methods for feeding representations\ninto pretrained unconditional language models and a corresponding set of methods\nto map sentences into and out of this representation space, the reparametrized\nsentence space. We then investigate the conditions under which a language model\ncan be made to generate a sentence through the identiﬁcation of a point in such\na space and ﬁnd that it is possible to recover arbitrary sentences nearly perfectly\nwith language models and representations of moderate size without modifying any\nmodel parameters.\n1 Introduction\nWe have recently seen great successes in using pretrained language models as encoders for a range\nof difﬁcult natural language processing tasks (Dai and Le, 2015; Peters et al., 2017, 2018; Radford\net al., 2018; Ruder and Howard, 2018; Devlin et al., 2018; Dong et al., 2019; Yang et al., 2019), often\nwith little or no ﬁne-tuning: Language models learn useful representations that allow them to serve\nas general-purpose encoders. A hypothetical general-purpose decoderwould offer similar beneﬁts:\nmaking it possible to both train models for text generation tasks with little annotated data and share\nparameters extensively across applications in environments where memory is limited. Then, is it\npossible to use a pretrained language model as a general-purpose decoderin a similar fashion?\nFor this to be possible, we would need both a way of feeding some form of continuous sentence\nrepresentation into a trained language model and a task-speciﬁc encoder that could convert some\ntask input into a sentence representation that would cause the language model to produce the\ndesired sentence. We are not aware of any work that has successfully produced an encoder that can\ninteroperate in this way with a pretrained language model, and in this paper, we ask whether it is\npossible at all: Are typical, trained neural network language models capable of recovering arbitrary\nsentences through conditioning of this kind?\nWe start by deﬁning the sentence spaceof a recurrent language model and show how this model maps\na given sentence to a trajectory in this space. We reparametrize this sentence space into a new space,\nthe reparametrized sentence space, by mapping each trajectory in the original space to a point in the\nnew space. To accomplish the reparametrization, we introduce two complementary methods to add\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:1907.04944v2  [cs.CL]  9 Jan 2020\nadditional bias terms to the previous hidden and cell state at each time step in the trained and frozen\nlanguage model, and optimize those bias terms to maximize the likelihood of the sentence.\nRecoverability inevitably depends on model size and quality of the underlying language model, so\nwe vary both along with different dimensions for the reparametrized sentence space. We ﬁnd that the\nchoice of optimizer (nonlinear conjugate gradient over stochastic gradient descent) and initialization\nare quite sensitive, so it is unlikely that a simple encoder setup would work out of the box.\nOur experiments reveal that we can achieve full recoverability with a reparametrized sentence space\nwith dimension equal to the dimension of the recurrent hidden state of the model, at least for large\nenough models: For nearly all sentences, there exists a single vector that can recover the sentence\nperfectly. We show that this trend holds even with sentences that come from a different domain\nthan the ones used to train the ﬁxed language model. We also observe that the smallest dimension\nable to achieve the greatest recoverability is approximately equal to the dimension of the recurrent\nhidden state of the model. Furthermore, we observe that recoverability decreases as sentence length\nincreases and that models ﬁnd it increasingly difﬁcult to generate words later in a sentence. In other\nwords, models rarely generate any correct words after generating an incorrect word when decoding\na given sentence. Lastly, experiments on recovering random sequences of words show that our\nreparametrized sentence space does not simply memorize the sequence, but also utilizes the language\nmodel. These observations indicate that unconditional language models can indeed be conditioned to\nrecover arbitrary sentences almost perfectly and may have a future as universal decoders.\n2 The Sentence Space of a Recurrent Language Model\nIn this section, we ﬁrst cover the background on recurrent language models. We then characterize\nits sentence space and show how we can reparametrize it for easier analysis. In this reparametrized\nsentence space, we deﬁne the recoverability of a sentence.\n2.1 Recurrent Language Models\nModel Description We train a 2-layer recurrent language model over sentences autoregressively:\np(x1,...,x T) =\nT∏\nt=1\np(xt|x1,...,x t−1) (1)\nA neural network models each conditional distribution (right side) by taking as input all the pre-\nvious tokens (x1,...,x t−1) and producing as output the distribution over all possible next tokens.\nAt every time-step, we update the internal hidden state ht−2, which summarizes (x1,...,x t−2),\nwith a new token xt−1, resulting in ht−1. This resulting hidden state, ht−1, is used to compute\np(xt|x1,...,x t−1):\nht−1 = fθ(ht−2,xt−1), (2)\np(xt = i|x1,...,x t−1) =gi\nθ(ht−1), (3)\nwhere fθ : Rd ×V →Rd is a recurrent transition function often implemented as an LSTM recurrent\nnetwork (as in Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010). The readout function gis\ngenerally a softmax layer with dedicated parameters for each possible word. The incoming hidden\nstate h0 ∈Rd at the start of generation is generally an arbitrary constant vector. We use zeroes. For a\nLSTM language model with llayers of dLSTM units, its model dimension d∗= 2dlbecause LSTMs\nhave two hidden state vectors (conventionally h and c) both of dimension d.\nTraining We train the full model using stochastic gradient decent with negative log likelihood loss.\nInference Once learning completes, a language model can be straightforwardly used in two ways:\nscoring and generation. To score, we compute the log-probability of a newly observed sentence\naccording to Eq. (1). To generate, we use ancestral sampling by sampling tokens (x1,...,x T)\nsequentially, conditioning on all previous tokens at each step via Eq. (1).\nIn addition, we can ﬁnd the approximate most likely sequence using beam search (Graves, 2012).\nThis procedure is generally used with language model variants like sequence-to-sequence models\n(Sutskever et al., 2014) that condition on additional context. We use this procedure in backward\nestimation to recover the sentence corresponding to a given point in the reparametrized space.\n2\n h1Wz z ...\nx1\nx2\n  h2\nx2\nx3\n  h3\nx3\nx4\n  hT\nxT-1\nxT\n���\n h1\n...\nx1\nx2\n  h2\nx2\nx3\n  h3\nx3\nx4\n  hT\nxT-1\nxT\nZ\nz z ... zK1 2 ���\nFigure 1: We add an additional bias, Wzz(left, when dim(z) ≤d∗) or Z = [z1 ...z K] (right, when\ndim(z) >d∗), to the previous hidden and cell state at every time step. Only the zvector or Zmatrix\nis trained during forward estimation: The main LSTM parameters are frozen and Wz is set randomly.\nIn the right hand case, we use soft attention to allow the model to use different slices of Zeach step.\n2.2 Deﬁning the Sentence Space\nThe recurrent transition function fθ in Eq. (2) deﬁnes a dynamical system driven by the observations\nof tokens (x1,...,x T) ∈X in a sentence. In this dynamical system, all trajectories start at the\norigin h0 = [0,..., 0]⊤and evolve according to incoming tokens (xt’s) over time. Any trajectory\n(h0,...,h T) is entirely embedded in a d-dimensional space, where dis equal to the dimension of the\nhidden state and H∈ Rd, i.e., ht ∈H. In other words, the language model embeds a sentence of\nlength T as a T + 1-step trajectory in a d-dimensional space H, which we refer to as the sentence\nspace of a language model.\nReparametrizing the Sentence Space We want to recover sentences from semantic representations\nthat do not encode sentence length symbolically. Given that and since a single replacement of an\nintermediate token can drastically change the remaining trajectory in the sentence space, we want\na ﬂat-vector representation. In order to address this, we propose to (approximately) reparametrize\nthe sentence space into a ﬂat-vector space Z∈ Rd′\nto characterize the sentence space of a language\nmodel. Under the proposed reparameterization, a trajectory of hidden states in the sentence space H\nmaps to a vector of dimension d′in the reparametrized sentence space Z. To accomplish this, we add\nbias terms to the previous hidden and cell state at each time step in the model and optimize them to\nmaximize the log probability of the sentence as shown in Figure 1. We add this bias in two ways:\n(1) if d′≤d∗, we use a random projection matrix to project our vector z ∈Rd′\nup to d∗and (2) if\nd′>d∗, we use soft-attention with the previous hidden state to adaptively project our vector z∈Rd′\ndown to d∗(Bahdanau et al., 2015).\nOur reparametrization must approximately allow us to go back (forward estimation) and forth (back-\nward estimation) between a sequence of tokens, (x1,...,x T), and a point zin this reparametrized\nspace Zvia the language model. We need back-and-forth reparametrization to measure recoverability.\nOnce this back-and-forth property is established, we can inspect a set of points in Zinstead of trajec-\ntories in H. A vector z∈Z resembles the output of an encoder acting as context for a conditional\ngeneration task. This makes analysis in Zresemble analyses of context on sequence models and thus\nhelps us understand the unconditional language model that we are trying to condition with zbetter.\nWe expect that our reparametrization will allow us to approximately go back and forth between a\nsequence and its corresponding point z ∈Z because we expect zto contain all of the information\nof the sequence. Since we’re adding zat every time-step, the information preserved in zwill not\ndegrade as quickly as the sequence is processed like it could if we just added it to the initial hidden\nand cell states. While there are other similar ways to integrate z, we choose to modify the recurrent\nconnection.\nUsing the Sentence Space In this paper, we describe the reparametrized sentence space Zof a\nlanguage model as a set of d′-dimensional vectors that correspond to a set D′of sentences that were\nnot used in training the underlying language model. This use of unseen sentences helps us understand\nthe sentence space of a language model in terms of generalization rather than memorization, providing\ninsight into the potential of using a pretrained language model as a ﬁxed decoder/generator. Using\nour reparametrized sentence space framework, evaluation techniques designed for investigating word\nvectors become applicable. One of those interesting techniques that we can do now is interpolation\nbetween different sentences in our reparameterized sentence space (Table 1 in Choi et al., 2017;\nBowman et al., 2016), but we do not explore this here.\n3\nForward Estimation X→Z The goal of forward estimation is to ﬁnd a pointz∈Z that represents\na sentence (x1,...,x T) ∈X via the trained language model (i.e., ﬁxed θ). When the dimension of z\nis smaller than the model dimension d∗, we use a random projection matrix to project it up to d∗and\nwhen the dimension of zis greater than the model dimension, we use soft attention to project it down\nto d∗. We modify the recurrent dynamics fθ in Eq. (2) to be:\nht−1 = fθ(ht−2 + z′,xt−1) (4)\nz′=\n{Wzz, if dim(z) ≤d∗\nsoftmax(h⊤\nt−2Z)Z⊤, if dim(z) >d∗ (5)\nwhere Z ∈Rd×k and is just the unﬂattened matrix of zconsisting of k= dim(z)/dvectors of dimen-\nsion d. We initialize the hidden state byh0 = z′. Wz ∈Rd×d′\nis a random matrix withL2-normalized\nrows, following Li et al. (2018) and is an identity matrix when d= d′: Wz = [w1\nz; ... ; wd′\nz ], where\nwl\nz = ϵl/∥ϵl∥2 and ϵl ∼N(0,12) ∈Rd. We then estimate zby maximizing the log-probability of\nthe given sentence under this modiﬁed model, while ﬁxing the original parameters θ:\nˆz= argmax\nz∈Z\nT∑\nt=1\nlog p(xt|x<t,z) (6)\nWe represent the entire sentence (x1,...,x T) in a single z. To solve this optimization problem,\nwe can use any off-the-shelf gradient-based optimization algorithm, such as gradient descent or\nnonlinear conjugate descent. This objective function is highly non-convex, potentially leading to\nmultiple approximately optimal z’s. As a result, to estimatezin forward estimation, we use nonlinear\nconjugate gradient (Wright and Nocedal, 1999) implemented in SciPy (Jones et al., 2014) with a\nlimit of 10,000 iterations, although almost all runs converge much more quickly. Our experiments,\nhowever, reveal that many of thesez’s lead to similar performance in recovering the original sentence.\nBackward Estimation Z→X Backward estimation, an instance of sequence decoding, aims at\nrecovering the original sentence (x1,...,x T) given a point z in the reparametrized sentence Z,\nwhich we refer to as recovery. We use the same objective function as in Eq. (6), but we optimize over\n(x1,...,x T) rather than over z. Unlike forward estimation, backward estimation is a combinatorial\noptimization problem and cannot be solved easily with a recurrent language model (Cho, 2016; Chen\net al., 2018). To circumvent this, we use beam search, which is a standard approach in conditional\nlanguage modeling applications such as machine translation. Our backward estimation procedure\ndoes not assume a true length when decoding with beam search—we stop when an end of token or\n100 tokens is reached.\n2.3 Analyzing the Sentence Space through Recoverability\nUnder this formulation, we can investigate various properties of the sentence space of the underlying\nmodel. As a ﬁrst step toward understanding the sentence space of a language model, we propose three\nround-trip recoverability metrics and describe how we use them to characterize the sentence space.\nRecoverability Recoverability measures how much information about the original sentence x =\n(x1,...,x T) ∈ Xis preserved in the reparameterized sentence space Z. We measure this by\nreconstructing the original sentence x. First, we forward-estimate the sentence vector z∈Z from\nx∈X by Eq. (6). Then, we reconstruct the sentence ˆxfrom the estimated zvia backward estimation.\nTo evaluate the quality of reconstruction, we compare the original and reconstructed sentences, xand\nˆxusing the following three metrics:\n1. Exact Match (EM): ∑T\nt=1 I(xt = ˆxt)/T\n2. BLEU (Papineni et al., 2002)\n3. Preﬁx Match (PM): argmaxtEM(x≤t = ˆx≤t)/T\nExact match gives information about the possibility of perfect recoverability. BLEU provides us with\na smoother approximation to this, in which the hypothesis gets some reward for n-gram overlap, even\nif slightly inexact. Since BLEU is 0 for sentences with less than 4 tokens, we smooth these by only\nconsidering n-grams up to the sentence length if sentence length is less than 4. Preﬁx match measures\nthe longest consecutive sequence of tokens that are perfectly recovered from the beginning of the\n4\nsentence and we divide this by the sentence length. We use preﬁx match because early experiments\nshow a very strong left-to-right falloff in quality of generation. In other words, candidate generations\nare better for shorter sentences and once an incorrect token is generated, future tokens are extremely\nunlikely to be correct. We compute each metric for each sentence x∈D′by averaging over multiple\noptimization runs, we show exact match (EM) in the equations, but we do the same for BLEU and\nPreﬁx Match. To counter the effect of non-convex optimization in Eq. (6), these runs vary by the\ninitialization of zand the random projection matrix Wz in Eq. (4). That is,\nEM(x,θ) =Ez0∈Z\n[\nEWz∈Rd×d′[EM(x,ˆx)]\n]\nEffective Dimension by Recoverability These recoverability measures allow us to investigate the\nunderlying properties of the proposed sentence space of a language model. If all sentences can be\nprojected into a d-dimensional sentence space Zand recovered perfectly, the effective dimension\nof Zmust be no greater than d. In this paper, when analyzing the effective dimension of a sentence\nspace of a language model, we focus on the effective dimension given a target recoverabilityτ:\nˆd′(θ,τ) = min\n{\nd′⏐⏐EM(D′,θ) >τ\n}\n(7)\nwhere EM(D′,θ) = 1\n|D′|\n∑\nx∈D′EM(x,θ). In other words, given a trained model (θ), we ﬁnd the\nsmallest effective dimension d′(the dimension of Z) that satisﬁes the target recoverability (τ). Using\nthis, we can answer questions like what is the minimum dimensiond′needed to achieve recoverability\nτ under the model θ. Using this, the unconstrained effective dimension, i.e. the smallest dimension\nthat satisﬁes the best possible recoverability, is:\nˆd′(θ) = argmin\nd′∈{1,...,d}\nmax 1\n|D′|\n∑\nx∈D′\nEM(x,θ) (8)\nWe approximate the effective dimension by inspecting various values ofd′on a logarithmic scale:\nd′= 128,256,512,..., 32768. Since our forward estimation process uses non-convex optimization\nand our backward estimation process uses beam search, our effective dimension estimates are\nupper-bound approximations.\n3 Experimental Setup\nCorpus We use the ﬁfth edition of the English Gigaword (Graff et al., 2003) news corpus. Our\nprimary model is trained on 50M sentences from this corpus, and analysis experiments additionally\ninclude a weaker model trained on a subset of only 10M. Our training sentences are drawn from\narticles published before November 2010. We use a development set with 879k sentences from the\narticles published in November 2010 and a test set of 878k sentences from the articles published\nin December 2010. We lowercase the entire corpus, segment each article into sentences using\nNLTK (Bird and Loper, 2004), and tokenize each sentence using the Moses tokenizer (Koehn et al.,\n2007). We further segment the tokens using byte-pair encoding (BPE; following Sennrich et al.,\n2016) with 20,000 merges to obtain a vocabulary of 20,234 subword tokens. To evaluate out-of-\ndomain sentence recoverability, we use a random sample of 50 sentences from the IWSLT16 English\nto German translation dataset (validation portion) processed in the same way and using the same\nvocabulary.\nRecurrent Language Models The proposed framework is agnostic to the underlying architecture\nof a language model. We choose a 2-layer language model with LSTM units (Graves, 2013). We\nconstruct a small, medium, and large language model consisting of 256, 512, and 1024 LSTM\nunits respectively in each layer. The input and output embedding matrices of 256, 512, and 1024-\ndimensional vectors respectively are shared (Press and Wolf, 2017). We use dropout (Srivastava et al.,\n2014) between the two recurrent layers and before the ﬁnal linear layer with a drop rate of 0.1, 0.25,\nand 0.3 respectively. We use stochastic gradient descent with Adam with a learning rate of 10−4 on\n100-sentence minibatches (Kingma and Ba, 2014), where sentences have a maximum length of 100.\nWe measure perplexity on the development set every 10k minibatches, halve the learning rate\nwhenever it increases, and clip the norm of the gradient to 1 (Pascanu et al., 2013). For each training\nset (10M and 50M), we train for only one epoch. Because of the large size of the training sets, these\nmodels nonetheless achieve a good ﬁt to the underlying distribution (Table 1).\n5\nTable 1: Language modeling perplexities on English Gigaword for the models under study.\n|Train| = 10M |Train| = 50M\nModel d Dev Ppl. Test Ppl. Dev Ppl. Test Ppl.\nSMALL 256 122.9 125.2 77.2 79.2\nMEDIUM 512 89.6 91.3 62.1 63.5\nLARGE 1024 65.9 67.7 47.4 48.9\nReparametrized Sentence Spaces We use a set D′of 100 randomly selected sentences from the\ndevelopment set in our analysis. We set zto have 128, 256, 512, 1024, 2048, 4096, 8192, 16384\nand 32768 dimensions for each language model and measure its recoverability. For each sentence\nwe have ten random initializations. When the dimension d′of the reparametrized sentence space is\nsmaller than the model dimension, we construct ten random projection matrices that are sampled\nonce and ﬁxed throughout the optimization procedure. We perform beam search with beam width 5.\n4 Results and Analysis\nRecoverability Results In Figure 2, we present the recoverability results of our experiments relative\nto sentence length using the three language models trained on 50M sentences. We observe that\nthe recoverability increases as d′increases until d′= d∗. After this point, recoverability plateaus.\nRecoverability between metrics for a single model are strongly positively correlated. We also observe\nthat recoverability is nearly perfect for the large model when d′= 4096achieving EM ≥99, and\nvery high for the medium model when d′≥2048 achieving EM ≥84.\nWe ﬁnd that recoverability increases for a speciﬁc d′as the language model is trained, although we\ncannot present the result due to space constraints. The corresponding ﬁgure to Figure 2 for the 10M\nsetting and tables for both of the settings detailing overall performance are provided in the appendix.\nAll these estimates have high conﬁdence (small standard deviations).\nEffective Dimension of the Sentence Space From Figure 2, the large model’s unconstrained effec-\ntive dimension is d∗= 4096with a slight degradation in recoverability when increasingd′beyond d∗.\nFor the medium model, we notice that its unconstrained effective dimension is also d∗= 2048with\nno real recoverability improvements when increasing d′beyond d∗. For the small model, however, its\nunconstrained effective dimension is 8192, which is much greater than d∗= 1024.\nWhen d′= 4096, we can recover any sentence nearly perfectly, and for large sentences, the large\nmodel with d′≥4096 achieves recoverability estimates τ ≥0.8. For other model sizes and other\ndimensions of the reparametrized space, we fail to perfectly recover some sentences. To ascertain\nwhich sentences we fail to recover, we look at the shapes of each curve. We observe that the vast\nmajority of these curves never increase, indicating recoverability and sentence length have a strong\nnegative correlation. Most curves decrease to 0 as sentence length exceeds 30 indicating that longer\nsentences are more difﬁcult to recover. Earlier observations in using neural sequence-to-sequence\nmodels for machine translation concluded exactly this (Cho et al., 2014; Koehn and Knowles, 2017).\nThis suggests that a ﬁxed-length representation lacks the capacity to represent a complex sentence\nand could sacriﬁce important information in order to encode others. The degradation in recoverability\nalso implies that the unconstrained effective dimension of the sentence space could be strongly related\nto the length of the sentence and may not be related to the model dimension d∗. The fact that the\nsmaller model has an unconstrained effective dimension much larger than d∗supports this claim.\nImpact of Beam Width & Optimization Strategy To analyze the impact of various beam widths,\nwe experimented with beam widths of 5, 10, and 20 in decoding. We ﬁnd that results are consistent\nacross these beam widths. As a result, all experimental results in this paper other than this one use a\nbeam width of 5. We provide a representative partial table of sentence recoverability varying just\nbeam width during decoding in Table 2.\nTo understand the importance of the choice of optimizer, we experimented with using Adam with\na learning rate of 10−4 with default settings on our best performing settings for each model size.\nWe ﬁnd that using Adam results in recovery estimates that do not exceed 1.0 BLEU for all three\nsituations, hinting at the highly non-convex nature of the optimization problem.\n6\nExact Match\nSmall Model (256d)\n Medium Model (512d)\n Large Model (1024d)\nBLEU\nPreﬁx Match\nSentence Length\n Sentence Length\n Sentence Length\nFigure 2: Plots of the three recoverability metrics with respect to varying sentence lengths for each\nof our three model sizes for the 50M sentence setting. Within each plot, the curves correspond to the\nvarying dimensions of zincluding error regions of ±σ. Regardless of metric, recoverability improves\nas the size and quality of the language model and dimension of the reparametrized sentence space\nincreases. The corresponding plot for the 10M sentence setting is in the appendix.\nSources of Randomness There are two points of stochasticity in the proposed framework: the\nnon-convexity of the optimization procedure in forward estimation (Eq. 6) and the sampling of a\nrandom projection matrix Wz. However, based on the small standard deviations in Figure 2, these\nhave minimal impact on recoverability. Also, the observation of high conﬁdence (low-variance)\nupper-bound estimates for recoverability supports the usability of our recoverability metrics for\ninvestigating a language model’s sentence space.\nOut-of-Domain Recoverability To study how well our pretrained language models can recover\nsentences out-of-domain, we evaluate recoverability on our IWSLT data. IWSLT is comrpised of\nTED talk transcripts, a very different style than the news corpora our language models were trained\non. The left and center graphs in Figure 3 show that recovery performance measured in BLEU is\nnearly perfect even for out-of-domain sentences for both the medium and large models when d′≥d∗,\nfollowing trends from the experiments on English Gigaword from Figure 2.\nMore than just Memorization Near-perfect performance on out-of-domain sentences indicates that\nthis methodology could either be learning important properties of language by leveraging the language\n7\nTable 2: Recoverability (BLEU) varying beam width on English Gigaword\nBLEU\nModel |Z| Width=5 Width=10 Width=20\nSMALL ; 50M 512 40.0 40.3 40.5\nSMALL ; 50M 8192 81.1 79.8 79.6\nMEDIUM ; 50M 512 41.1 41.1 42.3\nMEDIUM ; 50M 16384 92.4 91.9 89.8\nLARGE ; 50M 512 54.8 54.1 53.8\nLARGE ; 50M 4096 99.8 99.8 99.5\nBLEU\nMedium (512d) - IWSLT\n Large (1024d) - IWSLT\n Large (1024d) - Random\nFigure 3: Recoverability (BLEU) on IWSLT for medium (left) and large models (center) and on the\nrandom data for the large model (right).\nmodel, which helps generalization, or just be memorizing any arbitrary sequence without using the\nlanguage model at all. To investigate this, we randomly sample 50 sentences of varying lengths where\neach token is sampled randomly with equal probability with replacement from the vocabulary. The\nright graph in Figure 3 shows BLEU recovery for the large model. Many of the shorter sequences\ncan be recovered well, but for sequences greater than 25 subword units, recoverability drops quickly.\nThis experiment shows that memorization cannot fully explain results on Gigaword or IWSLT16.\nTowards a General-Purpose Decoder In this formulation, our vector z′ can be considered as\ntrainable context used to condition our unconditioned language models to generate arbitrary sentences.\nSince we ﬁnd that well-trained language models of reasonable size have an unconstrained effective\ndimension with high recoverability that is approximately its model dimension on both in-domain and\nout-of-domain sequences, unconditional language models are able to utilize our context z′effectively.\nFurther experiments conﬁrm that our context vectors do not simply memorize arbitrary sequences, but\nleverage the language model to generate well-formed sequences. As a result, such a model could be\nused as a task-independent decoder given an encoder with the ability to generate an optimal context\nvector z′.\nWe observe that recoverability isn’t perfect for both the small and medium models, falling off\ndramatically for longer sentences, indicating that the minimum model size for high recoverability is\nfairly large. Since the sentence length distribution is a Zipf distribution (heavily right-tailed), if we\ncan increase the recoverability degredation cutoff point, the number of sentences we fail to recover\nperfectly would decrease exponentially. However, since we ﬁnd that larger and better-trained models\ncan exhibit near perfect recoverability for both in-domain and out-of-domain sequences and can more\neasily utilize our conditioning strategy, we think that this may only be a concern for lower capacity\nmodels. Our methodology could use a regularization mechanism to smooth the implicit sentence\nspace. This may improve recoverability and reduce the unconstrained effective dimension, whereby\nincreasing the applicability of an unconditional language model as a general-purpose decoder.\n8\n5 Related Work\nLatent Variable Recurrent Language Models The way we describe the sentence space of a lan-\nguage model can be thought of as performing inference over an implicit latent variablezusing a ﬁxed\ndecoder θ. This resembles prior work on sparse coding (Olshausen and Field, 1997) and generative\nlatent optimization (Bojanowski et al., 2018). Under this lens, it also relates to work on training\nlatent variable language models, such as models based on variational autoencoders by Bowman et al.\n(2016) and sequence generative adversarial networks by Yu et al. (2017). The goal of identifying the\nsmallest dimension of the sentence space for a speciﬁc target recoverability resembles work looking\nat continuous bag-of-words representations by Mu et al. (2017). Our approach differs from these\napproaches in that we focus entirely on analyzing a ﬁxed model that was trained unconditionally. Our\nformulation of the sentence space also is more general, and potentially applies to all of these models.\nPretrained Recurrent Language Models Pretrained or separately trained language models have\nlargely been used in two contexts: as a feature extractor for downstream tasks and as a scoring\nfunction for a task-speciﬁc decoder (Gulcehre et al., 2015; Li et al., 2016; Sriram et al., 2018). None\nof the above analyze how a pretrained model represents sentences nor investigate the potential of\nusing a language model as a decoder. The work by Zoph et al. (2016) transfers a pretrained language\nmodel, as a part of a neural machine translation system, to another language pair and ﬁne-tunes.\nThe positive result here is speciﬁc to machine translation as a downstream task, unlike the proposed\nframework, which is general and downstream task independent. Recently, there has been more\nwork in pretraining the decoder using BERT (Devlin et al., 2018) for neural machine translation and\nabstractive summarization (Edunov et al., 2019; Lample and Conneau, 2019; Song et al., 2019).\n6 Conclusion\nTo answer whether unconditional language models can be conditioned to generate held-out sentences,\nwe introduce the concept of the reparametrized sentence space for a frozen, pretrained language\nmodel, in which each sentence is represented as a point vector, which is added as a bias and optimized\nto reproduce that sentence during decoding. We design optimization-based forward estimation and\nbeam-search-based backward estimation procedures, allowing us to map a sentence to and from the\nreparametrized space. We then introduce and use recoverability metrics that allow us to measure the\neffective dimension of the reparametrized space and to discover the degree to which sentences can be\nrecovered from ﬁxed-sized representations by the model without further training.\nWe observe that we can indeed condition our unconditional language models to generate held-out\nsentences both in and out-of-domain: our large model achieves near perfect recoverability on both\nin and out-of-domain sequences with d′ = 8192 across all metrics. Furthermore, we ﬁnd that\nrecoverability increases with the dimension of the reparametrized space until it reaches the model\ndimension, after which, it plateaus for well-trained, sufﬁciently-large (d≥512) models.\nThese experiments reveal two properties of the sentence space of a language model. First, recoverabil-\nity improves with the size and quality of the language model and is nearly perfect when the dimension\nof the reparametrized space equals that of the model. Second, recoverability is negatively correlated\nwith sentence length, i.e., recoverability is more difﬁcult for longer sentences. Our recoverability-\nbased approach for analyzing the sentence space gives conservative estimates (upper-bounds) of the\neffective dimension of the space and lower-bounds for the associated recoverabilities.\nWe see three avenues for further work. Measuring the realtionship between regularization (encourag-\ning the reparametrized sentence space to be of a certain form) and non-linearity would be valuable.\nIn addition, although our framework is downstream task- and network architecture-independent, we\nwant to compare recoverability and downstream task performance and analyze the sentence space of\ndifferent architectures of language models. We also want to utilize this framework to convert encoder\nrepresentations for use in a data- and memory-efﬁcient conditional generation model.\nAcknowledgments\nThis work was supported by Samsung Electronics (Improving Deep Learning using Latent Structure).\nWe gratefully acknowledge the support of NVIDIA Corporation with the donation of a Titan V GPU\nused at NYU for this research.\n9\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by\njointly learning to align and translate. In ICLR.\nSteven Bird and Edward Loper. 2004. Nltk: the natural language toolkit. In ACL.\nPiotr Bojanowski, Armand Joulin, David Lopez-Pas, and Arthur Szlam. 2018. Optimizing the latent\nspace of generative networks. In ICML.\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous space. CoNLL 2016.\nYun Chen, Victor OK Li, Kyunghyun Cho, and Samuel Bowman. 2018. A stable and effective\nlearning strategy for trainable greedy decoding. In EMNLP.\nKyunghyun Cho. 2016. Noisy parallel approximate decoding for conditional recurrent language\nmodel. arXiv preprint arXiv:1605.03835.\nKyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the\nproperties of neural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8,\nEighth Workshop on Syntax, Semantics and Structure in Statistical Translation.\nHeeyoul Choi, Kyunghyun Cho, and Yoshua Bengio. 2017. Context-dependent word representation\nfor neural machine translation. Computer Speech & Language.\nAndrew M. Dai and Quoc V . Le. 2015. Semi-supervised sequence learning. InNIPS.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. CoRR.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and\nHsiao-Wuen Hon. 2019. Uniﬁed language model pre-training for natural language understanding\nand generation. arXiv preprint arXiv:1905.03197.\nSergey Edunov, Alexei Baevski, and Michael Auli. 2019. Pre-trained language model representations\nfor language generation. arXiv preprint arXiv:1903.09722.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2003. English gigaword. Linguistic Data\nConsortium, Philadelphia.\nAlex Graves. 2012. Sequence transduction with recurrent neural networks. arXiv preprint\narXiv:1211.3711.\nAlex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi\nBougares, Holger Schwenk, and Yoshua Bengio. 2015. On using monolingual corpora in neural\nmachine translation. arXiv preprint arXiv:1503.03535.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation.\nEric Jones, Travis Oliphant, Pearu Peterson, et al. 2014. Scipy: Open source scientiﬁc tools for\npython, 2014. http://www.scipy.org.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola\nBertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open\nsource toolkit for statistical machine translation. In ACL.\nPhilipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. ACL\n2017.\n10\nGuillaume Lample and Alexis Conneau. 2019. Cross-lingual language model pretraining. arXiv\npreprint arXiv:1901.07291.\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. 2018. Measuring the intrinsic\ndimension of objective landscapes. arXiv preprint arXiv:1804.08838.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting\nobjective function for neural conversation models. In NAACL.\nTomáš Mikolov, Martin Karaﬁát, Lukáš Burget, JanˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Eleventh Annual Conference of the International\nSpeech Communication Association.\nJiaqi Mu, Suma Bhat, and Pramod Viswanath. 2017. Representing sentences as low-rank subspaces.\nIn ACL.\nBruno A Olshausen and David J Field. 1997. Sparse coding with an overcomplete basis set: A\nstrategy employed by v1? Vision research.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic\nevaluation of machine translation. In ACL.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difﬁculty of training recurrent\nneural networks. In ICML.\nMatthew E. Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised\nsequence tagging with bidirectional language models. In ACL.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke S. Zettlemoyer. 2018. Deep contextualized word representations. In NAACL-HLT.\nOﬁr Press and Lior Wolf. 2017. Using the output embedding to improve language models. In ACL.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. Unpublished ms. available through a link at https:\n//blog.openai.com/language-unsupervised/.\nSebastian Ruder and Jeremy Howard. 2018. Universal language model ﬁne-tuning for text classiﬁca-\ntion. In ACL.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words\nwith subword units. In ACL.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. Mass: Masked sequence to\nsequence pre-training for language generation. In ICML.\nAnuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and Adam Coates. 2018. Cold fusion: Training\nseq2seq models together with language models. In Interspeech.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks from overﬁtting. JMLR.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014. Sequence to sequence learning with neural\nnetworks. In NIPS.\nStephen Wright and Jorge Nocedal. 1999. Numerical optimization. Springer Science.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial\nnets with policy gradient. In AAAI.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer learning for low-resource\nneural machine translation. In EMNLP.\n11\n7 Appendix\nExact Match\nSmall Model (256d)\n Medium Model (512d)\n Large Model (1024d)\nBLEU\nPreﬁx Match\nSentence Length\n Sentence Length\n Sentence Length\nFigure 4: We plot the three recoverability metrics with respect to varying sentence lengths for each\nof our three model sizes in the 10M sentence setting. Within each plot, the curves correspond to\nthe varying dimensions of z. We include error regions of ±σ. Regardless of the divergence metric\n(BLEU, EM or PM), recoverability tends to improve as the size and quality of the language model\nimproves and as the dimension of the reparametrized sentence space increases, though we see weaker\noverall recoverablitiy than in the better-ﬁt 50M setting, and no cases of perfect recoverability for long\nsentences.\n12\nTable 3: Recoverability results when varying the dimension of the reparametrized sentence space |Z|\non exact match, BLEU, and preﬁx match. EM is the sample mean over each of the 100 sentences’\nsample EM. EM is the sample mean for a sentence over its 10 restarts. The same applies to BLEU.\nPM is the sample mean of preﬁx match medians. We also provide standard deviations.\n|Z| EM σ EM BLEU σ BLEU PM σ PM\nsmall; 10M 128 4.5 0.532 6.62 0.708 6.38 0.563\nsmall; 10M 256 9.0 1.060 14.1 0.627 12.3 0.854\nsmall; 10M 512 16.0 0.000 22.9 0.443 22.3 0.660\nsmall; 10M 1024 28.5 1.410 42.2 0.710 40.1 0.816\nsmall; 10M 2048 23.0 1.060 34.4 0.939 33.6 0.869\nsmall; 10M 4096 34.5 1.600 46.9 1.160 45.7 0.961\nsmall; 10M 8192 38.5 1.190 47.3 1.090 46.4 0.928\nsmall; 10M 16384 33.0 1.510 41.0 1.120 40.1 0.749\nsmall; 10M 32768 29.5 1.190 36.8 0.766 35.1 0.953\nsmall; 50M 128 8.0 0.753 10.2 0.574 8.79 0.689\nsmall; 50M 256 22.0 1.510 28.5 1.120 26. 1.190\nsmall; 50M 512 33.5 1.600 40.0 0.960 37.1 1.180\nsmall; 50M 1024 64.5 1.190 71.3 0.821 69.3 0.801\nsmall; 50M 2048 66.0 1.840 73.1 1.360 72.0 1.100\nsmall; 50M 4096 66.0 1.990 74.0 1.240 72.2 1.250\nsmall; 50M 8192 73.0 1.680 81.1 1.010 79.8 0.945\nsmall; 50M 16384 70.5 1.920 76.6 1.270 74.1 1.100\nsmall; 50M 32768 65.0 1.510 72.8 1.140 68.7 0.964\nmedium; 10M 128 6.5 0.532 9.26 0.619 7.56 0.634\nmedium; 10M 256 13.0 0.753 19.9 0.598 15.0 0.941\nmedium; 10M 512 28.0 1.060 35.0 0.993 30.3 0.975\nmedium; 10M 1024 39.5 0.922 45.6 0.623 42.3 0.200\nmedium; 10M 2048 71.0 1.060 76.6 0.660 75.9 0.874\nmedium; 10M 4096 67.0 1.840 75.4 1.150 73.0 1.090\nmedium; 10M 8192 71.5 1.600 79.0 0.813 77.1 1.050\nmedium; 10M 16384 66.5 2.060 74.6 1.030 72.5 1.010\nmedium; 10M 32768 67.0 1.680 76.1 0.812 71.2 0.920\nmedium; 50M 128 6.0 0.753 10.9 0.933 7.71 0.911\nmedium; 50M 256 24.5 0.922 28.0 0.742 26.6 0.661\nmedium; 50M 512 36.5 0.922 41.1 0.698 37.8 0.806\nmedium; 50M 1024 51.0 1.300 57.3 0.980 55.9 0.883\nmedium; 50M 2048 87.0 1.510 91.2 0.544 89.8 0.807\nmedium; 50M 4096 84.0 1.990 89.4 0.876 89.1 1.040\nmedium; 50M 8192 85.0 1.680 89.1 0.985 89.2 0.989\nmedium; 50M 16384 88.0 1.680 92.4 0.687 92.5 0.743\nmedium; 50M 32768 84.5 1.600 90.6 0.596 89.3 0.646\nlarge; 10M 128 7.0 0.753 11.8 0.741 7.65 0.542\nlarge; 10M 256 21.0 1.300 27.5 0.853 22.7 1.080\nlarge; 10M 512 42.0 1.060 46.3 0.794 43.2 1.140\nlarge; 10M 1024 58.0 1.510 62.1 1.170 59.9 1.340\nlarge; 10M 2048 67.0 0.000 68.2 0.213 67.4 0.045\nlarge; 10M 4096 95.0 1.300 97.6 0.577 97.3 0.529\nlarge; 10M 8192 90.0 1.510 93.7 0.609 93.5 0.664\nlarge; 10M 16384 88.5 1.770 92.4 0.704 92.6 0.587\nlarge; 10M 32768 90.5 1.920 95.3 0.942 95.7 0.821\nlarge; 50M 128 12.5 0.922 15.2 1.000 13.3 0.873\nlarge; 50M 256 29.0 1.300 32.7 1.200 30.4 1.060\nlarge; 50M 512 51.5 1.190 54.8 1.040 54.1 1.110\nlarge; 50M 1024 67.5 0.922 69.5 0.717 68.4 0.633\nlarge; 50M 2048 75.0 1.300 77.0 0.883 76.2 1.050\nlarge; 50M 4096 99.0 0.753 99.8 0.204 99.8 0.189\nlarge; 50M 8192 94.5 1.190 96.3 0.407 96.2 0.427\nlarge; 50M 16384 88.5 1.770 93.8 0.363 93.8 0.351\nlarge; 50M 32768 94.5 1.190 96.5 0.303 96.5 0.316\n13"
}