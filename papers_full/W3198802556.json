{
  "title": "On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets",
  "url": "https://openalex.org/W3198802556",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5071482462",
      "name": "Cheng-Han Chiang",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5040508737",
      "name": "Hung-yi Lee",
      "affiliations": [
        "National Taiwan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3091917188",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3092281475",
    "https://openalex.org/W3088325477",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3034617741",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W6765039553",
    "https://openalex.org/W3104570641",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W3105069964",
    "https://openalex.org/W3092185277",
    "https://openalex.org/W3101860695",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2932893307",
    "https://openalex.org/W4288379066",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W3098666169"
  ],
  "abstract": "Pre-training language models (LMs) on large-scale unlabeled text data makes the model much easier to achieve exceptional downstream performance than their counterparts directly trained on the downstream tasks. In this work, we study what specific traits in the pre-training data, other than the semantics, make a pre-trained LM superior to their counterparts trained from scratch on downstream tasks. We propose to use artificially constructed datasets as the pre-training data to exclude the effect of semantics, and further control what characteristics the pre-training corpora have. By fine-tuning the pre-trained models on GLUE benchmark, we can learn how beneficial it is to transfer the knowledge from the model trained on the dataset possessing that specific trait. We define and discuss three different characteristics in the artificial dataset: 1) matching the token's uni-gram or bi-gram distribution between pre-training and downstream fine-tuning, 2) the presence of the explicit dependencies among the tokens in a sequence, 3) the length of the implicit dependencies among the tokens in a sequence. Our experiments show that the explicit dependencies in the sequences of the pre-training data are critical to the downstream performance. Our results also reveal that models achieve better downstream performance when pre-trained on a dataset with a longer range of implicit dependencies. Based on our analysis, we find that models pre-trained with artificial datasets are prone to learn spurious correlation in downstream tasks. Our work reveals that even if the LMs are not pre-trained on natural language, they still gain transferability on certain human language downstream tasks once the LMs learn to model the token dependencies in the sequences. This result helps us understand the exceptional transferability of pre-trained LMs.",
  "full_text": "On the Transferability of Pre-trained Language Models:\nA Study from Artiﬁcial Datasets\nCheng-Han Chiang, Hung-yi Lee\nNational Taiwan University, Taiwan\ndcml0714@gmail.com, hungyilee@ntu.edu.tw\nAbstract\nPre-training language models (LMs) on large-scale unlabeled\ntext data makes the model much easier to achieve excep-\ntional downstream performance than their counterparts di-\nrectly trained on the downstream tasks. In this work, we study\nwhat speciﬁc traits in the pre-training data, other than the\nsemantics, make a pre-trained LM superior to their counter-\nparts trained from scratch on downstream tasks. We propose\nto use artiﬁcially constructed datasets as the pre-training data\nto exclude the effect of semantics, and further control what\ncharacteristics the pre-training corpora have. By ﬁne-tuning\nthe pre-trained models on GLUE benchmark, we can learn\nhow beneﬁcial it is to transfer the knowledge from the model\ntrained on the dataset possessing that speciﬁc trait. We de-\nﬁne and discuss three different characteristics in the artiﬁcial\ndataset: 1) matching the token’s uni-gram or bi-gram distribu-\ntion between pre-training and downstream ﬁne-tuning, 2) the\npresence of the explicit dependencies among the tokens in a\nsequence, 3) the length of the implicit dependencies among\nthe tokens in a sequence. Our experiments show that the ex-\nplicit dependencies in the sequences of the pre-training data\nare critical to the downstream performance. Our results also\nreveal that models achieve better downstream performance\nwhen pre-trained on a dataset with a longer range of implicit\ndependencies. Based on our analysis, we ﬁnd that models pre-\ntrained with artiﬁcial datasets are prone to learn spurious cor-\nrelation in downstream tasks. Our work reveals that even if\nthe LMs are not pre-trained on natural language, they still\ngain transferability on certain human language downstream\ntasks once the LMs learn to model the token dependencies in\nthe sequences. This result helps us understand the exceptional\ntransferability of pre-trained LMs.\n1 Introduction\nPre-training LMs by masked language modeling (MLM) is\nprevalent in the natural language processing (NLP) commu-\nnity, and they are indispensable to a variety of NLP tasks.\nThe popularity of pre-trained LMs mainly lies in their ex-\nceptional transferability on downstream tasks: ﬁne-tuning\nthese downstream-agnostic pre-trained models on miscella-\nneous downstream tasks often gives extraordinary perfor-\nmances compared with training from scratch. While the\nexact reasons for the success of MLM is unclear, some\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nhave attributed this success to the pre-trained models hav-\ning learned the semantic dependencies among tokens (Saun-\nshi, Malladi, and Arora 2020) and being able to model the\ncomplex co-occurrence statistics of the tokens (Sinha et al.\n2021). While these justiﬁcations are reasonable, it is unclear\nwhether there are other factors of the pre-training data that\naffect the transferability of the pre-trained LMs.\nThe core problem we determine to answer is: What spe-\nciﬁc traits in the pre-training data, other than the seman-\ntics, make a pre-trained LM more easier to achieve better\ndownstream performance. To answer the above question, we\ndesign the following experiments: we create miscellaneous\nartiﬁcial datasets, each possessing different traits, and we\npre-train many different transformer LMs on those datasets.\nWe then ﬁne-tune the pre-trained models on English down-\nstream tasks. The process is illustrated in Figure 1. Illustra-\ntions of the artiﬁcial datasets used in this paper are in Fig-\nure 2. This is the ﬁrst paper to study a transformer-based\nLM’s transferability through the lens of artiﬁcial datasets.\nBased on the experiments, we have the following take-\naway observations:\n• We ﬁnd that, surprisingly, pre-training MLMs on certain\nartiﬁcial datasets with no natural language semantics in-\nformation makes their downstream task performance su-\nperior to models pre-trained from scratch on downstream\ntasks.\n• We discover that pre-training on data with a longer range\nof both explicit and implicit token dependencies1 makes\nthem superior to their counterparts pre-trained on data\nwith shorter token dependencies. This indicates that the\nability to model long-term dependency among tokens in\nthe sequence is important for the transferability of the\npre-trained model.\n• We analyze the models’ behaviors against two challeng-\ning datasets and show that models pre-trained with artiﬁ-\ncial datasets are vulnerable toward spurious correlation.\n2 Related Work\nOur work uses artiﬁcial datasets to understand pre-trained\nLMs’ ability to be ﬁne-tuned. Bhattamishra, Ahuja, and\n1The deﬁnition of explicit token dependency and implicit token\ndependency will be given in Section 6 and Section 7, respectively.\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10518\nStage 1\nL1 MLM pre-train\nStage 2\nGLUE fine-tune\nStage 3\nGLUE Testing\n5\nLM Head Classifier Head\nIt\npositive\nTransformer\nmasking\n1 0 43\n1 0 435\nTransformer\n1 0 43 5\nis a good movie\nClassifier Head\nIt\nTransformer\n1 0 43 6\nis a bad movie\n?\nToken ID Input Token\n0 is\n1 it\n2 very\n3 good\n4 movie\n5 a\n6 bad\n7 ,\nFigure 1: The workﬂow in our studies. Stage 1: Pre-training the whole MLM on language 1 (L1), in which each sentence is a\nsequence of token ID generated by certain rules. Stage 2: Fine-tuning the whole model on English downstream tasks (GLUE\ntasks). Stage 3: Evaluating the performance of the ﬁne-tuned models on the English downstream tasks. In stages 2 and 3, the\nmodel takes English token sequences as input. To do that, each token embedding in the token embedding layer needs to be\nmapped to a token in English. For example, we map the ﬁrst token embedding in the embedding table, whose token ID is 0, to\nthe English token ”is”; and the second token embedding in the embedding layer, whose token ID is 1, to the English token ”it”.\nThe whole process, from stage 1 to stage 3, takes three days on a single V100 GPU.\nGoyal (2020) also use artiﬁcial dataset (formal language) to\nstudy a transformer model’s behavior, but their work only\ninvolves the transformer model’s ability to recognize certain\ntypes of formal languages.\nOur work may seem to resemble the Test for Inductive\nBias via Language Model Transfer (TILT) (Papadimitriou\nand Jurafsky 2020a) at ﬁrst sight, which trains a long short-\nterm memory (LSTM) LM on one language, which may\nbe non-natural language, followed by only ﬁne-tuning word\nembeddings on Spanish and test the perplexity on Spanish.\nIn fact, this work is very different from TILT. The main\npurpose of TILT is to analyze the encoding of grammati-\ncal structure in LSTM LMs, so they do not ﬁne-tune the\nLSTM on Spanish. The setting of TILT does not match\nthe common setting widely applied nowadays, in which we\nﬁne-tune pre-trained LMs on downstream tasks. Different\nfrom TILT, our goal is to understand what trait of the pre-\ntraining datasets makes the ﬁne-tuned model perform better\nthan models trained from scratch on downstream tasks, thus\nwe ﬁne-tune the transformer LMs on the downstream tasks.\nSinha et al. (2021) construct datasets from natural lan-\nguage corpora for pre-training by breaking the word order\nin a sentence. Our work can be seen as a complementary\nwork to theirs: Sinha et al. (2021) break the word order\nwhile preserving the ’local statistics’ of a sentence, thus pre-\nserving semantic features in the pre-training data to some\nextent. From their experiments, they show that purely distri-\nbutional information (local co-occurrences of words) largely\nexplains the success of MLMs. Contrarily, we discard the se-\nmantics in the pre-training data while equipping the model\nwith the ability to model token dependencies during pre-\ntraining. In our experiments, we illustrate that part of the\nsuccess of MLMs can be attributed to their ability to model\ntoken dependencies within a sequence.\n3 Analyzing LM’s Transferability by\nArtiﬁcial Data\nThe core idea of our experiment is to use different artiﬁcial\ndatasets that have different traits to understand how a spe-\nciﬁc trait in the pre-training data affects the downstream per-\nformance. In our experiments, we pre-trainn RoBERTa (Liu\net al. 2019) models on n different types of pre-training data.\nDue to our constrained resources, we use RoBERTa-medium\nin our experiments; we will refer to RoBERTa-medium as\nRoBERTa in our paper. We call the pre-training data L1 (ﬁrst\nlanguage). The number of tokens in the pre-training cor-\npora of all our L1s is around 100 million.While the dataset\nis small, Micheli, d’Hoffschmidt, and Fleuret (2020) have\nshown that this amount of data is sufﬁcient to pre-train a\ncompact language model.\nWe then evaluate the pre-trained models’ ability by ﬁne-\ntuning them on different downstream tasks. We adopt the\nGLUE (Wang et al. 2019; Socher et al. 2013; Dolan and\nBrockett 2005; Cer et al. 2017; Williams, Nangia, and Bow-\nman 2018; Rajpurkar et al. 2016) benchmarks to evalu-\nate the models pre-trained on different L1s. We exclude\nWNLI, RTE, and CoLA for their notoriously unstable per-\nformance (Devlin et al. 2019; Dodge et al. 2020). We use a\nspeciﬁc set of hyperparameters and three different random\nseeds to ﬁne-tune the model for each task. We report the\naverage and standard deviation over different seeds of the\nresults on the evaluation set. The overall workﬂow is illus-\ntrated in Figure 1. Details regarding all experiments can be\nfound in Appendix A.\n3.1 Artiﬁcial Datasets\nWe construct different artiﬁcial datasets as L1 in this paper.\nEach artiﬁcial dataset is constructed based on certain rules,\nwhich can be deemed as the grammar governing the artiﬁ-\n10519\ncial dataset. Detailed construction and illustration of artiﬁ-\ncial datasets can be found in Section 5 to 7 and Figure 2.\nWe design different grammar to scrutinize how the down-\nstream performance may vary due to the characteristic of\nthe pre-training data. The vocabulary 2 of artiﬁcial datasets\ncontains integers ranging from 0 to 29994 and 5 special to-\nkens. We choose to use 29995 tokens since the vocabulary\nsize for our downstream English tasks is 29995 plus 5 spe-\ncial tokens. We will use the word ’token’, ’token ID’, and\n’integer’ interchangeably to refer to the token for the artiﬁ-\ncial datasets. The sequence length of each sequence in the\nartiﬁcial pre-training data is determined by uniformly sam-\npling between 100 and 120 (128 is the max sequence length\nthe model we trained can process, and this is sufﬁcient for\nGLUE downstream tasks in most of the cases).\n3.2 Vocabulary during Pre-training and\nFine-tuning\nThe vocabulary for ﬁne-tuning the English downstream\ntasks is obtained by Byte Pair Encoding (BPE), the vocab-\nulary size is 30K, including ﬁve special tokens. Since the\nvocabulary used during pre-training has no overlap with that\nduring ﬁne-tuning, the model cannot transfer any semantic\nknowledge from the pre-trained models. The token embed-\nding that represents an integer during pre-training will be\nused to represent a different and unrelated token in English.\n4 Baseline Models\nIn this paper, we use three different baseline performances\nto benchmark the performances pre-trained on different L1s.\nEnglish: We pre-train a RoBERTa-medium using a subset\nof English Wikipedia. The GLUE score obtained by ﬁne-\ntuning this model serves as the performance upper bound\nfor other models in our paper.\nKannada: We pre-train a RoBERTa-medium using Kan-\nnada from OSCAR dataset (Su ´arez, Romary, and Sagot\n2020). Kannada is a language spoken by the people in the\nsouthwestern region of India. The main reason we choose\nthis dataset lies in its subject(S)-object(O)-verb(V) structure,\ndifferent from the S-V-O structure of our target language\nused in ﬁne-tuning. This model helps us understand how En-\nglish downstream performance can beneﬁt from pre-training\non a non-English human language.\nTraining from scratch: We train transformer models with\nthe same architecture of RoBERTa-medium directly on\ndownstream GLUE tasks without pre-training. This baseline\nperformance, compared with other pre-trained models, helps\nus understand how effective pre-training is.\nWe show the performance of baseline models in the ﬁrst\nand second block of Table 1. Without any surprise, the model\npre-trained on English performs the best, and the model\ntrained from scratch on GLUE performs the worst, while\nthe performance of the model transferred from Kannada is\nin between the other two baseline models.\n2V ocabulary is the set of tokens used by the language model,\nincluding special tokens such as [MASK ] and [CLS].\n5 Characteristic 1: Matching the\nDistribution between Pre-training and\nFine-tuning\nThe ﬁrst characteristic we examine is the token distribution\nof the pre-training data. We construct three artiﬁcial datasets\nwith different token distributions to understand how the to-\nken distributions for pre-training affect downstream GLUE\nperformance after ﬁne-tuning.\n5.1 Datasets\nUniform The ﬁrst artiﬁcial dataset is constructed by sam-\npling 29995 integers from the vocabulary based on the uni-\nform distribution to form a sequence. The model learns noth-\ning but randomly picks a token during pre-training.\nUni-gram The second dataset, called Uni-gram, is de-\nsigned to match the uni-gram token distribution of the real\nEnglish token distribution. This dataset is constructed by\ncalculating the token distribution of the English Wikipedia\ndataset we used as the baseline, follows by sampling to-\nkens based on that distribution to form sequences of vari-\nable lengths. Being able to perform MLM pre-task over this\ndataset indicates the model learns to model the downstream\ntasks’ token distribution.\nBi-gram The third dataset is constructed such that the bi-\ngram distribution of this dataset matches that of the En-\nglish Wikipedia corpus. This dataset is constructed by sam-\npling tokens based on the bi-gram distribution of the English\nWikipedia subset.\n5.2 Results\nThe results are in the third block of Table 1. We see that\nthe model pre-trained with uniform token distribution per-\nforms the worst, which is as bad as the models trained from\nscratch. The other two models that have learned the down-\nstream tokens’ distribution perform marginally better, while\nstill falling far behind the two baseline models trained on hu-\nman languages. This implies that only modeling the down-\nstream task’s uni-gram distribution or bi-gram distribution\nis not enough to make a pre-trained language model able to\nperform well on the downstream tasks.\n6 Characteristic 2: Explicit Token\nDependencies in a Sequence\nIn this section, we intend to focus on the explicit depen-\ndencies between tokens in the sequences of the pre-training\ndata. In this paper, we say there exists anexplicit dependency\nbetween two tokens xi and xj if knowing one of the two to-\nkens can tell us what the other token is. Explicit token depen-\ndency is rich in human languages, such as the subject-verb\nagreement: if the verb is in singular form, then the subject\nmust be singular. We ask whether the extraordinary down-\nstream performance of a pre-trained LM springs from its\nskillfulness in modeling the explicit dependencies among to-\nkens in a sequence. To this end, we construct datasets with\nexplicit token dependencies.\n10520\nL1 STS-B QNLI QQP SST-2 MNLI MRPC Avg\nScratch 18.6 (1.4) 62.1 (0.5) 77.6 (0.2) 82.2 (0.5) 62.1 (0.5) 70.6 (3.5) 61.6\nPre. En +65.1 (1.0) +22.4 (1.0) +7.0(0.2) +4.3 (0.3) +12.7 (0.5) +11.3 (0.5) +20.5\nPre. Ka +55.0 (1.5) +14.7 (0.5) +4.2 (0.1) +0.3 (0.7) +5.6 (0.0) +8.5 (0.4) +14.7\nUniform -1.3 (0.1) -1.7 (0.6) -0.1 (0.2) -0.3 (0.4) +0.9 (0.2) +1.4 (0.8) -0.2\nUni-gram +2.9 (1.6) -0.6 (0.5) +0.5 (0.2) -1.0 (0.3) +0.9 (0.1) +5.1 (1.1) +1.3\nBi-gram +5.0 (1.7) +0.0(0.3) -0.4 (0.1) -0.5 (0.4) +1.4 (0.3) +7.7 (0.9) +2.2\nFlat-2 +19.9 (6.0) +13.7 (0.1) +0.8 (0.1) -2.7 (0.1) +3.2 (0.1) +8.0 (1.4) +7.2\nFlat-4 +49.4(4.2) +16.2 (0.4) +1.5 (0.1) -0.9 (1.4) +4.7 (0.4) -6.6 (1.5) +10.7\nFlat-6 +55.3(0.0) +16.3 (0.3) +2.3 (0.2) +0.3 (0.9) +5.9 (0.3) +1.0 (2.2) +13.5\nFlat-128 +58.5 (0.8) +16.0 (0.4) +2.2 (0.0) -1.1 (0.2) +4.1 (0.3) +7.5 (0.8) +14.5\nNest Par. +43.4 (4.3) +17.3 (0.2) +3.3 (0.3) -1.1 (0.7) +6.3 (0.3) +3.2 (0.9) +12.0\nShuff.-64 +49.7 (0.3) +13.5 (0.3) +1.5 (0.2) -1.7 (0.7) +3.3 (0.5) +8.4 (0.3) +12.4\nShuff.-32 +44.7 (0.2) +14.2 (0.7) +1.3 (0.2) -0.9 (0.5) +3.4 (0.1) +8.5 (1.3) +11.9\nShuff.-16 +34.0 (0.2) +14.6 (0.2) +2.1 (0.1) -3.1 (0.4) +3.7 (0.4) +7.2 (1.4) +9.8\nShuff.-8 +19.8 (7.8) +13.0 (0.2) +2.5 (0.2) -0.0 (0.6) +4.0 (0.2) +5.8 (0.9) +7.5\nShuff.-6 +10.3 (0.4) +13.8 (0.0) +1.9 (0.1) -1.2 (0.2) +4.3 (0.1) +9.3 (1.3) +6.4\nShuff.-4 +8.5 (2.3) +9.0 (1.6) +1.5 (0.1) -0.4 (0.1) +2.3 (1.0) +7.1 (0.2) +4.7\nTable 1: Downstream results of models in Section 4 to Section 7. We report the performance training from scratch on GLUE\nin the ﬁrst block, and we report the relative improvement over the trained from scratch models for all other artiﬁcial datasets.\nThe value in the parentheses is the standard deviation. The evaluation metrics of MRPC and QQP are F1 score, Spearman\ncorrelation coefﬁcient is reported for STS-B, and the rest tasks are evaluated with accuracy. Pre. is the abbreviation of pre-\ntraining, En stands for English, Ka stands for Kannada, Par. is short for Parentheses, and Shuff. is short for Shufﬂe.\n6.1 Data\nFlat Parentheses The ﬁrst dataset that contains explicit\ntoken dependencies is called Flat Parentheses, and the rea-\nson for its name will be clear later. We construct this dataset\nby ﬁrst determining a half sequence length T=2, and sample\nT=2 (not necessarily consecutive) integers from English’s\nuni-gram token distribution to form a sequence. We dupli-\ncate each token in the sampled sequence and then shufﬂe\nthe sequence to form our ﬁnal data with lengthT. Visualiza-\ntion can be found in Figure 2a. Each integer in the generated\nsequence will occur even times, so we can view the same\nintegers in the sequence as a pair of depending integers. We\ncall the distance between a pair of depending integers in the\nsequence the length of dependency.\nTo understand how the length of dependency in the\npre-training data affects the ﬁne-tuned downstream perfor-\nmance, we construct different Flat Parentheses datasets that\nhave a different maximum length of dependency. We use\nFlat Parentheses-L to denotes the Flat Parentheses dataset\nthat has a maximum dependency length of L.\nNesting Parentheses In the Flat Parenthesesdataset, if we\nconnect each pair of depending integers with an arc as in\nFigure 2a, we can see that these arcs might cross with each\nother. A special case ofFlat Parenthesesis when all the arcs\nconnecting all pairs of depending integers do not cross with\neach other, forming a hierarchical (nesting) structure, as in\nFigure 2b. This dataset is called Nesting Parentheses in Pa-\npadimitriou and Jurafsky (2020a). We follow Papadimitriou\nand Jurafsky (2020b) to generate this dataset by a stack-\nbased grammar; the detailed procedure is in the Appendix\nB.Figure 2b shows a simple example. We can observe from\nFigure 2b that a sequence generated in this manner contains\n(a) Flat Parentheses (Section 6)\n(b) Nesting Parentheses (Section 6)\n(c) Shufﬂe-4 (Section 7)\nFigure 2: Illustration of the artiﬁcial datasets in Section 6\nand 7. The arcs in 2a, 2b and blocks in 2c are showed here\nfor easier understanding.\na nesting hierarchical parentheses structure, which is similar\nto the dependency tree structure in natural language.\n6.2 Results\nThe results for the Parentheses datasets are presented in the\nforth block of Table 1. Comparing with the results in the\nﬁrst and third block of Table 1, it is clear that the Parenthe-\nses datasets result in far better average GLUE scores than\ntraining from scratch on downstream tasks, or transferring\nfrom randomly generated sequences in Section 5. It is in-\nteresting to note that the Flat Parentheses dataset is quite\nsimilar to the Uni-gram dataset in Section 5 since tokens in\nboth datasets are sampled from the same uni-gram distribu-\n10521\ntion. The only difference is that the occurrence of each token\nin Flat Parentheses must be even. While bearing signiﬁcant\nsimilarity, their performance diverges largely. This indicates\nthat learning to model the explicit token dependencies be-\ntween tokens is critical to how well the pre-trained model\ncan perform on downstream tasks. Another astonishing ob-\nservation is that pre-training on both Parentheses datasets\nyields GLUE scores comparable to the model trained on an-\nother human language, Kannada.\nThe results in the fourth block show that the RoBERTa\nmodels trained on both Nesting Parenthesesand Flat Paren-\ntheses have similar performances on downstream tasks.\nWhile we expect the model pre-trained onNesting Parenthe-\nses to learn to model hierarchical structure among tokens,\nthe downstream performance does not surpass the model\ntrained on Flat Parentheses, which does not learn the hierar-\nchical structure in pre-training. The result implies that being\nskilled at modeling the hierarchical structure among tokens\nmight not be critical to a pre-trained LM’s transferability.\nComparing the results of Flat Parentheses-N, we observe\nthat when N = 2, their performance already outperforms\nthat of the baselines trained on Uni-gram or Bi-gram. While\nFlat Parentheses-2degenerates to only repeating each token\ntwice, pre-training on this trivial dataset still makes the pre-\ntrained model performs better than other baseline models.\nWhile Flat Parentheses-2 already outperforms models pre-\ntrained on Uni-gram and Bi-gram, its performance still falls\nbehind other Flat Parentheses datasets with longer depen-\ndency length. This indicates that the knowledge transferred\nfrom datasets with explicit token dependencies is beneﬁcial\nto English downstream tasks, and the longer dependency\nlength the pre-training data has, the better downstream per-\nformance can be acquired.\n7 Characteristic 3: Implicit Token\nDependencies in a Sequence\nIn this section, we focus on theimplicit dependencies among\ntokens presented in pre-training data. In natural languages,\na token in a sequence may implicitly depend on multiple\nneighboring tokens in the sequence. For example, consider\nthe following sentence: I can’t believe I spent two hours\nwatching that [MASK] movie. We might expect the[MASK]\nto be a negative sentimental word, and we make this infer-\nence based on all the tokens in the sentence. The dependency\nthat a token depends on a set of neighboring tokens instead\nof a speciﬁc token is called implicit dependency in this pa-\nper. In terms of downstream performance, we want to know\nhow important it is for a pre-trained LM to learn the implicit\ntoken dependencies among tokens in the sequence. We are\nalso interested in how the length of this implicit dependency\naffects downstream performance. We design a dataset called\nShufﬂe-N to answer the previous questions.\n7.1 Shufﬂe-N\nWe explain how to construct this dataset, while the readers\ncan refer to the example of Shufﬂe-4 in Figure 2c. Each se-\nquence in this dataset is formed by concatenating blocks of\nN integers. In each block, we sample N consecutive num-\nbers from 0 to 29994, and we shufﬂe the sampled integers to\nform a block. Integers in different blocks are sampled inde-\npendently. To solve the MLM task on this dataset, the model\nonly needs to focus on a context that is no more than2N −1\ntokens (the previous N −1 tokens and the next N −1 to-\nkens). The maximum N we used in our experiments is 64,\nsince the maximum sequence length in our model is128. By\nvarying N, we can examine how the length of the implicit\ndependency in the pre-training data affects the downstream\nperformance.\nNote that we do not require that each block to have non-\noverlapping integers when constructing the dataset. Chances\nare that the same integer may occur in the same sequence\nmultiple times, forming an explicit dependency. However,\nwe ﬁnd that the generated datasets contain few explicit token\ndependencies, so learning this explicit token dependency\ndoes not help the model perform MLM. We also carry out\nfurther analysis on Section 8 to show that the models learned\nfrom this dataset indeed focus on the 2N neighborhoods.\nThis dataset is designed to capture the constituent struc-\nture of human language, and different N can capture the vari-\nable length that a constituent may span in human language.\nSince words in the same constituent have some implicit de-\npendency, we hypothesize that LMs trained on human lan-\nguage will learn how to model the dependency among the\ntokens in it, leading to the superior transferability of natural\nlanguage trained LMs. In Shufﬂe-N, the ‘block’ corresponds\nto the ‘constituent’ in human language, and those tokens in\nthe block correspond to words in a constituent.\n7.2 Results\nThe results for Shufﬂe-N is presented in the ﬁfth block in Ta-\nble 1. We ﬁnd that all models pre-trained on Shufﬂe-N yield\nbetter downstream performance than the Uni-gram and Bi-\ngram, showing that learning to model implicit dependency\namong tokens during pre-training is critical for the model to\nperform well on downstream tasks. Comparing the perfor-\nmance of Shufﬂe with different N, we observe that averaged\nperformance increases as N increases. Most GLUE tasks\nshow improvement when N increases from 4 to 6, while\nSTS-B’s performance consistently goes up as N increases\nto 64. While we yet to know the reason behind the positive\ncorrelation between the downstream STS-B’s performance\nand the pre-train implicit dependency length N, it is still\nsurprising to see that only varying a speciﬁc characteristic\nof the pre-training data can make such a great difference on\na downstream task’s performance.\n8 Analysis: How Does Different Pre-training\nArtiﬁcial Datasets Affect the Models’\nBehavior on Human Language?\nIn the previous three sections, we use different artiﬁcial\ndatasets to understand how a speciﬁc trait in the pre-training\ndata affects the downstream performance. While we antici-\npate that the model pre-trained with an artiﬁcial dataset with\na certain characteristic will learn speciﬁc ways to process the\nartiﬁcial input data during pre-training, it is unclear whether\nthe model will have a similar behavior when processing a\n10522\n0\n100\n200\n300\n400\nBi-gram\n0\n50\n100\n150\n200\nShuffle-4\n0\n25\n50\n75\n100\n125\n150\nNesting Parentheses\n0\n50\n100\n150\n200\n250\nFlat Parentheses-4\n40\n 30\n 20\n 10\n 0 10 20 30 40\n0\n25\n50\n75\n100\n125\n150\nFlat Parentheses-6\nFigure 3: The distribution of j\u0003. The x-axis is the relative\nposition with respect to the ⌊T=2⌋-th token in the input se-\nquence. The height of the histogram when x-axis value is j\nrepresents the number of count thatj = j\u0003among sentences\nin SQuAD.\nhuman language. In this section, we design an experiment\nto analyze how the models pre-trained on artiﬁcial datasets\nbehave when faced with human language.\n8.1 Method\nDuring MLM pre-training, the model needs to reconstruct\na masked token based on the remaining tokens in the se-\nquence. Given an input sequence X with T tokens, let xi\ndenotes the i-th token in X, and let Xnfi;jgdenotes the se-\nquence X with xi and xj being masked. We would like to\nknow how the presence of xi affects how well the model\ncan predict xj. To that end, we compare the LM’s prediction\nat the j-th position when the input sequence is Xnfjg and\nXnfi;jg. Since the LM’s prediction is a probability distribu-\ntion over the set of all tokens in the vocabulary, we can cal-\nculate the entropy of the model’s prediction. We use Pnfjg\nand Pnfi;jgto denote the LM’s prediction at the j-th position\nwhen the input sequence is Xnfjgand Xnfi;jg, and denote\nthe entropy of a distribution P as H(P). If knowing xi is\nimportant for the prediction of xj, then H(Pnfjg) should be\nlower than H(Pnfi;jg) since knowing xi will reduce the un-\ncertainty for predicting xj.\nIn our experiment, given a sequence X, we choose the\n⌊T=2⌋-th token in the input sequence asxi and iterate over\nall the tokens in the sequence as xj to ﬁnd which position j\nis most related to the ⌊T=2⌋-th token as below.\nj\u0003= arg max\nj:xj 2X\nH(PnfbT=2c;j g) −H(Pnfjg): (1)\nThen we ﬁnd the j\u0003for each input sentence and accumulate\nthe distribution of j\u0003for all sentences in a dataset.\n8.2 Result\nThe sentences used for computing the distribution ofj\u0003here\nare from SQuAD (Rajpurkar et al. 2016)3. The results are in\nFigure 3. First, we immediately ﬁnd that the j\u0003distribution\nof Bi-gram peaks around the center. This is expected since\nduring pre-training, the model trained on Bi-gram learns to\nonly focus on a very small context. We can observe that both\nthe distributions ofFlat Parentheses-6and Flat Parentheses-\n4 center near the ⌊T=2⌋-th token, which is the expected be-\nhavior. Furthermore, the distribution of Flat Parentheses-4\nis more condensed around the ⌊T=2⌋-th token than that of\nFlat Parentheses-6, showing that pre-training on the former\ndataset makes the model focus on a smaller context. Another\ninteresting observation is that the distribution ofj\u0003for Nest-\ning Parentheses shows a high-low-high-low pattern around\nthe ⌊T=2⌋-th token, this phenomenon springs from the na-\nture of the Nesting Parentheses dataset. The distribution of\nj\u0003 for Shufﬂe 4 is also focused within the previous 3 to-\nkens and the next 3 tokens of the ⌊T=2⌋-th token, which\nalso results from the nature of the dataset the model is pre-\ntrained on. We thus verify that what we expect the model to\nlearn from the artiﬁcial dataset can be transferred to English\ndownstream tasks.\n9 How Robust are Models Pre-trained with\nDifferent Datasets?\nWe have evaluated the pre-trained models trained with ar-\ntiﬁcial datasets of different traits, and the results show that\nsome models are as good as the models pre-trained from a\nhuman language in terms of GLUE scores. Are these models\nas robust as the models pre-trained from human language?\nChances are the GLUE scores on the evaluation set are de-\ncent, but the model only learns to ﬁt some spurious correla-\ntion of the GLUE tasks, making the model less robust. Here\nwe use two challenging datasets to assess the robustness of\nthe ﬁne-tuned models pre-trained on different datasets.\n9.1 Experiment Setup\nWe use RoBERTa models pre-trained on different L1s, ﬁne-\ntuning them using the original GLUE training set, and test\nthe ﬁne-tuned model on a more challenging evaluation set.\nWe use two GLUE tasks, QQP and QNLI, and their corre-\nsponding challenging datasets, QQPPAWS and QNLI-Adv.\n3We choose SQuAD instead of GLUE since sentences in\nSQuAD are longer.\n10523\nTasks Pre-train En From Scratch Pre-train Ka Bi-gram Nest.Par. Shufﬂe-4\nQQP 84.6 77.6 81.8 77.2 80.9 79.1\nQQPPA WS 43.0 43.9 44.0 42.7 43.0 42.8\nQNLI 84.5 62.1 76.8 59.8 79.4 71.1\nQNLI-Adv 64.8 0.1 8.6 53.8 1.0 1.1\nTable 2: Evaluation performance of models ﬁne-tuned on QQP and QNLI. The evaluation metrics are the same as Table 1.\nQQPPA WS The Quora Question Pairs (QQP) (Iyer, Dan-\ndekar, and Csernai 2017) contains real-world question pairs\ncollected from Quora. While this dataset is widely used\nto train and evaluate a paraphrase model, it is shown that\nparaphrase pairs in QQP tend to contain high lexical over-\nlap. This makes us hard to determine whether a model re-\nally learns how to distinguish between paraphrases and non-\nparaphrases, or it just learns the spurious correlation that\nsimilar bag-of-word implies paraphrase.\nQQPPAWS is proposed by Zhang, Baldridge, and He\n(2019) to solve the above problem. Zhang, Baldridge, and\nHe (2019) use word swapping and back translation along\nwith human annotation to create a high-quality paraphrase\ndataset. In QQPPAWS, two questions with high lexical over-\nlap do not often imply they are a pair of paraphrases.\nQNLI-Adv In the original Question-answering NLI\n(QNLI), given a question-answer pair, the model needs to\ndetermine whether the answer corresponds to the question,\ni.e., entailment. To see whether the model only learns to la-\nbel question-answer pairs with high lexical overlap as entail-\nment, we propose a challenging evaluation dataset from the\noriginal QNLI evaluation split. We call this dataset QNLI-\nAdv. The construction of QNLI-Adv is extremely simple: in-\nstead of feeding the model a pair of question and answer, we\ngive the model two identical questions, and the model has\nto answer non-entailment to this input. That is, the ground-\ntruth labels of data in QNLI-Adv are all non-entailment.\n9.2 Result\nFrom Table 2, we observe that all pre-trained models are\nequally vulnerable to QQP PAWS. This may result from the\nfact that QQP PAWS is deliberately constructed to be a chal-\nlenging dataset and has a low discrimination index.\nOn the other hand, when evaluated with QNLI-Adv, the\nmodels behave differently. The model pre-trained on En-\nglish is the most robust one, with the accuracy only drop-\nping 20% comparing to the original QNLI evaluation set.\nWe ﬁnd that both Nesting Parentheses and Shufﬂe-4 per-\nforms unbelievably poorly, showing that while learning to\nmodel the explicit and implicit token dependencies enables\nthe model to perform well on downstream tasks, it is vul-\nnerable to spurious correlations in the downstream tasks.\nThis vulnerability against spurious correlation can not only\nbe observed on model pre-trained on artiﬁcial datasets; even\nthe model pre-trained on Kannada, a human language, per-\nforms poorly on QNLI-Adv, indicating that not learning the\nsemantics of the downstream language will make the model\nless robust toward challenging datasets. The unreasonably\nhigh robustness of Bi-gram may be attributed to the fact that\nthe performance of Bi-gram on the original evaluation set\nis not much better than random guessing among entailment\nand non-entailment, so randomly guessing will result in ac-\ncuracy around 50%.\n10 Discussion and Conclusion\nIn this work, we study what traits besides semantics in\nthe pre-training data make the pre-trained LMs able to\nyield exceptional downstream performance. We propose to\nstudy this problem with the aid of artiﬁcial datasets. The\nframework is general, and thus if one would like to study\nwhether other characteristics will affect the transferability\nof transformer-based LM, they can adopt this framework.\nSpeciﬁcally, we construct linguistic-inspired artiﬁcial\ndatasets, and ﬁnding that pre-training on certain artiﬁcial\ndatasets makes the MLMs’ English downstream perfor-\nmance comparable to transferring from an MLM pre-trained\non an non-English human language. We show that both the\nexplicit and implicit dependencies between tokens in the se-\nquences are critical to the transferability of the pre-trained\nmodel. The result in Tabel 1 indicates that pre-training on\nartiﬁcial datasets with explicit/implicit token dependencies\nmakes the pre-trained LMs superior to the from-scratch and\nUni/Bi-gram baselines.\nThe downstream performance of LMs pre-trained on\ndatasets with explicit/implicit token dependencies still falls\nbehind models pre-trained with English. This performance\ngap is expected: when using artiﬁcial datasets to pre-train an\nLM, the LM cannot learn any semantic features useful for\ndownstream tasks. We also carry out experiments to test the\nmodels’ behavior when faced with challenging datasets and\nshowing that models pre-trained without English are more\nprone to learn spurious correlation of the downstream tasks.\nOverall, our results contribute to an important problem in\nthe NLP community: where does the transferability of pre-\ntrained transformers arise from? While one may infer that\ntransformers pre-trained on natural language can model to-\nken dependencies in the sequences, it is unclear how much\nthis contributes to the transferability of the pre-trained trans-\nformer LMs. We disentangle the effect of semantic simi-\nlarity during pre-training and downstream ﬁne-tuning. We\nshow that even when the pre-training data and downstream\ntasks share no semantic features, the transformer LMs pos-\nsess positive transferability to natural language downstream\ntasks if it has the ability to model the token dependencies in\nthe sequences. We attribute the transferability of pre-trained\ntransformer LMs to their capability of modeling the depen-\ndencies among tokens, and we envision that the results may\nhelp researchers in different disciplines to apply transformer\npre-trained models to their domains of interests.\n10524\nAcknowledgements\nWe want to thank Wei-Tsung Kao, Yist Y . Lin, and Yung-\nSung Chuang for their valuable feedbacks on the draft of\nour paper. We also want to thank the anonymous review-\ners for providing insightful and actionable suggestions on\nour work. We thank National Center for High-performance\nComputing (NCHC) of National Applied Research Labo-\nratories (NARLabs) in Taiwan for providing computational\nand storage resources.\nReferences\nBhattamishra, S.; Ahuja, K.; and Goyal, N. 2020. On\nthe Ability and Limitations of Transformers to Recognize\nFormal Languages. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Process-\ning (EMNLP), 7096–7116. Online: Association for Compu-\ntational Linguistics.\nCer, D.; Diab, M.; Agirre, E.; Lopez-Gazpio, I.; and Specia,\nL. 2017. SemEval-2017 Task 1: Semantic Textual Similar-\nity Multilingual and Crosslingual Focused Evaluation. In\nProceedings of the 11th International Workshop on Seman-\ntic Evaluation (SemEval-2017), 1–14. Vancouver, Canada:\nAssociation for Computational Linguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186.\nDodge, J.; Ilharco, G.; Schwartz, R.; Farhadi, A.; Hajishirzi,\nH.; and Smith, N. 2020. Fine-tuning pretrained language\nmodels: Weight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nDolan, W. B.; and Brockett, C. 2005. Automatically Con-\nstructing a Corpus of Sentential Paraphrases. In Proceed-\nings of the Third International Workshop on Paraphrasing\n(IWP2005).\nIyer, S.; Dandekar, N.; and Csernai, K. 2017. First quora\ndataset release: Question pairs. https://dl.fbaipublicﬁles.\ncom/glue/data/QQP-clean.zip. Accessed: 2021-04-01.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv preprint arXiv:1907.11692.\nMicheli, V .; d’Hoffschmidt, M.; and Fleuret, F. 2020. On\nthe Importance of Pre-training Data V olume for Compact\nLanguage Models. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing\n(EMNLP), 7853–7858.\nPapadimitriou, I.; and Jurafsky, D. 2020a. Learning Mu-\nsic Helps You Read: Using Transfer to Study Linguistic\nStructure in Language Models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language\nProcessing (EMNLP), 6829–6839. Online: Association for\nComputational Linguistics.\nPapadimitriou, I.; and Jurafsky, D. 2020b. Learning Music\nHelps You Read: Using Transfer to Study Linguistic Struc-\nture in Language Models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), 6829–6839.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension\nof Text. In Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, 2383–2392.\nSaunshi, N.; Malladi, S.; and Arora, S. 2020. A Mathe-\nmatical Exploration of Why Language Models Help Solve\nDownstream Tasks. arXiv preprint arXiv:2010.03648.\nSinha, K.; Jia, R.; Hupkes, D.; Pineau, J.; Williams, A.; and\nKiela, D. 2021. Masked language modeling and the distribu-\ntional hypothesis: Order word matters pre-training for little.\narXiv preprint arXiv:2104.06644.\nSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,\nC. D.; Ng, A.; and Potts, C. 2013. Recursive Deep Models\nfor Semantic Compositionality Over a Sentiment Treebank.\nIn Proceedings of the 2013 Conference on Empirical Meth-\nods in Natural Language Processing, 1631–1642. Seattle,\nWashington, USA: Association for Computational Linguis-\ntics.\nSu´arez, P. J. O.; Romary, L.; and Sagot, B. 2020. A Mono-\nlingual Approach to Contextualized Word Embeddings for\nMid-Resource Languages. In ACL 2020-58th Annual Meet-\ning of the Association for Computational Linguistics.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019. GLUE: A Multi-Task Benchmark and\nAnalysis Platform for Natural Language Understanding. In\nthe Proceedings of ICLR.\nWilliams, A.; Nangia, N.; and Bowman, S. 2018. A Broad-\nCoverage Challenge Corpus for Sentence Understanding\nthrough Inference. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), 1112–1122. New Orleans, Louisiana:\nAssociation for Computational Linguistics.\nZhang, Y .; Baldridge, J.; and He, L. 2019. PAWS: Para-\nphrase Adversaries from Word Scrambling. In Proceed-\nings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers),\n1298–1308.\n10525",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8177686929702759
    },
    {
      "name": "Spurious relationship",
      "score": 0.5989996194839478
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5631954669952393
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.5517376661300659
    },
    {
      "name": "Language model",
      "score": 0.5481035709381104
    },
    {
      "name": "Natural language processing",
      "score": 0.47979527711868286
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.43882817029953003
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4370187819004059
    },
    {
      "name": "Machine learning",
      "score": 0.42743220925331116
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4234717786312103
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}