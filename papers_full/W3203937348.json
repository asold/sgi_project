{
  "title": "The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension",
  "url": "https://openalex.org/W3203937348",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2072350497",
      "name": "Samuel A. Nastase",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2638668115",
      "name": "Yun Fei Liu",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2969858885",
      "name": "Hanna Hillman",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2553178210",
      "name": "Asieh Zadbood",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2329653257",
      "name": "Liat Hasenfratz",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A1521226346",
      "name": "Neggin Keshavarzian",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2125200079",
      "name": "Janice Chen",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2048193252",
      "name": "Christopher J. Honey",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2017734865",
      "name": "Yaara Yeshurun",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2262741468",
      "name": "Mor Regev",
      "affiliations": [
        "Montreal Neurological Institute and Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2096366472",
      "name": "Mai Nguyen",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2719936066",
      "name": "Claire H.C. Chang",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A1968434930",
      "name": "Christopher Baldassano",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2548631588",
      "name": "Olga Lositsky",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A2041556944",
      "name": "erez simony",
      "affiliations": [
        "Weizmann Institute of Science",
        "Holon Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4260555744",
      "name": "Michael A. Chow",
      "affiliations": [
        "Data & Society Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2106136219",
      "name": "Yuan Chang Leong",
      "affiliations": [
        "Neurosciences Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2887040553",
      "name": "Paula P. Brooks",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A3022335299",
      "name": "Emily Micciche",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2239913410",
      "name": "Gina Choe",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2285538401",
      "name": "Ariel Goldstein",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2084465013",
      "name": "Tamara Vanderwal",
      "affiliations": [
        "British Columbia Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2308590913",
      "name": "Yaroslav O. Halchenko",
      "affiliations": [
        "Dartmouth College"
      ]
    },
    {
      "id": "https://openalex.org/A2148191515",
      "name": "Kenneth A. Norman",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2182450596",
      "name": "Uri Hasson",
      "affiliations": [
        "Princeton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2127271556",
    "https://openalex.org/W2102506398",
    "https://openalex.org/W2807486098",
    "https://openalex.org/W2887025330",
    "https://openalex.org/W2883992483",
    "https://openalex.org/W2884989790",
    "https://openalex.org/W3023276083",
    "https://openalex.org/W2128672892",
    "https://openalex.org/W2049980174",
    "https://openalex.org/W1965248225",
    "https://openalex.org/W1987298438",
    "https://openalex.org/W1982352919",
    "https://openalex.org/W1988812422",
    "https://openalex.org/W2172056482",
    "https://openalex.org/W2002660165",
    "https://openalex.org/W2050717100",
    "https://openalex.org/W2114104729",
    "https://openalex.org/W2072522618",
    "https://openalex.org/W2136100138",
    "https://openalex.org/W4250114416",
    "https://openalex.org/W1497204367",
    "https://openalex.org/W2135006625",
    "https://openalex.org/W1985086416",
    "https://openalex.org/W2162593248",
    "https://openalex.org/W2124397031",
    "https://openalex.org/W2022856253",
    "https://openalex.org/W2112891119",
    "https://openalex.org/W2155034734",
    "https://openalex.org/W2417198991",
    "https://openalex.org/W3088131600",
    "https://openalex.org/W2065050119",
    "https://openalex.org/W2162949739",
    "https://openalex.org/W2048789521",
    "https://openalex.org/W2157211812",
    "https://openalex.org/W2107627409",
    "https://openalex.org/W2079092292",
    "https://openalex.org/W2118992292",
    "https://openalex.org/W2051220130",
    "https://openalex.org/W2124590545",
    "https://openalex.org/W2132473597",
    "https://openalex.org/W2130680620",
    "https://openalex.org/W2141185178",
    "https://openalex.org/W2160538606",
    "https://openalex.org/W2407291067",
    "https://openalex.org/W3049635842",
    "https://openalex.org/W1992570774",
    "https://openalex.org/W2344975321",
    "https://openalex.org/W2606089314",
    "https://openalex.org/W2996556191",
    "https://openalex.org/W1662133657",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W2084341220",
    "https://openalex.org/W2284729062",
    "https://openalex.org/W2886027299",
    "https://openalex.org/W3004619146",
    "https://openalex.org/W2883630435",
    "https://openalex.org/W2008607322",
    "https://openalex.org/W2024729467",
    "https://openalex.org/W2130915922",
    "https://openalex.org/W1824528708",
    "https://openalex.org/W2780601052",
    "https://openalex.org/W2052341330",
    "https://openalex.org/W2571093196",
    "https://openalex.org/W2893111280",
    "https://openalex.org/W2086096185",
    "https://openalex.org/W2324155410",
    "https://openalex.org/W2536613192",
    "https://openalex.org/W2952101405",
    "https://openalex.org/W2989219085",
    "https://openalex.org/W3091937808",
    "https://openalex.org/W2792496527",
    "https://openalex.org/W2951521108",
    "https://openalex.org/W2164495629",
    "https://openalex.org/W2560204497",
    "https://openalex.org/W2950229496",
    "https://openalex.org/W1976193721",
    "https://openalex.org/W2950799585",
    "https://openalex.org/W2953135365",
    "https://openalex.org/W3042584003",
    "https://openalex.org/W6906740731",
    "https://openalex.org/W2951103577",
    "https://openalex.org/W615893297",
    "https://openalex.org/W6950058592",
    "https://openalex.org/W6788725042",
    "https://openalex.org/W2085055894",
    "https://openalex.org/W2158568315",
    "https://openalex.org/W2809658419",
    "https://openalex.org/W2955712969",
    "https://openalex.org/W2991340255",
    "https://openalex.org/W2901214657",
    "https://openalex.org/W2052091366",
    "https://openalex.org/W2063951486",
    "https://openalex.org/W1985466488",
    "https://openalex.org/W2622627557",
    "https://openalex.org/W2800311957",
    "https://openalex.org/W2168217710",
    "https://openalex.org/W2782213998",
    "https://openalex.org/W3037273551",
    "https://openalex.org/W2172168442",
    "https://openalex.org/W2951248265",
    "https://openalex.org/W2949433222",
    "https://openalex.org/W3015354840",
    "https://openalex.org/W2301881409",
    "https://openalex.org/W2952634495",
    "https://openalex.org/W2949698470",
    "https://openalex.org/W3031175989",
    "https://openalex.org/W2560385304",
    "https://openalex.org/W2537240939",
    "https://openalex.org/W2950457889",
    "https://openalex.org/W3152665355",
    "https://openalex.org/W3128453260",
    "https://openalex.org/W2481432072",
    "https://openalex.org/W2734370695",
    "https://openalex.org/W3008499038",
    "https://openalex.org/W3089515377",
    "https://openalex.org/W4294214781",
    "https://openalex.org/W1964694219",
    "https://openalex.org/W2913606428",
    "https://openalex.org/W2594719555",
    "https://openalex.org/W2950532129",
    "https://openalex.org/W3209679345",
    "https://openalex.org/W2951583631",
    "https://openalex.org/W6931169039",
    "https://openalex.org/W2113619522",
    "https://openalex.org/W3209896292",
    "https://openalex.org/W2151591509",
    "https://openalex.org/W2613409207",
    "https://openalex.org/W2117140276",
    "https://openalex.org/W1989334446",
    "https://openalex.org/W2117340355",
    "https://openalex.org/W1970928383",
    "https://openalex.org/W2136573752",
    "https://openalex.org/W2151721316",
    "https://openalex.org/W4241074797",
    "https://openalex.org/W2591418495",
    "https://openalex.org/W6893730208",
    "https://openalex.org/W2025009638",
    "https://openalex.org/W1992265439",
    "https://openalex.org/W2110437310",
    "https://openalex.org/W2590234371",
    "https://openalex.org/W2325305575",
    "https://openalex.org/W2020044743",
    "https://openalex.org/W4235770099",
    "https://openalex.org/W2006096283",
    "https://openalex.org/W2977883299",
    "https://openalex.org/W2090687144",
    "https://openalex.org/W1973776237",
    "https://openalex.org/W2130010412",
    "https://openalex.org/W2007318901",
    "https://openalex.org/W2010802712",
    "https://openalex.org/W2025175823",
    "https://openalex.org/W3017013110",
    "https://openalex.org/W1969576961",
    "https://openalex.org/W2151227755",
    "https://openalex.org/W2028405559",
    "https://openalex.org/W1967660841",
    "https://openalex.org/W2996637160",
    "https://openalex.org/W2597348720",
    "https://openalex.org/W2953090092",
    "https://openalex.org/W1978694642",
    "https://openalex.org/W2949455193",
    "https://openalex.org/W2084097533",
    "https://openalex.org/W2004428985",
    "https://openalex.org/W2146292423",
    "https://openalex.org/W3035965352",
    "https://openalex.org/W3003257820",
    "https://openalex.org/W2342249984",
    "https://openalex.org/W3209920844",
    "https://openalex.org/W2061939373",
    "https://openalex.org/W1596936080",
    "https://openalex.org/W4251152452",
    "https://openalex.org/W2011301426",
    "https://openalex.org/W1991825895",
    "https://openalex.org/W2159237454",
    "https://openalex.org/W2053319755",
    "https://openalex.org/W2020051759",
    "https://openalex.org/W2015039597",
    "https://openalex.org/W2951410128",
    "https://openalex.org/W2598802110",
    "https://openalex.org/W2747342147",
    "https://openalex.org/W3008360102",
    "https://openalex.org/W2029130465",
    "https://openalex.org/W2950529334",
    "https://openalex.org/W2760590908",
    "https://openalex.org/W2974239887",
    "https://openalex.org/W2299660060",
    "https://openalex.org/W2575773685",
    "https://openalex.org/W2952175763",
    "https://openalex.org/W3012102178",
    "https://openalex.org/W2949078032",
    "https://openalex.org/W1968340857",
    "https://openalex.org/W2889716325",
    "https://openalex.org/W3138468503",
    "https://openalex.org/W3085241805",
    "https://openalex.org/W6893635002",
    "https://openalex.org/W2944503153",
    "https://openalex.org/W2146421134",
    "https://openalex.org/W652438635",
    "https://openalex.org/W2302501749",
    "https://openalex.org/W2953134824",
    "https://openalex.org/W2290321704",
    "https://openalex.org/W4287752443",
    "https://openalex.org/W2950120067",
    "https://openalex.org/W6912313188",
    "https://openalex.org/W1990134753",
    "https://openalex.org/W2044634376",
    "https://openalex.org/W2139447054",
    "https://openalex.org/W2098866999",
    "https://openalex.org/W2142432333",
    "https://openalex.org/W2057307785",
    "https://openalex.org/W1980054394",
    "https://openalex.org/W2590284759",
    "https://openalex.org/W2961763468",
    "https://openalex.org/W3129635232",
    "https://openalex.org/W2499800833",
    "https://openalex.org/W2518046090",
    "https://openalex.org/W2139376466",
    "https://openalex.org/W2082906925",
    "https://openalex.org/W2100509922",
    "https://openalex.org/W2151116952",
    "https://openalex.org/W2403594978",
    "https://openalex.org/W2532340526",
    "https://openalex.org/W2998367416",
    "https://openalex.org/W6893763539",
    "https://openalex.org/W6912329121",
    "https://openalex.org/W2968308266",
    "https://openalex.org/W2014817911",
    "https://openalex.org/W2952856751",
    "https://openalex.org/W2183801377",
    "https://openalex.org/W2160455305",
    "https://openalex.org/W3098867371",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W97072897",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3122320078",
    "https://openalex.org/W4251944343",
    "https://openalex.org/W2122941717",
    "https://openalex.org/W3100385063",
    "https://openalex.org/W1722938138",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2052644075",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2015103469",
    "https://openalex.org/W3103145119",
    "https://openalex.org/W2053892603",
    "https://openalex.org/W3099878876",
    "https://openalex.org/W2160654481",
    "https://openalex.org/W2773787581",
    "https://openalex.org/W2950133940"
  ],
  "abstract": null,
  "full_text": "1Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdata\nthe “Narratives” fMRI dataset for \nevaluating models of naturalistic \nlanguage comprehension\nSamuel a. Nastase  1 ✉, Yun-Fei Liu2, Hanna Hillman1, asieh Zadbood1, Liat Hasenfratz1, \nNeggin Keshavarzian1, Janice Chen  2, Christopher J. Honey2, Yaara Yeshurun3, Mor Regev4, \nMai Nguyen1, Claire H. C. Chang  1, Christopher Baldassano5, Olga Lositsky6, Erez Simony7,8, \nMichael a. Chow9, Yuan Chang Leong10, Paula P . Brooks1, Emily Micciche11, Gina Choe1, \nariel Goldstein1, tamara Vanderwal12, Yaroslav O. Halchenko  13, Kenneth a. Norman  1 & \nUri Hasson  1\nthe “Narratives” collection aggregates a variety of functional MRI datasets collected while human \nsubjects listened to naturalistic spoken stories. The current release includes 345 subjects, 891 functional \nscans, and 27 diverse stories of varying duration totaling ~4.6 hours of unique stimuli (~43,000 words). \nthis data collection is well-suited for naturalistic neuroimaging analysis, and is intended to serve as \na benchmark for models of language and narrative comprehension. We provide standardized MRI \ndata accompanied by rich metadata, preprocessed versions of the data ready for immediate use, and \nthe spoken story stimuli with time-stamped phoneme- and word-level transcripts. all code and data \nare publicly available with full provenance in keeping with current best practices in transparent and \nreproducible neuroimaging.\nBackground & Summary\nWe use language to build a shared understanding of the world. In speaking, we package certain brain states into \na sequence of linguistic elements that can be transmitted verbally, through vibrations in the air; in listening, we \nexpand a verbal transmission into the intended brain states, bringing our own experiences to bear on the inter-\npretation\n1. The neural machinery supporting this language faculty presents a particular challenge for neurosci -\nence. Certain aspects of language are uniquely human 2, limiting the efficacy of nonhuman animal models and \nrestricting the use of invasive methods (for example, only humans recursively combine linguistic elements into \ncomplex, hierarchical expressions with long-range dependencies\n3,4). Language is also very dynamic and contex-\ntualized. Linguistic narratives evolve over time: the meaning of any given element depends in part on the history \nof elements preceding it and certain elements may retroactively resolve the ambiguity of preceding elements. This \nmakes it difficult to decompose natural language using traditional experimental manipulations\n5–7.\nNoninvasive neuroimaging tools, such as functional MRI, have laid the groundwork for a neurobiology of \nlanguage comprehension8–14. Functional MRI measures local fluctuations in blood oxygenation—referred to as \nthe blood-oxygen-level-dependent (BOLD) signal—associated with changes in neural activity over the course of \nan experimental paradigm\n15–18. Despite relatively poor temporal resolution (e.g. one sample every 1–2 seconds), \n1Princeton neuroscience i nstitute and Department of Psychology, Princeton University, Princeton, nJ, USA. \n2Department of Psychological and Brain Sciences, Johns Hopkins University, Baltimore, MD, USA. 3School of \nPsychological Sciences, t el Aviv University, t el Aviv, israel. 4Montreal neurological institute, McGill University, \nMontreal, Qc, canada. 5Department of Psychology, columbia University, new York, nY, USA. 6Department of \ncognitive, Linguistic and Psychological Sciences, Brown University, Providence, Ri, USA. 7faculty of electrical \nengineering, Holon i nstitute of technology, Holon, i srael. 8Department of neurobiology, Weizmann i nstitute of \nScience, Rehovot, israel. 9Datacamp, inc., new York, nY, USA. 10Helen Wills neuroscience institute, University \nof california, Berkeley, c A, USA. 11Peabody college, Vanderbilt University, nashville, tn, USA. 12Department of \nPsychiatry, University of British columbia, and Bc children’s Hospital Research i nstitute, Vancouver, Bc, canada. \n13Department of Psychological and Brain Sciences and Department of computer Science, Dartmouth college, \nHanover, nH, USA. ✉e-mail: sam.nastase@gmail.com\nData DESCRIPt OR\nOPEN\n\n2Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nfMRI allows us to map brain-wide responses to linguistic stimuli at a spatial resolution of millimeters. A large \nbody of prior work has revealed a partially left-lateralized network of temporal and frontal cortices encoding \nacoustic-phonological\n19–22, syntactic23–25, and semantic features26,27 of linguistic stimuli (with considerable indi-\nvidual variability28–30). This work has historically been grounded in highly-controlled experimental paradigms \nfocusing on isolated phonemes 31,32, single words 33–35, or contrasting sentence manipulations 36–41 (with some \nexceptions 42–44). While these studies have provided a tremendous number of insights, the vast majority rely \non highly-controlled experimental manipulations with limited generalizability to natural language 5,45. Recent \nwork, however, has begun extending our understanding to more ecological contexts using naturalistic text46 and \nspeech47.\nIn parallel, the machine learning community has made tremendous advances in natural language process -\ning48,49. Neurally-inspired computational models are beginning to excel at complex linguistic tasks such as \nword-prediction, summarization, translation, and question-answering 50,51. Rather than using symbolic lexical \nrepresentations and syntactic trees, these models typically rely on vectorial representations of linguistic content52: \nlinguistic elements that are similar in some respect are encoded nearer to each other in a continuous embed -\nding space, and seemingly complex linguistic relations can be recovered using relatively simple arithmetic opera-\ntions\n53,54. In contrast to experimental traditions in linguistics and neuroscience, machine learning has embraced \ncomplex, high-dimensional models trained on enormous corpora of real-world text, emphasizing predictive \npower over interpretability\n55–57.\nWe expect that a reconvergence of these research trajectories supported by “big” neural data will be mutually \nbeneficial45,58. Public, well-curated benchmark datasets can both accelerate research and serve as useful didactic \ntools (e.g. MNIST59, CIFAR60). Furthermore, there is a societal benefit to sharing human brain data: fMRI data are \nexpensive to acquire and the reuse of publicly shared fMRI data is estimated to have saved billions in public fund-\ning\n61. Public data also receive much greater scrutiny, with the potential to reveal (and rectify) “bugs” in the data or \nmetadata. Although public datasets released by large consortia have propelled exploratory research forward62,63, \nrelatively few of these include naturalistic stimuli (cf. movie-viewing paradigms in Cam-CAN64,65, HBN66, HCP \nS120063). On the other hand, maturing standards and infrastructure 67–69 have enabled increasingly widespread \nsharing of smaller datasets from the “long tail” of human neuroimaging research 70. We are beginning to see a \nproliferation of public neuroimaging datasets acquired using rich, naturalistic experimental paradigms46,71–84. The \nmajority of these datasets are not strictly language-oriented, comprising audiovisual movie stimuli rather than \naudio-only spoken stories.\nWith this in mind, we introduce the “Narratives” collection of naturalistic story-listening fMRI data for eval-\nuating models of language\n85. The Narratives collection comprises fMRI data collected over the course of seven \nyears by the Hasson and Norman Labs at the Princeton Neuroscience Institute while participants listened to 27 \nspoken story stimuli ranging from ~3 minutes to ~56 minutes for a total of ~4.6 hours of unique stimuli (~43,000 \nwords; Table 1). The collection currently includes 345 unique subjects contributing 891 functional scans with \naccompanying anatomical data and metadata. In addition to the MRI data, we provide demographic data and \ncomprehension scores where available. Finally, we provide the auditory story stimuli, as well as time-stamped \nphoneme- and word-level transcripts in hopes of accelerating analysis of the linguistic content of the data. The \ndata are standardized according to the Brain Imaging Data Structure\n86 (BIDS 1.2.1; https://bids.neuroimaging.io/; \nRRID:SCR_016124), and are publicly available via OpenNeuro87 (https://openneuro.org/; RRID:SCR_005031): \nhttps://openneuro.org/datasets/ds002345. Derivatives of the data, including preprocessed versions of the data \nand stimulus annotations, are available with transparent provenance via DataLad\n88,89 (https://www.datalad.org/; \nRRID:SCR_003931): http://datasets.datalad.org/?dir=/labs/hasson/narratives.\nWe believe these data have considerable potential for reuse because naturalistic spoken narratives are rich \nenough to support testing of a wide range of meaningful hypotheses about language and the brain5,7,45,90–95. These \nhypotheses can be formalized as quantitative models and evaluated against the brain data96,97 (Fig. 1a). For exam-\nple, the Narratives data are particularly well-suited for evaluating models capturing linguistic content ranging \nfrom lower-level acoustic features\n98–100 to higher-level semantic features47,101–103. More broadly, naturalistic data \nof this sort can be useful for evaluating shared information across subjects 104,105, individual differences77,106–108, \nalgorithms for functional alignment81,109–113, models of event segmentation and narrative context114–118, and func-\ntional network organization119–122. In the following, we describe the Narratives data collection and provide several \nperspectives on data quality.\nMethods\nParticipants. Data were collected over the course of roughly seven years, from October, 2011 to September, \n2018. Participants were recruited from the Princeton University student body as well as non-university-affil-\niated members of the broader community in Princeton, NJ. All participants provided informed, written con-\nsent prior to data collection in accordance with experimental procedures approved by Princeton University \nInstitutional Review Board. Across all datasets, 345 adults participated in data collection (ages 18–53 years, mean \nage 22.2 ± 5.1 years, 204 reported female). Demographics for each dataset are reported in the “Narrative datasets” \nsection. Both native and non-native English speakers are included in the dataset, but all subjects reported fluency \nin English (our records do not contain detailed information on fluency in other languages). All subjects reported \nhaving normal hearing and no history of neurological disorders.\nExperimental procedures. Upon arriving at the fMRI facility, participants first completed a simple demo-\ngraphics questionnaire, as well as a comprehensive MRI screening form. Participants were instructed to listen \nand pay attention to the story stimulus, remain still, and keep their eyes open. In some cases, subject wake-\nfulness was monitored in real-time using an eye-tracker. Stimulus presentation was implemented using either \nPsychtoolbox\n123,124 or PsychoPy 125–127. In some cases a centrally-located fixation cross or dot was presented \n3Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nthroughout the story stimulus; however, participants were not instructed to maintain fixation. Auditory story \nstimuli were delivered via MRI-compatible insert earphones (Sensimetrics, Model S14); headphones or foam \npadding were placed over the earphones to reduce scanner noise. In most cases, a volume check was performed \nto ensure that subjects could comfortably hear the auditory stimulus over the MRI acquisition noise prior to \ndata collection. A sample audio stimulus was played during an EPI sequence (that was later discarded) and the \nvolume was adjusted by either the experimenter or subject until the subject reported being able to comfortably \nhear and understand the stimulus. In the staging/ directory on GitHub, we provide an example stimulus \npresentation PsychoPy script used with the “Pie Man (PNI)” , “Running from the Bronx (PNI)” , “I Knew Y ou Were \nBlack” , and “The Man Who Forgot Ray Bradbury” stories (story_presentation.py), as well as an example \nvolume-check script (soundcheck_presentation.py; see “Code availability”).\nFor many datasets in this collection, story scans were accompanied by additional functional scans including \ndifferent stories or other experimental paradigms, as well as auxiliary anatomical scans. In all cases, an entire \nstory stimulus was presented in a single scanning run; however, in some cases multiple independent stories were \ncollected in a single scanning run (the “Slumlord” and “Reach for the Stars One Small Step at a Time” stories, and \nthe “Schema” stories; see the “Narrative datasets” section below). In scanning sessions with multiple stories and \nscanning runs, participants were free to request volume adjustments between scans (although this was uncom-\nmon). Participants were debriefed as to the purpose of a given experiment at the end of a scanning session, and \nin some cases filled out follow-up questionnaires evaluating their comprehension of the stimuli. Specifical proce-\ndural details for each dataset are described in the “Narrative datasets” section below.\nStimuli. To add to the scientific value of the neuroimaging data, we provide waveform (W AV) audio files \ncontaining the spoken story stimulus for each dataset (e.g. pieman_audio.wav ). Audio files were edited \nusing the open source Audacity software (https://www.audacityteam.org). These audio files served as stimuli \nfor the publicly-funded psychological and neuroscientific studies described herein. The stimuli are intended for \nnon-profit, non-commercial scholarly use—principally feature extraction—under “fair use” or “fair dealing” pro-\nvisions; (re)sharing and (re)use of these media files is intended to respect these guidelines\n128. Story stimuli span \na variety of media, including commercially-produced radio and internet broadcasts, authors and actors reading \nwritten works, professional storytellers performing in front of live audiences, and subjects verbally recalling pre-\nvious events (in the scanner). Manually written transcripts are provided with each story stimulus. The specific \nstory stimuli for each dataset are described in more detail in the “Narrative datasets” section below. In total, the \nauditory stories sum to roughly 4.6 hours of unique stimuli corresponding to 11,149 TRs (excluding TRs acquired \nwith no auditory story stimulus). Concatenating across all subjects, this amounts to roughly 6.4 days worth of \nstory-listening fMRI data, or 369,496 TRs.\nBy including all stimuli in this data release, researchers can extract their choice of linguistic features for model \nevaluation\n7,129, opening the door to many novel scientific questions. To kick-start this process, we used Gentle \nStory Duration TRs Words Subjects\n“Pie Man” 07:02 282 957 82\n“Tunnel Under the World” 25:34 1,023 3,435 23\n“Lucy” 09:02 362 1,607 16\n“Pretty Mouth and Green My Eyes” 11:16 451 1,970 40\n“Milky Way” 06:44 270 1,058 53\n“Slumlord” 15:03 602 2,715 18\n“Reach for the Stars One Small Step at a Time” 13:45 550 2,629 18\n“It’s Not the Fall That Gets Y ou” 09:07 365 1,601 56\n“Merlin” 14:46 591 2,245 36\n“Sherlock” 17:32 702 2,681 36\n“Schema” 23:12 928 3,788 31\n“Shapes” 06:45 270 910 59\n“The 21st Y ear” 55:38 2,226 8,267 25\n“Pie Man (PNI)” 06:40 267 992 40\n“Running from the Bronx (PNI)” 08:56 358 1,379 40\n“I Knew Y ou Were Black” 13:20 534 1,544 40\n“The Man Who Forgot Ray Bradbury” 13:57 558 2,135 40\nTotal: 4.6 hours 11,149 TRs 42,989 words\nTotal across subjects: 6.4 days 369,496 TRs 1,399,655 words\nTable 1. The “Narratives” datasets summarized in terms of duration, word count, and sample size. Some \nsubjects participated in multiple experiments resulting in overlapping samples. Note that for the “Milky Way” \nand “Shapes” datasets, we tabulate the duration and word count only once for closely related experimental \nmanipulations, respectively (reflected in the total durations at bottom). For the “Schema” dataset, we indicate \nthe sum of the duration and word counts across eight brief stories. We do not include the temporally scrambled \nversions of the “It’s Not the Fall That Gets Y ou Dataset” in the duration and word totals.\n4Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\n0.10.1130 to create time-stamped phoneme- and word-level transcripts for each story stimulus in an automated \nfashion. Gentle is a robust, lenient forced-alignment algorithm that relies on the free and open source Kaldi auto-\nmated speech recognition software131 and the Fisher English Corpus132. The initial (non-time-stamped) written \ntranscripts were manually transcribed by the authors and supplied to the Gentle forced-alignment algorithm \nalongside the audio file. The first-pass output of the forced-alignment algorithm was visually inspected and any \nerrors in the original transcripts were manually corrected; then the corrected transcripts were resubmitted to the \nforced-alignment algorithm to generate the final time-stamped transcripts.\nThe Gentle forced-alignment software generates two principal outputs. First, a simple word-level transcript \nwith onset and offset timing for each word is saved as a tabular CSV file. Each row in the CSV file corresponds to a \nword. The CSV file contains four columns (no header) where the first column indicates the word from the written \ntranscript, the second column indicates the corresponding word in the vocabulary, the third column indicates \nthe word onset timing from the beginning of the stimulus file, and the fourth column indicates the word offset \ntiming. Second, Gentle generates a more detailed JSON file containing the full written transcript followed by a list \nof word entries where each entry contains fields indicating the word (“word”) and whether word was found in the \nvocabulary (“alignedWord”), the onset (“start”) and offset (“end”) timing of the word referenced to the beginning \nof the audio file, the duration of each phone comprising that word (“phones” containing “phone” and “duration”), \nand a field (“case”) indicating whether the word was correctly localized in the audio signal. Words that were not \nfound in the vocabulary are indicated by “<unk>” for “unknown” in the second column of the CSV file or in the \n“alignedWord” field of the JSON file. Words that are not successfully localized in the audio file receive their own \nrow but contain no timing information in the CSV file, and are marked as “not-found-in-audio” in the “case” field \nof the JSON file. All timing information generated by Gentle is indexed to the beginning of the stimulus audio file. \nThis should be referenced against the BIDS-formatted events.tsv files accompanying each scan describing \nwhen the story stimulus began relative to the beginning of the scan (this varies across datasets as described in the \n“Narratives datasets” section below). Some word counts may be slightly inflated by the inclusion of disfluencies \n(e.g. “uh”) in the transcripts. Transcripts for the story stimuli contain 42,989 words in total across all stories; 789 \nwords (1.8%) were not successfully localized by the forced-alignment algorithm and 651 words (1.5%) were not \nfound in the vocabulary (see the get_words.py script in the code/ directory). Concatenating across all sub-\njects yields 1,399,655 words occurring over the course of 369,496 TRs.\nFig. 1 Schematic depiction of the naturalistic story-listening paradigm and data provenance. (a) At bottom, the \nfull auditory story stimulus “Pie Man” by Jim O’Grady is plotted as a waveform of varying amplitude (y-axis) \nover the duration of 450 seconds (x-axis) corresponding to 300 fMRI volumes sampled at a TR of 1.5 seconds. \nAn example clip (marked by vertical orange lines) is expanded and accompanied by the time-stamped word \nannotation (“I began my illustrious career in journalism in the Bronx, where I worked as a hard-boiled reporter \nfor the Ram…”). The story stimulus can be described according to a variety of models; for example, acoustic, \nsemantic, or narrative features can be extracted from or assigned to the stimulus. In a prediction or model-\ncomparison framework, these models serve as formal hypotheses linking the stimulus to brain responses. At \ntop, preprocessed fMRI response time-series from three example voxels for an example subject are plotted for \nthe full duration of the story stimulus (x-axis: fMRI signal magnitude; y-axis: scan duration in TRs; red: early \nauditory cortex; orange: auditory association cortex; purple: temporoparietal junction). See the plot_\nstim.py script in the code/ directory for details. (b) At bottom, MRI data, metadata, and stimuli \nare formatted according to the BIDS standard and publicly available via OpenNeuro. All derivative data are \nversion-controlled and publicly available via DataLad. The schematic preprocessing workflow includes the \nfollowing steps: realignment, susceptibility distortion correction, and spatial normalization with fMRIPrep; \nunsmoothed and spatially smoothed workflows proceed in parallel; confound regression to mitigate artifacts \nfrom head motion and physiological noise; as well as intersubject correlation (ISC) analyses used for quality \ncontrol in this manuscript. Each stage of the processing workflow is publicly available and indexed by a commit \nhash (left) providing a full, interactive history of the data collection. This schematic is intended to provide a \nhigh-level summary and does not capture the full provenance in detail; for example, derivatives from MRIQC \nare also included in the public release alongside other derivatives (but are not depicted here).\n5Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nGentle packages the audio file, written transcript, CSV and JSON files with an HTML file for viewing in \na browser that allows for interactively listening to the audio file while each word is highlighted (with its cor -\nresponding phones) when it occurs in the audio. Note that our forced-alignment procedure using Gentle was \nfully automated and did not include any manual intervention. That said, the time-stamped transcripts are not \nperfect—speech disfluencies, multiple voices speaking simultaneously, sound effects, and scanner noise provide \na challenge for automated transcription. We hope this annotation can serve as a starting point, and expect that \nbetter transcriptions will be created in the future. We invite researchers who derive additional features and anno-\ntations from the stimuli to contact the corresponding author and we will help incorporate these annotations into \nthe publicly available dataset.\nMRI data acquisition.  All datasets were collected at the Princeton Neuroscience Institute Scully Center \nfor Neuroimaging. Acquisition parameters are reproduced below in compliance with the guidelines put forward \nby the Organization for Human Brain Mapping (OHBM) Committee on Best Practices in Data Analysis and \nSharing\n133 (COBIDAS), and are also included in the BIDS-formatted JSON metadata files accompanying each \nscan. All studies used a repetition time (TR) of 1.5 seconds. Several groups of datasets differ in acquisition param-\neters due to both experimental specifications and the span of years over which datasets were acquired; for exam-\nple, several of the newer datasets were acquired on a newer scanner and use multiband acceleration to achieve \ngreater spatial resolution (e.g. for multivariate pattern analysis) while maintaining the same temporal resolution. \nThe “Pie Man” , “Tunnel Under the World” , “Lucy” , “Pretty Mouth and Green My Eyes” , “Milky Way” , “Slumlord” , \n“Reach for the Stars One Small Step at a Time” , “It’s Not the Fall that Gets Y ou” , “Merlin” , “Sherlock” , and “The \n21st Y ear” datasets were collected on a 3 T Siemens Magnetom Skyra (Erlangen, Germany) with a 20-channel \nphased-array head coil using the following acquisition parameters. Functional BOLD images were acquired in \nan interleaved fashion using gradient-echo echo-planar imaging (EPI) with an in-plane acceleration factor of 2 \nusing GRAPPA: TR/TE = 1500/28 ms, flip angle = 64°, bandwidth = 1445 Hz/Px, in-plane resolution = 3 × 3 mm, \nslice thickness = 4 mm, matrix size = 64 × 64, FoV = 192 × 192 mm, 27 axial slices with roughly full brain cover-\nage and no gap, anterior–posterior phase encoding, prescan normalization, fat suppression. In cases where full \nbrain coverage was not attainable, inferior extremities were typically excluded (e.g. cerebellum, brainstem) to \nmaximize coverage of the cerebral cortex. At the beginning of each run, three dummy scans were acquired and \ndiscarded by the scanner to allow for signal stabilization. T1-weighted structural images were acquired using a \nhigh-resolution single-shot MPRAGE sequence with an in-plane acceleration factor of 2 using GRAPPA: TR/\nTE/TI = 2300/3.08/900 ms, flip angle = 9°, bandwidth = 240 Hz/Px, in-plane resolution 0.859 × 0.859 mm, slice \nthickness 0.9 mm, matrix size = 256 × 256, FoV = 172.8 × 220 × 220 mm, 192 sagittal slices, ascending acquisi-\ntion, anterior–posterior phase encoding, prescan normalization, no fat suppression, 7 minutes 21 seconds total \nacquisition time.\nThe “Schema” and “Shapes” datasets were collected on a 3 T Siemens Magnetom Prisma with a 64-channel \nhead coil using the following acquisition parameters. Functional images were acquired in an interleaved fash-\nion using gradient-echo EPI with a multiband (simultaneous multi-slice; SMS) acceleration factor of 4 using \nblipped CAIPIRINHA and no in-plane acceleration: TR/TE 1500/39 ms, flip angle = 50°, bandwidth = 1240 Hz/\nPx, in-plane resolution = 2.0 × 2.0 mm, slice thickness 2.0 mm, matrix size = 96 × 96, FoV = 192 × 192 mm, 60 \naxial slices with full brain coverage and no gap, anterior–posterior phase encoding, 6/8 partial Fourier, no pres-\ncan normalization, fat suppression, three dummy scans. T1-weighted structural images were acquired using a \nhigh-resolution single-shot MPRAGE sequence with an in-plane acceleration factor of 2 using GRAPPA: TR/TE/\nTI = 2530/2.67/1200 ms, flip angle = 7°, bandwidth = 200 Hz/Px, in-plane resolution 1.0 × 1.0 mm, slice thickness \n1.0 mm, matrix size = 256 × 256, FoV = 176 × 256 × 256 mm, 176 sagittal slices, ascending acquisition, no fat sup-\npression, 5 minutes 52 seconds total acquisition time.\nThe “Pie Man (PNI), ” “Running from the Bronx, ” “I Knew Y ou Were Black, ” and “The Man Who Forgot Ray \nBradbury” datasets were collected on the same 3 T Siemens Magnetom Prisma with a 64-channel head coil using \ndifferent acquisition parameters. Functional images were acquired in an interleaved fashion using gradient-echo \nEPI with a multiband acceleration factor of 3 using blipped CAIPIRINHA and no in-plane acceleration: TR/\nTE 1500/31 ms, flip angle = 67°, bandwidth = 2480 Hz/Px, in-plane resolution = 2.5 × 2.5 mm, slice thickness \n2.5 mm, matrix size = 96 × 96, FoV = 240 × 240 mm, 48 axial slices with full brain coverage and no gap, anterior–\nposterior phase encoding, prescan normalization, fat suppression, three dummy scans. T1-weighted structural \nimages were acquired using a high-resolution single-shot MPRAGE sequence with an in-plane acceleration factor \nof 2 using GRAPPA: TR/TE/TI = 2530/3.3/1100 ms, flip angle = 7°, bandwidth = 200 Hz/Px, in-plane resolution \n1.0 × 1.0 mm, slice thickness 1.0 mm, matrix size = 256 × 256, FoV = 176 × 256 × 256 mm, 176 sagittal slices, \nascending acquisition, no fat suppression, prescan normalization, 5 minutes 53 seconds total acquisition time. \nT2-weighted structural images were acquired using a high-resolution single-shot MPRAGE sequence with an \nin-plane acceleration factor of 2 using GRAPPA: TR/TE = 3200/428 ms, flip angle = 120°, bandwidth = 200 Hz/\nPx, in-plane resolution 1.0 × 1.0 mm, slice thickness 1.0 mm, matrix size = 256 × 256, FoV = 176 × 256 × 256 mm, \n176 sagittal slices, interleaved acquisition, no prescan normalization, no fat suppression, 4 minutes 40 seconds \ntotal acquisition time.\nMRI preprocessing.  Anatomical images were de-faced using the automated de-facing software pydeface \n2.0.0134 prior to further processing (using the run_pydeface.py script in the code/ directory). MRI data \nwere subsequently preprocessed using fMRIPrep 20.0.5135,136 (RRID:SCR_016216; using the run_fmriprep.\nsh script in the code/ directory). FMRIPrep is a containerized, automated tool based on Nipype 1.4.2 137,138 \n(RRID:SCR_002502) that adaptively adjusts to idiosyncrasies of the dataset (as captured by the metadata) to \napply the best-in-breed preprocessing workflow. Many internal operations of fMRIPrep functional processing \n6Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nworkflow use Nilearn 0.6.2139 (RRID:SCR_001362). For more details of the pipeline, see the section correspond-\ning to workflows in fMRIPrep’s documentation. The containerized fMRIPrep software was deployed using \nSingularity 3.5.2-1.1.sdl7140. The fMRIPrep Singularity image can be built from Docker Hub (https://hub.docker.\ncom/r/poldracklab/fmriprep/; e.g. singularity build fmriprep-20.0.5.simg docker://pol-\ndracklab/fmriprep:20.0.5). The fMRIPrep outputs and visualization can be found in the fmriprep/ \ndirectory in derivatives/ available via the DataLad release. The fMRIPrep workflow produces two prin-\ncipal outputs: (a) the functional time series data in one more output space (e.g. MNI space), and (b) a collection of \nconfound variables for each functional scan. In the following, we describe fMRIPrep’s anatomical and functional \nworkflows, as well as subsequent spatial smoothing and confound regression implemented in AFNI 19.3.0\n141,142 \n(RRID:SCR_005927).\nThe anatomical MRI T1-weighted (T1w) images were corrected for intensity non-uniformity with \nN4BiasFieldCorrection 143, distributed with ANTs 2.2.0 144 (RRID:SCR_004757), and used as \nT1w-reference throughout the workflow. The T1w-reference was then skull-stripped with a Nipype implementa-\ntion of the antsBrainExtraction.sh (from ANTs) using the OASIS30ANTs as the target template. Brain \ntissue segmentation of cerebrospinal fluid (CSF), white-matter (WM), and gray-matter (GM) was performed \non the brain-extracted T1w using fast\n145 (FSL 5.0.9; RRID:SCR_002823). Brain surfaces were reconstructed \nusing recon-all146,147 (FreeSurfer 6.0.1; RRID:SCR_001847), and the brain mask estimated previously was \nrefined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations \nof the cortical gray-matter from Mindboggle\n148 (RRID:SCR_002438). Volume-based spatial normalization to \ntwo commonly-used standard spaces (MNI152NLin2009cAsym, MNI152NLin6Asym) was performed through \nnonlinear registration with antsRegistration (ANTs 2.2.0) using brain-extracted versions of both T1w \nreference and the T1w template. The following two volumetric templates were selected for spatial normaliza-\ntion and deployed using TemplateFlow\n149: (a) ICBM 152 Nonlinear Asymmetrical Template Version 2009c 150 \n(RRID:SCR_008796; TemplateFlow ID: MNI152NLin2009cAsym), and (b ) FSL ’s MNI ICBM 152 Non-linear \n6th Generation Asymmetric Average Brain Stereotaxic Registration Model151 (RRID:SCR_002823; TemplateFlow \nID: MNI152NLin6Asym). Surface-based normalization based on nonlinear registration of sulcal curvature was \napplied using the following three surface templates\n152 (FreeSurfer reconstruction nomenclature): fsaverage, fsav-\nerage6, fsaverage5.\nThe functional MRI data were preprocessed in the following way. First, a reference volume and its \nskull-stripped version were generated using a custom methodology of fMRIPrep. A deformation field to cor -\nrect for susceptibility distortions was estimated using fMRIPrep’s fieldmap-less approach. The deformation \nfield results from co-registering the BOLD reference to the same-subject T1w-reference with its intensity \ninverted153,154. Registration was performed with antsRegistration (ANTs 2.2.0), and the process was reg-\nularized by constraining deformation to be nonzero only along the phase-encoding direction, and modulated \nwith an average fieldmap template\n155. Based on the estimated susceptibility distortion, a corrected EPI reference \nwas calculated for more accurate co-registration with the anatomical reference. The BOLD reference was then \nco-registered to the T1w reference using bbregister (FreeSurfer 6.0.1), which implements boundary-based \nregistration\n156. Co-registration was configured with six degrees of freedom. Head-motion parameters with respect \nto the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) \nare estimated before any spatiotemporal filtering using mcflirt (FSL 5.0.9)\n157–159. BOLD runs were slice-time \ncorrected using 3dTshift from AFNI 20160207 160. The BOLD time-series were resampled onto the follow-\ning surfaces: fsaverage, fsaverage6, fsaverage5. The BOLD time-series (including slice-timing correction when \napplied) were resampled onto their original, native space by applying a single, composite transform to correct for \nhead-motion and susceptibility distortions. These resampled BOLD time-series are referred to as preprocessed \nBOLD in original space, or just preprocessed BOLD. The BOLD time-series were resampled into two volumet-\nric standard spaces, correspondingly generating the following spatially-normalized, preprocessed BOLD runs: \nMNI152NLin2009cAsym, MNI152NLin6Asym. A reference volume and its skull-stripped version were first gen-\nerated using a custom methodology of fMRIPrep. All resamplings were performed with a single interpolation \nstep by composing all the pertinent transformations (i.e. head-motion transform matrices, susceptibility distor-\ntion correction, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were \nperformed using antsApplyTransforms (ANTs 2.2.0), configured with Lanczos interpolation to minimize \nthe smoothing effects of other kernels\n161. Non-gridded (surface) resamplings were performed using mri_vol-\n2surf (FreeSurfer 6.0.1).\nSeveral confounding time-series were calculated based on the preprocessed BOLD: framewise displacement \n(FD), DV ARS, and three region-wise global signals. FD and DV ARS are calculated for each functional run, both \nusing their implementations in Nipype\n162. The three global signals are extracted within the CSF , the WM, and the \nwhole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based \nnoise correction (CompCor)\n163. Principal components are estimated after high-pass filtering the preprocessed \nBOLD time-series (using a discrete cosine filter with 128 s cut-off) for the two CompCor variants: temporal \n(tCompCor) and anatomical (aCompCor). The tCompCor components are then calculated from the top 5% var-\niable voxels within a mask covering the subcortical regions. This subcortical mask is obtained by heavily eroding \nthe brain mask, which ensures it does not include cortical GM regions. For aCompCor, components are calcu-\nlated within the intersection of the aforementioned mask and the union of CSF and WM masks calculated in T1w \nspace, after their projection to the native space of each functional run (using the inverse BOLD-to-T1w transfor-\nmation). Components are also calculated separately within the WM and CSF masks. For each CompCor decom-\nposition, the k components with the largest singular values are retained, such that the retained components’ time \nseries are sufficient to explain 50 percent of variance across the nuisance mask (CSF , WM, combined, or tempo-\nral). The remaining components are dropped from consideration. The head-motion estimates calculated in the \ncorrection step were also placed within the corresponding confounds file. The confound time series derived from \n7Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nhead motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic \nterms for each164. Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardised DV ARS were annotated as \nmotion outliers. All of these confound variables are provided with the dataset for researchers to use as they see fit. \nHTML files with quality control visualizations output by fMRIPrep are available via DataLad.\nWe provide spatially smoothed and non-smoothed versions of the preprocessed functional data returned \nby fMRIPrep (smoothing was implemented using the run_smoothing.py script in the code/ directory). \nAnalyses requiring voxelwise correspondence across subjects, such as ISC analysis\n165, can benefit from spatial \nsmoothing due to variability in functional–anatomical correspondence across individuals—at the expense of \nspatial specificity (finer-grained intersubject functional correspondence can be achieved using hyperalignment \nrather than spatial smoothing\n81,113,166). The smoothed and non-smoothed outputs (and subsequent analyses) \ncan be found in the afni-smooth/ and afni-nosmooth/ directories in derivatives/ available via \nDataLad. To smooth the volumetric functional images, we used 3dBlurToFWHM in AFNI 19.3.0\n141,142 which \niteratively measures the global smoothness (ratio of variance of first differences across voxels to global variance) \nand local smoothness (estimated within 4 mm spheres), then applies local smoothing to less-smooth areas until \nthe desired global smoothness is achieved. All smoothing operations were performed within a brain mask to \nensure that non-brain values were not smoothed into the functional data (see the brain_masks.py script \nin the code/ directory). For surface-based functional data, we applied smoothing SurfSmooth in AFNI, \nwhich uses a similar iterative algorithm for smoothing surface data according to geodesic distance on the cortical \nmantle\n167,168. In both cases, data were smoothed until a target global smoothness of 6 mm FWHM was met (i.e. \n2–3 times original voxel sizes169). Equalizing smoothness is critical to harmonize data acquired across different \nscanners and protocols170.\nWe next temporally filtered the functional data to mitigate the effects of confounding variables. Unlike tra-\nditional task fMRI experiments with a well-defined event structure, the goal of regression was not to estimate \nregression coefficients for any given experimental conditions; rather, similar to resting-state functional connec-\ntivity analysis, the goal of regression was to model nuisance variables, resulting in a “clean” residual time series. \nHowever, unlike conventional resting-state paradigms, naturalistic stimuli enable intersubject analyses, which \nare less sensitive to idiosyncratic noises than within-subject functional connectivity analysis typically used with \nresting-state data\n119,171. With this in mind, we used a modest confound regression model informed by the rich \nliterature on confound regression for resting-state functional connectivity172,173. AFNI’s 3dTproject was used \nto regress out the following nuisance variables (via the extract_confounds.py and run_regression.\npy scripts in the code/ directory): six head motion parameters (three translation, three rotation), the first five \nprincipal component time series from an eroded CSF and a white matter mask\n163,174, cosine bases for high-pass \nfiltering (using a discrete cosine filter with cutoff: 128 s, or ~0.0078 Hz), and first- and second-order detrend-\ning polynomials. These variables were included in a single regression model to avoid reintroducing artifacts by \nsequential filtering\n175. The scripts used to perform this regression and the residual time series are provided with \nthis data release. This processing workflow ultimately yields smoothed and non-smoothed versions of the “clean” \nfunctional time series data in several volumetric and surface-based standard spaces.\nComputing environment. In addition to software mentioned elsewhere in this manuscript, all data process-\ning relied heavily on the free, open source GNU/Linux ecosystem and NeuroDebian distribution 176,177 (https://\nneuro.debian.net/; RRID:SCR_004401), as well as the Anaconda distribution (https://www.anaconda.com/) and \nconda package manager (https://docs.conda.io/en/latest/; RRID:SCR_018317). Many analyses relied on scientific \ncomputing software in Python (https://www.python.org/; RRID:SCR_008394), including NumPy\n178,179 (http://\nwww.numpy.org/; RRID:SCR_008633), SciPy 180,181 (https://www.scipy.org/; RRID:SCR_008058), Pandas 182 \n(https://pandas.pydata.org/; RRID:SCR_018214), NiBabel 183 (https://nipy.org/nibabel/; RRID:SCR_002498), \nIPython184 (http://ipython.org/; RRID:SCR_001658), and Jupyter 185 (https://jupyter.org/; RRID:SCR_018315), \nas well as Singularity containerization 140 (https://sylabs.io/docs/) and the Slurm workload manager 186 (https://\nslurm.schedmd.com/). All surface-based MRI data were visualized using SUMA187,188 (RRID:SCR_005927). All \nother figures were created using Matplotlib189 (http://matplotlib.org/; RRID:SCR_008624), seaborn (http://sea-\nborn.pydata.org/; RRID:SCR_018132), GIMP (http://www.gimp.org/; RRID:SCR_003182), and Inkscape (https://\ninkscape.org/; RRID:SCR_014479). The “Narratives” data were processed on a Springdale Linux 7.9 (Verona) sys-\ntem based on the Red Hat Enterprise Linux distribution (https://springdale.math.ias.edu/). An environment.\nyml file specifying the conda environment used to process the data is included in the staging/ directory on \nGitHub (as well as a more flexible cross-platform environment-flexible.yml file; see “Code availability”).\nNarratives datasets. Each dataset in the “Narratives” collection is described below. The datasets are listed in \nroughly chronological order of acquisition. For each dataset, we provide the dataset-specific subject demograph-\nics, a summary of the stimulus and timing, as well as details of the experimental procedure or design.\n“Pie Man”. The “Pie Man” dataset was collected between October, 2011 and March, 2013, and comprised \n82 participants (ages 18–45 years, mean age 22.5 ± 4.3 years, 45 reported female). The “Pie Man” story was told \nby Jim O’Grady and recorded live at The Moth, a non-profit storytelling event, in New Y ork City in 2008 (freely \navailable at https://themoth.org/stories/pie-man). The “Pie Man” audio stimulus was 450 seconds (7.5 minutes) \nlong and began with 13 seconds of neutral introductory music followed by 2 seconds of silence, such that the story \nitself started at 0:15 and ended at 7:17, for a duration 422 seconds, with 13 seconds of silence at the end of the scan. \nThe stimulus was started simultaneously with the acquisition of the first functional MRI volume, and the scans \ncomprise 300 TRs, matching the duration of the stimulus. The transcript for the “Pie Man” story stimulus con-\ntains 957 words; 3 words (0.3%) were not successfully localized by the forced-alignment algorithm, and 19 words \n8Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\n(2.0%) were not found in the vocabulary (e.g. proper names). The “Pie Man” data were in some cases collected \nin conjunction with temporally scrambled versions of the “Pie Man” stimulus, as well the “Tunnel Under the \nWorld” and “Lucy” datasets among others, and share subjects with these datasets (as specified in the partici-\npants.tsv file). The current release only includes data collected for the intact story stimulus (rather than the \ntemporally scrambled versions of the stimulus). The following subjects received the “Pie Man” stimulus on two \nseparate occasions (specified by run-1 or run-2 in the BIDS file naming convention): sub-001, sub-\n002, sub-003, sub-004, sub-005, sub-006, sub-008, sub-010, sub-011, sub-012, sub-013, \nsub-014, sub-015, sub-016. We recommend excluding subjects sub-001 (both run-1 and run-2), \nsub-013 (run-2), sub-014 (run-2), sub-021, sub-022, sub-038, sub-056, sub-068, and \nsub-069 (as specified in the scan_exclude.json file in the code/ directory) based on criteria explained \nin the “Intersubject correlation” section of “Data validation. ” Subsets of the “Pie Man” data have been previously \nreported in numerous publications\n109,115,119,190–198, and additional datasets not shared here have been collected \nusing the “Pie Man” auditory story stimulus199–202. In the filename convention (and figures), “Pie Man” is labeled \nusing the task alias pieman.\n“tunnel Under the World”. The “Tunnel Under the World” dataset was collected between May, 2012, and \nFebruary, 2013, and comprised 23 participants (ages 18–31 years, mean age 22.5 ± 3.8 years, 14 reported female). \nThe “Tunnel Under the World” science-fiction story was authored by Frederik Pohl in 1955 which was broadcast \nin 1956 as part of the X Minus One series, a collaboration between the National Broadcasting Company and \nGalaxy Science Fiction magazine (freely available at https://www.oldtimeradiodownloads.com). The “Tunnel \nUnder the World” audio stimulus is 1534 seconds (~25.5 minutes) long. The stimulus was started after the first \ntwo functional MRI volumes (2 TRs, 3 seconds) were collected, with ~23 seconds of silence after the stimulus. The \nfunctional scans comprise 1040 TRs (1560 seconds), except for subjects sub-004 and sub-013 which have \n1035 and 1045 TRs respectively. The transcript for “Tunnel Under the World” contains 3,435 words; 126 words \n(3.7%) were not successfully localized by the forced-alignment algorithm and 88 words (2.6%) were not found \nin the vocabulary. The “Tunnel Under the World” and “Lucy” datasets contain largely overlapping samples of \nsubjects, though were collected in separate sessions, and were, in many cases, collected alongside “Pie Man” scans \n(as specified in the participants.tsv file). We recommend excluding the “Tunnel Under the World” scans \nfor subjects sub-004 and sub-013 (as specified in the scan_exclude.json file in the code/  \ndirectory). The “Tunnel Under the World” data have been previously reported by Lositsky and colleagues\n203. In \nthe filename convention, “Tunnel Under the World” is labeled using the task alias tunnel.\n“Lucy”. The “Lucy” dataset was collected between October, 2012 and January, 2013 and comprised 16 partic-\nipants (ages 18–31 years, mean age 22.6 ± 3.9 years, 10 reported female). The “Lucy” story was broadcast by the \nnon-profit WNYC public radio in 2010 (freely available at https://www.wnycstudios.org/story/91705-lucy). The \n“Lucy” audio stimulus is 542 seconds (~9 minutes) long and was started after the first two functional MRI volumes \n(2 TRs, 3 seconds) were collected. The functional scans comprise 370 TRs (555 seconds). The transcript for “Lucy” \ncontains 1,607 words; 22 words (1.4%) were not successfully localized by the forced-alignment algorithm and \n27 words (1.7%) were not found in the vocabulary. The “Lucy” and “Tunnel Under the World” datasets contain \nlargely overlapping samples of subjects, and were acquired contemporaneously with “Pie Man” data. We recom-\nmend excluding the “Lucy” scans for subjects sub-053 and sub-065 (as specified in the scan_exclude.\njson file in the code/ directory). The “Lucy” data have not previously been reported. In the filename conven-\ntion, “Lucy” is labeled using the task alias lucy.\n“Pretty Mouth and Green My Eyes”. The “Pretty Mouth and Green My Eyes” dataset was collected \nbetween March, 2013, and October, 2013, and comprised 40 participants (ages 18–34 years, mean age 21.4 ± 3.5 \nyears, 19 reported female). The “Pretty Mouth and Green My Eyes” story was authored by J. D. Salinger for \nThe New Y orker magazine (1951) and subsequently published in the Nine Stories collection (1953). The spoken \nstory stimulus used for data collection was based on an adapted version of the original text that was shorter and \nincluded a few additional sentences, and was read by a professional actor. The “Pretty Mouth and Green My Eyes” \naudio stimulus is 712 seconds (~11.9 minutes) long and began with 18 seconds of neutral introductory music \nfollowed by 3 seconds of silence, such that the story started at 0:21 (after 14 TRs) and ended at 11:37, for a dura-\ntion of 676 seconds (~451 TRs), with 15 seconds (10 TRs) of silence at the end of the scan. The functional scans \ncomprised 475 TRs (712.5 seconds). The transcript for “Pretty Mouth and Green My Eyes” contains 1,970 words; \n7 words (0.4%) were not successfully localized by the forced-alignment algorithm and 38 words (1.9%) were not \nfound in the vocabulary.\nThe “Pretty Mouth and Green My Eyes” stimulus was presented to two groups of subjects in two different \nnarrative contexts according to a between-subject experimental design: (a) in the “affair” group, subjects read a \nshort passage implying that the main character was having an affair; (b) in the “paranoia” group, subjects read a \nshort passage implying that the main character’s friend was unjustly paranoid (see Y eshurun et al., 2017, for the \nfull prompts). The two experimental groups were randomly assigned such that there were 20 subjects in each \ngroup and each subject only received the stimulus under a single contextual manipulation. The group assign-\nments are indicated in the participants.tsv file, and the scans.tsv file for each subject. Both groups \nreceived the identical story stimulus despite receiving differing contextual prompts. Immediately following the \nscans, subjects completed a questionnaire assessing comprehension of the story. The questionnaire comprised 27 \ncontext-independent and 12 context-dependent questions (39 questions in total). The resulting comprehension \nscores are reported as the proportion of correct answers (ranging 0–1) in the participants.tsv file and \nscans.tsv file for each subject. We recommend excluding the “Pretty Mouth and Green My Eyes” scans for \nsubjects sub-038 and sub-105 (as specified in the scan_exclude.json file in the code/directory). The \n9Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\n“Pretty Mouth and Green My Eyes” data have been reported in existing publications109,204. In the filename conven-\ntion, “Pretty Mouth and Green My Eyes” is labeled using the task alias prettymouth.\n“Milky Way”. The “Milky Way” dataset was collected between March, 2013, and April, 2017, and comprised \n53 participants (ages 18–34 years, mean age 21.7 ± 4.1 years, 27 reported female). The “Milky Way” story stimuli \nwere written by an affiliate of the experimenters and read by a member of the Princeton Neuroscience Institute \nnot affiliated with the experimenters’ lab. There are three versions of the “Milky Way” story capturing three dif-\nferent experimental conditions: (a) the original story (labeled original), which describes a man who visits a \nhypnotist to overcome his obsession with an ex-girlfriend, but instead becomes fixated on Milky Way candy bars; \n(b) an alternative version (labeled vodka) where sparse word substitutions yield a narrative in which a woman \nobsessed with an American Idol judge visits a psychic and becomes fixated on vodka; (c) a control version (labeled \nsynonyms) with a similar number of word substitutions to the vodka version, but instead using synonyms of \nwords in the original version, yielding a very similar narrative to the original. Relative to the original version, both \nthe vodka and synonyms versions substituted on average 2.5 ± 1.7 words per sentence (34 ± 21% of words \nper sentence). All three versions were read by the same actor and each sentence of the recording was temporally \naligned across versions. All three versions of the “Milky Way” story were 438 seconds (7.3 min; 292 TRs) long and \nbegan with 18 seconds of neutral introductory music followed by a 3 seconds of silence (21 TRs total), such that \nthe story started at 0:21 and ended at 7:05, for a duration 404 s, with 13 seconds of silence at the end of the scan. \nThe functional runs comprised 297 volumes (444.5 seconds). The transcript for the original version of the \nstimulus contains 1,059 words; 7 words (0.7%) were not successfully localized by the forced-alignment algorithm \nand 16 words (1.5%) were not found in the vocabulary. The vodka version contains 1058 words; 2 words (0.2%) \nwere not successfully localized and 21 words (2.0%) were not found in the vocabulary. The synonyms version \ncontains 1,066 words; 10 words (0.9%) were not successfully localized and 13 words (1.2%) were not found in the \nvocabulary.\nThe three versions of the stimuli were assigned to subjects according to a between-subject design, such that \nthere were 18 subjects in each of the three groups, and each subject received only one version of the story. The \ngroup assignments are indicated in the participants.tsv file, and the scans.tsv file for each subject. \nThe stimulus filename includes the version (milkywayoriginal, milkywayvodka, milkywaysyno-\nnyms) and is specified in the events.tsv file accompanying each run. The data corresponding to the orig-\ninal and vodka versions were collected between March and October, 2013, while the synonyms data were \ncollected in March and April, 2017. Subjects completed a 28-item questionnaire assessing story comprehension \nfollowing the scanning session. The comprehension scores for the original and vodka groups are reported \nas the proportion of correct answers (ranging 0–1) in the participants.tsv file and scans.tsv file for \neach subject. We recommend excluding “Milky Way” scans for subjects sub-038, sub-105, and sub-123 (as \nspecified in the scan_exclude.json file in the code/ directory). The “Milky Way” data have been previ-\nously reported\n197. In the filename convention, “Milky Way” is labeled using the task alias milkyway.\n“Slumlord” and “Reach for the Stars One Small Step at a time”. The “Slumlord” and “Reach for the \nStars One Small Step at a Time” dataset was collected between May, 2013, and October, 2013, and comprised 18 \nparticipants (ages 18–27 years, mean age 21.0 ± 2.3 years, 8 reported female). The “Slumlord” story was told by \nJack Hitt and recorded live at The Moth, a non-profit storytelling event, in New Y ork City in 2006 (freely available \nat https://themoth.org/stories/slumlord). The “Reach for the Stars One Small Step at a Time” story was told by \nRichard Garriott and also recorded live at The Moth in New Y ork City in 2010 (freely available at https://themoth.\norg/stories/reach-for-the-stars). The combined audio stimulus is 1,802 seconds (~30 minutes) long in total and \nbegins with 22.5 seconds of music followed by 3 seconds of silence. The “Slumlord” story begins at approximately \n0:25 relative to the beginning of the stimulus file, ends at 15:28, for a duration of 903 seconds (602 TRs), and is \nfollowed by 12 seconds of silence. After another 22  seconds of music starting at 15:40, the “Reach for the Stars \nOne Small Step at a Time” story starts at 16:06 (965 seconds; relative to the beginning of the stimulus file), ends \nat 29:50, for a duration of 825 seconds (~550 TRs), and is followed by 12 seconds of silence. The stimulus file was \nstarted after 3 TRs (4.5 seconds) as indicated in the events.tsv files accompanying each scan. The scans were \nacquired with a variable number of trailing volumes following the stimulus across subjects, but can be truncated \nas researchers see fit (e.g. to 1205 TRs). The transcript for the combined “Slumlord” and “Reach for the Stars One \nSmall Step at a Time” stimulus contains 5,344 words; 116 words (2.2%) were not successfully localized by the \nforced-alignment algorithm, and 57 words (1.1%) were not found in the vocabulary. The transcript for “Slumlord” \ncontains 2,715 words; 65 words (2.4%) were not successfully localized and 25 words (0.9%) were not found in \nthe vocabulary. The transcript for “Reach for the Stars One Small Step at a Time” contains 2,629 words; 51 words \n(1.9%) were not successfully localized and 32 words (1.2%) were not found in the vocabulary. We recommend \nexcluding sub-139 due to a truncated acquisition time (as specified in the scan_exclude.json file in the \ncode/ directory).\nAfter the scans, each subject completed an assessment of their comprehension of the “Slumlord” story. To \nevaluate comprehension, subjects were presented with a written transcript of the “Slumlord” story with 77 blank \nsegments, and were asked to fill in the omitted word or phrase for each blank (free response) to the best of their \nability. The free responses were evaluated using Amazon Mechanical Turk (MTurk) crowdsourcing platform. \nMTurk workers rated the response to each omitted segment on a scale from 0–4 of relatedness to the correct \nresponse, where 0 indicates no response provided, 1 indicates an incorrect response unrelated to the correct \nresponse, and 4 indicates an exact match to the correct response. The resulting comprehension scores are reported \nas the proportion out of a perfect score of 4 (ranging 0–1) in the participants.tsv file and scans.tsv \nfile for each subject. Comprehension scores for “Reach for the Stars One Small Step at a Time” are not provided. \n10Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nThe “Slumlord” and “Reach for the Stars One Small Step at a Time” data have not been previously reported; how-\never, a separate dataset not included in this release has been collected using the “Slumlord” story stimulus205. In \nthe filename convention, the combined “Slumlord” and “Reach for the Stars One Small Step at a Time” data are \nlabeled slumlordreach, and labeled slumlord and reach when analyzed separately.\n“It’s Not the Fall that Gets You”.  The “It’s Not the Fall that Gets Y ou” dataset was collected between \nMay, 2013, and October, 2013, and comprised 56 participants (ages 18–29 years, mean age 21.0 ± 2.4 years, 31 \nreported female). The “It’s Not the Fall that Gets Y ou” story was told by Andy Christie and recorded live at The \nMoth, a non-profit storyteller event, in New Y ork City in 2009 (freely available at https://themoth.org/stories/\nits-not-the-fall-that-gets-you ). In addition to the original story (labeled intact), two experimental manipu -\nlations of the story stimulus were created: (a ) in one version, the story stimulus was temporally scrambled at a \ncoarse level (labeled longscram); ( b) in the other version, the story stimulus was temporally scrambled at a \nfiner level (labeled shortscram). In both cases, the story was spliced into segments at the level of sentences \nprior to scrambling, and segment boundaries used for the longscram condition are a subset of the boundaries \nused for the shortscram condition. The boundaries used for scrambling occurred on multiples of 1.5 seconds \nto align with the TR during scanning acquisition. We recommend using only the intact version for studying \nnarrative processing because the scrambled versions do not have a coherent narrative structure (by design). All \nthree versions of the “It’s Not the Fall that Gets Y ou” stimulus were 582 seconds (9.7 minutes) long and began \nwith 22 seconds of neutral introductory music followed by 3 seconds of silence such that the story started at 0:25 \n(relative to the beginning of the stimulus file) and ended at 9:32 for a duration of 547 seconds (365 TRs), with \n10 seconds of silence at the end of the stimulus. The stimulus file was started after 3 TRs (4.5 seconds) as indicated \nin the events.tsv files accompanying each scan. The functional scans comprised 400 TRs (600 seconds). The \ntranscript for the intact stimulus contains 1,601 words total; 40 words (2.5%) were not successfully localized \nby the forced-alignment algorithm and 20 words (1.2%) were not found in the vocabulary.\nThe scrambled stimuli were assigned to subjects according to a mixed within- and between-subject design, \nsuch that subjects received the intact stimulus and either the longscram stimulus (23 participants) or the \nshortscram stimulus (24 participants). The group assignments are indicated in the participants.tsv file, \nand the scans.tsv file for each subject. Due to the mixed within- and between-subject design, the files are \nnamed with the full notthefallintact, notthefalllongscram, notthefallshortscram task \nlabels. We recommend excluding the intact scans for sub-317 and sub-335, the longscram scans for \nsub-066 and sub-335, and the shortscram scan for sub-333 (as specified in the scan_exclude.json \nfile in the code/ directory). The “It’s Not the Fall that Gets Y ou” data have been recently reported\n206.\n“Merlin” and “Sherlock”. The “Merlin” and “Sherlock” datasets were collected between May, 2014, and \nMarch, 2015, and comprised 36 participants (ages 18–47 years, mean age 21.7 ± 4.7 years, 22 reported female). \nThe “Merlin” and “Sherlock” stimuli were recorded while an experimental participant recalled events from pre-\nviously viewed television episodes during fMRI scanning. Note that this spontaneous recollection task (during \nfMRI scanning) is cognitively distinct from reciting a well-rehearsed story to an audience, making this dataset \ndifferent from others in this release. This release contains only the data for subjects listening to the auditory verbal \nrecall; not data for the audiovisual stimuli or the speaker’s fMRI data\n207. The “Merlin” stimulus file is 915 seconds \n(9.25 minutes) long and began with 25 seconds of music followed by 4 seconds of silence such that the story \nstarted at 0:29 and ended at 15:15 for a duration of 886 seconds (591 TRs). The transcript for the “Merlin” stimulus \ncontains 2,245 words; 111 words (4.9%) were not successfully localized by the forced-alignment algorithm and \n13 words (0.6%) were not found in the vocabulary. The “Sherlock” stimulus file is 1,081 seconds (~18 minutes) \nlong and began with 25 seconds of music followed by 4 seconds of silence such that the story started at 0:29 and \nended at 18:01 for a duration of 1,052 seconds (702 TRs). The stimulus files for both stories were started after 3 \nTRs (4.5 seconds) as indicated in the events.tsv files accompanying each scan. The “Merlin” and “Sherlock” \nscans were acquired with a variable number of trailing volumes following the stimulus across subjects, but can be \ntruncated as researchers see fit (e.g. to 614 and 724 TRs, respectively). The transcript for “Sherlock” contains 2681 \nwords; 171 words (6.4%) were not successfully localized and 17 words (0.6%) were not found in the vocabulary. \nThe word counts for “Merlin” and “Sherlock” may be slightly inflated due to the inclusion of disfluencies (e.g. \n“uh”) in transcribing the spontaneous verbal recall.\nAll 36 subjects listened to both the “Merlin” and “Sherlock” verbal recall auditory stimuli. However, 18 sub-\njects viewed the “Merlin” audiovisual clip prior to listening to both verbal recall stimuli, while the other 18 \nsubjects viewed the “Sherlock” audiovisual clip prior to listening to both verbal recall stimuli. In the “Merlin” \ndataset, the 18 subjects that viewed the “Sherlock” clip (and not the “Merlin” audiovisual clip) are labeled with \nthe naive condition as they heard the “Merlin” verbal recall auditory-only stimulus without previously having \nseen the “Merlin” audiovisual clip. The other 18 subjects in the “Merlin” dataset viewed the “Merlin” audiovisual \nclip (and not the “Sherlock” clip) prior to listening to the “Merlin” verbal recall and are labeled with the movie \ncondition. Similarly, in the “Sherlock” dataset, the 18 subjects that viewed the “Merlin” audiovisual clip (and \nnot the “Sherlock” audiovisual clip) are labeled with the naive  condition, while the 18 subjects that viewed \nthe “Sherlock” audiovisual clip (and not the “Merlin” audiovisual clip) are labeled with the movie condition. \nThese condition labels are indicated in both the participants.tsv file and the scans.tsv files for each \nsubject. Following the scans, each subject performed a free recall task to assess memory of the story. Subjects \nwere asked to write down events from the story in as much detail as possible with no time limit. The quality of \ncomprehension for the free recall text was evaluated by three independent raters on a scale of 1–10. The resulting \ncomprehension scores are reported as the sum across raters normalized by the perfect score (range 0–1) in the \nparticipants.tsv file and scans.tsv file for each subject. Comprehension scores are only provided for \nthe naive condition. We recommend excluding the “Merlin” scan for subject sub-158 and the “Sherlock” \n11Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nscan for sub-139 (as specified in the scan_exclude.json file in the code/ directory. The “Merlin” and \n“Sherlock” data have been previously reported207. In the filename convention, “Merlin” is labeled merlin and \n“Sherlock” is labeled sherlock.\n“Schema”. The “Schema” dataset was collected between August, 2015, and March, 2016, and comprised 31 \nparticipants (ages 18–35 years, mean age 23.7 ± 4.8 years, 13 reported female). The “Schema” dataset comprised \neight brief (~3-minute) auditory narratives based on popular television series and movies featuring restaurants \nand airport scenes: The Big Bang Theory, Friends, How I Met Your Mother, My Cousin Vinnny, The Santa Clause, \nShame, Seinfeld, Up in the Air; labeled bigbang, friends, himym, vinny, santa, shame, seinfeld, \nupintheair, respectively. These labels are indicated in the events.tsv accompanying each scanner run, and \nthe corresponding stimuli (e.g. bigbang_audio.wav). Two of the eight stories were presented in each of \nfour runs. Each run had a fixed presentation order of a fixed set of stories, but the presentation order of the four \nruns was counterbalanced across subjects. The functional scans were acquired with a variable number of trailing \nvolumes across subjects, and in each run two audiovisual film clips (not described in this data release) were also \npresented; the auditory stories must be spliced from the full runs according to the events.tsv file. Overall, the \ntranscripts for the auditory story stimuli in the “Schema” dataset contain 3,788 words; 20 words (0.5%) were not \nsuccessfully localized by the forced-alignment algorithm and 69 words (1.8%) were not found in the vocabulary. \nThe “Schema” data were previously reported\n116. In the filename convention, the “Schema” data are labeled using \nthe task alias schema.\n“Shapes”. The “Shapes” dataset was collected between May, 2015, and July, 2016, and comprised 59 partici-\npants (ages 18–35 years, mean age 23.0 ± 4.5 years, 42 reported female). The “Shapes” dataset includes two related \nauditory story stimuli describing a 7-minute original animation called “When Heider Met Simmel” (copyright \nY ale University). The movie uses two-dimensional geometric shapes to tell the story of a boy who dreams about \na monster, and was inspired by Heider and Simmel\n208. The movie itself is available for download at https://www.\nheadspacestudios.org. The two verbal descriptions based on the movie used as auditory stimuli in the “Shapes” \ndataset were: (a) a purely physical description of the animation (labeled shapesphysical), and (b ) a social \ndescription of the animation intended to convey intentionality in the animated shapes (labeled shapesso-\ncial). Note that the physical description story (shapesphysical) is different from other stimuli in this \nrelease in that it describes the movements of geometrical shapes without any reference to the narrative embedded \nin the animation. Both stimulus files are 458  seconds (7.6 minutes) long and began with a 37-second introduc-\ntory movie (only the audio channel is provided in the stimulus files) followed by 7 seconds of silence such that \nthe story started at 0:45 and ended at roughly 7:32 for a duration of ~408 seconds (~272 TRs) and ended with \n~5 seconds of silence. The stimulus files for both stories were started after 3 TRs (4.5 seconds) as indicated in the \nevents.tsv files accompanying each scan. The functional scans were acquired with a variable number of trail-\ning volumes following the stimulus across subjects, but can be truncated as researchers see fit (e.g. to 309 TRs). \nThe transcript for the shapesphysical stimulus contains 951 words; 9 words (0.9%) were not successfully \nlocalized by the forced-alignment algorithm and 25 words (2.6%) were not found in the vocabulary. The tran-\nscript for the shapessocial stimulus contains 910 words; 6 words (0.7%) were not successfully localized and \n14 words (1.5%) were not found in the vocabulary.\nEach subject received both the auditory physical description of the animation (shapesphysical), the \nauditory social description of the animation (shapessocial), and the audiovisual animation itself (not \nincluded in the current data release) in three separate runs. Run order was counterbalanced across participants, \nmeaning that some participants viewed the animation before hearing the auditory description. The run order \nfor each subject (e.g. physical-social-movie) is specified in the participants.tsv file and the \nsubject-specific scans.tsv files. Immediately following the story scan, subjects performed a free recall task in \nthe scanner (the fMRI data during recall are not provided in this release). They were asked to describe the story \nin as much detail as possible using their own words. The quality of the recall was evaluated by an independent \nrater naive to the purpose of the experiment on a scale of 1–10. The resulting comprehension scores are reported \nnormalized by 10 (range 0–1) and reported in the participants.tsv file and scans.tsv file for each \nsubject. Comprehension scores are only provided for scans where the subject first listened to the story stimulus, \nprior to hearing the other version of the story or viewing the audiovisual stimulus. We recommend excluding the \nshapessocial scan for subject sub-238 (as specified in the scan_exclude.json file in the code/  \ndirectory. The “Shapes” data were previously reported\n209.\n“The 21st Year”. The “The 21st Y ear” dataset was collected between February, 2016, and January, 2018, and \ncomprised 25 participants (ages 18–41 years, mean age 22.6 ± 4.7 years, 13 reported female). The story stimulus \nwas written and read aloud by author Christina Lazaridi, and includes two seemingly unrelated storylines relayed \nin blocks of prose that eventually fuse into a single storyline. The stimulus file is 3,374 seconds (56.2 minutes) long \nand began with 18 seconds of introductory music followed by 3 seconds such that the story started at 0:21 and \nended at 55:59 for a duration of 3,338 seconds (2,226 TRs) followed by 15 seconds of silence. The functional scans \ncomprised 2,249 volumes (3,373.5 seconds). The story stimulus contains 8,267 words; 88 words (1.1%) were not \nsuccessfully localized by the forced-alignment algorithm and 143 words (1.7%) were not found in the vocabulary. \nData associated with “The 21st Y ear” have been previously reported\n210. In the filename convention, data for “The \n21st Y ear” are labeled using the task alias 21styear.\n“Pie Man (PNI)” and “Running from the Bronx”. The “Pie Man (PNI)” and “Running from the Bronx” \ndata were collected between May and September, 2018, and comprised 47 participants (ages 18–53 years, mean \n12Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nage 23.3 ± 7.4 years, 33 reported female). The “Pie Man (PNI)” and “Running from the Bronx” story stimuli were \ntold by Jim O’Grady while undergoing fMRI scans at the Princeton Neuroscience Institute (PNI). The spoken \nstory was recorded using a noise cancelling microphone mounted on the head coil and scanner noise was min-\nimized using Audacity. The “Pie Man (PNI)” stimulus file is 415 seconds (~7 minutes) long and begins with a \nperiod of silence such that the story starts at :09 and ends at 6:49 for a duration of 400 seconds (267 TRs) followed \nby 7 seconds of silence. The “Pie Man (PNI)” stimulus conveys the same general story as the original “Pie Man” \nstimulus but with differences in timing and delivery (as well as poorer audio quality due to being recorded during \nscanning). The “Running from the Bronx” stimulus file is 561 seconds (9.4 minutes) long and begins with a period \nof silence such that the story starts at 0:15 and ends at 9:11 for a duration of 536 seconds (358 TRs) followed by \n10 seconds of silence. The “Pie Man (PNI)” and “Running from the Bronx” functional scans comprised 294 TRs \n(441 seconds) and 390 TRs (585 seconds), respectively. Both stimulus files were started after 8 TRs (12 seconds) \nas indicated in the accompanying events.tsv files. The program card on the scanner computer was organized \naccording to the ReproIn convention\n211 to facilitate automated conversion to BIDS format using HeuDiConv \n0.5.dev1212. The transcript for the “Pie Man (PNI)” stimulus contains 992 words; 7 words (0.7%) were not suc-\ncessfully localized by the forced-alignment algorithm and 19 words (1.9%) were not found in the vocabulary. The \ntranscript for “Running from the Bronx” contains 1,379 words; 23 words (1.7%) were not successfully localized \nand 19 words (1.4%) were not found in the vocabulary.\nAt the end of the scanning session, the subjects completed separate questionnaires for each story to evaluate \ncomprehension. Each questionnaire comprised 30 multiple choice and fill-in-the-blank questions testing memory \nand understanding of the story. The resulting comprehension scores are reported as the proportion of correct \nanswers (ranging 0–1) in the participants.tsv file and the scans.tsv file for each subject. Data for both \nstories have been previously reported\n166,213. The “Pie Man (PNI)” and “Running from the Bronx” data are labeled \nwith the piemanpni and bronx task aliases in the filename convention. The “Pie Man (PNI)” and “Running \nfrom the Bronx” data were collected in conjunction with the “I Knew Y ou Were Black” and “The Man Who Forgot \nRay Bradbury” data and share the same samples of subjects.\n“I Knew You Were Black” and “the Man Who Forgot Ray Bradbury”. The “I Knew Y ou Were Black” \nand “The Man Who Forgot Ray Bradbury” data were collected by between May and September, 2018, and com-\nprised the same sample of subjects from “Pie Man (PNI)” and “Running from the Bronx” data (47 participants, \nages 18–53 years, mean age 23.3 ± 7.4 years, 33 reported female). Unlike the “Pie Man (PNI)” and “Running \nfrom the Bronx” story stimuli, the “I Knew Y ou Were Black” and “The Man Who Forgot Ray Bradbury” stim-\nuli are professionally recorded with high audio quality. The “I Knew Y ou Were Black” story was told by Carol \nDaniel and recorded live at The Moth, a non-profit storytelling event, in New Y ork City in 2018 (freely availa-\nble at https://themoth.org/stories/i-knew-you-were-black). The “I Knew Y ou Were Black” story is 800 seconds \n(13.3 minutes, 534 TRs) long and occupies the entire stimulus file. The “I Knew Y ou Were Black” functional \nscans comprised 550 TRs (825 seconds). The transcript for the “I Knew Y ou Were Black” stimulus contains 1,544 \nwords; 5 words (0.3%) were not successfully localized by the forced-alignment algorithm and 4 words (0.3%) were \nnot found in the vocabulary. “The Man Who Forgot Ray Bradbury” was written and read aloud by author Neil \nGaiman at the Aladdin Theater in Portland, OR, in 2011 (freely available at https://soundcloud.com/neilgaiman/\nthe-man-who-forgot-ray-bradbury). The “The Man Who Forgot Ray Bradbury” audio stimulus file is 837 seconds \n(~14 minutes, 558 TRs) long and occupies the entire stimulus file. The “The Man Who Forgot Ray Bradbury” \nfunctional scans comprised 574 TRs (861 seconds). The transcript for “The Man Who Forgot Ray Bradbury” \ncontains 2,135 words; 16 words (0.7%) were not successfully localized and 29 words (1.4%) were not found in the \nvocabulary. For both datasets, the audio stimulus was prepended by 8 TRs (12 seconds) and followed by 8 TRs \n(12 seconds) of silence. The program card on the scanner computer was organized according to the ReproIn con-\nvention\n211 to facilitate automated conversion to BIDS format using HeuDiConv 0.5.dev1212.\nSimilarly to the “Pie Man (PNI)” and “Running from the Bronx” stories, the subjects completed separate ques-\ntionnaires for each story to evaluate comprehension after scanning. Each questionnaire comprised 25 multiple \nchoice and fill-in-the-blank questions testing memory and understanding of the story. The resulting comprehen-\nsion scores are reported as the proportion of correct answers (ranging 0–1) in the participants.tsv file \nand the scans.tsv file for each subject. Data for both stories have been previously reported\n166,213. The “I Knew \nY ou Were Black” and “The Man Who Forgot Ray Bradbury” data are labeled with the black and forgot task \naliases in the filename convention.\nData Records\nThe core, unprocessed NIfTI-formatted MRI data with accompanying metadata and stimuli are publicly avail-\nable on OpenNeuro: https://openneuro.org/datasets/ds002345 (https://doi.org/10.18112/openneuro.ds002345.\nv1.1.4)\n85. All data and derivatives are hosted at the International Neuroimaging Data-sharing Initiative (INDI)214 \n(RRID:SCR_00536) via the Neuroimaging Informatics Tools and Resources Clearinghouse (NITRC)215 (https://\nwww.nitrc.org/; RRID:SCR_003430): http://fcon_1000.projects.nitrc.org/indi/retro/Narratives.html (https://doi.\norg/10.15387/fcp_indi.retro.Narratives)\n216. The full data collection is available via the DataLad data distribution \n(RRID:SCR_019089): http://datasets.datalad.org/?dir=/labs/hasson/narratives.\nData and derivatives have been organized according to the machine-readable Brain Imaging Data Structure \n(BIDS) 1.2.1 86, which follows the FAIR principles for making data findable, accessible, interoperable, and \nreusable 217. A detailed description of the BIDS specification can be found at http://bids.neuroimaging.io/. \nOrganizing the data according to the BIDS convention facilitates future research by enabling the use of automated \nBIDS-compliant software (BIDS Apps)\n218. Briefly, files and metadata are labeled using key-value pairs where key \nand value are separated by a hyphen, while key-value pairs are separated by underscores. Each subject is identified \n13Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nby an anonymized numerical identifier (e.g. sub-001). Each subject participated in one or more story-listening \nscans indicated by the task alias for the story (e.g. task-pieman). Subject identifiers are conserved across data-\nsets; subjects contributing to multiple datasets are indexed by the same identifier (e.g. sub-001 contributes to \nboth the “Pie Man” and “Tunnel Under the World” datasets).\nThe top-level BIDS-formatted narratives/ directory contains a tabular participants.tsv file where \neach row corresponds to a subject and includes demographic variables (age, sex), the stories that subject \nreceived (task), as well as experimental manipulations (condition) and comprehension scores where avail-\nable (comprehension). Cases of omitted, missing, or inapplicable data are indicated by “n/a” according to the \nBIDS convention. The top-level BIDS directory also contains the following: (a) a dataset_description.\njson containing top-level metadata, including licensing, authors, acknowledgments, and funding sources; (b) a \ncode/ directory containing scripts used for analyzing the data; (c) a stimuli/ directory containing the audio \nstimulus files (e.g. pieman_audio.wav), transcripts (e.g. pieman_transcript.txt), and a gentle/ \ndirectory containing time-stamped transcripts created using Gentle (i.e.; described in “Stimuli” in the “Methods” \nsection); (d) a derivatives/ directory containing all MRI derivatives (described in more detail in the “MRI \npreprocessing” and “Technical validation” sections); and (e) descriptive README and CHANGES files.\nBIDS-formatted MRI data for each subject are contained in separate directories named according to the \nsubject identifiers. Within each subject’s directory, data are segregated into anatomical data (the anat/ direc-\ntory) and functional data (the func/ directory). All MRI data are stored as gzipped (compressed) NIfTI-1 \nimages\n219. NIfTI images and the relevant metadata were reconstructed from the original Digital Imaging and \nCommunications in Medicine (DICOM) images using dcm2niix 1.0.20180518 220. In instances where multiple \nscanner runs were collected for the same imaging modality or story in the same subject, the files are differentiated \nusing a run label (e.g. run-1, run-2). All anatomical and functional MRI data files are accompanied by sidecar \nJSON files containing MRI acquisition parameters in compliance with the COBIDAS report\n133. Identifying meta-\ndata (e.g. name, birth date, acquisition date and time) have been omitted, and facial features have been removed \nfrom anatomical images using the automated de-facing software pydeface 2.0.0\n134. All MRI data and metadata are \nreleased under the Creative Commons CC0 license (https://creativecommons.org/), which allows for free reuse \nwithout restriction.\nAll data and derivatives are version-controlled using DataLad\n88,89 (https://www.datalad.org/; RRID:SCR_003931).  \nDataLad uses git (https://git-scm.com/; RRID:SCR_003932) and git-annex (https://git-annex.branchable.com/; \nRRID:SCR_019087) to track changes made to the data, providing a transparent history of the dataset. Each stage \nof the dataset’s evolution is indexed by a unique commit hash and can be interactively reinstantated. Major sub-\ndivisions of the data collection, such as the code/, stimuli/, and derivative directories, are encapsulated as \n“subdatasets” nested within the top-level BIDS dataset. These subdatasets can be independently manipulated \nand have their own standalone version history. For a primer on using DataLad, we recommend the DataLad \nHandbook\n221 (http://handbook.datalad.org/).\ntechnical Validation\nImage quality metrics. To provide an initial assessment of the data, we used MRIQC 0.15.1222,223 to derive \na variety of image quality metrics (IQMs; via the run_mriqc.sh and run_mriqc_group.sh scripts \nin the code/ directory). MRIQC was deployed using Singularity\n140. The MRIQC Singularity image can be \nbuilt from Docker Hub (https://hub.docker.com/r/poldracklab/mriqc/; e.g. singularity build mri-\nqc-0.15.1.simg docker://poldracklab/mriqc:0.15.1). The MRIQC outputs can be found in \nthe mriqc/ directory in derivatives/ available via the DataLad release, and includes dozens of IQMs per \nscan as well as a summary across the entire dataset. These IQMs can be visualized in a browser (e.g. group_\nbold.html) and are aggregated in tabular format across the entire data collection (e.g. group_bold.tsv). \nHere we do not exclude any subjects based on IQMs, but subsequent researchers can use the available IQMs to \nexclude scans as they see fit.\nWe briefly report three common metrics for functional images summarizing head motion, intrinsic spatial \nsmoothness, and temporal signal quality (Fig.  2). To summarize head motion, we computed the mean frame-\nwise displacement (FD)162,224 for each functional scan across all subjects and story stimuli (Fig. 2a). The median \nframewise displacement across scans for all subjects and story stimuli was 0.132 mm (SD = 0.058 mm). Although \nthis indicates relatively low head motion, some subjects have notably high motion; researchers may opt to cen-\nsor time points with high FD (e.g. greater 0.5 mm) or omit entire scans with high overall FD as they see fit. To \nquantify the intrinsic spatial smoothness of the raw functional data, we used 3dFWHMx in AFNI 19.3.0, which \ncomputes the ratio of variance of first differences across voxels to global variance across the image and expresses \nsmoothness as the full width at half-maximum (FWHM) of a 2D Gaussian filter\n225. Smoothness was computed in \neach subject’s native brain space within an anatomically-defined brain mask, and the functional time series were \ntemporally detrended prior to smoothness estimation (using the get_fwhm.py script in the code/ directory). \nSpatial smoothness varied considerably across datasets due to different acquisition parameters used with different \nscanner models (Fig. 2b). For example, scans acquired using older pulse sequences without multiband accelera-\ntion are smoothest in the slice acquisition (z-) axis, whereas newer scans acquired using multiband acceleration \nhave higher overall spatial resolution (resulting in lower smoothness), and are smoothest in anterior–posterior \nphase-encoding (y-) axis. Finally, we computed the temporal signal-to-noise ratio (tSNR) as a measure of func-\ntional signal quality using MRIQC. A tSNR map was constructed for each scan by computing the voxelwise \naverage of the signal magnitude over time divided by the standard deviation of the signal over time in each \nsubject’s native space following realignment using 3dvolreg and brain extraction using 3dSkullStrip in \nAFNI\n226; the median of this tSNR map was used to summarize tSNR across voxels for a given scan (Fig. 2c). The \nmedian tSNR across all subjects and story stimuli was 50.772 (SD = 13.854). We also computed vertex-wise tSNR \nin fsaverage6 space following preprocessing with fMRIPrep to visualize the spatial distribution of tSNR across \n14Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\ncortex (Fig. 2d; using the get_tsnr.py script in the code/ directory). Low tSNR in orbitofrontal and anterior \nmedio-temporal cortex reflects signal dropout due to the proximity of sinuses and ear canals227.\nIntersubject correlation. To measure the reliability of stimulus-evoked responses across subjects, we per-\nformed an intersubject correlation (ISC) analysis 104,105,228. For each story stimulus, we computed ISC using the \nleave-one-out approach: for each subject, we averaged the response time series for the remaining subjects, then \ncorrelated this mean response time series for a given voxel or brain area with the response time series for the \nleft-out subject in that voxel or brain area. Note that this approach yields higher correlation values than com-\nputing ISC between pairs of subjects. In large samples of subjects, leave-one-out ISC provides an estimate of the \nupper bound of variance that can be accounted for by between-subject models predicting voxel- or region-wise \nresponse time series (i.e. noise ceiling)\n105,229; that is, the mean data from other subjects serves as a surrogate \nfor the optimal model that generalizes across subjects (within-subject models, however, may capture additional \nvariance not shared across subjects). ISC was computed only on functional volumes corresponding to presenta-\ntion of the story stimulus, and we trimmed the first 6 TRs (9 seconds) after story onset to reduce the impact of \nFig. 2 Data quality summarized in terms of head motion, spatial smoothness, and temporal signal-to-noise \nratio (tSNR). (a) Median framewise displacement (FD) summarizes head motion. Each black tick mark \nrepresents the median FD (x-axis) for a given scanning run and subject for each task (i.e. story; y-axis). Violin \nplots illustrate the distribution of median FDs across all runs and subjects for a given task. The red markers \nindicate the median FD across all runs for a given task, and the red error bars indicate the 95% bootstrap \nconfidence interval for the median based on randomly sampling runs with replacement. At bottom, FD is \nsummarized across all tasks. (b) Spatial smoothness is summarized using AFNI’s FWHM smoothness for each \nscanning run across each task. Spatial smoothness was computed in volumetric subject-specific functional (EPI) \nspace using subject-specific whole-brain masks with minimal preprocessing (realignment and susceptibility \ndistortion in fMRIPrep, detrending in AFNI). Each tick mark represents the spatial smoothness for a given run \nin a given acquisition axis (orange: x-axis, i.e. left–right; yellow: y-axis, i.e. anterior–posterior; red: z-axis, i.e. \ninferior–superior). Violin plots capture the distribution of smoothness estimates for x-, y-, and z-axes across all \nruns and subjects in a given task. The red markers indicate the median combined (i.e. geometric mean across x-, \ny-, and z-axes) smoothness across all runs and subjects for a given task (red error bars indicate 95% bootstrap \nconfidence interval of median). At bottom, smoothness is summarized across all tasks. The multiple lobes of the \ndistribution reflect differing acquisition parameters used with the Skyra and Prisma scanner models. (c) tSNR \nmaps were constructed by computing the voxelwise mean signal over time divided by the standard deviation \nover time. Each black tick mark represents the median of the tSNR map (x-axis) for a given scanner run and \nsubject for each task (y-axis). Violin plots reflect the distribution of tSNR values across all runs and subjects for \na given task. The red markers indicate the mean tSNR across all runs for a given task (red error bars indicate \n95% bootstrap confidence interval of mean). At bottom, the tSNR is summarized across all tasks. The two lobes \nof the distribution reflect older pulse sequences with larger voxels (and higher tSNR) and newer multiband \npulse sequences with smaller voxels (and lower tSNR). See the plot_qc.py script in the code/ directory for \ndetails. (d) Distribution of tSNR across cortex. The color bar reflects the median vertex-wise tSNR. Note that \nunlike panel c, here tSNR was computed in fsaverage6 space for vertex-wise summarization and visualization, \nwhich may inflate the tSNR values.\n15Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nstimulus onset transients. All ISC values were Fisher z-transformed prior to averaging, and the mean was inverse \nFisher z-transformed for visualization 230. ISC analysis was implemented using the Brain Imaging Analysis Kit \n(BrainIAK)231–233 (RRID:SCR_014824).\nISCs were computed separately for different experimental groups or manipulations; for example, ISCs were \ncomputed separately for the “affair” and “paranoid” contextual manipulation conditions for “Pretty Mouth and \nGreen My Eyes. ” On the other hand, ISCs were computed as a single group for datasets where the only experimen-\ntal manipulation was naively listening to the story stimulus versus listening to the story stimulus after viewing the \naudiovisual movie (“Merlin, ” “Sherlock, ” and “Shapes”). ISCs were computed separately for the “Slumlord” and \n“Reach for the Stars One Small Step at a Time” stories despite being presented in the same scanning run. The brief \n“Schema” stories and the scrambled versions of “It’s Not the Fall that Gets Y ou” were excluded from ISC analysis \nfor the sake of simplicity.\nTo assess the temporal synchronization of responses to the story stimuli, we computed ISC in an early auditory \ncortex (EAC) region of interest (ROI). The left- and right-hemisphere EAC ROIs were defined on the cortical sur-\nface in fsaverage6 space as combination of five areas (A1, MBelt, LBelt, PBelt, RI) from a multimodal cortical par-\ncellation\n234,235 (see the roi_masks.py script in the code/ directory). The non-smoothed response time series \nfollowing preprocessing with fMRIPrep and confound regression with AFNI were averaged across vertices in \nthe left and right EAC ROIs (using the roi_average.py and roi_regression.py scripts in the code/ \ndirectory). We used two measures of temporal alignment in left EAC as quality-control criteria for excluding \nscans. First, we computed leave-one-out ISCs and set an exclusion cut-off at ISC < 0.10 in left EAC (Fig.  3a; \nusing the roi_isc.py script in the code/ directory). Second, we computed leave-one-out ISC at temporal \nlags ranging from −45 to 45 seconds; for each left-out subject, we computed the correlation between the mean \nresponse time series for the remaining subjects and the left-out subject’s response time series at 61 lags ranging \nfrom -30 to 30 TRs (Fig. 3b; using the roi_lags.py script in the code/ directory). We then set an exclusion \ncut-off at a peak ISC at a lag >  ± 3 TRs (4.5 seconds) to accommodate some inter-individual variability in the \nhemodynamic response\n236,237. Both of these criteria are intended to exclude scans with poor entrainment to the \nstimulus (e.g. due to an error in acquisition), and yield a partly overlapping group of scans for exclusion. Overall, \nthe combined criteria resulted in excluding 27 scans from 19 subjects, or 3.0% of the 891 total scans. We do not \nexclude these subjects from the dataset entirely because some analyses (e.g. comparing methods for mitigating \nartefacts) may in fact benefit from the inclusion of lower-quality scans. Rather, we provide a scan_exclude.\njson file listing scans to exclude (and a convenience function exclude_scans.py) in the code/ directory \nso that researchers can easily exclude scans flagged as poor quality according to the criteria above. Note that \nsome studies\n206 temporally shift response time series to ensure that peak lags match across subjects; here, we \ndo not shift or edit the scans, but provide the lagged ISCs for all scans (group_roi-EAC_lags.json in \nderivatives/afni-nosmooth/) so that researchers can apply temporal shifting as they see fit. Ultimately, \nmean ISC in left and right EAC across all scans was 0.549 (SD = 0.173) and 0.493 (SD = 0.179), respectively. This \nanalysis also revealed qualitatively higher ISC in left EAC than right EAC across several stories (Fig. 3a). This may \nreflect a left-lateralized preference for speech sounds observed in previous studies\n10,238,239.\nFinally, we visualized the cortical distribution of ISCs (Fig.  4). Leave-one-out ISCs were computed on \nvertex-wise response time series in fsaverage6 surface space following preprocessing using fMRIPrep, spatial \nFig. 3 Intersubject correlation (ISC) in early auditory cortex (EAC). (a) ISC in left and right EAC across \nall subjects and tasks (i.e. stories). Each tick mark represents the leave-one-out ISC (x-axis) for a left-out \nsubject in either left- (purple) or right- (orange) hemisphere EAC across tasks (y-axis). Violin plots illustrate \nthe distribution of leave-one-out ISC across all left-out subjects for each task. The circular markers indicate \nthe mean ISC across left-out subjects for each task and hemisphere (error bars indicate the 95% bootstrap \nconfidence interval of the mean). At bottom, ISC for left and right EAC is summarized across all tasks. (b) \nLagged ISC captures temporal synchronization in left EAC. Leave-one-out ISC (y-axis) was computed for left \nEAC at 61 lags ranging from -30 to + 30 TRs (−45 to 45 seconds; x-axis). In the upper plot, each line represents \nthe mean ISC across lags for each task. At bottom, all lagged ISCs are visualized in a single plot where each left-\nout subject corresponds to a semi-transparent gray line; the red line reflects the average lagged ISC across all \nsubjects and tasks. See the plot_isc.py script in the code/ directory for details.\n16Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nsmoothing to 6 mm, and confound regression using AFNI (via the run_isc.py script in the code/ direc-\ntory). ISCs were computed separately for each story stimulus (and experimental group or condition, as described \nabove), then averaged across all stories to construct a summary ISC map (Fig. 4a). Here we are not interested in \nstatistical testing and therefore arbitrarily threshold these ISC maps at a mean correlation of 0.10 for visualization; \nwe refer researchers to other work for a detailed statistical treatment\n240–242. Cortical areas with high ISC during \nstory-listening were largely bilateral, spanning temporal and frontal areas reported in the fMRI literature on lan-\nguage, and extending into default-mode network (DMN) areas such as the angular gyrus and precuneus. Cortical \nISC maps are strikingly similar across stories with considerable variability in sample size, duration, and content \n(Fig. 4b). We emphasize that a wide variety of stimulus content can drive cortical ISCs, and that decomposing the \nstimuli according to different models will ultimately provide a window onto the linguistic and narrative features \nencoded across cortex.\nUsage Notes\nThe MRI data are organized according to the BIDS standard, and are therefore well-suited for automated process-\ning using BIDS Apps (e.g. MRIQC, fMRIPrep, FitLins\n243; https://bids-apps.neuroimaging.io/), the Brainlife cloud \nplatform (https://brainlife.io/), and Neuroscout244 (https://neuroscout.org/). The full data collection can be easily \n“installed” using DataLad (datalad install -r ///labs/hasson/narratives). This clones the \nfile hierarchy without downloading any large content files, which can be installed selectively as needed (data-\nlad get). Particular subsets of the full dataset can be retrieved using datalad get with shell wildcards. \nFor example, particular stories can be retrieved using, e.g. datalad get sub-*/func/*task-pieman*; \nor derivatives in a particular output space can be retrieved using, e.g. datalad get derivatives/\nafni-nosmooth/sub-*/func/*task-pieman*space-fsaverage6*. For convenience, we also \ninclude the task_meta.json file in the code directory, which contains a dictionary mapping from each task \n(i.e. story) to subjects and filenames beloging to that task. We also recommend the Python-based PyBIDS tool for \nquerying and manipulating BIDS data\n245.\nCode availability\nAll code used for aggregating and analyzing the data is version-controlled and publicly available via the associated \nGitHub repository (https://github.com/snastase/narratives) and the code/ directory in the top-level BIDS \ndirectory distributed via DataLad (https://datasets.datalad.org/?dir= /labs/hasson/narratives). The GitHub \nrepository contains both scripts used to prepare the data for sharing (staging/) and scripts used to analyze the \nBIDS-formatted data (code/). See Table S1 for a brief description of the scripts used to process the “Narratives” \ndata.\nReceived: 4 March 2021; Accepted: 18 August 2021;\nPublished: xx xx xxxx\nReferences\n 1. Hasson, U., Ghazanfar, A. A., Galantucci, B., Garrod, S. & Keysers, C. Brain-to-brain coupling: a mechanism for creating and \nsharing a social world. Trends Cogn. Sci. 16, 114–121 (2012).\n 2. Berwick, R. C., Friederici, A. D., Chomsky, N. & Bolhuis, J. J. Evolution, brain, and the nature of language. Trends Cogn. Sci. 17, \n89–98 (2013).\n 3. Bolhuis, J. J., Beckers, G. J. L., Huybregts, M. A. C., Berwick, R. C. & Everaert, M. B. H. Meaningful syntactic structure in songbird \nvocalizations? PLoS Biol. 16, e2005157 (2018).\n 4. Townsend, S. W ., Engesser, S., Stoll, S., Zuberbühler, K. & Bickel, B. Compositionality in animals and humans. PLoS Biol. 16, \ne2006425 (2018).\nFig. 4 Intersubject correlations across cortex. (a) The mean vertex-wise leave-one-out ISC was computed across \nall subjects and stories. The color bar reflects the mean leave-one-out ISC and ranges from 0 to 0.60. ISC maps \nwere arbitrarily thresholded at ISC = 0.10. (b) Mean ISC maps for 12 representative stories plotted according to \nthe same color map and thresholded at ISC = 0.10. Where applicable (e.g. “Pretty Mouth and Green My Eyes”), \nISCs were computed separately for different experimental groups or conditions, then averaged; ISCs were also \ncomputed separately for “Slumlord” and “Reach for the Stars One Small Step at a Time, ” then averaged.\n\n17Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\n 5. Hamilton, L. S. & Huth, A. G. The revolution will not be controlled: natural stimuli in speech neuroscience. Lang. Cogn. Neurosci. \n35, 573–582 (2020).\n 6. Hasson, U., Egidi, G., Marelli, M. & Willems, R. M. Grounding the neurobiology of language in first principles: The necessity of \nnon-language-centric explanations for language comprehension. Cognition 180, 135–157 (2018).\n 7. Willems, R. M., Nastase, S. A. & Milivojevic, B. Narratives for neuroscience. Trends Neurosci. 43, 271–273 (2020).\n 8. Bookheimer, S. Functional MRI of language: new approaches to understanding the cortical organization of semantic processing. \nAnnu. Rev. Neurosci. 25, 151–188 (2002).\n 9. Vigneau, M. et al. Meta-analyzing left hemisphere language areas: phonology, semantics, and sentence processing. Neuroimage 30, \n1414–1432 (2006).\n 10. Hickok, G. & Poeppel, D. The cortical organization of speech processing. Nat. Rev. Neurosci. 8, 393–402 (2007).\n 11. Price, C. J. The anatomy of language: a review of 100 fMRI studies published in 2009. Ann. N. Y. Acad. Sci. 1191, 62–88 (2010).\n 12. Price, C. J. A review and synthesis of the first 20 years of PET and fMRI studies of heard speech, spoken language and reading. \nNeuroimage 62, 816–847 (2012).\n 13. Friederici, A. D. The brain basis of language processing: from structure to function. Physiol. Rev. 91, 1357–1392 (2011).\n 14. Friederici, A. D. The cortical language circuit: from auditory perception to sentence comprehension. Trends Cogn. Sci. 16, 262–268 \n(2012).\n 15. Kwong, K. K. et al. Dynamic magnetic resonance imaging of human brain activity during primary sensory stimulation. Proc. Natl. \nAcad. Sci. USA 89, 5675–5679 (1992).\n 16. Ogawa, S. et al. Intrinsic signal changes accompanying sensory stimulation: functional brain mapping with magnetic resonance \nimaging. Proc. Natl. Acad. Sci. USA 89, 5951–5955 (1992).\n 17. Logothetis, N. K., Pauls, J., Augath, M., Trinath, T. & Oeltermann, A. Neurophysiological investigation of the basis of the fMRI \nsignal. Nature 412, 150–157 (2001).\n 18. Logothetis, N. K. What we can do and what we cannot do with fMRI. Nature 453, 869–878 (2008).\n 19. Démonet, J. F . et al. The anatomy of phonological and semantic processing in normal subjects. Brain 115, 1753–1768 (1992).\n 20. Zatorre, R. J., Evans, A. C., Meyer, E. & Gjedde, A. Lateralization of phonetic and pitch discrimination in speech processing. Science \n256, 846–849 (1992).\n 21. Belin, P ., Zatorre, R. J., Lafaille, P ., Ahad, P . & Pike, B. Voice-selective areas in human auditory cortex. Nature 403, 309–312 (2000).\n 22. Vouloumanos, A., Kiehl, K. A., Werker, J. F . & Liddle, P . F . Detection of sounds in the auditory stream: event-related fMRI evidence \nfor differential activation to speech and nonspeech. J. Cogn. Neurosci. 13, 994–1005 (2001).\n 23. Dapretto, M. & Bookheimer, S. Y . Form and content: dissociating syntax and semantics in sentence comprehension. Neuron 24, \n427–432 (1999).\n 24. Ben-Shachar, M., Hendler, T., Kahn, I., Ben-Bashat, D. & Grodzinsky, Y . The neural reality of syntactic transformations: evidence \nfrom functional magnetic resonance imaging. Psychol. Sci. 14, 433–440 (2003).\n 25. Noppeney, U. & Price, C. J. An FMRI study of syntactic adaptation. J. Cogn. Neurosci. 16, 702–713 (2004).\n 26. Patterson, K., Nestor, P . J. & Rogers, T. T. Where do you know what you know? The representation of semantic knowledge in the \nhuman brain. Nat. Rev. Neurosci. 8, 976–987 (2007).\n 27. Binder, J. R., Desai, R. H., Graves, W . W . & Conant, L. L. Where is the semantic system? A critical review and meta-analysis of 120 \nfunctional neuroimaging studies. Cereb. Cortex 19, 2767–2796 (2009).\n 28. Fedorenko, E., Hsieh, P .-J., Nieto-Castañón, A., Whitfield-Gabrieli, S. & Kanwisher, N. New method for fMRI investigations of \nlanguage: defining ROIs functionally in individual subjects. J. Neurophysiol. 104, 1177–1194 (2010).\n 29. Mahowald, K. & Fedorenko, E. Reliable individual-level neural markers of high-level language processing: a necessary precursor \nfor relating neural variability to behavioral and genetic variability. Neuroimage 139, 74–93 (2016).\n 30. Braga, R. M., DiNicola, L. M., Becker, H. C. & Buckner, R. L. Situating the left-lateralized language network in the broader \norganization of multiple specialized large-scale distributed networks. J. Neurophysiol. 124, 1415–1448 (2020).\n 31. Jäncke, L., Wüstenberg, T., Scheich, H. & Heinze, H.-J. Phonetic perception and the temporal cortex. Neuroimage  15, 733–746 \n(2002).\n 32. Obleser, J., Zimmermann, J., Van Meter, J. & Rauschecker, J. P . Multiple stages of auditory speech perception reflected in event-\nrelated FMRI. Cereb. Cortex 17, 2251–2257 (2007).\n 33. Petersen, S. E., Fox, P . T., Posner, M. I., Mintun, M. & Raichle, M. E. Positron emission tomographic studies of the cortical anatomy \nof single-word processing. Nature 331, 585–589 (1988).\n 34. Wise, R. et al. Distribution of cortical neural networks involved in word comprehension and word retrieval. Brain 114, 1803–1817 \n(1991).\n 35. Poldrack, R. A. et al . Functional specialization for semantic and phonological processing in the left inferior prefrontal cortex. \nNeuroimage 10, 15–35 (1999).\n 36. Just, M. A., Carpenter, P . A., Keller, T. A., Eddy, W . F . & Thulborn, K. R. Brain activation modulated by sentence comprehension. \nScience 274, 114–116 (1996).\n 37. Kuperberg, G. R. et al . Common and distinct neural substrates for pragmatic, semantic, and syntactic processing of spoken \nsentences: an fMRI study. J. Cogn. Neurosci. 12, 321–341 (2000).\n 38. Ni, W . et al. An event-related neuroimaging study distinguishing form and content in sentence processing. J. Cogn. Neurosci. 12, \n120–133 (2000).\n 39. Scott, S. K., Blank, C. C., Rosen, S. & Wise, R. J. Identification of a pathway for intelligible speech in the left temporal lobe. Brain \n123, 2400–2406 (2000).\n 40. Vandenberghe, R., Nobre, A. C. & Price, C. J. The response of left temporal cortex to sentences. J. Cogn. Neurosci. 14, 550–560 \n(2002).\n 41. Humphries, C., Binder, J. R., Medler, D. A. & Liebenthal, E. Syntactic and semantic modulation of neural activity during auditory \nsentence comprehension. J. Cogn. Neurosci. 18, 665–679 (2006).\n 42. Y arkoni, T., Speer, N. K. & Zacks, J. M. Neural substrates of narrative comprehension and memory. NeuroImage 41, 1408–1425 \n(2008).\n 43. Brennan, J. et al. Syntactic structure building in the anterior temporal lobe during natural story listening. Brain Lang. 120, 163–173 \n(2012).\n 44. Brennan, J. R., Stabler, E. P ., Van Wagenen, S. E., Luh, W .-M. & Hale, J. T. Abstract linguistic structure correlates with temporal \nactivity during naturalistic comprehension. Brain Lang. 157–158, 81–94 (2016).\n 45. Nastase, S. A., Goldstein, A. & Hasson, U. Keep it real: rethinking the primacy of experimental control in cognitive neuroscience. \nNeuroimage 222, 117254 (2020).\n 46. Wehbe, L. et al. Simultaneously uncovering the patterns of brain regions involved in different story reading subprocesses. PLoS One \n9, e112575 (2014).\n 47. Huth, A. G., de Heer, W . A., Griffiths, T. L., Theunissen, F . E. & Gallant, J. L. Natural speech reveals the semantic maps that tile \nhuman cerebral cortex. Nature 532, 453–458 (2016).\n 48. Goldberg, Y . Neural network methods for natural language processing. Synth. Lectures Hum. Lang. Technol. 10, 1–309 (2017).\n 49. Baroni, M. Linguistic generalization and compositionality in modern artificial neural networks. Philos. Trans. R. Soc. Lond. B Biol. \nSci. 375, 20190307 (2020).\n18Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\n 50. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. BERT: pre-training of deep bidirectional transformers for language \nunderstanding. in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) 4171–4186 (Association for Computational \nLinguistics, 2019).\n 51. Radford, A. et al. Language models are unsupervised multitask learners. OpenAI Blog (2019).\n 52. Turney, P . D. & Pantel, P . From frequency to meaning: vector space models of semantics. J. Artif. Intell. Res. 37, 141–188 (2010).\n 53. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. & Dean, J. Distributed representations of words and phrases and their \ncompositionality. in Advances in Neural Information Processing Systems 26 (eds. Burges, C. J. C., Bottou, L., Welling, M., \nGhahramani, Z. & Weinberger, K. Q.) 3111–3119 (Curran Associates, Inc., 2013).\n 54. Manning, C. D., Clark, K., Hewitt, J., Khandelwal, U. & Levy, O. Emergent linguistic structure in artificial neural networks trained \nby self-supervision. Proc. Natl. Acad. Sci. USA 117, 30046–30054 (2020).\n 55. Breiman, L. Statistical modeling: the two cultures. Stat. Sci. 16, 199–231 (2001).\n 56. Y arkoni, T. & Westfall, J. Choosing prediction over explanation in psychology: lessons from machine learning. Perspect. Psychol. \nSci. 12, 1100–1122 (2017).\n 57. Varoquaux, G. & Poldrack, R. A. Predictive models avoid excessive reductionism in cognitive neuroimaging. Curr. Opin. Neurobiol. \n55, 1–6 (2019).\n 58. Hasson, U., Nastase, S. A. & Goldstein, A. Direct fit to nature: an evolutionary perspective on biological and artificial neural \nnetworks. Neuron 105, 416–434 (2020).\n 59. LeCun, Y ., Cortes, C. & Burges, C. J. MNIST handwritten digit database. (2010).\n 60. Krizhevsky, A. Learning multiple layers of features from tiny images. (University of Toronto, 2009).\n 61. Milham, M. P . et al. Assessment of the impact of shared brain imaging data on the scientific literature. Nat. Commun. 9, 2818 \n(2018).\n 62. Biswal, B. B. et al. Toward discovery science of human brain function. Proc. Natl. Acad. Sci. USA 107, 4734–4739 (2010).\n 63. Van Essen, D. C. et al. The WU-Minn Human Connectome Project: an overview. Neuroimage 80, 62–79 (2013).\n 64. Shafto, M. A. et al. The Cambridge Centre for Ageing and Neuroscience (Cam-CAN) study protocol: a cross-sectional, lifespan, \nmultidisciplinary examination of healthy cognitive ageing. BMC Neurol. 14, 204 (2014).\n 65. Taylor, J. R. et al. The Cambridge Centre for Ageing and Neuroscience (Cam-CAN) data repository: structural and functional MRI, \nMEG, and cognitive data from a cross-sectional adult lifespan sample. Neuroimage 144, 262–269 (2017).\n 66. Alexander, L. M. et al. An open resource for transdiagnostic research in pediatric mental health and learning disorders. Sci Data 4, \n170181 (2017).\n 67. Poldrack, R. A. & Gorgolewski, K. J. Making big data open: data sharing in neuroimaging. Nat. Neurosci. 17, 1510–1517 (2014).\n 68. Poldrack, R. A. et al. Scanning the horizon: towards transparent and reproducible neuroimaging research. Nat. Rev. Neurosci. 18, \n115–126 (2017).\n 69. Poldrack, R. A., Gorgolewski, K. J. & Varoquaux, G. Computational and informatic advances for reproducible data analysis in \nneuroimaging. Annu. Rev. Biomed. Data Sci. 2, 119–138 (2019).\n 70. Ferguson, A. R., Nielson, J. L., Cragin, M. H., Bandrowski, A. E. & Martone, M. E. Big data from small data: data-sharing in the \n‘long tail’ of neuroscience. Nat. Neurosci. 17, 1442–1447 (2014).\n 71. Hanke, M. et al. A high-resolution 7-Tesla fMRI dataset from complex natural stimulation with an audio movie. Sci Data 1, 140003 \n(2014).\n 72. Hanke, M. et al. A studyforrest extension, simultaneous fMRI and eye gaze recordings during prolonged natural stimulation. Sci \nData 3, 160092 (2016).\n 73. Aly, M., Chen, J., Turk-Browne, N. B. & Hasson, U. Learning naturalistic temporal structure in the posterior medial network. J. \nCogn. Neurosci. 30, 1345–1365 (2018).\n 74. DuPre, E., Hanke, M. & Poline, J.-B. Nature abhors a paywall: how open science can realize the potential of naturalistic stimuli. \nNeuroimage 216, 116330 (2020).\n 75. Aliko, S., Huang, J., Gheorghiu, F . & Meliss, S. & Skipper, J. I. A naturalistic neuroimaging database for understanding the brain \nusing ecological stimuli. Sci Data 7, 347 (2020).\n 76. Richardson, H., Lisandrelli, G., Riobueno-Naylor, A. & Saxe, R. Development of the social brain from age three to twelve years. Nat. \nCommun. 9, 1027 (2018).\n 77. Finn, E. S., Corlett, P . R., Chen, G., Bandettini, P . A. & Constable, R. T. Trait paranoia shapes inter-subject synchrony in brain \nactivity during an ambiguous social narrative. Nat. Commun. 9, 2043 (2018).\n 78. Chen, J. et al. Accessing real-life episodic information from minutes versus hours earlier modulates hippocampal and high-order \ncortical dynamics. Cereb. Cortex 26, 3428–3441 (2016).\n 79. Chen, J. et al. Shared memories reveal shared structure in neural activity across individuals. Nat. Neurosci. 20, 115–125 (2017).\n 80. O’Connor, D. et al. The Healthy Brain Network Serial Scanning Initiative: a resource for evaluating inter-individual differences and \ntheir reliabilities across scan conditions and sessions. GigaScience 6, 1–14 (2017).\n 81. Haxby, J. V . et al. A common, high-dimensional model of the representational space in human ventral temporal cortex. Neuron 72, \n404–416 (2011).\n 82. Nastase, S. A. et al . Attention Selectively Reshapes the Geometry of Distributed Semantic Representation. Cereb. Cortex 27, \n4277–4291 (2017).\n 83. Nastase, S. A., Halchenko, Y . O., Connolly, A. C., Gobbini, M. I. & Haxby, J. V . Neural responses to naturalistic clips of behaving \nanimals in two different task contexts. Front. Neurosci. 12, 316 (2018).\n 84. Castello, M. V . di O., di Oleggio Castello, M. V ., Chauhan, V ., Jiahui, G. & Ida Gobbini, M. An fMRI dataset in response to ‘The \nGrand Budapest Hotel’ , a socially-rich, naturalistic movie. Scientific Data vol. 7 (2020).\n 85. Nastase, S. A. et al. Narratives. OpenNeuro https://doi.org/10.18112/openneuro.ds002345.v1.1.4 (2019).\n 86. Gorgolewski, K. J. et al . The brain imaging data structure, a format for organizing and describing outputs of neuroimaging \nexperiments. Sci Data 3, 160044 (2016).\n 87. Poldrack, R. A. & Gorgolewski, K. J. OpenfMRI: Open sharing of task fMRI data. Neuroimage 144, 259–261 (2017).\n 88. Hanke, M. et al. datalad/datalad: 0.13.3 (August 28, 2020). Zenodo https://doi.org/10.5281/zenodo.4006562 (2020).\n 89. Hanke, M. et al. In defense of decentralized research data management. Neuroforum 27, 17–25 (2021).\n 90. Spiers, H. J. & Maguire, E. A. Decoding human brain activity during real-world experiences. Trends Cogn. Sci. 11, 356–365 (2007).\n 91. Hasson, U. & Honey, C. J. Future trends in neuroimaging: neural processes as expressed within real-life contexts. Neuroimage 62, \n1272–1278 (2012).\n 92. Matusz, P . J., Dikker, S., Huth, A. G. & Perrodin, C. Are we ready for real-world neuroscience? J. Cogn. Neurosci. 31, 327–338 \n(2019).\n 93. Sonkusare, S., Breakspear, M. & Guo, C. Naturalistic stimuli in neuroscience: critically acclaimed. Trends Cogn. Sci. 23, 699–714 \n(2019).\n 94. Redcay, E. & Moraczewski, D. Social cognition in context: a naturalistic imaging approach. Neuroimage 216, 116392 (2020).\n 95. Vanderwal, T., Eilbott, J. & Castellanos, F . X. Movies in the magnet: naturalistic paradigms in developmental functional \nneuroimaging. Dev. Cogn. Neurosci. 36, 100600 (2018).\n19Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\n 96. Kriegeskorte, N., Mur, M. & Bandettini, P . A. Representational similarity analysis—connecting the branches of systems \nneuroscience. Front. Syst. Neurosci. 2, 4 (2008).\n 97. Naselaris, T., Kay, K. N., Nishimoto, S. & Gallant, J. L. Encoding and decoding in fMRI. Neuroimage 56, 400–410 (2011).\n 98. Santoro, R. et al. Encoding of natural sounds at multiple spectral and temporal resolutions in the human auditory cortex. PLoS \nComput. Biol. 10, e1003412 (2014).\n 99. de Heer, W . A., Huth, A. G., Griffiths, T. L., Gallant, J. L. & Theunissen, F . E. The hierarchical cortical organization of human speech \nprocessing. J. Neurosci. 37, 6539–6557 (2017).\n 100. Kell, A. J. E., Y amins, D. L. K., Shook, E. N., Norman-Haignere, S. V . & McDermott, J. H. A task-optimized neural network \nreplicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy. Neuron 98, 630–644.e16 \n(2018).\n 101. Mitchell, T. M. et al. Predicting human brain activity associated with the meanings of nouns. Science 320, 1191–1195 (2008).\n 102. Pereira, F . et al. Toward a universal decoder of linguistic meaning from brain activation. Nat. Commun. 9, 963 (2018).\n 103. Schrimpf, M. et al. The neural architecture of language: integrative reverse-engineering converges on a model for predictive \nprocessing. Preprint at https://doi.org/10.1101/2020.06.26.174482 (2020).\n 104. Hasson, U., Nir, Y ., Levy, I., Fuhrmann, G. & Malach, R. Intersubject synchronization of cortical activity during natural vision. \nScience 303, 1634–1640 (2004).\n 105. Nastase, S. A., Gazzola, V ., Hasson, U. & Keysers, C. Measuring shared responses across subjects using intersubject correlation. Soc. \nCogn. Affect. Neurosci. 14, 667–685 (2019).\n 106. Vanderwal, T. et al. Individual differences in functional connectivity during naturalistic viewing conditions. Neuroimage  157, \n521–530 (2017).\n 107. Feilong, M., Nastase, S. A., Guntupalli, J. S. & Haxby, J. V . Reliable individual differences in fine-grained cortical functional \narchitecture. Neuroimage 183, 375–386 (2018).\n 108. Finn, E. S. et al. Idiosynchrony: from shared responses to individual differences during naturalistic neuroimaging. Neuroimage 215, \n116828 (2020).\n 109. Chen, P .-H. et al. A reduced-dimension fMRI shared response model. in Advances in Neural Information Processing Systems 28 (eds. \nCortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M. & Garnett, R.) 460–468 (Curran Associates, Inc., 2015).\n 110. Guntupalli, J. S. et al. A model of representational spaces in human cortex. Cereb. Cortex 26, 2919–2934 (2016).\n 111. Guntupalli, J. S., Feilong, M. & Haxby, J. V . A computational model of shared fine-scale structure in the human connectome. PLoS \nComput. Biol. 14, e1006120 (2018).\n 112. Van Uden, C. E. et al. Modeling semantic encoding in a common neural representational space. Front. Neurosci. 12, 437 (2018).\n 113. Haxby, J. V ., Guntupalli, J. S., Nastase, S. A. & Feilong, M. Hyperalignment: modeling shared information encoded in idiosyncratic \ncortical topographies. eLife 9 (2020).\n 114. Milivojevic, B., Varadinov, M., Vicente Grabovetsky, A., Collin, S. H. P . & Doeller, C. F . Coding of event nodes and narrative context \nin the hippocampus. J. Neurosci. 36, 12412–12424 (2016).\n 115. Baldassano, C. et al. Discovering event structure in continuous narrative perception and memory. Neuron 95, 709–721.e5 (2017).\n 116. Baldassano, C., Hasson, U. & Norman, K. A. Representation of real-world event schemas during narrative perception. J. Neurosci. \n38, 9689–9699 (2018).\n 117. Chang, L. J. et al . Endogenous variation in ventromedial prefrontal cortex state dynamics during naturalistic viewing reflects \naffective experience. Sci. Adv. 7, eabf7129 (2021).\n 118. Heusser, A. C., Fitzpatrick, P . C. & Manning, J. R. Geometric models reveal behavioural and neural signatures of transforming \nexperiences into memories. Nat. Hum. Behav. 5, 905–919 (2021).\n 119. Simony, E. et al. Dynamic reconfiguration of the default mode network during narrative comprehension. Nat. Commun. 7, 12141 \n(2016).\n 120. Kim, D., Kay, K., Shulman, G. L. & Corbetta, M. A new modular brain organization of the BOLD signal during natural vision. \nCereb. Cortex 28, 3065–3081 (2018).\n 121. Betzel, R. F ., Byrge, L., Esfahlani, F . Z. & Kennedy, D. P . Temporal fluctuations in the brain’s modular architecture during movie-\nwatching. Neuroimage 213, 116687 (2020).\n 122. Meer, J. N., van der, Breakspear, M., Chang, L. J., Sonkusare, S. & Cocchi, L. Movie viewing elicits rich and reliable brain state \ndynamics. Nat. Commun. 11, 5004 (2020).\n 123. Brainard, D. H. The Psychophysics Toolbox. Spat. Vis. 10, 433–436 (1997).\n 124. Kleiner, M., Brainard, D. & Pelli, D. What’s new in Psychtoolbox-3? Perception 36 ECVP Abstract Supplement (2007).\n 125. Peirce, J. W . PsychoPy—psychophysics software in Python. J. Neurosci. Methods 162, 8–13 (2007).\n 126. Peirce, J. W . Generating stimuli for neuroscience using PsychoPy. Front. Neuroinform. 2, 10 (2009).\n 127. Peirce, J. et al. PsychoPy2: experiments in behavior made easy. Behav. Res. Methods 51, 195–203 (2019).\n 128. DuPre, E., Hanke, M. & Poline, J.-B. Nature abhors a paywall: how open science can realize the potential of naturalistic stimuli. \nNeuroimage 216, 116330 (2019).\n 129. McNamara, Q., De La Vega, A. & Y arkoni, T. Developing a comprehensive framework for multimodal feature extraction. in \nProceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1567–1574 (ACM, 2017).\n 130. Ochshorn, R. M. & Hawkins, M. Gentle: a robust yet lenient forced aligner built on Kaldi. (2016).\n 131. Povey, D. et al. The Kaldi speech recognition toolkit. in IEEE 2011 workshop on automatic speech recognition and understanding \n(IEEE Signal Processing Society, 2011).\n 132. Cieri, C., Miller, D. & Walker, K. The Fisher Corpus: a resource for the next generations of speech-to-text. Proceedings of the Fourth \nInternational Conference on Language Resources and Evaluation (LREC) 4, 69–71 (2004).\n 133. Nichols, T. E. et al. Best practices in data analysis and sharing in neuroimaging using MRI. Nat. Neurosci. 20, 299–303 (2017).\n 134. Gulban, O. F . et al. poldracklab/pydeface: v2.0.0. Zenodo https://doi.org/10.5281/zenodo.3524401 (2019).\n 135. Esteban, O. et al. fMRIPrep: a robust preprocessing pipeline for functional MRI. Nat. Methods 16, 111–116 (2019).\n 136. Esteban, O. et al. fMRIPrep: a robust preprocessing pipeline for functional MRI. Zenodo https://doi.org/10.5281/zenodo.3724468 \n(2020).\n 137. Gorgolewski, K. et al. Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python. Front. \nNeuroinform. 5, 13 (2011).\n 138. Esteban, O. et al. nipy/nipype: 1.4.2. Zenodo https://doi.org/10.5281/zenodo.3668316 (2020).\n 139. Abraham, A. et al. Machine learning for neuroimaging with scikit-learn. Front. Neuroinform. 8, 14 (2014).\n 140. Kurtzer, G. M., Sochat, V . & Bauer, M. W . Singularity: scientific containers for mobility of compute. PLoS One 12, e0177459 (2017).\n 141. Cox, R. W . AFNI: software for analysis and visualization of functional magnetic resonance neuroimages. Comput. Biomed. Res. 29, \n162–173 (1996).\n 142. Cox, R. W . AFNI: what a long strange trip it’s been. Neuroimage 62, 743–747 (2012).\n 143. Tustison, N. J. et al. N4ITK: improved N3 bias correction. IEEE Trans. Med. Imaging 29, 1310–1320 (2010).\n 144. Avants, B. B., Epstein, C. L., Grossman, M. & Gee, J. C. Symmetric diffeomorphic image registration with cross-correlation: \nevaluating automated labeling of elderly and neurodegenerative brain. Med. Image Anal. 12, 26–41 (2008).\n 145. Zhang, Y ., Brady, M. & Smith, S. Segmentation of brain MR images through a hidden Markov random field model and the \nexpectation-maximization algorithm. IEEE Trans. Med. Imaging 20, 45–57 (2001).\n20Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\n 146. Dale, A. M., Fischl, B. & Sereno, M. I. Cortical surface-based analysis. I. Segmentation and surface reconstruction. Neuroimage 9, \n179–194 (1999).\n 147. Fischl, B. FreeSurfer. Neuroimage 62, 774–781 (2012).\n 148. Klein, A. et al. Mindboggling morphometry of human brains. PLoS Comput. Biol. 13, e1005350 (2017).\n 149. Esteban, O., Ciric, R., Markiewicz, C. J., Poldrack, R. A. & Gorgolewski, K. J. TemplateFlow Client: accessing the library of \nstandardized neuroimaging standard spaces. Zenodo https://doi.org/10.5281/zenodo.3981009 (2020).\n 150. Fonov, V . S., Evans, A. C., McKinstry, R. C., Almli, C. R. & Collins, D. L. Unbiased nonlinear average age-appropriate brain \ntemplates from birth to adulthood. Neuroimage 47, S102 (2009).\n 151. Evans, A. C., Janke, A. L., Collins, D. L. & Baillet, S. Brain templates and atlases. Neuroimage 62, 911–922 (2012).\n 152. Fischl, B., Sereno, M. I., Tootell, R. B. & Dale, A. M. High-resolution intersubject averaging and a coordinate system for the cortical \nsurface. Hum. Brain Mapp. 8, 272–284 (1999).\n 153. Huntenburg, J. M. Evaluating nonlinear coregistration of BOLD EPI and T1w images. (Freie Universität Berlin, 2014).\n 154. Wang, S. et al. Evaluation of Field Map and Nonlinear Registration Methods for Correction of Susceptibility Artifacts in Diffusion \nMRI. Front. Neuroinform. 11, 17 (2017).\n 155. Treiber, J. M. et al. Characterization and Correction of Geometric Distortions in 814 Diffusion Weighted Images. PLoS One 11, \ne0152472 (2016).\n 156. Greve, D. N. & Fischl, B. Accurate and robust brain image alignment using boundary-based registration. Neuroimage 48, 63–72 \n(2009).\n 157. Jenkinson, M., Bannister, P ., Brady, M. & Smith, S. Improved optimization for the robust and accurate linear registration and \nmotion correction of brain images. Neuroimage 17, 825–841 (2002).\n 158. Smith, S. M. et al. Advances in functional and structural MR image analysis and implementation as FSL. Neuroimage 23, S208–19 \n(2004).\n 159. Jenkinson, M., Beckmann, C. F ., Behrens, T. E. J., Woolrich, M. W . & Smith, S. M. FSL. Neuroimage 62, 782–790 (2012).\n 160. Cox, R. W . & Hyde, J. S. Software tools for analysis and visualization of fMRI data. NMR Biomed. 10, 171–178 (1997).\n 161. Lanczos, C. Evaluation of Noisy Data. J. Soc. Ind. Appl. Math. B Numer. Anal. 1, 76–85 (1964).\n 162. Power, J. D. et al. Methods to detect, characterize, and remove motion artifact in resting state fMRI. Neuroimage  84, 320–341 \n(2014).\n 163. Behzadi, Y ., Restom, K., Liau, J. & Liu, T. T. A component based noise correction method (CompCor) for BOLD and perfusion \nbased fMRI. Neuroimage 37, 90–101 (2007).\n 164. Satterthwaite, T. D. et al . An improved framework for confound regression and filtering for control of motion artifact in the \npreprocessing of resting-state functional connectivity data. Neuroimage 64, 240–256 (2013).\n 165. Pajula, J. & Tohka, J. Effects of spatial smoothing on inter-subject correlation based analysis of FMRI. Magn. Reson. Imaging 32, \n1114–1124 (2014).\n 166. Nastase, S. A., Liu, Y .-F ., Hillman, H., Norman, K. A. & Hasson, U. Leveraging shared connectivity to aggregate heterogeneous \ndatasets into a common response space. Neuroimage 217, 116865 (2020).\n 167. Chung, M. K. et al. Cortical thickness analysis in autism with heat kernel smoothing. Neuroimage 25, 1256–1265 (2005).\n 168. Hagler, D. J. Jr, Saygin, A. P . & Sereno, M. I. Smoothing and cluster thresholding for cortical surface-based group analysis of fMRI \ndata. Neuroimage 33, 1093–1103 (2006).\n 169. Triantafyllou, C., Hoge, R. D. & Wald, L. L. Effect of spatial smoothing on physiological noise in high-resolution fMRI. Neuroimage \n32, 551–557 (2006).\n 170. Friedman, L., Glover, G. H., Krenz, D. & Magnotta, V ., FIRST BIRN. Reducing inter-scanner variability of activation in a \nmulticenter fMRI study: role of smoothness equalization. Neuroimage 32, 1656–1668 (2006).\n 171. Simony, E. & Chang, C. Analysis of stimulus-induced brain dynamics during naturalistic paradigms. Neuroimage  216, 116461 \n(2019).\n 172. Ciric, R. et al. Benchmarking of participant-level confound regression strategies for the control of motion artifact in studies of \nfunctional connectivity. Neuroimage 154, 174–187 (2017).\n 173. Parkes, L., Fulcher, B., Yücel, M. & Fornito, A. An evaluation of the efficacy, reliability, and sensitivity of motion correction \nstrategies for resting-state functional MRI. Neuroimage 171, 415–436 (2018).\n 174. Muschelli, J. et al. Reduction of motion-related artifacts in resting state fMRI using aCompCor. Neuroimage 96, 22–35 (2014).\n 175. Lindquist, M. A., Geuter, S., Wager, T. D. & Caffo, B. S. Modular preprocessing pipelines can reintroduce artifacts into fMRI data. \nHum. Brain Mapp. 40, 2358–2376 (2019).\n 176. Halchenko, Y . O. & Hanke, M. Open is not enough. Let’s take the next step: an integrated, community-driven computing platform \nfor neuroscience. Front. Neuroinform. 6, 22 (2012).\n 177. Hanke, M. & Halchenko, Y . O. Neuroscience runs on GNU/Linux. Front. Neuroinform. 5, 8 (2011).\n 178. Walt, S., van der, Colbert, S. C. & Varoquaux, G. The NumPy Array: a structure for efficient numerical computation. Comput. Sci. \nEng. 13, 22–30 (2011).\n 179. Harris, C. R. et al. Array programming with NumPy. Nature 585, 357–362 (2020).\n 180. Jones, E., Oliphant, T. & Peterson, P . SciPy: open source scientific tools for Python (2001).\n 181. Virtanen, P . et al. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nat. Methods 17, 261–272 (2020).\n 182. McKinney, W . Data structures for statistical computing in Python. in Proceedings of the 9th Python in Science Conference 51–56 \n(2010).\n 183. Brett, M. et al. nipy/nibabel: 3.1.1. Zenodo https://doi.org/10.5281/zenodo.3924343 (2020).\n 184. Perez, F . & Granger, B. E. IPython: a system for interactive scientific computing. Computing in Science Engineering 9, 21–29 (2007).\n 185. Kluyver, T. et al. Jupyter Notebooks—a publishing format for reproducible computational workflows. in Positioning and Power in \nAcademic Publishing: Players, Agents and Agendas (eds. Loizides, F . & Schmidt, B.) 87–90 (IOS Press, 2016).\n 186. Jette, M. A., Y oo, A. B. & Grondona, M. SLURM: Simple Linux Utility for Resource Management. in Job Scheduling Strategies for \nParallel Processing (eds. Feitelson, D., Rudolph, L. & Schwiegelshohn, U.) 44–60 (Springer, Berlin, Heidelberg, 2003).\n 187. Saad, Z. S., Reynolds, R. C., Argall, B., Japee, S. & Cox, R. W . SUMA: an interface for surface-based intra- and inter-subject analysis \nwith AFNI. 2004 2nd IEEE International Symposium on Biomedical Imaging: Nano to Macro 2, 1510–1513 (2004).\n 188. Saad, Z. S. & Reynolds, R. C. SUMA. Neuroimage 62, 768–773 (2012).\n 189. Hunter, J. D. Matplotlib: A 2D Graphics Environment. Comput. Sci. Eng. 9, 90–95 (2007).\n 190. Lerner, Y ., Honey, C. J., Silbert, L. J. & Hasson, U. Topographic mapping of a hierarchy of temporal receptive windows using a \nnarrated story. J. Neurosci. 31, 2906–2915 (2011).\n 191. Ben-Y akov, A., Honey, C. J., Lerner, Y . & Hasson, U. Loss of reliable temporal structure in event-related averaging of naturalistic \nstimuli. Neuroimage 63, 501–506 (2012).\n 192. Regev, M., Honey, C. J., Simony, E. & Hasson, U. Selective and invariant neural responses to spoken and written narratives. J. \nNeurosci. 33, 15978–15988 (2013).\n 193. Stephens, G. J., Honey, C. J. & Hasson, U. A place for time: the spatiotemporal structure of neural dynamics during natural audition. \nJ. Neurophysiol. 110, 2019–2026 (2013).\n 194. Lerner, Y ., Honey, C. J., Katkov, M. & Hasson, U. Temporal scaling of neural responses to compressed and dilated natural speech. J. \nNeurophysiol. 111, 2433–2444 (2014).\n21Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\n 195. Liu, Y . et al. Measuring speaker-listener neural coupling with functional near infrared spectroscopy. Sci. Rep. 7, 43293 (2017).\n 196. Vodrahalli, K. et al. Mapping between fMRI responses to movies and their natural language annotations. Neuroimage 180, 223–231 \n(2018).\n 197. Y eshurun, Y ., Nguyen, M. & Hasson, U. Amplification of local changes along the timescale processing hierarchy. Proc. Natl. Acad. \nSci. USA 114, 9475–9480 (2017).\n 198. Zuo, X. et al. Temporal integration of narrative information in a hippocampal amnesic patient. Neuroimage 213, 116658 (2020).\n 199. Gross, J. et al. Speech rhythms and multiplexed oscillatory sensory coding in the human brain. PLoS Biol. 11, e1001752 (2013).\n 200. Blank, I. A. & Fedorenko, E. Domain-general brain regions do not track linguistic input as closely as language-selective regions. J. \nNeurosci. 37, 9999–10011 (2017).\n 201. Iotzov, I. et al. Divergent neural responses to narrative speech in disorders of consciousness. Ann Clin Transl Neurol 4, 784–792 \n(2017).\n 202. Loiotile, R. E., Cusack, R. & Bedny, M. Naturalistic audio-movies and narrative synchronize ‘visual’ cortices across congenitally \nblind but not sighted individuals. J. Neurosci. 39, 8940–8948 (2019).\n 203. Lositsky, O. et al. Neural pattern change during encoding of a narrative predicts retrospective duration estimates. Elife 5 (2016).\n 204. Y eshurun, Y . et al. Same story, different story: the neural representation of interpretive frameworks. Psychol. Sci. 28, 307–319 \n(2017).\n 205. Regev, M. et al. Propagation of Information Along the Cortical Hierarchy as a Function of Attention While Reading and Listening \nto Stories. Cereb. Cortex 29, 4017–4034 (2019).\n 206. Chien, H.-Y . S. & Honey, C. J. Constructing and forgetting temporal context in the human cerebral cortex. Neuron 106, 675–686.\ne11 (2020).\n 207. Zadbood, A., Chen, J., Leong, Y . C., Norman, K. A. & Hasson, U. How we transmit memories to other brains: constructing shared \nneural representations via communication. Cereb. Cortex 27, 4988–5000 (2017).\n 208. Heider, F . & Simmel, M. An experimental study of apparent behavior. Am. J. Psychol. 57, 243–259 (1944).\n 209. Nguyen, M., Vanderwal, T. & Hasson, U. Shared understanding of narratives is correlated with shared neural responses. \nNeuroimage 184, 161–170 (2019).\n 210. Chang, C. H. C., Lazaridi, C., Y eshurun, Y ., Norman, K. A. & Hasson, U. Relating the past with the present: Information integration \nand segregation during ongoing narrative processing. J. Cogn. Neurosci. 33, 1106–1128 (2021).\n 211. Visconti di Oleggio Castello, M. et al. ReproNim/reproin 0.6.0. Zenodo https://doi.org/10.5281/zenodo.3625000 (2020).\n 212. Halchenko, Y . et al. nipy/heudiconv v0.8.0. Zenodo https://doi.org/10.5281/zenodo.3760062 (2020).\n 213. Lin, X. et al. Data-efficient mutual information neural estimator. Preprint at https://arxiv.org/abs/1905.03319 (2019).\n 214. Mennes, M., Biswal, B. B., Castellanos, F . X. & Milham, M. P . Making data sharing work: the FCP/INDI experience. Neuroimage 82, \n683–691 (2013).\n 215. Kennedy, D. N., Haselgrove, C., Riehl, J., Preuss, N. & Buccigrossi, R. The NITRC image repository. Neuroimage 124, 1069–1073 \n(2016).\n 216. Nastase, S. A. et al. Narratives Dataset. FCP/INDI https://doi.org/10.15387/fcp_indi.retro.Narratives (2021).\n 217. Wilkinson, M. D. et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018 (2016).\n 218. Gorgolewski, K. J. et al. BIDS apps: improving ease of use, accessibility, and reproducibility of neuroimaging data analysis methods. \nPLoS Comput. Biol. 13, e1005209 (2017).\n 219. Cox, R. W . et al. A (sort of) new image data format standard: NIfTI-1. in 10th Annual Meeting of the Organization for Human Brain \nMapping, Budapest, Hungary (2004).\n 220. Li, X., Morgan, P . S., Ashburner, J., Smith, J. & Rorden, C. The first step for neuroimaging data analysis: DICOM to NIfTI \nconversion. J. Neurosci. Methods 264, 47–56 (2016).\n 221. Wagner, A. S. et al. The DataLad Handbook. Zenodo https://doi.org/10.5281/zenodo.3905791 (2020).\n 222. Esteban, O. et al. MRIQC: advancing the automatic prediction of image quality in MRI from unseen sites. PLoS One 12, e0184661 \n(2017).\n 223. Esteban, O. et al. MRIQC: advancing the automatic prediction of image quality in MRI from unseen sites. Zenodo https://doi.\norg/10.5281/zenodo.3352432 (2019).\n 224. Power, J. D., Barnes, K. A., Snyder, A. Z., Schlaggar, B. L. & Petersen, S. E. Spurious but systematic correlations in functional \nconnectivity MRI networks arise from subject motion. Neuroimage 59, 2142–2154 (2012).\n 225. Forman, S. D. et al . Improved assessment of significant activation in functional magnetic resonance imaging (fMRI): use of a \ncluster‐size threshold. Magn. Reson. Med. 33, 636–647 (1995).\n 226. Krüger, G. & Glover, G. H. Physiological noise in oxygenation-sensitive magnetic resonance imaging. Magn. Reson. Med. 46, \n631–637 (2001).\n 227. Ojemann, J. G. et al . Anatomic localization and quantitative analysis of gradient refocused echo-planar fMRI susceptibility \nartifacts. Neuroimage 6, 156–167 (1997).\n 228. Hasson, U., Malach, R. & Heeger, D. J. Reliability of cortical activity during natural stimulation. Trends Cogn. Sci. 14, 40–48 (2010).\n 229. Nili, H. et al. A toolbox for representational similarity analysis. PLoS Comput. Biol. 10, e1003553 (2014).\n 230. Silver, N. C. & Dunlap, W . P . Averaging correlation coefficients: should Fisher’s z transformation be used? Journal of Applied \nPsychology 72, 146–148 (1987).\n 231. Cohen, J. D. et al. Computational approaches to fMRI analysis. Nat. Neurosci. 20, 304–313 (2017).\n 232. Kumar, M. et al. BrainIAK tutorials: user-friendly learning materials for advanced fMRI analysis. PLoS Comp. Biol. 16, e1007549 \n(2020).\n 233. Kumar, M. et al. BrainIAK: the brain imaging analysis kit. Preprint at https://doi.org/10.31219/osf.io/db2ev (2020).\n 234. Glasser, M. F . et al. A multi-modal parcellation of human cerebral cortex. Nature 536, 171–178 (2016).\n 235. Mills, K. HCP-MMP1.0 projected on fsaverage. figshare https://doi.org/10.6084/m9.figshare.3498446.v2 (2016).\n 236. Aguirre, G. K., Zarahn, E. & D’ esposito, M. The variability of human, BOLD hemodynamic responses. Neuroimage  8, 360–369 \n(1998).\n 237. Handwerker, D. A., Ollinger, J. M. & D’Esposito, M. Variation of BOLD hemodynamic responses across subjects and brain regions \nand their effects on statistical analyses. Neuroimage 21, 1639–1651 (2004).\n 238. Binder, J. R. et al. Human temporal lobe activation by speech and nonspeech sounds. Cereb. Cortex 10, 512–528 (2000).\n 239. Zatorre, R. J., Belin, P . & Penhune, V . B. Structure and function of auditory cortex: music and speech. Trends Cogn. Sci. 6, 37–46 (2002).\n 240. Chen, G. et al. Untangling the relatedness among correlations, part I: nonparametric approaches to inter-subject correlation \nanalysis at the group level. Neuroimage 142, 248–259 (2016).\n 241. Chen, G., Taylor, P . A., Shin, Y .-W ., Reynolds, R. C. & Cox, R. W . Untangling the relatedness among correlations, Part II: inter-\nsubject correlation group analysis through linear mixed-effects modeling. Neuroimage 147, 825–840 (2017).\n 242. Chen, G. et al. Untangling the relatedness among correlations, part III: inter-subject correlation analysis through Bayesian \nmultilevel modeling for naturalistic scanning. Neuroimage 216, 116474 (2020).\n 243. Markiewicz, C. J. et al. poldracklab/fitlins. Zenodo https://doi.org/10.5281/zenodo.5120201 (2021).\n 244. de la Vega, A., Blair, R. & Y arkoni, T. neuroscout/neuroscout. Zenodo https://doi.org/10.5281/zenodo.4456028 (2021).\n 245. Y arkoni, T. et al. PyBIDS: Python tools for BIDS datasets. J. Open Source Softw. 4 (2019).\n22Scientific  Data |           (2021) 8:250  | https://doi.org/10.1038/s41597-021-01033-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nacknowledgements\nWe thank Leigh Nystrom, Mark Pinsk, Garrett McGrath, and the administrative staff at the Scully Center for the \nNeuroscience of Mind and Behavior and the Princeton Neuroscience Institute, as well as Elizabeth McDevitt, \nAnne Mennen, and members of Pygers support group. We thank Franklin Feingold for assistance in data sharing, \nas well as Chris Gorgolewski, Tal Y arkoni, Satrajit S. Ghosh, Avital Hahamy, Mohamed Amer, Indranil Sur, Xiao \nLin, and Ajay Divarakian for helpful feedback on the data and analysis. This work was supported by the National \nInstitutes of Health (NIH) grants R01-MH094480 (U.H.), DP1-HD091948 (U.H.), R01-MH112566 (U.H.), \nR01-MH112357 (K.A.N., U.H), T32-MH065214 (K.A.N), by the Defense Advanced Research Projects Agency \n(DARPA) Brain-to-Brain Seedling contract number FA8750-18-C-0213 (U.H.), and by the Intel Corporation. The \nviews, opinions, and/or conclusions contained in this paper are those of the authors and should not be interpreted \nas representing the official views or policies, either expressed or implied of the NIH, DARPA, or Intel.\nauthor contributions\nS.A.N. converted the data to BIDS format, analyzed the data, and wrote the paper. S.A.N., Y .F .L., L.H., H.H., \nA.Z. and E.M. curated the data. J.C., C.J.H., E.S., M.A.C., M.R. and U.H. designed and collected the “Pie Man” \ndataset. O.L. and U.H. designed and collected the “Tunnel Under the World” and “Lucy” datasets. Y .Y . and U.H. \ndesigned and collected the “Pretty Mouth and Green My Eyes” and “Milky Way” datasets. J.C., M.R., M.C. and \nU.H. designed and collected the “Slumlord” , “Reach for the Stars One Small Step at a Time” , and “It’s Not the Fall \nthat Gets Y ou” datasets. A.Z., J.C., Y .C.L., K.A.N. and U.H. designed and collected the “Merlin” and “Sherlock” \ndatasets. C.B., U.H. and K.A.N. designed and collected the “Schema” dataset. M.N. and U.H. designed and \ncollected the “Shapes” dataset. C.H.C.C., Y .Y ., K.A.N. and U.H. designed and collected the “The 21st Y ear” dataset. \nS.N., P .P .B. and U.H. designed and collected the “Pie Man” , “Running from the Bronx” , “I Knew Y ou Were Black” , \nand “The Man Who Forgot Ray Bradbury” datasets. G.C. and A.G. provided technical assistance in stimulus \ntranscription. Y .O.H. provided technical assistance with DataLad. U.H., K.A.N. and Y .O.H. supervised the project.\nCompeting interests\nThe authors declare no competing interests.\nadditional information\nSupplementary information The online version contains supplementary material available at https://doi.\norg/10.1038/s41597-021-01033-3.\nCorrespondence and requests for materials should be addressed to S.A.N.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-\native Commons license, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\nThe Creative Commons Public Domain Dedication waiver http://creativecommons.org/publicdomain/zero/1.0/ \napplies to the metadata files associated with this article.\n \n© The Author(s) 2021",
  "topic": "Narrative",
  "concepts": [
    {
      "name": "Narrative",
      "score": 0.6950514912605286
    },
    {
      "name": "Computer science",
      "score": 0.6387798190116882
    },
    {
      "name": "Metadata",
      "score": 0.6219172477722168
    },
    {
      "name": "Comprehension",
      "score": 0.6132424473762512
    },
    {
      "name": "Neuroimaging",
      "score": 0.5702747702598572
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5620260238647461
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.536532998085022
    },
    {
      "name": "Data collection",
      "score": 0.4695872366428375
    },
    {
      "name": "Natural language processing",
      "score": 0.4270440936088562
    },
    {
      "name": "Psychology",
      "score": 0.33617186546325684
    },
    {
      "name": "Artificial intelligence",
      "score": 0.334652304649353
    },
    {
      "name": "Linguistics",
      "score": 0.24392852187156677
    },
    {
      "name": "World Wide Web",
      "score": 0.1358625292778015
    },
    {
      "name": "Neuroscience",
      "score": 0.08819666504859924
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I1292859797",
      "name": "Montreal Neurological Institute and Hospital",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27804330",
      "name": "Brown University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I157943965",
      "name": "Holon Institute of Technology",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I200719446",
      "name": "Vanderbilt University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210153200",
      "name": "BC Children's Hospital",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I107672454",
      "name": "Dartmouth College",
      "country": "US"
    }
  ]
}