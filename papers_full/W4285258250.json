{
  "title": "Grafting Transformer on Automatically Designed Convolutional Neural Network for Hyperspectral Image Classification",
  "url": "https://openalex.org/W4285258250",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2364101116",
      "name": "Xue Xizhe",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A4223251056",
      "name": "Zhang, Haokui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2360967565",
      "name": "Fang Bei",
      "affiliations": [
        "Shaanxi Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2627962013",
      "name": "Bai, Zongwen",
      "affiliations": [
        "Yan'an University"
      ]
    },
    {
      "id": "https://openalex.org/A1909470079",
      "name": "Li Ying",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2793272303",
    "https://openalex.org/W2743255627",
    "https://openalex.org/W2615977073",
    "https://openalex.org/W2058195997",
    "https://openalex.org/W2789224430",
    "https://openalex.org/W2997272341",
    "https://openalex.org/W2719511702",
    "https://openalex.org/W2810170362",
    "https://openalex.org/W3217526930",
    "https://openalex.org/W3200959564",
    "https://openalex.org/W2159284541",
    "https://openalex.org/W2029316659",
    "https://openalex.org/W2090424610",
    "https://openalex.org/W2787870708",
    "https://openalex.org/W1521436688",
    "https://openalex.org/W1966580635",
    "https://openalex.org/W1950365613",
    "https://openalex.org/W2577238056",
    "https://openalex.org/W2572303978",
    "https://openalex.org/W2500751094",
    "https://openalex.org/W2764276316",
    "https://openalex.org/W2941387379",
    "https://openalex.org/W2614326984",
    "https://openalex.org/W6752515464",
    "https://openalex.org/W2943270518",
    "https://openalex.org/W6788138943",
    "https://openalex.org/W2548340849",
    "https://openalex.org/W3145183637",
    "https://openalex.org/W3051556114",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W3034959114",
    "https://openalex.org/W2963136578",
    "https://openalex.org/W2796265726",
    "https://openalex.org/W2965658867",
    "https://openalex.org/W6745614327",
    "https://openalex.org/W2997884640",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W3128776197",
    "https://openalex.org/W3171853541",
    "https://openalex.org/W3214821343",
    "https://openalex.org/W6796931752",
    "https://openalex.org/W6809978379",
    "https://openalex.org/W6802648153",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2963778169",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3105255022",
    "https://openalex.org/W4286910290",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W4221146853",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3165607318"
  ],
  "abstract": "Hyperspectral image (HSI) classification has been a hot topic for decides, as hyperspectral images have rich spatial and spectral information and provide strong basis for distinguishing different land-cover objects. Benefiting from the development of deep learning technologies, deep learning based HSI classification methods have achieved promising performance. Recently, several neural architecture search (NAS) algorithms have been proposed for HSI classification, which further improve the accuracy of HSI classification to a new level. In this paper, NAS and Transformer are combined for handling HSI classification task for the first time. Compared with previous work, the proposed method has two main differences. First, we revisit the search spaces designed in previous HSI classification NAS methods and propose a novel hybrid search space, consisting of the space dominated cell and the spectrum dominated cell. Compared with search spaces proposed in previous works, the proposed hybrid search space is more aligned with the characteristic of HSI data, that is, HSIs have a relatively low spatial resolution and an extremely high spectral resolution. Second, to further improve the classification accuracy, we attempt to graft the emerging transformer module on the automatically designed convolutional neural network (CNN) to add global information to local region focused features learned by CNN. Experimental results on three public HSI datasets show that the proposed method achieves much better performance than comparison approaches, including manually designed network and NAS based HSI classification methods. Especially on the most recently captured dataset Houston University, overall accuracy is improved by nearly 6 percentage points. Code is available at: https://github.com/Cecilia-xue/HyT-NAS.",
  "full_text": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 1\nGrafting Transformer on Automatically Designed\nConvolutional Neural Network for Hyperspectral\nImage Classiﬁcation\nXizhe Xue*, Haokui Zhang*, Bei Fang, Zongwen Bai, and Ying Li\nAbstract—Hyperspectral image (HSI) classiﬁcation has been\na hot topic for decides, as hyperspectral images have rich\nspatial and spectral information and provide strong basis for\ndistinguishing different land-cover objects. Beneﬁting from the\ndevelopment of deep learning technologies, deep learning based\nHSI classiﬁcation methods have achieved promising performance.\nRecently, several neural architecture search (NAS) algorithms\nhave been proposed for HSI classiﬁcation, which further improve\nthe accuracy of HSI classiﬁcation to a new level. In this\npaper, NAS and Transformer are combined for handling HSI\nclassiﬁcation task for the ﬁrst time. Compared with previous\nwork, the proposed method has two main differences. First, we\nrevisit the search spaces designed in previous HSI classiﬁcation\nNAS methods and propose a novel hybrid search space, consisting\nof the space dominated cell and the spectrum dominated cell.\nCompared with search spaces proposed in previous works,\nthe proposed hybrid search space is more aligned with the\ncharacteristic of HSI data, that is, HSIs have a relatively low\nspatial resolution and an extremely high spectral resolution.\nSecond, to further improve the classiﬁcation accuracy, we attempt\nto graft the emerging transformer module on the automatically\ndesigned convolutional neural network (CNN) to add global\ninformation to local region focused features learned by CNN.\nExperimental results on three public HSI datasets show that\nthe proposed method achieves much better performance than\ncomparison approaches, including manually designed network\nand NAS based HSI classiﬁcation methods. Especially on the most\nrecently captured dataset Houston University, overall accuracy\nis improved by nearly 6 percentage points. Code is available at:\nhttps://github.com/Cecilia-xue/HyT-NAS.\nIndex Terms —Hyperspectral image classiﬁcation, hybrid\nsearch space, transformer, global information.\nI. I NTRODUCTION\nRemote sensing observation plays an important role in\nearth observation and has many applications in agriculture and\n* represents equal contribution. Manuscript received April 17, 2022;\naccepted May 26, 2022. Date of publication June 8, 2022; date of current\nversion June 20, 2022 This work was supported by the National Natu-\nral Science Foundation of China (61871460, 62107027), Natural Science\nFoundation of Shaanxi Province (2020JM-556), China Postdoctoral Science\nFoundation (2021M692006). X. Xue and Y . Li are with the School of\nComputer Science, National Engineering Laboratory for Integrated Aero-\nSpace-Ground-Ocean Big Data Application Technology, Shaanxi Provincial\nKey Laboratory of Speech & Image Information Processing, Northwestern\nPolytechnical University, Xi’an, China. (email: xuexizhe@mail.nwpu.edu.cn,\nlybyp@nwpu.edu.cn). H. Zhang is with Harbin Institute of Technology,\nShenzhen. (e-mail: hkzhang1991@mail.nwpu.edu.cn). B. Fang is with the Key\nLaboratory of Modern Teaching Technology, Ministry of Education, Shaanxi\nNormal University, Xi’an, China. (e-mail: beifang@snnu.deu.cn). Z. Bai is\nwith with the School of Physics and Electronic Information, Yan’an Uni-\nversity, Yan’an, China. (e-mail: ydbzw@yau.edu.cn). (Corresponding author:\nYing Li.)\nmilitary [1], [2]. Among various remote sensing observation\ntechnologies, hyperspectral image (HSI) classiﬁcation is a fun-\ndamental but essential technique. Captured by the amounts of\nhyperspectral remote sensing imagers, the HSIs of hundreds of\nbands contain much richer spectral information than ordinary\nremote sensing images, and the characteristic of containing\nboth spatial and rich spectral information makes HSIs very\nuseful for distinguishing ground-cover objects. Due to this,\nthe HSI classiﬁcation technology is widely applied in various\nscenes, e.g., mineral exploration [3], plant stress detection [4],\nand environmental science [5], etc. However, in HSIs, feature\nvectors containing thousands of bands can be extracted from\neach spatial pixel location. Such high-dimensional features,\non the one hand, help classify the ground objects, and on\nthe other hand, increase difﬁculty in feature extraction. There-\nfore, it is worth exploring how to efﬁciently extract features\nfrom HSIs. During past decades, various features extractions\nhave been applied and designed to extract robust features\nfrom HSIs [6], [7], [8]. Very recently, Luo et.al proposed\na multi-structure uniﬁed discriminative embedding (MUDE)\nmethod, which overcomes the drawbacks of previous graph-\nbased methods that only consider the individual information\nof each sample [9]. In MUDE, the neighborhood, tangential,\nand statistical properties of each sample are introduced by\nusing neighborhood structure graphs. Duan et.al considered\nthe manifold structure and multivariate relationship of samples\nfrom HSI in their proposed method geodesic-based sparse\nmanifold hypergraph (GSMH) [10]. The non-linear similarity\nof the distribution of the sample on the manifold space is\nmeasured with the geodesic distance to build a manifold\nneighborhood for each sample. The ﬁnal method achieves\npromising performance.\nSince the year of 2012, deep learning has been developing\nrapidly and achieving remarkable results in various ﬁelds.\nInspired by this, researchers have brought the deep learning\nmethods in solving the problem of HSI classiﬁcation, and\ngained impressive performance. In 2013, Lin et al. utilized\nPCA to reduce the dimensionality of HSI from hundreds of\nspectral dimensions to dozens, then extract deep features from\na neighborhood region via SAE. From 2014 to 2015, Chen\net.al introduced another spectral dimension channel based on\nthe [11]. This additional channel directly takes the spectral\nfeatures extracted from the pixel to be classiﬁed as input, and\nits output is integrated with the spatial spectrum channels to\nform a dual-channel structure [12], [13]. In the same period,\nsome other methods tried to apply 1D and 2D-CNN in the\narXiv:2110.11084v3  [cs.CV]  6 Apr 2023\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 2\nHSI classiﬁcation. Speciﬁcally, 1D-CNNs are used to extract\ndeep spectral features [14], [15], and 2D-CNN are employed\nto extract deep spatial features from HSI blocks that have\nbeen compressed along the spectral dimensions [16], [17].\nAfter 2017, deep HSI classiﬁcation methods primarily focused\non extracting spatial-spectral features. Some work construct\na dual-channel network structure to obtain spectral features\nand spatial features separately, and then merge them to form\nspatial-spectral features [18]. Additionally, 3D-CNN is also\na popular choice to capture the spatial-spectral joint features\ndirectly [19], [20]. Since 2017, various optimized 3D-CNN\nhave been applied on the HSI classiﬁcation task [21], [22],\nbesides which some transfer learning methods have also been\ndrawn into the classiﬁcation of HSI images [22], [23]\nThe deep HSI classiﬁcation methods give full play to the\nability of extracting robust features independently. These deep\nHSI classiﬁcation approaches show a signiﬁcant advantage\nin classiﬁcation performance compared to traditional HSI\nclassiﬁcation algorithms. However, these deep HSI classiﬁ-\ncation approaches face a problem. Speciﬁcally, the network\narchitectures in these methods are manually designed. For deep\nlearning methods, designing an efﬁcient network architecture\nis difﬁcult, time-consuming, labor-intensive and requires a lot\nof veriﬁcation experiments. This problem is even more serious\nin HSI classiﬁcation. Because HSIs data are very different\nfrom each other in the number of bands, spectral range and\nspatial resolution, the suitable architectures are also different\nfor different HSIs data. Therefore, it is usually necessary to\ndesign different network architectures for different HSIs data.\nMoving beyond manually designed network architectures,\nNeural Architecture Search (NAS) techniques [24] seek to\nautomate this process and ﬁnd not only good architectures, but\nalso their associated weights for a given image classiﬁcation\ntask. NAS provides an ideal solution to liberate people from\nthe heavy work of network architecture design. Chenet al. [25]\nﬁrst introduced DARTS into the HSI classiﬁcation task. This\nwork compressed the spectral dimension of HSIs to tens of\ndimensions through a point wise convolution, and then directly\nused DARTS to search a 2D CNN that is suitable for specﬁc\nHSI dataset. Later, Zhang et al. [26] made an in-depth analysis\nof the structural characteristics of HSIs and proposed 3D-\nANAS. In their work, a 3D asymmetric CNN is automatically\ndesigned under a pixel to pixel classiﬁcation framework, which\novercomes the problem of redundant operation existing in the\nprevious classiﬁcation framework and signiﬁcantly improves\nthe model inferring speed.\nIn this work, further improvements have been made on 3D-\nANAS from two aspects. 1) In 3D-ANAS, an asymmetric\ndecomposition convolution is introduced in the search space,\nconsidering the difference between the spatial resolution and\nthe spectral resolution of the HSI. However, this distinction\nbetween space and spectrum is only reﬂected on the operation\nlevel and is not free enough on the search space level.\nTo be more speciﬁc, the entire search space consists of a\nsequence of blocks, each of which contains a number of\noperations. 3D-ANAS takes some asymmetric decomposition\nconvolutions and other common convolutions as candidate\noperations, therefore 3D-ANAS can only separably process\nspatial and spectral information in operation level. It is difﬁ-\ncult for such an approach to incorporate some classic hand-\ndesigned experience. For example, in the classical manually\ndesigned HSI classiﬁcation network SSRN [27], the operation\nis completely separated into spectral processing and spatial\nprocessing. So in this article, we have constructed a new and\nmore efﬁcient search space, with more freedom to process the\ndifferences between spatial and spectral information. 2) The\npure CNN structure is good at capturing local information but\nignores global information, which has been proven to be very\nimportant for a lot of vision tasks. Inspired by this, we attempt\nto further improve the performance of automatically designed\nnetworks by integrating global information through grafting\ntransformer modules. Before classiﬁcation, we captured the\nrelative relationship of pixels in different spatial positions\nand used this relationship to ﬁne-tune the spatial-spectral\nfeatures to achieve better classiﬁcation accuracy. The main\ncontributions of this work include the following three aspects:\n1) By analyzing the characteristics of HSI, we propose\na NAS algorithm to automatically design CNN for\nHSI. Speciﬁcally, we proposed a novel hybrid search\nspace, which contains two types of cells, including space\ndominated cell and spectral dominated cell. The entire\nsearch space is built on these two cells and can be\ndivided into inner and outer spaces. The inner space\ndetermines the topology structure in the cell, and the\nouter space decides whether the space dominated cell\nor the spectral dominated cell is selected on the speciﬁc\nlayer.\n2) To further improve the classiﬁcation accuracy, we at-\ntempt to graft the emerging transformer module on the\nautomatically designed CNN to add global information\nto local region focused features learned by CNN. Ben-\neﬁting from the pixel to pixel classiﬁcation framework\nwe adopted here, the transformer module can be seam-\nlessly grafted to the end layer of CNN. Such a grafted\nstructure takes advantage of the ability of a transformer\nto capture inner relationship of pixels, while avoiding\nthe difﬁculties of training a complete transformer.\n3) Experimental results on three typical HSI classiﬁcation\ndatasets, including Pavia Center, Pavia University and\nHouston University have validated that the proposed\napproach obviously improves the classiﬁcation accuracy\nof auto designed HSI classiﬁcation approaches.\nThe rest of this paper is organized as follows. Section II\nreviews related work. Our approach is elaborated in Section\nIII. Section IV provides algorithm implementation details,\nextensively evaluates and compares the proposed Hy-NAS\nand HyT-NAS approaches with state-of-the-art competitors.\nFinally, we conclude this work in Section V .\nII. R ELATED WORK\nA. Hyperspectral Image Classiﬁcation via CNNs\nRecent years have witnessed growing interests in using\nCNNs to deal with HSI classiﬁcation problem. The develop-\nment of HSI classiﬁcation based on CNNs has mainly gone\nthrough three stages.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 3\nFrom 2015 to early 2016, researchers focused primarily\non HSI classiﬁcation based on 1D-CNNs and 2D-CNNs. The\nmethods based on 1D-CNNs generally employ 1D-CNNs to\nperform convolution along the spectral dimension to extract\nspectral features [28], [14]. Beyond methods based on 1D-\nCNNs, a series of 2D-CNNs based HSI classiﬁcation ap-\nproaches are with good prospects. Intuitively, regions sur-\nrounding the pixel can provide additional visual information\nfacilitating the classiﬁcation. After compressing HSIs to low-\ndimension, 2D-CNNs based methods crop a neighborhood\npatch around the pixel to be classiﬁed. Then, this patch is fed\nto a 2D-CNN to extract the spatial-spectral features [16], [17].\nCompared with the 1D-CNN based approaches, the methods\nbased on 2D-CNNs achieve higher accuracy. However, the\nclassiﬁcation results of methods that only use 2D-CNNs may\nnot keep structural information very well. Their visual results\nare much smoother than those of the 1D-CNNs methods. The\nsecond development stage mainly focuses on combining 1D-\nCNN and 2D-CNN to perform the HSI classiﬁcation. Taking\nthe advantages of 1D-CNN and 2D-CNN, the dual-channel\nCNN structure can further improve the accuracy of HSI\nclassiﬁcation [18], [23]. The third stage is the 3D-CNN stage.\nInspired by the 3D structure of HSIs, 3D-CNNs have been\ngradually used in HSI classiﬁcation approaches. Such methods\ndirectly construct 3D-CNNs to extract the spatial spectrum\nfeatures. Compared with those of dual-channel CNNs, the\nstructures of 3D-CNNs are always more simple, intuitive and\npowerful [19], [20].\nIn recent years, optimizing the structures of 3D-CNNs for\nHSI classiﬁcation has become mainstream. For example, the\nintroduction of efﬁcient residual structure [21], lightweight\ndesign and so on [22], [29], [30]. Based on the classical\nresidual structure, Zhong et al. [21] integrated the spectral\nresidual and spatial residual modules, and then constructed\na HSI classiﬁcation model SSRN based on the two residual\nmodules. Zhang et al. [22] developed a lightweight 3D-CNN\nto optimize the model structure and proposed two transfer\nlearning strategies (cross-sensor and cross-modality) to handle\nthe problem of small sample [22]. Zhao et al. [29] proposed\na lightweight spectral-spatial convolution HSI classiﬁcation\nmodule (LS2CM) to reduce network parameters and compu-\ntational complexity.\nB. Neural network architecture search\nTo overcome the heavy burden in manually designing\nnetwork architecture, researchers turn their attention to NAS,\nwhich can automatically and efﬁciently discover the neural ar-\nchitectures that are suitable for certain tasks. Recent years have\nwitnessed the success of NAS algorithms in plenty of general\ncomputer vision tasks, such as image classiﬁcation [31], object\ndetection [32] and semantic segmentation [33]. So far, the de-\nvelopment of NAS always happened in three phases: architec-\nture search based on evolutionary algorithm (EA) , architecture\nsearch based on reinforcement learning (RL) and architecture\nsearch based on gradient. RL based methods [31], [34] often\ncontain a recurrent neural network (RNN) to perform as a\nmeta-controller, generating potential architectures. In the NAS\nmethods enlightened by EA algorithms [35], [36], [37], a\nseries of randomly constructed models are evolved into a better\narchitecture through EA. However, most RL methods and EA\nmethods suffer from heavy computational cost and are less\nefﬁcient in searching stage. The gradient-based NAS methods\nare proposed recently and can alleviate this problem to some\nextent. The ﬁrst attempt DARTS is proposed in [24]. Unlike\nthe EA and RL-based method that train plenty of student\nnetworks, DARTS merely trains one super network in the\nsearching phase, reducing training workload signiﬁcantly. Get-\nting inspiration from DARTS, Chen et.al. [25] proposed a 3D\nAuto-CNN for HSI classiﬁcation. In the preprocessing stage,\n3D Auto-CNN heavily compresses the spectral dimension of\nraw HSIs through point wise convolution. The search space\nof 3D Auto-CNN are made up of 2D convolution operations\nin fact.\nVery recently, Zhang et.al. [26] proposed 3D-ANAS, in\nwhich pixel-to-pixel classiﬁcation framework and 3D hierar-\nchical search space are jointly used. In conventional patch-to-\npixel classiﬁcation frameworks, all information in a cropped\npatch is used to classify a single pixel. In a pixel-to-pixel\nframework, all pixels in a cropped patch are classiﬁed in one\niteration. Adopting a pixel-to-pixel classiﬁcation framework\nreduces repeat operations, speeding up inference efﬁciency\nsigniﬁcantly. In the 3D hierarchical search space, all operations\nare in 3D structure and the widths of networks can be adjusted\nadaptively in this work according to the characteristics of\ndifferent HSIs. Beneﬁting from these two points, 3D-ANAS\nachieves promising performance. Unfortunately, 3D-ANAS\nstill has two shortcomings:\n1) Previous work has indicated that learning the spectral\nand spatial representations separately is beneﬁcial to\nextracting more discriminative features, such as SSRN.\nAlthough various asymmetric convolutions in the search\nspace of 3D-ANAS allow the ﬁne-tuning of the convo-\nlution kernel size and receptive ﬁeld along spectral and\nspatial dimensions, this adjustment is limited inside a\ncell. Adjusting the proportions of spectral and spatial\nconvolutions across the entire network is infeasible in\nthis framework.\n2) The pure convolutional structure mainly focuses on local\nneighborhood information, while ignoring the global\nrelationship information among the whole input patch,\nwhich is often critical for high-level classiﬁcation task.\nTo overcome these two issues mentioned above, we propose\na new NAS method for HSI classiﬁcation. Speciﬁcally, to\naddress the ﬁrst issue, we design a hybrid search space that\nconsists of two kinds of cells. One is space dominated cell and\nanother is spectrum dominated cell. The hybrid search space\nhas more ﬂexible structures in selecting spatial or spectral\nconvolution than the search space proposed in 3D-ANAS.\nAiming to solve the second problem, a light transformer\nstructure is grafted to the end of CNN, playing a similar role\nas a CRF to dig out the relationship between pixels.\nC. Vision Transformer\nBy in-depth analysis of the attention mechanism, Vaswani et\nal. [38] proposed the Transformer model. Compared with the\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 4\nFig. 1. Workﬂow of proposed method.\nRNN model previously applied to the NLP problem, Trans-\nformer improves the computational efﬁciency signiﬁcantly.\nBecause its structure can handle the elements in sequence in\nparallel. Besides, the Transformer inherits and further expands\nthe ability of capturing the relationship between elements\nin the sequence, in comprehension with RNN. As a result,\nthe introduction of Transformer has greatly promoted the\ndevelopment of NLP ﬁelds.\nIn recent years, transformer models have been adopted in\nimage processing and achieved very promising performance.\nDosovitskiy et al. proposed ViT [39], where the image is\ncut into patches then the patches are arranged into the input\nsequence for feature extraction. In order to keep sensitive to\nthe position information of the patches, position embedding\nis introduced in the ViT. Besides, an additional class token\nis designed to perform the ﬁnal classiﬁcation. ViT’s success\nin the fundamental visual tasks has greatly inspired the ﬁeld\nof CV . Although the performance of ViT is relatively good,\nthere still exist some problems, for instance, ViT has low\ncomputational efﬁciency and is hard to train. To alleviate the\nproblem that the ViT is hard to train, Touvron et al. [40]\nproposed to use knowledge distillation to train ViT models,\nand achieved competitive accuracy with the less pre-training\ndata. From the perspective of reducing computational cost\nand improving inference speed, Liu et al. [41] proposed the\nSwin transformer. Swin Transformer limits the calculation of\nattention to pixels within a small window, which reduces\nthe amount of calculation. Moreover, a shifted window based\nMSA is proposed, which makes the attention cross different\nwindows. The Swin transformer has achieved higher accuracy\nthan previous CNN models on tasks such as dense prediction.\nVery recently, after conducting a detailed analysis of the\nworking principle of CNN and transformer, Graham et al.\nmixed CNN and transformer in their LeVit model, which\nsigniﬁcantly outperforms previous CNNs and ViT models with\nrespect to the speed/accuracy tradeoff [42].\nVery recently, there are several methods adopting trans-\nformer models to classify HSIs [43], [44], [45]. In [43], He\net al. designed a spatial-spectral Transformer, where a CNN\nis used to capture spatial information and a ViT is introduced\nto extract spectral relationship. Similarly, two parallel works\nalso adopt Transformer to extract the spectral relationship. The\nnetwork proposed in [44] starts with a spectral relationship\nextraction Transformer and ends with several decoders. In\nSpectralFormer [45], a sequence of patches extracted from the\ninput HSI is fed into Transformer.\nRelevant to fusing the strength of CNN and the transformer\nmodel, our work is closely related to Levit. The difference\nis that the main body of our network still relies on an\nautomatically designed CNN. In Levit, the transformer part\nis also the main part of feature extraction. The structure of the\nhigh-level CNN is equivalently replaced with the transformer\nstructure. In our work, the transformer model is just to further\ncapture the spatial relationship based on the features extracted\nby CNN. Compared to works that also adopt transformer\nmodels, our proposed HyT-NAS is a hybrid structure which\ninherits advantages of NAS and strengths of Transformer. Such\nstructure makes it more stable and easier to train, while gaining\nbetter performance. For example, on the Houston University\ndataset, with only 450 training samples, our proposed HyT-\nNAS achieves 91.14% overall accuracy, which is 3.13 percent-\nage points higher than 88.01%, the overall accuracy of Spec-\ntralFormer trained with 2823 training samples. In fact, such\na phenomenon is consistent with the discovery presented in\nrecent research works [42], [46], [47], [48]. Hybrid structures\ngenerally achieve better performance than pure CNNs or ViTs\nas hybrid structures combine both advantages from CNN and\nViT. In addition, previous Transformer structures adopted in\nHSI classiﬁcation methods always focus on capturing spectral\nrelationship, while in our work, the Transformer is responsible\nfor capturing the relationship from all input space.\nIII. P ROPOSED METHOD\nIn this section, the proposed method is introduced in de-\ntail. First, as the proposed method contains more steps than\nprevious deep learning based HSI classiﬁcation approaches,\nwe introduce the overall workﬂow brieﬂy. Next, we elaborate\non the proposed hybrid search space and compare it with the\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 5\nFig. 2. Architectures of supercells and the founded compact cells. (a) Candidate operations in space dominated supercell; (b) Candidate operations in spectrum\ndominated supercell; (d) Founded space dominated compact cell; (e) Founded spectrum dominated compact cell. In (a) and (b), different types of possible\noperation is represented as arrows with different colors. Topological architecture searching strategy aims to ﬁnd a compact cell, in which each node only keep\nthe two most valuable inputs, each of which is fed to the selected operation. For a more clear and concise presentation, we only show three nodes in the cell\nand three convolution operations in the search space.\nsearch space proposed in 3D-ANAS [26]. Then we explain\nthe reason for grafting the transformer module to the searched\nCNN and present the architecture of the grafted transformer\nmodule. Finally, we will brieﬂy introduce our training process.\nA. Overall workﬂow\nAs shown in Fig.1, the workﬂow of the overall classiﬁcation\nframework can be divided into the following steps:\n1) Samples extraction: Some pixels are randomly extracted\nfrom the whole HSIs according to certain proportions and\nrules. The collected sample pixels are divided into training\nset and validation set. The rest are reserved as test set.\n2) Searching: The collected training samples are fed into the\nCNN super network stacked by the space dominated cell and\nspectrum dominated cell. The training loss aims to minimize\nthe loss between the prediction label and the groundtruth.\nThe prediction accuracy of the network is validated on the\nvalidation set at a certain interval, and the loss and veriﬁcation\naccuracy are recorded.\n3) Deducing the ﬁnal network and grafting transformer:\nThe weight of the super network model with highest validation\naccuracy is used to deduce the ﬁnal component network.\nAccording to the weight of the search model, the type of cell\nand the topology inside the cell are ﬁxed in each layer. Besides,\na ﬂexible transformer structure is grafted at the end of the CNN\nnetwork to capture the relationship between pixels.\n4) Optimizing the ﬁnal compact network: The training set\nis taken to optimize the grafted CNN-Transformer network\nstructure, using the same loss as the searching stage.\n5) Inference: After training, the compact model with the\nhighest veriﬁcation accuracy and the smallest loss is tested\non the test set.\nB. Hybrid search space\nCell structure: In 3D-ANAS, authors have already noticed\nthat processing spatial and spectral information separately has\nbetter performance than using 3D convolution. In their work,\nclassiﬁcation accuracy is improved by introducing asymmetric\nsearch space. In this work, we further extend this discovery\nand propose hybrid search space, which consists of space\ndominated cells and spectrum dominated cells. As shown in\nFig.2, The space dominated cell only contains spatial convo-\nlutions and 3D convolutions (instead of using the standard 3D\nconvolution, we adopt separable 3D convolution as it has fewer\nparameters. In the following paragraphs, we call separable 3D\nconvolution as 3D convolution for short), and the spectrum\ndominated cell includes some spectral convolutions and 3D\nconvolutions. After searching, each layer can only keep one\nspace cell or spectrum cell, and different layers do not share\nthe cell structure.\nCompared with the search space proposed in 3D-ANAS,\nour designed hybrid search space is more ﬂexible in selecting\ndifferent operations to process spatial and spectral information.\nNote that this is relatively important for HSI datasets, as HSI\ndatasets have a special characteristic, that is HSI datasets have\ndifferent relatively low spatial resolution and extremely high\nspectral resolution. Roughly processing spatial and spectral\ninformation usually generates inferior classiﬁcation accuracy.\nIn speciﬁc, the space dominated cell includes the following\noperations:\n• acon 3-1: LReLU-Conv(1 ×3 ×3)-BN;\n• acon 5-1: LReLU-Conv(1 ×5 ×5)-BN;\n• asep 3-1: LReLU-Sep(1 ×3 ×3)-BN;\n• asep 5-1: LReLU-Sep(1 ×5 ×5)-BN;\n• con 3-3: LReLU-Conv(1 ×3 ×3)-Conv(3 ×1 ×1)-BN;\n• con 3-5: LReLU-Conv(1 ×3 ×3)-Conv(5 ×1 ×1)-BN;\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 6\nFig. 3. Architecture of our ﬁnal classiﬁcation framework. Up: The searched compact network. The ﬁnal selected blocks are marked as Selected; Down: The\nstructure of grafted transformer.\n• skip connection: f(x) =x;\n• discarding: f(x) = 0.\nThe spectrum dominated cell includes the following opera-\ntions:\n• econ 3-1: LReLU-Conv(3 ×1 ×1)-BN;\n• econ 5-1: LReLU-Conv(3 ×1 ×1)-BN;\n• esep 3-1: LReLU-Sep(3 ×1 ×1)-BN;\n• esep 5-1: LReLU-Sep(5 ×1 ×1)-BN;\n• con 3-3: LReLU-Conv(1 ×3 ×3)-Conv(3 ×1 ×1)-BN;\n• con 3-5: LReLU-Conv(1 ×3 ×3)-Conv(5 ×1 ×1)-BN;\n• skip connection: f(x) =x;\n• discarding: f(x) = 0.\nwhere LReLU, BN, Conv and Sep represent LeakyReLU\nactivation function, batch normalization, common convolution\nand separable convolution.\nArchitecture searching strategy: The network architecture\nsearch process can be divided into inner and outer search parts.\nThe outer search part determines the cell type of this layer and\nthe inner search strategy decides the cell’s internal topology\nstructure. The ﬁnally searched L-layer network may contain\nL different cell structures and every cell contains a sequence\nof N nodes.\nThe inputs for each node consist of the outputs of all\nprevious nodes and two inputs of the current cell. Assuming\nthat each path in a cell contains all the P candidate operations,\nthe output of node xi is:\nxi =\nj=P∑\nj=1\n(\nωj\ni •oj\ni\n)\n(1)\nwhere o and ω represent the different convolution operations\nand their corresponding weights, respectively. This weight is\nlearnt through inner search according to the back propagation\ngradient. The output of a cell hk\nl is obtained by:\nhk\nl = concat\n(\nxk\ni |i ∈{1, 2, ··· , N}\n)\n. (2)\nwhere k denotes the cell type and l represents the layer\nnumber.\nWhen optimizing the internal topology of a cell, the outer\nselection on cell types is also ongoing. Speciﬁcally, two types\nof cells are provided for each layer, focusing on spatial\ninformation and spectral information, respectively. In each\nlayer, the outputs corresponding to two types of cells are\nweighted via learnable weights αi and βi, and then combined\nto form the cell output hk\nl . The output of layer l can be\nexpressed as:\nhl = concat (αl ·hspa\nl + βl ·hspe\nl ) (3)\nAfter the stage of searching for network architectures, we\nbuild a compact network according to the learned structure\nparameters ω, α and β. Speciﬁcally, for inner topology struc-\nture, we keep the two operations corresponding to the top two\nweights and prune the rest in each cell. For outer structure,\nwe compare the α and β, then reserve the cell whose weight\nis bigger.\nC. The structure of Transformer\nSo far, the proposed architecture has been a pure convolu-\ntion structure. The ﬁnal compact network founded by Hy-NAS\nis also a pure CNN. The Hy-NAS algorithm not only improves\nthe external structure but also reserves the inductive bias\nof convolution operations. In other words, the ﬁnal compact\nnetwork founded by Hy-NAS also inherits the disadvantage\nof pure CNN. Pure CNN is good at extracting local features\nand ignores global relationships. Adding global information to\nlocal features always leads to much better performance, which\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 7\nFig. 4. False color composites and ground truth maps of Pavia University.\nhas been veriﬁed by previous non-local related works [49] and\nthe emerging transformer models [40]. Especially for some\ndense prediction tasks, using global information may bring a\nsigniﬁcant improvement [50].\nTherefore, to further improve the performance of Hy-NAS,\nwe attempt to integrate global information to features learned\nby pure CNN. A natural idea is to add some non-local modules\ninto the search space. As Hy-NAS is a NAS algorithm,\nadding non-local modules into search space does integrate\ninformation, but it also increases the complexity of search\nspace and the difﬁculty of training super net. Here, we make a\ntrade-off. Speciﬁcally, we graft a ﬂexible transformer module\nat the end of the ﬁnal compact network founded by Hy-NAS.\nFinally, we obtain a new HSI classiﬁcation method, HyT-NAS.\nSuch a grafting operation integrates global information\nto features learned by CNN, while avoiding introducing a\ncomplex search space which may improve the workload of\narchitecture search. Inspired by the promising performance of\ntransformer models, we choose to graft a transformer module\nto integrate global information. As shown in the bottom half\nof Fig.3.\nBefore being split to sequence and sent to the transformer\nunit, the feature map f of size(B,C,W,H ) from the encoder\nis reshaped and transposed to f ∈(B,N,C ). The Q,K,V\nis calculated through a linear layer and batch normalization\nlayer (refers to linear projection in Figure 3). Here, linear-\nprojection is responsible for mapping input vectors to three\ndifferent feature spaces Q, Kand V, which play different roles\nin the following computational procedure. The deﬁnitions and\nfunctions of Q,K,V can be found in [38]. The f is the input\nof the attention layer, and fattn is computed according to Eq.4:\nfattn = softmax\n(QKT\n√dk\n+ P\n)\nV (4)\nwhere dk denotes the dimensionality of V, and P means\nrelative position embedding (RPB).\nPh\n(x,y),(x′,y′) = Q(x,y),: ·K(x′,y′),: + Bh\n|x−x′|,|y−y′|· (5)\nwhere Bh represents the translation-invariant attention bias.\nThe output fout of transformer can be generated as Eq.6 and\nthen reshaped to the same dimensionality as the input f:\nfout = MLP (MLP (AF(BN (fatt )) +f)) (6)\nFig. 5. False color composites and ground truth maps of Pavia Center.\nin which the MLP and BN denote multi-layer perception and\nbatch normalization layer, respectively. AF means activation\nfunction. Speciﬁcally, a Hardswish function is employed in\nthis work.\nD. Training Process\nIn this work, we have followed the pixel-to-pixel classi-\nﬁcation framework of 3D-ANAS. Therefore, to fairly verify\nthe effectiveness of the proposed contributions, we apply the\nsame sampling rules, searching and training strategies as those\nin 3D-ANAS. After taking a 3D image cube from raw HSI\nand predicting the class of each 2D position in the cube, the\ncross entropy loss has been calculated according to the sparse\ntraining label map.\nIV. E XPERIMENTS\nExperiments are conducted on a server with an Intel(R)\nXeon(R) Gold 6230 CPU @ 2.10GHz, 512 GB of memory,\nand Nvidia Tesla V100 32 GB graphics card. The training\nand testing experiments were implemented by using the open-\nsource framework Pytorch 1.8 1\nA. Data Description\nTo evaluate the effectiveness of the proposed NAS algo-\nrithm, we conduct comparison experiments on three represen-\ntative HSI datasets, namely Pavia University, Pavia Center and\nHouston University. In turn, the false color composites and\nground truth maps of these three HSIs are presented in Fig.4-\nFig.6. The corresponding sample distribution information is\nlisted in Table I.\nPavia University and Pavia Center were captured by the\nROSIS-3 sensor in 2001 during a ﬂight campaign over Pavia,\nNothern Italy. Due to low SNR, some frequency bands were\nremoved. The remaining 103 channels are used for classiﬁca-\ntion. These datasets have the same geometric resolution, that\nis 1.3 meters. Each dataset covers nine different land cover\ncategories. Part of the categories are overlapped. Please ﬁnd\nmore details in Fig.4 and Fig.5. Pavia University consists of\n610 × 340 pixels and Pavia Centre covers 1096×715 pixels.\nThe Houston University was captured by the ITRES-CASI\n1500 hyperspectral Imager over the University of Houston\n1https://pytorch.org/docs/1.8.0/\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 8\nFig. 6. False color composites and ground truth maps of Houston University.\nTABLE I\nSAMPLE DISTRIBUTION INFORMATION OF DATASETS\nPavia University Pavia Center Houston University\nClass Land Cover Type No.of Samples Land Cover Type No.of Samples Land Cover Type No.of Samples\n1 Asphalt 6631 Water 824 Healthy Grass 1251\n2 Meadows 18649 Trees 820 Stressed Grass 1254\n3 Gravel 2099 Asphalt 816 Synthetic Grass 697\n4 Trees 3064 Self-Blocking Bricks 808 Trees 1244\n5 Painted Metal Sheets 1345 Bitumen 808 Soil 1242\n6 Bare Soil 5029 Tiles 1260 Water 325\n7 Bitumen 1330 Shadows 476 Residential 1268\n8 Self-Blocking Bricks 3682 Meadows 824 Commercial 1244\n9 Shadows 947 Bare Soil 820 Road 1252\n10 - - - - Highway 1227\n11 - - - - Railway 1235\n12 - - - - Parking Lot 1 1233\n13 - - - - Parking Lot 2 469\n14 - - - - Tennis Court 428\n15 - - - - Running Track 660\nTotal 42776 Total 7456 Total 15029\nTABLE II\nTHE DISTRIBUTION INFORMATION OF TRAINING , VALIDATION AND TEST\nSETS\nSetting Dataset Training Validation Test Training%\nPavia U 180 90 42506 0.42%\n20 pixels/class Pavia C 180 90 7186 2.41%\nHouston U 300 150 14579 2.00%\nPavia U 270 90 42416 0.63%\n30 pixels/class Pavia C 270 90 7096 3.62%\nHouston U 450 150 14429 2.99%\ncampus and the neighboring urban area. Compared with the\naforementioned two datasets, Houston University has lower\nspatial resolution but much higher spectral resolution. Its\nspatial resolution is 2.5 m and it contains 144 spectral bands,\ncovering the wavelength range of 360–1050 µm. This dataset\nalso covers a wider area and more abundant land cover objects.\nIn speciﬁc, the Houston University dataset consists of 349 ×\n1905 pixels and includes 15 land-cover classes of interest.\nB. Experiment Design\nIn order to validate the effectiveness of the proposed al-\ngorithm, we conduct experiments in two different settings. In\nsetting one, 20 and 10 labeled pixels are randomly extracted\nfrom each class to build a training set and validation set. The\nrest is used as a test set. In setting two, the number of training\nsamples of each class is increased to 30. Others keep the\nsame with that in setting one. More details about the sample\ndistribution are listed in Table II. To ensure the fairness and\nstability of the comparison, we repeat each experiment ﬁve\ntimes and take the average values as the ﬁnal results.\nC. Implementation Details\nSimilar to 3D-ANAS [26], the proposed method also has\ntwo optimizing stages and one inference stage. In this section,\nwe introduce the different settings in the aforementioned\nstages on three different datasets. For brevity, the settings\nthat are consistent with the baseline 3D-ANAS would not be\nmentioned here.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 9\nT ext \n（ a ） （ b ） （ c ） （ d ） （ e ） \n（ f ） （ g ） （ g ） （ h ） （ i ） \nFig. 7. Comparison experimental results on Pavia University using 30 training samples in each class. (a) False color composite; (b) 3D-LWNet, OA=89.25%; (c)\nSSRN, OA=91.95; (d) 1-D Auto-CNN, OA=81.75%; (e) 3-D Auto-CNN, OA=93.36%; (f) 3D-ANAS, OA=98.05%; (g) Hy-NAS, OA=98.55%; (h) HyT-NAS,\nOA=99.18%; (i) HyT-NAS+OV , OA=99.52%; (j) Ground truth map;\nTABLE III\nCOMPARISON EXPERIMENTAL RESULTS ON PAVIA UNIVERSITY USING 20\nTRAINING SAMPLES EACH CLASS\nMod-\nels\n3D-\nLWNet SSRN\n1-D\nAuto-\nCNN\n3-D\nAuto-\nCNN\n3D-\nANAS†\nHy-\nNAS\nHyT-\nNAS\nHyT-\nNAS\n+OV\n1 82.43 99.54 69.69 88.24 92.72 97.77 98.88 98.79\n2 84.76 99.31 76.37 90.72 96.18 98.88 97.86 99.16\n3 76.88 94.36 73.43 92.11 97.32 98.07 98.44 98.12\n4 91.45 94.95 90.2 81.27 95.49 95.81 97.45 98.06\n5 96.23 99.25 96.54 93.12 100 99.92 99.31 99.77\n6 92.50 71.76 75.48 98.47 96.89 94.48 99.56 98.92\n7 93.56 73.64 88.83 96.14 97.19 99.92 100 100\n8 96.01 86.27 77.59 96.84 93.64 92.03 95.19 96.44\n9 89.16 98.72 96.66 79.62 100 100 100 100\nOA 87.10 91.99 77.65 91.16 95.74 97.43 98.03 98.77\nAA 89.22 90.87 82.75 90.72 96.60 97.43 98.41 98.81\nK 83.35 89.60 71.51 88.51 94.37 96.59 97.39 98.37\nSearching: For three different datasets, we construct three\ndifferent super nets, which share the same outline structure.\nSpeciﬁcally, in the outer structure, each super net consists of\nfour layers of super cells and each layer is made up with two\ndifferent super cells, space dominated super cell and spectrum\ndominated super cell. In the inner structure, each cell has\na sequence of three nodes. The entire searching process is\ncarried out on a NVIDIA V100 card with 32G memory. For\nPavia University and Pavia Centre, we crop the patches with\nspatial resolution of 24×24 as searching samples, and the\nbatch size is set to 6. On Houston University, the crop size\nTABLE IV\nCOMPARISON EXPERIMENTAL RESULTS ON PAVIA UNIVERSITY USING 30\nTRAINING SAMPLES EACH CLASS\nMod-\nels\n3D-\nLWNet SSRN\n1-D\nAuto-\nCNN\n3-D\nAuto-\nCNN\n3D-\nANAS†\nHy-\nNAS\nHyT-\nNAS\nHyT-\nNAS\n+OV\n1 82.32 99.90 76.43 89.70 95.24 94.43 99.07 99.03\n2 88.84 99.53 81.9 97.92 98.16 99.74 99.26 99.80\n3 83.74 88.26 74.04 92.31 97.44 99.32 99.17 99.81\n4 94.11 93.16 93.38 71.22 98.52 98.52 97.45 97.82\n5 96.90 100 96.13 95.12 99.77 100 100 100\n6 92.10 66.97 81.81 96.96 99.46 100 99.74 100\n7 95.46 99.69 88.23 95.99 99.92 100 100 99.92\n8 93.19 89.77 74.05 94.98 98.85 96.19 98.90 99.18\n9 92.43 99.24 95.65 80.64 99.89 100 100 100\nOA 89.25 91.95 81.75 93.36 98.05 98.55 99.18 99.52\nAA 91.01 92.95 84.62 90.54 98.58 98.69 99.29 99.51\nK 86.05 89.59 76.55 91.50 97.42 98.08 98.92 99.37\nof patches is set to 14x14, and the batch size is 5. On all\nthree datasets, the Adam optimizer with both learning rate and\nweight attenuation of 0.001 is used to optimize the architecture\nparameters ( α, β and ω). The standard SGD optimizer is\napplied to update the super net parameters (learnable kernels in\ncandidate operations), where momentum and weight decay are\nset to 0.9 and 0.0003, respectively. The learning rate decays\nfrom 0.025 to 0.001 according to the cosine annealing strategy.\nFor Pavia University and Pavia Centre, the ﬁrst 15 epochs\nare the warm-up stage, in which we only optimize super net\nparameters. Because Houston University is more challenging,\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 10\n（ a ） （ b ） （ c ） （ d ） （ e ） \n（ f ） （ g ） （ h ） （ i ） （ j ） \nTrees\nWater Bare Soil\nSelf-Blocking Bricks\nAsphalt\nTiles\nBitumen\nMeadows\nShadows\nFig. 8. Comparison experimental results on Pavia Centre using 30 training samples each class. (a) False color composite; (b) 3D-LWNet, OA=94.22%;\n(c)SSRN, OA=98.72; (d)1-D Auto-CNN, OA=96.79%; (e) 3-D Auto-CNN, OA=96.99%; (f) 3D-ANAS, OA=99.24%; (g) Hy-NAS, OA=99.33%; (h) HyT-\nNAS, OA=99.44%; (i) HyT-NAS+OV , OA=99.49%; (j) Ground truth map;\nTABLE V\nCOMPARISON EXPERIMENTAL RESULTS ON PAVIA CENTRE USING 20\nTRAINING SAMPLES EACH CLASS\nMod-\nels\n3D-\nLWNet SSRN\n1-D\nAuto-\nCNN\n3-D\nAuto-\nCNN\n3D-\nANAS†\nHy-\nNAS\nHyT-\nNAS\nHyT-\nNAS\n+OV\n1 99.61 100 99.81 99.56 99.61 99.62 99.71 99.69\n2 91.85 98.68 80.9 87.79 93.16 95.18 96.61 96.43\n3 86.77 87.39 89.04 82.98 96.67 93.43 89.77 89.28\n4 92.85 87.79 73.53 98.85 99.81 96.80 100 100\n5 95.53 99.75 90.05 95.83 98.14 99.80 99.99 100\n6 80.97 88.58 95.53 93.22 98.70 99.23 97.06 97.97\n7 85.66 99.28 84.23 94.94 94.72 95.48 98.85 99.21\n8 85.06 100 98.17 95.12 99.89 99.77 99.95 99.96\n9 91.34 97.79 98.89 85.29 99.82 100 99.75 99.89\nOA 92.42 98.51 96.18 96.25 98.95 99.05 99.23 99.28\nAA 89.96 95.47 90.02 92.62 97.84 97.70 97.98 98.05\nK 89.41 97.89 94.6 94.71 98.51 98.65 98.81 98.98\nwe set 30 epochs for warming up. After the warming-up stage,\nwe alternately update the architecture parameters and super\nnetwork parameters in each iteration.\nGrafted network optimization: We crop patches with spatial\nresolution 32×32 to train the ﬁnal grafted network. Random\ncropping, ﬂipping, and rotation are introduced as data en-\nhancement strategies. Batch sizes on Pavia University and\nPavia Centre are set to 12. Batch size on Houston University\nis set to 16. At this stage, we use the SGD optimizer.\nThe initial learning rate is set to 0.1, decayed according\nTABLE VI\nCOMPARISON EXPERIMENTAL RESULTS ON PAVIA CENTRE USING 30\nTRAINING SAMPLES EACH CLASS\nMod-\nels\n3D-\nLWNet SSRN\n1-D\nAuto-\nCNN\n3-D\nAuto-\nCNN\n3D-\nANAS†\nHy-\nNAS\nHyT-\nNAS\nHyT-\nNAS\n+OV\n1 99.52 100 99.76 99.78 100.0 100.0 99.99 100.0\n2 93.92 99.34 87.9 92.48 95.83 96.75 96.40 96.98\n3 89.26 86.05 91.16 85.81 93.34 95.28 96.43 96.20\n4 91.1 79.70 79.72 97.32 97.77 99.85 99.96 100.0\n5 96.09 99.93 92.1 97.02 98.36 96.82 99.95 99.97\n6 90.73 93.62 96.47 95.91 99.57 98.97 99.69 99.90\n7 93.24 99.51 85.43 95.29 97.97 97.92 98.65 98.58\n8 87.32 99.99 97.88 95.13 99.40 99.69 99.27 99.31\n9 93.7 99.23 98.58 91.84 100.0 99.89 99.79 100.0\nOA 94.22 98.72 96.79 96.99 99.24 99.33 99.44 99.49\nAA 92.76 95.26 92.11 94.51 98.03 98.34 98.90 98.99\nK 91.92 98.18 95.47 95.76 98.92 99.05 99.20 99.28\nto the poly learning rate policy with power of 0.9 ( lr =\ninit lr·(1 − iter\nmax iter\npower\n)). The performance of the network\nis validated every 100 iterations.\nInference: For the grafted framework based on hybrid CNN\nand transformer, we introduced an overlap inference strategy\n(OV) to further improve the performance. Speciﬁcally, we\nuse a sliding window to crop small blocks (the stride is half\nof the window size), and input the cropped blocks into the\ncompact network. The average result of the overlapping area\nis considered as the ﬁnal prediction result. As the number\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 11\nTABLE VII\nCOMPARISON EXPERIMENTAL RESULTS ON HOUSTON UNIVERSITY\nUSING 20 T RAINING SAMPLES EACH CLASS\nMod-\nels\n3D-\nLWNet SSRN\n1-D\nAuto-\nCNN\n3-D\nAuto-\nCNN\n3D-\nANAS†\nHy-\nNAS\nHyT-\nNAS\nHyT-\nNAS\n+OV\n1 79.16 69.69 69.98 84.01 77.56 83.62 87.37 86.00\n2 71.61 96.81 60.19 85.65 82.27 79.66 88.30 88.48\n3 94.4 97.81 77.30 93.86 86.96 82.01 87.67 86.96\n4 74.25 85.79 49.02 66.77 82.78 70.92 71.76 72.41\n5 90.37 95.82 83.09 93.83 91.17 93.15 97.00 96.70\n6 84.92 81.71 50.46 80.43 96.27 94.58 98.95 98.98\n7 84.73 64.64 30.93 72.21 72.29 80.45 83.39 84.09\n8 52.01 96.53 50.53 70.96 60.79 78.83 80.15 80.56\n9 70.47 76.11 46.58 71.18 69.72 68.74 70.38 72.67\n10 92.15 83.35 69.73 96.43 86.55 92.23 97.14 99.00\n11 89.39 85.10 50.06 93.73 91.95 86.14 92.72 91.70\n12 60.04 74.15 63.44 87.95 84.37 85.12 88.60 86.95\n13 86.99 84.25 53.35 84.90 97.27 91.57 95.57 95.44\n14 92.99 93.14 76.73 92.99 99.25 98.24 100 100\n15 92.83 96.01 54.00 88.30 91.43 94.13 92.58 92.70\nOA 78.95 82.55 58.35 83.37 82.10 83.39 86.97 87.11\nAA 81.09 85.39 59.03 84.21 84.71 85.29 88.77 88.84\nK 77.32 81.13 55.18 82.07 80.67 82.06 85.92 86.07\nof tokens in our transformer module is ﬁxed, the multi-scale\nveriﬁcation method (MS) is not adopted here. While, using OV\nstrategy alone already achieves promising performance. The\nstructure we designed requires the input sequence to be a ﬁxed\nlength. Therefore, the image blocks should be on the same\nscale during training and veriﬁcation. Relaxing this restriction\nis considered as one of our future work.\nD. Comparison with state-of-the-art methods\nIn this section, we compare the proposed Hy-NAS and HyT-\nNAS with other four recent CNN-based HSI classiﬁcation\nmethods. The codes for all comparison methods are derived\nfrom the ofﬁcial codes: 3DLWNet 2, 1-D Auto-CNN and 3-D\nAuto-CNN3 and 3D-ANAS4. TABLE III-VIII list the results\nof the comparative experiment, and Figs.7-9 show the corre-\nsponding visual results. Here, we do not compare the inference\nspeeds, as these methods are implemented with different\nframeworks, which may introduce biases. The inference of\npixel-to-pixel framework has higher efﬁciency than that of\npatch-to-pixel framework, because the former framework does\nnot have repeat operations as explained in [26]. The method\nproposed in this paper adopts a pixel-to-pixel framework and\ninherits the advantage in inference efﬁciency.\nThe performance on the Pavia University is listed in TA-\nBLE III and TABLE IV. The corresponding visual comparison\nresults are shown in Fig.7. From the comparison results, we\ncan draw the following conclusions: 1) Compared with the\nmethod based on 1D CNN, the method based on 3D CNN\nusually gains better performance. Because jointly using the\nspectral and spatial information is beneﬁcial to improve the\nclassiﬁcation accuracy.\n2https://github.com/hkzhang91/LWNet\n3https://github.com/YushiChen/Auto-CNN-HSI-Classiﬁcation\n4https://github.com/hkzhang91/3D-ANAS\nTABLE VIII\nCOMPARISON EXPERIMENTAL RESULTS ON HOUSTON UNIVERSITY\nUSING 30 T RAINING SAMPLES EACH CLASS\nMod-\nels\n3D-\nLWNet SSRN\n1-D\nAuto-\nCNN\n3-D\nAuto-\nCNN\n3D-\nANAS†\nHy-\nNAS\nHyT-\nNAS\nHyT-\nNAS\n+OV\n1 84.81 90.91 72.25 87.50 90.01 90.01 78.94 80.59\n2 80.22 98.61 63.96 77.91 79.49 85.75 92.42 92.75\n3 93.45 97.75 76.61 92.74 98.78 85.08 98.17 99.85\n4 79.74 82.11 51.29 72.65 85.22 86.63 91.53 91.94\n5 90.90 91.51 82.25 96.14 99.00 99.50 96.84 98.34\n6 81.44 65.83 55.32 84.86 91.93 97.89 99.30 99.30\n7 87.83 81.52 34.20 73.03 66.53 76.30 83.06 91.43\n8 63.69 98.15 62.11 76.64 75.50 72.43 77.24 77.99\n9 77.50 67.71 49.84 71.10 81.93 74.34 86.72 88.45\n10 93.26 91.49 64.48 96.67 90.14 89.81 97.72 96.88\n11 91.04 93.38 50.23 92.31 87.36 87.03 94.73 96.23\n12 79.40 90.44 62.47 91.48 89.52 87.18 92.20 92.29\n13 90.41 69.84 50.92 85.8 88.81 94.64 98.60 99.07\n14 90.65 84.09 76.07 90.65 99.23 96.13 96.91 96.39\n15 92.47 97.27 62.94 88.85 85.97 96.29 94.03 98.39\nOA 84.17 86.60 60.36 84.45 85.81 86.22 90.04 91.14\nAA 85.12 86.71 61.00 85.22 87.29 87.93 91.89 92.66\nK 82.96 85.53 57.35 83.25 84.66 85.10 89.64 90.42\nCompared to 3D-ANAS 5, the proposed method adopts\nhybrid search space, which improves the ﬂexibility in pro-\ncessing spectral and spatial with different operations, achieving\nhigher classiﬁcation accuracy. 2) After grafting the transformer\nstructure, the proposed HyT-NAS achieves better performance\nthan other auto-designed methods. For example, HyT-NAS\nachieves 98.03% OA, 98.41% AA, and 97.39% K when 20\ntraining samples are extracted from each class, which are\n2.29%, 1.81%, and 3.02% higher than 3D-ANAS, respectively.\n3) The overlap inference enhancement strategy can further\nimprove the performance. As shown in Table III, using OV\nincreases OA, AA, and K by 0.74%, 0.40%, and 0.98%,\nrespectively.\nTo save space, we only present the visual results using\n30 training samples per class in Fig.7. In order to clearly\nillustrate the difference, we placed a partially enlarged patch\nin the upper right corner of each result map. It can be easily\nfound from the partially enlarged patch that there are fewer\nmisclassiﬁed pixels in the results of a series of HyT-NAS.\nSome Asphalt pixels (class 1, cyan) are incorrectly classiﬁed\nas Self-Blocking Bricks (class 8, red) by 3D-LWNet and 3-D\nAuto-CNN. A lot of pixels belonging to Self-Blocking Bricks\nare incorrectly classiﬁed as Meadows (Class 2, green) by 3D-\nANAS. But in the results of a series of HyT-NAS, all pixels\nbelonging to Asphalt and Self-Blocking Bricks are correctly\nclassiﬁed.\nTABLE V and VI collects the comparison results on Pavia\ncentre, and Fig.8 shows the visual results of qualitative anal-\nysis. Compared with the results on Pavia University, the ac-\ncuracy of these seven methods all improved to certain extents\nand the proposed HyT-NAS still attains the best performance.\n5For fairness, we rerun the code of 3D-ANAS on the same device (with\nnvidia V100 GPUs) with the proposed methods and reported the results as\n3D-ANAS†. For 3D-ANAS†, the performance is a little bit different from that\nin the orignial paper, we conjecture this is caused by the re-implemention on\ndifferent devices.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 12\n（ a ） \n（ b ） \n（ c ） \n（ d ） \n（ e ） \n（ f ） \n（ g ） \n（ h ） \n（ i ） \n（ j ） \nHealth grass\nStressed grass\nSynthetic grass\nTrees\nSoil\nWater\nResidential\nCommercial\nRoad\nHighway\nRallway\nParking Lot 1\nParking Lot 2\nTennis court\nRunning Track\nFig. 9. Comparison experimental results on Houston University using 30 training samples in each class. (a) False color composite; (b) 3D-LWNet, OA=84.17%;\n(c) SSRN, OA=86.60; (d) 1-D Auto-CNN, OA=60.36%; (e) 3-D Auto-CNN, OA=84.45%; (f) 3D-ANAS, OA=85.81%; (g) Hy-NAS+MS, OA=86.22%; (h)\nHyT-NAS, OA=90.04%; (i) HyT-NAS+OV , OA=91.14%; (g) Ground truth map;\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 13\nFig. 10. The ﬁnal architectures found for three datasets. From up to down are: Pavia University; Pavia Centre and Houston University.\nTABLE IX\nComparisons between different search spaces on Houston University\nSearch Space Model Size Transformer OA AA K\nSpectral 1.41 MB \u0017 83.04 85.25 81.68\nSpatial 1.48 MB \u0017 84.85 86.90 83.63\nSpectral + Spatial 1.44 MB \u0017 86.22 87.93 85.10\nSpectral 9.98 MB \u0013 88.18 89.75 87.22\nSpatial 10.04 MB \u0013 89.53 90.96 88.67\nSpectral + Spatial 10.00 MB \u0013 90.04 91.89 89.64\nObserving from Fig.8, the number of bitumen pixels that a\nseries of HyT-NAS approaches incorrectly classiﬁed into self-\nblocking Bricks is signiﬁcantly less than that of other meth-\nods. Although Hy-NAS with only improved hybrid spatial-\nspectrum search space still makes some false predictions on\nthe bitumen class, the introduction of the transformer ﬁnally\nhandles the problems very well.\nThe comparison results on Houston University are shown\nin Table VII and VIII and Fig.9. Compared with the ﬁrst\ntwo datasets, the Houston University contains more spectral\nbands and more object categories. Therefore, the classiﬁcation\naccuracy of all methods on this dataset is relatively low.\nThe classiﬁcation performance of different methods is quite\ndifferent. As shown in Fig.9, the result map of 1D Auto-CNN\nclearly shows the structural outlines of different buildings.\nFor example, the dark red part of the partially enlarged\narea (commercial, level 8). But many misclassiﬁed pixels are\ndistributed throughout the result image and look like salt\nand pepper noise, resulting in relatively poor visual effects.\nOn the contrary, 3D Auto-CNN showed very smooth results,\nin which the outline of the structure was almost lost. 3D-\nANAS and HyT-NAS have kept a relatively good balance\nbetween displaying good visual effects and maintaining the\ncontour structure, and gained better performance than other\nalgorithms. As shown in the enlarged image in Fig.9, 3D-\nANAS misclassiﬁes some pixels classiﬁed as land into stressed\ngrass and highway, while HyT-NAS has very few misclassiﬁed\npixels. From the TABLE VII and VIII, it is obvious that\nthe results of HyT-NAS are better than those of 3D-ANAS\nregardless of whether the training samples are 20 or 30.\nSpeciﬁcally, when there are 20 training samples for each\nclass, the OA, AA, and K of HyT-NAS are 86.97%, 88.77%,\nand 85.92%, respectively, which are signiﬁcantly higher than\nthat of 3D-ANAS. When the training samples of each class\nincrease to 30, the advantages of HyT-NAS and 3D-ANAS\nare more obvious. increasing by 4.23%, 4.60%, and 4.98% on\nOA, AA, and K respectively.\nE. Ablation study\nHSI has different spatial and spectral resolutions. During the\nsearching stage, different layers tend to select different types of\ncells. We speculate that merely maintaining a space dominated\ncell or a spectrum dominated cell would affect the performance\nof the algorithm, although both kinds of cells contain the\n3D convolution. Here, ablative experiments are conducted to\nverify the effectiveness of hybrid search space. Besides, we\nalso compared the classiﬁcation accuracy of the model with\nand without the transformer unit. The experiments are carried\nout on the most challenging dataset Houston University with\n30 training samples per class.\nAs shown in TABLE IX, the proposed method with hybrid\nsearch space achieves the highest accuracy. When only the\nspectral search space is retained in the model, the classiﬁcation\naccuracy is the lowest. Importing different types of cells\ncan dig out the spatial and spectral information jointly and\nfreely in HSI classiﬁcation tasks. In addition, the introduction\nof the transformer has brought about a 5% improvement in\naccuracy in all different search space settings. This illustrates\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 14\nthe importance of fully mining the associated information\nbetween pixels in the classiﬁcation of HSIs.\nF . Architecture analyze\nIn this section, the architectures searched by HyT-NAS are\nshown in Fig.10 and analyzed. Since the three datasets have\ndifferent spectral and spatial resolutions, and land covers,\nwe searched for the architecture on each dataset separately.\nAlthough these three architectures are different in topology\nand operations, they also have some common characteristics:\n1) 2D spatial convolution and 2D spectral convolution play\nimportant roles in the ﬁnal selected operations. As introduced\nin Section III-B, the search space we construct for searching\nthe internal topology includes not only 2D spatial convolution\nand 2D spectral convolution, but also 3D convolution. Even so,\nHyT-NAS tends to build a network with both 2D convolution\noperations and 3D convolution operations. In most cases,\n2D convolution operations are the main operation and 3D\nconvolution operations play the part of the complementary\noperation. The proportion of 2D convolution operations in the\nﬁnal network designed on Pavia centre and Pavia University\nare 41.67% and 52.78%, respectively. Under the architecture\nsearched for Houston University, 2D convolution operations\noccupied 44.44% of all operations. The proportions of 3D\nconvolution operations are 13.89%, 8.33%, and 16.67% on\nPavia centre, Pavia University and Houston University. This\nshows that although 3D convolution ﬁts the data characteristics\nof HSIs, widely utilizing it as in traditional algorithms is\nnot necessary. The 2D-3D mixed network architecture we\nsearched has fewer parameters and higher parameter utilization\ncompared with models under the same scale.\n2) In the ﬁnal network, 3D convolution operations are\ndistributed from the beginning to the end. However, 2D\nspectral convolutions dominate in the shallow layer, while the\nnumber of 2D spatial convolutions is relatively large in the\ndeep layer, as shown in Fig.10. In the architectures for the\nPavia Centre and Pavia University, the spectral convolutions\naccount for the majority in the ﬁrst two layers, but the spatial\nconvolutions account for the highest proportion in the last two\nlayers. In the ﬁnal architecture for Houston University, the\nﬁrst three layers are almost all spectral convolutions, and only\nthe last layer of the network is mainly spatial convolution. In\nclassic HSI classiﬁcation networks such as SSRN [21], spectral\nconvolution is always performed ﬁrst, followed by the spatial\nconvolution. Our experimental results are consistent with the\nmanual design experience.\n3) With the enrichment of spectral information, the pro-\nportion of spectral convolution in the ﬁnal network gradually\nincreases. In different HSIs, the richness of spectral informa-\ntion and spatial information are quite different. For example,\nboth Pavia University and Pavia Centre have only 102 bands,\nwhile Houston University has 144 bands. Traditional 3D\nconvolution pays the same attention to spatial information\nand spectral information. The hybrid spatial-spectral search\nspace we proposed can ﬂexibly adjust the ratio of spatial and\nspectral convolutions freely according to the proportion of the\nspatial-spectral information of the data itself. As shown in the\nFig. 11. Attention map analyse. (a) Input patch. Two pixels marked in red\ncolor are pixels to be classiﬁed ; (b) Region of interesting of CNN; (c)\nattention maps of different heads of transformer unit.\nFig.10, although the search space is exactly the same, 2D-\nspectral convolution and 3D convolution account for 33.33%\nand 25.00% of operations in Pavia University and Pavia centre,\nrespectively. In the architecture for Houston University, this\nproportion has risen to 44.44% . This may be because Houston\nUniversity has a larger number of spectra bands and requires\nmore spectral convolutions to extract rich spectral information.\nThe experimental results prove that the hybrid search space\ncan adapt well to the characteristics of different data.\nG. Attention map analyze\nIn this section, we analyze the difference between using\nand not using the transformer unit on Houston University.\nA visualized result is presented in Fig.11. After introducing\nthe transformer unit, the receptive ﬁeld of the feature map\nhas expanded from a small range to a wider global one. In\naddition, the CNN structure pays the same level of attention\nto each pixel in a local receptive ﬁeld, while the transformer\nunit can capture the relationship between global pixels in HSI\nadaptively.\nFig. 11(c) shows that the attention maps produced by\ndifferent heads inside the transformer are also different. This\nobservation indicates the effectiveness of multi-attention head\nmechanism in HSI classiﬁcation tasks. Various heads inside the\ntransformer focus on different types of information, according\nto the spatial position and neighborhood of the pixel itself. The\nmulti-attention mechanism fuses the information of different\nheads, resulting in a more comprehensive and robust feature\nmap.\nV. C ONCLUSION\nIn this paper, we have proposed an auto-designed HSI\nclassiﬁcation method based on the hybrid CNN-Transformer\nframework. The proposed HyT-NAS has been compared with\nother manual designed CNN based HSI classiﬁcation methods\n(3D-LWNet), automatic design CNN based methods (1-D\nAuto-CNN, 3-D Auto-CNN and 3D-ANAS) comprehensively\non three typical public HSI datasets. The experimental results\nshow that the HyT-NAS outperforms other state of the art\nDL based algorithms. Additionally, abundant ablation stud-\nies have been carried out to verify the effectiveness of the\nproposed hybrid spatial-spectral search space and the grafted\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 15\ntransformer. The results of the ablation study demonstrated\nthat the HyT-NAS does ﬁnd a local optimum architecture in the\narchitecture search space. Compared with the pure CNN based\nHSI classiﬁcation framework, the hybrid CNN-Transformer\nframework captures the global relationship between pixels.\nIn future work, we will focus on designing a more efﬁcient\nneural architecture search approach to automatically design a\nfull transformer architecture for HSI classiﬁcation.\nREFERENCES\n[1] M. J. Khan, H. S. Khan, A. Yousaf, K. Khurshid, and A. Abbas, “Modern\ntrends in hyperspectral image analysis: A review,” IEEE Access, vol. 6,\npp. 14 118–14 129, 2018.\n[2] Y . Gu, J. Chanussot, X. Jia, and J. A. Benediktsson, “Multiple kernel\nlearning for hyperspectral image classiﬁcation: A review,” IEEE Trans-\nactions on Geoscience and Remote Sensing , vol. 55, no. 11, pp. 6547–\n6565, 2017.\n[3] T. A. Carrino, A. P. Cr ´osta, C. L. B. Toledo, and A. M. Silva, “Hyper-\nspectral remote sensing applied to mineral exploration in southern peru:\nA multiple data integration approach in the chapi chiara gold prospect,”\nInternational journal of applied earth observation and geoinformation ,\nvol. 64, pp. 287–300, 2018.\n[4] J. Behmann, J. Steinr ¨ucken, and L. Pl ¨umer, “Detection of early plant\nstress responses in hyperspectral images,” ISPRS Journal of Photogram-\nmetry and Remote Sensing , vol. 93, pp. 98–111, 2014.\n[5] J. Transon, R. d’Andrimont, A. Maugnard, and P. Defourny, “Survey of\nhyperspectral earth observation applications from space in the sentinel-2\ncontext,” Remote Sensing, vol. 10, no. 2, p. 157, 2018.\n[6] W. Sun, J. Peng, G. Yang, and Q. Du, “Fast and latent low-rank\nsubspace clustering for hyperspectral band selection,”IEEE Transactions\non Geoscience and Remote Sensing, vol. 58, no. 6, pp. 3906–3915, 2020.\n[7] X. Zheng, Y . Yuan, and X. Lu, “Dimensionality reduction by spatial–\nspectral preservation in selected bands,” IEEE Transactions on Geo-\nscience and Remote Sensing , vol. 55, no. 9, pp. 5185–5197, 2017.\n[8] R. Hang and Q. Liu, “Dimensionality reduction of hyperspectral image\nusing spatial regularized local graph discriminant embedding,” IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote\nSensing, vol. 11, no. 9, pp. 3262–3271, 2018.\n[9] F. Luo, Z. Zou, J. Liu, and Z. Lin, “Dimensionality reduction and classi-\nﬁcation of hyperspectral image via multi-structure uniﬁed discriminative\nembedding,” IEEE Transactions on Geoscience and Remote Sensing ,\n2021.\n[10] Y . Duan, H. Huang, and T. Wang, “Semisupervised feature extraction\nof hyperspectral image using nonlinear geodesic sparse hypergraphs,”\nIEEE Transactions on Geoscience and Remote Sensing , 2021.\n[11] Z. Lin, Y . Chen, X. Zhao, and G. Wang, “Spectral-spatial classiﬁcation\nof hyperspectral image using autoencoders,” in 2013 9th International\nConference on Information, Communications & Signal Processing .\nIEEE, 2013, pp. 1–5.\n[12] Y . Chen, Z. Lin, X. Zhao, G. Wang, and Y . Gu, “Deep learning-based\nclassiﬁcation of hyperspectral data,” IEEE J. Sel. Topics Appl. Earch\nObserv. Remote Sens. , vol. 7, no. 6, pp. 2094–2107, Jun. 2014.\n[13] Y . Chen, X. Zhao, and X. Jia, “Spectral–spatial classiﬁcation of hyper-\nspectral data based on deep belief network,” IEEE J. Sel. Topics Appl.\nEarch Observ. Remote Sens. , vol. 8, no. 6, pp. 2381–2392, Jun. 2015.\n[14] H. Zhang and Y . Li, “Spectral-spatial classiﬁcation of hyperspectral\nimagery based on deep convolutional network,” in 2016 International\nConference on Orange Technologies (ICOT) . IEEE, 2016, pp. 44–47.\n[15] W. Hu, Y . Huang, L. Wei, F. Zhang, and H. Li, “Deep convolutional\nneural networks for hyperspectral image classiﬁcation,” Journal of\nSensors, vol. 2015, 2015.\n[16] K. Makantasis, K. Karantzalos, A. Doulamis, and N. Doulamis, “Deep\nsupervised learning for hyperspectral data classiﬁcation through convo-\nlutional neural networks,” in Proc. IEEE Conf. Int. Geosci. Remote Sens.\nSymp (IGARSS),, Jul. 2015, pp. 4959–4962.\n[17] J. Yue, W. Zhao, S. Mao, and H. Liu, “Spectral–spatial classiﬁcation of\nhyperspectral images using deep convolutional neural networks,”Remote\nSensing Letters, vol. 6, no. 6, pp. 468–477, 2015.\n[18] H. Zhang, Y . Li, Y . Zhang, and Q. Shen, “Spectral-spatial classiﬁcation\nof hyperspectral imagery using a dual-channel convolutional neural\nnetwork,” Remote Sensing Letters , vol. 8, no. 5, pp. 438–447, 2017.\n[19] Y . Li, H. Zhang, and Q. Shen, “Spectral–spatial classiﬁcation of hyper-\nspectral imagery with 3d convolutional neural network,”Remote Sensing,\nvol. 9, no. 1, p. 67, 2017.\n[20] Y . Chen, H. Jiang, C. Li, X. Jia, and P. Ghamisi, “Deep feature extraction\nand classiﬁcation of hyperspectral images based on convolutional neural\nnetworks,”IEEE Trans. Geosci. Remote Sens., vol. 54, no. 10, pp. 6232–\n6251, Oct. 2016.\n[21] Z. Zhong, J. Li, Z. Luo, and M. Chapman, “Spectral–spatial residual\nnetwork for hyperspectral image classiﬁcation: A 3-d deep learning\nframework,”IEEE Trans. Geosci. Remote Sens., vol. 56, no. 2, pp. 847–\n858, Feb. 2018.\n[22] H. Zhang, Y . Li, Y . Jiang, P. Wang, and C. Shen, “Hyperspectral\nclassiﬁcation based on lightweight 3-d-cnn with transfer learning,” IEEE\nTransactions on Geoence and Remote Sensing , vol. 57, no. 8, pp. 5813–\n5828, 2019.\n[23] J. Yang, Y .-Q. Zhao, and J. C.-W. Chan, “Learning and transferring deep\njoint spectral–spatial features for hyperspectral classiﬁcation,” IEEE\nTrans. Geosci. Remote Sens., vol. 55, no. 8, pp. 4729–4742, Aug. 2017.\n[24] H. Liu, K. Simonyan, and Y . Yang, “Darts: Differentiable architecture\nsearch,” in Proc. Int. Conf. Learn. Representations , 2019.\n[25] Y . Chen, K. Zhu, L. Zhu, X. He, P. Ghamisi, and J. A. Benediktsson,\n“Automatic design of convolutional neural network for hyperspectral\nimage classiﬁcation,” IEEE Transactions on Geoscience and Remote\nSensing, vol. 57, no. 9, pp. 7048–7066, 2019.\n[26] H. Zhang, C. Gong, Y . Bai, Z. Bai, and Y . Li, “3d-anas: 3d asymmetric\nneural architecture search for fast hyperspectral image classiﬁcation,”\narXiv preprint arXiv:2101.04287 , 2021.\n[27] Z. Zhong, J. Li, Z. Luo, and M. Chapman, “Spectral–spatial residual\nnetwork for hyperspectral image classiﬁcation: A 3-d deep learning\nframework,” IEEE Transactions on Geoscience and Remote Sensing ,\nvol. 56, no. 2, pp. 847–858, 2017.\n[28] S. Mei, J. Ji, Q. Bi, J. Hou, Q. Du, and W. Li, “Integrating spectral\nand spatial information into deep convolutional neural networks for\nhyperspectral classiﬁcation,” in Proc. IEEE Conf. Int. Geosci. Remote\nSens. Symp (IGARSS), , Jul. 2016, pp. 5067–5070.\n[29] Z. Meng, L. Jiao, M. Liang, and F. Zhao, “A lightweight spectral-\nspatial convolution module for hyperspectral image classiﬁcation,” IEEE\nGeoscience and Remote Sensing Letters , 2021.\n[30] S. Jia, Z. Lin, M. Xu, Q. Huang, J. Zhou, X. Jia, and Q. Li, “A\nlightweight convolutional neural network for hyperspectral image classi-\nﬁcation,” IEEE Transactions on Geoscience and Remote Sensing , 2020.\n[31] B. Zoph, V . Vasudevan, J. Shlens, and Q. V . Le, “Learning transferable\narchitectures for scalable image recognition,” inProc. IEEE Conf. Comp.\nVis. Patt. Recogn., 2018, pp. 8697–8710.\n[32] N. Wang, Y . Gao, H. Chen, P. Wang, Z. Tian, and C. Shen, “NAS-FCOS:\nFast neural architecture search for object detection,” in Proc. IEEE Conf.\nComp. Vis. Patt. Recogn. , 2020.\n[33] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille, and\nL. Fei-Fei, “Auto-deeplab: Hierarchical neural architecture search for\nsemantic image segmentation,” in Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., 2019, pp. 82–92.\n[34] Z. Zhong, J. Yan, W. Wu, J. Shao, and C.-L. Liu, “Practical block-wise\nneural network architecture generation,” in Proc. IEEE Conf. Comp. Vis.\nPatt. Recogn., 2018, pp. 2423–2432.\n[35] E. Real, A. Aggarwal, Y . Huang, and Q. V . Le, “Regularized evolution\nfor image classiﬁer architecture search,” in Proc. AAAI Conf. Artiﬁcial\nIntell., vol. 33, 2019, pp. 4780–4789.\n[36] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu,\n“Hierarchical representations for efﬁcient architecture search,” in Proc.\nInt. Conf. Learn. Representations , 2018.\n[37] D. Song, C. Xu, X. Jia, Y . Chen, C. Xu, and Y . Wang, “Efﬁcient residual\ndense block search for image super-resolution.” in AAAI, 2020, pp.\n12 007–12 014.\n[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems , 2017, pp. 5998–6008.\n[39] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[40] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efﬁcient image transformers & distillation\nthrough attention,” in International Conference on Machine Learning .\nPMLR, 2021, pp. 10 347–10 357.\n[41] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” arXiv preprint arXiv:2103.14030 , 2021.\n[42] B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. J ´egou,\nand M. Douze, “Levit: a vision transformer in convnet’s clothing for\nfaster inference,” arXiv preprint arXiv:2104.01136 , 2021.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 16\n[43] X. He, Y . Chen, and Z. Lin, “Spatial-spectral transformer for hyper-\nspectral image classiﬁcation,” Remote Sensing , vol. 13, no. 3, p. 498,\n2021.\n[44] Y . Qing, W. Liu, L. Feng, and W. Gao, “Improved transformer net for\nhyperspectral image classiﬁcation,” Remote Sensing, vol. 13, no. 11, p.\n2216, 2021.\n[45] D. Hong, Z. Han, J. Yao, L. Gao, B. Zhang, A. Plaza, and J. Chanus-\nsot, “Spectralformer: Rethinking hyperspectral image classiﬁcation with\ntransformers,” IEEE Transactions on Geoscience and Remote Sensing ,\n2021.\n[46] Z. Dai, H. Liu, Q. Le, and M. Tan, “Coatnet: Marrying convolution and\nattention for all data sizes,” Advances in Neural Information Processing\nSystems, vol. 34, 2021.\n[47] H. Zhang, W. Hu, and X. Wang, “Edgeformer: Improving light-\nweight convnets by learning from vision transformers,” arXiv preprint\narXiv:2203.03952, 2022.\n[48] S. Mehta and M. Rastegari, “Mobilevit: light-weight, general-\npurpose, and mobile-friendly vision transformer,” arXiv preprint\narXiv:2110.02178, 2021.\n[49] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7794–7803.\n[50] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, “Pyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,” arXiv preprint arXiv:2102.12122 ,\n2021.\nXizhe Xue received the B.E. degree from North-\nwestern Polytechnical University, Xi’an, China in\n2018. She is currently pursuing the Ph.D. degree\nin School of Computer Science, Northwestern Poly-\ntechnical University.\nHer research interests include visual object track-\ning, HSI image processing, image segmentation\ntechniques.\nHaokui Zhang received the PhD degree and the\nMS degree in computer application technology from\nShannxi Provincial Key Laboratory of Speech and\nImage Information Processing in 2021 and 2016\nrespectively. He is currently work as a postdoctor\nin Harbin Institute of Technology, Shenzhen.\nHis research interests cover information retrieval,\nimage restoration and hyperspectral image classiﬁ-\ncation.\nBei Fang received her Ph.D. degree in computer sci-\nence and technology from the School of Computer\nScience at Northwestern Polytechnical University,\nXi’an, China, in 2019. She is currently working as\na Postdoctoral Research Associate with Key Lab-\noratory of Modern Teaching Technology, Ministry\nof Education, Shaanxi Normal University, Xi’an,\nChina.\nHer research interests include computer vision,\nHSI image processing and deep learning techniques.\nZongwen Bai received the MS degree from the\nYan’an University, in 2008. He is currently pursuing\nthe Ph.D degree with the School of Computer Sci-\nence, Northwestern Polytechnical University, Xi’an,\nChina. He is an associate professor with the School\nof Physics and Electronic Information, Yan’an Uni-\nversity, His research interests cover computer vision,\nnature language processing and deep learning.\nHis research interests include hyperspectral image\nsuper resolution, image fusion and deep learning.\nYing Li received PhD degree in electrical circuit and\nsystem from the National Key Laboratory of Radar\nSignal Processing, Xidian University in 2002.\nCurrently, she is a Professor with the School\nof Computer Science, Northwestern Polytechnical\nUniversity. Her Interests cover image processing,\ncomputation intelligence and signal processing",
  "topic": "Hyperspectral imaging",
  "concepts": [
    {
      "name": "Hyperspectral imaging",
      "score": 0.8394467830657959
    },
    {
      "name": "Computer science",
      "score": 0.7577176690101624
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6533280611038208
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6461541056632996
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5685903429985046
    },
    {
      "name": "Contextual image classification",
      "score": 0.5249742865562439
    },
    {
      "name": "Deep learning",
      "score": 0.4170774221420288
    },
    {
      "name": "Image resolution",
      "score": 0.41094326972961426
    },
    {
      "name": "Image (mathematics)",
      "score": 0.16833868622779846
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I17145004",
      "name": "Northwestern Polytechnical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I88830068",
      "name": "Shaanxi Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210102412",
      "name": "Yan'an University",
      "country": "CN"
    }
  ]
}