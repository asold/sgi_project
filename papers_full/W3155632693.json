{
    "title": "Through the Looking Glass: Learning to Attribute Synthetic Text Generated by Language Models",
    "url": "https://openalex.org/W3155632693",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3155145532",
            "name": "Shaoor Munir",
            "affiliations": [
                "Lahore University of Management Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A3152692494",
            "name": "Brishna Batool",
            "affiliations": [
                "Lahore University of Management Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2298902016",
            "name": "Zubair Shafiq",
            "affiliations": [
                "University of California, Davis"
            ]
        },
        {
            "id": "https://openalex.org/A2237621063",
            "name": "Padmini Srinivasan",
            "affiliations": [
                "University of Iowa"
            ]
        },
        {
            "id": "https://openalex.org/A2254740840",
            "name": "Fareed Zaffar",
            "affiliations": [
                "Lahore University of Management Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2962819541",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W2991318208",
        "https://openalex.org/W2969958763",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W2963426391",
        "https://openalex.org/W2954835819",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W3016339201",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3035225773",
        "https://openalex.org/W3034287667",
        "https://openalex.org/W2964341837",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W3046764764",
        "https://openalex.org/W2523437799",
        "https://openalex.org/W2945824677",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2106120499",
        "https://openalex.org/W2948975009",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W2886945075",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2153890685",
        "https://openalex.org/W2984284833",
        "https://openalex.org/W2051267297",
        "https://openalex.org/W2951080837",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2963071100",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2092475127",
        "https://openalex.org/W2077414053",
        "https://openalex.org/W2971008823",
        "https://openalex.org/W4288334893",
        "https://openalex.org/W4293398859"
    ],
    "abstract": "Shaoor Munir, Brishna Batool, Zubair Shafiq, Padmini Srinivasan, Fareed Zaffar. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021.",
    "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1811–1822\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n1811\nThrough the Looking Glass:\nLearning to Attribute Synthetic Text Generated by Language Models\nShaoor Munir† Brishna Batool† Zubair Shaﬁq‡ Padmini Srinivasan∗ Fareed Zaffar†\n†Lahore University of Management Sciences, Pakistan\nshaoor.munir@lums.edu.pk\n‡University of California at Davis, USA\nzubair@ucdavis.edu\n∗University of Iowa, USA\npadmini-srinivasan@uiowa.edu\nAbstract\nGiven the potential misuse of recent advances\nin synthetic text generation by language mod-\nels (LMs), it is important to have the capacity\nto attribute authorship of synthetic text. While\nstylometric organic (i.e., human written) au-\nthorship attribution has been quite successful,\nit is unclear whether similar approaches can be\nused to attribute a synthetic text to its source\nLM. We address this question with the key\ninsight that synthetic texts carry subtle distin-\nguishing marks inherited from their source LM\nand that these marks can be leveraged by ma-\nchine learning (ML) algorithms for attribution.\nWe propose and test several ML-based attribu-\ntion methods. Our best attributor built using a\nﬁne-tuned version of XLNet (XLNet-FT) con-\nsistently achieves excellent accuracy scores\n(91% to near perfect 98%) in terms of attribut-\ning the parent pre-trained LM behind a syn-\nthetic text. Our experiments show promising\nresults across a range of experiments where\nthe synthetic text may be generated using pre-\ntrained LMs, ﬁne-tuned LMs, or by varying\ntext generation parameters.\n1 Introduction\nRecent advancements in natural language process-\ning have enabled synthetic text generation that is\noften of comparable quality to the organic text (Ip-\npolito et al., 2020; Radford et al., 2019; Zellers\net al., 2019; Gehrmann et al., 2019). This ca-\npability has the potential to be misused by mali-\ncious actors to launch misinformation, spam, and\nphishing campaigns (Solaiman et al., 2019; Brown\net al., 2020). To prevent potential misuse, prior\nresearch has shown considerable success in build-\ning machine learning (ML) algorithms that detect\n(Zellers et al., 2019) or assist humans in detecting\n(Gehrmann et al., 2019) synthetic text.\nWhile prior research has shown promise in distin-\nguishing between synthetic and organic text, very\nlittle has been done on attributing the authorship of\nthe language model (LM) generating the synthetic\ntext (Pan et al., 2020). It is important to be able to\ntrack the provenance of synthetic text to the source\nLM. This can be useful in identifying perpetrators\nof potential misuse and the unauthorized use of an\nLM (e.g., in case it is stolen through sophisticated\nmodel inversion attacks (Fredrikson et al., 2015) or\noutright security breaches).\nIt is particularly challenging to attribute the au-\nthorship of the synthetic texts because of the variety\nand number of available LMs and conﬁgurations.\nWhile there are only a handful of public pre-trained\nLMs, it is common to further ﬁne-tune them before\nusing them to generate synthetic text (Devlin et al.,\n2019; Sanh et al., 2020). Fine-tuning can signiﬁ-\ncantly impact the characteristics of the generated\ntext (Howard and Ruder, 2018; Cruz and Cheng,\n2019). Moreover, variations in the sampling param-\neters used while generating synthetic text whether\nfrom pre-trained or ﬁne-tuned LMs can further im-\npact text characteristics (Zellers et al., 2019).\nIn this paper, we design and evaluate ML-based\ntechniques for attributing the LM and conﬁguration\nused to generate a synthetic text. We do this in the\ncontext of four problem scenarios, each represent-\ning a variation of a threat posed by an adversary or\nmalicious user. The scenarios vary in terms of what\ninformation the LM attribution system has about\nthe adversary’s strategy for generating fake text.\nMethodologically, our key insight for attributing\nthe LM used by the adversary is that differences\nbetween LM architecture (i.e., layers, parameters),\ntraining (i.e., pre-training and ﬁne-tuning), and gen-\neration techniques (i.e., sampling parameters) will\nleave their subtle mark on the generated synthetic\ntexts. The success of our attributors at identifying\nthe LM and conﬁguration used relies on the pres-\nence of these subtle distinguishing marks and on\nthe ability to exploit them effectively. As our re-\n1812\nsults indicate, this success holds especially in terms\nof attributing pre-trained models used to generate\ntext even under varying conditions.\nIn summary, our key contributions are:\n• We evaluate a variety of attribution techniques\non their ability to attribute the LM and conﬁg-\nuration used to generate text. These include\nattributors making use of stylometric features\nas well as static and dynamic embeddings.\n• We evaluate these attributors on a corpus of\n350,000 synthetic texts that we generated in\na controlled manner using combinations of\nLMs, sampling parameters, and ﬁne-tuning.\n• Our best attributor built on top of a ﬁne-tuned\nversion of XLNet (XLNet-FT) performs ex-\ncellently at identifying pre-trained LM used\nto generate coherent synthetic texts. Accuracy\nranges between 91% and close to perfect 98%.\nThis performance holds for various experi-\nments where we use ﬁne-tuning and different\nsampling parameters. However, the perfor-\nmance is mediocre when attributing the ﬁne-\ntuned LM used to generate the text.\nPaper Organization: The rest of the paper is or-\nganized as follows. Section 2 presents the different\nthreat models based on the adversary’s strategy for\ngenerating synthetic text and assumptions made by\nthe attributor. We then describe our data and attri-\nbution methods in Section 3. Experimental results\nare in Section 4. Section 5 contextualizes our work\nwith respect to prior literature. Section 6 concludes\nthe paper with an outlook on future work.\n2 Threat Model\nThis section describes different threat models that\nwe consider in this paper. The adversary’s goal\nis to generate synthetic text using language mod-\nels (LMs). The attributor’s goal is to attribute the\nsynthetic text to the source LM used by the adver-\nsary. All of the threat models operate under the\nclosed world scenario, where the attributor is as-\nsumed to know the universe of LMs. The threat\nmodels differ based on the adversary’s LM train-\ning (i.e., pre-training or ﬁne-tuning) and sampling\nstrategies.\n2.1 Attributing pre-trained LMs\nIn the ﬁrst scenario, the adversary uses a pre-trained\nLM to generate synthetic text. The attributor trains\na classiﬁer to attribute the synthetic text to the\nsource pre-trained LM. We assume a closed-world\nscenario where both the adversary and attributor\nhave access to the set of off-the-shelf pre-trained\nLMs such as GPT-2.1\nMore formally, the scenario can be described as:\nGiven n pre-trained LMs PM1, PM2, ..., PMn,\nthe goal is to train a n-class attributor to attribute\ntest instances to the correct source pre-trained LM.\nIn this scenario, the adversary generates texts using\nPMk where 1 ≤ k ≤ n and the attributor’s goal\nis to predict label PMk for the generated texts.\n2.2 Attributing ﬁne-tuned LMs to parent\npre-trained LMs\nIn this scenario, the adversary ﬁne-tunes a pre-\ntrained LM to generate synthetic text. The attribu-\ntor trains a classiﬁer to attribute the synthetic text\nto the source pre-trained LM. The main difference\nfrom the ﬁrst scenario is that the attributor is un-\naware of the ﬁne-tuning used by the adversary be-\nfore generating text. Note that the goal of the attrib-\nutor is to detect the source pre-trained LM rather\nthan the ﬁne-tuned LM that is used to generate\nsynthetic text.\nMore formally, the scenario can be described as:\nGiven n pre-trained LMs PM1, PM2, ..., PMn,\nand a LM FMk, generated by ﬁne-tuning PMK\nwhere 1 ≤ k ≤ n, the goal is to train a n-class\nattributor to attribute test instances to the correct\nsource pre-trained LM. In this scenario, the adver-\nsary generates text using ﬁne-tuned LM FMk and\nthe attributor’s goal is to predict label PMk for\ngenerated text.\n2.3 Attributing pre-trained or ﬁne-tuned\nLMs with different sampling parameters\nIn this scenario, the attributor trains a classiﬁer\nto attribute the synthetic text generated by the ad-\nversary using a pre-trained or ﬁne-tuned LM. The\nmain difference from the ﬁrst scenario is that the\nadversary potentially uses different sampling pa-\nrameters for text generation than those used by the\nattributor to train the classiﬁer.\nMore formally, the scenario can be described\nas: Given n pre-trained or ﬁne-tuned LMs\nM1, M2, ..., Mn, the goal is to train a n-class at-\ntributor to attribute test instances to the correct\nsource model. As per this scenario the adversary\n1This assumption holds for the rest of the paper, unless\nstated otherwise.\n1813\ngenerates texts using model Mk, 1 ≤ k ≤ n, with\nsampling parameters Sk that are unknown to the\nattributor, and the attributor’s goal is to predict\nlabel Mk for the generated text.\n2.4 Attributing ﬁne-tuned variants of a\npre-trained LM\nIn this scenario, the adversary ﬁne-tunes a pre-\ntrained LM to generate synthetic text. The attribu-\ntor trains a classiﬁer to attribute the synthetic text to\nthe source ﬁne-tuned LM. The main difference as\ncompared to the second scenario is that the attribu-\ntor is aware of the ﬁne-tuning used by the adversary.\nNote that there are multiple ﬁne-tuned variants of\nthe same parent pre-trained LM.\nMore formally, the scenario can be described\nas: Given n ﬁne-tuned LMs FM1, FM2, ..., FMn,\nthe goal is to train a n-class attributor to attribute\ntest instances to the correct ﬁne-tuned LM. As per\nthis scenario, the adversary generates text using a\nﬁne-tuned LM FMk and the attributor’s goal is to\npredict label FMk for the generated text.\n3 Data & Methods\nIn this section, we present details about (1) the\ntext generating language models (LMs) and their\nconﬁgurations, and (2) about the attributors studied.\nTo address our research goals, we need a dataset\nof synthetic texts generated by various pre-trained\nand ﬁne-tuned LMs under different conﬁgurations.\nPublicly available datasets are unsuitable because\nthere can be high variability in the conditions under\nwhich text was generated 2. It is crucial for us to\nbe able to control the underlying conditions such\nas: the architecture of LM, prompt used for text\ngeneration, sampling parameters, and the data size\nand topics used for ﬁne-tuning. Details about this\ngenerated dataset are also provided in this section.\n3.1 Text Generation: LMs, parameters, and\nconﬁgurations\nWe used four pre-trained LMs: OpenAI GPT (Rad-\nford et al., 2018), OpenAI GPT2 (Radford et al.,\n2019), XLNet (Yang et al., 2019), and BART\n(Lewis et al., 2020). BART and XLNet are both\nbased on the BERT architecture, which makes use\nof the bidirectional context of input text to de-\nvelop a deep understanding of language. XLNet\nimproves on BERT with a form of generalized auto-\nregressive pre-training using permutation model-\n2https://www.kaggle.com/abhishek/gpt2-output-data\ning. It outperforms BERT on several classiﬁcation\ntasks (Yang et al., 2019). BART combines the\nbidirectional encoder used by BERT with an auto-\nregressive decoder used by GPT, which, through\na noising and text reconstruction pre-training task,\nachieves good performance in both language un-\nderstanding and language generation tasks. In\nother words, both BART and XLNet augment their\ntraining strategies to make up for the lack of lan-\nguage generation capabilities in BERT. GPT and\nGPT2 are architecturally identical LMs with GPT2\ntrained on 10 times the data used for training orig-\ninal GPT LM. These use a more traditional gen-\nerative pre-training approach, looking only at the\ncontext coming before a part of the text and not\nafter (Radford et al., 2019). All four pre-trained\nLMs are publicly available.\n3.1.1 Text generation parameters\nThree key parameters when generating texts are: p,\nk, and temperature. The range of values tested are\ngiven in Table 1, with default values emphasized\nin boldface. Note that one chooses to use either p-\nvalue or k-value sampling since they have the same\ngoal - controlling the number of words taken into\nconsideration while sampling text from an LM.\nWith top-k sampling, the LM randomly chooses\none from the top k words. With top-p sampling, it\nchooses from the set of words whose cumulative\nprobability exceeds p. Both Zellers et al. (2019)\nand Holtzman et al. (2020) conclude that synthetic\ntext matches organic text closely when the p-value\nis kept in range [0.9, 1.0]. Higher values lead to\nrepetitions as the length of text increases. Thus,\nwe choose the lower limit of p from the range [0.9,\n1.0].\nFor top-k sampling, we use a range of values\nboth higher and lower than 40, which is used as\nthe default for text generation by Radford et al.\n(2019) in their breakthrough GPT2 paper. Between\na choice of top-p or top-k sampling, we chose top-p\n(p = 0.9) as default due to its lower dependency\non vocabulary size and extensive use in previous\nresearch on GPT2 (Radford et al., 2019; Zellers\net al., 2019; Ippolito et al., 2020).\nTemperature controls the likelihood of low prob-\nability words appearing in the ﬁnal pool of words\nused for random selection (Holtzman et al., 2020).\nHigher temperatures produce text containing highly\nunusual words that are normally not favored bytop-\nk or top-p sampling. At the other end, Holtzman\net al. (2020) note that temperatures below 1 reduce\n1814\n(a) Scenario 1: Attributing pre-trained LM\n (b) Scenario 2: Attributing ﬁne-tuned LMs to the parent\npre-trained LMs\n(c) Scenario 3: Attributing LM with different sampling\nparameters\n(d) Scenario 4: Attributing ﬁne-tuned variants of a pre-trained\nLM\nFigure 1: Illustration of different threat models studied in this work\nword diversity but at the cost of increasing word\nrepetition. To avoid this we set temperature as 1\nin experiments where evaluation of its effect on\nsynthetic text is not of concern.\n# Parameter Values\n1 Architecture GPT, GPT2, XLNet, BART\n2 Text Length Short, Medium, Long\n3 Fine-tuning\nTopic\nr/changemyview, r/technology,\nr/relationships, r/conspiracy\n4 p-value 0.9, 0.92, 0.94, 0.96, 0.98, 1.0\n5 Temperature 0.1, 0.5, 1, 1.5, 2\n6 k-value 1, 20, 40, 80, 160\nTable 1: The parameters explored (defaults in bold)\n3.1.2 Data for ﬁne-tuning\nFor scenarios where we need synthetic text gen-\nerated using ﬁne-tuned LMs, we limit text gen-\neration to the GPT2 LM. GPT2 has been shown\nto have state of the art performance in language\ngeneration tasks (Radford et al., 2019; Klein and\nNabi, 2019). Data from four Reddit communi-\nties was used for ﬁne-tuning LMs: r/relationships,\nr/technology, r/changemyview, and r/conspiracy.\nThese subreddits were chosen based on qualitative\ndifferences between their content. r/technology\ncontains technical jargon, while r/relationships fo-\ncuses on personal pronouns and adopts a critical\napproach towards writing. r/changemyview has\nconfrontational content with members attempting\nto challenge and disprove each other’s views, while\nr/conspiracy focuses on hyperbolic statements. In\nessence, each subreddit is considered a different\ntopic area. Table 2 shows the number of posts and\ncomments scraped from each subreddit.\n3.1.3 Dataset details\nWe generate text of three different lengths: short\n(up to 40 words), medium (between 40 and 100\nwords), and long (above 100 words). In experi-\nments where length is not the focus, we usemedium\nas the default. Each synthetic text is generated us-\ning a randomly selected subreddit submission as a\nprompt. We start by sampling words equal to the\nlength of the prompt from the LM. We trim the\ngenerated text to follow standard sentence structure\nsuch as start capitalization and end punctuation,\nafter which text is sorted into one of three length\n1815\nFigure 2: Attributor training on XLNet embeddings. The dashed lines are part of the ﬁne-tuned pathway.\ncategories.\nWe generated 10,000 synthetic texts for each tar-\nget class in our experiments. For example, when\nevaluating the performance of attributors against\nﬁne-tuned LMs, we generated 10,000 samples for\neach of four GPT2 LMs ﬁne-tuned on one of the\nfour Reddit topics mentioned previously. In total,\n35 distinct sets of synthetic documents, each with\n10,000 examples, were used for a total of 350,000\nunique synthetic documents 3. We build training\nand test datasets that are balanced in classes for\neach scenario because while there is growing evi-\ndence that synthetic text is appearing in the wild,\nthere is little to no information about the relative\nimpact of the source LMs. Thus, any split beyond\nan even split across classes has little justiﬁcation.\nSubreddit Posts Comments Total\nr/changemyview 136,775 321,527 458,302\nr/relationships 200,047 167,219 367,266\nr/technology 174,431 143,199 317,630\nr/conspiracy 99,302 161,993 261,295\nTable 2: Data scraped per each subreddit.\n3.2 Attributors\nWe test six attributors in their ability to identify\nthe source LMs. The ﬁrst attributor is a decision\ntree classiﬁer with Writeprints (Abbasi and Chen,\n2008) feature set, second is a CNN with GloVe\n(Pennington et al., 2014) embeddings as the feature\nset. The next four attributors are softmax classiﬁers,\nwith the ﬁrst two built on top of pre-trained XLNet\nand GPT2, and the other two on top of XLNet\nand GPT2 ﬁne-tuned on training data used in the\ncorresponding scenario.\n3.2.1 Decision tree with Writeprints features\nThe Writeprints features have been used exten-\nsively and successfully for authorship attribution.\n3We will make this dataset available for research upon\npublication of our paper\n(Abbasi and Chen, 2008; Mahmood et al., 2020)\nWhen combined with SVMs and decision trees\nthese have shown good performance in attribu-\ntion tasks (Abbasi and Chen, 2008; Pearl and\nSteyvers, 2012). Due to ease of interpretability\nof features, we implement a decision tree classiﬁer\nwith Writeprints features to test our intuition that\nstylistic, rather than topical, differences contribute\ntowards the attribution of synthetic text.\n3.2.2 CNN with GloVe embeddings\nPre-trained GloVe embeddings have been shown to\noutperform word frequency and count-based em-\nbeddings for sentence and sequence classiﬁcation\ntasks (Pennington et al., 2014; Le-Hong and Le,\n2018). Also, the use of GloVe with CNNs has\nshown good results in classiﬁcation tasks like news-\ngroup identiﬁcation (Gupta et al., 2018).\n3.2.3 Attributors from LM embeddings\nEmbeddings generated through LMs like BERT,\nXLNet, and GPT2 have been shown to capture lan-\nguage semantics and context much better than static\nembeddings generated through GloVe and other\nword count or frequency-based embeddings gener-\nators (Sun et al., 2020; Howard and Ruder, 2018).\nBecause of their extensive pre-training, these LMs\ncan capture long-term dependencies and incorpo-\nrate contextual and hierarchical relations between\nwords better than pre-computed static embeddings.\nLMs such as XLNet make use of a special[CLS]\ntoken to get pooled output representing a complete\ntext sequence. We use the ﬁnal network layer em-\nbeddings of this token for attribution. Speciﬁcally,\nwe train a softmax output layer that takes as input\nXLNet’s[CLS] token embeddings and generates\nprobabilities for each decision class in the experi-\nment setup. For GPT2 we use a parallel strategy\nwith pooled output from the complete ﬁnal layer of\nthe model for a particular input text. Again this out-\nput is connected to a softmax output layer which,\nsimilar to our strategy with XLNet, is trained to\n1816\ngenerate class predictions based on the input em-\nbeddings.\nIn addition to using the pre-trained versions of\nXLNet and GPT2, we also evaluate attributors built\nfrom ﬁne-tuned versions of these LMs. Note that\nhere ﬁne-tuning is on training data used to train all\nattributors in the corresponding experiment. Fig-\nure 2 illustrates these strategies with XLNet as an\nexample. The sequence with dashed lines repre-\nsents the ﬁne-tuned versions.\n4 Results\nWe present attribution accuracy results in the same\norder of scenarios described earlier in Section 2.4\n4.1 Attributing pre-trained LMs\nTable 3 presents the accuracy results for short (up\nto 40 words), medium (between 40 and 100 words),\nand long (more than 100 words) synthetic text gen-\nerated using pre-trained GPT, GPT2, XLNet, and\nBART language models (LMs). Decision tree and\nthe two XLNet versions achieve accuracy between\n82 and, near perfect 98%, across the three types of\ntexts. In comparison, CNN and GPT2 attributors\nlag behind.\nWhile both XLNet attributors score higher than\ndecision tree, XLNet-FT has the best performance\nwhich when compared with the next best XLNet-\nPT ranges from 3% to 7%. Note that apart from\nthe pre-trained GPT2 attributor, all show marked\nimprovement in accuracy scores with an increase in\ntext length. Similar results showing direct propor-\ntionality of classiﬁer performance with text length\nwere also observed by Ippolito et al. (2020) in ex-\nperiments detecting synthetic text.\nPrior work has shown that uni-directional LMs\nare more suited for language generation due to gen-\nerative pre-training (Lewis et al., 2020) where the\nLM learns to predict the next word based on the\nprevious context. Bidirectional LMs like BERT\nand XLNet excel at classiﬁcation as they make use\nof masked modeling and next sentence prediction\ntasks to improve understanding of necessary lan-\nguage attributes (Devlin et al., 2019; Yang et al.,\n2019). Our results are consistent in that XLNet\nperformance is better than GPT2.\nInterestingly, the decision tree with Writeprints\noutperforms GPT2 based attributors in all three text\n4We measured performance using F1 score as well. How-\never, since there were no remarkable differences, we only\nreport accuracy results to be concise.\nlengths. Our investigation into speciﬁc Writeprints\nfeatures emphasized by the decision tree (see ap-\npendix A.1) reveals a greater emphasis on stylistic\nfeatures. This gives further credence to our intu-\nition that variations between texts generated by dif-\nferent LMs are more stylistic than topical in nature.\nOur results suggest that GPT2 based attributors are\nnot adept at capturing such stylistic differences.\n4.2 Attributing ﬁne-tuned LMs to the parent\npre-trained LMs\nOur goal in this scenario is to attribute the synthetic\ntext generated using a ﬁne-tuned variant (using an\nunknown dataset) of a pre-trained LM. Note that\nthe attributor is unaware of ﬁne-tuning. We limit\nﬁne-tuned text generation in this experiment to just\nGPT2 for reasons described in Section 3.\nThe ﬁrst row in Table 4 reports the accuracy re-\nsults. We note that XLNet-FT again performs the\nbest with XLNet-PT in the second place. CNN\nhas the weakest results. Interestingly, Writeprints\ncontinues to do fairly well – once again empha-\nsizing the role of style in identifying source LM.\nComparing these results with Table 3 (for medium\nlength texts), we see all accuracies drop slightly as\nexpected when the adversary chooses to ﬁne-tune\nthe LM that is unknown to the attributor.\nWe run a second variation of the same experi-\nment – one where the attributor has partial knowl-\nedge of the adversary’s strategy. Speciﬁcally, the\nfact that the adversary is using a ﬁne-tuned LM\nto generate text is known but the dataset used for\nﬁne-tuning remains unknown. In response, we\npick some dataset (here r/relationships) to add ﬁne-\ntuned LM generated texts to our training data. Note\nthat the adversary uses r/changemyview. The sec-\nond row in Table 4 reports similar results as the ﬁrst\nrow. Thus, it seems that this additional knowledge\ndoes not help improve attribution accuracy.\nIn sum, the accuracy for XLNet-FT across all\nexperiments thus far is above 90%. This indicates\nthat even when the adversary ﬁne-tunes the LM for\ntext generation, the parent pre-trained LM is still\nidentiﬁable. This result conﬁrms our intuition that\nas ﬁne-tuning is known to leave the majority of\nlayers unchanged, the text generated retains char-\nacteristics of the parent pre-trained LM, making\naccurate attribution possible.\n1817\nSynthetic Text Length DT\n(Writeprints)\nCNN\n(GloVe)\nXLNet-PT XLNet-FT GPT2-\nPT\nGPT2-\nFT\nShort (Upto 40 Words) 82 68 85 91 72 74\nMedium (40 to 100 words) 86 73 90 96 71 72\nLong (Above 100 words) 93 83 95 98 72 72\nTable 3: Accuracy percentages for attributing source pre-trained LMs. Datasets contain synthetic texts of different\nlengths generated using pre-trained BERT, GPT, GPT2 and XLNet.\nTraining data\nincludes\nTest data includes DT\n(Writeprints)\nCNN\n(GloVe)\nXLNet-\nPT\nXLNet-\nFT\nGPT2-\nPT\nGPT2-\nFT\nGPT2 (PT) GPT2 (FT-\nr/changemyview)\n78 64 86 93 70 71\nGPT2 (FT-\nr/relationships)\nGPT2 (FT-\nr/changemyview)\n77 67 86 91 70 70\nTable 4: Accuracy percentages in attributing source pre-trained LM when adversary generates synthetic text using\na ﬁne-tuned LM. In addition to GPT2 variants mentioned in columns 1 and 2, training and testing data also includes\nclasses representing XLNet, BART, and GPT.\n4.3 Attributing LM with different sampling\nparameters\nHere we consider the scenario where both the at-\ntributor and the adversary use the same LM but\nthey differ in parameter choices when generating\ntexts. We run this experiment assuming the ad-\nversary uses GPT2 ﬁne-tuned on r/changemyview.\nThe attributor is aware of this but not the sampling\nparameters. Selecting parameter values that are\nquite different from each other we see from Ta-\nble 5 that there is virtually no performance drop for\nXLNet-FT. That is, our best performing attributor\nis resilient to these differences. Temperature sam-\npling shows weaker results all around. This is not\na concern as discussed later in this section.\nWe next explore the parameter differences angle\nfurther to get a sense of what would happen if the\nadversary chose a parameter value other than the\nones explored in Table 5. The different values\nfor k, p, and temperature are as listed in Table 1.\nWe remind the reader that one uses either top −\nk or top − p sampling to control the number of\nwords under consideration during text generation.\nWe use top − p sampling as the default strategy.\nWhen varying k or p, the temperature is ﬁxed at\nthe default value. When varying temperature, p is\nkept at the default value.\nThe results in Table 6 show that it is challenging\nto tell apart synthetic texts generated by different\nvalues of k and p. Given their strong similarities\nwe expect to see results as in Table 5 when the\nadversary picks other parameter values. With tem-\nperature variations we get accuracy above 80%,\nindicating marked differences between texts gen-\nerated at different temperatures. However, taking\na closer look at the text reveals a serious problem:\ntemperature > 1 produces erratic and confusing\ntext. This problem becomes more acute as the tem-\nperature approaches its upper limit. This is consis-\ntent with the observation by Holtzman et al. (2020)\nshowing that temperatures above 1 produce inco-\nherent and confusing text. This reduces viability in\na setting where the synthetic text is to be used as a\nsuitable replacement for organic text.\nWe conclude from Tables 5 and 6 that our attrib-\nutors should be resilient even when the adversary\nchooses parameter values for text generation be-\nyond the ones explicitly tested here.\n4.4 Attributing ﬁne-tuned variants of a\npre-trained LM\nFinally, we explore the scenario where the ad-\nversary is using different ﬁne-tuned LMs with\nthe same parent pre-trained LM. The attributor\nis aware of this ﬁne-tuning and is attempting to\ntell apart these ﬁne-tuned LMs. Table 7 presents\nthe accuracy results when synthetic text is gener-\nated by ﬁne-tuning GPT2 on 4 different subred-\ndits (r/changemyview, r/technology, r/relationships,\nr/conspiracy). XLNet-FT again achieved the best\naccuracy, however, this time it is less than 60%.\nCuriously, CNN which was the least successful in\nearlier experiments performed almost identically\nto XLNet-FT. GPT2 performed just slightly better\nthan a random attributor (1/4, i.e., 25%). Overall,\nvariations between texts generated by different ﬁne-\ntuned variants of the same pre-trained LM are not\n1818\nTraining data\nincludes\nTest data\nincludes\nDT\n(Writeprints)\nCNN\n(GloVe)\nXLNet-\nPT\nXLNet-\nFT\nGPT2-\nPT\nGPT2-\nFT\nGPT2 (k=20) GPT2 (k=160) 76 71 88 95 71 71\nGPT2 (p=0.9) GPT2 (p=1.0) 80 70 88 96 70 70\nGPT2 (t=0.1) GPT2 (t=1.0) 60 67 70 77 71 72\nTable 5: Accuracy percentages for attributing source LM when adversary generates synthetic text using different\nsampling parameters. In addition to GPT2 variants mentioned in columns 1 and 2, training and testing data also\nincludes classes representing XLNet, BART, and GPT.\nAttributor k-value p-value Temperature\nDT (Writeprints) 53 43 82\nCNN (GloVe) 47 26 79\nXLNet-PT 42 22 81\nXLNet-FT 45 25 86\nGPT2-PT 28 17 44\nGPT2-FT 28 18 44\nTable 6: Accuracy percentages for identifying texts\ngenerated by GPT2 LM ﬁne-tuned on r/changemyview\nwith varying sampling parameters. Parameter values\ntested are as reported in Table 1.\nAttributor Accuracy\nDT (Writeprints) 52\nCNN (GloVe) 56\nXLNet-PT 53\nXLNet-FT 57\nGPT2-PT 29\nGPT2-FT 29\nTable 7: Accuracy percentage in attributing ﬁne-tuned\nGPT2 LMs. Dataset contains texts generated by four\nGPT2 LMs ﬁne-tuned on each subreddit in Table 2.\npronounced enough to be leveraged by the attri-\nbution techniques we consider. Our preliminary\nanalysis shows that there is some correlation be-\ntween the attributor’s mistakes and the vocabulary\nsimilarity of the corresponding subreddits. How-\never, further research is needed to probe the causes\nof this lackluster performance and devise ways to\nimprove the attribution of text produced by ﬁne-\ntuned LMs.\n5 Related Work\nWe contextualize our work with respect to prior\nliterature on detection and attribution of organic\nand synthetic text.\n5.1 Synthetic text detection\nThere has been a lot of recent interest in develop-\ning ML approaches to distinguish between organic\nand synthetic text. GLTR (Giant Language Model\nTest Room) leveraged the statistical tendency of\nLMs to produce words with higher probability of\noccurrence to help users differentiate between syn-\nthetic and organic text (Gehrmann et al., 2019).\nGrover used a purpose-built LM to train a classi-\nﬁer for synthetic and organic text (Zellers et al.,\n2019). Bakhtin et al. (2019) proposed energy based\nmodels for differentiating between synthetic and\norganic text. Our work takes this line of work a\nstep further by trying to attribute synthetic text to\nthe source LM.\n5.2 Synthetic text attribution\nPan et al. (2020) proposed a dynamic embedding\nbased approach to attribute synthetic text gener-\nated by a pre-trained LM as part of their broader\ninvestigation of sensitive information exposed by\nLMs. We signiﬁcantly build on this work from\nboth the methodological and application perspec-\ntives. Differently from this work, we use stylomet-\nric as well as static and dynamic embeddings. We\nalso consider more realistic threat models where\nthe synthetic text is generated by either pre-trained\nor ﬁne-tuned LMs and using different sampling\nparameters.\n5.3 Organic text attribution\nThere is a rich body of literature on authorship at-\ntribution of organic text using stylometric features.\nWe discuss a few classic papers here. Mosteller\nand Wallace (1964) used word frequency analy-\nsis for authorship attribution. Abbasi and Chen\n(2008) proposed a ML-based approach for author-\nship attribution using an exhaustive stylometric fea-\nture set called Writeprints. While there is impres-\nsive progress in stylometric organic text attribution\n(e.g., Narayanan et al., 2012; Ruder et al., 2016),\nthese approaches do not work as effectively for syn-\nthetic text attribution. As our evaluation showed,\nWriteprints were signiﬁcantly outperformed by\nother approaches for synthetic text attribution. This\nis because LMs are trained on large text corpora\n1819\nfrom different authors thus there are no clear-cut\nstylometric differences in synthetic text generated\nby different LMs.\n5.4 Synthetic image attribution\nRecent advances in Generative Adversarial Net-\nworks (GANs) have led to impressive results in\nsynthetic image generation (Bao et al., 2017; Taig-\nman et al., 2017; Ma et al., 2017). For example,\nChen et al. (2020) proposed image models simi-\nlar to pre-trained LMs to learn an unsupervised\nrepresentation of images for various downstream\ntasks. Most related to our work, Yu et al. (2019)\nproposed an ML approach to attribute synthetic\nimages generated by GANs with different archi-\ntectures and parameters. At the most basic level,\nthe problem of synthetic image attribution differs\nfrom synthetic text attribution because images are\nsmooth and local where words in a text document\nmay be correlated even if they are far apart (Sharir\net al., 2020). For instance, Yu et al. (2019) showed\nthat their ML classiﬁer could use only part of the\nsynthetic image for attribution. In contrast, we ob-\nserved a large drop in accuracy when we make use\nof only part of input synthetic text.\n6 Conclusion\nIn this paper, we presented an ML approach to\nattribute authorship of synthetic text to its source\nLM. Our results showed that an attributor based on\nﬁne-tuned XLNet embeddings outperformed other\napproaches based on stylometric features as well\nas static and dynamic embeddings. Our results also\nshowed there is signiﬁcant room for improvement\nin distinguishing between synthetic text generated\nby different ﬁne-tuned variants of an LM. Further\nresearch is also needed for effective attribution of\nsynthetic text generated by more diverse ﬁne-tuned\nLMs in both closed-world and-open world settings.\nFinally, future research on synthetic text attribution\nshould also consider more sophisticated LMs (e.g.,\nGPT-3 with 175 billion parameters (Brown et al.,\n2020) and Google’s trillion parameter LM (Fedus\net al., 2021)) when they are publicly released.\nReferences\nAhmed Abbasi and Hsinchun Chen. 2008. Writeprints:\nA stylometric approach to identity-level identiﬁca-\ntion and similarity detection in cyberspace. ACM\nTrans. Inf. Syst., 26(2).\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian\nDeng, Marc’Aurelio Ranzato, and Arthur Szlam.\n2019. Real or Fake? Learning to Discriminate\nMachine from Human Generated Text. CoRR,\nabs/1906.03351.\nJ. Bao, D. Chen, F. Wen, H. Li, and G. Hua.\n2017. Cvae-gan: Fine-grained image generation\nthrough asymmetric training. In 2017 IEEE Inter-\nnational Conference on Computer Vision (ICCV) ,\npages 2764–2773.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu,\nHeewoo Jun, David Luan, and Ilya Sutskever. 2020.\nGenerative pretraining from pixels. In Proceedings\nof the 37th International Conference on Machine\nLearning, volume 119 of Proceedings of Machine\nLearning Research, pages 1691–1703. PMLR.\nJan Christian Blaise Cruz and Charibeth Cheng.\n2019. Evaluating language model ﬁnetuning tech-\nniques for low-resource languages. arXiv preprint\narXiv:1907.00409.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch Transformers: Scaling to Trillion Parameter\nModels with Simple and Efﬁcient Sparsity. arXiv\n2101.03961.\nMatt Fredrikson, Somesh Jha, and Thomas Ristenpart.\n2015. Model inversion attacks that exploit conﬁ-\ndence information and basic countermeasures. In\nProceedings of the 22nd ACM SIGSAC Conference\non Computer and Communications Security , CCS\n’15, page 1322–1333, New York, NY , USA. Asso-\nciation for Computing Machinery.\nSebastian Gehrmann, Hendrik Strobelt, and Alexander\nRush. 2019. GLTR: Statistical Detection and Visu-\nalization of Generated Text. In 57th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL): Demo Track.\n1820\nVarun Gupta, Akhilesh Kumar, and Aashish Bhardwaj.\n2018. Newsgroup classiﬁcation using cnn and glove\nembeddings. International Journal of Applied Re-\nsearch on Information Technology and Computing ,\n9:135.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations (ICLR).\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nDaphne Ippolito, Daniel Duckworth, Chris Callison-\nBurch, and Douglas Eck. 2020. Automatic detec-\ntion of generated text is easiest when humans are\nfooled. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1808–1822, Online. Association for Computa-\ntional Linguistics.\nTassilo Klein and Moin Nabi. 2019. Learning to An-\nswer by Learning to Ask: Getting the Best of GPT-2\nand BERT Worlds. arXiv, 1911.02365.\nPhuong Le-Hong and Anh-Cuong Le. 2018. A compar-\native study of neural network models for sentence\nclassiﬁcation. In 5th NAFOSTED Conference on In-\nformation and Computer Science (NICS).\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nLiqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne\nTuytelaars, and Luc Van Gool. 2017. Pose guided\nperson image generation. In Advances in Neural\nInformation Processing Systems , volume 30, pages\n406–416. Curran Associates, Inc.\nAsad Mahmood, Zubair Shaﬁq, and Padmini Srini-\nvasan. 2020. A Girl Has A Name: Detecting Au-\nthorship Obfuscation. In 58th Annual Meeting of the\nAssociation for Computational Linguistics (ACL).\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations (ICLR).\nFrederick Mosteller and David L. Wallace. 1964. In-\nference and Disputed Authorship: The Federal-\nist. Addison-Wesley series in behavioral science.\nAddison-Wesley.\nArvind Narayanan, Hristo Paskov, Neil Zhenqiang\nGong, John Bethencourt, Emil Stefanov, Eui\nChul Richard Shin, and Dawn Song. 2012. On the\nfeasibility of internet-scale author identiﬁcation. In\n2012 IEEE Symposium on Security and Privacy.\nX. Pan, M. Zhang, S. Ji, and M. Yang. 2020. Pri-\nvacy risks of general-purpose language models. In\n2020 IEEE Symposium on Security and Privacy\n(SP), pages 1314–1331.\nLisa Pearl and Mark Steyvers. 2012. Detecting author-\nship deception: A supervised machine learning ap-\nproach using author writeprints. Literary and Lin-\nguistic Computing, 27:183–196.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI Blog.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nSebastian Ruder, Parsa Ghaffari, and John G. Breslin.\n2016. Character-level and multi-channel convolu-\ntional neural networks for large-scale authorship at-\ntribution. arXiv 1609.06686.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nOr Sharir, Barak Peleg, and Yoav Shoham. 2020. The\nCost of Training NLP Models: A Concise Overview.\narXiv 2004.08900.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Radford,\nGretchen Krueger, Jong Wook Kim, Sarah Kreps,\nMiles McCain, Alex Newhouse, Jason Blazakis,\nKris McGufﬁe, and Jasmine Wang. 2019. Release\nstrategies and the social impacts of language mod-\nels.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2020. How to Fine-Tune BERT for Text Classiﬁca-\ntion? arXiv 1905.05583.\nYaniv Taigman, Adam Polyak, and Lior Wolf. 2017.\nUnsupervised cross-domain image generation. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\n1821\nInformation Processing Systems , volume 32, pages\n5753–5763. Curran Associates, Inc.\nN. Yu, L. Davis, and M. Fritz. 2019. Attributing fake\nimages to gans: Learning and analyzing gan ﬁnger-\nprints. In 2019 IEEE/CVF International Conference\non Computer Vision (ICCV), pages 7555–7565.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in Neural Information Process-\ning Systems, volume 32, pages 9054–9065. Curran\nAssociates, Inc.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In IEEE International\nConference on Computer Vision (ICCV).\nA Appendix\nA.1 Analysis of importance given by Decision\nTree to Writeprints\nSynthetic texts generated by pre-trained LMs are\ndistinguishable to a high degree. This holds even\nwhen the adversary decides to use a ﬁne-tuned LM\nor varies text generation parameters like p-value,\nk-value, or temperature. Thus, these texts carry\nexploitable model ﬁngerprints.\nMost challenging is the ability to tell apart texts\ngenerated by different ﬁne-tuned versions of the\nsame pre-trained model. XLNet ﬁne-tuned on\nthe training data yields excellent results; except\nfor attributing ﬁne-tuned models accuracies are al-\nmost entirely above 90%. Interestingly, decision\ntrees fare quite well offering the advantage of inter-\npretability of decisions. Decision Tree based classi-\nﬁer focusing only on stylistic differences achieves\nan accuracy of higher than 80% in all three conﬁgu-\nrations. Investigating the importance given by clas-\nsiﬁer to different Writeprints show stylistic features\nlike spaces, percentage of characters, and special\ncharacters being given the highest importance.\nFigure 3 shows a comparison of importance\ngiven by a decision tree based attributor to features\nbefore and after eliminating whitespace as a feature.\nRunning the experiment again after eliminating the\nhighest rated feature (frequency of white space)\nresults in minimal drop in performance (within\na range of 1%- 2%) and shows continued focus\non more stylistic language features as key indi-\ncators of differences between these texts. This\nconﬁrms our intuition that different pre-trained ver-\nsions of language models have different writing\nstyles which are discernible through text classi-\nﬁcation techniques. Moreover, our experiments\nshowed that ﬁne-tuned LMs retained characteris-\ntics from their parent pre-trained LM, allowing an\nattributor trained entirely on pre-trained text to suc-\ncessfully attribute ﬁne-tuned LM text with above\n90% accuracy even in a worst case scenario.\nComparing importance maps from a decision\ntree attributor trained on Writeprints from pre-\ntrained and ﬁne-tuned GPT2 LMs shows interest-\ning results. From ﬁgure 4, it is apparent that top\ntwo most important features are common among\npre-trained and ﬁne-tuned variants, with a num-\nber of other similarities in features given relatively\nless importance. It shows that there are certain\nstylistic characteristics that are passed down from\na pre-trained LM to its ﬁne-tuned variant.\nA.2 Details of pre-trained language models\nused\nIn our experiments we have made use of four pub-\nlicly available pre-trained language models: XL-\nNet, BART, GPT, GPT2. Details about those are\ngiven in Table 8. Comparing sizes, XLNet is\ntrained on largest dataset with over 142 GB of doc-\numents. Although no explicit size is mentioned for\nGPT2, it is said to be trained on 10 times more data\nthan GPT.\n1822\n(a) Before removing whitespace\n (b) After removing whitespace\nFigure 3: Comparison of feature importance with and without whitespace\n(a) Importance given by attributor trained on pre-trained text\ndata to different Writeprints features\n(b) Importance given by attributor trained on ﬁne-tuned text\ndata to different Writeprints features\nFigure 4: Comparison of feature importance for pre-trained and ﬁne-tuned variants of GPT2 LM\nLanguage\nModel\nDataset Words Size Number of\nDocuments\nOpenAI\nGPT\nBooksCorpus (Zhu et al., 2015) ∼985M Not\nAvailable\nNot Available\nOpenAI\nGPT2\nWebText (Radford et al., 2018) Not Available 40GB ∼8 million\ndocuments\nBART WikiText-103 (Merity et al., 2017) ∼103M Words 181MB 28,475 articles\nXLNet BookCorpus + English Wikipedia + CommonCrawl +\nGiga5 + ClueWeb 2012-B\n∼32.8B\nsubword pieces\n142GB Not Available\nTable 8: Breakdown of pre-trained LMs used"
}