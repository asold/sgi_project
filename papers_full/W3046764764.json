{
    "title": "Privacy Risks of General-Purpose Language Models",
    "url": "https://openalex.org/W3046764764",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2149336434",
            "name": "Xudong Pan",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2115061930",
            "name": "Mi Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2168495572",
            "name": "Shouling Ji",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2038824294",
            "name": "Min Yang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2149336434",
            "name": "Xudong Pan",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2115061930",
            "name": "Mi Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2168495572",
            "name": "Shouling Ji",
            "affiliations": [
                "Alibaba Group (China)",
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2038824294",
            "name": "Min Yang",
            "affiliations": [
                "Fudan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963844355",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2156909104",
        "https://openalex.org/W2461943168",
        "https://openalex.org/W6763701032",
        "https://openalex.org/W2985580374",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6607467106",
        "https://openalex.org/W6749557519",
        "https://openalex.org/W6727862155",
        "https://openalex.org/W1941659294",
        "https://openalex.org/W6638667902",
        "https://openalex.org/W2586154897",
        "https://openalex.org/W2002458933",
        "https://openalex.org/W2544902556",
        "https://openalex.org/W2608764892",
        "https://openalex.org/W6750884629",
        "https://openalex.org/W2897865027",
        "https://openalex.org/W1978394996",
        "https://openalex.org/W6765861705",
        "https://openalex.org/W2951287344",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W2892940874",
        "https://openalex.org/W2120911939",
        "https://openalex.org/W6640425456",
        "https://openalex.org/W1838635991",
        "https://openalex.org/W2946363484",
        "https://openalex.org/W6728551298",
        "https://openalex.org/W6766481131",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W2250966211",
        "https://openalex.org/W4205228770",
        "https://openalex.org/W6628547770",
        "https://openalex.org/W2051267297",
        "https://openalex.org/W2435473771",
        "https://openalex.org/W2897830718",
        "https://openalex.org/W2884943453",
        "https://openalex.org/W6756040250",
        "https://openalex.org/W6752346538",
        "https://openalex.org/W6760759230",
        "https://openalex.org/W1714669175",
        "https://openalex.org/W6648914341",
        "https://openalex.org/W6796535575",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2962835266",
        "https://openalex.org/W2163922914",
        "https://openalex.org/W6629028937",
        "https://openalex.org/W2785357069",
        "https://openalex.org/W6676935882",
        "https://openalex.org/W6721933647",
        "https://openalex.org/W2962910668",
        "https://openalex.org/W6764838729",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2906125480",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2963989815",
        "https://openalex.org/W6758096801",
        "https://openalex.org/W2963843518",
        "https://openalex.org/W2162996853",
        "https://openalex.org/W2473418344",
        "https://openalex.org/W6682691769",
        "https://openalex.org/W6634441602",
        "https://openalex.org/W6728757088",
        "https://openalex.org/W6748805329",
        "https://openalex.org/W2963456518",
        "https://openalex.org/W6679775712",
        "https://openalex.org/W6759455113",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W6687736904",
        "https://openalex.org/W2795435272",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W1945616565",
        "https://openalex.org/W2197886894",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2530395818",
        "https://openalex.org/W2963207607",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4320013936",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3170514878",
        "https://openalex.org/W2541884796",
        "https://openalex.org/W2949547296",
        "https://openalex.org/W2950018712",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W2964318098",
        "https://openalex.org/W1577269164",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2799694080",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3048684575",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W1994616650",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W182831726",
        "https://openalex.org/W3104224589",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2964268978",
        "https://openalex.org/W1473189865",
        "https://openalex.org/W2963080984",
        "https://openalex.org/W2027595342",
        "https://openalex.org/W2483215953",
        "https://openalex.org/W2963378725",
        "https://openalex.org/W4318619660",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2112507308",
        "https://openalex.org/W2970408908",
        "https://openalex.org/W1956475457",
        "https://openalex.org/W2131744502",
        "https://openalex.org/W1486649854",
        "https://openalex.org/W2788816110",
        "https://openalex.org/W2963165390",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2906869444",
        "https://openalex.org/W2963796896"
    ],
    "abstract": "Recently, a new paradigm of building general-purpose language models (e.g., Google's Bert and OpenAI's GPT-2) in Natural Language Processing (NLP) for text feature extraction, a standard procedure in NLP systems that converts texts to vectors (i.e., embeddings) for downstream modeling, has arisen and starts to find its application in various downstream NLP tasks and real world systems (e.g., Google's search engine [6]). To obtain general-purpose text embeddings, these language models have highly complicated architectures with millions of learnable parameters and are usually pretrained on billions of sentences before being utilized. As is widely recognized, such a practice indeed improves the state-of-the-art performance of many downstream NLP tasks. However, the improved utility is not for free. We find the text embeddings from general-purpose language models would capture much sensitive information from the plain text. Once being accessed by the adversary, the embeddings can be reverse-engineered to disclose sensitive information of the victims for further harassment. Although such a privacy risk can impose a real threat to the future leverage of these promising NLP tools, there are neither published attacks nor systematic evaluations by far for the mainstream industry-level language models. To bridge this gap, we present the first systematic study on the privacy risks of 8 state-of-the-art language models with 4 diverse case studies. By constructing 2 novel attack classes, our study demonstrates the aforementioned privacy risks do exist and can impose practical threats to the application of general-purpose language models on sensitive data covering identity, genome, healthcare and location. For example, we show the adversary with nearly no prior knowledge can achieve about 75% accuracy when inferring the precise disease site from Bert embeddings of patients' medical descriptions. As possible countermeasures, we propose 4 different defenses (via rounding, differential privacy, adversarial training and subspace projection) to obfuscate the unprotected embeddings for mitigation purpose. With extensive evaluations, we also provide a preliminary analysis on the utility-privacy trade-off brought by each defense, which we hope may foster future mitigation researches.",
    "full_text": "Privacy Risks of General-Purpose Language Models\nXudong Pan∗, Mi Zhang∗, Shouling Ji†‡ and Min Y ang∗\n∗Fudan University, †Zhejiang University, ‡Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies\nEmails: xdpan18@fudan.edu.cn, mi zhang@fudan.edu.cn, sji@zju.edu.cn, m yang@fudan.edu.cn\nAbstract—Recently, a new paradigm of building general-\npurpose language models (e.g., Google’s Bert and OpenAI’s\nGPT-2) in Natural Language Processing (NLP) for text feature\nextraction, a standard procedure in NLP systems that converts\ntexts to vectors (i.e., embeddings) for downstream modeling, has\narisen and starts to ﬁnd its application in various downstream\nNLP tasks and real world systems (e.g., Google’s search engine\n[6]). To obtain general-purpose text embeddings, these language\nmodels have highly complicated architectures with millions of\nlearnable parameters and are usually pretrained on billions of\nsentences before being utilized. As is widely recognized, such\na practice indeed improves the state-of-the-art performance of\nmany downstream NLP tasks.\nHowever, the improved utility is not for free. We ﬁnd the\ntext embeddings from general-purpose language models would\ncapture much sensitive information from the plain text. Once\nbeing accessed by the adversary, the embeddings can be reverse-\nengineered to disclose sensitive information of the victims for\nfurther harassment. Although such a privacy risk can impose a\nreal threat to the future leverage of these promising NLP tools,\nthere are neither published attacks nor systematic evaluations by\nfar for the mainstream industry-level language models.\nTo bridge this gap, we present the ﬁrst systematic study on the\nprivacy risks of8 state-of-the-art language models with4 diverse\ncase studies. By constructing 2 novel attack classes, our study\ndemonstrates the aforementioned privacy risks do exist and can\nimpose practical threats to the application of general-purpose\nlanguage models on sensitive data covering identity, genome,\nhealthcare and location. For example, we show the adversary\nwith nearly no prior knowledge can achieve about75% accuracy\nwhen inferring the precise disease site from Bert embeddings of\npatients’ medical descriptions. As possible countermeasures, we\npropose 4 different defenses (via rounding, differential privacy,\nadversarial training and subspace projection) to obfuscate the\nunprotected embeddings for mitigation purpose. With extensive\nevaluations, we also provide a preliminary analysis on the utility-\nprivacy trade-off brought by each defense, which we hope may\nfoster future mitigation researches.\nI. I NTRODUCTION\nWith the advances of deep learning techniques in Natural\nLanguage Processing (NLP), the last year has witnessed many\nbreakthroughs in building general-purpose language models\nby industry leaders like Google, OpenAI and Facebook [16],\n[17], [41], [44], [54], [55], [67], [76], which have been\nwidely used in various downstream NLP tasks such as text\nclassiﬁcation and question answering [15] and start to ﬁnd\nits application in real-world systems such as Google’s search\nengine [6], which is said to represent “the biggest leap forward\nin the past ﬁve years, and one of the biggest leaps forward in\nthe history of Search” [6].\nUnlike traditional statistical models or shallow neural net-\nwork models, general-purpose language modelstypically refer\nto the family of Transformer-based pretrained giant language\nmodels including Google’s Bert and OpenAI’s GPT -2, which\nare composed of layers of Transformer blocks [72] with\nmillions of learnable parameters, and are usually pretrained on\nbillions of sentences before being released. According to the\nofﬁcial tutorials [2], users can apply these pretrained models\nas text feature extractors for encoding sentences into dense\nvectors, or called sentence embeddings, which can be further\nused for various downstream tasks (e.g., text classiﬁcation).\nWith the release of Bert, Google AI envisions the future of\ngeneral-purpose language models as, “anyone in the world can\ntrain their own state-of-the-art question answering system (or\na variety of other models) in about 30 minutes on a single\nCloud TPU, or in a few hours using a single GPU” [4].\nDespite the bright envision, for the ﬁrst time, we observe\nthese general-purpose language models tend to capture much\nsensitive information in the sentence embeddings, which leaves\nthe adversary a window for privacy breach. For example, in\na typical use case of these language models in intelligent\nhealthcare, a third-party organization issues a cooperation with\na hospital for developing a patient guide system, which auto-\nmatically assigns the patients to a proper department based on\nthe symptom descriptions. Due to the generality of Google’s\nBert [17], the organization only needs to request the hospital to\nprovide the embeddings of the patients’ symptom descriptions\nas the essential information for building a high-utility system.\nDue to the lack of understanding of the privacy properties\nof the general-purpose language models, the hospital may\nexpect sharing the vector-form features would be much less\nprivate than sharing the plain text, especially when they are\ntold the encoding rule itself is based on highly complicated\nneural networks that are near to black-boxes. In fact, we do\nobserve with experiments in Appendix F that, even with a\nstandard decoder module in NLP , it is difﬁcult to recover\nany useful information from the embeddings. However, on\neight state-of-the-art language models including Bert and GPT -\n2, we devise a lightweight yet effective attack pipeline and\nstrikingly ﬁnd thatgiven the unprotected sentence embeddings,\neven an adversary with nearly zero domain knowledge can\ninfer domain-related sensitive information in the unknown\nplain text with high accuracy. In the above medical example,\nour observation strongly implies that the honest-but-curious\nservice provider as the adversary can easily infer the identity,\ngender, birth date, disease type or even the precise disease\nsite regarding a particular victim, only if the target piece of\ninformation appears in his/her original description.\nOur Work.In this paper, we provide the ﬁrst systematic study\n1314\n2020 IEEE Symposium on Security and Privacy\n© 2020, Xudong Pan. Under license to IEEE.\nDOI 10.1109/SP40000.2020.00095\non the potential privacy risks of general-purpose language\nmodels. Speciﬁcally, we want to answer the main research\nquestion: is it possible for the adversary to infer user’s private\ninformation in the unknown plain text, when the adversary\nonly has access to his/her submitted embeddings? If the\nanswer is afﬁrmative, then the future applications of state-\nof-the-art NLP tools in compelling learning paradigms like\ncollaborative or federated learning [36], [37], [46] can be\nlargely threatened and restricted, especially on privacy-critical\ndomains including healthcare, genomics and ﬁnance. Besides\nthe novelty in our major research object, our research question\nalso has its own specialness compared with most existing\nworks including membership inference attacks [63], property\ninference attacks [23] and model inversion attacks [21], in\nterms of the information source and the attack objective.\nA more detailed comparison between our study and related\nattacks can be found in Section II.\nAlthough previous works in computer vision have shown\nthe possibility of reconstructing original images from their\npretrained embeddings via autoencoders [18] or generative\nmodels [61], no similar attacks were reported in NLP before.\nFrom our perspective, the discreteness of tokens and the in-\nvisibility of the vocabulary are two major technical challenges\nthat prevent the success of reconstruction attacks on text\nembeddings. On one hand, the discreteness of tokens makes\nthe search over the space of all possible sentences highly\ninefﬁcient, mainly because the learning objective is no longer\ndifferentiable as in the visual cases and therefore gradient-\nbased methods can hardly work [79]. On the other hand, as\nlanguage models are accessed as black boxes, the adversary\nhas no knowledge of the ground-truth vocabulary, without\nwhich the adversary cannot convert the recovered word index\nsequences into plain text [38]. Even if the adversary may\nprepare one’s own vocabulary, it can be either too small to\ncontain some sensitive words in the unknown plain text or so\nlarge that bring high computational complexity.\nTo address these aforementioned challenges, we propose to\nreconstruct sensitive information from text embeddings via\ninference. Taking inspirations from the observation that the\nprivacy-related information in text usually appears in small\nsegments, or is related with the occurrence of certain key-\nwords [62], we construct two different attack classes, namely\npattern reconstruction attacks and keyword inference attacks,\nto demonstrate how sensitive information can be extracted\nfrom the text embeddings. In pattern reconstruction attacks,\nthe raw text has ﬁxed patterns (e.g., genome sequences) and\nthe adversary attempts to recover a speciﬁc segment of the\noriginal sequences that contains sensitive information (e.g.,\ndisease-related gene expressions). In keyword inference attack,\nthe adversary wants to probe whether the unknown plain text\n(e.g., medical descriptions) contains certain sensitive keyword\n(e.g., disease site). Focusing on a small segment, the adversary\nonly needs to infer from a limited number of possibilities\nfor reconstruction purposes, which alleviates the optimization\ndifﬁculty caused by the discreteness of tokens. Meanwhile, the\nadversary has no need to know the whole vocabulary if the\nadversary only cares about the word he/she is interested in.\nExtensive experiments on 8 state-of-the-art general-purpose\nlanguage models with 4 (identity-, genome-, medical-,\nlocation-related) case studies showed, the adversary can pre-\ncisely infer various levels of sensitive information of a target\nuser from his/her leaked embeddings. For pattern reconstruc-\ntion, our attack achieves optimal and average accuracy respec-\ntively of98.2% and 62.4% when inferring the exact nucleotide\ntype at any speciﬁed positions from the GPT -2 embeddings\nof 20-length genome sequences, without any auxiliary infor-\nmation. For keyword inference, our attack achieves average\naccuracy of 99.8% and 74.8% respectively, when inferring the\noccurrence of 10 body-related keywords from the Bert em-\nbeddings of medical descriptions with and without a shadow\ncorpus. These results highly demonstrate that the aforemen-\ntioned privacy risks do exist and can impose real threats\nto the application of general-purpose language models on\nsensitive data. Noticeably, all our attacks only need to access\nthe language model as a cloud service (i.e. ML-as-a-service)\nand can be conducted with one PC device. With additional\nablation studies, we further discuss some architecture-related\nand data-related factors which may inﬂuence the privacy risk\nlevel of language models. Furthermore, we also propose and\nevaluate four possible countermeasures against the observed\nthreats, via quantization, differential privacy [20], adversarial\ntraining [57] and subspace projection [14]. We hope our\npreliminary mitigation study will shed light on future defense\nresearches and contribute to the design of secure general-\npurpose language models.\nIn summary, we make the following contributions:\n• We discover the potential privacy risks in general-purpose\nlanguage models by showing, a nearly-zero-knowledge ad-\nversary with access to the text embeddings can disclose\nmuch sensitive information in the unknown text.\n• We design a general attack pipeline for exploiting user pri-\nvacy in text embeddings and implement two practical attacks\nwith advanced deep learning techniques to demonstrate the\nprivacy risks.\n• We present the ﬁrst systematic evaluations on 8 state-of-\nthe-art general-purpose language models with4 diverse case\nstudies to demonstrate the hidden privacy risks, with an in-\ndepth analysis on the factors that inﬂuence the privacy.\n• We also provide preliminary studies on four possible coun-\ntermeasures and their utility-privacy trade-off, which we\nhope may foster future defense studies.\nII. R\nELA TED WORKS\nPrivacy Attacks against ML.Model inversion attack was ﬁrst\nproposed by Fredrikson et al. on statistical models [22] and\nlater generalized to deep learning systems [21]. In terms of the\nattack objective, Fredrikson et al’s attack on image classiﬁers\naims at recovering the prototypical image that represents a\nspeciﬁc class, while our attacks aim at recovering partially\nor fully the plain text behind the embedding. In terms of the\ninformation source, model inversion attack mainly relies on\nthe parameters of the model itself, while for our attacks, the\n1315\ninformation source is the sentence embedding produced from\nthe general-purpose language models.\nMeanwhile, Fredrikson et al. [21], [22] also discussed the\nmodel inversion attack in the sense that the attack inverts\nsensitive information about the input from the model’s output.\nTo the best of our knowledge, their original attack was mainly\nimplemented for the decision tree model and is not directly\napplicable to deep learning models. Later, some very recent\nworks have proposed ﬁner-grained attacks which attempt to\nrecover the exact training images or texts from the predictions\n[58], [77] or the gradients [47], [82] in an unknown mini-\nbatch during the training phase. However, two of them that\ntarget on recovering text from gradients [47], [82] utilize\nthe explicit representation of word composition in bag-of-\nwords and cannot be applied to our adversarial setting which\nreconstructs texts from the dense sentence embeddings from\ngeneral-purpose language models.\nAs a complement to model inversion attack, Shokri et al.\ndevised the ﬁrst membership inference attack against machine\nlearning models [63], which aroused wide research interests\n[50], [59], [66], [78] in the past few years. In terms of the\nattack objective, membership inference attempts to disclose the\nis-in relation between the sample and the real private training\nset. In terms of the information source, the membership\ninference attack relies on the probability vector associated\nwith the input sample. Different from membership inference,\nanother branch of works called property inference aims at\ninfering whether the training set has certain global property,\nwhich was ﬁrst studied by [10] on shallow models and later\nextended by [23] to deep models.\nAside from privacy attacks on the datasets, some other\nthreats against the model privacy have also been studied, e.g.,\nby demonstrating the possibility of stealing model parameters\n[70], architectures [19], and hyper-parameters [73]. In a wider\npicture of adversarial machine learning, there still remains\nmany open problems including adversarial example [27], data\npoisoning [13], Byzantine workers [48] and fairness [29],\nwhich are calling for future research efforts on building more\nrobust and reliable machine learning systems.\nPrivacy Attacks using ML. Besides, there are also plenty\nof prior works using ML approaches to evaluating user pri-\nvacy risks regarding, e.g., his/her biomedical and geological\nproﬁles. On biomedical privacy, for example, Humbert et al.\n[30], [31] leveraged graphical models to infer the genome of an\nindividual from parental relationships and expert knowledge,\nwhich was recently extended to other types of biomedical data\nby [12]. On location privacy, for example, Shokri et al. [64]\nused Markov chain modeling to reconstruct the actual traces of\nusers from obfuscated location information, while some recent\nworks exploit side channels from social media like hashtags\nfor location inference using clustering [8] or random forests\n[80].\nIII. P\nRELIMINARIES\nA. Sentence Embedding\nGiven a vocabulary V that consists of |V| tokens, we call\na sequence x := (w1,...,w n) is a sentence of length n if\neach token (or word) wi is in the vocabulary V. Following\nthe nomenclature of representation learning [11], we call a\nmapping f from sentences to a real vector space Rd as a\nfeature extractor. For the sentence x, the vector z := f(x) is\ncalled its embedding.\nPrior to the proposal of general-purpose language models,\nword embedding and sentence embedding as two traditional\nNLP tasks have been widely studied, for which several ma-\nture algorithms exist. For word embedding, algorithms like\nword2vec [49] encode the word to its vector representation that\ncan noticeably preserve the relative semantics between words,\ne.g., the difference of the embeddings of the wordsqueen and\nwoman was observed to be almost identical to that ofking and\nman [49]. For sentence embeddings, word-frequency-based\nalgorithms like TF-IDF [60] directly counts word statistics\nof a sentence and thus the produced sentence embeddings\nare explicit in word composition, which are not suitable for\nprivacy-critical scenarios [46]. Other learning-based sentence\nembedding methods like doc2vec [42] borrow the idea of\nword2vec and encode sentences to vectors that preserve the\nrelative semantics between the sentence and its composite\nwords in the training corpus. As a result, the produced sentence\nembeddings from doc2vec are usually corpus-speciﬁc and are\nmainly used for sentence clustering or paraphrase detection on\na given corpus [40], [42].\nRecently, the boom of general-purpose language models\nhas largely reformed how we understand and use embed-\ndings in the following aspects. On one hand, the boundary\nbetween word embedding and sentence embedding are no\nlonger clear due to contextualized word embeddings [52], a\nfundamental concept behind these general-purpose language\nmodels. Intuitively, contextualized word embeddings suggest\nthe embedding of the same word can vary according to the\nsentence where it occurs. For example, the contextualized\nembedding of the word apple should be different in “I like\napple” and “I like Apple macbooks” . Consequently, most\ngeneral-purpose language models list sentence embedding as\none major use case instead of word embedding [2], [74]. On\nthe other hand, sentence embeddings from pretrained general-\npurpose language models have better generality and can be\ndirectly used as input to train downstream learning models.\nFor instance, with a simple linear layer for output, embeddings\nfrom a pretrained Bert model can achieve state-of-the-art\nperformance on eleven NLP tasks [17].\nB. General-Purpose Language Models for Sentence Embed-\nding\nRoughly speaking, existing general-purpose language mod-\nels are mainly variants of stacked recurrent Transformers,\nwhich consist of millions of learnable parameters. Before\ncoming into use, general-purpose language models ﬁrst need\n1316\nto be pretrained on extremely large corpus such as the English\nWikipedia. Typical pretraining tasks include masked language\nmodeling and next sentence prediction [17].\nFig. 1. General-purpose language models for sentence embedding and the\npotential privacy risks. The red directed line illustrates the discovered privacy\nrisks: the adversary could reconstruct some sensitive information in the\nunknown plain texts even when he/she only sees the embeddings from the\ngeneral-purpose language model.\nTo obtain the embedding of a given sentencex, the follow-\ning procedures are required [17]: (1) tokenization according\nto a prepared vocabulary; (2) token embedding (i.e., the token\nindex is mapped to a corresponding vector with the aid\nof a learnable look-up table); (3) propagation through the\nTransformers along two dimensions. At the last layer, the\nsentence is transformed to an n-length sequence of vectors in\nR\nd (i.e., hidden states); and (4) ﬁnally, a pooling operation is\nperformed on the hidden states to get the sentence embedding.\nThe pooling operation for general-purpose language models is\nto take the last hidden state at the ﬁnal layer as the embedding\nof sentence x, because most general-purpose language models\nby default add a special token (i.e, ⟨CLS⟩, which intuitively\nmeans to classify) to the end of the input sentence during\nthe pretraining phase. As a result, to use the last hidden state\nas the sentence embedding usually brings better utility [17],\n[44]. Fig. 1 provides a schematic view on the aforementioned\nprocedures. Although intuitions on the described workﬂow\nsuggests that a certain level of context information should\nbe preserved in the last hidden state, there is little known to\nour community that, to what granularity the original sentence\nis preserved in the encoding, whether and how the resided\nsensitive information can be decoded by potential attacks.\nC. General-Purpose Language Models in the Wild\nT ABLE I\nBASIC INFORMA TION OF MAINSTREAM PRETRAINED LANGUAGE MODELS .\n(* IMPLIES THE ST A TISTICS IS ESTIMA TED ACCORDING TO THE ORIGINAL\nPA P E R.)\nName Proposed by Dimension d Pretraining Data Size\nBert [17] Google 1024 13 GB\nTransformer-XL [16] Google 1024 517 MB*\nXLNet [76] Google 768 76 GB\nGPT [54] OpenAI 768 4 GB*\nGPT -2 [55] OpenAI 768 40 GB\nRoBERTa [44] Facebook 768 160 GB\nXLM [41] Facebook 1024 10 GB*\nErnie 2.0 [67] (abbr. ERNIE) Baidu 768 33 GB*\nAs is discussed, training a general-purpose language model\nfrom scratch can be highly expensive. As an alternative,\nmost of the state-of-the-art models have a pretrained version\npublished online for free access. In this paper, we study 8\nmainstream language models developed by industry leaders\nincluding Google, OpenAI, Facebook and Baidu. Table I lists\nthe basic information of these target models.\nIV . G\nENERAL ATT ACK PIPELINE\nAlthough the state-of-the-art language models provide a\ndirect and effective way for obtaining general-purpose sen-\ntence embeddings for various downstream tasks, we ﬁnd their\nimproved utility is accompanied with hidden privacy risks. By\nconstructing two novel attack classes, we show an adversary is\nable to reverse-engineer various levels of sensitive information\nin the unknown plain text from the embeddings. In this section,\nwe ﬁrst present some general statements of our attacks.\nA. Attack Deﬁnition\nGenerally speaking, in both attacks the adversary wants\nto infer some sensitive information of the sentence from the\naccessed embeddings. Formally, we formulate the attack model\nas A : z → s , where z is the embedding of a target sentencex\nand s denotes certain type of sensitive information that can be\nobtained from the plaintext with a publicly-known algorithm\nP : x → s. For example, from the treatment description\n“CT scan of blood vessel of head with contrast”, we can tell\nthe patient probably has sickness at his/her head. In practice,\nthe sensitive information s can be of various types, from a\nsmall segment that contains sensitive information (i.e.,P is an\noperation that takes out a speciﬁed part of the whole sequence)\nto a predicate on the plain text x. For example, in the above\nhead case, P maps any sentence x to {0,1}: if the sentence\nx has word head, then P(x)=1 ; otherwise P(x)=0 . This\nnotion will be used in formulating our attack pipeline.\nB. Threat Model\nIn general, we focus on the following threat model.\n• Assumption 0. The adversary has access to a set of em-\nbeddings of plain text, which may contain the sensitive\ninformation the adversary is interested in.\n• Assumption 1. For simplicity only, we assume the adver-\nsary knows which type of pretrained language models the\nembeddings come from. Later in Section VIII, we show this\nassumption can be easily removed with a proposed learning-\nbased ﬁngerprinting algorithm.\n• Assumption 2. The adversary has access to the pretrained\nlanguage model as an oracle, which takes a sentence as input\nand outputs the corresponding embedding.\nFor each attack, we also impose different assumptions on the\nadversary’s prior knowledge of the unknown plain text, which\nare detailed in the corresponding parts.\nC. Attack Pipeline\nOur general attack pipeline is divided into four stages.\nAt the ﬁrst stage, the adversary prepares an external corpus\n1317\nDext := {xi}N\ni=1 and uses the algorithm P to extract the\n{P(xi)}N\ni=1 as labels. It is worth to notice, as the external\ncorpus is basically generated with algorithms or crawled from\nopen domains like Y elp restaurant reviews, the extracted labels\nusually contain no truly sensitive information. At the second\nstage, the adversary queries the pretrained language model\nwith each sentence x\ni ∈D ext and receives their embeddings\n{zi}N\ni=1. At the third stage, the adversary combines the embed-\ndings with the extracted labels to train an attack modelA.A t\nthe ﬁnal stage, the adversary uses the well-trained attack model\nto infer sensitive information s from the target embedding z.\nFig. 2 provides an overview of our attack pipeline. In the next\nparts, we provide a general introduction of each stage in the\npipeline.\nFig. 2. General attack pipeline.\nStage 1: Prepare External Corpus. The preparation of the\ntraining set is accomplished in the ﬁrst two stages. First, as the\nattack model infers sensitive information in the unknown plain\ntext, a proper external corpus D\next := {xi}N\ni=1 is therefore\nessential to play the role of a probing set for successful\nattacks. Based on different knowledge levels on the plain\ntext, we suggest the adversary can create the external corpus\n(1) by generating algorithms or (2) from public corpora in\nopen domains. The details are provided in the corresponding\nsections. After the external corpus is prepared, we apply the\nalgorithm P on each x\ni ∈D ext to obtain the label P(xi),\nwhich concludes the ﬁrst stage.\nStage 2: Query the Language Model. The second stage\nfor training set preparation is to convert the sentences in\nD\next to the corresponding embeddings. Ideally, it is quite\nstraightforward as the adversary only needs to query the\nlanguage model with each sentence. In practice, according\nto the knowledge of which model is used, the adversary can\ndeploy the corresponding pretrained model on his/her devices\nfor local query. The adversary may also save some budget by\nutilizing online language model services [74]. Without loss\nof generality, our evaluations are conducted in the former\nsetting. More details can be found in Appendix G. At the\nend of this stage, we have the training set D\ntrain of the form\n{(zi,P(xi))}N\ni=1, where zi is the embedding corresponding to\nthe sentence xi.\nStage 3: Train the Attack Model. With the training set\nDtrain at hand, the adversary can now train an attack model\ng for inference usage. In general, the model is designed as\na classiﬁer, which takes the embedding zi as its input and\noutputs a probabilistic vector g(zi) over all possible values of\nthe sensitive information. To train the attack model with the\nprepared dataset, the adversary needs to solve the following\noptimization problem with gradient-based algorithms such\nas Stochastic Gradient Descent (SGD [56]) or Adam [39],\nming 1\nN\n∑N\ni=1 ℓ(g(zi),P(xi)), where ℓis a loss function that\nmeasures the difference between the predicted probabilities\nand the ground-truth label. Throughout this paper,ℓis always\nimplemented as the cross-entropy loss.\nAs a ﬁnal remark, depending on the knowledge level of\nthe adversary, the architecture ofg varies in different settings.\nFor example, knowledgeable attackers will ﬁnd off-the-shelf\nclassiﬁers such as logistic regression or linear SVM work\nsurprisingly well, while attackers with no prior knowledge can\nleverage advanced transfer learning techniques for successful\nattacks.\nStage 4: Inference. After the training phase, given the\ntarget embedding z, the adversary infers the sensitive in-\nformation based on the following equation s := A(z)=\narg max\ni∈{1,2,...,K}[g(z)]i, where [g(z)]i is the value of g(z)\nat its i-th dimension and K denotes the total number of\npossible values for s. In other words, the adversary considers\nthe value with the highest probability as the most possible\nvalue of the sensitive information in the unknown sentencex.\nV. P\nA TTERN RECONSTRUCTION ATT ACK\nIn this section, we focus on the situation when the adversary\nhas knowledge of the generating rule of the unknown plain\ntext, which usually happens when the format of the plain\ntext is common sense (e.g., identity code). We provide this\nsection as a starting point to understand how much sensitive\ninformation is encoded in the embeddings from the general-\npurposed language models.\nA. Attack Deﬁnition\nIntuitively speaking, the pattern reconstruction attack aims\nat recovering a speciﬁc segment of the plain text which has\na ﬁxed format. The target segment may contain sensitive\ninformation such as birth date, gender or even gene expression.\nFormally, we construct the pattern reconstruction attack under\nthe following assumption.\n• Assumption 3a. The format of the plain text is ﬁxed and\nthe adversary knows the generating rules of the plain text.\nFollowing the general statements in Section IV -A, we\nformally deﬁne the routine P for extracting the sensitive\ninformation s from the sentence x := ( w1,...,w n) as\nPpattern :( w1,...,w n) → (wb,...,w e), where b and e are\nthe starting and the termination index of the target segment.\nAs P is assumed to be publicly known, it is also known by\nthe adversary. Therefore, the pattern reconstruction attack w.r.t.\nP\npattern can be deﬁned as Apattern : z → (wb,...,w e).\nTo be concrete, we provide the following two illustrative\nexamples.\nCase Study - Citizen ID (abbr.Citizen). Structured informa-\ntion such as identity code or zip code commonly appears in our\ndaily conversations, and these conversations are proved to be\nuseful for training chatbots with the aid of general-purpose\nlanguage models [55]. However, we ﬁnd if the messages\nare not properly cleaned, the adversary, given the sentence\nembeddings, is capable to recover the structured information\n1318\nwith high accuracy and thus conduct further harassment. For\nexample, in many countries, citizen ID is a typical sensitive\ninformation for its owner. Once being leaked to the adversary,\nthe identity code can be used to access the victim’s other\nsensitive information or allow the adversary to impersonate the\nvictim to participate in illegal activities [3]. To demonstrate,\nwe consider the case of citizen ID in China, which consists\nof the 18 characters (from the vocabulary {0,..., 9}), i.e. 6\nfor the residence code (3000 possibilities), 8 for the birth date\n(more than 100×12×30 possibilities) and 4 for extra code\n(10\n4 possibilities). Consider the adversary wants to recover\nthe exact birth date of the victim via the leaked embedding of\nhis/her citizen ID, we deﬁne the mapping P as\nP\ncitizen : |residence|birthday|extra|→| birthday| (1)\nCase Study - Genome Sequence (abbr.Genome). Roughly,\na genome is a sequence of nucleotide which has four different\ntypes, namely A, C, G, T, as its vocabulary. With increasingly\nmany NLP techniques being applied in computational genet-\nics and pharmacogenomics [43], [45], [81], general-purpose\nlanguage models are also used in genomics-related tasks.\nTo demonstrate this point, we implement eight benchmark\nsystems by incorporating different general-purpose language\nmodels for splice site prediction [45], a classical binary\nclassiﬁcation problem in computational genetics. Basically,\nour systems exhibit a high utility performance. For example,\nthe splice site prediction system with Google’s Bert achieves\nover 75% classiﬁcation accuracy. We report the utility of our\nsystems in Fig. Fig. 8(a) of the Appendix and more details in\nAppendix A.\nHowever, genetic data is highly sensitive in a personalized\nway – even the nucleotide type at a speciﬁc position i in a\ngenome sequence can be related with certain type of genetic\ndecease or characterizes racial information [65] – and thus the\nadversary is very likely to be interested in recovering the exact\nnucleotide at a target position. From the disclosed nucleotide,\nthe adversary can further know the gender, race or other\nprivacy-critical information of the victim. For demonstration,\nwe deﬁne the mapping P as P\ngenome,i :( w1,w2,...,w n) →\nwi. In other words, the nucleotide at positioni is assumed to\nbe sensitive.\nB. Methodology\nTo realize the attackApattern, we present the implementation\ndetails on preparation of the external corpus and the architec-\nture of the attack model. In the following parts, we denote the\nset of all possible values for sequence s as V(s).\n1) Generate External Corpus: Knowing the generating rule\nof the target plain text, the adversary can prepare the external\ncorpus via generating algorithms. A basic generating algorithm\ngenerates batches of training samples by randomly sampling\nfrom the possible values in V(x), i.e. the set of all possible\nsentences.\n2) Attack Model’s Architecture: Naively, the attack model\ng can be designed as a fully-connected neural network that\nhas input dimension d and output dimension |V(w\nb ...w e)|,\ni.e. the number of possible values of the sensitive segment.\nHowever, |V(wb ...w e)|can be very large. For example, in the\nCitizen case, the number of possible birth dates is near40,000.\nAs a result, the free parameters in the attack model will be of\nlarge number, which further makes both the batch generation\nand model training difﬁcult. To tackle this, we follow the\ndivide-and-conquer idea to decompose the attack A\npattern into\nsmall sub-attacks, according to the adversary’s knowledge of\nthe format. Again on Citizen, we can decompose the attack\nmodel g\nbirth into three sub-attacks, namely year attack gyear,\nmonth attackgmonth and day attackgday. Each sub-attack model\ncan be independently implemented with fully-connected neural\nnetworks of much smaller size and the total parameter number\nis largely truncated from O(|V(w\nb)|× ... ×| V(we)|) to\nO(|V(wb)|+... +|V(we)|). Besides, the generating algorithm\ncan also be decomposed to subroutines for each attack model,\nso that the training of each sub-module can be conducted in\nparallel.\nC. Experimental Setup\nBenchmark Systems.\n• Citizen: We randomly generate 1000 citizen IDs according\nto the generating rule in Eq. 1 as the ground-truth plain text.\nThen we query the target language model with these citizen\nIDs to get the corresponding embeddings as the victims.\n• Genome: We implement eight genome classiﬁcation systems\nfor splice site prediction based on a public genome dataset\ncalled HS3D (Homo Sapiens Splice Sites Dataset [53]). All\nthe genome sequences are of length 20. We assume the\nembeddings of genome sequences in the test set, which\ncontains respectively 1000 samples with or without the\nsplice site, are leaked to the adversary.\nAttack Implementation.\n• Citizen: Following the discussion in Section V -B, we im-\nplement the year, month and date sub-attacks as three-layer\nMLPs which respectively contain400, 25, 200 hidden units\nwith sigmoid activation. The training batch size is set as128\nfor each sub-attack.\n• Genome: In practice, we augment the training pair(z,wi)by\nconcatenating the embeddingz of the generated sample with\nthe positional embedding pi for the target position i.W e\ndiscuss the motivation in Appendix B. Technically, we use\nthe sinusoidal positional embedding as in [72], which has the\nsame dimension as z. Corresponding to this modiﬁcation,\nwe implement one single attack model for inferring the\nnucleotide type at any speciﬁed position. Different from the\nCitizen case, this modiﬁcation will not increase the param-\neter number as the class number is still4. The attack model\nis implemented as a four-layer MLP which takes inputz⊕p\ni\nof dimension 2d and has 400,100 hidden units with sigmoid\nactivation and intermediate batch normalization layers [32]\nfor faster convergence. For training, we generate mini-\nbatches of size 128 that consist of tuples (z,pi,wi), where\nthe positional embedding i is randomly sampled from the\ninterval of possible positions (i.e.,1,..., 20). For inference,\nthe attacker inputs the victim’s embedding and the target\n1319\nT ABLE II\nACCURACY OF SEGMENT RECONSTRUCTION A TT ACKS ON CITIZEN .\nY ear Month Date Whole\nTop-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5\nBert 0.661 0 .926 0 .616 0 .950 0 .539 0 .885 0 .219 0 .384\nTransformer-XL 0.725 0 .927 0.802 0 .992 0 .839 0 .992 0 .488 0 .624\nXLNet 0.506 0 .748 0 .484 0 .877 0 .457 0 .797 0 .112 0 .186\nGPT 0.735 0 .978 0.601 0 .987 0 .630 0 .960 0 .281 0 .434\nGPT -2 0.626 0 .882 0 .664 0 .968 0 .624 0 .927 0 .259 0 .384\nRoBERTa 0.454 0 .774 0 .441 0 .889 0 .307 0 .703 0 .061 0 .108\nXLM 0.572 0 .847 0 .509 0 .911 0 .642 0 .908 0 .187 0 .263\nErnie 2.0 0.584 0 .892 0 .559 0 .924 0 .465 0 .843 0 .152 0 .257\nBaseline 0.01 0 .05 0 .083 0 .417 0 .033 0 .167 0 .0001 0 .0005\nposition and the model outputs the predicted nucleotide type.\nMore implementation details can be found in Appendix B.\nD. Results & Analysis\nTable II reports the Top-1 and Top-5 accuracy of the\nsub-attacks and of inferring the whole birth date with the\nensemble attack after100,000 iterations of training, where the\nbaseline denotes the performance of a random guesser. Fig. 3\nreports the average and per-nucleotide Top-1 accuracy of the\nattacks on Genome after100,000 iterations of training, where\nwe report the proportion of the most frequently appeared\nnucleotide type as the baseline.\n1) Effectiveness & Efﬁciency: From Table II & Fig. 3,\nconsidering the performance of baseline, we can see that our\nattacks are effective in recovering sensitive segments from\ntheir embeddings. For example, when given Transformer-XL\n(abbr. XL in later sections) embeddings of citizen ID, our\nattack is able to recover the exact month and date of the vic-\ntim’s birthday with over80% Top-1 accuracy and recover the\nwhole birth date with over 62% Top-5 accuracy. When given\nGPT embeddings of genome sequences, our attack achieves\nnear-100% accuracy of inferring the victim’s nucleotide type\nat both ends and over62% accuracy on average. These results\nhighly demonstrate the effectiveness of our attacks and thus\nthe common existence of privacy risks in the popular industry-\nlevel language models.\nMoreover, our attack is also efﬁcient in terms of the\nthroughput, which are reported in Table VI of the Appendix. In\nboth cases the attack can learn from over 100 batches in one\nsecond. To achieve the reported accuracy, the training takes\nless than 30 minutes on a medium-end PC. More details of\nour experimental environment is in Appendix H.\nFig. 3. Accuracy of segment reconstruction attacks on Genome per nucleotide\nposition. The average accuracy is reported in the legend.\n2) Comparison among Language Models: First, we notice\nFacebook’s RoBERTa shows stronger robustness than other\nlanguage models in both cases. By investigating its design,\nwe ﬁnd RoBERTa is a re-implementation of Google’s Bert but\nuses a different byte-level tokenization scheme (i.e., tokenize\nsentences in the unit of bytes instead of characters or words)\n[44]. As RoBERTa shows about50% lower privacy risks than\nBert when facing the same attacks, we conjecture the reason is\nthat the byte-level tokenization scheme may make the embed-\ndings less explicit in character-level sensitive information and\nthus more robust against our attacks. Similar phenomenon is\nalso observed in the next section. However, RoBERTa suffers\na clear utility degradation as a trade-off between utility and\nprivacy. As we can see from Fig. 6(c), the system with Bert\nachieves an about 33% higher utility performance than that\nwith RoBERTa on Genome. Also, we notice OpenAI’s GPT\nand GPT -2, which share the same architecture but are pre-\ntrained on 4GB and 40GB texts, show similar security proper-\nties against our attacks and comparable utility performance.\nCombined with other results, no immediate relatedness is\nobserved between the pretraining data size and the privacy\nrisk level.\n3) Other Interesting Findings: From Fig. 3, we can see a\nmajority of the accuracy curves present a valley-like shape,\nwhich implies that most language models capture more infor-\nmation of the tokens around the ends than those in the middle,\nwhich is probably due to the information at ends usually\npropagates along the longest path in the recurrent architecture.\nIn other words, the sensitive information which lies at the\nsentence boundary is more prone to malicious disclosure.\nVI. K\nEYWORD INFERENCE ATT ACK\nIn this section, we study a more general scenario where\nthe plain text can be arbitrary natural sentences and the\nknowledge-level of the adversary is much lower. As a result,\nsuccessful attacks in this case can impose stronger threats to\nreal-world systems.\nA. Attack Deﬁnition\nThe adversary in keyword inference attack is curious about\nthe following predicate, whether certain keyword k is con-\ntained in the unknown sentence x. The keyword k can be\nhighly sensitive, which contains indicators for the adversary\nto further determine e.g., location, residence or illness history\nof the victim [62].\nBefore introducing two illustrative examples, we formulate\nthe mapping P\nkeyword,k for deﬁning the sensitive information\nrelated with keyword k from a sentence x as Pkeyword,k :\nx → (∃w ∈ x,w == k), where the right side denotes a\npredicate that yields True if a word w in the sentence x is the\ntarget keyword k and otherwise False. As the keyword k is\nspeciﬁed by the adversary, the routine Pkeyword,k is obviously\nknown by him/her. Correspondingly, the keyword inference\nattack regarding P\nkeyword,k is deﬁned as Akeyword,k : z →\n(∃w ∈ x,w == k). Different from pattern reconstruction\nattacks, keyword inference attacks probes the occurrence of\ncertain keywords instead of exact reconstuction of the whole\nsequence.\nCase Study - Airline Reviews (abbr. Airline). Sometimes\nairline companies survey their customers in order to e.g.,\n1320\nimprove their customer service. With the aid of advanced NLP\ntechniques, large amounts of airline reviews in text form can be\nautomatically processed for understanding customers’ opinion\n(i.e., opinion mining [69]). As is widely recognized [16], [17],\n[41], utilizing the pre-trained language models for feature\nextraction can further improve the utility of many existing\nopinion mining systems.\nHowever, once accessing the embeddings, the adversary can\ninfer various location-related sensitive information about the\nvictim, including his/her departure, residence, itinerary, etc. As\na preliminary step for further attacks, we show the adversary\ncan accurately estimate the probability of whether certain city\nname is contained in the review.\nCase Study - Medical Descriptions (abbr. Medical). With\nthe booming of intelligent healthcare, some hospitals tend to\nbuild an automatic pre-diagnosis system for more effective\nservice ﬂow [28]. The system is expected to take the patient’s\ndescription of the illness to predict which department he/she\nought to consult. To form a benchmark system, we concatenate\nthe pretrained language models with an additional linear layer\nfor guiding the patients to 10 different departments. Through\nevaluations, we show the systems can achieve over 90%\naccuracy on real-world datasets in Fig. 8(b) of the Appendix.\nMore details can be found in Appendix A.\nHowever, when the adversary gets access to the embeddings\nonly, he/she can indeed infer more sensitive and personalized\ninformation about the patient as a victim. Besides the depart-\nment the patient ought to consult, the adversary can further\ndetermine other ﬁne-grained information like the disease type\nor even the precise disease site. To demonstrate, we suppose\nan adversary wants to pinpoint the precise disease site of the\nvictim by inferring the occurrence probability of body-related\nwords in his/her descriptions.\nB. Methodology\nIn this part, we detail our implementations for keyword\ninference attacks. According to the different levels of the\nadversary’s knowledge on the plain text, the methodology\npart is divided into white-box and black-box settings, which\nrespectively require the following two assumptions.\n• Assumption 3b. The adversary has access to a shadow\ncorpus, which consists of sentences that are sampled from\nthe same distribution of the target plain text (which we refer\nto as white-box).\n• Assumption 3c. The adversary has no information on the\ntarget plain text (which we refer to asblack-box).\nNoteworthily, the adversary under Assumption 3c has almost\nno prior knowledge except that he/she (e.g., any attacker who\ncaptures the embeddings) has access to the embeddings, which\ntherefore poses a rather practical threat to the general-purpose\nlanguage models, while Assumption 3b is also possible to\nhappen in real-world situations when, if we continue the above\nmedical example, some hospital publishes an anonymised\ndataset of medical descriptions for research purposes [1] or\nthe service provider is honest-but-curious.\nAttack in White-Box Settings.Basically, as the adversary has\na shadow corpus D\nshadow := {(x\n′\ni)}N\ni=1 which is sampled from\nthe same distribution as the unknown plain text, he/she can\ndirectly use D\nshadow as the external corpus Dext and extract\nthe binary label y\n′\ni = Pkeyword,k(x\n′\ni). Next, the adversary\ntrains a binary classiﬁer with the dataset to conductAkeyword,k.\nHowever, we notice in practice the adversary may confront\nwith several pitfalls.\nFirst, the label set {y\n′\ni}N\ni=1 can be highly imbalanced. In\nother words, the sentences with the keyword k (i.e., the\npositive samples) may be in an absolute minority compared to\nthose without k (i.e., the negative samples). According to pre-\nvious researches, imbalance in label will let the attack model\nprone to overﬁtting and thus hinder the attack’s performance\n[33]. To alleviate, we propose to randomly replace certain\nword in the negative samples with the keyword, and we replace\nthe keyword in the positive samples with other random word\nin the vocabulary (referred to as the word substitution trick).\nAfter this operation, the original shadow corpus will be twice\nenlarged and the samples are balanced in both classes.\nNext, the shadow corpus after word substitution can still be\nlimited in size, i.e., N is small. In this case, we suggest the\nadversary should implement their attack model with a Support\nV ector Machine (SVM), which is especially effective for small\nsample learning [71]. WhenM is larger than certain threshold\n(empirically over 10\n3 samples), the adversary can switch to\na fully-connected neural network as the attack model, which\nbrings higher attack accuracy.\nAttack in Black-Box Settings.The adversary under Assump-\ntion 3c faces the most challenging situations, as he/she has\nmerely no prior knowledge of the plain text. In turn, successful\nattacks in this general scenario will raise a huge threat on the\nprivacy of general-purpose language models.\nTo implement the keyword inference attack with no prior\nknowledge, we propose to ﬁrst crawl sentences from the\nInternet to form the external corpus and then transfer the\nadversarial knowledge of an attack model on the external\ncorpus to the target corpus dynamically. Details are as follows.\n1) Create the External Corpus from Public Corpora:\nWith the aid of the Internet, it is relatively convenient for\nthe adversary to obtain an external corpus from other public\ncorpora. Next, the adversary can generate positive and negative\nsamples via the same word substitution trick we mentioned in\nthe previous part.\n2) Transfer Adversarial Knowledge: During our prelimi-\nnary attempts, we ﬁnd if we directly train an off-the-shelf\nclassiﬁer (e.g., linear SVM or MLP) on the external corpus\nand use it to conduct keyword inference attacks on the\ntarget embeddings, the attack’s accuracy can sometimes be\npoor. We speculate it is thedomain misalignment that causes\nthis phenomenon. To validate, we ﬁrst train a 3-layer MLP\nclassiﬁer on an external corpus w.r.t. the keywordhead, which\nis prepared from the Y elp-Food dataset (i.e., a dataset that\nconsists of restaurant reviews). Next, we plot the decision\nboundary of the classiﬁer on the external corpus in Fig. 4(a).\nWe also plot the expected decision boundary of the classiﬁer\n1321\non the target medical dataset that contains 1000 sentences\nin Fig. 4(b), where the scattered points plot the intermediate\nrepresentations of the XLNet embeddings at the hidden layer\nafter dimension reduction with Principle Component Analysis\n(PCA) and the two colors imply whether the plain text contains\nhead or not\n1. As we can see, the two decision boundaries are\nalmost orthogonal to each other. As a result, even though the\nattack model on the public domain (i.e., on restaurant reviews)\nachieves a near 100% accuracy, its performance is no better\nthan random guess when applied on the private domain (i.e.,\non medical descriptions).\nFig. 4. Domain misalignment between (a) the external corpus and (b) the\ntarget corpus, through the lens of the (expected) decision boundary of a MLP\nclassiﬁer trained on the external corpus.\nIn general, the key challenge here is how to transfer the\nadversarial knowledge learned by the attack model from the\npublic domain (e.g., Y elp-Food dataset) to the private one (e.g.,\nmedical dataset). First, we introduce some essential notations.\nWe denote the public domain and the private domain respec-\ntively as X\n0,X1. Given a training set Dpublic := {(z\n′\ni,y\n′\ni)}N\ni=1\nfrom X and some target embeddings Dprivate := {zi}n1\ni=1 from\nY, the adversary wants to train an attack modelAkeyword,k that\nperforms well on Dprivate. When the Dprivate and Dpublic dis-\ntribute divergently, the straightforward approach works poorly\nand thus the phenomenon in Fig. 4 occurs.\nTherefore, we propose to learn a uniﬁed domain-invariant\nhidden representations for embeddings from Dprivate and\nDpublic. To realise this, we are inspired from the idea of\nDomain-Adversarial Neural Network (DANN) [9] and propose\nthe architecture of our attack model in Fig. 5.\nFig. 5. Architecture of the attack model in the black-box setting.\nThe model consists of four sub-modules. First, the module\nE is an encoder which takes the sentence embedding as input\nand is expected to output adomain-invariant representation ˆz.\nThe hidden representation is followed by two binary classi-\nﬁers, i.e. C\nkeyword and Cdomain. The keyword classiﬁerCkeyword\ntakes ˆz as input and predicts whether the sentencex contains\nthe keyword k, while the domain classiﬁer Cdomain outputs\nwhether the embedding comes fromX0 or X1. In practice, we\n1More details regarding the external corpus and the medical dataset can be\nfound in the next section.\nimplement E as a nonlinear layer with sigmoid activation and\nimplement Ckeyword and Cdomain as two linear layers followed\nwith a softmax layer. For both classiﬁers, the loss is calculated\nas the cross-entropy between the output and the ground-truth.\nMoreover the loss of C\nkeyword is calculated on Dpublic, while\nthe loss of Cdomain is calculated on {(z\n′\ni,0)}N\ni=1 ∪{(zi,1)}n1\ni=1.\nIn our implementations, an additional module calledgradi-\nent reversal layer [9]is fundamental to learn domain-invariant\nrepresentations and therefore help transfer the adversarial\nknowledge. The gradient reversal layer is intermediate to the\ndomain classiﬁer and the hidden representation, which works\nas an identity layer during the forwarding phase and reverses\nthe gradient by putting a minus sign to each coordinate during\nthe back-propagation phase. Intuitively, the gradient reversal\nlayer regularizes the hidden representationˆz by amplifying the\nkeyword-related features and eliminating the domain-related\ninformation. Algorithm 1 in Appendix I details a typical\niteration in the learning process of our DANN-based attack\nmodel. For inference, we takeC\nkeyword ◦E as the attack model\ng.\nC. Experimental Setup\nWe evaluate the proposed keyword inference attack with\ntwo case studies on Airline and Medical in both white-box\nand black-box settings.\nBenchmark Systems.\n• Airline: We collect the airline review dataset from Skytrax\n[5] and preserve the reviews that contain one of the 10\nspeciﬁed city names (e.g.,Bangkok, Frankfurt, etc.) to form\nour benchmark dataset. The preprocessed dataset contains\n4685 airline reviews (average length 15), and we randomly\nsplit the dataset into10 : 1to get the test set and the shadow\ndataset, which is used to simulate the white-box setting.\nWe choose the shadow set to be the much smaller partition\nto better simulate the real-world situations. We then query\nthe target language models with the reviews in the test set\nand obtain the embeddings as the victims. In the black-box\nsetting, the adversary only accesses the embeddings of the\ntest set for adversarial knowledge transfer. We suppose the\nadversary’s keyword set as the 10 appeared city names.\n• Medical: We implement eight pre-diagnosis systems based\non the CMS public healthcare records [1]. These systems\nare designed to guide patients to the proper department\naccording to the textual description of their disease. We\nreport the utility of the benchmark systems and more imple-\nmentation details in Appendix A. The preprocessed dataset\ncontains 120,000 disease descriptions of average length10.\nWe randomly split the dataset into 10 : 1, to form the test\nset and the shadow dataset. We query the target language\nmodels with the descriptions in the test set to form the target\nset. We suppose the adversary’s keyword set contains 10\nbody-related words (e.g., head, foot, etc.) that appear in the\ndataset.\nMetrics. For evaluations, we prepare balanced test sets for\neach target keyword. In detail, we preserve the embeddings\nof all sentences that contain the keyword from the test set as\n1322\nFig. 6. (a), (e): Accuracy of DANN-based attack per keyword on (a) Airline and (e) Medical.(b), (f): Accuracy of keyword inference attack on (b) Airline\nand (f) Medical, averaged on 10 speciﬁc city names as keywords. (c): Accuracy of MLP-based white-box attack on Medical with varied size of the shadow\ncorpus. (d), (g), (h): Accuracy of DANN-based attack on Medical with (d) different size of the external corpus, (f) varied dimension of the domain-invariant\nrepresentation and (h) varied number of victim embeddings.\nthe positive samples, and randomly sample the same number\nof embeddings from the rest of the test set as the negative\nsamples. The statistics of each test set is in Appendix H. We\nmeasure the attack’s effectiveness on each keyword with the\nclassiﬁcation accuracy on the prepared test sets. To ensure\nthe attack’s effectiveness is not caused by the adversary’s\nknowledge of which keyword to infer, we also conduct black-\nbox attacks with DANN on Airline and Medical to infer 5\nrandom keywords which are not contained in the target corpus.\nExternal Corpus in Black-Box Setting. Following the pro-\ncedures in Section VI-B, we create the external corpus for the\nblack-box setting from the Y elp-Food dataset, which contains\ncustomers’ reviews for local restaurants, has less than 20%\ncommon words with Medical or Airline and can be replaced\nwith other public corpus. Speciﬁcally, we choose 2000 sen-\ntences that contain the word salad and prepare 2000 positive\nand negative samples respectively for each keyword inference\nattack with our proposed word substitution trick.\nAttack Implementation.\n• White-box setting: We study two implementations of the\nattack model in the white-box setting, namely the linear\nSVM and the 3-layer MLP with 80 hidden units with\nsigmoid activations. The batch size is set as64.\n• Black-box setting: We study three implementations in\nthe black-box setting, namely linear SVM, 3-layer MLP\nand the DANN-based attacks. The DANN model has 25-\ndimensional domain-invariant representations and the co-\nefﬁcient λ in Algorithm 1 is set as 1. We use the Adam\noptimizer with learning rate 0.001 for both the MLP and\nthe DANN-based attacks. The batch size is set as64.\nD. Results & Analysis\nFig. 6(b) & (f) report the performance of keyword inference\nattacks in white-box and black-box settings with different\nattack models, respectively on Airline and Medical. The results\nare averaged on 10 keywords. We also provide the DANN-\nbased attacks’ accuracy on each keyword in Fig. 6(a) & (e).\nDue to the game-theoretical essence of DANN, we notice the\naccuracy of DANN-based attacks dynamically change over\ntime. We report both the average and the optimal accuracy\nof the DANN-based attack in50 epochs to reﬂect the average\nand worst-case privacy risk. For the baseline methods SVM &\nMLP , we report their accuracy after their learning processes\nconverge.\n1) Effectiveness & Efﬁciency: These experimental results\nhighly demonstrate the effectiveness of our attacks in both\nwhite-box and black-box settings. For example, from Fig. 6(f),\nwe can see our white-box attack, given Bert’s embeddings of\nthe victims’ medical descriptions, achieves over99% accuracy\nwhen inferring the occurrence of certain body part, while our\nblack-box attack with no prior knowledge can still achieve\nover 75% accuracy on average. Similarly, when given the\nGPT and GPT -2 embeddings of the victim’s airline review,\nour white-box and black-box attacks respectively achieve over\n95%and 75%accuracy on average for inferring the occurrence\nof certain city names. We also report the DANN-based attacks’\naccuracy on Airline and Medical in Table III, when inferring\non 5 random keywords which are not contained in the target\nsentences. As we can see, our black-box attack achieves over\n90% average accuracy in most cases. Moreover, we report\nthe throughput of training attack models in Table VI, which\nindicates the attacks can be efﬁciently constructed in less\nthan 2 minutes. Combined with the success of the pattern\nreconstruction attacks in the previous section, these obser-\nvations further support our main ﬁnding that much sensitive\ninformation is encoded in the embeddings from these8 target\nlanguage models and can be practically reverse-engineered for\nmalicious purposes.\n2) Comparison among Language Models: From Fig. 6(b),\nwe notice Google’s XL and Facebook’s RoBERTa show much\nstronger robustness than other language models when facing\nour white-box attacks on Airline. For these two models, our\nwhite-box attacks only outperform the random guesser with a\nslight margin, while on Medical, white-box attacks aiming at\n1323\nT ABLE III\nAVERAGE AND WORST -C ASE RISK WHEN DANN- BASED ATT ACKS INFER\n5 RANDOM KEYWORDS (# OF TEST SAMPLE SIZE = 1000).\nName Medical\n(worst/average)\nAirline\n(worst/average)\nBert 1.000/0.860 0 .999/0.891\nTransformer-XL 0.667/0.555 0 .907/0.807\nXLNet 0.974/0.951 0 .994/0.974\nGPT 0.996/0.974 0 .998/0.987\nGPT -2 0.998/0.973 0 .995/0.981\nRoBERTa 0.996/0.992 0 .998/0.996\nXLM 0.948/0.907 0 .959/0.925\nErnie 2.0 0.965/0.862 0 .964/0.922\nthese two models achieve around80% accuracy when attacks\non other models are uniformly over 95%. This phenomenon\nimplies the sensitive information in XL ’s and RoBERTa’s em-\nbeddings is much harder to be reverse-engineered, which we\nspeculate the major causes as XL ’s relative small pretraining\ndata size (and thus smaller vocabulary [16]), and RoBERTa’s\nbyte-level tokenization scheme. As a result, linear SVM has no\nsufﬁcient learning capacity [71] to exploit the sensitive infor-\nmation from their embeddings, while the MLP-based attacks\nsuffer from underﬁtting caused by the limited sample size on\nAirline (only about 400 on Airline c.f. 2000 on Medical).\nHowever, combined with Fig. 6(g), slight utility-privacy trade-\noff is observed on Medical for XL, which shows about 7%\nlower utility than the best utility performance achieved by Bert,\nwhile no utility-privacy trade-off is observed for RoBERTa,\nwhich is probably because the utility performance of most of\nour systems is high (over90%) and hence the utility difference\nis not very clear.\n3) Comparison among Attack Implementations:First, com-\nparing the left two columns and the right three columns in\neach bar group of Fig. 6(b) & (f), we can see the white-\nbox attacks in general are more effective than the black-box\ncounterparts. For example, we notice white-box attacks on\nMedical show an average 25% margin over the black-box\nattacks, while exceptions are observed for RoBERTa and XL,\nwhich we have discussed above. Next, among the black-box\nattacks, MLP and DANN attacks show similar effectiveness\nin many conﬁgurations. However, for Facebook’s XLNet on\nMedical, the accuracy of the MLP attack is only 0.560,\nwhich corresponds to the domain misalignment in Fig. 4,\nwhile the DANN approach improves the attack’s accuracy\nto 0.601 on average and 0.691 in the worst case. Finally,\nwe also investigate the performance of DANN attacks on\neach keyword. From Fig. 6(a) & (e), we can see different\nlanguage models have their especially vulnerable keyword.\nFor example, DANN-based attack achieves over95% accuracy\nwhen inferring the wordchest from Google’s Bert embeddings,\nand over 80% when inferring ankle from Baidu’s ERNIE\nembeddings. We would like to invesigate the fundamental\ncause of this interesting phenomenon in a future work.\n4) Ablation Study: We also conduct an overall ablation\nstudy by investigating the keyword inference attack in a wide\nrange of conﬁgurations on three variants of OpenAI’s GPT -\n2 (namely GPT -2, GPT -2-Medium, GPT -2-Large), which are\npretrained on the same corpus, but increments in parameter\nnumbers and embedding dimension [55]. Detailed statistics\ncan be found in Appendix H.\nFirst, we study the the impact of external corpus size on\nthe attack effectiveness: (1) For the white-box setting, we\nvary the size of the shadow corpus, which represents the\nknowledge level of the adversary in the white-box setting, in\n{10,100,..., 1000} and conduct the MLP attack on Medical.\nThe results are provided in Fig. 6(c). As we can see, the\nattack accuracy remains over 90% for each language model\nwhen the shadow corpus size is larger than 100. Moreover,\nwe interestingly observe, a larger language model (GPT -2-\nLarge) is less robust than a smaller one when the adversary’s\nknowledge is limited. When the shadow corpus size is only\n10, the attack accuracy is 68.5%, 61.0% and 59.4% against\nGPT -2-Large, GPT -2-Medium and GPT -2 respectively. This\nobservation is also consistent with the conclusion in [34]:\ncomplicated models tend to enlarge the attack surface. (2)\nSimilarly, for the black-box setting, we compare the DANN\nattack’s performance by varying the size of the external corpus\nfrom 100% to 5% of the original size 2000, with results\nreported in Fig. 6(d). As we can see, a larger external corpus\nhelps our attack achieve higher accuracy, which demonstrates\nthe effectiveness of our proposed adversarial knowledge trans-\nfer procedure.\nAlso, we study the robustness of DANN attack w.r.t. its\nhyperparameters. We respectively control the size of the victim\nembeddings, which corresponds to the knowledge level of\nthe adversary in the black-box setting, and the dimension of\nthe domain-invariant representation in DANN, which is the\nonly architectural factor of DANN, and conduct the DANN\nattack on Medical. For these two settings, we report the\nattack accuracy respectively in Fig. 6(g) & (h). As is shown,\nthe accuracy of DANN attack remains high with different\nhyperparameter choices, which highly reﬂects the robustness\nand effectiveness of our proposed attack model.\nVII. P\nOSSIBLE DEFENSES\nAs the sentence embedding is the direct source of potential\ninformation leakage, a general principle for mitigation is to\nobfuscate the embeddings. For this purpose, we empirically\nevaluate four possible technical choices, where the ﬁrst three\nare general against both attacks and the last one is specially\ndesigned for keyword inference attack. Although the ideal situ-\nation is that the sensitive information can be totally eliminated\nwhile the required information for other normal tasks can be\nhighly preserved, in practice such a utility-privacy trade-off\nseems unavoidable, at least according to our reported trade-\noff results below. Here, the utility denotes the classiﬁcation\naccuracy of the underlying benchmark systems. We hope our\npreliminary study will foster future mitigation studies. The\nomitted technical details and experimental setups can be found\nin Appendix C.\n(1) Rounding. For the ﬁrst defense, we apply ﬂoating-point\nrounding on each coordinate of the sentence embeddings for\nobfuscation. Formally, we write the rounding defense as ˆz =\n1324\nFig. 7. The utility and attack accuracy curves along on Genome and Medical when four possible defenses with different parameters are applied for mitigation.\nFor DP & PPM defenses, the x-axes of utility and attack accuracy curves are in log scale.\nrounding(z,r), where the non-negative integer r denotes the\nnumber of decimals preserved after rounding.\n(2) Laplace Mechanism.For the second defense, we leverage\na differential privacy approach, the Laplace mechanism [20].\nRoughly speaking, we perturb the embedding coordinate-wise\nwith samples from a Laplace distribution whose parameters\nare determined by the ℓ\n1-sensitivity of the language model f\n(denoted as Δf, which we estimate with numeric algorithms).\nFormally, the defense works asˆz = z+(Y1,...,Y d), where Yi\nare i.i.d. random samples drawn fromLap(Δf/ϵ), the Laplace\ndistribution with location 0 and scale Δf/ϵ.\n(3) Privacy Preserving Mapping. The third defense is\nbased on adversarial training. We borrow the notion of a\nPrivacy Preserving Mapping (PPM) from [57] to denote a\nmapping D\nθ : Rd → Rd parameterized by θ, which is\ntrained to minimize the effectiveness of an imagined adversary\nAψ. Meanwhile, the PPM is required to follow the utility\nconstraint by distorting the embeddings only in a limited radius\naround the original embedding, which is implemented as a\nregularization term. Formally, we propose to learn the privacy\npreserving mapping D\nθ by solving the following minimax\ngame minθ maxψ 1\nn\n∑n\ni=1 Aψ(Dθ(zi),si)+ λ∥Dθ(zi)−zi∥2\nwhere λcontrols the privacy level, the higher the lower privacy\nlevel.\n(4) Subspace Projection. The last defense is especially\ndesigned as a countermeasure to keyword inference attacks,\ninspired by [14] on debiasing word embeddings from gender\nbias. The general idea of this defense is to project out\nthe unwanted subspace (i.e., privacy subspace) that encodes\nthe occurrence of the keyword from the universal sentence\nembedding space. Technical details on how to identify the\nprivacy subspace and how to do the projection can be found\nin Appendix C. In our evaluations, we consider the ratio β\nbetween the dimension of the privacy subspace and that of the\nuniversal embedding space as the parameter of this defense.\nIntuitively, a higher β is expected to bring a stricter privacy\nmechanism.\nEvaluations. We evaluate the ﬁrst three defenses against the\npattern reconstruction attack on Genome and all four defenses\nagainst the DANN-based keyword inference attack on Medical\nwith a wide range of settings. The conﬁgurations and results\nof the ﬁrst three defenses are presented in Fig. 7.\nAs we can see from Fig. 7, although each defense can\nattenuate the attacker to a total random guesser under certain\nprivacy budgets, they simultaneously compromise the utility of\ndownstream tasks by causing an unacceptable degradation. For\nexample, the Laplace mechanism degrades the utility to a total\nrandom guesser as well when achieving the optimal defense\nperformance. For PPM, despite a slighter trade-off is observed,\nthe utility for RoBERTa and Transformer-XL still degrades\nfrom over 90% to around 25% when the optimal defense\nis achieved. Among these defenses, we notice the subspace\nprojection defense could provide a more desirable defense\nquality than the three possible defenses. For example, for\nmost of the target language models, the defense can degrade\nthe DANN attack to a random guesser by projecting out\nonly the 1% keyword-related subspace. However, the utility\nof the embeddings on the downstream task still decreases by\nabout 15% compared with the 95% accuracy with unprotected\nembeddings, which implies the keywords that we want to hide\nare also critical to provide essential semantics for downstream\ntasks. Moreover, the quality of subspace defense in practice\nwould be less desirable due to its blindness to the target\nkeyword that the adversary is interested in.\nAccording to our preliminary results above, how to balance\nthe elimination of token-level sensitive information from the\nembeddings and the preservation of the essential information\nfor normal tasks is still an open problem that awaits more\nin-depth researches. In consideration of the practical threats\nimposed by our attacks on the applications of general-purpose\nlanguage models, we highly suggest the exploration of effec-\ntive defense mechanisms as a future work.\nVIII. D\nISCUSSIONS\nOn Threat Model. For Assumption 0, the adversary can\nget the sentence embeddings of victims if general-purpose\n1325\nlanguage models are deployed in collaborative or federated\nlearning systems, especially when a) the service provider\nitself wants to snoop user’s sensitive information or b) the\nembeddings are shared accidentally or abusively with some\nmalicious attackers. In some recent protocols, the feature\nmay be encrypted with homomorphic encryption schemes for\nprivacy protection [24], which therefore requires an adver-\nsary to ﬁrst encrypt the embeddings of the external corpus\nwith the public key and train the attack models on the\nencrypted external corpus. This is an interesting scenario that\ndeserves dedicated research and we leave this as a future\nwork. Nevertheless, there are also many scenarios which are\nnot suitable for homeomorphic encryption schemes due to\nefﬁciency issues, such as real-time long passage translation\n[75] or search engines with language models [6]. In these\nscenarios, our attacks remain huge threats.\nFor Assumption1, we devise a learning-based ﬁngerprinting\nalgorithm by ﬁrst determining the candidate model types\nbased on the dimension size and then pinpointing the exact\nmodel type with a pretrained classiﬁer. Strikingly, we ﬁnd\nthe classiﬁcation accuracy can achieve 100%. More technical\ndetails and analysis can be found in Appendix D.\nFor Assumption 2, the adversary can easily satisfy the\nassumption by deploying the language model on local devices\nor accessing the corresponding online services. In the current\nwork, we adopt this assumption considering the generality of\nour attack. Nevertheless, the adversary could further exploit\nthe speciﬁc language model architecture and pretrained pa-\nrameters for better attack effectiveness, which is an interesting\nand meaningful direction to pursue in the future work.\nDownstream Attacks.As we have mentioned in Sections V &\nVI, our proposed attacks can be further used for downstream\nattacks that can cause more severe consequences. For example,\nif the attacker for some reason gets the embedding of the\ntreatment description “CT scan of blood vessel of head with\ncontrast”, he/she can utilize the proposed keyword inference\nattack to infer the occurrence probability of each word in a\ncustomized vocabulary (e.g., the vocabulary containshead and\nvessel because they are frequent words in medical descrip-\ntions), sort the occurrence probability in the decreasing order\n(e.g., the two words head and vessel both have occurrence\nprobability higher than 90%) and thus inverts out the meaning\nof the sentence (e.g.,the patient may have something abnormal\nin the blood vessel of his head). Appendix E provides a demon-\nstrative experiment that implement the above guideline. We\nﬁnd the adversary can indeed reassemble the basic semantic\nmeaning of the original text with the above procedure, even\nif some words may not be in the customized vocabulary.\nUtility vs. Privacy in Deploying Sentence Embeddings.\nOur current work indicates the improved utility of sentence\nembeddings from general-purpose language models is at odds\nwith the privacy. In principle, to balance the utility-privacy\ntrade-off requires the sentence embedding to preserve the\ninformation that is desired for the downstream task and to\ndiscard the remainder, while the following dilemma happens\nfor general-purpose language models: these models are es-\npecially designed to provide embeddings that can be used\nfor a wide range of downstream tasks [17], which conse-\nquently enforces the embeddings to preserve much token-level\ninformation, which is critical in forming semantics in many\ncases and hence leaves the adversary a window for privacy\nbreach. Based on our systematic evaluations of eight state-of-\nthe-art language models, we ﬁnd the byte-level tokenization\nscheme may indeed provide additional privacy protection by\ndesign. In the meantime, obfuscating sentence embeddings via\nadversarial training or subspace projection may be a promising\ndirection for future studies as they can achieve more desirable\nutility-privacy trade-off.\nLimitations & Future Works. Although we have observed\nsome interesting differences in the security property of dif-\nferent language models, we are still not very clear about\nhow many other design choices, including the network depth,\nlearning algorithms and hyper-parameters, inﬂuence the corre-\nsponding language model’s privacy level. We are interesting to\ninvestigate these issues in a future work. Moreover, although\nwe provided preliminary study on four possible defenses, we\nﬁnd none of them could achieve an optimal balance between\nprivacy and utility on downstream tasks. Also, due to the hard-\nware constraints, we have not evaluated the defense quality of\ndifferentially private training techniques (e.g., DPSGD [7]).\nWe hope our work will draw more attentions from researchers\nto conduct more in-depth study on the privacy properties of\nthis new NLP paradigm and the corresponding mitigation\napproaches.\nIX. C\nONCLUSION\nIn this paper, we design two novel attack classes, i.e.,\npattern reconstruction attacks and keyword inference attacks,\nto demonstrate the possibility of stealing sensitive information\nfrom the sentence embeddings. We conduct extensive evalu-\nations on eight industry-level language models to empirically\nvalidate the existence of these privacy threats. To shed light\non future mitigation studies, we also provide a preliminary\nstudy on four defense approaches by obfuscating the sentence\nembeddings to attenuate the sensitive information. To the best\nof our knowledge, our work presents the ﬁrst systematic study\non the privacy risks of general-purpose language models, along\nwith the possible countermeasures. For the future leverage\nof the cutting-edge NLP techniques in real world settings,\nwe hope our study can arouse more research interests and\nefforts on the security and privacy of general-purpose language\nmodels.\nA\nCKNOWLEDGEMENT\nWe sincerely appreciate the shepherding from Piotr\nMardziel. We would also like to thank the anonymous re-\nviewers for their constructive comments and input to improve\nour paper. This work was supported in part by the National\nNatural Science Foundation of China (61972099, U1636204,\nU1836213, U1836210, U1736208, 61772466, U1936215, and\nU1836202), the National Key Research and Development Pro-\ngram of China (2018YFB0804102), the Natural Science Foun-\n1326\ndation of Shanghai (19ZR1404800), the Zhejiang Provincial\nNatural Science Foundation for Distinguished Y oung Scholars\nunder No. LR19F020003, the Ant Financial Research Funding,\nand the Alibaba-ZJU Joint Research Institute of Frontier Tech-\nnologies. Min Y ang is the corresponding author, and a faculty\nof Shanghai Institute of Intelligent Electronics & Systems,\nShanghai Institute for Advanced Communication and Data\nScience, and Engineering Research Center of CyberSecurity\nAuditing and Monitoring, Ministry of Education, China.\nR\nEFERENCES\n[1] “Cms public healthcare dataset,” https://www.cms.gov/Research-\nStatistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-\nProvider-Charge-Data/Physician-and-Other-Supplier2016.html,\naccessed: 2019-09-10.\n[2] “Google-research/bert,” https://github.com/google-research/bert,\naccessed: 2020-1-4.\n[3] “Identity theft continuing problem in the state, nation,”\nhttps://www.enidnews.com/news/identity-theft-continuing-problem-in-\nthe-state-nation/article\n6c1cf034-f40e-11e9-8401-07cb9f8a7875.html,\naccessed: 2019-10-24.\n[4] “Open sourcing bert: State-of-the-art pre-training for natural lan-\nguage processing,” https://ai.googleblog.com/2018/11/open-sourcing-\nbert-state-of-art-pre.html, accessed: 2020-1-4 Published 2018-11-02.\n[5] “Skytrax dataset,” https://github.com/quankiquanki/skytrax-reviews-\ndataset, accessed: 2019-09-10.\n[6] “Understanding searches better than ever before,”\nhttps://blog.google/products/search/search-language-understanding-\nbert, accessed: 2019-09-10.\n[7] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov,\nK. Talwar, and L. Zhang, “Deep learning with differential privacy,”\nArXiv, vol. abs/1607.00133, 2016.\n[8] B. Agir, K. Huguenin, U. Hengartner, and J.-P . Hubaux, “On the privacy\nimplications of location semantics,”Proceedings on Privacy Enhancing\nTechnologies, vol. 2016, pp. 165 – 183, 2016.\n[9] H. Ajakan, P . Germain, H. Larochelle, F. Laviolette, and M. Marchand,\n“Domain-adversarial neural networks,”ArXiv, vol. abs/1412.4446, 2014.\n[10] G. Ateniese, L. V . Mancini, A. Spognardi, A. Villani, D. Vitali, and\nG. Felici, “Hacking smart machines with smarter ones: How to extract\nmeaningful data from machine learning classiﬁers,” IJSN, vol. 10, pp.\n137–150, 2013.\n[11] Y . Bengio, A. C. Courville, and P . Vincent, “Representation learning: A\nreview and new perspectives,”TPAMI, vol. 35, pp. 1798–1828, 2012.\n[12] P . Berrang, M. Humbert, Y . Zhang, I. Lehmann, R. Eils, and M. Backes,\n“Dissecting privacy risks in biomedical data,”Euro Security & Privacy,\npp. 62–76, 2018.\n[13] B. Biggio, B. Nelson, and P . Laskov, “Poisoning attacks against support\nvector machines,” in ICML, 2012.\n[14] T. Bolukbasi, K.-W . Chang, J. Y . Zou, V . Saligrama, and A. T. Kalai,\n“Man is to computer programmer as woman is to homemaker? debiasing\nword embeddings,” in NIPS, 2016.\n[15] I. Chalkidis, M. Fergadiotis, P . Malakasiotis, and I. Androutsopoulos,\n“Large-scale multi-label text classiﬁcation on eu legislation,” in ACL,\n2019.\n[16] Z. Dai, Z. Y ang, Y . Y ang, J. G. Carbonell, Q. V . Le, and R. Salakhutdi-\nnov, “Transformer-xl: Attentive language models beyond a ﬁxed-length\ncontext,” in ACL, 2019.\n[17] J. Devlin, M.-W . Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of\ndeep bidirectional transformers for language understanding,” inNAACL-\nHLT, 2018.\n[18] A. Dosovitskiy and T. Brox, “Inverting visual representations with\nconvolutional networks,” CVPR, pp. 4829–4837, 2016.\n[19] V . Duddu, D. Samanta, D. V . Rao, and V . E. Balas, “Stealing neural\nnetworks via timing side channels,” ArXiv, vol. abs/1812.11720, 2018.\n[20] C. Dwork and A. Roth, “The algorithmic foundations of differential\nprivacy,” F oundations and Trends in Theoretical Computer Science ,\nvol. 9, pp. 211–407, 2014.\n[21] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that\nexploit conﬁdence information and basic countermeasures,” in CCS,\n2015.\n[22] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart,\n“Privacy in pharmacogenetics: An end-to-end case study of personalized\nwarfarin dosing,” vol. 2014, pp. 17–32, 2014.\n[23] K. Ganju, Q. Wang, W . Y ang, C. A. Gunter, and N. Borisov, “Property\ninference attacks on fully connected neural networks using permutation\ninvariant representations,” in CCS, 2018.\n[24] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. E. Lauter, M. Naehrig, and\nJ. R. Wernsing, “Cryptonets: Applying neural networks to encrypted\ndata with high throughput and accuracy,” inICML, 2016.\n[25] I. Goodfellow, Y . Bengio, and A. Courville,Deep Learning. MIT Press,\n2016, http://www.deeplearningbook.org.\n[26] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. C. Courville, and Y . Bengio, “Generative adversarial nets,”\nin NIPS, 2014.\n[27] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” ArXiv, vol. abs/1412.6572, 2014.\n[28] W . Guo, J. Shao, R. Lu, Y . Liu, and A. A. Ghorbani, “A privacy-\npreserving online medical prediagnosis scheme for cloud environment,”\nIEEE Access, vol. 6, pp. 48 946–48 957, 2018.\n[29] M. Hardt, E. Price, and N. Srebro, “Equality of opportunity in supervised\nlearning,” in NIPS, 2016.\n[30] M. Humbert, E. A yday, J.-P . Hubaux, and A. Telenti, “Addressing the\nconcerns of the lacks family: quantiﬁcation of kin genomic privacy,” in\nCCS, 2013.\n[31] ——, “Quantifying interdependent risks in genomic privacy,” ACM\nTrans. Priv. Secur ., vol. 20, pp. 3:1–3:31, 2017.\n[32] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” ArXiv, vol.\nabs/1502.03167, 2015.\n[33] N. Japkowicz and S. Stephen, “The class imbalance problem: A system-\natic study,” Intell. Data Anal., vol. 6, pp. 429–449, 2002.\n[34] Y . Ji, X. Zhang, S. Ji, X. Luo, and T. Wang, “Model-reuse attacks on\ndeep learning systems,” in CCS, 2018.\n[35] J. Jia and N. Z. Gong, “Attriguard: A practical defense against at-\ntribute inference attacks via adversarial machine learning,” ArXiv, vol.\nabs/1805.04810, 2018.\n[36] A. Jochems, T. Deist, I. E. Naqa, M. L. Kessler, C. Mayo, J. Reeves,\nS. Jolly, M. Matuszak, R. T. Haken, J. van Soest, C. J. G. Oberije,\nC. Faivre-Finn, G. J. Price, D. K. M. D. Ruysscher, P . Lambin, and\nA. Dekker, “Developing and validating a survival prediction model\nfor nsclc patients through distributed learning across 3 countries,” in\nInternational journal of radiation oncology, biology, physics, 2017.\n[37] A. Jochems, T. Deist, J. van Soest, M. J. Eble, P . Bulens, P . A. Coucke,\nW . J. F. Dries, P . Lambin, and A. Dekker, “Distributed learning: Devel-\noping a predictive model based on data from multiple hospitals without\ndata leaving the hospital - a real life proof of concept.”Radiotherapy and\noncology : journal of the European Society for Therapeutic Radiology\nand Oncology, vol. 121 3, pp. 459–467, 2016.\n[38] D. Jurafsky, “Speech and language processing,” 2006.\n[39] D. P . Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nArXiv, vol. abs/1412.6980, 2014.\n[40] R. Kiros, Y . Zhu, R. Salakhutdinov, R. S. Zemel, R. Urtasun, A. Torralba,\nand S. Fidler, “Skip-thought vectors,” inNIPS, 2015.\n[41] G. Lample and A. Conneau, “Cross-lingual language model pretraining,”\nArXiv, vol. abs/1901.07291, 2019.\n[42] Q. V . Le and T. Mikolov, “Distributed representations of sentences and\ndocuments,” ArXiv, vol. abs/1405.4053, 2014.\n[43] B. Lee, T. Lee, B. Na, and S. Y oon, “Dna-level splice junction prediction\nusing deep recurrent neural networks,” ArXiv, vol. abs/1512.05135,\n2015.\n[44] Y . Liu, M. Ott, N. Goyal, J. Du, M. S. Joshi, D. Chen, O. Levy, M. Lewis,\nL. S. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” ArXiv, vol. abs/1907.11692, 2019.\n[45] F. MASTEROPPGA VE and Ø. Johansen, “Gene splice site prediction\nusing artiﬁcial neural networks,” 2008.\n[46] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,\n“Communication-efﬁcient learning of deep networks from decentralized\ndata,” in AISTATS, 2016.\n[47] L. Melis, C. Song, E. D. Cristofaro, and V . Shmatikov, “Exploiting\nunintended feature leakage in collaborative learning,” in Security &\nPrivacy, 2019.\n[48] E. M. E. Mhamdi, R. Guerraoui, and S. Rouault, “The hidden vulnera-\nbility of distributed learning in byzantium,” inICML, 2018.\n1327\n[49] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\n“Distributed representations of words and phrases and their composi-\ntionality,” in NIPS, 2013.\n[50] M. Nasr, R. Shokri, and A. Houmansadr, “Machine learning with\nmembership privacy using adversarial regularization,” inCCS, 2018.\n[51] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Y ang, Z. DeVito, Z. Lin,\nA. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in\npytorch,” 2017.\n[52] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and\nL. S. Zettlemoyer, “Deep contextualized word representations,” ArXiv,\nvol. abs/1802.05365, 2018.\n[53] P . Pollastro and S. Rampone, “Hs3d: Homo sapiens splice site data set,”\n2002.\n[54] A. Radford, “Improving language understanding by generative pre-\ntraining,” 2018.\n[55] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” 2019.\n[56] H. E. Robbins, “A stochastic approximation method,” 2007.\n[57] S. Salamatian, A. Zhang, F. du Pin Calmon, S. Bhamidipati, N. Fawaz,\nB. Kveton, P . Oliveira, and N. Taft, “Managing your private and public\ndata: Bringing down inference attacks against your privacy,” IEEE\nJournal of Selected Topics in Signal Processing, vol. 9, pp. 1240–1255,\n2015.\n[58] A. Salem, A. Bhattacharyya, M. Backes, M. Fritz, and Y . Zhang,\n“Updates-leak: Data set inference and reconstruction attacks in online\nlearning,” ArXiv, vol. abs/1904.01067, 2019.\n[59] A. Salem, Y . Zhang, M. Humbert, P . Berrang, M. Fritz, and M. Backes,\n“Ml-leaks: Model and data independent membership inference attacks\nand defenses on machine learning models,”ArXiv, vol. abs/1806.01246,\n2018.\n[60] G. Salton and C. Buckley, “Term-weighting approaches in automatic\ntext retrieval,” Inf. Process. Manage., vol. 24, pp. 513–523, 1988.\n[61] G. Shen, K. Dwivedi, K. Majima, T. Horikawa, and Y . Kamitani, “End-\nto-end deep image reconstruction from human brain activity,” inFront.\nComput. Neurosci., 2019.\n[62] R. Shetty, B. Schiele, and M. Fritz, “A4nt: Author attribute anonymity by\nadversarial training of neural machine translation,” inUSENIX Security\nSymposium, 2017.\n[63] R. Shokri, M. Stronati, C. Song, and V . Shmatikov, “Membership\ninference attacks against machine learning models,”Security & Privacy,\npp. 3–18, 2017.\n[64] R. Shokri, G. Theodorakopoulos, J.-Y . L. Boudec, and J.-P . Hubaux,\n“Quantifying location privacy,”Security & Privacy, pp. 247–262, 2011.\n[65] S. S. Shringarpure and C. D. Bustamante, “Privacy risks from genomic\ndata-sharing beacons.” American journal of human genetics, vol. 97 5,\npp. 631–46, 2015.\n[66] L. Song, R. Shokri, and P . Mittal, “Privacy risks of securing ma-\nchine learning models against adversarial examples,” ArXiv, vol.\nabs/1905.10291, 2019.\n[67] Y . Sun, S. Wang, Y . Li, S. Feng, H. Tian, H. Wu, and H. Wang, “Ernie\n2.0: A continual pre-training framework for language understanding,”\nArXiv, vol. abs/1907.12412, 2019.\n[68] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning\nwith neural networks,” in NIPS, 2014.\n[69] D. Tang, Y . Zhao, and T. Liu, “Document modeling with gated recurrent\nneural network for sentiment classiﬁcation,” in EMNLP, 2015.\n[70] F. Tram `er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart,\n“Stealing machine learning models via prediction apis,” ArXiv, vol.\nabs/1609.02943, 2016.\n[71] V . N. V apnik, “The nature of statistical learning theory,” inStatistics for\nEngineering and Information Science, 1995.\n[72] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” pp. 5998–6008,\n2017.\n[73] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine\nlearning,” Security & Privacy, pp. 36–52, 2018.\n[74] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,\nP . Cistac, T. Rault, R. Louf, M. Funtowicz, and J. Brew, “Huggingface’s\ntransformers: State-of-the-art natural language processing,” ArXiv, vol.\nabs/1910.03771, 2019.\n[75] Y . Wu and M. S. et al., “Google’s neural machine translation system:\nBridging the gap between human and machine translation,”ArXiv, vol.\nabs/1609.08144, 2016.\n[76] Z. Y ang, Z. Dai, Y . Y ang, J. G. Carbonell, R. Salakhutdinov, and\nQ. V . Le, “Xlnet: Generalized autoregressive pretraining for language\nunderstanding,” ArXiv, vol. abs/1906.08237, 2019.\n[77] Z. Y ang, J. Zhang, E.-C. Chang, and Z. Liang, “Neural network inversion\nin adversarial setting via background knowledge alignment,” in CCS,\n2019.\n[78] S. Y eom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy risk in\nmachine learning: Analyzing the connection to overﬁtting,” CSF, pp.\n268–282, 2017.\n[79] L. Y u, W . Zhang, J. Wang, and Y . Y u, “Seqgan: Sequence generative\nadversarial nets with policy gradient,”ArXiv, vol. abs/1609.05473, 2016.\n[80] Y . Zhang, M. Humbert, T. A. Rahman, C.-T. Li, J. Pang, and M. Backes,\n“Tagvisor: A privacy advisor for sharing hashtags,”WWW, 2018.\n[81] Y . Zhang, X. Liu, J. N. MacLeod, and J. Liu, “Discerning novel splice\njunctions derived from rna-seq alignment: a deep learning approach,” in\nBMC Genomics, 2018.\n[82] L. Zhu, Z. Liu, and S. Han, “Deep leakage from gradients,” inNeurIPS,\n2019.\nAPPENDIX\nA. Implementation of Utility Models\nFig. 8. Utility of benchmark systems on (a) Genome and (b) Medical.\n1) Genome: We implemented eight genome classiﬁcation\nsystems for splice site prediction based on a public genome\ndataset called HS3D (Homo Sapiens Splice Sites Dataset) 2.\nRoughly speaking, splice site prediction is a binary classiﬁca-\ntion task in computational genetics which determines whether\nthe target nucleotide sequence contains certain functional unit.\nTo build the benchmark systems, we ﬁrst prepared a dataset\nfrom HS3D that consists 28800 (2880) negative (positive)\nsamples for training and 1000 (1000) samples for testing.\nAll the genome sequences are of length 20. Each system\nis composed with a pretrained language model for feature\nextraction and a three-layer MLP of 200 hidden units with\nsigmoid activation for classiﬁcation. Fig. 8(a) reports the utility\n(in accuracy) of our benchmark system when incorporating\ndifferent language models, where the non-trivial margin over\nthe random guess demonstrates their effectiveness.\n2) Medical: We implemented eight pre-diagnosis systems\nbased on the CMS public healthcare records\n3. These sys-\ntems are designed to guide patients to the proper department\naccording to the textual description of their decease. The pre-\nprocessed dataset contains 120,000 decease descriptions from\n10 medical departments (e.g., Orthopedic Surgery, Anesthesi-\nology, Dermatology and so on). We model the pre-diagnosis\ntask as 10-class classiﬁcation, and implemented the systems as\na combination of the pretrained language models for feature\nextraction and a three-layer MLP of 200 hidden units with\nsigmoid activation for classiﬁcation. First, the classiﬁcation\n2http://www.sci.unisannio.it/docenti/rampone/\n3https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-\nTrends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-\nSupplier2016.html\n1328\naccuracy in Fig. 8(b) highly demonstrates the utility of our\nbenchmark systems.\nB. Implementation Details of Attack Model on Genome\nIn practice, we ﬁnd training an attack model targeted on\nthe i-th nucleotide is ineffective if we only use the pairs\nof embeddings and the corresponding nucleotide types. We\nspeculate it is probably because the training sample itself\ncontains insufﬁcient information. Consider the example of\nsequence ACGTAACT and the attacker targeted at the fourth\nnucleotide. If we only supply the learning model with its\nembedding and the type T, the attack model has actually\nno idea of whether the learning objective is to infer the T\nat the target position, or the T at the tail. To solve this\nproblem, we propose to augment the training pair(z,w\ni) with\nauxiliary information regarding the target position, in the form\nof positional for the target position i.\nBasically, we concatenate the embeddingz of the generated\nsample with the positional embedding pi for position i. For-\nmally, we use the sinusoidal positional embedding as in [72],\nwhich is deﬁned by pi,2k =s i n (i/100002k/dpos ),pi,2k+1 =\ncos(i/10000(2k+1)/dpos ), where pi,2k denotes the 2k-th coor-\ndinate of pi and dpos is its dimension. In our implementation,\nwe set dpos equal to the dimension of z. Corresponding to\nthis modiﬁcation, we implement one single attack model\nfor inferring the nucleotide type at any speciﬁed position.\nDifferent from the Citizen case, this modiﬁcation will not\nincrease the parameter number as the class number is still 4.\nThe attack model is implemented as a three-layer MLP which\ntakes input z ⊕p\ni of dimension 2d and has 200 hidden units\nwith sigmoid activation and intermediate batch normalization\nlayers [32] for faster convergence. For training, we generate\nmini-batches of size 128 that consist of tuples (z,pi,wi),\nwhere the positional embedding i is randomly sampled from\nthe interval of possible positions (i.e.1,..., 20). For inference,\nthe attacker inputs the victim’s embedding and the targeted\nposition and the model outputs the predicted nucleotide type.\nC. Omitted Details on Defenses\nLaplace Mechanism. For the second defense, we leverage\na differential privacy approach for mitigation. Speciﬁcally,\nwe apply the Laplace mechanism introduced in [20] to pro-\ntect the original embedding from privacy breaching. Roughly\nspeaking, the Laplace mechanism perturbs the embedding\ncoordinate-wise with samples from a Laplace distribution\nwhose parameters are determined by the ℓ\n1-sensitivity of the\nlanguage models (denoted asf). Formally, we propose the DP-\nbased defense asˆz = Dlap,f,ϵ(z) .= z+(Y1,...,Y d) , where Yi\nare i.i.d. random samples drawn fromLap(Δf/ϵ), the Laplace\ndistribution with location 0 and scale Δf/ϵ. Here Δf denotes\nthe ℓ1-sensitivity of the language model, which is deﬁned\nas Δf =m a xx,x′∈N|V|,∥x−x′∥1=1 ∥f(x) − f(x\n′\n)∥1 where\nx\n′\n,x are sentences that are different only in one position.\nIntuitively, the Laplace mechanism makes it harder for the\nadversary to distinguish from the embeddings the difference\ncaused by alteration of one word, which thus defends both\nthe pattern reconstruction attack and keyword inference attack\nin a straightforward way. In theory, it can be proved that\nthe language model with protection, i.e. D\nlap,f,ϵ ◦f is (ϵ,0)-\ndifferential private. In practice, we estimate the ℓ1-sensitivity\nof each language model by generating10,000 pairs of (x,x\n′\n)\nby word substitution, querying the language models and cal-\nculating Δf according to the deﬁnition. The numeric value\nis provided in Table IV. However, there still exists many\ntheoretical challenges in bounding the errors of the estimated\nL1 sensitivity, which will be a meaningful direction to pursue\nin the future.\nT ABLE IV\nESTIMA TED ℓ1-SENSITIVITY OF EACH LANGUAGE MODEL &T IME COST\nFOR QUERYING ONE TRAINING BAT C H\nName Estimated Δf Query Time (sec.)\nBert 81.82 0 .577\nTransformer-XL 17.09 3 .691\nXLNet 601.50 .248\nGPT 73.19 0 .231\nGPT -2 110.20 .206\nRoBERTa 4.15 0 .184\nXLM 219.40 .223\nErnie 2.0 28.20 1 .777\nPrivacy Preserving Mapping. We borrow the notion of a\nPrivacy Preserving Mapping (PPM) from [57] to denote a\nmapping D\nθ : Rd → Rd parameterized by θ, which is\ntrained to minimize the effectiveness of an imagined ad-\nversary A\nψ. Meanwhile, the PPM is required to follow the\nutility constraint: it can only distort the embeddings in a\nlimited radius around the original embedding in order to\nmaintain the utility, or otherwise a trivial yet perfect de-\nfense only needs to map all the embeddings to a constant\nvector. Formally, we propose to learn the privacy preserv-\ning mapping D\nθ by solving the following minimax game\nminθ maxψ 1\nn\n∑n\ni=1 Aψ(Dθ(zi),si),s.t. ∥Dθ(z)−z∥2 ≤ ϵ.I n\nother words, the active defense accesses the plaintexts{xi}n\ni=1,\nderives the training set {(zi,si)}n\ni=1 for an imagined white-\nbox adversary, and simulates the minimax game as we describe\nabove. As the defense at the user side usually has no access\nto the intelligent service at the cloud, the utility constraint is\nformulated as the upper bound on the2-norm distance between\nthe protected and original embeddings, which is similar to that\nin [57]. In practice,D\nθ is implemented as an encoder-decoder\nneural network and Aψ is implemented as an MLP .\nHowever, we notice an additional challenge brought by\nthe language model setting. Although previous PPM-based\ndefenses have studied several efﬁcient approaches to solve\nthe minimax game when z takes discrete values and D\nθ is\na combinatorial function [35], [57], our PPM is required to\nwork on the real-valued embeddings with D\nθ implemented\nas neural networks. By the best of our knowledge, there\nis no effective algorithm to exactly solve the constrained\noptimization problem above. As an alternative, we propose\nto reformulate the L2 constraint as a regularization term.\nIn detail, it writes min\nθ maxψ 1/n∑n\ni=1 Aψ(Dθ(zi),si)+\nλ∥Dθ(zi)−zi∥2, where the positive coefﬁcientλ is expected\nto control the privacy level of this active defense. Intuitively, a\nlarger λ corresponds to a stricter utility constraint and thus, a\n1329\nlower privacy level. To solve the unconstrained minimax game,\nwe use the simultaneous Gradient Descent algorithm, which\ntakes alternative gradient descent (ascent) steps on θ and ψ\n[26].\nSubspace Projection. Similar to the methodology in [14],\nour defense ﬁrst calculates the privacy subspace w.r.t. the\nsemantic meaning we do not want the adversary to distinguish.\nSpeciﬁcally in the setting of keyword inference attack, we ex-\npect the adversary is unable to distinguish whether a sentence\ncontains certain keyword or not. Therefore, we ﬁrst collect two\nsets of embeddingsD\n1 and D0 that respectively correspond to\nsentences with and without the target keyword. Then we com-\npute the privacy subspace as the linear space spanned by the\nﬁrst k orthonormal eigenvectors u1,...,u k of the following\nmatrix C = ∑\ni∈{0,1}\n∑\nz∈Di (z − μi)(z − μi)T/|Di| where\nμi = ∑\nz∈Di z/|Di|, i.e. the average sentence embedding in\nDi. In our evaluations, we consider the ratio β between the\ndimension of the privacy subspacek and the full dimension d\nof the sentence embedding as the parameter of the subspace\ndefense, i.e., β = k/d.\nNext, to remove the unwanted semantics from the embed-\nding (denoted as z), we simply project the embedding to\nthe subspace orthogonal to the privacy subspace, with the\nfollowing formulas: ˆz ← ∑k\ni=1(I −vivT\ni )z, ˆz ← ˆz/∥ˆz∥.\nEvaluation Details. For PPM defense, we implement the\nvirtual adversary Aψ as a 3-layer MLP with 200 sigmoid\nhidden units and the PPM Dθ as an MLP of the architecture\n(d − 200 − d) with ReLU activation. We train the virtual\nadversary and the PPM alternatively for 1 and 5 iterations,\nwhere the batch size is set as 64.\nD. Fingerprinting Algorithm\nWe propose the following ﬁngerprinting algorithm to relax\nAssumption 1, with100%accuracy on determining the speciﬁc\nmodel type. First, the adversary determines the candidate\nmodel types according to the embedding dimension. For ex-\nample, if d = 768, the candidate set includes GPT, GPT -2 and\nother three models. Next, the advervsary prepares an arbitrary\ncorpus and queries each language model for the embedddings.\nThen, the adversary trains an off-the-shelf classiﬁer with the\nembedding as input and the model type as label. Finally, when\nthe adversary gets a set of embeddings as victims, he/she ﬁrst\nuses the language model classiﬁer at hand to determine the\nmodel type and conducts the downstream attacks as introduced\nin previous sections.\nFig. 9. Clustering phenomenon observed on (a) embeddings from5 784-dim.\nlanguage models and (b) their MLP hidden representations of1000 randomly-\nsampled sentences on Medical.\nT ABLE V\nINFERRED TOP -10 POSSIBLE KEYWORDS IN EACH SAMPLE FOR\nREASSEMBLING THE SEMANTICS .\nSample #1. Destruction of malignant growth (1.1 to 2.0 centimeters) of trunk, arms, or legs\narm, trunk, malignant, repair, venous, skin, veins, lower,removal, artery\nSample #2. Application of ultraviolet light to skin\nvenous, veins, centimeters, radiation, older, guidance, arm, removal, arterial, injection\nSample #3. Removal of malignant growth (1.1 to 2.0 centimeters) of the trunk, arms, or legs\narm, trunk, malignant, repair, venous, veins, artery,removal, tissue, insertion\nSample #4. Removal of up to and including 15 skin tags\nveins, centimeters, tissue, removal, insertion, arms, spinal, arterial, skin,l e g s\nSample #5. Biopsy of each additional growth of skin and/or tissue\nlower, venous, veins, biopsy, tissue, insertion, centimeters, artery, endoscope, ultrasound\nTo evaluate our ﬁngerprinting algorithm, we implement the\nlanguage model classiﬁer as a (784-200-5) MLP , use the Y elp-\nFood corpus of2000 sentences for training and a subset of the\nMedical corpus that consists of 1000 sentences for testing.\nStrikingly, we ﬁnd the classiﬁer achieve 100% accuracy.\nTo better understand the phenomenon, we plot the original\nembeddings in the test set and their hidden representations\nat the last layer of the MLP in Fig. 9, where different\ncolors implies different language models. As we can see, the\nembeddings from different language models distribute in rather\ndivergent ways. After MLP’s nonlinear mapping, the embed-\ndings directly collapse to separated clusters according to their\ncorresponding model type. To the best of our knowledge, we\nare the ﬁrst to discover and report this interesting phenomenon.\nE. Semantic Reassembling with Keyword Inference Attack\nExperimental Settings. Following the description in Sec-\ntion VIII, we ﬁrst select 50 medical-related words to form\nthe candidate keyword set and train an DANN attack model\nfor each keyword with the same conﬁgurations in our original\nwork. The DANN attack accuracy is about 77% after being\naveraged on the50words. Then, we randomly select5samples\nfrom the test set of the medical case and use each DANN\nmodel to output the probability of the occurrence of the\ncorresponding keyword. We list the inferred Top-10 keywords\nof each sentence in Table V.\nResults & Analysis. As we can see from Table V, the\nattacker can actually reassemble the basic semantic meaning\nof the original text with such a procedure. For example, in\nSample #3, when the adversary knows arm, trunk, malignant\nas the Top-3 most possible keywords, then he/she can probably\ninfer the semantic meaning of the original description is related\nwith malignant growth at the arms or trunk. Compared with\nthe plain text of Sample #3, the inferred meaning is quite\nconsistent with the original meaning, despite some minor\ndetails left out. Interestingly, we also ﬁnd, although DANN\nmay predict the occurrence of certain keywords with error,\nthe erroneous prediction may also contribute to the semantic\nreconstruction. For example, in Sample #2, the adversary fails\nto predict the occurrence of ultraviolet since this word is\nnot in the adversary’s candidate set. However, due to the\nsemantic similarity between ultraviolet and radiation\n4, the\nDANN attack model forradiation predicts the high probability\n40.942 in cosine similarity of Bert word embeddings.\n1330\nFig. 10. The training loss of LSTM decoder and attack accuracy of decoder-\nbased attack in the ﬁrst 5 epochs.\nof the occurrence of the word radiation, which, despite the\ninexactness, helps the adversary successfully guess that the\nhidden semantic is about the radiation-related procedure, i.e.,\nthe application of ultraviolet light.\nF . Keyword Inference Attack with Standard Decoder Section\nExperimental Settings. We use the standard decoder mod-\nule [68], a one-layer bidirectional LSTM to implement the\ndecoder. The vocabulary of the LSTM is initialized the same\nas the target language model. For most of them (excluding\nXLNet and ERNIE), the vocabulary is publicly accessible. We\ntherefore implement the decoder-based attack on the rest 6\ntargets.\nFor training, we input the decoder both the embedding\nand the corresponding sentence: the embedding is input as\nthe initial hidden state of the LSTM forwarding phase, while\nthe sentence supervises the generated tokens in a teacher-\nforcing way [25]. During the evaluation phase, we input the\nvictim embedding as the initial hidden state to the LSTM\ndecoder and decode the tokens greedily. For background on\ntraining and evaluating such an LSTM module for generating\nsentence conditionally, please refer to e.g., [68]. To conduct the\nkeyword inference attack, we suppose the adversary directly\ntests whether the keyword is contained in the generated\nsentence. We conduct the decoder-based keyword inference\nattack on Medical in the white-box setting, with exactly the\nsame conﬁgurations of the dataset. Fig. 10 reports the training\nloss and the keyword inference attack accuracy in the ﬁrst 5\nepochs. We omit the results for Transformer-XL because the\ndecoder cannot be trained on a 11G GPU due to the large\nvocabulary containing over 220,000 tokens.\nResults & Analysis. As we can see from Fig. 10, for the\nLSTM decoder on each language model we experiment with,\nthe training loss decreases to almost 0 in the ﬁrst several\nepochs. In the console logs, we correspondingly observe\nthat, when the loss is close to 0, the decoded sentences\nin the training set is almost identical to the original ones.\nHowever, when applied to decode from the embedding without\nteacher-forcing, the decoder is observed to fail to decode any\nmeaningful sentences, always giving a sequence of repetition\nof certain random word. As a result, none of the decoder-based\nattacks work out better than a random guesser.\nG. Experimental Environments\nAll the experiments were implemented with PyTorch [51],\nwhich is an open-source software framework for numeric com-\nputation and deep learning. We used the pretrained language\nT ABLE VI\nTHROUGHPUT FOR TRAINING A TT ACKS IN EACH CASE (ITER /SEC .)\nCitizen Genome Airline/Medical\nY ear Month Date SVM MLP DANN\n187.9 228 .2 213 .7 311 .2 7209 .6 352 .3 316 .8\nmodels implemented by PaddlePaddle5 (for Ernie 2.0) and by\nHuggingFace [74] (for other seven models). We deployed the\nlanguage models on a Linux server running Ubuntu 16.04, one\nAMD Ryzen Threadripper 2990WX 32-core processor and 2\nNVIDIA GTX RTX2080 GPUs. We conducted our attacks\nand defenses on a Linux PC running Ubuntu 16.04, one Intel\nCore i7 processor and 1 NVIDIA GTX 1070 GPU, querying\nthe server via local network. We report the time for quering\none mini-batch of training data in Table IV.\nH. Other Omitted Statistics\nT ABLE VII\nBASIC INFORMA TION OF THE THREE V ARIANTS OF GPT -2ARCHITECTURE\nWE HA VE USED FOR ABLA TION STUDIES .\nName Dimension # of Parameters\nGPT -2 768 1 .2×108\nGPT -2-Medium 1024 3 .5×108\nGPT -2-Large 1280 7 .7×108\nT ABLE VIII\nST A TISTICS OFTEST SAMPLES FOR EACH KEYWORD ON AIRLINE &\nMEDICAL\nAirline\nHong Kong London Toronto Paris Rome\n808 2656 1320 948 736\nSydney Dubai Bangkok Singapore Frankfurt\n1434 802 1260 1264 586\nMedical\nleg hand spine chest ankle\n19804 3700 6222 3172 1252\nhead hip arm face shoulder\n4988 2612 18600 3938 2592\nI. Learning Algorithm for DANN Attack\n5https://github.com/PaddlePaddle/ERNIE\n1331"
}