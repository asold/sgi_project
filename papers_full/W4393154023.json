{
  "title": "MedSegDiff-V2: Diffusion-Based Medical Image Segmentation with Transformer",
  "url": "https://openalex.org/W4393154023",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2119285445",
      "name": "Junde Wu",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A1972758884",
      "name": "Wei Ji",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2230394843",
      "name": "Huazhu Fu",
      "affiliations": [
        "Institute of High Performance Computing"
      ]
    },
    {
      "id": "https://openalex.org/A2107415212",
      "name": "Min Xu",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2108763793",
      "name": "Yueming Jin",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2101245483",
      "name": "Yanwu Xu",
      "affiliations": [
        "Singapore Eye Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2119285445",
      "name": "Junde Wu",
      "affiliations": [
        "University of Oxford",
        "Mohamed bin Zayed University of Artificial Intelligence",
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2230394843",
      "name": "Huazhu Fu",
      "affiliations": [
        "Institute of High Performance Computing"
      ]
    },
    {
      "id": "https://openalex.org/A2107415212",
      "name": "Min Xu",
      "affiliations": [
        "Star Technology and Research (United States)",
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2101245483",
      "name": "Yanwu Xu",
      "affiliations": [
        "Singapore Eye Research Institute",
        "Star Technology and Research (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3160284783",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3111852437",
    "https://openalex.org/W2998354581",
    "https://openalex.org/W3165252795",
    "https://openalex.org/W3137561054",
    "https://openalex.org/W6779823529",
    "https://openalex.org/W3182906273",
    "https://openalex.org/W4283080861",
    "https://openalex.org/W2807861409",
    "https://openalex.org/W3168491317",
    "https://openalex.org/W3011537551",
    "https://openalex.org/W6795475546",
    "https://openalex.org/W6657413401",
    "https://openalex.org/W4364387778",
    "https://openalex.org/W4226317937",
    "https://openalex.org/W3036234074",
    "https://openalex.org/W3215327553",
    "https://openalex.org/W3205816419",
    "https://openalex.org/W2954594726",
    "https://openalex.org/W3135385363",
    "https://openalex.org/W2148347694",
    "https://openalex.org/W6753412334",
    "https://openalex.org/W4200015473",
    "https://openalex.org/W4283383251",
    "https://openalex.org/W4282813766",
    "https://openalex.org/W3044360657",
    "https://openalex.org/W2935464137",
    "https://openalex.org/W6797602710",
    "https://openalex.org/W4200631545",
    "https://openalex.org/W4308023490",
    "https://openalex.org/W4226000809",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W4386057777",
    "https://openalex.org/W4367189325",
    "https://openalex.org/W2929753309",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W4286698628",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W4281485151",
    "https://openalex.org/W4283072464",
    "https://openalex.org/W4298183756",
    "https://openalex.org/W4297791576",
    "https://openalex.org/W4288034826",
    "https://openalex.org/W2027685755",
    "https://openalex.org/W3109585842",
    "https://openalex.org/W4293433089",
    "https://openalex.org/W4386352882",
    "https://openalex.org/W4221160406",
    "https://openalex.org/W4287756134",
    "https://openalex.org/W3096193903",
    "https://openalex.org/W4312428231",
    "https://openalex.org/W1986649315",
    "https://openalex.org/W3035367255",
    "https://openalex.org/W2970642280",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W4225565617",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W2954913281",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W4287100534"
  ],
  "abstract": "The Diffusion Probabilistic Model (DPM) has recently gained popularity in the field of computer vision, thanks to its image generation applications, such as Imagen, Latent Diffusion Models, and Stable Diffusion, which have demonstrated impressive capabilities and sparked much discussion within the community. Recent investigations have further unveiled the utility of DPM in the domain of medical image analysis, as underscored by the commendable performance exhibited by the medical image segmentation model across various tasks. Although these models were originally underpinned by a UNet architecture, there exists a potential avenue for enhancing their performance through the integration of vision transformer mechanisms. However, we discovered that simply combining these two models resulted in subpar performance. To effectively integrate these two cutting-edge techniques for the Medical image segmentation, we propose a novel Transformer-based Diffusion framework, called MedSegDiff-V2. We verify its effectiveness on 20 medical image segmentation tasks with different image modalities. Through comprehensive evaluation, our approach demonstrates superiority over prior state-of-the-art (SOTA) methodologies. Code is released at https://github.com/KidsWithTokens/MedSegDiff.",
  "full_text": "MedSegDiff-V2: Diffusion-Based Medical Image Segmentation with Transformer\nJunde Wu1,2,3,4, Wei Ji5, Huazhu Fu6, Min Xu*7,3, Yueming Jin2, Yanwu Xu*8\n1University of Oxford\n2National University of Singapore\n3Mohamed bin Zayed University of Artificial Intelligence\n4Kids with Tokens\n5University of Alberta\n6Institute of High Performance Computing, A*STAR\n7Carnegie Mellon University\n8Singapore Eye Research Institute\njundewu@ieee.org, ywxu@ieee.org, xumin100@gmail.com\nAbstract\nThe Diffusion Probabilistic Model (DPM) has recently gained\npopularity in the field of computer vision, thanks to its im-\nage generation applications, such as Imagen, Latent Diffu-\nsion Models, and Stable Diffusion, which have demonstrated\nimpressive capabilities and sparked much discussion within\nthe community. Recent investigations have further unveiled\nthe utility of DPM in the domain of medical image analy-\nsis, as underscored by the commendable performance exhib-\nited by the medical image segmentation model across various\ntasks. Although these models were originally underpinned by\na UNet architecture, there exists a potential avenue for en-\nhancing their performance through the integration of vision\ntransformer mechanisms. However, we discovered that simply\ncombining these two models resulted in subpar performance.\nTo effectively integrate these two cutting-edge techniques\nfor the Medical image segmentation, we propose a novel\nTransformer-based Diffusion framework, called MedSegDiff-\nV2. We verify its effectiveness on 20 medical image segmen-\ntation tasks with different image modalities. Through com-\nprehensive evaluation, our approach demonstrates superiority\nover prior state-of-the-art (SOTA) methodologies. Code is\nreleased at https://github.com/KidsWithTokens/MedSegDiff\nIntroduction\nMedical image segmentation is to divide a medical image\ninto distinct regions of interest. It is a crucial step in many\nmedical applications, such as diagnosis (Li et al. 2022; Han\net al. 2022) and image-guided surgery. In recent years, there\nhas been a growing interest in automated segmentation meth-\nods, as they have the potential to improve the consistency and\naccuracy of results. With the advancement of deep learning\ntechniques, several studies have successfully applied neural\nnetwork-based models, including classical convolutional neu-\nral networks (CNNs) (Ji et al. 2021; Wu et al. 2022c, 2020)\nand the recently popular vision transformers (ViTs)(Wu et al.\n2022d,b, 2023), to medical image segmentation tasks.\nVery recently, the Diffusion Probabilistic Model\n(DPM)(Ho, Jain, and Abbeel 2020) has gained popularity as\na powerful class of generative models, capable of generating\n*co-corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nhigh-quality and diverse images(Ramesh et al. 2022; Saharia\net al. 2022; Rombach et al. 2022). Inspired by its success,\nmany researches have applied DPM in the field of medical\nimage segmentation(Wu et al. 2022e; Wolleb et al. 2021;\nKim, Oh, and Ye 2022; Guo et al. 2022; Rahman et al. 2023).\nMany of them reported new SOTA on several benchmarks by\nusing the DPM. The remarkable performance of this model\nstems from its inherent stochastic sampling process(Wu\net al. 2022e; Rahman et al. 2023). DPM has the capability\nto generate different segmentation predictions by running\nmultiple times. The diversity among these samples directly\ncaptures the uncertainty associated with targets in medical\nimages, where organs or lesions commonly have ambiguous\nboundaries. However, it is worth noting that all these\nmethods rely on classical UNet backbones. In comparison\nto the increasingly popular vision transformers, classical\nUNet models compromise on segmentation quality, which\ncan lead to the generation of divergent yet incorrect masks\nin ensemble, ultimately introducing noise that permanently\nhampers the performance. A natural next step is to combine\nthe transformer-based UNet, such as TransUNet(Chen et al.\n2021), with DPM. However, we found that implementing it\nin a straightforward manner resulted in subpar performance.\nOne issue is that the transformer-abstracted conditional\nfeature is not compatible with the feature of the diffusion\nbackbone. The transformer is able to learn deep semantic\nfeatures from the raw image, whereas the diffusion backbone\nabstracts features from a corrupted and noisy mask, making\nfeature fusion more challenging. Additionally, the dynamic\nand global nature of the transformer makes it more sensitive\nthan CNNs (Naseer et al. 2021). Thus, the adaptive condition\nstrategy used in previous diffusion-based methods(Wu et al.\n2022e) will cause large variance in the transformer setting.\nThis leads more ensemble and converge difficulties.\nTo overcome these challenges, we have designed a novel\nTransformer-based Diffusion framework for the Medical im-\nage segmentation, called MedSegDiff-V2. We employ two\nconditioning techniques over the backbone with the raw im-\nage in the diffusion process. One is the Anchor Condition,\nwhich integrates the conditional segmentation features into\nthe diffusion model encoder to reduce the diffusion variance.\nWe design a novel Uncertain Spatial Attention (U-SA) mech-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6030\nFigure 1: An illustration of MedSegDiff-V2, which starts from (a) an overview of the pipeline, and continues with zoomed-in\ndiagrams of individual Models, including (b) SS-Former, and (c) NBP-Filter.\nanism for the integration, which relaxes the conditional seg-\nmentation feature with more uncertainty, thus providing the\ndiffusion more flexibility to further calibrate the predictions.\nThe other is the Semantic Condition that integrates the condi-\ntional embedding into the diffusion embedding. To effectively\nbridge the gap between these two embedding, we propose\na novel transformer mechanism called the Spectrum-Space\nTransformer (SS-Former) for the embedding integration. SS-\nFormer is a cross-attention chain in frequency domain, with\na timestep-adaptive Neural Band-pass Filter (NBP-Filter) to\nalign the noise and semantic features each time.\nIn brief, the contributions of this paper are:\n• We are the first to integrate transformer into a diffusion-\nbased model for general medical image segmentation.\n• We propose an Anchor Condition with U-SA to mitigate\nthe diffusion variance.\n• We propose Semantic Condition with SS-Former to model\nthe segmentation noise and semantic feature interaction.\n• We achieve SOTA performance on 20 organ segmentation\ntasks including 5 image modalities.\nRelated Work\nMedical Segmentation with Transformers\nPrevious studies have highlighted the potential of transformer-\nbased models to achieve SOTA results in medical image seg-\nmentation. One notable example is TransUNet(Chen et al.\n2021), which combined the transformer with UNet as a bot-\ntleneck feature encoder. Since then, several works have pro-\nposed incorporating cutting-edge transformer techniques into\nthe backbone of medical image segmentation models, in-\ncluding Swin-UNet(Cao et al. 2022), Swin-UNetr(Tang et al.\n2022), and DS-TransUNet(Lin et al. 2022). As recently UNet\nbased diffusion-based segmentation models have recently\nemerged as achieving new SOTA in medical image segmenta-\ntion, it is worthwhile to explore ways to integrate recognized\ntransformer architectures into this powerful new backbone.\nDiffusion Model for Medical Segmentation\nDiffusion models have recently demonstrated significant po-\ntential in various segmentation tasks, including medical im-\nages (Armato III et al. 2011; Caron et al. 2021; Cao et al.\n2022; Chen, Ma, and Zheng 2019). In fact, these models\nleverage a stochastic sampling process to generate an implicit\nensemble of segmentations, leading to enhanced segmenta-\ntion performance (Zhai et al. 2022). However, without ef-\nfective control of diversity, the ensemble often struggles to\nconverge , resulting in multiple time-consuming sampling\niterations. Moreover, these divergent samples not only fail to\nmeet the desired target but also introduce noise that hampers\nsegmentation quality. Therefore, it is crucial to improve the\nsample accuracy with each sampling iteration.\nMethod\nDiffusion Process of MedSegDiff-V2\nWe have designed our model based on the diffusion model\nmentioned in (Ho, Jain, and Abbeel 2020). Diffusion models\nare generative models that consist of two stages: a forward\ndiffusion stage and a reverse diffusion stage. In the forward\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6031\nprocess, Gaussian noise is gradually added to the segmen-\ntation label x0 through a series of steps T. In the reverse\nprocess, a neural network is trained to recover the original\ndata by reversing the noise addition process. This can be\nmathematically represented as follows:\npθ(x0:T−1|xT ) =\nTY\nt=1\npθ(xt−1|xt), (1)\nwhere θ represents the parameters of the reverse process.\nStarting from a Gaussian noise distribution, pθ(xT ) =\nN(xT ; 0, In×n), where I is the raw image, the reverse pro-\ncess transforms the latent variable distribution pθ(xT ) to the\ndata distribution pθ(x0). To maintain symmetry with the for-\nward process, the reverse process recovers the noisy image\nstep by step, ultimately obtaining the final clear segmentation.\nFollowing the standard implementation of DPM, we utilize\nan encoder-decoder network for the learning. To achieve\nsegmentation, we condition the step estimation function ϵ on\nthe prior information from the raw image. This conditioning\ncan be expressed as:\nϵθ(xt, I, t) =D(T ransF(EI\nt , Ex\nt ), t), (2)\nHere, T ransFdenotes the transformer based attention mech-\nanism. EI\nt represents the conditional feature embedding,\nwhich, in our case, corresponds to the embedding of the\nraw image. Ex\nt represents the feature embedding of the seg-\nmentation map for the current step. These two components\nare incorporated together by transformer and passed through\na UNet decoder D for reconstruction. The step index t is\nintegrated with the combined embedding and decoder fea-\ntures, and each step index is embedded using a shared learned\nlook-up table, following the approach described in (Ho, Jain,\nand Abbeel 2020).\nOverall Architecture\nThe overall flow of MedSegDiff-V2 is shown in 1. To intro-\nduce the process, consider a single step t of the diffusion\nprocess. The noisy mask xt is first inputted to a UNet, called\nthe Diffusion Model. Diffusion Model is conditioned by the\nsegmentation features extracted from the raw images through\nanother standard UNet, called the Condition Model. Two\ndifferent conditioning manners are applied to the Diffusion\nModel: Anchor Condition and Semantic Condition. Follow-\ning the flow of the input, the Anchor Condition is first im-\nposed on the encoder of the Diffusion Model. It integrates the\nanchor segmentation features, which are the decoded segmen-\ntation features of the Condition Model, into the encoded fea-\ntures of the Diffusion Model. This allows the diffusion model\nto be initialized by a rough but static reference, which helps\nto reduce the diffusion variances. The Semantic Condition\nis then imposed on the embedding of the Diffusion Model,\nwhich integrates the semantic segmentation embedding of the\nCondition Model into the embedding of the Diffusion Model.\nThis conditional integration is implemented by SS-Former,\nwhich bridges the gap between the noise and semantic em-\nbedding, and abstracts a stronger representation with the\nadvantage of the global and dynamic nature of transformer.\nMedSegDiff-V2 is trained using a standard noise predic-\ntion loss Ln following DPM(Ho, Jain, and Abbeel 2020) and\nan anchor loss Lanc supervising the Condition Model. Lanc\nis a combination of soft dice loss Ldice and cross-entropy\nloss Lce. Specifically, the total loss function is represented\nas:\nLt\ntotal = Lt\nn + (t ≡ 0 (mod α))(Ldice + βLce) (3)\nwhere t ≡ 0 (mod α) control the times of supervision over\nCondition Model through hyper-parameter α, cross-entropy\nloss is weighted by hyper-parameter β , which are set as 5\nand 10 respectively.\nAnchor Condition with U-SA\nWithout the inductive bias of convolution layer, transformer\nblocks have stronger representation capability but are also\nmore sensitive to the input variance when training data is\nlimited(Naseer et al. 2021). Directly adding the transformer\nblock to the Diffusion Model will cause the large variance on\neach time outputs. To overcome this negative effect, we adapt\nthe structure of MedSegDiff(Wu et al. 2022e) and introduce\nthe Anchor Condition operation to the Diffusion Model.\nAnchor Condition provides a rough anchor feature from\nthe Condition Model and integrates it into the Diffusion\nModel. This provides the Diffusion Model with a correct\nrange for predictions while also allowing it to further refine\nthe results. Specifically, we integrate the decoded segmenta-\ntion features of the Condition Model into the encoder features\nof the Diffusion Model. We propose U-SA mechanism for\nthe feature fusion to represent the uncertainty nature of the\ngiven conditional features. Formally, consider we integrate\nthe last conditional feature f−1\nc into the first diffusion feature\nf0\nd . U-SA can be expressed as:\nfanc = Max(f−1\nc ∗ kGauss, f−1\nc ), (4)\nf\n′0\nd = Sigmoid(fanc ∗ kConv1×1 ) · f0\nd + f0\nd , (5)\nwhere ∗ denotes slide-window kernel manipulation, · de-\nnotes general element-wise manipulation. In the equation,\nwe first apply a learnable Gaussian kernel kG over f−1\nc to\nsmooth the activation, as f−1\nc serves as an anchor but may\nnot be completely accurate. We then select the maximum\nvalue between the smoothed map and the original feature\nmap to preserve the most relevant information, resulting in a\nsmoothed anchor feature fanc. Then we integrate fanc into\nf0\nd to obtain an enhanced feature f\n′0\nd . Specifically, we first\napply a 1 × 1 convolution k1×1conv to reduce the anchor fea-\nture channels to 1 and multiply it with f0\nd after the Sigmoid\nactivation, then add it to each channel of f0\nd , similar to the\nimplementation of spatial attention(Woo et al. 2018).\nSemantic Condition with SS-Former\nThe Diffusion Model predicts redundant noise from a noisy\nmask input, leading to a domain gap between its embedding\nand the conditional segmentation semantic embedding. This\ndivergence compromises performance when using matrix ma-\nnipulations, such as in a stranded transformer. To address\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6032\nthis challenge, we propose a novel Spectrum-Space Trans-\nformer (SS-Former). Our key idea is to learn the interaction\nof condition semantic feature and diffusion noise feature in\nthe frequency domain. We use a filter, called the Neural Band-\npass Filter (NBP-Filter) to align them to a unified range of\nfrequencies, i.e., spectrum. NBP-Filter learns to pass a spe-\ncific spectrum while constraining the others. We learn this\nspectrum in a self-adaptive way to the diffusion time steps,\nas the noise-level (frequency range) is specific for each step.\nThis allows for the mixup of features based on the frequency-\naffinity and also align them based on the diffusion steps.\nA bird-eye view of SS-Former is shown in the 1 (b), which\nis composed of N blocks that share the same architecture.\nWe set N = 4in the paper. Each block consists of two cross-\nattention-like modules. The first encodes the diffusion noise\nembedding into the condition semantic embedding, and the\nnext symmetric module encodes the last semantic embedding\ninto the diffusion noise embedding. This allows the model to\nlearn the interaction between noise and semantic features and\nachieve a stronger representation. Formally, consider\nc0 is the\ndeepest feature embedding of Condition Model and e is that\nof Diffusion Model. We first transfer c0 and e to the Fourier\nspace, denoted as F(c0) and F(e), respectively. Note that the\nfeature maps are all patchlized and liner projected in accor-\ndance with the standard vision transformer method. Then we\ncompute an affinity weight map over Fourier space taking e\nas the query and c0 as the key, which can be represented by\nM = (F(c0)Wq)(F(e)Wk)T , where Wq and Wk are the\nlearnable query and key weights in Fourier space.\nWe then apply a NBP-Filter to align the representation of\nfrequency. We note that each point in M now represents a\nparticular frequency, and since we need to control a continu-\nous range of frequencies, it is intuitive to establish a smooth\nprojection from the feature map position to the frequency\nmagnitude. To accomplish this, we use a neural network to\nlearn a weight map from a coordinate map. By doing so,\ninductive bias of the network will facilitate the learning of a\nsmooth projection, as similar inputs will naturally produce\nsimilar outputs(Sitzmann et al. 2020; Wu and Fu 2019). This\nidea is widely used in 3D vision tasks and is known as Neural\nRadiance Fields (NeRF)(Mildenhall et al. 2020). But dif-\nferent from the original NeRF, we further condition it with\ntime-step information. Specifically, the network takes a coor-\ndinate map as input and produces an attention map to serve\nas the filter, both of which have the same size M. We im-\nplement it using a simple stack of convolutional blocks with\nintermediate layer normalization. To condition the network\nwith timestep information, we scale and shift the normalized\nfeatures with the timestep embedding of the diffusion model.\nWe use two MLP layers to project the current timestep embed-\nding to two values representing the mean and variance, which\nare used for scaling and shifting, respectively. We stack a to-\ntal of R = 6such blocks and a Sigmoid function to produce\nthe final filter. Finally, the filter is element-wise multiplied\nwith the affinity map M in the pipeline. NBP-Filter is trained\nin an end-to-end manner with the whole pipeline.\nThe filtered affinity map M′ is then transferred back\nto Euclidean space using inverse fast Fourier transform\n(IFFT) and applied to condition features in value: f =\nF−1(M′)(c0wv), where Wv is the learnable value weights.\nWe also use a MLP to further refine the attention result, ob-\ntaining the final feature ˜c0. The following attention module is\nsymmetric to the first one, but using the combined feature ˜c0\nas the query and noise embedding e as the key and value, in\norder to transform the segmentation features to the noise do-\nmain. The transformed feature c1 will serve as the condition\nembedding for the next block.\nExperiments\nDataset\nWe conduct the experiments on five different datasets. Two\ndatasets are used to verify the general segmentation perfor-\nmance, which are public AMOS2022(Ji et al. 2022) dataset\nwith sixteen anatomies and public BTCV(Fang and Yan 2020)\ndataset with twelve anatomies annotated for abdominal multi-\norgan segmentation. The other four public datasets REFUGE-\n2 (Fang et al. 2022), BraTs-2021 dataset (Baid et al. 2021),\nISIC dataset(Milton 2019) and TNMIX dataset (Pedraza et al.\n2015) are used to verify the performance on multi-modal\nimages, which are the optic-cup segmentation from fundus\nimages, the brain tumor segmentation from MRI images, and\nthe thyroid nodule segmentation from ultrasound images.\nImplementation Details\nAll experiments were conducted using the PyTorch platform\nand trained/tested on 4 NVIDIA A100 GPUs. All images\nwere uniformly resized to a resolution of 256 ×256 pixels.\nThe networks were trained in an end-to-end manner using\nthe AdamW(Loshchilov and Hutter 2017) optimizer with a\nbatch size of 32. The initial learning rate was set to 1 ×10−4.\nWe employed 100 diffusion steps for the inference. We run\nthe model 10 times for the ensemble, which is much fewer\nthan the 25 times in MedSegDiff(Wu et al. 2022e). Then\nwe use STAPLE algorithm(Warfield, Zou, and Wells 2004)\nto fuse the different samples. We evaluate the segmentation\nperformance by Dice score, IoU, HD95 metrics.\nMain Results\nComparing with SOTA on Abdominal Multi-organ Seg-\nmentation To verify the general segmentation performance,\nwe compare MedSegDiff-V2 with SOTA methods on multi-\norgan segmentation dataset AMOS and BTCV . The Dice\nscore are shown in 2 and 3 respectively. In the table, we\ncompare with the segmentation methods which are widely-\nused and well-recognized in the community, including\nthe CNN-based method nnUNet(Isensee et al. 2021), the\ntransformer-based methods TransUNet(Chen et al. 2021),\nUNetr(Hatamizadeh et al. 2022), Swin-UNetr(Jiang et al.\n2022) and the diffusion based method EnsDiff (Wolleb et al.\n2021), SegDiff(Amit et al. 2021), MedSegDiff (Wu et al.\n2022e). We also compare with a simple combination of diffu-\nsion and transformer model. We replace the UNet model in\nMedSegDiff to TransUNet and denoted it as ’MedSegDiff +\nTransUNet’ in the table.\nAs seen in 2 and 3, advanced network architectures and\nsophisticated designs are crucial for achieving good perfor-\nmance. Considering the architecture, transformer-based mod-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6033\nFigure 2: The visual comparison with SOTA segmentation models on BTCV . Red box is the zoomed-in ground-truth. Green box\nis the zoomed-in prediction.\nels such as Swin-UNetr outperform the carefully designed\nCNN-based model, nnUNet. The diffusion-based model Med-\nSegDiff again outperforms the transformer-based models on\nmost of the organs. However, network architecture alone is\nnot the determining factor for performance. For example, the\nwell-designed CNN-based model nnUNet considerably out-\nperforms the transformer-based model TransUNet and UNetr\nin the table. This is also true for diffusion-based models. We\ncan see that a straightforward adoption of the diffusion model\nfor medical image segmentation, i.e., EnsDiff and SegDiff,\nperform worse than UNetr and Swin-UNetr. A simple combi-\nnation of transformer and diffusion model, i.e., MedSegDiff\n+ TransUNet, obtains even worse performance than the stan-\ndard MedSegDiff. By introducing Anchor Condition and\nSS-Former, MedSegDiff-V2 overcomes these challenges and\nshows superior performance. We also present a qualitative\ncomparison in 2. It can be observed that MedSegDiff-V2\npredicts segmentation maps with more precise details, even\nin low-contrast or ambiguous areas.\nComparing with SOTA on Multi-modality Images We\nalso compare MedSegDiff-V2 to SOTA segmentation meth-\nods proposed for three specific tasks with different image\nmodalities. The results are presented in 1. In the table, Re-\nsUnet(Yu et al. 2019) and BEAL(Wang et al. 2019) are pro-\nposed for optic cup segmentation, TransBTS(Wang et al.\n2021b) and SwinBTS(Wang et al. 2021b) are proposed for\nbrain tumor segmentation, MTSeg(Gong et al. 2021) and\nUltraUNet(Chu, Zheng, and Zhou 2021) are proposed for\nthyroid nodule segmentation, and FAT-Net(Wu et al. 2022a)\nand BAT(Wang et al. 2021a) are proposed for skin lesion\nsegmentation. From the table, we can see that MedSegDiff-\nV2 surpasses all other methods in five different tasks, high-\nlighting its remarkable generalization capability across var-\nious medical segmentation tasks and image modalities. In\ncomparison to the UNet-based MedSegDiff, MedSegDiff-V2\nexhibits improvements of 2.0% on Optic-Cup, 1.9% on Brain-\nTumor, and 3.9% on Thyroid Nodule in terms of the Dice\nscore, underscoring the effectiveness of its transformer-based\nbackbone. Furthermore, when compared to MedSegDiff plus\nTransUNet, MedSegDiff-V2 outperforms it by an even larger\nmargin, clearly demonstrating the efficacy of the proposed\nU-SA and SS-Former in enhancing performance.\nAblation Study We conducted a comprehensive ablation\nstudy to verify the effectiveness of the proposed modules. The\nresults are shown in 4, where Anc.Cond. and Sem.Cond. de-\nnote Anchor Condition and Semantic Condition, respectively.\nAs shown in the table, Anc.Cond. significantly improves\nthe vanilla diffusion model, with the proposed U-SA out-\nperforming the previous Spatial Attention on all datasets. In\nSem.Cond., using SS-Former alone provides only marginal\nimprovement, but combining it with the NBP-Filter results in\na significant improvement, demonstrating the effectiveness\nof the proposed SS-Former design.\nAnalysis and Discussion\nImplicit Ensemble Effect As confirmed by numerous pre-\nvious studies (Wolleb et al. 2021; Wu et al. 2022e; Amit et al.\n2021), the implicit ensemble of multiple sampling runs plays\na crucial role in diffusion-based methods. In diffusion model\ncontext, implicit ensemble refers to combining predictions\nfrom multiple samplings of a single diffusion model, rather\nthan fusing predictions from different models. In this study,\nwe evaluate the ensemble performance of various diffusion-\nbased medical segmentation models, as shown in 3. The\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6034\nREFUGE2-Disc REFUGE2-Cup BraTs TNMIX ISIC\nDice IoU Dice IoU Dice IoU HD95 Dice IoU Dice IoU\nOptic\nDisc/Cup\nResUNet 92.9 85.5 80.1 72.3 78.4 71.3 18.71 78.3 70.7 87.1 78.2\nBEAL 93.7 86.1 83.5 74.1 78.8 71.7 18.53 78.6 71.6 86.6 78.0\nBrain\nTumor\nTransBTS 94.1 87.2 85.4 75.7 87.6 78.44 12.44 83.8 75.5 88.1 80.6\nSwinBTS 95.2 87.7 85.7 75.9 88.7 81.2 10.03 84.5 76.1 89.8 82.4\nThyroid\nNodule\nMTSeg 90.3 83.6 82.3 73.1 82.2 74.5 15.74 82.3 75.2 87.5 79.7\nUltraUNet 91.5 82.8 83.1 73.78 84.5 76.3 14.03 84.5 76.2 89.0 81.8\nSkin\nLesion\nFAT-Net 91.8 84.8 80.9 71.5 79.2 72.8 17.35 80.8 73.4 90.7 83.9\nBAT 92.3 85.8 82.0 73.2 79.6 73.5 15.49 81.7 74.2 91.2 84.3\nGeneral\nMed Seg\nnnUNet 94.7 87.3 84.9 75.1 88.5 80.6 11.20 84.2 76.2 90.8 83.6\nTransUNet 95.0 87.7 85.6 75.9 86.6 79.0 13.74 83.5 75.1 89.4 82.2\nUNetr 94.9 87.5 83.2 73.3 87.3 80.6 12.81 81.7 73.5 89.7 82.8\nSwin-UNetr 95.3 87.9 84.3 74.5 88.4 81.8 11.36 83.5 74.8 90.2 83.1\nDiffusion\nBased\nEnsemDiff 94.3 87.8 84.2 74.4 88.7 80.9 10.85 83.9 75.3 88.2 80.7\nSegDiff 92.6 85.2 82.5 71.9 85.7 77.0 14.31 81.9 74.8 87.3 79.4\nMedsegDiff 95.1 87.6 85.9 76.2 88.9 81.2 10.41 84.8 76.4 91.3 84.1\nMedsegDiff+TransUNet 91.8 84.5 82.1 72.6 86.1 78.0 13.88 79.2 71.4 84.6 75.5\nProposed MedSegDiff-V2 96.7 88.9 87.9 80.3 90.8 83.4 7.53 88.7 81.5 93.2 85.3\nTable 1: The comparison of MedSegDiff-V2 with SOTA segmentation methods on different image modalities. The grey\nbackground denotes the methods are proposed for that/these particular tasks.\nMethods Spleen R.Kid L.Kid Gall. Eso. Liver Stom. Aorta IVC Panc. RAG LAG Duo. Blad. Pros. Avg\nTransUNet 0.881 0.928 0.919 0.813 0.740 0.973 0.832 0.919 0.841 0.713 0.638 0.565 0.685 0.748 0.692 0.792\nUNetr 0.926 0.936 0.918 0.785 0.702 0.969 0.788 0.893 0.828 0.732 0.717 0.554 0.658 0.683 0.722 0.762\nSwin-UNetr 0.959 0.960 0.949 0.894 0.827 0.979 0.899 0.944 0.899 0.828 0.791 0.745 0.817 0.875 0.841 0.880\nnnUNet 0.965 0.959 0.951 0.889 0.820 0.980 0.890 0.948 0.901 0.821 0.785 0.739 0.806 0.869 0.839 0.878\nEnsDiff 0.905 0.918 0.904 0.732 0.723 0.947 0.838 0.915 0.838 0.704 0.677 0.618 0.715 0.673 0.680 0.786\nSegDiff 0.885 0.872 0.891 0.703 0.654 0.852 0.702 0.874 0.819 0.715 0.654 0.632 0.697 0.652 0.695 0.753\nMedSegDiff 0.963 0.965 0.953 0.917 0.846 0.971 0.906 0.952 0.918 0.854 0.803 0.751 0.819 0.868 0.855 0.889\nMedSegDiff\n+ TransUNet 0.941 0.932 0.921 0.934 0.813 0.946 0.867 0.921 0.880 0.821 0.793 0.528 0.788 0.813 0.837 0.849\nAnchor 0.872 0.901 0.892 0.784 0.802 0.910 0.835 0.908 0.810 0.735 0.682 0.651 0.583 0.631 0.728 0.781\nMedSegDiff-V2 0.971 0.969 0.964 0.932 0.864 0.976 0.934 0.968 0.925 0.871 0.815 0.762 0.827 0.873 0.871 0.901\nTable 2: The comparison of MedSegDiff-V2 with SOTA segmentation methods over AMOS dataset evaluated by Dice Score.\nBest results are denoted as bold.\nevaluation is based on the average Dice Score calculated on\nthe AMOS dataset. Each configuration is run 20 times, and\nthe average Dice Score is used as the performance metric. In\nthe figure, we denote ”MedSegDiff+TransUNet” setting as\n”MSD-Trans”. Our findings indicate a common trend, where\nthe model performance improves rapidly in the initial 50 en-\nsembles and then stabilizes. Typically, the best performance\nis achieved after approximately 50 ensembles.\nComparing MedSegDiff-V2 variant with other diffusion\nmethods, we observe that it requires fewer ensembles to con-\nverge. It start significantly better, surpassing MedSegDiff by\n5%, and consistently maintains a lead of over 2% through-\nout. This highlights the efficiency of MedSegDiff-V2, as it\nachieves satisfactory results even with fewer ensemble itera-\ntions.\nAnalysis of Uncertainty In 5, we compare the sample di-\nversity on REFUGE2-Cup dataset. We compare the previous\nDPM-based methods, backbone with individual proposed\nmodules, and final MedSegDiff-V2 together. We evaluated\nthe variance among the samples using the Generalized En-\nergy Distance (GED) and Confidence Interval (CI). GED is a\ncommonly used metric to measures the agreement between\npredictions and the ground truth distribution of segmentation\nby comparing their distributions(Kohl et al. 2018). A lower\nenergy value indicates better agreement. From the table, we\ncan see that the proposed U-SA achieves lower CI and higher\nGED compared to previous methods, indicating a larger sam-\nple diversity. However, it is also observed that the proposed\nmodel reaches a higher or comparable performance, suggest-\ning that its generated samples mostly fall within the uncer-\ntainty region of the targets. When using SS-Former alone,\nwithout\nU-SA, the model achieves the best agreement with\nthe highest CI and lowest GED. Although SS-Former gets a\nfine performance with largest confidence, it fails to fully use\nthe diversity ensemble capability of the diffusion model. By\ncombining U-SA and SS-Former as MedSegDiff-V2, the per-\nformance is significantly improved with still high confidence.\nIt suggests that SS-Former helps mitigate the noise generated\nin U-SA, while U-SA provides more diversity to the model,\nresulting in mutual improvement.\nModel Efficiency and Complexity In 5, we also present\na comparison of model complexity and Gflops with other\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6035\nModel Spleen R.Kid L.Kid Gall. Eso. Liver Stom. Aorta IVC Veins Panc. AG Ave\nTransUNet 0.952 0.927 0.929 0.662 0.757 0.969 0.889 0.920 0.833 0.791 0.775 0.637 0.838\nUNetr 0.968 0.924 0.941 0.750 0.766 0.971 0.913 0.890 0.847 0.788 0.767 0.741 0.856\nSwin-UNetr 0.971 0.936 0.943 0.794 0.773 0.975 0.921 0.892 0.853 0.812 0.794 0.765 0.869\nnnUNet 0.942 0.894 0.910 0.704 0.723 0.948 0.824 0.877 0.782 0.720 0.680 0.616 0.802\nEnsDiff 0.938 0.931 0.924 0.772 0.771 0.967 0.910 0.869 0.851 0.802 0.771 0.745 0.854\nSegDiff 0.954 0.932 0.926 0.738 0.763 0.953 0.927 0.846 0.833 0.796 0.782 0.723 0.847\nMedSegDiff 0.973 0.930 0.955 0.812 0.815 0.973 0.924 0.907 0.868 0.825 0.788 0.779 0.879\nMedSegDiff\n+TransUNet 0.912 0.876 0.846 0.645 0.718 0.947 0.824 0.876 0.715 0.775 0.672 0.618 0.785\nAnchor 0.928 0.882 0.873 0.652 0.750 0.951 0.829 0.855 0.731 0.714 0.683 0.602 0.787\nMedSegDiff-V2 0.978 0.941 0.963 0.848 0.818 0.985 0.940 0.928 0.869 0.823 0.831 0.817 0.895\nTable 3: The comparison of MedSegDiff-V2 with SOTA segmentation methods over BTCV dataset evaluated by Dice Score.\nBest results are denoted as bold.\nAnc.Cond. Sem.Cond. AMOS BTCV OpticCup BrainTumor ThyroidNodule\nSA U-SA SS-Former\n(w/o Filter) NBP-Filter Ave-Dice (%) Ave-Dice (%) Dice (%) Dice (%) Dice (%)\n78.6 85.4 84.6 88.2 84.1\n✓ 83.5 85.8 85.2 88.7 84.6\n✓ 86.7 86.6 85.7 89.4 86.5\n✓ ✓ 87.8 87.1 86.5 89.8 86.8\n✓ ✓ ✓ 90.1 89.5 87.9 90.8 88.7\nTable 4: An ablation study on Anchor Conditioning and SS-Former. SA denotes Spatial Attention.\nFigure 3: The comparison of ensemble effect of DPM-based\nmethods. We show their performance of average Dice Score\non AMOS with increasing sampling times.\ndiffusion-based segmentation methods. The reported Gflops\nis the processing speed for a single 256 × 256 image until\nstability is reached in the implicit ensemble. We consider a\nvariance of performance less than 0.1% across the last ten\nensembles as an indicator of convergence. This metric is\nsignificant for the practical application of diffusion-based\nsegmentation models, as users commonly run the diffusion\nmodel iteratively to obtain a stable result.\nWe can see from the table that, unlike traditional deep\nlearning models, the amount of parameters in diffusion-based\nmodels is not directly correlated with Gflops, due to the pres-\nence of the implicit ensemble. For instance, even though\nMedSegDiff-V2 incorporates transformer blocks and occu-\npies more parameters, it requires fewer Gflops as it achieves\nstability in fewer steps. In comparison, MedSegDiff-V2 con-\nsumes only half the Gflops of MedSegDiff while outperform-\ning it in various segmentation tasks, as demonstrated above.\nThis underscores the efficiency of MedSegDiff-V2 and its\npotential in real-world application.\nModel Params (M) Gflops CI GED Dice\nEnsemDiff 23 2203 76.3 28.9 84.2\nSegDiff 23 2399 75.4 26.4 82.5\nMedSegDiff 25 1770 77.5 27.9 85.9\nMSD-Trans 118 2581 75.8 28.7 82.1\nbone + U-SA - - 73.2 34.6 85.7\nbone + SS-Former - - 84.2 21.7 86.1\nMedSegDiff-V2 46 983 82.6 23.5 87.9\nTable 5: Comparison of model parameters, Gflops, and gen-\nerated samples uncertainty\nConclusion\nIn this paper, we enhance the diffusion-based medical im-\nage segmentation framework by incorporating the trans-\nformer mechanism into the original UNet backbone, called\nMedSegDiff-V2. We propose a novel SS-Former architecture\nto learn the interaction between noise and semantic features.\nThe comparative experiments show our model outperformed\nprevious SOTA methods on 20 different medical image seg-\nmentation tasks with various image modalities. As the first\ntransformer-based diffusion model for medical image seg-\nmentation, we believe MedSegDiff-V2 will serve as a bench-\nmark for future research.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6036\nReferences\nAmit, T.; Nachmani, E.; Shaharbany, T.; and Wolf, L. 2021.\nSegdiff: Image segmentation with diffusion probabilistic\nmodels. arXiv preprint arXiv:2112.00390.\nArmato III, S. G.; McLennan, G.; Bidaut, L.; McNitt-Gray,\nM. F.; Meyer, C. R.; Reeves, A. P.; Zhao, B.; Aberle, D. R.;\nHenschke, C. I.; Hoffman, E. A.; et al. 2011. The lung image\ndatabase consortium (LIDC) and image database resource\ninitiative (IDRI): a completed reference database of lung\nnodules on CT scans. Medical physics, 38(2): 915–931.\nBaid, U.; Ghodasara, S.; Mohan, S.; Bilello, M.; Calabrese,\nE.; Colak, E.; Farahani, K.; Kalpathy-Cramer, J.; Kitamura,\nF. C.; Pati, S.; et al. 2021. The rsna-asnr-miccai brats 2021\nbenchmark on brain tumor segmentation and radiogenomic\nclassification. arXiv preprint arXiv:2107.02314.\nCao, H.; Wang, Y .; Chen, J.; Jiang, D.; Zhang, X.; Tian, Q.;\nand Wang, M. 2022. Swin-unet: Unet-like pure transformer\nfor medical image segmentation. In European conference on\ncomputer vision, 205–218. Springer.\nCaron, M.; Touvron, H.; Misra, I.; J ´egou, H.; Mairal, J.;\nBojanowski, P.; and Joulin, A. 2021. Emerging properties\nin self-supervised vision transformers. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n9650–9660.\nChen, J.; Lu, Y .; Yu, Q.; Luo, X.; Adeli, E.; Wang, Y .; Lu, L.;\nYuille, A. L.; and Zhou, Y . 2021. Transunet: Transformers\nmake strong encoders for medical image segmentation. arXiv\npreprint arXiv:2102.04306.\nChen, S.; Ma, K.; and Zheng, Y . 2019. Med3d: Transfer\nlearning for 3d medical image analysis. arXiv preprint\narXiv:1904.00625.\nChu, C.; Zheng, J.; and Zhou, Y . 2021. Ultrasonic thyroid\nnodule detection method based on U-Net network. Computer\nMethods and Programs in Biomedicine, 199: 105906.\nFang, H.; Li, F.; Fu, H.; Sun, X.; Cao, X.; Son, J.; Yu, S.;\nZhang, M.; Yuan, C.; Bian, C.; et al. 2022. REFUGE2 Chal-\nlenge: Treasure for Multi-Domain Learning in Glaucoma\nAssessment. arXiv preprint arXiv:2202.08994.\nFang, X.; and Yan, P. 2020. Multi-organ segmentation over\npartially labeled datasets with multi-scale feature abstraction.\nIEEE Transactions on Medical Imaging, 39(11): 3619–3629.\nGong, H.; Chen, G.; Wang, R.; Xie, X.; Mao, M.; Yu, Y .;\nChen, F.; and Li, G. 2021. Multi-task learning for thyroid\nnodule segmentation with thyroid region prior. In 2021 IEEE\n18th International Symposium on Biomedical Imaging (ISBI),\n257–261. IEEE.\nGuo, X.; Yang, Y .; Ye, C.; Lu, S.; Xiang, Y .; and Ma, T.\n2022. Accelerating Diffusion Models via Pre-segmentation\nDiffusion Sampling for Medical Image Segmentation. arXiv\npreprint arXiv:2210.17408.\nHan, R.; Cheng, G.; Zhang, B.; Yang, J.; Yuan, M.; Yang, D.;\nWu, J.; Liu, J.; Zhao, C.; Chen, Y .; et al. 2022. Validating\nautomated eye disease screening AI algorithm in community\nand in-hospital scenarios. Frontiers in Public Health, 10:\n944967.\nHatamizadeh, A.; Tang, Y .; Nath, V .; Yang, D.; Myronenko,\nA.; Landman, B.; Roth, H. R.; and Xu, D. 2022. Unetr: Trans-\nformers for 3d medical image segmentation. In Proceedings\nof the IEEE/CVF Winter Conference on Applications of Com-\nputer Vision, 574–584.\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion\nprobabilistic models. Advances in Neural Information Pro-\ncessing Systems, 33: 6840–6851.\nIsensee, F.; Jaeger, P. F.; Kohl, S. A.; Petersen, J.; and Maier-\nHein, K. H. 2021. nnU-Net: a self-configuring method for\ndeep learning-based biomedical image segmentation. Nature\nmethods, 18(2): 203–211.\nJi, W.; Yu, S.; Wu, J.; Ma, K.; Bian, C.; Bi, Q.; Li, J.; Liu, H.;\nCheng, L.; and Zheng, Y . 2021. Learning calibrated medi-\ncal image segmentation via multi-rater agreement modeling.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 12341–12351.\nJi, Y .; Bai, H.; Yang, J.; Ge, C.; Zhu, Y .; Zhang, R.; Li, Z.;\nZhang, L.; Ma, W.; Wan, X.; et al. 2022. Amos: A large-\nscale abdominal multi-organ benchmark for versatile medical\nimage segmentation. arXiv preprint arXiv:2206.08023.\nJiang, Y .; Zhang, Y .; Lin, X.; Dong, J.; Cheng, T.; and Liang,\nJ. 2022. SwinBTS: A method for 3D multimodal brain tumor\nsegmentation using swin transformer. Brain sciences, 12(6):\n797.\nKim, B.; Oh, Y .; and Ye, J. C. 2022. Diffusion adversarial rep-\nresentation learning for self-supervised vessel segmentation.\narXiv preprint arXiv:2209.14566.\nKohl, S.; Romera-Paredes, B.; Meyer, C.; De Fauw, J.; Led-\nsam, J. R.; Maier-Hein, K.; Eslami, S.; Jimenez Rezende, D.;\nand Ronneberger, O. 2018. A probabilistic u-net for segmen-\ntation of ambiguous images. Advances in neural information\nprocessing systems, 31.\nLi, F.; Pan, J.; Yang, D.; Wu, J.; Ou, Y .; Li, H.; Huang, J.; Xie,\nH.; Ou, D.; Wu, X.; et al. 2022. A multicenter clinical study\nof the automated fundus screening algorithm. Translational\nVision Science & Technology, 11(7): 22–22.\nLin, A.; Chen, B.; Xu, J.; Zhang, Z.; Lu, G.; and Zhang, D.\n2022. Ds-transunet: Dual swin transformer u-net for medical\nimage segmentation. IEEE Transactions on Instrumentation\nand Measurement, 71: 1–15.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101.\nMildenhall, B.; Srinivasan, P. P.; Tancik, M.; Barron, J. T.;\nRamamoorthi, R.; and Ng, R. 2020. NeRF: Representing\nscenes as neural radiance fields for view synthesis. In The\nEuropean Conference on Computer Vision (ECCV).\nMilton, M. A. A. 2019. Automated skin lesion classification\nusing ensemble of deep neural networks in isic 2018: Skin\nlesion analysis towards melanoma detection challenge. arXiv\npreprint arXiv:1901.10802.\nNaseer, M. M.; Ranasinghe, K.; Khan, S. H.; Hayat, M.;\nShahbaz Khan, F.; and Yang, M.-H. 2021. Intriguing proper-\nties of vision transformers. Advances in Neural Information\nProcessing Systems, 34: 23296–23308.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6037\nPedraza, L.; Vargas, C.; Narv´aez, F.; Dur´an, O.; Mu˜noz, E.;\nand Romero, E. 2015. An open access thyroid ultrasound\nimage database. In 10th International symposium on medical\ninformation processing and analysis, volume 9287, 188–193.\nSPIE.\nRahman, A.; Valanarasu, J. M. J.; Hacihaliloglu, I.; and Pa-\ntel, V . M. 2023. Ambiguous medical image segmentation\nusing diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n11536–11546.\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M.\n2022. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-resolution image synthesis with latent dif-\nfusion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 10684–10695.\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Den-\nton, E.; Ghasemipour, S. K. S.; Ayan, B. K.; Mahdavi, S. S.;\nLopes, R. G.; et al. 2022. Photorealistic Text-to-Image Dif-\nfusion Models with Deep Language Understanding. arXiv\npreprint arXiv:2205.11487.\nSitzmann, V .; Martel, J.; Bergman, A.; Lindell, D.; and Wet-\nzstein, G. 2020. Implicit neural representations with periodic\nactivation functions. Advances in Neural Information Pro-\ncessing Systems, 33: 7462–7473.\nTang, Y .; Yang, D.; Li, W.; Roth, H. R.; Landman, B.; Xu,\nD.; Nath, V .; and Hatamizadeh, A. 2022. Self-supervised pre-\ntraining of swin transformers for 3d medical image analysis.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 20730–20740.\nWang, J.; Wei, L.; Wang, L.; Zhou, Q.; Zhu, L.; and Qin, J.\n2021a. Boundary-aware transformers for skin lesion segmen-\ntation. In Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2021: 24th International Conference,\nStrasbourg, France, September 27–October 1, 2021, Proceed-\nings, Part I 24, 206–216. Springer.\nWang, S.; Yu, L.; Li, K.; Yang, X.; Fu, C.-W.; and Heng, P.-A.\n2019. Boundary and entropy-driven adversarial learning for\nfundus image segmentation. In International Conference on\nMedical Image Computing and Computer-Assisted Interven-\ntion, 102–110. Springer.\nWang, W.; Chen, C.; Ding, M.; Yu, H.; Zha, S.; and Li, J.\n2021b. Transbts: Multimodal brain tumor segmentation using\ntransformer. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, 109–119.\nSpringer.\nWarfield, S. K.; Zou, K. H.; and Wells, W. M. 2004. Simul-\ntaneous truth and performance level estimation (STAPLE):\nan algorithm for the validation of image segmentation. IEEE\ntransactions on medical imaging, 23(7): 903–921.\nWolleb, J.; Sandk ¨uhler, R.; Bieder, F.; Valmaggia, P.; and\nCattin, P. C. 2021. Diffusion Models for Implicit Image\nSegmentation Ensembles. arXiv preprint arXiv:2112.03145.\nWoo, S.; Park, J.; Lee, J.-Y .; and Kweon, I. S. 2018. Cbam:\nConvolutional block attention module. In Proceedings of the\nEuropean conference on computer vision (ECCV), 3–19.\nWu, H.; Chen, S.; Chen, G.; Wang, W.; Lei, B.; and Wen,\nZ. 2022a. FAT-Net: Feature adaptive transformers for auto-\nmated skin lesion segmentation. Medical image analysis, 76:\n102327.\nWu, J.; Fang, H.; Shang, F.; Yang, D.; Wang, Z.; Gao, J.; Yang,\nY .; and Xu, Y . 2022b. SeATrans: Learning Segmentation-\nAssisted Diagnosis Model via Transformer. InMedical Image\nComputing and Computer Assisted Intervention–MICCAI\n2022: 25th International Conference, Singapore, September\n18–22, 2022, Proceedings, Part II, 677–687. Springer.\nWu, J.; Fang, H.; Wang, Z.; Yang, D.; Yang, Y .; Shang, F.;\nZhou, W.; and Xu, Y . 2022c. Learning self-calibrated optic\ndisc and cup segmentation from multi-rater annotations. In\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, 614–624. Springer.\nWu, J.; Fang, H.; Yang, D.; Wang, Z.; Zhou, W.; Shang, F.;\nYang, Y .; and Xu, Y . 2022d. Opinions Vary? Diagnosis First!\nIn International Conference on Medical Image Computing\nand Computer-Assisted Intervention, 604–613. Springer.\nWu, J.; Fang, H.; Zhang, Y .; Yang, Y .; and Xu, Y . 2022e.\nMedSegDiff: Medical Image Segmentation with Diffusion\nProbabilistic Model. arXiv preprint arXiv:2211.00611.\nWu, J.; and Fu, R. 2019. Universal, transferable and targeted\nadversarial attacks. arXiv preprint arXiv:1908.11332.\nWu, J.; Fu, R.; Fang, H.; Liu, Y .; Wang, Z.; Xu, Y .; Jin,\nY .; and Arbel, T. 2023. Medical sam adapter: Adapting\nsegment anything model for medical image segmentation.\narXiv preprint arXiv:2304.12620.\nWu, J.; Yu, S.; Chen, W.; Ma, K.; Fu, R.; Liu, H.; Di, X.; and\nZheng, Y . 2020. Leveraging undiagnosed data for glaucoma\nclassification with teacher-student learning. InMedical Image\nComputing and Computer Assisted Intervention–MICCAI\n2020: 23rd International Conference, Lima, Peru, October\n4–8, 2020, Proceedings, Part I 23, 731–740. Springer.\nYu, S.; Xiao, D.; Frost, S.; and Kanagasingam, Y . 2019. Ro-\nbust optic disc and cup segmentation with deep learning for\nglaucoma detection. Computerized Medical Imaging and\nGraphics, 74: 61–71.\nZhai, X.; Kolesnikov, A.; Houlsby, N.; and Beyer, L. 2022.\nScaling vision transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n12104–12113.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6038",
  "topic": "Computer vision",
  "concepts": [
    {
      "name": "Computer vision",
      "score": 0.6005409359931946
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5827444791793823
    },
    {
      "name": "Computer science",
      "score": 0.49296045303344727
    },
    {
      "name": "Segmentation",
      "score": 0.4665752351284027
    },
    {
      "name": "Image segmentation",
      "score": 0.4604499936103821
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I154425047",
      "name": "University of Alberta",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I3004594783",
      "name": "Institute of High Performance Computing",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210116917",
      "name": "Singapore Eye Research Institute",
      "country": "SG"
    }
  ],
  "cited_by": 173
}