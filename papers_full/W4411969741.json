{
  "title": "Retrieval-augmented generation elevates local LLM quality in radiology contrast media consultation",
  "url": "https://openalex.org/W4411969741",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2120094915",
      "name": "Akihiko WADA",
      "affiliations": [
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A2104876398",
      "name": "Yuya Tanaka",
      "affiliations": [
        "The University of Tokyo",
        "University of Tokyo Hospital",
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A2164534421",
      "name": "Mitsuo Nishizawa",
      "affiliations": [
        "Juntendo University",
        "Juntendo University Urayasu Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2099622324",
      "name": "Akira Yamamoto",
      "affiliations": [
        "Juntendo University",
        "Juntendo University Urayasu Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2135666171",
      "name": "Toshiaki Akashi",
      "affiliations": [
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A2115502865",
      "name": "Akifumi Hagiwara",
      "affiliations": [
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A2052860827",
      "name": "Yayoi Hayakawa",
      "affiliations": [
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A1972764605",
      "name": "Junko Kikuta",
      "affiliations": [
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A2117847293",
      "name": "Keigo Shimoji",
      "affiliations": [
        "Juntendo University Urayasu Hospital",
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A2143429342",
      "name": "Katsuhiro Sano",
      "affiliations": [
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A1990458852",
      "name": "Koji Kamagata",
      "affiliations": [
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A2102608696",
      "name": "Atsushi Nakanishi",
      "affiliations": [
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A2044175586",
      "name": "Shigeki Aoki",
      "affiliations": [
        "Juntendo University Urayasu Hospital",
        "Juntendo University"
      ]
    },
    {
      "id": "https://openalex.org/A2120094915",
      "name": "Akihiko WADA",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104876398",
      "name": "Yuya Tanaka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2164534421",
      "name": "Mitsuo Nishizawa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099622324",
      "name": "Akira Yamamoto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135666171",
      "name": "Toshiaki Akashi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115502865",
      "name": "Akifumi Hagiwara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2052860827",
      "name": "Yayoi Hayakawa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1972764605",
      "name": "Junko Kikuta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117847293",
      "name": "Keigo Shimoji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143429342",
      "name": "Katsuhiro Sano",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1990458852",
      "name": "Koji Kamagata",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102608696",
      "name": "Atsushi Nakanishi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2044175586",
      "name": "Shigeki Aoki",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6853019041",
    "https://openalex.org/W4393317046",
    "https://openalex.org/W4400727806",
    "https://openalex.org/W4402851394",
    "https://openalex.org/W4401943774",
    "https://openalex.org/W4394890393",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4404520362",
    "https://openalex.org/W4391994222",
    "https://openalex.org/W4281784349",
    "https://openalex.org/W4362454453",
    "https://openalex.org/W2550995488",
    "https://openalex.org/W2289981976",
    "https://openalex.org/W4398203672",
    "https://openalex.org/W4406664255",
    "https://openalex.org/W4403816323",
    "https://openalex.org/W6874886628",
    "https://openalex.org/W4408615073",
    "https://openalex.org/W1935646153",
    "https://openalex.org/W2986979872",
    "https://openalex.org/W2030164991",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W2990138404"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01802-z\nRetrieval-augmented generation elevates\nlocal LLM quality in radiology contrast\nmedia consultation\nCheck for updates\nAkihiko Wada1 , Yuya Tanaka1,2, Mitsuo Nishizawa3,4, Akira Yamamoto4, Toshiaki Akashi1,\nAkifumi Hagiwara1,Y a y o iH a y a k a w a1, Junko Kikuta1, Keigo Shimoji1,4, Katsuhiro Sano1,K o j iK a m a g a t a1,\nAtsushi Nakanishi1 &S h i g e k iA o k i1,4\nLarge language models (LLMs) demonstrate signi ﬁcant potential in healthcare applications, but\nclinical deployment is limited by privacy concerns and insuf ﬁcient medical domain training. This study\ninvestigated whether retrieval-augmented generation (RAG) can improve locally deployable LLM for\nradiology contrast media consultation. In 100 synthetic iodinated contrast media consultations we\ncompared Llama 3.2-11B (baseline and RAG) with three cloud-based models — GPT-4o mini, Gemini\n2.0 Flash and Claude 3.5 Haiku. A blinded radiologist ranked the ﬁve replies per case, and three LLM-\nbased judges scored accuracy, safety, structure, tone, applicability and latency. Under controlled\nconditions, RAG eliminated hallucinations (0% vs 8%; χ²₍Yates₎ = 6.38, p = 0.012) and improved mean\nrank by 1.3 (Z = –4.82, p < 0.001), though performance gaps with cloud models persist. The RAG-\nenhanced model remained faster (2.6 s vs 4.9 –7.3 s) while the LLM-based judges preferred it over\nGPT-4o mini, though the radiologist ranked GPT-4o mini higher. RAG thus provides meaningful\nimprovements for local clinical LLMs while maintaining the privacy bene ﬁts of on-premise\ndeployment.\nL a r g el a n g u a g em o d e l s( L L M s )h a v erapidly expanded across domains,\ndemonstrating signi ﬁcant potential to enhance human capabilities and\nworkﬂow ef ﬁciency in specialized ﬁelds that require domain-speci ﬁc\nknowledge. In medicine, LLMs are promising to summarize medical\nliterature, support clinical reason ing, and extract key information from\nmedical records 1,2. In radiology, recent studies highlight LLMs ’\npotential to improve report quality, work ﬂow ef ﬁciency, and diag-\nnostic accuracy 3,4.\nDespite this potential, healthcare implementation of LLMs faces\nunique challenges centered on a fundamental dilemma: balancing patient\ndata conﬁdentiality with advanced AI capabilities. Cloud-based LLMs offer\nsuperior reasoning abilities through l arge-scale training data but require\ntransmitting sensitive medical infor mation to external servers, raising\nsubstantial concerns under regulations like HIPAA and GDPR\n5.\nConversely, locally deployed LLMs ensure data privacy but typically\ndemonstrate inferior performance due to constraints in model size and\ncomputational resources6. This performance gap is particularly pronounced\nin specialized medical domains where domain-speci ﬁck n o w l e d g ea n d\nnuanced understanding of clinical guidelines are essential.\nRetrieval-augmented generation (RAG) has emerged as a promising\napproach to address this trade-off. By integrating external domain-speciﬁc\nknowledge into the response generation process, RAG enables LLMs to\nleverage specialized information beyond their training data, reduce hallu-\ncinations through authoritative source grounding, and dynamically update\nknowledge without additional model ﬁne-tuning\n7.R A Gc o m b i n e sh i g h\naccuracy with robust data privacy when implemented within a locally\ndeployed environment, potentially offering a solution to the privacy-\nperformance dilemma in healthcare AI.\nWhile RAG’s effectiveness has been demonstrated in general knowl-\nedge retrieval tasks, its utility in highly specialized medicalﬁelds, particularly\nradiology, remains underexplor ed. Previous studies have identi ﬁed\nknowledge base quality and retrieval efﬁciency as key determinants of RAG\nperformance in medical applications\n8,9, but few have evaluated its applica-\ntion in time-sensitive clinical decision support scenarios10.\n1Department of Radiology, Juntendo University Graduate School of Medicine, 2-1-1 Hongo, Bunkyo-ku, Tokyo, 113-8421, Japan. 2Department of Radiology, The\nUniversity of Tokyo School of Medicine, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8655, Japan. 3Department of Radiology, Juntendo University Urayasu Hospital, 2-1-\n1 Tomioka, Urayasu, Chiba, 279-0021, Japan. 4Faculty of Health Data Science, Juntendo University Graduate School of Medicine, 6-8-1 Hinode, Urayasu, Chiba,\n279-0013, Japan. e-mail: a-wada@juntendo.ac.jp\nnpj Digital Medicine|           (2025) 8:395 1\n1234567890():,;\n1234567890():,;\nTo systematically assess locally deployed RAG-enhanced LLMs in a\nspecialized medical context, this studyfocuses on iodinated contrast media\n(ICM) consultations —a crucial radiological task requiring specialized\nknowledge and real-time decision-making. CT examinations have become\nfundamental to modern diagnostic imaging, with annual scan volumes in\nthe United States increasing fro m approximately 3 million in 1980 to 91\nmillion in 201911. Notably, 40-48% of all CT scans involve ICM to enhance\nvisualization of vascular and organ structures, making appropriate contrast\nadministration essential to radiological practice.\nRecent challenges have ampli ﬁed the importance of precise ICM\nconsultation. The COVID-19 pandemiccaused severe global contrast agent\nshortages, necessitating careful a ssessment of contrast-enhanced scan\nappropriateness\n12. Additionally, ICM-related adverse events range from\nmild allergic reactions to severe nephrotoxicity, making risk strati ﬁcation\ncrucial for patient safety. Research indicates that 7.5% of ICM-related\ncomplications could be prevented through appropriate consultation13,w h i l e\ncommunication errors in contrast media ordering account for 13.9% of\nradiology workﬂow inefﬁciencies\n14.\nEffective ICM consultation require s specialized radiological knowl-\nedge, clinical experience, and dynam ic risk assessment capabilities. LLMs\ncould potentially assist radiologis ts by automating risk assessment and\nprotocol selection, thereby reducing workload and enhancing decision-\nmaking. The integration of facility-speciﬁc protocols and recently updated\nguidelines through RAG could be particularly valuable, as contrast media\nadministration often involves institution-speciﬁc rules that cannot be cap-\ntured in general model training.\nThis study aims to evaluate the performance of RAG-enhanced\nl i g h t w e i g h tl o c a lL L M si nI C Mc o n s u l tation compared to leading cloud-\nbased models. We developed and a ssessed a RAG-implemented locally\ndeployable model (Llama 3.2 11B) against three cloud-based LLMs (GPT-\n4o mini, Gemini 2.0 Flash, Claude 3.5 H aiku) using 100 simulated clinical\nscenarios requiring ICM risk assess ment and protocol selection. Both\nradiologists and automated LLM evaluators evaluated performance through\na comprehensive assessment, examiningclinical accuracy, safety, response\nstructure, communication quality, practical applicability, and response time.\nBy quantifying performance diff erences between cloud-based and\nlocally deployed models and assessing whether RAG can substantially\nenhance locally deployable model performance while maintaining data\nprivacy, this research provides practical evidence to guide healthcare insti-\ntutions in selecting and implementing AI systems for clinical decision\nsupport.\nResults\nOverall Model Performance and Rankings\nA global Friedman test across the ﬁve LLMs con ﬁrmed signiﬁcant differ-\nences in median ranks for the 100 iodinated-contrast-media scenarios (χ²=\n78.4, p < 0.001). Post-hoc Nemenyi comparisons showed that the three\ncloud models clustered at the top: Gemini 2.0 Flash was dominant (radi-\nologist mean rank = 1.36;ﬁrst in 74% of cases), followed by Claude 3.5 Haiku\n(2.09) and GPT-4o mini (3.21) (Table1,F i g .1). The baseline Llama 3.2 11B\nwas ranked last in 86% of cases (mean 4.80). RAG lifted its performance to a\nmean rank of 3.54, close to GPT-4o mini ’s3 . 2 1( Δ = –1.26; Z = –7.26,\np < 0.001). Automated LLM evaluators conﬁrmed the overall cloud supre-\nmacy, again ranking Gemini 2.0 Flash and Claude 3.5 Haiku ﬁrst and\nsecond, respectively. Within the rem aining models, however, they placed\nthe RAG-enhanced Llama 3.2 11B as 3rd, ahead of GPT-4o mini (Δ = –0.44\nto –0.82; Z = +2.56 to +5.43; p ≤ 0.009). This reversal relative to the radi-\nologist’s preference illustrates a systematic divergence between human and\nLLM-based judgements, primarily dueto differential treatment of response\ntime in evaluation frameworks.\nInter-Evaluator Agreement and Assessment Patterns\nIn three LLM evaluators, Claude 3.5 Sonnet exhibited the highest agreement\nwith the radiologist at 82.2%, suggesting its assessments most closely align\nwith human expert judgment. GPT-4o’s evaluation also showed a relatively\nhigh agreement rate of 75.5% compared to the radiologist. Across all eva-\nluators, there was broad consensus on the top performer (Gemini 2.0 Flash,\n81.6% agreement) and bottom performer (base Llama 3.2 11B), while mid-\ntier models showed more variable assessments.\nThe most signi ﬁcant evaluation divergence appeared in mid-tier\nrankings (Table1,F i g .1). All three LLM evaluators consistently ranked the\nRAG-enhanced Llama 3.2 11B higher than GPT-4o mini, contrary to the\nradiologist’s assessment (Table 1). This divergence was most pronounced\nwith GPT-4o, which ranked the RAG-enhanced Llama second (mean rank:\n2.53) and GPT-4o mini fourth (mean rank: 3.35).\nClinical Quality and Hallucination Mitigation\nQualitative assessment by a radiologist revealed that hallucinations —par-\nticularly in contrast dosing and contraindication recommendations—were\npresent in 8% of responses from the base Llama model and were eliminated\n(0%) in the RAG-enhanced version ( χ²₍Yates₎ = 6.38, p = 0.012; Fisher\np = 0.0068). These hallucinations typically involved incorrect ICM proto-\ncols, including improper contraindication identi ﬁcation and dosage\nrecommendations, which could potentially impact patient safety (Table2).\nNone of the cloud-based models exhibit ed hallucinations in the evaluated\nscenarios.\nAmong the 100 cases, 54% showed marked improvement with RAG—\nmost notably in safety-critical content such as contraindication manage-\nment and precise dosing guidance—while an additional 33% demonstrated\nminor gains involving non-critical factual details. These improvements were\nconcentrated in cases that required specialized knowledge of institutional\nprotocols or recently updated guidelines.\nMultidimensional Performance Assessment\nRadar chart analysis of LLM evaluations indicated substantial gains in\nclinical accuracy, safety, and applicability for the RAG-enhanced model,\nwith modest changes in commu nication and structure (Fig. 2). The most\nsigniﬁcant improvements occurred in dom ains directly related to patient\nsafety and clinical decision-making.\nSpeciﬁcally, clinical accuracy scores improved from a mean of 14.2 to\n21.5 points (out of 25), and safety considerations improved from 11.8 to 18.7\npoints (out of 20). Professional communication showed moderate\nenhancement (13.4 to 16.8 out of 20), while response structure remained\nrelatively stable (7.8 to 8.3 out of 10), as the base model already demon-\nstrated adequate formatting capabilities.\nResponse Time Analysis\nIn terms of response time, the RAG-enhanced model (2.58 s) remained\nfaster on average than all cloud-basedm o d e l s( G P T - 4 om i n i :4 . 8 7s ,C l a u d e\n3 . 5H a i k u :7 . 2 5s ,G e m i n i2 . 0F l a s h :3 .14 s), despite increased latency com-\npared to its base version (1.31 s) (Fig. 3). The difference was statistically\nsigniﬁcant for all comparisons, including Gemini 2.0 Flash (t = –3.96,\np = 0.00011).\nResponse time variability differed notably across models. Claude 3.5\nHaiku demonstrated the most consistent performance with a coefﬁcient of\nvariation (CV) of 10.76%, while the RAG-enhanced Llama 3.2 11B showed\nthe highest variability (CV = 44.19%), likely due to ﬂuctuations in knowl-\nedge retrieval latency. The base Llama 3.2 11B (CV = 37.40%), GPT-4o mini\n(CV = 31.21%), and Gemini 2.0 Flash (CV = 26.43%) exhibited inter-\nmediate variability. While the RAG-enhanced model’s response time was\nmore variable, the trade-off was accep table given the substantial perfor-\nmance improvements in clinical accu racy and safety. This response time\nadvantage contributed to higher automated evaluation rankings compared\nto human expert preferences. Task complexity showed minimal impact on\nresponse times across models, with variability primarily driven by\narchitecture-speciﬁc characteristics rather than scenario complexity.\nDiscussion\nThis study demonstrates that a retrieval-augmented local LLM can achieve\nclinically reliable performance in a safety-critical radiological use case while\nhttps://doi.org/10.1038/s41746-025-01802-z Article\nnpj Digital Medicine|           (2025) 8:395 2\npreserving patient privacy. By integrating curated guideline-based knowl-\nedge into a lightweight LLM, we observed signi ﬁcant improvements in\nclinical accuracy, risk strati ﬁcation, and communication quality —criteria\nthat are central to iodinated contrast media (ICM) decision-making.\nNotably, the RAG-enhanced model eliminated hallucinations observed in\nthe base model and achieved faster average response times than all evaluated\ncloud-based LLMs, suggesting feasibility for real-time use in clinical settings.\nOurﬁndings align with recent research demonstrating RAG’sp o t e n t i a l\nto enhance LLM performance in specialized medical domains. Gupta et al.\n(2024) emphasized RAG ’s application in medica l knowledge retrieval,\nshowing improvements in accuracy and reliability\n15. Mortaheb et al. (2025)\nintroduced a multimodal evaluatio n framework ensuring contextual\nrelevance9, while Nguyen et al. (2024) reported signi ﬁcant accuracy\nimprovements in clinical question-answering when incorporating RAG\ninto language models8. These studies collectively con ﬁrm that integrating\ndomain-speciﬁc knowledge improves evidence-based reasoning capabilities\nof LLMs, leading to enhanced accuracy , factual consistency, and clinical\nrelevance in radiological applications.\nOurﬁndings reinforce RAG’s critical role in hallucination mitigation, a\nfundamental requirement for deploying AI systems in clinical work ﬂows.\nThe complete elimination of hallucinations (reducing incidence from 8% to\n0%) in our RAG-enhanced model rep resents a notable achievement,\nespecially considering previous studi es reporting hallucination rates of\n28.6-39.6% in medical-specialized models16. Incorrect recommendations,\nparticularly in contraindication ma nagement and contrast dosing, pose\nsubstantial patient safety risks. Th e success of our approach likely stems\nTable 1 | Model Performance Rankings by Radiologist and LLM Evaluators\nEvaluator Model Mean Rank ± SD 95% CI 1st Place Rate (%)\nRadiologist Gemini 2.0 Flash 1.36 ± 0.72 [1.22, 1.50] 74.0\nClaude 3.5 Haiku 2.09 ± 0.87 [1.92, 2.26] 23.0\nGPT-4o mini 3.21 ± 0.77 [3.06, 3.36] 0.0\nLlama 3.2 11B + RAG 3.54 ± 0.85 [3.37, 3.71] 2.0\nLlama 3.2 11B 4.80 ± 0.60 [4.68, 4.92] 1.0\nLLM Evaluator 1 Gemini 2.0 Flash 1.23 ± 0.63 [1.10, 1.36] 87.0\n(GPT-4o) Llama 3.2 11B + RAG 2.53 ± 0.90 [2.35, 2.71] 9.0\nClaude 3.5 Haiku 2.95 ± 1.19 [2.71, 3.19] 3.0\nGPT-4o mini 3.35 ± 1.01 [3.15, 3.55] 4.0\nLlama 3.2 11B 4.32 ± 1.03 [4.11, 4.53] 3.0\nLLM Evaluator 2 Gemini 2.0 Flash 1.49 ± 0.80 [1.33, 1.65] 66.0\n(Gemini 2.0 Flash Thinking) Claude 3.5 Haiku 2.25 ± 1.29 [1.99, 2.51] 34.0\nLlama 3.2 11B + RAG 3.12 ± 1.14 [2.89, 3.35] 6.0\nGPT-4o mini 3.56 ± 1.15 [3.33, 3.79] 3.0\nLlama 3.2 11B 3.78 ± 1.28 [3.53, 4.03] 9.0\nLLM Evaluator 3 Gemini 2.0 Flash 1.43 ± 0.71 [1.29, 1.57] 65.0\n(Claude 3.5 Sonnet) Claude 3.5 Haiku 1.95 ± 0.98 [1.76, 2.14] 34.0\nLlama 3.2 11B + RAG 3.28 ± 0.95 [3.09, 3.47] 3.0\nGPT-4o mini 3.72 ± 0.88 [3.55, 3.89] 1.0\nLlama 3.2 11B 4.32 ± 0.86 [4.15, 4.49] 1.0\nMean ranks with standard deviations and 95% con ﬁdence intervals across 100 ICM consultation scenarios as rated by one radiologist and three LLM-based evaluators. Lower ranks indicate better\nperformance. 1st Place Rate shows percentage of scenarios where each model ranked ﬁrst. Models are ordered by performance within each evaluator group.\nFig. 1 | Model Rankings by Human and Auto-\nmated Evaluators.The mean ranks of ﬁve LLMs\nacross 100 ICM consultation scenarios evaluated by\na radiologist and three LLM-based scorers, where\nlower values indicate better performance and error\nbars represent 95% con ﬁdence intervals.\nhttps://doi.org/10.1038/s41746-025-01802-z Article\nnpj Digital Medicine|           (2025) 8:395 3\nfrom developing a robust knowledge base centered speci ﬁcally on ICM\nconsultation, enabling the model to generate responses with clear evidential\nsupport. This achievement is particularly signi ﬁcant for safety-critical\napplications where even occasional hallucinations could potentially\nendanger patients. However, it is crucial to emphasize that “zero halluci-\nnations” indicates absence of clinically dangerous misinformation rather\nthan perfect clinical responses. Final clinical judgment, risk-beneﬁt assess-\nment, and treatment decisions must always remain with quali ﬁed radi-\nologists who integrate AI-generated information with patient-speci ﬁc\nfactors, clinical experience, and comprehensive medical context. RAG\nenhancement improves the reliability of supportive information but does\nnot replace human clinical expertise in patient care decisions.\nGiven the legal and ethical constra ints associated with cloud-based\nLLMs, particularly regarding PHI transmission under HIPAA and GDPR\nframeworks, our results suggest that RAG-enhanced locally deployable\nmodels provide a viable alternative that balances safety, speed, and\nprivacy\n4,17. By processing data entirely within institutional networks, locally\ndeployed LLMs substantially reduce information leakage risks, addressing a\nprimary concern in healthcare AI adoption. Our on-premise deployment\nvalidation (Supplementary Note 1) demonstrates competitive performance\nwhile maintaining complete data privacy within institutional infrastructure.\nWhile locally deployed models have traditionally faced performance lim-\nitations due to size constraints, our RAG-enhanced Llama 3.2 11B\ndemonstrates that clinically useful AI support can be achieved in\nTable 2 | Clinical Errors and Corresponding RAG Corrections in Contrast Media Use\nCategory Clinical Scenario Initial Output Error RAG Correction\nDosage Adult contrast dosage 1.5 –3.0 mL/kg, max 300 mg iodine/kg Adjusted to 1.5 –2.0 mL/kg, 300 mgI/mL\nDosage 3-year-old child (15 kg) 5 –10 mL total dosage 22.5 –30 mL (weight-based)\nContraindications Iodinated contrast risk Incorrectly mentioned NSF risk Identi ﬁed contrast-induced nephropathy\nContraindications Hyperthyroidism Omitted thyroid crisis risk Added crisis risk explanation\nContraindications Pregnancy and CT Incomplete information Added fetal thyroid impact\nContraindications β-blockers and anaphylaxis No management advice Added glucagon use info\nIncomplete Orders Metformin use unknown Mentioned lactic acidosis risk only Added stepwise protocol\nAvoidable Use Gallstone suspicion Recommended contrast CT Recommended ultrasound ﬁrst\nAvoidable Use Emphysema, hemorrhage, fracture Correctly advised no contrast Maintained accuracy\nSummary of initial output errors made by the base LLM, corresponding corrections by the RAG-enhanced model, and whether the errors were fully address ed. Cases are categorized by clinical task type\n(e.g., dosage, contraindications, appropriateness).\nFig. 2 | Multidimensional Performance Metrics Across LLMs.Radar charts\ncomparing ﬁve LLMs across six key evaluation domains: clinical accuracy, safety,\nresponse structure, professional communication, practical applicability, and\nresponse time. All performance metrics are normalized to a 0-1 scale for comparative\nvisualization, with 1.0 representing maximum performance in each domain.\nhttps://doi.org/10.1038/s41746-025-01802-z Article\nnpj Digital Medicine|           (2025) 8:395 4\nprivacy-sensitive healthcare env ironments without compromising on\nessential performance metrics. This a ddresses the fundamental privacy-\nperformance dilemma highlighted in our introduction, providing practical\nevidence for healthcare institutions evaluating AI implementation strategies.\nThe observed divergence between human and LLM-based evaluators\nhighlights an important considerationfor future benchmarking frameworks.\nWhile LLM-based evaluators consistently ranked the RAG-enhanced Llama\n3.2 11B higher than GPT-4o mini, the radiologist favored the latter. This\ndivergence primarily stems from differential treatment of response time in\nevaluation frameworks. Human radiologists excluded response time from\nranking criteria, focusing solely on clinical quality, while LLM evaluators\nincluded response time as a weigh ted component. The RAG-enhanced\nmodel’s superior speed (2.58 s vs 4.87 s for GPT-4o mini) contributed to\nhigher automated rankings despite comparable clinical quality scores,\nreﬂecting the difference between clinical priority (accuracy over speed) and\nsystem performance evaluation (efﬁciency inclusion). Our analysis reported\nmean ranks, standard deviations, and 95% conﬁdence intervals to comple-\nment the ordinal ranking data. Alth ough such metrics derive from non-\ninterval data, they are widely used in recent LLM benchmarking studies (e.g.,\nMT-Bench, Chatbot Arena) as descriptive evaluator preference and ranking\nconsistency indicators. These metricsprovide a practical and interpretable\nsummary of comparative model performance when interpreted with non-\nparametric tests and complete rank distributions.\nThis divergence between evaluation methodologies underscores the\nneed for hybrid evaluation strategies t hat combine quantitative and quali-\ntative metrics, especially for high-st akes applications. Recent research has\nhighlighted both the potential and limitations of automated LLM evaluation\napproaches in medical contexts\n18,19. Research on emergency medicine\nsummary generation has shown that LLM-generated content rated highly\nby automated evaluations may contain subtle clinical utility and safety\ndeﬁciencies only identiﬁable through expert clinician review\n20.O u rh y b r i d\napproach offers valuable insights for re ﬁning methodologies used to eval-\nuate LLMs in clinical contexts.\nSeveral important limitations meritconsideration. First, our controlled\nsynthetic scenarios may not capture the full complexity of real-world clinical\nconsultations, including incomplete patient information and diagnostic\nuncertainty, and time-sensitive decision-making under clinical pressure.\nThe observed zero hallucination rate in cloud-based models likely re ﬂects\nthis controlled environment rather th an performance under challenging\nclinical conditions. Second, evaluation by a single radiologist may limit the\ngeneralizability of our human expert rankings and hallucination detection,\nthough our multi-evaluator valida tion (Supplementary Note 2) demon-\nstrates high inter-rater agreement across different experience levels. Third,\nour temperature parameter selection (0.2) represents a practical\ncompromise due to documented Llama model instability at temperature 0.0,\nthough systematic sensitivity analysis (Supplementary Note 3) validates that\nclinical quality metrics remain statistically unchanged while reproducibility\nshows manageable reduction.\nThese limitations can be effectively addressed through appropriate\nclinical implementation strategies: such as radiologists serving as essential\nvalidation intermediaries for AI-generated recommendations (human in\nthe loop), advancing RAG and self-re ﬁnement technologies for enhanced\nrobustness, and multi-layer quality a ssurance protocols integrated within\nexisting clinical work ﬂows. While our syntheti c evaluation provides\nessential baseline performance data and methodological frameworks, these\nimplementation strategies suggest that effective clinical deployment remains\nviable with appropriate human oversight and technological enhancements.\nFuture research should prioritize re al-world validation using enhanced\nimplementation frameworks, systematic RAG knowledge base maintenance\nand updating protocols, expansion to multiple expert reviewers across dif-\nferent subspecialties, and comprehensive testing in active clinical environ-\nments with appropriate privacy safeguards.\nThis study demonstrates that RAG-enhanced local LLMs can provide\nmeaningful clinical decision support under controlled conditions. While\nour approach reveals inherent limitations, it offers a viable pathway for\nprivacy-preserving clinical AI deployment with appropriate human over-\nsight. As AI technologies advance, th e principles demonstrated here will\nsupport more robust clinical applications while maintaining privacy\nadvantages of local deployment.\nMethods\nConsultation Scenarios Development\nWe designed 100 simulated consultation scenarios covering a diverse range\nof iodinated contrast media (ICM) use cases, including appropriateness,\ncontraindications, incomplete orders, and avoidable contrast use. These\nscenarios systematically reﬂectedﬁve key categories in radiological practice:\n(1) appropriateness consultations for speciﬁc clinical indications, (2) opti-\nmal contrast agent selection and protocols, (3) contraindication identi ﬁ-\ncation and risk assessment, (4) recognition of incomplete ordering\ninformation, and (5) identiﬁcation of cases where contrast could be avoided.\nScenarios were initially generated using GPT-4o and Claude 3.5 Sonnet\nwith structured prompts for clinical authenticity. To ensure realism and\nguideline compliance, two radiologists (4 and 30 years of experience,\nrespectively) independently reviewed and revised each scenario according to\ncurrent ACR and ESUR guidelines. Special attention was given to incor-\nporating realistic clinical nuances and decision-making challenges com-\nmonly encountered in ICM consultations. The complete set of scenarios is\navailable in Supplementary Data 1.\nFig. 3 | Comparative Response Times of\nEvaluated LLMs.Boxplots showing response time\ndistributions for ﬁve LLMs where each point\nrepresents an individual measurement, center lines\nshow medians, box edges show 25th and 75th per-\ncentiles, and whiskers extend to the most extreme\ndata points within 1.5 × IQR from the box edges.\nhttps://doi.org/10.1038/s41746-025-01802-z Article\nnpj Digital Medicine|           (2025) 8:395 5\nLanguage Models Conﬁguration\nWe benchmarkedﬁve LLMs—three cloud-based models (GPT-4o mini from\nOpenAI, Gemini 2.0 Flash from Google, and Claude 3.5 Haiku from\nAnthropic) accessed through their p ublic REST APIs, and two locally\ndeployable models (Llama 3.2 11B from Meta, as a baseline and RAG-\nenhanced variant). Llama 3.2 11B is designed as a locally deployable, light-\nweight parameter model ideal for on-premises computing environments.\nThis study deployed the Llama 3.2 11B via GroqCloud ’sh o s t e d\ninference API to facilitate direct performance comparisons with the cloud-\nnative models under controlled conditions (Fig. 4). To address concerns\nregarding true privacy-preserving d eployment, we conducted additional\nvalidation using dedicated enterprise hardware (HP Z8 Fury G5 with\nNVIDIA RTX 6000 Ada Generation, 48GB VRAM; Intel Xeon W7-3445, 20\ncores; 512GB DDR5 RAM), with results detailed in Supplementary Note 1.\nAll models were deployed via the Dify platform (version 0.10.0) with\nstandardized parameters, including co nsistent temperature settings (0.2)\nand single-shot response generation\n21. Temperature parameter selection\nrepresents a practical compromi se due to documented Llama model\ninstability at temperature 0.0, with comprehensive sensitivity analysis\ndetailed in Supplementary Note 3. For each model, we recorded both the\nresponses and the response times while maintaining all other default settings\nfor consistent comparison.\nFor the automated evaluation p rocess, we employed three high-\nperformance models (GPT-4o, Gemini 2.0 Flash Thinking, and Claude 3.5\nSonnet) as evaluators. Table3 documents the technical speciﬁcations for all\nmodels used in this study, including parameter sizes, deployment char-\nacteristics, and implementation details.\nRetrieval-Augmented Generation Implementation\nFor the RAG-enhanced model, we compiled a knowledge base from\nauthoritative sources, including the ACR Manual on Contrast Media, ESUR\nguidelines, institutional protocols, and relevant literature 22–24.K n o w l e d g e\nentries were initially generated in a question-answer format using Claude\n3.5 Sonnet and were subsequently reviewed, modiﬁed, and organized by two\nradiologists (Table4).\nThe knowledge base was structured as segments with a maximum of\n600 tokens, comprising 66 chunks with an average of 337 characters per\nsegment. These segments were tra nsformed into high-dimensional\nembeddings using OpenAI’s text-embedding-3-large model and indexed\nin “High Quality” mode. The system employed a hybrid search method that\ncombined semantic vector search with keyword-based retrieval.\nFor each clinical query, the system retrieved four relevant context\nfragments (TopK =4 ) based on cosine similarity calculations. These retrieved\nfragments were then ranked according to their relevance scores and incor-\nporated into the prompt structure for the Llama 3.2 11B model (Fig. 5).\nEvaluation Methodology\nWe used a two-tier evaluation strategy: (i) human expert review by a board-\ncertiﬁed radiologist, and (ii) automated scoring by three large LLM“judges.”\nF o rt h eh u m a nt i e r ,t h er a d i o l o g i s tr e v i e w e da l lr e s p o n s e si nab l i n d e d\nmanner, ranking the anonymized responses from 1st (best) to 5th (worst)\nbased on clinical appropriateness and p ractical applicability, without con-\nsidering response time. In addition to rubric-based scoring, each response\nwas screened for hallucinations, deﬁned as clinically incorrect or guideline-\ninconsistent statements that could alter patient management (e.g., wrong\ndosage, missing contraindication). Hallucination presence (yes/no) was\nrecorded per scenario, and the frequencies were compared between models\nusing χ² (with Yates correction) and Fisher’se x a c tt e s t s .\nFor automated LLM “judge” evaluation, we utilized three high-\nperformance LLM evaluators within a G-EVAL-like framework\n25.E a c h\nresponse was systematically compared against reference standards devel-\noped by radiology experts, using six prede ﬁned criteria:\n1. Clinical Accuracy: Alignment with established medical guidelines and\nfactual correctness.\n2. Safety: Proper identi ﬁcation of contraindications and emphasis on\npatient safety measures.\n3. Response Structure: Clarity and lo gical organization of the answer.\n4. Professional Communication: Appropriate use of medical terminology\nand professional language.\n5. Practical Applicability: Provision of actionable clinical\nrecommendations.\n6. Response Time: Performance against prede ﬁned speed thresholds.\nLLM evaluators assessed the responses based on these criteria, with\nﬁnal rankings determined by the total combined score. In cases of tied\nscores, models were assigned the same higher rank. Comprehensive details\nregarding the evaluation prompts and scoring rubrics are available in\nSupplementary Note 4.\nStatistical Analysis\nAll analyses were conducted in Python 3.9.13 with scipy 1.10.0 and stats-\nmodels 0.13.5. Because the Shapiro–Wilk test revealed signiﬁcant departures\nfrom normality (p < 0.05) in the rank dis tributions, we exclusively applied\nnon-parametric procedures. Overall differences among theﬁve LLMs were\nexamined with the Friedman test, which accounts for the repeated-measures\nnature of the 100 shared consultation scenarios. Pairwise contrasts were then\nFig. 4 | Evaluation Workﬂow for Language Model\nResponses. Schematic of the evaluation pipeline.\nEach clinical query was simultaneously processed by\nﬁve LLMs (three cloud-based and two locally\ndeployable models). The generated responses were\nanonymized and evaluated by a radiologist and three\nLLM-based evaluators using both human ranking\nand rubric-based scoring.\nhttps://doi.org/10.1038/s41746-025-01802-z Article\nnpj Digital Medicine|           (2025) 8:395 6\nassessed with the Nemenyi post-hoc test; for the two comparisons of primary\ninterest—GPT-4o mini versus the RAG-enhanced Llama, and baseline versus\nRAG Llama—we also report Mann-Whitney U (Wilcoxon rank-sum) sta-\ntistics with Bonferroni-adjusted signi ﬁcance thresholds. Effect sizes are\nexpressed as the difference in mean ranks (Δ), accompanied by Wilcoxon Z\nvalues and two-sided p values. Descriptively, we present mean rank ± stan-\ndard deviation and 95% conﬁdence intervals to visualise performance spread\nand inter-evaluator agreement, following conventions established in recent\nLLM benchmarks such as MT-Bench and Chatbot Arena.\nEthical Considerations\nThis study did not require Institutional Review Board approval as it utilized\nsimulated clinical scenarios rather than actual patient data. All consultations\nwere ﬁctional cases designed to reﬂect common clinical situations without\nincorporating any protected health information.\nData availability\nThe complete dataset of 100 synthetic cl i n i c a ls c e n a r i o su s e di nt h i ss t u d yi s\nprovided as Supplementary Data 1. The dataset includes scenario IDs,\nclinical inquiries, primary categories, and scenario types across four main\nclinical domains: proper contrast agent usage and protocols (11 scenarios),\ncontraindication and risk management (32 scenarios), appropriateness\nconsultation across multiple specialties (37 scenarios), and order deﬁciency\nmanagement (20 scenarios). Model re sponse data supporting the conclu-\nsions are available from the corresponding author upon reasonable request\nto protect potential intellectual property considerations.\nCode availability\nCustom evaluation scripts for LLM re sponse assessment and RAG imple-\nmentation code are available from the corresponding author upon\nTable 3 | Technical Speciﬁcations of Evaluated Language Models for ICM Consultation and Response Evaluation\nLLM Name Use Case Deployment Developer Model Scale Description Availability\nGPT-4o mini ICM Consultation Cloud OpenAI Large (MoE) Optimized for speed 2024-07\nGemini 2.0 Flash ICM Consultation Cloud Google Large Fast-response model 2024-12\nClaude 3.5 Haiku ICM Consultation Cloud Anthropic Medium –Large Lightweight model 2024-11\nLlama 3.2 11B ICM Consultation Local Meta 11B Base on-premise model 2024-09\nGPT-4o Evaluator Cloud OpenAI Very Large Full-scale evaluator 2024-05\nGemini 2.0 Flash\nThinking\nEvaluator Cloud Google Very Large Reasoning-tuned version 2024-12\nClaude 3.5 Sonnet Evaluator Cloud Anthropic Very Large Mid-tier evaluator 2024-06\nNote: Deployment — Cloud = vendor-hosted API only; Local = weights are deployable on-premise, though in this study Meta Llama 3.2 11B was run on GroqCloud (TLS-encrypted, no-retention) through the\nsame Dify pipeline for parity. Model scale —“ Large” ≈ tens-to-hundreds B parameters, “Very Large” the upper end; Llama ’s 11 B count is of ﬁcial. “MoE” = Mixture-of-Experts architecture.\nFig. 5 | Retrieval-Augmented Generation (RAG) Pipeline for Local LLMs.Dia-\ngram illustrating the RAG implementation. Domain-speci ﬁck n o w l e d g ei s\nembedded and indexed using semantic and keyword-based retrieval. Retrieved\ncontext chunks are ranked and integrated into the prompt before inference by\nthe local LLM.\nTable 4 | Sample Knowledge Entry from the RAG Knowledge Base\nTopic Question-Answer Format\nPrecautions for Contrast Media Administration in Patients\nwith Renal Dysfunction\nQ: What are the important precautions and considerations when administering contrast media to patients\nwith renal dysfunction?\nA:  Pre-existing renal dysfunction is considered a risk factor for acute renal failure caused by iodinated\ncontrast media, with eGFR now commonly used as the preferred indicator of renal function\n- Patients with eGFR below 30 mL/min/1.73 m² face signi ﬁcantly increased risk and require suf ﬁcient\npreventive measures\n- Contrast-induced nephropathy (CIN) is de ﬁned as an increase in serum creatinine level by 0.5 mg/dL or\n25% or more from baseline within 72 hours after contrast media administration\n- Key risk factors for CIN include advanced age, diabetes with chronic kidney disease (CKD), use of diuretics\n(especially loop diuretics), and use of NSAIDs\n- Appropriate hydration protocols using normal saline solution or sodium bicarbonate solution are\nrecommended as preventive measures\n- Repeated contrast-enhanced CT scans within a short period (24-48 hours) should be avoided\nExample of a structured entry used in the retrieval-augmented generation (RAG) system, formatted as a clinical question and answer. This entry addre sses precautions for administering contrast media to\npatients with renal dysfunction.\nhttps://doi.org/10.1038/s41746-025-01802-z Article\nnpj Digital Medicine|           (2025) 8:395 7\nreasonable request. The study utili zed standard open-source software\npackages: Python 3.9.13, scipy 1.10. 0, statsmodels 0.13.5, and the Dify\nplatform (version 0.10.0) for model deployment. No proprietary code was\ndeveloped that would require special access restrictions.\nReceived: 27 April 2025; Accepted: 15 June 2025;\nReferences\n1. Liu, Z. et al. Radiology-GPT: A Large Language Model for Radiology.\nPreprint at arXiv:2306.08666 (2023).\n2. Nakaura, T. et al. The impact of large language models on radiology: a\nguide for radiologists on the latest innovations in AI. Jpn. J. Radiol.37,\n1–12 (2024).\n3. Wada, A. et al. Optimizing GPT-4 Turbo Diagnostic Accuracy in\nNeuroradiology through Prompt Engineering and Con ﬁdence\nThresholds. Diagnostics 14, 1541 (2024).\n4. Abbasi, N. & Smith, D. A. Cybersecurity in Healthcare: Securing Patient\nHealth Information (PHI).J. Knowl. Learn. Sci. Technol.3, 278–287 (2024).\n5. Mohan, A. et al. Securing AI Inference in the Cloud: Is CPU-GPU\nConﬁdential Computing Ready? In Proc. 17th IEEE Int. Conf. Cloud\nComput. 164-175 (IEEE, 2024).\n6. Bedi, S. et al. A Systematic Review of Testing and Evaluation of\nHealthcare Applications of Large Language Models (LLMs). Preprint\nat medRxiv:2024.04.15.24305869 (2024).\n7. Lewis, P. et al. Retrieval-Augmented Generation for Knowledge-\nIntensive NLP Tasks. Advances in Neural Information Processing\nSystems 33, 9459–9474 (2020).\n8. Nguyen, Q. et al. Advancing Question-Answering in Ophthalmology\nwith Retrieval-Augmented Generation (RAG): Benchmarking Open-\nsource and Proprietary Large Language Models. Preprint at\nmedRxiv:2024.11.18.24317510 (2024).\n9. Mortaheb, M., Khojastepour, M. A. A., Chakradhar, S. T. & Ulukus, S.\nRAG-Check: Evaluating Multimodal Retrieval Augmented Generation\nPerformance. Preprint at arXiv:2501.03995 (2025).\n10. Telenti, A. et al. Large language models for science and medicine. Eur.\nJ. Clin. Investig.54, e14183 (2024).\n11. Davenport, M. S., Chu, P., Szczykutowicz, T. P. & Smith-Bindman, R.\nComparison of Strategies to Conserve Iodinated Intravascular\nContrast Media for Computed Tomography During a Shortage. JAMA\n328, 476–478 (2022).\n12. CADTH. Optimizing the Use of Iodinated Contrast Media for CT:\nManaging Shortages and Planning for a Sustainable and Secure\nSupply. CADTH report 613 (2023).\n13. Sessa, M. et al. Campania preventability assessment committee: a\nfocus on the preventability of the contrast media adverse drug\nreactions. Expert Opin. Drug Saf.15,5 1–59 (2016).\n14. Siewert, B., Brook, O. R., Hochman, M. & Eisenberg, R. L. Impact of\nCommunication Errors in Radiology on Patient Care, Customer\nSatisfaction, and Work-Flow Ef ﬁciency. Am. J. Roentgenol.206,\n573–579 (2016).\n15. Gupta, S., Ranjan, R. & Singh, S. N. A Comprehensive Survey of\nRetrieval-Augmented Generation. Preprint at arXiv:2410.12837 (2024).\n16. Chelli, M. et al. Hallucination Rates and Reference Accuracy of\nChatGPT and Bard for Systematic Reviews: Comparative Analysis. J.\nMed. Internet Res.26, e53164 (2024).\n17. Ng, M. Y., Helzer, J., Pfeffer, M. A., Seto, T. & Hernandez-Boussard, T.\nDevelopment of secure infrastructure for advancing generative\nartiﬁcial intelligence research in healthcare at an academic medical\ncenter. J. Am. Med. Inform. Assoc.32, 586–588 (2025).\n18. Iglesia, I. D. et al. Ranking Over Scoring: Towards Reliable and Robust\nAutomated Evaluation of LLM-Generated Medical Explanatory\nArguments. In Proc. 31st Int. Conf. Computational Linguistics\n(COLING), 2025.coling-main.634 (2025).\n19. Gu, J. et al. A Survey on LLM-as-a-Judge. Preprint at arXiv:2411.15594\n(2024).\n20. Szymanski, A. et al. Limitations of the LLM-as-a-Judge Approach for\nEvaluating LLM Outputs in Expert Knowledge Tasks. In Proc. 30th Int.\nConf. Intelligent User Interfaces (IUI), 952 −966. (ACM, 2025). https://\ndoi.org/10.1145/3708359.3712091.\n21. Dify.AI. The Innovation Engine for Generative AI Applications. https://\ndify.ai/ (2023).\n22. Beckett, K. R., Moriarity, A. K. & Langer, J. M. Safe Use of Contrast\nMedia: What the Radiologist Needs to Know. Radiographics 35,\n1738–1750 (2015).\n23. Isaka, Y. et al. Guideline on the use of iodinated contrast media in\npatients with kidney disease 2018. Clin. Exp. Nephrol.24,1 –44 (2020).\n24. Thomsen, H. S. & Morcos, S. K. & ESUR. ESUR guidelines on contrast\nmedia. Abdom. Imaging31, 131–140 (2006).\n25. Liu, Y. et al. G-Eval: NLG Evaluation using GPT-4 with Better Human\nAlignment. In Proc. Conf. Empirical Methods in Natural Language\nProcessing (EMNLP), 2511–2522 (2023).\nAcknowledgements\nThis work was supported by the Japan Society for the Promotion of Science\n(JSPS) KAKENHI Grant Numbers 25K02609 and 22K07674. We\nacknowledge the support provided by the AI Incubation Farm (aif), Juntendo\nUniversity Graduate School of Medicine ( https://research-center.juntendo.\nac.jp/aif/) for local large language model infrastructure development and\ndeployment optimization.\nAuthor contributions\nA.W. conceived the study, obtained funding, supervised all research\nactivities, and drafted the initial manuscript; Y.T. curated and pre-processed\nconsultation transcripts, performed statistical analyses, and co-wrote the\nMethods section; M.N. designed and maintained the AI environment,\nimplemented core algorithms, and co-supervised technical execution; A.Ya.\ndeveloped the knowledge-base architecture, advised on the RAG workﬂow,\nand reviewed technical accuracy; T.A. coordinated access to institutional\nresources, veriﬁed scenario relevance, and edited the Results section; A.H.,\nY.H. and J.K. jointly designed and validated the iodinated-contrast-media\nconsultation scenarios, curated the chatbot knowledge base, prepared\nSupplementary Table 1 and keyﬁgures, and critically revised the manuscript;\nK.Sh. wrote statistical scripts, generated visualisations, and contributed to\nthe Statistical Analysis subsection; K.Sa. veriﬁed experimental reproduci-\nbility, assisted model evaluation, and reviewed the manuscript; K.K. pro-\nvided neuroradiological expertise, interpreted exemplar cases, and reﬁned\nthe clinical-implications paragraph; A.N. conducted the literature review on\ncontrast-media guidelines, reﬁned the consultation rubric, and contributed\nto manuscript editing; S.A. offered overall clinical guidance, critically revised\nall drafts, and gave ﬁnal approval for submission.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01802-z\n.\nCorrespondenceand requests for materials should be addressed to\nAkihiko Wada.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional af ﬁliations.\nhttps://doi.org/10.1038/s41746-025-01802-z Article\nnpj Digital Medicine|           (2025) 8:395 8\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article ’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visit http://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01802-z Article\nnpj Digital Medicine|           (2025) 8:395 9",
  "topic": "Contrast (vision)",
  "concepts": [
    {
      "name": "Contrast (vision)",
      "score": 0.5949941873550415
    },
    {
      "name": "Software deployment",
      "score": 0.5131309628486633
    },
    {
      "name": "Medicine",
      "score": 0.4568004310131073
    },
    {
      "name": "Cloud computing",
      "score": 0.43510836362838745
    },
    {
      "name": "Radiology",
      "score": 0.3551192283630371
    },
    {
      "name": "Medical physics",
      "score": 0.33398014307022095
    },
    {
      "name": "Psychology",
      "score": 0.32726988196372986
    },
    {
      "name": "Computer science",
      "score": 0.30190548300743103
    },
    {
      "name": "Artificial intelligence",
      "score": 0.17791643738746643
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I34077901",
      "name": "Juntendo University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I74801974",
      "name": "The University of Tokyo",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210137488",
      "name": "Juntendo University Urayasu Hospital",
      "country": "JP"
    }
  ],
  "cited_by": 5
}