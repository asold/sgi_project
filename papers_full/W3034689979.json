{
  "title": "A Transformer-based Approach for Source Code Summarization",
  "url": "https://openalex.org/W3034689979",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2184437607",
      "name": "Wasi Ahmad",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2152902633",
      "name": "Saikat Chakraborty",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2166284654",
      "name": "Baishakhi Ray",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2208999240",
      "name": "Kai-Wei Chang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2516621648",
    "https://openalex.org/W2807964941",
    "https://openalex.org/W3086449553",
    "https://openalex.org/W2963515589",
    "https://openalex.org/W2963661253",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2979271470",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2979486033",
    "https://openalex.org/W2952768586",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3091730360",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2082160726",
    "https://openalex.org/W2971270287",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2886906914",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2964194820",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2962995178",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2953035525",
    "https://openalex.org/W2971008324",
    "https://openalex.org/W2964645190",
    "https://openalex.org/W2888557792",
    "https://openalex.org/W2964268484"
  ],
  "abstract": "Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens’ position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4998–5007\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n4998\nA Transformer-based Approach for Source Code Summarization\nWasi Uddin Ahmad\nUniversity of California, Los Angeles\nwasiahmad@cs.ucla.edu\nSaikat Chakraborty\nColumbia University\nsaikatc@cs.columbia.edu\nBaishakhi Ray\nColumbia University\nrayb@cs.columbia.edu\nKai-Wei Chang\nUniversity of California, Los Angeles\nkwchang@cs.ucla.edu\nAbstract\nGenerating a readable summary that describes\nthe functionality of a program is known as\nsource code summarization. In this task,\nlearning code representation by modeling the\npairwise relationship between code tokens to\ncapture their long-range dependencies is cru-\ncial. To learn code representation for sum-\nmarization, we explore the Transformer model\nthat uses a self-attention mechanism and has\nshown to be effective in capturing long-range\ndependencies. In this work, we show that de-\nspite the approach is simple, it outperforms\nthe state-of-the-art techniques by a signiﬁcant\nmargin. We perform extensive analysis and\nablation studies that reveal several important\nﬁndings, e.g., the absolute encoding of source\ncode tokens’ position hinders, while relative\nencoding signiﬁcantly improves the summa-\nrization performance. We have made our code\npublicly available1 to facilitate future research.\n1 Introduction\nProgram comprehension is an indispensable ingre-\ndient of software development and maintenance\n(Xia et al., 2018). A natural language summary\nof source code facilitates program comprehension\nby reducing developers’ efforts signiﬁcantly (Srid-\nhara et al., 2010). Source code summarization\nrefers to the task of creating readable summaries\nthat describe the functionality of a program.\nWith the advancement of deep learning and the\navailability of large-scale data through a vast num-\nber of open-source repositories, automatic source\ncode summarizing has drawn attention from re-\nsearchers. Most of the neural approaches generate\nsource code summaries in a sequence-to-sequence\nfashion. One of the initial works Iyer et al. (2016)\ntrained an embedding matrix to represent the indi-\nvidual code tokens and combine them with a Re-\n1https://github.com/wasiahmad/NeuralCodeSum\ncurrent Neural Network (RNN) via an attention\nmechanism to generate a natural language sum-\nmary. Subsequent works (Liang and Zhu, 2018;\nHu et al., 2018a,b) adopted the traditional RNN-\nbased sequence-to-sequence network (Sutskever\net al., 2014) with attention mechanism (Luong\net al., 2015) on different abstractions of code.\nThe RNN-based sequence models have two lim-\nitations in learning source code representations.\nFirst, they do not model the non-sequential struc-\nture of source code as they process the code tokens\nsequentially. Second, source code can be very\nlong, and thus RNN-based models may fail to cap-\nture the long-range dependencies between code to-\nkens. In contrast to the RNN-based models,Trans-\nformer (Vaswani et al., 2017), which leverages\nself-attention mechanism, can capture long-range\ndependencies. Transformers have been shown to\nperform well on many natural language genera-\ntion tasks such as machine translation (Wang et al.,\n2019), text summarization (You et al., 2019), story\ngeneration (Fan et al., 2018), etc.\nTo learn the order of tokens in a sequence or\nto model the relationship between tokens, Trans-\nformer requires to be injected with positional en-\ncodings (Vaswani et al., 2017; Shaw et al., 2018;\nShiv and Quirk, 2019). In this work, we show\nthat, by modeling the pairwise relationship be-\ntween source code tokens using relative position\nrepresentation (Shaw et al., 2018), we can achieve\nsigniﬁcant improvements over learning sequence\ninformation of code tokens using absolute position\nrepresentation (Vaswani et al., 2017).\nWe want to emphasize that our proposed ap-\nproach is simple but effective as it outperforms\nthe fancy and sophisticated state-of-the-art source\ncode summarization techniques by a signiﬁcant\nmargin. We perform experiments on two well-\nstudied datasets collected from GitHub, and the\nresults endorse the effectiveness of our approach\n4999\nover the state-of-the-art solutions. In addition, we\nprovide a detailed ablation study to quantify the\neffect of several design choices in the Transformer\nto deliver a strong baseline for future research.\n2 Proposed Approach\nWe propose to use Transformer (Vaswani et al.,\n2017) to generate a natural language summary\ngiven a piece of source code. Both the code and\nsummary is a sequence of tokens that are repre-\nsented by a sequence of vectors,x = (x1,...,x n)\nwhere xi ∈Rdmodel . In this section, we brieﬂy\ndescribe the Transformer architecture (§ 2.1) and\nhow to model the order of source code tokens or\ntheir pairwise relationship (§ 2.2) in Transformer.\n2.1 Architecture\nThe Transformer consists of stacked multi-head\nattention and parameterized linear transformation\nlayers for both the encoder and decoder. At each\nlayer, the multi-head attention employshattention\nheads and performs the self-attention mechanism.\nSelf-Attention. We describe the self-attention\nmechanism based on Shaw et al. (2018). In\neach attention head, the sequence of input vec-\ntors, x = (x1,...,x n) where xi ∈Rdmodel are\ntransformed into the sequence of output vectors,\no = (o1,...,o n) where oi ∈Rdk as:\noi =\nn∑\nj=1\nαij(xjWV ),\neij = xiWQ(xjWK)T\n√dk\n,\nwhere αij = exp eij∑n\nk=1 exp eik\nand WQ,WK ∈\nRdmodel×dk ,WV ∈Rdmodel×dv are the parameters\nthat are unique per layer and attention head.\nCopy Attention. We incorporate the copying\nmechanism (See et al., 2017) in the Transformer to\nallow both generating words from vocabulary and\ncopying from the input source code. We use an\nadditional attention layer to learn the copy distri-\nbution on top of the decoder stack (Nishida et al.,\n2019). The copy attention enables the Transformer\nto copy rare tokens (e.g., function names, variable\nnames) from source code and thus improves the\nsummarization performance signiﬁcantly (§ 3.2).\n2.2 Position Representations\nNow, we discuss how to learn the order of source\ncode tokens or model their pairwise relationship.\nDataset Java Python\nTrain 69,708 55,538\nValidation 8,714 18,505\nTest 8,714 18,502\nUnique tokens in code 66,650 307,596\nUnique tokens in summary 46,895 56,189\nAvg. tokens in code 120.16 47.98\nAvg. tokens in summary 17.73 9.48\nTable 1: Statistics of the experiment datasets. We thank\nthe authors of Wei et al. (2019) for kindly sharing the\nPython dataset splits. The Java dataset splits are pub-\nlicly available.\nEncoding absolute position.To allow the Trans-\nformer to utilize the order information of source\ncode tokens, we train an embedding matrix WPe\nthat learns to encode tokens’ absolute positions\ninto vectors of dimension dmodel. However, we\nshow that capturing the order of code tokens is not\nhelpful to learn source code representations and\nleads to poor summarization performance (§ 3.2).\nIt is important to note that we train another em-\nbedding matrix WPd that learns to encode the ab-\nsolute positions of summary tokens.2\nEncoding pairwise relationship. The semantic\nrepresentation of a code does not rely on the abso-\nlute positions of its tokens. Instead, their mutual\ninteractions inﬂuence the meaning of the source\ncode. For instance, semantic meaning of the ex-\npressions a+band b+aare the same.\nTo encode the pairwise relationships between\ninput elements, Shaw et al. (2018) extended the\nself-attention mechanism as follows.\noi =\nn∑\nj=1\nαij(xjWV + aV\nij),\neij =\nxiWQ(xjWK + aK\nij )T\n√dk\n,\nwhere, aV\nij and aK\nij are relative positional represen-\ntations for the two position i and j. Shaw et al.\n(2018) suggested clipping the maximum relative\nposition to a maximum absolute value ofkas they\nhypothesize that precise relative position informa-\ntion is not useful beyond a certain distance.\naK\nij = wK\nclip(j−i,k),aV\nij = wV\nclip(j−i,k),\nclip(x,k) = max(−k,min(k,x)).\nHence, we learn 2k+ 1relative position repre-\nsentations: (wK\n−k,...,w K\nk ), and (wV\n−k,...,w V\nk ).\n2In this work, we do not study alternative ways of learning\nposition representation for the summary tokens.\n5000\nMethods Java Python\nBLEU METEOR ROUGE-L BLEU METEOR ROUGE-L\nCODE-NN (Iyer et al., 2016) 27.60 12.61 41.10 17.36 09.29 37.81\nTree2Seq (Eriguchi et al., 2016) 37.88 22.55 51.50 20.07 08.96 35.64\nRL+Hybrid2Seq (Wan et al., 2018) 38.22 22.75 51.91 19.28 09.75 39.34\nDeepCom (Hu et al., 2018a) 39.75 23.06 52.67 20.78 09.98 37.35\nAPI+CODE (Hu et al., 2018b) 41.31 23.73 52.25 15.36 08.57 33.65\nDual Model (Wei et al., 2019) 42.39 25.77 53.61 21.80 11.14 39.45\nOur models and ablation study\nBase Model 43.41 25.91 52.71 31.08 18.57 44.31\nFull Model 44.58 26.43 54.76 32.52 19.77 46.73\nFull Model w/o Relative Position 44.26 26.23 53.58 31.38 18.69 44.68\nFull Model w/o Copy Attention 44.14 26.34 53.95 31.64 19.17 45.42\nTable 2: Comparison of our proposed approach with the baseline methods. The results of the baseline methods\nare directly reported from (Wei et al., 2019). The “Base Model” refers to the vanilla Transformer (uses absolute\nposition representations) and the “Full Model” uses relative position representations and includes copy attention.\nIn this work, we study an alternative of the rela-\ntive position representations that ignores thedirec-\ntional information (Ahmad et al., 2019). In other\nwords, the information whether the j’th token is\non the left or right of the i’th token is ignored.\naK\nij = wK\nclip(|j−i|,k),aV\nij = wV\nclip(|j−i|,k),\nclip(x,k) = min(|x|,k).\n3 Experiment\n3.1 Setup\nDatasets and Pre-processing. We conduct our\nexperiments on a Java dataset (Hu et al., 2018b)\nand a Python dataset (Wan et al., 2018). The statis-\ntics of the two datasets are shown in Table 1. In\naddition to the pre-processing steps followed by\nWei et al. (2019), we split source code tokens of\nthe form CamelCase and snake case to respective\nsub-tokens3. We show that such a split of code\ntokens improves the summarization performance.\nMetrics. We evaluate the source code summariza-\ntion performance using three metrics, BLEU (Pap-\nineni et al., 2002), METEOR (Banerjee and Lavie,\n2005), and ROUGE-L (Lin, 2004).\nBaselines. We compare our Transformer-based\nsource code summarization approach with ﬁve\nbaseline methods reported in Wei et al. (2019) and\ntheir proposed Dual model. We refer the readers\nto (Wei et al., 2019) for the details about the hy-\nperparameter of all the baseline methods.\nHyper-parameters. We follow Wei et al. (2019)\nto set the maximum lengths and vocabulary sizes\n3The CamelCase and snake case tokenization reduces the\nvocabulary signiﬁcantly. For example, the number of unique\ntokens in Java source code reduced from 292,626 to 66,650.\nfor code and summaries in both the datasets. We\ntrain the Transformer models using Adam opti-\nmizer (Kingma and Ba, 2015) with an initial learn-\ning rate of 10−4. We set the mini-batch size and\ndropout rate to 32 and 0.2, respectively. We train\nthe Transformer models for a maximum of 200\nepochs and perform early stop if the validation\nperformance does not improve for 20 consecutive\niterations. We use a beam search during infer-\nence and set the beam size to 4. Detailed hyper-\nparameter settings can be found in Appendix A.\n3.2 Results and Analysis\nOverall results. The overall results of our pro-\nposed model and baselines are presented in Ta-\nble 2. The result shows that the Base model out-\nperforms the baselines (except for ROUGE-L in\njava), while the Full model improves the perfor-\nmance further.4 We ran the Base model on the\noriginal datasets (without splitting the CamelCase\nand snake case code tokens) and observed that the\nperformance drops by 0.60, 0.72 BLEU and 1.66,\n2.09 ROUGE-L points for the Java and Python\ndatasets respectively. We provide a few qualitative\nexamples in Appendix C showing the usefulness\nof the Full model over the Base model.\nUnlike the baseline approaches, our proposed\nmodel employs the copy attention mechanism. As\nshown in Table 2, the copy attention improves the\nperformance 0.44 and 0.88 BLEU points for the\nJava and Python datasets respectively.\nImpact of position representation. We per-\nform an ablation study to investigate the beneﬁts\n4We observe a more signiﬁcant gain on the Python dataset\nand a detailed discussion on it is provided in Appendix B.\n5001\nSource Target BLEU METEOR ROUGE-L\n\u0013 \u0013 43.41 25.91 52.71\n\u0013 \u0017 42.34 24.74 50.96\n\u0017 \u0013 43.59 26.00 52.88\n\u0017 \u0017 41.85 24.32 50.87\nTable 3: Ablation study on absolute positional repre-\nsentations using the “Base Model” on the Java dataset.\nk Directional BLEU METEOR ROUGE-L\n8 \u0013 44.22 26.35 53.86\n\u0017 42.61 24.67 51.10\n16 \u0013 44.14 26.34 53.95\n\u0017 44.06 26.31 53.51\n32 \u0013 44.55 26.66 54.30\n\u0017 43.95 26.28 53.24\n2i \u0013 44.37 26.58 53.96\n\u0017 43.58 25.95 52.73\nTable 4: Ablation study on relative positional represen-\ntations (in encoding) for Transformer. While 8, 16, and\n32 represents a ﬁxed relative distance for all the layers,\n2i (where i= 1,...,L ; L= 6) represents a layer-wise\nrelative distance for Transformer.\nof encoding the absolute position of code tokens or\nmodeling their pairwise relationship for the source\ncode summarization task, and the results are pre-\nsented in Table 3 and 4. Table 3 demonstrates that\nlearning the absolute position of code tokens are\nnot effective as we can see it slightly hurts the per-\nformance compared to when it is excluded. This\nempirical ﬁnding corroborates the design choice\nof Iyer et al. (2016), where they did not use the\nsequence information of the source code tokens.\nOn the other hand, we observe that learning the\npairwise relationship between source code tokens\nvia relative position representations helps as Table\n4 demonstrates higher performance. We vary the\nclipping distance, k, and consider ignoring the di-\nrectional information while modeling the pairwise\nrelationship. The empirical results suggest that the\ndirectional information is indeed important while\n16, 32, and 2i relative distances result in similar\nperformance (in both experimental datasets).\nVarying model size and number of layers.We\nperform ablation study by varyingdmodel and land\nthe results are presented in Table 5. 5 In our ex-\nperiments, we observe that a deeper model (more\nlayers) performs better than a wider model (larger\ndmodel). Intuitively, the source code summariza-\n5Considering the model complexity, we do not increase\nthe model size or number of layers further.\n#Param. BLEU METEOR ROUGE-L\nVarying the model size (dmodel)\n256 15.8 38.21 21.54 48.63\n384 28.4 41.71 24.51 51.42\n512 44.1 43.41 25.91 52.71\n768 85.1 45.29 27.56 54.39\nVarying the number of layers (l)\n3 22.1 41.26 23.54 51.37\n6 44.1 43.41 25.91 52.71\n9 66.2 45.03 27.21 54.02\n12 88.3 45.56 27.64 54.89\nTable 5: Ablation study on the hidden size and number\nof layers for the “Base Model” on the Java dataset. We\nuse dmodel = H, dff = 4H, h= 8, and dk = dv = 64\nin all settings. We set l = 6 and dmodel = 512 while\nvarying dmodel and lrespectively. #Param. represents\nthe number of trainable parameters in millions (only\nincludes Transformer parameters).\ntion task depends on more semantic information\nthan syntactic, and thus deeper model helps.\nUse of Abstract Syntax Tree (AST).We perform\nadditional experiments to employ the abstract syn-\ntax tree (AST) structure of source code in the\nTransformer. We follow Hu et al. (2018a) and\nuse the Structure-based Traversal (SBT) technique\nto transform the AST structure into a linear se-\nquence. We keep our proposed Transformer archi-\ntecture intact, except in the copy attention mech-\nanism, we use a mask to block copying the non-\nterminal tokens from the input sequence. It is im-\nportant to note that, with and without AST, the av-\nerage length of the input code sequences is 172\nand 120, respectively. Since the complexity of the\nTransformer is O(n2 ×d) where nis the input se-\nquence length, hence, the use of AST comes with\nan additional cost. Our experimental ﬁndings sug-\ngest that the incorporation of AST information in\nthe Transformer does not result in an improvement\nin source code summarization. We hypothesize\nthat the exploitation of the code structure informa-\ntion in summarization has limited advantage, and\nit diminishes as the Transformer learns it implic-\nitly with relative position representation.\nQualitative analysis.We provide a couple of ex-\namples in Table 6 to demonstrate the usefulness\nof our proposed approach qualitatively (more ex-\namples are provided in Table 9 and 10 in the Ap-\npendix). The qualitative analysis reveals that, in\ncomparison to the Vanilla Transformer model, the\ncopy enabled model generates shorter summaries\n5002\npublic static String selectText(XPathExpression expr, Node context) {\ntry {\nreturn (String)expr.evaluate(context, XPathConstants.STRING );\n} catch (XPathExpressionException e) {\nthrow new XmlException(e);\n}\n}\nBase Model: evaluates the xpath expression to a xpath expression .\nFull Model w/o Relative Position: evaluates the xpath expression .\nFull Model w/o Copy Attention Attention: evaluates the xpath expression as a single element .\nFull Model: evaluates the xpath expression as a text string .\nHuman Written: evaluates the xpath expression as text .\ndef get_hosting_service(name):\ntry:\nreturn hosting_service_registry.get(u'hosting service id', name)\nexcept ItemLookupError:\nreturn None\nBase Model: returns the color limits from the current service name .\nFull Model w/o Relative Position: return the hosting service .\nFull Model w/o Copy Attention: return the name of the service .\nFull Model : return the hosting service name .\nHuman Written: return the hosting service with the given name .\nTable 6: Qualitative example of different models’ performance on Java and Python datasets.\nwith more accurate keywords. Besides, we ob-\nserve that in a copy enabled model, frequent to-\nkens in the code snippet get a higher copy prob-\nability when relative position representations are\nused, in comparison to absolute position represen-\ntations. We suspect this is due to the ﬂexibility of\nlearning the relation between code tokens without\nrelying on their absolute position.\n4 Related Work\nMost of the neural source code summarization ap-\nproaches frame the problem as a sequence genera-\ntion task and use recurrent encoder-decoder net-\nworks with attention mechanisms as the funda-\nmental building blocks (Iyer et al., 2016; Liang\nand Zhu, 2018; Hu et al., 2018a,b). Different from\nthese works, Allamanis et al. (2016) proposed a\nconvolutional attention model to summarize the\nsource codes into short, name-like summaries.\nRecent works in code summarization utilize\nstructural information of a program in the form of\nAbstract Syntax Tree (AST) that can be encoded\nusing tree structure encoders such as Tree-LSTM\n(Shido et al., 2019), Tree-Transformer (Harer\net al., 2019), and Graph Neural Network (LeClair\net al., 2020). In contrast, Hu et al. (2018a) pro-\nposed a structure based traversal (SBT) method to\nﬂatten the AST into a sequence and showed im-\nprovement over the AST based methods. Later,\nLeClair et al. (2019) used the SBT method and de-\ncoupled the code structure from the code tokens to\nlearn better structure representation.\nAmong other noteworthy works, API usage in-\nformation (Hu et al., 2018b), reinforcement learn-\ning (Wan et al., 2018), dual learning (Wei et al.,\n2019), retrieval-based techniques (Zhang et al.,\n2020) are leveraged to further enhance the code\nsummarization models. We can enhance a Trans-\nformer with previously proposed techniques; how-\never, in this work, we limit ourselves to study dif-\nferent design choices for a Transformer without\nbreaking its’ core architectural design philosophy.\n5 Conclusion\nThis paper empirically investigates the advantage\nof using the Transformer model for the source\ncode summarization task. We demonstrate that the\nTransformer with relative position representations\nand copy attention outperforms state-of-the-art ap-\nproaches by a large margin. In our future work,\nwe want to study the effective incorporation of\ncode structure into the Transformer and apply the\ntechniques in other software engineering sequence\ngeneration tasks (e.g., commit message generation\nfor source code changes).\nAcknowledgments\nThis work was supported in part by National\nScience Foundation Grant OAC 1920462, CCF\n1845893, CCF 1822965, CNS 1842456.\n5003\nReferences\nWasi Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard\nHovy, Kai-Wei Chang, and Nanyun Peng. 2019. On\ndifﬁculties of cross-lingual transfer with order dif-\nferences: A case study on dependency parsing. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , pages\n2440–2452, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nMiltiadis Allamanis, Hao Peng, and Charles A. Sut-\nton. 2016. A convolutional attention network for\nextreme summarization of source code. In Proceed-\nings of the 33nd International Conference on Ma-\nchine Learning, ICML 2016, New York City, NY,\nUSA, June 19-24, 2016, volume 48 of JMLR Work-\nshop and Conference Proceedings , pages 2091–\n2100. JMLR.org.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization , pages 65–72, Ann Ar-\nbor, Michigan. Association for Computational Lin-\nguistics.\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa\nTsuruoka. 2016. Tree-to-sequence attentional neu-\nral machine translation. In Proceedings of the 54th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n823–833, Berlin, Germany. Association for Compu-\ntational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 889–898, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nJacob Harer, Chris Reale, and Peter Chin. 2019. Tree-\ntransformer: A transformer-based method for cor-\nrection of tree-structured data. arXiv preprint\narXiv:1908.00449.\nXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018a.\nDeep code comment generation. In Proceedings of\nthe 26th Conference on Program Comprehension ,\npage 200–210, New York, NY , USA. Association for\nComputing Machinery.\nXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi\nJin. 2018b. Summarizing source code with trans-\nferred api knowledge. In Proceedings of the Twenty-\nSeventh International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-18 , pages 2269–2275. Interna-\ntional Joint Conferences on Artiﬁcial Intelligence\nOrganization.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2016. Summarizing source code\nusing a neural attention model. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 2073–2083, Berlin, Germany. Association for\nComputational Linguistics.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean\nSenellart, and Alexander Rush. 2017. OpenNMT:\nOpen-source toolkit for neural machine translation.\nIn Proceedings of ACL 2017, System Demonstra-\ntions, pages 67–72, Vancouver, Canada. Association\nfor Computational Linguistics.\nAlexander LeClair, Sakib Haque, Linfgei Wu, and\nCollin McMillan. 2020. Improved code summariza-\ntion via a graph neural network. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics.\nAlexander LeClair, Siyuan Jiang, and Collin McMil-\nlan. 2019. A neural model for generating natural\nlanguage summaries of program subroutines. In\nProceedings of the 41st International Conference on\nSoftware Engineering, page 795–806. IEEE Press.\nYuding Liang and Kenny Qili Zhu. 2018. Automatic\ngeneration of text descriptive comments for code\nblocks. In Thirty-Second AAAI Conference on Ar-\ntiﬁcial Intelligence.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1412–1421, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\nKyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazu-\ntoshi Shinoda, Atsushi Otsuka, Hisako Asano, and\nJunji Tomita. 2019. Multi-style generative reading\ncomprehension. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2273–2284, Florence, Italy. Associa-\ntion for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\n5004\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada. Association for Compu-\ntational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nYusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto,\nAtsushi Miyamoto, and Tadayuki Matsumura. 2019.\nAutomatic source code summarization with ex-\ntended tree-lstm. In 2019 International Joint Con-\nference on Neural Networks (IJCNN) , pages 1–8.\nIEEE.\nVighnesh Shiv and Chris Quirk. 2019. Novel positional\nencodings to enable tree-based transformers. In Ad-\nvances in Neural Information Processing Systems\n32, pages 12081–12091. Curran Associates, Inc.\nGiriprasad Sridhara, Emily Hill, Divya Muppaneni,\nLori Pollock, and K. Vijay-Shanker. 2010. Towards\nautomatically generating summary comments for\njava methods. In Proceedings of the IEEE/ACM In-\nternational Conference on Automated Software En-\ngineering, page 43–52, New York, NY , USA. Asso-\nciation for Computing Machinery.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in Neural Information Pro-\ncessing Systems 27 , pages 3104–3112. Curran As-\nsociates, Inc.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nYao Wan, Zhou Zhao, Min Yang, Guandong Xu,\nHaochao Ying, Jian Wu, and Philip S Yu. 2018. Im-\nproving automatic source code summarization via\ndeep reinforcement learning. In Proceedings of the\n33rd ACM/IEEE International Conference on Auto-\nmated Software Engineering, pages 397–407. ACM.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822, Florence, Italy. Associa-\ntion for Computational Linguistics.\nBolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019.\nCode generation as a dual task of code summariza-\ntion. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems\n32, pages 6563–6573. Curran Associates, Inc.\nXin Xia, Lingfeng Bao, David Lo, Zhenchang Xing,\nAhmed E. Hassan, and Shanping Li. 2018. Mea-\nsuring program comprehension: A large-scale ﬁeld\nstudy with professionals. In Proceedings of the 40th\nInternational Conference on Software Engineering ,\nICSE ’18, page 584, New York, NY , USA. Associa-\ntion for Computing Machinery.\nYongjian You, Weijia Jia, Tianyi Liu, and Wenmian\nYang. 2019. Improving abstractive document sum-\nmarization with salient information modeling. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2132–\n2141, Florence, Italy. Association for Computa-\ntional Linguistics.\nJian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun,\nand Xudong Liu. 2020. Retrieval-based neural\nsource code summarization. In Proceedings of the\n42nd International Conference on Software Engi-\nneering. IEEE.\n5005\nA Hyper-Parameters\nTable 7 summarizes the hyper-parameters that we\nused in our experiments.\nHyper-parameter Value\nEmbedding k 16\nModel\nl 6\nh 8\ndmodel 512\ndk,dv 64\ndff 2048\nTraining\ndropout 0.2\noptimizer Adam\nlearning rate 0.0001\nbatch size 32\nTesting beam size 4\nTable 7: Hyper-parameters in our experiments. l and\nh indicates the number of layers and heads in Trans-\nformer respectively. krefers to the clipping distance in\nrelative position representations in Transformer.\nB Recurrent Encoder-Decoder vs.\nTransformer on Python Dataset\nModels BLEU METEOR ROUGE-L\nSeq2seq 30.57 17.86 43.64\nSeq2seq ∗ 29.08 17.12 42.97\nTransformer 31.08 18.57 44.31\nTransformer ∗ 31.38 18.69 44.68\nTable 8: Comparison between recurrent sequence-to-\nsequence (Seq2seq) model and Transformer on the\nPython dataset. ∗ indicates models are equipped with\nthe copy attention mechanism.\nWhile conducting our study using the Trans-\nformer on the Python dataset, we observed a sig-\nniﬁcant gain over the state-of-the-art methods as\nreported in Wei et al. (2019). However, our ini-\ntial experiments on this dataset using recurrent\nsequence-to-sequence models also demonstrated\nhigher performance compared to the results re-\nport in Wei et al. (2019). We suspect that such\nlower performance is due to not tuning the hyper-\nparameters correctly. So for the sake of fairness\nand to investigate the true advantages of Trans-\nformer, we present a comparison on recurrent\nSeq2seq model and Transformer in Table 8 using\nour implementation.6\n6Our implementation is based on Open-NMT (Klein et al.,\n2017) and PyTorch 1.3.\nWe can see from Table 8, the performance of the\nrecurrent Seq2seq model is much better than the\nresults reported in prior works. However, to our\nsurprise, the copy attention mechanism does not\nresult in improvement for the recurrent Seq2seq\nmodel. When we looked into the training per-\nplexity and the validation performance, we also\nobserved lower performance in comparison to the\nbase recurrent Seq2seq model. In comparison,\nour proposed Transformer-based approach outper-\nforms the recurrent Seq2seq models by a large\nmargin showing its effectiveness for source code\nsummarization.\n5006\nC Qualitative Examples\npublic static terminal find(String with_name) {\nif(with_name == null)\nreturn null;\nelse\nreturn (terminal)all.get(with_name);\n}\nBase Model: lookup a non terminal by name string\nFull Model w/o Relative Position: lookup a terminal terminal by name string\nFull Model w/o Copy Attention: lookup a non terminal by name string\nFull Model: lookup a terminal by name\nHuman Written: lookup a terminal by name string .\npublic static String selectText(XPathExpression expr, Node context) {\ntry {\nreturn (String)expr.evaluate(context, XPathConstants.STRING );\n} catch (XPathExpressionException e) {\nthrow new XmlException(e);\n}\n}\nBase Model: evaluates the xpath expression to a xpath expression .\nFull Model w/o Relative Position: evaluates the xpath expression .\nFull Model w/o Copy Attention Attention: evaluates the xpath expression as a single element .\nFull Model: evaluates the xpath expression as a text string .\nHuman Written: evaluates the xpath expression as text .\npublic CTaggingPanel(\nfinal JFrame parent, final ZyGraph graph, final ITagManager manager) {\nsuper(new BorderLayout());\nmtagsTree = new CTagsTree(parent, graph, manager);\nfinal JScrollPane pane = new JScrollPane(mtagsTree);\npane.setVerticalScrollBarPolicy(\nScrollPaneConstants.VERTICAL_SCROLLBAR_AS_NEEDED);\npane.setHorizontalScrollBarPolicy(\nScrollPaneConstants.HORIZONTAL_SCROLLBAR_AS_NEEDED);\nadd(pane);\nsetBorder(new TitledBorder(new LineBorder(Color.LIGHT_GRAY, NUM, BOOL), STRING));\nsetDoubleBuffered(BOOL);\n}\nBase Model: creates a new dnetscapesslservername dialog .\nFull Model w/o Relative Position: creates a new settings dialog .\nFull Model w/o Copy Attention: creates a new toolbar panel .\nFull Model: creates a new api panel object .\nHuman Written: creates a new panel object .\npublic DSignCsr(JFrameparent, PKCS10CertificationRequest pkcs10Csr, File csrFile,\nPrivateKey signPrivateKey, KeyPairType signKeyPairType,\nX509Certificate verificationCertificate, Provider provider)\nthrows CryptoException{\nsuper(parent, Dialog.ModalityType.DOCUMENT_MODAL);\nthis.pkcs10Csr = pkcs10Csr;\nthis.csrFile = csrFile;\nthis.signPrivateKey = signPrivateKey;\nthis.signKeyPairType = signKeyPairType;\nthis.verificationCertificate = verificationCertificate;\nthis.provider = provider;\nsetTitle(res.getString(STRING));\ninitComponents();\n}\nBase Model: creates a new dsigncsr dialog for a spkac formatted csr .\nFull Model w/o Relative Position: creates a new signer dialog for a pkcs # 10 formatted .\nFull Model w/o Copy Attention: creates a new dsigncsr dialog for a spkac formatted csr .\nFull Model: creates a new dsigncsr dialog for a pkcs # 10 formatted csr .\nHuman Written: creates a new dsigncsr dialog for a pkcs # 10 formatted csr .\nTable 9: Qualitative example of different models’ performance in Java dataset.\n5007\ndef get_hosting_service(name):\ntry:\nreturn hosting_service_registry.get(u'hosting service id', name)\nexcept ItemLookupError:\nreturn None\nBase Model: returns the color limits from the current service name .\nFull Model w/o Relative Position: return the hosting service .\nFull Model w/o Copy Attention: return the name of the service .\nFull Model : return the hosting service name .\nHuman Written: return the hosting service with the given name .\ndef save_pickle(obj, fname):\nwith get_file_obj(fname, 'wb') as fout:\ncPickle.dump(obj, fout, protocol=-1)\nBase Model: pickle object obj to ﬁle fname .\nFull Model w/o Relative Position: save object to ﬁle .\nFull Model w/o Copy Attention: raw data: object obj to ﬁle fname .\nFull Model : save object to ﬁle fname .\nHuman Written: save the object to ﬁle via pickling .\ndef get_temp_dir:\ntemp = get_environ_variable('TMP')\nif temp is None:\ntemp = get_environ_variable('TEMP')\nif temp is None or '' in temp and os.name == 'nt':\ntemp = 'C \\\\temp'\nif temp None or '' in temp and os.name == 'posix':\ntemp = '/tmp'\nreturn temp\nBase Model: returns the name of the sample environment variable .\nFull Model w/o Relative Position: returns the next temporary directory of a ﬁle .\nFull Model w/o Copy Attention: get the directory related to store the stubbed .\nFull Model : return a temporary ﬁlename .\nHuman Written: returns a temporary directory .\ndef get_exploration_memcache_key(exploration_id, version=None):\nif version:\nreturn 'exploration-version %s %s' % exploration_id, version\nelse:\nreturn 'exploration %s' % exploration_id\nBase Model: returns the key for an instance for the project .\nFull Model w/o Relative Position: returns a memcache key for the given version .\nFull Model w/o Copy Attention: returns a memcache for the exploration id .\nFull Model : returns a memcache key for the speciﬁed exploration .\nHuman Written: returns a memcache key for an exploration .\ndef get_svc_avail_path():\nreturn AVAIL_SVR_DIRS\nBase Model: get the actual path .\nFull Model w/o Relative Position: returns a list of services .\nFull Model w/o Copy Attention: return a list of services that are available .\nFull Model : returns a list of available services .\nHuman Written: return list of paths that may contain available services .\ndef volume_attach(provider, names, **kwargs):\nclient.get_client_info()\nclient.extra_action(provider=provider, names=names, action='volume attach',\n**kwargs)\nreturn info\nBase Model: attempt to attach volume .\nFull Model w/o Relative Position: attach volume cli example: .\nFull Model w/o Copy Attention: attach volume cli example: .\nFull Model : attach volume information cli example: .\nHuman Written: attach volume to a server cli example: .\nTable 10: Qualitative example of different models’ performance in Python dataset.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.927881121635437
    },
    {
      "name": "Computer science",
      "score": 0.8594852089881897
    },
    {
      "name": "Source code",
      "score": 0.6921613812446594
    },
    {
      "name": "Pairwise comparison",
      "score": 0.5803734064102173
    },
    {
      "name": "Code (set theory)",
      "score": 0.514761209487915
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4767366647720337
    },
    {
      "name": "Transformer",
      "score": 0.4588615894317627
    },
    {
      "name": "Representation (politics)",
      "score": 0.41585856676101685
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3754175305366516
    },
    {
      "name": "Programming language",
      "score": 0.25266918540000916
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.10325664281845093
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    }
  ],
  "cited_by": 343
}