{
  "title": "Prior Knowledge and Memory Enriched Transformer for Sign Language Translation",
  "url": "https://openalex.org/W4285216477",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2040484645",
      "name": "Tao Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2011399166",
      "name": "Zhou Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2067694021",
      "name": "Meng Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2804038696",
      "name": "Xingshan Zeng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970060478",
    "https://openalex.org/W3206487321",
    "https://openalex.org/W2908497602",
    "https://openalex.org/W2799020610",
    "https://openalex.org/W3034593503",
    "https://openalex.org/W3034765865",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2896487916",
    "https://openalex.org/W3126451397",
    "https://openalex.org/W2522165970",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3093061880",
    "https://openalex.org/W2970756316",
    "https://openalex.org/W2941870244",
    "https://openalex.org/W2962990649",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2759302818",
    "https://openalex.org/W3114337930",
    "https://openalex.org/W3127711005",
    "https://openalex.org/W2970212637",
    "https://openalex.org/W2746301562",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2133564696"
  ],
  "abstract": "This paper attacks the challenging problem of sign language translation (SLT), which involves not only visual and textual understanding but also additional prior knowledge learning (i.e. performing style, syntax). However, the majority of existing methods with vanilla encoder-decoder structures fail to sufficiently explore all of them. Based on this concern, we propose a novel method called Prior knowledge and memory Enriched Transformer (PET) for SLT, which incorporates the auxiliary information into vanilla transformer. Concretely, we develop gated interactive multi-head attention which associates the multimodal representation and global signing style with adaptive gated functions. One Part-of-Speech (POS) sequence generator relies on the associated information to predict the global syntactic structure, which is thereafter leveraged to guide the sentence generation. Besides, considering that the visual-textual context information, and additional auxiliary knowledge of a word may appear in more than one video, we design a multi-stream memory structure to obtain higher-quality translations, which stores the detailed correspondence between a word and its various relevant information, leading to a more comprehensive understanding for each word. We conduct extensive empirical studies on RWTH-PHOENIX-Weather-2014 dataset with both signer-dependent and signer-independent conditions. The quantitative and qualitative experimental results comprehensively reveal the effectiveness of PET.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3766 - 3775\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nPrior Knowledge and Memory Enriched Transformer for\nSign Language Translation\nTao Jin\nZhejiang University\njint_zju@zju.edu.cn\nZhou Zhao†\nZhejiang University\nzhaozhou@zju.edu.cn\nMeng Zhang\nHuawei Noah’s Ark Lab\nzhangmeng92@huawei.com\nXingshan Zeng\nHuawei Noah’s Ark Lab\nzxshamson@gmail.com\nAbstract\nThis paper attacks the challenging problem\nof sign language translation (SLT), which in-\nvolves not only visual and textual understand-\ning but also additional prior knowledge learn-\ning (i.e. performing style, syntax). How-\never, the majority of existing methods with\nvanilla encoder-decoder structures fail to suf-\nﬁciently explore all of them. Based on\nthis concern, we propose a novel method\ncalled Prior knowledge and memory Enriched\nTransformer (PET) for SLT, which incorpo-\nrates the auxiliary information into vanilla\ntransformer. Concretely, we develop gated in-\nteractive multi-head attention which associates\nthe multimodal representation and global sign-\ning style with adaptive gated functions. One\nPart-of-Speech (POS) sequence generator re-\nlies on the associated information to predict\nthe global syntactic structure, which is there-\nafter leveraged to guide the sentence gener-\nation. Besides, considering that the visual-\ntextual context information, and additional\nauxiliary knowledge of a word may appear in\nmore than one video, we design a multi-stream\nmemory structure to obtain higher-quality\ntranslations, which stores the detailed corre-\nspondence between a word and its various rel-\nevant information, leading to a more compre-\nhensive understanding for each word. We con-\nduct extensive empirical studies on RWTH-\nPHOENIX-Weather-2014T dataset with both\nsigner-dependent and signer-independent con-\nditions. The quantitative and qualitative exper-\nimental results comprehensively reveal the ef-\nfectiveness of PET.\n1 Introduction\nRecently, the combination of vision and language\nattracts increasing attention. Sign language transla-\ntion which aims to provide translated natural sen-\ntences for sign language videos is a valuable but\nchallenging task in this topic (Camgoz et al., 2018,\n† corresponding author\nTranslation: Im (ADP) | westen (NOUN) | ist (VERB) | es (PRON) | freundlich (ADJ)\nFigure 1: An example of sign language translation,\nwhere the video frames and the sentence correspond to\neach other. Besides, each word (red) has its syntactic\nattribute (green).\n2020a,b; Jin and Zhao, 2021). Since the visual\nand textual modalities are not aligned strictly in a\nweakly-supervised manner, the difﬁculties of sign\nlanguage translation mainly lie in the multimodal\nrepresentation learning of both modalities and the\nalignments between them. Besides, additional prior\nknowledge (i.e. the performing style of different\nsigners, the common syntactic structures of sen-\ntences) also has a strong inﬂuence on multimodal\nlearning.\nEncoder-decoder structures built upon long\nshort-term memory unit (Hochreiter and Schmidhu-\nber, 1997) (LSTM) or transformer (Vaswani et al.,\n2017) are widely used in end-to-end sign language\ntranslation, which directly generates natural sen-\ntences without intermediate products like gloss se-\nquences. Generally, the encoder extracts and en-\ncodes the sign language information, the decoder\nmakes full use of the encoded results with cross-\nmodal interaction. Camgoz et al. (2018) ﬁrst pro-\nposes the sign language translation task and utilizes\nLSTMs combined with attention mechanism (e.g.\nLuong Attention (Luong et al., 2015), Badanau At-\ntention (Bahdanau et al., 2014)) to solve it. Due to\nthe insufﬁcient capacity to capture the long-range\ntemporal correlations, Camgoz et al. (2020b) re-\nplaces LSTM with transformer, which could cor-\nrelate any two-time steps of sequential features.\nThe stacked attention blocks improve most of the\nmetrics by a large margin. Camgoz et al. (2020a)\ncombines multiple articulatory channels with an-\n3766\nchoring losses and proposes a novel multi-channel\ntransformer architecture for sign language transla-\ntion. Li et al. (2020) employs video segment rep-\nresentation with multiple temporal granularities to\ndevelop a semantic pyramid network. In summary,\nmany endeavors are devoted to the improvement of\ndeep architectures for multimodal representation\nlearning. However, the inﬂuences of additional\nprior knowledge are totally ignored. For example,\nas shown in Fig. 1, the natural sentence has its\nunique syntactic structure.\nMotivated by the above observations, we pro-\npose a new method called prior knowledge and\nmemory enriched transformer for sign language\ntranslation. Speciﬁcally, we develop gated inter-\nactive multi-head attention which associates the\nmultimodal representation and global signing style\nwith adaptive gated functions. Besides, we employ\nsentence templates that consist of POS tags to rep-\nresent the syntactic structures of natural sentences,\nand accordingly, syntax learning is performed by\ndirectly inferring POS tags with the style-speciﬁc\nmultimodal representation. The natural sentences\nare generated conditioned on such auxiliaries. Fur-\nthermore, we ﬁnd that the visual and textual con-\ntext information, and additional auxiliary knowl-\nedge of a word may appear in more than one sign\nlanguage video. For example, a word that comes\nup with different words may lead to various con-\ntextual visual perceptions, and the general gestu-\nral tendency of a word could support the decod-\ning process. Therefore, we design a multi-stream\nmemory structure to store the full-spectrum corre-\nspondence between a word and its various relevant\ninformation in training data. The obtained mem-\nory contents are employed to aid in decoding. We\nconduct extensive empirical studies on the bench-\nmark dataset, RWTH-PHOENIX-Weather-2014T\n(PHOENIX14T) (Camgoz et al., 2018), with both\nsigner-dependent and signer-independent condi-\ntions. The quantitative and qualitative results com-\nprehensively reveal the effectiveness and general-\nization of PET. The main contributions of this paper\ncan be summarized as follows:\n•We propose a new method called prior knowl-\nedge and memory enriched transformer for\nsign language translation, which explores not\nonly multimodal understanding but also the\ninﬂuences of additional prior knowledge on\nmultimodal learning.\n•We develop gated interactive multi-head atten-\ntion by associating the multimodal represen-\ntation and global signing style with adaptive\ngated functions. The POS sequence gener-\nator relies on the style-speciﬁc multimodal\ninformation to predict the syntactic structure,\nwhich is leveraged to guide the natural sen-\ntence generation.\n•We design a multi-stream memory structure\nto store the full-spectrum correspondence be-\ntween a word and its various relevant informa-\ntion in training data, leading to a more com-\nprehensive understanding for each word.\n•The quantitative and qualitative results on the\nchallenging dataset, PHOENIX14 T of both\nsigner-dependent and signer-independent con-\nditions comprehensively reveal the effective-\nness and generalization of PET.\n2 Related Work\n2.1 Sign Language Translation\nSign language recognition (SLR) aims to recognize\nsingle gestures from an input video clip. Many en-\ndeavors are devoted to SLR (Camgoz et al., 2016,\n2017; Cui et al., 2019; Graves et al., 2006; Wang\net al., 2018; Cui et al., 2017). Sign language trans-\nlation is the ﬁnal goal of recognition, which aims\nto directly translate the sign language videos into\nnatural sentences. SLT is similar to video caption-\ning (Jin et al., 2019a, 2020, 2019b; Pei et al., 2019),\nto some extent. Existing methods are categorized\ninto two-stage and end-to-end methods. Two-stage\nmethods ﬁrst transform the videos into gloss (ges-\nture) sequences and then rearrange them to gen-\nerate natural sentences. To guarantee the ﬂuency\nof sentences, some words that do not carry visual\ninformation are added (Camgoz et al., 2018). End-\nto-end sign language translation aims to directly\ntranslate the original sign language videos into nat-\nural sentences without intermediate products. Cam-\ngoz et al. (2018) ﬁrst proposes the sign language\ntranslation task and utilizes both two-stage and end-\nto-end methods to solve it. Camgoz et al. (2018)\nadopts vanilla LSTM-based encoder-decoder struc-\nture. Due to the insufﬁcient capacity to capture the\nlong-range temporal correlations. Camgoz et al.\n(2020b) replaces LSTM with transformer, which\ncould correlate any two-time steps of sequential fea-\ntures. The stacked attention blocks improve most\nof the metrics with a large margin. Li et al. (2020)\n3767\nemploys video segment representation with mul-\ntiple temporal granularities to develop a semantic\npyramid network.\nHowever, the methods mentioned above fail to\nexplore the multimodal understanding and addi-\ntional prior knowledge learning sufﬁciently. In this\npaper, we propose PET to solve this problem.\n3 Approach\nFig. 2 shows the overall framework of prior knowl-\nedge and memory enriched transformer based on\nencoder-decoder structure. We develop gated in-\nteractive multi-head attention in all the attention\nblocks with adaptive gated control of signing style\nembeddings. In the decoder, we treat the sen-\ntence templates which consist of POS tags as the\nsyntax-aware auxiliary for natural sentence gen-\neration. Practically, two consecutive decoding\nblocks (syntactic and textual blocks) rely on the\nstyle-speciﬁc multimodal representation to predict\nthe target words. Furthermore, we design a multi-\nstream memory structure to enhance the compre-\nhensive understanding for each word.\n3.1 Style-Aware Gated Interactive Encoder\nFollowing (Camgoz et al., 2020b), we utilize the\n2D-CNN (Tan and Le, 2019) pre-trained with\nrecognition task (Koller et al., 2019) to extract vi-\nsual features of sign language videos. Concretely,\nwe ﬁrst sample video frames and then send them\nto 2D-CNN. For convenience, we use I ∈RTi×d\nto denote the extracted features, where Ti is the\nnumber of video frames. As shown in Fig. 2,\nthe encoder consists of stacked attention blocks.\nConsidering the fact that different signers have\ncorresponding performing styles (i.e. body, pose),\nwe perform adaptive gated interaction for the self-\nattention mechanism, which associates the visual\nrepresentation and signing style with adaptive gated\nfunctions. Note that, for each speciﬁc signer, we\nobtain the performing style embedding gby simply\nmean-pooling all the visual features of the corre-\nsponding signer (both videos and frames) in the\ndataset. Speciﬁcally, the self-attention layer is for-\nmulated as:\nGI_Self(I) =GI_MH(I,I|g) (1)\nwhere “GI”, “Self”, “MH” denote gated interac-\ntive, self attention, and multi-head attention, respec-\ntively. The ﬁrst “ I” in GI_MH(.) denotes query,\nthe second “I” denotes key and value. Further, the\ncalculation of each head is expressed as:\nGI_MH(I,I|g) = [hd1,...,hd h]W1\nhdi = GI_AT(IWQ\ni ,IW K\ni ,IW V\ni |gWG\ni )\n(2)\nwhere [.] denotes concatenation operation, hdi\ndenotes the output of i-th head, W1 ∈ Rd×d,\nWQ\ni ,WK\ni ,WV\ni ∈ Rd×d\nh are trainable variables.\n“GI_AT” takes the signing style embedding into\nconsideration and the process is as below:\nGI_AT(Q,K,V |s) =softmax(Q\n′\nK\n′T\n√dk\n)V (3)\nwhere we utilize Q, K, V, and sto denote IWQ\ni ,\nIWK\ni , IWV\ni , and gWG\ni to save space. Q\n′\nand\nK\n′\nare the results of style-speciﬁc interaction with\nadaptive gated functions:\nQ\n′\n=(1+Gq)⊙Q, Gq=σ([s,QM,s⊙QM]Wq)\nK\n′\n=(1+Gk)⊙K, Gk=σ([s,KM,s⊙KM]Wk)\n(4)\nwhere ⊙denotes element-wise multiplication, σ(.)\ndenotes sigmoid gated function, the subscript of\nKM ∈R\nd\nh and QM ∈R\nd\nh denotes mean-pooling,\nWq, Wk ∈R\n3d\nh ×d\nh are trainable variables. We em-\nploy residual connection and layer normalization\nfollowing the self-attention layer:\nI\n′\n= LN(I+ GI_Self(I)) (5)\nwhere “LN” denotes layer normalization, followed\nby a feed-forward layer (FFN) to introduce non-\nlinear transformation:\nFFN(I\n′\n) =Max(0,I\n′\nW2 + b2)W3 + b3\nI\n′′\n= LN(I\n′\n+ FFN(I\n′\n))\n(6)\nwhere W2 ∈Rd×4d, b2 ∈R4d, W3 ∈R4d×d, b3 ∈\nRd are trainable variables, I\n′′\n∈RTi×d represents\nthe encoded visual features.\n3.2 Syntax-Aware Memory Enriched\nDecoder\nThe decoder also consists of stacked attention\nblocks as shown in Fig. 2. Note that the struc-\ntures of syntactic and textual blocks are the same as\nthose of encoder-decoder attention blocks. Specif-\nically, to predict the word yte at te-th time step,\nwe utilize E<te ∈Rte×d that denotes the embed-\ndings of “BOS” token and the words whose time\nsteps are less than te. The process of the masked\n3768\nGated Interactive Self-Attention Block\n Encoder-Decoder Attention Block\nSyntactic Block\nTextual Block\nTAG\nMulti-Stream Memory Reasoning\nEMB\nTAG\nEMB\nTAG\nEMB\nWord Embedding\nPre-trained Visual CNN\n...\n...\n...\nMulti-Stream Memory Structure\nIS_Self\nAdd & Normalize\nFeed-Forward Layer\nAdd & Normalize\nGI_Self\nAdd & Normalize\nFeed-Forward Layer\nAdd & Normalize\nGI_Self\nAdd & Normalize\nGI_MH (Encoder-Decoder)\nAdd & Normalize\nFeed-Forward Layer\nAdd & Normalize\nGated Interactive Self-Attention Block\nEncoder-Decoder Attention Block\n<BOS>\n<EOS>\nFigure 2: Left is the overall framework of PET, where the encoder processes extracted video features with stacked\ngated interactive self-attention blocks and the decoder makes full use of the visual features with encoder-decoder\nattention blocks. Note that the structures of syntactic and textual blocks are the same as those of encoder-decoder\nattention blocks. “TAG” and “EMB” denote POS tag and embedding, respectively. The multi-stream memory\nstructure is leveraged for auxiliary decoding, where v, u, and x denote visual, textual, and syntactic memory,\nrespectively. Right is the structures of self-attention block and encoder-decoder attention block.\nself-attention layer and the following normalization\nlayer is formulated as:\nE\n′\n<te = LN(E<te + GI_Self(E<te )) (7)\nwhere we also perform adaptive gated interaction\nfor self-attention mechanism. The obtained E\n′\n<te\nare utilized to correlate the encoded visual features\nin the following layer with cross-modal attention:\nE\n′′\n<te = LN(E\n′\n<te + GI_MH(E\n′\n<te ,I\n′′\n|g))\nO= LN(E\n′′\n<te + FFN(E\n′′\n<te ))\n(8)\nwhere E\n′\n<te and I\n′′\nare treated as query and key,\nrespectively. O∈Rte×d denotes the output of one\nencoder-decoder attention block.\n3.2.1 Syntax-Aware Decoding\nSince the decoder has N attention blocks, we dis-\ntinguish the output of different blocks with super-\nscripts, O1, O2,...,ON ∈Rte×d. Note that ON−1\nand ON are the output of syntactic and textual\nblocks, respectively. We calculate the probability\ndistributions of different POS tags as:\nPs,te = softmax(WsON−1\nte ) (9)\nwhere Ws ∈RNs×d is trainable, Ns is the vocabu-\nlary size of POS tags. We combine the syntactic in-\nformation and ON−1\nte for the subsequent process. In\npractice, we project the POS tags into correspond-\ning embeddings: (ON−1\nte )\n′\n= ON−1\nte + Es\nte , where\nEs\nte denotes POS embedding atte-th time step. The\nobtained synthetic representation (ON−1\nte )\n′\nis con-\nsidered as the input of textual block. Due to the\nspace limitation, we omit the calculation in textual\nblock which is similar to Eqns. 7 and 8. The output\nof textual block is used to predict words:\nPb,te = softmax(WpON\nte ) (10)\nwhere Wp ∈RNw×d is also trainable, Nw is the vo-\ncabulary size of words. Overall, we jointly model\nthe multimodal representation and global syntactic\nstructure for sign language translation by develop-\ning an end-to-end trainable neural network.\n3.2.2 Multi-Stream Memory Structure\nWe develop a multi-stream memory structure for\nauxiliary decoding. The rationale behind this de-\nsign is that a word in the vocabulary may appear in\nmultiple sign language videos. Since a word that\ncomes up with different words may lead to various\ncontextual visual perceptions and one word may\ncorrespond to more than one syntactic category, the\nmemory structure is developed to capture the de-\ntailed relevant information from different sign lan-\nguage videos where the same word appears, leading\nto a comprehensive understanding for this word.\n(1). Weakly-Aligned Visual Memory:The mem-\nory structure is developed to store the descriptive\ninformation for each word in the vocabulary. We\n3769\nconstruct a dictionary ⟨w,r⟩to record the words\nwand corresponding representation r. Since the\nﬁne-grained alignments between natural words and\nvideo frames are not provided, we could not di-\nrectly obtain the visual memory. However, the\nend-to-end training of PET provides the weakly-\nsupervised alignments through the cross-modal in-\nteraction in the encoder-decoder attention blocks.\nTherefore, we adopt a separate training scheme.\nConcretely, we ﬁrst train a basic sign language\ntranslation model with prior knowledge enriched\ntransformer introduced in previous sections to ac-\nquire the weakly-supervised alignments between\nwords and video frames. In practice, we only keep\nthe cross-modal attention weights in the textual\nblock. The visual context information vj,i for the\nj-th word i-th head is modeled as:\nvj,i =\n∑Nv\nnv=1\n∑Nf\nnf =1(ai\nnv,nf fv,i\nnv,nf )\n∑Nv\nnv=1\n∑Nf\nnf =1(ainv,nf )\nvj = [vj,1,...,v j,i,...,v j,h]\n(11)\nwhere Nv denotes the number of related videos in\nthe training set, Nf denotes the number of frames.\nNote that we only retain the top-Nf relevant video\nframes to reduce the invalid information. ai\nnv,nf\ndenotes nf -th attention weight among the top-Nf\nweights and fv,i\nnv,nf denotes the corresponding vi-\nsual features in i-th head. Note that we only focus\non the visual features of the last encoding block.\nThe context features are normalized to make the\nmagnitude consistent for words with different fre-\nquencies. The ﬁnal context information vj is ob-\ntained by concatenating the results of all the heads.\n(2). Global Syntactic Memory:Considering the\nfact that a word appearing in multiple sentences\nmay have different syntactic information, we calcu-\nlate the ratio of different POS categories for each\nword. The syntactic representation uj for the j-th\nword is modeled as:\nuj =\n∑Ns\nns=1\nbs\nns fs\nns ,\n∑Ns\nns=1\nbs\nns = 1 (12)\nwhere bs\nns and fs\nns denote the weight and embed-\nding of ns-th POS tag, respectively.\n(3). Adjacent Textual Memory: The vanilla\ntransformer-based decoder does not model the com-\npatibility between adjacent words explicitly. Thus,\nthe textual memory is designed to capture the infor-\nmation of adjacent words. Concretely, we set the\nmaximal adjacent step to Na, which means that we\nretain the word embeddings of adjacent words and\nthe threshold is Na. The context representation xj\nfor the j-th word is modeled as:\nxj =\n∑Nv\nnv=1\n∑2Na+1\nna=1 ft\nnv,na\nNv(2Na + 1) (13)\nwhere ft\nnv,na denotes the na-th word embedding\namong the 2Na + 1adjacent embeddings. We also\nemploy normalization for the ﬁnal result. In sum-\nmary, we obtain the multi-stream memory struc-\nture which records full-spectrum information rj\nfor each word wj with a map structure: ⟨wj,rj⟩=\n⟨wj,{vj,uj,xj}⟩.\n3.2.3 Memory Enriched Decoding\nWe employ the constructed multi-stream memory\nstructure to build an auxiliary decoding system,\nwhere the translation results are further combined\nwith the generated sentences by the syntax-aware\ndecoding system. In this way, the translation qual-\nity is improved.\nIn detail, the memory enriched decoding system\nis built upon the backbone of the syntax-aware de-\ncoding system as an auxiliary module. The proba-\nbility distributions of different words are calculated\nsimilarly to Eqn. 10:\nPm,te = softmax(Qte ) (14)\nwhere Qte ∈RNw denotes the relevance scores\nof different words and Qte,j ∈R denotes the j-th\nelement among them. We employ Qte,j to measure\nthe qualiﬁcation of j-th word for te-th time step\nbased on its memory contents:\nQte,j =wT\np tanh(Wv[vj,ON\nte ]+Wu[uj,Es\nte ]\n+ Wx[xj,Ey\nte-1])\n(15)\nwhere we concatenate the memory contents (vj, uj,\nxj) with corresponding representation ( ON\nte , Es\nte ,\nEy\nte-1) at te-th time step. For instance,uj,Es\nte ∈Rd\nboth denote syntactic information, xj,Ey\nte-1 ∈Rd\nboth denote textual information. Wv,Wu,Wx ∈\nRd×2d,wp ∈Rd are all trainable variables.\n3.3 Training\nThe optimization goal of sign language translation\nis to minimize the cross-entropy loss function de-\nﬁned as accumulative loss from all the time steps.\nConsequently, the syntax-aware decoder is trained\nby minimizing the combined loss:\n3770\nLb =−\nTe∑\nte=1\n[\nlogPb,te (yte )+λlogPs,te (ste )\n]\n(16)\nwhere yte and ste denote the ground-truth word\nand POS tag at te-th time step, respectively. λis\na hyper-parameter to balance the two losses. In\npractice, we set it to 0.5. The memory enriched\ndecoder is trained in a similar way:\nLm = −\nTe∑\nte=1\nlogPm,te (yte ) (17)\nThe syntax-aware decoder and memory enriched\ndecoder are trained in order. We ﬁx the trainable\nvariables except for those in Eqn. 15 when training\nmemory enriched decoder. During inference, we\ncombine the generated results of both decoders.\n4 Experiments\nIn this section, we present the experimental settings\nof sign language translation and report the results\non the benchmark datasets.\nTable 1: The statistical results of PHOENIX14T, where\nthe total number of samples is 8257.\nSigner 1 2 3 4 5 6 7 8 9\nAll 2191 95 683 1207 1933 47 866 966 269\n4.1 Dataset and Protocols\nPHOENIX14T (Signer-Dependent) is the ﬁrst\ncomplete sign language understanding dataset,\nwhere a training or testing sample contains a sign\nlanguage video and the corresponding signer, gloss\nannotations, natural language translation. Con-\ncretely, PHOENIX14T is labeled by 9 different\nsigners (the training, validation, and test sets all\ncontain these signers) with a vocabulary of 1066\ndifferent sign glosses. In general, one gloss may\ncorrespond to multiple natural words, and some\nwords that do not carry visual information are\nadded to guarantee the ﬂuency of sentences, lead-\ning to a vocabulary of 2887 words for translation\ninto German language.\nPHOENIX14T (Signer-Independent) is ob-\ntained by re-dividing the original PHOENIX14 T\ndataset. Since the 9 signers are in both the training\nset and test set, there are no unseen signers for eval-\nuating the generalization. We simply choose the\nTable 2: Evaluation results on PHOENIX14 T (Signer-\nDependent), where B@{1, 2, 3, 4} denotes BLEU-{1,\n2, 3, 4} and R denotes ROUGE-L.\nMethod PHOENIX14T\nB@1 B@2 B@3 B@4 R\nMultitask 37.22 23.88 17.08 13.25 36.28\nDeepHand 38.50 25.64 18.59 14.56 38.05\nMul-Ch. - - - 19.51 45.90\nNSLT 32.24 19.03 12.83 9.58 31.80\nTSPNet 36.10 23.12 16.88 13.41 34.96\nSL-Trans. 47.20 34.46 26.75 21.80 -\nST-Trans. 48.61 35.97 28.37 23.65 -\nSTMC-T 48.73 36.53 29.03 24.00 46.77\nPET 49.54 37.19 29.30 24.02 49.97\nTable 3: Evaluation results on PHOENIX14 T (Signer-\nIndependent), where * denotes that we implement the\nmethods by ourselves, since none of the previous work\nconducts experiments on signer-independent setting.\nMethod PHOENIX14T\nB@1 B@2 B@3 B@4 R\nNSLT* 26.01 13.84 8.95 6.28 25.22\nTSPNet* 28.10 16.81 11.82 9.15 31.00\nSL-Trans.* 40.15 26.70 19.22 14.78 40.22\nPET 41.72 28.97 21.36 16.94 42.45\nsigners 8, 9 (1235 samples) for testing and the other\nsigners (7022 samples) for training and validation,\nthe statistical info is shown in Table 1.\nWe follow the commonly used protocol\nSign2Text (S2T)in the previous work (Camgoz\net al., 2020b), which aims to directly translate the\nsign language videos into natural sentences with-\nout converting the input into intermediate prod-\nucts. Since the visual and textual modalities are\nnot aligned strictly in a weakly-supervised man-\nner, the difﬁculties of Sign2Text mainly lie in the\nmultimodal alignments.\n4.2 Implementation Details\nFramework: Following (Camgoz et al., 2020b),\na modiﬁed version of JoeyNMT (Kreutzer et al.,\n2019) is employed to implement PET. We utilize\nPyTorch and Tensorﬂow frameworks. Except for\nthe CTC beam search decoding module which is\n3771\nTable 4: Evaluation results of style-speciﬁc interaction, where P14 T (SD) and P14T (SI) denote PHOENIX14T\nwith signer-dependent and singer-independent settings.\nMethod P14T (SD) P14 T (SI)\nB@1 B@2 B@3 B@4 R B@1 B@2 B@3 B@4 R\nw/o. GI 48.61 35.24 27.58 22.89 48.34 40.22 27.36 20.05 15.42 40.36\nAdd 49.04 36.05 28.32 23.40 48.88 40.91 28.19 20.65 16.36 40.76\nEnc. 49.45 36.57 28.95 23.45 49.15 41.37 28.54 20.57 16.66 41.54\nDec. 49.30 36.32 28.84 23.42 49.08 41.43 28.52 20.89 16.72 41.28\nPET 49.54 37.19 29.30 24.02 49.97 41.72 28.97 21.36 16.94 42.45\nimplemented with Tensorﬂow, the other modules\nare developed with PyTorch.\nNetwork Details: The hidden size is set to\n512 for all the multi-head attention mechanisms.\nThe numbers of heads and attention blocks are\n8 and 3, respectively. The ground-truth POS\ntags could be obtained by Stanford POS Tagger,\nwhich are divided into 13 categories: ADJ, ADV ,\nADP, VERB, NOUN, DET, PRON, AUX, CONJ,\nPROPN, NUM, UNK, PUNCT, we project them\ninto 512-dimensional syntactic embeddings. We\ntrain all of the networks from scratch.\nTraining: In the training stage, we utilize Adam\nalgorithm (Kingma and Ba, 2014) to optimize the\nloss function. The batch size is set to 64. The\nlearning rate is set to5×10−4 initially. We evaluate\nour network every 100 iterations. If the metric on\nvalidation set does not improve for 9 evaluation\nsteps, we decrease the learning rate by a factor of\n0.5. When the learning rate is less than 10−6, we\nﬁnish the training stage.\nTesting: Since the test set may have unseen sign-\ners, we calculate the style embedding with mean-\npooling operation for the acquired visual features\nsimilarly. Beam search is a commonly used method\nto decode words during evaluation. We adopt the\nbeam size 5. We employ the commonly-used met-\nrics, BLEU-n and ROUGE-L.\n4.3 Compared Baseline Methods\nNSLT (Camgoz et al., 2018): NSLT ﬁrst proposes\nthe SLT task and employs LSTM-based structure\nto translate sign language videos.\nMultitask (Orbay and Akarun, 2020): Multitask\nemploys joint learning scheme to enhance the SLT\nperformance.\nDeepHand (Orbay and Akarun, 2020): DeepHand\ntransfers the knowledge of hand dataset to the SLT\ntask.\nFigure 3: The trade-off between different losses in Eqn.\n16, where we set λ= 0as the baseline.\nSL-Trans. (Camgoz et al., 2020b): SL-Trans. is\nthe recent mainstream method for SLT, the encoder\nand decoder both consist of Transformer modules.\nTSPNet (Li et al., 2020): TSPNet employs video\nsegment representation with multiple temporal\ngranularities to develop a semantic pyramid net-\nwork.\nMul-Ch. (Camgoz et al., 2020a): Mul-Ch. com-\nbines multiple articulatory channels with anchoring\nlosses and proposes a novel multi-channel trans-\nformer architecture for sign language translation.\nST-Trans. (V oskou et al., 2021): ST-Trans. equips\nTransformer with stochastically competing linear\nunits and performs variational Bayesian inference\nover all connection weights, throughout the net-\nwork.\nSTMC-T (Yin and Read, 2020): STMC-T em-\nploys spatial-temporal multi-channel Transformer\nto solve the SLT task.\n4.4 Quantitative Results\nWe compare PET with the recent state-of-the-art\nmethods. Following the previous work (Cam-\ngoz et al., 2020b), for PHOENIX14 T (Signer-\nDependent), we develop the gloss-based PET by\n3772\nTable 5: Evaluation of memory-enriched decoding\nMethod P14T (SD) P14 T (SI)\nB@1 B@2 B@3 B@4 R B@1 B@2 B@3 B@4 R\nw/o. Vis 49.63 36.28 28.58 23.40 49.32 41.24 28.35 20.89 16.30 41.46\nw/o. Tex 49.52 36.54 28.83 23.44 49.12 41.05 28.16 20.74 16.44 40.93\nw/o. Syn 49.69 36.42 28.75 23.55 49.48 41.58 28.55 21.07 16.64 41.32\nw/o. Mem 48.94 35.64 28.07 22.71 49.05 40.54 27.53 20.25 15.56 40.64\nPET 49.54 37.19 29.30 24.02 49.97 41.72 28.97 21.36 16.94 42.45\nadding the gloss supervision with CTC loss in the\nencoder. Table 2 shows the experimental results,\nwe could ﬁnd that PET (model-based) outperforms\nall the model-based and feature-based methods,\nNSLT (Camgoz et al., 2018), Multitask (Orbay\nand Akarun, 2020), DeepHand (Orbay and Akarun,\n2020), SL-Trans. (Camgoz et al., 2020b), TSPNet\n(Li et al., 2020), Mul-Ch. (Camgoz et al., 2020a),\nST-Trans.(V oskou et al., 2021) and STMC-T (Yin\nand Read, 2020) on all the metrics. In particu-\nlar, PET achieves 49.97% on ROUGE-L, making a\nlarge improvement of 3.20% over STMC-T.\nTable 3 shows the results on PHOENIX14 T\n(Signer-Independent), we implement several state-\nof-the-art methods manually, since none of the pre-\nvious work conducts experiments on the signer-\nindependent setting (PET is model-based method,\nso we mainly reproduce the model-based methods,\nsince the methods of other types are compatible\nwith PET). Note that, to keep fairness, we employ\nthe same method of feature extraction in the origi-\nnal paper for NSLT, TSPNet, and SL-Transformer,\nrespectively. The experimental results demonstrate\nthe generalization of PET for unseen signers.\n4.5 Ablation Study\nIn this section, we evaluate the effectiveness of all\nthe contributions with ablation experiments.\n4.5.1 Effect of Adaptive Gated Interaction\nAs shown in Table 4, we design four control exper-\niments to demonstrate the effectiveness of adaptive\ngated interaction, where w/o. GIdenotes that we\nremove the adaptive gated interaction from all at-\ntention blocks and keep the other contributions,\nAdd denotes that we add the style embedding to\nthe multimodal features, Enc (only)denotes that\nwe only keep the adaptive gated interaction in the\nencoder, while Dec (only)denotes that we discard\nthe adaptive gated interaction in the encoder. It\nis observed that PET outperforms four ablation\nmethods on the benchmark datasets and w/o. GI\nachieves the worst performances on both BLEU\nand ROUGE-L, which demonstrates that the trans-\nlation results beneﬁt from the style information.\nThe remaining ablation results illustrate that gated\ninteraction is better than naive addition. In addi-\ntion, the adaptive gated interaction enhances the\nmultimodal alignments, corresponding results are\nshown in the appendix.\n4.5.2 Effect of Syntax-Aware Auxiliary\nWe adjust the ratio of different losses in Eqn. 16\nand obtain the experimental results that are shown\nin Fig. 3. To make the comparison more intu-\nitive, we set λ = 0 as the baseline and provide\nthe relative performances of BLEU-1 and BLEU-4\non PHOENIX14T (SD). We ﬁnd that the perfor-\nmances improve as the λincreases when λis less\nthan 0.5. Subsequently, the performances are be-\nginning to level off. Such results demonstrate the\neffectiveness of syntax-aware auxiliary.\n4.5.3 Effect of Memory-Enriched Decoding\nAs shown in Table 5, we also design several control\nexperiments to evaluate the impact of the memory\nenriched decoding, where w/o. Memdenotes the\nmodel without memory mechanism, w/o. Visde-\nnotes the model only without visual memory, w/o.\nTex, w/o. Syn denote the models without textual\nmemory and syntactic memory, respectively. We\nﬁnd that PET outperforms all the ablation methods\non both BLEU-4 and ROUGE-L. Particularly, com-\npared with w/o. Mem, PET achieves a signiﬁcant\nimprovement on BLEU-4 (1.38% for SI, 1.31% for\nSD).\n4.6 Qualitative Results\nWe would like to investigate the generation process\nof our model by qualitative results in this section.\n3773\nTable 6: Qualitative results of PET, where “Ref” de-\nnotes reference, “SL-Trans.” denotes SL-Transformer.\nAs the annotations in the PHOENIX14 T dataset are\nin German, we share both the produced sentences and\ntheir translations in English. Note that the words high-\nlighted in red are those that require critical translation,\nthe words highlighted in blue are the failure cases of\ncurrent mainstream method SL-Transformer.\nRef: und zum wochenende wird es dann sogar wieder ein bisschen kälter .( and at the weekend it even gets a little colder again . )SL-Trans.: und der januar .( and january . )PET: und das wird dann am wochenende ein bisschen kälter .( and that gets a bit colder on the weekend . )\nRef: ganz ähnliche temperaturen wie heute zwischen sechs und elf grad .( very similar temperatures as today between six and eleven degrees . )SL-Trans.: hier und da ähnliche temperaturen wie heute meist ein grad .( here and there temperatures similar to today, mostly one degree . )PET: ähnliches wetter heute nacht nur sechs bis elf grad .( similar weather tonight only six to eleven degrees . )Ref: deutschland liegt morgen unter hochdruckeinﬂuss der die wolken weitgehendvertreibt .( tomorrow germany will be under the inﬂuence of high pressure which willlargely drive away the clouds . )SL-Trans.: in deutschland liegt morgen unter tiefdruckeinﬂuss und wolken .( in germany tomorrow is under the inﬂuence of low pressure and clouds . )PET: Morgen wird Deutschland von hohem Druck betroffen sein .( tomorrow germany will be hit by high pressure . )\nHere we provide some sign language translation\nexamples in Table 6. As the annotations in the\nPHOENIX14T dataset are in German, we share\nboth the produced sentences and their translations\nin English. Note that the words highlighted in red\nare those that require critical translation, the words\nhighlighted in blue are the failure cases of current\nmainstream method SL-Transformer. Beneﬁting\nfrom the style-speciﬁc interaction, syntax-aware\nauxiliary, and memory enriched decoding, PET\ncould accurately translate some detailed informa-\ntion compared with SL-Transformer and retain the\nwhole contents of the ground truth better than SL-\nTransformer, which demonstrates the effectiveness\nagain.\n5 Conclusion\nIn this paper, we have proposed a new method\ncalled prior knowledge and memory enriched trans-\nformer for sign language translation. Speciﬁcally,\nwe develop the adaptive gated interaction which as-\nsociates the multimodal representation and global\nsigning style in all the attention blocks. One POS\nsequence generator relies on the associated infor-\nmation to predict the global syntactic structure,\nwhich is thereafter leveraged to guide the sentence\ngeneration. Besides, considering that the visual and\ntextual context information, and additional auxil-\niary knowledge of a word appear in more than one\nvideo, we design a memory structure to store the\nfull-spectrum correspondence between a word and\nits various relevant information in the training data.\nThe experimental results reveal the effectiveness\nand generalization of PET.\nAcknowledgments\nThis work was supported in part by the Na-\ntional Key R&D Program of China under Grant\nNo.2020YFC0832505, National Natural Science\nFoundation of China under Grant No.61836002,\nNo.62072397 and Zhejiang Natural Science Foun-\ndation under Grant LR19F020006.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nNecati Cihan Camgoz, Simon Hadﬁeld, Oscar Koller,\nand Richard Bowden. 2017. Subunets: End-to-end\nhand shape and continuous sign language recogni-\ntion. In 2017 IEEE International Conference on\nComputer Vision (ICCV), pages 3075–3084. IEEE.\nNecati Cihan Camgoz, Simon Hadﬁeld, Oscar Koller,\nHermann Ney, and Richard Bowden. 2018. Neu-\nral sign language translation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 7784–7793.\nNecati Cihan Camgoz, Ahmet Alp Kindirouglu, and\nLale Akarun. 2016. Sign language recognition\nfor assisting the deaf in hospitals. In Interna-\ntional Workshop on Human Behavior Understand-\ning, pages 89–101. Springer.\nNecati Cihan Camgoz, Oscar Koller, Simon Hadﬁeld,\nand Richard Bowden. 2020a. Multi-channel trans-\nformers for multi-articulatory sign language transla-\ntion. In European Conference on Computer Vision,\npages 301–319. Springer.\nNecati Cihan Camgoz, Oscar Koller, Simon Hadﬁeld,\nand Richard Bowden. 2020b. Sign language trans-\nformers: Joint end-to-end sign language recognition\nand translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 10023–10033.\nRunpeng Cui, Hu Liu, and Changshui Zhang. 2017.\nRecurrent convolutional neural networks for con-\ntinuous sign language recognition by staged opti-\nmization. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n7361–7369.\nRunpeng Cui, Hu Liu, and Changshui Zhang. 2019. A\ndeep neural framework for continuous sign language\n3774\nrecognition by iterative training. IEEE Transactions\non Multimedia, 21(7):1880–1891.\nAlex Graves, Santiago Fernández, Faustino Gomez,\nand Jürgen Schmidhuber. 2006. Connectionist\ntemporal classiﬁcation: labelling unsegmented se-\nquence data with recurrent neural networks. In Pro-\nceedings of the 23rd international conference on Ma-\nchine learning, pages 369–376.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nTao Jin, Siyu Huang, Ming Chen, Yingming Li, and\nZhongfei Zhang. 2020. Sbat: Video captioning with\nsparse boundary-aware transformer. arXiv preprint\narXiv:2007.11888.\nTao Jin, Siyu Huang, Yingming Li, and Zhongfei\nZhang. 2019a. Low-rank hoca: Efﬁcient high-order\ncross-modal attention for video captioning. arXiv\npreprint arXiv:1911.00212.\nTao Jin, Yingming Li, and Zhongfei Zhang. 2019b. Re-\ncurrent convolutional video captioning with global\nand local attention. Neurocomputing, 370:118–127.\nTao Jin and Zhou Zhao. 2021. Contrastive disentangled\nmeta-learning for signer-independent sign language\ntranslation. In Proceedings of the 29th ACM Interna-\ntional Conference on Multimedia, pages 5065–5073.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nOscar Koller, Necati Cihan Camgoz, Hermann Ney,\nand Richard Bowden. 2019. Weakly supervised\nlearning with multi-stream cnn-lstm-hmms to dis-\ncover sequential parallelism in sign language videos.\nIEEE transactions on pattern analysis and machine\nintelligence, 42(9):2306–2320.\nJulia Kreutzer, Jasmijn Bastings, and Stefan Riezler.\n2019. Joey nmt: A minimalist nmt toolkit for\nnovices. arXiv preprint arXiv:1907.12484.\nDongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Ben\nSwift, Hanna Suominen, and Hongdong Li. 2020.\nTspnet: Hierarchical feature learning via tempo-\nral semantic pyramid for sign language translation.\narXiv preprint arXiv:2010.05468.\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. arXiv preprint\narXiv:1508.04025.\nAlptekin Orbay and Lale Akarun. 2020. Neural sign\nlanguage translation by learning tokenization. arXiv\npreprint arXiv:2002.00479.\nWenjie Pei, Jiyuan Zhang, Xiangrong Wang, Lei Ke,\nXiaoyong Shen, and Yu-Wing Tai. 2019. Memory-\nattended recurrent network for video captioning. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 8347–\n8356.\nMingxing Tan and Quoc Le. 2019. Efﬁcientnet: Re-\nthinking model scaling for convolutional neural net-\nworks. In International Conference on Machine\nLearning, pages 6105–6114. PMLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nAndreas V oskou, Konstantinos P Panousis, Dimitrios\nKosmopoulos, Dimitris N Metaxas, and Sotirios\nChatzis. 2021. Stochastic transformer networks\nwith linear competing units: Application to end-to-\nend sl translation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n11946–11955.\nShuo Wang, Dan Guo, Wen-gang Zhou, Zheng-Jun\nZha, and Meng Wang. 2018. Connectionist tempo-\nral fusion for sign language translation. In Proceed-\nings of the 26th ACM international conference on\nMultimedia, pages 1483–1491.\nKayo Yin and Jesse Read. 2020. Better sign language\ntranslation with stmc-transformer. arXiv preprint\narXiv:2004.00588.\n3775",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8784038424491882
    },
    {
      "name": "Natural language processing",
      "score": 0.6887452602386475
    },
    {
      "name": "Transformer",
      "score": 0.6705418825149536
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5732263326644897
    },
    {
      "name": "Sentence",
      "score": 0.5692915320396423
    },
    {
      "name": "Machine translation",
      "score": 0.511195957660675
    },
    {
      "name": "Speech recognition",
      "score": 0.37802332639694214
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}