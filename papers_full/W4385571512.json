{
  "title": "Decouple knowledge from paramters for plug-and-play language modeling",
  "url": "https://openalex.org/W4385571512",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100692493",
      "name": "Xin Cheng",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5043098453",
      "name": "Yankai Lin",
      "affiliations": [
        null,
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5110999110",
      "name": "Xiuying Chen",
      "affiliations": [
        "Bioscience Research"
      ]
    },
    {
      "id": "https://openalex.org/A5037132097",
      "name": "Dongyan Zhao",
      "affiliations": [
        null,
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5100716372",
      "name": "Rui Yan",
      "affiliations": [
        null,
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4294554825",
    "https://openalex.org/W3212335714",
    "https://openalex.org/W4281777585",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4310885633",
    "https://openalex.org/W4288289156",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W3104330316",
    "https://openalex.org/W4372272503",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3174531908",
    "https://openalex.org/W3152884768",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4221166192",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W4221166196",
    "https://openalex.org/W4297971002",
    "https://openalex.org/W2971044268",
    "https://openalex.org/W4318620964",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4306317304",
    "https://openalex.org/W4306178240",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4223626817",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2279376656",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3102844651",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W4296878971",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4367046781",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3176913643",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4288284003",
    "https://openalex.org/W3097252660",
    "https://openalex.org/W2963997607",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W4289550782",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3171434230",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2955041501",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2963491014",
    "https://openalex.org/W3097517997",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3037854022",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W4286905627",
    "https://openalex.org/W4287747814",
    "https://openalex.org/W378747138",
    "https://openalex.org/W2963448850",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Pre-trained language models (PLM) have made impressive results in a wide range of NLP tasks and it has been revealed that one of the key factors to their success is the parameters of these models implicitly learn various types of knowledge in the pre-training corpus.However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents us from understanding what kind of knowledge PLM needs to solve a certain task. In this paper, we introduce {pasted macro 'MODEL'}, a pre-training model with differentiable plug-in memory (DPM). The key intuition behind is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the {pasted macro 'MEMORY'}. We conduct extensive experiments under various settings to justify this design choice. In domain adaptation setting, {pasted macro 'MODEL'} could be easily adapted to different domains with pluggable in-domain memory—obtaining 3.95 F1 improvements across four domains, without any in-domain training. {pasted macro 'MODEL'} could also keep absorbing new knowledge after pre-training is done by knowledge updating operation in the {pasted macro 'MEMORY'} without re-training. Finally, we show that by incorporating training samples into {pasted macro 'MEMORY'} with knowledge prompting, {pasted macro 'MODEL'} could further be improved by the instruction of in-task knowledge.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 14288–14308\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDecouple knowledge from paramters for plug-and-play language modeling\nXin Cheng1, Yankai Lin2,6, Xiuying Chen3, Dongyan Zhao1,4,5∗, Rui Yan2,6∗\n1 Wangxuan Institute of Computer Technology, Peking University\n2 Gaoling School of Artificial Intelligence, Renmin University of China\n3 Computational Bioscience Reseach Center, KAUST 4 BIGAI, Beijing, China\n5 National Key Laboratory of General Artificial Intelligence\n6 Engineering Research Center of\nNext-Generation Intelligent Search and Recommendation, Ministry of Education\nchengxin1998@stu.pku.edu.cn yankailin@ruc.edu.cn\nxiuying.chen@kaust.edu.sa zhaody@pku.edu.cn\nruiyan@ruc.edu.cn\nAbstract\nPre-trained language models (PLM) have made\nimpressive results in various NLP tasks. It has\nbeen revealed that one of the key factors to\ntheir success is the parameters of these models\nimplicitly learn all kinds of knowledge during\npre-training. However, encoding knowledge\nimplicitly in the model parameters has two fun-\ndamental drawbacks. First, the knowledge is\nneither editable nor scalable once the model is\ntrained, which is especially problematic in that\nknowledge is consistently evolving. Second, it\nlacks interpretability and prevents humans from\nunderstanding which knowledge PLM requires\nfor a certain problem. In this paper, we intro-\nduce PlugLM, a pre-training model with differ-\nentiable plug-in memory (DPM). The key intu-\nition is to decouple the knowledge storage from\nmodel parameters with an editable and scalable\nkey-value memory and leverage knowledge in\nan explainable manner by knowledge retrieval\nin the DPM. To justify this design choice, we\nconduct evaluations in three settings includ-\ning: (1) domain adaptation. PlugLM obtains\n3.95 F1 improvements across four domains on\naverage without any in-domain pre-training.\n(2) knowledge update. PlugLM could absorb\nnew knowledge in a training-free way after pre-\ntraining is done. (3) in-task knowledge learn-\ning. PlugLM could be further improved by\nincorporating training samples into DPM with\nknowledge prompting1.\n1 Introduction\nLarge pre-trained language models (PLM) (Peters\net al., 2018; Devlin et al., 2019; Radford et al.,\n2018) have become a revolutionary breakthrough\nin NLP area. Optimized by carefully designed\nself-supervised objectives on unlabeled corpus and\n∗Corresponding author.1Code available at https://github.com/Hannibal046/\nPlugLM\nfine-tuned on downstream tasks, PLMs perform re-\nmarkably well in a wide range of NLP benchmarks.\nRecent studies (Warstadt et al., 2019; Petroni et al.,\n2019) have revealed that one of the key factors to\nthe success of PLMs is that the parameters of these\nmodels implicitly learn various types of knowl-\nedge in the pre-training corpus. Owing to these\nlearned syntactic, semantic, factual and common-\nsense knowledge, PLMs show great understand-\ning, generalization and reasoning abilities in multi-\nple downstream tasks (Rogers et al., 2020; Izacard\net al., 2022). As Geva et al. (2021) pointed out, the\nfeed-forward layers (FFN), constituting two-thirds\nof a transformer model’s parameters, are essen-\ntially key-value memories and store all kinds of\nknowledge of PLM. The first linear layer of FFN\nacts like a set of sparsely activated keys detecting\ninput patterns while the second is the correspond-\ning value. To aggressively capture more knowl-\nedge, larger PLMs are continuously proposed, from\n110M BERT (Devlin et al., 2019) to 530B MT-\nNLG (Smith et al., 2022), yet PLM has not reached\nupper bound (Ouyang et al., 2022).\nHowever, a fundamental question still remains:\nFor PLM, is it the optimal way to implicitly\nencode knowledge in its parameters? We ar-\ngue that the implicit knowledge encoding approach\nhas two fundamental drawbacks. First, the learned\nknowledge is neither editable nor scalable once the\nmodel is trained (e.g., BERT doesn’t know what\nis a BERT). Nevertheless, world knowledge is ac-\ntually infinite and evolving. We thus would never\nexpect an ever-large model to capture all the knowl-\nedge in its parameters and to be continuously re-\ntrained for the newly coming one. Second, the cur-\nrent PLMs lack interpretability at the knowledge\nlevel. Implicit knowledge encoding fails to provide\nprovenance for model’s prediction and makes PLM\na black box preventing humans from understand-\n14288\ning which knowledge PLM requires for a certain\nproblem.\nIn this work, we propose a novel architecture\nof PLM, PlugLM, which decouples the knowledge\nstorage from model parameters and explicitly lever-\nages the knowledge in an explainable manner. As\nshown in Figure 1, we balance the functionality of\nFFN layer with a differentiable plug-in key-value\nmemory (DPM), which is highly scalable as well as\neditable. Each slot of DPM encodes the knowledge\nto a pair of key and value, and thus we can ex-\nplicitly retrieve the required knowledge in natural\nlanguage from DPM rather than unnamed vectors\nin FFN.\nTo justify the design choice of decoupling the\nknowledge from parameters, we conduct exten-\nsive evaluations under different settings. In the\ndomain adaptation setting, PlugLM could be eas-\nily adapted to different domains with pluggable in-\ndomain memory—obtaining 3.95 F1 improvements\nacross four domains on average and up to 11.55 F1\nimprovement on ACL-ARC citation intent classifi-\ncation dataset, without any in-domain pre-training.\nIn the knowledge update setting, PlugLM could\nabsorb new knowledge after pre-training is done in\na training-free way by knowledge updating opera-\ntion in the DPM, with an improvement up to 4 F1\nscores in LINNAEUS NER dataset. PlugLM could\nfurther be improved by incorporating training sam-\nples into DPM with knowledge prompting as a kind\nof in-task knowledge.\n2 Related Work\nInvestigating FFN Feed-forward layers consti-\ntute two-thirds of a transformer model’s parameters\nand are essential to unveil modern PLMs (Geva\net al., 2021, 2022). A surge of works have investi-\ngated the knowledge captured by FFN (Dai et al.,\n2022a; Meng et al., 2022; Geva et al., 2021, 2022;\nJiang et al., 2020; Yao et al., 2022; Wallat et al.,\n2021). Based on the view that FFN is essentially\nan unnormalized key-value memory network, Dai\net al. (2022a) detects knowledge neurons in FFN\nand edit specific factual knowledge without fine-\ntuning. Meng et al. (2022) modifies FFN weights\nto update specific factual associations using Rank-\nOne Model Editing. Yao et al.(2022) injects knowl-\nedge into the FFN via BM25. Dai et al. (2022b)\nand Lample et al. (2019) enhance the model by ex-\npanding the size of FFN with extra trainable keys\nand values.\nKnowledge-Augmented Language Model\nThere are two lines of works to equip PLM with\nknowledge. The first is introduce additional\nKnowledge Graph (KG) and knowledge-based\ntraining signal (e.g., entity linking) into the\nlanguage model pre-training, like ERNIE (Zhang\net al., 2019; Sun et al., 2019), KnowBERT (Peters\net al., 2019) and KEPLER (Wang et al., 2021).\nAnother line of works adopt retrieval mechanism\nto incorporate knowledge, either symbolic (Verga\net al., 2020; Agarwal et al., 2021; Févry et al.,\n2020) or texual (Guu et al., 2020; Lewis et al.,\n2020c; Borgeaud et al., 2022; Lewis et al.,\n2020a; Verga et al., 2020; de Jong et al., 2022).\nThey formulate the task as retrieve then predict\nprocess by using extra neural dense retriever or\nsparse retriever to find most relevant supporting\nknowledge and combine it with input using either\nconcatenation (Guu et al., 2020; Lewis et al.,\n2020c), attention methods (de Jong et al., 2022;\nChen et al., 2022) or interpolation (Khandelwal\net al., 2020; Zhong et al., 2022)\nPlugLM differs from previous works in that we\ndo not try to equip the model with additional knowl-\nedge to perform knowledge-intensive tasks. The\nkey insight is to transform FFN architecture into\ndeep retrieval in the interest of decoupling the\nknowledge which would otherwise be stored in the\nparameters and this is orthogonal to all retrieval-\naugmented PLMs.\n3 Preliminary\nFeed-forward Layers Transformer (Vaswani\net al., 2017), the backbone for all PLMs, is\nmade of stacked self-attention (Self-Attn) and feed-\nforward (FFN) layers. The former captures the\ncontextual interaction among inputs and the latter\nprocess each input independently. Let x∈Rd1 be\na vector as input, the FFN could be formulated as:\nFFN(x) =σ(x·W⊤\n1 ) ·W2 (1)\nwhere W1,W2 ∈Rd2×d1 and σis the activation\nfunction. The bias term is omitted for brevity.\nKey-Value Memory Network The Key-Value\nMemory Network (Weston et al., 2014; Sukhbaatar\net al., 2015) corresponds to d2 key-value pairs and\neach key/value is a vector in Rd1 . They are the\ngeneralization of the way knowledge is stored (Eric\net al., 2017; Miller et al., 2016). For an input x∈\nRd1 , there are two stages for a key-value memory\n14289\nSelf-Attention Layer\nFFN Layer\nKnowledge Attention\nMIPS\n[MASK] is the ﬁrst transformer-based PLM.\nEnd-to-End Backprop.\nKnowledge\nEncoder\nWikipedia\nS2ORC\nPubMed\nKeys Values\nFigure 1: Overview of our PlugLM. We replace FFN in PLM with a Differentiable Plug-in key-value Memory (DPM)\nby which PLM could store and leverage knowledge in an explainable manner.\nnetwork. First, the lookup (addressing) stage would\ncompute the matching degree between xand each\nkey. In the second stage, xwould be transformed\nby the weighted sum of values according to the\ndistribution of the matching degree in the first stage.\nWe can formally define it as:\nMemoryNetwork(x) =softmax(x·K⊤) ·V (2)\nwhere K,V ∈Rd2×d1 . Comparing equation (1)\nand (2), we could find that the FFN is an unnor-\nmalized version of MemoryNetwork. The keys in\nFFN are pattern detectors and would be activated\nonly when certain patterns occur in the input. This\nexplains how FFN stores knowledge in a key-value\nmanner (Geva et al., 2021; Sukhbaatar et al., 2019).\n4 PlugLM\nThe overall architecture of PlugLM is illustrated in\nFigure 1. Because FFN is essentially a key-value\nmemory network (Geva et al., 2021; Dai et al.,\n2022a; Meng et al., 2022), PlugLM creatively de-\ncouples the knowledge storage from model param-\neters by replacing2 FFN with a Differential Plug-in\nkey-value Memory, DPM (§4.1) and conducting\nknowledge retrieval in DPM with knowledge at-\ntention (§4.2) for explicit knowledge usage instead\nof storing all knowledge implicitly in the model\nparameters. In §4.3, we detailedly explain how\nPlugLM is trained in both pre-training and fine-\ntuning stages.\n2Because different layers in transformer capture different\nknowledge, the lower layer for shallow patterns while the\nupper layers for more semantic ones (Geva et al., 2021; ?),\nwe only consider replacing FFN in Top-L layers with DPM\nwhile keeping FFN in the lower layers untouched to encode\nthe intrinsic language understanding knowledge as detailed in\n§5.4.\n4.1 Differential Plug-in Memory\nIn this paper, we view n-th knowledge dn =\n{t1\nn,t2\nn,...,t |dn|\nn }as consecutive tokens from unla-\nbeled corpora as in Guu et al. (2020). For each dn,\nwe get its dense representation hn from a knowl-\nedge encoder KnowEncoder(·):\nhn = AttnPooling(EToken(dn) +EPos(dn)) (3)\nwhere AttentivePooling function (Xu et al., 2021;\nCheng et al., 2023a) corresponds to a trainable\npattern detector aggregating information from a se-\nquence of input. And EToken and EPos denote token\nembedding and positional embedding. Then we\nuse two independent mapping functions to project\nhn to the key space and value space:\nkn = Wk ·hn + bk (4)\nvn = Wv ·hn + vk (5)\nwhere Wk, Wv, bk and vk are trainable parame-\nters. And DPM is a triplet of ⟨D,K,V⟩:\nD = {d1,d2,...,d |D|} (6)\nK = {k1,k2,...,k |D|} (7)\nV = {v1,v2,...,v |D|} (8)\n4.2 Memory Fusion\nFor hidden states h ∈Rl×d from Self-Attn, FFN\nwould transform hwith unnormalized key-value\nmemory as in Equation (1). Our key insight\nis that instead of interacting with unnamed vec-\ntors in FFN, we conduct Maximum Inner Product\nSearch (MIPS) to retrieve knowledge in natural\nlanguage from ⟨D,K,V⟩where each triplet corre-\nsponds to one knowledge along with its key and\n14290\nvalue representation. For h, we first get its sentence-\nlevel representation zby an attentive pooling func-\ntion z= AttentivePooling(h), then we use zas the\nquery vector to ⟨D,K,V⟩. Since PLM is internally\nsparse (Li et al., 2022), we only consider Top-N\nknowledge Dz with corresponding keys Kz and\nvalues Vz:\nKz = Top-N(MIPS(z,K)) (9)\nVz = {vi if ki in Kz} (10)\nDz = {di if ki in Kz} (11)\nwhere Top-N also corresponds to the indexing oper-\nation. With Kz and Vz, we use knowledge attention\nto fuse retrieved knowledge into our model:\nAttention(h,Kz,Vz) =softmax(hK⊤\nz√\nd\n)Vz (12)\nwhere dis the head dimension. By knowledge re-\ntrieval and fusion, we explore an interpretable way\nto incorporate knowledge into the model where\nDz is the actual knowledge that PLM would lever-\nage. And direct modification on D without chang-\ning model parameters empowers PlugLM with\nmuch flexibility and scalability in domain adapta-\ntion (§5.1) and knowledge update (§5.2) scenarios.\n4.3 Training\nThe backbone of our model is a multi-layer bidirec-\ntional transformer encoder (Devlin et al., 2019).\nThere are two phases in our framework: pre-\ntraining and fine-tuning. In the pre-training phase,\nto make the whole training process end-to-end train-\nable, we use asynchronous index refreshing to op-\ntimize our model as done in Guu et al. (2020) and\nCai et al. (2021). Concretely, we update the indices\nof DPM every T steps. The MIPS results are based\non the stale index while the scores of selected Top-\nN results are recomputed using KnowEncoder(·)\nwhich facilitates the gradient flow back to memory.\nThe training objective is Masked Language Model-\ning (Devlin et al., 2019) where we randomly mask\ntokens in a sentence and ask PlugLM to predict it.\nIn the pre-training phase, Wikipedia is chosen as\nthe source of knowledge and in the domain adapta-\ntion fine-tuning stage, corpora from other domains\nare treated as knowledge sources detailed in §5.1.\nMore details are shown in Appendix A. In the fine-\ntuning phase, the K and V of DPM are fixed, and\nwe view it as an editable and scalable knowledge\nlookup table.\n5 Experiments\nPlugLM mainly tries to decouple the knowledge\nstorage from parameters and leverage knowledge\nin an explainable way. We conduct comprehensive\nexperiments to show the superiority of this novel\narchitecture: we could easily adapt the model to\ndifferent domains without in-domain pre-training\nby switching DPM (§5.1.1 and §5.1.2), alleviate\ncatastrophic forgetting by storing DPM (§5.1.1),\ninject new knowledge into the model by enlarging\nDPM (§5.2), further enhance the model by inject-\ning in-task knowledge into DPM (§5.3) and unveil\nthe black-box PLM with direct access to the knowl-\nedge retrieved from DPM (Appendix D). We also\ncarefully examine each key design in PlugLM and\npoint the direction for future work in §5.4.\n5.1 Domain Adaptation\nLearning robust and transferable representation has\nbeen the core of language model pre-training (Pe-\nters et al., 2019). For the general-purposed PLM\nto generalize well on domain-specific tasks, en-\ndowing the model with domain knowledge via in-\ndomain training remains the go-to approach (Guru-\nrangan et al.,2020; Whang et al.,2020; Zhang et al.,\n2020; Li et al., 2023). In this section, we show that\nwithout any in-domain pre-training, PlugLM could\nflexibly adapt to multiple domains with domain-\nspecific DPM. For the existing PLM encoding\nknowledge in parameters, this is a challenging\ntask in that it can not guarantee the generalization\nacross multiple domains due to catastrophic forget-\nting (Kirkpatrick et al., 2016) and sometimes it is\neven computationally unaffordable to keep training\nthe super large models (Smith et al., 2022; Brown\net al., 2020).\nWe consider two adaptation scenarios: domain\nadaptive post-training (§5.1.1) and in-domain pre-\ntraining (§5.1.2). The former is conducted after\nPLM was trained on the general domain and the\nlatter trains a domain-specific PLM from scratch.\n5.1.1 Domain Adaptive Post-Training\nExperimental Setting Following Gururangan\net al. (2020), we conduct experiments on four do-\nmains: B IOMED, CS, N EWS and REVIEWS across\neight domain-specific downstream tasks, in both\nlow and high resource settings. More details can be\nfound in Appendix B. When fine-tuning, we pass\nthe final [CLS] representation to a task-specific\nhead as in Devlin et al. (2019).\n14291\nModel B IOMED CS NEWS REVIEWS\nCHEM. RCT ACL. SCI. HYP. AG. HP. IMDB Avg.\nGain\nAvg.\nCost\nWikiBERT 77.72 86.52 61.58 79.95 83.54 93.38 67.62 89.79 - -\n+ DAPT 78.24 86.71 67.56 80.82 86.22 93.49 68.11 90.12 +1.40 47.7 h\n¬DAPT 75.82 86.11 62.11 78.42 80.12 93.31 68.11 89.54 -0.82 -\n+ DACT 76.34 86.11 61.19 78.56 80.52 93.29 68.08 89.88 -0.77 -\nREALM 78.28 85.12 62.07 78.41 84.12 92.58 67.06 90.56 - -\n+ DAA 79.32 85.98 68.92 80.41 85.36 92.61 68.51 93.01 +1.98 6.3 h\n¬DAA 77.61 85.12 64.78 75.31 82.28 92.41 66.13 91.21 -0.41 -\n+ DAR 80.56 85.32 70.12 81.16 86.58 93.01 67.42 92.16 +2.26 6.3 h\nPlugLM 78.02 87.12 63.77 78.56 84.32 93.23 67.83 91.24 - -\n+ DAA 82.56 88.13 72.51 83.00 88.16 94.11 69.28 92.56 +3.28 0.16 h\n¬DAA 77.98 86.13 64.78 78.13 84.18 92.99 67.56 90.88 -0.18 -\n+ DAR 83.80 88.98 75.32 82.56 89.26 93.55 69.41 92.78 +3.95 0.16 h\nTable 1: Performance of domain adaptive post-training. Each result is averaged with five different random seeds.\nReported results are test macro-F1, except for RCT and C HEM PROT, for which we report micro-F1, following\nBeltagy et al. (2019). The best scores are in bold, and the second best are underlined.\nWe have the following baselines: WikiBERT\nuses the architecture of BERTbase (Devlin et al.,\n2019) and is pre-trained on Wikipedia. To adapt\nWikiBERT to other domains, we use DAPT follow-\ning the training setting in Gururangan et al. (2019).\nREALM (Guu et al., 2020) and PlugLM are mod-\nels that have an external knowledge base and can\nbe simply adapted to other domains with a differ-\nent base. We have two adaptation strategies: DAA,\nshort for Domain Adaptive Addition, appends do-\nmain knowledge to the knowledge base, and DAR,\nDomain Adaptive Replacement, replaces general\nknowledge with domain-specific knowledge in the\nknowledge base.\nWe also include the results of ¬DAPT, ¬DAA\nand DACT. The former two use irrelevant domain\ncorpora for post-training and knowledge base con-\nstruction, which are used to test the robustness of\nthe adaptation method and rule out the factor that\nimprovements might be attributed simply to expo-\nsure to more data3. For DACT, Domain Adaptive\nContinual Training, we sequentially use DAPT for\nWikiBERT in multiple domains in the hope that it\ncan capture and store knowledge from various do-\nmains in a lifelong learning way (Rostami, 2021).\n3Following Gururangan et al. (2020), we use the following\nirrelevant domain mapping: for N EWS , we use a CS LM;\nfor R EVIEWS , a B IOMED LM; for CS, a N EWS LM; for\nBIOMED, a REVIEWS LM.\nExperimental Results The results are shown in\nTable 1. The Avg.Cost is the cost for adaptation\nmeasured by hour. For WikiBERT, it’s the time to\npost-train model in domain-specific corpus. For\nREALM and PlugLM, it is the time to encode do-\nmain knowledge into the knowledge base. We can\nobserve: (1) In-domain training helps model better\ngeneralize to tasks requiring domain knowledge\nwhile irrelevant knowledge misleads the model and\ncauses performance degradation. And by com-\nparing ¬DAPT and ¬DAA, it shows that mod-\nels with external knowledge base (PlugLM and\nREALM) are more robust when faced with noisy\nout-of-domain knowledge. (2) For the model that\nimplicitly encodes knowledge in the parameters,\nit fails to generalize across domains as the result\nof DACT indicates. For example, we keep train-\ning WikiBERT in N EWS domain after DAPT in\nCS domain and fine-tune it on the CS downstream\ntasks. It performs on par with model that is never\nexposed to CS domain (¬DAPT). PlugLM could\nalleviate this catastrophic forgetting problem by\nstoring all kinds of knowledge in DPM and using\nit in a plug-and-play manner. (3) Direct modifica-\ntion on external memory helps PlugLM efficiently\nand effectively adapt to different domains without\nin-domain training. In 254 ×less time compared\nwith DAPT and in 40×less time compared with\nREALM, PlugLM significantly outperforms DAPT\nand REALM-based methods.\n14292\nWikipedia\nCS\nOri. DAA DAR 5\n4\n3\n2\n1\n0101 20 30 40 50 1 10 20 30 40 50 1 10 20 30 40 50\nFigure 2: Knowledge retrieval visualization. We randomly sample 50 samples from ACL-ARC test set and check\nwhat kind of knowledge does PlugLM use to solve CS-specific tasks. Each column is one sample and the row is the\nindex of retrieved knowledge in DPM. Their corresponding F1 scores are 63.77, 72.51 and 75.32.\nTo further understand PlugLM, in Figure 2, we\npresent a visualization for the distribution of actual\nretrieved knowledge for DAA, DAR and original\nPlugLM. A clear pattern here is that with more\ndomain knowledge involved, the model performs\nbetter (63.77, 72.51 and 75.32) and remarkably,\nalthough pre-trained on the general domain, the\nPlugLM has managed to learn what to retrieve\nwhen there are both general knowledge and domain-\nspecific knowledge in DPM shown in DAA visual-\nization.\n5.1.2 In-domain Pre-Training\nIn-domain pre-training is another line of work for\ndomain-specific PLM training from scratch like\nBioBERT (Lee et al., 2019), SciBERT (Beltagy\net al., 2019) and FinBERT (Araci, 2019).\nExperimental Setting In this section, we choose\nthe biomedical domain and compare PlugLM with\nmodel in the architecture of BERTbase, pre-trained\non the general domain, Wikipedia (i.e., WikiB-\nERT) and pre-trained on the biomedical domain,\nPubmed (i.e., PubmedBERT). The statistics of\ndatasets and pre-training details are listed in Ap-\npendix F. We test two kinds of abilities of these\nPLMs. First, we test how they perform in biomed-\nrelevant downstream tasks. Specifically, we con-\nduct experiments on eight representative biomedi-\ncal NER datasets which aim at recognizing domain-\nspecific proper nouns in the biomedical corpus.\nThen we test their general language understanding\nability in GLUE (Wang et al., 2019) and SQUAD (Ra-\njpurkar et al., 2016, 2018). For SQUAD and GLUE,\nthe DPM is constructed from Wikipedia, and for\nbiomedical NER, DPM is from PubMed (Canese\nand Weis, 2013).\nExperimental Results The results are shown\nin Table 3. Both pre-trained on the Wikipedia,\nPlugLM outperforms WikiBERT in 8/8 NER tasks\nwith average 1.75 F1 scores by simply switch-\ning the knowledge domain of DPM. PlugLM also\ngives comparable results with PubmedBERT in\nBC4CHEMD, JNLPBA and LINNAEUS datasets. Al-\nthough PubmedBERT works well for biomedical\ntasks, it shows less general language understanding\nability and underperforms WikiBERT and PlugLM\nin GLUE (Table 4) and SQUAD (Table 2), especially\nin low resource scenario (i.e., RTE, COLA and MRPC\ndatasets). With DPM, PlugLM shows great flexibil-\nity and performs well in both general domain and\nbiomedical domain. In Appendix D, we give con-\ncrete cases of PlugLM with respect to the retrieved\nknowledge.\nPubmedBERT WikiBERT PlugLM\nEM F1 EM F1 EM F1\nSQUAD(v1) 76.68 84.56 81.32 88.68 82.19 89.44\nSQUAD(v2) 68.44 71.12 72.64 75.89 73.76 76.90\nTable 2: SQUAD results measured by EM and F1.\n5.2 Knowledge Update\nSince the world is not fixed as a snapshot once the\npre-training corpus is collected, the current PLM,\nno matter how large it is, fails to adapt to this chang-\ning world. For colossal PLMs like GPT-3 (Brown\net al., 2020) and MT-NLG (Smith et al., 2022), ef-\nficiently fine-tuning for downstream tasks remains\nan open challenge, let alone re-training it on the\nnewly coming knowledge.\nExperimental Setting In this section, we show\nthat PlugLM can efficiently absorb new knowledge\nby updating the ⟨D,K,V⟩without re-training. We\n14293\nType Dataset # Annotation WikiBERT PlugLM PubmedBERT\nDisease NCBI-disease 6811 83.65 85.96 88.39\nBC5CDR 12694 80.37 82.10 83.89\nDrug/Chem. BC4CHEMD 79842 87.07 89.93 89.35\nBC5CDR 15411 88.79 90.56 92.75\nGene/Protein. B2CGM 20703 80.63 82.14 83.16\nJNLPBA 35460 75.49 76.39 76.25\nSpecies LINNAEUS 4077 85.32 87.01 86.11\nSPECIES-800 3708 68.54 69.73 71.32\nTable 3: Performance of biomedical NER measured by F1 score across eight datasets.\n#Paras Avg.\nLatency RTE COLA MRPC STS-B SST-2 QNLI QQP MNLI\n-(m/mm)\nPubmedBERT 110M ×1.00 61.17 50.06 84.56 85.73 88.64 90.11 88.78 82.14/82.56\nWikiBERT 110M ×1.00 65.70 53.53 88.85 88.64 92.32 90.66 89.71 83.91/84.10\nPlugLM 109M ×2.54 70.40 52.68 91.54 89.20 91.86 91.28 90.56 84.56/85.35\nTable 4: GLUE results. Detailed metrics and latency of each model is in Appendix C\nconsider the following two settings. (1) We only\npre-train PlugLM with limited data and gradually\nenlarge the DPM with unseen knowledge when\nfine-tuning. (2) We pre-train PlugLM with full\ngeneral-domain data and ask the model to perform\ndomain adaptation in DAR manner by gradually\nincreasing domain knowledge in ⟨D,K,V⟩.\nExperimental Results The results are shown\nin Figure 3a and 3b. For the first setting, we\ntest on QA (SQUAD) and Sentiment Classification\ntasks ( SST-2). Both WikiBERT and PlugLM\nare pre-trained with only 1/4 Wikipedia corpus.\nWe have the following observations: (1) PlugLM\ntrained with limited data already outperforms Wik-\niBERT in both tasks (0.39 EM in QA and 0.59\nAccuracy in classification) which verifies the effec-\ntiveness of PlugLM in low-resource setting; (2) A\nconsistent pattern across two tasks verifies PlugLM\ncould absorb new knowledge simply by adding\nmore slots in ⟨D,K,V⟩without heavy re-training.\nFor the second setting, Figure 3c shows our\nmodel can absorb new cross-domain knowledge\nunder adaptation setting. It achieves a higher F1\nscore on the LINNAEUS NER dataset with increas-\ningly more biomed-specific knowledge injected.\n5.3 In-task Knowledge\nInspired by in-context learning (Brown et al.,2020)\nand example-augmented generation (Cheng et al.,\n2022, 2023b), the training samples can also be\nviewed as a kind of in-task knowledge. In this\nsection, we broaden the scope of DPM knowledge\nby including the training samples.\nExperimental Setting Since the knowledge from\nWikipedia is a textual description from domain ex-\nperts while the training sample from a Question-\nanswering NLI dataset is in the form of [Q, A,\nLabel], this surface form distribution shift may\naffect the knowledge retrieval. We consider the\nfollowing injection methods. (1) Concate. We di-\nrectly concatenate each training sample as a long\nstring in the form of “Q [SEP] A [SEP] Label\"\nand append this to DPM. (2) Tagged. To build the\nconnection between model inputs and DPM, we\ntag each training sample by prepending a special\ntoken ([Tagged]), and use these tagged samples\nin both DPM and as model input. (3) Knowledge\nPrompting. Inspired by prompting method (Liu\net al., 2021; Schick and Schütze, 2021), we trans-\nfer in-task knowledge to knowledge in the form of\nWikipedia by a natural language prompting. For ex-\nample, in QNLI dataset, we transform [Q, A, Label]\nwith the following prompting: “The first sentence\n(doesn’t) entail(s) with the second. The first sen-\ntence is [Q] and the second is [A]\". We choose\nmoderate-sized QNLI and QQP tasks because in-task\nknowledge injection doesn’t apply to low-resource\nsetting in our preliminary experiments.\n14294\n1/4 2/4 3/4 4/4\n70.0\n70.5\n71.0\n71.5\nEM\nQA:Squad(V2)\nPlugLM\nBERT\n(a) QA results.\n1/4 2/4 3/4 4/4\n90.5\n91.0\n91.5\nAcc.\nClassification:SST-2\nPlugLM\nBERT (b) Classification results.\n1M 5M 10M 13M\n82.5\n83.0\n83.5\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\nF1 NER:LINNAEUS\nPlugLM (c) NER in adaptation setting.\nFigure 3: Knowledge update results in QA, Sentiment Classification and NER.\nExperimental Results The result is shown in Ta-\nble 5. We can observe that PlugLM has managed\nto learn from in-task knowledge and the surface-\nform of knowledge affect the model performance.\nConcatenation of training sample fails to inform\nPlugLM the actual in-task knowledge (zero re-\ntrieval in QNLI) and building connection between\ndata and knowledge by a special tagged token\nonly gives minor improvements. Instead, a well-\ndesigned knowledge prompting can help PlugLM\nlearn task-specific knowledge.\nTask Ori. Concate. Tagged. Prompting.\nQNLI 91.28 91.28 91.37 91.58\nQQP 90.56 90.12 90.76 91.47\nTable 5: Performance of in-task knowledge onQNLI and\nQQP measured by accuracy.\n5.4 Tuning PlugLM\nWe investigate how each key design affects the per-\nformance of PlugLM. (1) Number of Retrieved\nKnowledge. Figure 4 shows the effects of differ-\nent N in STS-B dataset and the sparsely activated\nTop-5 knowledge proves to be optimal. (2) Lay-\ners equipped with DPM.Considering that the up-\nper layers in PLM capture more semantic infor-\nmation (Geva et al., 2021), we equip the last en-\ncoder layer with DPM in PlugLM. Figure 4 shows\nthat increasing DPM-enhanced encoder layer gives\nminor improvements but brings much latency be-\ncause of extra MIPS search. (3) FFN and DPM.\nTo further explore the relation between FFN and\nDPM, we propose two model variants. First, we\nreplace FFN in all encoder layers with a shared\nDPM denoted as PlugLM All. Then we fuse\nFFN and DPM by modifying the model architec-\nture from LayerNorm(h+KnowAttn(h,Kh′,Vh′))\nto LayerNorm(h + KnowAttn(h,Kh′,Vh′) +\nFFN(h)) and we name it PlugLM Fuse. The\nSpearman correlation (more results are shown in\nAppendix E) in STS-B dataset for WikiBERT,\nPlugLM All, PlugLM and PlugLM Fuse is 88.64,\n86.82, 89.20 and 89.10. We could find that PlugLM\nAll, where there is no FFN, underperforms Wik-\niBERT. And PlugLM performs comparably with\nPlugLM Fuse. We conjecture that FFN in different\nlayers may play different roles, which is also re-\nported in Geva et al. (2021). For the upper layer\nwhich captures more semantic knowledge (Jawa-\nhar et al., 2019), DPM is a flexible and extensible\nsubstitution of FFN, but for lower layers, shallow\nfeatures should be captured in the model parame-\nters.\n1 2 3 4 5 6 7 8 9 10\nT op-N Knowledge\n88.8\n88.9\n89.0\n89.1\n89.2\n89.3Spearman correlation\n1 2 3 4 5 6\nNumber of DPM Layers\n88.8\n88.9\n89.0\n89.1\n89.2\n89.3\nSpearman\n0\n1\n2\n3\n4\nLatency\nLatency\nFigure 4: Effect of the number of retrieved knowledge\nand the number of DPM-enhanced layers in STS-B mea-\nsured by spearman correlation.\n6 Conclusion\nFor the first time, we challenge the current implicit\nknowledge encoding mechanism for PLMs with\ntwo fundamental drawbacks and insightfully pro-\npose to decouple knowledge storage from model\nparameters with an editable and scalable key-value\nmemory. Inspired by the findings that FFN stores\nall kinds of knowledge and is essentially a key-\nvalue memory network, we transform FFN archi-\n14295\ntecture into deep retrieval with a differentiable plug-\nin memory (DPM), which makes the knowledge\nencoding of PLMs more flexible and interpretable.\nExtensive experimental results in different scenar-\nios including domain adaptation, knowledge update\nand in-task knowledge learning verify the design\nchoice of PlugLM. We believe this architectural de-\nsign would pave a new direction for future research\non PLM, especially for super-large PLM.\nLimitations\nWe discuss the limitations of PlugLM as follows:\n(1) Despite the strong performance achieved by\nour approach with DPM, it results in a reduced\ninference efficiency at the same time due to the\nMIPS search. For example, PlugLM is about two\ntimes slower than pure transformer-based models\nin GLUE. This would be more crucial when the ex-\nternal memory is much larger. Potential solutions\nto this issue include (1) constructing the memory\nusing a coarser granularity (Borgeaud et al., 2022);\n(2) compressing DPM by semantic clustering as in\nTay et al. (2022) or knowledge summarization as\nin Xu et al. (2022).\n(2) In this paper, we choose Wikipedia for DPM\nconstruction and PlugLM pre-training. While\nWikipedia is the most commonly used data source\nfor language model pre-training (Devlin et al.,\n2019; Liu et al., 2019), there are also many other\ntypes of knowledge not covered in Wikipedia,\nand how to integrate different types of knowl-\nedge (e.g., factual, commonsense, syntactic and\nsemantic knowledge) into our framework remains\nunder-explored.\n(3) Although this paper proposes a general ar-\nchitecture that is applicable to PLMs of all kinds\nand sizes including bidirectional (Devlin et al.,\n2019; Liu et al., 2019; Yang et al., 2019), unidi-\nrectional (Radford et al., 2018, 2019; Brown et al.,\n2020) and encoder-decoder-based PLM (Lewis\net al., 2020b; Raffel et al., 2020; Song et al., 2019),\nwe only experiment with bidirectional models in\nmoderate size. In particular, we believe this ar-\nchitectural design would be greatly beneficial for\nLLM (Smith et al., 2022; Chowdhery et al., 2022;\nOuyang et al., 2022) for the following reasons: (1)\nthe parameters of LLM could not be easily updated\nonce the pre-training is done due to the unafford-\nable training cost. (2) the additional latency cost\nby MIPS retrieval is negligible compared with that\nof the whole LLM.\nAcknowledgement\nThis work was supported by National Nat-\nural Science Foundation of China (NSFC\nGrant No. 62122089), the National Key Re-\nsearch and Development Program of China (No.\n2021YFC3340304).\nReferences\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami\nAl-Rfou. 2021. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language\nmodel pre-training. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2021, Online, June\n6-11, 2021, pages 3554–3565. Association for Com-\nputational Linguistics.\nDogu Araci. 2019. Finbert: Financial sentiment\nanalysis with pre-trained language models. CoRR,\nabs/1908.10063.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-\nert: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 3613–3618.\nAssociation for Computational Linguistics.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n2022. Improving language models by retrieving from\ntrillions of tokens. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 2206–2240.\nPMLR.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\n14296\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and\nLemao Liu. 2021. Neural machine translation with\nmonolingual translation memory. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7307–7318, Online.\nAssociation for Computational Linguistics.\nKathi Canese and Sarah Weis. 2013. Pubmed: the bibli-\nographic database. The NCBI handbook, 2(1).\nWenhu Chen, Pat Verga, Michiel de Jong, John Wiet-\ning, and William W. Cohen. 2022. Augmenting pre-\ntrained language models with qa-memory for open-\ndomain question answering. CoRR, abs/2204.04581.\nXin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao,\nand Rui Yan. 2022. Neural machine transla-\ntion with contrastive translation memories. CoRR,\nabs/2212.03140.\nXin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang,\nXiuying Chen, Mingzhe Li, Dongyan Zhao, and Rui\nYan. 2023a. Towards personalized review summa-\nrization by modeling historical reviews from cus-\ntomer and product separately.\nXin Cheng, Di Luo, Xiuying Chen, Lemao Liu,\nDongyan Zhao, and Rui Yan. 2023b. Lift yourself\nup: Retrieval-augmented text generation with self\nmemory.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022a. Knowledge neurons\nin pretrained transformers. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 8493–\n8502. Association for Computational Linguistics.\nDamai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu,\nQiaoqiao She, and Zhifang Sui. 2022b. Neural\nknowledge bank for pretrained transformers. CoRR,\nabs/2208.00399.\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\nald, Fei Sha, and William W. Cohen. 2022. Mention\nmemory: incorporating textual knowledge into trans-\nformers through entity mention attention. In The\nTenth International Conference on Learning Repre-\nsentations, ICLR 2022, Virtual Event, April 25-29,\n2022. OpenReview.net.\nFranck Dernoncourt and Ji Young Lee. 2017. Pubmed\n200k RCT: a dataset for sequential sentence classi-\nfication in medical abstracts. In Proceedings of the\nEighth International Joint Conference on Natural\nLanguage Processing, IJCNLP 2017, Taipei, Taiwan,\nNovember 27 - December 1, 2017, Volume 2: Short\nPapers, pages 308–313. Asian Federation of Natural\nLanguage Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMihail Eric, Lakshmi Krishnan, François Charette, and\nChristopher D. Manning. 2017. Key-value retrieval\nnetworks for task-oriented dialogue. In Proceedings\nof the 18th Annual SIGdial Meeting on Discourse\nand Dialogue, Saarbrücken, Germany, August 15-17,\n2017, pages 37–49. Association for Computational\nLinguistics.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 4937–4951. Association for Computational\nLinguistics.\nMor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav\nGoldberg. 2022. Transformer feed-forward layers\nbuild predictions by promoting concepts in the vo-\ncabulary space. CoRR, abs/2203.14680.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021, pages\n5484–5495. Association for Computational Linguis-\ntics.\n14297\nSuchin Gururangan, Tam Dang, Dallas Card, and\nNoah A. Smith. 2019. Variational pretraining for\nsemi-supervised text classification. In Proceedings\nof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n5880–5894. Association for Computational Linguis-\ntics.\nSuchin Gururangan, Ana Marasovic, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020, pages 8342–8360.\nAssociation for Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research,\npages 3929–3938. PMLR.\nRuining He and Julian J. McAuley. 2016. Ups and\ndowns: Modeling the visual evolution of fashion\ntrends with one-class collaborative filtering. In Pro-\nceedings of the 25th International Conference on\nWorld Wide Web, WWW 2016, Montreal, Canada,\nApril 11 - 15, 2016, pages 507–517. ACM.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Few-shot learning with retrieval aug-\nmented language models. CoRR, abs/2208.03299.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 3651–3657. Association\nfor Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. Trans. Assoc. Comput. Linguistics,\n8:423–438.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Daniel A.\nMcFarland, and Dan Jurafsky. 2018. Measuring the\nevolution of a scientific field through citation frames.\nTrans. Assoc. Comput. Linguistics, 6:391–406.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 Task 4: Hyperpartisan news detection. In Se-\nmEval.\nJames Kirkpatrick, Razvan Pascanu, Neil C. Rabi-\nnowitz, Joel Veness, Guillaume Desjardins, Andrei A.\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2016. Overcoming catastrophic forgetting in neural\nnetworks. CoRR, abs/1612.00796.\nJens Kringelum, Sonny Kim Kjærulff, Søren Brunak,\nOle Lund, Tudor I. Oprea, and Olivier Taboureau.\n2016. ChemProt-3.0: a global chemical biology dis-\neases mapping. In Database.\nGuillaume Lample, Alexandre Sablayrolles,\nMarc’Aurelio Ranzato, Ludovic Denoyer, and\nHervé Jégou. 2019. Large memory layers with\nproduct keys. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 8546–8557.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nCoRR, abs/1901.08746.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020a. Pre-training via paraphrasing. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020b.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020c. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nJinpeng Li, Yingce Xia, Xin Cheng, Dongyan Zhao, and\nRui Yan. 2023. Learning disentangled representation\nvia domain adaptation for dialogue summarization.\n14298\nIn Proceedings of the ACM Web Conference 2023,\nWWW ’23, page 1693–1702, New York, NY , USA.\nAssociation for Computing Machinery.\nZonglin Li, Chong You, Srinadh Bhojanapalli, Daliang\nLi, Ankit Singh Rawat, Sashank J. Reddi, Ke Ye,\nFelix X. Chern, Felix X. Yu, Ruiqi Guo, and San-\njiv Kumar. 2022. Large models are parsimonious\nlearners: Activation sparsity in trained transformers.\nCoRR, abs/2210.06313.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nCoRR, abs/2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv:1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel S. Weld. 2020.S2ORC: the semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 4969–4983. Association for Computa-\ntional Linguistics.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identification of enti-\nties, relations, and coreference for scientific knowl-\nedge graph construction. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018, pages 3219–3232. Association\nfor Computational Linguistics.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn The 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Proceedings of the Conference, 19-24 June,\n2011, Portland, Oregon, USA, pages 142–150. The\nAssociation for Computer Linguistics.\nJulian J. McAuley, Christopher Targett, Qinfeng Shi,\nand Anton van den Hengel. 2015. Image-based rec-\nommendations on styles and substitutes. In Proceed-\nings of the 38th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, Santiago, Chile, August 9-13, 2015, pages\n43–52. ACM.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associa-\ntions in gpt. arXiv preprint arXiv:2202.05262.\nAlexander H. Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason Weston.\n2016. Key-value memory networks for directly read-\ning documents. In Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2016, Austin, Texas, USA, Novem-\nber 1-4, 2016, pages 1400–1409. The Association for\nComputational Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. CoRR, abs/2203.02155.\nMatthew E. Peters, Mark Neumann, Robert L. Logan\nIV , Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced contex-\ntual word representations. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 43–54. Association for Compu-\ntational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 2463–2473. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable questions\n14299\nfor squad. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 2: Short Papers, pages 784–789. Association\nfor Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383–2392.\nThe Association for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow BERT works. Trans. Assoc. Comput. Linguis-\ntics, 8:842–866.\nMohammad Rostami. 2021. Lifelong domain adapta-\ntion via consolidated internal distribution. In Ad-\nvances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Process-\ning Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual, pages 11172–11183.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, EACL 2021, Online, April 19 - 23, 2021, pages\n255–269. Association for Computational Linguistics.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zheng, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022. Using deepspeed and megatron to train\nmegatron-turing NLG 530b, A large-scale genera-\ntive language model. CoRR, abs/2201.11990.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: masked sequence to sequence\npre-training for language generation. In Proceedings\nof the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, volume 97 of Proceedings of Ma-\nchine Learning Research, pages 5926–5936. PMLR.\nSainbayar Sukhbaatar, Edouard Grave, Guillaume\nLample, Hervé Jégou, and Armand Joulin. 2019.\nAugmenting self-attention with persistent memory.\nCoRR, abs/1907.01470.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and\nRob Fergus. 2015. End-to-end memory networks. In\nAdvances in Neural Information Processing Systems\n28: Annual Conference on Neural Information Pro-\ncessing Systems 2015, December 7-12, 2015, Mon-\ntreal, Quebec, Canada, pages 2440–2448.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng,\nXuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,\nHao Tian, and Hua Wu. 2019. ERNIE: enhanced\nrepresentation through knowledge integration. CoRR,\nabs/1904.09223.\nYi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni,\nDara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\nZhao, Jai Prakash Gupta, Tal Schuster, William W.\nCohen, and Donald Metzler. 2022. Transformer\nmemory as a differentiable search index. CoRR,\nabs/2202.06991.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam W. Cohen. 2020. Facts as experts: Adapt-\nable and interpretable neural memory over symbolic\nknowledge. CoRR, abs/2007.00849.\nJonas Wallat, Jaspreet Singh, and Avishek Anand. 2021.\nBertnesia: Investigating the capture and forgetting of\nknowledge in BERT. CoRR, abs/2106.02902.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKEPLER: A unified model for knowledge embed-\nding and pre-trained language representation. Trans.\nAssoc. Comput. Linguistics, 9:176–194.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019. Investigating\nbert’s knowledge of language: Five analysis methods\nwith npis. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages 2877–\n2887. Association for Computational Linguistics.\nJason Weston, Sumit Chopra, and Antoine Bordes. 2014.\nMemory networks. arXiv preprint arXiv:1410.3916.\nTaesun Whang, Dongyub Lee, Chanhee Lee, Kisu Yang,\nDongsuk Oh, and Heuiseok Lim. 2020. An effective\ndomain adaptive post-training method for BERT in\nresponse selection. In Interspeech 2020, 21st Annual\n14300\nConference of the International Speech Communi-\ncation Association, Virtual Event, Shanghai, China,\n25-29 October 2020, pages 1585–1589. ISCA.\nTianyu Xu, Wen Hua, Jianfeng Qu, Zhixu Li, Jiajie\nXu, An Liu, and Lei Zhao. 2022. Evidence-aware\ndocument-level relation extraction. In Proceedings of\nthe 31st ACM International Conference on Informa-\ntion & Knowledge Management, Atlanta, GA, USA,\nOctober 17-21, 2022, pages 2311–2320. ACM.\nYichong Xu, Chenguang Zhu, Ruochen Xu, Yang Liu,\nMichael Zeng, and Xuedong Huang. 2021. Fusing\ncontext into knowledge graph for commonsense ques-\ntion answering. In Findings of the Association for\nComputational Linguistics: ACL/IJCNLP 2021, On-\nline Event, August 1-6, 2021, volume ACL/IJCNLP\n2021 of Findings of ACL, pages 1201–1207. Associ-\nation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5754–5764.\nYunzhi Yao, Shaohan Huang, Li Dong, Furu Wei,\nHuajun Chen, and Ningyu Zhang. 2022. Kformer:\nKnowledge injection in transformer feed-forward lay-\ners. In Natural Language Processing and Chinese\nComputing - 11th CCF International Conference,\nNLPCC 2022, Guilin, China, September 24-25, 2022,\nProceedings, Part I, volume 13551 of Lecture Notes\nin Computer Science, pages 131–143. Springer.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n9051–9062.\nRong Zhang, Revanth Gangi Reddy, Md Arafat Sul-\ntan, Vittorio Castelli, Anthony Ferritto, Radu Florian,\nEfsun Sarioglu Kayi, Salim Roukos, Avi Sil, and\nTodd Ward. 2020. Multi-stage pre-training for low-\nresource domain adaptation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5461–5468,\nOnline. Association for Computational Linguistics.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural In-\nformation Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pages 649–657.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers, pages 1441–1451. Association for Computa-\ntional Linguistics.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation.\nCoRR, abs/2205.12674.\n14301\nA PlugLM Pretraining Details\nThe details of PlugLM pre-training is shown in\nTable 6\nHyperparameter Assignment\nvocab size 30522\nnum layers with DPM top-1\ntop-N 5\nnumber of layers 12\nattention head 12\nmlm masking static\nmlm masking rate 0.15\nffn size 3072\nmax knowledge length 288\nUncased True\nmemory size 14802866\nbatch size 64\ngradient accumulation steps 128\nmax train steps 8000\noptimizer FusedLAMBAMP\nlearning rate 1e-4\nindex refreshing step 200\nlearning rate scheduler PolyWarmUpScheduler\nWarmup proportion 0.2843\nweight decay 0.01\nTable 6: Hyperparameters for PlugLM pretraining.\nB Data for Domain Adaptive\nPost-Training\nThe detailed statistics of domain corpora for post-\ntraining is listed in the Table 7 and downstream\ntasks in Table 8.\nC Latency\nIn Table 9, we show the detailed latency of WikiB-\nERT and PlugLM.\nD Case Study\nWe show three concrete examples from QNLI and\nACL-ARC in Table 13,14,15.\nE More Experiments for Tuning PlugLM\nIn Table 10, we show more results in Section 5.4\non STS-b, MRPC and QNLI.\nWikiBERT PlugLM All PlugLM Fuse PlugLM\nSTS-B 88.64 86.82 89.20 89.10\nMRPC 88.85 87.42 91.27 91.54\nQNLI 90.66 88.19 91.36 91.28\nTable 10: Experimental Results as in Section 5.4 on\nSTS-b, MRPC and QNLI. The evaluation metrics are\nSpearman correlation, F1 score and Accuracy respec-\ntively.\nF Details for Wikipedia and Pubmed\nThe source and size of Wikipedia and Pubmed are\nshown in Table 11. And hyper-parameters for Wik-\niBERT and PubmedBERT pre-training is shown in\nTable 12.\nHyperparameter Assignment\nvocab size 30522\nUncased True\nnumber of Layers 12\nattention Head 12\nffn Size 3072\nmlm masking static\nbatch size 64\ngradient accumulation steps 128\nmax train steps 8000\noptimizer FusedLAMBAMP\nlearning rate 6e-3\nindex refreshing step 200\nlearning rate scheduler PolyWarmUpScheduler\nWarmup proportion 0.2843\nweight decay 0.01\nTable 12: Hyperparameters for WikiBERT and Pubmed-\nBERT pretraining.\n14302\nDomain Pretraining Corpus # Tokens Size\nBIOMED 1.24M papers from S2ORC (Lo et al., 2020) 2.67B 12GB\nCS 5.07M papers from S2ORC (Lo et al., 2020) 4.3B 18GB\nNEWS 11.90M articles from REAL NEWS (Zellers et al., 2019) 6.66B 39GB\nREVIEWS 24.75M AMAZON reviews (He and McAuley, 2016) 2.11B 11GB\nTable 7: List of the domain-specific unlabeled datasets.\nDomain Task Label Type Train (Lab.) Dev. Test Classes\nBIOMED CHEM PROT relation classification 4169 2427 3469 13\n†RCT abstract sent. roles 18040 30212 30135 5\nCS ACL-ARC citation intent 1688 114 139 6\nSCIERC relation classification 3219 455 974 7\nNEWS HYPER PARTISAN partisanship 515 65 65 2\n†AGNEWS topic 115000 5000 7600 4\nREVIEWS\n†HELPFULNESS review helpfulness 115251 5000 25000 2\n†IMDB review sentiment 20000 5000 25000 2\nTable 8: Specifications of the various target task datasets. †indicates high-resource settings. Sources: C HEM PROT\n(Kringelum et al., 2016), RCT (Dernoncourt and Lee, 2017), ACL-ARC (Jurgens et al., 2018), SCIERC (Luan\net al., 2018), HYPER PARTISAN (Kiesel et al., 2019), AGN EWS (Zhang et al., 2015), HELPFULNESS (McAuley\net al., 2015), IMDB (Maas et al., 2011).\nRTE COLA MRPC STS-B SST-2 QNLI QQP MNLI-(m/mm)\nSize 0.27K 1.04K 0.41K 1.5K 0.87K 5.47K 40.43K 9.82K/9.83K\nMetrics Accuracy Matthews F1 Spearman Accuracy Accuracy Accuracy Accuracy\nWikiBERT 1.01 1.98 1.33 2.43 1.75 7.01 52.32 15.03/15.02\nPlugLM 1.73 4.41 2.22 5.94 3.86 20.01 141.15 34.60/34.58\nTable 9: Testing Latency of WikiBERT and PlugLM measured by seconds. All experiments are computed in the\nsame computational device with same batch size. The CPU is AMD EPYC 7K62 48-Core Processor. GPU is\nA100-SXM4. Driver Version is 450.156.00. CUDA Version is 11.1.\nDataset Domain Source Size\nWikipedia General https://dumps.wikimedia.org 14.35GB\nPubMed Biomedical https://github.com/naver/biobert-pretrained 28.12GB\nTable 11: List of the PubMed and Wikipedia.\n14303\nQuestion Answer Prediction Label\nHow much\nof Jack-\nsonville is\nmade up of\nwater?\nAccording to the United States Census Bureau, the city has a total area\nof 874.3 square miles (2,264 km2), making Jacksonville the largest\ncity in land area in the contiguous United States; of this, 86.66%\n(757.7 sq mi or 1,962 km2) is land and ; 13.34% (116.7 sq mi or 302\nkm2) is water.\nEntailment Entailment\nKnowledge\n(1) this article lists the 3, 143 states of america. the 50 states of the united states are divided into\n3, 007 \" counties \", political and geographic subdivisions of a state ; 236 other local governments\nand geographic places are also first - order administrative divisions of their respective state /\ndistrict / territory, but are called by different names. the latter are referred to collectively as \"\ncounty equivalents \" by the united states census bureau. the 236 county equivalents include\n100 equivalents in the territories ( such as those in puerto rico ) outside the 50 states and the\ndistrict of columbia. the large majority of counties and equivalents were organized by 1970.\nsince that time, most creations, boundary changes and dissolutions have occurred in alaska\nand virginia. among the 50 states, 44 are partitioned entirely into counties, with no county\nequivalents. louisiana is instead divided into 64 equivalent parishes.\n(2) the united states census bureau ( usc ##b ) , officially the bureau of the census , is a principal\nagency of the u . s . federal statistical system , responsible for producing data about the american\npeople and economy . the census bureau is part of the u . s . department of commerce and its\ndirector is appointed by the president of the united states . the census bureau ’ s primary mission\nis conducting the u . s . census every ten years , which all ##oca ##tes the seats of the u . s\n. house of representatives to the states based on their population . [ 1 ] the bureau ’ s various\ncensus ##es and surveys help all ##oca ##te over $ 67 ##5 billion in federal funds every year\nand it assists states , local communities , and businesses make informed decisions . [ 2 ] [ 3 ] [\n4 ] the information provided by the census informs decisions on where to build and maintain\nschools , hospitals , transportation infrastructure , and police and fire departments\n(3) the crestview – fort walton beach – destin, florida, metropolitan statistical area, as defined by\nthe united states census bureau, is a metropolitan area consisting of two counties in northwest\nflorida, anchored by the cities of crestview, florida, and fort walton beach, florida. as of the\n2010 census, the msa had a population of 235, 865, and a 2012 population estimate of 247,\n665. the metropolitan area is a part of the \" northwest corridor \" which includes the pensacola\nmetropolitan area and the panama city metropolitan area. demographics. as of the census of\n2010, there were 235, 865 people, 95, 892 households, and 63, 964 families residing within the\nmsa. the racial makeup of the msa was 81. 1 % white, 9. 3 % african american, 0. 3 % native\namerican, 2. 9 % asian, 0. 1 % pacific islander, 0. 2 % from other races, and 3. 9 % from two or\nmore races. hispanic or latino of any race were 6. 8 % of the population. according to the 2010\namerican community survey 1 - year\n(4) analog to digital conversions were achieved through steinberg, and in some cases mytek,\nconverters. the album was recorded and mixed exclusively with steinberg cubase digital audio\nworkstations on microsoft windows operating systems with waves ssl and abbey road tg12413\nplugins. it was revealed that neither brahm nor marc know how to operate autotune, so it was\nnot used. the songs were often performed to a click track, but there was no \" snapping the\ndrums to a grid \", which is a popular computerized technique to ensure that drums are in perfect\ntime while simultaneously sucking the life out of an otherwise real performance. production.\n\" tears of the enchanted mainframe \" was produced and engineered by taylor and kaducak.\nbackmasking is used on the track \" superusurper \" during an interlude that features a reversed\nreading of a passage from the george orwell novel nineteen eighty four. the album was mastered\nby geoff pesche and alex wharton at abbey road studios in london. title and artwork. \" tears of\nthe enchanted mainframe \"\n(5) the zafarnama (, lit. \" book of victory \" ) is a biography of timur written by the historian\nnizam ad - din shami. it served as the basis for a later and better - known \" zafarnama \" by sharaf\nad - din ali yazdi. one translation by felix tauer was published in prague in 1937.\nTable 13: Example from QNLI dataset.\n14304\nInput Prediction Label\nVarious approaches for computing semantic relatedness of words or\nconcepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) ,\nontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow ,\n1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997\n) or distributional ( Weeds and Weir , 2005 ).\nBackground Background\nKnowledge\n(1) instrumentation and control engineering ( ice ) is a branch of engineering that studies the\nmeasurement and control of process variables, and the design and implementation of systems\nthat incorporate them. process variables include pressure, temperature, humidity, flow, ph,\nforce and speed. ice combines two branches of engineering. instrumentation engineering\nis the science of the measurement and control of process variables within a production or\nmanufacturing area. meanwhile, control engineering, also called control systems engineering, is\nthe engineering discipline that applies control theory to design systems with desired behaviors.\ncontrol engineers are responsible for the research, design, and development of control devices\nand systems, typically in manufacturing facilities and process plants. control methods employ\nsensors to measure the output variable of the device and provide feedback to the controller so\nthat it can make corrections toward desired performance. automatic control manages a device\nwithout the need of human inputs for correction, such as cruise control for regulating a car’s\nspeed. control systems engineering activities are multi - disciplinary in nature. they focus on\nthe implementation of control systems, mainly derived by mathematical modeling. because\ninstrumentation and control play a significant role in gathering information from a system and\nchanging its parameters, they are a key part of control loops. as profession. high demand for\nengineering professionals is found in fields associated with process automation. specializations\ninclude industrial instrumentation, system dynamics, process control, and control systems.\nadditionally, technological knowledge, particularly in computer systems, is essential to the job\nof\n(2) instrumentation is the art and science of measurement and control. instrumentation may also\nrefer to:\n(3) the scientific and technological innovation ability of colleges and universities, and strength-\nening the evaluation research of the scientific and technological innovation ability and efficiency\nof colleges and universities, can we better promote the scientific and technological innovation\nability of colleges and universities. universities the evaluation of scientific and technological\ninnovation ability in colleges and universities is a complex system engineering, and the under-\nstanding of its connotation is the most important problem to be considered in the comprehensive\nevaluation. by consulting the data, it is found that the previous researches are mainly focused\non the following three aspects : 1. from the perspective of innovative resource demand and\ninnovative achievements, the scientific and technological innovation in colleges and universities\nis regarded as an organic whole composed of various elements. in the whole innovation system,\ncolleges and universities undertake the functions and tasks of knowledge production and dissem-\nination, technological innovation and transformation as well as personnel training. according\nto the relationship between innovation elements, the scientific and technological innovation\nability of colleges and universities is divided into basic strength of scientific and technological\ninnovation, scientific and technological innovation input ability, knowledge innovation ability,\ntechnological innovation ability, scientific and technological innovation output ability. science\nand technology innovation achievement transformation ability, talent innovation ability. 2. from\nthe perspective of innovation process, the ability of scientific and technological innovation\nin colleges and universities is embodied in the process of knowledge creation, knowledge\ndissemination, transformation and diffusion of technological inventions. it also includes the\ntechnological, economic and managerial abilities that the university relies on\n(4) automation engineering has two different meanings : automation engineer. automation\nengineers are experts who have the knowledge and ability to design, create, develop and manage\nmachines and systems, for example, factory automation, process automation and\n(5) this learning methodology is called blended learning. blended learning can also incorporate\nmachine learning and other such technologies to implement adaptive learning.\nTable 14: Example from ACL-ARC dataset.\n14305\nInput Prediction Label\nAlthough there are other discussions of the paragraph as a central element of dis-\ncourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt\net al. 1980 ) , all of them share a certain limitation in their formal techniques for\nanalyzing paragraph structure .\nCompareOrContrast CompareOrContrast\nKnowledge\n(1) automation engineering has two different meanings : automation engineer. automation engineers are experts who\nhave the knowledge and ability to design, create, develop and manage machines and systems, for example, factory\nautomation, process automation and warehouse automation. scope. automation engineering is the integration of\nstandard engineering fields. automatic control of various control system for operating various systems or machines to\nreduce human efforts & amp ; time to increase accuracy. automation engineers design and service electromechanical\ndevices and systems to high - speed robotics and programmable logic controllers ( plcs ). work and career after\ngraduation. graduates can work for both government and private sector entities such as industrial production,\ncompanies that create and use automation systems, for example paper industry, automotive industry, food and\nagricultural industry, water treatment, and oil & amp ; gas sector such as refineries, power plants. job description.\nautomation engineers can design, program, simulate and test automated machinery and processes, and usually are\nemployed in industries such as the energy sector in plants, car manufacturing facilities or food processing plants\nand robots. automation engineers are responsible for creating detailed design specifications and other documents,\ndeveloping automation based on specific requirements for the process involved, and conforming to international\nstandards like iec - 61508, local standards, and other process specific guidelines and specifications, simulate, test and\ncommission electronic equipment for automation.\n(2) abstract. manipulator is a powerful tool which can help people to carry out the safe operation, production\nautomation and improve the productivity of labor. based on the summary of the situation of research and development\nof manipulator, this article analyzes the functions of parts moving manipulator and carries out mechatronic design of\nparts moving manipulator according to the practical project items of parts moving manipulator of enterprises. on the\nbasis of the analysis of the performance requirement and the operating characteristics of parts moving manipulator,\nthis article analyses and designs the whole schemes for the mechanical structure, driving system, driving mode\nand the software and hardware control system of manipulator, and in which, the form of mechanical structure\nof cylindrical coordinate system is determined to be adopted in the design of manipulator, the driving scheme of\npneumatic transmission is adopted, and the system control is carried out by plc. on this basis, this article analyses the\nkinematics and dynamics of parts moving manipulator and summarizes the relationship between displacement, speed,\nacceleration and joint angle. with the progress of science and technology and the development of social economy,\nthe application area of manipulator has been becoming wider and wide. the manipulator can be found everywhere in\nhuman society. the application of manipulator has been extended to the civilian application fields such\n(3) in working environments with large manipulators, accidental collisions can cause severe personal injuries and\ncan seriously damage manipulators, necessitating the development of an emergency stop algorithm to prevent such\noccurrences. in this paper, we propose an emergency stop system for the efficient and safe operation of a manipulator\nby applying an intelligent emergency stop algorithm. our proposed intelligent algorithm considers the direction of\nmotion of the manipulator. in addition, using a new regression method, the algorithm includes a decision step that\ndetermines whether a detected object is a collision - causing obstacle or a part of the manipulator. we apply our\nemergency stop system to a two - link manipulator and assess the performance of our intelligent emergency stop\nalgorithm as compared with other models. increasing the safety of robots, especially industrial manipulators, is just\nas important as improving their performance. a collision between a manipulator and a person, for example, may\ncause severe personal injury as well as damage to the machinery. thus, it is necessary to develop an algorithm that\ncan detect collisions before they occur and make the manipulator stop before damage is done. various emergency\nstop or obstacle avoidance algorithms for robots, particularly those utilizing distance - measuring sensors [ 1 ] [ 2 ] [\n3 ] [ 4 ] or vision sensors have been reported [ 5 ] [ 6 ] [ 7 ] [ 8 ] and those algorithms using each\n(4) the reliability of kinematic trajectory of manipulators describes the ability that manipulators keep kinematic\naccurate. it is an important parameter to evaluate the performance of manipulators. the kinematic accuracy of\nmanipulators can be improved when piezoelectricity material are used as a transducer to suppress the vibration of\nflexible manipulators. first, a 3 degree - of - freedom parallel manipulator system and its dynamic equations are\nintroduced. the theory and experiment of a vibration suppression system are then presented. the calculation method\nof both error and reliability of kinematic trajectory of manipulator is further implemented. finally, the reliability of\nkinematic accuracy are calculated and analyzed for the 3 degree - of - freedom parallel manipulator with or without\nvibration suppressing control. the results show that the reliability of kinematic accuracy is improved using vibration\nsuppressing control. the reliability of kinematic accuracy of manipulators is an important indicator to evaluate the\naccuracy of manipulator motion [ 1 ]. in manipulators, light weight linkages are employed to achieve high speed\nand acceleration motions for better performance. however, the light weight linkage will result in inherent structural\nvibration, and the structural vibration leads to inaccurate kinematic trajectory of manipulators. different methods\nhave been proposed to reduce the vibration of the flexible link\n(5) abstract - economic dispatch and frequency regulation are typically viewed as fundamentally different problems in\npower systems and, hence, are typically studied separately. in this paper, we frame and study a joint problem that co -\noptimizes both slow timescale economic dispatch resources and fast timescale frequency regulation resources. we\nshow how the joint problem can be decomposed without loss of optimality into slow and fast timescale subproblems\nthat have appealing interpretations as the economic dispatch and frequency regulation problems, respectively. we\nsolve the fast timescale subproblem using a distributed frequency control algorithm that preserves network stability\nduring transients. we solve the slow timescale subproblem using an efficient market mechanism that coordinates\nwith the fast timescale subproblem. we investigate the performance of our approach on the ieee 24 - bus reliability\ntest system. abstract - economic dispatch and frequency regulation are typically viewed as fundamentally different\nproblems in power systems and, hence, are typically studied separately. in this paper, we frame and study a joint\nproblem that co - optimizes both slow timescale economic dispatch resources and fast timescale frequency regulation\nresources. we show how the joint problem can be decomposed without loss of optimality into slow and fast timescale\nsubproblems that have appealing interpretations as the economic dispatch and frequency regulation problems,\nrespectively. we solve the fast timescale subproblem\nTable 15: Example from ACL-ARC dataset.\n14306\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nthe last section\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nsection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\ncode will be released when published\n□\u0013 B1. Did you cite the creators of artifacts you used?\nsection 5\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nsection 5\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nappendix B\nC □\u0013 Did you run computational experiments?\nsection 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nsection 5\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14307\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nsection 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nsection 5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nsection 5\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n14308",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7458071708679199
    },
    {
      "name": "Macro",
      "score": 0.6886003017425537
    },
    {
      "name": "Interpretability",
      "score": 0.6471861004829407
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5709549784660339
    },
    {
      "name": "Domain knowledge",
      "score": 0.5345296859741211
    },
    {
      "name": "Scalability",
      "score": 0.49324697256088257
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45444804430007935
    },
    {
      "name": "Language model",
      "score": 0.45402082800865173
    },
    {
      "name": "Procedural knowledge",
      "score": 0.4269034266471863
    },
    {
      "name": "Key (lock)",
      "score": 0.425129234790802
    },
    {
      "name": "Natural language processing",
      "score": 0.34469783306121826
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3238140940666199
    },
    {
      "name": "Programming language",
      "score": 0.2281484305858612
    },
    {
      "name": "Database",
      "score": 0.11397695541381836
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210091018",
      "name": "Bioscience Research",
      "country": "US"
    }
  ]
}