{
    "title": "VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection",
    "url": "https://openalex.org/W4393153999",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2018313556",
            "name": "Peng Wu",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2130856360",
            "name": "Xuerong Zhou",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2583648180",
            "name": "Guansong Pang",
            "affiliations": [
                "Singapore Management University"
            ]
        },
        {
            "id": "https://openalex.org/A5100313029",
            "name": "Lingru Zhou",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2621082335",
            "name": "Qingsen Yan",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A1984679711",
            "name": "Peng Wang",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2153170226",
            "name": "Yanning Zhang",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2018313556",
            "name": "Peng Wu",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2130856360",
            "name": "Xuerong Zhou",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2583648180",
            "name": "Guansong Pang",
            "affiliations": [
                "Singapore Management University"
            ]
        },
        {
            "id": "https://openalex.org/A5100313029",
            "name": "Lingru Zhou",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2621082335",
            "name": "Qingsen Yan",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A1984679711",
            "name": "Peng Wang",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2153170226",
            "name": "Yanning Zhang",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2619082050",
        "https://openalex.org/W6811387395",
        "https://openalex.org/W3143609527",
        "https://openalex.org/W6703852671",
        "https://openalex.org/W4313291134",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W4311405981",
        "https://openalex.org/W4226283196",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W4283811196",
        "https://openalex.org/W6845288475",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4285606530",
        "https://openalex.org/W4286233435",
        "https://openalex.org/W4290055985",
        "https://openalex.org/W3215495159",
        "https://openalex.org/W2105497548",
        "https://openalex.org/W2783882151",
        "https://openalex.org/W3136793533",
        "https://openalex.org/W1522734439",
        "https://openalex.org/W3134566480",
        "https://openalex.org/W3040892440",
        "https://openalex.org/W6807389121",
        "https://openalex.org/W3110145754",
        "https://openalex.org/W2921491036",
        "https://openalex.org/W4320559344",
        "https://openalex.org/W6800895557",
        "https://openalex.org/W4221145783",
        "https://openalex.org/W4382464562",
        "https://openalex.org/W4385261821",
        "https://openalex.org/W4312424618",
        "https://openalex.org/W4226058394",
        "https://openalex.org/W2341058432",
        "https://openalex.org/W3187344870",
        "https://openalex.org/W4312480274",
        "https://openalex.org/W2963795951",
        "https://openalex.org/W4382461973",
        "https://openalex.org/W3200114289",
        "https://openalex.org/W4312558481",
        "https://openalex.org/W4225832925",
        "https://openalex.org/W4307106676",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4210416950",
        "https://openalex.org/W4312614039",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4386597062",
        "https://openalex.org/W3193402170",
        "https://openalex.org/W4287241064",
        "https://openalex.org/W4386075882",
        "https://openalex.org/W4386065412",
        "https://openalex.org/W3198377975",
        "https://openalex.org/W4386076087",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W4313194685"
    ],
    "abstract": "The recent contrastive language-image pre-training (CLIP) model has shown great success in a wide range of image-level tasks, revealing remarkable ability for learning powerful visual representations with rich semantics. An open and worthwhile problem is efficiently adapting such a strong model to the video domain and designing a robust video anomaly detector. In this work, we propose VadCLIP, a new paradigm for weakly supervised video anomaly detection (WSVAD) by leveraging the frozen CLIP model directly without any pre-training and fine-tuning process. Unlike current works that directly feed extracted features into the weakly supervised classifier for frame-level binary classification, VadCLIP makes full use of fine-grained associations between vision and language on the strength of CLIP and involves dual branch. One branch simply utilizes visual features for coarse-grained binary classification, while the other fully leverages the fine-grained language-image alignment. With the benefit of dual branch, VadCLIP achieves both coarse-grained and fine-grained video anomaly detection by transferring pre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments on two commonly-used benchmarks, demonstrating that VadCLIP achieves the best performance on both coarse-grained and fine-grained WSVAD, surpassing the state-of-the-art methods by a large margin. Specifically, VadCLIP achieves 84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and features are released at https://github.com/nwpu-zxr/VadCLIP.",
    "full_text": "VadCLIP: Adapting Vision-Language Models for Weakly Supervised\nVideo Anomaly Detection\nPeng Wu1, Xuerong Zhou1, Guansong Pang2*, Lingru Zhou1, Qingsen Yan1,\nPeng Wang1∗, Yanning Zhang1\n1ASGO, School of Computer Science, Northwestern Polytechnical University, China\n2School of Computing and Information Systems, Singapore Management University, Singapore\n{xdwupeng, zxr2333}@gmail.com, gspang@smu.edu.sg, {lingruzhou, yqs}@mail.nwpu.edu.cn,\n{peng.wang, ynzhang}@nwpu.edu.cn\nAbstract\nThe recent contrastive language-image pre-training (CLIP)\nmodel has shown great success in a wide range of image-level\ntasks, revealing remarkable ability for learning powerful vi-\nsual representations with rich semantics. An open and worth-\nwhile problem is efficiently adapting such a strong model to\nthe video domain and designing a robust video anomaly de-\ntector. In this work, we propose VadCLIP, a new paradigm\nfor weakly supervised video anomaly detection (WSV AD)\nby leveraging the frozen CLIP model directly without any\npre-training and fine-tuning process. Unlike current works\nthat directly feed extracted features into the weakly super-\nvised classifier for frame-level binary classification, VadCLIP\nmakes full use of fine-grained associations between vision\nand language on the strength of CLIP and involves dual\nbranch. One branch simply utilizes visual features for coarse-\ngrained binary classification, while the other fully leverages\nthe fine-grained language-image alignment. With the bene-\nfit of dual branch, VadCLIP achieves both coarse-grained\nand fine-grained video anomaly detection by transferring pre-\ntrained knowledge from CLIP to WSV AD task. We conduct\nextensive experiments on two commonly-used benchmarks,\ndemonstrating that VadCLIP achieves the best performance\non both coarse-grained and fine-grained WSV AD, surpassing\nthe state-of-the-art methods by a large margin. Specifically,\nVadCLIP achieves 84.51% AP and 88.02% AUC on XD-\nViolence and UCF-Crime, respectively. Code and features are\nreleased at https://github.com/nwpu-zxr/VadCLIP.\nIntroduction\nIn recent years, weakly supervised video anomaly detection\n(WSV AD, V AD) has received growing concerns due to its\nbroad application prospects. For instance, with the aid of\nWSV AD, it is convenient to develop more powerful intel-\nligent video surveillance systems and video content review\nsystems. In WSV AD, the anomaly detector is expected to\ngenerate frame-level anomaly confidences with only video-\nlevel annotations provided. The majority of current research\nin this field follows a systematic process, wherein the ini-\ntial step is to extract frame-level features using pre-trained\nvisual models, e.g., C3D (Tran et al. 2015; Sultani, Chen,\nand Shah 2018), I3D (Carreira and Zisserman 2017; Wu\n*Corresponding Authors\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n... ...\nanomaly or not\nBinary\nClassifier\n... ...\nBinary\nClassifier\n... ...\ndual branch\nD\nanomaly or not\n         anomaly category\nproductM\nM\nvisual\nencoder\n[abuse]\n[arrest]\n[assault]......\ntextual\nencoder\nvisual\nencoder\nClassification-based Paradigm\nOur Proposed Paradigm\nFigure 1: Comparisons of different paradigms for WSV AD.\net al. 2020), and ViT (Dosovitskiy et al. 2020; Li, Liu, and\nJiao 2022), followed by feeding these features into multi-\nple instance learning (MIL) based binary classifiers for the\npurpose of model training, and the final step is to detect\nabnormal events based on predicted anomaly confidences.\nDespite their simple schemes and promising results, such a\nclassification-based paradigm fails to take full advantage of\ncross-modal relationships, e.g, vision-language associations.\nDuring the past two years, we have witnessed great\nprogress in the development of vision-language pre-training\n(VLP) models (Kim, Son, and Kim 2021; Jia et al. 2021;\nWang et al. 2021; Chen et al. 2023a), e.g., CLIP (Radford\net al. 2021), for learning more generalized visual repre-\nsentations with semantic concepts. The main idea of CLIP\nis to align images and texts by contrastive learning, that\nis, pull together images and matched textual descriptions\nwhile pushing away unmatched pairs in the joint embedding\nspace. Thanks to hundreds of million noisy image-text pairs\ncrawled from the web, such models pre-trained at a large\nscale really demonstrate their strong representation learning\nas well as associations between vision and language. In view\nof the breakthrough performance of CLIP, recently, building\ntask-specific models on top of CLIP is becoming emerging\nresearch topics and applied to a broad range of vision tasks,\nand these models achieve unprecedented performance.\nAlthough CLIP and its affiliated models demonstrate the\ngreat potential on various vision tasks, these methods mainly\nfocus on the image domain. Therefore, how to efficiently\nadapt such a model learned from image-text pairs to more\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6074\ncomplex video anomaly detection task under weak super-\nvision deserves a thorough exploration. Recently, a few\nworks (Joo et al. 2023; Lv et al. 2023) attempt to make use\nof the learned knowledge of CLIP, however, these methods\nlimit their scope to directly using visual features extracted\nfrom the image encoder of CLIP, and neglect to exploit se-\nmantic relationships between vision and language.\nIn order to make effective use of generalized knowledge\nand enable CLIP to reach its full potential on WSV AD\ntask, based on the characteristics of WSV AD, there are sev-\neral critical challenges that need to be addressed. First, it\nis vital to explore ways to capture contextual dependen-\ncies across time. Second, it is essential to determine how\nto harness learned knowledge and the visual-language con-\nnections. Third, it is crucial to maintain optimal CLIP per-\nformance under weak supervision.\nIn this work, we propose a novel paradigm based on CLIP\nfor WSV AD, which is dubbed asVadCLIP. VadCLIP con-\nsists of several components to overcome the above chal-\nlenges. Specifically, for the first challenge, we present a\nlocal-global temporal adapter (LGT-Adapter), which is a\nlightweight module for video temporal relation modeling.\nLGT-Adapter involves two components, i.e., local tempo-\nral adapter and global temporal adapter, wherein the former\nmainly captures local temporal dependencies with high ef-\nficiency, since in most cases the current events are highly\nrelated to the adjacent events, and the latter smooths fea-\nture information in a more holistic view with less parame-\nters. For the second challenge, unlike current methods (Joo\net al. 2023; Lv et al. 2023) that solely use visual features,\nwe encourage VadCLIP to also leverage textual features to\npreserve learned knowledge as much as possible. As shown\nin Figure 1, VadCLIP is devised as a dual-branch fashion,\nwhere one simply and directly utilizes visual features for\nbinary classification (C-branch), while the other employs\nboth visual and textual features for language-image align-\nment (A-branch). Moreover, such dual branch seamlessly\nachieves coarse-grained and fine-grained WSV AD (Wu, Liu,\nand Liu 2022). For A-branch, we build bridge between\nvideos and video-level textual labels. Moreover, we propose\ntwo prompt mechanisms (Wu et al. 2023), i.e., learnable\nprompt and visual prompt, to specify that the succinct text\nis about the video. Learnable prompt does not require exten-\nsive expert knowledge compared to the handcrafted prompt,\neffectively transfers pre-trained knowledge into the down-\nstream WSV AD task. Visual prompt is inspired by that vi-\nsual contexts can make the text more accurate and discrimi-\nnate. Imagine that if there is a car in the video, two types of\nabnormal events of ”car accident” and ”fighting” would be\nmore easily distinguished. Hence, In the visual prompt, we\nfocus on anomaly information in videos and integrate these\nanomaly-focus visual contents from C-branch with textual\nlabels from A-branch for automatic prompt engineering.\nSuch a practice seamlessly creates connections between dual\nbranch. For the third challenge, multiple instance learning\n(MIL) (Sultani, Chen, and Shah 2018; Wu et al. 2020) is the\nmost commonly used method. For the language-visual align-\nments in A-branch, we introduce a MIL-Align mechanism,\nthe core idea is to select the most matched video frames for\neach label to represent the whole video.\nNote that during training, the weights of CLIP image and\ntext encoders are kept fixed, and the gradients are back-\npropagated to optimise these learnable parameters of the de-\nvised adapter and prompt modules.\nOverall, the contributions of our work are threefold:\n(1) We present a novel diagram, i.e., VadCLIP, which in-\nvolves dual branch to detect video anomaly in visual classifi-\ncation and language-visual alignment manners, respectively.\nWith the benefit of dual branch, VadCLIP achieves both\ncoarse-grained and fine-grained WSV AD. To our knowl-\nedge, VadCLIP is the first work to efficiently transfer pre-\ntrained language-visual knowledge to WSV AD.\n(2) We propose three non-vital components to address new\nchallenges led by the new diagram. LGT-Adapter is used to\ncapture temporal dependencies from different perspectives;\nTwo prompt mechanisms are devised to effectively adapt the\nfrozen pre-trained model to WSV AD task; MIL-Align re-\nalizes the optimization of alignment paradigm under weak\nsupervision, so as to preserve the pre-trained knowledge as\nmuch as possible.\n(3) We show that strength and effectiveness of VadCLIP on\ntwo large-scale popular benchmarks, and VadCLIP achieves\nstate-of-the-art performance, e.g., it obtains unprecedented\nresults of 84.51% AP and 88.02% AUC on XD-Violence\nand UCF-Crime respectively, surpassing current classifica-\ntion based methods by a large margin.\nRelated Work\nWeakly Supervised Video Anomaly Detection\nRecently, some researchers (Zaheer et al. 2020; Feng, Hong,\nand Zheng 2021; Wu et al. 2021; Chen et al. 2023b) have\nproposed weakly supervised methods for V AD. Sultani et\nal. (Sultani, Chen, and Shah 2018) firstly proposed a deep\nmultiple instance learning model, which considers a video\nas a bag and its multiple segments as instances. Then sev-\neral follow-up works made effort to model temporal rela-\ntions based on self-attention models and transformers. For\nexample, Zhong et al. (Zhong et al. 2019) proposed a graph\nconvolutional network (GCN) based method to model the\nfeature similarity and temporal consistency between video\nsegments. Tian et al. (Tian et al. 2021) used a self-attention\nnetwork to capture the global temporal context relationship\nof videos. Li et al. (Li, Liu, and Jiao 2022) proposed a\ntransformer based multi-sequence learning framework, and\nHuang et al. (Huang et al. 2022) proposed a transformer\nbased temporal representation aggregation framework. Zhou\net al. (Zhou, Yu, and Yang 2023) presented a global and local\nmulti-head self attention module for the transformer layer\nto obtain more expressive embeddings for capturing tempo-\nral dependencies in videos. The above methods only detect\nwhether video frames are anomalous, on the contrary, Wu\net al. (Wu, Liu, and Liu 2022) proposed a fine-grained WS-\nV AD method, which distinguishes between different types\nof anomalous frames. More recently, the CLIP model has\nalso attracted great attentions in the V AD community. Based\non visual features of CLIP, Lv et al. (Lv et al. 2023) pro-\nposed a new MIL framework called Unbiased MIL (UMIL)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6075\nto learn unbiased anomaly features that improve WSV AD\nperformance. Joo et al. (Joo et al. 2023) proposed to employ\nvisual features from CLIP to efficiently extract discrimina-\ntive representations, and then model long- and short-range\ntemporal dependencies and nominate the snippets of interest\nby leveraging temporal self-attention. All the above meth-\nods are based on the classification paradigm, which detect\nanomalous events by predicting the probability of anoma-\nlous frames. However, this classification paradigm does not\nfully utilize the semantic information of textual labels.\nVision-Language Pre-training\nVision-language pre-training has achieved impressive\nprogress over the past few years, which aims to learn\nthe semantic correspondence between vision and language\nthrough pre-training on large-scale data. As one of the most\nrepresentative works, CLIP has shown impressive perfor-\nmance on a range of vision-language downstream tasks, in-\ncluding image classification (Zhou et al. 2022a), image cap-\ntioning (Mokady, Hertz, and Bermano 2021), object detec-\ntion (Zhou et al. 2022b), scene text detection (Yu et al.\n2023), dense prediction (Zhou et al. 2022c; Rao et al.\n2022), and so on. Recently, some follow-up works at-\ntempted to leverage the pre-trained models for video do-\nmains. For example, CLIP4Clip (Luo et al. 2022) trans-\nferred the knowledge of CLIP model to the video-text re-\ntrieval, some works (Wang, Xing, and Liu 2021; Lin et al.\n2022; Ni et al. 2022) attempted to take advantages of CLIP\nfor video recognition, furthermore, CLIP is used to tackle\nthe more complex video action localization task (Nag et al.\n2022; Ju et al. 2022). More generally, Ju et al. (Ju et al. 2022)\npresented a simple yet strong baseline to efficiently adapt\nthe pre-trained image-based visual-language model, and ex-\nploited its powerful ability for general video understanding.\nIn this work, we deeply explore how to adapt pre-trained\nvision-language knowledge of CLIP from image-level into\nvideo-level downstream WSV AD efficiently.\nMethod\nProblem Definition\nThe WSV AD task supposes that only video-level labels are\navailable during the training stage. Given a video v, if all\nframes of this video do not contain abnormal events, this\nvideo is defined as normal with the label y = 0; Otherwise,\nif there is at least one frame contains abnormal events, this\nvideo is labeled as abnormal with the label y = 1. The goal\nof WSV AD task is to train a detection model that is able to\npredict frame-level anomaly confidences while only video-\nlevel annotations are provided.\nPrevious works generally make use of pre-trained 3D\nconvolutional models, e.g., C3D (Tran et al. 2015) and\nI3D (Carreira and Zisserman 2017), to extract video fea-\ntures, and then feed these features into MIL-based binary\nclassifiers, such paradigms are referred as the classification-\nbased paradigm in this paper. Recently, CLIP, as a large-\nscale language-vision pre-trained model, has revolutionized\nmany fields in computer vision, and has shown great gen-\neralization capabilities across a wide range of downstream\ntasks. Inspired by CLIP, our work not only uses the image\nencoder of CLIP as the backbone to extract video features,\nbut also attempts to utilize the text encoder of CLIP to take\nfull advantage of the powerful associations between visual\ncontents and textual concepts. Our work is demonstrated in\nFigure 2.\nLocal and Global Temporal Adapter\nAs we know, CLIP is pre-trained on large-scale image-text\npairs crawled from the web. In this section, we investigate\nhow to model temporal dependencies and bridge the gap be-\ntween the image domain and video domain for CLIP. Mean-\nwhile, it is also significant to learn long-range and short-\nrange temporal dependencies for WSV AD task (Zhou, Yu,\nand Yang 2023; Wu and Liu 2021). From the perspective of\nthe efficiency and receptive field, we design a new temporal\nmodeling method compatible with local and global receptive\nfield.\nLocal Module. To capture local temporal dependencies,\nwe introduce a transformer encoder layer on top of frame-\nlevel features Xclip ∈ Rn×d from the frozen image encoder\nof CLIP, where n is the length of video, d is the dimension\nsize, which is set as 512 in this work. Note that this layer\ndiffers from the ordinary transformer encoder layer since\nit limits self-attention computation to local windows (Liu\net al. 2021) instead of the global scope. Specifically, we split\nframe-level features into equal-length and overlapping win-\ndows over temporal dimension, self-attention calculation is\nlimited within each window, and no information exchange\namong windows. Such an operation possesses local recep-\ntive field like the convolution, and leads to the lower compu-\ntation complexity.\nGlobal Module. To further capture global temporal de-\npendencies, we introduce a lightweight GCN module fol-\nlowing local module, we adopt GCN to capture global\ntemporal dependencies due to its widespread adoption and\nproven performance in V AD (Zhong et al. 2019; Wu et al.\n2020; Wu and Liu 2021). Following the setup in (Zhong\net al. 2019; Wu et al. 2020), we use GCN to model global\ntemporal dependencies from the perspective of feature simi-\nlarity and relative distance, it can be summarized as follows,\nXg = gelu ([Softmax (Hsim) ;Softmax (Hdis)] XlW)\n(1)\nwhere Hsim and Hdis are the adjacency matrices, the Soft-\nmax normalization is used to ensure the sum of each row\nof Hsim and Hdis equals to one. Xl is the frame-level video\nfeature obtained from local module,W is the only one learn-\nable weight that is used to transform the feature space, this\nsetup demonstrates the lightweight of global module.\nFeature similarity branch is designed to generated a\nsimilarity relationship adjacency matrix for GCN. We use\nthe frame-wise cosine similarity to calculate the adjacency\nmatrix Hsim, which is presented as follows,\nHsim = XlX⊤\nl\n∥Xl∥2 · ∥Xl∥2\n(2)\nwe also use the thresholding operation to filter weak rela-\ntions (Wu et al. 2020).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6076\nBinary\nClassifier\n[abuse]\n[arrest]\n[assault]......\nCLIP\nImage\nEncoder\nCLIP\nText\nEncoder\nvideo \nfeatures\ncoarse-grained\nanomaly confidence\nfine-grained \nalignment map\nproduct\nK\nm\nLGT\nAdapter\nVisual\nPrompt\nA\nD M\nℒ푏�\u0000\nℒ��\u0000\nm\nTop-K\nMIL-Alignlabel \nfeatures\naggregation\nvideo-level\nbinary GT\nvideo-level\ncategory GT\nA-branch\nC-branch\nLearnable\nPrompt\n..\n.\n..\n.\n+\nFFN\nGlobal\nGCN\nLocal\nTransformer\nEncoder\nVision\nText\nFigure 2: The framework of our proposed VadCLIP.\nPosition distance branch is used to capture long-range\ndependencies based on positional distance between each two\nframes. The proximity adjacency matrix is shown as follows:\nHdis(i, j) =−|i − j|\nσ (3)\nthe proximity relation between ith and jth frames only de-\ntermined by their relative temporal position. σ is a hyper-\nparameter to control the range of influence of distance rela-\ntion. Both local transformer and GCN layer employ residual\nconnection to prevent feature over-smoothing.\nDual Branch and Prompt\nDual Branch. Unlike other previous WSV AD works, our\nVadCLIP contains dual branch, more precisely, in addition\nto the traditional binary classification branch (C-Branch),\nwe also introduce a novel video-text alignment branch,\ndubbed as A-Branch. Specifically, after temporal modeling,\nthe video feature Xg is fed into a fully connected (FC) layer\nto obtain the final video featureX ∈ Rn×d. In C-Branch, we\nfeed X into a binary classifier that contains a feed-forward\nnetwork (FFN) layer, an FC layer and a Sigmoid activation\nto obtain the anomaly confidence A ∈ Rn×1.\nA = Sigmoid (FC (FFN (X) +X)) (4)\nIn A-Branch, textual labels, e.g., abuse, riot, fighting, etc, are\nno longer encoded as one-hot vectors, on the contrary, they\nare encoded into class embeddings using the text encoder\nof CLIP, we leverage the frozen pre-trained text encoder\nof CLIP throughout, as the text encoder can provide lan-\nguage knowledge prior for video anomaly detection. Then\nwe calculate the match similarities between class embed-\ndings and frame-level visual features to obtain the alignment\nmap M ∈ Rn×m, where m is the number of text labels,\nsuch a setup is similar to that of CLIP. In A-Branch, each\ninput text label represents a class of abnormal events, thus\nnaturally achieving fine-grained WSV AD.\nLearnable Prompt. In WSV AD, text labels are words\nor phrases, which are too succinct to summarize abnor-\nmal events very well. To learn robust transferability of\ntext embedding, we take inspirations from CoOp (Zhou\net al. 2022a), and add the learnable prompt to original class\nembeddings. Concretely, the original text labels are first\ntransformed into class tokens through CLIP tokenizer, i.e.,\ntinit = Tokenizer (Label), where Label is the discrete text\nlabel, e.g., fighting, shooting, road accident, etc. Then we\nconcatenate tinit with the learnable prompt {c1, ..., cl} that\ncontains l context tokens to form a complete sentence token,\nthus the input of text encoder is presented as follows:\ntp = {c1, ..., tinit, ..., cl} (5)\nhere we place the class token at the middle of a sequence.\nThen this sequence token is added to the positional embed-\nding to obtain positional information, and finally, the text\nencoder of CLIP takes as input tp and generates class em-\nbedding tout ∈ Rd.\nAnomaly-Focus Visual Prompt. In order to further im-\nprove the representation ability of text labels for abnormal\nevents, we investigate how to use visual contexts to refine\nthe class embedding, since visual contexts can make the suc-\ncinct text labels more accurate. To this end, we propose an\nanomaly-focus visual prompt, which focuses on the visual\nembeddings in abnormal segments, and aggregate these em-\nbeddings as the video-level prompt for class embeddings.\nWe first use the anomaly confidence A obtained from C-\nBranch as the anomaly attention, then compute the video-\nlevel prompt by the dot product of anomaly attention and\nvideo feature X, which is presented as follows,\nV = Norm\n\u0000\nA⊤X\n\u0001\n(6)\nwhere Norm is the normalization, and V ∈ Rd is the\nanomaly-focus visual prompt. We then add V to the class\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6077\nembedding tout and obtain the final instance-specific class\nembedding T by a simple FFN layer and a skip connection.\nT = FFN (ADD (V, tout)) +tout (7)\nwhere ADD is the element-wise addition. Such a imple-\nmentation allows class embeddings to extract the related vi-\nsual context from videos.\nWith X and T in hands, we calculate the match similar-\nities between all class embeddings and frame-level visual\nfeatures to obtain the alignment map M.\nObjective Function\nFor C-Branch, we follow previous works (Wu et al. 2020)\nand use Top-K mechanism to select K high anomaly con-\nfidences in both abnormal and normal videos as the video-\nlevel predictions. Then we use the binary cross entropy be-\ntween video-level predictions and ground-truth to compute\nclassification loss Lbce.\nFor A-Branch, we are confronted with new challenges:\n1) there is no anomaly confidence; 2) facing multi-classes\ninstead of binary classes. To address this dilemma, we pro-\npose the MIL-Align mechanism which is similar to vanilla\nMIL. Specifically, we consider the align map M since it\nexpresses the similarity between frame-level video features\nand all class embeddings. For each row, we select top K\nsimilarities and compute the average to measure the align-\nment degree between this video and the current class. Then\nwe can obtain a vector S = {s1, ..., sm} that represents the\nsimilarity between this video and all classes. We hope the\nvideo and its paired textual label emit the highest similarity\nscore among others. To achieve this, the multi-class predic-\ntion is firstly computed as follows,\npi = exp (si/τ)\nP\nj exp (sj/τ) (8)\nwhere pi is the prediction with respect to the ith class, and\nτ refers to the temperature hyper-parameter for scaling. Fi-\nnally, the alignment loss Lnce can be computed by the cross\nentropy.\nIn addition to classification loss Lbce and alignment loss\nLnce, we also introduce a contrastive loss to slightly push the\nnormal class embedding and other abnormal class embed-\ndings away, here we first calculate cosine similarity between\nnormal class embedding and other abnormal class embed-\ndings, and then compute the contrastive lossLcts as follows,\nLcts =\nX\nj\nmax\n\u0012\n0, t⊤\nn taj\n∥tn∥2 · ∥taj∥2\n\u0013\n(9)\nwhere tn is the normal class embedding, and ta is abnormal\nclass embeddings.\nOverall, the final total objective of VadCLIP is given by:\nL = Lbce + Lnce + λLcts (10)\nInference\nVadCLIP contains dual branch that enables itself to ad-\ndress both fine-grained and coarse-grained WSV AD tasks.\nIn regard to fine-grained WSV AD, we follow previous\nworks (Wu, Liu, and Liu 2022) and utilize a thresholding\nstrategy on alignment map M to predict anomalous events.\nIn regard to coarse-grained WSV AD, there are two ways to\ncompute the frame-level anomaly degree. The first one is to\ndirectly use the anomaly confidences in C-Branch, the sec-\nond one is to use the alignment map in A-Branch, specif-\nically, subtracting the similarities between videos and the\nnormal class by one is the anomalous degree. Finally, we\nselect the best of these two ways for computing the frame-\nlevel anomaly degree.\nExperiments\nDatasets and Evaluation Metrics\nDatasets. We conduct experiments on two popular WS-\nV AD datasets, i.e., UCF-Crime and XD-Violence. Notably,\ntraining videos only have video-level labels on both datasets.\nEvaluation Metrics. For coarse-grained WSV AD, we fol-\nlow previous works, and utilize the frame-level Average Pre-\ncision (AP) for XD-Violence, and frame-level AUC and the\nAUC of anomaly videos (termed as AnoAUC) for UCF-\nCrime. For fine-grained WSV AD, we follow the standard\nevaluation protocol in video action detection and use the\nmean Average Precision (mAP) values under different in-\ntersection over union (IoU) thresholds. In this work, we use\nIoU thresholds ranging from 0.1 to 0.5 with a stride of 0.1\nto compute mAP values. Meanwhile, we also report an aver-\nage of mAP (A VG). Note that we only compute mAP on the\nabnormal videos in the test set.\nImplementation Details\nFor network structure, frozen image and text encoders are\nadopted from pre-trained CLIP (ViT-B/16). FFN is a stan-\ndard layer from Transformer, and ReLU is replaced with\nGELU. For hyper-parameters, we set σ in Eq.3 as 1, τ in\nEq.8 as 0.07, and the context length l as 20. For window\nlength in LGT-Adapter, we set it as 64 and 8 on XD-Violence\nand UCF-Crime, respectively. For λ in Eq.10, we set it as\n1 × 10−4 and 1 × 10−1 on XD-Violence and UCF-Crime,\nrespectively. For model training, VadCLIP is trained on a\nsingle NVIDIA RTX 3090 GPU using PyTorch. We use\nAdamW as the optimizer with batch size of 64. On XD-\nViolence, the learning rate and total epoch are set as2×10−5\nand 20, respectively, and on UCF-Crime, the learning rate\nand total epoch are set as 1 × 10−5 and 10, respectively.\nComparison with State-of-the-Art Methods\nVadCLIP can simultaneously realize coarse-grained and\nfine-grained WSV AD, therefore we present the performance\nof VadCLIP and compare it with several state-of-the-art\nmethods on coarse-grained and fine-grained WSV AD tasks.\nFor the sake of fairness, all comparison methods use the\nsame visual features extracted from CLIP as VadCLIP .\nCoarse-grained WSV AD Results. We show comparison\nresults in Tables 1 and 2. Here Ju et al. (Ju et al. 2022)\nis a CLIP-based work for action recognition, which is sig-\nnificantly inferior to our method. Such results demonstrate\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6078\nchallenges on WSV AD task, and also show the strength\nof our method with respect to Ju et al. (Ju et al. 2022)\nfor the specific WSV AD task. Besides, we found that Vad-\nCLIP significantly outperforms both semi-supervised meth-\nods and classification-based weakly supervised methods on\ntwo commonly-used benchmarks and across all evaluation\nmetrics. More precisely, VadCLIP attains 84.51% AP and\n82.08% AUC on XD-Violence and UCF-Crime, respec-\ntively, a new state-of-the-art on both datasets. By compar-\nison, VadCLIP achieves an absolute gain of 2.3% and 2.1%\nin terms of AP over the best competitors CLIP-TSA (Joo\net al. 2023) and DMU (Zhou, Yu, and Yang 2023) on XD-\nViolence, and on UCF-Crime, VadCLIP also outperforms\nthem by 0.4% and 1.3% in terms of AUC. More impor-\ntantly, among all comparison methods, A VVD (Wu, Liu, and\nLiu 2022) uses fine-grained class labels exactly, and it only\nachieves 78.10% AP and 82.45% AUC on XD-Violence and\nUCF-Crime, respectively, which lags behind VadCLIP by a\nlarge margin. Such a result shows simply using fine-grained\nlabels cannot lead to performance gains, since excessive in-\nputs of label increases the difficulty of binary classifica-\ntion. The performance advantage of VadCLIP is partially at-\ntributable to the vision-language associations, since all com-\nparison baselines use the same visual features as VadCLIP.\nFine-grained WSV AD Results. For fine-grained WS-\nV AD task, we compare VadCLIP with previous works\nA VVD and Sultani et al. (Sultani, Chen, and Shah 2018; Wu,\nLiu, and Liu 2022) in Tables 3 and 4. Here A VVD is the\nfirst work to propose the fine-grained WSV AD, and we re-\nimplement it with visual features of CLIP, then we also fine-\ntune Sultani et al. based on the setup in A VVD for adapt-\ning fine-grained WSV AD. As we can see, the fine-grained\nWSV AD is a more challenging task with respect to coarse-\nfined WSV AD since the former needs to consider both multi-\ncategory classification accuracy and detection segment con-\ntinuity. On this task, VadCLIP is also clearly superior to\nthese excellent comparison methods on both XD-Violence\nand UCF-Crime datasets. For instance, On XD-Violence,\nVadCLIP achieves a performance improvement of 13.1%\nand 4.5% in terms of A VG compared to Sultani et al. and\nA VVD.\nAblation Studies\nExtensive ablations are carried out on XD-Violence dataset.\nHere we choose the similarity map to compute the frame-\nlevel anomaly degree for coarse-grained WSV AD.\nEffectiveness of LGT-Adapter. As shown in Table 5,\nfirstly, without the assistance of LGT-Adapter for tempo-\nral modeling, the baseline model only achieves 72.22% AP\nand 15.64% A VG, this results in a considerably drop of\n12.3% AP and 9.1% A VG. Secondly, only using global\ntransformer encoder layer, local transformer encoder layer\nor GCN layer gets clear performance boosts, especially in\nterms of AP, which convincingly indicates transformer en-\ncoder and GCN both can efficiently capture temporal de-\npendencies by means of the self-attention mechanism across\nvideo frames. Thirdly, the combination of global transformer\nencoder and GCN yields the slightly improved performance\nCategory Method AP(%)\nSVM baseline 50.80\nSemi OCSVM (1999) 28.63\nHasan et al. (2016) 31.25\nJu et al. (2022) 76.57\nSultani et al. (2018) 75.18\nWu et al. (2020) 80.00\nRTFM (2021) 78.27\nWeak A VVD (2022) 78.10\nDMU (2023) 82.41\nCLIP-TSA (2023) 82.17\nVadCLIP (Ours) 84.51\nTable 1: Coarse-grained comparisons on XD-Violence.\nMethod AUC(%) Ano-AUC(%)\nSVM baseline 50.10 50.00\nOCSVM (1999) 63.20 51.06\nHasan et al. (2016) 51.20 39.43\nJu et al. (2022) 84.72 62.60\nSultani et al. (2018) 84.14 63.29\nWu et al. (2020) 84.57 62.21\nA VVD (2022) 82.45 60.27\nRTFM (2021) 85.66 63.86\nDMU (2023) 86.75 68.62\nUMIL (2023) 86.75 68.68\nCLIP-TSA (2023) 87.58 N/A\nVadCLIP (Ours) 88.02 70.23\nTable 2: Coarse-grained comparisons on UCF-Crime.\nMethod mAP@IOU(%)\n0.1 0.2\n0.3 0.4 0.5 A VG\nRandom 1.82 0.92\n0.48 0.23 0.09 0.71\nSultani et al. 22.72 15.57\n9.98 6.20 3.78 11.65\nA VVD 30.51 25.75\n20.18 14.83 9.79 20.21\nVadCLIP 37.03 30.84 23.38 17.90 14.31 24.70\nTable 3: Fine-grained comparisons on XD-Violence.\nMethod mAP@IOU(%)\n0.1 0.2 0.3 0.4 0.5 A VG\nRandom 0.21 0.14 0.04 0.02 0.01 0.08\nSultani et al. 5.73 4.41 2.69 1.93 1.44 3.24\nA VVD 10.27 7.01 6.25 3.42 3.29 6.05\nVadCLIP 11.72 7.83 6.40 4.53 2.93 6.68\nTable 4: Fine-grained comparisons on UCF-Crime.\nin terms of AP (+0.4%) over the combination of local trans-\nformer encoder and GCN, while the latter achieves signif-\nicantly better performance in terms of A VG (+3.9%). We\nalso attempt a combination of local Transformer encoder and\nglobal Transformer encoder, which results in significant per-\nformance degradation in terms of AP listed in the 5th row.\nThe possible reason is that, compared to Transformer, GCN\ncan be regarded as a lightweight variant, and fewer parame-\nters prevent learned knowledge of CLIP from being affected\nduring the transfer process. Therefore, local transformer en-\ncoder and GCN are the optimum combination, which can\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6079\nMethod AP(%) A VG(%)\nBaseline (w/o temporal modeling) 72.22 15.64\nGlobal TF-Encoder 82.54 16.76\nLocal TF-Encoder 81.18 18.41\nOnly GCN 81.56 23.31\nLocal TF-Encoder+ Global TF-Encoder 79.91 19.78\nGlobal TF-Encoder+GCN 84.87 20.84\nLGT-Adapter 84.51 24.70\nTable 5: Effectiveness of LGT-Adapter.\nC-Branch A-Branch L-Prompt V-Prompt AP(%)√ 80.53√ 68.15√ √ 75.03√ √ √ 78.27√ √ √ 82.35√ √ √ √ 84.51\nTable 6: Effectiveness of dual branch.\nAP(%) A VG(%)\nHand-crafted Prompt 81.06 22.46\nLearnable-Prompt 84.51 24.70\nAverage-Frame Visual Prompt 81.34 21.57\nAnomaly-Focus Visual Prompt 84.51 24.70\nTable 7: Effectiveness of prompt.\ncapture different range temporal dependencies.\nEffectiveness of Dual Branch. As shown in Table 6, our\nmethod with only C-Branch belongs to the classification-\nbased paradigm, and can compete current state-of-the-art\nmethods on XD-Violence. On the other hand, our method\nwith only A-Branch achieves unsatisfactory performance in\nterms of AP since it is mainly focus on fine-grained WS-\nV AD. With the assistance of coarse-grained classification\non feature optimization in C-Branch, A-Branch obtains a\nleap of about 7% AP improvement. By further adding the\nlearnable prompt and visual prompt that are ad-hoc designs\nin A-Branch, we notice that a consistent performance im-\nprovement can be achieved, leading to a new state-of-the-art.\nThese results clearly show dual branch that contains coarse-\ngrained classification paradigm and fine-grained alignment\nparadigm can boost the performance by leveraging the com-\nplementary of different granularity.\nEffectiveness of Prompt. As shown in Table 7, using\nhand-crafted prompt results in a drop of 3.5% AP and\n2.2% A VG, demonstrating that the learnable prompt has bet-\nter potential for adapting pre-trained knowledge from the\nlarge language-vision model to WSV AD task. Furthermore,\nsimply using the average of frame-level features in visual\nprompt (Ni et al. 2022) produces a drop of 3.2% AP and\n3.1% A VG, such results show focusing on abnormal snip-\npets in the video can support VadCLIP to obtain more accu-\nrate instance-specific text representations, which boosts the\nability of video-language alignment that is useful for WS-\nV AD task.\nFigure 3: t-SNE visualizations for XD-Violence. Left: Raw\nCLIP features; Right: VadCLIP features.\nAbuse Riot Normal Event\nShooting Robbery Normal Event\nFigure 4: Qualitative results of coarse-grained WSV AD.\nQualitative Analyses\nFeature Discrimination Visualization. We visualize the\nfeature distribution by using t-SNE for XD-Violence, and\npresent results in Figure 3, where star icons denote textual\nlabel features. As we can see, although CLIP has learned\ngeneralized capacities based on image-text pairs, such ca-\npacities still cannot allow it to effectively distinguish dif-\nferent categories for WSV AD due to intrinsic problems on\nWSV AD task. After specialized optimization by VadCLIP,\nthese visual features have more distinguishable boundaries\nand also surround the corresponding text class features.\nCoarse-grained Qualitative Visualization. We illustrate\nthe qualitative visualizations of coarse-grained WSV AD in\nFigure 4, where the blue curves represent the anomaly pre-\ndiction, and the pink regions correspond to the ground-truth\nabnormal temporal location. As we can see, VadCLIP pre-\ncisely detects abnormal region of different categories on two\nbenchmarks, meanwhile, it also produces considerably low\nanomaly predictions on normal videos.\nConclusion\nIn this work, we propose a new paradigm named VadCLIP\nfor weakly supervised video anomaly detection. To effi-\nciently adapt the pre-trained knowledge and vision-language\nassociations from frozen CLIP to WSV AD task, we first de-\nvise a LGT-Adapter to enhance the ability of temporal mod-\neling, and then we design a series of prompt mechanisms to\nimprove the adaptation of general knowledge to the specific\ntask. Finally we introduce the MIL-Align operation for facil-\nitating the optimization of vision-language alignment under\nweak supervision. We empirically verify the effectiveness\nof VadCLIP through state-of-the-art performance and suffi-\ncient ablations on two WSV AD benchmarks. In future, we\nwill continue to explore vision-language pre-trained knowl-\nedge and further devote to open-set V AD task.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6080\nAcknowledgments\nThis work is supported by the National Natural Sci-\nence Foundation of China (No. 62306240, U23B2013,\nU19B2037, 62301432, 62101453), China Postdoctoral Sci-\nence Foundation (No. 2023TQ0272), National Key R&D\nProgram of China (No.2020AAA0106900), Shaanxi Provin-\ncial Key R&D Program (No.2021KWZ-03), Natural Sci-\nence Basic Research Program of Shaanxi (No. 2021JCW-03,\n2023-JC-QN-0685), and the Fundamental Research Funds\nfor the Central Universities (No. D5000220431).\nReferences\nCarreira, J.; and Zisserman, A. 2017. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 6299–6308.\nChen, F.-L.; Zhang, D.-Z.; Han, M.-L.; Chen, X.-Y .; Shi, J.;\nXu, S.; and Xu, B. 2023a. Vlp: A survey on vision-language\npre-training. Machine Intelligence Research, 20(1): 38–56.\nChen, Y .; Liu, Z.; Zhang, B.; Fok, W.; Qi, X.; and Wu, Y .-C.\n2023b. MGFN: Magnitude-Contrastive Glance-and-Focus\nNetwork for Weakly-Supervised Video Anomaly Detection.\nvolume 37, 387–395.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFeng, J.-C.; Hong, F.-T.; and Zheng, W.-S. 2021. Mist: Mul-\ntiple instance self-training framework for video anomaly de-\ntection. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 14009–14018.\nHasan, M.; Choi, J.; Neumann, J.; Roy-Chowdhury, A. K.;\nand Davis, L. S. 2016. Learning temporal regularity in video\nsequences. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 733–742.\nHuang, C.; Liu, C.; Wen, J.; Wu, L.; Xu, Y .; Jiang, Q.; and\nWang, Y . 2022. Weakly Supervised Video Anomaly Detec-\ntion via Self-Guided Temporal Discriminative Transformer.\nIEEE Transactions on Cybernetics.\nJia, C.; Yang, Y .; Xia, Y .; Chen, Y .-T.; Parekh, Z.; Pham, H.;\nLe, Q.; Sung, Y .-H.; Li, Z.; and Duerig, T. 2021. Scaling\nup visual and vision-language representation learning with\nnoisy text supervision. In International Conference on Ma-\nchine Learning, 4904–4916. PMLR.\nJoo, H. K.; V o, K.; Yamazaki, K.; and Le, N. 2023. Clip-tsa:\nClip-assisted temporal self-attention for weakly-supervised\nvideo anomaly detection. In 2023 IEEE International Con-\nference on Image Processing (ICIP), 3230–3234. IEEE.\nJu, C.; Han, T.; Zheng, K.; Zhang, Y .; and Xie, W. 2022.\nPrompting visual-language models for efficient video under-\nstanding. In Computer Vision–ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23–27, 2022, Proceed-\nings, Part XXXV, 105–124. Springer.\nKim, W.; Son, B.; and Kim, I. 2021. Vilt: Vision-and-\nlanguage transformer without convolution or region super-\nvision. In International Conference on Machine Learning,\n5583–5594. PMLR.\nLi, S.; Liu, F.; and Jiao, L. 2022. Self-training multi-\nsequence learning with transformer for weakly supervised\nvideo anomaly detection. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 36, 1395–1403.\nLin, Z.; Geng, S.; Zhang, R.; Gao, P.; de Melo, G.; Wang, X.;\nDai, J.; Qiao, Y .; and Li, H. 2022. Frozen clip models are\nefficient video learners. In Computer Vision–ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23–27,\n2022, Proceedings, Part XXXV, 388–404. Springer.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n10012–10022.\nLuo, H.; Ji, L.; Zhong, M.; Chen, Y .; Lei, W.; Duan, N.; and\nLi, T. 2022. CLIP4Clip: An empirical study of CLIP for end\nto end video clip retrieval and captioning. Neurocomputing,\n508: 293–304.\nLv, H.; Yue, Z.; Sun, Q.; Luo, B.; Cui, Z.; and Zhang, H.\n2023. Unbiased Multiple Instance Learning for Weakly\nSupervised Video Anomaly Detection. arXiv preprint\narXiv:2303.12369.\nMokady, R.; Hertz, A.; and Bermano, A. H. 2021. Clip-\ncap: Clip prefix for image captioning. arXiv preprint\narXiv:2111.09734.\nNag, S.; Zhu, X.; Song, Y .-Z.; and Xiang, T. 2022. Zero-shot\ntemporal action detection via vision-language prompting. In\nComputer Vision–ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23–27, 2022, Proceedings, Part III,\n681–697. Springer.\nNi, B.; Peng, H.; Chen, M.; Zhang, S.; Meng, G.; Fu, J.;\nXiang, S.; and Ling, H. 2022. Expanding language-image\npretrained models for general video recognition. In Com-\nputer Vision–ECCV 2022: 17th European Conference, Tel\nAviv, Israel, October 23–27, 2022, Proceedings, Part IV, 1–\n18. Springer.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nRao, Y .; Zhao, W.; Chen, G.; Tang, Y .; Zhu, Z.; Huang,\nG.; Zhou, J.; and Lu, J. 2022. Denseclip: Language-guided\ndense prediction with context-aware prompting. InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 18082–18091.\nSch¨olkopf, B.; Williamson, R. C.; Smola, A.; Shawe-Taylor,\nJ.; and Platt, J. 1999. Support vector method for novelty de-\ntection. Advances in neural information processing systems,\n12.\nSultani, W.; Chen, C.; and Shah, M. 2018. Real-world\nanomaly detection in surveillance videos. In Proceedings of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6081\nthe IEEE conference on computer vision and pattern recog-\nnition, 6479–6488.\nTian, Y .; Pang, G.; Chen, Y .; Singh, R.; Verjans, J. W.; and\nCarneiro, G. 2021. Weakly-supervised video anomaly de-\ntection with robust temporal feature magnitude learning. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, 4975–4986.\nTran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; and Paluri,\nM. 2015. Learning spatiotemporal features with 3d convo-\nlutional networks. In Proceedings of the IEEE international\nconference on computer vision, 4489–4497.\nWang, M.; Xing, J.; and Liu, Y . 2021. Actionclip: A\nnew paradigm for video action recognition. arXiv preprint\narXiv:2109.08472.\nWang, Z.; Yu, J.; Yu, A. W.; Dai, Z.; Tsvetkov, Y .; and Cao,\nY . 2021. Simvlm: Simple visual language model pretraining\nwith weak supervision. arXiv preprint arXiv:2108.10904.\nWu, J.; Zhang, W.; Li, G.; Wu, W.; Tan, X.; Li, Y .; Ding,\nE.; and Lin, L. 2021. Weakly-supervised spatio-temporal\nanomaly detection in surveillance video. arXiv preprint\narXiv:2108.03825.\nWu, P.; and Liu, J. 2021. Learning causal temporal rela-\ntion and feature discrimination for anomaly detection. IEEE\nTransactions on Image Processing, 30: 3513–3527.\nWu, P.; Liu, J.; He, X.; Peng, Y .; Wang, P.; and Zhang,\nY . 2023. Towards Video Anomaly Retrieval from Video\nAnomaly Detection: New Benchmarks and Model. arXiv\npreprint arXiv:2307.12545.\nWu, P.; Liu, J.; Shi, Y .; Sun, Y .; Shao, F.; Wu, Z.; and Yang,\nZ. 2020. Not only look, but also listen: Learning multimodal\nviolence detection under weak supervision. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XXX 16, 322–\n339. Springer.\nWu, P.; Liu, X.; and Liu, J. 2022. Weakly supervised audio-\nvisual violence detection. IEEE Transactions on Multime-\ndia, 1674–1685.\nYu, W.; Liu, Y .; Hua, W.; Jiang, D.; Ren, B.; and Bai, X.\n2023. Turning a CLIP Model into a Scene Text Detector.\narXiv preprint arXiv:2302.14338.\nZaheer, M. Z.; Mahmood, A.; Astrid, M.; and Lee, S.-I.\n2020. Claws: Clustering assisted weakly supervised learning\nwith normalcy suppression for anomalous event detection.\nIn Computer Vision–ECCV 2020: 16th European Confer-\nence, Glasgow, UK, August 23–28, 2020, Proceedings, Part\nXXII 16, 358–376. Springer.\nZhong, J.-X.; Li, N.; Kong, W.; Liu, S.; Li, T. H.; and Li,\nG. 2019. Graph convolutional label noise cleaner: Train a\nplug-and-play action classifier for anomaly detection. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, 1237–1246.\nZhou, H.; Yu, J.; and Yang, W. 2023. Dual Memory Units\nwith Uncertainty Regulation for Weakly Supervised Video\nAnomaly Detection. In Proceedings of the AAAI Conference\non Artificial Intelligence.\nZhou, K.; Yang, J.; Loy, C. C.; and Liu, Z. 2022a. Learning\nto prompt for vision-language models.International Journal\nof Computer Vision, 130(9): 2337–2348.\nZhou, X.; Girdhar, R.; Joulin, A.; Kr ¨ahenb¨uhl, P.; and\nMisra, I. 2022b. Detecting twenty-thousand classes using\nimage-level supervision. In Computer Vision–ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23–27,\n2022, Proceedings, Part IX, 350–368. Springer.\nZhou, Z.; Zhang, B.; Lei, Y .; Liu, L.; and Liu, Y . 2022c.\nZegCLIP: Towards Adapting CLIP for Zero-shot Semantic\nSegmentation. arXiv preprint arXiv:2212.03588.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6082"
}