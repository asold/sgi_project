{
    "title": "Unveiling the potential of diffusion model-based framework with transformer for hyperspectral image classification",
    "url": "https://openalex.org/W4394675339",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5092312556",
            "name": "Neetu Sigger",
            "affiliations": [
                "University of Buckingham"
            ]
        },
        {
            "id": "https://openalex.org/A5079131461",
            "name": "Quoc‐Tuan Vien",
            "affiliations": [
                "Middlesex University"
            ]
        },
        {
            "id": "https://openalex.org/A5103091856",
            "name": "Sinh Van Nguyen",
            "affiliations": [
                "Vietnam National University Ho Chi Minh City"
            ]
        },
        {
            "id": "https://openalex.org/A5052128677",
            "name": "Gianluca Tozzi",
            "affiliations": [
                "University of Greenwich"
            ]
        },
        {
            "id": "https://openalex.org/A5101956736",
            "name": "Tuan T. Nguyen",
            "affiliations": [
                "University of Greenwich"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2155281621",
        "https://openalex.org/W3075397214",
        "https://openalex.org/W4318465483",
        "https://openalex.org/W2962770389",
        "https://openalex.org/W2793848630",
        "https://openalex.org/W2991616716",
        "https://openalex.org/W3122028341",
        "https://openalex.org/W2098057602",
        "https://openalex.org/W2144841545",
        "https://openalex.org/W2001298023",
        "https://openalex.org/W2962860144",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W6605394581",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4283370461",
        "https://openalex.org/W2159284541",
        "https://openalex.org/W3043181422",
        "https://openalex.org/W3134136658",
        "https://openalex.org/W2888539709",
        "https://openalex.org/W4240485910",
        "https://openalex.org/W3214821343",
        "https://openalex.org/W4385308856",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2609880332",
        "https://openalex.org/W4319923785",
        "https://openalex.org/W4383890157",
        "https://openalex.org/W3083068801",
        "https://openalex.org/W4313229413",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W6603963165",
        "https://openalex.org/W4386275830",
        "https://openalex.org/W4387185378",
        "https://openalex.org/W4392411975",
        "https://openalex.org/W3099850646",
        "https://openalex.org/W2940678725",
        "https://openalex.org/W4386496074",
        "https://openalex.org/W3155072588",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3105255022",
        "https://openalex.org/W3098388691"
    ],
    "abstract": "Abstract Hyperspectral imaging has gained popularity for analysing remotely sensed images in various fields such as agriculture and medical. However, existing models face challenges in dealing with the complex relationships and characteristics of spectral–spatial data due to the multi-band nature and data redundancy of hyperspectral data. To address this limitation, we propose a novel approach called DiffSpectralNet, which combines diffusion and transformer techniques. The diffusion method is able extract diverse and meaningful spectral–spatial features, leading to improvement in HSI classification. Our approach involves training an unsupervised learning framework based on the diffusion model to extract high-level and low-level spectral–spatial features, followed by the extraction of intermediate hierarchical features from different timestamps for classification using a pre-trained denoising U-Net. Finally, we employ a supervised transformer-based classifier to perform the HSI classification. We conduct comprehensive experiments on three publicly available datasets to validate our approach. The results demonstrate that our framework significantly outperforms existing approaches, achieving state-of-the-art performance. The stability and reliability of our approach are demonstrated across various classes in all datasets.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports\nUnveiling the potential of diffusion \nmodel‑based framework \nwith transformer for hyperspectral \nimage classification\nNeetu Sigger 1, Quoc‑Tuan Vien 2, Sinh Van Nguyen 3, Gianluca Tozzi 4 & Tuan Thanh Nguyen 5*\nHyperspectral imaging has gained popularity for analysing remotely sensed images in various fields \nsuch as agriculture and medical. However, existing models face challenges in dealing with the complex \nrelationships and characteristics of spectral–spatial data due to the multi‑band nature and data \nredundancy of hyperspectral data. To address this limitation, we propose a novel approach called \nDiffSpectralNet, which combines diffusion and transformer techniques. The diffusion method is able \nextract diverse and meaningful spectral–spatial features, leading to improvement in HSI classification. \nOur approach involves training an unsupervised learning framework based on the diffusion model to \nextract high‑level and low‑level spectral–spatial features, followed by the extraction of intermediate \nhierarchical features from different timestamps for classification using a pre‑trained denoising U‑Net. \nFinally, we employ a supervised transformer‑based classifier to perform the HSI classification. We \nconduct comprehensive experiments on three publicly available datasets to validate our approach. \nThe results demonstrate that our framework significantly outperforms existing approaches, achieving \nstate‑of‑the‑art performance. The stability and reliability of our approach are demonstrated across \nvarious classes in all datasets.\nHyperspectral images (HSI) are now being captured more effectively by imaging spectrometers aboard satellites \nand aircraft. Unlike regular optical images with just three channels, Red, Green, Blue, each pixel of HSI contains \nabundant and continuous spectral information. This allows for the identification of complicated spectral char -\nacteristics of subjects that might be unnoticed. HSI is extensively used in various earth remote sensing applica-\ntions, including land use and land cover  classification1, precision  agriculture2,3, object  detection4, tree species \n classification5, brain cancer  detection6, and more.\nThe challenges of classification in HSI arise from their high dimensionality, strong correlations between adja-\ncent bands, a nonlinear data structure, and limited training  samples7. To address these challenges and improve \nclassification accuracy, researchers have proposed several  methods8. While traditional approaches like Maxi-\nmum Likelihood Classification have been foundational, they often face challenges with high-dimensional data \nspaces, known as the curse of  dimensionality9. Initially, spectral information for each pixel was fed into neural \nnetworks to identify the corresponding  class10. As data dimensionality increased, feature selection and dimen -\nsionality reduction became crucial. Techniques like principal component analysis (PCA) 11 and support vector \nmachine (SVM)12 were often employed to achieve better classification results. However, traditional techniques \nfaced difficulties in effectively utilising the spatial–spectral relationships and capturing complex information in \nHSI. By considering the neighbouring pixels along with their corresponding spectral values, we can gain valu-\nable insights into their underlying structures and extract meaningful information of different materials which \nultimately enhance accurate analysis.\nConvolutional neural networks (CNNs) 13,14 have better feature representation and high accuracy in clas-\nsification and have demonstrated promising performance in HSI classification. The CNNs can automatically \nextract hierarchical features from  HSI15. As datasets become larger, deeper architectures like residual networks \n(ResNets)16 were introduced, specifically adapted to capture complex patterns in HSI data for  classification17. \nOPEN\n1School of Computing, The University of Buckingham, Buckingham MK181EG, UK. 2Faculty of Science and \nTechnology, Middlesex University, London, UK. 3School of Computer Science and Engineering, International \nUniversity-Vietnam National University of HCMC, Ho Chi Minh City, Vietnam. 4School of Engineering, University \nof Greenwich, Chatham Maritime ME44TB, UK. 5School of Computing and Mathematical Sciences, University of \nGreenwich, London SE109LS, UK. *email: tuan.nguyen@greenwich.ac.uk\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports/\nAdvanced architectures such as autoencoders were later developed to extract a compressed representation of \nHSI data for classification  purposes18. Attention mechanisms were integrated into CNN architectures to enhance \nthe accuracy of classification by weighing the importance of different spectral  bands19. Furthermore, advance-\nments in the CNNs led to the introduction of novel pooling and unpooling mechanisms that better preserve \nspatial information during  classification20. In recent years, the CNNs have been shown to be effective in HSI \nclassification; however, there are still several limitations. For instance, the convolutional operations handle a \nlocal neighborhood. Hence, the number of layers and kernel size restrict the CNNs’ receptive field, making it less \neffective at capturing long-range dependencies in input  data21. As a result, learning the long-range dependencies \nof the HSI, often consisting of hundreds of spectral bands, is challenging.\nRecurrent neural network (RNNs)22 are capable of capturing the spatial–spectral relationship from long-range \nsequence data, they face challenges such as vanishing gradients and dependency on the order of spectral bands. \n Transformers23, originally designed for natural language processing (NLP), have shown promising results when \nintegrated into HSI classification. They effectively capture long-range dependencies in hyperspectral  data24,25. \nHere, CNN is a vector-based method that considers the inputs as collection of pixel  vectors26, and thus it can \nlead to information loss when processing with hyperspectral pixel  vectors27. In the  work28, a multispectral image \nclassification framework was introduced to overcome the limitations of the CNNs in pixel-wise remote sens-\ning classification and spectral sequence representation and, integrates fully connected (FC) layers, CNNs, and \ntransformers. Unlike the classic transformers that focus on band-wise representations,  SpectralFormer24 is an \nexample of such a framework that captures spectrally local sequence information, creates group-wise spectral \nembeddings, and introduces cross-layer skip connections to retain crucial information across layers through \nadaptive residual fusion. Another novel model, namely  SS1DSwin29, is based on transformers and implements \nthe network architecture of swin  transformer30. It was shown to effectively capture reliable spatial and spectral \ndependencies for HSI classification.\nEffectively learning rich representations and addressing the complexities of spectral–spatial relations in \nhigh-dimensional data are crucial for achieving optimal HSI classification results. However, transformer-based \nmethods face challenges in directly capturing reliable and informative spatial–spectral representations available \nin HSI. They generally do not fully leverage spatial  information31 and have limitations in extracting fine-grained \nlocal feature  patterns32. Recently, the denoising diffusion probabilistic model (DDPM)33 has emerged as a ground-\nbreaking class of generative models, adept at modeling complex relationships and effectively learning high-level \nand low-level visual features.  SpectralDiff34 leveraged a diffusion model to extract potent features. However, it \nemployed a pixel-wise classification approach, which limits the ability to effectively capture and identify distinct \nspatial–spectral relationships in HSI.\nTo overcome these challenges, we have thoroughly re-evaluated the process of extracting features of the HSI \ndata from different perspectives. Consequently, we have developed a novel HSI classification method that incor-\nporates diffusion and transformer techniques leveraging their respective advantages. The features’ representation \nlearned from the diffusion models have been demonstrated to be highly effective in various discriminating tasks \nwith impressive performance like semantic  segmentation35, object  detection36, and face  generation37.\nThis paper presents a novel classification framework called DiffSpectralNet, combining a diffusion-based \nspectral–spatial network with transformers. This diffusion model, a type of generative models, excels in captur-\ning the relationships between spectral and spatial information in HSI data. Deep features are extracted both \neffectively and efficiently to make the most of the spectral–spatial information present in the data. The main \nstages of the framework are summarized as follows: first, we ultilise forward and reverse diffusion processes to \nlearn high-level and low-level features from HSI. Second, to make effective use of the extensive timestamps-\nwise features, we extract intermediate hierarchical features from the denoising U-Net at different timestamps. \nSubsequently, we employ a proposed supervised transformer-based classifier for performing HSI classification.\nWe examine the effectiveness of the proposed method conducted on three widely known datasets that their \ndownload link can be found in the Data availability section. Our results clearly demonstrate that the proposed \nmethod significantly improves classification results and outperforms other advanced HSI classification meth-\nods. Moreover, this study also opens the way for further investigations into the potential of diffusion models in \nlearning high- and low-level spectral–spatial features with significant flexibility in HSI. Ongoing research will \nlikely enhance the application of diffusion models in processing complex, high-dimensional hyperspectral data, \nopening up promising prospects for diverse applications.\nResults\nIn this section, we begin by providing an introduction to three different experimental datasets for HSI. After \nthat, we delve into the details of the experimental results that have been produced by our proposed model. In \naddition, we conduct a thorough analysis of parameters of the framework to gain a better understanding of their \nsignificance and implications.\nDataset\nThree well-known available datasets, Indian Pines, Pavia University and Salinas Scene, were used to examine the \nclassification performance. Number of categories and their correspondent samples were shown in Table 1. First, \nthe Indian Pines dataset collected in 1992 using the Airborne Visible Infrared Imaging Spectrometer (AVIRIS) \nSensor, covering the northwestern region of Indiana in the United States. It consists of 145 × 145 pixels with each \npixel having a spatial resolution of 20 metres (m) and 220 spectral bands in the wavelength range of 400–2500 \nnm. The dataset contains labeled pixels with 16 categories. We use 10% of the labeled samples for training and the \nrest for testing. The second HSI dataset, Pavia University, was acquired by the Reflective Optics System Imaging \nSpectrometer (ROSIS) sensor. The ROSIS sensor acquired 103 bands covering the spectral range from 430 to 860 \n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports/\nnm, and the dataset consists of 610 × 340 pixels at GSD of 1.3 m. Moreover, there are 9 land cover classes in the \ndataset. We use 5% of the labeled samples for training and the rest for testing. Lastly, Salinas Scene dataset was \ncollected using the AVIRIS sensor and is situated in Salinas Valley, California. The spatial resolution is set at 3.7 \nm. and the dataset includes 16 crop types and has been widely utilized in classification. After the exclusion of 20 \nbands associated with water vapor and noise, a total of 204 bands remained, resulting in a data size of 512 × 217 . \nWe use 5% of the labeled samples for training and the rest for testing.\nTraining process\nWe used the PyTorch framework to implement and train the DiffSpectralNet model. The training was done on \na basic hardware setup, which consists of a POWER8NVL production-grade CPU with 128 CPU threads spread \nacross 2 sockets for efficient processing. Additionally, four NVIDIA Tesla P100 GPUs were used for enhanced \ngraphical computations, each offering a memory of approximately 16 GB.\nThe diffusion model was optimised using the Adam optimizer and trained for 30, 000 epochs for all datasets. \nWe set the learning rate to 1 × 10−4 , with a batch size of 128 and a patch size of 32 × 32 . Due to hardware \nlimitations, we use batch size 64 for the Salinas scene dataset. To determine the amount of spectral information \npreserved in the compressed data, we employed PCA. Given that each dataset presents a distinct number of \nfeatures post-pre-training with the diffusion model, the range of PCA components varies among three datasets. \nThe classification model was trained using the Adam optimizer, maintaining the same learning rate of 1 × 10−4 \nand a batch size of 128 for Indian Pines, Pavia University, and 64 for Salinas Scene. The size of feature patch is \nempirically set as 7 × 7. The number of epochs was set to 300 for Indian Pines and 600 for Pavia University and \nSalinas Scene datasets.\nPerformance evaluation\nWe evaluate the performance using three prominent metrics: overall accuracy (OA), average accuracy (AA), and \nKappa coefficient ( κ ). OA gives a direct insight into general model performance, and AA ensures each class has a \nbalanced contribution, especially in imbalanced datasets. On the other hand, κ measures the reliability between \nthe ground truth and model predictions.\nTo demonstrate the effectiveness of our proposed DiffSpectralNet, we compare its classification performance \nwith various state-of-the-art approaches, and the following methods were chosen:  DMVL38, similar to our \nproposed model, follows the two-stage algorithms. It performs unsupervised feature extraction followed by \nclassification using an SVM classifier.  3DCAE39 is an unsupervised method to learn spectral–spatial features. \nIt uses the encoder–decoder backbone with 3D convolution operations, GSSCRC 40 algorithm incorporates \nthe cooperative representation classification model and introduces the geodesic distance calculation method \nto select spectral nearest-neighbour information, thereby effectively utilising the neighbour information in \nHSI. This approach facilitates the exploration and utilization of the spatial–spectral neighbourhood structure \nof HSI data for classification.  SS1DSwin29 design reveals local and hierarchical spatial–spectral links through \ntwo modules: the Groupwise Feature Tokenization Module (GFTM) and the 1DSwin Transformer with Cross-\nBlock Normalized Connection Module (TCNCM). GFTM processes overlapping cubes and uses multihead self-\nattention for spatial–spectral relationships. Meanwhile, TCNCM utilises window-based strategies for spectral \nrelationships and cross-block feature fusion.  SpectralFormer24 uses transformers from a sequential perspective for \nclassification, learns spectrally local sequence information from neighbouring bands of HSI, yielding group-wise \nTable 1.  Details of Indian Pines, Pavia University, and Salinas Scene Datasets.\nIndian Pines Dataset Pavia University Dataset Salinas Scene Dataset\nLand cover type Samples Land cover type Samples Land cover type Samples\nAlfalfa 46 Asphalt 6631 Brocoli_green_weeds_1 2009\nCorn-notill 1428 Meadows 18649 Brocoli_green_weeds_2 3726\nCorn-min 830 Gravel 2099 Fallow 1976\nCorn 237 Trees 3064 Fallow_rough_plow 1394\nGrass/pasture 483 Painted metal sheets 1345 Fallow_smooth 2678\nGrass/trees 730 Bare Soil 5029 Stubble 3959\nGrass/pasture-mowed 28 Bitumen 1330 Celery 3579\nHay-windrowed 478 Self-Blocking Bricks 3682 Grapes_untrained 11,271\nOats 20 Shadows 947 Soil_vinyard_develop 6203\nSoybeans-notill 972 Corn_senesced_green_weeds 3278\nSoybeans-min 2455 Lettuce_romaine_4wk 1068\nSoybeans-clean 693 Lettuce_romaine_5wk 1927\nWheat 205 Lettuce_romaine_6wk 916\nWoods 1265 Lettuce_romaine_7wk 1070\nBldg-grass-tree-drives 386 Vinyard_untrained 7268\nStone-steel towers 93 Vinyard_vertical_trellis 1807\nTotal 10,349 Total 42,776 Total 54,129\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports/\nspectral embeddings. Also, to reduce the possibility of losing valuable information in the layer-wise propagation \nprocess, a cross-layer skip connection is devised from shallow to deep layers by adaptively learning across layers. \nWe conducted experiments on the Salinas dataset, not covered in the  SpectralFormer24, using the same train-\ntest split ratio employed in our experiments for consistent comparison.  SpectralDiff34 employs an unsupervised \nfeature extraction using a spectral–spatial diffusion module. These features are then processed per pixel by the \nsupervised attention-based classification module.\nIt is worth mentioning that we directly used the outcomes reported in the papers of each of these methods. \nBoth CNN-based and transformer-based methods produced good classification results.\nBased on the analysis of classification results obtained for the Indian Pines, Pavia University, and Salinas Scene \ndatasets presented in Table 2, the DiffSpectralNet algorithm proposed in this study shows improved classifica-\ntion accuracy for most ground objects when compared to other classification methods. The proposed method \nachieves the best OA, AA, and κ values, with OA reaching 99.06%, 99.74 %, and 99.87%  on the Indian Pines, Pavia \nUniversity and Salinas Scene datasets, respectively. Visualisation in the Fig. 1 clearly shows DiffSpectralNet out-\nperformed others. Moreover, we conducted additional statistical analyses using Analysis of Variance (ANOV A) \nand Mann-Whitney U Test, both with a confidence score of 95% . The p value from these tests were lower than \n0.05, indicating that DiffSpectralNet’s performance is significantly different across three measurement metrics. \nAll of these results proves that the DiffSpectralNet algorithm efficiently and effectively learns low and high-level \nfeatures using the diffusion model. Additionally, the DiffSpectralNet algorithm leverages the combination of \nspectral and spatial information, enabling it to extract a greater amount of information for classification. There-\nfore, the DiffSpectralNet algorithm proposed in this study demonstrates promising potential for improving the \naccurate classification of ground objects.\nIn addition to the above quantitative metrics, classification maps in the proposed method have been \nproduced, as shown in Figs.  2, 3 and 4 . Compared with ground truth, the proposed method obtains more \naccurate classification results, which further proves the effectiveness of the proposed method in the classification \nof hyperspectral data.\nFigure 2 illustrates the classification results obtained using the DiffSpectralNet and the comparison algorithms \non the Indian Pines dataset. The map highlights that the algorithm proposed in this study exhibits classification \nperformance that closely resembles the actual terrain map of the Indian Pines dataset. The misclassification of \nterrain pixels is observed to be relatively minimal, resulting in a smoother overall effect. Notably, the algorithm \ndemonstrates superior performance in classifying Grass-pasture-mowed, Oats, Wheat, and Woods features.\nTable 2.  Classification results of different HSIs, and the best result is bolded.\nModel\nIndian Pines Pavia University Salinas Scene\nOA (%) AA (%) κ OA (%) AA (%) κ OA (%) AA (%) κ\nDMVL + SVM 78.01 84.98 0.7531 86.96 80.10 0.8246 94.60 94.59 0.9400\n3DCAE 92.35 92.04 – 95.39 95.36 - 95.81 97.45 –\nGSSCRC 91.33 93.81 0.9013 95.77 94.13 0.9438 95.62 97.30 0.9384\nSS1DSwin 89.66 94.13 0.8819 93.04 91.92 0.9068 95.45 97.78 0.9493\nSpectralFormer 81.76 87.81 0.7919 91.07 90.20 0.8805 96.27 97.82 0.9585\nSpectralDiff 93.15 96.43 0.9217 94.77 93.84 0.9306 98.97 99.46 0.9885\nOurs 99.06 98.00 0.9893 99.74 99.18 0.9965 99.87 99.82 0.9986\nFigure 1.  The boxplots to visualise the performance of each model using three prominent metrics. Note that, \nthere is no published result of 3DCAE on these datasets, therefore it was not included in the visualisation.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports/\nMoving forward, Fig.  3 provides a visual representation of the classification performance of the proposed \nmodel on the Pavia University dataset. The algorithm exhibited fewer misclassifications in the dataset, resulting \nin a smoother overall effect. Notably, in the classification of the Meadows, Metal sheets, and Bare soil features, \nthe performance of the proposed algorithm is superior. This observation highlights the capability of the DiffSpec-\ntralNet to extract spectral and spatial information more comprehensively with the usage of the diffusion model.\nFigure 4 presents the classification effect maps of the proposed model and the comparison algorithms on the \nSalinas Scene dataset. By observing the classification effect map of the model, it can be concluded that in the \nBrocoli_green_weeds_1, Brocoli_green_weeds_2, Fallow, Soil_vinyard_develop, Lettuce_romaine_4wk, Lettuce_\nromaine_5wk, Lettuce_romaine_6wk and Vinyard_vertical_trellis regions, there are fewer misclassified pixels \nof ground features compared with the comparison algorithms, resulting in a smoother overall effect map. This \nFigure 2.  Classification results of on the Indian Pines dataset (a) Original HSI (b) ground truth (c) proposed \nmethod.\nFigure 3.  Classification results of on the Pavia University dataset (a) original HSI (b) ground truth (c) proposed \nmethod.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports/\ndemonstrates that the DiffSpectralNet proposed in this paper can effectively reveal the intrinsic features hidden \nbehind a HSI by learning low and high-level features.\nFor a comprehensive examination of the detailed performance metrics of each class for all three datasets, \nreaders are directed to the Supplementary materials provided. In supplementary sections, we thoroughly \ncompare our classification performance across various classes against a range of state-of-the-art methodologies \nto demonstrate the stability and reliability of our approach.\nDiscussion\nIn this section, we explore further experiments and discussions on the following three aspects to explore the \noptimal classification performance and the application of the proposed model in practical remote sensing \nclassification. First, we conduct experiments to discuss how to extract features from the pre-trained diffusion \nmodel to achieve optimal performance at various Timestamp and Feature index values. Second, we analyse the \nimpact of the number of training samples directly affecting the network’s performance. Finally, we examine the \ninfluence of the quantity of PCA components on the spectral information in HSI datasets.\n• Sensitivity analysis of Timestamps and Feature index: In order to analyse the features obtained from the dif-\nfusion pre-trained model, we have conducted classification experiments on various Timestamp and Feature \nindex values and then recorded the change in the classification performance. Using the DDPM, we monitored \nclassification efficacy alterations when Timestamp and Feature Index varied, and the optimal combination of \nTimestamp and Feature Index is essential to ensure accurate outcomes. Table 3 showcases the performance is \nsensative to Timestep and FeatureIndex. For the Indian Pines and Pavia University datasets, there is a certain \ncorrelation between Timestamp and FeatureIndex. When considering the Timestamp dimension, a decreasing \ntrend in classification performance is observed when using features with larger Timestamps, and the optimal \nFigure 4.  Classification results of on the Salinas Scene dataset (a) original HSI (b) ground truth (c) proposed \nmethod.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports/\nperformance generally occurs in smaller Timestamp groups. Considering the FeatureIndex dimension, both \ndatasets (Indian Pines and Pavia University) performed better at FeatureIndex 1 than at FeatureIndex 0 and \n2. For Salinas Scene, there are some fluctuations in classification performance for different Timestamp and \nFeatureIndex values but no significant changes.\n• Percentage of training samples: It is common knowledge that the number of training samples directly \naffects the performance of the network. To verify this with the proposed DiffSpectralNet, We evaluated the \ntraining dataset using random proportions ranging from 10 to 100% with increments of 10% , and depict the \ncomparative results in Fig. 5a. As expected, the classification accuracy gradually improves with an increase \nin training samples. It is worth noting that OA tends to be stable when the percentage of training samples \nis greater than 50% . However, when the percentage of training samples in the Indian Pines dataset is less \nthan 50% , the performance is unsatisfactory may be due to the insufficient number of samples for a proper \ntraining. Therefore, it is reasonable to extrapolate that DiffSpectralNet is reliable and stable for this task.\n• Effect of PCA components on diffusion feature: We investigate the impact of the number of PCA components \non the compressed spectral data. The data retain more spectral details with more PCA components but at \nthe cost of increased computational demand and redundancy. The number of diffusion features varies across \ndatasets, influencing the range of PCA components, which varies from D/6 to D/15, where D represents the \ndiffusion features in a dataset. The results in Fig. 5b suggest optimal performance with D/8 PCA components.\nMethods\nIn this section, we describe a novel method called DiffSpectralNet that consists of two stages: an unsuper -\nvised diffusion process and a supervised classification. The unsupervised diffusion process is derived from the \nDDPM with the purpose to learn spectral–spatial representations effectively. In this process, we extract plenty \nof spectral–spatial features from various time steps t  during the reverse diffusion process of DDPM to capture \nthe characteristics of different objects in HSI data. Finally, these features are inputted into the supervised clas -\nsification model for classification.\nTable 3.  The performance of different layer indices and timestamps in the Indian Pines, Pavia University, and \nSalinas Scene. Significant values are in bold.\nFeatureIndex Timestamp\nIndian Pines Pavia University Salinas Scene\nOA (%) AA (%) κ OA (%) AA (%) κ OA (%) AA (%) κ\n0\n5 98.47 95.37 0.9826 98.94 97.93 0.9860 99.74 99.73 0.9971\n10 98.41 96.40 0.9818 99.15 98.68 0.9887 99.87 99.82 0.9985\n100 97.92 96.85 0.9762 99.03 98.27 0.9871 99.71 99.67 0.9967\n200 97.62 94.45 0.9728 98.63 97.91 0.9818 98.63 97.91 0.9818\n400 98.15 96.38 0.9789 92.86 89.98 0.9053 98.29 97.74 0.9809\n1\n5 99.06 98.00 0.9893 99.74 99.16 0.9965 99.83 99.76 0.9981\n10 98.34 96.20 0.9811 99.63 99.09 0.9951 99.76 99.73 0.9973\n100 98.40 96.30 0.9817 99.54 99.18 0.9939 99.87 99.81 0.9986\n200 98.45 97.48 0.9823 98.79 97.53 0.9839 98.45 97.48 0.9823\n400 98.29 96.35 0.9805 92.61 88.75 0.9015 98.06 97.70 0.9784\n2\n5 98.59 95.17 0.9839 98.52 97.07 0.9803 99.26 99.32 0.9917\n10 98.82 94.99 0.9865 97.32 95.29 0.9644 98.95 99.00 0.9883\n100 98.01 96.05 0.9773 95.19 91.13 0.9361 98.04 97.98 0.9782\n200 96.37 93.26 0.9587 93.54 90.25 0.9139 96.37 93.26 0.9587\n400 95.71 92.52 0.9510 86.66 81.28 0.8202 91.84 88.39 0.9089\nFigure 5.  Classification accuracy (OA) achieved by the proposed DiffSpectralNet with (a) varying percentages \nof training samples (b) different PCA components on three benchmark datasets.\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports/\nDiffusion‑based unsupervised spectral–spatial feature learning\nIn order to capture complex spectral–spatial relations and label-agnostic information of HSI data effectively, \nthe first step of our proposed approach is to train a diffusion model in an unsupervised manner, as shown in \nFig. 6a. We introduce the detailed formulation of our unsupervised feature learning procedure, which involves \ndiffusion-based forward and backward processes with the HSI data.\n• Forward diffusion process: DDPM represents a category of models based on likelihood estimations. In \nthe forward process, Gaussian noise is added to the original training data. In our proposed model, we aim \nto learn spectral–spatial features effectively in an unsupervised manner. We start by training our DDPM \nusing unlabeled patches randomly cropped from the HSI dataset. To prepare the data for training, the data \nis pre-processed by patch cropping operation. Next, patches are randomly sampled from HSI for DDPM \ntraining. Formally, given an unlabeled patch x0 ∈ RP×P ×B , where P denote the height and width of patch x0 , \nB represents the number of spectral channels, respectively. During the forward diffusion process, Gaussian \nnoise is gradually added to the HSI patch according to the variance schedule {βt}T\nt=0  in the diffusion process \nwhere T is the total number of the timestep. The process follows the Markov  chain33 process: \nFigure 6.  Overview of our proposed DiffSpectralNet (a) unsupervised spectral–spatial feature learning \nnetwork. x0 and xT represent HSI patches of timestep 0 and timestep T . q(xt |xt−1 ) and p(xt−1 |xt) represent \nforward and reverse spectral–spatial diffusion processes, respectively. (b) Supervised classification (1) extracting \nhierarchical features from the pretrained denoising U-Net decoder in terms of different timestep t. (2) Using the \npatch-wise feature vectors to train an cross-layer transformer for HSI classification.\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports/\n where N is a Gaussian distribution. The above formulation leads to the probability distribution of the HSI \nat a given time t + 1 is obtained by its state at time t. During the first diffusion, the spectral–spatial instance \nwith noise is expressed as follows: \n At the tth step, the spectral–spatial instance incorporated with noise is expressed as follows: \n where αt = 1 − βt and, αt represents the product of α1 to αt . Given these inputs, the hyperspectral instance \nat timestep t can be straightforwardly produced by Eq. (3).\n• Reverse diffusion process: In the reverse diffusion process, a spectral–spatial U -Net41 denoising network is \nemployed is trained to predict the noise added on xt−1 , taking noisy patch xt and timestep t as inputs. And \nxt−1 is calculated by subtracting the predicted noise from xt . DDPM uses a Markov chain process to remove \nthe noisy sample xT to x0 step by step. Under large T  and small βt , the probability of reverse transitions is \napproximated as a Gaussian distribution and is predicted by a U-Net as follows: \n where the reverse process can be re-parameterized by estimating µθ (xt, t) and σθ (xt, t) . σθ (xt, t) is set to σ2\nt I , \nwhere σ2\nt  is not learned. To obtain the mean of the conditional distribution pθ (xt−1 |xt) , we need to train the \nnetwork to predict the added noise. The mean of µθ (xt, t) is derived as follows: \n where εθ (·, ·) denote the spectral–spatial denoising network whose input is the timestep t  and the noisy \nhyperspectral instance xt at timestep t . The denoising network takes in the noisy hyperspectral instance \nalong with the timestep to produce the predicted noise. The U-Net denoising model εθ (xt, t) is optimised by \nminimising the loss function of the spectral–spatial diffusion process can be expressed as follows: \nSupervised classification using spectral–spatial diffusion feature\nAfter training the network using unsupervised spectral–spatial methods, we start extracting useful diffusion \nfeatures from the pre-trained DDPM. Next, we employ a transformer-based classifier for classification.\nDuring the feature extraction step, we utilize the U -Net denoising network to extract a spectral–spatial \ntimestep-wise feature. The pre-training of DDPM enables it to capture rich and divers information from the \ninput data during the reverse process. As a result, we extract features from the intermediate hierarchies of DDPM \nat various timesteps to create robust representations that encapsulate the salient features of the input HSI. The \nparameters of the pre-trained DDPM remain constant, as shown in Fig.  6b. We gradually add Gaussian noise \nto the input patch x0 ∈ RP×P  through the diffusion process. For a noisy input patch xt at timestep t  , the noisy \nversion xt can be directly determined using Eq. (3). Subsequently, xt is fed into the pre-trained spectral–spatial \ndenoising U-Net to derive hierarchical features from the U-Net decoder. Diffusion features from various decoder \nlayers are collectively upsampled to P × P and then merged to form the feature ft in RP×P×L at timestep t  , where \nP represents the height and width of the patch and L denotes the feature channel. For each feature fti ∈ RP ×P ×L  , \nwe retain only the vector associated with the center pixel, indexed as C i ∈ Rp×p×L . This approach significantly \nreduces the computational cost due to a decrease in parameters. We input the extracted diffusion features (C (fti) \npatch-wise to learn group-wise spectral embeddings. By proposing to learn group-wise spectral embeddings, we \naim to precisely identify and classify the diverse features based on their distinct spectral properties. The group-\nwise spectral embedding features use a linear projection layer for mapping features to a token sequence for the \ntransformer. Positional embedding is added to the input token sequence before feeding it to the transformer. This \nprovides the transformer with information about the relative positions of the patches. Therefore, the abundant \nfeatures contain diverse and multi-level information of the input HSI data, which we use for classification.\nAfter mapping the patch representation, a network is needed to predict the classification label. Transformer-\nbased classifiers are trained based on the inspiration  from24, as shown in Fig.  6b. The classification module \ncombines the CNN and transformer structures to form an effective classifier. These classifiers take positionally \nembedded feature patches as inputs and use an MLP head to predict the final classification scores. Inspired by \nthe success of skip connection in U-Net 42, and  ResNet16 for image segmentation and recognition, respectively. \nA cross-layer skip connection is introduced in the classifier to minimise the possibility of losing valuable \ninformation in the layer-wise propagation process and enhance the information transitivity between layers. The \nclassifier model utilises skip connection, multi-head attention mechanisms, feed-forward neural networks to \nspectral–spatial feature mapping, and a transformer structure for deep feature extraction, resulting in outstanding \nclassification performance.\n(1)q(xt|xt− 1 ) = N\n(√\n(1 − βt)xt− 1 ,βtI\n)\n(2)x1 = √α1x0 +\n√\n1 − α1ε\n(3)x t = √α tx 0 +\n√\n1 − α tε, ε ∼ N(0,I)\n(4)pθ (xt−1 |xt) = N(xt−1 ;µθ (xt,t),σθ (xt,t))\n(5)µ θ (xt,t) = 1√αt\n(\nxt − 1 −αt√1 −αt\nεθ (xt,t)\n)\n(6)L(θ) = Et,x0 ,ε\n[(\nε − εθ\n(√α tx0 +\n√\n1 − α tε,t\n))2 ]\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports/\nConclusion\nHSI contains rich spectral–spatial information and complex relations, which are critical for classification tasks. \nThe proposed method provides a unique viewpoint for the spectral–spatial diffusion process, which is capable \nof modeling complex relationships for understanding inputs and learning both high-level and low-level features. \nIn conclusion, most current methods for HSI classification rely on CNN or Transformer models, which may not \nefficiently extract patterns and information. In contrast, our proposed method, employing the diffusion model, \neffectively and efficiently learns discriminative spectral–spatial features. This approach allows us to explore and \nutilise the spatial–spectral neighborhood structure of hyperspectral data, resulting in the effective extraction of \ndeep features. Instead of processing on a pixel-by-pixel basis, the diffusion features are introduced in patches to \nimprove the ability to capture details for more accurate classification. We employed a transformer-based model \nwith a cross-layer skip connection, which reduces the possibility of losing valuable information in the layer-wise \npropagation process. We demonstrated the superiority of our proposed DiffSpectralNet approach by achieving \nstate-of-the-art results in HSI classification based on quantitative trials conducted on three HSI datasets. In future \nstudies, we aim to validate and enhance the performance of our proposed model on additional hyperspectral \ndatasets across various domains, such as the medical field. Our model can be generalised and shows promise in \nHSI classification due to its ability to capture complex relationships between bands.\nData availability\nThe datasets analysed during the current study are available in the Grupo de Inteligencia Computacional (GIC) \nHyper spect ral Remot e Sensi ng Scenes. Supplementary information is available on the online version of the paper \nwhich shows the detailed information of these three datasets.\nReceived: 22 December 2023; Accepted: 26 March 2024\nReferences\n 1. Shankar, V . D. G. & Shankar, T. Hyperspectral data for land use/land cover classification. Int. Arch. Photogram. Remote Sens. Spat. \nInf. Sci. 8, 991–995. https:// doi. org/ 10. 5194/ isprs archi ves- XL-8- 991- 2014 (2014).\n 2. Lu, B., Dao, P . D., Liu, J., He, Y . & Shang, J. Recent advances of hyperspectral imaging technology and applications in agriculture. \nRemote Sens.https:// doi. org/ 10. 3390/ rs121 62659 (2020).\n 3. Tang, Y . et al. Active and low-cost hyperspectral imaging for the spectral analysis of a low-light environment. Sensorshttps:// doi. \norg/ 10. 3390/ s2303 1437 (2023).\n 4. Audebert, N., Le Saux, B. & Lefevre, S. Deep learning for classification of hyperspectral data: A comparative review. IEEE Geosci. \nRemote Sens. Mag. 7, 159–173. https:// doi. org/ 10. 1109/ MGRS. 2019. 29125 63 (2019).\n 5. Bandyopadhyay, D. et al. Tree species classification from hyperspectral data using graph-regularized neural networks. arXiv: 2208. \n08675 (2023).\n 6. Fabelo, H. et al. Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations. PLoS \nOne 13, 1–27. https:// doi. org/ 10. 1371/ journ al. pone. 01937 21 (2018).\n 7. Paoletti, M. E., Haut, J. M., Plaza, J. & Plaza, A. J. Deep learning classifiers for hyperspectral imaging: A review. ISPRS J. Photo-\ngramm. Remote. Sens. 158, 279–317 (2019).\n 8. Ahmad, M. et al. Hyperspectral image classification-traditional to deep models: A survey for future prospects. IEEE J. Sel. Top. \nApp. Earth Observ. Remote Sens. 15, 968–999. https:// doi. org/ 10. 1109/ jstars. 2021. 31330 21 (2022).\n 9. Hughes, G. P . On the mean accuracy of statistical pattern recognizers. IEEE Trans. Inf. Theory 14, 55–63 (1968).\n 10. Benediktsson, J., Swain, P . & Ersoy, O. Neural network approaches versus statistical methods in classification of multisource remote \nsensing data. IEEE Trans. Geosci. Remote Sens. 28, 540–552. https:// doi. org/ 10. 1109/ TGRS. 1990. 572944 (1990).\n 11. Rodarmel, C. & Shan, J. Principal component analysis for hyperspectral image classification. Surv. Land Inf. Sci. 62, 115–122 (2002).\n 12. Fauvel, M., Tarabalka, Y ., Benediktsson, J. A., Chanussot, J. & Tilton, J. C. Advances in spectral-spatial classification of hyperspectral \nimages. Proc. IEEE 101, 652–675. https:// doi. org/ 10. 1109/ JPROC. 2012. 21975 89 (2013).\n 13. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv: 1409. 1556 (2015).\n 14. Lecun, Y ., Bengio, Y . & Hinton, G. Deep learning. Nature 521, 436–444. https:// doi. org/ 10. 1038/ natur e14539 (2015).\n 15. Zeng, H., Liu, Q., Zhang, M., Han, X. & Wang, Y . Semi-supervised hyperspectral image classification with graph clustering con-\nvolutional networks. arXiv: 2012. 10932 (2020).\n 16. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR), 770–778. https:// doi. org/ 10. 1109/ CVPR. 2016. 90 (2016).\n 17. Song, L., Feng, Z., Y ang, S., Zhang, X. & Jiao, L. Self-supervised assisted semi-supervised residual network for hyperspectral image \nclassification. Remote Sens.https:// doi. org/ 10. 3390/ rs141 32997 (2022).\n 18. Lin, Z., Chen, Y ., Zhao, X. & Wang, G. Spectral-spatial classification of hyperspectral image using autoencoders. In 9th International \nConference on Information, Communications Signal Processing. https:// doi. org/ 10. 1109/ ICICS. 2013. 67827 78 (2013).\n 19. Hang, R., Li, Z., Liu, Q., Ghamisi, P . & Bhattacharyya, S. S. Hyperspectral image classification with attention aided cnns. arXiv:  \n2005. 11977 (2020).\n 20. Xie, F ., Gao, Q., Jin, C. & Zhao, F . Hyperspectral image classification based on superpixel pooling convolutional neural network \nwith transfer learning. Remote Sens.https:// doi. org/ 10. 3390/ rs130 50930 (2021).\n 21. Tang, G., Müller, M., Rios, A. & Sennrich, R. Why self-attention? A targeted evaluation of neural machine translation architectures \n(2018). arXiv: 1808. 08946.\n 22. Mou, L., Ghamisi, P . & Zhu, X. Deep recurrent neural networks for hyperspectral image classification. IEEE Trans. Geosci. Remote \nSens. 55, 3639–3655 (2017).\n 23. Vaswani, A. et al. Attention is all you need. arXiv: 1706. 03762 (2023).\n 24. Hong, D. et al. SpectralFormer: Rethinking hyperspectral image classification with transformers. IEEE Trans. Geosci. Remote Sens. \n60, 1–15. https:// doi. org/ 10. 1109/ tgrs. 2021. 31307 16 (2022).\n 25. Liu, B., Liu, Y ., Zhang, W ., Tian, Y . & Kong, W . Spectral swin transformer network for hyperspectral image classification. Remote \nSens.https:// doi. org/ 10. 3390/ rs151 53721 (2023).\n 26. Linzen, T., Dupoux, E. & Goldberg, Y . Assessing the ability of lstms to learn syntax-sensitive dependencies. arXiv: 1611. 01368  \n(2016).\n 27. Hang, R., Liu, Q., Hong, D. & Ghamisi, P . Cascaded recurrent neural networks for hyperspectral image classification. IEEE Trans. \nGeosci. Remote Sens. 57, 5384–5394. https:// doi. org/ 10. 1109/ TGRS. 2019. 28991 29 (2019).\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:8438  | https://doi.org/10.1038/s41598-024-58125-4\nwww.nature.com/scientificreports/\n 28. Y an, C. et al. Hyformer: Hybrid transformer and cnn for pixel-level multispectral image land cover classification. Int. J. Environ. \nRes. Public Healthhttps:// doi. org/ 10. 3390/ ijerp h2004 3059 (2023).\n 29. Xu, Y . et al. Spatial-spectral 1dswin transformer with groupwise feature tokenization for hyperspectral image classification. IEEE \nTrans. Geosci. Remote Sens. 61, 1–16. https:// doi. org/ 10. 1109/ TGRS. 2023. 32944 24 (2023).\n 30. Liu, S., Shi, Q. & Zhang, L. Few-shot hyperspectral image classification with unknown classes using multitask deep learning. IEEE \nTrans. Geosci. Remote Sens. 59, 5085–5102. https:// doi. org/ 10. 1109/ tgrs. 2020. 30188 79 (2021).\n 31. Sun, L., Zhao, G., Zheng, Y . & Wu, Z. Spectral-spatial feature tokenization transformer for hyperspectral image classification. IEEE \nTrans. Geosci. Remote Sens. 60, 1–14 (2022).\n 32. Gulati, A. et al. Conformer: Convolution-augmented transformer for speech recognition (2020). arXiv:  2005. 08100.\n 33. Ho, J., Jain, A. & Abbeel, P . Denoising diffusion probabilistic models. CoRR (2020). arXiv: 2006. 11239.\n 34. Chen, N., Yue, J., Fang, L. & Xia, S. Spectraldiff: A generative framework for hyperspectral image classification with diffusion \nmodels. IEEE Trans. Geosci. Remote Sens. 61, 1–16. https:// doi. org/ 10. 1109/ tgrs. 2023. 33100 23 (2023).\n 35. Baranchuk, D., Rubachev, I., Voynov, A., Khrulkov, V . & Babenko, A. Label-efficient semantic segmentation with diffusion models. \narXiv: 2112. 03126 (2022).\n 36. Chen, Z., Gao, R., Xiang, T.-Z. & Lin, F . Diffusion model for camouflaged object detection. arXiv: 2308. 00303 (2023).\n 37. Perera, M. V . & Patel, V . M. Analyzing bias in diffusion-based face generation models. arXiv: 2305. 06402 (2023).\n 38. Liu, B. et al. Deep multiview learning for hyperspectral image classification. IEEE Trans. Geosci. Remote Sens. 59, 7758–7772. \nhttps:// doi. org/ 10. 1109/ TGRS. 2020. 30341 33 (2021).\n 39. Mei, S. et al. Unsupervised spatial-spectral feature learning by 3d convolutional autoencoder for hyperspectral classification. IEEE \nTrans. Geosci. Remote Sens. 57, 6808–6820. https:// doi. org/ 10. 1109/ TGRS. 2019. 29087 56 (2019).\n 40. Zheng, G. et al. Hyperspectral image classification using geodesic spatial. Electronicshttps:// doi. org/ 10. 3390/ elect ronic s1218 3777 \n(2023).\n 41. Saharia, C. et al. Image super-resolution via iterative refinement. arXiv: 2104. 07636 (2021).\n 42. Ronneberger, O., Fischer, P . & Brox, T. U-net: Convolutional networks for biomedical image segmentation (2015). arXiv: 1505.  \n04597.\nAuthor contributions\nT.N. initialised concepts and directions. N.S. and T.N. conceived experiments. N.S. conducted experiments and \nanalysed results. T.N. and G.T. provided critical updates and suggestions that significantly enhanced the scope \nand direction of the research. N.S. and T.N. wrote the paper with important input from G.T. N.S. authored the \nSupplementary Material and conducted the supplementary experiments. All authors, N.S., T.N., G.T., Q.T., S.N., \nreviewed and approved the final manuscript.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 024- 58125-4.\nCorrespondence and requests for materials should be addressed to T.T.N.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© Crown 2024"
}