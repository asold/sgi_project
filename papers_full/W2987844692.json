{
    "title": "Transformer-based Model for Single Documents Neural Summarization",
    "url": "https://openalex.org/W2987844692",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2988933584",
            "name": "Elozino Egonmwan",
            "affiliations": [
                "University of Lethbridge"
            ]
        },
        {
            "id": "https://openalex.org/A108026667",
            "name": "Yllias Chali",
            "affiliations": [
                "University of Lethbridge"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2012561700",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W2507756961",
        "https://openalex.org/W2962849707",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2963658612",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W2394938058",
        "https://openalex.org/W2962965405",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W3101913037",
        "https://openalex.org/W2081265723",
        "https://openalex.org/W2889518897",
        "https://openalex.org/W4322422052",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W4297747548",
        "https://openalex.org/W2118119027",
        "https://openalex.org/W2001135856",
        "https://openalex.org/W2696068122",
        "https://openalex.org/W2307381258",
        "https://openalex.org/W1815076433",
        "https://openalex.org/W2543215135",
        "https://openalex.org/W2122311631",
        "https://openalex.org/W2741375528",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W179757531",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964165364",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2164755781",
        "https://openalex.org/W2810224363",
        "https://openalex.org/W2574535369",
        "https://openalex.org/W2803930360",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2896807716",
        "https://openalex.org/W2963463583",
        "https://openalex.org/W2612675303",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963045354",
        "https://openalex.org/W2963385935",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2251654079",
        "https://openalex.org/W2612920290",
        "https://openalex.org/W2962985882",
        "https://openalex.org/W2260901818",
        "https://openalex.org/W2952138241",
        "https://openalex.org/W2755124548",
        "https://openalex.org/W2962972512",
        "https://openalex.org/W2963545005",
        "https://openalex.org/W1513168555",
        "https://openalex.org/W4297979306",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2250968833",
        "https://openalex.org/W2799149803",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W2176263492",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2150869743",
        "https://openalex.org/W2467173223",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2963125472"
    ],
    "abstract": "We propose a system that improves performance on single document summarization task using the CNN/DailyMail and Newsroom datasets. It follows the popular encoder-decoder paradigm, but with an extra focus on the encoder. The intuition is that the probability of correctly decoding an information significantly lies in the pattern and correctness of the encoder. Hence we introduce, encode –encode – decode. A framework that encodes the source text first with a transformer, then a sequence-to-sequence (seq2seq) model. We find that the transformer and seq2seq model complement themselves adequately, making for a richer encoded vector representation. We also find that paying more attention to the vocabulary of target words during abstraction improves performance. We experiment our hypothesis and framework on the task of extractive and abstractive single document summarization and evaluate using the standard CNN/DailyMail dataset and the recently released Newsroom dataset.",
    "full_text": "Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 70–79\nHong Kong, China, November 4, 2019.c⃝2019 Association for Computational Linguistics\nwww.aclweb.org/anthology/D19-56%2d\n70\nTransformer-based Model for Single Documents Neural Summarization\nElozino Egonmwan and Yllias Chali\nUniversity of Lethbridge\nLethbridge, AB, Canada\n{elozino.egonmwan, yllias.chali}@uleth.ca\nAbstract\nWe propose a system that improves perfor-\nmance on single document summarization\ntask using the CNN/DailyMail and News-\nroom datasets. It follows the popular encoder-\ndecoder paradigm, but with an extra focus on\nthe encoder. The intuition is that the probabil-\nity of correctly decoding an information sig-\nniﬁcantly lies in the pattern and correctness of\nthe encoder. Hence we introduce, encode –\nencode – decode. A framework that encodes\nthe source text ﬁrst with a transformer, then a\nsequence-to-sequence (seq2seq) model. We\nﬁnd that the transformer and seq2seq model\ncomplement themselves adequately, making\nfor a richer encoded vector representation. We\nalso ﬁnd that paying more attention to the vo-\ncabulary of target words during abstraction im-\nproves performance. We experiment our hy-\npothesis and framework on the task of ex-\ntractive and abstractive single document sum-\nmarization and evaluate using the standard\nCNN/DailyMail dataset and the recently re-\nleased Newsroom dataset.\n1 Introduction\nDocument summarization has been an active area\nof research, especially on the CNN/DailyMail\ndataset. Even with recent progress (Gehrmann\net al., 2018; Chen and Bansal, 2018), there is\nstill some work to be done in the ﬁeld. Al-\nthough extractive summarization seem\nto be less challenging because new words are not\ngenerated, identifying salient parts of the docu-\nment without any guide in the form of a query, is\na substantial problem to tackle.\nEarlier approaches for extractive summariza-\ntion use manual-feature engineering implemented\nwith graphs (Parveen and Strube, 2015; Erkan and\nRadev, 2004), integer linear programming (ILP)\n(Boudin et al., 2015; Nayeem and Chali, 2017).\nMore recent approaches are data-driven and im-\nplement a variety of neural networks (Jadhav and\nRajan, 2018; Narayan et al., 2017) majorly with an\nencoder-decoder framework (Narayan et al., 2018;\nCheng and Lapata, 2016).\nSimilar to the work of Nallapati et al. (2017), we\nconsider the extractive summarization task as a se-\nquence classiﬁcation problem. A major challenge\nwith this approach, is the fact that the training data\nis not sequentially labelled. Hence creating one\nfrom the abstractive ground-truth summary, is cru-\ncial. We improve on Nallapati et al. (2017)’s ap-\nproach to generate this labelled data, and evalua-\ntion shows that our extractive labels are more ac-\ncurate. Another hurdle in this task, is the imbal-\nance in the created data, that is, most of the doc-\nument’s sentences are labelled 0 (excluded from\nthe summary) than 1, because just a few sentences\nactually make up a summary. Hence the neural ex-\ntractor tends to be biased and suffer from a lot of\nfalse-negative labels. We also present a simple ap-\nproach to reduce this bias. Most importantly, our\nneural extractor uses the recent bidirectional trans-\nformer encoder (Vaswani et al., 2017) with details\nprovided in Section 3.1.\nMore interesting than extractive summaries,\nabstractive summaries correlate better with\nsummaries that a human would present.\nAbstractive summarization does not\nsimply reproduce salient parts of the document\nverbatim, but rewrites them in a concise form,\nusually introducing novel words along the way\nby utilizing some key abstraction techniques such\nas paraphrasing (Gupta et al., 2018), compres-\nsion (Filippova et al., 2015) or sentence fusion\n(Barzilay and McKeown, 2005). However, it is\nmet with major challenges like grammatical cor-\nrectness and repetition of words especially when\ngenerating long-worded sentences. Nonetheless\nremarkable progress have been achieved with the\n71\nuse of seq2seq models (Gehrmann et al., 2018;\nSee et al., 2017; Chopra et al., 2016; Rush et al.,\n2015) and a reward instead of loss function via\ndeep-reinforcement learning (Chen and Bansal,\n2018; Paulus et al., 2017; Ranzato et al., 2015).\nWe see abstractive summarization in same light\nas several other authors (Chen and Bansal, 2018;\nHsu et al., 2018; Liu et al., 2018) – extract salient\nsentences and then abstract; thus sharing similar\nadvantages as the popular divide-and-conquer al-\ngorithm. More-so, it mitigates the problem of in-\nformation redundancy, since the mini-source, ie\nextracted document, contains distinct salient sen-\ntences. Our abstractive model is a blend of the\ntransformer and seq2seq model. We notice im-\nprovements using this framework in the abstrac-\ntive setting. This is because, to generate coher-\nent and grammatically correct sentences, we need\nto be able to learn long-term dependency rela-\ntions. The transformer complements the seq2seq\nmodel in this regard with its multi-head self at-\ntention. Also the individual attention heads in the\ntransformer model mimics behavior related to the\nsyntactic and semantic structure of the sentence\n(Vaswani et al., 2017, 2018). Hence, the trans-\nformer produces a richer meaningful vector repre-\nsentation of the input, from which we can encode\na ﬁxed state vector for decoding.\nThe main contributions of this work are:\n•We present a simple algorithm for building a\nsentence-labelled corpus for extractive sum-\nmarization training that produces more accu-\nrate results.\n•We propose a novel framework for the task\nof extractive single document summarization\nthat improves the current state-of-the-art on\ntwo speciﬁc datasets.\n•We introduce the encode - encode - decode\nparadigm using two complementary models,\ntransformer and seq2seq for generating ab-\nstractive summaries that improves current top\nperformance on two speciﬁc datasets.\n2 Task Deﬁnition\nGiven a document D = ( S1,...,S n) with n\nsentences comprising of a set of words DW =\n{d1,...,d w}, the task is to produce an extractive\n(SE) or abstractive (SA) summary that contains\nsalient information in D, where SE ⊆DW and\nSA = {w1,...,w s}|∃ wi ̸∈DW .\nFigure 1: Extractive Model Architecture\n3 Method\nWe describe our summarization model in two\nmodules – Extraction and Abstraction. The ab-\nstraction module simply learns to paraphrase and\ncompress the output of the extracted document\nsentences.\n3.1 Extraction\nAs illustrated in Figure 1, our model classiﬁes\neach sentence in a document as being summary-\nworthy or not. However, in order to enhance this\nsequence classiﬁcation process, we encode the in-\nput document with a T RANSFORMER . A logistic\nclassiﬁer then learns to label each sentence in the\ntransformed document.\n3.1.1 TRANSFORMER Encoder\nThe input to the Transformer is the document rep-\nresentation, which is a concatenation of the vec-\ntor representation of its sentences. Each sentence\nrepresentation is obtained by averaging the vector\nrepresentation of its constituent words.\nSi = 1/m\nm∑\ni=1\nwi (1)\nDj = S1∥S2∥... ∥Sn (2)\n72\nThe transformer encoder is composed of 6\nstacked identical layers. Each layer contains\n2 sub-layers with multi-head self attention and\nposition-wise fully connected feed-forward net-\nwork respectively. Full details with implementa-\ntion are provided in (Vaswani et al., 2017, 2018).\nThe bidirectional Transformer often referred to as\nthe Transformer encoder learns a rich representa-\ntion of the document that captures long-range syn-\ntactic and semantic dependency between the sen-\ntences.\n3.1.2 Sentence Extraction\nThe ﬁnal layer of our extraction model is a soft-\nmax layer which performs the classiﬁcation. We\nlearn the probability of including a sentence in the\nsummary,\nyi\np = softmax(WS‘\ni + b) (3)\nwhere W and bare trainable parameters and S‘\ni is\nthe transformed representation of the ith sentence\nin document Dj, by minimizing the cross-entropy\nloss\n(4)L= −(ytlog(yp) + (1−yt)log(1 −yp))\nbetween the predicted probabilities, yp and true\nsentence-labels, yt during training.\n3.1.3 Extractive Training\nFiltering Currently, no extractive summariza-\ntion dataset exists. Hence it is customary to create\none from the abstractive ground-truth summaries\n(Chen and Bansal, 2018; Nallapati et al., 2017).\nWe observe however, that some summaries are\nmore abstractive than others. Since the extractive\nlabels are usually gotten by doing some n-gram\noverlap matching, the greater the abstractiveness\nof the ground-truth the more inaccurate the tuned\nextractive labels are. We ﬁlter out such samples 1\nas illustrated in Table 1. In our work, we consider\na reference summary Rj as overly abstractive if\nit has zero bigram overlap with the corresponding\ndocument Dj, excluding stop words.\n#bigram(Dj,Rj) == 0 (5)\nSee et al. (2017) and Paulus et al. (2017) trun-\ncate source documents to 400 tokens and target\n1Filtering is used only for the training set, to ensure that\nevaluation comparisons on the test set with existing models\nare fair\nsummaries to 100 tokens. We totally exclude doc-\numents with more than 30 sentences and trun-\ncate or pad as necessary to 20 sentences per doc-\nument. From the over 280,000 and 1.3M train-\ning pairs in the CNN/DM and Newsroom training\ndataset respectively, our ﬁltering yields approxi-\nmately 150,000 and 250,000 abstractive summa-\nrization sub-dataset. We report evaluation scores\nusing the training sets as-is versus our ﬁltered\ntraining sets, to show that ﬁltering the training\nsamples does improve results.\nDocument: world-renowned chef, author\nand emmy winning television personality an-\nthony bourdain visits quebec in the next\nepisode of “ anthony bourdain : parts un-\nknown, ” airing sunday, may 5, at 9 p.m. et.\nfollow the show on twitter and facebook.\nSummary: 11 things to know about quebec.\no canada! our home and delicious land.’\nTable 1: Example of an overly abstractive summary\nwith zero bigram overlap with the document from a\nCNN/DM training sample.\nTuning We use a very simple approach to cre-\nate extractive labels for our neural extractor. We\nhypothesize that each reference summary sen-\ntence originates from at least one document sen-\ntence. The goal is to identify the most-likely doc-\nument sentence. Different from Nallapati et al.\n(2017)’s approach to greedily add sentences to\nthe summary that maximizes the ROUGE score,\nour approach is more similar to Chen and Bansal\n(2018)’s model that calculates the individual refer-\nence sentence-level score as per its similarity with\neach sentence in the corresponding document.\nHowever, our sentence-level similarity score is\nbased on its bigram overlap:\n(6)score(Rt\nj) =amaxi(bigram(Di\nj,Rt\nj))\nfor each tth sentence in the reference summary,\nRj, per ith sentence in document Dj, in contrast\nto Chen and Bansal (2018)’s that uses ROUGE-\nLrecall score. Additionally, for every time both\nwords in the set of bigrams-overlap are stopwords,\nwe decrement the similarity score by 1, for exam-\nple, (on, the) is an invalid bigram-overlap while\n(the, President) is valid. We do this, to capture\nmore important similarities instead of trivial ones.\nFor statistical purposes, we evaluate our extrac-\ntive trainer for tuning the document’s sentences to\n73\n0’s and 1’s against (Nallapati et al., 2017)’s which\nis our foundation.\nExtractive Trainer R-1 R-2 R-L\nOurs 49.5 27.8 45.8\nOurs + ﬁlter 51.4 31.7 50.3\n(Nallapati et al., 2017) 48.4 27.5 44.4\nTable 2: ROUGE-F1 (%) scores of manually crafted\nextractive trainers for producing sentence-level extrac-\ntive labels for CNN/DM.\nWe apply our tuned dataset to the neural extrac-\ntive summarizer explained in Sections 3.1.1 and\n3.1.2 and report results in Tables 3 and 4.\nImbalanced Extractive Labels Because a sum-\nmary is a snippet of the document, the major-\nity of the labels are rightly 0 (excluded from the\nsummary). Hence a high classiﬁcation accuracy\ndoes not necessarily translate to a highly salient\nsummary. Therefore, we consider the F1 score,\nwhich is a weighted average of the precision and\nrecall, and apply an early stopping criteria when\nminimizing the loss, if the F1 score does not in-\ncrease after a set number of training epochs. Addi-\ntionally during training, we synthetically balance\nthe labels, by forcing some random sentences to\nbe labelled as 1 and subsequently masking their\nweights.\nNumber of sentences to extract The number of\nextracted sentences is not trivial, as this signiﬁ-\ncantly affects the summary length and hence eval-\nuation scores. Chen and Bansal (2018) introduced\na stop criterion in their reinforcement learning pro-\ncess. We implemented a basic subjective approach\nbased on the dataset. Since the gold summaries are\ntypically 3 or 4 sentences long, we extract the top\n3 sentences by default, but proceed to additionally\nextract a 4th sentence if the conﬁdence score from\nthe softmax function is greater than 0.55.\n3.2 Abstraction\nThe input to our abstraction module is a subset of\nthe document’s sentences which comprises of the\noutput of the extraction phase from Section 3.1.2.\nFor each document Dj, initially comprising of n\nsentences, we abstract its extracted sentences,\nSE\nj = {S1\nj ,S2\nj ,...,S m\nj } (7)\nwhere m < nand SE\nj ⊆Dj, by learning to\njointly paraphrase (Gupta et al., 2018) and com-\npress (Filippova et al., 2015). We add one more\nencoding layer to the standard encoder-aligner-\ndecoder (Bahdanau et al., 2014; Luong et al.,\n2015), ie, encode-encode-align-decode. The in-\ntuition is to seemingly improve the performance\nof the decoder by providing an interpretable and\nrichly encoded sequence. For this, we interleave\ntwo efﬁcient models – transformer (Vaswani et al.,\n2017) and sequence-to-sequence (Sutskever et al.,\n2014), speciﬁcally GRU -RNN (Chung et al., 2014;\nCho et al., 2014). Details are presented in subse-\nquent subsections.\n3.2.1 Encoder – TRANSFORMER\nThe transformer encoder has same implementation\nfrom Vaswani et al. (2017) as explained in Section\n3.1.1, except the inputs are sentence-level vector\nrepresentations not document. Also, the sentence\nrepresentations in this module are not averaged\nconstituent word representations as in the extrac-\ntion module but concatenated. That is, for eachith\nsentence in equation 7, its vector representation, is\nthe concatenation of its constituent word embed-\ndings\nSi\nj = w1∥w2∥... ∥wn (8)\nThe output of equation 8 serves as the input vec-\ntor representation to the transformer encoder. We\nuse the transformer-encoder during abstraction as\nsort of a pre-training module of the input sentence.\n3.2.2 Encoder – GRU -RNN\nWe use a single layer uni-directional GRU -RNN\nwhose input is the output of the transformer. The\nGRU -RNN encoder (Chung et al., 2014; Cho et al.,\n2014) produces ﬁxed-state vector representation\nof the transformed input sequence using the fol-\nlowing equations:\nz= σ(stUz + xt−1Wz) (9)\nr= σ(stUr + xt−1Wr) (10)\nh= tanh(stUh + (xt−1 ⊙r)Wh) (11)\nxt = (1−z) ⊙h+ z⊙xt−1 (12)\nwhere rand zare the reset and update gates re-\nspectively,W and Uare the network’s parameters,\nxt is the hidden state vector at timestep t, st is the\ninput vector and ⊙represents the Hadamard prod-\nuct.\n74\nExtractive Model R-1 R-2 R-L\nLEAD (See et al., 2017) 40.3 17.7 36.5\nLEAD (Narayan et al., 2018) 39.6 17.7 36.2\nLEAD (ours) 40.1 17.6 36.0\n(Nallapati et al., 2017) 39.6 16.2 35.3\nREFRESH (Narayan et al., 2018) 40.0 18.2 36.6\nFAST (Chen and Bansal, 2018) 41.4 18.7 37.7\nNEUSUM (Zhou et al., 2018) 41.6 19.0 37.0\nContent Selector (Gehrmann et al., 2018) 42.0 15.9 37.3\nTRANS -ext 41.0 18.4 36.9\nTRANS -ext + ﬁlter 42.8 21.1 38.4\nTable 3: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various extractive models on the CNN/DM\ntest set. The ﬁrst section shows LEAD-3 model scores. The second section shows scores for baseline models. The\nthird section shows our model’s scores\nExtractive Model R-1 R-2 R-L\nLEAD* (Grusky et al., 2018) 30.49 21.27 28.42\nTextRank* (Barrios et al., 2016) 22.77 9.79 18.98\nTRANS -ext 37.21 25.17 32.41\nTRANS -ext + ﬁlter 41.52 30.62 36.96\nTable 4: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various extractive models on the Newsroom\nreleased test set. * marks results taken from Grusky et al. (2018)\n3.3 Decoder – GRU -RNN\nThe ﬁxed-state vector representation produced by\nthe GRU -RNN encoder is used as initial state for\nthe decoder. At each time step, the decoder re-\nceives the previously generated word, yt−1 and\nhidden state st−1 at time step t−1. The output\nword, yt at each time step, is a softmax probability\nof the vector in equation 11 over the set of vocab-\nulary words, V.\n4 Experiments\nWe used pre-trained 300-dimensional gloVe2\nword-embeddings (Pennington et al., 2014).\nThe transformer encoder was setup with the\ntransformer base hyperparameter setting from\nthe tensor2tensor library (Vaswani et al., 2018) 3,\nbut the hidden size and dropout were reset to 300\nand 0.0 respectively. We also use 300 hidden\nunits for the GRU -RNN encoder. The tensor2tensor\nlibrary comes with pre-processed/tokenized ver-\nsions of the dataset, we however perform these op-\nerations independently. For abstraction, our tar-\nget vocabulary is a set of approximately 50,000\nand 80,000 words for CNN/DM and Newsroom\n2https://nlp.stanford.edu/projects/\nglove/\n3https://github.com/tensorflow/\ntensor2tensor\ncorpus respectively. It contains words in our\ntarget training and test sets that occur at least\ntwice. Experiments showed that using this sub-\nset of vocabulary words as opposed to over\n320,000 vocabulary words contained in gloVe\nimproves both training time and performance of\nthe model. During the abstractive training, we\nmatch summary sentence with its corresponding\nextracted document sentence using equation 6 and\nlearn to minimize the seq2seq loss implemented\nin tensorflow API4 with AdamOptimizer\n(Kingma and Ba, 2014). We employ early stop-\nping when the validation loss does not decrease\nafter 5 epochs. We apply gradient clipping at 5.0\n(Pascanu et al., 2013). We use greedy-decoding\nduring training and validation and set the maxi-\nmum number of iterations to 5 times the target sen-\ntence length. Beam-search decoding is used dur-\ning inference.\n4.1 Datasets\nWe evaluate our models on the non-anonymized\nversion of the CNN -DM corpus (Hermann et al.,\n2015; Nallapati et al., 2016) and the recent News-\nroom dataset (Grusky et al., 2018) released by\nConnected Experiences Lab 5. The Newsroom\n4https://www.tensorflow.org/api_docs/\npython/tf/contrib/seq2seq/sequence_loss\n5https://summari.es\n75\nAbstractive Model R-1 R-2 R-L\nRL+Intra-Att (Paulus et al., 2017) 41.16 15.75 39.08\nKIGN+Pred (Li et al., 2018) 38.95 17.12 35.68\nFAST (Chen and Bansal, 2018) 40.88 17.80 38.54\nBottom-Up (Gehrmann et al., 2018) 41.22 18.68 38.34\nTRANS -ext + abs 41.05 17.87 36.73\nTRANS -ext + ﬁlter +abs 41.89 18.90 38.92\nTable 5: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various abstractive models on the CNN/DM\ntest set.\nAbstractive Model R-1 R-2 R-L\nAbs-N* (Rush et al., 2015) 5.88 0.39 5.32\nPointer* (See et al., 2017) 26.02 13.25 22.43\nTRANS -ext + abs 33.81 15.37 28.92\nTRANS -ext + ﬁlter + abs 35.74 16.52 30.17\nTable 6: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various abstractive models on the Newsroom\nreleased test set. * marks results taken from Grusky et al. (2018)\ncorpus contains over 1.3M news articles together\nwith various metadata information such as the ti-\ntle, summary, coverage and compression ratio.\nCNN/DM summaries are twice as long as News-\nroom summaries with average word lengths of 66\nand 26 respectively.\n4.2 Evaluation\nFollowing previous works (See et al., 2017; Nal-\nlapati et al., 2017; Chen and Bansal, 2018), we\nevaluate both datasets on standard ROUGE-1,\nROUGE-2 and ROUGE-L (Lin, 2004). It cal-\nculates the appropriate n-gram word-overlap be-\ntween the reference and system summaries.\n4.3 Results Analysis\nWe used the ofﬁcial pyrouge script6 with op-\ntion7. Table 3 and 5 presents extractive and ab-\nstractive results on the CNN/DM dataset respec-\ntively, while Tables 4 and 6 for the Newsroom\ndataset. For clarity, we present results separately\nfor each model and dataset.\nOur baseline non-ﬁltered extractive ( TRANS -\next) model is highly competitive with top mod-\nels. Our TRANS -ext + ﬁlter produces an average\nof about +1 and +9 points across reported ROUGE\nvariants on the CNN/DM and Newsroom datasets\nrespectively, showing that our model does a bet-\nter job at identifying the most salient parts of the\ndocument than existing state-of-the-art extractive\n6https://github.com/andersjo/pyrouge/\ntree/master/tools/ROUGE-1.5.5\n7-n 2 -w 1.2 -m -a -c 95\nmodels. We observe the large margin in the News-\nroom dataset results, as existing baselines are just\nthe LEAD-3 and TEXTRANK of (Barrios et al.,\n2016). The Newsroom dataset was recently re-\nleased and is yet to be thoroughly explored, how-\never it is a larger dataset and contains more diverse\nsummaries as analyzed by Grusky et al. (2018).\nWe also experimented with the empirical out-\ncome of using imbalanced extractive labels which\nusually leads to bias towards the majority class.\nInterestingly, our extractive model has +20%\nF Score increase when trained with balanced la-\nbels. Switching the transformer encoder with a\nseq2seq encoder, resulted in a drop of about 2\nROUGE points, showing that the transformer en-\ncoder does learn features that adds meaning to the\nvector representation of our input sequence.\nOur baseline non-ﬁltered abstractive ( TRANS -\next + abs) model is also highly competitive with\ntop models, with a drop of -0.81 ROUGE-2 points\nagainst Gehrmann et al. (2018)’s model which is\nthe current state-of-the art. Our TRANS -ext + ﬁl-\nter + abs produces an average of about +0.5 and\n+7 points across reported ROUGE variants on the\nCNN/DM and Newsroom datasets respectively,\nshowing empirically that our model is an improve-\nment of existing abstractive summarization mod-\nels.\nOn the abstractiveness of our summaries, af-\nter aligning with the ground-truth as explained\nin Section 3.2 about 60% of our extracted docu-\nment sentences were paraphrased and compressed.\n76\nO: the two clubs, who occupy the top two spots\nin spain’s top ﬂight, are set to face each other at\nthe nou camp on sunday.\nG: real madrid face barcelona in the nou camp\nR: real madrid will travel to the nou camp to face\nbarcelona on sunday.\nO: dangelo conner, from new york, ﬁlmed him-\nself messing around with the powerful weapon in\na friend’s apartment, ﬁrst waving it around, then\nsending volts coursing through a coke can .\nG: dangelo conner from new york was fooling\naround with his gun\nR: dangelo conner, from new york ,was fooling\naround with stun gun.\nO: jamie peacock broke his try drought with a\ndouble for leeds in their win over salford on sun-\nday.\nG: jamie adam scored to win over salford for\nleeds\nR: jamie peacock scored two tries for leeds in\ntheir win over salford.\nO: britain’s lewis hamilton made the perfect start\nto his world title defense by winning the opening\nrace of the f1 season in australia sunday to lead a\nmercedes one-two in melbourne .\nG: lewis hamilton wins ﬁrst race of season in\naustralia\nR: lewis hamilton wins opening race of 2015 f1\nseason in australia .\nTable 7: Examples of some of our generated para-\nphrases from the CNN/DM dataset, where O, G, R\nrepresents Originating document sentence, our model’s\nGenerated paraphrase and Reference sentences from\nthe ground-truth summary respectively.\nWe highlight examples of some of the generated\nparaphrases in Table 7. Table 7 show that our\nparaphrases are well formed, abstractive (e.g pow-\nerful weapon – gun, messing around – fooling\naround), capable of performing syntactic manip-\nulations ( e.g for leeds in their win over sadford\n– win over salford for leeds ) and compression as\nseen in all the examples.\n5 Related Work\nSummarization has remained an interesting and\nimportant NLP task for years due to its diverse\napplications - news headline generation, weather\nforecasting, emails ﬁltering, medical cases, rec-\nommendation systems, machine reading compre-\nhension MRC and so forth (Khargharia et al.,\n2018).\nEarly summarization models were mostly ex-\ntractive and manual-feature engineered (Knight\nand Marcu, 2000; Jing and McKeown, 2000; Dorr\net al., 2003; Berg-Kirkpatrick et al., 2011). With\nthe introduction of neural networks (Sutskever\net al., 2014) and availability of large training data,\ndeep learning became a viable approach (Rush\net al., 2015; Chopra et al., 2016).\nExtraction has been handled on different lev-\nels of granularity – word (Cheng and Lapata,\n2016), phrases (Bui et al., 2016; Gehrmann et al.,\n2018), sentence (Cheng and Lapata, 2016; Nalla-\npati et al., 2016, 2017) each with its challenges.\nWord and phrase level extraction although more\nconcise usually suffers from grammatical incor-\nrectness, while sentence-level extraction are too\nlengthy and sometimes contain redundant infor-\nmation. Hence Berg-Kirkpatrick et al. (2011); Fil-\nippova et al. (2015); Durrett et al. (2016) learn to\nextract and compress at sentence-level.\nIdentifying the likely most salient part of the\ntext as summary-worthy is very crucial. Some\nauthors have employed integer linear program-\nming (Martins and Smith, 2009; Gillick and Favre,\n2009; Boudin et al., 2015), graph concepts (Erkan\nand Radev, 2004; Parveen et al., 2015; Parveen\nand Strube, 2015), ranking with reinforcement\nlearning (Narayan et al., 2018) and mostly related\nto our work – binary classiﬁcation (Shen et al.,\n2007; Nallapati et al., 2017; Chen and Bansal,\n2018)\nOur binary classiﬁcation architecture differs\nsigniﬁcantly from existing models because it uses\na transformer as the building block instead of a\nbidirectional GRU-RNN (Nallapati et al., 2017),\nor bidirectional LSTM-RNN (Chen and Bansal,\n2018). To the best of our knowledge, our utiliza-\ntion of the transformer encoder model as a build-\ning block for binary classiﬁcation is novel, al-\nthough the transformer has been successfully used\nfor language understanding (Devlin et al., 2018),\nmachine translation ( MT) (Vaswani et al., 2017)\nand paraphrase generation (Zhao et al., 2018).\nFor generation of abstractive summaries, before\nthe ubiquitous use of neural nets, manually crafted\nrules and graph techniques were utilized with con-\nsiderable success. Barzilay and McKeown (2005);\nCheung and Penn (2014) fused two sentences into\none using their dependency parsed trees. Re-\n77\ncently, sequence-to-sequence models (Sutskever\net al., 2014) with attention (Bahdanau et al., 2014;\nChopra et al., 2016), copy mechanism (Vinyals\net al., 2015; Gu et al., 2016), pointer-generator\n(See et al., 2017), graph-based attention (Tan et al.,\n2017) have been explored. Since the system gener-\nated summaries are usually evaluated on ROUGE,\nits been beneﬁcial to directly optimize this met-\nric during training via a suitable policy using rein-\nforcement learning (Paulus et al., 2017; Celikyil-\nmaz et al., 2018).\nSimilar to Rush et al. (2015); Chen and Bansal\n(2018) we abstract by simplifying our extracted\nsentences. We jointly learn to paraphrase and\ncompress, but different from existing models\npurely based on RNN , we implement a blend of\ntwo proven efﬁcient models – transformer encoder\nand GRU -RNN . Zhao et al. (2018) paraphrased\nwith a transformer-decoder, we ﬁnd that using the\nGRU -RNN decoder but with a two-level stack of\nhybrid encoders (transformer andGRU -RNN ) gives\nbetter performance. To the best of our knowledge,\nthis architectural blend is novel.\n6 Conclusion\nWe proposed two frameworks for extractive and\nabstractive summarization and demonstrated that\nthey each improve results over existing state-of-\nthe art. Our models are simple to train, and\nthe intuition/hypothesis behind the formulation are\nstraightforward and logical. The scientiﬁc correct-\nness is provable, as parts of our model architecture\nhave been used in other NLG -related tasks such as\nMT with state-of-the art results.\nAcknowledgments\nWe would like to thank the anonymous review-\ners for their useful comments. The research re-\nported in this paper was conducted at the Univer-\nsity of Lethbridge and supported by Alberta Inno-\nvates and Alberta Education.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nFederico Barrios, Federico L ´opez, Luis Argerich, and\nRosa Wachenchauzer. 2016. Variations of the simi-\nlarity function of textrank for automated summariza-\ntion. CoRR.\nRegina Barzilay and Kathleen R McKeown. 2005.\nSentence fusion for multidocument news summa-\nrization. Computational Linguistics , 31(3):297–\n328.\nTaylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.\n2011. Jointly learning to extract and compress. In\nProceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies-Volume 1, pages 481–490. As-\nsociation for Computational Linguistics.\nFlorian Boudin, Hugo Mougard, and Benoit Favre.\n2015. Concept-based summarization using integer\nlinear programming: From concept pruning to mul-\ntiple optimal solutions. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP)\n2015.\nDuy Duc An Bui, Guilherme Del Fiol, John F Hurdle,\nand Siddhartha Jonnalagadda. 2016. Extractive text\nsummarization system to aid data extraction from\nfull text in systematic review development. Journal\nof biomedical informatics, 64:265–272.\nAsli Celikyilmaz, Antoine Bosselut, Xiaodong He, and\nYejin Choi. 2018. Deep communicating agents for\nabstractive summarization. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 1662–1675.\nYen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-\ntive summarization with reinforce-selected sentence\nrewriting. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 675–686.\nJianpeng Cheng and Mirella Lapata. 2016. Neural\nsummarization by extracting sentences and words.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), volume 1, pages 484–494.\nJackie Chi Kit Cheung and Gerald Penn. 2014. Unsu-\npervised sentence enhancement for automatic sum-\nmarization. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 775–786.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint\narXiv:1406.1078.\nSumit Chopra, Michael Auli, and Alexander M Rush.\n2016. Abstractive sentence summarization with at-\ntentive recurrent neural networks. In Proceedings of\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 93–98.\n78\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. arXiv preprint arXiv:1412.3555.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nBonnie Dorr, David Zajic, and Richard Schwartz.\n2003. Hedge trimmer: A parse-and-trim approach\nto headline generation. In Proceedings of the HLT-\nNAACL 03 on Text summarization workshop-Volume\n5, pages 1–8. Association for Computational Lin-\nguistics.\nGreg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.\n2016. Learning-based single-document summariza-\ntion with compression and anaphoricity constraints.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), volume 1, pages 1998–2008.\nG¨unes Erkan and Dragomir R Radev. 2004. Lexrank:\nGraph-based lexical centrality as salience in text\nsummarization. Journal of artiﬁcial intelligence re-\nsearch, 22:457–479.\nKatja Filippova, Enrique Alfonseca, Carlos A Col-\nmenares, Lukasz Kaiser, and Oriol Vinyals. 2015.\nSentence compression by deletion with lstms. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing , pages\n360–368.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109.\nDan Gillick and Benoit Favre. 2009. A scalable global\nmodel for summarization. In Proceedings of the\nWorkshop on Integer Linear Programming for Natu-\nral Langauge Processing, pages 10–18. Association\nfor Computational Linguistics.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 708–719.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor OK\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\nvolume 1, pages 1631–1640.\nAnkush Gupta, Arvind Agarwal, Prawaan Singh, and\nPiyush Rai. 2018. A deep generative framework for\nparaphrase generation. In Thirty-Second AAAI Con-\nference on Artiﬁcial Intelligence.\nKarl Moritz Hermann, Tomas Kocisky, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. Teaching ma-\nchines to read and comprehend. In Advances in\nneural information processing systems, pages 1693–\n1701.\nWan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui\nMin, Jing Tang, and Min Sun. 2018. A uniﬁed\nmodel for extractive and abstractive summarization\nusing inconsistency loss. In Proceedings of the 56th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n132–141.\nAishwarya Jadhav and Vaibhav Rajan. 2018. Extrac-\ntive summarization with swap-net: Sentences and\nwords from alternating pointer networks. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), volume 1, pages 142–151.\nHongyan Jing and Kathleen R McKeown. 2000. Cut\nand paste based text summarization. In Proceed-\nings of the 1st North American chapter of the As-\nsociation for Computational Linguistics conference,\npages 178–185. Association for Computational Lin-\nguistics.\nDebabrata Khargharia, Nabajit Newar, and Nomi\nBaruah. 2018. Applications of text summariza-\ntion. International Journal of Advanced Research\nin Computer Science, 9(3).\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKevin Knight and Daniel Marcu. 2000. Statistics-\nbased summarization-step one: Sentence compres-\nsion. AAAI/IAAI, 2000:703–710.\nChenliang Li, Weiran Xu, Si Li, and Sheng Gao. 2018.\nGuiding generation for abstractive text summariza-\ntion based on key information guide network. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 55–60.\nChin-Yew Lin. 2004. Rouge: A package for auto-\nmatic evaluation of summaries. Text Summarization\nBranches Out.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and\nNoam Shazeer. 2018. Generating wikipedia by\nsummarizing long sequences. arXiv preprint\narXiv:1801.10198.\nThang Luong, Hieu Pham, and Christopher D Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1412–1421.\n79\nAndr´e FT Martins and Noah A Smith. 2009. Summa-\nrization with a joint model for sentence extraction\nand compression. In Proceedings of the Workshop\non Integer Linear Programming for Natural Lan-\ngauge Processing, pages 1–9. Association for Com-\nputational Linguistics.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.\nSummarunner: A recurrent neural network based se-\nquence model for extractive summarization of docu-\nments. In AAAI, pages 3075–3081.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nC ¸ a glar Gulc ¸ehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nrnns and beyond. CoNLL 2016, page 280.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n2018. Ranking sentences for extractive summariza-\ntion with reinforcement learning. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), volume 1, pages 1747–1759.\nShashi Narayan, Nikos Papasarantopoulos, Shay B\nCohen, and Mirella Lapata. 2017. Neural extrac-\ntive summarization with side information. arXiv\npreprint arXiv:1704.04530.\nMir Tafseer Nayeem and Yllias Chali. 2017. Extract\nwith order for coherent multi-document summariza-\ntion. In Proceedings of TextGraphs-11: the Work-\nshop on Graph-based Methods for Natural Lan-\nguage Processing, pages 51–56.\nDaraksha Parveen, Hans-Martin Ramsl, and Michael\nStrube. 2015. Topical coherence for graph-based ex-\ntractive summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1949–1954.\nDaraksha Parveen and Michael Strube. 2015. Inte-\ngrating importance, non-redundancy and coherence\nin graph-based extractive summarization. In IJCAI,\npages 1298–1304.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difﬁculty of training recurrent neural\nnetworks. In International conference on machine\nlearning, pages 1310–1318.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization. arXiv preprint arXiv:1705.04304.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2015. Sequence level train-\ning with recurrent neural networks. arXiv preprint\narXiv:1511.06732.\nAlexander M Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 379–389.\nAbigail See, Peter J Liu, and Christopher D Man-\nning. 2017. Get to the point: Summarization\nwith pointer-generator networks. arXiv preprint\narXiv:1704.04368.\nDou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and\nZheng Chen. 2007. Document summarization us-\ning conditional random ﬁelds. In IJCAI, volume 7,\npages 2862–2867.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nJiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017.\nAbstractive document summarization with a graph-\nbased attentional neural model. In Proceedings of\nthe 55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1171–1181.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan Gomez, Stephan Gouws, Llion\nJones, Łukasz Kaiser, Nal Kalchbrenner, Niki Par-\nmar, et al. 2018. Tensor2tensor for neural machine\ntranslation. In Proceedings of the 13th Conference\nof the Association for Machine Translation in the\nAmericas (Volume 1: Research Papers) , volume 1,\npages 193–199.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Proceedings of the\n28th International Conference on Neural Informa-\ntion Processing Systems-Volume 2 , pages 2692–\n2700. MIT Press.\nSanqiang Zhao, Rui Meng, Daqing He, Andi Saptono,\nand Bambang Parmanto. 2018. Integrating trans-\nformer and paraphrase rules for sentence simpliﬁ-\ncation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 3164–3173, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nQingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,\nMing Zhou, and Tiejun Zhao. 2018. Neural docu-\nment summarization by jointly learning to score and\nselect sentences. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 654–663."
}