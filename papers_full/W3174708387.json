{
  "title": "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search",
  "url": "https://openalex.org/W3174708387",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2114721219",
      "name": "Gyuwan Kim",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2115080942",
      "name": "Kyunghyun Cho",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970454332",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2995983533",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W4310492983",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2981757109",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3034408420",
    "https://openalex.org/W2950014519",
    "https://openalex.org/W2994749257",
    "https://openalex.org/W4287901267",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035030897",
    "https://openalex.org/W3035251378",
    "https://openalex.org/W4302023899",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W3101731278",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3154971029",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3008851394",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2767421475",
    "https://openalex.org/W3118895645",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3033188311",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2899948949",
    "https://openalex.org/W3005957694",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3034742519",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W2895976713",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3159727696",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2160660594",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2963643655",
    "https://openalex.org/W3171750540",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W3174461835",
    "https://openalex.org/W2965658867",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W3106171539",
    "https://openalex.org/W2853336868",
    "https://openalex.org/W2963766446",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W3025165719",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W3023662336",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2981698279",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W3022774126",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2950305991",
    "https://openalex.org/W2905741102",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2779809129",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3164115288",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W3174401451"
  ],
  "abstract": "Gyuwan Kim, Kyunghyun Cho. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 6501–6511\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6501\nLength-Adaptive Transformer:\nTrain Once with Length Drop, Use Anytime with Search\nGyuwan Kim\nClova AI, NA VER Corp.\ngyuwan.kim@navercorp.com\nKyunghyun Cho\nNew York University\nkyunghyun.cho@nyu.edu\nAbstract\nDespite transformers’ impressive accuracy,\ntheir computational cost is often prohibitive\nto use with limited computational resources.\nMost previous approaches to improve infer-\nence efﬁciency require a separate model for\neach possible computational budget. In this\npaper, we extend PoWER-BERT (Goyal et al.,\n2020) and propose Length-Adaptive Trans-\nformer that can be used for various infer-\nence scenarios after one-shot training. We\ntrain a transformer with LengthDrop, a struc-\ntural variant of dropout, which stochastically\ndetermines a sequence length at each layer.\nWe then conduct a multi-objective evolution-\nary search to ﬁnd a length conﬁguration that\nmaximizes the accuracy and minimizes the\nefﬁciency metric under any given computa-\ntional budget. Additionally, we signiﬁcantly\nextend the applicability of PoWER-BERT be-\nyond sequence-level classiﬁcation into token-\nlevel classiﬁcation with Drop-and-Restore pro-\ncess that drops word-vectors temporarily in in-\ntermediate layers and restores at the last layer\nif necessary. We empirically verify the util-\nity of the proposed approach by demonstrating\nthe superior accuracy-efﬁciency trade-off un-\nder various setups, including span-based ques-\ntion answering and text classiﬁcation. Code is\navailable at https://github.com/clovaai/length-\nadaptive-transformer.\n1 Introduction\nPre-trained language models (Peters et al., 2018;\nDevlin et al., 2018; Radford et al., 2019; Yang\net al., 2019; He et al., 2020) have achieved notable\nimprovements in various natural language process-\ning (NLP) tasks. Most of them rely on transform-\ners (Vaswani et al., 2017), and the number of model\nparameters ranges from hundreds of millions to bil-\nlions (Shoeybi et al., 2019; Raffel et al., 2019; Ka-\nplan et al., 2020; Brown et al., 2020). Despite this\nhigh accuracy, excessive computational overhead\nduring inference, both in terms of time and memory,\nhas hindered its use in real applications. This level\nof excessive computation has further raised the con-\ncern over energy consumption as well (Schwartz\net al., 2019; Strubell et al., 2019; Cao et al., 2020).\nRecent studies have attempted to address these\nconcerns regarding large-scale transformers’ com-\nputational and energy efﬁciency (see §7 for a more\nextensive discussion.) Among these, we focus on\nPoWER-BERT (Goyal et al., 2020) which pro-\ngressively reduces sequence length by eliminat-\ning word-vectors based on the attention values as\npassing layers. PoWER-BERT establishes the su-\nperiority of accuracy-time trade-off over earlier\napproaches (Sanh et al., 2019; Sun et al., 2019;\nMichel et al., 2019). However, it requires us to\ntrain a separate model for each efﬁciency constraint.\nIn this paper, we thus develop a framework based\non PoWER-BERT such that we can train a single\nmodel that can be adapted in the inference time to\nmeet any given efﬁciency target.\nIn order to train a transformer to cope with a di-\nverse set of computational budgets in the inference\ntime, we propose to train once while reducing the\nsequence length with a random proportion at each\nlayer. We refer to this procedure as LengthDrop,\nwhich was motivated by the nested dropout (Rip-\npel et al., 2014). We can extract sub-models of\nshared weights with any length conﬁguration with-\nout requiring extra post-processing nor additional\nﬁne-tuning.\nIt is not trivial to ﬁnd an optimal length con-\nﬁguration given the inference-time computational\nbudget, although it is extremely important in order\nto deploy these large-scale transformers in practice.\nOnce a transformer is trained with the proposed\nLengthDrop, we search for the length conﬁguration\nthat maximizes the accuracy given a computational\nbudget. Because this search is combinatorial and\nhas multiple objectives (accuracy and efﬁciency),\n6502\nin this work, we propose to use an evolutionary\nsearch algorithm, which further allows us to ob-\ntain a full Pareto frontier of accuracy-efﬁciency\ntrade-off of each model.\nPoWER-BERT, which forms the foundation of\nthe proposed two-stage procedure, is only appli-\ncable to sequence-level classiﬁcation, because it\neliminates some of the word vectors at each layer\nby design. In other words, it cannot be used for\ntoken-level tasks such as span-based question an-\nswering (Rajpurkar et al., 2016) because these tasks\nrequire hidden representations of the entire input se-\nquence at the ﬁnal layer. We thus propose to extend\nPoWER-BERT with a novel Drop-and-Restore pro-\ncess (§3.3), which eliminates this inherent limita-\ntion. Word vectors are dropped and set aside, rather\nthan eliminated, in intermediate layers to maintain\nthe saving of computational cost, as was with the\noriginal PoWER-BERT. These set-aside vectors are\nthen restored at the ﬁnal hidden layer and provided\nas an input to a subsequent task-speciﬁc layer, un-\nlike the original PoWER-BERT.\nThe main contributions of this work are two-\nfold. First, we introduce LengthDrop, a structured\nvariant of dropout for training a single Length-\nAdaptive Transformer model that allows us to au-\ntomatically derive multiple sub-models with dif-\nferent length conﬁgurations in the inference time\nusing evolutionary search, without requiring any\nre-training. Second, we design Drop-and-Restore\nprocess that makes PoWER-BERT applicable be-\nyond classiﬁcation, which enables PoWER-BERT\nto be applicable to a wider range of NLP tasks such\nas span-based question answering. We empirically\nverify Length-Adaptive Transformer works quite\nwell using the variants of BERT on a diverse set of\nNLP tasks, including SQuAD 1.1 (Rajpurkar et al.,\n2016) and two sequence-level classiﬁcation tasks\nin GLUE benchmark (Wang et al., 2018). Our ex-\nperiments reveal that the proposed approach grants\nus ﬁne-grained control of computational efﬁciency\nand a superior accuracy-efﬁciency trade-off in the\ninference time compared to existing approaches.\n2 Background\nIn this section, we review some of the building\nblocks of our main approach. In particular, we re-\nview transformers, which are a standard backbone\nused in natural language processing these days, and\nPoWER-BERT, which was recently proposed as an\neffective way to train a large-scale, but highly efﬁ-\ncient transformer for sequence-level classiﬁcation.\n2.1 Transformers and BERT\nA transformer is a particular neural network that\nhas been designed to work with a variable-length\nsequence input and is implemented as a stack of\nself-attention and fully connected layers (Vaswani\net al., 2017). It has recently become one of the\nmost widely used models for natural language pro-\ncessing. Here, we give a brief overview of the\ntransformer which is the basic building block of\nthe proposed approach.\nEach token xt in a sequence of tokens x =\n(x1, . . . , xN ), representing input text, is ﬁrst turned\ninto a continuous vector h0\nt ∈RH which is the\nsum of the token and position embedding vec-\ntors. This sequence is fed into the ﬁrst transformer\nlayer which returns another sequence of the same\nlength h1 ∈RN×H. We repeat this procedure L\ntimes, for a transformer with L layers, to obtain\nhL = (hL\n1 , . . . , hL\nN ). We refer to each vector in the\nhidden sequence at each layer as a word vector to\nemphasize that there exists a correspondence be-\ntween each such vector and one of the input words.\nAlthough the transformer was ﬁrst introduced\nfor the problem of machine translation, Devlin\net al. (2018) demonstrated that the transformer can\nbe trained and used as a sentence encoder. More\nspeciﬁcally, Devlin et al. (2018) showed that the\ntransformer-based masked language model, called\nBERT, learns a universally useful parameter set that\ncan be ﬁne-tuned for any downstream task, includ-\ning sequence-level and token-level classiﬁcation.\nIn the case of sequence-level classiﬁcation, a\nsoftmax classiﬁer is attached to the word vector hL\n1\nassociated with the special token [CLS], and the\nentire network, including the softmax classiﬁer and\nBERT, is ﬁne-tuned. For token-level classiﬁcation,\nwe use each hL\nt as the ﬁnal hidden representation of\nthe associated t-th word in the input sequence. This\nstrategy of pre-training followed by ﬁne-tuning, of-\nten referred to as transfer learning, is a dominant\napproach to classiﬁcation in natural language pro-\ncessing.\n2.2 PoWER-BERT\nPoWER-BERT keeps only the topmost lj word\nvectors at each layer j by eliminating redundant\nones based on the signiﬁcance score which is the\ntotal amount of attention imposed by a word on the\nother words (Goyal et al., 2020). lj is the hyper-\nparameter that determines how many vectors to\n6503\nkeep at layer j. PoWER-BERT has the same model\nparameters as BERT, but the extraction layers are\ninterspersed after the self-attention layer in every\ntransformer block (Vaswani et al., 2017).\nPoWER-BERT reduces inference time success-\nfully, achieving better accuracy-time trade-off than\nDistilBERT (Sanh et al., 2019), BERT-PKD (Sun\net al., 2019), and Head-Prune (Michel et al., 2019).\nDespite the original intention of maximizing the\ninference efﬁciency with the minimal loss in accu-\nracy, it is possible to set up PoWER-BERT to be\nboth more efﬁcient and more accurate compared to\nthe original BERT, which was observed but largely\noverlooked by Goyal et al. (2020).\nTraining a PoWER-BERT model consists of\nthree steps: (1) ﬁne-tuning, (2) length conﬁgura-\ntion search, and (3) re-training. The ﬁne-tuning\nstep is just like the standard ﬁne-tuning step of\nBERT given a target task. A length conﬁguration\nis a sequence of retention parameters (l1, ···lL),\neach of which corresponds to the number of word\nvectors that are kept at each layer. These retention\nparameters are learned along with all the other pa-\nrameters to minimize the original task loss together\nwith an extra term that approximately measures the\nnumber of retained word vectors across layers. In\nthe re-training step, PoWER-BERT is ﬁne-tuned\nwith the length conﬁguration ﬁxed to its learned\none.\nFor each computational budget, we must train\na separate model going through all three steps de-\nscribed above. Moreover, the length conﬁguration\nsearch step above is only approximate, as it relies\non the relaxation of retention parameters which are\ninherently discrete. This leads to the lack of guar-\nanteed correlation between the success of this stage\nand true run-time. Even worse, it is a delicate act\nto tune the length conﬁguration given a target com-\nputational budget because the trade-off isimplicitly\nmade via a regularization coefﬁcient. Furthermore,\nPoWER-BERT has an inherent limitation in that\nit only applies to sequence-level classiﬁcation be-\ncause it eliminates word vectors in intermediate\nlayers.\n3 Length-Adaptive Transformer\nIn this section, we explain our proposed frame-\nwork which results in a transformer that reduces\nthe length of a sequence at each layer with an ar-\nbitrary rate. We call such a resulting transformer\na Length-Adaptive Transformer. We train Length-\nAdaptive Transformer with LengthDrop which ran-\ndomly samples the number of hidden vectors to\nbe dropped at each layer with the goal of making\nthe ﬁnal model robust to such drop in the infer-\nence time. Once the model is trained, we search\nfor the optimal trade-off between accuracy and ef-\nﬁciency using multi-objective evolutionary search,\nwhich allows us to use the model for any given\ncomputational budget without ﬁne-tuning nor re-\ntraining. At the end of this section, we describe\nDrop-and-Restore process as a way to greatly in-\ncrease the applicability of PoWER-BERT which\nforms a building block of the proposed framework.\nIn short, we train a Length-Adaptive Trans-\nformer once with LengthDrop and Drop-and-\nRestore, and use it with an automatically deter-\nmined length conﬁguration for inference with any\ntarget computational budget, on both sequence-\nlevel and token-level tasks.\n3.1 LengthDrop\nEarlier approaches to efﬁcient inference with trans-\nformers have focused on a scenario where the target\ncomputational budget for inference is known in ad-\nvance (Sanh et al., 2019; Goyal et al., 2020). This\ngreatly increases the cost of deploying transform-\ners, as it requires us to train a separate transformer\nfor each scenario. Instead, we propose to train one\nmodel that could be used for a diverse set of target\ncomputational budgets without re-training.\nBefore each SGD update, LengthDrop ran-\ndomly generates a length conﬁguration by se-\nquentially sampling a sequence length li+1 at the\n(i + 1)-th layer based on the previous layer’s se-\nquence length li, following the uniform distribu-\ntion U((1 −p)li, li), where l0 is set to the length\nof the input sequence, and p is the LengthDrop\nprobability. This sequential sampling results in a\nlength conﬁguration (l1, ···, lL). Length-Adaptive\nTransformer can be thought of as consisting of a\nfull model and many sub-models corresponding to\ndifferent length conﬁgurations, similarly to a neu-\nral network trained with different dropout masks\n(Srivastava et al., 2014).\nLayerDrop From the perspective of each word\nvector, the proposed LengthDrop could be thought\nof as skipping the layers between when it was set\naside and the ﬁnal layer where it was restored. The\nword vector however does not have any informa-\ntion based on which it can determine whether it\nwould be dropped at any particular layer. In our\n6504\npreliminary experiments, we found that this greatly\nhinders optimization. We address this issue by us-\ning LayerDrop (Fan et al., 2019) which skips each\nlayer of a transformer uniformly at random. The\nLayerDrop encourages each word vector to be ag-\nnostic to skipping any number of layers between\nwhen it is dropped and when it is restored, just\nlike dropout (Srivastava et al., 2014) prevents hid-\nden neurons from co-adapting with each other by\nrandomly dropping them.\nSandwich Rule and Inplace Distillation We\nobserved that standard supervised training with\nLengthDrop does not work well in the preliminary\nexperiments. We instead borrow a pair of train-\ning techniques developed by Yu and Huang (2019)\nwhich are sandwich rule and inplace distillation,\nfor better optimization as well as ﬁnal generaliza-\ntion. At each update, we update the full model\nwithout LengthDrop as usual to minimize the super-\nvised loss function. We simultaneously update ns\nrandomly-sampled sub-models (which are called\nsandwiches) and the smallest-possible sub-model,\nwhich corresponds to keeping only (1 −p)li word\nvectors at each layer i, using knowledge distilla-\ntion (Hinton et al., 2015) from the full model. Here,\nsub-models mean models with length reduction.\nThey are trained to their prediction close to the full\nmodel’s prediction (inplace distillation).\n3.2 Evolutionary Search of Length\nConﬁgurations\nAfter training a Length-Adaptive Transformer with\nLengthDrop, we search for appropriate length con-\nﬁgurations for possible target computational bud-\ngets that will be given at inference time. The length\nconﬁguration determines the model performance\nin terms of both accuracy and efﬁciency. In order\nto search for the optimal length conﬁguration, we\npropose to use evolutionary search, similarly to\nCai et al. (2019) and Wang et al. (2020a). This\nprocedure is efﬁcient, as it only requires a single\npass through the relatively small validation set for\neach length conﬁguration, unlike re-training for a\nnew computational budget which requires multiple\npasses through a signiﬁcantly larger training set for\neach budget.\nWe initialize the population with constant-ratio\nconﬁgurations. Each conﬁguration is created by\nli+1 = (1−r)li for each layer i with r so that the\namount of computation within the initial population\nis uniformly distributed between those of the small-\nest and full models. At each iteration, we evolve\nthe population to consist only of conﬁgurations\nlie on a newly updated efﬁciency-accuracy Pareto\nfrontier by mutation and crossover. Mutation al-\nters an original length conﬁguration (l1, ···, lL)\nto (l′\n1, ···, l′\nL) by sampling l′\ni from the uniform\ndistribution U(l′\ni−1, li+1) with the probability pm\nor keeping the original length l′\ni = li, sweeping\nthe layers from i = 1 to i = L. A crossover\ntakes two length conﬁgurations and averages the\nlengths at each layer. Both of these operations are\nperformed while ensuring the monotonicity of the\nlengths over the layers. We repeat this iteration\nG times while maintaining nm mutated conﬁgura-\ntions and nc crossover’d conﬁgurations. Repeating\nthis procedure pushes the Pareto frontier further\nto identify the best trade-off between two objec-\ntives, efﬁciency and accuracy, without requiring\nany continuous relaxation of length conﬁgurations\nnor using a proxy objective function.\n3.3 Drop-and-Restore Process\nThe applicability of the PoWER-BERT, based on\nwhich our main contribution above was made, is\nlimited to sequence-level classiﬁcation because it\neliminates word vectors at each layer. In addition\nto our main contribution above, we thus propose to\nextend the PoWER-BERT so that it is applicable\nto token-level classiﬁcation, such as span-based\nquestion-answering. Our proposal, to which we re-\nfer as Drop-and-Restore, does not eliminate word\nvectors at each layer according to the length con-\nﬁguration but instead sets them aside until the ﬁnal\nhidden layer. At the ﬁnal hidden layer, these word\nvectors are brought back to form the full hidden\nsequence, as illustrated graphically in Figure 1.\n4 Experiment Setup\nDatasets We test the proposed approach on both\nsequence-level and token-level tasks, the latter of\nwhich could not have been done with the original\nPoWER-BERT unless for the proposed Drop-and-\nRestore. We use MNLI-m and SST-2 from GLUE\nbenchmark (Wang et al., 2018), as was done to test\nPoWER-BERT earlier, for sequence-level classi-\nﬁcation. We choose them because consistent ac-\ncuracy scores from standard training on them due\nto their sufﬁciently large training set imply that\nthey are reliable to verify our approach. We use\nSQuAD 1.1 (Rajpurkar et al., 2016) for token-level\nclassiﬁcation.\n6505\n(a) PoWER\n (b) Drop-and-Restore\nFigure 1: Illustration of (a) word-vector elimination process in PoWER-BERT (Goyal et al., 2020) and (b) Drop-\nand-Restore process in Length-Adaptive Transformer. Yellow box and blue boxes imply the output of embedding\nlayer and transformer layers, respectively. Green boxes mean vectors dropped in lower layers and restored at the\nlast layer. Red box is the task-speciﬁc layer. Though word-vectors in the middle could be eliminated (or dropped),\nremaining vectors are left-aligned for the better illustration. In this case, the number of transformer layers is four.\nEvaluation metrics We use the number of ﬂoat-\ning operations (FLOPs) as a main metric to mea-\nsure the inference efﬁciency given any length con-\nﬁguration, as it is agnostic to the choice of the\nunderlying hardware, unlike other alternatives such\nas hardware-aware latency (Wang et al., 2020a)\nor energy consumption (Henderson et al., 2020).\nWe later demonstrate that FLOPs and wall-clock\ntime on GPU and CPU correlate well with the pro-\nposed approach, which is not necessarily the case\nfor other approaches, such as unstructured weight\npruning (Han et al., 2015; See et al., 2016).\nPre-trained transformers Since BERT was in-\ntroduced by Devlin et al. (2018), it has become\na standard practice to start from a pre-trained\n(masked) language model and ﬁne-tune it for each\ndownstream task. We follow the same strategy\nin this paper and test two pre-trained transformer-\nbased language models; BERTBase (Devlin et al.,\n2018) and DistilBERT (Sanh et al., 2019), which\nallows us to demonstrate that the usefulness and\napplicability of our approach are not tied to any\nspeciﬁc architectural choice, such as the number of\nlayers and the maximum input sequence length. Al-\nthough we focus on BERT-based masked language\nmodels here, the proposed approach is readily ap-\nplicable to any transformer-based models.\nLearning We train a Length-Adaptive Trans-\nformer with LengthDrop probability and Layer-\nDrop probability both set to 0.2. We use ns = 2\nrandomly sampled intermediate sub-models in ad-\ndition to the full model and smallest model for\napplying the sandwich learning rule.\nWe start ﬁne-tuning the pre-trained transformer\nwithout Drop-and-Restore ﬁrst, just as Goyal et al.\n(2020) did with PoWER-BERT. We then continue\nﬁne-tuning it for another ﬁve epochs with Drop-\nand-Restore. This is unlike the recommended three\nepochs by Devlin et al. (2018), as learning pro-\ngresses slower due to a higher level of stochasticity\nintroduced by LengthDrop and LayerDrop. We use\nthe batch size of 32, the learning rate of 5e −5 for\nSQuAD 1.1 and 2e −5 for MNLI-m and SST, and\nthe maximum sequence length of 384 for SQuAD\n1.1 and 128 for MNLI-m and SST.\nSearch We run up to G = 30iterations of evo-\nlutionary search, using nm = 30mutated conﬁg-\nurations with mutation probability pm = 0.5 and\nnc = 30 crossover’d conﬁgurations, to ﬁnd the\nPareto frontier of accuracy and efﬁciency.\n5 Results and Analysis\nEfﬁciency-accuracy trade-off We use SQuAD\n1.1 to examine the effect of the proposed approach\non the efﬁciency-accuracy trade-off. When the\nunderlying classiﬁer was not trained with Length-\nDrop, as proposed in this paper, the accuracy drops\neven more dramatically as more word vectors are\ndropped at each layer. The difference between stan-\ndard transformer and Length-Adaptive Transformer\nis stark in Figure 2. This veriﬁes the importance\nof training a transformer in a way that makes it\nmalleable for inference-time re-conﬁguration.\nWhen the model was trained with the proposed\nLengthDrop, we notice the efﬁcacy of the proposed\napproach of using evolutionary search to ﬁnd the\noptimal trade-off between inference efﬁciency and\naccuracy. The trade-off curve from the proposed\nsearch strategy has a larger area-under-curve (AUC)\nthan when constant-rate length reduction was used\nto meet a target computational budget. It demon-\nstrates the importance of using both LengthDrop\nand evolutionary search.\nWe make a minor observation that the proposed\napproach ends up with a signiﬁcantly higher accu-\nracy than DistillBERT when enough computational\n6506\nlog FLOPs (G)\nF1\n80\n82\n84\n86\n88\n90\n9 10 20 30\nBERT-Base Length-Adaptive BERT-Base Length-Adaptive BERT-Base ES\nDistilBERT Length-Adaptive DistilBERT Length-Adaptive DistilBERT ES\nFigure 2: Pareto curves of F1 score and FLOPs on SQuAD 1.1 (Rajpurkar et al.,\n2016). We apply the proposed method to BERT Base (solid lines) and DistilBERT\n(dotted lines). For each model, we draw three curves using (1) standard ﬁne-tuned\ntransformer with constant-rate length reduction, (2) Length-Adaptive Transformer\nwith constant-rate length reduction, and (3) Length-Adaptive Transformer with\nlength conﬁgurations obtained from the evolutionary search.\nFLOPs (G)\nLatency (ms/instance)0\n5\n10\n15\n20\n10 15 20 25 30 35\nGPU (1) GPU (4)\nGPU (16) GPU (64)\n(a) GPU\nFLOPs (G)\nLatency (ms/instance)0\n250\n500\n750\n1000\n1250\n10 15 20 25 30 35\nCPU (1) CPU (4)\nCPU (16) CPU (64)\n(b) CPU\nFigure 3: Correlation\nbetween FLOPs and\nlatency with different\nlength conﬁgurations.\nbudget is allowed for inference (log FLOPs > 10).\nThis makes our approach desirable in a wide array\nof scenarios, as it does not require any additional\npre-training stage, as does DistilBERT. With a se-\nvere constraint on the computational budget, the\nproposed approach could be used on DistilBERT to\nsigniﬁcantly improve the efﬁciency without com-\npromising the accuracy.\nMaximizing inference efﬁciency We consider\nall three tasks, SQuAD 1.1, MNLI-m, and SST-2,\nand investigate how much efﬁciency can be gained\nby the proposed approach with minimal sacriﬁce\nof accuracy. First, we look at how much efﬁciency\ncould be gained without losing accuracy. That is,\nwe use the length conﬁguration that maximizes\nthe inference efﬁciency (i.e., minimize the FLOPs)\nwhile ensuring that the accuracy is above or the\nsame as the accuracy of the standard approach with-\nout any drop of word vectors. The results are pre-\nsented in the rows marked with Length-Adaptive†\nfrom Table 1. For example, in the case of BERTBase,\nthe proposed approach reduces FLOPs by more\nthan half across all three tasks.\nFrom Figure 2, we have observed that the pro-\nposed Length-Adaptive Transformer generalizes\nbetter than the standard, base model in some cases.\nThus, we try to maximize both the inference ef-\nﬁciency and accuracy in order to see whether it\nis possible for the proposed algorithm to ﬁnd a\nlength conﬁguration that both maximizes inference\nefﬁciency and improves accuracy. We present the\nresults in the rows marked with Length-Adaptive⋆\nfrom Table 1. For all cases, Length-Adaptive Trans-\nformer achieves higher accuracy than a standard\ntransformer does while reducing FLOPs signiﬁ-\ncantly. Although it is not apparent from the table,\ntor MNLI-m and SST-2, the accuracy of the small-\nest sub-model is already greater than or equal to\nthat of a standard transformer.\nFLOPs vs. Latency As has been discussed in\nrecent literature (see, e.g., Li et al. (2020); Chin\net al. (2020)), the number of FLOPs is not a perfect\nindicator of the real latency measured in wall-clock\ntime, as the latter is affected by the combination of\nhardware choice and network architecture. To un-\nderstand the real-world impact of the proposed ap-\nproach, we study the relationship between FLOPs,\nobtained by the proposed procedure, and wall-clock\ntime measured on both CPU and GPU by measur-\ning them while varying length conﬁgurations. As\nshown in Figure 3, FLOPs and latency exhibit near-\nlinear correlation on GPU, when the minibatch size\nis ≥16, and regardless of the minibatch size, on\nCPU. In other words, the reduction in FLOPs with\n6507\nModel SQuAD 1.1 MNLI-m SST-2\nPretrained\nTransformer Method F1 FLOPs Acc FLOPs Acc FLOPs\nBERTBase\nStandard 88.5 1.00x 84.4 1.00x 92.8 1.00x\nLength-Adaptive⋆ 89.6 0.89x 85.0 0.58x 93.1 0.36x\nLength-Adaptive† 88.7 0.45x 84.4 0.35x 92.8 0.35x\nDistilBERT\nStandard 85.8 1.00x 80.9 1.00x 90.6 1.00x\nLength-Adaptive⋆ 86.3 0.81x 81.5 0.56x 92.0 0.55x\nLength-Adaptive† 85.9 0.59x 81.3 0.54x 91.7 0.54x\nTable 1: Comparison results of standard Transformer and Length-Adaptive Trans-\nformer. Among length conﬁgurations on the Pareto frontier of Length-Adaptive\nTransformer, we pick two representative points: Length-Adaptive ⋆ and Length-\nAdaptive† as the most efﬁcient one while having the highest accuracy and the ac-\ncuracy higher than (or equal to) standard Transformer, respectively.\nIteration\nAUC\n88.7\n88.8\n88.9\n89.0\n89.1\n89.2\n0 10 20 30\nFigure 4: Example\nof area under Pareto\ncurve as the evolutionary\nsearch of length conﬁgu-\nrations proceeds.\nthe proposed approach directly implies the reduc-\ntion in wall-clock time.\nConvergence of search Although the proposed\napproach is efﬁcient in that it requires only one\nround of training, it needs a separate search stage\nfor each target budget. It is important for evolution-\nary search to converge quickly in the number of\nforward sweeps of a validation set. As exempliﬁed\nin Figure 4, evolutionary search converges after\nabout ﬁfteen iterations.\n6 Comparison with Other Works\nOur framework allows a novel method for anytime\nprediction with adaptive sequence length given any\ntransformers. Thus, our goal is not state-of-the-art\nclassiﬁcation accuracy, although our experimen-\ntal results (§5) demonstrate that our method still\nattains a good accuracy level.\nWe emphasize that other adaptive computation\nworks ( §7) are orthogonal with ours, meaning\nthat various adaptive dimensions (sequence length,\ndepth, attention head, hidden dimension, etc.) can\nbe jointly used. In other words, even if other adap-\ntive methods show better curves than ours, our\nmethod and theirs can boost each other when com-\nbined. We provide some comparison results with\nPoWER-BERT (not anytime prediction method)\nand DynaBERT (Hou et al., 2020) (concurrent\nadaptive computation method) as follows.\nComparison with PoWER-BERT According\nto Goyal et al. (2020), PoWER-BERT achieves\n2.6x speedup for MNLI-m and 2.4x speedup for\nSST-2 by losing 1% of their accuracy. Length-\nAdaptive Transformer obtains a 2.9x speedup in\nterms of FLOPs without losing accuracy on MNLI-\nm and SST-2. Considering Figure 3, our speedup in\nexecution time would be close to 2.9x in the same\nsetting of PoWER-BERT where the time measure-\nment is done with a batch size of 128 on GPU. It\nindicates that our model offers a better trade-off\nthan PoWER-BERT, even with a single model.\nComparison with DynaBERT According to\nHou et al. (2020), DyanBERT obtains a gain of\n+1.0, +0.1, +0.4 for the best accuracy in SQuAD\n1.1, MNLI-m, and SST-2, respectively, while\nLength-Adaptive Transformer achieves a gain of\n+1.1, +0.6, +0.3. These results imply that Length-\nAdaptive Transformer can give a comparable (or\nbetter) performance with DynaBERT.\n7 Related Work\nThe main purpose of the proposed algorithm is to\nimprove the inference efﬁciency of a large-scale\ntransformer. This goal has been pursued from\nvarious directions, and here we provide a brief\noverview of these earlier and some concurrent at-\ntempts in the context of the proposed approach.\nWeight pruning Weight pruning (Han et al.,\n2015) focuses on reducing the number of parame-\nters that directly reﬂects the memory footprint of\na model and indirectly correlates with inference\nspeed. However, their actual speed-up in runtime\nis usually not signiﬁcant, especially while execut-\ning a model with parallel computation using GPU\ndevices (Tang et al., 2018; Li et al., 2020).\nAdaptive architecture There are three major\naxes along which computation can be reduced in\na neural network; (1) input size/length, (2) net-\nwork depth, and (3) network width. The proposed\napproach, based on PoWER-BERT, adaptively re-\nduces the input length as the input sequence is pro-\n6508\ncessed by the transformer layers. In our knowledge,\nGoyal et al. (2020) is the ﬁrst work in this direction\nfor transformers. Funnel-Transformer (Dai et al.,\n2020) and multi-scale transformer language mod-\nels (Subramanian et al., 2020) also successfully\nreduce sequence length in the middle and rescale\nto full length for the ﬁnal computation. However,\ntheir inference complexity is ﬁxed differently with\nPoWER-BERT because they are not designed to\ncontrol efﬁciency. More recently, TR-BERT (Ye\net al., 2021) introduces a policy network trained\nvia reinforcement learning to decide which vectors\nto skip.\nLayerDrop (Fan et al., 2019) drops random lay-\ners during the training to be robust to pruning in-\nspired by Huang et al. (2016). Word-level adaptive\ndepth in Elbayad et al. (2019) and Liu et al. (2020b)\nmight seemingly resemble with length reduction,\nbut word vectors that reached the maximal layer\nare used for self-attention computation without up-\ndating themselves. Escaping a network early (Teer-\napittayanon et al., 2016; Huang et al., 2017) based\non the conﬁdence of the prediction (Xin et al., 2020,\n2021; Schwartz et al., 2020; Liu et al., 2020a; Li\net al., 2021) also offers a control over accuracy-\nefﬁciency trade-off by changing a threshold, but it\nis difﬁcult to tune a threshold for a desired computa-\ntional budget because of the example-wise adaptive\ncomputation.\nSlimmable neural networks (Yu et al., 2018; Lee\nand Shin, 2018) reduce the hidden dimension for\nthe any-time prediction. DynaBERT (Hou et al.,\n2020) can run at adaptive width (the number of at-\ntention heads and intermediate hidden dimension)\nand depth. Hardware-aware Transformers (Wang\net al., 2020a) construct a design space with arbi-\ntrary encoder-decoder attention and heterogeneous\nlayers in terms of different numbers of layers, at-\ntention heads, hidden dimension, and embedding\ndimension. SpAtten (Wang et al., 2020b) performs\ncascade token and head pruning for an efﬁcient\nalgorithm-architecture co-design.\nStructured dropout A major innovation we in-\ntroduce over the existing PoWER-BERT is the use\nof stochastic, structured regularization to make a\ntransformer robust to the choice of length conﬁg-\nuration in the inference time. Rippel et al. (2014)\nproposes a nested dropout to learn ordered repre-\nsentations. Similar to LengthDrop, it samples an\nindex from a prior distribution and drops all units\nwith a larger index than the sampled one.\nSearch There have been a series of attempts at\nﬁnding the optimal network conﬁguration by solv-\ning a combinatorial optimization problem. In com-\nputer vision, Once-for-All (Cai et al., 2019) use an\nevolutionary search (Real et al., 2019) to ﬁnd a bet-\nter conﬁguration in dimensions of depth, width, ker-\nnel size, and resolution given computational budget.\nSimilarly but differently, our evolutionary search\nis mutli-objective to ﬁnd length conﬁgurations on\nthe Pareto accuracy-efﬁciency frontier to cope with\nany possible computational budgets. Moreover, we\nonly change the sequence length of hidden vectors\ninstead of architectural model size like dimensions.\nSequence Length Shortformer (Press et al.,\n2020) initially trained on shorter subsequences and\nthen moved to longer ones achieves improved per-\nplexity than a standard transformer with normal\ntraining while reducing overall training time. Novel\narchitectures with the efﬁcient attention mechanism\n(Kitaev et al., 2020; Beltagy et al., 2020; Zaheer\net al., 2020; Ainslie et al., 2020; Choromanski et al.,\n2020; Peng et al., 2021) are suggested to reduce the\ntransformer’s quadratic computational complexity\nin the input sequence length. Tay et al. (2020b)\nand Tay et al. (2020a) provide a survey of these\nefﬁcient transformers and their benchmark compar-\nison, respectively.\n8 Conclusion and Future Work\nIn this work, we propose a new framework for train-\ning a transformer once and using it for efﬁcient\ninference under any computational budget. With\nthe help of training with LengthDrop and Drop-\nand-Restore process followed by the evolutionary\nsearch, our proposed Length-Adaptive Transformer\nallows any given transformer models to be used\nwith any inference-time computational budget for\nboth sequence-level and token-level classiﬁcation\ntasks. Our experiments, on SQuAD 1.1, MNLI-\nm and SST-2, have revealed that the proposed al-\ngorithmic framework signiﬁcantly pushes a better\nPareto frontier on the trade-off between inference\nefﬁciency and accuracy. Furthermore, we have\nobserved that the proposed Length-Adaptive Trans-\nformer could achieve up to 3x speed-up over the\nstandard transformer without sacriﬁcing accuracy,\nboth in terms of FLOPs and wallclock time.\nAlthough our approach ﬁnds an optimal length\nconﬁguration of a trained classiﬁer per computa-\ntional budget, it leaves a open question whether\nthe proposed approach could be further extended\n6509\nto support per-instance length conﬁguration by for\ninstance training a small, auxiliary neural network\nfor each computational budget. Yet another aspect\nwe have not investigated in this paper is the ap-\nplicability of the proposed approach to sequence\ngeneration, such as machine translation. We leave\nboth of these research directions for the future.\nOur approach is effective, as we have shown in\nthis paper, and also quite simple to implement on\ntop of existing language models. We release our im-\nplementation at https://github.com/clovaai/length-\nadaptive-transformer, which is based on Hugging-\nFace’sTransformers library (Wolf et al., 2019), and\nplan to adapt it for a broader set of transformer-\nbased models and downstream tasks, including\nother modalities (Dosovitskiy et al., 2020; Touvron\net al., 2020; Gulati et al., 2020).\nAcknowledgments\nThe authors appreciate Clova AI members and the\nanonymous reviewers for their constructive feed-\nback. Speciﬁcally, Dongyoon Han and Byeongho\nHeo introduced relevant works and gave insights\nfrom the view of the computer vision community.\nWe use Naver Smart Machine Learning (Sung et al.,\n2017; Kim et al., 2018) platform for the experi-\nments.\nReferences\nJoshua Ainslie, Santiago Ontan ´on, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. Etc: Encoding long and structured inputs in\ntransformers. arXiv preprint arXiv:2004.08483.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nHan Cai, Chuang Gan, and Song Han. 2019. Once for\nall: Train one network and specialize it for efﬁcient\ndeployment. arXiv preprint arXiv:1908.09791.\nQingqing Cao, Aruna Balasubramanian, and Niranjan\nBalasubramanian. 2020. Towards accurate and re-\nliable energy measurement of nlp models. arXiv\npreprint arXiv:2010.05248.\nTing-Wu Chin, Ruizhou Ding, Cha Zhang, and Diana\nMarculescu. 2020. Towards efﬁcient model com-\npression via learned global ranking. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 1518–1528.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2020. Rethinking attention\nwith performers. arXiv preprint arXiv:2009.14794.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc V Le.\n2020. Funnel-transformer: Filtering out sequential\nredundancy for efﬁcient language processing. arXiv\npreprint arXiv:2006.03236.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2019. Depth-adaptive transformer. arXiv\npreprint arXiv:1910.10073.\nAngela Fan, Edouard Grave, and Armand Joulin. 2019.\nReducing transformer depth on demand with struc-\ntured dropout. arXiv preprint arXiv:1909.11556.\nSaurabh Goyal, Anamitra Roy Choudhury, Saurabh\nRaje, Venkatesan Chakaravarthy, Yogish Sabharwal,\nand Ashish Verma. 2020. Power-bert: Accelerat-\ning bert inference via progressive word-vector elim-\nination. In International Conference on Machine\nLearning, pages 3690–3699. PMLR.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, et al.\n2020. Conformer: Convolution-augmented trans-\nformer for speech recognition. arXiv preprint\narXiv:2005.08100.\nSong Han, Huizi Mao, and William J Dally. 2015.\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huff-\nman coding. arXiv preprint arXiv:1510.00149.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nPeter Henderson, Jieru Hu, Joshua Romoff, Emma\nBrunskill, Dan Jurafsky, and Joelle Pineau. 2020.\nTowards the systematic reporting of the energy\nand carbon footprints of machine learning. arXiv\npreprint arXiv:2002.05651.\n6510\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nLu Hou, Lifeng Shang, Xin Jiang, and Qun Liu. 2020.\nDynabert: Dynamic bert with adaptive width and\ndepth. arXiv preprint arXiv:2004.04037.\nGao Huang, Danlu Chen, Tianhong Li, Felix Wu,\nLaurens van der Maaten, and Kilian Q Wein-\nberger. 2017. Multi-scale dense networks for re-\nsource efﬁcient image classiﬁcation. arXiv preprint\narXiv:1703.09844.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and\nKilian Q Weinberger. 2016. Deep networks with\nstochastic depth. In European conference on com-\nputer vision, pages 646–661. Springer.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nHanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong\nKim, Heungseok Park, Soeun Park, Hyunwoo Jo,\nKyungHyun Kim, Youngil Yang, Youngkwan Kim,\net al. 2018. Nsml: Meet the mlaas platform\nwith a real-world case study. arXiv preprint\narXiv:1810.09957.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. arXiv\npreprint arXiv:2001.04451.\nHankook Lee and Jinwoo Shin. 2018. Anytime neu-\nral prediction via slicing networks vertically. arXiv\npreprint arXiv:1807.02609.\nXiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan,\nXipeng Qiu, and Xuanjing Huang. 2021. Acceler-\nating bert inference for sequence labeling via early-\nexit. arXiv preprint arXiv:2105.13878.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joseph E Gonzalez.\n2020. Train large, then compress: Rethinking model\nsize for efﬁcient training and inference of transform-\ners. arXiv preprint arXiv:2002.11794.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang,\nHaotang Deng, and Qi Ju. 2020a. Fastbert: a self-\ndistilling bert with adaptive inference time. arXiv\npreprint arXiv:2004.02178.\nYijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen,\nand Jinan Xu. 2020b. Explicitly modeling adap-\ntive depths for transformer. arXiv preprint\narXiv:2004.13542.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems ,\npages 14014–14024.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah A Smith, and Lingpeng Kong.\n2021. Random feature attention. arXiv preprint\narXiv:2103.02143.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nOﬁr Press, Noah A Smith, and Mike Lewis. 2020.\nShortformer: Better language modeling using\nshorter inputs. arXiv preprint arXiv:2012.15832.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nEsteban Real, Alok Aggarwal, Yanping Huang, and\nQuoc V Le. 2019. Regularized evolution for image\nclassiﬁer architecture search. In Proceedings of the\naaai conference on artiﬁcial intelligence, volume 33,\npages 4780–4789.\nOren Rippel, Michael Gelbart, and Ryan Adams.\n2014. Learning ordered representations with nested\ndropout. In International Conference on Machine\nLearning, pages 1746–1754.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and\nOren Etzioni. 2019. Green ai. arXiv preprint\narXiv:1907.10597.\nRoy Schwartz, Gabi Stanovsky, Swabha Swayamdipta,\nJesse Dodge, and Noah A Smith. 2020. The right\ntool for the job: Matching model and instance com-\nplexities. arXiv preprint arXiv:2004.07453.\nAbigail See, Minh-Thang Luong, and Christopher D\nManning. 2016. Compression of neural machine\ntranslation models via pruning. arXiv preprint\narXiv:1606.09274.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using gpu model paral-\nlelism. arXiv preprint arXiv:1909.08053.\n6511\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nEmma Strubell, Ananya Ganesh, and Andrew Mc-\nCallum. 2019. Energy and policy considera-\ntions for deep learning in nlp. arXiv preprint\narXiv:1906.02243.\nSandeep Subramanian, Ronan Collobert, Marc’Aurelio\nRanzato, and Y-Lan Boureau. 2020. Multi-scale\ntransformer language models. arXiv preprint\narXiv:2005.00581.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nNako Sung, Minkyu Kim, Hyunwoo Jo, Youngil Yang,\nJingwoong Kim, Leonard Lausen, Youngkwan Kim,\nGayoung Lee, Donghyun Kwak, Jung-Woo Ha, et al.\n2017. Nsml: A machine learning platform that en-\nables you to focus on your models. arXiv preprint\narXiv:1712.05902.\nRaphael Tang, Ashutosh Adhikari, and Jimmy Lin.\n2018. Flops as a direct optimization objective for\nlearning sparse neural networks. arXiv preprint\narXiv:1811.03060.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang\nShen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. 2020a.\nLong range arena: A benchmark for efﬁcient trans-\nformers. arXiv preprint arXiv:2011.04006.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020b. Efﬁcient transformers: A survey.\narXiv preprint arXiv:2009.06732.\nSurat Teerapittayanon, Bradley McDanel, and Hsiang-\nTsung Kung. 2016. Branchynet: Fast inference via\nearly exiting from deep neural networks. In 2016\n23rd International Conference on Pattern Recogni-\ntion (ICPR), pages 2464–2469. IEEE.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. 2020. Training data-efﬁcient image trans-\nformers & distillation through attention. arXiv\npreprint arXiv:2012.12877.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nHanrui Wang, Zhanghao Wu, Zhijian Liu, Han\nCai, Ligeng Zhu, Chuang Gan, and Song Han.\n2020a. Hat: Hardware-aware transformers for ef-\nﬁcient natural language processing. arXiv preprint\narXiv:2005.14187.\nHanrui Wang, Zhekai Zhang, and Song Han. 2020b.\nSpatten: Efﬁcient sparse attention architecture with\ncascade token and head pruning. arXiv preprint\narXiv:2012.09852.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Fun-\ntowicz, et al. 2019. Transformers: State-of-the-\nart natural language processing. arXiv preprint\narXiv:1910.03771.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. Deebert: Dynamic early exit-\ning for accelerating bert inference. arXiv preprint\narXiv:2004.12993.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. Berxit: Early exiting for bert with better\nﬁne-tuning and extension to regression. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 91–104.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nDeming Ye, Yankai Lin, Yufei Huang, and Maosong\nSun. 2021. Tr-bert: Dynamic token reduction\nfor accelerating bert inference. arXiv preprint\narXiv:2105.11618.\nJiahui Yu and Thomas S Huang. 2019. Universally\nslimmable networks and improved training tech-\nniques. In Proceedings of the IEEE International\nConference on Computer Vision, pages 1803–1811.\nJiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and\nThomas Huang. 2018. Slimmable neural networks.\narXiv preprint arXiv:1812.08928.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. arXiv preprint arXiv:2007.14062.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.596228837966919
    },
    {
      "name": "Transformer",
      "score": 0.5225950479507446
    },
    {
      "name": "Drop (telecommunication)",
      "score": 0.4668968617916107
    },
    {
      "name": "Natural language processing",
      "score": 0.34479087591171265
    },
    {
      "name": "Speech recognition",
      "score": 0.3394439220428467
    },
    {
      "name": "Engineering",
      "score": 0.21533381938934326
    },
    {
      "name": "Electrical engineering",
      "score": 0.21190056204795837
    },
    {
      "name": "Telecommunications",
      "score": 0.21096277236938477
    },
    {
      "name": "Voltage",
      "score": 0.08283761143684387
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60922564",
      "name": "Naver (South Korea)",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 52
}