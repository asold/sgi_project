{
    "title": "Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems",
    "url": "https://openalex.org/W3094083263",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5102375916",
            "name": "Yanjie Gou",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A5102831936",
            "name": "Yinjie Lei",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A5070976480",
            "name": "Lingqiao Liu",
            "affiliations": [
                "The University of Adelaide"
            ]
        },
        {
            "id": "https://openalex.org/A5046017342",
            "name": "Yong Dai",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A5102314566",
            "name": "Chunxu Shen",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2808093377",
        "https://openalex.org/W2971261034",
        "https://openalex.org/W2963789888",
        "https://openalex.org/W3082549344",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2962886331",
        "https://openalex.org/W2438667436",
        "https://openalex.org/W1975244201",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2964006684",
        "https://openalex.org/W2963797754",
        "https://openalex.org/W3013192639",
        "https://openalex.org/W4297733535",
        "https://openalex.org/W2949141958",
        "https://openalex.org/W3099453223",
        "https://openalex.org/W4295838474",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3035301094",
        "https://openalex.org/W2951008357",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2964077278",
        "https://openalex.org/W3162000275",
        "https://openalex.org/W3017074538",
        "https://openalex.org/W3106274079",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963491014",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W2798914047",
        "https://openalex.org/W2970260827",
        "https://openalex.org/W2891732163",
        "https://openalex.org/W2171837816",
        "https://openalex.org/W3102521862",
        "https://openalex.org/W2963858333"
    ],
    "abstract": "Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue systems is challenging, since it requires to properly represent the entity of KB, which is associated with its KB context and dialogue context. The existing works represent the entity with only perceiving a part of its KB context, which can lead to the less effective representation due to the information loss, and adversely favor KB reasoning and response generation. To tackle this issue, we explore to fully contextualize the entity representation by dynamically perceiving all the relevant entities and dialogue history. To achieve this, we propose a COntext-aware Memory Enhanced Transformer framework (COMET), which treats the KB as a sequence and leverages a novel Memory Mask to enforce the entity to only focus on its relevant entities and dialogue history, while avoiding the distraction from the irrelevant entities. Through extensive experiments, we show that our COMET framework can achieve superior performance over the state of the arts.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4300–4310\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n4300\nContextualize Knowledge Bases with Transformer for End-to-end\nTask-Oriented Dialogue Systems\nYanjie Gou1, Yinjie Lei1∗, Lingqiao Liu2, Yong Dai3, Chunxu Shen4\n1College of Electronics and Information Engineering, Sichuan University, China\n2School of Computer Science, The University of Adelaide, Australia\n3University of Electronic Science and Technology of China, China 4Tencent\ngouyanjie@stu.scu.edu.cn, yinjie@scu.edu.cn, lingqiao.liu@adelaide.edu.au\ndaiyongya@yahoo.com, lineshen@tencent.com\nAbstract\nIncorporating knowledge bases (KB) into end-\nto-end task-oriented dialogue systems is chal-\nlenging, since it requires to properly represent\nthe entity of KB, which is associated with its\nKB context and dialogue context. The existing\nworks represent the entity with only perceiv-\ning a part ofits KB context, which can lead\nto the less effective representation due to the\ninformation loss, and adversely favor KB rea-\nsoning and response generation. To tackle this\nissue, we explore to fully contextualize the\nentity representation by dynamically perceiv-\ning all the relevant entities and dialogue his-\ntory. To achieve this, we propose a COntext-\naware Memory Enhanced Transformer frame-\nwork (COMET), which treats the KB as a se-\nquence and leverages a novel Memory Mask\nto enforce the entity to only focus on its rele-\nvant entities and dialogue history, while avoid-\ning the distraction from the irrelevant entities.\nThrough extensive experiments, we show that\nour COMET framework can achieve superior\nperformance over the state of the arts.\n1 Introduction\nTask-oriented dialogue systems aim to achieve spe-\nciﬁc goals such as hotel booking and restaurant\nreservation. The traditional pipelines (Young et al.,\n2013; Wen et al., 2017) consist of natural language\nunderstanding, dialogue management, and natural\nlanguage generation modules. However, designing\nthese modules often requires additional annotations\nsuch as dialogue states. To simplify this procedure,\nthe end-to-end dialogue systems (Eric and Man-\nning, 2017) are proposed to incorporate the KB\n(normally relational databases) into the learning\nframework, where the KB and dialogue history can\nbe directly modeled for response generation, with-\nout the explicit dialogue state or dialogue action.\n∗Corresponding author\nPoi Poi type Trafﬁc Address Distance\nStanford Express Carehospital moderate 214 El Camino Real2 miles\nTom’s house friend’s house no 580 Van Ness Ave 6 miles\nPhilz coffee or tea placeno 583 Alester Ave 4 miles\n5672 Barringer Streetcertain address no 5672 Barringer Street2 miles\nUser Where does my friend live ?\nSystem Tom’s house is 6 miles away at 580 Van Ness Ave .\nUser Is that the fastest route ?\nSystem I’ll send the route with no trafﬁc on your screen , drive carefully !\nTable 1: An example in SMD dataset (Eric et al., 2017).\nThe top is the entities in KB and the bottom is a two-\nturn dialogue between the user and system.\nAn example of the end-to-end dialogue systems\nis shown in Tab. 1. When generating the second\nresponse about the “trafﬁc info”: (1) the targeted\nentity “no trafﬁc” is associated with its same-row\nentities (KB context) like “Tome’s house”, “friend’s\nhouse” and “6 miles”. These entities can help with\nenriching the information of its representation and\nmodeling the structure of KB. (2) Also, the entity\nis related to the dialogue history (dialogue context),\nwhich provides clues about the goal-related row\n(like “Tom’s house” and “580 Van Ness Ave” in the\nﬁrst response). These clues can be leveraged to\nfurther enhance the corresponding representations\nand activate the targeted row, which beneﬁts the\nretrieval of “no trafﬁc”. Therefore, how to fully\ncontextualize the entity with its KB and dialogue\ncontexts, is the key point of end-to-end dialogue\nsystems (Madotto et al., 2018; Wu et al., 2019; Qin\net al., 2020), where the full-context enhanced entity\nrepresentation can make the reasoning over KB and\nthe response generation much easier.\nHowever, the existing works can only contextual-\nize the entity with perceiving parts of its KB context\nand ignoring the dialogue context : (1) (Madotto\net al., 2018; Wu et al., 2019; Qin et al., 2020) rep-\n4301\ne11 e12 e13 e14 e21 e22 e23 e24\n(a) Triplet rep.\nR1 R2\ne11 e12 e13 e14 e21 e22 e23 e24 (b) Row-entity rep.\ne11\ne12 e13 e·4 e22 e23\ne21\n(c) Graph rep.\nD\ne11 e12 e13 e14 e21 e22 e23 e24 (d) Ours.\nFigure 1: Four ways to represent the KB, where ei,j\nmeans the entity representation for the j-th entity of\nthe i-th row; Ri means the row representation of the i-\nth row; e·,j means the entities shared between different\nrow, like “no trafﬁc” in Tab. 1; D means the dialogue\ncontext. Note that the existing three representations\n(a-c) only consider parts of the KB context and ignore\nthe dialogue context, whereas our method (d) can fully\ncontextualize the entity with both of them.\nresent an entity as a triplet (cf. Fig. 1(a)), i.e.,\n(Subject, Relation, Object). However, breaking one\nrow into several triplets can only model the relation\nbetween two entities, whereas the information from\nother same-row entities and dialogue history are\nignored. (2) (Gangi Reddy et al., 2019; Qin et al.,\n2019) represent KB in a hierarchical way, i.e., the\nrow and entity-level representation (cf. Fig. 1(b)).\nThis representation can only partially eliminate this\nissue at the row level. However, at the entity level,\nthe entity can only perceive the information of it-\nself, which is isolated with other KB and dialogue\ncontexts. (3) (Yang et al., 2020) converts KB to a\ngraph (cf. Fig. 1(c)). However, they fails to answer\nwhat is the optimal graph structure for KB. That\nindicates their graph structure may need manual\ndesign1. Also, the dialogue context is not encoded\ninto the entity representation, which can also lead\nto the suboptimal entity representation. To sum up,\nthese existing methods can notfully contextualize\nthe entity, which leads to vulnerable KB reasoning\nand response generation.\nIn this work, we propose COntext-aware Mem-\nory Enhanced Transformer (COMET), which pro-\nvides a uniﬁed solution to fully contextualizethe\nentity with the awareness of both the KB and di-\nalogue contexts (shown in Fig. 1(d)). The key\nidea of COMET is that: a Memory-Masked En-\n1For instance, on the SMD dataset, they only activate the\nedges between the primary key (“ poi”) and other keys(e.g.,\n“address”) in the Navigation domain, but assign a fully-\nconnected graph to the Schedule domain.\ncoder is used to encode the entity sequence of KB,\nalong with the information of dialogue history. The\ndesigned Memory Mask is utilized to ensure the\nentity can only interact with its same-row entities\nand the information in dialogue history, whereas\nthe distractions from other rows are prohibited.\nMore speciﬁcally, (1) for the KB context, we rep-\nresent the entities in the same row as a sequence.\nThen, a Transformer Encoder (Vaswani et al., 2017)\nis leveraged to encode them, where the same-row\nentities can interact with each other. Furthermore,\nto retain the structure of KB and avoid the distrac-\ntions from the entities in different rows, we design\na Memory Mask(shown in Fig. 3) and incorporate\nit into the encoder, which only allows the interac-\ntions between the same-row entities. (2) For the\ndialogue context, we create a Summary Representa-\ntion (Sum. Rep) to summarize the dialogue history,\nwhich is input into the encoder to interact with the\nentity representations (gray block in Fig. 2). We\nalso utilize the Memory Mask to make the Sum.\nRep overlook all of the entities for better entity rep-\nresentations, which will serve as the context-aware\nmemory for further response generation.\nBy doing so, we essentially extend the entity of\nKB to (N+ 1)-tuple representation, where Nis\nthe number of entities in one row and “1” is for the\nSum. Rep of the dialogue history. By leveraging\nthe KB and dialogue contexts, our method can ef-\nfectively model the information existing in KB and\nactivate the goal-related entities, which beneﬁts\nthe entity retrieval and response generation. Please\nnote that the function of fully contextualizing entity\nis uniﬁed by the designed Memory Mask scheme,\nwhich is the key of our work.\nWe conduct extensive experiments on two public\nbenchmarks, i.e., SMD (Eric et al., 2017; Madotto\net al., 2018) and Multi-WOZ 2.1 (Budzianowski\net al., 2018; Yang et al., 2020). The experimental\nresults demonstrate signiﬁcant performance gains\nover the state of the arts. It validates that contextual-\nizing KB with Transformer beneﬁts entity retrieval\nand response generation.\nIn summary, our contributions are as follows:\n• To the best of our knowledge, we are the ﬁrst\nto fully contextualize the entity representation\nwith both the KB and dialogue contexts, for\nend-to-end task-oriented dialogue systems.\n• We propose Context-aware Memory En-\nhanced Transformer, which incorporates a de-\nsigned Memory Mask to represent entity with\n4302\nInput \nEmbedding\nN灤\n[SUM] where does my friend live …\nPositional \nEncoding\nDialogue History Encoding\n(Section 2.2)\nOutput \nEmbedding\nN灤\nLinear\nSoftMax\n[sos] @poi is\n@distance\nSketch Response Generation\n(Subsection 2.4.1)\nTransformer \nEncoder\nK灤\n… Tom’s_house … … poi …\n6 miles\nContext-aware Memory Generation\nEntity Linking\n(Subsection 2.4.2)\n(Section 2.3)\nSum. Rep.\nTransformer \nEncoder\nTransformer \nDecoder\nType \nEmbedding\nEntity \nEmbedding\n濖\n澷\nMemory Mask\nFigure 2: Overview of COMET. The gray block in the top left means Sum. Rep of dialogue history,which is used\nas the input for the Memory Generation. © means concatenation. The detailed construction of the Memory Mask\ncan be found in Fig. 3.\nawareness of both the relevant entities and\ndialogue history.\n• Extensive experiments demonstrate that our\nmethod gives a state-of-the-art performance.\n2 Methodology\nIn this section, we ﬁrst introduce the general work-\nﬂow for this task. Then, we elaborate on each part\nof COMET, i.e., the Dialogue History Encoder,\nContext-aware Memory Generation, and Response\nGeneration Decoder (as depicted in Fig. 2). Finally,\nthe objective function will be introduced.\n2.1 General Workﬂow\nGiven a dialogue history with k turns, which is\ndenoted as H= {u1, s1, u2, s2, ..., uk}(ui and si\ndenote the i-th turn utterances between the user\nand the system), the goal of dialogue systems is\nto generate the k-th system response sk with an\nexternal KB B= {[b11, ..., b1c], ...,[br1, ..., brc]},\nwhich has r rows and c columns. Formally, the\nprocedure mentioned above is deﬁned as:\np(sk|H, B) =\nn∏\ni=1\np(sk,t|sk,1, ..., sk,t−1, H, B),\nwhere we ﬁrst derive the dialogue history represen-\ntation (Section 2.2) and generate the Context-aware\nMemory, a.k.a., contextualized entity representa-\ntion (Section 2.3), where these two parts will be\nused to generate the response sk (Section 2.4).\n2.2 Dialogue History Encoder\nWe ﬁrst transform H into the word-by-word\nform with a special token [ SUM]: ˆH =\n{x1, x2, ..., xn}, x1 = [SUM], which is used to\nglobally aggregate information from H.\nThen, the sequence ˆHis encoded by a standard\nTransformer Encoder and generate the dialogue his-\ntory representation Henc\nN , where Henc\nN,1 is denoted as\nthe Summary Representation (Sum. Rep) of the di-\nalogue history.2 It will be used to make the memory\naware of the dialogue context.\n2.3 Context-aware Memory Generation\nIn this subsection, we describe how to “fully con-\ntextualize KB”. That is, the Memory Mask is lever-\naged to ensure the entities of KB with the aware-\nness of all of its related entities and dialogue history,\nwhich is the key contribution of our method.\n2.3.1 Memory Generation\nDifferent from existing works which fail to contex-\ntualize all the useful context information for the\nentity representation, we treat KB as a sequence,\nalong with Sum. Rep . Then, a Transformer En-\ncoder with the Memory Mask is utilized to model\nit, which can dynamically generate the entity rep-\nresentation with the awareness of its all favorable\ncontexts, i.e., the same-row entities and dialogue\nhistory, while blocking the distraction from the\n2This module is as same as the standard Transformer En-\ncoder, please refer to (Vaswani et al., 2017) for more details.\n4303\nirrelevant entities. The procedure of memory gen-\neration is as follows.\nFirstly, the entities in the KB B is ﬂat-\nten as a memory sequence, i.e., M =\n[b11, ..., b1c, ..., br1, ..., brc] = [ m1, m2, ..., m|M|],\nwhere the memory entitymi means an entity of KB\nin the k-th row. By doing so, the Memory-Masked\nTransformer Encoder can interact the same-row en-\ntities with each other while retaining the structure\ninformation of KB.3\nThen, Mwill be transformed into the entity\nembeddings, i.e., E = [ em\n1 , ..., em\n|M|], where em\ni\ncorresponds to mi in Mand it is the sum of the\nword embedding ui and the type embedding ti, i.e.,\nem\ni = ui + ti. Note that, the entity types are the\ncorresponding column names, e.g., “poi_type” in\nTable 1. For the entities which have more than\none token, we simply treat them as one word, e.g.,\n“Stanford Exp” →“Stanford_Exp”.\nNext, the entity embeddings are concatenated\nwith the Sum. Rep from the Dialogue History En-\ncoder, i.e. E0 = [Henc\nN,1; E]. The purpose of intro-\nducing Henc\nN,1 is that it passes the information from\nthe dialogue history and further enhances the entity\nrepresentation with the dialogue context.\nFinally, E0 and the Memory Mask Mmem are\nused as the input of the Transformer Encoder\n(tf_enc(·)) to generate the context-aware memory\n(a.k.a, contextualized entity representation):\nEl = tf_enc(El−1, Mmem), l∈[1, K],\nwhere K is the total number of Transformer En-\ncoder layers. EK ∈R(|M|+1)×dm is the generated\nmemory, which is queried when generating the re-\nsponse for entity retrieval.\n2.3.2 Memory Mask Construction\nTo highlight, we design a special Memory Mask\nscheme to take ALL the contexts grounded by the\nentity into account, where the Memory Mask en-\nsures that the entity can only attend to its context\npart, which is the key contribution of this work.\nThis is in contrast to the standard Transformer En-\ncoder, where each entity can attend to all of the\nother entities. The rationale of our design is that by\ndoing so, we can avoid the noisy distraction of the\nnon-context part.\n3When the memory sequence is long, some existing meth-\nods like the linear attention (Kitaev et al., 2020) can be used\nto tackle the issue of O(N2) complexity of Self Attention.\nFormally, Mmem ∈ R(|M|+1)×(|M|+1) is de-\nﬁned as:\nMmem\ni,j =\n\n\n\n1, if Mi−1, Mj−1 ∈bk,\n1, if i or j = 1,\n−∞, else.\nA detailed illustration of the Memory Mask con-\nstruction is shown in Fig. 3. With this designed\nMemory Mask, a masked attention mechanism is\nleveraged to make the entity only attend the entities\nwithin the same row and the Sum. Rep.\nC1 C2 C3 C4 C5\ne11 e12 e13 e14 e15\ne21 e22 e23 e24 e25\nKnowledge Base\nMemory Mask Construction\n(Subsection 2.3.2)\n[SUM]\ne11\ne12\ne13\ne14\ne15\ne21\ne22\ne23\ne24\ne25\n[SUM]e11 e12 e13 e14 e15 e21 e22 e23 e24 e25\nMask Construction\nFigure 3: The Construction of Memory Mask. Ci\nmeans the column name (e.g., “ Poi”). eij means the\nj-th entity of i-th row. [ SUM] means the Sum. Rep.\nOnly two rows of KB are shown for simplicity.\n2.4 Response Generation Decoder\nGiven the dialogue history representationHenc\nN and\ngenerated memory EK, the decoder will use them\nto generate the response for a speciﬁc query. In\nCOMET, we use a modiﬁed Transformer Decoder,\nwhich has two cross attention modules to model the\ninformation in Henc\nN and EK, respectively. Then,\na gate mechanism is leveraged to adaptively fuse\nHenc\nN and EK for the decoder, where the response\ngeneration is tightly anchored by them.\nFollowing (Wu et al., 2019; Qin et al., 2020;\nYang et al., 2020), we ﬁrst generate a sketch re-\nsponse that replaces the exact slot values with\nsketch tags.4 Then, the decoder links the entities in\n4For instance, “Tom’s house is 6 miles away at 580 Van\nNess Ave .\" → “@poi is @distance away at @address.\".\n4304\nthe memory to their corresponding slots.\n2.4.1 Sketch Response Generation\nFor the k-th turn generating sketch response Y=\n[y1, ...yt−1], it is converted to the word representa-\ntion Hdec\n0 = [wd\n1, ..., wd\nt−1]. wd\ni = vi + pi, where\nvi and pi means the word embedding and absolute\nposition embedding of i-th token in Y.\nAfterward, N-stacked decoder layers are applied\nto decode the next token with the inputs of Hdec\n0 ,\nEK and Henc\nN . The process in one decoder layer\ncan be expressed as:\nHd−d\nl = MHA (Hdec\nl−1, Hdec\nl−1, Hdec\nl−1, Mdec),\nHd−e\nl = MHA (Hd−d\nl , Henc\nN , Henc\nN ),\nHd−m\nl = MHA (Hd−d\nl , EK, EK),\ng = sigmoid(FC (Hd−m\nl )),\nHagg\nl = g ⊙Hd−e\nl + (1 −g) ⊙Hd−m\nl ,\nHdec\nl = FFN (Hagg\nl ), l∈[1, N],\nwhere the input {Q, K, V, M}of the Multi-Head\nAttention MHA (Q, K, V, M) means the query,\nkey, value, and optional attention mask. FFN (·)\nmeans the Feed-Forward Networks. Mdec is the\ndecoder mask, so as to make the decoded word\ncan only attend to the previous words. FC (·) is a\nfully-connected layer to generate the gating signals,\nwhich maps a dm-dimension feature to a scalar. N\nis the number of the total decoder layers.\nAfter obtaining the ﬁnal Hdec\nN , the posterior dis-\ntribution for the t-th token, pv\nt ∈R|V |(|V |denotes\nthe vocabulary size), is calculated by:\npv\nt = softmax(Hdec\nN,t−1Wv + bv).\n2.4.2 Entity Linking\nAfter the sketch response generation, we replace\nthe sketch tags with the entities in the context-\naware memory. We denote the representation from\nthe decoder at the t-th time step, i.e., the t-th token,\nas Hdec\nN,t, and represent the time steps that need to\nreplace sketch tags with entities as T. The proba-\nbility distribution over all possible linked entities\ncan then be calculated by\nps\nt = softmax(Hdec\nN,tET\nK), ∀t ∈T\nwhere EK means the ﬁnal generated memory.\n2.5 Objective Function\nFor the training process of COMET, we use the\nthe cross-entropy loss to supervise the response\ngeneration and entity linking5.\nMoreover, we propose an additional regulariza-\ntion term to further regularize ps\nt . The regulariza-\ntion is based on the prior knowledge that for a given\nresponse, only a small subset of entities should be\nlinked. Formally, we construct the following entity\nlinking probability matrix Ps = [ps\nt1 , ps\nt2 , ..., ps\nt|T|]\nand minimize its L2,1-norm (Nie et al., 2010):\nLr =\n|M|∑\ni=1\n√∑\nt∈T\n(ps\nt,i)2 ,\nwhere ps\nt,i denotes the i-th dimension of ps\nt . This\nregularization term can encourage the network to\nselect a small subset of entities to generate the\nresponse. The same idea has been investigated in\n(Nie et al., 2010) for multi-class feature selection.\nFinally, COMET is trained by jointly minimizing\nthe combination of the above three losses.\n3 Experiments\n3.1 Datasets\nTwo public multi-turn task-oriented dialogue\ndatasets are used to evaluate our model, i.e.,\nSMD6 (Eric et al., 2017) and Multi-WOZ 2.1 7\n(Budzianowski et al., 2018). Note that, for Multi-\nWOZ 2.1, to accommodate end-to-end settings, we\nuse the revised version released by (Yang et al.,\n2020), which equips the corresponding KB to every\ndialogue. We follow the same partition as (Madotto\net al., 2018) on SMD and (Yang et al., 2020) on\nMulti-WOZ 2.1.\n3.2 Experimental Settings\nThe dimension of embeddings and hidden vectors\nare all set to 512. The number of layers ( N) in\nDialogue History Encoder and Response Genera-\ntion Decoder is set to 6. The number of layers for\nContext-aware Memory Generation (K) is set to 3.\nThe number of heads in each part of COMET is set\nto 8. A greedy strategy is used without beam-search\nduring decoding. The Adam optimizer (Kingma\n5The label construction procedure of the entity linking\nmodule can be found in Appendix A.1.\n6https://github.com/jasonwu0731/GLMP/\ntree/master/data/KVR\n7https://github.com/shiquanyang/\nGraphDialog/tree/master/data/MULTIWOZ2.1\n4305\nSMD Multi-WOZ2.1\nModel BLEU F1 F1-Sch. F1-Wea.F1-Nav.BLEU F1 F1-Res. F1-Att. F1-Hot. F1-Tra.\nMem2Seq 12.6 33.4 49.3 32.8 20.0 4.1 3.2 2.9 2.1 4.5 1.5\nKB-Transformer13.9 37.1 51.2 48.2 23.3 - - - - - -\nKB-Retriever 13.9 53.7 55.6 52.2 54.5 - - - - - -\nGLMP 13.9 60.7 72.9 56.5 54.6 4.3 6.7 11.4 9.4 3.9 3.5\nDF-Net 14.4 62.7 73.1 57.6 57.9 - - - - - -\nGraphDialog 13.7 60.7 72.8 55.2 54.2 6.2 11.3 16.0 14.1 10.8 4.4\nCOMET (Ours) 17.3 63.6 77.6 58.3 56.0 8.3 18.6 27.5 17.9 15.2 9.8\nTable 2: BLEU and Entity F1 comparison of COMET with other counterparts. The best results are in bold font\nand the second-best results are underlined. The results on the SMD and Multi-WOZ 2.1 datasets are adopted from\n(Qin et al., 2020) and (Yang et al., 2020), respectively.\nand Ba, 2014) is used to train our model from\nscratch with a learning rate of 1e−4. More details\nabout the hyper-parameter settings can be found in\nAppendix A.2.\n3.3 Baselines\nWe compare COMET with the following methods:\n• Mem2Seq (Triplet) (Madotto et al., 2018):\nMem2Seq incorporates the multi-hop atten-\ntion mechanism in memory networks into the\npointer networks.\n• KB-Transformer (Triplet) (E. et al., 2019):\nKB-Transformer combines a Multi-Head Key-\nValue memory network with Transformer.\n• KB-Retriever (Row-entity) (Qin et al., 2019):\nKB-retriever improves the entity-consistency\nby ﬁrst selecting the target row and then pick-\ning the relevant column in this row.\n• GLMP (Triplet) (Wu et al., 2019): GLMP\nuses a global memory encoder and a local\nmemory decoder to incorporate the external\nknowledge into the learning framework.\n• DF-Net (Triplet) (Qin et al., 2020): DF-Net\napplies a dynamic fusion mechanism to trans-\nfer knowledge in different domains.\n• GraphDialog (Graph) (Yang et al., 2020):\nGraphDialog exploits the graph structural in-\nformation in KB and in the dependency pars-\ning tree of the dialogue.\n3.4 Results\nFollowing the existing works (Qin et al., 2020;\nYang et al., 2020), we use the BLEU and Entity\nF1 metrics to evaluate model performance. The\nresults are shown in Tab. 2.\nIt is observed that: COMET achieves the best\nperformance over both datasets, which indicates\nthat our COMET framework can better leverage\nthe information in the dialogue history and external\nKB, to generate more ﬂuent responses with more\naccurate linked entities. Speciﬁcally, for the BLEU\nscore, it outperforms the previous methods by 2.9%\non the SMD dataset and 2.1% on the Multi-WOZ\n2.1 dataset, at least. Also, COMET achieves the\nhighest Entity F1 score on both datasets. That is,\nthe improvements of 0.9% and 7.3% are attained on\nthe SMD and Multi-WOZ 2.1 datasets, respectively.\nIn each domain of the two datasets, improvement or\ncompetitive performance can be clearly observed.\nThe results indicate the superior of our COMET\nframework.\nTo highlight, KB-Transformer (E. et al., 2019)\nalso leverages Transformer, but our COMET out-\nperforms it by a large margin. On the SMD dataset,\nthe BLEU score of COMET is higher than that\nof KB-Transformer by 3.4%. The improvement\nintroduced by COMET on Entity F1 score is as\nsigniﬁcant as 26.5%. This shows naively introduc-\ning Transformer to the end-to-end dialogue system\nwill not necessarily lead to higher performance. A\ncareful design of the whole dialogue system, such\nas our proposed one, plays a vital role.\n3.5 Ablation Study\nIn this subsection, we ﬁrst investigate the effects of\nthe different components, i.e., the Memory Mask,\nSum. Rep, gate mechanism, and L2,1-norm reg-\nularization (Tab. 3). Then, we design careful ex-\nperiments to further demonstrate the effect of the\nMemory Mask, which is the key contribution of this\nwork: (1) we replace the context-aware memory\nof COMET with the existing three representations\nof KB, (i.e., triplet, row-entity, and graph) to show\nthe superior of the fully contextualized entity (Tab.\n4). (2) We also replace our Memory Mask with the\nfull attention layer by layer, which further shows\n4306\nthe importance of our Memory Mask (Tab. 5). Our\nablation studies are based on the SMD dataset.\nModel BLEU Entity F1 ∆\nCOMET 17.3 63.6 -\nw/o Memory Mask 15.4 49.6 14.0\nw/o Sum. Rep 17.0 61.4 2.2\nonly useHencN (gate) 17.2 61.1 2.5\nonly useEK (gate) 17.1 61.4 2.2\nw/oL2,1-norm 17.4 62.3 1.3\nTable 3: The effects of different components.\nThe effects of the key components in the\nCOMET framework are reported in Tab. 3. As\nobserved, removing any key component of the\nCOMET, both the BLEU and Entity F1 metrics\ndegrade to some extend. More speciﬁcally: (1) If\nthe Memory Mask is removed, the Entity F1 score\ndrops to 49.6. This signiﬁcant discrepancy demon-\nstrates the importance of restricting self-attention\nas our designed Memory Mask did. (2) For the\nvariant without the Sum. Rep, the Entity F1 score\ndrops to 61.4. That indicates the effectiveness of\ncontextualizing the KB with the dialogue history,\nwhich can further boost the performance. (3) We\nalso remove the gate and only use the information\nfrom the dialogue history (Henc\nN ) or memory (EK).\nWe can see that the former case can only achieve\n61.1 while the latter case achieves 61.4 of theEntity\nF1 score. It is obvious that using the gate mech-\nanism to fuse both information sources is helpful\nfor the entity linking. (4) When removing the L2,1-\nnorm, the performance also drops to 62.3, which\nmeans regularizing the entity-linking distribution\ncan further beneﬁt the performance.\nModel BLEUF1 F1-Sch.F1-Wea.F1-Nav.\nContext-aware memory17.3 63.6 77.6 58.3 56.0\nOnly KB context17.0 61.4 75.5 55.2 54.4\nTriplet 14.9 59.8 73.1 54.0 53.0\nRow&Ent 13.0 41.4 51.2 54.6 19.3\nGraph 14.4 56.7 71.6 48.7 50.4\nTable 4: The performance of replacing the context-\naware memory with Triplet, Row-Ent and Graph repre-\nsentations in COMET. Note that in the second row, we\nalso report the result of a variant which only considers\nthe KB context and ignores the dialogue context.\nWe also replace our context-aware memory with\nother ways of representing KB, while other parts\nof our framework keep unchanged8. The result is\nreported in Tab. 4. It is observed that, After replac-\n8The implementation details are in Appendix A.3.\ning our context-aware memory with the existing\nthree representations of KB, the performance drops\na lot in all the metrics, where theBLEU score drops\n2.4% and the Entity F1 score drops 3.8% at least.\nBesides, the result of the variant which only con-\nsiders the KB context part (i.e., w/o Sum. Rep), is\nalso reported, so as to further fairly compare with\nthe aforementioned KB representations. The result\nshows that only considering the KB context, our\nmethod can still outperform other KB representa-\ntions by 1.6% of Entity F1 at least. That further\nindicates the fully contextualizing entity with its\nrelevant entity and the dialogue history, can better\nrepresent the KB for dialogue systems.\nScheme BLEU Entity F1 ∆\nMMM 17.3 63.6 -\nMMF 16.5 61.2 2.4\nMFF 16.5 59.1 4.5\nFFF 15.4 49.6 14.0\nTable 5: The performance of replacing the Memory\nMask with the full attention. The meanings of the\nscheme names are that the Memory Mask ( M) is re-\nplaced with the Full attention (F).\nWe also conduct the experiment which replaces\nthe Memory Mask with the full attention, layer by\nlayer. That is, the ﬁrst (n-k) layers use the proposed\nMemory Mask (M) and the last k layers use the full\nattention (F). As shown in Tab. 5, the more full at-\ntention is added, the more performance of COMET\ndrops in all of the metrics since the full attention\nintroduces too much distraction from other rows.\nThe result further indicates that the Memory Mask\nis indeed a better choice which takes the inductive\nbias of KB into account.\nNote that we also explore other Memory Mask\nschemes, but these schemes can not further boost\nthe performance, where the results are omitted due\nto the page limitation. For further improvement,\nmore advanced techniques like Pre-trained Model\n(Devlin et al., 2018; Radford et al., 2019) may be\nneeded to deeply understand the dialogue and KB\ncontext, which we leave for future work.\n3.6 Case Study\nTo demonstrate the superiority of our method, sev-\neral examples on the SMD test set, which are gen-\nerated by our COMET and the existing state of the\narts GLMP (Wu et al., 2019) and DF-Net (Qin et al.,\n2020), are given in Tab. 6. As reported, compared\nwith GLMP and DF-Net, COMET can generate\n4307\nQuery&Response Example\nGoal: [yoga_activity, 11am, thursday, alex]\nQuery: what time do i go to yoga and who is going with me ?\nResponse: yoga is with [alex] at [11am].\nGLMP: your yoga is on [thursday]✓ with [alex]✓.\nDF-Net: your yoga is on [thursday]✓ at [5pm]×.\nCOMET: your yoga is on [thursday]✓ at [11am]✓ with [alex]✓. (More informative response.)\nGoal: [stanford_express_care, 6_miles, heavy_trafﬁc, hospital, 214_el_camino_real]\nQuery: car i need a hospital , please send me a quick route to the one near me in the direction we are driving.\nResponse: stanford_express_care is on the way.\nGLMP: stanford_express_care is [2_miles]×away. (Wrong distance info.)\nDF-Net: stanford_express_care is [4_miles]×away. (Wrong distance info.)\nCOMET: stanford_express_care is [6_miles]✓ away.\nGoal: [mountain_view, friday, clear_skies, 30f, 50f]\nQuery: what will be the highest_temperature on friday in mountain_view ?\nResponse: on friday , the highest_temperature in mountain_view is predicted to be [50f].\nGLMP: it will be friday on mountain_view in highest_temperature.×(Fail to retrieve the temperature info.)\nDF-Net: it will be highest_temperature on friday in mountain_view.×(Fail to retrieve the temperature info.)\nCOMET: on friday the temperature in mountain_view will be low of [30f]✓ , and high of [50f]✓.\nGoal: [teavana, 5_miles, moderate_trafﬁc, coffee_or_tea_place, 145_amherst_st]\nQuery: get me directions to the nearest starbucks.\nResponse: there isNOstarbucks locally.\nGLMP: there is a starbucks [1_miles]×away. (Unfaithful response.)\nDF-Net: the nearest starbucks is teavana , it s [1_miles]×away. (Not ﬂuent and wrong entities retrieved.)\nCOMET: there isNOstarbucks nearby , but [teavana]✓ is [5_miles]✓ away would you like directions there?\nTable 6: Responses generated by our COMET, GLMP (Wu et al., 2019) and DF-Net (Qin et al., 2020) from the\nSMD dataset. Goal means the row that the user is queried. ✓ and ×mean the right or wrong entity linked.\nmore ﬂuent, informative, and accurate responses.\nSpeciﬁcally, in the ﬁrst example, GLMP and DF-\nNET are lack of the necessary information “11am”\nor provide the wrong entity “5pm”. But COMET\ncan obtain all the correct entities, which is more\ninformative. In the second example, our method\ncan generated the response with the right “distance”\ninformation but GLMP and DF-Net can not. In the\nthird example, GLMP and DF-Net can not even\ngenerate a ﬂuent response, let alone the correct\ntemperature information. But COMET can still per-\nform well. The fourth example is more interesting:\nthe user queries the information about “starbucks”\nwhich does not exist in the current KB. GLMP\nand DF-Net both fail to faithfully respond, whereas\nCOMET can better reason KB to generate the right\nresponse and even provide an alternative option.\n4 Related Work\nTask-oriented dialogue systems can be mainly cat-\negorized into two parts: modularized (Williams\nand Young, 2007; Wen et al., 2017) and end-to-\nend (Eric and Manning, 2017). For the end-to-end\ntask-oriented dialogue systems, (Eric and Manning,\n2017) ﬁrst explores the end-to-end method for the\ntask-oriented dialogue systems. However, it can\nonly link to the entities in the dialogue context and\nno KB is incorporated. To effectively incorporate\nthe external KB, (Eric et al., 2017) proposes a key-\nvalue retrieval mechanism to sustain the grounded\nmulti-domain discourse. (Madotto et al., 2018) aug-\nments the dialogue systems with end-to-end mem-\nory networks (Sukhbaatar et al., 2015). (Wen et al.,\n2018) models a dialogue state as a ﬁxed-size dis-\ntributed representation and uses this representation\nto query KB. (Lei et al., 2018) designs belief spans\nto track dialogue believes, allowing task-oriented\ndialogue systems to be modeled in a sequence-to-\nsequence way. (Gangi Reddy et al., 2019) proposes\na multi-level memory to better leverage the external\nKB. (Wu et al., 2019) proposes a global-to-local\nmemory pointer network to reduce the noise caused\nby KB. (Lin et al., 2019) proposes Heterogeneous\nMemory Networks to handle the heterogeneous in-\nformation from different sources. (Qin et al., 2020)\nproposes a dynamic fusion mechanism to transfer\nthe knowledge among different domains. (Yang\net al., 2020) exploits the graph structural informa-\n4308\ntion in KB and the dialogue. Other works also\nexplore how to combine the Pre-trained Model (De-\nvlin et al., 2018; Radford et al., 2019) with the end-\nto-end task-oriented dialogue systems. (Madotto\net al., 2020a) directly embeds the KB into the pa-\nrameters of GPT-2 (Radford et al., 2019) via ﬁne-\ntuning. (Madotto et al., 2020b) proposes a dialogue\nmodel that is built with a ﬁxed pre-trained conver-\nsational model and multiple trainable light-weight\nadapters.\nWe also notice that some existing works also\ncombine Transformer with the memory component,\ne.g., (Ma et al., 2021). However, our method is dis-\ntinguishable from them, since the existing works\nlike (Ma et al., 2021) simply inject the memory\ncomponent into Transformer. In contrast, inspired\nby the dynamic generation mechanism (Gou et al.,\n2020), the memory in COMET (i.e., the entity rep-\nresentation) is dynamically generated by fully con-\ntextualizing the KB and dialogue context via the\nMemory-masked Transformer.\n5 Conclusion\nIn this work, we propose a novel COntext-aware\nMemory Enhanced Transformer (COMET) for the\nend-to-end task-oriented dialogue systems. By the\ndesigned Memory Mask scheme, COMET can fully\ncontextualize the entity with all its KB and dia-\nlogue contexts, and generate the (N+ 1)-tuple\nrepresentations of the entities. The generated entity\nrepresentations can further augment the framework\nand lead to better capabilities of response genera-\ntion and entity linking. The extensive experiments\ndemonstrate the effectiveness of our method.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their valuable comments.\nReferences\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gaši ´c. 2018. MultiWOZ - a\nlarge-scale multi-domain Wizard-of-Oz dataset for\ntask-oriented dialogue modelling. In Proceedings\nof the Conference on Empirical Methods in Natu-\nral Language Processing , pages 5016–5026, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nH. E., W. Zhang, and M. Song. 2019. Kb-\ntransformer: Incorporating knowledge into end-to-\nend task-oriented dialog systems. In 15th Inter-\nnational Conference on Semantics, Knowledge and\nGrids, pages 44–48.\nMihail Eric, Lakshmi Krishnan, Francois Charette, and\nChristopher D. Manning. 2017. Key-value retrieval\nnetworks for task-oriented dialogue. In Proceedings\nof the Annual SIGdial Meeting on Discourse and Di-\nalogue, pages 37–49, Saarbrücken, Germany. Asso-\nciation for Computational Linguistics.\nMihail Eric and Christopher Manning. 2017. A copy-\naugmented sequence-to-sequence architecture gives\ngood performance on task-oriented dialogue. In Pro-\nceedings of the Conference of the European Chap-\nter of the Association for Computational Linguis-\ntics, pages 468–473, Valencia, Spain. Association\nfor Computational Linguistics.\nRevanth Gangi Reddy, Danish Contractor, Dinesh\nRaghu, and Sachindra Joshi. 2019. Multi-level\nmemory for task oriented dialogs. In Proceedings\nof the Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies , pages 3744–3754,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nYanjie Gou, Yinjie Lei, Lingqiao Liu, Pingping Zhang,\nand Xi Peng. 2020. A dynamic parameter enhanced\nnetwork for distant supervised relation extraction.\nKnowledge-Based Systems, 197:105912.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations.\nWenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun\nRen, Xiangnan He, and Dawei Yin. 2018. Sequicity:\nSimplifying task-oriented dialogue systems with sin-\ngle sequence-to-sequence architectures. In Proceed-\nings of the Annual Meeting of the Association for\nComputational Linguistics, pages 1437–1447, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nZehao Lin, Xinjing Huang, Feng Ji, Haiqing Chen, and\nYin Zhang. 2019. Task-oriented conversation gen-\neration using heterogeneous memory networks. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing and the Inter-\nnational Joint Conference on Natural Language Pro-\ncessing, pages 4558–4567, Hong Kong, China. As-\nsociation for Computational Linguistics.\n4309\nXutai Ma, Yongqiang Wang, Mohammad Javad Dousti,\nPhilipp Koehn, and Juan Pino. 2021. Stream-\ning simultaneous speech translation with augmented\nmemory transformer. In IEEE International Con-\nference on Acoustics, Speech and Signal Processing,\npages 7523–7527. IEEE.\nAndrea Madotto, Samuel Cahyawijaya, Genta Indra\nWinata, Yan Xu, Zihan Liu, Zhaojiang Lin, and\nPascale Fung. 2020a. Learning knowledge bases\nwith parameters for task-oriented dialogue systems.\narXiv preprint arXiv:2009.13656.\nAndrea Madotto, Zhaojiang Lin, Yejin Bang, and Pas-\ncale Fung. 2020b. The adapter-bot: All-in-one\ncontrollable conversational model. arXiv preprint\narXiv:2008.12579.\nAndrea Madotto, Chien-Sheng Wu, and Pascale Fung.\n2018. Mem2Seq: Effectively incorporating knowl-\nedge bases into end-to-end task-oriented dialog sys-\ntems. In Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics , pages\n1468–1478, Melbourne, Australia. Association for\nComputational Linguistics.\nFeiping Nie, Heng Huang, Xiao Cai, and Chris H Ding.\n2010. Efﬁcient and robust feature selection via joint\nl2, 1-norms minimization. In Advances in neural in-\nformation processing systems, pages 1813–1821.\nLibo Qin, Yijia Liu, Wanxiang Che, Haoyang Wen,\nYangming Li, and Ting Liu. 2019. Entity-consistent\nend-to-end task-oriented dialogue system with KB\nretriever. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing , pages 133–142, Hong\nKong, China. Association for Computational Lin-\nguistics.\nLibo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, and\nTing Liu. 2020. Dynamic fusion network for multi-\ndomain end-to-end task-oriented dialog. In Proceed-\nings of the Annual Meeting of the Association for\nComputational Linguistics , pages 6344–6354, On-\nline. Association for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston,\nand Rob Fergus. 2015. End-to-end memory net-\nworks. In Proceedings of the International Con-\nference on Neural Information Processing Systems ,\npage 2440–2448, Cambridge, MA, USA. MIT Press.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, undeﬁne-\ndukasz Kaiser, and Illia Polosukhin. 2017. Attention\nis all you need. In Proceedings of the International\nConference on Neural Information Processing Sys-\ntems, page 6000–6010, Red Hook, NY , USA. Curran\nAssociates Inc.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\n2018. Graph Attention Networks. International\nConference on Learning Representations.\nHaoyang Wen, Yijia Liu, Wanxiang Che, Libo Qin,\nand Ting Liu. 2018. Sequence-to-sequence learning\nfor task-oriented dialogue with dialogue state repre-\nsentation. In Proceedings of the International Con-\nference on Computational Linguistics , pages 3781–\n3792, Santa Fe, New Mexico, USA. Association for\nComputational Linguistics.\nTsung-Hsien Wen, David Vandyke, Nikola Mrkši ´c,\nMilica Gaši´c, Lina M. Rojas-Barahona, Pei-Hao Su,\nStefan Ultes, and Steve Young. 2017. A network-\nbased end-to-end trainable task-oriented dialogue\nsystem. In Proceedings of the Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics, pages 438–449, Valencia, Spain.\nAssociation for Computational Linguistics.\nJason D Williams and Steve Young. 2007. Partially\nobservable markov decision processes for spoken\ndialog systems. Computer Speech & Language ,\n21(2):393–422.\nChien-Sheng Wu, Richard Socher, and Caiming Xiong.\n2019. Global-to-local memory pointer networks for\ntask-oriented dialogue. In International Conference\non Learning Representations.\nShiquan Yang, Rui Zhang, and Sarah Erfani. 2020.\nGraphDialog: Integrating graph knowledge into end-\nto-end task-oriented dialogue systems. In Proceed-\nings of the Conference on Empirical Methods in Nat-\nural Language Processing , pages 1878–1888, On-\nline. Association for Computational Linguistics.\nS. Young, M. Gaši´c, B. Thomson, and J. D. Williams.\n2013. Pomdp-based statistical spoken dialog sys-\ntems: A review. Proceedings of the IEEE ,\n101(5):1160–1179.\n4310\nA Appendices\nA.1 Label Construction of Entity Linking\nIn practice, the datasets do not provide the golden\nlinked entity. However, We could obtain a pseudo\nannotation by following (Qin et al., 2019) to use a\ndistant supervision method. Speciﬁcally, we match\nthe entities in the golden response against the enti-\nties in the memory Mand use the matching result\nas the golden entity. For entities like “no_trafﬁc”,\none may ﬁnd matches in multiple rows. We resolve\nthis ambiguity by choosing the entity from the row\nwhich has the most matches for all entities in the\nutterances.\nA.2 Hyper-parameter Settings\nHyper-parameter SMD Multi-WOZ 2.1\nBatch Size 32 16\nHidden Size 512 512\nEmbedding Size 512 512\n#Layer of Dialogue Enc. 6 6\n#Layer of Response Dec. 6 6\n#Layer for Memory 3 3\n#Head 8 8\nLearning Rate 0.0001 0.0001\nKB Mask Prob. 0.2 0.05\nDropout Prob. 0.1 0.1\nTable 7: Hyper-parameters used in the two datasets.\nWe follow (Wu et al., 2019) to randomly mask a\nsmall number of entities into an unknown token to\nimprove the generalization of our model. Besides,\nin the sketch generation and entity linking stages,\nwe also use the label smoothing to regularize the\nmodel. The hyper-parameters such as dropout rate\nare tuned over the development set by grid search\n(Entity F1 for both datasets). The model is imple-\nmented in PyTorch. The hyper-parameters used in\ntwo datasets are shown in Tab. 7.\nA.3 Implementation Details of Other KB\nRepresentations with Transformer\nTo further compare the different methods of rep-\nresenting KB with our method, we also adopt\nthe triplet, row-entity, and graph representation to\nreplace our contextualized entity representation,\nwhere we keep the other parts of COMET un-\nchanged.\nSpeciﬁcally, for the triplet representation, we\nfollow (Madotto et al., 2018; Wu et al., 2019; Qin\net al., 2020) to implement Transformer+Triplet,\nwhere the entity representation is the sum of the\nsubject, relation, and object. Besides, the multi-\nhop reasoning (Sukhbaatar et al., 2015) is lever-\naged to further boost the performance. For the\nrow-ent representation, we refer to (Gangi Reddy\net al., 2019; Qin et al., 2019) to implement Trans-\nformer+Row&Ent, where Bag-of-word embed-\nding and entity-type embedding are used for the\nrow-level representation and entity-level repre-\nsentation. Besides, the row-level representation\nand entity-level representation are hierarchically\nqueried, where the distribution of the entity-level\nembedding is used for the response generation. For\nthe graph representation, we adopt the memory part\nof GraphDialog (Yang et al., 2020) to implement\nTransformer+Graph, where the entity embedding\nis further augmented by Graph Neural Networks\n(Veliˇckovi´c et al., 2018). Besides, the last hop of\nthe triplet and graph representation, and the entity-\nlevel representation of Row&Entity representation\nwill be also used to adaptively fuse the information\nof KB in the Decoder of COMET. More details can\nbe found in the aforementioned papers."
}