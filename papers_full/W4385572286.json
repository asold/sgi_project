{
  "title": "Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization",
  "url": "https://openalex.org/W4385572286",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2111108288",
      "name": "Chong Yu",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2084622737",
      "name": "Tao Chen",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2152658811",
      "name": "Zhongxue Gan",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2606722458",
    "https://openalex.org/W4313484599",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2912281309",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W3204998121",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W4385573119",
    "https://openalex.org/W4287208846",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3195894202",
    "https://openalex.org/W4281651027",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3017746288",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4200200013",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3168125510",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Along with the performance improvement in NLP domain, the sizes of transformer-based language models (TLM) are also dramatically increased. Some prior works intend to compress TLM models into more compact forms, but do not fully consider the hardware characters may not support the efficient execution for these forms, leading to the deployment of TLM on hardware with noticeable acceleration is still challenging. This paper thoroughly designs a compression scheme named GPUSQ-TLM to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization characters. Especially, a dense TLM model is first pruned to meet the GPU's acceleration constraint of sparse patterns with FP16 type, then it is further quantized into a fixed-point one by quantization-aware training, to provide an extra speedup for integer tensors on GPU. A mixed-strategy knowledge distillation of labels, logits and feature maps is used for best accuracy compensation during pruning and quantization process. Experiment results show GPUSQ-TLM scheme achieves state-of-the-art compression on TLM model of various encoder and decoder blocks with negligible accuracy degradation on SQuAD, GLUE, CNN-DM & XSum and WikiText benchmarking tasks. Moreover, GPUSQ-TLM can boost actual deployment performance by up to 4.08-4.25x latency and 6.18-6.79x throughput on A100 GPU.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 218–235\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nBoost Transformer-based Language Models with\nGPU-Friendly Sparsity and Quantization\nChong Yu1, Tao Chen2,∗, Zhongxue Gan1,∗\n1Academy for Engineering and Technology, Fudan University\n2School for Information Science and Technology, Fudan University\n21110860050@m.fudan.edu.cn, {eetchen, ganzhongxue}@fudan.edu.cn\nAbstract\nAlong with the performance improvement in\nNLP domain, the sizes of transformer-based\nlanguage models (TLM) are also dramatically\nincreased. Some prior works intend to com-\npress TLM models into more compact forms,\nbut do not fully consider the hardware charac-\nters may not support the efficient execution for\nthese forms, leading to the deployment ofTLM\non hardware with noticeable acceleration is still\nchallenging. This paper thoroughly designs a\ncompression scheme named GPUSQ-TLM to\nmaximally utilize the GPU-friendly 2:4 fine-\ngrained structured sparsity and quantization\ncharacters. Especially, a dense TLM model is\nfirst pruned to meet the GPU’s acceleration con-\nstraint of sparse patterns with FP16 type, then\nit is further quantized into a fixed-point one by\nquantization-aware training, to provide an extra\nspeedup for integer tensors on GPU. A mixed-\nstrategy knowledge distillation of labels, logits\nand feature maps is used for best accuracy com-\npensation during pruning and quantization pro-\ncess. Experiment results show GPUSQ-TLM\nscheme achieves state-of-the-art compression\non TLM model of various encoder and de-\ncoder blocks with negligible accuracy degra-\ndation on SQuAD, GLUE, CNN-DM & XSum\nand WikiText benchmarking tasks. Moreover,\nGPUSQ-TLM can boost actual deployment\nperformance by up to 4.08-4.25×times latency\nand 6.18-6.79×throughput on A100 GPU.\n1 Introduction\nEquipped with the attention mechanism and ar-\nchitecture (Vaswani et al., 2017), the transformer-\nbased language models (TLM) are proficient\nin handling long-range dependencies of the se-\nquence inputs. The subsequent studies showed\nthat transformer-based pre-trained language mod-\nels (Devlin et al., 2019; Radford et al., 2018)\ncould achieve state-of-the-art performances on var-\nious natural language processing (NLP) (Wolf\n∗ Tao Chen and Zhongxue Gan are corresponding authors.\net al., 2019) benchmarks, including question an-\nswering (Rajpurkar et al., 2016), paraphrase detec-\ntion (Dolan and Brockett, 2005), sentiment anal-\nysis (Socher et al., 2013), natural language in-\nference (Bowman et al., 2015), and text classi-\nfication (Howard and Ruder, 2018), etc. Mean-\nwhile, the transformer-based structure also ex-\npanded its success to other disciplines like com-\nputer vision (Carion et al., 2020; Dosovitskiy\net al., 2020), music (Huang et al., 2018), chem-\nistry (Schwaller et al., 2019), life sciences (Rives\net al., 2021), and pharmaceutics (Yang et al., 2021).\nFigure 1: State-of-the-art NLP models size comparison.\nAlong with the performance improvement,\ntransformer-based language models’ scales are also\ndramatically increased. BERT model (Devlin et al.,\n2019) sets the milestone for pre-trained language\nmodels with transformer encoder as its backbone.\nIt has 340 million parameters for the large version.\nGenerative Pre-trained Transformer (GPT) series\nare dedicated to scaling pre-trained transformer de-\ncoder architecture and proved that a large-scale pre-\ntrained language model could achieve impressive\nfew-shot performance with diverse downstream\ntasks. The parameters scale increases from 110\nmillion of GPT-1 (Radford et al., 2018) to 1.5 bil-\nlion of GPT-2 (Radford et al., 2019) and finally\nboosts to 175 billion of GPT-3 (Brown et al., 2020).\nMegatron-LM model (Narayanan et al., 2021) is\n218\nwith 1 trillion parameters trained on 3072 GPUs.\nSwitch Transformers (Fedus et al., 2022) further\nincrease the scale of pre-trained language models\nto 1.6 trillion parameters with a Mixture of Experts\n(MoE) style. A detailed comparison of state-of-the-\nart transformer-based models can refer to Figure 1.\nModel compression techniques by transferring\nthe large-scale TLM models to a lightweight ver-\nsion can benefit more efficient computation with\nless memory and energy consumption. There are\nsome previous studies to compress the TLM mod-\nels to compact forms. However, there are some\nmain drawbacks in these prior arts:\n• Prior arts aim to reduce the theoretical model\nsize, which is not directly leading to better\nefficiency on deployed hardware. (Chen et al.,\n2020; Xu et al., 2021; Kurtic et al., 2022) can\nprune 50%-97% of BERT weights. However\nthe left weights have irregular sparse pattern\ndo not match hardware supported acceleration\ncharacteristics, leading to only 2%-8% latency\nspeedup on GPU hardware.\n• How to keep the best accuracy with mul-\ntiple compression methods and generalize\non various TLMs lack systematical investiga-\ntion. (Sanh et al., 2019; Sun et al., 2019; Jiao\net al., 2020) compress by pruning several en-\ntire transformer blocks in BERT models, lead-\ning to apparent accuracy drop. (Frantar and\nAlistarh, 2023) can prune 50% weights for\nGPT models. But the accuracy drop is even\nlarger. Moreover, these compression methods\nare specifically designed according to each\nmodel structure. So they cannot directly apply\nto other model types.\nBecause the 2:4 fine-grained structured sparse\npattern (See section 3.1 for more details) is well\nsupported on NVIDIA GPUs and corresponding\nlibraries for math acceleration and memory sav-\ning, so we are motivated to design the compres-\nsion strategy for TLM models to meet such sparse\npattern. Moreover, the 2:4 sparse GEMM sup-\nports low-precision formats like INT8. So it is\nnatural to design the compression schemeGPUSQ-\nTLM, by combining the GPU-friendly Sparsity\nand Quantization to boost deployment efficacy for\nTransformer-based Language models, especially\non GPU platforms. GPUSQ-TLM method con-\nsists of three stages. First, an original TLM model\nis pruned to meet the GPU’s acceleration constraint\nof structured sparse patterns with FP16 type. Then\nthe floating-point sparse model is quantized into\na fixed-point one by quantization-aware training\n(QAT), which can provide an extra speedup for\nGPU integer tensors. Finally, a mixed strategy\nknowledge distillation of labels, logits and feature\nmaps is developed for best accuracy compensation\nduring the above pruning and quantization process.\nOur main contributions include:\n• Unlike prior arts aiming at reducing theo-\nretical metrics, GPUSQ-TLM utilizes GPU-\nfriendly 2:4 sparsity with low-precision quan-\ntization, achieving better GPU acceleration.\n• GPUSQ-TLM combines mixed knowl-\nedge distillation with sparse pruning and\nquantization-aware training, which can best\ncompensate for compressed models’ accuracy.\n• GPUSQ-TLM can apply to various TLM\nstructures and tasks, boosting up to 4.08-\n4.25×times latency and 6.18-6.79×through-\nput on A100 GPU.\n• GPUSQ-TLM can work as plug-in to further\naccelerate compressed models generated by\nother methods (See section 4.5 for details).\n2 Related Work\n2.1 Pruning for TLM Compression\nSparsity is a standard technology (Han et al., 2015)\nfor deep learning model compression, which can\nsave computational power and reduce the memory\nbandwidth with storage burden. Pruning the ele-\nments with less influence on the model’s output is\na common way to compress a neural model into\na sparse form. (Xu et al., 2021) and (Chen et al.,\n2020) are the typical works to prune 50%-95%\nof model weights and finetune to recover most of\nthe accuracy. (Kurtic et al., 2022) further improves\nthe pruning effect with second-order Hessian ap-\nproximation. However, the pruned sparse format\nis irregular and difficult to accelerate with algebra\nlibraries and hardware (Mishra et al., 2021).\nSome studies have started to prune the entire\ntransformer blocks to improve the real hardware\ndeployment efficiency. For accuracy compensation,\nknowledge distillation (KD) (Hinton et al., 2015)\ntechnology is applied with the principle of using\na teacher model with better accuracy as the super-\nvisor for the compressed model to mimic. With\nKD, (Sanh et al., 2019; Sun et al., 2019; Jiao et al.,\n2020; Sun et al., 2020) succeed in compressing the\nBERT model with various tiny versions. We prove\n219\nthe proposed GPUSQ-TLM method in section 4.5\ncan help such coarse-grained pruning methods to\ncompress inside each transformer block with fur-\nther acceleration on GPU.\n2.2 Quantization for TLM Compression\nQuantization is another orthogonal model compres-\nsion technique (Wu et al., 2020) by applying lower-\nprecision formats other than the standard 32-bit\nfloating-point (FP32) data type for weight parame-\nters, inputs, and activations when executing a neu-\nral model. Quantization can speed up deployment\nefficiency because the low-precision formats have\nhigher computational throughput support in several\nprocessors (NVIDIA, 2020; Jouppi et al., 2017;\nArafa et al., 2019), with the extra benefit of reduc-\ning the memory pressure.\n(Shen et al., 2020) quantizes BERT models to\nultra-low precision using second-order Hessian in-\nformation. But ultra-low precision like 3-bit is not\nsupported on hardware. To facilitate the deploy-\nment, (Kim et al., 2021) makes an integer-only\napproximation for all operations in BERT to avoid\nfloating point calculation. But it also increases the\ndifficulty of maintaining accuracy. Our method\ncombines distillation strategies in calibration and\nuses quantization-aware training (QAT) for improv-\ning accuracy compensation effect.\n3 Boost TLM on GPU\nGPUSQ-TLM mainly contains structured sparse\npruning and sparse-distillation-combined QAT\nworkflows. We explain the structured sparse pattern\non GPU in section 3.1 and Appendix A.1, and how\nto compress each part of a transformer-based lan-\nguage model according to the GPU-friendly sparse\npattern in sections 3.2 and 3.3. Section 3.4 de-\nscribes the GPUSQ-TLM design as a whole.\n3.1 Fine-grained Structured Sparsity on GPU\nGeneral Matrix Multiplication (GEMM) is the\nfundamental operation inside the common parts\nof TLM models, such as convolution, linear pro-\njection, and multi-head attention blocks. A spe-\ncific unit called sparse Tensor Core (NVIDIA,\n2017a) was introduced in NVIDIA Ampere archi-\ntecture (NVIDIA, 2020) for hardware acceleration.\nAccordingly, a constraint named 2:4 fine-grained\nstructured sparsity (Mishra et al., 2021) is im-\nposed on the allowed sparsity pattern, i.e., two val-\nues from every four contiguous elements on rows\nmust be zero. Due to the 2:4 sparsity support on\nGPU Tensor Core hardware, sparse GEMM can\nreduce memory storage and bandwidth by almost\n2×and provide 2×math throughput compared to\ndense GEMM by skipping the redundant zero-value\ncomputation. NVIDIA Ampere GPU architecture\nsupports various numeric precision for 2:4 spar-\nsity, including FP32, FP16, INT8, and INT4, etc.\nMore details on structured sparsity can refer to Ap-\npendix A.1.\n3.2 Apply 2:4 Sparsity in Transformer Block\nThe transformer block (Vaswani et al., 2017) is\nthe fundamental building structure in various TLM\nmodels. The majority of the weight parameters and\nthe execution time are taken in stacked transformer\nblocks. For example, about 90.2% of the weight pa-\nrameters and 99.3% of the inference time are from\nthe transformer blocks in BERT-large model, and\nabout 77.7% of the weight parameters and 97.5% of\nthe inference time are from the transformer blocks\nin BERT-base model (Devlin et al., 2019). For the\nGPT-3-6.7B and GPT-3-175B models (Brown et al.,\n2020), about 96.8% & 99.6% of the weight param-\neters and 97.3% & 99.7% of the inference time\nare from the transformer blocks. So in this subsec-\ntion, we focus on how to apply the2:4 fine-grained\nstructured sparsity in the transformer block.\nTransformer blocks used in TLM models are\ndirectly borrowed from or made tiny changes on\nthe standard transformer block introduced in the\nnaive attention mechanism (Vaswani et al., 2017).\nUsually, the TLM models can be divided into three\nmain categories, i.e., only use the encoder trans-\nformer blocks (Devlin et al., 2019), only use the\ndecoder transformer blocks (Brown et al., 2020),\nand use both the encoder and decoder transformer\nblocks (Lewis et al., 2020). The essential compo-\nnents, like feed forward, residual add, and layer\nnorm, are almost identical in an encoder and a\ndecoder transformer block. The main difference\nis an encoder transformer block usually uses a\nmulti-head self-attention; in contrast, a decoder\ntransformer block usually uses a masked multi-\nhead self-attention and a multi-head cross-attention.\nHowever, the basic GEMM operations inside the\nmulti-head self-attention, masked multi-head self-\nattention, and multi-head cross-attention are almost\nidentical. Without losing the generalization of the\nproposed method, we illustrate the utilization of\n2:4 sparsity with a language model with standard\n220\nFigure 2: Illustration about applying the 2:4 fine-grained structured sparsity in transformer-based language model.\nThe above part of the figure shows a TLM model structure, with hierarchical zoom-in details of transformer block\nand multi-head attention module. The below part in the orange dashed box shows how to compress a dense matrix\nwith the sparse Tensor Core. The sparse target layers include the final linear projection, as well as the feed forward\nand linear projection inside each transformer block. To facilitate the quantization-aware training, we also insert the\nquantization simulation and de-quantization simulation operation pairs before and after the sparse target layers.\nencoder transformer blocks. 2:4 fine-grained struc-\ntured sparsity mainly targets accelerating GEMM\noperations. So the Q, K, and V projection layers,\nthe linear projection layer in the multi-head atten-\ntion module, and the linear projection layers in the\nfeed-forward module are the proper targets to ap-\nply, as shown in the zoomed-in parts (marked with\ngreen blocks) in Figure 2.\nThe input of a TLM is often a input tensor with\nshape RB×L, where B is the batch size, Lis the\nsequence length. The input tensor will firstly pass\nthrough an input embedding layer that converts\neach one-hot token representation into a ddimen-\nsional embedding vector, wheredis the embedding\nsize. The output tensor of input embedding layer\nwith shape RB×L×d additively composed with the\noutput of position embedding layer to generate the\ninput tensor X of N series of transformer blocks.\nThe input tensor X transfers to query Q, key K\nand value V with separate linear projection layers\nin a multi-head self-attention module with head\nnumber as H. For each head with head index h:\nQh = XWq\nh, Kh = XWk\nh, Vh = XWv\nh, (1)\nwhere Wq\nh, Wk\nh, Wv\nh ∈Rd× d\nH are the weight pa-\nrameters of query, key and value linear projection\nlayers in head h. Then the query, key and value\ntensors in each head go into a scaled dot-product\nattention to get the attention output tensor Ah:\nAh = softmax\n( QhKT\nh√\nd\n)\nVh, (2)\n221\nwhere the softmax is a row-wise operation, and\nthe dot-product of query and key is divided by\n√\nd\nas a form of normalization to alleviate gradient\nvanishing problem of the softmax function. The\noutputs of all heads A1,··· ,AH are concatenated\ntogether and go through an output linear projection\nlayer with weight tensor Wo. So the final output\ntensor MHA of a multi-head self-attention module\ncan be calculated as follows:\nMHA = Concat(A1,··· ,AH)Wo (3)\nMHA tensor will add the input tensor X in a resid-\nual way, and go through the layer-normalization\nlayer to get the output tensor Y, followed by a\nfully connected feed-forward layer with weight ten-\nsor Wff as well as another residual add and layer-\nnormalization pair to get the final output tensor Z\nfor one transformer block, i.e.,\nY = LayerNorm (MHA + X)\nZ = LayerNorm\n(\nYWff + Y\n) (4)\nThe output of the last transformer block will\ngo through the final linear projection layer with\nweight tensor Wfp to get the output tensor\nfor the entire language model. So the over-\nall size of trainable parameters 1 in a dense\ntransformer-based language model is SD\nTLM =\n16 ×N\n[\nH\n(\nWq\nh + Wk\nh + Wv\nh\n)\n+ Wo + Wff ]\n+\n16 ×Wfp bits. If we applying the 2:4 structured\nsparsity as shown in Figure 2 with FP16 format, the\noverall size of trainable parameters can be reduced\nto 0.5625 ×SD\nTLM .\nIn Figure 2, we also insert the quantization\nsimulation and de-quantization simulation oper-\nation pairs before and after the 2:4 sparse target\nlayers. With these operation pairs, we can trans-\nfer the sparse target layers as INT format during\nthe quantization-aware training. And the final 2:4\nsparse INT8 model can further reduce the overall\nsize of trainable parameters to 0.3125 ×SD\nTLM .\n3.3 Apply Sparsity in Multi-head Attention\nBased on the analysis of the scaled dot-product\nattention in equation 2, Qh, kh, Vh are output acti-\nvation tensors from the query, key and value linear\nprojection layers in head h, so it does not have any\ntrainable parameters. However, its computational\ncost is non-zero due to the softmax and the two\n1We do not count the weight parameters of the input embed-\nding and position embedding layers into the overall trainable\nparameters of the model. Because the embedding operation\nmay implement as a given lookup table which is not learnable.\ndot-product operations, i.e., Qh with KT\nh and the\noutput of softmax with Vh.\nThe row-wise softmax has a specific effect of\nnormalizing each row of the softmax output tensor\nas only several elements have large magnitudes.\nIn contrast, the other majority have very close-to-\nzero magnitudes. Inspired by this phenomenon, we\ncan apply row-wise sparsity to the output of soft-\nmax to help further improve the efficiency of the\nscaled dot-product attention. Inspired by the 2:4\nsparse pattern in sparse Tensor Core, we explore\nthe general N:M structured sparsity, i.e., N values\nfrom every M contiguous elements in a row must\nbe zero2. The sparse Tensor Core has the hard-\nware components to accelerate the compression\nand decompression, while the general N:M sparsity\nimplements the compression and decompression\nwith software. So 2:4 sparsity is more efficient than\ngeneral N:M sparsity implementation with N = 2\nand M = 4. However, if N ≪M, i.e., N\nM < 0.5,\nthe general N:M sparsity can compress the tensor\nas a more compact form, which helps to save more\nmemory traffic and load-store cost. As the output\nof softmax usually has a higher sparse ratio than\n0.5, it is more suitable to apply the N:M sparsity.\nThen the scaled dot-product attention for getting\nthe attention output tensor Ah with N:M sparsity\nenabled can be expressed as follows:\nAh = spN:M\n[\nsoftmax\n( QhKT\nh√\nd\n)]\nVh, (5)\n3.4 Overall GPUSQ-TLM Compression\nOur method utilizes the GPU-friendly structured\nSparsity and Quantization characters to compress\nthe Transformer-based Language Model, so we\nname the compression scheme GPUSQ-TLM.\nGPUSQ-TLM mainly contains structured sparse\npruning and sparse-distillation-combined QAT\nworkflows, as shown in Figure 3. Featured-based\nand logits distillations are applied in each workflow\nas auxiliary accuracy compensation.\nStructured Sparse Pruning aims to compress\nthe dense floating-point model MDF as the sparse\nfloating-point model MSF . Based on the discus-\nsion in subsections 3.2 and 3.3, we can compress\nGEMM-intensive parts of a transformer-based lan-\nguage model according to the GPU-friendly 2:4\nfine-grained structured sparse pattern, and further\ncompress the dot-production in multi-head atten-\ntion modules with N:M sparsity. To best compen-\n2M is a power of 2, and M <= 256, N < M\n222\nFigure 3: GPUSQ-TLM scheme with two sub-workflows. For the structured sparse pruning workflow, the\ndense floating-point model MDF is compressed as the sparse floating-point model MSF . Hard label, soft logits and\nfeature-based distillation losses are accumulated as the overall sparse pruning loss. The sparse floating-point model\nMSF is quantized as the sparse quantized model MSQ for the sparse-distillation-combined QAT workflow. Hard\nlabel, soft logits and feature-based calibration losses are accumulated as the overall quantization calibration loss.\nsate for the accuracy of MSF , we apply knowledge\ndistillation (KD) (Hinton et al., 2015), which can\neffectively transfer the predicted hard label of the\none-hot representation or soft logits of probabilities\nover several classes from a teacher model with ap-\npealing performance to a student model. If the stu-\ndent model wants more supervision, feature-based\nKD is applied to mimic the teacher model’s feature\nmaps. Because we compress the feature maps with\nN:M sparsity in the multi-head attention, in struc-\ntured sparse pruning workflow, three KD strategies\nare jointly used.\nDenoting distillation losses for the hard label,\nsoft logits and feature maps are Lp\nhard, Lp\nsoft ,\nLp\nfeature , respectively, and their weight factors are:\nα,β,γ , then the overall sparse pruning loss Lp is\ncalculated as follows:\nLp = α∗Lp\nhard + β∗Lp\nsoft + γ∗Lp\nfeature (6)\nStructured sparse pruning workflow minimizes the\nLp loss w.r.t weight parameters of MSF model.\nSparse-distillation-combined QAT aims to fur-\nther compress the sparse floating-point modelMSF\nas the sparse quantized model MSQ on data for-\nmat, i.e., quantize from the floating-point formats\nto INT8. We mainly discuss the quantization-\naware training (QAT) strategy for the following\nreasons. From the performance perspective, QAT\ncan achieve the same deployment efficiency with\nthe toolkit (NVIDIA, 2022). From the accuracy\nperspective, QAT learns the scale factor adjust-\nment during training, so the learned scale factor\nleads to less quantization noise and a better ac-\ncuracy compensation effect. Moreover, compres-\nsion by GPU-friendly structured sparsity needs the\npremise (Mishra et al., 2021) to access the training\nset and undergo a fine-tuning process. So we can\nfully utilize the training set and fine-tuning process\nto calibrate the quantization scale factor and boost\nthe accuracy of quantized sparse model.\nWe borrow the KD idea and jointly learn to cali-\nbrate the quantization scale factor from the teacher\nmodel’s hard label prediction, soft logits, and fea-\nture maps from critical layers. Unlike the sparse\npruning workflow in which MDF model serves as\nthe teacher andMSF model serves as the student, in\nthe QAT process,MSF model serves as the teacher,\nand MSQ model serves as the student.3\n3Using the dense floating-point model serves as the teacher\nin the QAT process is not recommended, even though it usu-\nally has better accuracy than the sparse floating-point model.\nBecause based on the previous study (Mirzadeh et al., 2020;\nYu, 2021), the distillation effectiveness will drop if the teacher\nand student models have a noticeable gap in scale or data\nformat.\n223\nDenoting calibration losses for the hard label,\nsoft logits and feature maps are Lc\nhard, Lc\nsoft ,\nLc\nfeature , respectively, and their weight factors are\nstill: α,β,γ , then the overall quantization calibra-\ntion loss Lc is calculated as follows:\nLc = α∗Lc\nhard + β∗Lc\nsoft + γ∗Lc\nfeature (7)\nSparse-distillation-combined QAT minimizes\nthe Lc loss w.r.t weight parameters of MSQ model.\nThe details about each loss item in GPUSQ-TLM\nare provided in Algorithm 1 in Appendix A.2.\n4 Experiments\nFor the experiments in this paper, we choose Py-\nTorch (Paszke et al., 2017) with version 1.12.0 as\nthe framework to implement all algorithms. The\nresults of the dense model training, sparse and QAT\ncompression experiments, and the acceleration per-\nformance are obtained with A100 (NVIDIA, 2020)\nGPU clusters. All the reference algorithms use the\ndefault data type provided in public repositories.\n4.1 Compression Efficacy for Encoder-Only\nTransformer-based Language Model\nTo evaluate the compression efficacy ofGPUSQ-\nTLM and make the comparison with prior arts on\nthe TLM only use the encoder transformer blocks,\nBERT-large and BERT-base (Devlin et al., 2019)4\nare chosen as the target models. For the prior\ncompression arts, we choose DistilBERT (Sanh\net al., 2019), TinyBERT (Jiao et al., 2020), PKD-\nBERT (Sun et al., 2019), MobileBERT (Sun et al.,\n2020), BERT-of-Theseus (Xu et al., 2020), Sparse-\nBERT (Xu et al., 2021), BERT-Tickets (Chen\net al., 2020) and BERT-Surgeon (Kurtic et al.,\n2022) as the reference sparse pruning methods,\nand we choose Q-BERT (Shen et al., 2020) and\nI-BERT (Kim et al., 2021) as the reference quanti-\nzation methods. For GPUSQ-TLM, the loss adjust-\nment factors for hard label, soft logits and feature-\nbased losses apply α= 1, β = 10, and γ = 1. The\ncomparison results are shown in Table 1.\nAll models are evaluated on Stanford Ques-\ntion Answering Dataset (SQuAD) for question an-\nswering with Exact Match (EM) and F1 metrics,\nand General Language Understanding Evaluation\n(GLUE) (Wang et al., 2018) benchmark, which con-\nsists of single-sentence tasks, i.e., CoLA (Warstadt\net al., 2019) and SST-2 (Socher et al., 2013),\nsentence similarity tasks, i.e., MRPC (Dolan\n4https://github.com/NVIDIA/DeepLearningExamples/tree/\nmaster/PyTorch/LanguageModeling/BERT\nand Brockett, 2005), STS-B (Cer et al., 2017),\nQQP, and natural language inference tasks, i.e.,\nMNLI (Williams et al., 2018), QNLI (Rajpurkar\net al., 2016) and RTE (Bentivogli et al., 2009) with\ncorresponding accuracy metrics.\nWe can apply GPUSQ-TLM to compress the\nmodel as sparse GEMM and sparse GEMM-MHA5\nversions. For both versions, the accuracy on\nSQuAD and GLUE benchmarks is almost equal to\nor even better than the naive BERT-base and BERT-\nlarge and better than other models compressed with\nprior arts. Moreover, GPUSQ-TLM compression\ncan significantly boost the deployment efficiency\non A100 GPU with the toolkit (NVIDIA, 2022)\nsupport of structured sparsity and quantization, i.e.,\n4.08-4.25×and 6.18-6.79×improvement of la-\ntency and throughput, apparently better than the\nother models compressed with prior arts.\n4.2 Compression Efficacy for Decoder-Only\nTransformer-based Language Model\nTo evaluate the compression efficacy ofGPUSQ-\nTLM and prior arts on the TLM only use the\ndecoder transformer blocks, OPT (Zhang et al.,\n2022)6 and GPT (Brown et al., 2020) 7 are cho-\nsen as the target models. For the prior arts, we\nchoose SparseGPT (Frantar and Alistarh, 2023)\nas the sparse pruning reference method, and\nwe choose ZeroQuant (Yao et al., 2022) and\nLLM.int8 (Dettmers et al., 2022) as the quanti-\nzation reference methods. For GPUSQ-TLM, the\nloss factors for hard label, soft logits and feature-\nbased losses apply α = 1, β = 10, and γ = 1).\nWe evaluate perplexity (ppl) for all the models on\nWikiText-103 (Merity et al., 2016) test dataset. The\ncomparison results are shown in Table 2.\nFor both GPUSQ-TLMGEMM and GPUSQ-\nTLMGEMM −MHA compressed models, the per-\nplexity on WikiText-103 benchmarks is equal to\nor with small drop than the naive OPT and GPT\nmodels and better than other models compressed\nwith prior arts. Moreover, GPUSQ-TLM can sig-\nnificantly boost the deployment efficiency on A100\nGPU, i.e., 2.46-2.48×and 3.24-3.29×improve-\nment of latency and throughput, apparently better\nthan the other models compressed with prior arts.\n5Because N:M sparsity is not officially support on A100\nGPU, so we need to write the kernel as the software imple-\nmentation for acceleration.\n6https://github.com/facebookresearch/metaseq\n7https://github.com/NVIDIA/Megatron-LM\n224\nModel Name with\nCompression Methods\nSQuAD 1.1 GLUE Speedup\nEM (%) F1 (%)CoLA SST-2 MRPC STS-B QQP MNLI-(m/mm) QNLI RTEBS=1 BS=32\nBERT-base 80.8 88.5 52.1 93.5 88.9 85.8 71.2 84.6/83.4 90.5 66.4 1x 1x\nGPUSQ-TLMGEMM 82.1 89.3 52.2 95.3 89.1 86.4 72.4 85.5/84.5 92.1 66.9 3.73x 4.92x\nGPUSQ-TLMGEMM−MHA 81.9 88.9 51.8 93.2 88.5 85.4 70.9 84.1/83.2 90.2 66.1 4.08x 6.18x\nDistilBERT 79.1 86.9 51.3 91.3 87.5 83.6 69.6 81.6/81.3 88.8 59.9 1.75x 1.93x\nTinyBERT 79.2 86.8 51.1 93.1 87.3 83.7 71.6 84.6/83.2 90.4 66.1 1.75x 1.93x\nPKD-BERT 79.5 87.1 51.3 92.0 85.0 85.2 70.7 81.5/81.0 89.0 65.5 1.75x 1.94x\nBERT-of-Theseus 79.6 87.2 47.8 92.2 87.6 84.1 71.6 82.4/82.1 89.6 66.2 1.73x 1.90x\nSparseBERT 78.2 85.6 48.2 90.4 88.5 82.9 68.9 81.8/80.6 87.4 66.0 1.08x 1.21x\nBERT-Tickets 80.1 87.7 51.3 91.9 88.5 85.4 70.8 84.3/83.2 88.9 66.0 1.02x 1.07x\nBERT-Surgeon 80.7 88.5 51.4 92.2 87.9 83.7 71.1 83.4/82.5 89.2 65.5 1.05x 1.15x\nQ-BERT 80.3 88.3 51.7 92.9 88.4 85.4 70.8 83.9/82.8 90.0 65.9 3.01x 3.38x\nI-BERT 80.5 88.2 52.0 94.1 89.0 85.3 70.8 84.2/83.3 90.3 67.7 3.01x 3.38x\nBERT-large 84.1 90.9 60.5 94.9 89.3 86.5 72.1 86.7/85.9 92.7 70.1 1x 1x\nGPUSQ-TLMGEMM 85.6 91.9 60.9 95.5 89.9 87.1 72.6 87.3/86.5 93.3 70.6 3.85x 5.33x\nGPUSQ-TLMGEMM−MHA 85.8 92.1 60.7 95.1 89.5 86.5 72.1 86.9/86.1 92.9 70.2 4.25x 6.79x\nBERT-Surgeon 84.2 90.7 59.9 93.9 88.4 85.6 71.4 85.8/85.0 91.7 69.4 1.06x 1.16x\nI-BERT 83.9 90.6 60.4 94.9 89.4 86.2 72.0 86.5/85.6 92.5 70.0 3.09x 3.61x\nTable 1: Compression efficacy of GPUSQ-TLM on encoder-only transformer-based language models. GPUSQ-\nTLMGEMM represents the GEMMs operations in models compressed by GPUSQ-TLM method with structured\nsparsity. GPUSQ-TLMGEMM −MHA represents both the GEMMs and the MHA are compressed with structured\nsparsity. The speedup is measured on single A100 GPU (NVIDIA, 2020) with sequence length 384 for each\nmodel. We use the performance of BERT-base and BERT-large as the baselines, and measure the speedup of other\ncompressed models against BERT-base and BERT-large with batch size (BS) equals to 1 & 32, respectively.\nModels WikiText-103 (ppl. Lower the ppl num means better)BaselineGPUSQ GPUSQSparseGPT ZeroQuant LLM.int8-TLMGEMM-TLMGEMM−MHA\nOPT-125M15.0915.17 15.25 20.13 15.40 15.21OPT-1.3B12.7612.81 12.85 15.22 13.19 12.87OPT-2.7B11.0311.07 11.15 11.90 11.25 11.13OPT-6.7B10.3110.32 10.35 10.97 10.52 10.36OPT-13B9.75 9.74 9.80 10.71 9.95 9.79GPT3-125M19.0119.15 19.26 25.35 19.35 19.25GPT3-1.3B10.1910.28 10.37 12.15 10.56 10.40GPT-2.7B9.41 9.48 9.55 10.49 9.93 9.60GPT3-6.7B8.51 8.56 8.62 9.06 8.68 8.57GPT-13B8.02 8.04 8.12 8.80 8.18 8.08SpeedupOPT-13B, BS=11x 2.12x 2.46x 1.07x 1.66x 1.66xOPT-13B, BS=1K1x 3.07x 3.24x 1.25x 1.83x 1.83xGPT-13B, BS=11x 2.12x 2.48x 1.07x 1.68x 1.68xGPT-13B, BS=1k1x 3.11x 3.29x 1.27x 1.85x 1.85x\nTable 2: Compression efficacy of GPUSQ-TLM on\ndecoder-only TLM. The speedup is measured on the\nA100 GPU with sequence length 2048 for each model.\nWe use the performance of OPT-13B and GPT-13B as\nthe baselines, and measure the speedup of other com-\npressed models against OPT-13B and GPT-13B with\nbatch size (BS) equals to 1 & 1024, respectively.\n4.3 Compression Efficacy for Language\nModel with Encoder and Decoder\nTo evaluate the compression efficacy ofGPUSQ-\nTLM on the TLM uses both of encoder and decoder\ntransformer blocks, BART (Lewis et al., 2020)8 is\nchosen as the target model. We evaluate rogue\nscores on the CNN-DM (Nallapati et al., 2016) and\nXSum (Narayan et al., 2018) datasets. The results\nare shown in Table 3.\n4.4 Different N:M Sparse Ratio for MHA\nWe use the BERT-base an BERT-large as examples\nto make an ablation study for different compression\n8https://github.com/NVIDIA/DeepLearningExamples/tree/\nmaster/PyTorch/LanguageModeling/BART\nModels CNN-DM XSum SpeeduprogueLSum rogueLSumBS=1 BS=32\nBART 40.99 36.61 1x 1xGPUSQ-TLM_GEMM41.14 36.73 1.55x 1.72xGPUSQ-TLM_GEMM_MHA40.98 36.56 1.79x 2.06x\nTable 3: Compression efficacy of GPUSQ-TLM on\nencoder-decoder TLM models. The speedup is mea-\nsured for XSum test set on the A100 GPU with source\nlength 1024, target length 60 and beam search 6.\nratios of the multi-head attention (MHA) module.\nComparison results are shown in Figure 4.\nFrom Figure 4, we can find the relative accuracy\ngap between the compressed and dense models en-\nlarges with the increased sparse ratio, i.e., 50%\n(2:4) is the best, followed by 25% (2:8 & 1:4),\nand 12.5% (2:16 & 1:8). Moreover, with the same\nsparse ratio, larger N and M leads to smaller accu-\nracy drop, e.g., with the same 25% sparse ratio, the\nmodel with 2:8 sparsity in multi-head attention has\nbetter accuracy than 1:4, though both sparse pat-\nterns are able to match the dense model’s baseline\nwith the proposed distillation.\nAnother finding is distillation is very helpful for\nthe accuracy maintenance. Without distillation, just\napplying the sparsity and quantization during the\npure finetune stage or both pretrain and finetune\nstages will lead to the accuracy drop from the dense\nmodel. However, with distillation, even if we apply\n2:4 sparsity on GEMMs and N:M sparsity on multi-\nhead attentions, the accuracy can be recovered and\neven slightly better than the dense models.\n225\nFigure 4: Different N:M sparsity for multi-head attention module in BERT-Base and BERT-large for SQuAD task.\nThe delta accuracy metric means the accuracy gap between the compressed model and the dense model divides the\noriginal dense model accuracy.\n4.5 Plug-In Compression Efficacy\nSome prior arts (Sanh et al., 2019; Jiao et al., 2020;\nSun et al., 2019) mainly prune the entire trans-\nformer blocks for TLM without considering the\noperations inside each block. So for these coarse-\ngrained compressed models, GPUSQ-TLM can\nhelp to compress the weights of layers inside each\ntransformer block into the GPU-friendly sparse\npattern and low precision format. The results are\nshown in Table 4. We can find GPUSQ-TLM\nmethod can further accelerate these coarse-grained\ncompressed BERT models on GPU without losing\naccuracy from their coarse-grained forms.\n5 Conclusion\nGPUSQ-TLM is a comprehensive scheme to keep\nthe accuracy to the best with multiple compres-\nsion strategies. The compressed model satisfies\nthe GPU-friendly structured sparsity and quantiza-\nModels withCompression MethodsSQuAD 1.1 GLUE SpeedupEM (%) F1 (%)SST-2 MRPCBS=1 BS=32\nBERT-base 80.8 88.5 93.5 88.9 1x 1xDistilBERT 79.1 86.9 91.3 87.5 1.75x 1.93xDistilBERTGPUSQ 80.1 87.4 91.5 87.7 6.13x 8.34xTinyBERT 79.2 86.8 93.1 87.3 1.75x 1.93xTinyBERTGPUSQ 80.1 87.3 93.3 87.5 6.13x 8.34xPKD-BERT 79.5 87.1 92.0 85.0 1.75x 1.94xPKD-BERTGPUSQ 80.5 87.6 92.2 85.2 6.13x 8.34x\nTable 4: Compression efficacy of plug-in GPUSQ-\nTLM on models compressed by other methods.\ntion characters. With the acceleration of GEMMs\nand MHA modules, GPUSQ-TLM can boost de-\nployment efficiency for TLM models with various\nencoder and decoder structures on GPU with negli-\ngible accuracy degradation on benchmarking tasks.\nLimitations\nWe should point out that the GPUSQ-TLM com-\npression scheme is highly related to the NVIDIA\nGPU’s features to support GPU-friendly 2:4 fine-\n226\ngrained structured sparsity with various data for-\nmats. So if the GPUSQ-TLM compressed mod-\nels are deployed on the different GPU types\nwithout such support, the deployment efficiency\nmay not be as high as expected. For example,\nthe last-generation V100 (NVIDIA, 2017b) and\nT4 (NVIDIA, 2018) GPUs have no support for\nstructured sparsity, so the deployment efficiency is\nlower than A100 (NVIDIA, 2020) GPU.\nWe should also point out NVIDIA AGX Orin\nchip also support GPU-friendly 2:4 fine-grained\nstructured sparsity as A100 GPU and mainly sup-\nport edge device use scenarios like autonomous\ndriving. So, in theory, we can also deploy the\ntransformer-based language models on the AGX\nOrin chip. However, the large language models\nneed to consume large on-chip memory, so they\nusually cannot be held by a single AGX Orin chip.\nFor A100 to represent the server use scenarios, we\ncan use multiple A100 GPUs for parallel execution,\nbut for AGX Orin, we usually only have one chip\nfor the deployment device. That’s why we do not\ntest the GPUSQ-TLM compressed model on the\nAGX Orin chip.\nEthics Statement\nGPUSQ-TLM compression scheme is proven ef-\nfective for various transformer-based language\nmodels with encoder and decoder structures. It\nwill have a broad impact to encourage the study to\nmodel compression and deployment improvement\nin the NLP community.\nWe should also point out that theGPUSQ-TLM\ncompression scheme uses knowledge distillation.\nSo GPUSQ-TLM needs more on-chip memory\nconsumption during the compression process be-\ncause we need a teacher model for distillation. For\ncompressing a huge transformer-based language\nmodel, we may need more GPUs to work in parallel\nto hold both the teacher model and the target model.\nSo GPUSQ-TLM may cost more power consump-\ntion during the compression process, which is not\nenvironment-friendly. But the compressed models\nare more efficient than the original dense model,\nleading to less power consumption during the in-\nference process. Moreover, the time and resources\nspent in model deployment will far outweigh the re-\nsources spent in training over the model’s life. This\npoint turns the time and resource increase from a\nsimple trade-off between training and inference to\na net positive, as overall resource consumption is\nreduced over the whole life of the model.\nAcknowledgements\nThis work is supported by Shanghai Natural Sci-\nence Foundation (No. 23ZR1402900), Shang-\nhai Municipal Science and Technology Major\nProject (No.2021SHZDZX0103), and Zhejiang\nLab Project (No. 2021KH0AB05).\nReferences\nMohamed Arafa, Bahaa Fahim, Sailesh Kottapalli,\nAkhilesh Kumar, Lily P Looi, Sreenivas Mandava,\nAndy Rudoff, Ian M Steiner, Bob Valentine, Geetha\nVedaraman, et al. 2019. Cascade lake: Next gen-\neration intel xeon scalable processor. IEEE Micro,\n39(2):29–36.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC.\nSamuel Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages 632–\n642.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems, 33:1877–1901.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. 2020. End-to-end object detection with\ntransformers. In European Conference on Computer\nVision, pages 213–229. Springer.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The lottery ticket hypothesis for pre-\ntrained bert networks. Advances in Neural Informa-\ntion Processing Systems, 33:15834–15846.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm. int8 (): 8-bit matrix mul-\ntiplication for transformers at scale. arXiv preprint\narXiv:2208.07339.\n227\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of NAACL-HLT, pages 4171–\n4186.\nBill Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Third International Workshop on Paraphrasing\n(IWP2005).\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. In International\nConference on Learning Representations.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022.\nSwitch transformers: Scaling to trillion parame-\nter models with simple and efficient sparsity. The\nJournal of Machine Learning Research, 23(1):5232–\n5270.\nElias Frantar and Dan Alistarh. 2023. Massive language\nmodels can be accurately pruned in one-shot. arXiv\npreprint arXiv:2301.00774.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefficient neural network. In Advances in Neural In-\nformation Processing Systems, pages 1135–1143.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification. In\nProceedings of the 56th Annual Meeting of the Associ-\nation for Computational Linguistics, pages 328–339.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszko-\nreit, Ian Simon, Curtis Hawthorne, Noam Shazeer,\nAndrew M Dai, Matthew D Hoffman, Monica Din-\nculescu, and Douglas Eck. 2018. Music transformer:\nGenerating music with long-term structure. In Inter-\nnational Conference on Learning Representations.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinybert: Distilling bert for natural language under-\nstanding. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 4163–4174.\nNorman P Jouppi, Cliff Young, Nishant Patil, David\nPatterson, Gaurav Agrawal, Raminder Bajwa, Sarah\nBates, Suresh Bhatia, Nan Boden, Al Borchers, et al.\n2017. In-datacenter performance analysis of a tensor\nprocessing unit. In Proceedings of the 44th Annual\nInternational Symposium on Computer Architecture,\npages 1–12.\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W\nMahoney, and Kurt Keutzer. 2021. I-bert: Integer-\nonly bert quantization. In International Conference\non Machine Learning, pages 5506–5518. PMLR.\nEldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Fran-\ntar, Mark Kurtz, Benjamin Fineran, Michael Goin,\nand Dan Alistarh. 2022. The optimal bert surgeon:\nScalable and accurate second-order pruning for large\nlanguage models. arXiv preprint arXiv:2203.07259.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871–7880.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Ang\nLi, Nir Levine, Akihiro Matsukawa, and Hassan\nGhasemzadeh. 2020. Improved knowledge distilla-\ntion via teacher assistant. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 34,\npages 5191–5198.\nAsit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko\nStosic, Dusan Stosic, Ganesh Venkatesh, Chong\nYu, and Paulius Micikevicius. 2021. Accelerat-\ning sparse deep neural networks. arXiv preprint\narXiv:2104.08378.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gulçehre, and Bing Xiang. 2016. Abstractive\ntext summarization using sequence-to-sequence rnns\nand beyond. In Proceedings of The 20th SIGNLL\nConference on Computational Natural Language\nLearning, pages 280–290.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, et al. 2021. Ef-\nficient large-scale language model training on gpu\nclusters using megatron-lm. In Proceedings of the\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1–\n15.\nNVIDIA. 2017a. NVIDIA Tensor Core.\nNVIDIA. 2017b. NVIDIA Tesla V100 GPU.\nNVIDIA. 2018. NVIDIA T4 GPU Accelerator.\n228\nNVIDIA. 2020. NVIDIA A100 Tensor Core GPU.\nNVIDIA. 2022. NVIDIA TensorRT.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin,\nAlban Desmaison, Luca Antiga, and Adam Lerer.\n2017. Automatic differentiation in pytorch. In Ad-\nvances in Neural Information Processing Systems-\nAutodiff Workshop.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth\nGoyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott,\nC Lawrence Zitnick, Jerry Ma, et al. 2021. Biologi-\ncal structure and function emerge from scaling unsu-\npervised learning to 250 million protein sequences.\nProceedings of the National Academy of Sciences ,\n118(15):e2016239118.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nPhilippe Schwaller, Teodoro Laino, Théophile Gaudin,\nPeter Bolgar, Christopher A Hunter, Costas Bekas,\nand Alpha A Lee. 2019. Molecular transformer: a\nmodel for uncertainty-calibrated chemical reaction\nprediction. ACS Central Science, 5(9):1572–1583.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8815–8821.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323–4332.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited de-\nvices. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2158–2170.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems, 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1112–1122.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nHao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev,\nand Paulius Micikevicius. 2020. Integer quantization\nfor deep learning inference: Principles and empirical\nevaluation. arXiv preprint arXiv:2004.09602.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. Bert-of-theseus: Compressing\nbert by progressive module replacing. InProceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing, pages 7859–7869.\nDongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, and Zhibin\nXiao. 2021. Rethinking network pruning–under the\npre-train and fine-tune paradigm. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2376–2382.\nLijuan Yang, Guanghui Yang, Zhitong Bing, Yuan\nTian, Yuzhen Niu, Liang Huang, and Lei Yang. 2021.\nTransformer-based generative model accelerating the\ndevelopment of novel braf inhibitors. ACS Omega,\n6(49):33864–33873.\n229\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,\nXiaoxia Wu, Conglong Li, and Yuxiong He. 2022.\nZeroquant: Efficient and affordable post-training\nquantization for large-scale transformers. arXiv\npreprint arXiv:2206.01861.\nChong Yu. 2021. Minimally invasive surgery for sparse\nneural networks in contrastive manner. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3589–3598.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nAppendix of this paper is in the following pages.\n230\nA Appendix\nIn this Appendix, we will provide some supplemen-\ntary materials and more experimental results for the\nproposed GPUSQ-TLM compression scheme be-\nyond the tight page limitation in manuscript. The\ndetailed outline is as follows.\n• Section A.1 provides the details of fine-\ngrained structured sparsity on GPU. It aims to\nfurther support the contents in Section 3.1 of\nthe manuscript.\n• Section A.2 provides the details and whole\nworkflow of the GPUSQ-TLM algorithm. It\naims to further support the contents in Sec-\ntion 3.4 of the manuscript.\n• Section A.3 provides the details about the\nhyper-parameters settings in experiments. It\naims to further support the contents in Sec-\ntion 4 of the manuscript.\n• Section A.4 provides an ablation study to\nmeasure the influence of the different adjust-\nment factors for the hard label, soft logits, and\nfeature-based losses ( α, β, γ) on GPUSQ-\nTLM compressed model accuracy. It aims to\nfurther support the contents in Section 4 of\nthe manuscript.\nA.1 Fine-grained Structured Sparsity on GPU\nIn this subsection, we first introduce the 2:4 fine-\ngrained structured sparse feature on GPU. Then we\nillustrate its benefit on math efficiency by compar-\ning the same matrix multiplication with and without\nthis sparse feature. And finally, we illustrate how\nto encode to meet the 2:4 fine-grained structured\nsparse pattern and its benefit on memory saving.\nGeneral Matrix Multiplication (GEMM) is the\nfundamental operation inside the common parts\nof TLM models, such as convolution, linear pro-\njection, and multi-head attention blocks. A spe-\ncific acceleration unit called Tensor Core (NVIDIA,\n2017a) was first introduced in NVIDIA V olta\nGPU (NVIDIA, 2017b) to accelerate these GEMM\noperations and further enhanced to support sparse\nGEMM in NVIDIA Ampere GPU architec-\nture (NVIDIA, 2020). To improve the GPU hard-\nware efficiency for sparse GEMM operation, a con-\nstraint named 2:4 fine-grained structured spar-\nsity (Mishra et al., 2021) is imposed on the allowed\nsparsity pattern, i.e., two values from every four\ncontiguous elements on rows must be zero. Due to\nthe 2:4 sparsity support on GPU Tensor Core hard-\nware, sparse GEMM can reduce memory storage\nand bandwidth by almost 2×and provide 2×math\nthroughput compared to dense GEMM by skipping\nthe redundant zero-value computation, as shown\nin Figure 5. NVIDIA Ampere GPU architecture\nsupports various numeric precision for 2:4 sparsity,\nincluding FP32, FP16, INT8, and INT4, etc.\nSparse M✕N✕K GEMMDense M✕N✕K GEMM\nK\nA matrix (Dense)\n☓\nAccumulator (result)\nN\nDense operation on Tensor Core\nM\nB matrix (Dense)\nC matrix (Dense)\nM\nK\nK/2A matrix (Sparse)\nNon-zero data values2-bits indices\nK/2\n☓\nAccumulator (result)\nSparse operation on Tensor Core\nSelect\nB matrix (Dense)\nC matrix (Dense)\nN\nChoose matching K/2 elements out of K elements\nM M\nK\nFigure 5: Comparison of computing a M ×N ×K\nGEMM onto a GPU Tensor Core. Dense matrix A of\nsize M×Kin left side becomes M×K\n2 in right side af-\nter compressing with2:4 fine-grained structured sparse\npattern. GPU sparse Tensor Core automatically picks\nonly the elements from B according to the nonzero el-\nements in A. Comparing the dense and sparse GEMM\noperations, B and C are the same dense K ×N and\nM×N matrices, respectively. By skipping the unneces-\nsary multiplications of redundant zeros, sparse GEMM\naccelerate the dense GEMM with 2×.\nThe sparse GEMM performs thesparse matrix ×\ndense matrix = dense matrix operation by skipping\nthe redundant zero-value computation with sparse\nTensor Core acceleration. For example, matrix A of\nsize M×Kfollows the2:4 fine-grained structured\nsparse pattern, and the dense matrix B is of size\nK×N. If we use the dense GEMM operation to\ncalculate between matrices A and B, the zero values\nin A would not be skipped during computation.\nAssuming that the entireM×N×Kdense GEMM\nwill calculate the result matrix C with M×N size\nin T GPU cycles. If we use the sparse GEMM\noperation, only the non-zero elements in each row\nof matrix A and the corresponding elements from\nmatrix B, which sparse Tensor Core automatically\npicks out without run-time overhead, are calculated.\nSo the entire M×N×Ksparse GEMM will also\ncalculate the same result matrix C with M ×N\nsize but only needs T/2 GPU cycles, i.e. leading to\n2×math throughput speedup.\nTo encode the matrix to meet the 2:4 fine-grained\nstructured sparse pattern, GPU uses 2-bit metadata\nper non-zero element to indicate the position of\ntwo non-zero elements in every four-wide chunk\nof elements in a row. We use an example to il-\nlustrate the storage scenario. For a matrix of only\n231\nFigure 6: Storage formats for 2:4 fine-grained struc-\ntured sparse pattern and metadata with FP16 and INT8\noperators. (x,y denote the non-zero elements.)\nfour elements with FP16 data format, storing as a\ndense pattern requires 4 ×16bits= 64bits, while\nstoring as a 2:4 sparse pattern requires2 ×16bits+\n2 ×2bits = 36bits, leading to 43.75% memory\nsavings in storage. For a matrix of only four ele-\nments with INT8 data format, storing as dense and\n2:4 sparse pattern requires 4 ×8bits= 32bitsand\n2 ×8bits+ 2×2bits= 20bits, respectively, and\nleading to 37.5% memory savings in storage. The\nreal matrices used in GEMM operations usually\ncontain elements with a multiple of four. So the\nstorage of these matrices duplicates the aforemen-\ntioned simple example multiple times. Without loss\nof generality, the conclusion is the 2:4 structured\nsparse pattern with FP16 and INT8 format lead to\n43.75% and 37.5% savings in storage.\nBecause the 2:4 fine-grained structured sparse\npattern is well supported on NVIDIA GPUs and\ncorresponding libraries for math acceleration and\nmemory saving, so we are motivated to design the\ncompression strategy for TLM models to meet\nsuch sparse pattern . Moreover, the 2:4 sparse\nGEMM supports low-precision formats like INT8.\nSo it is natural to combine the sparsity and quanti-\nzation in the proposed strategy jointly. GPUSQ-\nTLM will firstly compress the language models as\na 2:4 sparse pattern with FP16 format, then further\nquantize to a 2:4 sparse INT8 format for boosting\nbest actual deployment efficiency on GPUs.\nA.2 Overall GPUSQ-TLM Compression\nIn GPUSQ-TLM, structured sparse pruning\naims to compress the dense floating-point model\nMDF as the sparse floating-point model MSF .\nSparse-distillation-combined QAT aims to fur-\nther compress the sparse floating-point modelMSF\nas the sparse quantized model MSQ on data format,\ni.e., quantize from float-point data type to integer\ndata type. The details about GPUSQ-TLM are\nprovided in Algorithm 1.\nModels Factor alpha Factor beta Factor gammaSQuAD 1.1GLUEEM (%) F1 (%)SST-2 MRPC\nBERT-base/enc-37 /enc-37 /enc-3780.8 88.593.5 88.9\nGPUSQ-TLMGEMM\n1 10 1 82.1 89.395.3 89.11 0 1 81.3 88.794.0 88.71 10 0 80.6 88.393.1 88.51 5 1 82.0 89.195.1 89.01 20 1 82.1 89.495.4 89.11 10 0.8 81.9 89.095.0 89.01 10 1.2 82.2 89.495.4 89.1BERT-large/enc-37 /enc-37 /enc-3784.1 90.994.9 89.3\nGPUSQ-TLMGEMM\n1 10 1 85.6 91.995.5 89.91 0 1 84.3 91.095.1 89.51 10 0 83.7 90.594.6 88.71 5 1 85.5 91.795.3 89.81 20 1 85.6 92.095.6 89.91 10 0.8 85.5 91.695.3 89.81 10 1.2 85.6 92.095.6 90.0\nTable 5: Ablation study of the loss adjustment factors\nof GPUSQ-TLM method.\nA.3 Hyper-Parameters in Experiments\nFor BERT-large and BERT-base (Devlin et al.,\n2019)9, OPT (Zhang et al., 2022) 10 and GPT\n(Brown et al., 2020) 11, and BART (Lewis et al.,\n2020)12 models, we follow the hyper-parameters\nsettings in public repositories marked by the foot-\nnotes and detailed list in Table 6. Multiple A100\nGPUs are used for data-parallel and pipeline-\nparallel in training or fine-tuning experiments.\nA.4 Ablation study of GPUSQ-TLM\nThe ablation study to measure the influence of\nthe different adjustment factors for the hard la-\nbel, soft logits, and feature-based losses (α, β, γ)\non GPUSQ-TLM compressed model accuracy is\nshown in Table 5.\nBy comparing the ablation results of row 2, row\n3 and row 4 for each model, we find disabling no\nmatter soft logits distillation or feature-based dis-\ntillation will lead to the accuracy degradation. We\ncan also find disabling the feature-based distillation\nwill lead to a more severe influence than disabling\nthe soft logits distillation. It indicates that mim-\nicking feature maps is very helpful for accuracy\ncompensation in GPUSQ-TLM compression.\nFinally, by comparing the ablation results ofrow\n2, row 5 and row 6 for each model we can find\nGPUSQ-TLM is relatively robust to the soft logits\nloss adjustment factor. By comparing the ablation\nresults of row 2, row 7 and row 8 for each model\nwe can find GPUSQ-TLM is also robust to the\nfeature-based loss adjustment factor, i.e., within\nthe close range of β = 10and γ = 1the accuracy\nof compressed models are stable.\n9https://github.com/NVIDIA/DeepLearningExamples/tree/\nmaster/PyTorch/LanguageModeling/BERT\n10https://github.com/facebookresearch/metaseq\n11https://github.com/NVIDIA/Megatron-LM\n12https://github.com/NVIDIA/DeepLearningExamples/tree/\nmaster/PyTorch/LanguageModeling/BART\n232\nAlgorithm 1: GPUSQ-TLM: transformer-based language model joint compression with 2:4 and\nN:M structured sparsity and sparse-distillation-combined QAT\nInput: Dense floating-point model MDF contains Ktransformer blocks, Input tokens x\nData: Distillation temperature t, Loss adjustment factors for hard label, soft logits and feature: α,β,γ , Overall pruning\nloss threshold δprune, Overall calibration loss threshold δcalibrate\nOutput: Sparse quantized model MSQ\n1 /* 2:4 and N:M structured sparse pruning compression workflow */\n2 Initialize sparse floating-point model MSF with the weight parameters from dense floating-point model MDF\n3 while Overall sparse pruning loss: Lp is larger than threshold δprune do\n4 Get feature maps of critical layers from MDF and MSF , e.g., multi-head attention and output of transformer block\ni: FMDF\ntfblocki and FMSF\ntfblocki, and the final projection layer: FMDF\nfproj and FMSF\nfproj\n5 // Calculate feature-based distillation loss with mean-squared-error (MSE) criterion\n6 Lp\nfeature = ∑K\ni=1\n[\nLMSE\n(\nFMDF\ntfblocki,FMSF\ntfblocki\n)]\n+ LMSE\n(\nFMDF\nfproj ,FMSF\nfproj\n)\n7 // Calculate hard label distillation loss with cross entropy (CSE) criterion\n8 if Ground truth labels: labelGround of input images xexists then\n9 Lp\nhard = LCSE (labelGround,MSF (x; T = 1))\n10 else\n11 Lp\nhard = LCSE (MDF (x; T = 1),MSF (x; T = 1))\n12 end\n13 // Calculate soft logits distillation loss with Kullback Leibler divergence (KLD) criterion\n14 Lp\nsoft = LKLD (MDF (x; T = t) ,MSF (x; T = t))\n15 Calculate the overall sparse pruning loss: Lp = α∗ Lp\nhard + β∗ Lp\nsoft + γ∗ Lp\nfeature\n16 Minimize the overall sparse pruning loss w.r.t weight parameters of sparse floating-point modelMSF\n17 end\n18 /* sparse-distillation-combined QAT compression workflow */\n19 Initialize sparse quantized model MSQ by PTQ the weight parameters from trained sparse floating-point model MSF\n20 while Overall quantization calibration loss: Lc is larger than threshold δcalibrate do\n21 Get feature maps of critical layers from MSF and MSQ, e.g., multi-head attention and output of transformer block i:\nFMSF\ntfblocki and F\nMSQ\ntfblocki, and the final projection layer: FMSF\nfproj and F\nMSQ\nfproj\n22 // Calculate feature-based calibration loss with mean-squared-error (MSE) criterion\n23 Lcalibrate\nfeature = ∑K\ni=1\n[\nLMSE\n(\nFMSF\ntfblocki,F\nMSQ\ntfblocki\n)]\n+ LMSE\n(\nFMSF\nfproj ,F\nMSQ\nfproj\n)\n24 // Calculate hard label calibration loss with cross entropy (CSE) criterion\n25 if Ground truth labels: labelGround of input images xexists then\n26 Lc\nhard = LCSE (labelGround,MSQ (x; T = 1))\n27 else\n28 Lc\nhard = LCSE (MSF (x; T = 1),MSQ (x; T = 1))\n29 end\n30 // Calculate soft logits calibration loss with Kullback Leibler divergence (KLD) criterion\n31 Lc\nsoft = LKLD (MSF (x; T = t) ,MSQ (x; T = t))\n32 Calculate the overall quantization calibration loss: Lc = α∗ Lc\nhard + β∗ Lc\nsoft + γ∗ Lc\nfeature\n33 Minimize the overall quantization calibration loss w.r.t weight and scale factor parameters of sparse quantized\nmodel MSQ\n34 end\nModels Task Optimizer Initial LR LR schedule Weight Decay Epochs Batch Size GPU Num\nBERT-base9 SQuAD Adam 3.0e-5 Linear with WarmUp 0.01 4 3 8\nBERT-base9 GELU Adam 2.4e-5 Linear with WarmUp 0.01 6 16 8\nBERT-large9 SQuAD Adam 3.0e-5 Linear with WarmUp 0.01 4 3 8\nBERT-large9 GELU Adam 2.4e-5 Linear with WarmUp 0.01 6 16 8\nOPT-125M10 WikiText-103 AdamW 6.0e-4 Linear with WarmUp 0.01 15 16 32\nOPT-1.3B10 WikiText-103 AdamW 2.0e-4 Linear with WarmUp 0.01 15 16 64\nOPT-2.7B10 WikiText-103 AdamW 1.6e-4 Linear with WarmUp 0.01 15 16 64\nOPT-6.7B10 WikiText-103 AdamW 1.2e-4 Linear with WarmUp 0.01 15 16 128\nOPT-13B10 WikiText-103 AdamW 1.0e-4 Linear with WarmUp 0.01 15 16 256\nGPT-125M11 WikiText-103 AdamW 6.0e-4 Linear with WarmUp 0.01 15 16 32\nGPT-1.3B11 WikiText-103 AdamW 2.0e-4 Linear with WarmUp 0.01 15 16 64\nGPT-2.7B11 WikiText-103 AdamW 1.6e-4 Linear with WarmUp 0.01 15 16 64\nGPT-6.7B11 WikiText-103 AdamW 1.2e-4 Linear with WarmUp 0.01 15 16 128\nGPT-13B11 WikiText-103 AdamW 1.0e-4 Linear with WarmUp 0.01 15 16 256\nBART12 CNN-DM Adam 5.5e-5 Linear with WarmUp 0.01 3 16 8\nBART12 XSum Adam 7.0e-5 Linear with WarmUp 0.01 3 16 8\nTable 6: Experiments hyper-parameters for the transformer-based language models tested in this paper.\n233\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection with name: Limitations.\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection with name: Limitations and another section with name: Ethics Statement.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection with name: Introduction.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nNot applicable. Left blank.\n□ B1. Did you cite the creators of artifacts you used?\nNot applicable. Left blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0013 Did you run computational experiments?\nSection with name: Experiments.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection with name: Experiments in manuscript and section with name: Hyper-Parameters in\nExperiments in Appendix.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n234\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection with name: Experiments.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n235",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7958284616470337
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.7127516269683838
    },
    {
      "name": "Speedup",
      "score": 0.6608904600143433
    },
    {
      "name": "Language model",
      "score": 0.47942131757736206
    },
    {
      "name": "Computer engineering",
      "score": 0.4510549306869507
    },
    {
      "name": "Transformer",
      "score": 0.4460146725177765
    },
    {
      "name": "Encoder",
      "score": 0.4344162940979004
    },
    {
      "name": "Algorithm",
      "score": 0.42834198474884033
    },
    {
      "name": "Parallel computing",
      "score": 0.4084765315055847
    },
    {
      "name": "Computer hardware",
      "score": 0.37551772594451904
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3366376757621765
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    }
  ],
  "cited_by": 4
}