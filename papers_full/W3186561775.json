{
  "title": "DAAI at CASE 2021 Task 1: Transformer-based Multilingual Socio-political and Crisis Event Detection",
  "url": "https://openalex.org/W3186561775",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2988218577",
      "name": "Hansi Hettiarachchi",
      "affiliations": [
        "Birmingham City University"
      ]
    },
    {
      "id": "https://openalex.org/A2320414655",
      "name": "Mariam Adedoyin-Olowe",
      "affiliations": [
        "Birmingham City University"
      ]
    },
    {
      "id": "https://openalex.org/A1985947784",
      "name": "Jagdev Bhogal",
      "affiliations": [
        "Birmingham City University"
      ]
    },
    {
      "id": "https://openalex.org/A1976146371",
      "name": "Mohamed Medhat Gaber",
      "affiliations": [
        "Birmingham City University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3088270371",
    "https://openalex.org/W3096266342",
    "https://openalex.org/W2250668331",
    "https://openalex.org/W3134381300",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W7293280",
    "https://openalex.org/W3101396667",
    "https://openalex.org/W3088226160",
    "https://openalex.org/W2511243365",
    "https://openalex.org/W3170547996",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2165800793",
    "https://openalex.org/W3004178028",
    "https://openalex.org/W2336307793",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3154166606",
    "https://openalex.org/W3116506157",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2912817604",
    "https://openalex.org/W2989622715",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W3071563765",
    "https://openalex.org/W3186179089",
    "https://openalex.org/W4298136482",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W2789202651",
    "https://openalex.org/W2938224028",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3164024373",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W3156955725",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2604444602",
    "https://openalex.org/W3121694563",
    "https://openalex.org/W3184121905"
  ],
  "abstract": "Automatic socio-political and crisis event detection has been a challenge for natural language processing as well as social and political science communities, due to the diversity and nuance in such events and high accuracy requirements. In this paper, we propose an approach which can handle both document and cross-sentence level event detection in a multilingual setting using pretrained transformer models. Our approach became the winning solution in document level predictions and secured the 3rd place in cross-sentence level predictions for the English language. We could also achieve competitive results for other languages to prove the effectiveness and universality of our approach.",
  "full_text": "Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE), pages 120–130\nAugust 5–6, 2021, ©2021 Association for Computational Linguistics\n120\nDAAI at CASE 2021 Task 1: Transformer-based Multilingual\nSocio-political and Crisis Event Detection\nHansi Hettiarachchi, Mariam Adedoyin-Olowe, Jagdev Bhogal,\nMohamed Medhat Gaber\nSchool of Computing and Digital Technology, Birmingham City University, UK\nhansi.hettiarachchi@mail.bcu.ac.uk\n{mariam.adedoyin-olowe,jagdev.bhogal,mohamed.gaber}@bcu.ac.uk\nAbstract\nAutomatic socio-political and crisis event de-\ntection has been a challenge for natural lan-\nguage processing as well as social and politi-\ncal science communities, due to the diversity\nand nuance in such events and high accuracy\nrequirements. In this paper, we propose an ap-\nproach which can handle both document and\ncross-sentence level event detection in a mul-\ntilingual setting using pretrained transformer\nmodels. Our approach became the winning\nsolution in document level predictions and se-\ncured the 3rd place in cross-sentence level pre-\ndictions for the English language. We could\nalso achieve competitive results for other lan-\nguages to prove the effectiveness and univer-\nsality of our approach.\n1 Introduction\nWith technological advancements, today, we have\naccess to a vast amount of data related to social and\npolitical factors. These data may contain informa-\ntion on a wide range of events such as political vi-\nolence, environmental catastrophes and economic\ncrises which are important to prevent or resolve\nconﬂicts, improve the quality of life and protect cit-\nizens. However, with the increasing data volume,\nmanual efforts for event detection have become too\nexpensive making the requirement of automated\nand accurate methods crucial (Hürriyeto˘glu et al.,\n2020).\nConsidering this timely requirement, CASE\n2021 Task 1: Multilingual protest news detection\nis designed (Hürriyeto˘glu et al., 2021). This task is\ncomposed of four subtasks targeting different data\nlevels. Subtask 1 is to identify documents which\ncontain event information. Similarly, subtask 2 is\nto identify event described sentences. Subtask 3\ntargets the cross-sentence level to group sentences\nwhich describe the same event. The ﬁnal subtask is\nto identify the event trigger and its arguments at the\nentity level. Since a news article can contain one\nor more events and a single event can be described\ntogether with some previous or relevant details, it is\nimportant to focus on different data levels to obtain\nmore accurate and complete information.\nThis paper describes our approach for document\nand cross-sentence level event detection including\nan experimental study. Our approach is mainly\nbased on pretrained transformer models. We use\nimproved model architectures, different learning\nstrategies and unsupervised algorithms to make\neffective predictions. To facilitate the effortless\ngeneralisation across the languages, we do not use\nany language-speciﬁc processing or additional re-\nsources. Our submissions achieved the 1st place in\ndocument level predictions and 3rd place in cross-\nsentence level predictions for the English language.\nDemonstrating the universality of our approach, we\ncould obtain competitive results for other languages\ntoo.\nThe remainder of this paper is organised as fol-\nlows. Section 2 describes the related work done in\nthe ﬁeld of socio-political event detection. Details\nof the task and datasets are provided in Section\n3. Section 4 describes the proposed approaches.\nThe experimental setup is described in Section 5\nfollowed by results and evaluation in Section 6.\nFinally, Section 7 concludes the paper. Addition-\nally, we provide our code to the community which\nwill be freely available to everyone interested in\nworking in this area using the same methodology1.\n2 Related Work\nIn early work, the majority of event detection ap-\nproaches were data-driven and knowledge-driven\n(Hogenboom et al., 2011). Since the data-driven\napproaches are only based on the statistics of the\n1The GitHub repository is publicly available on https:\n//github.com/HHansi/EventMiner\n121\nunderlying corpus, they missed the important se-\nmantical relationships. The knowledge-driven or\nrule-based approaches were proposed to tackle this\nlimitation, but they highly rely on the targeted do-\nmains or languages (Danilova and Popova, 2014).\nLater, there was a more focus on traditional ma-\nchine learning-based models (e.g. support vector\nmachines, decision trees) including different fea-\nture extraction techniques (e.g. natural language\nparsing, word vectorisation) (Schrodt et al., 2014;\nSonmez et al., 2016). Also, there was a tendency to\napply deep learning-based approaches (e.g. CNN,\nFFNN) too following their success in many infor-\nmation retrieval and natural language processing\n(NLP) tasks (Lee et al., 2017; Ahmad et al., 2020).\nHowever, these approaches are less expandable to\nlow-resource languages, due to the lack of training\ndata to ﬁne-tune the models.\nTargeting this major limitation, in this paper we\npropose an approach which is based on pretrained\ntransformer models. Due to the usage of general\nknowledge available with the pretrained models\nand their multilingual capabilities, our approach\ncan easily support event detection in multiple lan-\nguages including low-resource languages.\n3 Subtasks and Data\nCASE 2021 Task 1: Multilingual protest news de-\ntection is composed of four subtasks targeting event\ninformation at document, sentence, cross-sentence\nand token levels (Hürriyeto˘glu et al., 2021). Mainly\nthe socio-political and crisis events which are in\nthe scope of contentious politics and characterised\nby riots and social movements are focused. Among\nthese subtasks, we participated in subtask 1 and\nsubtask 3 which are further described below.\nSubtask 1: Document Classiﬁcation Subtask 1\nis designed as a document classiﬁcation task. Par-\nticipants need to predict a binary label of ‘1’ if the\nnews article contains information about a past or\nongoing event and ‘0’ otherwise. To preserve the\nmultilinguality of the task, four different languages\nEnglish, Spanish, Portuguese and Hindi have been\nconsidered for data preparation. Comparatively, a\nhigh number of training instances were provided\nwith English than Spanish and Portuguese. No\ntraining data were provided for the Hindi language.\nFor ﬁnal evaluations, test data were provided with-\nout labels. The data split sizes in each language are\nsummarised in Table 1.\nLanguage Train Test\nEnglish (en) 9324 2971\nSpanish (es) 1000 250\nPortuguese (pt) 1487 372\nHindi (hi) - 268\nTable 1: Data distribution over train and test sets in\nsubtask 1\nSubtask 3: Event Sentence Coreference Identi-\nﬁcation (ESCI) Subtask 3 is targeted at the cross-\nsentence level with the intention to identify the\ncoreference of sentences or sentences about the\nsame event. Given event-related sentences, the\ntargeted output is the clusters which represent sep-\narate events. As training data, per instance, a set\nof sentences and corresponding event clusters were\nprovided as shown below:\n{\"sentence_no\":[1,2,3],\n\"sentences\":[\n\"Maoist banners found 10th\nApril 2011 05:14 AM\nKORAPUT : MAOIST banners\nwere found near the\nDistrict Primary Education\nProject ( DPEP ) office\ntoday in which the ultras\nthreatened to kill Shikhya\nSahayak candidates ,\noutsiders to the district\n, who have been selected\nto join the service here\n.\",\n\"Maoists , in the banners ,\nhave also demanded release\nof hardcore cadre Ghasi\nwho was arrested by police\nearlier this week .\",\n\"Similar banners were also\nfound between Sunki and\nAmpavalli where Maoists\nalso blocked road by\nfelling trees .\"],\n\"event_clusters\":[[1,2],[3]]}\nListing 1: Subtask 3 training data sample\nData from three different languages: English,\nSpanish and Portuguese were provided. A few\ntraining data instances are available with non-\nEnglish languages as summarised in Table 2. Simi-\n122\nlar to subtask 1, test datasets were provided with no\nlabels (event clusters) to use with ﬁnal evaluations.\nLanguage Train Test\nEnglish (en) 596 100\nSpanish (es) 11 40\nPortuguese (pt) 21 40\nTable 2: Data distribution over train and test sets in\nsubtask 3\n4 Methodology\nThe main motivation behind the proposed ap-\nproaches for event document identiﬁcation and\nevent sentence coreference identiﬁcation is the re-\ncent success gained by transformer-based archi-\ntectures in various NLP and information retrieval\ntasks such as language detection (Jauhiainen et al.,\n2021) question answering (Yang et al., 2019) and\noffensive language detection (Husain and Uzuner,\n2021; Ranasinghe and Zampieri, 2021). Apart from\nproviding strong results compared to RNN based\narchitectures, transformer models like BERT (De-\nvlin et al., 2019) and XLM-R (Conneau et al., 2020)\nprovide pretrained language models that support\nmore than 100 languages which is a huge beneﬁt\nwhen it comes to multilingual research. The avail-\nable models have been trained on general tasks like\nlanguage modelling and then can be ﬁne-tuned for\ndownstream tasks like text classiﬁcation (Sun et al.,\n2019). Depending on the nature of the targeted\nsubtask, we involved different transformer models\nalong with different learning strategies to extract\nevent information as mentioned below.\n4.1 Subtask1: Document Classiﬁcation\nDocument classiﬁcation can be considered as a se-\nquence classiﬁcation problem. According to recent\nliterature, transformer architectures have shown\npromising results in this area (Ranasinghe et al.,\n2019b; Hettiarachchi and Ranasinghe, 2020).\nTransformer models take an input of a sequence\nand output the representations of the sequence.\nThe input sequence could contain one or two seg-\nments separated by a special token [SEP]. In this\napproach, we considered a whole document or a\nnews article as a single sequence and no [SEP]\ntoken is used. As the ﬁrst token of the sequence,\nanother special token [CLS] is used and it returns\na special embedding corresponding to the whole\nsequence which is used for text classiﬁcation tasks\n(Sun et al., 2019). A simple softmax classiﬁer is\nadded to the top of the transformer model to predict\nthe probability of a class. The architecture of the\ntransformer-based sequence classiﬁer is shown in\nFigure 1.\nFigure 1: Text Classiﬁcation Architecture\nUnfortunately, the majority of transformer mod-\nels such as BERT (Devlin et al., 2019) and XLM-R\n(Conneau et al., 2020) fails to process documents\nwith a higher sequence length than 512. This limi-\ntation is introduced due to the self-attention op-\neration used by these architectures which scale\nquadratically with the sequence length (Beltagy\net al., 2020). Therefore, we speciﬁcally focused\non improved transformer models targetting long\ndocuments: Longformer (Beltagy et al., 2020) and\nBigBird (Zaheer et al., 2020). Longformer utilises\nan attention mechanism that scales linearly with\nsequence length and BigBird utilises a sparse atten-\ntion mechanism to handle long sequences.\nData Preprocessing: We applied a few prepro-\ncessing techniques to data before inserting them\ninto the models. All the selected techniques are\nlanguage-independent to support multilingual ex-\nperiments. Analysing the datasets, there were doc-\numents with very low sequence length (< 5) and\nthey were removed. Further, URLs were removed\nand repeating symbols more than three times (e.g.\n=====) were replaced by three occurrences (e.g.\n===) because they are uninformative.\n123\n4.2 Subtask3: ESCI\nEvent Sentence Coreference Identiﬁcation (ESCI)\ncan be considered as a clustering problem. If a\nset of sentences are assigned to clusters based on\ntheir semantic similarity, each cluster will represent\nseparate events. To perform clustering, each sen-\ntence needs to be mapped to an embedding which\npreserves its semantic details.\n4.2.1 Sentence Embeddings\nDifferent approaches were proposed to obtain sen-\ntence embeddings by previous research. Based on\nthe word embedding models such as GloVe (Pen-\nnington et al., 2014), the average of word embed-\ndings over a sentence was used. Later, more im-\nproved architectures like InferSent (Conneau et al.,\n2017) which is based on a siamese BiLSTM net-\nwork with max pooling, and Universal Sentence\nEncoder (Cer et al., 2018) which is based on a\ntransformer network and augmented unsupervised\nlearning were developed. However, with the im-\nproved performance on NLP tasks by transformers,\nthere was a tendency to input sentences into mod-\nels like BERT and get the output of the ﬁrst token\n([CLS]) or the average of output layer as a sentence\nembedding (May et al., 2019; Qiao et al., 2019).\nThese approaches were found as worse than aver-\nage GloVe embeddings due to the architecture of\nBERT which was designed targeting classiﬁcation\nor regression tasks (Reimers et al., 2019).\nConsidering these limitations and characteristics\nof transformer-based models, Reimers et al. (2019)\nproposed a new architecture named Sentence Trans-\nformer (STransformer), a modiﬁcation to the trans-\nformers to derive semantically meaningful sentence\nembeddings. According to the experimental stud-\nies, STransformers outperformed average GloVe\nembeddings, specialised models like InferSent and\nUniversal Sentence Encoder, and BERT embed-\ndings (Reimers et al., 2019). Considering these\nfacts, we adopt STransformers to generate sentence\nembeddings in our approach.\nSTransformer creates a siamese network using\ntransformer models like BERT to ﬁne-tune the\nmodel to produce effective sentence embeddings.\nA pooling layer is added to the top of the trans-\nformer model to generate ﬁxed-sized embeddings\nfor sentences. The siamese network takes a sen-\ntence pair as the input and passes them through\nthe network to generate embeddings (Ranasinghe\net al., 2019a). Then compute the similarity between\nembeddings using cosine similarity and compare\nthe value with the gold score to ﬁne-tune the net-\nwork. The architecture of STransformer is shown\nin Figure 2.\nFigure 2: Siamese Sentence Transformer (STrans-\nformer) Architecture\nData Formatting: To facilitate the STrans-\nformer ﬁne-tuning or training, we formatted given\nsentences into pairs and assigned the similarity of\n‘1’ if both sentences belong to the same cluster\nand ‘0’ if not. During the pairing, the order of\nsentences is not considered. Thus, for n sentences,\n(n × (n − 1))/2 pairs were generated. For exam-\nple, sentence pairs and labels generated for the data\nsample given in Listing 1 are shown in Table 3.\nSentence 1 Sentence 2 Label\n1 2 1\n1 3 0\n2 3 0\nTable 3: Sentence pairs and labels of data sample in\nListing 1\n4.2.2 Clustering\nAs clustering methods, we focused on hierarchi-\ncal clustering and the pairwise prediction-based\nclustering approach proposed by Örs et al. (2020).\nHierarchical clustering is widely used with event\ndetection approaches over ﬂat clustering because\nﬂat clustering algorithms (e.g. K-means) require\nthe number of clusters as an input which is unpre-\ndictable (Hettiarachchi et al., 2021). Considering\nthe availability of training data and recent success-\nful applications, the pairwise prediction-based clus-\ntering approach is focused.\nHierarchical Clustering: For the hierarchical\nclustering algorithm, we used Hierarchical Ag-\nglomerative Clustering (HAC). Each sentence is\n124\nconverted into embeddings to input to the cluster-\ning algorithm. HAC considers all data points as sep-\narate clusters at the beginning and then merge them\nbased on cluster distance using a linkage method.\nThe tree-like diagram generated by this process is\nknown as a dendrogram and a particular distance\nthreshold is used to cut it into clusters (Manning\net al., 2008). For the distance metric, cosine dis-\ntance is used, because it proved to be effective for\nmeasurements in textual data (Mikolov et al., 2013;\nAntoniak and Mimno, 2018) and a variant of it\nis used with STransformer models. For the link-\nage method, single, complete and average schemes\nwere considered for initial experiments and the av-\nerage scheme was selected among them because it\noutperformed others. We picked the optimal dis-\ntance threshold automatically using the training\ndata. If training data is further split into training\nand validation sets to use with STransformers, only\nthe validation set is used to pick the cluster thresh-\nold, because the rest of the data is known to the\nembedding generated model.\nPairwise Prediction-based Clustering: We\nused the pairwise prediction-based clustering\nalgorithm proposed by Örs et al. (2020) which\nbecame the winning solution of the ESCI task in\nthe AESPEN-2020 workshop (Hürriyeto˘glu et al.,\n2020). Originally this algorithm used the BERT\nmodel to predict whether a certain sentence pair\nbelongs to the same event or not. In this research,\nwe used STransformers to make those predictions\nexcept general transformers. Since a STransformer\nmodel is designed to obtain embeddings, to derive\nlabels (i.e. ‘1’ if the sentence pair belong to the\nsame event and ‘0’ if not) from them we used\ncosine similarity with a threshold. The optimal\nvalue computed during the model evaluation\nprocess is used as the threshold.\n5 Experimental Setup\nThis section describes the learning conﬁgurations,\ntransformer models and hyper-parameters used for\nthe experiments.\n5.1 Learning Conﬁgurations\nWe focused on different learning conﬁgurations de-\npending on data and model availability, and multi-\nlingual setting. Considering the availability of data\nand models, we used the following conﬁgurations\nfor the experiments.\nPretrained (No Learning): Pretrained models\nare used without making any modiﬁcations to them\nto make the predictions. In this case, models pre-\ntrained using a similar objective to the target objec-\ntive need to be selected.\nFine-tuning: Under ﬁne-tuning, we retrain an\navailable model to a downstream task or the same\ntask model already trained. This learning allows\nthe model to be familiar with the targeted data.\nFrom-scratch Learning: Models are built from\nscratch using the targeted data. This procedure\nhelps to mitigate the unnecessary biases made by\nthe data used to train available models.\nLanguage Modelling (LM): In LM, we retrain\nthe transformer model on the targeted dataset\nusing the model’s initial training objective before\nﬁne-tuning it for the downstream task. This step\nhelps increase the model understanding of data\n(Hettiarachchi and Ranasinghe, 2020).\nFor multilingual data, the following conﬁgurations\nare considered to support both high- and low-\nresource languages.\nMonolingual Learning: In monolingual learn-\ning, we build the model from the training data only\nfrom that particular language.\nMultilingual Learning: In multilingual learn-\ning, we concatenate available training data from all\nlanguages and build a single model.\nZero-shot Learning: In zero-shot learning, we\nuse the models ﬁne-tuned for the same task using\ntraining data from other language(s) to make the\npredictions. The multilingual and cross-lingual na-\nture of the transformer models has provided the\nability to do this (Ranasinghe et al., 2020; Het-\ntiarachchi and Ranasinghe, 2021).\n5.2 Transformers\nWe used monolingual and multilingual general\ntransformers as well as pretrained STransformers\nfor our experiments.\nGeneral Transformers: As monolingual mod-\nels, we used transformer models built for each of\nthe targeted languages. For English, BigBird (Za-\nheer et al., 2020), Longformer (Beltagy et al., 2020)\nand BERT English (Devlin et al., 2019) models\nwere considered. For Spanish, BETO (Canete et al.,\n2020) and for Portuguese, BERTimbau (Souza\n125\nSeq. Length Model Macro R Macro P Macro F1\n256\nBERT-large-cased 0.8717 0.8489 0.8595\nBigBird-roberta-large 0.8790 0.9119 0.8941‡\nLongformer-base 0.8800 0.8868 0.8833\n512\nBERT-large-cased 0.8697 0.8683 0.8690\nBigBird-roberta-base 0.8763 0.9018 0.8882 ‡\nLongformer-base 0.8608 0.9100 0.8824 ‡\n700\nBigBird-roberta-base 0.8770 0.8807 0.8788\nLongformer-base 0.8748 0.8846 0.8796\nTable 4: Results: Macro Recall (R), Precision (P) and F1 of document classiﬁcation experiments for English using\ndifferent sequence lengths and models. Best is in Bold and submitted systems are marked with ‡.\nModel Training Data Macro R Macro P Macro F1\nEnglish BERT-multilingual-cased en+es+pt 0.8505 0.8567 0.8536\nXLM-R-base en+es+pt 0.8280 0.8727 0.8476\nSpanish\nBETO-cased es 0.6944 0.8681 0.7475 ‡\nBERT-multilingual-cased es NT NT NT\nBERT-multilingual-cased en+es+pt 0.7831 0.8111 0.7962 ‡\nXLM-R-base es NT NT NT\nXLM-R-base en+es+pt 0.7888 0.8530 0.8167‡\nPortuguese\nBERTimbau-large pt 0.7672 0.8900 0.8126 ‡\nBERT-multilingual-cased pt 0.7595 0.8331 0.7896\nBERT-multilingual-cased en+es+pt 0.8384 0.8890 0.8611‡\nXLM-R-base pt NT NT NT\nXLM-R-base en+es+pt 0.7845 0.8449 0.8104 ‡\nTable 5: Results of multilingual document classiﬁcation experiments. Training Data column summarises the lan-\nguage(s) of used datasets to train models. Due to training data limitations, a few models were found to be not\ntrainable and they are indicated with NT. Best is in Bold and submitted systems are marked with ‡.\net al., 2020) models which are variants of the BERT\nmodel were considered. As multilingual models,\nBERT multilingual version and XLM-R (Conneau\net al., 2020) models were used. Among these mod-\nels, a higher sequence length than 512 is only sup-\nported by BigBird and Longformer models avail-\nable for English. We used HuggingFace’s Trans-\nformers library (Wolf et al., 2020) to obtain the\nmodels.\nSentence Transformers: STransformers pro-\nvide pretrained models for different tasks2. Among\nthem, we selected the best-performed models\ntrained for semantic textual similarity (STS) and\nduplicate question identiﬁcation, because these ar-\neas are related to the same event prediction.\n5.3 Hyper-parameter Conﬁgurations\nWe used a Nvidia Tesla K80 GPU to train the mod-\nels. Each input dataset is divided into a training\n2Sentence Transformer pretrained models are available\non https://www.sbert.net/docs/pretrained_\nmodels.html\nset and a validation set using a 0.9:0.1 split. We\npredominantly ﬁne-tuned the learning rate and the\nnumber of epochs of the model manually to obtain\nthe best results for the validation set. For docu-\nment classiﬁcation, we obtained 1e−5 as the best\nvalue for the learning rate and 3 as the best value\nfor the number of epochs. The same learning rate\nwas found as the best value for STransformers with\nepochs of 5. For the sequence length, different\nvalues have experimented with document classiﬁ-\ncation and they are further discussed in Section 6.1.\nA ﬁxed sequence length of 136 was used for ESCI\nconsidering its data.\nTo improve the performance of document classi-\nﬁcation, we used the majority-class self-ensemble\napproach mentioned in (Hettiarachchi and Ranas-\ninghe, 2020). During the training, we trained three\nmodels with different random seeds and considered\nthe majority-class returned by the models as the\nﬁnal prediction.\nTo train STransformers, we selected the online\ncontrastive loss, an improved version of the con-\n126\nModel Training Data Seq. Length Macro F1\nEnglish\nBest System 0.8455BigBird-roberta-large en 256\nBigBird-roberta-base en 512 0.8220\nSpanish\nBest System 0.7727\nXLM-R-base en+es+pt 512 0.6931\nBERT-multilingual-cased en+es+pt 512 0.6886\nPortuguese\nBest System 0.8400\nXLM-R-base en+es+pt 512 0.8243\nBERT-multilingual-cased en+es+pt 512 0.7982\nHindi\nBest System 0.7877\nXLM-R-base en+es+pt 512 0.7707\nBERT-multilingual-cased en+es+pt 512 0.4647\nTable 6: Document classiﬁcation results for test data\ntrastive loss function. The contrastive loss func-\ntion learns the parameters by reducing the distance\nbetween neighbours or semantically similar em-\nbeddings and increasing the distance between non-\nneighbours or semantically dissimilar embeddings\n(Hadsell et al., 2006). The online version automati-\ncally detects the hard cases (i.e. negative pairs with\na low distance than the largest distance of positive\npairs and positive pairs with a high distance than\nthe lowest distance of negative pairs) in a batch and\ncalculates the loss only for them.\n6 Results and Evaluation\nIn this section, we report the conducted experi-\nments and their results.\n6.1 Subtask1: Document Classiﬁcation\nTask organisers used Macro F1 as the evaluation\nmetric for subtask 1. Since only the training data\nwere released, we separated a dev set from each\ntraining dataset to evaluate our approach. Depend-\ning on the data size, 20% from English and 10%\nfrom other-language training data were separated\nas dev data.\nInitially, we analysed the performance of ﬁne-\ntuned document classiﬁers for English using BERT\nand improved transformer models for long docu-\nments, along with varying sequence length. Consid-\nering the sequence length distribution in data, we\npicked the lengths of 256, 512 and 700 for these ex-\nperiments. The obtained results are summarised in\nTable 4. Even though we targeted large versions of\nthe models (e.g. BigBird-roberta-large), due to the\nresource limitations, we had to use base versions\n(e.g. BigBird-roberta-base) for some experiments.\nAccording to the results, BERT models improve\nthe F1 when we increase the sequence length. In\ncontrast to it, both BigBird and Longformer models\nhave higher F1 with low sequence lengths.\nFor predictions in Spanish and Portuguese doc-\numents, we ﬁne-tuned the models using both\nmonolingual and multilingual learning approaches.\nSince transformers with the maximum sequence\nlength of 512 are used, we ﬁxed the sequence\nlength to 512 based on the ﬁndings in English ex-\nperiments. The obtained results and training con-\nﬁgurations are summarised in Table 5. For the\nhigh-resource language (i.e. English), multilin-\ngual learning returns a low F1 than monolingual\nlearning. However, low-resource languages show\na clear improvement in F1 with multilingual learn-\ning. Since there were no training data for the Hindi\nlanguage, the best multilingual models were picked\nto apply the zero-shot learning approach.\nWe report the results we obtained for test data\nin Table 6. According to the results, our approach\nwhich used the BigBird model became the best sys-\ntem for the English language. For other languages,\nmultilingual learning performed best. Among mod-\nels, XLM-R outperformed the BERT-multilingual\nmodel. Compared to the best systems submitted,\nour approach has very competitive results for these\nlanguages too.\n6.2 Subtask3: ESCI\nTo evaluate subtask 3 responses, organisers used\nCoNLL-2012 average score3 (Pradhan et al., 2014).\nSimilar to subtask 1, for evaluation purpose, we\nseparated 20% from the English training dataset\nas dev data. There were no sufﬁcient data in other\n3The implementation of the scorer is available on https:\n//github.com/LoicGrobol/scorch\n127\nBase Model STransformer Clustering CoNLL Average Score\nPretrained DistilBERT-base-uncased quora-distilbert-base HAC 0.8360\nMPNet-base stsb-mpnet-base-v2 HAC 0.8360\nFine-tune\nDistilBERT-base-uncased quora-distilbert-base HAC 0.8392\nDistilBERT-base-uncased quora-distilbert-base (Örs et al., 2020) 0.8376\nMPNet-base stsb-mpnet-base-v2 HAC 0.8370\nMPNet-base stsb-mpnet-base-v2 (Örs et al., 2020) 0.8264\nFrom-scratch BERT-large-cased - HAC 0.8688‡\nBERT-large-cased - (Örs et al., 2020) 0.8656 ‡\nLM +\nFrom-scratch\nBERT-large-cased - HAC 0.8543 ‡\nBERT-large-cased - (Örs et al., 2020) 0.8328\nTable 7: Results of ESCI for English along with different strategies experimented. Best is in Bold and submitted\nsystems are marked with ‡.\nBase Model STransformer Clustering CoNLL Average Score\nPretrained DistilBERT-base-uncased quora-distilbert-multilingual HAC 0.8360\nFine-tune DistilBERT-base-uncased quora-distilbert-multilingual HAC 0.8423 ‡\nDistilBERT-base-uncased quora-distilbert-multilingual (Örs et al., 2020) 0.8362\nFrom-scratch\nBERT-multilingual-cased - HAC 0.8464‡\nBERT-multilingual-cased - (Örs et al., 2020) 0.8414\nXLM-R-large - HAC 0.8360\nXLM-R-large - (Örs et al., 2020) 0.8350\nTable 8: Results of ESCI for English using multilingual models. Best is in Bold and submitted systems are marked\nwith ‡.\nlanguages for further splits.\nFor the English language, we experimented with\nthe clustering approaches using the embeddings\ngenerated by different STransformer models. Ini-\ntially, we focused on pretrained models and their\nﬁne-tuned versions on task data. Later we built\nSTransformers from scratch using general trans-\nformer models and further integrated LM too. The\nobtained results and corresponding model details\nare summarised in Table 7. According to the results,\nSTransformers build from scratch outperformed the\npretrained and ﬁne-tuned models. LM did not im-\nprove the results and it is possible when data is\nnot enough for modelling. Among the clustering\nalgorithms, HAC showed the best results.\nWe could not train any STransformer for other\nlanguages because the organisers provided a lim-\nited number of labelled instances for those lan-\nguages. We used pretrained multilingual models\nand adhering to zero-shot learning, ﬁne-tuned them\nusing English data. Further English data were used\nto build STransformers from scratch too. All the\nevaluations were also done on English data and\nbest-performing systems were chosen to make pre-\ndictions for other languages. The obtained results\nare summarised in Table 8. Similar to the English\nmonolingual scenario, from-scratch multilingual\nmodels performed best.\nWe report the results for test data in Table 9. Ac-\ncording to the results, for all languages, we could\nobtain competitive results compared to the results\nof the best-submitted system. Since our approach\ncan be easily extended to different languages with\nvery few training instances, we believe the results\nare at a satisfactory level.\n7 Conclusions\nIn this paper, we presented our approach for doc-\nument and cross-sentence level subtasks of CASE\n2021 Task 1: Multilingual protest news detection.\nWe mainly used pretrained transformer models in-\ncluding their improved architectures for long docu-\nment processing and sentence embedding genera-\ntion. Further, different learning strategies: mono-\nlingual, multilingual and zero-shot and, classiﬁca-\ntion and clustering approaches were involved. For\ndocument level predictions, our approach achieved\nthe 1st place for the English language while being\nwithin the top 4 solutions for other languages. For\ncross-sentence level predictions, we secured the\n128\nModel Clustering CoNLL Average Score\nEnglish\nBest System 0.8444\nBERT-large-casedfrom−scratch HAC 0.8040\nBERT-large-casedfrom−scratch (Örs et al., 2020) 0.7951\nSpanish\nBest system 0.8423\nquora-distilbert-multilingualfine−tune(en) HAC 0.8183\nBERT-multilingual-casedfrom−scratch(en) HAC 0.8167\nPortuguese\nBest System 0.9303\nquora-distilbert-multilingualfine−tune(en) HAC 0.9023\nBERT-multilingual-casedfrom−scratch(en) HAC 0.9023\nTable 9: ESCI results for test data\n3rd place for the English language with competi-\ntive results for other languages. Despite that, our\napproach can support multiple languages with low\nor no training resources.\nAs future work, we hope to further improve se-\nmantically meaningful sentence embedding gener-\nation using improved architectures, learning strate-\ngies and ensemble methods. Also, we would like\nto analyse the impact of different clustering ap-\nproaches on cross-sentence level predictions.\nReferences\nFaizan Ahmad, Ahmed Abbasi, Brent Kitchens, Don-\nald A Adjeroh, and Daniel Zeng. 2020. Deep learn-\ning for adverse event detection from web search.\nIEEE Transactions on Knowledge and Data Engi-\nneering.\nMaria Antoniak and David Mimno. 2018. Evaluating\nthe stability of embedding-based word similarities.\nTransactions of the Association for Computational\nLinguistics, 6:107–119.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nJosé Canete, Gabriel Chaperon, Rodrigo Fuentes, and\nJorge Pérez. 2020. Spanish pre-trained bert model\nand evaluation data. PML4DC at ICLR, 2020.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Céspedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder. arXiv\npreprint arXiv:1803.11175.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670–680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nVera Danilova and Svetlana Popova. 2014. Socio-\npolitical event extraction using a rule-based ap-\nproach. In OTM Confederated International Con-\nferences\" On the Move to Meaningful Internet Sys-\ntems\", pages 537–546. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. 2006.\nDimensionality reduction by learning an invariant\nmapping. In 2006 IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR’06), volume 2, pages 1735–1742. IEEE.\nHansi Hettiarachchi, Mariam Adedoyin-Olowe, Jagdev\nBhogal, and Mohamed Medhat Gaber. 2021. Em-\nbed2detect: Temporally clustered embedded words\nfor event detection in social media. Machine Learn-\ning, pages 1–39.\nHansi Hettiarachchi and Tharindu Ranasinghe. 2020.\nInfoMiner at WNUT-2020 task 2: Transformer-\nbased covid-19 informative tweet extraction. In\nProceedings of the Sixth Workshop on Noisy User-\ngenerated Text (W-NUT 2020), pages 359–365, On-\nline. Association for Computational Linguistics.\nHansi Hettiarachchi and Tharindu Ranasinghe. 2021.\nTransWiC at SemEval-2021 Task 2: Transformer-\nbased Multilingual and Cross-lingual Word-in-\nContext Disambiguation. In Proceedings of Se-\nmEval.\n129\nFrederik Hogenboom, Flavius Frasincar, Uzay Kay-\nmak, and Franciska De Jong. 2011. An overview\nof event extraction from text. In DeRiVE@ ISWC,\npages 48–57. Citeseer.\nAli Hürriyeto ˘glu, Osman Mutlu, Farhana Ferdousi\nLiza, Erdem Yörük, Ritesh Kumar, and Shyam\nRatan. 2021. Multilingual protest news detection -\nshared task 1, case 2021. In Proceedings of the 4th\nWorkshop on Challenges and Applications of Auto-\nmated Extraction of Socio-political Events from Text\n(CASE 2021), online. Association for Computational\nLinguistics (ACL).\nAli Hürriyeto ˘glu, Vanni Zavarella, Hristo Tanev, Er-\ndem Yörük, Ali Safaya, and Osman Mutlu. 2020.\nAutomated extraction of socio-political events from\nnews (aespen): Workshop and shared task report. In\nProceedings of the Workshop on Automated Extrac-\ntion of Socio-political Events from News 2020, pages\n1–6.\nFatemah Husain and Ozlem Uzuner. 2021. Leveraging\noffensive language for sarcasm and sentiment detec-\ntion in arabic. In Proceedings of the Sixth Arabic\nNatural Language Processing Workshop, pages 364–\n369.\nTommi Jauhiainen, Tharindu Ranasinghe, and Marcos\nZampieri. 2021. Comparing approaches to dravid-\nian language identiﬁcation. In Proceedings of the\n7th Workshop on NLP for Similar Languages, Vari-\neties and Dialects.\nKathy Lee, Ashequl Qadir, Sadid A Hasan, Vivek\nDatla, Aaditya Prakash, Joey Liu, and Oladimeji\nFarri. 2017. Adverse drug event detection in tweets\nwith semi-supervised convolutional neural networks.\nIn Proceedings of the 26th international conference\non world wide web, pages 705–714.\nChristopher D Manning, Prabhakar Raghavan, and Hin-\nrich Schütze. 2008. Introduction to information re-\ntrieval. Cambridge university press.\nChandler May, Alex Wang, Shikha Bordia, Samuel\nBowman, and Rachel Rudinger. 2019. On measur-\ning social biases in sentence encoders. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 622–628.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nFaik Kerem Örs, Süveyda Yeniterzi, and Reyyan Yen-\niterzi. 2020. Event clustering within news articles.\nIn Proceedings of the Workshop on Automated Ex-\ntraction of Socio-political Events from News 2020,\npages 63–68.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nSameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-\nuard Hovy, Vincent Ng, and Michael Strube. 2014.\nScoring coreference partitions of predicted men-\ntions: A reference implementation. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 30–35, Baltimore, Maryland. Associa-\ntion for Computational Linguistics.\nYifan Qiao, Chenyan Xiong, Zhenghao Liu, and\nZhiyuan Liu. 2019. Understanding the behaviors of\nbert in ranking. arXiv preprint arXiv:1904.07531.\nTharindu Ranasinghe, Constantin Orasan, and Ruslan\nMitkov. 2019a. Semantic textual similarity with\nSiamese neural networks. In Proceedings of the In-\nternational Conference on Recent Advances in Nat-\nural Language Processing (RANLP 2019), pages\n1004–1011, Varna, Bulgaria. INCOMA Ltd.\nTharindu Ranasinghe, Constantin Orasan, and Ruslan\nMitkov. 2020. TransQuest: Translation quality esti-\nmation with cross-lingual transformers. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5070–5081, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nTharindu Ranasinghe and Marcos Zampieri. 2021.\nMUDES: Multilingual detection of offensive spans.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies:\nDemonstrations, pages 144–152, Online. Associa-\ntion for Computational Linguistics.\nTharindu Ranasinghe, Marcos Zampieri, and Hansi\nHettiarachchi. 2019b. BRUMS at HASOC 2019:\nDeep learning models for multilingual hate speech\nand offensive language identiﬁcation. In In Proceed-\nings of the 11th annual meeting of the Forum for In-\nformation Retrieval Evaluation.\nNils Reimers, Iryna Gurevych, Nils Reimers, Iryna\nGurevych, Nandan Thakur, Nils Reimers, Johannes\nDaxenberger, and Iryna Gurevych. 2019. Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics.\nPhilip A Schrodt, John Beieler, and Muhammed Idris.\n2014. Three’sa charm?: Open event data coding\nwith el: Diablo, petrarch, and the open event data\nalliance. In ISA Annual Convention. Citeseer.\nCagil Sonmez, Arzucan Özgür, and Erdem Yörük.\n2016. Towards building a political protest database\nto explain changes in the welfare state. In Proceed-\nings of the 10th SIGHUM Workshop on Language\n130\nTechnology for Cultural Heritage, Social Sciences,\nand Humanities, pages 106–110.\nFábio Souza, Rodrigo Nogueira, and Roberto Lotufo.\n2020. BERTimbau: pretrained BERT models for\nBrazilian Portuguese. In 9th Brazilian Conference\non Intelligent Systems, BRACIS, Rio Grande do Sul,\nBrazil, October 20-23 (to appear).\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune bert for text classiﬁcation?\nIn Chinese Computational Linguistics, pages 194–\n206, Cham. Springer International Publishing.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nBERTserini. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics (Demonstra-\ntions), pages 72–77, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. arXiv preprint arXiv:2007.14062.",
  "topic": "Politics",
  "concepts": [
    {
      "name": "Politics",
      "score": 0.6434358358383179
    },
    {
      "name": "Transformer",
      "score": 0.587171196937561
    },
    {
      "name": "Computer science",
      "score": 0.5412243604660034
    },
    {
      "name": "Task (project management)",
      "score": 0.5165044665336609
    },
    {
      "name": "Event (particle physics)",
      "score": 0.48660242557525635
    },
    {
      "name": "Political science",
      "score": 0.4225444197654724
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35074537992477417
    },
    {
      "name": "Law",
      "score": 0.17823007702827454
    },
    {
      "name": "Engineering",
      "score": 0.15081650018692017
    },
    {
      "name": "Voltage",
      "score": 0.08683240413665771
    },
    {
      "name": "Electrical engineering",
      "score": 0.08488988876342773
    },
    {
      "name": "Physics",
      "score": 0.05557677149772644
    },
    {
      "name": "Systems engineering",
      "score": 0.05492642521858215
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12870472",
      "name": "Birmingham City University",
      "country": "GB"
    }
  ]
}