{
  "title": "Efficient Classification of Long Documents Using Transformers",
  "url": "https://openalex.org/W4285261371",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2163577848",
      "name": "Hyunji Park",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129752654",
      "name": "Yogarshi Vyas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117207849",
      "name": "Kashif Shah",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2955041501",
    "https://openalex.org/W3008736151",
    "https://openalex.org/W1540841088",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W2962910668",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W3103677861",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2963159735",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1525595230",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3156333129",
    "https://openalex.org/W1493526108"
  ],
  "abstract": "Several methods have been proposed for classifying long textual documents using Transformers. However, there is a lack of consensus on a benchmark to enable a fair comparison among different approaches. In this paper, we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets — both in terms of accuracy as well as time and space overheads. Our datasets cover binary, multi-class, and multi-label classification tasks and represent various ways information is organized in a long text (e.g. information that is critical to making the classification decision is at the beginning or towards the end of the document). Our results show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets. These findings emphasize the need for future studies to consider comprehensive baselines and datasets that better represent the task of long document classification to develop robust models.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 702 - 709\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nEfficient Classification of Long Documents Using Transformers\nHyunji Hayley Park\nUniversity of Illinois∗\nhpark129@illinois.edu\nYogarshi Vyas\nAWS AI Labs\nyogarshi@amazon.com\nKashif Shah\nMicrosoft∗\nkashifshah@microsoft.com\nAbstract\nSeveral methods have been proposed for clas-\nsifying long textual documents using Trans-\nformers. However, there is a lack of consensus\non a benchmark to enable a fair comparison\namong different approaches. In this paper, we\nprovide a comprehensive evaluation of the rela-\ntive efficacy measured against various baselines\nand diverse datasets — both in terms of accu-\nracy as well as time and space overheads. Our\ndatasets cover binary, multi-class, and multi-\nlabel classification tasks and represent various\nways information is organized in a long text\n(e.g. information that is critical to making the\nclassification decision is at the beginning or\ntoward the end of the document). Our results\nshow that more complex models often fail to\noutperform simple baselines and yield incon-\nsistent performance across datasets. These find-\nings emphasize the need for future studies to\nconsider comprehensive baselines and datasets\nthat better represent the task of long document\nclassification to develop robust models.1\n1 Introduction\nTransformer-based models (Vaswani et al., 2017)\nhave achieved much progress across many ar-\neas of NLP including text classification (Minaee\net al., 2021). However, such progress is often\nlimited to short sequences because self-attention\nrequires quadratic computational time and space\nwith respect to the input sequence length. Widely-\nused models like BERT (Devlin et al., 2019) or\nRoBERTa (Liu et al., 2019) are typically pretrained\nto process up to 512 tokens. This is problematic\nbecause real-world data can be arbitrarily long. As\nsuch, different models and strategies have been\nproposed to process longer sequences.\nIn particular, we can identify a few standard ap-\nproaches for the task of long document classifi-\n∗Work done while at Amazon\n1Our code is available at\nhttps://github.com/amazon-research/\nefficient-longdoc-classification.\ncation. The simplest approach is to truncate long\ndocuments — using BERT or RoBERTa on the first\n512 tokens is often used as a baseline. More effi-\ncient Transformer models like Longformer (Belt-\nagy et al., 2020) and Big Bird (Zaheer et al.,\n2020) use sparse self-attention instead of full self-\nattention to process longer documents (e.g. up\nto 4,096 tokens). Other approaches process long\ndocuments in their entirety by dividing them into\nsmaller chunks (e.g. Pappagari et al., 2019). An\nalternative idea proposed by recent work is to se-\nlect sentences from the document that are salient\nto making the classification decision (Ding et al.,\n2020).\nHowever, the relative efficacy of these models\nis not very clear due to a lack of consensus on\nbenchmark datasets and baselines. Tay et al. (2021)\npropose a benchmark for comparing Transformers\nthat can operate over long sequences, but this only\nincludes a single, simulated2 long document clas-\nsification task. Novel variants of efficient Trans-\nformers are often compared to a BERT/RoBERTa\nbaseline only, without much comparison to other\nTransformer models designed for the task (e.g. Belt-\nagy et al., 2020; Zaheer et al., 2020). Conversely,\nmodels designed for long document classification\noften focus exclusively on state-of-the-art mod-\nels for particular datasets, and do not consider a\nBERT/RoBERTa baseline or any other Transformer\nmodels (e.g. Ding et al., 2020; Pappagari et al.,\n2019).\nThis paper provides a much-needed comprehen-\nsive comparison among existing models for long\ndocument classification by evaluating them against\nunified datasets and baselines. We compare mod-\nels that represent different approaches on various\ndatasets and against Transformer baselines. Our\ndatasets cover binary, multi-class, and multi-label\n2The benchmark considers the task of classifying IMDB\nreviews (Maas et al., 2011) using byte-level information to\nsimulate longer documents.\n702\nclassification. We also consider different ways in-\nformation that is relevant to the classification is or-\nganized in texts (e.g. in the beginning or toward the\nend) and how this affects model performance. We\nalso compare the models in terms of their training\ntime, inference time, and GPU memory require-\nments to account for additional complexity that\nsome of the models have relative to a BERT base-\nline. This allows us to compare the practical effi-\ncacy of the models for real-world usage.\nOur results show that more sophisticated models\nare often outperformed by simpler models (often\nincluding a BERT baseline) and yield inconsistent\nperformance across datasets. Based on these find-\nings, we highlight the importance of considering di-\nverse datasets while developing models, especially\nthose that represent different ways key information\nis presented in long texts. Additionally, we rec-\nommend that future research should also always\ninclude simpler baseline models. To summarize,\nour contributions are:\n• We provide insights into the practical efficacy\nof existing models for long document classi-\nfication by evaluating them across different\ndatasets, and against several baselines. We\ncompare the accuracy of these models as well\nas their runtime and memory requirements.\n• We present a comprehensive suite of evalua-\ntion datasets for long document classification\nwith various data settings for future studies.\n• We propose simple models that often outper-\nform complex models and can be challenging\nbaselines for future models for this task.\n2 Methods\nIn this paper, we compare models representing dif-\nferent approaches to long document classification\n(Beltagy et al., 2020; Pappagari et al., 2019; Ding\net al., 2020) on unified datasets and baselines.\n2.1 Existing Models\nAs described in §1, four distinct approaches have\nbeen proposed for long document classification: 1)\ndocument truncation, 2) efficient self-attention, 3)\nchunk representations, 4) key sentence selection.\nWe evaluate a representative model from each cate-\ngory in this work.\nBERT (document truncation) The simplest ap-\nproach consists of finetuning BERT after truncating\nlong documents to the first 512 tokens.3 As in De-\nvlin et al. (2019), we use a fully-connected layer\non the [CLS] token for classification. This is an\nessential baseline as it establishes the limitations\nof a vanilla BERT model in classifying long doc-\numents yet is still competitive (e.g. Beltagy et al.,\n2020; Chalkidis et al., 2019). However, some prior\nwork fails to consider this baseline (e.g. Ding et al.,\n2020; Pappagari et al., 2019).\nLongformer (efficient self-attention) We select\nLongformer (Beltagy et al., 2020) as a model de-\nsigned to process longer input sequences based on\nefficient self-attention that scales linearly with the\nlength of the input sequence (see Tay et al., 2020,\nfor a detailed survey). Longformer also truncates\nthe input, but it has the capacity to process up to\n4,096 tokens rather than 512 tokens as in BERT.\nFollowing Beltagy et al. (2020), we use a fully-\nconnected layer on top of the first [CLS] token\nwith global attention. Longformer outperformed a\nRoBERTa baseline significantly on a small binary\nclassification dataset (Beltagy et al., 2020). How-\never, it has not been evaluated against any other\nmodels for text classification or on larger datasets\nthat contain long documents.\nToBERT (chunk representations) Transformer\nover BERT (ToBERT, Pappagari et al., 2019) takes\na hierarchical approach that can process documents\nof any lengths in their entirety. The model divides\nlong documents into smaller chunks of 200 tokens\nand uses a Transformer layer over BERT-based\nchunk representations. It is reported to outper-\nform previous state-of-the-art models on datasets\nof spoken conversations. However, it has not been\ncompared to other Transformer models. We re-\nimplement this model based on the specifications\nreported in Pappagari et al. (2019) as the code is\nnot publicly available.\nCogLTX (key sentence selection) Cognize Long\nTeXts (CogLTX, Ding et al., 2020) jointly trains\ntwo BERT (or RoBERTa) models to select key sen-\ntences from long documents for various tasks in-\ncluding text classification. The underlying idea that\na few key sentences are sufficient for a given task\nhas been explored for question answering (e.g. Min\net al., 2018), but not much for text classification. It\nis reported to outperform ToBERT and some other\n3In practice, the first 510 tokens are used along with the\n[CLS] and [SEP] tokens. We use the token count including\nthe two special tokens throughout the paper for simplicity.\n703\nneural models (e.g. CNN), but it is not evaluated\nagainst other Transformer models.\nWe use their multi-class classification code for\nany classification task with appropriate loss func-\ntions.4 Following Beltagy et al. (2020), we use\nsigmoid and binary cross entropy loss on the logit\noutput of the models for binary classification. The\nsame setting is used for multi-label classification\nwith softmax normalization and cross entropy loss.\n2.2 Novel Baselines\nIn addition to the representative models above, we\ninclude two novel methods that serve as simple but\nstrong baseline models.\nBERT+TextRank While the BERT truncation\nbaseline is often effective, key information required\nto classify documents is not always found within\nthe first 512 tokens. To account for this, we aug-\nment the first 512 tokens, with a second set of 512\ntokens obtained via TextRank, an efficient unsuper-\nvised sentence ranking algorithm (Mihalcea and\nTarau, 2004). TextRank provides an efficient alter-\nnative to more complex models designed to select\nkey sentences such as CogLTX. Specifically, we\nconcatenate the BERT representation of the first\n512 tokens with that of the top ranked sentences\nfrom TextRank (up to another 512 tokens). As\nbefore, we use a fully-connected layer on top of\nthe concatenated representation for classification.\nWe use PyTextRank (Nathan, 2016) as part of the\nspaCy pipeline (Honnibal et al., 2020) for the im-\nplementation with the default settings.\nBERT+Random As an alternative approach to\nthe BERT+TextRank model, we select random sen-\ntences up to 512 tokens to augment the first 512\ntokens. Like BERT+TextRank, this can be a sim-\nple baseline approach in case key information is\nmissing in truncated documents.5\n2.3 Hyperparameters\nWe use reported hyperparameters for the existing\nmodels whenever available. However, given that\nwe include different datasets that the original pa-\npers did not use, we additionally explore different\nhyperparameters for the models. Detailed informa-\ntion is available in Appendix A.\n4https://github.com/Sleepychord/CogLTX\n5For simplicity, sentences included in the first 512 tokens\nare not excluded in the random selection process. Different\nsettings are possible, but our preliminary results did not show\nmuch difference.\nDataset # BERT Tokens % Long\nHyperpartisan 744.2 ± 677.9 53.5\n20NewsGroups 368.8 ± 783.8 14.7\nEURLEX-57K 707.99 ± 538.7 51.3\nBook Summary 574.3 ± 659.6 38.8\n– Paired 1,148.6 ± 933.9 75.5\nTable 1: Statistics on the datasets. # BERT Tokens refers\nto the average token count obtained via the tokenizer of\nthe BERT base (uncased) model. % Long refers to the\npercentage of documents with over 512 BERT tokens.\n2.4 Data\nWe select three classification datasets containing\nlong documents to cover various kinds of classifica-\ntion tasks: Hyperpartisan (Kiesel et al., 2019) (bi-\nnary classification), 20NewsGroups (Lang, 1995)\n(multi-class classification) and EURLEX-57K\n(Chalkidis et al., 2019) (multi-label classification).\nWe also re-purpose the CMU Book Summary\nDataset (Bamman and Smith, 2013) as an addi-\ntional multi-label classification dataset.\nWe also modify the EURLEX and Book Sum-\nmary datasets to represent different data settings\nand further test all models under these challenging\nvariations. A document in the EURLEX dataset\ncontains a legal text divided into several sections,\nand the first two sections (header, recitals) carry\nthe most relevant information for classification\n(Chalkidis et al., 2019). We invert the order of\nthe sections so that this key information is located\ntoward the end of each document (Inverted EU-\nRLEX). This creates a dataset particularly chal-\nlenging for models that focus only on the first 512\ntokens. We also combine pairs of book summaries\nfrom the CMU Book Summary dataset to create a\nnew dataset (Paired Book Summary) that contains\nlonger documents with two distinctive information\nblocks. Again, this challenges models not to solely\nrely on the signals from the first 512 tokens. In\naddition, it further challenges models to detect two\nseparate sets of signals for correct classification\nresults. In all, these modified datasets represent dif-\nferent ways information may be presented in long\ntexts and test how robust the existing models are to\nthese. Table 1 summarizes characteristics of all our\ndatasets, with more details in Appendix B.\n2.5 Metrics\nFor the binary (Hyperpartisan) and multi-class\n(20NewsGroups) classification tasks, we report ac-\n704\nModel Hyper- 20News EURLEX Inverted Book Paired\npartisan Groups EURLEX Summary Summary\nBERT 92.00 84.79 73.09 70.53 58.18 52.24\nBERT+TextRank 91.15 84.99 72.87 71.30 58.94 55.99\nBERT+Random 89.23 84.65 73.22 71.47 59.36 56.58\nLongformer 95.69 83.39 54.53 56.47 56.53 57.76\nToBERT 89.54 85.52 67.57 67.31 58.16 57.08\nCogLTX 94.77 84.63 70.13 70.80 58.27 55.91\nTable 2: Performance metrics on the test set for all datasets. The average accuracy (%) over five runs is reported for\nHyperpartisan and 20NewsGroups while the average micro-F1 (%) is used for the other datasets. The highest value\nper column is in bold and the second highest value is underlined. Results below the BERT baseline are shaded.\ncuracy (%) on the test set. For the rest, multi-label\nclassification datasets, we use micro-F1 (%), which\nis based on summing up the individual true posi-\ntives, false positives, and false negatives for each\nclass.6\n3 Results\nTable 2 summarizes the average performance of the\nmodels over five runs with different random seeds.\nOverall, the key takeaway is that more sophisti-\ncated models (Longformer, ToBERT, CogLTX)\ndo not outperform the baseline models across the\nboard. In fact, these models are significantly more\naccurate than the baselines only on two datasets.\nAs reported in Beltagy et al. (2020), Longformer\nrecorded the strongest performance on Hyperpar-\ntisan, with CogLTX also performing well. Long-\nformer and ToBERT performed the best for Paired\nBook Summary. Paired Book Summary seems to\nbe most challenging for all models across the board\nand is the only dataset where the BERT baseline\ndid the worst. However, it is worth noting that\nsimple augmentations of the BERT baseline as in\nBERT+TextRank and BERT+Random were not far\nbehind the best performing model even for this\nchallenging dataset. ToBERT’s reported perfor-\nmance was the highest for 20NewsGroups, but we\nwere unable to reproduce the results due to its mem-\nory constraints. For the other datasets, these more\nsophisticated models were outperformed by the\nbaselines. In particular, the simplest BERT base-\nline that truncates documents up to the first 512\ntokens shows competitive performance overall, out-\nperforming the majority of models for Hyperparti-\n6The choice of these metrics are based on previous liter-\nature. An exploration of other metrics (e.g. macro- F1) may\nprovide further insights. However, we did not see significant\ndifferences in preliminary results, and we believe the general\ntrend of results would not differ.\nModel Train Inference GPU\nTime Time Memory\nBERT 1.00 1.00 <16\n+TextRank 1.96 1.96 16\n+Random 1.98 2.00 16\nLongformer 12.05 11.92 32\nToBERT 1.19 1.70 32\nCogLTX 104.52 12.53 <16\nTable 3: Runtime and memory requirements of each\nmodel, relative to BERT, based on experiments on the\nHyperpartisan dataset. Training and inference time were\nmeasured and compared in seconds per epoch. GPU\nmemory requirement is in GB. Longformer and To-\nBERT were trained on a GPU with a larger memory\nand compared to a comparable run on the machine.\nsan, 20NewsGroups and EURLEX. It is only the\nPaired Book Summary dataset where the BERT\nbaseline performed particularly worse than other\nmodels. In general, we observe little-to-no per-\nformance gains from more sophisticated models\nacross the datasets as compared to simpler models.\nA similar trend was observed even when the mod-\nels were evaluated only on long documents in the\ntest set (Appendix C). These finding suggests that\nthe existing models do not necessarily work better\nfor long documents across the board when diverse\ndatasets are considered.\nThe relatively inconsistent performance of these\nexisting models is even more underwhelming con-\nsidering the difference in runtime and memory re-\nquirements as summarized in Table 3. Compared\nto BERT on the first 512 tokens, Longformer takes\nabout 12x more time for training and inference\nwhile CogLTX takes even longer. ToBERT is faster\nthan those two, but it requires much more GPU\nmemory to process long documents in their en-\ntirety. Taken together with the inconsistency in\n705\naccuracy/F1 scores, this suggests that sophisticated\nmodels are not necessarily a good fit for real word\nuse cases where efficiency is critical.\n4 Discussion and Recommendations\nOur results show that complex models for long doc-\nument classification do not consistently outperform\nsimple baselines. The fact that the existing mod-\nels were often outperformed by the simplest BERT\nbaseline suggests that the datasets tend to have key\ninformation accessible in the first 512 tokens. This\nis somewhat expected as the first two sections of\nEURLEX are reported to carry the most informa-\ntion (Chalkidis et al., 2019) and 20NewsGroups\ncontains mostly short documents. Including these\ndatasets to evaluate models for long document clas-\nsification is still reasonable given that a good model\nshould work well across different settings. How-\never, these datasets alone do not represent various\nways information is presented in long texts.\nInstead, future studies should evaluate their mod-\nels across various datasets to create robust models.\nWhile it is often difficult to obtain datasets suited\nfor long document classification, our modifications\nof existing datasets may provide ways to repurpose\nexisting datasets for future studies. We invert the\norder of the sections of EURLEX to create the In-\nverted EURLEX dataset, where key information is\nlikely to appear toward the end of each document.\nOur results in Table 2 show that selective mod-\nels (BERT+TextRank, BERT+Random, CogLTX)\nperformed better than those that read longer con-\nsecutive sequences (Longformer, ToBERT) on this\ndataset. This suggests that this inverted dataset may\ncontain parts of texts that should be ignored for bet-\nter performance, thus providing a novel test bed for\nfuture studies. The Paired Book Summary dataset\npresents another challenging data setting with two\ndistinctive information blocks. While Longformer\nand ToBERT performed significantly better for this\ndataset than others, the overall model performance\nwas quite underwhelming, leaving room for im-\nprovement for future models.\nMany of these findings were revealed only due\nto the choice of relevant baselines, and future\nwork will benefit from including these as well. A\nBERT/RoBERTa baseline is essential to motivate\nthe problem of long document classification using\nTransformers and reveal how much information is\nretrievable in the first 512 tokens. BERT+TextRank\nand BERT+Random are stronger baselines that of-\nten outperform more complex models that select\nkey sentences. In fact, they outperformed CogLTX\non five of the six datasets.\n5 Conclusion\nSeveral approaches have been proposed to use\nTransformers to classify long documents, yet their\nrelative efficacy remains unknown. In this paper,\nwe compare existing models and baselines on var-\nious datasets and in terms of their time and space\nrequirements. Our results show that existing mod-\nels, while requiring more time and/or space, do\nnot perform consistently well across datasets, and\nare often outperformed by baseline models. Future\nstudies should consider the baselines and datasets\nto establish robust performance.\nAcknowledgments\nWe would like to thank the reviewers and area\nchairs for their thoughtful comments and sugges-\ntions. We also thank the members of AWS AI\nLabs for many useful discussions and feedback\nthat shaped this work.\nReferences\nDavid Bamman and Noah A. Smith. 2013. New align-\nment methods for discriminative book summarization.\narXiv:1305.1319.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document Transformer.\narXiv:2004.05150.\nIlias Chalkidis, Emmanouil Fergadiotis, Prodromos\nMalakasiotis, and Ion Androutsopoulos. 2019. Large-\nscale multi-label text classification on EU legislation.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 6314–\n6322, Florence, Italy. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional Transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMing Ding, Chang Zhou, Hongxia Yang, and Jie Tang.\n2020. CogLTX: Applying BERT to long texts. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 12792–12804. Curran Associates,\nInc.\n706\nMatthew Honnibal, Ines Montani, Sofie Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy: Industrial-\nstrength natural language processing in Python.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 task 4: Hyperpartisan news detection. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation, pages 829–839, Minneapolis,\nMinnesota, USA. Association for Computational Lin-\nguistics.\nKen Lang. 1995. Newsweeder: Learning to filter net-\nnews. In Proceedings of the Twelfth International\nConference on Machine Learning, pages 331–339.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv:1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nRada Mihalcea and Paul Tarau. 2004. TextRank: Bring-\ning order into text. In Proceedings of the 2004 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 404–411, Barcelona, Spain. Asso-\nciation for Computational Linguistics.\nSewon Min, Victor Zhong, Richard Socher, and Caim-\ning Xiong. 2018. Efficient and robust question an-\nswering from minimal context over documents. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1725–1735, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nShervin Minaee, Nal Kalchbrenner, Erik Cambria, Nar-\njes Nikzad, Meysam Chenaghlu, and Jianfeng Gao.\n2021. Deep learning–based text classification. ACM\nComputing Surveys, 54(3):1–40.\nPaco Nathan. 2016. PyTextRank, a Python implemen-\ntation of TextRank for phrase extraction and summa-\nrization of text documents.\nRaghavendra Pappagari, Piotr Zelasko, Jesús Villalba,\nYishay Carmiel, and Najim Dehak. 2019. Hierarchi-\ncal Transformers for long document classification.\nIn 2019 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU), pages 838–844.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2021. Long\nrange arena: A benchmark for efficient Transformers.\nIn International Conference on Learning Representa-\ntions.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efficient Transformers: A survey.\narXiv:2009.06732.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 5998–6008. Curran Asso-\nciates, Inc.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\nand Amr Ahmed. 2020. Big bird: Transformers for\nlonger sequences. arXiv:2007.14062.\nA Hyperparameters\nAcross all datasets, we used Adam optimizer with\na learning rate of {5e-5, 3e-5, 0.005} for one run of\neach model and picked the best performing learn-\ning rate for the model. The learning rate of 0.005\nwas used for Longformer only because it did not\nperform well with a learning rate of 5e-5 or 3e-5\nfor most of the datasets. We set dropout rate at\n0.1 as suggested by Devlin et al. (2019). The num-\nber of epochs needed for finetuning the models for\ndifferent datasets is likely to vary, so we trained\nall models for 20 epochs and selected the best per-\nforming model based on the performance metric\non the validation set. We report the average results\non the test set over five different seeds.\nAll experiments on baseline models and\nCogLTX were conducted on a single Tesla V100\nGPU with 16GB memory. For Longformer and\nToBERT, we used a NVIDIA A100 SXM4 with\n40GB memory. More details on the selected hyper-\nparameters are available with our code at https:\n//github.com/amazon-research/\nefficient-longdoc-classification.\nB Datasets\nHyperpartisan is a binary classification dataset,\nwhere each article is labeled as True (hyperpartisan)\nor False (not hyperpartisan) (Kiesel et al., 2019).\nMore than half of the documents exceed 512 tokens.\nIt is quite different from other datasets in that it\nis a very small dataset: the training set contains\n516 documents while the development and test sets\ncontain 64 and 65 documents, respectively.\n20NewsGroups is a widely-used multi-class clas-\nsification dataset (Lang, 1995). The documents are\ncategorized into well-balanced, 20 classes. Only\n707\nDataset Type # Train # Dev # Test # Labels # BERT Tokens % Long\nHyperpartisan binary 516 64 65 2 744.18 ± 677.87 53.49\n20NewsGroups multi-class 10,182 1,132 7,532 20 368.83 ± 783.84 14.71\nEURLEX-57K multi-label 45,000 6,000 6,000 4,271 707.99 ± 538.69 51.30– Inverted\nBook Summary multi-label 10,230 1,279 1,279 227 574.31 ± 659.56 38.76\n– Paired multi-label 5,115 639 639 227 1,148.62 ± 933.97 75.54\nTable 4: Statistics on the datasets. # BERT Tokens refers to the average token count obtained via the tokenizer of\nthe BERT base model (uncased). % Long refers to the percentage of documents with more than 512 BERT tokens.\nModel Hyper- 20News EURLEX Inverted Book Paired\npartisan Groups EURLEX Summary Summary\nBERT 88.00 86.09 66.76 62.88 60.56 52.23\nBERT+TextRank 85.63 85.55 66.56 64.22 61.76 56.24\nBERT+Random 83.50 86.18 67.03 64.31 62.34 56.77\nLongformer 93.17 85.50 44.66 47.00 59.66 58.85\nToBERT 86.50 – 61.85 59.50 61.38 58.17\nCogLTX 91.91 86.07 61.95 63.00 60.71 55.74\nTable 5: Performance metrics evaluated on long documents in the test set for all datasets. The average accuracy (%)\nover five runs is reported for Hyperpartisan and 20NewsGroups while the average micro-F1 (%) is used for the\nother datasets. The highest value per column is in bold and the second highest value is underlined. Results below\nthe BERT baseline are shaded. Running ToBERT on 20NewsGroups seems to require further preprocessing, which\nwe were unable to replicate with the reported information.\nabout 15% of the documents exceed 512 tokens.\nWhile the original dataset comes in train and test\nsets only, we report results on the train/dev/test split\nas used in Pappagari et al. (2019), where we take\n10% of the original train set as the development\nset. Note that CogLTX reported their accuracy at\n87.00% on the test set and 87.40% on the long doc-\numents in the test set, using the original train and\ntest sets only. Our implementation of CogLTX in\nthe same setting with five different runs resulted\nin a much lower performance at 85.15% on the\ntest set and 86.57% on the long documents only.\nIn addition, we were unable to replicate ToBERT\nresults on 20NewsGroups. It is unclear how the\ndataset is further preporcessed for ToBERT, and our\nimplementation of ToBERT caused a GPU out-of-\nmemory error on 20NewsGroups. Thus, we show\nthe reported results for ToBERT on this dataset.\nEURLEX-57K is a multi-label classification\ndataset based on EU legal documents (Chalkidis\net al., 2019). In total, there are 4,271 labels avail-\nable, and some of them do not appear in the train-\ning set often or at all, making it a very challeng-\ning dataset. About half of the datasets are long\ndocuments. Each document contains four major\nzones: header, recitals, main body, and attachments.\nChalkidis et al. (2019) observe that processing the\nfirst two sections only (header and recitals) results\nin almost the same performance as the full docu-\nments and that BERT on the first 512 tokens outper-\nforms all the other models they considered. After\nexamining the dataset, we exclude the attachments\nsection as it does not seem to provide much textual\ninformation.\nCMU Book Summary contains book summaries\nextracted from Wikipedia with corresponding meta-\ndata from Freebase such as the book author and\ngenre (Bamman and Smith, 2013). We use the\nsummaries and their corresponding genres for a\nmulti-label classification task. We keep 12,788 out\nof 16,559 documents after removing data points\nmissing any genre information and/or adequate\nsummary information (e.g. less than 10 words).\nIn total, there are 227 genre labels such as ‘Fiction’\nand ‘Children’s literature’.\nC Results on long documents only\nTable 5 shows the results as evaluated on long doc-\numents (with over 512 tokens) in the test set only.\nOverall, the results show a similar trend as ob-\n708\nserved in Table 2, which reports the results on the\nentire documents in the test set. In general, the\nexisting models were often outperformed by the\nBERT truncation baseline. This suggest that these\nmodels designed for long document classification\ndo not perform particularly well on the long docu-\nments in the datasets. The only difference is that\nBERT+Random and ToBERT perform better than\nthe BERT baseline when evaluated on long docu-\nments only for 20NewsGroups and Book Summary,\nrespectively. However, the performance gain does\nnot seem significant, and the relative performance\nwith respect to the other models remains largely\nunchanged. In general, the relative strength of a\nmodel for a given dataset stays the same whether or\nnot the model is evaluated on the entire documents\nor long documents in the test set.\n709",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8196301460266113
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6124307513237
    },
    {
      "name": "Document classification",
      "score": 0.5281324982643127
    },
    {
      "name": "Binary classification",
      "score": 0.5270393490791321
    },
    {
      "name": "Machine learning",
      "score": 0.519839346408844
    },
    {
      "name": "Data mining",
      "score": 0.5108854174613953
    },
    {
      "name": "Task (project management)",
      "score": 0.4688951373100281
    },
    {
      "name": "Transformer",
      "score": 0.4641479253768921
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4539319574832916
    },
    {
      "name": "Information retrieval",
      "score": 0.41016528010368347
    },
    {
      "name": "Support vector machine",
      "score": 0.18082880973815918
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801919071",
      "name": "University of Illinois System",
      "country": "US"
    }
  ]
}