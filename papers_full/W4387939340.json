{
    "title": "An Efficient Plug-and-Play Post-Training Pruning Strategy in Large Language Models",
    "url": "https://openalex.org/W4387939340",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2123933215",
            "name": "Yingtao Zhang",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2994411469",
            "name": "Haoli Bai",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2616445257",
            "name": "Haokun Lin",
            "affiliations": [
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2102126031",
            "name": "Jialin Zhao",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2123717260",
            "name": "Lu Hou",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2072877244",
            "name": "Carlo Vittorio Cannistraci",
            "affiliations": [
                "Tsinghua University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6893809404",
        "https://openalex.org/W6689029123",
        "https://openalex.org/W3101584733",
        "https://openalex.org/W4381586827",
        "https://openalex.org/W4313484599",
        "https://openalex.org/W3209017628",
        "https://openalex.org/W2764043458",
        "https://openalex.org/W2894740066",
        "https://openalex.org/W2954698171",
        "https://openalex.org/W4287777801",
        "https://openalex.org/W4292692470",
        "https://openalex.org/W4226078967",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W3211718241",
        "https://openalex.org/W4287327026",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4380346028",
        "https://openalex.org/W3127067080",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4379260375",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4282973707",
        "https://openalex.org/W2156150815",
        "https://openalex.org/W3213238321",
        "https://openalex.org/W4307934016",
        "https://openalex.org/W2990844796",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W3131107792",
        "https://openalex.org/W4309591680"
    ],
    "abstract": "With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation for LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation, yet the performance is still sub-optimal. In this paper, we present a plug-and-play solution for post-training pruning of LLMs. The proposed solution has two innovative components: 1) **Relative Importance and Activations** (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs; and 2) **Channel Permutation**, a new approach to maximally preserve important weights under N:M sparsity. The proposed two components can be readily combined to further enhance the N:M structuredly pruned LLMs. Our empirical experiments show that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, e.g., LLaMA ranging from 7B to 65B. Furthermore, N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks, together with practical speed-up on specific hardware.",
    "full_text": null
}