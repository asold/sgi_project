{
    "title": "Integrating Pretrained Language Model for Dialogue Policy Learning",
    "url": "https://openalex.org/W3208837891",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1938003352",
            "name": "Wang Hong-ru",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1578569810",
            "name": "Wang Huimin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A980932792",
            "name": "Wang Zezhong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2087743882",
            "name": "Wong, Kam-Fai",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962912551",
        "https://openalex.org/W2736601468",
        "https://openalex.org/W2128970689",
        "https://openalex.org/W2970828515",
        "https://openalex.org/W3015731157",
        "https://openalex.org/W1211946649",
        "https://openalex.org/W3034330559",
        "https://openalex.org/W2810840719",
        "https://openalex.org/W3099293669",
        "https://openalex.org/W2798494119",
        "https://openalex.org/W3007894275",
        "https://openalex.org/W2891732163",
        "https://openalex.org/W3005912270",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2523728418",
        "https://openalex.org/W2765111838",
        "https://openalex.org/W3104546989",
        "https://openalex.org/W3017861469"
    ],
    "abstract": "Reinforcement Learning (RL) has been witnessed its potential for training a dialogue policy agent towards maximizing the accumulated rewards given from users. However, the reward can be very sparse for it is usually only provided at the end of a dialog session, which causes unaffordable interaction requirements for an acceptable dialog agent. Distinguished from many efforts dedicated to optimizing the policy and recovering the reward alternatively which suffers from easily getting stuck in local optima and model collapse, we decompose the adversarial training into two steps: 1) we integrate a pre-trained language model as a discriminator to judge whether the current system action is good enough for the last user action (i.e., \\textit{next action prediction}); 2) the discriminator gives and extra local dense reward to guide the agent's exploration. The experimental result demonstrates that our method significantly improves the complete rate (~4.4\\%) and success rate (~8.0\\%) of the dialogue system.",
    "full_text": "INTEGRATING PRETRAINED LANGUAGE MODEL\nFOR DIALOGUE POLICY EV ALUATION\nHongru Wang, Huimin Wang, Zezhong Wang, Kam-Fai Wong‚àó\nDepartment of Systems Engineering and Engineering Management\nThe Chinese University of Hong Kong\nABSTRACT\nReinforcement Learning (RL) has been witnessed its po-\ntential for training a dialogue policy agent towards maximiz-\ning the accumulated rewards given from users. However, the\nreward can be very sparse for it is usually only provided at\nthe end of a dialog session, which causes unaffordable inter-\naction requirements for an acceptable dialog agent. Distin-\nguished from many efforts dedicated to optimizing the policy\nand recovering the reward alternatively which suffers from\neasily getting stuck in local optima and model collapse, we\ndecompose the adversarial training into two steps: 1) we inte-\ngrate a pre-trained language model as a discriminator to judge\nwhether the current system action is good enough for the last\nuser action (i.e., next action prediction); 2) the discriminator\ngives and extra local dense reward to guide the agent‚Äôs explo-\nration. The experimental result demonstrates that our method\nsigniÔ¨Åcantly improves the complete rate ( 4.4%) and success\nrate ( 8.0%) of the dialogue system.\nIndex Terms‚Äî Reward Shaping, Dialogue Policy Learn-\ning, Pre-trained Language Model\n1. INTRODUCTION\nReinforcement learning has revolutionized the way to model\nthe dialogue policy which decides the next action of the dia-\nlogue system suited to the current state [1, 2, 3, 4, 5, 6, 7]. On\nthe Ô¨Çip side, one notorious limitation of reinforcement learn-\ning is reward sparsity issue where here the system usually\nreceive a positive (or negative) reward signal when the dia-\nlogue ends successfully (or unsuccessfully). Thus, the reward\nsignal is delayed and sparse, making it extremely difÔ¨Åcult to\nconnect a long series of actions to a distant future reward es-\npecially for complex goals across multiple domains [6].\nA typical reward function, for example, often apply a\nminor negative penalty (i.e., -1) in the middle of the ses-\nsion to encourage the system to accomplish the task in fewer\nturns, with a huge positive (i.e., +40) or negative (i.e., -40)\nreward at the end [1]. This kind of global reward based\non (state,action) pairs is not informative and lead to ex-\nploration in large action space inefÔ¨Åcient [8]. To get more\n‚àóCorresponding Author\ndense and enlightening reward signals, most previous works\nrecovers the intrinsic local reward from expert demonstra-\ntions through reward shaping [5, 9]. More speciÔ¨Åcally, some\nworks train a discriminator to differentiate (state,action)\ngenerated by dialogue agents from (state,action) by expert\nand then regards the discriminator as a reward estimator to\nprovide intrinsic reward signals, where the dialogue policy\nmodel and discriminator updates alternately on the Ô¨Çy [10, 6].\nA further line of work decomposes the whole training into\ntwo steps by Ô¨Årstly training the discriminator with an aux-\niliary dialogue generator and secondly incorporating it into\ncommon RL method, since the alternative update mechanism\nlimits the policy model to policy-gradient-based algorithms\n[7]. However, the vast bulk of annotated(state,action) pairs\nfrom expert demonstration is hard to acquire. Moreover, the\nreward model based on state-action pair might cause unstable\npolicy learning and affect optimization speed with the limited\namount of annotated dialogues [11].\nOur work keeps in line with the methods to decompose the\nwhole training into two sequential steps. We incorporate the\npretrained language model as the reward model into common\nRL method to provide denselocal rewardsignals, guiding the\naction decision of dialogue policy learning. SpeciÔ¨Åcally, we\nre-formulate one of the pre-training sub-tasks of BERT Next\nSentence Prediction as Next Action Predictionat the Ô¨Årst step.\nGiven current user action au, the classiÔ¨Åer will distinguish\nwhether or not the response system action as is suitable or\nacceptable. Intuitively, if the system chooses the right action\nto answer user‚Äôs query at each turn, then the dialogue natu-\nrally succeeds at the end. Secondly, the trained classiÔ¨Åer as\nthe dialogue reward model will be incorporated into the RL\nprocess to guide the dialogue policy learning without updates.\nThere are several advantages of our proposed method: 1) Our\nmethod is model-agnostic which can be incorporated in any\nRL algorithm to guide the policy learning, 2) Only action-\npairs demonstration reduce the cost of annotation compared\nwith state-action pairs demonstration, and 3) Pre-trained lan-\nguage model has been proved powerful and transferable in\nmany NLP tasks which can capture the subtle difference of\naction-pairs, providing more dense reward signal at each turn.\nThe main contribution of this paper is two-fold: 1) we\npropose a simple yet effective reward estimator at the action-\narXiv:2111.01398v1  [cs.CL]  2 Nov 2021\nlevel to guide the action decision, and 2) We investigate the ef-\nfects of global reward and local reward, and the experimental\nresults on MultiWOZ [12] show that our methods outperform\nsingle global rewardabout 4.4%, and furthermore, almost 8%\nwhen combined with global reward, which indicates the com-\nplementary effects of these two types of reward.\n2. RELATED WORK\nThe Ô¨Årst focus of reward shaping is recovering the intrin-\nsic local reward from expert demonstration by inverse rein-\nforcement learning or adversarial training. Peng et al. [10]\nproposed an adversarial advantage actor-critic (Adversarial\nA2C) method based on (state,action) pairs from simu-\nlation and expert demonstration. Similarly, Takanobu et\nal. [6] utilizes Adversarial Inverse Reinforcement Learning\nto jointly estimate reward and optimize dialogue policy in\nmulti-domain task-oriented dialog. Wang et al. [8] directly\nestimate potential-based reward function from demonstra-\ntions. Nevertheless, methods alternately learning the reward\nmodel and policy cannot be extended to off-policy methods\nwhich beneÔ¨Åt from self-learned reward functions [7].\nThe second focus of reward shaping is to train reward\nmodel and dialogue policy consecutively, rather than alterna-\ntively as mentioned above [7]. Li et al. [7] appropriate the\nreward model with multilayer perceptron (MLP) to train a\ndiscriminator based on (state,action) pairs. Besides that,\nGabriel et al. [13] propose a No Label Expert (NLE) that uses\nan unannotated dialog dataset consisting of pairs of sentences\n(su,ss), representing user utterances and the corresponding\nagent responses, guiding the dialogue policy. Distinguishing\nfrom these works, we tackle reward sparsity by incorporat-\ning pretrained language model as a reward estimator and train\na discriminator at action-level rather than sentence-level or\n(state,action) pairs.\n3. METHODOLOGY\nIn this section, we Ô¨Årstly introduce the detail of next ac-\ntion prediction (namely, how to train the discriminator), and\nthen presents an algorithm that incorporates the reward signal\ngiven in the Ô¨Årst step (namely, how to update the dialogue pol-\nicy). Figure 1 shows the overall framework of our proposed\nmethod.\n3.1. Next Action Prediction\nNext Sentence Prediction is one of pre-trained task in BERT,\naiming to predict whether, for a given pair of sentences\n(s1,s2), s2 is an next sentence to s1. Similarly, Next Action\nPrediction task is to determine whether or not the current\nsystem action as is an appropriate response to the last user\naction au in dialogue. An action (i.e., au and as) consists of\n[CLS]HotelInform{area=don‚Äôtcare;choice=any}[SEP]HotelInform{area=east;name=starwars}[SEP]\nSystemactionUseraction\nSigmoidlayerPolicyLearning\nUserSimulatorùë†LocalDenseReward ExpertDemo\nDialogueStateTrackingùëé!,ùëü\nùëé\"\nùë†\n‚Ä¶ ‚Ä¶BERT[CLS]\nFig. 1. Our proposed method: Integrating pre-trained lan-\nguage model into reinforcement learning as reward model\nto provide local dense reward signal based on action pairs\n(au,as), guiding the optimization of dialogue policy learn-\ning.\ndomain, intent, slots and its corresponding value as follows\n[14]:\nA= [ DÓ¥ôÓ¥òÓ¥óÓ¥ö\nDomain\nIÓ¥ôÓ¥òÓ¥óÓ¥ö\nIntent\n{s1 = v1 ; ¬∑¬∑¬∑ ; sP = vP }Ó¥ô Ó¥òÓ¥ó Ó¥ö\nSlot-value pairs\n] (1)\nAt each turn of multi-domain dialogue, the user and sys-\ntem may inquire and provide information across different do-\nmain. In this case, we represent the action as:\nau = [A1\nu,A2\nu,..., An\nu] as = [A1\ns,A2\ns,..., An\ns] (2)\nThen, we concatenate the user action au and system ac-\ntion as and feed it into BERT. Two special tokens[CLS] and\n[SEP] are added to indicate the start and separation of ac-\ntions respectively. The input representations consist of token\nembedding, segment embedding and positional embedding.\nH = BERT(emb(au,as)) (3)\nThe embedding of Ô¨Årst token (i.e. [CLS]) then is then feed\ninto a sigmoid layer to do classiÔ¨Åcation as deÔ¨Åned:\nf(x) = sigmoid(Wh1 + b) (4)\nThe Ô¨Ånal output of the binary classiÔ¨Åer f(x) is a probabil-\nity that indicates the conÔ¨Ådent score that the system action is\nsuitable and appreciate given last user action. The objective\nof the classiÔ¨Åer DœÜ can be represented as follows:\nLD = E(log(1‚àíDœÜ(au,as)sim))‚àíE(DœÜ(au,as)real) (5)\nAfter the discriminator is trained, we will keep it as the\nreward function for future dialogue policy learning without\nupdates, which is illustrated at later section.\n3.2. PPO-OFF\nA trajectory (s0,a0,s1,a1,...) is generated by sampling ac-\ntions according to the policy at ‚àº œÄ(at|st) consecutively,\nuntil the terminal states is reached. Here, the action at can be\nfurther divided into the user actionau and system action as in\ndialogue. A reward signal rt is received at each time step.\nT = [(s0,(au,as),r0,s1),..., (sn,(au,as),rn,sn+1)] (6)\nwhere st,au,as,rt,st+1 represents the dialogue state,\nuser action, system action, reward and next state at time step\nt respectively. The main objective of agent is to maximize\nthe cumulative reward R = ‚àëT\n0 Œ≥trt, where Œ≥ is a discount\nfactor. Given the objective function, the gradient can be\ncomputed as follows:\ng= E[\nT‚àë\n0\nŒ®t‚àáŒ∏logœÄŒ∏(at|st)] (7)\nwhere Œ®t can be estimated with different methods. We\nadapt generalized advantage function AœÄ(st,at) here as fol-\nlows 1:\nAGAE(Œ≥,Œª)\nt =\n‚àû‚àë\nl=0\n(Œ≥Œª)lŒ¥V\nt+l (8)\nwhere Œ¥V\nt = ‚àíV(st) + rt + Œ≥V(St+1). Both Œ≥ and Œª\nplays key roles in the bias-variance trade-off and serve differ-\nent purposes when using an approximate value function. To\nstabilize the policy updates, we adapt the clipped surrogate\nobjective as follows [15].\nLCLIP = E[min(rt(Œ∏)AGAE\nt ,clip(rt(Œ∏),1‚àíœÉ,1+œÉ)AGAE\nt )]\n(9)\nwhere rt(Œ∏) = œÄŒ∏(at|st)\nœÄŒ∏old(at|st) . In addition to policy, we also\nparametrize the value function and add entropy bonus to en-\nsure sufÔ¨Åcient exploration[15]. The whole training objective\nis deÔ¨Åned as follows:\nLCLIP+VF +S = E[LCLIP ‚àíc1 ‚àóLVF +c2 ‚àóS[œÄ](s)] (10)\nwhere c1,c2 are coefÔ¨Åcients, and S denotes an entropy\nbonus, and LVF is a squared-error loss (VŒ∏(st) ‚àíVtar\nt )2.\nIt is obvious that the reward model plays a key role in di-\nalogue policy learning since it directly affect the training ob-\njective. To investigate the effects of different reward signals,\nwe replace the rt in the trajectory with three different types of\nreward: global reward, local reward and combination.\nFor global reward, we simply assign it a large positive\n(v.s. negative) reward +40 (v.s. -40) when the dialogue ends\nsuccessfully (v.s. unsuccessfully), while -1 during the middle\nof session to encourage shorter session. For local reward,\nwe remap the output conÔ¨Ådent score to a range of [-1, 1] as\nfollows, aiming to encourage the policy to decide more high\nqualiÔ¨Åed actions.\nrlocal = ‚àí1+2 ‚àóDœÜ(au,as) rcomb = rglobal+rlocal (11)\n1at refers the system action as while au is decided by a user simulator\n4. EXPERIMENTS AND ANALYSIS\n4.1. Dataset and Evaluation Metric\nAll models are evaluated on MultiWOZ [12], a multi-domain,\nmulti-intent task-oriented dialog corpus that contains 7 do-\nmains, 13 intents, 25 different slots and 10483 dialog ses-\nsions. We report the average number of dialog turns, averag-\ning over successful dialog sessions and all dialog sessions re-\nspectively, to measure the efÔ¨Åcient of accomplishing a task. A\ndialog turn consists of a user utterance and a subsequent sys-\ntem (i.e. utterance pairs). Precision, recall and F1 are calcu-\nlated based on dialog acts (i.e. action pairs) [16]. Match rate\nassesses whether the offered entity meets all the constraints\nspeciÔ¨Åed in a user goal. The dialog is marked as successful if\nand only if both inform recall and match rate are 1.\n4.2. Baselines\nMLE: One of representative work of supervised learning to\nlearn dialogue policy, which employs a multi-class classiÔ¨Åca-\ntion via imitation learning (i.e., behavioral cloning) with a set\nof compositional actions where a compositional action con-\nsists of a set of dialog act items.\nGDPL: [6] A method which alternatively updates dialogue\npolicy and reward estimation model by using adversarial in-\nverse reinforcement learning. It is noted that reward estimator\nrecovers the reward signal form the state-action pairs at each\ndialogue turn.\nPPO [15] A policy-based reinforcement learning method\nwhich uses multiple epochs of stochastic gradient ascent and\na constant clipping mechanism the soft constraint to perform\neach policy update. The dialogue policy model in GDPL is\nalso PPO.\nPPO-OFF-Comb The reward model offers the combination\nof global reward and local reward while the policy updated\nas illustrated in section 3.2. Similarly, PPO-OFF-Local only\nreceives local dense reward from pre-trained language model\n(i.e., BERT) and PPO-OFF-Global receives only global re-\nward respectively.\n4.3. Implementation Details\nFor next action prediction, we Ô¨Årstly build the binary classi-\nÔ¨Åcation dataset from MultiWoZ [12] automatically and get\n99370, 13157, 13073 labeled samples for training, testing\nand validation respectively 2. And then we use BERT[17]\nas backbone and Adamas optimization algorithm. The spe-\nciÔ¨Åc hyper-parameters are deployed as follows: batch size as\n4, learning rate as 5e-5, epochs as 10, adam epsilon as 1e-8,\nthe max sequence length as 512 3.\n2We sample user action au with system action as from same dialogue\nas positive sample, and randomly sample another system action from other\ndialogue to form negative sample.\n3It is noted that the trained BERT model achieved 97.34% accuracy at\nthe binary classiÔ¨Åcation task, which proves the pre-trained language model is\nModel Agenda\nPrecision/Recall/F1 Match Success\nHuman 81.9/93.4/85.3 92.1 82.7\nMLE 63.1/72.6/64.5 50.1 47.0\nGDPL 63.1/73.0/64.6 50.0 47.2\nPPO 64.4/78.3/68.2 64.7 61.2\nPPO-OFF-Global 63.0/81.2/67.8 70.8 63.2\nPPO-OFF-Local 60.9/82.9/67.4 70.9 67.6\nPPO-OFF-Comb 67.2/85.7/72.7 79.6 71.4\nTable 1. Performance of different dialog agents on the multi-\ndomain dialog corpus by interacting with the agenda-based\nuser simulator. Success average turns / all average turns\nFor dialogue policy learning , we adapt ConvLab-2 [18] as\nour environment and evaluate the policy at sentence-level\ninstead of action-level. In this case, other components in\ntask-oriented dialogue system are BERTNLU, RuleDST and\nTemplate-based NLG. More speciÔ¨Åcally, we set the maximum\nturn of one conversation as 20, which means the dialogue will\nbe terminated and regarded as failure when the turn excesses\n20. We sample 1024 trajactories each epoch and the max\nepoch is set as 200. Other hyper-parameter settings follows\nTakanobu et al. [6].\n4.4. Main Result\nTable 1 demonstrates the performance of different methods\non the MultiWoZ dataset [12]. Consistent with intuition, the\ncombination (i.e., PPO-OFF-Comb) of global and local re-\nward reaches highest performance in both match rate (9% im-\nprovement) and success rate (8% improvement). Besides that,\nPPO-OFF-Local achieves comparable match rate but much\nhigher success rate compared with PPO-OFF-Global. We at-\ntribute this to more semantic frame are correct in the dialogue\nsince success rate is much harder to improve than match rate.\nCompared with previous methods such as PPO, our proposed\nmethods demonstrate superior performance by improving al-\nmost 10%.\nFig. 2. Learning curves of our proposed method under differ-\nent reward signals and action-level\ncapable to provide reliable reward signal.\nhotel train attraction police restaurant taxi hospital\n20\n30\n40\n50\n60\n70\n80\n90\n100F1\n32.5\n80.8\n76.4\n94.7\n63.4\n97.1\n89.0\n41.9\n81.3\n74.7\n94.7\n63.5\n97.3\n89.0\n41.7\n83.8\n75.5\n94.7\n64.6\n98.4\n89.8\nglobal/uni00A0reward\nlocal/uni00A0reward\ncomb\nFig. 3. F1 score for different domain by executing policy\nlearned with different rewards\n4.5. Analysis\nWe also conduct evaluation at the action-level (i.e., without\nNLU and NLG part) to learn the converge speed and perfor-\nmance under different domains respectively.\nConverge SpeedThe learning curves under different reward\ntypes are presented in Fig 2. We can see that the comb re-\nward converge faster than only local and only global reward,\nleading to higher performance. Besides, the error band of\ncomb reward is dramatically smaller than the other two re-\nwards which indicates more stable policy learning. Therefore,\nwe conclude that local reward signals serve as a complement\nof global reward to better guide and stabilize the behavior of\ndialogue policy.\nThe Effects of Different DomainsWe also report the F1\nscore of different domains with policy learned from differ-\nent rewards as shown in Fig 3. The comb reward consistently\noutperforms single global or local reward signals, especially\nat the hotel, restaurant, taxi and hospital domain. We empha-\nsize that the hotel domain contains the most slots (i.e., 14 dif-\nferent slots) which have the biggest improvement over such\na large action space. However, we noticed that there still a\nlarge gap between different domains. We conjecture this to\nthe data distribution of the original dataset and the difÔ¨Åculty\nand complexity of different domains.\n5. CONCLUSION\nIn this paper, we integrates pre-trained language model (i.e.\nBERT) as a reward model to provide local reward that com-\nplements with global reward, guiding the dialogue policy\nlearning. The experimental results demonstrate the com-\nbination of global reward and local reward reaches highest\nperformance compares with only global or only local reward.\nWe left more investigation such as pretraining in future work.\n6. ACKNOWLEDGEMENTS\nThis work is partially supported by HK GRF#14204118 and\nHK RSFS#3133237.\n7. REFERENCES\n[1] Jianfeng Gao, Michel Galley, and Lihong Li, ‚ÄúNeural\napproaches to conversational ai,‚Äù in The 41st Interna-\ntional ACM SIGIR Conference on Research & Develop-\nment in Information Retrieval, 2018, pp. 1371‚Äì1374.\n[2] Marilyn A Walker, ‚ÄúAn application of reinforcement\nlearning to dialogue strategy selection in a spoken di-\nalogue system for email,‚Äù Journal of ArtiÔ¨Åcial Intelli-\ngence Research, vol. 12, pp. 387‚Äì416, 2000.\n[3] Lihong Li, Jason D Williams, and Suhrid Balakrishnan,\n‚ÄúReinforcement learning for dialog management using\nleast-squares policy iteration and fast feature selection,‚Äù\nin Tenth Annual Conference of the International Speech\nCommunication Association, 2009.\n[4] Pierre-Luc Bacon, Jean Harb, and Doina Precup, ‚ÄúThe\nOption-Critic Architecture,‚Äù arXiv:1609.05140 [cs] ,\nDec. 2016.\n[5] Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli\nCelikyilmaz, Sungjin Lee, and Kam-Fai Wong, ‚ÄúCom-\nposite Task-Completion Dialogue Policy Learning via\nHierarchical Deep Reinforcement Learning,‚Äù in Pro-\nceedings of the 2017 Conference on Empirical Methods\nin Natural Language Processing , Copenhagen, Den-\nmark, Sept. 2017, pp. 2231‚Äì2240, Association for Com-\nputational Linguistics.\n[6] Ryuichi Takanobu, Hanlin Zhu, and Minlie Huang,\n‚ÄúGuided Dialog Policy Learning: Reward Estimation\nfor Multi-Domain Task-Oriented Dialog,‚Äù in Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), Hong Kong, China, Nov. 2019,\npp. 100‚Äì110, Association for Computational Linguis-\ntics.\n[7] Ziming Li, Sungjin Lee, Baolin Peng, Jinchao Li, Ju-\nlia Kiseleva, Maarten de Rijke, Shahin Shayandeh, and\nJianfeng Gao, ‚ÄúGuided Dialog Policy Learning without\nAdversarial Learning in the Loop,‚Äù arXiv:2004.03267\n[cs], Sept. 2020.\n[8] Huimin Wang, Baolin Peng, and Kam-Fai Wong,\n‚ÄúLearning efÔ¨Åcient dialogue policy from demonstrations\nthrough shaping,‚Äù in Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguis-\ntics, Online, July 2020, pp. 6355‚Äì6365, Association for\nComputational Linguistics.\n[9] Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, and\nKam-Fai Wong, ‚ÄúDeep Dyna-Q: Integrating planning\nfor task-completion dialogue policy learning,‚Äù in Pro-\nceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\nMelbourne, Australia, July 2018, pp. 2182‚Äì2192, Asso-\nciation for Computational Linguistics.\n[10] Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu,\nYun-Nung Chen, and Kam-Fai Wong, ‚ÄúAdversarial\nadvantage actor-critic model for task-completion dia-\nlogue policy learning,‚Äù in 2018 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 6149‚Äì6153.\n[11] Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, and\nTaylor Berg-Kirkpatrick, ‚ÄúUnsupervised text style trans-\nfer using language models as discriminators,‚Äù in Pro-\nceedings of the 32nd International Conference on Neu-\nral Information Processing Systems , 2018, pp. 7298‚Äì\n7309.\n[12] Pawe≈Ç Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan,\nand Milica Ga Àási¬¥c, ‚ÄúMultiwoz‚Äìa large-scale multi-\ndomain wizard-of-oz dataset for task-oriented dialogue\nmodelling,‚Äù arXiv preprint arXiv:1810.00278, 2018.\n[13] Gabriel Gordon-Hall, Philip John Gorinski, and Shay B\nCohen, ‚ÄúLearning dialog policies from weak demon-\nstrations,‚Äù arXiv preprint arXiv:2004.11054, 2020.\n[14] Baolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun Li,\nJinchao Li, Michael Zeng, and Jianfeng Gao, ‚ÄúFew-\nshot natural language generation for task-oriented dia-\nlog,‚Äù arXiv preprint arXiv:2002.12328, 2020.\n[15] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov, ‚ÄúProximal policy opti-\nmization algorithms,‚ÄùarXiv preprint arXiv:1707.06347,\n2017.\n[16] Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth\nShriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-\nlor, Rachel Martin, Carol Van Ess-Dykema, and Marie\nMeteer, ‚ÄúDialogue act modeling for automatic tagging\nand recognition of conversational speech,‚Äù Computa-\ntional linguistics, vol. 26, no. 3, pp. 339‚Äì373, 2000.\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, ‚ÄúBert: Pre-training of deep bidirec-\ntional transformers for language understanding,‚Äù arXiv\npreprint arXiv:1810.04805, 2018.\n[18] Qi Zhu, Zheng Zhang, Yan Fang, Xiang Li, Ryuichi\nTakanobu, Jinchao Li, Baolin Peng, Jianfeng Gao, Xi-\naoyan Zhu, and Minlie Huang, ‚ÄúConvlab-2: An open-\nsource toolkit for building, evaluating, and diagnosing\ndialogue systems,‚Äù arXiv preprint arXiv:2002.04793 ,\n2020."
}