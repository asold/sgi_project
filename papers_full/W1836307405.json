{
  "title": "Regularization and nonlinearities for neural language models: when are they needed?",
  "url": "https://openalex.org/W1836307405",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287908820",
      "name": "Pachitariu, Marius",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2322040186",
      "name": "Sahani, Maneesh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W104184427",
    "https://openalex.org/W2157006255",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2402302915",
    "https://openalex.org/W2950797609",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2949190276",
    "https://openalex.org/W2124059530",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2118706537"
  ],
  "abstract": "Neural language models (LMs) based on recurrent neural networks (RNN) are some of the most successful word and character-level LMs. Why do they work so well, in particular better than linear neural LMs? Possible explanations are that RNNs have an implicitly better regularization or that RNNs have a higher capacity for storing patterns due to their nonlinearities or both. Here we argue for the first explanation in the limit of little training data and the second explanation for large amounts of text data. We show state-of-the-art performance on the popular and small Penn dataset when RNN LMs are regularized with random dropout. Nonetheless, we show even better performance from a simplified, much less expressive linear RNN model without off-diagonal entries in the recurrent matrix. We call this model an impulse-response LM (IRLM). Using random dropout, column normalization and annealed learning rates, IRLMs develop neurons that keep a memory of up to 50 words in the past and achieve a perplexity of 102.5 on the Penn dataset. On two large datasets however, the same regularization methods are unsuccessful for both models and the RNN's expressivity allows it to overtake the IRLM by 10 and 20 percent perplexity, respectively. Despite the perplexity gap, IRLMs still outperform RNNs on the Microsoft Research Sentence Completion (MRSC) task. We develop a slightly modified IRLM that separates long-context units (LCUs) from short-context units and show that the LCUs alone achieve a state-of-the-art performance on the MRSC task of 60.8%. Our analysis indicates that a fruitful direction of research for neural LMs lies in developing more accessible internal representations, and suggests an optimization regime of very high momentum terms for effectively training such models.",
  "full_text": "Regularization and nonlinearities for neural language\nmodels: when are they needed?\nMarius Pachitariu\nGatsby Computational Neuroscience Unit\nUniversity College London, UK\nmarius@gatsby.ucl.ac.uk\nManeesh Sahani\nGatsby Computational Neuroscience Unit\nUniversity College London, UK\nmaneesh@gatsby.ucl.ac.uk\nAbstract\nNeural language models (LMs) based on recurrent neural networks (RNN) are\nsome of the most successful word and character-level LMs. Why do they work so\nwell, in particular better than linear neural LMs? Possible explanations are that\nRNNs have an implicitly better regularization or that RNNs have a higher capac-\nity for storing patterns due to their nonlinearities or both. Here we argue for the\nﬁrst explanation in the limit of little training data and the second explanation for\nlarge amounts of text data. We show state-of-the-art performance on the popu-\nlar and small Penn dataset when RNN LMs are regularized with random dropout.\nNonetheless, we show even better performance from a simpliﬁed, much less ex-\npressive linear RNN model without off-diagonal entries in the recurrent matrix.\nWe call this model an impulse-response LM (IRLM). Using random dropout, col-\numn normalization and annealed learning rates, IRLMs develop neurons that keep\na memory of up to 50 words in the past and achieve a perplexity of 102.5 on the\nPenn dataset. On two large datasets however, the same regularization methods are\nunsuccessful for both models and the RNN’s expressivity allows it to overtake the\nIRLM by 10 and 20 percent perplexity, respectively. Despite the perplexity gap,\nIRLMs still outperform RNNs on the Microsoft Research Sentence Completion\n(MRSC) task. We develop a slightly modiﬁed IRLM that separates long-context\nunits (LCUs) from short-context units and show that the LCUs alone achieve a\nstate-of-the-art performance on the MRSC task of 60.8%. Our analysis indicates\nthat a fruitful direction of research for neural LMs lies in developing more acces-\nsible internal representations, and suggests an optimization regime of very high\nmomentum terms for effectively training such models.\n1 Introduction\nThe main paradigm shift in language modelling more than 20 years ago moved the ﬁeld from rule-\nbased systems to statistical, learned models. The most popular such models are based on frequency\nstatistics of short sequences of words, called n-grams. Various smoothing techniques were proposed\nto solve the fundamental problem of language modelling: most words and combinations of words\nappear very rarely in language. In fact, from the point of view of n-gram techniques, language\nmodelling is fundamentally a smoothing or in other words a regularization problem. More recently,\nmodels based on neural networks (NNLMs) have been shown to result in better representations\nof language due to their lower dimensional parametrization and higher ability to generalize [1].\nRecurrent NNLMs are a subclass of NNLMs that achieve even better performance with a clever\nparametrization of the predictive likelihood function through a recurrent neural network.\nHowever, to yield top perplexity scores, the neural network language models currently still need\nto be combined with N-gram based models, caching techniques and averaged over an ensemble\nof different models as shown in [2]. Furthermore, the RNN based LMs already require very long\n1\narXiv:1301.5650v2  [stat.ML]  20 Jun 2013\ntraining times individually and can be slow at run time if the average over an ensemble needs to be\ncomputed. Here we show that random dropout based regularization [3] improves the performance\nof RNN LMs on small datasets. Furthermore, a simpler model that we study here, the impulse-\nresponse LM (IRLM), achieves equal performance to the nonlinear RNN when both are regularized\nin this manner. The IRLM is similar to the log-bilinear language model [4] (LBL) and can also\nbe seen as a learned Long Short Term Memory (LSTM) network [5]. Special units in the LSTM\nhave a connection strength of 1 to themselves and no connections to the rest of the network, which\nalleviates the problem of decaying gradients in learning RNNs. Our IRLM is composed exclusively\nof these special LSTM units, with the generalization that the self-connection strength can be any-\nthing between -1 and 1 and it is learned together with the other parameters of the IRLM. In practice,\nthe IRLM learns to incorporate information from very large contexts of up to 50 words, while also\ncapturing local information.\n2 Recurrent neural networks\n2.1 Backpropagaton through time\nStandard recurrent neural networks are functions of input data. We consider sequential data that\ncomes in the form of discrete tokens, such that the tokens may for example be either characters or\nwords. An RNN function takes the following form:\nxt = f(W yt + R xt−1) ,\nwhere yt is the data and xt is the representation computed by the RNN. In the case of language\nmodelling, yt is a one-hot encoding of the token in position t, meaning yt is a vector of mostly\nzeros with just a one in the position of the active token. We call W the encoding matrix and R\nthe recurrent or transformation matrix. f is typically taken to be a sigmoidal nonlinearity such as\nthe logistic σ(x) = 1 /(1 + exp(−x)). It is generally believed that such strong nonlinearities are\nnecessary to model the apparently complicated dependencies in real-world data. The disadvantages\nof sigmoidal nonlinearities will become apparent when we consider the optimization problem below.\nTo make a statistical language model out of an RNN, we deﬁne a soft-max probability over the tokens\nin the sequence, such that this probability depends only on the representation xt computed by the\nRNN:\nP (yt+1|yt,yt−1,...) ∝exp(yT\nt+1Zxt), (1)\nwhere Z is a decoding matrix. We use the terminology of encoding, transformation and decoding\nmatrices for W,R and Z due to the similarity with methods based on autoencoders, RBMs or sparse\ncoding for static data. To obtain the likelihood of a full sequence we multiply together the conditional\nprobabilities deﬁned by equation 1 for every index T from 1 to the length of the sequence.\nThe likelihood of the RNN LM can be optimized by gradient descent. Notice that in order to learn\nW and R the gradient has to be backpropagated through time (BPTT) over successive activations\nxt. The intermediate gradients atxtwhich we callDtcan be computed incrementally in the reverse-\ntime direction\nDt = f′(xt) ◦\n(\nZT ∂P (yt+1|xt)\n∂yt+1\n+ RT Dt+1\n)\n, (2)\nwhere “◦” is elementwise multiplication and f′is the derivative. This backward pass for BPTT has\na similar functional form to the forward pass of equation 1 and the same computational complexity.\nIt has been observed early on in [6] that during training the contribution to the gradient of W and\nRfrom future times tends to vanish as it is backpropagated through several time steps. This can be\nunderstood by considering the Jacobians ∂xt/∂xt−τ of the transformations which the RNN is com-\nputing sequentially, and noticing that two main effects alter the size of the gradient/Jacobian. First,\nthe transformation matrix itself generally has eigenvalues less than 1 in absolute value in order to be\nstable. Consequently, the projection of xT on the eigenvectors of R decays exponentially over time\nsteps, both in the forward direction when computing the likelihood and in the backward direction\nwhen computing the gradients, because R and RT have the same eigenvalues. The second effect\ncomes from the nonlinearity which further multiplies the gradient Dt elementwise by the gradient\nof f. Typically during learning the sigmoid saturates either at 0 or 1 where the derivative f′is 0,\nwhich drastically reduces the contribution to the gradient from future time steps. We hypothesized\nthat this second effect has a much greater inﬂuence on BPTT than the eigenvalues of the transfor-\nmation matrix R. In fact, with linear RNNs where f is just the identity, we ﬁnd that the network\neasily learns matrices R with 20% of their eigenvalues larger than 0.9.\n2\n2.2 Optimizing RNNs on character-level language modelling\nWe developed the RNN model used here on character-level language modelling where instead of\nusing the RNN to predict words sequentially we use it to predict characters. RNNs can be difﬁcult\nto train and require many passes through the training data so it is important to have good optimization\ntechniques. In this section we show that training RNNs with very large values of momentum can\noptimize the difﬁcult cost function associated with character-level language modelling. Such cost\nfunctions were previously shown to be hard to optimize by [7] and [8]. The authors of those studies\npropose instead a Hessian-Free training method which uses second order information but can be\ncomputationally very demanding. The implementation of [7] takes ﬁve days to train in parallel on\neight high-end GPUs. Instead we push the momentum term to very high values, such that every\ngradient update is an effective average over the past approximately one million tokens. We also take\nadvantage of GPUs to run many gradient updates and ﬁnd that the RNN optimizes to prediction\nlevels comparable to those reported with Hessian-Free optimization in [8] on the ‘text8’ dataset, in\nless than a day on a single high-end GPU.\nWe use a version of RNN in which the nonlinearity f has been replaced with a rectiﬁer function of\nthe form f(x) = max(0,x). Unlike the sigmoidal nonlinearity, rectiﬁer functions have derivative\n1 for positive input which means they fully propagate the gradient in equation 2. This architecture\nis also currently and independently investigated by [9], though we were not aware of their results\nwhen we ran our own experiments. We found that the standard rectiﬁer nonlinearity they use there\nis relatively unstable during learning on character-level problems. Furthermore, the results reported\nin [9] for this architecture are well behind the state-of-the-art character level results reported in [8]\nwith the Hessian-Free optimized M-RNN of [7] (see table 1a). We adopted instead a smoothed\nversion of the rectiﬁer nonlinearity which is differentiable everywhere and still 0 when the input\nis negative f(x) = max(0,x −atanh x/a) and found that this simple smooth nonlinearity can be\nused stably with large learning rates. When ais made small this function approaches the standard\nrectiﬁer nonlinearity. The H-RNN can model highly nonlinear distributions of sequences as shown\nby its performance on the character-level modelling task. We also used the RNN in word-level\nexperiments. There we reverted back to the standard rectiﬁer nonlinearity, as we did not observe the\nsame instabilities.\nTo our knowledge this is the ﬁrst published result showing that stochastic gradient descent (SGD)\nlearning of nonlinear RNNs can achieve the same performance as the second order optimization\nmethods proposed by [7] and evaluated on the text8 dataset by [8]. The training cost of RNN with\ngradient descent is about 15h on a single high-end GPU card, while the HF-MRNN takes ﬁve days\non 8 high-end GPUs, so we observe a more than 50 fold speedup for the same predictive likelihoods.\nNotice that while a very similar RNN architecture is evaluated in [9], their results are worse than\nours, owing perhaps to the small size of the network they use (512 neurons as opposed to our 2048)\nand the non-smooth version of the rectiﬁer nonlinearity used there. We did not ﬁnd it necessary\nto clip the gradients as done in [9], because the very high momentum term smoothed the gradient\nsufﬁciently. Although we did observe sharp falls of the cost function a few times during training,\nthese were not associated with large changes in the parameters, and the network recovered with a\nfew parameter updates to its previous value of the cost function. Large momentum seemed however\nto be crucial for fast learning. Without it, the network diverged even with much smaller learning\nrates. The analysis of [10] also suggests momentum as an important technique for training RNNs\nand deep neural networks, although the authors suggests it should be used in conjunction with a\nspecial initialization of the RNN as an Echo State Network [11] but we did not investigate such\nan initialization. While we did initialize the parameters of the encoder and decoder matrices to\nrelatively large values, we initialized the recurrent weights close to 0.\nIn separate experiments we trained the same RNN on the raw version of the same ﬁrst 100 MB\nof Wikipedia. We found that SGD training diverged much more easily in that case and required\nclipping the gradients as is also done in [9]. On closer inspection, we found that all such cases of\ndivergence were associated with very rare events not related to language at all, such as multiple\nrepeats of the same low-probability character like dashes and exclamation marks. With a minimum\nof preprocessing we removed such events and the RNN was able to learn successfully from raw\nWikipedia without clipping gradients in SGD. More research is needed to clarify the actual geometry\nof neural networks because as [9] points out, the classical image of long narrow ravines might be\nquite wrong.\n3\nBits per character\nHF-MRNN1 1.54\nRNN2 1.80\nIRLM w/ NN output 2.12\nRNN3 1.55\nskipping RNN 1.48\nsubword HF-RNN1 1.49\n(a) Character-level models.\n1 as reported in [8]. 2 as reported in [9]. 3\nour implementation.\nPerplexity\nKN5 298\nKN5+cache 228\nIRLM 197\nRNN 179\n(b) Word-level results on ‘text8’\nPerplexity MRSC\nKN4 39%\nLBL 54.7 %\nIRLM 103.5 52.6%\nRNN 79 51.3%\n(c) Word-level results on project Gutenberg\nMRSC\nIRLM 54.8%\nRNN 52.5 %\nIRLM (LCUs) 60.8%\nRNN (384 block) 55.0%\n(d) Models initialized with long contexts\nTable 1: Results on text8 (a,b), Project Gutenberg (c) and the MRSC (c, d).\nAs a curious fact, we note that the RNN models we train discover an ingenious solution for main-\ntaining stability during learning. This is further detailed in the supplemental material.\n2.3 Skipping RNN extension\nIt is much easier to extend RNNs when gradient descent is used for learning, as opposed to when\nthe Hessian-Free method is used. This can be important; as shown in [8] character-level modelling\ngenerally lags behind word-level modelling in terms of predictive likelihoods so further work is\nneeded to make character-level RNNs competitive. We present one enhancement we found with a\nsimple extension of the RNN. We added skipping connections between the hidden units correspond-\ning to the ﬁrst letters of consecutive words. These connections formed a new matrix of parameters\nRskip of the same size as R. Rskip can potentially propagate information over longer distances in\ntext. The skipping RNN is effectively a hybrid between a word-level and a character-level model.\nAdding in Rskip lowered the cross-entropy on the test set from 1.55 bpc to 1.48 bpc, while slightly\nincreasing training times. This performance is still only about on par with state-of-the-art word-level\nn-gram models on this dataset, as estimated in [8]. The skipping RNN may also extend to word-level\nlanguage modelling. Instead of adding skipping connections between the ﬁrst letters of words, we\ncould add skipping connections between the ﬁrst words of phrases, sentence clauses or even full\nsentences. We are currently investigating a hierarchy of such skipping connections but do not have\nany results yet.\nWe note that a different solution for improving character-level language models has been discussed\nin [8], where the authors split words into their most frequent parts. These subparts consisted of short\nsequences of 1-3 characters. Indeed the HF-MRNN trained on the fragmented words achieved better\ncross-entropies that match the skipping RNN proposed here as can be seen in table 1a.\n3 Impulse response language model (IRLM)\nWe considered completely giving up the nonlinearity f and replacing it with the identity. Addi-\ntionally, we set the recurrent matrix to be diagonal. We call this model an impulse response neural\nnetwork. This model has completely linear propagation of the gradient in equation 2, for both pos-\nitive and negative inputs. The IRLM computes a linear function xt of the previous observations\nbefore passing yT\nt+1Zxt through the softmax nonlinearity. Note however that the one-hot embed-\nding of tokens is itself a highly nonlinear operation. The log-bilinear LM of [4] has a similar linear\nparametrization, however it lacks the long timescales in xt which the IRLM can generate. As we\nwill see in the results section, the IRLM is able to model the complex sequential patterns in language\nat the word-level.\nThe linear contribution from yt−τ to P (yt+1|yt,yt,...) before the soft-max nonlinearity can be\nrewritten as yT\nt+1ZRτWyt−τ, regardless of whether R is diagonal or not. The LBL model of [4]\nuses a different parametrization whereRτ is replaced with arbitrary interaction matricesDτ. In fact,\njust like we do here, [4] use diagonal matricesDτ. There are several advantages to usingDτ = Rτ.\n4\nThe ﬁrst and most important is the implicit inductive bias the IRLM has for long timescales: words\nseparated by several other words have an interaction term that only depends weakly on the length of\nthe separation because Rτ and Rτ+1 are similar for entries of R close to 1. In the LBL model the\ninteraction terms are entirely different at different delays τ and τ + 1. Consequently the IRLM can\ngeneralize information learned at one delay τ to all other delays. For self-connections in R close to\n1, the hidden units of the IRLM can be thought of as representing the topics of text. In fact, a recent\nstate-of-the-art document topic model [12] uses a similar parametrization of a neural network, there\ncalled NADE (neural autoregressor density estimation). NADE is the same model as the IRLM with\nself-connections of 1 but with an additional nonlinearity in front ofxt before multiplication with the\nmatrix Z. To model bag-of-words representations of documents, NADE assumes a random ordering\nof the words in each document.\nFor optimization, we noticed that the gradient terms on the self-recurrent connections of the IRLM\nwere very large compared to the gradients on the encoding and decoding matrices. This can be\nexplained by the fact that the gradient of R is updated on every iteration, whereas entries in the\nencoding matrix are only updated when their associated words are present. Entries in the decoding\nmatrix, although non-zero on every iteration, are only large when their associated words are present\ndue to the softmax nonlinearity. We used 1000-fold smaller learning rates for the self-recurrent\nconnections than for the encoding and decoding parameters. We also considered a dynamic version\nof the IRLM, where the parameters of the model are dynamically adapted at test time in the direction\nof the gradient with respect to the new observations. We found modest but signiﬁcant improvements,\nof a similar magnitude to those reported in [2].\n4 Regularization with random dropout and column normalization\nWe found that on small corpora like Penn, the IRLM was still able to overﬁt severely despite its\nrelatively low-dimensional parametrization (compared to N-grams). On such problems, we applied\nthe regularization method proposed by [3] and called there random dropout. The idea is to introduce\nnoise into the function computed by the neural network so as to make the model more robust to\ntest examples never seen before. The intuition provided by [3] is that the noise prevents units in\nthe neural network from co-adapting their weights to overﬁt the training data. To avoid introducing\ninstabilities into the recurrent part of the LMs, we added the noise only on the decoding portion of\nthe model.\nFormally we deﬁne η to be a vector of Bernoulli random variables with probabilities 0.5 and\nlength the dimensionality of the RNN. Then ¯xt = xt ◦η, and ¯x replaces x in equation 1 so that\nPη(yt+1|η,yt,yt−1,...) ∝exp(yT\nt+1Z¯xt). We deﬁne\nP (yt+1|yt,yt−1,...) = ⟨Pη(yt+1|η,yt,yt−1,...)⟩η\nBecause each Pη is a normalized probability, so is the average over allη-s. This is a slightly different\ngenerative model than the standard RNN, but it overﬁts less to the training data. A tractable lower\nbound on the likelihood of the noisy RNN model follows from a simple application of Jensen’s\ninequality\nlog P (yt+1|yt,yt−1,...) = log⟨Pη(yt+1|η,yt,yt−1,...)⟩η\n≥⟨log⟨Pη(yt+1|η,yt,yt−1,...)⟩η\nWe will estimate the gradient of this likelihood with samples from η. The resulting algorithm is\nexactly that of [3].\nWe also found that column normalization (CN) helped to further increase the performance of the\nIRLM on the Penn corpus. CN consists of ﬁxing the L2 norm of the incoming/outgoing weights\nto each hidden unit. In the IRLM we ﬁnd that low values of the column norm result in longer\ntimescales. This is because large values of the hidden units can only be obtained by having large self-\nconnections. In turn, large values of the hidden units are needed to generate low entropy predictive\ndistributions for each word. Notice CN was also used in [3] in conjunction with the random dropout\nmethod but apparently for different reasons. The authors of [3] found that CN helped them use\nlarger gradient steps during learning, while we found that CN helped generalization. Unfortunately\nthe magnitude to which the columns of W and Z are normalized does inﬂuence the performance of\nthe model so we had to crossvalidate this parameter separately by running 5 different experiments\nto ﬁnd a good value (15) for the norm. One additional regularization strategy that we always used\nwas to anneal the learning rates towards 0 on every epoch for which the validation cost decreased.\n5\n0 128 256 384 512\n0.25\n1\n3\n10\n30\n60\nHidden dimension #\nTime scale (words in the past)\n0 200 400 600\n1\n3\n10\n50\nTimescales of IRLM with LCU constraints\non Project Gutenberg\nwords in the past\nIRLM unit number\nFigure 1: a. The timescales of the IRLM on the Penn Corpus. b. The timescales of the IRLM\ninitialized with LCUs on the Project Gutenber dataset. Note that the IRLM has discovered long-\nrange dependencies in language, as indicated by the long timescales of up to ﬁfty words. It is\ninteresting that in our experiments with caching models, the unigram cache also only helps the\nperplexity up to a length of 50-100 words in the past.\n5 Results\nWe report the performance of the studied models on three datasets commonly used in the literature.\nThe ﬁrst is the Penn Corpus which contains 930k training word tokens. The second is the ’text8’\ndataset which contains a cleaned up version of the ﬁrst 100 million characters from Wikipedia 1.\nBoth of these corpora have well-deﬁned cross-validation folds as described in [8], which allows us\nto directly compare our results to those reported elsewhere. Finally, we learned word-level models\nfrom the training set of the Microsoft Research Sentence Completion Challenge. The training dataset\nconsists of about 500 copyright-free 19th century novels available through Project Gutenberg [13].\nAll of the models reported that we implemented have 512 hidden units, unless mentioned otherwise.\nThis many hidden units are typical for word-level modelling. For the Penn dataset with a 10k\nvocabulary, this results in about 10 million parametes, while on the large datasets the models had\nabout 70 million parameters.\n5.1 Small dataset: Penn corpus\nIt is commonly believed that the recurrent neural network (RNN) based models are able to cap-\nture highly nonlinear distributions of sequential data which simpler feedforward NNs cannot, thus\nexplaining the better performance of RNNs for word-level language modelling. The unregular-\nized IRLM achieves similar perplexities on the training data with the unregularized nonlinear RNN,\nmeaning they can capture similarly many patterns (table 2). However, the RNN generalizes better\nand achieves lower perplexities on test data. This means the nonlinear nature of the RNN serves as\na kind of regularizer, allowing the network to learn some important patterns present in language and\npreventing some spurious associations to be made, which the feedforward models make more easily.\nAs a generative model, the RNN can be said to have better inductive biases than a feedforward neural\nnetwork, perhaps owing to its dynamical representation and nonlinearities. However, after random\ndropout regularization, we ﬁnd that the performance of the two models considered here increases\nsigniﬁcantly and to similar levels (table table 2). The regularized IRLM still overﬁts slightly more\nto the training data than the regularized RNN, but their predictive performances are nearly the same.\nThe IRLM but not the RNN is further improved by column normalization. The regularized IRLM\nachieves the state-of-the-art result on the Penn corpus for a single model with a perplexity of 102.5\ndown from the previous best published result of 123 [2]. Mixtures of models usually fare better and\ncan enhance performance to a perplexity of 77 when a large number of very different models are\naveraged together [2].\nIn the literature, it is common to also report the results of models averaged with KN5, unigram\ncaches and/or averaged with multiple copies of the model trained independently. All of these further\nimprove the results considerably as shown in table 2. We highlight that of special interest are results\nfor single models, or for single models combined with n-gram based models, as these will have\nthe lowest computational cost at runtime and lowest memory trace. This cost can be signiﬁcant with\nlarge vocabularies. Also, the Penn dataset is very small and on larger datasets training more than one\n1http://mattmahoney.net/dc/text8.zip\n6\nSingle train/test +KN5+cache x10 x10+KN5+cache\n5-gram Kneser-Ney1 10/141.2 125.7\nfeedforward NNLM1 ?/140.2 106.6\nLog-bilinear LM1 ?/144.5 105.8\nRNN1 124.7 97.5 102.1 89.4\ndynamic RNN1 ?/123.2 98.0 101.0 90.0\nRNN3 (no reg) 34/126\nRNN3 (2DO&CN) 40/107\nIRLM(no reg) 32/137\nIRLM(L1 reg) ?/125\nIRLM (2DO) 38/109\nIRLM (2DO&CN) 42/102.5 94 98.5 92.5\ndynamic IRLM(2DO&CN) ?/98.5 90.5 95 89\nTable 2: Results on Penn corpus.\n1 as reported in [2],2 trained with random dropouts and column normalization,3 our implementation\nmodel can be computationally expensive and memory taxing. One interesting aspect of the IRLM is\nthat we can directly assess how large of a context the network uses by looking at the diagonal matrix\nR. The effective contribution to xt from a timelag τ >0 in the past is Rτ−1Wyt−τ which then\ndecays like Rτ = exp(log(R)τ). The timescales of the network are deﬁned as −1/log(R), where\nthe division operation is understood elementwise on the diagonal of R. For an IRLM trained on the\nPenn corpus we plot these timescales in ﬁgure ﬁgure 1a. One can see that the IRLM has learned\nseveral very long timescales of up to 50 words. This reminds of the beneﬁts offered by caching\nmethods to language models: many language models are improved at test time if the predictive\nprobability of the model is interpolated with a unigram model learned exclusively from the previous\n50-100 words. We can see that nonetheless most of the timescales of the IRLM are relatively short\nin order to model the local grammar of language. Each experiment on the Penn dataset took about\nthree hours to run on a GTX 680 GPU.\n5.2 Large datasets: ’text8’ and Project Gutenberg\nWe also ran the same experiments on a much larger corpus known as ’text8’, consisting of the ﬁrst\n100M characters of Wikipedia. As a preprocessing step we considered only words that appear at\nleast ﬁve times in the training set, converting all others to the special ’<unk >’ token. This left us\nwith a large vocabulary of 67428 words and a training corpus of 15301600 words. Training RNN\nmodels on this much data would require several days for a single experiment so we turned to the\nfast approximate training method called noise contrastive estimation (NCE), proposed by [14] and\nadapted for neural language modelling by [15]. The reader is advised to read [15] for full details of\nthe procedure. Brieﬂy, the method involves training a classiﬁer to distinguish between real sampled\nsequences of words and sequences where one word is sampled from noise distributions, where the\nclassiﬁer is based on the generative model being trained. The perplexity results that we get with NCE\ntrained neural language models are signiﬁcantly better than those of n-gram models. In addition, the\nRNN-LM achieves 10% lower perplexity than IRLM on both training and test data, indicating that\nthe RNN has a larger capacity for storing sequences 2.\nWe also ran experiments on the training dataset for the Microsoft Sentence Completion Challenge\n[13]. This dataset consists of about 500 19th century novels from the Project Gutenberg database.\nThe RNN-LM was again able to capture many more patterns in the data compared to the IRLM,\nresulting in a 20% perplexity difference.\n2In the ﬁrst version of this arXiv paper we reported a perplexity of 191 for the RNN model on this dataset.\nThat result was after 10 epochs of training, which is usually considered sufﬁcient especially for such large\ndatasets. However, in further experiments we found that RNN but not IRLM models improved up until 50\nepochs of training to a perplexity of 179, resulting in signiﬁcant perplexity differences between the two models.\nWe thank Yoshua Bengio for suggesting that this might be the case.\n7\nWe ﬁnd that dropout regularization no longer helps generalization with this large amount of training\ndata. Instead, the performance of the regularize and non-regularized moedls is almost the same, and\nmuch better (∼35%) than the vanilla n-gram model to which comparisons are usually made in the\nliterature (Kneser-Ney of order 5). However, cache models signiﬁcantly improve n-grams and in this\ncase by a very large margin of∼25% perplexity. The neural language models offered improvements\nof 20-30% perplexity over that.\n5.3 Microsoft Research Sentence Completion Challenge\nThe MRSC challenge consists of 1040 SAT-style sentences with blanked words and ﬁve possible\nchoices for each blank. The task is to choose the sentence that makes most sense out of the ﬁve\npossible ones. Humans perform on this task at around ∼90 % correct [13]. The training set for\nlanguage models on this task consists of the Project Gutenberg novels. N-gram models perform\npoorly in the MRSC challenge at around 40 % correct but RNN-LMs perform at around 50%, while\nthe LBL model of [15] obtains 54.7%. Although [16] reports results for RNN-LMs, it might be that\nthe advantage of the LBL model is related to the more efﬁcient training method, thus resulting in\nmore epochs of learning. We trained both RNN-LMs and IRLMs on the Project Gutenberg novels\nusing NCE, and report in table 1c their respective performance.\nIn agreement with [16] we ﬁnd that the NCE-trained RNN-LM performs at ∼50% correct. Despite\nhaving a much lower perplexity, the IRLM performs slightly better at 52.5%, though not as well as\nthe LBL model reported in [15]. This ordering is highly surprising, because a 20% perplexity gap\nexists between the RNN-LM and the IRLM. We conjecture that due to its restricted representation\nthe IRLM needs to focus more than the RNN on the long range dependencies in language, which\nhelp it have better semantic comprehension. To verify our conjecture, we ran an IRLM experiment\nwhere all the units were initialized to 0.9. These units effectively ‘see’ about ten words into the\npast and we call these long context units (LCUs). During training, a proportion of about 25% of the\nunits dropped in value to code for short context dependencies but the other units remained above\n0.5. This model scored 0.55% correct on the MRSC task. We then designed another IRLM in which\nwe enforced LCUs to keep large values by always constraining them to lie between 0.7 and 1. We\nused 128 short-context units initialized at 0 and 384 LCUs. After training, we used only the values\nof the LCUs to make predictions about which MRSC sentence is correct by setting the values of the\nshort-context units to 0. This is potentially highly disruptive for the normalization constant in each\ncontext, so we did not normalize the predictions of the LCUs. The LCUs alone achieved 60.8%\ncorrect which is a new state-of-the-art result on this task. After learning, most of the LCUs were in\nthe range of 0.7 to 0.9 (ﬁgure 1b), suggesting the relevant timescales analyzed are between 3 and 10\nwords in the past.\nWe attempted similar experiments with RNN-LMs. To this end we partitioned the recurrent matrix\ninto two blocks of units and set connections between blocks to 0. We initialized connections within\nthe larger block of 384 units with 0.9 on the diagonal and the smaller block of 128 units with\nall zeros. This way, like the LCUs, the 384 block of the RNN is biased by initialization to keep\ninformation from long contexts. However, it was not possible to enforce this constraint during\nlearning. The best result we could get using the 384 unit block of the RNN was 55% correct, and\nonly when using very small learning rates for the 384 unit block. Initializing the 384 unit block to\nan Echo State Network as proposed in [10] was even less succesful.\n6 Conclusions\nWe have presented language modelling experiments with RNNs and IRLMs aimed at evaluating the\ntwo models’ intrinsic regularization properties, storage capacity and ability to capture long contexts.\nOn a small dataset we found that the regularization method (random dropout) was far more important\nthan the model used. The best model was an IRLM that scored 102.5 perplexity on the test set,\nand used several regularization techniques: random dropout, column normalization and annealed\nlearning rates. On large datasets, the high capacity of the RNN allows it to store and recognize more\npatterns in language. Nonetheless, on a sentence comprehension challenge that requires integrating\ninformation over long contexts we found that the IRLM was slightly superior. In addition, the\nIRLM’s simple representation allowed us to use only the long context units for scoring sentences,\nwhich resulted in a large boost in accuracy to 60.8% correct. This represents a new best result on this\n8\ndataset improving on the 54.7% from [15]. While the same information from long contexts might\nin principle be embedded in the representation of the RNN, it is not straightforward to obtain. We\nsee that there is reason to develop more easily accessible representations for neural language models\nand IRLM constitues a ﬁrst step in that direction.\nAcknowledgements\nWe thank Andriy Mnih and Yoshua Bengio for reading versions of this article and providing helpful\ncomments. We also thank Andriy for introducing us to neural language models and for suggesting\nthe name IRLM and we thank Yoshua for pointing out that the nonlinear dynamics of the RNN really\nshould matter for storing patterns and making predictions.\nReferences\n[1] Y Bengio, R Ducharme, P Vincent, and C Jauvin. A neural probabilistic language model. Journal of\nMachine Learning Research, 3:1137–1155, 2003.\n[2] T Mikolov, A Deoras, S Kombrink, L Burget, and JH Cernocky. Empirical evaluation and combination\nof advanced language modeling techniques. Conference of the International Speech Communication\nAssociation, 2011.\n[3] GE Hinton, N Srivastava, A Krizhevsky, I Sutskever, and RR Salakhutdinov. Improving neural networks\nby preventing co-adaptation of feature detectors. arXiv:1207.0580v1, 2012.\n[4] A Mnih and G Hinton. Three new graphical models for statistical language modelling. International\nConference on Machine Learning, 2007.\n[5] S Hochreiter and J Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, 1997.\n[6] Y Bengio, P Simard, and P Frasconi. Learning long-term dependencies with gradient descent is diffcult.\nIEEE Transactions on Neural Networks, 5(2):157–166, 1994.\n[7] I Sutskever, J Martens, and G Hinton. Generating text with recurrent neural networks. International\nConference on Machine Learning, 2011.\n[8] T Mikolov, I Sutskever, A Deoras, HS Le, S Kombrink, and J Cernocky. Subword language modelling\nwith neural networks. unpublished, 2012.\n[9] R Pascanu, T Mikolov, and Y Bengio. Understanding the exploding gradient problem.\narXiv:1211.5063v1, 2012.\n[10] I Sutskever, J Martens, G Dahl, and G Hinton. On the importance of initialization and momentum in deep\nlearning. International Conference on Machine Learning, 2013.\n[11] H Jaeger and H Hass. Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless\ncommunication. Science, 304:78–80, 2004.\n[12] H Larochelle and S Lauly. A neural autoregressive topic model. Advances in Neural Information Pro-\ncessing, 2012.\n[13] G Zweig and CJC Burges. The microsoft research sentence completion challenge. Technical Report\nMSR-TR-2011-129, Microsoft Research, 2011.\n[14] M Guttmann and A Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormal-\nized statistical models. Proc Int Conf on Artiﬁcial Intelligence and Statistics (AISTATS2010), 2010.\n[15] A Mnih and YW Teh. A fast and simple algorithm for training neural probabilistic language models.\nInternational Conference on Machine Learning, 2012.\n[16] T Mikolov. Statistical language models based on neural networks. PhD thesis, Brno University of Tech-\nnology, 2012.\n9",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9388024210929871
    },
    {
      "name": "Recurrent neural network",
      "score": 0.8187094926834106
    },
    {
      "name": "Computer science",
      "score": 0.7730040550231934
    },
    {
      "name": "Language model",
      "score": 0.6906566619873047
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.6119088530540466
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4908352494239807
    },
    {
      "name": "Context (archaeology)",
      "score": 0.42028170824050903
    },
    {
      "name": "Bottleneck",
      "score": 0.4144701659679413
    },
    {
      "name": "Artificial neural network",
      "score": 0.3981543481349945
    },
    {
      "name": "Speech recognition",
      "score": 0.3832591772079468
    },
    {
      "name": "Machine learning",
      "score": 0.3494971990585327
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800173700",
      "name": "UCL Australia",
      "country": "AU"
    }
  ],
  "cited_by": 32
}