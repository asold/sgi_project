{
  "title": "An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach",
  "url": "https://openalex.org/W4389245454",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4367266980",
      "name": "Suhaima Jamal",
      "affiliations": [
        "Georgia Southern University"
      ]
    },
    {
      "id": "https://openalex.org/A1976080149",
      "name": "Hayden Wimmer",
      "affiliations": [
        "Georgia Southern University"
      ]
    },
    {
      "id": "https://openalex.org/A4208687727",
      "name": "Iqbal Sarker",
      "affiliations": [
        "Edith Cowan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3094117827",
    "https://openalex.org/W4313204135",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4378574344",
    "https://openalex.org/W3138953784",
    "https://openalex.org/W4205958384",
    "https://openalex.org/W4375854095",
    "https://openalex.org/W3018948480",
    "https://openalex.org/W2295464403",
    "https://openalex.org/W2405958784",
    "https://openalex.org/W3119876417",
    "https://openalex.org/W327991062",
    "https://openalex.org/W2949836779",
    "https://openalex.org/W2565439473",
    "https://openalex.org/W3006139231",
    "https://openalex.org/W3003576327",
    "https://openalex.org/W2894829535",
    "https://openalex.org/W4385574724",
    "https://openalex.org/W3057377271",
    "https://openalex.org/W4206088862",
    "https://openalex.org/W3161935901",
    "https://openalex.org/W3163881436",
    "https://openalex.org/W4226327328",
    "https://openalex.org/W4206833365",
    "https://openalex.org/W4372260394",
    "https://openalex.org/W4225454120",
    "https://openalex.org/W6603609622",
    "https://openalex.org/W3015534721",
    "https://openalex.org/W3192478068",
    "https://openalex.org/W4313015712",
    "https://openalex.org/W6608277728",
    "https://openalex.org/W4367156098"
  ],
  "abstract": "Abstract Phishing and spam detection is a long standing challenge that has been the subject of much academic research. Large Language Models (LLM) have vast potential to transform society and provide new and innovative approaches to solve well-established challenges. Phishing and spam have caused financial hardships and lost time and resources to email users all over the world and frequently serve as an entry point for ransomware threat actors. While detection approaches exist, especially heuristic-based approaches, LLMs offer the potential to venture into a new unexplored area for understanding and solving this challenge. LLMs have rapidly altered the landscape from business, consumers, and throughout academia and demonstrate transformational potential for the potential of society. Based on this, applying these new and innovative approaches to email detection is a rational next step in academic research. In this work, we present IPSDM, an improved phishing spam detection model based on fine-tuning the BERT family of models to specifically detect phishing and spam email. We demonstrate our fine-tuned version, IPSDM, is able to better classify emails in both unbalanced and balanced datasets.",
  "full_text": "An Improved Transformer-based Model for\nDetecting Phishing, Spam, and Ham: A Large\nLanguage Model Approach\nSuhaima Jamal \nGeorgia Southern University\nHayden Wimmer \nGeorgia Southern University\nIqbal Sarker  (  m.sarker@ecu.edu.au )\nEdith Cowan University\nResearch Article\nKeywords: Large language model (LLM), phishing, spam, arti\u0000cial intelligence, cyber security, \u0000ne tuning,\nDistilBERT, RoBERTA\nPosted Date: December 1st, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3608294/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations:\nNo competing interests reported.\nFunding: The authors have no relevant \u0000nancial or non-\u0000nancial interests to disclose.\nCompeting interests: The authors have no con\u0000icts of interest to declare that are relevant to the content\nof this article.\nAuthors' contributions: The primary draft of the manuscript was prepared by Suhaima Jamal; Reviewed,\ncritically reviewed and edited by Hayden Wimmer and Iqbal H. Sarker. All authors read and approved the\n\u0000nal manuscript.\nAn Improved Transformer-based Model for\nDetecting Phishing, Spam, and Ham: A Large\nLanguage Model Approach\nSuhaima Jamal1, Hayden Wimmer1, Iqbal H. Sarker2\n1Department of Information Technology, Georgia Southern University,\nStatesboro, 30458, Georgia, USA.\n2Security Research Institute, Edith Cowan University, Perth, W A-6027,\nAustralia.\nContributing authors: sj14077@georgiasouthern.edu;\nhwimmer@georgiasouthern.edu ; m.sarker@ecu.edu.au;\nAbstract\nPhishing and spam detection is a long standing challenge tha t has been the\nsubject of much academic research. Large Language Models (LLM) h ave vast\npotential to transform society and provide new and innovative ap proaches to solve\nwell-established challenges. Phishing and spam have cause d ﬁnancial hardships\nand lost time and resources to email users all over the world and freq uently\nserve as an entry point for ransomware threat actors. While detectio n approaches\nexist, especially heuristic-based approaches, LLMs oﬀer the potential to venture\ninto a new unexplored area for understanding and solving this cha llenge. LLMs\nhave rapidly altered the landscape from business, consumers, and throughout\nacademia and demonstrate transformational potential for the pot ential of society.\nBased on this, applying these new and innovative approaches t o email detection\nis a rational next step in academic research. In this work, we prese nt IPSDM, an\nimproved phishing spam detection model based on ﬁne-tuning t he BERT family\nof models to speciﬁcally detect phishing and spam email. We d emonstrate our\nﬁne-tuned version, IPSDM, is able to better classify emails in b oth unbalanced\nand balanced datasets.\nKeywords: Large language model (LLM), phishing, spam, artiﬁcial inte lligence, cyber\nsecurity, ﬁne tuning, DistilBERT, RoBERTA\n1\n1 Introduction\nPhishing and spam are a continual concern that continues to plague users and cost\neconomies the world over ﬁnancial resources. Individuals and busine sses suﬀer from\nﬁscal loss and ﬁnancial setback due to such issues caused by spam and phi shing\nattacks. This kind of fraudulent endeavors attempt to deceive indi vidual into reveal-\ning sensitive and conﬁdential information, such as ﬁnancial details or l ogin credentials\netc. While Artiﬁcial Intelligence (AI) approaches have attempted to assuage these\nissues [\n1], heuristic-based systems continue to dominate. Radical new approac hes have\nemerged due to advances in technology and increased research investme nt by both\nthe public and private sector. The recent advancements in AI-based s olutions have led\nto the development of innovative and unconventional strategies to comb at spam and\nphishing tactics such as demonstrated by Anand et al. [ 2].\nTransformer-based models possess a revolutionary impact on the devel opment of spam\nand phishing classiﬁcation models while processing, understandi ng, and interpreting\nthe text data inputs. For email-based datasets, such models are const antly evolv-\ning with several regularization methods. Furthermore, attention-b ased mechanisms in\ntransformer allows model interpretability and makes it easier in un derstanding certain\nclassiﬁcation decision [ 3]. Large Language Models (LLMs), made famous by Open AI’s\nChatGPT, have emerged triumphant in solving new problems while be ing adapted to\nwell-established challenges such as phishing and spam. Open AI’s Ch atGPT runs on\nits GPT engine and has been able to make large strides in consumer and bus iness\nadoption [ 4]. The most famous competing LLMs are available from a plethora of ven-\ndors such as Google, Meta, and MIT to name a few and the emergence of competi ng\nLLMs such as Llama and Bert have been open sourced thereby fueling rese arch and\ndevelopment from large institutions all the way down to the consumer . While these\nmodels are available for download, the ability to run pre-trained mod els such as BERT\nis still in nascent stages with more consumers having access to local GPU technology\nas well as organizations like Google with Collaboratory and Hugging Face with its\ntransformer’s library and model hosting.\nLLMs are general in nature and pre-trained by the creators and published f or com-\nmercial and non-commercial licenses. There are a multitude of input s to train a LLM\nsuch as web scraping, document corpus, and even text sources such as e mail and tran-\nscribed books, discussions, or speeches. While LLMs perform well on general tasks,\nthey can be ﬁne-tuned to improve their performance on more speciﬁc tasks. One\nsuch example is FinBERT [\n5] where BERT (Bidirectional Encoder Representations\nfrom Transformers) was trained on ﬁnancial speciﬁc documents and is abl e to better\nrespond to use prompts on ﬁnance. Other such advances are in progress for medical\ndata to aid in both physician decision making and end-user queries. B ERT employs a\nself-attention mechanism which enables the model to capture both c ontextual informa-\ntion and dependencies among words in any text sequence. Through the se lf-attention\nmethod, the weights of relevant important words are calculated. Atten tion scores are\nmeasured for all words or input tokens and passed through SoftMax function. A rich\ncontextual embedding can be generated by BERT based models which all ow to excel\nin several natural language understanding tasks.\n2\nWithin the family of BERT-based models, DistilBERT and RoBERTA are two sig-\nniﬁcant variants and have been used for tasks such as fake news [ 6] or to make\npredictions via Twitter data [ 7]. Both models are built based on transformer archi-\ntecture while excelling in NLP processing tasks. DistilBERT is designed for reducing\nthe number of parameters making it faster and smaller version of BERT. W hereas\nRoberta is considered as more optimized and robust version. In this wor k, we present\nan Improved Phishing and Spam Detection Model (IPSDM), a custom tr ained and\nﬁne-tuned version of DistilBERT and RoBERTA. We ﬁne-tuned these m odels specif-\nically on phishing, spam, and ham data from multiple sources. We demonst rate that\nour ﬁne-tuned IPSDM outperforms basic BERT and RoBERTA on both imbalanc ed\nand balanced datasets of phishing, spam, and ham.\nThe rest of the paper is outlined as follows; section 2 describes the related current-\nstate-of arts research works. Section 3 provides a detailed explanati on of the proposed\nmodel’s framework and methodology. In section 4, the experimental outc omes and\nresults are broadened. Furthermore, section 5 encompasses an elaborate d discussion\nof the results. Finally, section 6 holds the concluding remarks and f uture prospects of\nthis work.\n2 Literature Survey\n2.1 Machine Learning and Deep Learning based Methods\nMachine learning and deep learning along with artiﬁcial intelligence , are progres-\nsively making way into a wide large of industries, such as healthcare , cyber security,\neducation, and so on [\n8]. Numerous machine learning and deep learning-based spam\nemail detection and classiﬁcation applications have been carried out ov er the past few\ndecades by many researchers. In such studies [ 9–14], authors have proposed, reviewed,\nand evaluated spam ﬁltering models where the classiﬁcation models ar e based on tra-\nditional machine learning algorithms, i.e., Na¨ ıve Bayes, Random Fores t, SMV mostly.\nGovil et al. [ 9] have created a dictionary, named “stopwards” for removing the helpi ng\nverbs from email. Then, the algorithm is executed for checking the p ossibility of being\nspam or not. A machine learning classiﬁer, Na¨ ıve Bayes has been applie d for the iden-\ntiﬁcation purpose where non-spam emails were classiﬁed as spam, 1 and non -spam, 0\n[9].\nSimilarly, Chen et al. [ 10] have evaluated machine learning algorithms for detecting\nspam tweets. A large dataset containing around 600 million public tweets h ave been\ncollected ﬁrst. Later, Trend Micro’s Web Reputation System was appli ed for labeling\nthe spam emails. Experiments on diﬀerent data sizes revealed that T P rate is increased\nfrom 78% to 85% following KNN and 70% to 75% following Random Forest classi-\nﬁer. Another potential ﬁnding is the classiﬁer could detect continu ously sampled spam\ntweets better than randomly selected tweets [\n10]. In the similar context of Twitter\nspam detection, a WordVector is introduced by a training based mode l with a classi-\nﬁcation accuracy of around 80%. Average 30% higher F-measures have been achieve d\nin this work compared to other existing models [ 15].\nMoreover, Guzella and Caminhas [13] reviewed the textual and image-based spam\nemail ﬁltering approaches focusing on designing new ﬁlters. Most com mon method\n3\nselecting the feature is information gain and this way of collecting fe atures might\nincrease accuracy. In terms of datasets, SpamAssassian and LingSpam are consi dered\nas the most popular ones, whereas TREC corpora can produce more realistic online\nsetting. Additionally, Chetty et al. [16] proposed a deep learning-based model combin-\ning Word Embedding and Neural Network aiming to detect spams from various text\ndocuments. Na¨ ıve Bayes model is considered as the baseline mode l for comparing with\nthe deep learning model. Datasets were collected from UCI machine le arning reposi-\ntory for developing the models. For SMS dataset, the highest performan ce (accuracy\n98.7%) is achieved from the combined model of Word Embedding and neural n etwork.\nApart from the supervised learning approaches, there are numerous work s on unsuper-\nvised modeling as well [ 17–22]. Utilizing Modiﬁed Density-Based Spatial Clustering\nof Applications with Noise (M-DBSCAN), 97.848% accuracy has been obtained by\nManaa et al. An online unsupervised spam detection scheme, SpamCampaignAssassin\n(SCA) could detect around 92.4% spam for DEPT trace email dataset\n2.2 Transformer model-based approaches\nThe research works and literature landscape on transformer-based meth ods are\nrelatively limited. The domain of ﬁne-tuned transformers or attenti on mechanism\ntechniques for identifying spam emails is still an emerging new ﬁe ld. Related to this\nspeciﬁc area,\nYaseen et al. [23] has introduced an eﬀective word embedding technique\nfor spam classiﬁcation. Pre-trained transformer, BERT is ﬁne tuned t o detect the spam\nemails from non-spam emails. Deep Neural Network with BiLSTM is consider ed as a\nbaseline model to compare the model. Two open-source datasets from UCI machine\nlearning repository and Kaggle have been employed to train and test the model. The\nproposed model could achieve 98.67% classiﬁcation accuracy. Similarly, Liu et al. [24]\nhave developed and evaluated a modiﬁed spam detector transformer us ing the pub-\nlicly available datasets, Spam Collection v.1 and UtkMI’s Twitter Spam Detection\nCompetition dataset. This model could obtain 98.92% accuracy with a recall an d F1\nscores rate respectively, 0.9451 and 0.9613.\nFurthermore,\nGuo et al. [25] and Tida and Hsu [26] focused on BERT models imply-\ning the signiﬁcance of self-attention mechanism. Guo et al. [25] utilized two public\ndatasets, Enron and simple spam email classiﬁer dataset from Kaggle for class ifying\nham or spam emails using pre-trained BERT model. Similarly, an Univer sal Spam\nDetection Model (USDM) has been developed and tested using four pub licly avail-\nable datasets which are Ling-spam dataset, spam text dataset from Kaggle, Enr on\ndataset and spam assassin dataset. This model has gained overall accuracy of 97%\nwith 0.96 F1 score [\n25] [ 25]. Moreover, for detecting phishing URL, researchers worked\non ﬁne tuning BERT based models [ 27, 28]. Wang et al. [27] have scrapped 2.19 mil-\nlion pieces of URL data from PhishTank while pre-training PhishBERT model. This\nmodel exhibited 92% accuracy in detecting phishing URLs. Similarly , Maneriker et al.\n[28] ﬁne-tuned BERT and RoBERTa models and proposed a URLTran transformer .\nMicrosoft Edge and Internet Explorer browsing telemetry data hav e been employed\nfor training, testing, and validating purpose. Down sampling method i s applied for\nbalancing the datasets where the ﬁnal training dataset had 77,870 URLs. The ﬁn al\n4\nmodels had a True Positive Rate (TPR), 86.80% compared to the baseline mo dels\nURL-Net [ 29] and Texception [ 30].\n3 Methodology\nFig. 1 Overall methodology. Illustrates data collection and prep rocessing, model optimization, ﬁne\ntuning, model validation, baseline model comparison and ev aluation metrics\nIn this paper, transformer-based self-attention mechanism models are explored\nwith an aim to improving the pre-trained baseline BERT models. Ou r collected and\nprepared dataset is used for developing and comparing models in two di ﬀerent set-\ntings. 1) DistilBERT and RoBERTA were pretrained using both imbalanc ed and\nbalanced phishing-ham-spam dataset, and 2) the base models’ training pr ocess has\nbeen improved through applying optimization and ﬁne-tuning mechani sm. We named\nour proposed model as Improved Phishing Spam Detection Model (IPSD M). This\nmodel’s classiﬁcation performance is compared with the baseline mod els (DistilBERT\n5\nand RoBERTA). At the end of the experiment, IPSDM exhibited subst antial improve-\nment in performance both for balanced and imbalanced scenarios compared to baseline\nmodels while detecting phishing and spam emails and texts. The top -level method-\nology of this research is presented in ﬁgure 1. Later, the breakdown of detailed ﬂow\ndiagram of model optimization and ﬁne-tuning are illustrated in ﬁgure 5 and 8 of\nsection 2.4.\n3.1 Data Collection and Preparation\nThe data for training, testing, and validating this experiment is dev eloped by concate-\nnating two opensource data sources [\n31, 32]. One dataset has ham and spam emails\nwhich is merged with another phishing email dataset. The concatenated dataset has\n747 spams, 189 phishing and 4825 ham samples which is highly imbalanced. This has\nbeen further resampled following adaptive synthetic sampling (AD ASYN) technique\nwhere minor classes (ham and spam) are oversampled by generating synth etic sam-\nples with a focus on diﬃcult-to-learn instances. This process re duces the bias towards\nthe majority class making the overall predictive model more accurat e and eﬃcient.\nThis versatile technique of sampling assists in mitigating the ris k of overﬁtting as well.\nFigures 2 presents the feature distribution before and after sampling. Figure 3 shows\na snapshot of ﬁnal dataset.\nFig. 2 Feature distribution. a. Original data distribution and b. resampled data distribution\n3.2 Data Splitting\nThe overall dataset is split into 80% (training set) and 20% (testing se t). Later, from\nthe 80% set, 60% kept for training and 20% for validation. This 20% validation set i s\nused after the completion of each training epoch which aids in identi fying the optimal\n6\nFig. 3 A snapshot of dataset overview\nmodel performance. It is an integral part of the development process t hat ensures the\nmodel’s eﬀectiveness on unseen data identiﬁcation and prediction .\n3.3 Model Selection\n3.3.1 DistilBERT\nDistilBERT is a derivation of Bidirectional Encoder Representati ons from Transform-\ners (BERT) which is a transformer-based model pre-trained for dev eloping natural\nlanguage processing tasks. The idea here is to compress the original mo del for making it\nmore computationally eﬃcient and faster [\n33]. The models can be further ﬁnetuned for\nany speciﬁc downstream tasks on any customized dataset. DitilBERT mo del achieves\nthe compression by mimicking a teacher-student model where the customized model\nis trained. The input tokens are the raw text inputs that need to be preprocessed. The\ntokenizer uses a vocabulary to tokenize the input words into sub-w ords. Later, the\ntokenized inputs are mapped to numerical embedding. The relationsh ips between the\nwords are captured through the attention layer. This attention mechani sm works by\ncalculating the attention score between tokens inside a sequence al lowing the model\nto focus more on the signiﬁcant relevant words than the irrelevant ones . The pooling\nsection indicates the entire input sequence having a ﬁxed repr esentation. The classi-\nﬁer head can be modiﬁed for any speciﬁc task and the ﬁnal prediction lay er predicts\nthe corresponding model output. For our case, this is detecting spam /ham/phishing\nemails.\n3.3.2 RoBERTA\nA Robustly Optimized BERT Pretraining Approach (RoBERTA) is an exten ded ver-\nsion of the transformer-based model, BERT where model can operate on lar ge batch\nsize and train longer sequence. The pretraining process follows im proved bidirectional\ncontext-oriented mechanism while learning the masked-out tokens f or longer sequences\n7\n[34]. The architecture is similar as DistilBERT having transformer e ncoder layers with\nmulti-head attention mechanisms. However, model has a byte-level tokenizer which is\ndiﬀerent than BERT. The dynamic masking works at diﬀerent epochs an d uses BPE\nas a subunit, not as characters. RoBERTA receives tokens as inputs and a tokenizer\npreprocess these. It passes through encoding, pooling, decoding an d attention mecha-\nnism. The basic architecture of DistilBERT and RoBERTA model is si milar which is\nillustrated in ﬁgure 4.\n3.4 Improving the Training Process\nEmploying the phishing-ham-spam dataset, base models of DistilBER T and RoBERTA\nhave been measured ﬁrst. We aim to improve the model performance an d eﬃciency\nthrough optimization, i.e., learning rate scheduling, adjusting batc h size, sequence\nlength and loss function, hyper parameter tuning, early stopping and ﬁn e tuning.\nNecessary measures have been taken to handle overﬁtting issue. At the end of the\nprocess, it has been demonstrated that the ﬁnal achieved accuracy is n ot aﬀected by\noverﬁtting. This proposed methodology is also employed on imbalanced d ataset which\nwas collected initially. A noteworthy improvement is observed whi le developing models\nwith imbalanced dataset as well.\n8\nFig. 4 Basic architecture of DistilBERT and RoBERTA\n3.4.1 Model Optimization\nAs mentioned about data preparation stage in section 2.1, the preprocessed ﬁnal\nphishing dataset is tokenized using Hugging Face Transformers tokeniz er [\n35]. A sub-\nword-based approach is utilized by this tokenizer while breaking do wn the text into\nsmall unit. This allows the model to acknowledge the meaning and con text of the\nwords. The pre-trained DistilBERT and RoBERTA models are initiali zed with their\nrespective pre-trained weight obtained from pre-training proces s. The batch size is set\n32 for training data and 64 for the validation data while trading oﬀ between m emory\nconsumption and training speed. Training data is shuﬄed in each epoc h ensuring the\nmodel’s visibility to the diﬀerent unseen data. This will help prevent overﬁtting issue.\nAdamW (Adam Weight Decay), an eﬃcient optimization algorithm is used here for\nupdating the weights of pre-trained models. This algorithm computes the adaptive\nlearning rate for each parameter by combining exponential moving gradie nt averages\nand root mean square gradients [ 36]. L2 regularization, a weight decay mechanism adds\n9\npenalty to the loss function which is proportional to magnitude squared weights. This\npromotes the model to utilize small weights and mitigate overﬁtting risk by reducing\nthe complexity of the acquired parameters. The model’s parameter, Z is initialized with\nexponential decay rate, Beta1, Beta2 and epsilon with a very small value preventing\ndivision by zero. Initially, the ﬁrst moment, m0 = 0 and second moment, v0 = 0.\nIn each iteration, the gradient loss is calculated as below,\nGradient loss, g = δzL(z)\nThen, the ﬁrst moment is updated,\nmi = β1mi− 1 + (1 − β1)g\nThe updated second moment,\nvi = β2vi− 1 + (1 − β2)g2\nLater, ﬁrst and second moment bias get corrected,\nˆmi = mi\n1 − βi\nˆvi = vi\n1 − βi\nFinally, the parameters are updated using AdamW updating rule,\nZi = Zi− 1 − learning rate√ ˆvi + ϵ ( ˆmi + weight decay ∗ Zi− 1)\nThis weight decay regularization process assists in controlling the growth of param-\neters values during the training, mitigating the risk of overﬁttin g.\nIn the context of loss function, as this is a multiclass classiﬁcation t ask, Cross-Entropy\nLoss is used which combines both SoftMax activation and negative log likeli hood into\na single loss term. The diﬀerence between ground truth label and prob ability are mea-\nsured here aiming to minimize the loss during the training proce ss. PyTorch provides\ncross-entropy loss implementation that handles SoftMax computation and l ogarithmic\ncomputation. For a single training epoch, the loss can be deﬁned as follo w,\nLossi =\nn∑\nk=1\nZi,K ·log(Pi,k)\nHere, Zi, k is the ground-truth label and Pi, k is predicted probability made by the\nmodel. The cross-entropy loss for the overall training is the average of individual loss,\nLosst = 1 /n\nn∑\nk=1\nLossi\n10\nThe models output logits for each of the class which is passed through a Sof tMax acti-\nvation function for converting them into class probability. The pred icted probability\np(i, k) is computed as below, where Zi, k is the produced logit value.\np(i, k) = e(Zi, k)/(\nk∑\nm=1\ne(Zi, m))\nThe optimization process diagram is presented in Figure 5.\n3.4.2 Learning Rate\nAn ideal learning rate for model optimization and ﬁne tuning depends on several\nfactors, including model architecture, optimization algorithms and t he speciﬁc task\ndomain. It is a crucial parameter which controls step size during the optimization\nprocess. Having a high learning rate might lead the model in unstable mode resulting\npoor performance for unseen data. Again, lower learning rate can slow down the\nconvergence process. The training process might require more epo chs for achieving\na good result resulting in higher computational cost. An ideal learning rate graph is\npresented in Figure\n6.\nIn our experiment, a commonly accepted learning rate, 2e-5 (0.00002) is set which\nis standard for BERT based models, i.e., RoBERTA and DistilBERT. Late r, we plot\nvalidation vs test accuracy comparison to demonstrate the eﬀectivenes s of the selected\nlearning rate.\nFig. 5 Model optimization\n11\nFig. 6 Learning rate graph\n3.4.3 Fine Tuning\nFine tuning process involves adapting a pre-trained model to get t rained on some spe-\nciﬁc tasks and datasets. This enhances the ability of any pre-trained NLP model to\nperform any domain-speciﬁc task, i.e., email classiﬁcation for our case. The models\nare ﬁnetuned using training dataset, the 80% of the data which was separat ed before-\nhand. Training data is passed in each epoch as batches through the model s, calculating\nthe gradient using backpropagation method. To facilitate an eﬃcient batc hing, Dat-\naLoader is used during the training. Code snippet is attached for RoBERT A model in\nﬁgure\n7. A similar approach is employed for DistilBERT as well.\nFig. 7 Code snippet ﬁne tuning\nTrain loader is conﬁgured for creating mini batches of size, 32, which promotes\nparallel processing and optimize the memory use. Val loader is designed to batch of 64\n12\nsamples for validation ensuring most eﬃcient evaluation method with out shuﬄing the\ndata. RobertaForSequenceClassiﬁcation class is used to adapt the pre-t rained model\nfor speciﬁcally email classiﬁcation task. This class enables an addition al classiﬁcation\nlayer for the target label prediction. The overall ﬁne-tuning proc ess ﬂow diagram is\nillustrated in ﬁgure 8.\nFig. 8 Fine tuning diagram\n4 Results\nThe proposed IPSDM model is validated and tested using both unbalance d and bal-\nanced datasets. The IPSDM result metrics are compared to baseline mo des, i.e.,\npretrained DistilBERT and RoBERTA models. To assess the performan ce more com-\nprehensively, various key metrics including overall accuracy, precision, recall and\nF1-score are calculated. These provide crucial insights of the model performance.\n4.1 Evaluation metrics\n4.1.1 Precision\nThe ratio of true positive predictions and the total number of positive predictions is\ncalled precision. It indicated how many predicted positive sample s made by the model\n13\nare actually positive. The formula for precision is as follow,\nPrecision=TP/(TP+FP)\nHigh precision value suggests that the model’s predicted positive in stance rate is truly\npositive and correct. Whereas low precision indicates about making man y false positive\nerrors by the model.\n4.1.2 Recall\nRecall is the measurement of model’s sensitivity for understandi ng true positive rate.\nIt presents the ratio of true positive instances which is predic ted as positive by the\nmodel. The formula for calculating recall is stated below,\nRecall=TP/(TP+FP)\nHigher recall conveys that the model can successfully predict the p ositive samples as\npositive making a little false negative error. However, low recall su ggests that a higher\nnumber of actual positive samples are getting missed while the model predicts the\nfalse negatives.\n4.1.3 F1- Score\nThis is a statistical metric which is the average of precision and rec all which balances\nthese values. This provides a comprehensive view on how a model d eals with imbal-\nanced datasets by trading oﬀ between precision and recall. If either of precision or\nrecall is low, then the overall F1 score will be lower. This metri c validates the model’s\nability for predicting the positive rates and how many instances are ac tually positive.\nThe formula is as follow,\nF1 Score=(2*Precision*Recall)/(Precision+Recall)\n4.1.4 Accuracy\nAccuracy is the ratio of accurately predicted samples to the total numb er of samples\nmade by the model. It is calculated by following formula,\nAccuracy=(TP+TN)/(Total samples)\nHowever, some notable points need to consider carefully when interp reting the model\naccuracy because it suﬀers from some limitations while dealing with i mbalanced data.\nFeature distribution across all the classes is required to be observ ed meticulously.\nOtherwise, it might raise a biased classiﬁcation result. Hence, in t his study, all of\nthe essential metrics are calculated and combined together to interp ret our proposed\nIPSDM results after running a vigilant examination.\n4.2 Imbalanced Dataset Results\nThis experiment was initially carried on imbalanced datasets to asses s IPSDM model’s\nperformance on imbalanced dataset. The initial collected dataset was high ly imbal-\nanced having a majority class, ham (Figure\n2a). Comparison tables (Table 1 and\n2) and graphs (Figure 9 and 10) between baseline model’s performance and IPSDM\nmodel’s performance clearly reﬂect that IPSDM has a better perform ance in the imbal-\nanced setting. Although due to highly uneven distribution of data sampl es across\n14\nTable 1 Baseline DistilBERT vs IPSDM\nperformance (imbalanced dataset)\nEvaluation Metrics Base DistilBERT IPSDM\nValidation Accuracy 30.28% 51.32%\nTest Accuracy 31.60% 53.67%\nValidation Precision 0.841 0.972\nTest Precision 0.852 0.981\nValidation Recall 0.302 0.561\nTest Recall 0.311 0.582\nValidation F1-Score 0.432 0.613\nTest F1-Score 0.451 0.621\nTable 2 Baseline RoBERTA vs IPSDM\nperformance (imbalanced dataset)\nEvaluation Metrics Base RoBERTA IPSDM\nValidation Accuracy 43.78% 66.97%\nTest Accuracy 45.24% 67.86%\nValidation Precision 0.892 0.981\nTest Precision 0.912 0.981\nValidation Recall 0.465 0.693\nTest Recall 0.492 0.731\nValidation F1-Score 0.567 0.874\nTest F1-Score 0.581 0.893\nthe three classes, the model performance is a biased towards ‘ham’ class, still it has\nachieved comparatively higher values than the baseline models.\nand 11, the precision values are higher than recall for both cases (DistilB ERT and\nRoBERTA). In the context of highly imbalanced characteristics of this d ataset, the\nmodel can identify the majority class, ‘ham’, however, for the mod el struggles for clas-\nsifying the minor classes, ‘spam’ and ‘phishing’.\nThere is a noticeable disparity between Precision and Recall values for both models.\nRecall values are considerably lower compared to the precision. Valid ation and test\nrecall for base DistilBERT model are 0.30 and 0.31 (shown in Table\n1). For base\nRoBERTA model, the recall values are 0.47 and 0.49 (shown in Table 2 ) which sug-\ngest that the models are facing challenges for identifying the minor c lasses, ‘spam’\nand ‘phishing’ due to the imbalanced nature. However, it is noteworth y that the per-\nformance of IPSDM for both DistilBERT and RoBERTA is notably higher even the\ndataset is imbalanced.\n15\nFig. 9 Comparison graph of baseline DistilBERT vs IPSDM performan ce (imbalanced dataset)\nFig. 10 Comparison graph of baseline RoBERTA vs IPSDM performance ( imbalanced dataset)\n4.3 Balanced Dataset Results\nThe collected email datasets have been resampled and balanced. After p reparing this\nbalanced dataset, baseline DistilBERT and RoBERTA models were train ed and val-\nidated. Again, using the similar dataset, we worked on model optimization and ﬁne\ntuning. The evaluation metrics of our proposed model, IPSDM and the base line mod-\nels are tabulated in Tables\n3 and 4. Accuracy, precision, recall for both validation and\n16\nTable 3 Baseline DistilBERT vs IPSDM\nperformance (balanced dataset)\nEvaluation Metrics Base RoBERTA IPSDM\nValidation Accuracy 82.63% 97.50%\nTest Accuracy 88.95% 97.10%\nValidation Precision 0.8543 0.9755\nTest Precision 0.9025 0.9716\nValidation Recall 0.6971 0.9750\nTest Recall 0.7532 0.9710\nValidation F1-Score 0.8867 0.9749\nTest F1-Score 0.8943 0.9710\nTable 4 Baseline ROBERTA vs IPSDM\nperformance (balanced dataset)\nEvaluation Metrics Base ROBERTA IPSDM\nValidation Accuracy 87.10% 98.99%\nTest Accuracy 93.29% 99.00%\nValidation Precision 0.921 0.982\nTest Precision 0.853 0.991\nValidation Recall 0.903 0.989\nTest Recall 0.923 0.991\nValidation F1-Score 0.911 0.982\nTest F1-Score 0.931 0.985\ntest cases are presented here. Also, the values are illustrated in com parison graphs\n(Figure 11 and Figure 12 ). The evaluation metrics exhibit an increase in validation\naccuracy- approximately 14.87% and 11.89%; test accuracy approximately 8.15% and\n5.71% respectively for base DistilBERT and RoBERTA models vs IPSDM. A consis-\ntent rise in F1scores suggests that the IPSDM has elevated performance across both\ncases. This score is the harmonic mean of recall and precision which is a crucial metric\nfor assessing the balance between the crucial aspects of classiﬁcation performance.\n17\nFig. 11 Comparison graph of baseline DistilBERT vs IPSDM performan ce (balanced dataset)\nFig. 12 Comparison graph of baseline RoBERTA vs IPSDM performance ( balanced dataset)\n4.4 Avoiding Overﬁtting\nA common issue in statistical modellings and machine learning is over ﬁtting which\noccurs when a model is performs too well on the training dataset, ho wever, too poorly\n18\nTable 5 Validation vs test accuracy\nModel Name Validation accuracy Test accuracy\nBalanced IPSDM/ DistilBERT 97.50% 97.10%\nBalanced IPSDM/ RoBERTA 98.99% 99.00%\nImbalanced IPSDM/ DistilBERT 51.32% 53.67%\nImbalanced IPSDM/ RoBERTA 66.97% 67.86%\non the new or unseen data, i.e., testing dataset. Overﬁtting can be eﬀ ectively managed\nin balanced situations while a model has consistent performance on vali dation and test\ndatasets. A close alignment between test and validation accuracy suggests that the\nclassiﬁcation models yield good results on unseen, new data. In the b alanced scenario,\ntest and validation accuracy values indicate minimal disparity, i.e., 97.10% vs 97.50%\nand 99. 00% vs 98.99%. Based on the Table 5 and Figure 13 data, there is not a\nlarge gap between validation and test accuracy. When training or validation ac curacy\nis notably higher than test accuracy, there is a high chance of overﬁtti ng. Moreover,\nthe precision, recall and F1 measures from table 1 through 4 also suggest a harmonic\ndistribution of these metrics which is also a positive indication. The comparison graph\nfor both validation and test accuracy lines are almost overlapping with eac h other\nindicating that the model is performing well on the unseen data.\nFig. 13 Validation vs test accuracy graph\n19\n5 Discussion\nThe results from both imbalanced and balanced settings depict an enhanc ement in\nperformance for IPSDM model. Validation and test accuracy are separately measured\nto understand if there is any overﬁtting issue persist. Baselin e DistilBERT has 82.63%\nof validation accuracy and 88.95% of testing accuracy whereas IPSD has 97.50% and\n97.10% validation and test accuracy respectively. The baseline model’s accuracy vari-\nation is 6 (+-32) % in training and testing performance reveal that base Di stilBERT\nis exhibiting a minor overﬁtting problem. However, this has been eﬀectively handled\nduring the development of IPSDM DistilBERT version. Again, a simil ar trait is visi-\nble for base RoBERTA and IPSDM for RoBERTA as well. Base RoBERTA mpdel’s\nvalidation and testing accuracy gap is around 6 (+- 19) % whereas IPSDM has 0.01%\nof diﬀerence between these two values.\nSuch evaluation has been also extended to imbalanced dataset to analyze h ow IPSDM\nis performing in challenging scenario. In the imbalanced setting, (T able\n3 and 4 ) con-\nvey that our proposed model has outperformed the baseline models in th is scenario as\nwell. Due to the heavy skewness of sample distribution, the resu lts are biased towards\n‘ham’ class. The precision values for both baseline and IPSDM models ar e notably\nhigher. 0.85 and 0.98 for base DistilBERT and IPSDM test precision; 0.91 and 0.98\nfor base RoBERTA and IPSDM respectively present that the model is p redicting most\nof the instances as ‘ham’. This impact results in higher precision and lower recall.\nHowever, later applying ADASYN, an advanced sampling technique, this cl ass imbal-\nanced situation is handled at the initial stage. A prominent change in per formance\nis hence demonstrated in IPSDM models both for DistilBERT and RoBER TA both\nfor balanced and imbalanced datasets scenarios. Our proposed such method ological\napproach for enhancing training performance can be further extended t o any BERT\nbased LLM architecture.\n6 Conclusion and Future Directions\nSolving long standing societal issues via radical new approaches, spe ciﬁcally LLMs,\nshows great promise to improving the lives and experiences of compu ting users the\nworld over. Phishing and Spam have long since been an issue causing l ost time and\nstraining ﬁnancial resources of consumers and organizations. We demonstr ate how\nleveraging new technology can be applied to these persistent challen ges. LLMs oﬀer\nsociety great beneﬁts and we have only scratched the surface on their potential. In\nthe future, improving the quality of life via multiple dimensi ons will be realized such\nas medical diagnoses, chat-bots, education, and security to name a few. T his work\ndemonstrates how LLMs can be leveraged to detect phishing and spam by le veraging\nLLMs and then presenting our ﬁne-tuned version, IPSDM. Following t he proposed\nmechanism, modiﬁed DistilBERT could achieve 97.50% of validation and 97.10% of\ntest accuracy with a F1-score of 0.97. Again, the modiﬁed RoBERTA model obtai ned\n98.99% of validation and 99.00% of test accuracy including a F1-score of 0.98. The\nresult of this study presents the eﬀectiveness of IPSDM model w hile reducing the over-\nﬁtting issues and handling imbalanced datasets. The attained accuracy has surpassed\nthe existing state-of-the art models.\n20\nFuture work entails further reﬁnement of IPSDM via incorporation of ad ditional tuning\ntechniques as well as hyper-parameter tuning and combining with en semble model-\ning. Applying data augmentation such as text rotation, contrastive learnin g, synonym\nreplacement might also assist in increasing the diversity and impr oving the training\nperformance. Furthermore, the ﬁeld of Large Language Models has attracted s ubstan-\ntial investment from industry and consumers causing it to develop r apidly with new\nopen-source models being released nearly daily. We aim to experime nt with further\nLLMs such as Meta’s Llama and Llama 2. Infusing such solutions into chatbot, w eb\napplications and other real-world practical systems would serve socie ty in numerous\nvaluable ways.\nData availability statements\nThe data for training, testing, and validating this experiment is dev eloped\nby concatenating two opensource data sources [\n31, 32]. Repository Link:\nhttps://www.kaggle.com/datasets/shantanudhakadd/email-spam-detection-dataset-\nclassiﬁcation\nhttps://github.com/TanusreeSharma/phishingdata-Analysis/blob/master/1st%20data/PhishingEmailData.csv\nReferences\n[1] Basit, A., Zafar, M., Liu, X., Javed, A.R., Jalil, Z., Kifayat, K.: A c omprehensive\nsurvey of ai-enabled phishing attacks detection techniques. Tele communication\nSystems 76, 139–154 (2021)\n[2] Anand, P., Bharti, A., Rastogi, R.: Time eﬃcient variants of twin extr eme learning\nmachine. Intelligent Systems with Applications 17, 200169 (2023)\n[3] Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., Wang, Y.: Transformer in transform er.\nAdvances in Neural Information Processing Systems 34, 15908–15919 (2021)\n[4] Roumeliotis, K.I., Tselikas, N.D.: Chatgpt and open-ai models: A pr eliminary\nreview. Future Internet 15(6), 192 (2023)\n[5] Araci, D.: Finbert: Financial sentiment analysis with pre-train ed language models.\narXiv preprint arXiv:1908.10063 (2019)\n[6] Khan, J.Y., Khondaker, M.T.I., Afroz, S., Uddin, G., Iqbal, A.: A ben ch-\nmark study of machine learning models for online fake news detection. Machine\nLearning with Applications 4, 100032 (2021)\n[7] Deb, S., Chanda, A.K.: Comparative analysis of contextual and context-f ree\nembeddings in disaster prediction from twitter data. Machine Learnin g with\nApplications 7, 100253 (2022)\n[8] Jamal, S., Cruz, M.V., Chakravarthy, S., Wahl, C., Wimmer, H.: Int egration of eeg\nand eye tracking technology: A systematic review. SoutheastCon 2023, 209–216\n21\n(2023)\n[9] Govil, N., Agarwal, K., Bansal, A., Varshney, A.: A machine learning base d spam\ndetection mechanism. In: 2020 Fourth International Conference on Comput ing\nMethodologies and Communication (ICCMC), pp. 954–957 (2020). IEEE\n[10] Chen, C., Zhang, J., Xie, Y., Xiang, Y., Zhou, W., Hassan, M.M., AlElaiwi, A. ,\nAlrubaian, M.: A performance evaluation of machine learning-based streami ng\nspam tweets detection. IEEE Transactions on Computational social syste ms 2(3),\n65–76 (2015)\n[11] Kumar, S., Gao, X., Welch, I., Mansoori, M.: A machine learning based web spam\nﬁltering approach. In: 2016 IEEE 30th International Conference on Advanced\nInformation Networking and Applications (AINA), pp. 973–980 (2016). IEEE\n[12] Baaqeel, H., Zagrouba, R.: Hybrid sms spam ﬁltering system using mach ine\nlearning techniques. In: 2020 21st International Arab Conference on Inform ation\nTechnology (ACIT), pp. 1–8 (2020). IEEE\n[13] Guzella, T.S., Caminhas, W.M.: A review of machine learning approac hes to spam\nﬁltering. Expert Systems with Applications 36(7), 10206–10222 (2009)\n[14] Dada, E.G., Bassi, J.S., Chiroma, H., Adetunmbi, A.O., Ajibuwa, O.E. , et al.:\nMachine learning for email spam ﬁltering: review, approaches and open re search\nproblems. Heliyon 5(6) (2019)\n[15] Wu, T., Liu, S., Zhang, J., Xiang, Y.: Twitter spam detection based on de ep learn-\ning. In: Proceedings of the Australasian Computer Science Week Multi conference,\npp. 1–8 (2017)\n[16] Chetty, G., Bui, H., White, M.: Deep learning based spam detect ion system.\nIn: 2019 International Conference on Machine Learning and Data Engineering\n(iCMLDE), pp. 91–96 (2019). IEEE\n[17] Qian, F., Pathak, A., Hu, Y.C., Mao, Z.M., Xie, Y.: A case for unsupervise d-\nlearning-based spam ﬁltering. ACM SIGMETRICS performance evaluation\nreview 38(1), 367–368 (2010)\n[18] Manaa, M., Obaid, A., Dosh, M.: Unsupervised approach for email spam ﬁlte ring\nusing data mining. EAI Endorsed Transactions on Energy Web 8(36) (2021)\n[19] Cabrera-Le´ on, Y., Garc´ ıa B´ aez, P., Su´ arez-Araujo, C.P.: E-mail spam ﬁlter based\non unsupervised neural architectures and thematic categories: desi gn and analysis.\nIn: International Joint Conference on Computational Intelligence, pp. 239–262\n(2016). Springer\n[20] Jaya, T., Kanyaharini, R., Navaneesh, B.: Appropriate detection of ham and spam\n22\nemails using machine learning algorithm. In: 2023 International Conferen ce on\nAdvances in Computing, Communication and Applied Informatics (ACCAI), p p.\n1–5 (2023). IEEE\n[21] Karim, A., Azam, S., Shanmugam, B., Kannoorpatti, K.: Eﬃcient cluste ring\nof emails into spam and ham: The foundational study of a comprehensive\nunsupervised framework. IEEE Access 8, 154759–154788 (2020)\n[22] Ghiassi, M., Lee, S., Gaikwad, S.R.: Sentiment analysis and spam ﬁl tering\nusing the yac2 clustering algorithm with transferability. Compute rs & Industrial\nEngineering 165, 107959 (2022)\n[23] Yaseen, Q., et al.: Spam email detection using deep learning techniques. Procedia\nComputer Science 184, 853–858 (2021)\n[24] Liu, X., Lu, H., Nayak, A.: A spam transformer model for sms spam detection .\nIEEE Access 9, 80253–80263 (2021)\n[25] Guo, Y., Mustafaoglu, Z., Koundal, D.: Spam detection using bidirect ional trans-\nformers and machine learning classiﬁer algorithms. Journal of Computation al and\nCognitive Engineering 2(1), 5–9 (2023)\n[26] Tida, V.S., Hsu, S.: Universal spam detection using transfer learni ng of bert\nmodel. arXiv preprint arXiv:2202.03480 (2022)\n[27] Wang, Y., Zhu, W., Xu, H., Qin, Z., Ren, K., Ma, W.: A large-scale pretr ained\ndeep model for phishing url detection. In: ICASSP 2023-2023 IEEE Intern ational\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1–5 (2023).\nIEEE\n[28] Maneriker, P., Stokes, J.W., Lazo, E.G., Carutasu, D., Tajaddodian far, F., Guru-\nrajan, A.: Urltran: Improving phishing url detection using transf ormers. In:\nMILCOM 2021-2021 IEEE Military Communications Conference (MILCOM), pp.\n197–204 (2021). IEEE\n[29] Le, H., Pham, Q., Sahoo, D., Hoi, S.C.: Urlnet: Learning a url represen tation with\ndeep learning for malicious url detection. arXiv preprint arXiv:1802.03162 (2018)\n[30] Tajaddodianfar, F., Stokes, J.W., Gururajan, A.: Texception: a c haracter/word-\nlevel deep learning model for phishing url detection. In: ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing ( ICASSP),\npp. 2857–2861 (2020). IEEE\n[31] Dhakad, S.: Email Spam Detection Dataset (classiﬁcation). Online; Ac cessed:\n08 October 2023.\nhttps://www.kaggle.com/datasets/shantanudhakadd/\nemail-spam-detection-dataset-classiﬁcation\n23\n[32] Sharma, T.: PhishingEmailData. Online; Accessed: 08 October 2023.\nhttps://github.com/TanusreeSharma/phishingdata-Analysis/blob/master/1st%\n20data/PhishingEmailData.csv\n[33] Liu, Z., Lin, W., Shi, Y., Zhao, J.: A robustly optimized bert pre-t raining\napproach with post-training. In: China National Conference on Chinese C ompu-\ntational Linguistics, pp. 471–484 (2021). Springer\n[34] Jain, S.M.: Hugging face. In: Introduction to Transformers for NLP: Wi th the\nHugging Face Library and Models to Solve Problems, pp. 51–67. Springer, ???\n(2022)\n[35] Zhuang, Z., Liu, M., Cutkosky, A., Orabona, F.: Understanding adamw thr ough\nproximal methods and scale-freeness. arXiv preprint arXiv:2202.00089 (2022)\n[36] Jamal, S., Wimmer, H.: Performance analysis of machine learning algorit hm on\ncloud platforms: Aws vs azure vs gcp. In: International Scientiﬁc and P ractical\nConference on Information Technologies and Intelligent Decision Making Systems,\npp. 43–60 (2022). Springer\n24",
  "topic": "Phishing",
  "concepts": [
    {
      "name": "Phishing",
      "score": 0.9298583269119263
    },
    {
      "name": "Computer science",
      "score": 0.6458333134651184
    },
    {
      "name": "Transformer",
      "score": 0.45395204424858093
    },
    {
      "name": "Point (geometry)",
      "score": 0.4115452468395233
    },
    {
      "name": "Computer security",
      "score": 0.3538990616798401
    },
    {
      "name": "Data science",
      "score": 0.3481610417366028
    },
    {
      "name": "World Wide Web",
      "score": 0.34520724415779114
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33902472257614136
    },
    {
      "name": "Internet privacy",
      "score": 0.3272216022014618
    },
    {
      "name": "The Internet",
      "score": 0.1563696265220642
    },
    {
      "name": "Engineering",
      "score": 0.14734706282615662
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39815113",
      "name": "Georgia Southern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I12079687",
      "name": "Edith Cowan University",
      "country": "AU"
    }
  ]
}