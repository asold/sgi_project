{
    "title": "FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model",
    "url": "https://openalex.org/W4401863519",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2155464992",
            "name": "Feijie Wu",
            "affiliations": [
                "Purdue University West Lafayette"
            ]
        },
        {
            "id": "https://openalex.org/A2105732091",
            "name": "Zitao Li",
            "affiliations": [
                "Bellevue Hospital Center",
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2116094297",
            "name": "Ya-liang Li",
            "affiliations": [
                "Bellevue Hospital Center",
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2642048689",
            "name": "Bolin Ding",
            "affiliations": [
                "Alibaba Group (United States)",
                "Bellevue Hospital Center"
            ]
        },
        {
            "id": "https://openalex.org/A1908942481",
            "name": "Gao Jing",
            "affiliations": [
                "Purdue University West Lafayette"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4401863415",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W4380558781",
        "https://openalex.org/W3136363192",
        "https://openalex.org/W4386075560",
        "https://openalex.org/W4224310605",
        "https://openalex.org/W4323343894",
        "https://openalex.org/W4392904059",
        "https://openalex.org/W4385572063",
        "https://openalex.org/W4385562549",
        "https://openalex.org/W4381855801"
    ],
    "abstract": "Large language models (LLMs) show amazing performance on many domain-specific tasks after fine-tuning with some appropriate data. However, many domain-specific data are privately distributed across multiple owners. Thus, this dilemma raises the interest in how to perform LLM fine-tuning in federated learning (FL). However, confronted with limited computation and communication capacities, FL clients struggle to fine-tune an LLM effectively. To this end, we introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL. Specifically, our method involves the server generating a compressed LLM and aligning its performance with the full model. Subsequently, the clients fine-tune a lightweight yet important part of the compressed model, referred to as an adapter. Notice that as the server has no access to the private data owned by the clients, the data used for alignment by the server has a different distribution from the one used for fine-tuning by clients. We formulate the problem into a bi-level optimization problem to minimize the negative effect of data discrepancy and derive the updating rules for the server and clients. We conduct extensive experiments on LLaMA-2, empirically showing that the adapter has exceptional performance when reintegrated into the global LLM. The results also indicate that the proposed FedBiOT significantly reduces resource consumption compared to existing benchmarks, all while achieving comparable performance levels.",
    "full_text": null
}