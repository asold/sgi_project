{
  "title": "An Unsupervised Momentum Contrastive Learning Based Transformer Network for Hyperspectral Target Detection",
  "url": "https://openalex.org/W4394773596",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2119777878",
      "name": "Yulei Wang",
      "affiliations": [
        "Dalian Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A1983188002",
      "name": "Xi Chen",
      "affiliations": [
        "Dalian Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2105044504",
      "name": "Enyu Zhao",
      "affiliations": [
        "Dalian Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2113679906",
      "name": "Chunhui Zhao",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2097937704",
      "name": "Meiping Song",
      "affiliations": [
        "Dalian Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2095960791",
      "name": "Chunyan Yu",
      "affiliations": [
        "Dalian Maritime University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2017014096",
    "https://openalex.org/W4312513332",
    "https://openalex.org/W4391305849",
    "https://openalex.org/W4390617924",
    "https://openalex.org/W3119997721",
    "https://openalex.org/W4367031974",
    "https://openalex.org/W3123995827",
    "https://openalex.org/W4226239104",
    "https://openalex.org/W4285214186",
    "https://openalex.org/W3211275894",
    "https://openalex.org/W4280624639",
    "https://openalex.org/W4282005289",
    "https://openalex.org/W2166682552",
    "https://openalex.org/W1973176871",
    "https://openalex.org/W2171521847",
    "https://openalex.org/W2099301440",
    "https://openalex.org/W2164160978",
    "https://openalex.org/W2120275993",
    "https://openalex.org/W2028740150",
    "https://openalex.org/W2107014267",
    "https://openalex.org/W2163957348",
    "https://openalex.org/W625476304",
    "https://openalex.org/W4312610008",
    "https://openalex.org/W4386175343",
    "https://openalex.org/W2772028350",
    "https://openalex.org/W3131024748",
    "https://openalex.org/W3096751897",
    "https://openalex.org/W4226038296",
    "https://openalex.org/W2979676460",
    "https://openalex.org/W3021150190",
    "https://openalex.org/W3212625467",
    "https://openalex.org/W3097141235",
    "https://openalex.org/W4225582357",
    "https://openalex.org/W3045657924",
    "https://openalex.org/W3087782035",
    "https://openalex.org/W6779997284",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3171007011",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3196660197",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W4313160444",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6844194202",
    "https://openalex.org/W3120451664",
    "https://openalex.org/W4205124618",
    "https://openalex.org/W3129037655",
    "https://openalex.org/W3087883793",
    "https://openalex.org/W3157052017",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4225688999",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Hyperspectral target detection plays a pivotal role in various civil and military applications. Although recent advancements in deep learning have largely embraced supervised learning approaches, they often hindered by the limited availability of labeled data. Unsupervised learning, therefore, emerges as a promising alternative, yet its potential has not been fully realized in current methodologies. This article proposes an innovative unsupervised learning framework employing a momentum contrastive learning-based transformer network specifically tailored for hyperspectral target detection. The proposed approach innovatively combines transformer-based encoder and momentum encoder networks to enhance feature extraction capabilities, adeptly capturing both local spectral details and long-range spectral dependencies through the novel overlapping spectral patch embedding and a cross-token feedforward layer. This dual-encoder design significantly improves the model&#x0027;s ability to discern relevant spectral features amidst complex backgrounds. Through unsupervised momentum contrastive learning, a dynamically updated queue of negative sample features is utilized so that the model can demonstrate superior spectral discriminability. This is further bolstered by a unique background suppression mechanism leveraging nonlinear transformations of cosine similarity detection results, with two nonlinearly pull-up operations, significantly enhancing target detection sensitivity, where the nonlinearly operations are the exponential function with its normalization and the power function with its normalization, respectively. Comparative analysis against seven state-of-the-art hyperspectral target detection methods across four real hyperspectral images demonstrates the effectiveness of the proposed method for hyperspectral target detection, with an increase in detection accuracy and a competitive computational efficiency. An extensive ablation study further validates the critical components of the proposed framework, confirming its comprehensive capability and applicability in hyperspectral target detection scenarios.",
  "full_text": "> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n1 \n  \nAbstract—Hyperspectral target detection plays a pivotal role in \nvarious civil and military applications. Although recent \nadvancements in deep learning have largely embraced supervised \nlearning approaches, they often hindered by the limited \navailability of labeled data. Unsupervised learning, therefore, \nemerges as a promising  alternative, yet its potential has not been \nfully realized in current methodologies. This paper proposes a n \ninnovative unsupervised learning framework employing a \nmomentum contrastive learning -based transformer network \nspecifically tailored for hyperspect ral target detection.  The \nproposed approach innovatively combines transformer -based \nencoder and momentum encoder networks to enhance feature \nextraction capabilities, adeptly capturing both local spectral \ndetails and long -range spectral dependencies through  the novel \noverlapping spectral patch embedding and a cross -token \nfeedforward layer. This dual -encoder design significantly \nimproves the model's ability to discern relevant spectral features \namidst complex backgrounds. Through unsupervised momentum \ncontrastive learning, a dynamically updated queue of negative \nsample features is utilized so that the model can demonstrate \nsuperior spectral discriminability. This is further bolstered by a \nunique background suppression mechanism leveraging nonlinear \ntransformations of cosine similarity detection results, with two \nnonlinearly pull -up operations, significantly enhancing target \ndetection sensitivity, where the nonlinearly operations are the \nexponential function with its normalization and the power \nfunction with its  normalization, respectively. Comparative \nanalysis against seven state -of-the-art hyperspectral target \ndetection methods across four real hyperspectral images  \ndemonstrates the effectiveness of the proposed method for \nhyperspectral target detection , with an  increase in detection \naccuracy and a competitive computational efficiency. An extensive \nablation study further validates the critical components of the \nproposed framework, confirming its comprehensive capability \nand applicability in hyperspectral target detection scenarios. \n \nIndex Terms —hyperspectral imagery, momentum contrastive \nlearning, target detection, Transformer, unsupervised learning.  \nI. INTRODUCTION \nYPERSPECTRAL imagery (HSI) is captured by \nhyperspectral sensors in the visible and short -wave \ninfrared (or mid -wave and long -wave infrared) regions of the \nspectrum[1]-[2], which not only contains the spatial \n \nManuscript received Feb 3, 2023. This work is supported in part by National \nNature Science Foundation of China (61801075, 42271355), Natural Science \nFoundation of Liaoning Province (2022-MS-160), China Postdoctoral Science \nFoundation (2020M670723), and the Fundamental Research Funds for the \nCentral Universities (3132023238). (Corresponding author: Xi Chen) \ninformation of the scene but also collects the spectral \ninformation of the ground objects to form the image cube data \nof three dimensions, with two spatial dimensions of the scene, \nand one spectral dimension co nsisting of the characteristics of \nthe electromagnetic wave reflection signal at a specific \nwavelength [3]. The spectrum of each pixel in the HSI can \nreflect the reflection characteristics of different ground objects \nin the scene [4]. Benefiting from the high spectral resolution of \nHSIs [5], hyperspectral target detection (HTD) can detect \ntargets based on the spectral differences of different ground \nobjects and has essential applications in  the fields of military \ncamouflage target identification [6]-[7], pollution detection [8]-\n[9], mineral exploration [10], food safety [11] and medical \ndiagnosis [12]. \nHTD has been developed over a long period of time with a \nlarge number of classical HTD methods. Spectral matched \nfiltering (SMF) [13] and adaptive coherence estimation (ACE) \n[14] are classical HTD methods based on probabilistic statistics \nassuming that the background conforms to a multivariate \nGaussian distribution. The constrained energy minimization \n(CEM) [15] method highlights the target and suppresses the \nbackground by designing a finite pulse filter that minimizes the \noverall energy output under the constraints of the target signal. \nThe orthogonal subspace projection (OSP) [16] method \nachieves HTD by projecting the target onto the orthogonal \nsubspace of the background subspace and then maximizing the \nsignal-to-noise ratio on the projection subspace. However, these \nHTD methods based on linear spectral information do not \nexplore the nonlinear relationship between spectral bands. \nTherefore, kernel -based learning theory is used for HTD to \nexploit the nonlinear corr elations of the HSI data. Some \nclassical HTD methods have been extended to the \ncorresponding kernel-based nonlinear versions, such as kernel \nspectral matched filtering (KSMF) [17], kernel adaptive \ncoherence estimation (KACE) [18], kernel constrained energy \nminimization (KCEM ) [19], kernel orthogonal subspace \nprojection (KOSP) [20], etc. In most cases, the kernel -based \nHTD methods assume that linearly inseparable dat a in low -\ndimensional space will likely become linearly separable in \nhigh-dimensional space. Recently, HTD methods based on \nY. W ang, X. Chen , E. Zhao , M. Song and C. Yu are with Information \nScience and Technology College, Dalian Maritime University, China. (e-mails: \nwangyulei@dlmu.edu.cn, xi_chen@nudt.edu.cn, zhaoenyu@dlmu.edu.cn, \nsmping@163.com, yuchunyan1997@126.com).  \nC. Zhao is with College of Information and Communication Engineering,  \nHarbin Engineering University, China. (email: zhaochunhui@hrbeu.edu.cn ) \nYulei Wang, Member, IEEE, Xi Chen, Enyu Zhao, Chunhui Zhao, Meiping Song, and Chunyan Yu \nAn Unsupervised Momentum Contrastive \nLearning based Transformer Network for \nHyperspectral Target Detection \nH \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n2 \nsparse representation have been proposed successively. Chen et \nal. first proposed a sparsity-based target detection method (STD) \n[21], which represents the pixels to be detected by a linear \napproximation of the atomic vectors in the complete dictionary, \nthen calculates the reconstruction error with the pixels to be \ndetected, and finally determines whether the pixels to be \ndetected are targets by a set threshold. Li et al. proposed a \ncombined sparse and collabor ative representation of HTD \nmethod (CSCR) [22], which implements target detection by \nrepresenting the pixels to be detected with a target library and a \nbackground library. However, the representation -based HTD \nmethods require prior information to construct the dictionary, \nwhich is difficult to obtain in practical applications. \nDue to the strong generalization and deep extraction of \nadvanced semantic featur es, deep learning has been gradually \napplied in HSI processing  [23]-[24]. In recent years, deep \nlearning based HTD algorithms have gradually been proposed. \nFor HTD tasks, normally the prior information is only a \nspectrum of the target of interest , and it is not possible to train \nthe deep neural network in a supervised manner directly based \non the prior target spectrum. From the perspective of transfer \nlearning, some methods transfer the model knowledge trained \non the data set with known labels to the target detection task, \nsuch as the convolutional neural network -based detection \n(CNND) [25] method, the spectral-spatial joint target detection \nmethod of hyperspectral image based on transfer learning [26], \nthe sensor -independent HTD (SIHTD) method [27], and the \nmeta-learning and Siamese network -based HTD (MLSN) [28] \nmethod. CNND pairs and assigns label 0 between similar pixels \nspectra and label 1 between different classes of pixels spectra \nbased on the known labeled information from a hyperspectral \ndataset with known labels in the source domain and then trains \na binary-classified multilayer CNN for HTD using the samples \ngenerated by the pixel pairing. However, the unmatched \nhyperspectral sensors in the source and target domains [29] can \nseriously affect the performance of transfer learning on HTD. \nTo solve this problem, SIHTD adaptively transfers the \nsimilarity and dissimilarity measurement from the source \ndomain to the target domain for HTD in an adversarial manner. \nSome methods start from the perspective of expanding the \ntraining samples. A deep CNN for HTD (denoted as HTD-Net) \n[30] uses a modified autoencoder with a contracting path and a \nsymmetric expanding path to generate target signatures, where \nthe background samples significantly different from the target \nsamples are found based on the linear prediction strategy, and \nthen the obtained target and background samples are paired to \ntrain the deep CNN to learn the spectral differences between the \npaired samples. A HTD method with an auxiliary generative \nadversarial network [31] expands the training set by generating \nsimulated target and background spectra using a generative \nadversarial network. A two -stream convolutional network -\nbased target detector [32] finds enough typical background \npixels by a hybrid sparse representation and classifica tion-\nbased pixel selection strategy, and then pairs the prior target \nwith the synthesized target and background samples \nrespectively to form positive and negative sample pairs to train \na binary classification network. Rao et al. proposed a Siamese \nTransformer network for HTD (STTD) [33], which extracts the \nhigh-purity background pixels in the HSI to be detected by \nendmember extraction and unmixing algorithms. There are also \ndeep learning -based HTD methods that rely on prior \ninformation obtained from traditional HTD methods to help \nmodel learning. Shi et al. proposed a method for HTD using \nROI feature transformation and multiscale spectral attention \n[34], where the region of interest (ROI) map is obtained by a \nCEM detector and an edge -preserving filter, and the HSI to be \ndetected is fed with the ROI map into a construc ted deep \nspatial-spectral network for extracting spatial and spectral \nfeatures of interest, and then the HTD detection results are \nobtained using the nearest neighbors (NNs). The background \nlearning based target suppression constraint (BLTSC) [35] \ndetector finds reliable background samples for training \nAdversarial Autoencoder (AAE) by performing coarse \ndetection by CEM detector, and then reconstructs the original \nHSI using the well -trained AAE, and finally the discrepancy \nbetween the reconstructed and original HSIs are examined to \nspot the targets. \nIn summary, the performance of transfer learning -based \nHTD methods is primarily limited by the adaptability of the \ntransferred knowledge. The performance of HTD m ethods that \nhelp model training with the help of the prior information \nobtained from traditional HTD methods can be limited by the \nperformance of traditional HTD methods. The HTD methods \nthat expand the training samples by pairing or mixing some \ntarget and  background samples found from the HSI to be \ndetected will be affected by the quality of the target and \nbackground samples found. The HTD methods using CNNs \nobtain the approximate global information of the spectrum by \nbuilding a deep CNN.  \nIn recent years, contrastive learning has been widely applied \nas an unsupervised representation learning method in various \nfields, including computer vision. Such as a simple framework \nfor contrastive learning of visual representations (SimCLR) \n[36], unsupervised learning of visual features by contrasting \ncluster assignments (SwAV) [37], momentum contrast for \nunsupervised visual representation learning (MoCo) [38] and a \nnew method BYOL for self -supervised image representation \nlearning without negative sample pairs proposed  in [39]. \nSimCLR directly uses negative samples coexisting in the \ncurrent batch, and it requires a large batch size to work well, \nand MoCo maintains a queue of negative samples and turns one \nbranch into a momentum encoder to improve consistency of the \nqueue [40]. Their competitive performance in downstream \ntasks brings a strong theoretical support for HTD methods \noriented to contrastive learning. \nTo overcome the reliance on explicit target and background \nsamples, this paper proposes a novel unsupervised learning \nframework based on unsupervised momentum contrast learning \nand Transformer (MCLT). Unlike existing approaches, the \nproposed method innovatively combines transformer-based and \nmomentum encoder networks for e nhanced spectral feature \nextraction, capitalizing on both the detailed local and the global \nspectral information. Furthermore, the application of the \nunsupervised momentum contrastive learning, complemented \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n3 \nby a strategic queuing mechanism for negative sample \nmanagement, sets a new standard for feature discriminability in \nthe absence of  labeled data. Additionally, the proposed \nbackground suppression technique, utilizing nonlinear \ntransformations, significantly improves detection sensitivity \nand accuracy. The main contributions of this paper can be \nsummarized as follows, where these contributions not only fill \na critical gap in the literature but also surpass existing methods, \nas evidenced by the comprehensive comparative analysis and \nablation study.  \n1) For spectral target detection, a novel encoder design that \nintegrates transformer-based and momentum encoding to \ncapture both local and global spectral features, addressing \nthe oversight of local spectral detail in existing models. \n2) Unsupervised momentum contrastive learning equips the \nmodel with the ability to discriminate differences between \nspectra, freeing it from dependence on labeled target and \nbackground samples.  \n3) An innovative  background suppression technique that \nleverages nonlinear transformations is proposed for a \nbetter separation of background and target pixels , where \nexponential an d normalization operations, and power \nfunction and normalization operations are used. \nThe remainder of this paper is organized as follows. Section \nII gives a detailed description of the proposed MCLT method. \nThe experimental studies and analysis to verify the proposed \nmethod are presented in Section III. Finally, the conclusions are \ndrawn in Section IV. \nII. PROPOSED METHOD \nThis section delineates  the proposed MCLT method  in a \ncomprehensive manner. Figure 1 presents the methodological \nflowchart of the MCLT appro ach, encapsulating its systematic \nworkflow. The methodology primarily unfolds in three \nsequential steps: the construction of a Transformer -based \nencoder tailored for Hyperspectral Target Detection (HTD), the \nimplementation of spectral discriminability lear ning with \nmomentum encoder, and the execution of background \nsuppression. Notably, the initial two steps are encompassed \nwithin the training phase, aimed at preparing the model by \nenhancing its ability to distinguish between spectral signatures. \nThe final s tep is situated within the detection phase, \nstrategically designed to optimize target-background separation. \nThis structured approach underscores the MCLT method’s \ncapability to effectively identify and isolate targets from \ncomplex hyperspectral backgrounds. \nA. Transformer Based Encoder For HTD \nThe Encoder module in Figure 1 depicts the architecture of \nthe Transformer based encoder designed specifically for HTD. \nThis encoder structure is pivotal in extracting and processing \nthe rich spectral and spatial inform ation inherent in \nhyperspectral images for effective target detection. The \narchitecture comprises three main components: overlapping \nspectral patch embedding, position embedding, and the \nTransformer block, each of which plays a critical role in the \n  Projection \nHead\nContrastive \nLoss\nOverlapping Spectral \nPatch Embedding\nConcat\nClass Token\nPosition Embedding\nLayer Norm\nMulti-Head \nSelf-Attention\nLayer Norm\nCross-Token \nFeedForward\n6\nOverlapping Spectral \nPatch Embedding\nConcat\nClass Token\nPosition Embedding\nLayer Norm\nMulti-Head \nSelf-Attention\nLayer Norm\nCross-Token \nFeedForward\n6\nGradient\nEncoder\nMomentum Encoder\nTransformer Block\nTransformer Block\nTarget prior\nDetect pixels\nEncoder\nEncoder\nCosine \nsimilarity\nExponential operation\nNormalized\nPower Function \nOperation\nNormalized\nBackground Suppression\nDetection result\nTraining\nDetection\nLayer NormLayer Norm\n  Projection \nHead\nQueue\nGaussian blur\nqX\nkX\n12; ; ; N N B\nq q q q\n=X x x x\n12; ; ; N N B\nk k k k\n=X x x x\nOriginal HSI\nAugmented HSI\n⚫ \n⚫ \n \nFig. 1. The overall flowchart of the proposed HTD method based on unsupervised momentum contrastive learning and transformer (MCLT). \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n4 \nencoder’s functionality, with detailed processing given below. \n1) Overlapping Spectral Patch Embedding and Position \nEmbedding: Transformer was first designed for machine \ntranslation tasks [41], using self -attention mechanisms to \nprocess sequence data. Since then it has been widely used in \nnatural language processing (NLP), such as BERT [42]. Due to \nthe successful application of Transformer in the field of NLP, \nTransformer has been concerned to be applied in the field of \ncomputer vision in recent years, such as Vision Transformer \n(ViT) [43], Swin Transformer [44], et al. ViT divides the input \nimage into non-overlapping image blocks and linearly projects \neach image block into a d-dimensional feature vector using the \nlearnable weight matrix [45]. Inspired by ViT, the spectrum is \ndivided into several patches of the same sequence length as the \ninput of Transformer to reduce the length of the input sequence, \nfacilitating straightforward processing and analysis with lower \ncomputational complexity. However, such non -overlapping \nspectral patches would overlook local information  between \nadjacent spectral patches when performing self -attention \noperations, potentially leading to information loss or \nsuboptimal performance. Therefore, overlapping spectral patch \nembedding is designed to provide higher quality token \nsequences to improve the performance of Transformer. \nThe overlapping spectral patch embedding divides the \nspectrum into a number of spectral patches o f fixed sequence \nlength with overlapping regions, capturing spectral \nrelationships between neighboring patches, enabling the \nTransformer to extract the global information of the spectrum \nwhile focusing on the local detail information of the spectrum. \nIt is implemented using a 1 -dimensional convolutional layer \nwith the number of convolutional kernels d, a convolutional \nkernel size k, a step size s and no zero padding for overlapping \nspectral patch segmentation and feature mapping. The number \nof convolution k ernels d controls the dimensionality of each \nspectral patch after feature mapping, the size of the convolution \nkernel k controls the length of each spectral patch, and the step \nsize s controls the size of the non -overlapping part between \nadjacent spectral patches. \nFor a pixel spectrum x with band B, the embedded spectral \ntoken sequence \nx Nd\ne\n  is obtained by overlapping spectral \npatch embedding, where N = ((B-k)/s+1) is the effective input \nsequence length of the Transformer and d is the dimension of \neach embedded spectral token sequence. Then, a learnable \nembedding xlearn is added before the embedded spectral token \nsequence xe, and the output of xlearn obtained by the Transformer \nblock is used as the representation of the spectrum of this pixel. \nFinally, the learnable 1-D position embeddings are added to the \nembedded spectral token sequence to retain the position \ninformation of the spectral patch in the original pixel spectrum. \nThe final embedded spectral token sequence as the Transformer \nblock input can be expressed as: \n \n12\n0 learn ; ; ; ; N\ne e e pos=+z x x x x E\n   (1) \nwhere \n( )1Nd\npos\n+\nE\n  is the learnable 1-D position embedding. \n2) Transformer Block:  The Transformer block, depicted in \nthe Transformer Block segment of Fig. 1, embodies a structured \nframework comprising essential compo nents including  layer \nnormalization (LN) [46], multi-head self-attention (MSA) [41], \nresidual connection [47] and cross -token feedforward layer \n(CTFFL). \nFor a given input sequence, denoted as \nNdz\n , the query \nQ, key K, and value V are derived through the projection of a \nlearnable feature ma trix. Th is process can be succinctly \ndelineated as follows. \n \n ,, QKV=Q K V zE   (2) \nwhere \n3 kdd\nQKV\nE\n  signifies the learnable feature matrix.  \nThe dot product between Q and K is computed and \nsubsequently scaled through divided by \nkd to mitigate \npotential gradient issues arising from large dot product values . \nThe weight of V is determined via the softmax function, which \nis multiplied by value V, yielding the self-attention mechanism \nfor the embedded spectral token sequence, formulated as: \n \n( )\nT\nSA , , softmax\nkd\n\n= \nQKQ K V V   (3) \nwhere \nkd  represents the dimension of Q, K, and V. \nThe MSA mechanism operates by conducting h self-attention \noperations, called \"heads\", in parallel, followed by projecting \ntheir concatenated outputs. The MSA can be formalized as: \n \n , , ,   1\ni i ii i i Q K V ih==Q K V zE\n   (4) \n \n( ) ( ) ( )\n( )\n1 1 1 1 2 2 2 2MSA SA , , ;SA , , ;\n  ;SA , ,h h h h MSA\n= \n\nz Q K V Q K V\nQ K V E\n  (5) \nwhere \n3E k\ni i i\ndd\nQ K V\n  denotes the learnable projection matrix \nemployed to project the input sequence into queries Qi, keys Ki \nand values Vi, \nE kh d d\nMSA\n  represents the feature matrix of \nthe projection concatenated output, and h is the number of self-\nattention operations used in parallel. To make the total \ncomputational cost of MSA similar to that of full -dimensional \nsingle-head self -attention, \nkd  is generally set to d/h. The \nTransformer Block is repeated L times to build an encoder with \nL layers. The output of MSA in each Transformer Block layer \ncan be formalized as: \n \n( )( )\n'\n11MSA LN ,   1l l l lL−−= + =z z z\n   (6) \nThe feedforward layer within the original Transformer \nencoder is a fully connected feedforward network consisting of \ntwo linear transforms with a ReLU activation function in [48]. \nIts fully connected layer is point -wise and cannot learn cross -\ntoken information [49]. The CTFFL is designed to complement \nthe local detail information in the feedforward layer, as shown \nin Fig. 2. \n \nFig. 2. Cross token Feedforward layer (CTFFL). \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n5 \nCTFFL enhances the capture of  local details within the \nfeedforward layer by incorporating depth-wise convolution \nbetween the two fully connected layers of the feedforward layer. \nThe process can be described as follows: \n \n( )\n'\n1FC ; =ZZ   (7) \n \n( )( )( )\n'' ' '\n2FC DWConv ; ;  =+Z Z Z   (8) \nwhere Z is the input of the CTFFL, ω1 and ω2 are the parameters \nof the two fully connected layers, ω is the parameter of the 1-D \ndepth-wise convolutional layer, and σ is the Gaussian error \nlinear unit (GELU) [50] activation function. The output of the \nCTFFL in each Transformer Block layer can be formalized as: \n \n( )( )\n''CTFFL LN ,   1l l l lL= + =z z z\n   (9) \nThis work uses h = 8 parallel self-attention operations and L \n= 2 layers of Transformer Block. The output of the encoder can \nbe expressed as: \n \n( )\n0LN L=yz   (10) \nwhere \n0\nLz  is the output of the corresponding position obtained \nafter the learnable embe dding \nlearnx  passes through the \nTransformer Block, and the output y after \n0\nLz  passes through \nthe LN is used as the representation of the spectrum. \nB. Spectral Discriminability Learning \nSpectral discriminability learning constructs pretext tasks for \nspectral instance discrimination by data augmentation and \ntrains the model with unsupervised momentum contrastive \nlearning [51] to obtain a discriminative encoder with  spectral \ndifference discrimination for HTD. \nData augmentation is achieved by applying Gaussian blur [52] \nto the original HSI \nH W BX\n . Gaussian blurring is achieved \nby convolving each band in t he original HSI with a Gaussian \nkernel. We randomly sampled the standard deviation δ = [0.1, \n2.0], with the kernel size set to 1 ×1. The spectral instance \ndiscrimination pretext task is achieved by pairing the original \nHSI with the spectra of pixels at the same location in the HSI \nafter performing Gaussian blurring. The spectra of pixels at the \nsame position in the original HSI and the HSI after performing \ndata augmentation can be considered as positive pairs. The \nspectral instance discrimination pretext tas k is constructed to \ngenerate a self -supervised signal to help train a discriminative \nencoder. \nThe flow of spectral discriminability learning is shown in the \ntraining part of Fig. 1. A mini -batch of pixel spectra Xq is \nrandomly sampled from the original HSI , and the augmented \nsamples Xk are obtained after data augmentation, expressed as:\n1 2 1 2; ; ; ; ; ;    =  =    X x x x X x x x\n ，N N B N N B\nq q q q k k k k\n. \nThe representation of Xq is extracted using the encoder \nfencoder(·), and the feature matrix Uq = [uq1, uq2, …, uqN]\nMLP Nd\nis obtained after mapping through the projection head. It can be \nformalized as: \n \nencoderencoder encoder encoder MLPMLP ( ( ; ); )qq f =UX   (11) \nThe representation of Xk is extracted using the momentum \nencoder \n( )m_encoderf  , and the feature matrix is obtained after \nmapping through the projection head, marked as Vk = [ vk1, \nvk2, …, vkN]\nMLP Nd\n . The process can be expressed as: \n \n( )( )m_encoderm_encoder m_encoder m_encoder MLPMLP ; ;kk f =VX   (12) \nWhere θencoder and θm_encoder are the parameters of the encoder \nfencoder(·) and the momentum encoder fm_encoder(·), respectively. \nThe projection head MLP encoder of the encoder and the \nprojection head MLP m_encoder of the momentum encoder are \nMLP containing a hidden layer with parameters \nencoderMLP  and \nm_encoderMLP\n, respectively. MLP encoder and MLP m_encoder use the \nReLU activation function. \nThen the feature Vk = [vk1, vk2, …, vkN]\nMLP Nd\n  outputted \nby the momentum encoder through the projection head is fed \ninto the queue. The queue has the property of first -in-first-out. \nThe feature samples in the queue are considered as negative \nsamples. The feature samples in the queue are gradually \nreplaced, with the current mini -batch of feature samples \nentering the queue and the oldest mini-batch of feature samples \nleaving the queue. The negative sample size used for \ncontrastive learning can be separated from the mini-batch input \nsample size by the queue. Therefore, the negative samples could \nbe stored by setting a large queue. A large queue can help the \nmodel to learn more discriminative features since it contains \nabundant negative samples. The queue size can be flexibly and \nindependently set to the hyperparameter K. \nFinally, the output features of the encoder fencoder(·) through \nthe projection head MLP encoder are fed into the contrastive loss \nwith the features in the queue. The contrastive loss maximizes \nthe similarity of positive pairs and minimizes t he similarity of \nnegative pairs, enabling the encoder to have spectral difference \ndiscrimination capability by optimizing the contrastive loss. In \nthis paper, the similarity between positive and negative pairs is \nmeasured by the dot product, and the contra stive loss uses the \nInfoNCE loss function [53], expressed as follows: \n \n( )\n( )\nInfoNCE\n1 0\nexp /1 log\nexp /\niiN qk\nK iji qkj\nL N\n\n= =\n\n=−\n \nuv\nuv   (13) \nwhere τ is the temperature hyperparameter, vkj includes a \npositive sample embedding feature (assuming vk0 = vkj) and K \nnegative sample embedding features. \nDuring training, in order to keep the features in the queue \nstay in step, the features in the queue should be generated using \nthe same or similar momentum encoder and projection head, \nthus to help the model avoid learning to shortcut solutions. \nTherefore, momentum is used to update the momentum encoder \nfm_encoder(·) and its projection head MLPm_encoder. The process can \nbe formalized as follows: \n \n( )m_encoder _ encoder 1m encodermm    + −   (14) \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n6 \n \n( )\nm_encoder m_encoder encoderMLP MLP MLP 1mm    + −   (15) \nwhere m\n [0,1] is the momentum coefficient, set to 0.999 in \nthis paper. It is important to note that the encoder fencoder(·) is \nupdated with its project ion head MLP encoder by gradient \nbackpropagation. \nC. Target Detection and Background Suppression \nThe procedure of target detection and background \nsuppression is shown in the detection part of Fig. 1. \n1) Target Detection: The target prior \n1 B\nt\nx\n  is compared \nwith each pixel spectrum in the original HSI expressed as\n  ( )\n12; ; ;\n\n=X x x x\nH W B\nHW\n by extracting corresponding \nrepresentation through a well -trained encoder \n( )encoderf  . The \nsimilarity of the representations between each pi xel under test \nin the original HSI and the target prior is then measured by \ncosine similarity to obtain the target detection result \n 12; ; ; =B\n HWb b b\n, calculated as follows: \n \n( ) ( )\n( ) ( )\nT\nencoder encoder\nencoder encoder\nit\ni\nit\nffb\nff\n= xx\nxx   (16) \n2) Background Suppression: The values of the target pixels \nin the detection result B obtained by cosine similarity is \nrelatively large with significance, however, the distance \ndifference between the values of background and target pixels \nis relatively small with less significance. The values of the \nbackground pixels in B can be kept away from the values of the \ntarget pixels by the exponential and normalization operations. \nThen the values of the background pixels are further kept away \nfrom the values of the target pixels by the power function and \nnormalization operations to achieve the purpose of background \nsuppression. Background suppression is achieved by \nexponential and normalization operations, power function and \nnormalization operations, which can be represented as follows: \n \n= BS   (17) \n \nmin\nmax min\ni\ni\nsss ss\n−= −   (18) \n \n=RS   (19) \n \nmin\nmax min\ni\ni\nrrr rr\n−= −   (20) \nwhere \n  and \n  are positive parameters that can adjust the \nbackground suppression performance. \nIII. EXPERIMENTAL RESULTS AND ANALYSIS \nA. Data Sets Description \nThe experiments are conducted on four r eal hyperspectral \nimages with different scenarios, where the data sets are \nobtained by three different HSI sensors. \n1) San Diego Data Set: The San Diego data set was collected \nby an Airborne Visible/Infrared Imaging Spectrometer \n(AVIRIS) at San Diego airport, CA, USA. It consists of 224 \nbands with wavelengths ranging from 370 to 2510 nm. Due to \nlow signal-to-noise ratio and water absorption,  bands 1-6, 33-\n35, 97, 107-113, 153-166, and 221-224 were removed, leaving \nthe rest 189 bands for HTD. The whole image has \n400 400  \npixels. The spatial resolution is 3.5 m, and the spectral \nresolution is 10 nm. In the experiment, two scene regions of size \n120 120\n and \n100 100 , named as San Diego A and San \nDiego B, are intercepted from the upper left corner and the \ncenter of the San Diego dataset, respectively.  The plane pixels \nin San Diego A and San Diego B scenes are considered targets \nfor HTD and contain 58 and 134 target pixels, respectively. The \npseudo-color images of San Diego A and San Diego B with \nground truth are shown in Fig. 3 (a)-(b) and Fig. 4 (a)-(b), \nrespectively. \n2) PaviaC Data set: The PaviaC data set was captured by the \nReflection Optical System Imaging Spectrometer (ROSIS -03) \nin the central city of Pavia, Italy. It has \n100 120  pixels and \ncontains 102 bands with wavelengths ranging from 430 to 860 \nnm. The spatial resolution is 1.3 m, and the spectral resolution \nis 4 nm. The background in this scene is mainly composed of \nwater and bridge, and the vehicles on the bridge are considered \nas the targets for HTD with a total of 68 pixels. Fig. 5 (a)-(b) \nshow the pseudo -color image and ground truth of the PaviaC \ndata set. \n3) MUUFL Gulfport Data Set:  The MUUFL Gulfport data \nset [54], [55] was collected in November 2010 at the University \nof Southern Mississippi Gulf Park campus in Long Beach, \nMississippi. The size of the original data set is \n325 337  pixels \nwith 72 bands. The first four and last four bands were removed \ndue to noise, yielding a new HSI with 64 bands. The lower right \ncorner of the original HSI contains invalid regions, so only the \nfirst 220 columns are used for ground truth mapping. The \ncropped HSI  size was \n325 220 64 , with a total of 269 \nclothing panel pixels in the scene considered as targets for HTD. \nFig. 6 (a)-(b) show the pseudo-color image and ground truth of \nthe MUUFL Gulfport data set. \nB. Experimental Setup \n1) Comparison Methods : A total of seven state -of-the-art \nHTD methods were used in the experiment to compare the \nperformance with the proposed MCLT.  The seven compared \nmethods include two classical HTD methods, CEM [15] and \nOSP [16], two representation -based methods, CSCR [22] and \nDM-BDL [56], three deep learning -based methods, BLTSC \n[35], MLSN [28] and ULMMDL [57]. The comparison \nmethods and the proposed MCLT use the same target  prior of \nthe same HSI data sets for HTD. \n2) Implementation Details: The proposed MCLT method is \nimplemented by building an encoder for HTD based on \nTransformer, spectral discriminability learning and background \nsuppression. For the encoder used to extract  the spectral \nrepresentation of each pixel in the HSI to be detected, the \nparameters (d, k, s) in the overlapping spectral patch embedding \nare set to (128, 9, 2), (128, 9, 2), (128, 6, 2) and (128, 4, 2) for \nthe San Diego A, San Diego B, PaviaC, and MUUFL Gulfport \nHSI data sets, respectively.  The dimensionality of the \nembedded spectral token sequence obtained after overlapping \nspectral patch embedding is d = 128. The dimensionality of both \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n7 \nthe learnable embedding xlearn and the learnable 1 -D location \nembedding is set to 128.  Two Transformer Blocks are used to \nconstruct an encoder with depth of 2 for extracting the \nrepresentation of each pixel spectrum. Multi-head self-attention \nin each Transformer Block is achieved by pa rallelly running \nh=8 self -attention operations (called \"heads\") and projecting \ntheir concatenated outputs. The representation obtained by the \n2-layer Transformer Block is used as the representation of the \nentire pixel spectrum.  Note that the input and outp ut of the \nTransformer Block have the same dimension.  In spectral \ndiscriminability learning, the outputs of the encoder and the \nmomentum encoder go through the corresponding projection \nheads to obtain the features of pixel spectra and their augmented \nsamples. The projection heads of the encoder and the \nmomentum encoder have the same structure, both being a two -\nlayer MLP. The number of neurons in the first and second layers \nof MLP encoder and MLP m_encoder is 128.  The queue stores the \noutput of the features by the momentum encoder through the \nprojection head. The queue sizes for San Diego A, San Diego \nB, PaviaC and MUUFL Gulfport HSI data sets are set to 7200, \n10000, 12000 and 13000, respectively.  During training, the \nlearning rate and the temperature coefficien t in the contrastive \nloss are set to 0.5 and 0.07, respectively. The epoch for training \nis set to 50.  The mini-batch during training is set to 480, 400, \n600 and 1300 for San Diego A, San Diego B, PaviaC and \nMUUFL Gulfport data sets, respectively. For the b ackground \nsuppression process, the exponential operation sets α to \n479 10  \nfor all data sets in the experiment, and β in the power function \noperation is set to 20, 60, 20, and 60 for the San Diego A, San \nDiego B, PaviaC, and MUUFL Gulfport data sets, respectively. \nThe comparison method follows the settings recommended in \nthe original literature. \nThe experimental hardware environment consists of an AMD \nRyzen Threadripper 3990X 64 -core processor with a Quadro \nRTX 8000 GPU with 48GB of RAM. Two classical HTD (CEM \nand OSP) methods and two representation -based HTD (CSCR \nand DM-BDL) methods are implemented in MATLAB R2017b, \nand three deep learning -based comparison methods (BLTSC, \nMLSN and ULMMDL) are implemented using Python 3.6 and \nTensorFlow 1.80. The proposed MCLT is implemented using \nPython 3.8.3 and PyTorch 1.60. \n3) Prior Target Spectrum Selection: The prior target \nspectrum in the experiment was obtained from hyperspectral \nimages, and the coordinates of the prior target spectrum were \ntaken as (11, 88), (36, 49), (57, 14), and (153, 159) for the four \ndatasets in the experiment, respectively. Notably, in each \ndataset, the prior target spectrum was chosen at the center point \nof the target, aiming to capture the original spectral properties \nof the target effectively. Only one target spectrum was taken as \nthe prior target spectrum. \n4) Evaluation Criterion:  To evaluate the HTD performance \nof the proposed MCLT method and other comparison methods, \nthree-dimensional receiver operating characteristic (3D ROC) \ncurve is used to measure the performance of the detector.  The \n3D ROC curve can be considered as a functi on of detection \nprobability PD, false alarm probability PF and threshold τ, and \ncan be obtained as the value of τ varies [58]. PD and PF can be \ncalculated as follows: \n \n( )\nTP,\nD\nTP, FN,\nP N\nNN\n\n\n = +   (21) \n \n( )\nFP,\nF\nFP, T N,\nP N\nNN\n\n\n = +   (22) \nwhere NTP,τ denotes the number of pixels that are correctly \ndetected as targets at a given threshold τ, NFN,τ represents the \nnumber of pixels that incorrectly detect targets as backgrounds \nat a given threshold τ, NFP,τ denotes the number of pixels that \nincorrectly detect backgrounds as targets at a given threshold τ, \nand NTN,τ denotes the number of pixels that are correctly \ndetected as backgrounds at a given threshold τ. Three 2D ROC \ncurves can be obtained from the 3D ROC curves, including the \n2D ROC curve of (PD, PF), the 2D ROC curve of (PD, τ), and the \n2D ROC curve of (P F, τ), respectively. The 2D ROC curve of \n(PD, PF) can evaluate the effectiveness of the detector, the 2D \nROC curve of (PD, τ) can evaluate the target detectability of the \ndetector, and the 2D ROC curve of (P F, τ) can evaluate the \nbackground suppression ability of the detector. The detector \nshould have better performance with the following three \nconditions of the corresponding 2D ROC curves: the closer the \n2D ROC curve of (P D, P F) is to the upper left corner of the \ncoordinate axis, the closer the 2D ROC curve of (PD, τ) is to the \nupper right corner of the coordinate axis, and the closer the 2D \nROC curve of (PF, τ) is to the lower left corner of the coordinate \naxis. \nFor quantitative analysis of the detector, the areas under the \ncurves (AUC) of the 2D ROC curves (P D, PF), (PD, τ), and (PF, \nτ) were used as quantitative indicators to quantify the \nperformance of the detector. The detector performs better when \nAUC(PD, PF) and AUC(PD, τ) are close to 1 and when AUC(PF, \nτ) is close to 0. The detection performance improves with higher \nvalues of AUC(PD, P F) and AUC(PD, τ), while background \nsuppression improves with lower values of AUC(PF, τ). By \nconsidering these three AUC values, Chang [59] devised a \ndetection measure AUCOD and a background suppression \ncapability measure AUCBS to evaluate the performance of the \ndetector, which was defined as: \n \n( ) ( ) ( )OD D F D FAUC AUC P ,P AUC P , AUC P ,= + −   (23) \n \n( ) ( )BS D F FAUC AUC P ,P AUC P , =−   (24) \nwhere \n OD 1,2AUC − , and \n BS 1,1AUC − . The larger the \ncalculated values of \nODAUC  and \nBSAUC , the better the \ndetection performance and background suppression effect of \nthe detector. \nC. Results and Discussion \nFigures (c)-(j) in Figs. 3-6 show the detection maps of the \nproposed MCLT and other comparison methods for San Diego \nA, San Diego B, PaviaC, and MUUFL Gulfport data sets, \nrespectively. Subjectively visual assessment from the detection \nmaps, CEM, BLTSC, and the proposed MCLT have good  \nbackground suppression effect compared with other \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n8 \ncomparison methods. However, many target pixels are missed \nin the detection maps of CEM and BLTSC algorithms, only \nremaining the proposed MCLT with good effect. OSP, CSCR, \nMLSN and ULMMDL can detect most o f the targets, but the \nbackground suppression is not good, making it very difficult to \nidentify them visually. The detection maps of the proposed \nMCLT method visually show excellent detection performance, \nwith targets highlighted clearly and background sup pressed \nwell. \nFigs. 7-10 show the 3D ROC and the corresponding three 2D \nROC curves for the proposed MCLT and seven state -of-the-art \ncomparison methods on the San Diego A, San Diego B, PaviaC, \nand MUUFL Gulfport data sets corresponding to their detection \nresults. For the 2D ROC curves of (PD, PF) used to evaluate the \ndetector effectiveness, as shown in Figs. 7(b)-10(b), the 2D \nROC curves of (P D, P F) of the MCLT for all HSIs in the \nexperiment are closer to the upper left corner than the \ncomparison methods. For the 2D ROC curves of (P D, τ) \nevaluating the detectability of the detector to the target, as \nshown in Figs. 7(c)-10(c), the proposed MCLT outperforms \nCEM and BLTSC, but OSP, CSCR, and ULMMDL perform \nbetter than the proposed MCLT. However, for the 2D ROC \ncurves of (PF, τ) evaluating the detector background suppression \nability, MCLT is very close to the lower left corner and has \nsignificantly better background suppression than OSP, CSCR, \nand ULMMDL. \nSince very close ROC curves cannot visually distinguish \nprecisely which detector performs better, the areas under the \ncurves AUC(PD, P F), AUC(PD, τ), and AUC(PF, τ) of the 2D \nROC curves of (PD, PF), (PD, τ) and (PF, τ) are used to evaluate \n     \n(a) (b) (c) (d) (e) \n     \n(f) (g) (h) (i) (j) \nFig. 3. Detection maps of different methods for San Diego A. (a) Pseudo-color image. (b) Ground truth. (c) CEM. (d) OSP. (e) CSCR. (f) DM-BDL. (g) BLTSC. \n(h) MLSN. (i) ULMMDL. (j) MCLT. \n  \n     \n(a) (b) (c) (d) (e) \n     \n(f) (g) (h) (i) (j) \nFig. 4. Detection maps of different methods for San Diego B. (a) Pseudo-color image. (b) Ground truth. (c) CEM. (d) OSP. (e) CSCR. (f) DM-BDL. (g) BLTSC. \n(h) MLSN. (i) ULMMDL. (j) MCLT. \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n9 \nthe performance of the detectors quantitatively. In addition, \nAUCBS and AUC OD are used to quantitatively evaluate the \nbackground suppression ability and the comprehensive \ndetection performance of the detector. Table Ⅰ provides specific \nvalues of the five AUC measures for MCLT and all comparison \nmethods on the four HSI data sets. The best results in each AUC \nmeasure are shown in bold, and the sub -optimal results are \nunderlined. Table Ⅰ shows that  the proposed MCLT always \nobtains the highest AUC(PD, PF) of all HSI data sets, verifying \nits effectiveness in HTD. ULMMDL obtained the highest \nAUC(PD, τ) on the San Diego A and San Diego B data sets, \ndemonstrating excellent target detection capabilities. This is \nmade possible by the hierarchical denoising autoencoder \n(HDAE) designed in the ULMMDL method. HDAE enhances \n     \n(a) (b) (c) (d) (e) \n     \n(f) (g) (h) (i) (j) \nFig. 5. Detection maps of different methods for PaviaC. (a) Pseudo -color image. (b) Ground truth. (c) CEM. (d) OSP. (e) CSCR. (f) DM -BDL. (g) BLTSC. (h) \nMLSN. (i) ULMMDL. (j) MCLT. \n  \n     \n(a) (b) (c) (d) (e) \n     \n(f) (g) (h) (i) (j) \nFig. 6. Detection maps of different methods for MUUFL Gulfport. (a) Pseudo -color image. (b) Ground truth. (c) CEM. (d) OSP. (e) CSCR. (f) DM -BDL. (g) \nBLTSC. (h) MLSN. (i) ULMMDL. (j) MCLT. \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n10 \nthe spectral coherence by iteratin g over the denoising \nautoencoder layer by layer, which alleviates the intra -class \ndifferences in the target spectra in the HSIs to be detected and \nmakes ULMMDL have a good target preservation capability. \nHowever, the values of AUC(PD, PF), AUC(PF, τ), and AUCBS \nof ULMMDL on four HSI data sets are lower than those of the \nproposed MCLT method. For the AUC(PF, τ), BLTSC achieved \noptimal results on the San Diego A, San Diego B, and MUUFL \nGulfport data sets, and MCLT achieved suboptimal results, \nonly slightly weaker than BLTSC. MCLT obtained the optimal \nresults on the PaviaC data set, and BLTSC got the suboptimal \nresults. BLTSC obta ins the weight map of distinguishable \ntargets by background learning. Then the weight map of \ndistinguishable targets is used to correct the results of CEM \n    \n(a) (b) (c) (d) \nFig. 7. 3D ROC and the corresponding 2D ROC curves of different methods for the San Diego A data set. (a) 3D ROC curve. (b) 2D ROC curve of (PD, PF). (c) \n2D ROC curve of (PD, τ). (d) 2D ROC curve of (PF, τ). \n  \n \n   \n(a) (b) (c) (d) \nFig. 8. 3D ROC and the corresponding 2D ROC curves of different methods for the San Diego B data set. (a) 3D ROC curve. (b) 2D ROC curve of (PD, PF). (c) \n2D ROC curve of (PD, τ). (d) 2D ROC curve of (PF, τ). \n \n    \n(a) (b) (c) (d) \nFig. 9. 3D ROC and the corresponding 2D ROC curves of different methods for the PaviaC data set. (a) 3D ROC curve. (b) 2D ROC curve of (P D, PF). (c) 2D \nROC curve of (PD, τ). (d) 2D ROC curve of (PF, τ). \n \n    \n(a) (b) (c) (d) \nFig. 10. 3D ROC and the corresponding 2D ROC curves of different methods for the MUUFL Gulfport data set. (a) 3D ROC curve. (b) 2D ROC curve of (PD, \nPF). (c) 2D ROC curve of (PD, τ). (d) 2D ROC curve of (PF, τ). \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n11 \ncoarse detection to detect targets and suppress background. \nHowever, the performance of BLTSC relies on the performance \nof the coarse detection method. MCLT does not rely on the prior \ninformation found by traditional methods, and the AUC(PD, PF), \nAUC(PD, τ) , AUCBS, and AUCOD of MCLT are better than \nthose of BLTSC on the four HSIs in the experiment. For the \nAUCBS used to combine the effects of PD and PF on background \nsuppression, the proposed MCLT achieved optimal results on \nfour HSI data sets. The excellen t background suppression \nshows that two nonlinear pull -ups by exponential and power \nfunction operations can effectively suppress the background \nand preserve the target. For the AUCOD used to evaluate the \noverall detection performance of the detector, the M CLT \nachieved the best overall performance on San Diego A, PaviaC, \nand MUUFL Gulfport. This shows that MCLT achieves \ncompetitive results with unsupervised momentum contrastive \nlearning for spectral discriminability learning and an encoder \nbased on Transform er constructed for extracting spectral \nfeatures of pixels. \nFig. 11 shows the target-background separability box plots of \nthe detection results of the MCLT and comparison methods on \nthe four HSI data sets. In the target-background separability box \nplots, target and background pixels with statistically distributed \nvalues are placed in the box, removing the highest and lowest \n10% of data in the target and background classes [60]. The red \nboxes indicate the distribution of targets, and the green boxes \nindicate the distribution of backgrounds. In the boxes, the \nmiddle horizontal line indicates the median value. The \nhorizontal lines at the top and bottom rows of each box indicate \nthe maximum and minimum values. The target -background \nseparability box plot not only reflects the separability of the \ntarget and background in the detection results but also observes \nthe distribution ra nge of the target and background pixels \ndetection values in the detection results. As can be seen in Fig. \n11, for the four HSI data sets, the MCLT suppresses the \ndetected values of the background pixels in the detection results \nto zero, demonstrating excel lent background suppression, and \nthe MCLT also separates the target from the background well. \nCompetitive separability suggests that unsupervised \nmomentum contrastive learning enables the model to learn \nspectral difference discrimination effectively and en ables the \nmodel to distinguish well between targets and backgrounds in \nHSIs to be detected. \nD. Ablation Studies \n1) Effect of Overlapping Spectral Patch Embedding on \nTarget Detection Accuracy:  To investigate the effect of \noverlapping spectral patch embedding o n the HTD accuracy, \nthe overlap between adjacent spectral patches is changed to \nobserve the impact of different sizes of overlapping patches on \nthe HTD accuracy. For the San Diego A and San Diego B data \nsets with 189 bands, each pixel spectrum is divided into spectral \npatches of length 9.  For the PaviaC data set with 102 bands, \neach pixel spectrum is divided into spectral patches of length 6. \nFor the MUUFL Gulfport data set with 64 bands, each pixel \nspectrum is divided into spectral patches of length 4.  Figs. 12 \nTABLE I \nACCURACY COMPARISON OF DIFFERENT METHODS FOR FOUR HSI DATA SETS. BOLDFACE HIGHLIGHTS THE BEST RESULT, WHILE UNDERLINE THE SECOND \nHSI Data Sets Methods \nCEM [15] OSP [16] CSCR [22] DM-BDL [49] BLTSC [33] MLSN [26] ULMMDL [50] MCLT \nSan Diego \nA \n( )DFP ,PAUC\n  0.96287 0.99475 0.99546 0.97586 0.96687 0.99688 0.99455 0.99877 \n( )DP,AUC \n  0.29733 0.74021 0.58313 0.33058 0.19136 0.89671 0.94333 0.51104 \n( )FP,AUC \n  0.03854 0.32051 0.18891 0.00874 0.00063 0.42555 0.63472 0.00192 \nBSAUC\n 0.92433 0.67424 0.80655 0.96712 0.96624 0.57133 0.35983 0.99685 \nODAUC\n  1.22166 1.41445 1.38968 1.29770 1.15760 1.46804 1.30316 1.50789 \nSan Diego \nB \n( )DFP ,PAUC\n  0.87253 0.96649 0.99151 0.95445 0.88019 0.99164 0.99143 0.99704 \n( )DP,AUC \n  0.12237 0.65370 0.60090 0.37670 0.05052 0.73711 0.75871 0.31875 \n( )FP,AUC \n  0.02637 0.41919 0.25346 0.00739 0.00019 0.41285 0.25950 0.00082 \nBSAUC\n 0.84616 0.54730 0.73805 0.94706 0.88000 0.57879 0.73193 0.99622 \nODAUC\n  0.96853 1.2010 1.33895 1.32376 0.93052 1.31590 1.49064 1.31497 \nPaviaC \n( )DFP ,PAUC\n  0.87176 0.93250 0.92624 0.96398 0.86545 0.83108 0.89930 0.99126 \n( )DP,AUC \n  0.16579 0.43401 0.73520 0.26354 0.10956 0.17797 0.57599 0.28855 \n( )FP,AUC \n  0.02262 0.20101 0.44477 0.00956 0.00051 0.04926 0.42257 0.00007 \nBSAUC\n 0.84914 0.73149 0.48147 0.95442 0.86494 0.78182 0.47673 0.99119 \nODAUC\n  1.01493 1.16550 1.21667 1.21796 0.97450 0.95979 1.05272 1.27974 \nMUUFL \nGulfport \n( )DFP ,PAUC\n  0.90268 0.85376 0.85760 0.94445 0.90693 0.97543 0.93750 0.99171 \n( )DP,AUC \n  0.31919 0.85032 0.62237 0.45658 0.27066 0.91938 0.59534 0.58308 \n( )FP,AUC \n  0.01624 0.67801 0.43781 0.01187 0.00015 0.33405 0.02928 0.00050 \nBSAUC\n 0.88644 0.17575 0.41979 0.93258 0.90678 0.64138 0.90822 0.99121 \nODAUC\n  1.20563 1.02607 1.04216 1.38916 1.17744 1.56076 1.50356 1.57429 \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n12 \n(a)-(d) show the effect of different size overlaps on HTD \naccuracy versus time consumption on the San Diego A, San \nDiego B, PaviaC, and MUUFL Gulfport data sets, respectively. \nThe horizontal coordinates of each sub -figure in Fig. 1 2 \nindicate the length of the divided spectral patches and the length \nof the non-overlapping part between adjacent spectral patches.  \nThe overlap between adjacent spectral patches in the horizontal \ncoordinate decreases from left to right until there is no overlap \nbetween adjacent spectral patches. For the San Diego A and San \nDiego B data sets with more bands, the detection accuracy of \nhyperspectral targets achieved when there is no overlap \nbetween adjacent spectral patches is much lower than the \ndetection accuracy when there is an overlap between adjacent \nspectral patches. The best detection accuracy was obtained at a \nlength of 2 for the non -overlapping parts between adjacent \nspectral patches.  However, as the overlap between adjacent \nspectral patches gradually increases, the length of the embedded \nspectral sequence would also increase accordingly, leading to \nan increased time consumption.  For the PaviaC and MUUFL \nGulfport data sets with fewer bands, the detection accuracy of \ntargets obtained with overlapping parts between adjacent \nspectral patches is higher than that of non -overlapping parts \nbetween adjacent spectral pat ches. However, target detection \naccuracy is decreased when there is excessive overlap between \nadjacent spectral patches.  The local information between \nadjacent spectral patches in the embedded sequence can be \nincreased with a suitable overlap, allowing the  Transformer to \nconcentrate on both the global knowledge of the spectrum and \nthe local details in the spectrum. \n 2) Impact of Cross -Token Feedforward Layer on Target \nDetection Accuracy:  To investigate the effect of cross -token \nfeedforward layers on the acc uracy of HTD, we perform HTD \non four HSI data sets using encoders composed of the designed \ncross-token feedforward layer and the feedforward layer in the \noriginal Transformer, respectively. All operations are point -\nwise in the original Transformer feedforw ard layer, and no \ncross-token information can be learned [49]. The cross -token \nfeedforward layer complements the local details in the \nfeedforward layer by adding depth -wise convolution between \nthe two fully connected layers of the original feedforward layer. \nFig. 13 and Fig. 14 represent the target detection accuracy and \ntime consumption of HTD using feedforward and cross -token \n \n  \n(a) (b) \n  \n(c) (d) \nFig. 11. Target-background separability box plots on four HSIs. (a) San Diego A, (b) San Diego B, (c) PaviaC, (d) MUUFL Gulfport. \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n13 \nfeedforward layers on four HSI data sets, respectively. It could \nbe proved that for all HSI data sets in the experiment, the \ndetection accuracy of HTD using the encoder composed of \ncross-token feedforward layers is greater than that of the \nencoder composed  of the original feedforward layers. This \ndemonstrates that performance improvements may result from \n  \n(a) (b) \n  \n(c) (d) \nFig. 12.  Effect of different overlap sizes of adjacent spectral patches on the four HSI data sets on the accuracy of HTD. (a)San Diego A, (b)San Diego B, (c)PaviaC, \n(d)MUUFL Gulfport. \n \n  \nFig. 13. Comparison of the detection performance of the cross-token \nfeedforward layer with the conventional feedforward layer for hyperspectral \ntarget detection on four HSI data sets. \nFig. 14. Time consumption of the cross-token feedforward layer versus the \ntraditional feedforward layer on four HSI data sets. \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n14 \nincluding local detail information in the feedforward layer. \nHowever, adding depth-wise convolution between the two fully \nconnected layers of the feedforwa rd layer would introduce \nadditional learnable parameters, resulting in increased time \nconsumption. \nE. Time Consumption \nTable Ⅱ shows the time consumption of the seven compared \nmethods and the proposed MCLT method  together with Table \nIII with the model complexity of deep -learning based \nalgorithms. Table Ⅱ shows that the classical HTD methods \n(CEM and OSP) and representation -based HTD methods \n(CSCR and DM-BDL) consume much less time than the deep \nlearning-based HTD methods. This is reasonable because deep \nlearning-based methods require training to obtain the \nparameters of the network. For four deep learning -based HTD \nmethods, the training time of the proposed method is lower than \nthat of BLTSC and MLSN, and very close to that of ULMMDL. \nThis is due to the Transformer network used in the proposed \nmethod, which has a multi -headed self -attention mechanism \nthat can be well parallelized on the GPU. Moreover, the training \nepoch of the proposed method is lower than that of the \ncompared BLTSC and MLSN in obtaining the optimal \ndetection results. The smaller training epoch further reduces the \ntraining time consumption of the proposed method. On ce the \nmodel is well -trained, the detection efficiency depends on the \ndetection time. The detection time of the deep learning -based \ndetection method starts from loading the model. It ends with the \ndetection result, as shown in Table Ⅱ. The detection time of the \nproposed MCLT is less than the other two deep learning -based \nmethods (BLTSC and MLSN) for the same HSI dataset. This is \nbecause the proposed method only needs to measure the \nsimilarity between the representation of the pixel spectrum to \nbe detected and the prior target spectrum by cosine similarity at \nthe time of detection, which can be achieved by matrix \nmultiplication. Although the detection time consumption of \nMCLT is slightly more than that of ULMMDL, the detection \naccuracy of MCLT is higher than that of ULMMDL. \nIV. CONCLUSION \nIn this paper, a new HTD method based on unsupervised \nmomentum contrastive learning and Transformer is proposed, \nwhich can achieve excellent detection results with only one \ntarget prior spectrum. The traditional Transformer has a n \nexcellent performance in focusing on spectral long -range \ndependencies and self-similarity, but it needs more attention to \nlocal details of the spectrum. In view of the above problem, \noverlapping spectral patch embedding and cross -token \nfeedforward layers  are designed in this paper to help the \nTransformer focus on spectral local detail information. Then, a \nmomentum encoder based on momentum update is used to \nextract the features of the pixel spectra for spectral \ndiscriminability learning. Finally, contrastive loss is performed \nfor spectral discriminability learning by maximizing the \nsimilarity of positive pairs while minimizing the similarity of \nnegative pairs. In the stage of target detection, the initial \ndetection results are pulled up nonlinearly twice to suppress the \nbackground by using exponential -normalization, and power \nfunction-normalization operations, inspired by the function \nTABLE Ⅱ \nTIME CONSUMPTION OF DIFFERENT METHODS FOR FOUR HYPERSPECTRAL DATA SETS. \nMethod San Diego A San Diego B PaviaC MUUFL Gulfport \nCEM 0.0141 0.0121 0.0079 0.0364 \nOSP 0.0681 0.0485 0.0299 0.1036 \nCSCR 3.5560 2.2939 3.1590 751.5010 \nDM-BDL 4.2658 3.4805 4.1044 16.5390 \nBLTSC Train 964.3975 645.8120 638.062 3274.605 \nDetect 5.9217 5.8889 3.6523 3.6311 \nMLSN Train 1163.754 1101.236 888.331 1099.945 \nDetect 36.9408 25.8605 30.1774 163.9716 \nULMMDL Train 70.9670 51.0487 49.1577 154.2198 \nDetect 0.0469 0.0416 0.2344 0.1719 \nMCLT Train 94.6355 66.6549 41.5688 153.1928 \nDetect 0.6437 0.4987 0.3617 0.8966 \nTABLE III \nMODEL PARAMETERS AND COMPUTATIONS FOR FOUR DEEP LEARNING-BASED DETECTORS ON FOUR HYPERSPECTRAL DATASETS (IN MILLIONS OR \nTHOUSANDS) \nMethods Indexes San Diego A San Diego B PaviaC MUUFL \nGulfport \nBLTSC Model Parameters(M) 1.57 1.57 1.50 1.47 \nFLOPs (M) 17.93 17.93 6.26 4.54 \nMLSN Model Parameters(M) 0.05 0.05 0.05 0.05 \nFLOPs (M) 9.83 9.83 6.12 3.31 \nULMMDL Model Parameters(K) 7.77 7.77 4.20 2.64 \nFLOPs (K) 15.53 15.53 8.39 5.28 \nMCLT Model Parameters(M) 0.94 0.94 0.94 0.94 \nFLOPs (M) 73.73 73.73 40.09 25.69 \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n15 \ncurve properties of exponential and power functions between 0 \nand 1. Experimental results on four real HSIs show that the \nproposed MCLT achieves excellent target detection and \nbackground suppression performance on HTD tasks. \nREFERENCES \n \n[1] N. M. Nasrabadi, “Hyperspectral target detection: An overview of current \nand future challenges,” IEEE Signal Process Mag., vol. 31, no. 1, pp. 34-\n44, 2013. \n[2] C. Zhao, B. Qin, S. Feng, W. Zhu, L. Zhang, and J. Ren, “An unsupervised \ndomain adaptation method towards multi -level features and decision \nboundaries for cross -scene hyperspectral image classification,” IEEE \nTrans. Geosci. Remote Sens., vol. 60, pp. 1-16, 2022. \n[3] C. Yu, X. Zhao, B. Gong, Y. Hu, M. Song, H. Yu, C. -I. Chang, \n“Distillation-Constrained Prototype Representation Network for \nHyperspectral Image Incremental Classification,” IEEE Trans. Geosci. \nRemote Sens., vol. 62, pp. 1-15, 2024. \n[4] Y. Wang, H. Ma, Y. Yang, E. Zhao, M. Song and C. Yu, “Self-Supervised \nDeep Multi -Level Representation Learning Fusion -Based Maximum \nEntropy Subspace Clustering for Hyperspectral Band Selection ,” Remote \nSens., vol. 16, no. 2, pp. 1-18, 2024. \n[5] Y. Zhang, W. Li, R. Tao, J. Peng, Q. Du, and Z. Cai, “Cross -scene \nhyperspectral image classification with discriminative cooperative \nalignment,” IEEE Trans. Geosci. Remote Sens., vol. 59, no. 11, pp. 9646-\n9660, 2021. \n[6] Y. Wang, X. Chen, E. Zhao and M. Song, “Self-Supervised Spectral-Level \nContrastive Learning for Hyperspectral Target Detection ,” IEEE Trans. \nGeosci. Remote Sens., vol. 61, pp. 1-15, 2023. \n[7] C. Zhao, M. Wang, S. Feng, and N. Su, “Hyperspectral target detection \nmethod based on nonlocal self-similarity and rank-1 tensor,” IEEE Trans. \nGeosci. Remote Sens., vol. 60, pp. 1-15, 2022. \n[8] X. Zhao, W. Li, C. Zhao, and R. Tao, “Hyperspectral target detection \nbased on weighted cauchy distance graph and local adaptive collaborative \nrepresentation,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1-13, 2022. \n[9] C. Zhao, W. Zhu, and S. Feng, “Superpixel guided deformable convolution \nnetwork for hyperspectral image classification,” IEEE Trans. Image \nProcess., vol. 31, pp. 3838-3851, 2022. \n[10] J. Wang, C. Tang, Z. Li, X. Liu, W. Zhang, E. Zhu, and L. Wang, \n“Hyperspectral band selection via region -aware latent features fusion \nbased clustering,” Inf. Fusion, vol. 79, pp. 162-173, 2022. \n[11] L. Zheng, Y. Wen, W. Ren, H. Duan, J. Lin, an d J. Irudayaraj, \n“Hyperspectral dark -field microscopy for pathogen detection based on \nspectral angle mapping,” Sens. Actuators, B, vol. 367, pp. 132042, 2022. \n[12] E. Aloupogianni, M. Ishikawa, N. Kobayashi, and T. Obi, “Hyperspectral \nand multispectral image processing for gross-level tumor detection in skin \nlesions: A systematic review,” J. Biomed. Opt., vol. 27, no. 6, pp. 060901, \n2022. \n[13] N. M. Nasrabadi, “Regularized spectral matched filter for target \nrecognition in hyperspectral imagery,” IEEE Signal Process Lett., vol. 15, \npp. 317-320, 2008. \n[14] S. Kraut, L. L. Scharf, and L. T. McWhorter, “Adaptive subspace \ndetectors,” IEEE Trans. Signal Process., vol. 49, no. 1, pp. 1-16, 2001. \n[15] J. Settle, “On constrained energy minimization and the partial unmixing of \nmultispectral images,” IEEE Trans. Geosci. Remote Sens., vol. 40, no. 3, \npp. 718-721, 2002. \n[16] C.-I. Chang, “Orthogonal subspace projection (osp) revisited: A \ncomprehensive study and analysis,” IEEE Trans. Geosci. Remote Sens.,  \nvol. 43, no. 3, pp. 502-518, 2005. \n[17] H. Kwon, and N. M. Nasrabadi, “Kernel spectral matched filter for \nhyperspectral imagery,” Int. J. Comput. Vision, vol. 71, no. 2, pp. 127-141, \n2007. \n[18] H. Kwon, and N. M. Nasrabadi, “Kernel adaptive subspace detector for \nhyperspectral imagery,” IEEE Geosci. Remote Sens. Lett., vol. 3, no. 2, pp. \n271-275, 2006. \n[19] X. Jiao, and C.-I. Chang, “Kernel-based constrained energy minimization \n(k-cem),” in Algorithms and Technologies for Multispectral, \nHyperspectral, and Ultraspectral Imagery XIV, 2008, pp. 523-533. \n[20] H. Kwon, and N. M. Nasrabadi, “Kernel orthogonal subspace projection \nfor hyperspectral signal classification,” IEEE Trans. Geosci. Remote Sens., \nvol. 43, no. 12, pp. 2952-2962, 2005. \n[21] Y. Chen, N. M. Nasrabadi, and T. D.  Tran, “Sparse representation for \ntarget detection in hyperspectral imagery,” IEEE J. Sel. Top. Signal \nProcess., vol. 5, no. 3, pp. 629-640, 2011. \n[22] W. Li, Q. Du, and B. Zhang, “Combined sparse and collaborative \nrepresentation for hyperspectral target detection,” Pattern Recognit., vol. \n48, no. 12, pp. 3904-3916, 2015. \n[23] B. Tu, Z. Wang, H. Ouyang, X. Yang, J. Li and A. Plaza,  “Hyperspectral \nAnomaly Detection Using the Spectral -Spatial Graph ,” IEEE Trans. \nGeosci. Remote Sens., vol. 60, pp. 1-14, 2022. \n[24] X. Yang, B. Tu, Q. Li, J. Li, and A. Plaza, “Graph evolution-based vertex \nextraction for hyperspectral anomaly detection,” IEEE Trans. Neural \nNetworks Learn. Syst., pp. 1-15, 2023. \n[25] W. Li, G. Wu, and Q. Du, “Transferred deep learning for hyperspec tral \ntarget detection,” in Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), \n2017, pp. 5177-5180. \n[26] Z. Feng, J. Zhang, and J. Feng, “Spectral -spatial joint target detection of \nhyperspectral image based on transfer learning,” in Proc. IEEE Int. Geosci. \nRemote Sens. Symp. (IGARSS), 2020, pp. 1770-1773. \n[27] Y. Shi, J. Li, Y. Li, and Q. Du, “Sensor -independent hyperspectral target \ndetection with semisupervised domain adaptive few-shot learning,” IEEE \nTrans. Geosci. Remote Sens., vol. 59, no. 8, pp. 6894-6906, 2020. \n[28] Y. Wang, X. Chen, F. Wang, M. Song, and C. Yu, “Meta -learning based \nhyperspectral target detection using siamese network,” IEEE Trans. \nGeosci. Remote Sens., vol. 60, pp. 1-13, 2022. \n[29] S. Mei, X. Liu, G. Zhang, and Q. Du, “Sensor -specific transfer learning \nfor hyperspectral image processing,” in Proc. Int. Workshop Anal. \nMultitemporal Remote Sens. Images (MultiTemp), 2019, pp. 1-4. \n[30] G. Zhang, S. Zhao, W. Li, Q. Du, Q. Ran, and R. Tao, “Htd -net: A deep \nconvolutional neural network for target detection in hyperspectral \nimagery,” Remote Sens., vol. 12, no. 9, pp. 1489, 2020. \n[31] Y. Gao, Y. Feng, and X. Yu, “Hyperspectral target detection with an \nauxiliary generative adversarial network,” Remote Sens., vol. 13, no. 21, \npp. 4454, 2021. \n[32] D. Zhu, B. Du, and L. Zhang, “Two -stream convolutional networks for \nhyperspectral target detection,” IEEE Trans. Geosci. Remote Sens., vol. 59, \nno. 8, pp. 6907-6921, 2020. \n[33] W. Rao, L. Gao, Y. Qu, X. Sun, B. Zhang, and J. Chanussot, “Siamese \ntransformer network for hyperspectral image target detection,” IEEE \nTrans. Geosci. Remote Sens., vol. 60, pp. 1-19, 2022. \n[34] Y. Shi, J. Li, Y. Zheng, B. Xi, and Y. Li, “Hyperspectral tar get detection \nwith roi feature transformation and multiscale spectral attention,” IEEE \nTrans. Geosci. Remote Sens., vol. 59, no. 6, pp. 5071-5084, 2021. \n[35] W. Xie, X. Zhang, Y. Li, K. Wang, and Q. Du, “Background learning \nbased on target suppression constraint for hyperspectral target detection,” \nIEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., vol. 13, pp. 5887-5897, \n2020. \n[36] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework \nfor contrastive learning of visual representations,” in Int. Conf. Mach. \nLearn., 2020, pp. 1597-1607. \n[37] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, \n“Unsupervised learning of visual features by contrasting cluster \nassignments,” Advances in Neural Information Processing Systems,  vol. \n33, pp. 9912-9924, 2020. \n[38] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for \nunsupervised visual representation learning,” in Proceedings of the \nIEEE/CVF conference on computer vision and pattern recognition, 2020, \npp. 9729-9738. \n[39] J.-B. Grill, F. Strub, F. Altché , C. Tallec, P. Richemond, E. Buchatskaya, \nC. Doersch, B. Avila Pires, Z. Guo, and M. Gheshlaghi Azar, “Bootstrap \nyour own latent-a new approach to self-supervised learning,” Advances in \nNeural Information Processing Systems, vol. 33, pp. 21271-21284, 2020. \n[40] X. Chen, and K. He, “Exploring simple siamese representation learning,” \nin Proceedings of the IEEE/CVF Conference on Computer Vision and \nPattern Recognition, 2021, pp. 15750-15758. \n[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. Adv. \nNeural Inf. Process. Syst., 2017, pp. 5998-6008. \n[42] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of \ndeep bidirecti onal transformers for language understanding,” \narXiv:1810.04805, 2018. \n[43] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. \nUnterthiner, M. Dehghani, M. Minderer, G. Heigold, and S. Gelly, “An \nimage is worth 16x16 words: Transformers for image recognition at scale,” \narXiv:2010.11929, 2020. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n16 \n[44] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \n“Swin transformer: Hierarchical vision transformer using shifted \nwindows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 10012-\n10022. \n[45] P. Wang, X. Wang, H. Luo, J. Zhou, Z. Zhou, F. Wang, H. Li, and R. Jin, \n“Scaled relu matters for training vision transformers,” in Proc. AAAI Conf. \nArtif. Intell., 2022, pp. 2495-2503. \n[46] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv \npreprint arXiv:1607.06450, 2016. \n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image \nrecognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2016, \npp. 770-778. \n[48] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jé gou, \n“Training data -efficient image transformers & distillation through \nattention,” in Proc. Int. Conf. Mach. Learn. (ICML) , 2021, pp. 10347 -\n10357. \n[49] S. Ren, D. Zhou, S. He, J. Feng, and X. Wang, “Shunted self-attention via \nmulti-scale token aggregation,” in Proc. IEEE/CVF Conf. Comput. Vis. \nPattern Recognit., 2022, pp. 10853-10862. \n[50] D. Hendrycks, and K. Gimpel, “Gaussian error linear units (gelus),” arXiv \npreprint arXiv:1606.08415, 2016. \n[51] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for \nunsupervised visual representation learning,” in Proc. IEEE/CVF Conf. \nComput. Vis. Pattern Recognit., 2020, pp. 9729-9738. \n[52] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework \nfor contrastive learning of visual representations,” in Proc. Int. Conf. Mach. \nLearn., 2020, pp. 1597-1607. \n[53] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with \ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018. \n[54] A. Z. P. Gader, R. Close, J. Aitken, G. Tuell, “Muufl gulfport \nhyperspectral and lidar airborne data set,” University of Florida, \nGainesville, FL, Tech. Rep, REP-2013-570, 2013. \n[55] X. D. a. A. Zare, “Technical report: Scene label ground truth map for \nmuufl gulfport data set,” University of Florida, Gainesville, FL, Tech. Rep, \n20170417, 2017, Available: http://ufdc.ufl.edu/IR00009711/00001. \n[56] T. Cheng, and B. Wang, “Decomposition model with background \ndictionary learning for hyperspectral target detection,” IEEE J. Sel. Top. \nAppl. Earth Obs. Remote Sens., vol. 14, pp. 1872-1884, 2021. \n[57] Y. Li, Y. Shi, K. Wang, B. Xi, J. Li, and P. Gamba, “Target detection with \nunconstrained linear mixture model and hierarchical denoising \nautoencoder in hyperspectral imagery,” IEEE Trans. Image Process., vol. \n31, pp. 1418-1432, 2022. \n[58] C.-I. Chang, and J. Chen, “Orthogonal subspace projection using data \nsphering and low-rank and sparse matrix decomposition for hyperspectral \ntarget detection,” IEEE Trans. Geosci. Remote Sens., vol. 59, no. 10, pp. \n8704-8722, 2021. \n[59] C.-I. Chang, “An effective evaluation tool for hyperspectral target \ndetection: 3d receiver operating characteristic curve analysis,” IEEE Trans. \nGeosci. Remote Sens., vol. 59, no. 6, pp. 5131-5153, 2020. \n[60] L. Zhang, and B. Cheng, “Fractional fourier transform and transferred cnn \nbased on tensor for hyperspectral anomaly detection,” IEEE Geosci. \nRemote Sens. Lett., vol. 19, pp. 1-5, 2021. \n \n \nYulei Wang  (Member, IEEE) was born in Yantai, \nShandong Province, China in 1986. She received the \nB.S. and Ph.D. degrees in Signal and information \nProcessing from Harbin Engineering University, \nHarbin, China, in 2009 and 2015 respectively. She is \nawarded by China Scholarship Council  in 2011 as a \njoint Ph.D. student to study in Remote Sensing Signal \nand Image Processing Laboratory, University of \nMaryland, Baltimore County for two years.  \nShe is currently an associate professor and doctoral \nsupervisor in Hyperspectral Imaging in Remote Sensing (CHIRS), Information \nScience and Technology College, Dalian Maritime University. Her current \nresearch interests include hyperspectral image processing and vital signs signal \nprocessing. More details could be found at https://YuleiWang1.github.io/ . \n \nXi Chen  was born in Kuitun, Xinjiang Uygur \nAutonomous Region, China in 2000. He received the \nB.E. degree in electronic information engineering \nfrom Dalian Maritime University, Dalian, China, in \n2020, and  the M.S. degree in Information and \nCommunication Engineering, Information Science \nand Technology College, Dalian Maritime University, \nDalian, China in 2023. \nHis research interests include hyperspectral target \ndetection and deep learning. \n \n \nEnyu Zhao was born in Dalian, Liaoning Province, \nChina in 1987. He received the Ph.D. degree in \ncartography and geographic information system from \nthe College of Resources and Environment, University \nof Chinese Academy of Sciences, Beijing, China, in \n2017. He was a joint Ph.D. Student with Engineering \nScience, Computer Science and Imaging Laboratory, \nUniversity of Strasbourg, Strasbourg, France, from \n2014 to 2016. \nHe is currently an Associate Professor with the College \nof Information Science and Technology, Dalian \nMaritime University, Dalian, China. His research interests include quantitative \nremote sensing and hyperspectral image processing \n \nChunhui Zhao received the B.S. and M.S. degrees \nfrom Harbin Engineering University, Harbin, China, \nin 1986 and 1989, respectively, and the Ph.D. degree \nfrom the Department of Automatic Measure and  \nControl, Harbin Institute of Technology, Harbin,  in \n1998. He was a Post-Doctoral Research Fellow with \nthe College of Underwater Acoustical Engineering, \nHarbin Engineering University.  \nHe is currently  a Professor and a Doctoral \nSupervisor with the  College of Information and \nCommunication Engineering, Harbin Engineering University. His research \ninterests include digital signal and image processing, mathematical morphology \nand hyperspectral remote sensing image processing.  \nProf. Zhao is a Senior Member of Chinese Electronics Academy. \n \nMeiping Song  received her Ph. D degree in the \nCollege of Computer Science and Technology from \nHarbin Engineering University in 2006. From 2013 -\n2014, she was a visiting associate research scholar at \nRemote Sensing Signal and Image Processing \nLaboratory, University of M aryland, Baltimore \nCounty.  \nShe is currently an Associate Professor in the \nCollege of Information Science and Technology, \nDalian Maritime University. Her research includes \nremote sensing and hyperspectral image processing. \n  \nChunyan Yu received the B.S. and Ph.D. degrees in \nenvironment engineering from Dalian Maritime \nUniversity, Dalian, China, in 2004 and 2012, \nrespectively. In 2004, she joined the College of \nComputer Science and Technology, Dalian Maritime \nUniversity. From 2 013 to 2016, she was a Post -\nDoctoral Fellow with the Information Science and \nTechnology College, Dalian Maritime University. \nFrom 2014 to 2015, she was a Visiting Scholar with \nthe College of Physicians and Surgeons, Columbia \nUniversity, New York City, NY, USA.  \nShe is currently an associate professor  with the Information Science and \nTechnology College, Dalian Maritime University. Her research interests \ninclude image segmentation, hyperspectral image classification, and pattern \nrecognition. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3387985\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Hyperspectral imaging",
  "concepts": [
    {
      "name": "Hyperspectral imaging",
      "score": 0.8344016075134277
    },
    {
      "name": "Computer science",
      "score": 0.6348325610160828
    },
    {
      "name": "Transformer",
      "score": 0.5382264852523804
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5219359397888184
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.47027355432510376
    },
    {
      "name": "Physics",
      "score": 0.10913598537445068
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}