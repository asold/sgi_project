{
  "title": "A study of calibration as a measurement of trustworthiness of large language models in biomedical natural language processing",
  "url": "https://openalex.org/W4412353632",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2137626592",
      "name": "Rodrigo de Oliveira",
      "affiliations": [
        "IQ Solutions",
        "IQVIA (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2185794407",
      "name": "Matthew Garber",
      "affiliations": [
        "IQVIA (United Kingdom)",
        "IQ Solutions"
      ]
    },
    {
      "id": "https://openalex.org/A3014616456",
      "name": "James M. Gwinnutt",
      "affiliations": [
        "IQVIA (United Kingdom)",
        "IQ Solutions"
      ]
    },
    {
      "id": "https://openalex.org/A4309061155",
      "name": "Emaan Rashidi",
      "affiliations": [
        "IQ Solutions"
      ]
    },
    {
      "id": null,
      "name": "Jwu-Hsuan (Shantina) Hwang",
      "affiliations": [
        "IQ Solutions",
        "IQVIA (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2324494069",
      "name": "William Gilmour",
      "affiliations": [
        "IQVIA (United Kingdom)",
        "IQ Solutions"
      ]
    },
    {
      "id": "https://openalex.org/A2183318397",
      "name": "Jay Nanavati",
      "affiliations": [
        "IQ Solutions",
        "IQVIA (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2978177334",
      "name": "Khaldoun Zine El Abidine",
      "affiliations": [
        "IQVIA (United Kingdom)",
        "IQ Solutions"
      ]
    },
    {
      "id": "https://openalex.org/A2955439321",
      "name": "Christina DeFilippo Mack",
      "affiliations": [
        "IQVIA (United Kingdom)",
        "IQ Solutions"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2002514548",
    "https://openalex.org/W4409190866",
    "https://openalex.org/W4407844694",
    "https://openalex.org/W4401042444",
    "https://openalex.org/W4399423107",
    "https://openalex.org/W4397047623",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W4394782456",
    "https://openalex.org/W6636915900",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4406266037",
    "https://openalex.org/W2012942264",
    "https://openalex.org/W2098824882",
    "https://openalex.org/W4404782660",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W1647671624",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W4307079201"
  ],
  "abstract": "Abstract Objectives To assess the calibration of 9 large language models (LLMs) within biomedical natural language processing (BioNLP) tasks, furthering understanding of trustworthiness and reliability in real-world settings. Materials and Methods For each LLM, we collected responses and corresponding confidence scores for all 13 datasets (grouped into 6 tasks) of the Biomedical Language Understanding &amp; Reasoning Benchmark (BLURB). Confidence scores were assigned using 3 strategies: Verbal, Self-consistency, and Hybrid. For evaluation, we introduced Flex-ECE (Flexible Expected Calibration Error), a novel adaptation of ECE that accounts for partial correctness in model responses, allowing for a more realistic assessment of calibration in language-based settings. Two post-hoc calibration techniques—isotonic regression and histogram binning—were evaluated. Results Across tasks, mean calibration ranged from 23.9% (Population-Intervention-Comparison-Outcome extraction) to 46.6% (Relation Extraction). Across LLMs, Medicine-Llama3-8B had the best mean overall calibration (29.8%), and Flan-T5-XXL had the highest ranking on 5/13 datasets. Across strategies, Self-consistency (mean: 27.3%) had better calibration than Verbal (mean: 42.0%) and Hybrid (mean: 44.2%). Post-hoc methods substantially improved calibration, with best mean calibrated Flex-ECEs ranging from 0.1% to 4.1%. Discussion The poor out-of-the-box calibration of LLMs poses a risk to trustworthy deployment of such models in real-world BioNLP applications. Calibration can be improved post-hoc and is a recommended practice. Non-binary metrics for LLM evaluation such as Flex-ECE provide a more realistic assessment of trustworthiness of LLMs, and indeed any model that can be partially right/wrong. Conclusion This study shows that out-of-the-box calibration of LLMs is very poor, but traditional post-hoc calibration techniques are useful to calibrate LLMs.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7322501540184021
    },
    {
      "name": "Calibration",
      "score": 0.7306376099586487
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.5366033911705017
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4966495633125305
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.4780990779399872
    },
    {
      "name": "Natural language processing",
      "score": 0.4448586702346802
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4287530481815338
    },
    {
      "name": "Machine learning",
      "score": 0.41510745882987976
    },
    {
      "name": "Confidence interval",
      "score": 0.4137868881225586
    },
    {
      "name": "Statistics",
      "score": 0.31993985176086426
    },
    {
      "name": "Mathematics",
      "score": 0.09585621953010559
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}