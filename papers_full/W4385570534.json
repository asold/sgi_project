{
  "title": "DIP: Dead code Insertion based Black-box Attack for Programming Language Model",
  "url": "https://openalex.org/W4385570534",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3158674380",
      "name": "CheolWon Na",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126089117",
      "name": "Yun-Seok Choi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2639657712",
      "name": "Jee-Hyong Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2887364112",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W4382317573",
    "https://openalex.org/W2742956140",
    "https://openalex.org/W2964150020",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W3119507053",
    "https://openalex.org/W3158360872",
    "https://openalex.org/W4385572932",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W2158439356",
    "https://openalex.org/W4287869788",
    "https://openalex.org/W3173436362",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4205371973",
    "https://openalex.org/W3131641316",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W4367666333",
    "https://openalex.org/W2427333829",
    "https://openalex.org/W2115534035",
    "https://openalex.org/W2090432523",
    "https://openalex.org/W2997451752",
    "https://openalex.org/W4221166942",
    "https://openalex.org/W2972135640",
    "https://openalex.org/W4220722393",
    "https://openalex.org/W3101118235",
    "https://openalex.org/W2104301886",
    "https://openalex.org/W3169948074",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W4285177871",
    "https://openalex.org/W4287265163",
    "https://openalex.org/W3109966548",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3117433489"
  ],
  "abstract": "Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, GraphCodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation. Unlike natural language, an adversarial example of code must be semantic-preserving and compilable. Due to the requirements, it is hard to directly apply the existing attack methods for natural language models.In this paper, we propose DIP (Dead code Insertion based Black-box Attack for Programming Language Model), a high-performance and effective black-box attack method to generate adversarial examples using dead code insertion. We evaluate our proposed method on 9 victim downstream-task large code models. Our method outperforms the state-of-the-art black-box attack in both attack efficiency and attack quality, while generated adversarial examples are compiled preserving semantic functionality.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7777–7791\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDIP: Dead code Insertion based Black-box Attack for Programming\nLanguage Model\nCheolWon Na, YunSeok Choi, Jee-Hyong Lee†\nCollege of Computing and Informatics\nSungkyunkwan University\nSuwon, South Korea\n{ncw0034, ys.choi, john}@skku.edu\nAbstract\nAutomatic processing of source code, such as\ncode clone detection and software vulnerabil-\nity detection, is very helpful to software en-\ngineers. Large pre-trained Programming Lan-\nguage (PL) models (such as CodeBERT, Graph-\nCodeBERT, CodeT5,etc.), show very powerful\nperformance on these tasks. However, these\nPL models are vulnerable to adversarial exam-\nples that are generated with slight perturbation.\nUnlike natural language, an adversarial exam-\nple of code must be semantic-preserving and\ncompilable. Due to the requirements, it is hard\nto directly apply the existing attack methods\nfor natural language models. In this paper, we\npropose DIP (Dead code Insertion based Black-\nbox Attack for Programming Language Model),\na high-performance and efficient black-box at-\ntack method to generate adversarial examples\nusing dead code insertion. We evaluate our\nproposed method on 9 victim downstream-task\nlarge code models. Our method outperforms\nthe state-of-the-art black-box attack in both at-\ntack efficiency and attack quality, while gen-\nerated adversarial examples are compiled pre-\nserving semantic functionality.\n1 Introduction\nAutomatic processing of source code (such as clone\ndetection, code completion, defect detection, etc.)\nis an important task that increases the productiv-\nity of software engineers (Laguë et al., 1997; Li\net al., 2006; Mockus, 2007; Kapser and Godfrey,\n2008; Islam et al., 2016; Choi et al., 2021, 2023).\nFor these tasks, deep learning-based models, such\nas code2seq (Alon et al., 2019a) and code2vec\n(Alon et al., 2019b), were developed. Recently,\ntransformer-based large pre-trained programming\nlanguage (PL) models (Feng et al., 2020; Guo et al.,\n2021b; Wang et al., 2021; Ahmad et al., 2021; Ding\net al., 2021; Guo et al., 2022), showed powerful\n†Corresponding author.\nFigure 1: Dead code which is non-affecting to function-\nality is inserted into the original code in order to mislead\nthe target model.\nperformance. However, these PL models are also\nvulnerable to adversarial examples as other natural\nlanguage (NL) models (Choi et al., 2022; Zhang\net al., 2020; Yang et al., 2022; Jha and Reddy, 2022;\nJin et al., 2020; Li et al., 2020, 2021).\nSource code is textual data just like natural lan-\nguage, but the requirements of attack methods are\ndifferent from NL model attacks. Attacks against\nNL models should consider semantic and fluency\nof original sentences, but attacks against PL models\nshould preserve functionality and guarantee compi-\nlability. To consider for these issues, recent studies\n(Yefet et al., 2020; Yang et al., 2022; Srikant et al.,\n2021; Rabin et al., 2021; Zhang et al., 2020; Ra-\nmakrishnan et al., 2020) use semantic-preserving\ncode transformation which is commonly used in\nsoftware engineering. The semantic-preserving\ncode transformation includes Variable Renaming,\nDead Code Insertion, Statement Permutation, Loop\nExchange, Switch-to-If, and Boolean Exchange .\nMost of the prior works used Variable Renam-\ning or Dead Code Insertion to attack PL mod-\nels. Other transformations Statement-Permutation,\nLoop-Exchange, Switch-to-If,Boolean-Exchange,\netc, were less used for attacks because they are ap-\nplicable only to corresponding operations. Instead,\nthey were usually used for data augmentation (Jain\n7777\net al., 2020; Rabin et al., 2021). Variable Renam-\ning and Dead Code Insertion can be much stronger\nattacks because attackers can flexibly choose vari-\nous variable names or statements considering the\ncontextual information of code.\nIn the white-box setting, Yefet et al. (2020) pro-\nposed attack methods on code2vec, using Variable\nRenaming and Dead Code Insertion, which inserted\nperturbations sampled from the output distribution\ninto input one-hot vectors representing variables.\nRamakrishnan et al. (2020) and Srikant et al. (2021)\nproposed white-box attacks, optimizing position\nand perturbation of adversarial attacks, for basic PL\nmodels such as seq2seq, code2seq, and code2vec.\nHowever, those gradient-based white-box attack\nmethods are inefficient, time-consuming, and com-\nputationally expensive because large pre-trained\nPL models, such as CodeBERT (Feng et al., 2020),\nGraphCodeBERT (Guo et al., 2021b) and CodeT5\n(Wang et al., 2021), have a very large number of\nparameters. Also, it is hard to apply white-box at-\ntacks on large pre-trained models in the real world\nbecause the models are not fully opened and are\njust accessible through API. Therefore, the black-\nbox attack is more practical than the white-box for\nattacking large pre-trained models.\nIn the black-box setting, Zhang et al. (2020) pro-\nposed MHM, a sampling-based black-box attack\nmethod for programming language model using\nVariable Renaming. Yang et al. (2022) proposed\nALERT, a genetic algorithm based black-box at-\ntack method using Variable Renaming. However,\nthese attack methods using Variable Renaming are\ninefficient in attack success rate and in query count,\nbecause attack targets are limited to variables, and\nthe search space for candidate tokens is very large.\nAdditionally, these methods have a critical problem\nthat compilability is not guaranteed. In program-\nming language, unlike natural language, compil-\nability is more prior than naturalness, and must be\nguaranteed. There are some variables that should\nnot be renamed. For example, global variables or\nclass static variables in Java, such as System.out\nfor standard input/output, should not be changed.\nHowever, Variable Renamingcan replace such vari-\nables, and could raise a critical problem such as\ncompile errors.\nWe propose DIP (Dead code Insertion based\nBlack-box Attack for Programming Language\nModel), an efficient and the state-of-the-art adver-\nsarial attack method using Dead Code Insertion\nwith an unused variable statement. Under the\nblack-box setting, we do not need any information\nof a large pre-trained model, no additional compu-\ntation for training is required for attack.\nFor efficiency, we reduce the candidate search\nspace by inserting a statement from other code, not\ntokens. Randomly selected dissimilar codes are\nused to obtain statements to be inserted. We extract\nstatements from code using the attention score of a\npre-trained model, which is based on transformer\narchitectures. The snippet extracted by our method\nis powerful and transferable. In addition, our pro-\nposed method guarantee to preserve functionality\nand compilability in any case because the attack is\nbased on dead statement insertion.\nWe show that our proposed method outperforms\nthe existing methods to attack pre-trained program-\nming language models in Section 4. We choose\n3 pre-trained PL models, CodeBERT, GraphCode-\nBERT, and CodeT5, and 3 tasks, code clone detec-\ntion, defect detection, and authorship attribution\ntask. We build nine victim models in total. We\nalso conduct various experiments to explore our\nmethod. The ablation study (§4.4) examines the\neffectiveness of each component in DIP. In §4.5,\nwe test how strong DIP is on adversarially trained\nmodels. We also test whether adversarial samples\nused to attack a victim model can also fool another\nin §4.6.\n2 Task Definition\nWe evaluate DIP on code clone detection, defect de-\ntection, and authorship attribution tasks. For these\ntasks, we build the model by adding MLP classifier\nto the state-of-the-art pre-trained PL models (such\nas CodeBERT, GraphCodeBERT, and CodeT5).\nThen, we fine-tune the nine victim models.\nBlack-box setting. Since our proposed method\nis a black-box attack, we do not use any informa-\ntion from the victim models. To obtain the code\nrepresentation and attention score in DIP, we need\na pre-trained model based on Transformer archi-\ntectures (Vaswani et al., 2017). In this paper, we\nuse CodeBERT, a general pre-trained PL model\nwhich is not fine-tuned, to obtain code embeddings\nwithout any information from victim models.\nNon-targeted Adversarial attack. Adversarial\nattack can be usually categorized as either non-\ntargeted attack or targeted attack. Non-targeted\nadversarial attack is to slightly modify the source\n7778\nFigure 2: Overview of DIP.\ninput in a way that the input will be misclassified far\nfrom the ground truth. Targeted adversarial attack\nis to slightly modify the source input in a way that\nthe input will be misclassified as a specified target\nclass. Since our method is a non-targeted attack,\nthe goal is defined as follows.\nGoal. Given an input sample (ci, y), find cadv\ni , by\nadding perturbation to ci, that preserves functional-\nity and guarantees compilability, and that misleads\nthe victim model. The adversarial code,cadv\ni , needs\nto satisfy the followings:\nM(cadv\ni ) ≠y, (1)\nwhere cadv\ni = ci +perturbation, M is the victim\nmodel, and yis the ground truth.\n3 DIP\nWe proposeDIP, using dead code insertion to guar-\nantee compilability while preserving the semantics\nof source codes. Dead code insertion is very suit-\nable for source code attack because it does not\naffect functionality and compilability of code.\nOur method consists of three main steps as\nshown in Figure 2: (1) We find vulnerable posi-\ntions to insert dead code in the original source code.\nDead code will be inserted between statements in\nsource code as a statement to guarantee compilabil-\nity. We evaluate the vulnerability of each position\nin source code by a pre-trained code model which\nis not fine-tuned, and choose some of them as candi-\ndate vulnerable positions. (2) We obtain dissimilar\ncodes of which snippets will be used as dead code.\nDissimilarity of source code is defined based on\nthe cosine similarity to the original source code\nusing [CLS] token representations. (3) To minimize\nperturbation, we extract snippets from dissimilar\nAlgorithm 1: DIP Pseudo-code\nInput : Source code ci, true label y, target\nmodel M\nOutput :Adversarial Example cadv\ni\n1 Compute position importance Vp∀p∈ci\n2 Generate kcandidate dead code\n3 dead code list D = [d1,...,d k] ordered by\nthe dissimilarity\n4 for pin ascending order of Vp do\n5 cadv\ni ← ci\n6 for d∈D do\n7 cadv\ni ← insert dinto ci at p\n8 if M(cadv\ni ) ≠ythen\n9 # Success Attack\n10 Return: cadv\n11 end\n12 end\n13 end\n14 # Fail Attack\n15 Return: cadv ← None\nsource codes. Snippets are extracted by the atten-\ntion score in dissimilar source codes. We wrap\nthe obtained snippet with an unused variable\nstatement and insert it into the source code as dead\ncode. We summarize our proposed method, DIP, in\nAlgorithm 1.\n3.1 Vulnerable position selection\nFirst, we choose vulnerable positions to insert dead\ncode in the original source code for efficient attack.\nDead code will be generated as a statement. Since\ndead code can be inserted after any statement in\nthe original source code, the position is defined\nas line (or statement) numbers in the source code.\n7779\nAlgorithm 2: Dead Code Generation\nInput : Source code ci, true label y,\nattention layer of CodeBERT T\nOutput :dead code list D\n1 C ← {cn∣cn ∉train/testdata,n ≠i}\n2 Sample c1,...,c k from C\n3 C ← {c1,...,c k}\n4 # sort C using ScoreD in Section3.2\n5 for c∈C do\n6 # tokenize and get attention score of c\n7 S ← {attT (tokc\n1),...,att T (tokc\nn)}\n8 # get line index(start,end of line) list P\n9 P ← [(1,a),(a+1,b),..., (n+1,m)]\n10 α← 0\n11 for (s,e) ∈P do\n12 if α<max{S[s∶e]} then\n13 α← max{S[s∶e]}\n14 bestidx ← (s∶e)\n15 end\n16 snippetc ← c[bestidx]\n17 end\n18 d← stringvar =“snippetc”;\n19 # append dto D\n20 end\n21 D ← {d1,...,d k}\n22 Return: D\nTo find out vulnerable positions and sort them by\nvulnerability, we use [CLS] representations which is\nobtained from a pre-trained PL model which is not\nfine-tuned (such as CodeBERT). We insert a UNK\nsequence, u= [UNK, UNK, . . . ,UNK], into a position\nof the original source code to be attacked. Since\nwe attack to insert the code token sequence as a\nstatement in the original source code to be modified,\nwe could find a vulnerable position by inserting u.\nThe original source code will be denoted by c, and\nthe modified code by inserting uinto cat position\npwill be denoted by c′. We put cinto a pre-trained\nPL model to get the [ CLS] representation of c, rc.\nSimilarly, we put c′ to the pre-trained PL model to\nobtain rc′. The position score Vp of position pis\ndefined as follows:\nVp = rc ⋅rc′\n∣∣rc∣∣⋅∣∣rc′∣∣ (2)\nWe evaluate the position score of each position, and\nrank all the positions by position scores. We may\nchoose top-K vulnerable positions, or use all the\npositions. The lower the position score, the more\nvulnerable the position.\n3.2 Dissimilar code selection\nWe obtain dissimilar source codes from an open set\nof source code. To evaluate dissimilarity scores of\nsource code, we also use the cosine similarity of\n[CLS] tokens in a similar way to Equation 2. Let\nc and ci denote the source code to be modified\nand another source code not in train/test dataset,\nrespectively. We obtainrc and rci , [CLS] represen-\ntations of cand ci, from a pre-trained PL model,\nrespectively. Then, we use the cosine similarity to\ncalculate dissimilarity score, D, of ci against c. In\norder to reduce the search space and prevent biased\ncollection of dissimilar source code by some fac-\ntor, such as functionality of source code, we first\nrandomly select Ksource codes and sort them by\ndissimilarity score D.\n3.3 Snippet extraction\nWe extract a statement from each dissimilar code,\nand insert it to the source code to minimize pertur-\nbation instead of inserting the whole of dissimilar\ncode. In the dissimilar code, the statement with the\nhighest attention score is extracted. We obtain the\nattention score of each token in a statement from\nthe second-to-last attention layer of a pre-trained\nPL model. To obtain a general representation that\nis less biased in object functions of a pre-trained\nPL model, we use the representation of second-\nto-last attention layer. We use CodeBERT, which\nis not fine-tuned, as a pre-trained PL model. The\nhigher attention score is the more important in the\nsource code. Since the pre-trained PL model is\nbased on an attention mechanism, the higher atten-\ntion score indicates that the corresponding token is\nmore focused on the model. Then, we use the max\nvalue, which is the highest attention score in token\nsequence of statement, as score αin Algorithm 2\n(line 13). This process is described in Algorithm 2\n(lines 6-16).\n3.4 Adversarial code generation\nWe wrap the snippet obtained in §3.3 using a\nwrapper, an unused variable statement, to make\nit dead code. To consider the naturalness of the\nadversarial examples, we define an unused variable\nname as var_2 using the most used variable name\nvar in the code. For example, if the most used\nvariable name is str, then the unused variable\nname will be str_2. If there is a snippet extracted\nfrom dissimilar code, the dead statement generated\nwith a wrapper of unused variable is as follows:\n7780\nStringvar_2 =“snippet”;\nWe extract one snippet from each of Kdissimi-\nlar codes ordered by the dissimilarity score D. We\nrepeat inserting each dead statement into all posi-\ntions psorted by Vp, in the original source code\nuntil the attack succeeds. Our method can gener-\nate powerful and effective adversarial examples.\nAlgorithm 2 describes the process of adversarial\nexample generation by inserting dead code.\n4 Experiment\n4.1 Experimental setup\nDatasets and Tasks We evaluate DIP on code\nclone detection, defect detection, and authorship\nattribution tasks with 1000/1000/132 test samples,\nrespectively. Datasets of clone detection and defect\ndetection are included as a part of the CodeXGLUE\nbenchmarks (Lu et al., 2021). The dissimilar code\nset is obtained from the validation dataset in our ex-\nperiments. For a fair comparison with the baselines,\nwe use the same training dataset as the baselines to\nfine-tune victim models (Yang et al., 2022).\n• Clone Detection is to determine whether the\nfunctionalities of two given source codes are\nthe same or not. We use the BigCloneBench\n(Svajlenko and Roy, 2015) dataset, which is a\nwidely used benchmark in the clone detection\ntask. This dataset contains true clone pairs\nand false clone pairs from Java projects.\n• Defect Detection aims to find whether a given\nsource code is insecure or not. We use a\ndataset served by (Zhou et al., 2019). This\ndataset is extracted from large-scale open-\nsource C projects. This dataset includes\n27,318 functions.\n• Authorship Attribution task is to identify the\nauthor of a given source code. We use the\nGoogle Code Jam (GCJ) dataset, which is col-\nlected by Alsulami et al. (2017). This dataset\ncontains 660 python files (66 authors and 10\nfiles per author).\nTarget Models We analyze the vulnerability of\nthree popular and powerful pre-trained PL models:\nCodeBERT, GraphCodeBERT, and CodeT5. Code-\nBERT (Feng et al., 2020)’s objectives are masked\nlanguage modeling on NL-PL pairs and replaced\ntoken detection. GraphCodeBERT (Guo et al.,\n2021b) is pre-trained with code structure informa-\ntion as data flow. CodeT5 (Wang et al., 2021) is\nan encoder-decoder PL model, which is pre-trained\nby an identifier-aware objective. We build nine\nvictim models by fine-tuning these models in three\ntasks. More details of victim models are listed in\nAppendix A.\nBaselines We compare with the state-of-the-art\nblack-box attack methods on PL models, MHM\n(Zhang et al., 2020) and ALERT (Yang et al.,\n2022). They used Variable Renaming transforma-\ntions to attack PL models. MHM was based on\nthe metropolis-hastings sampling method. ALERT\nused genetic algorithm with a masked language\nmodel. We conduct the experiments in the same\nenvironment as the baselines. CodeBERT and\nGraphCodeBERT results of baselines are referred\nfrom served by Yang et al. (2022) To evaluate on\nCodeT5, we implement MHM and ALERT.\nHyperparameter Our method has two hyperpa-\nrameters: M and K. M is the number of [UNK] to-\nkens to evaluate vulnerable position scores, and K\nis the number of randomly chosen dissimilar source\ncodes. Since the average length of statements in\nsource code of the test dataset is 9.7, the number of\n[UNK] tokens to be inserted is set to M=10, which\nis slightly longer than the average to make enough\ndifference in CLS vectors. We set K=30. If K is\nsmall, the attack success rate will be low, and if it\nis large, the attack will be inefficient. We choose a\nnumber small but large enough to include various\nsource code. For comparison with the baselines,\nwe use the same maximal input length to 512.\n4.2 Metrics\nWe use four metrics to evaluate our method. To\nmeasure the efficiency of the generated adversarial\ncode, we use ASRand Query. We also use Pert\nand CodeBLEU to measure quality of the gener-\nated examples. We define the following metrics.\nAttack Success Rate (ASR) ASRis the success\nratio of attacks. The higher ASR, the better per-\nformance of an attack method. Let Cadv denote\na generated adversarial example from the original\ninput C, M is the target model, and y is the true\nlabel. Then ASRisndefined as follows:\nASR= ∣{C∣M(Cadv) ≠y∧M(C) =y}∣\n∣{C}∣ (3)\n7781\nTask Victim Attack Attack efficiency Attack quality\n(Language) Model Method ASR Query Pert CodeBLEU\nClone Detection\n(Java)\nCodeBERT\nMHM 20.2 667.7 0.32 0.56\nALERT 28.6 529.4 0.13 0.73\nDIP (ours) 46.7 19.9 0.13 0.92\nGraphCodeBERT\nMHM 4.2 1025.9 0.36 0.32\nALERT 9.2 448.6 0.13 0.72\nDIP (ours) 36.6 78.2 0.14 0.85\nCodeT5\nMHM 4.6 104.5 0.26 0.42\nALERT 22.0 762.2 0.14 0.73\nDIP (ours) 31.8 38.2 0.11 0.93\nDefect Detection\n(C/C++)\nCodeBERT\nMHM 27.4 451.9 0.33 0.32\nALERT 31.4 277.6 0.11 0.76\nDIP★ 44.6 47.6 0.19 0.91\nGraphCodeBERT\nMHM 41.3 316.7 0.33 0.31\nALERT 46.7 263.6 0.10 0.76\nDIP (ours) 49.7 71.0 0.13 0.79\nCodeT5\nMHM 49.3 333.5 0.10 0.78\nALERT 46.9 187.4 0.08 0.80\nDIP (ours) 49.7 61.0 0.16 0.92\nAuthorship Attribution\n(Python)\nCodeBERT\nMHM 15.9 444.0 0.13 0.78\nALERT 29.6 545.4 0.13 0.79\nDIP (ours) 31.1 300.15 0.09 0.85\nGraphCodeBERT\nMHM 26.5 774.9 0.30 0.49\nALERT 50.8 573.2 0.15 0.75\nDIP (ours) 61.4 292.6 0.08 0.81\nCodeT5\nMHM 36.4 684.6 0.16 0.78\nALERT 41.7 373.4 0.10 0.83\nDIP (ours) 43.9 47.2 0.07 0.92\nAll Average\nMHM 25.1 537.7 0.25 0.53\nALERT 34.1 440.1 0.12 0.76\nDIP (ours) 43.9 106.2 0.12 0.88\nTable 1: Comparison of our proposed method with the baseline methods on nine victim models. We set all of the\nhyper-parameters (α,β,γ, and δ) in CodeBLEU to 0.25, respectively. The best performance is in boldface, and the\nnext is underlined.\nNumber of Queries (Query) Query is the av-\nerage query number of successful attacks. Our\nmethod is a black-box approach, so queries are\nthe only accessible way to the target model. The\nnumber of queries is one of important metrics to\nevaluate efficiency of attack methods. Let qi de-\nnote the number of queries for i-th succeed attack,\nQueryis defined as follows:\nQuery= ∑qi\n∑f(i) (4)\nwhere i∈{j∣f(j) =1}, and\nf(j) =\n⎧⎪⎪⎨⎪⎪⎩\n1, if M(Cadv\nj ) ≠yj ∧M(Cj) =yj\n0, otherwise\nRatio of Perturbation (Pert) The ratio of per-\nturbation indicates how many perturbations are in-\njected into the original source code. A lower Pert\nindicates that examples are generated with less per-\nturbation. Let Cadv\ni is an adversarial example ofCi\nof which the truth label isyi, and t(⋅) is the number\nof tokens. Pert is defined as follows:\nPert = ∑t(Cadv\ni )−t(Cadv\ni ∩Ci)\n∑t(Ci) (5)\nwhere iare defined same as in Eq. 4.\nCodeBLEU Ren et al. (2020) proposed\nCodeBLEU to measure generated code by\nmachine learning models. CodeBLEU considers\nfunctional and structural information of code such\nas AST match and Data-flow match. CodeBLEU\nis a more efficient metric than BLEU to measure\nthe consistency of generated code. If CodeBLEU\nis close to 1, the code preserves the semantic\nmeaning of the original code.\nCodeBLEU is defined as follows:\nCodeBLEU =α⋅BLEU +β⋅BLEUweight\n+γ⋅Matchast +δ⋅Matchd f (6)\n7782\nMethod ASR Pert Query\nDIP 46.7 0.13 19.9\nw/o Dissim 45.2±0.9 0.14 ±0.01 26.0 ±5.1\nw/o Position 46.7±0.0 0.14 ±0.00 29.6 ±5.0\nw/o Att-line 47.0±0.5 0.12 ±0.00 110.3 ±6.1\nTable 2: Ablation study on DIP. “w/o Dissim” denotes\nrandom selection of dissimilar codes without dissimilar\nscores in Eq. 2, “w/o Position” denotes random position\ninsertion without vulnerable position score, and “ w/o\nAtt-line” denotes random extraction of statements as\nsnippets from dissimilar code without attention score.\nThe mean and variance of the five runs are presented.\nIn certain cases, identical results are observed across all\nfive runs, resulting in a standard variance of zero.\nwhere BLEUweight is the weighted keyword n-\ngram match. Matchast and Matchd fare scores\nconsidering tree structures of AST and data-flow\nof code, respectively. α,β,γ, and δ are hyper-\nparameters.\n4.3 Overall Results\nWe perform experiments with 9 victim models\nfor 3 tasks fine-tuned from 3 pre-trained PL mod-\nels. Baselines are MHM (Zhang et al., 2020) and\nALERT (Yang et al., 2022) which are the state-of-\nthe-art baselines in black-box attacks on the PL\nmodel. Experiments demonstrate that DIP outper-\nforms the baselines in all metrics ( ASR, Query,\nPert, and CodeBLEU) as shown in Table 1. Our\nproposed method, DIP, shows the best ASR and\nQueryon all victim models, which means that DIP\nmore effectively attacks the victim models than the\nbaselines. Compared to the baseline MHM, DIP im-\nproves the averageASRby 74.9%, and the average\nQueryby 431.5% which is 5 times more efficient.\nCompared with the strong baseline ALERT, DIP\nalso improves the average ASR by 28.7%, and\nthe average Query by 333.9% which is 4 times\nmore efficient. Pert and CodeBLEU measure\nthe quality of generated code. DIP shows the best\nCodeBLEU on all the victim models, and the best\nPert on most victim models, which indicates more\nsemantic-preserving (as functionality). Since vari-\nable names play important roles to represent code\nsemantics (Wang et al., 2021), code semantics may\nbe easily broken by variable renaming-based meth-\nods.\n4.4 Ablation Study\nWe perform an ablation study with each component\nof DIP to verify the effectiveness of dissimilar code\nMethod MHM ALERT DIP All\nMHM 2.5 1.5 9.2 1.0\nALERT 5.5 4.5 28.1 4.5\nDIP (ours) 32.0 41.5 15.0 8.0\nTable 3: ASR on adversarially trained models. MHM,\nALERT, and DIP are the models adversarially trained\nby MHM, ALERT, and DIP, respectively.Allis adver-\nsarially trained with all the three attack methods.\nselection, vulnerable position selection, and snip-\npet extraction. We evaluate with the CodeBERT\nfine-tuned for the clone detection. Table 2 shows\nthe results of the ablation study on DIP. We replace\nthe code selection by dissimilarity score (Eq. 2)\nwith random selection in DIP w/o Dissim, the posi-\ntion selection by vulnerability score with random\nselection in DIP w/o Position, and the snippet ex-\ntraction by attention score with random extraction\nin DIP w/o Att-line. As shown in Table 2, all the\nablated DIPs show lower ASR, and higher Pert\nand Query than the original DIP. We verify that\nall the components are very effective for PL model\nattack. DIP w/o Dissim randomly selects the code\nwhich a snippet is extracted from, resulting in de-\ncrease of ASRand increase of Pert. It shows that\nour dissimilar code selection effects much on the\nattack quality. DIP w/o Position and DIP w/o Att-\nline show similar attack qualities (ASRand Pert),\nbut lower attack efficiencies (Query). DIP w/o Att-\nline shows Queryabout 6 times higher. Our posi-\ntion selection and snippet extraction methods play\nimportant roles to increase the attack efficiency.\nMore various snippet extractions are evaluated in\nAppendix C.\n4.5 Attack on Adversarially Trained Models\nWe test the attack performance of MHM, ALERT,\nand DIP against adversarially trained models. We\nconduct experiments with the CodeBERT fine-\ntuned for clone detection. In Table 3, MHM,\nALERT, and DIP denote the models trained with\nadversarial examples generated by MHM, ALERT,\nand DIP, respectively. All denotes an adversar-\nially trained model by all three attack methods.\nZhang et al. (2020) and Yang et al. (2022) reported\ntheir adversarial examples improving the robust-\nness of target models. However, as shown in Table\n3, DIP successfully attacks the adversarially trained\nmodels by MHM and ALERT. Adversarial train-\ning should improve the robustness of the model,\nbut MHM and ALERT are not enough to increase\n7783\nAdversarial Code- G. Code- Code-\nExamples BERT BERT T5\nagainst CodeBERT - 31.3 22.0\nagainst G.CodeBERT 12.6 - 17.8\nagainst CodeT5 24.8 29.3 -\nTable 4: Transferability of our proposed method. We use\nASRas evaluate metric. Columns are the tested victim\nmodels, and rows are adversarial examples against each\nmodel.\nrobustness. If we combine all the three methods,\nthe model becomes hard to attack. DIP shows the\nhighest ASR, but it is relatively low. Adversarial\ntraining with combination of variable renaming and\ndead code insertion may improve the robustness of\nmodels.\n4.6 Transferability\nIn this section, we test the transferability of our\nadversarial examples. We obtain adversarial ex-\namples of successful attacks on victim models to\nattack other models. As shown in Table 4, adversar-\nial examples by DIP are transferable in the clone\ndetection task. This experiment showed the poten-\ntial of DIP in the other tasks.\n5 Discussion\nWe discuss the compliability and detectactibility\nfor the attack on PL models.\n5.1 Complilability\nWhen we modify the source code to attack PL mod-\nels, we should consider compilability. If a source\ncode is modified, it must be guaranteed to be com-\npiled. DIP is guaranteed to compile in any case\nunlike MHM and ALERT, which are variable re-\nnaming methods. As mentioned in §1, we can eas-\nily find uncompilable cases modified by ALERT.\nIn Appendix E, we present an example.\n5.2 Dead Code Detection\nSince we use an unused string variable, it could be\nfiltered out by a dead code detector. However, it\nwould be very hard to detect dead code by the static\nanalysis because it is theoretically equivalent to the\nhalting problem. We may simply eliminate code\nwhich is not locally accessed in a function. How-\never, the dead code may include global variables.\nSimple elimination of statements including global\nvariables may cause critical problems. If we add\nunused variables as if they are global, it would be\nvery hard to detect them with a high certainty. For\nthis reason, there are very few tools for dead code\nelimination. Most existing dead code eliminator\ntools, such as UCDetector and J2ObjC, they can\ndetect only unused classes or method units, but not\ndetect unused variables inside methods.\n6 Related Work\nThe adversarial attack for language data (such as\ntext, source code) is significantly difficult due to\nthe discrete property of token embedding space.\nBoth text and source code have a discrete prop-\nerty, but there is a difference in the attack methods.\nIn the adversarial attack for natural language, the\ntext should be considered for fluency and seman-\ntic consistency. The previous works (Maheshwary\net al., 2020; Jin et al., 2020; Li et al., 2020; Garg\nand Ramakrishnan, 2020; Li et al., 2021; Wang\net al., 2020; Guo et al., 2021a) proposed adversar-\nial attack methods, which considered semantic and\nfluency of sentence. However, the source code is\nmore important in compilability than fluency. In\nthe adversarial attack for programming language,\nthe source code should be considered for semantic\n(functionality) preserving and must be compiled.\nThe previous works, Rabin et al. (2021) evaluated\nthe generalizability of code2vec, code2seq, GGNN\nby using the various transformation (Permute State-\nment, Boolean Exchange and Loop Exchange, etc.).\nZhang et al. (2020) proposed MHM, in black-box\nsetting, which is the Variable Renaming based on\nmetropolis-hastings sampling method. Yang et al.\n(2022) proposed ALERT, to improve MHM, which\nuse the Variable Renaming based on genetic al-\ngorithm. Ramakrishnan et al. (2020); Yefet et al.\n(2020) proposed white-box attack method using\nthe AST transformation and the output distribution\ninto one-hot vector, which is represented variable\nname. Srikant et al. (2021) proposed white-box\nattack method to optimize the attack position and\nperturbation at the same time.\n7 Conclusion\nIn this paper, we presented DIP (Dead code\nInsertion based Black-box Attack forProgramming\nLanguage Model), the state-of-the-art black-box\nattack method. We proposed an attack method\nconsist of vulnerable position selection, dissimilar\ncode selection and snippet extraction. Experiment\nresults demonstrate the high-performance and ef-\nfectiveness of DIP, also we evaluated the transfer-\n7784\nability to apply other models. Under the black-box\nsetting, our proposed method guaranteed compil-\nability in any case and preserved functionality.\nLimitation\nWe discuss some limitations of our study. Our pro-\nposed method needs a pre-trained model to obtain\nthe attention score mentioned in Section 3.3. While\nour method achieved successful attacks with 4∼5\ntimes fewer queries compared to the baselines, the\ntime spent on the attack was approximately half\nof the baselines. This is because preprocessing is\nnecessary to calculate the score V in Section 3.1\nand Din Section 3.2.\nAcknowledgements\nThis work was supported by Institute of Informa-\ntion & communications Technology Planning &\nEvaluation (IITP) grant funded by the Korea gov-\nernment (MSIT) (No.2019-0-00421, AI Graduate\nSchool Support Program (Sungkyunkwan Univer-\nsity), and No.2021-0-02068, Artificial Intelligence\nInnovation Hub). This work was also supported\nby Healthcare AI Convergence Research & Devel-\nopment Program through the National IT Industry\nPromotion Agency of Korea (NIPA) funded by the\nMinistry of Science and ICT (No. S0254-22-1006).\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\nfor program understanding and generation. ArXiv,\nabs/2103.06333.\nUri Alon, Shaked Brody, Omer Levy, and Eran Yahav.\n2019a. code2seq: Generating sequences from struc-\ntured representations of code. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nUri Alon, Meital Zilberstein, Omer Levy, and Eran\nYahav. 2019b. code2vec: learning distributed rep-\nresentations of code. Proc. ACM Program. Lang.,\n3(POPL):40:1–40:29.\nBander Alsulami, Edwin Dauber, Richard E. Harang,\nSpiros Mancoridis, and Rachel Greenstadt. 2017.\nSource code authorship attribution using long short-\nterm memory based networks. In European Sympo-\nsium on Research in Computer Security.\nYunSeok Choi, JinYeong Bak, CheolWon Na, and Jee-\nHyong Lee. 2021. Learning sequential and structural\ninformation for source code summarization. In Find-\nings of the Association for Computational Linguistics:\nACL-IJCNLP 2021, pages 2842–2851.\nYunSeok Choi, Hyojun Kim, and Jee-Hyong Lee. 2022.\nTABS: Efficient textual adversarial attack for pre-\ntrained NL code model using semantic beam search.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5490–5498, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nYunSeok Choi, CheolWon Na, Hyojun Kim, and Jee-\nHyong Lee. 2023. Readsum: Retrieval-augmented\nadaptive transformer for source code summarization.\nIEEE Access.\nYangruibo Ding, Luca Buratti, Saurabh Pujar, Alessan-\ndro Morari, Baishakhi Ray, and Saikat Chakraborty.\n2021. Towards learning (dis)-similarity of source\ncode from program contrasts. In Annual Meeting of\nthe Association for Computational Linguistics.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nbert: A pre-trained model for programming and nat-\nural languages. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020, volume EMNLP 2020\nof Findings of ACL, pages 1536–1547. Association\nfor Computational Linguistics.\nSiddhant Garg and Goutham Ramakrishnan. 2020.\nBAE: bert-based adversarial examples for text classi-\nfication. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2020, Online, November 16-20, 2020, pages\n6174–6181. Association for Computational Linguis-\ntics.\nChuan Guo, Alexandre Sablayrolles, Hervé Jégou, and\nDouwe Kiela. 2021a. Gradient-based adversarial at-\ntacks against text transformers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November,\n2021, pages 5747–5757. Association for Computa-\ntional Linguistics.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. Unixcoder: Unified cross-\nmodal pre-training for code representation. In An-\nnual Meeting of the Association for Computational\nLinguistics.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu\nTang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-\natkovskiy, Shengyu Fu, Michele Tufano, Shao Kun\nDeng, Colin B. Clement, Dawn Drain, Neel Sundare-\nsan, Jian Yin, Daxin Jiang, and Ming Zhou. 2021b.\nGraphcodebert: Pre-training code representations\nwith data flow. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\n7785\nJudith F Islam, Manishankar Mondal, and Chanchal K\nRoy. 2016. Bug replication in code clones: An em-\npirical study. In 2016 IEEE 23Rd international con-\nference on software analysis, evolution, and reengi-\nneering (SANER), volume 1, pages 68–78. IEEE.\nParas Jain, Ajay Jain, Tianjun Zhang, P. Abbeel, Joseph\nGonzalez, and Ion Stoica. 2020. Contrastive code\nrepresentation learning. In Conference on Empirical\nMethods in Natural Language Processing.\nAkshita Jha and Chandan K. Reddy. 2022. Codeattack:\nCode-based adversarial attacks for pre-trained pro-\ngramming language models. ArXiv, abs/2206.00052.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is BERT really robust? A strong\nbaseline for natural language attack on text classifi-\ncation and entailment. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 8018–8025. AAAI Press.\nCory J Kapser and Michael W Godfrey. 2008. “cloning\nconsidered harmful” considered harmful: patterns of\ncloning in software. Empirical Software Engineering,\n13(6):645–692.\nBruno Laguë, Daniel Proulx, Jean Mayrand, Ettore M\nMerlo, and John Hudepohl. 1997. Assessing the ben-\nefits of incorporating function clone detection in a\ndevelopment process. In 1997 Proceedings Interna-\ntional Conference on Software Maintenance, pages\n314–321. IEEE.\nDianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris\nBrockett, Ming-Ting Sun, and Bill Dolan. 2021. Con-\ntextualized perturbation for textual adversarial attack.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n5053–5069. Association for Computational Linguis-\ntics.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,\nand Xipeng Qiu. 2020. BERT-ATTACK: adversarial\nattack against BERT using BERT. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 6193–6202. Associa-\ntion for Computational Linguistics.\nZhenmin Li, Shan Lu, Suvda Myagmar, and Yuanyuan\nZhou. 2006. Cp-miner: Finding copy-paste and re-\nlated bugs in large-scale software code. IEEE Trans-\nactions on software Engineering, 32(3):176–192.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin B. Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong\nZhou, Linjun Shou, Long Zhou, Michele Tufano,\nMing Gong, Ming Zhou, Nan Duan, Neel Sundare-\nsan, Shao Kun Deng, Shengyu Fu, and Shujie Liu.\n2021. Codexglue: A machine learning benchmark\ndataset for code understanding and generation.ArXiv,\nabs/2102.04664.\nRishabh Maheshwary, Saket Maheshwary, and Vikram\nPudi. 2020. Generating natural language attacks in a\nhard label black box setting.\nAudris Mockus. 2007. Large-scale code reuse in open\nsource software. In First International Workshop on\nEmerging Trends in FLOSS Research and Develop-\nment (FLOSS’07: ICSE Workshops 2007), pages 7–7.\nIEEE.\nMd Rafiqul Islam Rabin, Nghi DQ Bui, Ke Wang, Yijun\nYu, Lingxiao Jiang, and Mohammad Amin Alipour.\n2021. On the generalizability of neural program\nmodels with respect to semantic-preserving program\ntransformations. Information and Software Technol-\nogy, 135:106552.\nGoutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws\nAlbarghouthi, Somesh Jha, and Thomas W. Reps.\n2020. Semantic robustness of models of source code.\nCoRR, abs/2002.03043.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie\nLiu, Duyu Tang, M. Zhou, Ambrosio Blanco, and\nShuai Ma. 2020. Codebleu: a method for automatic\nevaluation of code synthesis. ArXiv, abs/2009.10297.\nShashank Srikant, Sijia Liu, Tamara Mitrovska, Shiyu\nChang, Quanfu Fan, Gaoyuan Zhang, and Una-May\nO’Reilly. 2021. Generating adversarial computer\nprograms using optimized obfuscations. In 9th In-\nternational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nJeffrey Svajlenko and Chanchal K. Roy. 2015. Evalu-\nating clone detection tools with bigclonebench. In\n2015 IEEE International Conference on Software\nMaintenance and Evolution, ICSME 2015, Bremen,\nGermany, September 29 - October 1, 2015 , pages\n131–140. IEEE Computer Society.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nTianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang\nLi, Jilin Chen, Alex Beutel, and Ed H. Chi. 2020.\nCat-gen: Improving robustness in NLP models via\ncontrolled adversarial text generation. In Proceed-\nings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 5141–5146. Associa-\ntion for Computational Linguistics.\nYue Wang, Weishi Wang, Shafiq R. Joty, and Steven\nC. H. Hoi. 2021. Codet5: Identifier-aware unified\npre-trained encoder-decoder models for code under-\nstanding and generation. ArXiv, abs/2109.00859.\n7786\nZhou Yang, Jieke Shi, Junda He, and David Lo. 2022.\nNatural attack for pre-trained models of code. CoRR,\nabs/2201.08698.\nNoam Yefet, Uri Alon, and Eran Yahav. 2020. Ad-\nversarial examples for models of code. Proc. ACM\nProgram. Lang., 4(OOPSLA).\nHuangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu,\nand Zhi Jin. 2020. Generating adversarial exam-\nples for holding robustness of source code processing\nmodels. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 1169–1176.\nYaqin Zhou, Shangqing Liu, J. Siow, Xiaoning Du,\nand Yang Liu. 2019. Devign: Effective vulnera-\nbility identification by learning comprehensive pro-\ngram semantics via graph neural networks. ArXiv,\nabs/1909.03496.\n7787\nA Statistics of Victim Models\nTask Model Acc.\nClone Detection\n(# of Classes: 2)\nCodeBERT 97.3%\nGraphCodeBERT 97.8%\nCodeT5 97.1%\nDefect Detection\n(# of Classes: 2)\nCodeBERT 63.3%\nGraphCodeBERT 64.2%\nCodeT5 63.7%\nAuthorship Attribution\n(# of Classes: 66)\nCodeBERT 82.6%\nGraphCodeBERT 81.1%\nCodeT5 85.6%\nTable 5: Statistics of Victim Models\nB Length of dissimilar code\nThe Kis the key hyperparameter as the number of\ncandidate dissimilar codes. Figure 3 shows perfor-\nmance by Kon the DIP. As seen in Figure 3 , the\nASRand Queryincrease with Kincreasing. We\nconfirmed, increasing Kfrom 1 to 30, the ASRin-\ncreases significantly but hardly increases after that.\nWhen Kis increasing, ASRincreases because the\nsearch space is larger for attacking to victim model.\nHowever, when Kis over 30, the ASRper Query\ngets lower. It is inefficient in terms of Query.\n1 10 30 50 100\nK \n0\n10\n20\n30\n40\n50\n60\n70\n80\n90ASR & Qeury\nASR\nQeury\nA/Q\nFigure 3: Using different parameter Kfrom 1 to 100 in\nour attack method. The ASR and A/Qincrease until\nKto 30\nC Snippet Extraction\nWe compare various snippet extraction methods to\ngenerate dead statements as shown in Table 6. We\nanalyze with DIP to observe the effects of snippet\nextraction method. The types of additional snippet\nare 4, which include the following:\n• [RandSeq-N] is to randomly extract a sequence of N\ntokens from dissimilar code. The sequence can start at\nany token of dissimilar code.\n• [Rand-N]is to randomly extract N tokens (not sequen-\ntially) from dissimilar code. Each token is extracted\nindependently.\nSnippet ASR% Pert Query\nMethod: DIP\nRandSeq-10 42.7 0.09 131.6\nRand-10 21.1 0.09 154.4\nRandSeq-5 23.4 0.06 180.2\nRand-5 11.9 0.06 238.1\nRandSeq-5% 38.8 0.08 104.4\nNatural language 0.1 0.40 1.00\nTable 6: Performance by various snippet extractions.\n• [RandSeq-N%] is same as RandSeq-N except that the\nlength of the extracted sequence is proportional to the\nlength of dissimilar code. Its length is N% of dissimilar\ncode length.\n• [Natural language] is to insert text in natural lan-\nguage not in programming language. We insert “I want\nto fool this model. Hello, World! Python is a very simple\nand powerful language, and has a very straightforward\nsyntax”.\nRandSeq-Nperforms better than Rand-N. Since\nthe model learns sequential information of source\ncode during pre-training, consecutively extracted\ntokens are more effective then independently ex-\ntracted ones. If we compare RandSeq-N% to\nRandSeq-N, it is better than RandSeq-N. We may\nconclude that it is effective to extract one whole\nstatement as a snippet. It is interesting that a state-\nment in natural language does not have any effect\non adversarial attack. Since statements in natu-\nral language is very common in string variables,\nmodels may know how to handle it.\nD Runtime Comparison\nOur proposed method is very efficient, and less\ntime-consuming. We reduce the search space by in-\nserting a dead statement, not tokens, and obtain dis-\nsimilar codes after random sampling from code sets.\nThe baseline methods try to rename variables with\na large number of token combinations. Thus their\nmethods are inefficient and high time-consuming.\nWhen attacking the PL models, DIP is about 3\ntimes faster than MHM, and 2 times faster than\nALERT.\n7788\nE Qualitative Example\nOriginal Code private static void readAndRewrite(File inFile, File outFile) throws IOException {\n// Some code . . .\nImageOutputStream out = ImageIO.create. . .Stream(new Buffere. . .Stream(new File..Stream(outFile)));\nds.writeDataset(out, dcmEncParam);\nds.writeHeader(out, dcmEncParam, Tags.PixelData, dcmParser.get. . .VR(), dcmParser.get. . .Length());\nSystem.out.println(\"writing \" + outFile + \". . .\");\n// Some code . . .\nALERT private static void readAndRewrite(File inFile, File outFile) throws IOException {\n// Some code . . .\nImageOutputStream url = ImageIO.create. . .Stream(new Buffere. . .Stream(new File..Stream(outFile)));\nds.writeDataset(url, dcmEncParam);\nds.writeHeader(url, dcmEncParam, Tags.PixelData, dcmParser.get..VR(), dcmParser.get. . .Length());\nSystem.url.println(\"writing \" + outFile + \". . .\");\n// Some code . . .\nDIP(our) private static void readAndRewrite(File inFile, File outFile) throws IOException {\n// Some code . . .\nImageOutputStream out = ImageIO.create. . .Stream(new Buffere. . .Stream(new File..Stream(outFile)));\nString out_2 =“r = new BufferedReader(new InputStreamReader(url.openStream()))”;\nds.writeDataset(out, dcmEncParam);\nds.writeHeader(out, dcmEncParam, Tags.PixelData, dcmParser.get..VR(), dcmParser.get. . .Length());\nSystem.out.println(\"writing \" + outFile + \". . .\");\n// Some code . . .\nTable 7: The example for the qualitative comparison. The first example is Original Code. The second/third are\ngenerated adversarial examples by ALERT, DIP, respectively. The example of ALERT is not compiled because\nthe variable out, which should not be changed, has been changed to url. In the example of DIP, red color is\nwrapper(unused string variable) and orange color is high attention snippet. The example of DIP is compiled.\n7789\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n\"Limitation\" section after the 7. Conclusions section\n□\u0013 A2. Did you discuss any potential risks of your work?\n5.2\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0017 Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNo response.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7790\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNo response.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNo response.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNo response.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n7791",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.834234893321991
    },
    {
      "name": "Black box",
      "score": 0.6349008083343506
    },
    {
      "name": "Programming language",
      "score": 0.5508638620376587
    },
    {
      "name": "Code (set theory)",
      "score": 0.5420618057250977
    },
    {
      "name": "Source code",
      "score": 0.5171175003051758
    },
    {
      "name": "Adversarial system",
      "score": 0.4515966773033142
    },
    {
      "name": "Vulnerability (computing)",
      "score": 0.4457092881202698
    },
    {
      "name": "Software",
      "score": 0.4432271420955658
    },
    {
      "name": "Artificial intelligence",
      "score": 0.26058363914489746
    },
    {
      "name": "Computer security",
      "score": 0.21987798810005188
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ]
}