{
  "title": "Adaptive language model training for molecular design",
  "url": "https://openalex.org/W4379875520",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2030487054",
      "name": "Andrew E. Blanchard",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1988289531",
      "name": "Debsindhu Bhowmik",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2049633499",
      "name": "Zachary Fox",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2734772578",
      "name": "John Gounley",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2114151169",
      "name": "Jens Gläser",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1844685798",
      "name": "Belinda S Akpa",
      "affiliations": [
        "Oak Ridge National Laboratory",
        "University of Tennessee at Knoxville"
      ]
    },
    {
      "id": "https://openalex.org/A1815861180",
      "name": "Stephan Irle",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2030487054",
      "name": "Andrew E. Blanchard",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1988289531",
      "name": "Debsindhu Bhowmik",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2049633499",
      "name": "Zachary Fox",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2734772578",
      "name": "John Gounley",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2114151169",
      "name": "Jens Gläser",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1844685798",
      "name": "Belinda S Akpa",
      "affiliations": [
        "Oak Ridge National Laboratory",
        "University of Tennessee at Knoxville"
      ]
    },
    {
      "id": "https://openalex.org/A1815861180",
      "name": "Stephan Irle",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3008443627",
    "https://openalex.org/W4303478269",
    "https://openalex.org/W3014476516",
    "https://openalex.org/W2790808809",
    "https://openalex.org/W3113182646",
    "https://openalex.org/W3167905129",
    "https://openalex.org/W4205410305",
    "https://openalex.org/W2953302413",
    "https://openalex.org/W2959938226",
    "https://openalex.org/W2406943157",
    "https://openalex.org/W2461470610",
    "https://openalex.org/W3158755582",
    "https://openalex.org/W3133325765",
    "https://openalex.org/W2774185825",
    "https://openalex.org/W2110791536",
    "https://openalex.org/W2914542247",
    "https://openalex.org/W2107160601",
    "https://openalex.org/W2900694120",
    "https://openalex.org/W1998693213",
    "https://openalex.org/W2027478081",
    "https://openalex.org/W2066810295",
    "https://openalex.org/W1592238003",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2907657781",
    "https://openalex.org/W2939314313",
    "https://openalex.org/W2916581152",
    "https://openalex.org/W2998571806",
    "https://openalex.org/W3214740101",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W4210592951",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W6601211009",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W2160592148",
    "https://openalex.org/W3006889321",
    "https://openalex.org/W2080635178",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W3114291043",
    "https://openalex.org/W3165630607",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W4221074165",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W2953128081",
    "https://openalex.org/W2989615256",
    "https://openalex.org/W4281619372"
  ],
  "abstract": null,
  "full_text": "Blanchard et al. Journal of Cheminformatics           (2023) 15:59  \nhttps://doi.org/10.1186/s13321-023-00719-7\nRESEARCH Open Access\n© UT-Battelle, LLC 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nJournal of Cheminformatics\nAdaptive language model training \nfor molecular design\nAndrew E. Blanchard1, Debsindhu Bhowmik1*, Zachary Fox1, John Gounley1, Jens Glaser2, \nBelinda S. Akpa3,4 and Stephan Irle1 \nAbstract \nThe vast size of chemical space necessitates computational approaches to automate and accelerate the design of \nmolecular sequences to guide experimental efforts for drug discovery. Genetic algorithms provide a useful frame-\nwork to incrementally generate molecules by applying mutations to known chemical structures. Recently, masked \nlanguage models have been applied to automate the mutation process by leveraging large compound libraries to \nlearn commonly occurring chemical sequences (i.e., using tokenization) and predict rearrangements (i.e., using mask \nprediction). Here, we consider how language models can be adapted to improve molecule generation for different \noptimization tasks. We use two different generation strategies for comparison, fixed and adaptive. The fixed strategy \nuses a pre-trained model to generate mutations; the adaptive strategy trains the language model on each new gen-\neration of molecules selected for target properties during optimization. Our results show that the adaptive strategy \nallows the language model to more closely fit the distribution of molecules in the population. Therefore, for enhanced \nfitness optimization, we suggest the use of the fixed strategy during an initial phase followed by the use of the adap-\ntive strategy. We demonstrate the impact of adaptive training by searching for molecules that optimize both heuristic \nmetrics, drug-likeness and synthesizability, as well as predicted protein binding affinity from a surrogate model. Our \nresults show that the adaptive strategy provides a significant improvement in fitness optimization compared to the \nfixed pre-trained model, empowering the application of language models to molecular design tasks.\nKeywords Masked language model, Drug discovery, Genetic algorithm\nThis manuscript has been authored by UT-Battelle LLC under Contract \nNo. DE-AC05-00OR22725 with the US Department of Energy (DOE). The \nUS government retains and the publisher, by accepting the article for \npublication, acknowledges that the US government retains a nonexclusive, \npaid-up, irrevocable, worldwide license to publish or reproduce the published \nform of the manuscript, or allow others to do so, for US government \npurposes. DOE will provide public access to these results of federally \nsponsored research in accordance with the DOE Public Access Plan (http:// \nenergy. gov/ downl oads/ doe- public- access- plan).\n*Correspondence:\nDebsindhu Bhowmik\nbhowmikd@ornl.gov\n1 Computational Sciences and Engineering Division, Oak Ridge National \nLaboratory, Oak Ridge, TN 37831, USA\n2 National Center for Computational Sciences, Oak Ridge National \nLaboratory, Oak Ridge, TN 37831, USA\n3 Biosciences Division, Oak Ridge National Laboratory, Oak Ridge, TN \n37831, USA\n4 Chemical & Biomolecular Engineering, University of Tennessee, \nKnoxville, TN 37996, USA\nPage 2 of 12Blanchard et al. Journal of Cheminformatics           (2023) 15:59 \nIntroduction\nThe goal of rational drug design is to identify molecules \nwith specified properties associated with therapeutic \nvalue. Emerging infectious diseases (e.g. SARS-CoV-2 \nand the associated pandemic) highlight the need for \nrational design to accelerate the discovery of drugs in \nresponse to novel protein targets [1, 2]. Computer aided \ndrug discovery (CADD) provides a set of tools to shorten \nthe time and cost of searching chemical space for new \napplications [2–7]. In addition to the development of bio-\nphysical models and simulations traditionally associated \nwith CADD  [5–7], much recent work has focused on \nusing methods from machine learning (ML) and artificial \nintelligence (AI) for molecular design [4, 5, 7–9].\nThe use of ML models in drug design has been ena -\nbled by the availability of large compound libraries  [10] \nand experimental datasets [11, 12] along with computa -\ntional libraries for cheminformatics [13]. Within a design \napplication, models generally serve one of two possibly \noverlapping roles, molecule generation and molecule \nscoring. Generative models, such as variational autoen -\ncoders  [8, 14] and generative adversarial networks  [15, \n16], are capable of sampling new molecules from chemi -\ncal space based off a training set. Scoring models, on the \nother hand, take a molecule as input and generate a pre -\ndiction for a given property (e.g. protein binding affinity). \nThrough iterations of generation and scoring, searches \nover chemical space can be performed to optimize a \ngiven property. The iterative process for optimization is \ncommonly referred to as a genetic algorithm [17].\nGenetic algorithms provide a useful strategy for the \ndesign of molecular sequences for drug discovery appli -\ncations. To use a genetic algorithm, a representation for a \nchemical sequence must be chosen along with a mutation \noperator to generate new sequences. The mutation oper -\nator is then used to explore chemical space and selection \nis performed according to a pre-defined fitness objective. \nPrevious studies have used genetic algorithms success -\nfully for a range of drug discovery applications [18–22]. \nFurthermore, benchmark studies have shown that genetic \nalgorithms can achieve state-of-the-art results for mol -\necule generation, comparing favorably to recent machine \nlearning techniques [19, 21].\nDespite the success of genetic algorithms, the need \nto define an appropriate representation for a chemical \nsequence and a mutation operator poses a challenge. \nPrevious studies have often utilized a simple representa -\ntion by enumerating individual atoms and bonds within \na molecule [18, 19, 22]. For mutation, hand-crafted rules, \nsuch as add an atom, delete an atom, or create a ring, \nhave been proposed and used for large scale explora -\ntion of chemical space [18]. Additional studies have used \ndata mining techniques to discover commonly occurring \nmulti-atom fragments and used custom mutation opera -\ntors to rearrange the specified fragments  [20, 22–25]. \nHowever, specifying fixed rules for rearrangements limits \nthe ability to adapt the optimization procedure to a given \ntask. Ideally, the mutation operator can be automatically \ninferred from the data, reducing the need for intuition \nand generalizing the genetic algorithm approach to new \nmolecular design tasks.\nA related approach to molecule generation uti -\nlizes recurrent neural network (RNN) based architec -\ntures such as the Long Short-Term Memory (LSTM). \nMore generally, statistical language-based models uti -\nlize different structural representations (e.g., molecu -\nlar fingerprints) for generation and optimization based \narchitectures. For example, Segler et al. [26] had showed \nhow a LSTM based models can be used for transfer \nlearning as they are fine-tuned on smaller population of \nmolecules to achieve activity towards certain biological \ntarget and thus be used to generate novel set of molecules \nwith desired activities. Along that direction, Arés-Pous \net al. [27] have carried out an extensive study on different \nRNN based models (such as LSTM and Gated recurrent \nunit or GRU) using different Simplified Molecular Input \nLine Entry System (SMILES) representations like canoni -\ncal, randomized and DeepSMILES versions. These differ -\nent experiments designs are then tested on various sizes \nof molecule populations ranging from 10k to 1 million. \nIn another recent RNN based work [28] on two different \nstring representations namely SMILES and SELF-refer -\nencing Embedded Strings (SELFIES) demonstrated that \nRNN-based language models can deliver powerful gen -\nerative capabilities while learning complex chemical rules \nof the molecular representations better than graph-based \nmodels. This observation is then further extended by the \nworks of Awale et al. [29] when they trained LSTM based \ngenerative models on different datasets including full \nsize drug molecules along with fragments and performed \ntransfer learning to demonstrate that fragments-based \ntraining is as capable as training on full size molecules \nin producing efficient drug analogs. In related work on \nbiogenic compounds Zheng et al. [30] developed a quasi-\nbiogenic molecule generator (QBMG) with GRU RNN \nto generate quasi-biogenic compounds, libraries includ -\ning stereochemistry and a de novo approach to produce \nfocused libraries influenced by certain scaffolds. On the \nother hand, recent proposed methods based on con -\nditional generative adversarial networks  [31] or GAN, \noffers an alternative strategy to take advantage of all \ninformation stored in compound-induced gene expres -\nsion data to generate active-like molecules. As their \nmethod requires no explicit activity or target annota -\ntion information during training process, this can be \nused as a target-independent generalized approach. But \nPage 3 of 12\nBlanchard et al. Journal of Cheminformatics           (2023) 15:59 \n \nalgorithm wise these types of models are very different \nthan bidirectional transformers-based models. Trans -\nformer based large language models (LLM) are different \nthan RNN or LSTM type language models. These trans -\nformer-based molecule generators in recent times dem -\nonstrate how effective these LLMs could be in designing \nnovel molecules for different purposes as required. Bidi -\nrectional Encoder Representations from Transform -\ners (BERT)  [32] based LLMs showed advantages while \ntested on established benchmark models and datasets for \ndownstream tasks and gCT [33] (i.e., generative chemical \nTransformer) showed improved or at-least on-par per -\nformance. Similarly generative pre-training (GPT)  [34] \nmodels delivers comparable performance in generating \nnovel, valid and unique molecules when tested on bench -\nmark datasets with other models.\nThe present work i.e., a novel strategy about how to \ngenerate a  new population of molecules resembling ini -\ntial highly optimized molecules by adapting the original \noptimized properties while restricting from generating \na generic broader population distribution of new mol -\necules, is a direct improvement over using fixed pre-\ntrained model as demonstrated. Under-the-hood our \nimplementation is based on Transformer architecture \nspecially to be mentioned as Bidirectional Encoder Rep -\nresentations from Transformers (BERT)  [32]. This par -\nticular type of architecture has shown proven advantage \nwhen used on established benchmark datasets such as \nGuacaMol  [21] for targeted benchmark tasks such as \nvirtual screening and QSAR applications by positively \nimpacting subsequent downstream tasks, augment -\ning the constancy of learnt molecular representation \nand improved performance over present dataset  [32]. \nIn related work using transformers model on chemical \ndesigning, analogous architecture namely gCT  [33] (i.e., \ngenerative chemical Transformer) also able to success -\nfully generate valid new molecules that satisfy various \nrequired target properties while showing either improved \n(or at-par in some cases) compared to other benchmark \nreference models (such as MOSES models [35]). Also, on \nusing related large language models (LLM) based archi -\ntecture such as using generative pre-training (GPT) [34] \nmodels we see results and performance that are compa -\nrable to previously implemented machine learning algo -\nrithms to task like designing valid, novel, and unique \nmolecules when compared with MOSES [35] benchmark \nmodels and datasets.\nInspired by the advances in natural language processing \n(NLP) [36], recent studies have shown how to automate \nboth the choice of representation for chemical structure \nand the mutation operator  [2, 37]. Starting with a text-\nbased representation for molecules, SMILES  [38], the \nprocess of tokenization is used to determine commonly \noccurring subsequences  [39, 40]. The subsequences are \nstored as a vocabulary and are used to map a given mole -\ncule sequence into a list of token IDs. Each token ID may \ncorrespond to multiple characters (i.e., atoms and bonds) \nin a given molecule. Once a tokenization scheme is \ndefined, the molecule data can be used to train a masked \nlanguage model. In the training for such a model, tokens \nare randomly masked and the loss is determined by how \nwell the model reproduces the original sequence when \npredicting the masked tokens [36].\nWithout the need for labels, unsupervised training \nof masked language models can be performed on large \ncompound libraries (e.g. Enamine REAL  database)  [10]. \nFor a given mask, a trained model will rank possible \nways to complete the molecular sequence based on the \nvocabulary. Therefore, sampling from the top mask pre -\ndictions provides an automated mutation operator for a \ngenetic algorithm [37]. Therefore, in contrast to manually \ndefining rules for mutations, masked language models \nprovide an automated solution for discovering both use -\nful molecular representations (i.e., through tokenization) \nand mutations (i.e., through mask prediction) as shown \nin Fig. 1.\nAlthough the use of a fixed pre-trained masked lan -\nguage model provides a useful improvement over man -\nually defined rules, the challenge to adapt molecule \ngeneration for different optimization tasks remains. For \nexample, the dataset used for model pre-training may \nhave certain biases that limit structural rearrangements \nuseful for a new task. In order to overcome this difficulty, \nwe here propose a novel way to use language models \nwithin genetic algorithm optimization. Specifically, we \ncontinue to train the masked language model on popula -\ntions selected for a specified fitness objective. By contin -\nued training on the selected population, we hypothesized \nthat the language model would adapt to new regions of \nchemical space useful for optimization.\nIn order to test our hypothesis, we implemented two \napproaches for comparison - fixed and adaptive. In the \nfixed approach, a pre-trained language model was used \nto generate new molecules. In the adaptive approach, the \npre-trained language model is used as a starting point \nand further trained using mask prediction on a speci -\nfied population. Continued training is performed after \neach iteration of the genetic algorithm to produce a new \npopulation of molecules. Our results show that the adap -\ntive approach produces data that more closely mimics \nthe genetic algorithm population. For optimization, the \nadaptive approach leads to increases in fitness for tasks \nusing both heuristic metrics and a ML surrogate model. \nTherefore, by introducing the adaptive approach for auto-\nmating mutations we broaden the capabilities of genetic \nalgorithm optimization for molecular design.\nPage 4 of 12Blanchard et al. Journal of Cheminformatics           (2023) 15:59 \nMethods\nGenetic algorithm\nIn this work, we focused on the molecule generation \ncapabilities of a masked language model for fitness opti -\nmization. The source code for this work can be found at \nhttps:// code. ornl. gov/ candle/ mlmol in the adaptive-lm \ndirectory. As described in previous work [37], a masked \nlanguage model can be used as an automated mutation \noperator within a genetic algorithm. Figure  1 shows the \nmajor components for optimization. An initial popula -\ntion of molecules, in the form of SMILES strings are \nused as input to the masked language model. Portions \nof a given SMILES string are then randomly masked \nand the language model is used to predict mutations to \nthe original molecule. The generated molecules are then \nscored and selection is performed based on the specified \nfitness to generate an optimized population. The process \nof mutation and selection can be repeated for a specified \nnumber of iterations.\nFor the language model acting as the mutation opera -\ntor, we considered two different training strategies, fixed \nand adaptive. In both cases, we started by pre-training a \nmasked language model on a dataset with billions of mol-\necules (for further details on the dataset, see Methods \nSection - Molecule Data). For the fixed strategy, weights \nof the pre-trained model were frozen, and the model was \nused only for inference (i.e., mask prediction) as part of \nthe genetic algorithm. For the adaptive strategy, however, \nmodel training based on mask prediction was performed \nfor one epoch during each generation, with the current \npopulation of molecules used as the training data. The \nlanguage model, therefore, adapted to the patterns found \nin the current population of the genetic algorithm before \ngenerating mutations.\nTo distinguish between the optimization performance \nof the fixed and adaptive strategies, we utilized a rela -\ntively simple genetic algorithm with a (µ + 5µ) survi -\nvor selection scheme. Random uniform sampling with \nreplacement was used to select µ parents from the popu-\nlation, and only mutation was used to generate new mol -\necules, similar to our previous work  [37]. A population \nsize ( µ ) of 105 was used for all reported genetic algorithm \nsimulations. Mutations were generated by taking the \ntop 5 predictions from the masked language model for a \ngiven set of masks. Validity and uniqueness of the gener -\nated molecules were determined using rdkit [13] to con -\nvert SMILES strings into canonical form. Only unique \nmolecules were retained in the population. All reported \nresults, except for example histograms, show the mean \nover six repeated runs, with the standard deviation used \nto calculate error bars. Example histograms show the dis-\ntribution of metric values for a single run.\nFor mask generation, we considered the following dif -\nferent values for the mutation rate (i.e., probability that a \ngiven token will be masked): [0.15, 0.30, 0.45, 0.60, 0.75]. \nIn addition, three different types of mutation (replace -\nment, insertion, and deletion) were used. For each type, \nthe number of mutations was determined using the bino-\nmial distribution for the appropriate number of tokens \nand mutation rate. A minimum number of 1 mask per \nFig. 1 Strategy for molecule optimization using a language model. An initial population of molecules is used as input. The language model then \ngenerates mutations using predictions for randomly placed masks. Molecules are ranked according to a specified score and top performers are \nselected for another round of mutations. Two approaches for the language model are investigated, fixed and adaptive. For the fixed approach, the \nlanguage model is pre-trained on a large molecule dataset and it does not change during the optimization process. For the adaptive approach, the \nlanguage model is trained on the selected population, which itself changes during the optimization process\nPage 5 of 12\nBlanchard et al. Journal of Cheminformatics           (2023) 15:59 \n \nmolecule was enforced. The locations for each mutation \nwithin the molecule string were then randomly sam -\npled. For replacement, the sampled token locations were \nreplaced with a mask. For insertion, one sampled loca -\ntion was used to insert a mask before the given token. \nSimilarly, for deletion, one sampled location was used to \ndelete the token following the mask. The remaining sam -\npled locations for both insertion and deletion were used \nfor replacement.\nFitness in the genetic algorithm simulations was deter -\nmined using the harmonic mean of multiple molecular \nmetrics. For example, for two metrics ( x1 and x2 ), we \nused a fitness F given by:\nBy default, we used quantitative estimations of drug-\nlikeness and normalized synthesizability, similar to sev -\neral previous studies on molecular optimization [15, 16, \n41, 42]. To apply the genetic algorithm strategies on a \nmore realistic drug discovery scenario, we also utilized a \nrecently released model for protein binding affinity pre -\ndiction to generate a molecular metric [43]. Specifically, \nwe used a predicted affinity score for the main protease \nof SARS-CoV-2. The resulting fitness was, therefore, the \nharmonic mean of drug-likeness, synthesizability, and \nthe predicted affinity score.\nMolecule data\nSimilar to previous work  [2], we generated a molecule \ndataset starting from the Enamine REAL  database  [10]. \nUsing a data augmentation strategy with a previously \ntrained language model, we increased the number of \nmolecules to approximately 3.6 · 10 10 . The strategy for \ndata augmentation is inspired by the pre-training process \nof the masked language models [2]. The pre-trained mod-\nels are capable of designing novel, valid and unique mol -\necules by structural rearrangements including combining \ntwo molecules. But in order to be selected to augmented \ndata the newly predicted molecules also should be valid, \nunique and with synthesizability score to be more than \ncertain threshold (in the case 0.30). In preparation for \nmodel training, the dataset was partitioned into 7.2· 104 \nfiles, each with 5 · 105 molecules, stored using the Web -\nDataset [44] library for shared data loading during model \ntraining.\nIn addition to the constructed molecule dataset, we \nused two additional datasets as the starting population \nfor genetic algorithm simulations. First, we used a sub -\nset of 105 molecules from QM9 [45, 46], referred to in the \ntext and figures as GDB9. Second, we selected the top \n105 in terms of drug-likeness and synthesizability from a \nhold-out set of the training data, referred to in the text \n(1)F (x1 ,x2 ) = 2x1 x1\nx1 + x2\nand figures as Top. These two datasets were used to show \nthe difference in performance for the fixed and adaptive \nstrategies when starting from a relatively low and high \ninitial fitness respectively.\nLanguage model training\nLanguage model pre-training consists of two different \nstages, tokenization and mask prediction. During tokeni -\nzation, a vocabulary is generated for the model based on \ncommonly occurring subsequences within the SMILES \nstring for molecules. Here, we split the SMILES string \nduring pre-processing based on punctuation, which is the \ndefault splitting used for the BERT WordPiece tokenizer \nin the Hugging Face transformers library [47]. The vocab-\nulary for the WordPiece tokenizer was then generated \nusing the full 36 billion molecule dataset, with the vocab -\nulary size set to 32,768.\nFor mask prediction, we used PyTorch and Hugging \nFace transformers along with DeepSpeed for distributed \ntraining [48]. The transformer architecture that has been \nused here for the molecule language model is BERT-\nbased. This has approximately 109 million parameters \nthat are learnable. We Pre-train the model with data par -\nallelism technique where each of the GPUs is trained with \nthe model on separate data. As described in [2], we used \ndata parallelism with DeepSpeed’s fused LAMB opti -\nmizer to train at scale on a dataset of 3 billion molecules \n(i.e., the first 6000 partitions of the full molecule dataset). \nPre-training was performed on the Summit supercom -\nputer using 1000 nodes (6 Nvidia 16 GB V100 GPUs per \nnode), with each partition of the dataset assigned to a \nsingle GPU. We used a batch size of 80 molecules with 3 \ngradient accumulation steps per GPU, leading to a global \nbatch size of 1.44 million. As stated the primary objec -\ntive has been to develop a novel algorithm that adapts to \ninitial highly optimized dataset generating similar opti -\nmized molecules and not to attain generic distribution of \nnovel molecules or to predict individual molecules with \nsome specific properties. For this purpose, we required \na dataset that will be as large as possible to begin with \nso that the pre-trained model will benefit from learning \nthrough the largest chemical dataset available. More so \nbecause having trained on as wide a distribution of train -\ning data as practicable, we minimize the bias related to \ndata being in or out of distribution in the results of the \nadaptivity experiment. To have a model that is trained on \nthis large and with wide distribution of molecule data -\nset we used required large number of GPUs. But once \nthese models are trained, these pre-trained models can \nbe used with one GPU on small dataset for fine-tuning or \ndownstream tasks as required. Pre-training was done for \n7 epochs, taking approximately 2.5 h, and model valida -\ntion was done using mask prediction on a hold-out set of \nPage 6 of 12Blanchard et al. Journal of Cheminformatics           (2023) 15:59 \nmolecules. The best validation accuracy occurred for the \nfinal epoch, and the resulting model weights were frozen \nfor language model mutations in the fixed strategy. The \nmodel weights were used as the initial conditions for con-\ntinued training in the adaptive strategy.\nTo further validate the pre-trained model, we randomly \nsampled 100,000 molecules with different mutation \nrates for each of the two data sets used throughout the \nmanuscript as initial populations. New molecules were \ngenerated by sampling masked tokens using the Gum -\nbel-softmax layer implemented in PyTorch. We com -\nputed the percent of novel and valid molecules present \nin each population, showing that increasing the mutation \nrate decreases the number of valid and novel molecules \n(Table 1).\nSurrogate model for binding affinity\nIn addition to the heuristic metrics for drug molecules, \nsynthesizability and drug-likeness, we also used an ML \nmodel to predict protein binding affinity for a given \ntarget, in this case the main protease of SARS-CoV-2. \nAs described in previous work  [2], the binding affinity \nmodel was generated by fine-tuning language models \nfor both molecule and protein sequences. The output of \nthe model is the predicted negative log (base 10) of the \nbinding affinity. To convert to an affinity score for fitness, \nwe divided the prediction by 10 and clipped the result -\ning values between 0 and 1. Although the validation and \ndiscussion of this model are beyond the scope of the cur -\nrent work, we chose it as an example to illustrate that our \nproposed optimization strategies can be applied to find \nhigh-scoring candidates for both heuristic and ML sur -\nrogate scoring models.\nResults\nFixed and adaptive strategies for molecule generation\nBefore analyzing the impact of continued language \nmodel training on molecule optimization, we consid -\nered a simpler task: generating mutations for a fixed set \nof initial molecules. We implemented this task by using \nthe genetic algorithm without selection (i.e. the parent \npopulation remains unchanged). During each generation, \nmutations are generated and the resulting unique mol -\necules are saved for further analysis. For the fixed strat -\negy, mutations are generated from the fixed pre-trained \nmodel, while for the adaptive strategy, the language \nmodel is trained for 1 epoch on the initial data in each \ngeneration before producing mutations.\nAs shown in Fig.  2, we used two different initial data -\nsets, GDB9  [46] and Top (see Methods Section - Mole -\ncule Data). The mutation rate determines the fraction of \ntokens that are randomly masked during the generation \nTable 1 Valid and novel molecules generated by the language \nmodel\nMutation rate 0.15 0.30 0.45 0.60 0.75\n% valid: GDB9 28 26 21 16 9.7\n% novel: GDB9 25 24 20 15 9.2\n% valid: TOP 29 31 26 15 4.7\n% novel: TOP 26 29 25 15 4.7\nFig. 2 Distributions of molecules produced by a fixed and adaptive approach. Two datasets (GDB9 and a custom dataset with the top scoring \nmolecules for drug-likeness and synthesizability) are used as training data. The fixed approach (blue) generates a broad distribution of molecule \nscores, while the adaptive approach (orange) more closely mimics the training dataset. Notice that for initial training data with low scores (i.e., \nGDB9), the adaptive approach produces lower scores on average than the fixed approach, while the situation is reversed for initial training data with \nhigh scores (i.e., Top)\nPage 7 of 12\nBlanchard et al. Journal of Cheminformatics           (2023) 15:59 \n \nof new molecules. Each genetic algorithm simulation \nwas run for 5 generations. For each run, the mean drug-\nlikeness and synthesizability scores were calculated for all \nunique molecules produced in each generation outside of \nthe original data. In terms of time there is no significant \ndifference in generating the molecules between these \napproaches. For example, the fixed strategy is able to gen-\nerate ∼308k valid molecules in ∼42 min out of which ∼\n284k are novel molecules using one GPU while adaptive \nstrategy is able to generate ∼250k valid molecules out of \nwhich ∼212k molecules are novel molecules in ∼32 min. \nThe histograms show an example of the distributions for \nnovel molecules with a metric value greater than zero \nproduced from the final generation of a single run with a \nmutation rate of 0.3.\nDue to the continued training of the language model, \nthe mutations generated by the adaptive strategy are \nmuch closer, in terms of synthesizability and drug-like -\nness, to the initial population of molecules. This leads \nto a decrease in typical values for the GDB9 dataset. \nHowever, for the Top molecules, the adaptive strategy \nproduces higher scores. This result can be intuitively \nexplained, as the fixed model is biased by the data used in \npre-training (i.e., the pre-trained model will tend to pro -\nduce mutations that were prevalent in its training data -\nset). Continued training allows the model to adapt to the \nnew data, either GDB9 or Top.\nFixed and adaptive strategies for molecule optimization\nFor molecular optimization, the ability to adapt to a \ngiven initial dataset may or may not be beneficial. In \nthe case of initial data with relatively low scores, we \nexpect the adaptive strategy to slow down optimi -\nzation, as the generated molecules with have scores \nsimilar to the poor initial data. To test this hypothesis, \nwe applied a genetic algorithm (GA) to optimize mol -\necules for drug-likeness and synthesizability starting \nfrom the GDB9 dataset. As shown in Fig.  3, the adap -\ntive strategy indeed results in decreased fitness relative \nto the fixed strategy. This molecular optimization task \ncan be contrasted with the fixed strategy for molecular \ngeneration in Fig.  2 as a baseline (shown in dark blue \nFig. 3 Optimization of molecules for drug-likeness and synthesizability produced by a fixed language model, adaptive language model, or \nfixed language model without a genetic algorithm based optimization scheme. Two datasets (GDB9 and a custom dataset with the top scoring \nmolecules for drug-likeness and synthesizability) are used as initial data. In the Fitness vs Generations subplots, the y-axis is the average fitness of \nthe population over six runs. The related standard deviations are small compared to the mean values in the order of 0.1%−0.2%. The fixed approach \n(blue) results in a faster increase in fitness, along with greater valid and accepted molecules for the GDB9 dataset. For the top dataset, however, the \nadaptive approach leads to a faster increase in fitness along with greater accepted molecules. Both the adaptive and fixed approaches outperform \nthe baseline of a fixed language model without the genetic algorithm. The histograms show synthesizability and drug-likeness of the final \npopulation after six generations for each approach\nPage 8 of 12Blanchard et al. Journal of Cheminformatics           (2023) 15:59 \nthroughout Fig.  3). The fitness plot shown over five \ngenerations and histograms of the final molecular pop -\nulations were generated with a mutation rate of 0.3. \nFurthermore, the adaptive strategy produced less valid \nmolecules and less accepted molecules (i.e., molecules \naccepted into the population during selection) for all \nmutation rates.\nThe same genetic algorithm applied to the Top \ndataset produces the opposite results in terms of fit -\nness. Here, the adaptive strategy outperforms the fixed \nstrategy for all mutation rates considered. Interest -\ningly, although the adaptive strategy produces fewer \nvalid molecules for most mutation rates (similar to the \nGDB9 dataset), it produces more accepted molecules \nin all cases. The decrease in valid molecules can be \nunderstood as adaptive training leading to possible \nissues with over-fitting the current dataset, rather than \nlearning from the large compound library used for pre-\ntraining. However, the increase in accepted molecules \nsuggests that molecular rearrangements learned from \na high scoring dataset can improve fitness optimiza -\ntion despite the decrease in valid molecules. The fixed \nand adaptive GA-based approaches provided much \nhigher fitness than random search despite generat -\ning a similar number of valid molecules. For the fol -\nlowing analysis, we fixed the mutation rate to 0.3 and \nfocused on ways to use the fixed and adaptive strate -\ngies together for molecular design.\nCombining fixed and adaptive strategies\nThe trade-off in performance for the fixed and adaptive \nstrategies, depending on the distribution of values in \nthe initial dataset, suggests that mixing fixed and adap -\ntive strategies may be useful for molecular optimization. \nFor a new optimization task, a previously optimized \ndataset will likely not exist to serve as an initial popula -\ntion. In many cases, generating a reasonably optimized \ndataset may be the entire goal of applying the optimiza -\ntion procedure. Therefore, we assume that the case with \npoorly optimized initial data, similar to GDB9, is more \nrepresentative of a typical molecular design problem. In \nthis case, our results have shown that the fixed strategy \noutperforms the adaptive strategy for optimization. How-\never, as the fitness of the population increases, we expect \nthat the adaptive strategy may provide a better alternative \nto optimize fitness.\nTo test this hypothesis, we implemented various sched-\nules for combining the fixed and adaptive strategies. As \nshow in Fig.  4, the fixed strategy was used initially and \nthen replaced by the adaptive strategy after a specified \nnumber of generations. As expected, the optimal strat -\negy involves a combination of the two strategies, with five \ngenerations of fixed followed by 20 generations of adap -\ntive. Interestingly, although the purely adaptive strategy \n(orange) increases much more slowly than the purely \nfixed strategy (blue), adaptive overtakes fixed in terms of \nfitness after approximately 15 generations. This suggests \nthat the difficulties associated with fitting more closely to \nFig. 4 Combining fixed and adaptive approaches during optimization. The fixed approach is used during optimization for 25 epochs. For \ncomparison, the adaptive approach is used starting from the output population of the fixed approach at different generations. The highest fitness \nis achieved in the case where the adaptive approach is used after 5 epochs of the fixed approach. In the Fitness vs Generations subplots, Y-axis \nis average fitness of the and calculated as mean over six runs with standard deviations in the order of 0.1–0.2% of mean value. Notice that the \nadaptive approach starting from the same initial data as the fixed approach achieves a higher fitness after approximately 15 epochs\nPage 9 of 12\nBlanchard et al. Journal of Cheminformatics           (2023) 15:59 \n \na poor initial dataset can be overcome with the ability to \nadapt to the population as fitness increases.\nMolecular optimization using a surrogate model\nAll of the results we have shown so far have used heu -\nristic functions to score molecules (i.e., synthesizability \nand drug-likeness scores). However, molecular optimiza -\ntion applications may involve additional ML-based surro-\ngate models for scoring. For example, a ML model may \nbe trained on a limited experimental dataset in order to \nsearch for molecules with related properties. Here, we \nuse a previously trained surrogate model, which is avail -\nable for download  [43], developed to predict binding \naffinity for a given protein and molecule. We fix the pro -\ntein sequence to the main protease of SARS-CoV-2, as \ndescribed previously [2], and generate a normalized affin-\nity score to use in fitness calculations. During the evalu -\nation of surrogate model for predicting binding affinity \nfor a given protein and molecule, that is also then used \nin calculating fitness function, the cost of computation \nincreased because the large population size. Due to long \nruntimes with the surrogate model, a given genetic algo -\nrithm simulation was split into multiple sequential jobs, \nwith each job running for five generations. Upon restart -\ning, the model weights were initialized to the fixed pre-\ntrained model.\nBuilding off the results for optimization with heuristic \nmetrics, we compare two optimization schedules. We \nfirst apply the fixed strategy for five generations. This is \nfollowed by the adaptive strategy for 20 generations, with \nthe continued fixed strategy for comparison. As shown \nin Fig.  5, the adaptive strategy results in a substantial \nincrease in fitness over the fixed strategy for optimiza -\ntion with the surrogate model. By comparing the histo -\ngrams for synthesizability, drug-likeness, and affinity \nscore, we determined that the increase in fitness values \nwas primarily the result of increases to the affinity score, \nsuggesting that the adaptive strategy is particularly use -\nful for optimizing the ML scoring model. We also show \nexamples of molecules with different values for the three \nmetrics used during fitness optimization. Beyond gener -\nating molecules with high values for all three metrics, the \nexamples show how changes in the chemical structure for \na family of molecules result in trade-offs amongst synthe-\nsizability, drug-likeness, and affinity score.\nDiscussion\nSequence‑only models for drug design\nThe models presented in this work for both molecule gen-\neration and scoring rely only on the molecular sequence \n(i.e., the SMILES is the only model input). A sequence-\nonly approach is in contrast to ML models that utilized \nmany local and global features (e.g. molecular finger -\nprints) [3]. Simulation and modeling approaches outside \nof ML, such as molecular dynamics and docking, use \nthe full three-dimensional structures of both the protein \nFig. 5 Fixed and adaptive approaches to optimize fitness given by the harmonic mean of synthesizability, drug-likeness, and affinity score. \nChanging to the adaptive approach after 5 generations results in an increase in fitness as shown by the histograms for drug-likeness and affinity \nscore. The histograms were generated from the final population for the runs with the highest fitness for fixed and adaptive approaches. In the \nFitness vs Generations subplots, Y-axis denotes average fitness of the population and plotted as mean over six runs. The related standard deviations \nare in the order of 0.1%−0.2% of the mean values. Sample molecules with similar chemical structures are shown for the adaptive approach. \nMutations proposed by the language model show how modifications result in changes in the metrics used to calculate fitness\nPage 10 of 12Blanchard et al. Journal of Cheminformatics           (2023) 15:59 \nand molecule to predict binding affinity [5]. The primary \nstrength and weakness, therefore, of sequence-only mod -\nels is the simplicity of the model input. By using SMILES, \nthe model has no direct information concerning geom -\netry or chemical properties. However, SMILES enables \nthe model to be used to train and make predictions on \ndata without known three dimensional structures or pre -\nviously calculated chemical properties, enabling searches \nand screening over large portions of chemical space. Fur -\nthermore, sequence-only models have been shown to \ncompare favorably to more traditional approaches with \nmanually defined features [49–51].\nMolecule generation through mutations\nIn this work we have considered molecule generation for \ndesign using a language model to generate mutations. \nThis strategy differs from other approaches to develop \ngenerative models, such as variational autoencoders [14, \n52] and generative adversarial networks  [15, 16]. The \nmutation strategy is dependent on an original molecule \nin which certain subsequences are changed rather than \ngenerating an entire molecule by sampling from a latent \nspace or noise distribution. Although mutation relies \nupon an original molecule, and thus limits the amount of \nchemical space accessible for a given round of molecule \ngeneration, it has multiple benefits. First, mode collapse \nshould not in principle present a problem for molecule \ngeneration through mutation. Because mutations are \nsampled from each molecule in the population, the full \ntraining set is represented in each generation of gener -\nated molecules. Second, each round of mutations can \nbe manually inspected along with the scores for each \nrespective molecule, enabling a user to better understand \nthe types of mutations being generated and their impact \non fitness. Furthermore, through multiple iterations of \nmutations and selection, large regions of chemical space \ncan be explored  [18], even though a single iteration \nremains close to the original data.\nAdaptive strategy\nAs mentioned earlier the masked language models \nenable us to attain two major design targets - (1) dis -\ncovering useful molecular representation through \ntokenization and (2) injecting mutation through mask -\ning. In this work our primary objectives are - firstly, \ngiven a highly optimized dataset available for initial \ndataset whether we could devise a certain novel way to \ngenerate newer dataset that also guarantees to be opti -\nmized in similar fashion. Towards that goal we develop \nan effective algorithm that overcomes the challenges \nto generate new set of molecules from an optimized \ndataset through necessary molecular reconstruc -\ntion for performing particular different optimization \ntasks while simultaneously restricting from attaining \na generic broader population distribution. In other \nwords, we intend to demonstrate a novel way that \nwill get rid of the certain biases that obstructs neces -\nsary structural rearrangements during mutation pro -\ncess, automatically adapt the required chemical region \nfrom the original population required for specific user-\ndefined new tasks so that a certain population distribu -\ntion can be produced. Secondly the other goal of the \nwork is to generate a new population of molecules that \nare more similar in nature to the original highly opti -\nmized data than finding few individual novel molecules \nwith certain specific properties. Together fulfilling \nboth of the above objectives means the improvement \nthat our new strategy offers will be to adapt to specific \nhighly optimized datasets for generating novel mol -\necules that are able to perform highly optimized tasks \nand be prevented from a broader generic distribution.\nConclusions\nMasked language models coupled with genetic algo -\nrithms provide a useful framework for molecular optimi -\nzation. During tokenization and pre-training, the model \ndetermines commonly occurring chemical sequences and \nrearrangements that can be leveraged for molecule gen -\neration through mutations. Furthermore, the language \nmodel can be refined using continued training on popu -\nlations of molecules selected for desired properties. Here, \nwe have shown that the continued training of a language \nmodel during genetic algorithm optimization provides a \npowerful approach to search for molecules according to \nboth heuristic and ML model scoring functions. Mod -\nels pre-trained on large compounds libraries serve as a \nuseful starting point for both initial optimization from a \npoorly optimized dataset and initial weights for contin -\nued training.\nAcknowledgements\nAn award of computer time was provided by the Innovative and Novel Com-\nputational Impact on Theory and Experiment (INCITE) program. This research \nused resources of the Oak Ridge Leadership Computing Facility at the Oak \nRidge National Laboratory, which is supported by the Office of Science of the \nU.S. Department of Energy under Contract No. DE-AC05–00OR22725.\nAuthor contributions\nAll authors contributed in developing the concept for the study. AEB, DB, JG, \nand JG developed the code and trained the models. AEB performed data \nanalysis and generated the figures. All authors assisted in manuscript prepara-\ntion. All authors read and approved the final manuscript.\nFunding\nThis research was funded by the AI Initiative, as part of the Laboratory \nDirected Research and Development Program of Oak Ridge National Labora-\ntory, managed by UT-Battelle, LLC, for the U.S. Department of Energy (DOE); \nthe Exascale Computing Project (ECP) (17-SC-20-SC), a collaborative effort \nof the U.S. Department of Energy Office of Science and the National Nuclear \nSecurity Administration.\nPage 11 of 12\nBlanchard et al. Journal of Cheminformatics           (2023) 15:59 \n \nAvailability of data and materials\nThe source code for this work can be found at https:// code. ornl. gov/ candle/ \nmlmol  in the adapt ive- lm direc tory.\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 9 May 2022   Accepted: 3 April 2023\nReferences\n 1. Dong E, Du H, Gardner L (2020) An interactive web-based dashboard to \ntrack COVID-19 in real time. Lancet Infect Dis 20(5):533–534. https:// doi. \norg/ 10. 1016/ S1473- 3099(20) 30120-1\n 2. Blanchard AE, Gounley J, Bhowmik D, Chandra Shekar M, Lyngaas I, Gao S, \nYin J, Tsaris A, Wang F, Glaser J (2022) Language models for the prediction \nof SARS-CoV-2 inhibitors. Int J High Perform Comput Appl 36:587\n 3. Minnich AJ, McLoughlin K, Tse M, Deng J, Weber A, Murad N, Madej BD, \nRamsundar B, Rush T, Calad-Thomson S, Brase J, Allen JE (2020) AMPL: a \ndata-driven modeling pipeline for drug discovery. J Chem Inform Model \n60(4):1955–1968. https:// doi. org/ 10. 1021/ acs. jcim. 9b010 53\n 4. Chen H, Engkvist O, Wang Y, Olivecrona M, Blaschke T (2018) The rise of \ndeep learning in drug discovery. Drug Discov Today 23(6):1241–1250. \nhttps:// doi. org/ 10. 1016/j. drudis. 2018. 01. 039\n 5. Acharya A, Agarwal R, Baker MB, Baudry J, Bhowmik D, Boehm S, Byler KG, \nChen SY, Coates L, Cooper CJ, Demerdash O, Daidone I, Eblen JD, Elling-\nson S, Forli S, Glaser J, Gumbart JC, Gunnels J, Hernandez O, Irle S, Kneller \nDW, Kovalevsky A, Larkin J, Lawrence TJ, LeGrand S, Liu S-H, Mitchell JC, \nPark G, Parks JM, Pavlova A, Petridis L, Poole D, Pouchard L, Ramanathan \nA, Rogers DM, Santos-Martins D, Scheinberg A, Sedova A, Shen Y, Smith \nJC, Smith MD, Soto C, Tsaris A, Thavappiragasam M, Tillack AF, Vermaas JV, \nVuong VQ, Yin J, Yoo S, Zahran M, Zanetti-Polzi L (2020) Supercomputer-\nbased ensemble docking drug discovery pipeline with application to \nCovid-19. J Chem Inf Model 60(12):5832–5852. https:// doi. org/ 10. 1021/ \nacs. jcim. 0c010 10\n 6. Cho E, Rosa M, Anjum R, Mehmood S, Soban M, Mujtaba M, Bux K, Moin \nST, Tanweer M, Dantu S, Pandini A, Yin J, Ma H, Ramanathan A, Islam B, \nMey ASJS, Bhowmik D, Haider S (2021) Dynamic profiling of β-coronavi-\nrus 3cl mpro protease ligand-binding sites. J Chem Inf Model 61(6):3058–\n3073. https:// doi. org/ 10. 1021/ acs. jcim. 1c004 49\n 7. Chen SH, Todd Young M, Gounley J, Stanley C, Bhowmik D (2021) How \ndistinct structural flexibility within sars-cov-2 spike protein reveals poten-\ntial therapeutic targets. IEEE. https:// doi. org/ 10. 1109/ BigDa ta525 89. 2021. \n96713 23\n 8. Bhowmik D, Gao S, Young MT, Ramanathan A (2018) Deep clustering of \nprotein folding simulations. BMC Bioinf 19(S18):484\n 9. Yang X, Wang Y, Byrne R, Schneider G, Yang S (2019) Concepts of \nartificial intelligence for computer-assisted drug discovery. Chem Rev \n119(18):10520–10594. https:// doi. org/ 10. 1021/ acs. chemr ev. 8b007 28\n 10. Enamine REAL Database. https:// enami ne. net/ compo und- colle ctions/ \nreal- compo unds/ real- datab ase. Accessed: 2020-04-01 through https:// \nvirtu al- flow. org/\n 11. Martins IF, Teixeira AL, Pinheiro L, Falcao AO (2012) A Bayesian approach \nto in Silico blood-brain barrier penetration modeling. J Chem Inf Model \n52(6):1686–1697. https:// doi. org/ 10. 1021/ ci300 124c\n 12. Subramanian G, Ramsundar B, Pande V, Denny RA (2016) Computa-\ntional modeling of β-secretase 1 (BACE-1) inhibitors using ligand based \napproaches. J Chem Inf Model 56(10):1936–1949. https:// doi. org/ 10. \n1021/ acs. jcim. 6b002 90\n 13. RDKit: Open-source cheminformatics. http:// www. rdkit. org\n 14. Jacobs SA, Moon T, McLoughlin K, Jones D, Hysom D, Ahn DH, Gyllenhaal \nJ, Watson P , Lightstone FC, Allen JE, Karlin I, Van Essen B (2021) Enabling \nrapid COVID-19 small molecule drug design through scalable deep learn-\ning of generative models. Int J High Perform Comput Appl. https:// doi. \norg/ 10. 1177/ 10943 42021 10109 30\n 15. Blanchard AE, Stanley C, Bhowmik D (2021) Using GANs with adaptive \ntraining data to search for new molecules. J Cheminform 13(1):4–11. \nhttps:// doi. org/ 10. 1186/ s13321- 021- 00494-3\n 16. De Cao N, Kipf T (2018) MolGAN: An implicit generative model for small \nmolecular graphs. ICML 2018 workshop on Theoretical Foundations and \nApplications of Deep Generative Models\n 17. Eiben AE, Smith JE (2015) Introduction to evolutionary computing, 2nd \nedn. Springer, Berlin\n 18. Virshup AM, Contreras-García J, Wipf P , Yang W, Beratan DN (2013) \nStochastic voyages into uncharted chemical space produce a repre-\nsentative library of all possible drug-like compounds. J Am Chem Soc \n135(19):7296–7303. https:// doi. org/ 10. 1021/ ja401 184g\n 19. Jensen JH (2019) A graph-based genetic algorithm and generative \nmodel/Monte Carlo tree search for the exploration of chemical space. \nChem Sci 10(12):3567–3572. https:// doi. org/ 10. 1039/ c8sc0 5372c\n 20. Brown N, McKay B, Gilardoni F, Gasteiger J (2004) A graph-based genetic \nalgorithm and its application to the multiobjective evolution of median \nmolecules. J Chem Inform Comput Sci 44(3):1079–1087. https:// doi. org/ \n10. 1021/ ci034 290p\n 21. Brown N, Fiscato M, Segler MHS, Vaucher AC (2019) GuacaMol: bench-\nmarking models for de novo molecular design. J Chem Inform Model \n59(3):1096–1108. https:// doi. org/ 10. 1021/ acs. jcim. 8b008 39\n 22. Lameijer EW, Kok JN, Bäck T, Ijzerman AP (2006) The molecule evolu-\nator. An interactive evolutionary algorithm for the design of drug-like \nmolecules. J Chem Inform Model 46(2):545–552. https:// doi. org/ 10. 1021/ \nci050 369d\n 23. Nicolaou CA, Apostolakis J, Pattichis CS (2009) De novo drug design using \nmultiobjective evolutionary graphs. J Chem Inform Model 49(2):295–307. \nhttps:// doi. org/ 10. 1021/ ci800 308h\n 24. Lameijer EW, Kok JN, Back T, Ijzerman AP (2006) Mining a chemical data-\nbase for fragment co-occurrence: discovery of “chemical clichés’’ . J Chem \nInform Model 46(2):553–562. https:// doi. org/ 10. 1021/ ci050 370c\n 25. Schneider G, Lee ML, Stahl M, Schneider P (2000) De novo design of \nmolecular architectures by evolutionary assembly of drug-derived build-\ning blocks. J Comput Aided Mol Design 14(5):487–494. https:// doi. org/ 10. \n1023/A: 10081 84403 558\n 26. Segler MHS, Kogej T, Tyrchan C, Waller MP (2018) Generating focused \nmolecule libraries for drug discovery with recurrent neural networks. ACS \nCentral Sci 4(1):120–131. https:// doi. org/ 10. 1021/ acsce ntsci. 7b005 12\n 27. Arés-Pous J, Johansson SV, Prykhodko O, Bjerrum EJ, Tyrchan C, Reymond \nJ-L, Reymond J-L, Chen H, Engkvist O (2019) Randomized smiles strings \nimprove the quality of molecular generative models. J Cheminform 11:1\n 28. Flam-Shepherd D, Zhu K, Aspuru-Guzik A (2022) Language models can \nlearn complex molecular distributions. Nat Commun 13(1):1–10. https:// \ndoi. org/ 10. 1038/ s41467- 022- 30839-\n 29. Awale M, Sirockin F, Stiefl N, Reymond J-L (2019) Drug analogs from \nfragment-based long short-term memory generative neural networks. J \nChem Inform Model 59(4):1347–1356. https:// doi. org/ 10. 1021/ acs. jcim. \n8b009 02\n 30. Zheng S, Yan X, Gu Q, Yang Y, Du Y, Lu Y, Xu J (2019) Qbmg: quasi-biogenic \nmolecule generator with deep recurrent neural network. J Cheminform \n11:1\n 31. Méndez-Lucio O, Baillif B, Clevert D-A, Rouquié D, Wichard JD (2018) De \nnovo generation of hit-like molecules from gene expression signatures \nusing artificial intelligence. Nat Commun 11:10\n 32. Fabian B, Edlich T, Gaspar H, Segler MHS, Meyers J, Fiscato M, Ahmed M \n(2020) Molecular representation learning with language models and \ndomain-relevant auxiliary tasks. ArXiv abs/2011.13230\n 33. Kim H, Na J, Lee WB (2021) Generative chemical transformer: Neural \nmachine learning of molecular geometric structures from chemical \nlanguage via attention. J Chem Inf Model 61(12):5804–5814. https:// doi. \norg/ 10. 1021/ acs. jcim. 1c012 89\n 34. Bagal V, Aggarwal R, Vinod PK, Priyakumar UD (2022) Molgpt: molecu-\nlar generation using a transformer-decoder model. J Chem Inf Model \n62(9):2064–2076. https:// doi. org/ 10. 1021/ acs. jcim. 1c006 00\n 35. Polykovskiy D, Zhebrak A, Sanchez-Lengeling B, Golovanov S, Tatanov O, \nBelyaev S, Kurbanov R, Artamonov A, Aladinskiy V, Veselov M, Kadurin A, \nJohansson S, Chen H, Nikolenko S, Aspuru-Guzik A, Zhavoronkov A (2020) \nMolecular sets (moses): a benchmarking platform for molecular genera-\ntion models. Front Pharmacol. https:// doi. org/ 10. 3389/ fphar. 2020. 565644\nPage 12 of 12Blanchard et al. Journal of Cheminformatics           (2023) 15:59 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 36. Devlin J, Chang MW, Lee K, Toutanova K (2019) BERT: Pre-training of deep \nbidirectional transformers for language understanding. NAACL HLT 2019 \n- 2019 Conference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies - Proceedings \nof the Conference 1(Mlm), 4171–4186. arXiv: 1810. 04805\n 37. Blanchard AE, Chandra Shekar M, Gao S, Gounley J, Lyngaas I, Glaser \nJ, Bhowmik D (2022) Automating genetic algorithm mutations for \nmolecules using a masked language model. IEEE Trans Evolut Comput. \nhttps:// doi. org/ 10. 1109/ TEVC. 2022. 31440 45\n 38. Weininger D (1998) SMILES, a chemical language and information system. \n1. Introduction to methodology and encoding rules. J Chem Inf Comput \nSci 28:31–36. https:// doi. org/ 10. 1021/ ci000 57a005\n 39. Schuster M, Nakajima K (2012) Japanese and korean voice search. In: 2012 \nIEEE International Conference on Acoustics, Speech and Signal Process-\ning (ICASSP), pp. 5149–5152. https:// doi. org/ 10. 1109/ ICASSP . 2012. 62890 \n79\n 40. Wu Y, Schuster M, Chen Z, Le QV, Norouzi M, Macherey W, Krikun M, Cao Y, \nGao Q, Macherey K, Klingner J, Shah A, Johnson M, Liu X, Kaiser Ł, Gouws \nS, Kato Y, Kudo T, Kazawa H, Stevens K, Kurian G, Patil N, Wang W, Young C, \nSmith J, Riesa J, Rudnick A, Vinyals O, Corrado G, Hughes M, Dean J (2016) \nGoogle’s Neural Machine Translation System: Bridging the Gap between \nHuman and Machine Translation, 1–23. arXiv: 1609. 08144\n 41. Bickerton GR, Paolini GV, Besnard J, Muresan S, Hopkins AL (2012) Quanti-\nfying the chemical beauty of drugs. Nat Chem 4(2):90–98. https:// doi. org/ \n10. 1038/ nchem. 1243\n 42. Ertl P , Schuffenhauer A (2009) Estimation of synthetic accessibility \nscore of drug-like molecules based on molecular complexity and \nfragment contributions. J Cheminf 1(1):1–11. https:// doi. org/ 10. 1186/ \n1758- 2946-1-8\n 43. jglaser/protein-ligand-mlp-1. https:// huggi ngface. co/ jglas er/ prote \nin- ligand- mlp-1\n 44. Aizman A, Maltby G, Breuel T (2019) High performance I/O for large scale \ndeep learning. In: 2019 IEEE International Conference on Big Data (Big \nData), pp. 5965–5967. IEEE\n 45. Ramakrishnan R, Dral PO, Rupp M, Von Lilienfeld OA (2014) Quantum \nchemistry structures and properties of 134 kilo molecules. Sci Data 1:1–7. \nhttps:// doi. org/ 10. 1038/ sdata. 2014. 22\n 46. gdb9 Dataset. http:// deepc hem. io. s3- websi te- us- west-1. amazo naws. \ncom/ datas ets/ gdb9. tar. gz. Accessed 28 May 2021\n 47. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, Cistac P , Rault \nT, Louf R, Funtowicz M, Davison J, Shleifer S, von Platen P , Ma C, Jernite \nY, Plu J, Xu C, Scao TL, Gugger S, Drame M, Lhoest Q, Rush AM (2020) \nTransformers: State-of-the-art natural language processing. In: Proceed-\nings of the 2020 Conference on Empirical Methods in Natural Language \nProcessing: System Demonstrations, pp. 38–45. Association for Compu-\ntational Linguistics, Online. https:// www. aclweb. org/ antho logy/ 2020. \nemnlp- demos.6\n 48. Rajbhandari S, Rasley J, Ruwase O, He Y (2020) Zero: Memory optimiza-\ntions toward training trillion parameter models. International Confer-\nence for High Performance Computing, Networking, Storage and \nAnalysis, SC 2020-Novem, 1–24. https:// doi. org/ 10. 1109/ SC414 05. 2020. \n00024.arXiv:1910.02054\n 49. Wang S, Guo Y, Wang Y, Sun H, Huang J (2019) Smiles-Bert: Large scale \nunsupervised pre-training for molecular property prediction. ACM-\nBCB 2019 - Proceedings of the 10th ACM International Conference on \nBioinformatics, Computational Biology and Health Informatics, 429–436. \nhttps:// doi. org/ 10. 1145/ 33073 39. 33421 86\n 50. Xue D, Zhang H, Xiao D, Gong Y, Chuai G, Sun Y, Tian H, Wu H, Li Y, Liu Q \n(2020) X-MOL: large-scale pre-training for molecular understanding and \ndiverse molecular analysis. bioRxiv. https:// doi. org/ 10. 1101/ 2020. 12. 23. \n424259\n 51. Kim H, Lee J, Ahn S, Lee JR (2021) A merged molecular representation \nlearning for molecular properties prediction with a web-based service. \nSci Rep 11(1):1–9. https:// doi. org/ 10. 1038/ s41598- 021- 90259-7\n 52. Gómez-Bombarelli R, Wei JN, Duvenaud D, Hernández-Lobato JM, \nánchez-Lengeling B, Sheberla D, Aguilera-Iparraguirre J, Hirzel TD, \nAdams RP , Aspuru-Guzik A, (2018) Automatic chemical design using a \ndata-driven continuous representation of molecules. ACS Central Sci \n4(2):268–276\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7441631555557251
    },
    {
      "name": "Training (meteorology)",
      "score": 0.5011780261993408
    },
    {
      "name": "Language model",
      "score": 0.4135604202747345
    },
    {
      "name": "Data science",
      "score": 0.34021756052970886
    },
    {
      "name": "Natural language processing",
      "score": 0.33876466751098633
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33013707399368286
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ]
}