{
  "title": "Script Induction as Language Modeling",
  "url": "https://openalex.org/W2250506749",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2010729824",
      "name": "Rachel Rudinger",
      "affiliations": [
        "Institute for Language and Speech Processing"
      ]
    },
    {
      "id": "https://openalex.org/A2139171091",
      "name": "Pushpendre Rastogi",
      "affiliations": [
        "Institute for Language and Speech Processing"
      ]
    },
    {
      "id": "https://openalex.org/A2134066734",
      "name": "Francis Ferraro",
      "affiliations": [
        "Institute for Language and Speech Processing"
      ]
    },
    {
      "id": "https://openalex.org/A2113965177",
      "name": "Benjamin Van Durme",
      "affiliations": [
        "Institute for Language and Speech Processing",
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2164585080",
    "https://openalex.org/W1508977358",
    "https://openalex.org/W1846895676",
    "https://openalex.org/W2145374219",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2250669704",
    "https://openalex.org/W1702669762",
    "https://openalex.org/W2169943035",
    "https://openalex.org/W1965605789",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W1662133657",
    "https://openalex.org/W2252052418",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W2250836735",
    "https://openalex.org/W2000900121",
    "https://openalex.org/W2089745520",
    "https://openalex.org/W69597389",
    "https://openalex.org/W2117226699",
    "https://openalex.org/W2335559472",
    "https://openalex.org/W2158794898",
    "https://openalex.org/W2252215150",
    "https://openalex.org/W1593045043"
  ],
  "abstract": "The narrative cloze is an evaluation metric commonly used for work on automatic script induction.While prior work in this area has focused on count-based methods from distributional semantics, such as pointwise mutual information, we argue that the narrative cloze can be productively reframed as a language modeling task.By training a discriminative language model for this task, we attain improvements of up to 27 percent over prior methods on standard narrative cloze metrics.",
  "full_text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1681–1686,\nLisbon, Portugal, 17-21 September 2015.c⃝2015 Association for Computational Linguistics.\nScript Induction as Language Modeling\nRachel Rudinger1, Pushpendre Rastogi1, Francis Ferraro1, and Benjamin Van Durme1,2\n1Center for Language and Speech Processing\n2Human Language Technology Center of Excellence\nJohns Hopkins University\nAbstract\nThe narrative cloze is an evaluation met-\nric commonly used for work on automatic\nscript induction. While prior work in this\narea has focused on count-based meth-\nods from distributional semantics, such as\npointwise mutual information, we argue\nthat the narrative cloze can be productively\nreframed as a language modeling task. By\ntraining a discriminative language model\nfor this task, we attain improvements of up\nto 27 percent over prior methods on stan-\ndard narrative cloze metrics.\n1 Introduction\nAlthough the concept of scripts in artiﬁcial intelli-\ngence dates back to the 1970s (Schank and Abel-\nson, 1977), interest in this topic has renewed with\nrecent efforts to automatically induce scripts from\ntext on a large scale. One particularly inﬂuential\nwork in this area, Chambers and Jurafsky (2008),\ntreats the problem of script induction as one of\nlearning narrative chains, which they accomplish\nusing simple textual co-occurrence statistics. For\nthe novel task of learning narrative chains, they\nintroduce a new evaluation metric, the narrative\ncloze test, which involves predicting a missing\nevent from a chain of events drawn from text.\nSeveral follow-up works (Chambers and Jurafsky,\n2009; Jans et al., 2012; Pichotta and Mooney,\n2014; Rudinger et al., 2015) employ and ex-\ntend Chambers and Jurafsky (2008)’s methods for\nlearning narrative chains, each using the narrative\ncloze to evaluate their work. 1\nIn this paper, we take the position that the nar-\nrative cloze test, which has been treated predom-\n1A number of related works on script induction use alter-\nnative task formulations and evaluations. (Chambers, 2013;\nCheung et al., 2013; Cheung and Penn, 2013; Frermann et\nal., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Reg-\nneri et al., 2010)\ninantly as a method for evaluating script knowl-\nedge, is more productively thought of simply as a\nlanguage modeling task. 2 To support this claim,\nwe demonstrate a marked improvement over pre-\nvious methods on this task using a powerful dis-\ncriminative language model – the Log-Bilinear\nmodel (LBL). Based on this ﬁnding, we believe\none of the following conclusions must follow: ei-\nther discriminative language models are a more\neffective technique for script induction than pre-\nvious methods, or the narrative cloze test is not a\nsuitable evaluation for this task.3\n2 Task Deﬁnition\nFollowing the deﬁnitions of Chambers and Juraf-\nsky (2008), a narrative chain is “a partially or-\ndered set of narrative events that share a common\nactor,” where a narrative event is “a tuple of an\nevent (most simply a verb) and its participants,\nrepresented as typed dependencies.” (De Marneffe\net al., 2006) Formally, e := (v, d), where e is a\nnarrative event, v is a verb lemma, and d is the\nsyntactic dependency ( nsubj or dobj) between v\nand the protagonist. As an example, consider the\nfollowing narrative:\nJohn studied for the exam and aced it.\nHis teacher congratulated him.\nWith John as protagonist, we have a se-\nquence of three narrative events: (study, nsubj),\n(ace, nsubj), and (congratulate, dobj).\nIn the narrative cloze test, a sequence of nar-\nrative events (like the example provided here) is\nextracted automatically from a document, and one\n2Manshadi et al. (2008) also take a language modeling\napproach to event prediction, although their experiments are\nnot directly comparable.\n3We note that, whether the narrative cloze was originally\nintended as a rigorous evaluation of script induction tech-\nniques or merely a preliminary metric, we are motivated by\nthe observation that this evaluation has nonetheless become a\nstandard metric for this task.\n1681\nnarrative event is removed; the task is to predict\nthe missing event.\nData Each of the models discussed in the fol-\nlowing section are trained and tested on chains of\nnarrative events extracted from stories in the New\nYork Times portion of the Gigaword corpus (Graff\net al., 2003) with Concrete annotations (Ferraro et\nal., 2014). Training is on the entirety of the 1994–\n2006 portion (16,688,422 chains with 58,515,838\nnarrative events); development is a subset of the\n2007–2008 portion (10,000 chains with 35,109\nevents); and test is a subset of the 2009–2010 por-\ntion (5,000 chains with 17,836 events). All ex-\ntracted chains are of length two or greater.\nChain Extraction To extract chains of narra-\ntive events for training and testing, we rely on\nthe (automatically-generated) coreference chains\npresent in Concretely Annotated Gigaword. Each\nnarrative event in an extracted chain is derived\nfrom a single mention in the corresponding coref-\nerence chain, i.e., it consists of the verb and syn-\ntactic dependency ( nsubj or dobj) that governs\nthe head of the mention, if such a dependency ex-\nists. Overlapping mentions within a coreference\nchain are collapsed to a single mention to avoid\nredundant extractions.\n3 Models\nIn this section we present each of the models we\ntrain for the narrative cloze evaluation. In a sin-\ngle narrative cloze test, a sequence of narrative\nevents, (e1, ··· , eL), with an insertion point, k,\nfor the missing event is provided. Given a ﬁxed\nvocabulary of narrative events, V, a candidate se-\nquence is generated for each vocabulary item by\ninserting that item into the sequence at index k.\nEach model generates a score for the candidate se-\nquences, yielding a ranking over the vocabulary\nitems. The rank assigned to the actual missing vo-\ncabulary item is the score the model receives on\nthat cloze test. In this case, we set Vto include\nall narrative events, e, that occur at least ten times\nin training, yielding a vocabulary size of 12,452.\nAll out-of-vocabulary events are converted to (and\nscored as) the symbol UNK .\n3.1 Count-based Methods\nUnigram Baseline ( UNI ) A simple but strong\nbaseline introduced by Pichotta and Mooney\n(2014) for this task is the unigram model: can-\ndidates are ranked by their observed frequency in\ntraining, without regard to context.\nUnordered PMI ( UOP ) The original model for\nthis task, proposed by Chambers and Jurafsky\n(2008), is based on the pointwise mutual informa-\ntion (PMI) between events.\npmi(e1, e2) ∝ log C(e1, e2)\nC(e1, ∗)C(∗, e2) (1)\nHere, C(e1, e2) is the number of times e1 and e2\noccur in the same narrative event sequence, i.e.,\nthe number of times they “had a coreferring entity\nﬁlling the values of [their] dependencies,” and the\nordering of e1 and e2 is not considered. In our\nimplementation, individual counts are deﬁned as\nfollows:\nC(e, ∗) :=\n∑\ne′∈V\nC(e, e′) (2)\nThis model selects the best candidate event in a\ngiven cloze test according to the following score:\nˆe = arg max\ne∈V\nL∑\ni=1\npmi(e, ei) (3)\nWe tune this model with an option to apply a mod-\niﬁed version of discounting for PMI from Pantel\nand Ravichandran (2004).\nOrdered PMI (OP) This model is a slight vari-\nation on Unordered PMI introduced by Jans et al.\n(2012). The only distinction is that C(e1, e2) is\ntreated as an asymmetric count, sensitive to the or-\nder in which e1 and e2 occur within a chain.\nBigram Probability (BG) Another variant intro-\nduced by Jans et al. (2012), the “bigram proba-\nbility” model uses conditional probabilities rather\nthan PMI to compute scores. In a cloze test, this\nmodel selects the following event:\nˆe = arg max\ne∈V\nk∏\ni=1\np(e|ei)\nL∏\ni=k+1\np(ei|e) (4)\nwhere p(e2|e1) = C(e1,e2)\nC(e1,∗) and C(e1, e2) is asym-\nmetric. We tune this model with an option to per-\nform absolute discounting. Note that this model is\nnot a bigram model in the typical language mod-\neling sense.\n1682\nLen UNI UOP OP BG LBL2 LBL4 Tests\n2 490 1887 2363 1613 369 371 5668\n3 452 1271 1752 1009 330 334 2793\n4 323 806 1027 502 229 232 1616\n5 364 735 937 442 254 243 1330\n6 347 666 891 483 257 249 942\n7 330 629 838 468 241 237 630\n8 259 466 510 278 208 201 512\n9 299 610 639 348 198 195 396\n10+ 331 472 397 277 240 229 3949\nALL 400 1115 1382 868 294 292 17836\n(a) Average Rank\nLen UNI UOP OP BG LBL2 LBL4 Tests\n2 .148 .053 .077 .149 .205 .204 5668\n3 .179 .043 .065 .164 .217 .215 2793\n4 .226 .042 .064 .195 .253 .253 1616\n5 .225 .049 .076 .213 .261 .266 1330\n6 .213 .054 .079 .214 .254 .263 942\n7 .213 .061 .092 .215 .243 .247 630\n8 .235 .063 .091 .244 .268 .278 512\n9 .259 .058 .107 .252 .280 .278 396\n10+ .191 .082 .113 .193 .198 .205 3949\nALL .186 .057 .083 .181 .221 .223 17836\n(b) Mean Reciprocal Rank (MRR)\nLen UNI UOP OP BG LBL2 LBL4 Tests\n2 23.9 09.4 11.9 23.8 34.0 34.1 5668\n3 28.8 08.2 11.1 28.0 36.3 35.6 2793\n4 33.9 07.7 14.4 32.2 38.7 38.7 1616\n5 33.4 10.1 18.7 34.0 39.6 40.3 1330\n6 34.8 10.9 22.2 36.8 40.5 41.9 942\n7 32.5 12.2 24.0 34.9 39.4 39.2 630\n8 36.7 13.7 21.7 38.7 41.6 43.2 512\n9 37.9 15.2 28.5 39.1 41.7 43.2 396\n10+ 31.4 18.5 24.0 32.7 35.7 35.7 3949\nALL 29.5 11.6 16.8 29.8 36.5 36.6 17836\n(c) Percent Recall at 10\nLen UNI UOP OP BG LBL2 LBL4 Tests\n2 41.7 16.9 25.5 38.6 51.2 51.0 5668\n3 46.8 20.2 30.2 45.0 54.8 54.0 2793\n4 53.8 25.3 37.8 54.0 59.0 60.0 1616\n5 52.5 29.9 40.5 54.3 59.1 61.1 1330\n6 53.9 33.2 40.7 55.2 60.6 61.7 942\n7 51.8 34.3 42.7 56.5 61.6 63.8 630\n8 58.2 42.2 47.7 61.3 67.2 67.0 512\n9 58.1 42.2 47.7 60.1 66.2 67.0 396\n10+ 49.9 47.4 50.1 54.2 58.4 59.8 3949\nALL 48.0 28.6 36.4 48.3 56.3 56.8 17836\n(d) Percent Recall at 50\nTable 1: Narrative cloze results bucketed by chain length for each model and scoring metric with best results in bold. The\nmodels are Unigram Model ( UNI ), Unordered PMI ( UOP ), Ordered PMI ( OP), Bigram Probability Model ( BG), Log-Bilinear\nModel N=2 (LBL 2), Log-Bilinear Model N=4 (LBL 4)\nSkip N-gram We tune the previous three\nmodels ( UOP , OP, and BG) with the skip n-gram\ncounting methods introduced by Jans et al. (2012)\nfor this task, varying the ways in which the\ncounts, C(e1, e2), are collected. Using skip-n\ncounting, C(e1, e2) is incremented every time e1\nand e2 co-occur within a window of size n. We\nexperiment with skip-0 (consecutive events only),\nskip-3 (window size 3), and skip-all (entire chain\nlength) settings.\nFor each of the four narrative cloze scoring\nmetrics we report on (average rank, mean re-\nciprocal rank, recall at 10, and recall at 50),\nwe tune the Unordered PMI, Ordered PMI, and\nBigram Probability models over the following\nparameter space: {skip-0, skip-3, skip-all} ×\n{discount, no-discount}×{ T=4, T=10, T=20},\nwhere T is a pairwise count threshold.\n3.2 A Discriminative Method\nLog-Bilinear Language Model ( LBL ) The\nLog-Bilinear language model is a language model\nthat was introduced by Mnih and Hinton (2007).\nLike other language models, the LBL produces\na probability distribution over the next possible\nword given a sequence of N previously observed\nwords. N is a hyper-parameter that determines the\nsize of the context used for computing the prob-\nabilities. While many variants of the LBL have\nbeen proposed since its introduction, we use the\nsimple variant described below.\nFormally, we associate one context vector ce ∈\nRd, one bias parameter be ∈ R, and one tar-\nget vector te ∈ Rd to each narrative event\ne ∈ V∪{ UNK , BOS , EOS }. V is the vocab-\nulary of events and BOS , EOS , and UNK are the\nbeginning-of-sequence, end-of-sequence, and out-\nof-vocabulary symbols, respectively. The proba-\nbility of an event e that appears after a sequence\ns = [s1, s2, . . . , sN ] of context words is deﬁned\nas:\np(e|s) = exp(t⊺\neˆts + be)∑\ne′∈V∪{UNK , EOS }\nexp(t⊺\ne′ ˆts + be′ )\nwhere ˆts =\nN∑\nj=1\nmj ⊙csj\nThe ⊙operator performs element-wise multiplica-\ntion of two vectors. The parameters that are opti-\nmized during training are mj ∀j ∈[1, . . . , N] and\nce, te ∀e ∈ V∪{UNK , BOS , EOS }. To calcu-\nlate the log-probability of a sequence of narrative\nevents E = (e1, . . . , eL) we compute:\nl(S) =\n( n∑\ni=1\nlog(p(ei|fE(ei)))\n)\n+ log(p(EOS |fE(EOS )))\n(5)\nHere fE is a function that returns the sequence\nof N words that precede the event ei in the se-\n1683\n0\n500\n1000\nUNI UOP OP BG LBL2LBL4\nmodel\navgrnk\n0\n10\n20\n30\nUNI UOP OP BG LBL2 LBL4\nmodel\nrec10\n0.00\n0.05\n0.10\n0.15\n0.20\nUNI UOP OP BG LBL2LBL4\nmodel\nmrr\n0\n20\n40\nUNI UOP OP BG LBL2 LBL4\nmodel\nrec50\nFigure 1: Narrative cloze results over all chain lengths. Unigram Model ( UNI ), Unordered PMI Model ( UOP ), Ordered PMI\nModel ( OP), Bigram Probability Model ( BG), Log-Bilinear Model with context size 2 or 4 ( LBL 2, LBL 4). Average Rank\n(avgrnk), Mean Reciprocal Rank (mrr), % Recall at 10 (rec10), % Recall at 50 (rec50).\nquence E′made by prepending N BOS tokens and\nappending a single EOS token to E.\nThe LBL models are trained by minimizing\nthe objective described in Equation 5 for all the\nsequences in the training corpus. We used the\nOxLM toolkit (Paul et al., 2014) which internally\nuses Noise-Contrastive Estimation (NCE) (Gut-\nmann and Hyv ¨arinen, 2010) and processor paral-\nlelization for speeding up the training. For this\ntask, we train LBL models with N = 2 (LBL 2)\nand N = 4 (LBL 4). In our experiments, increas-\ning context size to N = 6 did not signiﬁcantly\nimprove (or degrade) performance.\n4 Experimental Results\nTable 1 shows the results of 17,836 narrative cloze\ntests (derived from 5,000 held-out test chains),\nwith results bucketed by chain length. Perfor-\nmance is reported on four metrics: average rank,\nmean reciprocal rank, recall at 10, and recall at 50.\nFor each of the four metrics, the best overall\nperformance is achieved by one of the two LBL\nmodels (context size 2 or 4); the LBL models\nalso achieve the best performance on every chain\nlength. Not only are the gains achieved by the\ndiscriminative LBL consistent across metrics and\nchain length, they are large. For average rank, the\nLBL achieves a 27.0% relative improvement over\nthe best non-discriminative model; for mean re-\nciprocal rank, a 19.9% improvement; for recall at\n10, a 22.8% improvement; and for recall at 50,\na 17.6% improvement. (See Figure 1.) Further-\nmore, note that both PMI models and the Bigram\nmodel have been individually tuned for each met-\nric, while the LBL models have not. (The two LBL\nmodels are tuned only for overall perplexity on the\ndevelopment set.)\nAll models trend toward improved performance\non longer chains. Because the unigram model also\nimproves with chain length, it appears that longer\nchains contain more frequent events and are thus\neasier to predict. However, LBL performance is\nalso likely improving on longer chains because\nof additional contextual information, as is evident\nfrom LBL 4’s slight relative gains over LBL 2 on\nlonger chains.\n5 Conclusion\nPointwise mutual information and other related\ncount-based techniques have been used widely\nto identify semantically similar words (Church\nand Hanks, 1990; Lin and Pantel, 2001; Tur-\n1684\nney and Pantel, 2010), so it is natural that these\ntechniques have also been applied to the task\nof script induction. Qualitatively, PMI often\nidentiﬁes intuitively compelling matches; among\nthe top 15 events to share a high PMI with\n(eat, nsubj) under the Unordered PMI model, for\nexample, we ﬁnd events such as(overeat, nsubj),\n(taste, nsubj), (smell, nsubj), (cook, nsubj),\nand (serve, dobj). When evaluated by the narra-\ntive cloze test, however, these count-based meth-\nods are overshadowed by the performance of a\ngeneral-purpose discriminative language model.\nOur decision to attempt this task with the Log-\nBilinear model was motivated by the simple ob-\nservation that the narrative cloze test is, in reality,\na language modeling task. Does the LBL’s suc-\ncess on this task mean that work in script induc-\ntion should abandon traditional count-based meth-\nods for discriminative language modeling tech-\nniques? Or does it mean that an alternative eval-\nuation metric is required to measure script knowl-\nedge? While we believe our results are sufﬁcient\nto conclude that one of these alternatives is the\ncase, we leave the task of determining which to\nfuture research.\nAcknowledgments\nThis work was supported by the Paul Allen In-\nstitute for Artiﬁcial Intelligence ( Acquisition and\nUse of Paraphrases in a Knowledge-Rich Setting),\na National Science Foundation Graduate Research\nFellowship (Grant No. DGE-1232825), the Johns\nHopkins HLTCOE, and DARPA DEFT (FA8750-\n13-2-001, Large Scale Paraphrasing for Natural\nLanguage Understanding). We would also like to\nthank three anonymous reviewers for their feed-\nback. Any opinions expressed in this work are\nthose of the authors.\nReferences\nNathanael Chambers and Dan Jurafsky. 2008. Unsu-\npervised learning of narrative event chains. In Pro-\nceedings of ACL-08: HLT , pages 789–797, Colum-\nbus, Ohio. Association for Computational Linguis-\ntics.\nNathanael Chambers and Dan Jurafsky. 2009. Unsu-\npervised learning of narrative schemas and their par-\nticipants. In Proceedings of the Joint Conference of\nthe 47th Annual Meeting of the ACL and the 4th In-\nternational Joint Conference on Natural Language\nProcessing of the AFNLP , pages 602–610, Suntec,\nSingapore. Association for Computational Linguis-\ntics.\nNathanael Chambers. 2013. Event schema induction\nwith a probabilistic entity-driven model. InEMNLP,\nvolume 13, pages 1797–1807.\nJackie Chi Kit Cheung and Gerald Penn. 2013. Prob-\nabilistic domain modelling with contextualized dis-\ntributional semantic vectors. In Proceedings of the\n51st Annual Meeting of the Association for Compu-\ntational Linguistics, pages 392–401. Association for\nComputational Linguistics.\nJackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-\nderwende. 2013. Probabilistic frame induction. In\nProceedings of the 2013 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 837–846, Atlanta, Georgia, June. Association\nfor Computational Linguistics.\nKenneth Ward Church and Patrick Hanks. 1990. Word\nassociation norms, mutual information, and lexicog-\nraphy. Computational linguistics, 16(1):22–29.\nMarie-Catherine De Marneffe, Bill MacCartney,\nChristopher D Manning, et al. 2006. Generat-\ning typed dependency parses from phrase structure\nparses. In Proceedings of LREC , volume 6, pages\n449–454.\nFrancis Ferraro, Max Thomas, Matthew R. Gormley,\nTravis Wolfe, Craig Harman, and Benjamin Van\nDurme. 2014. Concretely Annotated Corpora. In\n4th Workshop on Automated Knowledge Base Con-\nstruction (AKBC).\nLea Frermann, Ivan Titov, and Manfred Pinkal. 2014.\nA hierarchical bayesian model for unsupervised in-\nduction of script knowledge. EACL 2014, page 49.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki\nMaeda. 2003. English gigaword. Linguistic Data\nConsortium, Philadelphia.\nMichael Gutmann and Aapo Hyv ¨arinen. 2010. Noise-\ncontrastive estimation: A new estimation princi-\nple for unnormalized statistical models. In Inter-\nnational Conference on Artiﬁcial Intelligence and\nStatistics, pages 297–304.\nBram Jans, Steven Bethard, Ivan Vuli ´c, and Marie-\nFrancine Moens. 2012. Skip n-grams and ranking\nfunctions for predicting script events. In Proceed-\nings of the 13th Conference of the European Chap-\nter of the Association for Computational Linguis-\ntics, pages 336–344, Avignon, France. Association\nfor Computational Linguistics.\nDekang Lin and Patrick Pantel. 2001. Dirt - discovery\nof inference rules from text. In Proceedings of the\nseventh ACM SIGKDD international conference on\nKnowledge discovery and data mining , pages 323–\n328. ACM.\n1685\nMehdi Manshadi, Reid Swanson, and Andrew S Gor-\ndon. 2008. Learning a probabilistic model of event\nsequences from internet weblog stories. In FLAIRS\nConference, pages 159–164.\nAndriy Mnih and Geoffrey Hinton. 2007. Three new\ngraphical models for statistical language modelling.\nIn Proceedings of the 24th international conference\non Machine learning, pages 641–648. ACM.\nAshutosh Modi and Ivan Titov. 2014. Inducing neural\nmodels of script knowledge. CoNLL-2014, page 49.\nPatrick Pantel and Deepak Ravichandran. 2004.\nAutomatically labeling semantic classes. In\nDaniel Marcu Susan Dumais and Salim Roukos, ed-\nitors, HLT-NAACL 2004: Main Proceedings, pages\n321–328, Boston, Massachusetts, USA. Association\nfor Computational Linguistics.\nBaltescu Paul, Blunsom Phil, and Hoang Hieu. 2014.\nOxlm: A neural language modelling framework for\nmachine translation. The Prague Bulletin of Mathe-\nmatical Linguistics, 102(1):81–92.\nKarl Pichotta and Raymond Mooney. 2014. Statisti-\ncal script learning with multi-argument events. In\nProceedings of the 14th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 220–229, Gothenburg, Sweden. As-\nsociation for Computational Linguistics.\nMichaela Regneri, Alexander Koller, and Manfred\nPinkal. 2010. Learning script knowledge with web\nexperiments. In Proceedings of the 48th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 979–988. Association for Computa-\ntional Linguistics.\nRachel Rudinger, Vera Demberg, Ashutosh Modi, Ben-\njamin Van Durme, and Manfred Pinkal. 2015.\nLearning to predict script events from domain-\nspeciﬁc text. Lexical and Computational Semantics\n(* SEM 2015), page 205.\nRoger Schank and Robert Abelson. 1977. Scripts,\nplans, goals and understanding: An inquiry into hu-\nman knowledge structures. Lawrence Erlbaum As-\nsociates, Hillsdale, NJ.\nPeter D. Turney and Patrick Pantel. 2010. From\nfrequency to meaning: Vector space models of se-\nmantics. Journal of Artiﬁcial Intelligence Research,\n37(1):141–188, January.\n1686",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7139232754707336
    },
    {
      "name": "Programming language",
      "score": 0.4517688751220703
    },
    {
      "name": "Natural language processing",
      "score": 0.44043612480163574
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210098364",
      "name": "Institute for Language and Speech Processing",
      "country": "GR"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ],
  "cited_by": 97
}