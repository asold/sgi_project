{
  "title": "LLMChain: Blockchain-Based Reputation System for Sharing and Evaluating Large Language Models",
  "url": "https://openalex.org/W4395064856",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Bouchiha, Mouhamed Amine",
      "affiliations": [
        "La Rochelle Université"
      ]
    },
    {
      "id": null,
      "name": "Telnoff, Quentin",
      "affiliations": [
        "La Rochelle Université"
      ]
    },
    {
      "id": "https://openalex.org/A4281963750",
      "name": "Bakkali, Souhail",
      "affiliations": [
        "La Rochelle Université"
      ]
    },
    {
      "id": "https://openalex.org/A4290617010",
      "name": "Champagnat, Ronan",
      "affiliations": [
        "La Rochelle Université"
      ]
    },
    {
      "id": "https://openalex.org/A4295653168",
      "name": "Rabah, Mourad",
      "affiliations": [
        "La Rochelle Université"
      ]
    },
    {
      "id": "https://openalex.org/A2746772621",
      "name": "Coustaty, Mickaël",
      "affiliations": [
        "La Rochelle Université"
      ]
    },
    {
      "id": "https://openalex.org/A4207291361",
      "name": "Ghamri-Doudane, Yacine",
      "affiliations": [
        "La Rochelle Université"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389523957",
    "https://openalex.org/W6852874933",
    "https://openalex.org/W3112103703",
    "https://openalex.org/W4389518784",
    "https://openalex.org/W6846922846",
    "https://openalex.org/W4396665924",
    "https://openalex.org/W4390035079",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W4386566861",
    "https://openalex.org/W4319314361",
    "https://openalex.org/W6853108251",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W2997035848",
    "https://openalex.org/W2724474800",
    "https://openalex.org/W4385210042",
    "https://openalex.org/W4386915056",
    "https://openalex.org/W4302552924",
    "https://openalex.org/W4288063939",
    "https://openalex.org/W3088538988",
    "https://openalex.org/W3033135532",
    "https://openalex.org/W3198185738",
    "https://openalex.org/W6797816506",
    "https://openalex.org/W4386576863",
    "https://openalex.org/W2800029302",
    "https://openalex.org/W2004373800",
    "https://openalex.org/W3003575154",
    "https://openalex.org/W4205528004",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W4377163996",
    "https://openalex.org/W4387596421",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4390833061",
    "https://openalex.org/W1844512392",
    "https://openalex.org/W2990138404"
  ],
  "abstract": "Large Language Models (LLMs) have witnessed rapid growth in emerging challenges and capabilities of language understanding, generation, and reasoning. Despite their remarkable performance in natural language processing-based applications, LLMs are susceptible to undesirable and erratic behaviors, including hallucinations, unreliable reasoning, and the generation of harmful content. These flawed behaviors undermine trust in LLMs and pose significant hurdles to their adoption in real-world applications, such as legal assistance and medical diagnosis, where precision, reliability, and ethical considerations are paramount. These could also lead to user dissatisfaction, which is currently inadequately assessed and captured. Therefore, to effectively and transparently assess users' satisfaction and trust in their interactions with LLMs, we design and develop LLMChain, a decentralized blockchain-based reputation system that combines automatic evaluation with human feedback to assign contextual reputation scores that accurately reflect LLM's behavior. LLMChain not only helps users and entities identify the most trustworthy LLM for their specific needs, but also provides LLM developers with valuable information to refine and improve their models. To our knowledge, this is the first time that a blockchain-based distributed framework for sharing and evaluating LLMs has been introduced. Implemented using emerging tools, LLMChain is evaluated across two benchmark datasets, showcasing its effectiveness and scalability in assessing seven different LLMs.",
  "full_text": "LLMChain: Blockchain-based Reputation System\nfor Sharing and Evaluating Large Language Models\nMouhamed Amine Bouchiha, Quentin Telnoff, Souhail Bakkali, Ronan Champagnat, Mourad Rabah,\nMicka¨el Coustaty, Yacine Ghamri-Doudane\nL3i - La Rochelle University, La Rochelle, France\n{mouhamed.bouchiha, quentin.telnoff, souhail.bakkali, ronan.champagnat, mourad.rabah, mickael.coustaty, yacine.ghamri }@univ-lr.fr\nAbstract—Large Language Models (LLMs) have witnessed a\nrapid growth in emerging challenges and capabilities of lan-\nguage understanding, generation, and reasoning. Despite their\nremarkable performance in natural language processing-based\napplications, LLMs are susceptible to undesirable and erratic\nbehaviors, including hallucinations, unreliable reasoning, and the\ngeneration of harmful content. These flawed behaviors under-\nmine trust in LLMs and pose significant hurdles to their adoption\nin real-world applications, such as legal assistance and medical\ndiagnosis, where precision, reliability, and ethical considerations\nare paramount. These could also lead to user dissatisfaction,\nwhich is currently inadequately assessed and captured. Therefore,\nto effectively and transparently assess users’ satisfaction and\ntrust in their interactions with LLMs, we design and develop\nLLMChain, a decentralized blockchain-based reputation system\nthat combines automatic evaluation with human feedback to\nassign contextual reputation scores that accurately reflect LLM’s\nbehavior. LLMChain helps users and entities identify the most\ntrustworthy LLM for their specific needs and provides LLM\ndevelopers with valuable information to refine and improve their\nmodels. To our knowledge, this is the first time that a blockchain-\nbased distributed framework for sharing and evaluating LLMs\nhas been introduced. Implemented using emerging tools, LLM-\nChain is evaluated across two benchmark datasets, showcasing\nits effectiveness and scalability in assessing seven different LLMs.\nIndex Terms —Blockchain, LLMs, Decentralized Reputation,\nTransparency, Human Feedback, Automatic Evaluation.\nPaper accepted at IEEE 48th Annual Computers, Software, and Applications\nConference (COMPSAC) IEEE, Osaka, Japan (2024).\nI. I NTRODUCTION\nL\nARGE Language Models (LLMs) have received a great\ndeal of attention in the last few years due to their\nsurprising capabilities in managing a wide range of Natural\nLanguage Processing (NLP) tasks including information re-\ntrieval, language understanding, generation, and reasoning [1],\n[2]. Despite their impressive capabilities, LLMs such as\nGPT-3, Llama, and Vicuna [3]–[5] exhibit certain challenges\nthat compromise their efficacy. One prominent issue is the\nmanifestation of biases and fairness concerns. LLMs often\ninherit biases present in their training data, reflecting societal\nprejudices and stereotypes [6]. Consequently, these models\ncan produce outputs that perpetuate or even exacerbate ex-\nisting social inequalities. Another limitation arises from the\nmodels’ difficulty in grasping common sense and contextual\nunderstanding. LLMs may struggle to interpret nuances in\nlanguage, leading to responses that appear nonsensical or\ndetached from real-world knowledge [7]. These behaviors\nencompass hallucinations, evident in the generation of text that\ninvents or imagines information lacking a factual or coherent\nbasis [8]. LLMs may also display unreliable reasoning [9],\ncharacterized by a lack of consistent or dependable logical\nabilities. Furthermore, there is a risk of generating harmful\ncontent [10], where LLMs may produce material that is of-\nfensive, inappropriate, or potentially harmful. These behaviors\ncan significantly deviate from the expected or desired output,\nundermining the credibility of LLMs and posing challenges to\ntheir widespread adoption. In summary, these flawed actions\nthat diminish trust in LLMs cause users to be cautious about\nrelying on AI-generated content due to its unpredictability\nand potential for producing incorrect information. They also\npresent hurdles to the utilization of LLMs in critical contexts\nsuch as medical diagnosis, legal advice, or sensitive informa-\ntion processing, where accuracy and reliability are essential.\nOne key way to assess the behavior of LLMs and measure\ntheir reliability involves soliciting inputs from users. Indi-\nviduals can highlight issues they encounter while engaging\nwith AI-generated content [11]. However, this method has two\nnotable drawbacks. First, collecting user feedback is costly as\nit requires analyzing and categorizing the gathered informa-\ntion. Second, human feedback lacks real-time capabilities as\nusers might not offer immediate responses. This delay hinders\nprompt evaluation given the absence of instant responses from\nhumans. Therefore, to reduce reliance on human involvement,\nan alternative strategy consists of employing automatic eval-\nuation methods. These techniques leverage automated feed-\nback [2], [10] or language models [12], [13] to evaluate LLMs’\nperformance in a cost-effective way. Despite the efficient\nprocessing of language data generated by LLMs, the automatic\nevaluation metrics they rely on may not perfectly align with\nhuman preferences or perceptions, thereby introducing certain\nlimitations. These assessments may fail to capture nuances or\nqualitative aspects that are crucial for understanding how users\nperceive the content generated by LLMs [14]. Additionally,\nexisting human and automatic evaluation-based methods face\nmany challenges linked to the lack of transparency and de-\ncentralization, as they currently all operate within centralized\nframeworks. Entities wishing to use LLMs for specific tasks\nmust choose between trusting centralized third-party evalua-\ntions or independent testing, which is a costly process that\narXiv:2404.13236v2  [cs.DC]  3 May 2024\ndepends on the availability of code and data. Moreover, most\nof the recent studies concentrate on either human feedback\nor automated evaluation [10], [11], [15], [16], missing the\nopportunity to capture human preferences while enhancing\nscalability and reducing costs.\nTo address the above-mentioned issues of evaluating LLMs\neffectively, dynamically, and transparently, we propose LLM-\nChain, which leverages Blockchain (BC) technology to build\na reputation system for LLMs. Blockchains have found exten-\nsive use in various trust-related applications such as supply\nchain [17], crowdsourcing [18], and e-commerce platforms\n[19]. Its utilization is particularly essential for the develop-\nment of efficient, decentralized, and transparent reputation\nsystems. These attributes are precisely the qualities we have\nalways envisioned for developing robust reputation systems.\nBlockchain - known for its resistance to tampering - can be\nused to track and manage the reputation of various LLMs\nvia smart contracts. LLMChain’s primary goal is to help\nusers find the most reliable LLM that meets their specific\nneeds and preferences. Therefore, it allows these individu-\nals to use language models shared by LLM providers and\nactively participate in the evaluation process. Additionally,\nit provides LLM developers with valuable insights, enabling\nthem to enhance and optimize their models by incorporating\nhuman feedback. Besides, it is discouraged within reputable\norganizations for employees to disseminate professional data\nonline or to external entities, a practice that is frequently\nobserved with commercial LLMs. LLMChain aims to address\nthis issue by enabling these organizations to identify open-\nsource LLMs that meet their needs and capabilities for local\ndeployment. This privacy assurance also extends to users who\nprefer not to share their activities and personal data with third\nparties. In summary, the contributions of this paper are:\n• A new reputation-based model. This one is proposed to\nassess user satisfaction and determine the level of trust\nassociated with each interaction with a language model, via\na comprehensive yet scalable evaluation of LLMs’ responses\n(using human feedback and automatic evaluation).\n• A fully decentralized, blockchain-powered platform that\nenables LLMs to be shared and evaluated thanks to the\ndesigned reputation-based model.\n• The preparation of LLMGooAQ 1, a comprehensive dataset\nencompassing diverse questions and answers across various\ndomains and contexts. This dataset consists of over 100k\nquestions pulled from the large-scale GooAQ dataset and\ntheir corresponding answers obtained by performing infer-\nence on seven open-source LLMs.\n• An extensive experimental evaluation with multiple scenar-\nios is performed to demonstrate the effectiveness of the\nproposed reputation model and the scalability of LLMChain.\n1https://github.com/mohaminemed/LLMGooAQ/\nII. R ELATED WORK\nA. LLMs Evaluation\nTo assess the credibility and capabilities of LLMs, several\nstudies have introduced diverse evaluation methods, including\npairwise comparison, single-answer grading, or reference-\nguided grading, employing another LLM as an evaluator.\n[2], [15]. These methodologies offer advantages in scalabil-\nity and interoperability. Nevertheless, it comes with notable\nlimitations: 1) Position Bias, where the evaluator tends to\nfavor the initial model; 2) Verbosity Bias, where the eval-\nuator prefers longer responses over shorter ones; and 3)\nSelf-Enhancement/Promotion Bias, where the judging model\nprioritizes its own text or that generated from a similar model.\nMoreover, evaluating a LLM using another LLM appears\nparadoxical since the evaluator itself is subject to evalua-\ntion. On the other hand, alignment-based methods are used\nto make large-scale alignment research more accessible like\nOpenAssistant conversations [20], which is a corpus of conver-\nsations that resemble interactions with assistants, created and\nannotated by humans. Nonetheless, alignment-based methods\nface some scalability challenges and annotation expenses.\nIn Core-GPT [21] and [22], authors focus on assessing the\ncredibility of LLMs. Core-GPT [21] proposes an approach\nthat combines open-access scientific literature with LLMs\nto improve their reliability and trustworthiness. However, its\nmethodology’s scope is limited to two LLMs, “GPT3.5” and\n“GPT-4”, failing to illuminate the credibility gap between\nopen-source and commercial models. In contrast, the approach\nproposed in [22] introduces an automated workflow designed\nto manage an increased number of requests/responses, facilitat-\ning the assessment of the credibility of multiple LLMs. In G-\nEval [16], which is a framework that leverages large language\nmodels, used a Chain-of-Thoughts (CoT) and a form-filling\nparadigm to evaluate the quality of Natural Language Gen-\neration (NLG) outputs. G-Eval experimentation involves two\ngeneration tasks: text summarization and dialogue generation.\nHowever, here again, the methodology is limited to only two\nLLMs which are “GPT3.5” and “GPT-4”.\nWhen delineating the prevailing approaches employed to\nassess the credibility of LLMs, typical challenges become\napparent. These approaches lack transparency and decentral-\nization as they all operate within centralized frameworks.\nTo determine the most credible LLM for a specific context,\nindividuals are faced with two alternatives: either relying on\ncentralized evaluations or carrying out tests independently.\nAdditionally, the majority of current studies focus on either\nhuman feedback or automated evaluation separately, missing\nan opportunity to effectively capture human preferences while\nenhancing scalability and reducing costs.\nB. Blockchain-based Reputation Systems\nThe inherent decentralized and tamper-proof nature of\nblockchain technology provides essential attributes for effec-\ntive reputation management. Several blockchain-based reputa-\ntion systems exist, demonstrating the maturity and usability\nof such solutions for novel applications. TrustChain [17] is\na three-layered blockchain-powered framework used for trust\nmanagement in IoT-supported Supply Chains. The solution\nconstitutes a service platform operating on a permissioned\nblockchain network. It leverages smart contracts to automate\nthe computation of reputations and incorporates an incen-\ntive mechanism based on rewards and penalties to motivate\nusers toward proper behavior. GuRuChain [19], introduces a\nblockchain-based service trading platform that incorporates\nguarantee and reputation at application and consensus layers\nto foster accountability and trust. It leverages smart contracts\nto implement the proposed reputation model and manage\nguarantees using deposits. ValidatorRep [23], is a verification\nscheme that utilizes blockchain with trust management to\nfoster accountability within crowdsourcing systems. Specif-\nically, this proposal entails a decoupled blockchain model\ndesigned for the distinct storage of business transactions\nand log transactions throughout data interaction. It uses a\ntrust model encompassing the reputation of participants and\nthe trust relationships among them. In REPUTABLE [24],\nthe authors propose a decentralized reputation system for\nassessing service providers’ activity within a blockchain-based\necosystem. The proposed solution integrates a centralized\noracle to perform off-chain computations and triggers on-\nchain smart contracts, impeding the system from achieving\ncomplete decentralization. TRUSTD [25] is an ecosystem\npowered by blockchain and collective signatures, designed\nto support content creators in garnering community backing\nfor their content. Additionally, it aids users in assessing the\ncredibility and accuracy of these contents.\nTherefore, to address the aforementioned challenges related\nto LLM’s evaluation, we believe in the consistency of ex-\ntending the use of such reputation systems, proposing a novel\ndecentralized framework for evaluating LLMs on open-ended\nquestion answering. The proposed concept aims to build a\nrobust and transparent blockchain-based reputation system that\nmerges human evaluation with automated metrics to assess\nLLMs responses effectively. To our knowledge, this work\nrepresents the first study of language model evaluation in a\ndecentralized setting.\nIII. LLMC HAIN FRAMEWORK\nIn this section, we introduce LLMChain, a Blockchain-\npowered reputation system for LLM’s evaluation. In particular,\nthe proposed framework aims to foster trust in LLMs by\namalgamating human feedback and automated evaluations.\nLLMChain can be seen as a decentralized reputation-based\nstore that allows sharing and evaluating LLMs. It serves a\ndual role by addressing the needs of users seeking reliable AI\nassistance, as well as assisting LLMs developers in enhancing\nthe performance and reliability of their models. Fig.1 illus-\ntrates an overview of the proposed LLMChain framework.\nA. LLMChain Architecture\nThe proposed LLMChain framework is composed of mul-\ntiple entities distributed over four main layers as depicted in\nFig.1a.\n• User Layer: is composed of individual participants. Each\nparticipant has at least one end-device to interact with the\nsystem. Users with different areas of expertise can join\nthe system to use shared, open-access LLMs and provide\nfeedback after engaging with any of the models. This allows\nusers not only to gain insights into the most suitable LLM\nfor their specific domains but also to actively participate in\nthe evaluation process by testing these models and sharing\ntheir feedback.\n• Blockchain Layer: functions as a permissioned blockchain,\ncomprising nodes initiated by LLM providers and/or devel-\nopers. To participate in the network, an entity must develop\nand share at least one LLM. LLMChain network employs\na consensus mechanism to uphold a uniform ledger copy.\nWe advocate for a reputation-based consensus [19], [26],\nleveraging an existing reputation model within the system.\nCompared to traditional consensus methods, reputation-\nbased consensus offers scalability and enhanced fairness.\nTo further improve the accessibility and performance of\nour decentralized application, we introduce an InterPlanetary\nFile System (IPFS) [27] as an off-chain storage system. The\ncore business logic of LLMChain is securely executed via\nsmart contracts deployed over the network and accessed\nthrough the submission of transactions. LLM providers\nbenefit from joining the network by gaining full access to\nLLMChain and, consequently, all the evaluations occurring\nwithin the system. This access allows them to accumulate\nextensive information that will help them to improve and\ncorrect their models.\n• Oracle Layer: comprises Oracle nodes that merge on-chain\ncode with off-chain infrastructure, facilitating the creation\nof a sophisticated Decentralized Application (DApp). This\napplication responds to real-world events and seamlessly\ninteracts with conventional systems (LLM servers). Hybrid\nsmart contracts deployed across the decentralized Oracle\nnetwork enable automating the evaluation process. The net-\nwork intercepts responses from models, conducts off-chain\nautomatic evaluations, and subsequently triggers on-chain\nsmart contracts to update the overall score of the targeted\nLLM. All of that is achieved in a decentralized and trustless\nway through the execution of an Oracle protocol [28].\n• LLM Layer: consists of language models that are ad-\nministered locally by LLM providers and/or developers.\nFor users who wish to utilize these models for inference\ntasks, developers need to maintain ongoing access to their\nshared models. The Oracle network conducts regular checks\non the connectivity of these shared models. Any model\nthat goes offline is automatically removed from the list of\nrunning models, keeping the view up-to-date and avoiding\ninteraction with non-operating models.\nB. LLMs’ Evaluation process in LLMChain: An overview\nUnlike centralized frameworks where the evaluation is im-\nplemented by a third party, we define end-to-end decentralized\nevaluation protocols. The proposed protocols are implemented\nDoctor\nBlockchain \nLayer\nLLM\n Layer\nUser\nLayer\n P2P Network\nNode\nLLM1 LLM2 LLMn\n…\nOff-chain Storage\nSmart Contracts\nDON: Decentralized Oracle NetworkOracles \nLayer\nEngineer Chemist Biologist\n…\nInterface\nOracle \n(a) System Architecture\n{LLMm}j=1.m..k\n6. Hi=H(prompti,{answerj}j=1..m.k)\n5. Post(prompti,{answerj}j=1..m.k)9. Post(Hi,feedback)\n10. Hf = H(Hi,feedback)\n4. Calculate \nscoreAuto Sa (b) LLM Evaluation Workflow\nFig. 1: Overview of the LLMChain framework. 1a presents the layered BC-powered architecture. It consists of four main layers: a user layer\nformed by individuals with different expertise, a BC layer built on a consortium BC managed by LLM providers, and an Oracle layer built\nup by a decentralized network interconnecting the BC layer with LLMs layer. 1b describes the LLMs evaluation process in LLMChain.\nin the LLMChain architecture using smart contracts. The\nevaluation process consists of three main phases:\n1) Registration: To obtain their credentials, including public\n(address) key and private key, Users and Developers must\nregister on LLMChain through the Identity Smart Contract\n(ISC). The registration process can be done in a decentralized,\nprivacy-preserving, and Sybil-resistant way using an IDentity\nManagement Ledger (IDML) [29].\n2) LLM Sharing: LLM developers can add a new model to\nLLMChain via Reputation Smart Contract (RSC) by call-\ning the addModel function. This creates a new LLM =\n{CID llm, Owner, Ra\n0 , Rh\n0 , R0}. Owner is the developer’s\npublic key. The initial human Rh\n0 , automatic Ra\n0 , and weighted\nreputations R0 for the model are calculated as the average\nvalues across all existing models in the system. CID llm is\nthe hash of the model’s details published on IPFS ( i.e. The\nContent Identifier). To ensure the security of LLMChain’s\nsmart contract functionalities, we implement role-based ac-\ncess control to manage permissions. This is realized through\nthe Access Control Smart Contract (ACSC). ACSC restricts\ncalling functions by role, for example, it restricts the ability\nto share models on LLMChain to developers only.\n3) LLM Evaluation: The comprehensive process, spanning\nfrom prompt submission to updating the global reputation for\nthe chosen model is illustrated in Fig.1b. It begins with the user\nformulating a request intended for a specific LLMm, directly\ntransmitted to the model via a dedicated interface (API).\nSubsequently, the response from LLMm is relayed back to the\nuser. To perform Automatic evaluation, the Oracle intercepts\nboth the request and the response. Then, it dispatches identical\nprompts to other k models {LLMj}j=1,...,k, to use their\nanswers as comparative references. Next, it calculates the\nautomatic score for LLMm using the model described in Sec.\nIV-B1. After that, it stores the prompt and its corresponding\nanswers off-chain using IPFS. Finally, it triggers the RSC\nto update the overall automatic score of LLMm by calling\nthe autoEval function. Upon receiving the answer, users can\nopt for direct Human evaluation by calling the humEval\nfunction or seek alternative candidate responses to gauge the\nquality of LLMm’s answer i.e. using the shared hash Hi,\nthey can retrieve all k answers from IPFS. Once this operation\nhas been completed, the overall weighted reputation score is\nupdated by calling the updateReputation function. Further\ndetails on the automatic and human evaluation procedures\nfollow in the next section.\nIV. R EPUTATION MODEL\nHuman evaluation entails the participation of human experts\nor users to assess the quality, coherence, and overall ade-\nquacy of generated answers. These metrics seek to encompass\nsubjective aspects that automated metrics may overlook [14].\nNevertheless, evaluating generated answers through human\nfeedback poses challenges as it relies on users’ willingness\nto offer genuine and immediate feedback. To better address\nthese, we investigate automatic methods, enabling LLMChain\nto evolve even in the absence of human feedback. In this\nsection, we introduce our reputation model that blends human\nand automated evaluations. This approach aims to leverage\nthe efficiency and scalability of automated methods while\nupholding strong alignment through human feedback.\nA. Reputation Formulation\nWe model the reputation of an LLM as a tuple denoted by\nREP = {Ra, Rh, R}. Our approach involves assigning an\ninitial reputation, noted REP0 = {Ra\n0 , Rh\n0 , R0}, to each new\nLLM. The values of Ra\n0 , Rh\n0 , and R0 are derived from the\naverage scores of all LLMs in the system.\nThe REP tuple undergoes updates after each interaction\ni, following two stages: i) Interaction Evaluation , which\ninvolves computing three scores for the targeted LLM - an\nautomatic score Sa, a human score Sh, and a weighted\ncombination Sθ between both scores - with their respective\nweights ωa, ωh, and ωθ. ii) Global Scores Updating. each\nglobal score Ri in REP is updated using a predefined\nfunction securely implemented in the RSC contract. For each\n(Ri, Scalc, ω) ∈ {(Ra, Sa, ωa), (Rh, Sh, ωh), (R, Sθ, ωθ)},\nU : [0 , 1] × [0, 1] × [0, 1] −→ [0, 1]\n(Ri, Scalc, ω) −→ Ri+1\n(1)\nB. Interaction Evaluation\n1) Automatic Evaluation: Several studies have demonstrated\nthat embedding-based metrics can effectively match human\njudgments by considering semantic relevance [30], [31].\nHowever, their effectiveness is influenced by the quality of\nthe underlying embedding. Consequently, when developing\nLLMChain, we emphasized a modular framework to retain\nflexibility in updating the automatic evaluation technique at\nany time. The metrics we explore to use for the Automatic\nevaluation requires a minimum of one reference to compute\nthe score Sa (cf. Sec. V-A3). Thus, we propose to use k\nreferences, denoted as {refj}j=1...k to evaluate the answer of\nthe targeted model for better precision. The k references are\nthe answers that the decentralized Oracle gets from the top k\nmodels within the context of the prompt. The final score of\nthe answer from the model LLM is computed as follows:\nSa = 1\nk\nkX\nj=1\nscoreAuto(answer, refj) (2)\nWe assess the quality of the automatic evaluation using a\nweighting function ωa ∈ [0, 1]. Its outcome varies depending\non the average reputation of the models used as references\n(i.e. the better the reputation the higher importance is given).\nOnce this is done, the Oracle triggers the autoEval function\nin RSC to update the overall automatic score of the LLMm\nusing the model described in the Sec. IV-C.\n2) Human Evaluation: While it is straightforward to carry out\nan automated evaluation by measuring the distance/similarity\nbetween generated answers, it is less easy to gather informa-\ntion about trust, user satisfaction, completeness, and usefulness\nof a generated text. Inspired by [14] and [32], our approach in-\nvolves employing a multi-item scale questionnaire for efficient\nand scalable human evaluation. Our focus encompasses two\ntypes of dimensions (constructs) essential for users to assess\ntext generated by LLM accurately:\n• Answer’s Constructs: are the metrics that allow the eval-\nuation of the quality of a single answer/response ( i.e. cal-\nculate Sh). To do so, we employ three metrics. First, the\nReliability, denoted as At, evaluates the trustworthiness of\nthe provided answer. Then, the Completeness, denoted as\nAc, measures the comprehensiveness or completeness of the\nanswer. Finally, the Utility, denoted as Au, determines the\nusefulness of the answer. The human score of an answer is\na linear combination of the three metrics:\nSh = [αrAt + βrAc + γrAu] ;αr + βr + γr = 1 (3)\n• User Constructs: encompass parameters that signify a\nuser’s proficiency and ability in evaluating the generated\ntext, showcasing the quality of their assessment and its\ninfluence on the overall human score ( i.e. calculate ωh). To\ndo so, we define four metrics. First, Duration, denoted as\nD, measures the time interval in minutes between the last\ntwo evaluations. Second Familiarity, denoted as F, gauges\nthe user’s familiarity with the response context. Third, LLM\nTrust, denoted as T, assesses the user’s belief in the exper-\ntise of the targeted LLM. Finally, Uncertainty, denoted as\nU, captures the user’s degree of uncertainty regarding the\nevaluation. The weight of the human evaluation is given by:\nωh = WhFD (4)\nWhere,\nWh = [αuF + βuT + γu(1 − U)] ;αu + βu + γu = 1\nand,\nFD = tanhλ(D) =1 − e−λ.D\n1 +e−λ.D\nWe normalize D using a hyperbolic tangent function FD ∈\n[0, 1]. FD is implemented in a way that thwarts potential\nabuse. It reduces the impact of successive evaluations per-\nformed within a short period, thereby protecting the LLM’s\noverall reputation and reinforcing the model’s effectiveness.\nFurthermore, the positive correlation with the other metrics\n(i.e. F, T, and 1−U ) leads to important considerations: first,\nratings from users less familiar with the context carry less\nweight in updating the model’s overall human reputation;\nsecond, ratings from users with minimal trust or with\nhigher uncertainty have less impact on updates compared to\nthose with lower uncertainty and higher trust in the overall\nexpertise of LLMs.\nC. Overall Scores Update\nIn LLMChain, we employ three types of updates. The\noverall automatic reputation Ra update occurs after each\ninteraction to keep tracking the LLM behavior, while changes\nin Rh and R only occur if the interaction includes a human\nevaluation. These updates depend on the outcome of the\nautomatic evaluation Sa, the human evaluation Sh, or the\nweighted evaluation Sθ. We use θ, a configurable weighting\nfactor, to give more emphasis to the human evaluation when\ncalculating Sθ and ωθ, as follows:\n\u001a Sθ = θSh + (1− θ)Sa\nωθ = θωh + (1− θ)ωa (5)\nThe updating formula Uψ,ξ : (Ri, Scalc, ω) −→ Ri+1 for\nthe three scores Rh, Ra, and R is thus defined as follows:\n0 20 40 60 80 100\nInteractions\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Reputation\nh=0.6-D=300\nh=0.6-D=600\nh=0.6-D=900\nD=500- h=0.3\nD=500- h=0.5\nD=500- h=0.7\n(a) Reputation growth following successive accurate answers\n0 5 10 15 20 25 30\nInteractions\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Reputation\nh=0.6-D=300\nh=0.6-D=600\nh=0.6-D=900\nD=500- h=0.3\nD=500- h=0.5\nD=500- h=0.7\nBehavior switch\n(b) Reputation changes after successive incorrect answers\nFig. 2: The Effectiveness of LLMChain’s Reputation model under\ndifferent Wh and D.\n∀(Ri, Scalc, ω) ∈ {(Ra, Sa, ωa), (Rh, Sh, ωh), (R, Sθ, ωθ)},\nRi+1 =\n\u001a (1 − ψω)Ri + ψωScalc ; Scalc ≥ Ri\n(1 − ξω)Ri + ξωScalc ; Scalc < Ri\n(6)\nwhere Ri and Ri are the current reputations and trust thresh-\nolds (i.e. before the interaction i), respectively. We define the\nthreshold Ri as the average of LLM reputations.\nBy employing two distinct formulas in (Eq. 6) for the update\nprocess using a trust threshold Ri, we separate expected good\nbehavior from unexpected bad behavior (no/bad response,\nhallucination, harmful content, etc. [33], [34]). Consequently,\nwe can put more weight ( i.e. ξ > ψ) on the newly calculated\nscore Scalc in the case of an incorrect response. Moreover,\nthe integration of the weighting function ω into both equations\nestablishes a direct relationship between the quality of the eval-\nuation and its impact on the update of the overall reputation.\nFor instance, for a Rh update, the greater the user’s familiarity,\ncertainty, and trust in the LLM expertise, the more significant\ntheir evaluation’s impact becomes. Moreover, the use of D\nallows the system to mitigate consecutive inaccurate ratings\nthat may be intended to enhance or damage LLM’s reputation.\nWe note that this metric is reset at regular intervals ( e.g. every\n24 hours), preventing users who abstain from evaluations for\na long time from exploiting the model.\nFig. 2 demonstrates the impacts of D and Wh on the overall\nreputation updates. It shows the shifts in reputation between a\nskilled model consistently providing accurate responses and\na less competent one that produces consecutive incorrect\nanswers after delivering multiple correct ones. Both positive\nand negative updates have a direct correlation with D and Wh.\nThis suggests that the longer the time interval between the last\ntwo evaluations, the more significant impact the user’s latest\nevaluation has. Likewise, increased levels of familiarity, trust,\nand certainty contribute to a more substantial impact.\nV. E XPERIMENTS\nA. Experimental Setup\n1) Environment: We conducted the experimental tests on two\nseparate clusters: a GPU cluster for hosting the LLM part\nof the system and a CPU cluster dedicated to running the\nblockchain network. The first cluster comprises two servers,\none featuring an NVIDIA RTX A6000 GPU card and the\nother equipped with an NVIDIA GeForce RTX 2080 Ti\ncard. Meanwhile, the second cluster consists of two HPE\nProLiant XL225n Gen10 Plus servers specifically allocated\nfor experimenting with blockchain solutions. Each server in\nthis cluster is powered by two AMD EPYC 7713 64-Core\nprocessors and 2x256 GB RAM.\n2) Datasets: We evaluate LLMChain on three datasets:\n• MTBench2 is a recent dataset extensively utilized in eval-\nuating LLMs [2]. MT-Bench consists of 3.3K expert-level\npairwise human preferences for answers generated by six\nmodels (“Llama-13B”, “Alpaca-13B”, “Vicuna-13B”, “GPT-\n3.5”, “Claud-v1”, and “GPT-4”) across 80 questions.\n• GooAQ3 is a large-scale dataset with a variety of answer\ntypes. This dataset comprises more than 5M questions and\n3M answers sourced from Google [35].\n• LLMGooAQ.4 We prepare this comprehensive database,\ncovering 100k questions and answers in 20 different\nfields/contexts. We randomly sample 100K tuples from the\nGooAQ dataset and perform inference using seven LLMs\n(“Alpaca-13b”, “Llama-2-13b”, “Chatglm-6b”, “Fastchat-t5-\n3b”, “Koala-13b”, “Vicuna-7b”, “Vicuna-13b”).\n3) Automatic Metrics: To pinpoint the optimal technique for\nour context, we conduct rigorous benchmarks among various\nembedding-based metrics that achieved SoTA performance.\n• BERTScore [12] is an automatic evaluation metric for text\ngeneration. It evaluates the similarity between tokens in\na candidate sentence and those in a reference sentence.\nUnlike N-Gram methods relying on exact matches like\nBLEU Score [36] and ROUGE Score [37], BERTscore relies\non contextual embeddings to gauge token similarity. The\napproach employs cosine similarity to measure the likeness\nbetween a reference token xi and a candidate token ˆxi. The\ntotal score involves comparing each token in x with tokens\nin ˆx to calculate recall, and each token in ˆxi with tokens in\nx to determine precision. To maximize the similarity score,\na greedy matching technique is used, wherein each token is\npaired with the most similar token from the other sentence.\nPrecision and recall are combined to derive an F1 score.\n• BARTScore [30] is an automated evaluation method that\nframes the evaluation of generated text as a text generation\n2https://huggingface.co/spaces/lmsys/mt-bench\n3https://huggingface.co/datasets/gooaq\n4https://github.com/mohaminemed/LLMGooAQ/\n0 1 2 3 4\nPredicted labels\n0\n1\n2\n3\n4 True labels\n63 18 6 13 9\n42 102 20 27 1\n90 67 174 58 20\n91 132 92 323 26\n108 68 43 74 141\n50\n100\n150\n200\n250\n300\n(a) DSFocus\n0 1 2 3 4\nPredicted labels\n0\n1\n2\n3\n4 True labels\n51 21 20 15 14\n38 100 41 40 15\n31 43 273 69 18\n53 69 101 491 39\n35 53 37 75 302\n100\n200\n300\n400\n (b) DSSent\n0 1 2 3 4\nPredicted labels\n0\n1\n2\n3\n4 True labels\n22 36 27 21 15\n11 109 53 50 12\n8 23 303 94 8\n5 24 57 655 19\n26 43 53 93 287\n100\n200\n300\n400\n500\n600\n (c) BERTScore\n0 1 2 3 4\nPredicted labels\n0\n1\n2\n3\n4 True labels\n15 30 29 22 25\n26 79 58 52 20\n7 10 288 104 27\n0 16 29 692 23\n2 11 28 85 376\n0\n100\n200\n300\n400\n500\n600\n (d) BARTScore\nFig. 3: -. Labels are denoted as: {“0:Llama-13B”, “1:Alpaca-13B”, “2:Vicuna-13B”, “3:GPT-3.5”, “4:Claud-v1” }\n.\nTABLE I: Hyperparameter’s Configuration.\nParameter Value Parameter Value\nψ 1 / 3 α r = β r = γ r 1 / 3\nξ 2 / 3 α u = β u = γ u 1 / 3\nλ 10 − 3 θ 2 / 3\nproblem, utilizing pre-trained sequence-to-sequence models.\nThe fundamental concept revolves around the notion that\nmodels trained to convert generated text into or from a\nreference output or the source text will yield higher scores\nfor superior generated text. This concept is implemented\nusing BART, a pre-trained model based on an encoder-\ndecoder architecture. The metric BARTScore offers various\nadaptable variants that can be applied in an unsupervised\nmanner to evaluate text from multiple perspectives, such as\ninformativeness, fluency, or factuality.\n• DISCOScore [31] is a parametrized discourse metric, which\nuses BERT to model discourse coherence from different\nperspectives, through the lens of readers’ focus, driven by\nCentering theory. DISCOScore offers two variations: Fo-\ncusDiff and SentGraph, differing in their treatment of focus.\nThis approach models the frequency and semantic relevance\nof focus and then compares the disparities between the\nhypothesis and the reference. It utilizes two adjacency ma-\ntrices to represent coherence based on focus. In FocusDiff,\nthe matrix represents relationships between foci and tokens,\nindicating focus frequency. Meanwhile, in SentGraph, the\nmatrix showcases the interdependence between sentences\nbased on shared foci and sentence proximity.\nB. Reputation Model Effectiveness\nIn the following, we first perform an experimental compar-\nison of the automatic metrics described in Sec. V-A3. Next,\nwe perform two additional experiments aiming to evaluate the\nefficiency of both the automatic and human models. The values\nof the configurable parameters used in these experiments are\nsummarized in Table. I.\n1) Metrics Benchmark. Determining the most fitting metric\nfor evaluating LLM-generated answers analytically is not\nstraightforward. That is why we embarked on a benchmark\nexperiment to pinpoint the best technique. This experiment\nTABLE II: Metrics Performance on the MTBench dataset.\nMetric Accuracy Kendall’s Correlation\nDSFocus 0.44414 -0.60\nDSSent 0.59540 0.60\nBertScore 0.66991 0.60\nBartScore 0.70594 0.80\naims to assess the metrics commonly used in automatically\nevaluating NLP tasks. Our goal is to identify the one that\nbest aligns with human judgments. To achieve this, we\nconduct an experiment that involves computing automatic\nscores on MTBench answers. These scores automatically\ndetermine the winner between two different LLMs for\neach question. Fig. 3 demonstrates the correlation between\nhuman-selected winners (true) and automatic winners (pre-\ndicted). The matrices show nearly diagonal patterns, indi-\ncating good correlations, yet variations in accuracy exist.\nFor instance, the DISCOScore DSSent variant boasts an\naccuracy of 59%, surpassing that of the DSFocus variant\n(44%). BARTScore, on the other hand, demonstrates supe-\nrior accuracy, with 71% of predicted winners matching ac-\ntual human winners, compared with 67% for BERTScore.\nTable II illustrates Kendall’s Tau correlation of these four\nmetrics. We can see that BARTScore can significantly\noutperform all other techniques by offering a superior\ncorrelation of 80% with human judgments. Based on these\nresults, we decided to use BARTScore in the following\nexperiments.\n2) Automatic Evaluation. To adequately evaluate the auto-\nmatic model, we use BARTScore to conduct a pairwise\ncomparison between the seven LLMs in LLMGooAQ us-\ning GooAQ’s answers as benchmarks. Subsequently, we\ncalculate the win rates for each LLM per context. The ex-\nperimental results, showcased in Fig. 4, highlight “Vicuna-\n13b” as the best model outperforming others in nearly 90%\nof the contexts. Furthermore, the resulting models’ overall\nwin rates align with previous human-based evaluation [2],\naffirming that the BARTScore metric correlates strongly\nwith human judgments.\nNow, to assess the efficacy of leveraging the best models’\nanswers within specific contexts, we conduct a subsequent\ntest using the answers from “Vicuna-13b” as references.\nComputer ScienceMathematics\nOther\nNutrition\nMedical Science\nLaw and legislation\nTechnology\nEconomics\nLiterature\nBiologyPolitical Science\nPhysics\nEngineering\nPsychology\nSociology\nChemistry\nSports\nPhilosophy\nHistory\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nModel\nAlpaca-13B\nLLaMA-2-13B\nChatglm-6B\nFastchat-t5-3B\nKoala-13B\nVicuna-7B\nVicuna-13B\nFig. 4: BARTScore-based Contextual Win-Rates on LLMGooAQ.\nFig. 5 presents the confusion matrix comparing the winners\n(true) computed using GooAQ answers with those (pre-\ndicted) computed using “Vicuna-13b” answers. The results\nare compelling, revealing robust accuracy (70%) between\nthe two cases. It is essential to note that, according to cur-\nrent benchmarks [2], [6] and leaderboards (ChatBotArena5,\nTrustLLM6), “Vicuna-13b” is a well-ranked open source\nmodel, but it is not the best. Despite this, the results\nobtained using it as a reference model are convincing.\n3) Reputation Evaluation. The third experiment involves\nemploying the proposed models and monitoring changes in\nreputations in a real scenario. We use our prepared dataset\nwith automatic scores computed using BARTScore to do\nthis. Given the high cost of obtaining human judgments, we\nemploy GPT-4 as an expert for human evaluation. GPT-4\nis recognized as the leading model in current benchmarks\n[2], [6], [21]. In this experiment, GPT-4 is used to play\nthe role of a human expert, responding to a questionnaire\nthat enables the calculation of metrics ( i.e. F, T, U, At,\nAc, and Au) used in the human model. Fig. 6 illustrates\nthe variations in Ra, Rh, and R for the seven LLMs\nin our dataset. Despite the disparities between the Ra\nand Rh scores, a consistent pattern emerges, with scores\nfor good models such as “Koala-13b”, “Vicuna-7b”, and\n“Vicuna-13b” steadily increasing, while scores for less\neffective models such as “Alpaca-13b” and “Llama-2-13b”\ncontinually decrease. Moreover, with an increasing number\nof evaluations, the distinctions between closely ranked\nmodels become more pronounced. This demonstrates the\neffectiveness of our models, showcasing their ability to\ndiscern even subtle differences between close LLMs like\n“Chatglm-6b” and “Fastchat-t5-3b”.\n5https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n6https://trustllmbenchmark.github.io/TrustLLM-Website/leaderboard.html\n0 1 2 3 4 5\nPredicted labels\n0\n1\n2\n3\n4\n5 True labels\n11460 1996 3000 2708 2996 2512\n2728 16528 3804 3584 4172 3844\n2098 1760 31939 3370 4666 4404\n2100 1890 4030 34747 5226 4860\n1204 1004 2168 2226 48559 3732\n1004 854 2212 2204 4262 53349\n10000\n20000\n30000\n40000\n50000\nFig. 5: Ground-Truth Answers vs Vicuna-13B Answers as Ref-\nerences for BARTScore-based Pairwise-comparison on the LLM-\nGooAQ dataset. Labels are denoted as: {0: “Alpaca-13b”, 1: “Llama-\n2-13b”, 2: “Chatglm-6b”, 3: “Fastchat-t5-3b”, 4: “Koala-13b”, 5:\n“Vicuna-7b”, 6: “Vicuna-13b”}.\nC. Blockchain Performance\n1) Business Model: Having evaluated all its components in\nthe previous subsection, we now implement the proposed\nblockchain-driven framework. This one is deployed on a\nblockchain network powered by Hyperledger Besu 7, an open-\nsource Ethereum client. Our evaluation approach includes:\n• Participants: Users with different expertise and Admins of\nthe organization or the consortium operating the system.\n• Assets: A data structure that represents the model on-chain.\n• Smart Contracts: Three types of smart contracts are used to\ndevelop the business model: Identity Smart Contract (ISC),\nAccess Control Smart Contract (ACSC), and Reputation\nSmart Contract (RSC). ISC implements the registration\nprocess, ACSC employs a role-based access control to\nmanage the permissions when calling RSC functions, e.g.\nonly Oracles can trigger the autoEval function. The RSC\nimplements four main functions, addModel, autoEval,\nhumEval, and updateReputation.\nWe develop the smart contracts of LLMChain using the\nSolidity programming language8 and establish a local network\nconsisting of sixteen validators using Hyperledger Besu with\nProof of Authority (PoA) as consensus protocol. We lastly use\nWeb3js library9 for developing the client side and deploying\nthe system’s smart contracts.\n2) Performance Evaluation: To conduct tests, we utilize Hy-\nperledger Caliper10, a benchmarking tool for blockchains. The\nexperiments involve changing the transaction sending rate\n(ranging from 50 to 1000 TPS) using a consistent network\nconfiguration for the main operations performed within LLM-\nChain. As a result, two metrics are measured:\n• Throughput: is the number of successful transactions per\nsecond (TPS).\n7https://besu.hyperledger.org\n8https://docs.soliditylang.org\n9https://web3js.readthedocs.io\n10https://github.com/hyperledger/caliper-benchmarks\n0 200 400 600 800 1000\nInteractions\n0.38\n0.40\n0.43\n0.45\n0.48\n0.50\n0.53\n0.55\n0.58Ra\nvicuna-13b\nvicuna-7b\nkoala-13b\nfastchat-t5-3b\nchatglm-6b\nalpaca-13b\nllama-2-13b\n(a) BARTScore-based Automatic scores\n0 200 400 600 800 1000\nInteractions\n0.30\n0.40\n0.50\n0.60\n0.70Rh\nvicuna-13b\nvicuna-7b\nkoala-13b\nfastchat-t5-3b\nchatglm-6b\nalpaca-13b\nllama-2-13b (b) GPT4-generated Human scores\n0 200 400 600 800 1000\nInteractions\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65R\nvicuna-13b\nvicuna-7b\nkoala-13b\nfastchat-t5-3b\nchatglm-6b\nalpaca-13b\nllama-2-13b (c) Weighted Reputation scores\nFig. 6: Changes in Ra, Rh, and R of seven LLMs using LLMGooAQ.\n50 100 150 190 230 280 350 440 550 660 800 1000\nSend Rate(TPS)\n0\n100\n200\n300\n400Throughput(TPS)\nThroughputs\naddModel\nautoEval\nhumEval\n(a) Throughput under different send rates\n50 100 150 190 230 280 350 440 550 660 800 1000\nSend Rate(TPS)\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25Latency(s)\nLatencies\naddModel\nautoEval\nhumEval\n(b) Latency under different send rates\nFig. 7: Throughput and Latency of LLMChain.\n• Latency: refers to the time difference in seconds between the\nsubmission and completion of a transaction.\nThe throughput and latency values for each function under\ndifferent sending rates are illustrated in Fig. 7. At the begin-\nning, the pattern is evident: throughput and latency increase as\nthe transaction send rate increases. With lower sending rates\n(<350 TPS), there is no significant difference in throughput\nbetween the three defined transaction types. However, nearing\nsystem capacity, distinctions emerge. The lightest function,\nautoEval, achieves a peak throughput of 440 TPS, surpassing\nhumEval at 426 TPS, and the heaviest function, addModel,\nmanaging 403 TPS, primarily due to the initialization and\nstorage of model information on-chain. This also explains the\ncomparatively higher latency of addModel compared with the\nother two functions. Nevertheless, leveraging storage scaling\nvia IPFS, LLMChain achieves an average throughput close\nto 420 TPS, comfortably meeting the specific demands of\nour use case. On top of that, since LLMChain operates on\nan EVM-based state machine, all the scaling techniques of\nEthereum-based blockchains, such as Sharding and zkRollups\ncan be applied to further enhance its performance for large-\nscale deployment if needed.\nVI. L IMITATIONS AND FUTURE RESEARCH DIRECTIONS\nTo the best of our knowledge, we are the first to design\nand develop a reputation model for evaluating LLMs within\na decentralized framework. While our experiments prove the\neffectiveness and scalability of LLMChain, we believe that\nthis work promotes future research on decentralized and\ntransparent language model evaluation. However, LLMChain\npresents some limitations regarding both human and auto-\nmatic evaluations. Firstly, human evaluation depends mainly\non users’ willingness to provide authentic feedback. Further\nassurance and incentive measures can be added to the frame-\nwork to improve the reliability of human evaluation. Secondly,\nautomatic evaluation relies on the availability of reference\nmodels. This approach has proved effective. However, it has\ntwo important shortcomings: i) Its accuracy depends on the\nperformance of available reference models, ii) and even if\nthe k responses can help the user to provide a better human\nevaluation, this approach generates off-chain communication\nand computational overheads.\nVII. C ONCLUSION\nIn this paper, we propose LLMChain, a novel blockchain-\npowered framework, specifically designed to share and evalu-\nate LLMs efficiently and transparently. LLMChain addresses\ntrust concerns associated with flawed behaviors like halluci-\nnations and unreliable reasoning of LLMs by employing a\ncontext-driven reputation system. Our efforts involve crafting\nand implementing a reputation model that evaluates user\nsatisfaction and trust in each interaction involving an LLM.\nThis model amalgamates human feedback with automatic eval-\nuation to assign contextual reputation scores that accurately\nmirror LLM behavior. Consequently, the system aids users\nand entities in pinpointing the most credible LLM for their\nrequirements while offering LLM providers valuable insights\nto refine and enhance their models. This research marks\nthe first initiative to introduce a distributed framework dedi-\ncated to LLMs evaluation. Through extensive experiments and\nbenchmarks, we demonstrate the effectiveness of both human\nand automatic evaluations in LLMChain. Moreover, the tests\nconducted on the deployed blockchain affirm LLMChain’s\nefficiency and scalability, validating its practical applicability\nin real-world scenarios. Finally, LLMGooAQ, a large dataset\nof over 100K questions and answers generated using seven\nLLMs, was prepared and released to the community to advance\nresearch in this area further.\nACKNOWLEDGEMENT\nThis work was supported by the 5G-INSIGHT bilateral ANR-FNR project , the Nouvelle-\nAquitaine Region - B4IoT project, the French government in the framework of the France\nRelance program, and the ITSOFT company under grant number AD 22-252.\nREFERENCES\n[1] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang, “Is\nChatGPT a general-purpose natural language processing task solver?” in\n2023 Conference on Empirical Methods in Natural Language Process-\ning, H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association\nfor Computational Linguistics, Dec. 2023, pp. 1339–1384.\n[2] L. Zheng, W.-L. Chiang, Y . Sheng, S. Zhuang, Z. Wu, Y . Zhuang, Z. Lin,\nZ. Li et al., “Judging LLM-as-a-judge with MT-bench and chatbot\narena,” in Thirty-seventh Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2023.\n[3] R. Dale, “Gpt-3: What’s it good for?” Natural Language Engineering,\nvol. 27, no. 1, pp. 113–118, 2021.\n[4] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei\net al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv\npreprint arXiv:2307.09288, 2023.\n[5] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang et al., “Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality,”\nMarch 2023.\n[6] L. Sun, Y . Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y . Huang, W. Lyu,\nY . Zhang, X. Li et al., “Trustllm: Trustworthiness in large language\nmodels,” arXiv preprint arXiv:2401.05561, 2024.\n[7] C. Wang, X. Liu, Y . Yue, X. Tang, T. Zhang, C. Jiayang, Y . Yao,\nW. Gao, X. Hu, Z. Qi et al., “Survey on factuality in large language\nmodels: Knowledge, retrieval and domain-specificity,” arXiv preprint\narXiv:2310.07521, 2023.\n[8] J. Li, X. Cheng, W. X. Zhao, J.-Y . Nie, and J.-R. Wen, “Halueval:\nA large-scale hallucination evaluation benchmark for large language\nmodels,” in Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, 2023, pp. 6449–6464.\n[9] O. Golovneva, M. Chen, S. Poff, M. Corredor, L. Zettlemoyer et al.,\n“ROSCOE: A suite of metrics for scoring step-by-step reasoning,” inThe\nEleventh International Conference on Learning Representations, ICLR\nKigali, Rwanda. OpenReview.net, 2023.\n[10] L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, and W. Y . Wang, “Au-\ntomatically correcting large language models: Surveying the landscape\nof diverse self-correction strategies,” 2023.\n[11] P. Fernandes, A. Madaan, E. Liu, A. Farinhas, P. H. Martins, A. Bertsch\net al., “Bridging the Gap: A Survey on Integrating (Human) Feedback\nfor Natural Language Generation,” Transactions of the Association for\nComputational Linguistics, vol. 11, pp. 1643–1668, 12 2023.\n[12] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and Y . Artzi, “Bertscore:\nEvaluating text generation with BERT,” in 8th International Conference\non Learning Representations, ICLR, Addis Ababa, Ethiopia, 2020.\n[13] J. Belouadi and S. Eger, “UScore: An effective approach to fully\nunsupervised evaluation metrics for machine translation,” inProceedings\nof the 17th Conference of the European Chapter of the Association\nfor Computational Linguistics, A. Vlachos and I. Augenstein, Eds.\nDubrovnik, Croatia: Association for Computational Linguistics, May\n2023, pp. 358–374.\n[14] H. Schuff, L. Vanderlyn, H. Adel, and N. T. Vu, “How to do human\nevaluation: A brief introduction to user studies in nlp,”Natural Language\nEngineering, vol. 29, no. 5, p. 1199–1222, 2023.\n[15] C.-H. Chiang and H. yi Lee, “Can large language models be an\nalternative to human evaluations?” 2023.\n[16] Y . Liu, D. Iter, Y . Xu, S. Wang, R. Xu, and C. Zhu, “G-eval: NLG\nevaluation using gpt-4 with better human alignment,” in Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language\nProcessing, H. Bouamor, J. Pino, and K. Bali, Eds. Singapore:\nAssociation for Computational Linguistics, Dec. 2023, pp. 2511–2522.\n[17] S. Malik, V . Dedeoglu, S. S. Kanhere, and R. Jurdak, “Trustchain:\nTrust management in blockchain and iot supported supply chains,” IEEE\nInternational Conference on Blockchain, pp. 184–193, 2019.\n[18] M. Li, J. Weng, A. Yang, W. Lu, Y . Zhang, L. Hou, J.-N. Liu,\nY . Xiang, and R. H. Deng, “Crowdbc: A blockchain-based decentralized\nframework for crowdsourcing,” IEEE Transactions on Parallel and\nDistributed Systems, vol. 30, no. 6, pp. 1251–1266, 2019.\n[19] M. A. Bouchiha, Y . Ghamri-Doudane, M. Rabah, and R. Champagnat,\n“Guruchain: Guarantee and reputation-based blockchain service trading\nplatform,” in IFIP Networking Conference, 2023, pp. 1–9.\n[20] A. K ¨opf, Y . Kilcher, D. von R ¨utte, S. Anagnostidis, Z.-R. Tam,\nK. Stevens, A. Barhoum et al., “Openassistant conversations – democ-\nratizing large language model alignment,” 2023.\n[21] D. Pride, M. Cancellieri, and P. Knoth, “Core-gpt: Combining open\naccess research and large language models for credible, trustworthy\nquestion answering,” in International Conference on Theory and Prac-\ntice of Digital Libraries. Springer, 2023, pp. 146–159.\n[22] W. Ye, M. Ou, T. Li, X. Ma, Y . Yanggong, S. Wu, J. Fu, G. Chen, J. Zhao\net al., “Assessing hidden risks of llms: An empirical study on robustness,\nconsistency, and credibility,” arXiv preprint arXiv:2305.10235, 2023.\n[23] R. Lai and G. Zhao, “Validatorrep: Blockchain-based trust management\nfor ensuring accountability in crowdsourcing,” in 2022 IEEE 46th An-\nnual Computers, Software, and Applications Conference (COMPSAC).\nIEEE, 2022, pp. 716–725.\n[24] J. Arshad, M. A. Azad, A. Prince, J. Ali, and T. G. Papaioannou, “Rep-\nutable–a decentralized reputation system for blockchain-based ecosys-\ntems,” IEEE Access, vol. 10, pp. 79 948–79 961, 2022.\n[25] Z. Jaroucheh, M. Alissa, W. J. Buchanan, and X. Liu, “Trustd: Combat\nfake content using blockchain and collective signature technologies,”\nin 2020 IEEE 44th Annual Computers, Software, and Applications\nConference (COMPSAC). IEEE, 2020, pp. 1235–1240.\n[26] X. Zhu, Y . Li, L. Fang, and P. Chen, “An improved proof-of-trust\nconsensus algorithm for credible crowdsourcing blockchain services,”\nIEEE Access, vol. 8, pp. 102 177–102 187, 2020.\n[27] J. Benet, “Ipfs - content addressed, versioned, p2p file system,” 2014.\n[28] L. Breidenbach, C. Cachin, B. Chan, A. Coventry, S. Ellis, A. Juels,\nF. Koushanfar, A. Miller, B. Magauran, D. Moroz et al., “Chainlink 2.0:\nNext steps in the evolution of decentralized oracle networks,” Chainlink\nLabs, vol. 1, pp. 1–136, 2021.\n[29] D. Maram, H. Malvai, F. Zhang, N. Jean-Louis, A. Frolov, T. Kell et al.,\n“Candid: Can-do decentralized identity with legacy compatibility, sybil-\nresistance, and accountability,” in 2021 IEEE Symposium on Security\nand Privacy (SP), 2021, pp. 1348–1366.\n[30] W. Yuan, G. Neubig, and P. Liu, “Bartscore: Evaluating generated text as\ntext generation,” in Advances in Neural Information Processing Systems,\nM. Ranzato, A. Beygelzimer, Y . Dauphin, P. Liang, and J. W. Vaughan,\nEds., vol. 34. Curran Associates, Inc., 2021, pp. 27 263–27 277.\n[31] W. Zhao, M. Strube, and S. Eger, “DiscoScore: Evaluating text gener-\nation with BERT and discourse coherence,” in in 17th Conference of\nthe European Chapter of the Association for Computational Linguistics.\nDubrovnik, Croatia: Association for Computational Linguistics, May\n2023, pp. 3865–3883.\n[32] M. K ¨orber, “Theoretical considerations and development of a ques-\ntionnaire to measure trust in automation,” in Proceedings of the 20th\nCongress of the International Ergonomics Association (IEA 2018) Vol-\nume VI: Transport Ergonomics and Human Factors (TEHF), Aerospace\nHuman Factors and Ergonomics 20. Springer, 2019, pp. 13–30.\n[33] Y . Zhang and M. van der Schaar, “Reputation-based incentive protocols\nin crowdsourcing applications,” in 2012 Proceedings IEEE INFOCOM,\n2012, pp. 2140–2148.\n[34] E. Bellini, Y . Iraqi, and E. Damiani, “Blockchain-based distributed trust\nand reputation management systems: A survey,” IEEE Access, vol. 8,\npp. 21 127–21 151, 2020.\n[35] D. Khashabi, A. Ng, T. Khot, A. Sabharwal, H. Hajishirzi, and\nC. Callison-Burch, “Gooaq: Open question answering with diverse\nanswer types,” arXiv preprint arXiv:2104.08727, 2021.\n[36] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method\nfor automatic evaluation of machine translation,” in Proceedings of the\n40th Annual Meeting of the Association for Computational Linguistics.\nPhiladelphia, Pennsylvania, USA: Association for Computational Lin-\nguistics, Jul. 2002, p. 311–318.\n[37] C.-Y . Lin, “ROUGE: A package for automatic evaluation of summaries,”\nin Text Summarization Branches Out. Barcelona, Spain: Association\nfor Computational Linguistics, Jul. 2004, pp. 74–81.",
  "topic": "Reputation",
  "concepts": [
    {
      "name": "Reputation",
      "score": 0.7735996246337891
    },
    {
      "name": "Computer science",
      "score": 0.6352576017379761
    },
    {
      "name": "Scalability",
      "score": 0.5339058637619019
    },
    {
      "name": "Blockchain",
      "score": 0.5184149742126465
    },
    {
      "name": "Trustworthiness",
      "score": 0.4792340099811554
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.44229844212532043
    },
    {
      "name": "Computer security",
      "score": 0.3878660202026367
    },
    {
      "name": "Knowledge management",
      "score": 0.3565877676010132
    },
    {
      "name": "Data science",
      "score": 0.3544108271598816
    },
    {
      "name": "Internet privacy",
      "score": 0.34044238924980164
    },
    {
      "name": "Political science",
      "score": 0.11591610312461853
    },
    {
      "name": "Database",
      "score": 0.10110947489738464
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78744979",
      "name": "La Rochelle Université",
      "country": "FR"
    }
  ],
  "cited_by": 4
}