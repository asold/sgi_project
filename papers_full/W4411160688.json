{
    "title": "A Mamba-based foundation model for materials",
    "url": "https://openalex.org/W4411160688",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5068590850",
            "name": "Eduardo Soares",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5077314929",
            "name": "Emílio Vital Brazil",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5094342765",
            "name": "Victor Shirasuna",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5090730388",
            "name": "Dmitry Yu. Zubarev",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5109100433",
            "name": "Renato Cerqueira",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5085426538",
            "name": "Kristin Schmidt",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4367049415",
        "https://openalex.org/W4313485929",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W3088999551",
        "https://openalex.org/W3093934881",
        "https://openalex.org/W4387218877",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3085139254",
        "https://openalex.org/W4306955484",
        "https://openalex.org/W4327737695",
        "https://openalex.org/W3209374680",
        "https://openalex.org/W4290802752",
        "https://openalex.org/W4389326242",
        "https://openalex.org/W4395686592",
        "https://openalex.org/W2887459817",
        "https://openalex.org/W4399657245",
        "https://openalex.org/W4307468223",
        "https://openalex.org/W3116865743",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W2785942661",
        "https://openalex.org/W2966357564",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W3005552578",
        "https://openalex.org/W4214868967",
        "https://openalex.org/W3080997787",
        "https://openalex.org/W4288090629",
        "https://openalex.org/W4321610465",
        "https://openalex.org/W4226145240",
        "https://openalex.org/W3118349318",
        "https://openalex.org/W4400362104",
        "https://openalex.org/W4200030022",
        "https://openalex.org/W2947423323",
        "https://openalex.org/W4387560715",
        "https://openalex.org/W4377864441",
        "https://openalex.org/W3206711231",
        "https://openalex.org/W4213077304",
        "https://openalex.org/W3095883070",
        "https://openalex.org/W4297632148",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W4323304388",
        "https://openalex.org/W4392817999",
        "https://openalex.org/W4401202672",
        "https://openalex.org/W4283331738",
        "https://openalex.org/W4318350716",
        "https://openalex.org/W4396816591",
        "https://openalex.org/W3103092523"
    ],
    "abstract": null,
    "full_text": "npj |artiﬁcial intelligence Article\nhttps://doi.org/10.1038/s44387-025-00009-7\nA Mamba-based foundation model for\nmaterials\nCheck for updates\nEduardo Soares1 ,E m i l i oV i t a lB r a z i l1, Victor Shirasuna1, Dmitry Zubarev2,R e n a t oC e r q u e i r a1 &\nKristin Schmidt2\nWe present a novel approach to chemical foundation models, leveraging structured state space\nsequence models (SSMs) to overcome the limitations of traditional Transformer-based architectures.\nWhile Transformers have achieved state-of-the-art results in chemical tasks such as property\nprediction and molecule generation, their self-attention mechanism is constrained by its inability to\nmodel data outside of aﬁnite context window and its quadratic scaling with respect to window length.\nIn contrast, SSMs offer a promising alternative for sequence modeling, enabling the capture of\ncomplex patterns and dependencies in molecular structures. Our Mamba architecture, a simpliﬁed\nend-to-end SSM-based neural network, eliminates the need for attention and MLP blocks, allowing for\nfaster inference. We pre-train Mamba on a large, curated dataset of 91 million SMILES samples\n(equivalent to 4 billion molecular tokens) sourced from PubChem, and evaluate its performance on\nvarious benchmark datasets. Our experiments demonstrate the SSM’s capacity to provide state-of-\nthe-art results while maintaining fast inference, supporting complex tasks such as molecular property\nprediction, classiﬁcation, molecular reconstruction, and synthesis yield prediction. This work\nadvances the state-of-the-art in AI methodology in chemical sciences, offering a promising direction\nfor future research in molecular modeling and discovery.\nLarge-scale pre-training methodologies for chemical language models\n(LMs) represent a signiﬁcant advancement in cheminformatics1.T h e s e\nmethodologies have demonstrated impressive results in challenging mole-\ncular tasks such as property prediction and molecule generation2.T h e i r\nsuccess is largely attributable to the ability of these models to learn rich,\ncontextualized representations of input tokens through self-supervised\nlearning on vast unlabeled corpora3.\nMost chemical foundation models currently available are based on the\nTransformer architecture and its core attention module4–6.T h ee fﬁcacy of\nself-attention lies in its capacity to densely route information within aﬁxed\ncontext window7, enabling the modeling of complex chemical data8.\nHowever, this mechanism is inherently limited by its inability to incorporate\ninformation outside theﬁnite context window, and it suffers from quadratic\nscaling with respect to sequence length\n9. In response, a substantial body of\nresearch has been devoted to developing more ef ﬁcient attention\nmechanisms10.\nStructured state space sequence models (SSMs) have recently emerged\nas a promising alternative for sequence modeling11.T h e s em o d e l sc o m b i n e\ncharacteristics of recurrent neural networks (RNNs) and convolutional\nneural networks (CNNs)12 to achieve efﬁcient computation via recurrence\nor convolution, offering linear or near-linear scaling with sequence length.\nMamba, a simpliﬁed end-to-end SSM-based neural network architecture\nthat eschews both traditional attention and even MLP blocks, exempliﬁes\nthis approach13. Mamba provides fast inference and scales linearly with\nsequence length, which is especially advantageous for processing the often\nlengthy SMILES strings encountered in chemical datasets.\nSSMs are particularly well-suitedfor modeling molecular data due\nto several inherent advantages. First, the near-linear scaling of SSMs\nenables ef ﬁcient handling of long SMILES strings — crucial when\ndealing with extensive chemical databases such as PubChem14,w h e r ea\nhigh percentage of molecules exhibit long sequences15.S e c o n d ,t h e\ncombination of recurrent and convolutional elements allows SSMs to\ncapture both local chemical interactions (e.g., bonding patterns) and\nglobal structural dependencies (e.g., overall molecular topology) more\neffectively than ﬁxed-context approaches like Transformers\n16.T h i s\nholistic capture of structural nuances underpins robust molecular\nproperty prediction, reaction-yield estimation, and molecule\nreconstruction.\nIn this study, we present a novel Mamba-based large foundation\nmodel, denoted as O\nSMI-SSM-336M. Our encoder-decoder foundation\nmodel is trained using an efﬁcient SSM-based encoder aligned with an auto-\nencoder mechanism on a large corpuso f9 1m i l l i o nc a r e f u l l yc u r a t e d\n1IBM Research, Rio de Janeiro, Brazil.2IBM Research, Almaden, CA, USA. e-mail: eduardo.soares@ibm.com\nnpj Artiﬁcial Intelligence|             (2025) 1:8 1\n1234567890():,;\n1234567890():,;\nmolecules from PubChem17, resulting in 4 billion molecular tokens. Our\nmain contributions are as follows:\n We pre-train a large-scale Mamba-based foundation model for\nmolecules, denoted as OSMI-SSM-336M, on over 91 million molecules\ncarefully curated from PubChem17, which is equivalent to 4 billion\nmolecular tokens.\n We perform extensive experimentation across 11 benchmark datasets\nencompassing quantum mechanical, physical, biophysical, and phy-\nsiological property prediction of small molecules.\n We evaluate the model’s capability to predict chemical reaction yields\nin both synthetic and process chemistry scenarios using the\nBuchwald–Hartwig cross-coupling reaction dataset, where reaction\nyields denote the percentage of reactants converted into products.\n We assess the reconstruction capacity of our model on the MOSES\nbenchmarking dataset\n18.\n We present a comparative study of inference speeds, demonstrating\nthat our Mamba-based model outperforms a Transformer-based\nmodel in predicting HOMO-LUMO properties for 10 million\nrandomly selected samples from PubChem.\nOur results indicate that O\nSMI-SSM-336M achieves state-of-the-art\nperformance across various tasks, including molecular property prediction,\nreaction-yield estimation, and molecule reconstruction. Importantly, the\nproposed model strikes an effective balance between inference speed and\npredictive performance, delivering state-of-the-art results in nearly half the\ntime required by Transformer-based architectures.\nResults\nExperiments\nTo evaluate the effectiveness of our proposed Mamba-based model OSMI-\nSSM-336M, we conducted experiments using a set of 11 datasets sourced\nfrom MoleculeNet19 as demonstrated in Table1.S p e c iﬁcally, we evaluated\nsix datasets for classiﬁcation task andﬁve datasets for regression tasks. To\nensure an unbiased assessment, we maintained consistency with the original\nbenchmark by adopting identical train/validation/test splits for all tasks19.\nWe also conducted the experiments considered ten different seeds for all the\ntests in other to guarantee the robustness of the approach.\nWe also conducted high-throughput experiments on Pd-catalyzed\nBuchwald–Hartwig C-N cross-coupling reactions, measuring the yields for\neach reaction as described in ref.20. The experiments utilized three 1536-\nwell plates, covering a matrix of 15 aryl and heteroaryl halides, four Buch-\nwald ligands, three bases, and 23 isoxazole additives, resulting in a total of\n3955 reactions. We employed the same data splits as in ref.20 to assess our\nmodel’s performance with training sets of varying sizes.\nTo evaluate the reconstruction and decoder capabilities of OSMI-SSM-\n336M, we utilized the MOSES benchmarking dataset\n18, which contains\n1,936,962 molecular structures. For the experiments, we adopted the dataset\nsplit proposed by ref.18, dividing it into training, test, and scaffold test sets,\ncomprising approximately 1.6 million, 176,000, and 176,000 molecules,\nrespectively. The scaffold test set includes unique Bemis-Murcko scaffolds\nthat are absent in the training and test sets, allowing us to assess the model’s\nability to generate previously unobserved scaffolds. Finally, we evaluated the\ninference speed of OSMI-SSM-336M by predicting HOMO-LUMO prop-\nerties for 10 million samples randomly selected from PubChem.\nComparison with SOTA on benchmarking tasks\nResults for classiﬁcation tasks. The analysis evaluates the comparative\nperformance of OSMI-SSM-336M in its ﬁne-tuned and frozen states\nrelative to state-of-the-art algorithms for molecular property classiﬁca-\ntion, as detailed in Table2.\nTable 2 summarizes the perform ance of various advanced\nmethods across several benchmar king datasets used for molecular\nclassi ﬁcation tasks. OSMI-SSM-336 M demonstrates comparative\nefﬁcacy against Transformer-based approaches, outperforming them\nin three out of six datasets. Notably, OSMI-SSM-336M with its initial\nconﬁguration yields results on par with current state-of-the-art\nmethods. Further ﬁne-tuning of OSMI-SSM-336 M enhances its per-\nformance, indicating its substantial potential for accurate molecular\nclassi ﬁcation and suggesting that additional performance gains may\nbe achieved through further optimization21–25.\nResults for regression tasks. Subsequently, we applied OSMI-SSM-\n336M to the prediction of chemical properties. The performance metrics\nacross ﬁve regression benchmarks— QM9, QM8, ESOL, FreeSolv, and\nLipophilicity— are presented in Table3.\nResults presented in Table3 indicate that OSMI-SSM-336M achieves\nperformance comparable to state-of-the-art models, securing the second-\nbest results in four of theﬁve regression benchmarks evaluated. This\ndemonstrates the efﬁcacy of the Mamba-based approach in delivering\nresults on par with Transformer-based methods, while also highlighting its\nrobustness across a range of chemical property prediction tasks. The design\nof OSMI-SSM-336M aims to strike an optimal balance between predictive\naccuracy and inference efﬁciency. To exemplify this balance, we provide an\nanalysis comparing the inference time for predicting HOMO-LUMO\nproperties on a dataset of 10 million samples randomly selected from\nPubChem. This study underscores the model’s capability to maintain high\nprediction accuracy while signiﬁcantly reducing computational time,\nthereby offering practical advantages for large-scale chemical property\npredictions.\nSpeed inference for HUMO-LUMO properties prediction. To assess\nthe inference speed of the proposed Mamba-based approach, we con-\nducted predictions of HOMO-LUMO properties for 10 million samples\nrandomly selected from PubChem. For comparison, we evaluated the\nTable 1 | Evaluated datasets description\nDataset Description # compounds # tasks Metric\nBBBP Blood brain barrier penetration dataset 2039 1 ROC-AUC\nHIV Ability of small molecules to inhibit HIV replication 41,127 1 ROC-AUC\nBACE Binding results for a set of inhibitors for β - secretase 1 1513 1 ROC-AUC\nClintox Clinical trial toxicity of drugs 1478 2 ROC-AUC\nSIDER Drug side effect on different organ classes 1427 27 ROC-AUC\nTox21 Toxicity measurements on 12 different targets 7831 12 ROC-AUC\nQM9 12 quantum mechanical calculations 133,885 12 Average MAE\nQM8 12 excited state properties of small molecules 21,786 12 Average MAE\nESOL Water solubility dataset 1128 1 RMSE\nFreeSolv Hydration free energy of small molecules in water 642 1 RMSE\nLipophilicity Octanol/water distribution coef ﬁcient of molecules 4200 1 RMSE\nhttps://doi.org/10.1038/s44387-025-00009-7 Article\nnpj Artiﬁcial Intelligence|             (2025) 1:8 2\ninference time of SMI-TED289M, a Transformer-based model recog-\nnized for its state-of-the-art performance. Figure1 illustrates the superior\ninference speed of OSMI-SSM-336M compared to SMI-TED289M.\nSpeciﬁcally, SMI-TED289M required 20,606.76 s for HOMO property\npredictions and 21,038.43 s for LUMO property predictions using a\nsingle NVIDIA V100 32GB GPU. In contrast, OSMI-SSM-336 M\ncompleted HOMO predictions in 9735.64 s and LUMO predictions in\n9823.64 s on the same GPU. These results highlight the substantial\nefﬁciency gains of the O\nSMI-SSM-336M model in terms of infer-\nence speed.\nThe Mamba-base approach demonstrates a substantial improve-\nment in efﬁciency, being approximately 54% faster and reducing GPU\nusage by 6 h, while also decreasing CO2 emissions by an average of\n0.78 kg equivalent26. This reduction in computational resources is cru-\ncial for minimizing the environmental impact of machine learning\nmodels, which requires signiﬁcant energy consumption and associated\ncarbon footprints\n27.\nReaction-yield prediction\nPreviously, we were able to show that the proposed Mamba-based model\nwas able to perform compared to transformer-based methods on single\nmolecule properties prediction. Here, we investigate the Mamba-based\napproach on chemical reactions. Chemical reactions in organic chemistry\nare described by writing the structural formula of reactants and products\nseparated by an arrow, representing the chemical transformation by spe-\ncifying how the atoms rearrange between one or several reactant molecules\nand one or several product molecules. Predicting outcomes of chemical\nreactions, such as their yield based on data gathered in high-throughput\nscreening, is an important task in machine learning for chemistry. Figure2\nshows the schema for chemical reaction.\nWe assessed this architecture against state-of-the-art methods using a\nhigh-throughput dataset of Buchwald–Hartwig cross-coupling reactions,\nfocusing on predicting reaction yields\n20. This involves estimating the per-\ncentage of reactants converted into products. Our evaluation adhered to the\nschema and data divisions outlined in ref.20.T a b l e4 presents the results for\nTable 2 | Methods and performance for the classiﬁcation tasks of MoleculeNet benchmark datasets\nMethod Dataset\nBBBP ClinTox HIV BACE SIDER Tox21\nGraphMVP37 72.4 ± 1.6 79.1 ± 2.8 77.0 ± 1.2 81.2 ± 0.9 63.9 ± 1.2 75.9 ± 0.5\nGEM38 72.4 ± 0.4 90.1 ± 1.3 80.6 ± 0.9 85.6 ± 1.1 67.2 ± 0.4 78.1 ± 0.1\nGROVERLarge\n39 69.5 ± 0.1 76.2 ± 3.7 68.2 ± 1.1 81.0 ± 1.4 65.4 ± 0.1 73.5 ± 0.1\nChemBerta5 64.3 90.6 62.2 –––\nChemBerta240 71.94 90.7 – 85.1 ––\nGalatica 30B41 59.6 82.2 75.9 72.7 61.3 68.5\nGalatica 120B41 66.1 82.6 74.5 61.7 63.2 68.9\nUni-Mol42 72.9 ± 0.6 91.9 ± 1.8 80.8 ± 0.3 85.7 ± 0.2 65.9 ± 1.3 79.6 ± 0.5\nMolFM42 72.9 ± 0.1 79.7 ± 1.6 78.8 ± 1.1 83.9 ± 1.1 64.2 ± 0.9 77.2 ± 0.7\nMoLFormer43 73.6 ± 0.8 91.2 ± 1.4 80.5 ± 1.65 86.3 ± 0.6 65.5 ± 0.2 80.46 ± 0.2\nSMI-TED289M (Frozen Weights)44 91.46 ± 0.47 93.49 ± 0.85 80.51 ± 1.34 85.58 ± 0.92 66.01 ± 0.88 81.53 ± 0.45\nSMI-TED289M (Fine-tuned)44 92.26 ± 0.57 94.27 ± 1.83 76.85 ± 0.89 88.24 ± 0.50 65.68 ± 0.45 81.85 ± 1.42\nOSMI-SSM-336M (Frozen) 90.81 ± 0.85 86.36 ± 0.74 77.04 ± 0.64 83.83 ± 0.76 63.52 ± 0.3 81.42 ± 0.8\nOSMI-SSM-336M(Fine-tuned) 92.81 ± 0.27 90.02 ± 0.5 83.14 ± 0.34 86.12 ± 0.96 63.17 ± 0.75 83.84 ± 0.2\nBold values indicate the best results for each task.\nTable 3 | Methods and performance for the regression tasks of MoleculeNet benchmark datasets\nBlue and Orange indicates best and second-best performing model, respectively.\nhttps://doi.org/10.1038/s44387-025-00009-7 Article\nnpj Artiﬁcial Intelligence|             (2025) 1:8 3\nthe OSMI-SSM-336M model and compares its performance with existing\nstate-of-the-art approaches.\nThe results presented in Table4 clearly demonstrate the superiority of\nthe proposed Mamba-based foundation model when benchmarked against\nstate-of-the-art methods, including gradient-boosting andﬁngerprint-\nbased approaches (DRFP)28, a DFT-based random forest model (DFT)28,\nand transformer-based models like Yield-BERT29 and its augmented var-\niant, Yield-BERT(aug.)29,a n dM S R 2 - R X N30. The performance of the\nMamba-based model can be attributedto its pre-training on an expansive\ndataset of 91 million curated molecules, which provides a robust foundation\nof chemical knowledge that signiﬁcantly enhances its predictive capabilities.\nThis pre-training enables the model to achieve high accuracy even with\nlimited training data, as evidenced by its sustained performance when\ntrained on just 2.5% of the available samples— a scenario where task-speciﬁc\nmodels experience a marked decline in accuracy. To ensure the robustness\nof our model, we conducted each experiment with ten different ran-\ndom seeds.\nOne key observation is the model’sr o b u s t n e s sa c r o s sv a r i o u sd a t a\nsplits, particularly in low-resource settings where only a small fraction of the\ndataset is used for training. This resilience underscores the importance of\nleveraging large-scale pre-training to encode generalized chemical knowl-\nedge, which can then beﬁne-tuned for speciﬁc tasks like reaction-yield\nFig. 2 |This ﬁgure illustrates the schema for chemical reaction-yield prediction based on reaction SMILES considering the OSMI-SSM-336M model.\nTable 4 | Performance of OSMI-SSM-336M compared with the state of the art in reaction-yield prediction on experimentally\ndetermined yields of Buchwald– Hartwig reactions through HTEs\nSubset/Split DFT Yield-BERT Yield-BERT (Aug) DRFP YieldGNN MSR2-RXN Ours\nRand 70/30 0.92 0.95 ± 0.005 0.97 ± 0.003 0.95 ± 0.005 0.96 ± 0.005 0.94 ± 0.005 0.9823 ± 0.0007\nRand 50/50 0.9 0.92 ± 0.01 0.95 ± 0.01 0.93 ± 0.01 – 0.93 ± 0.01 0.982 ± 0.0004\nRand 30/70 0.85 0.88 ± 0.01 0.92 ± 0.01 0.89 ± 0.01 – 0.90 ± 0.01 0.978 ± 0.0013\nRand 20/80 0.81 0.86 ± 0.01 0.89 ± 0.01 0.87 ± 0.01 – 0.87 ± 0.01 0.973 ± 0.0006\nRand 10/90 0.77 0.79 ± 0.02 0.81 ± 0.02 0.81 ± 0.01 – 0.80 ± 0.02 0.952 ± 0.0023\nRand 5/95 0.68 0.61 ± 0.04 0.74 ± 0.03 0.73 ± 0.02 – 0.69 ± 0.03 0.903 ± 0.0043\nRand 2.5/97.5 0.59 0.45 ± 0.05 0.61 ± 0.04 0.62 ± 0.04 – 0.57 ± 0.05 0.846 ± 0.0044\nTest 1 0.8 0.84 ± 0.01 0.80 ± 0.01 0.81 ± 0.01 – 0.83 ± 0.03 0.9827 ± 0.0002\nTest 2 0.77 0.84 ± 0.03 0.88 ± 0.02 0.83 ± 0.003 – 0.83 ± 0.01 0.9827 ± 0.0005\nTest 3 0.64 0.75 ± 0.04 0.56 ± 0.08 0.71 ± 0.001 – 0.69 ± 0.04 0.9823 ± 0.0012\nTest 4 0.54 0.49 ± 0.05 0.43 ± 0.04 0.49 ± 0.004 – 0.51 ± 0.04 0.9825 ± 0.0008\nAverage 1-4 0.69 0.73 0.58 ± 0.33 0.71 ± 0.16 – 0.72 ± 0.15 0.9826 ± 0.0005\nBold values indicate the best results for each task.\nFig. 1 |The ﬁgure shows the inference speed for OSMI-SSM-336M and SMI-TED289M for HOMO-LUMO predictions considering a dataset of 10M samples randomly\nselected from PubChem and a single NVIDIA V100 32GB GPU.\nhttps://doi.org/10.1038/s44387-025-00009-7 Article\nnpj Artiﬁcial Intelligence|             (2025) 1:8 4\nprediction. In contrast, models that are tailored speciﬁcally for a given task\ntend to overﬁt to the nuances of the training data and struggle to generalize\nwhen the training set size is reduced, highlighting a critical limitation in their\ndesign.\nMoreover, the robustness of the Mamba-based model extends to its\nperformance on out-of-domain test sets. The ability to generalize well to\ndata distributions that differ from thetraining set is a crucial aspect of model\nevaluation, particularly in real-world applications where the diversity of\nchemical reactions is vast. The Mamba-based model’s consistent perfor-\nmance across both in-domain and out-of-domain test sets illustrates the\nefﬁcacy of pre-training on a diverse and comprehensive dataset, which\nequips the model with theﬂexibility to handle a wide range of chemical\nenvironments and reaction conditions.\nThe comparative analysis between the Mamba-based model and other\nstate-of-the-art methods also sheds light on the limitations of traditional\napproaches like DFT-based models, which, despite their theoretical\ngrounding in quantum chemistry, may not capture the full complexity of\nreaction mechanisms in practical scenarios. Similarly, while transformer-\nbased models like Yield-BERT and its augmented variant exhibit strong\nperformance, they fall short of the Mamba-based model, particularly in low-\ndata regimes, indicating that the sheerscale and diversity of the pre-training\ndata play a pivotal role in achieving superior results.\nThese ﬁndings underscore the potential of foundation models in\nchemistry, where pre-training on large, diverse datasets can serve as a\npowerful paradigm for developing models that are not only accurate but also\nrobust and generalizable. The implications of this work extend beyond\nreaction-yield prediction, suggesting that similar strategies could be applied\nto other domains within computational chemistry and materials science,\nwhere the ability to generalize across diverse datasets is of paramount\nimportance.\nDecoder evaluation over MOSES benchmarking dataset\nNext, conducted a comparative evaluation of the OSMI-SSM-336M model\nagainst several baseline models for SMILES reconstruction and decoding,\nusing a test set comprising 176,000 molecules. The evaluation metrics,\ndetailed in Table5, provide a comprehensive view of the model’sp e r f o r -\nm a n c ei nk e ya r e a ss u c ha sf r a g m e n ts i milarity (Frag), scaffold similarity\n(Scaf), similarity to the nearest neighbor (SNN), internal diversity (IntDiv),\nand Fréchet ChemNet Distance (FCD).\nThe results indicate that O\nSMI-SSM-336M n o to n l ym a t c h e sb u ts u r -\npasses the performance of state-of-the-art models in generating unique,\nvalid, and novel molecules. Its near-perfect score in the Frag metric high-\nlights its remarkable ability to retainthe structural integrity of molecular\nfragments, a crucial aspect in ensuring the generated molecules remain\nchemically viable and relevant to real-world applications. This high frag-\nment similarity, coupled with the model’s low FCD score, suggests that the\ndistribution of generated molecules closely mirrors that of natural\nmolecules.\nIn addition to fragment-level accuracy, O\nSMI-SSM-336M demon-\nstrates superior performance in scaffold similarity (Scaf) and nearest\nneighbor similarity (SNN). These metrics are particularly important in drug\ndiscovery and design, where the preservation of core molecular scaffolds is\nessential for maintaining biological activity. The model’s ability to generate\nmolecules with high scaffold similarity indicates that it can reliably repro-\nduce the core structural features of molecules, which is a requirement for\ngenerating candidate compounds thatretain their intended biological\nfunction.\nAnother signiﬁcant ﬁnding is the model’s performance in internal\ndiversity (IntDiv). While high similarity scores are important, diversity\nwithin the generated set is equally crucial, especially in scenarios where a\nbroad exploration of chemical space is required. The O\nSMI-SSM-336M\nmodel achieves a commendable balance, maintaining high similarity\nmetrics while also generating molecules with substantial pairwise dissim-\nilarity. This capability to generatea diverse array of molecules without\nsacriﬁcing structural integrity makes the model highly valuable for appli-\ncations in drug discovery, where exploring a wide range of chemical pos-\nsibilities is often necessary toidentify optimal candidates.\nFurthermore, when compared to traditional methods such as\nCharRNN and more advanced approaches like JT-VAE and MolGen-7b,\nthe OSMI-SSM-336M model consistently outperforms across all evaluated\nmetrics. This includes models like LIMO, which, despite its strong internal\ndiversity, fails to match the other metrics, indicating a trade-off in these\napproaches that OSMI-SSM-336M successfully mitigates. The model’s\nability to achieve high scaffold similarity while maintaining diverse mole-\ncular structures suggests that its pre-training on a large-scale dataset equips\nit with a broad understanding of chemical space, enabling it to generalize\neffectively across various molecular conﬁgurations.\nDiscussion\nThis paper introduces OSMI-SSM-336M, a Mamba-based chemical foun-\ndation model pre-trained on a curated dataset of 91 million SMILES samples\nfrom PubChem, encompassing 4 billion molecular tokens. The model is\ndesigned to achieve a balance between high performance in evaluation\nmetrics and faster inference capabilities.\nThe efﬁcacy of O\nSMI-SSM-336M was rigorously assessed across a\nvariety of tasks, including molecular property classiﬁcation and prediction.\nThe model not only achieved state-of-the-art results but also demonstrated\nsigniﬁcant efﬁciency improvements. Speciﬁcally, it was approximately 54%\nfaster than existing state-of-the-artTransformer-based approaches, redu-\ncing GPU usage by 6 h and lowering CO2 emissions by an average of 0.78 kg\nCO2 equivalent\n26 during the prediction of HOMO-LUMO gaps for a dataset\nof 10 million randomly selected samples from PubChem.\nWe also explored the model’s capabilities in predicting chemical reac-\ntion outcomes, such as reaction yields based on high-throughput screening\ndata, a critical task in machine learning for chemistry. The consistent per-\nformance of the Mamba-based model across both in-domain and out-of-\ndomain test sets underscores the effectiveness of pre-training on a diverse and\ncomprehensive dataset. This pre-training enables the model to adapt to a wide\nrange of chemical environments and reaction conditions. Our comparative\nanalysis revealed that while traditional approaches, such as DFT-based\nmodels, are grounded in quantum chemistry, they may not fully capture the\ncomplexity of reaction mechanisms in practical scenarios. Similarly,\ntransformer-based models like Yield-BERT and its augmented variant,\ndespite their strong performance, are outperformed by the Mamba-based\nmodel, particularly in low-data regimes.This highlights the critical role that\nlarge-scale, diverse pre-training data plays in achieving superior results.\nFinally, we conducted a comparative evaluation of the O\nSMI-SSM-\n336M model against several baseline models for SMILES reconstruction and\ndecoding. The model’s performance across diverse metrics demonstrates\nthe importance of leveraging large-scale dataset for pre-training, which can\nlead to models that not only excel in generating high-quality molecules but\nalso possess theﬂexibility required to tackle complex challenges in com-\nputational chemistry and drug design.\nThe Mamba-based foundation model presented in this paper offers\nboth ﬂexibility and scalability for a wide range of scientiﬁc applications.\nTable 5 | MOSES benchmarking dataset evaluation\nMetric Frag ↑ Scaf ↑ SNN ↑ IntDiv ↑ FCD ↓\nCharRNN18 0.9998 0.9242 0.6015 0.8562 0.0732\nVAE18 0.9984 0.9386 0.6257 0.8558 0.0990\nJT-VAE45 0.9965 0.8964 0.5477 0.8551 0.3954\nLIMO46 0.6989 0.0079 0.2464 0.9039 26.78\nMolGen-7b47 0.9999 0.6538 0.5138 0.8617 0.0435\nGP-MoLFormer48 0.9998 0.7383 0.5045 0.8655 0.0591\nOSMI-SSM-336M 0.9999 0.9994 0.9960 0.8561 0.0025\nBold values indicate the best results for each task.\nhttps://doi.org/10.1038/s44387-025-00009-7 Article\nnpj Artiﬁcial Intelligence|             (2025) 1:8 5\nMethods\nThis section presents an overview of the proposed OSMI-SSM-336M foun-\ndation model for small molecules. Here, we outline the process of collecting,\ncurating, and pre-processing the pre-train data. Additionally, we describe\nthe token encoder process and the SMILES encoder-decoder process.\nPre-training data\nThe pre-training data was sourced from the PubChem data repository, a\npublic database containing informati o no nc h e m i c a ls u b s t a n c e sa n dt h e i r\nbiological activities\n17. Initially, 113 million SMILES strings were collected\nfrom PubChem. These molecular strings underwent deduplication and\ncanonicalization using standard procedures implemented in RDKit\n31,32;\neach SMILES string was standardized, converted to its canonical form, and\nsanitized to ensure both uniqueness and chemical validity by checking for\nconformant valence and bonding rules. Following this, a molecular trans-\nformation process was applied— incorporating additional chemical saniti-\nzation checks— to validate the molecules derived from the unique SMILES\nstrings, resulting in aﬁnal curated set of 91 million unique and chemically\nsound molecules.\nTo construct the vocabulary, we utilized the molecular tokenizer\nproposed by ref.33. The tokenization process was applied to all 91 million\ncurated molecules from PubChem, yielding a set of 4 billion molecular\ntokens. From this output, we extracted 2988 unique tokens, along with\n5 special tokens. In contrast, MoLFormer, which was trained on 1 billion\nsamples with minimal curation, generated a vocabulary of 2362 tokens using\nthe same tokenization method\n2. This indicates that our comprehensive\ncuration process led to an enhanced and more representative vocabulary\nmodel. Detailed statistics of the pre-training dataset are provided in Table6.\nModel architecture\nWe conduct training for OSMI-SSM-336M using a Mamba-based token\nencoder together with an encoder-decoder architecture to effectively map\nSMILES sequences into a latent embedding space and back. O\nSMI-SSM-\n336M design leverages the strengths of SSMs to capture long-range\ndependencies and process sequences with near-linear scaling. In our\napproach, the encoder processes input tokens via Mamba blocks, while the\ndecoder reconstructs these embeddings to accurately generate SMILES\nsequences.\nThe hyperparameters for the model are speciﬁed in Table 7.T h e\nhidden size is set to 768 with 24 layers in total. It is important to highlight\nthat the layer count in our Mamba-based model is effectively doubled\ncompared to a Transformer with a similar size. In traditional Transformers,\neach layer comprises a multi-head attention (MHA) block followed by a\nmultilayer perceptron (MLP) block. In contrast, our architecture imple-\nments two distinct Mamba blocks for each such Transformer layer— one\nhandling the functions analogous to the MHA and the other corresponding\nto the MLP— resulting in a doubled layer count.\nAdditional parameters— such as dt rank (set automatically), d state of\n16, d conv of 4, and an expansion factor of 2— a r et u n e dt ob a l a n c em o d e l\nexpressiveness and computational efﬁciency. The parameters dt min\n(0.001), dt max (0.1), dt scale (1.0), and dt initﬂoor (1e−4) govern the\ndynamics of the continuous-time system underlying our model, ensuring\nstable training and effective representation learning.\nMamba models originates from a continuous-time system that maps\nan input function or sequencexðtÞ2 R\nM to an output response signal\nyðtÞ2 RO through an implicit latent statehðtÞ2 RN which can be\nmathematically formulated using the following ordinary differential Eq. (1):\nh0ðtÞ¼ AhðtÞþ BxðtÞ;\nyðtÞ¼ ChðtÞþ DxðtÞ ð1Þ\nwhere A 2 RN × N and C 2 RO × N control how the current state evolves\nover time and translates to the output,B 2 RN × M andD 2 RO × M depict\nhow the input inﬂuences the state and the output, respectively.\nThe tokens extracted from SMILES trough the SSM encoder are\nembedded in a 768-dimensional space. The encoder-decoder layer is\ndesigned to process molecular token embeddings, represented as\nx 2 R\nT × L,w h e r eT denotes the maximum number of tokens andL\nrepresents the embedding space dimension. We limitedT at 202 tokens, as\n99.4% of molecules in the PubChem dataset contain fewer tokens than this\nthreshold\n2.\nIn encoder-only models, a mean pooling layer is typically employed to\nrepresent tokens as SMILES in the latent space34.H o w e v e r ,t h i sa p p r o a c hi s\nlimited by the lack of a natural inversion process for the mean pooling\noperation. To overcome this limitation, we aim to construct a latent space\nrepresentation for SMILES by submersing thex in a latent space, denoted as\nz, as described in Eq. (2):\nz ¼ LayerNorm GELU xW1 þ b1\n/C0/C1/C0/C1/C0/C1\nW2; ð2Þ\nwhere z 2 RL, W1 2 RL, b1 2 RL, W2 2 RL × L,w i t hL denoting the\nlatent space size (speciﬁcally,L = 768). Subsequently, we can immersez back\nTable 7 | OSMI-SSM-336M base architecture speciﬁcity\nHidden\nsize\nLayers dt\nrank\nd state d conv expand\nfactor\ndt min dt max dt scale dt init ﬂoor\n768 24 auto 16 4 2 0.001 0.1 1.0 1e −4\nconv bias bias lr start lr multiplier Vocab size # SMILES # Mol tokens # Encoder # Decoder Total params\nTrue False 3e −5 1 2993 91M 4B 94M 242M 336M\nTable 6 | Pre-training dataset statistics\nProperty Mean Std Min 25% 50% 75% Max\nNumber of atoms 48.95 45.19 1.00 30.00 40.00 53.00 1687.00\nMolecular Weight (Daltons) 344.15 137.79 1.01 265.32 330.37 402.47 18,838.70\nLogP 3.18 2.18 −88.97 2.12 3.29 4.36 59.81\nNumber of H-bond acceptors 4.29 2.62 0 3.00 4.00 5.00 191\nNumber of H-bond donors 1.18 1.48 0 0.00 1.00 2.00 116\nNumber of rotatable bonds 4.79 4.09 0 3.00 4.00 6.00 240\nTopological polar surface area 67.81 50.11 0 40.54 61.77 84.22 4201.50\nNumber of aliphatic rings 0.72 1.07 0 0.00 0.00 1.00 54\nNumber of aromatic rings 1.96 1.24 0 1.00 2.00 3.00 32\nhttps://doi.org/10.1038/s44387-025-00009-7 Article\nnpj Artiﬁcial Intelligence|             (2025) 1:8 6\nby calculating Eq. (3):\n^x ¼ LayerNorm GELU zW3 þ b3\n/C0/C1/C0/C1/C0/C1\nW4 ð3Þ\nwhere ^x 2 RT × L, W3 2 RL × L, b3 2 RL, W4 2 RL × T .W h e r eT repre-\nsenting the output feature space size (namely,T = 202).\nA language layer (decoder) is used to process^x, where it applies non-\nlinearity and normalization, and projects the resulting vector into a set of\nlogits over the vocabulary, which can then be used to predict the next token in\nthe molecular\n35. This architecture serves as a tool for dimensionality reduction\nand representation learning in the domain of molecular structures.\nPre-training strategies\nPre-training of OSMI-SSM-336M was performed for 130 epochs on the\nentire curated PubChem dataset using aﬁxed learning rate of 3 × 10−5 and a\nbatch size of 128 molecules on 24 NVIDIA V100 (16G) GPUs, distributed\nacross 4 nodes via DDP andtorch run. The pre-training process is divided\ninto two distinct phases:\n Phase 1: in this initial phase, the token encoder is pre-trained using 95%\nof the available samples, while the remaining 5% is reserved exclusively\nfor training the encoder-decoder layer. This partitioning is necessary to\nmitigate convergence difﬁculties in the early epochs of token\nembedding learning, which could otherwise adversely affect the\ntraining of the encoder-decoder component.\n Phase 2: once the token embeddings have converged, the pre-training is\nexpanded to utilize 100% of the available samples for both phases. This\napproach enhances the performance of the encoder-decoder layer,\nparticularly in terms of token reconstruction accuracy.\nFor encoder pre-training, we employ the masked language modeling\nstrategy as deﬁned in ref.36. Initially, 15% of the tokens are selected for\npossible prediction; of these, 80% are replaced with the [MASK] token, 10%\nare substituted with a random token, and the remaining 10% remain\nunchanged. This strategy facilitates the learning of robust, contextualized\nrepresentations of the SMILES tokens.\nOur approach also incorporates a latent space embedding mechanism\nthat supersedes conventional mean pooling. Instead of aggregating token\nembeddings via an averaging operation (which lacks a natural inversion\nprocess), the token embeddings are transformed into a latent vectorz (as\ndetailed in Eqs. (2)a n d(3)o ft h e“Model architecture” section). This latent\nrepresentation captures intricate structural nuances of the SMILES strings\nand supports a reversible mapping, thereby enabling both accurate SMILES\nreconstruction and effective downstream tasks such as molecular property\nprediction.\nThe pre-training procedure is guided by two distinct loss functions.\nThe ﬁrst loss function, based on the masked language model objective, uses\ncross-entropy loss to predict the masked tokens. The second loss function\ngoverns the reconstruction task and is measured using the mean squared\nerror (MSE) between the original tokens and their reconstructions gener-\nated by the encoder-decoder layer. Monitoring these metrics ensures the\nconvergence of the token embeddings and the stability of the latent space\nrepresentation throughout training.\nData availability\nNo datasets were generated or analyzed during the current study.\nCode availability\nAll Python codes for training andﬁne-tuning OSMI-SSM-336M, together\nwith Python notebooks for experimental evaluations, are available athttps://\ngithub.com/IBM/materials/tree/main/models/smi_ssed. Pre-trained model\nweights can be accessed via our HuggingFace repository at https://\nhuggingface.co/ibm-research/materials.smi_ssed. For other enquiries con-\ntact the corresponding authors.\nReceived: 8 November 2024; Accepted: 3 May 2025;\nReferences\n1. Sadybekov, A. V. & Katritch, V. Computational approaches\nstreamlining drug discovery.Nature 616, 673– 685 (2023).\n2. Ross, J. et al. Large-scale chemical language representations capture\nmolecular structure and properties.Nat. Mach. Intell.4, 1256– 1264\n(2022).\n3. Bommasani, R. et al. On the opportunities and risks of foundation\nmodels. Preprint atarXiv https://doi.org/10.48550/arXiv.2108.07258\n(2021).\n4. Pesciullesi, G., Schwaller, P., Laino, T. & Reymond, J.-L. Transfer\nlearning enables the molecular transformer to predict regio-and\nstereoselective reactions on carbohydrates.Nat. Commun.11, 4874\n(2020).\n5. Chithrananda, S., Grand, G. & Ramsundar, B. Chemberta: large-scale\nself-supervised pretraining for molecular property prediction. Preprint\nat arXiv https://doi.org/10.48550/arXiv.2010.09885 (2020).\n6. Janakarajan, N., Erdmann, T., Swaminathan, S., Laino, T. & Born, J.\nLanguage models in molecular discovery. Preprint atarXiv https://doi.\norg/10.48550/arXiv.2309.16235 (2023).\n7. Vaswani, A. et al. Attention is all you need. InAdvances in Neural\nInformation Processing Systems30 (2017).\n8. Tay, Y., Dehghani, M., Bahri, D. & Metzler, D. Efﬁcient transformers: a\nsurvey. ACM Comput. Surv.55,1 – 28 (2022).\n9. Lin, T., Wang, Y., Liu, X. & Qiu, X. A survey of transformers.AI Open3,\n111– 132 (2022).\n10. Kotei, E. & Thirunavukarasu, R. A systematic review of transformer-\nbased pre-trained language models through self-supervised learning.\nInformation 14, 187 (2023).\n11. Gu, A., Goel, K. & Ré, C. Efﬁciently modeling long sequences with\nstructured state spaces. Preprint atarXiv https://doi.org/10.48550/\narXiv.2111.00396 (2021).\n12. Smith, J. T., Warrington, A. & Linderman, S. W. Simpliﬁed state space\nlayers for sequence modeling. Preprint atarXiv https://doi.org/10.\n48550/arXiv.2208.04933 (2022).\n13. Gu, A. & Dao, T. Mamba: Linear-time sequence modeling with\nselective state spaces. Preprint atarXiv https://doi.org/10.48550/\narXiv.2312.00752 (2023).\n14. Patro, B. N. & Agneeswaran, V. S. Mamba-360: Survey of state space\nmodels as transformer alternative for long sequence modelling:\nMethods, applications, and challenges. Preprint atarXiv https://doi.\norg/10.48550/arXiv.2404.16112 (2024).\n15. Hähnke, V. D., Kim, S. & Bolton, E. E. Pubchem chemical structure\nstandardization. J. Cheminformatics10,1 – 40 (2018).\n16. Waleffe, R. et al. An empirical study of mamba-based language models.\nPreprint atarXivhttps://doi.org/10.48550/arXiv.2406.07887(2024).\n17. Kim, S. et al. Pubchem 2023 update.Nucleic Acids Res.51,\nD1373– D1380 (2023).\n18. Polykovskiy, D. et al. Molecular sets (moses): a benchmarking\nplatform for molecular generation models.Front. Pharmacol.11,\n565644 (2020).\n19. Wu, Z. et al. Moleculenet: a benchmark for molecular machine\nlearning. Chem. Sci.9, 513– 530 (2018).\n20. Ahneman, D. T., Estrada, J. G., Lin, S., Dreher, S. D. & Doyle, A. G.\nPredicting reaction performance in c– n cross-coupling using machine\nlearning. Science 360, 186– 190 (2018).\n21. Yang, K. et al. Analyzing learned molecular representations for\nproperty prediction.J. Chem. Inf. Model.59, 3370– 3388 (2019).\n22. Liu, S., Demirel, M. F. & Liang, Y. N-gram graph: simple unsupervised\nrepresentation for graphs, with applications to molecules. In\nAdvances in Neural Information Processing Systems32 (2019).\n23. Hu, W. et al. Strategies for pre-training graph neural networks. Preprint\nat arXiv https://doi.org/10.48550/arXiv.1905.12265 (2019).\nhttps://doi.org/10.1038/s44387-025-00009-7 Article\nnpj Artiﬁcial Intelligence|             (2025) 1:8 7\n24. Wang, Y., Wang, J., Cao, Z. & Barati Farimani, A. Molecular\ncontrastive learning of representations via graph neural networks.\nNat. Mach. Intell.4, 279– 287 (2022).\n25. Hu, Z., Dong, Y., Wang, K., Chang, K.-W. & Sun, Y. Gpt-gnn:\ngenerative pre-training of graph neural networks. InProceedings of\nthe 26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, 1857– 1867 (2020).\n26. Lacoste, A., Luccioni, A., Schmidt, V. & Dandres, T. Quantifying the\ncarbon emissions of machine learning. Preprint atarXiv https://doi.\norg/10.48550/arXiv.1910.09700 (2019).\n27. Rillig, M. C.,Ågerstrand, M., Bi, M., Gould, K. A. & Sauerland, U. Risks\nand beneﬁts of large language models for the environment.Environ.\nSci. Technol.57, 3464– 3466 (2023).\n28. Probst, D., Schwaller, P. & Reymond, J.-L. Reaction classiﬁcation and\nyield prediction using the differential reactionﬁngerprint drfp.Digital\nDiscov. 1,9 1– 97 (2022).\n29. Schwaller, P., Vaucher, A. C., Laino, T. & Reymond, J.-L. Prediction of\nchemical reaction yields using deep learning.Mach. Learn. Sci.\nTechnol. 2, 015016 (2021).\n30. Boulougouri, M., Vandergheynst, P. & Probst, D. Molecular set\nrepresentation learning.Nat. Mach. Intell.6, 754– 763 (2024).\n31. Landrum, G. Rdkit documentation.Release 1, 4 (2013).\n32. Heid, E., Liu, J., Aude, A. & Green, W. H. Inﬂuence of template size,\ncanonicalization, and exclusivity for retrosynthesis and reaction\nprediction applications.J. Chem. Inf. Model.62,1 6– 26 (2021).\n33. Schwaller, P. et al. Molecular transformer: a model for uncertainty-\ncalibrated chemical reaction prediction.ACS Cent. Sci.5, 1572– 1583\n(2019).\n34. Bran, A. M. & Schwaller, P. Transformers and large language models\nfor chemistry and drug discovery. Preprint atarXiv https://doi.org/10.\n48550/arXiv.2310.06083 (2023).\n35. Ferrando, J., Gállego, G. I., Tsiamas, I. & Costa-jussà, M. R. Explaining\nhow transformers use context to build predictions. Preprint atarXiv\nhttps://doi.org/10.48550/arXiv.2305.12535 (2023).\n36. D e v l i n ,J . ,C h a n g ,M . - W . ,L e e ,K .&T o u t a n o v a ,K .B e r t :p r e - t r a i n i n go fd e e p\nbidirectional transformers for language understanding. InNorth American\nChapter of the Association for Computational Linguistics(2019).\n37. Liu, S. et al. Pre-training molecular graph representation with 3d\ngeometry. Preprint atarXiv https://doi.org/10.48550/arXiv.2110.\n07728 (2021).\n38. Fang, X. et al. Geometry-enhanced molecular representation learning\nfor property prediction.\nNat. Mach. Intell.4, 127– 134 (2022).\n39. Rong, Y. et al. Self-supervised graph transformer on large-scale\nmolecular data.Adv. Neural Inf. Process. Syst.33, 12559– 12571 (2020).\n40. Ahmad, W., Simon, E., Chithrananda, S., Grand, G. & Ramsundar, B.\nChemberta-2: towards chemical foundation models. Preprint atarXiv\nhttps://doi.org/10.48550/arXiv.2209.01712 (2022).\n41. Taylor, R. et al. Galactica: A large language model for science. Preprint\nat arXiv https://doi.org/10.48550/arXiv.2211.09085 (2022).\n42. Zhou, G. et al. Uni-mol: a universal 3d molecular representation\nlearning framework.ChemRxiv (2023).\n43. Chang, J. & Ye, J. C. Bidirectional generation of structure and\nproperties through a single molecular foundation model.Nat.\nCommun. 15, 2323 (2024).\n44. Soares, E. et al. A large encoder-decoder family of foundation models\nfor chemical language. Preprint atarXiv https://doi.org/10.48550/\narXiv.2407.20267 (2024).\n45. Jin, W., Barzilay, R. & Jaakkola, T. Junction tree variational\nautoencoder for molecular graph generation. InInternational\nConference on Machine Learning, 2323– 2332 (PMLR, 2018).\n46. Eckmann, P. et al. Limo: latent inceptionism for targeted molecule\ngeneration. Proc. Mach. Learn. Res.162, 5777 (2022).\n47. Fang, Y. et al. Domain-agnostic molecular generation with self-feedback.\nPreprint atarXivhttps://doi.org/10.48550/arXiv.2301.11259(2023).\n48. Ross, J. et al. Gp-molformer: a foundation model for molecular\ngeneration. Preprint atarXiv https://doi.org/10.48550/arXiv.2405.\n04912 (2024).\nAuthor contributions\nE.S., E.V.B., and D.Z. conceived the computational experiments. E.S. and\nV.S. carried out the experiments, while E.S., E.V.B., and D.Z. analyzed the\nresults. E.V.B., R.C., and K.S. designed and supervised the project. All\nauthors contributed to and reviewed the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondenceand requests for materials should be addressed to\nEduardo Soares.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s44387-025-00009-7 Article\nnpj Artiﬁcial Intelligence|             (2025) 1:8 8"
}