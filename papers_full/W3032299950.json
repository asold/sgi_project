{
  "title": "ParsBERT: Transformer-based Model for Persian Language Understanding",
  "url": "https://openalex.org/W3032299950",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2589520192",
      "name": "Mehrdad Farahani",
      "affiliations": [
        "Islamic Azad University North Tehran Branch",
        "Umeå University"
      ]
    },
    {
      "id": "https://openalex.org/A3029070076",
      "name": "Mohammad Gharachorloo",
      "affiliations": [
        "Queensland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3008428067",
      "name": "Marzieh Farahani",
      "affiliations": [
        "Umeå University",
        "Islamic Azad University North Tehran Branch"
      ]
    },
    {
      "id": "https://openalex.org/A2794578270",
      "name": "Mohammad Manthouri",
      "affiliations": [
        "Shahed University"
      ]
    },
    {
      "id": "https://openalex.org/A2589520192",
      "name": "Mehrdad Farahani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3029070076",
      "name": "Mohammad Gharachorloo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3008428067",
      "name": "Marzieh Farahani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2794578270",
      "name": "Mohammad Manthouri",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2943064529",
    "https://openalex.org/W2800737156",
    "https://openalex.org/W2808838104",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2892839738",
    "https://openalex.org/W2921584913",
    "https://openalex.org/W1611884111",
    "https://openalex.org/W2768542007",
    "https://openalex.org/W2920894300",
    "https://openalex.org/W2989092762",
    "https://openalex.org/W4391156274",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964584413",
    "https://openalex.org/W2943031643"
  ],
  "abstract": null,
  "full_text": "ParsBERT: Transformer-based Model forPersian Language\nUnderstanding\nPreprint, compiled June 2, 2020\nMehrdad Farahani1, Mohammad Gharachorloo2, Marzieh Farahani3, and Mohammad Manthouri4\n1Department of Computer Engineering\nIslamic Azad University North Tehran Branch\nTehran, Iran\nm.farahani@iau-tnb.ac.ir\n2School of Electrical Engineering and Robotics\nQueensland University of Technology\nBrisbane, Australia\nmohammad.gharachorloo@connect.qut.edu.au\n3Department of Computing Science\nUmeå University\nUmeå, Sweden\nmafa2431@student.umu.se\n4Department of Electrical and Electronic Engineering\nShahed Univerisity\nTehran, Iran\nmmanthouri@shahed.ac.ir\nAbstract\nThe surge of pre-trained language models has begun a new era in the ﬁeld of Natural Language Processing\n(NLP) by allowing us to build powerful language models. Among these models, Transformer-based models\nsuch as BERT have become increasingly popular due to their state-of-the-art performance. However, these\nmodels are usually focused on English, leaving other languages to multilingual models with limited resources.\nThis paper proposes a monolingual BERT for the Persian language (ParsBERT), which shows its state-of-\nthe-art performance compared to other architectures and multilingual models. Also, since the amount of data\navailable for NLP tasks in Persian is very restricted, a massive dataset for di ﬀerent NLP tasks as well as pre-\ntraining the model is composed. ParsBERT obtains higher scores in all datasets, including existing ones as well\nas composed ones and improves the state-of-the-art performance by outperforming both multilingual BERT\nand other prior works in Sentiment Analysis, Text Classiﬁcation and Named Entity Recognition tasks.\nKeywords Persian · Transformers · BERT · Language Models · NLP · NLU\n1 I ntroduction\nNatural language is the tool humans use to communicate with\neach other. Thus, a vast amount of data is encoded as texts\nusing this tool. Extracting meaningful information from this\ntype of data and manipulating them using computers lie within\nthe ﬁeld of Natural Language Processing (NLP). There are dif-\nferent NLP tasks such as Named Entity Recognition (NER),\nSentiment Analysis (SA), and Question /Answering, each fo-\ncusing on a particular aspect of the text data to achieve success-\nful performance on each of these tasks, a variety of pre-trained\nword embedding and language modeling methods have been\nproposed in the recent years.\nWord2Vec [1] and GloVe [2] are pre-trained word embeddings\nmethods based on Neural Networks (NNs) that investigate the\nsemantic, syntactic, and logical relationships between words\nin a sequence to provide a static word representation vectors,\nbased on the training data. While these methods leave the con-\ntext of the input sequence out of the equation, contextualized\nword embedding methods such as ELMo [3] provide dynamic\nword embeddings by taking the context into account.\nThere are two approaches towards pre-trained language rep-\nresentations [4]: feature-based such as ELMo and ﬁne-tuning\nsuch as OpenAI GPT [5]. Fine-tuning approaches (also known\nas Transfer Learning methods) seek to train a language model\nwith large datasets of unlabeled plain texts. The parameters\nof these models are then ﬁne-tuned using task-speciﬁc data to\nachieve state-of-the-art performance over various NLP tasks\n[4, 5, 6]. The ﬁne-tuning phase, relative to pre-training, re-\nquires much less energy and time. Therefore, pre-trained lan-\nguage models can be used to save energy, time, and cost. How-\never, this comes with speciﬁc challenges. The amount of data\nand the computational resources required to pre-train an e ﬃ-\ncient language model with acceptable performance is substan-\ntial; hundreds of gigabytes of text-documents and hundreds of\nGraphical Processing Units (GPUs) [7, 6, 8, 9].\narXiv:2005.12515v2  [cs.CL]  31 May 2020\nPreprint – ParsBERT: Transformer-based Model forPersian Language Understanding 2\nAs a solution, multilingual models have been developed, which\ncan be beneﬁcial for languages with similar morphology and\nsyntactic structure (e.g., Latin-based languages). Other Non-\nLatin languages diﬀer from Latin-based languages signiﬁcantly\nand can not beneﬁt from their shared representations. There-\nfore, a language-speciﬁc approach should be adapted. For in-\nstance, the framework of Recurrent Neural Network (RNN),\nalong with morpheme representation, is proposed to overcome\nfeature engineering and data sparsity for the Mongolian NER\ntask [10].\nA similar situation applies to the Persian language. Although\nsome multilingual models include Persian, they are susceptible\nto fall behind monolingual models that are concretely trained\nover language-speciﬁc vocabulary with more massive amounts\nof Persian text data. To the best of our knowledge, no spe-\nciﬁc eﬀort has been made to pre-train a Bidirectional Encoder\nRepresentation Transformer (BERT) [4] model for the Persian\nlanguage.\nIn this paper, we take advantage of the BERT architecture [4] to\nbuild a pre-trained language model for the Persian Language,\nwhich we call ParsBERT hereafter. We evaluate this model\non three Persian NLP downstream tasks: (a) Sentiment Anal-\nysis, (b) Text Classiﬁcation, and (c) Named Entity Recogni-\ntion. We show that for all these tasks, ParsBERT outperforms\nseveral baselines, including previous multilingual and mono-\nlingual models. Thus, our contribution can be summarized as\nfollows:\n• Proposing a monolingual Persian language model\n(ParsBERT) based on the BERT architecture.\n• ParsBERT achieves better performances regarding\nother multilingual and deep-hybrid architectures.\n• ParsBERT is lighter than the original multilingual\nBERT model.\n• During this procedure, the research provided a mas-\nsive set of Persian text corpora and NLP tasks for other\nuses cases.\nThe rest of this paper is organized as follows. In section 2,\na comprehensive study of previous related works is provided.\nSection 3 outlines the methodology used to pre-train ParsBERT.\nIn the next section, 4 describes the NLP downstream tasks and\nbenchmark datasets on which the model is evaluated. Section\n5 provides a thorough discussion of the obtained results. Sec-\ntion 6 concludes this paper by providing a guideline for possi-\nble future works. Finally, section 7 appreciates everyone who\nsupports and provides the chance to possible this research.\n2 R elated Work\n2.1 Language Modelling\nLanguage modeling has gained popularity in recent years, and\nmany works have been dedicated to building models for dif-\nferent languages based on varying contexts. Some works\nhave sought to build character-level models. For example, a\ncharacter-level model with Recurrent Neural Network (RNN) is\npresented in [11]. This model reasons about word spelling and\ngrammar dynamically. Another multi-task character-level at-\ntentional network model for the medical concept has been used\nto address Out-Of-V ocabulary (OOV) problem and to sustain\nmorphological information inside the concept [12].\nContextualized language modeling is centered around the idea\nthat words can be represented di ﬀerently based on the context\nin which they appear. Encoder-decoder language models, se-\nquence autoencoders, and sequence-to-sequence models have\nthis concept [13, 14, 15]. ELMo and ULMFiT [16] are contex-\ntualized language models pre-trained on large general domain\ncorpora. They are both based on LSTM networks [17]; ULM-\nFiT beneﬁts from a regular multi-layer LSTM network while\nELMo utilizes a bidirectional LSTM structure to predict both\nnext and previous words in a sequence of words. It then com-\nposes the ﬁnal embedding for each token by concatenating the\nleft-to-right and the right-to-left representations. Both ULM-\nFiT and ELMo show considerable improvement in downstream\ntasks as compared to preceding language models and word em-\nbedding methods.\nAnother candidate for sequence-to-sequence mapping is the\nTransformer model [18], which is based on the attention mecha-\nnism to evaluate dependencies between input/output sequences.\nUnlike LSTM, this model does not incorporate any recurrence.\nThe Transformer model depends on two entities named encoder\nand decoder; the encoder takes the input sequence and maps it\nto a higher dimensional vector. This vector is then mapped to an\noutput sequence by the decoder. Several pre-trained language\nmodeling architectures are based on the transformer model,\nnamely GPT [5] and BERT [4].\nGPT includes a stack of twelve Transformer decoders. How-\never, its structure is unidirectional, meaning that each token at-\ntends only to the previous one in the sequence. On the other\nhand, BERT performs joint conditioning on both left and right\ncontexts by using a Masked Language Model (MLM) and a\nstack of transformer encoders along with the decoders. This\nway, BERT achieves an accurate pre-trained deep bidirectional\nrepresentation. There are other Transformer-based architec-\ntures such as XLNet [7], RoBERTa [6], XLM [19], T5 [8], and\nALBERT [20], all of which have presented state-of-the-art re-\nsults on multiple NLP tasks such as [21] and SQuAD [22].\nMonolingual pre-trained models have been developed for sev-\neral languages other than English. ELMo models are avail-\nable for Portuguese, Japanese, German, and Basque 1. Regard-\ning BERT-based models, BERTje for Dutch [23], Alberto for\nItalian [24], AraBERT for Arabic [25], and other models for\nFinnish [26], Russian [27] and Portuguese [28] have been re-\nleased.\nFor the Persian language, several word embeddings such as\nWord2Vec, GloVe, and FastText [29] have been presented. All\nthese word embeddings models are trained on Wikipedia cor-\npus. A thorough comparison between these models is pro-\nvided in [30] and shows that FastText and Word2Vec outper-\nform other models. Another LSTM-based language model for\nPersian is presented in [31]. Their model utilizes word embed-\ndings as word representations and achieves the best performing\nmodel with a two-layer bidirectional LSTM network.\n1https://allennlp.org/elmo\nPreprint – ParsBERT: Transformer-based Model forPersian Language Understanding 3\n2.2 NLP Downstream Tasks\nAlthough several works are presented to address NLP down-\nstream tasks such as NER and Sentiment Analysis for the\nPersian language, the subject of pre-trained networks in the\nPersian language is a new topic. Most of the work done in\nthis area is centered around machine learning or neural net-\nwork methods built from scratch for each task, due to inca-\npability of ﬁne-tuning these approaches. For instance, a ma-\nchine learning-based approach for Persian NER, using Hidden\nMarkov Model (HMM), is presented in [32]. Another approach\nfor Persian NER is provided by [33] which combines a rule-\nbased grammatical approach. Moreover, a Deep Learning ap-\nproach for Persian NER is provided in [34] facilitating bidirec-\ntional LSTM networks. Beheshti-NER [35] uses multilingual\nGoogle BERT to form a ﬁne-tuned model for Persian NER and\nis the closest work to present work. However, it only involves\na ﬁne-tuning phase for NER and does not entail developing a\nmonolingual BERT-based model for the Persian Language.\nThe same situation applies to Persian sentiment analysis as Per-\nsian NER. In [36] a hybrid combination of Convolutional Neu-\nral Networks (CNN) and Structural Correspondence, Learning\nis presented to improve sentiment classiﬁcation. Also, a graph-\nbased text representation along with Deep Neural Learning is\ncomposed in [37]. The closest work in sentiment analysis to the\npresent work is DeepSentiPers [38], which leverages CNN and\nbidirectional LSTM networks combined with FastText trained\nover a balanced and augmented version of a Persian sentiment\ndataset known as SentiPers [39].\nIt should be noted that none of these works uses pre-trained net-\nworks, and all of them focus solely on designing and combining\nmethods to produce a task-speciﬁc approach.\n3 P arsBERT: Methodology\nIn this section, the methodology of our proposed model is pre-\nsented. It consists of ﬁve main tasks, of which the ﬁrst three\nconcern the dataset and the next two concern model devel-\nopment. These tasks are data gathering, data pre-processing,\naccurate sentence segmentation, pre-training setup, and ﬁne-\ntuning.\n3.1 Data Gathering\nAlthough a few Persian text corpora are provided by the Uni-\nversity of Leipzig [40] and University of Sorbonne [41], the\nsentences in those corpora do not follow a logical corpora-level\norder and are somewhat erroneous. Also, these resources cover\nonly a limited number of writing styles and subjects. There-\nfore, to increase the generality and eﬃciency of our pre-trained\nmodel in words, phrases, and sentence levels, it was necessary\nto compose a new form of the corpus from scratch to tackle the\nlimitations mentioned earlier. This was done by crawling many\nsources such as Persian Wikipedia 2, BigBangPage 3, Chetor\n2https://dumps.wikimedia.org/fawiki/\n3https://bigbangpage.com/\n4, Eligasht 5, Digikala 6, Ted Talks subtitles 7, several ﬁctional\nbooks and novels, and MirasText [42]. The latter source has\ncrawled more than 250 Persian news websites. Table 1 demon-\nstrates the statistics of our general-domain corpus:\nTable 1: Statistics and types of each source in the proposed\ncorpus, entailing a varied range of written styles.\n# Source Type Total Docu-\nments\n1 Persian\nWikipedia\nGeneral(encyclopedia) 1,119,521\n2 BigBang Page Scientiﬁc 135\n3 Chetor Lifestyle 3,583\n4 Eligasht Itinerary 9,629\n5 Digikala Digital magazine 8,645\n6 Ted Talks General (conversational) 2,475\n7 Books Novels, storybooks, short\nstories from old to the\ncontemporary era\n13\n8 Miras-Text News categories 2,835,414\n3.2 Data Pre-Processing\nAfter gathering the pre-training corpus, an immense hierarchy\nof processing steps, including cleaning, replacing, sanitizing,\nand normalizing 8, is vital to transform the dataset into a proper\nformat. This is done via a two-step process and is illustrated in\nFigure 1.\n3.3 Document Segmentation into True Sentences\nAfter the corpus is pre-processed, it should be segmented into\nTrue Sentences related to each document to achieve remarkable\nresults for the pre-training model. A True Sentence in Persian\nis recognized based on this notations [:.!? ]. However, divid-\ning content based merely on these notations has shown to cause\nproblems. In Figure 2, an example of such issues is illustrated.\nIt can be seen that the result includes short meaningless sen-\ntences without any vital information because there are abbre-\nviations in Persian separated with the dot (.) notation. As an\nalternative, Part Of Speech (POS) can be a proper solution to\nhandle these types of errors and to produce desired outputs.\nThis procedure enables the system to learn the real relationship\nbetween the sentences in each document. Table 2 shows the\nstatistics for the pre-training corpus segmented with the POS\napproach, resulting in 38,269,471 lines of True Sentences.\n3.4 Pre-training Setup\nOur model is based on BERT model architecture [4], which in-\ncludes a multi-layer bidirectional Transformer. In particular, we\nuse the original BERT BASE conﬁguration: 12 hidden layers, 12\nattention heads, 768 hidden sizes. The total number of param-\neters in this conﬁguration is 110M. As per the original BERT\n4https://www.chetor.com/\n5https://www.eligasht.com/Blog/\n6https://www.digikala.com/mag/\n7https://www.ted.com/talks\n8https://github.com/sobhe/hazm\nPreprint – ParsBERT: Transformer-based Model forPersian Language Understanding 4\n(a) Step 1\n(b) Step 2\nFigure 1: Speciﬁc Persian corpus pre-processing that includes\ntwo steps: (a) removing all the trivial and junk characters and\n(b) standardizing the corpus with respect to Persian characters.\nTable 2: Statistics of the pre-training corpus.\n# Source Total True Sentences\n1 Persian Wikipedia 1,878,008\n2 BigBang Page 3,017\n3 Chetor 166,312\n4 Eligasht 214,328\n5 Digikala 177,357\n6 Ted Talks 46,833\n7 Books 25,335\n8 Miras-Text 35,758,281\npre-training objective, our pre-training objective consists of two\ntasks:\n1. A Masked Language Model (MLM) is employed to\ntrain the model to predict randomly masked tokens by\nusing cross-entropy loss. For this purpose given N to-\nkens, 15% of them are selected at random. From these\nselected tokens, 80% of them are replaced by an exclu-\nsive [MASK] token, 10% are replaced with a random\ntoken, and 10% remain unchanged.\n2. Implementing Next Sentence prediction (NSP) task,\nin which the model learns to predict whether the sec-\nond sentence in a pair of sentences is the actual next\nsentence of the ﬁrst one or not. In the original BERT\npaper [4], it has been argued that removing NSP\nfrom pre-training can attenuate the performance of the\nmodel on some tasks. Therefore, we employ NSP in\nour model to ensure high eﬃciency on diﬀerent tasks.\nFor model optimization [43], Adam optimizer with β1 = 0.9\nand β2 = 0.98 is used for 1.9M steps. The batch size is set to\n32, and each sequence contains 512 tokens at most. Finally, the\nlearning rate is set to 1e-4.\nSubword tokenization, which is necessary for better perfor-\nmance, is achieved using the WordPiece method [44]. Word-\nPiece operates as an intermediary between BPE [45] and Un-\nigram Language Model (ULM) approaches. WordPiece is\ntrained on our pre-training corpus with a minimum frequency of\nthree and 1.5K alphabet token limitations. The resulting vocab-\nulary consists of 100K tokens, including unique BERT-speciﬁc\ntokens, namely [PAD], [UNK], [CLS], [MASK] [SEP] and [##]\nwhich is used as a preﬁx for word relation tokenization. Table\n3 shows an example of the tokenization process based on the\nWordPiece method.\n3.5 Fine-Tuning Setup\nThe ﬁnal language model (our proposed model) should be ﬁne-\ntuned towards di ﬀerent tasks: Sentiment Analysis, Text Clas-\nsiﬁcation, and Named Entity Recognition. Sentiment Analy-\nsis and Text Classiﬁcation belong to a broader task called Se-\nquence Classiﬁcation. Sentiment Analysis recognized as a spe-\nciﬁc task of Text Classiﬁcation in representing the emotions\nbehind the text.\n3.5.1 Sequence Classiﬁcation\nSequence classiﬁcation is the process of labeling texts in a su-\npervised manner. In our model, we incorporated the corre-\nsponding class for each sequence into the distinctive [CLS] to-\nken. We then added a simple feed-forward Softmax layer to\npredict the output classes. During this process, to maximize\nthe log-probability of the correct class, both classiﬁer and pre-\ntrained model weights are adjusted.\n3.6 Named Entity Recognition\nThis task aims to extract named entities in the text, such as\nnames and label with appropriate NER classes such as loca-\ntions, organizations, etc. The datasets used for this task contain\nsentences that are labeled with IOB format. In this format, to-\nkens that are not part of an entity are tagged as ”O”, the ”B” tag\ncorresponds to the ﬁrst word of an entity, and the ”I” tag cor-\nresponds to the rest of the words of the same entity. Both ”B”\nand ”I” tags are followed by a hyphen (or underscore), followed\nby the entity category. Therefore, the NER task is a multi-class\ntoken classiﬁcation problem that labels the tokens upon being\nfed a raw text.\n4 E valuation\nParsBERT is evaluated on three downstream tasks: Sentiment\nAnalysis (SA), Text Classiﬁcation, and Named Entity Recogni-\nPreprint – ParsBERT: Transformer-based Model forPersian Language Understanding 5\n(a) Notation Segmentation\n(b) POS Segmentation\nFigure 2: Example of segmenting a document into its sentences based on (a) only writing notations and (b) POS\nTable 3: Example of the segmentation process: (1) unsegmented sentence (2) segmented sentence using WordPiece method (\ninterpret as -).\n\tàA\u0010JQîD\u0011 éK . \u0010Qå\u0011 \tP@ , \tPQ\u001e.Ë@ øAë èñ» éK . H. ñ\tJk. \tP@ ,P \tQ \tk øAK\nPX éK . ÈAÖÞ\u0011 \tP@ é» øQîD \u0011 , YK\nðQK. QîD\u0011ñ\tK éK . YK\nAK. éÒ \u0011k\u0012 ñK\nX \tP@ YK\nX \tPAK. ø@QK.\n. Xñ \u0011ú× úæî \u0010D\tJÓ ñËAg\u0012 éK. H. Q \t« \tP@ ð Pñ \tK (1)\n\u0004 \tP@ \u0004 , \u0004 P \tQ \tk \u0004 øAK \nPX \u0004 éK . \u0004 ÈAÖ Þ\u0011 \u0004 \tP@ \u0004 é» \u0004 øQîD \u0011 \u0004 , \u0004 YK \nðQK. \u0004 QîD\u0011ñ\tK \u0004 éK . \u0004 YK\nAK. \u0004 éÒ \u0011k\u0012## \u0004 ñK \nX \u0004 \tP@ \u0004 YK \nX \tPAK. \u0004 ø@QK.\n. \u0004 Xñ \u0011ú× \u0004 úæî \u0010D\tJÓ \u0004 ñËAg \u0012 \u0004 éK . \u0004 H. Q \t« \u0004 \tP@ \u0004 ð \u0004 Pñ \tK \u0004 \tàA\u0010JQîD\u0011 \u0004 éK . \u0004 \u0010Qå\u0011 \u0004 \tP@ \u0004 , \u0004 \tPQ\u001e.Ë@ \u0004 øAë èñ» \u0004 éK . \u0004 H. ñ\tJk. (2)\nPreprint – ParsBERT: Transformer-based Model forPersian Language Understanding 6\ntion (NER). Each of these tasks requires their speciﬁc datasets\nfor the model to be ﬁne-tuned and evaluated on.\n4.1 Sentiment Analysis\nIt aims to classify text, such as comments based on their emo-\ntional bias. The proposed model is evaluated on three sentiment\ndatasets as follows:\n1. Digikala user comments provided by Open Data Min-\ning Program 9 (ODMP). This dataset contains 62,321\nuser comments with three labels: (0) No Idea, (1) Not\nRecommended and (2) Recommended.\n2. Snappfood 10 (an online food delivery company) user\ncomments containing 70,000 comments with two la-\nbels (i.e. polarity classiﬁcation): (0) Happy and (1)\nSad.\n3. DeepSentiPers [38], which is a balanced and aug-\nmented version of SentiPers [39], contains 12,138\nuser opinions about digital products labeled with ﬁve\ndiﬀerent classes; two positives (i.e., happy and de-\nlighted), two negatives (i.e., furious and angry) and\none neutral class. Therefore, this dataset can be uti-\nlized for both multi-class and binary classiﬁcation.\nIn the case of binary classiﬁcation, the neutral class\nand its corresponding sentences are removed from the\ndataset.\nThe second dataset of the above list was not readily available.\nWe extracted it using our tools to provide a more comprehen-\nsive evaluation. Figure 3 illustrates the class distribution for all\nthree sentiment datasets.\nBaselines: Since no work has been done regarding the Digikala\nand SnappFood datasets, our baseline for these datasets is the\nmultilingual BERT model. As for the DeepSentiPers [38]\ndataset, we compare our results with those reported in this pa-\nper. Their methodology for addressing the SA task entails a\nhybrid CNN and BiLSTM networks.\n4.2 Text Classiﬁcation\nText classiﬁcation is an important NLP task in which the objec-\ntive is to classify a text-based on pre-determined classes. The\nnumber of classes is usually higher than that of sentiment anal-\nysis and words distribution makes ﬁnding the right and main\nclass so tricky. The datasets used for this task come from two\nsources:\n1. A total of 8,515 articles scraped from Digikala on-\nline magazine 11. This dataset includes seven diﬀerent\nclasses.\n2. A dataset of various news articles scraped from dif-\nferent online news agencies’ websites. The total num-\nber of articles is 16,438, spread over eight di ﬀerent\nclasses.\n9https://www.digikala.com/opendata/\n10https://snappfood.ir/\n11https://www.digikala.com/mag/\nWe have scraped and prepared both of these datasets using our\nown tools. Figure 4 shows the class distribution for each of\nthese datasets.\nBaseline: Since we have prepared both datasets for this task\nusing our tool, no prior work has been done. Therefore, we\nonly have the monolingual BERT model to compare our model\nto for this task.\n4.3 Named Entity Recognition\nFor the NER task evaluation, PEYMA [46] and ARMAN [47]\nreadily available datasets are used. PEYMA dataset includes\n7,145 sentences with a total of 302,530 tokens from which\n41,148 tokens are tagged with seven di ﬀerent classes. On the\nother hand, the ARMAN dataset holds 7,682 sentences with\n250,015 sentences tagged over six di ﬀerent classes. The class\ndistribution for these datasets is shown in Figure 5.\nBaselines: We compare the result of our model for the NER\ntask to that of Beheshti-NER [35]. Beheshti-NER utilizes a\nmultilingual BERT model to tackle the same NER task as ours.\n5 R esults\n5.1 Sentiment Analysis Results\nTable 4 shows the results obtained on Digikala and SnaooFood\ndatasets. This table shows that ParsBERT outperforms the mul-\ntilingual BERT model in terms of accuracy and F1 score.\nTable 4: ParsBERT performance on Digikala and SnappFood\ndatasets compared to multilingual BERT model.\nModel Digikala SnappFood\nAcuuracy F 1 Acuuracy F 1\nParsBERT 82.52 81.74 87.80 88.12\nmultilingualBERT 81.83 80.74 87.44 87.87\nThe results for DeepSentiPers dataset are presented in table 5.\nIt can be seen that ParsBERT achieves signiﬁcantly higher F1\nscores for both multi-class and binary sentiment analysis com-\npared to methods mentioned in DeepSentiPers [38].\nTable 5: ParsBERT performance on DeepSentiPers dataset\ncompared to methods mentioned in DeepSentiPers [38]\nModel Multi-Class Binary\nF1 F1\nParsBERT 71.11 92.13\nCNN + FastText [38] 66.30 80.06\nCNN [38] 66.65 91.90\nBiLSTM + FastText [38] 69.33 90.59\nBiLSTM [38] 66.50 91.98\nSVM [38] 67.62 91.31\n5.2 Text Classiﬁcation Results\nThe obtained results for text classiﬁcation task are summarized\nin Table 6. It can be seen that ParsBERT achieves better accu-\nracy and scores compared to multilingual BERT model on both\nDigikala Magazine and Persian news datasets.\nPreprint – ParsBERT: Transformer-based Model forPersian Language Understanding 7\n(a)\n (b)\n(c)\n (d)\nFigure 3: Class distribution for (a) Multi-class DeepSentiPers, (b) Binary-class DeepSentiPers, (c) Digikala and (d) SnappFood\ndatasets.\n(a)\n (b)\nFigure 4: Class distribution for (a) Digikala Online Magazine and (b) Persian news articles scraped from various websites.\nPreprint – ParsBERT: Transformer-based Model forPersian Language Understanding 8\n(a)\n(b)\nFigure 5: Class distribution for (a) ARMAN and (b) PEYMA\ndatasets\nTable 6: ParsBERT performance on text classiﬁcation task\ncompared to multilingual BERT model.\nModel Digikala Magazine Persian News\nAcuuracy F 1 Acuuracy F 1\nParsBERT 94.28 93.59 97.20 97.19\nmultilingualBERT 91.31 90.72 95.80 95.79\n5.3 Named Entity Recognition Results\nObtained results for NER task indicates that ParsBERT outper-\nforms all prior works in this area by achieving F1 scores as\nhigh as 93.10 and 98.79 for PEYMA and ARMAN datasets,\nrespectively. A thorough comparison between ParsBERT per-\nformance and other works on these two datasets is provided in\ntable 7.\nTable 7: ParsBERT performance on PEYMA and ARMAN\ndatasets for the NER task compared to prior works.\nModel PEYMA ARMAN\nF1 F1\nParsBERT 93.10 98.79\nMorphoBERT [48] - 89.9\nBeheshti-NER [35] 90.59 84.03\nLSTM-CRF [49] - 86.55\nRule-Based-CRF [46] 84.00 -\nBiLSTM-CRF [47] - 77.45\nLSTM [49] - 73.61\nDeep CRF [34] - 81.50\nDeep Local [34] - 79.10\nSVM-HMM [50] - 72.59\n5.4 Discussion\nParsBERT successfully achieves state-of-the-art performance\non all mentioned downstream tasks. This conclusively proves\nthat monolingual language models outmatch multilingual ones.\nIn the case of ParsBERT, this improvement roots in several\ncauses. Firstly, the standardization and pre-processing em-\nployed in the current methodology overcomes the lack of cor-\nrect sentences in Persian corpora and takes into account the\ncomplexities of the Persian language. Secondly, the range of\ntopics and writing styles included in the pre-training dataset is\nmuch more diverse than that of multilingual BERT that only\napplies the Wikipedia dataset. Another limitation of the mul-\ntilingual model caused by using the small Wikipedia corpus is\nthat it contains a vocab size of 70K tokens for all 100 languages\nit supports. ParsBERT, on the other hand, incorporates a 14GB\ncorpus with more than 3.9M documents with a vocab size of\n100K. All in all, the obtained results indicate that ParsBERT\nis more competent at perceiving and understanding the Persian\nlanguage than multilingual BERT or any of the previous works\nthat have followed the same objective.\n6 C onclusion\nThere are few speciﬁc language models for the Persian lan-\nguage capable of providing state-of-the-art performance on dif-\nferent NLP tasks. ParsBERT is a fresh model that is lighter\nthan multilingual BERT and represents state-of-the-art results\nin downstream tasks, such as Sentiment Analysis, Text Classiﬁ-\ncation, and Named Entity Recognition. Compared to other Per-\nsian NER competitor models, ParsBERT outperforms all prior\nworks in terms of F1 score by achieving 93% and 98% scores\nfor PEYMA and ARMAN datasets, respectively. Moreover, in\nthe SA task, ParsBERT gained better performance on the Sen-\ntiPers dataset against the DeepSentiPers model by achieving\nPreprint – ParsBERT: Transformer-based Model forPersian Language Understanding 9\nF1 scores as high as 92% and 71% for both binary and multi-\nlabel scenarios. In all cases, ParsBERT outperforms multilin-\ngual BERT and other suggestion networks.\nThe number of datasets for downstream tasks in Persian is lim-\nited. Therefore, we composed a considerable set of datasets to\nevaluate ParsBERT performance on them. These datasets will\nsoon be published for public use12. Also, we happily announce\nthat ParsBERT synchronizes to Huggingface Transformers 13\nfor any public use and to serve as a new baseline for numerous\nPersian NLP use cases 14.\n7 A cknowledgments\nWe hereby, express our gratitude to the Tensorﬂow Research\nCloud (TFRC) program 15 for providing us with the necessary\ncomputation resources. We also thank Hooshvare 16 Research\nGroup for facilitating dataset gathering and scraping online text\nresources.\nReferences\n[1] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Je ﬀrey Dean. Distributed representations\nof words and phrases and their compositionality. ArXiv,\nabs/1310.4546, 2013.\n[2] Je ﬀrey Pennington, Richard Socher, and Christopher D.\nManning. Glove: Global vectors for word representation.\nIn EMNLP, 2014.\n[3] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. Deep contextualized word representations. ArXiv,\nabs/1802.05365, 2018.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. ArXiv,\nabs/1810.04805, 2019.\n[5] Alec Radford. Improving language understanding by gen-\nerative pre-training. In OpenAI, 2018.\n[6] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly opti-\nmized bert pretraining approach. ArXiv, abs/1907.11692,\n2019.\n[7] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. Xlnet:\nGeneralized autoregressive pretraining for language un-\nderstanding. In NeurIPS, 2019.\n[8] Colin Ra ﬀel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J. Liu. Exploring the limits of transfer\nlearning with a uniﬁed text-to-text transformer. ArXiv,\nabs/1910.10683, 2019.\n12https://hooshvare.github.io/\n13https://huggingface.co/\n14https://github.com/hooshvare/parsbert\n15https://tensorflow.org/tfrc\n16https://hooshvare.com\n[9] Alexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, F. Guzm ´an,\nEdouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin\nStoyanov. Unsupervised cross-lingual representation\nlearning at scale. ArXiv, abs/1911.02116, 2019.\n[10] Weihua Wang, Feilong Bao, and Guanglai Gao. Learn-\ning morpheme representation for mongolian named entity\nrecognition. Neural Processing Letters, pages 1–18, 2019.\n[11] Gengshi Huang and Haifeng Hu. c-rnn: A ﬁne-grained\nlanguage model for image captioning. Neural Processing\nLetters, 49:683–691, 2018.\n[12] Jinghao Niu, Yehui Yang, Siheng Zhang, Zhengya Sun,\nand Wensheng Zhang. Multi-task character-level atten-\ntional networks for medical concept normalization. Neu-\nral Processing Letters, 49:1239–1256, 2018.\n[13] Andrew M. Dai and Quoc V . Le. Semi-supervised se-\nquence learning. ArXiv, abs/1511.01432, 2015.\n[14] Prajit Ramachandran, Peter J. Liu, and Quoc V . Le. Un-\nsupervised pretraining for sequence to sequence learning.\nArXiv, abs/1611.02683, 2016.\n[15] Ilya Sutskever, Oriol Vinyals, and Quoc V . Le. Se-\nquence to sequence learning with neural networks. ArXiv,\nabs/1409.3215, 2014.\n[16] Jeremy Howard and Sebastian Ruder. Universal language\nmodel ﬁne-tuning for text classiﬁcation. In ACL, 2018.\n[17] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural Computation, 9:1735–1780, 1997.\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. ArXiv,\nabs/1706.03762, 2017.\n[19] Guillaume Lample and Alexis Conneau. Cross-lingual\nlanguage model pretraining. ArXiv, abs /1901.07291,\n2019.\n[20] Zhen-Zhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut. Al-\nbert: A lite bert for self-supervised learning of language\nrepresentations. ArXiv, abs/1909.11942, 2020.\n[21] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language un-\nderstanding. In BlackboxNLP@EMNLP, 2018.\n[22] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know\nwhat you don’t know: Unanswerable questions for squad.\nArXiv, abs/1806.03822, 2018.\n[23] Wietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and Malv-\nina Nissim. Bertje: A dutch bert model. ArXiv,\nabs/1912.09582, 2019.\n[24] Marco Polignano, Pierpaolo Basile, Marco Degemmis,\nGiovanni Semeraro, and Valerio Basile. Alberto: Ital-\nian bert language understanding model for nlp challeng-\ning tasks based on tweets. In CLiC-it, 2019.\n[25] Wissam Antoun, Fady Baly, and Hazem M. Hajj. Arabert:\nTransformer-based model for arabic language understand-\ning. ArXiv, abs/2003.00104, 2020.\nPreprint – ParsBERT: Transformer-based Model forPersian Language Understanding 10\n[26] Antti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. Multilingual is not enough: Bert for\nﬁnnish. ArXiv, abs/1912.07076, 2019.\n[27] Yuri Kuratov and Mikhail Arkhipov. Adaptation of deep\nbidirectional multilingual transformers for russian lan-\nguage. ArXiv, abs/1905.07213, 2019.\n[28] F ´abio Barbosa de Souza, Rodrigo Nogueira, and Roberto\nde Alencar Lotufo. Portuguese named entity recognition\nusing bert-crf. ArXiv, abs/1909.10649, 2019.\n[29] Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. Learning word vectors\nfor 157 languages. ArXiv, abs/1802.06893, 2018.\n[30] Mohammad Sadegh Zahedi, Mohammad Hadi Bokaei,\nFarzaneh Shoeleh, Mohammad Mehdi Yadollahi, Ehsan\nDoostmohammadi, and Mojgan Farhoodi. Persian word\nembedding evaluation benchmarks. Electrical Engineer-\ning (ICEE), Iranian Conference on , pages 1583–1588,\n2018.\n[31] Seyed Habib Hosseini Saravani, Mohammad Bahrani,\nHadi Veisi, and Sara Besharati. Persian language mod-\neling using recurrent neural networks. 2018 9th Inter-\nnational Symposium on Telecommunications (IST), pages\n207–210, 2018.\n[32] Farid Ahmadi and Hamed Moradi. A hybrid method for\npersian named entity recognition. 2015 7th Conference\non Information and Knowledge Technology (IKT) , pages\n1–7, 2015.\n[33] Kia Dashtipour, Mandar Gogate, Ahsan Adeel, Abdulrah-\nman Algaraﬁ, Newton Howard, and Amir Hussain. Per-\nsian named entity recognition. 2017 IEEE 16th Interna-\ntional Conference on Cognitive Informatics & Cognitive\nComputing (ICCI*CC), pages 79–83, 2017.\n[34] Mohammad Hadi Bokaei and Maryam Mahmoudi. Im-\nproved deep persian named entity recognition. 2018 9th\nInternational Symposium on Telecommunications (IST) ,\npages 381–386, 2018.\n[35] Ehsan Taher, Seyed Abbas Hoseini, and Mehrnoush\nShamsfard. Beheshti-ner: Persian named entity recogni-\ntion using bert. ArXiv, abs/2003.08875, 2020.\n[36] Mohammad Bagher Dastgheib, Sara Koleini, and Farzad\nRasti. The application of deep learning in persian docu-\nments sentiment analysis. International Journal of Infor-\nmation Science and Management, 18:1–15, 2020.\n[37] Kayvan Bijari, Hadi Zare, Emad Kebriaei, and Hadi Veisi.\nLeveraging deep graph-based text representation for sen-\ntiment polarity applications. Expert Syst. Appl. , 144:\n113090, 2020.\n[38] Javad PourMostafa Roshan Sharami, Parsa Abbasi\nSarabestani, and Seyed Abolghasem Mirroshandel.\nDeepsentipers: Novel deep learning models trained over\nproposed augmented persian sentiment corpus. ArXiv,\nabs/2004.05328, 2020.\n[39] Pedram Hosseini, Ali Ahmadian Ramaki, Hassan Maleki,\nMansoureh Anvari, and Seyed Abolghasem Mirroshandel.\nSentipers: A sentiment analysis corpus for persian.ArXiv,\nabs/1801.07737, 2018.\n[40] Dirk Goldhahn, Thomas Eckart, and Uwe Quastho ﬀ.\nBuilding large monolingual dictionaries at the leipzig cor-\npora collection: From 100 to 200 languages. In LREC,\n2012.\n[41] Pedro Javier Ortiz Su ´arez, Benoˆıt Sagot, and Laurent Ro-\nmary. Asynchronous pipeline for processing huge corpora\non medium to low resource infrastructures. In CMLC-7,\n2019.\n[42] Behnam Sabeti, Hossein Abedi Firouzjaee, Ali Janal-\nizadeh Choobbasti, S. H. E. Mortazavi Najafabadi, and\nAmir Vaheb. Mirastext: An automatically generated text\ncorpus for persian. In LREC, 2018.\n[43] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. CoRR, abs/1412.6980, 2015.\n[44] Taku Kudo. Subword regularization: Improving neural\nnetwork translation models with multiple subword candi-\ndates. In ACL, 2018.\n[45] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neu-\nral machine translation of rare words with subword units.\nArXiv, abs/1508.07909, 2016.\n[46] Mahsa Sadat Shahshahani, Mahdi Mohseni, Azadeh\nShakery, and Heshaam Faili. Peyma: A tagged corpus\nfor persian named entities. ArXiv, abs/1801.09936, 2018.\n[47] Hanieh Poostchi, Ehsan Zare Borzeshi, and Massimo Pic-\ncardi. Bilstm-crf for persian named-entity recognition\narmanpersonercorpus: the ﬁrst entity-annotated persian\ndataset. In LREC, 2018.\n[48] Nasrin Taghizadeh, Zeinab Borhani-fard, Melika Golesta-\nniPour, and Heshaam Faili. Nsurl-2019 task 7: Named\nentity recognition (ner) in farsi. ArXiv, abs/2003.09029,\n2020.\n[49] Leila Hafezi and Mehdi Rezaeian. Neural architecture for\npersian named entity recognition. 2018 4th Iranian Con-\nference on Signal Processing and Intelligent Systems (IC-\nSPIS), pages 61–64, 2018.\n[50] Hanieh Poostchi, Ehsan Zare Borzeshi, Mohammad Ab-\ndous, and Massimo Piccardi. Personer: Persian named-\nentity recognition. In COLING, 2016.",
  "topic": null,
  "concepts": []
}