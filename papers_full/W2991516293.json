{
    "title": "Dependency-Based Relative Positional Encoding for Transformer NMT",
    "url": "https://openalex.org/W2991516293",
    "year": 2019,
    "authors": [
        {
            "id": null,
            "name": "Ehime University, Japan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2568703091",
            "name": "Yutaro Omote",
            "affiliations": [
                "Ehime University"
            ]
        },
        {
            "id": "https://openalex.org/A2040756077",
            "name": "Akihiro Tamura",
            "affiliations": [
                "Ehime University"
            ]
        },
        {
            "id": "https://openalex.org/A2103085203",
            "name": "Takashi Ninomiya",
            "affiliations": [
                "Ehime University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963648186",
        "https://openalex.org/W2758137671",
        "https://openalex.org/W2963661253",
        "https://openalex.org/W2594047108",
        "https://openalex.org/W2963876447",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2798569372",
        "https://openalex.org/W2884083742",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2946028745",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2798833929",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2962788148",
        "https://openalex.org/W2116957398",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2576482813",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2127863960",
        "https://openalex.org/W2595715041"
    ],
    "abstract": "In this paper, we propose a novel model for Transformer neural machine translation that incorporates syntactic distances between two source words into the relative position representations of a selfattention mechanism.In particular, the proposed model encodes pair-wise relative depths on a source dependency tree, which are the differences between the depths of two source words, in the encoder's selfattention.Experiments show that our proposed model achieved a 0.5 point gain in BLEU on the Asian Scientific Paper Excerpt Corpus Japanese-to-English translation task.",
    "full_text": null
}