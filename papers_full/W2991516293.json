{
  "title": "Dependency-Based Relative Positional Encoding for Transformer NMT",
  "url": "https://openalex.org/W2991516293",
  "year": 2019,
  "authors": [
    {
      "id": null,
      "name": "Ehime University, Japan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2568703091",
      "name": "Yutaro Omote",
      "affiliations": [
        "Ehime University"
      ]
    },
    {
      "id": "https://openalex.org/A2040756077",
      "name": "Akihiro Tamura",
      "affiliations": [
        "Ehime University"
      ]
    },
    {
      "id": "https://openalex.org/A2103085203",
      "name": "Takashi Ninomiya",
      "affiliations": [
        "Ehime University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963648186",
    "https://openalex.org/W2758137671",
    "https://openalex.org/W2963661253",
    "https://openalex.org/W2594047108",
    "https://openalex.org/W2963876447",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2798569372",
    "https://openalex.org/W2884083742",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2946028745",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2798833929",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2116957398",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2576482813",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2127863960",
    "https://openalex.org/W2595715041"
  ],
  "abstract": "In this paper, we propose a novel model for Transformer neural machine translation that incorporates syntactic distances between two source words into the relative position representations of a selfattention mechanism.In particular, the proposed model encodes pair-wise relative depths on a source dependency tree, which are the differences between the depths of two source words, in the encoder's selfattention.Experiments show that our proposed model achieved a 0.5 point gain in BLEU on the Asian Scientific Paper Excerpt Corpus Japanese-to-English translation task.",
  "full_text": null,
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8094944953918457
    },
    {
      "name": "Computer science",
      "score": 0.7707945108413696
    },
    {
      "name": "Encoder",
      "score": 0.716560959815979
    },
    {
      "name": "Machine translation",
      "score": 0.700347900390625
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6374119520187378
    },
    {
      "name": "Natural language processing",
      "score": 0.5745192766189575
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5427728891372681
    },
    {
      "name": "Dependency (UML)",
      "score": 0.5326525568962097
    },
    {
      "name": "Rule-based machine translation",
      "score": 0.41230854392051697
    },
    {
      "name": "Voltage",
      "score": 0.09749865531921387
    },
    {
      "name": "Engineering",
      "score": 0.07271558046340942
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I43545212",
      "name": "Ehime University",
      "country": "JP"
    }
  ],
  "cited_by": 11
}