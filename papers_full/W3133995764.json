{
    "title": "Neural Language Models Capture Some, But Not All, Agreement Attraction Effects",
    "url": "https://openalex.org/W3133995764",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A817205692",
            "name": "Tal Linzen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2956761303",
            "name": "Suhas Arehalli",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2028994331",
        "https://openalex.org/W2922523190",
        "https://openalex.org/W2963614302",
        "https://openalex.org/W2054125330",
        "https://openalex.org/W2100520417",
        "https://openalex.org/W2099639915",
        "https://openalex.org/W2122236285",
        "https://openalex.org/W2055457833",
        "https://openalex.org/W2056680116",
        "https://openalex.org/W2963073938",
        "https://openalex.org/W2886663262",
        "https://openalex.org/W2963751529",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2147571305",
        "https://openalex.org/W2952208026",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2889015880",
        "https://openalex.org/W2158711980"
    ],
    "abstract": "The number of the subject in English must match the number of the corresponding verb (dog runs but dogs run). Yet in real-time language production and comprehension, speakers often mistakenly compute agreement between the verb and a grammatically irrelevant non-subject noun phrase instead. This phenomenon, referred to as agreement attraction, is modulated by a wide range of factors; any complete computational model of grammatical planning and comprehension would be expected to derive this rich empirical picture. Recent developments in Natural Language Processing have shown that neural networks trained only on word-prediction over large corpora are capable of capturing subject-verb agreement dependencies to a significant extent, but with occasional errors. The goal of this paper is to evaluate the potential of such neural word prediction models as a foundation for a cognitive model of real-time grammatical processing. We simulate six experiments taken from the agreement attraction literature with LSTMs, one common type of neural language model. The LSTMs captured the critical human behavior in three of them, indicating that (1) some agreement attraction phenomena can be captured by a generic sequence processing model, but (2) capturing the other phenomena may require models with more language-specific mechanisms",
    "full_text": null
}