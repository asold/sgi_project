{
  "title": "Examining the Inductive Bias of Neural Language Models with Artificial Languages",
  "url": "https://openalex.org/W3166522443",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5017298392",
      "name": "Jennifer C. White",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A5061951606",
      "name": "Ryan Cotterell",
      "affiliations": [
        "ETH Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2027087446",
    "https://openalex.org/W2963737810",
    "https://openalex.org/W2964243640",
    "https://openalex.org/W3164594392",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W381804301",
    "https://openalex.org/W1605758875",
    "https://openalex.org/W2803214681",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963447120",
    "https://openalex.org/W2963351454",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2613045005",
    "https://openalex.org/W2292711447",
    "https://openalex.org/W1484082930",
    "https://openalex.org/W2120438065",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2953092638"
  ],
  "abstract": "Since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages. Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup. Languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders. We propose a novel method for investigating the inductive biases of language models using artificial languages. These languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated, such as word order. We then use them to train and test language models. This constitutes a fully controlled causal framework, and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models. Using this method, we find that commonly used neural architectures exhibit different inductive biases: LSTMs display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others. Further, we find that neither the inductive bias of the LSTM nor that of the transformer appears to reflect any tendencies that we see in attested natural languages.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 454–463\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n454\nExamining the Inductive Bias of Neural Language Models with Artiﬁcial\nLanguages\nJennifer C. White\n Ryan Cotterell\n ,\nUniversity of Cambridge,\n ETH Z¨urich\njw2088@cam.ac.uk, ryan.cotterell@inf.ethz.ch\nAbstract\nSince language models are used to model a\nwide variety of languages, it is natural to ask\nwhether the neural architectures used for the\ntask have inductive biases towards modeling\nparticular types of languages. Investigation\nof these biases has proved complicated due\nto the many variables that appear in the ex-\nperimental setup. Languages vary in many\ntypological dimensions, and it is difﬁcult to\nsingle out one or two to investigate without\nthe others acting as confounders. We propose\na novel method for investigating the induc-\ntive biases of language models using artiﬁcial\nlanguages. These languages are constructed\nto allow us to create parallel corpora across\nlanguages that differ only in the typological\nfeature being investigated, such as word or-\nder. We then use them to train and test lan-\nguage models. This constitutes a fully con-\ntrolled causal framework, and demonstrates\nhow grammar engineering can serve as a use-\nful tool for analyzing neural models. Using\nthis method, we ﬁnd that commonly used neu-\nral architectures exhibit different inductive bi-\nases: LSTMs display little preference with re-\nspect to word ordering, while transformers dis-\nplay a clear preference for some orderings over\nothers. Further, we ﬁnd that neither the induc-\ntive bias of the LSTM nor that of the trans-\nformer appears to reﬂect any tendencies that\nwe see in attested natural languages.\n1 Introduction\nModern neural architectures used for language\nmodeling, e.g. Transformer-based language models\n(Vaswani et al., 2017) and language models based\non long-short term memory (LSTM) (Hochreiter\nand Schmidhuber, 1997; Sundermeyer et al., 2012),\nare intrinsically black boxes. This makes it difﬁ-\ncult to understand whether their structure leads to\nan inductive bias which results in certain types of\nlanguage being easier to learn and model. To make\nthis point more plainly, we cannot easily conclude\n22.5 25.0 27.5 30.0 32.5 35.0 37.5 40.0\nAverage Perplexity\n0.0\n0.2\n0.4\n0.6\n0.8Density\nTransformer\nLSTM\nFigure 1: Distribution of average perplexities achieved\nby transformer- and LSTM-based language models on\nour artiﬁcial languages with varying word order.\nmuch about whether an LSTM language model\nwill perform better on SVO or SOV languages by\nsimply examining its structure. Moreover, satis-\nfactorily investigating the inductive bias of neural\nmodels has the potential to yield useful insight into\nhow they work. In this work, we explore whether\nneural language models exhibit biases towards cer-\ntain types of languages in a novel causal framework\nthrough the use of artiﬁcial languages.\nOne of the key problems involved in investigat-\ning the effect of typological features on language\nmodel performance is the difﬁculty in isolating\nonly the features being investigated, without inﬂu-\nence from other features of the languages being\ninvestigated or the data being used. For example, if\none were to compare language model performance\non English, an SVO language, and Japanese, an\nSOV language, it would be difﬁcult to directly at-\ntribute differences in performance to the difference\nin word ordering alone. This is because English\nand Japanese also differ in many other typological\ndimensions, such as how subjects are marked, the\nextent of subject–verb agreement and use of post-\npositions or prepositions, which could contribute\nto the difference in performance. Indeed, recent\ncorrelational studies have failed to ﬁnd an effect\n455\nbetween language model performance and typolog-\nical features (Cotterell et al., 2018; Mielke et al.,\n2019). Moreover, the sentences used for training\nand testing may differ in content, style or infor-\nmation density, which could further contribute to\ndifferences in performance.\nThus, we offer a study investigating the inductive\nbiases of language models through the construc-\ntion of artiﬁcial languages. Our approach involves\ncreating small context-free grammars resembling\nsubsets of attested languages, which we then use to\ntrain and evaluate language models. In an approach\ninspired by Chomsky’s (1981) framework of prin-\nciples and parameters, we imbue our grammars\nwith “switches” that indicate how to permute the\nordering of the non-terminals in a given production.\nThrough generating grammars with all possible\ncombinations of these switches, we can create ar-\ntiﬁcial languages of differing typological proﬁles.\nThis experimental paradigm allows us to conduct\ncarefully controlled studies by varying only the\ntypological parameter and make a causal claim.\nUsing our method, we investigate inductive\nbiases related to the head-directionality of several\nconstructions. We ﬁnd that LSTM-based architec-\ntures show little bias towards any particular order-\ning, achieving similar average perplexities on all\ngrammar variations tested. This contradicts recent\nﬁndings by Ravfogel et al. (2019) who ﬁnd LSTMs\nhave a preference for SVO word order. Conversely,\nwe ﬁnd that performance of transformer-based\narchitectures varies signiﬁcantly across our\nartiﬁcial languages; this is visualized in Figure 1.\nThis indicates that some combinations of the\nswitches result in languages with word orders that\nare harder for the transformer to model than others.\nOur analysis suggests that neither the performance\nof the transformer-based architectures nor of the\nLSTM-based architectures reﬂects any known ten-\ndencies in attested natural languages, with the best\nperformance being achieved on languages with the\nrarely-attested OVS sentence ordering. Importantly,\nour method exposes that transformer-based lan-\nguage models and LSTM-based language models\nhave vastly different inductive biases, a result that\nhas not been clearly stated in the NLP literature.\n2 Why Artiﬁcial Languages?\n2.1 Previous Work\nArtiﬁcial languages have previously been used to\ninvestigate the ability of neural architectures with\nrespect to speciﬁc phenomenon, such as their abil-\nity to acquire hierarchical generalizations (McCoy\net al., 2018) and whether they can use systematic\ncomposition skills to make generalizations (Lake\nand Baroni, 2018). Bowman et al. (2015) also used\nartiﬁcial languages to investigate the ability of\nLSTMs to learn compositional structure, and com-\npare their ability to that of tree-structured models.\nThe work most closely related to ours is that\nof Ravfogel et al. (2019). Taking methodologi-\ncal inspiration from Wang and Eisner (2016), they\ncreate artiﬁcial versions of English with modiﬁed\nword order and case systems, including a version\nwith object–verb agreement. They use the task\nof predicting the number of the subject and ob-\nject of a missing verb to examine language model\nperformance across these variations. They ﬁnd\nthat the models perform better on this task for the\nlanguage with SVO word order. What they leave\nunchanged in their experiment, however, is the\noriginal English ordering within the constituents,\ne.g. the adjective–noun ordering in a noun phrase.\nHowever, constituent order correlates with order-\ning of other grammatical constituents typologically\n(Greenberg, 1963), and this could lead to unwar-\nranted preferences for the original English ordering.\nOur work addresses this problem by using fully arti-\nﬁcial languages rather than modifying English sen-\ntences. This allows for our experiment to be more\ncontrolled by eliminating possible confounders.\nOther work conducted on the topic of inductive\nbiases of language models has tended to focus on\ncorrelational studies investigating the relationship\nbetween typological features extracted from the\nWorld Atlas of Language Structures (W ALS; Dryer\nand Haspelmath, 2013), which have only found\nnegative results (Cotterell et al., 2018; Mielke et al.,\n2019). Since this work looked exclusively at the\nfeatures of attested natural languages, it is difﬁ-\ncult to control for the multiple typological dimen-\nsions along which any two natural languages differ.\nFurther, given the large number of typological fea-\ntures exhibited among the world’s languages, there\nare simply not enough attested languages to make\nstrong correlational claims. Mielke et al. (2019)\nultimately concluded with a negative result; this\nnegative result, in part, motivates our study.\n2.2 The Necessity of Artiﬁcial Languages\nWe suggest that properly investigating the in-\nductive biases of language models will likely\n456\nrequire artiﬁcial languages. Choosing languages\nto investigate the inductive bias of a language\nmodel requires a trade-off between the experiment\nbeing realistic and being controlled. Using attested\nnatural languages gives us the most realistic\nrepresentation of natural language and all its\ncomplexities, but this also reduces the level of\ncontrol and makes it difﬁcult to disentangle the\nvarious typological variables that differ between\nlanguages. Indeed, this was the conclusion of\nMielke et al. (2019). Work such as Ravfogel et al.\n(2019) ﬁnds some mid-point by using artiﬁcial\nlanguages which have been modiﬁed from English.\nThis means that the language is less natural and\nmore controlled, but does not maximize either.\nIn our experiments, we have chosen to maximize\nthe level of control. This means that our grammars\nare simple and do not necessarily cover all possible\nconstructions that one would expect to see in a natu-\nral language. However, our reward for this sacriﬁce\nis that we can precisely control and understand how\ntwo languages tested differ from one another. We\nargue that this provides a good base for the explo-\nration of inductive bias, as when differences are\nobserved under these conditions we may now make\na causal claim about their origin. In future work,\nthe base grammars could be changed and extended\nas much as necessary to test additional hypotheses.\n3 Constructing Controlled Languages\n3.1 A Fully Controlled Experiment\nA context-free grammar (CFG) is a quadruple (N,\nS, Σ, R) where Nis a set of non-terminals, S ∈\nNis a distinguished start non-terminal, Σ is an\nalphabet and Ris a set of production rules. An\nelement r ∈ Rtakes the form N → αwhere\nα∈(N∪ Σ)∗. A CFG deﬁnes a subset of Σ∗.\nProbabilistic context-free grammars (PCFG) are\na probabilistic generalization of CFGs. Rather than\nsimply deﬁning a subset of Σ∗, a PCFG gives us a\nprobability distribution of Σ∗ where the structure\nof the grammar gives us the structural zeros of the\ndistribution. Given a PCFG, we can take samples\nfrom it in order to generate sentences.\nWe set out to construct a set of PCFGs to ex-\npose the inductive bias of neural language mod-\nels. These grammars are parametrized by several\n“switches”, which determine the ordering of con-\nstituents within the grammar. The “switches” used\nare described in more detail in §3.3.\nWe write an initial base PCFG in which\nS\nNPSubj\nNP\nVP\nNPObj\nNP\nPronoun\nme\nObj\nob\nVerb\npovify\nRel\nrel\nNP\nfusbenders\nSubj\nsub\nVP\nSComp\nS\nNPSubj\nNP\nserds\nSubj\nsub\nVerb\npovicateda\nComp\nsa\nVerb\nstrovokicizeda\n(a) Grammar 000000: me ob povify rel fusbenders sub serds\nsub povicateda sa strovokicizeda .\nS\nNPSubj\nNP\nNP\nfusbenders\nRel\nrel\nVP\nVerb\npovify\nNPObj\nNP\nPronoun\nme\nObj\nob\nSubj\nsub\nVP\nVerb\nstrovokicizeda\nSComp\nComp\nsa\nS\nNPSubj\nNP\nserds\nSubj\nsub\nVerb\npovicateda\n(b) Grammar 011101: fusbenders rel povify me ob sub\nstrovokicizeda sa serds sub povicateda .\nS\nVP\nVerb\nstrovokicizeda\nSComp\nComp\nsa\nS\nVerb\npovicateda\nNPSubj\nNP\nserds\nSubj\nsub\nNPSubj\nNP\nNP\nfusbenders\nRel\nrel\nVP\nVerb\npovify\nNPObj\nNP\nPronoun\nme\nObj\nob\nSubj\nsub\n(c) Grammar 111111: strovokicizeda sa povicateda serds\nsub fusbenders rel povify me ob sub .\nFigure 2: Trees showing the structure of parallel sen-\ntences across 3 of our artiﬁcial languages\nproductions are written to correspond with the\nordering obtained when all switches are “off”. 1\nIn this base PCFG, the rules which are affected\nby the toggling of each switch are marked. From\nthis, sentences are sampled. On generation, each\nproduction in these sentences is marked with the\nswitch it is associated with. We then work through\nevery combination of switches, replicating this\nsame set of generated sentences and reversing\nproductions as required by the switches, to produce\n1The choice of which permutation is “on” or “off” is ar-\nbitrary. In this case, “off” switches correspond to head-ﬁnal\norderings.\n457\nmultiple parallel corpora, identical in their content\nup to a reordering of constituents.\nThis experimental set-up allows us to ensure that\nsentences in the corpus for each of our artiﬁcial\nlanguages differ only in the conﬁguration of the\nswitches. In this way we can be conﬁdent in at-\ntributing any differences in performance to a causal\ndifference in these switches rather than any differ-\nences caused by confounders, e.g. content, style or\ncomplexity of the sentences.\n3.2 Our Context-Free Grammar\nNow we describe the construction of the PCFG\nwith which we experiment in this work. Ex-\nample sentences from several of our generated\nlanguages are shown in Figure 2. The base\ngrammar and the scripts for sampling from it\nand generating corpora for all switch conﬁgura-\ntions will be released at https://github.com/\nrycolab/artificial-languages.\nThe Alphabet Σ. Open-class words were taken\nfrom a list of phonotactically plausible English\npseudowords (Kharkwal, 2014). These pseu-\ndowords included verbs, nouns and adjectives. We\ninﬂected the nouns manually for English plural-\nity (adding s or es) depending on what English\nphonotactics requires. We conjugated the verbs for\npresent and past tense, again, using the rules of\nEnglish. Additional morphological markers that\nare not present in English, e.g. subject and ob-\nject markers and an additional marker to denote\na plural past tense verb form, were obtained by\nrandomly sampling two-letters slices from the list\nof morphological plausible words.2 Pronouns and\nprepositions were also obtained in this fashion.\nThe Non-Terminals N. Our grammar has a sin-\ngle distinguished start symbol S. It describes verb\nphrases (VP), containing transitive and intransi-\ntive verbs, as well as verbs that take a sentential\ncomplement (complementizers are denotedComp).\nNouns are marked as being objects or subjects us-\ning a particle (denoted Obj or Subj). Verbs in our\ngrammar have two tenses (past and present). Noun\nphrases (NP), including those modiﬁed by adjec-\ntives (Adj), relative clauses (where relativizers are\ndenoted Rel) and prepositional phrases (PP), are\ndescribed in our grammar.\n2This sampling occurred only once, and markers used were\nthe same for all words.\nRule for each switch value\nSwitch 0 1\nS S → NP VP S → VP NP\nVP VP → NP VP VP → VP NP\nComp SComp → S Comp S Comp → Comp S\nPP NP → PP NP\nPP → NP Prep\nNP → NP PP\nPP → Prep NP\nNP NP → Adj NP NP → NP Adj\nRel NP → VP Rel Noun NP → Noun Rel VP\nTable 1: Rules that are switchable in our grammar. Sub-\nscripts for tense and number agreement are not shown\nfor simplicity.\nThe Production Rules R. Our production rules\nRcover several common productions seen in natu-\nral language. We list the production rules which are\nsubject to switching in our experiment in Table 1.\nModeling Morphological Agreement. Our\ngrammar models a simple form of morphological\nagreement: verbs agree with their subjects in\nnumber (singular or plural). This introduces\nan element of long-term dependencies into our\nlanguages – if a language model is to correctly\npredict a verb form, it must carry information\nabout the number of the subject. In order to enforce\nthis agreement in our grammar, non-terminals are\nsubscripted with their number (where applicable).\nAssigning Probabilities. Weights given to each\nproduction were chosen manually through experi-\nmentation. Some principles for choosing weights\nfor a grammar in this manner are described by Eis-\nner and Smith (2008). An automated method of\nassigning weights could be explored in future work.\n3.3 Controlled Typological Variation\nOur end goal is to construct a grammar parameter-\nized by a binary vector of Kswitches. We denote\nsuch a vector of switches b ∈{0,1}K. Toggling\nan individual switch in the grammar reverses the\norder of the right-hand sides of a set of produc-\ntion rules. For example, the switch that we term\nthe S switch reverses the order of the production\nS →NP VPto create S →VP NP.3 2K different\ngrammars are possible from Kbinary switches. In\nthe following paragraphs, we describe each of the\nswitches we consider in this work.\nPosition of subject in sentence (S Switch). This\nswitch determines the order in which a subject and\nits verb phrase appear within a sentence. If the\n3Details of all switches are shown in Table 1.\n458\nJapanese English Spanish\nSwitch Value Example Value Example Value Example\nS 0 猫が食べる。 0 The cat eats. 0 El gato come.\nVP 0 猫がネズミを食べる。 1 The cat eats the mouse. 1 El gato come el rat ´on.\nComp 0 猫が食べると思う。 1 I think that the cat eats. 1 Pienso que el gato come.\nPP 0 テーブルの上の猫が食べる。 1 The cat on the table eats. 1 El gato sobre la mesa come.\nNP 0 小さな猫が食べる。 0 The small cat eats. 1 El gato peque ˜no come.\nRel 0 ミルクを飲む猫が食べる。 1 The cat that drinks milk eats. 1 El gato que bebe leche come.\nTable 2: Demonstration of the orders of the switch constituents in Japanese, English and Spanish\nswitch has a value of 0, the rule S →NP VP is\nused, which is the order used in the vast major-\nity of the world’s languages, including SVO lan-\nguages such as English and SOV languages such\nas Japanese. If the switch has a value of 1, the rule\nbecomes S →VP NP. This order is rare among\nattested natural languages, but can be seen in VOS\nlanguages such as Malagasy and OVS languages\nsuch as Hixkaryana.\nPosition of verb in verb phrase ( VP Switch).\nThis switch determines whether a direct object pre-\ncedes or follows its verb. If the switch has a value\nof 0, we use the head-ﬁnal order, with the object\npreceding the verb. This is seen in languages such\nas Japanese and Turkish. If the switch has a value\nof 1, the head-initial order is used, with the object\nfollowing the verb. This is seen in languages such\nas English and Chinese. This switch, in combi-\nnation with the S switch, determines the overall\nordering of subject, object and verb within a sen-\ntence. If the values of these switches are (0,0), the\nlanguage will have SOV word order, like Japanese\nand Turkish. If they are (1,1), the language will\nhave VOS order, which is rare but can be seen in\nlanguages such as Malagasy. SVO languages such\nas English correspond to (0,1). (1,0) corresponds\nto OVS order, which is attested in only a very small\nnumber of human languages.\nPosition of complementizer in sentential com-\nplement ( Comp switch). This switch deter-\nmines whether a complementizer begins or ends a\nsentential complement. If the switch has a value\nof 0, the complementizer appears in head-ﬁnal po-\nsition, at the end of the complement. This is the\norder seen in Japanese. If the switch has a value\nof 1, the complementizer appears in head-initial\nposition, at the beginning of the complement. This\nis the order seen in English.\nOrdering of prepositional phrase ( PP Switch).\nThis switch determines the ordering of a preposi-\ntional phrase. If the switch has a value of 0, the\nprepositional phrase precedes the noun it modiﬁes,\nand the prepositional phrase ends with a prepo-\nsition, in head-ﬁnal order. This order is seen in\nJapanese. If the switch has a value of 1, the prepo-\nsitional phrase follows the noun it modiﬁes, and\nthe preposition begins the prepositional phrase, in\nhead-initial order. This order is seen in English.\nPosition of adjective in noun phrase ( NP\nSwitch). This switch determines whether an ad-\njective appears before or after the noun it modiﬁes.\nIf the switch is 0, the adjective precedes the noun\n(as in English and Japanese) and if it is 1, the ad-\njective follows the noun (as in Spanish and Irish).\nPosition of relative clause ( Rel switch). This\nswitch determines the position of a relative clause\nwith respect to the noun it modiﬁes. If the switch\nhas a value of 0, a relative clause is followed by a\nrelativizer and then the noun it modiﬁes. This order\nis seen in Japanese. If the switch has a value of 1,\nthe noun being modiﬁed appears ﬁrst, followed by\na relativizer and the clause. This order is seen in\nFrench and English.\nThe unmarked word order of some attested\nlanguages can be approximately identiﬁed with\nparticular switch vectors. 4 For example, stan-\ndard English order corresponds approximately to\n(0,1,1,1,0,1), Japanese to (0,0,0,0,0,0) and\nSpanish to (0,1,1,1,1,1).5 This is demonstrated\nin Table 2. We note that our conﬁgurations cannot\naccount for all possible word orders seen in attested\nlanguages (VSO languages are not represented, for\nexample), but constitute a subset of possible orders.\n4This is, of course, a simpliﬁcation, since word order\nwithin a natural language can follow more complex rules,\nor allow for ﬂexibility.\n5From this point on, grammars will be referred to by\ntheir conﬁguration of switches, sans brackets, e.g. Grammar\n011101.\n459\n000000\n000001\n000010\n000011\n000100\n000101\n000110\n000111\n001000\n001001\n001010\n001011\n001100\n001101\n001110\n001111\n010000\n010001\n010010\n010011\n010100\n010101\n010110\n010111\n011000\n011001\n011010\n011011\n011100\n011101\n011110\n011111\n100000\n100001\n100010\n100011\n100100\n100101\n100110\n100111\n101000\n101001\n101010\n101011\n101100\n101101\n101110\n101111\n110000\n110001\n110010\n110011\n110100\n110101\n110110\n110111\n111000\n111001\n111010\n111011\n111100\n111101\n111110\n111111\nOrdering\n0\n10\n20\n30\n40Average Perplexity\nTransformer\nLSTM\nFigure 3: All scores achieved by LSTM- and transformer-based models\n4 Experiments\nArchitectures and Data. In order to compare\ninductive biases across architectures, two neural ar-\nchitectures were tested: transformers and LSTMs.\nWe used the implementation available as part of\nFairseq (Ott et al., 2019). Our base grammar has\nK = 6 switches, i.e. 6 binary choice points as\ndescribed in §3.3. This results in 26 = 64 pos-\nsible grammars. For each of these grammars we\ngenerated 100,000 sentences, which were divided\ninto 10 splits of 10,000.6 The sentences generated\nfor each grammar differed only in the designated\nchoice points, i.e. in the ordering of their con-\nstituents. This meant that each sentence appeared\nin an equivalent form in each grammar. As such,\nfor each sentence, we can compare the perplex-\nity of the 64 variants of the sentence as calculated\nby language models trained on the corresponding\ngrammars. Each split of 10,000 sentences was di-\nvided into an 80–10–10 train–dev–test split.7\nProcedure. We trained both a transformer-based\nand an LSTM-based language model on each train\nsplit and the models were evaluated on the test split.\nThis procedure resulted in 10 language models per\narchitecture for each possible grammar, each of\nwhich was evaluated on 1,000 sentences in their re-\nspective test set. The perplexity achieved on these\ntest sets was averaged across the 10 splits, to give\nthe average perplexity for that grammar. This ap-\nproach helps to account for the variability between\nindividual training runs.\n610,000 sentences may sound like a relatively small num-\nber, but we note that our artiﬁcial languages are simple with\nsmall vocabularies, so we consider this number to be sufﬁcient.\n7Equivalent sentences across grammars were assured to be\nin the equivalent splits for each grammar, so train, dev and\ntest sets across grammars contained the same sentences up to\nreordering of constituents.\n5 Results and Analysis\n5.1 Perplexity Evaluation\nThe average perplexity on the test set was mea-\nsured for each grammar. This measures how well\na language model explains the held-out test set.\nThe lower the perplexity the better the language\nmodel ﬁts the held-out data. Average perplexity\nachieved across all grammars by the transformer-\nand LSTM-based models are shown in Figure 3.8\n5.2 Mixed-Effects Modeling\nWe use a linear mixed-effects model to investigate\nthe effects of each choice point in the grammar.\nThis allows us to model the effect of each switch in\nthe grammar, and ﬁrst-order interaction terms be-\ntween them, on the perplexity of a sentence, while\ncontrolling for the fact that perplexities for parallel\nsentences across grammars are related (by using\na random intersect per sentence grouping). This\nmodel is explained in detail below.\nAssume we have N paired sentences from each\nof our 2K grammars. Let L∈RN×2K\n≥0 be a non-\nnegative real matrix of the perplexity obtained for\nevery test sentence across every grammar. Specif-\nically, we have that Lnk is the perplexity for the\nnth sentence under the kth grammar. Furthermore,\nlet S ∈{0,1}2K×\n(K(K−1)\n2 +K\n)\nbe the binary ma-\ntrix containing the conﬁguration of switches and\nthe K(K−1)\n2 + K switch–switch interactions for\neach of the 2K grammars in contrast coding (Wu,\n2009). Thus, we have that the column vector Sk•\nis a binary vector of length K(K−1)\n2 + K. Let\nβ∈R\nK(K−1)\n2 +K be a vector of real coefﬁcients to\nbe estimated describing the effect of each switch\nand their interactions. Let un ∼N (0,σ2\ndif.) be\n8Error bars are omitted, but across grammars the error on\neach measurement is generally between 0.25 and 0.5.\n460\nS VP Comp PP NP Rel\nSVPCompPPNPRel\n-0.253\n p=0.000\n-0.352\n p=0.000\n-0.438\n p=0.000\n-0.060\n p=0.000\n-0.084\n p=0.000\n-0.042\n p=0.000\n-0.945\n p=0.000\n0.989\n p=0.000\n-0.076\n p=0.000\n-0.095\n p=0.000\n0.552\n p=0.000\n-0.317\n p=0.000\n-0.068\n p=0.000\n-0.061\n p=0.000\n-0.048\n p=0.000\n-0.068\n p=0.000\n-0.074\n p=0.000\n-0.080\n p=0.000\n-0.051\n p=0.000\n0.330\n p=0.000\n-0.601\n p=0.000\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n(a)\nS VP Comp PP NP Rel\nSVPCompPPNPRel\n-0.115\n p=0.000\n-0.138\n p=0.000\n-0.064\n p=0.000\n-0.004\n p=0.074\n0.006\n p=0.008\n-0.009\n p=0.000\n-0.113\n p=0.000\n0.140\n p=0.000\n-0.006\n p=0.011\n0.008\n p=0.001\n0.071\n p=0.000\n0.006\n p=0.010\n0.003\n p=0.167\n0.011\n p=0.000\n0.039\n p=0.000\n0.000\n p=0.921\n0.002\n p=0.361\n0.000\n p=0.973\n-0.030\n p=0.000\n0.027\n p=0.000\n-0.129\n p=0.000\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6 (b)\nFigure 4: Heat maps showing the coefﬁcients obtained for a mixed-effects model for perplexity as predicted by (a)\ntransformers and (b) LSTMs.\na sentence-speciﬁc difﬁculty term (a random ef-\nfect) and let ε∼N(0,σ2) be a sentence–grammar-\nspeciﬁc noise term. Now, we model an individual\nperplexity Lnk, which corresponds to the nth sen-\ntence and the kth grammar, as follows:\nLnk = Sk• ·β+ un + ε (1)\nImportantly, we draw one un for each unique sen-\ntence. It is in this sense that un acts as a term for\nmodeling sentence difﬁculty. We may write eq. (1)\nas the following\nLnk ∼N(Sk• ·β,σ2\ndif. + σ2) (2)\nwhich reveals that it is no more than a simple Gaus-\nsian model with tied parameters. We estimate β,\nσ2\ndif. and σ2 through maximum-likelihood estima-\ntion, which, in Gaussian models, is equivalent to\nleast-squares estimation.\nA positive coefﬁcient βj for a given switch\nmeans that models perform worse with head-initial\nordering for that switch, while a negative coefﬁ-\ncient means the opposite. Since the ﬁxed effects\nwere input using contrast coding, the interaction\nterms in our model deal with the effects of two\nconstituents sharing head-directionality. A posi-\ntive coefﬁcient for an interaction means that the\nmodels perform worse when they share head direc-\ntionality, and a negative coefﬁcient means the op-\nposite. Head-directionality is commonly correlated\nbetween sentence constituents in attested natural\nlanguages, so if the biases of these architectures re-\nﬂected human languages, we would expect most in-\nteraction terms to be negative. The coefﬁcients ob-\ntained for the transformers are shown in Figure 4a.\nThose for the LSTMs are shown in Figure 4b.\n6 Discussion\nDifferences Between Architectures. It is clear\nfrom Figure 3 that the transformer- and LSTM-\nbased models do not show the same inductive bi-\nases with respect to the switches we investigated.\nAcross all possible conﬁgurations of the switches,\nLSTMs achieve very similar average perplexities,\nsuggesting that they have little preference for any\nparticular set of constituent orderings. In contrast,\nthe average perplexities achieved by the transform-\ners vary considerably between grammars. This\ndemonstrates clearly that the two models exhibit\ndistinctly different preferences with regard to or-\nderings of words within in a sentence. Further,\nthe clear contrast between the coefﬁcients obtained\nby the mixed-effects models for transformers and\nLSTMs (shown in Figure 4a and Figure 4b, re-\nspectively) demonstrates a stark contrast between\nthe two models. None of the switches investi-\ngated, or their ﬁrst-order interactions, appear to\nhave a substantial effect on the scores obtained\nin the case of the LSTM-based models, whereas\nthe transformer-based models are clearly affected\nto a much greater degree by the conﬁguration of\nthese switches. Given that these two architectures\nare both commonly used for similar tasks, such a\ndifference in their inductive biases is noteworthy.\nCorrelated Switches. Figure 4a shows the coef-\nﬁcients obtained by the mixed-effects model em-\nployed to investigate the effects of the switches\n461\nSOV SVO OVS VOS\nWord Order\n0\n10\n20\n30\n40\n50Percentage of world languages\n 0\n5\n10\n15\n20\n25\n30\nAverage Perplexity\nTransformer\nLSTM\nFigure 5: The prevalence of word orders across lan-\nguages (Dryer, 2013), plotted with the average perplex-\nities achieved on each of these groups of grammars by\ntransformer- and LSTM-based models\non performance for the transformer-based mod-\nels. The diagonal values (for single switches) are\nall negative coefﬁcients, which indicates that the\nmodel performance is better when these have head-\nﬁnal ordering. Off-diagonal values are the coefﬁ-\ncients obtained for the interaction terms between\ntwo switches. A positive value here indicates that\nwhen these two switches have the same value (ei-\nther both head-initial or both head-ﬁnal), the per-\nformance of the model is worse. A negative value\nmeans that when the two switches have the same\nvalue, the performance is better. Most of the off-\ndiagonal elements have small values, with a few\nexceptions. The coefﬁcients of the cross terms be-\ntween the S and VP switches and the S and Comp\nswitches are larger negative values, which indi-\ncates that when these constituents share their head-\ndirectionality the performance of the transformer-\nbased models is better. The coefﬁcients of the cross\nterms between the VP and Comp, VP and Rel and\nNP and Rel switches are larger postive values, in-\ndicating that the transformers perform worse when\nthese constituents share head-directionality. Gen-\nerally, attested natural languages tend to exhibit a\ntendency towards one head-directionality, but the\ntransformer does not seem to have inductive biases\nthat reﬂect this. The corresponding coefﬁcients for\nthe LSTM-based models, shown in Figure 4b, are\nall small, further demonstrating that the LSTMs are\nlargely agnostic to word ordering.\nTendencies in Attested Natural Languages.\nWe wish to consider the question of whether the\nbiases of these models are in any way reﬂective of\nword order tendencies that we see across attested\nnatural languages. All word orders are not equally\ncommon among natural languages, and it is inter-\nesting to consider whether the word orders that\nthese models are able to model more successfully\nare those which are more commonly seen in natural\nlanguage. Some have speculated that the skew of\nword orders in human languages could possibly be\nreﬂective of human cognitive biases (Culbertson\net al., 2012, 2019), so it would be interesting to see\nto what extent the inductive biases of these models\nreﬂects this skew. Since LSTMs appear to show\nno preference for any word order over the others,\nthey are clearly not reﬂective of attested tendencies\nin word order. To attempt to answer this question\nfor the transformers, we begin by comparing the\nperformance of the models on subsets of grammars\nwith the prevalence of similar languages among\nhumans. In Figure 5, the grammars are grouped\nby how they order the verb, object and subject of\na sentence, and the average perplexities achieved\nby the language models on each of these groups\nis shown. On the same ﬁgure, we display the es-\ntimated prevalence of these orderings among the\nworld’s languages (Dryer, 2013). It is clear that\nthese two things are not correlated, with the trans-\nformer performing similarly on SOV languages,\nthe most common among the world’s languages,\nand OVS languages, which are rarely attested. This\nshows that the bias exhibited by transformers does\nnot reﬂect tendencies among attested languages. A\nfurther indication of this is the lack of a strong pref-\nerence for switches sharing head-directionality as\nshown in Figure 4a. In human languages, the head-\nedness of constituents is often correlated (Green-\nberg, 1963). We would expect to see this through\nnegative coefﬁcients for interaction terms in the\nmixed-effects model for constituents whose orders\ncommonly correlate. However, we do not observe\nthis for all correlations. For example, we would\nexpect the PP switch to show a strong preference\nfor shared head-directionality with other switches,\nwhich we do not observe.\n7 Conclusion\nWe propose a novel methodology for the investiga-\ntion of the inductive bias of language models using\nthe technique of creating carefully controlled artiﬁ-\ncial languages. This approach allows for the elimi-\nnation of differences in corpora between languages\nand means that typological variation between lan-\nguages can be restricted exclusively to the typologi-\ncal features being investigated. We use this method-\nology to investigate the inductive bias of two neu-\n462\nral architectures which are commonly used for this\ntask: LSTMs and transformers. We found that\nthese two models have starkly different inductive\nbiases with respect to word order, with the LSTM\nshowing little variation in performance across word\norder, while the performance of the transformer var-\nied signiﬁcantly across artiﬁcial languages.\nAcknowledgements\nWe thank Simone Teufel for providing feedback on\nan early draft.\nEthical Considerations\nThe authors foresee no ethical concerns with the\nresearch presented in this paper.\nReferences\nSamuel R. Bowman, Christopher D. Manning, and\nChristopher Potts. 2015. Tree-structured composi-\ntion in neural networks without tree-structured ar-\nchitectures. In Proceedings of the 2015 Interna-\ntional Conference on Cognitive Computation: Inte-\ngrating Neural and Symbolic Approaches, volume\n1583, page 37–42.\nNoam Chomsky. 1981. Lectures on Government and\nBinding: The Pisa Lectures. Walter de Gruyter.\nRyan Cotterell, Sabrina J. Mielke, Jason Eisner, and\nBrian Roark. 2018. Are all languages equally hard\nto language-model? In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 536–541, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nJennifer Culbertson, Marieke Schouwstra, and Simon\nKirby. 2019. From the world to word order: the\nlink between conceptual structure and language.\nPsyArXiv.\nJennifer Culbertson, Paul Smolensky, and G ´eraldine\nLegendre. 2012. Learning biases predict a word or-\nder universal. Cognition, 122(3):306–329.\nMatthew S. Dryer. 2013. Order of subject, object and\nverb. In Matthew S. Dryer and Martin Haspelmath,\neditors, The World Atlas of Language Structures On-\nline. Max Planck Institute for Evolutionary Anthro-\npology, Leipzig.\nMatthew S. Dryer and Martin Haspelmath, editors.\n2013. WALS Online. Max Planck Institute for Evo-\nlutionary Anthropology, Leipzig.\nJason Eisner and Noah A. Smith. 2008. Competitive\ngrammar writing. In Proceedings of the Third Work-\nshop on Issues in Teaching Computational Linguis-\ntics, pages 97–105, Columbus, Ohio. Association for\nComputational Linguistics.\nJoseph H. Greenberg. 1963. Some universals of gram-\nmar with particular reference to the order of mean-\ningful elements. In Joseph H. Greenberg, editor,\nUniversals of Human Language, pages 73–113. MIT\nPress, Cambridge, Mass.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong Short-Term Memory. Neural Computation,\n9(8):1735–1780.\nGaurav Kharkwal. 2014. Taming the Jabberwocky:\nExamining Sentence Processing with Novel Words.\nPh.D. thesis, Rutgers University-Graduate School-\nNew Brunswick.\nBrenden Lake and Marco Baroni. 2018. Generalization\nwithout systematicity: On the compositional skills\nof sequence-to-sequence recurrent networks. In Pro-\nceedings of the 35th International Conference on\nMachine Learning, volume 80 ofProceedings of Ma-\nchine Learning Research, pages 2873–2882, Stock-\nholmsm¨assan, Stockholm Sweden. PMLR.\nR. Thomas McCoy, Robert Frank, and Tal Linzen.\n2018. Revisiting the poverty of the stimulus: Hi-\nerarchical generalization without a hierarchical bias\nin recurrent neural networks. In Proceedings of the\n40th Annual Conference of the Cognitive Science So-\nciety, pages 2093–2098.\nSabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian\nRoark, and Jason Eisner. 2019. What kind of lan-\nguage is hard to language-model? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4975–4989, Florence,\nItaly. Association for Computational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nShauli Ravfogel, Yoav Goldberg, and Tal Linzen. 2019.\nStudying the inductive biases of RNNs with syn-\nthetic variations of natural languages. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 3532–3542, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. LSTM neural networks for language model-\ning. In Thirteenth Annual Conference of the Inter-\nnational Speech Communication Association, pages\n194–197.\n463\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, page 6000–6010.\nDingquan Wang and Jason Eisner. 2016. The galactic\ndependencies treebanks: Getting more data by syn-\nthesizing new languages. Transactions of the Asso-\nciation for Computational Linguistics, 4:491–505.\nLang Wu. 2009. Mixed Effects Models for Complex\nData. CRC Press.",
  "topic": "Inductive bias",
  "concepts": [
    {
      "name": "Inductive bias",
      "score": 0.8648769855499268
    },
    {
      "name": "Computer science",
      "score": 0.7903695106506348
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6393970847129822
    },
    {
      "name": "Natural language processing",
      "score": 0.6033620834350586
    },
    {
      "name": "Artificial neural network",
      "score": 0.5215049386024475
    },
    {
      "name": "Transformer",
      "score": 0.5172975063323975
    },
    {
      "name": "Natural language",
      "score": 0.4864601790904999
    },
    {
      "name": "Grammar",
      "score": 0.4525271952152252
    },
    {
      "name": "Language model",
      "score": 0.44308027625083923
    },
    {
      "name": "Task (project management)",
      "score": 0.4332542419433594
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4319218397140503
    },
    {
      "name": "Multi-task learning",
      "score": 0.37106674909591675
    },
    {
      "name": "Machine learning",
      "score": 0.3217756748199463
    },
    {
      "name": "Linguistics",
      "score": 0.18842706084251404
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}