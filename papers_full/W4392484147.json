{
    "title": "Large language model augmented exercise retrieval for personalized language learning",
    "url": "https://openalex.org/W4392484147",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2655257423",
            "name": "Austin Xu",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2118063804",
            "name": "Will Monroe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117820577",
            "name": "Klinton Bicknell",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2953577593",
        "https://openalex.org/W4284669679",
        "https://openalex.org/W4224313754",
        "https://openalex.org/W4385570422",
        "https://openalex.org/W4385565351",
        "https://openalex.org/W2946601762",
        "https://openalex.org/W2138621090",
        "https://openalex.org/W4283651011",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W3171885108",
        "https://openalex.org/W4252076394",
        "https://openalex.org/W4385573057",
        "https://openalex.org/W3094091106",
        "https://openalex.org/W3128513560",
        "https://openalex.org/W4200635123",
        "https://openalex.org/W4385574070",
        "https://openalex.org/W3092100739",
        "https://openalex.org/W4385573358",
        "https://openalex.org/W2957747000",
        "https://openalex.org/W4205918858",
        "https://openalex.org/W3026389132"
    ],
    "abstract": "We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO [2]. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner's input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learner input content, and (3) low semantic similarity between input and retrieval candidates. mHyER outperforms several strong baselines on two novel benchmarks created from crowdsourced data and publicly available data.",
    "full_text": null
}