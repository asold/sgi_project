{
  "title": "PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer",
  "url": "https://openalex.org/W3217437548",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5062522283",
      "name": "Zitong Yu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5052917688",
      "name": "Yuming Shen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5064150384",
      "name": "Jingang Shi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5078109015",
      "name": "Hengshuang Zhao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5042899882",
      "name": "Philip H. S. Torr",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5082301986",
      "name": "Guoying Zhao",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2066454034",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3175452902",
    "https://openalex.org/W3176258108",
    "https://openalex.org/W3021508980",
    "https://openalex.org/W1986273245",
    "https://openalex.org/W3164690902",
    "https://openalex.org/W3209712295",
    "https://openalex.org/W2893517024",
    "https://openalex.org/W2964962925",
    "https://openalex.org/W3204093119",
    "https://openalex.org/W2903559293",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W2786529723",
    "https://openalex.org/W3173279311",
    "https://openalex.org/W3081152303",
    "https://openalex.org/W2787182113",
    "https://openalex.org/W3205363408",
    "https://openalex.org/W3129515546",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2805424946",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W2902449706",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2218803975",
    "https://openalex.org/W2891834928",
    "https://openalex.org/W2520509592",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2003922338",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3212973774",
    "https://openalex.org/W3108080438",
    "https://openalex.org/W3201844719",
    "https://openalex.org/W3204118251",
    "https://openalex.org/W2069692225",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W3212756788",
    "https://openalex.org/W2472200183",
    "https://openalex.org/W3171649327",
    "https://openalex.org/W3173536475",
    "https://openalex.org/W2122098299",
    "https://openalex.org/W3100063341",
    "https://openalex.org/W3101998545",
    "https://openalex.org/W2008821584",
    "https://openalex.org/W3173459793",
    "https://openalex.org/W3202141913",
    "https://openalex.org/W3170898657",
    "https://openalex.org/W3103690655",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W1984554603",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3110179947",
    "https://openalex.org/W2982196965",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W3150177490",
    "https://openalex.org/W2903521046",
    "https://openalex.org/W3169612303",
    "https://openalex.org/W3205588468",
    "https://openalex.org/W3213057589",
    "https://openalex.org/W2807904173",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2958501326",
    "https://openalex.org/W2553156677",
    "https://openalex.org/W3156109214"
  ],
  "abstract": "Remote photoplethysmography (rPPG), which aims at measuring heart activities and physiological signals from facial video without any contact, has great potential in many applications (e.g., remote healthcare and affective computing). Recent deep learning approaches focus on mining subtle rPPG clues using convolutional neural networks with limited spatio-temporal receptive fields, which neglect the long-range spatio-temporal perception and interaction for rPPG modeling. In this paper, we propose the PhysFormer, an end-to-end video transformer based architecture, to adaptively aggregate both local and global spatio-temporal features for rPPG representation enhancement. As key modules in PhysFormer, the temporal difference transformers first enhance the quasi-periodic rPPG features with temporal difference guided global attention, and then refine the local spatio-temporal representation against interference. Furthermore, we also propose the label distribution learning and a curriculum learning inspired dynamic constraint in frequency domain, which provide elaborate supervisions for PhysFormer and alleviate overfitting. Comprehensive experiments are performed on four benchmark datasets to show our superior performance on both intra- and cross-dataset testings. One highlight is that, unlike most transformer networks needed pretraining from large-scale datasets, the proposed PhysFormer can be easily trained from scratch on rPPG datasets, which makes it promising as a novel transformer baseline for the rPPG community. The codes will be released at https://github.com/ZitongYu/PhysFormer.",
  "full_text": "PhysFormer: Facial Video-based Physiological Measurement with\nTemporal Difference Transformer\nZitong Yu1, Yuming Shen2, Jingang Shi3, Hengshuang Zhao4,2, Philip Torr2, Guoying Zhao1*\n1CMVS, University of Oulu 2TVG, University of Oxford\n3Xi’an Jiaotong University 4The University of Hong Kong\nAbstract\nRemote photoplethysmography (rPPG), which aims at\nmeasuring heart activities and physiological signals from\nfacial video without any contact, has great potential in\nmany applications. Recent deep learning approaches fo-\ncus on mining subtle rPPG clues using convolutional neu-\nral networks with limited spatio-temporal receptive fields,\nwhich neglect the long-range spatio-temporal perception\nand interaction for rPPG modeling. In this paper, we\npropose the PhysFormer, an end-to-end video transformer\nbased architecture, to adaptively aggregate both local and\nglobal spatio-temporal features for rPPG representation\nenhancement. As key modules in PhysFormer, the tempo-\nral difference transformers first enhance the quasi-periodic\nrPPG features with temporal difference guided global at-\ntention, and then refine the local spatio-temporal represen-\ntation against interference. Furthermore, we also propose\nthe label distribution learning and a curriculum learning\ninspired dynamic constraint in frequency domain, which\nprovide elaborate supervisions for PhysFormer and al-\nleviate overfitting. Comprehensive experiments are per-\nformed on four benchmark datasets to show our supe-\nrior performance on both intra- and cross-dataset testings.\nOne highlight is that, unlike most transformer networks\nneeded pretraining from large-scale datasets, the proposed\nPhysFormer can be easily trained from scratch on rPPG\ndatasets, which makes it promising as a novel transformer\nbaseline for the rPPG community. The codes are available\nat https://github.com/ZitongYu/PhysFormer.\n1. Introduction\nPhysiological signals such as heart rate (HR), respiration\nfrequency (RF), and heart rate variability (HRV) are impor-\ntant vital signs to be measured in many circumstances, es-\npecially for healthcare or medical purposes. Traditionally,\nthe Electrocardiography (ECG) and Photoplethysmograph\n(PPG) are the two most common ways for measuring heart\n*Corresponding author\nrPPG signals\nt1 t2 t3\ndirection\nFigure 1. The trajectories of rPPG signals around t1, t2, and t3\nshare similar properties (e.g., trends with rising edge first then\nfalling edge later, and relatively high magnitudes) induced by skin\ncolor changes. It inspires the long-range spatio-temporal attention\n(e.g., blue tube around t1 interacted with red tubes from intra- and\ninter-frames) according to their local temporal difference features\nfor quasi-periodic rPPG enhancement. Here ‘tube’ indicates the\nsame regions across short-time consecutive frames.\nactivities and corresponding physiological signals. How-\never, both ECG and PPG sensors need to be attached to\nbody parts, which may cause discomfort and are inconve-\nnient for long-term monitoring. To counter for this issue,\nremote photoplethysmography (rPPG) [12, 36, 66] methods\nare developing fast in recent years, which aim to measure\nheart activity remotely without any contact.\nIn earlier studies of facial rPPG measurement, most\nmethods analyze subtle color changes on facial regions\nof interest (ROI) with classical signal processing ap-\nproaches [30, 49, 50, 55, 57]. Besides, there are a few\ncolor subspace transformation methods [13, 59] which uti-\nlize all skin pixels for rPPG measurement. Based on the\nprior knowledge from traditional methods, a few learning\nbased approaches [25, 44, 45, 51] are designed as non-end-\nto-end fashions. ROI based preprocessed signal represen-\ntations (e.g., time-frequency map [25] and spatio-temporal\nmap [44, 45]) are generated first, and then learnable mod-\nels could capture rPPG features from these maps. However,\nthese methods need the strict preprocessing procedure and\nneglect the global contextual clues outside the pre-defined\nROIs. Meanwhile, more and more end-to-end deep learn-\ning based rPPG methods [11, 34, 53, 65, 67] are developed,\nwhich treat facial video frames as input and predict rPPG\nand other physiological signals directly. However, pure end-\nto-end methods are easily influenced by the complex sce-\nnarios (e.g., with head movement and various illumination\nconditions) and rPPG-unrelated features can not be ruled\nout in learning, resulting in large performance decrease [63]\nin realistic datasets (e.g., VIPL-HR [45]).\nRecently, due to its excellent long-range attentional mod-\neling capacities in solving sequence-to-sequence issues,\ntransformer [22, 32] has been successfully applied in many\nartificial intelligence tasks such as natural language pro-\ncessing (NLP) [56], image [15] and video [3] analysis. Sim-\nilarly, rPPG measurement from facial videos can be treated\nas a video sequence to signal sequence problem, where the\nlong-range contextual clues should be exploited for seman-\ntic modeling. As shown in Fig. 1, rPPG clues from different\nskin regions and temporal locations (e.g., signal trajectories\naround t1, t2, and t3) share similar properties (e.g., trends\nwith rising edge first then falling edge later and relative\nhigh magnitudes), which can be utilized for long-range fea-\nture modeling and enhancement. However, different from\nthe most video tasks aiming at huge motion representation,\nfacial rPPG measurement focuses on capturing subtle skin\ncolor changes, which makes it challenging for global spatio-\ntemporal perception. Furthermore, video-based rPPG mea-\nsurement is usually a long-time monitoring task, and it is\nchallenging to design and train transformers with long video\nsequence inputs.\nMotivated by the discussions above, we propose an end-\nto-end video transformer architecture, namely PhysFormer,\nfor remote physiological measurement. On one hand, the\ncascaded temporal difference transfomer blocks in Phys-\nFormer benefit the rPPG feature enhancement via global\nspatio-temporal attention based on the fine-grained tempo-\nral skin color differences. On the other hand, to alleviate\nthe interference-induced overfitting issue and complement\nthe weak temporal supervision signals, elaborate supervi-\nsion in frequency domain is designed, which helps Phys-\nFormer learn more intrinsic rPPG-aware features.\nThe contributions of this work are as follows:\n• We propose the PhysFormer, which mainly consists\nof a powerful video temporal difference transformer\nbackbone. To our best knowledge, it is the first time\nto explore the long-range spatio-temporal relationship\nfor reliable rPPG measurement.\n• We propose an elaborate recipe to supervise Phys-\nFormer with label distribution learning and curriculum\nlearning guided dynamic loss in frequency domain to\nlearn efficiently and alleviate overfitting.\n• We conduct intra- and cross-dataset testings and show\nthat the proposed PhysFormer achieves superior or on\npar state-of-the-art performance without pretraining on\nlarge-scale datasets like ImageNet-21K.\n2. Related Work\nRemote physiological measurement. An early study\nof rPPG-based physiological measurement was reported\nin [57]. Plenty of traditional hand-crafted approaches have\nbeen developed on this field since then. Selective merg-\ning information from different color channels [30,49,50] or\ndifferent ROIs [27, 30] are proven to be efficient for sub-\ntle rPPG signal recovery. To improve the signal-to-noise-\nratio of the recovered rPPG signals, several signal decom-\nposition methods such as independent component analy-\nsis (ICA) [27, 49, 50] and matrix completion [55] are also\nproposed. In recent years, deep learning based approaches\ndominate the field of rPPG measurement due to the strong\nspatio-temporal representation capabilities. On one hand,\nfacial ROI based spatial-temporal signal maps [40, 41, 44,\n46, 47] are developed, which alleviate the interference from\nnon-skin regions. Based on these signal maps, 2D-CNNs\nare utilized for rPPG feature extraction. On the other hand,\nend-to-end spatial networks [11, 53] and spatio-temporal\nmodels [20,34,35,48,63,65,67] are developed, which could\nrecover rPPG signals from the facial video directly. How-\never, previous methods only consider the spatio-temporal\nrPPG features from adjacent frames and neglect the long-\nrange relationship among quasi-periodic rPPG features.\nTransformer for vision tasks. Transformer [32] is pro-\nposed in [56] to model sequential data in the field of\nNLP. Then vision transformer (ViT) [15] is proposed re-\ncently by feeding transformer with sequences of image\npatches for image classification. Many other ViT variants\n[8, 14, 22, 23, 26, 38, 54, 60, 70] are proposed from then,\nwhich achieve promising performance compared with its\ncounterpart CNNs for image analysis tasks [6, 24, 74]. Re-\ncently, some works introduce vision transformer for video\nunderstanding tasks such as action recognition [1, 3, 4, 16,\n21, 39, 42], action detection [37, 58, 62, 73], video super-\nresolution [5], video inpainting [33, 71], and 3D anima-\ntion [9, 10]. Some works [21, 42] conduct temporal contex-\ntual modeling with transformer based on single-frame fea-\ntures from pretrained 2D networks, while other works [1,\n3, 4, 16, 39] mine the spatio-temporal attentions via video\ntransformer directly. Most of these works are incompati-\nble for long-video-sequence ( >150 frames) signal regres-\nsion task. There are two related works [35, 64] using ViT\nfor rPPG feature representation. TransRPPG [64] extracts\nrPPG features from the preprocessed signal maps via ViT\nfor face 3D mask presentation attack detection [68]. Based\non the temporal shift networks [31, 34], EfficientPhys-\nT [35] adds several swin transformer [38] layers for global\nspatial attention. Different from these two works, the\nproposed PhysFormer is an end-to-end video transformer,\nwhich is able to capture long-range spatio-temporal atten-\ntional rPPG features from facial video directly.\nStem\nTube Tokens\nTemporal Difference\nMulti-head Self-attention\nAdd  \n&  \nNorm\nSpatio-temporal  \nFeed-forward\nAdd  \n&  \nNorm\nTemporal Difference Transformer \nPredictor\nVideo x N\nrPPG Signals\nT T' T \nTDCTDC1x1x1\nVid2Seq\nVid2Seq\nVid2Seq\nSoftmax\nTemporal Difference Multi-head Self-attention\nMulti-head\nConcat \n& \nProjection\nSeq2Vid\n1x1x1\nDepth-wise \n3x3x3\nSpatio-temporal \nFeed-forward\nQ\nK\nV 1x1x1\nFigure 2. Framework of the PhysFormer. It consists of a shallow stem, a tube tokenizer, several temporal difference transformers, and a\nrPPG predictor head. The temporal difference transformer is formed from the Temporal Difference Multi-head Self-attention (TD-MHSA)\nand Spatio-temporal Feed-forward (ST-FF) modules, which enhances the global and local spatio-temporal representation, respectively.\n‘TDC’ is short for the temporal difference convolution [63, 69].\n3. Methodology\nWe will first introduce the architecture of PhysFormer in\nSec. 3.1, then introduce label distribution learning for rPPG\nmeasurement in Sec. 3.2, and at last present the curriculum\nlearning guided dynamic supervision in Sec. 3.3.\n3.1. PhysFormer\nAs illustrated in Fig. 2, PhysFormer consists of a shal-\nlow stem Estem, a tube tokenizer Etube, N temporal differ-\nence transformer blocks Ei\ntrans (i = 1, ..., N) and a rPPG\npredictor head. Inspired by the study in [61], we adopt\na shallow stem to extract coarse local spatio-temporal fea-\ntures, which benefits the fast convergence and clearer subse-\nquent global self-attention. Specifically, the stem is formed\nby three convolutional blocks with kernel size (1x5x5),\n(3x3x3) and (3x3x3), respectively. Each convolution op-\nerator is cascaded with a batch normalization (BN), ReLU\nand MaxPool. The pooling layer only halves the spatial\ndimension. Therefore, given an RGB facial video input\nX ∈ R3×T×H×W , the stem output Xstem = Estem(X),\nwhere Xstem ∈ RD×T×H/8×W/8, and D, T, W, H indicate\nchannel, sequence length, width, height, respectively. Then\nXstem would be partitioned into spatio-temporal tube tokens\nXtube ∈ RD×T′×H′×W′\nvia the tube tokenizerEtube. Subse-\nquently, the tube tokens will be forwarded withN temporal\ndifference transformer blocks and obtain the global-local re-\nfined rPPG features Xtrans, which has the same dimensions\nwith Xtube. Finally, the rPPG predictor head temporally up-\nsamples, spatially averages, and projects the features Xtrans\nto 1D signal Y ∈ RT .\nTube tokenization. Here the coarse feature Xstem would\nbe partitioned into non-overlapping tube tokens via\nEtube(Xstem), which aggregates the spatio-temporal neigh-\nbor semantics and reduces computational costs for the sub-\nsequent transformers. Specifically, with the targeted tube\nsize Ts × Hs × Ws (the same as the partition step size\nin non-overlapping setting), the tube token map Xtube ∈\nRD×T′×H′×W′\nhas length, height and width\nT′ =\n\u0016 T\nTs\n\u0017\n, H′ =\n\u0016H/8\nHs\n\u0017\n, W′ =\n\u0016W/8\nWs\n\u0017\n. (1)\nPlease note that there are no position embeddings after the\ntube tokenization as the stem at early stage already captures\nrelative spatio-temporal positions.\nTemporal difference multi-head self-attention. In self-\nattention mechanism [15, 56], the relationship between the\ntokens is modeled by the similarity between the projected\nquery-key pairs, yielding the attention score. Instead of\npoint-wise linear projection, we utilize temporal difference\nconvolution (TDC) [63, 69] for query (Q) and key (K) pro-\njection, which could capture fine-grained local temporal dif-\nference features for subtle color change description. TDC\nwith learnable w can be formulated as\nTDC(x) =\nX\npn∈R\nw(pn) · x(p0 + pn)\n| {z }\nvanilla 3D convolution\n+θ · (−x(p0) ·\nX\npn∈R′\nw(pn))\n| {z }\ntemporal difference term\n,\n(2)\nwhere p0, R and R′ indicate the current spatio-temporal\nlocation, sampled local (3x3x3) neighborhood and sampled\nadjacent neighborhood, respectively. Then query and key\nare projected as\nQ = BN(TDC(Xtube)), K= BN(TDC(Xtube)). (3)\nFor the value ( V ) projection, point-wise linear projection\nwithout BN is utilized. Then Q, K, V∈ RD×T′×H′×W′\nare flattened into sequence, and separated into h heads\n(Dh = D/h for each head). For the i-th head ( i ≤ h),\nthe self-attention (SA) can be formulated\nSAi = Softmax(QiKT\ni /τ)Vi, (4)\nwhere τ controls the sparsity. We find that the default set-\nting τ = √Dh in [15, 56] performs poorly for rPPG mea-\nsurement. According to the periodicity of rPPG features,\nwe use smaller τ value to obtain sparser attention activa-\ntion. The corresponding study can be found in Table 6. The\noutput of TD-MHSA is the concatenation of SA from all\nheads and then with a linear projection U ∈ RD×D\nTD-MHSA = Concat(SA1; SA2; ...; SAh)U. (5)\nAs illustrated in Fig. 2, residual connection and layer nor-\nmalization (LN) would be conducted after TD-MHSA.\nSpatio-temporal feed-forward. The vanilla feed-forward\nnetwork consists of two linear transformation layers, where\nthe hidden dimension D′ between two layers is expanded to\nlearn a richer feature representation. In contrast, we intro-\nduce a depthwise 3D convolution (with BN and nonlinear\nactivation) between these two layers with extra slight com-\nputational cost but remarkable performance improvement.\nThe benefits are two-fold: 1) as a complementation of TD-\nMHSA, ST-FF could refine the local inconsistency and parts\nof noisy features; 2) richer locality provides TD-MHSA suf-\nficient relative position cues.\n3.2. Label Distribution Learning\nSimilar to the facial age estimation task [18, 19] that\nfaces at close ages look quite similar, facial rPPG signals\nwith close HR values usually have similar periodicity. In-\nspired by this observation, instead of considering each facial\nvideo as an instance with one label (HR), we regard each\nfacial video as an instance associated with a label distribu-\ntion. The label distribution covers a certain number of class\nlabels, representing the degree that each label describes the\ninstance. Through this way, one facial video can contribute\nto both targeted HR value and its adjacent HRs.\nTo consider the similarity information among HR classes\nduring the training stage, we model the rPPG-based HR es-\ntimation problem as a specific L-class multi-label classifi-\ncation problem, where L=139 in our case (each integer HR\nvalue within [42, 180] bpm as a class). A label distribu-\ntion p = {p1, p2, ..., pL} ∈RL is assigned to each facial\nvideo X. It is assumed that each entry of p is a real value\nin the range [0,1] such that PL\nk=1 pk = 1. We consider the\nGaussian distribution function, centred at the ground truth\nHR label YHR with the standard deviationσ, to construct the\ncorresponding label distribution p.\npk = 1√\n2πσ exp\n\u0012\n−(k − (YHR − 41))2\n2σ2\n\u0013\n. (6)\nThe label distribution loss can be formulated as LLD =\nKL(p, Softmax(ˆ p)), where divergence measure KL(·) de-\nnotes the Kullback-Leibler (KL) divergence [17], and ˆ pis\nthe power spectral density (PSD) of predicted rPPG signals.\nPlease note that the previous work [43] also considers\nthe distribution learning for HR estimation. However, it is\ntotally different with our work: 1) the motivation in [43] is\nto smooth the temporal HR outliers caused by facial move-\nments across continuous video clips, while our work is more\ngeneric, aiming at efficient feature learning across adjacent\nlabels under limited-scale training data; 2) the technique\nused in [43] is after a post-HR-estimation for the hand-\ncrafted rPPG signals, while our work is to design a reason-\nable supervision signal LLD for PhysFormer.\n3.3. Curriculum Learning Guided Dynamic Loss\nCurriculum learning [2], as a major machine learning\nregime with philosophy of easy-to-hard curriculum, is uti-\nlized to train PhysFormer. In the rPPG measurement task,\nthe supervision signals from temporal domain (e.g., mean\nsquare error loss [11], negative Pearson loss [65, 67]) and\nfrequency domain (e.g., cross-entropy loss [46, 63], signal-\nto-noise ratio loss [53]) provide different extents of con-\nstraints for model learning. The former one gives signal-\ntrend-level constraints, which is straightforward and easy\nfor model convergence but overfitting after that. In con-\ntrast, the latter one with strong constraints on frequency do-\nmain enforces the model learning periodic features within\ntarget frequency bands, which is hard to converged well\ndue to the realistic rPPG-irrelevant noise. Inspired by the\ncurriculum learning, we propose the dynamic supervision\nto gradually enlarge the frequency constraints, which alle-\nviates the overfitting issue and benefits the intrinsic rPPG-\naware feature learning gradually. Specifically, exponential\nincrement strategy is adopted, and comparison with other\ndynamic strategies (e.g., linear increment) will be shown in\nTable 7. The dynamic loss Loverall can be formulated as\nLoverall = α · Ltime| {z }\ntemporal\n+ β · (LCE + LLD)| {z }\nfrequency\n,\nβ = β0 · (η(Epochcurrent−1)/Epochtotal ),\n(7)\nwhere hyperparameters α, β0 and η equal to 0.1, 1.0 and\n5.0, respectively. Negative Pearson loss [65, 67] and fre-\nquency cross-entropy loss [46, 63] are adopted as Ltime and\nLCE, respectively. With the dynamic supervision, Phys-\nFormer could perceive better signal trend at the beginning\nwhile such perfect warming up facilitates the gradually\nstronger frequency knowledge learning later.\n4. Experimental Evaluation\nExperiments of rPPG-based physiological measurement\nfor three types of physiological signals, i.e., heart rate (HR),\nheart rate variability (HRV), and respiration frequency (RF),\nare conducted on four benchmark datasets (VIPL-HR [45],\nMAHNOB-HCI [52], MMSE-HR [55], and OBF [29]).\n4.1. Datasets and Performance Metrics\nVIPL-HR [45] is a large-scale dataset for remote phys-\niological measurement under less-constrained scenarios. It\ncontains 2,378 RGB videos of 107 subjects recorded with\ndifferent head movements, lighting conditions and acqui-\nsition devices. MAHNOB-HCI [52] is one of the most\nwidely used benchmark for remote HR measurement evalu-\nations. It includes 527 facial videos of with 61 fps framerate\nand 780x580 resolution from 27 subjects. MMSE-HR [55]\nis a dataset including 102 RGB videos from 40 subjects, and\nthe raw resolution of each video is at 1040x1392.OBF [29]\nis a high-quality dataset for remote physiological signal\nmeasurement. It contains 200 five-minute-long RGB videos\nwith 60 fps framerate recorded from 100 healthy adults.\nAverage HR estimation task is evaluated on all four\ndatasets while HRV and RF estimation tasks on high-quality\nOBF [29] dataset. Specifically, we follow existing meth-\nods [41, 46, 67] and report low frequency (LF), high fre-\nquency (HF), and LF/HF ratio for HRV and RF estimation.\nWe report the most commonly used performance metrics\nfor evaluation, including the standard deviation (SD), mean\nabsolute error (MAE), root mean square error (RMSE), and\nPearson’s correlation coefficient (r).\n4.2. Implementation Details\nOur proposed method is implemented with Pytorch. For\neach video clip, we use the MTCNN face detector [72]\nto crop the enlarged face area at the first frame and fix\nthe region through the following frames. The videos in\nMAHNOB-HCI and OBF are downsampled to 30 fps for ef-\nficiency. The settings N=12, h=4, D=96, D′=144 are used\nfor PhysFormer while θ=0.7 and τ=2.0 for TD-MHSA. The\ntargeted tube size Ts × Hs × Ws equals to 4×4×4. In the\ntraining stage, we randomly sample RGB face clips with\nsize 160×128×128 (T ×H ×W) as model inputs. Random\nhorizontal flipping and temporally up/down-sampling [63]\nare used for data augmentation. The PhysFormer is trained\nwith Adam optimizer and the initial learning rate and weight\ndecay are 1e-4 and 5e-5, respectively. We cannot find ob-\nvious performance improvement using AdamW optimizer.\nWe train models with 25 epochs with fixed setting α=0.1\nTable 1. Intra-dataset testing results on VIPL-HR [45]. The sym-\nbols ▲, ♦ and ⋆ denote traditional, non-end-to-end learning based\nand end-to-end learning based methods, respectively. Best results\nare marked in bold and second best in underline.\nMethod SD ↓\n(bpm)\nMAE ↓\n(bpm)\nRMSE ↓\n(bpm) r ↑\nTulyakov2016 [55]▲ 18.0 15.9 21.0 0.11\nPOS [59]▲ 15.3 11.5 17.2 0.30\nCHROM [13]▲ 15.1 11.4 16.9 0.28\nRhythmNet [45]♦ 8.11 5.30 8.14 0.76\nST-Attention [47]♦ 7.99 5.40 7.99 0.66\nNAS-HR [40]♦ 8.10 5.12 8.01 0.79\nCVD [46]♦ 7.92 5.02 7.97 0.79\nDual-GAN [41]♦ 7.63 4.93 7.68 0.81\nI3D [7]⋆ 15.9 12.0 15.9 0.07\nPhysNet [65]⋆ 14.9 10.8 14.8 0.20\nDeepPhys [11]⋆ 13.6 11.0 13.8 0.11\nAutoHR [63]⋆ 8.48 5.68 8.68 0.72\nPhysFormer (Ours)⋆ 7.74 4.97 7.79 0.78\nfor temporal loss while exponentially increased parameter\nβ ∈ [1, 5] for frequency losses. We set σ=1.0 for label dis-\ntribution learning. The batch size is 4 on one 32G V100\nGPU. In the testing stage, similar to [45], we uniformly sep-\narate 30-second videos into three short clips with 10 sec-\nonds, and then video-level HR is calculated via averaging\nHRs from three short clips.\n4.3. Intra-dataset Testing\nHR estimation on VIPL-HR. In these experiments,\nwe follow [45] and use a subject-exclusive 5-fold cross-\nvalidation protocol on VIPL-HR. As shown in Table 1, all\nthree traditional methods (Tulyakov2016 [55], POS [59]\nand CHROM [13]) perform poorly due to the complex sce-\nnarios (e.g., large head movement and various illumination)\nin the VIPL-HR dataset. Similarly, the existing end-to-end\nlearning based methods (e.g., PhysNet [65], DeepPhys [11],\nand AutoHR [63]) predict unreliable HR values with large\nRMSE compared with non-end-to-end learning approaches\n(e.g., RhythmNet [45], ST-Attention [47], NAS-HR [40],\nCVD [46], and Dual-GAN [41]). Such the large perfor-\nmance margin might be caused by the coarse and overfit-\nted rPPG features extracted from the end-to-end models. In\ncontrast, all five non-end-to-end methods first extract fine-\ngrained signal maps from multiple facial ROIs, and then\nmore dedicated rPPG clues would be extracted via the cas-\ncaded models. Without strict and heavy preprocessing pro-\ncedure in [40, 41, 45–47], our proposed PhysFormer can be\ntrained from scratch on facial videos directly, and achieves\ncomparable performance with state-of-the-art non-end-to-\nend learning based method Dual-GAN [41]. It indicates\nthat PhysFormer is able to learn the intrinsic and periodic\nrPPG-aware features automatically.\nTable 2. Performance comparison of HR and RF measurement as well as HRV analysis on OBF [29].\nHR(bpm) RF(Hz) LF(u.n) HF(u.n) LF/HF\nMethod SD RMSE r SD RMSE r SD RMSE r SD RMSE r SD RMSE r\nROI green [29]▲ 2.159 2.162 0.99 0.078 0.084 0.321 0.22 0.24 0.573 0.22 0.24 0.573 0.819 0.832 0.571\nCHROM [13]▲ 2.73 2.733 0.98 0.081 0.081 0.224 0.199 0.206 0.524 0.199 0.206 0.524 0.83 0.863 0.459\nPOS [59]▲ 1.899 1.906 0.991 0.07 0.07 0.44 0.155 0.158 0.727 0.155 0.158 0.727 0.663 0.679 0.687\nCVD [46]♦ 1.257 1.26 0.996 0.058 0.058 0.606 0.09 0.09 0.914 0.09 0.09 0.914 0.453 0.453 0.877\nrPPGNet [67]⋆ 1.756 1.8 0.992 0.064 0.064 0.53 0.133 0.135 0.804 0.133 0.135 0.804 0.58 0.589 0.773\nPhysFormer (Ours)⋆ 0.804 0.804 0.998 0.054 0.054 0.661 0.085 0.086 0.912 0.085 0.086 0.912 0.389 0.39 0.896\nTable 3. Intra-dataset results on MAHNOB-HCI [52].\nMethod SD ↓\n(bpm)\nMAE↓\n(bpm)\nRMSE↓\n(bpm) r ↑\nPoh2011 [49]▲ 13.5 - 13.6 0.36\nCHROM [13]▲ - 13.49 22.36 0.21\nLi2014 [30]▲ 6.88 - 7.62 0.81\nTulyakov2016 [55]▲ 5.81 4.96 6.23 0.83\nSynRhythm [44]♦ 10.88 - 11.08 -\nRhythmNet [45]♦ 3.99 - 3.99 0.87\nHR-CNN [53]⋆ - 7.25 9.24 0.51\nrPPGNet [67]⋆ 7.82 5.51 7.82 0.78\nDeepPhys [11]⋆ - 4.57 - -\nAutoHR [63]⋆ 4.73 3.78 5.10 0.86\nMeta-rPPG [28]⋆ 4.9 3.01 3.68 0.85\nPhysFormer (Ours)⋆ 3.87 3.25 3.97 0.87\nHR estimation on MAHNOB-HCI. For the HR esti-\nmation tasks on MAHNOB-HCI, similar to [67], subject-\nindependent 9-fold cross-validation protocol is adopted. In\nconsideration of the convergence difficulty due to the low\nillumination and high compression videos in MAHNOB-\nHCI, we finetune the VIPL-HR pretrained model on\nMAHNOB-HCI for further 15 epochs. The HR estimation\nresults are shown in Table 3. The proposed PhysFormer\nachieves the lowest SD (3.87 bpm) and highest r (0.87)\namong the traditional, non-end-to-end learning, and end-to-\nend learning methods, which indicates the reliability of the\nlearned rPPG features from PhysFormer under sufficient su-\npervision. Our performance is on par with the latest end-to-\nend learning method Meta-rPPG [28] without transductive\nadaptation from target frames.\nHR, HRV and RF estimation on OBF.We also conduct\nexperiments for three types of physiological signals, i.e.,\nHR, RF, and HRV measurement on the OBF [29] dataset.\nFollowing [46, 67], we use a 10-fold subject-exclusive pro-\ntocol for all experiments. All the results are shown in Ta-\nble 2. From the results, we can see that the proposed ap-\nproach outperforms the existing state-of-the-art traditional\n(ROI green [29], CHROM [13], POS [59]) and end-to-end\nlearning (rPPGNet [67]) methods by a large margin on all\nevaluation metrics for HR, RF and all HRV features. The\nproposed PhysFormer also gives more accurate estimation\nTable 4. Cross-dataset results on MMSE-HR [55].\nMethod SD ↓\n(bpm)\nMAE ↓\n(bpm)\nRMSE↓\n(bpm) r ↑\nLi2014 [30]▲ 20.02 - 19.95 0.38\nCHROM [13]▲ 14.08 - 13.97 0.55\nTulyakov2016 [55]▲ 12.24 - 11.37 0.71\nST-Attention [47]♦ 9.66 - 10.10 0.64\nRhythmNet [45]♦ 6.98 - 7.33 0.78\nCVD [46]♦ 6.06 - 6.04 0.84\nPhysNet [65]⋆ 12.76 - 13.25 0.44\nTS-CAN [34]⋆ - 3.85 7.21 0.86\nAutoHR [63]⋆ 5.71 - 5.87 0.89\nEfficientPhys-C [35]⋆ - 2.91 5.43 0.92\nEfficientPhys-T1 [35]⋆ - 3.48 7.21 0.86\nPhysFormer (Ours)⋆ 5.22 2.84 5.36 0.92\nin terms of HR, RF, and LF/HF compared with the prepro-\ncessed signal map based non-end-to-end learning method\nCVD [46]. These results indicate that PhysFormer could\nnot only handle the average HR estimation task but also give\na promising prediction of the rPPG signal for RF measure-\nment and HRV analysis, which shows its potential in many\nhealthcare applications.\n4.4. Cross-dataset Testing\nBesides of the intra-dataset testings on the VIPL-HR,\nMAHNOB-HCI, and OBF datasets, we also conduct cross-\ndataset testing on MMSE-HR [55] following the protocol\nof [45]. The models trained on VIPL-HR are directly tested\non MMSE-HR. All the results of the proposed approach and\nthe state-of-the-art methods are shown in Table 4. It is clear\nthat the proposed PhysFormer generalizes well in unseen\ndomain. It is worth noting that PhysFormer achieves the\nlowest SD (5.22 bpm), MAE (2.84 bpm), RMSE (5.36 bpm)\nas well as the highest r (0.92) among the traditional, non-\nend-to-end learning and end-to-end learning based meth-\nods, indicating 1) the predicted HRs are highly correlated\nwith the ground truth HRs, and 2) the model learns domain-\ninvariant intrinsic rPPG-aware features. Compared with the\nspatio-temporal transformer based EfficientPhys-T1 [35],\nour proposed PhysFormer is able to predict more accurate\nphysiological signals, which indicates the effectiveness of\nthe long-range spatio-temporal attention.\nTable 5. Ablation of Tube Tokenization of PhysFormer. The three\ndimensions in tensors indicate length× height×width.\nInputs [Stem]\nFeature Size\n[Tube Size]\nToken Numbers\nRMSE↓\n(bpm)\n160×128×128 [×]\n160×128×128\n[4×32×32]\n40×4×4 10.62\n160×128×128 [√]\n160×16×16\n[4×4×4]\n40×4×4 7.56\n160×96×96 [√]\n160×12×12\n[4×4×4]\n40×3×3 8.03\n160×128×128 [√]\n160×16×16\n[4×16×16]\n40×1×1 10.61\n160×128×128 [√]\n160×16×16\n[2×4×4]\n80×4×4 7.81\nTable 6. Ablation of TD-MHSA and ST-FF in PhysFormer.\nMHSA τ Feed-forward RMSE (bpm)↓\n- - ST-FF 9.81\nTD-MHSA √Dh ≈4.9 ST-FF 9.51\nTD-MHSA 2.0 ST-FF 7.56\nvanilla MHSA 2.0 ST-FF 10.43\nTD-MHSA 2.0 vanilla FF 8.27\nTable 7. Ablation of dynamic loss in the frequency domain. The\ntemporal loss Ltime is with fixed α=0.1 here. ‘CE’ and ‘LD’ de-\nnote cross-entropy and label distribution, respectively.\nFrequency loss β Strategy RMSE (bpm)↓\nLCE + LLD 1.0 fixed 8.48\nLCE + LLD 5.0 fixed 8.86\nLCE + LLD [1.0, 5.0] linear 8.37\nLCE + LLD [1.0, 5.0] exponential 7.56\nLCE [1.0, 5.0] exponential 8.09\nLLD [1.0, 5.0] exponential 8.21\nLLD (real distribution)[1.0, 5.0] exponential 8.72\n4.5. Ablation Study\nWe also provide the results of ablation studies for HR\nestimation on the Fold-1 of the VIPL-HR [45] dataset.\nImpact of tube tokenization. In the default setting of\nPhysFormer, a shallow stem cascaded with a tube tokeniza-\ntion is used. In this ablation, we consider other four tok-\nenization configurations with or w/o stem. It can be seen\nfrom the first row in Table 5 that the stem helps the Phys-\nFormer see better [61], and the RMSE increases dramati-\ncally (+3.06 bpm) when w/o the stem. Then we investigate\nthe impacts of the spatial and temporal domains in tube to-\nkenization. It is clear that the result in the fourth row with\nfull spatial projection is quite poor (RMSE=10.61 bpm), in-\ndicating the necessity of the spatial attention. In contrast,\ntokenization with smaller tempos (e.g., [2x4x4]) or spatial\ninputs (e.g., 160x96x96) reduces performance slightly.\nImpact of TD-MHSA and ST-FF.As shown in Table 6,\nboth the TD-MHSA and ST-FF play vital roles in Phys-\nFormer. The result in the first row shows that the per-\nformance degrades sharply without spatio-temporal atten-\ntion. Moreover, it can be seen from the last two rows\nFigure 3. Testing results of fixed and dynamic frequency supervi-\nsions on the Fold-1 of VIPL-HR.\nthat without TD-MHSA/ST-FF, PhysFormer with vanilla\nMHSA/FF obtains 10.43/8.27 bpm RMSE. One important\nfinding in this research is that, the temperature τ influ-\nences the MHSA a lot. When the τ = √Dh like previ-\nous ViT [1, 15], the predicted rPPG signals are unsatisfied\n(RMSE=9.51 bpm). Regularizing the τ with smaller value\nenforces sparser spatio-temporal attention, which is effec-\ntive for the quasi-periodic rPPG task.\nImpact of label distribution learning.Besides the tem-\nporal loss Ltime and frequency cross-entropy loss LCE, the\nablations w/ and w/o label distribution loss LLD are shown\nin the last four rows of Table 7. Although the LLD per-\nforms slightly worse (+0.12 bpm RMSE) than LCE, the\nbest performance can be achieved using both losses, indi-\ncating the effectiveness of explicit distribution constraints\nfor extreme-frequency interference alleviation and adjacent\nlabel knowledgement propagation. It is interesting to find\nfrom the last two rows that using real PSD distribution from\ngroundtruth PPG signals as p, the performance is inferior\ndue to the lack of an obvious peak and partial noise. We can\nalso find from the Fig. 4(a) that theσ ranged from 0.9 to 1.2\nfor LLD are suitable to achieve good performance.\nImpact of dynamic supervision. Fig. 3 illustrates the\ntesting performance on Fold-1 VIPL-HR when training with\nfixed and dynamic supervision. It is clear that with expo-\nnential increased frequency loss, models in the blue curve\nconverge faster and achieve smaller RMSE. We also com-\npare several kinds of fixed and dynamic strategies in Ta-\nble 7. The results in the first four rows indicate 1) using\nfixed higher β leads to poorer performance caused by the\nconvergency difficulty; 2) models with the exponentially in-\ncreased β perform better than using linear increment.\nImpact of θ and layer/head numbers. Hyperparameter\nθ tradeoffs the contribution of local temporal gradient in-\nformation. As illustrated in Fig. 3(b), PhysFormer could\nachieve smaller RMSE when θ=0.4 and 0.7, indicating the\nimportance of the normalized local temporal difference fea-\ntures for global spatio-temporal attention. We also investi-\ngate how the layer and head numbers influence the perfor-\nmance. As shown in Fig. 5(a), with deeper temporal trans-\n(a) \n(b) \nFigure 4. Impacts of the (a) σ in label distribution learning and\n(b) θ in TD-MHSA.\n6 9 12\nlayers\nRMSE (bpm)\nRMSE (bpm)\n1 2 3 4 6\nheads\n(a) (b)\nFigure 5. Ablation of the (a) layers and (b) heads in PhysFormer.\nformer blocks, the RMSE are reduced progressively despite\nheavier computational cost. In terms of the impact of head\nnumbers, it is clear to find from Fig. 5(b) that PhysFormer\nwith four heads perform the best while fewer heads lead to\nsharp performance drops.\n4.6. Visualization and Discussion\nWe visualize the attention map from the last TD-MHSA\nmodule as well as one example about the query-key interac-\ntion in Fig. 6. The x and y axes indicate the attention con-\nfidence from key and query tube tokens, respectively. From\nthe attention map, we can easily find periodic or quasi-\nperiodic responses along both axes, indicating the period-\nicity of the intrinsic rPPG features from PhysFormer. To be\nspecific, given the 530th tube token (in blue) from the fore-\nhead (spatial face domain) and peak (temporal signal do-\nmain) locations as a query, the corresponding key responses\nare illustrated at the blue line in the attention map. On one\nhand, it can be seen from the key responses that dominant\nspatial attentions focus on the facial skin regions and dis-\ncard unrelated background. On the other hand, the temporal\nlocalizations of the key responses are around peak positions\nin the predicted rPPG signals. All these patterns are rea-\nsonable: 1) the forehead and cheek regions [57] have richer\nblood volume for rPPG measurement and are also reliable\nsince these regions are less affected by facial muscle move-\nments due to e.g., facial expressions, talking; and 2) rPPG\nsignals from healthy people are usually periodic.\nHowever, we also find two limitations of the spatio-\ntemporal attention from Fig. 6. First, there are still some\nunexpected responses (e.g., continuous query tokens with\nsimilar key responses) in the attention map, which might\nintroduce task-irrelevant noise and damage to the perfor-\n1\n5\n10\n15\n20\n25\n30\n35\n40\n4035302520151051\n1\n2\n3\n4\n1 2 3 4\nQuery ( T'xH'xW' = 40x4x4 )\nKey ( T'xH'xW' = 40x4x4 )\nA Query\nAttention Map \nKey Responses \nrPPG signals \n(downsampled) \nQuery\nFigure 6. Visualization of the attention map from the 1st head\nin last TD-MHSA module. Given the 530th tube token in blue as\na query, representative key responses are illustrated (the brighter,\nthe more attentive). The predicted downsampled rPPG signals are\nshown for temporal attention understanding.\nmance. Second, the temporal attentions are not always ac-\ncurate, and some are coarse with phase shifts (e.g., the first\nvertical dotted line of the rPPG signals in bottom Fig. 6).\n5. Conclusions and Future Work\nIn this paper, we propose an end-to-end video trans-\nformer architecture, namely PhysFormer, for remote phys-\niological measurement. With temporal difference trans-\nformer and elaborate supervisions, PhysFormer is able to\nachieve superior performance on benchmark datasets. The\nstudy of video transformer based physiological measure-\nment is still at an early stage. Future directions include:\n1) Designing more efficient architectures. The proposed\nPhysFormer is with 7.03 M parameters and 47.01 GFLOPs,\nwhich is unfriendly for mobile deployment; 2) Exploring\nmore accurate yet efficient spatio-temporal self-attention\nmechanism especially for long sequence rPPG monitoring.\nAcknowledgment This work was supported by the\nAcademy of Finland for Academy Professor project Emo-\ntionAI (grants 336116, 345122), and ICT 2023 project\n(grant 328115), by Ministry of Education and Culture of\nFinland for AI forum project, the National Natural Sci-\nence Foundation of China (Grant No. 62002283), the EP-\nSRC grant: Turing AI Fellowship: EP/W002981/1, EP-\nSRC/MURI grant EP/N019474/1. We would also like to\nthank the Royal Academy of Engineering and FiveAI. As\nwell, the authors wish to acknowledge CSC-IT Center for\nScience, Finland, for computational resources.\nReferences\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu ˇci´c, and Cordelia Schmid. Vivit: A video vi-\nsion transformer. ICCV, 2021.\n[2] Yoshua Bengio, J ´erˆome Louradour, Ronan Collobert, and Ja-\nson Weston. Curriculum learning. In ICML, 2009.\n[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nspace-time attention all you need for video understanding?\narXiv:2102.05095, 2021.\n[4] Adrian Bulat, Juan-Manuel Perez-Rua, Swathikiran Sud-\nhakaran, Brais Martinez, and Georgios Tzimiropoulos.\nSpace-time mixing attention for video transformer.NeurIPS,\n2021.\n[5] Jiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool.\nVideo super-resolution transformer. arXiv:2106.06847,\n2021.\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020.\n[7] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In CVPR,\n2017.\n[8] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit:\nCross-attention multi-scale vision transformer for image\nclassification. ICCV, 2021.\n[9] Haoyu Chen, Hao Tang, Nicu Sebe, and Guoying Zhao. An-\niformer: Data-driven 3d animation with transformer. BMVC,\n2021.\n[10] Haoyu Chen, Hao Tang, Zitong Yu, Nicu Sebe, and Guoying\nZhao. Geometry-contrastive transformer for generalized 3d\npose transfer. In AAAI, 2022.\n[11] Weixuan Chen and Daniel McDuff. Deepphys: Video-\nbased physiological measurement using convolutional atten-\ntion networks. In ECCV, 2018.\n[12] Xun Chen, Juan Cheng, Rencheng Song, Yu Liu, Rabab\nWard, and Z Jane Wang. Video-based heart rate measure-\nment: Recent advances and future prospects. IEEE Transac-\ntions on Instrumentation and Measurement, 2018.\n[13] Gerard De Haan and Vincent Jeanne. Robust pulse rate from\nchrominance-based rppg. IEEE Transactions on Biomedical\nEngineering, 2013.\n[14] Mingyu Ding, Xiaochen Lian, Linjie Yang, Peng Wang, Xi-\naojie Jin, Zhiwu Lu, and Ping Luo. Hr-nas: Searching ef-\nficient high-resolution neural architectures with lightweight\ntransformers. In CVPR, 2021.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. ICLR, 2021.\n[16] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.\nMultiscale vision transformers. ICCV, 2021.\n[17] Bin-Bin Gao, Chao Xing, Chen-Wei Xie, Jianxin Wu, and\nXin Geng. Deep label distribution learning with label ambi-\nguity. IEEE TIP, 2017.\n[18] Bin-Bin Gao, Hong-Yu Zhou, Jianxin Wu, and Xin Geng.\nAge estimation using expectation of label distribution learn-\ning. In IJCAI, 2018.\n[19] Xin Geng, Chao Yin, and Zhi-Hua Zhou. Facial age esti-\nmation by learning from label distributions. IEEE TPAMI,\n2013.\n[20] John Gideon and Simon Stent. The way to my heart is\nthrough contrastive learning: Remote photoplethysmogra-\nphy from unlabelled video. In ICCV, 2021.\n[21] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In CVPR, 2019.\n[22] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,\nJianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chun-\njing Xu, Yixing Xu, et al. A survey on visual transformer.\narXiv:2012.12556, 2020.\n[23] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chun-\njing Xu, and Yunhe Wang. Transformer in transformer.\narXiv:2103.00112, 2021.\n[24] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. Transreid: Transformer-based object re-\nidentification. ICCV, 2021.\n[25] Gee-Sern Hsu, ArulMurugan Ambikapathi, and Ming-\nShiang Chen. Deep learning with time-frequency represen-\ntation for pulse estimation from facial videos. InIJCB, 2017.\n[26] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv:2101.01169,\n2021.\n[27] Antony Lam and Yoshinori Kuno. Robust heart rate mea-\nsurement from video using select random patches. In ICCV,\n2015.\n[28] Eugene Lee, Evan Chen, and Chen-Yi Lee. Meta-rppg: Re-\nmote heart rate estimation using a transductive meta-learner.\nIn ECCV, 2020.\n[29] Xiaobai Li, Iman Alikhani, Jingang Shi, Tapio Seppanen,\nJuhani Junttila, Kirsi Majamaa-V oltti, Mikko Tulppo, and\nGuoying Zhao. The obf database: A large face video\ndatabase for remote physiological signal measurement and\natrial fibrillation detection. In FG, 2018.\n[30] Xiaobai Li, Jie Chen, Guoying Zhao, and Matti Pietikainen.\nRemote heart rate measurement from face videos under real-\nistic situations. In CVPR, 2014.\n[31] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift\nmodule for efficient video understanding. In CVPR, 2019.\n[32] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.\nA survey of transformers. arXiv:2106.04554, 2021.\n[33] Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei\nLu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hong-\nsheng Li. Fuseformer: Fusing fine-grained information in\ntransformers for video inpainting. In ICCV, 2021.\n[34] Xin Liu, Josh Fromm, Shwetak Patel, and Daniel McDuff.\nMulti-task temporal shift attention networks for on-device\ncontactless vitals measurement. NeurIPS, 2020.\n[35] Xin Liu, Brian L Hill, Ziheng Jiang, Shwetak Patel, and\nDaniel McDuff. Efficientphys: Enabling simple, fast and ac-\ncurate camera-based vitals measurement. arXiv:2110.04447,\n2021.\n[36] Xin Liu, Shwetak Patel, and Daniel McDuff. Camera-\nbased physiological sensing: Challenges and future direc-\ntions. arXiv:2110.13362, 2021.\n[37] Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Song Bai,\nand Xiang Bai. End-to-end temporal action detection with\ntransformer. arXiv:2106.10271, 2021.\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. ICCV, 2021.\n[39] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer.\narXiv:2106.13230, 2021.\n[40] Hao Lu and Hu Han. Nas-hr: Neural architecture search\nfor heart rate estimation from face videos. Virtual Reality &\nIntelligent Hardware, 2021.\n[41] Hao Lu, Hu Han, and S Kevin Zhou. Dual-gan: Joint bvp\nand noise modeling for remote physiological measurement.\nIn CVPR, 2021.\n[42] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Assel-\nmann. Video transformer network. arXiv:2102.00719, 2021.\n[43] Xuesong Niu, Hu Han, Shiguang Shan, and Xilin Chen. Con-\ntinuous heart rate measurement from face: A robust rppg ap-\nproach with distribution learning. In IJCB, 2017.\n[44] Xuesong Niu, Hu Han, Shiguang Shan, and Xilin Chen. Syn-\nrhythm: Learning a deep heart rate estimator from general to\nspecific. In ICPR, 2018.\n[45] Xuesong Niu, Shiguang Shan, Hu Han, and Xilin Chen.\nRhythmnet: End-to-end heart rate estimation from face via\nspatial-temporal representation. IEEE TIP, 2019.\n[46] Xuesong Niu, Zitong Yu, Hu Han, Xiaobai Li, Shiguang\nShan, and Guoying Zhao. Video-based remote physiologi-\ncal measurement via cross-verified feature disentangling. In\nECCV, pages 295–310. Springer, 2020.\n[47] Xuesong Niu, Xingyuan Zhao, Hu Han, Abhijit Das, Antitza\nDantcheva, Shiguang Shan, and Xilin Chen. Robust remote\nheart rate estimation from face utilizing spatial-temporal at-\ntention. In FG, 2019.\n[48] Ewa M Nowara, Daniel McDuff, and Ashok Veeraraghavan.\nThe benefit of distraction: Denoising camera-based phys-\niological measurements using inverse attention. In ICCV,\n2021.\n[49] Ming-Zher Poh, Daniel McDuff, and Rosalind Picard. Ad-\nvancements in noncontact, multiparameter physiological\nmeasurements using a webcam. IEEE transactions on\nbiomedical engineering, 2010.\n[50] Ming-Zher Poh, Daniel J McDuff, and Rosalind W Picard.\nNon-contact, automated cardiac pulse measurements using\nvideo imaging and blind source separation. Optics express,\n2010.\n[51] Ying Qiu, Yang Liu, Juan Arteaga-Falconi, Haiwei Dong,\nand Abdulmotaleb El Saddik. Evm-cnn: Real-time contact-\nless heart rate estimation from facial video. IEEE TMM ,\n2018.\n[52] Mohammad Soleymani, Jeroen Lichtenauer, Thierry Pun,\nand Maja Pantic. A multimodal database for affect recog-\nnition and implicit tagging. IEEE transactions on affective\ncomputing, 2011.\n[53] Radim ˇSpetl´ık, V ojtech Franc, and Jir´ı Matas. Visual heart\nrate estimation with convolutional neural network. InBMVC,\n2018.\n[54] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In ICML, 2021.\n[55] Sergey Tulyakov, Xavier Alameda-Pineda, Elisa Ricci, Lijun\nYin, Jeffrey F Cohn, and Nicu Sebe. Self-adaptive matrix\ncompletion for heart rate estimation from face videos under\nrealistic conditions. In CVPR, 2016.\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017.\n[57] Wim Verkruysse, Lars O Svaasand, and J Stuart Nelson. Re-\nmote plethysmographic imaging using ambient light. Optics\nexpress, 2008.\n[58] Lining Wang, Haosen Yang, Wenhao Wu, Hongxun Yao,\nand Hujie Huang. Temporal action proposal generation with\ntransformers. arXiv:2105.12043, 2021.\n[59] Wenjin Wang, Albertus C den Brinker, Sander Stuijk, and\nGerard de Haan. Algorithmic principles of remote ppg.IEEE\nTransactions on Biomedical Engineering, 2017.\n[60] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions. ICCV, 2021.\n[61] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr\nDoll´ar, and Ross Girshick. Early convolutions help trans-\nformers see better. NeurIPS, 2021.\n[62] Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu Li, Wei Xia,\nZhuowen Tu, and Stefano Soatto. Long short-term trans-\nformer for online action detection. arXiv:2107.03377, 2021.\n[63] Zitong Yu, Xiaobai Li, Xuesong Niu, Jingang Shi, and Guoy-\ning Zhao. Autohr: A strong end-to-end baseline for remote\nheart rate measurement with neural searching. IEEE SPL,\n2020.\n[64] Zitong Yu, Xiaobai Li, Pichao Wang, and Guoying Zhao.\nTransrppg: Remote photoplethysmography transformer for\n3d mask face presentation attack detection. IEEE SPL, 2021.\n[65] Zitong Yu, Xiaobai Li, and Guoying Zhao. Remote photo-\nplethysmograph signal measurement from facial videos us-\ning spatio-temporal networks. In BMVC, 2019.\n[66] Zitong Yu, Xiaobai Li, and Guoying Zhao. Facial-video-\nbased physiological signal measurement: Recent advances\nand affective applications. IEEE Signal Processing Maga-\nzine, 2021.\n[67] Zitong Yu, Wei Peng, Xiaobai Li, Xiaopeng Hong, and\nGuoying Zhao. Remote heart rate measurement from highly\ncompressed facial videos: an end-to-end deep learning solu-\ntion with video enhancement. In ICCV, 2019.\n[68] Zitong Yu, Yunxiao Qin, Xiaobai Li, Chenxu Zhao, Zhen\nLei, and Guoying Zhao. Deep learning for face anti-\nspoofing: a survey. arXiv:2106.14948, 2021.\n[69] Zitong Yu, Benjia Zhou, Jun Wan, Pichao Wang, Haoyu\nChen, Xin Liu, Stan Z Li, and Guoying Zhao. Searching\nmulti-rate and multi-modal temporal enhanced networks for\ngesture recognition. IEEE TIP, 2021.\n[70] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. arXiv:2101.11986, 2021.\n[71] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning\njoint spatial-temporal transformations for video inpainting.\nIn ECCV, 2020.\n[72] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.\nJoint face detection and alignment using multitask cascaded\nconvolutional networks. IEEE SPL, 2016.\n[73] Jiaojiao Zhao, Xinyu Li, Chunhui Liu, Shuai Bing, Hao\nChen, Cees GM Snoek, and Joseph Tighe. Tuber: Tube-\ntransformer for action detection. arXiv:2104.00969, 2021.\n[74] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. In CVPR, 2021.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7713781595230103
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6137012243270874
    },
    {
      "name": "Transformer",
      "score": 0.577113687992096
    },
    {
      "name": "Feature learning",
      "score": 0.508243978023529
    },
    {
      "name": "Deep learning",
      "score": 0.4852466583251953
    },
    {
      "name": "Overfitting",
      "score": 0.4748387932777405
    },
    {
      "name": "Machine learning",
      "score": 0.4643305838108063
    },
    {
      "name": "Convolutional neural network",
      "score": 0.44443196058273315
    },
    {
      "name": "Scalability",
      "score": 0.43018966913223267
    },
    {
      "name": "Segmentation",
      "score": 0.42790257930755615
    },
    {
      "name": "Artificial neural network",
      "score": 0.2448047697544098
    },
    {
      "name": "Engineering",
      "score": 0.07975244522094727
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}