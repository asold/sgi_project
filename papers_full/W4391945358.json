{
  "title": "Spatial linear transformer and temporal convolution network for traffic flow prediction",
  "url": "https://openalex.org/W4391945358",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5040004342",
      "name": "Zhibo Xing",
      "affiliations": [
        "Shenyang Jianzhu University"
      ]
    },
    {
      "id": "https://openalex.org/A5101412513",
      "name": "Mingxia Huang",
      "affiliations": [
        "Shenyang Jianzhu University"
      ]
    },
    {
      "id": "https://openalex.org/A5101690530",
      "name": "Wentao Li",
      "affiliations": [
        "Shenyang Jianzhu University"
      ]
    },
    {
      "id": "https://openalex.org/A5101306736",
      "name": "Dan Peng",
      "affiliations": [
        "Shenyang Jianzhu University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1973943669",
    "https://openalex.org/W1967514707",
    "https://openalex.org/W2277710627",
    "https://openalex.org/W2463743813",
    "https://openalex.org/W2903871660",
    "https://openalex.org/W2968149264",
    "https://openalex.org/W3195009220",
    "https://openalex.org/W3212604410",
    "https://openalex.org/W3159993297",
    "https://openalex.org/W4281711047",
    "https://openalex.org/W2989567700",
    "https://openalex.org/W2997848713",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W4317495521",
    "https://openalex.org/W6603242443",
    "https://openalex.org/W2756203131",
    "https://openalex.org/W2965341826",
    "https://openalex.org/W3080253043",
    "https://openalex.org/W4382239616",
    "https://openalex.org/W3103720336"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports\nSpatial linear transformer \nand temporal convolution network \nfor traffic flow prediction\nZhibo Xing , Mingxia Huang *, Wentao Li  & Dan Peng \nAccurately obtaining accurate information about the future traffic flow of all roads in the \ntransportation network is essential for traffic management and control applications. In order to \naddress the challenges of acquiring dynamic global spatial correlations between transportation links \nand modeling time dependencies in multi-step prediction, we propose a spatial linear transformer \nand temporal convolution network (SL TTCN). The model is using spatial linear transformers to \naggregate the spatial information of the traffic flow, and bidirectional temporal convolution network \nto capture the temporal dependency of the traffic flow. The spatial linear transformer effectively \nreduces the complexity of data calculation and storage while capturing spatial dependence, and the \ntime convolutional network with bidirectional and gate fusion mechanisms avoids the problems of \ngradient vanishing and high computational cost caused by long time intervals during model training. \nWe conducted extensive experiments using two publicly available large-scale traffic data sets and \ncompared SL TTCN with other baselines. Numerical results show that SL TTCN achieves the best \npredictive performance in various error measurements. We also performed attention visualization \nanalysis on the spatial linear transformer, verifying its effectiveness in capturing dynamic global \nspatial dependency.\nKeywords Traffic forecasting, Deep learning, Spatial linear transformer, Bidirectional temporal convolution \nnetwork, Dynamic global spatial dependency\nTraffic state prediction can be regarded as the problem of spatial–temporal prediction, the traffic states (volumes, \nspeeds, occupancy, etc.) are recorded at a fixed time with corresponding locations which are distributed in con-\ntinuous space. As is shown in Fig. 1, it is evident that, on the one hand, the predictions of future traffic states are \nimpacted by the observations of historical traffic states with respect to temporal factors, on the other hand, the \ntraffic states of currents area are influenced by its neighbor areas with respect to spatial factors. Therefore, we \nneed to mining the temporal and spatial relationship of traffic data to help us predict the traffic states efficiently.\nNowadays, the transportation technology brings convenience for collecting traffic data. The traffic device \ndetects the traffic data in real time with very short time interval so the characters of the traffic data are non-\nlinear and complex. Therefore, it is a challenge to discover the inherent temporal-spatial relationship and predict \nthe traffic state accurately. After decades of studies and practices, various approaches of traffic prediction have \ncome into being. For statistical approach, models of time series analyses, especially the autoregressive integrated \nmoving average (ARIMA), are widely used in traffic  prediction1 firstly applied the ARIMA model for incident \ndetection in univariate single-point settings, then a lot of  variations2,3 come into being. For machine learning \napproach, the classical models including the k-nearest neighbor (KNN) 4 and support vector machine (SVM) 5. \nThese approaches can model more complex traffic data but they require the careful feature engineering.\nRecent years, the deep learning approaches, as a kind of end-to-end learning mode, can handle nonlinear \nhigh-dimensional problems, which has attracted interest of researchers. Furthermore, the graph neural net-\nwork (GNN) as one of the deep learning fields, has been proposed for temporal-spatial  modeling6. Vividly, the \nsensors in road network are like the  nodes7,8 in graph, and the connectivity between sensors are like the edges \nbetween nodes with the weights. In recent years, a popular and effective analysis has emerged, that is combing \nthe GNN with the sequence learning model. For  instance9,10, integrate the graph convolution network (GCN) \nwith recurrent network (RNN) to introduce the inherent road network topology into time series model, so their \nperformance apparently improves. With the rise of attention  mechanisms11 in various fields, many  studies12,13 \nhave combined GNN with transformers to model spatio-temporal dependencies. It can capture the correlation \nOPEN\nSchool of Transportation and Geomatics Engineering, Shenyang Jianzhu University, Shenyang  110168, Liaoning, \nChina. *email: mingxia@sjzu.edu.cn\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\ncoefficients of multi-dimensional features of raw data and obtain more accurate prediction results. However, the \nabove studies are having two issues to overcome in temporal and spatial dependency modeling.\nFor spatial dependency, the problem is that they explore spatial dependency based on the assumption of a \npredefined graph structure which is impacted directly by their spatial connectivity. But in some situation the \nspatial connectivity cannot correctly reflect the real dependency between two nodes. To solve this problem, the \ngraph attention network (GAT) theory came into being. The GAT 14,15 focuses on construct the dynamic spatial \ndependency by compute the spatial dependency from the traffic state of object node to the relational node and \nit becomes a popular research direction.\nFor temporal dependency, the problem is that despite the wide usage of RNN and its  variants16,17 in capturing \ntemporal dependencies, they still suffer from drawbacks such as time-consuming computations, complex gating \nmechanisms, and slow response to dynamic changes. In addition, it inconvenient to deal with the long-term \ntemporal dependency by stacking CNN layers whose receptive field size grows linearly with the number of layers \n increasing18,19. The problem faced by the transformer structure is that as the time series grows, the computational \ncomplexity of its internal self-attention mechanism increases significantly, which greatly affects the performance \nof the transformer in handling long sequence data.\nTwo address the above two limitations, we proposed a new framework called Spatial Linear Transformer \nwith Temporal Convolution Network (SLTTCN). With regard to spatial dependency, we proposed a spatial \nlinear transformer network (SLTN) which aims at dynamically capture the spatial dependency by taking the \nreal-time traffic state, connectivity among nodes and traffic flow directions into consideration. The classical \n transformer20 has the disadvantage of high memory and computational complexity. Therefore, to make the SLTN \nworks efficiently, we optimize the original form of self-attention mechanism into a linear form to reduce the \nresource demand of SLTN. With regard to temporal dependency, inspired by the temporal convolution network \n(TCN)21, we capture temporal dependency stack the by dilated casual convolution layers, and the receptive field \nsize grows exponentially with the number of stack layers increasing. In additional, we introduce a bidirectional \nand gate-fusion mechanism to TCN, that is mining the temporal dependency from past to future and from future \nto past, then fuse the two kinds of dependency to express the complex temporal dependency better. The main \ncontributions are summarized as follows:\n• We construct a novel framework named Spatial Linear Transformer with Temporal Convolution Network \n(SLTTCN) to dynamically model the temporal and spatial dependency of traffic flow.\n• We propose the spatial linear transformer network (SLTN) to dynamically capture the spatial dependency, \nand for improving the efficiency of network, we change the structure of SLTN to reduce the memory and \ncomputational complexity.\n• We propose the temporal convolution network with bidirectional and gate fusion mechanism to capture the \ntemporal dependency to take the advantage of steady gradient, parallel computing and simple structure.\nThe structure of the paper is as follows: In section \"Introduction\" we introduce the reason for traffic predic-\ntion, the issues for modeling the temporal and spatial dependency, the related works and the framework of our \nmodel. In section \"Methodology\", we transform the traffic prediction into a formula temporal-spatial graph \nprediction problem. And we describe the framework of SLTTCN, and analyze the composition respectively. \nIn section \"Experiment\", we conduct the experiments of our model on real world traffic states data and make \ncomparation with the other state-of-the-arts. In section \"Conclusion \", we make conclusions about our paper \nand look forward to our further work.\nMethodology\nIn this section, firstly we give the mathematical definition of problem we analyze in this paper. Secondly, we \nintroduce the two main blocks of our framework, spatial transformer network and the temporal convolution \nnetwork. Lastly, we summarize the structure of our framework.\nĂĂ\n8:05 am ma01:9ma50:9ma00:9ma01:8ma00:8\nsnoitciderPsnoitavresbO\nFigure 1.  The observations and predictions for traffic states, where the different color node represent the \ndifferent area and different lines connecting the area represent the traffic states evolving with time.\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\nProblem definition and notations\nA traffic network can be regarded as a graph G = (V, E, A) , where the V ={ v1 ,v2 ,v3 , ...,vn} is the collection of \nall vertices representing the sensors, and the E ={ ei,j} is the collection of all edges representing the connectivity \namong these sensors. The adjacent matrix A ={ aij}∈ RdN ×dN  is the representing the connectivity where the \nA i,j = 1 if e i,j exists otherwise A i,j = 0 .The traffic prediction is a temporal-spatial problem. The aim of this prob-\nlem is to use the XF ={ xt−T +1 ,xt−T +2 ,..., xt}∈ RdN ×dT which is the input features over the past T time slices \nof traffic state at the time t from N sensor, to predict the future traffic states ˆY ={ xt+1 ,xt+2 ,..., xt+T }∈ RdN ×dT , \nit can be formulated as follows:\nwhere the F is the mapping relationship, that is, the model we need to learn.\nSpatial linear transformer network\nIn this section, we propose the spatial linear transformer network, which is composed of two parts: the posi-\ntion embedding and the linear self-attention layer. The position embedding is used to incorporate the position \ninformation into each node information. The linear self-attention layer is used to capture dynamical spatial \ndependencies evolving with the time goes by. The structure is shown in Fig. 2.\nPosition embedding\nGiven the original input information XF ∈ RdN ×dF , the spatial-transformer only apply feed-forward operation \nignoring the spatial position information of each node. In fact, the position information occupies an import \nplace in modeling spatial correlation. Therefore, what we should do first is to obtain the position embedding \nSM ∈ RdN ×dF and merge it into the original input information XF , that is XS = XF + SM .\nThe graph embedding theory  ProNE22 can help us to get the embedding of each node to capture the spatial \nproperties of the graph, and the process is shown in Fig. 3.\nWe define structure-edge-to which can represent a node-context pair. The edge set then forms a node-context \npair set D = E.Given node vi,the −→c j, −→v i ∈ Rd F  respectively represent the embedding of context node v j and \nembedding of current node vi,furthermore the inner product ⃗cT\nj ⃗vi represents the similarity of embedding between \ncontext v j and node vi.Then we define the occurrence probability matrix P ∈ RdN ×dN  where the occurrence prob-\nability of context v j is ˆpi,j = σ\n(−→c T\nj\n−→v i\n)\n,where the σ( ·) is the sigmoid function.\nHere the context only considers directly connected vertices, the loss function is as follows:\n(XF, G) F−→ ˆY\n(1)L =−\n∑\n(i,j)∈D\npi,j · ln ˆpi,j =−\n∑\n(i,j)∈D\npi,j· ln σ( �cT\nj �vi)\nYS\nLinear Self-attention Layer\nQ K V\nXS\nXF SM\nsoftmaxs oftmax\nmerge\nWQ WK WV\nFigure 2.  The structure of spatial linear transformer, where the all notations are concretely shown in later part \nof paper.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\nThe object function can be expressed as sum of log loss over all edges, where pi,j = ai,j/d i,i and d i,i = ∑\nj\na i,j\n.The p i,j represents the normalized weight of (v i, v j) which is also the regarded as the edge e ij normalized in D.\nThere is a trivial solution ( −→c j = −→v i  and ˆp i,j = 1 ) for the above loss function. The trivial solution means \nthat there only exist positive edges but no negative edges, that is, the edge exists between every node which is \nunreasonable. For each observed pair of vertices (v i, v j) , the context v j is also may come from negative samples \nP D,j,so we update the loss function as:\nwhere /afii9838 is the negative sample ratio and the negative sample P D,j with context node v j from set of node-context \npair can be define as:\nwhere α is equals to 1 or 0.7523.\nTo minimize the loss function, the sufficient condition is that let its partial derivative with grad to cT\nj vi equal \nto zero. Therefore, by calculate ∂L\n∂(cT\nj vi) =0 , we can get:\nTherefore, we further define a new similarity matrix M ∈ RdN ×dN,where the element of this matrix is:\nHence, the problem of distributional similarity-based network embedding is transformed to matrix factoriza-\ntion. We use the truncated singular value decomposition (TSVD), that is,M ≈ UM\n∑\nM VT\nM  , Note that the /Sigma1M is \nthe diagonal matrix composed of the largest dF singular values, and the singular values are arranged in descending \norder, the UM ∈ RdN ×dF and VM ∈ RdN ×dF orthogonal matrices of which column vectors are orthogonal and the \neigenvectors corresponding to the selected singular values. Finally, our embedding matrix SM ∈ RdN ×dF can be \nexpressed as SM =UM\n∑1/2\nM  where each row in SM  represents the embedding of corresponding node.\nLinear self-attention layer\nThe linear self-attention layer is used to calculate, for every position, an average of the features of all other \npositions with a weight which is proportional to the similarity score SS ∈ R⊖dN ×dN  between these features. \nFormally, the input sequence XS ∈ RdN ×dF containing position embedding information is projected into three \nfeature matrices, that are called query Q ∈ R⊖dN ×dK , key K ∈ R⊖dN ×dK and value V ∈ RdN ×dV , and the three \ncomputed as follows:\nwhere W Q ∈ RdF×dK , W K ∈ RdF×dK , W V ∈ RdF×dV are the learnable projection matrixes.\nLike computing the similarity between node embedding and context embedding, the similarity score \nSS ∈ RdN ×dN  computed as equation is also the inner product of Q and K after the softmax function respectively.\nConcretely, the S S (i,j) which is the i-th row and j-th column element of SS is as follows:\n(2)L =−\n∑\n(i,j)∈D\n[\npi,j ln σ( �cT\nj �vi)+/afii9838PD,j(−�cT\nj �vi)\n]\n(3)P D,j ∝\n\n �\ni:(i,j)∈D\npi,j\n\n\nα\n=\n\n �\ni:(i,j)∈D\nai,j\ndi,i\n\n\nα\n(4)cT\nj vi = ln pi,j − ln(/afii9838· P D,j)( i,j) ∈ D\n(5)M i,j =\n{\nln pi,j − ln(/afii9838· PD,j) ,(i,j) ∈ D\n0, (i,j)/∈ D\n(6)\nQ = XSW Q\nK = XSW K\nV = XSW V\n(7)SS = QSKS = softmax(Q)softmax(KT)\nInput Graph \nG(V,E,A)\nSimilarity Matrix \nM\nNode \nEmbedding \nMatrix SM\nTSVD\nFigure 3.  the process of ProNE, where we firstly obtain the similarity matrix M from initial graph information, \nand secondly apply the truncated singular value decomposition (TSVD) to M to get the node embedding matrix \nSM.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\nThe softmax function is the exponential normalization function to make Q  and KT non-negative and row- \nnormalized, and the multiplication of them SS is also non-negative and row-normalized. Then the output feature \nYS ∈ RdN ×dT of self-attention layer marked as Attention(Q,K,V) is calculated as follows:\nTo reduce the memory and computation resource demand, we adopt the linear self-attention mechanism \nwhich a kind of efficient self-attention  mechanism24. As is shown in picture, because we apply the softmax func-\ntion for Q and K respectively, we can switch the computation order which is more efficient. In detail, as is shown \nin Fig. 4, we firstly compute softmax\n(\nKT)\nV ∈ RdK ×dV and secondly compute softmax(Q)\n(\nsoftmax(KT)V\n)\n , the \nmemory complexity is O(dN · dK +d2\nK ) and the computational complexity is O(dN · d2\nV ) . Hence the linear self-\nattention mechanism is efficient in reducing resource requirement by change the computation order when \ndK ≪ dN.\nTo stabilize the training process and prevent overfitting, we further adopt the multi-heads attention mecha-\nnism. Specifically, rather than performs a single-head attention operation which computes query, key and value \nonce, the multi-heads attention parallelly projects the X S to the queries, keys and values for n A times of single-\nhead attention operations, then concatenates these outputs and projects again to get the final result. The multi-\nheads attention H is computed as follows:\nwith the i-th head  Hi computed as:\nwhere the H i ∈ RdF ×dk , W C ∈ R(nA·dv)×dV with dK = nA · dk and dV = nA · dv.\nBidirectional temporal convolution network\nThe bidirectional temporal convolution layer is composed of sequential and reverse-sequential temporal convolu-\ntion layers, which is concretely shown in Fig. 5, and both of the two contain three convolution operations, that \nare, the one-dimension full convolution, the casual and anti-casual convolution, and the dilated convolution. \nMore specifically, we define the input features XF ∈ RdN ×dT , where the i-th element X F(i) ∈ RdT  is one dimension \nsequence which contains T time steps. We define the fitter collection FC ǫRdN ×dC , where the j-th filter FC (j) ∈ RdC  \nhas the kernel size dC. Therefore, the casual convolution output YC ∈ RdN ×dT and anti-casual convolution output \nYA ∈ RdN ×dT with YC(i) ∈ RdN  and anti-casual output yA(i) ∈ RdN  at i-th time step t can be formulated as follows:\n(8)SS(i,j)=\ndK∑\nj=1\nexp (Q i,j) · exp (K i,j)\ndK∑\nj=1\nexp (Q i,j) ·\ndN∑\ni=1\nexp (K i,j)\n(9)YS = Attention( Q,K,V) =SSVS\n(10)H = Concat(H 1,H 2, ..., HnA )W C\n(11)H i= Attention(Qi,Ki,Vi)\n(12)\nY C (i) =\ndC −1∑\nj=0\nFC (j) · X F (t−df·j)\nY A(i) =\ndC −1∑\nj=0\nFC (j) · X F (t+df·j)\nӽ\nXS:dN×dF\nQ:dK×dN\nKT:dK×dN\nQKT:dN×dN\nYS:dN×dVV:dN×dV\nӽ\nsoftmax\nTraditional self-attention mechanism\nӽ\nXS:dN×dF\nQ:dK×dN\nKT:dK×dN\nYS:dN×dV\nKTV:dN×dV\nV:dN×dV\nӽ\nsoftmax\nsoftmax\nEfficient self-attention mechanism\nFigure 4.  The comparison of Traditional Self-Mechanism and Efficient Self-Attention Mechanism, where the \ndenotes the matrix multiplication.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\nwhere the d f is the dilation factor.\nThe dilated convolution sampling the input features in fixed interval, and d f controls the ratio of sampling. \nTo capture long-term dependency in exponential growth, we stack the dilation convolution layers with the d f \nin ascending order.\nAfter we getting the output features, the casual convolution output YC ∈ RdN ×dT and anti-casual convolution \noutput YA ∈ RdN ×dT of bidirectional temporal convolution layers, we adopt the gate fusion mechanism to fuse \nthem. The Gate fusion mechanism is formulated as follows:\nwith the gate G computed as:\nwhere W TC ∈ RdN ×dN and W TF ∈ RdN ×dN are weight matrices, bG ∈ RdT is the bias. σ( ·) is the sigmoid func-\ntion.ReLU(·) is the rectified linear unit as the activation function, is the element-wise product operation. The \ngate G fuses the different kinds of input information and control the ratio of them.\nModel structure\nThe model structure of SLTTCN are shown in Fig. 6. It consists of spatial linear transformer block (SLT), bidi-\nrectional temporal convolution block (Bi-TCN) with gate fusion mechanism (GF), the residual block (Res). we \nadopt the fully connected (FC) layer firstly transformed the input historical traffic data XF ∈ RdN ×dT to hidden \nstate H (1)\nF ∈ RdN ×dF and lastly transformed back to the prediction of traffic volume data YP ∈ RdN ×dT . The rea-\nson is that the hidden state H (2)\nF ∈ RdN ×dF in spatial linear transformer block adopts the multi-heads attention \nmechanism, which requires the condition that the dimension dF must be divisible by the number of heads n A. \nWe adopt the residual  block27 for stable and efficiently training. We adopt the mean absolute error (MAE) as the \nloss function to train the model which can be formulated as:\nwhere the θL denotes the all of the parameters in our model. The Y Pi and ˆY i respectively denotes the prediction \nand ground truth at i-th time step.\nExperiment\nDatasets\nWe conducted experiments on two highway data set to validate our model: the PeMSD4 and PeMSD8. The traffic \nflow data processing methods we adopt  from25. Firstly, we exclude some redundant detectors to guarantee the \ndistance between any two adjacent detectors is longer than 3.5 miles. Secondly, we aggregate the raw traffic data \nevery 5 min, which means the traffic data contains 288 timestamps per day. Then, we use the linear interpola -\ntion method to fill the missing data and apply the zero-mean normalization operation x′ = x − mean(x) to the \n(13)YB = G ⊙ ReLU(YC) + (1 − G) ⊙ ReLU(YA)\n(14)G = σ( W TCYTC + W TAYTA + bG)\n(15)L(θL ) = 1\nT\nt+T∑\ni=t+1\n⏐⏐⏐Y Pi − ˆY i\n⏐⏐⏐\nZero padding\nXF(1)XF(2)\nYC(1)YC(2)\nXF(T-2)XF(T-1)XF(T)\nYC(T-2)YC(T-1)YC(T)\ndf = 1\ndf = 2\ndf = 4\nZero padding\nXF(1)XF(2)\nYA(2)YA(1)\nXF(T-2)XF(T-1)XF(T)\nYA(T)YA(T-1)YA(T-2)\ndf = 1\ndf = 2\ndf = 4\nYB(1)YB(2) YB(T-2)YB(T-1)YB(T)\nGate fusion \nmechanism\nvnoclaropmetlaitneuqes-esreveRnoitulovnoclaropmetlaitneuqeS olution\nFigure 5.  The Bi-TCN layer which is composed of sequential temporal convolution and reverse-sequential \ntemporal convolution layers with the convolution kernel size 3 and the exponential growth of dilation factor df.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\ntraffic data t the average equals to zero. Last, we split the 60% data for training, 20% data for validation and 20% \nfor testing.\nMetrics\nWe adopt mean absolute error (MAE), root mean squared error (RMSE) to evaluate our model performance. \nNote that YP ={ yP1 ,yP2 ,... ,yPQ }∈R dQ represents the prediction data, ˆY = {ˆy1 ,ˆy2 ,... ,ˆyQ }∈ RdQ represents \nthe ground truth, where the QN is the size of data set, the MAE and RMSE is calculated as:\nBaselines\nWe adopt these baselines as follows to compare with our models: Historical Average model(HA);  STGCN26; \n DCRNN9;  ASTGCN7;Graph Wave  Net27;  MTGNN28;  AGCRN29 and  PDFormer30.\nSettings\nFor HA model, we adopt the average value of the last 12 time steps to forecast the next time step value. The \nspecific parameters of other models are set according to the original references. For our model, We set the input \nand output dimensions of each attention head in the Spatial Linear Transformer Network to 12, and the num -\nber of attention heads to 4. For the Bidirectional Temporal Convolution Network, we set the input and output \ndimensions of each layer to the number of nodes, the number of layers, and the convolution kernel size to 3.\nPrediction performance comparison with baselines\nWe compare the performance of our model and baselines within one hour prediction which contains 12 time \nslices. The results of 3, 6 and 12 horizon on PeMSD4 and PeMSD8 are shown in Table  1. We can obviously \nobserve from the results that for traditional HA model, its performance is always not ideal because they have \n(16)MAE = 1\nQN\nQN∑\ni=1\n⏐⏐⏐YPi− ˆYi\n⏐⏐⏐\n(17)RMSE =\n√ 1\nQN\nQN∑\ni=1\n(\nYPi− ˆYi\n)2\nXF\nFC Layer\nHF\n(1)\nBi-TCN Layer\nHF\n(2)\nSLT Layer\nFC Layer\nYF\nRes Block\nRes Block\nGF\nFigure 6.  The structure of SLTTCN.\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\nthe limitation on modeling the traffic data with the complex and nonlinear characteristics. In addition, the \nother deep learning models outperform the traditional models due to the ability of modeling the nonlinear data.\nAs shown in Table 1, our method achieves better performance on most metrics on all datasets. This is because, \ncompared to GNN models(STGCN; DCRNN; ASTGCN; GWNet; MTGNN; AGCRN and PDFormer), our \nproposed Space Linear Transformer Network (SLT) uses self-attention mechanism to calculate the attention \nweights of each node. Specifically, the weight of self-looping is determined by the feature vector itself, while the \nweight of interconnection is calculated by the attention score. To balance the weights between self-looping and \ninterconnection, we introduce position encoding in SLT to incorporate distance information between nodes into \nthe calculation of attention weights, thereby capturing spatial relationships between different nodes. The Spatial \nLinear Transformer network (SLT) can consider the relationships between nodes from a global perspective, rather \nthan just local neighboring nodes. Therefore, SLT is able to better capture long-range dependencies, even when \nthe distance between two nodes is large, it can establish effective connections. In addition, the bidirectional TCN \nwe employed excels in modeling temporal dependencies. The prediction pattern in Bi-TCN (Bidirectional Tem-\nporal Convolutional Network) belongs to direct multi-step prediction, avoiding the risk of accuracy degradation \nthat conventional methods may encounter as the time steps increase during the recursive process.\nIn our Spatial Linear Transformer network, we introduce spatial positional encoding to differentiate the posi-\ntional information of different nodes. This allows our model to simultaneously consider the similarity between \nnode features and the distance information between nodes. In contrast, traditional GNN models often rely only \non neighboring node information and have weaker handling of positional relationships between nodes. Lastly, \nSLT has a linear structure that enables parameter sharing. This means that the number of parameters to be trained \nis reduced, which improves computational efficiency of the model.\nIn subsequent chapters, we also demonstrate the advantages of SLTTCN in terms of computational costs.\nAblation study\nTo evaluate the effectiveness of each part in SLTTCN, we conduct ablation studies with 3 variants of our model \nas follows:\n• ·w/o SLT. It removes spatial linear transformer block (SLT).\n• ·w/o TCN. It removes bidirectional temporal convolution block (Bi-TCN) with gate fusion mechanism.\n• ·w/o Res. It removes residual block (Res).\nFigure 7 reveals the importance of each module for our model performance. The SLT and Bi-TCN modules \ncan respectively model the temporal and spatial dependencies, which are crucial for traffic modeling. Addition-\nally, the performance degradation observed when removing the Res module indicates that the residual connec-\ntions we adopted effectively alleviate the problem of gradient vanishing and enhance the network’s capability to \nlearn nonlinear functions.\nModel analysis\nFirstly, In order to better prove the significance of global dynamic spatial dependency (GDSD) mechanism of \nSLTTCN, we compare it with other two attention-based spatial models, the one is the local dynamic spatial \ndependency (LDSD) mechanism which is adopted by GAT, the other is the global static spatial dependency \n(GSSD) mechanism which is proposed by Graph WaveNet. Their performances are also shown in Table  2. We \nconclude that firstly, the GDSD and GSSD outperforms the LDSD due to the global dependency which can gather \nnon-local node information, and the GDSD outperform more than LDSD since its dependency is evolving with \nTable 1.  The result of different methods on PeMSD4 and PeMSD8. Highlight the optimal performance of each \nhorizon in bold.\nModel\nDataset PeMSD4 PeMSD8\nMetrics MAE RMSE MAE RMSE\nHorizon 3 6 12 3 6 12 3 6 12 3 6 12\nHA 37.71 37.71 37.71 58.03 58.03 58.03 34.86 34.86 34.86 51.98 51.98 51.98\nSTGCN 23.43 25.23 31.05 37.42 39.73 46.12 18.92 21.36 26.77 29.35 32.65 37.54\nDCRNN 21.33 24.12 30.07 35.11 36.91 42.97 18.31 19.54 23.42 25.97 29.43 35.73\nGWNet 20.89 23.64 29.83 33.43 34.34 40.87 17.53 18.12 21.33 25.01 26.03 29.43\nMTGNN 19.36 21.21 27.13 30.71 33.07 40.01 17.09 18.43 20.01 23.49 26.01 28.43\nASTGCN 20.83 22.59 26.97 31.51 33.04 39.87 16.75 18.09 20.99 25.57 28.52 32.93\nAGCRN 20.24 21.77 26.82 29.43 31.07 37.85 16.39 17.49 19.95 24.73 27.56 31.84\nPDFormer 19.44 21.11 26.51 28.53 29.98 37.23 16.03 16.94 19.27 23.63 26.92 29.53\nSLTTCN (ours) 19.21 20.65 25.11 28.02 29.61 35.21 15.21 16.74 18.58 23.45 25.87 28.22\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\ntime. And to observe the three dependency performances, we visualize the three dependencies of first 30 nodes \non PeMSD4 data set, and the result is shown in Fig.  8. It is obviously that for GDSD, the spatial dependency \ndistributes on each node and change following the time. For GSSD, the spatial dependency also distributes on \neach node but fixed. For LDSD, the spatial dependency is distributed on partial nodes and sparse, the reason \nis that the spatial dependency is computed between the source node and directly connected object nodes, not \nindirectly connected.\nSecondly, to demonstrate the computational efficiency of our novel attention mechanism-based SLTN and the \nfused gate mechanism-based Bi-TCN, we compared the time costs of the main baseline models with SLTTCN \non the PemsD4 dataset. We record the average consuming time for each epoch of training and the total consum-\ning time of validating. The result is presented in Table 3, we can see that HA is the fastest because of its simplest \nstructure among all models. We can observe that STGCN is efficient during training due to its fully convolution \nstructure during the training process, and the GWNet is slower due to its combination of diffusion graph con-\nvolution and self-adaptive adjacency matrix. DCRNN adopts an iterative approach instead of directly generat-\ning all predictions, which increases training time. Meanwhile, AGCRN has a significantly increased number of \nparameters to learn node-specific patterns, which slows down the computation speed. The ASTGCN is slower \nbecause it combing the spectral graph convolution operation and attention mechanism, and the running time \nis also impacted by the stack of spatial–temporal blocks. The delay module of Pdformer significantly increases \nthe computational cost of the model. Thanks to the linear structure of the spatial transformer and the parallel \ncomputation of TCN, our model is the most efficient.\nConclusion\nIn this paper, we propose a novel deep learning model for traffic flow forecasting called SLTTCN. It can capture \nthe spatial dependency by spatial linear transformer which belongs to dynamical graph convolution operation \nincluding the graph embedding theory for learning embedding of each nodes. It also capture the time depend-\nency by bidirectional temporal convolution which can construct the time relationship among muti-step data \nfrom past to future and vice versa. The performance and computational efficiency of SLTTCN on PeMSD4 and \nPeMSD8 data set outperforms all the baselines we chose. Furthermore, we will explore more methods for traffic \nforecasting. For data set and information, there are more external factors which also influence the traffic flow, \nsuch as weather, social events and traffic policy and so on. For model structure, we consider mining the edges \nrelationship of traffic network and combing with the nodes features domain in our future study.\nMAER MSE\n5\n10\n15\n20\n25\n30\n35\n20.82\n29.75\n22.56\n33.47\n20.07\n30.38\n18.43\n28.66\nPemsD4\nw/o SLT\nw/o TCN\nw/o Res\n SLTTCN\nMAER MSE\n5\n10\n15\n20\n25\n30\n17.32\n25.51\n18.56\n27.75\n16.41\n24.83\n15.07\n23.31\nPemsD8\nw/o SLT\nw/o TCN\nw/o Res\n SLTTCN\nFigure 7.  Ablation Study on PEMSD4 and PEMSD8.\nTable 2.  The comparison of different spatial dependency on PeMSD4.\nModel\n15 min 30 min 60 min\nMAE RMSE MAE RMSE MAE RMSE\nSLTTCN (GDSD-TCN) 18.43 29.66 19.42 31.38 21.05 33.88\nGLSD-TCN 18.94 29.93 19.72 31.55 21.52 34.26\nGSSD-TCN 18.56 29.86 19.69 31.45 21.39 34.07\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\nData availability\nThe data that support the findings of this study are openly available at https:// pems. dot. ca. gov/.\nReceived: 2 August 2023; Accepted: 8 February 2024\nReferences\n 1. Ahmed, M. S. Analysis of freeway traffic time series data and their application to incident detection. Equine Vet. Educ. 6, 32–35 \n(1979).\n 2. Williams, B. M. & Hoel, L. A. Modeling and forecasting vehicular traffic flow as a seasonal arima process: Theoretical basis and \nempirical results. J. Transp. Eng. 129, 664–672 (2003).\nFigure 8.  We take first 30 nodes for example in 3rd, 6th and 12th time step on PeMSD4. (a, b, c) Illustration of \nGDSD. (d, e, f) Illustration of GSSD. (g)(h)(i) Illustration of LDSD.\nTable 3.  The comparison computation time on PeMSD4 dataset.\nModel\nComputation Time\nTraining (s/epoch) Inference (s)\nHA 7.76 0.14\nSTGCN 16.47 0.63\nDCRNN 39.69 3.48\nGWNet 37.66 2.31\nASTGCN 47.22 10.03\nAGCRN 34.67 2.65\nPDFormer 121.83 18.02\nSLTTCN 12.07 0.26\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\n 3. Ding, Q. Y ., Wang, X. F ., Zhang, X. Y . & Sun, Z. Q. Forecasting traffic volume with space-time arima model. Adv. Mater. Res.  \n156–157, 979–983. https:// doi. org/ 10. 4028/ www. scien tific. net/ AMR. 156- 157. 979 (2010).\n 4. Bin, Yu., Song, X., Guan, F ., Y ang, Z. & Y ao, B. k-nearest neighbor model for multiple-time-step prediction of short-term traffic \ncondition. J. Transp. Eng. 142, 04016018 (2016).\n 5. Cong, Y ., Wang, J. & Li, X. Traffic flow forecasting by a least squares support vector machine with a fruit fly optimization algorithm. \nProc. Eng. 137, 59–68 (2016).\n 6. Y e, J., Zhao, J., Y e, K. & Xu, K. How to build a graph-based deep learning architecture in traffic domain: A survey. IEEE Trans. \nIntell. Transp. Syst. (2020).\n 7. Guo, S., Lin, Y ., Feng, N., Song, C. & Wan, H. Attention based spatial temporal graph convolutional networks for traffic flow \nforecasting. in AAAI, 922–929 (2019).\n 8. Ge, L., Li, H., Liu, J. & Zhou, A. Temporal graph convolutional networks for traffic speed prediction considering external factors. \nMDM, 234–242 (2019).\n 9. Li, Y ., Yu, R., Shahabi, C. & Liu, Y . Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. in International \nConference on Learning Representations (ICLR’18) (2018).\n 10. Fang, Y ., Jiang, J. & He, Y . Traffic speed prediction based on LSTM-graph attention network (L-GAT). in 2021 4th International \nConference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE), Changsha, China, 788–793 (2021).\n 11. Brauwers, G. & Frasincar, F . A general survey on attention mechanisms in deep learning. IEEE Trans. Knowl. Data Eng. 35(4), \n3279–3298 (2023).\n 12. Zhang, Z. & Jiao, X. A deep network with analogous self-attention for short-term traffic flow prediction. IET Intel. Transport Syst. \n15(7), 902–915 (2021).\n 13. Zhang, H., Zou, Y ., Y ang, X. & Y ang, H. A temporal fusion transformer for short-term freeway traffic speed multistep prediction. \nNeurocomputing 500, 329–340 (2022).\n 14. Zhang, C., Yu, J. J. Q. & Liu, Y . Spatial-temporal graph attention networks: A deep learning approach for traffic forecasting. IEEE \nAccess 7, 166246–166256. https:// doi. org/ 10. 1109/ ACCESS. 2019. 29538 88 (2019).\n 15. Zheng, C., Fan, X., Wang, C. & Qi, J. Gman: A graph multi-attention network for traffic prediction. in Proceedings of the AAAI \nConference on Artificial Intelligence, 34, 1234–1241( 2020).\n 16. Schmidhuber, J. & Hochreiter, S. Long short-term memory. Neural Comput. 9, 1735–1780 (1997).\n 17. Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F ., Schwenk, H. & Bengio, Y . Learning phrase representations using rnn \nencoder-decoder for statistical machine translation. in Conference on Empirical Methods in Natural Language Processing (EMNLP \n2014) (2014).\n 18. Ren, H., Kang, J. & Zhang, K. Spatio-temporal graph-TCN neural network for traffic flow prediction. in 2022 19th International \nComputer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP), Chengdu, China, 1–4 (2022).\n 19. Gao, H., Jia, H., Y ang, L. & Li, R. An improved CEEMDAN-FE-TCN model for highway traffic flow prediction. J. Adv. Transp.  \n2022, 1–20 (2022).\n 20. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. & Polosukhin, I. Attention is all you need. in \nNIPS (2017).\n 21. Bai, S., Kolter, J. Z. & Koltun, V . An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. \narXiv: 1803. 01271 (2018).\n 22. Zhang, J., Dong, Y ., Wang, Y ., Tang, J. & Ding, M. Prone: Fast and scalable network representation learning. IJCAI 19, 4278–4284 \n(2019).\n 23. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J. Distributed representations of words and phrases and their composi-\ntionality. in Proceedings of the 26th International Conference on Neural Information Processing Systems 2, 3111–3119 (2013).\n 24. Shen, Z., Zhang, M., Zhao, H., Yi, S. & Li, H. Efficient attention: Attention with linear complexities. in Proceedings of the IEEE/\nCVF Winter Conference on Applications of Computer Vision, 3531–3539 (2021).\n 25. Guo, S., Lin, Y ., Feng, N., Song, C. & Wan, H. Attention based spatial-temporal graph convolutional networks for traffic flow \nforecasting. in Proceedings of the AAAI Conference on Artificial Intelligence, 33, 922–929 (2019).\n 26. Yu, B., Yin, H. & Zhu, Z. Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting. in \nProceedings of the 27th International Joint Conference on Artificial Intelligence, 3634–3640 (2018).\n 27. Wu, Z., Pan, S., Long, G., Jiang, J. & Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. in The 28th International \nJoint Conference on Artificial Intelligence (IJCAI). International Joint Conferences on Artificial Intelligence Organization (2019).\n 28. Wu, Z., Pan, S., Long, G., Jiang, J., Chang, X. & Zhang, C. Connecting the dots: Multivariate time series forecasting with graph \nneural networks. in Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 753–763 \n(2020).\n 29. Bai, L., Y ao, L., Li, C., Wang, X. & Wang, C. Adaptive graph convolutional recurrent network for traffic forecasting. Adv. Neural \nInf. Proc. Syst. 33(2020), 17804–17815 (2020).\n 30. Jiang, J., Han, C., Zhao, W . X. & Wang, J. PDFormer: Propagation delay-aware dynamic long-range transformer for traffic flow \nprediction. in AAAI. AAAI Press (2023).\nAcknowledgements\nThis work was supported by 2021 Liaoning Provincial Education Department Surface Project LJKZ0588.\nAuthor contributions\nZ.X. proposed the methodology, and drafted the initial manuscript. M.H. conceptualized the experimental \napproach. W .L. conducted data organization, analysis, and participated in the initial manuscript writing. D.P . \ncreated figures and tables. M.H., W .L., Z.X., and D.P . collaborated on code design and conclusion analysis. All \nauthors critically revised the paper and approved the final version.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to M.H.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:4040  | https://doi.org/10.1038/s41598-024-54114-9\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7573340535163879
    },
    {
      "name": "Temporal resolution",
      "score": 0.6029706001281738
    },
    {
      "name": "Data mining",
      "score": 0.5611631274223328
    },
    {
      "name": "Spatial analysis",
      "score": 0.49122875928878784
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.4565737545490265
    },
    {
      "name": "Real-time computing",
      "score": 0.45638683438301086
    },
    {
      "name": "Transformer",
      "score": 0.4423264265060425
    },
    {
      "name": "Algorithm",
      "score": 0.3792037069797516
    },
    {
      "name": "Simulation",
      "score": 0.32148683071136475
    },
    {
      "name": "Artificial intelligence",
      "score": 0.21330896019935608
    },
    {
      "name": "Artificial neural network",
      "score": 0.12558376789093018
    },
    {
      "name": "Mathematics",
      "score": 0.11129343509674072
    },
    {
      "name": "Statistics",
      "score": 0.10283103585243225
    },
    {
      "name": "Engineering",
      "score": 0.09124591946601868
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}