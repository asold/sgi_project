{
  "title": "RiNALMo: general-purpose RNA language models can generalize well on structure prediction tasks",
  "url": "https://openalex.org/W4411886797",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2999639884",
      "name": "Rafael Josip Penić",
      "affiliations": [
        "University of Zagreb"
      ]
    },
    {
      "id": "https://openalex.org/A4378867922",
      "name": "Tin Vlašić",
      "affiliations": [
        "Genome Institute of Singapore",
        "Agency for Science, Technology and Research"
      ]
    },
    {
      "id": "https://openalex.org/A2128638662",
      "name": "Roland G. Huber",
      "affiliations": [
        "Bioinformatics Institute",
        "Agency for Science, Technology and Research"
      ]
    },
    {
      "id": "https://openalex.org/A2127282048",
      "name": "Yue Wan",
      "affiliations": [
        "Agency for Science, Technology and Research",
        "Genome Institute of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A4287569683",
      "name": "Mile Šikić",
      "affiliations": [
        "Agency for Science, Technology and Research",
        "Genome Institute of Singapore",
        "University of Zagreb"
      ]
    },
    {
      "id": "https://openalex.org/A2999639884",
      "name": "Rafael Josip Penić",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378867922",
      "name": "Tin Vlašić",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128638662",
      "name": "Roland G. Huber",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127282048",
      "name": "Yue Wan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287569683",
      "name": "Mile Šikić",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W4286669150",
    "https://openalex.org/W4286500588",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2995514860",
    "https://openalex.org/W3035486570",
    "https://openalex.org/W3101509328",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4290672950",
    "https://openalex.org/W4321615987",
    "https://openalex.org/W4386704695",
    "https://openalex.org/W4225858649",
    "https://openalex.org/W4384405439",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3095979265",
    "https://openalex.org/W4327861599",
    "https://openalex.org/W4388539614",
    "https://openalex.org/W4394763992",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W4387103580",
    "https://openalex.org/W4387432134",
    "https://openalex.org/W4393981110",
    "https://openalex.org/W3155806510",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W4301581299",
    "https://openalex.org/W4384648639",
    "https://openalex.org/W2098571862",
    "https://openalex.org/W2102017611",
    "https://openalex.org/W2990528340",
    "https://openalex.org/W3049756659",
    "https://openalex.org/W3212533323",
    "https://openalex.org/W3126773939",
    "https://openalex.org/W2951298881",
    "https://openalex.org/W2130479394",
    "https://openalex.org/W6631828510",
    "https://openalex.org/W2091705944",
    "https://openalex.org/W2109638684",
    "https://openalex.org/W1981576666",
    "https://openalex.org/W2146721220",
    "https://openalex.org/W3100348855",
    "https://openalex.org/W3032032947",
    "https://openalex.org/W4300861274",
    "https://openalex.org/W4283447041",
    "https://openalex.org/W4366990642",
    "https://openalex.org/W2536860838",
    "https://openalex.org/W2933083636",
    "https://openalex.org/W2131054067",
    "https://openalex.org/W2170157200",
    "https://openalex.org/W2158267024",
    "https://openalex.org/W2141494692",
    "https://openalex.org/W2141152740",
    "https://openalex.org/W3217274194",
    "https://openalex.org/W3015796860",
    "https://openalex.org/W2766848373",
    "https://openalex.org/W3103512449",
    "https://openalex.org/W2799254492",
    "https://openalex.org/W2955231772",
    "https://openalex.org/W3014000455",
    "https://openalex.org/W3180908419",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6966984731",
    "https://openalex.org/W4402749070",
    "https://openalex.org/W6772381481",
    "https://openalex.org/W2119896557",
    "https://openalex.org/W2148419405",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W4231041617",
    "https://openalex.org/W4308834638",
    "https://openalex.org/W4307843054",
    "https://openalex.org/W6846337841",
    "https://openalex.org/W6930704337"
  ],
  "abstract": "While RNA has recently been recognized as an interesting small-molecule drug target, many challenges remain to be addressed before we take full advantage of it. This emphasizes the necessity to improve our understanding of its structures and functions. Over the years, sequencing technologies have produced an enormous amount of unlabeled RNA data, which hides a huge potential. Motivated by the successes of protein language models, we introduce RiboNucleic Acid Language Model (RiNALMo) to unveil the hidden code of RNA. RiNALMo is the largest RNA language model to date, with 650M parameters pre-trained on 36M non-coding RNA sequences from several databases. It can extract hidden knowledge and capture the underlying structure information implicitly embedded within the RNA sequences. RiNALMo achieves state-of-the-art results on several downstream tasks. Notably, we show that its generalization capabilities overcome the inability of other deep learning methods for secondary structure prediction to generalize on unseen RNA families.",
  "full_text": "Article https://doi.org/10.1038/s41467-025-60872-5\nRiNALMo: general-purpose RNA language\nmodels can generalize well on structure\nprediction tasks\nRafael Josip Penić1,T i nV l ašić 2,R o l a n dG .H u b e r3,Y u eW a n2 &\nMile Šikić 1,2\nWhile RNA has recently been recognized as an interesting small-molecule drug\ntarget, many challenges remain to be addressed before we take full advantage\nof it. This emphasizes the necessityto improve our understanding of its\nstructures and functions. Over the years, sequencing technologies have pro-\nd u c e da ne n o r m o u sa m o u n to fu n l a beled RNA data, which hides a huge\npotential. Motivated by the successes of protein language models, we intro-\nduce RiboNucleic Acid Language Model(RiNALMo) to unveil the hidden code\nof RNA. RiNALMo is the largest RNA language model to date, with 650M\nparameters pre-trained on 36M non-coding RNA sequences from several\ndatabases. It can extract hidden knowledge and capture the underlying\nstructure information implicitly embedded within the RNA sequences.\nRiNALMo achieves state-of-the-art results on several downstream tasks.\nNotably, we show that its generalization capabilities overcome the inability of\nother deep learning methods for secondary structure prediction to generalize\non unseen RNA families.\nLarge language models (LLMs) trained on massive text corpora have\nbeen performing remarkably on various natural language under-\nstanding and generation tasks\n1–6. In recent years, the exploration of\nlanguage models (LMs) has gone beyond the domain of natural\nlanguage processing (NLP), reaching into the realms of biology and\nits data. A vast amount of sequenced protein data provided a\nground for training protein LMs, and since they have proven to be\nan extremely valuable asset in protein generative\n7–9 and structure\nprediction tasks10,11.\nMost efforts in applying the ideas originally developed for NLP\nhave been focused on proteins after the success of AlphaFold12 in the\nprediction of protein structures. ESM-1b13 was one of theﬁrst models\nthat applied the self-supervised language modeling approaches to\nprotein data. It was pre-trained on 250M protein sequences and\ntested on several downstream tasks, including the secondary struc-\nture and tertiary contact prediction, where it achieved state-of-the-\nart results. Later on, several other protein LMs were proposed and\ntested on various downstream tasks\n14–17. Protein LMs play an impor-\ntant role in protein tertiary structure prediction. ESM-2 11 and\nOmegaPLM10 are examples of protein LMs that efﬁciently replace a\nmultiple sequence alignment (MSA) step in deep learning (DL)\nmethods for structure prediction.\nRNAs play crucial roles in fundamental biological processes,\nincluding transcription, cell signaling, chromatin remodeling, and\ngenome imprinting. Like proteins, RNAs have recently become an\nattractive drug target, whose function and interaction with other\nmolecules are closely related to their structure\n18,19. However, much less\nattention has been given to applying LMs to RNA-related problems,\npartly because there is no such amount of available data and corre-\nsponding structures, and partly because similar problems tend to be\nmore difﬁcult than for proteins. Furthermore, RNA structure predic-\ntion is severely hindered by the scarcity of high-resolution structural\nReceived: 21 October 2024\nAccepted: 6 June 2025\nCheck for updates\n1Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia.2Genome Institute of Singapore (GIS), Agency for Science Tech-\nnology and Research (A*STAR), Singapore, Republic of Singapore.3Bioinformatics Institute (BII), Agency for Science, Technology and Research (A*STAR),\nSingapore, Republic of Singapore. e-mail: mile_sikic@gis.a-star.edu.sg\nNature Communications|         (2025) 16:5671 1\n1234567890():,;\n1234567890():,;\ndata and the lack of unbiased, diverse sequence alignments, which are\noften used as a source of evolutionary information20.\nCurrently, there are only two single-input-sequence RNA foun-\ndation models, RNA-FM21 and Uni-RNA22, that have found applications\nin several structure and function prediction tasks. RNA-FM is a 100M\nparameters Transformer encoder based on the original implementa-\ntion by ref.23 and trained exclusively on 23.7M non-coding RNAs\n(ncRNAs) from the RNAcentral database\n24,22 pre-trained an ensemble of\nLMs ranging from 25M to 400M parameters trained on a much larger\ndataset of 1B sequences with the architecture analogous to the ESM\nprotein LM\n13 enhanced by several advanced techniques such as RoPE\nand fused layer norm. The authors pre-trained language models of\ndifferent sizes and reported when the model parameters exceeded\n400M, the performance in downstream tasks reached a plateau with\ntheir architectures and datasets. Both foundation models used the\nstandard BERT-style masked language modeling (MLM) pre-training\ntask\n1. Unlike the Uni-RNA, RNA-FM is publicly available.\nBesides the RNA-FM and Uni-RNA foundation models, several\nauthors proposed LMs to solve a few speciﬁcd o w n s t r e a mt a s k s .R N A -\nMSM25, an MSA-based BERT-style RNA LM, specialized in particular for\nsecondary structure prediction, that instead of a single input sequence\nutilizes a set of homologous sequences. However, obtaining the MSAs\nis a very time-consuming procedure—it takes RNAcmap, a homology\nsearch tool used by RNA-MSM, on ave r a g e9ht oo b t a i na nM S Af o ro n e\nRNA sequence of length 60\n25,26 proposed SpliceBERT, a BERT-style\nencoder pre-trained exclusively on more than 2M precursor messenger\nRNA (pre-mRNA) sequences from different vertebrates for studying\nRNA splicing. SpliceBERT outperforms DNABERT27, an LM trained only\non a human genome, both on human and non-human splice-site pre-\ndiction tasks. It demonstrates better generalization capability of LMs\npre-trained on multiple species\n28. Proposed single-cell BERT (scBERT), a\nBERT-style LM pre-trained on huge amounts of unlabeled single-cell\nRNA-seq data for cell type annotation. BigRNA29 is an LM pre-trained on\nthe genomes of 70 individuals on atask to predict DNA-matched RNA-\nseq data. BigRNA accurately predicts tissue-speciﬁc RNA expression\nand the binding sites of proteins and microRNAs. UTR-LM30 is an RNA\nlanguage model for 5′ untranslated region (5′ UTR) of mRNAs, which\nwas pre-trained on endogenous 5′ UTRs from multiple species. It is\nspecialized for mRNA translation-related downstream tasks such as\nmRNA translation efﬁciency and expression level prediction.\nMotivated by the recent successes of protein LMs and the latest\narchitectural improvements in LLMs, we propose RiNALMo, a novel\nRNA language model. We pre-trained RiNALMo on a set of carefully\ncurated 36M ncRNA sequences from the RNAcentral database aug-\nmented by several other RNA databases. RiNALMo is a 650M para-\nmeters BERT-style Transformer encoder advanced by modern\narchitectural techniques such as rotary positional embedding (RoPE)\n31,\nSwiGLU activation function32, and FlashAttention-233. During pre-\ntraining, RiNALMo can extract hidden knowledge and capture the\nunderlying structural information embedded within the sequences at\nthe single-nucleotide level. Later, its output embeddings serve as a\npowerful sequence representation that improves the performance on\nvarious structural and functional RNA downstream tasks compared to\nother foundation models and state-of-the-art methods. In particular,\nRiNALMo shows remarkable generalization capability on secondary\nstructure prediction of RNA families not encountered in the training\ndataset where other DL methods fail.\nThe main contributions of the paper are as follows:\n• We propose RiNALMo, a 650M parameters RNA LM, which is the\nlargest RNA language model to date that can fully leverage the\npotential of a vast amount of public unannotated RNA sequences;\n• We show that the generalization capability of RiNALMo can\novercome the problem of other DL methods for secondary\nstructure prediction to perform well on RNA families not seen in\nthe training dataset;\n• We conducted extensive experiments on several RNA structural\nand functional downstream tasks whose results show that\nRiNALMo outperforms other RNA LMs and DL methods on most\ndatasets.\n• We release the pre-trained andﬁne-tuned RiNALMo weights and\nscripts forﬁne-tuning the model for the downstream tasks.\nResults\nGeneral-purpose RNA language model\nA schematic diagram of RiNALMo and its pre-training procedure and\ndownstream tasks is shown in Fig. 1. Our LM is a Transformer\nencoder focused on understanding and unveiling the RNA code. At\nthe heart of the model is the self-attention mechanism\n23, which\ncaptures important local and global contextual information. We pre-\ntrained RiNALMo using the MLM, where we tasked the model to\nreconstruct corrupted, unlabeled RNA sequences. In this paper,\neach nucleotide is a single token. To corrupt the input sequence, we\nrandomly mask 15% of the tokens in the training sequence. To\nreconstruct the masked tokens, RiNALMo’s embeddings are utilized\nby the MLM prediction head whose outputs are used in the cross-\nentropy loss function. More technical details, pretraining details,\nand ablation study are given in “Methods” and Supplementary\nInformation.\nOnce pre-trained, RiNALMo’s output embeddings can serve as a\npowerful sequence representation that has embedded structural\nand evolutionary information. First, its embeddings can be used for\nvisualization and clustering analysis of RNA sequences. Second,\nsuch a representation can be used as an enriched input to structural\nand functional downstream tasks. We employed RiNALMo in a few\ntasks to assess its performance and generalization capabilities.\nNamely, we show how RiNALMo can improve and generalize well on\nsecondary structure, multi-species splice-site, translation efﬁciency\n(TE), expression level (EL), and mean ribosome loading (MRL) pre-\ndiction tasks as well as for multi-class ncRNA family classiﬁcation\ntasks. However, we anticipate it can be leveraged in many other tasks\nrelated to RNA structure and function. Particularly interesting\nwould be the employment of RiNALMo in RNA tertiary structure\nprediction tasks, where, motivated by the results from ESMFold\n11\nand OmegaFold10, we believe RiNALMo’s embeddings can success-\nfully replace the MSA.\nTo analyze the interpretability of our model, we visualized pre-\ntrained RiNALMo’s sequence representations by applying t-SNE on\nthe classiﬁcation token embeddings for RNAs from a secondary\nstructure prediction dataset and an ncRNA functional family clas-\nsiﬁcation dataset (see Fig. 2). RNA structures and functions vary\nacross different RNA families, and we expect RiNALMo has learned\nthese properties during the MLM pre-training and is able to encode\nthem within its RNA sequence representations. We compared the\nclassiﬁcation token embeddings of RiNALMo and RNA-FM\n21. Con-\ntrary to the RNA-FM’s embedding space, in the RiNALMo’s embed-\nding space, the RNAs are clustered by families with, in general, clean\nboundaries between clusters. This is especially evident when com-\nparing Fig.2c and Fig.2d, which illustrate the embeddings of RNAs\nfrom the ncRNA functional family classiﬁcation dataset. The ana-\nlyses of embeddings revealed that RNAs with similar structure and\nfunction properties are grouped, implicating that RiNALMo has\nlearned these properties beyond their primary structure. Further-\nmore, the t-SNE visualization shows RiNALMo’s ability to cluster and\ndistinguish different RNA families and conﬁrms it can be used in\nvarious clustering analyses of RNA sequences. Later in“Results”,\nRiNALMo’s ability to reason beyond RNA primary structure was\nadditionally backed by the state-of-the-art performance on an\nncRNA Rfam family classi ﬁcation downstream task. The t-SNE\nvisualization is important from the RNA structure perspective as\nwell, since the RNAs from the same families fold similarly, meaning\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 2\nthe structure information is also contained in the embedding space.\nThis was later backed up by the RiNALMo’s state-of-the-art perfor-\nmance on the inter-family generalization secondary structure pre-\ndiction downstream task.\nFine-tuning RiNALMo for intra-family secondary structure\nprediction\nWhen RNAs fold into complex structures, many of their bases pair up\nand form hydrogen bonds. These pairs are vital for the structure’s\nstability and function. These bonds can be represented by secondary\nstructure, which can tell us a lot about RNA and which is often used as\nan input to the tertiary structure prediction tools. An example of a\nsecondary structure can be seen in Fig.3a.\nMost popular secondary structure prediction tools often rely on\nthermodynamic models, aiming to identify secondary structures that\npossess the lowest free energy\n34. There are also popular probabilistic\nmethods based on statistical learning procedures that act as an alter-\nnative to free energy minimization methods, such as CONTRAfold35.\nSeveral DL methods have been developed as well. They often outper-\nform the thermodynamic models on RNA families on which they were\ntrained, i.e., on in-distribution data.\nWe ﬁne-tuned RiNALMo on a simple binary classiﬁcation task with\nbinary cross-entropy loss, where we tasked the model to classify each\nnucleotide pair as either paired or unpaired. The pipeline for deter-\nmining secondary structures is illustrated in Fig.3b. We utilized a\ndataset proposed in ref.36 a n dc o m p a r e do u rm o d e lt oR N A - F M\n21 and\npopular DL methods specialized for secondary structure prediction\nSPOT-RNA\n36,U F o l d37, and MXfold238. The proposed dataset is derived\nfrom the bpRNA database39 which compiled secondary structures from\nseven different sources. Most structures were obtained with com-\nparative sequence analysis while a smaller portion was extracted from\natomic coordinates from PDB\n40 using the annotation tool RNAView41.\nAuthors ﬁltered out redundant RNAs by clustering similar sequences\nwhich yielded 13,419 non-redundant secondary structures which were\nthen randomly split into training, validation and testing datasets\ndenoted as TR0, VL0 and TS0, respectively. All models were trained on\nthe same training dataset (TR0) except SPOT-RNA which was addi-\ntionallyﬁne-tuned on a smaller dataset derived from PDB\n40.A sc a nb e\nseen in Fig. 3c, RiNALMo outperforms other state-of-the-art DL\napproaches in terms of precision, recall and consequently F1 score. We\nprovide F1 score distributions in Fig.3d. A TS0 target example and the\npredictions from different DL methods are given in Fig.3g.\nBeyond sequence similarity, it is important to consider structure\nsimilarity and evaluate the ability of structure prediction tools to gen-\neralize to RNAs structurally dissimilar to ones found in the training\ndatasets. Therefore, RiNALMo was further evaluated using the datasets\nTrainSetA and TestSetB proposed by Rivas et al.\n42. TrainSetA consists of\nRNAs collected from datasets proposed in several different studies35,43,44.\nTo ensure sequence diversity, similar sequences were removed, leaving a\ntotal of 3166 RNAs in the dataset. TestSetB contains RNAs from 22 Rfam\n45\nfamilies with known structures not found in TrainSetA, making it struc-\nturally dissimilar. RNAs from these families are thenﬁltered by removing\nsimilar sequences, resulting in 430 RNAs in the TestSetB. The model was\nﬁrst ﬁne-tuned with RNAs from the TrainSetA dataset. All RNAs in this\ndataset longer than 500 nucleotides were used for validation. Once\ntrained, the model was evaluated on the TestSetB. RiNALMo’sp e r f o r -\nmance on the TestSetB dataset was compared to RNAstructure\n34,\nContraFold46,M X f o l d 238,a n dR N A - F M21.A ss h o w ni nF i g .3e, f, RiNALMo\noutperforms other tools in terms of F1 score.\nRNA language models can generalize well on inter-family\nstructure prediction tasks\nWhile DL methods for secondary structure prediction outperform\nthermodynamic models on in-distribution data, they are usually\nunable to generalize well on new RNA families47,48.T h i si sas e v e r e\nlimitation as it hinders the practical usage of such tools.\nTo test the generalization capabilities of RiNALMo, we utilized the\nbenchmark proposed by ref.47. The benchmark utilizes the ArchiveII\nPre-training\ndatabase RNA sequences\nRNAcentral\nRfam nt\nEnsembl\nANACAUAUUACCG...\nAACUACGGCUACA...\nACCAAACGCNUAA...\nCUGUAAAAGGCUN...\nCUGUAAAAGGCUN...\nMasking\nCUG AACAG CUN...GU\nRiNALMo\nN Transformer Blocks \nTokenizer\nInput\nEmbedding\nMulti-Head\nAttention\nLayerNorm\nFeed Forward\nNetwork\nLayerNorm\nPre-training\nDownstream tasks\nMLM\nPrediction Head\nSecondary structure prediction\nOutput Embeddings\nPrediction Head\n2D ResNet\nSplice-site prediction\nexon2exon1exon1 exon3 exon4\nmRNA\n5' 3'\nCLS Token Embedding\nPrediction Head\n2-layer MLP\nMRL, TE & EL predicition\nOutput Embeddings\nPrediction Head\n1D ResNet\nmRNA\n5' 3'\nstart\nstop\nribosome\npolypeptide\nncRNA classiﬁcation\nCLS Token Embedding\nPrediction Head\n2-layer MLP\nFig. 1 | RiNALMo pre-training and applications.In the pre-training stage,\nRiNALMo is trained on unlabeled RNA sequences from several databases using\nmasked language modeling (MLM). To corrupt the input sequence, we randomly\nmask 15% of the tokens in the training sequence. Before being passed to the\nTransformer, an RNA sequence is tokenized and turned into a 1280-dimensional\nvector using a learned input embedding module. The language model comprises 33\nTransformer blocks. Each Transformer block consists of a multi-head attention and\na feed-forward network. Once pre-trained, RiNALMo can be separatelyﬁne-tuned\nfor various structural and functional downstream tasks in which its expressive\noutput embeddings, utilized by the prediction heads, signiﬁcantly improve per-\nformance. In this work, weﬁne-tuned RiNALMo for secondary structure, multi-\nspecies splice-site, mean ribosome loading (MRL), translation efﬁciency (TE), and\nexpression level (EL) prediction, as well as for ncRNA family classiﬁcation.\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 3\ndataset49,50 of 3865 RNAs from nine families which was split nine times,\nand in each split, a different family was held out for evaluation while\nthe other eight families were used for training and validation. RiNAL-\nMo’s ability to generalize across different RNA families was compared\nto RNA-FM\n21, popular thermodynamics-based tool RNAstructure34,\nwidely used probabilistic method CONTRAfold35,a n dt w oD La p p r o a -\nches specialized for secondary structure prediction UFold37 and\nMXFold238.T h eL M sw e r eﬁne-tuned and the other two DL models\nab\nd\n−20−40−60−80\n−40\n−60\n−80\n−40\n−60\n−20\n0\n20\n40\n60\n0\n−25\n−50\n25\n50\n75\n100\n−75\n−100\n0−25−50 25 50 75 100−75 125\n0\n−50\n50\n100\n−100\n0−50 50 100−100\n02 0\n−20\n0\n20\n40 60\n40\n60\n−20−40−60−80 0 20 40 6080\nc\nFig. 2 | t-SNE visualizations of RNA sequence embeddings outputted by RNA-\nFM21 and RiNALMo.RNA-FM (a)a n dR i N A L M o(b) classiﬁcation token\nembeddings for the inter-family generalization evaluation dataset. RNA-FM\n(c)a n dR i N A L M o(d) classiﬁcation token embeddings for one part of the\nncRNA functional family classiﬁcation task dataset.\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 4\nFig. 3 | Secondary structure prediction. aRNAs fold into various shapes according\nto their function and while doing so, many of their nucleotides pair up using a\nhydrogen bond. These pairings are crucial for structural stability and form struc-\ntural motifs such as hairpin loops and bulges.b, RiNALMo produces nucleotide\nembeddings for the given RNA sequence. Nucleotide pair embeddings are con-\nstructed by applying outer concatenation to RiNALMo’s outputs. Finally, pair\nrepresentations are fed into the convolutional bottleneck residual neural network\n(ResNet) which produces base pairing probabilities that are then converted into the\nﬁnal secondary structure prediction.c, Precision, recall and F1 performance of\ndifferent deep learning models on the TS0 evaluation dataset.d Distribution of F1\nscores for predictions of different models on the TS0 dataset (sample size\nn =1 3 0 5 ) .e Precision, recall and F1 performance of different structure prediction\ntools on the TestSetB evaluation dataset.f Distribution of F1 scores for predictions\nof different structure prediction tools on the TestSetB dataset (n = 430). Cfold\ndenotes CONTRAFold and RNAstruct denotes RNAstructure.g At a r g e tR N Af r o m\nthe TS0 evaluation dataset and its predictions from different deep learning models.\nIn (c, e), the best result for each metric is shown in bold. In (d, f), Box plots show the\nmedian (center line), 25th and 75th percentiles (bounds of box), whiskers extending\nto the smallest and largest values within 1.5× the interquartile range, and individual\noutliers beyond the whiskers.\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 5\nwere separately trained on each of the previously described dataset\nsplits and evaluated on a corresponding unseen RNA family. For pre-\ndicting the secondary structures using CONTRAfold, we used Eterna-\nFold parameters46 trained on the EternaBench dataset, a set of more\nthan 20,000 RNAs.\nAverage F1 scores and F1 score distributions for different dataset\nsplits are shown in Fig4. Fine-tuned RiNALMo demonstrates that it is\ncapable of inter-family generalization as it outperforms RNAs-\ntructure and CONTRAfold in eight out of nine families by high mar-\ngins, unlike other DL models. To the best of our knowledge, this is\nthe ﬁrst paper to show that LMs can generalize well on inter-family\nsecondary structure prediction, mitigating the limitations of other\nDL methods. We noted, however, that RiNALMo struggles to gen-\neralize on telomerase RNAs, but it achieves the highest F1 score on all\nother families. Visualization of RiNALMo’s sequence embeddings for\nall RNAs in the dataset is presented in Fig.2b. One can notice that\ntelomerase RNAs are clustered together, however, there is no clear\nboundary between them and SRP RNAs. We also noticed that telo-\nmerase RNAs are the longest in the dataset, on average around 25%\nlonger than the second-longest in the dataset. Please refer to Sup-\nplementary Table S2 for the mean lengths and standard deviations of\nthe RNA families in the ArchiveII dataset. Interestingly, UFold per-\nforms best on telomerase RNA, while achieving much worse results\non the other families. We are currently unable to conclude why\nRiNALMo fails on telomerase RNAs, but we will take more focus on\nthis problem in the future. Technical details and more secondary\nstructure prediction results and examples can be found in the Sec-\nondary Structure Prediction section, Supplementary Note 2 and\nSupplementary Fig. S3.\nRiNALMo’s secondary structure prediction generalization capabilities\nwere also compared to homology-based tools CentroidHomfold\n51,\nlocARNA52, and CentroidAlifold53. To identify RNA homologs, we used the\napproach presented in ref.54. Using the Infernal’s cmscan55 tool, weﬁrst\ndetermine which RNA family the target RNA belongs to. Then, from the\nseed alignment of the identiﬁed family, we randomly select up to 19 RNAs\nthat share between 65% and 95% sequence identity with the target\nsequence. The identiﬁed homologs and the target RNA sequence are\nforwarded to CentroidHomfold and locARNA. Besides the structure pre-\ndiction, locARNA also outputs alignment of given RNAs which is then\nforwarded to CentroidAlifold since it requires sequence alignment as an\ninput. It is worth noting that locARNA and CentroidAlifold output con-\nsensus structure prediction, so to obtain the structure prediction of the\ntarget RNA, the consensus structure is mapped onto the target sequence.\nTo ensure a fair comparison, RNAs for which no homologs were found\nhave been excluded from the evaluation. 16S and 23S rRNAs were not\nincluded in the comparison as they are split into independent folding\ndomains, making the identiﬁcation of homologs more challenging than\nfor other families. Except for telomerase RNAs, RiNALMo outperformed\nor showed comparable performance to other tools across all families. A\ndetailed comparison of RiNALMo’s performance with homology-based\nmethods can be found in Fig.5.\nFine-tuning RiNALMo for classiﬁcation tasks\nRiNALMo can also beﬁne-tuned for downstream tasks that determine\nimportant functions of the observed RNA. Splice-site prediction and\ndetermination of the ncRNA family are two important functional tasks\nthat can be cast as classiﬁcation tasks.\nRNA splicing plays an important role in eukaryotic gene expres-\nsion, involving the removal of introns from pre-mRNAs and the ligation\nFig. 4 | Inter-family secondary structure prediction. a, Average F1 scores for\nsecondary structure prediction on the ArchiveII evaluation datasets. The best result\nfor each evaluation dataset in the tables is shown in bold.b, Distribution of\nF1 scores for different methods on the ArchiveII evaluation datasets (sample sizesn:\n1, 283, 918, 557, 462, 454, 74, 67, 35, and 15, respectively). Box plots show the\nmedian (center line), 25th and 75th percentiles (bounds of box), whiskers extending\nto the smallest and largest values within 1.5× the interquartile range, and individual\noutliers beyond the whiskers. Minimum and maximum values are 0 and 1,\nrespectively.\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 6\nof exons to form mature mRNAs (see Fig.6a). Precisely pinpointing\nsplice sites-the donor and acceptor sites that mark the boundaries\nbetween exons and introns, and vice versa-is essential for accurately\npredicting gene structure and location.\nIdentifying the splice sites can be cast as a binary sequence-level\nclassiﬁcation task. A widely used dataset of positive and negative\nsubsets of splice-site sequences was proposed in ref.56. The dataset\nwas constructed by randomly selecting sequences from the exon/\nintron regions of the G3PO+ genomic sequences\n57. The dataset con-\nsists of error-free splice-site sequences from a diverse set of 148\neukaryotic organisms, including humans. The test dataset consists of\nfour different species not seen in the training dataset.\nWe separatelyﬁne-tuned the model,ﬁrst for donor and then for\nacceptor splice-site prediction. The splice-site prediction pipeline using\nRiNALMo embeddings is illustrated in Fig.6b. Finally, we compared our\nmodel’s performance with other RNA LMs RNA-FM\n21 and Uni-RNA22,a n d\nseveral established methods such as Spliceator56 and SpliceBERT26.W e\npresent the results in Fig.6c. Separate results for donor and acceptor\nsplice-site prediction can be found in Supplementary Tables S6 and S7,\nrespectively. Figure6c reports the average value of donor and acceptor\nprediction results. Weﬁne-tuned other models and used the same\nprediction head if they were publicly available and easy toﬁne-tune. Our\nﬁne-tuned model outperforms other models, showing its powerful\ngeneralization properties. Notice that RiNALMo even outperforms\nSpliceBERT, an LLM pre-trained exclusively on pre-mRNA sequences.\nMore details on the splice-site prediction task and the model’sh y p e r -\nparameters can be found in the Multi-Species Splice-Site Prediction\nsection and Supplementary Note 4.\nNon-coding RNAs are RNA molecules that play vital regulatory\nroles in a wide range of biological processes. Among the most abun-\ndant and functionally signiﬁcant ncRNAs are transfer RNAs and\nribosomal RNAs (rRNAs), both playing an important part in protein\nsynthesis, microRNAs which are essential in regulating gene expres-\nsion, small nuclear RNAs involved in the processing and splicing of pre-\nmRNA, etc.\nDetermining the family of ncRNA is a multiclass classiﬁcation\ntask. We utilized RiNALMo to predict short noncoding RNA func-\ntional families from Rfam\n58 using the sequence as an input. The\ndataset and data preprocessing were adopted from ref.59. After\ndata preprocessing, the dataset comprised ncRNAs shorter than 200\nnucleotides arranged in 88 different Rfam families. We assessed the\nclassiﬁcation performance of RiNALMo for the original ncRNA\nsequences, for which we denoted the experiment as 0% boundary\nnoise, and sequences with random nucleotides, equivalent to 100%\nof the sequence length, added at both ends of the original sequence.\nRandom parts maintained the same single-nucleotide and di-\nnucleotide frequencies as the original sequence. The second\nexperimental setup we denoted as 200% boundary noise. This way,\nwe introduced the uncertainty of where the ncRNA sequence starts\nand ends. Adding random nucleotides to the original ncRNA\nsequences was also adopted from ref.59.\nWe ﬁne-tuned RiNALMo separately on datasets with 0% and 200%\nboundary noise. The ncRNA functional family classiﬁcation pipeline\nusing RiNALMo embeddings is illustrated in Fig.6d. We compared\nRiNALMo’s performance against other RNA LMs Uni-RNA\n22 and RNA-\nFM21, as well as against CNN-based methods with different sequence\nrepresentation approaches proposed by ref.59.W ep r e s e n tt h er e s u l t s\nin Fig.6e. Weﬁne-tuned RNA-FM and used the same prediction head as\nfor RiNALMo. Ourﬁne-tuned model outperforms other state-of-the-art\nmodels and shows robustness against boundary noise. This further\nvalidates the ability of RiNALMo to extract evolutionary information.\nMore details on the ncRNA classi ﬁcation task and the model ’s\nFig. 5 | Inter-family secondary structure prediction for RiNALMo and\nhomology-based tools. aSecondary structure prediction average F1 scores for the\nArchiveII evaluation datasets. The numbers in brackets next to each RNA family\nname represent the count of RNAs for which at least one homolog was identiﬁed,\nfollowed by the total number of RNAs in that dataset. RNAs for which no homologs\nwere found are ignored. The best result for each evaluation dataset in the tables is\nshown in bold.b Distribution of secondary structure prediction F1 scores for dif-\nferent tools on the ArchiveII evaluation datasets (sample sizesn: 1278, 738, 510, 456,\n442, 37, and 35, respectively). Box plots show the median (center line), 25th and\n75th percentiles (bounds of box), whiskers extending to the smallest and largest\nvalues within 1.5× the interquartile range, and individual outliers beyond the\nwhiskers. Minimum and maximum values are 0 and 1, respectively.\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 7\nhyperparameters can be found in the Multi-Species Splice-Site Pre-\ndiction section and Supplementary Note 4.\nFine-tuning RiNALMo for mRNA translation downstream tasks\nDue to efﬁciency reasons, cells usually use groups of multiple ribo-\nsomes to translate the mRNA. These groups are called polyribosomes,\nenabling the cell to create multiple proteins from a single mRNA. To\nquantify protein synthesis activity, an MRL metric, deﬁned as the\naverage number of ribosomes that translate the mRNA instructions\ninto polypeptides, has been introduced. Several other metrics describe\nthe translation of mRNA to polypeptides and protein production. The\nmRNA TE quantiﬁes the rate of translation into proteins and the mRNA\nEL indicates the relative abundance of the mRNA transcript in the cell.\nThese quantities are predictive of the 5’ untranslated region (UTR)\nof mRNAs. Figure 7a illustrates the translation of an mRNA to\npolypeptides.\nMRL, TE, and EL prediction tasks can be viewed as regression tasks\nwhere the input is the 50 UTR region of the mRNA. Commonly used\ndatasets of 50 UTR sequences with measured MRL values are provided\nby ref. 60. Two evaluation datasets, namely Random7600 and\nc\nFish\n97.70\n96.70\n96.35\n96.15\n95.10\nModel\nSpliceBERT\nUni-RNA\nRNA-FM\nSpliceator\nRiNALMo\nFT\nᅛ\nᅛ\nᅛ\nFly\n96.11\n95.83\n94.46\n94.80\n94.98\nPlant\n96.25\n94.96\n92.08\n94.90\n93.62\nWorm\n95.63\n94.82\n92.09\n94.10\n93.94\ne\nModel\nUni-RNA\nRNA-FM\n2-mer\nRiNALMo\n3-mer\n1-mer\n0% boundary noise\n0.990\n0.985\n0.915\n0.890\n0.880\n0.870\nFT\nᅛ\nᅛ\n200% boundary noise\n0.987\n0.984\n0.840\n0.810\n0.810\n0.959\nAverage\n0.989\n0.985\n0.865\n0.845\n0.840\n0.937\na b\nRiNALMo\nCLS t oken em beddingCLS token embedding\ninput layer\nhidden layer\n...AAGUGGCUGAUUCUU\nclassiﬁcation\nhead\nAG AAGCCCCAACUAGCA...\noutput layer\nexon1exon1 exon2 exon3intron1 intron2 exon4intron3\n5' 3'\ndonor site acceptor site\n...GCGG GUAAAU... ...UAUU AGAUCG...\nsplicing\nexon2exon1exon1 exon3 exon4\n5' 3'\npre-mRNA\nmRNA\nGGGGUGUCAGGAGUU...\nCLS token embeddingCLS token em bedding\nRiNALMoRiNALMo Classiﬁcation\nhead\nClassiﬁcation\nhead\nd\nFig. 6 | RNA functional classiﬁcation tasks.Splice-site prediction. a A pre-mRNA\ntranscript consists of non-coding, i.e., introns, and coding regions, i.e., exons.\nIntrons are located between two exons of a gene. As part of the RNA processing\npathway, introns are removed by cleavage at splice sites. These sites are found at 50\nand 30 ends of introns, known as donor and acceptor splice sites, respectively. Most\nfrequently, the 50 end of introns begins with the dinucleotide GU, and the 30 end of\nintrons ends with AG.b An input to RiNALMo is a 400-nucleotide-long RNA\nsequence from the GS_1 dataset. We utilize only the CLS embedding that then\npasses through a two-layer MLP classiﬁcation head. The output layer gives infor-\nmation on whether a sequence contains a donor/acceptor site or not.\nc Classiﬁcation F1 score for splice-site prediction. Here, we report the average value\nof donor and acceptor prediction results.ncRNA family classiﬁcation. d Given an\nRNA sequence the goal is to classify its ncRNA family. The procedure is again similar\nto the procedure in (b): Original and noisy RNA sequences from the Rfam dataset\nare input to RiNALMo. We utilize only the CLS embedding that then passes through\na two-layer MLP classiﬁcation head. The output layer determines which of the 88\nRfam families the input ncRNA belongs to.e ncRNA family classiﬁcation accuracy\nfor noiseless and noisy input sequences and the average accuracy. In (c and e), FT\ndenotes whether weﬁne-tuned the model or represented direct citations from the\noriginal papers with the same split train/test datasets. The best result for each\nevaluation dataset is shown in bold.\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 8\nHuman7600, were created by sampling the original dataset containing\nhuman and random UTR sequences. For the TE and EL prediction\ndownstream tasks, we followed the work reported by ref.30.T h r e e\nendogenous human 50 UTR datasets analyzed by ref.61 were used.\nEach dataset originated from a distinct cell line or tissue type: human\nmuscle tissue (Muscle), human prostate cancer cell (PC3), and human\nembryonic kidney 293T (HEK). Each sequence of these three datasets\nprovides measurements of translation efﬁciency and expression level.\nWe separatelyﬁn e - t u n e dR i N A L M ot op r e d i c tt h eM R Lv a l u e s ,T E ,\nand EL for the 50 UTR sequences from datasets. The prediction pipeline\nis illustrated in Fig.7b. RiNALMo outputs are fed into a prediction head\nconsisting of six ResNet\n62 blocks. The mean squared error was used as\nthe loss function for MRL prediction. In TE and EL downstream tasks,\nwe used the Huber loss function.\nFor the MRL downstream task, the model’sp e r f o r m a n c ew a s\ncompared to other RNA LMs Uni-RNA\n22 and RNA-FM21. We also com-\npared our model to the popular Optimus 5-prime model60 specialized\nfor MRL prediction.R2 was used as the evaluation metric, and the\nresults are reported in Fig.7c. Fine-tuned RiNALMo outperforms other\nmodels. Notice that RiNALMo can generalize on human UTRs despite\nbeingﬁne-tuned only on sequences of random origin, again proving its\ngeneralization capability. More details on MRL prediction can be found\nin the Multi-Species Splice-Site Prediction section and Supplemen-\ntary Note 4.\nFor the TE and EL prediction, RiNALMo’s performance was com-\npared to other LMs UTR-LM\n30 and RNA-FM21.U n i - R N A22 was not eval-\nuated on the TE and EL prediction downstream tasks. We also\ncompared RiNALMo to a random forest method proposed in ref.61\nand again to the popular Optimus 5-prime model\n60. The Spearman\ncorrelation coefﬁcient was used as the evaluation metric. In Fig.7da n d\nFig. 7e, we report the average Spearman correlation coefﬁcient across\nten folds for TE and EL tasks, respectively. From the tables, it can be\nseen that theﬁne-tuned RiNALMo outperforms other models on all the\nevaluation datasets. More details on TE and EL prediction can be found\nin the Translation Efﬁciency and Expression Level Prediction section\nand Supplementary Note 4.\nFine-tuning the model for mRNA translation-related downstream\ntasks and the achieved state-of-the-art results demonstrate good\ngeneralization capabilities of RiNALMo to other types of RNAs.\nNamely, RiNALMo was pre-trained on ncRNAs without seeing a single\nmRNA or its UTR parts. Despite this, it outperforms other general-\npurpose RNA language models and UTR-LM which was speciﬁcally pre-\ntrained on 5’ UTR sequences and designed for mRNA translation-\nrelated tasks.\nab\nModel\nRiNALMo\nUni-RNA\nRNA-FM\nOptimus 5-Prime\nRandom7600\n0.92\n0.91\n0.84\n0.84\nHuman7600\n0.86\n0.85\n0.79\n0.78\nFT\nᅛ\nᅛ\nᅛ\nc\nde\n...ACUUUCCGGGCACAGU...\nRiNALMoRiNALMo\nSequence emeddings\n1D ResNet1D ResNet Mean Ribosome\nLoad (MRL)\n5' UTR\nmRNA\n5' 3'\nstart\nstop\nribosome\npolypeptide\nExpression\nLevel\nTranslation\nEfﬁciency\nModel\nRiNALMo\nUTR-LM\nRNA-FM\nMuscle\n0.72\n0.67\n0.62\n0.63\nHEK\n0.66\n0.60\n0.55\n0.55\nPC3\n0.70\n0.65\n0.60\n0.63\nFT\nᅛ\nᅛ\nAVG\n0.69\n0.64\n0.59\n0.60Cao-RF\nOptimus 5-Prime 83.06 3.014.0 0.38\nModel\nRiNALMo\nUTR-LM\nRNA-FM\nMuscle\n0.70\n0.66\n0.59\n0.64\nHEK\n0.69\n0.65\n0.51\n0.57\nPC3\n0.67\n0.63\n0.49\n0.59\nFT\nᅛ\nᅛ\nAVG\n0.69\n0.65\n0.53\n0.60Cao-RF\nOptimus 5-Prime 91.08 1.051.0 0.17\nFig. 7 | Mean ribosome loading, translation efﬁciency, and expression level\nprediction. aTo produce multiple proteins from a single mRNA strand, cells utilize\nmultiple ribosomes. The mean ribosome load metric (MRL), calculated based on a\n5' UTR sequence, measures the number of ribosomes translating an mRNA at a\ngiven time. Further, 5' UTR is also predictive of the mRNA translation efﬁciency (TE)\nthat quantiﬁes the rate of translation into proteins and the mRNA expression level\n(EL) that reﬂects the relative quantity of the mRNA transcript in the cell.b 5' UTR\nrepresentation produced by RiNALMo is forwarded to the one-dimensional con-\nvolutional ResNet which outputs the MRL, TE, or EL prediction.c, R\n2 score for MRL\nprediction on the Random7600 and Human7600 datasets.d Average Spearman\ncorrelation coefﬁcient across ten folds for TE prediction for human muscle tissue\n(Muscle), human prostate cancer cell (PC3), and human embryonic kidney 293T\n(HEK), and the average across cell lines and tissue types.e Average Spearman\ncorrelation coefﬁcient across ten folds for EL prediction for the same datasets as for\nthe TE downstream task. In (c, d, e) FT denotes whether we trained the model or\nrepresented direct citations from the original papers with the same split train/test\ndatasets. The best result for each evaluation dataset is shown in bold.\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 9\nDiscussion\nWe pre-trained our RNA language model on a vast amount of ncRNA\nsequences from several databases, including RNAcentral, Rfam, nt, and\nEnsembl. The data was carefully curated to ensure sequence diversity\nin each training batch, which subsequently led to better generalization\ncapabilities of the pre-trained language model. We used t-SNE on the\nclassiﬁcation token embeddings to show that RiNALMo’so u t p u t\nrepresentations contain information about the RNA family and that the\nRNAs with similar structures are close in the model’s embedding space.\nA more insightful way of assessing the embeddings’ expressiveness\nand whether the model captures hidden structure information is to\nassess it on downstream tasks.\nFirst, we ﬁne-tuned and tested RiNALMo on two secondary\nstructure prediction tasks. Theﬁrst task was an intra-family secondary\nstructure prediction, where RNAs from the same family came in both\nthe training and test datasets. The results showed thatﬁne-tuned\nRiNALMo’s output representation embeds important structure infor-\nmation about the input sequence and, when utilized with a proper\nprediction head, leads to state-of-the-art performance. The second\ntask was an inter-family secondary structure prediction, where an RNA\nfamily from the test dataset was not seen in the training dataset. It was\nshown previously that deep learning methods do not generalize well\nacross RNA families\n47. However, RiNALMo outperformed both\nthermodynamics-based and deep learning methods and showed that\nRNA language models can generalize well on unseen RNA families. This\ndemonstrates the outstanding generalization capability of RiNALMo\nfor secondary structure prediction tasks.\nFurthermore, we ﬁne-tuned RiNALMo on ﬁve function-related\nRNA downstream tasks. Four of these tasks were related to the pre-\nmRNA or the untranslated region of the mRNA whose examples were\nnot contained in the pre-training dataset. The“Results” section showed\nthat RiNALMo again generalizes well and can capture important\nfunctional information from RNA sequences from previously unseen\nRNA types. RiNALMo outperforms other general-purpose RNA lan-\nguage models and language models trained on RNA types speciﬁcf o r\nthose downstream tasks, such as SpliceBERT\n26 and UTR-LM30.\nIn future work, we will explore the augmentation of the pre-\ntraining dataset by adding coding RNAs and evaluate how it\naffects the model performance on the structural and functional\ndownstream tasks. Having bigger data might require a larger RNA\nLM which we also have in consideration. Multimodal pre-training,\nincluding sequence and chemical mapping data, is another\ndirection for improving the representations for structure-related\ndownstream tasks, which we will explore in the future. Finally, we\nplan to employ RiNALMo in several other structure-related tasks.\nOf particular interest would be testing RiNALMo on tertiary\nstructure prediction tasks and seeing whether its expressive\noutput embeddings and generalization capability can improve the\nperformance of existing or new prediction tools. Another inter-\nesting application would be employing RiNALMo ’s embeddings\nfor sequence conditioning for RNA design\n63,64.\nTo conclude, we presented RiNALMo, a new largest-to-date\ngeneral-purpose RNA LM pre-trained on a dataset of 36M ncRNA\nsequences using MLM. We showed that by pre-training on a carefully\ncurated RNA dataset and using the most modern Transformer tech-\nniques, an LM can capture hidden knowledge and important struc-\ntural information from unlabeled RNA sequences. The results on\ndownstream tasks proved the generalization capability of RiNALMo\nand the expressiveness of its output representations. Of particular\nimportance are the results of the inter-family secondary structure\nprediction task, where we showed that RiNALMo can generalize well\non RNA families unseen duringﬁne-tuning, unlike other DL methods.\nDue to the signiﬁcance of the results, we believe RiNALMo presents a\nvaluable asset for advancing our understanding of RNA structures\nand functions.\nMethods\nRNA language model\nRiNALMo is an encoder-only Transformer. An input RNA sequence is\ntokenized, turned into a 1280-dimensional vector using a learned\ninput embedding model and passed to the Transformer. RiNALMo\nconsists of 33 Transformer blocks, and each block comprises a multi-\nhead attention and a feed-forward network (FFN). The position of the\ntokens is encoded using the RoPE\n31, which effectively encapsulates\nboth the relative and the absolute positional information. Each multi-\nhead attention has 20 attention heads. Multi-head attention is illu-\nstrated in Supplementary Fig. S1. To improve the pre-training efﬁ-\nciency of such a large model, we employed the IO-aware\nFlashAttention-2\n33, a fast and memory-efﬁcient exact attention. In the\nFFN, we use two linear layers and the SwiGLU activation function32\nwhich combines the advantages of the Swish activation function and\nthe gated linear unit (GLU). The FFN layers have hidden size\ndff = 3413, scaling the model to 650M parameters. The Transformer\nmodules are interconnected using residual connections. We use the\nlayer normalization put inside the residual blocks to stabilize the\ntraining and have well-behaved gradients at initialization\n65. We did an\nablation study for RoPE and the SwiGLU activation function which\nshowed the effectiveness of these techniques on the downstream\ntasks compared to the conventional absolute positional encoding\nand GELU activation function used in RNA-FM. The ablation study\nresults are provided in Supplementary Tables S14 and S15.\nTokenization\nIn this work, each nucleotide is a single token. During tokenization, we\nreplace all“U \" si nt h es e q u e n c e sw i t h“T\"s. This leads to a vocabulary\ninvolving standard IUPAC nucleotide codes (\"A\",“C\", “T\", “G\", “R\", “Y\",\n“K\", “M\", “S\", “W\", “B\", “D\", “H\", “V\", “N\", and“-\") and a special token for\ninosine (\"I\"). We use additional tokens commonly used in MLM, such as\n[CLS], [EOS], [PAD],a n d [MASK]. During masking, we change\nnucleotides with the[MASK] token or replace it with one of the four\nmain types of nucleotides from the vocabulary or the“any nucleotide\"\n(\"N\") token. Tokens[CLS] and [EOS] are added at the beginning and\nend of the sequence. The[PAD]token is appended at the end of shorter\nsequences to have all the sequences in a batch of the same length.\nData preprocessing\nFor pre-training dataset preprocessing, we implemented a multi-step\npreparation pipeline. First, we collected noncoding RNA sequences\nfrom publicly available datasets RNAcentral, nt, Rfam and Ensembl.\nWe removed sequences shorter than 16 and longer than 8192. Fur-\nthermore, we also removed sequence duplicates withseqkit rmdup\nand the resulting unique sequences were clustered withmmseqs\neasy-linclust with options--min-seq-id 0.7and -c 0.8. Finally, we saved\nthe processed dataset intoLMDB (Lightning Memory Mapped Data-\nbase) so we could easily and quickly access any data sample during\nthe pre-training. In the end, the dataset consisted of 36M unique\nncRNA sequences clustered into 17M clusters. In comparison, ref.21\ncollected ncRNAs from the RNAcentral database only. As a pre-\nprocessing step, the authors removed only duplicate RNA sequences.\nWe clustered the RNA sequences to ensure sequence diversity in\neach training batch since we later sample from each cluster once\nper epoch.\nPre-training\nWe pre-trained RiNALMo using the MLM task where we corrupted\nunlabeled RNA sequences and thentasked the model to reconstruct\nthem. To corrupt the input sequence, we randomly selected 15% of the\ntokens in the training sequence. Of these, 80% are masked, i.e.,\nr e p l a c e dw i t ht h eu n i q u ev o c a b u l a r yt o k e n[MASK],1 0 %a r er e p l a c e d\nwith a randomly selected token from the vocabulary, and the\nremaining 10% are left intact.\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 10\nLet us denote the corrupted sequence by ~X = f~xig. We train\nRiNALMo parameterized by θ to reconstruct X by predicting the\nmasked tokens conditioned on~X. For a given input token~xi, the loss is\nthe probability of the correct nucleotide, given~X:\nLMLM = /C0 log pθð~xi = xij ~XÞ: ð1Þ\nThe weight updates are based on the average loss over the sampled\ntokens from a single training sequence (or a batch of sequences):\nLMLM = /C0 1\njMj\nX\ni2M\nlog pθð~xi = xij ~XÞ, ð2Þ\nwhere M is the index set of masked tokens.\nDue to the computational limitations, during training, we limited\nthe length of the input into the language model to 1024 tokens. Every\nsequence begins with a[CLS] token, i.e., a classiﬁcation token, and\nends with an[EOS] token, i.e., an end-of-sequence token. If an input\nsequence is longer than 1022 nucleotides, a random part of the\nsequence will be cropped out and fed into the model during each\nepoch, similar to what was done in ref.13. To implicitly give informa-\ntion to the model that the sequence was cropped, it does not begin\nwith the[CLS] token nor ends with the[EOS] token unless we crop\nthe beginning or end of the sequence.\nTo ensure sequence diversity in each training batch during pre-\ntraining, we randomly sampled each sequence in the batch from a\ndifferent sequence cluster. Effectively, it means that in every epoch\nRiNALMo saw 17M samples, i.e., one sequence from each of the clus-\nters, and in each epoch, sequences were sampled randomly from the\nclusters using a new seed.\nWe pre-trained RiNALMo with seven A100 GPUs of 80 GB memory\nfor 2 weeks. The batch size was set to 192 per GPU. We adopted the\ncosine annealing learning rate schedule with a linear warm-up. During\nthe warm-up period, learning rate increases from 10\n−7 to 5 × 10−5 for\n2000 steps. For the cosine annealing schedule, the minimum learning\nrate was set to 10\n−5. The gradient norm was clipped to 1.0.\nWe pre-trained several conﬁgurations of the LM: RiNALMo-650M\nwith 650M parameters, RiNALMo-150M with 148M parameters, and\nRiNALMo-33M with 33.5M parameters. We did several experiments to\nevaluate how the model’s size inﬂuences its performance on the pre-\ntraining and downstream tasks. First, we measured perplexity. Per-\nplexity is a metric used to evaluate how well an LM predicts a sample of\ntext or, in our case, an RNA sequence. The perplexity is deﬁned as the\nexponential of the negative log-likelihood of the sequence. To efﬁ-\nciently calculate the perplexity, we used the following term\nPERPLEXITYðxÞ =e x pf/C0 log p\nθð~xi2M = xij~XÞg: ð3Þ\nLower perplexity means the LM is better at reconstructing the masked\ntokens in the MLM pre-training task.\nIn order to evaluate how the number of parameters affects the\nmodel’s ability to reconstruct the masked tokens, we calculated the\nperplexity of RiNALMo for several different model sizes. Validation\nperplexity was measured on a 1% random-split holdout of the pre-\ntraining dataset. The perplexity values are given in Supplementary\nFig. S2. It can be seen that as we increase the model size, the perplexity\ndecreases, meaning the larger the LM gets, the more capable it is of\nreconstructing the missing tokens.\nThe hyperparameters of RiNALMo models of different sizes are\ngiven in Supplementary Table S1, and the comparison of their per-\nformance on the downstream tasks are given in Supplementary\nTables S8, S9, S10, S11, S12, and S13. Based on these results, we can\nconclude that besides novel techniques such as RoPE and SwiGLU,\nmodel size is important in achieving a boost in performance on pre-\ntraining and consequently downstream tasks.\nSecondary structure prediction\nTo adapt RiNALMo for the secondary structure prediction, weﬁne-\ntuned the model with a simple binary classiﬁcation task with binary\ncross-entropy loss, where we tasked the model to classify each\nnucleotide pair as either paired or unpaired. A simple prediction head\nwas attached to the language model and was fed with nucleotide pair\nrepresentations. The prediction head was a bottleneck residual neural\nnetwork (ResNet) with two convolutional blocks. These representa-\ntions are obtained by applying outer concatenation to RiNALMo’s\noutput. For example, to obtain the vector representation of the\nnucleotide pair (i, j), we simply concatenate the representation of the\nnucleotidej to the representation of the nucleotidei.\nIn the end, our secondary structure prediction pipeline outputs a\nmatrix where each element represents a pairing probability logit for a\ncertain nucleotide pair. Because of the symmetry of secondary struc-\ntures (if nucleotidei is paired with nucleotidej,t h e nj is paired withi as\nwell), we calculate training loss only on matrix elements“above\" the\nmain diagonal.\nDuring ﬁne-tuning, we utilized gradual parameter unfreezing.\nAfter every three epochs, we unfroze an additional three RiNALMo’s\nlayers. In theﬁrst three epochs, we trained only the prediction head.\nThe model wasﬁne-tuned for 15 epochs and the learning rate was\ninitially set to 10\n−4 with a linear decay schedule. We used the same\nprediction head architecture for RNA-FMﬁne-tuning. Due to archi-\ntectural differences between RiNALMo and RNA-FM, we decided to\nmodify theﬁne-tuning schedule. The prediction head wasﬁrst pre-\ntrained while RNA-FM parameters were kept frozen. After three epochs\nwe unfroze RNA-FM andﬁne-tuned it alongside the prediction head.\nTo convert base pairing probabilities to a proper secondary\nstructure, we implemented a simple greedy approach where we\niteratively set nucleotide pairs with the highest pairing probability as\npaired and then excluded all possible clashing pairs (pairs where the\nsame nucleotide is paired with multiple other nucleotides) from being\nset as paired in future iterations of the algorithm. During this proce-\ndure, we ignore non-canonical nucleotide pairings and pairings that\nwould cause a“sharp” hairpin loop (i- t hn u c l e o t i d ec a n n o tb ep a i r e d\nwith thej-th nucleotide if∣i − j∣ < 4). The classiﬁcation threshold was\ntuned on the validation set to ensure a balanced pairing ratio.\nWhen assessing the performance of secondary structure predic-\ntion, it is important to consider RNA structural dynamics, and that’s\nwhy it is helpful to view predictions that are close enough as correct.\nSame as the other methods we compared RiNALMo to, we employed\nthe metric calculation approach proposed in ref.50. To be more pre-\ncise, for a nucleotide pairing (i, j)w h e r ei and j represent nucleotide\nindices in the RNA sequence, (i ±1 ,j)a n d(i, j ± 1) pairings are also\nconsidered correct predictions. To obtain the F1 scores we reported in\nthis study, we calculated the F1 score separately on each structure and\nthen averaged those values. Again, for the sake of fairness, notice that\nwe have reported F1 scores for all other methods and tools using the\nexact same calculation.\nIn the RNA community, it is also customary to use the interaction\nnetwork ﬁdelity (INF) measure\n66, which is tailored for evaluating RNA\nbase pairs. The INF measure provides a more balanced assessment of\nthe predictive algorithm’s performance than the F1 score across both\npositive and negative classes. Therefore, and for the completeness of\nour study, we computed INF scores for the secondary structure pre-\ndiction evaluation datasets, which are provided in Supplementary\nTables S3, S4, and S5. Regarding the INF measure, RiNALMo still out-\nperforms other methods we used for the comparison.\nIn Supplementary Table S2, we report sequence-length weighted\nF1 scores on the inter-family secondary structure prediction task. We\nalso provide the performance of smaller LMs, the ablation study and\nthe impact ofﬁne-tuning in Supplementary Note 4.\nSecondary structure visualizations shown in theﬁgures were\ngenerated usingforna\n67 and RNAstructures’s draw34.\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 11\nMulti-species splice-site prediction\nSplice-site prediction task is essentially a binary sequence-level clas-\nsiﬁcation task, where a method has to detect whether a query RNA\nsequence contains a donor/acceptor site or not. We ﬁne-tuned\nRiNALMo on the training dataset to predict splice sites in the\nsequences. The classiﬁcation token (CLS) was used as input to\nthe classiﬁc a t i o nh e a d .T h i sh e a di sc o nﬁgured as a two-layer\nmultilayer perceptron (MLP) featuring a hidden layer with 128 neu-\nrons and employing the GELU activation function. Cross-entropy loss\nserved as our choice for the loss function. We separatelyﬁne-tuned the\nmodel, ﬁrst for the donor and then for the acceptor splice-site\nprediction task.\nWe used the dataset denoted as GS_1 in the paper, which was\nproposed in ref.56. GS_1 has the same ratio of positive to negative\nsamples, whose negative samples consist of exon, intron, or false-\npositive sequences. The length of the input sequences was set to 400\nnucleotides for both the training and test datasets. The training dataset\nfor the acceptor splice-site prediction consists of 22,178 samples and\nthe training dataset for the donor splice-site prediction comprises\n21,973 samples. Each test dataset for both donor and acceptor splice-\nsite prediction consists of 20,000 samples.\nThe prediction head was uniformly initialized from\nUð/C0\nﬃﬃﬃﬃﬃﬃﬃﬃ\n1=d\np\n,\nﬃﬃﬃﬃﬃﬃﬃﬃ\n1=d\np\nÞ, where d is the input feature dimension. The\nlearning rate was set to 10\n−5 and the language model with the pre-\ndiction head wasﬁne-tuned for two epochs when it achieved the best\nresults on the validation dataset. The batch size was set to 32.\nWe ﬁne-tuned the SpliceBERT model and the RNA-FM model in\ncombination with a two-layer MLP prediction head. Weﬁne-tuned the\nmodels on the same training/validation dataset used forﬁne-tuning\nRiNALMo with the same learning rate.\nThe performance of smaller LMs and the impact ofﬁne-tuning for\nthe splice-site prediction task can be found in Supplementary\nTable S10.\nncRNA family classiﬁcation\nThe ncRNA functional family classiﬁcation downstream task is a\nsequence-level multiclass classiﬁcation task. We separatelyﬁne-tuned\nRiNALMo on the two training datasets. The CLS was used as input to\nthe classiﬁcation head. The head was conﬁgured as a two-layer MLP\nfeaturing a hidden layer with 256 neurons and employing the GELU\nactivation function. Cross-entropy loss served as our choice for the\nloss function.\nThe noiseless and noisy datasetswere constructedf o l l o w i n gt h e\npreprocessing reported by ref.59. Starting from the original Rfam\ndataset\n58, the procedure excludes classes annotated as long ncRNAs and\nwith an average sequence length greater than 200 nucleotides. This\nleads to a dataset with 371,619 sequences among 177 Rfam families.\nFurthermore, the procedure removes families whose clustering was\nhighly correlated with sequence length and families with less than 400\nRNA sequences to ensure data quality. This dataset consisted of\n306,016 sequences distributed among 88 different Rfam classes. Each\nRfam family was randomly split into three subsets: training (84%), vali-\ndation (8%), and test (8%). The procedure ensured that all sequences in\nthe validation and test sets have a similarity, in terms of normalized\nHamming distance, less than 0.5 with any other sequence in the training\nset. This is to limit the potential bias arising from an over-representation\nof highly similar homologous sequences in random splits. Finally, the\ntraining and validation sets were sampled with replacements to address\nthe imbalanced class distribution. Theﬁnal dataset contained 105,864\ntraining, 17,324 validation and 25,342 test sequences.\nThe classi ﬁcation head was uniformly initialized from\nUð/C0\nﬃﬃﬃﬃﬃﬃﬃﬃ\n1=d\np\n,\nﬃﬃﬃﬃﬃﬃﬃﬃ\n1=d\np\nÞ,w h e r ed is the input features dimension. We trained\nthe model using an AdamW optimizer with a weight decay of 0.01. The\nlearning rate was set to 8 × 10\n−6 and the language model with the\nclassiﬁcation head wasﬁne-tuned for 25 epochs. The batch size was set\nto 128. Theﬁnal model weights were chosen based on the best accu-\nracy achieved on the validation set.\nWe ﬁne-tuned RNA-FM in combination with a two-layer MLP\nprediction head. The best classiﬁcation performance it achieved was\nwith 128 neurons in the hidden layer and 10−5 learning rate. Weﬁne-\ntuned the models on the same training/validation dataset used forﬁne-\ntuning RiNALMo.\nIn Supplementary Table S11, we provide the results of smaller LMs\nand the impact ofﬁne-tuning on the ncRNA family classiﬁcation task.\nMean ribosome loading prediction\nWe ﬁne-tuned RiNALMo on the appropriate training dataset to\npredict the MRL value for the given 50 UTR sequence. Language\nmodel outputs are fed into a prediction head which consists of six\nResNet convolutional blocks. The mean squared error was used as\nthe loss function. RNA-FM wasﬁne-tuned with the same prediction\nhead as RiNALMo.\nEach block of the prediction head consists of two 1D convolution\nlayers followed by instance normalization and ELU activation function.\nParameters of the prediction head were initialized with the default\nPytorch\n68 parameter initialization method.\nTraining and evaluation datasets were produced with the proce-\ndure described in ref.60. Two evaluation datasets were created by\nsampling the original dataset which contains 83,919 human and random\nUTR sequences of varying lengths. To ensure that each sequence length\nis represented equally in the dataset, 100 sequences with the deepest\nread coverage were selected for every length from 25 to 100 nucleotides.\nThe same approach has been applied to both the human and random 50\nUTR sequences, yielding two evaluation datasets of size 7600 called\nRandom7600 and Human7600. All remaining random 50 UTRs with\nacceptable read coverage depth were used as the training dataset.\nAll MRL targets were standardized with the mean and standard\ndeviation of training MRL values.\nThe model wasﬁne-tuned for 50 epochs. In theﬁrst 5 epochs of\nthe training, only the prediction head was trained. The learning rate\nwas set to 10\n−4 and linearly decayed to 10−5 over theﬁrst 5000 training\nsteps after which it remained constant. The batch size was set to 64.\nRNA-FM wasﬁne-tuned with the same training procedure.\nThe performance of smaller LMs and the impact ofﬁne-tuning for\nthe MRL prediction task can be found in Supplementary Table S12.\nTranslation efﬁciency and expression level prediction\nFine-tuning and prediction of mRNA TE and EL from 5′ UTR sequences\nfollowed the procedure for the MRL prediction downstream task.\nLanguage model outputs are fed into a prediction head which consists\nof six ResNet convolutional blocks. The Huber loss was used as the loss\nfunction. The architecture and the initialization of the prediction head\nwere the same as for the MRL prediction task. RNA-FM wasﬁne-tuned\nw i t ht h es a m ep r e d i c t i o nh e a da sR i N A L M o .\nEvaluation data were the same as reported in the UTR-LM paper\n30.\nThe data consisted of three datasets, namely human muscle tissue,\nhuman prostate cancer cell line PC3, and human embryonic kidney\n293T cell line. The Muscle, PC3, and HEK datasets contained 1257,\n12,579, and 14,410 5′ UTR sequences, respectively. Following ref.30\nand to achieve a fair comparison, only aﬁxed 5′ UTR length of 100\nnucleotides was chosen for training. These 100 nucleotides are located\nupstream of the coding region in the mRNA. Following ref.30,f o r\ntraining and testing we used tenfold cross-validation for each cell line\nfor both TE and EL prediction tasks.\nIn the datasets, mRNA expression level was determined using\nRNA-seq reads per kilobase of transcript per million mapped reads\n(RPKM), and translation efﬁciency for each transcript was calculated\nby dividing the Ribo-seq RPKM by the RNA-seq RPKM. The transla-\ntion ef ﬁciency and expression level labels are in the natural\nlogarithm space.\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 12\nThe model wasﬁne-tuned for 30 epochs. In theﬁrst 5 epochs of\nthe training, only the prediction head was trained. The learning rate\nwas set to 8 × 10\n−5 and linearly decayed to 8 × 10−6 over theﬁrst 3000\ntraining steps after which it remained constant. The batch size for the\nMuscle dataset was set to 8, and for the PC3 and HEK datasets to 32.\nRNA-FM wasﬁne-tuned with the same training procedure.\nIn Supplementary Table S13, we provide the results of smaller LMs\nand the impact ofﬁne-tuning on the TE and EL prediction task.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nWe used several databases with unannotated RNA sequences, namely\nRNAcentral24,n t69,R f a m45 and Ensembl70. The RNAcentral dataset is\navailable at https://ftp.ebi.ac.uk/pub/databases/RNAcentral/releases/\n22.0/sequences/. The nt dataset is available athttps://ftp.ncbi.nlm.nih.\ngov/blast/db/FASTA/. The Rfam 14.9 dataset is available athttps://ftp.\nebi.ac.uk/pub/databases/Rfam/14.9/fasta-ﬁles/Rfam.fa.gz. Finally, the\nEnsembl dataset is available athttps://ftp.ensembl.org/pub/release-\n109/fasta/and https://ftp.ensemblgenomes.ebi.ac.uk/pub/bacteria/\nrelease-56/.The intra-family RNA secondary structure dataset with\ntrain/test splits is available athttps://dl.dropboxusercontent.com/s/\nw3kc4iro8ztbf3m/bpRNA_dataset.zip. Structurally dissimilar datasets\nTrainSetA and TestSetB are available athttps://github.com/mxfold/\nmxfold2/releases/download/v0.1.1/Rivas.tar.gz. The inter-family sec-\nondary structure dataset consists of nine families and train/test splits\nare available at https://github.com/marcellszi/dl-rna/releases/\ndownload/Data/ct-splits.tar.gz. For the multi-species splice-site\ndownstream task, we used the dataset denoted as GS_1 in ref.56.T h e\ndataset is available at https://git.unistra.fr/nscalzitti/spliceatorand\nhttps://zenodo.org/records/7995778. The dataset\n59 used in the ncRNA\nfamily classiﬁcation task can be found at https://github.com/\nbioinformatics-sannio/ncrna-deep/tree/master/datasets/Rfam-novel.\nThe mean ribosome loading dataset60 used in the paper can be found\nat https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE114002&\nformat=ﬁle. The mRNA translation efﬁciency and expression level\ndatasets reported in ref.61 and used in ref.30 as well as in this paper\nare available at https://drive.google.com/drive/folders/190oihtrw\nCxWjtDCK9kJzyhXPKxbr5xoR and https://codeocean.com/capsule/\n4214075/tree/v1.\nCode availability\nT h ec o d er e p o s i t o r y71 is available on https://github.com/lbcb-sci/\nRiNALMo and the pre-trained andﬁne-tuned weights are available on\nhttps://zenodo.org/records/15043668.T h ew e i g h t sc a nb ea u t o -\nmatically downloaded using the script provided in the repository. We\nprovide scripts forﬁne-tuning the pre-trained model on the down-\nstream tasks from the“Results” section. The data used in the down-\nstream tasks can be automatically downloaded and preprocessed\nusing the scripts in the code repository.\nReferences\n1. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), (Minneapolis,\nMinnesota, USA, 2019).\n2. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I. Improving\nlanguage understanding with unsupervised learning. OpenAI\nhttps://openai.com/index/language-unsupervised/Accessed 9\nDec 2024 (2018).\n3. Raffel, C. et al. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer.J .M a c h .L e a r n .R e s .21,5 4 8 5– 5551 (2020).\n4. Brown, T. et al. Language models are few-shot learners.Adv. Neural\nInf. Process. Syst.33,1 8 7 7– 1901 (2020).\n5. Touvron, H. et al. Llama: open and efﬁcient foundation language\nmodels. Eprinthttp://arxiv.org/abs/2302.13971arXiv:2302.13971\n(2023).\n6. Chowdhery, A. et al. PaLM scaling language modeling with path-\nways. J. Mach. Learn. Res.24,1 – 113 (2023).\n7 . F e r r u z ,N . ,S c h m i d t ,S .&H ö c k e r ,B. ProtGPT2 is a deep unsupervised\nlanguage model for protein design.Nat. Commun.13,4 3 4 8( 2 0 2 2 ) .\n8. Madani, A. et al. Large language models generate functional protein\nsequences across diverse families.Nat. Biotechnol.41,1 0 9 9– 1106\n(2023).\n9 . N i j k a m p ,E . ,R u f f o l o ,J .A . ,W e i n s t e i n ,E .N . ,N a i k ,N .&M a d a n i ,A .\nProGen2 exploring the boundaries of protein language models.Cell\nSyst. 14, 968– 978 (2023).\n10. Wu, R. et al. High-resolution de novo structure prediction from\nprimary sequence. bioRxivhttps://doi.org/10.1101/2022.07.21.\n500999 (2022).\n11. Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein\nstructure with a language model.Science 379, 1123– 1130 (2023).\n12. Jumper, J. et al. Highly accurate protein structure prediction with\nAlphaFold.Nature 596,5 8 3– 589 (2021).\n13. Rives, A. et al. Biological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences.\nP r o c .N a t lA c a d .S c i .118, 2016239118 (2021).\n14. Heinzinger, M. et al. Modeling aspects of the language of life\nthrough transfer-learning protein sequences.BMC Bioinforma.20,\n1– 17 (2019).\n15. Nambiar, A. et al. Transforming the language of life: transformer\nneural networks for protein prediction tasks. In:Proc. 11th ACM\nInternational Conference on Bioinformatics, Computational Biology\nand Health Informatics,1 – 8 (ACM, 2020).\n1 6 . B r a n d e s ,N . ,O f e r ,D . ,P e l e g ,Y . ,R a p p o p o r t ,N .&L i n i a l ,M .P r o -\nteinBERT: a universal deep-learning model of protein sequence and\nfunction.Bioinformatics38,2 1 0 2– 2110 (2022).\n17. Elnaggar, A. et al. ProtTrans toward understanding the language of\nlife through self-supervised learning.IEEE Trans. Pattern Anal. Mach.\nIntell. 44, 7112– 7127 (2022).\n18. Childs-Disney, J. L. et al. Targeting RNA structures with small\nmolecules.Nat. Rev. Drug Discov.21,7 3 6– 762 (2022).\n19. Garner, A. L. Contemporary progress and opportunities in RNA-\ntargeted drug discovery.ACS Med. Chem. Lett.14, 251– 259\n(2023).\n20. Schneider, B. et al. When will RNA get its AlphaFold moment?\nNucleic Acids Res.51,9 5 2 2– 9532 (2023).\n21. Chen, J. et al. Interpretable RNA foundation model from unan-\nnotated data for highly accurate RNA structure and function pre-\ndictions.https://doi.org/10.48550/arXiv.2204.00300(2022).\n2 2 . W a n g ,X .e ta l .U n i - R N AU n i v e r s a lpre-trained models revolutionize\nRNA research. bioRxivhttps://doi.org/10.1101/2023.07.11.548588\n(2023).\n23. Vaswani, A. et al. Attention is all you need.Adv. Neural Inf. Process.\nSyst. 30 (2017).\n24. RNAcentral Consortium RNAcentral 2021: secondary structure\nintegration, improved sequence search and new member data-\nbases. Nucleic Acids Res. 49 212– 220 (2020).\n25. Zhang, Y. et al. Multiple sequence alignment-based RNA language\nmodel and its application to structural inference.Nucleic Acids Res.\n52,3 – 3 (2024).\n26. Chen, K. et al. Self-supervised learning on millions of primary RNA\nsequences from 72 vertebrates improves sequence-based RNA\nsplicing prediction,Brief. Bioinform. 25, bbae163 (2024).\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 13\n27. Ji, Y., Zhou, Z., Liu, H. & Davuluri, R. V. DNABERT pre-trained bidir-\nectional encoder representations from transformers model for\nDNA-language in genome.Bioinformatics37, 2112– 2120 (2021).\n28. Yang, F. et al. scBERT as a large-scale pretrained deep language\nmodel for cell type annotation of single-cell RNA-seq data.Nat.\nMach. Intell.4,8 5 2– 866 (2022).\n29. Celaj, A. et al. An RNA foundation model enables discovery of dis-\nease mechanisms and candidate therapeutics. bioRxivhttps://doi.\norg/10.1101/2023.09.20.558508(2023).\n3 0 . C h u ,Y .e ta l .A5’ UTR language model for decoding untranslated\nregions of mRNA and function predictions.Nat. Mach. Intell.6,\n449– 460 (2024).\n31. Su, J. et al. RoFormer enhanced transformer with rotary position\nembedding.Neurocomputing568, 127063 (2024).\n32. Shazeer, N. GLU variants improve transformer.https://doi.org/10.\n48550/arXiv.2002.05202(2020).\n33. Dao, T. FlashAttention-2 Faster attention with better parallelism and\nwork partitioning.https://doi.org/10.48550/arXiv.2307.08691\n(2023).\n34. Reuter, J. S. & Mathews, D. H. RNAstructure software for RNA sec-\nondary structure prediction and analysis.BMC Bioinform.11,1 2 9\n(2010).\n35. Do, C. B., Woods, D. A. & Batzoglou, S. CONTRAfold: RNA secondary\nstructure prediction without physics-based models.Bioinformatics\n22,9 0– 98 (2006).\n36. Singh, J., Hanson, J., Paliwal, K. & Zhou, Y. RNA secondary structure\nprediction using an ensemble of two-dimensional deep neural\nnetworks and transfer learning.Nat. Commun.10,5 4 0 7( 2 0 1 9 ) .\n37. Fu, L. et al. UFold: fast and accurate RNA secondary structure pre-\ndiction with deep learning.Nucleic Acids Res.50,1 4– 14 (2021).\n38. Sato, K., Akiyama, M. & Sakakibara, Y. RNA secondary structure\nprediction using deep learning with thermodynamic integration.\nNat. Commun.12, 941 (2021).\n39. Danaee, P. et al. bpRNA large-scale automated annotation and\nanalysis of RNA secondary structure.Nucleic Acids Res.46,\n5381– 5394 (2018).\n40. Berman, H. M. et al. The Protein Data Bank.Nucleic Acids Res.28,\n235– 242 (2000).\n41. Yang, H. et al. Tools for the automatic identiﬁcation and classiﬁca-\ntion of RNA base pairs.Nucleic Acids Res.31,3 4 5 0–\n3460 (2003).\n42. Rivas, ElenaandLang,RaymondandEddy,SeanR. A range of complex\nprobabilistic models for RNA secondary structure prediction that\nincludes the nearest-neighbor model and more.RNA 18,1 9 3– 212\n(2011).\n43. Andronescu, M., Condon, A., Hoos ,H .H . ,M a t h e w s ,D .H .&M u r p h y ,\nK. P. Efﬁcient parameter estimation for RNA secondary structure\nprediction.Bioinformatics23,1 9– 28 (2007).\n44. Andronescu, M., Condon, A., Hoos ,H .H . ,M a t h e w s ,D .H .&M u r p h y ,\nK. P. Computational approaches for RNA energy parameter esti-\nmation. RNA 16,2 3 0 4– 2318 (2010).\n45. Kalvari, I. et al. Rfam 14: expanded coverage of metagenomic, viral\nand microRNA families.Nucleic Acids Res.49,1 9 2– 200 (2020).\n46. Wayment-Steele, H. K. et al. RNA secondary structure packages\nevaluated and improved by high-throughput experiments.Nat.\nMethods 19,1 2 3 4– 1242 (2022).\n4 7 . S z i k s z a i ,M . ,W i s e ,M . ,D a t t a ,A . ,W a r d ,M .&M a t h e w s ,D .H .D e e p\nlearning models for RNA secondary structure prediction (probably)\ndo not generalize across families.Bioinformatics38,3 8 9 2– 3899\n(2022).\n48. Justyna, M., Antczak, M. & Szachniuk, M. Machine learning for RNA\n2D structure prediction benchmarked on experimental data.Brief.\nBioinforma.24, 153 (2023).\n49. Sloma, MichaelFandMathews,DavidH. Exact calculation of loop\nformation probability identiﬁes folding motifs in RNA secondary\nstructures.RNA 22,1 8 0 8– 1818 (2016).\n50. Mathews, D. H. How to benchmark RNA secondary structure pre-\ndiction accuracy.Methods 162,6 0– 67 (2019).\n5 1 . H a m a d a ,M . ,S a t o ,K . ,K i r y u ,H . ,M i t u y a m a ,T .&A s a i ,K .P r e d i c t i o n so f\nRNA secondary structure by combining homologous sequence\ninformation.Bioinformatics25, 330– 338 (2009).\n5 2 . W i l l ,S . ,R e i c h e ,K . ,H o f a c k e r ,I .L . ,S t a d l e r ,P .F .&B a c k o f e n ,R .\nInferring noncoding RNA families and classes by means of genome-\nscale structure-based clustering.PLoS Comput. Biol.3,6 5( 2 0 0 7 ) .\n53. Hamada, M., Sato, K. & Asai, K. Improving the accuracy of predicting\nsecondary structure for aligned RNA sequences.Nucleic Acids Res.\n39,3 9 3– 402 (2010).\n54. Puton, T., Kozlowski, L. P., Rother, K. M. & Bujnicki, J. M. CompaRNA:\na server for continuous benchmarking of automated methods\nfor RNA secondary structure prediction.Nucleic Acids Res.\n41,\n4307– 4323 (2013).\n55. Nawrocki, E. P. & Eddy, S. R. Infernal 1.1: 100-fold faster RNA\nhomology searches.Bioinformatics29,2 9 3 3– 2935 (2013).\n56. Scalzitti, N. et al. Spliceator: multi-species splice site prediction\nusing convolutional neural networks.BMC Bioinforma.22,1 – 26\n(2021).\n57. Scalzitti, N., Jeannin-Girardon, A., Collet, P., Poch, O. &\nThompson, J. D. A benchmark study of ab initio gene predic-\ntion methods in diverse eukaryotic organisms.BMC Genom.21,\n1– 20 (2020).\n58. Kalvari, I. et al. Rfam 13.0: shifting to a genome-centric resource\nfor non-coding RNA families.Nucleic Acids Res.46, 335– 342\n(2018).\n59. Noviello, T. M. R., Ceccarelli, F., Ceccarelli, M. & Cerulo, L. Deep\nlearning predicts short non-coding RNA functions from only raw\nsequence data.PLoS Comput. Biol.16, 1008415 (2020).\n60. Sample, P. J. et al. Human 5’ UTR design and variant effect pre-\ndiction from a massively parallel translation assay.Nat. Biotechnol.\n37,8 0 3– 809 (2019).\n61. Cao, J. et al. High-throughput 5’ UTR engineering for enhanced\nprotein production in non-viral gene therapies.Nat. Commun.12,\n4138 (2021).\n62. He, K., Zhang, X., Ren, S., Sun, J. Deep residual learning for image\nrecognition. In:Proc. IEEE Conference on Computer Vision and\nPattern Recognition770– 778 (IEEE, 2016).\n63. Anand, R. ett al. RNA-FrameFlow:ﬂow matching for de novo 3D\nRNA backbone design.https://doi.org/10.48550/arXiv.2406.\n13839 (2024).\n6 4 . C h u r k i n ,A . ,B a r a s h ,D .( e d s . )R N Ad e s i g n :m e t h o d sa n dp r o t o c o l s .\nhttps://doi.org/10.1007/978-1-0716-4079-1(Springer, 2024).\n65. Xiong, R. et al. On layer normalization in the transformer archi-\ntecture. In:Proc. International Conference on Machine Learning,\n10524– 10533 (PMLR, 2020).\n6 6 . P a r i s i e n ,M . ,C r u z ,J .A . ,W e s t h o f ,E .&M a j o r ,F .N e wm e t r i c sf o r\ncomparing and assessing discrepancies between RNA 3D struc-\ntures and models.RNA 15,1 8 7 5– 1885 (2009).\n67. Kerpedjiev, P., Hammer, S. & Hofacker, I. L. Forna (force-directed\nrna): Simple and effective online rna secondary structure diagrams.\nBioinformatics31,3 3 7 7– 3379 (2015).\n68. Paszke, A. et al. Pytorch: an imperative style, high-performance\ndeep learning library.Adv. Neural Inf. Process. Syst.32,8 0 2 4– 8035\n(2019).\n69. Sayers, E. W. et al. Database resources of the National Center for\nBiotechnology Information in 2023.Nucleic Acids Res.51,2 9– 38\n(2022).\n70. Martin, F. J. et al. Ensembl 2023.Nucleic Acids Res.51,9 3 3– 941\n(2022).\n71. Peni ć,R . J . ,V l ašić,T . ,H u b e r ,R . G . ,W a n ,Y . ,Šikić,M .R i N A L M o :\ngeneral-purpose RNA languagemodels can generalize well on\nstructure prediction tasks. Zenodohttps://doi.org/10.5281/zenodo.\n15437847(2025).\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 14\nAcknowledgements\nThe authors would like to thank Ivona Martinović for the valuable com-\nments and fruitful discussion on this work. This work was supported in\npart by the National Research Foundation (NRF) Competitive Research\nProgramme (CRP) under Project Identifying Functional RNA Tertiary\nStructures in Dengue Virus (NRF-CRP27-2021RS-0001 to Y.W.) and in\npart by the A*STAR under Grant GAP2: A*STAR RNA-Foundation Model\n(A*STAR RNA-FM)(I23D1AG079 to M.Š.). The computational work for this\narticle was partially performed on the resources of the National Super-\ncomputing Centre, Singapore (https://www.nscc.sg).\nAuthor contributions\nR.J.P. and M.Š.c o n c e i v e dt h ep r o j e c t .R . J . P .a n dT . V .d e s i g n e dt h e\nmethod. R.J.P. and T.V. designed and conducted the numerical experi-\nm e n t s .R . G . H . ,Y . W .a n dM .Š. supervised the study. R.J.P. and T.V. wrote\nthe manuscript. R.G.H., Y.W. and M.Š.p r o v i d e dm e n t o r s h i pa n ds u p p o r t\nduring the project. All authors approved the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-025-60872-5.\nCorrespondenceand requests for materials should be addressed to\nMile. Šikić.\nPeer review informationNature Communicationsthanks the anon-\nymous reviewers for their contribution to the peer review of this work. A\npeer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if you modiﬁed the licensed\nmaterial. You do not have permission under this licence to share adapted\nmaterial derived from this article or parts of it. The images or other third\nparty material in this article are included in the article’s Creative\nCommons licence, unless indicatedotherwise in a credit line to the\nmaterial. If material is not included in the article’s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this licence, visithttp://\ncreativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025\nArticle https://doi.org/10.1038/s41467-025-60872-5\nNature Communications|         (2025) 16:5671 15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5857022404670715
    },
    {
      "name": "RNA",
      "score": 0.5136289000511169
    },
    {
      "name": "Computational biology",
      "score": 0.4711301028728485
    },
    {
      "name": "Biology",
      "score": 0.24123671650886536
    },
    {
      "name": "Genetics",
      "score": 0.1219046413898468
    },
    {
      "name": "Gene",
      "score": 0.11502256989479065
    }
  ]
}