{
  "title": "Are Pre-trained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection",
  "url": "https://openalex.org/W3173075577",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104112628",
      "name": "Jianguo Zhang",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2095768988",
      "name": "Kazuma Hashimoto",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2171492316",
      "name": "Yao Wan",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101062761",
      "name": "ZhiWei Liu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2110002789",
      "name": "Ye Liu",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2095665791",
      "name": "Caiming Xiong",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2147553045",
      "name": "Philip Yu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1815076433",
    "https://openalex.org/W3034533785",
    "https://openalex.org/W2531327146",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3016625483",
    "https://openalex.org/W2945475330",
    "https://openalex.org/W3014773921",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3119649668",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2971525065",
    "https://openalex.org/W3100247553",
    "https://openalex.org/W3106241909",
    "https://openalex.org/W2810840719",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3175204457",
    "https://openalex.org/W4226198591",
    "https://openalex.org/W3045492832",
    "https://openalex.org/W3100110884",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2963924212",
    "https://openalex.org/W3213730158",
    "https://openalex.org/W3094476119",
    "https://openalex.org/W3199079601",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W2980282514"
  ],
  "abstract": "Pre-trained Transformer-based models were reported to be robust in intent classification. In this work, we first point out the importance of in-domain out-of-scope detection in few-shot intent recognition tasks and then illustrate the vulnerability of pre-trained Transformer-based models against samples that are in-domain but out-of-scope (ID-OOS). We construct two new datasets, and empirically show that pre-trained models do not perform well on both ID-OOS examples and general out-of-scope examples, especially on fine-grained few-shot intent detection tasks.",
  "full_text": "Proceedings of the 4th Workshop on NLP for Conversational AI, pages 12 - 20\nMay 27, 2022 ©2022 Association for Computational Linguistics\nAre Pre-trained Transformers Robust in Intent Classification?\nA Missing Ingredient in Evaluation of Out-of-Scope Intent Detection\nJianguo Zhang1 Kazuma Hashimoto2 Yao Wan3 Zhiwei Liu4\nYe Liu1 Caiming Xiong1 Philip S. Yu4\n1Salesforce Research, Palo Alto, USA\n2Google Research, Mountain View, USA\n3Huazhong University of Science and Technology, Wuhan, China\n4University of Illinois at Chicago, Chicago, USA\njianguozhang@salesforce.com\nAbstract\nPre-trained Transformer-based models were\nreported to be robust in intent classification. In\nthis work, we first point out the importance of\nin-domain out-of-scope detection in few-shot\nintent recognition tasks and then illustrate the\nvulnerability of pre-trained Transformer-based\nmodels against samples that are in-domain but\nout-of-scope (ID-OOS). We construct two new\ndatasets, and empirically show that pre-trained\nmodels do not perform well on both ID-OOS\nexamples and general out-of-scope examples,\nespecially on fine-grained few-shot intent\ndetection tasks. To figure out how the models\nmistakenly classify ID-OOS intents as in-scope\nintents, we further conduct analysis on confi-\ndence scores and the overlapping keywords, as\nwell as point out several prospective directions\nfor future work. Resources are available\nat https://github.com/jianguoz/\nFew-Shot-Intent-Detection .\n1 Introduction\nIntent detection, which aims to identify intents\nfrom user utterances, is a vital task in goal-oriented\ndialog systems (Xie et al., 2022). However, the per-\nformance of intent detection has been hindered by\nthe data scarcity issue, as it is non-trivial to collect\nsufficient examples for new intents. In practice, the\nuser requests could also be not expected or sup-\nported by the tested dialog system, referred to as\nout-of-scope (OOS) intents. Thus, it is important to\nimprove OOS intents detection performance while\nkeeping the accuracy of detecting in-scope intents\nin the few-shot learning scenario.\nRecently, several approaches (Zheng et al., 2019;\nZhang et al., 2020; Wu et al., 2020; Cavalin et al.,\n2020; Zhan et al., 2021; Xu et al., 2021) have been\nproposed to improve the performance of identify-\ning in-scope and OOS intents in few-shot scenar-\nios. Previous experiments have shown that a sim-\nple confidence-based out-of-distribution detection\nmethod (Hendrycks and Gimpel, 2017; Hendrycks\net al., 2020a) equipped with pre-trained BERT can\nimprove OOS detection accuracy. However, there\nis a lack of further study of pre-trained Transform-\ners on few-shot fine-grained OOS detection where\nthe OOS intents are more relevant to the in-scope\nintents. Besides, those studies mainly focus on the\nCLINC dataset (Larson et al., 2019), in which the\nOOS examples are designed such that they do not\nbelong to any of the known intent classes. Their\ndistribution is dissimilar to each other, and thus\nthey are easy to be distinguished from the known\nintent classes. Moreover, CLINC is not enough\nto study more challenging few-shot fine-grained\nOOS detection as it lacks such semantically similar\nOOS examples to in-scope intents, and other popu-\nlar used datasets such as BANKING77 (Casanueva\net al., 2020) do not contain OOS examples.\nIn this paper, we aim to investigate the following\nresearch question: “Are pre-trained Transformers\nrobust in intent classification w.r.t. general and rel-\nevant OOS examples?”. We first define two types\nof OOS intents: out-of-domain OOS (OOD-OOS)\nand in-domain OOS ( ID-OOS). We then investi-\ngate how robustly state-of-the-art pre-trained Trans-\nformers perform on these two OOS types. The\nOOD-OOS is identical to the OOS in the CLINC\ndataset, where the OOS and in-scope intents (e.g.,\nrequesting an online TV show service in a banking\nsystem) are topically rarely overlapped. We con-\nstruct an ID-OOS set for a domain, by separating\nsemantically-related intents from the in-scope in-\ntents (e.g., requesting a banking service that is not\nsupported by the banking system).\nEmpirically, we evaluate several pre-trained\nTransformers (e.g., BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), ALBERT (Lan et al.,\n2020), and ELECTRA (Clark et al., 2020)) in the\nfew-shot learning scenario, as well as pre-trained\nToD-BERT (Wu et al., 2020) on task-oriented di-\nalog system. The contributions of this paper are\ntwo-fold. First, we constructed and released two\n12\nnew datasets for OOS intent detection based on the\nsingle-domain CLINC dataset and the large fine-\ngrained BANKING77 dataset. Second, we reveal\nseveral interesting findings through experimental\nresults and analysis: 1) the pre-trained models are\nmuch less robust on ID-OOS than on the in-scope\nand OOD-OOS examples; 2) both ID-OOS and\nOOD-OOS detections are not well tackled and re-\nquire further explorations on the scenario of fine-\ngrained few-shot intent detection; and 3) it is sur-\nprising that pre-trained models can predict undesir-\nably confident scores even when masking keywords\nshared among confusing intents.\n2 Evaluation Protocol\nTask definition We consider a few-shot intent de-\ntection system that handles pre-defined Kin-scope\nintents. The task is, given a user utterance textu, to\nclassify uinto one of the Kclasses or to recognize\nuas OOS (i.e., OOS detection). To evaluate the sys-\ntem, we adopt in-scope accuracy Ain = Cin/Nin,\nand OOS recall Roos = Coos/Noos, following Lar-\nson et al. (2019) and Zhang et al. (2020). We addi-\ntionally report OOS precision, Poos = Coos/N′\noos.\nCin and Coos are the number of correctly predicted\nin-scope and out-of-scope examples, respectively;\nNin and Noos are the total number of the in-scope\nand out-of-scope examples evaluated, respectively;\nif an in-scope example is predicted as OOS, it is\ncounted as wrong. N′\noos (≤Nin + Noos) is the\nnumber of examples predicted as OOS.\nInference We use a confidence-based\nmethod (Hendrycks et al., 2020a) to evalu-\nate the five pre-trained Transformers. We compute\na hidden vector h = Encoder(u) ∈R768 for u,\nwhere Encoder ∈{BERT, RoBERTa, ALBERT,\nELECTRA, ToD-BERT}, and compute a proba-\nbility vector p(y|u) = softmax(Wh + b) ∈RK,\nwhere W and b are the model parameters. We\nfirst take the class c with the largest value of\np(y = c|u), then output c if p(y = c|u) > δ,\nwhere δ ∈ [0.0,1.0] is a threshold value, and\notherwise we output OOS. δis tuned by using the\ndevelopment set, so as to maximize (Ain + Roos)\naveraged across different runs (Zhang et al., 2020).\nTraining To train the model, we use training ex-\namples of the in-scope intents, without using any\nOOS examples. This is reasonable as it is nontriv-\nial to collect sufficient OOS data to model the large\nspace and distribution of the unpredictable OOS\nintents (Zhang et al., 2020; Cavalin et al., 2020).\n3 Dataset Construction\nWe describe the two types of OOS (i.e., OOD-OOS\nand ID-OOS), using the CLINC dataset (Larson\net al., 2019) and the fine-grained BANKING77\ndataset (Casanueva et al., 2020). The CLINC\ndataset covers 15 intent classes for each of the 10\ndifferent domains, and it also includes OOS exam-\nples. We randomly select two domains, i.e., the\n“Banking” and “Credit cards”, out of the ten do-\nmains for models evaluation. The BANKING77\ndataset is a large fine-grained single banking do-\nmain intent dataset with 77 intents, and it initially\ndoes not include OOS examples. We use these\ntwo datasets since CLINC dataset focuses on the\nOOS detection task, and we can evaluate models\non the large single fine-grained banking domain on\nBANKING77 dataset.\nOOD-OOS We use the initially provided OOS\nexamples of CLINC dataset as OOD-OOS exam-\nples for both datasets. To justify our hypothesis that\nthe CLINC’s OOS examples can be considered as\nout of domains, we take 100 OOS examples from\nthe development set, and check whether the ex-\namples are related to each domain. Consequently,\nonly 4 examples are relevant to “Banking”, while\nnone of them is related to “Credit cards”. There\nare also no overlaps between the added OOS exam-\nples and the original BANKING77 dataset. These\nfindings show that most of the OOS examples are\nnot related to the targeted domains, and we cannot\neffectively evaluate the model’s capability to detect\nOOS intents within the same domain.\nID-OOS Detecting the OOD-OOS examples is\nimportant in practice, but we focus more on how\nthe model behaves on ID-OOS examples. For the\nID-OOS detection evaluation, we separate 5 intents\nfrom the 15 intents in each of the domains and\nuse them as the ID-OOS samples for the CLINC\ndataset, following the previous work (Shu et al.,\n2017). In contrast to the previous work that ran-\ndomly splits datasets, we intentionally design a\nconfusing setting for each domain. More specifi-\ncally, we select 5 intents that are semantically sim-\nilar to some of the 10 remaining intents. As for\nthe BANKING77 dataset, we randomly separate\n27 intents from the 77 intents and use them as the\nID-OOS samples, following the above process.\nTable 1 and Table 2 show which intent labels\n13\nDomain IN-OOS In-scope\nBanking balance, bill_due, min_payment, account_blocked, bill_balance, interest_rate, order_checks, pay_bill,\nfreeze_account, transfer pin_change, report_fraud, routing, spending_history, transactions\nCredit report_lost_card, improve_credit_score, credit_score, credit_limit, new_card, card_declined, international_fees,\ncards rewards_balance, application_status, apr, redeem_rewards, credit_limit change, damaged_card\nreplacement_card_duration expiration_date\nTable 1: Data split of the ID-OOS and in-scope intents for the CLINC dataset.\nID-OOS\n“pin_blocked”, “top_up_by_cash_or_cheque” “top_up_by_card_charge”, “verify_source_of_funds”,\n“transfer_into_account”, “exchange_rate”, “card_delivery_estimate”, “card_not_working”,\n“top_up_by_bank_transfer_charge”, “age_limit”, “terminate_account”, “get_physical_card”,\n“passcode_forgotten”, “verify_my_identity”, “topping_up_by_card”, “unable_to_verify_identity”,\n“getting_virtual_card”, “top_up_limits”, “get_disposable_virtual_card”, “receiving_money”,\n“atm_support”, “compromised_card”, “lost_or_stolen_card”, “card_swallowed”, “card_acceptance”,\n“virtual_card_not_working”, “contactless_not_working”\nTable 2: Data split of the ID-OOS intents for the BANKING77 dataset. Where 27 intents are randomly selected as\nID-OOS intents and the rest are treated as in-scope intents. Here we show the 27 selected ID-OOS intents.\nare treated as ID-OOS for the CLINC dataset and\nBANKING77 dataset, respectively.\nData Statistics For each domain, the original\nCLINC dataset has 100, 20, and 30 examples for\neach in-scope intent, and 100, 100, and 1000 OOD-\nOOS examples for the train, development, and\ntest sets, respectively. We reorganize the origi-\nnal dataset to incorporate the ID-OOS intents and\nconstruct new balanced datasets. For each in-scope\nintent in the training set, we keep 50 examples as\na new training set, and move the rest 30 examples\nand 20 examples to the development and test sets\nthrough random sampling. For the examples of\neach ID-OOS intent in the training set, we ran-\ndomly sample 60 examples, add them to the devel-\nopment set, and add the rest of the 40 examples to\nthe test set. We move the unused OOD-OOS exam-\nples of the training set to the validation set and keep\nthe OOD-OOS test set unchanged. For the BANK-\nING77 dataset, we move the training/validation/test\nexamples of the selected 27 intents to the ID-OOS\ntraining/validation/test examples, and we copy the\nOOD-OOS examples of CLINC as the OOD-OOS\nexamples of BANKING77.\nWe name the two new datasets as CLINC-\nSingle-Domain-OOS and BANKING77-OOS, re-\nspectively. Table 3 shows the dataset statistics.\n4 Empirical Study\n4.1 Experimental Setting\nWe implement all the models following public\ncode from Zhang et al. (2020), based on the\nHuggingFace Transformers library (Wolf et al.,\nCLINC-Single-Domain-OOSK Train Dev. Test\nIn-scope 10 500 500 500ID-OOS - - 400 350OOD-OOS - - 200 1000\nBANKING77-OOS K Train Dev. Test\nIn-scope 50 5905 1506 2000ID-OOS - - 530 1080OOD-OOS - - 200 1000\nTable 3: Statistics of CLINC-Single-Domain-OOS and\nBANKING77-OOS dataset.\n2019) for the easy reproduction of experiments.\nFor each component related to the five pre-\ntrained models, we use their base configura-\ntions. We use the roberta-base configu-\nration for RoBERTa; bert-base-uncased\nfor BERT; albert-base-v2 for ALBERT;\nelectra-base-discriminator for ELEC-\nTRA; tod-bert-jnt-v1 for ToDBERT. All\nthe model parameters are updated during the fine-\ntuning process. We use the AdamW (Hendrycks\net al., 2020b) optimizer with a weight decay coef-\nficient of 0.01 for all the non-bias parameters. We\nuse a gradient clipping technique (Pascanu et al.,\n2013) with a clipping value of 1.0, and also use\na linear warmup learning-rate scheduling with a\nproportion of 0.1 w.r.t. to the maximum number of\ntraining epochs.\nFor each model, we perform hyper-\nparameters searches for learning rate values\n∈{1e−4,2e−5,5e−5}, and the number of the\ntraining epochs ∈{8,15,25,35}. We set the batch\nsize to 10 and 50 for CLINC- Single-Domain-OOS\nand BANKING77-OOS, respectively. We take the\nhyper-parameter sets for each experiment and train\nthe model ten times for each hyper-parameter set to\n14\nIn-scope accuracy OOS recall OOS precision5-shot Banking Credit cards BANKING77-OOS Banking Credit cards BANKING77-OOS Banking Credit cards BANKING77-OOS\nID-OOS\nALBERT 54.1±6.9 55.5±8.1 20.3 ±2.4 86.3 ±8.1 75.9±11.2 89.5 ±1.5 57.9 ±3.3 55.8±4.3 39.8 ±0.7BERT 75.2±2.9 74.1±4.6 25.4 ±3.6 81.8 ±10.5 76.5±9.7 90.9 ±0.6 70.8 ±2.5 68.1±3.2 41.3 ±1.4ELECTRA 64.8±4.8 71.0±7.3 30.9 ±2.3 89.4 ±4.3 75.8±6.1 87.5 ±2.4 65.1 ±3.0 67.1±4.8 43.0 ±0.8RoBERTa 83.8±1.7 64.5±5.6 43.0 ±2.9 78.4 ±6.2 86.8±5.4 83.1 ±4.3 78.6 ±1.5 63.3±3.4 46.3 ±1.9ToD-BERT 75.1±2.3 67.4±4.2 35.5 ±1.5 75.8 ±9.5 72.3±3.4 82.7 ±1.8 69.4 ±3.6 61.3±2.3 43.8 ±0.1\nOOD-OOS\nALBERT 63.1±5.7 55.5±8.1 20.3 ±2.4 85.3 ±5.4 92.5±4.0 97.3 ±2.5 83.4 ±1.7 81.5±3.1 39.9 ±1.3BERT 75.2±2.9 74.1±4.6 39.0 ±3.1 93.4 ±3.7 95.5±2.7 94.1 ±1.6 88.8 ±1.4 88.4±1.9 49.0 ±1.8ELECTRA 75.5±4.0 71.0±7.3 39.1 ±2.7 87.3 ±4.3 87.6±4.2 93.1 ±4.3 88.8 ±2.1 87.0±2.7 48.7 ±1.1RoBERTa 83.8±1.7 81.2±4.0 62.1 ±2.9 97.0 ±0.9 96.7±1.4 93.9 ±1.4 92.9 ±0.6 91.4±1.8 68.7 ±2.2ToD-BERT 83.0±1.6 75.8±5.0 52.9 ±1.5 91.9 ±1.0 96.7±0.9 88.4 ±1.7 92.8 ±0.6 89.6±2.1 66.0 ±1.210-shot\nID-OOS\nALBERT 77.8±2.7 66.7±7.8 27.3 ±3.4 77.6 ±13.0 79.8±6.4 87.6 ±1.3 72.2 ±2.9 64.0±4.1 42.4 ±1.3BERT 77.5±1.7 80.3±3.7 52.5 ±1.7 87.5 ±9.2 74.5±6.9 77.3 ±3.2 73.8 ±1.7 73.1±3.3 50.8 ±1.1ELECTRA 79.5±2.9 78.0±2.5 40.1 ±2.7 85.2 ±9.1 86.5±5.8 84.0 ±1.7 75.4 ±2.7 73.3±2.9 46.1 ±1.1RoBERTa 76.6±0.9 81.0±5.5 59.7 ±1.2 86.4 ±6.3 83.9±6.9 79.1 ±1.7 72.7 ±1.5 75.8±5.2 55.8 ±1.1ToD-BERT 80.7±2.5 80.6±0.9 54.3 ±1.8 79.5 ±6.1 70.2±5.9 76.9 ±2.7 75.4 ±1.4 71.9±2.6 52.1 ±1.2\nOOD-OOS\nALBERT 77.8±2.7 66.7±7.8 30.5 ±6.5 90.6 ±4.0 95.0±3.4 92.7 ±6.3 89.8 ±1.0 85.7±2.7 47.1 ±1.9BERT 77.5±1.7 90.1±1.9 64.2 ±0.5 96.8 ±1.2 91.1±4.4 91.4 ±3.2 90.0 ±0.7 95.5±1.1 68.9 ±1.0ELECTRA 79.5±2.9 88.6±2.1 40.1 ±2.7 94.8 ±1.7 89.1±2.2 97.6 ±1.0 90.7 ±1.2 94.2±1.1 47.9 ±1.4RoBERTa 89.2±1.3 87.5±3.3 70.3 ±0.3 95.6 ±1.0 94.6±2.4 94.0 ±0.8 95.4 ±0.5 94.0±1.4 73.3 ±1.5ToD-BERT 86.5±2.6 86.5±0.6 60.6 ±1.8 96.0 ±0.5 96.4±0.5 94.9 ±0.9 94.2 ±1.2 93.7±0.3 63.3 ±0.9\nTable 4: Testing results on the “Banking” and “Credit cards” domains in CLINC-Single-Domain-OOS and\nBANKING77-OOS datasets. Note that as the best δis selected based on (Ain + Roos), the in-scope accuracy could\nbe different in the scenarios of OOD-OOS and ID-OOS (see Figure 2).\nFigure 1: Model confidence on the development set of “Banking” domain in CLINC-Single-Domain-OOS dataset\nunder 5-shot setting. Darker colors indicate overlaps.\nselect the best threshold δ(introduced in Section 2)\non the development set. We then select the best\nhyper-parameter set along with the corresponding\nthreshold. Finally, we apply the best model and\nthe threshold to the test set. Experiments were\nconducted on single NVIDIA Tesla V100 GPU\nwith 32GB memory.\nWe mainly conduct the experiments in 5-shot,\ne.g., five training examples per in-scope intent, and\n10-shot; we also report partial results in the full-\nshot scenario.\n4.2 Overall Results\nTable 4 shows the results of few-shot intent de-\ntection on the test set for 5-shot and 10-shot set-\ntings. In both settings, the in-scope accuracy of\nID-OOS examples tends to be lower than that of\nOOD-OOS examples, and the gap becomes larger\nfor OOS recall and precision. It is interesting to\nsee that ToD-BERT, which is pre-trained on sev-\neral task-oriented dialog datasets, does not perform\nwell in our scenario. The results indicate that the\npre-trained models are much less robust on the ID-\nOOS intent detection. Compared with the results\non the two single domains of the CLINC-Single-\nDomain-OOS dataset, we can find that the perfor-\nmances become much worse on the larger fine-\ngrained BANKING77-OOS dataset. Especially the\nin-scope accuracy and OOS precision are pretty\nlow, even with more training examples. This find-\ning encourages more attention to be put on fine-\ngrained intent detection with OOS examples.\n4.3 Analysis and Discussions\nOne key to the OOS detection is a clear separation\nbetween in-scope and OOS examples in terms of\nthe model confidence score (Zhang et al., 2020).\nFigure 1 illustrates the differences in confidence\nscore distributions. The confidence scores of ID-\nOOS examples are close or mixed with the scores\nof in-scope intents, and are higher than the OOD-\nOOS examples, showing that separating ID-OOS\nexamples is much harder than separating OOD-\nOOS examples.\nAmong comparisons of the pre-trained models,\nALBERT performs worst, and RoBERTa performs\nbetter than other models in general since the con-\nfidence score received by in-scope examples is\n15\nFigure 2: Results on the “Banking” domain in CLINC-Single-Domain-OOS dataset (Dev. set) under 5-shot setting.\nFigure 3: Full-shot confusion matrices on the devel-\nopment set with and without masking (“Banking”,\nRoBERTa). Vertical axis: ID-OOS; horizontal axis:\nin-scope (only predicted intents considered).\nhigher than that received by the OOS examples.\nFigure 2 also shows similar results. We conjec-\nture that pre-trained models with more data, bet-\nter architecture and objectives, etc., are relatively\nmore robust to OOD-OOS and ID-OOS examples\nthan the others. Comparing the RoBERTa 5-shot\nand full-shot confidence distributions, the ID-OOS\nconfidence scores are improved, indicating over-\nconfidence to separate semantically-related intents\n(i.e., ID-OOS examples).\nNext, we inspect what ID-OOS examples are\nmisclassified, and we take RoBERTa as an example\nas it performs better than other models in general.\nFigure 3 shows the confusion matrices of RoBERTa\nw.r.t. the “Banking” domain in the CLINC-Single-\nDomain-OOS dataset, under full-shot setting. We\ncan see that the model is extremely likely to con-\nfuse ID-OOS intents with particular in-scope in-\ntents. We expect this is from our ID-OOS design,\nand the trend is consistent across evaluated models.\nNow one question arises: what causes the\nmodel’s mistakes? One presumable source is the\nkeyword overlap. We checked unigram overlap,\nafter removing stop words, for the intent pairs with\nthe three darkest colors in “Banking” based on\nIntent pair bill_due & bill_balanceUnigram overlap bill (60), pay (9), need (9), know (8), due (7)\nMasked ID-OOS examplei [mask] to [mask] what day i [mask] to [mask]my water [mask]→bill_balance (confidence:0.84)Intent pair improve_credit_score & credit_scoreUnigram overlap credit (99), score (76), tell (7), want (3), like (3)\nMasked ID-OOS examplei’d [mask] to make my [mask] [mask] better→credit_limit_change (confidence:0.86)\nTable 5: Examples investigated for the unigram overlap\nanalysis. The overlap frequency is also presented.\nFigure 3. We then masked top-5 overlapped uni-\ngrams from the corresponding intent examples in\nthe development set using the mask token in the\nRoBERTa masked language model pretraining and\nconducted the same evaluation.1 Figure 3 shows\nthat most of the confusing intent pairs are still mis-\nclassified even without the keyword overlap. Ta-\nble 5 shows two intent pairs with the overlapped\nwords and their masked ID-OOS examples. It is\nsurprising that the examples show counterintuitive\nresults. That is, even with the aggressive mask-\ning, the model still tends to assign high confidence\nscores to some other in-scope intents. We also\nadopted state-of-the-art methods with contrastive\nlearning on few-shot text classification (Liu et al.,\n2021) and intent detection (Zhang et al., 2021).\nHowever, we did not achieve promising improve-\nments on OOD-OOS and ID-OOS detection, and\nwe leave more explorations to future work.\n5 Conclusion\nWe have investigated the robustness of pre-trained\nTransformers in few-shot intent detection with\nOOS samples. Our results on two new constructed\ndatasets show that pre-trained models are not ro-\nbust on ID-OOS examples. Both the OOS detection\ntasks are challenging in the scenario of fine-grained\nintent detection. Our work encourages more atten-\ntion to be put on the above findings.\n1We did not mask the top-10 or top-15 overlapped uni-\ngrams, as many tokens are already masked in the user utter-\nance when setting the threshold to 5, as shown in Table 5.\n16\nReferences\nIñigo Casanueva, Tadas Tem ˇcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli´c. 2020. Efficient\nintent detection with dual sentence encoders. In Pro-\nceedings of the 2nd Workshop on Natural Language\nProcessing for Conversational AI, pages 38–45.\nPaulo Cavalin, Victor Henrique Alves Ribeiro, Ana Ap-\npel, and Claudio Pinhanez. 2020. Improving out-\nof-scope detection in intent classification by using\nembeddings of the word graph space of the classes.\nIn EMNLP, pages 3952–3961.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL-HLT, pages 4171–4186.\nDan Hendrycks and Kevin Gimpel. 2017. A baseline\nfor detecting misclassified and out-of-distribution ex-\namples in neural networks. In ICLR.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020a.\nPretrained Transformers Improve Out-of-Distribution\nRobustness. In ACL, pages 2744–2751.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020b.\nPretrained Transformers Improve Out-of-Distribution\nRobustness. arXiv preprint arXiv:2004.06100.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In ICLR.\nStefan Larson, Anish Mahendran, Joseph J. Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K. Kummerfeld, Kevin Leach, Michael A.\nLaurenzano, Lingjia Tang, and Jason Mars. 2019. An\nEvaluation Dataset for Intent Classification and Out-\nof-Scope Prediction. In EMNLP, pages 1311–1316.\nFangyu Liu, Ivan Vuli ´c, Anna Korhonen, and Nigel\nCollier. 2021. Fast, effective, and self-supervised:\nTransforming masked language models into universal\nlexical and sentence encoders. In EMNLP, pages\n1442–1459.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Ben-\ngio. 2013. On the difficulty of training recurrent\nneural networks. In Proceedings of the 30th Inter-\nnational Conference on Machine Learning (ICML),\npages 1310–1318.\nLei Shu, Hu Xu, and Bing Liu. 2017. DOC: Deep Open\nClassification of Text Documents. In EMNLP, pages\n2911–2916.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nChien-Sheng Wu, Steven Hoi, Richard Socher, and\nCaiming Xiong. 2020. ToD-BERT: Pre-trained Natu-\nral Language Understanding for Task-Oriented Dia-\nlogues. EMNLP.\nTian Xie, Xinyi Yang, Angela S Lin, Feihong Wu,\nKazuma Hashimoto, Jin Qu, Young Mo Kang, Wen-\npeng Yin, Huan Wang, Semih Yavuz, et al. 2022.\nConverse–a tree-based modular task-oriented dia-\nlogue system. arXiv preprint arXiv:2203.12187.\nKeyang Xu, Tongzheng Ren, Shikun Zhang, Yihao\nFeng, and Caiming Xiong. 2021. Unsupervised out-\nof-domain detection via pre-trained transformers. In\nACL, pages 1052–1061.\nLi-Ming Zhan, Haowen Liang, Bo Liu, Lu Fan, Xiao-\nMing Wu, and Albert YS Lam. 2021. Out-of-scope\nintent detection with self-supervision and discrimina-\ntive training. In ACL, pages 3521–3532.\nJianguo Zhang, Trung Bui, Seunghyun Yoon, Xiang\nChen, Zhiwei Liu, Congying Xia, Quan Hung Tran,\nWalter Chang, and S Yu Philip. 2021. Few-shot intent\ndetection via contrastive pre-training and fine-tuning.\nIn EMNLP, pages 1906–1912.\nJianguo Zhang, Kazuma Hashimoto, Wenhao Liu,\nChien-Sheng Wu, Yao Wan, S Yu Philip, Richard\nSocher, and Caiming Xiong. 2020. Discriminative\nnearest neighbor few-shot intent detection by trans-\nferring natural language inference. In EMNLP, pages\n5064–5082.\nYinhe Zheng, Guanyi Chen, and Minlie Huang. 2019.\nOut-of-domain Detection for Natural Language Un-\nderstanding in Dialog Systems. arXiv preprint\narXiv:1909.03862.\n17\nA More Results\nFigure 4 shows the model confidence level on the\ndevelopment set of the “Credit cards” domain in\nthe CLINC-Single-Domain-OOS dataset. We can\nsee that RoBERTa is relatively more robust with\nlimited data. Figure 5 shows the confusion matri-\nces of RoBERTa w.r.t. the “Credit cards” domain\nin the CLINC-Single-Domain-OOS dataset. The\nmodel is confused to identify ID-OOS intents. Fig-\nure 6 shows the tSNE visualizations for ID-OOS\nintents w.r.t. the “Banking” domain in the CLINC-\nSingle-Domain-OOS dataset. The models struggle\nto classify the ID-OOS intents even with more data.\n18\nFigure 4: Model confidence on the development set of the “Credit cards” domain in CLINC-Single-Domain-OOS\ndataset under 5-shot setting. Darker colors indicate overlaps.\nFigure 5: Full-shot confusion matrices on the development set with and without masking (“Credit cards”, RoBERTa).\nVertical axis: ID-OOS; horizontal axis: in-scope (only predicted intents considered).\n19\nFigure 6: RoBERTa (first row) and ELECTRA (second row) tSNE visualizations on the development set of the\n“Banking” domain in CLINC-Single-Domain-OOS dataset.\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7382500171661377
    },
    {
      "name": "Transformer",
      "score": 0.7000055313110352
    },
    {
      "name": "Scope (computer science)",
      "score": 0.6960345506668091
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5591562986373901
    },
    {
      "name": "Machine learning",
      "score": 0.48967650532722473
    },
    {
      "name": "Data mining",
      "score": 0.3740825653076172
    },
    {
      "name": "Engineering",
      "score": 0.16335132718086243
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}