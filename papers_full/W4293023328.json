{
    "title": "A length adaptive algorithm-hardware co-design of transformer on FPGA through sparse attention and dynamic pipelining",
    "url": "https://openalex.org/W4293023328",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4284263665",
            "name": "Peng, Hongwu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2611893885",
            "name": "Huang Shao-yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2349273192",
            "name": "Chen Shi-yang",
            "affiliations": [
                "Stevens Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A1623828274",
            "name": "Li Bingbing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2348800416",
            "name": "Geng, Tong",
            "affiliations": [
                "Pacific Northwest National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A1963693760",
            "name": "Li Ang",
            "affiliations": [
                "Pacific Northwest National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2354500975",
            "name": "Jiang, Weiwen",
            "affiliations": [
                "George Mason University"
            ]
        },
        {
            "id": "https://openalex.org/A2744247839",
            "name": "Wen, Wujie",
            "affiliations": [
                "Lehigh University"
            ]
        },
        {
            "id": "https://openalex.org/A3089493679",
            "name": "Bi, Jinbo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2076313181",
            "name": "Liu Hang",
            "affiliations": [
                "Stevens Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2744413473",
            "name": "Ding, Caiwen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2088589173",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3093091803",
        "https://openalex.org/W4299802238",
        "https://openalex.org/W3189877953",
        "https://openalex.org/W4295838474",
        "https://openalex.org/W3159727696",
        "https://openalex.org/W2984864519",
        "https://openalex.org/W4283314135",
        "https://openalex.org/W3188928057",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W4287117998",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W3097363806",
        "https://openalex.org/W4285653943",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W4287780251",
        "https://openalex.org/W3098576111",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W3175199633",
        "https://openalex.org/W3086908689",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4226064176",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2912616132",
        "https://openalex.org/W3162542754",
        "https://openalex.org/W131533222",
        "https://openalex.org/W4299286379",
        "https://openalex.org/W3199137829"
    ],
    "abstract": "Transformers are considered one of the most important deep learning models since 2018, in part because it establishes state-of-the-art (SOTA) records and could potentially replace existing Deep Neural Networks (DNNs). Despite the remarkable triumphs, the prolonged turnaround time of Transformer models is a widely recognized roadblock. The variety of sequence lengths imposes additional computing overhead where inputs need to be zero-padded to the maximum sentence length in the batch to accommodate the parallel computing platforms. This paper targets the field-programmable gate array (FPGA) and proposes a coherent sequence length adaptive algorithm-hardware co-design for Transformer acceleration. Particularly, we develop a hardware-friendly sparse attention operator and a length-aware hardware resource scheduling algorithm. The proposed sparse attention operator brings the complexity of attention-based models down to linear complexity and alleviates the off-chip memory traffic. The proposed length-aware resource hardware scheduling algorithm dynamically allocates the hardware resources to fill up the pipeline slots and eliminates bubbles for NLP tasks. Experiments show that our design has very small accuracy loss and has 80.2 $\\times$ and 2.6 $\\times$ speedup compared to CPU and GPU implementation, and 4 $\\times$ higher energy efficiency than state-of-the-art GPU accelerator optimized via CUBLAS GEMM.",
    "full_text": "A Length Adaptive Algorithm-Hardware Co-design of\nTransformer on FPGA Through Sparse Attention and Dynamic\nPipelining\nHongwu Peng1,+, Shaoyi Huang1,+, Shiyang Chen2, Bingbing Li1, Tong Geng3, Ang Li3, Weiwen\nJiang4, Wujie Wen5, Jinbo Bi1, Hang Liu2 and Caiwen Ding1\n+These authors contributed equally.\n1University of Connecticut, USA. 2Stevens Institute of Technology, USA. 3Pacific Northwest National\nLaboratory, USA. 4George Mason University, USA. 5Lehigh University, USA.\n1{hongwu.peng, shaoyi.huang, bingbing.li, jinbo.bi, caiwen.ding}@uconn.edu, 2{schen94, hliu77}@stevens.edu,\n3{tong.geng, ang.li}@pnnl.gov, 4wjiang8@gmu.edu, 5wuw219@lehigh.edu\nABSTRACT\nTransformers are considered one of the most important deep learn-\ning models since 2018, in part because it establishes state-of-the-art\n(SOTA) records and could potentially replace existing Deep Neural\nNetworks (DNNs). Despite the remarkable triumphs, the prolonged\nturnaround time of Transformer models is a widely recognized\nroadblock. The variety of sequence lengths imposes additional com-\nputing overhead where inputs need to be zero-padded to the max-\nimum sentence length in the batch to accommodate the parallel\ncomputing platforms. This paper targets the field-programmable\ngate array (FPGA) and proposes a coherent sequence length adap-\ntive algorithmâ€“hardware co-design for Transformer acceleration.\nParticularly, we develop a hardware-friendly sparse attention oper-\nator and a length-aware hardware resource scheduling algorithm.\nThe proposed sparse attention operator brings the complexity of\nattention-based models down to linear complexity and alleviates\nthe off-chip memory traffic. The proposed length-aware resource\nhardware scheduling algorithm dynamically allocates the hardware\nresources to fill up the pipeline slots and eliminates bubbles for\nNLP tasks. Experiments show that our design has very small ac-\ncuracy loss and has 80.2 Ã—and 2.6 Ã—speedup compared to CPU\nand GPU implementation, and 4 Ã—higher energy efficiency than\nstate-of-the-art GPU accelerator optimized via CUBLAS GEMM.\nKEYWORDS\nTransformer, Attention, BERT, Length adaptive, FPGA\n1 INTRODUCTION\nTransformers are considered as one of the most important deep\nlearning models since 2018 [1], in part because it could potentially\nreplace existing Deep Neural Networks (DNNs), such as Convo-\nlutional Neural Networks (CNN) and Recurrent Neural Networks\n(RNN) [2]. By leveraging self-attention [3], Transformers have es-\ntablished state-of-the-art (SOTA) records (beyond human level) in\nvarious fields. Using computer vision as an example, CNNs were the\nfirst choice previously [4]; but nowadays, Transformer is gradually\nbecoming the potential alternative, both from theoretical demon-\nstration and empirical explorations, e.g., [2, 5â€“7].\nDespite the remarkable triumphs, the prolonged turnaround\ntime of Transformer models is a widely recognized roadblock that\nconcerns real-world applications. Fig.s 1(a) and (b) depict the archi-\ntecture of one encoder a.k.a. the building block for Transformers.\nLayerNorm\nLinear transformation\nLayerNorm\nActivation\nMatMul\nScale\nMasking\nInput\nSelf-attention\nLinear transformation\nMatMul\nQ K V\nLinear transformation\nSoftMax\nLinear transformation\n(a) Encoder workflow (b) Self-attention workflow (c) Time consumption breakdown\nSelf-attention: Linear Self-attention: MatMul\nSelf-attention: Scale Self-attention: Masking\nSelf-attention: Softmax Self-attention: MatMul\nSelf-attention: Linear Other: 2xLayerNorm\nOther: 2xLinear Other: Activation\nFeedforward\nFigure 1: The architecture of a four-head encoder. The\ntime consumption is measured on for TensorRT [15] on\nWikiText-2 dataset [16], where the input sequence has 128\ntokens.\nBriefly, one encoder takes the word embeddings of a sequence as in-\nput. These embeddings pass through the self-attention mechanism\nto produce an attention matrix. This matrix is fed through layer\nnormalization, linear transformation, and activation operations to\nderive the output. Various Transformer model variants often stack\ndifferent numbers of encoders and decoders together [3]. The bad\nnews is that the time consumption of a single encoder in Fig. 1\ncould easily reach 100s of ğœ‡s, which is âˆ¼10Ã—slower than a typical\nCNN model. Of such a long latency, around 60% of the time is spent\nin the self-attention workflow. According to our preliminary study,\nthe time consumption ratio of self-attention is projected to climb if\nthe number of tokens in the input sequence increases, especially in\nthe NLP field [8] further. There exist several initial attention-aware\noptimization attempts, such as sparse attention [9] and attention\napproximation [10â€“14], to leverage runtime approximations or do-\nmain knowledge, i.e., tokens only attend their nearby tokens & a\nfew sampled tokens as the summary of the sentence instead of at-\ntending all tokens. However, these explorations, unfortunately, fall\nshort by either lacking generality or high computation overheads.\nTo make it worse, Transformer brings the challenge of a wide\nvariety of input lengths, where inputs need to be zero-padded to the\nmaximum sentence length in the batch to accommodate the parallel\ncomputing platforms such as GPU and Field-Programmable Gate\nArray (FPGA) [17]. By nature, RNN-based models, e.g., GRU and\nLSTM, process the inputs sequentially. Thus, the inputs could be\ndivided into unified fixed-length sub-inputs and processed indepen-\ndently. Transformers leverages parallel processing and therefore\ncannot enjoy the benefit of fixed-length sub-inputs. Existing works\narXiv:2208.03646v2  [cs.LG]  20 Aug 2022\nPeng and Huang, et al.\non sequence length standardization fall into two categories. The\nfirst one is padding or truncation, which forces the sequence length\nto be the same. It leads to enormous computation overhead due to\nthe unnecessary computation of the padding part. The second cate-\ngory divides a sequence batch into micro-batches (padding within\nthe micro-batch) to mitigate the computation overhead. However,\nthe various and irregular sequence length undermines overall per-\nformance and throughput at the inter micro-batch level. Together\nwith the prolonged turnaround time, achieving fast and efficient\nTransformer models becomes a grand challenge.\nAcross all the popular hardware, e.g., CPUs, GPUs, FPGAs, and\nApplication-Specific Integrated Circuits (ASICs), FPGAs strike an\neffective balance among massive parallelism, high energy efficiency\nand short development cycle, hence lend themselves as the top\nchoice to expedite the Transformer architecture. In this paper, we\nbelieve that the ideal Transformers acceleration should have a co-\nherent algorithmâ€“hardware co-design. We believe that (i) Trans-\nformers should have their dedicated efficient algorithm de-\nsigns. Since self-attention cares more about the value relativity\nof all the attention scores than the absolute value of any specific\nattention score, we propose an efficient scheme to exploit two\ndifferent self-attention approximations adaptively. Note, our ap-\nproximation mechanisms are quantization-based designs that are\nnot only computation-efficient but also hardware-friendly. We also\nthink that (ii) Transformers should efficiently support vari-\nous sequence length inputs . For instance, SQuAD v2.0 [8] has\nan average and maximum sequence length of 171 and 975, respec-\ntively. When padding the sequence with 975, it causes 5.7Ã—compu-\ntational and memory bandwidth overhead on average. The inputs\nare sorted and processed according to the order of length. Com-\npared to existing works, we achieve 4 Ã—higher energy efficiency\nthan GPU accelerator optimized through CUBLAS GEMM routine\n[18, 19] with small accuracy loss, and comparable energy efficiency\ncompared to ASIC accelerator designs [12, 13]. Our contributions\nare:\nâ€¢We propose sparse attention which is computation-efficient\nand hardware-friendly to reduce the need for computational\nresources and memory bandwidth.\nâ€¢We propose a sequence length adaptive design to allocate\ncoarse pipeline stages dynamically to eliminate pipeline bub-\nbles and achieve the highest possible throughput under dif-\nferent sequence length inputs.\nâ€¢Transformer exhibits a highly skewed distribution of compu-\ntation complexity among the operators. We further develop\na loop fusion to orchestrate the multiple attention operators\nand re-arrange various Transformer computations to en-\nhance temporal locality and efficient hardware design with\nfiner granularity.\n2 RELATED WORK\nAttention-aware Optimization. We also notice several recent\nattention-aware optimization attempts, such as sparse attention [9]\nand attention approximation [10â€“14], which unfortunately fall short\nby either lack of generality or high computation overheads. Partic-\nularly, sparse attention mechanisms [9] leverage domain knowledge,\ni.e., tokens only attend their nearby tokens and a few sampled\ntokens as the summary of the sentence instead of attending all\ntokens, to reduce computation and memory consumption during\nself-attention computation. Such design requires a pre-determined\nattention mask that lacks generality.\nApproximation-based attention leverages runtime approxima-\ntions to derive sparse attention which faces non-trivial overheads.\nBP-Transformer [10] and Reformer [11] convert the self-attention\ncomputation into a nearest neighbor search problem and use either\ntree-based search, Locality Sensitive Hashing (LSH), or low-rank\napproximationto find similar tokens for attention. A3 [12] embraces\narchitecture innovation to estimate the closeness between tokens.\nHowever, the estimation process still requires full access to original\nmatrices and does not alleviate memory bottleneck for attention\ncomputation according to SpAtten [13]. ELSA [14] uses LSH dis-\ntance for attention rank approximation, but it again suffers signifi-\ncant overheads for LSH.\nSequence length standardization. TensorRT [15] utilized the\npadding and truncation [ 17] to standardize the sequence length\nfor parallel computing. It makes the hardware design regularized\nbut leads to enormous computation overhead due to the unneces-\nsary computation of the zero-padding part. TurboTransformer [20]\ndivided a batch into micro-batches with similar lengths; however,\nwithin the micro-batch, it still required maximum sequence length\npadding. To make things worse, when we implement this method\non FPGA, it introduces significant pipeline bubbles.\n3 SPARSE ATTENTION ALGORITHM\n3.1 Overview\nApproximation-based attention leverages run-time approximations\nto derive sparse attention which faces non-trivial overheads. BP-\nTransformer [10] and Reformer [11] convert the self-attention com-\nputation into a nearest neighbor search problem and use either\ntree-based search, Locality Sensitive Hashing (LSH), or low-rank\napproximation to find similar tokens for attention. A3 [12] embraces\narchitecture innovation to estimate the closeness between tokens.\nHowever, the estimation process still requires full access to original\nmatrices and does not alleviate memory bottleneck for attention\ncomputation according to SpAtten [13].\nWe propose to compute sparse self-attention via rapid attention\nrank approximation and sparse attention computation. We firstly\nquantize the full precision Q and K into low bits representation, i.e.,\n1 bit or 4 bits. Then we conduct matrix multiplication on quantized\nvalue and get the Top-ğ‘˜ candidates index. At last, we conduct full\nprecision sparse attention computation based on Top-ğ‘˜ candidates.\nThe algorithm reduces the attention complexity from ğ‘‚(ğ‘›2)\nto ğ‘‚(ğ‘›), where ğ‘›is sequence length.\n3.2 Sparse Attention Via Q & K Quantization\nIn self-attention, the input is transformed into three matrices Q,\nK and V. Then Q and K are multiplied to arrive at S = Q Â·Kğ‘‡,\nwhere each element in the resultant matrix is an attention score. A\nrow of attention scores in S represent the dot-product between a\nrow vector in Q and all row vectors in K respectively. Step 0 in\nFig. 3 illustrates one row of Q multiplying with K, where q is one\nrow in matrix Q. Subsequently, we perform a softmax operation\non S, which is Sğ‘– = exp(qkğ‘–)Ã4\nğ‘—=1 exp(qkğ‘—), i.e., step 1 in Fig. 3. The key\nobservation is: since softmax is a normalization method, it is the\nA Length Adaptive Algorithm-Hardware Co-design of\nTransformer on FPGA Through Sparse Attention and Dynamic Pipelining\nInput Data Buffer\nTiled PEs\n+\n+\n+\nComptr\nComptrComptr\nFIFO\nFIFO\nFIFO\nFIFO\nFIFO\nFIFO\nFIFO\n+\n+\n+\nComptr\nComptrComptr\nFIFO\nFIFO\nFIFO\nFIFO\nFIFO\nFIFO\nFIFO\nin\nHBM(PC0-31)\nMatMul (MM) Unit\nBits Seletor\n(a) (b)\nV\nK\nQ\nX Wq| |Wk Wv\nBuffer +\n+\n+\nComptr\nComptrComptr\nFIFO\nFIFO\nFIFO\nFIFO\nFIFO\nFIFO\nFIFO\nStage 3: Feedforward (FdFwd)  (MM Unit)Stage 1: Linear Transformation (MatMul (MM) | Attention \ncandidate pre-selection (At-Sel)\nMM | At-Sel Idle\nStart StateMM\nAt-CompIdle\nStateAtten\nFdFwd\nEnd\nWorking\nIdle\nStateFF\nhead\nhead2\nbuffer\nQrow1\nKs1\n1 ex\nLUT\nVs2\nâˆ‘ 1/x\nhead1\nLoop fusion\nbuffer\nStage 2.2Stage 2.1\nâ€¦\nMM Unit\nMM Unit\nStage 2.3\ni\nStage 2: Attention computation (At-Comp) \nBuf0 Buf1\n++\n+\nÃ— Ã— Ã— Ã—\n++\n+\nÃ— Ã— Ã— Ã— ...\nCross Bar\nCross Bar\nOutput Data Buffer\nBuf0 Buf1\nFigure 2: (a) Sparse attention on FPGA; (b) State machine.\nqkà¬µkà¬¶kà¬·kà¬¸ğâ‹…ğŠğ“1.17 0.30 1.05 -0.83qkà¬µğqkà¬¶qkà¬·qkà¬¸Softmax0.41 0.17 0.37 0.05Sà¬µSà¬¶Sà¬·Sà¬¸3 7 12 5 7-5 3 44 4 1-3 -2 -6qâ€²kâ€²à¬µkâ€²à¬¶kâ€²à¬·kâ€²à¬¸ğâ€²4-bit QuantizationğŠ48 10 41 -29(qkà¬µ)â€²(qkà¬¶)â€²(qkà¬·)â€²(qkà¬¸)â€²ğâ€²â‹…ğŠâ€²ğ“0.41 1.09 0.110.66 1.88 0.11-1.95 1.13 1.411.48 1.33 0.41-1.24 -0.85 -2.08Top-k rank & Select1.17 1.05qkà¬µqkà¬·0.66 1.88 0.111.48 1.33 0.41kà¬µkà¬·Softmax32 4560.53 0 0.47 0Sà¬µSà¬¶Sà¬·Sà¬¸ğŠâ€²01: original attention computationğâ‹…ğŠğ“ğâ„ğŸğâ„\nFigure 3: Candidate selection from quantized result.\nvalue relativity of all the attention scores, as opposed to the absolute\nvalue of any specific attention score, that matters.\nWe propose to quantize Q and K from the full-precision repre-\nsentation (usually 32-bit floating-point) into a low-precision integer\nrepresentation. Because both quantization and exponential opera-\ntions used in softmax are monotonically increasing operators, the\nquantized results maintain the order of attention scores. We use our\nfast quantized matrix multiplication to extract dominant attention\nvalues. Afterward, we perform accurate attention computation only\nfor dominant attention scores. The design is depicted in Fig. 3.\nParticularly, we first find out the suitable scaling factor ğ‘€ for\nthe given tensor to quantization, then perform ğ‘¥â€²= ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘(23âˆ’1\n|ğ‘€|ğ‘¥),\nwhich casts all the floating point values into a desired integer. For\nexample, the scaling factor ğ‘€ of K in Figure 3 is 0.77, so each ele-\nment is be multiplied with 23âˆ’1\n0.77 and rounded to the nearest integer.\nWe follow a similar procedure to quantize q into qâ€™. Subsequently,\nwe again use a look-up table to perform the multiplication. For\ninstance, if we multiply two 4-bit integers, the look-up table only\nneeds 256 entries. We can easily estimate the multiplied value. At\nthe end of step 2 , we derive the Qâ€²Â·Kâ€²ğ‘‡. As the examples indicate,\nthe quantized results keep the same rank and distribution compared\nwith their full-precision counterpart.\nWe conduct Top-ğ‘˜ sort and select the Top-ğ‘˜ ranked attention\nscores for exact matrix multiplication, which derives more accurate\nsoftmax values. This is faster than the original design because we\nonly need to compute Top-ğ‘˜ attention scores. In step 4 in Fig. 3,\nwe select Top-2 elementğ‘˜1 and ğ‘˜3 to perform matrix multiplication\nand softmax, which is used as an approximation of the result of\nself-attention. Subsequently, we will perform full-precision Q Â·Kğ‘‡\nfor the selected attention scores at 5 and final softmax at step 6 .\n4 HARDWARE ACCELERATOR DESIGN AND\nSCHEDULING ALGORITHM\nThe proposed quantization-based sparse attention system design\nis more fit on FPGAs than general-purpose processors because\nthe latter are instruction-driven architecture. At the same time,\nFPGAs are data-driven architecture that avoids instruction fetch\nand related memory access. When compared to the popular GPU\naccelerators, FPGAs excel for the following reasons: (1) The on-chip\nmemory capacity of FPGAs is much higher (360Ã—) than that of GPUs\n(i.e., 35 MB in Xilinx Alveo U200 vs. 96 KB in V100). The FPGA\non-chip memory features its high memory bandwidth (31 TB/s) and\nlow access latency (single clock cycle), enabling higher throughput\nand lower latency design [ 21â€“28]. With more on-chip memory\nsize, we can achieve a better computation to communication (CTC)\nratio for the same operations, i.e., matrix multiply and matrix add.\n(2) FPGA provides more design opportunities on fine-grained and\ncoarse-grained pipelining and loop fusion techniques. We can have\nbetter data locality optimization, and design space freedom on\nFPGA through polyhedral analysis and proper loop reschedule. (3)\nFPGAs are more power-efficient, and therefore more suitable for\nresource-constrained scenarios.\nThe FPGA platform enables better intra-attention coarse grain\npipelining design and leaves more freedom on FPGA resource allo-\ncation. The commonly used way for NLP tasks with variable length\ninputs is to unify input sequence to a fixed length through padding\nand cutting. However, length padding introduces unnecessary over-\nhead, and length cutting leads to information loss. To accelerate\nthe NLP tasks with variable sequence length, we propose sequence\nlength adaptive Transformer hardware design on the FPGA plat-\nform and the corresponding hardware scheduling algorithms to\noptimize the coarse-grained stage throughput. Compared to com-\nmonly used padding and cutting methods, our proposed method\nhas higher hardware throughput and less information loss.\n4.1 Sparse Attention Accelerator Design\nWe break down the original single Transformer Encoder stage\npipeline into three coarse-grained pipelines and overlap their exe-\ncution time by inserting buffers for each concatenated pipeline pair,\nas shown in Fig. 2(a). Stage 1 contains the linear transformation\n(using MatMul (MM)) and quick attention approximations (a.k.a,\npre-selection) (At-Sel) hardware. The MM result is directly fed into\nthe bits selector hardware for ultra-low bit quantization. The result\nis stored in the on-chip buffer for candidate pre-selection computa-\ntion (utilizing LUT hardware for approximate distance calculation).\nThe approximate distance output and address are then fed to the\nPeng and Huang, et al.\nfor i = 1 to Ks.dim2:\nfor j = 1 to Ks.dim1:\n#pragma HLS PIPELINE II = 1\n#pragma HLS UNROLL factor = p\nSrowi, j += Qrowi, iêKs\nT\nj, i\nif i == Ks.dim2\nSrowi, j ê =ğŸ/ ğ\nSrowi, j = mask(Srowi, j)\nSrowi, j = eSrowi, j\nLoop fusion\nQrowi\nKs\nclk1\nclk2\nâ€¦â€¦\nâ€¦â€¦\nâ€¦â€¦\nâ€¦â€¦\nâ€¦â€¦\nclk15\nclk16\nLoop 1: Srowi, 1=0.04\nLoop 2: Srowi, 2=0.05\nLoop 3: Srowi, 3=0.13\nLoop 4: Srowi, 1=0.18\nLoop 5: Srowi, 2=0.47\nLoop 6: Srowi, 3=0.48\nLoop 7: Srowi, 1=0.53\nLoop 8: Srowi, 2=1.02\nLoop 9: Srowi, 3=0.83\nLoop 10: Srowi, 1= \nLoop 11: Srowi, 2= \nLoop 12: Srowi, 3= \n1.5\n1.8\n0.0\n3 X XX\n3 XXX\n32 4 5 . 1\n32 4 5 . 1\n32 4 5 . 1\nLoad buffer\n0.1 0.7 0.5 0.3\n0.4 0.2 0.7 0.9\n0.5 0.6 1.1 0.7\n1.3 0.5 0.7 0.8\nSrowi Store buffer1.5 1.8 0.0\nLoop fusion (p = 1)\nâ€¦â€¦\nâ€¦â€¦\nâ€¦â€¦\nFigure 4: An attention kernel fusion example.\nmerge sort hardware for high throughput (II=1) scalable Top-ğ‘˜sort\n[29]. The Top-ğ‘˜ results (e.g., index and value pairs) are stored back\nto HBM for inter-stage buffering. Stage 2 is attention computation\n(At-Comp) and Stage 3 is feedforward (FdFwd). Stage 2 is divided\ninto three sub-stages and implemented with the intra-layer coarse-\ngrained pipeline to enhance hardware utilization. Stage 2.1, the data\nloading stage, utilizes the Top-ğ‘˜ results from stage 1 to choose the\nattention candidates. As discussed in Section. 3.2, each ğ‘„ğ‘Ÿğ‘œğ‘¤ğ‘– have\nselected candidates ğ¾ğ‘ ğ‘– and ğ‘‰ğ‘ ğ‘– matrix for attention computation.\nStage 2.2 is implemented with loop fusion and is composed of oper-\nators 4 to 6 in Fig. 3. Stage 2.3 is composed ofğ‘ğ‘– = ğ‘†ğ‘–Â·ğ‘‰/ğ‘ ğ‘¢ğ‘š(ğ‘†ğ‘–)\noperation for each row ğ‘–. The double-buffers added between stages\nbuffer the data produced/consumed by the previous/current stage\nfor coarse-grained pipelining. Stage 2.3 conduct the final MM oper-\nation between ğ‘ğ‘– and ğ‘‰ Stage 3 (feedforward) is composed of MM,\nadd, layer normalization, and GELU unit.\nIn stage 2, We divide the softmax 6 in Fig. 3 into two operations:\nexponent calculation and normalization. We leverage the FPGAâ€™s\nfine-grained pipelining characteristic to fuse multiple attention\noperators (i.e., 5 to 6.1 ) into a single loop. An example is given in\nFig. 4, the scaling, mask, and exponential operations are conducted\nat the last loop iterations. ğ‘„ğ‘Ÿğ‘œğ‘¤ğ‘– is the processed row and ğ¾ğ‘  is\nthe corresponding selected candidate for the attention. ğ¾ğ‘ .ğ‘‘ğ‘–ğ‘š1\nand ğ¾ğ‘ .ğ‘‘ğ‘–ğ‘š2 are the size of 1st and 2st dimension of ğ¾ğ‘ . ğ‘†ğ‘Ÿğ‘œğ‘¤ğ‘– is\nthe processed attention score result. Thanks to the reconfigurable\narchitecture of FPGAs, we enable fusing the loops with different\niteration trip counts while GPU designs (e.g., TensorRT) only can\nfuse specific loops (e.g., scale + matrix multiply). Both Stage 1 and\n3 leverage on-chip memory and proper loop allocation to mitigate\ncommunication bottlenecks between the on and off-chip memory.\n4.2 Length-aware Scheduling Algorithm\nDifferent length sequences tasks consume different computational\nresources and have different latency, which introduces pipeline\nbubbles due to irregular and unpredictable dataflow. Because all\noperators have ğ‘‚(ğ‘›)complexity where n is sequence length, we\nproposed a novel length-aware coarse-grained pipeline algorithm to\ndynamically adjust the hardware resource allocation to illuminate\nthe pipeline bubble. We leverage FPGA characteristics to adjust the\nresource allocation according to stagesâ€™ computation complexity,\neliminate redundant computation and achieve ultra-high through-\nput. We propose the techniques detailed below.\nResource Scheduling Algorithm. The slowest stage constrains\nthe throughput of the coarse-grained pipeline. We first develop a\nEncoder coarse-grained stage allocation algorithm (Algorithm 1)\nto schedule operators efficiently. For original operator graph ğº =\nAlgorithm 1: Encoder coarse-grained Stage Allocation.\nInput: Encoder operator graph ğº = (ğ‘‰,ğ¸ ), Encoder operator weight set ğ‘Š(ğ‘‰,ğ‘ ğ‘ğ‘£ğ‘”),\nand Encoder priority set ğ‘ƒ(ğ‘‰,ğ‘ ğ‘ğ‘£ğ‘”);\nOutput: operator subgraph of each Encoder computation stage ğºğ‘˜ = (ğ‘‰ğ‘˜,ğ¸ğ‘˜);\nTraverse ğº = (ğ‘‰,ğ¸ )and compute priority set ğ‘ƒ(ğ‘‰,ğ‘ ğ‘ğ‘£ğ‘”)for the Encoder;\nğ‘˜ â†1,ğ‘ (ğ‘‰)â†{ 1}, ğº1 â†ğ‘£1; // add a operator to a new stage\nforeach ğ‘£ğ‘– âˆˆV in decreasing order of ğ‘ƒ(ğ‘£,ğ‘ ğ‘ğ‘£ğ‘”)do\nforeach ğ‘â€²(ğ‘£ğ‘—)âˆˆ ğºğ‘˜ do\nğ‘â€²(ğ‘£ğ‘—)â† ğ‘(ğ‘£ğ‘—)Â·âŒˆ\nğ‘Š(ğ‘£ğ‘—,ğ‘ ğ‘ğ‘£ğ‘”)\nğ‘Š(ğ‘£ğ‘–,ğ‘ ğ‘ğ‘£ğ‘”)âŒ‰;\nend\nif resource constraints are satisfied then\nğºğ‘— â†ğ‘£; // add a operator to current stage\nğ‘(ğ‘‰)â† ğ‘â€²(ğ‘‰); // update operator parallelisms\nelse\nğ‘˜,ğ¾ â†ğ‘˜+1;\nğºğ‘˜ â†ğ‘£ğ‘–; // add a operator to a new stage\nend\nend\nreturn {ğº1,ğº2,...,ğº ğ¾};\n(ğ‘‰,ğ¸ ), each vertex ğ‘£ğ‘– âˆˆğ‘‰ represents an operator and the edge ğ‘’ğ‘–ğ‘—\nrepresents the data dependency between ğ‘£ğ‘– and ğ‘£ğ‘—. Each vertex ğ‘£ğ‘–\nhas a weight ğ‘Š(ğ‘¤ğ‘–,ğ‘ ğ‘ğ‘£ğ‘”)which is the associated arithmetic com-\nputational complexity. It takes the ğº and Encoder operator weight\nset ğ‘Š(ğ‘‰,ğ‘ ğ‘ğ‘£ğ‘”)at average sequence length ğ‘ ğ‘ğ‘£ğ‘”, and Encoder op-\nerator priority set at average sequence length ğ‘ƒ(ğ‘‰,ğ‘ ğ‘ğ‘£ğ‘”)as input\nand outputs operator subgraph of each Encoder computation stage\nğºğ‘˜ = (ğ‘‰ğ‘˜,ğ¸ğ‘˜)shown in Eq. 1, where ğ‘Š(ğ‘£ğ‘–,ğ‘ ğ‘ğ‘£ğ‘”)and ğ‘ƒ(ğ‘£ğ‘–,ğ‘ ğ‘ğ‘£ğ‘”)\ndenote their value at vertex (ğ‘£ğ‘–, ğ‘ ğ‘ğ‘£ğ‘”). To fully utilize the resources\nof a certain FPGA chip for sequence length adaptive design, we\nfurther adjust the operator parallelism ğ‘(ğ‘£ğ‘–,ğ‘ ğ‘–)for intra coarse-\ngrained pipeline stages and enumerate pipeline replication factor\nğ‘…(ğºğ‘˜,ğ‘ ğ‘–)to obtain the optimal setting with the help of analytical\nperformance and resource models.\nğ‘ƒ(ğ‘£ğ‘–,ğ‘ ğ‘ğ‘£ğ‘”)=\nï£±ï£´ï£´ ï£²\nï£´ï£´ï£³\nğ‘Š(ğ‘£ğ‘–,ğ‘ ğ‘ğ‘£ğ‘”)+ max\nğ‘£ğ‘—âˆˆğ‘†ğ‘¢ğ‘ğ‘(ğ‘£ğ‘–)\nğ‘ƒ(ğ‘£ğ‘—,,ğ‘ ğ‘ğ‘£ğ‘”), ğ‘£ğ‘– â‰  ğ‘£ğ‘ ğ‘–ğ‘›ğ‘˜\nğ‘Š(ğ‘£ğ‘ ğ‘–ğ‘›ğ‘˜,ğ‘ ), otherwise\n(1)\nLength-aware Coarse-grained Pipeline Algorithm. We then\ndevelop a length adaptive resource scheduling method to dynami-\ncally patch the pipeline bubbles for a batch of tasks with different\nsequence lengths. The effectiveness of the proposed scheduling\nmethod relies on the fact that all operators have O(n) complex-\nity. The batch inputs are sorted and processed according to the\ndecreasing order of length, under the control of a dedicated state\nmachine (three states shown in Fig. 2(b):ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘€ğ‘€ , ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ğ´ğ‘¡ğ‘¡ğ‘’ğ‘› and\nğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ğ¹ğ¹ ) during its Encoder activation period. The state machines\ndynamically allocate hardware resources (stages and buffers) to\neliminate pipeline bubbles and ensure a high hardware utilization\nfor batch sentences with varying lengths. Each stage has almost\n100% utilization, and there is no pipeline bubble. We could signifi-\ncantly reduce the latency (denoted as \"saved\").\nA length-aware scheduling timing diagram example is given in\nFig. 5, where the batch size is 5, and the input sequence length\nvaries from 72 to 140. The batch inputs are sorted and fed into\nthe Encoder coarse-grained stages in decreasing order of sequence\nlength. Fig. 5(a) shows how each of the Encoder coarse-grained\nstages processes each sequence input. Most attention-based models\nhave multiple Encoder layers, so the batch input is processed by\nthe layer order. Fig. 5(b) shows the hardware resource occupation\nof Encoder coarse-grained stages. With the state machine-based\nA Length Adaptive Algorithm-Hardware Co-design of\nTransformer on FPGA Through Sparse Attention and Dynamic Pipelining\nMM1|At-Sel1\nAt-Comp1\nI2\nI1\nI3\nI4\nI5 MM5|At-Sel5 At-Comp5 FdFwd5\nMM4|At-Sel4 At-Comp4 FdFwd4\nMM3|At-Sel3 At-Comp3 FdFwd3\nMM2|At-Sel2 At-Comp2 FdFwd2\nMM1|At-Sel1 At-Comp1 FdFwd1\nSaved\nSaved\nSaved\nMM5|At-Sel5\nAt-Comp5\nMM4|At-Sel4\nAt-Comp4\nMM3|At-Sel3\nAt-Comp3\nMM2|At-Sel2\nAt-Comp2\nMM1|At-Sel1\nAt-Comp1\nFdFwd5FdFwd4FdFwd3FdFwd2FdFwd1\nMM & At-Sel:  \nAt-Comp:  \nFdFwd:  \nMM1.1 Sel1.1 W1.1R1.1\nMM1.2 Sel1.2 W1.2R1.2\nMM1.3 Sel1.3 W1.3R1.3\nâ€¦\nAt2.1.1 At2.1.2 W2.1R2.1\nW2.2R2.2\nW3.3R3.3\nâ€¦At2.2.1 At2.2.2\nAt3.3.1 At3.3.2\nFF3.1 W3.1R3.1\nFF3.1 W3.1R3.1\nFF3.1R3.1\nâ€¦\nW3.1\nSorted batch input:\n140\n100\n82\n78\n72\nAttention\nI1 I2 I3 I4 I5\nâ€¦â€¦\nâ€¦â€¦\nMM1|At-Sel1 At-Comp1\nEncoder layer 1 Encoder layer 2\nâ€¦â€¦\nâ€¦â€¦\nMM2|At-Sel2 At-Comp2\nMM2|At-Sel2\nAt-Comp2\nâ€¦â€¦\n(a)\n(b)\nFigure 5: Length-aware coarse-grained dynamic pipeline algorithm example: (a) timing diagram; (b) hardware utilization.\nscheduling algorithm implemented, the pipeline stages of different\nsequence length inputs and different Encoder layers are patched\ntogether without pipeline bubble, so both stages have almost 100%\nhardware utilization. The intra-layer coarse-grained pipeline is\nimplemented in each stage to exploit the trade-off between spatial\n& temporal data locality and hardware resource occupation. The\ncommunication and computation are overlapped with each other\nthrough coarse-grained pipeline and data prefetching.\n5 EXPERIMENT\nWe evaluate several well-known self-attention centric models to\ndemonstrate the algorithm & hardware design performance. For\nNLP models, we choose four of the most popular ones: BERT-base\n[30], BERT-large, and DistilBERT [31], and RoBERTa [32]. Model\nconfigurations are given in Table 1. These models are self-attention-\ncentric and have a similar structure. For BERT-base, DistilBERT\nand RoBERTa, we run 3 representative datasets to evaluate the\nperformance: SQuAD v1.1 [33], RTE [34], and MRPC [35]. For BERT-\nlarge, we run SQuAD v1.1 for evaluation. The minimum sequence\nlength, average sequence, and maximum sequence of those datasets\nare also given. Max/Avg ratio also corresponds to the computational\noverhead introduced through padding.\nTable 1: Model & evaluation dataset.\nModel Layers Hidden dim Num. of Heads\nDistilBERT 6 768 12\nBERT-base, RoBERTa 12 768 12\nBERT-large 24 1024 16\nEvaluation dataset Avg Max Max/Avg\nSQuAD v1.1 177 821 4.6\nRTE 68 253 3.7\nMRPC 53 86 1.6\nThe FPGA hardware design and evaluation are conducted on\nthe Alveo U280 platform. We also evaluate the hardware perfor-\nmance on CPU, edge GPU, and GPU server platforms for cross-\nplatform comparison: Intel(R) Xeon(R) Gold 5218 CPU, Jetson TX2,\nand Quadro RTX 6000. The FPGA design is conducted on software\nversion Vivado 2020.1, whereas GPU and CPU design is completed\non Pytorch 1.10.0 and Transformers 4.13.0.dev0.\n5.1 Sparse Attention Accuracy Evaluation\nWe evaluate the models mentioned above and datasets on the Top-ğ‘˜\nsparse attention algorithm. The state-of-the-art models are quan-\ntized into 8 bits fixed-point representation without accuracy drop\n[36]. The ğ‘„ & ğ¾ quantization is conducted based on 1-bit quantiza-\ntion, which is a sign function. For the sparse attention algorithm, the\nquantized models are directly used without model fine-tuning. For\nSQuAD v1.1 and MRPC datasets, we use the F1 score as our accuracy\nmeasure. For the RTE dataset, the raw accuracy is reported. Experi-\nmented are conducted on the corresponding validation dataset.\nFig. 6 shows the accuracy test of evaluated models and datasets.\nWe choose ğ‘˜ value from 10 to 50 to assess the effectiveness of\nsparse attention, where the ğ‘˜ value determines the degree of ap-\nproximation for sparse attention computation. Generally, smaller ğ‘˜\nindicates aggressive approximation and leads to a higher accuracy\ndrop. For most of the evaluation, Top-10 sparse attention lead to\nnon-negligible performance degradation. Top-30 provides a good\ntrade-off between the accuracy drop and sparsity ratio, whereas all\nevaluations have less than 2% accuracy drop. With a Top-30 sparse\nattention, the attention computation complexity can be reduced by\nmore than 80% in average.\n5.2 Cross-platform Throughput Evaluation\nBased on the model accuracy evaluation, we choose a sweet point\nğ‘˜ = 30 for Top-ğ‘˜ pre-selection, then we mapped models into FPGA\nhardware coarse-grained stages through Algorithm 1. We exploit\nthe design space to maximize the hardware throughput and CTC ra-\ntio for the hardware design. The attainable FPGA design frequency\nis 200 MHz, and most of the hardware resources (BRAM, FF, LUT)\nconsumption are congested inside the SLR0 of the Alveo U280 board\nsince the only SLR0 is connected to HBM channels. HBM channels\nprovide a maximum of 460 GB/s bandwidth. The batch size is set\nas 16 to maximize the hardware utilization.\nFor the FPGA platform, 8 bits fixed-point number multiply &\naccumulate consumes 1 DSP unit. And there are 3000 DSP units\nwithin the SLR0 in total. So the maximum attainable computation\nthroughput of the FPGA platform is 1.2 TFLOPS. Thanks to the\nreconfiguration structure, the FPGA design can efficiently map\nvariable sequence length computation into hardware through the\nscheduling algorithm discussed in Sec. 4.2. The FPGA design can\nsurpass GPU server performance in all the models & tasks evalua-\ntion through efficient scheduling. BERT-base on SQuAD v1.1, RTE,\nand MRPC, and BERT-large model on SQuAD v1.1 are used for hard-\nware design demonstration. DistilBERT and RoBERTa has similar\nstructure and thus similar hardware performance to BERT-base.\nThe end-to-end hardware throughput comparison is given in\nFig. 7a. FPGA baseline indicates the FPGA design without length-\nware scheduling and sparse attention algorithm implemented. The\nsequence length is padded to the maximum sequence length for the\nCPU and GPU design to evaluate the tasks. The geomean speedup of\nthe FPGA length-aware sparse attention design is 80.2Ã—, 41.3 Ã—, 2.6\nPeng and Huang, et al.\nBERT-base\nSQuAD v1.1\nBERT-base\nRTE\nBERT-base\nMRPC\nBERT-large\nSQuAD v1.1\nDistilBERT\nSQuAD v1.1\nDistilBERT\nRTE\nDistilBERT\nMRPC\nRoBERTa\nSQuAD v1.1\nRoBERTa\nRTE\nRoBERTa\nMRPC\n20\n40\n60\n80\n100Accuracy (%)\nBaseline Top-50 Top-40 Top-30 Top-20 Top-10\nFigure 6: Accuracy evaluation of Top- ğ‘˜ sparse attention.\nBERT-base\n SQuAD v1.1\nBERT-base\n RTE\nBERT-base\n MRPC\nBERT-large\n SQuAD v1.1\nGeomean\n10 1\n100\n101\n102\nSpeed up (X)\nCPU Jetson TX2 RTX 6000 FPGA Baseline FPGA length-aware\n(a) End to end cross platform throughput comparison\nBERT-base\n SQuAD v1.1\nBERT-base\n RTE\nBERT-base\n MRPC\nBERT-large\n SQuAD v1.1\nGeomean\n10 1\n101\n103\nSpeed up (X)\nCPU Jetson TX2 RTX 6000 FPGA Baseline FPGA sparse attention (b) Cross platform attention throughput comparison\nFigure 7: Cross platform hardware evaluation.\nÃ—, 3.1Ã—than CPU, edge GPU, GPU server, and FPGA baseline design.\nAs for the self-attention computation, the hardware throughput is\nalso recorded during the evaluation, and the corresponding speedup\nis given in Fig. 7b. The FPGA sparse attention hardware achieves\na geomean speedup of 1073 Ã—, 550 Ã—, 35 Ã—, 41 Ã—than CPU, edge\nGPU, GPU server, and FPGA baseline design. Our FPGA design\nachieves an equivalent hardware throughput of 3.6 TFLOPS on\n8 bits fixed-point operations with length-aware scheduling and\nsparse attention algorithm implemented.\n5.3 Cross-work Energy Efficiency Comparison\nWe further conduct cross-platform and cross-work energy efficiency\ncomparison between GPU baseline, GPU design optimized through\nCUBLAS GEMM routine [18], FPGA [37], and ASIC implementa-\ntions [12, 13] of Transformer accelerator. The result is shown in\nTable 2. Our FPGA surpasses GPU baseline implementation and\nexisting state-of-the-art FPGA design in terms of both throughput\nand energy efficiency with an acceptable accuracy drop. With the\nlength-aware scheduling and sparse attention algorithms imple-\nmented, our work has a comparable energy efficiency to existing\nASIC designs dedicated to transformer acceleration.\nTable 2: Energy efficiency & throughput comparison.\nWork/platform Throughput Energy eff. Accuracy drop\n(GOPS) (GOP/J) (average)(%)\nGPU RTX 6000 1380 8 1.8\nGPU V100: E.T. [18] 7550 25 2.1\nOurs FPGA 3600 102 1.8\nFPGA design [37] 76 N/A 3.8\nASIC: A3 [12] 221 269 1.6\nASIC: SpAtten [13] 360 382 1.1\n6 CONCLUTION\nIn this paper, we propose a hardware-friendly sparse attention algo-\nrithm through query and key values quantization, where we bring\ndown the complexity of self-attention from ğ‘‚(ğ‘›2)to ğ‘‚(ğ‘›). We\ndevelop a length-aware hardware scheduling algorithm to accom-\nmodate variable sequence length computation into coarse-grained\npipeline stages without pipeline bubbles. To alleviate the off-chip\nmemory access, we further develop an attention kernel fusion to\nprocess the attention computation. We exploit temporal data local-\nity through the on-chip buffer to enhance the CTC ratio and push\nthe hardware design to the computation roof. Experimental results\nshow that we achieve 80.2 Ã—and 2.6 Ã—speedup compared to CPU\nand GPU implementation with 1.8% accuracy loss. Our FPGA design\nhas more than 4 Ã—higher energy efficiency than GPU accelerator\noptimized through CUBLAS GEMM routine and has comparable\nenergy efficiency compared to state-of-the-art ASIC designs.\nACKNOWLEDGEMENT\nThis work was in part supported by the NSF CRII Award No. 2000722,\nCAREER Award No. 2011236, No. 2006748, No. 2046102, and DOE\nAward No. 66150. Any opinions, findings and conclusions, or rec-\nommendations expressed in this material are those of the authors\nand do not necessarily reflect the views of the funding agencies.\nREFERENCES\n[1] Nikolas Adaloglou. Why Multi-head Self Attention Works: Math, Intuitions and\n10+1 Hidden Insights. https://theaisummer.com/self-attention/, 2021. [Online;\naccessed August 30, 2021].\n[2] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the Relationship\nBetween Self-Attention and Convolutional Layers. In ICLR, 2019.\n[3] Ashish Vaswani et al. Attention is All You Need. InAdvances in neural information\nprocessing systems , pages 5998â€“6008, 2017.\n[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning\nfor Image Recognition. In CVPR, 2016.\n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth\n16Ã—16 Words: Transformers for Image Recognition at Scale. In ICLR, 2021.\n[6] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. Swin Transformer: Hierarchical Vision Transformer using\nShifted Windows. In CVPR, 2021.\n[7] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-End Human Pose and Mesh\nReconstruction with Transformers. In CVPR, 2021.\n[8] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know What You Donâ€™t Know:\nUnanswerable Questions for SQuAD. In ACL, pages 784â€“789, 2018.\n[9] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\nand Amr Ahmed. Big Bird: Transformers for Longer Sequences. In H. Larochelle,\nM. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, NeurIPS, volume 33,\npages 17283â€“17297. Curran Associates, Inc., 2020.\n[10] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. BP-\nTransformer: Modelling Long-range Context via Binary Partitioning. arXiv\npreprint arXiv:1911.04070 , 2019.\n[11] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The Efficient\nTransformer. In ICLR, 2019.\nA Length Adaptive Algorithm-Hardware Co-design of\nTransformer on FPGA Through Sparse Attention and Dynamic Pipelining\n[12] Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H Oh, Yeonhong Park,\nYoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W Lee, and Deog-\nKyoon Jeong. AË† 3: Accelerating Attention Mechanisms in Neural Networks with\nApproximation. In 2020 HPCA , pages 328â€“341. IEEE, 2020.\n[13] Hanrui Wang, Zhekai Zhang, and Song Han. SpAtten: Efficient Sparse Attention\nArchitecture with Cascade Token and Head Pruning. In2021 HPCA, pages 97â€“110.\nIEEE, 2021.\n[14] Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soosung Kim, Hyunji Choi, Sung Jun\nJung, and Jae W Lee. ELSA: Hardware-Software Co-design for Efficient, Light-\nweight Self-Attention Mechanism in Neural Networks. In ISCA. IEEE, 2021.\n[15] NVIDIA. TensorRT. Retrived from https://developer.nvidia.com/tensorrt. Online;\naccessed: October 6, 2021.\n[16] Merity Stephen, Xiong Caiming, Bradbury James, and Socher Richard. Pointer\nSentinel Mixture Models. ICLR, 2016.\n[17] Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue,\nAnthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, et al.\nTransformers: State-of-the-art natural language processing. In EMNLP, pages\n38â€“45, 2020.\n[18] Shiyang Chen, Shaoyi Huang, Santosh Pandey, Bingbing Li, Guang R Gao, Long\nZheng, Caiwen Ding, and Hang Liu. Et: re-thinking self-attention for trans-\nformer models on gpus. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis , pages 1â€“18, 2021.\n[19] Shaoyi Huang, Shiyang Chen, Hongwu Peng, Daniel Manu, Zhenglun Kong,\nGeng Yuan, Lei Yang, Shusen Wang, Hang Liu, and Caiwen Ding. Hmc-tran:\nA tensor-core inspired hierarchical model compression for transformer-based\ndnns on gpu. In Proceedings of the 2021 on Great Lakes Symposium on VLSI , pages\n169â€“174, 2021.\n[20] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. Turbotransformers: an\nefficient gpu serving system for transformer models. In Proceedings of the 26th\nACM SIGPLAN Symposium on Principles and Practice of Parallel Programming ,\npages 389â€“402, 2021.\n[21] Hongwu Peng, Balaji Narayanasamy, Asif Imran Emon, Zhao Yuan, Rongxuan\nZhang, and Fang Luo. Selective digital active emi filtering using resonant con-\ntroller. In 2020 IEEE International Symposium on Electromagnetic Compatibility &\nSignal/Power Integrity (EMCSI) , pages 632â€“639. IEEE, 2020.\n[22] Zeke Wang, Hongjing Huang, Jie Zhang, and Gustavo Alonso. Benchmarking\nHigh Bandwidth Memory on FPGAs. arXiv preprint arXiv:2005.04324 , 2020.\n[23] Panjie Qi, Yuhong Song, Hongwu Peng, Shaoyi Huang, Qingfeng Zhuge, and\nEdwin Hsing-Mean Sha. Accommodating transformer onto fpga: Coupling\nthe balanced model compression and fpga-implementation optimization. In\nProceedings of the 2021 on Great Lakes Symposium on VLSI , pages 163â€“168, 2021.\n[24] Hongwu Peng, Shanglin Zhou, Scott Weitze, Jiaxin Li, Sahidul Islam, Tong Geng,\nAng Li, Wei Zhang, Minghu Song, Mimi Xie, et al. Binary complex neural\nnetwork acceleration on fpga. In 2021 IEEE 32nd International Conference on\nApplication-specific Systems, Architectures and Processors (ASAP) , pages 85â€“92.\nIEEE, 2021.\n[25] Hongwu Peng, Shaoyi Huang, Tong Geng, Ang Li, Weiwen Jiang, Hang Liu,\nShusen Wang, and Caiwen Ding. Accelerating transformer-based deep learning\nmodels on fpgas using column balanced block pruning. In2021 22nd International\nSymposium on Quality Electronic Design (ISQED) , pages 142â€“148. IEEE, 2021.\n[26] Geng Yuan, Zhiheng Liao, Xiaolong Ma, Yuxuan Cai, Zhenglun Kong, Xuan Shen,\nJingyan Fu, Zhengang Li, Chengming Zhang, Hongwu Peng, et al. Improving\ndnn fault tolerance using weight pruning and differential crossbar mapping for\nreram-based edge ai. In 2021 22nd International Symposium on Quality Electronic\nDesign (ISQED) , pages 135â€“141. IEEE, 2021.\n[27] Shaoyi Huang, Ning Liu, Yueying Liang, Hongwu Peng, Hongjia Li, Dongkuan\nXu, Mimi Xie, and Caiwen Ding. An automatic and efficient bert pruning for edge\nai systems. In 2022 23rd International Symposium on Quality Electronic Design\n(ISQED), pages 1â€“6. IEEE, 2022.\n[28] Hongwu Peng, Balaji Narayanasamy, Asif Imran Emon, Zhao Yuan, Mustafeez Ul\nHassan, and Fang Luo. Design and implementation of selective active emi filter\nwith digital resonant controller. In 2020 IEEE Energy Conversion Congress and\nExposition (ECCE) , pages 5855â€“5861. IEEE, 2020.\n[29] Hongwu Peng et al. Optimizing FPGA-based Accelerator Design for Large-Scale\nMolecular Similarity Search. In ICCAD, 2021.\n[30] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nIn 2019 ACL, pages 4171â€“4186, 2019.\n[31] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT,\na Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter. arXiv preprint\narXiv:1910.01108, 2019.\n[32] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.\n[33] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,\n000+ questions for machine comprehension of text. In EMNLP, 2016.\n[34] Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. Recognizing textual\nentailment: Rational, evaluation and approachesâ€“erratum. Natural Language\nEngineering, 16(1):105â€“105, 2010.\n[35] Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential\nparaphrases. In Third International Workshop on Paraphrasing . AFNLP, 2005.\n[36] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun\nLiu. TernaryBERT: Distillation-aware Ultra-low Bit BERT. In 2020 EMNLP , pages\n509â€“521, 2020.\n[37] Panjie Qi, Edwin Hsing-Mean Sha, Qingfeng Zhuge, Hongwu Peng, Shaoyi\nHuang, Zhenglun Kong, Yuhong Song, and Bingbing Li. Accelerating framework\nof transformer by hardware design and model compression co-optimization. In\n2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD) ,\npages 1â€“9. IEEE, 2021."
}