{
  "title": "Zero-Shot Recommendation as Language Modeling",
  "url": "https://openalex.org/W4225886166",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2755994633",
      "name": "Damien Sileo",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A5007376249",
      "name": "Wout Vossen",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A5011180543",
      "name": "Robbe Raymaekers",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A2755994633",
      "name": "Damien Sileo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5007376249",
      "name": "Wout Vossen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5011180543",
      "name": "Robbe Raymaekers",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964341035",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2726499916",
    "https://openalex.org/W3208163087",
    "https://openalex.org/W2016420487",
    "https://openalex.org/W2219888463",
    "https://openalex.org/W2054141820",
    "https://openalex.org/W2905461678",
    "https://openalex.org/W2809112621",
    "https://openalex.org/W3046039946",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2158515176",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W3103410128"
  ],
  "abstract": null,
  "full_text": "Zero-Shot Recommendation as Language Modeling\nDamien Sileo[0000−0002−3274−291X], Wout V ossen, and Robbe Raymaekers\nKU Leuven, Belgium\ndamien.sileo@kuleuven.be\nAbstract. Recommendation is the task of ranking items (e.g. movies or prod-\nucts) according to individual user needs. Current systems rely on collaborative ﬁl-\ntering and content-based techniques, which both require structured training data.\nWe propose a framework for recommendation with off-the-shelf pretrained lan-\nguage models (LM) that only used unstructured text corpora as training data. If\na user u liked Matrix and Inception, we construct a textual prompt, e.g. ”Movies\nlike Matrix, Inception,<m>” to estimate the afﬁnity between u and m with LM\nlikelihood. We motivate our idea with a corpus analysis, evaluate several prompt\nstructures, and we compare LM-based recommendation with standard matrix fac-\ntorization trained on different data regimes. The code for our experiments is pub-\nlicly available1.\n1 Introduction\nRecommender systems predict an afﬁnity score between users and items. Current rec-\nommender systems are based on content-based ﬁltering (CB), collaborative ﬁltering\ntechniques (CF), or a combination of both. CF recommender systems rely on (U SER ,\nITEM , I NTERACTION ) triplets. CB relies on (I TEM , F EATURES ) pairs. Both system\ntypes require a costly structured data collection step. Meanwhile, web users express\nthemselves about various items in an unstructured way. They share lists of their favorite\nitems and ask for recommendations on web forums, as in (1)2 which hints at a similarity\nbetween the enumerated movies.\n(1) Films like Beyond the Black Rainbow, Lost River, Suspiria, and The Neon Demon.\nThe web also contains a lot of information about the items themselves, like synopsis or\nreviews for movies. Language models such as GPT-2 [14] are trained on large web cor-\npora to generate plausible text. We hypothesize that they can make use of this unstruc-\ntured knowledge to make recommendations by estimating the plausibility of items be-\ning grouped together in a prompt. LM can estimate the probability of a word sequence,\nP(w1,...wn). Neural language models are trained over a large corpus of documents: to\ntrain a neural network, its parameters Θare optimized for next word prediction likeli-\nhood maximization over k-length sequences sampled from a corpus. The loss writes as\nfollows:\nLLM = −log\n∑\ni\nP(wi|wi−k....wi−1; Θ) (1)\n1 https://colab.research.google.com/drive/...?usp=sharing\n2 https://www.reddit.com/r/MovieSuggestions/...lost river/\narXiv:2112.04184v1  [cs.CL]  8 Dec 2021\n2 D. Sileo et al.\nWe rely on existing pretrained language models. To make a relevance prediction ,\nwe build a prompt for each user:\npu,i = Movies like<m1>,...<mn>,<mi> (2)\nwhere <mi> is the name of the movie mi and <m1...mn> are those of randomly\nordered movies liked by u. We then directly use ˆRu,i = PΘ(pu,i) as a relevance score\nto sort items for user u.\nOur contributions are as follow (i) we propose a model for recommendation with\nstandard LM; (ii) we derive prompt structures from a corpus analysis and compare\ntheir impact on recommendation accuracy; (iii) we compare LM-based recommenda-\ntion with next sentence prediction (NSP) [12] and a standard supervised matrix factor-\nization method [9,15].\n2 Related work\nLanguage models and recommendationPrevious work leveraged language modeling\ntechniques to perform recommendations. However, they do not rely on natural lan-\nguage: they use sequences of user/item interactions, and treat these sequences as sen-\ntences to leverage the architectures inspired by NLP, such as Word2Vec [7,1,4,11] or\nBERT [19].\nZero-shot prediction with language modelsNeural language models have been used\nfor zero-shot inference on many NLP tasks [14,2]. For example, they manually con-\nstruct a prompt structure to translate text, e.g. Translate english to french : ”cheese”\n=>, and use the language model completions to ﬁnd the best translations. Petroni et\nal. [13] show that masked language models can act as a knowledge base when we use\npart of a triplet as input, e.g. Paris in<mask>. Here, we apply LM-based prompts to\nrecommendation.\nHybrid and zero-shot recommendationThe cold start problem [17], i.e. dealing with\nnew users or items is a long-standing problem in recommender systems, usually ad-\ndressed with hybridization of CF-based and CB-based systems. Previous work [20,10,5,6]\nintroduced models for zero-shot recommendation, but they use zero-shot prediction\nwith a different sense than ours. They train on a set of (U SER , I TEM , I NTERACTION )\ntriplets, and perform zero-shot predictions on new users or items with known attributes.\nThese methods still require (U SER , I TEM , I NTERACTION ) or (I TEM , F EATURES ) tu-\nples for training. To our knowledge, the only attempt to perform recommendations with-\nout such data at all is from Penha et al. [12] who showed that BERT [3] next sentence\nprediction (NSP) can be used to predict the most plausible movie after a prompt. NSP\nis not available in all language models and requires a speciﬁc pretraining. Their work is\ndesigned as a probing of BERT knowledge about common items, and lacks comparison\nwith a standard recommendation model, which we here address.\nZero-Shot Recommendation as Language Modeling 3\n3 Experiments\n3.1 Setup\nDataset We use the standard the MovieLens 1M dataset [8] with 1M ratings from\n0.5 to 5, 6040 users, and 3090 movies in our experiments. We address the relevance\nprediction task3, so we consider a rating ras positive if r ≥4.0, as negative if ≤2.5\nand we discard the other ratings. We select users with at least 21 positive ratings and\n4 negative ratings and thus obtain 2716 users. We randomly select 20% of them as test\nusers4. 1 positive and 4 negative ratings are reserved for evaluation for each user, and\nthe goal is to give the highest relevance score to the positively rated item. We use 5\npositive ratings per user unless mentioned otherwise. We remove the years from the\nmovie titles and reorder the articles ( a, the) in the movie titles provided in the dataset\n(e.g. Matrix, The (1999)→The Matrix).\nEvaluation metric We use the mean average precision at rank 1 (MAP@1) [18] which\nis the rate of correct ﬁrst ranked prediction averaged over test users, because of its\ninterpretability.\nPretrained language modelsIn our experiments we use the GPT-2 [14] language mod-\nels, which are publicly available in several sizes. GPT-2 is trained with LM pretraining\n(equation 1) on the WebText corpus [14], which contains 8 million pages covering var-\nious domains. Unless mentioned otherwise, we use the GPT-base model, with 117M\nparameters.\n3.2 Mining prompts for recommendation\n3-6 gram #Count\n<m>and <m> 387\n<m>, <m>, <m> 232\nMovies like<m> 196\n<m>, <m>, <m>, <m> 85\nMovies similar to<m> 25\nTable 1: Occurrence counts of 3-6 grams\nthat contain movie names in the Reddit\ncorpus. <m>denotes a movie name.\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35\nMAP@1\n    <m >,...,<m >\n   <m >,...,<m >\n <m >,...,<m >\n    <m >,...,<m >\n    \nFig. 1: Comparison of LM recommen-\ndations MAP@1 with different prompt\nstructures.\nWe analyze the Reddit comments from May 20155 to ﬁnd out how web users mention\nlists of movies in web text. This analysis will provide prompt candidates for LM-based\n3 Item relevance could be mapped to ratings but we do not address rating prediction here.\n4 Training users are only used for the matrix factorization baseline.\n5 https://www.kaggle.com/reddit/reddit-comments-may-2015\n4 D. Sileo et al.\nrecommendations. We select comments where a movie name of the MovieLens dataset\nis present and replace movies with a <m>tag. This ﬁltered dataset of comments has a\nsize of >900kwords. We then select the most frequent pattern with at least three words,\nas shown in table 1. Movie names are frequently used in enumerations. The patterns\nMovies like<m>and Movies similar toconﬁrm that users focus on the similarity of\nmovies.\nFigure 1 shows that prompt design is important but not critical for high accuracy.\nOur corpus-derived prompts signiﬁcantly outperform if you like<m1...mn>, you will\nlike <mi>used in [12]. We will use<m1...mn>,<mi>in the remaining of the paper\ndue to its superior results and its simplicity.\n3.3 Effect of the number of ratings per test user\nWe investigate the effect of the number of mentioned movies in prompts. We expect\nthe accuracy of the models in making recommendations to increase when they get more\ninfo about movies a user likes. We compare the recommendation accuracy on the same\nusers 0,1,2,3,5,10,15 or 20 movies per prompt.\n0 2 5 7 10 12 15 17 20\n#Movies per user\n0.25\n0.30\n0.35\n0.40MAP@1\nFig. 2: MAP@1 of LM models with a varying number of movies per user sampled in\nthe input prompt.\nFigure 2 shows that increasing the number of ratings per user has diminishing re-\nturns and lead to increasing instability, so specifyingn≈5 seems to lead to the best re-\nsults with the least user input. After 5 items, adding more items might make the prompt\nZero-Shot Recommendation as Language Modeling 5\nless natural, even though the LM seems to adapt when the number of items keeps in-\ncreasing. It is also interesting to note that when we use an empty prompt, accuracy is\nabove chance level because the LM captures some information about movie popularity.\n3.4 Comparison with matrix factorization and NSP\nWe now use a matrix factorization as a baseline, with the Bayesian Personalised Rank-\ning algorithm (BPR) [15]. Users and items are mapped to drandomly initialized latent\nfactors, and their dot product is used as a relevance score trained with ranking loss. We\nuse [16] implementation with default hyperparameters 6 d = 10 and a learning rate of\n0.001.\nWe also compare GPT-2 LM to BERT next sentence prediction [12] which models\nafﬁnity scores with ˆRu,i = BERTNSP(pu,<mi>), where pu is a prompt containing\nmovies liked by u. BERT was pretrained with contiguous sentence prediction task [3]\nand Penha et al. [12] proposed to use it as a way to probe BERT for recommendation\ncapabilities.\n0 100 200 300 400 500\n#Users in training set\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50MAP@1\nBPR [15]\nBERT-base NSP [12]\nBERT-large NSP [12]\nGPT-2-base\nGPT-2-medium\nFig. 3: MAP@1 for BPR models with increasing numbers of users compared the zero-\nshot language models (with 0 training user). BERT-base and BERT-large respectively\nhave 110M and 340M parameters. GPT-2-base and GPT-2-medium have 117M and\n345M parameters.\n6 https://cornac.readthedocs.io/en/latest/models.html#\nbayesian-personalized-ranking-bpr , we experimented with other hyperpa-\nrameter conﬁgurations but did not observe signiﬁcant changes.\n6 D. Sileo et al.\nFigure 3 shows that the proposed LM-based recommendation signiﬁcantly outper-\nforms BERTNSP. We explain the difference by the fact that LM are generative and ac-\ntually model text likelihood, while next sentence prediction is discriminative and can\nbe based on simple discursive coherence features. It is also interesting to note that LM-\nbased models outperform matrix factorization when there are few users, i.e < 50 and\n<100 for BASE and MEDIUM GPT-2, which demonstrates that LM-based recommenda-\ntion is viable for cold start regimes. Using models larger than theBASE versions lead to\nbetter results, however when we evaluated with larger versions (we did not perform the\nfull experiments due to memory limitations), we did not see additional improvement,\nwhich could be explained by overﬁtting.\n3.5 Qualitative analysis of generations\nUp until there, we have used LM to score the likelihood of sequences. LM can also be\nused directly for text generation, unlike BERT. We here show LM-generated prompt\ncompletions randomly sampled in our dataset, using greedy decoding.\nPrompt (P1): Forrest Gump, Blade Runner, Modern Times, Amelie, Lord of the Rings\nThe Return of the King, Shaun of the Dead, Alexander, Pan’s Labyrinth, Cashback,\nAvatar:\nCompletion (C1): 3, The Hunger Games: Mockingjay Part 2, King Arthur, A Feast for\nCrows, The Hunger Games: Catching Fire, Jackass, Jackass 2, King Arthur\nPrompt (P2): Independence Day, Winnie the Pooh and the Blustery Day, Raiders of\nthe Lost Ark, Star Wars Episode VI - Return of the Jedi, Quiet Man, Game, Labyrinth,\nReturn to Oz, Song of the South, Matrix:\nCompletion (C2): and many more. The list can be read by clicking on the relevant\nsection at the left of the image. To access the list of releases\nSome prompts, i.e. (P1) generate valid movie names, but others, like (P2), do not.\nLM-based recommender do need a post-processing to match movie names in the possi-\nble sampled generations.\n4 Conclusion\nWe showed that standard language models can be used to perform item recommen-\ndations without any adaptation and that they are competitive with supervised matrix\nfactorization when the number of users is very low (less than 100 users). LM can there-\nfore be used to kickstart recommender systems if items are frequently discussed in the\ntraining corpora. Further research could explore ways to adjust LM for recommendation\npurposes or to combine LM with matrix factorization into hybrid systems. Another way\nto use of our ﬁndings would be to generate movie recommendation datasets by mining\nweb data which could feed standard supervised recommendation techniques.\nZero-Shot Recommendation as Language Modeling 7\n5 Acknowledgements\nThis work is part of the CALCULUS project, which is funded by the ERC Advanced\nGrant H2020-ERC-2017 ADG 7885067.\nReferences\n1. Barkan, O., Koenigstein, N.: Item2vec: Neural item embedding for collaborative ﬁltering.\nIn: 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing\n(MLSP). pp. 1–6 (2016). https://doi.org/10.1109/MLSP.2016.7738886\n2. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan,\nT., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,\nI., Amodei, D.: Language models are few-shot learners (2020)\n3. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. In: Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, V olume 1 (Long and Short Papers). pp. 4171–\n4186. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019).\nhttps://doi.org/10.18653/v1/N19-1423, https://aclanthology.org/N19-1423\n4. Devooght, R., Bersini, H.: Long and short-term recommendations with recurrent neu-\nral networks. p. 13–21. UMAP ’17, Association for Computing Machinery, New York,\nNY , USA (2017). https://doi.org/10.1145/3079628.3079670, https://doi.org/10.\n1145/3079628.3079670\n5. Ding, H., Ma, Y ., Deoras, A., Wang, Y ., Wang, H.: Zero-shot recommender systems (2021)\n6. Feng, P.J., Pan, P., Zhou, T., Chen, H., Luo, C.: Zero shot on the cold-start prob-\nlem: Model-agnostic interest learning for recommender systems. In: Proceedings of\nthe 30th ACM International Conference on Information & Knowledge Management. p.\n474–483. CIKM ’21, Association for Computing Machinery, New York, NY , USA (2021).\nhttps://doi.org/10.1145/3459637.3482312, https://doi.org/10.1145/3459637.\n3482312\n7. Gu `ardia-Sebaoun, E., Guigue, V ., Gallinari, P.: Latent trajectory modeling: A light and ef-\nﬁcient way to introduce time in recommender systems. In: Proceedings of the 9th ACM\nConference on Recommender Systems. pp. 281–284 (2015)\n8. Harper, F.M., Konstan, J.A.: The movielens datasets: History and context. ACM Trans. Inter-\nact. Intell. Syst. 5(4) (Dec 2015). https://doi.org/10.1145/2827872, https://doi.org/\n10.1145/2827872\n9. Koren, Y ., Bell, R., V olinsky, C.: Matrix factorization techniques for recommender systems.\nComputer 42(8), 30–37 (2009)\n10. Li, J., Jing, M., Lu, K., Zhu, L., Yang, Y ., Huang, Z.: From zero-shot learning to cold-\nstart recommendation. Proceedings of the AAAI Conference on Artiﬁcial Intelligence\n33(01), 4189–4196 (Jul 2019). https://doi.org/10.1609/aaai.v33i01.33014189, https://\nojs.aaai.org/index.php/AAAI/article/view/4324\n11. Li, Z., Zhao, H., Liu, Q., Huang, Z., Mei, T., Chen, E.: Learning from history and present:\nNext-item recommendation via discriminatively exploiting user behaviors. In: Proceedings\nof the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Min-\ning. pp. 1734–1743 (2018)\n7 https://calculus-project.eu/\n8 D. Sileo et al.\n12. Penha, G., Hauff, C.: What does bert know about books, movies and music? probing bert\nfor conversational recommendation. In: Fourteenth ACM Conference on Recommender\nSystems. p. 388–397. RecSys ’20, Association for Computing Machinery, New York,\nNY , USA (2020). https://doi.org/10.1145/3383313.3412249, https://doi.org/10.\n1145/3383313.3412249\n13. Petroni, F., Rockaschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y ., Miller, A.: Language\nmodels as knowledge bases? In: Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th International Joint Conference on Nat-\nural Language Processing (EMNLP-IJCNLP). pp. 2463–2473. Association for Computa-\ntional Linguistics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/v1/D19-1250,\nhttps://aclanthology.org/D19-1250\n14. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language\nmodels are unsupervised multitask learners (2019), https://openai.com/blog/\nbetter-language-models/\n15. Rendle, S., Freudenthaler, C., Gantner, Z., Schmidt-Thieme, L.: Bpr: Bayesian personalized\nranking from implicit feedback. p. 452–461. UAI ’09, AUAI Press, Arlington, Virginia, USA\n(2009)\n16. Salah, A., Truong, Q.T., Lauw, H.W.: Cornac: A comparative framework for multimodal\nrecommender systems. Journal of Machine Learning Research 21(95), 1–5 (2020)\n17. Schein, A.I., Popescul, A., Ungar, L.H., Pennock, D.M.: Methods and metrics\nfor cold-start recommendations. In: Proceedings of the 25th Annual International\nACM SIGIR Conference on Research and Development in Information Retrieval.\np. 253–260. SIGIR ’02, Association for Computing Machinery, New York, NY ,\nUSA (2002). https://doi.org/10.1145/564376.564421, https://doi.org/10.1145/\n564376.564421\n18. Schr ¨oder, G., Thiele, M., Lehner, W.: Setting goals and choosing metrics for recommender\nsystem evaluations. In: UCERSTI2 workshop at the 5th ACM conference on recommender\nsystems, Chicago, USA. vol. 23, p. 53 (2011)\n19. Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., Jiang, P.: Bert4rec: Sequential rec-\nommendation with bidirectional encoder representations from transformer. In: Proceed-\nings of the 28th ACM International Conference on Information and Knowledge Man-\nagement. p. 1441–1450. CIKM ’19, Association for Computing Machinery, New York,\nNY , USA (2019). https://doi.org/10.1145/3357384.3357895, https://doi.org/10.\n1145/3357384.3357895\n20. V olkovs, M., Yu, G., Poutanen, T.: Dropoutnet: Addressing cold start in recommender sys-\ntems. In: Guyon, I., Luxburg, U.V ., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,\nGarnett, R. (eds.) Advances in Neural Information Processing Systems. vol. 30. Curran As-\nsociates, Inc. (2017), https://proceedings.neurips.cc/paper/2017/file/\ndbd22ba3bd0df8f385bdac3e9f8be207-Paper.pdf",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8338557481765747
    },
    {
      "name": "Shot (pellet)",
      "score": 0.6762038469314575
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.672615647315979
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.40654948353767395
    },
    {
      "name": "Natural language processing",
      "score": 0.3862397372722626
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3812744617462158
    },
    {
      "name": "Programming language",
      "score": 0.34758231043815613
    },
    {
      "name": "Linguistics",
      "score": 0.1675683856010437
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99464096",
      "name": "KU Leuven",
      "country": "BE"
    }
  ]
}