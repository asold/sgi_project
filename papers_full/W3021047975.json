{
    "title": "Can BERT Reason? Logically Equivalent Probes for Evaluating the Inference Capabilities of Language Models.",
    "url": "https://openalex.org/W3021047975",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5070613964",
            "name": "Pei Zhou",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5038426107",
            "name": "Rahul Khanna",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5013041264",
            "name": "Bill Yuchen Lin",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5058408154",
            "name": "Daniel E. Ho",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5009408707",
            "name": "Xiang Ren",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5073443117",
            "name": "Jay Pujara",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W2016089260",
        "https://openalex.org/W2970161131",
        "https://openalex.org/W2119409989",
        "https://openalex.org/W1525961042",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W2970863760",
        "https://openalex.org/W2626639386",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2000900121",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2970862333"
    ],
    "abstract": "Pre-trained language models (PTLM) have greatly improved performance on commonsense inference benchmarks, however, it remains unclear whether they share a human's ability to consistently make correct inferences under perturbations. Prior studies of PTLMs have found inference deficits, but have failed to provide a systematic means of understanding whether these deficits are due to low inference abilities or poor inference robustness. In this work, we address this gap by developing a procedure that allows for the systematized probing of both PTLMs' inference abilities and robustness. Our procedure centers around the methodical creation of logically-equivalent, but syntactically-different sets of probes, of which we create a corpus of 14,400 probes coming from 60 logically-equivalent sets that can be used to probe PTLMs in three task settings. We find that despite the recent success of large PTLMs on commonsense benchmarks, their performances on our probes are no better than random guessing (even with fine-tuning) and are heavily dependent on biases--the poor overall performance, unfortunately, inhibits us from studying robustness. We hope our approach and initial probe set will assist future work in improving PTLMs' inference abilities, while also providing a probing set to test robustness under several linguistic variations--code and data will be released.",
    "full_text": null
}