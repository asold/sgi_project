{
    "title": "DTCC: Multi-level dilated convolution with transformer for weakly-supervised crowd counting",
    "url": "https://openalex.org/W4362588015",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2606444372",
            "name": "Zhuangzhuang Miao",
            "affiliations": [
                "Beijing University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2106380920",
            "name": "Yong Zhang",
            "affiliations": [
                "Beijing University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2097438177",
            "name": "Yuan Peng",
            "affiliations": [
                "Taisei (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2963926252",
            "name": "Haocheng Peng",
            "affiliations": [
                "Beijing University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2162226292",
            "name": "Baocai Yin",
            "affiliations": [
                "Beijing University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2120815373",
        "https://openalex.org/W1987118352",
        "https://openalex.org/W2520826941",
        "https://openalex.org/W1978232622",
        "https://openalex.org/W2058907003",
        "https://openalex.org/W3203845557",
        "https://openalex.org/W4214665794",
        "https://openalex.org/W3176458063",
        "https://openalex.org/W2914913933",
        "https://openalex.org/W2963693541",
        "https://openalex.org/W4225264236",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W6600654476",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3206836360",
        "https://openalex.org/W2463631526",
        "https://openalex.org/W2741077351",
        "https://openalex.org/W2964209782",
        "https://openalex.org/W3004672782",
        "https://openalex.org/W3214228243",
        "https://openalex.org/W4312613051",
        "https://openalex.org/W3175725565",
        "https://openalex.org/W6600995197",
        "https://openalex.org/W3189653508",
        "https://openalex.org/W2514654788",
        "https://openalex.org/W4385257175",
        "https://openalex.org/W3081099313",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W6601865935",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W2072232009",
        "https://openalex.org/W2967069910",
        "https://openalex.org/W3035193053",
        "https://openalex.org/W2967776630",
        "https://openalex.org/W2996703886",
        "https://openalex.org/W3097407159",
        "https://openalex.org/W2962720716",
        "https://openalex.org/W2963035940",
        "https://openalex.org/W2798490576",
        "https://openalex.org/W3175630421",
        "https://openalex.org/W3035307763",
        "https://openalex.org/W2982007926",
        "https://openalex.org/W2895051362",
        "https://openalex.org/W2886443245",
        "https://openalex.org/W4377101865",
        "https://openalex.org/W6601756569",
        "https://openalex.org/W3176047859",
        "https://openalex.org/W2982014038",
        "https://openalex.org/W2914020154",
        "https://openalex.org/W4362597616"
    ],
    "abstract": "Abstract Crowd counting provides an important foundation for public security and urban management. Due to the existence of small targets and large density variations in crowd images, crowd counting is a challenging task. Mainstream methods usually apply convolution neural networks (CNNs) to regress a density map, which requires annotations of individual persons and counts. Weakly-supervised methods can avoid detailed labeling and only require counts as annotations of images, but existing methods fail to achieve satisfactory performance because a global perspective field and multi-level information are usually ignored. We propose a weakly-supervised method, DTCC, which effectively combines multi-level dilated convolution and transformer methods to realize end-to-end crowd counting. Its main components include a recursive swin transformer and a multi-level dilated convolution regression head. The recursive swin transformer combines a pyramid visual transformer with a fine-tuned recursive pyramid structure to capture deep multi-level crowd features, including global features. The multi-level dilated convolution regression head includes multi-level dilated convolution and a linear regression head for the feature extraction module. This module can capture both low- and high-level features simultaneously to enhance the receptive field. In addition, two regression head fusion mechanisms realize dynamic and mean fusion counting. Experiments on four well-known benchmark crowd counting datasets (UCF_CC_50, ShanghaiTech, UCF_QNRF, and JHU-Crowd++) show that DTCC achieves results superior to other weakly-supervised methods and comparable to fully-supervised methods.",
    "full_text": "Computational Visual Media\nhttps://doi.org/10.1007/s41095-022-0313-5 Vol. 9, No. 4, December 2023, 859–873\nResearch Article\nDTCC: Multi-level dilated convolution with transformer for\nweakly-supervised crowd counting\nZhuangzhuang Miao1, Yong Zhang1 (\u0000 ), Yuan Peng2, Haocheng Peng1, and Baocai Yin1\nc⃝ The Author(s) 2023.\nAbstract Crowd counting provides an important\nfoundation for public security and urban management.\nDue to the existence of small targets and large den-\nsity variations in crowd images, crowd counting is a\nchallenging task. Mainstream methods usually apply\nconvolution neural networks (CNNs) to regress a\ndensity map, which requires annotations of individual\npersons and counts. Weakly-supervised methods can\navoid detailed labeling and only require counts as\nannotations of images, but existing methods fail to\nachieve satisfactory performance because a global\nperspective ﬁeld and multi-level information are usually\nignored. We propose a weakly-supervised method,\nDTCC, which eﬀectively combines multi-level dilated\nconvolution and transformer methods to realize end-\nto-end crowd counting. Its main components include\na recursive swin transformer and a multi-level dilated\nconvolution regression head. The recursive swin trans-\nformer combines a pyramid visual transformer with a\nﬁne-tuned recursive pyramid structure to capture deep\nmulti-level crowd features, including global features.\nThe multi-level dilated convolution regression head\nincludes multi-level dilated convolution and a linear\nregression head for the feature extraction module. This\nmodule can capture both low- and high-level features\nsimultaneously to enhance the receptive ﬁeld. In\naddition, two regression head fusion mechanisms realize\ndynamic and mean fusion counting. Experiments on\nfour well-known benchmark crowd counting datasets\n1 Beijing Key Laboratory of Multimedia and Intelligent\nSoftware Technology, Beijing Institute of Artiﬁcial\nIntelligence, Faculty of Information Technology,\nBeijing University of Technology, Beijing 100124,\nChina. E-mail: Z. Miao, mzhuangzhuang2023@163.com;\nY. Zhang, zhangyong2010@bjut.edu.cn ( \u0000 ); H. Peng,\nhaocheng.peng@ucdconnect.ie; B. Yin, ybc@bjut.edu.cn.\n2 Taiji Computer Corporation Ltd., China. E-mail:\nyuan.peng@outlook.com.\nManuscript received: 2022-03-22; accepted: 2022-09-12\n(UCF CC 50, ShanghaiTech, UCF QNRF, and JHU-\nCrowd++) show that DTCC achieves results superior\nto other weakly-supervised methods and comparable to\nfully-supervised methods.\nKeywords crowd counting; transformer; dilated con-\nvolution; global perspective ﬁeld; pyramid\n1 Introduction\nCrowd counting is an important topic in the ﬁeld of\ncrowd analysis: the aim is to estimate the number\nof people in an image. With increasing population\nand urbanization, there are more and more crowd-\ncontaining localities: e.g., subway platforms, bus\nstations, airports, tourist attractions, and shopping\nmalls. Crowd congestion can occur during peak\nhours, with a serious negative impact on public\nsafety. Accurate crowd counting can help to avoid\ncrowd congestion, and plays an essential role in public\nsecurity, abnormal situation warning, and pedestrian\ncontrol.\nSigniﬁcant progress has been made in crowd\ncounting via computer vision through years of relevant\nresearch. As Fig. 1 shows, existing crowd counting\nmethods can be classiﬁed as depending on object\ndetection, density estimation, point-supervision, and\nweak-supervision. Deep learning-based methods can\nalso be divided into CNN-based and transformer-\nbased methods. In an earlier study, researchers used\nobject detection to solve the crowd counting problem\n[1, 2]. However, such methods do not work for dense\nscenes: severe occlusion and complex backgrounds\ntypically occur in such cases, leading to unsatisfactory\nresults. To solve these problems, some regression-\nbased approaches have appeared. They usually learn\nlow-level features (e.g., texture features, edge feature,\netc.) using traditional algorithms and map features to\n859\n\n860 Z. Miao, Y. Zhang, Y. Peng, et al.\nFig. 1 Mainstream crowd counting methods can be classiﬁed as\ndepending on (a) object detection, (b) density estimation, (c) point-\nsupervision, and (d) weak-supervision.\nthe number of persons in the crowd through regression\nmodels. However, these methods ignore crowd\ndistribution information in the image. To make use of\nit, Lempitsky and Zisserman [ 3] proposed a method\nbased on density estimation which learns a linear\nor nonlinear mapping between image features and\ndensity maps. Nonetheless, the features extracted by\ntraditional methods cannot capture deep-level feature\nrepresentations. Therefore, Walach and Wolf [ 4]\nand others [ 5, 6] have used CNN-based approaches\nto regress density maps. The powerful image feature\nextraction capability of CNNs enables these methods\nto achieve better results. Nowadays, CNN-based\nmethods have become mainstream for dense scenes.\nRecent CNN-based fully-supervised methods\n[7–9] achieve excellent results; they require both\na count and annotation of individual people as\nsupervision. These methods generate the true\ndensity map from individual annotation and regress\nthe predicted density map. Nevertheless, detailed\nindividual labeling is tedious, limiting its application.\nTherefore, it is fundamental to ﬁnd a method\nthat can obtain precise results simply using crowd\ncounts as annotations. Corresponding deep learning-\nbased weakly-supervised methods have thus emerged\n[10, 11]. However, these existing weakly-supervised\nmethods usually ignore the extraction of global\nreceptive ﬁelds and multi-level information; they\npredict the total count directly from the entire image,\nso the global receptive ﬁeld is important for these\nmethods. A CNN is limited to extracting a global\nreceptive ﬁeld without using a density map due to\nthe characteristics of local feature extraction. In\n2021, a transformer was introduced to the weakly-\nsupervised crowd counting task [\n12]. The global\nattention of the corresponding network can eﬀectively\novercome the limited receptive ﬁeld of CNN-based\nmethods. However, this work cannot eﬀectively\nextract multi-level information about the target.\nFigure 2 shows an image with targets of diﬀerent sizes\nin the two regions marked in red. Thus, for weakly-\nsupervised crowd counting, multi-level information is\nvery important. Suﬃcient features cannot be learned\nto regress counting if multi-level information is not\nproperly utilized.\nThis paper proposes DTCC, a pyramid vision\ntransformer network for weakly-supervised crowd\ncounting. It comprises a transformer feature extra-\nction module and a multi-level dilated convolution\nregression module. The main contributions of this\npaper are:\n(1)\nDTCC, a multi-level transformer dilated\nconvolution weakly-supervised framework, which\nis capable of accurate end-to-end crowd counting.\n(2) A multi-level crowd information feature\nextraction module for dense prediction. The\nﬁnal feature representation can distinguish\nbetween dense crowd heads and larger scale\ncrowd heads. The overall framework is a\nrecursive pyramid structure, which combines a\npyramid vision transformer backbone network\nand a ﬁne-tuned recursive pyramid structure\n(recursive ﬁne-FPN) to obtain multi-level\ncontextual crowd information.\n(3) A multi-level dilated convolution regression\nmodule, which can enhance the receptive ﬁeld for\nfeatures and capture stronger global features. It\nis combined with two networks, DTCC-Dynamic\nFig. 2 An image containing a crowd with people at diﬀerent scales.\n\nDTCC: Multi-level dilated convolution with transformer for weakly-supervised crowd counting 861\nand DTCC-Mean, as multi-level regression heads\nadapted to diﬀerent crowd scenes.\n(4) Experiments on four well-known benchmark\ndatasets demonstrating better accuracy than\nother weakly-supervised methods, with com-\npetitive results to mainstream fully-supervised\nmethods.\n2 Related work\n2.1 Background\nCrowd counting approaches can be divided into two\ncategories: fully-supervised and weakly-supervised\nmethods. Fully-supervised methods use a density\nmap as supervisory information to train the model,\nwhich requires point-level annotations of the crowd.\nWeakly-supervised methods only need a count of the\ncrowd. Mainstream crowd counting methods usually\nutilize CNN to regress a density map [23, 26, 44]. The\nsuccess of transformer-based methods in computer\nvision tasks such as image classiﬁcation [ 13–15],\nobject detection [13, 16, 17], and image segmentation\nsuggests use of a transformer framework as the\nbackbone network for crowd counting.\n2.2 Fully-supervised crowd counting\nCNN-based methods regress a density map and obtain\nthe total number of people in the crowd by integrating\nthe density map. Zhang et al. [\n18] proposed a network\nwith three diﬀerently-sized receptive ﬁelds, which\nwas capable of learning multi-level crowd features.\nThis method replaces the fully connected layer by\na convolution layer and can modify the size of the\ninput images. Sam et al. [\n19] proposed a selective\nCNN with several convolution kernels of diﬀerent\nsizes as the density map regression head; a selection\nclassiﬁer selects the optimal regression head for the\ninput to predict the result. Li et al. [ 20] presented\na deeper framework with convolution layers as the\nbackbone, based on a combination of VGG16 and\ndilated convolution layers to expand the receptive\nﬁeld. It extracted deeper features without losing\nresolution. Later advances considered new density\nmap loss functions with better results: Ma et al.\n[\n21] illustrated a point-supervised loss function for\ncrowd count estimation, converting a sparse point\nlabeling into a ground truth density map using\na Gaussian kernel. This was used as a learning\ntarget to train the density map estimator. Liu\net al. [ 22] used a swin transformer as the backbone\nnetwork and a top–down fusion mechanism to fully\nutilize the various spatial information extracted from\ndiﬀerent stages of the model. Abousamrad et al. [ 24]\nreported a method that uses topological constraints\ninstead of binary region maps to compute L2 loss\nfunctions for the head and background region. Only\na few methods are based on transformer networks\nto realize fully-supervised crowd counting. Among\nthem, Liang et al. [\n23] proposed an elegant, end-\nto-end crowd localization transformer that solves\nthe task using a regression-based paradigm. Sun\net al. [ 25] investigated the role of global contextual\ninformation in crowd counting. This method extracts\nglobal information from overlapping image blocks\nusing a transformer, and adds contextual tags to the\ninput sequence. In addition, a token attention module\nand regression token module are proposed to predict\nthe total number of people in images. Gao et al. [ 26]\nshowed a dilated convolutional transformer method,\nintroducing a window-based vision transformer for\ncrowd counting.\nIn summary, fully-supervised crowd counting\nmethods have been extensively studied and have\nachieved good results. However, the application of\nfully-supervised methods to speciﬁc scenes is very\nlimited, because they require individual annotation to\ngenerate density maps, and it is tedious and diﬃcult\nto perform accurate individual annotation for dense\nscenes.\n2.3 Weakly-supervised crowd counting\nWeakly-supervised counting methods just rely on\ncrowd counts for training. Shang et al. [ 27] proposed\nan end-to-end CNN architecture that exploits shared\ncomputation over overlapping regions. Wang et al.\n[28] presented a novel and eﬃcient counter, which\nexplores embedded global dependency modeling and\ntotal count regression by designing a multi-granularity\nregressor. Lei et al. [ 29] suggested a new multi-\nassisted task training strategy, MATT, which learns\nfrom a few images with individual annotations and\nmany simply with counts to obtain more accurate\npredictions. Transformers have an inherent advantage\nin weakly-supervised crowd counting, since they\ncan enhance global information about features and\ncapture contextual knowledge. TransCrowd [ 12] was\nthe ﬁrst transformer-based crowd counting framework,\nwhich reformulates the counting problem from a\n\n862 Z. Miao, Y. Zhang, Y. Peng, et al.\nsequential perspective to a counting perspective.\nCCTrans [31] is applicable to both fully-supervised\nand weakly-supervised data, and uses Twins [ 32] as\na feature extraction framework. It combines the\nfeatures of multiple stages of the Twins network\nthrough multi-level dilated convolutions for feature\nfusion, finally predicting the number of people through\na regression head. Savner and Kanhangad [52] proposed\nan architecture based on a pyramid vision transformer\nnetwork to extract multi-scale features with global\ncontext. Wang et al. [\n53] proposed a joint CNN\nand transformer network based on weakly-supervised\nlearning to reduce the number of parameters and\novercome the problem of target segmentation.\nWithout annotations of individuals, weakly-\nsupervised crowd counting is challenging. Existing\nweakly-supervised methods cannot extract suﬃcient\nglobal features and multi-level information, leading\nto the loss of collective semantic information and a\nfailure to provide rich global features for the ﬁnal\nregression. Using a global attention mechanism\nprovides a new way to design an eﬀective weakly-\nsupervised crowd counting model.\n3 Method\n3.1 Approach\nExisting weakly-supervised crowd counting methods\nhave two problems to be solved: extraction of a\nglobal receptive ﬁeld and utilization of multi-level\ninformation. Therefore, this paper introduces a swin\ntransformer to capture global features. A feature\npyramid structure is also introduced to enrich the\nmulti-level feature representation, so that single-level\nfeatures contain rich multi-level information. In\naddition, since the window attention mechanism of\nthe swin transformer processes image patches, this\nalleviates the problem of uneven distribution of the\ncrowd to a certain extent. To enhance the receptive\nﬁeld of features, a multi-level dilated convolution\nmodule is designed for the swin transformer, to\nsolve the problem of local domain loss by dilated\nconvolutions. Based on the above ideas, we propose\nDTCC, an end-to-end weakly-supervised method for\ncrowd counting, which can provide accurate crowd\ncounts based only on crowd count annotations.\n3.2 Network architecture of DTCC\nThe framework of DTCC is shown in Fig. 3. The\ninput image is divided into blocks of the same\nsize and converted into a 1D sequence for the\nswin transformer. DTCC is composed of two main\nmodules. The recursive swin transformer feature\nextraction module consists of a swin transformer [ 13]\nand the recursive ﬁne-FPN. The multi-level dilated\nconvolution regression head module consists of a\nmulti-level dilated convolution and a linear regression\nhead. The counting results from multi-level feature\nregression are given diﬀerent weights to obtain the\nﬁnal count.\nFor feature extraction, the swin transformer is\ncomposed of a transformer-encoder. Therefore,\nthe 2D image structure must be converted to a\n1D sequence required as input to a transformer-\nencoder. This network is commonly used in natural\nlanguage processing, but can also get good results\nFig. 3 Network architecture of DTCC.\n\nDTCC: Multi-level dilated convolution with transformer for weakly-supervised crowd counting 863\nin computer vision by using ViT [ 30] to solve the\ninput problem.\nThe input image to the swin transformer is deﬁned\nas X ∈RH×W×C, where H, W, C represent the\nheight and width of the image and the number of\nchannels, respectively. Firstly, the image is divided\ninto image patches of size\nP ×P. Thus the image\nof size H ×W ×C is divided into patches X′∈\nRN×P×P×C, where N = (H/P)×(W/P). Then, each\nimage patch is linearly transformed into a sequence\nof length P2 ×C for input to the model. Therefore,\nthe input image is transformed into Z ∈RN×T by\npreprocessing, where T = P2 ×C. The feature\nextraction backbone network of swin transformer\ncalculates local window attention, and performs image\nembedding operations in the window for each image\npatch. It uses two operations for dividing windows.\nThe image is ﬁrst divided into image patches, and\nthen the image patches are divided by moving\nwindows. This method moves non-overlapping local\nwindows, which reduces computational complexity to\nlinear in image size.\n3.3 Recursive swin transformer\n3.3.1 Approach\nThe recursive swin transformer (RST) eﬀectively\ncombines the pyramid structure transformer back-\nbone with the recursive ﬁne-FPN. For crowd counting\nin dense prediction tasks, the swin transformer has\na pyramid structure similar to a CNN which can\nextract multi-level feature representations of images.\nThe self-attention mechanism of transformer solves\nthe disadvantages of local feature extraction in CNNs\nand can capture stronger global features. In addition,\nthe window attention mechanism of swin transformer\nis executed on image patches, which alleviates the\nproblem of uneven distribution of the crowd to a\ncertain extent. Recursive ﬁne-FPN iteratively fuses\nmulti-level features to observe multiple views of the\nimage, and produces richer feature representations\nfor the regression head.\n3.3.2 Transformer backbone network\nThe transformer backbone network inputs Z ∈RN×T\nto the transformer encoder and uses a multi-headed\nattention mechanism to extract features (since the\nvisual task does not require the encoding part,\nonly the decoding part is utilized). The transformer\nencoder consists of multi-head self-attention (MSA)\nand MLP layers, while each layer uses residual\nconnections and layer normalization (LN). The overall\nprocess is given by\nZ′\nl−1 = MSA (LN (Zl−1)) + Zl−1 (1)\nZl = MLP (LN (Z′\nl)) + Z′\nl−1 (2)\nwhere Z′\nl−1 is the output of MSA.\nSelf-attention is the most important contribution\nof the transformer. The attention mechanism can\nassign diﬀerent weights to input information when\naggregating information. Brieﬂy, the mechanism\ncan learn the attention between a sequence and\nother sequences, which is a weight matrix from an\noperational point of view. There are three concepts\nin attention: the query ( Q), the key ( K), and the\nvalue ( V ). Each sequence outputs Q, K, and V\nby multiplying by the WQ, WK, and WV matrices\nwhere K and V exist in pairs. The attention between\ndiﬀerent sequence pairs for each subsequence Q is\nAttention (Q, K, V)=SoftMax\n(\nQKT/\n√\nd\n)\nV (3)\nwhere d is the size of the query and key.\nThe original transformer structure also adds a\npositional encoding to provide location information.\nViT [ 30] does not use the default ﬁxed positional\nencoding and instead sets the positional encoding to a\nset of learnable 1D sequences. The position encoding\nused by swin transformer has two diﬀerences: the\nposition encoding is diﬀerent, and it is added to the\nattention matrix. Relative position information is\nused instead of absolute position information. The\nattention can be written as\nAttention (Q, K, V) = SoftMax(QKT/\n√\nd + B)V\n(4)\nwhere B is the relative position bias matrix.\nThe attention mechanism in the transformer is\nmulti-head attention, using h heads to compute\nattention. This allows the model to focus on diﬀerent\naspects of information. The input sequence Z ∈\nRN×T is divided into h sequential inputs of size\nZ ∈RN×d, where T = h ×d. Finally, this module\nconcatenates the output from the h heads and obtains\nthe ﬁnal output using a linear transformer.\nThe swin transformer is a pyramid structure\nthat can handle multiple levels as well as reducing\ncomplexity. This network consists of four transformer\nnetwork layers to calculate local attention, with step\nsizes for the four stages given by P = [4,8,16,32].\nThe swin transformer combines two types of window\n\n864 Z. Miao, Y. Zhang, Y. Peng, et al.\ndivision method which eﬀectively captures both local\nand global attention.\n3.3.3 Recursive ﬁne-FPN\nAs Fig. 4 shows, we add a recursive feature pyramid\nstructure [46] after the transformer; it is inspired by\nthe idea of looking and thinking twice before acting.\nThis network can deliver better semantic information\nthrough feedback connections in the structure of the\nﬁne-FPN. Multi-level features are extracted and fed\nback to the transformer backbone layer to realize\nbottom–up connections for the corresponding network\nlayer. It is important to note that our method uses a\nrecursive feature pyramid network to fuse the features\nfrom diﬀerent stages, which diﬀers from the winner\nof ICCV-VisDrone [22]. This architecture can look at\nimages twice or more, so can better observe detailed\ninformation in a dense crowd image.\nThe ﬁne-FPN solves the multi-level problem\nin crowd counting tasks through simple network\nconnections. The overall display is a bottom–up\nstructure that integrates features at diﬀerent scales.\nEach layer of ﬁne-FPN ﬁrst adjusts the number of\nchannels using 1 ×1 Conv, and up-sampling features\nfrom the previous stage. Next, it performs fusion\nby a simple add operation, and ﬁnally the 3\n×3\nConv is used to eliminate blending eﬀects. ﬁne-FPN\nimproves upon the original FPN: we up-sample the\nfused features after 3 ×3 Conv to give the higher-\nlevel fused features, improving robustness. A two-\nlevel recursive feature pyramid is used in this paper,\ndeﬁned as\nM′= ﬁne FPN(SwinT(M′)) (5)\nM = ﬁne FPN(SwinT(M′+ X)) (6)\nwhere M′is the output of the ﬁrst stage ﬁne-FPN and\nM is the ﬁnal output. We use the same method to\ncombine them as for combining the ﬁne-FPN output\nand the output of the swin transformer.\n3.4 Multi-level dilated convolution regression\nhead\nThe density of people in images varies greatly\nin the crowd counting task, and images contain\nobjects at diﬀerent scales. Therefore, extraction\nof global features is an important foundation for\nweakly-supervised crowd counting. We use a multi-\nlevel dilated convolution regression head to enhance\nthe receptive ﬁeld of features. As Fig. 3(b) shows,\nthe multi-level dilated convolution regression head\n(M-DRH) module consists of multi-level dilated\nconvolution and multi-headed linear regression layers.\nDilated convolution is commonly used in computer\nvision to collect contextual information without\nadding extra parameters, while widening the receptive\nﬁeld. The dilation rate represents the interval used\nin the convolution kernel. When the rate is equal to\n1, the result is the same as ordinary convolution.\nUsing the output of multi-level features from RST,\nthe M-DRH module performs mutli-level dilated\nconvolution for various features. The dilation rate\nis inversely proportional to feature level: [2, 3, 4].\nAt the same time, the M-DRH module avoids the\nproblem of local information loss resulting from\ndilated convolution; the swin transformer has four\nstages which can realize down-sampling to extract\nmulti-level features as in CNN-based crowd counting\nmethods. The down-sampling rate of each stage\nis 2, so the elements are selected at a row- and\nFig. 4 Recursive ﬁne-FPN network architecture.\n\nDTCC: Multi-level dilated convolution with transformer for weakly-supervised crowd counting 865\ncolumn-wise interval of 2. See Fig. 5: K′is the\nnext stage of output to K, and green shows where\nconvolutional operations are performed while white\nmeans that no convolution is performed. Down-\nsampling for the swin transformer merges features\nnear four points. So, K′\n12 is composed of K(1−2)(3−4)\nand K′\n13 is composed of K(1−2)(5−6). We perform\ndilated convolution with dilated rate of 2 and 3\nfor\nK′and K, respectively. K′\n12 does not pass\nthe operation of convolution and K(1−2)(3−4) has\nconvolution operation. Although K(1−2)(5−6) pixel\nblocks are not the operation of convolution, but K′\n13\nhas convolution operation, and so on for the other\nparts. This mechanism ensures that our proposed\nM-DRH is able to avoid the problem of local feature\nloss by dilated convolution.\nSince crowd images contain head information at\nmultiple scales, diﬀerent levels of crowd head feature\nmaps have diﬀerent advantages. Therefore, we use\nmulti-level feature maps to regress results and a\ndynamic layer to learn optimal fusion parameters.\nSpeciﬁcally, an activation function and linear layer\noverlay component are designed to perform regression\non multi-level features simultaneously. We use two\nkinds of fusion mechanism. In Fig. 6(left), we add a\ndynamic layer of parameters to learn fusion weights\nfor the three regression results; this layer contains\nthree learnable parameters. In Fig. 6(right), we\ndirectly average the three regression results to get\nthe ﬁnal result.\n3.5 Loss function\nThe number of people in dense scenes can be relatively\nlarge. However, the L1 loss function commonly used\nin related studies has fold points which can lead to\nFig. 5 Dilated convolution for diﬀerent levels of image patches.\nFig. 6 Two types of regression head for DTCC.\ninstability in the case of large counts. In this paper,\nthe SmoothL1 [ 33] loss function is used to ensure\nsmooth output and enhance robustness; it is less\nlikely to cause gradient explosion. It is given by\nSmoothL1(p, D) =\n{\n0.5x2, |x|< 1\n|x|−0.5, otherwise\n(7)\nwhere x = p −D represents the diﬀerence between\nthe predicted result p and the ground truth D.\nThe feature extraction backbone network outputs\nmulti-level feature representations which solves the\nproblem of target scale change in images. Therefore,\ntwo regression head fusion mechanisms are used in\nthis paper. DTCC-Dynamic uses a dynamic network\nlayer to automatically learn diﬀerent fusion weights,\nwith loss function:\nLD = SmoothL1((ae1 + be2 + ce3), D) (8)\nwhere the multi-level regression values are e1, e2, e3,\nand the output of the dynamic layer is deﬁned as a,\nb, c. DTCC-Mean takes the mean of the predicted\nvalues, so the loss function of DTCC-Mean is\nLM = SmoothL1\n(e1 + e2 + e3\n3 , D\n)\n(9)\n4 Experiments\n4.1 Overview\nIn this section, we evaluate DTCC using several public\ncrowd counting datasets: ShanghaiTech, UCF CC 50,\nUCF QNRF, and JHU-Crowd++. We compare our\nresults to those of both weakly-supervised and fully-\nsupervised methods in Tables 1–3. In addition, results\nof ablation experiments conducted to evaluate each\ncomponent of the proposed framework are shown in\nTables 4–6.\n4.2 Experimental setting\n4.2.1 Datasets\nWe used the following datasets:\n\n866 Z. Miao, Y. Zhang, Y. Peng, et al.\nTable 1 Comparison, in terms of MAE and MSE, of the proposed method to other popular methods on UCF CC 50, ShangHaiA, ShangHaiB,\nUCF QNRF\nMethod Venue Label UCF CC 50 ShanghaiA ShanghaiB UCF QNRF\nLocation Number MAE MSE MAE MSE MAE MSE MAE MSE\nMCNN [35] CVPR16 ✓ ✓ 277.0 426.0 110.2 173.2 26.4 41.3 277.0 426.0\nCMTL [43] AVSS17 ✓ ✓ 322.8 397.9 101.3 152.4 20.0 31.1 252.0 514.0\nSwitching CNN [17] CVPR17 ✓ ✓ 318.1 439.2 90.4 135.4 21.6 33.4 228.0 445.0\nCP-CNN [44] ICCV17 ✓ ✓ 298.8 320.9 73.6 106.4 20.1 30.1 — —\nACSCP [45] CVPR18 ✓ ✓ 291.0 404.6 75.7 102.7 17.2 27.4 — —\nCSRNet [20] CVPR18 ✓ ✓ 266.1 397.5 68.2 115.0 10.6 16.0 — —\nCAN [37] CVPR19 ✓ ✓ 212.2 243.7 62.3 100.0 7.8 12.2 107 183\nPACNN [39] CVPR19 ✓ ✓ 267.9 357.8 66.3 106.4 8.9 13.5 — —\nS-DCNet [40] ICCV19 ✓ ✓ 204.2 301.3 58.3 95.0 6.7 10.7 104.4 176.1\nBL [21] ICCV19 ✓ ✓ 229.3 308.2 62.8 101.8 7.7 12.7 88.7 154.8\nRPNet [47] CVPR20 ✓ ✓ — — 61.2 96.9 8.1 11.6 — —\nADSCNet [38] CVPR20 ✓ ✓ — — 55.4 97.7 6.4 11.3 71.3 132.5\nGL [48] CVPR21 ✓ ✓ — — 61.3 95.4 7.3 11.7 — —\nP2PNet [41] ICCV21 ✓ ✓ 172.7 256.2 52.7 85.1 6.3 9.9 85.3 154.5\nSASNet [54] AAAI21 ✓ ✓ 161.4 234.5 53.5 88.3 6.3 9.9 85.2 147.3\nCCTrans [31] arXiv21 ✓ ✓ 168.7 234.5 52.3 84.9 6.2 9.9 82.8 142.3\nMATT [29] PR21 % ✓ 355.0 550.2 80.1 129.4 11.7 17.5 — —\nTransCrowd [12] SCIS22 % ✓ — — 66.1 105.1 9.3 16.1 97.2 168.5\nCCTrans [31] arXiv21 % ✓ 245.0 343.6 64.4 95.4 7.0 11.5 92.1 158.9\nDTCC-Dynamic (ours∗) — % ✓ 211.1 319.9 60.8 97.0 7.2 10.8 88.7 162.4\nDTCC-Mean (ours∗) — % ✓ 182.9 312.6 64.8 100.0 8.3 12.2 93.2 168.9\nTable 2 Results on the JHU-Crowd++ validation set. Low, Medium, and High refer to images with up to 50, 50–500, and over 500 people,\nrespectively\nMethod Venue Label JHU-Low JHU-Medium JHU-High JHU-Total\nLocation Number MAE MSE MAE MSE MAE MSE MAE MSE\nMCNN [35] CVPR16 ✓ ✓ 90.6 202.9 125.3 259.5 494.9 856.0 160.6 377.7\nCMTL [43] AVSS17 ✓ ✓ 50.2 129.2 88.1 170.7 583.1 986.5 138.1 379.5\nDSSI-Net [49] ICCV19 ✓ ✓ 50.3 85.9 82.4 164.5 436.6 814.0 116.6 317.4\nCAN [37] CVPR19 ✓ ✓ 34.2 69.5 65.6 115.3 336.4 619.7 89.5 239.3\nSANet [50] ECCV18 ✓ ✓ 13.6 26.8 50.4 78.0 397.8 749.2 82.1 272.6\nCSRNet [46] CVPR18 ✓ ✓ 22.2 40.0 49.0 99.5 302.5 669.5 72.2 249.9\nCG-DRCN [36] PAMI20 ✓ ✓ 17.1 44.7 40.8 71.2 317.4 719.8 67.9 262.1\nMBTTBF [55] ICCV19 ✓ ✓ 23.3 48.5 53.2 119.9 294.5 674.5 73.8 256.8\nSFCN [11] CVPR19 ✓ ✓ 11.8 19.8 39.3 73.4 297.3 679.4 62.9 247.5\nBL [21] ICCV19 ✓ ✓ 6.9 10.3 39.7 85.2 279.8 620.4 59.3 229.2\nTransCrowd-Token [12] SCIS22 % ✓ 7.1 10.7 33.3 54.6 302.5 557.4 58.4 201.1\nTransCrowd-GAP [12] SCIS22 % ✓ 6.7 9.5 34.5 55.8 285.9 532.8 56.8 193.6\nDTCC-Dynamic (our*) — % ✓ 4.8 7.0 28.6 44.9 261.2 546.4 51.6 204.1\nDTCC-Mean (our*) — % ✓ 4.6 6.8 29.3 44.7 266.5 566.1 54.0 187.8\n(1) UCF CC 50 [34] consists of 50 images in total,\ndivided into training and validation sets in a ratio\nof 4:1. The dataset contains a small number\nof images and high density variation, with a\nmaximum of 4633 people and a minimum of 96\npeople, with an average count of 1297.\n(2) ShanghaiTechA [ 35] consists of 482 images\nin total, with 300 training images and 182\nvalidation images. The images were randomly\ncrawled from the Internet, so the images have a\nvery wide range of sources. The images contain an\naverage of 501 and a range of 33–3139 people.\n\nDTCC: Multi-level dilated convolution with transformer for weakly-supervised crowd counting 867\nTable 3 Results on the JHU-Crowd++ test set. Low, Medium, and High refer to images with up to 50, 50–500, and over 500 people,\nrespectively\nMethod Venue Label JHU-Low JHU-Medium JHU-High JHU-Total\nLocation Number MAE MSE MAE MSE MAE MSE MAE MSE\nMCNN [35] CVPR16 ✓ ✓ 97.1 192.3 121.4 191.3 618.6 1166.7 188.9 483.4\nCMTL [43] AVSS17 ✓ ✓ 58.5 136.4 81.7 144.7 635.3 1225.3 157.8 490.4\nDSSI-Net [49] ICCV19 ✓ ✓ 53.6 112.8 70.3 108.6 525.5 1047.4 133.5 416.5\nCAN [37] CVPR19 ✓ ✓ 37.6 78.8 56.4 86.2 384.2 789.0 100.1 314.0\nSANet [50] ECCV18 ✓ ✓ 17.3 37.9 46.8 69.1 397.9 817.7 91.1 320.4\nCSRNet [46] CVPR18 ✓ ✓ 22.2 40.0 49.0 99.5 302.5 669.5 72.2 249.9\nCG-DRCN [36] PAMI20 ✓ ✓ 19.5 58.7 38.4 62.7 367.3 837.5 82.3 328.0\nMBTTBF [55] ICCV19 ✓ ✓ 19.2 58.8 41.6 66.0 352.2 760.4 81.8 299.1\nSFCN [11] CVPR19 ✓ ✓ 16.5 55.7 38.1 59.8 341.8 758.8 77.5 297.6\nBL [21] ICCV19 ✓ ✓ 10.1 32.7 34.2 54.5 352.0 768.7 75.0 299.9\nTransCrowd-Token [12] SCIS22 % ✓ 8.5 23.2 33.3 71.5 368.3 816.4 76.4 319.8\nTransCrowd-GAP [12] SCIS22 % ✓ 7.6 16.7 34.8 73.6 354.8 752.8 74.9 295.6\nDTCC-Dynamic (our*) — % ✓ 7.7 19.1 30.9 50.8 296.0 652.3 64.1 254.7\nDTCC-Mean (our*) — % ✓ 8.4 17.3 33.7 55.9 302.1 651.8 66.9 255.1\nTable 4 Numbers of parameters and GFlops used by DTCC and\nvarious mainstream crowd counting methods\nDTCC DCST SASNet TransCrowd\nVenue — CVPR21 AAAI21 SCIS22\nGFlops 218.2 154.8 130.9 49.3\nParams 205.1 252.3 38.9 89.1\nTable 5 Multi-level dilated convolution regression head ablation\nexperiment\nMethod ShanghaiA ShanghaiB\nMAE MSE MAE MSE\nDTCC (w/o M-DRH) 63.8 103.5 8.9 13.3\nDTCC 60.8 97.0 7.2 10.8\nTable 6 Experiment on choice of dilation rates\nDilation rates ShanghaiA ShanghaiB\nMAE MSE MAE MSE\n1, 2, 3 64.9 101.0 7.7 11.5\n2, 3, 4 60.8 97.0 7.2 10.8\n3, 4, 5 62.5 97.2 7.7 11.7\n(3) ShanghaiTechB [ 35] consists of 716 images\nin total, with 400 training images and 316\nvalidation images. These are real images from\nthe streets of Shanghai, captured by road\ncameras. The images contain an average of 123\nand a range of 9–578 people.\n(4) JHU-Crowd++ [36] consists of 4822 images in\ntotal, with 2722 training images, a validation set\nof 500 images, and a test set of 1600 images. This\ndataset has rich image information including\ncount, person center coordinates, head frame\ncoordinates, weather information, and lighting\nconditions. It can also be divided into three\ndatasets according to the number of people\ncontained: JHU-Low, JHU-Medium, and JHU-\nHigh. The images contain an average of 437 and\na range of 2–7286 people.\n(5) UCF QNRF [ 51] consists of 1535 images in\ntotal, with 1201 training images and a validation\nset of 334 images. It contains real scenes\nfrom around the world, including buildings,\nvegetation, sky, and roads, which are important\nfor counting crowds in diﬀerent situations. The\nimages contain an average of 815 and a range of\n49–12,865 people.\n4.2.2 Baselines and compared methods\nIn order to verify the eﬀectiveness of our method,\nwe choose a large number of comparator methods\nincluding mainstream fully-supervised and state-of-\nthe-art weakly-supervised methods. Fully-supervised\nmethods need both person location and count\nannotations, and include CAN [\n37], ADSCNet [ 38],\nPACNN [ 39], S-DCNet [ 40], and P2PNet [ 41].\nWeakly-supervised methods only need count\nannotations, and include that of MATT, TransCrowd,\nand CCTrans. In particular, TransCrowd and CCTrans\nalso use a transformer as the backbone network for\nfeature extraction.\n\n868 Z. Miao, Y. Zhang, Y. Peng, et al.\n4.2.3 Implementation details\nWe used the Swin-L model pre-trained on ImageNet-\n22K to speed up convergence of the model. For\nthe backbone network of the swin transformer, the\nnumber of heads used was [6, 12, 24, 48], the position\nembedding was a position bias matrix, the window\nsize was 12, the number of layers was [6, 12, 24, 48],\nand the number of channels in the hidden layer of\nthe ﬁrst stage was 192. In the training section, we\nstrictly followed the input image size requirement of\n384 ×384 for Swin-L. We used the same approach\nas TransCrowd [12]: we resize all original images to\n1152 ×768 (landscape) or 768 ×1152 (portrait), then\ncropped each image into 6 blocks of size 384×384, and\ncalculated the number of people in each image block\nby location annotation of the people in the image. We\nalso utilized data augmentation strategies, such as\nrandom ﬂipping and gray scaling. For compatibility\nwith the operation of dividing each image into 6\nimage blocks, the training batch size was set to 24.\nAll experiments were executed on a Linux system\nwith an Intel E5-2620v4 Xeon CPU at 2.10 GHz and\nan NVidia P100 16 GB Tesla. The learning rate was\nset to 10−5 initially and decreased to 10−6 in the ﬁnal\nepoch.\nIn the evaluation phase, we choose the widely\naccepted MSE and MAE as metrics:\nMAE = 1\nN\nN∑\ni=1\n|Pi −Gi| (10)\nMSE =\n\n√\n1\nN\nN∑\ni=1\n|Pi −Gi|2 (11)\nwhere N is the number of images, and Pi and\nGi represent the i-th predicted count and ground\ntruth, respectively. The MAE represents the mean\nabsolute error, and is a very intuitive evaluation\nmetric representing the distance between predicted\nvalue and ground truth. MSE better represents the\nstability of the model.\n4.3 Comparison to existing methods\nOur method, with alternatives DTCC-Dynamic and\nDTCC-Mean, shows good accuracy compared to other\nweakly-supervised methods. Table 1 gives errors for\nthe UCF CC 50, ShanghaiTech, and UCF QNRF\ndatasets. UCF CC 50 has only a few images, and\nthey are all of dense crowds. Without increasing the\namount of data, our method has made signiﬁcant\nprogress compared to other weakly-supervised\nmethods. DTCC-Mean achieves better results than\nDTCC-Dynamic, showing that using an average\nfusion mechanism for the regression head can provide\ngood accuracy and robustness given a small amount\nof data. The ShanghaiTech partA dataset comes from\na wide range of scenes with large variations in crowd\ndensity, so accurately estimating the number of people\nis very challenging. Our proposed method DTCC-\nDynamic achieves the best MAE. This indicates\nthat our backbone swin transformer network can\nbetter adapt to diﬀerent densities, while the dynamic\nfusion regression head can learn optimal ratios from\na large number of datasets. However, the MSE\nmetrics is less satisfactory. So, in the presence of\nanomalies, our method still needs to be improved. On\nShanghaiTech partB, DTCC again achieves signiﬁcant\nimprovements over other weakly-supervised methods.\nFor the JHU-Crowd++ dataset, we conducted\nexperiments on the validation set (Table 2) and test\nset (Table 3) separately. We divided the dataset into\nthree count levels of low (0–50), medium (51–500),\nhigh (500+), and also aggregated total results. For\nthe validation set, DTCC achieves the better results\nthan other weakly-supervised methods, and shows\ncompetitive results when compared to mainstream\nfully-supervised methods. To further demonstrate the\neﬀectiveness of the proposed DTCC, we conducted\nfurther experiments on the test set using the pre-\ntrained model parameters of JHU-Total. In this\ncase, for the JHU-Low dataset, our proposed method\nachieves competitive results, diﬀering slightly from\nthe state-of-the-art weakly-supervised method. On\nJHU-Medium, our method achieves the best estimates\ncompared to other weakly-supervised methods; it\nshows a strong advantage for this higher density\ndataset. On JHU-High further improvements are seen.\nThe proposed dynamic fusion mechanism achieves\nbetter results: dynamic learning parameters provide\ngood fault tolerance for ultra-dense scenes. JHU-\nTotal contains all horizontal images, and the density\nrange of the dataset is large, which requires a robust\nmodel. The good improvements to MAE and MSE,\nshow that our method has not only high accuracy\nbut also good stability.\n4.4 Visualization of feature maps\nTo verify the eﬀectiveness of our method, we\nvisualized feature maps on ShanghaiTech PartA using\n\nDTCC: Multi-level dilated convolution with transformer for weakly-supervised crowd counting 869\nheat maps. As noted, each image is split into six\nsub-images to input into the model, which can be\nseen in the visualization. Figure 7 shows that our\nmethod pays more attention to dense image regions\nand adapts to diﬀerent scenes. It can also be seen\nthat there are a few areas incorrectly given attention\ndue to the lack of individual person annotations.\n4.5 Computational cost\nTo evaluate the all-round performance of DTCC, we\ncalculated the number of parameters and GFlops\nconsumed by the model. Table 4 compared the\nresults with other mainstream full-supervised and\nweakly-supervised crowd counting methods. DTCC\nconsumes more computing resources than other\nmethods, because it uses the swin transformer as its\nbackbone network. Using a recursive swin transformer\nfurther increases the cost. However, the cost of our\nwork is still of the same order of magnitude as that\nof other works, with no major impact on practical\napplications.\n4.6 Ablation experiments and tuning\nWe conducted various ablation experiments on the\nDTCC-Dynamic version using the ShanghaiTech\ndataset to verify the contribution of each module\nand to justify the reasoning behind it, and to tune\noperation.\n4.6.1 Multi-level dilated convolution regression head\nTable 5 shows results of an ablation experiment on\nFig. 7 Visualization of feature maps.\nuse of the multi-level dilated convolution regression\nmodule. By comparing the results with and without\nM-DRH, we can see that introducing the multi-\nlevel dilated convolutional regression head improves\ncounting accuracy. This justiﬁes our assumption\nthat multi-level feature relationship modeling can\ncapture diﬀerent scales of crowd information in\nimages with dense crowd scenes. In addition, the\npresence of the dilated convolution can enhance the\nglobal receptive ﬁeld, which is important for weakly-\nsupervised methods in crowd counting.\nWe conducted further experiments to assess\ndiﬀerent choices of dilation rate. As Table 6 shows,\noptimal results were obtained by setting the dilation\nrates to [2, 3, 4]. Using too small dilation rates does\nnot enhance the receptive ﬁeld of features enough,\nwhile using too large dilation rates may lead to loss of\nlocal features. As a compromise, we set the dilation\nrates to [2, 3, 4].\nWe also performed experiments with diﬀerent multi-\nlevel features for weakly-supervised methods, as\nreported in Table 7. Dilation rates 1, 2, 3 represent\nfeature maps with resolutions of 12 ×12, 24 ×24, and\n48 ×48, respectively. Adding successive feature maps\nof diﬀerent resolutions improves the model’s results\nsigniﬁcantly, demonstrating that multi-level features\nare important to our method.\n4.6.2 Recursive ﬁne-FPN\nIn Table 8, a baseline of DTCC-Dynamic was used; it\ncompares results of using the baseline with recursive\nFPN, and using the baseline with recursive ﬁne-FPN.\nWe can see that using ﬁne-FPN achieves better results\nthan the original FPN. This indicates that for crowd\ncounting, fusion of deep features upsampled by 3 ×3\nConv can provide better performance.\nTable 7 Experiment on use of multi-level features\nDilation rates ShanghaiA ShanghaiB\nMAE MSE MAE MSE\n1 63.8 97.2 8.1 13.0\n1, 2 62.7 96.4 7.9 12.1\n1, 2, 3 60.8 97.0 7.2 10.8\nTable 8 Ablation experiment on recursive ﬁne-FPN\nModule ShanghaiA ShanghaiB\nMAE MSE MAE MSE\nRecursive FPN 62.1 99.0 8.0 12.5\nRecursive ﬁne-FPN 60.8 97.0 7.2 10.8\n\n870 Z. Miao, Y. Zhang, Y. Peng, et al.\nWe performed further experiments on the pyramid\nstructure. The baseline was DTCC-Dynamic method\nwithout any pyramid structures. Table 9 compares\nresults of three sets of experiments, using baseline,\nbaseline with ﬁne-FPN, and baseline with recursive\nﬁne-FPN. Addition of the pyramid structure\neﬀectively improves the accuracy of the model;\nthe recursive pyramid structure achieves the best\naccuracy. Due to the features at the last level of the\ntransformer output, pixel information is easily lost\nin the process of increasing the step size for patches.\nUsing the recursive ﬁne-FPN causes extra feedback\nconnections from ﬁne-FPN to be incorporated into\nthe bottom–up backbone layers, allowing all levels of\nfeature maps to have strong contextual information.\n4.6.3 Loss function\nWe separately evaluated the commonly used L1 and\nSmoothL1 [ 33] loss functions. From the results in\nTable 10, it can be concluded that SmoothL1 gives\nbetter results. SmoothL1 is more stable and can\nadapt well to both large and small errors.\nTable 9 Experiments on feature pyramid\nModule\nShanghai-A Shanghai-B\nMAE MSE MAE MSE\nDTCC w/o ﬁne-FPN 68.1 118.6 9.6 16.9\nﬁne-FPN 64.5 103.1 8.3 13.1\nRecursive ﬁne-FPN 60.8 97.0 7.2 10.8\nTable 10 Choice of loss function\nMethod\nShanghaiA ShanghaiB\nMAE MSE MAE MSE\nL1 62.1 99.0 8.0 12.5\nSmoothL1 60.8 97.0 7.2 10.8\n5 Conclusions\nThis work proposes a pyramidal vision transformer\nnetwork for weakly-supervised crowd counting; it\ncan achieve end-to-end crowd counting. A multi-\nlevel feature extraction module and a multi-level\ndilated convolutional regression module are designed\nfor dense prediction tasks; they can better capture\nglobal features and generate more reasonable features\nfor weakly-supervised crowd counting. Extensive\nexperiments on four well-known benchmark datasets\ndemonstrate that DTCC achieves superior counting\nperformance compared to other mainstream weakly-\nsupervised methods and is competitive with some\nfully-supervised methods.\nIn future, we plan to further investigate a more\nconcise feature extraction backbone network for\ncrowd counting, and design a better regression head\nfor prediction. In addition, we also intend to further\nextend DTCC to other dense prediction scenarios,\nsuch as traﬃc counting for intelligent transportation.\nAcknowledgements\nThis research project was partially supported by\nthe National Natural Science Foundation of China\n(Grant Nos. 62072015, U19B2039, U1811463), and\nthe National Key R&D Program of China (Grant\nNo. 2018YFB1600903).\nA portion of the work in this paper was carried\nout using the Taiji machine learning engine, and we\nthank Taiji for their support.\nDeclaration of competing interest\nThe authors have no competing interests to declare\nthat are relevant to the content of this article.\nReferences\n[1] Li, M.; Zhang, Z. X.; Huang, K. Q.; Tan, T. N.\nEstimating the number of people in crowded scenes by\nMID based foreground segmentation and head-shoulder\ndetection. In: Proceedings of the 19th International\nConference on Pattern Recognition, 1–4, 2008.\n[2] Wu, B.; Nevatia, R. Detection and tracking of multiple,\npartially occluded humans by Bayesian combination of\nedgelet based part detectors. International Journal of\nComputer Vision Vol. 75, No. 2, 247–266, 2007.\n[3] Lempitsky, V. S.; Zisserman, A. Learning to\ncount objects in images. In: Proceedings of the\n23rd International Conference on Neural Information\nProcessing Systems, Vol. 1, 1324–1332, 2010.\n[4] Walach, E.; Wolf, L. Learning to count with CNN\nboosting. In: Computer Vision – ECCV 2016. Lecture\nNotes in Computer Science, Vol. 9906. Leibe, B.; Matas,\nJ.; Sebe, N.; Welling, M. Eds. Springer Cham, 660–676,\n2016.\n[5] Wang, C.; Zhang, H.; Yang, L.; Liu, S.; Cao, X. C.\nDeep people counting in extremely dense crowds. In:\nProceedings of the 23rd ACM International Conference\non Multimedia, 1299–1302, 2015.\n\nDTCC: Multi-level dilated convolution with transformer for weakly-supervised crowd counting 871\n[6] Fu, M.; Xu, P.; Li, X. D.; Liu, Q. H.; Ye, M.; Zhu,\nC. Fast crowd density estimation with convolutional\nneural networks. Engineering Applications of Artiﬁcial\nIntelligence Vol. 43, 81–88, 2015.\n[7] Song, Q. Y.; Wang, C. G.; Jiang, Z. K.; Wang, Y.\nB.; Tai, Y.; Wang, C. J.; Li, J. L.; Huang, F. Y.;\nWu, Y. Rethinking counting and localization in crowds:\nA purely point-based framework. In: Proceedings of\nthe IEEE/CVF International Conference on Computer\nVision, 3345–3354, 2021.\n[8] Meng, Y. D.; Zhang, H. R.; Zhao, Y. T.; Yang, X.\nY.; Qian, X. S.; Huang, X. W.; Zheng, Y. Spatial\nuncertainty-aware semi-supervised crowd counting.\nIn: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 15529–15539, 2021.\n[9] Wan, J.; Liu, Z. Q.; Chan, A. B. A generalized\nloss function for crowd counting and localization. In:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 1974–1983, 2021.\n[10] Liu, X. L.; van de Weijer, J.; Bagdanov, A. D.\nExploiting unlabeled data in CNNs by self-supervised\nlearning to rank. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence Vol. 41, No. 8, 1862–\n1878, 2019.\n[11] Wang, Q.; Gao, J. Y.; Lin, W.; Yuan, Y. Learning\nfrom synthetic data for crowd counting in the wild. In:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 8190–8199, 2019.\n[12] Liang, D. K.; Chen, X. W.; Xu, W.; Zhou, Y.; Bai, X.\nTransCrowd: Weakly-supervised crowd counting with\ntransformers. Science China Information Sciences Vol.\n65, No. 6, Article No. 160104, 2022.\n[13] Liu, Z.; Lin, Y. T.; Cao, Y.; Hu, H.; Wei, Y.\nX.; Zhang, Z.; Lin, S.; Guo, B. Swin transformer:\nHierarchical vision transformer using shifted windows.\nIn: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 9992–10002, 2021.\n[14] Chen, C. F R.; Fan, Q. F.; Panda, R. CrossViT:\nCross-attention multi-scale vision transformer for image\nclassiﬁcation. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 347–356,\n2021.\n[15] Huang, Z.; Ben, Y.; Luo, G.; Cheng, P.; Yu, G.; Fu,\nB. Shuﬄe transformer: Rethinking spatial shuﬄe for\nvision transformer. arXiv preprint arXiv:2106.03650,\n2021.\n[16] Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.;\nKirillov, A.; Zagoruyko, S. End-to-end object detection\nwith transformers. In: Computer Vision – ECCV\n2020. Lecture Notes in Computer Science, Vol. 12346.\nVedaldi, A.; Bischof, H.; Brox, T.; Frahm, J. M. Eds.\nSpringer Cham, 213–229, 2020.\n[17] He, L.; Zhou, Q. Y.; Li, X. T.; Niu, L.; Cheng,\nG. L.; Li, X.; Liu, W.; Tong, Y.; Ma, L.; Zhang,\nL. End-to-end video object detection with spatial-\ntemporal transformers. In: Proceedings of the 29th\nACM International Conference on Multimedia, 1507–\n1516, 2021.\n[18] Zhang, Y. Y.; Zhou, D. S.; Chen, S. Q.; Gao, S. H.;\nMa, Y. Single-image crowd counting via multi-column\nconvolutional neural network. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, 589–597, 2016.\n[19] Sam, D. B.; Surya, S.; Babu, R. V. Switching\nconvolutional neural network for crowd counting. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 4031–4039, 2017.\n[20] Li, Y. H.; Zhang, X. F.; Chen, D. M. CSRNet: Dilated\nconvolutional neural networks for understanding the\nhighly congested scenes. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, 1091–1100, 2018.\n[21] Ma, Z. H.; Wei, X.; Hong, X. P.; Gong, Y. H. Bayesian\nloss for crowd count estimation with point supervision.\nIn: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 6141–6150, 2019.\n[22] Liu, Z.; He, Z.; Wang, L.; Wang, W.; Yuan, Y.;\nZhang, D.; Zhang, J.; Zhu, P.; Van Gool, L.; Han,\nJ.; et al. VisDrone-CC2021: The vision meets drone\ncrowd counting challenge results. In: Proceedings of\nthe IEEE/CVF International Conference on Computer\nVision Workshops, 2830–2838, 2021.\n[23] Liang, D.; Xu, W.; Bai, X. An end-to-end\ntransformer model for crowd localization. arXiv\npreprint arXiv:2202.13065, 2022.\n[24] Abousamra, S.; Hoai, M.; Samaras, D.; Chen, C.\nLocalization in the crowd with topological constraints.\nProceedings of the AAAI Conference on Artiﬁcial\nIntelligence Vol. 35, No. 2, 872–881, 2021.\n[25] Sun, G. L.; Liu, Y.; Probst, T.; Paudel, D. P.;\nPopovic, N.; Van Gool, L. Boosting crowd counting\nwith transformers. arXiv preprint arXiv:2105.10926,\n2021.\n[26] Gao, J. Y.; Gong, M. G.; Li, X. L. Congested crowd\ninstance localization with dilated convolutional swin\ntransformer. arXiv preprint arXiv:2108.00584, 2021.\n[27] Shang, C.; Ai, H. Z.; Bai, B. End-to-end crowd\ncounting via joint learning local and global count. In:\nProceedings of the IEEE International Conference on\nImage Processing, 1215–1219, 2016.\n\n872 Z. Miao, Y. Zhang, Y. Peng, et al.\n[28] Wang, M. J.; Zhou, J.; Cai, H.; Gong, M. L.\nCrowdMLP: Weakly-supervised crowd counting\nvia multi-granularity MLP. arXiv preprint arXiv:\n2203.08219, 2022.\n[29] Lei, Y. J.; Liu, Y.; Zhang, P. P.; Liu, L. Q. Towards\nusing count-level weak supervision for crowd counting.\nPattern Recognition Vol. 109, 107616, 2021.\n[30] Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X. H.; Unterthiner, T.; Dehghani, M.;\nMinderer, M.; Heigold, G.; Gelly, S.; et al. An\nimage is worth 16x16 words: Transformers for image\nrecognition at scale. In: Proceedings of the International\nConference on Learning Representations, 2021.\n[31] Tian, Y.; Chu, X.; Wang, H. CCTrans: Simplifying\nand improving crowd counting with transformer. arXiv\npreprint arXiv:2109.14483, 2021.\n[32] Chu, X.; Tian, Z.; Wang, Y.; Zhang, B.; Ren, H.;\nWei, X.; Xia, H.; Shen, C. Twins: Revisiting the\ndesign of spatial attention in vision transformers. In:\nProceedings of the Advances in Neural Information\nProcessing Systems, Vol. 34, 9355–9366, 2021.\n[33] Girshick, R. Fast R-CNN. In: Proceedings of the IEEE\nInternational Conference on Computer Vision, 1440–\n1448, 2015.\n[34] Idrees, H.; Saleemi, I.; Seibert, C.; Shah, M. Multi-\nsource multi-scale counting in extremely dense crowd\nimages. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2547–2554,\n2013.\n[35] Zhang, Y. Y.; Zhou, D. S.; Chen, S. Q.; Gao, S. H.;\nMa, Y. Single-image crowd counting via multi-column\nconvolutional neural network. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, 589–597, 2016.\n[36] Sindagi, V. A.; Yasarla, R.; Patel, V. M. JHU-CROWD:\nLarge-scale crowd counting dataset and a benchmark\nmethod. IEEE Transactions on Pattern Analysis and\nMachine Intelligence Vol. 44, No. 5, 2594–2609, 2022.\n[37] Liu, W. Z.; Salzmann, M.; Fua, P. Context-aware\ncrowd counting. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, 5094–5103, 2020.\n[38] Bai, S.; He, Z. Q.; Qiao, Y.; Hu, H. Z.; Wu,\nW.; Yan, J. J. Adaptive dilated network with self-\ncorrection supervision for counting. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 4593–4602, 2020.\n[39] Shi, M. J.; Yang, Z. H.; Xu, C.; Chen, Q. J. Revisiting\nperspective information for eﬃcient crowd counting.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 7271–7280,\n2019.\n[40] Xiong, H. P.; Lu, H.; Liu, C. X.; Liu, L.; Cao,\nZ. G.; Shen, C. H. From open set to closed\nset: Counting objects by spatial divide-and-conquer.\nIn: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 8361–8370, 2019.\n[41] Song, Q.; Wang, C.; Jiang, Z.; Wang, Y.; Tai, Y.;\nWang, C.; Li, J.; Huang, F.; Wu, Y. Rethinking\ncounting and localization in crowds: A purely point-\nbased framework. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 3345–\n3354, 2021.\n[42] Yang, Y.; Li, G.; Wu, Z.; Su, L.; Huang, Q.; Sebe, N.\nWeakly-supervised crowd counting learns from sorting\nrather than locations. In: Computer Vision – ECCV\n2020. Lecture Notes in Computer Science, Vol. 12353 .\nVedaldi, A.; Bischof, H.; Brox, T.; Frahm, J. M. Eds.\nSpringer Cham, 1–17, 2020.\n[43] Sindagi, V. A.; Patel, V. M. CNN-based cascaded multi-\ntask learning of high-level prior and density estimation\nfor crowd counting. In: Proceedings of the 14th IEEE\nInternational Conference on Advanced Video and Signal\nBased Surveillance, 1–6, 2017.\n[44] Sindagi, V. A.; Patel, V. M. Generating high-quality\ncrowd density maps using contextual pyramid CNNs.\nIn: Proceedings of the IEEE International Conference\non Computer Vision, 1879–1888, 2017.\n[45] Shen, Z.; Xu, Y.; Ni, B. B.; Wang, M. S.; Hu,\nJ. G.; Yang, X. K. Crowd counting via adversarial\ncross-scale consistency pursuit. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 5245–5254, 2018.\n[46] Qiao, S. Y.; Chen, L. C.; Yuille, A. DetectoRS:\nDetecting objects with recursive feature pyramid and\nswitchable atrous convolution. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 10208–10219, 2021.\n[47] Yang, Y. F.; Li, G. R.; Wu, Z.; Su, L.; Huang,\nQ. M.; Sebe, N. Reverse perspective network for\nperspective-aware object counting. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 4373–4382, 2020.\n[48] Wan, J.; Liu, Z. Q.; Chan, A. B. A generalized\nloss function for crowd counting and localization. In:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 1974–1983, 2021.\n[49] Liu, L. B.; Qiu, Z. L.; Li, G. B.; Liu, S. F.; Ouyang, W.\nL.; Lin, L. Crowd counting with deep structured scale\nintegration network. In: Proceedings of the IEEE/CVF\n\nDTCC: Multi-level dilated convolution with transformer for weakly-supervised crowd counting 873\nInternational Conference on Computer Vision, 1774–\n1783, 2019.\n[50] Cao, X.; Wang, Z.; Zhao, Y.; Su, F. Scale aggregation\nnetwork for accurate and eﬃcient crowd counting.\nIn: Computer Vision – ECCV 2018. Lecture Notes\nin Computer Science, Vol. 11209 . Ferrari, V.; Hebert,\nM.; Sminchisescu, C.; Weiss, Y. Eds. Springer Cham,\n757–773, 2018.\n[51] Idrees, H.; Tayyab, M.; Athrey, K.; Zhang, D.; Al-\nMaadeed, S.; Rajpoot, N.; Shah, M. Composition loss\nfor counting, density map estimation and localization\nin dense crowds. In: Computer Vision – ECCV 2018.\nLecture Notes in Computer Science, Vol. 11206. Ferrari,\nV.; Hebert, M.; Sminchisescu, C.; Weiss, Y. Eds.\nSpringer Cham, 544–559, 2018.\n[52] Savner, S. S.; Kanhangad, V. CrowdFormer: Weakly-\nsupervised crowd counting with improved genera-\nlizability. arXiv preprint arXiv:2203.03768, 2022.\n[53] Wang, F. S.; Liu, K.; Long, F.; Sang, N.; Xia, X. F.;\nSang, J. Joint CNN and transformer network via weakly\nsupervised learning for eﬃcient crowd counting. arXiv\npreprint arXiv:2203.06388, 2022.\n[54] Song, Q.; Wang, C.; Wang, Y.; Tai, Y.; Wang, C.;\nLi, J.; Wu, J.; Ma, J. To choose or to fuse? Scale\nselection for crowd counting. Proceedings of the AAAI\nConference on Artiﬁcial Intelligence Vol. 35, No. 3,\n2576–2583, 2021.\n[55] Sindagi, V. A.; Patel, V. M. Multi-level bottom–top\nand top–bottom feature fusion for crowd counting.\nIn: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 1002–1012, 2019.\nZhuangzhuang Miao is a master\nstudent in the Faculty of Information\nTechnology of Beijing University of\nTechnology (BJUT). He got his B.S.\ndegree from Shijiazhuang University in\n2020. His research interests include deep\nlearning and computer graphics.\nYong Zhangreceived his Ph.D. degree\nin computer science from BJUT in\n2010. He is currently an associate\nprofessor of computer science at BJUT.\nHis research interests include intelligent\ntransportation systems, big data analysis,\nvisualization, and computer graphics.\nYuan Peng received his M.S. degree\nin software engineering and IT methods\napplied to business management from\nJules Verne University of Picardy,\nFrance in 2011 and 2012, respectively.\nHe is currently a senior engineer in\nChina Electronics Technology Group.\nHis current research interests include\ngeographic information systems, air traﬃc control, computer\ngraphics, atmospheric operation modes, and radar echos.\nHaocheng Pengis currently studying\nfor a bachelor degree in IoT in Beijing\nDublin International College. His current\nresearch interests include deep learning\nand block chains.\nBaocai Yin received his B.S., M.S.,\nand Ph.D. degrees in computational\nmathematics from Dalian University of\nTechnology, China, in 1985, 1988, and\n1993, respectively. He is currently a\nprofessor in the Beijing Key Laboratory\nof Multimedia and Intelligent Software\nTechnology, BJUT. His research interests\ninclude multimedia, image processing, computer vision, and\npattern recognition.\nOpen Access This article is licensed under a Creative\nCommons Attribution 4.0 International License, which permits\nuse, sharing, adaptation, distribution and reproduc-tion in any\nmedium or format, as long as you give appropriate credit to\nthe original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made.\nThe images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and\nyour intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission\ndirectly from the copyright holder.\nTo view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\nOther papers from this open access journal are available\nfree of charge from http://www.springer.com/journal/41095.\nTo submit a manuscript, please go to https://www.\neditorialmanager.com/cvmj.\n"
}