{
  "title": "Use of AI-based tools for healthcare purposes: a survey study from consumers’ perspectives",
  "url": "https://openalex.org/W3044430020",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1765690965",
      "name": "Pouyan Esmaeilzadeh",
      "affiliations": [
        "Florida International University"
      ]
    },
    {
      "id": "https://openalex.org/A1765690965",
      "name": "Pouyan Esmaeilzadeh",
      "affiliations": [
        "Florida International University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2899856450",
    "https://openalex.org/W2796755741",
    "https://openalex.org/W2947308709",
    "https://openalex.org/W2931283717",
    "https://openalex.org/W2997048718",
    "https://openalex.org/W3005831385",
    "https://openalex.org/W1966351988",
    "https://openalex.org/W2788710759",
    "https://openalex.org/W2622061173",
    "https://openalex.org/W2997096971",
    "https://openalex.org/W2986485804",
    "https://openalex.org/W2149967206",
    "https://openalex.org/W2091299820",
    "https://openalex.org/W2995678499",
    "https://openalex.org/W2905810301",
    "https://openalex.org/W3000255801",
    "https://openalex.org/W2256333837",
    "https://openalex.org/W2913916101",
    "https://openalex.org/W2989512989",
    "https://openalex.org/W2073242082",
    "https://openalex.org/W2916650597",
    "https://openalex.org/W2896245505",
    "https://openalex.org/W1141762044",
    "https://openalex.org/W2969625533",
    "https://openalex.org/W2906295032",
    "https://openalex.org/W2912710844",
    "https://openalex.org/W2902461656",
    "https://openalex.org/W2899768131",
    "https://openalex.org/W2911526543",
    "https://openalex.org/W2801037981",
    "https://openalex.org/W6889540269",
    "https://openalex.org/W2905988626",
    "https://openalex.org/W2789970635",
    "https://openalex.org/W2897312458",
    "https://openalex.org/W2177363093",
    "https://openalex.org/W2280321044",
    "https://openalex.org/W2792457039",
    "https://openalex.org/W2898970033",
    "https://openalex.org/W3006913750",
    "https://openalex.org/W2884686506",
    "https://openalex.org/W2887382745",
    "https://openalex.org/W2066004246",
    "https://openalex.org/W2915444077",
    "https://openalex.org/W2901946084",
    "https://openalex.org/W2900603051",
    "https://openalex.org/W2806853752",
    "https://openalex.org/W2664267452",
    "https://openalex.org/W1988050849",
    "https://openalex.org/W2137872373",
    "https://openalex.org/W2129509892",
    "https://openalex.org/W2088476684",
    "https://openalex.org/W3123895079",
    "https://openalex.org/W2141536144",
    "https://openalex.org/W1970811284",
    "https://openalex.org/W2012888175",
    "https://openalex.org/W3159378964",
    "https://openalex.org/W135304209",
    "https://openalex.org/W2038001094",
    "https://openalex.org/W2108750986",
    "https://openalex.org/W2170219224",
    "https://openalex.org/W1596452301",
    "https://openalex.org/W4234082134",
    "https://openalex.org/W2916945592",
    "https://openalex.org/W2991833891",
    "https://openalex.org/W2032160998",
    "https://openalex.org/W2907378463",
    "https://openalex.org/W2163241745",
    "https://openalex.org/W2954398798",
    "https://openalex.org/W3144406692",
    "https://openalex.org/W2622458265",
    "https://openalex.org/W4254895593",
    "https://openalex.org/W2741788848",
    "https://openalex.org/W2934302500",
    "https://openalex.org/W2913777302",
    "https://openalex.org/W1999806522",
    "https://openalex.org/W2952429584",
    "https://openalex.org/W2937411581",
    "https://openalex.org/W2902525495",
    "https://openalex.org/W2582333088",
    "https://openalex.org/W2122410182",
    "https://openalex.org/W3000239480",
    "https://openalex.org/W4207074774",
    "https://openalex.org/W1608836379",
    "https://openalex.org/W3124817805",
    "https://openalex.org/W2331182140",
    "https://openalex.org/W2782945138",
    "https://openalex.org/W1551700376",
    "https://openalex.org/W3008574300",
    "https://openalex.org/W2029487046",
    "https://openalex.org/W2089643036",
    "https://openalex.org/W3193476893"
  ],
  "abstract": null,
  "full_text": "RESEARCH ARTICLE Open Access\nUse of AI-based tools for healthcare\npurposes: a survey study from consumers ’\nperspectives\nPouyan Esmaeilzadeh\nAbstract\nBackground: Several studies highlight the effects of artificial intelligence (AI) systems on healthcare delivery. AI-\nbased tools may improve prognosis, diagnostics, and care planning. It is believed that AI will be an integral part of\nhealthcare services in the near future and will be incorporated into several aspects of clinical care. Thus, many\ntechnology companies and governmental projects have invested in producing AI-based clinical tools and medical\napplications. Patients can be one of the most important beneficiaries and users of AI-based applications whose\nperceptions may affect the widespread use of AI-based tools. Patients should be ensured that they will not be\nharmed by AI-based devices, and instead, they will be benefited by using AI technology for healthcare purposes.\nAlthough AI can enhance healthcare outcomes, possible dimensions of concerns and risks should be addressed\nbefore its integration with routine clinical care.\nMethods: We develop a model mainly based on value perceptions due to the specificity of the healthcare field.\nThis study aims at examining the perceived benefits and risks of AI medical devices with clinical decision support\n(CDS) features from consumers ’perspectives. We use an online survey to collect data from 307 individuals in the\nUnited States.\nResults: The proposed model identifies the sources of motivation and pressure for patients in the development of AI-\nbased devices. The results show that technological, ethical (trust factors), and regulatory concerns significantly\ncontribute to the perceived risks of using AI applications in healthcare. Of the three categories, technological concerns\n(i.e., performance and communication feature) are found to be the most significant predictors of risk beliefs.\nConclusions: This study sheds more light on factors affecting perceived risks and proposes some recommendations\non how to practically reduce these concerns. The findings of this study provide implications for research and practice\nin the area of AI-based CDS. Regulatory agencies, in cooperation with healthcare institutions, should establish\nnormative standard and evaluation guidelines for the implementation and use of AI in healthcare. Regular audits and\nongoing monitoring and reporting systems can be used to continuously evaluate the safety, quality, transparency, and\nethical factors of AI-based services.\nKeywords: Artificial intelligence (AI), AI medical devices, Clinical decision support, Perceived benefits, Perceived risks,\nIntention to use\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this article are included in the article's Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nThe Creative Commons Public Domain Dedication waiver ( http://creativecommons.org/publicdomain/zero/1.0/) applies to the\ndata made available in this article, unless otherwise stated in a credit line to the data.\nCorrespondence: pesmaeil@fiu.edu\nDepartment of Information Systems and Business Analytics, College of\nBusiness, Florida International University, Miami, FL 33199, USA\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 \nhttps://doi.org/10.1186/s12911-020-01191-1\nBackground\nArtificial Intelligence (AI) ge nerally refers to a computer-\nized system (hardware or softw are) that is able to perform\nphysical tasks and cognitive functions, solve various prob-\nlems, or make decisions with out explicit human instruc-\ntions [1]. A range of techniques and applications are under\nthe broad umbrella of AI, such as genetic algorithms, neural\nnetworks, machine learning, and pattern recognition [ 2]. AI\ncan replace human tasks and activities within a wide range\nof industrial, intellectual, and social applications with result-\ning impacts on productivity and performance. AI, as non-\nhuman intelligence programmed to complete specific tasks,\ncan overcome some of the compu tationally intensive and\nintellectual limitations of humans [ 3]. For example, AI\ncould be a computer application that is competent to solve\na complicated business probl em for managers. AI-enabled\ndevices generate personalized recommendations to cus-\ntomers based on an analysis of a huge dataset. Thus, it is\nbelieved that AI could be smarter than the best humans\nand experts in any field [ 2]. The value of using AI tools is\nperceived based on the trade-off between possible benefit\nand risk as the benefit is higher than the risk, greater value\nof using the technology is perceived.\nAI technology, including al gorithmic machine learning\nand autonomous decision-ma king, creates new opportun-\nities for continued innovation in different industries ranging\nfrom finance, healthcare, manu facturing, retail, supply\nchain, logistics, and utilities [ 4] .A Ic a nb eu s e di nt h ef o r m\nof clinical decision support (CDS) to support patient-\nspecific diagnosis and treatment decisions and perform\npopulation-based risk prediction analytics [ 5]. Promoting\nAI-based services has become one of the focal points of\nmany companies ’ strategies [ 6]. The important changes\nmade by AI have inspired recent studies to examine the im-\npacts and consequences of the technology and to investi-\ngate the performance impli cations of AI. Though, this\nobjective needs an in-depth understanding of the factors af-\nfecting the acceptance of AI-based services by potential\nusers in different manufacturing and service fields. Previous\nstudies highlight the importance of AI in healthcare, espe-\ncially in medical informatics [ 7]. AI is able to provide im-\nproved patient care, diagno sis, and interpretation of\nmedical data [ 8]. A study shows that AI technology used\nfor breast cancer screening reduces human detection errors,\nbut some of the interrelated ethical and societal trust fac-\ntors, as well as reliance on AI, are yet to be developed [ 9].\nThe use of AI-driven recommendations in health care may\nbe different from other sectors, mainly because of highly\nsensitive health information and high levels of consumers ’\nvulnerability to possible medical errors.\nIn April 2018, the FDA (Food and Drug Administration)\nauthorized the first AI device to diagnose diabetic retinop-\nathy without a physician’sh e l pi nt h eU S A[10]. An increas-\ning number of healthcare service companies are investing in\nthe development of AI embedded in mobile health devices\nor health apps to improve patient safety, increase practice\nquality, enhance patient care management, and decrease\nhealthcare costs. However, previous studies suggest that not\nall individuals are willing to accept the use of medical AI de-\nvices [ 10]. Successful implementation of AI-based systems\nrequires a careful examination of users ’ attitudes and per-\nceptions about AI [ 5]. Thus, investing in AI technology\nwithout recognizing potential users’ beliefs and willingness\nto accept AI devices may lead to a waste of resources and/\nor even a loss of customers. This is especially true in the\nhealthcare sector, where patient engagement is considered\nas one of the most critical determinants of healthcare qual-\nity. If individuals do not view interacting with a medical AI\ndevice as useful, they may demand interactions with physi-\ncians, and in turn, the AI-based devices may remain unused.\nTherefore, understanding the decision drivers and barriers\nthat lead to acceptance or refusal of the use of AI-based de-\nvices in healthcare delivery is fundamental for healthcare\nproviders and hospitals that plan to introduce and/or in-\ncrease AI device presence during healthcare delivery.\nMoreover, following previous studies, healthcare profes-\nsionals still express fundamental concerns about the im-\nplementation of AI-based tools in care services [ 11].\nThere is a need for researchers to more efficiently under-\nstand the current challenges related to AI technologies\nand analyze the urgent needs of health systems to design\nAI-enabled tools that are able to address them. However,\nlittle is know about the antecedents of risk beliefs associ-\nated with the use of AI-based devices for treatments from\nthe general public ’s perspective. Theoretical and qualita-\ntive study results demonstrate some factors that contrib-\nute to risk beliefs and individuals ’ withdrawal from using\nAI clinical devices [ 10]. But, empirical studies to examine\nthe positive and negative sides of using AI in medicine\nfrom consumers’ perspectives are still scarce. Besides, the\nsignificance and generality of this value-based mindset,\nand its actual connection to the intention to use AI in\nhealth care, have not been investigated.\nThe value is estimated based on the trade-off of tech-\nnology [ 12]. Most AI-related studies use various accept-\nance models (e.g., TAM, UTAUT) and do not include\nvalue perceptions (benefit and risk beliefs) associated\nwith AI, which may lead to intention to use [ 3]. How-\never, the value-based adoption model is viewed as a\nmore appropriate approach to explain the behavior of\nservice consumers by indicating that most consumers\nsupport new technologies based on their personal per-\nceptions [ 13]. A comparative study also proposes that\nthe value-based adoption model best explains consumer\nacceptance of AI-based intelligent products compared to\nother widely used technology acceptance theory (i.e.,\nTAM, TPB, UTAUT) [ 14]. Thus, we expect that risk-\nbenefit evaluations of AI technology in human-centered\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 2 of 19\nservices (such as health care) significantly affect the use\nof AI clinical tools by individuals.\nCurrently, the issues related to AI-based tools in health-\ncare are still within the realm of research. However, it is\nwidely believed that these systems will fundamentally\nchange medical practice in the near future [ 15]. Historic-\nally, the medical sector does not integrate technology as\nfast as other industries. Moreover, without the involve-\nment, cooperation, and endorsement of stakeholders (such\nas healthcare professionals and patients) and a robust le-\ngislative and regulatory framework, the integration of AI\ninto current medical workflow could be very challenging.\nThe main objective of this study is to examine how poten-\ntial users (individuals) perceive benefits, risks, and use of\nAI-based devices for their healthcare purposes. The bene-\nfit perceptions and risk beliefs of prospective users may\naffect their future adoption of AI devices. Patients may\nnot decide what tools healthcare professionals should use\nin their practice, but they can definitely highlight possible\nconcerns, challenges, and barriers that may refrain them\nfrom supporting and using the tools implemented and\npromoted by clinicians.\nUsing value-based consideration to predict behavioral\nintention in this study acknowledges the distinctive na-\nture of the healthcare sector compared to other less sen-\nsitive service work [ 11]. Extending the previously\nsuggested AI acceptance model by including a value-\nbased approach to the intention to use AI devices, we\npropose a model for health care to be used in academic\nand practical studies with an aim to statistically model\nacceptance of AI-based devices among potential users.\nIn this study, we survey individuals ’ acceptance of AI\ntechnology and identify the factors that determine the\nintention to use AI tools specifically in the context of\nhealth care. We categorize possible factors underlying\nrisk beliefs associated with AI clinical tools as threefold:\ntechnological, ethical, and regulatory concerns.\nThere are different types of medial AI, and this study\nfocuses on devices with AI-based CDS features. This re-\nsearch offers significant and timely insight into the appli-\ncations of AI-based CDS in healthcare. The findings of\nthis study will provide critical insights to researchers and\nmanagers on the determinants of individuals ’ intention\nto use AI-based devices for healthcare delivery. Results\nimply that incompatibility with instrumental, technical,\nethical, or regulatory values can be a reason for rejecting\nAI technology in healthcare. Multi-dimensional concerns\nassociated with AI clinical tools may also be viewed as a\ncause of technostress, which occurs when an individual\nis unable to adapt to using technology [ 16]. In the fu-\nture, it is the patient ’s or customer ’s right to choose AI-\ndriven recommendations over human care or vice versa.\nNevertheless, we propose that AI device developers and\nprogrammers can devise some practical strategies to\nanticipate possible concerns, and in turn, minimize risks\n(i.e., the subject of the concern) to encourage individuals\nto use devices with AI-based CDS for healthcare\npurposes.\nMethods\nThis study drew on the existing literature to measure\nthe constructs included in the model, and minor changes\nwere made to the instrument to fit the AI context. This\nstudy adapted items to measure constructs from existing\nscales developed by studies in the AI domain. The final\nmeasure items used in this study are listed in the Ap-\npendix. Table 1 shows the definition of constructs used\nin this study.\nHypotheses development\nWe bring perceived risks and its antecedents as well as\nperceived benefits together in a theoretical synthesis in\nwhich these concepts are seen to interact in ways that help\nshape the behavioral intention of AI users. The research\nmodel indicates that three concerns (technological, ethical,\nand regulatory) directly influence the general perceived\nrisks associated with AI tools. Technological concerns in-\nclude two dimensions: perceived performance anxiety and\nperceived communication barriers. Ethical concerns con-\nsist of three dimensions: perceived privacy concerns, per-\nceived mistrust in AI mechanisms, and perceived social\nbiases. Regulatory concerns comprise two dimensions:\nperceived unregulated standard and perceived liability is-\nsues. Moreover, both risk beliefs and benefit perceptions\nwill influence individuals ’ intention to use AI-based de-\nvices. In this study, we control for the effects of demo-\ngraphics and technology experience factors. The control\nvariables are age, gender, race, income, employment, edu-\ncation, general computer skills, technical knowledge about\nAI technology, and experience with an AI-based service,\nwhich are found and tested by prior research as factors af-\nfecting the adoption of AI devices e.g., [ 3]. Figure 1 shows\nthe proposed research model.\nTechnological concerns\nTechnological concerns include the nature of diagnostic\ntasks, lack of transparency of AI process, safety of AI-\ndriven recommendations, complexities in interpreting\nresults, and issues with AI-user interaction architecture\n[25]. In this study, we consider two dimensions for tech-\nnical concerns: perceived performance anxiety and per-\nceived communication barriers.\nPerceived performance anxiety\nPerceived performance anxiety refers to users ’ perception\nof the likelihood that an IT system malfunctions, does not\nwork as intended, and become unable to deliver the de-\nsired services [ 26]. AI-related studies consider the safety\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 3 of 19\nTable 1 Operationalization of variables\nConstruct Construct definition Source\nPerceived performance\nanxiety\nThe degree to which an individual believes that AI-based tools and their features exhibit pervasive\ntechnological uncertainties.\nSarin, Sego [ 17]\nPerceived\ncommunication barriers\nThe degree to which an individual feels that AI devices may reduce human aspects of relations in the\ntreatment process.\nLu, Cai [ 18]\nPerceived social biases The degree to which a person believes that data used in the AI devices may lead to societal\ndiscrimination to a certain patient group (e.g., minority groups).\nReddy, Allan\n[19]\nPerceived privacy\nconcerns\nThe extent to which individuals concern about how AI-based devices collect, access, use, and protect\ntheir personal information\nStewart and\nSegars [ 20]\nPerceived mistrust in AI\nmechanisms\nThe degree to which an individual believes that AI models and AI-driven diagnostics and recommenda-\ntions in health care are still not trustworthy.\nLuxton [ 21]\nPerceived unregulated\nstandard\nThe extent to which an individual believes that regulatory standards and guidelines to assess AI\nalgorithmic safety are yet to be formalized.\nCath [ 22]\nPerceived liability issues The extent to which an individual is concerned about the liability and responsibility of using AI clinical\ntools.\nLaï, Brian [ 10]\nPerceived risks The extent to which an individual believes that, in general, it would be risky for patients to use AI-\nbased tools in health care.\nBansal, Zahedi\n[23]\nPerceived benefits The extent to which an individual believes that AI-based tools can improve diagnostics and care plan-\nning for patients.\nLo, Lei [ 24]\nIntention to use AI-based\ntools\nThe extent to which an individual is willing to use AI-based services for diagnostics and treatments. Turja, Aaltonen\n[11]\nFig. 1 Research model\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 4 of 19\nand quality of autonomous operations an essential factor af-\nfecting the use of AI devices [ 27]. According to Mitchell\n[28], AI systems are still vulnerable in many areas, such as\nhacker attacks. Hackers can change text files or images\nwhich may not have a human cognitive effect but could\ncause potentially catastrophice r r o r s .S i n c et h eA Ip r o g r a m\nmay not understand the input and outputs, they are suscep-\ntible to unexpected errors and untraceable attacks. Conse-\nquently, these medical errors could endanger patient safety\nand result in death or injuries. Thus, users may become\nconcerned that the mechanisms used by AI-based devices\ncould lead to incorrect diagnoses or wrong treatments. A\nstudy indicates that incomplete and nonrepresentative data-\nsets in AI models can produce inaccurate predictions and\nmedical errors [29]. Thus, we propose that individuals may\nconsider that possible functi onal errors resulting from the\nuse of AI devices could lead to more risks.\nH1: Perceived performance anxiety positively influ-\nences perceived risks.\nPerceived communication barriers\nThe use of AI devices in service delivery (such as health-\ncare) may cause noteworthy communication barriers be-\ntween customers and AI devices [ 18]. Reliance on AI\nclinical tools may reduce the interactions and conversa-\ntion between physicians and patients [ 30]. Consumers\nmay refuse to use AI devices because of their need for\nhuman social interaction during service encounters [ 3].\nAI technology fundamentally changes the traditional\nphysician-patient communications [ 31]. Thus, individ-\nuals may worry as they may lose face-to-face cues and\npersonal interactions with physicians. AI causes chal-\nlenges to patient-clinician interactions, as clinicians need\nto learn how to interact with the AI system for health-\ncare delivery, and patients are required to reduce the\nfear of technology [ 32]. As AI continues to proliferate,\nusers still encounter some challenges about the effective\nuse of AI, such as how the partnership between AI sys-\ntems and humans could be synergic? [ 2]. A study pro-\nposes that more sophisticated technologies should be\nintegrated into current medical AI systems to improve\nhuman-computer interactions and streamline the flow of\ninformation between two parties [ 25]. Therefore, AI\ntools may reduce conversation between physicians and\npatients, and it may emerge more risk beliefs. We de-\nvelop the second hypothesis as follows:\nH2: Perceived communication barriers positively influ-\nence perceived risks.\nEthical concerns\nEthical concerns include trust issues about AI and hu-\nman behavior, compatibility of machine versus human\nvalue judgment, moral dilemmas, and AI discrimination\n[25]. In this study, we consider three dimensions for\nethical concerns: perceived privacy concerns, perceived\nmistrust in AI mechanisms, and perceived social biases.\nPerceived privacy concerns\nHealth-related data is one of the most sensitive informa-\ntion about a person [ 30]. In healthcare services, respect-\ning a person ’s privacy is an essential ethical principle\nbecause patient privacy is associated with wellbeing and\npersonal identity [ 22]. Thus, patients ’ confidentiality\nshould be respected by healthcare providers by protect-\ning their health records, preventing secondary use of\ndata, and developing a robust system to obtain informed\nconsent from them for healthcare purposes [ 33]. If the\nprivacy needs of patients are not met, patients will be af-\nfected by psychological and reputational harm [ 34]. Data\nbreaches would increase risk believes associated with AI\nmodels designed to share personal health information.\nThere is a concern that anonymized data can be reiden-\ntified through AI processes, and this anxiety may exacer-\nbate privacy invasion and data breach risks [ 19].\nAI technology in public health requires large datasets.\nThus, collection, storage, and sharing of medical data\nraise ethical questions related to safety, governance, and\nprivacy [ 35]. Privacy is one of the most critical concerns\nwhile using AI systems because users ’ personal data\n(such as habits, preferences, and health records) is likely\nto be stored and shared across the AI network [ 25].\nMethod of data collection for AI may increase risks as\nAI systems need huge datasets, and patients are con-\ncerned that their personal information will be collected\nwithout their knowledge [ 30]. Accordingly, the next hy-\npothesis is proposed as follows:\nH3: Perceived privacy concerns positively influence\nperceived risks.\nPerceived mistrust in AI mechanisms\nTrust between public and healthcare systems is essential\nf o re f f e c t i v eh e a l t h c a r ed e l i v e r y[36]. Gaining the trust of\nthe general public in the use of AI in healthcare is consid-\nered as an important challenge to the successful imple-\nmentation of AI in medical practices [ 37]. Perceived\nmistrust in AI mechanisms refers to users ’ perception that\nAI’s predictive and diagnostic models are not trustworthy\n[19]. A study reports that, in general, individuals are likely\nto exhibit a lack of trust in the features of AI systems [ 38].\nFor instance, people may not trust AI ‘s predictive power\nand diagnostic ability for treatment purposes. Lee, Kim\n[39] indicate that the autonomy of AI systems affects the\nusers’ perception of trustworthiness. Trust in AI-based\ntools (such as AI medical devices) is found to be a signifi-\ncant factor affecting adoption decisions [ 40]. When pa-\ntients cannot understand the inside workings of AI\ndevices, they may exhibit lower trust in their functions\nand how they generate treatment solutions and\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 5 of 19\nrecommendations. The nature of AI models (such as deep\nlearning) may increase a lack of transparency related to AI\nsystems and threaten patient trust, which in turn, result in\ngreater risk belives [ 37]. Therefore, we hypothesize:\nH4: Perceived mistrust in AI mechanisms positively in-\nfluences perceived risks.\nPerceived social biases\nStudies in other contexts show that AI models overesti-\nmate the risk of crime among members of a certain ra-\ncial group [ 41]. In the healthcare context, biased AI\nmodels may overestimate or underestimate health risks\nin specific patient populations. For instance, AI systems\nmay engage in stereotyping and exhibit gender or racial\nbias. Bias in AI models my also occur when datasets are\nnot representative of the target population, or incom-\nplete and inaccurate data are used by AI systems for\ndecision-making [ 30]. Societal discrimination (such as\npoor access to healthcare) and small samples (such as\nminority groups) can lead to unrepresentative data and\nAI bias [ 19]. A study reports that the current architec-\nture of AI systems needs a more sophisticated structure\nto understand human moral values [ 42]. If the AI algo-\nrithm is not transparent, it may exhibit some levels of\ndiscrimination, even though humans are not involved in\nthe decision-making process [ 25]. The main purpose of\nAI is to create an algorithm that functions autonomously\nto find the best possible solutions to questions [ 43].\nHowever, researchers argue that predictive programs can\nbe inevitably biased due to an overrepresentation of the\nsocial minorities in the pattern recognition process [ 44].\nSome studies support this argument by showing that AI\nalgorithms may be coded biasedly, which can produce\nracist decisions [ 45]. Therefore, if people are concerned\nthat AI devices could lead to morally flawed practices in\nhealthcare by overestimating or underestimating health\nrisks in a certain patient population, they become more\nlikely to perceive more risks associated with AI. This dis-\ncussion results in the following hypothesis:\nH5: Perceived social biases positively influence per-\nceived risks.\nRegulatory concerns\nRegulatory concerns include governance of autonomous\nAI systems, responsibility and accountability, lack of\nrules of accountability in the use of AI, and lack of offi-\ncial industry standards of AI use and performance evalu-\nation [ 25]. In this study, we consider two dimensions for\nregulatory concerns: perceived unregulated standards\nand perceived liability issues.\nPerceived unregulated standards\nRegulatory concerns are found as critical challenges to\nthe use of AI in healthcare as policies and guidelines for\nAI tools are not transparent yet [ 22]. Existing literature\nindicates that regulatory agencies require to agree on a\nset of standards that medical AI rollout must be rated\nagainst. For instance, how decent is the auditability of\ndecisions made by autonomous AI-based devices? [ 25].\nDue to the intelligence nature of AI systems, regulatory\nagencies should establish new requirements, official pol-\nicy, and safety guidelines regarding AI rollout in health-\ncare [ 10]. For example, there is a legal need to evaluate\nthe decision made by AI systems in case of litigation. AI\ntools operate based on the auto-learn models, which im-\nprove their performance over time [ 46]. This inner\nmechanism differentiates AI-based devices from other\ntools in healthcare and gives rise to new regulatory con-\ncerns that may not be a case in other domains. Gener-\nally, algorithms that change continuously with features\nthat are not limited to the original accepted clinical trials\nmay need a new range of policies and guidelines [ 30].\nRegulatory authorities are yet to formalize standards to\nevaluate and maintain the safety and impact of AI in\nmany countries [ 19]. Thus, people may become con-\ncerned if an appropriate regulatory and accreditation\nsystem regarding AI-based devices is not in place yet.\nThe lack of clear guidelines to monitor the performance\nof AI tools in the medical context can lead to higher risk\nbeliefs associate with AI. We propose the following\nhypothesis:\nH6: Perceived unregulated standards positively influ-\nence perceived risks.\nPerceived liability issues\nAccountability and liability are another concern related\nto AI. Previous studies in public health demonstrate\nlegal concerns about who will account for AI-based deci-\nsions when errors occur using AI systems [ 47]. Wirtz,\nWeyerer [ 48] emphasize the challenges connected to the\nresponsibility and accountability of AI systems. It is still\nnot clear how the regulatory concerns around responsi-\nbility and accountability of using solutions made by AI\nsystems can be dealt with formally [ 25]. As AI-based de-\nvices in healthcare make autonomous decisions, the ac-\ncountability question becomes very hard to answer. For\ninstance, it will create a risky situation for both clinicians\nand patients when it is still not clear who becomes re-\nsponsible if AI-based tools offer wrong recommenda-\ntions in healthcare [ 29]. Liability complexity becomes\nhigher since it is not transparent to what extent AI sys-\ntems are able to guide and control clinical practices [ 49].\nResponsibility concerns are not only limited to the inci-\ndents that AI may generate errors. Another aspect of li-\nability risk refers to the situation where appropriate\ntreatment options recommended by AI are mistakenly\ndismissed [ 21]. Thus, the higher the perceived liability\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 6 of 19\nissues, the greater the risk beliefs associated with AI will\nbe. We pose the next hypothesis as follows:\nH7: Perceived liability issues positively influence per-\nceived risks.\nPerceived risks\nThe risk perceptions related to an IS system can reduce\nthe possible utility attached to the technology [ 50]. As\nAI-based devices are not in line with traditional medical\npractices, the ambiguity about the safety and efficacy of\nAI models in healthcare are still strong reasons that fa-\ncilitate users ’ risks [ 51]. Since the nature of diagnostic\ntasks causes a lack of transparency, current AI systems\nused in healthcare are considered as a black box to\nusers, which acts as a barrier to the adoption of AI tech-\nnology [ 52]. If the degree of uncertainty associated with\nthe use of AI-based tools is high, individuals are less\nlikely to use them in the future. The general risk of using\nAI-based devices for medical purposes exacerbates indi-\nvidual intention to adopt AI. The higher the potential\nloss associated with the use of AI devices, the lower the\npeople’s willingness to use them. Thus, we propose:\nH8: Perceived risks negatively influence individuals ’in-\ntention to use AI-based tools.\nPerceived benefits\nAI can be used in healthcare for risk prediction and rec-\nommendation generation. Big data and AI significantly\nimprove patient health based diagnosis and predictive\ncapability [ 53]. Recent studies show new opportunities\nfor AI applications within medical diagnosis and path-\nology where medical tasks can be done automatedly with\nhigher speed and accuracy [ 52]. AI can improve different\naspects of healthcare delivery, such as diagnostics, prog-\nnosis, and patient management [ 19]. For instance, AI is\nshown to diagnose skin cancer more efficiently than der-\nmatologists [ 54]. A study demonstrates that hedonic as-\npects, such as enjoyment and curiosity about AI\ntechnology, are stronger in predicting behavioral\nintention to use AI products than utilitarian aspects\n(e.g., usefulness) [ 14]. This point does not hold in the\nhealthcare context since AI systems are mainly used in\nhealthcare for utilitarian aspects such as patient-specific\ndiagnosis, treatment decision making, and population\nrisk prediction analysis [ 55]. Thus, in the benefit percep-\ntions, we only focus on utilitarian aspects, not other mo-\ntivational factors. Sun and Medaglia [ 38] identify the\nlack of sufficient knowledge of the AI technologies ’\nvalues and advantages as potential barriers to the adop-\ntion of AI systems. Individuals will endorse and use AI\nclinical tools if they believe that AI will bring essential\nbenefits to their healthcare delivery. The higher the per-\nceived benefits from AI-based devices, the higher the\nindividuals’ intention to use them in the future. Thus,\nwe develop the last hypothesis as follows:\nH9: Perceived benefits positively influence individuals ’\nintention to use AI-based tools.\nPilot test\nOnce the initial questionnaire was developed, we con-\nsulted five professionals in the AI domain to improve\nthe content validity of our study and finalize the AI defi-\nnitions, the mechanisms of AI-based CDS, and the ques-\ntions used in this study. Consistent with the experts ’\nsuggestions, we modified the terms used to define AI\nand improved the scenario and questions to ensure that\nthey were transparent enough and easy to understand\nfor the public. Then, we conducted a face validity with\n15 students (6 Ph.D. and 9 Master ’s degree in IS) to en-\nsure that the readability and wording of the questions\nwere acceptable and consistent with the objectives of\nour study. Thus, we reworded some ambiguous terms\nand removed technical language and jargon to describe\nthe scenario most understandably and straightforwardly.\nFinally, prior to the main data collection, we conducted\na pilot test with 117 undergraduate students at a large\nSoutheastern university in the United States to ensure\nthat the instrument had acceptable reliability and valid-\nity. We computed the Cronbach ’s alpha for each con-\nstruct (i.e., perceived benefits α = 0.94, perceived riks\nα = 0.90, performance risks α = 0.91, perceived social\nbiases α = 0.88, perceived privacy concerns α = 0.94, per-\nceived mistrust in AI mechanisms in AI mechanisms\nα = 0.92, perceived communication barriers α = 0.93, per-\nceived unregulated standard α = 0.94, perceived liability\nissues α = 0.94, intention to use AI-based devices α =\n0.94). All Cronbach ’s alpha values were above the cutoff\npoint of 0.7, indicating that the instrument was intern-\nally consistent [ 56].\nData collection\nData were collected in April 2020 from Amazon ’s Mech-\nanical Turk (MTurk) to obtain a representative group of\nsubjects in the United States. MTurk is a survey tool\nthat has been used in several studies as an acceptable\nmeans to collect individual-level data from the general\npopulation of interest [ 57]. Studies highlight that recruit-\ning survey respondents from MTurk can improve the re-\nliability of data compared to traditional subject pools\n[58]. Researchers as requesters can use this crowdsour-\ncing website to reach out to potential subjects (i.e.,\nMTurk workers) in numerous countries to conduct a\nsurvey. MTurk workers with an Amazon account can\nperform a task (such as participating in a study) in ex-\nchange for a monetary payment.\nSince AI may not be considered as a routine technol-\nogy for many individuals, a detailed description of AI\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 7 of 19\nwas provided at the beginning of the online survey to en-\nsure that respondents completely comprehended the con-\ntext and purpose of the study. Moreover, since the focus\nof this study is on AI-based CDS, we defined a scenario\nabout AI-supported devices with CDS features used for\nhealth care purposes. In the scenario, we described a situ-\nation in which individuals have the option of using an AI-\nbased device when they are suffering from a disease. The\nsteps of using AI technology are clearly explained to re-\nspondents. For instance, in case of feeling sick, they can\ndirectly enter their signs, symptoms, and critical health\ncomplaints into the AI-based device. Their health infor-\nmation will be recorded in a big database. Then, the AI\nsystem analyzes their health data and compares them to\nthe learned patterns (for example, the list of diseases and\nmedicines) and draws some clinical conclusions. Finally,\nbased on the pattern found, AI creates a report including\nsome diagnostic options, some treatment choices, pre-\nscription advice (e.g., dose, frequency, and name of medi-\ncations they need to take), care planning (e.g., resting at\nhome, taking suggested medicines for a specific period or\nvisiting a professional immediately). In summary, we\nhighlighted that the devices with AI-based CDS are able\nto analyze clinical data and make medical decisions for pa-\ntients without direct physician interactions. It should be\nmentioned that the definitions and given scenario were il-\nlustrated in a way in which they are understandable for\nthe general public.\nAfter reading the scenario, respondents were asked to\nreflect their perceptions about possible risks, potential\nbenefits, and intention to use devices with AI-based fea-\ntures in the future. Since the data collection was per-\nformed anonymously, respondents only entered their\ndata related to the main variables of interest and some\nstandard demographic variables (such as age, race, gen-\nder, and age), but their names or any identification num-\nbers were not requested in the survey. The incentive for\nparticipation was a monetary reward ($2).\nAnalyses\nTotally, in a month, 500 individuals completed the sur-\nvey (surveys with incomplete answers were discarded).\nAs mentioned in previous studies, one general concern\nin data collection is a potential lack of attention and ran-\ndom responses [ 59]. Consistent with other studies, we\nused “ captcha” questions to prevent and identify care-\nless, hurried, or haphazard answers [ 60]. For instance, in\nthe captcha questions, the respondents were presented\nwith a challenge (such as reverse coding questions) to\ncapture whether they completed the survey carelessly or\nin a hurry. Based on answers to these verification ques-\ntions, seventy-three responses were dropped. This ratio\nis similar to those reported in previous studies that used\nMTurk for data collection [ 58]. Thus, concerns that\nonline respondents might reply randomly or haphazardly\nto complete the survey quickly were alleviated. After ex-\ncluding responses that failed the response quality ques-\ntions, the final set of useable and valid responses\ncontained 427 samples. We also used Mplus to assess the\npower of analysis and determine the sample size [ 61].\nGiven the number of observed and latent variables in the\nmodel, the anticipated effect size (0.3), the desired prob-\nability (0.8), and statistical power levels ( α =0 . 0 5 a n d\npower β = 0.95), the minimum sample size for the model\nstructure is 400. Therefore, this study is adequately pow-\nered, as 427 samples could be sufficient to reduce possible\nsampling errors and minimize Type II errors. This is con-\nsistent with both the ratio of indicators to the latent vari-\nables approach and the function of minimum effect,\npower and significance suggested by Westland [ 62]. In this\nstudy, data are analyzed with IBM SPSS AMOS (version\n26) in order to test the hypotheses within a Structural\nEquation Modeling (SEM) framework.\nBefore data were statistically analyzed, normality was\nevaluated as this is important for distributions of data to\nexhibit this trait, to facilitate unbiased and consistent\nmodels [ 63]. According to Hair, Black [ 64], skewness ex-\npresses the symmetry while kurtosis explains the\npeakedness of distributions. Thus, all the constructs used\nin the model were scrutinized against the normality as-\nsumptions. An examination of the skewness and kurtosis\nof the constructs showed a skewness range from − 0.068\nto 0.003, and a kurtosis range from − 1.192 to − 0.216.\nBased on these findings, all the values fall within the pre-\nscribed limit and maximum acceptable levels of 2 for\nskewness and 7 for kurtosis tests [ 65].\nTo validate the survey instrument, we performed a Con-\nfirmatory Factor Analysis (CFA) on all the constructs to\nassess the measurement model. IBM SPSS AMOS (version\n26) was used to test convergent validity and discriminant\nvalidity. According to Gefen, Straub [ 66], convergent val-\nidity can be tested by examining the standardized factor\nloading, composite reliability, and the Average Variance\nExtracted (AVE). Table 2 shows the results of convergent\nvalidity test. All values of composite reliabilities were more\nthan the threshold value of 0.7, which highlighted that the\nreliability of constructs was adequate [ 67]. According to\nHair, Black [ 64], a factor loading of 0.7 or greater is ac-\nceptable. In this study, all reported standardized factor\nloadings were greater than 0.7. The AVE of each construct\nwas calculated using standardized factor loadings. All re-\nported values of the AVE were also greater than 0.5, which\nmet the minimum requirement [ 68]. These measures indi-\ncated that the convergent validity of the measurement\nmodel was acceptable.\nWe also tested the discriminant validity of the con-\nstructs (Table 3). All the diagonal values (the square\nroots of the AVEs) were greater than 0.7 and exceeded\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 8 of 19\nTable 2 Results of convergent validity\nConstruct Items Standardized Factor loading (> 0.7) Composite reliability (> 0.7) AVE (> 0.5)\nPerceived Performance Anxiety PPA1 0.86 0.924 0.709\nPPA2 0.86\nPPA3 0.85\nPPA4 0.80\nPPA5 0.84\nPerceived Social Biases PSB1 0.80 0.919 0.694\nPSB2 0.84\nPSB3 0.88\nPSB4 0.78\nPSB5 0.86\nPerceived Privacy Concerns PPC1 0.80 0.952 0.767\nPPC2 0.89\nPPC3 0.90\nPPC4 0.87\nPPC5 0.92\nPPC6 0.87\nPerceived Mistrust in AI Mechanisms PMT1 0.87 0.938 0.751\nPMT2 0.85\nPMT3 0.89\nPMT4 0.89\nPMT5 0.83\nPerceived Communication Barriers PCB1 0.87 0.934 0.738\nPCB2 0.87\nPCB3 0.88\nPCB4 0.90\nPCB5 0.77\nPerceived Unregulated Standards PUS1 0.86 0.944 0.771\nPUS2 0.89\nPUS3 0.88\nPUS4 0.90\nPUS5 0.86\nPerceived Liability Issues PL1 0.89 0.945 0.742\nPL2 0.86\nPL3 0.90\nPL4 0.86\nPL5 0.86\nPL6 0.76\nPerceived Benefits PB1 0.84 0.943 0.705\nPB2 0.85\nPB3 0.89\nPB4 0.84\nPB5 0.85\nPB6 0.74\nPB7 0.86\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 9 of 19\nthe correlations between any pair of constructs [ 69].\nTherefore, the result indicates that the model fulfills the\nrequirements of discriminant validity, and it is assumed\nthat the model also has adequate discriminant validity.\nAlthough the correlations among constructs were not\nhighly noticeable, we checked for multicollinearity by\ncomputing the Variance Inflation Factor (VIF) and toler-\nance values for the predictor variables. The resultant\nVIF values were between 1.79 and 2.64, which were\nbelow the cutoff value of 5, and the tolerance values\nwere in the range of 0.55 and 0.37, which were higher\nthan the threshold of 0.1 [ 56]. Thus, multicollinearity is\nnot an issue in this research. Finally, as using the self-\nreport survey method can raise the common method\nvariance issue, we examined the potential for common\nmethod bias [ 70]. We conducted Harman ’s one-factor\ntest to check if the common method bias would be a\nproblem [ 71]. All factors together could explain 75.40%\nof the total variance, while none of the factors accounted\nfor most of the covariance among measures (< 20%). Ac-\ncordingly, test results implied that common method bias\nwas a non-significant threat in our sample.\nControl variables\nFactors that do not represent the core variables (i.e.,\nthose included in the causal model) of this study, but\nwhich may affect the inter-relationships between the\ncore variables, have been controlled. As mentioned pre-\nviously, we controlled age, gender, race, income, employ-\nment, education, general computer skills, technical\nknowledge about AI technology, and experience with an\nAI-based service. Although the causal model seems to\nrepresent individuals ’ intention to use AI-based devices,\nwe found that the effects of control variables were not\nnegligible. Based on the findings, gender (ß = − 0.12, p <\n0.05), annual household income (ß = 0.11, p < 0.05) edu-\ncation level (ß = 0.14, p < 0.01), employment (ß = 0.12,\np < 0.05), technical knowledge about AI technology (ß =\n0.13, p < 0.01) and familiarity with an AI-based service\n(ß = 0.22, p < 0.001) influence intention to use. These re-\nsults imply that employed male users with higher educa-\ntion levels, higher annual household income, advanced\ntechnical knowledge about AI, and greater experience\nwith AI may exhibit a higher intention to use AI tech-\nnology for healthcare purposes. However, no effects of\nTable 2 Results of convergent validity (Continued)\nConstruct Items Standardized Factor loading (> 0.7) Composite reliability (> 0.7) AVE (> 0.5)\nPerceived Risks PR1 0.8 0.910 0.670\nPR2 0.85\nPR3 0.84\nPR4 0.82\nPR5 0.78\nIntention to Use AI-based Tools INT1 0.83 0.940 0.758\nINT2 0.87\nINT3 0.90\nINT4 0.89\nINT5 0.86\nTable 3 Results of discriminant validity\nConstruct Mean SD. PPA PSB PPC PT PCB PUS PL PB PR INT\nPPA 3.48 0.99 0.842\nPSB 3.19 1.03 0.499 0.833\nPPC 3.41 1.10 0.437 0.439 0.875\nPMT 3.14 0.98 −0.328 −0.177 −0.127 0.866\nPCB 3.62 1.08 0.377 0.403 0.320 −0.168 0.859\nPUS 3.67 1.04 0.437 0.438 0.446 −0.160 0.459 0.878\nPL 3.69 1.06 0.455 0.396 0.399 −0.235 0.555 0.526 0.861\nPB 3.76 1.03 −0.186 − 0.137 − 0.048 0.495 − 0.045 0.049 − 0.045 0.839\nPR 3.49 0.96 0.451 0.464 0.515 −0.478 0.596 0.522 0.553 −0.272 0.818\nINT 3.33 1.08 −0.338 −0.192 − 0.158 0.453 − 0.214 −0.156 − 0.220 0.610 − 0.338 0.870\nTable legend: PPA Perceived Performance Anxiety, PSB Perceived Social Biases, PPC Perceived Privacy Concerns, PMT Perceived Mistrust in AI Mechanisms, PCB\nPerceived Communication Barriers, PUS Perceived Unregulated Standards, PL Perceived Liability Issues, PB Perceived Benefits, PR Perceived Risks, INT Intention to\nUse AI-based Tools\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 10 of 19\nage, race, and general computer skills were found on the\nintention to use.\nResults\nDescriptive statistics\nTable 4 depicts the respondents ’ characteristics. The\ndemographic characteristics show that the majority of\nrespondents were white (69.7%) with a full-time job\n(64.5%). The gender was equally distributed with males\n(51.1%) and females (48.9%). Respondents were fairly\nyoung as 67.1% of them were younger than 40 years old.\nAround 73% of respondents had some college, 2-year\nold degree, or bachelor ’s degree. About 65% had an an-\nnual household income between $25,000 and less than\n$100,000. 41% of participants reported that they used an\nAI-based service for other reasons not related to health-\ncare (such as financial decision making), and around\n55% declared that they were moderately or very familiar\nwith general AI-based devices. Only 23% used an AI-\nenabled health service (such as AI embedded in smart\nmedical devices), and 60% of them were either not famil-\niar or slightly familiar with the AI applications in health-\ncare. Regarding general computer literacy, 80% indicated\nthat their computer skills were good or excellent, and\n74% rated their technical knowledge about AI average or\ngood. Finally, 70% of respondents reported that their\ngeneral health literacy was good or excellent. We can in-\nterpret that, although most of the respondents did not\nexperience an AI-based device for healthcare purposes,\nthey were familiar with general AI tools (for other pur-\nposes) either through direct experience, reading articles,\nfollowing the news, or social media activities.\nStructural model\nI B MS P S SA M O S( v e r s i o n2 6 )w a sa l s ou s e dt ot e s tt h e\nhypotheses within a SEM framework. According to Ho\n[72], the goodness of fit statistics can evaluate the entire\nstructural model and assess the overall fit. The findings in-\ndicated that the normed Chi-square value ( χ2/df) was\n2.23. The indices values for CFI = 0.91, NFI = 0.90, RFI =\n0.93, and TLI = 0.90 were above 0.9 and the SRMR = 0.05\nand RMSEA = 0.06 were below 0.08 [ 73]. The value of\nAGFI was 0.92 that exceeded 0.90. All these measures of\nfit were in the acceptable range, and only GFI = 0.84 were\nmarginal. Based on Kline [ 74], at least four of the statis-\ntical values met the minimum recommended values,\nwhich supported a good fit between the hypothesized\nmodel and the observed data. Figure 2 displays the stan-\ndardized path coefficients of the structural model under\ninvestigation.\nThe structural model was assessed by examining path\ncoefficients. We performed bootstrapping (with 5000\nbootstrap samples) to determine the significance of each\npath. The results of hypotheses testing are summarized\nin Table 5. With respect to technological concerns, the\nfindings support H1 by showing the significant positive\nrelationship between perceived performance anxiety and\nperceived risks ( β = 0.22, p < 0.001). The findings provide\nenough evidence to support H2, which indicates that\nperceived communication barriers significantly reinforce\nperceived risks ( β = 0.36, p < 0.001).\nRegarding ethical concerns, support is not found for H3,\nwhich initially proposes that perceived privacy concerns sig-\nnificantly would contribu te to perceived risks ( β = 0.06,\nnon-significant path). H4 is supported where higher per-\nceived mistrust in AI mechanisms leads to greater per-\nceived risks ( β = 0.20, p < 0.001). H5, which posits that the\nperceived social biases would directly affect perceived risks,\nis not supported ( β = 0.07, non-significant path). With re-\ngard to regulatory concerns, the analysis also demonstrates\nthat individuals’ perception of unregulated standards posi-\ntively influences perceived riks (β = 0.19, p < 0.001), and this\np o s i t i v el i n k a g es u p p o r t sH 6 .T h ep a t hc o e f f i c i e n to ft h er e -\nlationship between perceived liability issues and perceived\nrisks is significant, supporting H7 ( β =0 . 2 1 ,p <0 . 0 5 ) . T h e\nnegative effect of perceived risks associated with AI on indi-\nviduals’ intention to use AI-based tools is significant, sup-\nporting H8 ( β = 0.54, p < 0.001). The findings also provide\nenough evidence to support H9 by indicating that the more\nperceived benefits associated with AI tools, the more likely\nindividuals are to use AI-based devices for healthcare pur-\nposes (β =0 . 8 3a n dp < 0.001).\nFinally, the variables used in the model explained 67%\nof the variance in perceived risks and 80% of the vari-\nance in individuals ’ intention to use AI-based tools. The\nR2 scores reflect that the model provides relatively\nstrong explanatory power to predict individuals ’ adop-\ntion behaviors in the context of AI technology for\nhealthcare purposes.\nDiscussion\nDue to the promising opportunities created by AI tech-\nnology (such as better diagnostic and decision support),\nthe main question is when AI tools will be part of rou-\ntine clinical practice [ 75]. AI embedded in smart devices\ndemocratizes healthcare by bringing AI-enabled health\nservices (such as AI-based CDS) into the homes of pa-\ntients [ 30]. Nevertheless, some concerns related to the\nuse of AI need to be addressed. Because of the sensitivity\nand novelty, the intention to use AI devices in healthcare\nmay involve an alternative approach with stronger pre-\ndiction power than existing models of technology ac-\nceptance. Furthermore, previous AI acceptance models\nare general and do not reflect particular professional\ncontexts and characteristics that may raise multiple con-\ncerns [ 14]. We consider healthcare a sector with distinct,\nvalue-based nature and, thus, in need of a unique model\nfor predicting the intention to use devices with AI-based\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 11 of 19\nTable 4 Sample characteristics\nVariable Categories Percentage (%)\nGender Male 51.1\nFemale 48.9\nAge Under 20 0.7\n20–29 29.6\n30–39 36.8\n40–49 17.6\n50–59 8.5\n60 or older 6.8\nAnnual household income <$25,000 15.6\n$25,000–$49,999 24.8\n$50,000–$74,999 23.1\n$75,000–$99,999 17.3\n$100,000- -$150,000 14.3\nMore than $150,000 4.9\nEducation Less than high school 1\nHigh school graduate 9.1\nSome college 15\n2-year degree 11.1\nBachelor’s degree 47.2\nMaster’s degree 13.4\nDoctorate 3.3\nEmployment status Employed- full time 64.5\nEmployed-part time 15.3\nUnemployed 11.1\nRetired 3.9\nStudent 5.2\nRace/ethnicity White 69.7\nAfrican American 8.8\nAsian 15\nHispanic 4.6\nMixed 1.6\nOther 0.3\nHave you ever used any AI-enabled services or devices for any reason\nexcept for healthcare? (Such as AI embedded in smart devices for any\npurposes such as financial decision-making)\nYes 41\nNo 59\nGenerally, how familiar are you with an AI-based device (used for any\npurposes except for healthcare)?\nNot familiar at all 10.7\nSlightly experienced 26.1\nModerately experienced 37.1\nVery experienced 18.2\nExtremely experienced 7.8\nHave you ever used any AI-enabled health services? (Such as AI\nembedded in smart medical devices)\nYes 23.5\nNo 76.5\nHow familiar are you with these AI-based devices used for clinical purposes? Not familiar at all 30.6\nSlightly experienced 30.0\nModerately experienced 22.8\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 12 of 19\nCDS. We propose a model of AI clinical device accept-\nance that, by an extension of established risk-benefit fac-\ntors [ 12], has a higher explanatory power to predict the\nintention to use AI devices in healthcare. As previous stud-\nies introduce several concerns and challenges with AI [ 25],\nthe main focus of this model is to better explain and\ncategorize factors affecting ris k beliefs associated with AI.\nIn our analysis, we demonstr ate three categories of con-\ncerns with AI: technological, ethical, and regulatory.\nTechnological concerns, which include two dimensions\n(perceived performance anxiety and perceived communica-\ntion barriers), directly shape perceived risk with the use of\nAI. Ethical concerns consist of three dimensions (perceived\nprivacy concerns, perceived mistrust in AI mechanism, and\nperceived social biases), and only trust factor emerge as a\nsignificant variable in the risk beliefs about AI tools. Regula-\ntory concerns with two dimensi ons (perceived unregulated\nstandard and perceived liability issues) directly contribute\nto risk beliefs associated with AI.\nFindings suggest that three categories of individuals ’\nconcerns have a significant impact on their assessment\nof the risks and benefits associated with AI-based CDS\nuse; the stronger the concerns, the higher the risk per-\nceptions, and the lower the benefit perceptions. Of the\nthree categories, technological concerns (i.e., perform-\nance and communication) are found to be the most sig-\nnificant predictors of risk beliefs. This result is in line\nwith previous findings that ambiguity about AI func-\ntional characteristics considerably influences risk percep-\ntions associated with potential future use of AI [ 25].\nThere is a growing interest in research about AI-centric\ntechnologies, yet individuals have not integrated AI de-\nvices into many aspects of their lives [ 76]. We can argue\nthat the general technical knowledge of the public about\nTable 4 Sample characteristics (Continued)\nVariable Categories Percentage (%)\nVery experienced 8.8\nExtremely experienced 7.8\nHave you ever experienced a data breach incident (i.e.., data loss,\nincluding personal, health, or financial information)?\nYes 32.7\nNo 67.3\nOverall, do you think your health information is .....? Sensitive 74.9\nNon-sensitive 16.0\nNo idea 9.1\nHow do you generally rate your computer skills? Terrible 0.7\npoor 0,7\naverage 18.6\nGood 45.0\nExcellent 35.2\nHow do you rate your technical knowledge about AI? Terrible 2.0\npoor 14.3\naverage 49.5\nGood 24.8\nExcellent 9.4\nHow did you gather information about general AI tools? Articles in magazines/newspapers 43.6\nSocial media 35.1\nFriends and family 16.7\nTechnical books 4.6\nHow do you rate your health literacy? Terrible 0.3\npoor 2.9\naverage 26.7\nGood 47.2\nExcellent 22.8\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 13 of 19\nFig. 2 Model paths * P < 0.05, *** P < 0.001\nTable 5 Results of hypotheses testing\nHypothesis Path Standardized Coefficient SE. CR. Results\nH1 PPA → PR 0.22*** 0.03 6.14 Supported\nH2 PCB → PR 0.36*** 0.05 6.03 Supported\nH3 PPC → PR 0.06 0.05 1.06 Not- Supported\nH4 PMT → PR 0.20*** 0.048 4.28 Supported\nH5 PSB → PR 0.07 0.10 0.69 Not-Supported\nH6 PUS → PR 0.19*** 0.04 3.85 Supported\nH7 PL → PR 0.21* 0.10 2.09 Supported\nH8 PR → INT −0.54*** 0.05 9.73 Supported\nH9 PB → INT 0.83*** 0.05 16.27 Supported\nPerceived Risks R2: 0.67\nIntention to Use AI-based Tools R2: 0.80\nTable legend: PPA Perceived Performance Anxiety, PSB Perceived Social Biases, PPC Perceived Privacy Concerns, PMT Perceived Mistrust in AI Mechanisms, PCB\nPerceived Communication Barriers, PUS Perceived Unregulated Standards, PL Perceived Liability Issues, PB Perceived Benefits, PR Perceived Risks, INT Intention to\nUse AI-based Tools. *** P < 0.001, * P < 0.05\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 14 of 19\nAI performance and how it works is still at an early\nstage. If AI-based devices gained more ground in every-\nday care work, people would possibly have more of a\nperspective about benefits and risks to accept the use of\nAI clinical tools.\nMoreover, of the seven antecedents of risk beliefs, com-\nmunication barriers are found to have the strongest rela-\ntionship with risk perceptions. Based on this finding,\nindividuals are concerned that AI devices may reduce hu-\nman- aspects of relations in medical contexts. Therefore,\nthey may lose face-to-face cues and personal interactions\nwith physicians and find themselves in a more passive\nposition for making health-related decisions. This finding\nis consistent with a study in the chatbot context (within\nthe area of AI systems), which indicates that users have\nstronger feelings of co-presence and closeness when the\nchatbot uses social cues [ 77]. In the context of robot care,\nas t u d ys h o w st h a tw h e nr o b o t su s e di nr e h a b i l i t a t i o n ,\nthey are viewed by patients as reducing human contact\n[78]. Developers need to add more interactive and enter-\ntaining social cues to devices with AI-based CDS features\nin healthcare to address the possible communication bar-\nriers between users and AI. For instance, AI-driven rec-\nommendations and assistance can be appealing if the\ndevice holds a promise of allowing users more time to\ninteract with it to establish empathy.\nFindings imply that perceived benefits from AI-based\nCDS significantly increase the intention to use AI tech-\nnology in healthcare. In line with other studies, if users\nbelieve that AI-based devices can improve diagnostics,\nprognosis, and patient management systems, they be-\ncome more likely to use them [ 79]. These results recom-\nmend that AI device developers highlight potential AI\nbenefits in their marketing campaign to promote usabil-\nity as well as the value of their AI tools and increase the\nuse rate. Specific marketing strategies in medical AI de-\nvice companies can be developed to enhance users ’ state\nof awareness about how AI-based tools can suggest ac-\ncurate care planning, reduce healthcare costs, and boost\nhealthcare outcomes.\nThe results also indicate that the effects of benefit percep-\ntions are higher than risk beliefs. This point may imply that\nAI-based CDS developers need to illustrate why AI-driven\nrecommendations are suitabl e for healthcare tasks (i.e.,\nhighlighting benefits), and mo st importantly, they need to\ntake action to address possible concerns (i.e., reducing\nrisks). Thus, if developers attempt to persuade users by fo-\ncusing on advantages, although concerns have not even\nbeen addressed, users are not likely to use AI tools for\nhealthcare purposes. To find ju stification for the use of AI,\nan individual might b e persuaded to adjust his/her value-\nbased evaluations by viewing the change as creating more\nopportunities (health-related benefits) rather than threats\n(technological, ethical, and re gulatory concerns). It should\nbe mentioned that even though the main dependent vari-\nable in this study is the intent ion to use AI-based devices,\nwe do not propose that an unconditional acceptance of AI\nclinical tools, is the ideal situation in healthcare. In contrast,\nwe attempt to exhibit how value -based consideration is im-\nportant when implementing AI devices in healthcare con-\ntexts. If the rejection of the use of medical AI is explained\nby huge and unaddressed technological, ethical, or regula-\ntory concerns, there is not much sense in partially coping\nwith these concerns by setting up the mandatory use of\nmedical AI covering the whole patients. We propose that a\nsuccessful rollout of AI-based devices in healthcare may\nneed to be managed with the knowledge and consideration\nof potential users’ risk-benefit analysis.\nThe results confirm that perceived benefits and risk be-\nliefs associated with AI predict higher intention to use AI-\nbased CDS (R 2 = 80%) compared to previous studies using\nextended acceptance models [ 3]. The data support all hy-\npotheses developed in this study except for H3 and H5.\nAlthough previous conceptual studies suggest the import-\nance of privacy and biases [ 19], our empirical research\ncannot provide evidence to confirm their effects. One pos-\nsible explanation is the characteristics of our participants.\nAround 67% of our sample did not experience a severe\nonline privacy breach (data loss, including personal,\nhealth, or financial information). Previous studies indicate\nthat invasion of privacy in the past significantly influence\nrisk beliefs [ 80]. Moreover, the lack of racial diversity in\nour sample (69.7%: white) may affect the direct relation-\nship between perceived social biases and risk beliefs. Ac-\ncording to previous studies, social biases are mostly\nbelieved against minority groups with insufficient data in\nAI datasets [ 81]. However, the overrepresentation of a ma-\njority group in our sample may cause that most of the re-\nspondents did not encounter societal discrimination due\nto unfair healthcare practice.\nThe second plausible justification is that respondents\ndid not completely believe in AI performance (H1) and\ndid not trust in AI ’s predictive and diagnostic ability for\ntreatment purposes (H4). Thus, we can argue that indi-\nviduals still concern about the competency of AI-driven\ndiagnostic options, treatment choices, prescription ad-\nvice, and care planning. However, they may believe in\nthe security system and technical safeguards embedded\nin AI devices to protect data privacy. Moreover, they\nmay not trust AI competence, but they might have\ntrusted in AI fair process. According to Komiak and\nBenbasat [ 82], people may not trust in expertise and\nknowledge of an information system (cognitive trust in\ncompetence), but they may trust in the integrity of the\nsystem (cognitive trust in integrity). Thus, based on the\nresults, individuals may still not believe in the compe-\ntence of the AI mechanism (such as accuracy of recom-\nmendations), but they think that AI provides unbiased\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 15 of 19\nand honest recommendations and advice to all social\nclasses.\nTheoretical and practical contributions\nThis study makes some important contributions to the\nliterature. Although several studies have examined a var-\niety of AI-related topics in different contexts (for in-\nstance, in service delivery) [ 3], there is still a lack of\nunderstanding of how individuals ’ perceptions toward\nAI-based CDS in healthcare are generated. Previous re-\nsearch mainly studies customers ’ intention to use AI de-\nvices using existing technology acceptance theories\n(such as TAM, UTAUT) [ 83, 84]. Traditional acceptance\nmodels are originally developed to study behavioral\nintention to non-intelligent technologies and do not\ncover the characteristics of intelligent systems [ 85]. In\nthe context of medical AI devices, individuals are likely\nto analyze whether an AI device can deliver the same\nlevel of or better service as physicians are expected to\ndeliver. Thus, there is a need for potential users of AI-\nbased tools within healthcare to understand possible\noutcomes and consequences (both opportunities and\nthreats) of medical diagnosis and solutions created by an\nAI system. Due to the specificity of the healthcare field,\nwe propose that a value-consideration approach would\nbe a better alternative than technology acceptance\nmodels to examine why people will use AI systems in\nhealthcare.\nThe findings suggest that the utilization of the value\nassessment approach may be more applicable to this\ncontext (R 2 = 80%). Therefore, drawing on the value per-\nspective, we propose that individuals ’ final decision to\naccept the use of medical AI devices is also likely to be\ndetermined by their risk-benefit analysis. Our research\nalso identifies the most critical concerns affecting indi-\nviduals’ willingness to use AI devices in healthcare and\nvalidates the antecedents of risk beliefs. This study pro-\nvides a conceptual framework for AI-based CDS accept-\nance that can be used by researchers to better examine\nAI-related topics in the other context.\nThis study has some practical implications for the dif-\nfusion of devices with AI-based CDS in healthcare. In\nthis study, individuals ’ positive perceptions toward AI-\nbased devices can lead to a higher intention to use AI.\nHighlighting the performance benefits such as accuracy\nof diagnosis, reliability of data analysis, the efficiency of\ncare planning, and consistency of treatments in commu-\nnication with users and marketing materials may in-\ncrease individuals ’ intention to at least try services\nprovided by AI devices in healthcare. Moreover, the con-\ncerns and challenges associated with AI have a substan-\ntial effect on the risk perceptions of people. If healthcare\nproviders are not able to reduce concerns, some individ-\nuals may refuse using AI-based devices and may request\ntraditional interactions with physicians. Even if hospitals\ndecide to use AI devices as supportive services under the\nsupervision of healthcare professionals, the mentioned\nconcerns should be eliminated prior to the implementa-\ntion of AI systems.\nAddressing the concerns contributing to risk beliefs\nabout AI is a priority. Society generally is yet to fully\ngrasp many of the ethical and regulatory considerations\nassociated with AI and big data [ 86]. Accountability in-\nvolves a number of stakeholders such as AI developers,\ngovernment agencies, healthcare institutions, healthcare\nprofessionals, and patient communities. Regulatory\nagencies, in cooperation with healthcare institutions,\nshould establish normative standard and evaluation\nguidelines for the implementation and use of AI-based\nCDS in healthcare. The policies should clarify how AI-\nbased devices will be designed and developed in health-\ncare to comply with accepted ethical principles (such as\nfairness and health equity). Regular audits and ongoing\nmonitoring and reporting systems can be used to con-\ntinuously evaluate the safety, quality, transparency, and\nethical factors of AI-based services.\nDevices with AI-based CDS should be designed in a\nway to respect patients ’ autonomy and decision-making\nfreedom. AI agents should not follow a coercive ap-\nproach to force patients to make health-related decisions\nunder pressure. Regulations should illuminate the role of\npatients in relation to AI devices so that they are aware\nof their position to refuse AI-base treatments where pos-\nsible [ 87]. An important aspect that needs to be built\ninto AI systems in healthcare is the transparency of AI\nalgorithms so that the AI system doesn ’t remain a black\nbox to users. Technical education, health knowledge,\nand explicit informed consent should be emphasized in\nthe AI implementation model to prepare patients for AI\nuse. Training should target the patient community to\nensure patients obtain enough information to make in-\nformed health decisions. Thus, if users understand the\nbasics of AI devices, and what benefits and limitations\nthey can bring to healthcare, they become more willing\nto accept AI use to obtain improved healthcare delivery.\nUnder this circumstance, users will be active partners of\nAI tools rather than passive receivers of AI\nrecommendations.\nLimitations and future studies\nIt should be mentioned that the study is based only on a\nsample of respondents drawn from the United States.\nCare work culture and technology use are different be-\ntween countries. Moreover, the lack of racial diversity\n(69% were white) and age variety (66% were between 20\nand 40) of the sample may be considered as a limitation\nin the generalizability of our results. Thus, it is recom-\nmended that future studies consider drawing samples\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 16 of 19\nwith more representative subjects in wider geographical\nareas, including other developed countries and also de-\nveloping countries where technological infrastructures\nand internet services are less developed than in the\nUnited States. Our study used an online survey to recruit\nparticipants digitally. Since a self-rated sample of partici-\npants on MTurk was used, there is a small chance that\nsome respondents were not completely aware of AI\ntechnology and formed their mental construal of the IT\nartifact. Therefore, we suggest that further studies use a\ndifferent method to ensure that subjects are\nknowledgeable about medical AI. For instance, future re-\nsearch can recruit informed patients who are directly re-\nferred by the providers using patient self-management\ntools such as wearable devices with embedded AI. Be-\nsides, our study used an online survey to recruit partici-\npants digitally, which might induce sample selection\nbias. Thus, we only considered individuals who could ac-\ncess the computer, mobile devices, and the Internet to\nparticipate in the online survey. Future studies can use\nother data collection means and sampling strategies to\nreach out to a sample that is generalizable to a wide\nrange of healthcare consumers. Moreover, control vari-\nables (such as familiarity with AI devices, computer\nskills, health literacy, technical knowledge about AI)\nwere rated by a self-assessment scale. Future research\ncould use standard scales (with validated items) to meas-\nure them.\nThis study can also serve as a starting point for further\nempirical studies in the context of individual adoption of\nAI clinical devices. In this study, we used the general\nconcept of AI, and no specific type of AI clinical tools\nwas examined. Value assessments may have different\nunderlying objectives, depending on the type of AI de-\nvice. For instance, it would be interesting to investigate\nhow alternative AI device brands influence risk beliefs\nand, in turn, affect intention. Moreover, we defined AI\ndevices as the tool that consumers can voluntarily\nchoose to use for healthcare management. Another\npromising research avenue would be to examine public\nperspectives in other healthcare contexts, e.g., when AI\ntools are implemented and used in hospitals and health-\ncare professionals recommend that patients use AI de-\nvices. Or for example, a follow-up study is needed to\nexamine users ’ value perceptions in a situation that the\nuse of the AI devices may be a mandatory part of per-\nforming diagnosis and completing patient treatments.\nFuture studies can also extend our findings to examine\nthe acceptance of AI devices among individuals with\nchronic physical diseases or mental disorders and\nanalyze the plausible differences. Since our study focuses\non patients, a follow-up study could investigate the hy-\npotheses with groups of nurses, physicians, therapists,\netc., to examine whether the same factors are associated\nwith technology acceptance of AI-based medical devices\nwhen it comes to healthcare professionals. We also\npropose that AI researchers conduct further studies\nfrom the perspectives of hospital management. Thus,\nother factors, such as economic and organizational chal-\nlenges, should be added to our model as new categories\nof concerns. Finally, although the predictive power of\nthe model is acceptable (80%), it might be useful if fu-\nture studies can add other factors to the model to in-\ncrease exploratory power. For instance, social influence\ncan be integrated into the model since the effect of this\nvariable is particularly important when individuals do\nnot have sufficient knowledge about the technology to\nmake an informed decision.\nConclusions\nDisruptive advances in technology inevitably change so-\ncieties, communications, and working life. One of the\nfundamental changes that could impose significant ef-\nfects on healthcare is the widespread implementation of\nAI devices. AI technology is an integral element of many\norganizations’ business models, and it is a critical stra-\ntegic component in the plans for many sectors of busi-\nness, such as healthcare institutions. Implementing\nadvanced information systems (such as AI) in healthcare\nrequires an in-depth understanding of the factors associ-\nated with technology acceptance among groups of stake-\nholders. One of the most important stakeholders of\ndevices with AI-based CDS is patients. Due to the spe-\ncial characteristics of the healthcare sector, the imple-\nmentation of AI devices should be conducted with\nseveral necessary considerations. From the public per-\nspective, using AI devices is to endorse them. Our model\nsuggests that during a decision-making process, indi-\nviduals go through a stage of appraisal, including\nevaluating the value of AI-based CDS (benefits versus\nrisks). If technological, ethical, ad regulatory concerns\nare not analyzed, rationalized, and resolved accord-\ningly, people may not only use them but also view AI\ndevices as a threat to their healthcare. AI device de-\nvelopers need to highlight potential benefits from AI\ntechnology and address different dimensions of con-\ncerns to justify the purchase and use of an AI tool to\nthe public. Healthcare regulatory agencies need to\nclearly define the right and the responsibility of\nhealthcare professionals, developers, programmers,\nand end-users to demonstrate acceptable approaches\nin the use of AI devices.\nSupplementary information\nSupplementary information accompanies this paper at https://doi.org/10.\n1186/s12911-020-01191-1.\nAdditional file 1 Appendix: Measurement instrument.\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 17 of 19\nAbbreviations\nAI: Artificial Intelligence; CDS: Clinical Decision Support; FDA: Food and Drug\nAdministration; TAM: Technology Acceptance Model; UTAUT: Unified Theory\nof Acceptance and Use of Technology; TPB: Theory of Planned Behavior;\nMTurk: Amazon ’s Mechanical Turk; SEM: Structural Equation Modeling;\nCFA: Confirmatory Factor Analysis; AVE: Average Variance Extracted;\nVIF: Variance Inflation Factor\nAcknowledgments\nNot applicable.\nAuthor’s contributions\nPE compiled the literature required for the study, collected data, conducted\nthe empirical work, prepared the first draft, and approved the final\nmanuscript.\nFunding\nThe author did not receive any funding.\nAvailability of data and materials\nA significant part of the data analyzed in this study is included in this\npublished article. The datasets used and analyzed during the current study\nare available from the corresponding author on reasonable request.\nEthics approval and consent to participate\nThe study was assessed and approved by the Florida International University\nOffice of Research Integrity.\n(IRB-20-0215). Written informed consent to participate in the study was\nobtained from all participants.\nConsent for publication\nNot applicable.\nCompeting interests\nThe author declares that there are no competing interests.\nReceived: 7 May 2020 Accepted: 16 July 2020\nReferences\n1. Kaplan A, Haenlein M. Siri, Siri, in my hand: Who ’s the fairest in the land? On\nthe interpretations, illustrations, and implications of artificial intelligence. Bus\nHoriz. 2019;62(1):15 –25.\n2. Jarrahi MH. Artificial intelligence and the future of work: human-AI\nsymbiosis in organizational decision making. Bus Horiz. 2018;61(4):577 –86.\n3. Gursoy D, Chi OH, Lu L, Nunkoo R. Consumers acceptance of artificially\nintelligent (AI) device use in service delivery. Int J Inf Manag. 2019;49:157 –\n69.\n4. López-Robles J-R, Otegi-Olaso J-R, Gómez IP, Cobo M-J. 30 years of\nintelligence models in management and business: a bibliometric review. Int\nJ Inf Manag. 2019;48:22 –38.\n5. Brufau SR, Wyatt KD, Boyum P, Mickelson M, Moore M, Cognetta-Rieke C. A\nlesson in implementation: a pre-post study of providers ’experience with\nartificial intelligence-based clinical decision support. Int J Med Inform. 2019;\n137:104072.\n6. Coombs C, Hislop D, Taneva SK, Barnard S. The strategic impacts of\nintelligent automation for knowledge and service work: an interdisciplinary\nreview. J Strateg Inf Syst. 2020;101600. In Press.\n7. Khanna S, Sattar A, Hansen D. Artificial intelligence in health –the three big\nchallenges. Australas Med J. 2013;6(5):315.\n8. Dreyer K, Allen B. Artificial intelligence in health care: brave new world or\ngolden opportunity? J Am Coll Radiol. 2018;15(4):655 –7.\n9. Houssami N, Turner RM, Morrow M. Meta-analysis of pre-operative magnetic\nresonance imaging (MRI) and surgical treatment for breast cancer. Breast\nCancer Res Treat. 2017;165(2):273 –83.\n10. Laï M-C, Brian M, Mamzer M-F. Perceptions of artificial intelligence in\nhealthcare: findings from a qualitative survey study among actors in France.\nJ Transl Med. 2020;18(1):1 –13.\n11. Turja T, Aaltonen I, Taipale S, Oksanen A. Robot acceptance model for care\n(RAM-care): a principled approach to the intention to use care robots. Inf\nManage. 2019;57(5):103220.\n12. Kim H-W, Chan HC, Gupta S. Value-based adoption of mobile internet: an\nempirical investigation. Decis Support Syst. 2007;43(1):111 –26.\n13. Chung N, Koo C. The use of social media in travel information search.\nTelematics Inform. 2015;32(2):215 –29.\n14. Sohn K, Kwon O. Technology acceptance theories and factors influencing\nartificial intelligence-based intelligent products. Telematics Inform. 2020;47:\n101324.\n15. Esteva A, Robicquet A, Ramsundar B, Kuleshov V, DePristo M, Chou K, et al.\nA guide to deep learning in healthcare. Nat Med. 2019;25(1):24 –9.\n16. Zhao X, Xia Q, Wayne WH. Impact of technostress on productivity from the\ntheoretical perspective of appraisal and coping processes. Inf Manage. 2020;\n103265. In Press.\n17. Sarin S, Sego T, Chanvarasuth N. Strategic use of bundling for reducing\nconsumers’perceived risk associated with the purchase of new high-tech\nproducts. J Mark Theory Pract. 2003;11(3):71 –83.\n18. Lu L, Cai R, Gursoy D. Developing and validating a service robot integration\nwillingness scale. Int J Hosp Manag. 2019;80:36 –51.\n19. Reddy S, Allan S, Coghlan S, Cooper P. A governance model for the\napplication of AI in health care. J Am Med Inform Assoc. 2020;27(3):\n491–7.\n20. Stewart KA, Segars AH. An empirical examination of the concern for\ninformation privacy instrument. Inf Syst Res. 2002;13(1):36 –49.\n21. Luxton DD. Should Watson be consulted for a second opinion? AMA J\nEthics. 2019;21(2):131 –7.\n22. Cath C. Governing artificial intelligence: ethical, legal and technical\nopportunities and challenges. Philos Trans A Math Phys Eng Sci. 2018;\n376(2133):20180080.\n23. Bansal G, Zahedi FM, Gefen D. Do context and personality matter? Trust and\nprivacy concerns in disclosing private information online. Inf Manage. 2016;\n53(1):1–21.\n24. Lo WLA, Lei D, Li L, Huang DF, Tong K-F. The perceived benefits of an\nartificial intelligence –embedded Mobile app implementing evidence-based\nguidelines for the self-Management of Chronic Neck and Back Pain:\nobservational study. JMIR Mhealth Uhealth. 2018;6(11):e198.\n25. Dwivedi YK, Hughes L, Ismagilova E, Aarts G, Coombs C, Crick T, et al.\nArtificial intelligence (AI): multidisciplinary perspectives on emerging\nchallenges, opportunities, and agenda for research, practice and policy. Int J\nInf Manag. 2019;101994. In Press.\n26. Yang Y, Liu Y, Li H, Yu B. Understanding perceived risks in mobile payment\nacceptance. In: Industrial Management & Data Systems; 2015.\n27. He J, Baxter SL, Xu J, Xu J, Zhou X, Zhang K. The practical implementation\nof artificial intelligence technologies in medicine. Nat Med. 2019;25(1):30 –6.\n28. Mitchell M. Artificial intelligence hits the barrier of meaning. Information.\n2019;10(2):51.\n29. Reddy S, Fox J, Purohit MP. Artificial intelligence-enabled healthcare\ndelivery. J R Soc Med. 2019;112(1):22 –8.\n30. Vayena E, Blasimme A, Cohen IG. Machine learning in medicine: addressing\nethical challenges. PLoS Med. 2018;15(11):e1002689.\n31. Dal Mas F, Piccolo D, Edvinsson L, Presch G, Massaro M, Skrap M, et al. The\neffects of artificial intelligence, robotics, and industry 4.0 technologies. In:\nInsights from the healthcare sector. ECIAIR 2019 European conference on\nthe impact of artificial intelligence and robotics. Oxford: Academic\nConferences and publishing limited; 2019.\n32. Xu J, Yang P, Xue S, Sharma B, Sanchez-Martin M, Wang F, et al. Translating\ncancer genomics into precision medicine with artificial intelligence:\napplications, challenges and future perspectives. Hum Genet. 2019;138(2):\n109–24.\n33. Esmaeilzadeh P. The effects of public concern for information privacy on\nthe adoption of health information exchanges (HIEs) by healthcare entities.\nHealth Commun. 2018:34(10):1202 –11.\n34. Dawson D, Schleiger E, Horton J, McLaughlin J, Robinson C, Quezada G,\net al. Artificial intelligence: Australia ’s ethics framework. Data 61 CSIRO,\nAustralia; 2019.\n35. Zandi D, Reis A, Vayena E, Goodman K. New ethical challenges of digital\ntechnologies, machine learning and artificial intelligence in public health: a\ncall for papers. Bull World Health Organ. 2019;97(1):2.\n36. Char DS, Shah NH, Magnus D. Implementing machine learning in health\ncare— addressing ethical challenges. N Engl J Med. 2018;378(11):981.\n37. Whittlestone J, Nyrup R, Alexandrova A, Dihal K, Cave S. Ethical and societal\nimplications of algorithms, data, and artificial intelligence: a roadmap for\nresearch. London: Nuffield Foundation; 2019.\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 18 of 19\n38. Sun TQ, Medaglia R. Mapping the challenges of artificial intelligence in the\npublic sector: evidence from public healthcare. Gov Inf Q. 2019;36(2):368 –83.\n39. Lee J-G, Kim KJ, Lee S, Shin D-H. Can autonomous vehicles be safe and\ntrustworthy? Effects of appearance and autonomy of unmanned driving\nsystems. Int J Human-Computer Interact. 2015;31(10):682 –91.\n40. Hengstler M, Enkel E, Duelli S. Applied artificial intelligence and trust — the\ncase of autonomous vehicles and medical assistance devices. Technol\nForecast Soc Chang. 2016;105:105 –20.\n41. Angwin J, Larson J, Mattu S, Kirchner L. Machine bias. ProPublica. 2016;23:\n2016.\n42. Edwards SD. The HeartMath coherence model: implications and challenges\nfor artificial intelligence and robotics. AI Soc. 2019;34(4):899 –905.\n43. Stuart R, Peter N. Artificial intelligence-a modern approach 3rd ed. Berkeley:\nPearson Education, Inc.; 2016.\n44. Kirkpatrick K. It's not the algorithm, it's the data. New York: ACM; 2017.\n45. Noble SU. Algorithms of oppression: how search engines reinforce racism:\nnyu press; 2018.\n46. Waring J, Lindvall C, Umeton R. Automated machine learning: review of the\nstate-of-the-art and opportunities for healthcare. Artif Intell Med. 2020;104:\n101822.\n47. Gupta RK, Kumari R. Artificial intelligence in public health: opportunities and\nchallenges. JK Sci. 2017;19(4):191 –2.\n48. Wirtz BW, Weyerer JC, Geyer C. Artificial intelligence and the public\nsector— applications and challenges. Int J Public Adm. 2019;42(7):596 –615.\n49. Pesapane F, Volonté C, Codari M, Sardanelli F. Artificial intelligence as a\nmedical device in radiology: ethical and regulatory issues in Europe and the\nUnited States. Insights Into Imaging. 2018;9(5):745 –53.\n50. Bansal G, Gefen D. The impact of personal dispositions on information\nsensitivity, privacy concern and trust in disclosing health information online.\nDecis Support Syst. 2010;49(2):138 –50.\n51. Parikh RB, Obermeyer Z, Navathe AS. Regulation of predictive analytics in\nmedicine. Science. 2019;363(6429):810 –2.\n52. Tizhoosh HR, Pantanowitz L. Artificial intelligence and digital pathology:\nchallenges and opportunities. J Pathol informatics. 2018;9(38).\n53. Beregi J, Zins M, Masson J, Cart P, Bartoli J, Silberman B, et al. Radiology and\nartificial intelligence: an opportunity for our specialty. Diagn Interv Imaging.\n2018;99(11):677.\n54. Haenssle HA, Fink C, Schneiderbauer R, Toberer F, Buhl T, Blum A, et al. Man\nagainst machine: diagnostic performance of a deep learning convolutional\nneural network for dermoscopic melanoma recognition in comparison to\n58 dermatologists. Ann Oncol. 2018;29(8):1836 –42.\n55. Jiang F, Jiang Y, Zhi H, Dong Y, Li H, Ma S, et al. Artificial intelligence in\nhealthcare: past, present and future. Stroke Vascular Neurol. 2017;2(4):230 –43.\n56. Hair JF, Ringle CM, Sarstedt M. PLS-SEM: indeed a silver bullet. J Mark\nTheory Pract. 2011;19(2):139 –52.\n57. Paolacci G, Chandler J. Inside the Turk: understanding mechanical Turk as a\nparticipant pool. Curr Dir Psychol Sci. 2014;23(3):184 –8.\n58. O'Leary MB, Wilson JM, Metiu A. Beyond being there: the symbolic role of\ncommunication and identification in perceptions of proximity to\ngeographically dispersed colleagues. MIS Q. 2014;38(4):1219 –43.\n59. Huang JL, Curran PG, Keeney J, Poposki EM, DeShon RP. Detecting and\ndeterring insufficient effort responding to surveys. J Bus Psychol. 2012;27(1):\n99–114.\n60. Mason W, Suri S. Conducting behavioral research on Amazon ’s mechanical\nTurk. Behav Res Methods. 2012;44(1):1 –23.\n61. Muthén LK, Muthén BO. How to use a Monte Carlo study to decide on\nsample size and determine power. Struct Equ Model Multidiscip J. 2002;9(4):\n599–620.\n62. Westland JC. Lower bounds on sample size in structural equation modeling.\nElectron Commer Res Appl. 2010;9(6):476 –87.\n63. Andersen PH, Kumar R. Emotions, trust and relationship development in\nbusiness relationships: a conceptual model for buyer –seller dyads. Ind Mark\nManag. 2006;35(4):522 –35.\n64. Hair JF, Black WC, Babin BJ, Anderson RE, Tatham RL. Multivariate data\nanalysis 6th edition. Pearson prentice hall New Jersey humans: critique and\nreformulation. J Abnorm Psychol. 2006;87:49 –74.\n65. West SG, Finch JF, Curran PJ. Structural equation models with nonnormal\nvariables: problems and remedies. Thousand Oaks: Sage Publications, Inc; 1995.\n66. Gefen D, Straub D, Boudreau M-C. Structural equation modeling and\nregression: guidelines for research practice. Commun Assoc Inf Syst. 2000;\n4(1):7.\n67. Chin WW. The partial least squares approach to structural equation\nmodeling. Mod Methods Bus Res. 1998;295(2):295 –336.\n68. Segars AH. Assessing the unidimensionality of measurement: a paradigm\nand illustration within the context of information systems research. Omega.\n1997;25(1):107–21.\n69. Fornell C, Tellis GJ, Zinkhan GM. Validity assessment: a structural equations\napproach using partial least squares. Chicago: Proceedings of the American\nMarketing Association Educators ’Conference; 1982.\n70. Sharma R, Yetton P, Crawford J. Estimating the effect of common method\nvariance: the method — method pair technique with an illustration from\nTAM research. MIS Q. 2009;33(3):473 –90.\n71. Cenfetelli RT, Benbasat I, Al-Natour S. Addressing the what and how of\nonline services: positioning supporting-services functionality and service\nquality for business-to-consumer success. Inf Syst Res. 2008;19(2):161 –81.\n72. Ho R. Handbook of univariate and multivariate data analysis and\ninterpretation with SPSS. Boca Raton: CRC Press Taylor & Francis Group;\n2006.\n73. Byrne BM. Structural equation modeling: perspectives on the present and\nthe future. Int J Test. 2001;1(3 –4):327–34.\n74. Kline RB. Principles and practice of structural equation modeling: Guilford\npublications; 2015.\n75. Froomkin AM, Kerr I, Pineau J. When AIs outperform doctors: confronting\nthe challenges of a tort-induced over-reliance on machine learning. Ariz L\nRev. 2019;61:33.\n76. Fiske A, Henningsen P, Buyx A. Your robot therapist will see you now:\nethical implications of embodied artificial intelligence in psychiatry,\npsychology, and psychotherapy. J Med Internet Res. 2019;21(5):e13216.\n77. Lee S, Lee N, Sah YJ. Perceiving a mind in a Chatbot: effect of mind\nperception and social cues on co-presence, closeness, and intention to use.\nInt J Human –Computer Interaction. 2019;36(10):1 –11.\n78. Sharkey A, Sharkey N. Granny and the robots: ethical issues in robot care for\nthe elderly. Ethics Inf Technol. 2012;14(1):27 –40.\n79. Tran V-T, Riveros C, Ravaud P. Patients ’views of wearable devices and AI in\nhealthcare: findings from the ComPaRe e-cohort. NPJ digital medicine. 2019;\n2(1):1–8.\n80. Malhotra NK, Kim SS, Agarwal J. Internet users' information privacy concerns\n(IUIPC): the construct, the scale, and a causal model. Inf Syst Res. 2004;15(4):\n336–55.\n81. Hong J-W, Williams D. Racism, responsibility and autonomy in HCI: testing\nperceptions of an AI agent. Comput Hum Behav. 2019;100:79 –84.\n82. Komiak SX, Benbasat I. The effects of personalization and familiarity on trust\nand adoption of recommendation agents. MIS Q. 2006;30(4):941 –60.\n83. Dwivedi YK, Rana NP, Jeyaraj A, Clement M, Williams MD. Re-examining the\nunified theory of acceptance and use of technology (UTAUT): towards a\nrevised theoretical model. Inf Syst Front. 2019;21(3):719 –34.\n84. Sundar SS, Waddell TF, Jung EH. The Hollywood robot syndrome media\neffects on older adults' attitudes toward robots and adoption intentions. In:\n2016 11th ACM/IEEE international conference on human-robot interaction\n(HRI): New Zealand: IEEE; 2016.\n85. Lee B, Cranage DA. Causal attributions and overall blame of self-service\ntechnology (SST) failure: different from service failures by employee and\npolicy. J Hosp Market Manag. 2018;27(1):61 –84.\n86. Duan Y, Edwards JS, Dwivedi YK. Artificial intelligence for decision making in\nthe era of big data –evolution, challenges and research agenda. Int J Inf\nManag. 2019;48:63 –71.\n87. Schiff D, Borenstein J. How should clinicians communicate with patients\nabout the roles of artificially intelligent team members? AMA J Ethics. 2019;\n21(2):138–45.\nPublisher’sN o t e\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\nEsmaeilzadeh BMC Medical Informatics and Decision Making          (2020) 20:170 Page 19 of 19",
  "topic": "Health care",
  "concepts": [
    {
      "name": "Health care",
      "score": 0.7396166324615479
    },
    {
      "name": "Health informatics",
      "score": 0.5651217699050903
    },
    {
      "name": "Applications of artificial intelligence",
      "score": 0.5206546187400818
    },
    {
      "name": "Knowledge management",
      "score": 0.49501317739486694
    },
    {
      "name": "Perception",
      "score": 0.4727776050567627
    },
    {
      "name": "Field (mathematics)",
      "score": 0.45765459537506104
    },
    {
      "name": "Clinical decision support system",
      "score": 0.435038298368454
    },
    {
      "name": "Computer science",
      "score": 0.3596351444721222
    },
    {
      "name": "Data science",
      "score": 0.35235151648521423
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3299972414970398
    },
    {
      "name": "Decision support system",
      "score": 0.29900506138801575
    },
    {
      "name": "Psychology",
      "score": 0.2587795853614807
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19700959",
      "name": "Florida International University",
      "country": "US"
    }
  ]
}