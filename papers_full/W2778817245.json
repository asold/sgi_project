{
    "title": "Topic Compositional Neural Language Model",
    "url": "https://openalex.org/W2778817245",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1972671302",
            "name": "Wang Wenlin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2365636915",
            "name": "Gan, Zhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1929489487",
            "name": "Wang Wen-qi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288513539",
            "name": "Shen, Dinghan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2350467844",
            "name": "Huang Jia-ji",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1980672870",
            "name": "Ping Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4302987930",
            "name": "Satheesh, Sanjeev",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2619478227",
            "name": "Carin, Lawrence",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2101105183",
        "https://openalex.org/W1516111018",
        "https://openalex.org/W1591706642",
        "https://openalex.org/W2508661145",
        "https://openalex.org/W2519314406",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2159426623",
        "https://openalex.org/W2962706528",
        "https://openalex.org/W2951183280",
        "https://openalex.org/W2952782394",
        "https://openalex.org/W2157006255",
        "https://openalex.org/W2769182847",
        "https://openalex.org/W1999965501",
        "https://openalex.org/W2951225148",
        "https://openalex.org/W2608962050",
        "https://openalex.org/W2476140796",
        "https://openalex.org/W2186080577",
        "https://openalex.org/W2100163972",
        "https://openalex.org/W2951707031",
        "https://openalex.org/W2250533720",
        "https://openalex.org/W2952723479",
        "https://openalex.org/W2951004968",
        "https://openalex.org/W2962754271",
        "https://openalex.org/W2951326654",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2963773425",
        "https://openalex.org/W2748679025",
        "https://openalex.org/W2250379827",
        "https://openalex.org/W2295103979",
        "https://openalex.org/W2100002341",
        "https://openalex.org/W2197590357",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2402218232",
        "https://openalex.org/W2621133045",
        "https://openalex.org/W2952339051",
        "https://openalex.org/W2170678468",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2147946282",
        "https://openalex.org/W2123442489",
        "https://openalex.org/W2130339025",
        "https://openalex.org/W932413789",
        "https://openalex.org/W2092961325",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3099640513",
        "https://openalex.org/W1880262756",
        "https://openalex.org/W2072644219"
    ],
    "abstract": "We propose a Topic Compositional Neural Language Model (TCNLM), a novel method designed to simultaneously capture both the global semantic meaning and the local word ordering structure in a document. The TCNLM learns the global semantic coherence of a document via a neural topic model, and the probability of each learned latent topic is further used to build a Mixture-of-Experts (MoE) language model, where each expert (corresponding to one topic) is a recurrent neural network (RNN) that accounts for learning the local structure of a word sequence. In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices. The degree to which each member of the ensemble is used is tied to the document-dependent probability of the corresponding topics. Experimental results on several corpora show that the proposed approach outperforms both a pure RNN-based model and other topic-guided language models. Further, our model yields sensible topics, and also has the capacity to generate meaningful sentences conditioned on given topics.",
    "full_text": "Topic Compositional Neural Language Model\nWenlin Wang1 Zhe Gan1 Wenqi Wang3 Dinghan Shen1 Jiaji Huang2\nWei Ping2 Sanjeev Satheesh2 Lawrence Carin1\n1Duke University 2Baidu Silicon Valley AI Lab 3Purdue University\nAbstract\nWe propose a Topic Compositional Neural\nLanguage Model (TCNLM), a novel method\ndesigned to simultaneously capture both the\nglobal semantic meaning and the local word-\nordering structure in a document. The TC-\nNLM learns the global semantic coherence\nof a document via a neural topic model,\nand the probability of each learned latent\ntopic is further used to build a Mixture-of-\nExperts (MoE) language model, where each\nexpert (corresponding to one topic) is a re-\ncurrent neural network (RNN) that accounts\nfor learning the local structure of a word se-\nquence. In order to train the MoE model eﬃ-\nciently, a matrix factorization method is ap-\nplied, by extending each weight matrix of the\nRNN to be an ensemble of topic-dependent\nweight matrices. The degree to which each\nmember of the ensemble is used is tied to the\ndocument-dependent probability of the cor-\nresponding topics. Experimental results on\nseveral corpora show that the proposed ap-\nproach outperforms both a pure RNN-based\nmodel and other topic-guided language mod-\nels. Further, our model yields sensible topics,\nand also has the capacity to generate mean-\ningful sentences conditioned on given topics.\n1 Introduction\nA language model is a fundamental component to nat-\nural language processing (NLP). It plays a key role\nin many traditional NLP tasks, ranging from speech\nrecognition (Mikolov et al., 2010; Arisoy et al., 2012;\nSriram et al., 2017), machine translation (Schwenk\nProceedings of the 21 st International Conference on Ar-\ntiﬁcial Intelligence and Statistics (AISTATS) 2018, Lan-\nzarote, Spain. PMLR: Volume 84. Copyright 2018 by the\nauthor(s).\net al., 2012; Vaswani et al., 2013) to image caption-\ning (Mao et al., 2014; Devlin et al., 2015). Training\na good language model often improves the underlying\nmetrics of these applications, e.g., word error rates for\nspeech recognition and BLEU scores (Papineni et al.,\n2002) for machine translation. Hence, learning a pow-\nerful language model has become a central task in\nNLP. Typically, the primary goal of a language model\nis to predict distributions over words, which has to\nencode both the semantic knowledge and grammat-\nical structure in the documents. RNN-based neural\nlanguage models have yielded state-of-the-art perfor-\nmance (Jozefowicz et al., 2016; Shazeer et al., 2017).\nHowever, they are typically applied only at the sen-\ntence level, without access to the broad document con-\ntext. Such models may consequently fail to capture\nlong-term dependencies of a document (Dieng et al.,\n2016).\nFortunately, such broader context information is of\na semantic nature, and can be captured by a topic\nmodel. Topic models have been studied for decades\nand have become a powerful tool for extracting high-\nlevel semantic structure of document collections, by\ninferring latent topics. The classical Latent Dirichlet\nAllocation (LDA) method (Blei et al., 2003) and its\nvariants, including recent work on neural topic mod-\nels (Wan et al., 2012; Cao et al., 2015; Miao et al.,\n2017), have been useful for a plethora of applications\nin NLP.\nAlthough language models that leverage topics have\nshown promise, they also have several limitations. For\nexample, some of the existing methods use only pre-\ntrained topic models (Mikolov and Zweig, 2012), with-\nout considering the word-sequence prediction task of\ninterest. Another key limitation of the existing meth-\nods lies in the integration of the learned topics into the\nlanguage model; e.g., either through concatenating the\ntopic vector as an additional feature of RNNs (Mikolov\nand Zweig, 2012; Lau et al., 2017), or re-scoring the\npredicted distribution over words using the topic vec-\ntor (Dieng et al., 2016). The former requires a bal-\nance between the number of RNN hidden units and\narXiv:1712.09783v3  [cs.LG]  26 Feb 2018\nTopic Compositional Neural Language Model\nµ log \u0000 2\n✓ ⇠ N ( µ, \u0000 2\n)\n0.15\n0.03\n0.10\n0.07\n0.09\nPolitics\nCompany\nTravel\nMarket\nArt\nNeural Language Model\ng ( · )\n<eos>\n<sos> The guilty\njudgeThe\nLSTM LSTM LSTM\nx M\n0.06\n0.01\n0.11\n0.02\nArmy\nMedical\nEducation\nSport\n0.36\nProportion\nLaw\nTopicNeural Topic Model\n d\n       \nMLP\nt\nq ( t | d )\np ( d | t )\nd\nx 1\ny 1 y 2\nx 2\ny M\nh 2 h M\nFigure 1: The overall architecture of the proposed model.\nthe number of topics, while the latter has to carefully\ndesign the vocabulary of the topic model.\nMotivated by the aforementioned goals and limitations\nof existing approaches, we propose the Topic Compo-\nsitional Neural Language Model (TCNLM), a new ap-\nproach to simultaneously learn a neural topic model\nand a neural language model. As depicted in Figure 1,\nTCNLM learns the latent topics within a variational\nautoencoder (Kingma and Welling, 2013) framework,\nand the designed latent code t quantiﬁes the proba-\nbility of topic usage within a document. Latent code\nt is further used in a Mixture-of-Experts model (Hu\net al., 1997), where each latent topic has a correspond-\ning language model (expert). A combination of these\n“experts,” weighted by the topic-usage probabilities,\nresults in our prediction for the sentences. A ma-\ntrix factorization approach is further utilized to reduce\ncomputational cost as well as prevent overﬁtting. The\nentire model is trained end-to-end by maximizing the\nvariational lower bound. Through a comprehensive\nset of experiments, we demonstrate that the proposed\nmodel is able to signiﬁcantly reduce the perplexity of\na language model and eﬀectively assemble the mean-\ning of topics to generate meaningful sentences. Both\nquantitative and qualitative comparisons are provided\nto verify the superiority of our model.\n2 Preliminaries\nWe brieﬂy review RNN-based language models and\ntraditional probabilistic topic models.\nLanguage Model A language model aims to learn\na probability distribution over a sequence of words in\na pre-deﬁned vocabulary. We denote Vas the vocab-\nulary set and {y1,...,y M }to be a sequence of words,\nwith each ym ∈V. A language model deﬁnes the like-\nlihood of the sequence through a joint probability dis-\ntribution\np(y1,...,y M ) = p(y1)\nM∏\nm=2\np(ym|y1:m−1) . (1)\nRNN-based language models deﬁne the conditional\nprobabiltiy of each word ym given all the previous\nwords y1:m−1 through the hidden state hm:\np(ym|y1:m−1) = p(ym|hm) (2)\nhm = f(hm−1,xm) . (3)\nThe function f(·) is typically implemented as a ba-\nsic RNN cell, a Long Short-Term Memory (LSTM)\ncell (Hochreiter and Schmidhuber, 1997), or a Gated\nRecurrent Unit (GRU) cell (Cho et al., 2014). The\ninput and output words are related via the relation\nxm = ym−1.\nTopic Model A topic model is a probabilistic graph-\nical representation for uncovering the underlying se-\nmantic structure of a document collection. Latent\nDirichlet Allocation (LDA) (Blei et al., 2003), for ex-\nample, provides a robust and scalable approach for\ndocument modeling, by introducing latent variables\nfor each token, indicating its topic assignment. Specif-\nically, let tdenote the topic proportion for document\nd, and zn represent the topic assignment for word wn.\nThe Dirichlet distribution is employed as the prior of\nt. The generative process of LDA may be summarized\nas:\nt∼Dir(α0),zn ∼Discrete(t) ,wn ∼Discrete(βzn ) ,\nwhere βzn represents the distribution over words for\ntopic zn, α0 is the hyper-parameter of the Dirichlet\nprior, n ∈[1,Nd], and Nd is the number of words in\ndocument d. The marginal likelihood for document d\ncan be expressed as\np(d|α0,β) =\n∫\nt\np(t|α0)\n∏\nn\n∑\nzn\np(wn|βzn )p(zn|t)dt.\n3 Topic Compositional Neural\nLanguage Model\nWe describe the proposed TCNLM, as illustrated in\nFigure 1. Our model consists of two key components:\nW. Wang, Z. Gan, W. Wang, D. Shen, J. Huang, W. Ping, S. Satheesh, L. Carin\n(i) a neural topic model (NTM), and ( ii) a neural\nlanguage model (NLM). The NTM aims to capture the\nlong-range semantic meanings across the document,\nwhile the NLM is designed to learn the local semantic\nand syntactic relationships between words.\n3.1 Neural Topic Model\nLet d ∈ZD\n+ denote the bag-of-words representation\nof a document, with Z+ denoting nonnegative inte-\ngers. D is the vocabulary size, and each element of\nd reﬂects a count of the number of times the corre-\nsponding word occurs in the document. Distinct from\nLDA (Blei et al., 2003), we pass a Gaussian random\nvector through a softmax function to parameterize the\nmultinomial document topic distributions (Miao et al.,\n2017). Speciﬁcally, the generative process of the NTM\nis\nθ∼N(µ0,σ2\n0) t= g(θ)\nzn ∼Discrete(t) wn ∼Discrete(βzn ) , (4)\nwhere N(µ0,σ2\n0) is an isotropic Gaussian distribution,\nwith mean µ0 and variance σ2\n0 in each dimension;\ng(·) is a transformation function that maps sample\nθ to the topic embedding t, deﬁned here as g(θ) =\nsoftmax( ˆWθ+ ˆb), where ˆW and ˆb are trainable pa-\nrameters.\nThe marginal likelihood for document dis:\np(d|µ0,σ0,β) =\n∫\nt\np(t|µ0,σ2\n0)\n∏\nn\n∑\nzn\np(wn|βzn )p(zn|t)dt\n=\n∫\nt\np(t|µ0,σ2\n0)\n∏\nn\np(wn|β,t)dt\n=\n∫\nt\np(t|µ0,σ2\n0)p(d|β,t)dt. (5)\nThe second equation in (5) holds because we can read-\nily marginalized out the sampled topic words zn by\np(wn|β,t) =\n∑\nzn\np(wn|βzn )p(zn|t) = βt. (6)\nβ= {β1,β2,..., βT }is the transition matrix from the\ntopic distribution to the word distribution, which are\ntrainable parameters of the decoder; T is the number\nof topics and βi ∈RD is the topic distribution over\nwords (all elements of βi are nonnegative, and they\nsum to one).\nThe re-parameterization trick (Kingma and Welling,\n2013) can be applied to build an unbiased and low-\nvariance gradient estimator for the variational distri-\nbution. The parameter updates can still be derived\ndirectly from the variational lower bound, as discussed\nin Section 3.3.\nDiversity Regularizer Redundance in inferred\ntopics is a common issue exisiting in general topic\nmodels. In order to address this issue, it is straightfor-\nward to regularize the row-wise distance between each\npaired topics to diversify the topics. Following Xie\net al. (2015); Miao et al. (2017), we apply a topic di-\nversity regularization while carrying out the inference.\nSpeciﬁcally, the distance between a pair of topics\nare measured by their cosine distance a(βi,βj) =\narccos\n( |βi·βj|\n||βi||2||βj||2\n)\n. The mean angle of all pairs of\nT topics is φ= 1\nT2\n∑\ni\n∑\nj a(βi,βj), and the variance\nis ν = 1\nT2\n∑\ni\n∑\nj(a(βi,βj) −φ)2. Finally, the topic\ndiversity regularization is deﬁned as R= φ−ν.\n3.2 Neural Language Model\nWe propose a Mixture-of-Experts (MoE) language\nmodel, which consists a set of “expert networks”, i.e.,\nE1,E2,...,E T . Each expert is itself an RNN with its\nown parameters corresponding to a latent topic.\nWithout loss of generality, we begin by discussing an\nRNN with a simple transition function, which is then\ngeneralized to the LSTM. Speciﬁcally, we deﬁne two\nweight tensors W ∈Rnh×nx×T and U ∈Rnh×nh×T ,\nwhere nh is the number of hidden units and nx is the\ndimension of word embedding. Each expert Ek corre-\nsponds to a set of parameters W[k] and U[k], which\ndenotes the k-th 2D “slice” of Wand U, respectively.\nAll T experts work cooperatively to generate an out-\nput ym. Sepciﬁcally,\np(ym) =\nT∑\nk=1\ntk ·softmax(Vh(k)\nm ) (7)\nh(k)\nm = σ(W[k]xm + U[k]hm−1) , (8)\nwhere tk is the usage of topic k (component k of t),\nand σ(·) is a sigmoid function; V is the weight matrix\nconnecting the RNN’s hidden state, used for comput-\ning a distribution over words. Bias terms are omitted\nfor simplicity.\nHowever, such an MoE module is computationally pro-\nhibitive and storage excessive. The training process is\nineﬃcient and even infeasible in practice. To remedy\nthis, instead of ensembling the output of the T experts\nas in (7), we extend the weight matrix of the RNN to\nbe an ensemble of topic-dependent weight matrices.\nSpeciﬁcally, the T experts work together as follows:\np(ym) = softmax(Vhm) (9)\nhm = σ(W(t)xm + U(t)hm−1) , (10)\nTopic Compositional Neural Language Model\nand\nW(t) =\nT∑\nk=1\ntk ·W[k], U(t) =\nT∑\nk=1\ntk ·U[k] . (11)\nIn order to reduce the number of model parameters,\nmotivated by Gan et al. (2016); Song et al. (2016),\ninstead of implementing a tensor as in (11), we de-\ncompose W(t) into a multiplication of three terms\nWa ∈ Rnh×nf , Wb ∈ Rnf ×T and Wc ∈ Rnf ×nx ,\nwhere nf is the number of factors. Speciﬁcally,\nW(t) = Wa ·diag(Wbt) ·Wc\n= Wa ·(Wbt⊙Wc) , (12)\nwhere ⊙represents the Hadamard operator. Wa and\nWc are shared parameters across all topics, to capture\nthe common linguistic patterns. Wb are the factors\nwhich are weighted by the learned topic embedding\nt. The same factorization is also applied for U(t).\nThe topic distribution t aﬀects RNN parameters as-\nsociated with the document when predicting the suc-\nceeding words, which implicitly deﬁnes an ensemble\nof T language models. In this factorized model, the\nRNN weight matrices that correspond to each topic\nshare “structure”.\nNow we generalize the above analysis by using LSTM\nunits. Speciﬁcally, we summarize the new topic com-\npositional LSTM cell as:\nim = σ(Wia ˜xi,m−1 + Uia ˜hi,m−1)\nfm = σ(Wfa ˜xf,m−1 + Ufa ˜hf,m−1)\nom = σ(Woa ˜xo,m−1 + Uoa ˜ho,m−1)\n˜cm = σ(Wca ˜xc,m−1 + Uca ˜hc,m−1)\ncm = im ⊙˜cm + fm ·cm−1\nhm = om ⊙tanh(cm) . (13)\nFor ∗= i,f,o,c , we deﬁne\n˜x∗,m−1 = W∗bt⊙W∗cxm−1 (14)\n˜h∗,m−1 = U∗bt⊙U∗chm−1 . (15)\nCompared with a standard LSTM cell, our LSTM unit\nhas a total number of parameters in size of 4 nf ·(nx +\n2T+3nh) and the additional computational cost comes\nfrom (14) and (15). Further, empirical comparison\nhas been conducted in Section 5.6 to verify that our\nproposed model is superior than using the naive MoE\nimplementation as in (7).\n3.3 Model Inference\nThe proposed model (see Figure 1) follows the varia-\ntional autoencoder (Kingma and Welling, 2013) frame-\nwork, which takes the bag-of-words as input and em-\nbeds a document into the topic vector. This vector is\nthen used to reconstruct the bag-of-words input, and\nalso to learn an ensemble of RNNs for predicting a\nsequence of words in the document.\nThe joint marginal likelihood can be written as:\np(y1:M ,d|µ0,σ2\n0,β) =\n∫\nt\np(t|µ0,σ2\n0)p(d|β,t)\nM∏\nm=1\np(ym|y1:m−1,t)dt. (16)\nSince the direct optimization of (16) is intractable, we\nemploy variational inference (Jordan et al., 1999). We\ndenote q(t|d) to be the variational distribution for t.\nHence, we construct the variational objective function,\nalso called the evidence lower bound (ELBO), as\nL= Eq(t|d) (log p(d|t)) −KL\n(\nq(t|d)||p(t|µ0,σ2\n0)\n)\n  \nneural topic model\n+ Eq(t|d)\n( M∑\nm=1\nlog p(ym|y1:m−1,t)\n)\n  \nneural language model\n(17)\n≤log p(y1:M ,d|µ0,σ2\n0,β) .\nMore details can be found in the Supplementary Mate-\nrial. In experiments, we optimize the ELBO together\nwith the diversity regularisation:\nJ= L+ λ·R. (18)\n4 Related Work\nTopic Model Topic models have been studied for\na variety of applications in document modeling. Be-\nyond LDA (Blei et al., 2003), signiﬁcant extensions\nhave been proposed, including capturing topic cor-\nrelations (Blei and Laﬀerty, 2007), modeling tempo-\nral dependencies (Blei and Laﬀerty, 2006), discover-\ning an unbounded number of topics (Teh et al., 2005),\nlearning deep architectures (Henao et al., 2015; Zhou\net al., 2015), among many others. Recently, neural\ntopic models have attracted much attention, build-\ning upon the successful usage of restricted Boltzmann\nmachines (Hinton and Salakhutdinov, 2009), auto-\nregressive models (Larochelle and Lauly, 2012), sig-\nmoid belief networks (Gan et al., 2015), and variational\nautoencoders (Miao et al., 2016).\nVariational inference has been successfully applied in\na variety of applications (Pu et al., 2016; Wang et al.,\n2017; Chen et al., 2017). The recent work of Miao\net al. (2017) employs variational inference to train\ntopic models, and is closely related to our work. Their\nmodel follows the original LDA formulation and ex-\ntends it by parameterizing the multinomial distribu-\ntion with neural networks. In contrast, our model\nW. Wang, Z. Gan, W. Wang, D. Shen, J. Huang, W. Ping, S. Satheesh, L. Carin\nDataset Vocabulary Training Development Testing\nLM TM # Docs # Sents # Tokens # Docs # Sents # Tokens # Docs # Sents # Tokens\nAPNEWS 32,400 7 ,790 50 K 0.7M 15M 2K 27.4K 0.6M 2K 26.3K 0.6M\nIMDB 34,256 8 ,713 75 K 0.9M 20M 12.5K 0.2M 0.3M 12.5K 0.2M 0.3M\nBNC 41,370 9 ,741 15 K 0.8M 18M 1K 44K 1M 1K 52K 1M\nTable 1: Summary statistics for the datasets used in the experiments.\nenforces the neural network not only modeling doc-\numents as bag-of-words, but also transfering the in-\nferred topic knowledge to a language model for word-\nsequence generation.\nLanguage Model Neural language models have re-\ncently achieved remarkable advances (Mikolov et al.,\n2010). The RNN-based language model (RNNLM)\nis superior for its ability to model longer-term tem-\nporal dependencies without imposing a strong condi-\ntional independence assumption; it has recently been\nshown to outperform carefully-tuned traditional n-\ngram-based language models (Jozefowicz et al., 2016).\nAn RNNLM can be further improved by utilizing the\nbroad document context (Mikolov and Zweig, 2012).\nSuch models typically extract latent topics via a topic\nmodel, and then send the topic vector to a language\nmodel for sentence generation. Important work in\nthis direction include Mikolov and Zweig (2012); Dieng\net al. (2016); Lau et al. (2017); Ahn et al. (2016). The\nkey diﬀerences of these methods is in either the topic\nmodel itself or the method of integrating the topic vec-\ntor into the language model. In terms of the topic\nmodel, Mikolov and Zweig (2012) uses a pre-trained\nLDA model; Dieng et al. (2016) uses a variational au-\ntoencoder; Lau et al. (2017) introduces an attention-\nbased convolutional neural network to extract seman-\ntic topics; and Ahn et al. (2016) utilizes the topic as-\nsociated to the fact pairs derived from a knowledge\ngraph (Vinyals and Le, 2015).\nConcerning the method of incorporating the topic vec-\ntor into the language model, Mikolov and Zweig (2012)\nand Lau et al. (2017) extend the RNN cell with addi-\ntional topic features. Dieng et al. (2016) and Ahn et al.\n(2016) use a hybrid model combining the predicted\nword distribution given by both a topic model and\na standard RNNLM. Distinct from these approaches,\nour model learns the topic model and the language\nmodel jointly under the VAE framework, allowing an\neﬃcient end-to-end training process. Further, the\ntopic information is used as guidance for a Mixture-\nof-Experts (MoE) model design. Under our factoriza-\ntion method, the model can yield boosted performance\neﬃciently (as corroborated in the experiments).\nRecently, Shazeer et al. (2017) proposes a MoE model\nfor large-scale language modeling. Diﬀerent from ours,\nthey introduce a MoE layer, in which each expert\nstands for a small feed-forward neural network on the\nprevious output of the LSTM layer. Therefore, it\nyields a signiﬁcant quantity of additional parameters\nand computational cost, which is infeasible to train\non a single GPU machine. Moreover, they provide\nno semantic meanings for each expert, and all experts\nare treated equally; the proposed model can generate\nmeaningful sentences conditioned on given topics.\nOur TCNLM is similar to Gan et al. (2016). However,\nGan et al. (2016) uses a two-step pipline, ﬁrst learning\na multi-label classiﬁer on a group of pre-deﬁned image\ntags, and then generating image captions conditioned\non them. In comparison, our model jointly learns a\ntopic model and a language model, and focuses on the\nlanguage modeling task.\n5 Experiments\nDatasets We present experimental results on three\npublicly available corpora: APNEWS, IMDB and\nBNC. APNEWS1 is a collection of Associated Press\nnews articles from 2009 to 2016. IMDB is a set of\nmovie reviews collected by Maas et al. (2011), and\nBNC (BNC Consortium, 2007) is the written por-\ntion of the British National Corpus, which contains\nexcerpts from journals, books, letters, essays, mem-\noranda, news and other types of text. These three\ndatasets can be downloaded from GitHub 2.\nWe follow the preprocessing steps in Lau et al. (2017).\nSpeciﬁcally, words and sentences are tokenized using\nStanford CoreNLP (Manning et al., 2014). We lower-\ncase all word tokens, and ﬁlter out word tokens that\noccur less than 10 times. For topic modeling, we ad-\nditionally remove stopwords 3 in the documents and\nexclude the top 0 .1% most frequent words and also\nwords that appear in less than 100 documents. All\nthese datasets are divided into training, development\nand testing sets. A summary statistic of these datasets\nis provided in Table 1.\n1https://www.ap.org/en-gb/\n2https://github.com/jhlau/topically-driven-language-\nmodel\n3We use the following stopwords list: https://github.\ncom/mimno/Mallet/blob/master/stoplists/en.txt\nTopic Compositional Neural Language Model\nDatasetLSTMbasic-LSTM∗ LDA+LSTM∗\nLCLM∗ Topic-RNN TDLM ∗ TCNLM\ntype 50 100 150 50 100 150 50 100 150 50 100 150\nAPNEWSsmall 64 .13 57 .05 55.52 54.83 54 .18 56 .77 54.54 54.12 53.00 52.75 52.65 52.75 52.63 52.59\nlarge 58 .89 52 .72 50.75 50.17 50 .63 53 .19 50.24 50.01 48.96 48.97 48.21 48.07 47.81 47.74\nIMDB small 72 .14 69 .58 69.64 69.62 67 .78 68 .74 67.83 66.45 63.67 63.45 63.82 63.98 62.64 62.59\nlarge 66 .47 63 .48 63.04 62.78 67 .86 63 .02 61.59 60.14 58.99 59.04 58.59 57.06 56.38 56.12\nBNC small 102 .89 96 .42 96.50 96.38 87 .47 94 .66 93.57 93.55 87.42 85.99 86.43 87.98 86.44 86.21\nlarge 94 .23 88 .42 87.77 87.28 80 .68 85 .90 84.62 84.12 82.62 81.83 80.58 80.29 80.14 80.12\nTable 2: Test perplexities of diﬀerent models on APNEWS, IMDB and BNC. (∗) taken from Lau et al. (2017).\nSetup For the NTM part, we consider a 2-layer\nfeed-forward neural network to model q(t|d), with 256\nhidden units in each layer; ReLU (Nair and Hinton,\n2010) is used as the activation function. The hyper-\nparameter λfor the diversity regularizer is ﬁxed to 0.1\nacross all the experiments. All the sentences in a para-\ngraph, excluding the one being predicted, are used to\nobtain the bag-of-words document representation d.\nThe maximum number of words in a paragraph is set\nto 300.\nIn terms of the NLM part, we consider 2 settings: ( i)\na small 1-layer LSTM model with 600 hidden units,\nand (ii) a large 2-layer LSTM model with 900 hidden\nunits in each layer. The sequence length is ﬁxed to 30.\nIn order to alleviate overﬁtting, dropout with a rate of\n0.4 is used in each LSTM layer. In addition, adaptive\nsoftmax (Grave et al., 2016) is used to speed up the\ntraining process.\nDuring training, the NTM and NLM parameters are\njointly learned using Adam (Kingma and Ba, 2014).\nAll the hyper-parameters are tuned based on the per-\nformance on the development set. We empirically ﬁnd\nthat the optimal settings are fairly robust across the\n3 datasets. All the experiments were conducted using\nTensorﬂow and trained on NVIDIA GTX TITAN X\nwith 3072 cores and 12GB global memory.\n5.1 Language Model Evaluation\nPerplexity is used as the metric to evaluate the perfor-\nmance of the language model. In order to demonstrate\nthe advantage of the proposed model, we compare TC-\nNLM with the following baselines:\n•basic-LSTM: A baseline LSTM-based language\nmodel, using the same architecture and hyper-\nparameters as TCNLM wherever applicable.\n•LDA+LSTM: A topic-enrolled LSTM-based\nlanguage model. We ﬁrst pretrain an LDA\nmodel (Blei et al., 2003) to learn 50/100/150 top-\nics for APNEWS, IMDB and BNC. Given a doc-\nument, the LDA topic distribution is incorporated\nby concatenating with the output of the hidden\nstates to predict the next word.\n•LCLM (Wang and Cho, 2016): A context-based\nlanguage model, which incorporates context infor-\nmation from preceding sentences. The preceding\nsentences are treated as bag-of-words, and an at-\ntention mechanism is used when predicting the\nnext word.\n•TDLM (Lau et al., 2017): A convolutional topic\nmodel enrolled languge model. Its topic knowl-\nedge is utilized by concatenating to a dense layer\nof a recurrent language model.\n•Topic-RNN (Dieng et al., 2016): A joint learn-\ning framework that learns a topic model and a\nlanguage model simutaneously. The topic infor-\nmation is incorporated through a linear transfor-\nmation to rescore the prediction of the next word.\nTopic-RNN (Dieng et al., 2016) is implemented by our-\nselves and other comparisons are copied from (Lau\net al., 2017). Results are presented in Table 2. We\nhighlight some observations. (i) All the topic-enrolled\nmethods outperform the basic-LSTM model, indicat-\ning the eﬀectiveness of incorporating global seman-\ntic topic information. ( ii) Our TCNLM performs the\nbest across all datasets, and the trend keeps improv-\ning with the increase of topic numbers. ( iii) The im-\nproved performance of TCNLM over LCLM implies\nthat encoding the document context into meaningful\ntopics provides a better way to improve the language\nmodel compared with using the extra context words di-\nrectly. (iv) The margin between LDA+LSTM/Topic-\nRNN and our TCNLM indicates that our model sup-\nplies a more eﬃcient way to utilize the topic informa-\ntion through the joint variational learning framework\nto implicitly train an ensemble model.\n5.2 Topic Model Evaluation\nWe evaluate the topic model by inspecting the co-\nherence of inferred topics (Chang et al., 2009; New-\nman et al., 2010; Mimno et al., 2011). Following Lau\net al. (2014), we compute topic coherence using nor-\nmalized PMI (NPMI). Given the top n words of a\ntopic, the coherence is calculated based on the sum\nof pairwise NPMI scores between topic words, where\nthe word probabilities used in the NPMI calculation\nare based on co-occurrence statistics mined from En-\nglish Wikipedia with a sliding window. In practice,\nW. Wang, Z. Gan, W. Wang, D. Shen, J. Huang, W. Ping, S. Satheesh, L. Carin\nDataset army animal medical market lottory terrorism law art transportation education\nAPNEWS\nafghanistan animals patients zacks casino syria lawsuit album airlines studentsveterans dogs drug cents mega iran damages music fraud mathsoldiers zoo fda earnings lottery militants plaintiﬀs ﬁlm scheme schoolsbrigade bear disease keywords gambling al-qaida ﬁled songs conspiracy educationinfantry wildlife virus share jackpot korea suit comedy ﬂights teachers\nIMDB\nhorror action family children war detective sci-ﬁ negative ethic epsiodezombie martial rampling kids war eyre alien awful gay seasonslasher kung relationship snoopy che rochester godzilla unfunny school episodesmassacre li binoche santa documentary book tarzan sex girls serieschainsaw chan marie cartoon muslims austen planet poor women columbogore fu mother parents jews holmes aliens worst sex batman\nBNC\nenvironment education politics business facilities sports art award expression crimepollution courses elections corp bedrooms goal album john eye policeemissions training economic turnover hotel score band award looked murdernuclear students minister unix garden cup guitar research hair killedwaste medau political net situated ball music darlington lips juryenvironmental education democratic proﬁts rooms season ﬁlm speaker stared trail\nTable 3: 10 topics learned from our TCNLM on APNEWS, IMDB and BNC.\n# Topic Model CoherenceAPNEWS IMDB BNC\n50\nLDA∗ 0.125 0.084 0.106NTM∗ 0.075 0.064 0.081TDLM(s)∗ 0.149 0.104 0.102TDLM(l)∗ 0.130 0.088 0.095Topic-RNN(s) 0.134 0.103 0.102Topic-RNN(l) 0.127 0.096 0.100TCNLM(s) 0.159 0.106 0.114TCNLM(l) 0.152 0 .100 0 .101\n100\nLDA∗ 0.136 0.092 0.119NTM∗ 0.085 0.071 0.070TDLM(s)∗ 0.152 0.087 0.106TDLM(l)∗ 0.142 0.097 0.101Topic-RNN(s) 0.158 0.096 0.108Topic-RNN(l) 0.143 0.093 0.105TCNLM(s) 0.160 0.101 0.111TCNLM(l) 0.152 0 .098 0 .104\n150\nLDA∗ 0.134 0.094 0.119NTM∗ 0.078 0.075 0.072TDLM(s)∗ 0.147 0.085 0.100TDLM(l)∗ 0.145 0.091 0.104Topic-RNN(s) 0.146 0.089 0.102Topic-RNN(l) 0.137 0.092 0.097TCNLM(s) 0.153 0.0960.107TCNLM(l) 0.155 0.093 0 .102\nTable 4: Topic coherence scores of diﬀerent models\non APNEWS, IMDB and BNC. (s) and (l) indicate\nsmall and large model, respectively.(∗) taken from Lau\net al. (2017).\nwe average topic coherence over the top 5 /10/15/20\ntopic words. To aggregate topic coherence score for\na trained model, we then further average the coher-\nence scores over topics. For comparison, we use the\nfollowing baseline topic models:\n• LDA: LDA (Blei et al., 2003) is used as a base-\nline topic model. We use LDA to learn the topic\ndistributions for LDA+LSTM.\n• NTM: We evaluate the neural topic model pro-\nposed in Cao et al. (2015). The document-topic\nand topic-words multinomials are expressed using\nneural networks. N-grams embeddings are incor-\nporated as inputs of the model.\n• TDLM (Lau et al., 2017): The same model as\nused in the language model evaluation.\n• Topic-RNN (Dieng et al., 2016): The same\nmodel as used in the language model evaluation.\nResults are summarized in Table 4. Our TCNLM\nachieves promising results. Speciﬁcally, ( i) we achieve\nthe best coherence performance over APNEWS and\nIMDB, and are relatively competitive with LDA on\nBNC. (ii) We also observe that a larger model may\nresult in a slightly worse coherence performance. One\npossible explanation is that a larger language model\nmay have more impact on the topic model, and the in-\nherited stronger sequential information may be harm-\nful to the coherence measurement. ( iii) Additionally,\nthe advantage of our TCNLM over Topic-RNN indi-\ncates that our TCNLM supplies a more powerful topic\nguidance.\nIn order to better understand the topic model, we pro-\nvide the top 5 words for 10 randomly chosen topics\non each dataset (the boldface word is the topic name\nsummarized by us), as shown in Table 3. These results\ncorrespond to the small network with 100 neurons. We\nalso present some inferred topic distributions for sev-\neral documents from our TCNLM in Figure 2. The\ntopic usage for a speciﬁc document is sparse, demon-\nstrating the eﬀectiveness of our NTM. More inferred\ntopic distribution examples are provided in the Sup-\nplementary Material.\n5.3 Sentence Generation\nAnother advantage of our TCNLM is its capacity to\ngenerate meaningful sentences conditioned on given\ntopics. Given topic i, we construct an LSTM genera-\ntor by using only the i-th factor of Wb and Ub. Then\nwe start from a zero hidden state, and greedily sample\nwords until an end token occurs. Table 5 shows the\ngenerated sentences from our TCNLM learned with\n50 topics using the small network. Most of the sen-\ntences are strongly correlated with the given topics.\nMore interestingly, we can also generate reasonable\nsentences conditioned on a mixed combination of top-\nics, even if the topic pairs are divergent, e.g., “an-\nimal” and “lottory” for APNEWS. More examples\nare provided in the Supplementary Material. It shows\nTopic Compositional Neural Language Model\nData Topic Generated Sentences\nAPNEWS\narmy •a female sergeant, serving in the fort worth, has served as she served in the military in iraq .animal •most of the bear will have stumbled to the lake .medical •physicians seeking help in utah and the nih has had any solutions to using the policy and uses oﬄine to be ﬁtted with a testing or body .market •the company said it expects revenue of $<unk>million to $<unk>million in the third quarter .lottory •where the winning numbers drawn up for a mega ball was sold .\narmy+terrorism•the taliban ’s presence has earned a degree from the 1950-53 korean war in pakistan ’s historic life since 1964 , with two example of<unk>soldiers from wounded iraqi army shootings and bahrain in the eastern army .animal+lottory•she told the newspaper that she was concerned that the buyer was in a neighborhood last year and had a gray wolf .\nIMDB\nhorror •the killer is a guy who is n’t even a zombie .action •the action is a bit too much , but the action is n’t very good .\nfamily •the ﬁlm is also the story of a young woman whose<unk>and<unk>and very yet ultimately sympathetic ,<unk>relationship ,<unk>,and palestine being equal , and the old man , a<unk>.children •i consider this movie to be a children ’s ﬁlm for kids .war •the documentary is a documentary about the war and the<unk>of the war .horror+negative•if this movie was indeed a horrible movie i think i will be better oﬀ the ﬁlm .\nsci-ﬁ+children•paul thinks him has to make up when the<unk>eugene discovers defeat in order to take too much time without resorting to mortal bugs ,and then ﬁnds his wife and boys .\nBNC\nenvironment•environmentalists immediate base calls to defend the world .education •the school has recently been founded by a<unk>of the next generation for two years .politics •a new economy in which privatization was announced on july 4 .business •net earnings per share rose<unk>% to $<unk>in the quarter , and $<unk>m , on turnover that rose<unk>% to $<unk>m.facilities •all rooms have excellent amenities .environment+politics•the commission ’s report on oct. 2 , 1990 , on jan. 7 denied the government ’s grant to ” the national level of water ” .art+crime •as well as 36, he is returning freelance into the red army of drama where he has ﬁnally been struck for their premiere .\nTable 5: Generated sentences from given topics. More examples are provided in the Supplementary Material.\n0 20 40 60 80 1000\n0.1\n0.2\n0.3\n0.4\n0 20 40 60 80 1000\n0.1\n0.2\n0.3\n0.4\n0 20 40 60 80 1000\n0.1\n0.2\n0.3\n0.4\nApnews IMDB BNC\nFigure 2: Inferred topic distributions on one sample document in each dataset. Content of the three documents\nis provided in the Supplementary Mateiral.\nthat our TCNLM is able to generate topic-related sen-\ntences, providing an interpretable way to understand\nthe topic model and the language model simulaneously.\nThese qualitative analysis further demonstrate that\nour model eﬀectively assembles the meaning of topics\nto generate sentences.\n5.4 Empirical Comparison with Naive MoE\nWe explore the usage of a naive MoE language model\nas in (7). In order to ﬁt the model on a single GPU ma-\nchine, we train a NTM with 30 topics and each NLM of\nthe MoE is a 1-layer LSTM with 100 hidden units. Re-\nsults are summarized in Table 6. Both the naive MoE\nand our TCNLM provide better performance than the\nbasic LSTM. Interestingly, though requiring less com-\nputational cost and storage usage, our TCNLM out-\nperforms the naive MoE by a non-trivial margin. We\nattribute this boosted performance to the “structure”\ndesign of our matrix factorization method. The inher-\nent topic-guided factor control signiﬁcantly prevents\noverﬁtting, and yields eﬃcient training, demonstrating\nthe advantage of our model for transferring semantic\nknowledge learned from the topic model to the lan-\nguage model.\nDataset basic-LSTM naive MoE TCNLM\nAPNEWS 101.62 85.87 82.67\nIMDB 105.29 96.16 94.64\nBNC 146.50 130.01 125.09\nTable 6: Test perplexity comparison between the naive\nMoE implementation and our TCNLM on APNEWS,\nIMDB and BNC.\n6 Conclusion\nWe have presented Topic Compositional Neural Lan-\nguage Model (TCNLM), a new method to learn a topic\nmodel and a language model simultaneously. The\ntopic model part captures the global semantic meaning\nin a document, while the language model part learns\nthe local semantic and syntactic relationships between\nwords. The inferred topic information is incorporated\ninto the language model through a Mixture-of-Experts\nmodel design. Experiments conducted on three cor-\npora validate the superiority of the proposed approach.\nFurther, our model infers sensible topics, and has the\ncapacity to generate meaningful sentences conditioned\non given topics. One possible future direction is to ex-\ntend the TCNLM to a conditional model and apply it\nfor the machine translation task.\nW. Wang, Z. Gan, W. Wang, D. Shen, J. Huang, W. Ping, S. Satheesh, L. Carin\nReferences\nS. Ahn, H. Choi, T. P¨ arnamaa, and Y. Bengio. A\nneural knowledge language model. arXiv preprint\narXiv:1608.00318, 2016.\nE. Arisoy, T. N. Sainath, B. Kingsbury, and B. Ram-\nabhadran. Deep neural network language models.\nIn NAACL-HLT Workshop, 2012.\nD. M. Blei and J. D. Laﬀerty. Dynamic topic models.\nIn ICML, 2006.\nD. M. Blei and J. D. Laﬀerty. A correlated topic model\nof science. The Annals of Applied Statistics , 2007.\nD. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirich-\nlet allocation. JMLR, 2003.\nB. BNC Consortium. The british national cor-\npus, version 3 (bnc xml edition). Dis-\ntributed by Bodleian Libraries, University of\nOxford, on behalf of the BNC Consortium.\nURL:http://www.natcorp.ox.ac.uk/, 2007.\nZ. Cao, S. Li, Y. Liu, W. Li, and H. Ji. A novel neural\ntopic model and its supervised extension. In AAAI,\n2015.\nJ. Chang, S. Gerrish, C. Wang, J. L. Boyd-Graber,\nand D. M. Blei. Reading tea leaves: How humans\ninterpret topic models. In NIPS, 2009.\nC. Chen, C. Li, L. Chen, W. Wang, Y. Pu, and\nL. Carin. Continuous-time ﬂows for deep generative\nmodels. arXiv preprint arXiv:1709.01179 , 2017.\nK. Cho, B. Van Merri¨ enboer, C. Gulcehre, D. Bah-\ndanau, F. Bougares, H. Schwenk, and Y. Ben-\ngio. Learning phrase representations using RNN\nencoder-decoder for statistical machine translation.\nIn EMNLP, 2014.\nJ. Devlin, H. Cheng, H. Fang, S. Gupta, L. Deng,\nX. He, G. Zweig, and M. Mitchell. Language models\nfor image captioning: The quirks and what works.\narXiv preprint arXiv:1505.01809 , 2015.\nA. B. Dieng, C. Wang, J. Gao, and J. Pais-\nley. Topicrnn: A recurrent neural network with\nlong-range semantic dependency. arXiv preprint\narXiv:1611.01702, 2016.\nZ. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin.\nScalable deep poisson factor analysis for topic mod-\neling. In ICML, 2015.\nZ. Gan, C. Gan, X. He, Y. Pu, K. Tran, J. Gao,\nL. Carin, and L. Deng. Semantic compositional\nnetworks for visual captioning. arXiv preprint\narXiv:1611.08002, 2016.\n´E. Grave, A. Joulin, M. Ciss´ e, D. Grangier, and\nH. J´ egou. Eﬃcient softmax approximation for gpus.\narXiv preprint arXiv:1609.04309 , 2016.\nR. Henao, Z. Gan, J. Lu, and L. Carin. Deep poisson\nfactor modeling. In NIPS, 2015.\nG. E. Hinton and R. R. Salakhutdinov. Replicated\nsoftmax: an undirected topic model. In NIPS, 2009.\nS. Hochreiter and J. Schmidhuber. Long short-term\nmemory. In Neural computation, 1997.\nY. H. Hu, S. Palreddy, and W. J. Tompkins. A patient-\nadaptable ecg beat classiﬁer using a mixture of ex-\nperts approach. IEEE transactions on biomedical\nengineering, 1997.\nM. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and\nL. K. Saul. An introduction to variational methods\nfor graphical models. Machine learning, 1999.\nR. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer,\nand Y. Wu. Exploring the limits of language mod-\neling. arXiv preprint arXiv:1602.02410 , 2016.\nD. Kingma and J. Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nD. P. Kingma and M. Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013.\nH. Larochelle and S. Lauly. A neural autoregressive\ntopic model. In NIPS, 2012.\nJ. H. Lau, D. Newman, and T. Baldwin. Machine\nreading tea leaves: Automatically evaluating topic\ncoherence and topic model quality. In EACL, 2014.\nJ. H. Lau, T. Baldwin, and T. Cohn. Topically\ndriven neural language model. arXiv preprint\narXiv:1704.08012, 2017.\nA. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y.\nNg, and C. Potts. Learning word vectors for senti-\nment analysis. In ACL, 2011.\nC. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel,\nS. Bethard, and D. McClosky. The stanford corenlp\nnatural language processing toolkit. In ACL, 2014.\nJ. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and\nA. Yuille. Deep captioning with multimodal re-\ncurrent neural networks (m-rnn). arXiv preprint\narXiv:1412.6632, 2014.\nY. Miao, L. Yu, and P. Blunsom. Neural variational\ninference for text processing. In ICML, 2016.\nY. Miao, E. Grefenstette, and P. Blunsom. Discov-\nering discrete latent topics with neural variational\ninference. arXiv preprint arXiv:1706.00359 , 2017.\nT. Mikolov and G. Zweig. Context dependent recur-\nrent neural network language model. SLT, 2012.\nT. Mikolov, M. Karaﬁ´ at, L. Burget, J. Cernock` y, and\nS. Khudanpur. Recurrent neural network based lan-\nguage model. In Interspeech, 2010.\nD. Mimno, H. M. Wallach, E. Talley, M. Leenders,\nand A. McCallum. Optimizing semantic coherence\nin topic models. In EMNLP, 2011.\nTopic Compositional Neural Language Model\nV. Nair and G. E. Hinton. Rectiﬁed linear units im-\nprove restricted boltzmann machines. In ICML,\n2010.\nD. Newman, J. H. Lau, K. Grieser, and T. Bald-\nwin. Automatic evaluation of topic coherence. In\nNAACL, 2010.\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.\nBleu: a method for automatic evaluation of machine\ntranslation. In ACL, 2002.\nY. Pu, Z. Gan, R. Henao, X. Yuan, C. Li, A. Stevens,\nand L. Carin. Variational autoencoder for deep\nlearning of images, labels and captions. In NIPS,\n2016.\nH. Schwenk, A. Rousseau, and M. Attik. Large, pruned\nor continuous space language models on a gpu for\nstatistical machine translation. In NAACL-HLT\nWorkshop, 2012.\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis,\nQ. Le, G. Hinton, and J. Dean. Outrageously\nlarge neural networks: The sparsely-gated mixture-\nof-experts layer. arXiv preprint arXiv:1701.06538 ,\n2017.\nJ. Song, Z. Gan, and L. Carin. Factored temporal\nsigmoid belief networks for sequence learning. In\nICML, 2016.\nA. Sriram, H. Jun, S. Satheesh, and A. Coates. Cold\nfusion: Training seq2seq models together with lan-\nguage models. arXiv preprint arXiv:1708.06426 ,\n2017.\nY. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.\nSharing clusters among related groups: Hierarchical\ndirichlet processes. In NIPS, 2005.\nA. Vaswani, Y. Zhao, V. Fossum, and D. Chiang. De-\ncoding with large-scale neural language models im-\nproves translation. In EMNLP, 2013.\nO. Vinyals and Q. Le. A neural conversational model.\narXiv preprint arXiv:1506.05869 , 2015.\nL. Wan, L. Zhu, and R. Fergus. A hybrid neural\nnetwork-latent topic model. In AISTAT, 2012.\nT. Wang and H. Cho. Larger-context language mod-\nelling with recurrent neural network. ACL, 2016.\nW. Wang, Y. Pu, V. K. Verma, K. Fan, Y. Zhang,\nC. Chen, P. Rai, and L. Carin. Zero-shot learning\nvia class-conditioned deep generative models. arXiv\npreprint arXiv:1711.05820, 2017.\nP. Xie, Y. Deng, and E. Xing. Diversifying restricted\nboltzmann machine for document modeling. In\nKDD, 2015.\nM. Zhou, Y. Cong, and B. Chen. The poisson gamma\nbelief network. In NIPS, 2015.\nW. Wang, Z. Gan, W. Wang, D. Shen, J. Huang, W. Ping, S. Satheesh, L. Carin\nSupplementary Material for:\nTopic Compositional Neural Language Model\nA Detailed model inference\nWe provide the detailed derivation for the model in-\nference. Start from (16), we have\nlog p(y1:M ,d|µ0,σ2\n0,β)\n= log\n∫\nt\np(t)\nq(t|d)q(t|d)p(d|t)\nM∏\nm=1\np(ym|y1:m−1,t)dt\n= log Eq(t|d)\n(\np(t)\nq(t|d)p(d|t)\nM∏\nm=1\np(ym|y1:m−1,t)\n)\n≥Eq(t|d)\n(\nlog p(d|t) −log q(t|d)\np(t) +\nM∑\nm=1\nlog p(ym|y1:m−1,t)\n)\n= Eq(t|d) (log p(d|t)) −KL\n(\nq(t|d)||p(t|µ0,σ2\n0)\n)\n  \nneural topic model\n+ Eq(t|d)\n(M∑\nn=1\nlog p(ym|y1:m−1,t)\n)\n  \nneural language model\n.\nB Documents used to infer topic\ndistributions\nThe documents used to infer the topic distributions\nploted in Figure 2 are provided below.\nApnews : colombia ’s police director says six police\noﬃcers have been killed and a seventh wounded in am-\nbush in a rural southwestern area where leftist rebels\noperate . gen. jose roberto leon tells the associated\npress that the oﬃcers were riding on four motorcycles\nwhen they were attacked with gunﬁre monday after-\nnoon on a rural stret ch of highway in the cauca state\ntown of padilla . he said a front of the revolutionary\narmed forces of colombia , or farc , operates in the area\n. if the farc is r esponsible , the deaths would bring to\n15 the number of security force members killed since\nthe government and rebels formally opened peace talks\nin norway on oct. 18 . the talks to end a nearly ﬁve-\ndecade-old conﬂict are set to begin in earnest in cuba\non nov. 15 .\nIMDB : having just watched this movie for a second\ntime , some years after my initial viewing , my feel-\nings remain unchanged . this is a solid sci-ﬁ drama\nthat i enjoy very much . what sci-ﬁ elements there are\n, are primarily of added interest rather than the main\nsubstance of the ﬁlm . what this movie is really about\nis wartime conﬂ ict , but in a sci-ﬁ setting . it has a\nsolid cast , from the ever reliable david warner to the\nup and coming freddie prinze jr , also including many\nbritish tv regu lars ( that obviously add a touch of class\n:) , not forgetting the superb tcheky karyo . i feel this\nis more of an ensemble piece than a starring vehicle\n. reminisc ent of wwii ﬁlms based around submarine\ncombat and air-combat ( the ﬁghters seem like adapta-\ntions of wwii corsairs in their design , evoking a retro\nfeel ) this is one of few american ﬁlms that i felt was\nnot overwhelmed by sentiment or saccharine . the sets\nand special eﬀects are all well done , never detracting\nform the bel ievability of the story , although the kil-\nrathi themselves are rather under developed and one\ndimensional . this is a ﬁlm more about humanity in\nconﬂict rather than a ﬁlm about exploring a new and\noriginal alien race or high-brow sci-ﬁ concepts . forget\nthat it ’s sci-ﬁ , just watch and enjoy .\nBNC: an army and civilian exercise went ahead in\nsecret yesterday a casualty of the general election .\nthe simulated disaster in exercise gryphon ’s lift was a\nmidair coll ision between military and civilian planes\nover catterick garrison . hamish lumsden , the min-\nistry of defence ’s chief press oﬃcer who arrived from\nlondon , said : ’ there ’s an absolute ban on proactive\npr during an election . ’ journalists who reported to\ngaza barracks at 7.15 am as instructed were told they\nwould not be all owed to witness the exercise , which\ninvolved 24 airmobile brigade , north yorkshire police\n, ﬁre and ambulance services , the county emergency\nplanning department a nd ’ casualties ’ from longlands\ncollege , middlesbrough . the aim of the gryphon lift\nwas to test army support for civil emergencies . brief\ndetails supplied to th e press outlined the disaster . a\nfully loaded civilian plane crashes in mid-air with an\narmed military plane over catterick garrison . the 1st\nbattalion the green ho wards and a bomb disposal squad\ncordon and clear an area scattered with armaments .\n24 airmobile ﬁeld ambulance , which served in the gulf\nwar , tends a burning , pa cked youth hostel hit by\npieces of aircraft . 38 engineer regiment from claro\nbarracks , ripon , search a lake where a light aircraft\ncrashed when hit by ﬂying wreck age . civilian emer-\ngency services , including the police underwater team\n, were due to work alongside military personnel under\nthe overall co-ordination of the police . mr lumsden\nsaid : ’ there is a very very strict rule that during a\ngeneral election nothing that the government does can\nintrude on the election process . ’\nTopic Compositional Neural Language Model\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\n0 50 1000\n0.1\n0.2\n0.3\n0.4\nApnews\nIMDB\nBNC\nFigure 3: Inferred topic distributions for the ﬁrst 5 documents in the development set over each dataset.\nC More inferred topic distribution\nexamples\nWe present the inferred topic distributions for the ﬁrst\n5 documents in the development set over each dataset\nin Figure 3.\nD More generated sentences\nWe present generated sentences using the topics listed\nin Table 3 for each dataset. The generated sentences\nfor a single topic are provided in Table 7, 8, 9; the\ngenerated sentences for a mixed combination of topics\nare provided in Table 10.\nW. Wang, Z. Gan, W. Wang, D. Shen, J. Huang, W. Ping, S. Satheesh, L. Carin\nTopic Generated Sentences\narmy\n•a female sergeant, serving in the fort worth, has served as she served in the military in iraq .•obama said the obama administration is seeking the state ’s expected endorsement of a family by afghan soldiers at the militaryin world war ii, whose lives at the base of kandahar .•the vfw announced ﬁnal results on the u.s. and a total of $ 5 million on the battleﬁeld , but he ’s still running for the democratic nominationfor the senate .\nanimal •most of the bear will have stumbled to the lake .•feral horses takes a unique mix to forage for their pets and is birth to humans .•the zoo has estimated such a loss of half in captivity , which is soaked in a year .\nmedical\n•physicians seeking help in utah and the nih has had any solutions to using the policy and uses oﬄine to be ﬁtted with a testing or body .•that triggers monday for the study were found behind a list of breast cancer treatment until the study , as does in nationwide , has 60 daysto sleep there .•right stronger , including the virus is reﬂected in one type of drug now called in the clinical form of radiation .\nmarket •the company said it expects revenue of $<unk>million to $<unk>million in the third quarter .•biglari said outside parliament district of january, up $ 4.30 to 100 cents per share , the last summer of its year to $ 2 .•four analysts surveyed by zacks expected $<unk>billion .\nlottory •the numbers drawn friday night were<unk>.•where the winning numbers drawn up for a mega ball was sold .•the jackpot is expected to be in july .\nterrorism\n•the russian oﬃcials have previously said the senior president made no threats .•obama began halting control of the talks friday and last year in another round of the peace talks after the north ’s artillery attack therewednesday have<unk>their highest debate over his cultural weapons soon .•the turkish regime is using militants into mercenaries abroad to take on gates was ﬁred by the west and east jerusalem in recent years\nlaw •the eeoc lawsuit says it ’s entitled to work time for repeated reporting problems that would kick a nod on cheap steel from the owner•the state allowed marathon to ﬁle employment , and the ncaa has a broken record of sale and ﬁned for $<unk>for a check•the taxpayers in the lawsuit were legally alive and march<unk>, or past at improper times of los alamos\nart\n•quentin tarantino ’s announcements that received the movie<unk>spanish•cathy johnson , jane ’s short-lived singer steve dean and ” the broadway music musical show , ” the early show , ” adds classics ,<unk>or 5,500 , while restaurants have picked other<unk>next•katie said , he ’s never created the drama series : the movies could drops into his music lounge and knife away in a long forgotten gown\ntransportation\n•young and bernard madoﬀ would pay more than $ 11 million in banks , the airline said announced by the<unk>•the fraud case included a delta ’s former business travel business oﬃcial whose fake cards ” led to the scheme , ” and to have been morethan $ 10,000 .•a former u.s. attorney ’s oﬃce cited in a fraud scheme involving two engines , including mining companies led to the government fromthe government .\neducation\n•the state ’s<unk>school board of education is not a<unk>.•assembly member<unk>, charter schools chairman who were born in new york who married districts making more than lifelong educationplay the issue , tells the same story that they ’ll be able to support the legislation .•the state ’s leading school of grant staﬀ has added the current schools to<unk>students in a<unk>class and ripley aims to servechild<unk>and social sciences areas ﬁlled into in may and the latest resources\nTable 7: More generated sentences using topics learned from APNEWS.\nTopic Generated Sentences\nhorror\n•the killer is a guy who is n’t even a zombie .•the cannibals want to one scene , the girl has something out of the head and chopping a concert by a shark in the head , and he heads in theshower while somebody is sent to<unk>.•a bunch of teenage slasher types are treated to a girl who gets murdered by a group of friends .\naction\n•the action is a bit too much , but the action is n’t very good .•some really may be that the war scene was a trademark<unk>when ﬁghting sequences were used by modern kung fu ’s rubbish .•action packed neatly , a fair amount of gunplay and science ﬁction acts as a legacy to a cross between 80s and , great gunplay and scenery .\nfamily\n•the ﬁlm is also the story of a young woman whose<unk>and<unk>and very yet ultimately sympathetic ,<unk>relationship ,<unk>., and palestine being equal , and the old man , a<unk>.•catherine seeks work and her share of each other , a<unk>desire , and submit to her , but he does not really want to rethink her issues ,or where he aborted his mother ’s quest to<unk>.•then i ’m about the love , but after a family meeting , her friend aditya ( tatum<unk>) marries a 16 year old girl , will be able tounderstand the amount of her boyfriend anytime\nchildren•snoopy starts to learn what we do but i ’m still bringing up in there .•i consider this movie to be a children ’s ﬁlm for kids .•my favorite childhood is a touch that depicts her how the mother was what they apparently would ’ve brought it to the right place to fox .\nwar •the documentary is a documentary about the war and the<unk>of the war.•one of the major failings of the war is that the germans also struggle to overthrow the death of the muslims and the nazi regime , and<unk>.•the ﬁlm goes , far as to the political , but the news that will be<unk>at how these people can be reduced to a rescue .\ndetective\n•hopefully that ’s starting<unk>as half of rochester takes the character in jane ’s way , though holmes managed to make tyrone powerperfected a lot of magical stuﬀ , playing the one with hamlet today .•while the ﬁlm was based on the stage adaptation , i know she looked up to suspect from the entire production .•there was no previous version in my book i saw , only to those that read the novel , and i thought that no part that he was to why are farmore professional .\nsci-ﬁ •the monster is much better than the alien in which the<unk>was required for nearly every moment of the ﬁlm .•were the astronauts feel like enough to challenge the space godzilla , where it ﬁrst prevails•but the adventure that will arise from the earth is having a monster that can make it clear that the aliens are not wearing the<unk>all thatwill<unk>the laser .\nnegative•the movie reinforces my token bad ratings - it ’s the worst movie i have ever seen .•it was pretty bad , but aside from a show to the 2 idiots in their cast members , i ’m psychotic.•we had the garbage using peckinpah ’s movies with so many<unk>, i can not recommend this ﬁlm to anyone else .\nethic •englund earlier in a supporting role , a closeted gay gal reporter who apparently hopes to disgrace the girls in being sexual .•this ﬁlm is just plain stupid and insane , and a little bit of cheesy .•the ﬁlm is well made during its turbulent , exquisite , warm and sinister joys , while a glimpse of teen relationships .\nepisode\n•3 episodes as a<unk>won 3 emmy series !•i remember the sopranos on tv , early 80 ’s , ( and in my opinion it was an abc show made to a minimum ) . .•the show is notable ( more of course , not with the audience ) , and most of the actors are excellent and the overall dialogue is nice towatch ( the show may have been a great episode )\nTable 8: More generated sentences using topics learned from IMDB.\nTopic Compositional Neural Language Model\nTopic Generated Sentences\nenvironment\n•environmentalists immediate base calls to defend the world .\n•on the formation of the political and the federal space agency , the ec ’s long-term interests were scheduled to assess global warming\nand aimed at developing programmes based on the american industrial plants .\n•companies can reconstitute or use large synthetic oil or gas .\neducation\n•the school has recently been founded by a<unk>of the next generation for two years .\n•the institute for student education committees depends on the attention of the ﬁrst received from the top of lasmo ’s ﬁrst round of\nthe year .\n•66 years later the team joined blackpool , with<unk>lives from the very good and beaten for all support crew – and a new<unk>\nstudent calling under ulster news may provide an<unk>modern enterprise .\npolitics\n•the restoration of the government announced on nov. 4 that the republic’s independence to direct cuba ( three years of electoral\n<unk>would be also held in the united kingdom (<unk>) .\n•a new economy in which privatization was announced on july 4 .\n•agreements were hitherto accepted from simpliﬁed terms the following august 1969 in the april elections [ see pp.<unk>. ]\nbusiness\n•net earnings per share rose<unk>% to $<unk>in the quarter , and $<unk>m , on turnover that rose<unk>% to $<unk>m.\n•the insurance management has issued the ﬁrst quarter net proﬁt up<unk>- $<unk>m , during turnover up<unk>% at $<unk>\nm ; net proﬁt for the six months has reported the cumulative losses of $\n•the ﬁrst quarter would also have a small loss of ﬁve ﬁgures in eﬀect , due following eﬀorts of $ 7.3 .\nfacilities\n•the hotel is situated in the<unk>and the<unk>.\n•all rooms have excellent amenities .\n•the restaurant is in a small garden , with its own views and<unk>.\nsports\n•the<unk>, who had a<unk>win over the<unk>, was a<unk>goal .\n•harvey has been an thrashing goal for both<unk>and the institutional striker fell gently on the play-oﬀ tip and , through in regular\n<unk>to in the season .\n•botham ’s team took the fourth chance over a season without playing .\nart\n•radio code said the band ’s ﬁrst album ’ newest ’ army club and<unk>album .\n•they have a<unk>of the ﬁrst album , ’ run ’ for a ’<unk>’ , the orchestra , which includes the<unk>, which is and the band ’s life .\n•nearly all this year ’s album ’s a sell-out tour !\naward\n•super french for them to meet winners : the label<unk>with just # 50,000 should be paid for a very large size .\n•a spokeswoman for<unk>said : this may have been a matter of practice .\n•female speaker they ’ll start discussing music at their home and why decisions are celebrating children again , but their popularity has\narisen for environmental research .\nexpression\n•roirbak stared at him , and the smile hovering .\n•but they must have seen it in the great night , aunt , so you made the blush poor that fool .\n•making her cry , it was fair .\ncrime\n•the prosecution say the case is not the same .\n•the chief inspector supt michael<unk>, across bristol road , and delivered on the site that the police had been accused to take him\njob because it is not above the legal services .\n•she was near the same time as one of the eight men who died taken prisoner and subsequently stabbed , where she was hit away .\nTable 9: More generated sentences using topics learned from BNC.\nData Topic Generated Sentences\nAPNEWS\narmy+terrorism\n•the taliban ’s presence has earned a degree from the 1950-53 korean war in pakistan ’s historic life since 1964 , with twoexample of<unk>soldiers from wounded iraqi army shootings and bahrain in the eastern army .•at the same level , it would eventually be given the government administration ’s enhanced military force since the war .•the<unk>previously blamed for the attacks in afghanistan , which now covers the afghan army , and the united nations will bea great opportunity to practice it .\nanimal+lottory\n•when the numbers began , the u.s. ﬁsh and wildlife service unveiled a gambling measure by agreeing to acquire a permit by animalprotection staﬀ after the previous permits became selected from the governor ’s oﬃce .•she told the newspaper that she was concerned that the buyer was in a neighborhood last year and had a gray wolf .•the tippecanoe county historical society says it was n’t selling any wolf hunts .\nIMDB\nhorror+negative\n•if this movie was indeed a horrible movie i think i will be better oﬀ the ﬁlm .•he starts talking to the woman when the person gets the town, she suddenly loses children for blood and it ’s annoying to deatheven though it is up to her fans and baby.•what ’s really scary about this movie is it ’s not that bad .\nsci-ﬁ+children\n•mystery inc is a lot of smoke , when a trivial , whiny girl<unk>troy<unk>and a woman gets attacked by the<unk>captain (played by hurley ) .•paul thinks him has to make up when the<unk>eugene discovers defeat in order to take too much time without resorting tomortal bugs , and then ﬁnds his wife and boys .•the turtles are grown up to billy ( as he takes the rest of the ﬁre ) and the scepter is a family and is dying .\nBNC\nenvironment+politics\n•<unk>shallow water area complex in addition an international activity had been purchased to hit<unk>tonnes of nuclear powerat the un plant in<unk>, which had begun strike action to the people of southern countries .•the national energy minister , michael<unk>of<unk>, has given a ” right ” route to the united kingdom ’s european parliament, but to be passed by<unk>, the ﬁrst and fourth states .•the commission ’s report on oct. 2 , 1990 , on jan. 7 denied the government ’s grant to ” the national level of water ” .\nart+crime •as well as 36 , he is returning freelance into the red army of drama where he has ﬁnally been struck for their premiere .•by alan donovan , two arrested by one guest of a star is supported by some teenage women for the queen .•after the talks , the record is already featuring shaun<unk>’s play ’<unk>’ in the quartet of the ira bomb .\nTable 10: More generated sentences using a mixed combination of topics."
}