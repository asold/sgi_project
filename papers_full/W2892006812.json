{
    "title": "Visual speech language models",
    "url": "https://openalex.org/W2892006812",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A5031962033",
            "name": "Helen L. Bear",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2082308025",
        "https://openalex.org/W1992790156",
        "https://openalex.org/W2741151796",
        "https://openalex.org/W3045485643",
        "https://openalex.org/W943204654",
        "https://openalex.org/W82494927",
        "https://openalex.org/W2963658982",
        "https://openalex.org/W2793493229"
    ],
    "abstract": "Language models (LM) are very powerful in lipreading systems. Language models built upon the ground truth utterances of datasets learn grammar and structure rules of words and sentences (the latter in the case of continuous speech). However, visual co-articulation effects in visual speech signals damage the performance of visual speech LM's as visually, people do not utter what the language model expects. These models are commonplace but while higher-order N-gram LM's may improve classification rates, the cost of this model is disproportionate to the common goal of developing more accurate classifiers. So we compare which unit would best optimize a lipreading (visual speech) LM to observe their limitations. We compare three units; visemes (visual speech units) \\cite{lan2010improving}, phonemes (audible speech units), and words.",
    "full_text": "Visual speech language models\nHelen L Bear\nQueen Mary University of London, UK & Technische Universit¨ at Mu¨ nchen, Germany\nh.bear@qmul.ac.uk, dr.bear@tum.de\n1 Introduction\nLanguage models (LM) are very powerful in lipreading systems (e.g. in [1,2]).\nLanguage models built upon the ground truth utterances of datasets learn gram-\nmar and structure rules of words and sentences (the latter in the case of con-\ntinuous speech). However, visual co-articulation eﬀects in visual speech signals\ndamage the performance of visual speech LM’s as visually, people do not ut-\nter what the language model expects [3]. These models are commonplace but\nwhile higher-order N-gram LM’s may improve classiﬁcation rates, the cost of\nthis model is disproportionate to the common goal of developing more accurate\nclassiﬁers. So we compare which unit would best optimize a lipreading (visual\nspeech) LM to observe their limitations. As in [4] we compare three units; visemes\n(visual speech units) [5], phonemes (audible speech units), and words.\nIn the ﬁrst two columns of Table 1 we list pairings of classiﬁer units and lan-\nguage model units. For each pair we build a conventional lipreading system with\nthe HTK toolkit [6] to classify Active Appearance model [7] features extracted\non 12 speakers from the RMAV audio-visual speech dataset[1]. Phonemes are\nthe International Phonetic Alphabet [8], and our visemes are speaker-dependent\nvisemes [9,10]. Word labels are from the RMAV ground truth. Classiﬁer units\nare the labels used to identify individual classiﬁcation models and language\nunits make up the label scheme used for building the post-classiﬁcation decod-\ning LM.\n2 Analysis\nFig 1 shows word correctness (on the y-axis) for each speaker along the x-axis\nover three ﬁgures, one per LM unit. The viseme LM is on the left, phoneme LM\nmiddle, and word LM on the right. The viseme LM is the lowest correctness\n(0.02 ± 0.0063). On the surface the idea of visemes classiﬁers is a good one\nbecause they take visual co-articulation into account to some extent. However\nas seen here, an LM of visemes is too complex due to the eﬀect of homophemes\n[11]. The phoneme LM (Fig 1, middle) is more exciting. For all speakers we see\na statistically signiﬁcant increase in Cw compared to the viseme LM Cw in Fig 1\nleft. Looking more closely between speakers, we see that for four speakers (2,\n9, 10 and 12), the viseme classiﬁers outperform the phonemes, yet for all other\nspeakers there is no signiﬁcant diﬀerence. On average they are identical with an\nall-speaker mean Cw of 0.19 ± 0.0036 (Table 1).\nLastly in Fig 1 (right) is Cw of a word model paired with classiﬁers built on\nviseme, phoneme, and word units. Here word classiﬁers perform very poorly. WearXiv:1809.06800v1  [eess.AS]  14 Sep 2018\n2 Bear H.L\nsp01 sp02 sp03 sp04 sp05 sp06 sp07 sp08 sp09 sp10 sp11 sp12\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nViseme network\nsp01 sp02 sp03 sp04 sp05 sp06 sp07 sp08 sp09 sp10 sp11 sp12\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nPhoneme network\nsp01 sp02 sp03 sp04 sp05 sp06 sp07 sp08 sp09 sp10 sp11 sp12\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nWord network\nFig. 1.Eﬀects of support network unit choice with each type of labeled HMM classiﬁer\nunits. Along the x-axis is each speaker, y-axis values are correctness, C. Viseme LM is\non the left, phoneme LM in the middle, and word LM on the right.\nattribute this to insuﬃcient training samples per class due to the extra number of\nclasses in the word space (> 1000 in RMAV) compared to the number of classes\nin the phoneme space (49) and so we do not recommend word-based classiﬁers\nwithout large volumes of visual speech data such as in [12,13]. Also in Fig 1\n(bottom), are the phoneme and viseme classiﬁers (in green and red respectively)\nwith a word LM. Here, for ﬁve of our twelve speakers (3, 5, 7, 8, and 11), the\nphoneme classiﬁers out-perform the visemes and for the other speakers there is\nno signiﬁcant diﬀerence once a word LM is applied. This demonstrates that the\nstrength of a good word network can help negate translations between acoustic\nand visual speech spaces. Fig 1 Cw values and one standard error values are in\nTable 1. This suggests phoneme units are most robust for visual speech language\nmodels but in practical terms this is not an easily intelligible output so words\nare preferred.\nTable 1.All speaker mean Cw for each pair of HMM and language model units.\nClassiﬁer units Network units Cw 1se\nViseme Viseme 0.02 0.0063\nViseme Phoneme 0.19 0.0036\nViseme Word 0.09 0.0\nPhoneme Phoneme 0.19 0.0036\nPhoneme Word 0.20 0.0043\nWord Word 0.19 0.0005\n3 Conclusion\nFor some speakers viseme classiﬁers with phoneme LMs are the better choice\nwhereas others are easier to lipread with phoneme classiﬁers with a word LM.\nAs experimenters we have no evidence to know which approach is best for a\ntest speakers until after testing all unit options, so we recommend using either\nphoneme or word-based language network for future lipreading system develop-\nment as these enable more interpretable prediction transcripts. Whilst lipreading\nLMs are powerful, they are not the solution to training machines to lipread all\nspeakers due to the great variation in speaker visual speech signals.\nVisual speech language models 3\nReferences\n1. Bowden, R., Cox, S., Harvey, R., Lan, Y., Ong, E.J., Owen, G., Theobald, B.J.:\nRecent developments in automated lip-reading. In: Optics and Photonics for Coun-\nterterrorism, Crime Fighting and Defence IX; and Optical Materials and Biomate-\nrials in Security and Defence Systems Technology X. Volume 8901., International\nSociety for Optics and Photonics (2013) 89010J\n2. Lan, Y., Harvey, R., Theobald, B.J.: Insights into machine lip reading. In: Inter-\nnational Conference on Acoustics, Speech and Signal Processing (ICASSP). (2012)\n4825–4828\n3. Lieberman, P.: Some eﬀects of semantic and grammatical context on the production\nand perception of speech. Language and speech 6(3) (1963) 172–187\n4. Bear, H.L., Harvey, R.: Alternative visual units for an optimized phoneme-based\nlipreading system *under review*. Computer Speech & Language (2018)\n5. Lan, Y., Theobald, B.J., Harvey, R., Ong, E.J., Bowden, R.: Improving visual\nfeatures for lip-reading. In: Auditory-Visual Speech Processing 2010. (2010)\n6. Young, S.J., Evermann, G., Gales, M., Kershaw, D., Moore, G., Odell, J., Ollason,\nD., Povey, D., Valtchev, V., Woodland, P.: The HTK book version 3.4. (2006)\n7. Matthews, I., Baker, S.: Active appearance models revisited. International Journal\nof Computer Vision 60(2) (2004) 135–164\n8. Association, I.P.: Handbook of the International Phonetic Association: A guide\nto the use of the International Phonetic Alphabet. Cambridge University Press\n(1999)\n9. Bear, H.L., Harvey, R.: Phoneme-to-viseme mappings: the good, the bad, and the\nugly. Speech Communication (2017)\n10. Bear, H.L., Harvey, R.: Comparing heterogeneous visual gestures for measuring\nthe diversity of visual speech signals. Computer Speech & Language (2018)\n11. Thangthai, K., Bear, H.L., Harvey, R.: Comparing phonemes and visemes with\ndnn-based lipreading. British Machine Vision Conference (BMVC) Deep learning\nfor machine lip reading workshop (2017)\n12. Chung, J.S., Zisserman, A.: Learning to lip read words by watching videos. Com-\nputer Vision and Image Understanding (2018)\n13. Stafylakis, T., Tzimiropoulos, G.: Deep word embeddings for visual speech recog-\nnition. ICASSP (2018)\nThis abstract is an extended extract from Alternative visual units for an optimized\nphoneme-based lipreading system, Bear & Harvey, Computer Speech and Language,\n2018 (in review)."
}