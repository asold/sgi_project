{
    "title": "Toward Transformer-Based Object Detection",
    "url": "https://openalex.org/W3114896399",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287158023",
            "name": "Beal, Josh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109358853",
            "name": "Kim Eric",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4289488894",
            "name": "Tzeng, Eric",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202120482",
            "name": "Park, Dong Huk",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4280750316",
            "name": "Zhai, Andrew",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287158027",
            "name": "Kislyuk, Dmitry",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3015146382",
        "https://openalex.org/W2099001231",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2970692043",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2943152387",
        "https://openalex.org/W3104962541",
        "https://openalex.org/W2296073425",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2991391304",
        "https://openalex.org/W3018997018",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963703197",
        "https://openalex.org/W2950628590",
        "https://openalex.org/W3042608254",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2079057609",
        "https://openalex.org/W3104911444",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2994759459",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3121480429",
        "https://openalex.org/W2945918281",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W3014732459",
        "https://openalex.org/W3100020884",
        "https://openalex.org/W2949533892",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2053782355",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W3094502228"
    ],
    "abstract": "Transformers have become the dominant model in natural language processing, owing to their ability to pretrain on massive amounts of data, then transfer to smaller, more specific tasks via fine-tuning. The Vision Transformer was the first major attempt to apply a pure transformer model directly to images as input, demonstrating that as compared to convolutional networks, transformer-based architectures can achieve competitive results on benchmark classification tasks. However, the computational complexity of the attention operator means that we are limited to low-resolution inputs. For more complex tasks such as detection or segmentation, maintaining a high input resolution is crucial to ensure that models can properly identify and reflect fine details in their output. This naturally raises the question of whether or not transformer-based architectures such as the Vision Transformer are capable of performing tasks other than classification. In this paper, we determine that Vision Transformers can be used as a backbone by a common detection task head to produce competitive COCO results. The model that we propose, ViT-FRCNN, demonstrates several known properties associated with transformers, including large pretraining capacity and fast fine-tuning performance. We also investigate improvements over a standard detection backbone, including superior performance on out-of-domain images, better performance on large objects, and a lessened reliance on non-maximum suppression. We view ViT-FRCNN as an important stepping stone toward a pure-transformer solution of complex vision tasks such as object detection.",
    "full_text": "Toward Transformer-Based Object Detection\nJosh Beal* Eric Kim* Eric Tzeng Dong Huk Park Andrew Zhai Dmitry Kislyuk\nPinterest\n{jbeal, erickim, etzeng, dhukpark, andrew, dkislyuk}@pinterest.com\nAbstract\nTransformers have become the dominant model in natu-\nral language processing, owing to their ability to pretrain\non massive amounts of data, then transfer to smaller, more\nspeciﬁc tasks via ﬁne-tuning. The Vision Transformer was\nthe ﬁrst major attempt to apply a pure transformer model\ndirectly to images as input, demonstrating that as compared\nto convolutional networks, transformer-based architectures\ncan achieve competitive results on benchmark classiﬁcation\ntasks. However, the computational complexity of the atten-\ntion operator means that we are limited to low-resolution\ninputs. For more complex tasks such as detection or seg-\nmentation, maintaining a high input resolution is crucial\nto ensure that models can properly identify and reﬂect ﬁne\ndetails in their output. This naturally raises the question of\nwhether or not transformer-based architectures such as the\nVision Transformer are capable of performing tasks other\nthan classiﬁcation. In this paper, we determine that Vision\nTransformers can be used as a backbone by a common de-\ntection task head to produce competitive COCO results. The\nmodel that we propose, ViT-FRCNN, demonstrates several\nknown properties associated with transformers, including\nlarge pretraining capacity and fast ﬁne-tuning performance.\nWe also investigate improvements over a standard detection\nbackbone, including superior performance on out-of-domain\nimages, better performance on large objects, and a lessened\nreliance on non-maximum suppression. We view ViT-FRCNN\nas an important stepping stone toward a pure-transformer\nsolution of complex vision tasks such as object detection.\n1. Introduction\nThe Transformer [36] model has become the preferred\nsolution for a wide range of natural language processing\n(NLP) tasks, showing impressive progress in machine trans-\nlation [21], question answering [10], text classiﬁcation [30],\ndocument summarization [41], and more. Part of this success\ncomes from the Transformer’s ability to learn complex de-\n*Authors contributed equally.\npendencies between input sequences via self-attention, and\nits scalability that makes it possible to pretrain models of\nremarkable size on large datasets with no signs of saturating\nperformance [10, 28, 29, 4].\nThe Vision Transformer (ViT) [12] demonstrated for the\nﬁrst time that a transformer architecture can be directly ap-\nplied to images as well, by treating an image as a sequence\nof patches. Although its performance on mid-sized datasets\ntrails behind convolution-based models, the ViT seems to\nretain the capacity seen in NLP transformers, enabling it to\npretrain on an unprecedented amount of data. In effect, ViT\nsuggests that the standard convolution, which has been the\nhallmark of vision modeling for decades, may be supple-\nmented or replaced by attention-based components.\nFor convolutional networks, the locality of the convolu-\ntion operation means that feature maps at higher layers of\nthe network spatially reﬂect the input—for example, fea-\ntures generated by objects on the left of an input image tend\nto appear on the left of higher level feature maps as well.\nThis property makes it relatively straightforward to general-\nize convolutional classiﬁers into object detectors by simply\nfeeding high level convolutional maps into detection heads\nthat output object classes and locations. In contrast, trans-\nformers are capable of globally attending at every layer of\nthe network, potentially making the spatial correspondence\nbetween the input and intermediate features weaker. This\nnaturally raises the question of whether or not Vision Trans-\nformers can be ﬁne-tuned to perform tasks that are more\nlocally-sensitive, such as object detection or segmentation.\nWe propose a new model, ViT-FRCNN, that attempts to\nanswer this question by augmenting a ViT with detection-\nspeciﬁc task heads to detect and localize objects in images.\nImportantly, ViT-FRCNN demonstrates that a transformer-\nbased backbone can retain sufﬁcient spatial information for\nobject detection. We show that ViT-FRCNN achieves com-\npetitive results on the COCO detection challenge [23], while\nexhibiting many of the desirable properties of transformer-\nbased models. In particular, our experiments suggest that\nobject detection tasks beneﬁt from the massive pretraining\nparadigm commonly used with transformers. Our experi-\nments also ﬁnd improved detection performance on large\n1\narXiv:2012.09958v1  [cs.CV]  17 Dec 2020\nobjects (perhaps due to the ability of the architecture to at-\ntend globally), and fewer spurious overdetections of objects.\nWe believe ViT-FRCNN shows that the commonly ap-\nplied paradigm of large scale pretraining on massive datasets\nfollowed by rapid ﬁne-tuning to speciﬁc tasks can be scaled\nup even further in the ﬁeld of computer vision, owing to the\nmodel capacity observed in transformer-based architectures\nand the ﬂexible features learned in such backbones.\n2. Related work\nSelf-attention for object detection:DETR [5] is notable\nin that it is the ﬁrst approach to successfully utilize trans-\nformers for the object detection task. Speciﬁcally, DETR\nadded a transformer encoder and decoder on top of a standard\nCNN model (e.g., ResNet-50/101), and uses a set-matching\nloss function. A notable property in their approach is that it\ndoes not need to use non-maximum suppression (NMS) as a\npost-processing step, as their decoder architecture learns to\nself-suppress duplicate bounding box predictions.\nDeformable DETR [42] improves upon some of the limi-\ntations exhibited by DETR: 1) requiring much longer train-\ning epochs than typical detectors for convergence and 2)\nachieving relatively low detection performance on small ob-\njects. The shortcomings mainly stem from prohibitive com-\nplexities in processing high-resolution feature maps and De-\nformable DETR addresses this by introducing a deformable\nattention module that learns to attend to small set of sam-\npling locations in the feature map. RelationNet++ [7] lever-\nages multi-head attention in a different manner, proposing\na “Bridging Visual Representations” (BVR) module based\non a Transformer decoder for integrating information from\ndifferent object representations. The above works build on\nprior work by RelationNet [ 17] and Non-Local Networks\n(NL) [38] in using attention between bounding box features\nand pixel features, respectively, for object detection.\nBoth DETR and Deformable DETR rely on convolutional\nnetworks to encode visual features while the Transformer\nis utilized for decoding such features into detection outputs.\nIn this paper, we explore a different variant of Transformer-\nbased detector wherein the Transformer is used to encode\nvisual features instead and a traditional region proposal net-\nwork (RPN) [31] is employed to output detections.\nSelf-attention for visual representations:ViT [12] is the\nﬁrst pure Transformer-based visual model to perform com-\nparably to state-of-the-art convolutional networks on image\nrecognition tasks. In particular, ViT demonstrates excellent\nperformance when ﬁrst pretrained on a large scale dataset\nand then transferred to tasks with fewer datapoints. Despite\nits effectiveness for image recognition, it is yet to be shown\nthat the approach can be generalized to spatial tasks such as\nobject detection or image segmentation.\nImage GPT (iGPT) [6] is an unsupervised generative pre-\ntraining approach for learning strong visual representations.\nApplying a GPT-2 [29] based model directly to the image\npixels was shown to yield compelling image completions and\nsamples, showing that a fully Transformer-based architecture\nis possible for some visual tasks, despite the input image\nresolution being quite limited in their approach.\nOther works have explored the relationship between self-\nattention modules and convolutional layers [8] as well as the\nlimitations of convolutional neural networks [40, 16].\nPretraining and self-training: Several works have ex-\nplored the effectiveness of pretraining on large-scale image\ndatasets such as JFT-300M [34] and IG-940M [25] for vi-\nsual representation learning. In Big Transfer (BiT) [ 19],\nlarge-scale classiﬁcation-based pretraining was found to\nbe beneﬁcial for detection transfer performance. Other\nworks have found that smaller-scale classiﬁcation-based\npretraining, i.e., pretraining on ILSVRC-2012 [ 9], does\nnot necessarily beneﬁt detection performance relative to\ntraining from scratch when the detection dataset is sufﬁ-\nciently large [13, 44]. In this work, we focus on pretrain-\ning paradigms for object detection; semi-supervised learn-\ning [39] and self-training [33, 44] paradigms are also beneﬁ-\ncial in the context of leveraging large-scale unlabeled data.\n3. Method\nWe now describe our model, ViT-FRCNN, which aug-\nments a Vision Transformer backbone with a detection net-\nwork so that it can produce bounding box classiﬁcations and\ncoordinates. In doing so, we demonstrate that the ViT is\ncapable of transferring representations learned for classiﬁca-\ntions to other tasks such as object detection, paving the path\nfor a general class of transformer-based vision models.\nBecause the ViT is primarily concerned with classiﬁca-\ntion, it uses only the state corresponding to the input class\ntoken at the ﬁnal transformer layer as the ﬁnal feature to be\nfed into the classiﬁcation head. The remaining tokens in the\nsequence are used only as features for the ﬁnal class token to\nattend to. However, these unused outputs correspond to the\ninput patches, and in theory could encode local information\nuseful for performing object detection. We thus propose rein-\nterpreting the ﬁnal transformer states, excluding the class\ntoken, as a spatial feature map.\nThis feature map is then fed to a detection network mod-\neled after Faster R-CNN [31]. Like in Faster R-CNN, ﬁrst a\nregion proposal network (RPN) densely predicts the presence\nor absence of objects throughout the image. The features cor-\nresponding to the top region proposals are then RoI pooled\nand fed into a detection head, which classiﬁes each region\ninto object categories and regresses to precise bounding box\ncoordinates. The ViT-FRCNN architecture is depicted picto-\nrially in Figure 1.\n2\nDetection Network\nTransformer Encoder 1 2 3\n4 5 6\n8 97\nLinear Projection of Flattened Patches\nPosition \nEmbeddings\n2 3 4 5 6 7 8 90 * 1\n2 3 4 5 6 7 8 91\nClass \nHead\nBox \nHead\nDog [0.33, 0.09, \n0.64, 0.50] \nReinterpret transformer patch \noutputs as a feature map\nRPN\nRoI pool \nfeatureResidual \nblocks\nFigure 1: We repurpose a Vision Transformer backbone to perform object detection by making use of the per-patch outputs in\nthe ﬁnal transformer layer. By reinterpreting these outputs spatially, we create a feature map that naturally lends itself as the\ninput to a detection model that produces class predictions and regresses to box coordinates. The resulting model, ViT-FRCNN,\nachieves strong performance while exhibiting many of the desirable properties of transformer-based models.\nThe RPN identiﬁes regions of interest likely to contain\nobjects by producing multiple predictions per location on\nthe feature map: each prediction corresponds to a different\nanchor of varying size and aspect ratio, centered at the lo-\ncation of the feature. Each prediction consists of a binary\nclassiﬁcation (object vs. no object) and a regression to box\ncoordinates. The bounding boxes are predicted as offsets\nfrom anchor boxes, using the parameterization\ntx = (x −xa)/wa\nty = (y −ya)/ha\ntw = log(w/wa)\nth = log(h/ha)\nt∗\nx = (x∗ −xa)/wa\nt∗\ny = (y∗ −ya)/ha\nt∗\nw = log(w∗/wa)\nt∗\nh = log(h∗/ha),\nwhere x, y, w, and h denote the the box center, width, and\nheight, and x, xa, x∗ correspond to the prediction, anchor,\nand ground truth, respectively. The box training loss is\nsimply a Huber loss between the predicted offsets t and the\nground truth offsets t∗.\nAfter the RPN densely predicts regions likely to contain\nobjects, the top candidates are used to RoI-pool regions from\nthe feature map, generating one feature per region proposal.\nThese pooled features are then fed to a pair of lightweight\nheads to produce object category (or background) predictions\nand bounding box regressions, parameterized in the same\nway as before. ViT-FRCNN is fully trainable end-to-end,\nand we jointly train the RPN and detection heads in a single\nphase.\nOne ﬁnal consideration is that, unlike classiﬁcation, detec-\ntion relies heavily on ﬁne detail present in small areas of the\ninput image. Thus, we train at much higher resolution than\nViT in an attempt to maintain as much resolution as possible,\nsubject to the limitations of GPU memory. We also maintain\nthe aspect ratio of the input image, as is standard practice\nfor detection models. Both of these changes require special\nhandling of the position embedding in the ViT backbone,\nwhich expects square images of a ﬁxed size. ViT-FRCNN\nsimply bilinearly interpolates the position embeddings at\nruntime to match any given input size and shape.\n3.1. Implementation details\nWe chose to use many of the same hyperparameter set-\ntings for ViT-FRCNN as in the original Faster R-CNN. The\nRPN predicts 15 anchors at each location: we use 5 areas\n(322, 642, 1282, 2562, and 5122 pixels) and 3 aspect ratios\n(1:1, 1:2, and 2:1). The region proposals produced by the\nRPN undergo a round of non-maximum suppression (NMS)\nwith overlap threshold 0.7. During training, the top 2,000\nproposals are fed to the detector heads, and for inference\nwe use the top 1,000. Finally, at inference time we apply a\nﬁnal round of NMS with threshold 0.5 to produce our ﬁnal\ndetections.\nWe explored two ways of utilizing the encoder outputs:\nusing only the ﬁnal encoder outputs, or using all intermedi-\nate encoder outputs via concatenation. For both approaches,\nwe reduce the dimensionality of the spatial feature map to\n512 channels via a learned transformation. Finally, in prac-\ntice we found it helpful to add intermediate residual blocks\nbetween the encoder spatial feature map and the detection\nmodule. See Figure 2 for a visualization of the residual\nblock structure, which mimics those of ResNet with minor\nchanges.\n4. Experiments\nWe validate the effectiveness of ViT-FRCNN with quan-\ntitative results on the benchmark COCO 2017 detection\ndataset. We additionally provide a detailed set of analyses\nand ablations of our model, demonstrating the advantages\nof a Transformer-based detector for ease of training and\nout-of-distribution generalization.\nUnless otherwise noted, we use the torchvision [26]\n3\n3x3 conv\nNormalize\nActivation\n3x3 conv\nNormalize\nActivation\nFigure 2: The intermediate residual convolution blocks [14]\nthat we use to connect the encoder outputs to the detection\nmodule. In all experiments, we use batch normalization [18]\nand GeLU [15] activation function.\nModel LR Batch Size Image Size\nViT-B/32-FRCNN 0.032 256 720/1200\nViT-B/32-FRCNNstride=0.5 0.004 32 720/1200\nViT-B/16-FRCNN 0.004 32 672/1092\nViT-B/16*-FRCNN 0.004 32 672/1092\nTable 1: Hyperparameters for different ViT-FRCNN com-\nbinations. “Image Size” denotes the target image size and\nmaximum image size respectively. Decreasing the patch size\nfrom 32 ×32 to 16 ×16 considerably improves detection\nperformance, but signiﬁcantly increases GPU memory usage,\nresulting in the smaller batch sizes and learning rates.\nResNet and Faster R-CNN (FRCNN) implementations in all\nof our object detection experiments.\nBaseline As a baseline, we compare against the ResNet-\nFRCNN-FPN detection model, which is a Faster-RCNN\nmodel with Feature Pyramid Networks [22]. We trained our\nown ResNet50-FRCNN-FPN and ResNet101-FRCNN-FPN\nbaselines using torchvision’s detection library.\nTraining hyperparameters For all ViT-FRCNN experi-\nments, we use SGD with momentum 0.9 and weight decay\nset to 1e-4. Due to GPU memory limits, the learning rates,\nbatch sizes, and target/max image size depend on the ViT\nvariant, and are explicitly enumerated Table 1. During detec-\ntion ﬁne-tuning, we do not freeze any layers, and train the\nentire model end-to-end.\nFor all ResNet-based experiments, we used SGD with\nlearning rate 0.08, momentum 0.9, weight decay 1e-4, a total\nbatch size of 256, and a target/max image size of 800 and\n1333 respectively. Notably, during detection ﬁne-tuning we\nfreeze the ﬁrst two ResNet blocks (“stem”, “res2”).\nAll experiments use the same train schedule of 21 train\nepochs, with a learning-rate drop by 10×at epoch 17. We\nalways report the detection evaluation metrics using the ﬁnal\nmodel snapshot. We also used a learning rate linear warm up\nschedule for the ﬁrst 1,000 steps, where the initial learning\nrate is multiplied by 1e-4 and gradually increased to the\ntarget learning rate.\nDuring training, we randomly ﬂip the image horizontally\nwith 50% probability. Each image is rescaled to a target\nscale that does not exceed a maximum size.\nPretraining datasets In prior work, the Vision Trans-\nformer architecture was shown to perform well when pre-\ntrained on large-scale image datasets, including ImageNet-\n21k [9] and JFT-300M [34]. We investigate the impact of\nlarge-scale classiﬁcation-based pretraining on the detection\ntransfer performance. We pretrained the ViT backbone on\nImageNet-21k, a public dataset consisting of 14.2 million\nimages with 21k supervised labels, and Annotations-1.3B,\nan internal dataset consisting of 1.3 billion images with 18k\nweakly-supervised labels. The Annotations-1.3B dataset is\nover 4×larger in image count than JFT-300M, and achieves\ncompetitive ILSVRC-2012 transfer performance when used\nfor large-scale pretraining, as shown in the appendix. For\na full comparision of pretraining datasets, we additionally\npretrain on ILSVRC-2012, though in Vision Transformers,\nthis dataset was found to be insufﬁcient for pretraining due\nto the smaller size of the dataset.\n4.1. COCO detection\nWe present evaluation results on the COCO 2017 val-\nidation set in Table 2. All ViT-FRCNN variants achieve\nrespectable performance on the detection task, though we\nobserve a number of interesting trends. The ﬁrst is that the\n16×16 models outperform the 32×32 models by a sizable\namount, indicating the importance of preserving ﬁne detail\nfor the detection task. We provide further analysis of this\ntrend in Section 4.2.\nSecond, we note that across all settings, increasing the\nsize of the pretraining dataset leads to better performance\non the downstream object detection task. For instance, by\nchanging the ViT-B/16 pretraining dataset from ImageNet-\n21k to Annotations-1.3B, we obtain a +1.2 AP boost (36.6 to\n37.8 AP). Likewise, when changing the ViT-B/32 pretraining\ndataset from ImageNet-21k to Annotations-1.3B, we obtain\na +1.6 AP boost (29.3 to 30.9 AP).\nThese results contrast some results from ViT [12], where\nthe lightweight ViT-B/32 model displayed worse ILSVRC-\n2012 transfer performance when scaling from ImageNet-\n21k to JFT-300M (81.28% to 80.73% top-1 accuracy). We\nobserved a similar trend in the ILSVRC-2012 transfer per-\nformance when scaling ViT-B/32 from ImageNet-21k to\nAnnotations-1.3B (81.22% to 80.82% top-1 accuracy).\nThese results suggest that pretraining with images of mul-\ntiple objects in a scene may harm the classiﬁcation transfer\nperformance, yet still improve the detection transfer perfor-\n4\nmance. A similar observation was made in a comparison\nof COCO self-training performance on ILSVRC-2012 and\nOpen Images [44]. Our pretraining results indicate that the\nILSVRC-2012 transfer performance is not a fully reliable\nmeasure of the representation quality for object detection.\nWhile ViT-FRCNN may not achieve state-of-the-art re-\nsults on COCO, we believe this signiﬁes an important step\nforward for transformers in the ﬁeld of computer vision. Our\nresults show that transformers trained on classiﬁcation can\nsuccessfully be transferred to other related tasks such as ob-\nject detection, while retaining many of the beneﬁts unique to\ntransformers. In particular, we are especially excited about\nViT-FRCNN’s ability to translate representations learned on\nmassive classiﬁcation datasets into improved performance\non downstream detection tasks. This serves as an important\nﬁrst step toward a class of transformer-based models that,\nwhen pretrained on massive quantities of data, can be rapidly\nﬁne-tuned for speciﬁc vision tasks while requiring relatively\nlittle task-speciﬁc labeled data.\n4.2. Ablations\nBecause the properties of transformers applied to vision\ntasks are still relatively poorly understood, we conduct a\ncomprehensive suite of additional analyses and ablations,\ncovering a variety of parameters such as the input patch size,\nthe handling of intermediate encoder features, and the trans-\nformer’s observed ability to reduce spurious overdetections.\nEncoder spatial resolution. We found that the encoder’s\nspatial resolution played a critical role in object detection\nperformance. In Table 2, we consistently see that there\nis a signiﬁcant AP gain by moving from a ViT backbone\nwith 32×32 pixel patches to a backbone with 16×16 pixel\npatches, which doubles the spatial resolution of the encoder.\nFor instance, for the ViT backbone trained on Annotations-\n1.3B, we see an absolute AP gain of +6.9 AP by decreasing\nthe patch size from 32 ×32 to 16 ×16, achieving 37.8 AP\noverall. This AP gain is accentuated for small objects: 9.7\nto 17.8 AP. Increasing the resolution of spatial features is a\nwell-known method to achieve better object detection per-\nformance, (i.e. Feature Pyramid Networks [ 22]), and we\nobserve that this carries over into this model architecture as\nwell.\nWe also investigated whether increasing the spatial resolu-\ntion of a trained ViT backbone in a post-hoc manner by using\noverlapping patches can help with the object detection task.\nIn Table 2, the ViT-B/32-FRCNNstride=0.5 row is the same\nbackbone as ViT-B/32-FRCNN, but using patches with 50%\noverlap. This is implemented by converting the encoder’s\nﬁrst linear projection layer to an equivalent convolution, and\nsetting its stride to half of the patch size. We see that there is\na signiﬁcant AP boost, particularly for small objects: 29.3\nto 34.5 overall, and 9.0 to 15.6 for small objects, indicating\n0.50 0.70 0.90 0.95\nNMS overlap threshold\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40COCO val2017 AP IoU=0:50:0.95 area=all\nResNet50-FRCNN-FPN\nResNet101-FRCNN-FPN\nViT-B/32-FRCNN\nViT-B/32-FRCNN (stride=0.5)\nViT-B/16-FRCNN\nViT-B/16*-FRCNN\nFigure 3: Increasing the non-maximum suppression IoU\nthreshold (thereby decreasing how often suppression occurs)\nnegatively impacts ViT-FRCNN less than the ResNet base-\nlines. This indicates that our transformer-based architecture\nis better able to avoid producing spurious overdetections,\npossibly due to its ability to have intermediate features at-\ntend to each other.\nthat the input handling is an important factor in ensuring the\nsuccess of ViT-FRCNN.\nNumber of connecting residual blocks.In Table 3, we\ninvestigate the impact of the number of residual blocks con-\nnecting the encoder outputs to the detection module on de-\ntection performance. We ﬁnd that introducing the residual\nblocks leads to a signiﬁcant AP boost from 24.1 AP to 28.8\nAP when using 4 blocks, but there are diminishing returns\npast that number. Our interpretation is that the encoder\noutputs require some learned post-processing before their\npotential can be fully utilized for the detection task. Unless\notherwise noted, we always use 8 blocks for our experiments.\nIntermediate encoder outputs. In Table 4, we investigate\ndifferent ways of utilizing the intermediate encoder outputs\nwhen constructing the spatial feature map used by the de-\ntection module. There is a slight AP gain of 0.2 when con-\ncatenating all intermediate encoder outputs instead of using\nonly the ﬁnal encoder output. Unless otherwise noted, we\nalways concatenate all intermediate encoder outputs for our\nexperiments.\nOverdetections. In Figure 3, we investigate whether the\nViT-FRCNN architecture is able to avoid producing spurious\noverdetections of objects.\nTo investigate this, we evaluated detection models on\n5\nModel AP AP 50 AP75 APS APM APL\nResNet50-FRCNN-FPN 36.0 57.7 38.4 20.8 40.0 46.2\nResNet101-FRCNN-FPN 38.8 59.9 42.0 22.2 43.0 50.9\nViT-B/32†-FRCNN 24.8 42.3 25.0 7.3 26.3 41.1\nViT-B/32-FRCNN 29.3 48.9 30.1 9.0 31.8 48.8\nViT-B/32*-FRCNN 30.9 50.5 31.7 9.7 33.7 51.5\nViT-B/32-FRCNNstride=0.5 34.5 53.4 36.8 15.6 36.9 52.3\nViT-B/16-FRCNN 36.6 56.3 39.3 17.4 40.0 55.5\nViT-B/16*-FRCNN 37.8 57.4 40.1 17.8 41.4 57.3\nTable 2: Average Precision (AP) on the COCO val2017 set. “B” denotes the “ViT-Base” backbone. “*” denotes that the\nbackbone was pretrained on Annotations 1.3B prior to detection ﬁne-tuning. “†” denotes that the backbone was pretrained on\nILSVRC2012 prior to detection ﬁne-tuning. All other ViT backbones are pretrained on ImageNet-21k. Due to GPU memory\nlimits, we are unable to explore a stride=0.5 variant of the “ViT-B/16” architectures.\nModel Res Blocks AP AP 50 AP75 APS APM APL\nViT-B/32-FRCNN 0 24.1 45.3 23.0 6.5 24.9 40.7\nViT-B/32-FRCNN 4 28.8 49.0 28.8 9.4 30.9 47.4\nViT-B/32-FRCNN 8 29.3 48.9 30.1 9.0 31.8 48.8\nViT-B/32-FRCNN 16 29.3 48.8 29.8 9.1 31.6 48.7\nTable 3: We investigate the impact of the number of connecting residual blocks between the encoder backbone and the\ndetection module. Increasing the number of residual connections leads to a signiﬁcant AP gain, indicating that a transformer\npretrained on classiﬁcation alone is not enough to solve the detection task.\nCOCO val2017, and controlled how aggressively the NMS\npostprocessing will suppress overlapping predicted boxes.\nAll experiments use the same inference postprocessing pa-\nrameters: the number of post-NMS detected boxes per image\nis capped at 100, and predicted boxes with conﬁdence less\nthan 0.05 are discarded. As the overlap threshold is increased\nfrom 0.5 to 0.9, i.e. NMS is nearly disabled, we see that the\nAP of ResNet-based detection models steeply drops: -9.7\nand -8.1 AP for ResNet50 and ResNet101 respectively. How-\never, the ViT-B/16* detection model has a much smaller drop\nof -2.8 AP, which suggests that the transformer-based model\nis able to reduce the occurrences of spurious overdetections.\nIt is informative to study what happens at the extreme\nNMS threshold of 0.95, where NMS is effectively disabled.\nAt the default NMS threshold of 0.5, ResNet101-FRCNN-\nFPN outperforms ViT-B/16*-FRCNN by 1.0 AP. However,\nwhen the NMS threshold is increased to 0.95, ViT-B/16*-\nFRCNN signiﬁcantly outperforms ResNet101-FRCNN-FPN\nby 8.1 AP. This quantitatively suggests that the transformer-\nbased detectors produce signiﬁcantly fewer overdetections\nthan ResNet-based detectors.\nQualitatively, in Figure 4, we observe that ViT-FRCNN\ngenerally avoids predicting multiple spurious boxes that\ncover different regions of the same object.\nThe ViT encoder’s ability to globally attend to the entire\nimage is a possible explanation for this phenomena. We\nspeculate that because the transformer’s features incorporate\nboth local and global context via the attention mechanism,\nthe FRCNN heads are better able to classify spurious overde-\ntections as the “background” category than the ResNet-based\nbaselines. The underlying mechanism for this behavior war-\nrants further study.\nSelf-attention maps. We also investigate whether utiliz-\ning the encoder self-attention maps as additional features\nto the detection module helps detection performance. We\naccomplish this by concatenating the encoder self-attention\nmaps to the encoder spatial outputs. As there are multiple\nself-attention heads, we aggregate these attentions into a\nsingle attention map via averaging before concatenation.\nIn Table 5, we see that utilizing the self-attention maps\nin this way has limited beneﬁt for detection. We theorize\nthat, because the self-attention is already integrated into the\nencoder outputs via a dot product, the self-attention maps\nthemselves are redundant for the detection downstream task.\nResNet baselines. In our experiments, for simplicity we\ngenerally chose to use off-the-shelf detector settings when-\never possible, without the addition of any bells and whistles.\nThis also applies to our ResNet-based baseline detectors. In\nthe appendix, we compare our ResNet-based baselines to\n6\nModel Encoder Outputs AP AP 50 AP75 APS APM APL\nViT-B/32-FRCNN Final 28.4 48.1 28.6 8.0 30.6 47.5\nViT-B/32-FRCNN All 29.3 48.9 30.1 9.0 31.8 48.8\nViT-B/16*-FRCNN Final 37.6 57.1 39.6 17.2 41.1 56.8\nViT-B/16*-FRCNN All 37.8 57.4 40.1 17.8 41.4 57.3\nTable 4: We investigate different ways of utilizing the intermediate encoder features as input to the detection network. “Final”\nindicates that we use only outputs from the ﬁnal transformer layer, whereas “All” indicates that all intermediate transformer\nstates are concatenated. We observe a small beneﬁt to utilizing the intermediate features, with the effect being more pronounced\nfor the 32×32 model architectures.\nModel Res Blocks Concat Attn Maps AP AP 50 AP75 APS APM APL\nViT-B/32-FRCNN 4 28.8 49.0 28.8 9.4 30.9 47.4\nViT-B/32-FRCNN 4 ✓ 29.3 49.0 29.9 9.0 31.7 48.5\nViT-B/32-FRCNN 8 29.3 48.9 30.1 9.0 31.8 48.8\nViT-B/32-FRCNN 8 ✓ 28.8 48.8 29.3 9.0 31.1 47.3\nTable 5: We investigate whether there is any beneﬁt to concatenating the self-attention maps with the encoder outputs prior to\nthe detection modules. When concatenating attention maps, we ﬁrst summarize the attention maps by averaging attention\nmaps across all attention heads. Using attention maps in this manner seems to have a neutral effect on the detection task.\nother results from the literature, which adopt various tweaks\nsuch as additional data augmentation and longer train sched-\nules to improve performance.\n4.3. Curriculum pretraining\nWe provide a preliminary investigation of a curriculum\npretraining [2, 37] approach for object detection, where the\nclassiﬁcation-pretrained model is ﬁne-tuned on a larger de-\ntection dataset prior to training on COCO 2017.\nFor this investigation, we consider Open Images V6 [20],\na supervised dataset consisting of 1.7 million images, with\n15.8 million bounding boxes and 600 categories. Relative to\nCOCO 2017 train, which consists of 118k images and 860k\nbounding boxes, this dataset is an order of magnitude larger\nin terms of image and bounding box count.\nTo improve the computational efﬁciency of pretraining,\nwe adopted a simpliﬁed detection model architecture in-\nspired by DETR [ 5]. The CNN-based detector head is re-\nplaced by feedforward networks attached to the ﬁnal encoder\nlayer outputs, which directly output a set of box predictions.\nThis model is trained with the set-based global loss and uses\nﬁxed box offsets at each encoder output to accelerate conver-\ngence in the early phases of training. The appendix contains\nmore details of this model architecture.\nThe simpliﬁed Transformer model is pretrained for\n100 epochs on the Open Images V6 dataset, using the\nAdamW [24] optimizer with a base learning rate of 3e-4,\nweight decay of 0.1, and a total batch size of 4,096. The\nViT-B/32 backbone is ﬁrst pretrained on ImageNet-21k for\nthese curriculum pretraining experiments.\nAs seen in Table 6, the addition of the pretraining phase\non Open Images V6 yields a +1.1 AP improvement for the\nViT-B/32-FRCNN model, and a +0.4 AP improvement for\nthe ViT-B/32-FRCNN model with overlapping patches. This\nphase of pretraining is shown to be most beneﬁcial for im-\nproving the performance on small and medium objects.\n4.4. Real-world generalization\nObjectNet [1] is a large-scale test set that measures the\ngeneralization performance of vision models in real-world\nsettings by controlling for the biases of rotation, back-\nground, and viewpoint in the development of the dataset.\nObjectNet has proven to be useful for the evaluation of the\ngeneralization and robustness of image classiﬁcation mod-\nels [19, 11, 35]. A reanalysis of this dataset provided bound-\ning box annotations for a subset of the original dataset [3].\nWe construct a novel subset of ObjectNet, referred to\nas OBJECT NET-D, by selecting the COCO 2017 overlap-\nping categories with bounding box annotations that were\ncreated in the reanalysis of the dataset. The OBJECT NET-D\ndataset consists of 4,971 test images and 23 object categories.\nNearly all of the objects (99.9%) in this dataset are “large”\nper the object size deﬁnitions in COCO 2017. Therefore,\nwe do not focus on the “small” and “medium” object detec-\ntion performance in our analysis. Figure 5 visualizes a few\nindicative samples from the OBJECT NET-D dataset. This\ndataset enables a robust test of the detection models with\nrespect to domain shift. The models are evaluated on the\ncorresponding categories without any kind of ﬁne-tuning.\nAs seen in Table 7, ViT-FRCNN models signiﬁcantly\n7\nModel IN-21k OIDV6 AP AP 50 AP75 APS APM APL\nViT-B/32-FRCNN ✓ 29.3 48.9 30.1 9.0 31.8 48.8\nViT-B/32-FRCNN ✓ ✓ 30.4 50.8 31.2 10.2 33.4 49.7\nViT-B/32-FRCNNstride=0.5 ✓ 34.5 53.4 36.8 15.6 36.9 52.3\nViT-B/32-FRCNNstride=0.5 ✓ ✓ 34.9 54.6 37.2 16.5 38.1 52.3\nTable 6: We investigate whether pretraining the transformer backbone on Open Images V6 helps improve detection performance.\n“IN-21k” refers to pretraining on ImageNet-21k, whereas the combination of “IN-21k” and “OIDV6” refers to pretraining on\nImageNet-21k, followed by pretraining on Open Images V6. All models are then ﬁne-tuned on COCO train2017, and we\nreport results on val2017. Keeping in line with prior work on transformers, we ﬁnd that ViT-FRCNN consistently reaps the\nbeneﬁts of larger-scale pretraining.\nResNet50-FRCNN-FPN ViT-B/16*-FRCNN\nFigure 4: Comparison of object detection outputs for ResNet-\nbased and transformer-based models. Left is ResNet50-\nFRCNN-FPN, right is ViT-B/16*-FRCNN. The visualized\nboxes are after NMS is applied with a threshold of 0.5, and\nboxes with conﬁdence less than 0.05 are discarded. A trend\nis that ResNet-FRCNN models generate more spurious boxes\nfor an object (“overdetections”) than ViT-FRCNN models.\noutperform the ResNet-FRCNN baselines on this dataset.\nViT-B/16*-FRCNN achieves 22.9 AP (+7 AP boost) relative\nto the ResNet101-FRCNN-FPN baseline. Increasing the\npretraining dataset size results in clear improvements to the\nout-of-distribution generalization (+2.7 AP boost).\nThese results suggest that large-scale Transformer pre-\ntraining is a promising avenue to improve the performance\nof detection models in challenging real-world settings.\nFigure 5: Examples from the OBJECT NET-D dataset on\ncategories such as umbrellas and tennis rackets, highlighting\nthe controls for rotation, background, and viewpoint.\nModel AP AP 50 AP75 APL\nResNet50-FRCNN-FPN 13.9 30.8 10.2 14.1\nResNet101-FRCNN-FPN 15.9 34.1 12.9 16.1\nViT-B/32-FRCNN 16.4 34.5 14.0 16.5\nViT-B/32-FRCNNstride=0.5 17.9 36.4 15.8 18.0\nViT-B/16-FRCNN 20.2 40.5 17.8 20.4\nViT-B/16*-FRCNN 22.9 44.6 20.9 23.0\nTable 7: Average Precision (AP) on the OBJECT NET-D\ndataset. ViT-FRCNN yields better out-of-domain perfor-\nmance than ResNet-FRCNN approaches. Note that we omit\nthe small and medium object size metrics because there are\ntoo few examples, leading to uninformative, noisy metrics.\n5. Conclusion\nIn this work, we introduced ViT-FRCNN, a competitive\nobject detection solution which utilizes a transformer back-\nbone, suggesting that there are sufﬁciently different archi-\ntectures from the well-studied CNN backbone plausible to\nmake progress on complex vision tasks. Transformer-based\nmodels have demonstrated an ability to pretrain with mas-\nsive datasets without saturation, and ﬁne-tune to new tasks\n8\nquickly, both of which are properties we observe with ViT-\nFRCNN. We believe that ViT-FRCNN is but the ﬁrst of\nmany transformer-based architectures that will tackle the\nwide array of vision problems in the research community.\nAcknowledgements\nWe thank Koﬁ Boakye, Vahid Kazemi, and Chuck Rosen-\nberg for valuable discussions regarding the paper.\nReferences\n[1] Andrei Barbu, David Mayo, Julian Alverio, William Luo,\nChristopher Wang, Dan Gutfreund, Josh Tenenbaum, and\nBoris Katz. Objectnet: A large-scale bias-controlled dataset\nfor pushing the limits of object recognition models. In Ad-\nvances in Neural Information Processing Systems , pages\n9453–9463, 2019. 7\n[2] Yoshua Bengio, J ´erˆome Louradour, Ronan Collobert, and\nJason Weston. Curriculum learning. In Proceedings of the\n26th annual international conference on machine learning,\npages 41–48, 2009. 7\n[3] Ali Borji. Objectnet dataset: Reanalysis and correction. arXiv\npreprint arXiv:2004.02042, 2020. 7\n[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 1\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020. 2, 7, 11\n[6] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo\nJun, Prafulla Dhariwal, David Luan, and Ilya Sutskever. Gen-\nerative pretraining from pixels. In Proceedings of the 37th\nInternational Conference on Machine Learning , volume 1,\n2020. 2\n[7] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++: Bridg-\ning visual representations for object detection via transformer\ndecoder. In NeurIPS, 2020. 2\n[8] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi.\nOn the relationship between self-attention and convolutional\nlayers. arXiv preprint arXiv:1911.03584, 2019. 2\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248–255. Ieee, 2009. 2, 4\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 1\n[11] Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romi-\njnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver,\nMatthias Minderer, Alexander D’Amour, Dan Moldovan, et al.\nOn robustness and transferability of convolutional neural net-\nworks. arXiv preprint arXiv:2007.08558, 2020. 7\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 4\n[13] Kaiming He, Ross Girshick, and Piotr Doll´ar. Rethinking im-\nagenet pre-training. In Proceedings of the IEEE international\nconference on computer vision, pages 4918–4927, 2019. 2\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. CoRR,\nabs/1512.03385, 2015. 4\n[15] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities\nand stochastic regularizers with gaussian error linear units.\nCoRR, abs/1606.08415, 2016. 4\n[16] Katherine Hermann, Ting Chen, and Simon Kornblith. The\norigins and prevalence of texture bias in convolutional neu-\nral networks. Advances in Neural Information Processing\nSystems, 33, 2020. 2\n[17] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3588–3597, 2018. 2\n[18] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. CoRR, abs/1502.03167, 2015. 4\n[19] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6, 2019. 2, 7\n[20] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings,\nIvan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov,\nMatteo Malloci, Alexander Kolesnikov, Tom Duerig, and\nVittorio Ferrari. The open images dataset v4: Uniﬁed im-\nage classiﬁcation, object detection, and visual relationship\ndetection at scale. IJCV, 2020. 7\n[21] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-\njad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and\nLuke Zettlemoyer. Bart: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461, 2019. 1\n[22] Tsung-Yi Lin, Piotr Doll´ar, Ross B. Girshick, Kaiming He,\nBharath Hariharan, and Serge J. Belongie. Feature pyramid\nnetworks for object detection. CoRR, abs/1612.03144, 2016.\n4, 5\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740–755.\nSpringer, 2014. 1\n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2018. 7\n[25] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaim-\ning He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and\nLaurens van der Maaten. Exploring the limits of weakly super-\nvised pretraining. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 181–196, 2018. 2, 11\n9\n[26] S´ebastien Marcel and Yann Rodriguez. Torchvision the\nmachine-vision package of torch. In Proceedings of the 18th\nACM International Conference on Multimedia, MM ’10, page\n1485–1488, New York, NY , USA, 2010. Association for Com-\nputing Machinery. 3\n[27] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,\nand Jeff Dean. Distributed representations of words and\nphrases and their compositionality. In Advances in neural\ninformation processing systems, pages 3111–3119, 2013. 11\n[28] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding with unsuper-\nvised learning. Technical report, OpenAI, 2018. 1\n[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. OpenAI blog, 1(8):9, 2019. 1, 2\n[30] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learn-\ning with a uniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019. 1\n[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Advances in neural information pro-\ncessing systems, pages 91–99, 2015. 2\n[32] Seyed Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak,\nAmir Sadeghian, Ian D. Reid, and Silvio Savarese. Gen-\neralized intersection over union: A metric and A loss for\nbounding box regression. CoRR, abs/1902.09630, 2019. 11\n[33] Chuck Rosenberg, Martial Hebert, and Henry Schneiderman.\nSemi-supervised self-training of object detection models. 2\n[34] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In Proceedings of the IEEE international\nconference on computer vision, pages 843–852, 2017. 2, 4\n[35] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini,\nBenjamin Recht, and Ludwig Schmidt. Measuring robustness\nto natural distribution shifts in image classiﬁcation. Advances\nin Neural Information Processing Systems, 33, 2020. 7\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 1\n[37] Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, and Zhenglu\nYang. Curriculum pre-training for end-to-end speech transla-\ntion. arXiv preprint arXiv:2004.10093, 2020. 7\n[38] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming\nHe. Non-local neural networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n7794–7803, 2018. 2\n[39] I Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and\nDhruv Mahajan. Billion-scale semi-supervised learning for\nimage classiﬁcation. arXiv preprint arXiv:1905.00546, 2019.\n2\n[40] Richard Zhang. Making convolutional networks shift-\ninvariant again. In ICML, 2019. 2\n[41] Xingxing Zhang, Furu Wei, and Ming Zhou. Hibert:\nDocument level pre-training of hierarchical bidirectional\ntransformers for document summarization. arXiv preprint\narXiv:1905.06566, 2019. 1\n[42] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 2\n[43] George Kingsley Zipf. The psycho-biology of language: An\nintroduction to dynamic philology, volume 21. Psychology\nPress, 1999. 10\n[44] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanx-\niao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-\ntraining and self-training. Advances in Neural Information\nProcessing Systems, 33, 2020. 2, 5\nA. Appendix\nA.1. ResNet baselines\nFor completeness, in Table 8 we compare our ResNet-\nbased baselines to results from the literature.\nWe acknowledge that it is possible to substantially im-\nprove detection performance of our ResNet-based baselines\nby adopting the various tweaks in Table 8, such as additional\ndata augmentation and longer train schedules. However, we\nfeel that because our ViT-FRCNN approaches are trained\nwith the same train settings as our ResNet baselines, it would\nbe inappropriate to compare our ViT-FRCNN results to, say,\nResNet101-FRCNN-FPN+ which utilizes substantially more\ntweaks to achieve better performance.\nA.2. ImageNet performance\nModels pretrained on Annotations-1.3B achieve competi-\ntive performance when ﬁne-tuned on ILSVRC-2012. Table 9\ncontains the full details of ViT-B/32 and ViT-B/16 perfor-\nmance for different pretraining datasets.\nA.3. Curriculum pretraining\nThe simpliﬁed detection model architecture consists of\na Vision Transformer backbone with a box prediction MLP\nhead and class prediction MLP head attached to each en-\ncoder output. The conﬁguration of these heads and the set\nprediction loss function follow the DETR implementation.\nTo improve convergence in the early phases of training, a\nbox prediction offset is provided at each output, where each\nvalue is set to the corresponding patch’s center location.\nA.4. Object distributions\nIt is well-known that the distributions of word frequencies\nin natural language corpora approximately follow Zipf’s\nlaw [43]. We investigated whether visual objects follow a\nsimilar distribution on web scale data. For this analysis,\nwe consider the previously mentioned detection datasets—\nCOCO 2017 and Open Images V6 —as well as Object Index,\na semi-supervised internal dataset consisting of 248 million\nimages, with 646 million bounding boxes and 220 categories.\n10\nModel AP AP 50 AP75 APS APM APL\nResNet50-FRCNN-FPNours 36.0 57.7 38.4 20.8 40.0 46.2\nResNet50-FRCNN-FPND2 40.2 61.0 43.8 24.2 43.5 52.0\nResNet50-FRCNN-FPN+ 42.0 62.1 45.5 26.6 45.4 53.4\nResNet101-FRCNN-FPNours 38.8 59.9 42.0 22.2 43.0 50.9\nResNet101-FRCNN-FPND2 42.0 62.5 45.9 25.2 45.6 54.6\nResNet101-FRCNN-FPN+ 44.0 63.9 47.8 27.2 48.1 56.0\nTable 8: Average Precision (AP) on the COCO val2017 set. “D2” denotes the Detectron2 implementation, which has a slightly\ndifferent ResNet architecture, train-time scale augmentation, and a longer “3x” train schedule (37 vs 25 train epochs). “+”\ndenotes a heavily-tuned [5] implementation: GIoU [32], random crop train-time augmentation, and a longer “9x” training\nschedule (111 vs 25 train epochs).\nDataset ViT-B/32 ViT-B/16\nImageNet 73.38 77.91\nImageNet-21k 81.28 83.97\nJFT-300M 80.73 84.15\nAnnotations-1.3B 80.82 83.15\nTable 9: Top-1 Accuracy on ILSVRC-2012 (ImageNet) for\ndifferent pretraining datasets and ViT model architectures.\nFigure 6 demonstrates that the distribution of class fre-\nquencies for visual objects approximately follows Zipf’s law\nas the detection dataset size increases in scale. This connec-\ntion to language modeling may be worthy of further investi-\ngation as pretraining datasets continue to increase in scale\nand develop a longer-tailed distribution of categories. In par-\nticular, there are well-known procedures for handling class\nimbalance in training on natural language datasets [27, 25].\nA.5. ObjectNet-D categories\nWe provide a list of the 29 ObjectNet categories that\noverlap with ILSVRC-2012 and COCO 2017 categories:\nalarm clock, backpack, banana, beer bottle, bench, bicycle,\ncellphone, chair, computer mouse, drinking cup, hair dryer,\nkeyboard, laptop (open), microwave, mixing / salad bowl,\nmug, orange, pill bottle, pop can, remote control, soup bowl,\ntennis racket, tie, toaster, TV , umbrella, vase, water bottle,\nwine bottle.\nThe OBJECT NET-D categories consist only of the 23\ncorresponding COCO 2017 categories: backpack, banana,\nbench, bicycle, bottle, bowl, cell phone, chair, clock, cup,\nhair drier, keyboard, laptop, microwave, mouse, orange, re-\nmote, tennis racket, tie, toaster, TV , umbrella, vase.\n100 101 102\nCategory Rank\n101\n102\n103\n104\n105\n106\n107\n108\n109\nObject Count\nObject Index\nOpen Images V6\nCOCO 2017\nFigure 6: Category distribution of bounding box annotations\nfor object detection datasets of various sizes. The object\ncategory distributions converge to a Zipﬁan distribution as\nthe object detection datasets increase in scale.\n11"
}