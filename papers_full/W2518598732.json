{
  "title": "Using Term Position Similarity and Language Modeling for Bilingual Document Alignment",
  "url": "https://openalex.org/W2518598732",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A5044046333",
      "name": "Thanh Hoa Le",
      "affiliations": [
        null,
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A5037899273",
      "name": "Hoa Trong Vu",
      "affiliations": [
        null,
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A5076132510",
      "name": "Jonathan Oberländer",
      "affiliations": [
        null,
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A5077746237",
      "name": "Ondřej Bojar",
      "affiliations": [
        null,
        "Charles University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2152925220",
    "https://openalex.org/W4240913316",
    "https://openalex.org/W2136542423",
    "https://openalex.org/W2116042738",
    "https://openalex.org/W2148708890",
    "https://openalex.org/W157432847",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W1965555277",
    "https://openalex.org/W4393371996",
    "https://openalex.org/W1524068846",
    "https://openalex.org/W2057900969",
    "https://openalex.org/W4206765718",
    "https://openalex.org/W2085086335",
    "https://openalex.org/W2065565011",
    "https://openalex.org/W2102749417",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2093390569"
  ],
  "abstract": "The WMT Bilingual Document Alignment\\nTask requires systems to assign\\nsource pages to their “translations”, in a\\nbig space of possible pairs. We present\\nfour methods: The first one uses the term\\nposition similarity between candidate document\\npairs. The second method requires\\nautomatically translated versions of the\\ntarget text, and matches them with the candidates.\\nThe third and fourth methods try\\nto overcome some of the challenges presented\\nby the nature of the corpus, by\\nconsidering the string similarity of source\\nURL and candidate URL, and combining\\nthe first two approaches.",
  "full_text": "Proceedings of the First Conference on Machine Translation, V olume 2: Shared Task Papers, pages 710–716,\nBerlin, Germany, August 11-12, 2016.c⃝2016 Association for Computational Linguistics\nUsing Term Position Similarity and Language Modeling for\nBilingual Document Alignment\nThanh C. Le, Hoa Trong Vu, Jonathan Oberl¨ander, Ondˇrej Bojar\nCharles University in Prague\nFaculty of Mathematics and Physics\nInstitute of Formal and Applied Linguistics\n{thanhlct, hoavutrongvn, jonathan.oberlaender}@gmail.com\nbojar@ufal.mff.cuni.cz\nAbstract\nThe WMT Bilingual Document Align-\nment Task requires systems to assign\nsource pages to their “translations”, in a\nbig space of possible pairs. We present\nfour methods: The ﬁrst one uses the term\nposition similarity between candidate doc-\nument pairs. The second method requires\nautomatically translated versions of the\ntarget text, and matches them with the can-\ndidates. The third and fourth methods try\nto overcome some of the challenges pre-\nsented by the nature of the corpus, by\nconsidering the string similarity of source\nURL and candidate URL, and combining\nthe ﬁrst two approaches.\n1 Introduction\nParallel data play an essential role in training\nof statistical machine translation (MT) systems.\nWhile big collections have been already created,\ne.g. the corpus OPUS (Tiedemann, 2012), the\nWorld Wide Web remains a largely underex-\nploited source. That is the motivation for the\nshared task “Bilingual Document Alignment” of\nthe ACL 2016 workshop First Conference on Ma-\nchine Translation (WMT16) which requires par-\nticipants to align web page in one language to their\ntranslation counterparts in another language.\nGiven a large collection of documents, the ﬁrst\nstep in extracting parallel data is to organize the\ndocuments into heaps by the language they are\nwritten in. For two languages of interest, a brute-\nforce approach would consider all pairs of docu-\nments from the two heaps. Since the number of\npossible pairings is too high, it is necessary to em-\nploy some broad and fast heuristics to ﬁlter out the\nobviously wrong pairs.\nSome approaches to the task rely on document\nmetadata (e.g. the similarity of document URLs\nor language tags within URLs), some emphasize\nmore the actual content of the documents. Previ-\nous work (Rapp, 1999; Ma and Liberman, 1999)\nfocused on document alignment by counting word\nco-occurrences between source and target docu-\nments in a ﬁxed-size window. More recently,\nmethods from cross-lingual information retrieval\n(CLIR) have been used (Snover et al., 2008; Ab-\ndul Rauf and Schwenk, 2011), ranking lists of tar-\nget documents given a source document by a prob-\nabilistic model. Locality sensitive hashing has also\nbeen applied (Krstovski and Smith, 2011).\nIn this paper, we describe our attempt. The rest\nof the paper is organized as follows: In Section 2,\nwe describe the methods we used in our four sub-\nmitted systems. Section 3 describes our experi-\nmental setup and compares the results of the pro-\nposed methods. We conclude the paper and dis-\ncuss possible future improvements in Section 4.\n710\n2 Methods\nWe submitted four different systems: UFAL-\n1 uses term position similarity (especially rare\nterms) between documents. UFAL-2 uses lan-\nguage modeling on automatically translated docu-\nments to perform the matching. UFAL-3 reorders\nthe results of UFAL-2 to take into account the sim-\nilarity in the URL structure, and UFAL-4 com-\nbines UFAL-3 and UFAL-1 to further improve the\nresults.\n2.1 Term position similarity (UFAL-1)\nTwo similar languages such as English and French\ncan easily share a portion of their lexicons, es-\npecially proper names, some acronyms and num-\nbers are likely to keep their forms after transla-\ntion. If two documents are mutual translations,\nthe sequence of positions of those terms should\nbe correlated. Much past research (Ma and Liber-\nman, 1999; Rapp, 1999) has exploited these fea-\ntures, using a ﬁxed-size window and counting\nthe co-occurrences in this range. This method,\nhowever, requires considerable tuning of param-\neters, and if two shared terms are located out-\nside of the window, no credit will be added. In\nthis work, we consider similarity which not only\ntakes into account co-occurrences of terms but\nalso their positions. This metric also assumes that\nco-occurrences of rare terms are more important\nthan those of common terms. Experiments below\nshow that our method performs much better than\nthe ﬁxed-window method.\nOur term position similarity is deﬁned as fol-\nlows:\nρ(S,T) =\n∑\nt∈S∩T\nlog(1 +max(c)\nct\n) ·\n·\nNt∑\ni\nlS −|pi\nSt −pi\nTt |\nlS\n(1)\nHere S, T are the source and target docu-\nments, respectively, S ∩T is the set containing\nall terms which occurs in both documents, Nt =\nmin(|St|,|Tt|) where St,Tt is the number of oc-\ncurrences of term t in the respective document.\nThe length of the source document is denoted lS.\npi\nSt is the position of i-th occurrence of the term\ntin the source document and similarly for the tar-\nget document ( pi\nTt ). Finally, ct is the total num-\nber occurrences of tin the data set and max(c) is\nFigure 1: The noisy channel model for Bilingual\nDocument Alignment\nthe total number of occurrences of the most fre-\nquent term in all the source documents. In sum,\nlog(1 +max(c)\nct ) is a weight to promote the impor-\ntance of rare terms and the inner sum ∑Nt\ni mea-\nsures the relative displacement of the term win S\ncompared to T.\nTo increase the number of terms contributing to\nthe metric result, we employ a bilingual dictionary\nand translate all words from target document that\ndo not appear in the source into their most frequent\ntranslation.\nThe submission using this method is called\nUFAL-1.\n2.2 Language model-based approach\n(UFAL-2)\nIn contrast to the method in Section 2.1, the ap-\nproach labeled UFAL-2 relies on automatic trans-\nlation from one side to the other (either source-\nto-target or vice versa). With documents on both\nsides converted to one language, we then treat the\ntask as a noisy channel problem, similarly to many\nworks of information retrieval based on language\nmodelling techniques (Ponte and Croft, 1998; Zhai\nand Lafferty, 2001; Xu et al., 2001).\nSpeciﬁcally, we assume that the observed out-\nput is the source page S, damaged by noisy trans-\nfer of some target page T. Through decoding, we\nwant to ﬁnd the target page T that most likely lead\nto the observed output S. The process is visual-\nized in Figure 1. Therefore, like in the noisy chan-\nnel model (Brill and Moore, 2000), to decode the\ninput T, we estimate the probability ofT given the\noutput observation S, P(T|S). Following Bayes’\nrule, the problem is characterized by Equation 2:\nP(T|S) =P(S|T)P(T)\nP(S) (2)\n(At this stage, it is no longer important, thatT was\nthe automatic translation of a French page into En-\nglish and Swas the original English source page.)\n711\nAs our ﬁnal aim is to ﬁnd the bestT that causes the\noutput S, we can ignore the denominator P(S) in\nEquation 2, since it is the same for every value of\nT. So we have the problem equation as follows:\nTbest = arg max\nT\nP(S|T)  \ngenerative model\nprior\n  \nP(T) (3)\nSince estimating the generative model P(S|T) in\nEquation 3 is intractable, we assume conditional\nindependence of terms ti,tj ∈Sgiven T:\nP(S|T) =P(t1,...,t |S||T) ≈\n|S|∏\ni=1\nP(ti|T) (4)\nTo slightly speed up the computation in Equa-\ntion 4, we can group all occurrences of the same\nterm together as in Equation 5. To avoid an un-\nderﬂow problem, we move the computation to log\nspace, see Equation 6:\nP(S|T) ≈\n∏\ndistinct t∈S\nP(t|T)tfS (5)\nlog(P(S|T)) ≈\n∑\ndistinct t∈S\ntfS log(P(t|T)) (6)\nwhere tfS is the number of occurrences of the term\nt in S. The remaining problem is to estimate\nP(t|T). Fortunately, this can be achieved sim-\nply using maximum likelihood estimation (Scholz,\n1985) and it turns out to be the unigram language\nmodel (LM) as follows:\nP(t|T) =tfT\n|T| (7)\nwhere tfT is the number of occurrences of the\nterm t in T. In order to avoid zero probabil-\nities, a smoothing technique is necessary. We\nused Jelinek-Mercer smoothing (Jelinek, 1980).\nThe estimation at document level in Equation 7\nis smoothed with the estimation over the domain\nlevel, P(t|D), where Dis the set of all page trans-\nlations available for webdomain Dof page T. We\nadditionally use add-one smoothing for P(t|D) to\nmake sure the model handles well also terms never\nseen in the webdomain data.\nBack to prior in the problem equation (Equa-\ntion 3), it may be used to integrate very useful in-\nformation for each target French page. For exam-\nple, a French page that has been selected to be a\npair with another page should have a lower prior\nFigure 2: Performance of UFAL-2 on individual\nwebdomains in the training set\nfor the next prediction. The prior may also re-\nﬂect the difference in length of T and S, avoiding\nthe alignment of pages differing too much. Here,\nfor simplicity, we use uniform distribution as the\nprior. The ﬁnal equation ranking target French\npages T with respect to a given English source\ndocument Sis thus:\nTbest = arg max\nT\n∑\ndistinct t∈S\ntfS log(λP(t|T)\n+(1 −λ)P(t|D)) (8)\nwhere P(t|D) , as mentioned, is the probability of\nthe term toccurring in the webdomain Dand the\nparameter λof Jelinek-Mercer smoothing is set to\n0.5. We submit this method for evaluation under\nthe label UFAL-2.\n2.3 Optimizing for top-1 evaluation\n(UFAL-3)\nWe noticed that there were many cases where sev-\neral documents contained the same (or almost the\nsame) text, which therefore get scored (roughly)\nthe same by each of UFAL-1 and UFAL-2. This\nissue will create noise that can harm us in the eval-\nuation of the shared task, as can be seen in Fig-\nure 2: There is a signiﬁcant difference between\nthe top 1 and top 2 accuracy of our UFAL-2 sys-\ntem from Section 2.2, see e.g. the webdomains 5,\n7, 13 ( kusu.com), or 34 ( www.eu2007.de).\nWhile both the 1st best and the 2nd best top predic-\ntions could be assumed correct since the two pre-\ndicted pages are not distinguishable or only differ\nin unimportant details (e.g. Google Ads), the ofﬁ-\n712\ncial scoring will be based on a single-best answer.1\nA closer investigation reveals that the URLs that\nare marked correct in the training data are usually\nthe ones most similar to the source URL. We there-\nfore look at the top 10 candidates from the UFAL-\n2, and choose the candidate that is within some\nthreshold of the top result and closest in Leven-\nshtein distance from the source URL. The thresh-\nold value of 85 was obtained experimentally on\nthe training data. The result after this reﬁnement\nis submitted for the evaluation under the name\nUFAL-3.\n2.4 Combining UFAL-3 and UFAL-1 into\nUFAL-4\nWe now have the outputs of two methods, UFAL-\n1 and UFAL-3 (as a replacement of UFAL-2), and\nwe would like to combine them to one method.\nSince the result of UFAL-3 is very good (see Sec-\ntion 3.2), we decided to report UFAL-3 in most\ncases and resort to UFAL-1 only if we do not trust\nthe proposal of UFAL-3.\nTo estimate the certainty of UFAL-3 predic-\ntion, we use Kullback-Leibler divergence (Kull-\nback and Leibler, 1951) and measure how mis-\nmatching the predicted pair of documents is. To do\nso, we model the English source text and transla-\ntion of the predicted candidate as multinomial dis-\ntributions, and then compute the KL-divergence to\nsee what their distance is. In particular, a higher\nKL-score presents a bigger distance between the\npairs, in other words, they are less likely to be a\ncorrect pair.\nGiven the overall good performance of UFAL-\n3, there are not many negative examples to opti-\nmize the threshold for rejecting the predicted pair.\nWe solve the issue by artiﬁcially creating new neg-\native cases: we remove automatic translations of\ncorrect target French pages for two webdomains,\nrerun the predictions and then compute the KL-\ndivergence for all predicted pairs. The result of\n1624 pairs predicted is reported in Figure 3, in\nwhich the artiﬁcial negative examples are high-\nlighted with a blue line.\nBased on observations for the modiﬁed training\ndata, we set the threshold to 0.35. If the KL diver-\ngence for a pair of documents predicted by UFAL-\n3 exceeds this value, the pair is considered a wrong\nprediction. In that case, we use the method from\n1We were told by the organizers later that the test set does\nnot suffer from this problem of many very similar pages.\nFigure 3: KL-divergence for all 1624 predicted\npairs in the modiﬁed training set where two cor-\nrect translations are removed.\nSection 2.1 (UFAL-1) with the bilingual dictio-\nnary size of 5000 entries. Similar to the method\nfrom Section 2.3 (UFAL-3), we consider the top 2\ncandidates and choose the one with a lower Lev-\nenshtein distance. We call this combined method\nUFAL-4 in the evaluation.\n3 Experiments\n3.1 Experimental setup\nWe used the data published with the Shared Task\non Bilingual Document Alignment (WMT 2016),\ncontaining roughly 4200 million pairs, in which\n1624 pairs have been labeled as mutual transla-\ntions to serve as a development set.\nWork on information extraction typically uses\nprecision and recall of the extracted information\nas an evaluation measure. However, in this task,\nmanually classifying all possible pairs is impos-\nsible, so the true recall cannot be established.\nThe organizers thus decided to evaluate the meth-\nods on the recall within the ﬁxed set of document\npairs, the development set released prior submis-\nsion deadline and the ofﬁcial test set disclosed\nonly with the ﬁnal results.\nWhile the ofﬁcial scores are top-1 recall (i.e.\nthe recall taking the single-best prediction for each\ninput sentence), we also evaluate our systems at\ntop 2 and top 5 outputs because, as discussed in\nSection 2.3, the there are many documents with\nthe same content, but the development set of pairs\nmentions only one of them.\nAll documents are tokenized by splitting on\nwhite-space and passed to a ﬁlter which prunes all\npairs having a ratio of the lengths in tokens of two\n713\nSystems\nDictionary\nsize Baseline Fixed\nwindow\nTerm\nposition\n0 67.92 78.94 88.30\n1000 67.92 80.6 88.36\n5000 67.92 81.9 89.53 (UFAL-1)\n10000 67.92 85.71 91.63\n25000 67.92 88.73 94.27\n50000 67.92 90.76 96.06\nTable 1: Recall measures by baseline system, sys-\ntem using ﬁxed-size window method and system\nusing term position similarity\ndocuments bigger than 2. Afterwards, all docu-\nments are ranked by the discussed methods. The\nﬁrst 1, 2 or 5 ranked documents with score higher\nthan a threshold are reported.\nIn the ﬁrst experiment, we prepare three sys-\ntems for comparison. We use the provided base-\nline system in the mentioned shared task which\nsimply ﬁnds matching URLs by discarding lan-\nguage identiﬁers, such as en, fr. We also imple-\nment a ﬁxed-size window method as described in\nMa and Liberman (1999). We compare the ﬁxed-\nsize window method with our term position simi-\nlarity in 6 tests with increasing size of the under-\nlying bilingual dictionary. This dictionary is ob-\ntained by running IBM Model 2 implemented by\nDyer et al. (2013) on the translations of the data set\nprovided by the organizers. We extract the 50000\nmost frequent word alignments fr −en having\nP(en | fr) > 0.7 and then randomly draw a\nsubset of this dictionary for each test. The variant\nwith 5000 entries is our submission called UFAL-\n1. If two documents have an identical score, the\none having a shorter URL is preferred.\nIn the second experiment, we compare the\nterm position similarity method (UFAL-1) with\nthe language model-based approach (UFAL-2 and\nUFAL-3) and the combination method (UFAL-4).\nThe term position similarity method uses a bilin-\ngual dictionary containing 5000 entries. Auto-\nmatic translations for all target documents were\nprovided by the organizers who used a baseline\nMoses setup trained on Europarl and the News\nCommentary corpus.\n3.2 Experiment result\nThe results for ﬁrst experiment are in Table 1.\nFrom these results, we can clearly see that term\nMethod Recall\nTop 1 Top 2 Top 5\nBaseline 67.92\nUFAL-1 89.53\nUFAL-2 88.40 97.40 98.30\nUFAL-3 93.70\nUFAL-4 94.70\nTable 2: Result on the development set\nposition similarity outperforms the ﬁxed-size win-\ndow method and surpasses the baseline system\nwith around 20% even without a bilingual dic-\ntionary. By increasing size of the bilingual dic-\ntionary up to 50000 entries, we can boost up the\nterm position similarity method by 8% to96.06%.\nHowever, there are still a number of avenues\nfor improvement. First, as we found that our\nmethod encountered many errors on the webdo-\nmain www.luontoportti.com that contains\nextremely specialized words not covered by our\ndictionary, this makes a domain-based bilingual\ndictionary one of the most desirable potential im-\nprovements. Secondly, the term position similarity\nmethod is very sensitive to the case when a tar-\nget document contains source language text, be-\ncause it increases the co-occurrence rate between\ntwo documents. Any errors in language identiﬁ-\ncation can thus adversely affect the ﬁnal extracted\nparallel corpus.\nWe present the results of the second experiment\nin Table 2. The improved methods UFAL-3 and\nUFAL-4 show signiﬁcant gains, achieving 93.7%\nand 94.7% in recall. We also clearly see the re-\nmarkable changes in recall for the top match vs.\ntop two matches caused by the similar documents\nin the corpus, as discussed in Section 2.3.\nFinally, we report the ofﬁcial scores in Table 3.\nThe ofﬁcial test set consists of 2402 document\npairs and methods are evaluated in terms of the\npercentage of these pairs that they reported (“Re-\ncall”). The shared task winner NovaLincs-url-\ncoverage (denoted “NovaLics” in the table for\nshort) reached 94.96%, our best method UFAL-\n4 ranked about in the middle of the methods with\nthe recall of 84.22%. As we see in the remain-\ning columns, UFAL-4 produces by far the highest\nnumber of document pairs (more that 1M). The of-\nﬁcial scoring script ﬁlters this list and keeps only\nthe pairs where neither the source URL nor the tar-\nget URL was previously reported (“After 1-1”).\n714\nOfﬁcial Pairs Lenient\nMethod Recall [%] Reported After 1-1 Recall\nNovaLincs 94.96 235812 235812 ?\nUFAL-4 84.22 1080962 268105 92.67\nUFAL-1 81.31 592337 248344 87.89\nUFAL-3 80.68 574434 207358 89.97\nUFAL-2 79.14 574433 178038 88.43\nTable 3: The winner and our methods on the ofﬁ-\ncial test set.\nAfter this style of deduplication, the number of\npairs reduces to about 268k, slightly higher than\nthe number of pairs reported by the winner.\nThe ofﬁcial test set results are in line with our\nobservation on the development set: term posi-\ntion similarity (UFAL-1) performs well (although\nnot as well as on the development set) and the\ntwo variations of the noisy-channel approach are\nslightly worse, with UFAL-3 (URL similarity) bet-\nter than UFAL-2. The combination (UFAL-4) is\nthe best of our methods.\nWe note that for systems like ours that produce\nall URL pairs they deem good enough, the 1-1\ndeduplication may be too strict. We thus also re-\nport a lenient form of the recall: whenever a pair\nof URLs from the test set appears (as an unordered\npair) among the pairs produced by our method, we\ngive a credit for it. As seen in the last column\nof Table 3, the noisy-channel methods seem bet-\nter than term position similarity in this measure.\nConsidering that UFAL-2 and UFAL-3 produced\nslightly fewer pairs than UFAL-1, it may seem that\nthey are more precise. This however need not be\nthe case; the set of pairs produced by the systems\nis again too large for manual validation so the true\nprecision cannot be evaluated.\n4 Conclusion and future work\nIn this paper, we presented four systems for\nthe Bilingual Document Alignment shared task.\nThese system all perform well on the provided de-\nvelopment set (roughly 90% accuracy for top 1\nrecall) as well as on the ofﬁcial test set (above\n80%; about in the middle of all the participating\nmethods). One system, UFAL-1, uses term posi-\ntion similarity. The second system, UFAL-2, uses\na probabilistic model inspired by language mod-\nelling and the noisy channel model. Two others\nsystems, UFAL-3 and 4, are improvements of the\ntwo former ones, where UFAL-3 tries to overcome\nthe fact that content is repeated in a web-based cor-\npus and UFAL-4 is a more advanced combination\nof UFAL-3 and 1.\nSeveral reﬁnements of the proposed approaches\nare worth further investigation. In particular, a sys-\ntematic method of creating a bilingual dictionary\ndedicated for each speciﬁc webdomain should in-\ncrease the accuracy of the term position similar-\nity method. For the language model approach,\nit might be valuable to use a more comprehen-\nsive generative model (e.g. bi/tri-gram language\nmodel). Adding a prior might also enhance model\naccuracy. Another potential for the LM-based ap-\nproach is, instead of depending on translations of\ntarget pages, to apply a bilingual dictionary or a\ntranslation model directly for the generative pro-\ncess.\nThe method of UFAL-3 still misses some of the\nstraightforward cases of URL mapping. For in-\nstance, it might be advisable to use a more spe-\nciﬁc variant of edit distance variant, e.g. to pe-\nnalize changes in special characters like “/” or “?”\ncompared to normal word characters.\nBeyond our submissions to the shared task, we\nsuggest that more attention should be paid to the\nevaluation method. The problem of repeated or\nvery similar content on the web is omnipresent, so\nany attempt to handle it is likely to improve the re-\nliability of top-1 recall measurements, improving\nthe bilingual alignment task itself.\nAcknowledgments\nThis work has received funding from the European\nUnion’s Horizon 2020 research and innovation\nprogramme under grant agreement no. 644402\n(HimL).\nComputational resources were supplied by the\nMinistry of Education, Youth and Sports of the\nCzech Republic under the Projects CESNET\n(Project No. LM2015042) and CERIT-Scientiﬁc\nCloud (Project No. LM2015085) provided within\nthe program Projects of Large Research, Develop-\nment and Innovations Infrastructures.\nReferences\nSadaf Abdul Rauf and Holger Schwenk. 2011.\nParallel sentence generation from comparable\ncorpora for improved smt. Machine Translation\n25(4):341–375.\nEric Brill and Robert C Moore. 2000. An im-\nproved error model for noisy channel spelling\ncorrection. In Proceedings of the 38th An-\nnual Meeting on Association for Computational\n715\nLinguistics. Association for Computational Lin-\nguistics, pages 286–293.\nChris Dyer, Victor Chahuneau, and Noah A Smith.\n2013. A simple, fast, and effective reparameter-\nization of ibm model 2. Association for Com-\nputational Linguistics.\nFrederick Jelinek. 1980. Interpolated estimation\nof markov source parameters from sparse data.\nPattern recognition in practice.\nKriste Krstovski and David A. Smith. 2011. A\nminimally supervised approach for detecting\nand ranking document translation pairs. In\nProceedings of the Sixth Workshop on Statisti-\ncal Machine Translation. Association for Com-\nputational Linguistics, Stroudsburg, PA, USA,\nWMT ’11, pages 207–216.\nSolomon Kullback and Richard A Leibler. 1951.\nOn information and sufﬁciency. The annals of\nmathematical statistics 22(1):79–86.\nXiaoyi Ma and Mark Liberman. 1999. Bits: A\nmethod for bilingual text search over the web.\nIn Machine Translation Summit VII . Citeseer,\npages 538–542.\nJay M Ponte and W Bruce Croft. 1998. A language\nmodeling approach to information retrieval. In\nProceedings of the 21st annual international\nACM SIGIR conference on Research and de-\nvelopment in information retrieval. ACM, pages\n275–281.\nReinhard Rapp. 1999. Automatic identiﬁcation\nof word translations from unrelated english and\ngerman corpora. In Proceedings of the 37th\nannual meeting of the Association for Compu-\ntational Linguistics on Computational Linguis-\ntics. Association for Computational Linguistics,\npages 519–526.\nFW Scholz. 1985. Maximum likelihood estima-\ntion. Encyclopedia of Statistical Sciences .\nMatthew Snover, Bonnie Dorr, and Richard\nSchwartz. 2008. Language and translation\nmodel adaptation using comparable corpora. In\nProceedings of the Conference on Empirical\nMethods in Natural Language Processing . As-\nsociation for Computational Linguistics, pages\n857–866.\nJ¨org Tiedemann. 2012. Parallel Data, Tools\nand Interfaces in OPUS. In Nicoletta Cal-\nzolari (Conference Chair), Khalid Choukri,\nThierry Declerck, Mehmet Ugur Dogan, Bente\nMaegaard, Joseph Mariani, Jan Odijk, and\nStelios Piperidis, editors, Proceedings of the\nEight International Conference on Language\nResources and Evaluation (LREC’12) . Euro-\npean Language Resources Association (ELRA),\nIstanbul, Turkey.\nJinxi Xu, Ralph Weischedel, and Chanh Nguyen.\n2001. Evaluating a probabilistic model for\ncross-lingual information retrieval. In Proceed-\nings of the 24th annual international ACM SI-\nGIR conference on Research and development\nin information retrieval. ACM, pages 105–110.\nChengxiang Zhai and John Lafferty. 2001. A study\nof smoothing methods for language models ap-\nplied to ad hoc information retrieval. In Pro-\nceedings of the 24th annual international ACM\nSIGIR conference on Research and development\nin information retrieval. ACM, pages 334–342.\n716",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8274831771850586
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.7685075998306274
    },
    {
      "name": "Term (time)",
      "score": 0.7179300785064697
    },
    {
      "name": "String (physics)",
      "score": 0.6683312654495239
    },
    {
      "name": "Task (project management)",
      "score": 0.6269665360450745
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5803768634796143
    },
    {
      "name": "Natural language processing",
      "score": 0.5761374831199646
    },
    {
      "name": "Position (finance)",
      "score": 0.5593321323394775
    },
    {
      "name": "String metric",
      "score": 0.5074905753135681
    },
    {
      "name": "Information retrieval",
      "score": 0.48874831199645996
    },
    {
      "name": "Edit distance",
      "score": 0.4361405074596405
    },
    {
      "name": "Space (punctuation)",
      "score": 0.41439956426620483
    },
    {
      "name": "String searching algorithm",
      "score": 0.16217660903930664
    },
    {
      "name": "Pattern matching",
      "score": 0.08861187100410461
    },
    {
      "name": "Mathematics",
      "score": 0.06954064965248108
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Mathematical physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I21250087",
      "name": "Charles University",
      "country": "CZ"
    }
  ]
}