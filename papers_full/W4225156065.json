{
  "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model",
  "url": "https://openalex.org/W4225156065",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2105001540",
      "name": "Seong-Jin Shin",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2124155521",
      "name": "Sang-Woo Lee",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A3035023854",
      "name": "Hwijeen Ahn",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2166198374",
      "name": "Sung-Dong Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096832772",
      "name": "Hyoung-Seok Kim",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2615824825",
      "name": "Boseop Kim",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2115080942",
      "name": "Kyunghyun Cho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156010124",
      "name": "Gi-Chang Lee",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2548379891",
      "name": "Woomyoung Park",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2232041247",
      "name": "Jung-Woo Ha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2566712049",
      "name": "Nako Sung",
      "affiliations": [
        "Naver (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3162296828",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4320086632",
    "https://openalex.org/W4229506649",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4288106555",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3105069964",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4297399052",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W4226155321",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4286769130",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3212496002"
  ],
  "abstract": "Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-Woo Ha, Nako Sung. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5168 - 5186\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nOn the Effect of Pretraining Corpora on\nIn-context Learning by a Large-scale Language Model\nSeongjin Shin∗,1 Sang-Woo Lee∗,1,2 Hwijeen Ahn1 Sungdong Kim2\nHyoungSeok Kim1 Boseop Kim1 Kyunghyun Cho3 Gichang Lee1\nWoomyoung Park1 Jung-Woo Ha1,2 Nako Sung1\nNA VER CLOV A1 NA VER AI Lab2 NYU3\nAbstract\nMany recent studies on large-scale language\nmodels have reported successful in-context\nzero- and few-shot learning ability. However,\nthe in-depth analysis of when in-context learn-\ning occurs is still lacking. For example, it is\nunknown how in-context learning performance\nchanges as the training corpus varies. Here, we\ninvestigate the effects of the source and size\nof the pretraining corpus on in-context learn-\ning in HyperCLOV A, a Korean-centric GPT-\n3 model. From our in-depth investigation, we\nintroduce the following observations: (1) in-\ncontext learning performance heavily depends\non the corpus domain source, and the size of\nthe pretraining corpus does not necessarily de-\ntermine the emergence of in-context learning,\n(2) in-context learning ability can emerge when\na language model is trained on a combination\nof multiple corpora, even when each corpus\ndoes not result in in-context learning on its\nown, (3) pretraining with a corpus related to a\ndownstream task does not always guarantee the\ncompetitive in-context learning performance of\nthe downstream task, especially in the few-shot\nsetting, and (4) the relationship between lan-\nguage modeling (measured in perplexity) and\nin-context learning does not always correlate:\ne.g., low perplexity does not always imply high\nin-context few-shot learning performance.\n1 Introduction\nNLP community has been surprised by emergence\nof in-context learning ability of a large-scale lan-\nguage model (LM) such as GPT-3 (Brown et al.,\n2020) despite no duplication between downstream\ntask data and the pretraining corpus. Indeed, in-\ncontext learning uses a natural language descrip-\ntion and a few examples to prime a language model.\nThen the language model can predict the answer\nof a new example without updating the language\nmodel’s parameters. Since the release of GPT-3,\n∗Equal contribution.\nvarious large-scale in-context language models\nhave been proposed (Black et al., 2021; Kim et al.,\n2021; Zeng et al., 2021; Rae et al., 2021; Hoffmann\net al., 2022; Chowdhery et al., 2022).\nThere still remain many questions on language\nmodels’ in-context learning capability despite these\nsuccessful reports. For example, the relationship\nbetween the choice of a pretraining corpus and\ndownstream in-context learning task accuracy is un-\nknown. Previous studies argue pretraining with the\ncorpus similar to the downstream task improves the\ndownstream performance, but these observations\nare often limited to the case where a pretrained lan-\nguage model is finetuned for the downstream task\n(Gururangan et al., 2020; Lee et al., 2020; Micheli\net al., 2020).\nIn addition, analysis on the relation between the\nvalidation perplexity of a language model and in-\ncontext learning performance is still less investi-\ngated. Previous research on in-context learning im-\nplicitly assumes that perplexity is predictive of in-\ncontext learning performance by showing scaling\nlaw property of their model (Kaplan et al., 2020;\nBrown et al., 2020; Kim et al., 2021). Rae et al.\n(2021) also use perplexity for the hyperparameter\nselection on corpus reweighting in the pretraining\nof their in-context learner. However, their explicit\ncorrelations are less discovered.\nMotivated by this lack of in-depth analysis on\nthe relationship between in-context learning and\ncorpus properties, we vary the sources and sizes\nof pretraining corpora and analyze their impact on\nin-context learning, using HyperCLOV A, which is\na Korean-centric large LM. (Kim et al., 2021). We\nmainly discover in-context few-shot learning as in\nthe previous work (Kim et al., 2021) but also ex-\nplore in-context zero-shot learning. We use Hyper-\nCLOV A corpus, which is a large-scale pretraining\ncorpus mainly in Korean collected by Kim et al.\n(2021), as a base corpus from which we derive\npretraining corpora for our experiments.\n5168\nOur major findings include:\n• Corpus Source: In-context learning perfor-\nmance depends heavily on corpus sources,\nand with some sources, in-context learning\ndoes not work effectively. For example, the\nmodel trained only on a subcorpus of blog\n(Blog) achieves competitive in-context few-\nshot learning performance, but training on a\nsubcorpus of community website (Cafe) or\nonline news articles (News) hardly yields in-\ncontext few-shot learning ability.\n• Corpus Combination: In-context learning\nability can emerge by fusing two corpora, even\nwhen each on its own does not result in in-\ncontext learning. For example, while training\nonly on KiN corpus, which consists of QnA\nwebsites, or Ency corpus, which consists of\nEncyclopedia websites, in-context few-shot\nlearning ability was not observed, but train-\ning on both corpora makes the emergence of\nin-context few-shot learning.\n• Domain Relevance: Pretraining with a corpus\nrelated to a downstream task seems to help in-\ncontext zero-shot learning performance, but\nis not indicative of the competitive in-context\nfew-shot learning performance. For example,\ntraining on only News corpus makes a rela-\ntively good in-context zero-shot learning abil-\nity on a news-related downstream task, e.g.,\nnews topic classification based on its title,\nKLUE-YNAT (Park et al., 2021), but does\nnot yield in-context few-shot learning ability.\n• Perplexity: Although perplexity and in-\ncontext learning accuracies correlate well\nwhen training a single model, perplexity alone\ndoes not reflect the difference in in-context\nlearning accuracies across different language\nmodels. This is prominent particularly when\nthey were trained using different pretrain-\ning corpora. For example, Cafe model, the\nmodel trained with Cafe corpus, has the sec-\nond lowest validation perplexity on various\ndomain sources after Blog model, but fails\nto emerge in-context few-shot learning.\n2 Related Work\n2.1 In-context Learning\nBrown et al. (2020) demonstrate the concept of\nin-context learning, where a few training examples\nand/or task descriptions are provided together with\na new input for a large-scale LM to produce a tar-\nget of this input, without requiring any parameter\nupdate. A few training examples are used in the\nin-context few-shot learning setting, whereas no\ntraining example is used in the in-context zero-shot\nsetting. A few follow-up studies have tried to im-\nprove the in-context learning ability (Zhao et al.,\n2021; Holtzman et al., 2021). On the other hand,\nanother group of papers tries to explain the mech-\nanism of in-context few-shot learning (Min et al.,\n2022; Xie et al., 2022).\n2.2 Domain Relevance on Pretraining Corpus\nPrevious studies argue a better downstream accu-\nracy is observed with a pretraining corpus more\nsimilar to the downstream task corpus (Gururangan\net al., 2020; Lee et al., 2020; Micheli et al., 2020).\nHowever, these observations are limited to the case\nwhere a pretrained language model is finetuned for\nthe downstream task.\nThere are a few studies on the effects of different\ncorpus on the relationship between pretraining and\nin-context learning. A notable example is Codex,\nwhere GPT-3 is trained on Github corpus so that\nthe model can generate code from comments (Chen\net al., 2021a). However, the corpus used for Codex\nis limited to code comments and the corresponding\ncode. We study the effect of pretraining corpus\non in-context learning performance using various\ndomains.\n2.3 Quantity and Quality of Pretraining\nCorpus\nThere have been several studies on the quantity\nand quality of pretraining data. Raffel et al. (2020)\nconduct an ablation study on different pretraining\ncorpus on T5, and their filtered C4 corpus makes\nT5 perform better in downstream tasks. As with\nGPT-3, researchers generally improve the quality of\ntheir language model through data filtering (Brown\net al., 2020; Kim et al., 2021). Our research differs\nfrom the existing work in that we focus on in-depth\nanalysis of how the amount of data and the corpus\nsource affect in-context learning.\n2.4 Multi-task Learning\nMulti-task learning approaches, which explicitly\nfinetune on the in-context learning objective by\nusing numerous NLP tasks, are proposed recently\nto tackle zero/few-shot transfer to the unseen task\n5169\nName Description Tokens\nBlog Blog corpus 273.6B\nCafe Online community corpus 83.3B\nNews News corpus 73.8B\nComments Crawled comments corpus 41.1B\nKiN Korean QnA website corpus 27.3B\nModu Collection of five datasets 6.0B\nEncy Encyclopedia corpus 1.7B\nOthers Other corpus 55.0B\nTotal 561.8B\nTable 1: Descriptions of HyperCLOV A corpus (Kim\net al., 2021).\nat test time (Wei et al., 2021; Sanh et al., 2021;\nChen et al., 2021b; Min et al., 2021).\nUnlike the studies in a finetuning paradigm,\nmany properties of the in-context learning related\nto pretraining corpus are still unknown. As the pre-\nvious multi-task studies show that diverse tasks im-\nprove the ability of in-context learning, our study\nshows that diverse pretraining corpora strengthen\nthe ability of in-context learning.\n3 Task Definition\n3.1 Model\nWe use the variants of HyperCLOV A with various\nparameter sizes and pretraining corpus. We mainly\nexperiment with models with 1.3B parameters, but\nwe also include the result for 6.9B-sized models.\nAll models have a maximum sequence length of\n2,048.\nWe emphasize that all models use the same vo-\ncabulary across all our experiments. We use the\nmorpheme-aware byte-level BPE tokenizer trained\nwith HyperCLOV A corpus (Kim et al., 2021) for\nall models. We train multiple models with differ-\nent portions of HyperCLOV A corpus to investigate\nthe effects of the source and size of the corpus on\nin-context learning ability.\n3.2 Pretraining with Different Corpus\nWe analyze the effect of seven subcorpora in\nthe HyperCLOV A corpus: Blog, Cafe, News,\nComments, KiN, Modu, and Ency. Table 1\nsummarizes the characteristics of the subcorpora.\nBlog, Cafe, and News are taken from blog,\ncommunity sites, and online news articles of\nNA VER1, a Korean web portal service, respectively.\nComments is the comment threads related to the\nthree subcorpora mentioned above. KiN comes\n1https://www.naver.com/\nfrom NA VER’s online community QnA service\nsimilar to Quora. Ency is a collection of encyclo-\npedic texts including Korean Wikipedia.Modu con-\nsists of five public datasets constructed by National\nInstitute of the Korean Language2, including 3.2B\nof news, 2.1B of written language, 0.4B of spoken\nlanguage, 0.2B of web corpus, and 0.02B tokens of\nmessenger. Others was excluded to investigate\nthe explicit effects of domain corpus sources on\nin-context learning because Others is the corpus\nwhere various subcorpora are taken from multi-\nple heterogeneous sources. Tables 12 and 13 in\nAppendix show the examples of seven pretraining\ncorpus in Korean and English, respectively. ALL\ndenotes the original HyperCLOV A corpus includ-\ning Others.\nFor corpora with less than 150B tokens, we as-\nsign 99% of each corpus to the pretraining corpus\nand randomly extract 10,000 examples from the\nremaining 1% to the validation corpus for measur-\ning validation perplexity. For corpora with more\nthan 150B tokens, we make the training corpora\n150B tokens via random sampling and construct a\nvalidation set with 10,000 examples randomly sam-\npled from the remaining. As a result, the maximum\ntraining set size of each corpus is 150B tokens.\nThe validation set for each corpus consists of\n10,000 examples and is used for the early stop-\nping of models trained with each corpus. However,\nwe combine all validation set to make the entire\nvalidation set contains 70,000 examples for seven\ndomains, and the entire validation set is used for\ncalculating perplexity, as described in Section 3.5.\n3.3 Downstream Tasks\nWe evaluate in-context learning performance of\neach corpus-specific model on four Korean down-\nstream task datasets used in Kim et al. (2021):\nNSMC3, KorQuAD (Lim et al., 2019), AI Hub\ntranslation4, and YNAT (Park et al., 2021). NSMC\nis a binary sentiment classification dataset on movie\nreview. KorQuAD is a machine reading compre-\nhension dataset similar to SQuAD 1.0 (Rajpurkar\net al., 2016). AI Hub translation dataset consists\nof Korean-English parallel sentences from news,\ngovernment websites, legal documents, etc. YNAT\nis a topic classification problem with seven classes.\nWe think that three datasets for downstream\ntasks are closely related to the HyperCLOV A cor-\n2https://corpus.korean.go.kr/\n3https://github.com/e9t/nsmc\n4https://aihub.or.kr/aidata/87\n5170\nFigure 1: V ocabulary overlap ratio between pretraining\ncorpus and downstream task. Top 1,000 nouns are used\nto calculate the ratio. Nouns are extracted using our in-\nhouse part-of-speech tagger.\npus. Passages which construct KorQuAD are taken\nfrom Korean Wikipedia, which is also a part of\nthe Ency. YNAT is a topic classification task of\nnews headlines, so the downstream task is deeply\nrelated to the News corpus. A significant por-\ntion of parallel sentences for AI Hub translation\ndataset also comes from news articles. KiN cor-\npus is also related to the translation task. About\n2.5% of QnA data in KiN includes Korean ques-\ntions on the English language, as a foreign lan-\nguage. These question-style passages often include\nKorean-English sentence pairs in the passage. V o-\ncabulary overlap between downstream tasks and\nHyperCLOV A corpus is depicted in Figure 1.\n3.4 Experimental Details\nWe try our best to make the same hyperparameter\nof Kim et al. (2021), including global batch size,\ntraining step, maximum sequence length, learning\nrate, and so on. In our experiments, the models are\ntrained for 72K steps with a global batch size of\n1,024. We note that under this setting, the number\nof tokens that were actually used in pretraining\nis 150B. Therefore, we set the maximum size of\ntraining corpus to 150B as in Section 3.2.\nIn most experiments, validation perplexity de-\ncreases monotonically as training goes on. Thus,\nwe use the checkpoint at 72K step. The only ex-\nception is the Ency model. The Ency model has\na minimum validation loss at 12K steps, which is\nlikely to be caused by overfitting to pretraining data\ndue to a small size of the data. Therefore, we use\nearly-stopping checkpoints at the 12K steps for the\nreport.\nFor optimization, AdamW (Loshchilov and Hut-\nter, 2019) with the learning rate of 2.0e-4 and the\ncosine learning rate scheduling are used. We use\nthe mixed precision training. Models are trained on\nthe Nvidia Superpod which consists of 1,024 A100\nGPUs spread across 128 nodes. Using Superpod, it\nspends around 18 hours to train 1.3B model with\n72K steps.\nFor classification tasks such as NSMC and\nYNAT, we use a rank classification approach (Wei\net al., 2021), where we compare pre-defined out-\nputs (“positive” and “negative”) and take the one\nwith higher probability. KorQuAD and AI Hub are\nfree-form completion tasks, where we directly gen-\nerate output tokens using the greedy decoding.\nIn the few-shot experiments, the number of shots\nis set to 70, 4, 4, and 70 for NSMC, KorQuAD, AI\nHub, and YNAT, respectively. Downstream tasks\nare performed 12, 1, 3, and 6 times with differ-\nent random seeds for NSMC, KorQuAD, AI Hub,\nand YNAT, respectively. We report the average per-\nformance. Random seed influences the sampling\nof shots from training data and their order. The\nreason KorQuAD has only one random seed is de-\nscribed in Appendix D. Appendix D also includes\nthe examples of the few-shot prompts used in our\nexperiments. These all experimental settings in the\nfew-shot experiments, from the number of shots\nto the number of random trials, basically come\nfrom the experimental setting of Kim et al. (2021).\nHowever, we change the number of trials of YNAT\nfrom 3 to 6, because we found that the standard\nderivation of YNAT is relatively high.\n3.5 Measuring Validation Perplexity\nWe report validation perplexity in various tables\nand figures to verify our argument. We use the\nterm “PPL” to denote validation perplexities on the\nvalidation set. The validation set consists of 70,000\nexamples from seven corpus sources, as described\nin Section 3.2. We emphasize that, for calculating\nPPL, all experiments use the same vocabulary\nand validation set.\nIn Tables 2 and 4, we use Italic font for the\nresults from a multi-domain model, which is pre-\ntrained with two or more mixed corpora. Since a\nmulti-domain model trains more domains than a\nsingle-domain model, the PPLs of multi-domain\nmodels are generally lower than those of single-\ndomain models. To keep readers from directly com-\nparing PPLs between a single-domain and a multi-\n5171\ndomain model, we use italic font for the results of\na multi-domain model.\n4 Experimental Results\nWe perform intensive experiments to answer these\nfour main questions:\n1. How large do the source and the size of pre-\ntraining corpora have the effects on emerging\nin-context learning ability? (Section 4.2 and\n4.3)\n2. What is the effect of combining various cor-\npora? (Section 4.4)\n3. How large does domain relevance of corpus\ninfluence on model performances of the down-\nstream task? (Section 4.5)\n4. How strong is the correlation between vali-\ndation perplexity and in-context learning of\nlanguage models? (Section 4.6)\n4.1 Main Results\nTables 2 and 4 show the in-context few-shot results\non various pretraining corpus sources and different\ncorpus combination, respectively. Tables 3 and 5\ndepict the in-context zero-shot results of some mod-\nels in Tables 2 and 4, respectively. All results in\nTables 2, 3, 4, and 5 come from models with 1.3B\nparameters. Tables 8 and 9 in Appendix A present\nthe standard derivation values on the results of Ta-\nbles 2 and 4.\nIn Tables 2, 3, 4, and 5,Purple-underline denotes\nthe score is below the mean performance value of\nALL and Majority baseline in Table 2, and Teal-\nbold denotes the score is above. We use this mean\nvalue of Majority and ALL in Table 2 as the per-\nformance basis to prevent the in-context learning\nperformance of each model from being distorted\nby the high basis performance of two classification\ntasks such as NSMC and YNAT.\nTables 2 and 6 include in-context few-shot re-\nsults on various pretraining corpus sizes. In Table\n6, for example, 56B and 6B correspond to the 1/10\nand 1/100 of the original HyperCLOV A corpus\nwith 560B tokens, respectively. The 56B tokens\nand 6B tokens models are trained with around 3\nand 25 epochs, respectively, so that both models\ncan be trained with 72K training steps. On the other\nhand, Table 2 compares 27B tokens models trained\nwith different corpus sources to show the results in\ncontrolled corpus size.\n4.2 Effect of Corpus Source\nIt is noticeable that in-context learning ability\nemerges differently depending on pretraining cor-\npus sources, as shown in Tables 2, 3, 4, and 5.\nFor example, Blog model makes competitive in-\ncontext few-shot learning performance to ALL\nmodel, while each of Cafe and News models\nhardly shows in-context few-shot learning ability\nfrom Table 2. It is also noticeable thatModu model\nperforms better than Cafe and News model al-\nthough the size of Modu corpus is less than 1/10\nof Cafe or News corpus, showing the corpus size\nis not the only factor to predict in-context learning\nperformance. Likewise, it is also interesting that\nCafe+News model also shows poor performance\ndespite the same size to Blog and ALL, as shown\nin Table 4.\nThese differences in in-context learning are dra-\nmatic compared to the finetuning results we expect\nin general. For a comparative experiment between\nin-context learning and finetuning in our setting,\nwe also finetuned the experimented models with\nLoRA (Hu et al., 2021). As Table 11 in Appendix\nC shows, the performance differences in finetun-\ning are much smaller than in the case of in-context\nlearning.\n4.3 Effect of Corpus Size\nTable 6 shows that reducing the corpus size\nfrom 150B to 56B does not decrease the per-\nformance severely despite training with 1/10 of\ncorpus. However, the performance degradation\nof 6B tokens model is remarkable comparing to\nALL model. Nevertheless, it is noticeable that\n6B tokens model still performs much better than\nCafe+News model, which trains 150B tokens of\nCafe and News corpus.\nWe can also see the similar results for three Blog\nmodels of different sizes in Table 2. Blog and\nBlog 54B achieve similar performance. However,\nlike in ALL 6B, Blog 27B performs quite worse\nthan Blog 54B.\nFigure 3 shows the comparison between 1.3B-\nsized model and 6.9B-sized model. In the 6.9B-\nsized models, the in-context few-shot performance\nwith 56B tokens does not decrease significantly\ncompared to 150B tokens, as in the 1.3B-sized\nmodels.\n5172\nModel Corpus PPL NSMC KorQuAD AI Hub (BLEU) YNAT\nTrain (Acc) (EM) (F1) Ko →En En →Ko (F1)\nMajority - - 50.35 0.0 0.0 0.0 0.0 8.26\nALL 150B 119.99 84.59 56.17 73.47 6.15 23.36 59.57\nALL w/o Others 150B 119.66 84.59 56.49 74.20 6.14 23.21 50.76\nBlog 150B 152.40 83.50 50.74 69.34 3.82 20.11 60.68\nCafe 82.5B 170.85 57.77 3.12 14.26 2.83 16.53 11.04\nNews 73.1B 234.78 50.72 0.14 9.96 1.10 15.88 14.36\nComments 40.7B 225.39 79.78 14.69 33.33 0.79 5.06 36.17\nKiN 27.0B 187.80 54.73 4.85 18.99 6.81 18.16 9.23\nModu 5.9B 226.01 69.91 30.20 49.29 1.21 6.13 43.27\nEncy 1.7B 549.40 53.81 0.71 11.88 0.58 0.69 27.99\nBlog 54B 54.0B 155.69 83.06 49.13 68.10 3.93 21.12 57.97\nBlog 27B 27.0B 165.60 80.27 10.91 23.41 5.35 12.32 48.19\nCafe 27B 27.0B 169.81 49.91 1.37 13.98 4.25 20.74 8.60\nNews 27B 27.0B 239.79 50.64 0.80 8.02 2.42 15.78 27.20\nComments 27B 27.0B 229.65 80.50 13.02 31.53 1.70 3.28 25.79\nTable 2: In-context few-shot learning performance with different pretraining corpus. Models with 1.3B parameters\nare used. Majority means classifying each label with the primary class, and its score is 0 for KorQuAD and AI\nHub. Purple-underline denotes the score is below the mean performance value of ALL and Majority baseline, and\nTeal-bold denotes the score is above.\nModel Corpus PPL NSMC KorQuAD AI Hub (BLEU) YNAT\nTrain (Acc) (EM) (F1) Ko →En En →Ko (F1)\nMajority - - 50.35 0.0 0.0 0.0 0.0 8.26\nALL 150B 119.99 61.68 56.17 73.47 7.43 24.81 42.79\nBlog 150B 152.40 75.28 50.74 69.34 5.44 22.88 49.34\nCafe 82.5B 170.85 69.38 3.12 14.26 4.34 16.44 38.12\nNews 73.1B 234.78 54.96 0.14 9.96 1.28 10.21 48.03\nComments 40.7B 225.39 57.69 14.69 33.33 1.98 3.94 32.48\nKiN 27.0B 187.80 65.43 4.85 18.99 4.64 10.42 36.06\nModu 5.9B 226.01 72.50 30.22 49.30 2.39 7.55 35.28\nEncy 1.7B 549.40 42.96 14.01 31.51 0.80 0.77 30.22\nTable 3: In-context zero-shot performance with different pretraining corpus.\n4.4 Effect of Combining Corpora\nOne of our main goals is to investigate the ef-\nfects of combining multiple corpora from various\nsources on in-context learning performance. Table\n4 shows that in-context few-shot learning ability\ncan be emerged by combining two corpora, even\nif each of both corpora cannot provide in-context\nfew-shot learning ability. For example,KiN+Ency\nmodel succeeds to make in-context learning ability\nin most tasks, while each of KiN and Ency fails\nin most tasks. Likewise, Cafe+KiN model suc-\nceeds to make in-context few-shot learning ability,\nwhile each of Cafe and KiN fails in most tasks. In-\ncontext zero-shot abilities of these models follow\nsimilar patterns as shown in Table 5.\nThis phenomenon is related to the argument that\nin-context learning emerges by multi-task learning.\nAccording to the argument, as the language model-\ning objective function requires a language model\nto learn variety of next word prediction tasks, the\ngeneralization pushes in-context learning ability on\nunseen tasks. In the example of KiN+Ency model,\nKiN+Ency may learn in-context learning ability\nof MRC task, by learning next word prediction\ntasks of both Ency (Wikipedia) and KiN (QnA).\nUnlike these positive cases, we observe that com-\nbining corpora does not assure the emergence of\ncompetitive in-context learning. For example, from\nthe case of Cafe+News in Table 4, even if the\nmixed corpus model shows slightly better perfor-\nmance on KorQuAD than each of two corpora, its\nin-context few-shot performances on NSMC, Ko-\nrQuAD, and YNAT are still below the basis. Fur-\nthermore, the performances on NSMC and YNAT\neven decrease.\n5173\nCorpus Type Corpus PPL NSMC KorQuAD AI Hub (BLEU) YNAT\nTrain (Acc) (EM) (F1) Ko →En En →Ko (F1)\nALL 150B 119.99 84.59 56.17 73.47 6.15 23.36 59.57\nThe Case where In-context few-shot learning Emerges by Combining Two Poor Corpora\nKiN+Ency 28.7B 164.69 59.17 42.09 61.00 8.99 23.12 42.84\nCafe+KiN 109.5B 141.92 76.42 38.45 59.00 8.41 23.41 56.96\nThe Case where In-context few-shot learning Does Not Emerge by Combining Two Poor Corpora\nCafe+News 150B 154.20 54.15 8.95 22.72 4.45 17.77 8.19\nThe Case of Combining In-context few-shot Emerging Corpora\nBlog+Comments+Modu 150B 144.67 82.82 54.94 72.27 4.09 21.17 65.01\nThe Case of Adding News into KiN+Ency to Try to Enhance the Performance of YNAT\nNews+KiN+Ency 101.8B 142.13 75.96 35.42 55.60 8.70 23.38 27.54\nTable 4: In-context few-shot learning performance with different corpus combination.\nCorpus Type Corpus PPL NSMC KorQuAD AI Hub (BLEU) YNAT\nTrain (Acc) (EM) (F1) Ko →En En →Ko (F1)\nALL 150B 119.99 61.88 56.17 73.47 7.43 24.81 42.79\nKiN+Ency 28.7B 164.69 56.78 42.09 61.00 11.51 24.93 37.71\nCafe+KiN 109.5B 141.92 59.27 38.45 59.00 10.12 24.95 45.44\nCafe+News 150B 154.20 66.92 8.95 22.85 3.49 15.77 47.34\nBlog+Comments+Modu 150B 144.67 69.15 54.94 72.27 6.06 22.03 48.25\nNews+KiN+Ency 101.8B 142.13 61.49 35.42 55.60 10.18 24.13 51.89\nTable 5: In-context zero-shot learning performance with different corpus combination.\n# of NSMC KorQuAD AI Hub (BLEU) YNAT\ntokens (Acc) (EM) Ko →En En →Ko (F1)\n150B 84.59 56.17 6.15 23.36 59.57\n56B 84.35 55.13 5.47 22.98 51.89\n6B 74.70 36.72 3.97 17.81 30.24\nTable 6: In-context few-shot learning performance of\nALL with different size of the pretraining data. The\ndataset is randomly sampled from the original corpus.\n4.5 Effect of Domain Relevance\nSpeaking of the few-shot results, Table 2 shows that\nthe close relationship between a pretraining corpus\nand a downstream task does not always guarantee\nin-context few-shot learning ability on the down-\nstream task. KiN and Ency do not perform well\non KorQuAD task, although KorQuAD is an MRC\ntask from Korean Wikipedia, Ency includes Ko-\nrean Wikipedia, and KiN consists of question an-\nswering pair, respectively. Likewise, News does\nnot perform well on YNAT task, although YNAT\nconsists of news headline queries. Table 4 further\nshows that News+KiN+Ency model shows more\ndegenerated F1 score on YNAT thanKiN+Ency,\neven though a large amount of News corpus is\nadded to News+KiN+Ency model.\nFor further investigation, we analyze vocabu-\nlary statistics of each corpus. Figure 1 shows the\nvocabulary overlapping ratio between pretraining\ncorpora and downstream tasks. The result shows\nthat high vocabulary overlap between a pretraining\ncorpus and a downstream task does not indicate\nhigh downstream task performance. Although the\nModu corpus has a large vocabulary overlapping ra-\ntio to AI Hub, in-context learning performances of\nthe Modu model on the translation tasks are much\nlower than Blog and KiN.\nThe counter example of above supports is AI\nHub task performance of KiN model. KiN model\nlearned the pattern of Korean-English sentence\npairs, since the corpus includes a lot of Korean\nquestions on English language. While KiN model\ndoes not work well in other downstream tasks, the\nperformance on AI Hub translation is competitive\nand makes the best performance in Ko→En among\nseven pretraining corpora.\nIn the zero-shot setting, on the other hand, do-\n5174\nFigure 2: In-context few-shot learning performance of\nvarious corpus models and their PPL. A score of the\nmodel is divided by that of ALL to calculate the nor-\nmalized performance. Blue and red lines denote the\nperformance of ALL model and majority baseline, and\nthe purple line represents the average of both defined in\nthe caption of Table 4.\nmain relevance seems to affect more positively. For\nexample, training theNews corpus helps in-context\nzero-shot learning in KLUE-YNAT consistently.\nAs shown in Tables 3 and 5, the models whose train-\ning corpus includes the News corpus (i.e., News,\nCafe+News, and News+KiN+Ency) even per-\nform better than the model trained whole Hyper-\nCLOV A corpus.\nIn the case of KiN and AI Hub, zero-shot perfor-\nmance increase for AI Hub tasks of the KiN model\nis less significant than few-shot. However, adding\nKiN corpus into the pretraining corpus in the exper-\niments of Table 5 (i.e., KiN+Ency, Cafe+KiN,\nand News+KiN+Ency) makes a consistent perfor-\nmance increase, and the model outperform ALL.\n4.6 Perplexity and Downstream Task\nFigure 2 presents the scatter plots of PPL (x-axis)\nand in-context few-shot learning performance (y-\nFigure 3: Comparison on two model sizes and two cor-\npus sizes of the original HyperCLOV A corpus such\nas 1.3B-sized model and 6.9B-sized model, and 56B\nand 150B tokens. Few-shot results are reported. Green\nrectangle denotes 1.3B-sized model and blue rectangle\ndenotes 6.9B-sized model.\naxis) on five downstream tasks for single corpus\nmodels and the ALL model. In Figure 2, we nor-\nmalized in-context few-shot learning performance\nby dividing ALL model performance for calibrat-\ning various task metrics. Because we observe less\nexplicit tendency of correlation between validation\nperplexity and in-context performance, we argue\nthat it is difficult to hypothesize better perplexity\nassures emerging of in-context few-shot learning\nability.\nAccording to Table 2, Blog model shows both\nthe lowest PPL and the best in-context learning per-\nformance, and Ency model shows both the high-\nest PPL and the worst in-context learning perfor-\nmance. On the contrary, while Cafe model and\nKiN model shows the second and third lowest\nPPL, in-context few-shot learning ability was not\nobserved. These results show that the perplexity\ndoes not serve as a strong predictor of in-context\nfew-shot learning performance in comparing mod-\nels trained using different corpora. Table 2 also\nshows that the corpus size affects in-context few-\nshot learning performance more than PPL. Blog\n27B performs notably worse than Blog, but PPL\nrelatively does not decrease as much.\nSpeaking of zero-shot results, it seems Table\n3 shows that in-context zero-shot learning perfor-\nmances relatively more correlate with perplexity\nthan the few-shot cases. Nevertheless, Modu still\nhas both relatively high perplexity and relatively\nhigh in-context zero-shot learning performances.\nTable 7 shows validation perplexity scores for\n5175\nBlog Cafe News Comments KiN Modu Ency All\nBlog 126.89 201.15 83.98 599.68 138.95 98.83 108.17 152.40\nCafe 168.03 135.37 107.78 596.27 163.94 124.71 142.15 170.85\nNews 281.33 432.90 60.21 1543.03 253.73 87.65 156.28 234.78\nComments 228.37 242.59 176.28 390.30 164.88 196.40 239.40 225.39\nKiN 232.13 278.89 150.78 689.89 50.06 172.78 141.45 187.80\nModu 267.59 411.35 84.36 1086.04 243.19 69.48 136.02 226.01\nEncy 841.53 1348.38 213.87 5889.79 543.69 266.11 73.06 549.40\nTable 7: Validation perplexity scores per each subcorpus. All denotes the validation perplexity on our main validation\nset from seven corpus sources. Italic font denotes the validation PPL of their corpus domain, and Bold denotes\nsecond best after own corpus. Overall, Blog has the best overall validation perplexity in most tasks.\nFigure 4: Relation between validation PPL and in-\ncontext few-shot learning performance for five down-\nstream tasks as pretraining steps proceed. The results\ncome from the 1.3B-sized ALL model.\neach subcorpus. Each row corresponds to the model\nand each column corresponds to the validation set’s\nsubcorpus. Each validation set except All in Table\n7 consists of 10,000 instances, and is the part of our\nmain validation sets, consists of 70,000 instances.\nOn the other hand, Figure 4 shows that PPL and\nin-context learning performance correlate well in\nthe perspective of training a single model. We can\nfind that the correlation trends between the cases\nin the training and the cases between the corpus\ndomain are different.\n5 Discussion\nOur knowledge can be used to increase the per-\nformance of in-context learning when the corpus\nis small or/and there exists demand for collecting\nmore corpus. In the case of XGLM (Lin et al.,\n2021), which is a concurrent work on multilin-\ngual GPT-3, achieved better in-context learning\nperformance for many languages. However, it does\nnot reach the performance of a single language\nmodel. We hope our observation can give insight\ninto what types of pretraining to be collected more,\nboth for multilingual model and low-resource lan-\nguage model.\nAnother notable example comes from Gopher\n(Rae et al., 2021), which is a concurrent work on\nstate-of-the-art in-context learner. Rae et al. (2021)\ndetermine the ratio between subcorpora based on\nthe perplexity of the validation corpus. They implic-\nitly claim that this ratio results in better downstream\ntask performance, but do not address explicit ev-\nidence for this. On the other hand, we are in a\nposition to doubt the strong correlation between\nperplexity and in-context learning, especially in the\nfew-shot setting. We hope our findings contribute to\nmaking better in-context learners along with other\nresearch.\n6 Conclusion\nThis paper investigates the effects of the source\nand the size of the training corpus on in-context\nlearning ability, using the HyperCLOV A corpus.\nOur discoveries include that corpus sources play a\ncrucial role in whether or not in-context learning\nability will emerge in a large-scale language model.\nOne direction for future work is to investigate\nlinguistic properties of corpus sources which make\na competitive in-context learning model. For exam-\nple, quantifying the difference between two corpora\ncan shed light on how to select suitable corpora for\nNLP practitioners who build large-scale language\nmodels. In addition, intensive studies on different\ncorpus sources other than the HyperCLOV A cor-\npus can help understand the properties of in-context\nlearning.\n5176\nBroader Impact Statement\nWe present multiple pieces of evidence that models\nusing only a part of the pretraining corpus are com-\nparable with those trained with the entire corpus\nin terms of in-context performances. Although we\nleave the validation on larger-scale models, such as\ntens of billion parameters, to future work, our anal-\nysis presents a hint to effectively training LMs with\nsmaller corpora. This approach can contribute to al-\nleviating severe energy consumption issues caused\nby large-scale LMs.\nMeanwhile, our study relates to the misuse and\nfairness of large-scale LMs. For example, reweight-\ning domain-specific corpus might cause LMs to be\nbiased inherent in the domain corpus. Therefore,\nalleviating domain corpus bias would be a valuable\nfuture direction.\nAcknowledgment\nThe authors thank all the members of CLOV A, AI\nLab for devoted supporting and discussion. In par-\nticular, they thank Joonsuk Park and Seok Ho Yoon\nfor proofreading.\nReferences\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. GPT-Neo: Large Scale\nAutoregressive Language Modeling with Mesh-\nTensorflow.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde, Jared Kaplan, Harri Edwards, Yura\nBurda, Nicholas Joseph, Greg Brockman, et al. 2021a.\nEvaluating large language models trained on code.\narXiv preprint arXiv:2107.03374.\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,\nand He He. 2021b. Meta-learning via language\nmodel in-context tuning.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding,\nTravis Hoppe, Charles Foster, Jason Phang, Horace\nHe, Anish Thite, Noa Nabeshima, et al. 2020. The\npile: An 800gb dataset of diverse text for language\nmodeling. arXiv preprint arXiv:2101.00027.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nACL.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form competi-\ntion: Why the highest probability answer isn’t always\nright. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7038–7051, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nBoseop Kim, HyoungSeok Kim, Sang-Woo Lee,\nGichang Lee, Donghyun Kwak, Dong Hyeon Jeon,\nSunghyun Park, Sungju Kim, Seonhoon Kim, Dong-\npil Seo, et al. 2021. What changes can large-scale lan-\nguage models bring? intensive study on hyperclova:\nBillions-scale korean generative pretrained transform-\ners. In EMNLP.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language rep-\nresentation model for biomedical text mining. Bioin-\nformatics, 36(4):1234–1240.\nSeungyoung Lim, Myungji Kim, and Jooyoul Lee. 2019.\nKorquad1.0: Korean qa dataset for machine reading\ncomprehension. arXiv preprint arXiv:1909.07005.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.\nFew-shot learning with multilingual language models.\narXiv preprint arXiv:2112.10668.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training data\nvolume for compact language models. In EMNLP.\n5177\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2021. Metaicl: Learning to learn in\ncontext.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nSungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik\nCho, Jiyoon Han, Jangwon Park, Chisung Song, Jun-\nseong Kim, Yongsook Song, Taehwan Oh, et al. 2021.\nKlue: Korean language understanding evaluation.\narXiv preprint arXiv:2105.09680.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In EMNLP.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Stella Biderman, Leo Gao, Tali Bers,\nThomas Wolf, and Alexander M. Rush. 2021. Multi-\ntask prompted training enables zero-shot task gener-\nalization.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2021. Finetuned\nlanguage models are zero-shot learners.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference. In ICLR.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao,\nZhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\nWang, Xiaoda Zhang, et al. 2021. Pangu- α: Large-\nscale autoregressive pretrained chinese language\nmodels with auto-parallel computation. arXiv\npreprint arXiv:2104.12369.\nTony Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improving\nfew-shot performance of language models. ArXiv,\nabs/2102.09690.\n5178\nFigure 5: Validation perplexity of different 1.3B-size\nmodels in log scale. Color indicates the source of vali-\ndation corpus.\nA Details on Experimental Results\nTables 8 and 9 show standard derivation value on\nTables 2 and 4. Table 10 shows score difference\nwith ALL in addition to in-context learning scores\non Table 2. Figure 5, supporting Table 7, shows\nthe validation perplexity of different model from\ndifferent corpus.\nB Details on Pretraining Corpus\nTables 12 and 13 show example instances of seven\npretraining corpus in Korean and English, respec-\ntively.\nFor preprocessing steps of our pretraining cor-\npus, we use HyperCLOV A corpus which is also\nused in (Kim et al., 2021) as described in Section\n3.1. Therefore, we share the preprocessing steps of\nKim et al. (2021). Appendix A in (Kim et al., 2021)\ndescribes their preprocessing methods on data de-\nscriptoin, data clearning, data anonymization, and\ndata postprocessing.\nB.1 Deduplication Preprocess\nWe additionally introduce the deduplication pre-\nprocess of HyperCLOV A corpus, which is used in\n(Kim et al., 2021). The deduplication preprocess\nwas applied to construct HyperCLOV A corpus to\nprevent explicit duplication within and between\nsubcorpora (Kim et al., 2021). According to the\nresponse of Kim et al. (2021), they use an in-house\nsearch engine and an in-house engineering trick to\ndetect document pairs that are very similar to each\nother. There are two pipelined steps: (1) removing\nduplicates within subparts of the corpus, and then\n(2) removing duplicates between subparts of the\ncorpus. Therefore, documents with high overlap\ndo not exist throughout the documents. Here, the\nnumber of subparts is 29. These 29 subparts are\ncategorized into the eight domains we deal with in\nthe paper (i.e., Blog, News, Cafe, Comments,\nKiN, Modu, Ency, and Others). Overall, there\nis no explicit overlap between each corpus, since\nvery similar documents have already been removed\nfrom the corpus. The overlap between eight Hy-\nperCLOV A subcorpora is quite small. There were\nmany overlaps within the subpart of the corpus.\nHowever, the overlap between subparts of the cor-\npus was only 0.024% of the total, according to the\ncounts in the second pipelined step of deduplication\nbetween subparts.\nC Experiments on LoRA\nTable 11 shows the results of LoRA (Hu et al.,\n2021) finetuning on some models in Tables 2 and\n4.\nD Examples of Few-shot Prompt\nTables 14, 16, 18, and 19 show the example few-\nshot prompt of NSMC, KorQuAD, AI Hub, and\nYNAT, respectively. Tables 15, 17, and 20 show\nthe translated version for NSMC, KorQuAD, and\nYNAT, respectively.\nOn the other hand, the number of random seed\nis one for KorQuAD. We explain why evaluation\non KorQuAD with many random seeds is difficult,\nfrom the perspective of prompt design. The way\nwe make randomness on trials is to change few-\nshot examples in the prompt. However, in the case\nof Kim et al. (2021) and in our case, there are no\nalternative examples to put into the prompt. The\nprompt examples of KorQuAD are one document\nand a few question-answer pairs, and not a few\ndocument-question-answer triples. In other words,\nin the prompt of KorQuAD, the number of the\ndocument is one. Thus, the document is used for\nboth few-shot question-answer pairs and a query\nquestion for the inference. In KorQuAD, there are\nfive corresponding question-answer pairs in each\ndocument. In the experimental setting of ours and\nKim et al. (2021), four question-answers are put\ninto the prompt and one question is used for the\ntest. Therefore, there are no other question-answer\npairs to replace the four pairs.\n5179\nModel NSMC AI Hub (BLEU) YNAT\n(Acc) Ko →En En →Ko (F1)\nALL 84.59(1.25) 6.15(0.16) 23.36(0.33) 59.57(4.30)\nALL w/o Others 84.59(1.25) 6.14(0.21) 23.21(0.45) 50.76(11.81)\nBlog 83.50(2.45) 3.82(0.10) 20.11(0.79) 60.68(5.75)\nCafe 57.77(10.28) 2.83(0.19) 16.53(0.63) 11.04(4.43)\nNews 50.72(0.56) 1.10(0.78) 15.88(0.84) 14.36(2.38)\nComments 79.78(2.38) 0.79(0.01) 5.06(0.16) 36.17(3.31)\nKiN 54.73(4.26) 6.81(0.88) 18.16(0.71) 9.23(1.96)\nModu 69.91(8.41) 1.21(0.06) 6.13(0.39) 43.27(6.72)\nEncy 53.81(2.22) 0.58(0.16) 0.69(0.49) 27.99(2.38)\nBlog 54B 83.06(2.26) 3.93(0.20) 21.12(0.19) 57.97(5.72)\nBlog 27B 80.27(2.28) 5.35(1.95) 12.32(5.68) 48.19(6.71)\nCafe 27B 49.91(0.29) 4.25(0.35) 20.74(1.57) 8.60(2.79)\nNews 27B 50.64(3.26) 2.42(1.41) 15.78(4.21) 27.20(5.86)\nComments 27B 80.50(1.44) 1.70(0.03) 3.28(0.16) 25.79(7.27)\nTable 8: The results of Table 2 with standard deviation in parentheses.\nCorpus Type NSMC AI Hub (BLEU) YNAT\n(Acc) Ko →En En →Ko (F1)\nKiN+Ency 59.17(9.34) 8.99(0.31) 23.12(0.40) 42.84(9.01)\nCafe+KiN 76.42(4.68) 8.41(0.68) 23.41(0.38) 56.96(5.79)\nCafe+News 54.15(3.90) 4.45(0.12) 17.77(2.19) 8.19(5.32)\nBlog+Comments+Modu 82.82(1.93) 4.09(0.16) 21.17(0.43) 65.01(2.90)\nNews+KiN+Ency 75.96(5.94) 8.70(0.45) 23.38(0.18) 27.54(7.46)\nTable 9: The results of Table 4 with standard deviation in parentheses.\nCorpus Type Corpus NSMC KorQuAD AI Hub (BLEU) YNAT\nTrain (Acc) (EM) (F1) Ko →En En →Ko (F1)\nALL 150B 84.59 56.17 73.47 6.15 23.36 59.57\nThe Case where In-context learning Emerges by Combining Two Poor Corpora\nKiN+Ency 28.7B 59.17 (-25.42) 42.09(-14.08) 61.00(-12.47) 8.99(+2.84) 23.12(-0.24) 42.84(-16.73)\nCafe+KiN 109.5B 76.42(-8.17) 38.45(-17.72) 59.00(-14.47) 8.41(+2.26) 23.41(+0.06) 56.96(-2.61)\nThe Case where In-context learning Does Not Emerge by Combining Two Poor Corpora\nCafe+News 150B 54.15 (-30.44) 22.86(-33.31) 22.72(-50.75) 4.45(-1.70) 17.77(-5.59) 8.19(-51.38)\nThe Case of Combining In-context Emerging Corpora\nBlog+Comments+Modu 150B 82.82(-1.77) 54.94(-1.23) 72.27(-1.20) 4.09(-2.06) 21.17(-2.19) 65.01(+5.44)\nThe Case of AddingNewsinto KiN+Encyto Try to Enhance the Performance of YNAT\nNews+KiN+Ency 101.8B 75.96(-8.63) 35.42(-20.75) 55.60(-17.87) 8.70(+2.55) 23.38(+0.02) 27.54(-32.03)\nTable 10: Table 4 which includes the difference fromALL in parentheses.\nE Generalization to Other Languages\nSomeone can ask whether our results can be ex-\ntended to other languages, including English. We\nhave left experiments on non-Korean language as\nfuture work. However, we describe some expla-\nnations below to defend our experiments on the\nKorean language and to discuss why experiments\non other languages are practically non-trivial.\nFirst, we think our findings are basically gener-\nalizable to other languages. From the perspective\nof pretraining and in-context learning, fundamen-\ntal differences between Korean and English were\nlimitedly reported. For example, XGLM (Lin et al.,\n2021), a concurrent work on, also does not show\ncritical evidence on language-specific properties.\nSecond, It is non-trivial to control various as-\n5180\nModel NSMC YNAT\n(Acc) (F1)\nALL 91.83 86.47\nComments 92.02 84.07\nBlog 91.93 86.21\nCafe 91.57 85.45\nNews 90.62 86.57\nKiN 90.89 84.46\nModu 90.60 86.31\nEncy 86.93 82.37\nKiN+Ency 90.92 84.00\nCafe+KiN 90.99 87.52\nCafe+News 91.37 86.37\nBlog+Comment+Modu 88.83 87.07\nNews+KiN+Ency 91.13 86.62\nTable 11: LoRA finetuning performance on different\npretraining corpus and its combination.\npects of corpora for our purpose. Most corpus for\nin-context few-shot learners comes from crawled\nwebsite which is not easy to distinguish from its\noriginal source. For example, 82% of OpenAI GPT-\n3 Corpus (Brown et al., 2020) is a filtered version\nof Common Crawl. In this regard, we used rela-\ntively a well-refined corpus which consist of sev-\neral subcorpus from a single web service. (Please\nsee also Section B.1 of this letter.) On the other\nhand, we have interests to extend our work onto\nPile dataset (Gao et al., 2020), by controlling the\nsubcorpora in the direction our study pursuits, in\nthe future.\n5181\nExample\nBlog 블로그\n제목: 촬영하러 온꼬맹이들ˆˆ\n본문: 엄마 회사에 오늘은모델로일하러온꼬맹이들. 신나게놀고까불고뛰어다니다가책보더니잠잠\n해진다. 조용해진아이들보고놀라는스탭들. 순간엄마얼굴엔 미소가! ㅋㅋ. 책잘읽는아이들이라\n자랑스러움이잠시ㅋㅋ그러다가촬영하고생각보다잘해줘서고맙네. 엉망진창으로못할줄알았는데\n카메라를보다니. ㅋ어린시절부터카메라를본경험이빛을발하긴하나보다. ㅋㅋ하여간. 엄마회사에\n모델로와준꼬맹이들. 고마워ˆˆ. 좋은추억이되었길.\nCafe 카페\n제목: 탐스우먼 상자채새신발(직구한것보다싸게내놓아요 ˆˆ)\n본문: 벼룩시장(중고), 판매중, 가격1원, 안전거래미사용, 탐스클래식, 판매양식아이디이메일싸이,블\n로그,타카페,타사이트 링크시삭제및강퇴 거주지역도,시,동까지정확히 기재판매 제품명 구입시기년,\n월기재희망가격정확히 기재: (3만 4만등의경매 유도글삭제) 거래방법직거래, 택배, 안전거래 상세설\n명 탐스공홈에서직구했는데사이즈가커서내놓아요 빨리 팔려구직구한것보다싸게내놓아요 ˆˆ 1.\n탐스우먼 유니버시티 애쉬그레이택포45,00 2. 탐스우먼 초코캔버스택포40,000 많은문의부탁드려\n요 ˆˆ\nNews 뉴스\n제목: 전명환, 이병기시문학상 수상\n본문: ‘2016 이병기청년시문학상ㆍ최명희 청년소설문학상’ 수상자가결정됐다. 지난 1일전북대총장\n실에서시상식을연 가운데이병기청년시문학상 대학부문에는‘대과거’를 쓴 전명환(중앙대국어국문\n2년), 고등부문에는‘몽상’을선보인황주연(경산여고2년) 이선정됐다. 최명희 청년소설문학상 대학\n부문에는‘꽃에서부터’를 쓴 윤선미(서울디지털대문창3년), 고등부문에는‘야간비행’을쓴 윤정은\n(안양예고2년)이수상의영예를 안았다. 전북대학교(총장이남호) 신문방송사와혼불기념사업회ㆍ최\n명희문학관(대표 장성수)이공동으로주관하는공모전에는올해시부문 167명 669편, 소설부문 108명\n116편이출품됐다. 시부문 심사는최승범양병호유인이승철위원이, 소설부문 심사는서철원황보윤\n장마리 김소윤최기우위원이맡았다. 박준호문학상 운영위원장및신문방송사 주간은\"수준높았으며\n시대를 바라보는청년들의녹록치않은고민과생각을엿볼수있었다\"고평했다.\nComments 대화\n본문: 하루를 엄청길게사용하시네요ˆˆ 점심은더많이드세요\n아점입니다ㅎㅎ저녁을기다려야죠ˆˆ\n이렇게드시고무슨운동까지하십니까?? 저녁윗몸일으키기는빼세요\nㅋㅋㅋ요즘가끔 빼먹습니다..저담주월.화중에 앤더슨님방문할까합니다..같이가시죠?\n다음주월,화요??ˆˆ 가야죠„갑니다..시간을만들어서라도가야죠ˆˆ 어찌움직이실건지요??\n화요일날로..저는전철을타야해서무찌르자님은어디서출발하시는지요\n전서울성수동에서출발합니다.. 성수까지만 오시면 제가모시겠습니다..ˆˆ\nKiN 질의응답\n질문: 독실라게???? 사투리라는데독실라게가 뭔뜻인가요? 경상도쪽이라는데.\n본문: 경상도방언에서는엄청나게.억수로강조하는부사입니다\nModu 뉴스\n본문: 춘분에 눈내린 부산...강풍까지불며 피해속출눈이잘 오지않는부산에 춘분인21일0.7cm의\n눈이내려산간지역도로가통제되는등피해가잇따랐다고연합뉴스가보도했다. 부산기상청에따르면\n이날 부산의아침최저기온은공식관측소가있는중구대청동기준1도였다. 해발고도가500m 이상인\n중구서대신동은영하1.4도, 영도구는영하0.9도를 기록했다. 강한바람까지불면서체감온도는영하\n2.6도까지떨어졌다. 아침최저기온이영하권을넘나들면서밤새 내리던비가진눈깨비로변했다. 부산\n기장군에 있는기상청적설자동관측장비에는적설량이0.7cm로기록됐다.\nEncy 문서\n제목: 설악면\n본문: 설악면(雪岳面)은대한민국경기도가평군의면이다. 넓이는141.53 km²이고, 인구는2016년12\n월말 주민등록기준으로8,986 명이다.설악면은북한강남쪽에 있어서본래 양평군에 속했는데, 1942년\n가평군에 편입되었다. 강원도에 있는설악산(雪嶽山)과는무관하다.\nTable 12: Example document from various domains. Note that Modu consists of 5 different subdomains and the\nexample is taken from the news subdomain, which is the largest.\n5182\nExample\nBlog Blog\nTitle: Kids who came to shoot ˆˆ\nBody: Kids who came to my mom’s company as models today. After having fun, playing around, and running\naround, they read a book and calmed down. Staff members are surprised to see the quiet kids. A smile on\nmy mom’s face! Haha. I’m proud of them because they’re good at reading books, and then I took phots\nand thank them for doing better than I thought. I thought they do mess it up, but I can’t believe that they\nare looking at the camera. The experience of looking at the camera since early childhood must have helped.\nAnyway. The kids who came to my mom’s company as models. Thank you ˆˆ. I hope it was a good memory.\nCafe Cafe\nTitle: Toms women’s shoes (It’s cheaper than what I bought directly ˆˆ)\nText: Flea market (used), selling, price of 1 won, not used for safety transactions, Tom’s Classic, sales form\nID Email, blog, other cafe, other site link, city, dong, exact date of purchase of sales product, desired price\nof 30,000 to 40,000 won, direct auction transaction. Urgent sale and sell it cheaper than what I bought\ndirectly. ˆˆ1. Tom’s Woman University Ash Gray 45,00 2. Tom’s Woman Chocolate Canvas 40,000. Please\ncontact us.ˆˆ\nNews News\nTitle: Jeon Myeonghwan and Lee Byungki won the Poem Literature Award.\nBody: The winners of the 2016 Young Poetry Literature Award and Choi Myung-hee Young Novel Literature\nAward have been decided. While the awards ceremony was held at Jeonbuk National University’s president’s\noffice on the 1st, Jeon Myung-hwan (second year of Chung-Ang University’s Korean Language Language)\nwho wrote \"the past\" in the college category and Hwang Joo-yeon (second year of Gyeongsan Girls’ High\nSchool) were selected. Yoon Sun-mi (3rd year of Moonchang, Seoul Digital University), who wrote \"From\nFlowers\" in the college category of Choi Myung-hee’s Youth Novel Literature Award, and Yoon Jung-eun\n(2nd year of Anyang Arts High School), who wrote \"Night Flight\" in the high school category, were honored.\nThe contest, co-hosted by Jeonbuk National University (President Lee Nam-ho) newspaper broadcasters,\nHonbul Memorial Society, and Choi Myung-hee Literature Museum (CEO Jang Sung-soo), featured 669\nworks of 167 people in the poetry category and 116 works of 108 people in the novel category this year.\nChoi Seung-beom, Yang Byung-ho, Yoo Seung-chul, a member of the Yoo In, and Seo Cheol-won, Hwang\nBo-yoon, Jang Mari, Kim So-yoon, and Choi Ki-woo, a member of the novel division, were in charge of\nthe screening. Park Joon-ho, chairman of the Literature Award’s steering committee and weekly newspaper\nbroadcaster, commented, \"It was high-quality, and I could get a glimpse of the difficult worries and thoughts\nof young people looking at the times.\"\nComments Conversation\nBody: You spend a long day.ˆˆ Eat more for lunch.\nIt’s brunch. We have to wait for dinner.ˆˆ\nWhat kind of exercise do you do after eating like this? Don’t do sit-ups in the evening.\nI’ve been skipping it from time to time. I’m going to visit Anderson next Monday and Tuesday.Let’s go\ntogether, right?\nNext Monday and Tuesday?ˆˆ I have to go, I’m going...I’ll make time to go there.ˆˆ How are you going to\nmove?\nOn Tuesday... I have to take the subway, so where will you leave?\nI’m departing from Seongsu-dong, Seoul. If you come all the way to Seongsu, I’ll take you.ˆˆ\nKiN QnA\nQuestion: Doksilagae? It’s a dialect. What does doksilagae mean? It’s used near Gyeongsang-do.\nText: In Gyeongsang-do dialect, it means tremendously.It’s an adverb to emphasize.\nModu News\nText: It snowed in the spring equinox in Busan...Strong winds are blowing and they’re avoiding it. Yonhap\nNews Agency reported that 0.7 centimeters of snow fell on the 21st, the spring equinox in Busan, where\nsnow was not easy, and roads in mountainous areas were controlled. According to the Busan Meteorological\nAdministration, the lowest temperature in the morning in Busan was 1 degree in Daecheong-dong, Jung-gu,\nwhere the official observation station is located. Seodaemun-dong, Jung-gu, with an altitude of more than\n500m above sea level, recorded minus 1.4 degrees Celsius and Yeongdo-gu recorded minus 0.9 degrees\nCelsius. As strong winds blew, the sensible temperature dropped to minus 2.6 degrees Celsius. As the\nmorning low temperature crossed below zero, the rain that had been falling all night turned into sleet. The\nautomatic snow observation equipment of the Korea Meteorological Administration in Gijang-gun, Busan\nrecorded a snowfall of 0.7cm.\nEncy Document\nTitle: Seorakmyeon.\nBody: Seorak-myeon is a myeon of Gapyeong-gun, Gyeonggi-do, Korea. The area is 141.53 km², and the\npopulation is 8,986 based on resident registration at the end of December 2016. Seorak-myeon was originally\npart of Yangpyeong-gun in the south of the Bukhangang River, but was incorporated into Gapyeong-gun in\n1942. It has nothing to do with Seoraksan Mountain in Gangwon-do.\nTable 13: An example document in Table 12, translated into English by a machine translator.\n5183\nContext -> 아 더빙.. 진짜짜증나네요목소리 (부정)\n흠...포스터보고초딩영화줄....오버연기조차가볍지않구나 (부정)\n너무재밓었다그래서보는것을추천한다(긍정)\n교도소이야기구먼 ..솔직히 재미는없다..평점조정 (부정)\n사이몬페그의익살스런 연기가돋보였던영화!스파이더맨에서늙어보이기만 했던커스\n틴 던스트가너무나도이뻐보였다(긍정)\n...\n원작의긴장감을제대로살려내지못했다.\nCorrect Answer -> (부정)\nIncorrect Answer -> (긍정)\nTable 14: Formatted dataset example for NSMC. (few-shot: 70)\nContext -> Ah dubbing.. It’s really annoying. voice (Negative)\nHm... I saw the poster and gave elementary school student movie lines...Even overacting\nisn’t light. (negative)\nIt was so much fun, so I recommend watching it (negative)\nIt’s about the prison...Honestly, it’s not fun.Adjusting the rating (negative)\nIt’s a movie where Simon Peg’s humorous acting stood out!Kirsten Dunst, who only looked\nold in Spider-Man, looked so pretty. (positive)\n...\nIt did not capture the tension of the original work properly.\nCorrect Answer -> (negative)\nIncorrect Answer -> (positive)\nTable 15: An example document in Table 14, translated into English by a machine translator\nContext -> 제목: 임종석\n지문: 1989년2월15일여의도농민 폭력시위를 주도한혐의(폭력행위등처벌에관\n한법률위반)으로지명수배되었다. 1989년3월12일서울지방검찰청공안부는임종\n석의사전구속영장을발부받았다. 같은해6월30일평양축전에 임수경을대표로\n파견하여 국가보안법위반혐의가추가되었다. 경찰은12월18일20일사이서울경\n희대학교에서임종석이성명 발표를 추진하고있다는첩보를 입수했고, 12월18일\n오전7시40분경가스총과전자봉으로무장한특공조및대공과직원12명 등22명\n의사복경찰을승용차8대에나누어경희대학교에투입했다. 1989년12월18일오전\n8시15분경서울청량리경찰서는호위학생 5명과함께경희대학교학생회관건물\n계단을내려오는임종석을발견, 검거해구속을집행했다. 임종석은청량리경찰서에\n서약 1시간동안 조사를 받은뒤오전9시50분경서울장안동의서울지방경찰청\n공안분실로인계되었다.\n질문: 1989년6월30일평양축전에 대표로파견된인물은?\n답변: 임수경\n질문: 임종석이여의도농민 폭력시위를 주도한혐의로지명수배된연도는?\n답변: 1989년\n질문: 임종석을검거한장소는경희대내 어디인가?\n답변: 학생회관건물 계단\n질문: 임종석이조사를 받은뒤인계된곳은어딘가?\n답변: 서울지방경찰청공안분실\n질문: 1989년2월15일여의도농민 폭력시위를 주도한혐의로지명수배된사람의\n이름은?\n답변:\nTarget Completion -> 임종석\nTable 16: Formatted dataset example for KorQuAD: Machine Reading Comprehension (MRC) (few-shot: 4)\n5184\nContext -> Title: Lim Jongseok.\nMain Text: On February 15, 1989, he was wanted for leading a violent demonstration\nagainst farmers in Yeouido (violence of the Punishment of Violence, etc. Act). On\nMarch 12, 1989, the Ministry of Public Security of the Seoul District Prosecutors’\nOffice received a preliminary arrest warrant for Lim Jong-seok. On June 30 of the\nsame year, Lim Soo-kyung was dispatched as a representative to the Pyongyang\nFestival, adding charges of violating the National Security Act. The police obtained\ninformation that Lim Jong-seok was pushing for a statement at Kyung Hee University\nin Seoul between December 18 and December 18, and distributed 22 plainclothes\npolice, including 12 special forces and anti-aircraft staff armed with gas guns and\nelectronic rods, to Kyung Hee University. At around 8:15 a.m. on December 18,\n1989, the Seoul Cheongnyangni Police Station found Lim Jong-seok, who came\ndown the stairs of the Kyunghee University Student Center building with five escort\nstudents, arrested him and executed his arrest. Lim Jong-seok was investigated by the\nCheongnyangni Police Station for about an hour and handed over to the Seoul Metropoli-\ntan Police Agency’s public security loss office in Jangan-dong, Seoul, at around 9:50 a.m.\nQuestion: Who was dispatched as a representative at the Pyongyang Festival on\nJune 30, 1989?\nAnswer : Lim Su-kyung\nQuestion: When was Lim Jong-seok wanted to be arrested for leading a violent\ndemonstration against farmers in Yeouido?\nAnswer: 1989\nQuestion: Where in Kyung Hee University did you arrest Lim Jong-seok?\nAnswer: Stairs in the building of the student center.\nQuestion: Where was Lim Jongseok handed over after being investigated?\nAnswer: Seoul Metropolitan Police Agency lost public security.\nQuestion: What is the name of the person who was wanted for leading the Yeouido\npeasant violence protest on February 15, 1989?\nAnswer:\nTarget Completion -> Lim Jongseok.\nTable 17: Example document in Table 16, translated into English by a machine translator\nContext -> 스키너가말한보상은대부분눈으로볼수있는현물이다.=Skinner’s reward is mostly\neye-watering.\n심지어 어떤문제가발생할건지도어느정도예측이가능하다.=Even some problems\ncan be predicted.\n...\n오직하나님만이그이유를 제대로알 수있을겁니다.=\nTarget Completion -> Only God will exactly know why.\nTable 18: Formatted dataset example for AI-Hub: Translation\nContext -> 네이버랩스3D 지도기술업체에피폴라 인수(과학)\n野北축구생중계거부에 대북정책현주소종합(정치)\n즐라탄 행선지정한듯. . .큰 소식알려드리겠다(스포츠)\n페루아마존지역서70대英 환경운동가불에 타 숨진채발견(세계)\n머리 맞댄경제부총리와한국은행총재(경제)\n전주MBC 전북출>신故 이용마 기자추모공간사흘간운영(사회)\n...\n구글인공지능다음도전은스타크래프트\nCorrect Answer -> (과학)\nIncorrect Answer -> (세계)\nTable 19: Formatted dataset example for YNAT: Topic Classification (few-shot: 70)\n5185\nContext -> NA VER LABS acquires 3D map technology company Epipolar(Science)\nNorth Korea’s refusal to broadcast live soccer matches the current state of North Korea\npolicy(Politics)\nIt seems that Zlatan’s destination has been decided... I’ll tell you the big news(Sport)\nBritish environmentalist 70-year-old found burnt to death in Peruvian Amazon(World)\nDeputy Prime Minister of Economy and Bank of Korea Governor(Economy)\nJeonju MBC Operates a three-day memorial space for the late reporter Lee Yong-ma from\nJeonbuk(Social)\n...\nGoogle’s next AI challenge is Starcraft\nCorrect Answer -> (Science)\nIncorrect Answer -> (World)\nTable 20: Example document in Table 19, translated into English by a machine translator\n5186",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6196189522743225
    },
    {
      "name": "Language model",
      "score": 0.5708645582199097
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5686099529266357
    },
    {
      "name": "Natural language processing",
      "score": 0.5432928204536438
    },
    {
      "name": "Linguistics",
      "score": 0.5278891324996948
    },
    {
      "name": "Computational linguistics",
      "score": 0.492411732673645
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4833928644657135
    },
    {
      "name": "Scale (ratio)",
      "score": 0.47049710154533386
    },
    {
      "name": "Cognitive science",
      "score": 0.45079922676086426
    },
    {
      "name": "Corpus linguistics",
      "score": 0.4273836612701416
    },
    {
      "name": "Psychology",
      "score": 0.2616058886051178
    },
    {
      "name": "History",
      "score": 0.18029245734214783
    },
    {
      "name": "Philosophy",
      "score": 0.14816462993621826
    },
    {
      "name": "Cartography",
      "score": 0.09985530376434326
    },
    {
      "name": "Geography",
      "score": 0.08372801542282104
    },
    {
      "name": "Archaeology",
      "score": 0.06749150156974792
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60922564",
      "name": "Naver (South Korea)",
      "country": "KR"
    }
  ],
  "cited_by": 31
}