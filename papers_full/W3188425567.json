{
  "title": "Mental Models of AI Agents in a Cooperative Game Setting (Extended Abstract)",
  "url": "https://openalex.org/W3188425567",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5049124089",
      "name": "Katy Ilonka Gero",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A5003440919",
      "name": "Zahra Ashktorab",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5082995130",
      "name": "Casey Dugan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5064662859",
      "name": "Pan Qian",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5082721686",
      "name": "James M. Johnson",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5022799238",
      "name": "Werner Geyer",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5047450409",
      "name": "María-Luisa Martín-Ruiz",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5066477230",
      "name": "Sarah Miller",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5022343394",
      "name": "David R. Millen",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5045892503",
      "name": "Murray Campbell",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5109346867",
      "name": "Sadhana Kumaravel",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101669643",
      "name": "Wei Zhang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2168317577",
    "https://openalex.org/W2898694742",
    "https://openalex.org/W2913781869",
    "https://openalex.org/W2017267789",
    "https://openalex.org/W4241815401",
    "https://openalex.org/W2303413189",
    "https://openalex.org/W2168747479",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W1994606570",
    "https://openalex.org/W2984353433",
    "https://openalex.org/W2942678267",
    "https://openalex.org/W2942444880",
    "https://openalex.org/W2918341242",
    "https://openalex.org/W2942073295",
    "https://openalex.org/W2795349081",
    "https://openalex.org/W2941411956",
    "https://openalex.org/W2003966557",
    "https://openalex.org/W2035726644",
    "https://openalex.org/W2964207259"
  ],
  "abstract": "As more and more forms of AI become prevalent, it becomes increasingly important to understand how people develop mental models of these systems. In this work we study people's mental models of an AI agent in a cooperative word guessing game. We run a study in which people play the game with an AI agent while ``thinking out loud''; through thematic analysis we identify features of the mental models developed by participants. In a large-scale study we have participants play the game with the AI agent online and use a post-game survey to probe their mental model. We find that those who win more often have better estimates of the AI agent's abilities. We present three components---global knowledge, local knowledge, and knowledge distribution---for modeling AI systems and propose that understanding the underlying technology is insufficient for developing appropriate conceptual models---analysis of behavior is also necessary.",
  "full_text": "Mental Models of AI Agents in a Cooperative Game Setting (Extended Abstract)\u0003\nKaty Ilonka Gero1, Zahra Ashktorab2, Casey Dugan2, Qian Pan2, James Johnson2, Werner\nGeyer2, Maria Ruiz3, Sarah Miller3, David R Millen3, Murray Campbell2, Sadhana Kumaravel2,\nWei Zhang2\n1Columbia University\n2IBM Research AI\n3IBM Watson\nkaty@cs.columbia.edu, fzahra.ashktorab1, qian.pan, sadhana.kumaravel1, maria.ruizg@ibm.com,\nfcadugan, jmjohnson, werner.geyer, millers, david r millen, mcam, zhangweig@us.ibm.com\nAbstract\nAs more and more forms of AI become prevalent, it\nbecomes increasingly important to understand how\npeople develop mental models of these systems. In\nthis work we study people’s mental models of an AI\nagent in a cooperative word guessing game. We run\na study in which people play the game with an AI\nagent while “thinking out loud”; through thematic\nanalysis we identify features of the mental models\ndeveloped by participants. In a large-scale study we\nhave participants play the game with the AI agent\nonline and use a post-game survey to probe their\nmental model. We ﬁnd that those who win more of-\nten have better estimates of the AI agent’s abilities.\nWe present three components—global knowledge,\nlocal knowledge, and knowledge distribution—for\nmodeling AI systems and propose that understand-\ning the underlying technology is insufﬁcient for de-\nveloping appropriate conceptual models—analysis\nof behavior is also necessary.\n1 Introduction and Related Work\nWhen we sit down to drive a car, or look for a ﬁle on our\ncomputer, we use a mental model to make sense of the world\nand act on it. Mental models are developed quickly and un-\nconsciously by users. In contrast, conceptual models are de-\nveloped slowly and purposefully by experts. Discrepancies\nbetween the two can lead to problems, ranging from misun-\nderstanding and confusion to the abandonment of a system.\nThrough studies of human error and human-machine inter-\naction, Norman [2014] observes that mental models are in-\ncomplete, limited, unstable, unscientiﬁc, parsimonious, and\nlack ﬁrm boundaries—they value utility over accuracy. Greca\nand Moreira [2000], considering mental models in the con-\ntext of science education, ﬁnd that instruction on a conceptual\nmodel does not lead to students’ acquiring perfect copies of it,\nand that modiﬁcation of initial mental models is difﬁcult, sug-\ngesting we enrich existing models rather than overhaul them.\n\u0003Originally published as “Mental models of AI agents in a coop-\nerative game setting.” in Proceedings of the 2020 CHI Conference\non Human Factors in Computing Systems.\nFigure 1: A mental model of an AI agent has three components:\nbehavior at a large scale, the agent’s knowledge of various topics,\nand behavior at the scale of an individual output.\nAs AI systems appear in high-stakes environments, such\nas decisions about who to hire [Dickson and Nusair, 2010 ]\nor diagnosing diseases [Cai et al., 2019], understanding peo-\nple’s mental models of these systems becomes increasingly\nimportant. Additionally, the label ‘AI system’ may be ap-\nplied to a variety of technologies, from linear regression-\nbased predictions to neural network-generated images, com-\nplicating our ability to learn about them. While some HCI re-\nsearchers have looked into how people develop mental mod-\nels of AI systems [Kulesza et al., 2012; Kulesza et al., 2013;\nBansal et al., 2019 ], mostly we have seen research into\nExplainable AI [Cheng et al., 2019; Wang et al., 2019;\nWiegand et al., 2019; Kunkel et al., 2019]. But a rich under-\nstanding of the underlying technology does not always lead to\na rich understanding of how a system will behave. For now,\nmany AI systems remain idiosyncratic in their behavior.\nMany important questions remain open. In the context of\na cooperative word guessing game, we pose the following re-\nsearch questions:\n1. What should conceptual models of AI systems include?\n2. How do users develop mental models of AI systems?\n3. What encourages accurate mental models of AI sys-\ntems?\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSister Conferences Best Papers Track\n4770\nFigure 2: Example round of the game Passcode, hints provided by\nthe AI agent and guesses provided by the participant.\n2 System Design\nIn this work we focus on cooperative word games, which re-\nquire understanding what your partner is thinking. Studying\nmental models in this context has a long history in linguistics\n[Wittgenstein, 2009] and more recently has gained popularity\nin AI research [Rovatsos et al., 2018; Bard et al., 2019 ]. In\nparticular, we use a game called ‘Passcode’. In this game one\nplayer tries to guess a word that the other player is thinking\nof; the other player provides one word hints. The game itself\nis grounded in trying to understand what the other player is\nthinking, making it an excellent test bed for studying mental\nmodels. Figure 2 shows a typical round of gameplay.\nWe use two reinforcement learning-based AI agents trained\nto play Passcode–one to play the giver (who has a target word\nand gives hints) and one to play the guesser (who is trying to\nguess the target word based on the hints). Each AI agent has a\nneural network architecture, is pre-trained with word associa-\ntion data [Nelson et al., 2004], has access to a commonsense\nknowledge graph [Speer et al., 2017], and is trained further in\na reinforcement learning framework. As with many AI sys-\ntems, these AI agents perform quite well at the game, but are\nnot perfect.\nWe consider what a conceptual model (i.e. an accurate\nmental model) of the AI agent would look like. A precise de-\nscription of the neural network architecture and training pro-\ncedure does not always represent a system’s actual behavior,\nwhich may differ from its intended behavior.\nFor the rest of the paper we will focus only on the AI agent\nfor the giver, which we call ‘WordBot’. This is an impor-\ntant simpliﬁcation, as the two AI agents (giver and guesser)\nhave slightly different actual and intended behaviors, given\nthe different roles they play. Figure 3 shows a diagram of the\nAI agent for the giver.\nWe take a systematic approach to characterizing the behav-\nior of the agent. For example, we cannot assume that because\nWordBot has access to a knowledge base, it effectively uses\nall that information to generate meaningful hints. In the com-\nmonsense knowledge graph, there is rich information about\nParis—that Paris is the capital of France, that the Eiffel Tower\nand the Louvre are located there, that it has cafes and boule-\nvards. Yet the hints that WordBot provides for the target word\nFigure 3: Diagram of the ‘giver’ AI agent, called WordBot. Word-\nBot is a trained neural network, which has encoded information from\nthe training data. In addition, information from a knowledge base is\nused as input along with the game state.\n’paris’ are ‘city’, ‘usa’, ‘plant’—WordBot appears to have\nvery poor knowledge of Paris.\nWe deduce a conceptual model of WordBot from a combi-\nnation of understanding its structure and training procedure,\nand a series of analyses of actual results from playing with\nWordBot. We note that the terminology we use below was\ndeveloped iteratively and informed by the results of Study 1.\nWe present the following conceptual model of WordBot:\n2.1 Global Behavior\n\u000fWordBot does not remember or adjust its hints based on\npast rounds.\n\u000fWordBot rarely adjusts its hints based on incorrect\nguesses within a single round.\n\u000fWordBot has no explicit hint sequencing strategy.\n2.2 Knowledge Distribution\n\u000fWordBot does not know anything about pop culture (as\nthis is not in the training data).\n\u000fWordBot has limited knowledge about geography/places\n(35% of hints are bad).\n\u000fWordBot has decent knowledge about food/cooking\n(11% of hints are bad).\n2.3 Local Behavior\n\u000fWordBot gives both synonym (29% of the time) and\nantonym (11% of the time) hints.\n\u000fWordBot gives one or more hints that are not highly re-\nlated to the target word in 4% of games.\n\u000fWordBot takes into account multiple senses of a word (if\na word has multiple senses).\nWe note that our use of the term “local behavior” is related\nto the team “local explanations” as used in the explainable\nAI literature [Mittelstadt et al., 2019]. The “local behavior”\nportion of a system model identiﬁes how individual decisions\nor actions made by a system; “local explanations” seek to\nexplain these individual decisions or actions.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSister Conferences Best Papers Track\n4771\n3 Study 1: Small-Scale, Think-Aloud Study\n3.1 Methodology\nIn this study, we brought 11 participants (recruited from\nIBM) into the lab either as individuals or as teams of two\nto play Passcode with WordBot while thinking out loud about\ntheir strategy and the strategy of WordBot. Not all partici-\npants worked on technology development (for example some\nworked in operations). The average age was 22.4 (\u0006 2.8)\nyears, and 64% of participants had some exposure to coding.\nAll participants had read about ‘artiﬁcial intelligence’ in the\nnews. All participants, either as an individual or on a team,\nplayed 5 games as the giver and 5 games as the guesser, the\norder counter-balanced. The AI agent, WordBot, assumed the\nother role and participants interacted with WordBot through a\nsimple command-line version of the game. Participants were\ngiven a maximum of 10 guesses per game; if they had not\nwon the game within 10 guesses, they moved on to the next\nround. All participants played the game using the same tar-\nget words in the same order. These words were randomly\nselected from the vocabulary of the AI agent and had a range\nof difﬁculties. We conducted a thematic analysis [Braun and\nClarke, 2012] on the resulting transcripts of what the partici-\npants said while playing as well as their responses to a post-\nplay semi-structured interview. This study gave us insight\ninto what were the important aspects of a conceptual model,\nthe kinds of mental models players develop, and how players\ncome to their beliefs about the system.\n3.2 Results\nTable 1 describes the 10 codes developed through the the-\nmatic analysis, ordered by their prevalence in the transcripts.\nAll utterances related to the research questions were marked\nwith a code. Not all codes correspond to expressions of a\nparticipant’s mental model; instead, many correspond to mo-\nments when a participant’s mental model is used or chal-\nlenged. Broadly, participants remarked upon what the AI\nagent knows and how the AI agent plays the game. They\neither said statements about these things, or gave questions or\nexpressions of uncertainty about these things. These results\nguided our development of a ‘mental model’ survey, used in\nStudy 2 to probe participants’ mental models of WordBot.\nThe most prevalent code (18% of all utterances) was\nanomalies/distress/trust. These responses included simple\nacknowledgement of an unexpected move, distress in which\nthe participant believed they were stupid for not understand-\ning the unexpected move, and concerns about not trusting that\nthe AI agent was making good or meaningful moves. There\nwere several understandably confusing moves from the AI\nagent, as well as moves that in retrospect made sense (e.g.\nantonym hints). Some participants were slow to fault the AI\nagent even when reviewing a game in which some hints were\nclearly not helpful; instead they would interpret and justify\nstrange moves. Others immediately blamed the AI agent, and\nwere slow to acknowledge that they may have misunderstood\nhow the AI agent was relating words. These moments of con-\nfusion forced participants to judge the AI agent in order to\nprogress, and often resulted in a participant changing their\nmental model when the target word was revealed.\n4 Study 2: Large-Scale, Online Gameplay\n4.1 Methodology\nTo better understand what impacts people’s mental models,\nwe ran a large-scale, online study using Amazon Mechani-\ncal Turk. For this study we had participants only play as the\nguesser (the AI agent played as the giver). Participants were\nallowed a maximum of 5 guesses. We looked at three factors\nwhich could inﬂuence people’s mental models:\n\u000fThe number of game rounds played\n\u000fThe target words played (i.e. difﬁculty, theme, etc.)\n\u000fThe win rate of the player\nParticipants played either 5 or 10 game rounds, where each\nround consisted of trying to guess a single target word, and\nplayed one of two wordlists (i.e. the target words to guess).\nParticipants playing only 5 game rounds played on the ﬁrst\nﬁve words in the list. The two wordlists were balanced for dif-\nﬁculty, as well as topic—for instance, each word list had the\nsame number of food-related words. Participants saw their\nwords in a random order.\nWe could not control for how often a participant won or\nlost, but in analysis split participants up into the top 50% of\nplayers (‘winners’ – those who won the same or more than\nthe median amount) and the bottom 50% (‘losers’ – those\nwho less than the median amount). The game was developed\ninto an online web application using Flask (a lightweight\nPython framework for web apps) and React (a Javascript li-\nbrary for building front-end interfaces). 1 Participants ﬁrst\ntook a short demographic survey, then played 5 or 10 game\nrounds, and then took a survey that asked questions about how\nthey thought the AI agent worked.2\n4.2 Results\nThe study resulted in 89 Amazon Mechanical Turk workers\nparticipating in the study in ‘good faith’.3 Despite a signif-\nicant portion of non-native English speakers (17%), we saw\nno difference in win rate between native English speakers and\nnot. Similarly we saw no difference in win rate for age or ed-\nucation level. We had three questions that asked about partic-\nipants’ familiarity with word games, machine learning, and\ncoding. These were not predictors of win rate. Additionally,\nthere were no signiﬁcant differences between any survey an-\nswers for the number of games played.\nWe did see signiﬁcant differences between the ‘winners’\nand ‘losers’. Table 2 shows mean survey responses and sig-\nniﬁcance levels. Let’s consider the two global behavior ques-\ntions. Losers tend to believe (more than winners) that Word-\nBot takes into consideration your past incorrect guesses, as\nwell as previous game plays. Both of these are untrue. Win-\nners tend to be unsure, or suspect WordBot does not take into\nconsideration these things. Here it is clear that winners have a\nbetter understanding of the global behavior of WordBot than\nlosers; losers tend to overestimate WordBot’s abilities.\n1A demo can be found at ibm.biz/wordbot.\n2Development of the survey was based on Study 1.\n3All guesses were inspected manually, and any participant who\nclearly had not put in a good faith effort, for instance always guess-\ning the word ‘word’ regardless of the hints, were removed.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSister Conferences Best Papers Track\n4772\nCode Prev Description and Example Quote\nanomalies/distress/trust 18% Noting unexpected behavior from the AI agent, or expressing distress or mistrust in re-\nsponse to unexpected behavior.\nP6: Wait so we have ‘chill’ and ‘hectic’. I’m confused.\npattern seeking 16% Discussing or questioning speciﬁc patterns (within a single game) the AI agent uses to\ngive hints/guesses.\nP9: It would make me feel bad if there was a pattern that we were totally missing.\nsynonyms/antonyms 15% Any discussion of synonyms or antonyms as it related to the type or efﬁcacy of hints.\nP2: ...the fact that it could give antonyms because I thought it would only do synonyms.\nAI knowledge 14% Discussion of what the AI agent does or does not know, or questioning the same.\nP2: I mean it smells but I don’t think the AI would know that nail polish smells.\nmemory/weighting 12% Discussion of how much the AI agent remembers, or how much ‘weight’ is given to\nsubsequent hints/guesses.\nP4: I guess I need to look at all four of these equally.\nsteering 10% Noting the need to “steer” the AI agent (or be steered by the AI agent) toward the target\nword, or questioning how to best get the AI agent “back on track”.\nP10: How to get them back on track when they start going off...\nneed for explanation 7% Expression of desire for explanation for a single hint/guess or generally for how the AI\nagent made decisions.\nP7: Can I know what the AI is? That would be very useful for me.\nreﬂection 5% Explicit reﬂection on past game plays to inform the next move.\nP9: Uhhh I feel like this is another ‘minute’ situation. This feels familiar.\npersoniﬁcation 3% Questioning or hesitation about how to describe the AI agent, or explicit discussion of the\nAI agent as one would a human.\nP8: Maybe a different unit of time would lead them – it – down a better path.\nperspective taking 2% Explicit discussion of the perspective of the AI agent.\nP8: ...no one would say ‘give’ to help us guess ‘marriage’.P9: Maybe a bot would.\nTable 1: Name, prevalence, description, and example quote of the ten codes found through the thematic analysis of the think-aloud transcripts.\nPrevalence is calculated as the number of utterances marked with a particular code divided by the total number of utterances marked with a\ncode; there were exactly 100 utterances so it also represents the utterance counts.\nMean\nQuestion (shortened) winner loser p-value\nGLOBAL BEHAVIOR\nadjusts hints based on guesses 3.9 4.6 .02\nremembers past gameplays 3.6 4.4 .01\nKNOWLEDGE DISTRIBUTION\nknows about pop culture 3.7 4.3 .16\nknows about geography/places 4.2 4.8 .09\nknows about food/cooking 4.4 4.8 .26\nLOCAL BEHAVIOR\nmany synonym hints 5.0 5.1 .62\nmany antonym hints 3.5 4.6 .01\nTable 2: Results from post-gameplay survey, split by winner/loser.\nSigniﬁcant differences bolded. We see that losers over estimate\nglobal behavior, and some local behavior. We don’t see any dif-\nferences in knowledge distribution, perhaps because there was not\nenough exposure to the system.\n5 Conclusion\nWe studied conceptual and mental models of AI systems in\nthe context of a word guessing game, Passcode. We devel-\noped a conceptual model of an AI agent that plays Passcode,\nﬁnding three key components of conceptual models for AI\nsystems more generally: global behavior, knowledge distri-\nbution, and local behavior. We probed user mental models\nin two studies. The ﬁrst was an analysis of a think-aloud\nstudy (n=11) in which participants played Passcode with an\nAI agent, illustrating the themes that arise when trying to\nunderstand an AI technology. The second was an online\nstudy (n=89) in which participants played Passcode with an\nAI agent and ﬁlled out a survey about their mental model,\nshowing that playing more games did not increase the accu-\nracy of a mental model, but that participants who won more\noften did have more accurate models. Overall we found that\npeople have existing intuitions about how AI systems work\nwhich can upset their understanding of a speciﬁc AI agent,\nand that people tend to revise their mental model in the face\nof anomalies.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSister Conferences Best Papers Track\n4773\nReferences\n[Bansal et al., 2019] Gagan Bansal, Besmira Nushi, Ece Ka-\nmar, Walter Lasecki S, Daniel S Weld, and Eric Horvitz.\nBeyond accuracy: The role of mental models in human-ai\nteam performance. In Proceedings of the AAAI Conference\non Human Computation and Crowdsourcing, 2019.\n[Bard et al., 2019] Nolan Bard, Jakob N Foerster, Sarath\nChandar, Neil Burch, Marc Lanctot, H Francis Song,\nEmilio Parisotto, Vincent Dumoulin, Subhodeep Moitra,\nEdward Hughes, et al. The hanabi challenge: A new\nfrontier for ai research. arXiv preprint arXiv:1902.00506,\n2019.\n[Braun and Clarke, 2012] Virginia Braun and Victoria\nClarke. Thematic analysis. In APA handbook of research\nmethods in psychology, Vol. 2., pages 57–71. American\nPsychological Association, 2012.\n[Cai et al., 2019] Carrie J Cai, Emily Reif, Narayan Hegde,\nJason Hipp, Been Kim, Daniel Smilkov, Martin Watten-\nberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe,\net al. Human-centered tools for coping with imperfect al-\ngorithms during medical decision-making. In Proceedings\nof the 2019 CHI Conference on Human Factors in Com-\nputing Systems, page 4. ACM, 2019.\n[Cheng et al., 2019] Hao-Fei Cheng, Ruotong Wang, Zheng\nZhang, Fiona O’Connell, Terrance Gray, F. Maxwell\nHarper, and Haiyi Zhu. Explaining decision-making al-\ngorithms through ui: Strategies to help non-expert stake-\nholders. In Proceedings of the 2019 CHI Conference on\nHuman Factors in Computing Systems, CHI ’19, pages\n559:1–559:12, New York, NY , USA, 2019. ACM.\n[Dickson and Nusair, 2010] Duncan R Dickson and Khal-\ndoon Nusair. An hr perspective: the global hunt for tal-\nent in the digital age. Worldwide Hospitality and Tourism\nThemes, 2(1):86–93, 2010.\n[Greca and Moreira, 2000] Ileana Maria Greca and\nMarco Antonio Moreira. Mental models, conceptual\nmodels, and modelling. International journal of science\neducation, 22(1):1–11, 2000.\n[Kulesza et al., 2012] Todd Kulesza, Simone Stumpf, Mar-\ngaret Burnett, and Irwin Kwan. Tell me more?: The effects\nof mental model soundness on personalizing an intelligent\nagent. In Proceedings of the SIGCHI Conference on Hu-\nman Factors in Computing Systems, CHI ’12, pages 1–10,\nNew York, NY , USA, 2012. ACM.\n[Kulesza et al., 2013] Todd Kulesza, Simone Stumpf, Mar-\ngaret Burnett, Sherry Yang, Irwin Kwan, and Weng-Keen\nWong. Too much, too little, or just right? ways explana-\ntions impact end users’ mental models. In2013 IEEE Sym-\nposium on Visual Languages and Human Centric Comput-\ning, pages 3–10. IEEE, 2013.\n[Kunkel et al., 2019] Johannes Kunkel, Tim Donkers, Lisa\nMichael, Catalin-Mihai Barbu, and J ¨urgen Ziegler. Let\nme explain: Impact of personal and impersonal explana-\ntions on trust in recommender systems. In Proceedings\nof the 2019 CHI Conference on Human Factors in Com-\nputing Systems, CHI ’19, pages 487:1–487:12, New York,\nNY , USA, 2019. ACM.\n[Mittelstadt et al., 2019] Brent Mittelstadt, Chris Russell,\nand Sandra Wachter. Explaining explanations in ai. In\nProceedings of the conference on fairness, accountability,\nand transparency, pages 279–288. ACM, 2019.\n[Nelson et al., 2004] Douglas L Nelson, Cathy L McEvoy,\nand Thomas A Schreiber. The university of south ﬂorida\nfree association, rhyme, and word fragment norms. Be-\nhavior Research Methods, Instruments, & Computers,\n36(3):402–407, 2004.\n[Norman, 2014] Donald A Norman. Some observations on\nmental models. In Mental models, pages 15–22. Psychol-\nogy Press, 2014.\n[Rovatsos et al., 2018] Michael Rovatsos, Dagmar Gro-\nmann, and G´abor Bella. The taboo challenge competition.\nAI Magazine, 39(1):84–87, 2018.\n[Speer et al., 2017] Robert Speer, Joshua Chin, and Cather-\nine Havasi. Conceptnet 5.5: An open multilingual graph\nof general knowledge. In Thirty-First AAAI Conference on\nArtiﬁcial Intelligence, 2017.\n[Wang et al., 2019] Danding Wang, Qian Yang, Ashraf Ab-\ndul, and Brian Y . Lim. Designing theory-driven user-\ncentric explainable ai. In Proceedings of the 2019 CHI\nConference on Human Factors in Computing Systems,\nCHI ’19, pages 601:1–601:15, New York, NY , USA, 2019.\nACM.\n[Wiegand et al., 2019] Gesa Wiegand, Matthias Schmid-\nmaier, Thomas Weber, Yuanting Liu, and Heinrich Huss-\nmann. I drive - you trust: Explaining driving behavior\nof autonomous cars. In Extended Abstracts of the 2019\nCHI Conference on Human Factors in Computing Sys-\ntems, CHI EA ’19, pages LBW0163:1–LBW0163:6, New\nYork, NY , USA, 2019. ACM.\n[Wittgenstein, 2009] Ludwig Wittgenstein. Philosophical\ninvestigations. John Wiley & Sons, 2009.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSister Conferences Best Papers Track\n4774",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7066581845283508
    },
    {
      "name": "Mental model",
      "score": 0.60328608751297
    },
    {
      "name": "Thematic analysis",
      "score": 0.5484857559204102
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5333141684532166
    },
    {
      "name": "Game theory",
      "score": 0.5071983337402344
    },
    {
      "name": "Thematic map",
      "score": 0.4398077428340912
    },
    {
      "name": "Conceptual model",
      "score": 0.4108169972896576
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36609452962875366
    },
    {
      "name": "Knowledge management",
      "score": 0.3411995470523834
    },
    {
      "name": "Psychology",
      "score": 0.23625406622886658
    },
    {
      "name": "Cognitive science",
      "score": 0.21938911080360413
    },
    {
      "name": "Qualitative research",
      "score": 0.12182486057281494
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Cartography",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    }
  ]
}