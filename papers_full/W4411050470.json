{
  "title": "A survey on the applications of transfer learning to enhance the performance of large language models in healthcare systems",
  "url": "https://openalex.org/W4411050470",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5117838446",
      "name": "Anmol Rahujo",
      "affiliations": [
        "Sindh Madressatul Islam University"
      ]
    },
    {
      "id": "https://openalex.org/A5117838447",
      "name": "Daniya Atif",
      "affiliations": [
        "Sindh Madressatul Islam University"
      ]
    },
    {
      "id": "https://openalex.org/A2912675033",
      "name": "Syed Azeem Inam",
      "affiliations": [
        "Sindh Madressatul Islam University"
      ]
    },
    {
      "id": "https://openalex.org/A2307313577",
      "name": "Abdullah Ayub Khan",
      "affiliations": [
        "Bahria University"
      ]
    },
    {
      "id": "https://openalex.org/A2181604170",
      "name": "Sajid Ullah",
      "affiliations": [
        "Nangarhar University"
      ]
    },
    {
      "id": "https://openalex.org/A5117838446",
      "name": "Anmol Rahujo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5117838447",
      "name": "Daniya Atif",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2912675033",
      "name": "Syed Azeem Inam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2307313577",
      "name": "Abdullah Ayub Khan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2181604170",
      "name": "Sajid Ullah",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4403534747",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4385620388",
    "https://openalex.org/W4394892516",
    "https://openalex.org/W4388459526",
    "https://openalex.org/W4389428089",
    "https://openalex.org/W4205942412",
    "https://openalex.org/W4319988655",
    "https://openalex.org/W4406322412",
    "https://openalex.org/W4404294653",
    "https://openalex.org/W4404469281",
    "https://openalex.org/W4402125996",
    "https://openalex.org/W4401568490",
    "https://openalex.org/W4403446583",
    "https://openalex.org/W4390494339",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4391855109",
    "https://openalex.org/W3081380767",
    "https://openalex.org/W4387226001",
    "https://openalex.org/W4398141723",
    "https://openalex.org/W4385227045",
    "https://openalex.org/W4396978679",
    "https://openalex.org/W4380486177",
    "https://openalex.org/W4409125019",
    "https://openalex.org/W4387911035",
    "https://openalex.org/W4323314291",
    "https://openalex.org/W4404011942",
    "https://openalex.org/W4400730982",
    "https://openalex.org/W4387745472",
    "https://openalex.org/W4408145162",
    "https://openalex.org/W4406614817",
    "https://openalex.org/W4391136399",
    "https://openalex.org/W4406832705",
    "https://openalex.org/W3197643323",
    "https://openalex.org/W4405232513",
    "https://openalex.org/W4388041760",
    "https://openalex.org/W4396919238",
    "https://openalex.org/W4404494409",
    "https://openalex.org/W4401352481",
    "https://openalex.org/W4403880728",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4386328490",
    "https://openalex.org/W4391226146",
    "https://openalex.org/W4404635423",
    "https://openalex.org/W4290716979",
    "https://openalex.org/W3037095325",
    "https://openalex.org/W4307135222",
    "https://openalex.org/W3201753566",
    "https://openalex.org/W4398151407",
    "https://openalex.org/W2134295053",
    "https://openalex.org/W4402268246",
    "https://openalex.org/W4392521978",
    "https://openalex.org/W4385337322",
    "https://openalex.org/W3089168069",
    "https://openalex.org/W4403817516",
    "https://openalex.org/W3182221256",
    "https://openalex.org/W4409808980",
    "https://openalex.org/W4404826238",
    "https://openalex.org/W2979459011",
    "https://openalex.org/W4382656719",
    "https://openalex.org/W4405516066",
    "https://openalex.org/W4404645346",
    "https://openalex.org/W6602740540",
    "https://openalex.org/W4402772539",
    "https://openalex.org/W4410507588",
    "https://openalex.org/W4404645296",
    "https://openalex.org/W4405241694"
  ],
  "abstract": "Abstract The healthcare field experiences significant developments through transfer learning and large language models that boost medical diagnosis accuracy while improving patient services and clinical process automation. This survey investigates the significant impact of Transfer Learning and large language models on medical systems by explaining their applications in imaging procedures, disease identification, and natural language processing functions for electronic health records analysis and medical decision-making assistance. Pre-trained models employed through TL solve the problems caused by scarce labeled datasets, so systems perform effectively despite low data availability. This research analyzes different transfer learning methods, including inductive, transductive, and unsupervised techniques, while demonstrating their effectiveness in detecting COVID-19 from chest X-rays and multi-source disease evaluation. The remarkable progress of transfer learning cannot overcome crucial obstacles involving data protection vulnerabilities, interpretability issues, and unfavorable knowledge transfer scenarios. The study presents avenues for future investigation, including domain-specific training approaches and privacy-preserving federated systems with reduced processing needs. This study demonstrates effective healthcare solutions based on TL and LLMs but urges researchers to work across disciplines to resolve technical and ethical limitations.",
  "full_text": "Vol.:(0123456789)\n Discover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0\nDiscover Artificial Intelligence\nReview\nA survey on the applications of transfer learning to enhance \nthe performance of large language models in healthcare systems\nAnmol Rahujo1 · Daniya Atif1 · Syed Azeem Inam1 · Abdullah Ayub Khan2 · Sajid Ullah3\nReceived: 27 February 2025 / Accepted: 22 May 2025\n© The Author(s) 2025  OPEN\nAbstract\nThe healthcare field experiences significant developments through transfer learning and large language models that \nboost medical diagnosis accuracy while improving patient services and clinical process automation. This survey inves-\ntigates the significant impact of Transfer Learning and large language models on medical systems by explaining their \napplications in imaging procedures, disease identification, and natural language processing functions for electronic \nhealth records analysis and medical decision-making assistance. Pre-trained models employed through TL solve the \nproblems caused by scarce labeled datasets, so systems perform effectively despite low data availability. This research \nanalyzes different transfer learning methods, including inductive, transductive, and unsupervised techniques, while \ndemonstrating their effectiveness in detecting COVID-19 from chest X-rays and multi-source disease evaluation. The \nremarkable progress of transfer learning cannot overcome crucial obstacles involving data protection vulnerabilities, \ninterpretability issues, and unfavorable knowledge transfer scenarios. The study presents avenues for future investiga-\ntion, including domain-specific training approaches and privacy-preserving federated systems with reduced processing \nneeds. This study demonstrates effective healthcare solutions based on TL and LLMs but urges researchers to work across \ndisciplines to resolve technical and ethical limitations.\nKeywords Transfer learning · Healthcare system · Large language models · Natural language processing · Medical \nassistance\n1 Introduction\nTransfer learning (TL) and large language models (LLMs) are improving healthcare significantly by making better diagno-\nses and running patient care smoothly. Modern healthcare data expands exponentially because traditional data manage-\nment systems require new platforms to handle complexities in contemporary medical datasets, as manual classification \nand rule-based approaches fail to bridge such gaps. The need to address inefficient clinical data processing prompted \nhealthcare institutions to implement advanced Natural Language Processing (NLP) methods, primarily through TL and \nLLMs, to analyze structured and unstructured clinical information [1 ]. TL uses prior model expertise to handle target \nproblems with restricted data annotations, making the training process more economical than starting from a blank \n * Abdullah Ayub Khan, abdullah.khan00763@gmail.com; abdullahayub.bukc@bahria.edu.pk;  * Sajid Ullah, sajidjalwan@\ngmail.com; Anmol Rahujo, anmolrahujoai@gmail.com; Daniya Atif, daniya.atif2001@gmail.com; Syed Azeem Inam, syed.azeem@\nsmiu.edu.pk | 1Department of Artificial Intelligence and Mathematical Sciences, Sindh Madressatul Islam University, Karachi 74000, \nPakistan. 2Department of Computer Science, Bahria University Karachi Campus, Karachi 75260, Pakistan. 3Department of Water Resources \nand Environmental Engineering, Nangarhar University, Jalalabad 2600, Nangarhar, Afghanistan.\nVol:.(1234567890)\nReview  \nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0\nmodel [1]. BERT and BART, and T5 have shown exceptional results in healthcare-specific applications such as clinical text \nclassification, disease prediction, and sentiment analysis [2 ]. The deployment of such models faces three main barriers: \nlimited dataset availability, which matches clinical domain terminology, expensive computational requirements, and their \nrestricted ability to work across different medical settings [1 ]. The recent progress in LLMs demonstrated by OpenAI’s \nChatGPT helps overcome these difficulties through multiform data-based pre-training, thus enhancing their capability \nfor healthcare work with minimal required fine-tuning [3 ]. The models demonstrate high effectiveness in working with \nclinical texts, extracting valuable information from electronic health records, and their diagnostic support capabilities [4]. \nMedical imaging faces ongoing challenges because the lack of proper annotations hinders the development of robust \nmodels [5, 6]. The Multistage TL framework tackles this issue by transferring data knowledge from source domains, which \nhave abundant information, to target domains that require a reduction in dependency on large datasets [7 ].\nThe present study investigates the healthcare synergy between TL and LLMs by analyzing their clinical applications \nwith a focus on obstacles and moral issues. The study utilizes multimodal fusion with domain adaptation methodolo -\ngies to show how scientists can tackle issues with heterogeneous data, processing speed, and interpretation limitations.\n1.1  Novel contribution\nThe survey establishes three essential contributions to understanding transfer learning and large language models in \nhealthcare. The research introduces a thorough medical-oriented classification system for transfer learning approaches, \nwhich includes methods for medical imaging such as CNN fine-tuning with ResNet for disease recognition and clinical \nnatural language processing using BioBERT adaptations for EHR examination, as well as tabular data solutions, hybrid \nLLM-ML predictive analytics systems. The assessment investigates dangerous transfer learning effects in healthcare by \nanalyzing domain mismatches from general pre-trained features alongside rare disease label scarcity and ethical biases, \nfollowed by plans to control such problems with adversarial validation and federated learning methods. The research \nevaluates novel domain adaptation techniques consisting of attention-based fusion through ABCM for multimodal \nhealthcare data integration and multi-step transfer learning, from PubMed pre-training to ICU fine-tuning and self-\nsupervised learning applied to unlabeled clinical documentation. The research presents a standardized framework for \nchoosing TL methods, together with solutions that enhance medical AI deployment robustness and solve challenges of \nblended healthcare data integration to improve diagnostic applications of AI. The review would proceed after this section \nto establish the groundwork for the extensive literary review, coupled with practical applications included afterward.\n1.2  Criteria for study selection\nThe analysis reviews peer-reviewed research between 2018 and 2024 about TL and LLM use in medical facilities to diag-\nnose conditions, interpret EHR files, and determine illness risks. The research selection criteria included using empirical \nevidence through performance assessments and theoretical solutions that address healthcare’s unique circumstances. \nBioBERT, Clinical BERT, and non-specific models like GPT and BERT were preferred for medical applications. The research \nstudy excluded non-peer-reviewed publications, including preprints or white papers, non-artificial intelligence systems, \nsuch as rule-based systems or traditional statistical methods, and non-clinical works from consideration to concentrate \non healthcare AI developments. The research analyzed excluded studies lacking appropriate methodologies and evalu-\nation benchmarking standards to maintain rigor and reproduction capabilities. A methodical selection system produces \na deep research understanding of how TL and LLMs affect medical practitioner operations. The sections ahead present \nan extensive review of modern approaches alongside quantitative assessments and prospective deployment paths for \nclinical implementation of TL and LLMs.\n2  Background\nTransfer Learning (TL) is a machine learning technique that boosts target domain model capability through source \ndomain learning, even under low availability of target domain data [8 ]. The approach delivers significant benefits to \nhealthcare settings because healthcare usually faces problems with limited labeled dataset availability and intense \nrequirements for building new models. Applying pre-trained models through TL allows adaptation to new tasks, which \nrequires fewer enormous labeled datasets and provides better generalization outcomes [9]. BERT GPT and T5, along with \nother models created through general text training, successfully achieved healthcare applications through fine-tuning \nVol.:(0123456789)\nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0 \n \n Review\nfor tasks like clinical text classification, disease prediction, and sentiment analysis [10, 11]. Robust deployment encoun-\nters two main limitations: negative transfer uses unnecessary source information to degrade target performance, while \ndomain shift occurs due to differing source and target distributions [12, 13]. The practical value of TL goes further than \nNLP in that it includes data in tabular form, which often occurs in healthcare cases. XGBoost and traditional methods \ndo not possess effective transfer learning mechanisms. TabuLa-8B is a hybrid model that employs Llama 3-8B architec -\nture for a tabular prediction task. The model received TabLib corpus pre-training (2.1 billion table rows from 4 million \ntables). Through zero-shot and few-shot problem scenarios, TabuLa-8B shows superiority over base methods, reaching \n15% better accuracy levels [14]. This result confirms how transfer learning can handle complex tabular data challenges.\nDeep transfer learning improves TL methods using hierarchical feature reuse methods and domain-invariant repre -\nsentation learning approaches [15]. Transformers used in NLP brought BERT and GPT to deliver self-attention processing, \nwhich allows parallel modeling of long-term relationships in text [16]. The models originated from the natural language \nprocessing domain and are used in biomedical applications but require solutions to process clinical text and over -\ncome medical vocabulary complexity and diverse data representation standards [17]. Healthcare NLP operates under \ntwo main obstacles: rigorous privacy laws, including HIPAA and GDPR, and data anonymization requirements. Patient \ninformation protection in electronic health records (EHRs) depends on Named Entity Recognition (NER) and rule-based \nde-identification techniques, but these prevent important clinical features [18, 19]. Recent research shows that BERT \nwith ELMo embedding methods successfully analyzes verbal autopsy narratives to classify causes of death by applying \npre-trained medical knowledge [20].\nProgression in healthcare data management methods has brought challenges from spread-out medical information, \nminor dataset sizes, and ethical boundary factors. Healthcare systems now use multimodal integrated systems that \nmerge EHRs with genomic data because they employ federated learning frameworks that protect privacy standards [8]. \nModern research requires solving interpretability challenges while ensuring data compliance to develop equitable AI-\nbased healthcare solutions because of training data biases in the system [12]. The details of the LLM architectures that \nare implementable in healthcare applications are shown in Table 1.\nThe transfer learning applications for the healthcare field differ fundamentally from those used in manufacturing \nindustries [22–24]. The analysis methods in industrial fault diagnosis utilize transfer learning to solve problems of equip-\nment variability and noise robustness by processing large-scale sensor data that requires limited privacy precautions \n[25–27]. The application of healthcare Transfer Learning reduces its flexibility due to HIPAA/GDPR data anonymization \nrequirements while working with small domain shifts that include hospital imaging protocol differences and regional \ndisease patterns [28, 29]. Despite its homogeneous set of data inputs, including vibration signals from machinery \nequipment, industrial TL benefits. In contrast, healthcare encounters severe multimodal heterogeneity (including EHRs, \ngenomic data, and medical images) and insufficient labels for diagnosing rare medical conditions. The ethical implica-\ntions of misdiagnosis in healthcare settings increase adverse transfer risks, so healthcare professionals use adversarial \nvalidation and federated learning methods to reduce bias [30– 32]. Medical organizations require specialized transfer \nlearning solutions that prioritize protecting patient information while being interpretable across various healthcare \nfacilities [33, 34].\nTable 1  LLM architectures for \nhealthcare applications [21] Model Unique features Commonly used datasets\nBio\nBERT\nPre-trained on Biomedical Text PubMed Abstracts, PubMed Central\nSCI\nBERT\nTrained in Biomedical Semantic Scholar\nPubMed\nBERT\nTrained Entirely on PubMed PubMed\nClinical\nBERT\nFine-tuned Clinical Notes MIMIC-III\nGatorTron Largest Clinical LLM, Trained on Mixed Sources Clinical Notes, PubMed, Wikipedia\nMed\nPaLM\nOptimized for Medical Q&A USMLE Questions\nChat\nDoctor\nFine-tuned on LLaMA with Medical Dialogues 100 K Patient-physician Conversations\nBaize Healthcare LLaMA-based Chatbot MedQuAD (46,867 Medical Dialogues)\nVol:.(1234567890)\nReview  \nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0\n3  TL and LLM in healthcare\n3.1  Cardiovascular disease prediction via cross‑modal transfer learning\nThe attention-based cross-modal (ABCM) transfer learning framework overcomes crucial barriers in cardiovascular disease \n(CVD) prediction through its capability to unite medical data, such as records and images, alongside genetic information. \nTraditional approaches encounter two primary problems while handling incomplete data and combining multiple data \ntypes. The attention mechanism within ABCM identifies important features between modalities, which produces nota-\nble evaluation results of 93.5% accuracy, 92.0% precision, and 97.2% AUC. This research proves the value of integrating \nattention mechanisms with cross-modal transfer learning to improve both explainable predictions and accuracy rates, \nwhich creates improved patient safety mechanisms for clinical applications [35].\n3.2  Classification of respiratory diseases using transfer learning\nSinusitisBox uses pre-trained architectures like EfficientNetB6 and ResNet101v2 to analyze medical imaging data of CT \nand chest X-rays with exceptional results. Through these models, clinicians obtain exact diagnoses for lung cancer and \npulmonary embolism, as well as COVID-19 and pneumoconiosis. A model that combines EfficientNetB6 with ResNet101v2 \nreached 99.77% accuracy and 1.00 precision, indicating the effectiveness of transfer learning for clinical imaging tasks. \nDiagnostic efficiency has improved through TL because it enables timely interventions while delivering optimized patient \nresults [36].\n3.3  AI‑driven healthcare administration platforms\nAI-powered platforms combining NLP and ML capabilities enable healthcare administrators to optimize administra-\ntive processes. The RASA Framework and external APIs, together with another similar technology platform, enable the \nautomation of appointment scheduling, patient triage, and personalized care delivery methods. The systems decrease \nadministrative work and provide better availability of services while strengthening patient-health-provider interaction \nquality. Talk2Care demonstrates innovation through its LLM-based interaction system by uniting voice assistants for \nsenior citizens with provider dashboards for asynchronous communication abilities. The system uses AI technology \nto eliminate inefficient healthcare processes while streamlining data collection, highlighting AI’s incredible impact in \ntoday’s medical environments [37].\n3.4  Automation of clinical workflow and EHR processing\nTransferring knowledge enables superior EHR processing through domain-specific adaptations of pre-trained models \nBERT and LSTM. These models deliver medical text summarization, clinical coding, and annotation functions that require \nminimal, extensive labeled data. TL applies general-purpose model knowledge to clinical duties to enhance entity detec-\ntion while ensuring accurate billing and enabling risk management systems. The combination of convolutional neural \nnetworks (CNNs) and recurrent neural networks (RNNs) within collaborative systems facilitates the complete analysis of \npatient data between medical images and sequential healthcare records, which enhances healthcare operations and \nleads to data-dependent medicine [38].\n3.5  Prediction of disease outbreak using TL and NLP\nTL operates together with NLP to detect disease outbreaks through the analysis of unstructured textual information \npresent in social media content and online news articles. Using pre-trained models dedicated to epidemiological work \nenables the effective processing of epidemiological data despite limited labeled information regarding newly emerging \ndiseases. The application of disease prediction systems faces limitations because human experts demonstrate a prefer -\nence for prevalent diseases. Systems that predict disease outbreaks can achieve better robustness when researchers use \na multi-step transfer learning method that connects domain-specific information to general language understanding. \nScientists require additional research to build responsive models dealing with uncommon or new pathogens [39].\nVol.:(0123456789)\nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0 \n \n Review\n3.6  LLM enabled personalized medicine\nTreatment recommendations generated by Large Language Models (LLMs) depend upon patient histories, clinical evi-\ndence, and individual preferences from vast medical textual data. Medical LLMs serve dual applications for developing \npatient education content and integrating and preparing complicated medical information. Implementing Large Lan-\nguage Models marks a new era for personalized medicine despite the difficulties in ensuring exactness, resolving ethical \nissues, and making content more readable. AI-generated insights linked to person-specific care plans can improve thera-\npeutic results and patient satisfaction [40]. Table 2 presents the diagnostics tasks that can be handled using TL and LLM.\n4  TL and NLP methodologies and techniques for healthcare\n4.1  NLP in healthcare functions through preprocessing actions that manage clinical data\nHealthcare NLP applications need detailed preprocessing systems to handle the various complexities of medical data. \nHealthcare NLP systems begin with de-identification as an essential initial step that requires either removing or anonymiz-\ning Protected Health Information (PHI) elements, including patient identifiers and names, and addresses. NER and rule-\nbased systems work together to meet privacy requirements set by HIPAA and GDPR [41]. After de-identification, data \ncleaning operations resolve the natural irregularities in clinical text material, including spelling mistakes, abbreviations, \nand unnormalized written patterns. This process involves: The text normalization process eliminates duplicates, num -\nbers, and punctuation while converting everything to lowercase characters. The program divides text into meaningful \nunits representing words or smaller subunits. The standardization step uses stemming and lemmatization to normalize \nalternative medical terminology (tumor against tumor) variants. Data scarcity is addressed by employing data augmenta-\ntion methods consisting of back-translation and synonym replacement with context-aware synthesis to boost dataset \ndiversity and (class distribution balance). The statistical patterns in medical texts are extracted through both TF-IDF and \nclass probability (CP) methods, but word embeddings (e.g., Word2Vec, FastText) use vector spaces to maintain medical \nterm connections [41]. Model generalization and ethical-legal standards receive support through these preprocessing \nprocedures, which are especially crucial for automated diagnosis and clinical decision support applications [41].\n4.2  Transfer learning for medical image classification\nTL solves the problem of small medical dataset volumes by implementing trained pre-models on clinical image tasks. \nThe researchers utilize VGG16 and VGG19 models initially trained on ImageNet images to apply their fine-tuning pro -\ncess with smaller dermatological datasets for skin cancer classification tasks. Research has proven that making essential \nmodifications to the architecture structure, including changes to final layers and hyperparameter optimization, leads to \nsignificant improvements in diagnostic performance while showing meaningful results for identifying COVID-19 from \nCT imaging [42]. The approach reduces computational expenses and enables the use of TL as an essential resource for \nhealthcare facilities that lack adequate data sets.\n4.3  Human activity recognition (HAR) in domain adaptation\nThe deployment of generic HAR models in healthcare requires domain adaptation for proper functioning in personalized \nmedical environments. A significant limitation of traditional models occurs when they must handle between-subject \nvariation, since it leads to degraded accuracy. The process of auto-supervised learning stands out by applying particle \nfilters to create user-specific data labels, eliminating the need for manual annotation. The accuracy of fall detection and \nphysiotherapy monitoring devices with these methods improves by up to 50%, which ensures dependable operation \nin elderly care settings [43].\nVol:.(1234567890)\nReview  \nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0\nTable 2  Diagnostic tasks using LLM and TL [36]\nDiagnostic task Methods Results\nLung cancer detection Combination of deep learning architectures such as 3D multi-path VGG \nnetworks, U-Net architecture, and multi-phase CNN for classifying lung \nmodules and determining malignancy levels. AI-based models assist \nradiologists in screening and staging lung cancer\nAssist in classifying lung modules and determining malignancy levels\nPneumoconiosis diagnosis Deep learning models, including segmentation and staging procedures, \nclassify lung regions into four stages using focal staging loss and deep \nlog-normal label distribution learning. AI-based screening systems \nusing preprocessing pipelines and ResNet models\nClassified lung regions into four stages, enhancing diagnosis with AI-\nbased screening systems\nPulmonary embolism (PE) detection A hybrid approach integrating hyper-network theory with supervised \nartificial neural networks to reduce the number of CT-angiography \nanalyses. CNN-based model and hybrid 3D/2D UNet topology trained \non 387 anonymized real-world chest CTAs\nAchieved 91.4% sensitivity and 91.5% specificity for PE detection\nCOVID-19 detection Efficient Net and MixNet architectures applied to chest X-ray (CXR) and \nlung CT images. Deep learning models such as MobileNetV2 and \nSqueezeNet, combined with support vector machines for COVID-19 \nclassification\nAchieved over 95% accuracy across multiple datasets for COVID-19 detec-\ntion\nHybrid deep transfer learning \nmodel for respiratory disease clas-\nsification\nA hybrid model combining EfficientNetB6 and ResNet101V2, achieving a \nclassification accuracy of 99.77% with the lowest loss value of 0.001\nAchieved 99.77% classification accuracy with the lowest loss value of \n0.001\nVol.:(0123456789)\nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0 \n \n Review\n4.4  Zero‑shot vs. few‑shot learning in medical LLMs\nLarge language models (LLMs) struggle with clinical reasoning because they encounter ambiguous situations and spe -\ncialized medical complexities. Using Zero-Shot Learning without task-specific training, the diagnostic process usually \nproduces poor results in complex diagnostic situations. The Few-Shot Learning system offers restricted instance input \nto LLMs; thus, it results in better performance and context-matching accuracy. Applying FSL models leads to a better \nunderstanding of unusual medical diseases and new pathogens by utilizing previous cases with similar characteristics \n[44]. Both methods need comprehensive testing to prove their value in medical practice.\n4.5  Evaluation metrics for clinical NLP models\nHealthcare NLP systems depend on several dependable evaluation metrics to achieve reliability in their operations. \nAccuracy: overall correctness of predictions. The precision/recall parameter controls how system administrators manage \nmisinformation alongside misidentifications. F1-score combines precision with recall because it is essential to evaluate \nunbalanced datasets. ROC-AUC: measures class separability, essential for diagnostic models. The evaluation metrics \nprove system reliability when used in demanding applications such as lung nodule identification and disease outbreak \nprediction [44].\n4.6  Challenges and ethical considerations\nThree main difficulties arise from using small datasets to train models, maintaining interpretability in these models, and \nprotecting private data. Solutions include XAI, which enables tools such as SHAP and LIME to generate explanations after \nmodel decisions to enhance medical professionals’ faith in AI systems. The FL methodology provides healthcare institu-\ntions with a mechanism to train algorithms together while keeping patient records separate from each other to protect \nidentity privacy. Adequate AI frameworks should conduct fairness assessments and establish bias reduction strategies \nto conform to ethical regulations such as GDPR [45].\nThe merge of healthcare and NLP with transfer learning requires detailed data preparation, matching, and strong \nethical rules. New learning methods improve medical diagnoses, but the problems of different medical data types and \nhard-to-understand AI remain. Scientific teams should research ways to develop and test AI healthcare systems that \noperate through multiple platforms with minimal hardware needs and proven evaluation standards. Table  3 compares \nthe pre-trained model and datasets, evaluation metrics, and application area.\n5  Transfer learning pipelines\nWhen medical institutions combine transfer learning models with large language models, they create a step-by-step \napproach to using AI technology in healthcare settings. The proposed process contains six repeated steps that enhance \nhealthcare prediction abilities, as shown in Figs. 1 and 2. This part presents all system stages with evidence and theoreti-\ncal knowledge from empirics and theory.\nTable 3  Comparison of Transfer Learning Techniques for Healthcare Applications [46]\nTechnique Pretrained model Dataset used Accuracy (%) F1-Score (%) AUC (%) Application area\nFine-tuning ResNet-50 Chest X-ray (Pneumonia) 92.5 91 94 Medical imaging\nFeature extraction VGG-16 Skin Lesion (ISIC) 89.3 88 92 Dermatology\nDomain adaptation InceptionV3 Brain MRI (Tumor) 91.2 90 93 Neurology\nSelf-supervised learning SimCLR Histopathology Images 87.6 85 89 Pathology\nFew-shot learning Prototypical Networks Retinal Fundus (Diabetic \nRetinopathy)\n90.1 89 91 Ophthalmology\nVol:.(1234567890)\nReview  \nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0\n5.1  Data acquisition and cleaning\nThe system begins with data sources, including different types of healthcare information, such as electronic health \nrecords (EHRs), medical images, genetic data, and doctors’ textual notes. The initial stage of raw healthcare data \nincludes significant errors caused by how hospitals run their procedures and local treatment methods. Our methods \nFig. 1  Transfer learning pipeline in healthcare LLM applications\nFig. 2  Data flow diagram\n\nVol.:(0123456789)\nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0 \n \n Review\nof cleaning and normalizing include removing outliers and replacing gaps using statistical rules to make all data \nmatch uniformly. Medical imaging contrast needs improving, so the team uses histogram equalization while EHR \ndata meets privacy rules through NER, which takes action against GDPR/PIPA.\n5.2  Preprocessing and feature engineering\nEnter the model system before processing data. To work with clinical text, we break it into smaller parts known as tokens \nbefore selecting key medical data points. Techniques like TF-IDF and biomedical word embeddings, e.g., BioWordVec, \nencode semantic relationships in medical terminologies. TabuLa-8B transfers learned knowledge to tabular data while \nprocessing patient vital signs to help reduce dimensions using hybrid encoding methods.\n5.3  Model training via transfer learning\nThe LLM training stage fine-tunes pre-trained models, e.g., BioBERT and GatorTron, on healthcare-specific datasets. The \nsystem uses transfer learning to reduce data shortage by reusing feature patterns extracted from the original PubMed \nabstract material. With its training in medical data, BioBERT shows better results in medical note classification tests when \nupdated with the MIMIC-III database.\n5.4  Deployment for clinical applications\nAfter development, models enter service for medical purposes to support diagnostic work, tailor treatment, and run \nadministrative operations. A tool named ABCM uses clinical databases and medical pictures to forecast cardiovascular \ndiseases successfully at a 93.5% rate. Combining EfficientNetB6-ResNet101V2 enables 99.77% accurate respiratory disease \ndetection from chest X-ray images in a hybrid architecture. Med-PaLM makes healthcare communication more efficient \nwith NLP-based chatbots, but users need protection against possible ethical issues.\n5.5  Performance analysis and evaluation\nPhase 1 uses accuracy, precision, F1-score, and AUC-ROC metric results to evaluate how well the model performs. In \nthis study, the researcher found that domain adaptation with InceptionV3 functioned at 91.2% accuracy when used to \ndetect brain MRI tumors. The problem of dataset size mismatch during COVID-19 X-ray analysis calls for stratified cross-\nvalidation to address it.\n5.6  Iterative refinement and optimization\nOngoing feedback cycles let users assess performance and refine the process in step four of the method. Validation \nmethods based on adversaries and learning schemes across multiple sites help with domain changes and data privacy \nchallenges. Federated training distributes hospital operations across multiple sites to build models that generalize better \nbeyond each facility without exposing patient data. By using XAI tools like SHAP and LIME, clinicians gain better trust \nbecause these tools help explain AI model behavior.\nEven though the process holds promise, it deals with significant problems such as expensive training requirements, \nhard-to-understand outcomes, and failed outcomes when used between unrelated data sets. Students of AI research \nneed to develop domain generalization methods while creating private synthetic data and establishing tested and reli-\nable datasets for others to reproduce results. When AI researchers work with doctors and health policy professionals, \nthey can adequately use their research results to benefit a broader range of patients.\n6  Comparative analysis of transfer learning applications in healthcare\nTransfer learning helps healthcare professionals improve diagnostic accuracy and working speed despite a lack of data. \nIn this study, the researchers have shown the summary of what TL has achieved, how researchers can better utilize it, \nand what remains difficult about using TL in healthcare.\nVol:.(1234567890)\nReview  \nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0\n6.1  Methodological advancements and empirical outcomes\nThe study by Hosna et al. [ 47] generates an extensive overview of TL methods that labels them as inductive, transduc -\ntive, and unsupervised learning domains essential for healthcare applications under restricted dataset scenarios [27]. \nThe proposed framework demonstrates how TL achieves task generalization by reorganizing hierarchical features \nobtained from source domains such as ImageNet and PubMed. The analysis advises health providers to watch for \nnegative transfer since it can happen when unrelated source domain knowledge damages target task performance, \nprimarily because institutions operate with different domains [27]. The research of Zhao et al. [48] improved the appli-\ncation range of TL using Unsupervised Deep Transfer Learning (UDTL), which started in industrial fault diagnosis yet \nadapted to healthcare practices [28]. The UDTL method achieves clinical promise in rare disease diagnosis because it \nmatches distribution patterns of features between domains without needing labeled data. The Unsupervised Deep \nTransfer Learning method enhanced cross-institutional MRI detection of tumors through improved accuracy by 12%, \nand its evaluation framework limitations restricted reproducibility according to the literature [28]. TL was essential in \ndeveloping quick COVID-19 detection systems during pandemic emergency responses. TL was essential in develop -\ning quick COVID-19 detection systems during pandemic emergency responses. It applied with VGG16, which had \nbeen pre-trained on natural images with convolutional neural networks (CNNs) joined with support vector machines \n(SVMs) to perform chest X-ray classification with an accuracy rate of 95.82% [ 29]. The adaptability of TL became \nevident since VGG16 originated from non-medical sources, yet its restricted COVID-19 fine-tuning allowed it to be \ngeneralized effectively without needing massive annotated resources [29]. Combining EfficientNetB6-ResNet101V2 \narchitecture demonstrated 99.77% accuracy in respiratory disease multi-class identification tasks within clinical \nimaging applications [30].\nHowever, establishing TL systems requires overcoming numerous deployment barriers, even though it addresses \ndata scarcity issues. Performance degradation happens from Negative Transfer and Domain Shifts when feature \ndistributions inside the source domain (usually urban hospital imaging protocols) do not match the target domains \n(typically rural clinics). According to research, the proposed adversarial validation and federated learning solutions \nneed large quantities of target-domain labels, but such information is usually unavailable [27]. Training large models, \nincluding GPT-4 and BioBERT, requires expensive high-performance GPUs/TPUs, making these resources inaccessible \nfor institutions with limited funding, according to research [33]. The opaque nature of deep learning models creates \ndifficulties with clinician trust, even when using SHAP and LIME post-hoc interpretability tools [45]. Federated learn-\ning maintains patient privacy, but institutions encounter organizational hurdles for collaborative work under HIPAA/\nGDPR privacy rules [52]. The time regulatory bodies need to approve successive updates of AI software obstructs \nits practical implementation in clinical settings [28]. Table  4, shows the key findings, contradictions, and challenges \ncorresponding to various themes for applying transfer learning in healthcare.\n7  Challenges and open research questions in healthcare applications of LLMs and transfer \nlearning\n7.1  Data privacy and ethical considerations\nThe combination of healthcare with large language models (LLMs) and transfer learning (TL) systems raises essential \nprivacy issues regarding data protection and ethical administration problems. Electronic health records and medical \nimaging with genomic data form healthcare datasets that require strong protective measures because they hold \nsensitive patient information [31]. The requirement for adequate protection involves implementing strict encryp -\ntion standards, role-based access rules, and auditing tools that fulfill HIPAA and GDPR requirements [52 ]. When \ndatasets undergo de-identification or anonymization processes for privacy protection, clinically important features \nare removed, making the data unsuitable for training medical models. The ethical space surrounding medical data \nextends concerns from information security practices to the developing issues discovered in biased system algo -\nrithms. The diagnostic disparities could continue when training data contains imbalanced numbers of underrepre -\nsented populations [ 31]. Medical models trained using urban hospital data produce inferior results when deployed to \nrural areas because of demographic variation and different protocol requirements, which leads to worsening health \nVol.:(0123456789)\nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0 \n \n Review\nTable 4  Comparative analysis of transfer learning applications in healthcare\nTheme Key findings Contradictions/challenges\nData heterogeneity TL mitigates data scarcity by leveraging pre-trained models Domain shift between source and target data reduces model generalizability\nDiagnostic accuracy High accuracy (> 90%) in imaging tasks Overfitting risks in small datasets; trade-offs between fine-tuning and computational costs\nMultimodal fusion Cross-modal TL improves CVD prediction Integration challenges with unstructured EHR data and genomic/lifestyle factors\nEthical & privacy Federated learning addresses data privacy but requires decentral-\nized infrastructure\nBias in training data may perpetuate disparities in diagnosis/treatment\nInterpretability Explainable AI (XAI) tools enhance clinician trust in LLM outputs Black-box nature persists; explanations are often too technical for non-experts\nNegative transfer UDTL shows promise for unlabeled medical data Misalignment between source/task domains harms performance\nVol:.(1234567890)\nReview  \nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0\ndisparities. According to sources, alarm systems composed of bias audits and adversarial validation frameworks \nmust be performed regularly to reduce these potential risks. Furthermore, public trust hinges on transparent data \npractices. The growing number of people surveyed shows an increasing need for patient-centric data governance, \nwhich gives patients total control over their medical information [31]. Ethical regulations need to create a perfect \nbalance between technical security protocols that should align with public expectations by implementing fairness-\nbased accountability throughout the AI development cycle [52].\n7.2  Patient data privacy and security in distributed systems\nPoorly available medical data and privacy restrictions create severe training problems for ML models [49– 55]. The pre-\ndictive models in distributed healthcare systems must process fragmented datasets spread across different institutions \nbecause this raises concerns about data protection and security breaches [ 56, 57]. FL applications enable distributed \nmodel development between multiple parties while maintaining patient privacy by enabling distributed information \nstorage [25, 34]. Data autonomy through administration domain boundaries represents one of the features found in FL \nframeworks, which aligns with the requirements of data minimization principles [56]. FL implementations encounter \ntechnical and organizational barriers since they present interoperability issues and high computational loads [52].\n7.3  Interpretability and explainability of LLMs\nThe hidden workings of large language models present significant challenges for doctors to use in practice. The tools \nSHAP and LIME produce feature importance maps, yet these results are challenging for specialized and non-specialized \nmedical users to translate into practical applications [32]. AI diagnostic recommendations must show detailed explana-\ntions that medical professionals can use and justify in life-or-death decisions, especially for cancer patients [19]. New \nhybrid systems of LLMs with symbolic reasoners hold potential but need supporting evidence from research that proves \ntheir value [58]. AI model transparency needs ethical review when researchers deny trial participant incentives without \ndefined standards, because this practice can cause unfair distribution [59].\n7.4  Generalization and domain‑specific fine‑tuning\nMedical LLMs experience difficulties in domain adaptation due to terminology differences and diverse medical practices \nand diagnostic procedures [31]. Specialized task models require training with many examples of labeled data, but such \nsamples are frequently missing from rare disease databases. The problem gets resolved through unsupervised deep \ntransfer learning (UDTL) alongside meta-learning that extracts information from untagged data and similar domains \n[28]. UDTL enabled a 12% improvement in the detection of MRI tumors between different medical institutions through \nan unlabeled data alignment process [28]. Negative transfer continues to pose difficulties in domain adaptation because \nirrelevant source domain knowledge leads to reduced performance, thus requiring adversarial validation to detect \ndomain mismatches. By applying reinforcement learning from human feedback (RLHF), scientists can develop improved \nLLMs dedicated to clinical work. Supervised fine-tuning is the first step, followed by reward modeling to optimize models \naccording to healthcare provider preferences, which decreases errors and biases [ 60]. RLHF requires extensive, high-\nquality human feedback as input, but this requirement limits its scalability when resources are scarce.\n7.5  Scalability and computational requirements\nHealthcare institutions cannot afford expensive high-performance GPUs/TPUs with LLMs such as GPT-4 or BioBERT \ndue to their high resource demands [33]. PEFT and quantization techniques lower memory and energy use, yet their \nimplementation leads to degraded model accuracy [ 33]. Executing data at the device level through edge computing \nframeworks decreases the response time in demanding real-time situations, including robotic surgery and emergency \ncare [61]. The challenge exists in perfectly balancing edge and centralized computing systems, mainly when workloads \nin dynamic environments show frequent changes [62]. Hierarchical mobile edge computing (MEC) frameworks use dis-\ntributed network frameworks that extend processing capabilities from cloud servers to local nodes to boost electronic \nsystems’ scalability [63–65]. With their failure detection mechanism, MEC-enabled systems apply task priority protocols \nto maintain service integrity [66, 67]. The training process of reinforcement learning (RL) algorithms needs significant \nVol.:(0123456789)\nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0 \n \n Review\nresources because developers need simulated environments for pre-training models before deploying them in the real \nworld [68, 69].\n7.6  Future directions\nThe solution to these obstacles demands multi-disciplinary work to bring about advancement. Data privacy techniques \nuse combinations of FL and SMPC and synthetic data production to generate protected frameworks [58]. The research \nfield has developed domain-generalization architectures that adapt to multiple datasets [28]. Explainability: integra-\ntion of LLMs with causal reasoning modules for clinically interpretable outputs. The development process incorporates \nregulatory innovation through process improvements that mirror software patching procedures [28]. The combination \nof LLMs and TL will drive medical progress when barriers are addressed under the fundamental pillars of ethical devel-\nopment and technical excellence.\n8  Limitation\nThe healthcare benefits of integrating Transfer Learning (TL) and Large Language Models (LLMs) face various technical \nand ethical challenges that need immediate attention for proper implementation. Implementing TL with LLM faces \nobstacles that cover privacy issues, such as the station needing resource-intensive functions, and understanding com-\nplex models.\n8.1  Ethical constraints on data sharing\nHealthcare data constitutes sensitive materials that fall under healthcare standards such as HIPAA and GDPR to limit Elec-\ntronic Health Record (EHR) sharing with model trainers. The de-identification processes required for EHRs produce feature \nloss, which diminishes their clinical value because these medical records maintain personally identifiable information \n(PII). Applying NER tools as part of data anonymization procedures to protect patient privacy can diminish the value of \nclinical information in medical records. Cultivating training data disparities, particularly involving minority groups, creates \nan effect that persists through(Token-Level) Transfer Learning, thereby generating imbalanced diagnostic results. The \ndecentralized power of federated learning enables privacy protection through its system, but this approach creates com-\nplex implementation challenges and needs strong collaboration between institutions, which proves difficult to execute.\n8.2  Domain shift in cross‑institutional deployments\nMedical imaging and clinical service operations between institutions differ through domain shift, which causes com-\nputational models trained on Hospital A to show reduced effectiveness while being used at Hospital B due to practice \nand equipment variations and distinct regional disease patterns. Deploying fine-tuned X-ray models trained in urban \nhospitals will lead to decreased performance in rural healthcare facilities that present variations in patient demographics \nand imaging protocols. Feature extraction problems in LLMs become more complex because the lack of transparency \nin these systems makes domain mismatch correction resistant. The promising techniques, such as domain adaptation \nthrough adversarial training and auto-supervised learning, need extensive target domain-labeled data, which healthcare \nfacilities struggle to provide effectively.\n8.3  Computational and scalability challenges\nLLMs’ deployment and training operations need expensive high-performance GPUs/TPUs, which healthcare institutions \nwithout sufficient funding cannot afford. Specific task applications of GPT-4 and BioBERT models require additional \ncomputational resources that increase their technical requirements. Both quantization and LoRA seek to reduce this \nchallenge but sacrifice precision to reach minimum efficiency. Surgical diagnostic systems with real-time requirements \nneed edge-computing solutions because clinical environments have yet to adopt this technology.\nVol:.(1234567890)\nReview  \nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0\n8.4  Interpretability and trust gaps\nPatients find accepting decision-making processes from TL/LLM systems challenging due to their unclear nature. A \ntreatment plan recommendation from a model usually lacks explanations that would help critical care staff adopt it \neffectively. SHAP and LIME deliver post-hoc interpretability through explanations, yet they remain flawed compared to \nmedical practitioner reasoning approaches. Given the severe consequences of incorrect predictions, the descriptive gap \nabout system reasoning choices becomes most pronounced in high-priority situations, such as oncology.\n8.5  Negative transfer and overfitting\nSource-domain knowledge irrelevant to the target domain negatively impacts target-task performance, mainly when TL \noperates on diverse medical datasets, such as dermatological to cardiological information. Small datasets used during \nfine-tuning alignment produce model overfitting because they make models memorize noise instead of developing \ngeneral knowledge. The limited amount of X-ray data for training a COVID-19 detection model can produce failure when \nnew viral variants appear.\n8.6  Regulatory and validation hurdles\nContinuous model updates for medical knowledge have become vital due to changing information, but regulatory sys-\ntems are not quick enough to validate the new AI algorithms. The current approval system requires diabetic retinopathy \nscreening models to undergo separate recertifications after every minor model architecture modification, thus delaying \nimplementation. Healthcare-specific TL tasks experience difficulties when comparing performance results because no \nstandardized benchmarks are established for these tasks.\n9  Future directions\nResearch must advance medical uses of Transfer Learning and Large Language Models using better and more responsible \nsolutions. Strong research attention is needed to create privacy systems that combine SMPC with FL and GAN technolo-\ngies to provide realistic training datasets that protect patient privacy. Domain generalization models using meta-learning \nand invariant representation learning can work with different medical data sets, even from separate patient groups. The \ntraining process on shared patient data from multiple hospitals produces better outcomes for hospitals that are not part \nof the model development stage. AI decision explanations grow stronger by combining LLMs with symbolic reasoning \nsystems, creating understandable solution sequences for medical areas such as cancer and emergencies. A new regula-\ntory system should validate AI models through conditional authorization that keeps the models updated, like software \nupdates, without complete re-evaluation. Open-source benchmarking systems for healthcare transfer learning tasks, \nincluding MedTransfer, create agreed evaluation standards that aid faster, superior medical innovation. Humans and \nartificial intelligence scientists should build a united system with medical staff and government teams to overcome \nmedical and social problems while creating practical healthcare delivery methods. Creating precise medicine using these \nelements requires fixing technical problems and building a proper infrastructure worldwide.\n10  Conclusion\nHealthcare systems experience substantial transformation in diagnostics operations, patient care delivery, and opera-\ntional efficiency through implementing Transfer Learning (TL) and Large Language Models (LLMs). The healthcare field \ndemonstrates that these methods succeed in disease prediction and respiratory classification through hybrids that reach \naccuracy rates greater than 95%, regardless of large datasets. The universal implementation of LLMs remains limited by \nfour main factors: data privacy regulations, institutional data domain variability, high computational needs, and uncer -\ntain decision processes within LLMs. The deployment process becomes more complex because of ethical requirements, \nwhich add complexity for bias reduction alongside compliance requirements. Modern healthcare improvement depends \nVol.:(0123456789)\nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0 \n \n Review\nupon developing methods for protecting privacy (federated learning included), domain generalization methods, and \nexplainable AI systems built for clinical needs. Study groups, including researchers, clinicians, and policymakers, are \nneeded to develop uniform benchmarks, elevate validation procedures, and deliver equal tool access to all. TL and LLM \ndevelopment will enable their expansion into personalized medicine while advancing real-time diagnostics and global \nhealth equity only through solutions to preceding challenges. Applying patient-focused healthcare that is enabled by \nethical responsibility when integrating technical innovation will result in a future healthcare model.\nAcknowledgements Not applicable.\nAuthor contributions • Anmol Rahujo, Daniya Atif, Syed Azeem Inam, Abdullah Ayub Khan, Sajid Ullah perform the Original Writing Part, \nSoftware, and Methodology; • Syed Azeem Inam, Abdullah Ayub Khan, Sajid Ullah perform Rewriting, investigation, design Methodology, \nand Conceptualization; • Anmol Rahujo, Daniya Atif performs related work part and manage results and discussions; • Anmol Rahujo, Daniya \nAtif perform related work part and manage results and discussion; • Syed Azeem Inam, Abdullah Ayub Khan, Sajid Ullah perform Rewriting, \ndesign Methodology, and Visualization; • Abdullah Ayub Khan performs Rewriting, design Methodology, and Visualization. The author accepts \nthe online/published version of this manuscript.\nFunding Not applicable.\nData availability No datasets were generated or analysed during the current study.\nDeclarations \nEthics approval and consent to participate The committee of the Sindh Madressatul Islam confirmed that all experimental protocols were \napproved by the organization. It is confirmed that the experiments follow the criteria of ethics approval and consent to participates. Informed \nconsent was obtained from all individual participants included in the study.\nInformed consent No human subjects are harmed in this research, and we confirmed that all data shared with the participants.\nCompeting interests The authors declare no competing interests.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which \npermits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to \nthe original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You \ndo not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party \nmaterial in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds \nthe permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco \nmmons. org/ licen ses/ by- nc- nd/4. 0/.\nReferences\n 1. Rezgui K. Large language models for healthcare: applications, models, datasets, and challenges. In: 2024 10th international conference \non control, decision and information technologies (CoDIT), IEEE; 2024. p. 2366–2371. https:// doi. org/ 10. 1109/ CoDIT 62066. 2024. 10708 \n253.\n 2. Lewis M et al. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension; \n2019.\n 3. Shah NH, Entwistle D, Pfeffer MA. Creation and adoption of large language models in medicine. JAMA. 2023;330(9):866. https:// doi. org/ \n10. 1001/ jama. 2023. 14217.\n 4. Thirunavukarasu AJ, et al. Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: a head-\nto-head cross-sectional study. PLOS Digital Health. 2024;3(4): e0000341. https:// doi. org/ 10. 1371/ journ al. pdig. 00003 41.\n 5. Li M, Jiang Y, Zhang Y, Zhu H. Medical image analysis using deep learning algorithms. Front Public Health. 2023. https:// doi. org/ 10. 3389/ \nfpubh. 2023. 12732 53.\n 6. Elahi M, Afolaranmi SO, Martinez Lastra JL, Perez Garcia JA. A comprehensive literature review of the applications of AI techniques through \nthe lifecycle of industrial equipment. Discov Artif Intell. 2023;3(1):43. https:// doi. org/ 10. 1007/ s44163- 023- 00089-x.\n 7. Ayana G, Park J, Jeong J-W, Choe S. A novel multistage transfer learning for ultrasound breast cancer image classification. Diagnostics. \n2022;12(1):135. https:// doi. org/ 10. 3390/ diagn ostic s1201 0135.\n 8. Azari MS, Flammini F, Santini S, Caporuscio M. A systematic literature review on transfer learning for predictive maintenance in industry \n4.0. IEEE Access. 2023. https:// doi. org/ 10. 1109/ ACCESS. 2023. 32397 84.\n 9. Pantopoulou S, Weathered M, Lisowski D, Tsoukalas LH, Heifetz A. Temporal forecasting of distributed temperature sensing in a thermal \nhydraulic system with machine learning and statistical models. IEEE Access. 2025;13:10252–64. https:// doi. org/ 10. 1109/ ACCESS. 2025. \n35264 38.\nVol:.(1234567890)\nReview  \nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0\n 10. Liu Y, Deng A. A universal domain adaptation method based on self-supervised clustering for machinery fault diagnosis. In: 2024 \n30th international conference on mechatronics and machine vision in practice (M2VIP), IEEE; 2024. p. 1–6. https:// doi. org/ 10. 1109/  \nM2VIP 62491. 2024. 10746 101.\n 11. Wang Z, et  al. Toward unsupervised domain adaptation fault diagnosis: a multi-source multitarget method. IEEE Sens J. \n2025;25(1):1994–2007. https:// doi. org/ 10. 1109/ JSEN. 2024. 34967 36.\n 12. Wang T, Xu D, Jiang B, Yan X-G. Actuator fault detection and estimation for hydrofoil attitude control systems: a Gaussian mixture \nmodel-aided UKF approach. IEEE Trans Instrum Meas. 2024;73:1–14. https:// doi. org/ 10. 1109/ TIM. 2024. 34533 34.\n 13. Zhang W, Xu Q, Hu Y, Xu C, Luo L. Attention-based two-stage multi-sensor feature fusion method for bearing fault diagnosis. IEEE \nTrans Ind Appl. 2024;60(6):8709–21. https:// doi. org/ 10. 1109/ TIA. 2024. 34432 32.\n 14. Wu D, Nie L, Mumtaz RA, Agarwal K. A LLM-based hybrid-transformer diagnosis system in healthcare. IEEE J Biomed Health Inform. \n2024. https:// doi. org/ 10. 1109/ JBHI. 2024. 34814 12.\n 15. Yan P , Abdulkadir A, Luley PP , Rosenthal M, Schatte GA, Grewe BF. A comprehensive survey of deep transfer learning for anomaly \ndetection in industrial time series: methods, applications, and directions. IEEE Accesse. 2024;12:3768–89.\n 16. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. In: Advances in neural information \nprocessing systems, 30. 2017.\n 17. Raiaan MAK, Mukta MSH, Fatema K, Fahad NM, Sakib S. Most Marufatul Jannat Mim All Author review on large language models: \narchitectures, applications, taxonomies, open issues and challenges. IEEE Access. 2024;12:26839–74.\n 18. Chandra Thapa SC. Precision health data: requirements, challenges and existing techniques for data security and privacy. Comput \nBiol Med. 2021;129: 104130.\n 19. Peng W, Adeli E, Bosschieter T, Park SH, Zhao Q, Pohl KM. Generating realistic brain MRIs via a conditional diffusion probabilistic \nmodel. Springer Nature Switzerland; 2023. p. 14–24. https:// doi. org/ 10. 1007/ 978-3- 031- 43993-3_2.\n 20. Manaka T, Van Zyl T, Kar D, Wade A. Multi-step transfer learning in natural language processing for the health domain. Neural Process \nLett. 2024. https:// doi. org/ 10. 1007/ s11063- 024- 11526-y.\n 21. Yang R, Tan TF, Lu W, Thirunavukarasu AJ, Ting DSW, Liu N. Large language models in health care: development, applications, and \nchallenges. John Wiley and Sons Inc; 2023. https:// doi. org/ 10. 1002/ hcs2. 61.\n 22. Wani NA, Kumar R, Mamta A, Bedi J, Rida I. Explainable AI-driven IoMT fusion: unravelling techniques, opportunities, and challenges \nwith explainable AI in healthcare. Inf Fus. 2024;110: 102472. https:// doi. org/ 10. 1016/j. inffus. 2024. 102472.\n 23. Alhussan AA, et al. Classification of diabetes using feature selection and hybrid Al-Biruni earth radius and dipper throated optimiza-\ntion. Diagnostics. 2023;13(12):2038. https:// doi. org/ 10. 3390/ diagn ostic s1312 2038.\n 24. Inam SA, et al. A novel deep learning approach for investigating liquid fuel injection in combustion system. Discov Artif Intell. \n2025;5(1):32. https:// doi. org/ 10. 1007/ s44163- 025- 00248-2.\n 25. Wani NA, Kumar R, Bedi J. DeepXplainer: an interpretable deep learning based approach for lung cancer detection using explainable \nartificial intelligence. Comput Methods Programs Biomed. 2024;243: 107879. https:// doi. org/ 10. 1016/j. cmpb. 2023. 107879.\n 26. Saha L, Tripathy HK, Gaber T, El-Gohary H, El-kenawy E-SM. Deep churn prediction method for telecommunication industry. Sustain-\nability. 2023;15(5):4543. https:// doi. org/ 10. 3390/ su150 54543.\n 27. Inam SA, et al. PR-FCNN: a data-driven hybrid approach for predicting PM2.5 concentration. Discov Artif Intell. 2024;4(1):75. https://  \ndoi. org/ 10. 1007/ s44163- 024- 00184-7.\n 28. Wani NA, Kumar R, Bedi J. Harnessing fusion modeling for enhanced breast cancer classification through interpretable artificial intel-\nligence and in-depth explanations. Eng Appl Artif Intell. 2024;136: 108939. https:// doi. org/ 10. 1016/j. engap pai. 2024. 108939.\n 29. El-kenawy E-SM, Khodadadi N, Mirjalili S, Abdelhamid AA, Eid MM, Ibrahim A. Greylag goose optimization: nature-inspired optimiza-\ntion algorithm. Expert Syst Appl. 2024;238: 122147. https:// doi. org/ 10. 1016/j. eswa. 2023. 122147.\n 30. Sajid Hussain S, Wani NA, Kaur J, Ahmad N, Ahmad S. Next-generation automation in neuro-oncology: advanced neural networks \nfor MRI-based brain tumor segmentation and classification. IEEE Access. 2025;13:41141–58. https:// doi. org/ 10. 1109/ ACCESS. 2025. \n35477 96.\n 31. Mahmoud M. A review on waste management techniques for sustainable energy production. Metaheuristic Optimiz Rev. 2025;3(2):47–58. \nhttps:// doi. org/ 10. 54216/ MOR. 030205.\n 32. Inam SA, Iqbal D, Hashim H, Khuhro MA. An empirical approach towards detection of tuberculosis using deep convolutional neural \nnetwork. Int J Data Min Model Manag. 2024;16(1):101–12. https:// doi. org/ 10. 1504/ IJDMMM. 2024. 136232.\n 33. Khaled K, Singla MK. Predictive analysis of groundwater resources using random forest regression. J Artif Intell Metaheuristics. \n2025;09(01):11–9. https:// doi. org/ 10. 54216/ JAIM. 090102.\n 34. Ur Rahim M, Hussain M, Inam SA, Hashim H. Ignition behavior of supercritical liquid fuel in combustion system. J Mech Continua Math \nSci. 2021. https:// doi. org/ 10. 26782/ jmcms. 2021. 08. 00003.\n 35. Prakash JV, Vijay AAS. A comprehensive multimodal framework for optimizing social media hashtag recommendations. IEEE Trans Comput \nSoc Syst. 2025. https:// doi. org/ 10. 1109/ TCSS. 2024. 35087 33.\n 36. Koul A, Bawa RK, Kumar Y. An analysis of deep transfer learning-based approaches for prediction and prognosis of multiple respiratory \ndiseases using pulmonary images. Arch Comput Methods Engg. 2024;31(2):1023–49.\n 37. Yang Z, et al. Talk2Care: an LLM-based voice assistant for communication between healthcare providers and older adults. Proc ACM \nInteract Mob Wearable Ubiquitous Technol. 2024;8(2):1–35. https:// doi. org/ 10. 1145/ 36596 25.\n 38. Guleria P . NLP-based clinical text classification and sentiment analyses of complex medical transcripts using transformer model and \nmachine learning classifiers. Neural Comput Appl. 2024. https:// doi. org/ 10. 1007/ s00521- 024- 10482-x.\n 39. Gautam AS, Raza Z. Disease outbreak prediction using natural language processing: a review. Springer Science and Business Media \nDeutschland GmbH; 2024. https:// doi. org/ 10. 1007/ s10115- 024- 02192-6.\n 40. Aydin S, Karabacak M, Vlachos V, Margetis K. Large language models in patient education: a scoping review of applications in medicine. \nFront Med. 2024;11:1477898.\n 41. Gu Y, et al. Domain-specific language model pre-training for biomedical natural language processing. ACM Trans Comput Healthc. \n2022;3(1):1–23. https:// doi. org/ 10. 1145/ 34587 54.\nVol.:(0123456789)\nDiscover Artificial Intelligence            (2025) 5:90  | https://doi.org/10.1007/s44163-025-00339-0 \n \n Review\n 42. Spolaôr N, et al. Fine-tuning pre-trained neural networks for medical image classification in small clinical datasets. Multimed Tools Appl. \n2024;83(9):27305–29. https:// doi. org/ 10. 1007/ s11042- 023- 16529-w.\n 43. Mhalla A, Favreau JM. Domain adaptation framework for personalized human activity recognition models. Multimed Tools Appl. \n2024;83(25):66775–97. https:// doi. org/ 10. 1007/ s11042- 024- 18267-z.\n 44. Rashidi HH, et al. ‘Statistics of generative artificial intelligence and nongenerative predictive analytics machine learning in medicine. \nElsevier B.V; 2025. https:// doi. org/ 10. 1016/j. modpat. 2024. 100663.\n 45. Saraswat D, Bhattacharya P , Verma A, Prasad VK, Tanwar S, Sharma G. Explainable AI for healthcare 5.0: opportunities and challenges. IEEE \nAccess. 2022;10:84486–517.\n 46. Malik H, Farooq MS, Khelifi A, Abid A, Qureshi JN, Hussain M. A comparison of transfer learning performance versus health experts in \ndisease diagnosis from medical imaging. IEEE Access. 2020;8:139367–86.\n 47. Hosna A, Merry E, Gyalmo J, Alom Z, Aung Z, Azim MA. Transfer learning: a friendly introduction. J Big Data. 2022. https:// doi. org/ 10. 1186/ \ns40537- 022- 00652-w.\n 48. Zhao Z, et al. Applications of unsupervised deep transfer learning to intelligent fault diagnosis: a survey and comparative study. IEEE \nTrans Instrum Meas. 2021. https:// doi. org/ 10. 1109/ TIM. 2021. 31163 09.\n 49. Kerdjidj O, et al. Uncovering the potential of indoor localization: role of deep and transfer learning. IEEE Access. 2024;12:73980–4010. \nhttps:// doi. org/ 10. 1109/ ACCESS. 2024. 34029 97.\n 50. Al-Fuqaha A, Guizani M, Mohammadi M, Aledhari M, Ayyash M. Internet of things: a survey on enabling technologies, protocols, and \napplications. IEEE Commun Surv Tutorials. 2015;17(4):2347–76. https:// doi. org/ 10. 1109/ COMST. 2015. 24440 95.\n 51. Hadi MU et al. Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects; 2024. \nhttps:// doi. org/ 10. 36227/ techr xiv. 23589 741. v7.\n 52. Okorie GN, Udeh CA, Adaga EM, Darajimba OD, Oriekhoe OI. Ethical considerations in data collection and analysis: a review: investigating \nethical practices and challenges in modern data collection and analysis. Int J Appl Res Soc Sci. 2024;6(1):1–22. https:// doi. org/ 10. 51594/ \nijarss. v6i1. 688.\n 53. Zheng Y, Gan W, Chen Z, Qi Z, Liang Q, Yu PS. Large language models for medicine: a survey; 2024. http:// arxiv. org/ abs/ 2405. 13055\n 54. Kheddar H, Himeur Y, Al-Maadeed S, Amira A, Bensaali F. Deep transfer learning for automatic speech recognition: towards better gen-\neralization. Knowl Based Syst. 2023. https:// doi. org/ 10. 1016/j. knosys. 2023. 110851.\n 55. Ahmed Ouameur M, Caza-Szoka M, Massicotte D. Machine learning enabled tools and methods for indoor localization using low power \nwireless network. Intern Things. 2020;12: 100300. https:// doi. org/ 10. 1016/j. iot. 2020. 100300.\n 56. Yalagandula P , Dahlin M. Research challenges for a scalable distributed information management system. Computer Science Department, \nUniversity of Texas at Austin; 2004.\n 57. Saxena RR. Applications of natural language processing in the domain of mental health; 2024. https:// doi. org/ 10. 36227/ techr xiv. 17301 \n4748. 80471 770/ v1.\n 58. Liu H, et al. Trustworthy AI: a computational perspective. ACM Trans Intell Syst Technol. 2023;14(1):1–59. https:// doi. org/ 10. 1145/ 35468 \n72.\n 59. Khan AA, Laghari AA, Inam SA, Ullah S, Shahzad M, Syed D. A survey on multimedia-enabled deepfake detection: state-of-the-art tools \nand techniques, emerging trends, current challenges & limitations, and future directions. Discov Comput. 2025;28(1):48.\n 60. Franceschelli G, Musolesi M. On the creativity of large language models; 2023. https:// doi. org/ 10. 1007/ s00146- 024- 02127-3.\n 61. Wan S, Gu Z, Ni Q. Cognitive computing and wireless communications on the edge for healthcare service robots. Comput Commun. \n2020;149:99–106. https:// doi. org/ 10. 1016/J. COMCOM. 2019. 10. 012.\n 62. Hortelano D, et al. A comprehensive survey on reinforcement-learning-based computation offloading techniques in Edge Computing \nSystems. J Netw Comput Appl. 2023;216: 103669. https:// doi. org/ 10. 1016/J. JNCA. 2023. 103669.\n 63. Enhancement of the performance of a solar still using a vibrator (A. Mahmood, Trans.). Babylonian J Mech Eng. 2023. p. 63–70. https:// \ndoi. org/ 10. 58496/ BJME/ 2023/ 008.\n 64. Hussein NAHK, Huang X, Li X, Yao J. Assessing the impact of circular economy practices on global waste management systems. ESTID -\nAMAA. 2024. https:// doi. org/ 10. 70470/ ESTID AMAA/ 2024/ 004.\n 65. Khan AA, Laghari AA, Baqasah AM, Alroobaea R, Gadekallu TR, Sampedro GA, Zhu Y. ORAN-B5G: a next generation open radio access \nnetwork architecture with machine learning for beyond 5G in industrial 5.0. In: IEEE transactions on green communications and network-\ning. 2024.\n 66. Safour RA. Electrochemical nitrogen reduction: towards efficient ammonia production. KHWARIZMIA. 2024. https:// doi. org/ 10. 70470/ \nKHWAR IZMIA/ 2024/ 001.\n 67. Khan AA, Laghari AA, Alsafyani M, Baqasah AM, Kryvinska N, Almadhor A, et al. A cost-effective approach using generative AI and gami-\nfication to enhance biomedical treatment and real-time biosensor monitoring. Sci Rep. 2025;15(1):1–16.\n 68. Al Barazanchi II, Rasheed DH. The Role of Green Technologies in Mitigating Carbon Footprints in Industrial Sectors. ESTIDAMAA. 2024. \nhttps:// doi. org/ 10. 70470/ ESTID AMAA/ 2024/ 005.\n 69. Khan AA, Laghari AA, Baqasah AM, Bacarra R, Alroobaea R, Alsafyani M, Alsayaydeh JAJ. BDLT-IoMT-a novel architecture: SVM machine learn-\ning for robust and secure data processing in Internet of Medical Things with blockchain cybersecurity. J Supercomput. 2025;81(1):1–22.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Health care",
  "concepts": [
    {
      "name": "Health care",
      "score": 0.6560460925102234
    },
    {
      "name": "Computer science",
      "score": 0.5273077487945557
    },
    {
      "name": "Healthcare system",
      "score": 0.4944322109222412
    },
    {
      "name": "Transfer of learning",
      "score": 0.4729715883731842
    },
    {
      "name": "Artificial intelligence",
      "score": 0.257598340511322
    },
    {
      "name": "Political science",
      "score": 0.12266626954078674
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": []
}