{
  "title": "Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy",
  "url": "https://openalex.org/W4393145913",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5070404711",
      "name": "Lin Ni",
      "affiliations": [
        "Huazhong Agricultural University",
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5100712619",
      "name": "Sijie Wang",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5100759399",
      "name": "Zeyu Zhang",
      "affiliations": [
        "Huazhong Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A5050038192",
      "name": "Xiaoxuan Li",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5053341467",
      "name": "Xianda Zheng",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5032890999",
      "name": "Paul Denny",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5083914998",
      "name": "Jiamou Liu",
      "affiliations": [
        "University of Auckland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3140287296",
    "https://openalex.org/W2612059685",
    "https://openalex.org/W2171106563",
    "https://openalex.org/W6753873778",
    "https://openalex.org/W4226253567",
    "https://openalex.org/W6799867964",
    "https://openalex.org/W3141740147",
    "https://openalex.org/W6732572413",
    "https://openalex.org/W3148979044",
    "https://openalex.org/W7005811008",
    "https://openalex.org/W3098404146",
    "https://openalex.org/W4285676377",
    "https://openalex.org/W4281930622",
    "https://openalex.org/W4285732232",
    "https://openalex.org/W2081095376",
    "https://openalex.org/W2980472839",
    "https://openalex.org/W3216494108",
    "https://openalex.org/W4224310786",
    "https://openalex.org/W2590973362",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W6621483976",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3023323599",
    "https://openalex.org/W4319297029",
    "https://openalex.org/W6683436842",
    "https://openalex.org/W4225109328",
    "https://openalex.org/W2555897561",
    "https://openalex.org/W2619193553",
    "https://openalex.org/W6755573351",
    "https://openalex.org/W3000412725",
    "https://openalex.org/W4214816390",
    "https://openalex.org/W6784694379",
    "https://openalex.org/W6854528606",
    "https://openalex.org/W4367046596",
    "https://openalex.org/W2905224888",
    "https://openalex.org/W6784711476",
    "https://openalex.org/W3152893301",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4308757526",
    "https://openalex.org/W3033039844",
    "https://openalex.org/W4330337479",
    "https://openalex.org/W2579564527",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4321365943",
    "https://openalex.org/W2887092413",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2995988440",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4384652088",
    "https://openalex.org/W3208430676",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3095746859",
    "https://openalex.org/W3011785238",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W650350307",
    "https://openalex.org/W3095602948",
    "https://openalex.org/W2158515176",
    "https://openalex.org/W3114961718",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Learnersourcing offers great potential for scalable education through student content creation. However, predicting student performance on learnersourced questions, which is essential for personalizing the learning experience, is challenging due to the inherent noise in student-generated data. Moreover, while conventional graph-based methods can capture the complex network of student and question interactions, they often fall short under cold start conditions where limited student engagement with questions yields sparse data. To address both challenges, we introduce an innovative strategy that synergizes the potential of integrating Signed Graph Neural Networks (SGNNs) and Large Language Model (LLM) embeddings. Our methodology employs a signed bipartite graph to comprehensively model student answers, complemented by a contrastive learning framework that enhances noise resilience. Furthermore, LLM's contribution lies in generating foundational question embeddings, proving especially advantageous in addressing cold start scenarios characterized by limited graph data. Validation across five real-world datasets sourced from the PeerWise platform underscores our approach's effectiveness. Our method outperforms baselines, showcasing enhanced predictive accuracy and robustness.",
  "full_text": "Enhancing Student Performance Prediction on Learnersourced Questions with\nSGNN-LLM Synergy\nLin Ni1,2 , Sijie Wang2, Zeyu Zhang1*, Xiaoxuan Li2, Xianda Zheng2, Paul Denny2, Jiamou Liu2\n1Huazhong Agricultural University, Wuhan 430070, China\n2School of Computer Science, The University of Auckland, Auckland 1010, New Zealand\nl.ni@auckland.ac.nz, swan387@aucklanduni.ac.nz, zhangzeyu@mail.hzau.edu.cn,\n{xli443, xzhe162}@aucklanduni.ac.nz, {p.denny, jiamou.liu}@auckland.ac.nz\nAbstract\nLearnersourcing offers great potential for scalable education\nthrough student content creation. However, predicting stu-\ndent performance on learnersourced questions, which is es-\nsential for personalizing the learning experience, is challeng-\ning due to the inherent noise in student-generated data. More-\nover, while conventional graph-based methods can capture\nthe complex network of student and question interactions,\nthey often fall short under cold start conditions where lim-\nited student engagement with questions yields sparse data. To\naddress both challenges, we introduce an innovative strategy\nthat synergizes the potential of integrating Signed Graph Neu-\nral Networks (SGNNs) and Large Language Model (LLM)\nembeddings. Our methodology employs a signed bipartite\ngraph to comprehensively model student answers, comple-\nmented by a contrastive learning framework that enhances\nnoise resilience. Furthermore, LLM’s contribution lies in gen-\nerating foundational question embeddings, proving especially\nadvantageous in addressing cold start scenarios character-\nized by limited graph data. Validation across five real-world\ndatasets sourced from the PeerWise platform underscores our\napproach’s effectiveness. Our method outperforms baselines,\nshowcasing enhanced predictive accuracy and robustness.\nIntroduction\nThe rapid growth of online education in recent years has\nresulted in considerable interest in new approaches for de-\nlivering effective learning at scale (Xianghan and Stern\n2022). A significant challenge for educators adopting new\nonline learning platforms is the need to develop new content\n(Mulryan-Kyne 2010). Educators face the task of creating\nextensive repositories of items for supporting personalized\nlearning and exploring innovative methods to immerse stu-\ndents in such material. Learnersourcing, a relatively new ap-\nproach, has emerged as a promising technique to address the\nchallenges of content development and student engagement\nin online learning (Khosravi et al. 2021; Denny et al. 2022).\nIt involves leveraging the collective knowledge, skills, and\ncontributions of learners to create educational resources.\nIn general, accurate prediction of student performance (Li\net al. 2020, 2022), such as future grades on assignments and\nexams, is immensely valuable for informing instructional\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nstrategies and enabling personalized support. Within learn-\nersourcing contexts, performance prediction on individual\nitems can support the recommendation of new items from\nlarge repositories to match the current ability level of stu-\ndents (Paramythis and Loidl-Reisinger 2003).\nMultiple-choice questions (MCQs) play a pivotal role\nin learnersourcing platforms like RiPPLE (Khosravi, Kitto,\nand Williams 2019), Quizzical (Riggs, Kang, and Rennie\n2020), UpGrade (Wang et al. 2019), and PeerWise (Denny,\nLuxton-Reilly, and Hamer 2008). Predicting student per-\nformance on MCQs is a crucial research challenge (Abdi,\nKhosravi, and Sadiq 2021). MCQs, comprising a stem and\nmultiple answer options, assess a wide range of cognitive\nskills, from factual recall to higher-order thinking (e.g., anal-\nysis, synthesis, and evaluation). They offer efficient grading\nand valuable insights into subject comprehension.\nAs learners engage in creating and responding to MCQs\non platforms like PeerWise, a graph emerges, with learn-\ners and MCQs as nodes and their interactions as edges.\nGNNs provide a powerful means to capture these intri-\ncate learner-to-question, learner-to-learner, and question-to-\nquestion relationships (Zhou et al. 2020). GNNs excel at\nextracting nuanced features from graph topology, offering\ninsights beyond traditional methods. However, while GNNs\nhold promise for predictive analytics in learnersourcing plat-\nforms, applying them to anticipate student performance on\nMCQs is relatively unexplored. A key challenge lies in\nmodeling both students’ responses and their correctness\n(i.e., correct or erroneous answers) within noisy data. Noise\ncan stem from inadvertent errors during question creation\nor participation dynamics discouraging accurate responses.\nFurthermore, learnersourcing repositories constantly evolve,\npresenting a cold start dilemma. Newly created MCQs with\nlimited responses lack substantial interaction data, hamper-\ning effective GNN utilization and predictive accuracy.\nIn summary, our study addresses two crucial challenges\nfor predicting student performance on MCQs in learner-\nsourcing platforms:\n• Modeling complex relationships involving students and\nMCQs, accommodating both accurate and erroneous re-\nsponses within noisy data.\n• Tackling the cold start problem inherent to questions with\nlow answer rates, where insufficient graph data under-\nmines predictive accuracy.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23232\nFigure 1: A scenario for the signed bipartite graph\nTo address these challenges, we propose an innovative\ntechnique that leverages the potential of GNNs for appli-\ncation to the unique dynamics present in learnersourcing\nrepositories. Our approach begins by adopting a signed bi-\npartite graph to model students’ response accuracy, as illus-\ntrated in Figure 1. This graph structure segregates students\nand questions into separate sets of nodes. Positive edges\ndenote correct student answers, while negative edges indi-\ncate incorrect responses. To counter the impact of noise,\nwe introduce a novel contrastive learning framework tai-\nlored for signed bipartite graphs. This approach draws from\nthe emerging paradigm of graph contrastive learning (Shuai\net al. 2022; Zhu et al. 2021, 2020; You et al. 2020), renowned\nfor its ability to resist noise by contrasting similar and dis-\nsimilar instances. Our choice of contrastive learning not only\nenhances model resilience but also addresses noise interfer-\nence within the data.\nFurthermore, we pioneer the integration of NLP tech-\nnology to extract intrinsic knowledge essential for accu-\nrate MCQ responses, offering a solution to the cold start\nproblem. Although previous student performance predic-\ntion research seldom leveraged NLP advancements, recent\nstudies underscore the potential of integrating advanced\nNLP in education to unravel question intricacies (Ni et al.\n2022). We posit that MCQ response accuracy hinges on\nstudents’ comprehension of underlying knowledge within\nquestions. Leveraging the gpt-3.5-turbo-0613 LLM 1, we\ncapture nuances within MCQs, extracting and weighing in-\nherent knowledge points. By fusing this semantic informa-\ntion with structural embeddings derived from the graph, our\nmodel gains a holistic view for enhanced student perfor-\nmance prediction, particularly in cold start scenarios. Our\nmain contributions include:\n1. Formulating the student performance prediction problem\nas a sign prediction on a signed bipartite graph.\n2. Exploiting contrastive learning to enhance the robustness\nof representations for students and questions.\n3. Employing an LLM to extract semantic embeddings of\nkey knowledge points in questions, and integrating this\ninformation with structural embeddings learned from a\ngraph to address the cold start problem.\n1https://platform.openai.com/docs/models/continuous-model-\nupgrades\n4. Validating the efficacy of our proposed model through ex-\nperiments conducted on five real-world datasets sourced\nfrom a prominent learnersourcing platform, achieving the\nhighest f1 value of 0.908.\nRelated Work\nWe investigate four domains: student academic performance\nprediction, the cold start problem, GNNs, and LLMs.\nPerformance Prediction: Significant research has been\ndevoted to predicting student performance on online learn-\ning platforms, with the aim of personalizing and improv-\ning the learning experience (Wang et al. 2023). This body\nof work primarily encompasses static and sequential mod-\nels (Li et al. 2020; Thaker, Carvalho, and Koedinger 2019).\nStatic models, exemplified by Logistic Regression, leverage\nhistorical data such as student scores and learning activities\nto forecast future performance (Jiang et al. 2014; Wei et al.\n2020; Daud et al. 2017). In contrast, sequential models like\nKnowledge Tracing and its variants explore the sequential\nrelationships within learning materials, allowing dynamic\nknowledge updates (Piech et al. 2015; Nakagawa, Iwasawa,\nand Matsuo 2019). Knowledge Tracing, a subset of educa-\ntional data mining and machine learning, aims to model and\npredict learners’ knowledge or skill acquisition over time\n(Lyu et al. 2022). However, our task presents unique chal-\nlenges. Unlike many previous approaches, we lack access to\nstudent profiles and additional contextual information. Fur-\nthermore, we do not possess information about the temporal\nsequence of learner responses. Consequently, our research is\nfocused on exploring deep learning solutions that rely solely\non question and learner-question interaction data to enhance\nstudent academic performance prediction.\nCold Start Problem: Addressing the cold start challenge\nis a fundamental concern in representation learning, partic-\nularly in machine learning scenarios in e-learning environ-\nments where minimal prior information is available about\nthe target data (Liu et al. 2022). Researchers have explored\ndiverse strategies to tackle this issue, tailoring their ap-\nproaches to the specific nature of the problem, available re-\nsources, and data characteristics. One common strategy is\ntransfer learning, which entails pre-training a model on a\nrelated task with a substantial dataset and then fine-tuning\nit for the target task with limited data (Pang et al. 2022).\nThis method leverages prior knowledge gained during pre-\ntraining to mitigate cold start challenges. In cases where data\nis scarce, knowledge-based systems employ explicit rules or\ndomain knowledge to provide recommendations or predic-\ntions, bypassing the need for extensive historical data (Du\net al. 2022). Recommendation systems often turn to content-\nbased filtering, relying on item and user attributes or features\nfor making recommendations (Schein et al. 2002; Safarov\net al. 2023). This approach proves effective, particularly in\nscenarios with little or no historical user-item interaction\ndata, by suggesting items based on their inherent characteris-\ntics. In alignment with previous studies that have leveraged\nontologies as supplementary data to mitigate the cold start\nproblem (Sun et al. 2017; Jeevamol and Renumol 2021),\nour approach capitalizes on the inherent semantic richness\nof MCQs sourced from learnersourcing platforms. These\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23233\nMCQs inherently contain vital knowledge points essential\nfor predictive tasks. To effectively address the challenges\nposed by the cold start problem, should this be we incor-\nporate embedding information from LLMs that encapsulate\nboth common sense and domain-specific knowledge. This\nintegration serves to enhance the representation of MCQs in\nour model.\nSigned Graph Representation Learning Methods: Pre-\ndicting student performance involves the unique task of\nsign prediction in a signed bipartite graph, where posi-\ntive edges signify correct answers and negative edges de-\nnote incorrect ones (Derr, Ma, and Tang 2018; Zhang et al.\n2023b). Traditional methods like SIDE (Kim et al. 2018)\nand SGDN (Jung, Yoo, and Kang 2020) leveraged random\nwalks, while neural-based approaches like SGCN (Derr, Ma,\nand Tang 2018) extended Graph Convolutional Networks\n(GCN) (Kipf and Welling 2016) to balance theory-guided\nsign prediction. However, adapting these methods to signed\nbipartite graphs is non-trivial. Graph Contrastive Learning\ntechniques, inspired by successes in image processing, aim\nto generate stable node representations under perturbations.\nDeep Graph Infomax (DGI) (Velickovic et al. 2019) maxi-\nmizes the mutual information between global graph and lo-\ncal node representations. GraphCL (You et al. 2020) intro-\nduces varied graph augmentations, while Graph Contrastive\nAugmentation (GCA) employs diverse views for agreement\nof node representations. Most of these models are designed\nfor unipartite graphs, with Signed Graph Contrastive Learn-\ning (SGCL) (Shu et al. 2021) adapting to signed unipartite\ngraphs. However, Signed Bipartite graph Contrastive Learn-\ning (SBCL) (Zhang et al. 2023a) is the only model combin-\ning both signed and bipartite graph aspects, albeit lacking\nconsideration for node features. Notably, these methods pri-\nmarily rely on large-scale node interaction data to extract\nstructural information for representation, which may limit\ntheir effectiveness in addressing the cold start problem.\nLLM in NLP: Recent years have witnessed signifi-\ncant advancements in LLMs in natural language processing\n(NLP). Models like BERT (Devlin et al. 2018) and T5 (Raf-\nfel et al. 2020) excel in various NLP tasks by pre-training on\nextensive text data, followed by fine-tuning. However, fine-\ntuning often demands a substantial amount of task-specific\ndata and introduces complexity. A groundbreaking develop-\nment came with GPT-3 (Brown et al. 2020), a massive au-\ntoregressive language model with 175B parameters. It in-\ntroduced In-Context Learning, allowing it to perform tasks\nwithout extensive fine-tuning or model updates. This inno-\nvation led to Instruction-Finetuned Language Models, ex-\nemplified by GPT-3.5 (Ye et al. 2023), which efficiently\nadapts to specific tasks using task-specific instructions. This\napproach overcomes previous limitations, making LLMs a\ncornerstone in NLP.\nProblem Definition\nIn this study, we model the student performance prediction\nproblem on a learnersourcing platform as the edge sign pre-\ndiction problem on a signed bipartite graph, as illustrated in\nFigure 1. We define our problem within the context of a bi-\npartite graph G = (U, V, E), where U = {u1, u2, ..., u|U|}\nand V = {v1, v2, ..., v|V|} represent the nodes for students\nand questions, respectively.|U| and |V| denote the total num-\nber of students and questions, where U ∩ V= ∅. The edge\nset E ⊂ U × Vsignifies the links between students and\nquestions and is further partitioned into positive and nega-\ntive edge sets, E+ and E−, respectively, with E = E+ ∪ E−\nand E+ ∩ E− = ∅. Given this bipartite graph and any\npair of nodes ui ∈ U, vj ∈ Vand the sign of the edge\neij ∈ Eis unknown, our goal is to leverage GNNs to learn\nthe embeddings zui ∈ Rd and zvj ∈ Rd of node ui and\nvj, respectively. Subsequently, we seek to find a mapping\nf : (zui , zvj ) 7→ {−1,+1} to determine the sign of the edge,\nthus producing our desired result.\nOne significant challenge in this context is the cold start\nproblem for newly generated questions. In early-stage situa-\ntions, a limited number of learners may have attempted the\nquestion, and consequently, the bipartite graph is sparsely\nconnected. The edges connecting students to these questions\nare uncertain or unknown, making the prediction of student\nperformance particularly challenging.\nProposed Method\nIn this section, we introduce the L\narge Language Model en-\nhanced Signed Bipartite graph Contrastive Learning (LLM-\nSBCL) model, a framework designed to learn representa-\ntions for users and questions while facilitated by LLM.\nOur framework, as depicted in Figure 2, capitalizes on\nstudent-MCQs relationships in learnersourcing platforms\nusing SBCL. It employs separate encoders for students and\nMCQs, with a graph contrastive learning model to han-\ndle dataset noise. To address cold start challenges, we en-\nhance question embeddings with semantic knowledge points\nextracted by LLM. This supplementary data augments the\nquestion structure embeddings to compute cross-entropy\nloss. In the following sections, we provide a detailed break-\ndown of our model’s architecture.\nSigned Bipartite Graph Contrastive Learning\nFig. 3 illustrates SBCL, which includes graph augmentation\nand graph encoding processes. During the graph augmen-\ntation phase, we use stochastic graph augmentation to gen-\nerate two augmented graphs. Each graph is then bifurcated\naccording to edge signs, resulting in a positive and a nega-\ntive graph. These are further processed using GNNs to learn\nthe structural embeddings for node representations.\nFigure 2: The Framework of LLM-SBCL model\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23234\nFigure 3: The framework of SBCL\nGraph Augmentation: To enhance the robustness and\ngeneralization of learned node representations in the con-\ntext of learnersourcing platforms with noisy data, we utilize\nstochastic perturbation for graph augmentation. We generate\ntwo distinct graphs from the original one by randomly flip-\nping edge signs, transforming a positive edge into a negative\none, and vice versa, with a probability p.\nGraph Encoder : Following data augmentation, we ac-\nquire two augmented graphs, denoted as G1 and G2, as illus-\ntrated in Fig 3. Considering the distinct semantic attributes\nof positive and negative edges—positive edges represent-\ning correct answers by users and negative edges indicat-\ning incorrect answers—we employ separate GNN encoders\nfor each edge type. This dual-GNN approach aligns with\nour contrastive objective, which we will delve into in the\nnext subsection. Consequently, we partition each augmented\ngraph into two sub-graphs exclusively featuring positive and\nnegative edges, denoted as positive graph G+\nm and negative\ngraph G−\nm, where m ∈ {1,2}. To facilitate this encoding, we\nadopt the Positive and Negative Graph Attention Network\n(GAT) (Veliˇckovi´c et al. 2017), consistent with the design\nin (Zhang et al. 2023a). Formally, the representation for the\ni-th node, denoted as zi, is computed as follows:\nz(l+1),σ\ni,m = PReLU(GNNσ\nm(z(l),σ\ni,m , Gσ\nm)) (1)\nzi =\nh\nz(L),+\ni,1 ∥z(L),−\ni,1 ∥z(L),+\ni,2 ∥z(L),−\ni,2\ni\nW (2)\nwhere σ ∈ {+, −}, m refers to the m-th augmented graph,\nL denotes the number of GNN layers and l denotes the l-th\nlayer. z(0),σ\ni,m denotes the input feature vector of thei-th node.\nW is a learn-able transformation matrix.\nContrastive Objective: Noise is commonly expected in\na learnersourcing platform where the majority of content\nis student-generated. Variations in course rules for question\ncreation and answering, along with student behaviors, can\nintroduce considerable noise. For instance, when the moti-\nvation for answering a question is low (e.g., performance on\nFigure 4: Inter-view and Intra-view contrastive loss\nsuch platform does not impact final scores), students might\nguess answers randomly. Likewise, students may uninten-\ntionally create incorrect questions. We categorize these in-\nstances as noise within our dataset. To address this, we em-\nploy a contrastive objective for robust signed bipartite graph\nrepresentation learning, encompassing two losses: (a) Inter-\nview contrastive loss and (b) Intra-view contrastive loss,\nwhere (a) is for nodes from the same encoder while (b) is\nfor those from different encoders as depicted in Fig 4.\nWe utilize InfoNCE loss (Sohn 2016; Oord, Li, and\nVinyals 2018) to define our inter-view contrastive loss for\npositive augmented graphs as follows:\nL+\ninter = −1\nI\nIX\ni=1\nlog\nexp\n\u0010\nsim\n\u0010\nz+\ni,m, z+\ni,m′\n\u0011\n/τ\n\u0011\nPI\nj=1,j̸=i exp\n\u0010\nsim\n\u0010\nz+\ni,m, z+\nj,m′\n\u0011\n/τ\n\u0011\n(3)\nwhere I is the number of nodes in a mini-batch, z+\ni,m rep-\nresents the representation of node i in the m-th augmented\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23235\nFigure 5: Flatten MCQ component key-value pairs into a se-\nquence to form an MCQ “sentence”\npositive graph, sim(·, ·) represents the similarity function\nbetween the two representations (e.g., cosine similarity)\nand τ denotes the temperature parameter. The perspective-\nspecific contrastive loss for negative graphs is similar.\nThe intra-view contrastive loss is defined as:\nLintra = −1\nI\nIX\ni=1\nlog\nPM\nm=1 exp\n\u0000\nsim\n\u0000\nzi, z+\ni,m\n\u0001\n/τ\n\u0001\nPM\nm=1 exp\n\u0000\nsim\n\u0000\nzi, z−\ni,m\n\u0001\n/τ\n\u0001 (4)\nwhere M denotes the number of augmented graphs, which\nis equal to 4 in our paper. The Combined contrastive loss is\nLCL = (1− α) · (L+\ninter + L−\ninter) +α · Lintra (5)\nwhere α is the weight coefficient that controls the signifi-\ncance between two losses.\nIncorporating Semantic Embedding into SBCL\nIn our LLM-SBCL framework (Fig 2), we employ a mul-\ntimodal approach, utilizing GNNs and NLP Models, to pro-\ncess User-Question Relationship and MCQ Textual Informa-\ntion respectively. We recognize that infusing semantic con-\ntext into MCQs has great potential for uncovering the under-\nlying knowledge, which are crucial for the prediction task.\nExtracting Semantic Embeddings via LLM: To include\nsemantic information, we utilize a LLM to extract knowl-\nedge from MCQs. We combine these semantic representa-\ntions with relational embeddings from the Graph Encoder,\ntraining them with cross-entropy loss LCE for a more com-\nprehensive representation.\nOur approach involves transforming MCQs into sen-\ntence format by flattening key-value pairs (Fig 5). This re-\nsults in input data, denoted as mcqj, comprising content\nfrom Stem, Answer, Options, and Explanation: mcqj =\n{Sk, Sv, Ak, Av, OA,k, OA,v, . . . , OE,k, OE,v, Ek, Ev}.\nNext, we employ LLM to extractn knowledge point terms\nti and their corresponding weights hi for each MCQ. GloVe\nword embeddings (Pennington, Socher, and Manning 2014)\nprovide word embeddings embi,k for words in ti. The av-\nerage of these embeddings represents a single knowledge\npoint kpi. We compute the weighted average of all knowl-\nedge points to obtain the Question Semantic Embeddingwj.\nwj =\nPn\ni=1 hi · kpi\nPn\ni=1 hi\n, kp i =\nPm\nk=1 embi,k\nm (6)\nNLP Large Language Model: We employ gpt-3.5-turbo-\n0613 2 as our LLM engine, capable of retrieving n pairs of\nti and hi with a single prompt API request.\n• Task Objective: Your task is to extract key knowledge\npoints from an MCQ created by a first-year student who\nstudies biology in college. The question consists of a stem,\nup to five options, an answer, and an explanation provided\nby the student author.\n• Special Requirements: Please provide your response in\nJSON format with the following keys and format:\n{\"Keywords\": [{\"keyword\": \"keyword_name\", \"percentage\": 0}]}\n1. Each \"keyword\nname\" should be a word or a short\nterm with less than five words.\n2. The percentages of all keywords should add up to 1.\n3. Only include the top five keywords to avoid excessive\nkeyword extraction.\n• In Context Learning: Here’s an example response format:\n{\"Keywords\": [\n{\"keyword\": \"Menstrual cycle\", \"percentage\": 0.8},\n{\"keyword\": \"Luteal phase\", \"percentage\": 0.2 }\n]}\n• Context: Here’s the MCQ from the student author:\nQuestion stem: Jamie has a Atherosclerosis, what does this\nimply?\n[A]: She constantly needs to visit the ladies room to pass\nurine.\n[B]: She has impaired vision.\n[C]: She has lost a lot of weight recently due to no apparent\nreason. (She hasn’t suddenly turned paleo).\n[D]: Her arterial walls have thickened making her more\nlikely to have a heart attack.\nQuestion answer: D\nExplanation: Atherosclerosis is the thickening of the artery\nwalls (generally due to poor diet and lack of exercise).\nThis can lead to heat attacks and strokes, as when pressure\nbuilds up in the arteries, arteries are more likely to burst.\nIn a normal person the walls of arteries are more likely\nto stretch and allow for the fluctuations of pressure in the\nblood.\n• LLM Response:\n[{\"keyword\": \"Atherosclerosis\", \"percentage\": 0.44},\n{\"keyword\": \"Arterial walls\", \"percentage\": 0.22},\n{\"keyword\": \"Thickening\", \"percentage\": 0.11},\n{\"keyword\": \"Heart attack\", \"percentage\": 0.11},\n{\"keyword\": \"Strokes\", \"percentage\": 0.11}]\nThe provided prompt encompasses the task objective,\nrequirements, in-context learning format, and context, en-\nsuring consistent and aligned responses. This well-crafted\nprompt guided the LLM to analyse the specific knowledge\ntopics related to the MCQs. As a result, the Question Se-\nmantic Embedding derived from this prompt becomes more\nmeaningful, thereby enhancing the accuracy of student per-\nformance prediction.\nSemantic Embedding Integration: LLM-SBCL en-\nhances SBCL with semantic question embeddings. Instead\nof directly predicting the edge sign between student ui and\n2https://platform.openai.com/docs/models/continuous-model-\nupgrades\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23236\nquestion vj, we also include semantic embeddings wj for\nquestion representations.\nThis results in d-dimensional embeddings for ui and\nvj and an additional d-dimensional embedding for wj. To\nmaintain uniform dimensions, we transform vj and wj into\na unified d-dimensional representation qvj using a learnable\nmatrix Wq. The transformed question embeddings, qvj , are\nconcatenated with ui and processed through a single-layer\nMLP for edge sign prediction:\nqvj = (vj∥wj)Wq, y pred = MLP\n\u0000\nui∥qvj\n\u0001\n(7)\nUse the predicted ypred in the cross-entropy loss function:\nLCE = −y · log ypred + (1− y) · log (1− ypred) (8)\nwhere y is the ground truth from {−1, 1} to {0, 1}.\nThe final joint loss of cross-entropy and contrastive loss:\nL = LCE + β · LCL (9)\nHere, β controls the contribution of the contrastive loss.\nExperiments\nIn this section, we evaluate the LLM-SBCL model’s\nperformance on five real-world datasets: biology, law,\ncardiff20102, sydney19351, and sydney23146. We intro-\nduce the datasets and compare our model against state-\nof-the-art baselines. We provide experimental details and\npresent results from both baseline and our models.\nDataset Background and Introduction\nPeerWise is a platform that allows students to share their\nknowledge with each other. Teachers can create courses\nand manage student access to them. MCQs are a common\ntype of assessment format that presents a question or state-\nment followed by a set of predetermined options or choices.\nWithin these courses, students can create and explain their\nown MCQs related to the course material. They can also an-\nswer and discuss questions posed by their peers. This allows\nfor a collaborative learning experience where students can\nlearn from each other and deepen their understanding of the\ncourse material.\nWe use five real-world datasets from PeerWise: biology\nand law from the University of Auckland, cardiff20102 from\nCardiff University School of Medicine with course id 20102,\nsydney19351 and sydney23146 are from two University of\nSydney biochemistry courses with the highest number of an-\nswering records (i.e. links). Detailed summaries for these\ndatasets are provided in Table 1.\nBiology Law Cardiff Sydney1 Sydney2\nStudents 761 528 383 382 198\nMCQs 380 5600 1171 457 748\nLinks 76613 88563 64524 24032 24050\nLinkP% 0.665 0.931 0.600 0.531 0.706\nLinkN% 0.335 0.069 0.400 0.469 0.294\nTable 1: Statistics on Signed Bipartite Networks.\nSince each course is independent of the others, we con-\nstruct a signed bipartite graph for each dataset using the an-\nswering records. For a student ui ∈ Uanswering a question\nvj ∈ V, we establish an edge between ui and vj with its\nsign being +1 if the answer of ui matches the correct an-\nswers for vj’s and -1 otherwise. We observe that the overall\ncorrectness for the questions exceeds 50% for each course,\nwith most courses ranging from 60% to 70%. However,\nthere are exceptions such as Law, where the overall correct-\nness reaches 93%. The inherent nature of such online peer-\npractice platforms suggests an innate data imbalance issue,\nwhich is handled by class weights in the experiment.\nExperimental Settings\nWe compared our model against state-of-the-art methods, in-\ncluding Random Embedding, Graph Convolutional Network\n(GCN), Graph Attention Networks (GAT), Signed Graph\nConvolutional Network (SGCN), Signed Bipartite Graph\nNeural Networks (SBGNN) (Huang et al. 2021), and Signed\nBipartite Graph Contrastive Learning (SBCL).\n• Random Embedding: Concatenates random 64-\ndimensional student (zui ∈ R64) and question (zvj ∈ R64)\nembeddings, followed by logistic regression to predict.\n• GCN: base on an efficient variant of convolutional neu-\nral networks which operate directly on graphs. The choice\nof convolutional architecture is motivated via a localized\nfirst-order approximation of spectral graph convolutions.\n• GAT: Implemented using PyTorch Geometric (PyG) 3\nwith two convolutional layers and 300 training epochs, ini-\ntializing with standard normal distribution embeddings.\n• SGCN: harnesses balance theory to correctly incorpo-\nrate negative links during the aggregation process. SGCN\naddresses the unique challenge posed by negative links,\nwhich not only carry a distinct semantic meaning com-\npared to positive links, but also form complex relations\nwith them due to their inherently different principles.\n• SBGNN: Utilizes innovative techniques for modeling\nsigned bipartite graphs 4. We used publicly available code\nwith default settings and 300 training epochs.\n• SBCL: Utilizes a single-headed GAT with 2 convolutional\nlayers and 64-dimensional random embeddings5. We used\npublicly available code with default settings and 300 train-\ning epochs. The final LLM-SBCL variant incorporates se-\nmantic question embeddings.\nData was split into training (85%), validation (5%), and\ntest (10%) sets. 64-dimensional embeddings were generated\nfor students and questions. Question representations were\nenriched by concatenating stem, choices, and explanations,\nthen processed through GPT-3.5 for knowledge points, re-\nduced to 64 dimensions with PCA. Hyperparameters were\nset to α = 0.8, β = 5e−4, and r = 0.1 (where r denotes the\nmask ratio, governing the percentage of links flipped during\ngraph augmentation) after experimentation.\n3https://pytorch-geometric.readthedocs.io/en/latest/index.html\n4https://github.com/huangjunjie-cs/SBGNN\n5https://github.com/Alex-Zeyu/SBGCL\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23237\nRandom\nEmbedding\nUnsigned\nNetwork Embedding\nSigned/Bipartite\nNetwork Embedding\nSigned Bipartite graph\nContrastive Learning models\nDataset Random GCN GAT SGCN SBGNN SBCL LLM-SBCL\nBiology 0.35±0.01 0.682±0.058 0.618±0.013 0.768±0.04 0.753±0.014 0.772±0.016 0.787±0.014\nLaw 0.472±0.01 0.823±0.01 0.817±0.05 0.84±0.013 0.861±0.034 0.901±0.016 0.908±0.018\nCardiff20102 0.136±0.062 0.677±0.024 0.571±0.013 0.607±0.033 0.712±0.016 0.718±0.018 0.734±0.023\nSydney19351 0.29±0.014 0.642±0.021 0.564±0.022 0.635±0.044 0.673±0.016 0.674±0.021 0.694±0.021\nSydney23146 0.288±0.035 0.728±0.013 0.608±0.02 0.726±0.04 0.712±0.021 0.733±0.019 0.76±0.022\nTable 2: The results of link sign prediction on five real-world educational datasets (average binary-F1± standard deviation).\nDataset Metrics SBCL LLM-SBCL\nBiology\nBinary-F1 0.616±0.049 0.629±0.061\nMicro-F1 0.537±0.027 0.553±0.043\nMacro-F1 0.512±0.015 0.528±0.036\nAUC 0.524±0.007 0.539±0.032\nLaw\nBinary-F1 0.885±0.039 0.894±0.031\nMicro-F1 0.803±0.059 0.814±0.048\nMacro-F1 0.559±0.025 0.548±0.017\nAUC 0.628±0.022 0.594±0.017\nCardiff\nBinary-F1 0.595±0.08 0.625±0.045\nMicro-F1 0.532±0.037 0.543±0.029\nMacro-F1 0.506±0.017 0.517±0.018\nAUC 0.521±0.019 0.52±0.019\nSydney1\nBinary-F1 0.573±0.057 0.623±0.055\nMicro-F1 0.539±0.035 0.555±0.031\nMacro-F1 0.533±0.027 0.533±0.033\nAUC 0.538±0.022 0.544±0.026\nSydney2\nBinary-F1 0.779±0.039 0.801±0.009\nMicro-F1 0.66±0.036 0.681±0.012\nMacro-F1 0.505±0.031 0.495±0.018\nAUC 0.518±0.017 0.511±0.013\nTable 3: The results for cold-start problem.\nResults\nIn this section, SBCL represents our proposed model with\ncontrastive learning, excluding NLP information. Variations\nof SBCL, denoted as LLM-SBCL, incorporate semantic\nembeddings from LLM. We evaluate model performance\nusing metrics such as Area Under the Receiver Operat-\ning Characteristic Curve (AUC), binary F1, micro-F1, and\nmacro-F1 scores. Higher metric values indicate superior link\nprediction.\nWe conducted each experiment ten times on each dataset\nfor each model. Table 2 presents mean and standard devia-\ntion results for baseline models (Random Embedding, GCN,\nGAT, SGCN, and SBGNN) alongside our proposed SBCL\nmodels. The best results are highlighted in bold, and runner-\nup results are underlined. Key insights from Table 2 include:\n• SGCN and SBGNN, which consider the sign of the links\nin signed networks, outperform unsigned methods (GCN,\nGAT) on most datasets.\n• SBCL consistently outperforms SGCN and SBGNN,\nhighlighting the ability of contrastive learning.\n• LLM-enhanced variants outperform SBCL across all\ndatasets, showcasing the benefit of incorporating NLP se-\nmantic embeddings.\nCold Start Problem Results\nOur study demonstrates a substantial enhancement in stu-\ndent performance prediction by incorporating semantic em-\nbeddings of MCQs, as shown in Table 3. To ensure a fair\ncomparison, we maintained a consistent experimental setup.\nWe randomly selected 10% of the questions to simulate a\ncold start scenario. Edges related to these questions were\nplaced in the test set, while the rest formed the training set.\nOur primary evaluation metric was the binary F1 score.\nAs a baseline, we used a generic representation learn-\ning model without semantic embeddings, which performed\nwell with abundant data but struggled in cold start scenarios.\nOur enhanced model consistently outperformed the baseline\nacross various question segments, achieving an average in-\ncrease of approximately 6.4% in binary F1 score. This im-\nprovement was particularly notable in subjects where se-\nmantic richness in MCQs was critical. The inclusion of\nMCQ-derived semantic embeddings holds promise for im-\nproving representation learning in e-learning environments,\nespecially under cold start conditions. Future research can\nfocus on optimizing these embeddings for diverse educa-\ntional contexts and expanding their applicability.\nConclusion and Future Work\nIn this paper, we initiate the study of performance predic-\ntion for students using learnersourcing platforms. We mod-\neled the relationship between students and questions using\na signed bipartite graph. To mitigate the negative impact of\nnoise in the datasets, which is a significant problem in learn-\nersourcing contexts, we adopted contrastive learning as a\nnovel learning paradigm. Additionally, we attempted to inte-\ngrate both semantic and structural information related to the\nquestions into our models. We evaluated the effectiveness\nof our approach on five real-world datasets from PeerWise,\na widely-used learnersourcing platform. Our experimental\nresults show that our approach achieves more accurate and\nstable predictions of student performance over several other\nbaseline methods.\nFor future work, we aim to utilize our performance pre-\ndiction model to enhance personalized learning experiences\non learnersourcing platforms. Our plan involves developing\na recommendation system for dynamically tailoring ques-\ntion suggestions based on individual student strengths and\nweaknesses. Exploring adaptive feedback mechanisms and\nreal-time interventions based on predicted performance will\ncontribute to a more effective and engaging learning envi-\nronment.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23238\nAcknowledgements\nThe research is supported by the Marsden Fund Council\nfrom Government funding (MFP-UOA2123), administered\nby the Royal Society of New Zealand.\nReferences\nAbdi, S.; Khosravi, H.; and Sadiq, S. 2021. Modelling\nLearners in Adaptive Educational Systems: A Multivari-\nate Glicko-Based Approach. In LAK21: 11th International\nLearning Analytics and Knowledge Conference, LAK21,\n497–503. New York, NY , USA: Association for Computing\nMachinery. ISBN 9781450389358.\nBrown, T.; Mann, B.; Ryder; et al. 2020. Language models\nare few-shot learners. Neurips, 33: 1877–1901.\nDaud, A.; Aljohani, N. R.; Abbasi, R. A.; Lytras, M. D.; Ab-\nbas, F.; and Alowibdi, J. S. 2017. Predicting student perfor-\nmance using advanced learning analytics. In Proceedings of\nthe 26th international conference on world wide web com-\npanion, 415–421.\nDenny, P.; Luxton-Reilly, A.; and Hamer, J. 2008. The Peer-\nWise system of student contributed assessment questions. In\nProceedings of the tenth conference on Australasian com-\nputing education-Volume 78, 69–74. Citeseer.\nDenny, P.; Sarsa, S.; Hellas, A.; and Leinonen, J. 2022. Ro-\nbosourcing Educational Resources – Leveraging Large Lan-\nguage Models for Learnersourcing. arXiv:2211.04715.\nDerr, T.; Ma, Y .; and Tang, J. 2018. Signed graph convolu-\ntional networks. In 2018 IEEE International Conference on\nData Mining (ICDM), 929–934. IEEE.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDu, Y .; Zhu, X.; Chen, L.; Fang, Z.; and Gao, Y . 2022.\nMetakg: Meta-learning on knowledge graph for cold-start\nrecommendation. IEEE Transactions on Knowledge and\nData Engineering.\nHuang, J.; Shen, H.; Cao, Q.; Tao, S.; and Cheng, X. 2021.\nSigned Bipartite Graph Neural Networks. In Proceedings of\nthe 30th ACM International Conference on Information &\nKnowledge Management, 740–749.\nJeevamol, J.; and Renumol, V . 2021. An ontology-based hy-\nbrid e-learning content recommender system for alleviating\nthe cold-start problem. Education and Information Tech-\nnologies, 26: 4993–5022.\nJiang, S.; Williams, A.; Schenke, K.; Warschauer, M.; and\nO’dowd, D. 2014. Predicting MOOC performance with\nweek 1 behavior. In Educational data mining 2014.\nJung, J.; Yoo, J.; and Kang, U. 2020. Signed Graph Diffu-\nsion Network. arXiv preprint arXiv:2012.14191.\nKhosravi, H.; Demartini, G.; Sadiq, S.; and Gasevic, D.\n2021. Charting the Design and Analytics Agenda of Learn-\nersourcing Systems. In LAK21: 11th International Learn-\ning Analytics and Knowledge Conference, LAK21, 32–42.\nNew York, NY , USA: Association for Computing Machin-\nery. ISBN 9781450389358.\nKhosravi, H.; Kitto, K.; and Williams, J. J. 2019. RiPPLE:\nA crowdsourced adaptive platform for recommendation of\nlearning activities. arXiv preprint arXiv:1910.05522.\nKim, J.; Park, H.; Lee, J.-E.; and Kang, U. 2018. Side: repre-\nsentation learning in signed directed networks. In Proceed-\nings of the 2018 World Wide Web Conference, 509–518.\nKipf, T. N.; and Welling, M. 2016. Semi-supervised classi-\nfication with graph convolutional networks. arXiv preprint\narXiv:1609.02907.\nLi, H.; Wei, H.; Wang, Y .; Song, Y .; and Qu, H. 2020. Peer-\ninspired student performance prediction in interactive online\nquestion pools with graph neural network. In Proceedings\nof the 29th ACM International Conference on Information\n& Knowledge Management, 2589–2596.\nLi, M.; Zhang, Y .; Li, X.; Cai, L.; and Yin, B. 2022. Multi-\nview hypergraph neural networks for student academic per-\nformance prediction. Engineering Applications of Artificial\nIntelligence, 114: 105174.\nLiu, T.; Wu, Q.; Chang, L.; and Gu, T. 2022. A review\nof deep learning-based recommender system in e-learning\nenvironments. Artificial Intelligence Review, 55(8): 5953–\n5980.\nLyu, L.; Wang, Z.; Yun, H.; Yang, Z.; and Li, Y . 2022. Deep\nknowledge tracing based on spatial and temporal represen-\ntation learning for learning performance prediction. Applied\nSciences, 12(14): 7188.\nMulryan-Kyne, C. 2010. Teaching large classes at college\nand university level: Challenges and opportunities.Teaching\nin higher Education, 15(2): 175–185.\nNakagawa, H.; Iwasawa, Y .; and Matsuo, Y . 2019. Graph-\nbased knowledge tracing: modeling student proficiency us-\ning graph neural network. In IEEE/WIC/ACM International\nConference on Web Intelligence, 156–163.\nNi, L.; Bao, Q.; Li, X.; Qi, Q.; Denny, P.; Warren, J.; Wit-\nbrock, M.; and Liu, J. 2022. Deepqr: Neural-based quality\nratings for learnersourced multiple-choice questions. InPro-\nceedings of the AAAI Conference on Artificial Intelligence ,\nvolume 36, 12826–12834.\nOord, A. v. d.; Li, Y .; and Vinyals, O. 2018. Representation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nPang, H.; Giunchiglia, F.; Li, X.; Guan, R.; and Feng, X.\n2022. PNMTA: A pretrained network modulation and task\nadaptation approach for user cold-start recommendation. In\nProceedings of the ACM Web Conference 2022, 348–359.\nParamythis, A.; and Loidl-Reisinger, S. 2003. Adaptive\nlearning environments and e-learning standards. In Second\neuropean conference on e-learning, volume 1, 369–379.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing (EMNLP).\nPiech, C.; Bassen, J.; Huang, J.; Ganguli, S.; Sahami, M.;\nGuibas, L. J.; and Sohl-Dickstein, J. 2015. Deep knowledge\ntracing. Advances in neural information processing systems,\n28.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23239\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1): 5485–5551.\nRiggs, C. D.; Kang, S.; and Rennie, O. 2020. Positive im-\npact of multiple-choice question authoring and regular quiz\nparticipation on student learning. CBE—Life Sciences Edu-\ncation, 19(2): ar16.\nSafarov, F.; Kutlimuratov, A.; Abdusalomov, A. B.; Nasi-\nmov, R.; and Cho, Y .-I. 2023. Deep Learning Recommen-\ndations of E-Education Based on Clustering and Sequence.\nElectronics, 12(4): 809.\nSchein, A. I.; Popescul, A.; Ungar, L. H.; and Pennock,\nD. M. 2002. Methods and metrics for cold-start recommen-\ndations. In Proceedings of the 25th annual international\nACM SIGIR conference on Research and development in in-\nformation retrieval, 253–260.\nShu, L.; Du, E.; Chang, Y .; Chen, C.; Zheng, Z.; Xing,\nX.; and Shen, S. 2021. SGCL: Contrastive Representation\nLearning for Signed Graphs. In Proceedings of the 30th\nACM International Conference on Information & Knowl-\nedge Management, 1671–1680.\nShuai, J.; Zhang, K.; Wu, L.; Sun, P.; Hong, R.; Wang, M.;\nand Li, Y . 2022. A review-aware graph contrastive learn-\ning framework for recommendation. In Proceedings of the\n45th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, 1283–1293.\nSohn, K. 2016. Improved deep metric learning with multi-\nclass n-pair loss objective. Advances in neural information\nprocessing systems, 29.\nSun, G.; Cui, T.; Beydoun, G.; Chen, S.; Dong, F.; Xu, D.;\nand Shen, J. 2017. Towards massive data and sparse data in\nadaptive micro open educational resource recommendation:\na study on semantic knowledge base construction and cold\nstart problem. Sustainability, 9(6): 898.\nThaker, K.; Carvalho, P.; and Koedinger, K. 2019. Com-\nprehension Factor Analysis: Modeling student’s reading be-\nhaviour: Accounting for reading practice in predicting stu-\ndents’ learning in MOOCs. In Proceedings of the 9th Inter-\nnational Conference on Learning Analytics & Knowledge ,\n111–115.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\nP.; and Bengio, Y . 2017. Graph attention networks. arXiv\npreprint arXiv:1710.10903.\nVelickovic, P.; Fedus, W.; Hamilton, W. L.; Li`o, P.; Bengio,\nY .; and Hjelm, R. D. 2019. Deep Graph Infomax. ICLR\n(Poster), 2(3): 4.\nWang, X.; Talluri, S. T.; Rose, C.; and Koedinger, K. 2019.\nUpGrade: Sourcing student open-ended solutions to create\nscalable learning opportunities. In Proc. of the Sixth (2019)\nACM Conf. on Learning@ Scale, 1–10.\nWang, Z.; Yan, W.; Zeng, C.; Tian, Y .; Dong, S.; et al. 2023.\nA unified interpretable intelligent learning diagnosis frame-\nwork for learning performance prediction in intelligent tu-\ntoring systems. International Journal of Intelligent Systems,\n2023.\nWei, H.; Li, H.; Xia, M.; Wang, Y .; and Qu, H. 2020. Pre-\ndicting student performance in interactive online question\npools using mouse interaction features. In Proceedings of\nthe tenth international conference on learning analytics &\nknowledge, 645–654.\nXianghan, O.; and Stern, J. 2022. Virtually the same?: On-\nline higher education in the post Covid-19 era. British Jour-\nnal of educational technology, 53(3): 437.\nYe, J.; Chen, X.; Xu, N.; Zu, C.; Shao, Z.; Liu, S.; Cui, Y .;\nZhou, Z.; Gong, C.; Shen, Y .; et al. 2023. A comprehensive\ncapability analysis of gpt-3 and gpt-3.5 series models. arXiv\npreprint arXiv:2303.10420.\nYou, Y .; Chen, T.; Sui, Y .; Chen, T.; Wang, Z.; and Shen,\nY . 2020. Graph contrastive learning with augmentations.\nAdvances in Neural Information Processing Systems, 33:\n5812–5823.\nZhang, Z.; Liu, J.; Zhao, K.; Yang, S.; Zheng, X.; and Wang,\nY . 2023a. Contrastive Learning for Signed Bipartite Graphs.\nIn Proceedings of the 46th International ACM SIGIR Con-\nference on Research and Development in Information Re-\ntrieval, 1629–1638.\nZhang, Z.; Liu, J.; Zheng, X.; Wang, Y .; Han, P.; Wang, Y .;\nZhao, K.; and Zhang, Z. 2023b. RSGNN: A Model-agnostic\nApproach for Enhancing the Robustness of Signed Graph\nNeural Networks. In Proceedings of the ACM Web Confer-\nence 2023, 60–70.\nZhou, J.; Cui, G.; Hu, S.; Zhang, Z.; Yang, C.; Liu, Z.; Wang,\nL.; Li, C.; and Sun, M. 2020. Graph neural networks: A\nreview of methods and applications. AI open, 1: 57–81.\nZhu, Y .; Xu, Y .; Yu, F.; Liu, Q.; Wu, S.; and Wang, L.\n2020. Deep graph contrastive representation learning. arXiv\npreprint arXiv:2006.04131.\nZhu, Y .; Xu, Y .; Yu, F.; Liu, Q.; Wu, S.; and Wang, L. 2021.\nGraph contrastive learning with adaptive augmentation. In\nProceedings of the Web Conference 2021, 2069–2080.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23240",
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.4445904493331909
    },
    {
      "name": "Mathematics education",
      "score": 0.32982438802719116
    }
  ]
}