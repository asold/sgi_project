{
  "title": "Finetuning Pretrained Transformers into Variational Autoencoders",
  "url": "https://openalex.org/W3191529106",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5100762910",
      "name": "Seongmin Park",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5118899059",
      "name": "Jihwa Lee",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2753738274",
    "https://openalex.org/W4391602018",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2964168257",
    "https://openalex.org/W1779483307",
    "https://openalex.org/W3034782902",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970134931",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2587284713",
    "https://openalex.org/W2963600562",
    "https://openalex.org/W2995197202",
    "https://openalex.org/W3005116366",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2951670304",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2988011020",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2978613765",
    "https://openalex.org/W3004665584",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3098708719",
    "https://openalex.org/W2924334974",
    "https://openalex.org/W3119469378",
    "https://openalex.org/W2963275229",
    "https://openalex.org/W2617566453",
    "https://openalex.org/W3105590657"
  ],
  "abstract": "Text variational autoencoders (VAEs) are notorious for posterior collapse, a phenomenon where the model's decoder learns to ignore signals from the encoder. Because posterior collapse is known to be exacerbated by expressive decoders, Transformers have seen limited adoption as components of text VAEs. Existing studies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et al., 2021) mitigate posterior collapse using massive pretraining, a technique unavailable to most of the research community without extensive computing resources. We present a simple two-phase training scheme to convert a sequence-to-sequence Transformer into a VAE with just finetuning. The resulting language model is competitive with massively pretrained Transformer-based VAEs in some internal metrics while falling short on others. To facilitate training we comprehensively explore the impact of common posterior collapse alleviation techniques in the literature. We release our code for reproducability.",
  "full_text": "Proceedings of the Second Workshop on Insights from Negative Results in NLP, pages 29–35\nNovember 10, 2021. ©2021 Association for Computational Linguistics\nISBN 978-1-954085-93-0\n29\nFinetuning Pretrained Transformers into Variational Autoencoders\nSeongmin Park Jihwa Lee\nActionPower\nSeoul, Republic of Korea\n{seongmin.park, jihwa.lee}@actionpower.kr\nAbstract\nText variational autoencoders (V AEs) are no-\ntorious for posterior collapse, a phenomenon\nwhere the model’s decoder learns to ignore\nsignals from the encoder. Because posterior\ncollapse is known to be exacerbated by ex-\npressive decoders, Transformers have seen lim-\nited adoption as components of text V AEs.\nExisting studies that incorporate Transform-\ners into text V AEs (Li et al., 2020; Fang\net al., 2021) mitigate posterior collapse using\nmassive pretraining, a technique unavailable\nto most of the research community without\nextensive computing resources. We present\na simple two-phase training scheme to con-\nvert a sequence-to-sequence Transformer into\na V AE with just ﬁnetuning. The resulting\nlanguage model is competitive with massively\npretrained Transformer-based V AEs in some\ninternal metrics while falling short on others.\nTo facilitate training we comprehensively ex-\nplore the impact of common posterior collapse\nalleviation techniques in the literature. We re-\nlease our code for reproducability1.\n1 Introduction\nProperly tamed latent models offer explainable and\ninterpolatable representations of observed data. Re-\ncent works have shown such models to be espe-\ncially useful in unsupervised learning settings. He\net al. (2019) adapt a generative latent text model for\nsuccessful unsupervised text style transfer and ma-\nchine translation. Li et al. (2020) achieve superior\nlanguage modeling performance against common\nconditional counterparts.\nA popular variant of deep latent models is\nthe variational autoencoder (V AE) (Kingma and\nWelling, 2014). For each observed x, the model\nassumes the existence of a corresponding multi-\ndimensional latent vector z. Since the log evidence\n1https://github.com/seongminp/\ntransformers-into-vaes\nlog p(x) is intractable for most interesting prob-\nlems, the training process for V AEs opts instead to\nminimize the log evidence lower bound (ELBO):\nEz∼q(z|x)[log(p(x|z))]−DKL(q(z|x)||p(z)) (1)\nq(z|x) is a tractable, assumed posterior commonly\nmodeled with a parametrized encoder qφ(z|x),\nwhile p(x|z) is the likelihood parametrized with a\ndecoder pθ(x|z) that optimizes against reconstruc-\ntion loss. While effective in theory, a common\nempirical challenge V AEs present during training\nis posterior collapse – a phenomenon where the\ndecoder ignores the latent signal from z (and thus\nthe originating input) during reconstruction. Pos-\nterior collapse can be diagnosed by checking if\nDKL(q(z|x)||p(z)) tends to zero during training.\nAfter Bowman et al. (2016) adopted V AE for\ntext, subsequent studies have been introduced with\nattempts to mitigate posterior collapse in V AE lan-\nguage models (LMs). However, the brittle training\nprocess of V AE LMs remains an unsolved problem.\nLi et al. (2020) present a method to utilize deep\nTransformer (Vaswani et al., 2017) models as com-\nponents of V AE LMs. Transformer-based V AEs tap\ninto the state-of-the-art capabilities of Transform-\ners while retaining representational advantages of\nV AE LMs. The paper mitigates posterior collapse\nby massive pretraining and a cyclical annealing\nschedule (Fu et al., 2019).\nWhile the study presents a promising outlook\nfor Transformer V AEs, the suggested method is not\naccessible to researchers who lack access to large,\ntarget-domain-speciﬁc corpora or the computing\npower for massive LM pretraining. Therefore, a\ndemand arises for a way to ﬁnetune an existing\nTransformer model into a V AE LM with limited re-\nsources. Our research attempts to ﬁll this gap in the\nliterature, and makes the following contributions:\n• We present a simple but reliable (as replicated\nacross several datasets) scheme to teach latent\n30\nstructure to a pretrained Transformer model\nby just ﬁnetuning.\n• We convert a pretrained sequence-to-sequence\nTransformer into a V AE, instead of using two\nseparate encoder-only (Devlin et al., 2019)\nand decoder-only (Radford et al., 2019) Trans-\nformers as in previous literature. This elimi-\nnates the need to maintain separate tokenizers\nand conﬁgurations for encoder and decoder.\n• We conduct ablation studies and extensive\nexperiments to gauge the effectiveness of\ncommonly used posterior collapse mitigation\nmethods in taming Transformer V AEs.\nThe resulting model extends existing Trans-\nformer architectures and can be initialized from\npretrained non-latent model checkpoints.\n2 Background\n2.1 Transformer Text V AEs\nMost V AE LMs employ recurrent neural networks\n(RNNs) as encoders and decoders. This is in\npart because enforcing a latent bottleneck layer\nundermines the effectiveness of encoder-decoder\ncross-attention in Transformers, and in signiﬁcant\npart due to the co-occurrence of posterior collapse\nand powerful and deep decoder layers. Li et al.\n(2020) overcome such training difﬁculties by mas-\nsively increasing the number of training samples\n(104,213,036 sentences) for LM pretraining.\nLiu and Liu (2019) and Fang et al. (2021) also\nﬁnds success with Transformer V AEs for text gen-\neration. To avoid posterior collapse, Fang et al.\n(2021) follow the exact cyclic KL mitigation ap-\nproach as that of Li et al. (2020), while Liu and Liu\n(2019) introduce noise to network input.\n2.2 Techniques to mitigate posterior collapse\nThis study identiﬁes and explores the effect of\npopular posterior collapse mitigation methods in\nlow-resource Transformer V AE training. We do\nnot examine importance-weighted autoencoders\n(Burda et al., 2016) and semi-amortized autoen-\ncoders (Kim et al., 2018) to limit the scope of our\nexperiments to unsophisticated prior distributions.\n2.2.1 KL Weighting / Annealing\nBowman et al. (2016) increases the KL term of the\nELBO from zero to its full value during early stages\nof training, where the decoder learns to simply treat\nlatent signal z as noise. Fu et al. (2019) extend this\ntechnique by cyclically manipulating the weight of\nthe KL term. β-V AE (Higgins et al., 2017) and Yan\net al. (2020) adopt a similar approach.\n2.2.2 Encoder warm-up (Li et al., 2019)\nWe train the network without the KL term of the\nELBO and retain encoder weights before jointly\ntraining the whole network.\n2.2.3 Input text denoising (Shen et al., 2020)\nDenoising text inputs by deleting random tokens\nmotivate autoencoders (AEs) to learn better latent\nrepresentations. Our study compares 0%, 15%, and\n40% deletion noising schemes.\n2.2.4 KL thresholding (Kingma et al., 2016)\nKL-thresholding enforces a minimum λfor each\ndimension of the KL term in the ELBO:\nLDKL =\n∑\ni\nmax[λ,Dkl(qφ(zi|x)||p(zi))] (2)\nwhere zi is a single dimension of z.\n2.2.5 Encoder pooling (Long et al., 2019)\nInstead of using the last hidden state as encoder\noutput, averaging or taking the maximum of all en-\ncoder hidden states results in a more diverse latent\nrepresentation. We experiment with both mean-\nand max-pooling schemes from the encoder.\n3 Model architecture\nWe extend the T5 architecture (Raffel et al., 2020)\ninto a V AE. We modify a popular pretrained T5\nmodel (Wolf et al., 2020) that deviates minimally\nfrom the original Transformer (Figure 1).\nHidden states from all layers of T5’s encoder\nqφ(z|x) are mean- or max-pooled into a vector\nhpooled ∈RH, where H is the encoder’s hidden\ndimension.\nAssumed prior q(z)’s meanµand log variance\nσvectors of dimension Lis obtained from hpooled:\nµ= hpooledWµ, logσ= hpooledWσ (3)\nwhere Wµ,Wσ ∈RL.\nAs in a standard V AE, a stochastic latent vector\nz is sampled using the reparameterization trick to\nenable back-propagation through sampling:\nz= µ+ σ⊙ϵ, ϵ∼N(0,1) (4)\nWe pass z into the decoder pθ(x|z) as the only\ncomponent of decoder’s cross attention. Our\n31\nFigure 1: Transformer V AE architecture. A \"bottleneck\" step ( Wσ and Wµ) is placed between the encoder and\nthe decoder of T5. Latent information from pooled encoder hidden states is captured in the bottleneck layer\nbefore being passed to the decoder. The network is optimized against regularization loss in the bottleneck and\nreconstruction loss at the decoder.\nmethod injects z into every layer of the decoder\nas in previous literature (Li et al., 2020; Fang et al.,\n2021), but deviates in two important ways: ﬁrst, we\npass z as the sole key and value of encoder-decoder\ncross attention, instead of self-attention; second,\nwe project z into the correct dimension (L×A×S,\nwhere Lis the decoder layer count,Ais the number\nof attention heads, and Sis the embedding dimen-\nsion per head) with a feed-forward network, instead\nof taking a copy of z to inject to each decoder layer.\n(Kca,Vca) = (zWproj,zWproj) (5)\nwhere Wproj ∈RL×A×S and Kca and Vca are\nkey and value in decoder cross-attention.\n4 Experiments\nDuring preliminary experiments, posterior collapse\nwas observed in all training schemes without en-\ncoder warmup training. The decoder learns to ig-\nnore the initially noisy input signal from the en-\ncoder. Thus, we compose our ﬁnetuning method in\ntwo separate phases.\nPhase 1 - Encoder warmup: Weight of KL loss\nis set to zero, making our model’s objective func-\ntion similar to that of an AE. Different input denois-\ning percentages, encoder pooling strategies, latent\ndimension sizes, and decoder freezing conﬁgura-\ntions are compared.\nPhase 2 - Full ﬁnetuning: KL loss is reinstated\nand full V AE training are conducted. We com-\npare different input denoising percentages, encoder\npooling strategies, KL annealing schedules, and\nKL thresholds.\nWe run our proposed two-phase ﬁnetuning train-\ning scheme on four standard V AE LM benchmark\ndatasets: PTB (Marcus et al., 1993), SNLI (Bow-\nman et al., 2016), Yahoo (Yang et al., 2017), and\nYelp (Shen et al., 2017).\nFollowing Li et al. (2020) and Li et al. (2019),\nwe perform intrinsic evaluation of our proposed\nTransformer V AE architecture. We report perplex-\nity (PPL), KL-divergence between model posterior\nand assumed posterior (KL), and negative ELBO\non the test set. To assess the quality of learned la-\ntent codes, we also report mutual information (MI)\n(Hoffman and Johnson, 2016) and the number of\nactive units (AU) (Burda et al., 2016). MI mea-\nsures the dependence of latent codes to encoder\ninput. AU measures the covariance between en-\ncoder input and latent codes.\nExperimental hyperparameters such as speciﬁc\nannealing schedules and training epochs per phase\nare detailed in the appendix.\n5 Results\n5.1 Phase 1\nWe ﬁnd that freezing the decoder and the mem-\nory projection layer Wproj while training with an\nAE objective is crucial in learning meaningful en-\ncoder outputs. Denoising is important for datasets\nwith longer inputs (Yahoo, Yelp), but not critical\nin datasets with shorter input lengths (PTB, SNLI).\nMean-pooling encoder hidden states presents a\ntrade-off between MI and AU. Max-pooling con-\nsistently learns more informative encoder represen-\ntations. Changes in MI and AU during training is\nillustrated in Figure 2.\nLatent dimensions of 64 and 128 were also\ntested. Increasing the latent dimension did not\nnecessarily boost representational quality in terms\n32\nModel PPL↓ KL -ELBO ↓ MI↑ AU↑\nOptimus (λ= 0.5) (Li et al., 2020) 23.11 17.45 301.21 8.85 32\nGPT-2 (Radford et al., 2019) 22.00 - - - -\nEncoder pretraining (λ= 3) (Li et al., 2019) 59.24 7.44 328.73 6.41 32\nOurs (Max pool) 20.90 0.21 343.02 0.04 0\nOurs (Max pool + Denoise) 30.13 41.49 301.86 1.32 24\nOurs (Max pool + Denoise + KLT) 60.44 119.89 223.69 4.73 29\nOurs (Max pool + Denoise + KLT + Deep) 54.40 155.50 140.57 5.43 28\nTable 1: Phase 2 results on Yahoo. Due to space constraints, we report experimental results on other datasets in the\nappendix. Results on baselines are quoted from Li et al. (2020) and (Li et al., 2019). KLT denotes KL thresholding\nwith λ= 3. Our models are ﬁnetuned from a pretrained 6-layer T5, except the deep variant with 12 layers.\nof AU percentage. For latent dimensions of 32 and\n64, 90% of latent dimension units were activated\nin best-performing models. For latent dimension\nof 128, around 60% of latent units were active.\nAnother interesting observation is that KL diver-\ngence on the validation set, although not part of\nthe AE training objective, plateaus after repeated\ntraining. We regard this phenomenon as the signal\nof convergence in terms of representation quality.\nFigure 2: Phase 1 training on Yahoo. Labels\nare in the form { pooling strategy}_{denoise percent-\nage}_{decoder frozen}.\n5.2 Phase 2\nWe observe, as in previous literature, a trade-off be-\ntween language modeling PPL and representation\nquality metrics (MI and AU). This trade-off is ex-\nacerbated when using KL thresholding. While KL\nthresholding does signiﬁcantly increase latent rep-\nresentation capabilities, it is not in itself sufﬁcient\nin preventing posterior collapse.\nDenoising and encoder pooling conﬁgurations\ndisplay the same characteristics as in Phase 1. No\nversion of the experiment existed where cyclical\nannealing schedule was able to prevent posterior\ncollapse, a result not in accordance with Li et al.\n(2020). Figure 3 illustrates the training progression\nof Phase 2.\nWe also experimented with increasing model\ndepth from 6 layers to 12 layers. Our proposed\ntwo-phase training scheme prevents posterior col-\nlapse for deeper models as well, resulting in higher\nperformance in most metrics compared to 6-layer\nmodels. Results are reported in Table 1. Note\nthat lower PPL does not necessarily indicate better\nlanguage modeling capabilities, since models with\ncollapsed posterior display better PPL.\nRows with KL above zero indicate successful\naversion of posterior collapse.\nFigure 3: Phase 2 training on Yahoo. Labels are in the\nform {KL threshold}_{denoise percentage}. Encoder\nhidden states in plotted experiments were max-pooled.\nIn the literature, no consensus yet exists on the\noptimal value of KL in training V AEs. Overall, we\n33\nﬁnd that a denoising scheme between 0.15 and 0.4\nin both phases, coupled with a low (0.5) KL thresh-\nold strikes a good balance between reconstruction\nand latent representation quality.\n6 Conclusions and Future Work\nThis paper explores common methods in the lit-\nerature for combatting posterior collapse, and the\nextent to which they help in teaching latent infor-\nmation to pretrained Transformer models.\nComprehensive experiments show that com-\nmonly employed posterior collapse mitigation tech-\nniques provide meaningful beneﬁts in transforming\nexisting language models into latent-aware archi-\ntectures. Among the tested procedures, we ﬁnd that\nLi et al. (2019)’s two-step training, coupled with\nShen et al. (2020)’s denoising through token dele-\ntion, was the most impactful in mitigating poste-\nrior collapse. However, language models obtained\nvia only ﬁnetuning exhibit consistent trade-offs be-\ntween their latent representation metrics (MI, AU)\nand language model metrics (PPL). Optimizing our\nmodel to be competitive with massively pretrained\nbaselines in one of the two metrics results in the\nmodel falling behind in the other.\nWe also ﬁnd that increasing training epochs fur-\nther improves the impact of tested techniques, a\nresult consistent with previous literature on large-\nscale text V AE pretraining.\nFrom our experiments, we identify several ques-\ntions to be answered by future research. The impact\nof homogenizing ﬁnetuning (as suggested in this\npaper) and original pretraining objectives on lan-\nguage model metrics has to be further explored.\nWhile the original T5 architecture was also pre-\ntrained with a self-supervised denoising scheme,\nthe model employs mask tokens for denoising, con-\ntrary to simple token deletions suggested by this\npaper.\nOur ﬁndings also highlight the need for an es-\ntablished heuristic to interpret the quality of latent\nrepresentations learned by language models. The\nresearch community has yet to decide on the opti-\nmal value of KL-divergence between the assumed\nprior and the model posterior to target during text\nV AE training. Empirical guidelines to dictate even\na vague threshold for the KL-divergence, below\nwhich we declare the occurrence of posterior col-\nlapse, will help both training and evaluation of\nlatent-aware language models.\nReferences\nSamuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew\nDai, Rafal Jozefowicz, and Samy Bengio. 2016.\nGenerating sentences from a continuous space. In\nProceedings of The 20th SIGNLL Conference on\nComputational Natural Language Learning , pages\n10–21.\nYuri Burda, Roger B Grosse, and Ruslan Salakhutdinov.\n2016. Importance weighted autoencoders. In ICLR\n(Poster).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLe Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen\nDong, and Changyou Chen. 2021. Transformer-\nbased conditional variational autoencoder for\ncontrollable story generation. arXiv preprint\narXiv:2101.00828.\nHao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao,\nAsli Celikyilmaz, and Lawrence Carin. 2019. Cycli-\ncal annealing schedule: A simple approach to mit-\nigating kl vanishing. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 240–250.\nJunxian He, Xinyi Wang, Graham Neubig, and Taylor\nBerg-Kirkpatrick. 2019. A probabilistic formulation\nof unsupervised text style transfer. In International\nConference on Learning Representations.\nIrina Higgins, Loïc Matthey, Arka Pal, Christopher\nBurgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, and Alexander Lerchner. 2017. beta-vae:\nLearning basic visual concepts with a constrained\nvariational framework. In 5th International Con-\nference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference\nTrack Proceedings. OpenReview.net.\nMatthew D Hoffman and Matthew J Johnson. 2016.\nElbo surgery: yet another way to carve up the vari-\national evidence lower bound. In Workshop in Ad-\nvances in Approximate Bayesian Inference, NIPS ,\nvolume 1.\nYoon Kim, Sam Wiseman, Andrew Miller, David Son-\ntag, and Alexander Rush. 2018. Semi-amortized\nvariational autoencoders. In Proceedings of the 35th\nInternational Conference on Machine Learning, vol-\nume 80 of Proceedings of Machine Learning Re-\nsearch, pages 2678–2687. PMLR.\n34\nDiederik P. Kingma and Max Welling. 2014. Auto-\nEncoding Variational Bayes. In 2nd International\nConference on Learning Representations, ICLR\n2014, Banff, AB, Canada, April 14-16, 2014, Con-\nference Track Proceedings.\nDurk P Kingma, Tim Salimans, Rafal Jozefowicz,\nXi Chen, Ilya Sutskever, and Max Welling. 2016.\nImproved variational inference with inverse autore-\ngressive ﬂow. In Advances in Neural Information\nProcessing Systems, volume 29. Curran Associates,\nInc.\nBohan Li, Junxian He, Graham Neubig, Taylor Berg-\nKirkpatrick, and Yiming Yang. 2019. A surprisingly\neffective ﬁx for deep latent variable modeling of text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3603–\n3614, Hong Kong, China. Association for Computa-\ntional Linguistics.\nChunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiu-\njun Li, Yizhe Zhang, and Jianfeng Gao. 2020. Opti-\nmus: Organizing sentences via pre-trained modeling\nof a latent space. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4678–4699.\nDanyang Liu and Gongshen Liu. 2019. A transformer-\nbased variational autoencoder for sentence genera-\ntion. In 2019 International Joint Conference on Neu-\nral Networks (IJCNN), pages 1–7. IEEE.\nTeng Long, Yanshuai Cao, and Jackie Chi Kit Cheung.\n2019. Preventing posterior collapse in sequence\nvaes with pooling. arXiv e-prints , pages arXiv–\n1911.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19(2):313–330.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In Proceedings of the 31st Inter-\nnational Conference on Neural Information Process-\ning Systems, NIPS’17, page 6833–6844, Red Hook,\nNY , USA. Curran Associates Inc.\nTianxiao Shen, Jonas Mueller, Regina Barzilay, and\nTommi Jaakkola. 2020. Educating text autoen-\ncoders: Latent representation guidance via denois-\ning. In International Conference on Machine Learn-\ning, pages 8719–8729. PMLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nChaochao Yan, Sheng Wang, Jinyu Yang, Tingyang Xu,\nand Junzhou Huang. 2020. Re-balancing variational\nautoencoder loss for molecule sequence generation.\nIn Proceedings of the 11th ACM International Con-\nference on Bioinformatics, Computational Biology\nand Health Informatics , BCB ’20, New York, NY ,\nUSA. Association for Computing Machinery.\nZichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and\nTaylor Berg-Kirkpatrick. 2017. Improved varia-\ntional autoencoders for text modeling using dilated\nconvolutions. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning - Volume 70,\nICML’17, page 3881–3890. JMLR.org.\n35\nA Phase 2 results on PTB, Yelp, and\nSNLI\nModel PPL↓ KL -ELBO ↓ MI↑ AU↑\nOptimus (λ= 0.5) (Li et al., 2020) 26.69 15.72 96.82 7.64 32\nGPT-2 (Radford et al., 2019) 24.23 - - - -\nEncoder pretraining (λ= 3) (Li et al., 2019) 96.75 3.85 101.56 3.19 32\nOurs (Max pool) 51.60 0.09 104.14 0 0\nOurs (Max pool + Denoise) 57.69 3.05 101.17 11\nOurs (Max pool + Denoise + KLT) 250.44 41.93 62.25 2.17 24\nOurs (Max pool + Denoise + KLT + Deep) 705.73 84.28 4.95 13\nTable 2: Phase 2 results on PTB\nModel PPL↓ KL -ELBO ↓ MI↑ AU↑\nOptimus (λ= 0.5) (Li et al., 2020) 22.79 15.09 344.10 9.13 32\nGPT-2 (Radford et al., 2019) 23.40 - - - -\nEncoder pretraining (λ= 3) (Li et al., 2019) - - - - -\nOurs (Max pool) 21.65 0.25 404.54 0 0\nOurs (Max pool + Denoise) 39.09 77.85 327.17 1.06 26\nOurs (Max pool + Denoise + KLT) 86.71 182.24 223.34 5.46 27\nOurs (Max pool + Denoise + KLT + Deep) 53.05 178.48 166.15 5.55 10\nTable 3: Phase 2 results on Yelp.\nModel PPL↓ KL -ELBO ↓ MI↑ AU↑\nOptimus (λ= 0.5) (Li et al., 2020) 16.67 16.35 38.50 8.89 32\nGPT-2 (Radford et al., 2019) 20.24 - - - -\nEncoder pretraining (λ= 3) (Li et al., 2019) 21.23 5.86 33.87 5.25 32\nOurs (Max pool) 12.79 0.11 34.61 0.06 0\nOurs (Max pool + Denoise) 15.16 2.41 32.32 0.194 7\nOurs (Max pool + Denoise + KLT) 85.88 26.12 8.62 0.88 13\nOurs (Max pool + Denoise + KLT + Deep) 2358.31 74.95 43.77 5.27 17\nTable 4: Phase 2 results on SNLI.\nB Experimental details\nFor all experiments we used a AdamW optimizer\n(Loshchilov and Hutter, 2019) with a starting learn-\ning rate of 1 ×10−3, β1 = 0.9, β2 = 0.999, and\nϵ= 1×10−3. The linear KL annealing schedule\nwe used was as follows:\nKLweight = currentglobalstep\nstepsperepoch ∗50 (6)\nOur slower, linear KL annealing schedule of 0 to\n1 over 50 epochs yielded better empirical results\nthan the linear schdule used in Li et al. (2019) (0\nto 1 over 10 epochs). We attribute this result to the\nsmall number of training samples in our ex-\nperiments.\nWe train for 5 epochs on Phase 1, and 3 epochs\non Phase 2. While further training leads to in-\ncreased MI and AU, we limit the number of epochs\nto confer to the spirit of this study, which is to learn\nlatent representations with minimal training. The\n5 epoch limit on Phase 1 was empirically deter-\nmined as the point where encoder MI begins to\nplateau. Most experiments were conducted with\nz dimension of 32 for comparison with previous\nliterature.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7798054814338684
    },
    {
      "name": "Computer science",
      "score": 0.778620719909668
    },
    {
      "name": "Encoder",
      "score": 0.7092610597610474
    },
    {
      "name": "Decoding methods",
      "score": 0.5113748908042908
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4910712242126465
    },
    {
      "name": "Language model",
      "score": 0.41551387310028076
    },
    {
      "name": "Natural language processing",
      "score": 0.32015731930732727
    },
    {
      "name": "Algorithm",
      "score": 0.20670664310455322
    },
    {
      "name": "Voltage",
      "score": 0.14095035195350647
    },
    {
      "name": "Engineering",
      "score": 0.09661155939102173
    },
    {
      "name": "Electrical engineering",
      "score": 0.09449034929275513
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 11
}