{
  "title": "Implementing an Intelligent Collaborative Agent as Teammate in Collaborative Writing: toward a Synergy of Humans and AI",
  "url": "https://openalex.org/W3126655779",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3025188097",
      "name": "Christina Wiethof",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2597930989",
      "name": "Navid Tavanapour",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231300341",
      "name": "Eva Bittner",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6694371765",
    "https://openalex.org/W6794522071",
    "https://openalex.org/W1501005121",
    "https://openalex.org/W2742558454",
    "https://openalex.org/W2300445845",
    "https://openalex.org/W2901418095",
    "https://openalex.org/W6646504800",
    "https://openalex.org/W2754021190",
    "https://openalex.org/W6683034941",
    "https://openalex.org/W2947567127",
    "https://openalex.org/W2928158432",
    "https://openalex.org/W2780734486",
    "https://openalex.org/W6686147204",
    "https://openalex.org/W2186181532",
    "https://openalex.org/W2413768264",
    "https://openalex.org/W1276866904",
    "https://openalex.org/W2311045093",
    "https://openalex.org/W1991015565",
    "https://openalex.org/W6772494849",
    "https://openalex.org/W159389675",
    "https://openalex.org/W2955337149",
    "https://openalex.org/W6760195395",
    "https://openalex.org/W2593451807",
    "https://openalex.org/W6659878960",
    "https://openalex.org/W2773244891",
    "https://openalex.org/W6757693662",
    "https://openalex.org/W1578885999",
    "https://openalex.org/W2195768362",
    "https://openalex.org/W7056106223",
    "https://openalex.org/W6644705102",
    "https://openalex.org/W2904767181",
    "https://openalex.org/W2787471476",
    "https://openalex.org/W2124566135",
    "https://openalex.org/W7002381323",
    "https://openalex.org/W2036936877",
    "https://openalex.org/W2273337586",
    "https://openalex.org/W1976819742",
    "https://openalex.org/W2156897926",
    "https://openalex.org/W3216596206",
    "https://openalex.org/W1984142299",
    "https://openalex.org/W187505942",
    "https://openalex.org/W4254114958",
    "https://openalex.org/W2151020819",
    "https://openalex.org/W2919272874",
    "https://openalex.org/W3150393589",
    "https://openalex.org/W2918027211",
    "https://openalex.org/W2740237533",
    "https://openalex.org/W1501419648",
    "https://openalex.org/W2908739919",
    "https://openalex.org/W3121940952",
    "https://openalex.org/W2921314419"
  ],
  "abstract": "This paper aims at implementing a hybrid form of group work through the incorporation of an intelligent collaborative agent into a Collaborative Writing process. With that it contributes to the overall research gap establishing acceptance of AI towards complementary hybrid work. To approach this aim, we follow a Design Science Research process. We identify requirements for the agent to be considered a teammate based on expert interviews in the light of Social Response Theory and the concept of the Uncanny Valley. Next, we derive design principles for the implementation of an agent as teammate from the collected requirements. For the evaluation of the design principles and the human teammates’ perception of the agent, we instantiate a Collaborative Writing process via a web-application incorporating the agent. The evaluation reveals the partly successful implementation of the developed design principles. Additionally, the results show the potential of hybrid collaboration teams accepting non-human teammates.",
  "full_text": "Implementing an Intelligent Collaborative Agent as Teammate in \nCollaborative Writing: toward a Synergy of Humans and AI \n \n \nChristina Wiethof \nUniversity of Hamburg \nwiethof@informatik.uni-\nhamburg.de \nNavid Tavanapour \nUniversity of Hamburg \ntavanapour@informatik.uni-\nhamburg.de \nEva A. C. Bittner \nUniversity of Hamburg \nbittner@informatik.uni-\nhamburg.de \n \n \nAbstract \nThis paper aims at implementing a hybrid form of \ngroup work through the incorporation of an intelligent \ncollaborative agent in to a Collaborative Writing \nprocess. With that it contributes to the overall research \ngap establishing acceptance of AI to wards \ncomplementary hybrid work. To approach this aim, we \nfollow a Design Science Research process. We identify \nrequirements for the agent to be considered a teammate \nbased on expert interviews in the light of Social \nResponse Theory and the concept of the Uncanny \nValley. Next, we derive design principles for the \nimplementation of an agent as teammate from the \ncollected requirements. For the evaluation of the design \nprinciples and the human teammates’ perception of the \nagent, we instantiate a Collaborative Writing process \nvia a web -application incorporating the agent. The \nevaluation reveals the partly successful implementation \nof the developed design principles. Addi tionally, the \nresults show the potential of hybrid collaboration teams \naccepting non-human teammates. \n \n \n1. Introduction  \n \nResearch on  Artificial Intelligence (AI) is \nincreasingly progressing shown by many new evolving \ntechnologies. Here, researchers mainly  work on \nquestions of effectiveness and efficiency regarding their \nnewest developments [1]. Especially in the field of \nMachine Learning (ML) researchers aim to create an AI, \nwhich resembles Human Intelligence and could \nconsequently replace a human being [2]. Thereby, they \nfocus on an automatic learning approach [3] resulting in \nintelligent, autonomous systems. In certain domains \nwith a huge amount of training data, this approach has \nalready been successfully recognized [1, 4, 5]. \nHowever, it is known that technology is not \neverything [6]. Researchers aim to achieve a synergy of \nboth humans and AI, i.e. combining the benefits and \nadvantages of both [2, 7–10]. Therefore, also the human \nusers’ social perspective [11] is required. Even the best \nstate-of-the-art technology will be useless , if its human \nusers refuse it  [6]. This also applies to ML approaches \nthemselves, especially when they involve human users \nin the training, e.g. Reinforcement Learning or Human-\nin-the-Loop [1, 5, 12]. Thus, to achieve that synergy of \nworking together and complementing as well as \nlearning from each other , the human needs to accept a  \ncollaborative agent willing to learn from its \ncontributions as well as to make corrections and \nimprove the agent [2]. It should be pointed out, that  \nthroughout the paper  we use the term  agent for any \ncollaborative agent and intelligent computer ag ent \nrespectively “[covering] the idea of creating machines \nthat can accomplish complex goals [including] facets \nsuch as natural language processing, perceiving objects, \nstoring of knowledge and apply ing it for solving \nproblems” [8] in collaboration settings. \nAs there is an advantage of combining human and \nartificial intelligence to achieve better collaboration \noutcomes [2, 8], the research gap and need for designing \nand developing such socio-technological teams has been \ndisclosed [8]. We therefore consider s ocio technical \nfactors of agent teammates  and exemplify the intended  \nsynergy by regarding hybrid teams  involving humans \nand agents. To specifically contribut e and extend the \nscope of this research, Dellermann et al. [8] call for \nmore research on practical applications in different \ndomains. For instance, Bittner et al. [13] developed a \ntaxonomy for conversational agents in collaborative \nwork. Epstein [9], on the other hand, investigated a \ncollaborative intelligence sharing a task with a person to \ndemonstrate the po tential synergy of humans and  \nagents. Eventually, “rather than re-design our world for \ncomputers or submit to their decisions, we should begin \nto share our tasks with them” [9]. As we found a study, \nwhich revealed that an  agent is capable of replacing \nactual human journalists [14], for our research at hand, \nwe specifically regard a Collaborative Writing (CW) \nscenario. After all, there could rather be an advantage in \nProceedings of the 54th Hawaii International Conference on System Sciences | 2021\nPage 400\nURI: https://hdl.handle.net/10125/70658\n978-0-9981331-4-0\n(CC BY-NC-ND 4.0)\n\nthe collaboration of an agent and human writers. For one \nthing, a n agent may have more memory space and a \nhigher computation rate as well as challenges the writer \nand promotes the writing process. For another thing, \nagents do not reach humans’ skills and knowledge  yet. \nThus, by co-writing, the skills of the agent as well as the \nwriter affect both the outcome as well as each other \ncomplementary. Manjavacas et al.  [15] addressed this \nby developing an intelligent text generation system, \nwhich produces sentences or paragraphs to enable co -\ncreation and CW between an author and an agent [15]. \nBy doing this, agents contribute with stor y fragments \nand ideas, which the human collaborator might not be \naware of  [16]. Additionally, computational creativity \nitself has already achieved several successes, e.g. \nNarrative Science [16], poetry [17], storytelling [18] or \nmelodic accompaniments for lyrics [19]. \nStill, “as machines evolve from tools to teammates, \none thing is clear: accepting them will be more than a \nmatter of simply adopting new technology”  [6]. By \nfostering co-creativity in CW within a hybrid team, we \nexamine the possibility of perceiving an agent as \nteammate [15]. This will enable further research on \nimplementing hybrid forms of group work covering \nmutual learning benefits and acceptance. With that we \naim to contribute to the overall research gap establishing \nacceptance of computer agents toward complementary \nhybrid work  [2, 8, 10] . Therefore, we are conducting \ndesign science research to implement an agent into a \ncollaborative writing process as teammate  [10]. By \ndoing this, we address three research questions: Q1: \nWhat are the requirements to ensure acceptance of  an \nagent as teammate in CW? Q2: How can an agent be \ndesigned and implemented as teammate contributing to \nthe goal of the CW process ? Q3: How do the human \nteammates perceive and accept the contributions of the \nagent?  \nTo support CW, we develop a CW process with a n \nagent teammate and implement it on a web platform. \n \n2. Research Approach  \n \nThe research aims to contribute prescriptive design \nknowledge to the knowledge base by connecting the \nresearch areas of Human -Computer-Interaction and  \nSocio-Technical Systems  to design a solution for the \nincorporation of a n agent teammate into a CW process \n[20]. In coherence with the design science research \n(DSR) approach, the DSR process by Peffers et al. [21] \nis used to derive design principles (DPs), which are then \nimplemented and evaluated with an instantiated CW \nprocess in form of a web-application (see Figure 1) [21]. \nThe problem identification and motivation are covered \nin the introduction. To define the objectives of the \nsolution, meta -requirements (MRs) for a n agent \nteammate are identified. This includes any pers onality \ntraits and skills, that need to be assigned to a n agent to \nbe considered and accepted as teammate. To do so, we  \nfirst consider related work from areas focusing on \nmachines as teammates and hybrid teams as wel l as \nsocio technical factors of agents. We then base our MRs \non the Social Response Theory by Nass and Moon [22] \naligned to the concept of the Uncanny Valley by Mori \n[23], and conduct expert interviews according to the \napproach of Meuser and Nagel [24]. For the design and \ndevelopment, the MRs are considered to derive DPs of \nan agent teammate, which are later on implemented. \nAfter the implementation of the agent in a CW process, \na demonstration is carried out by instanti ating the CW \nprocess in form of a prototypic we b-application \nincorporating the agent [25]. Four groups of five \nparticipants took  part in a test run [26] and in expert \ninterviews [24] to evaluate the human teammates’ \nperception and acceptance toward s the contributions of \nthe agent ex post [27]. Communication wil l be \ncompleted with this paper.  \n \n \n \nFigure 1. Structure along the DSR process \nPage 401\n3. Related Work  \n \nCombining the strengths of humans and agents in \ncollaborative work is not easy and neither is it enough \nto make a good collaboration team. Humans still thin k \nof technology as a tool, but need to consider and accept \nit a s teammate of a hybrid group  [6, 10] . Therefore, \nresearch considers social science findings about \nbehaviors or attitudes toward humans and applies them \nfor agents. The “computers are social actors” (CASA)  \nparadigm introduces the relevance of assigning human \ncharacteristics, and social cues  respectively, to  agents \n[11] encouraging their acceptance [6, 13, 28, 29] . \nConsidering the Uncanny Valley [23] and the balance of \nsocial cues a nd competence, researchers examined the \nleast actual capabilities of an agent, which are to \nunderstand its teammates and to react appropriately with \nadequate length. Eventuall y, the outcome depends on \nthe co ntributions of each member  including the agent \n[30–33].  \nNext, researchers consider the aspect of \ntransparency fostering the understanding of an agent, its \nbehavior and purpose to accept it as a teammate [29, 34, \n35]. This allows its human teammates to still critize and \nimprove it [34], which eventually ensures a certain \nfeeling of control as well as an enhancement of the \ngroup process and its outcomes [10, 29, 31, 36] . For \ninstance, Gnewuch et al. [33] demand to include error -\nhandling strategies considering potential \nmisunderstandings. Also, Frick [6] suggests to give the \nhuman teammates the possibility to influence the \ncomputer algorithm’s output . Still, due to the fact that \nmuch of today’s technology including its technical \ndetails and mechanisms are very complex, humans \ncannot rely on a full system transparency and \nunderstanding to accept an agent teammate [37–39]. \nTherefore, it is recommended to  establish trust  and \nacceptance right at the beginning. Andras et al. [40] \nsuggest making use of an explainable AI. An \nexplainable AI will introduce itself in advance of \nstarting the hybrid collaboration process. Thereby it will \ngive its teammates insights into its behavior covering \nthe how and the why [40].  \nAt last, considering the enhancement of the process \noutcomes, res earchers found out, that  agents can \ncontribute to group creativity effects and concurrently \navoid negative effects including social loafing and free-\nriding, evaluation apprehension and production \nblocking by contributing with its own decisions [36]. \nHowever, the competence of an  agent is not to be \nneglected. It involves the knowledge, abilities and skills \nof a teammate to satisfy the expectations of the other  \nteammates. These expectations refer to the performance, \nspecifically the contributions toward the tea m goal \nwithin the teamwork [35, 37, 38, 41].  \nAs of our research at hand, we aim at the acceptance \nof an agent as teammate within a hybrid form of group \nwork, specifically CW. Therefore , we consider these  \nfindings toward the acceptance of  agents in human -\ncomputer-interactions, i.e. the application of social cues \nin terms of CASA and the Uncanny Valley. \n \n4. Theoretical Background \n \nIn terms of accepting agents, many researchers and \npractitioners refer to human characteristcs, and social \ncues respectively [6, 13, 28, 29, 33] . Here, Nass and \nMoon [22] developed the Social Response Theory  \nbased on several previous studies, among others around \nthe CASA paradigm , demonstrating the mindless \napplication of social rules and expectations to \ncomputers. With that, they disclose the application of \nhuman social categories, social behaviors as well as \npremature c ognive commitments to computers, and \nrefute alternative explanations like anthropomorphism \nand intentional responses for their studies . They state \nthat “inviduals are responding mindlessly to computers \nto the extent that the y apply social scripts […] that are \ninappropriate for human -computer interaction ”. \nTherefore, “individuals must be presented with an \nobject that has enough cues to lead the person to \ncategorize it as worthy of social responses, while also \npermitting indivi duals who are sensitive to the enti re \nsituation to note that social behaviors were clearly not \nappropriate” [22]. Thus, social cues assigned to an agent \ntrigger humans to apply social behaviors and rules \ntowards the agent [11]. Such social cues could be a \nname, emotions [6] or also typing indicators [42]. The \nlatter also addresses the concept of social presence [42], \ni.e. an agent is perceived as socially present, aware and \nconscious [32]. Still, next to the Social Response \nTheory, researchers also refer to the concept of the \nUncanny Valley by Mori [23] reasoning the application \nof less social cues in order to match the human likeness \nwith competence for maximum affinity [23, 33, 42]. As \nit is quite easy to generate a social relationship between \nhumans and computers, it is recommended to make use \nof rudimentary but powerful cues instead of developing \nhighly complex agents [11]. \n \n5. Objectives of the Solution \n \nTo derive MRs for an agent as teammate from theory \nand real-live problems, we conducted semi-structured \nqualitative expert interviews  along the approach by \nMeuser and Nagel  [24] and analyzed them in the light \nof the theoretical background [22, 23]. For the selection \nof experts  (E1-E9), we considered ni ne diverse \nresearchers from the  fields of Information and \nPage 402\nKnowledge Technology, Human-Computer-Interaction, \nPsychology and Sociology.  The interview guideline \nincluded interdisciplinary open questions to reveal the \nexperts’ insights in a r eliable and unbiased way. The \nquestions a sked covered 1) socio -technical factors \nwithin human-human- and human-machine-interaction, \n2) agents influencing human -machine-interaction with \nsocio-technical factors, 3) desires, demands and \nanxieties toward the application of agents, and 4) vision \nand future prospect about the interaction between agents \nand humans.  To analyze the expert interviews , a \nthematic comparison was conducted along categories \n[24]. The categories were determined inductively after \nan initial scanning of the interview transcripts. Thus, the \ninformation from the interviews could be extracted and \nseparated into the following categories: C ompetence, \nSocial Cues and Feedback. Consequently, the experts’ \nrelevant remarks were extracted, merged and collocated \nalong the established categories . Eventually, we  \nconnected the expert references to the Social Resp onse \nTheory [22] and the concept of the Uncanny Va lley to \nderive the MRs [23]. \nCompetence: an agent is not expected to have a \ngeneral human intelligence, but to have a certain \nexpertise in the application area . As such, the agent \nshould be able to enhance and contribute to the group \nprocess and its outcomes (MR1)  with all the required \nskills (MR2)  (E1, E2, E4 -7). Accordingly, its \ninteractions within the group should be transparent, easy \nto understand and intuitive  through an intelligible \ndisplay (MR3) (E3, E4, E7-9). Referring to the Uncanny \nValley, this display does not ha ve to be utterly human. \nIn fact, too much human likeness might raise higher \nexpectations toward the competence of the agent (E2, \nE4-7, E9). Eventually, the human teammates should \nhave the right expectations and know that their agent \nteammate acts in their interest (MR4) (E2, E4-7, E9). \nSocial Cues: specifically regarding the appearance \nof an agent, referring to Social Response Theory, it is \nrecommended to assign some humanness to the agent, \ne.g. a name, a face or an emoticon (MR5), as long as the \ncomplexity of the agent’s functionality matches the \ncomplexity of its appearance (MR6) (E2, E4-7, E9). To \nfurther encourage social presence (MR7) in the light of \nSocial Response Theory in terms of social cues, \ngraphical typing indicators within the team interactions  \nare useful (MR8). Additionally, as it is beneficial to \ninitially establish tr ust and an emotional relation ship \nbetween the humans and the agent in order to jointly \nwork toward a common goal (E2, E4, E5), transparency \nabout the agent’s purpos e and processe s is required \n(MR9). Therefore, the schema of childlike \ncharacteristics might be of interest (E4). Hence, an agent \nintroduces itself and asks for support within the \ncollaboration (MR10) (E3, E7). As a result, the human \nteammates do not  expect the  agent to not make any \nmistakes. This approach resembles self -deprecation, \ne.g. the agent knows, that it is an agent (E5).  \nConsidering Social Response Theory, making use of an \nexplainable AI with self -depreciating and childlike \ncharacteristics aims at enocuraging social responses \ntoward the agent leading to a closer and more emotional \nrelationship. \nFeedback: in the light of Social Response Theory, \nthere are a few underlying characteristics of an agent, \nwhich trigger humans’ social responses. The first aspect \nis inter activity covering responses based on inputs. \nDespite the writing process, human teammates should \nhave the possibility to  understand and control the \nsituation (MR11), i.e. they are able to give feedback and \ninfluence (MR12) or even intervene, rectify and amend \nthe ag ent’s contributions at any time  (MR13) (E2-5). \nTherefore, the agent’s contributions need to be exposed \nfor criticism and improvement (MR14 ). This feature is \ncrucial for mutual learning benefits of both humans and \nagent. Following, an agent teammate should also show \nan interest in the human teammates. This is possible by \ngiving it the  same ability to give f eedback (E1, E2 ) \ncovering a second aspect for social responses: the filling \nof human roles. Therefore, the agent needs to react \nappropriately (M R15) by making the right decisions \n(MR16). With that, all teammates  should be able to  \nequally contribute to the process outcome (MR17). \nTable 1 includes all identified MRs as objectives of \nthe solution.  \n \nTable 1. MRs with description and expert \nreference \nMeta-requirements Expert \nreference \nMR1: The agent enhances the group \nprocess and its outcomes. \nE1, 2,  \n4-7 \nMR2: The agent has all skills to \ncontribute to the team goal. \nE1, 2,  \n4-7 \nMR3: The display of the agent is \nintelligible for its teammates. \nE3, 4,  \n7-9 \nMR4: The human teammates have the \nright expectations and know that th e \nagent teammate acts in their interest. \nE1-7, 9 \nMR5: The agent is humanoid owning \na name and lifelike characteristics. \nE2, 4-7, 9 \nMR6: The agent remains a balance of \nsocial cues and competence. \nE2, 4-7,  \n9 \nMR7: The agent is perceived as \nsocially present. \nE2, 4-7, 9 \nMR8: Graphical typing indicators are \ninvolved within the team interactions. \nE2, 4-7, 9 \nPage 403\nMR9: The agent’s purpose and \nprocesses are transparent  and \ndisclosed via an informative opening \nmessage. \nE3-5, 7 \nMR10: The agent is an explainable AI \nintroducing itself in advance. \nE3-5, 7 \nMR11: The human teammates \nunderstand the situation and retain \ncontrol. \nE2-5 \nMR12: The human teammates are \nable to influence the agent teammate’s \noutput. \nE2-5 \nMR13: The human teammates require \nerror-handling strategies for \ninterventions. \nE2-5 \nMR14: The agent’s contributions are \nexposed for criticism and \nimprovement. \nE2-5 \nMR15: The agent reacts \nappropriately. \nE1, 2 \nMR16: The agent is able to make \ndecisions. \nE1, 2 \nMR17: All teammates equally \ncontribute to the process outcome. \nE1, 2 \n \n6. Artifact Design and Development  \n \nBased on the MRs, preliminary action oriented DPs \ntoward the incorporation of an agent into a CW process \nas teammate were developed according to Chandra et al. \n[43] (Table 2).  \n \nTable 2. DPs with corresponding MRs \nDesign Principles (DP) Source \nDP1: Provide the agent with the capability \nof domain-specific natural language \nprocessing (NLP) in order for the human \nteammates to feel understood and obtain \nappropriate contextual contributions, \ngiven that its knowledge is trained, but \nlimited to the context of the teamwork \napplication. \nMR15, \nMR16 \nDP2: Provide the agent with a \ncontrollability in order for the human \nteammates to have the opportunity to \nintervene and rectify its contributions, \ngiven that the modified new contribution \nof the machine teammate is qualitatively \nbetter and more suitable in regard to th e \ngroup goal. \nMR11, \nMR12, \nMR13, \nMR14 \nDP3: Provide the agent with the ability to \nreact based on the human teammates’ \ncontributions by giving feedback to each \nindividual contribution in order for the \nhuman teammates to perceive it as \nsocially present, aware and conscious.  \nMR1, \nMR2, \nMR7, \nMR15, \nMR16, \nMR17 \nDP4: Provide the agent with explainable \ncapabilities introducing itself including \npurpose and processes in advance in order \nfor the human teammates to have the right \nexpectations and to understand and accept \nthe agent teammate, given that it is still not \nperfectly trained and may not make \nappropriate and useful contributions. \nMR3, \nMR4, \nMR9, \nMR10, \nMR11 \nDP5: Provide the agent with a humanoid \nidentity and social cues  in order for the \nhuman teammates to percei ve it as an \nequally social teammate , given a balance \nof social cues and competence. \nMR3, \nMR5, \nMR6, \nMR7, \nMR8 \n \nTo support CW, we developed a CW process and \nimplemented it on a web platform. The process enables \nthe participants to collaborate in writing a story. We use \nthe process to design and implement an agent as \nteammate according to the DPs (Q2). The process steps \nand activities incorporating the agent are as follows. \n1) Prepare \nThe agent introduces and presents itself right at the \nbeginning to clarify its intended role as a teammate. It \nexplains how it generally works for transparency. Next \nto its name it also has a picture (DP2, DP4, DP5). \n2) Write Sentence \nAfter that, the iterative part of the process starts: the first \nparticipant writes a sentence, which ex tends the story. \nHere, the agent is included in the order of the \nparticipants. When it is its turn, the agent processes the \nlast written sentence to generate a new and contextual \nappropriate senten ce contributing to the story  like its \nhuman teammates. In d oing so, it also takes some time \nto generate the next sentence. In this waiting period \ngraphical typing indicators show up (DP1, DP5). \n3) Extend Story with / without Reaction \nThere are then three exclusive activities to follow: either \nclaiming, liking or not reacting to the contributed \nsentence. Exactly like its human teammates, the agent \ncan react to the writer by showing that it li kes the \nreleased sentence (DP3). The claim-functionality is only \navailable for the human teammates: thereby, they can \nintervene a nd demand a rectification o f the released \nsentence of the agent. The agent then generates a better \nand more suitable new contribution (DP2). \n4) Completion \nAfter the first participant’s turn, the next participant in \nline has a turn and  writes a sentence. The p rocess ends \nPage 404\nwhen the participants consider the story complete. \nAdditionally, for the overall process, the agent  has a  \npicture and a name, which is used all along (DP5). \n \n7. Demonstration  \n \nTo assess the incorporation of the DPs (Q2) as well \nas to eval uate the user perception of the agent (Q3) an \ninstantiation of the collaborative writing process was \ndeployed in form of a web-application (Figure 2). This \nis done by means of Prototyping: developing and \nevaluating a standalone version, which is quickly \navailable. It is typically limited to the functionalities, \nwhich are relevant for the research involving for one \nthing the feasibility  and for another thing the user \nperception [25].  \n \n \nFigure 2. Lobby of the web-application:  \nintroduction of the collaborative agent \n \nThe DPs were accordingly implemented as follows: \nCapability of domain-specific NLP (DP1): For the \nagent to generate contextually appropriate sentences for \nthe story, it needs to refer to and process the preceding \nstory fragment. The agent is therefore provided with the \ncapability of NLP using Recurrent Neural Networks. \nThereby, a word -level language model is developed to \npredict the probability of the next word in a sentence \nbased on the previous words.  \nClaim Functionality (DP2 ): In case the human \nteammates are not satisfied with the contribution of the \nagent, they have the opportunity to intervene and claim. \nThen, the agent has a second chance to rectify its \ncontribution by replacing its generated sentence with a \nnew one. The human teammates therefore have an \naction panel. Here, they can choose a reaction to each \nsentence contributed to the story. If they want to claim, \nthey can choose the “Claim”-Button. For a qualitatively \nbetter and more suitable new contribution, the second \noutput of the agent is strictly limite d by hard-coding in \nterms of prototyping. In doing so, grammatically correct \nsentences giving neutral descriptions , which are likely \nto fit into any story, can be provided.  \nLike Functionality (DP3 ): Just like its human \nteammates, the agent is able to reac t by liking the \ncontributed sentences. As soon as the agent liked a \nsentence, the human teammates receive a pop-up, which \nstates “Andre like the sentence!”. The decision on \nwhether the agent likes a sentence or not is made \nrandomly. In terms of prototyping  this is a fast and \neffective way to implement the functionality for the test \nrun in order to be evaluated. \nExplainable AI (DP4): The introduction is used to \nset the right expectations a nd foster the acceptance of \nthe agent. Here, the agent presents and explains itself. It \nreveals that it may not contribute appropriate sentences \nto the story as it is new in this field and still has to learn \na lot. However, it is positive and motivated towards its \nhuman teammates (Figure 2). \nIdentity and Social Cues (DP5): To merge into the \nteam as social teammate, the agent is assigned to an \nidentity covering a name, which is Andre, and a picture, \nwhich is shown at the end of the introduction in the \nlobby of the web -application. Its name is used \nthroughout the whole process within the web -\napplication. Thus, the list of participants involved also \ncontains its name. Furthermore, while waiting for the \none who has a turn, three animated dots indicate that this \nperson is still wr iting. In order to perceive the agent as \nequally soc ial present, the graphical typing indicators \nalso show up when it has a turn including a certain \nwaiting period. As the sentence generation takes some \ntime from approximately ten up to twenty seconds, we \ndid not implement a fixed waiting period. Due to the fact \nthat the NLP capability of the intelligent is still not \nperfect, only a few social cues are used to not generate \ndisappointment, but to establish a level of trust and \nsympathy.  \n \n8. Evaluation  \n \nIn order to assess the developed DP s and examine \nthe hu man teammates’ perception of the agent, we \nconduct a natur alistic ex post evaluation according to \nVenable et al. [27]. Therefore, four groups of five \nparticipants (P1-20) took part in a test run based on the \ninstantiated web-application incorporating the agent. As \nthe CW process does not specify a target group, the \nparticipants were selected based on availability, access \nto a computer and internet connection as well as the \nPage 405\nability to write. Eventually they cover both female and \nmale participants with an age range from around twenty \nto sixty years. To ensure a smooth induction, each group \nforgathered at the same place, though the application \nenables distributed collaboration.  \nThe test runs proceeded without any obstructive \nproblems. Each test run lasted about forty up to sixty \nminutes including around ten minutes of pr eparation. \nAfter the test run, the participants were asked to reflect \non th eir perception of the agent in qualitative semi -\nstructured interviews. The interviews  were aligned to \nthe expert interview concept by Meuser and Nagel [24]. \nThe guideline was designed to address the specific DPs \nas well as  the user perception of the agent. Thus, each \nparticipant was asked about the specific instanatiation of \neach design principle covering their perception an d \noverall satisfaction with the agent. All relevant remarks \nthroughout the interviews have then been extracted, \nmerged and collocated along the DPs and user \nsatisfaction considering the agent and the overall \nprocess. \nCapability of domain-specific Natural Language \nProcessing (DP 1): Most of the participants were not \nsatisfied with the contributions of the agent generated  \nby means of NLP, i.e. it was without context and \nconfusing (P2-6, P8, P12 -14, P16-19). However, some \ncontributions were perceived as appropriate (P9, P10, \nP14, P17, P 19) and as interesting (P8). Some \nparticipants appreciated that the agent remained in th e \nabstract theme of the story (P7, P11, P 20). Though the \nimplementation of DP1  enabled the  agent to generate \nsentences and make contributi ons to the story, there is \nmuch potential for improvement, e.g. it could be trained \non a larger te xt corpus. The development of another \nlanguage model is also an option. \nClaim Functionality (DP2 ): The claim -\nfunctionality was perceived as very good, helpful and \nimportant (P1-5, P7-20). For most of th e participants it \nwas very easy to claim promptly, especially when \nsentences did n ot make any sense (P1, P2, P4, P6, P7, \nP10, P11, P13, P15-17, P19, P20). Additionally, some \nstated that it is easier to claim a sentence of an agent than \nof a human teammate. This is because they knew that it \nis a computer agent and did not perceive  it as \nemotionally vulnerable (P17, P19, P 20). Only one  \nparticipant admitted to having felt sorry for the a gent \nwhen claiming a sentence (P 12). The number of claims \nadditionally supports the low inhibition level. Only one \nout of 19 contributed sentences by the agent throughout \nthe four  groups was accepted with out claiming. With \nthat, the implementation of the claim -functionality can \nbe partly confirmed. On the one hand, the button was \naccepted very well by the participants, but on the other \nhand, they might have overly  relied on the second \nsentence. \nLike Functionality (DP3): For one thing, the likes \nwere perceived as funny (P10) and cute (P 17), but for \nanother thing also very random ( P1, P7, P8, P12, P 17). \nAlso, three experts did not even recognize the likes (P3, \nP8, P 11). So, while half of the participants did not \nperceive any difference on the  social presence of the \nagent (P1, P4 -8, P11, P12, P16, P 17), half of them did \nperceive a positive effect on the social presence (P2, P7, \nP10, P13-15, P17-20). Two participants even stressed a \nhumanization of the agent (P9 , P15). For a successful \nimplementation of DP3 the distribution of likes needs to \nbe improved.  \nExplainable Artificial Intelligence (DP4 ): Due to \nthe self -introduction of the agent , most of the \nparticipants had realistic expectations toward it (P4, P7, \nP9, P11-20). Thus, they were more likely to forgive \nmistakes of the agent (P12, P 19). Even two of the \nparticipants stated that their expectations have been \nexceeded ( P1, P 10). Still three participants’ \nexpectations could not be met. Consequently, they were \nmore disappointed by the agent (P2, P3, P 8). For \ninstance, one of them expected the agent to contribute \nuseful complex sentences, which refer to the story and  \nmay even include sub-clauses (P8). Another one of them \nstated that the introduction was too well -formulated to \nlower any further expectations (P 3). As the self -\nintroduction of the agent achieved to set the correct \nexpectations for almost all participants, the \nimplementation of DP4 can be partly confirmed. \nIdentity and Social Cues (DP5): Regarding the \nidentity of the agent, its name encouraged a more social \nand personal relation (P3, P6, P7, P9, P12-14, P16, P18-\n20). The participants within the groups also used its \nname when talking about the agent instead of calling it \na bot. Thus, it coul d better merge into the group (P 10). \nThe picture was perceived as social by only a few of the \nparticipants (P3, P7, P14, P16, P19). In fact, the picture \nwas considered impersonal  (P2, P12, P 15). Besides, \nthere were several participants who did not even \nrecognize nor care  about its identity (P 4, P5, P 17). \nThough it was still obvious that the agent is not a real \nhuman, its identity, especially its name fostered the \nperception of a social artificial teammate. Thus, most of \nthe participants accepted  the agent in its entirety as a \ncomputer agent  (P1-3, P5, P7, P8, P10 -12, P14, P16, \nP17, P19, P 20). Furthermore, a s the graphical typing \nindicators during the waiting period were used for all \nparticipants, they had the same effect for the agent. \nThus, several participants could bett er perceive it as a \nsocial present teammate thinking about its next \ncontribution. In fact, without a waiting period and \ngraphical typing indicators, the opposite effect would \noccur (P1, P2, P5, P7, P9, P11, P13, P14, P16, P18, P19, \nP20). However, two participants just considered the \nwaiting period and typing indicators as loading time for \nPage 406\nthe agent, not as humanoid thinking time. Three other \nparticipants did not really recognize the graphical typing \nindicators and did not perceive any influen ce on the \nsocial presence (P4, P6, P 10). Only one participant  \nstated that the agent w as expected to react promptly \n(P17). As most participants had the right expectations \nbeing met by the social cues and competence, DP 5 was \nsuccessfully implemented.  \nAsking the participants about their overall \nperception, half of the participants perceived and \nconsidered the agent a teammate (P5, P9, P10 -14, P17, \nP19, P20). For instance, it contributed to the process like \neveryone else, i.e. it was part of the process and thereby \npart of the team (P11, P13). Even though the participants \ncomplained about some o f the generated contributions \n(P1, P2, P4, P8, P 10-13), it was appreciated that it at \nleast tried to collaborate (P 12). Furthermore, many of \nthem enjoyed collaborating with the agent. They \nconsidered it fun (P2, P3), entertaining , amusing (P1, \nP4) and interesting (P 4). Additionally, it sometimes \ndiverted the topic by giving new ideas ( P1). Stil l the \nother half did not consider it a real teammate (P1-4, P6-\n8, P15, P16, P 18). This was mai nly because the  agent \nwas perceived very inconspicuous (P1, P6, P8, P15, \nP18).  \n \n9. Discussion and Limitations \n \nOverall, five formulated DPs rely on 1 7 MRs, that \nwere identified through theory and expert interviews \nand eventually assessed by a test run and  reflective \ninterviews with the participants. Based on Social \nResponse Theory [22] and the concept of the Uncanny \nValley [23] we formulated DPs toward acceptance of an \nagent teammate and a complementary synergy of the \nagent and the human teammates (Q1). With the \ninstantiation, test run and following interviews we could \nthen evaluate the hybrid work (Q2) as well as the \nperception and acceptance of the agent and its \ncontributions as teammate (Q3). \nIt was revealed that the five  DPs could be partly \nsuccessfully implemented within the instantiated CW \nprocess. As half of the part icipants in the test r un \nperceived and considered the agent as teammate, the \nother half did eventually not consider it a real teammate. \nAs DP1 is most criticized and shows much room for \npotential improvement, this might be the main influence \nfor the overal l perception of the teammate. This \nassumption might be further supported by the evaluation \nof DP2: most of the co ntributed sentences by the agent \nwere claimed. Nevertheless, almost half of the \nparticipants appreciated the domain -specific \ncontributions and ideas of the agent within the reflective \ninterviews. Aiming at a synergy of bo th humans and  \ncomputer agents, future research could define n ew \nstrategies for dissatisfying contributions of an agent, e.g. \ngrammatically correcting or adjusting them. This way,  \nthe human teammates could benefit from the agent’s \nideas and the agent could learn from the corrections and \nthe adjustments made. However, within our research, \nwe successfully revealed the positive acceptance of the \nclaim-button showing a low inhibition l evel of the \nhuman teammates to easily help and  intervene within \nhybrid work. Here, future research can further examine \nand elaborate on the right balance of trust and distru st, \ni.e. balancing the number of human interventions. \nAs of the Uncanny Valley and the balance of \ncompetence and social cues, DP5 was confirmed setting \nthe right expectations for most of the participants. The \nExplainable AI element supported the right expectation \nsetting for almost all participants, which is why DP4 can \nbe partly confirmed.  \nRegarding Social Response Theory, we did not only \ngive the agent a primitive identity successfully \nimplemented with DP5, but it was further provided with \nthe ability to like sentence s as well as with graphical \ntyping indicators . With the ability to li ke, DP3 was \npartly successfully implemented. Though it suppo rted \nthe social presence of the agent, the functionality was \nmore perceived as random. For future research, instead \nof relying on a random 50 % probability, it could either \nbe implemented by a rule-based-system or even by NLP. \nThus, the participants might recognize real preferences \nof the agent and thereby perceive it as more socially \npresent. Also, the agent might use the received likes to \nlearn from them for future contributions.  \nAll in all, the  results show the potential toward a \nsynergy of humans and computer agents  in hybrid \ncollaborative work. With a convenient competence and \nsuitable appropriate social cues covering Social \nResponse Theory  and the Uncanny Valley, human \nteammates do not refuse , but accept working with an \nagent almost perceiving it as real teammate. What is \nmore, a complementary synergy within the hybrid work \ncan be easily achieved with further research work basing \non the humans’ willingness and low inhibition l evel to \ncorrect a nd improve the agent with their human \nintelligence.  \nBesides the promising results of this research , there \nare a few limitations to consider. First, the research at \nhand is onl y a small, qualitative study of collaborative \nagents in CW. It does not lead to general and solid \nconclusions about trust, performance or learning. \nHence, it rather serves as a starting point showing the \npotential of hybrid teamwork. Thereby, it encourages to \nfurther conduct detailed studies and to generalize the \nfindings toward a syn ergy of humans and computer \nagents in hybrid teams. Furthermore, during the test run \nthe agent was the only teammate, which was not \nPage 407\nphysically present, i.e. the human teammates could talk \nand socialize outside the process recognizing voices and \ngestures. This could have affected the user perception of \nthe agent. For further research, it would be interesting to \nhave all participants at separated locations. Besides, the \nparticipants were  selected convenience-based and did \nnot have a connection to the practice of CW. With future \nresearch the DPs could be tested for their applicability \nto other CW practices, especially in work environments, \nwhere the practitioners’ work ethic and job description \ninvolves CW. At last, with this research we did not aim \nto optimize  the tec hnological implementation of an  \nagent’s NLP capabilities, but to examine the general \nacceptance, perception and synergy of computer agents \nin hybrid teams. Still, we assume that with further focus \non the development of the NLP capabilities, the uti lity \nof such a collaborative agent will probably increase.  \n \n10. Conclusion and Contribution \n \nThe findings of this paper serve as a starting point  \nfor further research in the field of Human -Computer-\nCollaboration. For this research we performed a test run \nvia an implemented web -application focusing on CW. \nThereby, we aim to contribute with prescriptive \nknowledge [20] towards a “theory of design and action” \n[44] with MRs and corresponding DPs. Based on Social \nResponse Theory [22] and the concept of the Uncanny \nValley [23], we examine related work and conduct \nexpert interviews finding appropriate social cues and \ncapabilities towards the acceptance of a collaborative \nagent and its contributions as well as the synergy of \nhumans and computer agents in hybrid teams. With that \nwe incorporated an intelligent collaborative agent into a \nCW process and evaluated its perception and acceptance \nwithin a hybrid group work to leverage the potentials of \nhybrid human -computer-collaboration teams. \nEventually, five DPs were established and evaluated to \nfoster a synerg y within hybrid teams as well as the \nacceptance of a  collaborative agent as teammate. The \nDPs should be further tested for their applicability to \nother hybrid collaborative processes.  Additionally, in \norder to prove the quality of the system in detail, fut ure \nresearch might conduct quantitative analyses comparing \nthe design against other forms and test the DPs against \ncontrol instances within an advanced experimental \nsetting. \n \n11. References \n \n[1] Holzinger, A., \"Interactive Machine  Learning (iML)\", \nInformatik Spektrum, 39, 2015, pp. 64–68. \n[2] Dellermann, D., M. Söllner, P. Ebel, and J.M. Leimeister, \n\"Hybrid Intelligence\", Business & Information Systems \nEngineering, 2019. \n[3] Amershi, S., M. Cakmak, W.B. Knox, and T. Kulesza, \n\"Power to the People: The Role of Humans in Interactive \nMachine Learning\", AI Magazine, 35, 2014, pp. 105–120. \n[4] Holzinger, A., M. Plass, K. Holzinger, G.C. Crisan, C.-M. \nPintea, and V. Palade, \"A glass -box interactive machine \nlearning approach for solving NP-hard problems with the \nhuman-in-the-loop\", arXiv:1708.01104. \n[5] Holzinger, A., \"Interactive machine learning for health \ninformatics: when do we need the human -in-the-loop?\", \nBrain Informatics, 3, 2016, pp. 119–131. \n[6] Frick, W., \"When Your Boss Wears Metal Pants\", Harvard \nBusiness Review, June 2015. \n[7] Bittner, E., S. Oeste -Reiß, P. Ebel, and M. Söllner, \n\"Mensch-Maschine-Kollaboration: Grundlagen, \nGestaltungsherausforderungen und Potenziale für \nverschiedene Anwendungsdomänen\", HMD Praxis der \nWirtschaftsinformatik, 56, 2019, pp. 34–49. \n[8] Dellermann, D., A. Calma, N. Lipusch, T. Weber, S. \nWeigel, and P. Ebel, \"The Future of Human -AI \nCollaboration: A Taxonomy of Design Knowledge for \nHybrid Intelligence Systems\", in Proceedings of the 52nd \nHawaii Int ernational Conference on System Sciences. \n2019: Maui, Hawaii, USA. \n[9] Epstein, S.L., \"Wanted: Collaborative intelligence\", \nArtificial Intelligence, 221, 2015, pp. 36–45. \n[10] Seeber, I., E. Bittner, R.O. Briggs, G.-J. de Vreede, T. de \nVreede, D. Druckenmiller, R. Maier, A.B. Merz, S. Oeste-\nReiß, N. Randrup, G. Schwabe, and M. Söllner, \n\"Machines as Teammates: A Collaboration Research \nAgenda\", in Proceedings of the 51st Hawaii International \nConference on System Sciences. 2018: Waikoloa Village, \nHawaii, USA. \n[11] Nass, C., J. Steuer, and E.R. Tauber, \"Computers are \nsocial actors\", in Proceedings of the SIGCHI Conference \non Human Factors in Computing Systems. 1994. \n[12] Martínez, M.A.M., M. Nadj, and A. Maedche, \"Towards \nAn Integrative Theoretical Framework  Of Interactive \nMachine Learning Systems\", in Proceedings of the 27th \nEuropean Conference on Information Systems. 2019: \nStockholm & Uppsala, Sweden. \n[13] Bittner, E., S. Oeste -Reiß, and J.M. Leimeister, \"Where \nis the Bot in our Team?: Toward a Taxonomy of  Design \nOption Combinations for Conversational Agents in \nCollaborative Work\", in Proceedings of the 52nd Hawaii \nInternational Conference on System Sciences. 2019: \nMaui, Hawaii, USA. \n[14] Adams, T., \"And the Pulitzer goes to…a computer\", The \nGuardian, 28.09.2015. \n[15] Manjavacas, E., F. Karsdorp, B. Burtenshaw, and M. \nKestemont, \"Synthetic Literature: Writing Science Fiction \nin a Co -Creative Process\", in Proceedings of the \nWorkshop on Computational Creativity in Natural \nLanguage Generation. 2017: Santiago de Compostela, \nSpain. \n[16] Ghuman, R. and R. Kumari, \"Narrative Science: A \nReview\", International Journal of Science and Research, \n2(9), 2013. \n[17] Colton, S., J. Goodwin, and T. Veale, \"Full-FACE Poetry \nGeneration\", in Proceedings of the Third Internati onal \nConference on Computational Creativity. 2012: Dublin, \nIreland. \nPage 408\n[18] Rank, S., S. Hoffmann, H.-G. Struck, U. Spierling, and P. \nPetta, \"Creativity in Configuring Affective Agents for \nInteractive Storytelling\", in Proceedings of the Third \nInternational Conference on Computational Creativity. \n2012: Dublin, Ireland. \n[19] Monteith, K., T. Martinez, and D. Ventura, \"Automatic \nGeneration of Melodic Accompaniments for Lyrics\", in \nProceedings of the Third International Conference on \nComputational Creativity. 2012: Dublin, Ireland. \n[20] Gregor, S. and A. Hevner, \"Positioning and Presenting \nDesign Science Research for Maximum Impact\", MIS \nQuarterly, 37(2), 2013, pp. 337–355. \n[21] Peffers, K., T. Tuunanen, C.E. Gengler, M. Rossi, W. \nHui, V. Virtanen, and J. Brag ge, \"The Design Science \nResearch Process: A Model For Producing And Presenting \nInformation Systems Research\", in Proceedings of the \nInternational Conference on Design Science Research in \nInformation Systems and Technology. 2006: Claremont, \nCA. \n[22] Nass, C. and Y. Moon, \"Machines and Mindlessness: \nSocial Responses to Computers\", Journal of Social Issues, \n56(1), 2000, pp. 81–103. \n[23] Mori, M., \"The Uncanny Valley\", IEEE Robotics & \nAutomation Magazine, 2012. \n[24] Meuser, M. and U. Nagel, ExpertInneninterv iews - \nvielfach erprobt, wenig bedacht, VS Verlag für \nSozialwissenschaften, Wiesbaden, 2002. \n[25] Wilde, T. and T. Hess, \"Forschungsmethoden der \nWirtschaftsinformatik: Eine empirische Untersuchung\", \nWIRTSCHAFTSINFORMATIK, 49, 2007, pp. 280–287. \n[26] Leimeister, J.M., Collaboration Engineering: IT-gestützte \nZusammenarbeitsprozesse systematisch entwickeln und \ndurchführen, Springer -Verlag, Berlin Heidelberg New \nYork, 2014. \n[27] Venable, J., J. Pries -Heje, and R. Baskerville, \"A \nComprehensive Framework for E valuation in Design \nScience Research\", in Proceedings of the 7th International \nConference on Design Science Research in Information \nSystems and Technology (DESRIST 2012), Las Vegas, \nNV, USA, May 2012. \n[28] Przybilla, L., L. Baar, M. Wiesche, and H. Krcmar , \n\"Machines as Teammates in Creative Teams: Digital \nFacilitation of the Dual Pathways to Creativity\", in \nProceedings of SIGMIS-CPR. 2019: Nashville, TN, USA. \n[29] Seeber, I., L. Waizenegger, S. Seidel, S. Morana, I. \nBenbasat, and P.B. Lowry, \"Reinventing Collaboration \nwith Autonomous Technology -Based Agents\", in \nProceedings of the 27th European Conference on \nInformation Systems. 2019: Stockholm &  Uppsala, \nSweden. \n[30] Kirchkamp, O. and C. Strobel, \"Sharing responsibility \nwith a machine\", Journal of Behavioral and Experimental \nEconomics, 80, 2019, pp. 25–33. \n[31] Norman, D., \"Design, business models, and human -\ntechnology teamwork\", Research -Technology \nManagement, 60(1), 2017, pp. 26–29. \n[32] Robb, A. and B. Lok, \"Social presence in mixed agency \ninteractions\", in IEEE Virtual Reality (VR). 2014: \nMinneapolis, MN. \n[33] Gnewuch, U., A. Maedche, and S. Morana, \"Towards \nDesigning Cooperative and Social Conversational Agents \nfor Customer Service\", in Proceedings of the 38th \nInternational Conference on Informatio n Systems. 2017: \nSeoul, South Korea. \n[34] Trujillo, A.C., I.M. Gregory, and K.A. Ackerman, \n\"Evolving Relationships between Humans and Machines\", \nIFAC-PapersOnLine, 51(34), 2019, pp. 366–371. \n[35] Gulati, S., S. Sousa, and D. Lamas, \"Modelling trust in \nhuman-like technologies\", in Proceedings of the 9th Indian \nConference on Human Computer Interaction. 2018: \nBangalore, India. \n[36] Siemon, D., L. Eckardt, and S. Robra-Bissantz, \"Tracking \nDown the Negative Group Creativity Effects with the Help \nof an Artific ial Intelligence -like Support System\", in \nProceedings of the 48th Hawaii International Conference \non System Sciences. 2015: Kauai, Hawaii, USA. \n[37] Cheng, X., G. Yin, A. Azadegan, and G.L. Kolfschoten, \n\"Trust Evolvement in Hybrid Team Collaboration: A \nLongitudinal Case Study\", Group Decision and \nNegotiation, 25(2), 2015. \n[38] Yu, K., S. Berkovsky, R. Taib, J. Zhou, and F. Chen, \"Do \nI trust my machine teammate?: an investigation from \nperception to decision\", in Proceedings of the 24th \nInternational Confer ence on Intelligent User Interfaces. \n2019: Marina del Ray, California. \n[39] Morris, A. and M. Ulieru, \"FRIEND:  A human -aware \nBDI agent architecture\", in Proceedings of the \nIEEE International Conference on Systems, Man and \nCybernetics. 2011: Anchorage, Alaska, USA. \n[40] Andras, P., L. Esterle, T.A. Han, M. Guckert, and P.R. \nLewis, \"Trusting Intelligent Machines: Deepening Trust \nWithin Socio-Technical Systems\", IEEE Technology and \nSociety Magazine, 37(4), 2018, pp. 76–83. \n[41] Elson, J.S., G.S. Ligon, and D.C. Derrick, \"Examining \nTrust and Reliance in Collaborations between Humans and \nAutomated Agents\", in Proceedings of the 51st Hawaii \nInternational Conference on System Sciences. 2018: \nWaikoloa Village, Hawaii, USA. \n[42] Gnewuch, U., S. Morana, M.T.P. Adam, and A. Maedche, \n\"\"The Chatbot is typing …\" - The Role of Typing \nIndicators in Human-Chatbot Interaction\", in Proceedings \nof the 17th Annual Pre -ICIS Workshop on HCI Research \nin MIS. 2018: San Francisco, CA, USA. \n[43] Chandra, L., S. Gregor, and S. Seid el, \"Prescriptive \nKnowledge in IS Research: Conceptualizing Design \nPrinciples in Terms of Materiality, Action, and Boundary \nConditions\", in Proceedings of the 48th Hawaii \nInternational Conference on System Sciences. 2015: \nKauai, Hawaii, USA. \n[44] Gregor, S., \"The Nature of Theory in Information \nSystems\", MIS Quarterly, 30(3), 2006, pp. 611–642. \nPage 409",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7058996558189392
    },
    {
      "name": "Process (computing)",
      "score": 0.6373291015625
    },
    {
      "name": "Knowledge management",
      "score": 0.4680129289627075
    },
    {
      "name": "Perception",
      "score": 0.46024200320243835
    },
    {
      "name": "Uncanny valley",
      "score": 0.45772016048431396
    },
    {
      "name": "Collaborative writing",
      "score": 0.4381554424762726
    },
    {
      "name": "Collaborative design",
      "score": 0.43450629711151123
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4300847053527832
    },
    {
      "name": "Software engineering",
      "score": 0.33736085891723633
    },
    {
      "name": "Artificial intelligence",
      "score": 0.25315192341804504
    },
    {
      "name": "World Wide Web",
      "score": 0.20923259854316711
    },
    {
      "name": "Systems design",
      "score": 0.14102256298065186
    },
    {
      "name": "Psychology",
      "score": 0.10255080461502075
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Robot",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I159176309",
      "name": "Universität Hamburg",
      "country": "DE"
    }
  ]
}