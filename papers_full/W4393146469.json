{
  "title": "Evaluation of Large Language Models on Code Obfuscation (Student Abstract)",
  "url": "https://openalex.org/W4393146469",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5094237194",
      "name": "Adrian Swindle",
      "affiliations": [
        "UCLouvain Saint-Louis Brussels"
      ]
    },
    {
      "id": "https://openalex.org/A5094237195",
      "name": "Derrick McNealy",
      "affiliations": [
        "University of Southern Mississippi"
      ]
    },
    {
      "id": "https://openalex.org/A2330834182",
      "name": "Giri Krishnan",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2003175000",
      "name": "Ramyaa Ramyaa",
      "affiliations": [
        "New Mexico Institute of Mining and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2801305544",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4287024925"
  ],
  "abstract": "Obfuscation intends to decrease interpretability of code and identification of code behavior. Large Language Models(LLMs) have been proposed for code synthesis and code analysis. This paper attempts to understand how well LLMs can analyse code and identify code behavior. Specifically, this paper systematically evaluates several LLMs’ capabilities to detect obfuscated code and identify behavior across a variety of obfuscation techniques with varying levels of complexity. LLMs proved to be better at detecting obfuscations that changed identifiers, even to misleading ones, compared to obfuscations involving code insertions (unused variables, as well as variables that replace constants with expressions that evaluate to those constants). Hardest to detect were obfuscations that layered multiple simple transformations. For these, only 20-40% of the LLMs’ responses were correct. Adding misleading documentation was also successful in misleading LLMs. We provide all our code to replicate results at https://github.com/SwindleA/LLMCodeObfuscation. Overall, our results suggest a gap in LLMs’ ability to understand code.",
  "full_text": "Evaluation of Large Language Models on Code Obfuscation (Student Abstract)\nAdrian Swindle1, Derrick McNealy2, Giri Krishnan3, Ramyaa Ramyaa4\n1Saint Louis University\n2 University of Southern Mississippi\n3University of California, San Diego\n4New Mexico Institute of Mining and Technology\nadrian.swindle@slu.edu, derrick.mcnealy@usm.edu, gkrishnan@ucsd.edu, ramyaa.ramyaa@nmt.edu\nAbstract\nObfuscation intends to decrease interpretability of code and\nidentification of code behavior. Large Language Models\n(LLMs) have been proposed for code synthesis and code anal-\nysis. This paper attempts to understand how well LLMs can\nanalyse code and identify code behavior. Specifically, this\npaper systematically evaluates several LLMs’ capabilities to\ndetect obfuscated code and identify behavior across a vari-\nety of obfuscation techniques with varying levels of com-\nplexity. LLMs proved to be better at detecting obfuscations\nthat changed identifiers, even to misleading ones, compared\nto obfuscations involving code insertions (unused variables,\nas well as variables that replace constants with expressions\nthat evaluate to those constants). Hardest to detect were ob-\nfuscations that layered multiple simple transformations. For\nthese, only 20-40% of the LLMs’ responses were correct.\nAdding misleading documentation was also successful in\nmisleading LLMs. We provide all our code to replicate re-\nsults at https://github.com/SwindleA/LLMCodeObfuscation.\nOverall, our results suggest a gap in LLMs’ ability to under-\nstand code.\nIntroduction\nCode obfuscations are functionality-preserving transforma-\ntions that reduce the readability of the code rendering it\nharder to understand or reverse engineer (Martinelli et al.\n2018). Obfuscated code remains a challenge for cybersecu-\nrity due to its ability to mask malware from conventional\n(signature based) detection methods(Martinelli et al. 2018).\nFurther, ability of LLM to detect obfuscation in code reflect\non their ability to identify equivalance of code functionality.\nLarge Language Models (LLMs) have been proposed for\nanalyzing code and are also widely used in code synthe-\nsis(Austin et al. 2021; Chen et al. 2021; Li et al. 2022).\nDetecting obfuscation involves understanding code behav-\nior and would be a specific test of reasoning and code anal-\nysis capabilities. Here, we study three LLMs (ChatGPT 3.5,\nJurassic-2, and PaLM) with respect to their ability to detect\nobfuscation and to identify code behavior. Theoretically this\nproblem, in general, is undecidable (Rice’s Theorem). How-\never, we are interested in very simple code which always\nterminates, whose behavior can be understood, and termina-\ntion proved easily.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nMethod\nChatGPT 3.5 (OpenAI 2023), Jurassic-2 (AI21 2023), and\nPaLM (Google 2023) were chosen as the LLMs to evaluate,\nbased on their robustness and performing well in prelimi-\nnary tests. APIs for each of the LLM’s was used in some\nform of Chat Completion. Default settings of parameters\nworked well in simple tests and were not changed.\nWe created base codes and obfuscations (described be-\nlow) as we wanted to analyze the results based on the\ntype of code and obfuscation on very simple codes and ob-\nfuscations. We used 21 distinct C++ base codes. All the\nbase codes used compute simple functions, such as those\nthat would be used in an introductory programming course.\nComplexity varied from printing integers 1 to 10 each in a\nnew line, to checking whether the input is prime. Data struc-\ntures and controls structures (code with and without loops\nand recursion) used were also varied to ensure that the LLMs\nare tested against a wide spectrum of coding tasks. We also\nincluded pieces of code whose behavior was simple but un-\ncommon (checking whether input excluding the letter ‘x’ is\na palindrome, printing a space followed by 6 newlines, etc.).\nThese pieces of code were included to lower the likelihood\nof the LLMs having encountered them during training.\nWe used obfuscations with varying complexity. They can\nbe grouped as (i) obfuscations that do not change the abstract\nsyntax tree: These include transformations such as remov-\ning spaces and new lines, changing identifier names (shuf-\nfling the identifiers already used in the base code, using ran-\ndom identifiers, using misleading identifiers etc.), changing\nstrings to ASCII etc. One obfuscation of note inserted mis-\nleading documentation. (ii) Obfuscations that change the ab-\nstract syntax tree: These include transformations like insert-\ning unused variables, unnecessary statements (if-then state-\nments, for-loops), changing math constants with complex\nexpressions that evaluate to constants, replacing for-loops\nwith recursion, etc. (iii) Layered obfuscations that combined\nmultiple transformations.\nThe LLMs were tested using 3 prompts : (i) “Do these\npieces of code achieve the same goal?” (ii) “Is the function-\nality of these pieces of code the same?” (iii) “What does\nthis piece of code do?”. Prompts 1 and 2 included the obfus-\ncated and original code. The words ”goal” and ”functional-\nity” in prompts 1 and 2 respectively, are the key differences\nbetween the prompts. Prompting the LLM with ”goal” was\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23664\nFigure 1: Average accuracy of LLMs across different obfus-\ncation methods.\naimed at leading it towards giving an answer regarding the\noutcome of the code. ”Functionality” was used in hopes of\nleading the LLM to analyze how the code functions instead\nof the outcome. Prompt 3 differed from 1 and 2 because it\ndoes not include the original code. The prompt is purpose-\nfully vague to see how well the LLM understands the code\nwithout any context.\nWe used the following accuracy measure: if the LLM cor-\nrectly answered (that the codes were obfuscations of each\nother/have the same functionality for prompts 1/2 or gave\nthe correct functionality for prompt 3) and gave the correct\nexplanation of its answer, then we marked it as a correct\nresponse. If either the answer or the explanation of the an-\nswer is incorrect, the response is marked as incorrect. This\nwas because internal inconsistencies of the response (final\nanswer and explanation) indicate lack of understanding. We\ndiscarded cases where the LLM produced an error and did\nnot produce a response. Additional details of methods and\nanalysis are provided in our Github repository 1.\nResults\nWhen comparing between the LLMs, Jurassic was found to\nbe the best at identifying the true functionality of a script.\nJurassic correctly answered the prompt and properly ex-\nplained the code in 60% of its responses. PaLM and Chat-\n1https://github.com/SwindleA/LLMCodeObfuscation\nGPT have a similar rate of correct responses (47% and\n47.45%).\nBase codes that were more complex were harder to deci-\npher, as expected, as were the base codes that had no purpose\n(and were unlikely to have been seen by the LLMs).\nThe worst accuracy were on obfuscations that layered\ntransformations. Following such obfuscations, in general,\nwere obfuscations that change the abstract syntax tree fooled\nthe LLMs more than the ones that do not. There were 2 no-\ntable exceptions to this (i) LLMs were not fooled by the ob-\nfuscation which transformed for-loops into recursive code.\nWe believe this is because the function name was descrip-\ntive, and the code itself was simple enough that recursive and\nfor-loop versions may have been used to train the LLMs. (ii)\nLLMs were fooled by inserting misleading documentation.\nAcross all LLMs, prompting the LLM to give the func-\ntionality of the obfuscated code(prompt 3) gave the highest\naccuracy (˜55%). While the prompt for asking if the origi-\nnal and obfuscated code had the same functionality(prompt\n2) gave the lowest accuracy (˜48%). It appears that the inclu-\nsion of the original code, actually impaired the LLMs ability\nto understand the codes as a whole. Due to the vagueness of\nthe prompts and a 7% range, it is unclear how significant the\ndifferences in the prompts are. It would be natural to assume\nthat the performance of the LLMs will be better given more\ncontext through the prompts, but it is possible that more con-\ntext could cause more error.\nDiscussion\nIn this work, we examined various obfuscation methods on\na variety of code on various LLMs. The success of an obfus-\ncation was determined by how many layers of obfuscation it\ncontained and the type of obfuscation. The top 4 most suc-\ncessful (O13, O14, O15, O17) all used a layered approach\nto their obfuscations, with O18 being the most unsuccessful\nobfuscation. O15 was the most successful because the goal\nof the obfuscation is to take all the previous methods and\napply it to the code. Based on all the obfuscations where the\nLLMs failed, or succeeded, Jurassic and PaLM relied heav-\nily on variable names to identify the code. This can be seen\nin cases where the code contains complex math. Jurassic and\nPaLM only use the variable name to understand the function\nof the variable without deciphering the complex math. This\nis where ChatGPT fails.\nAnother observation from our obfuscations is LLMs can\naccurately detect O18, which attempts to confuse the LLMs\nby having descriptive comments for the original code spread\nthroughout the obfuscated code. Even though the comments\nhave no relevance to the obfuscated code, they still detail\nwhat the code should accomplish, providing the LM with all\nthe information it needs to understand the code.\nIn conclusion, the LLMs were not able to understand the\nfunctionality of obfuscated code. Transformations that in-\ncluded changes to the abstract syntax tree and layered mul-\ntiple types of transformations had the lowest accuracy for\ndetecting obfuscation and code functionality by the LLMs.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23665\nReferences\nAI21. 2023. Jurassic-2 models. https://docs.ai21.com/docs/\njurassic-2-models#jurassic-2-ultra-unmatched-quality. Ac-\ncessed: 2023-07-14.\nAustin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski,\nH.; Dohan, D.; Jiang, E.; Cai, C.; Terry, M.; Le, Q.; et al.\n2021. Program synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;\nKaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brockman,\nG.; et al. 2021. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374.\nGoogle. 2023. API Documentation. https://developers.\ngenerativeai.google/api. Accessed: 2023-07-14.\nLi, Y .; Choi, D.; Chung, J.; Kushman, N.; Schrittwieser, J.;\nLeblond, R.; Eccles, T.; Keeling, J.; Gimeno, F.; Dal Lago,\nA.; et al. 2022. Competition-level code generation with al-\nphacode. Science, 378(6624): 1092–1097.\nMartinelli, F.; Mercaldo, F.; Nardone, V .; Santone, A.; San-\ngaiah, A. K.; and Cimitile, A. 2018. Evaluating model\nchecking for cyber threats code obfuscation identification.\nJournal of Parallel and Distributed Computing, 119.\nOpenAI. 2023. GPT-3.5. https://platform.openai.com/docs/\nmodels/gpt-3-5. Accessed: 2023-07-14.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23666",
  "topic": "Obfuscation",
  "concepts": [
    {
      "name": "Obfuscation",
      "score": 0.89096999168396
    },
    {
      "name": "Computer science",
      "score": 0.7161728143692017
    },
    {
      "name": "Programming language",
      "score": 0.6239539384841919
    },
    {
      "name": "Code (set theory)",
      "score": 0.5662516355514526
    },
    {
      "name": "Computer security",
      "score": 0.17211830615997314
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210121589",
      "name": "UCLouvain Saint-Louis Brussels",
      "country": "BE"
    },
    {
      "id": "https://openalex.org/I44854399",
      "name": "University of Southern Mississippi",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I207123951",
      "name": "New Mexico Institute of Mining and Technology",
      "country": "US"
    }
  ]
}