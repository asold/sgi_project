{
  "title": "Transformer Language Models with LSTM-based Cross-utterance Information Representation",
  "url": "https://openalex.org/W3131505732",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2767893154",
      "name": "Sun G.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2672901607",
      "name": "Zhang, C.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Woodland, P. C.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2748679025",
    "https://openalex.org/W179875071",
    "https://openalex.org/W3096471021",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1508165687",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W3015586278",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2889152503",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2986102593",
    "https://openalex.org/W2963362078",
    "https://openalex.org/W2026149468",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W2005708641",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2886180730",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2125336414",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2399550240",
    "https://openalex.org/W2937649809",
    "https://openalex.org/W2545177271",
    "https://openalex.org/W3082335021",
    "https://openalex.org/W2901265786",
    "https://openalex.org/W2956480774",
    "https://openalex.org/W2949640357",
    "https://openalex.org/W2888867175",
    "https://openalex.org/W2533523411",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2940744433"
  ],
  "abstract": "The effective incorporation of cross-utterance information has the potential to improve language models (LMs) for automatic speech recognition (ASR). To extract more powerful and robust cross-utterance representations for the Transformer LM (TLM), this paper proposes the R-TLM which uses hidden states in a long short-term memory (LSTM) LM. To encode the cross-utterance information, the R-TLM incorporates an LSTM module together with a segment-wise recurrence in some of the Transformer blocks. In addition to the LSTM module output, a shortcut connection using a fusion layer that bypasses the LSTM module is also investigated. The proposed system was evaluated on the AMI meeting corpus, the Eval2000 and the RT03 telephone conversation evaluation sets. The best R-TLM achieved 0.9%, 0.6%, and 0.8% absolute WER reductions over the single-utterance TLM baseline, and 0.5%, 0.3%, 0.2% absolute WER reductions over a strong cross-utterance TLM baseline on the AMI evaluation set, Eval2000 and RT03 respectively. Improvements on Eval2000 and RT03 were further supported by significance tests. R-TLMs were found to have better LM scores on words where recognition errors are more likely to occur. The R-TLM WER can be further reduced by interpolation with an LSTM-LM.",
  "full_text": "TRANSFORMER LANGUAGE MODELS WITH LSTM-BASED CROSS-UTTERANCE\nINFORMATION REPRESENTATION\nG. Sun, C. Zhang, P . C. Woodland\nCambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.\n{gs534,cz277,pcw}@eng.cam.ac.uk\nABSTRACT\nThe effective incorporation of cross-utterance information has the\npotential to improve language models (LMs) for automatic speech\nrecognition (ASR). To extract more powerful and robust cross-\nutterance representations for the Transformer LM (TLM), this paper\nproposes the R-TLM which uses hidden states in a long short-term\nmemory (LSTM) LM. To encode the cross-utterance information,\nthe R-TLM incorporates an LSTM module together with a segment-\nwise recurrence in some of the Transformer blocks. In addition\nto the LSTM module output, a shortcut connection using a fusion\nlayer which bypasses the LSTM module is also investigated. The\nproposed system was evaluated on the AMI meeting corpus, the\nEval2000 and the RT03 telephone conversation evaluation sets. The\nbest R-TLM achieved 0.9%, 0.6% and 0.8% absolute WER reduc-\ntions over the single-utterance TLM baseline, and 0.5%, 0.3%, 0.2%\nabsolute WER reductions over a strong cross-utterance TLM base-\nline on the AMI evaluation set, Eval2000 and RT03 respectively.\nImprovements on Eval2000 and RT03 were further supported by\nsigniﬁcance tests. R-TLMs were found to have better LM scores on\nwords where recognition errors are more likely to occur. The R-TLM\nWER can be further reduced by interpolation with an LSTM-LM.\nIndex Terms— language models, Transformer, cross-utterance,\nLSTM, speech recognition\n1. INTRODUCTION\nA language model (LM) estimates the probability of a word se-\nquence which is often decomposed into a product of conditional\nword prediction probabilities using the chain rule. LMs are widely\nused in many machine learning tasks, such as natural language un-\nderstanding, machine translation and automatic speech recognition\n(ASR). In ASR, high performance LMs are found to be crucial\nto achieving good performance for both traditional noisy source-\nchannel model systems [1–3] and more recent end-to-end systems\n[4–6]. While traditional n-gram LMs only provide context infor-\nmation from a small ﬁxed number of preceding words [7], neural\nnetwork-based LMs including the recurrent neural network (RNN)\nLM (RNNLM), the long short-term memory (LSTM) LM [8–10],\nand the Transformer LM (TLM) [11–13], can provide richer context\ninformation from the full history of an utterance and achieve better\nASR performance. More recently, context information from both\npast and future utterances has been taken into account in language\nmodelling [11, 14, 15].\nOver the years, improvements have been found by explor-\ning more powerful and robust cross-utterance representations for\nRNNLMs and TLMs separately. Cross-utterance RNNLMs were\nusually improved by including richer information into a compact\nG. Sun is funded by the Cambridge Trust\nvector. In [16], an LM was trained without resetting the hidden states\nat utterance boundaries so that a longer history can be encoded. Al-\nternatively, global and local topic vectors, and neural-based cache\nmodels were integrated into LMs [17–19]. More recently, an extra\nneural network component, such as a hierarchical RNN or a pre-\ntrained LM [20], was used to encode the cross-utterance information\ninto a vector representation for LM adaptation [21–23]. On the other\nhand, improvements in cross-utterance TLMs were mainly from\nefﬁcient extension of attention spans, such as using segment-wise\nrecurrence between two adjacent segments [11], adopting adaptive\nattention spans, or applying specially-designed masks to cope with\nmuch longer input sequences [24, 25]. It has been found that such\nTLMs can reduce ASR word error rates (WERs) via LM rescoring\n[12, 26, 27]. Moreover, an RNN structure can also be used in a TLM\nto enhance its local correlations [28, 29].\nWhile the performance of both RNNLMs and TLMs can be im-\nproved by taking into account cross-utterance information, methods\nto incorporate such information are potentially complementary. To\nintroduce the complementary LSTM hidden states into TLMs, the\nR-TLM structure (R stands for recurrence) is proposed which inte-\ngrates an LSTM module into the TLM. The LSTM module contains\na single layer and an optional shortcut connection, and LSTM hidden\nstates are carried over from the preceding segment. The proposed\nLSTM module can be used in conjunction with the segment-wise\nrecurrence from Transformer-XL [11] that leads to the R-TLM XL\nstructure. The proposed R-TLM and R-TLM XL structures are eval-\nuated using the AMI meeting corpus, the Eval2000 and the RT03\ntelephone conversation evaluation sets, where consistent improve-\nments in word error rate (WER) were found. Further reductions in\nWER were found by interpolating an R-TLM with an LSTM-LM.\nThe rest of this paper is organised as follows. Section 2 reviews\nthe TLM and the Transformer XL structures. Section 3 describes the\nproposed R-TLM. The experimental setup and results are given in\nSec. 4 and 5 respectively, followed by the conclusion in Sec. 6.\n2. TRANSFORMER AND TRANSFORMER-XL LMS\nThe uni-directional TLM uses the modiﬁed decoder part of the full\nTransformer model in [13] as shown in Fig 1. The model consists of\na stack of blocks where each block includes a masked multi-head\nattention (MHA) module with a residual connection and a set of\nfully-connected (FC) layers. Taking segments with a ﬁxed number\nof words as the LM input during training, the computation procedure\nof the masked MHA module in the l-th layer at segment τ is\nQτ\ni = XτWQ\ni , Kτ\ni = XτWK\ni , Vτ\ni = XτWV\ni , (1)\nHτ\ni = Softmax((Qτ\niKτ\ni\nT)/\n√\ndk)Vτ\ni, (2)\n˜Oτ = Xτ + Concat(Hτ\n1 ,Hτ\n2 ,..., Hτ\nn)WO, (3)\nTo appear in Proc. ICASSP2021, June 06-11, 2021, Toronto, Ontario, Canada © IEEE 2021\narXiv:2102.06474v1  [cs.CL]  12 Feb 2021\nMasked MHA\nFC & Norm layers \nx NX\n⌧ \u0000 1\nX\n⌧\nO\n⌧\nFig. 1. Transformer (w/o Xτ−1) and Transformer-XL (with Xτ−1)\nstructures with N repeated blocks. The MHA layer uses upper-\ntriangular masks. “Norm layers” stands for layer normalisation.\nwhere Xτ = [xτ\n1 ,xτ\n2 ,..., xτ\nT] is the concatenation of the input vec-\ntor (word embedding) sequence in the τ-th utterance with T input\nwords. WQ\ni ,WK\ni ,WV\ni are model parameter matrices which trans-\nform the input sequence into the query, key, and value. dk is the\ndimension of the key vectors. The Concat( ·) function concatenates\nthe output of each head to form a longer vector which is transformed\ninto the output of the MHA, ˜Oτ, using the parameter matrix WO\nand a shortcut connection to the input Xτ. The ﬁnal output Oτ of\nthe current block is denoted as TransformerBlock (Xτ) and gener-\nated by forwarding ˜Oτ through a stack of FC layers as\nOτ = FFN(˜Oτ) +˜Oτ, (4)\nwhere FFN(·) denotes a feed-forward network. The output of the\ncurrent block, which has the same dimension as Xτ, becomes the\ninput to the next block until the output layer. To indicate relative\npositions of tokens in a segment, relative positional encoding rep-\nresenting the index difference between the vector in the query se-\nquence and the vector in the key sequence, can be included when\ncalculating the dot-product in Eqn. (2), as described in [11, 36].\nTo leverage more information by taking into account segments\npreceding the current one, segment-wise recurrence [11] is designed\nto append the input sequence to each TLM block of the current seg-\nment with the inputs from the previous segment. That is\n˜Xτ = [sg(xτ−1\n1 ),sg(xτ−1\n2 ),..., sg(xτ−1\nT ),xτ\n1 ,xτ\n2 ,..., xτ\nT],\nwhere sg(·) denotes the stop-gradient operation which prevents gra-\ndients from back propagating through the appended history input.\nThe remaining procedure is the same as the Transformer except\nthat only the outputs corresponding to the current segment will\nbe passed to the next block. The Transformer-XL is denoted as\nTransformerBlock(Xτ,sg(Xτ−1)) or TLM XL, where XL stands\nfor the use of segment-wise recurrence for extra-long sequences.\nThe TLM and TLM XL are trained with the cross-entropy loss.\nTo match the training condition with segments of T words at test-\ntime, the start of each utterance in the LM rescoring stage is extended\nto form a T-word segment by concatenating the 1-best ASR output\nhypotheses from past utterances, denoted as the extended history.\nWhen segment-wise recurrence is used for rescoring, output vectors\nfrom each Transformer block of the preceding T-word segment will\nalso be used when computing the current segment.\n3. R-TLM STRUCTURE\nTo make use of the complementary nature of the LSTM long-term\nrepresentation and the segment-wise recurrence in the TLM XL, an\nLSTM module is added to a subset of Transformer blocks as shown\nMasked MHA\nFC & Norm layers \nN x\nFusion Layer\nLSTM\nTransformer \nBlock\nLSTM \nmodule\nX\n⌧\nO\n⌧\nX\n⌧ \u0000 1\nlstm\nX ⌧\nlstm\nh\n⌧ \u0000 1\nlstm\nFig. 2 . R-TLM structure where one of the N repeated blocks is\nshown. The Transformer block is the same as in Fig. 1, and the\nLSTM module is added to only a subset of Transformer blocks. The\nfusion layer and the shortcut in the LSTM module are optional.\nin Fig. 2. For a Transformer block that includes the LSTM module,\nthe input sequence Xτ will be processed by a single-layer LSTM\nﬁrst before being sent to the masked MHA. The hidden states of\nthe LSTM, including the output state and the memory cell state, are\ninitialised by the ﬁnal hidden states of the past segment τ −1. The\nLSTM output sequence will be either sent to the MHA directly, or\ncombined with the LSTM input through a fusion layer to form a\nshortcut connection that bypasses the LSTM layer. The computation\nprocedure can be summarised as\nXτ\nlstm,hτ\nlstm = LSTM(Xτ,sg(hτ−1\nlstm )), (5)\n˜Xτ\nlstm = Fusion(Xτ\nlstm,Xτ), (6)\nOτ = TransformerBlock( ˜Xτ\nlstm), (7)\nwhere Xτ\nlstm and ˜Xτ\nlstm are the outputs of the LSTM and the fusion\nlayer respectively, and Fusion(·) refers to the combination operation\nin the optional fusion layer. The TransformerBlock(·) operation im-\nplements Eqns. (1)–(4). When segment-wise recurrence is applied to\nthe MHA module during training, the extended input will be drawn\nfrom the LSTM output of the last segment as\nOτ = TransformerBlock( ˜Xτ\nlstm,sg( ˜Xτ−1\nlstm )), (8)\nwhich guarantees that the vectors added in MHA have gone through\nthe same computation procedure. Meanwhile, in the fusion layer,\neach output vector of the LSTM layer is concatenated with the input\nvector at the corresponding position, and the fused vectors are passed\nthrough an FC layer, as in Eqn. (9) shown below.\n˜Xτ\nlstm = f(WcXτ\nlstm + UcXτ + bc), (9)\nwhere Wc, Uc and bc are model parameters, f( ·) represents any\nactivation functions among which the linear activation and the rec-\ntiﬁed linear unit (ReLU) were used in this paper. The training and\nrescoring procedure is the same as for TLM XL.\nThe advantages of using the R-TLM structure are as follows.\nFirst, during the rescoring stage at test-time, there usually exists\nword errors in the transcriptions of past utterances, and the LSTM\nlayer in the R-TLM structure provides LSTM hidden state represen-\ntations which are more robust against such errors [31]. Second, the\nR-TLM provides complementary history representations from both\n2\nthe LSTM and Transformer-XL to the attention-based representa-\ntion, and increases the network capability. Third, since empirically\nLSTM-LMs usually perform better than TLMs on small datasets, the\nR-TLM, as a combination of the two, can potentially perform more\nconsistently regardless of the size of the LM training set.\n4. EXPERIMENTAL SETUP\n4.1. Data Sets\nThe R-TLM was evaluated on the AMI meeting transcription [30]\nand the Switchboard (SWB) telephone conversational transcription\ntask separately. The AMI training set consists of 80 hours of data\nfrom 137 meetings with 3–5 speakers per meeting recorded by inde-\npendent headset microphones, and the corresponding reference tran-\nscriptions with about 0.9 million words are used for AMI LM train-\ning. The merged transcriptions from both SWB and Fisher training\nare combined as the SWB LM training set, which consists of about\n27 million words from 14,107 telephone conversations, and the SWB\nacoustic model training set has around 300 hours of audio. For val-\nidation, texts from the AMI development ( Dev) set were used for\nAMI LM, and about 10% of the conversations were randomly se-\nlected and held out from SWB LM training. The AMI evaluation\n(Eval) set is used as the unseen test set to evaluate the performance\nof the AMI LMs. For SWB LM evaluation, the Eval2000 test set\nwhich contains conversations from CallHome (CH) and SWB, as\nwell as the RT03 telephone conversation test set were used.\n4.2. Model Speciﬁcations\nEach AMI LM has 8 Transformer blocks with each block taking\n512-dimensional (-d) input vectors and having 8 attention heads.\nThe segment length T for the AMI LM is 64 for the standard TLM\nwhereas the segment length is 32 when segment-wise recurrence is\nused, to ensure that each attention mechanism covers the same se-\nquence length. Each SWB LM uses 24 Transformer blocks with\neach block taking 512-d input vectors and using 8 attention heads.\nThe segment length for the SWB LMs is 128 for the standard TLM\nand is 64 when using segment-wise recurrence. The LSTM mod-\nule of the R-TLM uses a 512-d single-layer unidirectional LSTM\nin both cases, and the fusion layer outputs 512-d vectors 1. The R-\nTLMs with a fusion layer are referred to as f-R-TLMs while those\nwithout are referred to as d-R-TLMs, where f- and d- stand for “fu-\nsion” and “direct” respectively. For completeness, LSTM-LMs were\nalso trained and tested for both tasks. For the AMI task, a single-\nlayer LSTM with 512-d hidden states was used, while for the SWB\ntask, a 2-layer LSTM-LM with 2048-d hidden states was used. Sim-\nilar to the extended history, the LSTM-LM uses the hidden states\nderived from the 1-best ASR outputs of the history utterances to ini-\ntialise the LSTM-LM at the beginning of the rescoring stage for each\nutterance.\nFor speech recognition experiments on AMI, a hybrid ASR\nsystem with the factorised time delay neural network (TDNN-F)\n[34] acoustic model was built with the Kaldi toolkit [32], and used\nlattice-free maximum mutual training [33] without any form of data\naugmentation, speaker adaptation or voice tract length normalisa-\ntion [35]. For SWB experiments, the TDNN-F model uses i-vector\nspeaker adaptation and speed perturbation for data augmentation\nfollowing the Kaldi recipe. The rescoring with neural network LMs\nused 100-best lists, where the 100-best lists were generated for each\ntest set using the corresponding 4-gram LMs respectively. LMs\n1https://github.com/BriansIDP/RTLM\nSystem PPL WER (%)\n4-gram LM 88.4 20.2\nLSTM-LM cross-utt. 56.9 18.0\nTLM single-utt. 74.4 18.8\nTLM 66.8 18.4\nTLM XL 62.8 18.3\nR-TLM 62.3 18.1\nd-R-TLM XL 57.1 17.9\nf-R-TLM XL 57.3 17.9\nTLM XL + LSTM-LM 53.8 17.7\nd-R-TLM XL + LSTM-LM 51.4 17.5\nTable 1. PPL and WER on AMI Eval set using different LMs. TLM\ndenoted with single-utt. refers to rescoring each utterance individ-\nually without the extended history. The last two rows present the\nresults for TLM and R-TLM XL interpolated with an LSTM-LM.\nwere evaluated in terms of both perplexity (PPL) and WER with\nASR systems. The statistical signiﬁcance of WER improvements\nwas evaluated by the matched pairs sentence segment word error\ntest (MPSSWE) using the NIST ASR scoring toolkit (SCTK).\n5. EXPERIMENTAL RESULTS\n5.1. AMI Experiments\nThe AMI data was ﬁrst used to search for the most suitable blocks\nto include the proposed LSTM module. It was found that adding\nthe LSTM module to any single transformer block led to improve-\nments but adding to more blocks did not improve the system further.\nFor simplicity, the Transformer block with the best PPL was chosen.\nAs the LSTM module was only added to one Transformer block,\nincrease in training and rescoring time was negligible. The PPL ex-\nperiments on the AMI Dev set showed that adding the LSTM module\nin the third block for the fused R-TLM, and in the ﬁrst block for the\ndirect R-TLM yielded the best results. The PPL and WER results\nusing different LMs on the AMI Eval set are listed in Table 1.\nAlthough the cross-utterance information in TLM brought 0.4%\nabsolute WER reduction, and the segment-wise recurrence during\ntraining brought a further 0.1% absolute WER reduction, on small\nscale text training corpora such as AMI, both PPL and WER for the\ncross-utterance TLM were worse than those for the LSTM-LM with\ncross-utterance information. However, when the LSTM module is\nadded to the most suitable Transformer block, both the WER and\nPPL were reduced to a similar level as the LSTM-LM. By using\nsegment-wise recurrence together with the R-TLM, both direct and\nfused systems achieved a 0.9% absolute WER reduction compared\nto the single-utterance TLM, and a 0.5% absolute WER reduction\ncompared to the TLM with extended history.\nMoreover, to demonstrate that the R-TLM is distinct from an\nLSTM-LM, interpolation between the R-TLM and the LSTM-LM\nwas performed. The interpolation weight was ﬁxed at 0.6 throughout\nthe paper. A further WER reduction of 0.4% absolute compared to\nthe R-TLM XL, and a WER reduction of 0.2% absolute compared\nto the TLM XL interpolated with the LSTM-LM, were obtained.\n5.2. Eval2000 and RT03 Experiments\nExperiments were performed on the SWB tasks using both the\nEval2000 and RT03 sets for testing. For simplicity, the LSTM mod-\nule was added to the ﬁrst Transformer block in all R-TLMs trained\n3\nSystem PPL WER (%)\n4-gram LM 87.2 13.1 (9.0 / 17.3)\nLSTM-LM cross-utt. 39.9 10.5 (7.0 / 14.0)\nTLM single-utt. 50.1 10.8 (7.2 / 14.4)\nTLM 36.4 10.5 (7.0 / 14.0)\nTLM XL 34.1 10.4 (7.0 / 13.8)\nR-TLM 38.5 10.5 (6.9 / 13.9)\nd-R-TLM XL 34.5 10.3 (6.9 / 13.6)\nf-R-TLM XL 34.5 10.2 (6.9 / 13.5)\nTLM XL + LSTM-LM 31.9 10.1 (6.8 / 13.3)\nd-R-TLM XL + LSTM-LM 32.6 10.0 (6.7 / 13.2)\nTable 2. The PPLs and WERs on Eval2000. The WERs are also\nsplit into the SWB and CH parts in the form of (SWB / CH).\nSystem Error-prone words Average occurrences\nTLM 10.56 7,041\nTLM XL 10.36 8,510\nR-TLM 10.31 3,428\nd-R-TLM XL 10.25 6,900\nTable 3. Average LM scores on error-prone words and average oc-\ncurrences of words with lower LM scores than the TLM with ex-\ntended history. The error words column contains average LM scores\nfor incorrectly recognised words in TLM, while the average occur-\nrence indicates how frequent the words with lower LM scores com-\npared to the TLM with extended history are in the training set.\non the combined SWB and Fisher corpora. The increase in training\nand rescoring time was also negligible for the larger model. The\nPPL and WER results on the Eval2000 set are shown in Table 2.\nFor moderate sized LM training sets, such as the combined\nSWB and Fisher corpus, the LSTM-LM and TLM with the ex-\ntended history gave similar performance in WER, while the TLM\nhas a lower PPL. Although the TLM trained with segment-wise\nrecurrence achieves the best PPL in Table 2, the best WER for sin-\ngle systems is obtained using the R-TLM XL with a fusion layer,\nwhich gave a 0.6% absolute WER reduction compared to the single-\nutterance TLM, and a 0.2% absolute WER reduction compared to\nthe TLM with extended history. The lowest WER in the table was\nachieved by the R-TLM model interpolated with the LSTM-LM.\nIt is believed that the R-TLM model has a more robust his-\ntory representation which is better at modelling less frequent words\nwhere acoustic errors are more likely to happen, which explains the\nfact that TLM XL ended up having the lowest PPL but not the lowest\nWER. To support this conjecture, negative log-probabilities (i.e. LM\nscores) from different TLMs were measured on error-prone words\nthat appeared in the test set as shown in Table 3. Error-prone words\nare here deﬁned as words where more than half of their occurrences\nin the test set are incorrectly recognised. Additionally, the average\nLM score was measured for each distinct word appeared in the test\nset. For words that have lower average LM scores than the TLM with\nextended history, their occurrences in the training set were averaged\nby the number of distinct words with lower LM scores to indicate\nhow frequent those words are in the training set. Note that the aver-\nage occurrence for the TLM is the average training set occurrence of\nall distinct words in the test set.\nThe TLM XL produces higher LM scores on error words than\nthe R-TLM, indicating those words in the transcriptions are less\nlikely. This explains why the TLM XL produces results with a\nSystem Signiﬁcance p-value\nTLM XL NS 0.384\nR-TLM NS 0.472\nd-R-TLM XL S 0.010\nf-R-TLM XL S 0.032\nTable 4. Signiﬁcance tests performed on the Eval2000 set where\nS denotes signiﬁcance and NS denotes the opposite. Systems com-\npared to TLM with extended history, and decisions were made at a\np-value of 0.05.\nSystem WER (%) Signiﬁcance\n4-gram LM 15.4 (19.2 / 11.5) -\nLSTM-LM 12.4 (15.7 / 8.8) -\nTLM single-utt. 12.7 (16.0 / 9.1) -\nTLM 12.1 (15.3 / 8.7) -\nTLM XL 12.0 (15.2 / 8.5) NS (0.084)\nR-TLM 12.0 (15.2 / 8.6) NS (0.509)\nd-R-TLM XL 11.9 (15.0 / 8.5) S (0.001)\nf-R-TLM XL 11.9 (15.0 / 8.5) S (0.001)\nTLM XL + LSTM-LM 11.8 (15.0 / 8.3) -\nd-R-TLM XL + LSTM-LM 11.8 (14.9 / 8.3) -\nTable 5. WER on RT03 evaluation set where the same naming con-\nvention is used. WER is split into SWB part and Fisher part in the\nform of (SWB/Fisher). Systems compared to TLM with extended\nhistory and signiﬁcance decisions made at p-value of 0.05.\nlower PPL but higher WER than the R-TLM. Moreover, the TLM\nXL is prone to reduce the PPLs of the common words among which\nrecognition errors are less likely to happen. In contrast, adding an\nLSTM module improves the modelling of less frequent words while\nmaintaining a similar test set PPL as the TLM XL. Therefore, the\ncombined cross-utterance modelling provides more robust represen-\ntations for ASR systems.\nFurthermore, to determine if the obtained improvements are sta-\ntistically signiﬁcant, MPSSWE was performed as shown in Table 4.\nFrom Table 4, using either segment-wise recurrence or an LSTM\nmodule on its own does not generate any signiﬁcant improvements,\nwhereas the proposed R-TLM with segment-wise recurrence, either\nusing the fusion layer or not, provides statistically signiﬁcant im-\nprovements at a p-value of 0.05.\nFinally, LMs were tested on the RT03 evaluation set, and similar\nimprovements to those on Eval2000 set were found as shown in Ta-\nble 5. The improvements found by using the R-TLM with segment-\nwise recurrence is signiﬁcant even at a p-value level of 0.001.\n6. CONCLUSIONS\nIn this paper, the R-TLM structure is proposed which incorporates an\nLSTM module into a subset of transformer blocks to obtain more ro-\nbust and powerful cross-utterance representations. The LSTM mod-\nule is able to provide a complementary history representation in ad-\ndition to the segment-wise recurrence. A fusion layer is proposed to\nconnect the LSTM module and the Transformer block. Experiments\non the AMI meeting and the SWB conversational transcription tasks\nshowed consistent improvements in WER over three different test\nsets. Improvements found in SWB task were further supported by\nsigniﬁcance tests. Moreover, the proposed R-TLM can be interpo-\nlated with an LSTM-LM to obtain further reductions in WER.\n4\nReferences\n[1] F. Jelinek, Statistical Methods for Speech Recognition, The MIT\npress, 1997.\n[2] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernock ´y & S. Khudan-\npur, “Recurrent neural network based language model”, Proc.\nInterspeech, Makuhari, 2010.\n[3] X. Liu, Y . Wang, X. Chen, M.J.F. Gales & P.C. Woodland, “Ef-\nﬁcient lattice rescoring using recurrent neural network language\nmodels”, Proc. ICASSP, Florence, 2014.\n[4] T. Hori, J. Cho & S. Watanabe, “End-to-end speech recognition\nwith word-based RNN language models”, Proc. SLT, Athens,\n2018.\n[5] S. Toshniwal, A. Kannan, C.C. Chiu, Y . Wu, T.N. Sainath &\nK. Livescu, “A comparison of techniques for language model\nintegration in encoder-decoder speech recognition”, Proc. SLT,\nAthens, 2018.\n[6] A. Sriram, H. Jun, S. Satheesh & A. Coates, “Cold fu-\nsion: Training seq2seq models together with language models”,\narXiv:1708.06426, 2017.\n[7] Y . Bengio, R. Ducharme, P. Vincent & C. Jauvin, “A neural\nprobabilistic language model”, Journal of Machine Learning\nResearch, vol. 3, pp. 1137–1155, 2003.\n[8] S. Hochreiter & J. Schmidhuber, “Long short-term memory”,\nNeural computation 9(8), 1997.\n[9] A. Graves, N. Jaitly & A. Mohamed “Hybrid speech recognition\nwith deep bidirectional LSTM”, Proc. ASRU, Olomouc, 2013.\n[10] H. Soltau, H. Liao & H. Sak “Neural speech recognizer:\nAcoustic-to-word LSTM model for large vocabulary speech\nrecognition”, Proc. Interspeech, Stockholm, 2017.\n[11] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q.V . Le & R. Salakhut-\ndinov “Transformer-XL: Attentive language models beyond a\nﬁxed-Length context”, Proc. ACL, Florence, 2019.\n[12] K. Irie, A. Zeyer, R. Schl ¨uter & H. Ney, “Language modeling\nwith deep Transformers”, Proc. Interspeech, Graz, 2019.\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA.N. Gomez, L. Kaiser & I. Polosukhin “Attention is all you\nneed”, Proc. NIPS, Long Beach, 2017.\n[14] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stol-\ncke, D. Yu & G. Zweig “Achieving human parity in conversa-\ntional speech recognition”, arXiv:1610.05256, 2016.\n[15] G. Sun, C. Zhang & P. C. Woodland “Cross-utterance language\nmodels with acoustic error sampling”, arXiv:2009.01008, 2020.\n[16] W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang & A. Stol-\ncke, “The Microsoft 2017 conversational speech recognition\nsystem”, arXiv 1708.06073, 2017.\n[17] T. Mikolov & G. Zweig. “Context dependent recurrent neural\nnetwork language model”, Proc. SLT, Miami, 2012.\n[18] X. Chen, T. Tan, X. Liu, P. Lanchantin, M. Wan, M.J.F. Gales\n& P.C. Woodland, “Recurrent neural network language model\nadaptation for multi-genre broadcast speech recognition”, Proc.\nInterspeech, Dresden, 2015.\n[19] K. Li, H. Xu, Y . Wang, D. Povey & S. Khudanpur, “Recurrent\nneural network language model adaptation for conversational\nspeech recognition”, Proc. Interspeech, Hyderabad, 2018.\n[20] J. Devlin, M. Chang, K. Lee & K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding”, Proc. NAACL, Minneapolis, 2019.\n[21] R. Masumura, T. Tanaka, T. Moriya, Y . Shinohara, T. Oba &\nY . Aono, “Large context end-to-end automatic speech recog-\nnition via extension of hierarchical recurrent encoder-decoder\nmodels”, Proc. ICASSP, Brighton, 2019.\n[22] S. Kim, S. Dalmia & F. Metze, “Gated embeddings in end-to-\nend speech recognition for conversational-context fusion”,Proc.\nACL, Florence, 2019.\n[23] M.E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark,\nK. Lee & L. Zettlemoyer, “Deep contextualized word represen-\ntations”, Proc. NAACL, New Orleans, 2018.\n[24] S. Sukhbaatar, ´E. Grave, P. Bojanowski & A. Joulin, “Adaptive\nAttention Span in Transformers”, Proc. ACL, Florence, 2019.\n[25] C. Rewon, G. Scott, R. Alec & S. Ilya, “Generating long se-\nquences with sparse transformers”, arXiv:1904.10509, 2019.\n[26] S. Parthasarathy, W. Gale, X. Chen, G. Polovets & S. Chang,\n“Long-span language modelling for speech recognition”,\narXiv:1911.04571, 2019.\n[27] K. Irie, A. Zeyer, R. Schl ¨uter & H. Ney, “Language models for\nlong-span cross-sentence evaluation”, Proc. ASRU, Singapore,\n2019.\n[28] Z. Wang, Y . Ma, Z. Liu & J. Tang, “R-Transformer: recur-\nrent neural network enhanced transformer”, arXiv:1907.05572,\n2019.\n[29] Y . Zheng, X. Li, F. Xie & L. Lu, “Improving end-to-end speech\nsynthesis with local recurrent neural network enhanced trans-\nformer”, Proc. ICASSP, 2020.\n[30] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot,\nT. Hain, J. Kadlec, V . Karaiskos, W. Kraaij, M. Kronenthal,\nG. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post,\nD. Reidsma & P. Wellner, “The AMI meeting corpus: A pre-\nannouncement”, Proc. MLMI, Bethesda, 2006.\n[31] R. V oleti, J. M. Liss & V . Berisha “Investigating the effects\nof word substitution errors on sentence embeddings”, Proc.\nICASSP, Brighton, 2019.\n[32] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian & P. Schwarz,\n“The Kaldi speech recognition toolkit”, Proc. ASRU, Hawaii,\n2011.\n[33] D. Povey, V . Peddinti, D. Galvez, P. Ghahremani, V . Manohar,\nX. Na, Y . Wang & S. Khudanpur, “Purely sequence-trained neu-\nral networks for ASR based on lattice-free MMI”, Proc. Inter-\nspeech, San Francisco, 2016.\n[34] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. Yarmohamadi\n& S. Khudanpur, “Semi-orthogonal low-rank matrix factoriza-\ntion for deep neural networks”, Proc. Interspeech, Hyderabad,\n2018.\n[35] P. Manakul, M.J.F. Gales & L. Wang, “Abstractive spoken doc-\nument summarisation using hierarchical model with multi-stage\nattention diversity optimization”, Proc. Interspeech, Shanghai,\n2020.\n[36] P. Shaw, J.Uszkoreit & A. Vaswani. “Self-attention with rela-\ntive position representations”,Proc. NAACL-HLT, New Orleans,\n2018.\n5",
  "topic": "Utterance",
  "concepts": [
    {
      "name": "Utterance",
      "score": 0.8659942150115967
    },
    {
      "name": "Transformer",
      "score": 0.7888655662536621
    },
    {
      "name": "Computer science",
      "score": 0.70292729139328
    },
    {
      "name": "Speech recognition",
      "score": 0.5536398887634277
    },
    {
      "name": "ENCODE",
      "score": 0.5041571855545044
    },
    {
      "name": "Conversation",
      "score": 0.4728158712387085
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4607810080051422
    },
    {
      "name": "Natural language processing",
      "score": 0.4234638512134552
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.322045236825943
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}