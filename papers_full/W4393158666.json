{
    "title": "VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning",
    "url": "https://openalex.org/W4393158666",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4366311893",
            "name": "Tangfei Liao",
            "affiliations": [
                "Wenzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2111146220",
            "name": "Xiaoqin Zhang",
            "affiliations": [
                "Wenzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2117608616",
            "name": "Li Zhao",
            "affiliations": [
                "Wenzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A1977132994",
            "name": "Tao Wang",
            "affiliations": [
                "Nanjing University"
            ]
        },
        {
            "id": "https://openalex.org/A2132649285",
            "name": "Guobao Xiao",
            "affiliations": [
                "Tongji University"
            ]
        },
        {
            "id": "https://openalex.org/A4366311893",
            "name": "Tangfei Liao",
            "affiliations": [
                "Wenzhou University",
                "Zhejiang Lab"
            ]
        },
        {
            "id": "https://openalex.org/A2111146220",
            "name": "Xiaoqin Zhang",
            "affiliations": [
                "Wenzhou University",
                "Zhejiang Lab"
            ]
        },
        {
            "id": "https://openalex.org/A2117608616",
            "name": "Li Zhao",
            "affiliations": [
                "Zhejiang Lab",
                "Wenzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A1977132994",
            "name": "Tao Wang",
            "affiliations": [
                "Nanjing University"
            ]
        },
        {
            "id": "https://openalex.org/A2132649285",
            "name": "Guobao Xiao",
            "affiliations": [
                "Tongji University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3134159130",
        "https://openalex.org/W6749473158",
        "https://openalex.org/W6675860037",
        "https://openalex.org/W4312322996",
        "https://openalex.org/W2775929773",
        "https://openalex.org/W4243493583",
        "https://openalex.org/W6658838613",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W4376481231",
        "https://openalex.org/W4319778255",
        "https://openalex.org/W3108011810",
        "https://openalex.org/W2151103935",
        "https://openalex.org/W2928800630",
        "https://openalex.org/W6782063225",
        "https://openalex.org/W3103648783",
        "https://openalex.org/W2950642167",
        "https://openalex.org/W2111073598",
        "https://openalex.org/W6755234859",
        "https://openalex.org/W3140551255",
        "https://openalex.org/W2955796932",
        "https://openalex.org/W6928992657",
        "https://openalex.org/W6747904511",
        "https://openalex.org/W3087268568",
        "https://openalex.org/W1985238052",
        "https://openalex.org/W6746247744",
        "https://openalex.org/W2967640080",
        "https://openalex.org/W3183323691",
        "https://openalex.org/W4283789017",
        "https://openalex.org/W3204057393",
        "https://openalex.org/W2967756832",
        "https://openalex.org/W3103455452",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2990655570",
        "https://openalex.org/W3035563186",
        "https://openalex.org/W2963059198",
        "https://openalex.org/W3047057232",
        "https://openalex.org/W3043075211",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W2963674285",
        "https://openalex.org/W3204035704",
        "https://openalex.org/W2979750740",
        "https://openalex.org/W2894971516",
        "https://openalex.org/W2033819227",
        "https://openalex.org/W3176124701",
        "https://openalex.org/W2106199912",
        "https://openalex.org/W3166285241",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2194775991"
    ],
    "abstract": "Correspondence pruning aims to find correct matches (inliers) from an initial set of putative correspondences, which is a fundamental task for many applications. The process of finding is challenging, given the varying inlier ratios between scenes/image pairs due to significant visual differences. However, the performance of the existing methods is usually limited by the problem of lacking visual cues (e.g., texture, illumination, structure) of scenes. In this paper, we propose a Visual-Spatial Fusion Transformer (VSFormer) to identify inliers and recover camera poses accurately. Firstly, we obtain highly abstract visual cues of a scene with the cross attention between local features of two-view images. Then, we model these visual cues and correspondences by a joint visual-spatial fusion module, simultaneously embedding visual cues into correspondences for pruning. Additionally, to mine the consistency of correspondences, we also design a novel module that combines the KNN-based graph and the transformer, effectively capturing both local and global contexts. Extensive experiments have demonstrated that the proposed VSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks. Our code is provided at the following repository: https://github.com/sugar-fly/VSFormer.",
    "full_text": "VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning\nTangfei Liao1, Xiaoqin Zhang1*, Li Zhao1, Tao Wang2, Guobao Xiao3*\n1Key Laboratory of Intelligent Informatics for Safety and Emergency of Zhejiang Province, Wenzhou University, China\n2State Key Lab for Novel Software Technology, Nanjing University, China\n3School of Electronics and Information Engineering, Tongji University, China\n{tangfeiliao, zhangxiaoqinnan, taowangzj}@gmail.com, lizhao@wzu.edu.cn, x-gb@163.com\nAbstract\nCorrespondence pruning aims to find correct matches (in-\nliers) from an initial set of putative correspondences, which\nis a fundamental task for many applications. The process of\nfinding is challenging, given the varying inlier ratios between\nscenes/image pairs due to significant visual differences. How-\never, the performance of the existing methods is usually lim-\nited by the problem of lacking visual cues (e.g., texture, il-\nlumination, structure) of scenes. In this paper, we propose\na Visual-Spatial Fusion Transformer (VSFormer) to identify\ninliers and recover camera poses accurately. Firstly, we ob-\ntain highly abstract visual cues of a scene with the cross\nattention between local features of two-view images. Then,\nwe model these visual cues and correspondences by a joint\nvisual-spatial fusion module, simultaneously embedding vi-\nsual cues into correspondences for pruning. Additionally, to\nmine the consistency of correspondences, we also design\na novel module that combines the KNN-based graph and\nthe transformer, effectively capturing both local and global\ncontexts. Extensive experiments have demonstrated that the\nproposed VSFormer outperforms state-of-the-art methods on\noutdoor and indoor benchmarks. Our code is provided at the\nfollowing repository: https://github.com/sugar-fly/VSFormer.\nIntroduction\nTwo-view correspondence learning aims to establish reli-\nable correspondences/matches across two images and accu-\nrately recover camera poses, which is a fundamental task\nin computer vision and plays an important role in many\napplications such as simultaneous localization and map-\nping (Mur-Artal, Montiel, and Tardos 2015), structure from\nmotion (Schonberger and Frahm 2016) and image registra-\ntion (Xiao et al. 2020). However, the outlier (false match)\nratio in initial correspondences is often over 90% due to var-\nious cross-image variations (e.g., low texture, illumination\nchanges, repetitive structures), which severely undermines\nthe performance of downstream tasks. Therefore, much re-\ncent research has focused on pruning false matches from ini-\ntial correspondences to obtain accurate two-view geometry.\nSome traditional methods such as RANSAC (Fischler\nand Bolles 1981) and its variants (Chum and Matas 2005;\n*Corresponding authors\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFeature\nExtractor\nCorrespondence\nEembedder\nVSFusion\nContextFormer\nCA\nCorrespondence\nEembedder\nContext\nCapturer\nVSFormer\nPrevious Methods\nVisualize\nFigure 1: Comparison between previous methods and ours.\nTop: the architecture of previous methods, which lack the\nvisual perception of a scene. Bottom: the architecture of our\nVSFormer introduces visual cues of a scene to guide cor-\nrespondence pruning. For visualization purposes, the corre-\nspondences (4D) across two-view images are projected into\na 2D space by t-SNE (Van der Maaten and Hinton 2008).\nThe circle CA represents the cross-attention layer.\nBarath, Matas, and Noskova 2019) search for correct corre-\nspondences (inliers) using iterative sampling strategies, but\ntheir running time grows exponentially with outlier ratio,\nthus making them unsuitable for tackling high-outlier prob-\nlems. Meanwhile, learning-based methods have achieved\npromising performance. PointCN (Yi et al. 2018) is a pi-\noneering work, handling the disordered property of corre-\nspondences (visualization in Fig. 1) with the multilayer per-\nceptron (MLP) architecture.\nUntil recently, the most popular networks commonly em-\nployed an iterative network to inherit weights from the pre-\nvious iteration, which greatly improved the performance of\ncorrespondence pruning. These iterative networks typically\ndesigned some extra structures to mine the geometric con-\nsistency within correspondences, such as OANet (Zhang\net al. 2019), ACNet (Sun et al. 2020), MS 2DG-Net (Dai\net al. 2022). While such a direction deserves further explo-\nration, these methods only considered spatial information of\ncorrespondences as input, which significantly hindered the\nacquisition of deep information and simultaneously dam-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3369\naged the network performance. Thus, there are some re-\nsearches (Luo et al. 2019; Liu et al. 2021) that employ fea-\nture point descriptors of a single image to enhance the rep-\nresentation ability of network inputs. In this paper, we lean\ntoward another perspective and ask the following question:\nCan we provide the network with a scene-aware prior at a\nhigher level to guide pruning?That is, if the inlier ratio of a\nscene/image pair can be perceived in advance, it will facil-\nitate the network in discriminating some ambiguous corre-\nspondences.\nTo this end, with the observation that the inlier ratio varies\ngreatly between scenes/image pairs due to significant vi-\nsual differences (e.g., texture, illumination, occlusion), we\nadopt some scene visual cues as an abstract representation\nof the inlier ratio. As shown in Fig. 1, compared to previous\nmethods, we add some steps for extracting and fusing scene\nvisual cues. Specifically, the proposed Visual-Spatial Fu-\nsion Transformer (VSFormer) is mainly composed of three\ncomponents: Visual Cues Extractor (VCExtractor), Visual-\nSpatial Fusion (VSFusion) module, and Context Trans-\nformer (ContextFormer). Firstly, the VCExtractor extracts\nscene visual cues with the cross attention between local\nfeatures of two-view images. Then, a novel Visual-Spatial\nFusion (VSFusion) module is designed to model the rela-\ntionship between visual cues and spatial cues, simultane-\nously embedding visual cues into correspondences. The VS-\nFusion involves two phases: i) the module adopts a trans-\nformer (Vaswani et al. 2017) to model the complex intra- and\ninter-modality relationships of visual and spatial cues; ii)\nthe module encodes visual and spatial cues separately, using\na simple element-wise summation operation to fuse them;\nMeanwhile, to facilitate fusion, VSFusion uses soft assign-\nment manner (Zhang et al. 2019) to project spatial cues into\nthe same space as visual cues. The proposed VSFusion ef-\nfectively embeds scene visual cues into correspondences for\nguiding subsequent correspondence pruning.\nTo fully mine contextual information of correspondences,\nwe also propose a novel structure called ContextFormer for\npruning, simply stacking a transformer sub-network on top\nof a graph neural network (GNN). In GNN, a novel graph at-\ntention block is designed to improve the representation abil-\nity of a KNN-based graph. The block adopts the squeeze-\nand-excitation mechanism to efficiently capture the potential\nspatial-, channel-, and neighborhood-wise relations inside\na KNN-based graph, facilitating neighborhood aggregation.\nThe proposed structure exploits the neighborhood informa-\ntion of a KNN-based graph and the global modeling ability\nof the transformer, explicitly capturing both local and global\ncontexts of correspondences, thus further improving the per-\nformance of our method.\nThe contributions of this paper are summarized as fol-\nlows:\n• A visual-spatial fusion transformer is proposed to extract\nand embed scene visual cues into correspondences for\nguiding pruning. Meanwhile, we design a joint visual-\nspatial fusion module to fuse visual and spatial cues in\nthe same space. To the best of our knowledge, this is the\nfirst time that scene cues have been introduced for corre-\nspondence pruning.\n• A simple yet effective ContextFormer is proposed to ex-\nplicitly capture both local and global contexts of corre-\nspondences. In this structure, we also design a graph at-\ntention block based on the squeeze-and-excitation mech-\nanism to enhance the representation ability of a KNN-\nbased graph.\n• The proposed VSFormer achieves a precision increase of\n15.79% and 4.45% compared with the state-of-the-art re-\nsult on outdoor and indoor benchmarks, respectively.\nRelated Work\nTraditional Methods. RANSAC (Fischler and Bolles\n1981) and its variants based on iterative sampling strategies\nto estimate a geometry model. To be specific, these methods\nresample the smallest subset of input correspondences to es-\ntimate a parametric model as a hypothesis, and then verify\nits confidence by counting the number of consistent inliers.\nPROSAC (Chum and Matas 2005) can significantly expedite\nthis process. USAC (Raguram et al. 2012) proposes a uni-\nfied framework that incorporates multiple advancements for\nRANSAC variants. MAGSAC (Barath, Matas, and Noskova\n2019) uses σ-consensus to eliminate the requirement for a\npredefined inlier-outlier threshold. RANSAC and its variants\nhave been widely recognized as the standard solution for ro-\nbust model estimation. However, their performance degrades\nseverely as the outlier ratio increases (Ma et al. 2021).\nLearning-Based Methods. PointCN (Yi et al. 2018) is a\npioneering work that formulates correspondence pruning as\nboth an essential matrix regression problem and a binary\nclassification problem. It employs MLPs to effectively pro-\ncess the disordered property of correspondences and pro-\nposes a context normalization technique to embed global\ninformation into each correspondence. DFE (Ranftl and\nKoltun 2018) proposes a distinct loss function and an itera-\ntive network based on PointCN. ACNe (Sun et al. 2020) em-\nploys the attention mechanism to enhance the performance\nof the network. OANet (Zhang et al. 2019) performs full-\nsize prediction for all initial correspondences and introduces\na clustering layer to capture local context. MSA-Net (Zheng\net al. 2022) and PGFNet (Liu et al. 2023) also propose some\njointly spatial-channel attention blocks to capture the global\ncontext of correspondences. After that, there are some re-\nsearches based on the graph neural network (Zhao et al.\n2021; Dai et al. 2022; Liao et al. 2023). CLNet (Zhao\net al. 2021) introduces a neighborhood aggregation manner\nand the pruning strategy to refine coarse correspondences.\nMS2DG-Net (Zhao et al. 2021) builds KNN-based graphs\nat different stages and employs the multi-head self-attention\nmechanism to enhance the representation ability of graphs.\nAlthough these methods have achieved promising perfor-\nmance, they only consider spatial information of correspon-\ndences as input, which severely limits the acquisition of\ndeep information and simultaneously impairs network per-\nformance. Therefore, LMCNet (Liu et al. 2021) exploits fea-\nture point descriptors of a single image to enhance the rep-\nresentation ability of correspondences. In this paper, with\nanother perspective, jointly visual-spatial cues as a scene-\naware prior to guide correspondence pruning.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3370\nMethod\nProblem Formulation\nGiven two-view images (IA, IB), our task is to precisely\nidentify correct correspondences from initial correspon-\ndences and recover camera poses. To be specific, feature\npoints and descriptors are first extracted from two-view im-\nages using a feature detector (e.g., SIFT (Lowe 2004) and\nSuperPoint (DeTone, Malisiewicz, and Rabinovich 2018)).\nThen, the initial correspondence set IC is built by a nearest\nneighbor matching strategy:\nIC = {c1, c2, ...,cN} ∈RN×4, ci = (xi, yi, x′\ni, y′\ni), (1)\nwhere ci is the i-th correspondence; (xi, yi) and (x′\ni, y′\ni) are\nthe feature point coordinates of the given two-view images\nthat have been normalized by camera intrinsics (Zhang et al.\n2019).\nIn our task, the correspondence pruning is typically for-\nmulated as an essential matrix regression problem and an in-\nlier/outlier classification problem (Yi et al. 2018). Following\nCLNet (Zhao et al. 2021), this paper iteratively uses Con-\ntextFormer for correspondence pruning and produces the fi-\nnal probability set Pf = {p1, ...,pi, ...,pN′ } of candidates,\nwhich indicates the probability of each candidate as an inlier.\nThe above process can be formulated as follows:\n(Cf , Wf ) =fϕ(IA, IB, IC), (2)\nPf = Softmax(Wf ), (3)\nwhere Wf = {w1, ...,wi, ...,wN′ } represents the weights\nof final candidates; Cf = {c1, ...,ci, ...,cN′ } represents the\nfinal candidate set; fϕ(·) indicates our proposed VSFormer;\nϕ indicates the network parameters.\nThen, the final candidate setCf and the probability setPf\nare taken as input, and a weighted eight-point algorithm (Yi\net al. 2018) is applied to regress the essential matrix. The\nprocess is presented as:\nbE = g(Cf , Pf ), (4)\nwhere g(·, ·) represents a function of the weighted eight-\npoint algorithm, and the matrix bE indicates the predicted\nessential matrix.\nIn addition, following (Zhao et al. 2021), this paper also\nadopts the full-size verification approach to deal with the\ninlier/outlier classification problem. Specifically, the matrix\nbE and the initial correspondence set IC are taken as inputs\nto produce the predicted symmetric epipolar distance set bD.\nNote that an empirical threshold (10−4) of epipolar distance\nis used criterion to discriminate outliers from inliers (Hart-\nley and Zisserman 2003). The process can be formulated as\nfollows:\nbD = h(bE, IC), (5)\nwhere h(·, ·) represents a function of the full-size verifica-\ntion.\nVisual-Spatial Fusion Module\nOn the one hand, the vast majority of methods only use\nthe spatial information of correspondences as input, which\nIA, IB\nMLP\nContextFormer\nIC∈RN4\nTransformer Layer\nEncoder\n............\n......\n............\nEncoder\nFf∈R2MC\nFvs\nVSFusion\nElement Summation\nSpatial Cues\nVisual Cues\nCross-Att......\n......\n...\n...\nCNNCNN\nFlatten\nFA, FB\nMLP\nVisual Cues Extractor\nFigure 2: The architecture of our VSFormer mainly con-\ntains Visual Cues Extractor (VCExtractor), Visual-Spatial\nFusion (VSFusion) Module, and Context Transformer (Con-\ntextFormer). Note that we omit the inlier predictor after Con-\ntextFormer for simplicity.\nis still challenging for datasets with a large number of out-\nliers. On the other hand, some researches (Luo et al. 2019;\nLiu et al. 2021) employ feature point descriptors of a single\nimage to further improve the network performance. How-\never, existing methods lack a scene-aware prior to guide\ncorrespondence pruning. In this paper, we first base on the\nfact that the inlier ratio varies greatly between scenes/image\npairs due to significant visual differences (e.g., texture, il-\nlumination, occlusion). Then, we extract some visual cues\nof a scene/image pair to abstractly represent the inlier ra-\ntio, which is beneficial for the network to distinguish some\nambiguous correspondences. To this end, as illustrated in\nFig. 2, we propose Visual Cues Extractor (VCExtractor) and\nVisual-Spatial Fusion (VSFusion) module for extracting and\nfusing visual cues into correspondences.\nVisual Cues Extractor. VCExtractor is used to extract\nscene visual cues, and when significant visual differences\nsuch as occlusions, large viewpoint changes, and illumina-\ntion changes between two-view images, the attention map\nscores in the cross-attention layer tend to be generally low.\nThis message is passed through visual cues to the VSFu-\nsion and embedded into each correspondence. To be spe-\ncific, in our VCExtractor, a standard convolution architec-\nture with ResNet34 (He et al. 2016) is first used to extract\nhigh-dimensional local features {FA, FB} ∈RCF ×H\n4 ×W\n4\nfrom two-view images {IA, IB} ∈R3×H×W . Then, local\nfeatures are flattened into 1-D vectors and delivered into\nthe cross-attention layer (Sun et al. 2021) to produce ini-\ntial visual cues of a scene. Subsequently, these initial vi-\nsual cues are embedded with an MLP to obtain visual cues\nFv ∈ RM×C for fusing. In addition, the initial correspon-\ndences IC ∈ RN×4 are also embedded with an MLP to ex-\ntract the deep feature Fs ∈ RN×C as spatial cues.\nVisual-Spatial Fusion. The VSFusion is responsible for\nfusing visual and spatial cues in the same space, and projects\njointly visual-spatial cues into the original space. Firstly,\nsince the fusion between two modalities is beneficial in the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3371\nsame space, VSFusion projects spatial cues into a unified\nspace with visual cues through a learnable soft assignment\nmanner (Zhang et al. 2019). The above process can be for-\nmulated as follows:\nF\n′\ns = (W)T Fs, (6)\nwhere W ∈ RN×M is a learnable matrix.\nThen, a transformer is adopted to robustly model the re-\nlationship between visual cues and spatial cues. The trans-\nformer layer takes the concatenated featureFf ∈ R2M×C of\nvisual and spatial cues as input. For each head in the multi-\nheaded self-attention layer, three learnable matrices WQ,\nWK and WV project the concatenated feature to query\nQ ∈ R2M×d, key K ∈ R2M×d and value V ∈ R2M×d,\nwhere d = C/h and h is the number of heads. Subsequently,\nthe attention matrix A ∈ R2M×2M is calculated by apply-\ning a row-wise softmax function on QKT . The messages\nF\n′\nf ∈ R2M×C are formulated as AV, which fuse the com-\nplex relations between visual and spatial cues. After that,\nthese messages are processed through a feed-forward net-\nwork and split into F\n′\nv ∈ RM×C and F\n′′\ns ∈ RM×C. The\nabove process can be simply described as:\nF\n′\nf = MHSA(Ff ), (7)\n(F\n′\nv, F\n′′\ns ) = Split(FFN(F\n′\nf )), (8)\nwhere MHSA(·) denotes the multi-headed self-attention\nlayer described above; FFN(·) represents the feed-forward\nnetwork.\nNext, an element-wise summation is employed to obtain\njointly visual-spatial cues Fvs ∈ RM×C. Meanwhile, skip\nconnections and resnet-like encoders (Yi et al. 2018) are\nused to rebuild the intra-modality context. The above pro-\ncess can be expressed as:\nFvs = R1(Fv + F\n′\nv) + R2(F\n′\ns + F\n′′\ns ), (9)\nwhere R1(·) and R2(·) represent the encoders with differ-\nent parameters. Finally, similar to Eq. 6, the jointly visual-\nspatial cues Fvs ∈ RM×C is projected into the original\nspace F\n′\nvs ∈ RN×C.\nContext Transformer\nIn our task, mining the consistency within correspondences\nis important to search for correct matches. In this paper, to\nfully capture contextual information of joint visual-spatial\ncorrespondences, a Context Transformer (ContextFormer)\nstructure is designed. Intuitively, correct correspondences\nshould be consistent in both their local and global contexts,\nthus ContextFormer explicitly captures local and global con-\ntexts by stacking the graph neural network and transformer,\nas shown in Fig. 3.\nLocal Context Capturer. Following (Wang et al. 2019;\nZhao et al. 2021), our ContextFormer first builds a KNN-\nbased graph according to the Euclidean distances between\neach jointly visual-spatial correspondences:\nGi = (Vi, Ei), i∈ [1, N] , (10)\nBuild Graph\nGraph Attention\nAggregator\nEncoder\nMHSA\nFFN CFvs'\nMls\nOutput\nPointCN\nSqueeze\nMLP\nGin\nNCK\nGout\nNCK\nHadamard Product\nC Concatenation\n3\nA1 A2\nFigure 3: Illustration of our proposed ContextFormer. Mean-\nwhile, we design a novel graph attention block to mine po-\ntential relationships along three different dimensions.\nwhere Vi = {fi1, ..., fik} represents the k-nearest neighbors\nof fi in the feature space; Ei = {ei1, ...,eik} represents the\nset of directed edges connecting fi and its neighbors. The\nconstruction of edges can be formulated as follows:\neij = [fi, fi − fij] , (11)\nwhere fi and fij represent the i-th jointly visual-spatial cor-\nrespondence and its j-th neighbor, respectively;[·, ·] denotes\nthe concatenate operation along the channel dimension.\nThen, the global graphGin = {G1, ...,Gi, ...,GN } has rich\ncontextual information, but capturing them only by neigh-\nborhood aggregation is not robust. To this end, a novel graph\nattention block adopts the squeeze-and-excitation mecha-\nnism to efficiently capture the potential spatial-, channel-\n, and neighborhood-wise relations inside the global graph.\nTo be specific, as shown in Fig. 3, the global graph Gin is\nembedded by a PointCN block (Yi et al. 2018); then, the\nmax-pooling and average-pooling operations along channel\ndimension to squeeze the global graph, and the element-\nwise summation operation is applied to produce an initial\nattention map A1 ∈ RN×K; subsequently, the initial atten-\ntion map is excited with an MLP to capture neighborhood-\nwise relations A2 ∈ RN×K of the global graph. finally,\nthese relationships are embedded into the global graph via\nHadamard product, adding a residual to obtain Gout. The\nabove operations can be described as:\nG\n′\nin = PointCN(Gin), (12)\nA2 = MLP(AvgPool(G\n′\nin) + MaxPool(G\n′\nin)), (13)\nGout = (G\n′\nin ⊙ A2) +Gin. (14)\nSimilar to the neighborhood-wise attention (as described\nabove), the operations of channel- and spatial-wise attention\nare omitted for simplicity. Despite its simplicity, the graph\nattention block effectively improves the representation abil-\nity of the global graph.\nNext, following (Zhao et al. 2021), we perform neighbor-\nhood aggregation on the enhanced global graph Gout to ob-\ntain the correspondence feature Flocal ∈ RN×C embedded\nwith both global and local contexts.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3372\nPlain image pair\n (a) OANet++\n (b) CLNet\n (c) Ours\nFigure 4: Partial typical visualization results on two challenging datasets, i.e., YFCC100M, SUN3D. From left to right: the\nresults of OANet++, CLNet, and our VSFormer. From top to bottom: the top three results come from unknown outdoor scenes\nand the rest come from unknown indoor scenes. The correspondence is drawn in green if it represents the true-positive and red\nfor the false-positive. Best viewed in color.\nGlobal Context Capturer. It mainly involves a multi-\nheaded self-attention (MHSA) layer employed to capture\nand fuse global context into each correspondence. In partic-\nular, this paper introduces length similarity (Bai et al. 2021)\ninto the MHSA layer, which produces a spatially aware at-\ntention matrix by combining length consistency and fea-\nture consistency. To be specific, given two correspondences\nci = (pA\ni , pB\ni ) and cj = (pA\nj , pB\nj ), the length similarity be-\ntween them is computed as:\nmi,j =\n\f\f∥pA\ni − pA\nj ∥ − ∥pB\ni − pB\nj ∥\n\f\f. (15)\nThen, the length consistency is fused into the attention ma-\ntrix A3 ∈ RN×N by the MHSA layer, while generating a\nspatial aware attention matrix A3 ∈ RN×N to guide mes-\nsage passing. This operation can be formulated as follows:\nA4 = A3 ⊙ Mls, (16)\nwhere Mls ∈ RN×N represents the length similarity matrix\nobtained by Eq. 15. Besides, the other operation details are\nsimilar to those of the transformer in VSFusion, thus this pa-\nper omits the rest for simplicity. Finally, we adopt the same\ninlier predictor as (Zhao et al. 2021) to process the concate-\nnated feature.\nLoss Function\nFollowing (Hartley and Zisserman 2003; Ranftl and Koltun\n2018), a hybrid loss function is employed to supervise the\ntraining process of our proposed method:\nL = Lc(oj, yj) +αLe(bE, E), (17)\nwhere Lc denotes the classification loss; Le represents the\nessential matrix loss;α is a hyper-parameter to balance these\ntwo losses.\nFollowing (Zhao et al. 2021), the classification loss Lc\ncan be formulated as:\nLc(oj, yj) =\nλX\nj=1\nH(ωj ⊙ oj, yj), (18)\nwhere H(·) denotes a binary cross entropy loss function; oj\nrepresents the relevant weights of thej-th iteration; yj repre-\nsents the weakly supervised labels, which are chosen under\nthe epipolar distance threshold10−4 as positive (Hartley and\nZisserman 2003); ωj is an adaptive temperature vector, and\n⊙ represents the Hadamard product.\nFollowing (Zhang et al. 2019), the essential matrix loss\nLe can be formulated as:\nLe = (p′T bEp)2\n∥Ep∥2\n[1] + ∥Ep∥2\n[2] + ∥Ep′∥2\n[1] + ∥Ep′∥2\n[2]\n, (19)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3373\nMethod Params. YFCC100M (%) SUN3D (%)\n- RANSAC - RANSAC\nPointNet++ (2017) 12.00M 16.48 46.25 8.10 15.29\nPointCN (2018) 0.39M 23.95 48.03 9.30 16.40\nDFE (2018) 0.40M 30.27 51.16 12.06 16.26\nOANet++ (2019) 2.47M 38.95 52.59 16.18 17.18\nACNe (2020) 0.41M 33.06 50.89 14.12 16.99\nT-Net (2021) 3.78M 48.20 55.85 17.24 17.57\nLMCNet (2021) 0.93M 47.50 55.03 16.82 17.38\nCLNet (2021) 1.27M 51.90 59.15 15.83 18.99\nMSA-Net (2022) 1.45M 50.65 56.28 16.86 17.79\nMS2DG-Net (2022) 2.61M 49.13 57.68 17.84 17.79\nPGFNet (2023) 2.99M 53.70 57.83 19.32 18.00\nOurs 2.57M 62.18 63.35 20.18 20.27\nTable 1: Quantitative comparison results of the camera pose\nestimation on unknown scenes. The mAP5° without/with\nRANSAC as a post-processing step is reported.\nMethod YFCC100M (%) SUN3D (%)\n- RANSAC - RANSAC\nPointNet++ (2017) 10.49 33.78 10.58 19.17\nPointCN (2018) 13.81 34.55 11.55 20.60\nOANet++ (2019) 32.57 41.53 20.86 22.31\nACNe (2020) 29.17 40.32 18.86 22.12\nT-Net (2021) 42.99 45.25 22.38 22.96\nLMCNet (2021) 33.73 40.39 19.92 21.79\nCLNet (2021) 38.75 44.88 19.20 23.83\nMSA-Net (2022) 39.53 44.57 18.64 22.03\nMS2DG-Net (2022) 38.36 45.34 22.20 23.00\nPGFNet (2023) 44.20 46.28 23.66 23.87\nOurs 48.83 49.03 24.81 24.76\nTable 2: Quantitative comparison results of the camera\npose estimation on known scenes. The mAP5° without/with\nRANSAC as a post-processing step is reported.\nwhere E denotes the ground truth of the essential matrix;\np and p′ represent virtual correspondence coordinates ob-\ntained by the essential matrix E; ∥·∥[i] denotes the i-th ele-\nment of vector.\nImplementation Details\nHolistically, SIFT (Lowe 2004) is adopted to establish N =\n2000 initial correspondences, channel dimension C is 128,\nnetwork iteration λ is 2, and pruning ratio r is 0.5; besides,\nconsidering reducing the training cost, this paper only uses\nVSFusion in the second iteration. In VCExtractor, the orig-\ninal images are resized to H = 120, W = 160, and the\nchannel dimension CF is 64. In ContextFormer, the number\nof k neighbors is set to 9 and 6 for two iterations. We adopt\nAdaptive Moment Estimation (Adam) with a weight decay\nof 0 as the optimizer to train our network, and the canonical\nlearning rate (for batch size is 32) is set to 10−3. Follow-\ning (Zhao et al. 2021), the weight α in Equation 17 is set\nas 0 during the first 20k iterations and 0.5 in the remaining\n480k iterations.\nResNet VSFusion ContextFormer 2x 3x mAP5° mAP20°\n✓ ✓ 43.25 67.12\n✓ ✓ ✓ 49.63 72.87\n✓ ✓ 57.55 78.12\n✓ ✓ ✓ 62.18 80.95\n✓ ✓ ✓ 55.70 76.08\nTable 3: Ablation study of network architecture. 2x and 3x\nrepresent the number of network iterations.\nExperiments and Analysis\nEvaluation Protocols\nWe test our method on both outdoor and indoor bench-\nmarks to evaluate the performance on relative pose esti-\nmation (Zhang et al. 2019). Yahoo’s YFCC100M (Thomee\net al. 2016) dataset is used as our outdoor scenes, which is\nmade up of 100 million outdoor photos from the Internet.\nThe SUN3D (Xiao, Owens, and Torralba 2013) dataset is\nused as our indoor scenes, which consists of large-scale in-\ndoor RGB-D videos with information about camera poses.\nFollowing the data division of (Zhang et al. 2019), all\nmethods are evaluated on both unknown scenes and known\nscenes. In this paper, the mAP of pose error at the thresh-\nolds (5° and 20°) are reported, where the pose error is the\nmaximum of angular errors from rotation and translation.\nComparison Results\nAs shown in Table 1 and Table 2, our VSFormer out-\nperforms other state-of-the-art (SOTA) methods on out-\ndoor and indoor scenes. To be specific, on outdoor scenes,\nour method achieves a performance improvement of 15.8%\nover the recent MLP-based SOTA method (PGFNet) on un-\nknown scenes at mAP5° without RANSAC. Similarly, com-\npared to the recent Graph-based SOTA method (CLNet),\nour method attains a performance improvement of 19.8%\nat mAP5° without RANSAC. On indoor scenes, our method\nachieves a performance improvement of 24.7% and 27.5%\nover two baselines (OANet++ and CLNet) on unknown\nscenes at mAP5° without RANSAC, respectively. Mean-\nwhile, our method also achieves the best performance\namong all methods with RANSAC. The results indicate that\nour proposed visual-spatial fusion and transformer-based\nstructure can further improve the network performance. Ad-\nditionally, as shown in Fig. 4, partial typical visualization\nresults of OANet++ (Zhang et al. 2019), CLNet (Zhao et al.\n2021), and our network are shown from left to right. It can\nbe seen that our method achieves the best performance under\nvarious challenging scenes.\nAblation Studies\nTo deeply analyze the proposed method, we perform de-\ntailed ablation studies on YFCC100M to demonstrate the\neffectiveness of each component in VSFormer.\nNetwork Architecture. As shown in Table 3, we intend to\ngradually add these components to the baseline. The base-\nline (Row-1) we used is PointCN (Yi et al. 2018) with the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3374\nContextFormer Cross TR Proj Sum mAP5° mAP20°\n✓ 57.55/61.82 78.12/81.20\n✓ ✓ 60.33/61.88 80.04/81.19\n✓ ✓ 60.18/61.68 79.89/81.24\n✓ ✓ ✓ 61.23/62.43 80.60/81.20\n✓ ✓ ✓ ✓ 62.18/63.35 80.95/81.84\nTable 4: Ablation study for the VSFusion. The results of\nmAP (%) with/without RANSAC on unknown scenes are\nreported. Cross, TR, Proj, and Sum respectively represent\nthe cross-attention layer, transformer layer, projection layer,\nand final element-wise summation.\nWith Scene Prior Without Scene Prior\nLarge viewpoint variation Repetitive structure Illumination change\nFigure 5: Qualitative comparison with/without scene prior.\npruning strategy. It can be seen that all our component com-\nbinations outperform the baseline on outdoor scenes. To be\nspecific, in the second row, the VSFusion is first introduced,\nwhich achieves a performance improvement of 14.8% over\nthe baseline at mAP5° without RANSAC. It indicates the\nimportance of exploiting visual cues of a scene/image pair\nto guide correspondence pruning. Meanwhile, as illustrated\nin the third row, replacing ResNet encoders (Yi et al. 2018)\nwith our ContextFormer, which obtained a performance im-\nprovement of 33.1% over the baseline at mAP5° without\nRANSAC. Moreover, when combining the proposed mod-\nules and using two iterations, the mAP5° is significantly bet-\nter than those of the baselines. As shown in the last row,\nthis paper also explores the effect of increasing the num-\nber of network iterations. The experiment demonstrates that\nthis leads to a performance penalty of 10.4% over the pro-\nposed method. This is mainly because the redundant itera-\ntions discard some inliers, which are important for the ge-\nometric model estimation. That is, the task of camera pose\nestimation requires sufficient and accurate inliers.\nVSFusion Module. As shown in Table 4, we explore the\ndetailed design of VSFusion. Comparing the results of Row-\n2 and Row-5, we can see that our VSFusion can achieve\nbetter performance than using a simple cross-attention layer\nto fuse visual and spatial cues. Experiments in the fourth and\nfifth rows verify the necessity of spatial projection and re-\nfusion. As illustrated in Fig. 5, visualization results in some\nchallenging scenes also highlight the importance of scene\nvisual cues. In addition, as shown in Table 5, our proposed\nVSFusion can be used as a plug-and-play module to improve\nthe performance of some baselines.\nMethod Known Scene (%) Unknown Scene (%)\nmAP5° mAP20° mAP5° mAP20°\nPointCN 13.81 35.20 23.95 52.44\nPointCN* 24.87+11.06 47.96+12.76 28.18+4.23 56.57+4.13\nOANet++ 32.57 56.89 38.95 66.85\nOANet++* 37.90+5.33 59.97+3.08 46.10+7.15 70.68+3.83\nCLNet 38.27 62.48 51.80 75.76\nCLNet* 40.58+2.31 63.06+0.58 55.20+3.40 76.83+1.07\nTable 5: Quantitative comparison on outdoor scenes without\nRANSAC. The performance of the baseline can be compre-\nhensively improved after using VSFusion.\nEncoder Graph NA GAB TR LS mAP5° mAP20°\n✓ 49.63/57.90 72.87/77.93\n✓ ✓ 51.53/57.78 74.60/78.43\n✓ ✓ ✓ 53.73/59.88 75.48/79.67\n✓ ✓ ✓ ✓ 58.93/61.95 79.69/81.50\n✓ ✓ ✓ ✓ ✓ 59.58/62.65 79.68/81.48\n✓ ✓ ✓ ✓ ✓ ✓ 62.18/63.35 80.95/81.84\nTable 6: Ablation study for the ContextFormer. Encoder,\nGraph, NA, GAB, TR, and LS represent the ReNet encoder,\nKNN-based graph, neighborhood aggregation, graph atten-\ntion block, transformer, and introduced length similarity ma-\ntrix, respectively.\nContextFormer. In this paper, we also conduct some ab-\nlation studies to verify the effectiveness of each component\nin the proposed structure. As shown in Table 6, each com-\nponent of the proposed ContextFormer can further improve\nthe network performance. Among them, the proposed graph\nattention block is the core component of the structure. Ex-\nperiments also show that our graph attention block achieves\na performance improvement of 9.68%. That is, the KNN-\nbased graph has rich context information, and our method\ncan effectively capture these potential relationships.\nConclusion\nIn this paper, with another perspective, we exploit visual\ncues of a scene/image pair to guide correspondence pruning.\nTo this end, we design a joint visual-spatial fusion module\nto fuse visual and spatial cues. Additionally, to mine consis-\ntency within correspondences, we propose a context trans-\nformer to explicitly capture both local and global contexts.\nMeanwhile, a graph attention block is designed to mine con-\ntextual information inside the KNN-based graph. Both com-\nparative and ablation experiments demonstrate the effective-\nness of our proposed method, which can achieve better per-\nformance with fewer parameters.\nAcknowledgements\nThis work was supported in part by the National Natural Sci-\nence Foundation of China under Grant U2033210 and Grant\n62072223 and in part by the Zhejiang Provincial Natural\nScience Foundation under Grant LDT23F02024F02.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3375\nReferences\nBai, X.; Luo, Z.; Zhou, L.; Chen, H.; Li, L.; Hu, Z.; Fu,\nH.; and Tai, C.-L. 2021. Pointdsc: Robust point cloud reg-\nistration using deep spatial consistency. In Proceedings of\nthe IEEE/CVF conference on Computer Vision and Pattern\nRecognition, 15859–15869.\nBarath, D.; Matas, J.; and Noskova, J. 2019. MAGSAC:\nmarginalizing sample consensus. In Proceedings of the\nIEEE/CVF conference on Computer Vision and Pattern\nRecognition, 10197–10205.\nChum, O.; and Matas, J. 2005. Matching with PROSAC-\nprogressive sample consensus. In Proceedings of the\nIEEE/CVF conference on Computer Vision and Pattern\nRecognition, volume 1, 220–226. IEEE.\nDai, L.; Liu, Y .; Ma, J.; Wei, L.; Lai, T.; Yang, C.; and Chen,\nR. 2022. MS2DG-Net: Progressive Correspondence Learn-\ning via Multiple Sparse Semantics Dynamic Graph. In Pro-\nceedings of the IEEE/CVF conference on Computer Vision\nand Pattern Recognition, 8973–8982.\nDeTone, D.; Malisiewicz, T.; and Rabinovich, A. 2018. Su-\nperpoint: Self-supervised interest point detection and de-\nscription. In Proceedings of the IEEE/CVF conference on\nComputer Vision and Pattern Recognition, 224–236.\nFischler, M. A.; and Bolles, R. C. 1981. Random sample\nconsensus: a paradigm for model fitting with applications\nto image analysis and automated cartography. Communica-\ntions of the ACM, 24(6): 381–395.\nHartley, R.; and Zisserman, A. 2003.Multiple view geometry\nin computer vision. Cambridge university press.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep\nresidual learning for image recognition. In Proceedings of\nthe IEEE/CVF conference on Computer Vision and Pattern\nRecognition, 770–778.\nLiao, T.; Zhang, X.; Xu, Y .; Shi, Z.; and Xiao, G. 2023.\nSGA-Net: A Sparse Graph Attention Network for Two-View\nCorrespondence Learning. IEEE Transactions on Circuits\nand Systems for Video Technology.\nLiu, X.; Xiao, G.; Chen, R.; and Ma, J. 2023. PGFNet:\nPreference-Guided Filtering Network for Two-View Corre-\nspondence Learning. IEEE Transactions on Image Process-\ning, 32: 1367–1378.\nLiu, Y .; Liu, L.; Lin, C.; Dong, Z.; and Wang, W. 2021.\nLearnable motion coherence for correspondence pruning. In\nProceedings of the IEEE/CVF conference on Computer Vi-\nsion and Pattern Recognition, 3237–3246.\nLowe, D. G. 2004. Distinctive image features from scale-\ninvariant keypoints. International Journal of Computer Vi-\nsion, 60(2): 91–110.\nLuo, Z.; Shen, T.; Zhou, L.; Zhang, J.; Yao, Y .; Li, S.; Fang,\nT.; and Quan, L. 2019. Contextdesc: Local descriptor aug-\nmentation with cross-modality context. In Proceedings of\nthe IEEE/CVF conference on Computer Vision and Pattern\nRecognition, 2527–2536.\nMa, J.; Jiang, X.; Fan, A.; Jiang, J.; and Yan, J. 2021. Im-\nage matching from handcrafted to deep features: A survey.\nInternational Journal of Computer Vision, 129(1): 23–79.\nMur-Artal, R.; Montiel, J. M. M.; and Tardos, J. D. 2015.\nORB-SLAM: a versatile and accurate monocular SLAM\nsystem. IEEE Transactions on Robotics, 31(5): 1147–1163.\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017. Pointnet:\nDeep learning on point sets for 3d classification and seg-\nmentation. In Proceedings of the IEEE/CVF conference on\nComputer Vision and Pattern Recognition, 652–660.\nRaguram, R.; Chum, O.; Pollefeys, M.; Matas, J.; and\nFrahm, J.-M. 2012. USAC: A universal framework for ran-\ndom sample consensus. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 35(8): 2022–2038.\nRanftl, R.; and Koltun, V . 2018. Deep fundamental matrix\nestimation. In Proceedings of the European Conference on\nComputer Vision, 284–299.\nSchonberger, J. L.; and Frahm, J.-M. 2016. Structure-from-\nmotion revisited. In Proceedings of the IEEE/CVF confer-\nence on Computer Vision and Pattern Recognition, 4104–\n4113.\nSun, J.; Shen, Z.; Wang, Y .; Bao, H.; and Zhou, X. 2021.\nLoFTR: Detector-free local feature matching with trans-\nformers. In Proceedings of the IEEE/CVF conference on\nComputer Vision and Pattern Recognition, 8922–8931.\nSun, W.; Jiang, W.; Trulls, E.; Tagliasacchi, A.; and Yi,\nK. M. 2020. Acne: Attentive context normalization for ro-\nbust permutation-equivariant learning. In Proceedings of\nthe IEEE/CVF conference on Computer Vision and Pattern\nRecognition, 11286–11295.\nThomee, B.; Shamma, D. A.; Friedland, G.; Elizalde, B.; Ni,\nK.; Poland, D.; Borth, D.; and Li, L.-J. 2016. YFCC100M:\nThe new data in multimedia research. Communications of\nthe ACM, 59(2): 64–73.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. Journal of machine learning research, 9(11).\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need.Advances in Neural Information Pro-\ncessing Systems, 30.\nWang, Y .; Sun, Y .; Liu, Z.; Sarma, S. E.; Bronstein, M. M.;\nand Solomon, J. M. 2019. Dynamic graph cnn for learning\non point clouds. Acm Transactions On Graphics, 38(5): 1–\n12.\nXiao, G.; Ma, J.; Wang, S.; and Chen, C. 2020. Determinis-\ntic model fitting by local-neighbor preservation and global-\nresidual optimization. IEEE Transactions on Image Process-\ning, 29: 8988–9001.\nXiao, J.; Owens, A.; and Torralba, A. 2013. Sun3d: A\ndatabase of big spaces reconstructed using sfm and object\nlabels. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 1625–1632.\nYi, K. M.; Trulls, E.; Ono, Y .; Lepetit, V .; Salzmann, M.;\nand Fua, P. 2018. Learning to find good correspondences.\nIn Proceedings of the IEEE/CVF conference on Computer\nVision and Pattern Recognition, 2666–2674.\nZhang, J.; Sun, D.; Luo, Z.; Yao, A.; Zhou, L.; Shen, T.;\nChen, Y .; Quan, L.; and Liao, H. 2019. Learning two-view\ncorrespondences and geometry using order-aware network.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3376\nIn Proceedings of the IEEE/CVF conference on Computer\nVision and Pattern Recognition, 5845–5854.\nZhao, C.; Ge, Y .; Zhu, F.; Zhao, R.; Li, H.; and Salzmann,\nM. 2021. Progressive correspondence pruning by consensus\nlearning. In Proceedings of the IEEE/CVF conference on\nComputer Vision and Pattern Recognition, 6464–6473.\nZheng, L.; Xiao, G.; Shi, Z.; Wang, S.; and Ma, J. 2022.\nMSA-Net: Establishing Reliable Correspondences by Multi-\nscale Attention Network. IEEE Transactions on Image Pro-\ncessing, 31: 4598–4608.\nZhong, Z.; Xiao, G.; Zheng, L.; Lu, Y .; and Ma, J. 2021.\nT-Net: Effective permutation-equivariant network for two-\nview correspondence learning. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n1950–1959.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3377"
}