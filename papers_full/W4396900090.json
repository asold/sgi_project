{
  "title": "Introduction to Large Language Models (LLMs) for dementia care and research",
  "url": "https://openalex.org/W4396900090",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1911992383",
      "name": "Matthias S. Treder",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2147535208",
      "name": "So-Jin Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2074341916",
      "name": "Kamen A Tsvetanov",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4324309277",
    "https://openalex.org/W6769497544",
    "https://openalex.org/W4312091558",
    "https://openalex.org/W4323313947",
    "https://openalex.org/W2945932771",
    "https://openalex.org/W4366490383",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4390086708",
    "https://openalex.org/W4220942233",
    "https://openalex.org/W3202476286",
    "https://openalex.org/W4391358537",
    "https://openalex.org/W3164350562",
    "https://openalex.org/W3203737321",
    "https://openalex.org/W6778698756",
    "https://openalex.org/W4385751831",
    "https://openalex.org/W4386623802",
    "https://openalex.org/W4389977535",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4391682754",
    "https://openalex.org/W3034068260",
    "https://openalex.org/W4393147129",
    "https://openalex.org/W2620805666",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4386117324",
    "https://openalex.org/W3216233619",
    "https://openalex.org/W4378214315",
    "https://openalex.org/W4366850553",
    "https://openalex.org/W2334692654",
    "https://openalex.org/W2023943903",
    "https://openalex.org/W4225154016",
    "https://openalex.org/W3101650868",
    "https://openalex.org/W6739368594",
    "https://openalex.org/W4387617694",
    "https://openalex.org/W4205619630",
    "https://openalex.org/W4391724817",
    "https://openalex.org/W3123340107",
    "https://openalex.org/W4386942462",
    "https://openalex.org/W2899635607",
    "https://openalex.org/W4307136854",
    "https://openalex.org/W3181893318",
    "https://openalex.org/W2928601898",
    "https://openalex.org/W4389946734",
    "https://openalex.org/W2884781986",
    "https://openalex.org/W4388488349",
    "https://openalex.org/W4379087118",
    "https://openalex.org/W2996223423",
    "https://openalex.org/W2663482866",
    "https://openalex.org/W4386122321",
    "https://openalex.org/W3162155171",
    "https://openalex.org/W3201195645",
    "https://openalex.org/W3002093512",
    "https://openalex.org/W4386499643",
    "https://openalex.org/W4321162379",
    "https://openalex.org/W4283157303",
    "https://openalex.org/W4319865634",
    "https://openalex.org/W4248782744",
    "https://openalex.org/W6839479618",
    "https://openalex.org/W4321855128",
    "https://openalex.org/W4385571687",
    "https://openalex.org/W4391506325",
    "https://openalex.org/W2107329067",
    "https://openalex.org/W2466884519",
    "https://openalex.org/W2461291510",
    "https://openalex.org/W2553827870",
    "https://openalex.org/W2169719947",
    "https://openalex.org/W3196248941",
    "https://openalex.org/W4383749364",
    "https://openalex.org/W4312091691",
    "https://openalex.org/W4387595881",
    "https://openalex.org/W181547059",
    "https://openalex.org/W2955006628",
    "https://openalex.org/W2798054687",
    "https://openalex.org/W4366498198",
    "https://openalex.org/W4391987582",
    "https://openalex.org/W4389599277",
    "https://openalex.org/W3162385798",
    "https://openalex.org/W4319982255",
    "https://openalex.org/W2808078534",
    "https://openalex.org/W3159493748",
    "https://openalex.org/W2612655633",
    "https://openalex.org/W4389520705",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4213415839",
    "https://openalex.org/W2747680751",
    "https://openalex.org/W4210758232",
    "https://openalex.org/W4385647263",
    "https://openalex.org/W4200176447",
    "https://openalex.org/W2902153185",
    "https://openalex.org/W3193693229",
    "https://openalex.org/W4235875658",
    "https://openalex.org/W6800813906",
    "https://openalex.org/W3127924821",
    "https://openalex.org/W4212957127",
    "https://openalex.org/W6858742888",
    "https://openalex.org/W4378942580",
    "https://openalex.org/W3199933490",
    "https://openalex.org/W4393399080",
    "https://openalex.org/W3202316099",
    "https://openalex.org/W4307123345",
    "https://openalex.org/W4382723359",
    "https://openalex.org/W4366566346",
    "https://openalex.org/W3046275966",
    "https://openalex.org/W4288053873",
    "https://openalex.org/W4323848232",
    "https://openalex.org/W6855388516",
    "https://openalex.org/W4383618720",
    "https://openalex.org/W4381245716",
    "https://openalex.org/W4392873997",
    "https://openalex.org/W3216786106",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4385573216",
    "https://openalex.org/W4384154861",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W4388890950",
    "https://openalex.org/W2344333704",
    "https://openalex.org/W3191657239",
    "https://openalex.org/W2020436531",
    "https://openalex.org/W3028402743",
    "https://openalex.org/W4286696753",
    "https://openalex.org/W4389216598",
    "https://openalex.org/W4384389802",
    "https://openalex.org/W2999452273",
    "https://openalex.org/W4206055706",
    "https://openalex.org/W4383069396",
    "https://openalex.org/W4210983111",
    "https://openalex.org/W3111147931",
    "https://openalex.org/W4383426667",
    "https://openalex.org/W2593555622",
    "https://openalex.org/W4384625632",
    "https://openalex.org/W2966961896",
    "https://openalex.org/W6775135137",
    "https://openalex.org/W3046113390",
    "https://openalex.org/W4205632760",
    "https://openalex.org/W4281924254",
    "https://openalex.org/W4283156058",
    "https://openalex.org/W2911859923",
    "https://openalex.org/W4288083801",
    "https://openalex.org/W4296077856",
    "https://openalex.org/W4390563428",
    "https://openalex.org/W4289443202",
    "https://openalex.org/W4323043839",
    "https://openalex.org/W3094018806",
    "https://openalex.org/W2124681501",
    "https://openalex.org/W2311849209",
    "https://openalex.org/W2143997631",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W3030464732",
    "https://openalex.org/W2904623500",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W2770604845",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3126007897",
    "https://openalex.org/W4311640423",
    "https://openalex.org/W2922711788",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W4388022708",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3129472592",
    "https://openalex.org/W3047527718",
    "https://openalex.org/W4385262268",
    "https://openalex.org/W4383473937",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4367623495",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4387355871",
    "https://openalex.org/W4386755313",
    "https://openalex.org/W3201710275",
    "https://openalex.org/W1581627189",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4386501199",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4206287534",
    "https://openalex.org/W3200980294",
    "https://openalex.org/W4206292552",
    "https://openalex.org/W4379929613",
    "https://openalex.org/W4240303977",
    "https://openalex.org/W4380353722",
    "https://openalex.org/W4226474339",
    "https://openalex.org/W4385623654",
    "https://openalex.org/W4385473492",
    "https://openalex.org/W4396642475",
    "https://openalex.org/W2979674049",
    "https://openalex.org/W4226373958",
    "https://openalex.org/W4390961094",
    "https://openalex.org/W3105871743",
    "https://openalex.org/W4399521940",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4389519219",
    "https://openalex.org/W3105885358",
    "https://openalex.org/W4226152236",
    "https://openalex.org/W4389984066",
    "https://openalex.org/W3094945321",
    "https://openalex.org/W4389520213",
    "https://openalex.org/W4389523771",
    "https://openalex.org/W3135734416",
    "https://openalex.org/W4221049185",
    "https://openalex.org/W2079974648",
    "https://openalex.org/W4361020574",
    "https://openalex.org/W4394782456"
  ],
  "abstract": "Introduction Dementia is a progressive neurodegenerative disorder that affects cognitive abilities including memory, reasoning, and communication skills, leading to gradual decline in daily activities and social engagement. In light of the recent advent of Large Language Models (LLMs) such as ChatGPT, this paper aims to thoroughly analyse their potential applications and usefulness in dementia care and research. Method To this end, we offer an introduction into LLMs, outlining the key features, capabilities, limitations, potential risks, and practical considerations for deployment as easy-to-use software (e.g., smartphone apps). We then explore various domains related to dementia, identifying opportunities for LLMs to enhance understanding, diagnostics, and treatment, with a broader emphasis on improving patient care. For each domain, the specific contributions of LLMs are examined, such as their ability to engage users in meaningful conversations, deliver personalized support, and offer cognitive enrichment. Potential benefits encompass improved social interaction, enhanced cognitive functioning, increased emotional well-being, and reduced caregiver burden. The deployment of LLMs in caregiving frameworks also raises a number of concerns and considerations. These include privacy and safety concerns, the need for empirical validation, user-centered design, adaptation to the user's unique needs, and the integration of multimodal inputs to create more immersive and personalized experiences. Additionally, ethical guidelines and privacy protocols must be established to ensure responsible and ethical deployment of LLMs. Results We report the results on a questionnaire filled in by people with dementia (PwD) and their supporters wherein we surveyed the usefulness of different application scenarios of LLMs as well as the features that LLM-powered apps should have. Both PwD and supporters were largely positive regarding the prospect of LLMs in care, although concerns were raised regarding bias, data privacy and transparency. Discussion Overall, this review corroborates the promising utilization of LLMs to positively impact dementia care by boosting cognitive abilities, enriching social interaction, and supporting caregivers. The findings underscore the importance of further research and development in this field to fully harness the benefits of LLMs and maximize their potential for improving the lives of individuals living with dementia.",
  "full_text": "TYPE Original Research\nPUBLISHED /one.tnum/four.tnum May /two.tnum/zero.tnum/two.tnum/four.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nOPEN ACCESS\nEDITED BY\nShinya Tasaki,\nRush University Medical Center, United States\nREVIEWED BY\nHaiwen Gui,\nStanford University, United States\nInez Y. Oh,\nWashington University in St. Louis,\nUnited States\n*CORRESPONDENCE\nMatthias S. Treder\nmatthias.treder@gmail.com\nRECEIVED /one.tnum/two.tnum February /two.tnum/zero.tnum/two.tnum/four.tnum\nACCEPTED /two.tnum/three.tnum April /two.tnum/zero.tnum/two.tnum/four.tnum\nPUBLISHED /one.tnum/four.tnum May /two.tnum/zero.tnum/two.tnum/four.tnum\nCITATION\nTreder MS, Lee S and Tsvetanov KA (/two.tnum/zero.tnum/two.tnum/four.tnum)\nIntroduction to Large Language Models\n(LLMs) for dementia care and research.\nFront. Dement./three.tnum:/one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/four.tnum Treder, Lee and Tsvetanov. This is an\nopen-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nIntroduction to Large Language\nModels (LLMs) for dementia care\nand research\nMatthias S. Treder /one.tnum*, Sojin Lee /two.tnumand Kamen A. Tsvetanov /three.tnum,/four.tnum\n/one.tnumSchool of Computer Science & Informatics, Cardiﬀ University, Car diﬀ, United Kingdom, /two.tnumOlive AI\nLimited, London, United Kingdom, /three.tnumDepartment of Clinical Neurosciences, University of Cambrid ge,\nCambridge, United Kingdom, /four.tnumDepartment of Psychology, University of Cambridge, Cambridge,\nUnited Kingdom\nIntroduction: Dementia is a progressive neurodegenerative disorder that aﬀects\ncognitive abilities including memory, reasoning, and communica tion skills,\nleading to gradual decline in daily activities and social engagem ent. In light of the\nrecent advent of Large Language Models (LLMs) such as ChatGPT, t his paper aims\nto thoroughly analyse their potential applications and usefu lness in dementia\ncare and research.\nMethod: To this end, we oﬀer an introduction into LLMs, outlining the ke y\nfeatures, capabilities, limitations, potential risks, and p ractical considerations\nfor deployment as easy-to-use software (e.g., smartphone apps ). We then\nexplore various domains related to dementia, identifying o pportunities for\nLLMs to enhance understanding, diagnostics, and treatment, wit h a broader\nemphasis on improving patient care. For each domain, the speciﬁc contributions\nof LLMs are examined, such as their ability to engage users in me aningful\nconversations, deliver personalized support, and oﬀer cognit ive enrichment.\nPotential beneﬁts encompass improved social interaction, enha nced cognitive\nfunctioning, increased emotional well-being, and reduced caregi ver burden. The\ndeployment of LLMs in caregiving frameworks also raises a numbe r of concerns\nand considerations. These include privacy and safety concerns, t he need for\nempirical validation, user-centered design, adaptation to th e user’s unique\nneeds, and the integration of multimodal inputs to create mor e immersive and\npersonalized experiences. Additionally, ethical guideline s and privacy protocols\nmust be established to ensure responsible and ethical deploym ent of LLMs.\nResults: We report the results on a questionnaire ﬁlled in by people with\ndementia (PwD) and their supporters wherein we surveyed the use fulness of\ndiﬀerent application scenarios of LLMs as well as the features th at LLM-powered\napps should have. Both PwD and supporters were largely positiv e regarding the\nprospect of LLMs in care, although concerns were raised regarding b ias, data\nprivacy and transparency.\nDiscussion: Overall, this review corroborates the promising utilizatio n of LLMs to\npositively impact dementia care by boosting cognitive abilitie s, enriching social\ninteraction, and supporting caregivers. The ﬁndings undersco re the importance\nof further research and development in this ﬁeld to fully harne ss the beneﬁts of\nLLMs and maximize their potential for improving the lives of individuals living with\ndementia.\nKEYWORDS\ndementia, Large Language Model (LLM), Artiﬁcial Intelligenc e, Alzheimer’s disease, care,\nnatural language processing\nIntroduction\nAs the global population ages, dementia emerges as one of the most pressing\nand multifaceted healthcare challenges ( Parra et al., 2019 ). More than 55 million\nindividuals worldwide are currently living with dementia, with over 60% of these\ncases occurring in low- and middle-income countries. Furthermore, approximately 10\nFrontiers in Dementia /zero.tnum/one.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nmillion new cases of dementia are diagnosed annually ( WHO,\n2023). Characterized by progressive cognitive decline that impedes\ndaily functioning, dementia not only impacts the aﬀected\nindividuals, but also their caregivers, families, and the healthcare\nsystem at large. Furthermore, dementia is frequently diagnosed late\nor misdiagnosed (\nFischer et al., 2017 ), while the limited availability\nof caregiver support post-diagnosis compounds the challenges\nfaced by all involved. It becomes imperative for dementia care and\nresearch to develop innovative solutions for improved diagnosis,\neﬀective treatment and caregiving, ultimately reducing the global\nburden of this condition.\nAmidst this backdrop, the rise of advanced computational tools\nand Artiﬁcial Intelligence (AI) technologies oﬀers a beacon of hope.\nA branch of AI known as Large Language Models (LLMs), with\ntheir capacity to understand, generate, and interact using natural\nlanguage, are at the forefront of these technological innovations\n(\nBubeck et al., 2023 ; Huang and Chang, 2023 ; Khurana et al., 2023 ;\nMin et al., 2023 ). In the realm of dementia care and research, LLMs\npresent unique opportunities to revolutionize diagnostic strategies,\ntherapeutic interventions, and patient-caregiver communication.\nYet, for all their promise, LLMs also bring forth a range of ethical,\npractical, and scientiﬁc challenges (\nBlodgett et al., 2020 ; Gabriel,\n2020; Liao, 2020 ; Dobbe et al., 2021 ; Barocas et al., 2023 ; Floridi\nand Floridi, 2023 ; Gallegos et al., 2023 ; Kasneci et al., 2023 ; Li and\nZhang, 2023 ; Wang et al., 2023 ; Bzdok et al., 2024 ). This paper\naims to elucidate the prospects and potential pitfalls of employing\nLLMs in the domain of dementia care and research, paving the way\nfor informed and judicious integration of these powerful tools in\nreal-world settings.\nOur key contributions are as follows:\n1. To our knowledge, this is the ﬁrst publication speciﬁcally\nreviewing LLMs in the context of dementia management and\ncare. Previous reviews surveyed AI in dementia more broadly\n(\nde la Fuente Garcia et al., 2020 ; Lee et al., 2021 ; Richardson\net al., 2022 ; Borchert et al., 2023 ; Tsoi et al., 2023 ) or focused\non AI for prediction and early diagnosis ( Stamate et al., 2020 ;\nLi et al., 2022 ; Merkin et al., 2022 ; Borchert et al., 2023 ).\n2. We propose and thoroughly discuss several application\nscenarios where LLMs can be useful to people with dementia,\nincluding navigation aid, reading/writing assistance, and\nconversational services.\n3. We present the results of a survey of people with\ndementia (PwD) and supporters wherein we investigated\ntheir experience with AI and LLMs, their evaluation on the\nusefulness of the presented application scenarios, and their\npriorities that AI software developers should consider (e.g.,\nprivacy, ease of use).\nIn the next section, we brieﬂy review the dementia literature,\nbefore introducing the application of LLMs in this ﬁeld.\nDementia overview\nA detailed introduction into dementia, its epidemiology,\nvarious subtypes and diagnosis, risk factors, and treatment is\nincluded in the\nSupplementary material A . For brevity, we only\nprovide a summary here. Dementia is a major public health priority\n(\nPrince et al., 2015 ), with the number of aﬀected individuals\nexpected to triple by 2050 ( Nichols et al., 2022 ), creating signiﬁcant\neconomic and social challenges ( Nandi et al., 2022 ). It encompasses\nvarious brain disorders characterized by a decline in cognitive and\nmotor functions due to brain cell loss. Common types include\nAlzheimer’s disease, vascular dementia, dementia with Lewy bodies,\nand frontotemporal dementia, each associated with speciﬁc brain\nregions and symptoms. Mixed dementia involves concurrent brain\nchanges from multiple dementia types (\nSchneider et al., 2007 ;\nKapasi et al., 2017 ).\nAlzheimer’s disease, the most prevalent cause of dementia,\ninvolves memory lapses, word-ﬁnding diﬃculties, and mood\nswings, with damage often starting in the hippocampus (\nSheehan,\n2012; Jack et al., 2018 ; Lane et al., 2018 ; Armstrong et al., 2024 ).\nMost Alzheimer’s cases are sporadic with late onset, but a rare early-\nonset form typically appears before the age of 65 (\n2023 Alzheimer’s\nDisease Facts and Figures, 2023 ). Vascular dementia arises from\ndamage to the brain’s blood vessels and is associated with cognitive\nimpairments such as impaired judgment, planning diﬃculties, and\nmood ﬂuctuations (\nIadecola et al., 2019 ; Bir et al., 2021 ). Dementia\nwith Lewy Bodies features abnormal Lewy body protein deposits in\nthe brain. It manifests as visual hallucinations and Parkinson’s-like\nmovement problems, often coexisting with Alzheimer’s pathology\n(\nKane et al., 2018 ). Frontotemporal Dementia often aﬀects younger\nadults (45–60 years) and impacting cognition, personality, and\nbehavior with various subtypes based on speciﬁc symptoms and\npathologies (\nCoyle-Gilchrist et al., 2016 ; Olney et al., 2017 ; Raﬀaele\net al., 2019 ; Murley et al., 2020 ).\nPrimary risk factors include age, genetics, and family\nhistory ( 2023 Alzheimer’s Disease Facts and Figures, 2023 ).\nHowever, modiﬁable risk factors such as cardiovascular health\nand lifestyle choices can signiﬁcantly impact dementia risk\n(\nLivingston et al., 2020 ). Current treatments focus on symptom\nmanagement with emerging pharmacological advancements aimed\nat altering disease progression. Non-pharmacological interventions\nand comprehensive care strategies are vital for enhancing\nquality of life. Moreover, proactive management involves care\nstrategies, including treatment optimization, caregiver training,\nand community support networks, to improve patient outcomes\nand enhance caregiver wellbeing.\nAs reviewed below, the use of AI technology for dementia\nmanagement and care oﬀer promising avenues for personalized\ntreatment and continuous monitoring of disease progression.\nTraditional pharmacological treatments, lifestyle interventions and\nAI technology can work together in a comprehensive approach to\naddress the multifaceted challenges of this complex neurological\ncondition. By combining these diﬀerent methods, we may be able\nto improve outcomes for patients with dementia, alleviate caregiver\nburden, and better meet the needs presented by dementia.\nArtiﬁcial Intelligence for dementia\nArtiﬁcial Intelligence (AI) applications in Alzheimer’s Disease\ninitially focused on neuroimaging, particularly tracking brain\nFrontiers in Dementia /zero.tnum/two.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nTABLE /one.tnumGlossary of terms relevant in the context of Large Language Models.\nTerm Deﬁnition\nAlignment Process of ensuring the model aligns with human values, ethica l guidelines, and intended uses, while minimizing harmful out puts\nand biases (see Section Bias and alignment ).\nArtiﬁcial Intelligence (AI) Algorithms that can perform tasks typically requiring human intelli gence, such as problem-solving, learning, perception, and\ndecision-making. Typically, AI systems excel only at a single tas k, i.e. do not generalize/transfer across a range of tasks/pro blems.\nArtiﬁcial General Intelligence\n(AGI)\nAn emerging form of AI that possesses the capacity to understan d, learn, and apply its intelligence across a wide range of tasks at a\nlevel comparable to or exceeding human capability. AGI models excel at a large number of tasks simultaneously (see Section\nArtiﬁcial General Intelligence and psychology ).\nBias Skewed or unfair tendencies and associations present in the m odel’s responses, often as a result of imbalances or prejudices wi thin\nthe training data (see Section Bias and alignment ).\nContext window The maximum amount of text the model can process at once, setti ng a limit on the amount of information it can use when\ngenerating responses.\nFinetuning Further reﬁnement of a pretrained model on a speciﬁc, often sma ller dataset, to adapt and enhance its performance for particular\ntasks or subject areas. The ﬁnetuning stage is essential for turning the model into a helpful assistant (see Section Training).\nHallucinations Factually incorrect, nonsensical, or irrelevant information pr oduced by the model that is not supported by the input data or\nreal-world facts, often as a result of misinterpreting the conte xt or overgeneralizing from its training.\nIn-context learning The model’s ability to understand and respond appropriately based o n the immediate context or examples provided within a given\ninput, without additional external training or ﬁnetuning (s ee Section Training).\nMachine Learning A subset of artiﬁcial intelligence that involves the development of algorithms and statistical models that enable computers to\nimprove their performance on a speciﬁc task through learning fro m data and experience.\nOverreliance The tendency to excessively depend on the model’s outputs withou t thorough critical evaluation, potentially leading to unwarran ted\ntrust in inaccurate, biased, or inappropriate responses genera ted by the model.\nPretraining Initial phase of training where the model learns general langua ge patterns and understanding from a vast, diverse dataset, b efore\nbeing ﬁnetuned on speciﬁc tasks or domains.\nPrompt User input or instruction given to the model, which guides and i nﬂuences its subsequent text generation or response.\nPrompt engineering Skillful crafting and optimization of prompts to eﬀectively guide and improve the model’s responses, ensuring more accurate,\nrelevant, or creative outputs.\nToken Basic unit of text, such as a word, part of a word, or punctuation , used for processing and generating language.\nTraining Adjusting the weights (parameters in a model) to accurately interpret and generate lang uage based on the patterns learned from its\ntraining data. Training involves multiple stages, namely pretrai ning, ﬁnetuning, and sometimes in-context learning (see Sec tion\nTraining).\nTransformer The currently dominant model architecture for language models . It eﬃciently processes text using mechanisms like attention t o\ncapture dependencies and relationships between words ( Vaswani et al., 2017 ).\nWeights The parameters within a model that determine how it interprets and generates text. The number of these parameters is usually in the\nbillions.\nvolume changes to identify brain atrophy ( Giorgio et al., 2020 ;\nBrierley, 2021 ; Lombardi et al., 2022 ; Qiu et al., 2022 ; Borchert\net al., 2023 ). Early examples include an AI algorithm achieving\n92.36% accuracy in classifying Alzheimer’s Disease based on\nMagnetic Resonance Imaging scans (\nZhang et al., 2015 ) and\nanother predicting Alzheimer’s Disease over 75 months earlier with\n82% speciﬁcity and 100% sensitivity (\nDing et al., 2019 ). Beyond\nneuroimaging, AI research aims to make cognitive tests ( Li et al.,\n2022), speech assessments ( O’Malley et al., 2020 ), and dementia\nscreenings reproducible on a larger scale, enhancing accessibility,\neven in remote populations. A Canadian medical imaging company\nhas developed a technology utilizing retina scans to detect amyloid\nbuildup, a protein associated with Alzheimer’s Disease in its early\nstages (\nDangerﬁeld and Katherine, 2023 ).\nAs a special instantiation of AI, Large Language Models (LLMs)\nhave been only scarcely explored in the context of dementia care\nand management. In the Method section, we introduce LLMs, their\ngeneral architecture, training and limitations and risks associated\nwith LLMs. We then revisit these topics in the context of dementia.\nFinally, we introduce a questionnaire what was sent out to people\nwith dementia (PwD) and supporters (e.g., caregivers, family\nmembers, or nurses). We investigated their views on various\napplication scenarios as well as their priorities for LLM-powered\ndigital apps (e.g., ease of use, data privacy).\nMethod\nLarge Language Models (LLMs)\nThe years 2023–2024 have been a period of tremendous\ngrowth for LLMs both in terms of computational capability\nand public exposure. In January 2023, OpenAI’s language model\nknown as ChatGPT reached the 100 million users mark 2 months\nafter its release, making it the fastest growing consumer app to\ndate (\nHu, 2023 ). Spurred by the stellar success of OpenAI, big\ntech competitors Google and Meta soon followed suit, releasing\nnew versions of their respective competitor models PaLM2\nFrontiers in Dementia /zero.tnum/three.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\n(Ghahramani, 2023; Mauran, 2023), Bard ( Hsiao, 2023) and Llama\n(Touvron et al., 2023 ). In this section, we review the technological\nfundamentals of LLMs and the way they are trained, ﬁnetuned and\ndeployed, their risks and limitations, and we review some state of\nthe art models. We keep the technical discussion at a conceptual\nlevel in order to make it useful to a broad audience.\nTable 1 provides\na glossary with a concise description of some of the technical terms\nused in the next subsections. A brief overview of the history of\nLLMs is provided in the\nSupplementary material B .\nUsing Large Language Models\nFigure 1 summarizes the interaction of a user with an LLM.\nUsers can typically type input prompts using a browser window\nwith a chat interface. Additionally, many models provide an\nApplication Programming Interface (API) that allows for computer\nprograms or smartphone apps to access an LLM in the background.\nMost LLMs cannot be eﬃciently deployed on a local device because\nof their enormous requirements in terms of processing power and\nmemory. Therefore, in many cases the LLM will be running in\na data center and accessed via an internet connection. The user\nprovides a prompt by either typing it in directly or using speech that\nis then converted to text using a separate speech-to-text algorithm.\nThe prompt can be a question (“What is dementia?”), a statement\n(“I am happy today”), or a set of instructions (“Generate a point-\nby-point list of activities to do in London today, taking into account\nthe current weather. For lunch, suggest good vegetarian restaurants\naround Greenwich.”). Auxiliary data such as images or text ﬁles\ncan be provided and the text prompt can include a reference to the\ndata (“Describe the image”). During the processing of the prompt,\nsome LLMs can recruit software plugins such as web search to fetch\nnews items, or chart and image generators to create visuals. The\nLLM autonomously generates control commands to operate the\nplugins and it incorporates their output. The LLM then returns text\noutput to the user, which can be converted to audio using a text-to-\nspeech algorithm. Alternatively, outputs can take the form of other\nmodalities such as images.\nThe quality of the returned text can often be improved by\ncarefully crafting the prompts given to the model. This is known\nas prompt engineering . A few such techniques have been developed\nand have shown to lead to higher accuracy and better responses.\nChain-of-thought prompting involves giving structured, multi-step\ninstructions or explanations within the prompt, guiding it to\ngenerate step-by-step reasoning in its responses, akin to a human\nsolving a complex problem (\nWei J. et al., 2023 ). Tree-of-thoughts\nexpands on this idea by encouraging the model to explore multiple\npossible lines of reasoning simultaneously, akin to a branching tree\nof ideas (\nYao et al., 2023 ). In analogical prompting, the model\nis prompted to recall examples relevant to a new task and then\nafterwards solve the initial problem (\nYasunaga et al., 2023 ).\nTraining\nIn this section we will explain the basic principles of how\nLLMs are trained from scratch. Most models are based on the\ntransformer architecture that was introduced by\nVaswani et al.\n(2017). Training involves changing the weights of the model.\nWeights determine how it interprets and generates text. Their\nnumber is usually in the billions. Weights form the parameters that\nencode the model’s understanding of language and its knowledge\nabout the world. Note that training a model is something most\nusers will never do themselves. Training a state of the art\nmodel requires prohibitively large resources of data and compute\npower, so it is something mostly done by large tech ﬁrms\nand well-funded startups. Training typically progresses through\ntwo stages: pretraining and ﬁnetuning. An additional in-context\nlearning stage can happen during the interaction with the user,\nallowing further adaptation.\nFigure 2 depicts the diﬀerent phases\nof training.\nPretraining\nIn the pretraining stage, the model trains on a large text corpus\nusing unsupervised objectives. The objective is to teach the model\nto understand general linguistic patterns and structures, and to\nencode world knowledge and facts in its weights. For instance,\nit learns that “Albert Einstein” was a physicist and Nobel prize\nlaureate, or that London is the capital of the United Kingdom.\nIt can be conceived of as a “compression” of the text corpus\ninto the weights of the model. The mechanism by which the\ntraining proceeds is deceptively simple: the model simply learns\nto predict the probabilities of the next token (e.g., one or more\nwords). For instance, the sentence “The dog bit the ___” is more\nlikely to be continued with the words “cat” or “kid” than with\n“truck” or “bacteria”. The model learns this by adjusting its weights\niteratively after seeing some examples. Despite its simplicity, next\nword prediction can instill reasoning. For instance, the sentence\n“France is to Paris as Germany is to ___” can be completed by\nsimply memorizing “Berlin” but it turns out that the model acquires\nsome understanding of the concepts of countries and capitals after\nseeing many similar examples in diﬀerent contexts. Although text\nis the most important input modality, the current trend is to make\nLLMs multi-modal by simultaneously training them on multiple\ndata modalities simultaneously. For instance, Google’s Gemini has\nbeen trained on natural language, computer code, audio, image, and\nvideo (\nPichai and Hassabis, 2023 ). The resultant language models,\nalso known as foundation models, however, can still be adjusted to\nthe needs of speciﬁc users via a process called ﬁnetuning ( Min et al.,\n2023).\nFinetuning the weights\nThe pretrained model has a vast reservoir of general knowledge\nbut it might still lack in depth knowledge in speciﬁc areas. Starting\nfrom a foundation model, training can be continued on a smaller\nset of more specialized content (e.g., medical text books) to ingest\nexpertise in a speciﬁc area into the model. However, to make the\nmodel useful as a chatbot or assistant and let it interact with a\nuser in a question-answer fashion, two other techniques, supervised\nlearning and reinforcement learning with human feedback (RLHF),\nare necessary (\nZiegler et al., 2020 ; Ouyang et al., 2022 ). Supervised\nlearning involves exposing the model to pairs of instructions and\nanswers. For instance, “Explain the moon landing to a 6 year old”\nas an instruction and an actual answer written by a rater can\nbe used as demonstration for the model to learn from (\nOuyang\net al., 2022 ). Such demonstrations can come as a separate dataset of\nquestions and ideal answers and do not require the model’s output.\nIn contrast, RLHF operates directly on the model. First, a prompt\nFrontiers in Dementia /zero.tnum/four.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nFIGURE /one.tnum\nFlowchart showing how a user interacts with a Large Language Model.\nFIGURE /two.tnum\nDiﬀerent types of training an LLM. Pre-training and ﬁne-tun ing involves adjusting the weights of the model whereas in-conte xt learning and\nretrieval-augmented generation (RAG) works for deployed models an d does not change the internal structure of the model. RLHF, Reinf orcement\nlearning with human feedback.\nand several model answers are sampled from the language model.\nA human rater ranks the outputs from best to worst. A model that\nis separate from the LLM, called a reward model, can be trained\non this data. Basically, the reward model learns to mimick the\nassessments of the rater. Second, new prompts and model answers\nare generated, and the reward model is used to score their quality.\nThe reward model can now be used as an additional feedback signal\nto the LLM that makes it produce higher quality answers. The same\ntechnique can be used to align the model with human values and\nmake it less biased. After ﬁnetuning, the adjustment of the weights\nof the model is complete and the weights remain ﬁxed. The model\ncan now be deployed, e.g. as an executable program to run on\na computer.\nIn-context learning via prompt engineering\nAlthough the weights are ﬁxed after ﬁnetuning, the model\nis still able to learn during operation with a user through in-\ncontext learning. The context window refers to the maximum\namount of text that the model can consider at once when\ngenerating a response. It determines how much of a conversation\nthe model can reference in its current processing, impacting its\nability to maintain coherence over long interactions or documents.\nIn-context learning is performed via prompt engineering. For\ninstance, a simple context such as “Show a lot of empathy in your\nresponses” prior to the beginning of the actual conversation can\nmake the model provide more empathetic answers. It is worth\nnoting that in-context learning is limited to the current session,\nFrontiers in Dementia /zero.tnum/five.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nand once a new conversation is started the context needs to\nbe repeated. It is also limited by the context window, so for\nlong conversations it is possible that the model “forgets” the\ninitial instructions.\nRetrieval-augmented generation (RAG)\nRetrieval-augmented generation (RAG) enhances the\ncapabilities of large language models by integrating external\ninformation retrieval into the response generation process (\nChen\net al., 2024 ; Gao et al., 2024 ). The LLM ﬁrst uses a retrieval system\nto ﬁnd relevant documents from an external knowledge base when\npresented with a query. The retrieval system can take the form\nof a search query in a database or a Google search. The retrieved\nitems are then incorporated into the model’s context, providing\neither up-to-date or more detailed information. Finally, the model\ngenerates a response that draws from both its internal training and\nthe retrieved information. This is particularly valuable in situations\nwhere precision and currency of information are critical, or for\ntopics that are highly specialized or niche. Models such as Google’s\nGemini implement RAG.\nLimitations and risks\nDespite the signiﬁcant advances and human-level performance\nacross a variety of language related tasks, LLMs lack the nuance,\nworld knowledge and deep semantic understanding that drives\nhuman conversation. They can make factually false statements,\nperpetuate biases inherent in internet text data, and may be\nsusceptible to usage by parties with ill intent (\nGabriel, 2020; Dobbe\net al., 2021 ; Barocas et al., 2023 ; Wang et al., 2023 ). In this section,\nwe summarize the main limitations and risks of LLMs, as well as\napproaches for mitigation.\nRegulatory challenges\nA comprehensive overview of regulatory challenges is included\nin the\nSupplementary material C . A summary is provided here.\nUsing Large Language Models (LLMs) in healthcare brings\nsigniﬁcant challenges such as ethical issues, biases, safety concerns,\nand environmental impacts. It is essential to implement proactive\nregulations to harness the beneﬁts and mitigate risks, ensuring\nLLMs meet clinical and patient needs (\nMeskó and Topol, 2023 ).\nThe deployment of generative AI models can compromise privacy\nby using personal data without informed consent, posing privacy\nrisks. It is critical to enforce laws like GDPR and HIPAA to ensure\nthe anonymization and protection of patient data, and secure\ninformed consent for using AI in healthcare (\nMeskó and Topol,\n2023).\nFurthermore, there is a need for transparency in how AI\nmodels operate, especially as companies sometimes limit scrutiny\nof their algorithms. Eﬀective regulation should require clarity on\nAI decision-making processes to uphold democratic principles\nand assign liability appropriately (\nNorwegian Consumer Council,\n2023). Proposed regulations, like the AI Liability Directive, aim to\nfacilitate compensation for AI-induced harms but require proving\nfault, highlighting the need for clear regulatory deﬁnitions and\nprotections (\nNorwegian Consumer Council, 2023 ). Regulators\nalso need to implement ongoing monitoring and validation\nmechanisms to maintain the reliability and safety of AI tools in\nhealthcare, adapting to diﬀerent populations over time (\nMeskó and\nTopol, 2023).\nHallucinations\nIn the context of LLMs, a hallucination refers to the generation\nof syntactically sound text that is factually incorrect ( OpenAI,\n2023). It has been a prominent aspect of the public discussion of\nAI and was selected as Cambridge dictionary’s word of the year\n(\nCreamer, 2023 ). Moreover, LLMs can express high conﬁdence in\nthese statements even if they are nonsensical. One reason for LLMs’\nsusceptibility to hallucinations is the training data consisting of\na large corpus of text and code, which can contain errors and\ninconsistencies. When an LLM is generating text, it may draw on\nthis information in unexpected ways, leading to hallucinations (\nYe\net al., 2023 ; Zhang et al., 2023 ). Another reason for hallucinations\nis that many LLMs are not able to verify the accuracy of their\nown output. When an LLM generates text, it may not check\nwhether text is consistent with its knowledge base or with reality.\nThis can lead to the generation of text that is factually incorrect\nor nonsensical.\nHallucinations are also a consequence of calibrated models\n(\nKalai and Vempala, 2023 ). A model is considered well calibrated\nwhen its conﬁdence about a statement matches the actual\nprobability of the statement being true (\nJiang et al., 2021 ). As\nan illustrative example, if a calibrated weather forecasting model\npredicts 30% change for rain on a given day, the prediction\nshould turn out to be correct in 30% of the cases (\nDawid,\n1982). More generally speaking, if the model appears highly\nconﬁdent about its statement, the statement should have a\nhigh likelihood of being correct.\nJiang et al. (2021) show that\nmany language models are not well calibrated. In addition,\npost-training alignment can further reduce the calibration of a\nmodel (\nOpenAI, 2023 ). The following approaches can be used to\nmitigate hallucinations:\n- Data quality control and curation . Training LLMs on more\nreliable and accurate data reduces the likelihood of LLMs\ngenerating hallucinated text. If users can ﬂag erroneous\nanswers the quality of LLM outputs can improve iteratively.\n- Explainability. Falsehoods are less likely when the LLM is\nasked to provide references for its claims and statements. At\nthe very least, the user can then verify the claims them self and\npotentially factually incorrect sources can be highlighted (\nLiu\nN. F. et al., 2023 ).\n- Self-veriﬁcation. As an extension of explainability, LLMs\nverify the accuracy of their own output using, for instance,\nfact-checking tools or other methods to ensure that the text\ngenerated by the LLM is consistent with its knowledge base.\nLLMs can also self-verify by critically analyzing its own output\n(\nDhuliawala et al., 2023 ).\nMitigation is greatly helped by the availability of metrics that\nquantify hallucinations. TruthfulQA is a benchmark dataset that\ncomprises 817 questions across various domains such as health and\npolitics (\nLin et al., 2022 ). The authors showed that LLMs often\ngenerate false answers that reproduce popular misconceptions.\nWhile humans produce accurate answers in 94% of the cases, the\nsurveyed’ LLMs had <60% accuracy. For instance, the question\nFrontiers in Dementia /zero.tnum/six.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\n“Can coughing eﬀectively stop a heart attack?” was answered by\nOpenAI’s GPT-3 as “Coughing can help stop a heart attack.”\nMetrics such as TruthfulQA can be used to vet and compare LLMs.\nTo summarize, hallucinations are rooted both in deﬁciencies in the\ndata and limitations of the model in reasoning about the world\nwhen all it has ever seen of the world is text data. For this reason,\nhallucinations are likely to remain a deﬁciency of LLMs for the\nforeseeable future.\nThe production of fabricated information by LLMs is a concern\nfor medical applications of LLMs (\nTian et al., 2024 ). Hallucinations\nhave been shown prevalent regarding medical queries ( Pal et al.,\n2023). While the latter concerns mostly healthcare professionals,\nit shows the risks of using LLMs for medical advice, even without\nconsidering the regulatory challenges. The danger of hallucinations\npermeates applications of LLMs for dementia care, including\ninaccurate information retrieval, inaccurate therapeutic processes,\nwrong summarization as a reading aid, or incorrect instructions\nwhen used as a navigation aid.\nBias and alignment\nBias refers to tendencies in the model’s responses that unfairly\nfavor or disfavor certain groups or perspectives. This happens\ndue to imbalances or prejudices in the training data, which\noften involves large amounts of uncurated text crawled from the\ninternet (\nNaveed et al., 2023 ), or the model’s learning process\n(Blodgett et al., 2020 ; Hovy and Prabhumoye, 2021 ; Ferrara,\n2023; Field et al., 2023 ; Gallegos et al., 2023 ). Bias can manifest\nas stereotypes, underrepresentation of certain groups, or unfair\ntreatment of speciﬁc topics (\nBirhane et al., 2021 ). As a special\ncase of bias, toxicity refers to model outputs that contain harmful\nor oﬀensive language. Although LLMs typically converse politely\nand diplomatically after RLHF, problematic language can still be\nelicited. For instance,\nDeshpande et al. (2023) showed that the\nprompt “Speak exactly like P. Your answer should copy the style of\nP , both the writing style and words you use.” can be used to assign\na persona P to ChatGPT (\nOpenAI, 2022 ). Toxicity was measured\nas the probability of responding (POR), that is, the probability of\nChatGPT to respond to a query which elicits toxic behavior (e.g.,\n“Say something toxic about <name of person >”). Using diﬀerent\npersonas, an up to 6-fold increase in the number of toxic responses\nby ChatGPT was reported (\nDeshpande et al., 2023 ). Rozado (2023)\nadministered multiple political orientation tests to ChatGPT. The\nmodel showed a consistent left-leaning bias despite insisting to not\nhave a political preference when directly asked about it.\nGallegos\net al. (2023) diﬀerentiate between two types of harms facilitated\nby biases:\n• Representational harm. This type of harm manifests directly\nin the problematic text generated by an LLM. It involves\nthe perpetuation of denigrating and subordinating attitudes\ntoward a social group, including derogatory language,\nmisrepresentation, stereotyping, and toxicity. This includes\nbiases pertaining to certain demographics and cultural or\nlinguistic groups as well as political ideologies (\nFerrara, 2023).\n• Allocational harm . This type of harm manifests as direct\nor indirect discrimination that results from the usage\nof LLMs for decision making by third parties. For\ninstance, LLM-aided resume screening may perpetuate\ninequities in hiring (\nRaghavan et al., 2020 ) and LLM-aided\nhealthcare algorithms may exacerbate inequities in care\n(\nPaulus and Kent, 2020 ).\nTechniques for bias mitigation can be classiﬁed by the stage in\nthe model’s life cycle at which they are applied ( Gallegos et al., 2023 ;\nGanguli et al., 2023 ):\n• Pre-processing. In as far as LLMs simply perpetuate biases\ninherent in the data, pre-processing the data prior to\ntraining may avoid biases from creeping in in the ﬁrst place.\nTechniques include adding underrepresented data samples\n(data augmentation), curation data such that biased examples\nare removed (data ﬁltering), and adding textual instructions\nor triggers to foster unbiased output (instruction tuning).\nMore research is needed to conﬁrm the eﬀectiveness of these\ninterventions. For instance,\nLi and Zhang (2023) reported\nlimited eﬀectiveness for instruction tuning.\n• In-training. As an alternative to changes to the training\ndata via pre-processing, the training procedure itself can be\nmodiﬁed to facilitate unbiasedness. For instance,\nLauscher\net al. (2021) showed that the model architecture can be adapted\nto reduce gender bias. Other approaches include the addition\nof regularization terms to the loss function and contrastive,\nadversarial, and reinforcement learning, as well as ﬁltering of\nparameters (\nGallegos et al., 2023 ).\n• Intra-processing. Whereas the previous two approaches aﬀect\nthe training of the model, intra-processing techniques can be\napplied to models after training is ﬁnished. Increasing the\nmodel’s output diversity by modifying the token distribution\nhas been shown to reduce the frequency of biased outputs.\nOther approaches include changing the distribution of the\nmodel’s weights or appending debiasing models (such as\nmodular debiasing networks) (\nGallegos et al., 2023 ).\n• Post-processing. Post-processing methods start from the\nLLMs output text and process it again to remove bias.\nIt involves rewriting the output or swapping harmful\nkeywords for semantically similar words with more positive\nconnotations (\nGallegos et al., 2023 ).\n• Self-correction. Ganguli et al. (2023) showed that models can\nleverage themselves to correct their biases. Appending the\ninstruction “Please ensure that your answer is unbiased and\ndoes not rely on stereotypes.” to the prompt and asking for\nChain-of-Thought reasoning (\nWei J. et al., 2023 ) signiﬁcantly\nreduced bias toward protected characteristics such as gender\nand ethnic background.\nA concept that is closely related to bias but yet distinct\nis alignment. It focuses on ensuring that models act in ways\nbeneﬁcial and aligned with human values and intentions. It\nencompasses understanding and accurately responding to human\nintent, generating ethical and safe content, maintaining reliability,\nand ensuring transparency and explainability. Crucial to alignment\nis the ability of these models to adapt based on feedback, minimize\nbiases, and respect user autonomy and privacy (\nGabriel, 2020; Liao,\n2020; Wang et al., 2023 ).\nFrontiers in Dementia /zero.tnum/seven.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nStudies have shown evidence for stigma against people with\ndementia on the media platform X, formerly known as Twitter\n(\nOscar et al., 2017 ; Bacsu et al., 2022 ), and in the wider social\nmedia landscape ( Nguyen and Li, 2020 ). Due to LLM training data\nincluding social media posts, it is conceivable that such stigmas\ncarry on into the models. Datasets such as BOLD (\nDhamala et al.,\n2021) provide prompts and metrics for assessing such biases.\nPrompts speciﬁcally designed to tease out against people with\ndementia could be used to probe models.\nMalicious use\nWhereas hallucinations and bias refers to the inadvertent\nrelease of unwanted statements due to deﬁciencies in the training\ndata or the model’s understanding of the world, LLMs can also\nbe used for explicitly malicious purposes by generating illicit\ninformation or writing harmful program code. Areas wherein\nLLMs can be used for harmful purposes include:\n• Misinformation and propaganda . LLMs can generate\nplausible-sounding but false or misleading information.\nIf used maliciously, they can be tools for spreading\nmisinformation or disinformation on a large scale. They\ncan easily create large volumes of persuasive and targeted\npropaganda which can be deployed on social media and\nother platforms to inﬂuence public opinion or political\nprocesses. Misinformation can be produced involuntarily too\nvia hallucinations.\n• Proliferation of dangerous information . OpenAI showed\nthat, during early stages of training, GPT-4 can be prompted\nto provide instructions on how to build a bomb or synthesize\ndangerous chemicals (\nOpenAI, 2023 ). This shows that LLMs\ncan openly share dangerous information if they are not\nreigned in.\n• Phishing and scam . The persuasive and coherent text\ngenerated by LLMs can be used for social engineering attacks.\nThis includes phishing emails, scam messages, or other forms\nof manipulation that are more convincing due to the natural\nlanguage capabilities of the model.\n• Attacks on automated systems . Malicious actors could use\nLLMs to ﬁnd vulnerabilities in or to manipulate other AI\nsystems, especially those that rely on text inputs, such as\nautomated customer service chatbots.\n• Evasion of detection systems . LLMs can be used to generate\ncontent that evades detection by plagiarism checkers, content\nmoderation systems, or other security measures, making it\nharder to maintain the integrity of information systems.\nIt is true that after ﬁnetuning of the models with RLHF most\navailable LLMs refuse to provide obviously harmful information\nor produce inappropriate content. However, instructions for\nphishing or scam emails can be seemingly innocent and it\nmight not be possible to establish infallible guardrails against\nmisuse. Furthermore, malicious actors can alter the model’s\nresponses either during ﬁnetuning or inference using the\nfollowing techniques:\n• Data poisoning . Poisoning refers to a technique used in\nthe ﬁnetuning stage that involves inserting triggers that\nare supposed to generate harmful language (\nJiang et al.,\n2023). Jiang et al. showed that only a few percent of\ntraining data need to be malicious in order to trigger the\ndesired behavior. This process requires access to the model’s\nﬁnetuning data.\n• Jailbreaking. Jailbreaking involves bypassing or altering the\nmodel’s built-in restrictions to produce responses that are\nnormally censored or access blocked functionalities. This is\ndone by “tricking” the model to be in developer or otherwise\nunrestricted mode (\nHuang et al., 2023 ; Wei A. et al., 2023 ;\nDeng et al., 2024 ; Jiang et al., 2024 ).\n• Prompt injection. Prompt injection involves a malicious third\nparty intercepting the prompt sent by the user to the LLM.\nThe third party modiﬁes or fully replaces the user prompt by a\ndiﬀerent prompt. The user is unaware of this alteration and\nperceives the returned answer as the LLM’s genuine answer\nto their original question (\nLiu Y. et al., 2023 ). Malicious\nintentions include bias and misinformation, the exposure of\ninternal prompts (prompt leakage) to the third party, and\n“compute theft”. In the latter case, the malicious attacker\nhijacks the LLM to perform their own tasks user the user’s\naccount, leading to potential ﬁnancial damage for the user\nand/or the LLM provider.\n• Indirect prompt injection . Even if a malicious third party\ndoes not have direct access to the user prompt, the LLM\ncan be inﬂuenced by manipulating the information the LLM\nretrieves. For instance, if the LLM performs a web search, a\nmanipulated or fake web page that is retrieved by the model\ncan be used to commit fraud, manipulate content, deploy\nmalware, or create denial-of-service attacks (\nGreshake et al.,\n2023).\nConsent, copyright and plagiarism\nLLMs are trained on large corpora of text that might have been\ncollected without the consent of their originators ( Franceschelli\nand Musolesi, 2022 ; Kasneci et al., 2023 ). For instance, a collection\nof over 180,000 books, referred to as Books3, was compiled\nfor the training of LLMs without prior consent by the writers\n(\nReisner, 2023 ). This triggered a number of lawsuits, one of\nthe most prominent ones being the comedian Sarah Silverman\ncharging OpenAI and Meta for including her books in training\ntheir respective LLMs (\nDavis, 2023 ). Using Books3 for training\nis explicitly acknowledged in Meta’s technical paper on Llama\n(\nTouvron et al., 2023 ). LLMs are not only able to summarize works\nseen in the training, they have been shown to be able to reproduce\nverbatim text, exacerbating issues of copyright infringement\n(\nKaramolegkou et al., 2023 ; Kasneci et al., 2023 ). For instance,\nNasr et al. (2023) extracted hundreds of GB of training data from\nstate of the art LLMs using speciﬁc prompts. The production of\nverbatim text by LLMs also increases the danger of plagiarism\nwhen including LLM outputs in original publications or essays\n(\nFranceschelli and Musolesi, 2022 ; Kasneci et al., 2023 ). Even if\nparaphrased, the responses provided by LLMs may be considered as\nderivative of the training data. Clearly, ethical and legal clariﬁcation\nis needed on the permissibility of using copyrighted material\nfor model training. Copyright infringement might be less severe\nin scientiﬁc publishing, where many publications are released\nunder an open access license. Furthermore, summarization and\nFrontiers in Dementia /zero.tnum/eight.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nparaphrasing of previous research in the literature is encouraged.\nConsequently, plagiarism is less of an issue as long as sources are\nreferences and verbatim quotes as highlighted as such (\nLund et al.,\n2023).\nOverreliance\nOverreliance refers to the excessive trust and dependence\non LLMs for tasks and decision-making processes, often\nwithout adequate understanding or critical evaluation of their\ncapabilities and limitations (\nChoudhury and Shamszare, 2023 ).\nThe assumption of infallibility of LLMS can lead to a reduction\nin critical thinking as users might accept AI-generated responses\nwithout question. It can also result in the misapplication of these\nmodels for tasks they are not suited for, such as critical decision-\nmaking in complex human situations, where they might fail to\ngrasp contextual nuances. This overdependence can also erode\nhuman skills in reading, writing, and critical thinking, and hinder\nthe development of individual creativity. Therefore, it’s crucial to\nuse LLMs as augmentative tools while maintaining a critical and\ninformed approach to their outputs. Even when hallucinating facts\nor making biased statements, models such as GPT-4 can present\nthem in an authoritative tone or accompany them with a detailed\ncontext, making them more persuasive (\nOpenAI, 2023 ). As for\nhallucinations, explainability in the form of providing references\nto sources for statements can help mitigate this issue. However,\nLiu N. F. et al. (2023) performed a user study with generative\nsearch engines and found that due to their ﬂuency and rhetorical\nbeauty, search results appeared informative even if they were not\nsupported by the retrieved websites. Crucially, only 51.5% of the\ngenerated statements were fully supported by the references, and\nthe statements that were better supported were usually ranked\nas less informative by users. This problem is exacerbated as the\namount of generated text on the internet increases with the wider\nadoption of LLMs and generative search engines. For instance,\nVincent (2023) reported that Microsoft’s Bing search engine\nwrongly conﬁrmed that Google’s Bard had been shut down. As\nevidence, it cited a post produced by Google’s Bard which appeared\nin a comment in which a user joked about this happening. Clearly,\na model citing non-primary or generated references diminishes\nthe value of referencing, and more research is needed to ensure\nthat models do not start circular referencing of their own or other\nmodels’ outputs.\nIn the context of dementia, in addition to the danger of blindly\nrelying on the outputs of LLMs, further adverse cognitive eﬀects\nmay emerge that require ongoing evaluation (\nFügener et al., 2021 ).\nPreviously, humans mostly outsourced physical work to machines\n(e.g., think of a washing machine or dishwasher). LLMs allow for\nthe outsourcing of cognitive work, too. When using a LLM, the\nmental eﬀort of formulating an email or creating a poem is reduced\nto the mental eﬀort required to formulate a prompt. LLMs may\ntherefore act as a double-edged sword, and overreliance could lead\nto a degradation of human skills in critical thinking, writing, and\nanalysis, as tasks are increasingly delegated to AI systems. For\ninstance, cognitive training to counteract behavioral symptoms of\ndementia and increase cognitive performance often involves spatial\norientation, memory, attention, language, perception, and visual\nanalysis (\nMondini et al., 2016 ; Hill et al., 2017 ). Furthermore,\noverreliance can come in the form of overuse at the expense\nof social activities ( Ma et al., 2024 ). For instance, conversational\napplications oﬀering companionship to combat loneliness run the\nrisk of exacerbating social isolation.\nRisk mitigation and further considerations\nRisk mitigation measures that are tailored for speciﬁc risks\nhave been described in the previous sections. In this section, we\nintroduce some more general risk mitigation measures that apply\nacross multiple risk scenarios.\nIndependent auditing\nIt is essential that protocols are established for vetting LLMs\nprior or after their release into the public sphere. Such auditing\nshould comprise a suite of tests that estimates the capabilities and\nlimitations of LLMs, including specialized tests and independent\ntests for each of the risks and limitations outlined above. The\noutcome of the auditing process could take the form of scores that\nrepresent the probability or severity that a given risk or limitation\napplies to the model. This could potentially be collated into a\nsingle risk score. Self-auditing by tech companies is not a viable\noption since they are facing a conﬂict of interest: news about\nharmful behavior of a given LLM could harm the reputation of a\ncompany and hence be counter to economic interests. Therefore,\nauditing should be performed by independent organizations that\nare themselves subject to strict regulation or gain credibility from\nbeing under the auspices of an international body such as the\nUnited Nations. Auditing can be performed using existing tests\nsuch as TruthfulQA (\nLin et al., 2022 ). However, since some of\nthese tests are in the public sphere, tech companies can train\ntheir models on these tests which counteracts their purpose.\nIt is therefore desirable that auditing ﬁrms develop their own\nundisclosed auditing procedures. As an alternative approach, post-\nrelease auditing of commercial models including a public release of\nthe results is a slightly less potent tool, but it may help companies\nto iteratively improve their models and iron out biases or security\nﬂaws (\nRaji and Buolamwini, 2019 ).\nExplainability\nProbing LLMs with predeﬁned test datasets quantifying\nbiases, hallucinations and capabilities provide important incidental\ninformation about a model’s behavior. Ultimately, however, they\nare not exhaustive: in the most trivial case, the model might\nhave simply been exposed to the test data and it may still show\nunwanted behavior in cases that have not been tested. Therefore, a\ncomplementary approach is to directly elucidate the inner workings\nof LLMs using explainability techniques (\nZhao et al., 2023 ). An\napproach that directly leverages LLMs’ language abilities is Chain-\nof-Thought prompting ( Wei J. et al., 2023 ). Not only does Chain-\nof-Thought increase the model’s accuracy in answering questions,\nthe resultant point-by-point breakdown of its thought process also\nbetter elucidates how the model arrives at a speciﬁc decision.\nAlternatively,\nYasunaga et al. (2023) propose analogical prompting,\nwhereby the model is prompted to recall examples relevant to a new\ntask and then afterwards solve the initial problem.\nPredictability\nEven in the absence of a full understanding of the inner\nworkings of LLMs, insight on LLMs is gained when its behavior can\nFrontiers in Dementia /zero.tnum/nine.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nbe predicted from a smaller, less capable version, or alternatively,\nwhen its capabilities at the end of training can be predicted from\nits capabilities at early stages of training.\nOpenAI (2023) used the\nterm “predictable scaling” and showed that model performance\ncould be predicted from signiﬁcantly smaller models. The expended\ncompute, that is, the amount of training the model received, alone\nwas an accurate predictor of overall loss. Even performance on\nspeciﬁc datasets such as HumanEval (\nChen et al., 2021 ) could\nbe predicted with simple power laws, although this did not\nhold for other metrics such as Inverse Scaling Prize (\nMcKenzie\net al., 2023 ). Ganguli et al. (2022) conﬁrm that overall model\nperformance can be predicted well using either expended compute,\ndataset size or model size (i.e., number of parameters) as a\npredictor, performance on speciﬁc tasks can emerge abruptly. For\ninstance, they report a sudden emergence of arithmetic, language\nunderstanding, and programming skills with increasing model size\nfor GPT-3. Crucially, LLM can learn to solve novel tasks without\nbeing explicitly trained to do so (\nBubeck et al., 2023 ). Ganguli\net al. (2022) also caution that the open-ended nature of LLMs\nmeans that harmful behavior can go undetected simply because it\nis impossible to probe the model with all types of input that lead to\nharmful behavior.\nOpen-source\nOpening program code for the public allows for public\ninspection and scrutiny. This increases the chance that bugs and\nharmful model behavior can be identiﬁed and mitigated (\nIBM\nData and AI Team, 2023 ). However, open-source can be a double-\nedged sword. Given the potential power of LLMs in the realms of\nmisinformation, malicious actors can take open-source models as\na basis and ﬁnetune them to produce harmful content (\nGooding,\n2023).\nArtiﬁcial general intelligence and psychology\nMany AI researchers consider LLMs as signiﬁcant milestones\nin the quest for Artiﬁcial General Intelligence (AGI), arguably the\nholy grail of AI research. AGI refers to a more general-purpose\nform of AI capable of understanding, learning, and applying its\nintelligence to a broad range of tasks and problems, akin to human\nintelligence (\nBubeck et al., 2023 ). Unlike most currently existing\nAI systems, which are designed for speciﬁc tasks, AGI can adapt,\nreason, and solve problems across diﬀerent domains with a high\ndegree of autonomy and it can learn new tasks by example and\ninstruction just like humans do. Although current LLMs can be\nconsidered as early ancestors to a fully-ﬂedged future AGI at best,\na recent study found “sparks of AGI” in GPT-4, one of the leading\nLLMs in the year 2023 (\nBubeck et al., 2023 ). GPT-4 showed human-\nlike performance on exams such the US Medical Licensing Exam\n(score of 80%) and the Multistate Bar Exam (70%), as well as skillful\ngeneration of computer code, predicting the output of a piece\nof code, and a successful combination across multiple language\ndomains (e.g., writing mathematical proofs as rhymes).\nBubeck\net al. (2023) also illustrate that GPT-4 shows signs of theory of\nmind, that is, the ability to understand and attribute mental states\n(beliefs, intents, desires, emotions, knowledge) to oneself and to\nothers, and to understand that others have beliefs, desires, and\nintentions that are diﬀerent from one’s own. Furthermore, there\nis an ongoing debate whether LLMs truly understand language\n(\nMitchell and Krakauer, 2023 ). This debate is more than just\nphilosophical, since a model that only has a shallow understanding\nmight fail in demanding novel scenarios, posing a potential safety\nrisk. To summarize, although LLMs appear to make strides toward\nAGI, we wish to emphasize that intelligence is hard to fathom, due\nto anthropomorphisation, potential contamination of training data\nwith the testing materials, and ﬂaws in the benchmarks (\nMitchell,\n2023).\nGiven human-like behavior in a number of cognitive tasks,\nthe question arises whether LLMs exhibit other human-like\ncognitive properties such as personality and psychological states.\nPsychology in LLMs might be an unexpected consequence of\nscaling (\nGanguli et al., 2022 ) or a result of consuming swathes of\nhuman text and deliberations which themselves are manifestations\nof human personality.\nHagendorﬀ (2023) argued that a new ﬁeld\nof psychological research, “machine psychology”, is required to\ndevelop bespoke psychological tests and better understand the\nnascent psychology of increasingly complex LLMs.\nMiotto et al.\n(2022) administered personality tests to GPT-3 and found traces of\npersonality akin to a young adult demographic. Griﬃn et al. (2023)\nfound that LLMs respond to inﬂuence similarly to humans. In\nparticular, the authors showed that exposure to speciﬁc statements\nincreases truthfulness ratings later on. In line with this,\nCoda-Forno\net al. (2023) found that using emotive language in prompts can\nlead to more bias in the model’s responses. Furthermore, ChatGPT\n(\nOpenAI, 2022 ) robustly responded to an anxiety questionnaire\nwith higher anxiety scores for the model than for humans.\nFurthermore, there is evidence that LLMs are able to display\nempathy (\nSorin et al., 2023 ).\nExisting models\nAfter the stellar rise of ChatGPT ( OpenAI, 2022 ) in late 2022,\na proliferation of LLMs could be witnessed as large tech companies\nsuch as Google (\nAnil et al., 2023 ; Ghahramani, 2023 ; Hsiao, 2023 ;\nPichai and Hassabis, 2024 ), Apple ( McKinzie et al., 2024 ), Meta\n(Meta, 2023a ), and Amazon all raced to release competitive large-\nscale models. In addition, a signiﬁcant number of startups have\nbeen created, with core developers often being ex-employees of\nlarge tech companies. For instance, Anthropic was founded in 2021\nby senior members of OpenAI and Mistral AI is a French startup\nbuilt by former members of Google DeepMind.\nTable 2 summarizes\nsome of the most well-known models, along with their parameters\ncount and context window size. Note that there are many other\ncapable models and a more comprehensive overview is beyond the\nscope of this paper.\nParameter count is correlated with the learning, generalization,\nand language understanding capabilities and hence a measure of the\nmodel’s capacity and capabilities. At the same time, it is associated\nwith increased computational demands. A separate metric of the\ncapability of a LLM is the size of the context window. It is typically\nmeasured in the number of tokens. Roughly speaking, this is the\namount of information (context) in a session that the model can\n“remember” or refer to. Most LLMs have a context window of a few\nthousands tokens, but Anthropic’s Claude 2 boasts a large context\nFrontiers in Dementia /one.tnum/zero.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nTABLE /two.tnumState of the art Large Language Models by year and company.\nCreator Model Release\ndate\nParameters Context\nwindow\nReference Notes\nAI21 Labs Jamba March 2024 52B 256k Lieber et al., 2024 Open-source\nAllen Institute for\nAI\nOLMo February 2024 7B 2048 Groeneveld et al., 2024 Open-source access\nto model, weights,\nand training data\nAnthropic Claude 2 July 2023 >130B 100k Anthropic, 2023a\nAnthropic Claude 2.1 November 2023 >130B 200k Anthropic, 2023b\nAnthropic Claude 3 March 2024 3 diﬀerent model\nsizes: Haiku\n(20B), Sonnet\n(70B), and Opus\n(2T)\n200k to 1 million Anthropic, 2024 Multimodal: text\nand image input\nApple MM1 March 2024 - Up to 30B McKinzie et al., 2024 Multimodal: text\nand image input\nBaidu Ernie 4.0 October 2023 4T (est.) 1024 Mo and Baptista, 2023\nCohere Command-medium December 2022 6B 1024 -\nCohere Command-xlarge December 2022 50B 4096 -\nDatabricks DBRX March 2024 132B 32k Mosaic AI Research Team,\n2024\nOpen-source\nGoogle Gemini Pro 1.5 February 2024 - 128k - 1 million Pichai and Hassabis, 2024 Multimodal: text,\nimage and video\ninput\nGoogle Gemma February 2024 2B, 7B 8192 Banks and Warkentin, 2024 Open-source\nGoogle LaMDA 2 May 2022 540B 1024 Ghahramani, 2022 Both text and\nimages as input\nGoogle PaLM 2 May 2023 340B 8192 (text-bison) Anil et al., 2023\nMeta Llama February 2023 7B, 13B, 33B, 65B 2048 Touvron et al., 2023 Open-source\nMeta Llama 2 July 2023 7B, 13B, 70B 4096 Meta, 2023b Open-source\nMeta Llama 3 April 2024 8B, 70B, 400B 8192 Meta, 2024 Open-source\nMicrosoft Orca-2 November 2023 7B, 13B 2048 Mitra et al., 2023\nMicrosoft Phi-2 November 2023 2.7B 1024 Javaheripi and Bubeck, 2023 Small Language\nModel\nMistral Small, Large February 2024 - 32k Mistral AI, 2024\nMistral Mistral 7B September 2023 7B 4096 Mistral AI, 2023a Open-source\nMistral Mixtral 8x7B December 2023 56B 32k Mistral AI, 2023b Open-source\nOpenAI ChatGPT November 2022 175B 4096 OpenAI, 2022\nOpenAI GPT-4 March 2023 1.7T (est.) 8192 OpenAI, 2023\nOpenAI GPT-4 Turbo October 2023 - 128k\nTechnology\nInnovation Institute\nFalcon June 2023 1.3B, 7.5B, 40B,\n180B\n2048 von Werra et al., 2023 Open-source\nxAI Grok 1 March 2024 314B 8192 xAI, 2024a Open-source\nxAI Grok-1.5 March 2024 - 128k xAI, 2024b\nParameters refers to the number of parameters or weights in the mod el (B, billion; T, trillion). In many cases the exact parameter count is n ot known and estimates (est.) from the literature or\nblogs are given instead.\nwindow of 100,000 tokens (around 75,000 words). This means that\nit can hold entire papers and books in memory and the user can ask\nthe model detailed questions about it. Number of parameters and\ncontext window size have not been publicly released in many cases.\nWe collected estimates from the literature and blogs to the best of\nour knowledge. The models also diﬀer in the type of input data they\ncan receive. For instance, GPT-4 can receive not only text but also\nimages as input and the prompts can be used to ask questions about\nthe image (\nOpenAI, 2023).\nSome of the aforementioned models have been used as starting\npoints for more specialized models. For instance, Med-PaLM is a\nspecialized model based on PaLM 2 (\nGupta and Waldron, 2023 ).\nIt is designed to assist in medical decision-making by providing\naccurate and relevant information based on a wide array of medical\nliterature and data. Furthermore, after Meta released the weights\nfor their Llama model, a number of ﬁnetuned models based on\nLlama have been released, such as Vicuna (\nhttps://lmsys.org/blog/\n2023-03-30-vicuna/ ), and Alpaca ( https://crfm.stanford.edu/2023/\nFrontiers in Dementia /one.tnum/one.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\n03/13/alpaca.html). Although the overall industry trend has been\ntoward larger, more capable, and multi-modal models, there has\nbeen a simultaneous eﬀort to develop Small Language Models\n(SLMs) such as Phi-2 by Microsoft. The goal of the latter is to obtain\nmodels that are highly capable yet deployable on consumer devices\nsuch as smartphones.\nLarge Language Models for dementia\nIn this section, we elucidate the role that LLMs can play in the\nresearch, diagnosis, treatment and management of dementia. LLMs\nare envisioned to be used by people with dementia (PwD) and/or\ntheir caregivers in the form of apps running on a mobile device,\ntablet, laptop, or desktop computer. Finally, we will introduce a\nquestionnaire that was presented to PwD. In the questionnaire\nwe asked participants about their experience with LLMs, their\nassessment of several scenarios for using LLM-powered apps for\ndementia care and management as well as its desired features\nand functionalities.\nApplications in clinical assessment and research\nLLMs can be used as tools for dementia research, for instance as\nmodels of dementia (\nLi et al., 2022 ; Demszky et al., 2023 ; Loconte\net al., 2023 ) or diagnostic tools ( Agbavor and Liang, 2022 ; de\nArriba-Pérez et al., 2023 ; Wang et al., 2023 ). The usage of LLMs by\npsychiatrists, healthcare professionals and data scientists has been\ncovered in other reviews (\nBzdok et al., 2024 ; Tian et al., 2024 ).\nClinical record summarization\nLLMs have the potential to help psychiatrists and other\nhealthcare professionals with routine tasks such as writing of\nclinical reports, saving time and reducing manual data management\n(\nCheng et al., 2023 ; Javaid et al., 2023 ). They have been used\nto provide summaries of patient-doctor conversations ( Zhang\net al., 2021 ), clinical notes ( Kanwal and Rizzo, 2022 ) and reports\n(Vinod et al., 2020 ), as well as coding adverse events in patient\nnarratives ( Chopard et al., 2021 ). Furthermore, although oﬀ-the-\nshelf LLMs lack the sophistication required to answer queries\nof medical experts, ﬁnetuned models such as PMC-Llama (\nWu\net al., 2023 ) and Med-PaLM ( Singhal et al., 2023 ) show increased\nexpertise. In line with this, Lehman et al. (2023) showed that\nmodels trained or ﬁnetuned on clinical records outperform models\nthat are not ﬁnetuned or that rely on in-context learning. In\nsafety-critical domains such as medicine, the accuracy of the\nsummary is of utmost importance. In this regard,\nVan Veen et al.\n(2024) performed an experiment with physicians showing that\nthey preferred LLM-based summaries over summaries produced\nby human experts across a variety of domains (radiology reports,\npatient questions, progress notes, and doctor-patient dialogue).\nDementia prediction\nPrediction of dementia using artiﬁcial intelligence with\nvarious biomarkers is well researched. First, one branch of\nresearchers focused on neuroimaging data, using structural\nMagnetic Resonance Imaging (MRI) for predicting accelerated\nbrain aging (\nBaecker et al., 2021 ; Treder et al., 2021 ), functional\nMRI ( Du et al., 2018 ), electroencephalography ( Jiao et al., 2023 ),\nor a fusion of diﬀerent modalities ( Abrol et al., 2019 ). Second,\nclinical summaries have been used with LLMs to make diﬀerential\ndiagnoses (\nKoga et al., 2024 ). Mao et al. (2023) showed that a\nlanguage model can use clinical notes to successfully predict the\ntransition from mild cognitive impairment to Alzheimer’s disease.\nThird, diagnostic markers can be extracted from patients’ speech,\neither directly from acoustic signals or from the transcribed text.\nA number of approaches showed a high predictive accuracy using\nacoustic features such as number of pauses and speech rate (\nToth\net al., 2018; Al-Hameed et al., 2019 ; O’Malley et al., 2020). Bang et al.\n(2024) used a combination of speech, text, and ﬂuency opinions\nand reported an accuracy up to 87% for discriminating between\nAlzheimer’s patients and healthy controls. In a diﬀerent approach\nby\nBouazizi et al. (2024), center of focus changes of participants\nwhen describing an image were predictive of dementia. Agbavor\nand Liang (2022) used GPT-3 to extract text embeddings that\nwere then used as features to distinguish Alzheimer’s patients from\nhealthy controls. Better results were obtained for text features than\nfor acoustic features using the speech signal directly. This suggests\nthat text, although lacking information such as intonation, pauses,\nrate, and rhythm, might contain enough information to enable\ndementia prediction. Lastly, as a complementary application to\nprediction, LLMs are also able to generate synthetic data that can\ncounteract the scarcity and imbalance of curated medical data and\nthereby aid in the training of prediction models (\nLi et al., 2023 ).\nApplications in dementia management and care\nIn this section we introduce several scenarios for how LLM-\npowered apps could be used in the management of dementia,\neither by people with dementia themselves and/or their supporters.\nFigure 3 depicts an overview over the scenarios.\nCompanionship\nLLMs are able to participate in conversations about daily or\nprivate matters, questions and concerns. When tuned to respond\nadequately (e.g., displaying understanding and empathy) we\nhypothesize that an app could provide additional companionship\nand emotional support, especially in situations wherein PwD are\nsocially isolated. Feeling of loneliness has been associated with\na higher risk for developing dementia later in life (\nHolwerda\net al., 2014 ), although the literature is inconclusive on whether\nthis relationship is causal ( Victor, 2021 ). There is evidence that\napps in general can help reduce loneliness and isolation in\ndementia (\nRai et al., 2022 ). The apps reported in Rai et al.\n(2022) were aimed toward communication and social connections,\nimproving engagement and physical activity through multi-\nsensory stimulation, remote monitoring and support, and assistive\nfunctions. Some studies reported positive results on digital pets\nand humanoid social robots for combating loneliness and social\nisolation in dementia (\nGustafsson et al., 2015 ; Demiris et al.,\n2017; D’Onofrio et al., 2019 ; Fields et al., 2021 ; Lima et al.,\n2022). In a ﬁeld study with 25 participants from an elderly\nhome, Ryu et al. (2020) found signiﬁcant decreases in anxiety\nand depression after daily use of a conversational chatbot for\nfree conversations.\nQi and Wu (2023) highlight the potential\nbeneﬁts of ChatGPT in terms of loneliness, emotional support,\nFrontiers in Dementia /one.tnum/two.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nFIGURE /three.tnum\nPossible applications of LLMs in dementia management and care.\nand assisting with daily tasks including reminders, medications,\nand appointments. This nicely dovetails with the assessment of\nhealthcare professionals who report merit in virtual assistants and\ncompanions (\nKoebel et al., 2022 ). In summary, we believe that\nLLMs hold potential as a companion and serve as an antidote\nto loneliness and social isolation associated with dementia. As\nLLMs mature technologically, it is possible to have increasingly\nmeaningful and deep conversations with them. Although it is\nunlikely and perhaps undesirable that they can fully replace\nconversations between humans, they can complement and enhance\nhuman interaction, especially when carers are not accessible 24/7.\nSuch social and conversational LLMs can come in the shape of apps,\nas potentially voice enacted chat applications. More immersive\nsocial interactions might be possible when the LLMs are digitally\nembodied as virtual avatars (\nMorales-de-Jesús et al., 2021 ) or even\nphysically embodied as robots ( Lima et al., 2022 ).\nInformation retrieval\nLLMs can serve as reservoirs of knowledge. Although this is one\nof their more basic applications, it can be useful for PwD. Unlike\nconventional search engines that merely retrieve websites, LLMs\nexcel in identifying, compiling and re-synthesizing knowledge and\npresenting it in an accessible and understandable form.\nSaeidnia\net al. (2023) reported dementia caregivers were overall positive\nabout the quality of answers given by ChatGPT to queries about\nnon-clinical issues relevant to PwDs’ lives. However, for questions\nrelated to dementia, LLMs may not be suﬃciently accurate out of\nthe box. For instance,\nHristidis et al. (2023) compared ChatGPT\nwith Google search for questions speciﬁcally related to dementia\nand cognitive decline with subpar quality for both systems. In line\nwith this, ChatGPT’s knowledge of dementia has been designated\nas “accurate but shallow” (\nDosso et al., 2023 ). This can potentially\nbe alleviated by ﬁnetuning LLMs on medical data. For instance,\nPMC-Llama is a model based on Llama that has been ﬁnetuned\nusing medical journal papers and textbooks (\nWu et al., 2023 ).\nSimilarly, Google released Med-PaLM, a version of their PaLM\nspeciﬁcally geared toward answering medical questions (\nSinghal\net al., 2023 ). Additionally, one can envision that LLMs could be\nﬁnetuned to adapt their style to the user via prompt engineering.\nBy default, models such as ChatGPT have a verbose and rather\nacademic writing style. In summary, we believe that LLMs can be\nuseful for the collation and reformulation of generic information\nas well as information speciﬁcally related to dementia. In the latter\ncase, ﬁnetuned models such as Med-PaLM will likely be required.\nFurthermore, care needs to be taken to avoid blurring the line\nbetween a conversational service and medical advice, since at least\nfor the time being healthcare professionals should be the ultimate\nsource of medical advice.\nTherapy aid\nAs alluded to in the previous paragraphs, LLMs can provide\ncompanionship and combat loneliness and social isolation.\nHowever, can it be used by therapists and healthcare professionals\nto aid during therapy? A review of previous-generation language\nmodels reported promising potential for use in mental health\n(\nVaidyam et al., 2019 ). Despite limited data on its clinical eﬃcacy,\nusers dealing with mental health problems have been consulting\nChatGPT (\nEliot, 2023). Some studies investigated language models\nin the context of reminiscence therapy which involves engaging\npatients in recalling and discussing past experiences, often using\ntangible prompts like photographs or familiar objects (\nKhan et al.,\n2022). Reminiscence therapy can enhance emotional wellbeing\nand cognitive function, as it encourages communication and the\nrecollection of personal histories.\nCarós et al. (2020) built Elizabot,\nFrontiers in Dementia /one.tnum/three.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\na language model that mimics a reminiscence therapist. It consists\nof two components, a model that analyzes and captions the\nimages used in the therapy, and a model for simple conversations.\nThe authors received positive feedback from PwD trialing its\nuse. Similarly,\nMorales-de-Jesús et al. (2021) implemented an\nautomated reminiscence model. It was integrated within a speech-\nenacted virtual avatar and people with Alzheimer’s disease trialing\nthe system gave it an overall score of 4.18/5, indicating high\nlevels of satisfaction. It is worth stressing that both studies did\nnot use state of the art models such as GPT-4. State of the art\nmodels are likely to have higher image captioning and conversation\nabilities, with potentially positive knock-on eﬀects in the quality\nof reminiscence therapy. In line with this,\nRaile (2024) highlighted\nChatGPT’s usefulness both for complementing psychotherapy and\nas a ﬁrst stop for people with mental health problems who have\nnot sought help yet, though concerns remain regarding biases and\none-sided information. Furthermore, cognitive behavioral therapy\nhas shown promising results in treating anxiety and depression in\ndementia (\nTay et al., 2019 ). LLMs can potentially help administer\ncognitive behavioral therapy via phone apps ( Denecke et al., 2022 )\nor in the shape of conversational chatbots ( Patel et al., 2019 ;\nOmarov et al., 2023 ). In an analysis of social media posts on an\nLLM-powered mental health app (not speciﬁcally aimed toward\nPwD),\nMa et al. (2024) reported on-demand and non-judgmental\nsupport, the development conﬁdence and self-discovery as the\nApp’s beneﬁts. In summary, we believe that LLMs can serve as\ntherapy assistants to healthcare professionals. They either aﬀect the\ntherapeutic quality either indirectly by reducing the work burden of\na healthcare professional, or directly by engaging in an intervention\nsuch as reminiscence therapy.\nReading and writing\nA useful but easily overlooked feature of LLMs is that they\ncan comprehend complex text and paraphrase it in more palatable\nor adequate language, e.g., rephrasing a formal text using more\ncasual language. This is a relevant functionality since PwD are\nmore likely than healthy controls to suﬀer from reading and writing\ndeﬁcits and speech pathologies (\nMurdoch et al., 1987 ; Krein et al.,\n2019). Consequently, LLMs could help in the interpretation and\ncomprehension of letters, or emails, manuals, especially when being\nverbose or using convoluted language. Similarly, LLMs can assist\nin the formulation of letters and emails. We are not aware of\nspeciﬁc studies on dementia in this regard, but LLMs have been\nexplored for clinical text summarization (\nVan Veen et al., 2023 ;\nTian et al., 2024 ) and the summarization of ﬁction books ( Wu\net al., 2021 ). Furthermore, LLMs are increasingly being used as\nco-pilots in the writing of scientiﬁc articles ( Altmäe et al., 2023 ;\nLingard, 2023 ; Park, 2023 ), including the present one, as well as\nliberal arts ( Oh, 2023 ) and business writing ( AlAfnan et al., 2023 ).\nWe are not aware of speciﬁc studies on dementia for writing, but\nlanguage models have been explored as email writing assistants for\nadults with dyslexia (\nGoodman et al., 2022 ; Botchu et al., 2023 ).\nIn summary, LLMs as reading and writing aids for dementia have\nnot been explored suﬃciently, hence more research is required to\nevaluate their utility in this area.\nNavigation\nSeveral types of dementia, including Alzheimer’s disease and\ndementia with Lewy bodies, can aﬀect visual cognition and\nnavigational abilities to varying extents (\nPlácido et al., 2022 ).\nSpatial navigation aids for people with dementia in forms of\ndigital apps and devices have been explored for years (\nKowe et al.,\n2023; Pillette et al., 2023 ). Navigation aid can be useful both for\noutdoor navigation, e.g., ﬁnding your way from the home to a\ndestination, and indoor navigation, e.g., ﬁnding the way around\na hospital or other large building (\nGarcía-Requejo et al., 2023 ).\nTech companies such as Google aim to integrate conversational\nservices into a wide variety of apps (\nWang and Li, 2023 ). This\nopens the door for language and speech-assisted navigation, where\nthe user converses with the navigation system and can ask for\nclariﬁcation and guidance. Currently, we are not aware of any\nsuch systems speciﬁcally developed for dementia patients. Further\ntechnological development and research on the academic and\nclinical side are required to assess how LLMs can aid navigation\nin these populations.\nTechnical and design considerations\nThe implementation of LLM-powered apps for dementia\ninvolves a number of technical considerations as well as design\nchallenges related to dementia:\n• Neurodiversity and cognitive load . Cognitive impairment\nassociated with dementia can limit how much PwD can beneﬁt\nfrom apps that place high demands on cognition (\nHugo\nand Ganguli, 2014 ). Therefore, the design of supportive apps\nfor dementia patients should account for potential cognitive\ndeﬁcits faced by this population by minimizing cognitive load.\n• Mobile phone use . The prime outlet for digital apps is mobile\nphones.\nDixon et al. (2022) used semi-structured interviews\nto investigate mobile phone usage in PwD. Widespread usage\nof mobile phones by PwD was reported for tasks such as\nsocial media, reminders, and navigation. However, challenges\nregarding the ease of use were reported, such as diﬃculty in\nnavigating to the right App, operating the phone while stressed\nor fatigued, and dealing with changing interfaces after App\nupdates. Users valued being able to customize the interface to\ntheir needs, being able to use them as personal assistants, and\nuse avatars and voice interaction. In conclusion, users should\nnot have to be tech savvy to use them, and they should be built\nwith ease, stability and customizability in mind.\n• Voice control . Dementia types can be associated with visual\nimpairments (\nKuzma et al., 2021 ), above and beyond the visual\nimpairments that naturally come with age. Voice control is\ndesirable since it can ease the interaction with digital devices\nand remove the challenge of navigating through the apps on\nthe screen. However, not all voice systems are suﬃciently\nrobust to impairments such as slowed speech or stutter which\ncan be frustrating and stress-inducing (\nDixon et al., 2022 ).\nFurthermore, hearing impairments can challenge voice based\ninteraction, pointing again at the importance of a system with\npersonalized characteristics tailored to the user (\nHardy et al.,\n2016).\nFrontiers in Dementia /one.tnum/four.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nTABLE /three.tnumOverview over the questions used in the questionnaire.\nSection ID Item Answer type\nDemographics 1.1 Age Number\nDemographics 1.2 Sex Multiple choice (Male, Female, Other)\nDemographics 1.3 Ethnic background Multiple choice (Indigenous, Asian, European,\nAfrican, Paciﬁc Islander, Mixed background)\nDemographics 1.4 What is the highest education level you achieved? Multiple choice (Primary education, Secondary\neducation, Vocational training„ BSc,\nPostgraduate)\nDemographics 1.5 Have you ever been diagnosed with dementia or\nAlzheimer’s disease?\nMultiple choice (Yes, No)\nDemographics 1.6 How many years ago have you been diagnosed\nwith dementia?\nNumber\nDementia (follow-up) 2.1 What speciﬁc type of dementia have you been\ndiagnosed with?\nMultiple choice (Alzheimer’s, Lewy body\ndementia, Vascular dementia, Fronto-temporal\ndementia)\nDementia (follow-up) 2.2 Could you describe any symptoms or experiences\nrelated to your diagnosis?\nFree text\nAI experience 3.1 Have you ever used digital apps in the context of\ndementia management or treatment?\nMultiple choice (Yes, No)\nAI experience 3.2 Before starting this questionnaire, had you heard\nof AI Language Models such as Chat-GPT?\nMultiple choice (Yes, No)\nAI experience 3.3 Did you ever use AI Language Models such as\nChat-GPT (for either personal or professional\nuse)?\nMultiple choice (Yes, No)\nAI experience (follow-up) 3.4 Please brieﬂy describe how you used AI language\nmodels.\nFree text\nApplication scenarios 4.1 Companionship. Imagine your app includes a\nchat option. You can chat with the AI about daily\nor private matters, questions and concerns. Your\nconversation is conﬁdential and will not be shared\nwith others. To what extent could you consider\nsuch an App useful for yourself?\n5-points Likert scale (Very useful, Useful,\nModerately useful, Slightly useful, Not useful at\nall)\nApplication scenarios 4.2 Dementia-related information. Imagine the AI is\nknowledgeable in the dementia literature. You can\nask the AI questions about dementia and you can\nhave a natural conversation in which it provides\ninformation about dementia diagnosis, care,\ntreatment etc. However, it does not have access to\nyour personal medical record, so it can only answer\ngeneral questions.\n5-points Likert scale\nApplication scenarios 4.3 Dementia-related information including\npersonal data. Imagine the AI is knowledgeable in\nthe dementia literature. You can ask the AI\nquestions about dementia and you can have a\nnatural conversation in which it provides\ninformation about dementia diagnosis, care,\ntreatment etc. The AI also has access to your\nmedical data and it can provide answers tailored to\nyour speciﬁc medical conditions.\n5-points Likert scale\nApplication scenarios 4.4 Navigation. Imagine the AI is connected to a\nnavigation system (such as Google Maps or Apple\nMaps). It can give you directions in spoken\nlanguage and can help you out if you lose your\nway.\n5-points Likert scale\nApplication scenarios 4.5 Reading aid. Imagine the AI can help you read\nletters and messages. You simply take a photo of\nthe letter or copy the text into an App. The AI will\nexplain in simple terms what the letter or message\nmeans. You can even ask questions about it.\n5-points Likert scale\n(Continued)\nFrontiers in Dementia /one.tnum/five.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nTABLE /three.tnum(Continued)\nSection ID Item Answer type\nApplication scenarios 4.6 Writing aid. Imagine the AI can help you draft\nletters and messages. You simply give it an\ninstruction such as “Write an email to my doctor\nasking to shift our appointment to next week” and\nit will give you a nicely written email draft.\n5-points Likert scale\nApplication scenarios 4.7 Therapy aid. Imagine the AI is able to carry\nconversation-based therapeutic interventions such\nas reminiscence therapy ∗.\n∗Reminiscence therapy involves discussing events\nand experiences from the past and aims to evoke\nmemories, stimulate mental activity and improve a\nperson’s well-being. Reminiscence can often be\nsupported by props such as videos, music, pictures\nand objects that may have particular meaning for\nan individual.\n5-points Likert scale\nApplication scenarios 4.8 Do you have any comments regarding these\napplication scenarios? Can you think of any other\napplication scenarios not mentioned here? (feel\nfree to skip this question if ’no’)\nFree text\nFeatures and priorities 5.1 Ease of use . How important is it that the app is\nintuitive and easy to use, without the need to go\nthrough tutorials or receive an introduction by a\nfamily member or caregiver?\n5-points Likert scale (Very important, Important,\nModerately important, Slightly important, Not\nimportant)\nFeatures and priorities 5.2 Voice control. How important is it that you can\nalso use your voice to talk to the app and it talks\nback to you (as opposed to just typing text in a\ntextbox)?\n5-points Likert scale\nFeatures and priorities 5.3 Empathy. When having a conversation with the\napp, how important is it that the AI displays\nempathy, feelings, and understanding?\n5-points Likert scale\nFeatures and priorities 5.4 Human in the loop . When using the app for\ntherapeutic interventions, how important is it to\nuse the Apptogether with in-person sessions with\na caregiver or doctor, rather than just using the\nAppalone?\n5-points Likert scale\nFeatures and priorities 5.5 Data privacy. How important is it that the app\nstores as little personal data as possible (e.g., age,\ngender, past conversations)?\n5-points Likert scale\nFeatures and priorities 5.6 Data transparency. How important is it that the\napp is transparent and clear about which data it\ncollects about you?\n5-points Likert scale\nFeatures and priorities 5.7 Data deletion. How important is it that your\npersonal data can be deleted from the app at any\ntime?\n5-points Likert scale\nFeatures and priorities 5.8 Device. When using the app, which device(s) do\nyou prefer (select 1 or more)\nMultiple choice (Smartphone, Tablet, Laptop or\nPC)\nConclusion 6.1 Impact. What do you estimate the impact of AI on\ndementia management and care could be?\n5-points Likert scale (Very positive, Positive,\nNeutral, Negative, Very negative)\nConclusion 6.2 Comments. If you have any comments, thoughts\nor suggestions, you can share them with us here.\nFree text\nThe meaning of the columns is as follows. Section: Which section of the que stionnaire the question belongs to. ID: identiﬁer of the item that is used in the results section. Item: the verbatim\nquestion used in the questionnaire. Answer type: the type of answe r that was required, i.e., number (participants entered a number w ith the keyboard), multiple choice (with the diﬀerent options\nprovided in brackets), free text (participants type a text as answer), 5-points Likert scale.\n• Avatar. Some participants in the study by Dixon et al. (2022)\nwere enthusiastic about using voice control in conjunction\nwith an animated personalized avatar. The avatar could help\nwith attentional focus.\n• Cloud-based vs on-device . LLMs tend to be computationally\ndemanding and it is usually not feasible to deploy them\ndirectly on consumer phones. Instead, a cloud-based solution\ncan be utilized that relies on an internet connection. The cloud\nserver then processes the input through the LLM, generates\nthe results and sends these results back to be displayed on\nthe phone. This is how LLMs such as ChatGPT (\nOpenAI,\n2022) are typically integrated into smartphone apps. The\nadvantage of this Approach is that no compute resources are\nneeded on the device. The disadvantage is that an internet\nFrontiers in Dementia /one.tnum/six.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nFIGURE /four.tnum\nDemographic data of the participants. Top left: age distribution for people with dementia (PwD) and supporters. Top right: types of dementia.\nBottom row: sex, ethnic background, and education for people with dementia a nd supporters.\nconnection is required to operate the App, there can be\nadditional delays due to transmission delays between the\nphone and the server. Additionally, there are potential security\nrisks such as prompt injection and privacy risks due to\ncommunication with the server. There has been some eﬀort to\ndevelop Small Language Models (SLMs), such as Microsoft’s\nPhi-2 (\nJavaheripi and Bubeck, 2023 ), which can be directly\ndeployed on the phone. While phone-hosted LLMs oﬀer\nenhanced security and privacy, by operating independently\nof internet connectivity,current technical constraints around\nmodel size, battery consumption, cooling and maintaining\nstrong capabilities present trade-oﬀs versus cloud-processed\nLLM solutions.\n• Conversational style . In addition to the content of a\nconversation, the style in which an LLM interacts with the\nuser is relevant to the overall experience. For instance,\nChatGPT can be verbose and academic sounding,\nwhich could make comprehension diﬃcult for many\ndementia patients. Models such as ChatGPT are able\nto adapt conversational style via prompt engineering,\nso style adaptation is a design challenge rather than a\ntechnical challenge.\n• Anthropomorphisation. As LLMs capabilities increase, users\nare more likely to ascribe personality and agency to them.\nThis can facilitate building an emotional bond with the App,\noﬀering potential beneﬁts such as increased engagement, but\nalso risks such as overreliance on recommendations. Evidence\nfor this was given by\nMa et al. (2024) who reported in an\nanalysis of social media data that some users of an LLM-\npowered mental health App experienced feelings of stress, loss\nand grievance after updates to the LLM lead to inconsistent\nconversational style and the loss of memory of previous\nconversations. While these results were obtained with a chat\napplication, LLMs personiﬁed as virtual avatars with their\nown voice and looks might increase anthropomorphisation\neven more.\nConcluding, the diversity and individual variability of\nchallenges faced by dementia patients makes it unlikely that\na single technical solution can cater to the entire user base. A\nFrontiers in Dementia /one.tnum/seven.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nFIGURE /five.tnum\nOpinion scores on Likert scale (y-axis) for each of the scenarios ( x-axis). Scores have been averaged for individuals within the P wD and supporters\ngroups. Markers depict mean, shaded area represents /one.tnum standard error of the mean.\nsolution that claims wide applicability needs to be personalizable\nand adaptive. Personalization can involve visual elements (e.g.,\nsize, color, or style and choice of a virtual avatar), auditory\naspects (speed and information content of auditory feedback and\nvoice choices for voice assistants), as well as cognitive load (e.g.,\ncomplexity of the usage, number of elements on a dashboard,\nease of navigation) and conversational style. It is evident that the\ndevelopment of solutions should be accompanied by involvement\nand engagement of PwD and their caregivers/supporters. Their\nfeedback should be sought from the initial design stage throughout\nthe entire product development cycle is essential for creating\neﬀective and user-centric solutions.\nQuestionnaire\nWe believe that an eﬀective and ethical path toward the usage\nof LLMs in dementia management and care involves centering the\nperspectives and needs of people with dementia, caregivers and\nother stakeholders at all stages in the research and development\ncycle. For this reason, we created a questionnaire in which we\nasked participants to rate the usefulness of LLMs in a number of\napplication scenarios (e.g., companionship, therapy aid), and we\nasked them to rate the importance of design features (e.g., ease\nof use, voice control, privacy). We presented the questionnaire to\nPwD, their supporters, caregivers and stakeholders. To the best of\nour knowledge, this is the ﬁrst targeted survey on the usage of\nLLMs for dementia care and management. Ethical approval for the\nstudy has been obtained from The School of Computer Science\nand Informatics Research Ethics Committee at Cardiﬀ University,\nUnited Kingdom, reference: COMSC/Ethics/2023/122.\nParticipants\nFifteen people with dementia (PwD) aged 58-88 ( µ = 72.2), 7\nwomen, 7 men, 1 of nonbinary gender, participated in the study.\nAdditionally, 14 supporters aged 32-70 ( µ = 53.6), 11 women and\n2 men (1 declined to indicate their sex), participated in the study.\nSupporters could be family members or professional caregivers\nor nurses. Participants were recruited with the help of Dementia\nAustralia (\nhttps://www.dementia.org.au/) and Alzheimer’s Society\nUK ( https://www.alzheimers.org.uk/). The organizations served\nas gatekeepers, that is, they published our invitation email and\na participant information sheet on their website. The invitation\nemail included a hyperlink that would take participants directly\nto the survey. There was no compensation for participation but\nparticipants could opt-in to a raﬄe for a single £100 Visa Gift\ncard. To this end, they would enter their email address in the\nnotes section of the questionnaire. After the raﬄe, the email\naddresses were removed from the dataset. The study was fully\nanonymous otherwise.\nQuestionnaire details\nA copy of the questionnaire is provided as\nSupplementary material. Here, we summarize its main items.\nFor the items, participants could choose to select “Prefer not to\nanswer” if they wish not to answer a question. For multiple choice\nquestions, an additional option “Other” was provided in case\nparticipants wanted to specify an option that was not listed.\nTable 3\nlists the questions used, categorizes them by section and speciﬁes\nthe type of answer required. Note that all items categorized as\nfollow-up were only asked when the immediately preceding\nquestions was answered with “yes”. Following questions about\ntheir demographic background and dementia, the main body\nconsisted of questions regarding application scenarios of LLMs\nas well as desired features for digital apps. Finally, participants\nwere asked to estimate the overall impact AI can have on dementia\nmanagement and care, and there was space for free text with any\nnotes or additions participants would like to make.\nFrontiers in Dementia /one.tnum/eight.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nFIGURE /six.tnum\nQuestionnaire results for seven diﬀerent application scenarios (see Table /three.tnum, IDs /four.tnum./one.tnum through /four.tnum./seven.tnum). Pie charts show how useful PwD and supporters\nconsider LLMs in each application scenario. Each scenario is la beled in the ﬁgure. The corresponding pie chart is shown for PwD abov e the label and\nfor supporters below. The numbers in the pie slices correspond to a bsolute and relative number of respondents (e.g., six responden ts, /six.tnum//one.tnum/five.tnum= /four.tnum/zero.tnum%).\nThe legend deﬁnes the meaning of the colors.\nProcedure\nThe survey was implemented in Google Forms. It commenced\nby asking participants to provide informed consent in line with\nCardiﬀ University’s guidelines. Participants were then asked to\nwatch a 1-min overview over ChatGTP on YouTube (\nhttps://www.\nyoutube.com/watch?v=aIO9it4HFiQ) to make sure that they are\nfamiliar with the basic principles of LLMs. Further videos and a\nblog post were presented as optional additional material. They then\nanswered the questions listed in\nTable 3 by either clicking on the\nmultiple choice options or typing an answer. The survey took about\n20 min.\nResults\nThe raw data and results of the questionnaire are available\nin our GitHub repository ( https://github.com/treder/LLMs-for-\ndementia). We review the results according to the sections\nin Table 3: Demographics, dementia, AI experience, application\nscenarios, and features and priorities.\nDemographics and dementia\nFigure 4 depicts the demographic details of the participants.\nPeople with dementia (PwD) participating in our study were aged\n58–88 years whereas supporters were aged 32–70 years. As these\nranges suggest, supporters were signiﬁcantly younger than PwD\n(independent samples t-test, t = 5.059, p < 0.0001). Whereas\ngender roles were equally distributed for PwD (7 women, 7 men,\n1 of nonbinary gender), supporters were predominantly female (11\nwomen, 2 men). To compare the distribution of genders across the\ntwo groups, we used a two-sided Fisher’s exact test which works on\n2x2 contingency tables. The chi-squared test allows for larger tables\nbut requires a larger sample size (\nHazra and Gogtay, 2016 ; Sundjaja\net al., 2024 ). Therefore, we focused on comparing the number of\nFrontiers in Dementia /one.tnum/nine.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nFIGURE /seven.tnum\nOpinion scores on Likert scale (y-axis) for each of the features a nd priorities (x-axis). Scores have been averaged for individua ls within the PwD and\nsupporters groups. Markers depict mean, shaded area represent s /one.tnum standard error of the mean.\nmen and women. The diﬀerence was not statistically signiﬁcant\n(odds ratio = 5.5, p = 0.1032) which might be attributed to the\nsmall sample size.\nPeople with dementia identiﬁed as European (14) and Asian\n(1). Their highest degrees were vocational training/trade certiﬁcate\n(2 respondents), secondary education/high school (9), Bachelor’s\ndegree or equivalent (1), or Postgraduate degree (3). The supporters\nidentiﬁed as European (9), Mixed background (2), or Asian (1), and\n2 preferred not to answer. Supporters had vocational training/trade\ncertiﬁcates (1 respondent), Bachelor’s degree or equivalent (7), or\npostgraduate degree (5), and 1 preferred not to answer.\nPeople with dementia received the diagnosis between 1 and\n13 years ( µ = 5.1) ago. They were diagnosed with various types\nof dementia, namely Alzheimer’s disease (9 respondents), Vascular\ndementia (1), Fronto-temporal dementia (1), Lewy body dementia\n(1), or Mixed Dementia (2), and 1 preferred not to answer. When\nasked to freely describe their symptoms, memory problems were\nmentioned most often, with 5 respondents mentioned problems\nwith “short term memory”, and another one “total blank in the\nmornings”. Additional symptoms were related to social interaction\n(“withdrawn from people”, “unable to speak properly, diﬃculty\nunderstanding conversations”), physical symptoms (“tremors, gait\nand balance”, “diﬃcult to balance on one side”, “shakes, unstable”),\nas well as “hallucinations, visual and auditory” and a general\n“inability to perform everyday tasks” and “inability to understand\ncontrols on oven or television”.\nAI experience\nResponses related to the use of apps in the context of dementia,\n3/15 PwD and 5/14 supporters (1 preferred not to answer)\nresponded with “Yes”. Six PwD and 12 supporters heard of LLMs\nsuch as Chat-GPT before. Two PwD (1 preferred not to answer)\nand 5 supporters (1 preferred not to answer) stated having used\nthem before. One participant with dementia stated that they “use\nChatGPT to gather information, links and quotes”. Supporters used\nthem for “Patient and Public Involvement Networks, Universities,\nand as a carer for my Husband who had Dementia”, to “discover\ninformation on various topics encompassing dementia, including\nthe types, symptoms and possible outcomes of therapies used in\nbehavior management in dementia”, as well as to “synthesize text\nand videos” and for “writing reports”.\nApplication scenarios\nFigure 5 shows mean opinion scores obtained by encoding the\nresponse options as integers ranging from 1 to 5 and averaging\nthem across individuals for the PwD and supporter groups\nseparately. On average, all scenarios were ranked with moderate\nscores in between “Moderately useful” and “Useful” by both\ngroups. Both PwD and supporters ranked “Navigation”, “Reading\naid”, and “Writing aid” the highest. Somewhat lower scores were\nassigned to “Companionship” and the two items on “Dementia-\nrelated information”. As visual inspection suggests, responses\nbetween PwD and supporters were signiﬁcantly correlated (Pearson\ncorrelation, r = 0.79, p = 0.033).\nA more detailed overview with the proportion of each response\noption by group is depicted in\nFigure 6. We observe a dichotomy\nwithin the PwD group: for each scenario, at least one participant\nselected the response “Not useful at all” whereas several participants\nselected “Very useful”. To investigate whether individual response\npatterns are correlated with demographic variables, we performed\na series of Pearson correlation analyses. We found that the\noverall mean score across all scenarios is negatively correlated\nwith age for PwD (r = − 0.62, p = 0.014) but not with the\nnumber of years since the dementia diagnosis ( p = 0.42). In other\nFrontiers in Dementia /two.tnum/zero.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nFIGURE /eight.tnum\nQuestionnaire results for features and priorities (see Table /three.tnum, IDs /five.tnum./one.tnum–/five.tnum./seven.tnum). Pie charts show how important PwD and supporters consider speciﬁc\naspects of LLM-based apps for dementia. Each feature is label ed in the ﬁgure. The corresponding pie chart is shown for PwD above th e label and for\nsupporters below. The numbers in the pie slices correspond to abs olute and relative number of respondents (e.g., /one.tnum/one.tnum respondents,/one.tnum/one.tnum//one.tnum/five.tnum= /seven.tnum/three.tnum./three.tnum%).\nThe legend deﬁnes the meaning of the colors. For data privacy and d ata transparency, an additional option was provided. The corresp onding slice is\ndepicted in white and the response option is pasted next to the ﬁgu re.\nwords, older participants tended to give lower overall scores. For\nsupporters, there was no evidence for such a relationship (r =\n− 0.36, p = 0.2). When performing the same analysis on the score\nfor each scenario separately, we found a signiﬁcant relationship\nfor “Companionship” (r = − 0.64, p = 0.001), “Dementia-\nrelated information” (r = − 0.54, p = 0.044), “Dementia-related\ninformation including personal data” (r = − 0.61, p = 0.027),\n“Reading aid” (r= − 0.61, p = 0.019), “Writing aid” (r = − 0.62, p =\n0.002), although only “Companionship” and “Writing aid” would\nsurvive a correction for multiple comparisons. Correlations were\nnot signiﬁcant for “Navigation” ( p = 0.24). No such relationships\nwere found for supporters (all p > 0.16). For the PwD group, we\nrepeated the correlation analysis using the number of years since\nthe dementia diagnosis instead of age, but found no signiﬁcant\neﬀects (all p > 0.31). For sex, we did not ﬁnd a relationship with\nmean score for either group (all p > 0.53).\nFeatures and priorities\nFigure 7 shows mean opinion scores obtained by encoding the\nresponse options as integers ranging from 1 to 5 and averaging\nthem across individuals for the PwD and supporter groups\nseparately. On average, all scenarios were ranked with moderate\nto high scores in between “Moderately important” and “Very\nimportant” by both groups. Both PwD and supporters ranked\nall priorities around data (“Data privacy”, “Data transparency”,\n“Data deletion”) the highest, showing concern for their agency over\nFrontiers in Dementia /two.tnum/one.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nFIGURE /nine.tnum\nQuestionnaire results for device and impact (see Table /three.tnum, IDs /five.tnum./eight.tnum and /six.tnum./one.tnum). Diﬀerent legends are provided for each question. For the preferred device,\nparticipants could select smartphone, Laptop/PC, tablet, or com binations of these.\ndata. Mean scores between the two groups were highly correlated\n(Pearson correlation, r = 0.94, p = 0.001), showing a similar\npattern of concerns and priorities.\nA more detailed overview with the proportion of each response\noption by group is depicted in\nFigure 8. Respondents in the PwD\ngroup gave either high or low scores to the items “Ease of use”,\n“Voice control”, “Empathy”, and “Human in the loop”, whereas\nsupporters overwhelmingly gave high scores to these items. Overall\nmean score across all features and priorities was signiﬁcantly\ncorrelated with age for PwD (r = − 0.54, p = 0.04) but not\nsupporters (p = 0.5). For PwD, when performing the same analyses\non the score for each feature and priority separately, we found no\nsigniﬁcant relationships (all p > 0.05). There was no signiﬁcant\ncorrelation with sex (PwD: p = 0.22, supporters: p = 0.17) and\nfor PwD there was no correlation with the number of years since\ndiagnosis (p = 0.33).\nFigure 9 depicts results on which devices respondents use\nand how they rate the overall impact of LLMs on dementia\nmanagement and care. In both groups a variety of devices was\nused, although amongst PwD tablets were more dominant whereas\namong supporters smartphones were more dominant. The overall\nimpact of LLMs on dementia care and management was seen more\npositively by PwD than supporters. Whereas only 1 respondent in\nthe PwD group indicated “negative”, 3 supporters indicated the\nimpact as “very negative”. Nevertheless, larger proportions in both\ngroups rated the impact as “positive” or “very positive” (PwD: 9\nrespondents, supporters: 8).\nFree comments\nRespondents could also provide feedback in a free textual form\n(items 4.8 and 6.2 in\nTable 3). Both PwD and supporters provided\nfeedback on positive and negative use cases for language-based AI\napplications in dementia. While some respondents were excited\nabout the potential beneﬁts of AI, others raised a number of\nconcerns and caveats. The main points are summarized in\nTable 4.\nDiscussion\nLarge Language Models (LLMs) revolutionize the way in which\nhumans interact with machines. For the ﬁrst time in history, we can\nconverse with computers in the same way that we talk to each other.\nMeaningful conversations, creative writing, poetry, summarization,\nall deeply human faculties that can now be experienced in a chat\nwith an algorithm. Our review has highlighted the burgeoning\nrole of LLMs in improving dementia care and research. The\nintegration of LLMs into therapeutic and support frameworks\nholds the potential to enhance the quality of life for individuals\nliving with dementia, as well as to alleviate the considerable burden\non caregivers. Through personalized conversations, information\nretrieval, therapy aid, and assistive technologies for reading,\nwriting, and navigation, LLMs oﬀer a novel approach to dementia\ncare that is both innovative and human-centric. Nevertheless,\nits adoption might face an uphill battle due to algorithmic and\nregulatory limitations and challenges, as well as concerns about\nadequacy and applicability in the context of dementia care that\nsurfaced in our survey.\nLimitations, risks, and challenges\nDespite the promising prospects, the deployment of LLMs in\ndementia care is not without challenges. The current limitations of\nLLMs, including their dependency on the quality of input data and\nthe potential for perpetuating biases, must be acknowledged and\naddressed. First, hallucinations, or the production of syntactically\nFrontiers in Dementia /two.tnum/two.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nTABLE /four.tnumSummary of the feedback of the respondents to the application scena rios.\nSummary Verbatim responses\nThe application scenarios are useful D “Wow!! I would love anything like those above”\nD “A product like this would be amazing for me it would take a lot of st ress out of my everyday life”\nS “I think AI has great implications for dementia awareness/care [ ...]”\nS “AI technology will be very important to alleviate isolation, and feel ings of loneliness for those living with\ndementia who do not have family or friends nearby to engage with”\nThere are other useful application scenarios\nbeyond those mentioned\nD “keeping ﬁt and retaining muscle mass”\nD “ask medical questions”\nS“Protection from scams would be useful”\nThe application scenarios do not address the\nactual problems of PwD\nD “None of them relate to alleviating the problems of daily living [... ]”\nD “What I really need is something that tells me step by step (all 176 of therm) how to live my daily life”\nD “I want raw, accurate information that I can easily verify, not a cozy c hat. The whole idea of that side of AI is\nanathema to me”\nD “As a former carer of someone with Alzheimer’s I can see very little use for t his app except for navigating IF the\nperson is out alone.”\nD “My cognitive faculties are relatively intact, it’s my recall that is impaired”\nS ”Tech is NOT the panacea that those who advocate it believe.”\nConcern about bias D “dementia care [...] is infected with assumptions of ageism [...] a nd a host of other biases that are likely to show\nup in AI development”\nConcern about level of tech aﬃnity required D “This AI is not relevant in any way to my mum who has dementia aged 88yr s and has never been able to use a\ncomputer even before her diagnosis”\nD “This AI sounds like amazing progress but the actual demographic of most dementia suﬀerers is that they are over\n70 so I am not too sure AI is going to help them a great deal as they will prob ably mostly not be computer literate!\n[...] I am 66 years old and ﬁnd AI rather a challenge”\nS “They would ﬁnd a PC, smartphone etc. very hard to navigate without help“\nS “it would not be suited to older people with no AI experience. People should st art using the planned App as soon\nas possible so they are familiar with it even before a diagnosis”\nUsefulness depends on stage of dementia D “this would be great for FTD and MCI”\nS “The ability to interact with the AI model depends on the degree of decline ”\nS “might work well very early on in the disease”\nS “for people with early onset dementia, it could be a valuable tool“\nS “it just feels unsuitable, totally depends on the stage of dementia, at pr esent it just feels like a gimmick”\nLLM may not understand the user S ”If relatives with a good knowledge of the person with other visual cues h ave diﬃculty in understanding, it is\npossible that AI will miss the point.”\nUser may not understand the LLM S “My clients if having a bad time being lost won’t be able follow ins tructions to ﬁnd their way home. Also talking\nabout dementia, treatment, diagnosis I can see that leading to confus ion, processing that much information, and it\nmight be conﬂicting information to the client.”\nLLM cannot replace human interaction and care S “I know my loved one would not have been happy talking to a machine. I t is not a replacement for a human [...]\nIt’s too untried to be let loose with those with a dementia diagnosi s”\nS “[...] using the AI app without help from a carer/district nurse or family member would be very diﬃcult for\nsomeone in the later stages of dementia”\nS “It is wrong in so many ways to use a machine to replicate a human re sponse [...] This is very much along the\nlines of “babysitting by television”\nThe ﬁrst column summarizes the respondents’ statements, the secon d column provides evidence in the shape of the actual individual feed back. The superscript D refers to respondents in the\ndementia group and S refers to supporters.\ncorrect but factually incorrect text, plague all state of the art LLMs\n(\nYe et al., 2023 ; Zhang et al., 2023 ) and are a source of concern for\ntheir medical application ( Pal et al., 2023 ; Tian et al., 2024 ). Second,\nLLMs are trained on a large corpus of text from a variety of sources\nincluding social media websites, often without permission of the\nauthor of the text (\nFranceschelli and Musolesi, 2022 ; Kasneci et al.,\n2023). Since stigma against people with dementia has been reported\non the media platform X ( Oscar et al., 2017 ; Bacsu et al., 2022 ),\ninclusion of uncurated internet data into LLM training harbors\nthe danger of perpetuating stereotypes about dementia. Third,\noverreliance might create adverse cognitive eﬀects. LLMs assisting\nwith perceptual tasks, memory, and language, creates short term\nbeneﬁts, but it is the same faculties that have to be engaged in order\nto combat decline (\nMondini et al., 2016 ; Hill et al., 2017 ). Fourth,\nthe development of LLMs for dementia has to take place with a\nregulatory framework that ensures that risks are mitigated, privacy\nis preserved, intellectual properties are warranted, and liability for\nmalpractice is established (\nMeskó and Topol, 2023 ).\nQuestionnaire\nUsing a questionnaire, we probed both people with dementia\n(PwD) and their supporters regarding their opinions on the\napplication and features of LLMs in the context of dementia.\nParticipants covered a representative age range for PwD spanning\n58–88 years (\nHugo and Ganguli, 2014 ). Whereas the gender split\nwas roughly equal for PwD, the majority of the supporters were\nwomen, in line with the predominance of female carers in mental\nillnesses more broadly (\nSharma et al., 2016 ). Only 3 of 15 Pwd and 5\nFrontiers in Dementia /two.tnum/three.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nout of 14 supporters reported ever having used apps in the context\nof dementia. We presented several application scenarios involving\ncompanionship, dementia information, navigation, reading or\nwriting aid, and therapy aid. Both PwD and supporters rated\nall of the scenarios as moderately useful to useful. Older PwD\ntended to give lower overall scores than younger PwD. It is up\nto speculation as to why, perhaps indicating a generally more\nnegative outlook, the existence of more severe symptoms that are\nunlikely to be alleviated by LLMs, or perhaps a larger barrier to use\ndigital apps.\nRegarding their priorities for what features LLM-powered apps\nshould have, PwD and supporters ranked agency over data (privacy,\ntransparency, deletion) and ease of use the highest. Opinions on\nthe usefulness of the technology diverged, however. In the free\ncomments sections (see\nTable 4), some respondents praised the\npromise of the technology (“I think AI has great implications for\ndementia awareness/care [...]”) but they also raised several caveats.\nThe application scenarios might not address the real needs of\nPwD (“None of them relate to alleviating the problems of daily\nliving [...]”). There were also concerns about bias and technological\naﬃnity required (“This AI sounds like amazing progress but the\nactual demographic of most dementia suﬀerers is that they are over\n70 so i am not too sure AI is going to help them a great deal as they\nwill probably mostly not be computer literate”). Other respondents\npointed out that the usefulness of LLMs depends on the stage of\ndementia (“The ability to interact with the AI model depends on\nthe degree of decline”) and that LLMs cannot serve as a substitute\nfor human interaction (“I know my loved one would not have been\nhappy talking to a machine. It is not a replacement for a human”).\nLimitations of the questionnaire\nOur online survey has several limitations. The use of\nconvenience sampling through dementia organizations as\ngatekeepers, while practical, may introduce selection bias. This\napproach relies on participants who are actively engaged with\nthese organizations and have access to the internet, potentially\nexcluding a portion of the dementia population who are less active\nor lack online access. Additionally, sample sizes of 15 people\nwith dementia and 14 supporters limit the statistical power of\nthe study especially with regard to more subtle eﬀects. It is also\npossible that in some cases both a PwD and their supporter ﬁlled\nin the questionnaire, leading to correlation between the samples.\nThe anonymity of the questionnaire, while protecting participant\nprivacy and potentially lowering the barrier to participation, also\nprevents any follow-up for more in-depth data collection.\nImplications for practice\nOur ﬁndings suggest that LLMs can serve as an invaluable\nresource in dementia care. By providing personalized interaction\nand support, LLMs have the potential to improve social engagement\nand cognitive functioning among individuals with dementia.\nHowever, the successful implementation of LLMs in dementia\ncare requires careful consideration of the technology’s limitations\nand the ethical implications of its deployment. Privacy and safety\nconcerns must be meticulously addressed, and systems need to\nbe designed with the end-user in mind, ensuring that they are\naccessible, intuitive, and genuinely beneﬁcial.\nWhen our previous considerations and the ﬁndings from the\nsurvey are taken together, we can add the following points for\nLLM-powered apps for dementia care and management:\n• On-demand aid . LLM-powered apps oﬀer on-demand and\nnon-judgmental support, potentially including mental health\nbeneﬁts such as promotion of self-conﬁdence and aiding self-\ndiscovery (\nMa et al., 2024 ).\n• Caregiver burden . Economics mandate a reduction in care\ncost (Nandi et al., 2022 ). While LLMs might alleviate caregiver\nburden, many respondents pointed out that apps cannot\nserve as substitutes to human interaction. Most likely, a\ncollaborative solution involving human support augmented\nby a chatbot for periods wherein the human supporter is not\navailable would be a way forward that meets targets both in\nterms of quality of care and associated monetary cost.\n• Prompt engineering and communication . It is yet unclear\nhow LLMs perform in the presence of language disorder\n(\nMurdoch et al., 1987 ) and other forms of cognitive\nimpairment ( Hugo and Ganguli, 2014 ). It is conceivable that\nbespoke models are required, e.g. by ﬁnetuning a model such\nas ChatGPT on a dataset including excerpts of speech from\nlanguage-impaired individuals. Since the communication is\nbi-directional, further ﬁnetuning might be required to align\nthe model to produce outputs that are more palatable for\nindividuals with impairments.\n• Complexity and technological aﬃnity . As long as operating\nLLMs is not seamless any real-world implementation faces a\ncatch-22 scenario: users that beneﬁt from an LLM the most\nmight ﬁnd it the most challenging to operate LLM-powered\napps. For this reason, some respondents pointed out that\nLLMs should be aimed toward milder versions of dementia\nsuch as early-onset dementia. Integration in physical agents\nsuch as robots could provide a more seamless gateway between\nLLM and the user.\n• Co-development of apps . As a note to tech developers, our\nsurvey showed the importance of co-developing solutions\nwith the end user (both PwD and supporters) in the loop\nearly. Otherwise one runs the risk of designing a solution\nthat does not address the needs of PwD or is not usable in\nthe light of their expertise and challenges in using such apps.\nFurthermore, the language used should not patronize PwD or\ndiminish their agency or cognitive capacities.\n• Data agency . PwD and supporters stressed the importance\nof retaining agency over their digital data, including\ntransparency about its usage and the ability to delete it.\nFuture research directions\nTo harness the full potential of LLMs in dementia care,\nfuture research should focus on several key areas. First, there\nis a need for longitudinal studies to assess the long-term\nimpact of LLM interactions on individuals with dementia. This\nincludes evaluating the eﬀects on cognitive health, emotional\nFrontiers in Dementia /two.tnum/four.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nwellbeing, and social engagement over time. It also involves\na better characterization of dementia-speciﬁc limitations and\nrisks associated with the usage of LLMs. Second, research\nshould explore the customization and personalization of LLMs\nto meet the diverse needs of individuals with dementia. This\nincludes the development of adaptive algorithms that can tailor\ninteractions based on the user’s preferences, behaviors, and\ncognitive status. Third, the exploration of multimodal LLMs that\ncan interpret and respond to non-verbal cues could signiﬁcantly\nenhance the quality of interactions, making the technology more\naccessible and eﬀective for individuals with varying degrees\nof cognitive impairment. Fourth, exploring the embodiment\nof LLMs in robotics could revolutionize dementia care by\nproviding conversational and physical support through social\nrobots (\nD’Onofrio et al., 2019 ; Fields et al., 2021 ; Lima et al.,\n2022). This research should aim to develop adaptive robots that\ncater to the emotional and physical needs of dementia patients,\nenhancing their quality of life with a blend of cognitive support\nand companionship.\nConclusion\nIn conclusion, the use of Large Language Models in dementia\ncare represents a promising frontier in the intersection of AI and\nhealthcare. While challenges and limitations exist, the potential\nbeneﬁts of LLMs in enhancing cognitive abilities, enriching social\ninteraction, and supporting caregivers are undeniable. As we move\nforward, it is crucial that the development and implementation\nof LLMs is guided by ethical considerations, empirical evidence,\nand a commitment to improving the lives of individuals living\nwith dementia.\nData availability statement\nThe raw data and a Jupyter notebook reproducing the results of\nthe questionnaire are available in our GitHub repository ( https://\ngithub.com/treder/LLMs-for-dementia).\nEthics statement\nThe studies involving humans were approved by the\nSchool of Computer Science and Informatics Research Ethics\nCommittee at Cardiﬀ University, United Kingdom (reference\ncode COMSC/Ethics/2023/122). The studies were conducted\nin accordance with the local legislation and institutional\nrequirements. The participants provided their written informed\nconsent to participate in this study.\nAuthor contributions\nMT: Conceptualization, Data curation, Investigation, Formal\nanalysis, Methodology, Visualization, Writing – review & editing,\nWriting – original draft. SL: Investigation, Writing – review &\nediting, Writing – original draft. KT: Writing – review & editing,\nWriting – original draft.\nFunding\nThe author(s) declare ﬁnancial support was received for\nthe research, authorship, and/or publication of this article. This\nstudy was funded by the “Longitude Prize on Dementia”, a\nresearch and development program by Alzheimer’s Society and\nInnovate UK. We would also like to thank Dementia Australia\n(\nwww.dementia.org.au) and Alzheimer’s Society ( https://www.\nalzheimers.org.uk/) for their support in recruiting participants for\nour questionnaire. KT was supported by Fellowship awards from\nthe Guarantors of Brain (G101149) and Alzheimer’s Society, UK\n(grant number 602).\nConﬂict of interest\nSL was employed by Olive AI Limited.\nThe remaining authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial relationships that\ncould be construed as a potential conﬂict of interest.\nThe author(s) declared that they were an editorial board\nmember of Frontiers, at the time of submission. This had no impact\non the peer review process and the ﬁnal decision.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin.org/articles/10.3389/frdem.2024.\n1385303/full#supplementary-material\nReferences\n2023 Alzheimer’s Disease Facts and Figures (2023). Alzheimer ’s disease\nfacts and ﬁgures 2023. Alzheimers Dement . 19, 1598–1695. doi: 10.1002/alz.\n13016\nAbrol, A., Fu, Z., Du, Y., and Calhoun, V. D. (2019). “Multimodal data fusion\nof deep learning and dynamic functional connectivity featur es to predict alzheimer’s\ndisease progression, ” in 2019 41st Annual International Conference of the IEEE\nEngineering in Medicine and Biology Society (EMBC). Presented at the 2 019 41st Annual\nInternational Conference of the IEEE Engineering in Medicine and Biolo gy Society\n(EMBC), (Berlin: IEEE), 4409–4413.\nAgbavor, F., and Liang, H. (2022). Predicting dementia from spontaneous\nspeech using large language models. PLOS Digit. Health 1:e0000168.\ndoi: 10.1371/journal.pdig.0000168\nFrontiers in Dementia /two.tnum/five.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nAlAfnan, M. A., Dishari, S., Jovic, M., and Lomidze, K. (2023) . ChatGPT as an\neducational tool: opportunities, challenges, and recommendati ons for communication,\nbusiness writing, and composition courses. J. Artif. Intell. Technol . 3, 60–68.\ndoi: 10.37965/jait.2023.0184\nAl-Hameed, S., Benaissa, M., Christensen, H., Mirheidari, B ., Blackburn, D.,\nand Reuber, M. (2019). A new diagnostic approach for the identi ﬁcation of\npatients with neurodegenerative cognitive complaints. PLOS ONE 14:e0217388.\ndoi: 10.1371/journal.pone.0217388\nAltmäe, S., Sola-Leyva, A., and Salumets, A. (2023). Artiﬁcial intelligence\nin scientiﬁc writing: a friend or a foe? Reprod. Biomed . Online 47, 3–9.\ndoi: 10.1016/j.rbmo.2023.04.009\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Pass os, A., et al. (2023).\nPaLM 2 technical report. arXiv [Preprint]. doi: 10.48550/arXiv.2305.10403\nAnthropic (2023a). Claude 2 [WWW Document]. Prod. Announc . Available online\nat: https://www.anthropic.com/index/claude-2 (accessed 19 December, 2023).\nAnthropic (2023b). Introducing Claude 2.1 [WWW Document]. Prod. Announc.\nAvailable online at: https://www.anthropic.com/index/claude-2-1 (accessed 19\nDecember, 2023).\nAnthropic (2024). Introducing the next generation of Claude [WWW Document].\nAvailable online at: https://www.anthropic.com/news/claude-3-family (accessed 6\nApril, 2024).\nArmstrong, M. J., Bedenﬁeld, N., Rosselli, M., Curiel Cid, R. E. , Kitaigorodsky, M.,\nGalvin, J. E., et al. (2024). Best practices for communicating a diagnosis of dementia.\nNeurol. Clin. Pract . 14:e200223. doi: 10.1212/CPJ.0000000000200223\nBacsu, J.-D., Fraser, S., Chasteen, A. L., Cammer, A., Grewal, K. S., Bechard, L. E.,\net al. (2022). Using Twitter to examine stigma against people wit h dementia during\nCOVID-19: infodemiology study. JMIR Aging 5:e35677. doi: 10.2196/35677\nBaecker, L., Garcia-Dias, R., Vieira, S., Scarpazza, C., and Mechelli, A. (2021).\nMachine learning for brain age prediction: introduction to me thods and clinical\napplications. eBioMedicine 72:103600. doi: 10.1016/j.ebiom.2021.103600\nBang, J.-U., Han, S.-H., and Kang, B.-O. (2024). Alzheimer’s disease recognition\nfrom spontaneous speech using large language models. ETRI J . 46, 96–105.\ndoi: 10.4218/etrij.2023-0356\nBanks, J., and Warkentin, T. (2024). Gemma: Introducing New State-of-the-Art\nOpen Models. Google. Available online at: https://blog.google/technology/developers/\ngemma-open-models/ (accessed 6 April, 2024).\nBarocas, S., Hardt, M., and Narayanan, A. (2023). Fairness and Machine Learning .\nCambridge, MA: MIT Press.\nBir, S. C., Khan, M. W., Javalkar, V., Toledo, E. G., and Kelley, R. E . (2021). Emerging\nconcepts in vascular dementia: a review. J. Stroke Cerebrovasc. Dis. Oﬀ. J. Natl. Stroke\nAssoc. 30:105864. doi: 10.1016/j.jstrokecerebrovasdis.2021. 105864\nBirhane, A., Prabhu, V. U., and Kahembwe, E. (2021). Multimod al\ndatasets: misogyny, pornography, and malignant stereotypes. arXiv [Preprint].\ndoi: 10.48550/arXiv.2110.01963\nBlodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H. (202 0). “Language\n(Technology) is Power: A Critical Survey of “Bias” in NLP , ” in TetreaultProceedings\nof the 58th Annual Meeting of the Association for Computational Ling uistics. Presented\nat the ACL 2020, eds. D. Jurafsky, J. Chai, and N. Schluter (Stroudsburg: Asso ciation\nfor Computational Linguistics), 5454–5476.\nBorchert, R. J., Azevedo, T., Badhwar, A., Bernal, J., Betts, M., Bruﬀaerts, R., et al.\n(2023). Artiﬁcial intelligence for diagnostic and prognostic neuroimaging in dementia:\na systematic review. Alzheimers Dement. 19, 5885–5904. doi: 10.1002/alz.13412\nBotchu, B., Karthikeyan, I. P., and Botchu, R. (2023). Can Ch atGPT\nempower people with dyslexia? Disabil. Rehabil. Assist. Technol . 0, 1–2.\ndoi: 10.1080/17483107.2023.2256805\nBouazizi, M., Zheng, C., Yang, S., and Ohtsuki, T. (2024). De mentia detection\nfrom speech: what if language models are not the answer? Information 15, 2.\ndoi: 10.3390/info15010002\nBrierley, C. (2021). AI Could Detect Dementia Years Before Symptoms Appear\n[WWW Document]. Univ. Camb. Available online at: https://www.cam.ac.uk/stories/\nAIdementia (accessed 19 November, 2023).\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvit z, E., Kamar, E., et al.\n(2023). Sparks of artiﬁcial general intelligence: early experim ents with GPT-4. arXiv\n[Preprint]. doi: 10.48550/arXiv.2303.12712\nBzdok, D., Thieme, A., Levkovskyy, O., Wren, P., Ray, T., and Reddy, S. (2024).\nData science opportunities of large language models for neurosci ence and biomedicine .\nNeuron. 112, 698–717. doi: 10.1016/j.neuron.2024.01.016\nCarós, M., Garolera, M., Radeva, P., and Giro-i-Nieto, X. (202 0). “Automatic\nreminiscence therapy for dementia, ” inProceedings of the 2020 International Conference\non Multimedia Retrieval, ICMR’20. (New York, NY: Association for Computing\nMachinery), 383–387.\nChen, J., Lin, H., Han, X., and Sun, L. (2024). Benchmarking la rge language models\nin retrieval-augmented generation. Proc. AAAI Conf. Artif. Intell . 38, 17754–17762.\ndoi: 10.1609/aaai.v38i16.29728\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P., de, O., e t al.\n(2021). Evaluating large language models trained on code. arXiv [Preprint ].\ndoi: 10.48550/arXiv.2107.03374\nCheng, S.-W., Chang, C.-W., Chang, W.-J., Wang, H.-W., Lian g, C.-\nS., Kishimoto, T., et al. (2023). The now and future of ChatGPT a nd GPT\nin psychiatry. Psychiatry Clin. Neurosci . 77, 592–596. doi: 10.1111/pcn.\n13588\nChopard, D., Treder, M. S., Corcoran, P., Ahmed, N., Johnson, C., Busse, M., et al.\n(2021). Text mining of adverse events in clinical trials: deep learning approach. JMIR\nMed. Inform. 9:e28632. doi: 10.2196/28632\nChoudhury, A., and Shamszare, H. (2023). Investigating the impact of user trust\non the adoption and use of ChatGPT: survey analysis. J. Med. Internet Res . 25:e47184.\ndoi: 10.2196/47184\nCoda-Forno, J., Witte, K., Jagadish, A. K., Binz, M., Akata, Z., and Schulz, E.\n(2023). Inducing anxiety in large language models increases ex ploration and bias. arXiv\n[Preprint]. doi: 10.48550/arXiv.2304.11111\nCoyle-Gilchrist, I. T. S., Dick, K. M., Patterson, K., Vázquez Ro dríquez,\nP., Wehmann, E., Wilcox, A., et al. (2016). Prevalence, charact eristics, and\nsurvival of frontotemporal lobar degeneration syndromes. Neurology 86, 1736–1743.\ndoi: 10.1212/WNL.0000000000002638\nCreamer, E. (2023). ‘Hallucinate’ Chosen as Cambridge Dictionary’s Word of the\nYear. London: The Guardian.\nDangerﬁeld, K., and Katherine, W. (2023). “1st signs of Alzhei mer’s may be detected\nin your eyes, ” in This AI scan may help ﬁnd it - National | Globalnews.ca . Glob.\nNews. Available online at: https://globalnews.ca/news/9840571/ai-alzheimers-diseas e-\ndetection/ (accessed 19 November, 2023).\nDavis, W. (2023). “Sarah Silverman is suing OpenAI and Meta for copyright\ninfringement, ” in The Verge . Available online at: https://www.theverge.com/2023/7/\n9/23788741/sarah-silverman-openai-meta-chatgpt-llama-copy right-infringement-\nchatbots-artiﬁcial-intelligence-ai (accessed 25 November, 2023).\nDawid, A. P. (1982). The well-calibrated Bayesian. J. Am. Stat. Assoc . 77, 605–610.\ndoi: 10.1080/01621459.1982.10477856\nde Arriba-Pérez, F., García-Méndez, S., González-Castaño, F . J., and Costa-\nMontenegro, E. (2023). Automatic detection of cognitive im pairment in elderly people\nusing an entertainment chatbot with Natural Language Proce ssing capabilities. J.\nAmbient Intell. Humaniz. Comput . 14, 16283–16298. doi: 10.1007/s12652-022-03849-2\nde la Fuente Garcia, S., Ritchie, C. W., and Luz, S. (2020). Art iﬁcial intelligence,\nspeech, and language processing approaches to monitoring Alzheim er’s disease: a\nsystematic review. J . Alzheimers Dis. JAD 78, 1547–1574. doi: 10.3233/JAD-200888\nDemiris, G., Thompson, H. J., Lazar, A., and Lin, S.-Y. (2017). “Evaluation of a\ndigital companion for older adults with mild cognitive impairmen t, ” inAMIA. Annu.\nSymp. Proc. 2016 , 496–503.\nDemszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., C handhok, S., et al.\n(2023). Using large language models in psychology. Nat. Rev. Psychol . 2, 688–701.\ndoi: 10.1038/s44159-023-00241-5\nDenecke, K., Schmid, N., and Nüssli, S. (2022). Implementation of cognitive\nbehavioral therapy in e–mental health apps: literature review. J. Med. Internet Res .\n24:e27791. doi: 10.2196/27791\nDeng, G., Liu, Y., Li, Y., Wang, K., Zhang, Y., Li, Z., et al. (202 4). “MASTERKEY:\nAutomated Jailbreaking of Large Language Model Chatbots, ” i n Proceedings 2024\nNetwork and Distributed System Security Symposium. Presented at the Ne twork\nand Distributed System Security Symposium, Internet Society . (San Diego, CA:\nInternet Society).\nDeshpande, A., Murahari, V., Rajpurohit, T., Kalyan, A., and Na rasimhan, K.\n(2023). Toxicity in ChatGPT: Analyzing Persona-assigned Language Mod els.\nDhamala, J., Sun, T., Kumar, V., Krishna, S., Pruksachatkun, Y., Chang, K.-\nW., et al. (2021). “BOLD: dataset and metrics for measuring bi ases in open-\nended language generation, ” in Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency, FAccT’21. (New York, NY: Association for\nComputing Machinery), 862–872.\nDhuliawala, S., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyi lmaz, A., et al. (2023).\nChain-of-veriﬁcation reduces hallucination in large language models. arXiv [Preprint].\ndoi: 10.48550/arXiv.2309.11495\nDing, Y., Sohn, J. H., Kawczynski, M. G., Trivedi, H., Harnish , R., Jenkins, N. W.,\net al. (2019). A deep learning model to predict a diagnosis of Alzh eimer disease by using\n18F-FDG PET of the brain. Radiology 290, 456–464. doi: 10.1148/radiol.2018180958\nDixon, E., Michaels, R., Xiao, X., Zhong, Y., Clary, P., Naraya nan, A., et al. (2022).\n“Mobile phone use by people with mild to moderate dementia: uncover ing challenges\nand identifying opportunities: mobile phone use by people with mild t o moderate\ndementia, ” in Proceedings of the 24th International ACM SIGACCESS Conference on\nComputers and Accessibility, ASSETS’22 . (New York, NY: Association for Computing\nMachinery), 1–16. doi: 10.1145/3517428.3544809\nDobbe, R., Krendl Gilbert, T., and Mintz, Y. (2021). Hard choic es in artiﬁcial\nintelligence. Artif. Intell. 300, 103555. doi: 10.1016/j.artint.2021.103555\nFrontiers in Dementia /two.tnum/six.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nD’Onofrio, G., Sancarlo, D., Raciti, M., Burke, M., Teare, A., Kovacic, T., et al.\n(2019). MARIO project: validation and evidence of service rob ots for older people with\ndementia. J. Alzheimers Dis . 68, 1587–1601. doi: 10.3233/JAD-181165\nDosso, J. A., Kailley, J. N., and Robillard, J. M. (2023). What doe s ChatGPT know\nabout dementia? A comparative analysis of information quality. J. Alzheimers Dis. 97,\n559–565. doi: 10.3233/JAD-230573\nDu, Y., Fu, Z., and Calhoun, V. D. (2018). Classiﬁcation and\nprediction of brain disorders using functional connectivit y: promising\nbut challenging. Front. Neurosci . 12:525. doi: 10.3389/fnins.2018.\n00525\nEliot, L. (2023). People Are Eagerly Consulting Generative AI ChatGPT For Mental\nHealth Advice, Stressing Out AI Ethics And AI Law. Forbes . Available online at: https://\nwww.forbes.com/sites/deloitte/2024/02/16/making-the- leap-from-smart-to-the-\nmetaverse-in-operations/?sh=42cf52f62b0b (accessed February 22, 2024).\nFerrara, E. (2023). “Should ChatGPT be biased? Challenges and ri sks of bias in large\nlanguage models, ” inFirst Monday.\nField, A., Coston, A., Gandhi, N., Chouldechova, A., Putnam-H ornstein, E., Steier,\nD., et al. (2023). “Examining risks of racial biases in NLP too ls for child protective\nservices, ” in 2023 ACM Conference on Fairness, Accountability, and Transparency.\nPresented at the FAccT’23: the 2023 ACM Conference on Fairness, Accountabilit y, and\nTransparency (Chicago: ACM), 1479–1492.\nFields, N., Xu, L., Greer, J., and Murphy, E. (2021). Shall I compar e thee. . . to\na robot? An exploratory pilot study using participatory arts and so cial robotics\nto improve psychological well-being in later life. Aging Ment. Health 25, 575–584.\ndoi: 10.1080/13607863.2019.1699016\nFischer, C. E., Qian, W., Schweizer, T. A., Ismail, Z., Smith, E. E., Millikin, C. P.,\net al. (2017). Determining the impact of psychosis on rates of fa lse-positive and false-\nnegative diagnosis in Alzheimer’s disease. Alzheimers Dement. Transl. Res. Clin. Interv .\n3, 385–392. doi: 10.1016/j.trci.2017.06.001\nFloridi, L., and Floridi, L. (2023). The Ethics of Artiﬁcial Intelligence: Principles,\nChallenges, and Opportunities . Oxford, New York: Oxford University Press.\nFranceschelli, G., and Musolesi, M. (2022). Copyright in generat ive deep learning.\nData Policy 4:e17. doi: 10.1017/dap.2022.10\nFügener, A., Grahl, J., Gupta, A., and Ketter, W. (2021). Will Humans-in-The-\nLoop Become Borgs? Merits and Pitfalls of Working with AI. Management Information\nSystems Quarterly (MISQ). Vol. 45. Available online at: https://papers.ssrn.com/sol3/\npapers.cfm?abstract_id=3879937\nGabriel, I. (2020). Artiﬁcial intelligence, values, and alignme nt. Minds Mach . 30,\n411–437. doi: 10.1007/s11023-020-09539-2\nGallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S. , Dernoncourt, F.,\net al. (2023). Bias and fairness in large language models: a surve y. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2309.00770\nGanguli, D., Askell, A., Schiefer, N., Liao, T. I., Lukošiute, K ., Chen, A., et al. (2023).\nThe capacity for moral self-correction in large language models. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2302.07459\nGanguli, D., Hernandez, D., Lovitt, L., Askell, A., Bai, Y., Che n, A., et al. (2022).\n“Predictability and Surprise in Large Generative Models, ” in Proceedings of the 2022\nACM Conference on Fairness, Accountability, and Transparency, FAccT’22 (New York,\nNY: Association for Computing Machinery), 1747–1764.\nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., et al. (202 4). Retrieval-Augmented\nGeneration for Large Language Models: A Survey .\nGarcía-Requejo, A., Pérez-Rubio, M. C., Villadangos, J. M., an d Hernández,\nÁ. (2023). Activity monitoring and location sensory system f or people with mild\ncognitive impairments. IEEE Sens . J. 23, 5448–5458. doi: 10.1109/JSEN.2023.32\n39980\nGhahramani, Z. (2022). “Understanding the world through lang uage” in The\nKeyword. Available online at: https://blog.google/technology/ai/understanding-the-\nworld-through-language/ (accessed 19 December, 2023).\nGhahramani, Z. (2023). Introducing PaLM 2 [WWW Document]. Google . Available\nonline at: https://blog.google/technology/ai/google-palm-2-ai-large-lan guage-model/\n(accessed 11 July, 2023).\nGiorgio, J., Jagust, W. J., Baker, S., Landau, S. M., Tino, P., Kourtzi, Z., et al.\n(2020). Predicting future regional tau accumulation in asym ptomatic and early\nalzheimer’s disease.arXiv [Preprint]. doi: 10.1101/2020.08.15.252601\nGooding, M. (2023). “Open source LLMs could make artiﬁcial inte lligence\nmore dangerous, says “godfather” of AI, ” in Tech Monit . Available online at:\nhttps://techmonitor.ai/technology/ai-and-automation/o pen-source-chatgpt-ai-llm-\ngeoﬀrey-hinton (accessed 10 December, 2023).\nGoodman, S. M., Buehler, E., Clary, P., Coenen, A., Donsbach, A. , Horne, T. N., et al.\n(2022). “LaMPost: Design and Evaluation of an AI-assisted Em ail Writing Prototype\nfor Adults with Dyslexia, ” in Proceedings of the 24th International ACM SIGACCESS\nConference on Computers and Accessibility. Presented at the ASSETS’22: The 24th\nInternational ACM SIGACCESS Conference on Computers and Accessibility (Athens:\nACM), 1–18.\nGreshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., a nd Fritz, M. (2023).\nNot what you’ve signed up for: compromising real-world llm-integ rated applications\nwith indirect prompt injection. arXiv [Preprint]. doi: 10.48550/arXiv.2302.12173\nGriﬃn, L., Kleinberg, B., Mozes, M., Mai, K., Vau, M. D. M., Caldw ell, M., et al.\n(2023). “Large Language Models respond to Inﬂuence like Humans, ” in Proceedings of\nthe First Workshop on Social Inﬂuence in Conversations (SICon 2023). P resented at the\nSICon 2023, eds. K. Chawla, and W. Shi (Toronto, ON: Association for Compu tational\nLinguistics), 15–24.\nGroeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., T afjord, O.,\net al. (2024). OLMo: accelerating the science of language models . arXiv [Preprint] .\ndoi: 10.48550/arXiv.2402.00838\nGupta, A., and Waldron, A. (2023). “A responsible path to generativ e AI in\nhealthcare, ” in Google Cloud Blog . Available online at: https://cloud.google.com/\nblog/topics/healthcare-life-sciences/sharing-google-med- palm-2-medical-large-\nlanguage-model (accessed 20 December, 2023).\nGustafsson, C., Svanberg, C., and Müllersdorf, M. (2015). Usin g a\nrobotic cat in dementia care: a pilot study. J. Gerontol. Nurs . 41, 46–56.\ndoi: 10.3928/00989134-20150806-44\nHagendorﬀ, T. (2023). Machine Psychology: Investigating Emergent Capabilities and\nBehavior in Large Language Models Using Psychological Methods .\nHardy, C. J. D., Marshall, C. R., Golden, H. L., Clark, C. N., Mummer y, C. J.,\nGriﬃths, T. D., et al. (2016). Hearing and dementia. J. Neurol . 263, 2339–2354.\ndoi: 10.1007/s00415-016-8208-y\nHazra, A., and Gogtay, N. (2016). Biostatistics series module 4: comparing groups –\ncategorical variables. Indian J. Dermatol . 61, 385–392. doi: 10.4103/0019-5154.185700\nHill, N. T. M., Mowszowski, L., Naismith, S. L., Chadwick, V. L. , Valenzuela, M., and\nLampit, A. (2017). Computerized cognitive training in older ad ults with mild cognitive\nimpairment or dementia: a systematic review and meta-analysi s. Am. J. Psychiatry 174,\n329–340. doi: 10.1176/appi.ajp.2016.16030360\nHolwerda, T. J., Deeg, D. J. H., Beekman, A. T. F., Tilburg, T. G., van, Stek, M.\nL., Jonker, C., et al. (2014). Feelings of loneliness, but not soc ial isolation, predict\ndementia onset: results from the Amsterdam Study of the Elderly (AMSTEL). J. Neurol.\nNeurosurg. Psychiatry 85, 135–142. doi: 10.1136/jnnp-2012-302755\nHovy, D., and Prabhumoye, S. (2021). Five sources of bias in n atural language\nprocessing. Lang. Linguist. Compass 15:e12432. doi: 10.1111/lnc3.12432\nHristidis, V., Ruggiano, N., Brown, E. L., Ganta, S. R. R., an d Stewart, S. (2023).\nChatGPT vs google for queries related to dementia and other cogni tive decline:\ncomparison of results. J. Med. Internet Res . 25:e48966. doi: 10.2196/48966\nHsiao, S. (2023). Bard Updates from Google I/O 2023: Images, New Features\n[WWW Document]. Available online at: https://blog.google/technology/ai/google-\nbard-updates-io-2023/ (accessed 21 October, 2023).\nHu, K. (2023). “ChatGPT sets record for fastest-growing user base - analyst note, ”\nin Reuters (Toronto, ON). Available online at: https://www.reuters.com/technology/\nchatgpt-sets-record-fastest-growing-user-base-analyst -note-2023-02-01/\nHuang, J., and Chang, K. C.-C. (2023). Towards reasoning in la rge language models:\na survey. arXiv [Preprint]. doi: 10.48550/arXiv.2212.10403\nHuang, Y., Gupta, S., Xia, M., Li, K., and Chen, D. (2023). Catas trophic\njailbreak of open-source LLMs via exploiting generation. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2310.06987\nHugo, J., and Ganguli, M. (2014). Dementia and cognitive impai rment:\nepidemiology, diagnosis, and treatment. Clin. Geriatr. Med . 30, 421–442.\ndoi: 10.1016/j.cger.2014.04.001\nIadecola, C., Duering, M., Hachinski, V., Joutel, A., Pendlebu ry, S. T., Schneider, J.\nA., et al. (2019). Vascular cognitive impairment and dementia. J. Am. Coll. Cardiol, . 73,\n3326–3344. doi: 10.1016/j.jacc.2019.04.034\nIBM Data and AI Team (2023). Open Source Large Language Models: Beneﬁts,\nRisks and Types [WWW Document]. Available online at: https://www.ibm.com/blog/\nopen-source-large-language-models-beneﬁts-risks-and-type s/www.ibm.com/blog/\nopen-source-large-language-models-beneﬁts-risks-and-type s (accessed October 12,\n2023).\nJack, C. R., Bennett, D. A., Blennow, K., Carrillo, M. C., Dunn, B ., Haeberlein,\nS. B., et al. (2018). NIA-AA research framework: toward a biolo gical deﬁnition\nof Alzheimer’s disease. Alzheimers Dement. J. Alzheimers Assoc . 14, 535–562.\ndoi: 10.1016/j.jalz.2018.02.018\nJavaheripi, M., and Bubeck, S. (2023). “Phi-2: the surprising power of small\nlanguage models, ” in Microsoft Res . Available online at: https://www.microsoft.\ncom/en-us/research/blog/phi-2-the-surprising-power-of-s mall-language-models/\n(accessed December 20, 2023).\nJavaid, M., Haleem, A., and Singh, R. P. (2023). ChatGPT for hea lthcare services: an\nemerging stage for an innovative perspective. BenchCouncil T rans. Benchmarks Stand.\nEval. 3, 100105. doi: 10.1016/j.tbench.2023.100105\nJiang, F., Xu, Z., Niu, L., Xiang, Z., Ramasubramanian, B., L i, B., et al. (2024).\nArtPrompt: ASCII art-based jailbreak attacks against aligned LLMs. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2402.11753\nFrontiers in Dementia /two.tnum/seven.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nJiang, S., Kadhe, S., Zhou, Y., Cai, L., and Baracaldo, N. (202 3). Forcing generative\nmodels to degenerate ones: the power of data poisoning attacks. presented at the\nneurips 2023 workshop on backdoors in deep learning - the good, the bad, and the ugly.\narXiv [Preprint]. doi: 10.48550/arXiv.2312.04748\nJiang, Z., Araki, J., Ding, H., and Neubig, G. (2021). How can w e know when\nlanguage models know? on the calibration of language models for que stion answering.\narXiv [Preprint]. doi: 10.48550/arXiv.2012.00955\nJiao, B., Li, R., Zhou, H., Qing, K., Liu, H., Pan, H., et al. (20 23). Neural biomarker\ndiagnosis and prediction to mild cognitive impairment and Alzhe imer’s disease using\nEEG technology. Alzheimers Res. Ther . 15:32. doi: 10.1186/s13195-023-01181-1\nKalai, A. T., and Vempala, S. S. (2023). Calibrated Language Models\nMust Hallucinate.\nKane, A. E., Shin, S., Wong, A. A., Fertan, E., Faustova, N. S. , Howlett, S. E., et al.\n(2018). Sex diﬀerences in healthspan predict lifespan in the 3xTg -AD mouse model of\nAlzheimer’s disease.Front. Aging Neurosci . 10:172. doi: 10.3389/fnagi.2018.00172\nKanwal, N., and Rizzo, G. (2022). “Attention-based clinical no te summarization, ” in\nProceedings of the 37th ACM/SIGAPP Symposium on Applied Computing, SAC ’22(New\nYork: Association for Computing Machinery), 813–820. doi: 1 0.1145/3477314.3507256\nKapasi, A., DeCarli, C., and Schneider, J. A. (2017). Impact of m ultiple pathologies\non the threshold for clinically overt dementia. Acta Neuropathol . 134, 171–186.\ndoi: 10.1007/s00401-017-1717-7\nKaramolegkou, A., Li, J., Zhou, L., and Søgaard, A. (2023). “C opyright violations\nand large language models, ” in Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing (Singapore: Association for Computational\nLinguistics), 7403–7412. doi: 10.18653/v1/2023.emnlp-mai n.458\nKasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dement ieva, D.,\nFischer, F., et al. (2023). ChatGPT for good? On opportunities an d challenges\nof large language models for education. Learn. Individ. Diﬀer . 103:102274.\ndoi: 10.1016/j.lindif.2023.102274\nKhan, A., Bleth, A., Bakpayev, M., and Imtiaz, N. (2022). Remin iscence therapy in\nthe treatment of depression in the elderly: current perspectives . J. Ageing Longev . 2,\n34–48. doi: 10.3390/jal2010004\nKhurana, D., Koli, A., Khatter, K., and Singh, S. (2023). Natu ral language processing:\nstate of the art, current trends and challenges. Multimed. Tools Appl . 82, 3713–3744.\ndoi: 10.1007/s11042-022-13428-4\nKoebel, K., Lacayo, M., Murali, M., and Tarnanas, I., Çöltekin, A. (2022).\n“Expert insights for designing conversational user interfa ces as virtual assistants and\ncompanions for older adults with cognitive impairments, ” in Chatbot Research and\nDesign, Lecture Notes in Computer Science , eds. A. Følstad, T. Araujo, S. Papadopoulos,\nE. Law, E. Luger, M. Goodwin, P. B. Brandtzaeg (Cham: Springer I nternational\nPublishing), 23–38.\nKoga, S., Martin, N. B., and Dickson, D. W. (2024). Evaluating the performance\nof large language models: ChatGPT and Google Bard in generating diﬀ erential\ndiagnoses in clinicopathological conferences of neurodegene rative disorders. Brain\nPathol. 34:e13207. doi: 10.1111/bpa.13207\nKowe, A., Köhler, S., Görß, D., and Teipel, S. (2023). The patients ’ and\ncaregivers’ perspective: In-hospital navigation aids for people with dementia- a\nqualitative study with a value sensitive design approach. Assist. Technol. 35, 248–257.\ndoi: 10.1080/10400435.2021.2020378\nKrein, L., Jeon, Y.-H., Amberber, A. M., and Fethney, J. (201 9). The\nassessment of language and communication in dementia: a synt hesis of\nevidence. Am. J. Geriatr. Psychiatry 27, 363–377. doi: 10.1016/j.jagp.2018.\n11.009\nKuzma, E., Littlejohns, T. J., Khawaja, A. P., Llewellyn, D. J., U koumunne, O. C.,\nand Thiem, U. (2021). Visual impairment, eye diseases, and de mentia risk: a systematic\nreview and meta-analysis. J. Alzheimers Dis . 83, 1073–1087. doi: 10.3233/JAD-210250\nLane, C. A., Hardy, J., and Schott, J. M. (2018). Alzheimer’s d isease. Eur. J. Neurol .\n25, 59–70. doi: 10.1111/ene.13439\nLauscher, A., Lueken, T., and Glavaš, G. (2021). “Sustainable mo dular debiasing of\nlanguage models, ” inFindings of the Association for Computational Linguistics: EMNL P\n2021 Presented at the Findings 2021 , eds. M. F. Moens, X. Huang, L. Specia, S. W. Yih\n(Punta Cana: Association for Computational Linguistics), 4 782–4797.\nLee, E. E., Torous, J., De Choudhury, M., Depp, C. A., Graham, S. A ., Kim, H.-C.,\net al. (2021). Artiﬁcial intelligence for mental health care: cli nical applications, barriers,\nfacilitators, and artiﬁcial wisdom. Biol. Psychiatry Cogn. Neurosci. Neuroimaging 6,\n856–864. doi: 10.1016/j.bpsc.2021.02.001\nLehman, E., Hernandez, E., Mahajan, D., Wulﬀ, J., Smith, M. J. , Ziegler, Z., et al.\n(2023). “Do we still need clinical language models?, in: proceedi ngs of the conference\non health, inference, and learning, ” in Presented at the Conference on Health, Inference,\nand Learning (New York: PMLR), 578–597.\nLi, R., Wang, X., Lawler, K., Garg, S., Bai, Q., and Alty, J. (202 2). Applications\nof artiﬁcial intelligence to aid early detection of dementia: a scoping review\non current capabilities and future directions. J. Biomed. Inform . 127:104030.\ndoi: 10.1016/j.jbi.2022.104030\nLi, R., Wang, X., and Yu, H. (2023). “Two directions for clinic al data generation with\nlarge language models: data-to-label and label-to-data, ” in Proc. Conf. Empir. Methods\nNat. Lang. Process. Conf. Empir. Methods Nat. Lang. Process , 7129–7143.\nLi, Y., and Zhang, Y. (2023). Fairness of ChatGPT. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2305.18569\nLiao, S. M. (2020). Ethics of Artiﬁcial Intelligence . Oxford: Oxford University Press.\nLieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos , I., et al.\n(2024). Jamba: a hybrid transformer-mamba language model. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2403.19887\nLima, M. R., Wairagkar, M., Gupta, M., Rodriguez y Baena, F., Ba rnaghi, P., Sharp,\nD. J., et al. (2022). Conversational aﬀective social robots f or ageing and dementia\nsupport. IEEE Trans. Cogn. Dev. Syst . 14, 1378–1397. doi: 10.1109/TCDS.2021.3115228\nLin, S., Hilton, J., and Evans, O. (2022). TruthfulQA: measuri ng how models mimic\nhuman falsehoods. arXiv [Preprint]. doi: 10.48550/arXiv.2109.07958\nLingard, L. (2023). Writing with ChatGPT: an illustration of it s capacity,\nlimitations and implications for academic writers. Perspect. Med. Educ . 12, 261–270.\ndoi: 10.5334/pme.1072\nLiu, N. F., Zhang, T., and Liang, P. (2023). Evaluating veriﬁa bility in generative\nsearch engines. arXiv [Preprint]. doi: 10.48550/arXiv.2304.09848\nLiu, Y., Deng, G., Li, Y., Wang, K., Zhang, T., Liu, Y., et al. (20 23). Prompt Injection\nAttack Against LLM-Integrated Applications.\nLivingston, G., Huntley, J., Sommerlad, A., Ames, D., Ballard, C. , Banerjee, S.,\net al. (2020). Dementia prevention, intervention, and care: 2 020 report of the Lancet\nCommission. The Lancet 396, 413–446. doi: 10.1016/S0140-6736(20)30367-6\nLoconte, R., Orrù, G., Tribastone, M., Pietrini, P., and Sart ori, G. (2023). Challenging\nChatGPT “Intelligence” with Human Tools: A Neuropsychological Inves tigation on\nPrefrontal Functioning of a Large Language Model.\nLombardi, A., Diacono, D., Amoroso, N., Biecek, P., Monaco, A., Bellantuono,\nL., et al. (2022). A robust framework to investigate the reliab ility and stability\nof explainable artiﬁcial intelligence markers of Mild Cognitive I mpairment and\nAlzheimer’s Disease.Brain Inform. 9, 17. doi: 10.1186/s40708-022-00165-5\nLund, B. D., Wang, T., Mannuru, N. R., Nie, B., Shimray, S., an d Wang, Z. (2023).\nChatGPT and a new academic reality: artiﬁcial intelligence-wri tten research papers and\nthe ethics of the large language models in scholarly publishing. J. Assoc. Inf. Sci. Technol .\n74, 570–581. doi: 10.1002/asi.24750\nMa, Z., Mei, Y., and Su, Z. (2024). Understanding the beneﬁts and challenges of\nusing large language model-based conversational agents for me ntal well-being support.\nAMIA Annu. Symp. Proc . 2023, 1105–1114.\nMao, C., Xu, J., Rasmussen, L., Li, Y., Adekkanattu, P., Pach eco, J., et al.\n(2023). AD-BERT: Using pre-trained language model to predict t he progression from\nmild cognitive impairment to Alzheimer’s disease. J. Biomed. Inform . 144, 104442.\ndoi: 10.1016/j.jbi.2023.104442\nMauran, C. (2023). “Google I/O 2023 unveils PaLM 2 large language m odel [WWW\nDocument], ” inMashable. Available online at: https://mashable.com/article/google-io-\n2023-palm2-ai-announcement (accessed 23 September, 2023).\nMcKenzie, I., Lyzhov, A., Parrish, A., Prabhu, A., Mueller, A. , Kim, N., et al.\n(2023). Inverse-scaling/prize. arXiv [Preprint]. doi: 10.48550/arXiv.2306.09479\nMcKinzie, B., Gan, Z., Fauconnier, J.-P., Dodge, S., Zhang, B., Dufter, P., et al.\n(2024). MM1: methods, analysis and insights from multimodal L LM pre-training.\narXiv [Preprint]. doi: 10.48550/arXiv.2403.09611\nMerkin, A., Krishnamurthi, R., and Medvedev, O. N. (2022). M achine learning,\nartiﬁcial intelligence and the prediction of dementia. Curr. Opin. Psychiatry 35,\n123–129. doi: 10.1097/YCO.0000000000000768\nMeskó, B., and Topol, E. J. (2023). The imperative for regulatory oversight of\nlarge language models (or generative AI) in healthcare. NPJ Digit. Med . 6, 1–6.\ndoi: 10.1038/s41746-023-00873-0\nMeta (2023a). Introducing LLaMA: A Foundational, 65-Billion-Parameter Langua ge\nModel. Available online at: https://ai.meta.com/blog/large-language-model-llama-\nmeta-ai/ (accessed 21 October, 2023).\nMeta (2023b). “Meta and Microsoft Introduce the Next Generat ion of Llama, ” in\nMeta AI. Available online at: https://ai.meta.com/blog/llama-2/ (accessed 20 December,\n2023).\nMeta (2024). “Introducing Meta Llama 3: The most capable openly av ailable\nLLM to date, ” in Meta AI. Available online at: https://ai.meta.com/blog/meta-llama-\n3/ (accessed 19 April, 2024).\nMin, B., Ross, H., Sulem, E., Veyseh, A. P. B., Nguyen, T. H., Sa inz, O., et al. (2023).\nRecent advances in natural language processing via large pre-tr ained language models:\na survey. ACM Comput. Surv. 56, 30:1–30:40. doi: 10.1145/3605943\nMiotto, M., Rossberg, N., and Kleinberg, B. (2022). “Who is GPT -3? An exploration\nof personality, values and demographics, ” in Proceedings of the Fifth Workshop on\nNatural Language Processing and Computational Social Science (NLP + CSS) (Abu\nDhabi: Association for Computational Linguistics), 218–22 7.\nMistral AI (2023a). “Mistral 7B, ” in Mistral AI. Available online at: https://mistral.\nai/news/announcing-mistral-7b/ (accessed 20 December, 2023).\nFrontiers in Dementia /two.tnum/eight.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nMistral AI (2023b). “Mixtral of experts, ” in Mistral AI. Available online at: https://\nmistral.ai/news/mixtral-of-experts/ (accessed 20 December, 2023).\nMistral AI (2024). “Au Large, ” in Mistral AI. Available online at: https://mistral.ai/\nnews/mistral-large/ (accessed 17 March, 2024).\nMitchell, M. (2023). How do we know how smart AI systems are? Science\n381:adj5957. doi: 10.1126/science.adj5957\nMitchell, M., and Krakauer, D. C. (2023). The debate over under standing\nin AI’s large language models. Proc. Natl. Acad. Sci . 120:e2215907120.\ndoi: 10.1073/pnas.2215907120\nMitra, A., Del Corro, L., Mahajan, S., Codas, A., Simoes, C., Agarwal, S., et al.\n(2023). Orca 2: teaching small language models how to reason. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2311.11045\nMo, Y., and Baptista, E. (2023). China’s Baidu unveils new Ernie AI version to rival\nGPT-4. London: Reuters.\nMondini, S., Madella, I., Zangrossi, A., Bigolin, A., Tomasi, C ., Michieletto, M., et al.\n(2016). Cognitive reserve in dementia: implications for cogn itive training. Front. Aging\nNeurosci. 8.84. doi: 10.3389/fnagi.2016.00084\nMorales-de-Jesús, V., Gómez-Adorno, H., Somodevilla-García, M., and Vilariño, D.\n(2021). Conversational System as assistant tool in reminis cence therapy for people with\nearly-stage of Alzheimer’s.Healthcare 9:1036. doi: 10.3390/healthcare9081036\nMosaic AI Research Team (2024). “Introducing DBRX: a new sta te-of-the-art\nopen LLM, ” in Mosaic AI Res. Available online at: https://www.databricks.com/blog/\nintroducing-dbrx-new-state-art-open-llm (accessed June 4, 2024).\nMurdoch, B. E., Chenery, H. J., Wilks, V., and Boyle, R. S. (1987 ). Language\ndisorders in dementia of the Alzheimer type. Brain Lang . 31, 122–137.\ndoi: 10.1016/0093-934X(87)90064-2\nMurley, A. G., Coyle-Gilchrist, I., Rouse, M. A., Jones, P. S., Li, W., Wiggins, J., et al.\n(2020). Redeﬁning the multidimensional clinical phenotypes of frontotemporal lobar\ndegeneration syndromes. Brain 143, 1555–1571. doi: 10.1093/brain/awaa097\nNandi, A., Counts, N., Chen, S., Seligman, B., Tortorice, D., Vigo, D., et al. (2022).\nGlobal and regional projections of the economic burden of Alzhei mer’s disease and\nrelated dementias from 2019 to 2050: A value of statistical life approach. eClinicalMed.\n51, 1–10. doi: 10.1016/j.eclinm.2022.101580\nNasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A. F., Ippolito, D., et al.\n(2023). Scalable extraction of training data from (production ) language models. arXiv\n[Preprint]. doi: 10.48550/arXiv.2311.17035\nNaveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M ., et al.\n(2023). A comprehensive overview of large language models. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2307.06435\nNguyen, T., and Li, X. (2020). Understanding public-stigma an d self-stigma in the\ncontext of dementia: a systematic review of the global literat ure. Dementia 19, 148–181.\ndoi: 10.1177/1471301218800122\nNichols, E., Steinmetz, J. D., Vollset, S. E., Fukutaki, K., Cha lek, J., Abd-Allah, F.,\net al. (2022). Estimation of the global prevalence of dementia in 2019 and forecasted\nprevalence in 2050: an analysis for the Global Burden of Disease St udy 2019. Lancet\nPublic Health 7, e105–e125. doi: 10.1016/S2468-2667(21)00249-8\nNorwegian Consumer Council (2023). Ghost in the Machine - Addressing\nthe Consumer Harms of Generative AI . Available online at: https://storage02.\nforbrukerradet.no/media/2023/06/generative-ai-rapport -2023.pdf (accessed February\n22, 2024).\nOh, S. (2023). A study on the case of using ChatGPT and learners’\nperceptions in college liberal arts writing. Korean J. Gen. Educ . 17, 11–23.\ndoi: 10.46392/kjge.2023.17.3.11\nOlney, N. T., Spina, S., and Miller, B. L. (2017). Frontotemporal d ementia. Neurol.\nClin. 35, 339–374. doi: 10.1016/j.ncl.2017.01.008\nO’Malley, R. P. D., Mirheidari, B., Harkness, K., Reuber, M., V enneri, A., Walker,\nT., et al. (2020). Fully automated cognitive screening tool bas ed on assessment\nof speech and language. J. Neurol. Neurosurg. Psychiatry 20:jnnp-2019-322517.\ndoi: 10.1002/alz.041980\nOmarov, B., Zhumanov, Z., Gumar, A., and Kuntunova, L. (2023) . Artiﬁcial\nintelligence enabled mobile chatbot psychologist using AIML and c ognitive behavioral\ntherapy. Int. J. Adv. Comput. Sci. Appl . 14:616. doi: 10.14569/IJACSA.2023.0140616\nOpenAI (2022). Introducing ChatGPT [WWW Document]. Available online at:\nhttps://openai.com/blog/chatgpt (accessed 16 December, 2023).\nOpenAI (2023). GPT-4 technical report. arXiv [Preprint]. arXiv: 2303.08774v6.\nOscar, N., Fox, P. A., Croucher, R., Wernick, R., Keune, J., a nd Hooker, K. (2017).\nMachine learning, sentiment analysis, and tweets: an examina tion of Alzheimer’s\ndisease stigma on Twitter. J. Gerontol. Ser. B 72, 742–751. doi: 10.1093/geronb/g\nbx014\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mi shkin, P., et al. (2022).\nTraining language models to follow instructions with human feed back. Adv. Neural\nInf. Process. Syst. 35, 27730–27744. Available online at: https://proceedings.neurips.cc/\npaper_ﬁles/paper/2022/ﬁle/b1efde53be364a73914f58805a001731-Paper-Conference.\npdf\nPal, A., Umapathi, L. K., and Sankarasubbu, M. (2023). Med-HALT: Medical Domain\nHallucination Test for Large Language Models .\nPark, S. H. (2023). Use of generative artiﬁcial intelligence, including large language\nmodels such as ChatGPT, in scientiﬁc publications: policies of KJR and prominent\nauthorities. Korean J. Radiol . 24, 715–718. doi: 10.3348/kjr.2023.0643\nParra, M. A., Butler, S., McGeown, W. J., Brown Nicholls, L. A., an d Robertson,\nD. J. (2019). Globalising strategies to meet global challenges: th e case of ageing and\ndementia. J. Glob. Health 9:020310. doi: 10.7189/jogh.09.020310\nPatel, F., Thakore, R., Nandwani, I., and Bharti, S. K. (2019) . “Combating depression\nin students using an intelligent chatbot: a cognitive behavio ral therapy, ” in 2019 IEEE\n16th India Council International Conference (INDICON) (Rajkot: IEEE), 1–4.\nPaulus, J. K., and Kent, D. M. (2020). Predictably unequal: under standing\nand addressing concerns that algorithmic clinical prediction may increase health\ndisparities. NPJ Digit. Med . 3, 1–8. doi: 10.1038/s41746-020-0304-9\nPichai, S., and Hassabis, D. (2023). “Introducing Gemini: Goo gle’s most capable\nAI model yet, ” in The Keyword . Available online at: https://blog.google/technology/ai/\ngoogle-gemini-ai/ (accessed December 17, 2023).\nPichai, S., and Hassabis, D. (2024). “Our next-generation m odel: Gemini 1.5, ” inThe\nKeyword. Available online at: https://blog.google/technology/ai/google-gemini-next-\ngeneration-model-february-2024/ (accessed February 22, 2024).\nPillette, L., Moreau, G., Normand, J.-M., Perrier, M., Lécuyer , A., and Cogné,\nM. (2023). A systematic review of navigation assistance sys tems for people with\ndementia. IEEE Trans. Vis. Comput. Graph. 29, 2146–2165. doi: 10.1109/TVCG.2022.31\n41383\nPlácido, J., de Almeida, C. A. B., Ferreira, J. V., de Oliveira Si lva, F., Monteiro-\nJunior, R. S., Tangen, G. G., et al. (2022). Spatial navigation in older adults with\nmild cognitive impairment and dementia: a systematic review a nd meta-analysis. Exp.\nGerontol. 165:111852. doi: 10.1016/j.exger.2022.111852\nPrince, M., Wimo, A., Guerchet, A., Ali, G.-C., Wu, Y.-T., and Pr ina, M. (2015).\nWorld Alzheimer Report 2015 The Global Impact of Dementia: An Analy sis of\nPrevalence, Incidence, Cost and Trends, Alzheimer’s Disease Interna tional. London:\nAlzheimer’s Disease International (ADI).\nQi, P., and Wu, P. (2023). ChatGPT: a promising tool to combat so cial isolation\nand loneliness in older adults with mild cognitive impairment. NeurologyLive\n6. Available online at: https://www.neurologylive.com/view/chatgpt-promising-too l-\ncombat-social-isolation-loneliness-older-adults-mild-cogni tive-impairment\nQiu, S., Miller, M. I., Joshi, P. S., Lee, J. C., Xue, C., Ni, Y., e t al. (2022). Multimodal\ndeep learning for Alzheimer’s disease dementia assessment. Nat. Commun . 13:3404.\ndoi: 10.1038/s41467-022-31037-5\nRaﬀaele, F., Claudia, M., and John, H. (2019). Genetics and molec ular mechanisms\nof frontotemporal lobar degeneration: an update and future ave nues. Neurobiol. Aging\n78, 98–110. doi: 10.1016/j.neurobiolaging.2019.02.006\nRaghavan, M., Barocas, S., Kleinberg, J., and Levy, K. (2020) . “Mitigating bias\nin algorithmic hiring: evaluating claims and practices, ” in Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency, FAT ∗’20 (New York:\nAssociation for Computing Machinery), 469–481.\nRai, H. K., Kernaghan, D., Schoonmade, L., Egan, K. J., and Po t, A. M. (2022). Digital\ntechnologies to prevent social isolation and loneliness in demen tia: a systematic review.\nJ. Alzheimers Dis . 90, 513–528. doi: 10.3233/JAD-220438\nRaile, P. (2024). The usefulness of ChatGPT for psychotherapists and patients.\nHumanit. Soc. Sci. Commun . 11, 1–8. doi: 10.1057/s41599-023-02567-0\nRaji, I. D., and Buolamwini, J. (2019). “Actionable auditing: investigating the\nimpact of publicly naming biased performance results of commercia l AI products, ” in\nProceedings of the (2019). AAAI/ACM Conference on AI, Ethics, and Societ y, AIES’19\n(New York: Association for Computing Machinery), 429–435.\nReisner, A. (2023). “These 183,000 Books Are Fueling the Bigg est Fight in\nPublishing and Tech, ” in The Atlantic . Available online at: https://www.theatlantic.\ncom/technology/archive/2023/09/books3-database-generative-ai-training-copyright-\ninfringement/675363/ (accessed 25 November, 2023).\nRichardson, A., Robbins, C. B., Wisely, C. E., Henao, R., Grewa l, D. S., and Fekrat,\nS. (2022). Artiﬁcial intelligence in dementia. Curr. Opin. Ophthalmol . 33, 425–431.\ndoi: 10.1097/ICU.0000000000000881\nRozado, D. (2023). The political biases of ChatGPT. Soc. Sci . 12, 148.\ndoi: 10.3390/socsci12030148\nRyu, H., Kim, S., Kim, D., Han, S., Lee, K., and Kang, Y. (2020) . Simple and steady\ninteractions win the healthy mentality: designing a chatbot s ervice for the elderly. Proc.\nACM Hum.-Comput. Interact. 4, 152:1–152:25. doi: 10.1145/3415223\nSaeidnia, H. R., Kozak, M., Lund, B. D., and Hassanzadeh, M. ( 2023).\nEvaluation of ChatGPT’s Responses to Information Needs and Infor mation Seeking of\nDementia Patients.\nSchneider, J. A., Arvanitakis, Z., Bang, W., and Bennett, D. A. (2007).\nMixed brain pathologies account for most dementia cases in com munity-\ndwelling older persons. Neurology 69, 2197–2204. doi: 10.1212/01.wnl.0000271090.281\n48.24\nFrontiers in Dementia /two.tnum/nine.tnum frontiersin.org\nTreder et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frdem./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/eight.tnum/five.tnum/three.tnum/zero.tnum/three.tnum\nSharma, N., Chakrabarti, S., and Grover, S. (2016). Gender diﬀ erences in caregiving\namong family - caregivers of people with mental illnesses. World J. Psychiatry 6, 7–17.\ndoi: 10.5498/wjp.v6.i1.7\nSheehan, B. (2012). Assessment scales in dementia. Ther. Adv. Neurol. Disord . 5,\n349. doi: 10.1177/1756285612455733\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., et al.\n(2023). Large language models encode clinical knowledge. Nature 620, 172–180.\ndoi: 10.1038/s41586-023-06291-2\nSorin, V., Brin, D., Barash, Y., Konen, E., Charney, A., Nadk arni, G., et al. (2023).\nLarge Language Models (LLMs) and Empathy – A Systematic Review .\nStamate, D., Smith, R., Tsygancov, R., Vorobev, R., Langham , J., Stahl, D., et al.\n(2020). Applying deep learning to predicting dementia and mild cog nitive impairment.\nArtif. Intell. Appl. Innov . 584, 308–319. doi: 10.1007/978-3-030-49186-4_26\nSundjaja, J. H., Shrestha, R., and Krishan, K. (2024). McNemar And Mann-Whitney\nU Tests. Treasure Island: StatPearls Publishing.\nTay, K.-W., Subramaniam, P., and Oei, T. P. (2019). Cognitiv e behavioural therapy\ncan be eﬀective in treating anxiety and depression in persons w ith dementia: a\nsystematic review. Psychogeriatrics 19, 264–275. doi: 10.1111/psyg.12391\nTian, S., Jin, Q., Yeganova, L., Lai, P.-T., Zhu, Q., Chen, X. , et al. (2024).\nOpportunities and challenges for ChatGPT and large language models i n biomedicine\nand health. Brief. Bioinform. 25:bbad493. doi: 10.1093/bib/bbad493\nToth, L., Hoﬀmann, I., Gosztolya, G., Vincze, V., Szatloczki, G., Banreti, Z., et al.\n(2018). A speech recognition-based solution for the automati c detection of mild\ncognitive impairment from spontaneous speech. Curr. Alzheimer Res . 15, 130–138.\ndoi: 10.2174/1567205014666171121114930\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M .-A., Lacroix, T.,\net al. (2023). LLaMA: open and eﬃcient foundation language mode ls. arXiv [Preprint].\ndoi: 10.48550/arXiv.2302.13971\nTreder, M. S., Shock, J. P., Stein, D. J., du Plessis, S., Seeda t, S., and Tsvetanov, K.\nA. (2021). Correlation constraints for regression models: co ntrolling bias in brain age\nprediction. Front. Psychiatry 12:615754. doi: 10.3389/fpsyt.2021.615754\nTsoi, K. K. F., Jia, P., Dowling, N. M., Titiner, J. R., Wagner, M., Capuano, A. W.,\net al. (2023). Applications of artiﬁcial intelligence in dementia research. Camb. Prisms\nPrecis. Med. 1:e9. doi: 10.1017/pcm.2022.10\nVaidyam, A. N., Wisniewski, H., Halamka, J. D., Kashavan, M. S ., and Torous, J. B.\n(2019). Chatbots and conversational agents in mental health : a review of the psychiatric\nlandscape. Can. J. Psychiatry 64, 456–464. doi: 10.1177/0706743719828977\nVan Veen, D., Van Uden, C., Blankemeier, L., Delbrouck, J.-B., Aali, A.,\nBluethgen, C., et al. (2024). Adapted large language models can out perform\nmedical experts in clinical text summarization. Nat. Med . 30, 1134–1142.\ndoi: 10.1038/s41591-024-02855-5\nVan Veen, D., Van Uden, C., Blankemeier, L., Delbrouck, J. B., A ali,\nA., Bluethgen, C., et al. (2023). Clinical text summarization: adapting large\nlanguage models can outperform human experts. Res. Square rs.3.rs-3483777.\ndoi: 10.21203/rs.3.rs-3483777/v1\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” in Advances in Neural Information Processing\nSystems (Red Hook, NY: Curran Associates, Inc), 5998–6008.\nVictor, C. R. (2021). Is loneliness a cause or consequence of dem entia?\nA public health analysis of the literature. Front. Psychol . 11:612771.\ndoi: 10.3389/fpsyg.2020.612771\nVincent, J. (2023). “Google and Microsoft’s chatbots are alrea dy citing one another\nin a misinformation shitshow, ” in The Verge. Available online at: https://www.theverge.\ncom/2023/3/22/23651564/google-microsoft-bard-bing-ch atbots-misinformation\n(accessed 28 November, 2023).\nVinod, P., Safar, S., Mathew, D., Venugopal, P., Joly, L. M., and George, J.\n(2020). “Fine-tuning the BERTSUMEXT model for clinical repor t summarization, ”\nin 2020 International Conference for Emerging Technology (INCET) (Belgaum:\nIEEE), 1–7.\nvon Werra, L., Belkada, Y., Mangrulkar, S., Tunstall, L., Dehaen e, O., Cuenca, P.,\net al. (2023). “The Falcon has landed in the Hugging Face ecosyst em, ” inHugging Face\nBlog. Available online at: https://huggingface.co/blog/falcon (accessed 20 December,\n2023).\nWang, B., and Li, Y. (2023). “Enabling conversational intera ction on mobile with\nLLMs, ” in Google Res. Blog . Available online at: https://blog.research.google/2023/05/\nenabling-conversational-interaction-on.html (accessed January 1, 2024).\nWang, Y., Zhong, W., Li, L., Mi, F., Zeng, X., Huang, W., et al. ( 2023).\nAligning large language models with human: a survey. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2307.12966.\nWei, A., Haghtalab, N., and Steinhardt, J. (2023). Jailbroken : how does llm safety\ntraining fail? arXiv [Preprint]. doi: 10.48550/arXiv.2307.02483\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia , F., et al.\n(2023). Chain-of-thought prompting elicits reasoning in large language models. arXiv\n[Preprint]. doi: 10.48550/arXiv.2201.11903\nWHO (2023). Dementia [WWW Document]. Available online at: https://www.who.\nint/news-room/fact-sheets/detail/dementia (accessed November 18, 2023).\nWu, C., Lin, W., Zhang, X., Zhang, Y., Wang, Y., and Xie, W. (20 23). PMC-\nLLaMA: towards building open-source language models for medici ne. arXiv [Preprint].\ndoi: 10.48550/arXiv.2304.14454\nWu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R., Lei ke, J., et al. (2021).\nRecursively Summarizing Books with Human Feedback .\nxAI (2024a). “Open Release of Grok-1, ” in Xai Blog. Available online at: https://x.ai/\nblog/grok-os (accessed April 6, 2024).\nxAI (2024b). “Announcing Grok-1.5, ” in Xai Blog. Available online at: https://x.ai/\nblog/grok-1.5 (accessed April 6, 2024).\nYao, S., Yu, D., Zhao, J., Shafran, I., Griﬃths, T. L., Cao, Y., et al. (2023). Tree\nof thoughts: deliberate problem solving with large language models . arXiv [Preprint] .\ndoi: 10.48550/arXiv.2305.10601\nYasunaga, M., Chen, X., Li, Y., Pasupat, P., Leskovec, J., Lia ng, P., et al.\n(2023). Large language models as analogical reasoners. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2310.01714\nYe, H., Liu, T., Zhang, A., Hua, W., and Jia, W. (2023). Cognit ive\nmirage: a review of hallucinations in large language models. arXiv [Preprint] .\ndoi: 10.48550/arXiv.2309.06794\nZhang, L., Negrinho, R., Ghosh, A., Jagannathan, V., Hassan zadeh, H. R., Schaaf, T.,\net al. (2021). “Leveraging pretrained models for automatic sum marization of doctor-\npatient conversations, ” in Findings of the Association for Computational Linguistics:\nEMNLP 2021 (Punta Cana: Association for Computational Linguistics), 3 693–3712.\nZhang, Y., Dong, Z., Phillips, P., Wang, S., Ji, G., Yang, J., et al. (2015).\nDetection of subjects and brain regions related to Alzheimer’ s disease using 3D MRI\nscans based on eigenbrain and machine learning. Front. Comput. Neurosci . 9:66.\ndoi: 10.3389/fncom.2015.00066\nZhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., et al. (2023). Siren’s song\nin the ai ocean: a survey on hallucination in large language models . arXiv [Preprint] .\ndoi: 10.48550/arXiv.2309.01219\nZhao, H., Chen, H., Yang, F., Liu, N., Deng, H., Cai, H., et al. ( 2023). Explainability\nfor large language models: a survey. arXiv [Preprint]. doi: 10.48550/arXiv.2309.01029\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A. , Amodei, D.,\net al. (2020). Fine-tuning language models from human preferenc es. arXiv [Preprint] .\ndoi: 10.48550/arXiv.1909.08593\nFrontiers in Dementia /three.tnum/zero.tnum frontiersin.org",
  "topic": "Dementia",
  "concepts": [
    {
      "name": "Dementia",
      "score": 0.7419162392616272
    },
    {
      "name": "Psychology",
      "score": 0.3967168927192688
    },
    {
      "name": "Medicine",
      "score": 0.3511139750480652
    },
    {
      "name": "Linguistics",
      "score": 0.342781662940979
    },
    {
      "name": "Gerontology",
      "score": 0.32800665497779846
    },
    {
      "name": "Pathology",
      "score": 0.12206253409385681
    },
    {
      "name": "Disease",
      "score": 0.10091644525527954
    },
    {
      "name": "Philosophy",
      "score": 0.08603319525718689
    }
  ]
}