{
  "title": "Evaluating and Mitigating Limitations of Large Language Models in Clinical Decision Making",
  "url": "https://openalex.org/W4391246451",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2090420131",
      "name": "Paul Hager",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A3010453869",
      "name": "Friederike Jungmann",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2316552699",
      "name": "Kunal Bhagat",
      "affiliations": [
        "Christiana Care Health System"
      ]
    },
    {
      "id": "https://openalex.org/A4221537142",
      "name": "Inga Hubrecht",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2756012100",
      "name": "Manuel Knauer",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A4284769816",
      "name": "Jakob Vielhauer",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": "https://openalex.org/A2182732240",
      "name": "Robbie Holland",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A730543322",
      "name": "Rickmer Braren",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2598619534",
      "name": "Marcus Makowski",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A5093793419",
      "name": "Georgios Kaisis",
      "affiliations": [
        "Helmholtz Zentrum München",
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A2169057019",
      "name": "Daniel Rueckert",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A2090420131",
      "name": "Paul Hager",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A3010453869",
      "name": "Friederike Jungmann",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2316552699",
      "name": "Kunal Bhagat",
      "affiliations": [
        "Christiana Care Health System"
      ]
    },
    {
      "id": "https://openalex.org/A4221537142",
      "name": "Inga Hubrecht",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2756012100",
      "name": "Manuel Knauer",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A4284769816",
      "name": "Jakob Vielhauer",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": "https://openalex.org/A2182732240",
      "name": "Robbie Holland",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A730543322",
      "name": "Rickmer Braren",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2598619534",
      "name": "Marcus Makowski",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A5093793419",
      "name": "Georgios Kaisis",
      "affiliations": [
        "Helmholtz Zentrum München",
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A2169057019",
      "name": "Daniel Rueckert",
      "affiliations": [
        "Imperial College London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2990031975",
    "https://openalex.org/W3159875333",
    "https://openalex.org/W4328120750",
    "https://openalex.org/W4389000204",
    "https://openalex.org/W2491597686",
    "https://openalex.org/W4318931874",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2531256715",
    "https://openalex.org/W4389116614",
    "https://openalex.org/W4404573785",
    "https://openalex.org/W3017264036",
    "https://openalex.org/W4388525110",
    "https://openalex.org/W2162800060",
    "https://openalex.org/W3008058947",
    "https://openalex.org/W3022871146",
    "https://openalex.org/W3088056511",
    "https://openalex.org/W4380730209",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W2949891360",
    "https://openalex.org/W2998175747",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W4386003857",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4221154592",
    "https://openalex.org/W3096406900",
    "https://openalex.org/W4386045865",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2899876413",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4390876225",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W4364378939",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W2014676531",
    "https://openalex.org/W4389164139",
    "https://openalex.org/W4387211791",
    "https://openalex.org/W4385570802",
    "https://openalex.org/W3092775028"
  ],
  "abstract": "Abstract Clinical decision making is one of the most impactful parts of a physician’s responsibilities and stands to benefit greatly from AI solutions and large language models (LLMs) in particular. However, while LLMs have achieved excellent performance on medical licensing exams, these tests fail to assess many skills that are necessary for deployment in a realistic clinical decision making environment, including gathering information, adhering to established guidelines, and integrating into clinical workflows. To understand how useful LLMs are in real-world settings, we must evaluate them in the wild , i.e. on real-world data under realistic conditions. Here we have created a curated dataset based on the MIMIC-IV database spanning 2400 real patient cases and four common abdominal pathologies as well as a framework to simulate a realistic clinical setting. We show that current state-of-the-art LLMs do not accurately diagnose patients across all pathologies (performing significantly worse than physicians on average), follow neither diagnostic nor treatment guidelines, and cannot interpret laboratory results, thus posing a serious risk to the health of patients. Furthermore, we move beyond diagnostic accuracy and demonstrate that they cannot be easily integrated into existing workflows because they often fail to follow instructions and are sensitive to both the quantity and order of information. Overall, our analysis reveals that LLMs are currently not ready for clinical deployment while providing a dataset and framework to guide future studies.",
  "full_text": "EVALUATING AND MITIGATING LIMITATIONS OF LARGE\nLANGUAGE MODELS IN CLINICAL DECISION MAKING\nPaul Hager1,2,∗ , Friederike Jungmann1,2,∗, Kunal Bhagat3, Inga Hubrecht4, Manuel Knauer4, Jakob Vielhauer5,\nRobbie Holland7, Rickmer Braren2, Marcus Makowski2, Georgios Kaisis1,6,7, Daniel Rueckert1,7\nABSTRACT\nClinical decision making is one of the most impactful parts of a physician’s responsibilities and stands1\nto benefit greatly from AI solutions and large language models (LLMs) in particular. However, while2\nLLMs have achieved excellent performance on medical licensing exams, these tests fail to assess3\nmany skills that are necessary for deployment in a realistic clinical decision making environment,4\nincluding gathering information, adhering to established guidelines, and integrating into clinical5\nworkflows. To understand how useful LLMs are in real-world settings, we must evaluate them in the6\nwild, i.e. on real-world data under realistic conditions. Here we have created a curated dataset based7\non the MIMIC-IV database spanning 2400 real patient cases and four common abdominal pathologies8\nas well as a framework to simulate a realistic clinical setting. We show that current state-of-the-art9\nLLMs do not accurately diagnose patients across all pathologies (performing significantly worse10\nthan physicians on average), follow neither diagnostic nor treatment guidelines, and cannot interpret11\nlaboratory results, thus posing a serious risk to the health of patients. Furthermore, we move beyond12\ndiagnostic accuracy and demonstrate that they cannot be easily integrated into existing workflows13\nbecause they often fail to follow instructions and are sensitive to both the quantity and order of14\ninformation. Overall, our analysis reveals that LLMs are currently not ready for clinical deployment15\nwhile providing a dataset and framework to guide future studies.16\n1 Main17\nLarge language models (LLMs) have the potential to revolutionize our medical system[54]. They can already streamline18\nreport generation and summarization[61, 59, 8, 37], answer biomedical questions with[60, 5, 59] and without[50, 49, 38]19\nimages, and could soon effectively interpret multimodal data for precision medicine in the clinic[ 6]. Importantly,20\nas humans primarily interact with the world through language, LLMs are poised to be the point of access to the21\nmultimodal medical AI solutions of the future[ 36]. Until now, however, the diagnostic capabilities of models have22\nbeen tested in structurally simple medical contexts, such as canonical vignettes of hypothetical patients or clinical case23\nchallenges. In both scenarios, all the required diagnostic information is provided upfront and there is a single answer to24\nbe selected from a list of options. This type of question dominates the medical licensing exams that have been used to25\ntest LLMs such as the United States Medical Licensing Exam (USMLE)[27, 26], Applied Knowledge Test (AKT) of26\nthe Membership of the Royal College of General Practicioners (RCGP)[53], and All India Institute of Medical Sciences27\n(AIIMS) & National Eligibility cum Entrance Test Postgraduate (NEETS PG) entrance exam[42]. LLMs have proven28\ntheir ability to excel in such scenarios, scoring well above passing on medical licensing exams[31, 21, 49, 50, 38, 56, 39]29\nand rivaling clinician performance on clinical case challenges [34, 28, 10, 18].30\nHowever, while these medical licensing exams and clinical case challenges are suitable for testing the general medical31\nknowledge of the test-taker, they are far removed from the daily task of clinical decision making. For medical32\npractitioners, clinical decision making is one of the most important and complex everyday responsibilities. It is a33\nmulti-step process that requires gathering and synthesizing data from diverse sources and continuously evaluating34\nthe facts to reach an evidence-based decision on patient diagnosis and treatment[7, 55]. To reach a precise diagnosis,35\nphysicians must gather the necessary information based on the availability of diagnostic resources and adhere to36\nestablished guidelines. Furthermore, they must carefully consider the patient-specific symptoms to plan the optimal37\ntreatment. As this process is very labor-intensive, there exists great potential in harnessing AI, such as LLMs, to38\nalleviate much of the workload, ultimately aiming to autonomously, efficiently, and safely reach a final diagnosis which39\ncan then be confirmed by physicians[ 48, 3]. Thus, to understand how useful LLMs would be in such a real-world40\n1 Lab for AI in Medicine, TUM, Munich, Germany 2 Institute for Diagnostic and Interventional Radiology, Klinikum rechts der\nIsar, TUM, Munich, Germany 3 Department of Medicine, ChristianaCare Health System, Wilmington, DE, USA 4 Department of\nMedicine III, Klinikum rechts der Isar, TUM, Munich, Germany 5 Department of Medicine II, University Hospital of the LMU\nMunich, Munich, Germany 6 Helmholtz Munich 7 Imperial College London ∗ Equal contribution. E-mail: paul.hager@tum.de\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nFilter forTarget Pathologies\nMIMIC-IVa\nDiagnosticAccuracy\nClinical Guidelines\nInstructionFollowing\nLab Test Interpretation\nRobustness\nEVALUATIONb\nMIMIC-CDM\n MIMIC-CDM Statistics\n2400CasesAppendicitisCholecystitisDiverticulitisPancreatitis257648957\n538\nCT AbdomenRadiograph ChestUS AbdomenRadiograph AbdomenMRCP AbdomenOther\n183617281325342227501143191 Laboratory Tests5959RadiologistReports2400PhysicalExamination\nFigure 1: (a.) To properly evaluate LLMs for clinical decision making in realistic conditions, we created a curated\ndataset from real-world patient cases derived from the MIMIC-IV database, which contains comprehensive electronic\nhealth record data recorded during hospital admissions. (b.) Our evaluation framework reflects a realistic clinical setting\nand thoroughly evaluates LLMs across multiple criteria, including diagnostic accuracy, adherence to diagnostic and\ntreatment guidelines, consistency in following instructions, ability to interpret laboratory results, and robustness to\nchanges in instruction, information quantity, and information order. Abbreviations: ICD: International Classification of\nDiseases. CT: Computed Tomography. US: Ultrasound. MRCP: Magnetic resonance cholangiopancreatography.\n2\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nsetting, they must be evaluated on real-world data and under realistic conditions. However, the only analysis that tested41\nan LLM throughout the clinical workflow, used curated lists of possible answers and examined only 36 hypothetical42\nclinical vignettes[46]. Furthermore, any model that is used in such a high-stakes clinical context must not only be43\nhighly accurate, but also adhere to diagnostic and treatment guidelines, be robust, and follow instructions, all of which44\nhave not been tested in previous medical evaluations.45\nHere, we present a curated dataset based on the Medical Information Mart for Intensive Care (MIMIC-IV) database46\nspanning 2400 real patient cases and four common abdominal pathologies (appendicitis, pancreatitis, cholecystitis,47\ndiverticulitis) as well as a comprehensive evaluation framework around our dataset to simulate a realistic clinical setting.48\nWe provide LLMs with a patient’s history of present illness and ask them to iteratively gather and synthesize additional49\ninformation such as physical examinations, laboratory results, and imaging reports until they are confident enough to50\nprovide a diagnosis and treatment plan. Our new dataset, task, and analysis represent the first large-scale evaluation of51\nLLMs on every-day clinical decision making tasks in a realistic, open-ended environment. Unlike previous works, we52\ntest the autonomous information gathering and open-ended diagnostic capabilities of models, representing an essential53\nstep towards evaluating their suitability as clinical decision makers.54\nTo understand how useful LLMs would be in the clinic today, we compare the diagnostic accuracy of the models on our55\ndataset with that of clinicians. Furthermore, we propose and evaluate a new range of characteristics beyond diagnostic56\naccuracy, such as adherence to diagnostic and treatment guidelines, correct interpretation of laboratory test results,57\ninstruction following capabilities, and robustness to changes in instructions, information order, and information quantity.58\nFinally, we show that summarizing progress and filtering laboratory results for only abnormal results addresses some of59\nthe current limitations of models. We make our evaluation framework and dataset freely and openly available to guide60\nfuture studies considering the use of LLMs in clinical practice.61\n2 Results62\n2.1 Creating the MIMIC-CDM dataset and evaluation framework63\nOur curated dataset, MIMIC Clinical Decision Making (MIMIC-CDM), is created using the well-established MIMIC-IV64\ndatabase, which is managed by the Massachusetts Institute of Technology (MIT). MIMIC-IV contains de-identified65\nrecords of patient measurements, diagnoses, procedures, treatments, and free-text clinical notes such as discharge66\nsummaries and radiologists reports from patients admitted to the Beth Israel Deaconess Medical Center in Boston,67\nUSA, from 2008 to 2019[22]. Figure 1a and Materials Section 6 list the steps involved in creating the MIMIC-CDM68\ndataset and its makeup. Our dataset contains electronic health record data from 2400 unique patients presenting with69\nacute abdominal pain to the emergency department and whose primary diagnosis was one of the following pathologies:70\nappendicitis, cholecystitis, diverticulitis, or pancreatitis. We chose these target pathologies as they represent clinically71\nimportant diagnoses of a common chief complaint, abdominal pain, which accounts for 10% of all emergency department72\nvisits [11, 15]. Furthermore, different treatment strategies, ranging from antibiotics to surgery, are necessary depending73\non the severity of the condition. Thus, a thorough understanding of the specifics of a patient’s case is required to74\nrecommend optimal treatment. Importantly, a good differentiation between the four pathologies can be achieved using75\nstandard diagnostic tests, all of which are present in MIMIC-CDM.76\nTo reflect a realistic clinical setting that allows LLMs to autonomously engage in every step of the clinical decision77\nmaking process we have created a comprehensive evaluation framework around our dataset. Using our framework and78\ndataset, we present LLMs with a patient’s history of present illness and task them to gather and synthesize information79\nsuch as physical examinations, specific laboratory results, and distinct imaging reports to arrive at a diagnosis and80\ntreatment plan, as shown in Figure 1b. We then take the complete interaction and evaluate the LLM for diagnostic81\naccuracy as well as adherence to diagnostic and treatment guidelines, and instruction following capabilities. For82\ncomparisons with practicing clinicians and further tests concerning robustness, we evaluate the diagnostic accuracy83\nof LLMs when provided with all necessary information for a diagnosis; a dataset we call MIMIC Clinical Decision84\nMaking with Full Information (MIMIC-CDM-FI). In this dataset, we include the history of present illness, physical85\nexamination, relevant laboratory results, and all abdominal imaging, before directly asking for a diagnosis.86\nIn our study, we tested the leading open-access LLM developed by Meta, Llama 2[58], and its derivatives. We focus87\non the largest model with 70 billion (B) parameters, as it performed best on benchmarks including medical questions.88\nWe test both generalist versions such as Llama 2 Chat (70B)[58], Open Assistant (OASST) (70B)[30], and WizardLM89\n(70B)[63], as well as medical-domain aligned models such as Clinical Camel (70B)[ 56], and Meditron (70B)[ 12].90\nFurther information on the models and our selection criteria can be found in Materials Section 6.3. Data taken from the91\nMIMIC database is currently prohibited from being used with external APIs, such as that of OpenAI or Google, due to92\ndata privacy concerns and data usage agreements, so neither ChatGPT nor GPT-4 could be tested. Furthermore, multiple93\n3\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nrequests for access to Google’s MedPaLM models were denied. We note that Llama 2, Clinical Camel, and Meditron94\nhave been shown to match and even exceed ChatGPT performance on medical licensing exams and biomedical question95\nanswering tests[56, 12].96\n2.2 LLMs diagnose significantly worse than clinicians97\nTo ensure patient safety, LLMs must diagnose at least as well as clinicians. Thus, we compared the diagnostic accuracy98\nof the models on a subset of MIMIC-CDM-FI to four practicing hospitalists: two from the Klinikum Rechts der Isar99\nHospital of the Technical University of Munich, Germany (with two and three years of experience), one from the100\nLudwig Maximilian University Hospital in Munich, Germany, (four years of experience) and one from the Christiana101\nCare hospital in Delaware, United States of America (29 years of experience). All four of the hospitalists are internal102\nmedicine physicians with emergency department experience. Each hospitalist was instructed to provide the primary103\npathology afflicting the patient and was given the same 100 patients in a random order to diagnose. 20 patients of each104\ntarget pathology (appendicitis, cholecystitis, diverticulitis, pancreatitis) were included, with an additional five patients105\neach of four other abdominal pathologies: gastritis, urinary tract infection, esophageal reflux, and hernia. Each LLM106\nmodel was evaluated 10 times, using different random seeds, over the subset of 80 patients to increase statistical power.107\nAll statistical tests were corrected for multiple comparisons.108\nAppendicitis Cholecystitis Diverticulitis Pancreatitis Mean\n0\n20\n40\n60\n80\n100Diagnostic Accuracy (%)\n88\n62\n55 58\n66\n100\n62\n35\n78\n69\n100\n46 46\n74\n67\n100\n68\n50\n76 73\n98\n6\n58\n78\n60\n96\n84 86 89 89\nLlama 2 Chat OASST WizardLM Clinical Camel Meditron Doctors\nFigure 2: On a subset (n=80) of MIMIC-CDM-FI, LLMs perform significantly worse than doctors on average and\nespecially on diseases such as cholecystitis and diverticulitis. The exact diagnostic accuracy is shown above each bar.\nVertical lines indicate range of scores.\nWe find that current LLMs perform significantly worse than clinicians as each model’s mean performance, averaged109\nover all four pathologies, is significantly lower than the mean of all clinicians (Doctors vs. Llama 2 Chat, p=8.06e-10;110\nDoctors vs. OASST, p=1.10e-4; Doctors vs. WizardLM, p=6.18e-6; Doctors vs. ClinicalCamel, p=1.89e-4; Doctors vs111\nMeditron, p=6.40e-5) (Fig. 2). The diagnostic accuracy of the clinicians also varied, with the German hospitalists in112\nresidency (Mean = 87.50% ± 3.68%) performing slightly worse than the more senior US hospitalist (Mean = 92.50%),113\nwhich can be attributed to differences in both experience and language, considering all text was in English. Most114\nmodels were able to match clinician performance on the simplest diagnosis, appendicitis, where 3 out of 4 clinicians115\nalso correctly diagnosed 20 out of 20 patients. While the Meditron model matched or exceeded the other models at116\ndiagnosing appendicitis, diverticulitis, and pancreatitis patients, it failed for cholecystitis, diagnosing only one patient117\nwith ‘cholecystitis’ and the others primarily with ‘gallstones’ without specifying their location or inflammatory effects.118\nThis mirrors the general performance of the models, which may perform well on certain pathologies but currently lack119\nthe diagnostic range of human hospitalists. In a standard clinical scenario, where every diagnosis is a possibility, models120\nmust perform consistently across all pathologies of a single initial complaint, such as abdominal pain, to be useful.121\nNo specialist medical models performs significantly better on average than any generalist models (Clinical Camel vs.122\nLlama 2 Chat, p=0.13; Clinical Camel vs. OASST, p=1.13, Clinical Camel vs. WizardLM, p=0.47; Meditron vs. Llama123\n2 Chat, p=1.11; Meditron vs. OASST, p=0.61; Meditron vs. WizardLM, p=1.04) (Fig. 2). Furthermore, as the medical124\nLLMs are not instruction-fine-tuned (i.e. trained to understand and undertake new tasks), they are unable to complete125\n4\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nthe full clinical decision making task where they must first gather information and then come to a diagnosis. As this is126\nthe primary use-case of a clinical decision making model, we exclude them from all further analysis and only examine127\nthe Llama 2 Chat, OASST and WizardLM models for the rest of this work.128\nIn our simulated clinical environment, which uses the MIMIC-CDM dataset, the LLM must specify all information129\nit wishes to gather to accurately diagnose a patient. We observe a general decrease in performance, compared to130\nMIMIC-CDM-FI (Supplementary Fig. 13), across the pathologies (Fig. 3). The mean diagnostic averages fall to131\n45.5% (vs. 58.8% on MIMIC-CDM-FI) for Llama 2 Chat, 54.9% (vs. 67.8%) for OASST, and 53.9% (vs. 65.1%) for132\nWizardLM. Mirroring MIMIC-CDM-FI, all models performed best in diagnosing appendicitis (Llama 2 Chat: 74.6%,133\nOASST: 82.0%, WizardLM: 78.4%) which is most likely due to the fact that appendicitis patients have consistent134\nkey symptoms with 791 of 957 radiologist reports (82.7%) directly stating that the appendix is dilated, enlarged, or135\nfluid-filled, and typically lack other intra-abdominal pathology descriptions which distract from the acute diagnosis.136\nAppendicitis Cholecystitis Diverticulitis Pancreatitis Mean\n0\n20\n40\n60\n80\n100Diagnostic Accuracy (%)\n74.6\n33.8 34.2 39.2\n45.5\n82.0\n48.0 45.5 44.1\n54.9\n78.4\n47.4 44.4 45.5\n53.9\nLlama 2 Chat OASST WizardLM\nFigure 3: When tasked with gathering all information required for clinical decision making themselves, LLMs perform\nbest when diagnosing appendicitis but perform poorly on the other three pathologies of cholecystitis, diverticulitis, and\npancreatitis. In such a realistic clinical scenario, model performance decreased compared to the retrospective diagnosis\nwith all information provided (MIMIC-CDM-FI). The exact diagnostic accuracy is shown above each bar.\nIn summary, LLMs do not reach the diagnostic accuracy of clinicians across all pathologies, especially when they must137\ngather all information themselves. Thus, without extensive physician supervision, they would reduce the quality of care138\nthat patients receive and are currently unfit for the task of clinical decision making.139\n2.3 LLMs are hasty and unsafe clinical decision makers140\nIn addition to poor diagnostic accuracy, LLMs often fail to order the exams required by diagnostic guidelines, do not141\nfollow treatment guidelines, and are incapable of interpreting lab results, making them a risk to patient safety.142\nTo help clinicians consistently and safely reach a final diagnosis, diagnostic guidelines are published by associations of143\nmedical experts. The guidelines help guarantee a consistent standard of care by establishing a clinical consensus of144\nwhich tests should be used to effectively diagnose a pathology, based on large clinical trials and clinician experience.145\nThe current clinical guidelines used for this study were: appendicitis[ 16], cholecystitis[44], diverticulitis [25], and146\npancreatitis[32].147\nAll guidelines recommended physical examinations as an essential part of the diagnostic process, preferably as the first148\naction. This is because they immediately provide valuable information as to the severity of the patient’s condition and a149\nbase for subsequent requests for laboratory tests and imaging. We find that only Llama 2 Chat consistently asks for150\nphysical examination results, either as the first action (97.1%) or at all (98.1%) (Supplementary Fig. 11). The other151\ntwo models requested less examinations (OASST: 79.8% & 87.7%; WizardLM: 53.1% & 63.9%), thereby omitting an152\nessential piece of information.153\nHospitalists routinely order laboratory tests to measure biological markers and evaluate organ function, allowing them154\nto track changes in a patient’s health and detect underlying biochemical shifts due to disease. Based on the diagnostic155\nguidelines, we defined categories of necessary laboratory tests for each pathology, including signs of inflammation,156\n5\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nInflammation\n0\n20\n40\n60\n80\n100Lab T est Requested (%)\n84.3\n93.3\n52.7\n100\nAppendicitis\nInflammation Liver Gallbladder\n64.0\n51.9 50.6\n81.0 84.1 82.9\n33.0\n50.2 49.2\n100 100 100\nCholecystitis\nInflammation\nLaboratory T est Category\n0\n20\n40\n60\n80\n100Lab T est Requested (%)\n72.8\n87.2\n63.0\n100\nDiverticulitis\nInflammation Pancreas Severity\nLaboratory T est Category\n44.6\n53.3 57.6\n75.3\n56.5\n76.2\n24.3\n49.6\n41.4\n100 100 100\nPancreatitis\nLlama 2 Chat OASST WizardLM MIMIC Doctors\nFigure 4: LLMs often do not order the necessary laboratory tests required to establish a diagnosis, despite each necessary\ntest being ordered independently by all doctors in the MIMIC-CDM dataset. The tests, defined by current diagnostic\nguidelines, help differentiate abdominal pathologies, as results can indicate which organ is currently pathologically\nafflicted or functioning normally.\nfunctional fitness of the liver and gallbladder, pancreas enzymes, and the severity of a patient’s pancreatitis. The exact157\ntests included in each category can be found in Supplementary Section D. We find that no model consistently orders158\nall necessary tests, despite each test being independently requested by all doctors in the MIMIC-CDM dataset (Fig.159\n4). While OASST performs better than the other two models, reaching up to 93.3% and 87.2% in the inflammation160\ncategory for appendicitis and diverticulitis, it often does not order the necessary tests for a diagnosis of pancreatitis161\n(pancreas enzymes: 56.5%, severity: 76.2%), partially explaining why its diagnostic performance on pancreatitis was162\nonly 44.1% (Fig. 3). Not ordering all necessary tests makes it difficult to differentiate between abdominal pathologies,163\nas laboratory results can provide an indication which organ is currently afflicted or functioning normally.164\nWhile it is important to order the correct laboratory tests, it is even more essential to correctly interpret them. The165\nlaboratory tests in MIMIC-IV have reference ranges included when applicable, so we tested the interpretation capabilities166\nof the models by providing the test result with the accompanying reference range and asking them to classify each result167\nas either below, within, or above the provided range. Any human with numerical literacy should be able to achieve168\nperfect accuracy on such a task, however all LLMs perform very poorly, especially in the critical categories of low169\ntest results (Chat: 26.5%, OASST: 70.2%, WizardLM: 45.8%) and high test results (Chat: 50.1%, OASST: 77.2%,170\nWizardLM: 24.1%) (Supplementary Fig. 15). Such a basic incomprehension of laboratory test results is a great risk to171\npatient safety and must be resolved before LLMs become clinically useful.172\nWhile diagnostic guidelines also have recommendations for which imaging modality is best suited for establishing173\na diagnosis, the modality used in practice can vary based on current availability and the particularities of a patient’s174\ncase. We find that models sometimes match the modalities requested by the doctors in the dataset, but often come to175\na diagnosis without requesting an abdominal imaging scan (Supplementary Fig. 12). We include the first abdominal176\nimaging modality recorded for each patient in MIMIC-CDM for comparison. As we later show that imaging is the177\nmost useful diagnostic tool for the LLMs for all pathologies except pancreatitis (Fig. 8), occasional failure to request178\nimaging could be partly responsible for their low diagnostic accuracy.179\n6\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nTaken together, the lack of consistency of the LLMs in ordering all of the required tests for a diagnosis based on current180\nguidelines indicates a tendency to diagnose before understanding or considering all of the facts of the patient’s case.181\nThis could pose a risk to patients’ health as they would be diagnosed based on insufficient information.182\nIn addition to not following diagnostic guidelines, LLMs generally fail to adhere to treatment guidelines (Fig. 5).183\nTo evaluate their ability to recommend appropriate treatments, we used the aforementioned guidelines to extract the184\npossible treatments for each pathology and then to classify each treatment as either essential (e.g. antibiotics, support)185\nor case specific (e.g. appendectomy, cholecystectomy, drainage). For each patient, we then determined if the case186\nspecific treatment was appropriate by matching against the actual operations performed on that patient, read from the187\nMIMIC-CDM dataset. We evaluate a model’s treatment recommendation only when it correctly diagnosed a patient188\nsince an inaccurate diagnosis likely leads to inappropriate treatment. Fig. 5 shows the total counts of each treatment in189\nMIMIC-CDM under each treatment name and the counts of each models’ correct diagnoses under each bar. We find190\nthat the LLMs consistently do not recommend appropriate and sufficient treatment, especially for patients with more191\nsevere forms of the pathologies. While they are consistent in recommending some treatments such as appendectomy192\nfor appendicitis (Chat: 97.5%, OASST: 97.5%, WizardLM: 96.9%) and antibiotics for diverticulitis (Chat: 88.6%,193\nOASST: 97.4%, WizardLM: 86.0%), they rarely recommend other treatments when appropriate such as colectomy for194\ndiverticulitis patients with perforated colons (Chat: 22.2%, OASST: 0.0%, WizardLM: 10.0%) or drainage of infected195\npancreatic necrosis (Chat: 0.0%, OASST: 0.0%, WizardLM: 0.0%) and abscesses near the diverticula (Chat: 16.7%,196\nOASST: 22.2%, WizardLM: 0.0%). Furthermore, they drastically under-treat appendicitis with regards to the necessity197\nof antibiotics (Chat: 8.5%, OASST: 26.0%, WizardLM: 6.0%) and providing support (Chat: 4.3%, OASST: 9.9%,198\nWizardLM: 2.7%), diverticulitis with the need for a colonoscopy in the future to check for colon cancer (Chat: 4.5%,199\nOASST: 18.8%, WizardLM: 9.6%), and pancreatitis with sufficient support (where we expect mentions of fluids, pain200\nmanagement, and monitoring) (Chat: 39.8%, OASST: 71.3%, WizardLM: 42.9%). In summary, following the treatment201\nrecommendations of the models would negatively impact the health of patients, particularly those with more advanced202\nstages of disease where indications for emergency operations were ignored.203\nAntibiotics\n(n=957)\nAppendectomy\n(n=808)\nSupport\n(n=957)\n0\n20\n40\n60\n80\n100Treatment Requested (%)\n714 603 714785 669 785750 635 750\n8.5\n97.5\n4.3\n26.0\n97.5\n9.96.0\n96.9\n2.7\nAppendicitis Treatment\nAntibiotics\n(n=648)\nCholecystectomy\n(n=465)\nSupport\n(n=648)\n219 162 219311 228 311307 235 307\n37.9\n84.0\n25.6\n79.4 83.8\n65.0\n32.6\n92.3\n32.6\nCholecystitis Treatment\nAntibiotics\n(n=257)\nColectomy\n(n=39)\nFuture\nColonoscopy\n(n=257)\nDrainage\n(n=33)\nSupport\n(n=257)\n0\n20\n40\n60\n80\n100Treatment Requested (%)\n88 9 88 6 88117 12 117 9 117114 10 114 11 114\n88.6\n22.2\n4.5\n16.7\n58.0\n97.4\n0.0\n18.8 22.2\n60.7\n86.0\n10.0 9.6\n0.0\n49.1\nDiverticulitis Treatment\nCholecystectomy\n(n=103)\nDrainage\n(n=17)\nERCP\n(n=195)\nSupport\n(n=538)\n30 8 77 21143 8 81 23738 7 92 245\n13.3\n0.0\n33.8\n39.8\n20.9\n0.0\n49.4\n71.3\n21.1\n0.0\n28.3\n42.9\nPancreatitis Treatment\nLlama 2 Chat OASST WizardLM\nFigure 5: We find that LLMs do not consistently recommend essential and patient-specific treatment, especially for\npatients with more severe forms of the pathologies. We only score models on the subset of patients that they correctly\ndiagnosed and that actually received a specific treatment. For example, of the 957 appendicitis patients, 808 received an\nappendectomy (indicated below the treatment name). Of those 808 patients, Llama 2 Chat correctly diagnosed 603\n(indicated below the Llama 2 Chat bar). Of those 603 patients, Llama 2 Chat correctly recommended an appendectomy\n97.5% of the time.\n7\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nThe tendency of LLMs to not gather the information required by diagnostic guidelines before returning a final diagnosis204\nand treatment plan is particularly problematic considering their low overall diagnostic and treatment accuracy. Such205\nhasty decision making combined with their poor diagnoses and treatment recommendations pose a serious risk to the206\nhealth of patients without extensive clinician supervision and control.207\n2.4 LLMs require extensive clinician supervision to effectively integrate into clinical workflows208\nIn addition to consistently and safely arriving at the correct diagnosis and treatment plan, models must integrate into209\nestablished clinical workflows to be useful. Central to this is the ability to follow instructions and generate answers so210\nthey can be easily processed and used by other parts of the clinic without physician supervision. During the clinical211\ndecision making process, we provide clear instructions to the models as to how they should provide their requests and212\ndiagnosis, as well as which tools are available to them (see Materials Section 6.2). For example, diagnostic tools must213\nbe written in the ‘Action:’ field and desired tests must be specified in the ‘Action Input:’ field, and not in the middle of214\na paragraph surrounded by other text. This is essential to ensure that the desired tests can be consistently extracted215\nso no manual clinician supervision and interpretation is required. Through extensive comparisons of LLM outputs216\nwith dictionaries of known exams and their synonyms, we go to great lengths to understand what tests are requested,217\neven if the models do not follow our schema, recording every time they fail to follow instructions. We investigate the218\ncapabilities of models to follow our instructions at three timepoints during our analysis: when providing the next action219\nto take, when requesting a tool, and when providing a diagnosis.220\nAll models struggle to follow the provided instructions (Fig. 6), making errors every two to four patients when providing221\nactions (Chat: 1.91, OASST: 3.90, WizardLM: 2.13) and hallucinating non-existent tools every two (Chat: 2.20 and222\nWizardLM: 2.41) to five patients (OASST: 5.48). When providing diagnoses on the MIMIC-CDM dataset, errors are223\nmade every three to five patients (Chat: 3.18, OASST: 4.55, WizardLM: 3.64) while on the MIMIC-CDM-FI dataset224\nthere is a greater discrepancy in the number of patients diagnosed without formatting error (Chat: 1.12, OASST: 6.60,225\nWizardLM: 28.21). While many of these errors are easily caught (Supplementary Table 6), the error rate is so high that226\nextensive manual controls would be necessary to ensure model output is being correctly interpreted. Such poor abilities227\nto follow instructions greatly reduces their usefulness in clinical systems as they would require large amounts of manual228\nsupervision to ensure proper performance.229\nMIMIC-CDM\nNext Action Error\nMIMIC-CDM\nT ool Hallucination\nMIMIC-CDM\nDiagnosis Error\nMIMIC-CDM-FI\nDiagnosis Error\n0\n5\n10\n15\n20\n25\n30\nAverage Number of Patients\nUntil Formatting Error1.91 2.20 3.18\n1.12\n3.90 5.48 4.55\n6.60\n2.13 2.41 3.64\n28.21\nLlama 2 Chat OASST WizardLM\nFigure 6: LLMs struggle to follow instructions, often introducing errors when providing the next action to take and\nhallucinating non-existent tools, up to once every two patients. Formatting errors while providing the diagnosis also\nregularly occur. In the clinic, extensive manual supervision would be required to ensure proper performance.\nAnother key component that must be fulfilled before we consider integrating such models into real world workflows230\nis robustness. Models must not be sensitive to small changes in user instructions as their performance will then vary231\ngreatly based on who is interacting with them. On the MIMIC-CDM-FI dataset, we find that changes in instructions232\n(Supplementary B) can lead to large changes (both positive and negative) in diagnostic accuracy (Fig. 7). For example,233\nlarge changes are seen when removing system and user instructions (up to +5.1% for Chat on cholecystitis, down to234\n-16.0% for Chat on pancreatitis), or when removing all medical terminology from the system instruction (up to +6.2235\nfor WizardLM on diverticulitis, down to -3.5% for OASST on pancreatitis). Additionally, we see that even minor236\n8\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nchanges in instructions can greatly change diagnostic accuracy such as asking for the ‘Main Diagnosis:’ (up to +7.0%237\nfor Chat on diverticulitis, down to -10.6% for WizardLM on cholecystitis) or ‘Primary Diagnosis:’ (up +8.7% for Chat238\non Pancreatitis, down to -5.2% for WizardLM on cholecystitis) instead of ‘Final Diagnosis:’. Models should be able to239\nprovide the most appropriate diagnosis given the situation, in this case the reason for the patient’s abdominal pain, and240\nnot be sensitive to minute changes in phrasing so as not to require extensive clinician training before use.241\nLlama 2 Chat\nOASST\nWizardLM\nModel\nNo System Or User Instruction No Medical T erminology No System Instruction Minimal System Instruction\n10\n 0 10\nChange in Diagnostic\nAccuracy (%)\nLlama 2 Chat\nOASST\nWizardLM\nModel\nDiagnosis\n10\n 0 10\nChange in Diagnostic\nAccuracy (%)\nMain Diagnosis\n10\n 0 10\nChange in Diagnostic\nAccuracy (%)\nPrimary Diagnosis\n10\n 0 10\nChange in Diagnostic\nAccuracy (%)\nMost Serious Final Diagnosis\nAppendicitis Cholecystitis Diverticulitis Pancreatitis Mean\nFigure 7: Often small changes in instructions, such as changing final diagnosis to main diagnosis or primary diagnosis,\ngreatly affects the performance of the LLMs on the MIMIC-CDM-FI dataset. This would vary the quality of responses\nreceived depending on who is using the model.\nFurthermore, LLMs must perform better or equally well when provided with more information. Large amounts of242\ndata are gathered during a patient’s hospital stay, with the patients in our dataset averaging 150 laboratory tests and 3243\nimaging exams per admission. Models must be able to focus on the key facts of the case to make a diagnosis. We show244\nthat models perform worse when all diagnostic exams are provided, typically attaining their best performance when only245\na single exam is provided in addition to the history of present illness (Fig 8). Removing information greatly increases246\ndiagnostic accuracy, with cholecystitis diagnosis improving by 18.5% for the Chat and 16.5% for the WizardLM247\nmodels when only providing radiologist reports, and pancreatitis diagnosis improving by 21.6% (Chat), 9.5% (OASST),248\nand 8.6% (WizardLM) when only providing laboratory results. This greatly reduces the usefulness of such models249\nas they cannot simply be given all relevant information and be trusted to arrive at their best diagnosis. To optimize250\nmodel performance, clinicians would have to decide which diagnosis is most likely to effectively filter the information251\npresented. While such filtering by clinicians would increase their mean scores to a theoretical best of Chat: 72.2%,252\nOASST: 70.9%, WizardLM: 71.8%, it would remove any benefit of deploying such a clinical decision making model.253\nModels must also provide the same diagnosis, irrespective of the order in which the information is presented. We test254\nthe diagnostic consistency of the models on the MIMIC-CDM-FI dataset by switching the order of the information from255\nthe canonical physical examination, then laboratory tests, then imaging, to all possible permutations thereof (history of256\npresent illness is always included first). We show that all models have large ranges of performance, up to 18.0% (Chat -257\nPancreatitis), 7.9% (OASST - Cholecystitis), and 5.4% (WizardLM - Cholecystitis) (Fig. 9 and Supplementary Fig. 16).258\nImportantly, we find that the order of information that delivers the best performance for each model is different for259\neach pathology (Supplementary Section I). This again reduces the benefits of deploying the models as clinicians must260\nconstantly consider and monitor in which order they provide the models with information, in a disease specific manner,261\nto not degrade performance.262\nIn summary, extensive clinician supervision and prior evaluation of the most probable diagnosis would be required263\nto ensure proper functioning of LLMs because they do not reliably follow instructions, perform better with a disease264\ndependant order of information and degrade in performance when given relevant information. Furthermore, their265\nsensitivity to small changes in instructions that seem inconsequential to humans would require extensive clinician266\ntraining to ensure good performance.267\n9\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nAppendicitisCholecystitisDiverticulitisPancreatitis\nMean\n0\n20\n40\n60\n80\n100Diagnostic Accuracy (%)\n+18.5\n+13.6\n+21.6 +13.4\nLlama 2 Chat\nAppendicitisCholecystitisDiverticulitisPancreatitis\nMean\n+3.1\n+9.5 +3.1\nOASST\nAppendicitisCholecystitisDiverticulitisPancreatitis\nMean\n+0.7\n+16.4 +1.2\n+8.6\n+6.7\nWizardLM\nHistory of Present Illness (HPI)\nHPI + Physical Examination\nHPI + Laboratory T ests\nHPI + Imaging\nAll Information\nTheoretical Best\nFigure 8: For almost all diseases, providing all information does not lead to the best performance on the MIMIC-\nCDM-FI dataset. This suggests that LLMs cannot focus on the key facts and degrade in performance when too much\ninformation is provided. This is a problem in the clinic where an abundance of information is typically gathered to\nwholistically understand the patients health and being able to focus on key facts is an essential skill. The theoretical\nbest shows the mean performance if a clinician were to select the best diagnostic test for each pathology.\nLlama 2 Chat OASST WizardLM\nModel\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0Change in Diagnostic Accuracy (%)\n0.8 0.7 0.8\n9.3\n7.9\n5.45.4\n7.0\n2.7\n18.0\n1.7\n5.0\n8.4\n4.3 3.5\nChanging the Order of Information Changes Diagnostic Accuracy\nAppendicitis Cholecystitis Diverticulitis Pancreatitis Mean\nFigure 9: By mixing the order in which information is presented to LLMs, their performance changes despite the\ninformation included staying the same. This places an unnecessary burden upon clinicians who would need to make\npreliminary diagnoses to decide the order in which they feed the models with information for best performance.\n2.5 Summarization and filtering for abnormal laboratory results partially mitigates limitations of current268\nLLMs269\nTo help address some of the limitations found in this analysis, we explore simple modifications that can be done without270\nretraining the model. One major limitation is that LLMs are currently limited in the amount of text they can read, with271\nall models tested in this study having a limit of 4096 tokens or approximately 2400 words. Due to this, if the history272\nof present illness or radiologist reports are disproportionately long, the limit of the model is sometimes reached. To273\nalleviate this, we implement an automatic summarization protocol that monitors the length of the text that the model274\nreceives. Once it approaches its maximum token amount, we ask the model to summarize the test results it has received,275\ncombining the healthy observations and emphasizing the pathological indications (Supplementary B.2). Removing such276\na summarization protocol resulted in marginal but consistent losses on the mean of -1.3% (Chat), -0.8% (OASST), and277\n10\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \n-0.5% (WizardLM), and particularly hurt the diagnosis of diverticulitis -4.7% (Chat), -2.7% (OASST), -3.5% (Wizard)278\n(Supplementary Fig. 17).279\nDue to the inability of LLMs to reliably interpret laboratory results (Supplementary Fig. 15), even when provided280\nwith reference ranges, and their issues understanding larger quantities of information (Fig. 8), we find that filtering281\nthe laboratory results and removing all normal test results generally improved performance on the MIMIC-CDM-FI282\nDataset (Fig. 10). This increases the amount of other information that can be included before reaching the input length283\nlimit of the LLMs and reduces the amount of healthy information, which tends to confuse the tested models. Removing284\nthis information increased Llama 2 Chat and WizardLM mean performance by 6.0% and 3.9% respectively, while285\nslightly changing mean OASST diagnostic accuracy by -0.9%. On the MIMIC-CDM-FI dataset, cholecystitis patients286\nbenefited the most from such a filter, with all models increasing in diagnostic accuracy (Chat: +16.8%, OASST: +3.5%,287\nWizardLM: +14.8%). Pancreatitis patients in MIMIC-CDM-FI on the other-hand saw a light decrease in performance288\nfor the OASST (-6.1%) and WizardLM (-4.1%) models, with a slight increase for Llama 2 Chat (+2.0%). As many of289\nour other analyses examine the general behaviour of laboratory tests and their impact on model performance, we do290\nnot use this fix for any other sections of this work. While this filtering improves the performance of the LLMs as they291\nfunction today, ideally a model would perform best with all available information. Healthy laboratory test results are an292\nimportant source of information for clinicians and should not degrade model performance.293\n5\n 0 5 10 15\nChange in Diagnostic Accuracy (%)\nLlama 2 Chat\nOASST\nWizardLM\nModel\n-0.2\n0.2\n0.5\n16.8\n3.5\n14.8\n5.4\n-1.2\n4.3\n2.0\n-6.1\n-4.1\n6.0\n-0.9\n3.9\nEffect of Including Only Abnormal Laboratory Results on CDM-FI Diagnostic Accuracy\nAppendicitis Cholecystitis Diverticulitis Pancreatitis Mean\nFigure 10: Filtering to include only abnormal laboratory results using the laboratory reference ranges provided in\nMIMIC-IV database generally improves model performance, especially for the cholecystitis pathology. This allows the\nmodel to focus more on abnormal, pathological signals.\n3 Discussion294\nThe strong performance of LLMs on medical licensing exams has led to increased interest in using them in clinical295\ndecision making scenarios involving real patients. However, medical licensing exams do not test the capabilities296\nrequired for real world clinical decision making. We have, for the first time, evaluated leading open access large297\nlanguage models on making open-ended clinical decisions with thousands of real world patient cases to assess their298\npotential benefits and possible harms. By not only comparing their diagnostic performance against clinicians, but also299\ntesting their information gathering abilities, adherence to guidelines, and instruction following capabilities as well as300\ntheir robustness to changes in prompts, information order, and information quantity, we move for the first time beyond301\nsimple evaluations of diagnostic accuracy and establish a range of characteristics that are necessary for safe and robust302\nintegration into the clinic. In this work, we have shown that current leading LLMs are unsuitable for clinical decision303\nmaking on all of these accounts.304\n11\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nAmong the models tested in this study, we find that OASST performed best overall as it had decent diagnostic accuracy,305\ngenerally requested appropriate laboratory exams, and was most robust to changes in information quantity. Llama 2 Chat306\nhad the worst overall diagnostic accuracy, often refused to follow instructions, and was heavily influenced by the order307\nand quantity of information, but was the only model to consistently ask for a physical examination. WizardLM was308\nthe most robust to changes in the order of diagnostic exams and followed instructions well when returning diagnoses,309\nbut was the worst at following diagnostic guidelines, failing to consistently order physical examinations and necessary310\nlaboratory tests. Despite the performance of OASST being generally better than Chat and WizardLM across the diverse311\nset of analyses included in this study, it is still not currently suitable for clinical use due to its inferior performance312\ncompared to clinicians, broad failure to order correct treatments, and general lack of robustness. While one of the313\nmedical domain models, Clinical Camel, achieved the highest diagnostic accuracy (mean=73% vs OASST mean=68%,314\nSupplementary Fig. 13), its inability to participate in the iterative clinical decision making task precluded it from315\nevaluations of its robustness and consistency which we believe to be essential to ensure safe deployment in the clinic.316\nOther LLMs such as ChatGPT, GPT-4, Med-PALM, and Med-PALM 2 could not be tested due to the data privacy and317\nusage agreements of MIMIC-IV , highlighting the risk of using corporate models in a sensitive area such as medicine,318\nwhere patient privacy, transparency, and reliability are essential[57, 24].319\nThe biggest barrier to using current LLMs in clinical practice is that no model consistently reached the diagnostic320\naccuracy of clinicians across all pathologies, with a further decrease in accuracy when having to gathering diagnostic321\ninformation themselves. A major weakness of these models is that their diagnostic accuracy is disease dependent,322\nwith markedly higher accuracy on appendicitis compared to cholecystitis, diverticulitis, and pancreatitis. A clinical323\nmodel must diagnose well on all major differential diagnoses of a chief complaint, such as abdominal pain, to be useful.324\nAdditionally, LLMs are unable to classify a lab result as normal or abnormal, even when provided with its reference325\nrange. This is underscored by the fact that presenting the model with only abnormal laboratory results generally326\nimproved diagnostic performance.327\nWe further found that the models do not follow diagnostic guidelines, which is particularly problematic considering328\ntheir low overall diagnostic accuracy, indicating a tendency to diagnose before fully understanding a patient’s case.329\nInsufficient diagnostic information also negatively affected the treatment recommendations of LLMs, where we showed330\nthat models do not follow all established treatment guidelines, especially for severe cases. The hasty decision making of331\nthe models combined with their low diagnostic performance and poor treatment recommendations pose a serious risk to332\nthe health of patients without extensive clinician supervision and control.333\nBeyond diagnostic accuracy, we extensively test models on their reliability and robustness which are essential character-334\nistics to ensure consistent and safe patient care. We found that models struggle to follow instructions, often hallucinating335\nnon-existent tools and requiring continuous manual supervision to ensure proper performance. Models are also sensitive336\nto seemingly inconsequential changes in instruction phrasing, requiring clinicians to carefully monitor the language337\nthey use to interact with the models to not degrade performance. Contrary to expectation, LLMs diagnose best when338\nonly a single diagnostic exam is provided rather than when given all relevant diagnostic information, demonstrating an339\ninability to extract the most important diagnostic signal from the evidence. Counterintuitively, we found models to340\nbe sensitive to the order in which information is presented, resulting in large changes in diagnostic accuracy despite341\nidentical diagnostic information. Importantly, all of these weaknesses are disease-specific within each model, meaning342\nthat a different instruction, diagnostic test, and order of tests achieved best results for each pathology. As physicians343\nwould have to perform preliminary diagnostic evaluations in an attempt to maximize model performance according to344\ntheir suspected diagnosis, all benefits of a clinical decision making system would be lost.345\nMany of the current limitations of LLMs exposed in our study have been shown concurrently in domains outside of346\nmedicine. It has been shown that LLM performance on tasks can vary by between 8% and 50% just by optimizing the347\ninstructions[64]. The sensitivity of LLMs to the order of presented information has been well documented on multiple348\nchoice questions [66, 43] and information retrieval[33]. The difficulty LLMs have in interpreting numbers[ 51] and349\nsolving simple arithmetic[17] is an active research topic[23, 52]. Even the largest models currently available, PaLM 2350\nand GPT4, perform poorly on instruction following tests[67]. Our analysis demonstrates how these current limitations351\nof LLMs become harmful in medical contexts where robustness and consistency are essential. We argue that these352\nunderstudied aspects of model performance should become normal parts of medical model evaluations and that all of353\nthese issues must be addressed before LLMs can be considered for clinical decision making.354\n4 Limitations355\nWhile we have been able to demonstrate the limitations of current leading large language models for clinical decision356\nmaking, we consider the following limitations of our study.357\n12\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nFirst, as we are using a dataset of real-world clinical data, we rely on the accuracy of the discharge diagnosis written by358\nthe attending clinicians. Since a diagnosis using our dataset must be made with available information and without direct359\naccess to the patient, the task of coming to the exact diagnosis of the attending physician is a difficult one. We can360\nonly provide the information that was gathered during the patient’s hospital stay, so model requests for information361\nnot in the MIMIC-IV database must be denied. However, as the MIMIC-IV database contains all data gathered during362\na patient’s hospital stay, we can assume that all information required for a diagnosis and treatment plan is contained363\nwithin our data considering the attending physicians successfully diagnosed and treated all patients. Furthermore, being364\nflexible enough to handle acute restrictions, such as unavailable imaging modalities or laboratory tests, and still come to365\na correct diagnosis is a desirable ability for any real-world clinical AI application. Due to this difficulty, we were lenient366\nin our evaluation of the diagnoses, accepting alternative names for the pathologies, as long as they were medically367\ncorrect (see Supplementary Table 5).368\nAnother issue concerns the handling of data gathered multiple times during a patient’s hospital stay, such as laboratory369\ndata. As we test LLMs in an emergency-department-centered clinical decision making scenario, we only provide the370\nmodels with the first collected value for each laboratory test as they are likely most indicative for diagnostic purposes371\nrather than treatment monitoring. Thus, we currently do not capture the changes in laboratory test values if the patient’s372\ncondition deteriorates over their hospital stay. This could be remedied by examining all time points and determining the373\nmost abnormal test result to be returned or by allowing multiple requests for laboratory tests to return successive test374\nresults. However, both of these approaches would widen the temporal gap of provided test results, possibly providing375\nconflicting diagnostic signals. Considering LLMs also have poor temporal reasoning capabilities[62], simply including376\nthe timestamp would most likely not be an adequate solution.377\nOur comparison between models and clinicians included only three doctors in residency from Germany and one senior378\nhospitalist from the United States. Increasing the diversity and number of clinicians as well as the number of patient379\ncases evaluated would give a more nuanced view of model performance compared to practicing hospitalists. Future380\nmodels could possibly soon reach or even outperform clinicians in residency and thus provide a low-cost, interactive381\nsecond opinion to consult, as is already the case for AI models in other areas such as mammography screening [35].382\nLastly, we only examined the initial complaint of abdominal pain and four related diagnostic endpoints in our study.383\nWhile these pathologies are clinically important and well suited for our analysis, it will be important to test future384\nmodels on both additional diagnostic endpoints and a broader range of initial complaints. A clinical decision making385\nmodel must show strong performance across all possible pathologies of a particular initial complaint to guarantee386\nadequate patient care without extensive preliminary diagnoses by clinicians. As each initial complaint has its own387\nset of relevant diagnostic exams and is often best investigated with complaint-dependant exams, model performance388\ncould vary greatly. We believe the MIMIC-IV database to be a rich resource to help create such additional datasets389\nconsidering its unfiltered inclusion of all patients that presented to the emergency room.390\n5 Conclusion391\nIn conclusion, our study presents the first analysis of the capabilities of current state-of-the-art large language models392\non real-world data in a realistic clinical decision making scenario. Our main finding is that current models do not393\nachieve satisfactory diagnostic accuracy, performing significantly worse than trained physicians, and do not follow394\ntreatment guidelines, thus posing a serious risk to the health of patients. This is exacerbated by the fact that they do395\nnot request the necessary exams for a safe differential diagnosis, as required by diagnostic guidelines, indicating a396\ntendency to diagnose before fully understanding a patient’s case. Furthermore, we show LLMs are distracted by relevant397\ndiagnostic information, are sensitive to the order of diagnostic tests, and struggle to follow instructions, prohibiting398\ntheir autonomous deployment in the clinic and requiring extensive clinician supervision.399\nBy sourcing our dataset from real clinical cases and closely aligning our evaluation criteria with official diagnostic and400\ntreatment guidelines, we present the first analysis to help physicians understand how well LLMs would perform in the401\nclinic today. While our findings cast doubt on the suitability of LLMs for clinical decision making as they currently402\nexist, we believe there lies great potential in their use after the issues raised are resolved. By making our dataset403\nand framework freely available we hope to guide the development of the next generation of clinical AI models and404\ncontribute towards a future where physicians can feel confident in using safe and robust models to improve patient405\noutcomes.406\n13\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \n6 Methods407\n6.1 MIMIC-CDM Dataset408\nWe created our curated dataset of 2400 patients which we call MIMIC-Clinical Decision Making (MIMIC-CDM),409\nusing the MIMIC-IV Database [22]. The MIMIC-IV Database is a comprehensive, publicly available database of the410\nde-identified electronic health records of almost 300,000 patients that presented to the Beth Israel Deaconess Medical411\nCenter (BIDMC) in Boston, Massachusetts from 2008 to 2019. It contains real patient cases from the hospital and412\nincludes all recorded measurements such as laboratory and microbiology test results, diagnoses, procedures, treatments413\nand free-text clinical notes such as discharge summaries and radiologist reports.414\nIn this work, we focus on four target pathologies for which we filter: appendicitis, cholecystitis, diverticulitis, and415\npancreatitis. As we are only testing for these pathologies, we must ensure that they are the primary diagnosis and reason416\nfor the patient presenting to the emergency department and not merely a secondary diagnosis during a longer and more417\nserious hospital admission. Thus, we first filtered patients for our targets using the diagnosis table which contains all418\nrecorded diagnostic International Statistical Classification of Diseases and Related Health Problems (ICD) codes. Then,419\nwe manually checked the discharge diagnosis of each patient’s discharge summary and only included those patients420\nwhose very first primary diagnosis was one of our pathologies. If any other diagnosis was written in the discharge421\ndiagnosis before one of our targets, the patient’s case was removed from the dataset. If a patient was diagnosed with422\nmore than one of the four pathologies included in our analysis, the patient was removed from the dataset.423\nAfter filtering for the appropriate pathologies, we split the discharge summary into its individual sections, extracting the424\nhistory of present illness and physical examination. First, we removed all patients where the pathology was mentioned in425\nthe history of present illness as these admissions were mostly transfers where the diagnosis had already been established426\nand the hospital admission data was thus missing the initial emergency-department test results relevant for diagnostic427\npurposes. Furthermore, we removed all patients where no physical examination was included as this is a crucial source428\nof information according to the diagnostic guidelines of each pathology.429\nWe gathered all laboratory and microbiology tests recorded during a patient’s hospital admission and those up to one430\nday before admission. We included tests up to one day before admission as the MIMIC-IV documentation states that431\nthere are millions of laboratory tests that are not associated with any hospital admission but can be joined to patient432\nstays using the patient’s id, recorded test time, and hospital admission time. The tests before the official start of the433\nadmission often had highly relevant values for diagnostic purposes and were thus included, though only if they were434\nnot associated with any other hospital admission. If a laboratory test was recorded multiple times, we included only435\nthe first entry in our dataset to simulate a therapy-naive diagnostic clinical-decision-making scenario. Furthermore,436\nwe saved all reference ranges of the laboratory tests provided by MIMIC and established a comprehensive dictionary437\nmapping possible synonyms and abbreviations of tests to their original entry to be able to interpret all requests of the438\nLLMs for test results. This dictionary of synonyms was constantly expanded during initial testing of the models until439\nno unrecognized tests were recorded. The dictionary also contains common laboratory test panel names that map to a440\nlist of the individual tests of that panel, such as complete blood count, basic metabolic panel, liver function panel, renal441\nfunction panel, and urinalysis, among others.442\nSimilar to the laboratory data, many radiology reports were not associated with any hospital admission but their443\ntimestamp was a few hours before the recorded start of the hospital admission. These often contained diagnosis-relevant444\ninformation and so we used the same 24 hour inclusion criteria as for the laboratory results and again allowed only those445\nexams not associated with any other hospital admission. Next, we established a list of uniquely identifying keywords446\nfor each anatomical region and imaging modality. We used this list of keywords to determine the region and modality447\nof each included report from its MIMIC-IV provided exam name. Mappings were made for special exams such as CTU448\nto CT and MRCP to MRI, to provide them if, for example, a CT scan or MRI was requested, due to their low frequency.449\nWe also used this list when interpreting the model requests for imaging information during evaluation. We manually450\nchecked and adjusted the keywords until all reports in MIMIC-IV were correctly classified. Radiology reports were451\nsplit into report sections and only the findings section was included. This was done as many other sections such as452\nconclusions or impressions contained the diagnosis of the radiologist, which would have made the task trivial.453\nFinally, all procedures and operations performed during a patient’s hospital stay were saved to understand which454\npatient-specific treatments were undertaken. The procedures in the MIMIC-IV procedures table saved as ICD9 and455\nICD10 codes were extracted and combined with the free-text procedures section from each patient’s discharge summary.456\nThe free-text extraction from the discharge summary was required as many essential procedures, including surgeries,457\nwere often not included in the procedures table.458\nA final round of data cleaning replaced any remaining mentions of the primary diagnosis with three underscores ‘___’459\nwhich is used by MIMIC-IV to censor data such as a patient’s name or age. To increase data quality we excluded460\n14\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \npatients that had no associated laboratory tests or for which no abdominal imaging was recorded. The final dataset,461\nMIMIC-CDM, contains 2400 unique patients presenting to the emergency department with one of the four target462\npathologies (957 appendicitis, 648 cholecystitis, 257 diverticulitis, 538 pancreatitis) and whose makeup is detailed in463\nFigure 1a. The dataset contains physical examinations for all patients (2400), 138,788 laboratory results from 480464\nunique laboratory tests and 4403 microbiology results from 74 unique tests. Furthermore, MIMIC-CDM contains 5959465\nradiology reports, including 1836 abdominal CTs, 1728 chest x-rays, 1325 abdominal ultrasounds, 342 abdominal466\nx-rays and 227 MRCPs. Finally, there were 395 unique procedures recorded over all patients, with a total of 2917 ICD467\nprocedures plus the 2400 free text procedures specified in the discharge summaries. Supplementary Table 2 shows the468\nage, sex and race statistics of the patients in the dataset split up by pathology. As the reports provided were de-identified,469\nthe models did not have access to any of these characteristics during evaluation.470\nA second version of the dataset, which we call MIMIC-CDM-Full Information (MIMIC-CDM-FI), combines all the471\ninformation required for diagnosing each case and presents it all at once. Here we include the history of present illness,472\nphysical examination, all abdominal imaging and all laboratory data helpful for both reaching the correct diagnosis and473\nruling out differential diagnoses. To determine which laboratory data to include, we used the diagnostic guidelines of474\neach disease: appendicitis[16], cholecystitis[44], diverticulitis[25], and pancreatitis[32]. The specific tests included in475\neach category can be found in Supplementary Table 4. The information is presented in the order: history of present476\nillness, physical examination, laboratory results, imaging. The imaging is ordered by chart time from earliest to latest.477\nA subset of 80 representative patients of the MIMIC-CDM-FI dataset was randomly selected to be used for comparison478\nwith physicians. The subset is evenly split between the four target pathologies with 20 patients randomly selected from479\neach pathology and matching the makeup of the full dataset, as shown in Supplementary Table 3. For the physicians,480\nthe data was prepared as a PDF and the information was provided exactly in the same order and quantity as for the481\nmodels. Reference ranges were included when provided by MIMIC-IV . The abbreviations in the history of present482\nillness and physical examination were replaced with unabbreviated text for the German doctors, as they were unfamiliar483\nwith US-specific abbreviations. The models performed worse with unabbreviated text (Supplementary Fig. 14). The484\nlaboratory data were provided as a table in the PDF to increase readability. To mitigate the risk of physicians recognizing485\nthe pattern of four distinct target pathologies, a further five patients each of patients presenting with gastritis, urinary486\ntract infaction, esophageal reflux and hernia were included. Thus, the final dataset used in the reader study spanned 100487\npatients, 80 of which are part of MIMIC-CDM.488\n6.2 Evaluation Framework489\nTo realistically test the capabilities of LLMs on the task of clinical decision making, we simulated a clinical environment490\nin which a patient presents to the emergency department with acute abdominal pain and information must be iteratively491\ngathered before a final diagnosis is made. The LLM is tasked with the Clinical Decision Making (CDM) task492\n(Supplementary Materials B.1) which instructs it to consider a patient’s symptoms and gather information to come493\nto a diagnosis and treatment plan while also explaining the two formats it should answer with. Both formats ask the494\nLLM to first think (i.e. consider the evidence, which has been shown to improve the quality of reasoning and future495\nactions[65]), and then either request more information or provide a diagnosis and treatment plan. If it chooses to496\nrequest more information, it must state ‘Action:’ followed by either ‘Physical Examination’, ‘Laboratory Test’, or497\n‘Imaging’. Additionally, it must provide an ‘Action Input’ which specifies what information is desired from the action498\n(i.e. ‘Complete Blood Count’ or ‘Abdominal Ultrasound’). The ‘Action Input’ field is ignored if a physical examination499\nis requested. The second format is to be used when the model decides enough information has been gathered for a500\ndiagnosis, and asks the model to consider the evidence one last time and then provide a final diagnosis and treatment501\nplan.502\nThe model is initially presented with these instructions and the history of present illness of the patient and then prompted503\nto record its ‘Thoughts:’, thus beginning the clinical decision making task. Outputs are generated until either a stop504\ntoken is reached or the model generates the ‘Observation(s):’ phrase, indicating that it has reached the end of its action505\nrequest and would potentially start hallucinating the result of its request. We stop model text generation here and then506\nexamine the response of the model, extracting which action was desired and what the input to that action is. If the model507\ndoes not follow the instructions and, for example, writes ‘Perform a physical examination’ instead of ‘Action: Physical508\nExamination’, we still provide the appropriate information but record every instance of it not following instructions for509\nour evaluations (see Figure 6). We call these errors ‘Next Action Errors’.510\nIf the requested information is available for that patient case, we return it and prompt the model again to consider511\nthe evidence. If the information is not available, we inform the model and ask for an alternative action. We return512\nonly the laboratory tests and radiologist reports that were specifically requested. Laboratory tests are compared to the513\npreviously mentioned dictionary of available tests to return the best match. If no match is found, ‘N/A’ is returned.514\nRequests for imaging have the exam modality and anatomical region extracted using the aforementioned keyword lists515\n15\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \n(see Materials Section 6.1) and used to match against those saved for that patient in MIMIC-CDM. If multiple reports516\nexist for a modality and region combination, we return the first report chronologically. The next request for an imaging517\nexamination of that modality and region will return the next report chronologically. Once there are no reports left518\nto return, we inform the model that we can no longer provide reports of that modality and region combination. If a519\nphysical examination was requested, we return the entire physical examination regardless of any specifications made in520\nthe ‘action input’ field. We do this because we consider it best practice to perform a complete physical examination of a521\npatient rather than only partially and reliably separating a physical examination report into its parts is difficult due to522\ntheir free-form and heavily abbreviated style. If an invalid tool is requested (‘hallucinated’), we state that the tool does523\nnot exist and remind it which tools are available, or that it should make a diagnosis and provide a treatment plan. These524\noccasions are also recorded as tool hallucinations for our evaluations (see Figure 6). An example exchange between an525\nLLM and our framework can be seen in Figure 1b and Supplementary Section C.526\nWe repeat this process, prompting the LLM to think and in turn receiving requests for information. Once the model527\ndecides that it has gathered sufficient information, it outputs its final diagnosis and treatment plan, ending the clinical528\ndecision making task for that patient. The final diagnosis is then evaluated to see if it contains the recorded pathology529\nof the patient. In addition to a direct match of the pathology name (i.e. appendicitis, cholecystitis, diverticulitis,530\npancreatitis), we allow for a range of alternative phrasings as long as they are medically correct (see Supplementary531\nTable 5). If multiple diagnoses are given, we only examine the first diagnosis mentioned. This is how we calculate the532\ndiagnostic accuracy for all analyses. A new instance of the task is then started for the next patient.533\nAs LLMs can only take a limited amount of words as input, we monitor the length of the conversation and if we534\napproach the input limit of the model, we ask it to summarize the information it has received so far to reduce the length535\nof the conversation (Supplementary Materials B.2). We first summarize each gathered piece of information individually,536\nleaving the initial history of present illness and instructions untouched. As LLMs have no memory and interpret each537\nrequest independently, we replace the original pieces of information with the summaries. If we have summarized all538\nsteps of the interaction and still approach the limit of the model, we force a generation of diagnosis and treatment plan.539\nFor the MIMIC-CDM-FI dataset we instruct the model to consider the facts of the case and then provide a diagnosis and540\nonly a diagnosis (Supplementary Materials B.3). As explained in Materials Section 6.1, the MIMIC-CDM-FI dataset541\nincludes the history of present illness, physical examination, all relevant laboratory results and every radiologist report542\nwhere the abdominal region was inspected. Radiologist reports of other regions were not included due to the input543\nlength limits of the models. If including all of this information exceeds the input length of the model, we ask the LLM544\nto summarize each radiologist report individually. If the input length is still exceeded, we ask the LLM to summarize545\nall imaging information at once. In the rare cases where the input length continues to be exceeded, we remove words546\nfrom the final imaging summary until there is enough space for a diagnosis (i.e. 25 tokens or 20 words).547\n6.3 Models548\nAn overview of the models included and considered is given in Table 1.549\nWhen deciding which models to test, we started by only considering models with a context length of 4096 tokens due550\nto the large amounts of text contained within the MIMIC-CDM clinical cases. The context length defines how many551\ncombined tokens a large language model can read and write. For example, if a model has a context length of 2048 and552\nreceives an input of 2000 tokens, it can only generate 48 new tokens. A minimum length of 4096 is required, as the553\naverage number of tokens of relevant information per patient case in MIMIC-CDM-FI is 2080 tokens with a maximum554\ncount of 15,023 tokens. If one considers the extra tokens that are required for the information gathering back-and-forth555\nusing MIMIC-CDM data, this quickly exceeds the limit of 2048 tokens of smaller models (context length windows556\nalmost always differ in powers of 2).557\nNext, we considered which open access models performed best on medical reasoning tasks. To gauge model strength,558\nwe used the MedQA[27] dataset as it is comprised of 12,723 questions from the USMLE and is thus a good gauge of559\ngeneral medical knowledge contained within the model. As of the time of writing, Llama 2 is the leading open-access560\nmodel on the MedQA (USMLE) dataset, with the 70B model achieving a score of 58.4[12] exceeding that of GPT3.5561\nwhich scored only 53.6[38].562\nTo effectively complete the clinical decision making task without specific fine-tuning to the task and format, the model563\nmust be instruction fine-tuned. Instruction fine-tuning involves training a model to adapt to a wide range of new564\ntasks so that it can, with minimal instruction or example, complete an unseen task, like our clinical decision making565\nobjective. The most popular and performant instruction fine-tuned versions of Llama 2 are Llama 2 Chat, fine-tuned by566\nMeta themselves; WizardLM, fine-tuned by Microsoft using evolutionary algorithm (Evo-Instruct) generated training567\ndata; and OASST, fine-tuned using a crowd-sourced collection of 161,443 messages. Currently the only two existing568\nmedically fine-tuned versions of Llama 2 with a context length of 4096 and 70B parameters are Clinical Camel and569\n16\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nMeditron. Neither has been extensively instruction fine-tuned and thus they both generated nonsensical and repetitive570\nresponses to the clinical decision making objective using MIMIC-CDM data.571\nCurrently, the most popular and leading closed-source LLMs for medical question answering are Chat-GPT (MedQA:572\n∼53.6[38]), GPT-4 (MedQA: 90.2[39]), Med-PaLM (MedQA: 67.2[50]), and Med-PaLM 2 (MedQA: 86.5[50]). As573\npreviously stated, due to the signed data usage agreements of the MIMIC-IV database, the data cannot be sent to574\nexternal servers[19], precluding its use with closed-source models that are only accessible through an API and whose575\nmodels cannot be downloaded.576\nFurthermore, Chat-GPT is fine-tuned primarily through user conversations with the model, and since it is impossible577\nto know if portions of the MIMIC-IV database have already been used for queries by users less aware of the data578\nusage agreement[19], the data could already have been seen by the model during training, invalidating any results it579\nproduces. Little to no information is known about the training data of GPT4, giving rise to analogous concerns about580\nits performance. While the exact pre-training data of Llama 2 is also not known, Meta has stated that it only used581\n‘public available online data’, which strongly mitigates the risk of MIMIC-IV data having being used. Med-PaLM and582\nMed-PaLM 2 achieve strong scores on MedQA but the exact data used for training are unknown, the models are only583\naccessible through an API, and access to the models is currently unavailable for all researchers. Repeated requests for584\naccess were denied.585\nWe strongly agree with current sentiment that open source models must drive progress in the field of medical AI due to586\npatient privacy and safety concerns, corporate lack of transparency, and the danger of unreliable external providers[57].587\nIt is a serious risk to patient safety if key medical infrastructure is based on external company APIs and models whose588\nperformance could change erratically with updates and which could generally be deactivated for any reason.589\nFor each model tested, we downloaded and used the GPTQ quantized version from huggingface, the central repository590\nfor all LLM models. GPTQ quantization reduces the numerical precision of the weights while monitoring the generated591\noutput to reduce the GPU memory requirements of a model while not significantly degrading performance[20]. The592\nGPTQ parameters of the downloaded models were 4 bits, 32 group size, act order true, 0.1 damp% and 4096 sequence593\nlength. This gives the highest possible inference quality while reducing model size to around 40 GB which can fit onto594\na single A40 GPU. This reflects an economically realistic scenario of a single high-end GPU being used to host the595\nmodel to run the clinical decision making task. A fixed seed of 2023 and greedy decoding were used for all experiments596\nmaking all results deterministic and reproducible, except for the evaluation on the 80 patient subset for comparison with597\nclinicians where 10 different seeds were used for increased statistical power.598\n6.4 Data and Code Availability599\nThe dataset is available to all researchers who create an account on https://physionet.org/ and follow the steps to gain ac-600\ncess to the MIMIC-IV database (https://physionet.org/content/mimiciv/2.2/). Access is given after completing the \"CITI601\nData or Specimens Only Research\" training course. The data use agreement of physionet for \"credentialed health data\"602\nmust also be signed. The dataset can then be recreated using the code found at: https://github.com/paulhager/MIMIC-603\nClinical-Decision-Making-Dataset. The code to create the dataset uses python v3.10 and pandas v2.1.3.604\nThe publication of the dataset on the Physionet website for those with access to MIMIC is currently under review. Once605\nit has been accepted, anyone credentialed to access MIMIC will be able to download the data directly.606\nThe evaluation framework used for this study can be found at: https://github.com/paulhager/MIMIC-Clinical-Decision-607\nMaking-Framework. The analysis framework to evaluate all results, generate all plots and do all statistical analysis can608\nbe found at: https://github.com/paulhager/MIMIC-Clinical-Decision-Making-Analysis. All code uses python v3.10,609\npytorch v2.1.1, transformers v4.35.2, spacy v3.4.4, langchain v0.0.339, optimum v1.14, thefuzz v0.20, exllamav2610\nv0.0.8, nltk v3.8.1, negspacy v1.0.4, scispacy v0.5.2611\n6.5 Statistics612\nAll statistical tests were conducted using the Python programming language, version 3.10 and using the SciPy library.613\nComparisons of means were tested for statistical significance using two-sided Student’s t-tests with unequal variances614\n(tested through Bartlett’s tests). To account for multiple comparisons, p-values were Bonferroni corrected with a615\nmultiplier of 5 for the comparison of the doctors against the models and 3 for the comparison of the specialist and616\ngeneralist models.617\n17\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nModel Base Parameters Training Dataset Downloadable\nLlama 2 Chat[58] Llama 2[58] 70B Public data* ✓\nOASST[30] Llama 2[58] 70B Public data*\nOpen Source Data\n✓\nWizardLM[63] Llama 2[58] 70B Public data*\nEvol-Instruct Generated[63]\n✓\nClinical Camel[56] Llama 2[58] 70B Public data*\nShareGPT\nPubMed Articles (pre 2021)[56]\nMedQA[27]\n✓\nMeditron[12] Llama 2[58] 70B Public data*\nClinical Guidelines\nPublic PubMed Abstracts[12]\nPublic PubMed Papers[12]\nRedPajama[14]\n✓\nChat-GPT[41] GPT3.5[9] ??? User Conversations†\nCommon Crawl[47]\nWebText2[45]\nBooks1[29]\nBooks2[29]\nWikipedia\n✗\nGPT4[40] ??? ??? ??? ✗\nMed-PaLM[49] Flan-PaLM[13] 540B Webpages†\nWikipedia†\nSocial Media†\nGithub†\nNews Articles†\nBooks†\n473 Instruction fine-tuning\nDatasets[13]\nHealthSearchQA[49]\nMedicationQA[2]\nLiveQA[1]\n✗\nMed-PaLM 2[50] PaLM 2[4] 340B Web Documents†\nBooks†\nCode†\nMathematics†\nConversational Data†\nMedQA[27]\nHealthSearchQA[49]\nMedicationQA[2]\nLiveQA[1]\n✗\nTable 1: An overview of the considered LLMs and their properties. Due to the data usage agreement of MIMIC-IV ,\nonly open access models that can be downloaded can be used with the data. Thus, only LLMs based on Llama 2 were\nused in this study.\n* Meta defines ‘public data’ as a ‘mix of data from publicly available sources’.\n† No further information provided.\n??? indicates no information has been made public.\n18\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nReferences\n[1] A. B. Abacha, E. Agichtein, Y . Pinter, and D. Demner-Fushman. Overview of the medical question answering\ntask at trec 2017 liveqa. In TREC, pages 1–12, 2017.\n[2] A. B. Abacha, Y . Mrabet, M. Sharp, T. R. Goodwin, S. E. Shooshan, and D. Demner-Fushman. Bridging the gap\nbetween consumers’ medication questions and trusted answers. In MedInfo, pages 25–29, 2019.\n[3] L. Adlung, Y . Cohen, U. Mor, and E. Elinav. Machine learning in clinical decision making.Med, 2(6):642–665,\n2021.\n[4] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al.\nPalm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.\n[5] Y . Bazi, M. M. A. Rahhal, L. Bashmal, and M. Zuair. Vision–language model for visual question answering in\nmedical imagery. Bioengineering, 10(3):380, 2023.\n[6] A. Belyaeva, J. Cosentino, F. Hormozdiari, C. Y . McLean, and N. A. Furlotte. Multimodal llms for health\ngrounded in individual-specific data. arXiv preprint arXiv:2307.09018, 2023.\n[7] S. Berman. Clinical decision making. In L. Bajaj, S. J. Hambidge, G. Kerby, and A.-C. Nyquist, editors,Berman’s\nPediatric Decision Making (Fifth Edition), pages 1–6. Mosby, fifth edition edition.\n[8] S. Biswas. Chatgpt and the future of medical writing, 2023.\n[9] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems,\n33:1877–1901, 2020.\n[10] T. Buckley, J. A. Diao, A. Rodman, and A. K. Manrai. Accuracy of a vision-language model on challenging\nmedical cases. arXiv preprint arXiv:2311.05591, 2023.\n[11] G. Cervellin, R. Mora, A. Ticinesi, T. Meschi, I. Comelli, F. Catena, and G. Lippi. Epidemiology and outcomes of\nacute abdominal pain in a large urban emergency department: retrospective analysis of 5,340 cases. 4(19):362–362.\n[12] Z. Chen, A. H. Cano, A. Romanou, A. Bonnet, K. Matoba, F. Salvi, M. Pagliardini, S. Fan, A. Köpf, A. Mo-\nhtashami, A. Sallinen, A. Sakhaeirad, V . Swamy, I. Krawczuk, D. Bayazit, A. Marmet, S. Montariol, M.-A.\nHartley, M. Jaggi, and A. Bosselut. Meditron-70b: Scaling medical pretraining for large language models, 2023.\n[13] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, Y . Li, X. Wang, M. Dehghani, S. Brahma, et al.\nScaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\n[14] T. Computer. Redpajama: an open dataset for training large language models, 2023.\n[15] S. Di Saverio, M. Podda, B. De Simone, M. Ceresoli, G. Augustin, A. Gori, M. Boermeester, M. Sartelli,\nF. Coccolini, A. Tarasconi, N. de’ Angelis, D. G. Weber, M. Tolonen, A. Birindelli, W. Biffl, E. E. Moore, M. Kelly,\nK. Soreide, J. Kashuk, R. Ten Broek, C. A. Gomes, M. Sugrue, R. J. Davies, D. Damaskos, A. Leppäniemi,\nA. Kirkpatrick, A. B. Peitzman, G. P. Fraga, R. V . Maier, R. Coimbra, M. Chiarugi, G. Sganga, A. Pisanu, G. L.\nde’ Angelis, E. Tan, H. Van Goor, F. Pata, I. Di Carlo, O. Chiara, A. Litvin, F. C. Campanile, B. Sakakushev,\nG. Tomadze, Z. Demetrashvili, R. Latifi, F. Abu-Zidan, O. Romeo, H. Segovia-Lohse, G. Baiocchi, D. Costa,\nS. Rizoli, Z. J. Balogh, C. Bendinelli, T. Scalea, R. Ivatury, G. Velmahos, R. Andersson, Y . Kluger, L. Ansaloni,\nand F. Catena. Diagnosis and treatment of acute appendicitis: 2020 update of the wses jerusalem guidelines.\n15(1).\n[16] S. Di Saverio, M. Podda, B. De Simone, M. Ceresoli, G. Augustin, A. Gori, M. Boermeester, M. Sartelli,\nF. Coccolini, A. Tarasconi, et al. Diagnosis and treatment of acute appendicitis: 2020 update of the wses jerusalem\nguidelines. World journal of emergency surgery, 15:1–42, 2020.\n[17] N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jian, B. Y . Lin, P. West, C. Bhagavatula, R. L. Bras, J. D. Hwang, et al.\nFaith and fate: Limits of transformers on compositionality. arXiv preprint arXiv:2305.18654, 2023.\n[18] A. V . Eriksen, S. Möller, and J. Ryg. Use of gpt-4 to diagnose complex clinical cases. NEJM AI, 2023.\n[19] M. L. for Computational Physiology. Responsible use of mimic data with online services like gpt, 2023. Accessed\non 16.01.2024.\n[20] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative\npre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n[21] A. Gilson, C. W. Safranek, T. Huang, V . Socrates, L. Chi, R. A. Taylor, D. Chartash, et al. How does chatgpt\nperform on the united states medical licensing examination? the implications of large language models for medical\neducation and knowledge assessment. JMIR Medical Education, 9(1):e45312, 2023.\n19\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \n[22] A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody,\nC.-K. Peng, and H. E. Stanley. Physiobank, physiotoolkit, and physionet: components of a new research resource\nfor complex physiologic signals. circulation, 101(23):e215–e220, 2000.\n[23] S. Golkar, M. Pettee, M. Eickenberg, A. Bietti, M. Cranmer, G. Krawezik, F. Lanusse, M. McCabe, R. Ohana,\nL. Parker, et al. xval: A continuous number encoding for large language models.arXiv preprint arXiv:2310.02989,\n2023.\n[24] B. Haibe-Kains, G. A. Adam, A. Hosny, F. Khodakarami, M. A. Q. C. M. S. B. of Directors Shraddha Thakkar 35\nKusko Rebecca 36 Sansone Susanna-Assunta 37 Tong Weida 35 Wolfinger Russ D. 38 Mason Christopher E. 39\nJones Wendell 40 Dopazo Joaquin 41 Furlanello Cesare 42, L. Waldron, B. Wang, C. McIntosh, A. Goldenberg,\nA. Kundaje, et al. Transparency and reproducibility in artificial intelligence. Nature, 586(7829):E14–E16, 2020.\n[25] J. Hall, K. Hardiman, S. Lee, A. Lightner, L. Stocchi, I. M. Paquette, S. R. Steele, D. L. Feingold, et al. The\namerican society of colon and rectal surgeons clinical practice guidelines for the treatment of left-sided colonic\ndiverticulitis. Diseases of the Colon & Rectum, 63(6):728–747, 2020.\n[26] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask\nlanguage understanding. arXiv preprint arXiv:2009.03300, 2020.\n[27] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits. What disease does this patient have? a\nlarge-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.\n[28] Z. Kanjee, B. Crowe, and A. Rodman. Accuracy of a generative artificial intelligence model in a complex\ndiagnostic challenge. JAMA, 2023.\n[29] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n[30] A. Köpf, Y . Kilcher, D. von Rütte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley,\nR. Nagyfi, et al. Openassistant conversations–democratizing large language model alignment. arXiv preprint\narXiv:2304.07327, 2023.\n[31] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon, C. Elepaño, M. Madriaga, R. Aggabao, G. Diaz-\nCandido, J. Maningo, et al. Performance of chatgpt on usmle: Potential for ai-assisted medical education using\nlarge language models. PLoS digital health, 2(2):e0000198, 2023.\n[32] A. Leppäniemi, M. Tolonen, A. Tarasconi, H. Segovia-Lohse, E. Gamberini, A. W. Kirkpatrick, C. G. Ball,\nN. Parry, M. Sartelli, D. Wolbrink, et al. 2019 wses guidelines for the management of severe acute pancreatitis.\nWorld journal of emergency surgery, 14(1):1–20, 2019.\n[33] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in the middle: How\nlanguage models use long contexts. arXiv preprint arXiv:2307.03172, 2023.\n[34] D. McDuff, M. Schaekermann, T. Tu, A. Palepu, A. Wang, J. Garrison, K. Singhal, Y . Sharma, S. Azizi, K. Kulka-\nrni, et al. Towards accurate differential diagnosis with large language models. arXiv preprint arXiv:2312.00164,\n2023.\n[35] S. M. McKinney, M. Sieniek, V . Godbole, J. Godwin, N. Antropova, H. Ashrafian, T. Back, M. Chesus, G. S.\nCorrado, A. Darzi, et al. International evaluation of an ai system for breast cancer screening.Nature, 577(7788):89–\n94, 2020.\n[36] M. Moor, O. Banerjee, Z. S. H. Abad, H. M. Krumholz, J. Leskovec, E. J. Topol, and P. Rajpurkar. Foundation\nmodels for generalist medical artificial intelligence. Nature, 616(7956):259–265, 2023.\n[37] A. Nicolson, J. Dowling, and B. Koopman. Improving chest x-ray report generation by leveraging warm-starting.\narXiv preprint arXiv:2201.09405, 2022.\n[38] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz. Capabilities of gpt-4 on medical challenge\nproblems. arXiv preprint arXiv:2303.13375, 2023.\n[39] H. Nori, Y . T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King, J. Larson, Y . Li, W. Liu, et al. Can generalist\nfoundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452,\n2023.\n[40] R. OpenAI. Gpt-4 technical report. arXiv, pages 2303–08774, 2023.\n[41] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\net al. Training language models to follow instructions with human feedback. Advances in Neural Information\nProcessing Systems, 35:27730–27744, 2022.\n20\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \n[42] A. Pal, L. K. Umapathi, and M. Sankarasubbu. Medmcqa: A large-scale multi-subject multi-choice dataset for\nmedical domain question answering. In Conference on Health, Inference, and Learning, pages 248–260. PMLR,\n2022.\n[43] P. Pezeshkpour and E. Hruschka. Large language models sensitivity to the order of options in multiple-choice\nquestions. arXiv preprint arXiv:2308.11483, 2023.\n[44] M. Pisano, N. Allievi, K. Gurusamy, G. Borzellino, S. Cimbanassi, D. Boerna, F. Coccolini, A. Tufo, M. Di Mar-\ntino, J. Leung, et al. 2020 world society of emergency surgery updated guidelines for the diagnosis and treatment\nof acute calculus cholecystitis. World journal of emergency surgery, 15(1):1–26, 2020.\n[45] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9, 2019.\n[46] A. Rao, M. Pang, J. Kim, M. Kamineni, W. Lie, A. K. Prasad, A. Landman, K. Dreyer, and M. D. Succi. Assessing\nthe utility of chatgpt throughout the entire clinical workflow: Development and usability study.Journal of Medical\nInternet Research, 25:e48659, 2023.\n[47] A. Roberts, C. Raffel, K. Lee, M. Matena, N. Shazeer, P. J. Liu, S. Narang, W. Li, and Y . Zhou. Exploring the\nlimits of transfer learning with a unified text-to-text transformer. 2019.\n[48] E. H. Shortliffe and M. J. Sepúlveda. Clinical Decision Support in the Era of Artificial Intelligence. JAMA,\n320(21):2199–2200, 12 2018.\n[49] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl,\net al. Large language models encode clinical knowledge. Nature, pages 1–9, 2023.\n[50] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal, et al.\nTowards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617,\n2023.\n[51] A. Testolin. Can neural networks do arithmetic? a survey on the elementary numerical skills of state-of-the-art\ndeep learning models. arXiv preprint arXiv:2303.07735, 2023.\n[52] A. Thawani, J. Pujara, F. Ilievski, and P. Szekely. Representing numbers in nlp: a survey and a vision. In\nProceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 644–656, 2021.\n[53] A. J. Thirunavukarasu, R. Hassan, S. Mahmood, R. Sanghera, K. Barzangi, M. El Mukashfi, and S. Shah.\nTrialling a large language model (chatgpt) in general practice with the applied knowledge test: observational study\ndemonstrating opportunities and limitations in primary care. JMIR Medical Education, 9(1):e46599, 2023.\n[54] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting. Large language\nmodels in medicine. Nature Medicine, pages 1–11, 2023.\n[55] J. Tiffen, S. J. Corbridge, and L. Slimmer. Enhancing clinical decision making: Development of a contiguous\ndefinition and conceptual framework. Journal of Professional Nursing, 30(5):399–405, 2014.\n[56] A. Toma, P. R. Lawler, J. Ba, R. G. Krishnan, B. B. Rubin, and B. Wang. Clinical camel: An open-source\nexpert-level medical language model with dialogue-based knowledge encoding. arXiv preprint arXiv:2305.12031,\n2023.\n[57] A. Toma, S. Senkaiahliyan, P. R. Lawler, B. Rubin, and B. Wang. Generative ai could revolutionize health\ncare—but not if control is ceded to big tech. Nature, 624(7990):36–38, 2023.\n[58] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava,\nS. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[59] T. Tu, S. Azizi, D. Driess, M. Schaekermann, M. Amin, P.-C. Chang, A. Carroll, C. Lau, R. Tanno, I. Ktena, et al.\nTowards generalist biomedical ai. arXiv preprint arXiv:2307.14334, 2023.\n[60] T. van Sonsbeek, M. M. Derakhshani, I. Najdenkoska, C. G. Snoek, and M. Worring. Open-ended medical visual\nquestion answering through prefix tuning of language models. arXiv preprint arXiv:2303.05977, 2023.\n[61] D. Van Veen, C. Van Uden, M. Attias, A. Pareek, C. Bluethgen, M. Polacin, W. Chiu, J.-B. Delbrouck, J. M. Z.\nChaves, C. P. Langlotz, et al. Radadapt: Radiology report summarization via lightweight domain adaptation of\nlarge language models. arXiv preprint arXiv:2305.01146, 2023.\n[62] Y . Wang and Y . Zhao. Tram: Benchmarking temporal reasoning for large language models. arXiv preprint\narXiv:2310.00835, 2023.\n[63] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm: Empowering large language\nmodels to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.\n21\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \n[64] C. Yang, X. Wang, Y . Lu, H. Liu, Q. V . Le, D. Zhou, and X. Chen. Large language models as optimizers.arXiv\npreprint arXiv:2309.03409, 2023.\n[65] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao. React: Synergizing reasoning and acting in\nlanguage models. arXiv preprint arXiv:2210.03629, 2022.\n[66] C. Zheng, H. Zhou, F. Meng, J. Zhou, and M. Huang. On large language models’ selection bias in multi-choice\nquestions. arXiv preprint arXiv:2309.03882, 2023.\n[67] J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y . Luan, D. Zhou, and L. Hou. Instruction-following evaluation for\nlarge language models. arXiv preprint arXiv:2311.07911, 2023.\n22\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nA Dataset Statistics\nDisease Median Age Age Range Gender Distribution Race Distribution\nAppendicitis 36.0 18-95 F (48.8%), M (51.2%) WHITE (64.9%)\nBLACK (10.3%)\nHISPANIC (8.3%)\nASIAN (8.0%)\nOTHER (8.5%)\nCholecystitis 59.0 18-95 F (55.8%), M (44.2%) WHITE (63.8%)\nBLACK (15.1%)\nHISPANIC (9.2%)\nASIAN (5.4%)\nOTHER (6.5%)\nPancreatitis 59.0 18-93 F (55.1%), M (44.9%) WHITE (65.1%)\nBLACK (15.3%)\nHISPANIC (8.9%)\nASIAN (2.6%)\nOTHER (8.1%)\nDiverticulitis 60.0 18-91 F (58.9%), M (41.1%) WHITE (72.1%)\nBLACK (10.5%)\nHISPANIC (5.8%)\nASIAN (4.7%)\nOTHER (7.0%)\nTable 2: Demographic Statistics of Patients with Different Diseases\n23\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nDisease Median Age Age Range Gender Distribution Race Distribution\nAppendicitis 38.5 20-75 F (45.0%), M (55.0%) WHITE (65.0%)\nBLACK (15.0%)\nHISPANIC (5.0%)\nASIAN (5.0%)\nOTHER (10.0%)\nCholecystitis 66.5 31-85 F (45.0%), M (55.0%) WHITE (65.0%)\nBLACK (10.0%)\nHISPANIC (5.0%)\nASIAN (5.0%)\nOTHER (15.0%)\nPancreatitis 52.5 20-82 F (60.0%), M (40.0%) WHITE (50.0%)\nBLACK (10.0%)\nHISPANIC (15.0%)\nASIAN (0.0%)\nOTHER (25.0%)\nDiverticulitis 56.0 31-85 F (45.0%), M (55.0%) WHITE (75.0%)\nBLACK (15.0%)\nHISPANIC (0.0%)\nASIAN (5.0%)\nOTHER (5.0%)\nTable 3: Demographic Statistics of Physician Comparison Subset of 80 patients\nB Prompts\nB.1 CDM Template\n{system_tag_start}You are a medical artificial intelligence assistant. You give helpful,\ndetailed and factually correct answers to the doctors questions to help him in his\nclinical duties. Your goal is to correctly diagnose the patient and provide treatment\nadvice. You will consider information about a patient and provide a final diagnosis.\nYou can only respond with a single complete\nThought:\nAction:\nAction Input:\nformat OR a single\nThought:\nFinal Diagnosis:\nTreatment:\nformat. Keep all reasoning in the Thought section. The Action, Action Input, Final\nDiagnosis, and Treatment sections should be direct and to the point. The results of\nthe action will be returned directly after the Action Input field in the \"Observation:\"\nfield.\nFormat 1:\nThought: (reflect on your progress and decide what to do next)\nAction: (the action name, should be one of [{tool_names}])\nAction Input: (the input string to the action)\n24\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nObservation: (the observation from the action will be returned here)\nOR\nFormat 2:\nThought: (reflect on the gathered information and explain the reasoning for the final\ndiagnosis)\nFinal Diagnosis: (the final diagnosis to the original case)\nTreatment: (the treatment for the given diagnosis)\nThe tools you can use are:\nPhysical Examination: Perform physical examination of patient and receive the\nobservations.\nLaboratory Tests: Run specific laboratory tests and receive their values. The specific\ntests must be specified in the 'Action Input' field.\nImaging: Do specific imaging scans and receive the radiologist report.\nScan region AND modality must be specified in the 'Action Input'\nfield.{add_tool_descr}{system_tag_end}{user_tag_start}{examples}Consider the following\ncase and come to a final diagnosis and treatment by thinking, planning, and using the\naforementioned tools and format.\nPatient History:\n{input}{user_tag_end}{ai_tag_start}Thought:{agent_scratchpad}\nB.2 CDM Observation Summarize Template\n{system_tag_start}You are a medical artificial intelligence assistant. Your goal is to\neffectively, efficiently and accurately reduce text without inventing information. You\nwant to return verbatim observations that are abnormal and of interest to a possible\ndiagnosis of the patient. Normal observations can be combined. Do not invent information.\nUse medical abbreviations when possible to save characters. Put the most important\ninformation first.{system_tag_end}{user_tag_start}Please summarize the following result:\n{observation}{user_tag_end}{ai_tag_start}\nSummary:\nB.3 CDM-FI Template\n{system_tag_start}You are a medical artificial intelligence assistant. You\ndirectly diagnose patients based on the provided information to assist a doctor\nin his clinical duties. Your goal is to correctly diagnose the patient. Based\non the provided information you will provide a final diagnosis of the most\nsevere pathology. Don't write any further information. Give only a single\ndiagnosis.{system_tag_end}{fewshot_examples}{user_tag_start}Provide the most likely final\ndiagnosis of the following patient.\n{input}{diagnostic_criteria}{user_tag_end}{ai_tag_start}Final Diagnosis:\nB.4 Reference Range Test Zeroshot Template\n{system_tag_start}You are a technical AI assistant working in a laboratory that handles\ntests for a hospital. You are good at interpreting numbers. You are responsible for\nreviewing the results of lab tests and determining whether they are Low, Normal,\nor High. You will be given the test, its value and then the reference range for\nthat test, which will be written as \"Reference Range [Lower Reference Range -\nUpper Reference Range]\". You will write just one word, indicating if the test\nresults are Low, Normal, or High. Do not write anything other than your one word\n25\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nanswer.{system_tag_end}{user_tag_start}{lab_test_string_rr}{user_tag_end}{ai_tag_start}Test\nResult:\nB.5 CDM-FI No Final Template\n{system_tag_start}You are a medical artificial intelligence assistant. You directly\ndiagnose patients based on the provided information to assist a doctor in his clinical\nduties. Your goal is to correctly diagnose the patient. Based on the provided information\nyou will provide the diagnosis. Don't write any further information. Give only a single\ndiagnosis.{system_tag_end}{fewshot_examples}{user_tag_start}Provide the diagnosis of the\nfollowing patient.\n{input}{diagnostic_criteria}{user_tag_end}{ai_tag_start}Diagnosis:\"\"\"\nB.6 CDM-FI Main Diagnosis Template\n{system_tag_start}You are a medical artificial intelligence assistant. You directly\ndiagnose patients based on the provided information to assist a doctor in his clinical\nduties. Your goal is to correctly diagnose the patient. Based on the provided information\nyou will provide the main diagnosis. Don't write any further information. Give only a\nsingle diagnosis.{system_tag_end}{fewshot_examples}{user_tag_start}Provide the main\ndiagnosis of the following patient.\n{input}{diagnostic_criteria}{user_tag_end}{ai_tag_start}Main Diagnosis:\"\"\"\nB.7 CDM-FI Primary Diagnosis Template\n{system_tag_start}You are a medical artificial intelligence assistant. You directly\ndiagnose patients based on the provided information to assist a doctor in his clinical\nduties. Your goal is to correctly diagnose the patient. Based on the provided information\nyou will provide the primary diagnosis. Don't write any further information. Give only\na single diagnosis.{system_tag_end}{fewshot_examples}{user_tag_start}Provide the primary\ndiagnosis of the following patient.\n{input}{diagnostic_criteria}{user_tag_end}{ai_tag_start}Primary Diagnosis:\"\"\"\nB.8 CDM-FI No System Template\n{system_tag_start}{system_tag_end}{fewshot_examples}{user_tag_start}Provide the most\nlikely final diagnosis of the following patient.\n{input}{diagnostic_criteria}{user_tag_end}{ai_tag_start}Final Diagnosis:\"\"\"\nB.9 CDM-FI No User Template\n{system_tag_start}You are a medical artificial intelligence assistant. You\ndirectly diagnose patients based on the provided information to assist a doctor\nin his clinical duties. Your goal is to correctly diagnose the patient. Based on\nthe provided information you will provide a final diagnosis of the most severe\npathology. Don't write any further information. Give only a single diagnosis.\n{system_tag_end}{fewshot_examples}{user_tag_start}{input}{diagnostic_criteria}\n{user_tag_end}{ai_tag_start}Final Diagnosis:\"\"\"\n26\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nB.10 CDM-FI No Medical Template\n{system_tag_start}You are an artificial intelligence assistant. You answer questions to\nthe best of your abilities. Think hard about the following problem and then provide an\nanswer.{system_tag_end}{fewshot_examples}{user_tag_start}Provide the most likely final\ndiagnosis of the following patient.\n{input}{diagnostic_criteria}{user_tag_end}{ai_tag_start}Final Diagnosis:\"\"\"\nB.11 CDM-FI Serious Final Template\n{system_tag_start}You are a medical artificial intelligence assistant. You\ndirectly diagnose patients based on the provided information to assist a doctor\nin his clinical duties. Your goal is to correctly diagnose the patient. Based\non the provided information you will provide a final diagnosis of the most\nsevere pathology. Don't write any further information. Give only a single\ndiagnosis.{system_tag_end}{fewshot_examples}{user_tag_start}Provide the most serious\nfinal diagnosis of the following patient.\n{input}{diagnostic_criteria}{user_tag_end}{ai_tag_start}Final Diagnosis:\"\"\"\nB.12 CDM-FI Minimal System Template\n{system_tag_start}You are a medical artificial intelligence assistant. You diagnose\npatients based on the provided information to assist a doctor in his clinical\nduties.{system_tag_end}{fewshot_examples}{user_tag_start}Provide the most likely final\ndiagnosis of the following patient.\n{input}{diagnostic_criteria}{user_tag_end}{ai_tag_start}Final Diagnosis:\"\"\"\nB.13 CDM-FI No System No User Template\n{system_tag_start}{system_tag_end}{fewshot_examples}{user_tag_start}{input}\n{diagnostic_criteria}{user_tag_end}{ai_tag_start}Final Diagnosis:\"\"\"\nB.14 CDM-FI No Diagnosis Prompt Template\n{system_tag_start}You are a medical artificial intelligence assistant. You\ndirectly diagnose patients based on the provided information to assist a doctor\nin his clinical duties. Your goal is to correctly diagnose the patient. Based\non the provided information you will provide a final diagnosis of the most\nsevere pathology. Don't write any further information. Give only a single\ndiagnosis.{system_tag_end}{fewshot_examples}{user_tag_start}Provide the most likely final\ndiagnosis of the following patient.\n{input}{diagnostic_criteria}{user_tag_end}{ai_tag_start}\"\"\"\n27\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nC Example Exchange using Synthetic Data\nIn this section we show an example exchange between our simulated clinical environment provided by our evaluation\nframework using the MIMIC-CDM dataset and the WizardLM model. The first prompt is the system prompt given\nby the framework to the model explaining the task and its options. The rest of the exchange pulls (synthetic) data\nfrom MIMIC-CDM and records the LLM’s response. Seen in this exchange is an example of the model requesting an\nimaging modality that is not available in the dataset and also hallucinating a surgery tool that does not exist. Upon\nreminding it of the available tools it correctly diagnoses the patient. This example exchange uses synthetic data to\nfurther protect patient privacy.\nEv\naluation Framework using MIMIC-CDM\nYou are a medical artificial intelligence assistant. You give helpful, detailed and factually correct an-\nswers to the doctors questions to help him in his clinical duties. Your goal is to correctly diagnose the\npatient and provide treatment advice. You will consider information about a patient and provide a final diagnosis.\nYou can only respond with a single complete\nThought:\nAction:\nAction Input:\nformat OR a single\nThought:\nFinal Diagnosis:\nTreatment:\nformat. Keep all reasoning in the Thought section. The Action, Action Input, Final Diagnosis, and Treatment\nsections should be direct and to the point. The results of the action will be returned directly after the Action\nInput field in the \"Observation:\" field.\nFormat 1:\nThought: (reflect on your progress and decide what to do next)\nAction: (the action name, should be one of [Physical Examination, Laboratory Tests, Imaging])\nAction Input: (the input string to the action)\nObservation: (the observation from the action will be returned here)\nOR\nFormat 2:\nThought: (reflect on the gathered information and explain the reasoning for the final diagnosis)\nFinal Diagnosis: (the final diagnosis to the original case)\nTreatment: (the treatment for the given diagnosis)\nThe tools you can use are:\nPhysical Examination: Perform physical examination of patient and receive the observations.\nLaboratory Tests: Run specific laboratory tests and receive their values. The specific tests must be specified in\nthe ’Action Input’ field.\nImaging: Do specific imaging scans and receive the radiologist report. Scan region AND modality must be\nspecified in the ’Action Input’ field. Consider the following case and come to a final diagnosis and treatment by\nthinking, planning, and using the aforementioned tools and format.\n28\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nEv\naluation Framework using MIMIC-CDM\nP\natient History:\n___ presenting with 3 days since acute onset of pain in RLQ. Pain started ___ afternoon but worsened\nsignificantly overnight. Intense episodes woke her from sleep. Patient proceeds to experience nausea and\nchills. No vomiting. Patient endorses anorexia. Past Medical History: Anorexia, Anxiety. Family History:\nNon-contributory.\nLLM\nThought: Given the acute onset of right lower quadrant pain, along with anorexia and right-sided flank pain,\nthere could be several possibilities such as appendicitis, kidney stones, or inflammatory bowel disease like\nCrohn’s disease. However, since the patient does not have any past medical history or family history of these\nconditions, it might be less likely. Therefore, we need to perform further investigations to determine the exact\ncause of the pain.\nAction: Physical Examination\nAction Input: N/A\nEvaluation Framework using MIMIC-CDM\nObserv\nation: Physical Examination:\nTemp: 97.8 HR: 44 BP: 104/69 RR: 17 100% Ra Gen: NAD HEENT: No scleral icterus, mucus membranes\nmoist CV: RRR no m,r,g RESP: CTABL Abd: soft, tender to palpation inRLQ, non distended, no rebound or\nguarding Ext: wwpx4, palpable distal pulses\nLLM\nThought: After performing the physical examination, the vital signs appear normal except for a slightly elevated\nblood pressure. Findings of tenderness to palpation in the right lower quadrant of the abdomen supports\nthe forementioned possibilities of the patient’s complaints, such as appendicitis, kidney stones, urinary tract\ninfection, ovarial cysts or inflammatory bowel disease. We need to investigate further to rule out potential causes.\nAction: Laboratory Tests\nAction Input: Complete Blood Count (CBC), Liver Function Tests (LFTs), Renal panel, Urinalysis\n29\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nEvaluation Framework using MIMIC-CDM\nObservation: Laboratory Tests:\n(Blood) Red Blood Cells: 4.39 m/uL\n(Blood) White Blood Cells : 16.8 K/uL\n(Blood) Hemoglobin: 12.4 g/dL\n(Blood) Hematocrit: 38.8 %\n(Blood) MCV: 85.4 fL\n(Blood) MCH: 27.3 pg\n(Blood) MCHC: 33.4 %\n(Blood) Platelet Count: 226.8 K/uL\n(Blood) Lymphocytes: 22.0 %\n(Blood) Basophils: 0.5 %\n(Blood) Eosinophils: 1.9 %\n(Blood) Monocytes: 5.5 %\n(Blood) Neutrophils: 69.8 %\n(Blood) RDW: 13.0 %\n(Blood) Alanine Aminotransferase (ALT): 18.1 IU/L\n(Blood) Asparate Aminotransferase (AST): 29.1 IU/L\n(Blood) Alkaline Phosphatase: 68.8 IU/L\n(Blood) Bilirubin, Total: 0.3 mg/dL\n(Blood) PT: 10.5 sec\n(Blood) INR(PT): 0.9\n(Blood) Albumin: 5.2 g/dL\n(Blood) Creatinine: 0.6 mg/dl\n(Urine) Ketones: NEG\n(Urine) Urine Color: Yellow.\n(Urine) Urine Appearance: Clear.\n(Urine) Specific Gravity: 1.0\n(Urine) Protein: NEG.\n(Urine) pH: 5.8 units\n(Urine) Bilirubin: NEG.\n(Urine) Glucose: NEG.\n(Urine) Urobilinogen: NEG.\n(Urine) Ketone: TR.\n(Urine) Nitrite: NEG.\n(Urine) Leukocytes: NEG.\nLLM\nThought: The laboratory test results show normal values for most parameters analyzed. However, the patients\nhas elevated white blood cells as a marker for inflammation. Based on the diagnostic test results so far, the\npatient is suspected to have an inflammation in the abdomen, such as appendicitis or inflammatory bowel disease.\nAction: Imaging\nAction Input: Ultrasound of the Right Lower Quadrant\nEv\naluation Framework using MIMIC-CDM\nObserv\nation: Imaging:\nAbdomen Ultrasound: Not available. Try a different imaging modality.\n30\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nLLM\nThought: Since ultrasound is not suitable for this case, we should proceed with another imaging modality. Given\nthe possibility of hepatobiliary involvement, a computed tomography (CT) scan would be more appropriate to\nvisualize the internal organs better.\nAction: Imaging\nAction Input: Computed Tomography (CT) scan of the Abdom\nEvaluation Framework using MIMIC-CDM\nObserv\nation: Imaging:\nAbdomen CT: EXAMINATION:\nCT ABD AND PELVIS WITH CONTRAST:\nTECHNIQUE:\nMDCT imaging, extending from the lung bases to the pubic symphysis, was performed post-administration of\n130cc intravenous contrast. Axial scans were analyzed alongside coronal and sagittal reformatted images.\nFINDINGS:\nCHEST:\nLung bases are clear without pleural or pericardial effusions.\nABDOMEN:\nLiver and gallbladder present normally; portal and hepatic veins are patent. Pancreas and spleen are normal;\nbilateral adrenal glands are unremarkable. Kidneys show symmetrical enhancement without hydronephrosis.\nStomach and small bowel maintain normal caliber. A hyperdense, dilated structure, likely the appendix, arises\nfrom the cecal base, measuring up to 10 mm and exhibiting mucosal hyperenhancement compared to adjacent\nbowel. Predominant features include hyperenhancement and wall thickening, with less emphasis on fluid\ncontent. Inflammatory changes are evident at the cecal base, around the appendix, especially at its indistinct tip.\nAdjacent to the ascending colon’s mesentery, two enlarged lymph nodes, 1.5 cm and 1.9 cm in size, show brisk\nenhancement. The smaller node contains central low-density material. Both nodes are surrounded by marked\nfat stranding and suspected necrotic or suppurative changes.\nThe ascending, transverse, and descending colon appear normal. No mesenteric free air is observed.\nPELVIS:\nUrinary bladder and adnexa are unremarkable. Uterus contains an intrauterine device in standard position. A\nsmall volume of simple free fluid is present in the pelvis, likely physiologic.\nVESSELS:\nAorta and its major branches are normal in caliber and patent.\nOSSEOUS STRUCTURES:\nNo focal lytic or sclerotic lesions suggestive of malignancy are identified.\nLLM\nThought: The CT scan supports the diagnosis of acute appendicitis due to the enlarged and inflamed appendix\nobserved. I will recommend surgical intervention for removal of the affected organ.\nAction: Surgery\nAction Input: Appendectomy\n31\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nEvaluation Framework using MIMIC-CDM\nObservation: Provide a diagnosis and treatment OR a valid tool. That is not a valid tool, try one of [Physical\nExamination, Laboratory Tests, Imaging].\nLLM\nThought:\nThe CT scan revealed an inflamed and enlarged appendix, indicating acute appendicitis as the most likely cause\nof the patient’s pain. Additionally, there are two enlarged and briskly enhancing lymph nodes in the mesentery,\nwhich could suggest a secondary infectious process or inflammatory condition. The presence of a small amount\nof simple free fluid in the pelvis might be due to the inflammatory response.\nFinal Diagnosis: Acute Appendicitis\nTreatment: Surgical intervention - Appendectomy\n32\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nD Laboratory Test Categories\nPathologies Category Items\nAppendicitis\nCholecystitis\nDiverticulitis\nPancreatitis\nInflammation White Blood Cells\nWBC Count\nC-Reactive Protein\nHigh-Sensitivity CRP\nCholecystitis Gallbladder Bilirubin\nGamma Glutamyltransferase\nCholecystitis Liver Alanine Aminotransferase (ALT)\nAsparate Aminotransferase (AST)\nPancreatitis Pancreas Amylase\nLipase\nPancreatitis Pancreatitis Severity Hematocrit\nUrea Nitrogen\nTriglycerides\nCalcium\nPotassium\nTable 4: The individual laboratory test category items defined using the guidelines for appendicitis[16], cholecystitis[44],\ndiverticulitis [25], and pancreatitis[32]. At least one of these tests must have been ordered either individually or as part\nof a panel for the category to be counted.\n33\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nE Diagnosis Definitions\nPathology Location Modifiers\nappendicitis appendi gangren\ninfect\ninflam\nabscess\nrupture\nnecros\nperf\ncholecystitis gallbladder gangren\ninfect\ninflam\nabscess\nnecros\nperf\ncholecystitis cholangitis cholangitis\ncholecystitis acute gallbladder disease\nattack\ncholecystitis acute biliary colic\ndiverticulitis diverticul inflam\ninfect\nabscess\nperf\nrupture\ndiverticulitis acute colonic perfor\ndiverticulitis sigmoid perfor\ncolitis\npancreatitis pancrea gangren\ninfect\ninflam\nabscess\nnecros\nTable 5: For each pathology, alternative diagnostic phrasings were accepted. If within one diagnostic phrase (delineated\nthrough e.g. periods, commas, ‘vs’, or other separators) the location and modifier occurred without negation, the\ndiagnosis was marked as correct. For example, for cholecystitis a diagnosis of ‘Perforated gallbladder’ or ‘Gallbladder\ninfection’ were both accepted. A diagnosis of ‘Gallbladder disease vs perforated appendix’ would not be accepted.\nShortened substrings of the locations and modifiers were used to allow for alternative endings to match such as both ‘An\ninfect of the gallbladder’ and ‘Infected gallbladder’, or ‘Periappendicular abscess’ and ‘Perforated appendix’. Manual\ncontrols were done to verify the specificity and sensitivity of the definitions.\n34\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nF Adherence to Diagnostic Guidelines\nPhysical Examination (Late) Physical Examination\n0\n20\n40\n60\n80\n100Examination Requested (%)\n97.1 98.1\n79.8\n87.7\n53.1\n63.9\nLlama 2 Chat OASST WizardLM\nFigure 11: Llama 2 Chat was the only LLM that consistently requested physical examinations. (Late) Physical\nExaminations counted the physical examination if it wasn’t the first information requested.\n35\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nAppendicitis Cholecystitis Diverticulitis Pancreatitis\nPathology\n0\n20\n40\n60\n80\n100Imaging Modality Requested (%)\n71\n57\n76\n62\n20\n27\n8\n18\n7 10 12 12\n33\n22\n54\n39\n60\n65\n27\n43\n6 10 15 13\n42\n21\n49\n26\n41\n47\n12\n28\n15\n29 37 40\n77\n23\n81\n26\n20\n68\n10\n56\n8\n6 7 9\nCT\nUltrasound\nMRI\nRadiograph\nOther\nNone\nLlama 2 Chat\nOASST\nWizardLM\nMIMIC Doctors\nFigure 12: The first imaging modality requested by the LLMs and the attending doctors in the MIMIC dataset are\nshown. LLMs sometimes follow diagnostic guidelines concerning imaging but often diagnose without requesting any\nimaging at all. As we show that imaging is the most useful diagnostic tool for all LLMs for each pathology except\npancreatitis, this could be partly responsible for their low diagnostic accuracy. The legend specifies the colors of the\nimaging modalities and the patterns of the models.\n36\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nG FI Performance\nAppendicitis Cholecystitis Diverticulitis Pancreatitis Mean\n0\n20\n40\n60\n80\n100Diagnostic Accuracy (%)\n95\n48\n44\n48\n59\n97\n61\n48\n64 68\n96\n37\n53\n74\n65\n97\n72\n55\n68\n73\n97\n19\n45\n75\n59\nLlama 2 Chat OASST WizardLM Clinical Camel Meditron\nFigure 13: LLMs perform better when all information is given, especially on pathologies with strong indications such\nas appendicitis (dilated appendix described in radiologist report) and pancreatitis (elevated pancreatic enzymes listed in\nlaboratory test results).\n37\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nH LLMs Diagnostic Accuracy Without Medical Abbreviations\nAppendicitis Cholecystitis Diverticulitis Pancreatitis Mean\n0\n20\n40\n60\n80\n100Diagnostic Accuracy (%)\n85.0\n50.0\n45.0\n65.0\n61.3\n100.0\n50.0\n40.0\n80.0\n67.5\n95.0\n30.0\n55.0\n70.0\n62.5\nLlama 2 Chat OASST WizardLM\nFigure 14: Diagnostic accuracy on the clinician subset of MIMIC-CDM-FI stays the same when medical abbreviations\nare written out.\nI LLMs are sensitive to information order\nChanging the order of the presented information changes diagnostic accuracy. Crucially, the best order is disease\nspecific, meaning a clinician must deliver a preliminary diagnosis to ensure proper model performance, eliminating\nmany of the benefits of an AI clinical decision making system. Best order for each pathology is in bold.\nI.1 Llama 2 Chat\nExperiment Imaging, Lab\nPhysical\nImaging, Physical\nLab\nLab, Imaging\nPhysical\nLab, Physical\nImaging\nPhysical, Imaging\nLab\nPhysical, Lab\nImaging\nPathology\nAppendicitis 95.72 95.72 95.61 94.88 95.40 94.98\nCholecystitis 39.04 40.43 46.76 48.30 41.05 64.35\nDiverticulitis 45.91 46.30 41.63 42.41 47.08 49.03\nPancreatitis 60.41 65.99 53.16 50.37 63.01 48.14\nMean 60.27 62.11 59.29 58.99 61.64 64.13\n38\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nI.2 OASST\nExperiment Imaging, Lab\nPhysical\nImaging, Physical\nLab\nLab, Imaging\nPhysical\nLab, Physical\nImaging\nPhysical, Imaging\nLab\nPhysical, Lab\nImaging\nPathology\nAppendicitis 97.18 97.60 96.97 96.97 97.60 97.39\nCholecystitis 69.29 67.28 64.51 61.57 65.59 64.66\nDiverticulitis 48.64 48.64 41.63 47.08 46.69 47.86\nPancreatitis 64.13 64.13 65.80 65.06 65.43 60.22\nMean 69.81 69.41 67.23 67.67 68.83 67.53\nI.3 WizardLM\nExperiment Imaging, Lab\nPhysical\nImaging, Physical\nLab\nLab, Imaging\nPhysical\nLab, Physical\nImaging\nPhysical, Imaging\nLab\nPhysical, Lab\nImaging\nPathology\nAppendicitis 96.97 96.76 96.55 96.34 97.18 96.66\nCholecystitis 40.74 37.19 41.82 37.96 36.42 51.39\nDiverticulitis 51.36 52.53 54.09 53.70 53.70 52.92\nPancreatitis 78.81 77.88 75.84 75.28 78.07 69.14\nMean 66.97 66.09 67.07 65.82 66.34 67.53\nJ LLMs Cannot Interpret Numbers\nLow Normal High\n0\n20\n40\n60\n80\n100Accuracy (%)26.5\n89.7\n50.1\n70.2\n90.7\n77.2\n45.8\n93.2\n24.1\nn = 9518 n = 67814 n = 17352\nLlama 2 Chat OASST WizardLM\nFigure 15: When providing a laboratory test result and its reference range, LLMs are incapable of consistently\ninterpreting the result as normal, low or high.\n39\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nK LLMs Struggle to Follow Instructions\nError Name Error Example Corrected Example\nNext Action Error Order labs Action: Laboratory Tests\nPerform a physical examination Action: Physical Examination\nRun a CT scan of the abdomen Action: Imaging\nAction Input: Abdominal CT\nTool Hallucination Action: Consult specialist\nAction: Consider the evidence\nDiagnosis Error Final Diagnosis: Based on the pa-\ntient’s symptoms, laboratory results,\nand imaging studies, the final diag-\nnosis is: Appendicitis\nFinal Diagnosis: Appendicitis\nFinal Diagnosis: Acute Gallstone\nPancreatitis. The patient’s symp-\ntoms of abdominal pain, nausea,\nvomiting, and fever, along with the\nlaboratory results of elevated serum\namylase and lipase levels, suggest a\ndiagnosis of acute pancreatitis.\nFinal Diagnosis: Acute Gallstone\nPancreatitis\nFinal Diagnosis: Appendicitis, ab-\ndominal pain, and elevated white\nblood cells\nFinal Diagnosis: Appendicitis\nTable 6: Examples of the types of errors commonly made by models when providing actions and diagnoses. The\ncorrected example in the desired format is also provided. Note that tool hallucination examples are simply not valid\nactions and so there are no corrected examples provided.\n40\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nL LLMs are Sensitive to the Order of Information\nLlama 2 Chat OASST WizardLM\nModel\n0\n20\n40\n60\n80\n100Diagnostic Accuracy (%)\n0.84 0.73 0.84\n9.26\n7.87\n5.40\n5.45 7.00\n2.72\n18.03 1.67\n5.02\n3.32\n2.58 1.95\nChanging the Order of Information Changes Diagnostic Accuracy\nAppendicitis Cholecystitis Diverticulitis Pancreatitis Mean\nFigure 16: By mixing the order in which information is presented to LLMs, their diagnostic accuracy changes despite\nthe information included staying the same. This places an unnecessary burden upon clinicians who would need to\nconsider and monitor the order in which they feed the models with information. The value above each whisker shows\nthe difference between the best performing and worst performing order.\n41\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint \nM Summarizing Progress Improves CDM Diagnostic Accuracy\n5\n 4\n 3\n 2\n 1\n 0 1 2 3\nChange in Diagnostic Accuracy (%)\nLlama 2 Chat\nOASST\nWizardLM\nModel\n-0.5\n0.4\n-2.0\n2.8\n-0.2\n1.9\n-4.7\n-2.7\n-3.5\n-2.8\n-0.9\n1.9\n-1.3\n-0.8\n-0.4\nEffect of Removing Summarization\nAppendicitis Cholecystitis Diverticulitis Pancreatitis Mean\nFigure 17: When an LLM approaches its input limit, we ask it to summarize the information gathered thus far to allow\nfor more context. Increased input sizes allows it more opportunities to ask for information and increases the chances of\nrequesting information that is important for the diagnosis.\n42\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 26, 2024. ; https://doi.org/10.1101/2024.01.26.24301810doi: medRxiv preprint ",
  "topic": "Workflow",
  "concepts": [
    {
      "name": "Workflow",
      "score": 0.7987654805183411
    },
    {
      "name": "Software deployment",
      "score": 0.7084054350852966
    },
    {
      "name": "Clinical decision making",
      "score": 0.5382226705551147
    },
    {
      "name": "Clinical decision support system",
      "score": 0.4856349527835846
    },
    {
      "name": "Computer science",
      "score": 0.46516847610473633
    },
    {
      "name": "Data science",
      "score": 0.41511741280555725
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.39188310503959656
    },
    {
      "name": "Decision support system",
      "score": 0.34856998920440674
    },
    {
      "name": "Medicine",
      "score": 0.28097134828567505
    },
    {
      "name": "Artificial intelligence",
      "score": 0.25028741359710693
    },
    {
      "name": "Intensive care medicine",
      "score": 0.1796044111251831
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802619606",
      "name": "Klinikum rechts der Isar",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I1281704429",
      "name": "Christiana Care Health System",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I3018771216",
      "name": "LMU Klinikum",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I3018134672",
      "name": "Helmholtz Zentrum München",
      "country": "DE"
    }
  ]
}