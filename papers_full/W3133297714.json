{
  "title": "Unsupervised Brain Anomaly Detection and Segmentation with Transformers",
  "url": "https://openalex.org/W3133297714",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4281507991",
      "name": "Pinaya, Walter Hugo Lopez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281507989",
      "name": "Tudosiu, Petru-Daniel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2280002640",
      "name": "Gray, Robert",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3189564902",
      "name": "Rees, Geraint",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3160815906",
      "name": "Nachev, Parashkev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743137889",
      "name": "Ourselin, Sebastien",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2824714594",
      "name": "Cardoso, M. Jorge",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2127167918",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2136145485",
    "https://openalex.org/W3095223219",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W2301358467",
    "https://openalex.org/W2913300775",
    "https://openalex.org/W2796762894",
    "https://openalex.org/W1641498739",
    "https://openalex.org/W2471406153",
    "https://openalex.org/W2607804943",
    "https://openalex.org/W2751069891",
    "https://openalex.org/W2911059780",
    "https://openalex.org/W2963546708",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W2952735543",
    "https://openalex.org/W2765961644",
    "https://openalex.org/W2900298334",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2522628945",
    "https://openalex.org/W3114951884",
    "https://openalex.org/W2411541852",
    "https://openalex.org/W3021584089",
    "https://openalex.org/W2140157574",
    "https://openalex.org/W2971074500"
  ],
  "abstract": "Pathological brain appearances may be so heterogeneous as to be intelligible only as anomalies, defined by their deviation from normality rather than any specific pathological characteristic. Amongst the hardest tasks in medical imaging, detecting such anomalies requires models of the normal brain that combine compactness with the expressivity of the complex, long-range interactions that characterise its structural organisation. These are requirements transformers have arguably greater potential to satisfy than other current candidate architectures, but their application has been inhibited by their demands on data and computational resource. Here we combine the latent representation of vector quantised variational autoencoders with an ensemble of autoregressive transformers to enable unsupervised anomaly detection and segmentation defined by deviation from healthy brain imaging data, achievable at low computational cost, within relative modest data regimes. We compare our method to current state-of-the-art approaches across a series of experiments involving synthetic and real pathological lesions. On real lesions, we train our models on 15,000 radiologically normal participants from UK Biobank, and evaluate performance on four different brain MR datasets with small vessel disease, demyelinating lesions, and tumours. We demonstrate superior anomaly detection performance both image-wise and pixel-wise, achievable without post-processing. These results draw attention to the potential of transformers in this most challenging of imaging tasks.",
  "full_text": "Proceedings of Machine Learning Research – Under Review:1–22, 2021 Full Paper – MIDL 2021 submission\nUnsupervised Brain Anomaly Detection and Segmentation\nwith Transformers\nWalter Hugo Lopez Pinaya 1 walter.diaz sanz@kcl.ac.uk\nPetru-Daniel Tudosiu 1 petru.tudosiu@kcl.ac.uk\nRobert Gray 2 r.gray@ucl.ac.uk\nGeraint Rees 3 g.rees@ucl.ac.uk\nParashkev Nachev2 p.nachev@ucl.ac.uk\nS´ ebastien Ourselin1 sebastien.ourselin@kcl.ac.uk\nM. Jorge Cardoso1 m.jorge.cardoso@kcl.ac.uk\n1 Department of Biomedical Engineering, School of Biomedical Engineering & Imaging Sciences,\nKing’s College London, London, UK\n2 Institute of Neurology, University College London, London, UK\n3 Institute of Cognitive Neuroscience, University College London, London, UK\nEditors: Under Review for MIDL 2021\nAbstract\nPathological brain appearances may be so heterogeneous as to be intelligible only as\nanomalies, deﬁned by their deviation from normality rather than any speciﬁc pathological\ncharacteristic. Amongst the hardest tasks in medical imaging, detecting such anomalies\nrequires models of the normal brain that combine compactness with the expressivity of\nthe complex, long-range interactions that characterise its structural organisation. These\nare requirements transformers have arguably greater potential to satisfy than other cur-\nrent candidate architectures, but their application has been inhibited by their demands\non data and computational resource. Here we combine the latent representation of vector\nquantised variational autoencoders with an ensemble of autoregressive transformers to en-\nable unsupervised anomaly detection and segmentation deﬁned by deviation from healthy\nbrain imaging data, achievable at low computational cost, within relative modest data\nregimes. We compare our method to current state-of-the-art approaches across a series of\nexperiments involving synthetic and real pathological lesions. On real lesions, we train our\nmodels on 15,000 radiologically normal participants from UK Biobank, and evaluate perfor-\nmance on four diﬀerent brain MR datasets with small vessel disease, demyelinating lesions,\nand tumours. We demonstrate superior anomaly detection performance both image-wise\nand pixel-wise, achievable without post-processing. These results draw attention to the\npotential of transformers in this most challenging of imaging tasks.\nKeywords: Transformer, Unsupervised Anomaly Segmentation, Anomaly Detection, Neu-\nroimaging, Vector Quantized Variational Autoencoder\n1. Introduction\nTransformers have revolutionised language modelling, becoming the de-facto network ar-\nchitecture for language tasks (Radford et al., 2018, 2019; Vaswani et al., 2017). They rely\non attention mechanisms to capture the sequential nature of an input sequence, dispensing\nwith recurrence and convolutions entirely. This mechanism allows the modelling of depen-\ndencies of the inputs without regard to their distance, enabling the acquisition of complex\n© 2021 W.L. , P.-D.T. , R.G. , G.R. , P. Nachev, S.O. & M. Cardoso.\narXiv:2102.11650v1  [eess.IV]  23 Feb 2021\nNachev Cardoso\nlong-range relationships. Since the approach generalises to any sequentially organised data,\napplications in other areas such as computer vision are increasingly seen, with impressive re-\nsults in image classiﬁcation (Chen et al., 2020a; Dosovitskiy et al., 2020) and image synthesis\n(Child et al., 2019; Esser et al., 2020; Jun et al., 2020). The power to absorb relationships\nvarying widely in their distance makes transformers of potential value in the arguably the\nhardest of neuroimaging tasks: anomaly detection.\nThe detection and segmentation of lesions in neuroimaging support an array of clin-\nical tasks, including diagnosis, prognosis, treatment selection and mechanistic inference.\nHowever, the ﬁne characterisation of these lesions requires an accurate segmentation which\nis generally both ill-deﬁned and dependent on human expertise (Kamnitsas et al., 2017).\nManual segmentation is expensive and time-consuming to obtain, greatly limiting clinical\napplication, and the scale and inclusivity of available labelled data. Qualitative, informal\ndescriptions or reduced measurements are often used instead in clinical routine (Porz et al.,\n2014; Yuh et al., 2012). For this reason, the development of accurate computer-aided au-\ntomatic segmentation methods has become a major endeavour in medical image research\n(Menze et al., 2014). Most methods, however, depend on an explicitly deﬁned target class,\nand are sensitive to the scale and quality of available labelled data, a sensitivity ampliﬁed\nby the many sources of complex variability encountered in clinical neuroimaging. Under\nreal-world distributional shift, such models behave unpredictably, limiting clinical utility.\nIn recent years, many machine learning algorithms have been proposed for automatic\nanomaly detection. To overcome the necessity of expensive labelled data, unsupervised\nmethods have emerged as promising tools to detect arbitrary pathologies (Baur et al., 2018,\n2020b; Chen et al., 2020b; Pawlowski et al., 2018), relying mainly on deep generative models\nof normal data to derive a probability density estimate of the input data deﬁned by the\nlandscape of normality. Pathological features then register as deviations from normality,\navoiding the necessity for either labels or anomalous examples in training. The state of\nthe art is currently held by variational autoencoder (VAE)-based methods (Baur et al.,\n2020a) which try to reconstruct a test image as the nearest sample on the learnt normal\nmanifold, using the reconstruction error to quantify the degree and spatial distribution of\nany anomaly. This approach’s success is limited by the ﬁdelity of reconstructions from most\nVAE architectures (Dumoulin et al., 2016), and unwanted reconstructions of pathological\nfeatures not present in the training data, suggesting a failure of the model to internalise\ncomplex relationships between remote imaging features.\nIn an eﬀort to address these problems, we propose a method for unsupervised anomaly\ndetection and segmentation using transformers, where we learn the distribution of brain\nimaging data with an ensemble of Performers (Choromanski et al., 2020). We create and\nevaluate a robust method and compare its performance on synthetic and real datasets with\nrecent state-of-the-art unsupervised methods.\n2. Background\nThe core of the proposed anomaly detector is a highly expressive transformer that learns\nthe probability density function of 2D images of healthy brains. This requires us to express\neach image’s contents as a sequence of observations on which transformers-like models can\noperate. Owing to the size and complexity of brain imaging data, instead of learning the\n2\nTransformers for Anomaly Detection\ndistributions on individual pixels directly, we use the compact latent discrete representation\nof a vector quantised variational autoencoder (VQ-VAE) (Razavi et al., 2019; van den Oord\net al., 2017). This approach allows us to compress the input data into a spatially smaller\nquantised latent image, thus reducing the computational requirements and sequence length,\nmaking transformers feasible in neuroimaging applications.\n2.1. Vector quantized variational autoencoder\nThe VQ-VAE (Razavi et al., 2019; van den Oord et al., 2017) is a model that learns discrete\nrepresentations of images. It comprises an encoder E that maps observations x ∈RH×W\nonto a latent embedding space ˆz ∈Rh×w×nz , where nz is the dimensionality of each latent\nembedding vector. Then, an element-wise quantization is performed for each spatial code\nˆzij ∈Rnz onto its nearest vector ek , k ∈1, ...Kfrom a codebook, where K denotes the\nvocabulary size of the codebook. This codebook is learnt jointly with the other model\nparameters. A decoder G reconstructs the observations ˆx ∈RH×W from the quantized\nlatent space. We obtain the latent discrete representation zq ∈Rh×w by replacing each\ncode by its index k from the codebook.\n2.2. Transformers\nAfter training the VQ-VAE, we can learn the probability density function of the latent\nrepresentation of healthy brain images using transformers. The deﬁning characteristic of a\ntransformer is that it relies on attention mechanisms to capture the interactions between\ninputs, regardless of their relative position to one another. Each layer of the transformer\nconsists of a (self-)attention mechanism described by mapping an intermediate representa-\ntion with three vectors: query, key, and value vectors. Since the output of this attention\nmechanism relies on the inner products between all elements in the sequence, its computa-\ntional costs scale quadratically with the sequence length. Several “eﬃcient transformers”\nhave recently been proposed to reduce this computational requirement (Tay et al., 2020).\nOur study uses the Performer, a model which uses an eﬃcient (linear) generalized atten-\ntion framework implemented by the FAVOR+ algorithm (Choromanski et al., 2020). This\nframework provides a scalable estimation of attention mechanisms expressed by random\nfeature map decompositions, making transformers feasible for longer sequences, of the size\nneeded for neuroimaging data.\nTo model brain images, after assuming an arbitrary ordering to transform the latent\ndiscrete variables zq into a 1D sequence s, we can train the transformer to maximize the\ntraining data’s log-likelihood in an autoregressive fashion - similar to training performed\non language modelling task. This way, the transformer learns the distribution of codebook\nindices for a position i given all previous values p(si) = p(si|s<i).\n3. Proposed Method\n3.1. Anomaly Segmentation\nTo segment an anomaly in a previously unseen, test image, ﬁrst, we obtain the latent discrete\nrepresentation zq from the VQ-VAE model. Next, we reshape zq into a sequence s, and we\nuse the autoregressive transformer to obtain the likelihood of each latent variable valuep(si).\n3\nNachev Cardoso\nThese likelihood values indicate which latent variable has a low probability of occurring in\nnormal data. Using an arbitrary threshold (we empirically determined a threshold of 0.005\non a holdout set), we then can select indices with the lowest likelihood values and create a\n“resampling mask” indicating which latent variables are abnormal and should be corrected\nto produce a “healed” version of the image. We then replace these abnormal values with\nvalues sampled by the transformer. This approach attenuates the inﬂuence of the anomalies\nby replacing them by values that conform to the healthy distribution in the discrete latent\nspace. This in-painted latent space can then reconstruct ˆ x′ without the anomalies, in\n“healed” form. Finally, we obtain the pixel-wise residuals from the diﬀerence |x −ˆx′|. The\nanomaly in the new sample is ﬁnally segmented by thresholding the highest residuals values.\n3.2. Spatial information from the latent space\nAutoencoders are known for creating blurry reconstructions (Dumoulin et al., 2016). This\ncan cause residuals with high values in areas of the image with high frequencies. To avoid\nthese areas being mislabelled as anomalous, we used the spatial information present in the\nin-painted “resampling mask” of the latent space.\nThe resampling mask indicates the spatial location of the latent values with anomalies\naccording to the transformer model. Since our VQ-VAE is relatively shallow, the latent\nspace preserves most of the spatial information of the input data. Based on this, we used\nthe resampling mask to avoid mislabelling of high-frequency regions. First, we upscaled\nthe resampling mask from the latent space resolution to the input data resolution. Next,\nwe multiply the residuals with the mask. This approach cleans areas of the residuals that\nwere not speciﬁed as anomalous by our transformer but where the reconstructions might\nbe largely due to lack of VQ-VAE capacity.\n3.3. Multiple views of the latent space through reordering\nRecent studies have reported some limitations of likelihood models, such as our autoregres-\nsive model, in identifying out-of-distribution samples (Nalisnick et al., 2018). Inspired by\nChoi et al. (2018), we made the detection of the anomalies more robust by using an ensem-\nble of models. Using the same VQ-VAE model, we trained an ensemble of autoregressive\ntransformers. However, unlike Choi et al. (2018), each one of our transformers uses a diﬀer-\nent reordering of the 2D latent image to create a sequence. This compels each transformer\nto use a diﬀerent context of the latent image when predicting the likelihood of an element.\nIn our study, we focused on the raster scan class ordering. We obtain diﬀerent orderings\nby reﬂecting the image horizontally, vertically, and both ways at the same time. We also\ndeﬁne our orderings in images rotated 90 degrees, generating 8 diﬀerent orderings from a\nsingle latent representation. Each resampled latent representation is independently recon-\nstructed, i.e. each model independently creates a residuals map. We use the mean residual\nto segment the anomalies.\n3.4. Image-wise Anomaly Detection\nSo far, the proposed methodology has been focusing on segmenting abnormalities. However,\ntransformers can also be used to perform image-wise anomaly detection, i.e. detecting if an\n4\nTransformers for Anomaly Detection\nabnormality exists somewhere in the image. To do so, we use the likelihood predicted by the\ntransformers. Like the segmentation approach, ﬁrst, we obtain the 1D latent representation\ns. Then, we use the transformers to obtain the likelihood p(s) of each latent variable. To\nobtain the log-likelihood image-wise, we compute logp(x) = logp(s) = ∑\ni logp(si). Finally,\nwe combined the predicted log-likelihood of each transformer (per orientation/ordering) by\ncomputing the mean value.\n4. Experiments and Results\n4.1. Experiment #1 – Anomaly Segmentation on Synthetic Data\nTo assess anomaly segmentation performance on synthetic data, we utilized a subsample\nof the MedNIST dataset 1, where we used the images of the “HeadCT” category to train\nour models. Our test set comprised 100 images contaminated with sprites (Matthey et al.,\n2017), thus producing ground-truth abnormality masks. We measure the performance using\nthe best achievable DICE-score (⌈DICE ⌉), which constitutes a theoretical upper-bound to\na model’s segmentation performance and is obtained via a greedy search for the residual\nthreshold which yields the highest DICE-score on the test set. We compared our results\nagainst state-of-the-art autoencoder models based on the architectures proposed in the\n(Baur et al., 2020a) comparison study, assessed in the same manner. We also performed an\nablation study of the proposed method demonstrating the values of each contribution.\nTable 1: Methods on anomaly segmentation using the synthetic dataset. The performance\nis measured with best achievable DICE-score on the test set.\nMethod ⌈DICE⌉\nAE (Dense) (Baur et al., 2020a) 0.213\nAE (Spatial) (Baur et al., 2020a) 0.165\nVAE (Dense) (Baur et al., 2020a) 0.533\nVQ-VAE (van den Oord et al., 2017) 0.457\nVQ-VAE + Transformer (Ours) 0.675\nVQ-VAE + Transformer + Masked Residuals (Ours) 0.768\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.895\nAs presented in Table 1, the models based only on autoencoders had a highest ⌈DICE ⌉\nof 0.533 (VAE). We observed an improvement in performance when using the transformer\nto in-paint the latent space, changing the VQ-VAE only performance from 0.457 to 0.675.\nThe spatial information in the resampling mask also contributed by attenuating the impact\nof the blurry reconstructions (Figure 1), achieving a 0.768 score. Finally, the variability\nof the generative models with diﬀerent orderings gave another boost in performance (for 8\ndiﬀerent raster ordering models ⌈DICE ⌉=0.895).\n4.2. Experiment #2 – Image-wise Anomaly Detection on Synthetic Data\nNext, we evaluated our method to detect anomalous (out-of-distribution - OOD) images,\nagain on a synthetic setting. Using the same models trained from Experiment #1, we ob-\ntained the mean log-likelihood for each image. We used 1,000 images from the HeadCT class\n1. Available athttps://github.com/Project-MONAI/tutorials\n5\nNachev Cardoso\nFigure 1: Residual maps on the synthetic dataset from the variational autoencoder and\ndiﬀerent steps of our approach.\nas the in-distribution test set, the 100 HeadCT images contaminated by sprite anomalies\nas the near OOD set, and 1.000 images of each other MedNIST classes as the far OOD set\n(see the appendix for details). We use the area under the receiver operating characteristic\ncurve (AUROC) as performance metric, with in-distribution test set and out-of-distribution\nbeing the labels. This way, we have a threshold-independent evaluation metric. We also\nmeasure the area under the precision recall curve (AUCPRC), where it provides a mean-\ningful measure for detection performance in the presence of heavy class-imbalance. Finally,\nwe also computed the false positive rate of anomalous examples when the true positive rate\nof in-distribution examples is at 95% (FPR95), 99% (FPR99) and 99.9% (FPR999).\nTable 2: Performance of the methods on image-wise anomaly detection using the synthetic\ndataset.\nAUCROCAUCPR\nIn\nAUCPR\nOut FPR95 FPR99 FPR999\nvs. far OOD classes\nVAE (Dense) (Baur et al., 2020a) 0.298 0.855 0.060 0.986 0.996 0.996\nOur approach 1.000 1.000 1.000 0.000 0.001 0.004\nOur approach with general purpose VQ-VAE1.000 1.000 1.000 0.000 0.000 0.000\nvs. near OOD classes\nVAE (Dense) (Baur et al., 2020a) 0.111 0.094 0.672 1.000 1.000 1.000\nOur approach 0.921 0.988 0.707 0.409 0.885 0.885\nOur approach with general purpose VQ-VAE0.932 0.990 0.721 0.482 0.882 0.882\nTable 2 shows that our transformer-based method achieved an AUCROC of 0.921 and\n1.000 for near OOD and far OOD, respectively. This is a improvement compared to a\nmethod based on the error of reconstruction obtained from a VAE model. We also evaluated\nour method with a VQ-VAE trained to reconstruct all the categories from the MedNIST\ndataset (“general purpose VQ-VAE”) and the ensemble of transformers trained on HeadCT\nimages only. In this conﬁguration, we try to mitigate the inﬂuence of the encoder in the\nanomaly detection. This approach would reduce the ability of the encoder to map an\nOOD image to a familiar in-distribution latent representation, which could possibly aﬀect\n6\nTransformers for Anomaly Detection\nthe transformer performance. This new conﬁguration achieves a slight better performance\n(AUCROC=0.932 for near OOD and AUCROC=1.000 for far OOD).\n4.3. Experiment #3 – Anomaly Segmentation on Real Neuroimaging Data\nTo evaluate our method’s performance on real world lesion data, we used the FLAIR images\nfrom the UK Biobank (UKB) dataset (Sudlow et al., 2015). We selected the 15,000 subjects,\nand their respective FLAIR images, with the lowest white matter hyperintensities volume,\nas provided by UKB, to train our models, as these subjects were the most radiologically\nnormal. Then, we used 18,318 subjects from the remaining UKB dataset to evaluate our\nmethod to detect white matter hyperintensities (WMH).\nIn order to test for model generalisability, we also evaluated our method on three other\ndatasets that also had FLAIR imaging data: the Multiple Sclerosis dataset from the Uni-\nversity Hospital of Ljubljana (MSLUB) dataset (Lesjak et al., 2018), which contains mul-\ntiple sclerosis lesions; the White Matter Hyperintensities Segmentation Challenge (WMH)\ndataset (Kuijf et al., 2019); and the Multimodal Brain Tumor Image Segmentation Bench-\nmark (BRATS) dataset (Bakas et al., 2017, 2018; Menze et al., 2014) that contain tumours\n(more information about the datasets and pre-processing on appendix A).\nTable 3: Results of the anomaly segmentation using real lesion data. We compared our\nmodels against the state-of-the-art autoencoder models based on the architecture proposed\nin Baur et al. (2020a). We measured the performance using the theoretically best possible\nDICE-score (⌈DICE ⌉) on each dataset.\nUKB Dataset ⌈DICE⌉\nAE (Dense) (Baur et al., 2020a) 0.016\nAE (Spatial) (Baur et al., 2020a) 0.054\nVAE (Dense) (Baur et al., 2020a) 0.016\nVQ-VAE (van den Oord et al., 2017) 0.028\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.232\nMSLUB Dataset\nAE (Dense) (Baur et al., 2020a) 0.041\nAE (Spatial) (Baur et al., 2020a) 0.061\nVAE (Dense) (Baur et al., 2020a) 0.039\nVQ-VAE (van den Oord et al., 2017) 0.040\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.378\nBRATS Dataset\nAE (Dense) (Baur et al., 2020a) 0.276\nAE (Spatial) (Baur et al., 2020a) 0.531\nVAE (Dense) (Baur et al., 2020a) 0.294\nVQ-VAE (van den Oord et al., 2017) 0.331\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.759\nWMH Dataset\nAE (Dense) (Baur et al., 2020a) 0.073\nAE (Spatial) (Baur et al., 2020a) 0.150\nVAE (Dense) (Baur et al., 2020a) 0.068\nVQ-VAE (van den Oord et al., 2017) 0.100\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.429\nOur method showed a better performance than the autoencoder approaches from the\nliterature in all datasets (Table 1 and Figure 2). Compared to the numbers in Baur et al.\n7\nNachev Cardoso\n(2020a), our autoencoder-based models got a lower performance on the common dataset\n(MSLUB dataset), where they achieved an DICE score of 0.271 with the AE (dense), 0.154\nwith the AE (spatial), and 0.323 with the VAE (dense). We believe that the discrep-\nancy comes mostly from the signiﬁcant post-processing of the Baur et al. (2020a) work as\npresented in Table 8 of this reference. Diﬀerences might also arise from the diﬀerence in\nresolution, as the DICE score is not invariant to resolution.\nFigure 2: Residual maps on the real lesions from the variational autoencoder and our\ntransformer-based method.\n5. Conclusion\nAutomatically determining the presence of lesion and delineating their boundaries is es-\nsential to the introduction complex models of rich neuroimaging features in clinical care.\nIn this study, we propose a novel transformer-based approach for anomaly detection and\nsegmentation which achieves state-of-the-art results in all tested tasks when compared to\ncompeting methods. Transformers are making impressive gains in image analysis, and here\nwe show that their use to identify anomalies holds great promise. We hope that our work\nwill inspire further investigation of the properties of transformers for anomaly detection in\nmedical images, the development of new network designs, exploration of a wider variety of\nconditioning information, and the application of transformers to other medical data.\n8\nTransformers for Anomaly Detection\nAcknowledgments\nWHLP and MJC are supported by Wellcome Innovations [WT213038/Z/18/Z]. PTD is sup-\nported by the EPSRC Research Council, part of the EPSRC DTP, grant Ref: [EP/R513064/1].\nPN is supported by Wellcome Innovations [WT213038/Z/18/Z] and the UCLH NIHR\nBiomedical Research Centre. This research has been conducted using the UK Biobank\nResource (Project number: 58292).\nReferences\nFidel Alfaro-Almagro, Mark Jenkinson, Neal K Bangerter, Jesper LR Andersson, Ludovica\nGriﬀanti, Gwena¨ elle Douaud, Stamatios N Sotiropoulos, Saad Jbabdi, Moises Hernandez-\nFernandez, Emmanuel Vallee, et al. Image processing and quality control for the ﬁrst\n10,000 brain imaging datasets from uk biobank. Neuroimage, 166:400–424, 2018.\nBrian B Avants, Nicholas J Tustison, Gang Song, Philip A Cook, Arno Klein, and James C\nGee. A reproducible evaluation of ants similarity metric performance in brain image\nregistration. Neuroimage, 54(3):2033–2044, 2011.\nSpyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S\nKirby, John B Freymann, Keyvan Farahani, and Christos Davatzikos. Advancing the\ncancer genome atlas glioma mri collections with expert segmentation labels and radiomic\nfeatures. Scientiﬁc data, 4(1):1–13, 2017.\nSpyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus Rempﬂer, Alessan-\ndro Crimi, Russell Takeshi Shinohara, Christoph Berger, Sung Min Ha, Martin Rozycki,\net al. Identifying the best machine learning algorithms for brain tumor segmentation, pro-\ngression assessment, and overall survival prediction in the brats challenge. arXiv preprint\narXiv:1811.02629, 2018.\nChristoph Baur, Benedikt Wiestler, Shadi Albarqouni, and Nassir Navab. Deep autoencod-\ning models for unsupervised anomaly segmentation in brain mr images. In International\nMICCAI Brainlesion Workshop , pages 161–169. Springer, 2018.\nChristoph Baur, Stefan Denner, Benedikt Wiestler, Nassir Navab, and Shadi Albarqouni.\nAutoencoders for unsupervised anomaly segmentation in brain mr images: A comparative\nstudy. Medical Image Analysis, page 101952, 2020a.\nChristoph Baur, Benedikt Wiestler, Shadi Albarqouni, and Nassir Navab. Scale-space au-\ntoencoders for unsupervised anomaly segmentation in brain mri. In International Confer-\nence on Medical Image Computing and Computer-Assisted Intervention , pages 552–561.\nSpringer, 2020b.\nMark Chen, Alec Radford, Rewon Child, Jeﬀrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In International Conference on Machine\nLearning, pages 1691–1703. PMLR, 2020a.\n9\nNachev Cardoso\nXiaoran Chen, Suhang You, Kerem Can Tezcan, and Ender Konukoglu. Unsupervised\nlesion detection via image restoration with a normative prior. Medical image analysis ,\n64:101713, 2020b.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences\nwith sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.\nHyunsun Choi, Eric Jang, and Alexander A Alemi. Waic, but why? generative ensembles\nfor robust anomaly detection. arXiv preprint arXiv:1810.01392 , 2018.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Re-\nthinking attention with performers. arXiv preprint arXiv:2009.14794 , 2020.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain\nGelly, et al. An image is worth 16x16 words: Transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929 , 2020.\nVincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Mar-\ntin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint\narXiv:1606.00704, 2016.\nPaul Elliott and Tim C Peakman. The uk biobank sample handling and storage protocol for\nthe collection, processing and archiving of human blood and urine. International journal\nof epidemiology, 37(2):234–244, 2008.\nPatrick Esser, Robin Rombach, and Bj¨ orn Ommer. Taming transformers for high-resolution\nimage synthesis. arXiv preprint arXiv:2012.09841 , 2020.\nLudovica Griﬀanti, Giovanna Zamboni, Aamira Khan, Linxin Li, Guendalina Bonifacio,\nVaanathi Sundaresan, Ursula G Schulz, Wilhelm Kuker, Marco Battaglini, Peter M Roth-\nwell, et al. Bianca (brain intensity abnormality classiﬁcation algorithm): A new tool for\nautomated segmentation of white matter hyperintensities. Neuroimage, 141:191–205,\n2016.\nMark Jenkinson, Christian F Beckmann, Timothy EJ Behrens, Mark W Woolrich, and\nStephen M Smith. Fsl. Neuroimage, 62(2):782–790, 2012.\nHeewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh, Alec Radford,\nand Ilya Sutskever. Distribution augmentation for generative modeling. In International\nConference on Machine Learning, pages 5006–5019. PMLR, 2020.\nKonstantinos Kamnitsas, Christian Ledig, Virginia FJ Newcombe, Joanna P Simpson, An-\ndrew D Kane, David K Menon, Daniel Rueckert, and Ben Glocker. Eﬃcient multi-scale\n3d cnn with fully connected crf for accurate brain lesion segmentation. Medical image\nanalysis, 36:61–78, 2017.\n10\nTransformers for Anomaly Detection\nHugo J Kuijf, J Matthijs Biesbroek, Jeroen De Bresser, Rutger Heinen, Simon Andermatt,\nMariana Bento, Matt Berseth, Mikhail Belyaev, M Jorge Cardoso, Adria Casamitjana,\net al. Standardized assessment of automatic segmentation of white matter hyperintensities\nand results of the wmh segmentation challenge. IEEE transactions on medical imaging ,\n38(11):2556–2568, 2019.\nˇZiga Lesjak, Alﬁia Galimzianova, Aleˇ s Koren, Matej Lukin, Franjo Pernuˇ s, Boˇ stjan Likar,\nand ˇZiga ˇSpiclin. A novel public mr image dataset of multiple sclerosis patients with lesion\nsegmentations based on multi-rater consensus. Neuroinformatics, 16(1):51–63, 2018.\nLoic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentan-\nglement testing sprites dataset, 2017.\nBjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Fara-\nhani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al.\nThe multimodal brain tumor image segmentation benchmark (brats). IEEE transactions\non medical imaging, 34(10):1993–2024, 2014.\nKarla L Miller, Fidel Alfaro-Almagro, Neal K Bangerter, David L Thomas, Essa Yacoub,\nJunqian Xu, Andreas J Bartsch, Saad Jbabdi, Stamatios N Sotiropoulos, Jesper LR\nAndersson, et al. Multimodal population brain imaging in the uk biobank prospective\nepidemiological study. Nature neuroscience, 19(11):1523–1536, 2016.\nEric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshmi-\nnarayanan. Do deep generative models know what they don’t know? arXiv preprint\narXiv:1810.09136, 2018.\nNick Pawlowski, MC Lee, Martin Rajchl, Steven McDonagh, Enzo Ferrante, Konstantinos\nKamnitsas, Sam Cooke, Susan Stevenson, Aneesh Khetani, Tom Newman, et al. Unsu-\npervised lesion detection in brain ct using bayesian convolutional autoencoders. Medical\nImaging with Deep Learning , 2018.\nNicole Porz, Stefan Bauer, Alessia Pica, Philippe Schucht, J¨ urgen Beck, Rajeev Kumar\nVerma, Johannes Slotboom, Mauricio Reyes, and Roland Wiest. Multi-modal glioblas-\ntoma segmentation: man versus machine. PloS one, 9(5):e96873, 2014.\nFerran Prados, Manuel Jorge Cardoso, Baris Kanber, Olga Ciccarelli, Raju Kapoor, Claudia\nAM Gandini Wheeler-Kingshott, and Sebastien Ourselin. A multi-time-point modality-\nagnostic patch-based method for lesion ﬁlling in multiple sclerosis. Neuroimage, 139:\n376–384, 2016.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018.\nAlec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAli Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images\nwith vq-vae-2. arXiv preprint arXiv:1906.00446 , 2019.\n11\nNachev Cardoso\nCathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh,\nPaul Downey, Paul Elliott, Jane Green, Martin Landray, et al. Uk biobank: an open\naccess resource for identifying the causes of a wide range of complex diseases of middle\nand old age. Plos med, 12(3):e1001779, 2015.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng\nRao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark\nfor eﬃcient transformers. arXiv preprint arXiv:2011.04006 , 2020.\nA¨ aron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation\nlearning. CoRR, abs/1711.00937, 2017. URL http://arxiv.org/abs/1711.00937.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\nJoanna M Wardlaw, Eric E Smith, Geert J Biessels, Charlotte Cordonnier, Franz Fazekas,\nRichard Frayne, Richard I Lindley, John T O’Brien, Frederik Barkhof, Oscar R Be-\nnavente, et al. Neuroimaging standards for research into small vessel disease and its\ncontribution to ageing and neurodegeneration. The Lancet Neurology , 12(8):822–838,\n2013.\nEsther L Yuh, Shelly R Cooper, Adam R Ferguson, and Geoﬀrey T Manley. Quantitative\nct improves outcome prediction in acute traumatic brain injury. Journal of neurotrauma,\n29(5):735–746, 2012.\nAppendix A. Experimental Details and Further Analysis\nA.1. Experiment #1 – Anomaly Segmentation on Synthetic Data\nDataset To assess the performance on anomaly segmentation, we utilized a subsample of\nthe MedNIST dataset, where we used the 2D images of the HeadCT category to train our\nVQ-VAE and transformer models (Figure A1). From the original 10,000 HeadCT images,\nwe used 8,000 images as the training set and 1,000 images for the validation set. The\ntest set was comprised of 100 images contaminated with sprites (i.e., synthetic anomalies)\nobtained from the dsprites dataset (Matthey et al., 2017). We selected the sprites images\nthat overlapped a signiﬁcant portion of the head, and their values were set as 0 or 1.\nModels Our VQ-VAE models had a similar architecture from van den Oord et al. (2017).\nThe encoder consists of three strided convolutional layers with stride 2 and window size 4\n× 4. All these convolution layers had a ReLU activation following them. This structure is\nfollowed by two residual 3 ×3 blocks (implemented as 3 ×3 conv, ReLU, 1 ×1 conv, ReLU).\nThe decoder similarly has two residual 3 ×3 blocks, followed by three transposed convolu-\ntions with stride 2 and window size 4 ×4. All the convolution layers have 256 hidden units.\nThe inputted images have the dimension of 64x64 pixels which result in a latent represen-\ntation of 8x8 latent variables. For this experiment, we used a codebook with 16 diﬀerent\ncodes. Our performers corresponded to the transformer’s decoder structure with 24 layers\n12\nTransformers for Anomaly Detection\nFigure A1: Examples of the training set (HeadCT class) from the MedNIST dataset.\nwith an embedding size of 256. The embedding, feed-forward and attention dropout had a\nprobability of 0.1.\nState-of-the-art Models We compared our models against state-of-the-art autoencoder-\nbased methods (AE dense, AE spatial, and VAE). In this experiment, we used an uniﬁed\nnetwork architecture adapted from a recent comparison study (Baur et al., 2020a). Since\nMedNIST images are smaller than those used in the comparison study, our models did not\nhave the ﬁrst block (resolution of 64x64x32) and the last block (resolution of 128x128x32).\nAll the other blocks and layers are similar to the original study. To train these models, we\nused the ADAM optimiser with learning rate 5e-4, an exponential learning rate decay with\ngamma of 0.9999, and we trained over 1,500 epochs with batch-size 256. Similar to Baur\net al. (2020a), we used the held-out validation set to select the models to be assessed.\nTraining settings To train the VQ-VAE models, we use the ADAM optimiser with\nlearning rate 5e-4, an exponential learning rate decay with gamma of 0.999, and we trained\nover 1,500 epochs with batch-size 256. We train the codebook using an exponential moving\naverage algorithm. To stabilise the codebook’s learning, in the ﬁrst 100 epoch, we warm up\nthe moving average decay from a gamma decay of 0.5, gradually increasing to a gamma decay\nof 0.99. This allows the codes to adapt faster to the frequent changes at the beginning of\nthe training. To train the performers, we used the ADAM optimiser with learning rate 5e-4,\nan exponential learning rate decay with gamma of 0.9999, and we trained over 1,500 epochs\nwith batch-size 128. We used data augmentation to increase the number of training images.\nWe randomly performed aﬃne transformations (scale, translate, and rotate operations) and\nhorizontally ﬂip the images.\nDiﬀerent Ordering Classes In our study, we also analysed 3 other classes of orderings\n(Figure A2): a S-curve order that traverses rows in alternating directions, and a Hilbert\nspace-ﬁlling curve order that generates nearby pixels in the image consecutively. In all\nthese classes, and a random class, where the sequence of latent variables is randomly sorted.\nSimilar to the raster class, we augmented the number of possible orderings by reﬂecting and\ntransposing the images, generating in total 8 diﬀerent orderings per class.\n13\nNachev Cardoso\nFigure A2: Diﬀerent orderings used to transform the 2D discrete representation into a\nsequence.\nIn the Table A1, we can observe the performance of each class. The orderings had\na performance varying from 0.843 to 0.895. We can observe that the random ordering\nperformed with the lowest performance. This was expected since we would expect that\nlocal information of the image would help the transformer modelling of the healthy data.\nSince the random ordering may not include the local data in the context to predict a variable\nvalue autoregressively, this might reduce its performance as anomaly detector too. Finally,\nwe evaluated the performance when combining all the orderings. It was observed a small\ngain when using an ensemble of all four classes compared to the raster class only. We opt to\nuse the raster ordering in the main analysis to reduce the time of training and processing.\nTable A1: Performance of diﬀerent classes of ordering and the ensemble with all classes.\nMethod ⌈DICE ⌉\n8 diﬀerent raster orderings 0.895\n8 diﬀerent S-curve orderings 0.883\n8 diﬀerent Hilbert curve orderings 0.890\n8 diﬀerent random orderings 0.843\n32 diﬀerent orderings 0.899\nSame ordering but diﬀerent random seed The ensemble of 8 transformers gave a\nboost in performance in the segmentation comparing with a single transformer. To verify\nif it was due to the variability of the generative models with diﬀerent orderings instead of\nusing an ensemble with more models, we trained 8 models using the same raster ordering\nbut with diﬀerent random seed. We can observe a drop in performance when using an\nensemble of transformers using the same ordering but diﬀerent random seeds, from 0.895\nto 0.826.\nAnomalies intensity We also evaluated the inﬂuence of the synthetic anomalies’ inten-\nsity and texture. For this, we varied the intensity of the sprites in the image from 0 to 1 and\nmeasured the segmentation performance (best achievable DICE-score). We also performed\nthis approach with the addition of an additive Gaussian noise with standard deviation of\n0.2. From Figure A3, we can observe that our transformer-based method is more robust to\nthe change in intensity with a narrow drop in performance when the anomaly intensity is\ncloser to the tissue mean values.\n14\nTransformers for Anomaly Detection\nFigure A3: Analysis of the intensity of the anomalies A) with and B) without the additive\nnoise.\nA.2. Experiment #2 – Image-wise Anomaly Detection on Synthetic Data\nDataset In this experiment, we used the same training set from the Experiment #1. For\nevaluation, we used 1,000 images from the HeadCT class as the in-distribution test set,\nthe 100 HeadCT images contaminated by sprites anomalies as the near out-of-distribution\nset (near OOD), and 1,000 images of each other classes from the MedNIST dataset (Ab-\ndomenCT, BreastMRI, CXR, ChestCT, and Hand) as the far out-of-distribution set (far\nOOD) (Figure A4). To train our general purpose VQ-VAE, we added 8,000 images from\neach other classes to our training set and 1,000 images to our validation set.\nModels and Training Settings To train the VQ-VAE with general purpose and its\ntransformers, we used the same architecture, and the same training settings from the models\nfrom Experiment #1.\nPerformance of Experiment #1 models on Anomaly Detection Besides the VAE,\nin Table A2 we also present the performance of the other autoencoder models from Exper-\niment #1 to perform the anomaly detection task.\nImpact of the general VQ-VAE on anomaly segmentation The VQ-VAE with\ngeneral purpose reduce the impact of the encoder to the analysis, working mainly as a\ncompression mechanism. In this analysis, we evaluate the performance of this models when\nperforming anomaly segmentation. Using the general purpose VQ-VAE, we obtained a\nDICE score of 0.886, just a small decrease compared to the models trained only on HeadCT\ndata.\n15\nNachev Cardoso\nFigure A4: Examples from the near out-of-distribution set (near OOD) and far out-of-\ndistribution set (far OOD).\nTable A2: Performance of the methods on image-wise anomaly detection.\nAUCROCAUCPR\nIn\nAUCPR\nOut FPR95 FPR99 FPR999\nvs. far OOD classes\nAE (Dense) (Baur et al., 2020a) 0.351 0.875 0.066 0.969 0.987 0.987\nAE (Spatial) (Baur et al., 2020a) 0.337 0.874 0.063 0.959 0.998 0.998\nVAE (Dense) (Baur et al., 2020a) 0.298 0.855 0.060 0.986 0.996 0.996\nVQ-VAE (van den Oord et al., 2017) 0.241 0.834 0.056 0.987 0.996 0.996\nvs. near OOD classes\nAE (Dense) (Baur et al., 2020a) 0.106 0.093 0.669 1.000 1.000 1.000\nAE (Spatial) (Baur et al., 2020a)0.215 0.103 0.721 1.000 1.000 1.000\nVAE (Dense) (Baur et al., 2020a) 0.111 0.094 0.672 1.000 1.000 1.000\nVQ-VAE (van den Oord et al., 2017) 0.024 0.089 0.645 1.000 1.000 1.000\nA.3. Experiment #3 – Anomaly Segmentation on Real Data\nMRI Datasets In our experiment, we used three datasets: the UK Biobank (UKB)\ndataset (Sudlow et al., 2015), the White Matter Hyperintensities Segmentation Challenge\n(WMH) dataset (Kuijf et al., 2019), the Multimodal Brain Tumor Image Segmentation\nBenchmark (BRATS) dataset (Bakas et al., 2017, 2018; Menze et al., 2014), and the Multiple\nSclerosis dataset from the University Hospital of Ljubljana (MSLUB) dataset (Lesjak et al.,\n2018) (Figure A5).\nThe UKB is a study that aims to follow the health and well-being of 500,000 volunteer\nparticipants across the United Kingdom. From these participants, a subsample was chosen\nto collect multimodal imaging, including structural neuroimaging. Here, we used an early\nrelease of the project’s data comprising of 33,318 HC participants. More details about the\ndataset and imaging acquisition can be found elsewhere (Alfaro-Almagro et al., 2018; Elliott\nand Peakman, 2008; Miller et al., 2016). The UK Biobank dataset has available a mask\nfor hyperintensities white matter lesions obtained using BIANCA (Griﬀanti et al., 2016;\nJenkinson et al., 2012). We selected the 15k subjects with the lowest lesion volume to train\nour VQ-VAE model.\n16\nTransformers for Anomaly Detection\nFigure A5: Examples from the test set of the Experiment #3.\nThe BRATS challenge is an initiative that aims to evaluate methods for the segmentation\nof brain tumours by providing a 3D MRI dataset with ground truth tumour segmentation\nannotated by expert board-certiﬁed neuroradiologists (Bakas et al., 2017, 2018; Menze et al.,\n2014). Our study used the 2018 version of the dataset composed by the MR scans of 420\npatients with glioblastoma or lower grade glioma. The images were acquired with diﬀerent\nclinical protocols and various scanners from multiple (n=19) institutions. Note, the available\nimages from the BRATS dataset were already skull stripped.\nThe WHM dataset is an initiative to directly compare automated WMH segmentation\ntechniques (Kuijf et al., 2019). The dataset was acquired from ﬁve diﬀerent scanners from\nthree diﬀerent vendors in three diﬀerent hospitals in the Netherlands and Singapore. It\nis composed by 60 subjects where the WMH were manually segmented according to the\nSTandards for ReportIng Vascular changes on nEuroimaging (STRIVE) (Wardlaw et al.,\n2013). The MSLUB dataset is a publicly available dataset for validation of lesion segmen-\ntation methods. The dataset consists of 30 images from multiple sclerosis patients that\nwere acquired using conventional MR imaging sequences. For each case, a reference lesion\nsegmentation was created by three independent raters and merged into a consensus. This\nway, we have access to a precise and reliable target to evaluate segmentation methods. Full\ndescription regarding data acquisition and imaging protocol can be found at Lesjak et al.\n(2018).\nIn our study, we used the FLAIR images from these three datasets to evaluate our\nmodels. For each of these FLAIR images, the UKB and WMH datasets had a white matter\nhyperintensity segmentation map, the BRATS dataset had a tumour segmentation map,\nand the MSLUB dataset had a white-matter lesion segmentation map. We also used T1w\nand brain mask to perform the MRI pre-processing.\n17\nNachev Cardoso\nMRI Pre-processing We pre-process our images to be normalized in a common space.\nFor this reason, all scans and lesion masks were registered to MNI space using rigid + aﬃne\ntransformation. This registration was performed using the Advanced Normalisations Tools\n(ANTs - version 2.3.4) (Avants et al., 2011). Since our anomaly segmentation method relies\non a training set composed by a population with a low occurrence of lesions and anomalies,\nwe tried to minimize the occurrence of lesions on the transformers’ training set. For this\nreason, after the traditional MRI pre-processing, we used the NiftySeg package (version 1.0)\n(Prados et al., 2016) to mitigate the inﬂuence of the lesions present our training set. Using\nthe seg FillLesions function and the lesion maps supplied by the UKB dataset, we in-painted\nthe few white matter hyperintensities present in the FLAIR images using a non-local lesion\nﬁlling strategy based on a patch based in-painting technique for image completion. Since the\nVQ-VAE performs mainly a dimensionality reduction in our method, it was trained using\nthe normalized dataset without the NiftySeg in-painting. We believe that the presence of\nthe lesions in the VQ-VAE training set is important to avoid that the autoencoder method\navoids performing the correction on its encoding part. If the encoder corrects the code by\nitself, the transformer would not be able to detect the presence of a lesion. This missing\ndetection would result in a resampling mask that ﬁlters out the encoder correction. On\nExperiment #2, we show that this approach does not prejudice the performance of the\nsegmentation. Finally, we selected 4 axials slices (z=89, 90, 91, 92) per FLAIR image and,\nwe center cropped these slices to have the dimensions of 224 x 224 pixels.\nModels Our VQ-VAE models had a similar architecture from the Experiment #1 but with\nthree residual 3×3 blocks (instead 2). All the convolution layers had 256 hidden units. The\ninputted images had 224x224 pixels which result in a latent representation of 28x28 latent\nvariables. For this experiment, we used a codebook with 32 diﬀerent codes. Our performers\ncorresponded to the transformer’s decoder structure with 16 layers with an embedding size\nof 256. The embedding, feed-forward and attention dropout had a probability of 0.3.\nTraining settings To train the VQ-VAE models, we use the ADAM optimiser with\nlearning rate 1e-3, an exponential learning rate decay with gamma of 0.99995, and we\ntrained over 500 epochs with batch-size 256. We train the codebook similar to Experiment\n#1. To train the performers, we used the ADAM optimiser with learning rate 1e-3, an\nexponential learning rate decay with gamma of 0.99992, and we trained over 150 epochs\nwith batch-size 128. We used data augmentation to increase the number of training images.\nWe randomly performed small translation transformations as well as random intensity shift\nand random adjusts in contrast.\nState-of-the-art Models In this experiment, we used the network architecture from\nBaur et al. (2020a) for the autoencoder only-based approaches. Even with our bigger input\nimages, we opted to use the same instead a version with an extra downsampling step to\navoid a model structure with a bottleneck with a much smaller resolution than our proposed\nmethod. To train these models, we used the ADAM optimiser with learning rate 1e-3, an\nexponential learning rate decay with gamma of 0.99995, and we trained over 1,500 epochs\nwith batch-size 256. Similar to Baur et al. (2020a), we used the held-out validation set to\nselect the models to be assessed.\n18\nTransformers for Anomaly Detection\nImpact of Mitigating Lesions on the Training set In our pre-processing, we in-\npainted the white matter hyperintensity of the training set using the NiftySeg package, as\nto simulate completely lesion-free data. Here, we compare the performance of our method\nwithout this step. By skipping this pre-processing step and training the transformers on the\nnew dataset, we can observe a drop in performance, from 0.232 to 0.051 in the UKB dataset,\nfrom 0.378 to 0.264 in the MSLUB dataset, from 0.759 to 0.677 in the BRATS dataset, and\nfrom 0.429 to 0.349 in the WMH dataset. We believe that the highly expressive transformers\ncan learn the few white matter hyperintensities present in the dataset and associate a higher\nprobability of occurrence, reducing the performance in detection.\nPerformance of diﬀerent stages of the method Like Experiment #1, we evaluated\nthe performance of each step of our method. From Table A3, we can observe that each step\npresents an incremental improvement.\nTable A3: Performance of each processing step of our transformer-based approach in the\nExperiment #3\nUKB Dataset ⌈DICE⌉\nVQ-VAE (van den Oord et al., 2017) 0.028\nVQ-VAE + Transformer (Ours) 0.079\nVQ-VAE + Transformer + Masked Residuals (Ours) 0.104\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.232\nMSLUB Dataset\nVQ-VAE (van den Oord et al., 2017) 0.040\nVQ-VAE + Transformer (Ours) 0.097\nVQ-VAE + Transformer + Masked Residuals (Ours) 0.234\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.378\nBRATS Dataset\nVQ-VAE (van den Oord et al., 2017) 0.331\nVQ-VAE + Transformer (Ours) 0.431\nVQ-VAE + Transformer + Masked Residuals (Ours) 0.476\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.759\nWMH Dataset\nVQ-VAE (van den Oord et al., 2017) 0.100\nVQ-VAE + Transformer (Ours) 0.205\nVQ-VAE + Transformer + Masked Residuals (Ours) 0.269\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.429\nPost-processing Impact Similar to Baur et al. (2020a), we veriﬁed the performance\nof the methods using the prior knowledge that multiple sclerosis lesions would appear as\npositive residuals as these lesions appear as hyper-intense in FLAIR images. We assumed\nthe same for the WMH lesions. By using only the positive values of the residuals as a\npost-processing step, we got a gain on performance on both the autoencoders only based\nmethods and our approach (Table A4).\n19\nNachev Cardoso\nTable A4: Performance of each method using post-processing step.\nUKB Dataset ⌈DICE⌉\nAE (Dense) (Baur et al., 2020a) 0.079\nAE (Spatial) (Baur et al., 2020a) 0.054\nVAE (Dense) (Baur et al., 2020a) 0.071\nVQ-VAE (van den Oord et al., 2017) 0.056\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.297\nMSLUB Dataset\nAE (Dense) (Baur et al., 2020a) 0.106\nAE (Spatial) (Baur et al., 2020a) 0.067\nVAE (Dense) (Baur et al., 2020a) 0.106\nVQ-VAE (van den Oord et al., 2017) 0.077\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.465\nWMH Dataset\nAE (Dense) (Baur et al., 2020a) 0.166\nAE (Spatial) (Baur et al., 2020a) 0.151\nVAE (Dense) (Baur et al., 2020a) 0.161\nVQ-VAE (van den Oord et al., 2017) 0.143\nVQ-VAE + Transformer + Masked Residuals + Diﬀerent Orderings (Ours)0.441\n20\nTransformers for Anomaly Detection\nAppendix B. More Residuals maps from Experiment #1\nFigure A6: More examples of residual maps on the synthetic dataset from the variational\nautoencoder and diﬀerent steps of our approach.\n21\nNachev Cardoso\nAppendix C. More Residuals maps from Experiment #3\nFigure A7: More examples of residual maps on the real lesions from the variational autoen-\ncoder and our method.\n22",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6759555339813232
    },
    {
      "name": "Anomaly detection",
      "score": 0.6534373164176941
    },
    {
      "name": "Segmentation",
      "score": 0.5911546945571899
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5826441049575806
    },
    {
      "name": "Normality",
      "score": 0.5458751320838928
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.522347092628479
    },
    {
      "name": "Autoregressive model",
      "score": 0.49357283115386963
    },
    {
      "name": "Transformer",
      "score": 0.47649091482162476
    },
    {
      "name": "Machine learning",
      "score": 0.3422638177871704
    },
    {
      "name": "Mathematics",
      "score": 0.13759082555770874
    },
    {
      "name": "Engineering",
      "score": 0.08447697758674622
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ]
}