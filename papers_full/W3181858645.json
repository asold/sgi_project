{
  "title": "GiT: Graph Interactive Transformer for Vehicle Re-identification",
  "url": "https://openalex.org/W3181858645",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2114693903",
      "name": "Shen Fei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108870577",
      "name": "Xie Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250576596",
      "name": "Zhu Jianqing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129393275",
      "name": "Zhu Xiao-bin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747921216",
      "name": "Zeng, Huanqiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2747685395",
    "https://openalex.org/W2982041213",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2951491521",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3110850179",
    "https://openalex.org/W3005394220",
    "https://openalex.org/W2756261853",
    "https://openalex.org/W3004918942",
    "https://openalex.org/W2963200533",
    "https://openalex.org/W3105077954",
    "https://openalex.org/W3099206234",
    "https://openalex.org/W2954392664",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3098409225",
    "https://openalex.org/W2549858646",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2989607888",
    "https://openalex.org/W2938503583",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3093418233",
    "https://openalex.org/W2959121057",
    "https://openalex.org/W2598634450",
    "https://openalex.org/W2750699122",
    "https://openalex.org/W2983188006",
    "https://openalex.org/W2977696760",
    "https://openalex.org/W3088275997",
    "https://openalex.org/W3034373842",
    "https://openalex.org/W3107652869",
    "https://openalex.org/W2749235995",
    "https://openalex.org/W2779954854",
    "https://openalex.org/W2999111188",
    "https://openalex.org/W2470322391",
    "https://openalex.org/W2980046511",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2889929596",
    "https://openalex.org/W2936222941"
  ],
  "abstract": "Transformers are more and more popular in computer vision, which treat an image as a sequence of patches and learn robust global features from the sequence. However, pure transformers are not entirely suitable for vehicle re-identification because vehicle re-identification requires both robust global features and discriminative local features. For that, a graph interactive transformer (GiT) is proposed in this paper. In the macro view, a list of GiT blocks are stacked to build a vehicle re-identification model, in where graphs are to extract discriminative local features within patches and transformers are to extract robust global features among patches. In the micro view, graphs and transformers are in an interactive status, bringing effective cooperation between local and global features. Specifically, one current graph is embedded after the former level's graph and transformer, while the current transform is embedded after the current graph and the former level's transformer. In addition to the interaction between graphs and transforms, the graph is a newly-designed local correction graph, which learns discriminative local features within a patch by exploring nodes' relationships. Extensive experiments on three large-scale vehicle re-identification datasets demonstrate that our GiT method is superior to state-of-the-art vehicle re-identification approaches.",
  "full_text": "1\nGiT: Graph Interactive Transformer for Vehicle\nRe-identiﬁcation\nFei Shen, Yi Xie, Jianqing Zhu, Xiaobin Zhu, and Huanqiang Zeng\nAbstract—Transformers are more and more popular in com-\nputer vision, which treat an image as a sequence of patches and\nlearn robust global features from the sequence. However, pure\ntransformers are not entirely suitable for vehicle re-identiﬁcation\nbecause vehicle re-identiﬁcation requires both robust global\nfeatures and discriminative local features. For that, a graph\ninteractive transformer (GiT) is proposed in this paper. In the\nmacro view, a list of GiT blocks are stacked to build a vehicle re-\nidentiﬁcation model, in where graphs are to extract discriminative\nlocal features within patches and transformers are to extract\nrobust global features among patches. In the micro view, graphs\nand transformers are in an interactive status, bringing effective\ncooperation between local and global features. Speciﬁcally, one\ncurrent graph is embedded after the former level’s graph and\ntransformer, while the current transform is embedded after the\ncurrent graph and the former level’s transformer. In addition\nto the interaction between graphs and transforms, the graph\nis a newly-designed local correction graph, which learns dis-\ncriminative local features within a patch by exploring nodes’\nrelationships. Extensive experiments on three large-scale vehicle\nre-identiﬁcation datasets demonstrate that our GiT method is\nsuperior to state-of-the-art vehicle re-identiﬁcation approaches.\nIndex Terms —Graph Network, Transformer Layer, Interac-\ntive, Vehicle Re-identiﬁcation\nI. I NTRODUCTION\nV\nEHICLE re-identiﬁcation [1–9] aims to search a target\nvehicle from an extensive gallery of vehicle images\ncaptured from different cameras, which has attracted much\nattention in the multimedia and computer vision community.\nWith the widespread use of intelligent video surveillance\nsystems, the demand for vehicle re-identiﬁcation is growing\nexponentially. The two extrinsic factors hindering vehicle re-\nidentiﬁcation are poor illumination and various viewpoints.\nThere are many vehicle re-identiﬁcation studies [10–14] focus\non dealing with bad illuminations and pay attention to handling\nThis work was supported in part by the National Key R&D Program\nof China under the Grant of 2021YFE0205400, in part by the National\nNatural Science Foundation of China under the Grants 61976098, 61871434,\n61802136 and 61876178, in part by the Natural Science Foundation for\nOutstanding Young Scholars of Fujian Province under the Grant 2022J06023,\nand in part by Collaborative Innovation Platform Project of Fuzhou-Xiamen-\nQuanzhou National Independent Innovation Demonstration Zone under the\ngrant 2021FX03 (Corresponding author: Jianqing Zhu and Huanqiang Zeng).\nFei Shen is with College of Engineering, Huaqiao University, Quanzhou,\n362021, China and School of Computer Science and Engineering, Nanjing\nUniversity of Science and Technology, Nanjing, 210094, China (e-mail:\nfeishen@njust.edu.cn).\nYi Xie, Jianqing Zhu and Huanqiang Zeng are with College of Engineering,\nHuaqiao University, Quanzhou, 362021, China (e-mail: yixie@stu.hqu.edu.cn,\ne-mail: {jqzhu and zeng0043 }@hqu.edu.cn).\nXiaobin Zhu is with School of Computer and Communication Engineering,\nUniversity of Science and Technology Beijing, Xueyuan Road 30, Haidian\nDistrict, Beijing, 100083 China (e-mail: zhuxiaobin@ustb.edu.cn).\nFig. 1. Vehicle re-identiﬁcation models use different architectures.\n(a) Pure convolutional neural networks (CNNs) learn global features.\n(b) Pure CNNs cooperate part divisions to learn global features and\nlocal features. (c) CNNs combine graph networks (GNs) to learn\nglobal features and local features via a multi-branch structure. (d)\nOur graph interactive transformer (GiT) method makes transformer\n(T) layers and local (L) correlation graph modules in an interactive\nstatus to learn global features and local features.\nwith various viewpoints. However, in addition to these two\nextrinsic factors, an intrinsic unfavorable factor is that vehicles\nof the same model and color are highly similar and difﬁcult to\nbe distinguished. The key to solving this inherent unfavorable\nfactor is to treat subtle differences in wheels, lights, and\nfront windows. Therefore, effectively combining the global\nfeatures and local features is crucial to improve the vehicle\nre-identiﬁcation performance.\nThe development of vehicle re-identiﬁcation technologies\ngoes through three stages. As shown in Fig. 1 (a), early\nmethods [15–19] apply pure convolutional neural networks\n(CNNs) to learn the vehicle images’ global features. The\narchitectures of these pure CNNs consist of a famous backbone\nnetwork (such as VGGNet [20], GoogLeNet [21] and ResNet\n[22]) and a global pooling layer. Although early methods can\nlearn global appearance features, they could not deal with local\nfeatures, limiting the vehicle re-identiﬁcation performance.\nVehicle re-identiﬁcation methods enter into the second\nstage, which focuses on addressing global features’ limitations.\nAs shown in Fig. 1 (b), based on CNNs, there is a straight-\nforward way of combining global features and local features\nlearned from vehicle image partitions. The partition division\nway can be further summarized into two kinds, i.e., uniform\nspatial division methods [14, 15, 23–28] and part detection\nmethods [11, 19, 29–33]. The uniform spatial division methods\ndo not require part annotations but are prone to suffer from\npartition misalignments. In contrast, the part detection methods\ncan relieve dis-alignments but encounter a high cost of extra\narXiv:2107.05475v3  [cs.CV]  11 Jan 2023\n2\nmanual part annotations and massive training computations.\nNo matter how to divide partitions, the subsequent feature\nlearning is individually implemented on each part region,\nignoring relationships among part regions.\nThird, to consider relationships among part regions, vehicle\nre-identiﬁcation methods enter the third stage, combining\ngraph network (GN) with CNNs. As shown in Fig. 1 (c), in this\nstage, vehicle re-identiﬁcation models [34–37] usually have a\nCNN branch for learning global features and a GN branch for\nexploring the relationship among local features extract from\npart regions. However, down-sampling pooling and large-stride\nconvolution operations reduce the resolution of feature maps,\ngreatly restricting the ability of CNNs to recognize vehicles\nwith similar appearances [38, 39]. Besides, CNN and GN lack\ninteractions as they are supervised with two independent loss\nfunctions, limiting vehicle re-identiﬁcation performance.\nRecently, transformers [38–41] have been attractive in com-\nputer vision. Transformers have two main advantages, namely,\n(1) global processing and (2) spatial information retaining.\nRegarding global processing, transformers use multi-head self-\nattention to capture global context information to establish\nlong-distance dependence among patches neglected in CNNs.\nRegarding spatial information retaining, transformers discard\ndown-sampling operations (i.e., 2-pixel stride convolution or\npooling) that are usually required in CNNs. However, trans-\nformers also have disadvantages, as follows. (1) Transformers\nlearning from patches violates the basic mechanism of human\nvision. (2) Transformers underrate the feature learning within\npatches, which simply uses fully connected or light convolu-\ntional layers to learn each patch’s features. Overall, transform-\ners achieve impressive performance boost over CNNs in many\nvision tasks [42–44]. Especially, transformers obtain state-of-\nthe-art performance on person re-identiﬁcation [39, 45, 46].\nIn this paper, we propose a graph interactive transformer\n(GiT) method for vehicle re-identiﬁcation, as shown in Fig.\n1 (d). The motivation is to let graphs learn local features\nwithin patches and transformers capture global relationships\nacross patches and further couple graphs and transformers to\ncooperate local and global features effectively for improving\nvehicle re-identiﬁcation.\nThe main contributions of this paper can be summarized as\nfollows:\n• We propose a graph interactive transformer (GiT) method\nto couple graphs and transformers , bringing an effective\ncooperation between local and global features. To the best\nof our knowledge, this paper is the ﬁrst work that explores\nthe interaction between local features and global features\nvia graphs and transformers.\n• We design a local correlation graph (LCG) module for\nwell learning discriminative local features from a patch,\nwhich subtly explores relationships of nodes within the\npatch. The LCG module does not require any extra ﬁne-\ngrained part annotations or part division operations.\n• Experiment results on three large-scale vehicle datasets\n(i.e., VeRi776 [47], VehicleID [48], and VeRi-Wild [49])\nshow that the proposed method is superior to a lot of\nstate-of-the-art vehicle re-identiﬁcation approaches.\nThe rest of this paper is organized as follows. Section II\nintroduces the related work. Section III elaborates our method.\nSection IV presents the experimental results to show our\nmethod’s superiority. Section V concludes this paper.\nII. R ELATED WORK\nIn this section, we brieﬂy review the most related works of\nvehicle re-identiﬁcation and transformer in vision.\nA. Vehicle Re-identiﬁcation\nConvolutional neural networks (CNN) are commonly-used\nin vehicle re-identiﬁcation. Hence, we review CNN models of\nvehicle re-identiﬁcation from two aspects: (1) pure CNN-based\nmethods and (2) combining CNN with graph network.\nPure CNN-based methods. In recent years, a variety of\npure CNN-based methods have been proposed for vehicle re-\nidentiﬁcation. These methods usually include a robust CNN\nmodel for extract global features or local features and a\nfeature aggregation structure for aggregate global features\nor local features. According to the difference in feature ag-\ngregation structure, we can subdivide it into global feature\nlearning methods and local feature learning methods. The\nglobal feature learning methods [9, 11, 13, 17, 48, 50–60]\nusually have a spatial global pooling layer to compress the\nentire vehicle features. However, due to the characteristic of\nspatial global pooling layers, discriminative local features will\ninevitably be underestimated, which is detrimental to vehicle\nre-identiﬁcation.\nAccording to the local feature learning methods, we can\nfurther summarize it as uniform spatial division [14, 15, 23–\n28] and part detection methods [11, 19, 29–33]. The uniform\nspatial division methods uniformly divide feature maps into\nseveral parts and then individually pools each region, as\ndone in [14, 23, 24]. These methods are usually divided into\nseveral parts along the horizontal or vertical direction and then\nindividually pool each region. There is also a method of using\na visual attention model to reﬁne local features in [26, 27].\nAlthough both vision transformer (ViT) and uniform spatial\ndivision methods learn attention on patches, the transformer\nuses multi-head attention to learn relationships among patches.\nThe latter only uses self-attention to enhance local feature\nlearning within patches. As a result, ViT can alleviate the\nproblem of inaccurate division better than uniform spatial\ndivision methods.\nThe part detection methods [11, 19, 29–33, 61] can solve the\nproblem of inaccurate division. These methods usually employ\nthe typical detectors to detect vehicle parts or locate dis-\ncriminative regions. For example, the Part Regularization [29]\nmethod uses you only look once (YOLO) [62] as a detector to\ndetect parts and feature extraction from part regions. Two-level\nAttention network supervised by a Multi-grain Ranking loss\n(TAMR) [28] detected the salient vehicle parts and proposed\na multi-grain ranking loss to enhance the relationship of part\nfeatures. Adaptive attention vehicle re-identiﬁcation (AA VER)\n[11] employed key-point detection module to localizing the\nlocal features and use adaptive key-point selection module to\nlearning the relationship of parts. However, the main pain-\npoint of part detection methods is that part detectors are\n3\ndeep networks in themselves, requiring a high cost of the\nextra manual part annotations, massive training, and inference\ncomputations.\nCombining CNN with Graph Network. Recently, various\nresearch efforts [34–37, 63–67] combine CNN with graph net-\nwork (GN) for re-identiﬁcation since GN can well extract local\nregional features. For example, the parsing-guided cross-part\nreasoning network (PCRNet) [34] extracts regional features\nfor each part from CNN and propagated local information\namong parts based on graph convolutional networks. The\nglobal structural graph [68] guides on features generated\nby a CNN to form ﬁne-grained representations for vehicles.\nThe structured graph attention network (SGAT) [36] creates\nan inherently structured graph and an extrinsic structured\ngraph to learn the vehicle’s structure relationship feature.\nThe hierarchical spatial structural graph convolutional network\n(HSS-GCN) [35] enhances the spatial geometrical structure\nrelationship among local regions or between the global region\nand local regions for vehicle re-identiﬁcation. In addition to\nthese vehicle re-identiﬁcation works, there are several GN\nbased person re-identiﬁcation. The dynamic dual-attentive\naggregation (DDAG) [65] method reinforces representations\nwith contextual relations crossing two modalities for visible-\ninfrared person re-identiﬁcation. For unsupervised video-based\nperson re-identiﬁcation, the dynamic graph matching (DGM)\n[66] approach obtains a good similarity measure from in-\ntermediate estimated labels via an iteratively reﬁning graph\nstructure. The modality-aware collaborative ensemble (MACE)\nmethod [67] respectively handles modality-discrepancy in both\nfeature and classiﬁer levels by a middle-level sharable two-\nstream network for cross-modal person re-identiﬁcation.\nB. Transformer in Vision\nThe transformer is a type of self-attention-based neural\nnetwork originally applied for natural language modeling\n(NLP) tasks. Recently, attention-based transformers have gone\nviral in vision. The vision transformer (ViT) [38] stacks\nmultiple transformer layers. Every transformer layer is a\nkind of residual structure. It is sequence packaged by layer\nnormalization (LN), multi-head self-attention layer (MHSA)\nand a multilayer perceptron (MLP) block. Based on the ViT\nmodels, many excellent networks are developed and veriﬁed\nfor their effectiveness and scalability in downstream tasks. For\nexample, ViT-BoT [39] combines the ViT framework with\nBNNeck [69] to construct a strong baseline for object re-\nidentiﬁcation. Deformable transformer (DETR) [40] proposes\nmulti-scale deformable attention modules to sample a set of\ncritical points of patch for small objects detection. The trans-\nformer version U-Net (TransUnNet) [41] takes the advantages\nof the CNN and transformer model to learn global context\nfeatures for medical image segmentation.\nConsidering the strong ability of the graph network to\naggregate local features and the superiority of the ViT model\nto model global features, we design a novel architecture to\ngraph network and transformer layers are in a coupled status\nand demonstrate its effectiveness in this paper. Besides, there\nare also a similar topology method in multi-branch CNN,\nsuch as HRNet [70] and Multi-scale DenseNet [71]. All\nthree methods both use a parallel structure in topology. How-\never, our GiT is different from HRNet [70] and Multi-scale\nDenseNet [71]. Topologically, HRNet can two-way coupled\nmulti-scale features, but the couple position is sparse. Multi-\nscale DenseNet [71] can densely couple multi-scale features,\nbut the couple is single-way, i.e., from high-resolution features\nto low-resolution features. The GiT can conveniently achieve\ndensely and two-way coupling of multi-scale features.\nIII. P ROPOSED METHOD\nA. Overview\nAs shown in Fig. 2, the proposed method’s overall frame-\nwork stacks a list of graph interactive transformer (GiT)\nblocks. Each GiT block contains a newly designed local\ncorrelation graph (LCG) and a transformer layer. The LCG\nlearns local features within patches, but it ignores global\nrelationships across patches. The transformer conducts multi-\nhead self-attention to learn global relationships across patches,\nhowever, it underrates the local feature learning within patches.\nThus, the interaction between LCG and transformer is of great\nsigniﬁcance to make the advantages of each complement the\ndisadvantages of the other. To this end, we make a two-way\nconnection design to achieve a good interaction between LCG\nand transformer, i.e., one current LCG is embedded after the\nformer level’s LCG and transformer layer, and the current\ntransform is embedded after the current LCG and the former\nlevel’s transformer layer. More detail are described as follows.\nB. Local Correlation Graph Module\nWe propose a local correlation graph (LCG) module to\naggregate and learn discriminative local features within every\npatch. Assume each vehicle image have N patches and the i-th\npatch Ni ∈ℜP2×C , where P2 and C respectively denote the\npatch’s size and the number of channels. In the LCG module,\neach patch is sampled into n local features of a sequence of\n(S,S) size with d dimensions features, where n = P2\nS2 and\nd = S2 ×C. Like patch embedding of ViT, we ﬂatten n\nlocal features d dimensions and map to d′ dimensions with\na trainable linear projection in every patch. The transformed\nlocal feature sequence of i-th patch Xi is formed according to\nEq. (1) as follows:\nXi =\n[\nx1\ni E,x2\ni E,··· ,xm\ni E,··· ,xn\ni E\n]\n, (1)\nwhere xm\ni ∈Rd is the m-th local feature of the i-th patch,\nXi ∈Rn×d′\n, and E ∈Rd×d′\n. Then, we create a learnable\nposition embedding Epos to preserve the position information\nfor each local feature, where Epos ∈ Rn×d′\n. Therefore,\naccording to Eq. (2), the initial embedding sequence of i-th\npatch is constructed as follows:\nXinit\ni =\n[\nx1\ni E,x2\ni E,··· ,xm\ni E,··· ,xn\ni E\n]\n+ Epos. (2)\nFor ease of description, the embedding sequence deﬁ-\nnition is assumed to be identical on each patch. Speciﬁ-\ncally, we deﬁne the node with d′ dimensions features as\nV = [ v1,v2,··· ,vm,··· ,vn] ∈ Rn×d′\n, where the m-th\n4\nFig. 2. The framework of the proposed graph interactive transformer (GiT) method. The BN and FC represent a batch normalization and a fully connected\nlayer, respectively.\nnode vm = xmE + Em\npos ∈ Rd′\nand m ∈ [1,2,··· ,n] .\nThen, a spatial graph of every patch can be described as\nG = {V ∈Rn×d′\n,E ∈Rn×n}. The spatial graph’s edges\nare constructed according to Eq. (3), as follows:\nEvi,j = exp (FCosine (vi,vj))∑n\nk=1 exp (FCosine (vi,vk)), (3)\nwhere i,j ∈[1,2,··· ,n]; Evi,j is the edge between the node\nvi and the node vj in a patch. FCosine(vi,vj) means similarity\nscore of the node vi and the node vj. Moreover, it has been\nempirically shown that the score of the cosine distanceFCosine\nis more effective in computing the correlation. And the cosine\ndistance FCosine is calculated according to Eq. (4), as follows:\nFCosine (vi,vj) = vi ·vj\n||vi||||vj||, (4)\nwhere ·and ||·|| respectively denote element-wise multiplica-\ntion operation and L2 regularization.\nFrom Eq. (2), Eq. (3) and Eq. (4), we can easily obtain\nthe graph’s adjacency matrix Ai,j = Evi,j ∈ Rn×n in a\npatch. To effectively mine and learn the relationship between\ndiscriminative local features, we adopt the graph network to\naggregate and update nodes by information propagation from\neach node to its neighbors’ nodes in the graph. And the\naggregation node U of i-th graph is updated according to Eq.\n(5), as follows:\nU = (D−1\n2 AD−1\n2 Xi) ·W, (5)\nwhere A ∈Rn×n is the adjacent matrix and D ∈Rn×n is\nthe degree matrix of A ; The ·and W ∈Rn×d′\nrepresent an\nelement-wise multiplication operation and a learnable param-\neter, respectively. It is worth noted that we use standard 2D\nlearnable parameters W since we have not observed signiﬁcant\nperformance gains from using more advanced 3D learnable\nparameters W. In other words, W ∈ Rn×d′\nis shared for\nN graphs of a patch, which signiﬁcantly reduces parameter\ncalculation complexity.\nFrom Eq. (5), both a node itself and its neighbor nodes are\naggregated and updated according to the learnable weight W\nto feed more local information. To introduce non-linearities\nand improve the convergence graph network, the node U is\nenhanced according to Eq. (5), as follows:\nO= GELU(LN(U)), (6)\nwhere GELU represents the gaussian error linerar units\n(GELU) and LN denotes the layer normalization (LN); There\nis nothing special about choosing GELU and LN functions,\njust to be consistent with the original transformer layer’s\nfunctions. According to the Eq. (6), O∈Rn×d′\nreplaces U as\nthe new node with discriminative local features of a graph.\nC. Transformer Layer\nThe transformer layer is used to model the global rela-\ntionship between the different patches, consisting of a multi-\nhead self-attention (MHSA) and a multi-layer perceptron\n(MLP) block. Assume that X ∈RN×M is N patches, where\nM = P2 ×C. The (P,P ) is the size of each image patch and\nC is the number of channels. The X ∈RN×M are linearly\ntransformed to queries Q∈RN×Mq , keys K ∈RN×Mk and\nvalues V ∈RN×Dv . The scaled dot-product self-attention is\napplied on Q, K, V according to the Eq. (7), as follows:\nAttention (Q,K,V ) = softmax\n(QKT\n√Mv\n)\nV. (7)\nThe MHSA splits the queries, keys, and values for htimes and\nperforms the h times self-attention function in parallel. Then\nthe output values of each head are concatenated and linearly\nprojected to form the ﬁnal output. Therefore, according to Eq.\n(8) and Eq. (9), The output of transformer layer Y ∈RN×M\nis calculated as follows:\nX′= MSA(LN(X)) +X, (8)\nY = MLP(LN(X′) +X′. (9)\nFrom Eq. (7), Eq. (8) and Eq. (9), one can see that since the\ntransformer layer models global features on all patches, the\nproposed LCG module is essential for local feature learning.\nD. Graph Interactive Transformer\nAs shown in Fig. 2 , the graph interactive transformer (GiT)\nmethod has l GiT blocks. Every GiT block consists of a local\ncorrelation graph (LCG) module and a transformer layer. It is\nworth noting that our proposed GiT block can make the global\nfeatures of the transformer layer and the local features of the\nLCG module form a coupling state. In other words, global\nfeatures and local features can interact with each other in the\n5\nFig. 3. The transformer layer and the LCG module interact each other.\nTABLE I\nTHE NODE CONFIGURATION FOR LCG MODULES .\nSampling Size\n(S ×S)\nNumber\n(m)\nDimension\n(d)\nStage 1 2 ×2 64 12\nStage 2 4 ×4 16 48\nStage 3 8 ×8 4 192\nentire feature learning process, as shown in Fig. 3. Speciﬁcally,\nas describe in section III-B, assume that Ol ∈ RN×n×d′\nrepresents the LCG module of the l-th GiT block has N graph\nwith discriminatory local feature. And each graph is composed\nof n nodes with d′ dimensional features. When embedding\nlocal features in the patch, we set d′= d to make a LCG and\na transformer layer can be directly coupled without requiring\nextra parameterized modules, such as linear projection or\nconvolution layers, saving parameters and computational cost.\nAccording to Eq. (10), the discriminative local feature Ol can\nbe seamlessly converted into feature Xl of a patch.\nXl = Reshape(Ol) (10)\nwhere the Ol ∈RN×m×d′\nand the Xl ∈RN×D. From node\nto patch, the Ol can be reshape N patches with D dimensions\nfeatures and l∈[1,2,··· ,12] in this paper.\nLet’s forget about speciﬁc formulas of section III-B and\nsection III-C, the l-th GiT blocks’ LCG module L(l) and\ntransformer layer T(l) are deﬁned according to Eq. (11) and\nEq. (12), as follows:\nL(l) = L(T(l−1)(L(l−1)(x)),L(l−1)(x)),l ≥2, (11)\nT(l) = T(L(l)(T(l−1)(x)),T(l−1)(x)),l ≥2. (12)\nFrom the Eq. (11) and Eq. (12), in the l-th GiT block,\nthe LCG module learns local features from local and global\nfeatures resulting from the LCG module and transformer layer\nof the (l−1)-th GiT block. Similarly, the transformer layer\nlearns global features from the global relationship generated\nby the transformer layer of the (l−1)-th GiT block and the new\nlocal features outputted via the LCG module of the l-th GiT\nblock. Therefore, GiT blocks’ LCG modules and transformer\nlayers are in a coupled status, bringing effective cooperation\nbetween local and global features.\nBesides, the GiT method has three stages and every stage\nhas 4 GiT blocks to handle feature maps of a speciﬁc scale.\nEach GiT block has the same architecture. It’s just that the\nnumber of nodes n and the sampling size (S,S) are different\nin different stages. In the subsequent experiments, each image\npatch’s resolution is the same as the ViT [38] method, i.e., the\nsize of each patch is (16,16). The speciﬁc conﬁguration of\nthe node size in the LCG module is shown in Table I.\nIn addition to the two-directional way in Eq.(11) and Eq.\n(12), there are two other skip-connection architectures, namely,\nGiT (Global →Local) and GiT (Global ←Local), which have\na single-directional feature ﬂow. The GiT (Global →Local)\ncase means that it only uses blue connection line in Fig. 2.\nThe GiT (Global ←Local) case means that it only uses red\nconnection line in Fig. 2. Intuitively, two-directional way could\ncouple global and local features more completely than the GiT\n(Global →Local) and GiT (Global ←Local).\nE. Loss Function Design\nFrom Fig. 2, the class token of the last GiT block serves\nas the image’s vehicle feature representation. Inspired by\nBNNeck [69] in the CNN, we introduce it after the ﬁnal class\ntoken. The features of the class token are directly applied\nto triplet loss of soft margin. After the feature of batch\nnormalization (BN) is fed to cross-entropy loss without label\nsmoothing. Therefore, the proposed GiT’s total loss function\nis formulated as follows:\nLtotal = αLCE + βLTriplet , (13)\nwhere LCE and LTriplet respectively denote cross-entropy\nloss and triplet loss. α and β are manually setting constants\nused to keep balance of two loss functions. To avoid excessive\ntuning those constants, we set α= β = 1in following exper-\niments. The cross-entropy loss function [72] is formulated as\nfollows:\nLCE (X,l) =−1\nM\nM∑\ni=1\nK∑\nj=1\n(li,j)log( eWT\nj Xi\n∑K\nk=1 eWT\nk Xi\n), (14)\nwhere X is a training set and lis the class label information;\nM is number of training samples; K is the number of classes;\n(Xi,li) is the i-th training sample and li ∈{1,2,3,...,K };\nW = [W1,W2,W3,...,W K] is a learn-able parameter matrix.\nThe triplet loss function [73] is formulated as follows:\nLTriplet (Xa, Xn, Xp) = −1\nM\nM∑\ni=1\nmax(∥Xa\ni −Xn\ni ∥2 −\nXa\ni −Xp\ni\n\n2, 0),\n(15)\nwhere (Xa,Xn,Xp) is a set of training triples; M is number\nof training triplets; for the i-th training triplet, (Xa\ni ,Xn\ni ) is\na negative pair holding different class labels, and (Xa\ni ,Xp\ni )\nis a positive pair having the same class label; ∥·∥2 denotes\nthe an Euclidean distance. Moreover, the hard sample mining\n[73] strategy is applied to improve the triplet loss, which aims\nto ﬁnd the most difﬁcult positive and negative image pairs\nin each mini-batch. Recently, there is a weighted regulariza-\ntion triplet (WRT) [74] loss function can well constrain the\npairwise distance of a positive/negative sample pair without\nintroducing any extra hyper-parameters for the re-identiﬁcation\ntask. Considering that most state-of-the-art methods do not\n6\napply WRT loss, our default conﬁguration still applies initial\ntriplet loss LTriplet .\nIV. E XPERIMENT AND ANALYSIS\nTo validate the proposed GiT method’s superiority,\nit is compared with multiple state-of-the-art vehicle re-\nidentiﬁcation approaches on three large-scale datasets, namely,\nVeRi776 [47], VehicleID [48] and VeRi-Wild [49]. The rank-\n1 identiﬁcation rate (R1) [47, 48, 75] and mean average\nprecision (mAP) [14, 76–78] are used to assess the accuracy\nperformance.\nA. Datasets\nVeRi776 [47] is captured by 20 cameras in unconstrained\ntrafﬁc scenarios, and each vehicle is captured by 2-18 cameras.\nFollowing the evaluation protocol of [47], VeRi776 is divided\ninto a training subset containing 37,746 images of 576 subjects\nand a testing subset including a probe subset of 1,678 images\nof 200 subjects and a gallery subset of 11,579 images of\nthe same 200 subjects. Moreover, only cross-camera vehicle\npairs are evaluated. Suppose the same camera captures a probe\nimage and a gallery image. In that case, the corresponding\nresult will be excluded from the evaluation process.\nVehicleID [48] includes 221,763 images of 26,267 subjects.\nEach vehicle is captured from either front or rear viewpoint.\nThe training subset consists of 110,178 images of 13,164\nsubjects. There are three testing subsets, i.e., Test800, Test1600\nand Test2400, for evaluating the performance at different\ndata scales. Speciﬁcally, Test800 includes 800 gallery images\nand 6,532 probe images of 800 subjects. Test1600 contains\n1,600 gallery images and 11,395 probe images of 1,600\nsubjects. Test2400 is composed of 2,400 gallery images and\n17,638 probe images of 2,400 subjects. Moreover, for three\ntesting subsets, the division of probe and gallery subsets is\nimplemented as follows: randomly selecting one image of a\nsubject to form the probe subset. All remaining images of this\nsubject are used to construct the gallery subset. This division\nis repeated and evaluated 10 times, and the average result is\nreported as the ﬁnal performance.\nVeRi-Wild [49] is newly dataset released in CVPR 2019.\nDifferent to the VeRi776 [47] and VehicleID [48] captured at\nday, its images captured at both day and night. VeRi-Wild\ncontains 416,314 images of 40,671 subjects in total. It is\ndivided into a training subset of 277,797 images of 30,671,\nand a testing subset of 128,517 images of 10,000 subjects.\nSimilar to VehicleID [48], the testing subset is organized\ninto three different scale subsets, i.e., Test3000, Test5000,\nand Test10000. To be more speciﬁc, Test3000 is composed\nof 41,816 gallery images and 3000 probe images of 3,000\nsubjects. Test5000 is made up of 69,389 gallery images and\n5,000 probe images of 5,000 subjects. Test10000 is consisted\nof 138,517 gallery images and 10,000 probe images of\n10,000 subjects.\nB. Implementation Details\nThe deep learning toolbox is PyTorch [79] with FP16\ntraining. Training conﬁgurations are summarized as follows.\n(1) We respectively use ImageNet pre-trained weights of ViT\nand a truncated normal distribution initialize to the transformer\nlayers and the LCG modules. (2) Random erasing [80] and z-\nscore normalization are used for the data augmentation. Both\nprobabilities of horizontal ﬂip and random erasing are set\nto 0.5. (3) The mini-batch stochastic gradient descent (SGD)\nmethod [81] is applied to train parameters. The weight decays\nare set to 1 ×10−4 and the momentums are set to 0.9. There\nare 150 epochs for the training process. The learning rates\nare initialized to 1 ×10−4, and they are linearly warmed up\n[69] to 1 ×10−2 in the ﬁrst 5 epochs. After warming up, the\nlearning rates are maintained at 1 ×10−2 from 11th to 50th\nepochs. Then, the learning rates are reduced to 1 ×10−3 from\n51st to 85th epochs, it decays to 1 ×10−4 after 85 epochs and\nfurther dropped to 1 ×10−5 between 120th and 150th epochs.\n(4) Unless otherwise speciﬁed, each mini-batch includes 8\nsubjects and each subject holds 6 images.\nDuring the testing phase, those 768-dimensional features\nresulted from the BN layer (see Fig. 2) as the vehicle images’\nﬁnal features. Moreover, the Cosine distance of those the ﬁnal\nfeatures is applied as the similarity measurement for vehicle\nre-identiﬁcation.\nC. Comparison with State-of-the-art Methods\nAccording to the development process of vehicle re-\nidentiﬁcation, those state-of-the-art methods under compari-\nson are roughly classiﬁed into three categorizes for a clear\npresentation. Speciﬁcally, Global (Pure CNN) denotes the\napproaches [8, 9, 16–19, 31, 48, 52, 53, 82] exploiting\nglobal features based on pure convolutional neural network\n(CNN); Global + Local (Pure CNN) represents the methods\n[11–14, 25, 26, 29, 33, 83–89] utilizing global features and\nlocal features based on pure CNN; Global + Local (CNN &\nGN) denotes the approaches [34, 36, 37] dealing with global\nfeatures and local features based on CNN and graph network\n(GN).\n1) Comparisons on VeRi776: Table II shows the perfor-\nmance comparison of the proposed GiT method and multiple\nstate-of-the-art approaches on VeRi776 dataset. It can be found\nthat the proposed GiT method achieves the highest rank-1 (R1)\nidentiﬁcation rate (i.e., 96.86%) and mAP (i.e., 80.34%). As\nshown in Table II, the method of combining global and local\nfeatures (i.e., Global + Local (Pure CNN) and Global + Local\n(CNN & GN)) are mostly superior to the method of only using\nglobal features (Global (Pure CNN)) on the VeRi776 dataset\nby a large margin. For example, the best Global + Local (CNN\n& GN) method (i.e., PCRNet [34]) and the best Global +\nLocal (Pure CNN) method (i.e., SA VER [83]) respectively\ndefeats the best Global (Pure CNN) method (i.e.,SDC-CNN\n[16]) by 11.91 % and 12.91% in term of R1 identiﬁcation\nrate. This demonstrates that part details and local features are\nimportant clues for vehicle re-identiﬁcation. Meanwhile, the\nR1 identiﬁcation rate and mAP of the GiT approach exceeds\n13.37% and 26.89 % over the best Global (Pure CNN) method\n(i.e.,SDC-CNN [16]).\nMoreover, among those compared state-of-the-art methods,\nthe proposed GiT approach outperforms the best Global +\n7\nTABLE II\nTHE PERFORMANCE (%) COMPARISON ON VERI776. T HE RED , GREEN\nAND BLUE ROWS REPRESENT THE 1ST, 2ND AND 3RD PLACES ,\nRESPECTIVELY .\nMethods R1 mAP\nProposed GiT 96.86 80.34\nTransformer *ViT [38] 95.84 78.92\nGlobal + Local\n(CNN & GN)\n*PCRNet [34] 95.40 78.60\n*SGAT [36] 94.65 76.32\nGlobal + Local\n(Pure CNN)\n*SA VER [83] 96.40 79.60\n*VOC-ReID [84] 96.30 79.70\n*PVEN [12] 95.60 79.50\nApp+License [85] 95.41 78.08\n*SFF+SAtt [26] 95.35 77.28\nCFVMNet [86] 95.30 77.06\nPart Regular [29] 94.30 74.30\n*SAN [25] 94.82 74.68\nPAMTRI [33] 92.86 71.88\nMRM [87] 91.77 68.55\nDMML [88] 91.20 70.10\nV ANet [13] 89.78 66.34\nAA VER [11] 88.97 61.18\nQD-DLF [14] 88.50 61.83\nV AMI [89] 85.92 61.32\nGlobal\n(Pure CNN)\nSDC-CNN [16] 83.49 53.45\nVST Path Proposals [17] 83.49 58.27\nPROVID [9] 81.56 53.42\n*DenseNet121 [18] 82.77 54.83\n*GoogLeNet [82] 78.64 51.42\nOIFE+ST [19] 68.30 51.42\nFACT [8] 52.21 18.75\n* Methods the same training tricks as GiT.\nTABLE III\nTHE PERFORMANCE (%) COMPARISON ON VEHICLE ID. T HE RED , GREEN\nAND BLUE ROWS REPRESENT THE 1ST, 2ND AND 3RD PLACES ,\nRESPECTIVELY .\nMethods Test800 Test1600 Test2400\nR1 mAP R1 mAP R1 mAP\nProposed GiT 84.65 90.12 80.52 86.77 77.94 84.26\nTransformer ViT [38] 82.41 87.05 76.36 83.29 73.31 80.10\nGlobal + Local\n(CNN & GN)\nHPGN [37] 83.91 89.60 79.97 86.16 77.32 83.60\nSGAT [36] 78.12 81.49 73.98 77.46 71.87 75.35\nHSS-GCN [35] 72.70 77.30 67.90 72.40 62.40 66.10\nGlobal + Local\n(Pure CNN)\nApp+License [85] 79.50 82.70 76.90 79.90 74.80 77.70\nMGL [90] 79.60 82.10 76.20 79.60 73.00 75.50\nMSV [57] 75.10 79.30 71.80 75.40 68.70 73.30\nEALN [58] 75.11 77.50 71.78 74.20 69.30 71.00\nQD-DLF [14] 72.32 76.54 70.66 74.63 64.14 68.41\nGlobal\n(Pure CNN)\nSDC-CNN [16] 56.98 63.52 50.57 57.07 42.92 49.68\nDenseNet121 [18] 66.10 68.85 67.39 69.45 63.07 65.37\nLocal (CNN & GN) method (i.e., PCRNet [34]) and the\nbest Global + Local (Pure CNN) method. For example, the\nproposed GiT method respectively higher 1.46% and 1.74%\nthan the best Global + Local(CNN & GN) method (i.e.,\nPCRNet [34]) on R1 identiﬁcation rate and mAP. It is noted\nthat the PCRNet [34] extra uses an image segmentation model\ntrained on vehicle parsing data as a preprocessing tool to\nobtain the parsed masks of vehicle images, while GiT does\nnot use any extra semantic annotation information.\n2) Comparisons on VehicleID: Table III shows the rank-1\nidentiﬁcation rate (R1) and mAP comparison of the proposed\nGiT method and state-of-the-art approaches on the VehicleID\n[48] dataset with a more signiﬁcant number of images than the\nVeRi776 [47] dataset. Three different type of testing subsets\nTABLE IV\nTHE PERFORMANCE (%) COMPARISON ON THE VERI-WILD . THE RED ,\nGREEN AND BLUE ROWS REPRESENT THE 1ST, 2ND AND 3RD PLACES ,\nRESPECTIVELY .\nMethods Test3000 Test5000 Test10000\nR1 mAP R1 mAP R1 mAP\nProposed GiT 92.65 81.76 89.92 75.64 85.41 67.55\nTransformer ViT [38] 89.29 78.66 86.18 72.73 81.13 63.91\nGlobal + Local\n(CNN & GN)\nPCRNet [34] 92.50 81.20 89.60 75.30 85.00 67.10\nHPGN [37] 91.37 80.42 88.21 75.17 82.68 65.04\nGlobal + Local\n(Pure CNN)\nGLAMOR [91] 92.10 77.15 N/A N/A N/A N/A\nUMTS [92] 84.50 72.70 79.30 66.10 72.80 54.20\nDFLNet [93] 80.68 68.21 70.67 60.07 61.60 49.02\nAA VER [11] 75.80 62.23 68.24 53.66 58.69 41.68\nGlobal\n(Pure CNN)\nFDA-Net [49] 64.03 35.11 57.82 29.80 49.43 22.78\nGSTE [94] 60.46 31.42 52.12 26.18 45.36 19.50\nGoogLeNet [21] 57.16 24.27 53.16 24.15 44.61 21.53\nHDC [95] 57.10 29.14 49.64 24.76 43.97 18.30\nDRDL [48] 56.96 22.50 51.92 19.28 44.60 14.81\nSoftmax [47] 53.40 26.41 46.16 22.66 37.94 17.62\nTriplet [96] 44.67 15.69 40.34 13.34 33.46 9.93\nare evaluated, including Test800, Test1600, and Test2400.\nIt can be found that the proposed GiT method consistently\noutperforms those state-of-the-art methods under comparison.\nFor example, on Test800, GiT defeats the second place HPGN\n[37] by a 0.52% higher R1 identiﬁcation rate, and outperforms\nthe third place ViT [38] by a 3.07% higher R1 identiﬁcation\nrate.\n3) Comparisons on VeRi-Wild: Table IV shows the compar-\nison result on VeRi-Wild [49] dataset. Among those methods,\nit can be found that the proposed GiT method obtains the\n1st place again on the Test3000, Test5000, and Test1000 of\nVeRi-Wild dataset. For example, on largest Test1000 subset,\nR1 and mAP of GiT method respectively are 0.41% and 0.45%\nhigher than those of the 2nd place method, i.e., PCRNet [34],\nwhich extra uses an image segmentation model. Compared\nwith the 3rd place method (i.e., HPGN [37]), the proposed GiT\nmethod defeats it by 2.73% in term of mAP and 2.51% in term\nof R1 identiﬁcation rate on largest Test1000 subset. Mean-\nwhile, the proposed GiT method obtains the state-of-the-art\nperformance on VeRi776, VehicleID, and VeRi-Wild, which\nshows the effectiveness and generalization of our method.\nBesides, one can see that a lot of approaches (the Global +\nLocal [11, 34, 37, 91–93]) have made signiﬁcant progress than\nthe Global (Pure CNN) methods [21, 47–49, 94–96] on the\nVeRi-Wild dataset. Those results also reﬂect the trend that the\nmethods which exploit both global and local features achieve\nbetter results than early Global (Pure CNN) methods that only\nuse global features.\nAs shown in Table II, Table III, and Table IV, our GiT\nmethod consistently outperforms ViT on three datasets. Es-\npecially, on two large sized dataset, namely, VehicleID and\nVeRi-Wild, GiT wins obvious improvements. For example, on\nTest2400 of VehicleID, our GiT’s R1 is 1.02% higher than\nViT’s R1, and our GiT’s mAP is 4.16% larger than the ViT’s\nmAP. These results demonstrate that GiT coupling graphs is\neffective to improve ViT.\n8\nTABLE V\nTHE ABLATION STUDY OF OUR GIT METHOD ON THE VERI776 AND\nVEHICLE ID DATASETS .\nNames VeRi776 VehicleID\nTest800 Test1600 Test2400\nR1 mAP R1 mAP R1 mAP R1 mAP\nBaseline (Global) 95.84 78.92 82.41 87.05 76.36 83.29 73.31 80.10\nGiT (No Interactive) 96.17 79.55 83.26 88.88 79.05 85.29 76.22 82.50\nGiT (Global →Local) 96.34 79.78 83.57 89.13 79.29 85.52 76.46 82.61\nGiT (Global ←Local) 96.42 79.87 83.61 89.16 79.43 85.66 76.75 82.97\nGiT (Interactive) 96.86 80.34 84.65 90.12 80.52 86.77 77.94 84.26\nFig. 4. The CMC cures on VeRi776 dataset.\nD. Ablation Studies and Analysis\nThe comparison results presented in Table II, Table III,\nand Table IV, demonstrate that the proposed GiT method\nis superior to many state-of-the-art vehicle re-identiﬁcation\nmethods. Recall Fig. 2, the proposed GiT method is mainly\nmade up of pure transformer layers for learning global fea-\ntures and local correlation graph (LCG) modules for mining\nlocal features. Moreover, the LCG module’s local features\nand global features of the transformer layer can interact and\nimprove each other. In what follows, the proposed GiT method\nis comprehensively analyzed from six aspects to investigate\nthe logic behind its superiority. (1) Role of LCG module and\ntransformer layer interaction. (2) Visualization. (3) Inﬂuence\nof sampling strategy in LCG module. (4) Role of backbones.\n(5) Complexity analysis. (6) Sensitivity analysis.\n1) Role of LCG Module and Transformer Layer In-\nteraction: Baseline (Global) denotes the method that uses\npure transformer layers without local correlation graph (LCG)\nmodules. GiT (No Interactive) represents the method applies\nGiT without the red and blue lines of all GiT blocks in Fig. 2.\nAnd similar to Fig. 1 (c), global features and local features are\nseparately supervised with two loss functions. GiT (Global →\nLocal) means the method utilizes GiT without the red lines of\nall GiT blocks in Fig. 2. GiT (Global ←Local) denotes the\nmethod employs GiT without the blue lines of all GiT blocks\nin Fig. 2. GiT (Interactive) indicates the full version of GiT,\ni.e., the red line and blue line are used simultaneously.\nFirstly, as can be seen in Table V, the GiT (No Interactive),\nGiT (Global → Local), GiT (Global ← Local) and GiT\nFig. 5. The CMC cures on VehicleID dataset.\nFig. 6. Rank-5 visualization examples on VeRi-Wild. The ﬁrst, second, and\nthird rows show the top ﬁve images returned by Baseline (Global), GiT (No\nInteractive), and GiT (Interactive), respectively.\nhave all consistently outperformed Baseline (Global) on two\ndatasets. This means that we proposed LCG modules that\ncan learn discriminative local features. The global appear-\nances and local details are complementary for discrimina-\ntive representations. Secondly, the result of GiT (Global →\nLocal) and GiT (Global ←Local) are better than GiT (No\nInteractive) by more than 0.23% and 0.32% mAP on the\nVeRi776 dataset, respectively. Furthermore, the proposed GiT\n(Interactive) outperforms both GiT (Global →Local) and GiT\n(Global ←Local) on two datasets. For example, compared\nto the GiT (Global →Local) and GiT (Global ←Local),\non VeRi776, the GiT (Interactive) holds a larger 0.52% and\n0.44% R1 identiﬁcation rate, respectively. And on Test2400\nof VehicleID, the GiT holds a larger 1.65% and 1.29 %\nmAP, respectively. These results showing that either adding\ntransformer’s global features to LCG or adding LCG’s local\nfeatures to transformer layer is beneﬁcial. Thus, the two-way\nconnection of our GiT fully couples LCG and transformer\nlayer to win the best performance. Thirdly, from Fig. 4 and\nFig. 5, it can be intuitively found that from rank-1 to rank-25,\nthe proposed GiT (Interactive) is better than Baseline (Global)\nand GiT (No Interactive) on VeRi776 [47] and Test2400\nof VehicleID [48]datasets by a large margin. These results\ndemonstrate that the interaction of that transformer layer and\nLCG module can effectively improve the performance of\nvehicle re-identiﬁcation.\n9\nTABLE VI\nTHE PERFORMANCE (%) COMPARISON AMONG DIFFERENT SAMPLING SIZES FOR FEATURE MAP IN THE GIT METHOD .\nNames Sampling Sizes VeRi776 VehicleID VeRi-Wild\nTest800 Test1600 Test2400 Test3000 Test5000 Test10000\nR1 mAP R1 mAP R1 mAP R1 mAP R1 mAP R1 mAP R1 mAP\nGiT1 (2, 2) ⇒(2, 2)⇒(2, 2) 96.51 79.88 84.01 89.25 79.44 85.93 76.82 83.17 91.54 80.79 88.80 74.56 84.12 66.31\nGiT2 (4, 4) ⇒(4, 4)⇒(4, 4) 96.44 79.71 83.69 89.13 79.30 85.64 76.61 82.83 91.35 80.42 88.67 74.17 83.85 66.08\nGiT3 (8, 8) ⇒(8, 8)⇒(8, 8) 96.33 79.62 83.38 88.91 79.17 85.35 76.36 82.61 91.19 80.03 88.28 73.64 83.19 65.26\nGiT (2, 2) ⇒(4, 4)⇒(8, 8) 96.86 80.34 84.65 90.12 80.52 86.77 77.94 84.26 92.65 81.76 89.92 75.64 85.41 67.55\nFig. 7. The grad class activation maps (Grad-CAM) visualization of attention\nmaps. The ﬁrst, second, and third rows show the original image, ViT, and GiT,\nrespectively.\nTABLE VII\nCOMPARISON OF DIFFERENT BACKBONES ON THE VERI776. A LL\nMETHODS ARE IMPLEMENTED ON THE ONE SAME V100 GPU. T HE\nFLOP S AND AIS RESPECTIVELY DENOTE FLOATING POINT OF\nOPERATIONS AND AVERAGE INFERENCE SPEED .\nMethods R1 mAP Parameters (M) FLOPs (G) AIS (ms/image)\nResNet50 [97] 95.36 76.53 25.6 4.3 31.47\nResNet101 [97] 95.47 77.18 44.7 8.9 58.26\nResNet152 [97] 95.22 77.35 134.9 13.7 87.61\nViT-Tiny [38] 94.28 77.84 29.7 18.4 53.38\nViT-Small [38] 95.12 78.38 48.5 29.2 97.24\nViT-Base [38] 95.84 78.92 86.7 55.6 136.17\nGiT-Tiny 96.16 79.45 35.1 18.7 54.29\nGiT-Small 96.26 79.72 53.9 29.3 98.38\nGiT-Base 96.86 80.34 92.1 55.7 137.63\n2) Visualization: In particular, to visualize our proposed\nGiT (Interactive) method’s effectiveness, the rank-5 retrieval\nresults of two example query images from VeRi-Wild are\nshown in Fig. 6. The ﬁrst, second, and third rows respectively\nshow the top ﬁve images returned by Baseline (Global), GiT\n(No Interactive), and GiT (Interactive). Images with blue boxes\ndenote queries. The images with green boxes and red boxes\nare correct and incorrect retrieve results, respectively. It can\nbe seen that Rank1-Rank5 errors of Baseline (Global) and\nGiT (No Interactive) are often caused by vehicles with high\nsimilarity, while the GiT (Interactive) performed well and\nretrieved the vehicle images with discriminative parts from\nthe query images accurately. The GiT (No Interactive) method\nranks second, while the baseline method has poor results.\nThis shows that the necessity and effectiveness of global\nappearances and local information interaction for vehicle re-\nidentiﬁcation.\nTABLE VIII\nTHE PERFORMANCE (%) COMPARISON AMONG DIFFERENT SCALED GIT\nON VERI776. F OR GIT, ‘H’MEANS HEAD , ‘D’IS DEPTH , HERE ,\nGIT-H12-D12 IS THE DEFAULT CONFIGURATION USED IN THIS PAPER .\nMethods Heads Depths R1 mAP Parameters (M)\nGiT-H3-D12\n3\n12 96.16 79.45 35.1\nGiT-H3-D15 15 96.22 79.61 41.9\nGiT-H3-D18 18 96.47 79.89 48.6\nGiT-H6-D12\n6\n12 96.26 79.72 53.9\nGiT-H6-D15 15 96.51 79.94 57.3\nGiT-H6-D18 18 96.73 80.11 76.8\nGiT-H12-D12\n12\n12 96.86 80.34 92.1\nGiT-H12-D15 15 97.07 80.65 113.1\nGiT-H12-D18 18 97.22 81.48 134.1\nBesides, we visualizing the class activation maps of some\nexamples with occlusion, as shown in Fig. 7. From Fig.\n7, ﬁrstly, the grad class activation maps (Grad-CAM) [98]\nvisualized show that both ViT and GiT can focus on non-\noccluded parts, which illustrates that the visual transformer\nis a powerful deep learning architecture. Secondly, compared\nwith ViT, one can observe that GiT pays more attention to\nlocal differentiated details, e.g., lights and rearview mirrors,\nwhich shows that coupling LCG to ViT is beneﬁcial.\n3) Inﬂuence of Sampling Strategy in LCG : LCG is a key\ncomponent of GiT method, thus the inﬂuence of sampling\nstrategy in LCG is evaluated. Recalling Table I, there are\nstage1, stage2 and stage3. In Table VI, the default GiT applies\nprogressive sampling sizes, i.e., LCGs of stage1, stage2 and\nstage3 use (2, 2), (4, 4) and (8, 8) sampling sizes, respectively.\nGiT1 method represents the case that always apply the (2, 2)\nsampling size to LCGs of stage1, stage2 and stage3, i.e., its\nsampling size does not change among with the depth. Similar\nto GiT1 method, GiT2 and GiT3 method respectively employ\n(4, 4) and (8, 8) sampling sizes.\nFrom Table VI, the GiT method of progressive sampling\nsizes consistently outperforms GiT1, GiT2 and GiT3 method\nthose of ﬁxed sampling sizes on three different datasets. For\nexample, on the largest Test 10000 of VeRi-Wild , GiT’s mAP\nis 1.24%, 1.47% and 2.29% higher than that of GiT1 method,\nGiT2 method and GiT3 method, respectively. On the largest\nTest2400 of VehicleID, GiT method defeats GiT1 method,\nGiT2 method and GiT3 method by 1.12%, 1.33% and 1.58%\nhigher R1 identiﬁcation rates, respectively. These results imply\nthat the progressive sampling strategy could deal with vehicles\nscale variations better, thus it is positive strategy for GiT to\npromote vehicle re-identiﬁcation performance.\n10\nTABLE IX\nTHE PERFORMANCE (%) C OMPARISON WITH STATE -OF-THE -ART\nMETHODS ON MARKET -1501 AND MSMT17.\nMethods Market-1501 [76] MSMT17 [99]\nR1 mAP R1 mAP\nPCB [100] 93.8 81.6 68.2 40.4\nOSNet [101] 94.8 84.9 78.7 52.9\nDG-Net [102] 94.8 86.0 77.2 52.3\nMGN [103] 95.7 86.9 76.9 52.1\nRGA-SC [104] 96.1 88.4 80.3 57.5\nSAN [25] 96.1 88.0 79.2 55.7\nSCSN [105] 95.7 88.5 83.8 58.5\nABDNet [106] 95.6 88.3 82.3 60.8\nViT [38] 94.9 87.0 82.1 61.5\nGiT 95.7 88.9 85.6 64.8\n4) Role of Backbones: In order to comprehensively the role\nof backbones, ResNet, ViT, and GiT are compared at different\ncomplexity scales. Following ViT [38], we use two critical\nparameters (i.e., H and D) to control model scales. H and D\ndenote respectively the number of multi-head self-attention’s\nhead and the number of GiT/ViT blocks. In Table VII, when\nthe number of depth maintaining 12, the Tiny, Small, and Base\nrespectively denote that those models cost 3, 6, and 12 heads.\nFrom Table VII, one can see that ViT and ResNet hold\ncomparable R1 and mAP. For example, the ResNet-101’s R1\nand ViT-Small’s R1 are 95.47% and 95.12% performance on\nVeRi776, respectively.These comparisons clearly show that\nGiT’s improvement is not caused by using better ViT back-\nbones.\n5) Complexity Analysis : We compare different CNN and\nTransformer models, as shown in Table VII. The FLOPs\ndenotes ﬂoating point of operations. Both GiT and ViT are\ninferior to that of ResNet, as shown in Table VII. But, even\ncoupling graph neural networks, both FLOPs and parameter\ncount of GiT are comparable to these of ViT. Especially, GiT-\nTiny defeats ViT-Base in terms of complexity and accuracy.\nBesides, for an intuitive analysis, the average inference speed\n(AIS) is applied, which is the average time for feature ex-\ntraction of each image. Similar to the FLOPs comparison, in\nterms of AIS, both GiT and ViT are inferior to that of ResNet,\nas shown in Table VII. However, GiT still are comparable to\nViT, even GiT extra introduces graph neural networks (GNN).\nTherefore, our GiT is an efﬁcient transformer, although its\nefﬁciency is weaker than ResNet.\n6) Sensitivity Analysis: For Table VIII, given the GiT-H3-\nD12 model as an example, it uses 3-head self-attention and\n12 GiT blocks. From Table VIII, one can see that either an\nincrease in H or D increases the number of GiT’s parameters\nand improves performance. For example, when H=12, D\nincreases from 12 to 18, parameters only raise from 92.1M\nto 134.1M, an increase of 45%. But when D=12, H increases\nfrom 3 to 12, parameters signiﬁcantly expand from 35.1M to\n92.1M, an increase of 162.3%.\nE. Generalization to Person Re-identiﬁcation\nAs shown in Table IX, our proposed GiT is compared with\nstate-of-the-art methods on person re-identiﬁcation datasets,\ni.e., Market-1501 [76] and MSMT17 [99] datasets. On Market-\n1501, our GiT obtains the highest mAP (i.e., 88.9%), although\nthe R1 is a bit lower than RGA-SC [104] and SAN [23] [25].\nOn the larger MSMT17 dataset, both RGA-SC [104] and SAN\n[23] [25] loss dominance, but or GiT still is strong to win best\nR1 and mAP. These results show that our GiT is general to\nperson re-identiﬁcation.\nV. C ONCLUSION\nIn this paper, we propose a graph interactive transformer\n(GiT) method for vehicle re-identiﬁcation. The GiT method\ncouples graphs and transformers to explore the interaction be-\ntween local features and global features, resulting in effective\ncooperation between local and global features. We also design\na new local correlation graph (LCG) module for learning local\nfeatures within patches. We construct extensive experiments\nto analyze our GiT method, including: (1) Role of LCG\nmodule and transformer layer interaction. (2) Visualization;\n(3) Inﬂuence of sampling strategy in LCG module; (4) Role of\nbackbones; (5) Complexity analysis; (6) Sensitivity analysis.\nREFERENCES\n[1] J.-M. Guo, C.-H. Hsia, K. Wong, J.-Y . Wu, Y .-T. Wu, and\nN.-J. Wang, “Nighttime vehicle lamp detection and tracking\nwith adaptive mask training,” IEEE Transactions on Vehicular\nTechnology, vol. 65, no. 6, pp. 4023–4032, 2015.\n[2] C.-H. Hsia and C.-H. Liu, “New hierarchical ﬁnger-vein fea-\nture extraction method for ivehicles,” IEEE Sensors Journal ,\nvol. 22, no. 13, pp. 13 612–13 621, 2022.\n[3] H. Guo, K. Zhu, M. Tang, and J. Wang, “Two-level at-\ntention network with multi-grain ranking loss for vehicle\nre-identiﬁcation,” IEEE Transactions on Image Processing ,\nvol. 28, no. 9, pp. 4328–4338, 2019.\n[4] Y . Lou, Y . Bai, J. Liu, S. Wang, and L.-Y . Duan, “Embedding\nadversarial learning for vehicle re-identiﬁcation,” IEEE Trans-\nactions on Image Processing , vol. 28, no. 8, pp. 3794–3807,\n2019.\n[5] X. Liu, S. Zhang, X. Wang, R. Hong, and Q. Tian, “Group-\ngroup loss-based global-regional feature learning for vehicle\nre-identiﬁcation,” IEEE Transactions on Image Processing ,\nvol. 29, pp. 2638–2652, 2019.\n[6] Y . Zhou, L. Liu, and L. Shao, “Vehicle re-identiﬁcation by deep\nhidden multi-view inference,” IEEE Transactions on Image\nProcessing, vol. 27, no. 7, pp. 3275–3287, 2018.\n[7] D. Shen, S. Zhao, J. Hu, H. Feng, D. Cai, and X. He, “Es-net:\nErasing salient parts to learn more in re-identiﬁcation,” IEEE\nTransactions on Image Processing , vol. 30, pp. 1676–1686,\n2020.\n[8] X. Liu, W. Liu, H. Ma, and H. Fu, “Large-scale vehicle re-\nidentiﬁcation in urban surveillance videos,” in International\nConference on Multimedia & Expo , 2016, pp. 1–6.\n[9] X. Liu, W. Liu, T. Mei, and H. Ma, “Provid: Progressive\nand multi-modal vehicle re-identiﬁcation for large-scale urban\nsurveillance,”IEEE Transactions on Multimedia, vol. 20, no. 3,\npp. 645–658, 2018.\n[10] H. Li, C. Li, X. Zhu, A. Zheng, and B. Luo, “Multi-spectral\nvehicle re-identiﬁcation: A challenge,” in Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence , vol. 34, no. 07,\n2020, pp. 11 345–11 353.\n[11] P. Khorramshahi, A. Kumar, N. Peri, S. S. Rambhatla, J.-C.\nChen, and R. Chellappa, “A dual-path model with adaptive\nattention for vehicle re-identiﬁcation,” in International Con-\nference on Computer Vision , 2019, pp. 6132–6141.\n11\n[12] D. Meng, L. Li, X. Liu, Y . Li, S. Yang, Z.-J. Zha, X. Gao,\nS. Wang, and Q. Huang, “Parsing-based view-aware embed-\nding network for vehicle re-identiﬁcation,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 2020, pp. 7103–7112.\n[13] R. Chu, Y . Sun, Y . Li, Z. Liu, C. Zhang, and Y . Wei, “Ve-\nhicle re-identiﬁcation with viewpoint-aware metric learning,”\nin International Conference on Computer Vision , 2019, pp.\n8282–8291.\n[14] J. Zhu, H. Zeng, J. Huang, S. Liao, L. Zhen, C. Cai, and\nL. Zheng, “Vehicle re-identiﬁcation using quadruple direc-\ntional deep learning features,” Transactions on Intelligent\nTransportation Systems, vol. 21, no. 1, pp. 410–420, 2020.\n[15] X. Liu, S. Zhang, Q. Huang, and W. Gao, “Ram: a region-\naware deep model for vehicle re-identiﬁcation,” in Interna-\ntional Conference on Multimedia & Expo , 2018, pp. 1–6.\n[16] J. Zhu, H. Zeng, Z. Lei, L. Zheng, and C. Cai, “A shortly\nand densely connected convolutional neural network for vehi-\ncle re-identiﬁcation,” in International Conference on Pattern\nRecognition, 2018, pp. 3285–3290.\n[17] Y . Shen, T. Xiao, H. Li, S. Yi, and X. Wang, “Learning deep\nneural networks for vehicle re-id with visual-spatio-temporal\npath proposals,” in International Conference on Computer\nVision, 2017, pp. 1918–1927.\n[18] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger,\n“Densely connected convolutional networks,” in Conference\non Computer Vision and Pattern Recognition, 2017, pp. 2261–\n2269.\n[19] Z. Wang, L. Tang, X. Liu, Z. Yao, S. Yi, J. Shao, J. Yan,\nS. Wang, H. Li, and X. Wang, “Orientation invariant feature\nembedding and spatial temporal regularization for vehicle\nre-identiﬁcation,” in International Conference on Computer\nVision, 2017, pp. 379–387.\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional\nnetworks for large-scale image recognition,” in International\nConference on Learning Representations , 2015.\n[21] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V . Vanhoucke, and A. Rabinovich, “Going deeper\nwith convolutions,” in Conference on Computer Vision and\nPattern Recognition, 2015, pp. 1–9.\n[22] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in\ndeep residual networks,” European Conference on Computer\nVision, pp. 630–645, 2016.\n[23] H. Chen, B. Lagadec, and F. Bremond, “Partition and reunion:\nA two-branch neural network for vehicle re-identiﬁcation,”\nin Conference on Computer Vision and Pattern Recognition\nWorkshops, 2019, pp. 184–192.\n[24] J. Zhu, J. Huang, H. Zeng, X. Ye, B. Li, Z. Lei, and L. Zheng,\n“Object re-identiﬁcation via joint quadruple decorrelation di-\nrectional deep networks in smart transportation,”IEEE Internet\nof Things Journal (Early Access) , 2019.\n[25] J. Qian, W. Jiang, H. Luo, and H. Yu, “Stripe-based and\nattribute-aware network: A two-branch deep model for vehicle\nre-identiﬁcation,” arXiv preprint arXiv:1910.05549 , 2019.\n[26] C. Liu, D. Q. Huynh, and M. Reynolds, “Urban area vehicle re-\nidentiﬁcation with self-attention stair feature fusion and tem-\nporal bayesian re-ranking,” in International Joint Conference\non Neural Networks , 2019, pp. 1–8.\n[27] X. Ma, K. Zhu, H. Guo, J. Wang, M. Huang, and Q. Miao,\n“Vehicle re-identiﬁcation with reﬁned part model,” in Interna-\ntional Conference on Multimedia & Expo Workshops , 2019,\npp. 603–606.\n[28] H. Guo, K. Zhu, M. Tang, and J. Wang, “Two-level atten-\ntion network with multi-grain ranking loss for vehicle re-\nidentiﬁcation,” IEEE Transactions on Image Processing , pp.\n4328–4338, 2019.\n[29] B. He, J. Li, Y . Zhao, and Y . Tian, “Part-regularized near-\nduplicate vehicle re-identiﬁcation,” in Conference on Com-\nputer Vision and Pattern Recognition , 2019, pp. 3997–4005.\n[30] X. Zhang, R. Zhang, J. Cao, D. Gong, M. You, and C. Shen,\n“Part-guided attention learning for vehicle re-identiﬁcation,”\narXiv preprint arXiv:1909.06023 , 2019.\n[31] Y . Sun, M. Li, and J. Lu, “Part-based multi-stream model\nfor vehicle searching,” in International Conference on Pattern\nRecognition, 2018, pp. 1372–1377.\n[32] P. Khorramshahi, N. Peri, A. Kumar, A. Shah, and R. Chel-\nlappa, “Attention driven vehicle re-identiﬁcation and unsuper-\nvised anomaly detection for trafﬁc understanding,” in Com-\nputer Vision and Pattern Recognition Workshops , 2019, pp.\n239–246.\n[33] Z. Tang, M. Naphade, S. Birchﬁeld, J. Tremblay, W. Hodge,\nR. Kumar, S. Wang, and X. Yang, “Pamtri: Pose-aware multi-\ntask learning for vehicle re-identiﬁcation using highly random-\nized synthetic data,” in International Conference on Computer\nVision, 2019, pp. 211–220.\n[34] X. Liu, W. Liu, J. Zheng, C. Yan, and T. Mei, “Beyond the\nparts: Learning multi-view cross-part correlation for vehicle re-\nidentiﬁcation,” in Proceedings of the 28th ACM International\nConference on Multimedia , 2020, pp. 907–915.\n[35] Z. Xu, L. Wei, C. Lang, S. Feng, T. Wang, and A. G. Bors,\n“Hss-gcn: A hierarchical spatial structural graph convolutional\nnetwork for vehicle re-identiﬁcation,” in Proc. ICPR’s Int.\nWorkshop on Human and Vehicle Analysis for Intelligent\nUrban Computing (IUC) . Springer, 2021.\n[36] Y . Zhu, Z.-J. Zha, T. Zhang, J. Liu, and J. Luo, “A struc-\ntured graph attention network for vehicle re-identiﬁcation,”\nin Proceedings of the 28th ACM international conference on\nMultimedia, 2020, pp. 646–654.\n[37] F. Shen, J. Zhu, X. Zhu, Y . Xie, and J. Huang, “Exploring\nspatial signiﬁcance via hybrid pyramidal graph network for\nvehicle re-identiﬁcation,” IEEE Transactions on Intelligent\nTransportation Systems, 2021.\n[38] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly et al., “An image is worth 16x16 words:\nTransformers for image recognition at scale,” arXiv preprint\narXiv:2010.11929, 2020.\n[39] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang, “Tran-\nsreid: Transformer-based object re-identiﬁcation,” in Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 15 013–15 022.\n[40] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable\ndetr: Deformable transformers for end-to-end object detection,”\narXiv preprint arXiv:2010.04159 , 2020.\n[41] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L.\nYuille, and Y . Zhou, “Transunet: Transformers make strong\nencoders for medical image segmentation,” arXiv preprint\narXiv:2102.04306, 2021.\n[42] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez,\nand P. Luo, “Segformer: Simple and efﬁcient design for\nsemantic segmentation with transformers,” Advances in Neural\nInformation Processing Systems , vol. 34, 2021.\n[43] B. Yu, M. Tang, L. Zheng, G. Zhu, J. Wang, H. Feng, X. Feng,\nand H. Lu, “High-performance discriminative tracking with\ntransformers,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2021, pp. 9856–9865.\n[44] B. Kim, J. Lee, J. Kang, E.-S. Kim, and H. J. Kim, “Hotr: End-\nto-end human-object interaction detection with transformers,”\nin Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2021, pp. 74–83.\n[45] G. Zhang, P. Zhang, J. Qi, and H. Lu, “Hat: Hierarchi-\ncal aggregation transformers for person re-identiﬁcation,” in\nProceedings of the 29th ACM International Conference on\nMultimedia, 2021, pp. 516–525.\n[46] S. Lai, Z. Chai, and X. Wei, “Transformer meets part model:\nAdaptive part division for person re-identiﬁcation,” inProceed-\nings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 4150–4157.\n12\n[47] X. Liu, W. Liu, T. Mei, and H. Ma, “A deep learning-based\napproach to progressive vehicle re-identiﬁcation for urban\nsurveillance,” in European Conference on Computer Vision ,\n2016, pp. 869–884.\n[48] H. Liu, Y . Tian, Y . Wang, L. Pang, and T. Huang, “Deep\nrelative distance learning: Tell the difference between similar\nvehicles,” in Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 2167–2175.\n[49] Y . Lou, Y . Bai, J. Liu, S. Wang, and L. Duan, “Veri-wild: A\nlarge dataset and a new method for vehicle re-identiﬁcation\nin the wild,” in Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 3235–3243.\n[50] K. Yan, Y . Tian, Y . Wang, W. Zeng, and T. Huang, “Ex-\nploiting multi-grain ranking constraints for precisely search-\ning visually-similar vehicles,” in International Conference on\nComputer Vision, 2017, pp. 562–570.\n[51] Y . Zhang, D. Liu, and Z.-J. Zha, “Improving triplet-wise\ntraining of convolutional neural network for vehicle re-\nidentiﬁcation,” in International Conference on Multimedia &\nExpo, 2017, pp. 1386–1391.\n[52] Y . Li, Y . Li, H. Yan, and J. Liu, “Deep joint discriminative\nlearning for vehicle re-identiﬁcation and retrieval,” in Interna-\ntional Conference on Image Processing , 2017, pp. 395–399.\n[53] N. Jiang, Y . Xu, Z. Zhou, and W. Wu, “Multi-attribute driven\nvehicle re-identiﬁcation with spatial-temporal re-ranking,” in\nInternational Conference on Image Processing, 2018, pp. 858–\n862.\n[54] H. Guo, C. Zhao, Z. Liu, J. Wang, and H. Lu, “Learning\ncoarse-to-ﬁne structured feature embedding for vehicle re-\nidentiﬁcation,” in AAAI Conference on Artiﬁcial Intelligence ,\n2018, pp. 6853–6860.\n[55] R. Kumar, E. Weill, F. Aghdasi, and P. Sriram, “Vehicle re-\nidentiﬁcation: an efﬁcient baseline using triplet embedding,”\nin International Joint Conference on Neural Networks , 2019,\npp. 1–9.\n[56] A. Kanacı, X. Zhu, and S. Gong, “Vehicle re-identiﬁcation in\ncontext,” in German Conference on Pattern Recognition, 2018,\npp. 377–390.\n[57] Y . Xu, N. Jiang, L. Zhang, Z. Zhou, and W. Wu, “Multi-scale\nvehicle re-identiﬁcation using self-adapting label smoothing\nregularization,” in IEEE International Conference on Acous-\ntics, Speech and Signal Processing , 2019, pp. 2117–2121.\n[58] Y . Lou, Y . Bai, J. Liu, S. Wang, and L.-Y . Duan, “Embed-\nding adversarial learning for vehicle re-identiﬁcation,” IEEE\nTransactions on Image Processing , pp. 3794–3807, 2019.\n[59] J. Peng, H. Wang, F. Xu, and X. Fu, “Cross domain knowledge\nlearning with dual-branch adversarial network for vehicle re-\nidentiﬁcation,” Neurocomputing, 2020.\n[60] Z. Zheng, T. Ruan, Y . Wei, Y . Yang, and T. Mei, “Vehi-\nclenet: learning robust visual representation for vehicle re-\nidentiﬁcation,” IEEE Transactions on Multimedia , 2020.\n[61] J.-B. Hou, X. Zhu, C. Liu, C. Yang, L.-H. Wu, H. Wang,\nand X.-C. Yin, “Detecting text in scene and trafﬁc guide\npanels with attention anchor mechanism,” IEEE Transactions\non Intelligent Transportation Systems , vol. 22, no. 11, pp.\n6890–6899, 2020.\n[62] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only\nlook once: Uniﬁed, real-time object detection,” in Conference\non Computer Vision and Pattern Recognition , 2016, pp. 779–\n788.\n[63] A. M. N. Tauﬁque and A. Savakis, “Labnet: Local graph\naggregation network with class balanced loss for vehicle re-\nidentiﬁcation,” arXiv preprint arXiv:2011.14417 , 2020.\n[64] D. Ji, H. Wang, H. Hu, W. Gan, W. Wu, and J. Yan, “Context-\naware graph convolution network for target re-identiﬁcation,”\narXiv preprint arXiv:2012.04298 , 2020.\n[65] M. Ye, J. Shen, D. J Crandall, L. Shao, and J. Luo, “Dynamic\ndual-attentive aggregation learning for visible-infrared person\nre-identiﬁcation,” in European Conference on Computer Vi-\nsion. Springer, 2020, pp. 229–247.\n[66] M. Ye, J. Li, A. J. Ma, L. Zheng, and P. C. Yuen, “Dy-\nnamic graph co-matching for unsupervised video-based person\nre-identiﬁcation,” IEEE Transactions on Image Processing ,\nvol. 28, no. 6, pp. 2976–2990, 2019.\n[67] M. Ye, X. Lan, Q. Leng, and J. Shen, “Cross-modality person\nre-identiﬁcation via modality-aware collaborative ensemble\nlearning,” IEEE Transactions on Image Processing , vol. 29,\npp. 9387–9399, 2020.\n[68] C. Wang, H. Fu, and H. Ma, “Global structure graph guided\nﬁne-grained vehicle recognition,” inICASSP 2020 - 2020 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2020, pp. 1913–1917.\n[69] H. Luo, W. Jiang, Y . Gu, F. Liu, X. Liao, S. Lai, and J. Gu, “A\nstrong baseline and batch normalization neck for deep person\nre-identiﬁcation,” IEEE Transactions on Multimedia (Early\nAccess), 2019.\n[70] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution\nrepresentation learning for human pose estimation,” in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2019, pp. 5693–5703.\n[71] G. Huang, D. Chen, T. Li, F. Wu, L. Van Der Maaten, and\nK. Q. Weinberger, “Multi-scale dense networks for resource ef-\nﬁcient image classiﬁcation,” arXiv preprint arXiv:1703.09844,\n2017.\n[72] Z. Zheng, L. Zheng, and Y . Yang, “Unlabeled samples gen-\nerated by gan improve the person re-identiﬁcation baseline in\nvitro,” in International Conference on Computer Vision, 2017,\npp. 3754–3762.\n[73] A. Hermans, L. Beyer, and B. Leibe, “In defense of\nthe triplet loss for person re-identiﬁcation,” arXiv preprint\narXiv:1703.07737, 2017.\n[74] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, and S. C. Hoi,\n“Deep learning for person re-identiﬁcation: A survey and\noutlook,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2021.\n[75] F. Shen, L. Lin, M. Wei, J. Liu, J. Zhu, H. Zeng, C. Cai, and\nL. Zheng, “A large benchmark for fabric image retrieval,” in\n2019 IEEE 4th International Conference on Image, Vision and\nComputing. IEEE, 2019, pp. 247–251.\n[76] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian,\n“Scalable person re-identiﬁcation: A benchmark,” in Interna-\ntional Conference on Computer Vision , 2015, pp. 1116–1124.\n[77] J. Zhu, H. Zeng, J. Huang, X. Zhu, Z. Lei, C. Cai, and\nL. Zheng, “Body symmetry and part locality guided di-\nrect nonparametric deep feature enhancement for person re-\nidentiﬁcation,” IEEE Internet of Things Journal , vol. 7, no. 3,\npp. 2053–2065, 2020.\n[78] F. Shen, M. Wei, J. Liu, H. Zeng, and J. Zhu, “Rgb and lbp-\ntexture deep nonlinearly fusion features for fabric retrieval,”\nHigh Technology Letters, vol. 26, no. 2, pp. 196–203, 2020.\n[79] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,\nG. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,\nA. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison,\nA. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and\nS. Chintala, “Pytorch: An imperative style, high-performance\ndeep learning library,” in Advances in Neural Information\nProcessing Systems, 2019, pp. 8024–8035.\n[80] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y . Yang, “Random\nerasing data augmentation,” arXiv preprint arXiv:1708.04896,\n2017.\n[81] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet\nclassiﬁcation with deep convolutional neural networks,” in\nAnnual Conference on Neural Information Processing Systems,\n2012, pp. 1097–1105.\n[82] L. Yang, P. Luo, C. L. Chen, and X. Tang, “A large-scale\ncar dataset for ﬁne-grained categorization and veriﬁcation,”\nin Conference on Computer Vision and Pattern Recognition ,\n2015, pp. 3973–3981.\n13\n[83] P. Khorramshahi, N. Peri, J.-c. Chen, and R. Chellappa, “The\ndevil is in the details: Self-supervised attention for vehicle re-\nidentiﬁcation,” in European Conference on Computer Vision .\nSpringer, 2020, pp. 369–386.\n[84] X. Zhu, Z. Luo, P. Fu, and X. Ji, “V oc-reid: Vehicle re-\nidentiﬁcation based on vehicle-orientation-camera,” in Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition Workshops, 2020, pp. 602–603.\n[85] Y . He, C. Dong, and Y . Wei, “Combination of appearance and\nlicense plate features for vehicle re-identiﬁcation,” in Interna-\ntional Conference on Image Processing, 2019, pp. 3108–3112.\n[86] Z. Sun, X. Nie, X. Xi, and Y . Yin, “Cfvmnet: A multi-\nbranch network for vehicle re-identiﬁcation based on common\nﬁeld of view,” in Proceedings of the 28th ACM International\nConference on Multimedia , 2020, pp. 3523–3531.\n[87] J. Peng, H. Wang, T. Zhao, and X. Fu, “Learning multi-\nregion features for vehicle re-identiﬁcation with context-based\nranking method,” Neurocomputing, vol. 359, pp. 427–437,\n2019.\n[88] G. Chen, T. Zhang, J. Lu, and J. Zhou, “Deep meta metric\nlearning,” in International Conference on Computer Vision ,\n2019, pp. 9547–9556.\n[89] Y . Zhou, L. Shao, and A. Dhabi, “Viewpoint-aware attentive\nmulti-view inference for vehicle re-identiﬁcation,” in Confer-\nence on Computer Vision and Pattern Recognition , 2018, pp.\n6489–6498.\n[90] X. Yang, C. Lang, P. Peng, and J. Xing, “Vehicle re-\nidentiﬁcation by multi-grain learning,” in International Con-\nference on Image Processing , 2019, pp. 3113–3117.\n[91] A. Suprem and C. Pu, “Looking glamorous: Vehicle re-id\nin heterogeneous cameras networks with global and local\nattention,” arXiv preprint arXiv:2002.02256 , 2020.\n[92] X. Jin, C. Lan, W. Zeng, and Z. Chen, “Uncertainty-aware\nmulti-shot knowledge distillation for image-based object re-\nidentiﬁcation,” in Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, vol. 34, no. 07, 2020, pp. 11 165–11 172.\n[93] Y . Bai, Y . Lou, Y . Dai, J. Liu, Z. Chen, and L.-Y . Duan, “Disen-\ntangled feature learning network for vehicle re-identiﬁcation,”\nin International Joint Conferences on Artiﬁcial Intelligence ,\n2020, pp. 474–480.\n[94] Y . Bai, Y . Lou, F. Gao, S. Wang, Y . Wu, and L.-Y . Duan,\n“Group-sensitive triplet embedding for vehicle reidentiﬁca-\ntion,” IEEE Transactions on Multimedia , vol. 20, no. 9, pp.\n2385–2399, 2018.\n[95] Y . Yuan, K. Yang, and C. Zhang, “Hard-aware deeply cascaded\nembedding,” in International Conference on Computer Vision,\n2017, pp. 814–823.\n[96] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A\nuniﬁed embedding for face recognition and clustering,” in\nConference on Computer Vision and Pattern Recognition ,\n2015, pp. 815–823.\n[97] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” in Conference on Computer Vision and\nPattern Recognition, 2016, pp. 770–778.\n[98] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh,\nand D. Batra, “Grad-cam: Visual explanations from deep\nnetworks via gradient-based localization,” in International\nConference on Computer Vision , 2017, pp. 618–626.\n[99] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer\ngan to bridge domain gap for person re-identiﬁcation,” in\nProceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 79–88.\n[100] Y . Sun, L. Zheng, Y . Yang, Q. Tian, and S. Wang, “Beyond\npart models: Person retrieval with reﬁned part pooling (and\na strong convolutional baseline),” in European Conference on\nComputer Vision, 2018, pp. 480–496.\n[101] K. Zhou, Y . Yang, A. Cavallaro, and T. Xiang, “Omni-scale\nfeature learning for person re-identiﬁcation,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision ,\n2019, pp. 3702–3712.\n[102] Z. Zheng, X. Yang, Z. Yu, L. Zheng, Y . Yang, and J. Kautz,\n“Joint discriminative and generative learning for person re-\nidentiﬁcation,” in proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2019, pp. 2138–2147.\n[103] G. Wang, Y . Yuan, X. Chen, J. Li, and X. Zhou, “Learning\ndiscriminative features with multiple granularities for person\nre-identiﬁcation,” in ACM international conference on Multi-\nmedia, 2018, pp. 274–282.\n[104] Z. Zhang, C. Lan, W. Zeng, X. Jin, and Z. Chen, “Relation-\naware global attention for person re-identiﬁcation,” inProceed-\nings of the ieee/cvf conference on computer vision and pattern\nrecognition, 2020, pp. 3186–3195.\n[105] X. Chen, C. Fu, Y . Zhao, F. Zheng, J. Song, R. Ji, and Y . Yang,\n“Salience-guided cascaded suppression network for person re-\nidentiﬁcation,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2020, pp. 3300–\n3310.\n[106] T. Chen, S. Ding, J. Xie, Y . Yuan, W. Chen, Y . Yang, Z. Ren,\nand Z. Wang, “Abd-net: Attentive but diverse person re-\nidentiﬁcation,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2019, pp. 8351–8361.",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.8092150092124939
    },
    {
      "name": "Transformer",
      "score": 0.697250247001648
    },
    {
      "name": "Computer science",
      "score": 0.6412481665611267
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4915216863155365
    },
    {
      "name": "Graph",
      "score": 0.4577331840991974
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4107632040977478
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2550652027130127
    },
    {
      "name": "Engineering",
      "score": 0.15702545642852783
    },
    {
      "name": "Voltage",
      "score": 0.09882259368896484
    },
    {
      "name": "Electrical engineering",
      "score": 0.07819616794586182
    }
  ]
}