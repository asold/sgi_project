{
  "title": "Large Language Models for Next Point-of-Interest Recommendation",
  "url": "https://openalex.org/W4396818449",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2108203263",
      "name": "Peibo Li",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A401833296",
      "name": "Maarten de Rijke",
      "affiliations": [
        "Amsterdam University of the Arts",
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2098278507",
      "name": "Hao Xue",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2140178614",
      "name": "Shuang Ao",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2021276105",
      "name": "Yang Song",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2145588295",
      "name": "Flora D. Salim",
      "affiliations": [
        "UNSW Sydney"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2110953678",
    "https://openalex.org/W4281758439",
    "https://openalex.org/W4212900074",
    "https://openalex.org/W4386729952",
    "https://openalex.org/W2471486255",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2808425487",
    "https://openalex.org/W3093261329",
    "https://openalex.org/W3128267727",
    "https://openalex.org/W2171279286",
    "https://openalex.org/W2998167534",
    "https://openalex.org/W4284679298",
    "https://openalex.org/W3034646226",
    "https://openalex.org/W6603599562",
    "https://openalex.org/W4384895066",
    "https://openalex.org/W2071702404",
    "https://openalex.org/W4284668299",
    "https://openalex.org/W4285602503",
    "https://openalex.org/W4365211555",
    "https://openalex.org/W3040157551",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W4212764525",
    "https://openalex.org/W3103248659",
    "https://openalex.org/W3164797320",
    "https://openalex.org/W2609547559",
    "https://openalex.org/W4239859962"
  ],
  "abstract": "The next Point of Interest (POI) recommendation task is to predict users'\\nimmediate next POI visit given their historical data. Location-Based Social\\nNetwork (LBSN) data, which is often used for the next POI recommendation task,\\ncomes with challenges. One frequently disregarded challenge is how to\\neffectively use the abundant contextual information present in LBSN data.\\nPrevious methods are limited by their numerical nature and fail to address this\\nchallenge. In this paper, we propose a framework that uses pretrained Large\\nLanguage Models (LLMs) to tackle this challenge. Our framework allows us to\\npreserve heterogeneous LBSN data in its original format, hence avoiding the\\nloss of contextual information. Furthermore, our framework is capable of\\ncomprehending the inherent meaning of contextual information due to the\\ninclusion of commonsense knowledge. In experiments, we test our framework on\\nthree real-world LBSN datasets. Our results show that the proposed framework\\noutperforms the state-of-the-art models in all three datasets. Our analysis\\ndemonstrates the effectiveness of the proposed framework in using contextual\\ninformation as well as alleviating the commonly encountered cold-start and\\nshort trajectory problems.\\n",
  "full_text": "Large Language Models\nfor Next Point-of-Interest Recommendation\nPeibo Li\nUniversity of New South Wales\nSydney, Australia\npeibo.li@student.unsw.edu.au\nMaarten de Rijke\nUniversity of Amsterdam\nAmsterdam, The Netherlands\nm.derijke@uva.nl\nHao Xue\nUniversity of New South Wales\nSydney, Australia\nhao.xue1@unsw.edu.au\nShuang Ao\nUniversity of New South Wales\nSydney, Australia\nshuang.ao@unsw.edu.au\nYang Song\nUniversity of New South Wales\nSydney, Australia\nyang.song1@unsw.edu.au\nFlora D. Salim\nUniversity of New South Wales\nSydney, Australia\nflora.salim@unsw.edu.au\nABSTRACT\nThe next point-of-interest (POI) recommendation task is to predict\nusersâ€™ immediate nextPOI visit given their historical data. Location-\nbased social network data, which is often used for the next POI\nrecommendation task, comes with challenges. One frequently disre-\ngarded challenge is how to effectively use the abundant contextual\ninformation present in location-based social network data. Pre-\nvious methods are limited by their numerical nature and fail to\naddress this challenge. In this paper, we propose a framework that\nuses pretrained large language models to tackle this challenge. Our\nframework allows us to preserve heterogeneous location-based\nsocial network data in its original format, hence avoiding the loss\nof contextual information. Furthermore, our framework is capable\nof comprehending the inherent meaning of contextual information\ndue to the inclusion of commonsense knowledge. In experiments,\nwe test our framework on three real-world location-based social\nnetwork datasets. Our results show that the proposed framework\noutperforms the state-of-the-art models in all three datasets. Our\nanalysis demonstrates the effectiveness of the proposed framework\nin using contextual information as well as alleviating the commonly\nencountered cold-start and short trajectory problems. Our source\ncode is available at: https://github.com/neolifer/LLM4POI\nCCS CONCEPTS\nâ€¢ Information systems â†’Recommender systems.\nKEYWORDS\nLarge language models, Point-of-interest recommendation\nACM Reference Format:\nPeibo Li, Maarten de Rijke, Hao Xue, Shuang Ao, Yang Song, and Flora\nD. Salim. 2024. Large Language Models for Next Point-of-Interest Rec-\nommendation. In Proceedings of the 47th International ACM SIGIR Con-\nference on Research and Development in Information Retrieval (SIGIR â€™24),\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07\nhttps://doi.org/10.1145/3626772.3657840\nJuly 14â€“18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 10 pages.\nhttps://doi.org/10.1145/3626772.3657840\n1 INTRODUCTION\nLocation-based social networks (LBSNs) have experienced massive\ngrowth, capitalizing on developments in mobile and localization\ntechniques, as they provide rich location-based geo-information.\nNext Point-of-interest (POI) recommendation, as one of the applica-\ntions that use LBSN data, predicts usersâ€™ next POI visit, given their\nhistorical trajectories. Existing next POI methods [18, 30, 32, 33]\nfocus on the short trajectory and cold-start problem, where users\nwith a small amount of data and short trajectories are harder to\npredict. While these methods alleviate the short trajectory and\ncold-start problem, they do not fully explore the potential of LBSN\ndata. In particular, the rich contextual information contained in\nLBSN data has the potential to precisely model usersâ€™ behavior. By\nusing contextual information, we can understand the data in a way\nbeyond statistics and even derive patterns that do not exist in the\ndata. For example, the data showing a user who frequently visits\ncollege buildings during teaching periods could indicate the userâ€™s\nidentity as a student or a college staff. Therefore, it is likely that\nthis user will behave differently during vacation between teaching\nperiods.\nTo exploit such contextual information in LBSN data, there are\nsome substantial challenges: (i) How to extract the contextual\ninformation from the raw data? And (ii) How to connect contextual\ninformation with commonsense knowledge to effectively benefit\nnext POI recommendation? Here, we consider contextual informa-\ntion as time, POI category, and geo-coordinates. And we define\ncommonsense knowledge in the context of the next POI recommen-\ndation as the capability to understand the semantics of contextual\ninformation without additional data, and to connect certain joint\npatterns of contextual information with behaviors in the real world.\nExisting next POI recommendation methods [18, 30, 32, 33] have\ntwo important limitations when dealing with contextual informa-\ntion: (i) Due to their numerical nature, they have to transform het-\nerogeneous LBSN data into numbers. For example, POI categories\nare often encoded from text into IDs. This transformation can result\nin a loss of inherent meaning associated with contextual informa-\ntion. (ii) They rely exclusively on statistics and human designs to\nunderstand contextual information and lack an understanding of\nthe semantic concepts provided by the contextual information.\narXiv:2404.17591v2  [cs.IR]  1 Aug 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Peibo Li et al.\nEncoder Decoder\nRaw data Embeddings Prediction\n(a) Feature transformation paradigm.\nLarge language model\nRaw data Response\nPrompt\n(b) Large language model-based paradigm.\nFigure 1: Comparison of two paradigms for the next POI\ntask: (a) a typical feature transformation paradigm and (b)\nthe proposed language model-based paradigm.\nLarge language models (LLMs) have demonstrated capabilities in\na variety of tasks. Question-answering, in particular, has benefited\nfrom the commonsense knowledge embedded inLLMs [1, 36]. LLMs\nhave a basic grasp of the concepts in daily life and can respond\nto usersâ€™ questions using these concepts. Inspired by this and the\ntextual nature of LBSN data, leveraging LLMs for the next POI\nrecommendation task seems a natural step. In our work, we adopt\nthe pretraining and fine-tuning paradigm, and fine-tune pretrained\nLLMs on LBSN data. As we will see below, by doing so, we are able\nto use a single LLM to deal with all types of LBSN data and better\nuse contextual information.\nMore specifically, to address challenge (i) , we transform the\nnext POI task into a question-answering task. We convert text-\nformed raw data into prompts constructed by blocks of sentences.\nEach block serves as a different module, and each sentence in it\ncontains the necessary information for that module in its original\nformat. Therefore, all heterogeneous data can be fed into a single\nmodel with tokenization that still keeps the contextual information.\nAs illustrated in Figure 1, unlike existing methods that we catego-\nrize as feature transformation paradigms, where data needs to be\ntransformed and fed into different embedding layers, our method\nallows the data to be directly used in its original format. We also\npropose a notion of trajectory similarity based on prompts, which\nis used for the cold-start problem.\nFor challenge (ii) , we use pretrained LLMs that have been\ntrained on a large corpus with rich commonsense knowledge. The\ncontextual information in the tokenized data can be understood\nwith its inherent meaning rather than being treated just as a code.\nAs an example, in Table 1, we present the POI category names in a\nreal-world dataset, categorized by their context, as done by Chat-\nGPT.1 This demonstrates that LLMs are capable of understanding\nthe inherent meaning of contextual information in LBSN data.\nContributions. The main contributions of our work are as follows:\n(1) We propose LLM4POI, a framework to use pretrained large\nlanguage models for the next POI recommendation task, which\nbrings commonsense knowledge for making use of the rich\ncontextual information in the data. To the best of our knowledge,\n1We provide ChatGPT with the dataset file and ask it to find the unique POI cate-\ngory names. Then we use the prompt â€œCan you list the category names that have\nintersections by their context?\" to get the content in Table 1. https://chat.openai.com\nTable 1: POI categories in a real-world dataset summarized\nby ChatGPT. The left column is summarized by ChatGPT. We\nfirst feed the raw csv file to ChatGPT and ask it to find the\nunique POI category names. Then we use the prompt â€˜Can\nyou list the category names that have intersections by their\ncontext?â€™ to generate the categories that are summarized\nfrom the right column, which represents the subcategories\nfrom the raw data.\nCategory (GPT) Subcategories/Category Names (Raw Data)\nFood and Dining\nRestaurant (General)\nSpecific Restaurants: American, Asian,\nItalian, Mexican, Korean, Thai,\nMediterranean, Caribbean\nSpecific Food Types : Seafood Restaurant,\nBBQ Joint, Steakhouse, Pizza Place,\nOther Dining: CafÃ©, Bistro, Diner, Bakery,\nFood Truck, Deli / Bodega, Dessert Shop\nBeverages Bar, Beer Garden, Coffee Shop,\nBrewery, Tea Room, Juice Bar\nAccommodations Hotel, Motel, Hostel, Bed and Breakfast\nShopping and\nRetail\nDepartment Store, Clothing Store,\nElectronics Store, Bookstore,\nMarket, Mall, Miscellaneous Shop\nOutdoor and\nRecreation\nPark, Beach, Zoo, Garden, Plaza,\nOther Great Outdoors,\nPlayground, Campground\nArts and\nEntertainment\nMuseum, Art Museum, Theater, Cinema,\nConcert Hall, Music Venue, Art Gallery,\nComedy Club, Performing Arts Venue\nHealth and\nFitness\nGym / Fitness Center, Spa / Massage,\nMedical Center, Yoga Studio\nTravel and\nTransport\nAirport, Train Station, Bus Station,\nSubway, Ferry, Taxi\nEducational\nInstitutions\nSchool, University, Library, Museum,\nCollege Academic Building\nProfessional and\nOffice\nOffice, Corporate Building,\nConference Room\nResidential Residential Building (Apartment / Condo),\nHome (private), Housing Development\nCultural and\nReligious Church, Temple, Shrine, Synagogue\nwe are the first to fine-tune language models on standard-sized\ndatasets to exploit commonsense knowledge for the next POI\nrecommendation task.\n(2) We propose a prompt-based trajectory similarity to combine\ninformation from historical trajectories and trajectories from\ndifferent users. With that, our proposed recommendation model\nis able to alleviate the cold-start problem and make predictions\nwith improved accuracy over trajectories of various lengths.\n(3) We conduct an extensive experimental evaluation on three real-\nworld LBSNs datasets, which shows that our proposed nextPOI\nLarge Language Models for Next Point-of-Interest Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nrecommendation model substantially outperforms state-of-the-\nart next POI recommendation models in all three datasets.\n2 RELATED WORK\n2.1 Next POI Recommendation\nSequence-based models. Early work on next POI recommenda-\ntion often treated the next POI recommendation as a sequential\nrecommendation task. Therefore, methods that had been widely\nused for other sequential recommendation tasks were adapted. For\ninstance, the next POI recommendation task was first introduced\nby Cheng et al. [4], and they adapted FMPC [21], implementing a\nlocalized region constraint where only neighborhood locations are\nconsidered for each user. He et al. [14] combined the personalized\nMarkov chain with the latent pattern by incorporating the softmax\nfunction. However, these methods are less capable of capturing\ncomplex sequential patterns compared to deep neural networks.\nSubsequent work has begun to apply RNN-based models with\nthe rise of deep learning. Kong and Wu [17] proposed HST-LSTM,\nwhere they added spatial-temporal factors into LSTM gates to guide\nthe learning and further employed a hierarchical extension to model\nthe periodicity of the visit sequence. LSTPM [22] employed three\nLSTM modules, utilizing non-local neural operations and a short-\nterm preference modeling module using geo-dilated LSTM. PLSPL\n[27] combined an embedding layer with the attention mechanism\nto learn long-term preferences and leveraged two LSTM models\nto model short-term preferences at both the location and category\nlevels. STAN [19] uses a multimodal embedding layer to learn the\nrepresentation of user, location, time, and spatial-temporal effect,\nwith a bi-layer attention architecture to learn the explicit spatial-\ntemporal relevance within the trajectories. CFPRec [ 33] focused\non the multi-step future plan of users by adopting an attention\nmechanism to extract future references from past and current pref-\nerence encoders that are transformer and LSTM encoders. These\nsequence-based models often become confined to a local view and\nalso suffer from short trajectories and the cold-start problem where\ninactive users have limited data. Our method deploys key-query\nsimilarity, which allows us to combine information from different\nusers, alleviating the cold-start problem.\nGraph-based models. More recently, graph-based methods have\nbeen incorporated to address the limitations of sequence-based\nmodels. STP-UDGAT [18] were the first to use a graph attention\nnetwork [24], enabling users to selectively learn from others in a\nglobal view. Zhang et al. [33] proposed a hierarchical multi-task\ngraph recurrent network (HMT-GRN) to learn user-POI and user-\nregion distribution, employing a GRN to replace the LSTM unit\nto learn both sequential and global spatial-temporal relationships\nbetween POIs. DRGN [26] investigated the intrinsic characteristics\nof POIs by learning disentangled representation from both distance-\nbased and transition-based relation graphs through a GCN layer.\nGETNEXT [32] addressed the cold start problem by exploiting col-\nlaborative signals from other users and proposing a global trajectory\nflow map and a novel graph-enhanced transformer model. STHGCN\n[30] alleviated cold-start issues by constructing a hypergraph to\ncapture higher-order information, including user trajectories and\ncollaborative relations. Although graph-based models handle the\ncold-start problem, they are not capable of combining contextual\ninformation with commonsense knowledge. Our method avoids\ncontextual information loss by trajectory prompting, and the LLMs\nthat we use contains commonsense knowledge to understand con-\ntextual information.\n2.2 LLMs for Time-series Data\nLLMs have proven to be effective for time-series data. The study\nby SHIFT [28] approached human mobility forecasting as a lan-\nguage translation problem rather than a traditional time-series\nproblem, utilizing a sequence-to-sequence language model comple-\nmented by a mobility auxiliary branch. AuxMobLCast [28] further\ninvestigated prompt engineering on time-series data. LLM4TS [2]\nemploys a two-stage fine-tuning approach, initially applying super-\nvised fine-tuning to align the LLM with time-series data, followed\nby downstream task-specific fine-tuning. Inspired by these works,\nwe design trajectory prompting specifically forLBSN data, allowing\nus to transform the next POI recommendation task into a question-\nanswering task.\n2.3 LLMs for Recommender Systems\nRecently, many works have adopted LLMs on recommender sys-\ntems. Zhang and Wang [34] designed multiple prompt templates\nfor different perspectives of news data and did prompt-learning\nBERT [10] to produce binary answers to templates. Then multi-\nprompt ensembling was applied to get final predictions. Harte et al.\n[13] proposed three approaches to leverage LLMs for sequential\nrecommendation. They first compute the embeddings of items and\nthen make recommendations based on the similarity of item embed-\ndings. They also directly fine-tuneLLMs to do a prompt completion\nwhere the prompt contains a list of item names without the last\nitem and LLMs are asked to complete the prompt with the name\nof the last product. They also enhanced existing sequential models\nwith embeddings from LLMs. Wang et al. [25] applied in-context\nlearning with LLMs for next POI recommendation. Our method not\nonly fine-tune LLMs for the next POI recommendation, but also\nhas carefully designed task-specific trajectory similarity to further\nutilize the power of LLM.\n3 PROBLEM DEFINITION\nThe research problem that we address in this paper is to fine-tune\nLLMs for the task of next POI recommendation. The problem can\nbe formalized as follows. Consider a dataset Dof user check-in\nrecords. Each record is represented by a tuple ğ‘ = (ğ‘¢,ğ‘,ğ‘,ğ‘¡,ğ‘” ),\nwhere:\nâ€¢ğ‘¢ denotes a user from the set ğ‘ˆ = {ğ‘¢1,ğ‘¢2,...,ğ‘¢ ğ‘ }, where ğ‘ is\nthe total number of users;\nâ€¢ğ‘ is a point of interest (POI) from the set ğ‘ƒ = {ğ‘1, ğ‘2, . . . ,ğ‘ğ‘€ },\nwhere ğ‘€ is the number of distinct POIs;\nâ€¢ğ‘ specifies the category of the POI;\nâ€¢ğ‘¡ represents the timestamp of the check-in; and\nâ€¢ğ‘”signifies the geometric coordinate of the POI.\nGiven a time interval Î”ğ‘¡, trajectories for a user ğ‘¢ are formed by\nsplitting the check-in records based on this interval. Each trajectory\nğ‘‡ğ‘¢\nğ‘– up to timestamp ğ‘¡ for user ğ‘¢is given by:\nğ‘‡ğ‘¢\nğ‘– (ğ‘¡)= {(ğ‘1,ğ‘1,ğ‘¡1,ğ‘”1),..., (ğ‘ğ‘˜,ğ‘ğ‘˜,ğ‘¡ğ‘˜,ğ‘”ğ‘˜ )},\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Peibo Li et al.\nRawcheck-in records\n[time]\n[poiid],  [catname] â€¦\n1 Key-query similarity 2 Trajectoryprompting 3 Supervisedfine-tuning\nCurrenttrajectory\nHistoricaltrajectory\n[user id]\nInstruction\nTarget\nResponseLLMPrompt\nFigure 2: Our overall large language model-based framework for next POI recommendation.\nwhere ğ‘¡1 < ğ‘¡2 < Â·Â·Â· < ğ‘¡ğ‘˜ = ğ‘¡ and ğ‘¡ğ‘˜ âˆ’ğ‘¡1 â‰¤Î”ğ‘¡.\nGiven this set of historical trajectories Tğ‘¢ = {ğ‘‡ğ‘¢\n1 ,ğ‘‡ğ‘¢\n2 ,...,ğ‘‡ ğ‘¢\nğ¿ }for\nuser ğ‘¢, where ğ¿ represents the number of trajectories for ğ‘¢, the\nobjective is to predict the POI ğ‘ğ‘˜+1 for a new trajectory ğ‘‡â€²ğ‘¢\nğ‘– (ğ‘¡),\nwhere user ğ‘¢will check in at the immediate subsequent timestamp\nğ‘¡ğ‘˜+1.\n4 METHODOLOGY\nThe overall framework of our work is presented in Figure 2. Our\nmethod includes three components: trajectory prompting, key-\nquery similarity, and supervised fine-tuning forLLMs. First, the raw\ndata is used to construct the prompt and compute the prompt-based\nkey-query similarity. Trajectory prompting uses both raw data and\nthe key-query similarity to form the prompts for theLLM. The LLM\nis then trained with supervised fine-tuning using the prompts.\n4.1 Trajectory Prompting\nInspired by [20, 29, 37], we propose trajectory prompting to convert\nsequences of user check-in data into a natural language question-\nanswering format for LLMs to follow the instruction from the\nprompt and generate the POI recommendation. This transforma-\ntion is crucial in leveraging the power of pretrainedLLMs. The idea\nof trajectory prompting is to unify heterogeneous LBSN data into\nmeaningful sentences that can be fed into LLMs. Specifically, we\nconstruct prompts by designing different blocks of sentences for\ntheir respective purposes. As shown in Table 2, a prompt consists\nof the current trajectory block, the historical trajectory block, the\ninstruction block, and the target block.\nTable 2: Structure of prompts and check-in record. Red in-\ndicates the current trajectory block. Purple indicates the\nhistorical trajectory block. Orange indicates the instruction\nblock. Blue indicates the target block.\nprompt\n<question> The following is a trajectory of\nuser [user id]: [check-in records]. There\nis also historical data: [check-in records].\nGiven the data, at [time], whichPOI id will\nuser [user id] visit? Note that POI id is an\ninteger in the range from 0 to [id range].\n<answer>: At [time], user [user id] will\nvisit POI id [poi id].\ncheck-in record\nAt [time], user [user id] visitedPOI id [poi\nid] which is a/an [poi category name] with\ncategory id [category id].\nThere are check-in record sentences for both the current tra-\njectory block and the historical trajectory block. These sentences\ncontain the necessary information in a check-in record (e.g., user\nID, timestamp, POI category name, POI category ID). Specifically,\nfor each check-in record ğ‘ = (ğ‘¢,ğ‘,ğ‘,ğ‘¡,ğ‘” ), we form the sentence\nas â€œAt [time], user [user id] visited POI id [poi id] which is a/an\n[poi category name] with category id [category id]. â€ We do not\ninclude geo-coordinates in the sentence to save the number of to-\nkens and we also find that LLMs, without specifically fine-tuning\non map data, are not able to distinguish geo-coordinates well. The\ncheck-in records then form trajectories. Note that for the current\ntrajectory block, there will only be one trajectory from the current\nuser, and there can be multiple trajectories from arbitrary users for\nthe historical trajectory block.\nThe current trajectory block provides information for the current\ntrajectory, excluding the last entry. The historical trajectory block\nincorporates historical information from both the current user and\nother users who have similar behavior patterns to the current user,\nwhich is used for dealing with short trajectory and cold start prob-\nlems. The details of selecting historical trajectories will be explained\nin Section 4.2. The instruction block guides the model on what to\nfocus on and also reminds the model of the range of POI IDs since\nthe POI IDs generated by LLMs are not by simple argmax over the\noutput probabilities of LLMs that are for the entire vocabulary. The\ntarget block contains the timestamp, user ID, and POI ID for the\ncheck-in record to be predicted, which serves as the ground truth\nfor fine-tuning and evaluation. The target block is excluded from\nthe input during prediction. We have tested addingPOI category\ninformation in both the instruction block and target block, expect-\ning to encourage the model to pay more attention to the relation\nbetween the POI ID and the POI category. However, it turned out\nthat the performance did not show a significant difference, and the\nmodel might have already learned that inner relationship.\nOur approach of prompting with blocks of sentences allows us to\nintegrate the heterogeneous LBSN data into meaningful sentences\nin its original format. The design of using blocks is easy to modify\nand extend.\n4.2 Key-Query Pair Similarity\nTo capture patterns of usersâ€™ behaviors from their historical tra-\njectories and different usersâ€™ trajectories, we propose a key-query\npair similarity computation framework that suits trajectories in a\nnatural language format. We treat each trajectory differently based\non its respective position in the prompt. When a trajectory is con-\nsidered for the current trajectory block, it is treated as the key,\nwhile any trajectory whose end time is earlier than this trajectory\nis treated as a query. We compute the similarity for all key-query\npairs. Subsequently, we select queries with high similarity values\nLarge Language Models for Next Point-of-Interest Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTrajectory\nKey Prompt Query Prompt\nTrajectory\nKey Prompt Query Prompt\nTrajectory Time\nKey Prompt Query Prompt\n... ...\nSimilaritySimilaritySimilarity\nFigure 3: The process of forming and pairing key and query\nprompts. Each trajectory is made into a key prompt and a\nquery prompt. The key prompt contains the check-in records\nexcluding the last entry of the trajectory, while the query\nprompt contains the entire trajectory. A key prompt is paired\nwith every query prompt representing the trajectories before\nthe current trajectory.\nPre-trained LLAMA2 Encoder\nHidden State\nMasked Multi-\nheaded Attention\nLayer Norm\nFeed Forward\nLayer Norm\nHidden State\nLayer Norm\nKey Prompt\nQuery Prompt\nSim\n32x\nFigure 4: Similarity computation for each pair of key and\nquery. Each pair of key and query prompts is fed into a pre-\ntrained LLAMA2 separately. We use the last hidden layer\nembeddings to compute their cosine similarity.\nfor the historical trajectory block. This approach allows us to in-\ncorporate information from other usersâ€™ trajectories that exhibit\nsimilar behavior patterns into the current trajectory.\nAs illustrated in Figure 3 and 4 , we first form the key and query\nprompts for every trajectory. Specifically, we use the template for\nthe current trajectory block. For the key prompts, we use the tra-\njectories without their last entry, and for the query prompts, we\nuse the entire trajectories. This is because when the query prompts\nare used as historical data, the historical trajectories are known,\nwhereas the key prompts are treated as the current trajectories. For\nevery key prompt, we compute the pairwise similarity for it and all\nthe query prompts representing trajectories that have an end time\nearlier than the start time of the trajectories represented by the key\nprompt.\nFor each key and query prompt, we feed it into an LLM encoder\nand get the embeddings from the last hidden layer. The process can\nbe formulated as:\nâ„ğ‘˜1 = Transformer Block(0)(Tokenizer(Key)),\nâ„ğ‘˜ğ‘™ = Transformer Block(ğ‘™âˆ’1)(â„ğ‘˜ğ‘™âˆ’1 ),\nğ¸ğ‘˜ = LN(â„ğ‘˜ğ‘› ),\n(1)\nFigure 5: For data in the training set, the <question> part in\nthe prompt is fed into the pre-trained LLM, and the <answer>\npart is used to guide the fine-tuning.\nâ„ğ‘1 = Transformer Block(0)(Tokenizer(Query)),\nâ„ğ‘ğ‘™ = Transformer Block(ğ‘™âˆ’1)(â„ğ‘˜ğ‘™âˆ’1 ),\nğ¸ğ‘ = LN(â„ğ‘˜ğ‘› ),\n(2)\nwhere Tokenizer is used for converting the prompt into a sequence\nof tokens, Transformer Block(ğ‘–) represents the ğ‘–-th Transformer\nblock in the model, LN is the layer normalization, ğ¸ğ‘˜ and ğ¸ğ‘ are\nthe final embeddings of the key and query, respectively.\nAfter getting embeddings for every key and query, we compute\nthe cosine similarity for each key-query pair. Formally,\nSim(Ek,Eq)= Ek Â·Eq\nâˆ¥Ekâˆ¥âˆ¥Eqâˆ¥. (3)\nFor each key, we select the top-ğ‘˜ queries with the highest simi-\nlarity to the key. The trajectories represented by these queries are\nthen used in the historical trajectory block for the key trajectory.\nThe process can be expressed as\nğ‘†(key)= arg topğ‘˜ {Sim(ğ‘ğ‘–,key)| ğ‘ğ‘– âˆˆğ‘„}, (4)\nwhere ğ‘„is the set of queries with the end time earlier than the start\ntime of the key.\n4.3 Supervised Fine-tuning\nFigure 5 shows the overall our overall fine-tuning process. Fine-\ntuning LLMs can be costly. We apply Parameter-Efficient-Fine-\nTuning (PEFT) techniques during the fine-tuning stage.\nLow-rank adaptation. We apply LoRA [16], freezing dense layers\nin LLMs and updating weights with rank decomposition matrices.\nFor a pretrained weight matrix ğ‘Š0 âˆˆğ‘…ğ‘‘Ã—ğ‘˜ , we replace weight\nupdates ğ‘Š0 +Î”ğ‘Š with low-rank decomposition ğ‘Š0 +ğ¿1ğ¿2, where\nğ¿1 âˆˆğ‘…ğ‘‘Ã—ğ‘Ÿ , ğ¿2 âˆˆğ‘…ğ‘Ÿ Ã—ğ‘˜ , and ğ‘Ÿ â‰ªğ‘šğ‘–ğ‘›(ğ‘‘,ğ‘˜). During training, only ğµ\nand ğ´receive gradient updates. For â„= ğ‘Š0ğ‘¥, the modified forward\npass is â„= ğ‘Š0ğ‘¥+ğ¿1ğ¿2ğ‘¥. LoRA is applied only to attention layers;\nMLP layers are frozen during fine-tuning. For an attention layer\nwith 4096 elements, LoRA reduces trainable parameters to 0.78%\nwith a rank of 16, compared to full fine-tuning.\nQuantization. To reduce GPU memory usage, we apply quanti-\nzation techniques [8, 9]. Quantization involves converting high-\nbit data types into lower-bit ones. We use the 4-bit NormalFloat\n(NF) proposed by [9], optimal for zero-mean normal distributions\nin [-1,1]. This process involves rescaling input tensors and ap-\nplying quantization constants. To reduce memory overhead, dou-\nble quantization is applied. NF4 is used for storage, while 16-bit\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Peibo Li et al.\nBrainFloating (BF) is used for forward and backward passes. Note\nthat the weight gradient is still only computed over the Low-Rank\nAdaptation (LoRA) parameters.\nFlashAttention. Long trajectories and the historical trajectory\nblock in the prompt require long context length fromLLMs. A typi-\ncal context length of 4096 is not enough for our purposes. Therefore,\nwe apply FlashAttention-2 [6, 7], which allows transformers to have\nlong context lengths.\n5 EXPERIMENT\n5.1 Experimental Setup\n5.1.1 Datasets. We conduct experiments on three public datasets:\nFoursquare-NYC, Foursquare-TKY [31], and Gowala-CA [5]. The\nfirst two datasets, collected over 11 months, comprise data from New\nYork City and Tokyo, sourced from Foursquare. The Gowala-CA\ndataset, from the Gowalla platform, covers a broader geographical\narea and time period, encompassing California and Nevada. We\nutilize data that has been preprocessed as per the methods detailed\nby Yan et al. [30]. The data is preprocessed as follows: (i) Filter out\nPoints of Interest (POIs) with fewer than 10 visit records in history;\n(ii) Exclude users with fewer than 10 visit records in history; (iii) Di-\nvide user check-in records into several trajectories with 24-hour\nintervals, excluding trajectories that contain only one check-in\nrecord. The check-in records are also sorted chronologically: the\nfirst 80% are used for the training set, the middle 10% are defined as\nthe validation set, and the last 10% are defined as the test set. Note\nthat the validation and test set has to contain all users and POIs\nthat appear in the training set. The unseen users and POIs would\nbe removed from the validation and test set.\n5.1.2 Baselines. We compare our model with the following base-\nlines:\nâ€¢FPMC [21]: rooted in the Bayesian Personalized Ranking frame-\nwork, employs a typical Markov chain combined with matrix\nfactorization to predict location transitions effectively.\nâ€¢LSTM [15]: A variant of RNN, is designed for processing sequen-\ntial data. Unlike standard RNNs, LSTMs are capable of capturing\nboth short-term and long-term dependencies in sequential pat-\nterns, making them more effective for a range of sequential data\ntasks.\nâ€¢PRME [12]: Utilizing a pairwise ranking metric embedding, this\npersonalized ranking model effectively learns sequential transi-\ntions of POIs along with capturing user-POI preferences in latent\nspace.\nâ€¢STGCN [35]: Based on LSTM, this model incorporates gating\nmechanisms to effectively model temporal and spatial intervals\nin check-in sequences, thereby capturing both short-term and\nlong-term user preferences.\nâ€¢PLSPL [27]: This recurrent model employs an attention mech-\nanism to learn short-term preferences and two parallel LSTM\nstructures for long-term preferences, integrating both through a\nuser-specific linear combination.\nâ€¢STAN [19]: Leveraging a bi-layer attention architecture, STAN\naggregates spatio-temporal correlations within user trajectories,\nlearning patterns across both adjacent and non-adjacent locations\nas well as continuous and non-continuous visits.\nâ€¢GETNext [32]: A transformer-based approach, GETNext uses a\nglobal trajectory flow map that is user-agnostic to enhance next-\nPOI predictions, alongside proposing a GCN model for generating\neffective POI embeddings.\nâ€¢STHGCN [30]: Constructing a hypergraph to capture inter and\nintra-user relations, STHGCN proposes a hypergraph transformer\nand solves the cold-start problem.\n5.1.3 Our Models. In our experiments we consider three versions\nof what we call â€œour modelâ€: (i) LLM4POI*: Our model, using\nprompts only the current trajectory block without the historical tra-\njectory block, where we use Llama-2-7b-longlora-32k [3, 23] as our\nbase LLM. (ii) LLM4POI**: A variation on LLAMA2-7b where we\nuse prompts with the historical trajectory block without applying\nkey-query similarity, where only the historical trajectories from the\ncurrent users are considered. (iii) LLM4POI: A second variation on\nLLAMA2-7b where we use prompts with the historical trajectory\nblock combined with key-query similarity, where historical trajec-\ntories from both the current users and other users are considered.\nBelow, unless stated otherwise, when we say â€œour model, â€ we refer\nto the LLM4POI variant.\n5.1.4 Evaluation Metrics. For evaluation we regard the next POI\nrecommendation task as a top-1 recommendation problem. The\nscenario we have in mind is one with â€œextreme position biasâ€ [11],\nwhere only a small amount of information can be presented to the\nuser during a single interaction. For example, a user who is on a\nbusiness trip wishes to explore the new city before the meeting.\nIn such a scenario, the user does not have the leisure to review\nmultiple options. This is a scenario that is usually considered in\nnext POI recommendation, often as the primary scenario [see, e.g.,\n30, 32]. Given this choice of scenario, our approach to the task as a\ntype of question-answering problem is a natural fit. We prioritize\nthe delivery of the most pertinent and contextually suitable recom-\nmendation, mirroring the objective of providing a single, correct\nanswer to a userâ€™s query.\nThe evaluation metric we use is Accuracy@1. It looks at what\nproportion of the test items would have been retrieved with the\ntop-1 recommended list and can be formalized as:\nAcc@1 = 1\nğ‘š\nğ‘šâˆ‘ï¸\nğ‘–=1\n1 (rank â‰¤1), (5)\nwhere 1 is the indicator function. Rank is the rank of the order of\nthe correct prediction in the recommendation list. A larger value\nrepresents better performance.\n5.1.5 Implementation Details. For fine-tuning, we use a constant\nlearning rate schedule with a learning rate of 2 Ã—105, combined\nwith a warm-up of 20 steps, a weight decay of 0, a batch size of 1\nper GPU, and a sequence length of 32,768 tokens. For each dataset,\nwe fine-tune the model for 3 epochs. We use approximately 300\nhistorical check-in records to construct the historical trajectory\nblock in the prompt. Our experiments are conducted on servers\nwith Nvidia A100 GPUs.\n5.2 Main Results\nWe compare the performance of our models and the baselines on\nthree datasets, as shown in Table 3.\nLarge Language Models for Next Point-of-Interest Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 3: Performance comparison in terms of Acc@1 on three\ndatasets. âœ“and Ã—in the History and Other Users columns\nindicate whether the model uses historical data or data from\nother users, respectively.\nModel History Other NYC TKY CA\nusers Acc@1 Acc@1 Acc@1\nFPMC Ã— Ã— 0.1003 0.0814 0.0383\nLSTM Ã— Ã— 0.1305 0.1335 0.0665\nPRME Ã— Ã— 0.1159 0.1052 0.0521\nSTGCN Ã— Ã— 0.1799 0.1716 0.0961\nPLSPL Ã— Ã— 0.1917 0.1889 0.1072\nSTAN Ã— Ã— 0.2231 0.1963 0.1104\nGETNext âœ“ âœ“ 0.2435 0.2254 0.1357\nSTHGCN âœ“ âœ“ 0.2734 0.2950 0.1730\nLLM4POI* Ã— Ã— 0.2356 0.1517 0.1016\nLLM4POI** âœ“ Ã— 0.3171 0.2836 0.1683\nLLM4POI âœ“ âœ“ 0.3372 0.3035 0.2065\nOur model substantially outperforms all baselines. Specifically,\nwe observe improvements of 23.3%, 2.8%, and 19.3% in top-1 accu-\nracy on the NYC, TKY, and CA datasets, respectively, compared\nto the state-of-the-art STHGCN. Models utilizing historical data\nperform better than those that do not, and those incorporating data\nfrom other users see further performance boosts, highlighting the\nsignificance of short trajectory and cold-start problems in next POI\nrecommendation tasks. All models except STHGCN perform best\nin NYC, with noticeable performance drops in TKY and CA. NYC\nhas the smallest number of users and POIs but a larger number of\nPOI categories than the other datasets, suggesting it has the easiest\ndata to learn. In contrast, CA covers a much wider area than NYC\nand TKY, leading to data scarcity and significantly lower model\nperformance.\n5.3 Analysis\n5.3.1 User Cold-start Analysis. Our approach incorporates the his-\ntorical trajectory block and key-query trajectory similarity to tackle\nthe cold-start problem by leveraging knowledge from diverse users.\nUser activity status greatly impacts model performance, with active\nusers providing more historical data and generally easier behavior\npatterns to learn. To assess our methodâ€™s effectiveness with inactive\nusers, we categorize users into inactive, normal, and very active\ngroups based on the number of trajectories in the training set, des-\nignating the top 30% users ranked by their number of trajectories\nas very active and the bottom 30% as inactive.\nWe compare our model with STHGCN, which is designed to\naddress the cold-start problem and has shown effectiveness with\ninactive users. The comparison, shown in Table 4, reveals our model\nsignificantly improves performance for inactive users, more than\ndoubling the baseline in NYC and increasing by over half in TKY\nand CA. This improvement underscores our methodâ€™s ability to\nleverage information from similar users effectively, especially for\nthose with limited historical data.\nInterestingly, our model performs better for inactive users than\nvery active ones, contrasting the baselinesâ€™ better performance\nwith very active users. This suggests that the similar trajectories\nwe identify for very active users are often from their own, leading\nTable 4: User cold-start analysis on the NYC, TKY, and CA\ndatasets.\nUser groups Model NYC TKY CA\nAcc@1 Acc@1 Acc@1\nInactive STHGCN 0.1460 0.2164 0.1117\nNormal STHGCN 0.3050 0.2659 0.1620\nVery active STHGCN 0.3085 0.3464 0.2040\nInactive LLM4POI 0.3417 0.3478 0.2132\nNormal LLM4POI 0.3841 0.3516 0.2057\nVery active LLM4POI 0.3088 0.2727 0.1920\nto less diverse behavior patterns and less effective prediction of\ndifficult data points. The less significant improvement in TKY and\nCA compared to NYC might be due to the higher user count in\nthese datasets, limiting the collaborative information our method\ncan utilize within the modelâ€™s context length constraints.\n5.3.2 Trajectory Length Analysis. The varying lengths of trajecto-\nries in the next POI recommendation task, reflecting different user\nbehaviors, pose another significant challenge. Short trajectories,\nwith their limited spatio-temporal information and non-significant\npatterns, are particularly challenging, especially those with only\none or two check-ins. While long trajectories contain more infor-\nmation, extracting useful patterns from them is also difficult. Our\nmethodâ€™s effectiveness varies with the trajectory length due to the\ncontext length limit, allowing fewer historical trajectories to be\nadded for long trajectories compared to short ones. To explore the\ntrade-off between long and short trajectories, we rank the lengths of\ntrajectories in the test set, defining the top 30% as long trajectories\nand the bottom 30% as short trajectories, with the rest classified as\nmiddle trajectories.\nComparing our method to STHGCN, Table 5 highlights our sub-\nstantial improvement for short trajectories in NYC. We achieve an\nimprovement of 24.4% in top-1 accuracy for short trajectories and\n31.6% for middle trajectories in NYC, demonstrating our methodâ€™s\nstrong capability to integrate historical data for short trajectories.\nInterestingly, while we perform better for short trajectories in NYC,\nthe opposite is true in TKY and CA. However, our modelâ€™s per-\nformance does not vary significantly across different trajectory\nlengths, indicating a balanced trade-off between long and short\ntrajectories.\nTable 5: Trajectory length analysis on the NYC, TKY, and CA\ndatasets.\nTrajectory types Model NYC TKY CA\nAcc@1 Acc@1 Acc@1\nShort STHGCN 0.2703 0.2787 0.1727\nMiddle STHGCN 0.2545 0.2823 0.1785\nLong STHGCN 0.3184 0.3116 0.1742\nShort LLM4POI 0.3364 0.2876 0.1955\nMiddle LLM4POI 0.3350 0.3013 0.1998\nLong LLM4POI 0.3271 0.3083 0.2037\n5.3.3 Number of Historical Data Variants. An important factor\nof our method is the number of historical trajectories used for\nthe historical trajectory block. Since there is a token limit for the\nprompt, we cannot use all the historical trajectories but only the\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Peibo Li et al.\nFigure 6: The proportion of test set prompts where the answer\nPOI IDs are included within the questions, in their respective\ndatasets.\nones with the highest similarity to the current trajectory. Because\ntrajectories vary in length, we use the number of check-in records\nin the historical trajectory block to evaluate the effect of the number\nof historical trajectories on performance.\nWe compare our model trained and evaluated with different\nnumbers of historical check-in records. As shown in Table 6, we\nobserve that the model archives the best performance in NYC with\n100 historical check-in records and decreases as the number of\ncheck-in records grows. On the other hand, the model has the best\nperformance in TKY with 300 historical check-in records, and the\nperformance is positively correlated to the number of historical\ncheck-in records. Note that the performance in TKY only improves\nby a tiny margin when the number of historical check-in records in-\ncreases from 200 to 300. The model performance remains closely in\nCA, with different numbers of different historical check-in records.\nThe results indicate that using more historical data does not neces-\nsarily improve the model performance. We can use less historical\ndata to reduce the token size of prompts but still achieve compet-\nitive model performance, which also speeds up the training and\ninference.\nTable 6: Analysis on NYC, TKY, and CA dataset for LLM4POI\ntrained on prompts with different numbers of historical tra-\njectories.\nNumber of historical NYC TKY CA\ncheck-ins Acc@1 Acc@1 Acc@1\n100 0.3420 0.2166 0.2056\n200 0.3400 0.3023 0.2035\n300 0.3372 0.3035 0.2065\n5.3.4 Generalization to Unseen Data Analysis. The fact that our\napproach does not rely on a linear classifier to output the POI IDs\nbut predicts with purely language modeling allows us to evaluate\nour models, fine-tuned on one dataset, on unseen data without any\nfurther training. We fine-tune our models on one of the NYC, TKY,\nand CA and then evaluate them on the rest.\nAs shown in Table 7, interestingly, the models achieve compet-\nitive performance on datasets which they are not fine-tuned on.\nSpecifically, the model trained in NYC has a top-1 accuracy lower\nthan STHGCN for TKY and better than the state-of-the-art models\nin CA. The model trained on TKY performs even better in NYC\nFigure 7: The proportion of test set prompts where the answer\nPOI IDs are included within the questions, in their respec-\ntive datasets, for the correct predictions made by the models\ntrained on the NYC, TKY, and CA.\nTable 7: The models are LLM4POI trained only on one of the\nNYC, TKY, and CA datasets and evaluated on the rest.\nTrained on NYC TKY CA\nAcc@1 Acc@1 Acc@1\nNYC 0.3372 0.2594 0.1885\nTKY 0.3463 0.3035 0.1960\nCA 0.3344 0.2600 0.2065\nthan in NYC and is also better than the state-of-the-art models in\nCA. The model trained on CA is better than the state-of-the-art\nmodels in NYC and has a top-1 accuracy lower than STHGCN for\nTKY. This suggests that our models generalize well to unseen data.\nWe look into the prompts to further investigate the reason for\nthe generalization ability. As shown in Figure 6, we find that 75.8%,\n73.5%, and 55.6% of the prompts in test sets of NYC, TKY, and CA,\nrespectively, have the answer POI IDs appear within the questions.\nThese portions are positively correlated to the overall model per-\nformance on each dataset. We also observe that the models trained\non different datasets behave closely on each dataset, as shown in\nFigure 7. Specifically, in their correct predictions, the portions of\nthe prompts with answer POI IDs that appear within the questions\nare almost identical. This suggests: (i) the reason why our mod-\nels have good performance is that our historical trajectory block\ncombined with key-query similarity accurately captures the useful\ninformation from the current userâ€™s historical trajectories and other\nusersâ€™ trajectories; (ii) the models learn to extract the correct POI\nIDs from the prompts directly, which helps them to generalize to\nunseen data.\n5.3.5 Contextual Information Analysis. What distinguishes our\nmethod from other works is that we use models embedded with\ncommonsense knowledge to exploit contextual information. To\nevaluate how contextual information helps our model, we replace\nthe POI category names in the prompts with texts of the same\nlengths with no meaning to mask the contextual information. We\nanalyze the model on the NYC, TKY, and CA datasets and compare\nthe results with prompts with and without contextual information.\nTable 8 shows the overall results for the models trained on NYC,\nTKY, and CA and tested with different prompts. Model performance\ndrops by a small margin when we remove the contextual informa-\ntion from POI category names in NYC. For TKY and CA, the model\nperformance decreases by 6.4% and 6.2%, respectively.\nTo further investigate the effectiveness of the contextual infor-\nmation, we evaluate the model on different levels of user activity\nLarge Language Models for Next Point-of-Interest Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nFigure 8: Statistics for different POI categories in the NYC, TKY, and CA datasets.\nTable 8: The results for LLM4POI tested with different\nprompts.\nPrompts NYC TKY CA\nAcc@1 Acc@1 Acc@1\nw/ context 0.3372 0.3035 0.2065\nw/o context 0.3310 0.2840 0.1935\nTable 9: The results for LLM4POI tested with different\nprompts in terms of users with different levels of activity.\nUser groups Prompts NYC TKY CA\nAcc@1 Acc@1 Acc@1\nInactive w/ context 0.3417 0.3478 0.2132\nNormal w/ context 0.3841 0.3516 0.2057\nVery active w/ context 0.3088 0.2727 0.1920\nInactive w/o context 0.3493 0.2751 0.2148\nNormal w/o context 0.3623 0.2715 0.1951\nVery active w/o context 0.3025 0.2884 0.1732\nwith and without contextual information, similar to Section 5.3.1.\nFrom Table 9, we observe a significant drop in performance for in-\nactive and normal users and an increase in performance for active\nusers in TKY. The performance increases for inactive users and\ndecreases for normal and active users in NYC and CA. The contex-\ntual information affects the model performance in two opposite\ndirections for datasets in two different countries. Figure 8 shows\nthe stats for POI categories of NYC, TKY, and CA. We can see that\nthe distributions of POI categories are indeed different between\nthe two countries, where almost half of the POIs in TKY are for\ntravel and transportation. Because POI contextual information dif-\nfers from two types of datasets, the models behave differently from\nTKY to NYC and CA. This supports the modelsâ€™ understanding of\nthe inherent meaning behind POI contextual information for the\nthree datasets.\n5.3.6 Effect of Different Components. We consider the performance\nof our model as the joint effect of (i) the historical trajectory block;\n(ii) key-query similarity; (iii) contextual information. To investigate\nthe effect of each component, we remove the historical trajectory\nblock in the prompt and only put the current userâ€™s historical tra-\njectories in the historical trajectory block, respectively.\nAs shown in Table 10, the results suggest that each component\ncontributes to the full model performance. Specifically, the histori-\ncal trajectory block plays a critical role, where the top-1 accuracy\ndrops as much as 50% in TKY and CA. Because without any histor-\nical trajectories, the model suffers from short trajectories, which\nthe datasets mostly consist of. With key-query similarity being\nremoved, the collaborative information from other users is miss-\ning from the historical block, which leads to the inability to deal\nwith the cold-start problem. On top of these two components, con-\ntextual information provides a further understanding of the data,\nimproving the modelâ€™s performance.\nTable 10: Ablation study results for LLM4POI over three\ndatasets.\nModel NYC TKY CA\nAcc@1 Acc@1 Acc@1\nFull model 0.3372 0.3035 0.2065\nw/o history 0.2356 0.1517 0.1016\nw/o similarity 0.3171 0.2836 0.1683\nw/o context 0.3310 0.2840 0.1935\n6 CONCLUSION AND FUTURE WORK\nIn this paper, we propose LLM4POI, a framework to deploy large\nlanguage models for the next point-of-interest recommendation\ntask, which is the first to utilize models with commonsense knowl-\nedge for the task. We developed trajectory prompting to transform\nthe task into a question-answering. We also introduce key-query\nsimilarity to alleviate the cold-start problem.\nOur comprehensive experiments conducted on three real-world\ndatasets show that we outperform all baseline models by a large\nmargin. Our analysis supports that our method is able to handle the\ncold-start problem and various lengths of trajectories. It also shows\nthe effectiveness of contextual information in our model. We have\nalso shown the potential of developing foundation models for the\nnext POI recommendation task, given the ability of large language\nmodels to generalize to unseen data.\nBecause of the nature of large language models, we have lim-\nitations with efficiency regarding model training and inference\ntime. Our design of the prompt is also limited by the context length\nand pretrained corpus of the model, e.g., the deprecation of geo-\ncoordinates. For future work, we plan to address the limitations\njust mentioned. In addition, we will investigate chain-of-thought\nreasoning for the next point-of-interest recommendation task, to\nboth further boost the performance and provide explanations for\nthe prediction. Another line of future work is to extend our models\nto scenarios without the extreme focus on a single best item, going\nbeyond a question-answering context that we focused on here.\nAcknowledgements. This research was conducted by the ARC\nCentre of Excellence for Automated Decision-Making and Society\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Peibo Li et al.\n(CE200100005), and funded by the Australian Government through\nthe Australian Research Council. In addition, this research was\nundertaken with the assistance of resources and services from the\nNational Computational Infrastructure (NCI), which is supported\nby the Australian Government. Peibo Li is supported by Google\nConference Scholarships (APAC). Maarten de Rijke was partially\nsupported by the Dutch Research Council (NWO), under project\nnumbers 024.004.022, NWA.1389.20.183, and KICH3.LTP.20.006.\nAll content represents the opinion of the authors, which is not\nnecessarily shared or endorsed by their respective employers and/or\nsponsors.\nREFERENCES\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language Models are Few-shot Learners. InAdvances in neural\ninformation processing systems , Vol. 33. 1877â€“1901.\n[2] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. 2023. LLM4TS: Two-Stage\nFine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. arXiv preprint\narXiv:2308.08469.\n[3] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and\nJiaya Jia. 2023. LongLoRA: Efficient Fine-tuning of Long-Context Large Language\nModels. arXiv:2309.12307.\n[4] Chen Cheng, Haiqin Yang, Michael R. Lyu, and Irwin King. 2013. Where You\nLike to Go Next: Successive Point-of-Interest Recommendation. In IJCAI 2013,\nProceedings of the 23rd International Joint Conference on Artificial Intelligence,\nBeijing, China, August 3-9, 2013 , Francesca Rossi (Ed.). IJCAI/AAAI, 2605â€“2611.\n[5] Eunjoon Cho, Seth A Myers, and Jure Leskovec. 2011. Friendship and Mobility:\nUser Movement in Location-based Social Networks. In Proceedings of the 17th\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining .\n1082â€“1090.\n[6] Tri Dao. 2023. FlashAttention-2: Faster Attention with Better Parallelism and\nWork Partitioning. arXiv preprint arXiv:2307.08691 .\n[7] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2022. FlashAt-\ntention: Fast and Memory-efficient Exact Attention with IO-awareness.Advances\nin Neural Information Processing Systems 35 (2022), 16344â€“16359.\n[8] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2021. 8-bit\nOptimizers via Block-wise Quantization. In International Conference on Learning\nRepresentations.\n[9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.\nQLoRA: Efficient Finetuning of Quantized LLMs.arXiv preprint arXiv:2305.14314 .\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association\nfor Computational Linguistics, Minneapolis, Minnesota, 4171â€“4186. https://doi.\norg/10.18653/v1/N19-1423\n[11] Yaron Fairstein, Elad Haramaty, Arnon Lazerson, and Liane Lewin-Eytan. 2022.\nExternal Evaluation of Ranking Models under Extreme Position-Bias. In Proceed-\nings of the Fifteenth ACM International Conference on Web Search and Data Mining .\nAssociation for Computing Machinery, New York, NY, USA, 252â€“261.\n[12] Shanshan Feng, Xutao Li, Yifeng Zeng, Gao Cong, and Yeow Meng Chee. 2015.\nPersonalized Ranking Metric Embedding for Next New POI Recommendation. In\nIJCAIâ€™15 Proceedings of the 24th International Conference on Artificial Intelligence .\nACM, 2069â€“2075.\n[13] Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Diet-\nmar Jannach, and Marios Fragkoulis. 2023. Leveraging Large Language Models\nfor Sequential Recommendation. In Proceedings of the 17th ACM Conference on\nRecommender Systems . 1096â€“1102.\n[14] Jing He, Xin Li, Lejian Liao, Dandan Song, and William Cheung. 2016. Inferring\na Personalized Next Point-of-interest Recommendation Model with Latent Be-\nhavior Patterns. In Proceedings of the AAAI Conference on Artificial Intelligence ,\nVol. 30. Issue: 1.\n[15] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long Short-term Memory.Neural\nComputation 9, 8 (1997), 1735â€“1780.\n[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-rank Adaptation of Large\nLanguage Models. arXiv preprint arXiv:2106.09685 .\n[17] Dejiang Kong and Fei Wu. 2018. HST-LSTM: A Hierarchical Spatial-Temporal\nLong-short Term Memory Network for Location Prediction.. In IJCAI, Vol. 18.\n2341â€“2347. Issue: 7.\n[18] Nicholas Lim, Bryan Hooi, See-Kiong Ng, Xueou Wang, Yong Liang Goh, Ren-\nrong Weng, and Jagannadan Varadarajan. 2020. STP-UDGAT: Spatial-Temporal-\nPreference User Dimensional Graph Attention Network for Next POI Recommen-\ndation. In Proceedings of the 29th ACM International Conference on Information &\nKnowledge Management . 845â€“854.\n[19] Yingtao Luo, Qiang Liu, and Zhaocheng Liu. 2021. Stan: Spatio-Temporal At-\ntention Network for Next Location Recommendation. In Proceedings of the web\nconference 2021 . 2177â€“2185.\n[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\n2021. Learning Transferable Visual Models from Natural Language Supervision.\nIn International conference on machine learning . PMLR, 8748â€“8763.\n[21] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factoriz-\ning Personalized Markov Chains for Next-basket Recommendation. InProceedings\nof the 19th international conference on World wide web . 811â€“820.\n[22] Ke Sun, Tieyun Qian, Tong Chen, Yile Liang, Quoc Viet Hung Nguyen, and\nHongzhi Yin. 2020. Where to Go Next: Modeling Long- and Short-term User\nPreferences for Point-of-interest Recommendation. In Proceedings of the AAAI\nConference on Artificial Intelligence , Vol. 34. 214â€“221. Issue: 01.\n[23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open Foundation and Fine-tuned Chat Models. arXiv\npreprint arXiv:2307.09288 .\n[24] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-\nference on Learning Representations .\n[25] Xinglei Wang, Meng Fang, Zichao Zeng, and Tao Cheng. 2023. Where Would I\nGo Next? Large Language Models as Human Mobility Predictors. arXiv preprint\narXiv:2308.15197 (2023).\n[26] Zhaobo Wang, Yanmin Zhu, Haobing Liu, and Chunyang Wang. 2022. Learning\nGraph-based Disentangled Representations for Next POI Recommendation. In\nProceedings of the 45th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 1154â€“1163.\n[27] Yuxia Wu, Ke Li, Guoshuai Zhao, and Xueming Qian. 2020. Personalized Long-\nand Short-term Preference Learning for Next POI Recommendation. IEEE Trans-\nactions on Knowledge and Data Engineering 34, 4 (2020), 1944â€“1957.\n[28] Hao Xue, Flora D. Salim, Yongli Ren, and Charles L.A. Clarke. 2022. Translating\nHuman Mobility Forecasting through Natural Language Generation. In Proceed-\nings of the Fifteenth ACM International Conference on Web Search and Data Mining .\n1224â€“1233.\n[29] Hao Xue, Bhanu Prakash Voutharoja, and Flora D. Salim. 2022. Leveraging\nLanguage Foundation models for Human Mobility Forecasting. In Proceedings of\nthe 30th International Conference on Advances in Geographic Information Systems .\n1â€“9.\n[30] Xiaodong Yan, Tengwei Song, Yifeng Jiao, Jianshan He, Jiaotuan Wang, Ruopeng\nLi, and Wei Chu. 2023. Spatio-Temporal Hypergraph Learning for Next POI\nRecommendation. In Proceedings of the 46th International ACM SIGIR Conference\non Research and Development in Information Retrieval . 403â€“412.\n[31] Dingqi Yang, Daqing Zhang, Vincent W Zheng, and Zhiyong Yu. 2014. Modeling\nUser Activity Preference by Leveraging User Spatial Temporal Characteristics in\nLBSNs. IEEE Transactions on Systems, Man, and Cybernetics: Systems 45, 1 (2014),\n129â€“142.\n[32] Song Yang, Jiamou Liu, and Kaiqi Zhao. 2022. GETNext: Trajectory Flow Map\nEnhanced Transformer for Next POI Recommendation. In Proceedings of the 45th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR â€™22) . 1144â€“1153.\n[33] Lu Zhang, Zhu Sun, Ziqing Wu, Jie Zhang, Yew Soon Ong, and Xinghua Qu.\n2022. Next Point-of-interest Recommendation with Inferring Multi-step Future\nPreferences. In Proceedings of the 31st International Joint Conference on Artificial\nIntelligence (IJCAI) . 3751â€“3757.\n[34] Zizhuo Zhang and Bang Wang. 2023. Prompt Learning for News Recommendation.\nIn Proceedings of the 46th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 227â€“237.\n[35] Pengpeng Zhao, Anjing Luo, Yanchi Liu, Jiajie Xu, Zhixu Li, Fuzhen Zhuang,\nVictor S Sheng, and Xiaofang Zhou. 2020. Where to Go Next: A Spatio-tTemporal\nGated Network for Next POI Recommendation. IEEE Transactions on Knowledge\nand Data Engineering 34, 5 (2020), 2512â€“2524.\n[36] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A Survey\nof Large Language Models. arXiv preprint arXiv:2303.18223 .\n[37] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning\nto Prompt for Vision-Language Models. International Journal of Computer Vision\n130, 9 (2022), 2337â€“2348.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7084952592849731
    },
    {
      "name": "Point (geometry)",
      "score": 0.5092324018478394
    },
    {
      "name": "Point of interest",
      "score": 0.4578169882297516
    },
    {
      "name": "Artificial intelligence",
      "score": 0.21297022700309753
    },
    {
      "name": "Mathematics",
      "score": 0.06429201364517212
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31746571",
      "name": "UNSW Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I4210135670",
      "name": "Amsterdam University of the Arts",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    }
  ],
  "cited_by": 36
}