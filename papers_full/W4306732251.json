{
  "title": "Longitudinal analysis of sentiment and emotion in news media headlines using automated labelling with Transformer language models",
  "url": "https://openalex.org/W4306732251",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2303884451",
      "name": "David Rozado",
      "affiliations": [
        "Te Pūkenga"
      ]
    },
    {
      "id": "https://openalex.org/A2123438191",
      "name": "Ruth Hughes",
      "affiliations": [
        "University of Otago"
      ]
    },
    {
      "id": "https://openalex.org/A2052149045",
      "name": "Jamin Halberstadt",
      "affiliations": [
        "University of Otago"
      ]
    },
    {
      "id": "https://openalex.org/A2303884451",
      "name": "David Rozado",
      "affiliations": [
        "New College"
      ]
    },
    {
      "id": "https://openalex.org/A2123438191",
      "name": "Ruth Hughes",
      "affiliations": [
        "New College"
      ]
    },
    {
      "id": "https://openalex.org/A2052149045",
      "name": "Jamin Halberstadt",
      "affiliations": [
        "New College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2014638150",
    "https://openalex.org/W6668813879",
    "https://openalex.org/W2139121205",
    "https://openalex.org/W3096450928",
    "https://openalex.org/W3121315632",
    "https://openalex.org/W3176626547",
    "https://openalex.org/W3110558290",
    "https://openalex.org/W2656044294",
    "https://openalex.org/W2159473174",
    "https://openalex.org/W6734354879",
    "https://openalex.org/W3183304904",
    "https://openalex.org/W3182519352",
    "https://openalex.org/W3016719704",
    "https://openalex.org/W2883859396",
    "https://openalex.org/W1988147352",
    "https://openalex.org/W3006544624",
    "https://openalex.org/W2794635328",
    "https://openalex.org/W3008291289",
    "https://openalex.org/W4206588201",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4283168218",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W1966797434",
    "https://openalex.org/W1984536024",
    "https://openalex.org/W1568955415",
    "https://openalex.org/W2153222072",
    "https://openalex.org/W2595186334",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3103365499",
    "https://openalex.org/W2073594025"
  ],
  "abstract": "This work describes a chronological (2000–2019) analysis of sentiment and emotion in 23 million headlines from 47 news media outlets popular in the United States. We use Transformer language models fine-tuned for detection of sentiment (positive, negative) and Ekman’s six basic emotions (anger, disgust, fear, joy, sadness, surprise) plus neutral to automatically label the headlines. Results show an increase of sentiment negativity in headlines across written news media since the year 2000. Headlines from right-leaning news media have been, on average, consistently more negative than headlines from left-leaning outlets over the entire studied time period. The chronological analysis of headlines emotionality shows a growing proportion of headlines denoting anger , fear , disgust and sadness and a decrease in the prevalence of emotionally neutral headlines across the studied outlets over the 2000–2019 interval. The prevalence of headlines denoting anger appears to be higher, on average, in right-leaning news outlets than in left-leaning news media.",
  "full_text": "RESEA RCH ARTICL E\nLongitudinal analysis of sentiment and\nemotion in news media headlines using\nautomated labelling with Transformer\nlanguage models\nDavid Rozado\nID\n1\n*, Ruth Hughes\n2\n, Jamin Halberstadt\n2\n1 Te Pūkenga–N ew Zealand Institute of Skills and Technolo gy, Dunedin, Otago, New Zealand,\n2 Department of Psychology, Univers ity of Otago, Dunedin, Otago, New Zealand\n* david.ro zado@op.ac .nz\nAbstract\nThis work describes a chronological (2000–2019) analysis of sentiment and emotion in 23\nmillion headlines from 47 news media outlets popular in the United States. We use Trans-\nformer language models fine-tuned for detection of sentiment (positive, negative) and\nEkman’s six basic emotions (anger, disgust, fear, joy, sadness, surprise) plus neutral to\nautomatically label the headlines. Results show an increase of sentiment negativity in head-\nlines across written news media since the year 2000. Headlines from right-leaning news\nmedia have been, on average, consistently more negative than headlines from left-leaning\noutlets over the entire studied time period. The chronological analysis of headlines emotion-\nality shows a growing proportion of headlines denoting anger, fear, disgust and sadness\nand a decrease in the prevalence of emotionally neutral headlines across the studied outlets\nover the 2000–2019 interval. The prevalence of headlines denoting anger appears to be\nhigher, on average, in right-leaning news outlets than in left-leaning news media.\nIntroduction\nHeadlines from written news media constitute an important source of information about cur-\nrent affairs. News and opinion articles headlines often establish the first point of contact\nbetween an article and potential readers, with the reader often deciding whether to engage\nmore in-depth with an article’s content after evaluating its headline [1]. In doing so, headlines\nalso set the tone about the main text body of the article and affect readers’ processing of arti-\ncles’ content to the point of constraining further information processing and biasing readers\ntowards specific interpretations of the article [2, 3].\nThe sentiment and emotionality of text has been shown to influence its virality [4]. Textual\ncontent that evokes high arousal, such as text conveying an emotion of anger, diffuses more\nprofusely through online platforms [5, 6]. Emotionally charged fake news also spread further\nand fastest through social media [7]. A study measuring the reach of tweets found that each\nmoral or emotional word used in a tweet increased its virality by 20 percent, on average [8].\nPLOS ONE\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 1 / 14\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Rozado D, Hughes R, Halberstadt J\n(2022) Longitudin al analysis of sentiment and\nemotion in news media headlines using automated\nlabelling with Transfo rmer language models. PLoS\nONE 17(10): e0276367. https://doi. org/10.1371/\njournal.pone .0276367\nEditor: Sergio Consoli, European Commission,\nITALY\nReceived: January 31, 2022\nAccepted: October 5, 2022\nPublished: October 18, 2022\nCopyright: © 2022 Rozado et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: The URLs sources of\narticles’ headlines, the Transform er models used\nfor sentiment/e motion predictions , the sentiment\nand emotion labels annotations generated by the\nTransform er language models for each headline,\nthe human sentimen t/emotion annotatio ns for a\nsmall subset of headlines used as ground truth to\nevaluate models’ performan ce and the analysis\nscripts are available in the following repository:\nhttps://do i.org/10.5281/ze nodo.5144113 .\nThus, user engagement can be maximized by news articles posts that trigger negative senti-\nment/emotions [9]. This creates a financial incentive for news outlets to maximize incoming\nweb traffic by modulating the emotional saliency of headlines.\nNews content has also been shown to be predictive of public mood [10], public opinion\n[11] and outlets’ biases [12, 13]. Thus, studying the sentiment (positive/negative) and emo-\ntional payload (anger, disgust, fear, joy, sadness, surprise or neutral) of news articles headlines\nis of sociological interest. As far as we can tell however, a comprehensive longitudinal analysis\nof news media headlines sentiment and emotion remains lacking in the existing literature.\nHere, we attempt to remedy this knowledge gap by documenting chronologically the senti-\nment and emotion of headlines in a representative sample of news media outlets.\nExamining written sources using human coders has been useful in the sociological analysis\nof text content [14–16]. Unfortunately, this approach is limited by its inability to scale to large\ncorpora and by low intercoder reliability when examining subtle themes. Computational con-\ntent analysis techniques circumvent some of the limitations of content analysis using human\nraters by permitting the quantification of textual attributes in vast text corpora [17, 18].\nModern machine learning language models constitute an important tool for the automated\nanalysis of text [13, 19–21]. In particular, Transformer models [22, 23] have achieved state-of-\nthe-art performance in numerous Natural Language Processing (NLP) tasks. A Transformer\nmodel is a deep neural network that learns words’ context and thus meaning by using a mecha-\nnism known as self-attention–a form of differentially weighting the significance of each part of\nthe input sentence when constructing word embeddings. Transformer architectures have\nreached prediction accuracies that match human annotations for text classification tasks such\nas the labelling of sentiment polarity [23]. Thus, computational content analysis of large chro-\nnological corpora using state-of-the-art machine learning models can provide insight about\nthe temporal dynamics of semantic content in vast textual corpora [19].\nThis work uses modern Transformer language models, fine-tuned for text classification, to\nautomatically label the sentiment polarity and emotional charge of a large data set of news arti-\ncles headlines (N = 23 million). The set of news outlets analyzed was derived from the AllSides\nMedia Bias Chart 2019 v1.1 [24] which lists 47 of the most popular news media outlets in the\nUnited States. Leveraging the diachronic nature of the corpus (2000–2019), we carry out a lon-\ngitudinal analysis of sentiment polarity and emotional payload over time. Using external labels\nof news media outlets political leanings from the AllSides organization [24], we also examine\nthe sentiment and emotional dynamics of headlines controlling for the ideological orientation\nof news outlets.\nMethods\nEthics approval\nInstitutional ethics approval for gathering from human raters the sentiment and emotion\nannotations of a subset of news media headlines was obtained from the University of Otago\nEthics Committee (reference number for proposal: D21/234). The human raters recruited for\nthe annotation of the headlines provided written informed consent to participate in the study.\nAnalysis scripts and data availability\nThe URLs sources of articles’ headlines, the Transformer models used for sentiment/emotion\npredictions, the sentiment and emotion labels annotations generated by the Transformer lan-\nguage models for each headline, the human sentiment/emotion annotations for a small subset\nof headlines used as ground truth to evaluate models’ performance and the analysis scripts are\navailable in the following repository: https://doi.org/10.5 281/zenodo.5144113.\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 2 / 14\nFunding: The author(s) received no specific\nfunding for this work.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nHeadlines data\nThe set of news media outlets analysed was derived from the AllSides organization 2019 Media\nBias Chart v1.1 [24]. The human ratings of outlets’ ideological leanings were also taken from\nthis chart. The AllSides Media Bias Chart has been used previously in the literature as a repre-\nsentative sample of popular U.S. news media outlets and as a ground truth of news outlets\nideological leanings [6, 12, 25].\nIn total, we analyzed 23+ Million headlines from 47 news media outlets over the period\n2000–2019. Average headline length in number of characters was 58.3. Average headline\nlength in number of tokens (i.e. unigrams) was 9.4. See S1 File for detailed histograms about\nthese metrics.\nNews articles headlines from the set of outlets listed in Fig 1 are available in the outlets’\nonline domains and/or public cache repositories such as The Internet Wayback Machine,\nGoogle cache and Common Crawl. Articles headlines were located in articles’ HTML raw data\nusing outlet-specific XPath expressions.\nTo avoid unrepresentative samples, we established an inclusion criteria threshold of at least\n100 outlet headlines in any given year in order for the year to be included in the outlet time\nseries. The temporal coverage of headlines across news outlets is not uniform. For some media\norganizations, news articles availability in online domains or Internet cache repositories\nbecomes sparse for earlier years. Furthermore, some news outlets popular in 2019, such as The\nHuffington Post or Breitbart, did not exist in the early 2000’s. Hence, our data set is sparser in\nheadlines sample size and representativeness for earlier years in the 2000–2019 range. Never-\ntheless, 18 outlets in our data set have chronologically continuous availability of headlines\nFig 1. The solid blue line shows the average yearly sentime nt of headlines across 47 popular news media outlets. The shaded area indicates the 95%\nconfiden ce interval around the mean. A statistical test for the null hypothesis of zero slope is shown on the bottom left of the plot. The percentag e change on\naverage yearly sentimen t across outlets between 2000 and 2019 is shown on the top left of the plot.\nhttps://do i.org/10.1371/j ournal.pone .0276367.g00 1\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 3 / 14\nfulfilling our inclusion criteria since the year 2000. This smaller subset with a total of 12.5 Mil-\nlion headlines was used to replicate our experiments and confirm the validity of the results\nwhen using a fixed set of outlets over time, see S1 File for a detailed report about the number\nof headlines per outlet/year in our analysis.\nUsing a Transformer language model to predict the sentiment of headlines\nAutomated sentiment polarity annotation refers to the usage of computational tools to predict\nthe sentiment polarity (positive or negative) of a text instance. Although the sentiment polarity\nof individual instances of text can sometimes be ambiguous, and humans can occasionally dis-\nagree about the sentiment of a particular piece of text, aggregating sentiment polarity over a\nlarge set of text instances provides a robust measurement of overall sentiment in a corpus\nsince automated individual annotations accuracy is well above chance guessing.\nIn recent years, Transformer models have reached state-of-the-art results for automated\nsentiment polarity detection in natural language text [23]. In this work we use SiEBERT, a pub-\nlic checkpoint of a RoBERTa-large Transformer architecture [26] previously fine-tuned and\nevaluated for sentiment analysis on 15 data sets from diverse text sources to enhance generali-\nzation of sentiment annotations across different types of text [27]. Due to the heterogeneity of\nsources used for fine-tuning, SiEBERT outperforms the accuracy of a DistilBERT-based model\nfine-tuned solely on the popular Stanford Sentiment Treebank 2 (SST-2) data set by more than\n15 percentage points (93.2 vs. 78.1 percent) [28]. The fine-tuning hyperparameters of SiEBERT\nwere: learning rate = 2×10\n−5\n, number of training epochs = 3.0, number of warmup steps = 500,\nweight decay = 0.01 [27, 28].\nTo validate the usage of the Transformer model for estimating headline sentiment, we mea-\nsured the performance of the fine-tuned SiEBERT model in a random sample of 1,120 head-\nlines from our data set that we had manually annotated for positive/negative sentiment using\nraters recruited through Mechanical Turk. We used these labels as ground truth to measure\nthe performance of the SiEBERT model when predicting the sentiment of news media head-\nlines. Only individuals over 18 years old and residents of the United States of America were\nallowed to take part. In total, 71 individuals (measured as independent IP addresses) took part\nin the headlines sentiment annotation task. The SiEBERT model fine-tuned for sentiment\nannotation reached an accuracy of 75% on this task. Note that human sentiment annotations\nintercoder agreement on the same task was 80% (Cohen’s Kappa: 0.59). These results hint at\nthe validity of the Transformer model to, on aggregate, measure the sentiment of news media\nheadlines on par with human annotations.\nWe used the SiEBERT model fine-tuned for sentiment classification to automatically anno-\ntate the sentiment of every headline in our data set. We then averaged the sentiment scores of\nall headlines of each news outlet in any given year to obtain time series of yearly headlines sen-\ntiment polarity for each outlet. Headlines with more than 32 tokens were truncated prior to\nautomated annotation for GPU memory computational efficiency. To further validate our\nresults, we replicated our experiments using the popular DistilBERT-based model fine-tuned\non the SST-2 data set [29].\nUsing a Transformer language model to predict the emotion of headlines\nMachine learning language models can also be used to detect the emotionality of text by gener-\nating emotional categories annotations for instances of natural language text. We used a public\nTransformer DistilRoBERTa-base checkpoint previously fine-tuned on 6 different emotion\ndata sets for recognizing Ekman’s 6 basic emotions (anger, disgust, fear, joy, sadness, and sur-\nprise) plus neutral [28, 30, 31]. The fine-tuning hyperparameters of this model were: learning\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 4 / 14\nrate = 5×10\n−5\n, number of training epochs = 3.0, number of warmup steps = 500, weight\ndecay = 0.01 [31].\nThe datasets used for fine tuning represent a diverse collection of text types, such as Twitter,\nReddit, student self-reports or TV dialogues. The heterogeneity of data sets used for fine tun-\ning was intended by the original authors to enhance the generalization of emotion predictions\nacross different types of text.\nTo validate the ability of the model to generate accurate emotional annotations of headlines\nin our data set, we used the DistilRoBERTa-base fine-tuned for emotion recognition on a ran-\ndom sample of 5,353 headlines from our data set that we had annotated through Mechanical\nTurk for Ekman’s 6 basic emotion types plus neutral and that we used as ground truth to esti-\nmate model’s performance. Only individuals over 18 years old and residents of the United\nStates of America were allowed to take part. In total, 143 individuals (measured as independent\nIP addresses) took part in the headlines’ emotion annotation task.\nThe DistilRoBERTa model achieved 39% classification accuracy on the task of classifying\nthe headlines for which we had human-generated classification labels and which we used as\nground truth (random guessing would be expected to reach 14%). Note that human interrater\nagreement on this task was also very low, 36%. See S1 File for detailed analysis. Also, since the\nemotion classes are not balanced in the data set of human annotated headlines’ emotionality,\nthe accuracy metric is not particularly informative. Thus, we report the weighted precision,\nrecall and F-1 scores of the model as 0.37, 0.39 and 0.36 respectively, see S1 File for detailed\nreporting for each emotional category and corresponding confusion matrices. Cohen’s kappa\nbetween model predictions and ground truth was 0.16. Matthew’s correlation coefficient\nbetween model predictions and ground truth was 0.16. Both metrics are relatively low but\nabove the 0 level indicative of weighted random guessing. The performance of the model was\nabove chance guessing for all emotional categories except surprise. Thus, in the Results section\nwe drop this category for all subsequent analyses.\nInterrater agreement between human raters for the emotion annotation task was 36%\n(Cohen’s Kappa = 0.16). Thus, interrater agreement was better than chance but relatively low.\nThis is suggestive of the emotional annotation task being inherently ambiguous and/or subjec-\ntive. For all emotional categories except surprise, interrater agreement between pairs of\nhumans and between humans and the model was very similar. Thus, the performance of the\nmodel is mostly on par with human annotations. When using such a model to annotate a large\nnumber of headlines aggregated by year, yearly central tendency estimations should be more\nrobust than noisy individual headline predictions.\nTo confirm that the automated model can detect overall trends in the emotional valence of\nheadlines over time, we carried out a simulation using the true positive and false positive rates\nof the model for the different emotion categories to generate simulated annotations of illustra-\ntive hardcoded trends (see S1 File for details), and averaging those simulated predictions per\nyear. When averaging a small set of simulated headlines emotion predictions per year\n(N = 100), the resulting average is unable to capture the underlying dynamics of headline emo-\ntionality. However, when aggregating a larger set of simulated headlines emotion predictions\nper year (N = 2,000), the resulting average is able to loosely capture the emotional dynamics of\nmost emotion categories. When aggregating an even larger set of simulated headlines emotion\npredictions per year (N = 10,000 or N = 100,000), the resulting average is able to capture the\nemotional dynamics of all emotion categories except surprise with moderate to very high cor-\nrelation. The underperformance in the simulation of the surprise category was expected since\nthe prediction accuracy of the model on this particular category was on par with chance guess-\ning. Note also that our data set contains a very large number of headlines per year: a minimum\nof more than 300,000 for the year 2000, and more than 1 million headlines per year since 2009\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 5 / 14\n(see S1 File for detailed breakdown by outlet and year). Thus, allowing yearly central tenden-\ncies to reliably determine the emotional dynamics of headlines. A word cloud of the most pre-\nvailing words in each emotional category of headlines is included as S1 File to provide further\nsupport for the accuracy of the automated annotation method.\nResults\nChronological analysis of sentiment in news articles headlines\nFig 1 shows the average yearly sentiment of news articles headlines across the 47 popular news\noutlets analyzed. A pattern of increasing negative sentiment in headlines over time is apparent.\nA linear regression t-test to determine whether the slope of the regression line differs signifi-\ncantly from zero was conducted: t(18) = -9.63, p<10\n−7\n. The percentage change in the average\nsentiment of headlines from the year 2000 to the year 2019 is -314%. The slope of growing neg-\nativity appears to increase post-2010. A Chow Test [32] for structural break detection in 2010\nis significant (F = 28.83, p<10–5).\nA potential confound in Fig 1 is that more recent years aggregate a larger number of outlets.\nThus, the pattern in Fig 1 could be due to a qualitatively different mix of outlets over time.\nHowever, redoing the analysis in Fig 1 using 12.5 million headlines from the 18 news media\noutlets in the data set with continuous availability of news articles headlines since the year\n2000 also shows a pattern of declining sentiment in headlines; see S1 File for details.\nWe replicate the analysis in Fig 1 using a different Transformer model (DistilBert) fine-\ntuned on the SST-2 sentiment data set. This variation of the analysis produces very similar\nresults to those reported in Fig 1; see S1 File for details.\nSentiment of news articles headlines by ideological leanings of news outlets\nAggregating the sentiment of headlines according to the ideological leanings of news outlets,\nusing human ratings of outlet political leanings from the 2019 AllSides Media Bias Chart v1.1\n[24], shows that the pattern of increasing negativity in news headlines is consistent across left-\nleaning and right-leaning outlets, see Fig 2. Both right-leaning and left-leaning news outlets\ndisplay increasing negative sentiment in their headlines since the year 2000. There is a high\ndegree of correlation in the sentiment of headlines between right-leaning and left-leaning out-\nlets (r = 0.82). On average, right-leaning news outlets have historically tended to use more neg-\native headlines than left-leaning news outlets and continue to do so in 2019. Centrist news\noutlets appear to use less negative headlines than both right and left-leaning news outlets but\nthe small set of outlets (N = 7) classified as centrists by the 2019 AllSides Media Bias Chart v1.1\nwarrants caution when interpreting the external validity of the centrist outlets trendline. Repli-\ncating this analysis using only the 18 media outlets with news articles headlines available since\nthe year 2000 shows similar trends to those in Fig 2, with the caveat that the declining senti-\nment trend for right-leaning outlets is milder (see S1 File).\nChronological analysis of emotion in news articles headlines\nNext, we analyze the emotional charge of headlines using the emotion predictions of the Distil-\nRoBERTa-base Transformer model fine-tuned for emotion labelling. The aggregation of aver-\nage yearly prevalence of emotional labels across the 47 popular news outlets analyzed is shown\nin Fig 3. Linear regression t-tests to determine whether the slope of the regression line differs\nsignificantly from zero were conducted for each emotion (See Fig 3 for each test’s results).\nReported p-values have been Bonferroni-corrected for multiple comparisons.\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 6 / 14\nAn increase of 104% in the prevalence of headlines denoting anger since the year 2000 is\napparent in Fig 3. There are also substantial increases in the prevalence of headlines denoting\nfear (+150%), disgust (29%) and sadness (+54%) in the 2000–2019 studied time range. In con-\ntrast, the prevalence of headlines with neutral emotion has experienced a continuous decrease\n(-30%) since the year 2000. The joy emotional category shows a curvilinear pattern with\nincreasing proportion of headlines denoting joy from 2000 to 2010 and a decreasing trend\nfrom 2010 to 2019. Chow Tests [32] (Bonferroni corrected for multiple comparisons) for\nstructural break detection in 2010 are significant for anger (F = 29.07, p<10\n−4\n), disgust\n(F = 27.97, p<10\n−4\n), joy (F = 23.69, p<10\n−4\n), sadness (F = 6.48, p<0.05) and neutral (F = 7.64,\np<0.05). Notice the different scale of the Y-axes for the different emotion types that might\nexaggerate the apparent temporal dynamics of emotion categories with low prevalence such as\ndisgust. To confirm that the patterns shown in Fig 3 are not the result of a different qualitative\ncomposition of outlets between the year 2000 and the year 2019, we replicate the experiment\nusing only the 18 outlets in the data set with continuous online availability of headlines since\nthe year 2000 (N = 12.5 million). Results show very similar trends to those displayed in Fig 3,\nsee S1 File. Replicating the previous analysis with the 12 news outlets with more than 2,000\nheadlines per year since 2000 (N = 12 million), shows very similar trends. Another replication\nFig 2. Average yearly sentiment of headlines grouped by the ideologica l leanings of news outlets using human ratings of outlets political bias from the\n2019 AllSides Media Bias Chart v1.1 [24]. The figure displays the standard error bars of the average yearly sentiment for outlets within each color-coded\npolitical orientation category. For each ideologic al grouping , statistical tests for the null hypothe sis of zero slope are shown on the bottom left of the plot.\nhttps://do i.org/10.1371/j ournal.pone .0276367.g00 2\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 7 / 14\nFig 3. Average yearly prevalence of news articles headline s denoting different types of emotionalit y in 47 popular news media outlets. The shaded gray\narea indicates the 95% confiden ce interval around the mean. Note the differe nt scale of the Y axes for the different emotio n types. For each emotiona l category ,\nstatistical tests for the null hypothe sis of zero slope are shown on the bottom left of each subplot. Reported p-values have been Bonferron i-correct ed for\nmultiple comparison s. The percentage changes between 2000 and 2019 are shown on the top left of each subplot.\nhttps://do i.org/10.1371/j ournal.pone .0276367.g00 3\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 8 / 14\nwith the six news outlets with more than 10,000 headlines per year since 2000 (N = 8 million),\nshows very similar results to those reported in Fig 3 (see S1 File for details).\nEmotionality of news articles headlines by ideological leanings of news\noutlets\nAggregating the emotionality of headlines according to the ideological leanings of the outlets,\nusing political bias ratings from the 2019 AllSides Media Bias Chart v1.1 [24], shows that the\nincreasing prevalence of headlines denoting anger is apparent in both right-leaning and left-\nleaning news outlets, see Fig 4. Centrist news outlets follow a similar trend over the studied\ntime frame. Anger denoting headlines appear more prevalent in right-leaning outlets than in\nleft-leaning outlets over the entire studied time period. Fear and sadness denoting headlines\nare also increasing across the entire ideological spectrum. The decreasing prevalence of head-\nlines with neutral emotional valence appears to be consistent in left, centrist and right-leaning\noutlets. The degree of correlation between the emotionality of headlines in left-leaning and\nright-leaning news outlets is substantial for most emotion types. Replicating this analysis using\nonly the 18 news outlets with headlines available since the year 2000 shows similar trends; see\nS1 File for details.\nDiscussion\nThe results of this work show an increase of sentiment negativity in headlines across news\nmedia outlets popular in the United States since at least the year 2000. The sentiment of head-\nlines in right-leaning news outlets has been, on average, more negative than the sentiment of\nheadlines in left-leaning news outlets for the entirety of the 2000–2019 studied time interval.\nAlso, since at least the year 2008, there has been a substantial increase in the prevalence of\nheadlines denoting anger across popular news media outlets. Here as well, right-leaning news\nmedia appear, on average, to have used a higher proportion of anger denoting headlines than\nleft-leaning news outlets. The prevalence of headlines denoting fear and sadness has also\nincreased overall during the 2000–2019 interval. Within the same temporal period, the propor-\ntion of headlines with neutral emotional valence has markedly decreased across the entire\nnews media ideological spectrum.\nThe higher prevalence of negativity and anger in right-leaning news media is noteworthy.\nPerhaps this is due to right-leaning news media simply using more negative language than left-\nleaning news media to describe the same phenomena. Alternatively, the higher negativity and\nanger undertones in headlines from right-leaning news media could be driven by differences\nin topic coverage between both types of outlets. Clarifying the underlying reasons for the dif-\nferent sentiment and emotional undertones of headlines between left-leaning and right-lean-\ning news media could be an avenue for relevant future research.\nThe structural break in the sentiment polarity and the emotional payload of headlines\naround 2010 is intriguing, although the short nature of the time series under investigation\n(just 20 years of observations) makes the reliability uncertain. Due to the methodological limi-\ntations of our observational study, we can only speculate about its potential causes.\nIn the year 2009, social media giants Facebook and Twitter added the like and retweet but-\ntons respectively to their platforms [33]. These features allowed those social media companies\nto collect information about how to capture users’ attention and maximize engagement\nthrough algorithmically determined personalized feeds. Information about which news articles\ndiffused more profusely through social media percolated to news outlets by user-tracking sys-\ntems such as browser cookies and social media virality metrics. In the early 2010s, media com-\npanies also began testing news media headlines across dozens of variations to determine the\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 9 / 14\nFig 4. Yearly prevalen ce of headline s denoting different types of emotiona lity in 47 popular news outlets grouped by human ratings of news media\nideologica l leanings from the 2019 AllSides Media Bias Chart v1.1 [24]. Note the different scale of the Y axes for the different emotion types. Only statistical\ntests within each ideological grouping for which the null hypothesis of zero slope was rejected (after Bonferroni correction for multiple comparison s) are shown\non the bottom left of each plot.\nhttps://do i.org/10.1371/j ournal.pone .0276367.g00 4\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 10 / 14\nversion that generated the highest click-through ratio [34]. Thus, a perverse incentive might\nhave emerged in which news outlets, judging by the larger reach/popularity of their articles\nwith negative/emotional headlines, started to drift towards increasing usage of negative senti-\nment/emotions in their headlines.\nA limitation of this work is the frequent semantic overloading of the sentiment/emotion\nannotation task. The negative sentiment category for instance often conflates into the same\numbrella notion of negativity text that describes suffering and/or being at the receiving end of\nmistreatment, as in “the Prime Minister has been a victim of defamation”, with text that\ndenotes negative behavior or character traits, as in “the Prime Minister is selfish”. Thus, it is\nuncertain whether the increasing prevalence of headlines with negative connotations empha-\nsize victimization, negative behavior/judgment or a mixture of the two.\nAn additional limitation of this work is the frequent ambiguity of the sentiment/emotion\nannotation task. The sentiment polarity and particularly the emotional payload of a text\ninstance can be highly subjective and intercoder agreement is generally low, especially for the\nlatter, albeit above chance guessing. For this reason, automated annotations for single\ninstances of text can be noisy and thus unreliable. Yet, as shown in the simulation experiments\n(see S1 File for details), when aggregating the emotional payload over a large number of head-\nlines, the average signal raises above the noise to provide a robust proxy of overall emotion in\nlarge text corpora. Reliable annotations at the individual headline level however would require\nmore overdetermined emotional categories.\nThe imbalanced nature of the emotion labels also represents a challenge for the classifica-\ntion analysis. For that reason, we used performance metrics that are recommended when han-\ndling imbalanced data such as confusion matrices, precision, recall and F-1 scores. Usage of\ndifferent algorithms such as decision trees are often recommended when working with imbal-\nanced data. However, since Transformer models represent the state-of-the-art for NLP text\nclassification, we circumscribed our analysis to their usage. Other techniques for dealing with\nimbalanced data such as oversampling the minority class or under sampling the majority class\ncould have also been used. However, our relatively small number of human annotated head-\nlines (1124 for sentiment and 5353 for emotion), constrained our ability to trim the human-\nannotated data set.\nAnother limitation of this work is the potential biases of the human raters that annotated\nthe sentiment and emotion of news media headlines. It is conceivable that our sample of\nhuman raters, recruited through Mechanical Turk, is not representative of the general US pop-\nulation. For instance, the distribution of socioeconomic status among raters active in Mechani-\ncal Turk might not match the distribution of the entire US population. The impact of such\npotential sample bias on headlines sentiment/emotion estimation is uncertain.\nA final limitation of our work is the small number of outlets falling into the centrist political\norientation category according to the AllSides Media Bias Chart v1.1. Such small sample size\nlimits the sample representativeness and constraints the external validity of the centrist outlets\nresults reported here.\nAn important question raised by this work is whether the sentiment and emotionality\nembedded in news media headlines reflect a wider societal mood or if instead they just reflect\nthe sentiment and emotionality prevalent or pushed by those creating news content. Financial\nincentives to maximize click-through ratios could be at play in increasing the sentiment polar-\nity and emotional charge of headlines over time. Conceivably, the temptation of shaping the\nsentiment and emotional undertones of news headlines to advance political agendas could also\nbe playing a role. Deciphering these unknowns is beyond the scope of this article and could be\na worthy goal for future research.\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 11 / 14\nTo conclude, we hope this work paves the way for further exploration about the potential\nimpact on public consciousness of growing emotionality and sentiment negativity of news\nmedia content and whether such trends are conductive to sustain public well-being. Thus, we\nhope that future research throws light on the potential psychological and social impact of pub-\nlic consumption of news media diets with increasingly negative sentiment and anger/fear/sad-\nness undertones embedded within them.\nSupporting information\nS1 File.\n(DOCX)\nAuthor Contributions\nConceptualization: David Rozado, Jamin Halberstadt.\nData curation: David Rozado, Ruth Hughes.\nFormal analysis: David Rozado.\nInvestigation: David Rozado.\nMethodology: David Rozado, Jamin Halberstadt.\nProject administration: Ruth Hughes.\nSoftware: David Rozado.\nSupervision: Jamin Halberstadt.\nValidation: David Rozado.\nVisualization: David Rozado.\nWriting – original draft: David Rozado, Jamin Halberstadt.\nWriting – review & editing: David Rozado, Jamin Halberstadt.\nReferences\n1. O’Brien H. L., “Explo ring user engagement in online news interactions, ” Procee dings of the American\nSociety for Informatio n Science and Technolo gy, vol. 48, no. 1, pp. 1–10, 2011, https:// doi.org/10.10 02/\nmeet.2011 .14504801 088.\n2. Ecker U. K. H., Lewandow sky S., Chang E. P., and Pillai R., “The effects of subtle misinform ation in\nnews headlines ,” Journal of Experime ntal Psychology: Applied, vol. 20, no. 4, pp. 323–335, 2014,\nhttps://doi.or g/10.103 7/xap000 0028 PMID: 25347407\n3. Bransford J. D. and Johnson M. K., “Contextual prerequis ites for understa nding: Some investigatio ns of\ncomprehe nsion and recall,” Journal of Verbal Learning and Verbal Behavio r, vol. 11, no. 6, pp. 717–\n726, Dec. 1972, https://doi.or g/10.1016/ S0022-537 1(72)8000 6-9\n4. Hasell A., “Shared Emotion: The Social Amplificati on of Partisan News on Twitter,” Digital Journal ism,\nvol. 9, no. 8, pp. 1085–1102, Sep. 2021, https://doi.or g/10.108 0/21670811.2 020.1831937\n5. Berger J. and Milkman K. L., “What Makes Online Content Viral?,” Journal of Marketing Research , vol.\n49, no. 2, pp. 192–205, Apr. 2012, https:// doi.org/10.15 09/jmr.10 .0353\n6. Rathje S., Bavel J. J. V., and van der Linden S., “Out-gro up animosity drives engage ment on social\nmedia,” PNAS, vol. 118, no. 26, Jun. 2021, https://doi.or g/10.107 3/pnas.20 24292118 PMID: 34162706\n7. de Souza M. P., da Silva F. R. M., Freire P. M. S., and Goldschmi dt R. R., “A Linguisti c-Based Method\nthat Comb ines Polarity, Emotion and Gramma tical Character istics to Detect Fake News in Portuguese,”\nin Proceedings of the Brazilian Symposium on Multimedia and the Web, New York, NY, USA, Nov.\n2020, pp. 217–22 4. https://doi.or g/10.114 5/3428658.343 0975\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 12 / 14\n8. Brady W. J., Wills J. A., Jost J. T., Tucker J. A., and Bavel J. J. V., “Emotio n shapes the diffusion of mor-\nalized content in social networks,” PNAS, vol. 114, no. 28, pp. 7313–7318, Jul. 2017, https://doi.or g/10.\n1073/pnas .1618923114 PMID: 28652356\n9. Hansen L. K., Arvidsson A., Nielsen F. A., Colleoni E., and Etter M., “Good Friends, Bad News—Af fect\nand Virality in Twitter,” in Future Informatio n Technolo gy, Berlin, Heidelbe rg, 2011, pp. 34–43. https://\ndoi.org/10.10 07/978-3-6 42-22309- 9_5\n10. Shapiro A. H., Sudhof M., and Wilson D. J., “Measuring news sentiment ,” Journal of Econometr ics,\nNov. 2020, https:// doi.org/10.10 16/j.jecon om.2020.0 7.053\n11. Rozado D., Al-Gharbi M., and Halberstad t J., “Prevalence of Prejudic e-Denoting Words in News Media\nDiscours e: A Chronologic al Analysis, ” Social Science Compu ter Review, p. 089443 93211031452 , Jul.\n2021, https://d oi.org/10.117 7/089443 93211031452\n12. Rozado D. and al-Gharbi M., “Using word embeddi ngs to probe sentim ent associations of politically\nloaded terms in news and opinion articles from news media outlets,” J Comput Soc Sc, 2021, https://\ndoi.org/10.10 07/s42001-021 -00130-y\n13. Rozado D., “Wide range screening of algorithmic bias in word embeddi ng models using large sentiment\nlexicons reveals underrepo rted bias types,” PLOS ONE, vol. 15, no. 4, p. e0231189, Apr. 2020, https://\ndoi.org/10.13 71/journal.p one.0231189 PMID: 32315320\n14. Krippendo rff K., Content Analysis: An Introdu ction to Its Methodol ogy, Third edition. Los Angeles; Lon-\ndon: SAGE Publications , Inc, 2012.\n15. Neuendor f K. A., The Content Analysis Guidebook , 1st edition. Thousand Oaks, Calif: SAGE Publica -\ntions, Inc, 2001.\n16. Rozado D. and Atkins S., “Why Are Nondisc rimination Statemen ts Not Diverse?,” Acad. Quest., vol.\n31, no. 3, pp. 295–303, Sep. 2018, https://doi.or g/10.1007/ s12129-018- 9719-z\n17. Lewis S. C., Zamith R., and Hermida A., “Content Analysis in an Era of Big Data: A Hybrid Approach to\nComputat ional and Manual Methods,” Journal of Broadcasting & Electro nic Media, vol. 57, no. 1, pp.\n34–52, Jan. 2013, https://doi. org/10.1080/0 8838151. 2012.761702\n18. Rozado D., “Prejudice and Victimization Themes in New York Times Discourse: a Chronol ogical Analy-\nsis,” Acad. Quest., vol. 33, no. 1, pp. 89–100, Mar. 2020, https://doi.or g/10.100 7/s12129 -019-0985 7-7\n19. Kozlowsk i A. C., Taddy M., and Evans J. A., “The Geome try of Culture: Analyzing the Meaning s of\nClass through Word Embeddings ,” Am Sociol Rev, vol. 84, no. 5, pp. 905–94 9, Oct. 2019, https://doi.\norg/10.1177/ 00031224198 77135\n20. S. Raza and C. Ding, “News Recommen der System Consideri ng Tempora l Dynamics and News Taxon-\nomy,” in 2019 IEEE International Conferen ce on Big Data (Big Data), Dec. 2019, pp. 920–929. https://\ndoi.org/10.11 09/BigData 47090.2019.90 05459\n21. S. Raza and C. Ding, “Deep Neural Network to Tradeo ff between Accuracy and Diversity in a News\nRecommen der System,” in 2021 IEEE Internat ional Conference on Big Data (Big Data), Dec.\n2021, pp. 5246–5 256. https://doi.or g/10.110 9/BigData 52589.2021.96 71467\n22. Vaswani A. et al., “Attent ion is All you Need,” p. 11.\n23. Devlin J., Chang M.-W., Lee K., and Toutanova K., “BERT: Pre-train ing of Deep Bidirectiona l Trans-\nformers for Language Understand ing,” 2019. https:// doi.org/10.18 653/v1/N 19-1423\n24. AllSides, “AllSides Media Bias Ratings,” AllSides, 2019. https:/ /www.allside s.com/blo g/updated-\nallsides-m edia-bias-cha rt-version-11 (accessed May 10, 2020).\n25. Spinde T., Kreute r C., and Gaissmaie r W., “How Can the Perception of Media Bias in News Article s Be\nObjective ly Measured ?,” p. 10.\n26. Liu Y. et al., “RoBERT a: A Robustly Optimized BERT Pretraining Approac h,” arXiv:1907.11 692 [cs],\nJul. 2019, Accessed: Jul. 28, 2021. [Online]. Available: http://arxiv.or g/abs/190 7.11692\n27. Hartmann J., Heitmann M., Siebert C., and Schamp C., “More than a Feeling: Accuracy and Application\nof Sentimen t Analysis,” International Journal of Research in Marketing , Jun. 2022, https://doi.or g/10.\n1016/j.ijresm ar.2022.05. 005\n28. Heitmann M., Siebert C., Hartmann J., and Schamp C., “More than a Feeling: Benchmar ks for Senti-\nment Analysis Accuracy,” Social Science Research Network, Rochester, NY, SSRN Scholarly Paper\nID 3489963, Jul. 2020. https://doi.or g/10.213 9/ssrn.3489 963\n29. Wolf T. et al., “Huggin gFace’s Transforme rs: State-of-the- art Natural Language Processin g,”\narXiv:1910. 03771 [cs], Feb. 2020, Accessed: May 08, 2020. [Online]. Available: http://arxiv .org/abs/\n1910.03771\n30. Ekman P., “An argument for basic emotions,” Cognition and Emotion, vol. 6, no. 3–4, pp. 169–200,\nMay 1992, https://doi.o rg/10.1080/02 6999392 08411068\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 13 / 14\n31. Hartmann J., “emotion-eng lish-disti lroberta-b ase.” https://huggi ngface.co /j-hartmann/em otion-english -\ndistilroberta- base (accessed Jul. 28, 2021).\n32. Chow G. C., “Tests of Equality Between Sets of Coefficient s in Two Linear Regressions,” Econometr ica,\nvol. 28, no. 3, pp. 591–605, 1960, https://doi.or g/10.230 7/1910133\n33. Haidt J. and Twenge J. M., “Opinion | This Is Our Chance to Pull Teenagers Out of the Smartphone\nTrap,” The New York Times, Jul. 31, 2021. Accessed: Aug. 18, 2021. [Online]. Availab le: https://www .\nnytimes. com/2021/07 /31/opinion/ smartphon e-iphone-so cial-medi a-isolation.htm l\n34. Rose-Stock well Tobias J. H., “The Dark Psychology of Social Networks,” The Atlantic, Nov. 12, 2019.\nhttps://www .theatlan tic.com/mag azine/arch ive/2019/1 2/social-me dia-democ racy/60076 3/ (accessed\nAug.20, 2021).\nPLOS ONE\nLongitu dinal analysis of sentiment and emotion in news media headlines\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02763 67 October 18, 2022 14 / 14",
  "topic": "Sadness",
  "concepts": [
    {
      "name": "Sadness",
      "score": 0.9033379554748535
    },
    {
      "name": "Disgust",
      "score": 0.8865756988525391
    },
    {
      "name": "Anger",
      "score": 0.8105447888374329
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6871126890182495
    },
    {
      "name": "Surprise",
      "score": 0.5709885954856873
    },
    {
      "name": "Psychology",
      "score": 0.455453097820282
    },
    {
      "name": "Social psychology",
      "score": 0.2863253355026245
    },
    {
      "name": "Computer science",
      "score": 0.1985929012298584
    },
    {
      "name": "Artificial intelligence",
      "score": 0.13281410932540894
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387152882",
      "name": "Te Pūkenga",
      "country": null
    },
    {
      "id": "https://openalex.org/I80281795",
      "name": "University of Otago",
      "country": "NZ"
    }
  ],
  "cited_by": 76
}