{
  "title": "Probing for Referential Information in Language Models",
  "url": "https://openalex.org/W3034685497",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5065356514",
      "name": "Ionut-Teodor Sorodoc",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250848545",
      "name": "Kristina Gulordava",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A142903237",
      "name": "Gemma Boleda",
      "affiliations": [
        "Institució Catalana de Recerca i Estudis Avançats",
        "Institut Català de Ciències del Clima"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963925965",
    "https://openalex.org/W2962961857",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2088911157",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2984147501",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W4290241052",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2101268022",
    "https://openalex.org/W2964222268",
    "https://openalex.org/W2952208026",
    "https://openalex.org/W1826363161",
    "https://openalex.org/W2986889180",
    "https://openalex.org/W2563734883",
    "https://openalex.org/W2922523190",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W1987971958",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2155069789",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W2951976932",
    "https://openalex.org/W4320013820",
    "https://openalex.org/W3003276437",
    "https://openalex.org/W4298882835",
    "https://openalex.org/W2997868155",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W4289552613",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W3110909889",
    "https://openalex.org/W2805589998",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2576837726",
    "https://openalex.org/W2999729612",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W2971016963",
    "https://openalex.org/W2891399254",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W2963118869",
    "https://openalex.org/W2945614092",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963290255",
    "https://openalex.org/W2963676655"
  ],
  "abstract": "Comunicació presentada al 58th Annual Meeting of the Association for Computational Linguistics celebrat del 5 al 10 de juliol de 2020 de manera virtual.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4177–4189\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n4177\nProbing for Referential Information in Language Models\nIonut-Teodor Sorodoc∗ Kristina Gulordava Gemma Boleda ∗†\n∗Universitat Pompeu Fabra\n†ICREA\nBarcelona, Spain\n{firstname.lastname}@upf.edu\nAbstract\nLanguage models keep track of complex lin-\nguistic information about the preceding con-\ntext – including, e.g., syntactic relations in a\nsentence. We investigate whether they also\ncapture information beneﬁcial for resolving\npronominal anaphora in English. We analyze\ntwo state of the art models with LSTM and\nTransformer architectures, respectively, using\nprobe tasks on a coreference annotated corpus.\nOur hypothesis is that language models will\ncapture grammatical properties of anaphora\n(such as agreement between a pronoun and\nits antecedent), but not semantico-referential\ninformation (the fact that pronoun and an-\ntecedent refer to the same entity). Instead, we\nﬁnd evidence that models capture referential\naspects to some extent –though they are still\nmuch better at grammar. The Transformer out-\nperforms the LSTM in all analyses, and ex-\nhibits in particular better semantico-referential\nabilities.\n1 Introduction\nNeural network-based language models (LMs)\nhave been shown to learn relevant properties of\nlanguage without being explicitly trained for them.\nIn particular, recent work suggests that they are\nable to capture syntactic relations to a large ex-\ntent (Gulordava et al., 2018; Kuncoro et al., 2018;\nWilcox et al., 2018).\nIn this paper, we extend this line of research\nto analyze whether they are able to capture refer-\nential aspects of language, focusing on anaphoric\nrelations (pronoun-antecedent relations, as in she-\nYeping Wangin Figure 1).\nPrevious work, such as Ji et al. (2017), Yang\net al. (2017) and Cheng and Erk (2019), showed\nthat augmenting language models with a compo-\nnent that uses an objective based on entity or coref-\nerence information improves their performance at\n. . . he1 was elected to be president of the Peo-\nple’s Republic of China, and chairman of the2\nCentral2 Military2 Commission2. Yeping 3\nWang3 was born in Shanghai in 1926. She 3\nstudied in Shanghai Foreign Language Col-\nlege, and started working in 1949. For a long\ntime, she3. . .\nFigure 1: Example from OntoNotes with a window\nof 60 tokens (as used in our ﬁrst probe task). Both\noccurrences of she refer to the same entity as Yeping\nWang. Note that not all entity mentions are annotated\nin OntoNotes –only those that enter into coreference\nrelationships in the document.\nlanguage modeling. Intuitively, in the example in\nFigure 1, understanding that the ﬁrst she refers to\nYeping Wang makes words related to studying or\nworking more likely to follow than other kinds of\nwords. That is, referential information helps lan-\nguage models do their task.\nThe cited work includes explicit coreference\nguidance; however, since referential information\nis useful for language modeling, we expect lan-\nguage models to learn referential information even\nwithout explicit supervision. Here we analyze to\nwhat extent this is the case.\nWe carry out our analysis using probe tasks,\nor tasks that check whether certain information\nis encoded in a model (Adi et al., 2016; Linzen\net al., 2016; Conneau et al., 2018; Giulianelli et al.,\n2018). The reasoning is as follows: Even if a lin-\nguistic property is encoded in the network, it is not\nnecessarily directly accessible through the model\noutput; therefore, we train a probe model to pre-\ndict a feature of interest, in this case anaphoric\ncoreference, given the model’s hidden representa-\ntions as input.\nWe focus on the two main linguistic levels that\nare relevant for coreference: morphosyntax, with\n4178\ngrammatical constraints such as the fact that pro-\nnouns agree in number and gender with their an-\ntecedents, and semantics – in particular reference,\nsuch as the fact that a pronoun refers to the same\nentity as its antecedent.\nOur hypothesis is that language models will\ncapture grammatical properties, but not seman-\ntic information. This hypothesis is based on the\nobservation that morphosyntax is a formal prop-\nerty of language that is easier to induce from co-\noccurrence patterns. The fact that language refers\nto entities is not obvious from language alone\n(Harnad, 1990), and LMs use only textual input.\nInstead, what we ﬁnd is that, while it is true\nthat language models are much better at gram-\nmar, they do show evidence of learning semantico-\nreferential information to some extent. Our expla-\nnation for this unexpected, partially positive result\nis that, because the same entity underlies all its\nmentions, the contexts in which the mentions ap-\npear are coherent and distinct from those of men-\ntions of other entities. For instance, in Figure 1,\nthe second she mention gives additional informa-\ntion about Yeping Wang that is consistent with the\ninformation given in the previous sentence.\nThis paper has two main contributions. The ﬁrst\nis an analysis methodology to probe for referen-\ntial information encoded in language models, on\ntwo linguistic levels (morphosyntax, semantics)\nand two kinds of context: local (around one para-\ngraph of context), and global (document context).\nThis methodology can be applied to any architec-\nture. The second contribution is a deeper under-\nstanding of the referential capabilities of current\nlanguage models, and of the differences between\nTransformers and LSTMs. The Transformer out-\nperforms the LSTM in all the analyses. For mor-\nphosyntax, the Transformer and the LSTM have\nthe same behavior with a performance difference;\ninstead, they show different behavior with regard\nto semantico-referential information.\n2 Related work\nCoreference and anaphora resolution (Mitkov,\n2002; Poesio et al., 2016) are among the old-\nest topics in computational linguistics and have\ncontinued to receive a lot of attention in the\nlast decade, as manifested by several shared\ntasks (Pradhan et al., 2011, 2012; Poesio et al.,\n2018). In our analysis we use the OntoNotes\ndataset (Hovy et al., 2006; Pradhan et al., 2012),\ndeveloped within the coreference resolution com-\nmunity. Our probe tasks are related to corefer-\nence resolution; however, our goal is not to train\na coreference system but to analyse whether lan-\nguage models extract features relevant for refer-\nence without explicit supervision.\nA recent line of work has focused on demon-\nstrating that neural networks trained on language\nmodeling, without any linguistic annotation, learn\nsyntactic properties and relations such as agree-\nment or ﬁller-gap dependencies (Linzen et al.,\n2016; Gulordava et al., 2018; Kuncoro et al., 2018;\nWilcox et al., 2018; Futrell et al., 2018). This\nis typically done by analysing the predictions of\nLMs on controlled sets of data. Part of this re-\nsearch uses probe models (also known as diagnos-\ntic models) to analyse the information contained\nin their hidden representations (Adi et al., 2016;\nConneau et al., 2018; Hupkes et al., 2018; Lakretz\net al., 2019; Giulianelli et al., 2018), as we do here\n—applying it to referential information.\nThere is less work on referential information\nthan on syntactic properties such as subject-verb\nagreement. As for anaphoric reference, Peters\net al. (2018) include a limited test using 904 sen-\ntences from OntoNotes. Their results suggest that\nLMs are able to do unsupervised coreference res-\nolution to a certain extent; our ﬁrst probe task can\nbe seen as an extended version of their task obtain-\ning more speciﬁc insights. Jumelet et al. (2019)\nanalyze the kind of information that LSTM-based\nLMs use to make decisions in within-sentence\nanaphora. They ﬁnd a strong male bias encoded\nin the network’s weights, while the information in\nthe input word embeddings only plays a role in the\ncase of feminine pronouns. We analyze anaphora\nin longer spans (60 tokens / whole document) and\ninclude also a Transformer.\nThe above work suggests that LMs capture mor-\nphosyntactic facts about anaphora to a large ex-\ntent. There is much less evidence that LMs can\ncapture a notion of entity, as that which nominal\nelements refer to, and that they are able to track\nentities across a discourse. Parvez et al. (2018)\nshow that LSTM-based models have poor results\non texts with a high presence of entities; Paperno\n(2014) that they cannot predict the last word of text\nfragments that require a context of a whole pas-\nsage (as opposed to the last sentence only), with\ndata that mostly contain nominal elements. Sev-\neral models (Henaff et al., 2019; Yang et al., 2017;\n4179\nJi et al., 2017) were developed as an augmentation\nof RNN LMs to deal better with entities, with the\nimplicit assumption that standard models do that\npoorly. Aina et al. (2019) achieved good results\non an entity-linking task, but showed that the net-\nwork was not acquiring entity representations.\nAs for Transformer-based architectures, recent\nresearch suggests that they give same or better\ncontextualized representations in comparison with\nLSTM language models, and that they better en-\ncapsulate syntactic information (Goldberg, 2019;\nWolf, 2019). On the other hand, van Schijndel\net al. (2019) show that big Transformer model rep-\nresentations perform on par or even poorer than\nsmaller LSTMs on tasks such as number agree-\nment or coordination, and that, like LSTMs, they\nhave the problem that agreement accuracy de-\ncreases as the subject becomes more distant from\nits verb. Most recent work on analysis of linguis-\ntic phenomena in NNs focuses on BERT (Ten-\nney et al., 2019; Clark et al., 2019; Reif et al.,\n2019; Broscheit, 2019). In this paper we chose\nto use TransformerXL (Dai et al., 2019) as our\nTransformer model, and not BERT, for compara-\nbility: We wanted to compare the two most stan-\ndard architectures for LMs on as equal ground\nas possible, and the two chosen models, Trans-\nformerXL and AWD-LSTM (Merity et al., 2017),\nshare the same training objective and are trained\non the same data, with comparable vocabularies.\n3 Morphosyntactic factors\nTo shed light into which morphosyntactic infor-\nmation LMs encode that is useful for coreference,\nwe train a simple anaphora resolution probe model\nusing the hidden layers of LMs as input. By the\nlogic of probe tasks, if the probe model is success-\nful then that means that the relevant information\nis encoded in the hidden states, and error analysis\ncan provide insight into which kinds of informa-\ntion are available.\n3.1 Experimental Setup\nData We train our probe models on data from\nOntoNotes 5.0 (Weischedel et al., 2013). We use\nthe annotated coreference chains, as well as the\nprovided part-of-speech tags (the latter only for\nanalysis purposes).\nWe take all pronouns that have at least one an-\ntecedent in a 60-token context window; the task of\nTokens Datapoints\nTrain 191,830 4,949\nDev 275,201 4,556\nTest 2,026,565 45,665\nTable 1: Dataset statistics for ﬁrst probe task. We re-\nverse the original train and test partitions (see text).\nthe probe model is to identify their antecedent. 1\nAn example datapoint is provided in Figure 1\nabove (note that a window of 60 tokens allows\nus to check anaphora beyond the sentence). For\nsimplicity, antecedents are tokens, but typically\nthere is more than one possible token antecedent\nfor a given pronoun: A mention can span several\ntokens (Yeping Wang), and the window can con-\ntain several mentions from the same coreference\nchain (Yeping Wangand the ﬁrst She in Figure 1);\nwe consider any of the tokens a correct answer.\nNote that we are not training the model to explic-\nitly identify mentions, their spans or the complete\ncoreference chains, but to identify the tokens that\nare antecedents of the target pronoun.\nTo obtain enough data for analysis, especially\nfor low-frequency phenomena, we follow Linzen\net al. (2016) in reversing the original partitions of\nthe corpus, using the original test set for training\nand the original training set for testing. 2 In addi-\ntion, we focus on the OntoNotes documents that\nbelong to narrative text sections because the dia-\nlogue data does not come with turn segmentation.3\nResulting data statistics for our task are provided\nin Table 1.\nLanguage models The base language models\nwe use are AWD-LSTM (Merity et al., 2017) and\nTransformerXL (Dai et al., 2019), two state-of-\nthe art models with the most standard architec-\n1We also experimented with windows 20 and 200, obtain-\ning a similar picture.\n2Using little training data has also been shown to lessen\nthe possibility of confounds in the probe model results; in\nparticular, it makes it more difﬁcult for the probe model to\nexploit regularities in the training data rather than capturing\nthe analyzed model’s ability to capture a phenomenon (He-\nwitt and Liang, 2019). See V oita and Titov (2020) for a theo-\nretical justiﬁcation from a information-theoretic perspective.\nResults on the original split conﬁrm that the conclusions\nof the paper are robust: we see an increase in performance\nof around 3% overall, as could be expected because we use\nmore data, but the same behavior patterns (on the data that\ncan be compared).\n3We keep newswire (NW), broadcast news (BN), mag-\nazine (MZ), web data (WB), and pivot text (PT), removing\nbroadcast conversation (BC), telephone conversation (TC).\n4180\ntures for language modeling as of 2020 (LSTM,\nTransformer). We chose these models for compar-\nison because they are trained on the same dataset\n(Wiki103; Merity et al., 2016), they have a com-\nparable vocabulary, and they are both very strong\nlanguage models, with perplexities of 24 for\nTransformerXL and 33 for AWD-LSTM. Trans-\nformerXL is a bit larger than AWD-LSTM, though\n(151 million parameters compared to 126), which\nshould be kept in mind when assessing results.4\nProbe model For each word xi in the window\nof size m preceding the target pronoun xt, we ob-\ntain its contextualized representation hi from the\nlast hidden layer of the language model (Eq. 1).\nThe probe model takes this representation as in-\nput and is trained to map it onto a vector oi us-\ning a non-linear transformation (Eq. 2). The tar-\nget pronoun representation is transformed in the\nsame way. The dot products between these trans-\nformed representations of target and context word\nvectors give the attention weightsrefi (Eq. 3) rep-\nresenting the similarity between two representa-\ntions. The weights are transformed into probabil-\nities using the softmax function (Eq. 4). Like this\nwe obtain a probability distributionpi over context\ntokens.\nDuring training, the probe model’s objective is\nto assign higher probabilities (and thus attention\nweights) to correct antecedents, and lower prob-\nabilities to incorrect ones, through the use of the\nKullback-Leibler divergence loss (Eq. 5). We use\nthe KL loss because we frame the task in terms of\na probability distribution over mentions in the con-\ntext. For the reasons discussed above, there can be\nk > 1 correct predictions out of m tokens in the\nwindow. We assume that gold probability distribu-\ntion is uniform over k correct tokens, that is, each\nof these tokens has a probability p∗\ni = 1\nk and all\nother tokens have a probability of 0.5\n4We also trained an in-house LSTM on data that are more\nsimilar to those of OntoNotes and a smaller vocabulary. The\nresults for this model (not reported) follow the same pat-\nterns as those found for the AWD-LSTM and TransformerXL\nmodels, although the performance on this probe task is much\nhigher than that of AWD-LSTM.\n5Note however that minimizing KL divergence and\nminimizing cross-entropy gives the same results, because\nKLdiv(p||q) = CrossEntropy (p, q) −entropy(p), and\nentropy(p) is constant. Technically, in PyTorch the cross-\nentropy loss is only implemented for classiﬁcation task tar-\ngets, while the more general KL loss is available for predict-\ning probability distributions.\nModel Accuracy\nclosest gold entity 56.1\nclosest same-form token 61.3\nunsup. sup.\nLSTM 41.7 64.8\nTransformer 48.5 75.9\nTable 2: Probe model results on anaphora resolution.\nhi = LSTM (xi) (1)\noi = ReLU(W ∗hi + b) (2)\nrefi = oi ⊙ot, ∀i ∈[t −m, t−1] (3)\npi = softmax(refi), ∀i ∈[t −m, t−1] (4)\nL = KL(pi, p∗\ni ) (5)\nAs mentioned above, we ﬁx m = 60. We train\nthe probe model for 50 epochs with a learning rate\nof 1e-5 and ADAM as optimizer. The transformed\nvectors oi have a dimensionality of 650 in the case\nof both models in comparison withhi which is 400\nfor the AWD-LSTM and 1024 for TransformerXL.\nBaselines We report two rule-based baselines\nthat give relatively good performance in anaphora\nresolution: Referring to the previous entity (given\nby the oracle gold annotation; in Figure 1, she\nwould refer to the previousShe), and always point-\ning to the token in the window that has the same\nform as the target pronoun (that is, in Figure 1,she\n→She —we ignore capitalization). In addition,\nto compare the result of the probe model with the\ninput representations, we also report an unsuper-\nvised baseline: Referring to the token in the win-\ndow that has the highest similarity cos(hi, ht) to\nthe target pronoun, i.e., relying on the similarity\nbetween the non-transformed hidden representa-\ntions.\n3.2 Results\nTable 2 summarizes the results of the pronominal\nanaphora probe task. The probe model trained on\ntop of the LSTM improves a bit over the strongest\nbaseline, and that of the Transformer does so sub-\nstantially (75.9 vs. 61.3; the LSTM obtains 64.8).\nThis performance suggests that the LMs use more\ninformation than simple heuristics like referring to\na token with the same form.\nThe unsupervised similarity baseline performs\nworse than the rule-based baselines. This is to\n4181\nbe expected: The “raw” similarity between hidden\nstates is based on many more aspects than those\nrelated to reference, given that hidden states are re-\nsponsible for capturing all the contextual features\nthat are relevant for word prediction. This is why\na probe model is needed to distill the reference-\nrelated information from the hidden layers.\nA single non-linear layer trained on only 5K\ndatapoints improves performance by 23-28 abso-\nlute accuracy points (supervised vs. unsupervised\nresults), which suggests that the referential infor-\nmation in the hidden layers is easy to extract.\nBehaviorally, the unsupervised hidden layers are\nquite similar to the baselines. First, they are biased\ntowards tokens of the same form: in 27.1% of the\ncases, the LSTM layer of the pronoun presents the\nhighest similarity to a token with the same form;\n29.1% in the case of the Transformer. Second,\nthey prefer close antecedents, although the LSTM\npresents this recency bias to a much higher de-\ngree: in 27.8% of the cases, the LSTM layer of the\npronoun has the highest similarity to the previous\ntoken (16.4% in the Transformer). The attention\nmechanism of the Transformer gives access to a\nbroader context and allows it to overcome the re-\ncency bias to some degree.\nThe great difference in performance between\nAWD-LSTM and TransformerXL could suggest\nthat the latter is using different strategies com-\npared to the former. Instead, except for the recency\nbias, what we ﬁnd are exactly the same patterns\nin behavior, with a systematic 10% accuracy gap.\nFor this reason, although we provide results for\nboth models everywhere to show that this obser-\nvation indeed holds, in this section we will mostly\nfocus on the Transformer when commenting re-\nsults.\n3.3 Analysis: Morphosyntactic Factors\nThe models clearly learn grammatical constraints\nrelated to anaphora that are well-studied in the lit-\nerature and are relied upon by traditional anaphora\nresolution models (Sukthanker et al., 2018). First,\nas shown in Table 3, the Transformer identi-\nﬁes mentions (elements inside some coreference\nchain) in 92.6% of the cases. Moreover, it cor-\nrectly learns that pronouns typically refer to nom-\ninal elements (almost 95% identiﬁed antecedents\nare pronouns, proper nouns, and elements within a\nnoun phrase headed by a common noun). Note\nthat pronouns can also have non-nominal an-\nLSTM Transformer\nin chain 90.2% 92.6%\nPOS Perc. Acc Perc. Acc\nNoun phrase 15.5 50.9 17.0 62.3\nProper noun 20.2 64.3 20.0 74.9\nPronoun 59.0 71.5 59.0 82.6\nOther 5.3 67.3 3.0 81.6\nTable 3: Statistics on types of mentions that the probe\nmodels refer to, for predictions that are in a coreference\nchain. ‘Noun phrase’ stands for elements that are typi-\ncally within a noun phrase (note that our system points\nto individual tokens): Determiners, nouns, and adjec-\ntives.\ntecedents, although these are the minority of the\nannotations in OntoNotes (cf. example 4 in Fig-\nure 3, where it refers to an event). Even in the\ncases in which the Transformer points to elements\noutside of a chain (7.4%), it points to nominal\nelements 87% of the time (not shown in the ta-\nble). The model is most accurate when referring\nto pronouns (82.6% accuracy), while noun phrases\nare the hardest category (62.3%). This is consis-\ntent with the strategies that the model learns, since\nit largely relies on pronominal agreement, as de-\nscribed below.\nSecond, not only do the models mostly point to\nnominal elements, but they also identify the mor-\nphosyntactic properties of pronouns and learn that\nthey should agree with their antecedents in gen-\nder and number. Figure 2 shows the distribution\nof pronoun antecedents that the Transformer pre-\ndicts, for the six most frequent target pronouns\n(see the Supplementary material for the corre-\nsponding LSTM ﬁgure). Its preferred type of an-\ntecedent are pronouns of the same form, but it\nis also able to point to other pronouns agreeing\nin number and gender. For instance, pronoun he\npoints to 3rd person, masculine, singular pronouns\n(mostly he, but also his, him) —a pattern consis-\ntent across all pronouns.\nFigure 2 is restricted to pronouns; Table 4 shows\nthat the model also largely follows number agree-\nment when predicting antecedents within noun\nphrases (the table collapses common noun and\nproper noun antecedents). Given a singular pro-\nnoun, the model chooses a singular antecedent\n98% of the time; given a plural pronoun, it identi-\nﬁes a plural antecedent in 73% of the cases.\nNote that in cases of plural pronouns such as\n4182\nFigure 2: Pronominal agreement with Transformer\nprobe model: Proportion of cases in which elements\nin the rows corefer with elements in the columns.\nLSTM Transformer\nPron-ant. Perc. Acc Perc. Acc\nsg-sg 97.7 66.3 98.7 76.0\nsg-pl 2.3 20.5 1.3 36.7\npl-sg 35.5 40.8 27.5 53.1\npl-pl 64.5 67.7 72.5 72.3\nTable 4: The types of noun phrase antecedents the mod-\nels choose, by number agreement (e.g., ‘sg-pl’ means\n‘anaphoric pronoun is singular, antecedent plural’).\nthey it is common that the referent be a singular\nnoun (e.g., the audience in example 3, Figure 3),\nreﬂected by the reasonable accuracy of the Trans-\nformer in pl-sg cases (53.1%).\n4 Semantic (referential) factors\nThe language model clearly captures morphosyn-\ntactic (grammatical) properties that constrain\nanaphora resolution; in this section, we show that\nit struggles more with is the semantic (referential)\naspect, but it still captures it to some extent.\n4.1 Sensitivity to distractors\nIf the model were able to model entities, it should\nbe robust to distractors, that is, mentions in the\ncontext that are not antecedents –in Figure 1, he\nand the Central Military Commission . Figure 4\nshows that the accuracy for the Transformer de-\ncreases as does the proportion of gold mentions.\nWe compute this proportion as the number of gold\nmentions in the 60-token window divided by the\ntotal number of mentions in the same window.\nWhen there are no distractors (gold proportion =\n1), accuracy is very high, which is to be expected\ngiven that the model learnt to identify mentions\nin the ﬁrst place (cf. previous section). The more\ndistractors (i.e., the lower the proportion of gold\nmentions), the lower the accuracy; however, accu-\nracy decreases rather gracefully. Even when there\nare only 10% gold mentions in the window, ac-\ncuracy for most pronoun types is still around 60-\n80%. The exception is it, which is the most difﬁ-\ncult pronoun for the model, presumably because it\ncan refer to many kinds of antecedents.6\nFigure 4 thus paints a nuanced picture: distrac-\ntors confuse the model, but they do not fool it com-\npletely. Given the results in the previous section,\nwe expect that distractors sharing morphosyntactic\nfeatures will be particularly challenging. Table 5\nconﬁrms this, zooming in into pronominal distrac-\ntors. We consider a datapoint having a pronominal\ndistractor if one of the antecedents is a pronoun\npointing to another entity.\nWhen there are no pronominal distractors\n(25.9% of the test set), the accuracy of the Trans-\nformer is 81.8%; with at least one distractor, it\ngoes down to 73.8% —clearly worse but not dra-\nmatically so. However, in cases where anaphoric\npronoun and antecedent have the same gender,\nnumber, or are the same pronoun, we get much\nlower accuracies (48.6, 65.3, and 49.1, respec-\ntively). This suggests that that the model overly re-\nlies on morphosyntactic features and recency (see\nprevious section).7\nHowever, accuracy in these cases goes down\nbut is still decent, compared to a reasonable base-\nline (last column in the table). For each target\nanaphoric pronoun, we calculate baseline accu-\nracy as the percentage of gold pronouns in the win-\ndow (pronouns that are in the same chain as the\ntarget), that is, number of gold pronouns divided\n6While most personal pronouns refer to people, which are\nrelatively homogeneous kinds of referents, it refers to very\nvaried kinds of referents. Qualitative analysis suggests that\nthe model is quite successful when it refers to concrete en-\ntities (province, peanut), but much less when it refers to ab-\nstract objects like propositions or events, as in example 4 of\nFigure 3 (where it refers to the event of trying to improperly\ninﬂuence a witness). A quantitative check conﬁrms this hy-\npothesis: Cases in which the model fails have around 18%\nof verbal references, compared to less than 2% for cases in\nwhich the model is right.\n7Among the hardest cases are those where two corefer-\nence chains in the window have the same pronoun (e.g. he)\nor gender (e.g. he-his). Most of these cases appear when the\ntext includes reported speech (see Figure 3, example 1). Oth-\nerwise, there are few cases of such local ambiguity, which is\npresumably avoided by language speakers. However, quali-\ntative analysis suggests that the presence of distractors is also\nproblematic in the case of nouns, as illustrated in example 2\nof Figure 3, where the model is presumably confused by a\nnoun of the same gender and number as the pronoun ( priest\nvs. Peter-him).\n4183\n1. Why had Mr. Korotich been called? “I told my driver,”he said, “that he\n2. While Peter was still in the yard, a servant girl of the high priest came there. She saw him\nwarming himself by the ﬁre. She looked closely at him\n3. The performance by more than 40 members of the Rome Philharmonic Orchestra intoxicated\nthe audience and the musical fountain, hi-ﬁ sound effect, fountain screen and stereographic\nprojection brough them\n4. Mr. Gonzalez expressed concern over a report that the two had been summoned to Washington\nby Mr. Wall last week to discuss their testimony in advance. “I think he istrying to improperly\ninﬂuence a witness, and by God I ’m not going to tolerate it\nFigure 3: Difﬁcult cases of anaphora. The target pronoun and its antecedent are in bold; the prediction of the\nmodel is in italic.\nFigure 4: Transformer probe model: Accuracy as a\nfunction of the proportion of mentions that are an-\ntecedents (vs. distractors) in the window.\nby the total number of pronouns in the window.\nThen we calculate the average of this accuracy\nover the respective subset (no distractors / distrac-\ntors / same gender, etc.). The baseline when there\nare no distractors is by deﬁnition 100%; when\nthere are distractors, it ranges between 15.7 and\n32%. All model accuracies are well above this\nbaseline.\nThe results thus suggest that the models are\nable to distinguish mentions of different entities\nto some extent, although they are far worse at this\nthan at capturing morphosyntactic features. In the\nfollowing subsection, we provide further support\nfor this interpretation.\n4.2 Distinguishing entities\nOur last piece of analysis looks at whole docu-\nments. We aim at testing whether the hidden rep-\nresentations of the language models contain infor-\nmation that can help distinguish mentions of the\nsame entity from mentions of some other entity,\nL T Base\nType Perc. Acc. Acc. Acc.\nNo distractor 25.9 74.9 81.8 100\nDistractor(s) 74.1 61.3 73.8 32.0\n= gender∗ 4.8 40.9 48.6 15.7\n= number 37.2 55.7 65.3 26.6\n= pron. 10.3 39.7 49.1 20.3\nTable 5: Percentage of datapoints with/without\npronominal distractors and accuracy of the models\n(LSTM - L, Transformer - T) and baseline (last col-\numn). ∗Excludes cases with no marked gender (like I,\nyou).\neven if they are of the same form; for instance, a\npronoun she referring to two different women. We\nuse coreference chains to identify the tokens refer-\nring to the same entity, and train a probe model\nto determine when two pronouns are referring to\nthe same entity, that is, whether they are part of\nthe same coreference chain in a document. In the\nprevious probe task, where the model was trained\nto ﬁnd a correct local antecedent, the model could\nuse cues such as linear distance and syntactic rela-\ntions; here it should rely on more persistent entity-\nrelated features in the hidden representations.\nExperimental Setup. We focus on pronouns be-\ncause they cannot be disambiguated on the basis of\nlexical features. We use the same train/test parti-\ntion as in the ﬁrst probe task. For each datapoint,\nwe have two pronouns: x and y, which can either\ncome from the same chain, or not. Again, we take\neach pronoun to be represented by the last hidden\nlayer representation of the language model (Eq.\n(1)): hx and hy. We call this representation un-\n4184\nsupervised, and will compare it to the supervised\none, obtained as follows.\nSimilarly to the previous probe task, the embed-\ndings are transformed through a learnt linear trans-\nformation to a 400-dimensional vector to extract\nfeatures relevant for the entity identiﬁcation task\n(Eqs. (6) and (7)). We take the cosine between the\ntransformed representations as the similarity be-\ntween the two pronouns.\nWe take as positive datapoints contain two pro-\nnouns belonging to the same chain, as nega-\ntive datapoints two pronouns from two different\nchains. During training, for each document, we\nextract all positive pairs and then randomly select\nthe same number of negative pairs. The model\noptimises max-margin loss on these datapoints\n(Eq. (8), where x and y belong to the same chain\nand x′and y′belong to two different chains).\nox = W ∗hx + b (6)\noy = W ∗hy + b (7)\nL = 1−cos(ox, oy) +cos(ox′ , oy′ ) (8)\nResults Figure 5 plots the similarities between\npositive and negative pairs (solid and dashed lines,\nrespectively) for the two analyzed language mod-\nels, compared to linear distance in the text. The\nleft graph corresponds to unsupervised similari-\nties, the right graph to supervised similarities. To\ncontrol for token form effect, we only include data\nwith the same pronoun pairs in this graph. Three\nresults stand out. First, despite training with a\nglobal objective, with no linear information, sim-\nilarities are negatively correlated with linear dis-\ntance in text. This is consistent with the tendency\nof the unsupervised cosine baseline of pointing to\nthe closest token (see Section 3).\nThe second result is that, crucially, after con-\ntrolling both for distance and for pronoun form,\nsimilarities are systematically higher for corefer-\nring pronoun pairs than for non-coreferring ones.\nThus, some properties make their way into the\nhidden representations (and the probe model) that\nmake coreferring mentions distinct from non-\ncorreferring mentions —modulo distance: If we\nattempt to globally distinguish chains, we instead\nobtain null results (see Supplementary Materials).\nThis is because, with linear distance, the simi-\nlarity in the entity-centered representation space\nshrinks very fast; same-chain mentions that are\nfurther away have lower average similarities than\ndifferent-chain mentions that are nearby.\nFinally, the third main result is that the super-\nvised model is able to extract discriminating in-\nformation from the hidden layers to a much larger\nextent in the Transformer than in the LSTM (cf.\ndistance between blue and red lines, respectively).\nWe interpret this to mean that such information is\nencoded to a larger extent in the Transformer. Also\nnote that the supervised LSTM model is more\nsensitive to linear distance than any of the other\nrepresentations (cf. the steeper curves between 0-\n100 token distances). As we signaled in the pre-\nvious section, LSTM is more prone to recency\nbiases, and it looks like global representations\ncontain less entity-related information than in the\ncase of the Transformer, such that the supervised\nmodel defaults to recency. We conclude from\nthis that the Transformer accounts for semantico-\nreferential aspects better than the LSTM.\nOverall, the results suggest that token form and\nproximity in text remain the main properties en-\ncoded in the hidden states of entity mentions, but\nother properties that discriminate between corefer-\nring and non-corefering mentions are present to\nsome extent, allowing for partial discrimination.\n5 Conclusion\nPrevious work has provided robust evidence that\nlanguage models capture grammatical information\nwithout being explicitly trained to do so (Linzen\net al., 2016; Gulordava et al., 2018). In this\npaper, we have analyzed to what extent they\nlearn referential aspects of language, focusing on\nanaphora. We have tested two models represen-\ntative of the prevailing architectures (Transformer,\nLSTM), and our methodology can be extended to\nany other architecture.\nWe ﬁnd that the two models behave similarly,\nbut the Transformer performs consistently better\n(around 10% higher accuracy in the probe tasks).8\nFuture work should test other architectures, like\nCNN-based LMs and LSTMs with attention, to\nprovide additional insights into the linguistic ca-\npabilities of language models.\nAs expected, our results show that lan-\nguage models capture morphosyntactic facts about\nanaphora: Based on the information in the hidden\nlayers, a simple linear transformation learns to link\n8With the caveat that the model we tested is slightly big-\nger than its LSTM counterpart.\n4185\nFigure 5: Linear distance in the discourse vs. cosine distance, for all the mention pairs with the same token pronoun.\nDistances averaged within bins of 20 tokens. Left: unsupervised, right: supervised.\npronouns to other pronouns or noun phrases, and\nto do so largely respecting agreement constraints\nin gender and number.\nAlthough it is much harder for models to in-\nduce a more global notion of entity (what we\nhave called semantico-referential aspects), mod-\nels seem to encode entity-speciﬁc information to\nsome extent. Models get confused when there\nare other mentions in the context, especially if\nthey match in some morphosyntactic feature, but\nless than could be expected; and they show some\nlimited ability to distinguish mentions that have\nthe same form but are in different coreference\nchains, though hampered by their heavy recency\nbias. The recency bias affects LSTMs more, but is\nalso found in Transformers, consistent with previ-\nous work on syntax (van Schijndel et al., 2019).\nOur results thus suggest that language models\nare more successful at learning grammatical con-\nstraints than they are at learning truly referential\ninformation, in the sense of capturing the fact that\nwe use language to refer to entities in the world;\nhowever, they still do surprisingly well at refer-\nential aspects, given that they are trained on text\nalone. Future work should investigate where these\nprimitive referential abilities stem from and how\nthey can be fostered in future architectures and\ntraining setups for language modeling, and neural\nmodels more generally.\nAcknowledgments\nWe gratefully acknowledge the AMORE team for\nthe feedback, advice and support. We are also\ngrateful to the anonymous reviewers for their valu-\nable comments. This project has received fund-\ning from the European Research Council (ERC)\nunder the European Union’s Horizon 2020 re-\nsearch and innovation programme (grant agree-\nment No 715154), and from the Spanish Ram ´on\ny Cajal programme (grant RYC-2015-18907). We\nthankfully acknowledge the computer resources at\nCTE-POWER and the technical support provided\nby Barcelona Supercomputing Center (RES-IM-\n2019-3-0006). We are grateful to the NVIDIA\nCorporation for the donation of GPUs used for this\nresearch. We are also very grateful to the Pytorch\ndevelopers. This paper reﬂects the authors’ view\nonly, and the EU is not responsible for any use that\nmay be made of the information it contains.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. arXiv preprint arXiv:1608.04207.\nLaura Aina, Carina Silberer, Ionut Sorodoc, Matthijs\nWestera, and Gemma Boleda. 2019. What do entity-\ncentric models learn? insights from entity linking in\nmulti-party dialogue. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 3772–3783.\nSamuel Broscheit. 2019. Investigating entity knowl-\nedge in BERT with simple neural end-to-end en-\ntity linking. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 677–685.\nPengxiang Cheng and Katrin Erk. 2019. Attending to\nentities for better text understanding. arXiv preprint\narXiv:1911.04361.\n4186\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv´e J´egou. 2018.\nWord Translation Without Parallel Data. In ICLR\n2018.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988.\nRichard Futrell, Ethan Wilcox, Takashi Morita, and\nRoger Levy. 2018. RNNs as psycholinguistic sub-\njects: Syntactic state and grammatical dependency.\narXiv preprint arXiv:1809.01329.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the hood: Using diagnostic classiﬁers to in-\nvestigate and improve how language models track\nagreement information. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP , pages 240–\n248.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. arXiv preprint arXiv:1901.05287.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nStevan Harnad. 1990. The symbol grounding problem.\nPhysica D: Nonlinear Phenomena , 42(1-3):335–\n346.\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine\nBordes, and Yann LeCun. 2019. Tracking the world\nstate with recurrent entity networks. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nEduard Hovy, Mitchell Marcus, Martha Palmer, Lance\nRamshaw, and Ralph Weischedel. 2006. Ontonotes:\nThe 90% solution. In Proceedings of the Human\nLanguage Technology Conference of the NAACL,\nCompanion Volume: Short Papers , pages 57–60,\nNew York City, USA. Association for Computa-\ntional Linguistics.\nDieuwke Hupkes, Sara Veldhoen, and Willem\nZuidema. 2018. Visualisation and’diagnostic classi-\nﬁers’ reveal how recurrent and recursive neural net-\nworks process hierarchical structure. Journal of Ar-\ntiﬁcial Intelligence Research, 61:907–926.\nYangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin\nChoi, and Noah A Smith. 2017. Dynamic entity rep-\nresentations in neural language models. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 1830–1839.\nJaap Jumelet, Willem Zuidema, and Dieuwke Hupkes.\n2019. Analysing neural language models: Con-\ntextual decomposition reveals default reasoning in\nnumber and gender assignment. In Proceedings of\nthe 23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL), pages 1–11.\nLeonard Kaufman and Peter J Rousseeuw. 1990. Find-\ning groups in data: an introduction to cluster analy-\nsis. John Wiley, New York.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLSTMs can learn syntax-sensitive dependencies\nwell, but modeling structure makes them better. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1426–1436, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nYair Lakretz, Germ ´an Kruszewski, Th ´eo Desbordes,\nDieuwke Hupkes, Stanislas Dehaene, and Marco\nBaroni. 2019. The emergence of number and syn-\ntax units in LSTM language models. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 11–20.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing LSTM\nlanguage models. arXiv preprint arXiv:1708.02182.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843.\nR. Mitkov. 2002. Anaphora Resolution . Studies in\nLanguage and Linguistics. Longman.\n4187\nDenis Paperno. 2014. Typology of adjectives bench-\nmark for compositional distributional models. In\nLREC.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2018. Building language\nmodels for text with named entities. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2373–2383.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018. Dissecting contextual word\nembeddings: Architecture and representation. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing , pages\n1499–1509.\nMassimo Poesio, Yulia Grishina, Varada Kolhatkar,\nNaﬁse Moosavi, Ina Roesiger, Adam Roussel,\nFabian Simonjetz, Alexandra Uma, Olga Uryupina,\nJuntao Yu, and Heike Zinsmeister. 2018. Anaphora\nresolution with the ARRAU corpus. In Proceedings\nof the First Workshop on Computational Models of\nReference, Anaphora and Coreference, pages 11–22,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nMassimo Poesio, Roland Stuckardt, and Yannick (Eds.)\nVersley. 2016.Anaphora resolution: Algorithms, re-\nsources, and applications. Springer.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012. CoNLL-\n2012 shared task: Modeling multilingual unre-\nstricted coreference in OntoNotes. In Joint Confer-\nence on EMNLP and CoNLL - Shared Task , pages\n1–40, Jeju Island, Korea. Association for Computa-\ntional Linguistics.\nSameer Pradhan, Lance A. Ramshaw, Mitchell P. Mar-\ncus, Martha Palmer, Ralph M. Weischedel, and Ni-\nanwen Xue. 2011. Conll-2011 shared task: Mod-\neling unrestricted coreference in ontonotes. In Pro-\nceedings of the Fifteenth Conference on Computa-\ntional Natural Language Learning: Shared Task,\nCoNLL 2011, Portland, Oregon, USA, June 23-24,\n2011, pages 1–27.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nBERT. In Advances in Neural Information Process-\ning Systems, pages 8592–8600.\nPeter J Rousseeuw. 1987. Silhouettes: a graphical aid\nto the interpretation and validation of cluster anal-\nysis. Journal of computational and applied mathe-\nmatics, 20:53–65.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with\nneural language models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5835–5841.\nRhea Sukthanker, Soujanya Poria, Erik Cambria, and\nRamkumar Thirunavukarasu. 2018. Anaphora and\ncoreference resolution: A review. arXiv preprint\narXiv:1805.11824.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw,\nNianwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2013. Ontonotes release 5.0\nldc2013t19. Linguistic Data Consortium, Philadel-\nphia, PA, 23.\nEthan Wilcox, Roger Levy, Takashi Morita, and\nRichard Futrell. 2018. What do RNN language\nmodels learn about ﬁller–gap dependencies? In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 211–221, Brussels, Belgium.\nAssociation for Computational Linguistics.\nThomas Wolf. 2019. Some additional experiments ex-\ntending the tech report “assessing BERT’s syntactic\nabilities” by Yoav Goldberg. Technical report.\nZichao Yang, Phil Blunsom, Chris Dyer, and Wang\nLing. 2017. Reference-aware language models. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n1850–1859.\nA Additional results for ﬁrst probe task\n(local context)\nThe probe models tend to refer to entities that\nare further away from the target than the closest\ngold entity (74.2% cases in the case of the Trans-\nformer), suggesting that they do not rely on a sim-\nple recency bias either (although both models do\nexhibit a recency bias, as we show in the main\npaper). This observation is conﬁrmed when look-\ning at the distribution of predicted antecedents and\ngold antecedents (Figures 6 and 7).\nFigure 8 presents a heatmap of pronominal\nagreement for AWD-LSTM. Similar to the Trans-\nformerXL heatmap from the main paper, we can\nobserve that in the majority of cases, the model\npredicts same form tokens with some variation ei-\nther at the gender level or at the number level.\n4188\nFigure 6: The distances between the pronoun and its\ngold and predicted antecedents for TransformerXL.\nFigure 7: The distances between the pronoun and its\ngold and predicted antecedents for AWD-LSTM.\nFigure 9 presents the performance of AWD-\nLSTM relative to the number of distractors in the\nwindow. While the tendencies seem to be the\nsame as the ones for TransformerXL, the curves\nare steeper, the model being more confused with a\nhigher number of distractors.\nB Additional results for second probe\ntask (global context)\nIn the main text, we say that, if we attempt to\nglobally distinguish chains, we obtain null results.\nHere we show the results of the experiment that\nleads to these null results.\nFigure 8: Pronominal agreement: Proportion of cases\nin which elements in the rows refer to elements in the\ncolumns for AWD-LSTM\nFigure 9: The accuracy of reference with respect to\nthe ratio of correct versus confounding mentions in the\nwindow for AWD-LSTM\nMethod To evaluate the distance metric learnt\nby the model we use the silhouette coefﬁcient\n(Rousseeuw, 1987), which is commonly used for\nintrinsic clustering evaluation. The silhouette co-\nefﬁcient for each pronounx is deﬁned as in Eq. (9),\nwhere a is the mean distance between x and all\nother items in the same chain, and b is the mean\ndistance between x and all other items in the clos-\nest chain (measured in the learnt space, not in\nterms of linear distance). Its range is [−1, 1], with\n1 corresponding to the pronoun being much closer\nto the other pronouns in its chain, 0 being border-\nline (equally close to the two compared chains),\nand -1 being much closer to the pronouns in the\nother chain. The average silhouette coefﬁcient is\nused as an overall measure of clustering quality.\nA score below 0.25 is usually deemed a null re-\nsult (Kaufman and Rousseeuw, 1990).\ns = b −a\nmax(a, b) (9)\nThe probe model is trained for 50 epochs, keep-\ning the model at the best validation epoch, i.e.,\nwhere the silhouette score over the validation data\nis highest.\nIn addition to the trained probe model, we pro-\nvide the results on global entity discrimination for\nthe unsupervised baseline which computes the co-\nsine similarity between the non-transformed hid-\nden representations of the language models, simi-\nlarly to the ﬁrst probe task.\nResults and Discussion All the obtained values\nare well below 0.25. Table 6 contains the results\nfor all the datapoints as well as divided into easy\nand difﬁcult documents. In easy documents, all\nthe chains have different pronouns, so they can\nbe distinguished by the token form only. Difﬁ-\ncult documents contain confusable chains, that is,\n4189\nthere are at least two different chains which share\nthe same pronoun. Coefﬁcients are a bit higher for\neasy documents, but still very low, and, for com-\nplex documents, they are virtually zero. Moreover,\nthe supervised models performs marginally better\nthan the cosine baselines, but clearly do not learn\nany reliable information.\nLSTM Transformer\nN unsup sup unsup sup\nall 1142 -0.09 0.02 -0.08 0.03\neasy 194 0.12 0.14 0.13 0.16\ndiff 948 -0.13 -0.007 -0.13 0.01\nTable 6: Results for the second probe task (average sil-\nhouette coefﬁcient).\nIndeed, the average distances within and across\nchains seem to conﬁrm these results. If mod-\nels were capturing global entity-related properties\nin their mention representations, we would ex-\npect pronouns with the same form but in different\nchains to be further away than pronouns (of any\nform) that belong to the same chain; instead, they\nare at the same distance (average cosines of 0.75\n/ 0.76 for Transformer, 0.74 / 0.73 for LSTM, re-\nspectively).\nWe conclude that the models’ sensitivity to\nwhether two identical pronouns belong to the same\nchain or not only shows if linear distance is fac-\ntored out (as in the main text). If it is not, as in the\ncurrent experiment, the models fail completely at\ndistinguishing entities.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8596029281616211
    },
    {
      "name": "Coreference",
      "score": 0.8410687446594238
    },
    {
      "name": "Transformer",
      "score": 0.7253422737121582
    },
    {
      "name": "Natural language processing",
      "score": 0.7234318256378174
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7001352906227112
    },
    {
      "name": "Sentence",
      "score": 0.6184822916984558
    },
    {
      "name": "Language model",
      "score": 0.5996003150939941
    },
    {
      "name": "Anaphora (linguistics)",
      "score": 0.5842072367668152
    },
    {
      "name": "Language understanding",
      "score": 0.4826255738735199
    },
    {
      "name": "Parsing",
      "score": 0.43847429752349854
    },
    {
      "name": "Resolution (logic)",
      "score": 0.2849324345588684
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170486558",
      "name": "Universitat Pompeu Fabra",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I11932220",
      "name": "Institució Catalana de Recerca i Estudis Avançats",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I4210104448",
      "name": "Institut Català de Ciències del Clima",
      "country": "ES"
    }
  ],
  "cited_by": 31
}