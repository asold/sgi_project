{
  "title": "A survey of Transformer applications for histopathological image analysis: New developments and future directions",
  "url": "https://openalex.org/W4387021831",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3169161675",
      "name": "Chukwuemeka Clinton Atabansi",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2055410055",
      "name": "Jing Nie",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2105808327",
      "name": "Haijun Liu",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2103453402",
      "name": "Qianqian Song",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2173247106",
      "name": "Lingfeng Yan",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2628280529",
      "name": "Xichuan Zhou",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A3169161675",
      "name": "Chukwuemeka Clinton Atabansi",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2055410055",
      "name": "Jing Nie",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2105808327",
      "name": "Haijun Liu",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2103453402",
      "name": "Qianqian Song",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2173247106",
      "name": "Lingfeng Yan",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A2628280529",
      "name": "Xichuan Zhou",
      "affiliations": [
        "Chongqing University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4324001652",
    "https://openalex.org/W4310263635",
    "https://openalex.org/W4386065374",
    "https://openalex.org/W3089090082",
    "https://openalex.org/W3182003946",
    "https://openalex.org/W4225488857",
    "https://openalex.org/W3172863135",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W3183943918",
    "https://openalex.org/W4375868826",
    "https://openalex.org/W6603884005",
    "https://openalex.org/W4319299841",
    "https://openalex.org/W4312204014",
    "https://openalex.org/W4313478927",
    "https://openalex.org/W4281853941",
    "https://openalex.org/W4293677254",
    "https://openalex.org/W4317473799",
    "https://openalex.org/W4312468136",
    "https://openalex.org/W4311597886",
    "https://openalex.org/W4312472844",
    "https://openalex.org/W3204764952",
    "https://openalex.org/W3203898052",
    "https://openalex.org/W4318566866",
    "https://openalex.org/W3138848412",
    "https://openalex.org/W3130835959",
    "https://openalex.org/W3195127593",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6605755270",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4312731085",
    "https://openalex.org/W4281765375",
    "https://openalex.org/W6606697098",
    "https://openalex.org/W4280514269",
    "https://openalex.org/W4313478500",
    "https://openalex.org/W4281257868",
    "https://openalex.org/W4386857835",
    "https://openalex.org/W4313314705",
    "https://openalex.org/W4366979990",
    "https://openalex.org/W4295917728",
    "https://openalex.org/W4308333450",
    "https://openalex.org/W4224991331",
    "https://openalex.org/W4380434577",
    "https://openalex.org/W4324030848",
    "https://openalex.org/W4321437409",
    "https://openalex.org/W4312856325",
    "https://openalex.org/W4353065773",
    "https://openalex.org/W3203328008",
    "https://openalex.org/W4353055921",
    "https://openalex.org/W4283706013",
    "https://openalex.org/W3201905155",
    "https://openalex.org/W4313910121",
    "https://openalex.org/W3203168750",
    "https://openalex.org/W3211647829",
    "https://openalex.org/W4283746543",
    "https://openalex.org/W4387225572",
    "https://openalex.org/W4385768167",
    "https://openalex.org/W4386071624",
    "https://openalex.org/W4387211569",
    "https://openalex.org/W4366750994",
    "https://openalex.org/W4206218744",
    "https://openalex.org/W4372271602",
    "https://openalex.org/W4225297260",
    "https://openalex.org/W4307843183",
    "https://openalex.org/W4307869614",
    "https://openalex.org/W4225142522",
    "https://openalex.org/W4381198685",
    "https://openalex.org/W4288809219",
    "https://openalex.org/W4281657029",
    "https://openalex.org/W4399244959",
    "https://openalex.org/W4309691108",
    "https://openalex.org/W4386362523",
    "https://openalex.org/W4323572175",
    "https://openalex.org/W4322097019",
    "https://openalex.org/W4313316059",
    "https://openalex.org/W4381054541",
    "https://openalex.org/W4384130825",
    "https://openalex.org/W4292264787",
    "https://openalex.org/W4205328020",
    "https://openalex.org/W4312523470",
    "https://openalex.org/W4402716294",
    "https://openalex.org/W4367147164",
    "https://openalex.org/W4361802136",
    "https://openalex.org/W4386352636",
    "https://openalex.org/W4367295871",
    "https://openalex.org/W4386076092",
    "https://openalex.org/W4391945515",
    "https://openalex.org/W3202476840",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W4385336812",
    "https://openalex.org/W4367315986",
    "https://openalex.org/W4221022458",
    "https://openalex.org/W4284963222",
    "https://openalex.org/W4226236462",
    "https://openalex.org/W4376114601",
    "https://openalex.org/W4225480823"
  ],
  "abstract": "Abstract Transformers have been widely used in many computer vision challenges and have shown the capability of producing better results than convolutional neural networks (CNNs). Taking advantage of capturing long-range contextual information and learning more complex relations in the image data, Transformers have been used and applied to histopathological image processing tasks. In this survey, we make an effort to present a thorough analysis of the uses of Transformers in histopathological image analysis, covering several topics, from the newly built Transformer models to unresolved challenges. To be more precise, we first begin by outlining the fundamental principles of the attention mechanism included in Transformer models and other key frameworks. Second, we analyze Transformer-based applications in the histopathological imaging domain and provide a thorough evaluation of more than 100 research publications across different downstream tasks to cover the most recent innovations, including survival analysis and prediction, segmentation, classification, detection, and representation. Within this survey work, we also compare the performance of CNN-based techniques to Transformers based on recently published papers, highlight major challenges, and provide interesting future research directions. Despite the outstanding performance of the Transformer-based architectures in a number of papers reviewed in this survey, we anticipate that further improvements and exploration of Transformers in the histopathological imaging domain are still required in the future. We hope that this survey paper will give readers in this field of study a thorough understanding of Transformer-based techniques in histopathological image analysis, and an up-to-date paper list summary will be provided at https://github.com/S-domain/Survey-Paper .",
  "full_text": "Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nREVIEW\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96  \nhttps://doi.org/10.1186/s12938-023-01157-0\nBioMedical Engineering\nOnLine\nA survey of Transformer applications \nfor histopathological image analysis: New \ndevelopments and future directions\nChukwuemeka Clinton Atabansi1, Jing Nie1*, Haijun Liu1, Qianqian Song1, Lingfeng Yan1 and Xichuan Zhou1* \nAbstract \nTransformers have been widely used in many computer vision challenges and have \nshown the capability of producing better results than convolutional neural networks \n(CNNs). Taking advantage of capturing long-range contextual information and learning \nmore complex relations in the image data, Transformers have been used and applied \nto histopathological image processing tasks. In this survey, we make an effort to present \na thorough analysis of the uses of Transformers in histopathological image analysis, cov-\nering several topics, from the newly built Transformer models to unresolved challenges. \nTo be more precise, we first begin by outlining the fundamental principles of the atten-\ntion mechanism included in Transformer models and other key frameworks. Second, \nwe analyze Transformer-based applications in the histopathological imaging domain \nand provide a thorough evaluation of more than 100 research publications across differ-\nent downstream tasks to cover the most recent innovations, including survival analysis \nand prediction, segmentation, classification, detection, and representation. Within this \nsurvey work, we also compare the performance of CNN-based techniques to Transform-\ners based on recently published papers, highlight major challenges, and provide inter-\nesting future research directions. Despite the outstanding performance of the Trans-\nformer-based architectures in a number of papers reviewed in this survey, we anticipate \nthat further improvements and exploration of Transformers in the histopathological \nimaging domain are still required in the future. We hope that this survey paper will give \nreaders in this field of study a thorough understanding of Transformer-based techniques \nin histopathological image analysis, and an up-to-date paper list summary will be pro-\nvided at https:// github. com/S- domain/ Survey- Paper.\nKeywords: Transformer, Histopathological imaging, CNN, Whole slide image, Survival \nanalysis, Digital pathology\nIntroduction\nHistopathological imaging has been regarded as a technique for identifying nearly all \ntypes of cancers since it provides a more thorough understanding of the diseases [1, 2]. \nThey are a very important source of primary information in clinical domains, which \nassists pathologists in performing cancer diagnosis. Histopathological images are mostly \n*Correspondence:   \njingnie@cqu.edu.cn; zxc@cqu.\nedu.cn\n1 School of Microelectronics \nand Communication \nEngineering, Chongqing \nUniversity, Chongqing 400044, \nChina\nPage 2 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nused for cancer grading and offer more detailed information for diagnosis when com -\npared to other medical imaging techniques, including magnetic resonance imaging \n(MRI), computerized tomography (CT), transrectal ultrasound (TRUS), mammography, \nand many others, and diseases are also examined by identifying the cells and tissue pre -\nsent in lesions [1, 3]. For various cancer types, pathologists may choose treatment plans \nbased on histopathological images coupled with genomic records. With the recent devel-\nopment and deployment of digital slide scanners in different clinical areas, the digitiza -\ntion of histopathological slides (i.e., whole slide images (WSIs)) into gigapixel images is \nbecoming more prevalent. In computational pathology, histopathological slides (WSIs) \ndisplay a hierarchical formation of visual tokens across different resolutions and can \nhave a pixel size up to 160,000 × 160,000 pixels at 20×magnification. Figure  1 shows \nsome samples of histopathological slides and some annotated patches extracted from the \nslides that contain different tissue types.\nThe technique of digitizing histopathological images, known as digital pathology, \ncreates a new approach to collecting image data for artificial intelligence technologies. \nIn recent years, artificial intelligence techniques that process and analyze histopatho -\nlogical images have become more common in both scientific research and clinical set -\ntings. This is primarily due to the rise of deep learning, especially convolutional neural \nnetworks (CNNs), which have achieved outstanding results in many computer vision \ntasks  [4–6]. Recently, an alternative CAD system that is capable of modeling long-\nrange pixel information, such as transformers, has been developed. Transformers [7 ] \nhave emerged as one of the most recent technological developments in deep learn -\ning for achieving robust results in many computer vision tasks. It was first built as a \nrobust example of using deep learning techniques to tackle sequential inference tasks \nin natural language processing (NLP). Dosovitskiy [8 ] et al. introduced a vision trans -\nformer (ViT)-based architecture for image classification tasks, demonstrating that \nrelying on CNNs for image classifications is unnecessary and that a pure transformer \napplied straight to sequences of image patches can get excellent results. Other than \nimages and NLP tasks, transformers have also been adopted and applied to other deep \nlearning domains, including autonomous driving  [9 ], video classification  [10], secu -\nrity [11], general audio representations [12], audio–video synchronization [13], mobile \ndevices [14] and so on. Motivated by this innovation, several studies have adopted a \nFig. 1 Some samples of histopathological images. a Whole slide images (WSIs). b Annotated PanNuke \ndataset from different tissue types for nuclei instance classification and segmentation\nPage 3 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nvariety of approaches to solve different deep learning challenges, including CNN- and \ntransformer-based approaches, but it still remains unclear whether ViT architectures \ncan produce better results than CNNs for histopathological image analysis. Trans -\nformers, like any other deep learning or machine learning technique, have pros and \ncons. Besides, transformers, unlike CNN-based approaches, are devoid of convolution-\ninduced biases, which enables them to capture long-range contextual information and \nlearn more complex relations in the image data. This is advantageous in histopatho -\nlogical imaging, where it is critical to consider not just the region of interest, but also \nthe neighboring tissues when diagnosing a particular disease. Transformers, on the \nother hand, are data-demanding and require greater computing effort. This can be a \ndifficult problem, especially in the field of histopathological imaging, where resources \nmay be inadequate due to concerns about patient privacy. At present, many studies \nhave been conducted in the field of histopathological imaging using transformer-based \napproaches, including image segmentation [2 , 15], classification [16, 17], detection [18, \n19], representation [20– 22], cross-modal retrieval [23], image generation [24], survival \nanalysis [25] and survival prediction [26]. Figure  2 displays current transformer appli -\ncations in histopathological image analysis, as surveyed in this research work, which \nwill be further explored in ’’ Current progress’’ Sect.\nHowever, based on the recently published studies, it has been shown that trans -\nformer architectures have the capacity to achieve higher performance on various his -\ntopathological imaging tasks than the previous models. \nMoreover, the primary aim of the paper is to provide a thorough review of trans -\nformer applications in the histopathological imaging field and demonstrate how \ntransformers are applied to a variety of tasks. In particular, it provides readers in this \nfield of study with a thorough understanding of transformer-based techniques in his -\ntopathological image analysis and also establishes the foundation for future innova -\ntion to improve the performance of transformer architectures in this domain. To this \nend, our key contributions include: (1) this work provides a thorough evaluation of \nmore than 100 research publications across the histopathological imaging field to \ncover the most recent innovations; (2) it provides a thorough overview of the entire \ndomain by classifying the research papers according to how they apply to histopatho -\nlogical imaging, as shown in Fig.  2; (3) it classified each of these applications, pointed \nout task-specific challenges, and highlighted the approaches used to address them \nFig. 2 Current transformer applications in histopathological image analysis, as surveyed in this research work\nPage 4 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nbased on the proposed work, as demonstrated in Tables  1, 2, 3, 4, and  5 in ’’ Current \nprogress ’’ , ’’ Discussion ’’ Section it provides a thorough analysis of designing trans -\nformer-based approaches for handling more difficult real-world challenges and also \ncompares transformers with CNN-based models based on recently published works. \nThe remainder of this survey paper is structured as follows: ’’Background ’’ Sect. Pro -\nvides a brief background on the study and basic components of transformers. In Cur -\nrent progressSect. Current applications of transformers in histopathological image \nanalysis are investigated. The discussions and conclusion are covered in Discussion, \n’’Conclusion ’’Sect. Respectively.\nBackground\nOver the years, histopathological imaging computer-aided diagnosis (CAD) systems \nhave witnessed a lot of technological advancement following the advent of transformer \narchitectures. However, in this part, we will give a quick overview of CNN-based \napproaches and outline the basic operating principles together with their main advan -\ntages and drawbacks in the field of histopathological imaging. In addition, we will also \ndiscuss the fundamental ideas that underlie the success of the transformer-based tech -\nniques and then provide further information in subsequent sections. Finally, we compare \nthe CNN methods versus the transformer methods.\nCNN applications in histopathological image analysis\nFor some years now, CNNs have proven to be good at analyzing image data and are \nthe most widely used deep learning networks for many medical and clinical challenges, \nespecially histopathological imaging. This is as a result of the strong prior that the con -\nvolution operations impose on the weights, forcing the identical weights to be shared \nacross each and every pixel [27]. The major advantage of CNN-based approaches com -\npared to previous architectures is their ability to automatically identify important fea -\ntures in an image without any form of human oversight. The process of building any \nCNN architecture for histopathological image analysis is a collaborative effort between \nresearchers and medical professionals. These innovations are primarily driven by a lot \nof architectural advancements, improved loss functions, the accessibility of specialized \nhardware devices, and publicly accessible libraries created for specific purposes. There -\nfore, we direct readers who are interested in this research direction to some previously \npublished survey papers on CNN applications in the histopathological imaging field [4–\n6]. Although CNN-based techniques have experienced a lot of architectural improve -\nments over the years, their ability to be applied to the full range of histopathological \nimage tasks is also constrained by their dependency on huge amounts of labeled data -\nsets. The study of histopathological imaging for different clinical tasks has also been \ncross-pollinated by the CNN models [28–30], and they sometimes function as black box \nsolutions and are typically more difficult to explain. However, the success of CNN-based \nmethods is primarily due to their capacity to extract useful information from input \nimages, doing away with the necessity for conventional manual image processing tech -\nniques. Despite increasing the receptive field, they still face a lot of challenges in mod -\neling long-range information as well as spatial dependencies due to their weight sharing \nand inductive bias locality. The local nature of the convolutional operations in CNNs is \nPage 5 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nthe major challenge associated with CNN-based techniques, as it prevents them from \ncapturing long-range semantic dependencies from the given input images. Thus, an \nalternative CAD system that is capable of modeling long-range pixel information, such \nas transformers, is required to achieve more robust results than the previous models.\nTransformers\nBasics\nTransformer-based architectures are the most advanced technique for handling \nsequences. They make use of attention mechanisms due to their capacity to model long-\nrange semantic information. Besides, they also make use of an encoder–decoder design \nstrategy that produces an output without relying on recurrence and convolutions. As a \nresult, we first begin by giving a brief introduction to the basic ideas behind the attention \nmechanism, followed by a comprehensive explanation of how the transformer operates.\nAttention mechanism\nThe attention mechanism evolved naturally from sequence-related challenges. Nowa -\ndays, it is often used to extract unimportant information from the data while concen -\ntrating on the relevant portions of the data, and it can be used for a number of deep \nlearning architectures across different clinical domains and downstream tasks. An atten-\ntion mechanism was initially developed to boost machine translation encoder–decoder \nperformance. It was initially introduced by Bahdanau et al. [31] for the language trans -\nlation task to tackle the bottleneck that results from the use of a fixed-length encod -\ning vector, where the decoder would have minimal access to the information delivered \nby the input. This is viewed as being especially troublesome for long or sophisticated \nsequences because the representation’s dimensionality would be limited to match that of \nunsophisticated or shorter sequences. \n(i) Attention mechanisms in computer vision tasks:\n The concept of emulating human attention emerged in the computer vision domain in \nan attempt to minimize the computational problem of image processing while increasing \naccuracy by adding a model that only focused on certain portions of images rather than the \nwhole image. However, the attention mechanisms we employ today in our various models \noriginated in the field of NLP . Several studies have been proposed in the past to incorpo-\nrate attention mechanisms into their architecture. For example, the work in  [32] instead \nfocuses on the interaction between channels and develops a new attention mechanism \nframework known as squeeze-and-excitation that explicitly models the interdependencies \nbetween channels and adaptively recalibrates channel-wise feature responses. In contrast to \nBahdanau attention, the attention mechanism, as proposed in  [7], has been reconstructed \nas a function that uses values, keys, and queries that are attained from the module’s input \nvectors. In practice, the values and keys are constructed together into matrices V and K, \nwhile the attention function is computed simultaneously on a set of queries and arranged \ntogether into a matrix Q. Then, the output function is determined as a weighted sum of val-\nues, where each value of the weight is computed as the attention between queries and keys, \nrespectively. In addition, the operation of self-attention, as illustrated in Fig. 3, is typically \nperformed in matrix formation in order to speed up the parallel calculation. Additionally, \nPage 6 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nin order to quickly demonstrate a clear picture of the self-attention mechanism, we begin \nby defining it in an element-wise manner. Let xi ∈ Rc,i= 1, ..,m,  be the input image, and \nthe corresponding vectors generated by the parameters (i.e., W q, W k, and W v ) be query \nqi ∈ Rg\nq , key ki ∈ Rg\nk  , and value vi ∈ Rg\nv , respectively. Again, gq , gk , and gv represent the \nnumber of features learned from x i and also the sizes of q i , ki , and vi , respectively.\nA softmax function is used to calculate the weights βij and is represented by the follow -\ning equation:\nwhere Ŵ represents the softmax function and β′\nij computes the contribution of the jth \ninput element to the ith output element. Throughout this process, β′\nij is considered to be \nthe attention attributed to the factor vi . As a result, the final resultant attention can be \ncalculated as a weighted total of each and every value, as shown below:\nIn addition, it is reasonable to extend element-wise self-attention into matrices. How -\never, for each input x i , parallel matrix computation is commonly used to produce and \ncreate the query q i , key ki , and value vi , respectively. Matrices can be formed by stack -\ning up the input x i , value vi , query q i , and key ki , accordingly. Let X ∈ Rn×c be the input \nmatrix, and the value, query, and key matrices be V, Q, and K, respectively. The number \n(1)\n\n\n\nqi = xi × W q, where W q ∈ Rc×gq ,\nki = xi × W k, where W k ∈ Rc×gk ,\nvi = xi × W v, where W v ∈ Rc×gv ,\ngq = gk.\n(2)βij = Ŵ\n(\nβ′\nij\n√gk\n)\n=\nexp\n(\nβ′\nij√gk\n)\n∑\nj exp\n(\nβ′\nij√gk\n).\n(3)β′\nij = qi × k T\nj ,\n(4)yi =\n∑\nj\nβij × vj.\nFig. 3 A schematic demonstration of the self-attention mechanism\nPage 7 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nof samples is represented by n, and each individual matrix is made up of the components \n(i.e., X =[ x1; x2; x3;··· ;x n]T ) . Therefore, the attention matrix A and resultant matrix Y \nare now computed as shown below:\nThe authors  [7] created a different form of attention mechanism known as multi-head \nself-attention (MHSA). They demonstrated that applying several self-attentions to the \nsame input allows for a more efficient acquisition of hierarchical information. However, \nin the mechanism, h (i.e., h = 8) distinct attention heads were generated, each with a \nunique set of weight matrices (W(Q), W(K),  and W(V)). The key, value, and query matri-\nces are then created for each attention head by multiplying the input matrix by each of \nthe weight matrices (W Q, W K, and W V) . Again, these query, key, and value matrices are \nsubjected to attention mechanisms in order to produce an output matrix from each atten-\ntion head. In addition, the output of the MHSA layer is produced by concatenating the \noutput matrix acquired from each attention head (h) and the dot product with the weight \n(W O) . Finally, given self-attentions (heads) denoted as h, the system produces the desired \noutput result by integrating the computed attentions as illustrated in the equation below:\nwhere MH denotes the multi-head self-attention operator and fC  is the concatenating \nfunction. The linear projection matrices W Q\ni , W K\ni  , and W V\ni  map the Q, K, and V matrices \ninto the appropriate subspaces.\nTransformer architecture\nTransformers are generally designed to handle sequence-related tasks while also deal -\ning with long-term dependencies. In the paper titled “ Attention Is All You Need”  [7], \nthe authors introduced a standard transformer architecture that employs an encoder–\ndecoder formation, as shown in Fig.  4, which will be discussed further in the subse -\nquent sections. In the architecture, the encoder framework converts an input sequence \n(x1 ,x2 ,x3 , ...,xn) into a series of continuous representations (i.e., an output sequence) z \n= (z1 ,z2 ,z3 , ...,zn ) . The decoder then produces the resultant sequence (y1 ,y2 ,y3 , ...,ym ) \none component at a time from the encoded representation z, using the previous output \nas additional input when generating the next. The transformer follows this general archi-\ntectural framework, which employs different layers in both the encoder and decoder \nmodules, as demonstrated on the left and right sides of Fig. 4.\n(i) Transformer encoder\nTransformer architectures, as shown in Fig.  4 mainly consist of both encoder and \ndecoder blocks. The encoder is composed of N = 6 identical layers built on top of one \n(5)A = Ŵ\n(Q × K T\n√gk\n)\n∈ Rn×n,\n(6)Y = A × V ∈ Rn×gv,\n(7)Yi = A(Q × W Q\ni , K × W K\ni , V × W V\ni ),\n(8)M H (Q,K,V) = fC(Y1 ,Y2 ,Y3 ,··· ,Yh)W O,\nPage 8 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nanother that extract features from the input sequence. Each layer is made up of two sub-\nlayers known as the feed-forward network layer (FFNL) and the multi-head self-atten -\ntion mechanism (MHSA). Again, residual connections were employed across each of \nthe sub-layers, followed by layer normalization. First, the multi-head attention is com -\nputed in each block, followed by a layer-wise normalization block. The sum of the multi-\nhead attention input and output is computed primarily using layer-wise normalization. \nAfter applying a feed-forward layer, the input and output of the feed-forward layer are \nsummed together using layer-wise normalization. \n (ii) Transformer decoder\nThe transformer decoder shown on the right-hand side of Fig.  4 uses the extracted \nfeatures to generate the output sequence. It consists of N = 6 identical layers with a few \nmodifications. An additional sub-layer block is added on top of the encoded output, \nwhich carries out multi-head attention over the encoder stack output. Since the predic -\ntion is based on a known state, masking was utilized in the first self-attention block to \nprevent further contributions to the state of the preceding position. In addition, after the \ndecoder’s output layer, a linear and a softmax layer are added to produce the final result.\nVision transformer (ViT)\nTransformers were initially introduced in NLP tasks where the objective was to under -\nstand the text and draw relevant and useful conclusions. Transformer architectures have \naccomplished significant results and have become a de facto standard in the field of NLP \nbecause of their generalization abilities and simplicity. Following their success in NLP \ntasks, researchers in this domain have made numerous attempts to adapt transformer \narchitectures to various vision challenges. Among the most common transformer-based \nFig. 4 A schematic demonstration of a standard transformer architecture\nPage 9 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \narchitectures in vision that have been established are the DETR  [33], Swin-trans -\nformer  [34], ViT  [8], DeiT  [35], and BEiT  [36]. In  [33], the authors were the first to \nmake use of transformers in computer vision for object detection tasks. The proposed \narchitecture, known as DETR, focuses on a transformer encoder–decoder architecture \nand a set-based global loss that forces unique predictions through bipartite matching. \nUnlike other traditional object detection approaches that rely heavily on handcrafted \ntechniques, the DEtection transformer does not need any special layers, which makes \nit easy to replicate in any model that has common transformer and CNN classes. On \nthe other hand, it is simple to generalize and create unified panoptic segmentation. In \n2021, Dosovitskiy et al. [8] introduced a vision-based transformer known colloquially as \nViT, stating that CNNs were no longer required and that a pure transformer architecture \napplied instantly to sequences of image patches can produce robust results, particularly \non image classification problems. The input image, as presented in Fig.  5, is split into \na number of patches, each of which is encoded spatially to provide spatial information \nusing a positional encoding technique. The ViTs have produced better or even higher \nresults, outperforming state-of-the art (SOTA) CNNs for many downstream tasks, espe -\ncially when pre-trained on huge datasets. To this end, transformer architectures require \nmore training data to obtain comparable results or even higher than CNNs, and more \ndetails will be provided in subsequent sections.\nPros and cons of a transformer architecture\nTransformers have been widely used in many computer vision challenges and have \nshown the capability of producing better results than other deep learning techniques. \nSome of the advantages of transformers in computer vision tasks include efficient paral -\nlel processing, adaptability with variable-length sequences, effective handling of global \ndependencies, higher network capacity, and so on. Due to the attention mechanisms \nincorporated into the networks, they can process sequences in parallel and also handle \nglobal dependencies, making them more efficient and faster than standard sequential \nFig. 5 A schematic diagram of a standard ViT model. Sequential image patches are used as the input, which \nis then processed with a transformer encoder and uses an MLP head module to generate a class prediction\nPage 10 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nnetworks such as recurrent neural networks. In addition, transformer architectures also \nproduce robust results on NLP tasks due to higher network capacity and the ability to \ncapture complicated relationships in sequential data. Despite the fact that transformers \ncan enable higher network capacity and learn more complex relations in the image data, \nthey also have some drawbacks. Some of the disadvantages of transformers in computer \nvision tasks include high computing costs, overfitting vulnerability, data inefficiencies, \nand so on. Transformers are more resource-intensive than any other deep learning tech-\nnique due to the self-attention mechanism built into the networks, which necessitates \na lot of computation as well as training time. Furthermore, insufficient data to train the \nmodel effectively is another notable disadvantage of transformers, which can pose a lot \nof problems in NLP tasks where there is a limited amount of labeled data.\nTransformer methods versus CNN methods\nOver the years, CNNs have shown outstanding performances for histopathological \nimage analysis, while transformers such as ViTs have produced better or even higher \nresults, outperforming SOTA CNNs for many downstream tasks, especially when pre-\ntrained on huge datasets. CNN architectures are more mature and make use of pixel \narrays, so they are easier to implement, study, and train when compared to transformer \narchitectures. During training, as the depth of the networks increases, the receptive field \nof CNNs significantly widens; therefore, the features mined at lower stages differ sig -\nnificantly from those at later stages. Besides, CNNs make use of convolution, a “local” \ntechnique limited to a tiny area of an image, which makes them more advantageous in \ncapturing local semantic structures. The feature maps created by the CNNs through the \nconvolution process using these trainable convolutional filters, which are hidden rep -\nresentations of the true image, only affect a tiny portion of the image at a time. Addi -\ntionally, CNNs are also limited in capturing long-distance correlations between image \nregions due to their small receptive field. On the other hand, transformer architectures \nmake use of a self-attention mechanism, a “global” technique since it gathers relevant \ninformation from the entire image. This enables them to effectively capture more distant \nand important information in an image. The representation in transformer architectures \nis similar in every layer and can gather global information early owing to self-attention. \nAgain, the MHSA in particular provides a global receptive field, which results in iden -\ntical representations in distinct numbers of layers. Moreover, all attention outputs are \nlinearly concatenated to the appropriate dimensions by the MHSA layer, and the block \nof each layer of the MHSA has the capacity of aggregating features globally to produce \naccurate knowledge of long-distance interactions. To this end, transformer architectures \nrequire more training data to obtain comparable results or even higher than CNNs, and \nmore details will be provided in subsequent sections.\nCurrent progress\nVision transformers (ViTs) have been generally used for a variety of clinical purposes. \nHowever, in this section, we will first discuss the searching procedures used to obtain \nall the papers reviewed in this survey (see ’’ Article searching and selection proce -\ndures’’ Sect.). Then, we will present and discuss different ways of employing transform -\ners for histopathological imaging in ’’Different ways of employing Transformers for \nPage 11 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nhistopathological imaging ’’ Sect. Finally, the current transformer applications in histo -\npathological image analysis, as shown in Fig.  2, are discussed in ’’ Current transformer \napplications in histopathological imaging’’ Sect.\nArticle searching and selection procedures\nThis section presents a brief discussion of the methods used in searching for and select -\ning the research papers. The newly built architectures, as shown in Fig.  6, are classified \nbased on their learning tasks.\nAs demonstrated in Fig.  7(b), the histopathological imaging domain has been slightly \nimpacted by transformer-based architectures since the inception of the first ViT archi -\ntecture. Figure 7(a) displays the statistics of the papers presented in this survey accord -\ning to histopathological imaging problem settings. In particular, we explore publications \nfrom Science Direct, Springer, Xplore, PubMed, IEEE, and conference proceeding \npapers, especially those from conferences on medical imaging like SPIE, RSNA, IPMI, \nMICCAI, ISBI, and so on. In addition, we use Google Scholar to search for paper refer -\nences and manuscripts. As a result of our search queries using various keywords such as \nFig. 6 Transformer-based architectures for histopathological image analysis. The figure shows some of the \nexisting approaches for different downstream tasks, including segmentation, survival analysis and prediction, \nrepresentation, detection, and classification\nFig. 7 The chart a displays the statistics of the papers presented in this survey according to histopathological \nimaging problem settings. The rightmost figure b demonstrates consistent growth in recent development \n(from 2019 to July 2023)\nPage 12 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nvision transformers, transformers in medical imaging, transformers in histopathologi -\ncal imaging, transformers in image classification and segmentation, and so on, we found \nmore than a thousand papers about the transformer, some of which are from the fields \nof natural imaging or language studies. Again, we construct the concepts of our survey \nfrom the self-attention and the ViT published papers, which are major milestones for \nthe investigation of transformers in histopathological image analysis. Finally, we limited \nthe survey research to exclusively cover transformer applications in the histopathologi -\ncal imaging domain. As presented in Fig.  6, we show the categorization of some recently \ndeveloped models based on the learning tasks in the histopathological imaging field. \nThen, in Fig. 7, we show the percentage of the papers presented in this survey according \nto histopathological imaging problem settings and consistent growth in recent develop -\nment, which will be further discussed in the following subsections.\nDifferent ways of employing transformers for histopathological imaging\nRecently, numerous studies have been conducted on how to apply transformers for his -\ntopathological image analysis. Some studies attempted to use only pure transformers \n(i.e., transformers without convolution blocks (see Fig.  5), while others tried to integrate \nthe benefits of transformers (e.g., DETR [33], ViT [8], DeiT [35], BEiT [36], Swin-trans -\nformer [34], and so on) and CNNs (e.g., EfficientNet [37], Unet [38], ResNet [39], and so \non) for different downstream tasks. However, in this section, we will classify them into \nthree distinct types, which will be further discussed in the following subsections.\n(i). Pure transformers: Pure transformers, as shown in Fig.  5, are described as those \nViT-based architectures that resemble the ones originally proposed by Dosovitskiy  et \nal.  [8] which typically do not include major structural adjustments. They outperform \nconventional CNN models in terms of scalability and efficiency at both small and large \ncomputational sizes. TransWS [40], MCAT [26], HIPT [22], PyT2T-ViT [41], and ViT-\nWSI [17] are some examples of pure transformer models developed for different histo -\npathological imaging tasks.\n(ii). Graph-based transformer methods: These are the types of transformer networks \nthat introduce graphs into traditional vision transformers (see Fig.  9(a) GTP). Moreo -\nver, graphs are a common type of data structure, and there are several areas of applica -\ntion in which datasets can be characterized as graphs, such as biological networks, social \nnetworks, and several other types of multimedia domain-specific data. However, using \ngraph-based learning methods is a normal practice in both histopathological and other \nmedical image analysis. As a result, analyzing graph data can reveal important informa -\ntion about node classification, and the basic idea behind graph learning is to use the data \ngraph to learn a dense representation of each and every sample, such as embeddings, \nwhile maintaining the intrinsic inter-sample relationships. Transformer, as an attention-\nbased model, is capable of processing graph data, including aggregating node informa -\ntion and determining the relationship between the nodes. Dwivedi et al. [42] developed a \ngraph transformer network (GTN) that supports the use of specific domain information \nas edge features and provides interpretability via self-attention modules that locate the \nkey regions of the graphs for prediction. AMIGO [43], LA-MIL [44], Wang et al. [45], \nand GTP [46], MEGT [47] are some examples of graph-based transformer models that \nhave been proposed for different histopathological image classification tasks.\nPage 13 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \n(iii). Hybrid transformer–CNN: In histopathological image analysis, there are many \nways in which transformers can be combined with CNN to form a hybrid model. The \nsimplest method is to use both in an effort to capitalize on both of their advantages. \nThese hybrid networks either use transformer to replace some parts of the network or \nincorporate transformer into the entire network by using CNN as the backbone of the \nnetwork. However, we find out that current research in histopathological image seg -\nmentation focuses mainly on the following three issues to develop transformers com -\nbined with the widely used U-shaped framework: firstly, transformer blocks are inserted \nat various positions in the U-shaped structure, as shown in Fig.  8. Secondly, employing \nseveral techniques to combine CNN and transformer networks. Finally, making use of \nattention mechanisms or employing multi-scale features. SeTranSurv [25], ATTransU -\nNet [2], TCNN [1], SwinCup [48], and DHUnet [49], TransNuSS  [50] are some examples \nof hybrid transformer–CNN models that have been developed for different histopatho -\nlogical imaging tasks.\nCurrent transformer applications in histopathological imaging\nThis section presents the current applications of transformers in histopathological image \nanalysis, such as classification, segmentation, survival analysis and prediction, represen -\ntation, detection and localization, and other tasks. These applications, as demonstrated \nin Fig. 2, are classified based on their learning tasks.\nHistopathological image classification\nVision transformer  [8] has demonstrated remarkable performance in several natural \nimage classification tasks since its inception. From previous and past studies, trans -\nformer-based techniques for cancer investigation and prediction are often referred to \nas classification tasks and can be classified into three distinct classes. Firstly, the direct \napplication of transformer architectures to histopathological images. Secondly, making \nuse of transformer architectures in conjunction with convolutions to learn more repre -\nsentative local features. Finally, making use of transformer architectures in conjunction \nwith graph representations will help better manage data with complex sizes. This section, \nas demonstrated in Table.  1, will provide a thorough overview of current transformer \napplications for histopathological image classification. Figure  9 shows some examples of \nSOTA transformer architectures developed for histopathological image classification. \nFig. 8 Some typical transformer U-shaped architectures\nPage 14 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nTable 1 Transformer applications in histopathological image classification tasks\nMethod Tissue Dataset Challenge Highlight ACC / F1 / AUC (%)\nScoreNet [16] Breast BRACS, BACH and CAMELYON16 The huge size of WSIs and the cost of \nexhaustive localized annotations\nEfficient transformer-based archi-\ntecture local and global attention \nmechanism\n–/ 81.10 /–\nBreaST-Net [51] Breast BreakHis Differentiating subtypes of benign and \nmalignant cancers\nEnsemble of Swin transformers 99.60 / 99.50 / 99.40\nHATNet [52] Breast Custom Diagnostic variability and misdiagnosis \nof breast cancer\nEnd-to-end ViTs with self-attention \nmechanism\n71.00 / 70.00 /–\ndMIL-transformer et al. [53] Breast (LNM) CAMELYON16 and 17 and the SLN-\nBreast\nTaking into account the morphology \nand spatial distribution of cancerous \nregions\nTwo-stage double max–min MIL trans-\nformer architecture\n89.23 / 84.83 / 91.67\nASI-DBNet  [54] Brain UHP Lack of precision and accuracy in grad-\ning brain tumor\nAn adaptive sparse interactive ResNet \nViT dual network\n95.24 / 95.23 / 96.83\nDing et al. [55] Brain NCT-CRC-HE, BreaKHis and LDCH Aliasing phenomena caused by down-\nsampling operations and smoothing \ndiscontinuous\nViT-based network with wavelet posi-\ntion embedding\n99.01 /–/–\nDT-DSMIL [56] Colorectal Custom Data annotations Weakly supervised ViT-based MIL 93.50 / 94.37 / 97.69\nIMGL-VTNet [57] Gastric IMGL The problem of identifying IM glands Multi-scale deformable transformer –/ 94.00 /–\ntRNAsformer  [58] Kidney TCGA Gather the information needed to learn \nWSI representations\nTransformer-based learning to predict \nRNA sequence expressions\n96.25 / 96.25 /–\ni-ViT [59] Kidney TCGA-KIRP Capturing cellular and cell-layer level \npatterns\nInstance-based Vision Transformer \nnetwork\n93.01 / 93.60 /–\nGTP [46] Lung CPTAC, TCGA and NLST Label noise Graph-transformer with vision trans-\nformer\n91.20 /–/ 97.70\nFDTrans [60] Lung TCGA-NSCLC Large intra-class differences and a lack \nof annotated datasets\nFrequency domain transformer-based \narchitecture\n92.33 / 94.64 / 93.16\nYacob et al. [45] Skin Custom Time-consuming and inter-pathologist \nvariability\nWeakly supervised approach using \ngraph-transformer\n93.50 /–/–\nKAT [61] Stomach Gastric-2K, Endometrial-2K Over-smoothing and High computa-\ntional complexity\nKernel attention transformer 94.9 /–/ 98.30\nDT-MIL [62] Lung and breast CPTAC-LUAD and BREAST-LNM The problem of learning an effective \nWSI representation\nDeformable transformer model for MIL –/ 96.92 / 99.06\nPage 15 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nTable 1 (continued)\nMethod Tissue Dataset Challenge Highlight ACC / F1 / AUC (%)\nTCNN [1] Breast, Lung, etc. MDD and RWD Artifacts in WSIs Transformer with CNN 96.90 / 97.40 / 98.50\nCWC-transformer  [63] Breast and Lung CAMELYON16, TCGA-LUNG and MSK Loss of spatial information and prob-\nlems associated with feature extraction \nin WSI\nCombination of transformer and CNN 92.59 /–/ 94.88\nTransPath [64] Breast, Lung, etc. TCGA, PAIP , PatchCam, etc. Data annotation Self-supervised learning transformer-\nbased network\n95.85 / 95.82 / 97.79\nTransMIL [65] Breast, Lung and Kidney CAMELYON16, TCGA (NSCLC and RCC) Correlation among different instances, \nHuge size and the lack of pixel-level \nannotations\nTransformer-based multiple-instance \nlearning (MIL)\n94.66 /–/ 98.82\nDecT [66] Breast, Endometrium BreakHis, BACH, and UC Not taking into account the staining \nproperties of histopathological images\nColor deconvolution with transformer \narchitecture\n93.02 / 93.89 /–\nLA-MIL [44] Colorectal and stomach TCGA-CRC and TCGA-STAD Quadratic complexity of transformer \narchitectures with respect to the \nsequence length\nMIL local attention graph-based trans-\nformer model\n–\nPrompt-MIL [67] Breast and colorectal TCGA(BRCA and CRC and BRIGHT Overfitting problems and a lack of \nannotated data\nPrompt Tuning MIL transformer 93.47 /–/–\nHAG-MIL [68] Breast, Gastric, Lung, etc. CAMELYON16, IMGC, TCGA-RCC and \nNSCLC\nThe difficulties in locating the most \ndiscriminative patches\nHierarchical attention-guided MIL \ntransformer framework\n91.40 / 89.40 / 98.20\nMI-Zero [69] Breast, cell, and lung TCGA (BRCA, NSCLC and RCC), etc. Computational issues and a scarcity of \nlarge-scale publicly available datasets\nTransformer-based visual language \npre-trained MI zero-shot transfer\n70.20 /–/ –\nHAG-MIL [69] Breast, cell, and lung TCGA (BRCA, NSCLC and RCC), etc. Computational issues and a scarcity of \nlarge-scale publicly available datasets\nTransformer-based visual language \npre-trained MI zero-shot transfer\n70.20 /–/ –\nMEGT [47] Kidney and breast TCGA-RCC and CAMELYON16 The problem of learning multi-scale \nimage representation from large \nimages like gigapixel WSIs\nMulti-scale efficient graph transformer-\nbased network\n96.91 / 96.26 / 97.30\nMSPT [70] Breast, and lung TCGA-NSCLC and CAMELYON16 The problem of uneven representation \nbetween the negative and positive \ninstances in bags\nMulti-scale prototypical transformer-\nbased network\n95.36 /–/ 98.69\nGLAMIL [71] Breast, lung, and kidney TCGA(RCC and NSCLC) and CAME-\nLYON16\nOverfitting, WSI-level feature aggrega-\ntion, and imbalanced data challenges\nLocal-to-global spatial learning 95.01 /–/ 99.26\nPage 16 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nFor breast cancer histopathological image classification, DCET-Net  [72] proposed a \ndual-stream convolution-expanded transformer architecture; Breast-Net  [51] explores \nthe ability of ensemble learning techniques using four Swin transformer architectures; \nHATNet  [52] uses end-to-end vision transformers with a self-attention mechanism; \nScoreNet  [16] developed an efficient transformer-based architecture that integrates a \ncoarse-grained global attention framework with a fine-grained local attention mecha -\nnism framework; LGVIT  [73] built a local–global ViT model by introducing a new \nlocal–global MHSA mechanism and a ghost geed-forward network block into the net -\nwork; dMIL-transformer [53] developed a two-stage double max–min multiple-instance \nlearning (MIL) transformer architecture that combines both the spatial and morphologi-\ncal information of the cancer regions. Other than breast cancer classification, transform-\ners have also been applied to other histopathological image cancer classification tasks, \nsuch as bone cancer classification (NRCA-FCFL [74]), brain cancer classification (ViT-\nWSI [17], ASI-DBNet [54], Ding et al. [55]), colorectal cancer classification (MIST [75], \nDT-DSMIL [56]), gastric cancer classification (IMGL-VTNet [57]), kidney subtype clas -\nsification (i-ViT [59], tRNAsformer [58]), thymoma or thymic carcinoma classification \n(MC-ViT [76]), lung cancer classification (GTP [46], FDTrans [60]), skin cancer classi -\nfication (Wang  et al.  [45]), and thyroid cancer classification (Wang  et al.  [77], PyT2T-\nViT [41], Wang et al. [78]) using different transformer-based architectures. Furthermore, \nother transformer models such as Transmil [65], KAT [61], ViT-based unsupervised con-\ntrastive learning architecture  [79], DecT  [66], StoHisNet  [80], CWC-transformer  [63], \nLA-MIL  [44], SETMIL  [81], Prompt-MIL  [67], GLAMIL  [67], MaskHIT  [82], HAG-\nMIL [68], MEGT [47], MSPT [70], and HistPathGPT [69] have also been evaluated on \nmore than one tissue type, such as liver, prostate, breast, brain, gastric, kidney, lung, \ncolorectal, and so on, for histopathological image classification using different trans -\nformer approaches. As shown in Fig.  9, GTP [46] introduced a graph-based transformer \narchitecture that combines a vision transformer for processing histopathological images \nand a graph-based representation of a WSI for disease grade prediction. Ding et al. [55] \nbuilt an improved ViT-based architecture by introducing a wavelet position embedding \nframework into the network to reduce the aliasing phenomenon in histopathological \nfeatures brought about by smooth discontinuous feature information and downsampling \noperations. CWC-transformer [63] presents a two-stage network module that success -\nfully addresses the feature extraction and spatial information loss problems in classifying \nFig. 9 Some examples of SOTA transformer architectures for histopathological image classification\nPage 17 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nWSIs. DT-DSMIL  [56] proposed a weakly supervised transformer architecture that is \nbased on MIL to do away with the time-consuming and labor-intensive manual annota -\ntions and also to handle gigapixel images at once.\nTo this end, structural improvements, newly built transformer architectures, CNN \nbackbones, pre-training, multiple-instance learning, and ensembling learning tech -\nniques are among the numerous innovations included in these transformer architectures \nfor a wide range of tasks. As listed in Table  1, even though pure transformers, trans -\nformers with graphs, and hybrid transformers perform exceptionally well in a number \nof papers surveyed, such as breast and lung cancer classification, further improvement \nis still required in future research. On the whole, we therefore summarize the trans -\nformer applications for histopathological image classification as follows: firstly, trans -\nformer architectures have obtained equal or superior results in many classification \ntasks in comparison with CNN-based models. Secondly, transformer architectures are \nsomewhat limited in their application, particularly in the field of histopathological imag-\ning, because of their desire for extensive annotated datasets. However, an alternative \napproach to resolving this challenge could be pre-training. Thirdly, it is computation -\nally expensive to train transformer models using gigapixel images. Therefore, in order \nto boost their performance, it is crucial to lower the computational cost of the model \nand create lightweight architectures. Fourthly, most of the current transformer-based \narchitectures focus on 2D histopathological imaging. With the increasing application \nof transformers in histopathological image classification and prediction, we believe that \nmore work will be put towards building 3D transformer models. Finally, the increasing \npopularity of hybrid transformers has recently gathered so much attention, as they have \ngained from both sides of transformers and conventional networks such as CNN and \nGNN.\nHistopathological image segmentation\nSemantic segmentation of tumor regions is a crucial task in histopathological image \nanalysis. During segmentation, a region of a whole slide image (WSI) is used as input, \nand the model then segments the region using predetermined features. Despite recent \ndevelopments in deep learning over the years, it was still a crucial and difficult task \nfor researchers to segment the region of interest or cancerous region of histopatho -\nlogical images until the advent of vision transformers. Nowadays, transformer-based \napproaches have been used to solve a number of segmentation challenges, such as colon \ncancer segmentation  [83], multi-organ nucleus segmentation  [2], and nuclei segmen -\ntation  [15, 50, 84, 85]. Some outstanding SOTA works are tabulated and detailed in \nTable 2, along with their associated network type, tissue type, dataset, challenge, high -\nlight, etc. Figure 10 shows some examples of SOTA transformer architectures developed \nfor histopathological image segmentation.\nThe U-shaped CNN-based methods, often known as UNet  [38], have obtained \nremarkable success in a number of histopathological image segmentation challenges. \nBesides, UNets are constrained in modeling long-term dependencies because of the \nconvolutional layers present in them. Hence, in order to solve this challenge, researchers \nhave made tremendous efforts over the years to develop high-performance hybrid trans-\nformers integrated with the UNet backbone. One of the most logical ways of inserting a \nPage 18 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nTable 2 Transformer applications in histopathological image segmentation\nMethod Tissue Dataset Challenge Highlight DSC / IoU / F1 \n(%)\nSwin-MIL [83] Intestine Custom Image annota-\ntion and lack of \nrelated informa-\ntion between \ninstances\nTransformer-\nbased weakly \nsupervised \napproach\n–/–/ 99.90\nMCTrans [84] Cell Pannuke Inability of CNN-\nbased methods \nto model long-\nterm dependen-\ncies\nMulti-compound \ntransformer with \nCNN\n68.90/– /–\nTSHVNet [85] Cell CoNSeP and \nPannuke\nDifficulties in \ndifferentiating \nvarious classes of \nnuclei and sepa-\nrating nuclear \ninstances with \nhigh clustering,\nIntegration of \nmultiatten-\ntion modules \n(transformer and \nSimAM)\n85.6 /– /82.00\nDiao et al. [86] Colon NPC2020 Insufficient \nglobal context \nencoding\nTransformer-\nbased network \nusing TransUNet\n83.30/73.00 /–\nDS-TransU-\nNet [15]\nColon GlaS Ignoring the \npixel-level intrin-\nsic structural \nfeatures inside \neach patch\nDual Swin trans-\nformer U-Net \nwith standard \nU-shaped arch\n87.19/78.45/–\nTransAttU-\nnet  [87]\nColon GlaS Modeling long-\nrange contextual \ndependencies \nand Computa-\ntional costs\nTransformer \nwith Multi-level \nAttention-guided \nU-Net\n89.11 / 81.13 /–\nATTransUNet  [2] Colon GlaS and \nMoNuSeg\nHeavy com-\nputational \nburden of paired \nattention mod-\neling between \nredundant visual \ntokens\nA transformer-\nenhanced hybrid \narchitecture \nbased on the \nadaptive token\n89.63 / 82.55 /–\nHiTrans [88] Liver PAIP 2019 The inherent \nheterogeneity of \nhepatocellular \ncarcinoma\nA hierarchical \ntransformer \nencoder-based \nnetwork\n–/ / 75.13\nTransWS [40] Colon and breast GlaS and Came-\nlyon16\nhighlighting \ntarget regions \nroughly, \nsub-optimal \nsolution and low \nefficiency\nTransformer-\nbased weakly \nsupervised \nlearning\n– /–/ 85.20\nTransNuSS  [50] Colon and breast TNBC and \nMoNuSeg\nThe challenges \nof pre-training \nnuclei segmenta-\ntion models with \nImageNet due \nto morphologi-\ncal and textural \ndifferences\nSelf-supervised \nlearning incorpo-\nrated with vision \ntransformer \nmodel\n83.07 / 68.72 /–\nNST [89] Liver, Breast, \nColon, etc.\nGCNS and \nMoNuSAC 2020\nThe staining of \nWSI sections is \nnot uniform and \nnuclei having dif-\nferent sizes and \nshapes\nA gastrointestinal \ntransformer-\nbased network\n79.60 / 66.30 /–\nPage 19 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \ntransformer block into the U-shaped network is to place the entire transformer architec-\nture between the encoder and decoder blocks so as to create long-range dependencies \nbetween high-level vision generalizations, as shown in Fig.  10. Some studies place the \nentire transformer architecture in the encoder part, while others place it in the decoder \npart. Methods such as TransNuSS  [50], SwinCup  [48], Diao  et al.  [86], DS-TransU -\nNet [15], HiTrans [88], and DHUnet [49] [see Fig.  10(b)] are some examples of trans -\nformer-based U-shaped networks developed for histopathological image segmentation. \nIn contrast to the various approaches mentioned above that incorporate transformer \nand U-shaped architectures within a single inference pathway, other studies looked into \nnew ways of bridging transformers and CNNs for more accurate and robust segmenta -\ntion. Although transformer-based architectures demonstrate the superiority of modeling \nlong-range contextual information, their inability to capture local features still poses a \nlot of problems. Rather than cascading the transformer and convolution blocks, many \nstudies recommend using the vision transformer and CNN as encoders that both accept \nhistopathological images as input. After that, the embedded features are combined to \nlink with the decoder. This approach benefits from simultaneously learning local and \nTable 2 (continued)\nMethod Tissue Dataset Challenge Highlight DSC / IoU / F1 \n(%)\nMedT [90] Colon and cell GlaS and \nMoNuSeg\nInherent induc-\ntive biases in \nCNNs and insuffi-\nciently annotated \ndatasets\nGated axial-\nattention \ntransformer-\nbased model\n–/ 69.61 / 81.02\nSwinCup [48] Colon and colo-\nrectal\nGlaS Inability of CNNs \nto model global \ncontext\nCascaded Swin \ntransformer-\nbased network\n–/–/ 92.00\nDHUnet [49] Breast, liver, and \nlung\nBCSS, WSSS-\n4LUAD, etc.\nInability of the \ntransformer \nmodel to capture \nfine-grained \ndetails in patho-\nlogical images\nDual-branch \nhierarchical \nglobal–local \nfusion network\n93.07 / 87.04 /–\nFig. 10 Some examples of SOTA transformer architectures for histopathological image segmentation\nPage 20 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nglobal information and then stacking representations sequentially  [89]. Other than \nusing U-shaped transformer-based architectures, some methods, such as MCTrans [84], \nTransAttUnet [87], MedT [90] and TSHVNet [85] applied multi-scaling techniques for \nhistopathological image segmentation. In addition, pure transformer-based architec -\ntures can also be applied to a variety of histopathological image segmentation tasks. \nWith the exception of the UNet network variations already mentioned, using the trans -\nformer in conjunction with convolution blocks, TransWS [40] introduced a transformer-\nbased weakly supervised learning method without convolution layers. The proposed \napproach was basically used to address the issues of low efficiency and sub-optimal \nsolutions as well as the challenge of producing a high-quality class activation map that \nidentifies the precise and integral target, leading to insufficient activation and undefined \nboundaries. Qian et al. [83] built a weakly supervised approach that inserts the trans -\nformer architecture into the MIL module to encode long-term or global dependencies. \nFigure  11 shows some visual segmentation results obtained from various transformer \nnetworks against the popular Unet architecture on different histopathological image \nsegmentation datasets.\nIn summary, from the research papers surveyed in this section, we can conclude that \nthe histopathological image segmentation domain has been slightly impacted by trans -\nformer-based architectures since the inception of the first ViT architecture, as shown in \nTable 2. In comparison to other medical imaging fields, we strongly believe that this is \ndue to a lack of annotated histopathological segmentation datasets and the high com -\nputational cost of training WSIs. As stated above, the high computational cost involved \nwith mining features at multiple intensities obstructs the applicability of multi-scale \nnetworks in histopathological image segmentation tasks. These multi-scale networks \nmake use of processing input image information at several levels and obtain significantly \nbetter performance than single-scale networks. As a result, building efficient trans -\nformer-based models for multi-scale processing needs better attention. Besides, most of \nthe recently developed transformer-based architectures are pre-trained mainly on the \nFig. 11 Examples of segmentation results of popular Unet architecture [38] and transformer-based models \n(ATTransUNet [2], DS-TransUNet [15], MedT [90], Diao et al. [86], TransAttUnet  [87], and NST [89]) on different \nhistopathological datasets\nPage 21 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nImageNet dataset for various downstream tasks. Hence, this technique is sub-optimal \nbecause of the huge domain gap between histopathological images and natural images. \nRecent ViT-based methods have largely focused on 2D histopathological image segmen -\ntation; therefore, building customized architectural frameworks by integrating tempo -\nral features for robust high-dimensional and high-resolution segmentation of WSI has \nnot been fully investigated. Furthermore, with the development of ViT-based methods, \nwe discovered that there is an urgent need to gather more varied and demanding his -\ntopathological image datasets. Although challenging and diverse datasets are also very \nimportant for evaluating the performance of transformers in other clinical settings, they \nare especially important for histopathological image segmentation because of the major \ninflux of transformer-based approaches in this domain. To this end, we anticipate that \nthese datasets will be crucial in determining the viability of ViT-based models for histo -\npathological image segmentation.\nHistopathological image detection and localization\nThe word “detection” has different meanings across many domains. As we mentioned \nearlier, it is frequently referred to as disease identification or diagnosis in clinical \ndomains, whereas in the technical field, it simply refers to determining whether lesions \nor diseases are present. However, disease detection in histopathological images is often \nreferred to as a technique for locating instances of diseases in a specific image and iden -\ntifying the potential region of a tumor, such as mitosis detection from breast cancer \nimages, and is generally an important aspect of disease identification. Disease diagno -\nsis is one of the most challenging tasks for clinicians, so it is important to have a reli -\nable CAD technique that can serve as a second observer and potentially speed up the \ndiagnosis process. Following the success of CNN-based methods in histopathological \nimage detection and localization, there have been a few attempts recently to improve \nperformance using transformer-based architectures. These techniques are primarily \nbased on the detection transformer (DETR)  [33]. Transformer architectures used for \ndetection tasks involving histopathological images often incorporate CNN blocks, where \nCNNs are mainly used to mine features from images while the transformers are used to \nimprove the mined features for other subsequent tasks. A few outstanding SOTA works \nare tabulated and detailed in Table  3. Figure  12 shows some examples of SOTA trans -\nformer architectures developed for histopathological image detection.\nRecently, Chen et al. [19] proposed a multi-scale ViT-based approach that makes use \nof a position-encoded ViT framework and a CNN with convolutional operation to mine \nglobal and local information. To tackle the large-scale context overflow challenges, Wen-\nkang  et al.  [91] developed a novel transformer-based technique that integrates global \nand local context within an end-to-end module. In addition, Ali et al. [92] introduced a \ntransformed-based CAD system by making use of deep CNN networks based on chan -\nnel boosting techniques. Takagi et al. [18] proposed a ViT-based personalized attention \nmechanism network for gigapixel WSIs with clinical records. Liaqat et al. [95] developed \na channel-boosted hybrid ViT-based network that makes use of transfer learning tech -\nniques to build boosted channels and uses both ViT and CNN models to analyze can -\ncerous images. As shown in Fig.  12, RAMST  [94] makes use of joint region attention \nand a multi-scale transformer network to alleviate the unstable predictions caused by \nPage 22 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nnoisy patches and aggregation techniques in WSIs. YOLOv5-transformer [93] built an \nimproved transformer architecture that integrates transformer into the YOLOv5 model \nfor mitoses detection. Hossain et al. [96], on the other hand, built a region of interest \n(ROI) selection ViT-based architecture to speed up the analysis of histopathological \nimages and improve the detection accuracy of cancerous regions. \nIn summary, the number of new ViT-based architectures for the histopathological \nimage detection and localization challenge task, as presented in Table  3 is lower than \nthat of the classification task as reported in this survey paper. This is in comparison to \nthe previous CNN-based methods that were promptly built for histopathological and \nTable 3 Transformer applications in histopathological image detection and localization\nMethods Tissue Dataset Challenge Highlight ACC / F1 (%)\nGasHis-trans-\nformer [19]\nStomach (Gastric) HE-GHI-DS Inability of CNN \nmodels to handle \nglobal informa-\ntion well\nGasHis-trans-\nformer and \nLW-GasHis-trans-\nformer\n97.97 / 97.97\nPathTR [91] Breast CAMELYON16 Neglecting the \nintrinsic WSI \nglobal correla-\ntions among the \npatches\nContext-Aware \nMemory ViT with \na CNN Backbone\n98.91 /–\nPVTCB-Lymph-\nDet [92]\nColon, breast and \nprostate\nLYSTO Detecting \nlymphocytes \nautomatically due \nto the presence \nof artifacts and \nmorphological \nvariations\nPyramid ViT-\nbased network \nand convolu-\ntion attention \nmechanism with \nResNet-50\n–/ 88.92\nYOLOv5-trans-\nformer [93]\nBreast, Colon, etc. Custom Accurate mitoses \ndetection and \nmorphological \nvariations\nImproved \nYOLOv5 trans-\nformer-based \narchitecture\n–/ 77.00\nRAMST [94] Stomach and \ncolorectal\nTCGA (CRC and \nSTAD)\nUnstable predic-\ntions caused by \nnoisy patches \nand aggregation \ntechniques\nJoint regional \nattention and \nmulti-scale trans-\nformer network\n–\nCB-HVTNet [95] Colorectal, breast, \netc.\nLYSTO and \nNuClick\nInsufficient fea-\nture representa-\ntions\nChannel-boosted \nhybrid ViT net-\nwork\n–/ 80.00\nHossain et al. [96] Breast, etc. TCGA and \nCustom\nViT-based net-\nwork\nROI selection ViT-\nbased network\n96.10 /–\nPersAM [18] Lymph Custom Attention region \nestimation in \ndigital pathologi-\ncal images\nPersonalized \nattention mecha-\nnism ViT network\n83.13 /–\nFig. 12 Some examples of SOTA transformer architectures for histopathological image detection\nPage 23 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nother clinical detection tasks. Some recent medical research papers demonstrate that the \ngeneric, class-agnostic detection system of multi-modal ViT-based models pre-trained \non other images rather than medical images performs horribly on histopathological and \nother clinical datasets. Hence, evaluating the performance of multi-modal ViT-based \narchitectures by pre-training them on modality-specific histopathological WSI datasets \nis a good research direction to investigate in the future.\nHistopathological image survival analysis and prediction\nSurvival analysis and prediction is an arduous regression problem that aims to predict \nthe time to an event, for example, the diagnosis of a disease or the relative risk of cancer \ndeath. Over the years, several techniques have been developed for survival analysis and \nprediction using histopathological WSIs. However, these techniques can be classified \ninto two distinct classes: ROI-based and WSI-based approaches, respectively. Due to \nthe high cost of computational resources, the majority of the existing literature has con -\ncentrated on regions of interest (tiles) chosen by pathologists from WSIs. Nowadays, a \nnumber of methods for histopathological image analysis have been proposed for a wide \nrange of downstream tasks, using the detailed and dense annotations on WSIs. Recently, \ntransformer-based architectures have demonstrated outstanding performance in pre -\ndicting survival rates. A few outstanding SOTA works are summarized and detailed in \nTable 4. Fig. 13 shows some examples of SOTA transformer architectures developed for \nhistopathological image survival analysis and prediction.\nTransformer-based methods such as HiMT [100], MCAT [26], PG-TFNet [98], Trans-\nSurV [97], and SURVPATH [101] combine genomic data and histopathological images \nfor survival analysis and prediction. As shown in Fig.  13, MCAT [26] introduced a mul -\ntimodal co-attention Transformer network to learn an interpretable, dense co-atten -\ntion mapping among genomic features and WSIs constructed in an embedding space. \nTransSurV [97] makes use of a Transformer-based multi-modal feature fusion network \nto extract useful predictive features from the multi-modal data. HiMT [100] introduced \na hierarchical transformer-based network to mine the instant-level tile features at ran -\ndom from WSIs with varying magnification levels. AMIGO [3] created a multi-modal \ngraph transformer architecture that predicts patient survival based on multi-modal his -\ntopathological images and shared related data. In addition, Huang et al. [25] designed a \ntransformer technique for survival prediction based on the combination of tile features \nvia an self-supervised learning (SSL) approach and a transformer. Shen et al. [99] make \nuse of an explainable survival analysis framework coupled with a convolution-involved \nViT-based network. More recently, Jaume  et al.  [101] introduced a memory-efficient \nmultimodal-based transformer architecture that combines patch tokens and transcrip -\ntomics for patient survival prediction. Wang et al. [102] developed a pattern-perceptive \nsurvival transformer-based network that can statistically interpret the predictions as well \nas directly quantify the important histopathological patterns. HMCAT [104] introduced \na hierarchical multi-modal co-attention Transformer-based network that addresses \nthe challenges of the large size of histopathological WSIs and the significant dispar -\nity between the spatial scales of radiology images and histopathological WSIs. Shao et \nal. [103] make use of a hierarchical ViT-based architecture to completely investigate the \ncontextual, spatial, and hierarchical relationships in the patient-level bag. \nPage 24 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nTable 4 Transformer applications in histopathological image survival analysis and prediction\nMethod Tissue Dataset Challenge Highlight C-index (%)\nTransSurv [97] Colorectal TCGA-CRC and \nNCT-CRC-HE\nInability of the \nprevious models \nto extract useful \npredictive features \nfrom the multi-\nmodal data\nTransformer-\nbased multi-\nmodal feature \nfusion network\n82.20\nPG-TFNet [98] Colorectal TCGA-CRC Inability to make \nuse of the power-\nful representation \nlearning capabili-\nties of the neural \nnetworks\nTransformer-\nbased multi-\nmodal feature \nfusion network\n81.60\nESAT [99] Lung NLST and \nCHCAMS\nUsing a pre-\nselected subset of \nmain patches or \npatch clusters as \ninput instead of \nusing the entire \nWSIs\nMake use of the \nViT backbone \nwith convolution \noperations.\n73.00\nMCAT [26] Bladder, Breast, \nLung, Uterine\nBLCA, UCEC, \nBRCA, BMLGG, \nLUAD\nComputational \ncomplexity \nand large data \nheterogeneity gap \nbetween genom-\nics and WSIs\nMultimodal \nCo-Attention \nTransformer for \nSurvival Prediction\n65.30\nHiMT [100] Bladder, Breast, \nLung, Brain, etc.\nBLCA, BRCA, \nUCEC, LUAD, LGG, \netc.\nHigh compu-\ntational cost of \nextracting patches \nfrom WSIs, which \nresults in a large \nbag size\nHierarchical-based \nmulti-modal \nTransformer \nframework\n67.30\nMaskHIT [82] Breast, Lung, etc. TCGA Huge number of \nnetwork param-\neters and insuffi-\ncient labeled data\nMasked pre-\ntraining of Trans-\nformers\n61.20\nSURVPATH  [101] Breast, Bladder, \nStomach, etc.\nTCGA Capturing dense \nmultimodal \ninteractions \nbetween different \nmodalities\nMemory-efficient \nmultimodal Trans-\nformer\n62.90\nSurformer  [102] Bladder, Breast, \nLung, etc.\nTCGA (BLCA, \nBRCA, LUAD, etc.)\nWeak interpret-\nability problems \nof the previous \ncomputational \npathology model\nPattern-per-\nceptive survival \nTransformer-\nbased Network\n68.70\nHVTSurv [103] Bladder, Breast, \nLung, etc.\nTCGA (BLCA, \nBRCA, LUAD, etc.)\nThe challenges \nof exploring \ncontextual, spatial, \nand hierarchical \ninteraction in the \npatient-level bag\nHierarchical ViT-\nbased architec-\nture\n63.40\nHMCAT [104] Low Grade Glioma TCGA-GBMLGG The significant \ndisparity between \nthe spatial scales \nof radiology \nimages and WSIs\nHierarchical \nmultimodal \nco-attention \ntransformer-based \nnetwork\n79.60\nAMIGO [3] Ovarian and \nbladder\nInUIT and MIBC ignoring specific \ndetails regarding \nthe individual cells \nin a tile image\nSparse multi-\nmodal graph \nTransformer-\nbased network\n61.00\nPage 25 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nIn summary, the number of new ViT-based architectures for the histopathologi -\ncal image survival and prediction task is lower compared to that of the classification \ntasks reported in this paper. This is in comparison to the previous CNN-based meth -\nods that were promptly built for histopathological and other clinical survival and pre -\ndiction tasks. It is also important to note that despite the fact that there are several \nsurvey papers covering the applications of CNNs in histopathological image analy -\nsis [4 –6], none of these studies have recently covered the use of Transformer archi -\ntectures in survival analysis and prediction, despite the outstanding performance that \nthese architectures have demonstrated over the last few years. We anticipate that this \npart will be a useful tool for researchers in this domain. In addition, we will briefly \ndiscuss some problems with transformer-based architectures for survival analysis and \nprediction below, along with some interesting future prospects. \nAs shown in Table.  4, transformer-based survival analysis and prediction architec -\ntures mostly rely on the concordance index metric (i.e., c-index) to evaluate the per -\nformance of the networks, which sometimes fails to accurately reflect clinical efficacy. \nSince the researchers currently depend only on the c-index metric as an evaluation \nmetric, we believe that further effort is needed to develop more accurate clinical eval -\nuation indicator to speed up the adoption of transformer-based survival analysis and \nprediction in clinical domains. Again, some of the transformer-based architectures \nTable 4 (continued)\nMethod Tissue Dataset Challenge Highlight C-index (%)\nSeTranSurv  [25] Breast, Lung, \nOvarian\nOV, LUSC, and \nBRCA \nIgnoring the \nimportant role of \nspatial information \nin patches and \nthe correlation \nbetween patches \nand WSIs\nIntegration of \npatch fea-\ntures through \nself-supervised \nlearning and \nTransformer\n70.50\nFig. 13 Some examples of SOTA transformer architectures for histopathological image survival analysis and \nprediction\nPage 26 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nsurveyed in this paper make use of histopathological images and genomic records \nfor survival analysis and prediction. Hence, generating reports from other clinical or \nmedical domains has its own challenges due to their unique nature and varied fea -\ntures. Besides, a few histopathological datasets, like TCGA , 1 are available that consist \nof different cancer types together with clinical records. This dataset has the potential \nto be a valuable baseline for evaluating the performance of future multimodal trans -\nformer-based architectures for survival analysis and prediction. We suggest that in \nthe future, transformer-based architectures tailored to particular tissues to predict \npatient survival should be investigated, with a focus on building challenging and var -\nied datasets of different tissues.\nHistopathological image representation\nDue to memory and processing time constraints, histopathological images are often \ndivided into smaller tiles (such as 256 × 256 pixels), and features are then mined concur-\nrently from each tile. The representation of a histopathological WSI using information \nfrom multiple tiles, however, is a developing field of study with limited results that have \nbeen published, particularly in the context of clinical prediction and prognosis. Recently, \nseveral studies have been developed for learning multi-scale representations of images \nusing transformer-based models, which can also be employed in convolutional pipelines \nin order to construct global representations of images. A few outstanding SOTA works \nare tabulated and summarized in Table  5. Figure  14 shows an example of the SOTA \ntransformer architecture developed for histopathological image representation.\nIn order to learn high-resolution image representations from histopathological images, \nHIPT [22], made use of a ViT-based hierarchical image pyramid network, CD-Net [105] \nproposed a Transformer-based pyramidal context-detail network, and H2T  [108] \nemployed a handcrafted histological Transformer. As presented in Fig.  14, HIPT  [22] \nuses two levels of self-supervised learning to take advantage of the natural hierarchical \nstructure present in histopathological WSIs. The proposed architecture was pre-trained \nacross 33 different cancer types by making use of 10,678 histopathological slides, 104 M \n256 × 256 images, and 408,218 4096 × 4096 images. In addition, DSCA [106] built a dual-\nstream Transformer architecture with cross-attention to address the challenges of the \nunseen semantical disparity in multi-resolution feature fusion and the high computa -\ntional complexity of histopathological WSI visual representation. ViT-AMCNet  [20] \nmakes use of an end-to-end transformer-based network with adaptive model fusion and \na multi-objective optimization technique to address the challenges of poor interpretabil-\nity and weak inductive bias ability for the laryngeal tumor grading task. Chan et al. [107] \nbuilt a heterogeneous-graph edge attribute transformer-based network that can benefit \nfrom both node and edge heterogeneity. \nIn summary, since the number of publications and transformer applications in \nhistopathology image representation is currently limited, as shown in Table.  5, it \nis challenging to draw any conclusions at this time. However, as the current trans -\nformer-based architectures give better results on histopathological image represen -\ntation tasks, we anticipate further development in this domain in the near future.\n1 https:// www. cancer. gov/ tcga.\nPage 27 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nTable 5 Transformer applications in histopathological image representation\nMethod Tissue Dataset Challenge Highlight ACC / AUC (%)\nCD-Net [105] Breast, Lung TCGA (LUAD and LUSC) Inability to leverage the rich multi-resolu-\ntion information\nTransformer-based pyramidal context-\ndetail network\n91.10 / 95.80\nDSCA [106] Lung, breast and brain NLST, TCGA (BRCA and LGG) High computational complexity and \nunnoticed semantic gap in multi-resolu-\ntion feature fusion\nA dual-stream Transformer network with \ncross-attention framework\n-\nHIPT [22] Breast, lung, stomach, cell IDC, LUAD, CCRCC, PRCC, CHRCC, and \nSTAD\nThe structure of phenotypes in tumors \nand learning a good representation of a \nWSI\nHierarchical image pyramid Transformer \nwith two levels of self-supervised learning\n–/ 98.00\nHEAT [107] Colon, breast and esophageal CAMELYON16, TCGA (COAD, BRCA, and \nESCA)\nThe challenges of extracting diverse inter-\nactions between various cell types\nHeterogeneous-graph edge attribute \nTransformer-based network\n99.90 / 99.90\nH2T [21] Lung, breast and kidney TCGA-NSCLC, CPTAC-LUAD, etc., BRCA, \nRCC and ACDC\nHigh discordance on how a tissue sample \nand higher predictive power that comes at \nthe cost of interpretability\nHandcrafted histological Transformer-\nbased network for unsupervised represen-\ntation WSIs\n-\nViT-AMCNet  [20] Laryngeal, breast, brain Laryngeal cancer, breast cancer, brain \ncancer\nProblems of poor transformer generaliza-\ntion bias and poor AMC interpretive ability\nViT-based network with adaptive model \nfusion and multi-objective optimization\n95.14 / 96.17\nPage 28 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nTransformer applications in other histopathological imaging tasks\nThis section briefly discusses the use of transformers in other histopathological \ngigapixel image domains, such as cross-modal retrieval analysis, image generation, \nimage synthesis, and so on. Dingyi  et al.  [23] introduced a cross-modal retrieval \ndual-transformer architecture that can simultaneously execute four retrieval tasks \nat a time for the histopathology dataset across diagnosis reports and WSIs, respec -\ntively. MedViTGAN [24] developed a conditional GAN transformer-based network \nthat can aid researchers in producing synthetic histopathological images for other \ndownstream tasks in an end-to-end approach. In addition, a ViT-based network to \nenhance the use of contextual information found in histopathological images was \nproposed in  [109]. The network is made up of two variations of ViT-based archi -\ntecture (PREViT and ClusterViT) to improve the local context of the tissue patch \nfeatures by adding prior knowledge to the network. Xu  et al.  [110] developed \na Transformer architecture for high-quality histopathological image synthesis \nthat combines ViT and diffusion autoencoders. The authors introduced a condi -\ntional denoising diffusion implicit model (DDIM) into the architecture, which was \nimproved by integrating a ViT model as a semantic encoder, allowing it to compre -\nhensively encode sophisticated phenotypic layouts particular to histopathology. \nTo this end, since the current transformer-based architectures give better results \non other histopathological image tasks, we anticipate further development in these \nfields in the near future.\nFig. 14 A schematic illustration of the HIPT [22] transformer architecture for histopathological image \nrepresentation\nPage 29 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nDiscussion\nTransformer architectures have been effectively used in a wide range of clinical tasks, \nincluding histopathological image analysis, as demonstrated in Tables  1, 2, 3, 4, and  5. \nDespite their strong performance due to the attention mechanisms incorporated into \nthem, there are a number of challenges that could prevent transformer models from \nperforming well, especially in real-world clinical applications. One of these challenges \nis the lack of supervised clinical information provided by experts in order to develop a \nsupervised Transformer architecture, which is critical in training a transformer model. \nTherefore, in this section, we will discuss the recent research directions in addressing \nthis challenge using the SOTA transformer models in different learning settings and also \ncompare transformers with CNNs based on recently published papers.\nDifferent learning settings with transformer architectures\nIn this section, we present and discuss different learning settings that are often used with \ntransformer architectures for histopathological image analysis, including weakly super -\nvised learning, self-supervised learning (SSL), multi-task learning (MTL), and multi-\nmodal learning (MML). \n(i) Weakly supervised learning for histopathological imaging: The creation of a weakly \nsupervised transformer-based architecture addresses the urgent need for histopatho -\nlogical image annotation, which is generally labor-intensive and also time-consuming. \nOn the other hand, weakly supervised learning is an arduous task where a huge number \nof instances occur within each bag while only a slide label is provided. Multiple instance \nlearning (MIL), which is also a subset of weakly supervised learning, has shown better \nresults in a number of downstream tasks in previous studies. Despite recent improve -\nments, they still have some drawbacks. One is that they concentrate on the regions that \nare easily distinguished as positive for the diagnosis while neglecting the positives that \nmake up a small proportion of the WSI. For the purpose of obtaining more discrimina -\ntive features, several studies have developed a number of weakly supervised approaches \nfor histopathological image analysis, including segmentation  [40, 83], and classifica -\ntion [2, 17, 45, 56, 111] tasks. Besides, the weakly supervised ViT-based MIL technique \nwas further adopted for colorectal cancer lymph node metastasis (LNM) prediction [56] \nand can be used to reduce the doctor’s workload and accelerate diagnostic operations. \n(ii) Self-supervised learning for histopathological imaging: Supervised learning tech -\nniques heavily rely on pathologists to manually annotate several regions on WSIs before \nthey can be used to train any network. However, this approach often requires a sig -\nnificant amount of annotated datasets for transformer architectures to be successfully \ntrained in many computer vision tasks, and some of these datasets are uncommon in \nreal clinical settings. Therefore, such problems were addressed by self-supervised learn -\ning (SSL) techniques. The primary objective of SSL is to enhance the performance of \ndifferent downstream tasks by conveying knowledge from the associated unsupervised \nupstream task and pre-training the network by making use of its self-contained features \nin the untagged data. The standard method for training SSL ViT architectures involves \npre-training the architecture primarily on ImageNet and then fine-tuning it on the tar -\ngeted histopathological image dataset. This will generally improve the performance of \nPage 30 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \ntransformers in contrast to CNNs and allow for the attainment of SOTA accuracy. A \nsignificant amount of studies have attempted to apply the SSL approach for a variety of \nobjectives in histopathological image analysis, including representation [22], classifica -\ntion [64, 75], and survival analysis and prediction [25] tasks. \n(iii) Multi-task learning for histopathological imaging: Multi-task learning (MTL) is a \ntechnique in which a shared network learns multiple tasks at the same time. It has shown \nbetter performance than single-task learning techniques in increasing the learning \ncapabilities of a deep learning architecture. Such techniques offer many benefits, such \nas preventing overfitting through the use of shared representations, speeding up learn -\ning by utilizing auxiliary information, and increasing data efficiency. However, building \ntransformer architectures with multiple tasks assists in increasing the network gener -\nalization ability, which is crucial in histopathological image analysis. Recently, MTL has \nbeen commonly applied to transformer-based networks to tackle various downstream \ntasks in computer vision, and a commonly used technique is to combine a segmentation \nand classification task into a single network  [40, 85]. In addition, Ali  et al.  [92] intro -\nduced a transformed-based CAD system by making use of deep CNN networks based \non channel boosting techniques to improve the learning capability of the entire network. \nWang et al. [45] built a weakly supervised transformer architecture by integrating graph \nneural networks and transformers for basal cell carcinoma classification and detection. \n(iv) Multi-modal learning for histopathological imaging: Multi-modal learning (MML) \nis an approach that aims to build and develop models that can integrate data from mul -\ntiple modalities, such as image data, genomic data, and clinical records. Over the years, \nresearch advancements in MML have grown rapidly in a number of computer vision \ntasks, particularly histopathological image analysis. It involves utilizing a single model \nto learn representations from various modalities. Using data from multiple modality \nsources, on the other hand, provides additional clues for disease diagnosis. Several stud -\nies have investigated the integration of genomic data and histopathological images for \nsurvival analysis and prediction using transformer-based architectures  [26, 100, 101]. \nTakagi et al. [18] proposed a ViT-based personalized attention mechanism network for \nhistopathological images with clinical records. AMIGO [3] created a multi-modal graph \ntransformer architecture that predicts patient survival based on multi-modal histo -\npathological images and shared related data. Cai et al. [60] created a frequency-domain \ntransformer architecture that integrates frequency and spatial domains for histopatho -\nlogical lung cancer image analysis and subtype determination. \nIn summary, transformer architecture is regarded as a promising technique for fusing \ncomputer vision and NLP tasks. However, there is still a need to develop more accu -\nrate and robust CAD systems for real-time clinical settings where multiple data types, \nsuch as imaging, clinical, and laboratory records, are regarded as multiple sources of \ninformation.\nComparison of transformers and CNNs on different downstream tasks\nOver the years, CNN-based architectures have been dominant in many research fields \nprior to the development of vision transformers (ViTs), including the field of histo -\npathological image analysis. Many studies have also been conducted in this domain to \nascertain whether CNN-based architectures can still work on ViT-based architectures. \nPage 31 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nRecently, ViT-based architectures have been shown to be capable of producing better \nresults than CNNs, especially when pre-trained on a large number of datasets. In com -\nparison to CNNs, ViTs have a weaker inductive bias, and as a result, they allow for more \nflexible feature detection. The performance comparison between ViTs and CNN-based \nmodels has received tremendous attention, as ViTs have excelled in a number of bench -\nmarks, as shown in Fig. 11. Nguyen et al. [112] comprehensively evaluated six frequently \nused Transformer-based architectures for cancer segmentation. The results obtained, \nwith the exception of Swin-UNet [113], show that Transformer-based architectures typi-\ncally outperform CNN-based techniques because of their capacity to encode global con -\ntext. Besides, this is one of the first studies to systematically compute the performance \nof transformer-based approaches on histopathological image segmentation. For the task \nof tumor detection and tissue type identification in digital pathology WSIs, Deininger et \nal. [114] compared the patch-wise classification result of the ViT DeiT-Tiny to the SOTA \nCNN-based ResNet18 model. Due to the limited number of annotated slide images, the \nauthors further compared the two architectures by pre-training them on a large num -\nber of unlabeled WSIs using SOTA self-supervised techniques. The obtained results \ndemonstrate that the ViT slightly outperformed the ResNet18 for three out of the four \ntissue types investigated in the study for tumor detection, while the ResNet18 architec -\nture slightly outperformed the ViT for the remaining tasks. In addition, Springenberg et \nal.  [115] conducted an extensive evaluation of deep learning architectures for histo -\npathological image classification by comparing Transformers and CNNs, respectively. \nThe study produced concrete architecture recommendations for medical practitioners as \nwell as a generic approach for quantifying architecture quality based on complementary \nconditions that can be applied to future network architectures. \nIn summary, many previous studies and SOTA on histopathological image analysis \nhave not completely shown that transformer-based architectures can outperform CNN-\nbased architectures in all ramifications, especially in few-shot and low-resolution his -\ntopathological image analysis. Thus, developing hybrid architectures with convolutions, \nsimilar to approaches in computer vision, has been adopted in most current research \nworks. In addition, apart from the excellent results achieved in most publications sur -\nveyed in this paper, as demonstrated in Tables 1, 2, 3, 4, and 5, transformer architectures \nare computationally expensive and require a large amount of data for training. Therefore, \nwe anticipate further development in reducing transformer computational complexity in \nthe near future.\nOther challenges and future directions\nWe primarily reviewed the current SOTA Transformer-based methods for histopatho -\nlogical image analysis. There are still a number of open challenges to be addressed in \nthe future, despite the excellent and outstanding results produced. (1) The first chal -\nlenge is the intensiveness of annotations. Transformer-based architectures often need a \nlarge number of annotated datasets and can produce better results when trained on huge \ndatasets, but their performance reduces when data or annotations are limited. To solve \nthis problem, SSL techniques offer better and more interesting solutions. Transformers, \non the other hand, can improve their capacity for representational learning by making \nuse of unlabeled data and proxy tasks like reconstruction and contrastive learning. A \nPage 32 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nsignificant number of studies have applied the self-supervised approach for a variety of \nobjectives in histopathological image analysis [22, 25, 64, 75] and have shown better per-\nformance. Some of these approaches have demonstrated that training networks using \nlarge-scale unlabeled 2D images is advantageous when fine-tuning them with small-\nscale datasets. However, we find that pre-training is computationally expensive, and \nfuture research should focus on simplifying and analyzing the efficiency of the pre-train-\ning model as well as fine-tuning it for small-scale datasets. (2) The second challenge is \nthe scalability of the task. The heterogeneous nature of histopathological images makes \nrepresentational learning very difficult. Studies in the past have mainly concentrated \non resolving specific histological tasks, and transformer architectures perform better at \nlearning heterogeneous tasks, especially when SSL techniques are adopted [22]. Again, \nthe advanced scaling operations also give the transformer-based architectures the capac-\nity to handle multi-domain and multi-scale tasks  [22]. In addition, networks may fit a \nvariety of datasets by scaling up transformer architectures, and researchers can modify \na network at training time to move from a low-data scheme to larger dimensions. (3) \nThe third challenge is the scalability of the data. Most ViT-based architectures, such as \nthe original ViT [8], perform poorly when trained on small-scale datasets because they \nlack inductive bias. However, If there is enough training data, transformer architectures \ncan overcome inductive bias challenges by employing different pre-training techniques. \nBesides, pre-training techniques  [22, 69, 82] have also shown better performance in \nincreasing the generalization ability of transformer architectures for histopathologi -\ncal imaging. Moreover, gathering large-scale datasets in the histopathological imag -\ning domain is sometimes impractical due to time-consuming manual annotations and \npatient privacy concerns. Since gathering large-scale datasets across different imaging \nmodalities still poses a lot of challenges, it is therefore essential to build transformer \narchitectures that are less data-demanding for histopathological imaging applications by \nincorporating inductive bias mechanisms into transformer models, and we hope to see \nfurther research addressing this challenge in the near future. \n(4) The fourth challenge is computational complexity. As shown in the previous sec -\ntions, transformer-based architectures are computationally expensive due to the com -\nputation of the self-attention mechanism, which is usually quadratic to the size of the \ninput image. This issue appears to be less of a problem with natural images, but with \nhistopathological images, it is a significant difficulty. Again, this is because histopatho -\nlogical images such as WSIs come in gigapixels and are larger in size compared to natu -\nral image datasets. Unlike natural images such as ImageNet, which have fewer pixels, \nhistopathological WSIs can be as huge as 150,000 x 150,000 pixels  [22]. Compared \nwith training strategies used for natural imaging models, transformer-based architec -\ntures for histopathological image analysis are typically more compacted and sometimes \ntrained using patched input or even smaller batch sizes. The majority of SOTA trans -\nformer-based methods for histopathological image analysis are either built upon the \nalready-existing transformer networks [15, 48, 51, 75] or make use of CNNs for feature \nextraction before being fed into a transformer  [56, 61, 63, 81, 82]. Moreover, several \nstudies have suggested that Softmax may be circumvented to linearize the computation \nof the self-attention mechanism, although none of these techniques have been used and \napplied to histopathological imaging yet. Therefore, we hope to see further research in \nPage 33 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \nthis particular direction. (5) The fifth challenge is the combination of data from differ -\nent sources. An emerging research area such as imaging genomics has opened up new \npossibilities for cancer detection and prediction. Using data from multiple modality \nsources, on the other hand, provides additional clues for disease diagnosis. Some of the \nTransformer-based architectures surveyed in this paper make use of histopathological \nimages and genomic records for different downstream tasks [100, 101]. However, gen -\nerating reports from other clinical or medical domains has its own challenges due to \ntheir unique nature and varied features. Therefore, how to properly incorporate data \nfrom multiple sources for more accurate disease identification and prediction is another \ninteresting and promising future research direction. (6) The sixth challenge is the black \nbox and its interpretability. Over the years, several studies have been conducted on his -\ntopathological imaging using various deep learning techniques. Deep learning methods \nsuch as CNN sometimes function as black box solutions and are typically more diffi -\ncult to explain. transformers, on the other hand, make use of a self-attention mechanism \nthat imitates some human behaviors but still functions as a black box and is unable to \nreveal how different factors are combined to generate results. Given the importance of \nnetwork interpretability in histopathological image analysis, it is critical to investigate \nthe interpretability of transformer-based architectures. One of the common methods for \nvisualizing Transformer architectures is to calculate relevancy scores from either single \nor shared attention blocks. The MHSA module in transformer architectures establishes \na direct link between tokens, providing an intuitive guide for decision-making. Recently, \nvisual language pre-training  [69] has also been adopted for histopathological imaging, \nand the majority of the WSI-level diagnosis or prediction networks are computed in \na black box, making it impossible for humans to understand which region of the slide \nhas the greatest influence on the final prediction. Hence, in order to make the networks \nmore understandable, it is preferable to construct a transformer-based architecture that \ncan identify discriminant patches from the histopathological WSI that generate clinical \nor medical results.\nConclusion\nTransformer architectures are now dominating almost all of the field of computer vision, \nwith a rapid increase in the field of histopathological imaging. In this survey paper, we \ncarry out a thorough review of the applications of transformer architectures in histo -\npathological image analysis. In particular, we survey the applications of transform -\ners in histopathological image classification, segmentation, detection, survival analysis \nand prediction, and representation and discuss their drawbacks. We found out that the \nmajority of the existing transformer architectures can be naturally and easily applied to \nhistopathological imaging challenges without significant modifications. As a matter of \nfact, many advanced approaches such as multi-task learning (MTL), weakly supervised \nlearning, multi-modal learning (MML), and model enhancement across various domains \nare rarely investigated. In addition, we also provided unsolved research problems for fur-\nther investigation. To this end, despite the outstanding performance of the transformer-\nbased architectures in a number of papers reviewed in this survey, we anticipate that \nthere will be much more exploration of transformers in histopathological image analysis \nto further increase the efficiency of clinicians, decrease subjectivity, and enhance patient \nPage 34 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \nsafety. Moreover, the majority of the diseases reviewed in this paper focused more on \nhistopathological image analysis, and it is expected that in the future, it will be extended \nto other imaging modalities where multiple data types, such as imaging, clinical, and \nlaboratory records, are regarded as multiple sources of information. We hope that this \nsurvey paper provides readers in this domain with a comprehensive idea of transformers.\nAuthor contributions\nCCA conceived and wrote the manuscript. JN read and checked the manuscript strictly; HL edited and supervised the \nmanuscript; QS and LY prepared figures and tables. XZ did the final supervision. All authors contributed to the article and \napproved the submitted version.\nFunding\nThis paper was supported in part by the National Natural Science Foundation of China under Grants  62001063, \n61971072 and U2133211, and in part by the Fundamental Research Funds for the Central Universities under Grant \n2023CDJXY-037.\nAvailability of data and materials\nNot applicable.\nDeclarations\n Ethics approval and consent to participate\nNot applicable. \nConsent for publication\nNot applicable. \nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 18 July 2023   Accepted: 15 September 2023\nReferences\n 1. Shakarami A, Nicolè L, Terreran M, Dei Tos AP , Ghidoni S. Tcnn: A transformer convolutional neural network for \nartifact classification in whole slide images. Biomed Signal Process Control. 2023;84: 104812.\n 2. Li X, Pang S, Zhang R, Zhu J, Fu X, Tian Y, Gao J. Attransunet: An enhanced hybrid transformer architecture for \nultrasound and histopathology image segmentation. Comput Biol Med. 2023;152: 106365.\n 3. Nakhli R, Moghadam PA, Mi H, Farahani H, Baras A, Gilks B, Bashashati A. Sparse multi-modal graph transformer \nwith shared-context processing for representation learning of giga-pixel images. In: Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition, pp. 11547–11557. 2023\n 4. Srinidhi CL, Ciga O, Martel AL. Deep neural network models for computational histopathology: a survey. Med \nImage Anal. 2021;67: 101813.\n 5. Wemmert C, Weber J, Feuerhake F, Forestier G. Deep learning for histopathological image analysis. deep learning \nfor biomedical data analysis: techniques, approaches, and applications, 153–169. 2021.\n 6. Hong R, Fenyö D. Deep learning and its applications in computational pathology. BioMedInformatics. \n2022;2(1):159–68.\n 7. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. \nAdv Neural Inform Process Syst 30 2017.\n 8. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, \nGelly S, Uszkoreit J, Houlsby N. An image is worth 16x16 words: transformers for image recognition at scale. ArXiv.  \n2020. abs/2010.11929\n 9. Prakash A, Chitta K, Geiger A. Multi-modal fusion transformer for end-to-end autonomous driving. In: Proceedings \nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7077–7087 2021.\n 10. Arnab A, Dehghani M, Heigold G, Sun C, Lučić M, Schmid C. Vivit: A video vision transformer. In: Proceedings of the \nIEEE/CVF International Conference on computer vision, pp. 6836–6846 2021.\n 11. George A, Marcel S. On the effectiveness of vision transformers for zero-shot face anti-spoofing. In: 2021 IEEE \nInternational Joint Conference on biometrics (IJCB), pp. 1–8 2021.\n 12. Atito S, Awais M, Wang W, Plumbley MD, Kittler J. Asit: Audio spectrogram vision transformer for general audio \nrepresentation. arXiv preprint arXiv: 2211. 13189 2022.\n 13. Gupta A, Tripathi R, Jang W. Modeformer: Modality-preserving embedding for audio-video synchronization using \ntransformers. In: ICASSP 2023-2023 IEEE International Conference on acoustics, speech and signal processing \n(ICASSP), pp. 1–5 2023.\n 14. Mehta S, Rastegari M. Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer. arXiv \npreprint arXiv: 2110. 02178 2021.\nPage 35 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \n 15. Lin A, Chen B, Xu J, Zhang Z, Lu G, Zhang D. Ds-transunet: dual swin transformer u-net for medical image segmen-\ntation. IEEE Trans Instru Measure. 2022;71:1–15.\n 16. Stegmüller T, Bozorgtabar B, Spahr A, Thiran J-P . Scorenet: Learning non-uniform attention and augmentation for \ntransformer-based histopathological image classification. In: Proceedings of the IEEE/CVF winter Conference on \napplications of computer vision, pp. 6170–6179 2023.\n 17. Li Z, Cong Y, Chen X, Qi J, Sun J, Yan T, Yang H, Liu J, Lu E, Wang L, et al. Vision transformer-based weakly supervised \nhistopathological image analysis of primary brain tumors. iScience. 2023;26(1): 105872.\n 18. Takagi Y, Hashimoto N, Masuda H, Miyoshi H, Ohshima K, Hontani H, Takeuchi I. Transformer-based personalized \nattention mechanism for medical images with clinical records. J Pathol Inform. 2023;14: 100185.\n 19. Chen H, Li C, Wang G, Li X, Rahaman MM, Sun H, Hu W, Li Y, Liu W, Sun C, et al. Gashis-transformer: a multi-scale \nvisual transformer approach for gastric histopathological image detection. Pattern Recogn. 2022. 130: 108827.\n 20. Huang P , He P , Tian S, Ma M, Feng P , Xiao H, Mercaldo F, Santone A, Qin J. A vit-amc network with adaptive model \nfusion and multiobjective optimization for interpretable laryngeal tumor grading from histopathological images. \nIEEE Trans Med Imaging. 2022. 42(1):15–28.\n 21. Vu QD, Rajpoot K, Raza SEA, Rajpoot N. Handcrafted histological transformer (h2t): unsupervised representation of \nwhole slide images. Med Image Anal. 2023. https:// doi. org/ 10. 1016/j. media. 2023. 102743.\n 22. Chen RJ, Chen C, Li Y, Chen TY, Trister AD, Krishnan RG, Mahmood F. Scaling vision transformers to gigapixel \nimages via hierarchical self-supervised learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision \nand Pattern Recognition, pp. 16144–16155. 2022.\n 23. Hu D, Xie F, Jiang Z, Zheng Y, Shi J. Histopathology cross-modal retrieval based on dual-transformer network. In: \n2022 IEEE 22nd International Conference on Bioinformatics and Bioengineering (BIBE), pp. 97–102. 2022.\n 24. Li M, Li C, Hobson P , Jennings T, Lovell BC. Medvitgan: End-to-end conditional gan for histopathology image \naugmentation with vision transformers. In: 2022 26th International Conference on Pattern Recognition (ICPR), pp. \n4406–4413 2022.\n 25. Huang Z, Chai H, Wang R, Wang H, Yang Y, Wu H. Integration of patch features through self-supervised learning \nand transformer for survival analysis on whole slide images. In: International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention, vol. 12908. Springer, pp. 561–570 2021.\n 26. Chen RJ, Lu MY, Weng W-H, Chen TY, Williamson DF, Manz T, Shady M, Mahmood F. Multimodal co-attention \ntransformer for survival prediction in gigapixel whole slide images. In: Proceedings of the IEEE/CVF International \nConference on Computer Vision, pp. 4015–4025 2021;\n 27. Li J, Chen J, Tang Y, Wang C, Landman BA, Zhou SK. Transforming medical imaging with transformers? a compara-\ntive review of key properties, current progresses, and future perspectives. Med Image Anal. 2023. https:// doi. org/ \n10. 1016/j. media. 2023. 102762.\n 28. Pinckaers H, Bulten W, Laak J, Litjens G. Detection of prostate cancer in whole-slide images through end-to-end \ntraining with image-level labels. IEEE Trans Med Imaging. 2021;40(7):1817–26.\n 29. Shen Y, Ke J. Sampling based tumor recognition in whole-slide histology image with deep learning approaches. \nIEEE/ACM Trans Comput Biol Bioinform. 2021;19(4):2431–41.\n 30. Senousy Z, Abdelsamea MM, Gaber MM, Abdar M, Acharya UR, Khosravi A, Nahavandi S. Mcua: multi-level context \nand uncertainty aware dynamic deep ensemble for breast cancer histology image classification. IEEE Trans \nBiomed Eng. 2021;69(2):818–29.\n 31. Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. arXiv preprint \narXiv: 1409. 0473 2014.\n 32. Hu J, Shen L, Sun G. Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on computer vision \nand pattern recognition, pp. 7132–7141. 2018.\n 33. Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S. End-to-end object detection with transformers. \nIn: European Conference on Computer Vision. Springer, pp. 213–229 2020\n 34. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B. Swin transformer: Hierarchical vision transformer using \nshifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022. \n2021.\n 35. Touvron H, Cord M, Douze M, Massa F, Sablayrolles A, Jégou H. Training data-efficient image transformers & distilla-\ntion through attention. In: International Conference on Machine Learning, pp. 10347–10357. 2021.\n 36. Bao H, Dong L, Wei F. Beit: Bert pre-training of image transformers. arXiv preprint arXiv: 2106. 08254 2021.\n 37. Tan M, Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In: International Conference \non machine learning, pp. 6105–6114. 2019.\n 38. Ronneberger O, Fischer P , Brox T. U-net: Convolutional networks for biomedical image segmentation. In: medical \nimage computing and computer-assisted intervention–MICCAI 2015, pp. 234–241, Springer, 2015.\n 39. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE Conference \non computer vision and pattern recognition, pp. 770–778 2016.\n 40. Zhang S, Zhang J, Xia Y. Transws: Transformer-based weakly supervised histology image segmentation. In: \nMachine Learning in Medical Imaging, Springer, pp. 367–376 2022.\n 41. Yin P , Yu B, Jiang C, Chen H. Pyramid tokens-to-token vision transformer for thyroid pathology image classification. \nIn: 2022 Eleventh International Conference on image processing theory, tools and applications (IPTA), pp. 1–6 \n2022.\n 42. Dwivedi VP , Bresson X. A generalization of transformer networks to graphs. arXiv preprint arXiv: 2012. 09699 2020.\n 43. Nakhli R, Moghadam PA, Mi H, Farahani H, Baras A, Gilks B, Bashashati A.. Sparse multi-modal graph transformer \nwith shared-context processing for representation learning of giga-pixel images. In: Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition, pp. 11547–11557 2023.\n 44. Reisenbüchler D, Wagner SJ, Boxberg M, Peng T. Local attention graph-based transformer for multi-target genetic \nalteration prediction. In: medical image computing and computer assisted intervention–MICCAI Springer, pp. \n377–386. 2022.2022.\nPage 36 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n 45. Yacob F, Siarov J, Villiamsson K, Suvilehto JT, Sjöblom L, Kjellberg M, Neittaanmäki N. Weakly supervised detection \nand classification of basal cell carcinoma using graph-transformer on whole slide images. Sci Rep. 2023;13(1):1–10.\n 46. Zheng Y, Gindra RH, Green EJ, Burks EJ, Betke M, Beane JE, Kolachalama VB. A graph-transformer for whole slide \nimage classification. IEEE Trans Med Imaging. 2022;41(11):3003–15.\n 47. Ding S, Li J, Wang J, Ying S, Shi J. Multi-scale efficient graph-transformer for whole slide image classification. arXiv \npreprint arXiv: 2305. 15773 2023.\n 48. Zidan U, Gaber MM, Abdelsamea MM. Swincup: Cascaded swin transformer for histopathological structures \nsegmentation in colorectal cancer. Expert Syst Appl. 2023;216: 119452.\n 49. Wang L, Pan L, Wang H, Liu M, Feng Z, Rong P , Chen Z, Peng S. Dhunet: Dual-branch hierarchical global-local \nfusion network for whole slide image segmentation. Biomed Signal Process Control. 2023;85: 104976.\n 50. Haq MM, Huang J. Self-supervised pre-training for nuclei segmentation. In: medical image computing and com-\nputer assisted intervention–MICCAI 2022, Springer, pp. 303–313. 2022\n 51. Tummala S, Kim J, Kadry S. Breast-net: Multi-class classification of breast cancer from histopathological images \nusing ensemble of swin transformers. Mathematics. 2022;10(21):4109.\n 52. Mehta S, Lu X, Wu W, Weaver D, Hajishirzi H, Elmore JG, Shapiro LG. End-to-end diagnosis of breast biopsy images \nwith transformers. Med Image Anal. 2022;79: 102466.\n 53. Chen Y, Shao Z, Bian H, Fang Z, Wang Y, Cai Y, Wang H, Liu G, Li X, Zhang Y. dmil-transformer: Multiple instance \nlearning via integrating morphological and spatial information for lymph node metastasis classification. IEEE J \nBiomed Health Inform. 2023. https:// doi. org/ 10. 1109/ JBHI. 2023. 32852 75.\n 54. Zhou X, Tang C, Huang P , Tian S, Mercaldo F, Santone A. Asi-dbnet: an adaptive sparse interactive resnet-vision \ntransformer dual-branch network for the grading of brain cancer histopathological images. Interdiscip Sci Comput \nLife Sci. 2023;15(1):15–31.\n 55. Ding M, Qu A, Zhong H, Lai Z, Xiao S, He P . An enhanced vision transformer with wavelet position embedding for \nhistopathological image classification. Pattern Recognition. 109532. 2023.\n 56. Tan L, Li H, Yu J, Zhou H, Wang Z, Niu Z, Li J, Li Z. Colorectal cancer lymph node metastasis prediction with weakly \nsupervised transformer-based multi-instance learning. Med Biol Eng Comput. 2023. https:// doi. org/ 10. 1007/ \ns11517- 023- 02799-x.\n 57. Barmpoutis P , Yuan J, Waddingham W, Ross C, Hamzeh K, Stathaki T, Alexander DC, Jansen M. Multi-scale deform-\nable transformer for the classification of gastric glands: The imgl dataset. In: Cancer Prevention Through Early \nDetection, Springer, pp. 24–33. 2022.\n 58. Alsaafin A, Safarpoor A, Sikaroudi M, Hipp JD, Tizhoosh H. Learning to predict rna sequence expressions from \nwhole slide images with applications for search and classification. Commun Biol. 2023;6(1):304.\n 59. Gao Z, Hong B, Zhang X, Li Y, Jia C, Wu J, Wang C, Meng D, Li C. Instance-based vision transformer for subtyping of \npapillary renal cell carcinoma in histopathological image. In: International Conference on Medical Image Comput-\ning and Computer-Assisted Intervention, Springer, pp. 29–308. 2021\n 60. Cai M, Zhao L, Hou G, Zhang Y, Wu W, Jia L, Zhao J, Wang L, Qiang Y. Fdtrans: Frequency domain transformer model \nfor predicting subtypes of lung cancer using multimodal data. Comput Biol Med. 2023;158: 106812.\n 61. Zheng Y, Li J, Shi J, Xie F, Jiang Z. Kernel attention transformer (kat) for histopathology whole slide image classifica-\ntion. In: International Conference on medical image computing and computer-assisted intervention, Springer, pp. \n283–292. 2022.\n 62. Li H, Yang F, Zhao Y, Xing X, Zhang J, Gao M, Huang J, Wang L, Yao J. Dt-mil: deformable transformer for multi-\ninstance learning on histopathological image. In: medical image computing and computer assisted intervention–\nMICCAI 2021, Springer, pp. 206–216. 2021.\n 63. Wang Y, Guo J, Yang Y, Kang Y, Xia Y, Li Z, Duan Y, Wang K. Cwc-transformer: a visual transformer approach for \ncompressed whole slide image classification. Neural Comput Appl. 1–13. 2023\n 64. Wang X, Yang S, Zhang J, Wang M, Zhang J, Huang J, Yang W, Han X. Transpath: Transformer-based self-supervised \nlearning for histopathological image classification. In: medical image computing and computer assisted interven-\ntion–MICCAI 2021. 186–195. 2021.\n 65. Shao Z, Bian H, Chen Y, Wang Y, Zhang J, Ji X, et al. Transmil: Transformer based correlated multiple instance learn-\ning for whole slide image classification. Adv Neural Inform Process Syst. 2021;34:2136–47.\n 66. Zhu H, Lin M, Xu Z, Yao Z, Chen H, Alhudhaif A, Alenezi F. Deconv-transformer (dect): A histopathological image \nclassification model for breast cancer based on color deconvolution and transformer architecture. Inform Sci. \n2022;608:1093–112.\n 67. Zhang J, Kapse S, Ma K, Prasanna P , Saltz J, Vakalopoulou M, Samaras D. Prompt-mil: Boosting multi-instance learn-\ning schemes via task-specific prompt tuning. arXiv preprint arXiv: 2303. 12214. 2023.\n 68. Xiong C, Chen H, Sung J, King I. Diagnose like a pathologist: Transformer-enabled hierarchical attention-guided \nmultiple instance learning for whole slide image classification. arXiv preprint arXiv: 2301. 08125. 2023.\n 69. Lu MY, Chen B, Zhang A, Williamson DF, Chen RJ, Ding T, Le LP , Chuang Y-S, Mahmood F. Visual language \npretrained multiple instance zero-shot transfer for histopathology images. In: Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 19764–19775. 2023.\n 70. Ding S, Wang J, Li J, Shi J. Multi-scale prototypical transformer for whole slide image classification. arXiv preprint \narXiv: 2307. 02308. 2023.\n 71. Yu J, Ma T, Fu Y, Chen H, Lai M, Zhuo C, Xu Y. Local-to-global spatial learning for whole-slide image representation \nand classification. Computer Med Imaging Graph. 2023;107: 102230.\n 72. Zou Y, Chen S, Sun Q, Liu B, Zhang J. Dcet-net: Dual-stream convolution expanded transformer for breast cancer \nhistopathological image classification. In: 2021 IEEE International Conference on bioinformatics and biomedicine \n(BIBM), pp. 1235–1240. 2021.\n 73. Wang L, Liu J, Jiang P , Cao D, Pang B. Lgvit: Local-global vision transformer for breast cancer histopathological \nimage classification. In: ICASSP 2023 - 2023 IEEE International Conference on acoustics, speech and signal process-\ning (ICASSP), pp. 1–5. 2023.\nPage 37 of 38\nAtabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n \n 74. Pan L, Wang H, Wang L, Ji B, Liu M, Chongcheawchamnan M, Yuan J, Peng S. Noise-reducing attention cross fusion \nlearning transformer for histological image classification of osteosarcoma. Biomed Signal Process Control. 2022;77: \n103824.\n 75. Cai H, Feng X, Yin R, Zhao Y, Guo L, Fan X, Liao J. Mist: Multiple instance learning network based on swin trans-\nformer for whole slide image classification of colorectal adenomas. J Pathol. 2022;259(2):125–35.\n 76. Zhang H, Chen H, Qin J, Wang B, Ma G, Wang P , Zhong D, Liu J. Mc-vit: Multi-path cross-scale vision transformer for \nthymoma histopathology whole slide image typing. Front Oncol. 2022;12: 925903.\n 77. Wang Z, Yu L, Ding X, Liao X, Wang L. Lymph node metastasis prediction from whole slide images with trans-\nformer-guided multiinstance learning and knowledge transfer. IEEE Trans Med Imaging. 2022;41(10):2777–87.\n 78. Wang Z, Yu L, Ding X, Liao X, Wang L. Shared-specific feature learning with bottleneck fusion transformer for multi-\nmodal whole slide image analysis. IEEE Trans Med Imaging. 2023. https:// doi. org/ 10. 1109/ TMI. 2023. 32872 56.\n 79. Wang X, Yang S, Zhang J, Wang M, Zhang J, Yang W, Huang J, Han X. Transformer-based unsupervised contrastive \nlearning for histopathological image classification. Med Image Anal. 2022;81: 102559.\n 80. Fu B, Zhang M, He J, Cao Y, Guo Y, Wang R. Stohisnet: A hybrid multi-classification model with cnn and transformer \nfor gastric pathology images. Computer Methods Program Biomed. 2022. https:// doi. org/ 10. 1016/j. cmpb. 2022. \n106924.\n 81. Zhao Y, Lin Z, Sun K, Zhang Y, Huang J, Wang L, Setmil Yao J. Spatial encoding transformer-based multiple instance \nlearning for pathological image analysis Medical Image Computing and Computer assisted intervention-MICCAI. \nBerlin: Springer; 2022.\n 82. Jiang S, Hondelink L, Suriawinata AA, Hassanpour S. Masked pre-training of transformers for histology image \nanalysis. arXiv preprint arXiv: 2304. 07434 2023.\n 83. Qian Z, Li K, Lai M, Chang EI-C, Wei B, Fan Y, Xu Y. Transformer based multiple instance learning for weakly super-\nvised histopathology image segmentation In Medical Image Computing and computer assisted intervention-\nMICCAI. Berlin: Springer; 2022.\n 84. Ji Y, Zhang R, Wang H, Li Z, Wu L, Zhang S, Luo P . Multi-compound transformer for accurate biomedical image \nsegmentation medical image computing and computer assisted intervention-MICCAI. Berlin: Springer; 2021.\n 85. Chen Y, Jia Y, Zhang X, Bai J, Li X, Ma M, Sun Z, Pei Z, Tshvnet, et al. Simultaneous nuclear instance segmentation \nand classification in histopathological images based on multiattention mechanisms. BioMed Res Int. 2022;2022. \nhttps:// doi. org/ 10. 1155/ 2022/ 79219 22.\n 86. Diao S, Tang L, He J, Zhao H, Luo W, Xie Y, Qin W. Automatic computer-aided histopathologic segmentation for \nnasopharyngeal carcinoma using transformer framework computational mathematics modeling in cancer analy-\nsis. Berlin: Springer; 2022.\n 87. Chen B, Liu Y, Zhang Z, Lu G, Kong AWK. Transattunet: multi-level attention-guided u-net with transformer for \nmedical image segmentation. arXiv preprint arXiv: 2107. 05274. 2021.\n 88. Guo Z, Wang Q, Müller H, Palpanas T, Loménie N, Kurtz C. A hierarchical transformer encoder to improve entire \nneoplasm segmentation on whole slide image of hepatocellular carcinoma. arXiv preprint arXiv: 2307. 05800. 2023.\n 89. Li Z, Tang Z, Hu J, Wang X, Jia D, Zhang Y. Nst: a nuclei segmentation method based on transformer for gastrointes-\ntinal cancer pathological images. Biomed Signal Process Control. 2023;84: 104785.\n 90. Valanarasu JMJ, Oza P , Hacihaliloglu I, Patel VM. Medical transformer: gated axial-attention for medical image \nsegmentation medical image computing and computer assisted intervention-MICCAI. Berlin: Springer; 2021.\n 91. Qin W, Xu R, Jiang S, Jiang T, Luo L. Pathtr: Context-aware memory transformer for tumor localization in gigapixel \npathology images. In: Proceedings of the Asian Conference on Computer Vision, pp. 3603–3619. 2022.\n 92. Ali ML, Rauf Z, Khan AR, Khan A. Channel boosting based detection and segmentation for cancer analysis in \nhistopathological images. In: 2022 19th International Bhurban Conference on applied sciences and technology \n(IBCAST), pp. 1–6 2022.\n 93. Yücel Z, Akal F, Oltulu P . Mitotic cell detection in histopathological images of neuroendocrine tumors using \nimproved yolov5 by transformer mechanism. Signal Image Video Process. 1–8 2023.\n 94. Lv Z, Yan R, Lin Y, Wang Y, Zhang F. Joint region-attention and multi-scale transformer for microsatellite instability \ndetection from whole slide images in gastrointestinal cancer medical image computing and computer assisted \nintervention-MICCAI. Berlin: Springer; 2022.\n 95. Liaqat Ali M, Rauf Z, Khan A, Sohail A, Ullah R, Gwak J. Cb-hvtnet: A channel-boosted hybrid vision transformer \nnetwork for lymphocyte assessment in histopathological images. arXiv e-prints. 2305. 2023.\n 96. Hossain MS, Shahriar GM, Syeed MM, Uddin MF, Hasan M, Shivam S, Advani S. Region of interest (roi) selection \nusing vision transformer for automatic analysis using whole slide images. Sci Rep. 2023;13(1):11314.\n 97. Lv Z, Lin Y, Yan R, Wang Y, Zhang F. Transsurv: Transformer-based survival analysis model integrating histopatho-\nlogical images and genomic data for colorectal cancer. IEEE/ACM Transactions on Computational Biol Bioinform \n1–10. 2022.\n 98. Lv Z, Lin Y, Yan R, Yang Z, Wang Y, Zhang F. Pg-tfnet: Transformer-based fusion network integrating pathological \nimages and genomic data for cancer survival analysis. In: 2021 IEEE International Conference on Bioinformatics \nand Biomedicine (BIBM), pp. 491–496. 2021.\n 99. Shen Y, Liu L, Tang Z, Chen Z, Ma G, Dong J, Zhang X, Yang L, Zheng Q. Explainable survival analysis with convolu-\ntion-involved vision transformer. Proc AAAI Conf Artif Intell. 2022;36:2207–15.\n 100. Li C, Zhu X, Yao J, Huang J. Hierarchical transformer for survival prediction using multimodality whole slide images \nand genomics. In: 2022 26th International Conference on Pattern Recognition (ICPR), pp. 4256–4262, 2022.\n 101. Jaume G, Vaidya A, Chen R, Williamson D, Liang P , Mahmood F. Modeling dense multimodal interactions between \nbiological pathways and histology for survival prediction. arXiv preprint arXiv: 2304. 06819 2023.\n 102. Wang Z, Gao Q, Yi X-P , Zhang X, Zhang Y, Zhang D, Liò P , Bain C, Bassed R, Li S, et al. Surformer: An interpretable \npattern-perceptive survival transformer for cancer survival prediction from histopathology whole slide images. \nSSRN 4423682. 2023.\n 103. Shao Z, Chen Y, Bian H, Zhang J, Liu G, Hvtsurv Zhang Y. Hierarchical vision transformer for patient-level survival \nprediction from whole slide image. Proc AAAI Conf Artif Intell. 2023;37:2209–17.\nPage 38 of 38Atabansi et al. BioMedical Engineering OnLine           (2023) 22:96 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 104. Li Z, Jiang Y, Lu M, Li R, Xia Y. Survival prediction via hierarchical multimodal co-attention aransformer: a computa-\ntional histology-radiology solution. IEEE Trans Med Imaging. 2023. https:// doi. org/ 10. 1109/ TMI. 2023. 32630 10.\n 105. Kapse S, Das S, Prasanna P . Cd-net: Histopathology representation learning using pyramidal context-detail net-\nwork. arXiv preprint arXiv: 2203. 15078. 2022.\n 106. Liu P , Fu B, Ye F, Yang R, Dsca Ji L. A dual-stream network with cross-attention on whole-slide image pyramids for \ncancer prognosis. Expert Syst Appl. 2023;227: 120280.\n 107. Chan TH, Cendra FJ, Ma L, Yin G, Yu L. Histopathology whole slide image analysis with heterogeneous graph \nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, \npp. 15661–15670. 2023.\n 108. Vu QD, Rajpoot K, Raza SEA, Rajpoot N. Handcrafted histological transformer (h2t): unsupervised representation of \nwhole slide images. Med Image Anal. 2023;85: 102743.\n 109. Wood R, Sirinukunwattana K, Domingo E, Sauer A, Lafarge MW, Koelzer VH, Maughan TS, Rittscher J. Enhancing \nlocal context of histology features in vision transformers Artificial Intelligence over infrared images for medical \napplications and medical image assisted biomarker discovery. Berlin: Springer; 2022.\n 110. Xu X, Kapse S, Gupta R, Prasanna P . Vit-dae: Transformer-driven diffusion autoencoder for histopathology image \nanalysis. arXiv preprint arXiv: 2304. 01053 2023.\n 111. Myronenko A, Xu Z, Yang D, Roth HR, Xu D. Accounting for dependencies in deep learning based multiple \ninstance learning for whole slide imaging. Berlin: Medical Image Computing and Computer Assisted Intervention-\nMICCAI. Springer; 2021.\n 112. Nguyen C, Asad Z, Deng R, Huo Y. Evaluating transformer-based semantic segmentation networks for pathological \nimage segmentation medical imaging 2022. Image Process. 2022;12032:942–7.\n 113. Cao H, Wang Y, Chen J, Jiang D, Zhang X, Tian Q, Wang M. Swin-unet: Unet-like pure transformer for medical image \nsegmentation. In: Computer Vision–ECCV 2022 Workshops, Springer, pp. 205–218. 2023.\n 114. Deininger L, Stimpel B, Yuce A, Abbasi-Sureshjani S, Schönenberger S, Ocampo P , Korski K, Gaire F. A comparative \nstudy between vision transformers and cnns in digital pathology. arXiv preprint arXiv: 2206. 00389. 2022.\n 115. Springenberg M, Frommholz A, Wenzel M, Weicken E, Ma J, Strodthoff N. From cnns to vision transformers–a \ncomprehensive evaluation of deep learning models for histopathology. arXiv preprint arXiv: 2204. 05044. 2022.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.4778568148612976
    },
    {
      "name": "Computer science",
      "score": 0.47108328342437744
    },
    {
      "name": "Engineering",
      "score": 0.38111382722854614
    },
    {
      "name": "Electrical engineering",
      "score": 0.17215698957443237
    },
    {
      "name": "Voltage",
      "score": 0.12273252010345459
    }
  ]
}