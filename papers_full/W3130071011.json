{
    "title": "Do We Really Need Explicit Position Encodings for Vision Transformers",
    "url": "https://openalex.org/W3130071011",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2953550412",
            "name": "Xiangxiang Chu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097431157",
            "name": "Bo Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2228339486",
            "name": "Zhi Tian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109469011",
            "name": "Xiaolin Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2151220250",
            "name": "Huaxia Xia",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2950813464",
        "https://openalex.org/W3108995912",
        "https://openalex.org/W2949650786",
        "https://openalex.org/W2743473392",
        "https://openalex.org/W2989226908",
        "https://openalex.org/W2949892913",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2937843571",
        "https://openalex.org/W2925359305",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W3109319753",
        "https://openalex.org/W3035206215",
        "https://openalex.org/W3033187248",
        "https://openalex.org/W2950628590",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2896197082",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W2747685395",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2953106684",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3001591165",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2186615578",
        "https://openalex.org/W2734663976",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3129012257",
        "https://openalex.org/W3129603602",
        "https://openalex.org/W2765407302",
        "https://openalex.org/W2963623257",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3168649818"
    ],
    "abstract": "Almost all visual transformers such as ViT or DeiT rely on predefined positional encodings to incorporate the order of each input token. These encodings are often implemented as learnable fixed-dimension vectors or sinusoidal functions of different frequencies, which are not possible to accommodate variable-length input sequences. This inevitably limits a wider application of transformers in vision, where many tasks require changing the input size on-the-fly. \r\nIn this paper, we propose to employ a conditional position encoding scheme, which is conditioned on the local neighborhood of the input token. It is effortlessly implemented as what we call Position Encoding Generator (PEG), which can be seamlessly incorporated into the current transformer framework. Our new model with PEG is named Conditional Position encoding Visual Transformer (CPVT) and can naturally process the input sequences of arbitrary length. We demonstrate that CPVT can result in visually similar attention maps and even better performance than those with predefined positional encodings. We obtain state-of-the-art results on the ImageNet classification task compared with visual Transformers to date. Our code will be made available at this https URL .",
    "full_text": null
}