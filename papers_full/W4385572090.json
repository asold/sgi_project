{
  "title": "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models",
  "url": "https://openalex.org/W4385572090",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2129761198",
      "name": "Qingqing Cao",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2618035253",
      "name": "Bhargavi Paranjape",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A91410043",
      "name": "Hannaneh Hajishirzi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3168124404",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W3154851733",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W3104738015",
    "https://openalex.org/W4284892042",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3214685499",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4309398208",
    "https://openalex.org/W4365446402",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4287901267",
    "https://openalex.org/W3086410487",
    "https://openalex.org/W1773149199",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W4289243726",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4322716158",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W4206634569",
    "https://openalex.org/W4226315084",
    "https://openalex.org/W3110662498",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4287240280",
    "https://openalex.org/W4312980231",
    "https://openalex.org/W4306886919",
    "https://openalex.org/W4312784228",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W3035030897"
  ],
  "abstract": "Large-scale vision language (VL) models use Transformers to perform cross-modal interactions between the input text and image. These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text. We present PuMer: a token reduction framework that uses text-informed Pruning and modality-aware Merging strategies to progressively reduce the tokens of input image and text, improving model inference speed and reducing memory footprint. PuMer learns to keep salient image tokens related to the input text and merges similar textual and visual tokens by adding lightweight token reducer modules at several cross-modal layers in the VL model. Training PuMer is mostly the same as finetuning the original VL model but faster. Our evaluation for two vision language models on four downstream VL tasks shows PuMer increases inference throughput by up to 2x and reduces memory footprint by over 50% while incurring less than a 1% accuracy drop.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 12890‚Äì12903\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nPuMer: Pruning and Merging Tokens for Efficient Vision Language Models\nQingqing Cao Bhargavi Paranjape\n{qicao,bparan,hannaneh}@cs.washington.edu\nUniversity of Washington\nHannaneh Hajishirzi\nAbstract\nLarge-scale vision language (VL) models use\nTransformers to perform cross-modal interac-\ntions between the input text and image. These\ncross-modal interactions are computationally\nexpensive and memory-intensive due to the\nquadratic complexity of processing the input\nimage and text. We present PuMer 1: a token\nreduction framework that uses text-informed\nPruning and modality-aware Merging strate-\ngies to progressively reduce the tokens of in-\nput image and text, improving model inference\nspeed and reducing memory footprint. PuMer\nlearns to keep salient image tokens related to\nthe input text and merges similar textual and\nvisual tokens by adding lightweight token re-\nducer modules at several cross-modal layers in\nthe VL model. Training PuMer is mostly the\nsame as finetuning the original VL model but\nfaster. Our evaluation for two vision language\nmodels on four downstream VL tasks shows\nPuMer increases inference throughput by up to\n2x and reduces memory footprint by over 50%\nwhile incurring less than a 1% accuracy drop. 2\n1 Introduction\nLarge-scale vision language models (Dou et al.,\n2022; Wang et al., 2022; Zeng et al., 2021; Kim\net al., 2021; Wang et al., 2021; Zhang et al., 2021)\nhave shown substantial progress on many vision\nlanguage tasks such as visual question answer-\ning, natural language visual reasoning, and vi-\nsual entailment. However, state-of-the-art lan-\nguage and vision models are memory intensive\nand computationally expensive because they use\nmulti-layer self-attention between many language\nand vision input tokens (small image patches) with\nquadratic complexity. This inefficiency limits high-\nthroughput cloud deployments and makes it infea-\nsible to run on resource-constrained devices.\n1Pronounced as ‚Äúpuma‚Äù\n2Code is available at https://github.com/\ncsarron/PuMer.\n2.\tWhat\tsport\tare\tthey\tplaying?\nPrune\tMerge1.\tHow\tmany\tpeople\tare\tplaying?\nPrune\tMerge\n Soccer\nFour\nFigure 1: PuMer applies token reduction to VL models\nvia pruning and merging. PuMer makes VL models run\nfaster by text-informed image pruning to remove text-\nirrelevant image tokens and modality-aware merging to\ncompress similar input tokens.\nThe key source of inefficiency in deep VL mod-\nels is that these models need to process the entire\ninput image and text tokens over all the layers. Our\nintuition is that the input image contains redundant\ninformation and only parts of the image ( salient\nregions, referred by the text) are required and re-\nlated to the end task. For example, in Figure 1,\nmost of the image content (the four persons, field)\nis not needed except for the bottom-center soccer\nregion to answer the visual question ‚ÄúWhat sport\nare they playing?‚Äù. This paper advocates using the\ncorrelations between image and text modalities to\nreduce tokens for VL problems.\nIn the vision-only or text-only domains, re-\nsearchers have shown that reducing image or text\ntokens can improve the model computational com-\nplexity through pruning (Liang et al., 2021; Rao\net al., 2021; Yin et al., 2022; Marin et al., 2021;\nGoyal et al., 2020) that learns to remove non-salient\nimage or text tokens for a given task; or merging\n(Bolya et al., 2022; Xu et al., 2022; Ryoo et al.,\n2021) that groups semantically similar tokens. Us-\ning either reduction method in isolation is not suf-\nficient for a VL problem setting since i) salient\nimage tokens are different given different text in-\nputs, ii) pruning alone causes big information loss,\nhurting the performance, iii) merging tokens irre-\nspective of their modality confuses the VL models\nsince text and image token representations cannot\n12890\nbe perfectly aligned to the same semantic space. In\nthis paper, we design a lightweight and effective\nframework that integrates these token reduction\nstrategies into VL models.\nWe introduce PuMer, a token reduction frame-\nwork that consists of Pruning-and-Merging oper-\nations to gradually reduce image tokens that are\nnot related to text and merge image and text to-\nkens respective to their modality. In particular,\nwe design (i) text-informed image token pruning\nto remove image tokens that are irrelevant to text\nand are unimportant to the VL task predictions (re-\nmoving tokens that describe persons and field for\nthe second question in the Figure 1 example); (ii)\nmodality-aware token merging to merge semanti-\ncally redundant tokens for text and image tokens\nmodality independently (combining the image to-\nkens describing each person for the first question\nin Figure 1). We keep the remaining tokens that are\nneither pruned nor merged. At the core of PuMer\nis a set of lightweight non-parametric token reduc-\ners that decide which image tokens are pruned and\nmerged as the VL model forward computation pro-\nceeds. To reduce abrupt image information loss and\nimprove computational efficiency, we scatter the\ntoken reducers at different cross-modal layers in\nthe VL model and reduce the tokens in a cascaded\nfashion. Fewer tokens are pruned and merged in\nearlier layers.\nPuMer is easy to train since the token reducers\ncontain no parameters and add little overhead. The\ntraining procedure is almost the same as finetun-\ning the original VL models, except that we add\na knowledge distillation loss that further reduces\nthe accuracy gap compared to finetuned models.\nThough we focus on inference efficiency, PuMer\nmakes VL models run faster for both training and\ninference because text and image tokens are re-\nduced in the forward computation.\nWe evaluate PuMer over two recent VL models\nViLT (Kim et al., 2021) and METER (Dou et al.,\n2022) across five vision language tasks: text-image\nretrieval tasks (including image-to-text and text-to-\nimage retrieval) (Plummer et al., 2015), visual ques-\ntion answering (VQAv2; Goyal et al. 2017), natu-\nral language visual reasoning (NLVR2; Suhr et al.\n2019), and visual entailment (SNLI-VE; Xie et al.\n2019). Compared to baselines, PuMer improves\nthe model inference throughput by 1.7x‚àº2.1x and\nreduces memory footprint by38%‚àº50% with min-\nimal (less than 1%) accuracy loss. Our analysis\nvalidates that both text-informed image pruning\nand modality-aware token merging contribute to\nthe token reduction effectiveness of PuMer.\n2 Related work\nToken Reduction in NLP and Vision. Prior\nwork in data pruning (Rao et al., 2021; Yin et al.,\n2022; Liang et al., 2021; Goyal et al., 2020) focus\non single-modality models by either pruning input\ntext or image alone. DynamicViT (Rao et al., 2021)\nand A-ViT (Yin et al., 2022) both progressively re-\nmove the uninformative content and keep salient\nregions in the input image. This type of pruning\ndoes not apply to language and vision tasks where\nthe salient regions depend on the input text. Our\nwork shows different input texts lead to pruning\ndifferent image regions even for the same input\nimage. PoWER-BERT (Goyal et al., 2020) speeds\nup the inference of text-based Transformers like\nBERT (Devlin et al., 2019) by removing the input\ntext tokens, which are not the main computation\nbottlenecks for most vision and language tasks.\nAnother line of work seeks to reduce input to-\nkens by merging tokens. SPViT (Kong et al., 2022)\nand EViT (Liang et al., 2021) select uninforma-\ntive image tokens and combine them into one to-\nken. And EViT also requires expensive pretraining.\nGroupViT (Xu et al., 2022) combines image to-\nkens via cross-attention to find similar objects for\nsemantic segmentation. Recently, ToMe (Bolya\net al., 2022), TokenLearner (Ryoo et al., 2021) and\nTokenPooling (Marin et al., 2021) combine tokens\nwithout pruning and achieves better speedup versus\naccuracy trade-offs.\nOur method is inspired by token pruning and\nmerging works but integrates them into a token re-\nduction framework suitable for VL models. Our\nkey difference is to leverage the relationships be-\ntween textual and visual tokens to remove and com-\nbine tokens. Our experiments (Section 5) show\nimprovements over these lines of work.\nEfficient Vision Language Models. Many tech-\nniques have focused on model pruning (Lagunas\net al., 2021; Yu and Wu, 2021; Yu et al., 2022; TPr),\ndynamic computation by early exiting (Xin et al.,\n2020; Zhou et al., 2020; Schwartz et al., 2020; Liu\net al., 2020; Cao et al., 2022) or designing small\nand efficient VL models (Fang et al., 2021; Wang\net al., 2020). Combining these orthogonal opti-\nmizations with our token reduction method could\nfurther accelerate the inference in VL models.\n12891\n3 Background and Overview\nVision Language Models. Figure 2 shows the\nbackbone of a VL model consisting of a text en-\ncoder, an image encoder, and a cross-modal en-\ncoder. The input sentence (e.g. a question or a\nstatement) is first tokenized as text tokens and fed\nto the text encoder to create contextualized text rep-\nresentations. Similarly, the input image is projected\ninto many small image patches, referred to as ‚Äúim-\nage tokens‚Äù, that are further contextualized by the\nimage encoder. Finally, the cross-modal encoder\ntakes the concatenated text and image tokens and\nfuses information between image and text modal-\nities via Transformer-style (Vaswani et al., 2017)\ncross-attention interactions.\nText\tEncoderImage\tEncoder\nImage\tText\tCross-Modal\tEncoder\nText\tTokens,\tImage\tTokens\nImage\tPatchesText\ttokenswhat\t\tsport\t\tare\t\tthey\t\tplaying?\t\nFigure 2: General architecture of vision language mod-\nels. The input image is projected into many small im-\nage patches (‚Äútokens‚Äù) that are processed by the image\nencoder. The cross-modal attention between text and\nimage tokens has quadratic time complexity, which is\ncomputationally expensive. Both ViLT and METER\nmodels follow this pattern.\nFor many VL tasks, the number of tokens of\nthe input image is an order of magnitude more\nthan that of the input text ‚Äî a visual question can\nhave at most a dozen tokens but the associated\nimage consists of a hundred image tokens. For\nexample, for an image with a resolution of 384x384\nand a patch size of 16, the number of tokens is\n(384/16)2 = 576.\nToken Reduction for Efficiency. In this paper,\nwe focus onreducing image tokens to improve com-\nputational efficiency of the model through pruning\nand merging. However, naively removing a large\npercentage of the image tokens inside the cross-\nmodal layers may cause abrupt image information\nloss, as the VL model is trained to build represen-\ntations of the full image for the downstream task.\nFor example, if the soccer region in Figure 1 gets\npruned, the VL model is unlikely to output the\nanswer ‚Äúsoccer‚Äù for the question ‚Äúwhat sport are\nthey playing?‚Äù. On the other hand, simply merging\nimage tokens without text guidance can lead to sub-\noptimal performance. For example, merging the\nimage regions of the background field and soccer\nin Figure 1 does not contribute to answering the\nvisual question ‚Äúhow many people are playing?‚Äù.\nThe next section describes our text-informed to-\nken reduction approach. The basic building blocks\nof PuMer are lightweight non-parametric token\nreducers that reduce image and text tokens in a\ncascaded manner to mitigate the information loss\nand improve the computational efficiency of a VL\nmodel.\n4 PuMer: Text-Informed Token\nReduction Framework\nGiven a VL cross-modal encoder, PuMer progres-\nsively reduces image tokens going through the\ncross-modal encoder (depicted in Figure 3). PuMer\nuses lightweight token reducers with no learnable\nparameters, adding them in different layers of the\ncross-modal encoder to predict which image tokens\nare removed or merged.\nToken Reducers. For an n-layer cross-modal en-\ncoder, after the first f (f < n) layers, a token\nreducer first removes k% of the image tokens at\nany layer ‚Ñì between f and n guided by the text\ninformation. The tokens removed in layer ‚Ñì are not\nused in subsequent layers. Then the token reducer\nmerges r% and t% of the image and text tokens\nrespectively in layer ‚Ñì. We scatter the token re-\nducers across the cross-modal layers to achieve a\nbetter accuracy and efficiency trade-off. Intuitively,\nreducing at early layers in the cross-modal encoder\nwill have higher inference efficiency but may have\nbigger performance loss and vice versa. We study\nthis trade-off in more detail in Section 6.2.\nThe token reduction algorithm is described\nin Algorithm 1. Each token reducer consists\nof two sequential non-parametric modules: first,\na text-informed pruner (TIP) prunes image to-\nkens that are not related to the accompanying\ntext (Section 4.1); second, a modality-aware\nmerger (MAM) reduces tokens by merging simi-\nlar tokens within the image or text modality (Sec-\ntion 4.2). These two steps reduce the image and\ntext tokens to benefit the computational efficiency,\nwhile not losing the accuracy. Note that if we only\napply text-informed pruning to the images with-\nout merging, to achieve similar efficiency gains,\nwe need to set a larger pruning ratio which will\n12892\ntheyareplaying\nwhat Token Reducer Token ReducerPrediction:\tsoccer\nClassifier\nCross-Modal LayersCross-Modal Layers\nCross-Modal Encoder\nImage Encoder\nText Encoder\npruned\nmerged mergedpruned\nmerged\n merged\nFigure 3: PuMer applies token reducers in the cross-modal layers of a VL model. Each token reducer is non-\nparametric and uses text-informed pruning and modality-aware merging to reduce image and text tokens.\nAlgorithm 1 Token Reduction via Text-Informed Image Pruning and Modality-Aware Merging\nInput: text token vectors T, text-to-image cross attention scores A, image token vectors V,\nprune ratio k, image merge ratio r, text merge ratio t\nOutput: merged text token vectors Tm, pruned and merged image token vectors Vm\n1: for image tokens V, compute text-saliency scores s using Eq1; ‚ñ∑ text-informed image pruning\n2: obtain indices idx of top-k‚Ä≤ items in score s, k‚Ä≤ = (1‚àík)|V|; ‚ñ∑ k‚Ä≤ is the # of kept image tokens\n3: select k‚Ä≤ image tokens by the top-k‚Ä≤ indices, Vp = V[idx];\n4: merge text tokens T by bipartite soft matching into Tm = bipartite_merge(T, t);\nmerge image tokens Vp into Vm = bipartite_merge(Vp, r) ‚ñ∑ modality-aware merging\n5: procedure BIPARTITE _MERGE (input tokens: X, merge ratio: r)\n6: divide tokens X into two sets of tokens O and E based on even and odd order\n7: for each token Oa in O, compute its top-1 similar token Eb in E, save the indices a and b into\na token edge (an edge between Oa and Eb), save all token edges as P and corresponding\ntop-1 similarity scores Sp ‚ñ∑ this can be implemented as a fast parallel operation\n8: r‚Ä≤ = r|X|, obtain indices ind of top-r‚Ä≤ items in Sp, select top-r‚Ä≤ edges: Pr = P[ind]\n9: for each token edge (a, b) in Pr, collect tokens from O and E, merge tokens in O and E that\nare connected via edges (sharing the same token as a vertex node) into OE by computing\nthe average of each token vectors, gather Orest and Erest from the rest (unmerged) indices.\n10: output: merged tokens Xm = gather(OE, Orest, Erest)\n11: end procedure\nhurt task performance due to substantial informa-\ntion loss. Instead of dropping such information,\nmodality-aware merging helps alleviate informa-\ntion loss by compressing semantically similar con-\ntent into fewer tokens while still providing effi-\nciency benefits.\n4.1 Text-Informed Image Pruning\nThe first step is to prune image tokens according\nto their relevance to the text. The intuition is that\nonly some parts of the image are important for\nthe end language-vision task, hence removing the\ntext-irrelevant parts will not hurt the performance,\nwhile it improves the computational efficiency. Un-\nlike previous works (Rao et al., 2021) that use extra\nlearnable parameters to predict which image tokens\nto prune, we take a different but faster approach\nwithout using any parameters. The key idea is to\nuse the text-to-image cross-attention scores3 that\nare already available in the VL model to compute\nhow important each image token is to the text. We\nkeep important image tokens and prune the rest.\nSince this text-informed pruning also removes im-\nage tokens during training, it trains faster 4 than\nparameter-based pruning approaches like Rao et al.\n3VL models use cross-attention to perform information\nfusion between different modalities.\n4We observe 15%‚àº20% faster training speed in practice.\n12893\n(2021).\nFor each cross-modal layer ‚Ñì where the token\nreducer is applied, we denote the input text token\nvectors as T, image token vectors as V, and text-\nto-image cross-attention scores as A (computed in\nthe cross-attention layer that already exists in a VL\nmodel). We first compute the text-saliency scores\ns for every image token:\nsv = 1\n|T|\nt=1‚àë\n|T|\nh=1‚àë\nH\nAh\ntv, (1)\nwhere |T|is the number of text tokens, H the num-\nber of attention heads, t and v are the indices of\ntext and image tokens. This text-saliency score\nfor the image token is text-informed because each\nvalue is summed over all text tokens, and an image\ntoken with a bigger text-saliency score means it‚Äôs\nattended more by the text and hence is more text-\nrelevant. Next, we keep top-k‚Ä≤ image tokens5 Vp\naccording to their text-saliency score and discard\nthe remaining image tokens.\n4.2 Modality-Aware Merging\nOnce text-irrelevant image tokens are pruned, the\nremaining image tokens contain more text-salient\ninformation but they might still be redundant. For\nexample, multiple image tokens describe the same\nperson in the Figure 1 image and their representa-\ntions might be similar (their vector distances are\nclose). For the text modality, the token redundancy\nstill exists due to the self-attention contextualiza-\ntion which progressively creates similar informa-\ntion (Goyal et al., 2020). In practice, text tokens\nare padded to max length for efficient training and\ninference, these padding tokens also contribute to\nredundancy.\nIn this section, we describe our modality-aware\nmerging approach to eliminate such redundancy. In\nparticular, our method merges semantically similar\nimage tokens Vp into a single image token and\nsimilar text tokens T into a single text token to fur-\nther reduce the number of tokens. We specifically\nmerge tokens within each modality, i.e., image to-\nkens are merged with similar image tokens, and\ntext tokens are merged with similar text tokens.\nTo implement modality-aware merging, we need\nto identify similar tokens and combine their infor-\nmation in a lightweight way. Existing methods such\nas k-means clustering (Marin et al., 2021), pool-\ning (Pietruszka et al., 2020; Nawrot et al., 2022),\n5k‚Ä≤ = (1‚àík)|V|is the number of kept tokens\ngrouping (Xu et al., 2022) or learning-based (Ryoo\net al., 2021) cause non-negligible overhead and\nslow down the VL model computation, instead, we\nuse the bipartite soft matching algorithm (Bolya\net al., 2022) to find similar tokens and combine\nthem in parallel.\nHere, we explain the bipartite matching ap-\nproach in more detail. Specifically, the inputs are\na set of token vectors X (can be Vp or T) and a\nmerge ratio r, we form a bipartite graph by divid-\ning the nodes (tokens) into two disjoint sets (say E\nand O) of equal size based on their order (even or\nodd). Then, for each token in O, we find its most\nsimilar token in E, and draw an edge between the\ntoken pair (lines in the left figure in Figure 4). We\nselect the top-r‚Ä≤ edges6 based on the similarity and\nmerge their corresponding (most similar) token in\nE and O. Figure 4 shows an example of bipartite\nmatching. Since the self-attention in a VL model\nlayer already has computed keys and values for\neach token to measure similarity, following Bolya\net al. (2022), we compute the similarity as the dot\nproduct St1t2\np = Kt1 Kt2 between the keys of each\ntoken vector Xi. We keep the rest non-top- r‚Ä≤ to-\nkens in Orest and unmerged tokens in Erest. We\nalso describe this procedure in Algorithm 1.\nùë°!ùë°\"ùë°#ùë°$\nùë°%ùë°&ùë°'ùë°(\nùë°!ùë°\"&)ùë°#'$)\nùë°% ùë°(\nmergedmerged\nFigure 4: Illustration of merging by bipartite match-\ning. In this example, there are 8 tokens, E consists\nof token t1, t3, t5 and t7, O has t2, t4, t6, t8. Assume\nfor t1, t3, t5, t7 in E, the most similar tokens in O are\nt4, t4, t6, t6 respectively, andt3 ‚àít4, t7 ‚àít6, t5 ‚àít6 are\nthe edges (darker and thicker lines mean larger similar-\nity values) with top-r‚Ä≤ (r‚Ä≤ = 3) most similarity, then we\nmerge (t3, t4) into one token tm\n34, (t5, t7, t6) into one\ntoken tm\n567, and keep t1, t2, t8, in this case, we reduce\nthree (3/8=37.5%) tokens.\n4.3 Training and Inference\nToken reducers in PuMer contain no trainable pa-\nrameters and can be incorporated into off-the-shelf\nVL models without changing model architectures\nfor both training and inference. PuMer is easy to\n6r‚Ä≤ = r|X|is the number of merge tokens\n12894\ntrain and follows the same setup as finetuning orig-\ninal VL models. To reduce the accuracy drop fur-\nther, we add a knowledge distillation (Hinton et al.,\n2015) loss. During training and inference, PuMer\nhas three configurable hyperparameters (keep ra-\ntio k, merge ratios r, and t for image and text) to\ncontrol the efficiency versus accuracy trade-offs.\nImplementation Details. We set the pruning and\nmerging ratio in the range of 0.1 to 0.5 in 3 or 4\nlocations in cross-modal layers. The exact values\nare in Appendix A.1. In Section 6.2, we study the\ndesign choices for different reduction ratios and\nreduction layer locations. More implementation\nand training details are in Appendix A.1.\n5 Evaluation Setup\n5.1 Backbone Vision-Language Models\nWe evaluate PuMer for two different VL models:\nViLT (Kim et al., 2021) with 110 million parame-\nters and a state-of-the-art VL model, METER (Dou\net al., 2022) with 330 million parameters. We de-\nnote PuMer-ViLT and PuMer-METER as PuMer\napplied for ViLT and METER respectively.\nViLT is a recent efficient VL model that uses\nBERT (Devlin et al., 2019) embeddings to encode\ntext and a linear layer to project image patches.\nViLT then concatenates the text and image tokens\nand uses a 12-layer Transformer encoder to per-\nform the cross-modal fusion. ViLT is a relatively\nlightweight model and has 110 million parameters.\nMETER is a state-of-the-art VL model that uses\nRoBERTa (Liu et al., 2019) as the text encoder and\nCLIP (Radford et al., 2021) as the image encoder,\nand 12 BERT-like cross-attention layers to fuse\nthe text and image modalities. METER is a large\nmodel and has 330 million parameters.\n5.2 Evaluation Tasks\nWe evaluate the models on five vision-language\nlanguage tasks:\nImage-Text Retrieval contains two subtasks:\nimage-to-text retrieval (IR) and text-to-image re-\ntrieval (TR). We finetune PuMer and evaluate on\nthe Flickr30K (Plummer et al., 2015).\nVisual Question Answering (VQAv2) dataset\n(Goyal et al., 2017) contains over 1 million diverse\nopen-ended questions about images both from the\nMSCOCO (Lin et al., 2014) and real-world scenes.\nAnswering these questions requires an understand-\ning of vision, language, and commonsense knowl-\nedge.\nVisual Entailment (VE) (Xie et al., 2019) is a\nvisual inference task that consists of 570K sentence\nimage pairs constructed from the Stanford Natural\nLanguage Inference corpus (Bowman et al., 2015)\nand Flickr30k (Young et al., 2014). The goal is\nto predict whether the image premise semantically\nentails the text.\nNatural Language for Visual Reasoning\n(NLVR2) corpora (Suhr et al., 2019) have over\n100K examples of linguistically diverse English\nsentences written by humans and are grounded in\npairs of visually complex images. The goal is to\npredict whether a sentence is true about two input\nimages.\n5.3 Baselines\nTo compare the benefits of PuMer, we additionally\nevaluate three baselines:\nDynamicViT (Rao et al., 2021) designs several\nprediction modules parameterized by MLPs to pre-\ndict which image tokens to prune in vision trans-\nformers (Dosovitskiy et al., 2020). For a fair com-\nparison, we use the original DynamicViT config-\nurations (pruning layers and ratios) for the ViLT\nmodel.\nToMe (Bolya et al., 2022) uses token merging to\nreduce the number of tokens in vision transformers.\nWe configure ToMe to make sure similar speedup\nas PuMer and compare their accuracy.\nNote that both DynamicViT and ToMe are de-\nsigned for vision Transformers and work for image\nmodality, therefore they do not distinguish between\nthe image and text tokens. On the contrary, PuMer\nis a more general token reduction framework that\nuses text to guide the image pruning and makes\nmerging modality aware.\nSmaller Resolution (SmRes): We downsample the\ninput image to smaller resolutions and finetune the\nVL models. Using smaller input images directly\nreduces the computation of VL models.\n5.4 Evaluation Metrics\nAccuracy Metrics. We measure VQA accu-\nracy (Goyal et al., 2017) for the VQAv2 dataset and\naccuracy for both the VE and NLVR2 datasets. For\ntext retrieval (TR) and image retrieval (IR) tasks,\nthe accuracy refers to Top1-recall. Unlike previous\nworks (Kim et al., 2021; Dou et al., 2022), where\n12895\nModel Datasets Original Accuracy PuMer Accuracy Throughput Increase Memory Reduction\nMETER (SoTA)\nFlickr30k TR 94.7 93.8 (-0.9) 1.81x 38%\nFlickr30k IR 82.0 81.2 (-0.8) 1.81x 38%\nVQAv2 77.5 76.8 (-0.7) 1.82x 38%\nSNLI-VE 81.1 80.3 (-0.8) 2.07x 43%\nNLVR2 82.7 82.2 (-0.5) 1.79x 38%\nViLT\nFlickr30k TR 78.2 77.6 (-0.6) 1.78x 46%\nFlickr30k IR 60.2 59.6 (-0.7) 1.78x 46%\nVQAv2 69.5 68.9 (-0.6) 1.76x 45%\nSNLI-VE 76.0 75.6 (-0.4) 2.01x 51%\nNLVR2 75.5 74.9 (-0.6) 1.74x 45%\nTable 1: Performance and inference efficiency comparison between the original fine-tuned vs PuMer fine-tuned\nmodels for the ViLT and METER over four downstream visual reasoning tasks.\ntheir models are trained on the combined training\nand validation sets, our focus is not to obtain state-\nof-the-art results, so we train the two VL models\non the training set and report the results on the test\nset. All the accuracy numbers are average values\nacross 3 runs.\nEfficiency Metrics. We measure the actual in-\nference throughput (examples per second) of the\nVL models on the GPU hardware and compare\nthem to the original finetuned models, and we re-\nport the throughput increase. We also measure\nthe peak memory consumed during the model in-\nference phase and report memory reduction ratio\ncompared to the original finetuned models. These\ntwo runtime metrics reflect actual efficiency and\nare found to be more accurate to compare resource\nconsumption instead of using the FLOPs complex-\nity metric (Graham et al., 2021). For comparison\npurposes, we include the FLOPs comparison in the\nappendix Appendix A.2.\nFor inference throughput measurements, we in-\ncrease the batch size until the model gets out of\nGPU memory, and run the inference with the batch\nsize that gives the biggest throughput for 30 sec-\nonds on a single GPU. For inference memory foot-\nprint, we use the same batch size for the original\nVL model and PuMer version and report the peak\nmemory difference. For ViLT models, we use GTX\n1080 Ti GPU and start the batch size from 32 with\na step of 8; for METER models, we use an A40\nGPU and start the batch size from 16 with a step of\n8.\n6 Experimental Results\n6.1 Main Results\nPuMer is faster and remains accurate. Ta-\nble 1 shows the main results comparing perfor-\nmance, inference speed, and memory reduction of\n1.0 1.2 1.4 1.6 1.8 2.0\nThroughput Increase\n67.5\n68.0\n68.5\n69.0\n69.5VQA Accuracy\nDynamicViT\nT oMe\nPuMer (Ours)\nOriginal\nFigure 5: Comparing PuMer with DynamicViT and\nToMe for the ViLT model on the VQAv2 dataset. Set-\nting different pruning and merging ratios for Dynam-\nicViT and ToMe gives different inference throughput\nand accuracy numbers. Right and top lines are better\ntrade-offs.\nPuMer versus the original models. Overall, we ob-\nserve over 1.7x ‚àº2x speedup in inference through-\nput and over 35% ‚àº51% reduction in memory\nfootprint for both ViLT and METER models on\nthe VL tasks. Importantly, the task performance of\nPuMer remains competitive compared to the orig-\ninal finetuned VL models with only <1% drop in\naccuracy.\nPuMer is more accurate and faster than previ-\nous token reduction methods. Figure 5 presents\nthe accuracy versus inference throughput increase\ntrade-offs for PuMer, DynamicViT and ToMe ap-\nplied to the ViLT model on the VQAv2 dataset.\nGiven a similar throughput increase (like 1.8x),\nPuMer has the best accuracy compared to Dynam-\nicViT and ToMe. Similarly, for a given accuracy\ndrop constraint (like < 1%), PuMer provides a big-\nger throughput increase.\n12896\nModel Image VQAv2 Throughput Memory\nResolution Accuracy Increase Reduction\nResolution\n192x192 74.3 (-3.2) 4.23x 75%\n224x224 75.2 (-2.3) 3.48x 66%\n256x256 76.1 (-1.4) 2.67x 54%\n320x320 77.0 (-0.5) 1.62x 37%\nPuMer 320x320 76.3 (-1.2) 2.86x 59%\nPuMer 384x384 76.8 (-0.7) 1.82x 38%\nOriginal 384x384 77.5 1x 0%\nTable 2: Performance and inference efficiency compari-\nson between the smaller resolution baselines and PuMer\nfor the METER model on the VQAv2 test set.\nPuMer provides larger efficiency gains over\nsmaller resolution baselines. Table 2 shows\nthe results for the METER model on the VQAv2\ndataset when comparing PuMer with downsam-\npling the input image to smaller resolutions. Using\nsmaller resolution input images improves the infer-\nence throughput and reduces memory footprint but\ncomes with larger accuracy drops. The closest res-\nolution is 320x320 which is slightly more (0.2%)\naccurate than PuMer, but it has 20% lower infer-\nence throughput. Meanwhile, PuMer is orthogonal\nto downsampling strategies, and applying PuMer\nto smaller images could provide additional effi-\nciency gains; for input image resolution 320x320,\nPuMer improves METER throughput by 1.76x with\na 0.7% accuracy drop7 (see the 3rd row numbers in\nTable 2).\n6.2 Ablation Study\nModel VQA Throughput\nAccuracy Increase\nViLT 69.5 1x\nPuMer-ViLT 68.9 (-0.6) 1.76x\nw/o text-informed image pruning 69.2 (-0.3) 1.52x\nw/o modality-aware merging 69.1 (-0.4) 1.46x\nw/o distillation 68.6 (-0.9) 1.76x\nTable 3: Ablation analysis for each component in PuMer\non the VQAv2 dataset for ViLT model.\nEffectiveness of PuMer Components. To show\nhow each component in PuMer affects the VL task\naccuracy and model inference efficiency, we ab-\nlate the three components ‚Äî text-informed image\npruning, modality-aware merging and distillation\n‚Äî in Table 3. Applying text-informed image prun-\ning or modality-aware merging individually has\n71.76=2.86/1.62, 0.7=77.0-76.3\nshown improvements in model inference through-\nput with smaller accuracy loss. But stacking the\ntwo techniques together provides bigger inference\nefficiency without losing much task performance.\nWithout knowledge distillation, PuMer is still ac-\ncurate and fast and adding it further reduces the\nperformance gap.\nToken Reduction Design Choices. Given a 12-\nlayer VL cross-modal encoder like ViLT, many\ncombinations of reduction locations and ratios\nachieve similar inference speedups. Reducing to-\nkens at earlier layers with lower ratios has similar\ncomputation efficiency to pruning at later layers\nwith higher ratios. For comparing the accuracy\nwith different numbers of reduction layers, we con-\ntrol the inference throughput to be similar to PuMer\nby selecting the pruning and merging ratios and lo-\ncations. Table 4 shows cascaded reduction at 4\nlayers (2th, 4th, 6th, 8th) has higher accuracy and\nspeedups.\nThe ratios row in Table 4 shows reducing (via\npruning or merging) more tokens leads to a bigger\nthroughput increase but has a significant ( >1%)\naccuracy drop while reducing fewer tokens is more\naccurate but causes lower throughput. As shown\nin the locations row, we find that reducing tokens\nin the earlier layers leads to bigger throughput but\ndrops accuracy by 1.8%, while reducing tokens in\nthe later layers is slightly more accurate but pro-\nvides fewer benefits in throughput. Overall, for\nViLT on the SNLI-VE task, we choose a 4-layer\ncascaded token reduction strategy with a pruning\nratio of 0.1 and merging ratio of 0.3 and 0.2 for\nimage and text respectively, and scatter the reduc-\ntion locations more evenly to balance accuracy and\nspeed trade-offs.\n7 Conclusion\nLarge vision language models have been effective\nat visual reasoning tasks due to their complex cross-\nmodal interactions between the text and image to-\nkens. These cross-modal interactions are compu-\ntationally expensive because all image and text to-\nkens are processed in many layers. We introduce\na token reduction framework ‚Äî PuMer that uses\ntext-informed image pruning and modality-aware\nmerging techniques to effectively reduce the image\nand text tokens inside cross-modal layers. PuMer\nprogressively removes the redundant image and\ntext information and makes VL models run faster\nwith minimal task performance drop. PuMer is\n12897\nChoice Reduction Layers Prune Ratio Image Merge Ratio Text Merge Ratio VE Accuracy Throughput Increase\nratios\n2,5,8 0.1 0.3 0.2 75.8 (-0.2) 1.77x\n2,5,8 0.3 0.3 0.2 74.7 (-1.3) 2.04x\n2,5,8 0.1 0.3 0.5 74.9 (-1.1) 1.89x\n2,5,8 0.1 0.5 0.2 73.8 (-2.1) 2.12x\n# of layers\n2 0.1 0.3 0.2 75.9 (-0.15) 1.43x\n2,4 0.1 0.3 0.2 75.8 (-0.2) 1.69x\n2,4,6 0.1 0.3 0.2 75.7 (-0.3) 1.80x\nlocations 2,3,4 0.2 0.2 0.2 74.2 (-1.8) 2.03x\n7,8,9 0.2 0.2 0.2 75.9 (-0.1) 1.31x\nPuMer (Ours) 2,4,6,8 0.1 0.3 0.2 75.6 (-0.4) 2.01x\nViLT - - - - 76.0 1.00x\nTable 4: Design choices analysis of prune and merge ratios, # of reduction layers, and reduction locations for the\nViLT model on SNLI-VE task.\neasy to train and speeds up both training and infer-\nence of vision and language models across diverse\ndownstream visual reasoning tasks.\nAcknowledgements\nThis research was supported partly by NSF IIS-\n2044660, an Allen Investigator Distinguished\naward. We thank the anonymous reviewers and\nthe members of the UW NLP group for their com-\nments and feedback on this paper.\n8 Limitations\nOur method does not apply to VL models where\nthe cross-modal encoder layers are relatively\nlightweight. For example, the vision encoder\nis much more computationally expensive than\nthe cross-modal encoder for VL models like AL-\nBEF (Li et al., 2021) and X-VLM (Zeng et al.,\n2021), therefore, the end to end inference speed im-\nprovement is marginal. Reducing the image tokens\ninside the vision encoder could further improve\nthe model efficiency, we leave this exploration to\nfuture work.\nReferences\nTPrune: Efficient Transformer Pruning for Mobile De-\nvices: ACM Transactions on Cyber-Physical Sys-\ntems: V ol 5, No 3.\n2022. Deepspeed.\n2022. huggingface/accelerate.\nDaniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao\nZhang, Christoph Feichtenhofer, and Judy Hoff-\nman. 2022. Token Merging: Your ViT But Faster.\nArXiv:2210.09461 [cs].\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632‚Äì642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nQingqing Cao, Prerna Khanna, Nicholas D. Lane, and\nAruna Balasubramanian. 2022. MobiVQA: Efficient\nOn-Device Visual Question Answering. Proceedings\nof the ACM on Interactive, Mobile, Wearable and\nUbiquitous Technologies, 6(2):44:1‚Äì44:23.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2020. An Image\nis Worth 16x16 Words: Transformers for Image\nRecognition at Scale.\nZi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang,\nShuohang Wang, Lijuan Wang, Chenguang Zhu,\nPengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng\nLiu, and Michael Zeng. 2022. An empirical study of\ntraining end-to-end vision-and-language transform-\ners. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR),\npages 18166‚Äì18176.\nZhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan\nWang, Yezhou Yang, and Zicheng Liu. 2021. Com-\npressing Visual-Linguistic Model via Knowledge Dis-\ntillation. pages 1428‚Äì1438.\n12898\nSaurabh Goyal, Anamitra Roy Choudhury, Saurabh\nRaje, Venkatesan Chakaravarthy, Yogish Sabharwal,\nand Ashish Verma. 2020. PoWER-BERT: Acceler-\nating BERT Inference via Progressive Word-vector\nElimination. In Proceedings of the 37th International\nConference on Machine Learning, pages 3690‚Äì3699.\nPMLR. ISSN: 2640-3498.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in VQA\nMatter: Elevating the Role of Image Understanding\nin Visual Question Answering. pages 6904‚Äì6913.\nBenjamin Graham, Alaaeldin El-Nouby, Hugo Tou-\nvron, Pierre Stock, Armand Joulin, Herv√© J√©gou, and\nMatthijs Douze. 2021. LeViT: A Vision Transformer\nin ConvNet‚Äôs Clothing for Faster Inference. pages\n12259‚Äì12269.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the Knowledge in a Neural Network.\nArXiv:1503.02531 [cs, stat].\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. ViLT:\nVision-and-Language Transformer Without Convo-\nlution or Region Supervision. In Proceedings of the\n38th International Conference on Machine Learning,\npages 5583‚Äì5594. PMLR. ISSN: 2640-3498.\nZhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng,\nMengshu Sun, Wei Niu, Xuan Shen, Geng Yuan,\nBin Ren, Minghai Qin, Hao Tang, and Yanzhi Wang.\n2022. SPViT: Enabling Faster Vision Transformers\nvia Soft Token Pruning. ArXiv:2112.13890 [cs].\nFran√ßois Lagunas, Ella Charlaix, Victor Sanh, and\nAlexander Rush. 2021. Block Pruning For Faster\nTransformers. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 10619‚Äì10629, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nIn Advances in neural information processing sys-\ntems, volume 34, pages 9694‚Äì9705. Curran Asso-\nciates, Inc.\nYouwei Liang, Chongjian Ge, Zhan Tong, Yibing Song,\nJue Wang, and Pengtao Xie. 2021. EViT: Expediting\nVision Transformers via Token Reorganizations.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\nand C. Lawrence Zitnick. 2014. Microsoft COCO:\nCommon Objects in Context. In Computer Vision\n‚Äì ECCV 2014, Lecture Notes in Computer Science,\npages 740‚Äì755, Cham. Springer International Pub-\nlishing.\nWeijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao,\nHaotang Deng, and Qi Ju. 2020. FastBERT: a Self-\ndistilling BERT with Adaptive Inference Time. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6035‚Äì\n6044, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. RoBERTa: A Robustly Optimized BERT\nPretraining Approach. Number: arXiv:1907.11692\narXiv:1907.11692 [cs].\nDmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan,\nAnish Prabhu, Mohammad Rastegari, and Oncel\nTuzel. 2021. Token Pooling in Vision Transform-\ners. ArXiv:2110.03860 [cs].\nPiotr Nawrot, Jan Chorowski, Adrian ≈Åa ¬¥ncucki, and\nEdoardo M. Ponti. 2022. Efficient Transformers with\nDynamic Token Pooling. ArXiv:2211.09761 [cs]\nversion: 1.\nMicha≈Ç Pietruszka, ≈Åukasz Borchmann, and Filip\nGrali¬¥nski. 2020. Sparsifying Transformer Mod-\nels with Differentiable Representation Pooling.\narXiv:2009.05169 [cs]. ArXiv: 2009.05169.\nBryan A. Plummer, Liwei Wang, Chris M. Cervantes,\nJuan C. Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k Entities: Collecting\nRegion-to-Phrase Correspondences for Richer Image-\nto-Sentence Models. In 2015 IEEE International\nConference on Computer Vision (ICCV), pages 2641‚Äì\n2649. ISSN: 2380-7504.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning Transferable Visual Models From Natural Lan-\nguage Supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, pages\n8748‚Äì8763. PMLR. ISSN: 2640-3498.\nYongming Rao, Wenliang Zhao, Benlin Liu, Jiwen\nLu, Jie Zhou, and Cho-Jui Hsieh. 2021. Dynam-\nicViT: Efficient Vision Transformers with Dynamic\nToken Sparsification. arXiv:2106.02034 [cs]. ArXiv:\n2106.02034.\nMichael S. Ryoo, A. J. Piergiovanni, Anurag Arnab,\nMostafa Dehghani, and Anelia Angelova. 2021. To-\nkenLearner: What Can 8 Learned Tokens Do for\nImages and Videos?\nRoy Schwartz, Gabriel Stanovsky, Swabha\nSwayamdipta, Jesse Dodge, and Noah A. Smith.\n2020. The Right Tool for the Job: Matching\nModel and Instance Complexities. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 6640‚Äì6651,\nOnline. Association for Computational Linguistics.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A Corpus for\n12899\nReasoning about Natural Language Grounded in Pho-\ntographs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6418‚Äì6428, Florence, Italy. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998‚Äì6008.\nJianfeng Wang, Xiaowei Hu, Pengchuan Zhang, Xi-\nujun Li, Lijuan Wang, Lei Zhang, Jianfeng Gao,\nand Zicheng Liu. 2020. MiniVLM: A Smaller and\nFaster Vision-Language Model. arXiv:2012.06946\n[cs]. ArXiv: 2012.06946.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jin-\ngren Zhou, and Hongxia Yang. 2022. OFA: Unify-\ning Architectures, Tasks, and Modalities Through\na Simple Sequence-to-Sequence Learning Frame-\nwork. Technical Report arXiv:2202.03052, arXiv.\nArXiv:2202.03052 [cs] version: 2 type: article.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai,\nYulia Tsvetkov, and Yuan Cao. 2021. SimVLM: Sim-\nple Visual Language Model Pretraining with Weak\nSupervision.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38‚Äì45, Online. Association\nfor Computational Linguistics.\nNing Xie, Farley Lai, Derek Doran, and Asim Ka-\ndav. 2019. Visual Entailment Task for Visually-\nGrounded Language Learning. Technical Report\narXiv:1811.10582, arXiv. ArXiv:1811.10582 [cs]\ntype: article.\nJi Xin, Rodrigo Nogueira, Yaoliang Yu, and Jimmy Lin.\n2020. Early Exiting BERT for Efficient Document\nRanking. In Proceedings of SustaiNLP: Workshop on\nSimple and Efficient Natural Language Processing,\npages 83‚Äì88, Online. Association for Computational\nLinguistics.\nJiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,\nThomas Breuel, Jan Kautz, and Xiaolong Wang.\n2022. GroupViT: Semantic Segmentation Emerges\nFrom Text Supervision. pages 18134‚Äì18144.\nHongxu Yin, Arash Vahdat, Jose M. Alvarez, Arun\nMallya, Jan Kautz, and Pavlo Molchanov. 2022. A-\nvit: Adaptive tokens for efficient vision transformer.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n10809‚Äì10818.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67‚Äì78.\nPlace: Cambridge, MA Publisher: MIT Press.\nFang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei\nChu, and Li Cui. 2022. Width & depth pruning for\nvision transformers. In AAAI Conference on Artificial\nIntelligence (AAAI), volume 2022.\nHao Yu and Jianxin Wu. 2021. A Unified\nPruning Framework for Vision Transformers.\narXiv:2111.15127 [cs]. ArXiv: 2111.15127.\nYan Zeng, Xinsong Zhang, and Hang Li. 2021. Multi-\nGrained Vision Language Pre-Training: Aligning\nTexts with Visual Concepts.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. VinVL: Revisiting Visual Repre-\nsentations in Vision-Language Models. pages 5579‚Äì\n5588.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. BERT loses\npatience: Fast and robust inference with early exit. In\nAdvances in neural information processing systems,\nvolume 33, pages 18330‚Äì18341. Curran Associates,\nInc.\n12900\nA Appendix\nA.1 PuMer Details\nImplementation. We use the Transformers (Wolf\net al., 2020) and Accelerate (Hug, 2022) with Deep-\nSpeed (Dee, 2022) library to implement the training\ntasks. We conduct training jobs on 4 Nvidia A100\nGPUs. For both ViLT and METER model, we first\nfollow the training hyperparameters in their orig-\ninal papers and finetune the pretrained model to\nobtain task-specific models. These models are used\nas baselines for measuring accuracy drop and also\nused as the teacher model for PuMer distillation.\nFor baseline VL models, we finetune both METER\nand ViLT models on the studied VL tasks for 10\nepochs. For PuMer, we finetune 20 epochs using\nearly stopping with a penitence of 5 (the accuracy\nwon‚Äôt improve after 5 epochs).\nWe list all training hyperparameters in Table 5.\nMETER Retrieval VQAv2 NLVR2 SNLI-VE\ncross-modal lr 2.5e-5 2.5e-5 5e-5 1e-5\nclassifier lr 2.5e-5 2.5e-4 1e-4 2e-5\nbatch size per gpu 32 32 16 32\nimage size 384 384 288 384\npatch size 16 16 16 16\nViLT Retrieval VQAv2 NLVR2 SNLI-VE\ncross-modal lr 1e-4 1e-4 1e-4 1e-4\nclassifier lr 1e-4 1e-3 1e-4 1e-3\nbatch size per gpu 32 64 32 64\nimage size 384 384 384 384\npatch size 32 32 32 32\nTable 5: Hyperparameters for finetuning PuMer and\noriginal VL models.\nWe list the default reduction layers and ratios for\ndifferent VL tasks in Table 6.\nMETER VQAv2 NLVR2 SNLI-VE Retrieval\nReduction Layers 0,2,4,6 2,4,6 0,2,4,6 2,4,6\nPrune Ratio 0.2 0.3 0.3 0.2\nImage Merge Ratio 0.2 0.5 0.5 0.5\nText Merge Ratio 0.2 0.2 0.2 0.2\nViLT VQAv2 NLVR2 SNLI-VE Retrieval\nReduction Layers 2,5,8 2,5,8 2,4,6,8 2,5,8\nPrune Ratio 0.1 0.1 0.1 0.1\nImage Merge Ratio 0.3 0.3 0.3 0.3\nText Merge Ratio 0.2 0.2 0.2 0.2\nTable 6: Reduction layers and ratios for PuMer-METER\nand PuMer-ViLT on the VL tasks.\nModel Datasets Original PuMer Speedup\nMETER\nVQAv2 92 64.7 1.42x\nSNLI-VE 92 59 1.56x\nNLVR2 184 131 1.40x\nViLT\nVQAv2 16 8.7 1.84x\nSNLI-VE 16 7.7 2.08x\nNLVR2 32 17.4 1.84x\nTable 7: GFLOPs comparison between PuMer and orig-\ninal VL models for METER and ViLT.\nA.2 Model Inference FLOPs Comparison\nWe measure FLOPs of both PuMer and the original\nmodel for METER and ViLT using the fvcore tool8.\nThe results are shown in Table 7.\n8https://github.com/facebookresearch/\nfvcore/blob/main/docs/flop_count.md\n12901\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nsection 8\n‚ñ° A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nSection 1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ° Did you use or create scientiÔ¨Åc artifacts?\nNot applicable. Left blank.\n‚ñ° B1. Did you cite the creators of artifacts you used?\nNo response.\n‚ñ° B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n‚ñ° B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n‚ñ° B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n‚ñ° B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n‚ñ° B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nNo response.\nC ‚ñ°\u0013 Did you run computational experiments?\nsection 5 and 6\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nsection 5 and appendix\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n12902\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nsection 5 and appendix\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nsection 5\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nappendix\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n12903",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8296654224395752
    },
    {
      "name": "Security token",
      "score": 0.7580727338790894
    },
    {
      "name": "Memory footprint",
      "score": 0.7148920297622681
    },
    {
      "name": "Inference",
      "score": 0.6005526185035706
    },
    {
      "name": "Modal",
      "score": 0.5716736316680908
    },
    {
      "name": "Language model",
      "score": 0.5352115631103516
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5196120142936707
    },
    {
      "name": "Transformer",
      "score": 0.4589771330356598
    },
    {
      "name": "Pruning",
      "score": 0.430935800075531
    },
    {
      "name": "Natural language processing",
      "score": 0.4265753924846649
    },
    {
      "name": "Computer vision",
      "score": 0.34215453267097473
    },
    {
      "name": "Programming language",
      "score": 0.13459128141403198
    },
    {
      "name": "Voltage",
      "score": 0.08283624053001404
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ],
  "cited_by": 16
}