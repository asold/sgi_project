{
  "title": "Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?",
  "url": "https://openalex.org/W4205410664",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3093982013",
      "name": "Arij Riabi",
      "affiliations": [
        "Université Paris 1 Panthéon-Sorbonne",
        "Institut national de recherche en informatique et en automatique",
        "Sorbonne University Abu Dhabi",
        "Sorbonne Université"
      ]
    },
    {
      "id": "https://openalex.org/A1632339985",
      "name": "Benoît Sagot",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique"
      ]
    },
    {
      "id": "https://openalex.org/A249347002",
      "name": "Djamé Seddah",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6603175991",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W4300973022",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W79543249",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W331019419",
    "https://openalex.org/W3093721400",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2998554035",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W3122044994",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W3115462295",
    "https://openalex.org/W3210115313",
    "https://openalex.org/W3113622461",
    "https://openalex.org/W3105069964",
    "https://openalex.org/W3193875190",
    "https://openalex.org/W4394643672",
    "https://openalex.org/W2970193165",
    "https://openalex.org/W1488557242",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4287607775",
    "https://openalex.org/W3011279327",
    "https://openalex.org/W3174418826",
    "https://openalex.org/W2970365965",
    "https://openalex.org/W2147272182",
    "https://openalex.org/W2970674617",
    "https://openalex.org/W2101761627",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3095771422",
    "https://openalex.org/W1014376541",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W4310918692",
    "https://openalex.org/W3035283133",
    "https://openalex.org/W2749874290",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2563351168",
    "https://openalex.org/W2529763891",
    "https://openalex.org/W2270364989",
    "https://openalex.org/W1518819622",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3166790124"
  ],
  "abstract": "Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages remains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Arabic written using an extension of the Latin script, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability set- tings.",
  "full_text": "Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 423–436\nNovember 11, 2021. ©2021 Association for Computational Linguistics\n423\nCan Character-based Language Models Improve Downstream Task\nPerformance in Low-Resource and Noisy Language Scenarios?\nArij Riabi1,2 Benoît Sagot1 Djamé Seddah1\n1 Inria Paris\n2 Sorbonne Université\n{arij.riabi,benoit.sagot,djame.seddah}@inria.fr\nAbstract\nRecent impressive improvements in NLP,\nlargely based on the success of contextual\nneural language models, have been mostly\ndemonstrated on at most a couple dozen high-\nresource languages. Building language mod-\nels and, more generally, NLP systems for non-\nstandardized and low-resource languages re-\nmains a challenging task. In this work, we fo-\ncus on North-African colloquial dialectal Ara-\nbic written using an extension of the Latin\nscript, called NArabizi, found mostly on so-\ncial media and messaging communication. In\nthis low-resource scenario with data display-\ning a high level of variability, we compare the\ndownstream performance of a character-based\nlanguage model on part-of-speech tagging and\ndependency parsing to that of monolingual and\nmultilingual models. We show that a character-\nbased model trained on only 99k sentences of\nNArabizi and ﬁned-tuned on a small treebank\nof this language leads to performance close to\nthose obtained with the same architecture pre-\ntrained on large multilingual and monolingual\nmodels. Conﬁrming these results a on much\nlarger data set of noisy French user-generated\ncontent, we argue that such character-based\nlanguage models can be an asset for NLP in\nlow-resource and high language variability set-\ntings.\n1 Introduction\nCurrent state-of-the-art monolingual and multi-\nlingual language models require large amounts of\ndata to be trained, showing limited performance on\nlow-resource languages (Howard and Ruder, 2018;\nDevlin et al., 2019). They lead to state-of-the-art\nresults on most NLP tasks (Devlin et al., 2018;\nRaffel et al., 2020). In order to achieve high perfor-\nmance, these models rely on transfer learning archi-\ntectures: the language models need to be trained on\nFirst version submitted on August 27th, 2021. Final on\nOctober 1st, 2021.\nlarge amounts of data (pre-training) to be able to\ntransfer the acquired knowledge to a downstream\ntask via ﬁne-tuning on a relatively small number\nof examples, resulting in a signiﬁcant performance\nimprovement with respect to previous approaches.\nThis dependency on large data sets for pre-training\nis a severe issue for low-resource languages, de-\nspite the emergence of large and successful multi-\nlingual pre-trained language models (Muller et al.,\n2021b). This is especially the case for languages\nwith unusual morphological and structural features,\nwhich struggle to take advantage from similari-\nties with high-resource, well represented languages\nsuch as Romance and Germanic languages.\nIn this work, we focus on one of such highly\nchallenging languages, namely North-African di-\nalectal Arabic. Its Latin transcription (Arabizi) dis-\nplays a high level of linguistic variability1, on top\nof scarce and noisy resource availability, making it\na particularly challenging language for most NLP\nsystems relying on pre-trained multilingual mod-\nels (Muller et al., 2020). 2 To tackle the resource\nscarcity issue regarding Arabic dialects, Antoun\net al. (2020) use BERT architecture (Devlin et al.,\n2019) to train a model on Arabic text to compare\nthis approach to standard multilingual models. In-\ndeed, Martin et al. (2020) show that ﬁne-tuning\na monolingual model leads to better results than\nﬁne-tuning a multilingual one, meaning that when\nﬁne-tuning is used there is no signiﬁcant perfor-\nmance improvement from cross-lingual transfer\nduring pre-training. However, such model is still\npre-trained on sentences written in a single lan-\nguage and was not trained to handle the presence\n1Language variability, or language variation, is a term\ncoming from socio-linguistics where, as stated by Nordquist\n(2019), it refers to regional, social or contextual differences in\nthe ways that a particular language is used. These variations\nin user-generated content can be characterized through their\nprevalent idiosyncraisies when compared to canonical texts\n(Seddah et al., 2012a; Sanguinetti et al., 2020).\n2Following Seddah et al. (2020), we refer to the Arabizi\nversion of North-African Arabic dialects as NArabizi.\n424\nof multiple languages in the same sentence (code-\nswitching), a frequent phenomenon in NArabizi.\nHowever both monolingual and multilingual\nmodel approaches bear the risk of being lim-\nited by a subword tokenization-based vocabu-\nlary when facing out-of-domain training data lan-\nguage, especially in high-variability noisy scenar-\nios (El Boukkouri et al., 2020; Clark et al., 2021),\neven though Muller et al. (2020) demonstrated a\npositive effect for NArabizi when using target lan-\nguage data to ﬁne-tune a multilingual language\nmodel on its own objective function before pre-\ntraining.\nFollowing a different approach, we investigate\nthe use of a recently issued character-based lan-\nguage model (El Boukkouri et al., 2020) that was\nshown to display a remarkable robustness to lexi-\ncal variation and noise when facing a new distant\ndomain, namely biomedical. The pipeline we de-\nveloped is simple and consists in ﬁne-tuning this\ncharacter-based model for several tasks in a noisy\nlow-resource language scenario. We show that a\ncharacter-based model trained on only 99k sen-\ntences of NArabizi and ﬁned-tuned on a small tree-\nbank of the language leads to performance close\nto that obtained with the same architecture pre-\ntrained on large multilingual and monolingual mod-\nels (mBERT and CamemBERT).\nInterestingly, we generalize this observation\nby using the same architecture on a much larger\nFrench user-generated Content treebank that ex-\nhibits similar language variability issues thanNAra-\nbizi. In fact, pre-training a character-based model\non 1% of the large-scale French instance of the\nmultilingual corpus OSCAR leads to similar per-\nformance as a subword based model trained on the\nfull corpus, showing that such character-based lan-\nguage model can reach similar performance levels\nand that the resulting models exhibit the same tol-\nerance to noise as their much larger BERT counter-\nparts. This demonstrates the value of such models\nin very scarce resource scenario. Our code and\nmodels are freely available.3\n2 NArabizi: A Challenging Use Case for\nNLP in Low-resource Scenarios\nAs the ofﬁcial language of 25 countries, Arabic\nshowcases a linguistic phenomenon called diglos-\nsia (Habash, 2010). It means that the speakers use\n3https://gitlab.inria.fr/ariabi/chara\ncter-bert-ugc\nModern Standard Arabic (MSA) for formal and\nofﬁcial situations but use other forms of Arabic in\ninformal situations. These dialectal forms consti-\ntute a dialect continuum with large variability from\none country to the other. Arabic dialects are de-\nﬁned by their spoken form and often exhibit a lack\nof standardized spelling when written. When Ara-\nbic speakers produce written text in such dialects,\nthey merely transcribe their spoken, colloquial lan-\nguage, which leads to different forms for the same\nword. Many users use the Latin script to express\nthemselves online in their dialect (Seddah et al.,\n2020). In particular, they transcribe phonemes that\ncannot be straightforwardly mapped to a Latin let-\nter using digits and symbols,4 with a high degree\nof variability at all levels; this is called Arabizi,\nwith its North-African version called NArabizi by\nSeddah et al. (2020). For cultural and historical\nreasons, NArabizi also exhibits a high degree of\ncode-switching with French and Amazigh (Ama-\nzouz et al., 2017), i.e. alternations between two or\nmore languages during the conversation. Besides,\nthe only available textual resources for Arabizi data\nare user-generated content, which is by itself inher-\nently noisy (Foster, 2010; Seddah et al., 2012a;\nEisenstein, 2013), making the production of super-\nvised models, assuming the availability of labeled\ndata, or even the collection of large pre-training\ndata set a rather difﬁcult task.\nThis data scarcity problem is often solved in\nthe NLP literature using transfer learning: trans-\nferring knowledge learnt by large scale language\nmodels pre-trained on larger corpora. However,\nthe use of Latin script makes it harder to transfer\nknowledge from language models trained on Ara-\nbic script corpora. For subword-based tokenization\nmodels, words are represented by a combination\nof subwords from a predeﬁned list. When applied\nto a highly variable language with code-switching,\na large vocabulary would be necessary to get a\ngood covering of all possible orthographic vari-\nations which makes this approach less practical.\nTable 1 presents several examples of lexical varia-\ntion within NArabizi. Interestingly, this variability\nalso affects the code-switched vocabulary, which\nis mostly French in this case.\n4For example, the digit “3” is often used to denote the ayin\nconsonant, because it is graphically similar to its rendition in\nArabic script.\n425\nGLOSS ATTESTED FORMS LANG\nwhy wa3lach w3alh 3alach NArabizi\nall ekl kal kolach koulli kol NArabizi\nmany beaucoup boucoup bcp Code-switched Fr.\nTable 1: Examples of lexical variation in NArabizi.\n(From Seddah et al., 2020)\n3 Related work\n3.1 Transfer Learning for Low-resource\nLanguages\nLow-resource languages, by deﬁnition, face a lack\nof textual resources – annotated or not –, which\nmakes it difﬁcult for the NLP community to de-\nvelop models and systems adapted to them (Joshi\net al., 2020). The majority of the almost-7000\nlanguages worldwide actually fall into the “low-\nresource” category. This makes the development\nof systems for low-resource languages necessary\nto widen the accessibility of NLP technology.\nFor deep learning approaches, which depend on\nthe availability of large data sets, the solution to\nthe low-resource problem comes from the idea of\ntransfer learning. Early instances of cross-lingual\ntransfer learning rely on non-contextualised word\nembeddings (Ammar et al., 2016). More recently,\nmultilingual pre-trained language models (Con-\nneau et al., 2020) has spread far and wide in NLP,\nenabling high-performance zero-shot cross-lingual\ntransfer for numerous tasks and languages. The\nmain idea is to exploit a large amount of unlabeled\ndata to pre-train a model using a self-supervised\ntask, such as masked language modeling (Lam-\nple and Conneau, 2019; Vania et al., 2019). This\npre-trained model is then ﬁne-tuned on a much\nsmaller annotated data set and used for another lan-\nguage, domain or task. Strategic knowledge shar-\ning has been shown to improve the performance\non downstream tasks and languages (Gururangan\net al., 2020). Therefore, this technique is crucial for\nmultilingual applications, as most of the world’s\nlanguages lack large amount of labeled data (Con-\nneau et al., 2019; Eisenschlos et al., 2019; Joshi\net al., 2020). However the performance of multi-\nlingual language model on low-resource languages\nis still limited compared to other languages since\nthey are naturally under-sampled during the train-\ning process (Wu and Dredze, 2020).\nTo improve performance on a speciﬁc low-\nresource languages, there are two possibilities. Ei-\nther to attempt to train a language model on it from\nscratch despite the scarceness of data; or ﬁne-tune a\npre-trained multilingual language model on the low-\nresource language corpus, also called cross-lingual\ntransfer learning (Muller et al., 2021a). The ﬁrst\noption can sometimes lead to decent performance,\nprovided that the training corpus is diverse enough\n(Martin et al., 2020).\nWhen following the ﬁne-tuning approach, unsu-\npervised methods can be implemented to facilitate\nthe transfer of knowledge (Pfeiffer et al., 2020).\nThe most widely used unsupervised ﬁne-tuning\ntask is masked language modeling (MLM). This\nsystem has proven its efﬁciency between languages\nthat have already been seen in the training cor-\npus (Pires et al., 2019); it is still a challenge for\nunseen languages, especially low-resource ones.\nHowever, Muller et al. (2020) achieved promising\nresults by performing unsupervised ﬁne-tuning on\nsmall amounts of NArabizi data. We follow their\napproach by comparing the performance of our\npipeline in two setups: MODEL +MLM+T ASK and\nMODEL +TASK . We describe these setups in more\ndetails in section 6.\n3.2 Tokenization & Character-based models\nStandard language models rely on a subword-based\napproach to process tokens in a sequence (Kudo,\n2018a). This allows the model to handle any word\nunseen in the training data, working in an “open-\nvocabulary setting,” where words are represented\nby a combination of subwords from a pre-deﬁned\nlist. On top of alleviating the issue of out-of-\nvocabulary words, this approach allows the model\nto handle sequences written in a language unseen\nduring training, as long as it uses the same script.\nTherefore, subword tokenization is a crucial fea-\nture of state-of-the-art models in NLP. But its suit-\nability for all types of data has always been ques-\ntioned. While splitting texts into subwords based\non their frequencies works well for English, mod-\nels using this kind of tokenization struggle with\nnoise, whether it is naturally present in the data\n(Sun et al., 2020) or artiﬁcially generated to chal-\nlenge the model (Pruthi et al., 2019). Moreover,\nlanguage models that use subword-based tokeniza-\ntion struggle to represent rare words (Schick and\nSchütze, 2020). Many research projects have fo-\ncused on improving subword tokenization. For ex-\nample, Wang et al. (2021) suggested a multi-view\nsubword regularization based on the sampling of\nmultiple segmentations of the input text, based on\n426\nthe work of Kudo (2018b).\nOther parallel efforts bid on character-based\nmodels. For example, El Boukkouri et al. (2020)\nproposed a possible solution to get a better tok-\nenization system more resilient to orthographic\nvariations and noise in the data set by using a\ncharacter-level model, inspired by a previous word-\nlevel open-vocabulary system (Peters et al., 2018a).\nThis new model gets better results than vanilla\nBERT on multiple tasks from the medical domain.\nFurthermore, the authors claim that it is more ro-\nbust to noise and misspellings. In the same vein,\nMa et al. (2020a) combined character-aware and\nsubword-based information to improve robustness\nto spelling errors. This initiated a new wave of\ntokenizer-free models based on characters or bytes\n(Tay et al., 2021; Xue et al., 2021; Clark et al.,\n2021).\nThe question of knowing if character-based lan-\nguage models can handle high language variability\nsince they are supposed to be resilient to noise and\nspelling variations is crucial when dealing with\nnon-normalized dialects and non-canonical forms\nof language as found on many user-generated con-\ntent platforms. This is why we focus on this work\non the analysis of the performance of character-\nbased models on several user-generated content\ndata sets that we now describe.\n4 Data sets\nIn this section, we describe the data sets we use\nto evaluate our pipeline on our downstream tasks,\nnamely Part-Of-Speech (POS) tagging and depen-\ndency parsing.\nNArabizi Data Set We use theNArabizi treebank\n(Seddah et al., 2020), containing about 1500 sen-\ntences randomly sampled from the romanized Al-\ngerian dialectal Arabic corpus of Cotterell et al.\n(2014) and from a small corpus of lyrics com-\ning from Algerian dialectal Arabic songs popu-\nlar among the younger generation. This treebank\nis manually annotated with morpho-syntactic in-\nformation (parts-of-speech and morphological fea-\ntures), together with glosses and code-switching\nlabels at the word level, as well as sentence-level\ntranslations. Moreover, this treebank also contains\n36% of French tokens. Within the NArabizi anno-\ntated corpus,5 In addition to labeled data, TheNAra-\nbizi treebanks provides about 2 millions words, 99k\n5http://almanach-treebanks.fr/NArabizi\nTreebank # Tokens # Sentences Genres\nGSD 389,363 16,342 Blogs, News\nReviews, Wiki\nSequoia 68,615 3,099 Medical, News\nNon-ﬁction, Wiki\nSpoken 34,972 2,786 Spoken\nFSMB 56,009 4,055 Twitter, Facebook\nWeb Forums\nTable 2: Statistics on the treebanks used in our POS\ntagging and dependency parsing experiments.\nsentences, of unlabeled data collected from vari-\nous sources.6 We use this corpus for unsupervised\nﬁne-tuning.\nFrench Data Sets We use the following Univer-\nsal Dependencies, UD, (Nivre et al., 2020) version\nof the following treebanks: French GSD (McDon-\nald et al., 2013), Sequoia (Candito and Seddah,\n2012) and Spoken, an automatic conversion of the\nRhapsodie corpus (Lacheret et al., 2014) to the UD\nannotation scheme.\nFor our experiments on noisy UGC treebank,\nwe use an extension of the French Social Media\nBank, FSMB 7 (Seddah et al., 2012a): a treebank\nof French sentences coming from various social\nmedia sources only available either in constituent\ntrees or in the native French Treebank dependency\nannotation scheme (Candito et al., 2010), along\nwith the Sequoia original treebank that we use with\nthe same annotation scheme for compatibility in\nour French UGC experiments. A brief overview of\nthe size and content of each treebank can be found\nin Table 2.\nPre-training Data Sets Note that in some of our\nexperiments, we use a fraction, 1% of the dedupli-\ncated French instance of the Oscar corpora (Suárez\net al., 2019), about 380M words, as a source of un-\nlabeled data to be either mixed with NArabizi pre-\ntraining data (as an 2.5M words additional sample)\nor used as pre-training data for characterBert when\nevaluated on French UGC (whole 1%). Statistics\non those data sets are presented on Table 3.\n5 Model\nCharacterBERT (El Boukkouri et al., 2020) is a\ncharacter-based variant of BERT that replaces the\nWordPiece embedding matrix with multiple CNN\n6The original data set provides 50k sentences of clean\nNArabizi sentences and an additional 49k sentences of more\nnoisy data, we use here a concatenation of both.\n7We use a shufﬂed version of the treebank split into a train\nset of about 2 000 sentences and a dev and test set of about\n1 000 sentences each.\n427\n# Tokens # Sentences Language\n99k Narabizi 2.527k 99k ar-dz\n1% Oscar 318.715k 9.342k fr\n10% Oscar 1.885.351k 55.261k fr\n100% Oscar 23.209.872k 558.092k fr\nTable 3: pre-training data set statistics.\nand a highway layer (Srivastava et al., 2015). This\nmethod for encoding token representations is in-\nspired from ELMo (Peters et al., 2018b), one of\nthe ﬁrst pre-trained language models for transfer\nlearning. It generates a context-independent rep-\nresentation from character embeddings and feeds\nthem to transformer encoder layers, similarly to\nBERT architecture. Therefore, it produces a sin-\ngle embedding for any input token and does not\nneed a WordPiece vocabulary. This avoids having\nan inconstant number of subword vectors for each\nword. We choose this model since its robustness\nto noise, when tested on biomedical domain by\nEl Boukkouri et al. (2020), can lead to interesting\nresult when facing our noisy experiment data. It\nis the ﬁrst character-based and BERT-like model,\nalong with CharBERT (Ma et al., 2020b), that uses\nboth character and subword embeddings and has\nthe advantage of being publicly available.8 Note\nthat we retrain CharacterBert from scratch on our\ndata sets and do not make use of any of its available\npretrained at models any point in our experiments.\n6 Experiments\nIn this section, we present our fully-supervised\nand semi-supervised baselines. We also evaluate\ndifferent ﬁne-tuning strategies combined with two\nlayers conﬁgurations. We use various embedding\nmodels that we contrast with the CharacterBERT\nmodel.\nBaseline Models For our fully-supervised base-\nline, we use FastText embeddings (Joulin et al.,\n2016) trained from scratch on our treebank training\nsets and used as input for our downstream tasks\nwithout any special treatment.\nTo measure the effectiveness of using a contex-\ntualized character-based language model, we com-\npare its performance to subword based language\nmodels, both monolingual and multilingual, that\nconstitute the basis of our semi-supervised base-\nlines. For multilingual subword based language\n8https://github.com/helboukkouri/char\nacter-bert-pre-training\nUPOS UAS LAS\nNo external embeddings 57.61 55.48 39.32\nTable 4: Pos-Tagging and Parsing Baseline Results for\nNArabizi test set.\nmodel, we use mBERT, the multilingual version of\nBERT (Devlin et al., 2018). It is trained for 104\ndifferent languages on Wikipedia data, including\nFrench and Arabic, languages for which Muller\net al. (2020) showed that, to a certain extent, they\ncould transfer to NArabizi. For our monolingual\nmodel we use CamemBERT (Martin et al., 2020)\nwhich is a contextualized language model based on\nthe RoBERTa model (Liu et al., 2019) trained and\noptimized speciﬁcally for French.\nMODEL +TASK We use the implementation of\nthe Biafﬁne graph parser (Dozat and Manning,\n2016) from Grobol and Crabbé (2021); they\nadapted it to handle several input sources, such\nas BERT representations. The parser performs its\nown tagging using a multi-layer perceptron. The\nword representations are a concatenation of word\nembeddings and tag embeddings learned together\nwith the model parameters on the treebank train-\ning data. We ﬁne-tune the overall model by back-\npropagating through the average of all sub-tokens\nof a word. Baseline results, without any external\nembeddings, for this parser are provided in Table 4.\nMODEL +MLM+T ASK Before ﬁne-tuning the\nmodel on the downstream task, we perform lan-\nguage adaptation by ﬁne-tuning it in a self-\nsupervised fashion with the MLM loss. We use\nthe NArabizi raw data; we train the model for 20\nepochs, keeping the best model obtained at the end.\nThe evaluation is done using the MLM log likeli-\nhood, with 10% of the data kept for validation.\n.\nFine-Tuning Strategy In addition to using only\nthe last layer (cf. Appendix A where we conducted\nseveral experiments to explore the effect of differ-\nent layer conﬁgurations in our downstream tasks),\nwe also test two options for the layers aggregation.\nThe ﬁrst one is simply taking the mean of selected\nlayers. The second is scalar mix, introduced by\nELMo (Peters et al., 2018b): a convex combina-\ntion of the transformer layers where the weights\nare learnt.\nFor each of these conﬁgurations, we test two\nsetup: with and without training the transformer\n428\nFine-tuning\nStrategy\nMODEL+TASK MODEL+MLM+TASK\nUPOS UAS LAS UPOS UAS LAS\nlast-layer-ft 81.14 72.59 60.35 83.08 73.77 62.00\nlast-layer-fz 56.05 62.48 43.95 70.23 67.86 53.45\nmean-ft 80.86 73.06 61.11 84.22 72.97 61.58\nmean-fz 65.41 64.27 48.11 73.53 68.05 53.92\nscalar-mix-ft 80.15 70.46 58.08 83.65 74.62 62.62\nscalar-mix-fz 58.74 63.80 46.93 61.20 67.25 52.84\n(a) CamemBERT\nFine-tuning\nStrategy\nMODEL+TASK MODEL+MLM+TASK\nUPOS UAS LAS UPOS UAS LAS\nlast-layer-ft 80.48 69.19 57.89 84.55 73.82 62.67\nlast-layer-fz 65.08 62.95 45.84 79.44 71.55 58.98\nmean-ft 84.40 72.83 61.81 84.50 71.31 60.96\nmean-fz 69.47 64.89 49.34 78.40 69.71 57.61\nscalar-mix-ft 80.25 69.19 56.14 84.31 72.78 61.67\nscalar-mix-fz 71.79 66.73 52.17 78.83 72.07 59.17\n(b) mBERT\nFine-tuning\nStrategy\nNArabizi Sample Oscar NArabizi + Oscar\n99k 99k 66k+33k\nUPOS UAS LAS UPOS UAS LAS UPOS UAS LAS\nlast-layer-ft 81.19 70.56 58.65 78.83 69.52 56.33 80.67 69.90 57.75\nlast-layer-fz 75.71 66.87 53.69 71.74 65.31 51.75 75.38 68.57 55.15\nmean-ft 81.00 70.70 58.84 78.78 69.52 55.81 80.91 69.57 57.18\nmean-fz 77.08 68.05 55.53 72.35 66.54 51.98 77.22 69.33 56.14\nscalar-mix-ft 79.96 69.47 57.84 80.62 68.76 56.76 80.15 69.90 57.56\nscalar-mix-fz 75.90 68.10 54.58 71.69 66.16 51.80 77.60 69.33 55.91\n(c) CharacterBERT (MODEL +TASK )\nTable 5: Performances of the models on the NArabizi treebank using different ﬁne-tuning strategy (We use ft to\nindicate that the embeddings are ﬁne-tuned for the tasks, while fz is used when the embeddings are frozen during\nthe ﬁne-tuning step).\nmodel weights during the ﬁne-tuning. We call the\nﬁrst strategy frozen representation, noted with a\n-fz sufﬁx in our results, where the language model\nis used to extract meaningful features that consist\nof contextual embeddings. In the second strategy,\nthe contextual embedding extractor – that is, the\npre-trained language model – is ﬁne-tuned on the\ndownstream task alongside the task-speciﬁc com-\nponent, noted with a -ft sufﬁx in our results.\n7 Results and Discussion\nIn this section, we compare all the systems pre-\nsented before on theNArabizi and French treebanks.\nWe especially focus on the impact of corpus size\non the models’ performance.\n7.1 Experiments on NArabizi treebank\nScores are reported as triplets describing UPOS/UAS/LAS\nresults. Highlighted cells marks the best results column-wise\nwhile bold marks best results row-wise\nWe report in Table 5 the scores for the different\nﬁne-tuning strategies for the three models Camem-\nBERT, mBERT and CharacterBERT.\nAdditional insights on the extraction of a repre-\nsentation from the different layers are provided in\nthe Appendix, Table 9, with analysis on the effect\nof the combination of different subsets of layers on\nthe accuracy.\nCharacterBERTNArabizi performs better overall\nin the Model+TASK setting Looking at table\n5, we notice that CharacterBERT NArabizi signiﬁ-\ncantly outperforms mBERT and CamemBERT in\nthe MODEL +TASK setup without MLM for almost\nall the conﬁgurations, except for the two conﬁg-\nurations mean-ft (when using the mean of all the\nﬁne-tuned layers) and scalar-mix-ft (when using\nthe scalar-mix of all the layers) where mBERT and\nCamemBERT show slightly better performance\nthan CharacterBERTNArabizi. In this latter case,\nmBERT outperforms CharacterBERT NArabizi on\nthe NArabizi data set with accuracy differences\nof (3.40/2.13/2.96) for (UPOS/UAS/LAS) using\nmean-ft, which is only signiﬁcant for UAS with p-\nvalue<0.05.9 The same observation can be done for\nCamemBERT in the MODEL +TASK setting which\noutperforms CharacterBERTNArabizi with accuracy\ndifferences of (-0.14/2.36/2.26).\nThe same is observed for the scalar-mix-ft\noption but the differences are not signiﬁcant\nwith p-value<0.05 for UAS and LAS. Besides\nthese two conﬁgurations, CharacterBERT NArabizi\nalways outperforms the other two models with-\nout MLM. The most notable difference using\nCharacterBERTNArabizi is when the latter out-\nperforms CamemBERT with accuracy differ-\nences of (19.66/4.39/9.74) and mBERT with\n(10.63/3.92/7.854) in the last-layer-fz setting with\na p-value<0.05 for both comparisons.\n9We tested the statistical signiﬁcance using the publicly\navailable Dan Bikel’s code at https://github.com/t\ndozat/Parser-v1\n429\nAdding MLM reverses the trend However,\nwhen we compare CharacterBERT NArabizi to\nCamemBERT+MLM and mBERT+MLM, we\nsee that while they both generally outperform\nCharacterBERTNArabizi, there are some settings\nwhere CharacterBERT NArabizi still gets better\nscores. mBERT+MLM gets the best scores\nfor all of the conﬁgurations we test among the\nthree models. While both CamemBERT and\nmBERT perform comparably, mBERT outper-\nforms CharacterBERT NArabizi when it is used\nwith MLM. Some of the best performance are\nachieved in the last-layer-ft setting with scores\nof (83.08/73.77/62.00) for CamemBERT and\n(84.55/73.82/62.67) for mBERT and lower scores\nof (81.19/70.56/58.6) for CharacterBERT NArabizi\n(both models outperform CharacterBERTNArabizi in\nthis case).\nIn other settings, CharacterBERTNArabizi is still\ncompetitive with the two other MLM-pretrained\nmodels, as illustrated by the last-layer-fz set-\nting with scores of (70.23/67.86/53.45) for\nCamemBERT, and (75.71/66.87/53.69) for\nCharacterBERTNArabizi and (79.44/71.55/58.98)\nfor mBERT. Still, the general tendency in the\nModel+MLM+TASK setting is that mBERT\noutperforms CharacterBERT NArabizi when\nused with MLM and CamemBERT exhibits\nperformance similar to CharacterBERTNArabizi\nin the same setting. This is in contrast with the\nearlier comparison without MLM pre-training\nwhere both CamemBERT and mBERT reached\nscores lower than those of CharacterBERTNArabizi.\nThis observation conﬁrms the ﬁndings of Muller\net al. (2021a) regarding the positive impact of unsu-\npervised ﬁne-tuning for BERT models even if the\nlanguage is not one of the pre-training languages.\nCharacterBERT pretrained on 1% of Os-\ncar performs roughly the same than Camem-\nBERT+Task If we compare the performance of\nCamemBERT in the MODEL +TASK setting to\nCharacterBERT trained on sub-sample of Oscar,\nwe see that both models are comparable. Typically,\nCamemBERT seems to outperform Character-\nBERT in some settings like last-layer-ft where the\nlatter records (78.83/69.52/56.33) for UPOS, UAS\nand LAS scores respectively while CamemBERT\nrecords higher at (81.14/72.59/60.35). In other\nsettings, CharacterBERT seems to outperform\nCamemBERT. In the mean-fz setting for instance,\nCharacterBERT has scores of (72.35/66.54/51.98)\nwhich surpasses the (69.47/64.89/49.34) scores of\nCamemBERT. No clear conclusion can be drawn\nabout the best use of one model over the other in\nthe different settings since they all display com-\npetitive scores. This is essentially due to the fact\nthat CamemBERT is trained on the full Oscar data\nset, while CharacterBERT is trained on just 0.01%\nof it. In addition, the test set is made of only\n140 NArabizi sentences, making any interpretation\nof the results difﬁcult. These two reasons make\ndifﬁcult the drawing of concrete conclusions on\nthe performance of both models compared to each\nother. Therefore in the next section, we will evalu-\nate the models using the best ﬁne-tuning strategy\non French treebanks with a larger evaluation set\nand a CharacterBERT trained on 1% Oscar.\n7.2 The Impact of Data Size: Experiments on\nFrench treebanks\nIn-domain experiments For our experiments on\nFrench treebanks, we keep the best setup from our\nprevious results and use the last layer of the model\nﬁne-tuned for the task.\nIn table 6, we report the scores obtained by both\nmodels CamemBERT and CharacterBERT on three\nFrench treebanks, compared to our parser with-\nout external embeddings from pre-trained models,\nwhich we consider as our baseline for those ex-\nperiments. All systems using CamemBERT or\nCharacterBERT models outperform our baseline.\nBoth models have competitive scores. For ex-\nample, when tested on the GSD treebank, our\nbaseline obtains scores of (96.35/89.98/86.96)\nwhile CamemBERT and CharacterBERT obtain\nboth better, yet relatively close scores, with\n(98.53/95.74/94.17) for the CamemBERT-based\nmodel and (98.07/95.22/93.50). The same observa-\ntions apply for the other two treebanks SEQUOIA\nand SPOKEN. These results are coherent with\nthe comparisons done earlier for NArabizi where\nboth CharacterBERT and CamemBERT models\nproduced comparable scores even though Charac-\nterBERT was trained on only 0.01% of the full data\nset on which CamemBERT was trained on. Our\nresults on those much larger data sets corroborate\nthen our NArabizi models results and conﬁrm the\ninterest of using characterBert-based models in our\nscenarios.\nExtremely noisy user-generated content exper-\niments In table 7, we compare both models\nCamemBERT and CharacterBERT in two different\n430\nModel-LayerConﬁg GSD SEQUOIA SPOKEN\nUPOS UAS LAS UPOS UAS LAS UPOS UAS LAS\nNo pre-training 96.35 89.98 86.96 92.57 82.28 77.72 83.60 67.65 58.04\nCamemBERT (100% Oscar) 98.53 95.74 94.17 99.23 95.78 94.59 97.41 88.15 83.00\nCharacterBERT (1% Oscar) 98.07 95.22 93.50 99.27 94.98 93.80 96.62 86.82 81.22\nTable 6: POS and dependency parsing scores on 3 French treebanks.\nModel-LayerConﬁg Dev Test\nUPOS UAS LAS UPOS UAS LAS\nFSMB\ntrain\nNo pre-training 81.24 70.04 59.46 81.62 69.19 59.17\nCamemBERT 95.34 87.01 81.56 95.48 87.47 82.66\nCharacterBERT 95.08 85.99 80.51 95.19 86.26 81.26\nSequoia\ntrain\nNo pre-training 71.50 59.04 47.30 72.79 59.92 48.81\nCamemBERT 89.47 81.80 74.33 90.10 82.68 75.85\nCharacterBERT 90.12 81.79 74.43 90.68 82.39 75.39\nTable 7: POS and dependency parsing scores on the FSMB .\nsettings. In the ﬁrst one, both models are ﬁne-tuned\non the FSMB training set. In the second one, the\nmodels are trained on the Sequoia training set. The\nperformance of these models is to be compared – as\nin table 7 – to the performance of a parser without\nexternal embeddings from a pre-trained model. As\nexpected, both models outperform the baseline and\n– similarly to previous results – are competitive in\ntheir respective obtained scores.\nIn the FSMB training setting, CamemBERT\nachieves scores of (95.34/87.01/81.56) when\ntested on the development part of the data set,\nand CharacterBERT achieves close scores of\n(95.08/85.99/80.51) on the same data set. Both\nscores outperform the ones obtained by the baseline\nof (81.24/70.04/59.46) also on the development set.\nThe same behavior can be observed in the Sequoia\ntraining setting where CamemBERT gets scores of\n(89.47/81.80/74.33) and CharacterBERT scores of\n(90.12/81.79/74.43) on the development set: both\nare still higher than the baseline scores. This com-\nparison is still valid when we consider the test set\nresults. For reasons probably tied to the random\nsampling done when splitting the data set, the score\nranking are reversed in the test set, yet results are\nvery similar and the slight differences between the\ntwo models results are not statistically signiﬁcant.\nThis shows the actual effectiveness of having a\ncharacter-based model trained on only a small frac-\ntion of its “classic” BERT counterpart when facing\nnoisy user-generated content from a data set that\nwas proven to be much more noisy that many other\nsimilar data sets (Rosales Núñez et al., 2019).\nUPOS UAS LAS %Oscar\nFSMB ﬁne-tuned (in-domain)\nCamemBERT 95.48 87.47 82.66 100\nCamemBERT4gb 95.13 85.73 80.72 2.38\nCharacter-BERT 95.19 86.26 81.26 1\nSequoia ﬁne-tuned (out domain)\nCamemBERT 90.10 82.68 75.85 100\nCamemBERT4gb 90.69 82.29 75.83 2.38\nCharacter-BERT 90.68 82.39 75.39 1\nTable 8: CharacterBert model performance compared\nwith a small CamemBERT (4gb) model on the F SMB\ntest set in in-domain and out-of-domain ﬁne-tuning sce-\nnarios. Full-size Camembert results are reported here\nfor reference.\n8 Discussion\nIn this work, we evaluate the beneﬁts of using a\ncharacter-based model in low-resource scenarios.\nOur results show that training such a model from\nscratch on much fewer data gives similar perfor-\nmance to a multilingual BERT adapted to the lan-\nguage using the same amount of data.\nOverall, our observations conﬁrm the ﬁndings\nof El Boukkouri et al. (2020) regarding the robust-\nness to noise and misspellings of the Character-\nBERT model. We showed that the model has com-\npetitive performance on noisy French UGC data\nwhen trained on only a fraction of the OSCAR\ncorpus compared to CamemBERT trained on the\nfull corpus and when trained on corpora containing\nabout 1M words in the extremely noisy and low-\nresource case of NArabizi. This is consistent with\nthe ﬁndings of Martin et al. (2020) and Micheli\net al. (2020), who showed that MLM could already\nlearn a lot from pre-training on smaller data set.\nExtending this investigation by training on a larger\n431\namount of data could help to explore the ability of\nthe model to handle highly variable noisy data.\nHowever, one could question the usefulness of\nsuch Character-BERT based models if small Bert-\nbased models were available on the same domain.\nTo build an answer to that question, we conducted\na quick set of experiments comparing our char-\nacterBert model trained on 1% of Oscar with the\noff-the-shelf Camembert version trained on 4gb of\nthe Oscar corpus French instance (2.38% of the\nfull corpus) and which was shown to perform al-\nmost as well as the full model (Martin et al., 2020)\non many downstream tasks. Both models were\nﬁned-tuned according to our MODEL+Task archi-\ntecture on either the FSMB or the Sequoia treebank,\nallowing us to evaluate their in-domain and out-of-\ndomain performance. Results on Table 8 conﬁrm\nthe effectiveness of our characterBert model with\noverall better results than CamemBERT4gb in the\nin-domain scenario and similar, if not slightly bet-\nter in the out-of-domain scenario, except for the\nlabeled attachment score (75.83 vs 75.39). The fact\nthat CamemBERT4gb was trained on more than\ntwice as much data and with 200k pre-training\nsteps while the characterBert pre-training stopped\nbelow 20k steps probably explains this small dis-\ncrepancy but further investigations are needed with\na fully parallel setting where both characterBert and\nCamemBERT are pretrained on the same amount\nof data and the same hyper-parameters. The take-\nhome message from this in-domain experiments is\nthat CharacterBert seems to be able to better cap-\nture at least some of the UGC idiosyncracies that\nare prevalent in the FSMB (Seddah et al., 2012b)\nthan its Bert-based counterparts. This was also\nshown by Rosales Núñez et al. (2021) in the con-\ntext of character-based neural machine translation.\nInterestingly, their results showed that transformer-\nbased models with subword tokenization also ex-\nhibit strong robustness to a certain type of lexi-\ncal noise. This behavior has been very recently\ndemonstrated by Itzhak and Levy (2021) and could\nexplain why the BERT-based models we tested per-\nformed so well in our experiments. The key seems\nto be relying on the ability of the subword distri-\nbution to model some forms of lexical variations.\nMuch more experiments are needed to clearly in-\nvestigate in what circumstances, besides noisy and\nresource-scarce scenarios, characterBERT models\nbring in a decisive advantage.\nOur results are based on the evaluation of two\nlow-level tasks. Therefore, it would be interesting\nto see if they can be generalized to other – e.g.\nmore semantic – tasks, as additional experiments\non model layers conﬁguration showed that most of\nthe important information is captured early in the\nlayers of the model (cf. Appendix A).\nRegarding the speciﬁc case of Arabic dialects\nwritten in Arabizi, a recent BERT-based model\nhave been pretrained on 7 millions Egyptian tweets\nand displayed effective results on a sentiment anal-\nysis task (Baert et al., 2020). Another very recent\nmodel, at the date of writing, was pre-trained on\n4 millions Algerian tweets and also demonstrated\ninteresting results on sentiment analysis (Abdaoui\net al., 2021). Unfortunately, the authors did not\nperform any experiments on the Narabizi data set,\nmaking thus the comparison with our work not\nstraightforward. It would be of course interesting\nto evaluate the interoperability between these new\ndata sets and the NArabizi resources we used to\nproduce our models. Head to head comparisons\nbetween these models and ours could be of value of\ncourse but we believe that given the shortcomings\nof ﬁnding enough data to pretrain large models for\ndialects, it would be probably better to ﬁrst consol-\nidate a large enough common pre-training data set\nand then work on model performance. We leave\nthis for future work.\nWe showed that CharacterBert models trained on\nvery little data could provide an interesting alterna-\ntive to large multilingual and monolingual models\nin resource-scarce and noisy scenarios. This is why\nwe release all the code, data and models to repro-\nduce our experiments, hoping our work will favor\nthe rise of efﬁcient robust NLP models for under-\nresourced languages, domains and dialects.10\nAcknowledgments\nWe thank the reviewers for their very valuable feed-\nback. The ﬁrst author was partly funded by Benoît\nSagot’s chair in the PRAIRIE institute funded by\nthe French national agency ANR as part of the “In-\nvestissements d’avenir” programme under the refer-\nence ANR-19-P3IA-0001. This work also received\nfunding from the European Union’s Horizon 2020\nresearch and innovation programme under grant\nagreement No. 101021607 and from the French Re-\nsearch Agency via the ANR ParSiTi project (ANR-\n16-CE33-0021).\n10https://gitlab.inria.fr/ariabi/chara\ncter-bert-ugc\n432\nReferences\nAmine Abdaoui, Mohamed Berrimi, Mourad Oussalah,\nand Abdelouahab Moussaoui. 2021. Dziribert: a\npre-trained language model for the algerian dialect.\nDjegdjiga Amazouz, Martine Adda-Decker, and Lori\nLamel. 2017. Addressing code-switching in\nfrench/algerian arabic speech. In Interspeech 2017,\npages 62–66.\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov,\nGuillaume Lample, Chris Dyer, and Noah A. Smith.\n2016. Massively multilingual word embeddings.\nArXiv, abs/1602.01925.\nWissam Antoun, Fady Baly, and Hazem Hajj.\n2020. Arabert: Transformer-based model for\narabic language understanding. arXiv preprint\narXiv:2003.00104.\nGaétan Baert, Souhir Gahbiche, Guillaume Gadek, and\nAlexandre Pauchet. 2020. Arabizi language models\nfor sentiment analysis. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 592–603, Barcelona, Spain (Online). In-\nternational Committee on Computational Linguis-\ntics.\nMarie Candito, Benoît Crabbé, and Pascal Denis.\n2010. Statistical French dependency parsing: Tree-\nbank conversion and ﬁrst results. In Proceedings\nof the Seventh International Conference on Lan-\nguage Resources and Evaluation (LREC’10) , Val-\nletta, Malta. European Language Resources Associ-\nation (ELRA).\nMarie Candito and Djamé Seddah. 2012. Le corpus\nsequoia: annotation syntaxique et exploitation pour\nl’adaptation d’analyseur par pont lexical. In TALN\n2012-19e conférence sur le Traitement Automatique\ndes Langues Naturelles.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2021. Canine: Pre-training an efﬁcient\ntokenization-free encoder for language representa-\ntion.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nRyan Cotterell, Adithya Renduchintala, Naomi Saphra,\nand Chris Callison-Burch. 2014. An Algerian\nArabic-French code-switched corpus. In Workshop\non Free/Open-Source Arabic Corpora and Corpora\nProcessing Tools Workshop Programme, page 34.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTimothy Dozat and Christopher D Manning. 2016.\nDeep biafﬁne attention for neural dependency pars-\ning. arXiv preprint arXiv:1611.01734.\nJulian Martin Eisenschlos, Sebastian Ruder, Piotr\nCzapla, Marcin Kardas, Sylvain Gugger, and\nJeremy Howard. 2019. Multiﬁt: Efﬁcient multi-\nlingual language model ﬁne-tuning. arXiv preprint\narXiv:1909.04761.\nJacob Eisenstein. 2013. What to do about bad language\non the internet. In Proceedings of the 2013 confer-\nence of the North American Chapter of the associa-\ntion for computational linguistics: Human language\ntechnologies, pages 359–369.\nHicham El Boukkouri, Olivier Ferret, Thomas\nLavergne, Hiroshi Noji, Pierre Zweigenbaum, and\nJun’ichi Tsujii. 2020. CharacterBERT: Reconciling\nELMo and BERT for word-level open-vocabulary\nrepresentations from characters. In Proceedings\nof the 28th International Conference on Compu-\ntational Linguistics , pages 6903–6915, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nJennifer Foster. 2010. “cba to check the spelling”: In-\nvestigating parser performance on discussion forum\nposts. In Human Language Technologies: The 2010\nAnnual Conference of the North American Chap-\nter of the Association for Computational Linguistics,\npages 381–384, Los Angeles, California. Associa-\ntion for Computational Linguistics.\nLoïc Grobol and Benoît Crabbé. 2021. Analyse en\ndépendances du français avec des plongements con-\ntextualisés. In Actes de la 28ème Conférence sur le\nTraitement Automatique des Langues Naturelles.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\n433\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nNizar Habash. 2010. Introduction to Arabic Natural\nLanguage Processing. Morgan and Claypool.\nJeremy Howard and Sebastian Ruder. 2018. Univer-\nsal language model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nItay Itzhak and Omer Levy. 2021. Models in a spelling\nbee: Language models implicitly learn the character\ncomposition of tokens.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does bert learn about the structure of\nlanguage? In ACL 2019-57th Annual Meeting of the\nAssociation for Computational Linguistics.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6282–6293, Online. Association for Computa-\ntional Linguistics.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, Hérve Jégou, and Tomas Mikolov.\n2016. Fasttext. zip: Compressing text classiﬁcation\nmodels. arXiv preprint arXiv:1612.03651.\nTaku Kudo. 2018a. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 66–\n75, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nTaku Kudo. 2018b. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates.\nAnne Lacheret, Sylvain Kahane, Julie Beliao, Anne\nDister, Kim Gerdes, Jean-Philippe Goldman, Nico-\nlas Obin, Paola Pietrandrea, and Atanas Tchobanov.\n2014. Rhapsodie: a prosodic-syntactic treebank for\nspoken french. In Language Resources and Evalua-\ntion Conference.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shi-\njin Wang, and Guoping Hu. 2020a. Charbert:\nCharacter-aware pre-trained language model. Pro-\nceedings of the 28th International Conference on\nComputational Linguistics.\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shi-\njin Wang, and Guoping Hu. 2020b. Charbert:\nCharacter-aware pre-trained language model. arXiv\npreprint arXiv:2011.01513.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n7203–7219, Online. Association for Computational\nLinguistics.\nRyan McDonald, Joakim Nivre, Yvonne Quirmbach-\nBrundage, Yoav Goldberg, Dipanjan Das, Kuzman\nGanchev, Keith Hall, Slav Petrov, Hao Zhang, Os-\ncar Täckström, et al. 2013. Universal dependency\nannotation for multilingual parsing. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 92–97.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training\ndata volume for compact language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7853–7858, Online. Association for Computa-\ntional Linguistics.\nBenjamin Muller, Antonios Anastasopoulos, Benoît\nSagot, and Djamé Seddah. 2021a. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 448–462, Online. Association for Computa-\ntional Linguistics.\nBenjamin Muller, Yanai Elazar, Benoît Sagot, and\nDjamé Seddah. 2021b. First align, then predict: Un-\nderstanding the cross-lingual ability of multilingual\nBERT. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pages 2214–2231,\nOnline. Association for Computational Linguistics.\nBenjamin Muller, Benoit Sagot, and Djamé Seddah.\n2020. Can multilingual language models transfer to\nan unseen dialect? a case study on north african ara-\nbizi. arXiv preprint arXiv:2005.00318.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection.\nIn Proceedings of the 12th Language Resources\nand Evaluation Conference, pages 4034–4043, Mar-\nseille, France. European Language Resources Asso-\nciation.\nRichard Nordquist. Linguistic variation [online]. 2019.\n434\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018b. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nDanish Pruthi, Bhuwan Dhingra, and Zachary C. Lip-\nton. 2019. Combating adversarial misspellings with\nrobust word recognition. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 5582–5591, Florence, Italy.\nAssociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nJosé Carlos Rosales Núñez, Djamé Seddah, and Guil-\nlaume Wisniewski. 2019. Comparison between\nNMT and PBSMT performance for translating noisy\nuser-generated content. In Proceedings of the 22nd\nNordic Conference on Computational Linguistics ,\npages 2–14, Turku, Finland. Linköping University\nElectronic Press.\nJosé Carlos Rosales Núñez, Guillaume Wisniewski,\nand Djamé Seddah. 2021. Noisy ugc translation at\nthe character level: Revisiting open-vocabulary ca-\npabilities and robustness of char-based models. In\nProceedings of the Seventh Workshop on Noisy User-\ngenerated Text (W-NUT 2022), Punta Cana, Domini-\ncan Republic.\nManuela Sanguinetti, Lauren Cassidy, Cristina Bosco,\nÖzlem Çetinoglu, Alessandra Teresa Cignarella,\nTeresa Lynn, Ines Rehbein, Josef Ruppenhofer,\nDjamé Seddah, and Amir Zeldes. 2020. Treebank-\ning user-generated content: a UD based overview\nof guidelines, corpora and uniﬁed recommendations.\nCoRR, abs/2011.02063.\nTimo Schick and Hinrich Schütze. 2020. Rare words:\nA major problem for contextualized embeddings and\nhow to ﬁx it by attentive mimicking. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 34, pages 8766–8774.\nDjamé Seddah, Farah Essaidi, Amal Fethi, Matthieu\nFuteral, Benjamin Muller, Pedro Javier Ortiz Suárez,\nBenoît Sagot, and Abhishek Srivastava. 2020. Build-\ning a user-generated content North-African Arabizi\ntreebank: Tackling hell. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 1139–1150, Online. Asso-\nciation for Computational Linguistics.\nDjamé Seddah, Benoit Sagot, Marie Candito, Vir-\nginie Mouilleron, and Vanessa Combet. 2012a. The\nFrench Social Media Bank: a treebank of noisy user\ngenerated content. In Proceedings of COLING 2012,\npages 2441–2458, Mumbai, India. The COLING\n2012 Organizing Committee.\nDjamé Seddah, Benoit Sagot, Marie Candito, Virginie\nMouilleron, and Vanessa Combet. 2012b. The\nfrench social media bank: a treebank of noisy user\ngenerated content. In COLING 2012-24th Interna-\ntional Conference on Computational Linguistics.\nRupesh Kumar Srivastava, Klaus Greff, and Jürgen\nSchmidhuber. 2015. Highway networks. arXiv\npreprint arXiv:1505.00387.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous pipeline for process-\ning huge corpora on medium to low resource infras-\ntructures. In 7th Workshop on the Challenges in the\nManagement of Large Corpora (CMLC-7). Leibniz-\nInstitut für Deutsche Sprache.\nLichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari\nAsai, Jia Li, Philip Yu, and Caiming Xiong. 2020.\nAdv-bert: Bert is not robust on misspellings! gener-\nating nature adversarial samples on bert.\nYi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta,\nHyung Won Chung, Dara Bahri, Zhen Qin, Si-\nmon Baumgartner, Cong Yu, and Donald Metzler.\n2021. Charformer: Fast character transformers via\ngradient-based subword tokenization.\nClara Vania, Yova Kementchedjhieva, Anders Søgaard,\nand Adam Lopez. 2019. A systematic comparison\nof methods for low-resource dependency parsing on\ngenuinely low-resource languages. arXiv preprint\narXiv:1909.02857.\nXinyi Wang, Sebastian Ruder, and Graham Neubig.\n2021. Multi-view subword regularization.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? InProceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\n435\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2021. Byt5: Towards a token-free\nfuture with pre-trained byte-to-byte models.\nA Layer Conﬁguration Experiments\nWe focus on the scalar mix strategy to study the\neffect of the combination of different subsets of\nlayers on the accuracy of the downstream task, in-\nstead of only aggregating all layers. For example,\nit has been shown that higher layers contain more\nsemantic information, while lower layers contain\nmore syntactic information (Jawahar et al., 2019).\nWe compare several layer conﬁgurations, that is\ndifferent subsets of the transformer from which we\nget the sentence representation.\nThe effect of the Layer Conﬁguration We re-\nport in tables 9a, 9b and 9c the scores for the\ndifferent layers combinations for CamemBERT,\nmBERT and CharacterBERT respectively. For\nmBERT and CamemBERT, the performance in-\ncreases when using the last layers, while for Char-\nacterBERT, there is no big difference between the\ndifferent layers combinations. For example for\nthe setup MODEL +TASK , the accuracy for POS\ntagging (UPOS) goes from around 70 with the\nﬁrst layer using CamemBERT and mBERT, and\nreaches 80 using CharacterBERT_arabizi. For\nCharacterBERT, these scores stay around 80 even\nwhen using farther layers (for layers 4 to 7 for\ninstance, the UPOS score is 80.53 and for lay-\ners 6-11 it is around 80.34), and while using the\nlast layer gives the best score of 81.19, the lat-\nter is still considered around 80 and the stagna-\ntion in the scores is hence visible. Contrarily, for\nCamemBERT and mBERT, the UPOS scores for\nthe MODEL +TASK setup increase from around 70\nusing only the ﬁrst layer to above 80 when us-\ning layers 6 through 11 (80.39 for CamemBERT\nand 80.72 for mBERT). The best UPOS score for\nCamemBERT appears when using the last layer\nalone (81.14) while for mBERT it is when us-\ning layers 6 through 11 (80.72). This clearly il-\nlustrates the increase of performance when using\nhigher layers for CamemBERT and mBERT. The\nsame observation can be made for UPOS scores in\nthe MODEL +MLM+T ASK setup for mBERT and\nCamemBERT, and for the other Unlabeled Attach-\nment Score (UAS) and Labeled Attachment Score\n(LAS) scores as well.\nOne possible explanation is that the information\ncaptured by CharacterBERT layers does not evolve\nalong the model’s layers. The model produces a\nsingle embedding for any input token based on an\naggregation of the characters embeddings while\nfor BERT-like-models, each sub-word unit in a\nword is embedded using a WordPiece embedding\nmatrix. Therefore, a possible interpretation is that\nCharacterBERT learns all the information at the\nearliest layers as we feed it the whole word directly\nand not an inconstant count of sub-words when\nWordPiece vocabulary is in use. Moreover, less\nthan 10% of the 100 000 most frequent sub-words\nin the NArabizi raw data are present in the mBERT\nvocab due to the high variability of NArabizi.\n436\nLayer\nConﬁg\nMODEL+TASK MODEL+MLM+TASK\nUPOS UAS LAS UPOS UAS LAS\n0 68.38 66.21 50.61 70.89 64.74 49.81\n0-5 77.93 70.51 57.04 80.95 72.21 59.45\n4-7 79.63 70.18 57.56 81.57 72.54 60.92\n6-11 80.39 71.88 58.41 83.65 74.15 62.15\n11 81.14 72.59 60.35 83.08 73.77 62.00\nall 80.15 70.46 58.08 83.65 74.62 62.62\n(a) CamemBERT\nLayer\nConﬁg\nMODEL+TASK MODEL+MLM+TASK\nUPOS UAS LAS UPOS UAS LAS\n0 73.63 65.36 52.08 75.43 66.21 52.98\n0-5 79.40 69.19 57.23 82.37 69.90 58.08\n4-7 80.62 69.61 57.33 83.74 72.45 62.05\n6-11 80.72 68.53 56.76 85.02 72.40 61.63\n11 80.48 69.19 57.89 84.55 73.82 62.67\nall 80.25 69.19 56.14 84.31 72.78 61.67\n(b) mBERT\nLayer\nConﬁg\nNArabizi Sample OSCAR NArabizi + Oscar\n99k 99k 66k+33k\nUPOS UAS LAS UPOS UAS LAS UPOS UAS LAS\n0 78.92 70.79 57.99 78.26 69.61 56.76 79.25 69.33 57.14\n0-5 79.63 70.23 57.99 79.54 70.13 56.76 79.06 68.05 55.25\n4-7 80.53 69.28 57.47 79.77 70.75 57.84 80.10 69.90 56.76\n6-11 80.34 70.60 57.84 78.26 68.43 55.58 80.91 69.66 57.80\n11 81.19 70.56 58.65 78.83 69.52 56.33 80.67 69.90 57.75\nall 79.96 69.47 57.84 80.62 68.76 56.76 80.15 69.90 57.56\n(c) CharacterBERT\nTable 9: Performances of the models on the NArabizi treebank using different combinations of the model layers\nfor embeddings.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.853222131729126
    },
    {
      "name": "Treebank",
      "score": 0.8299088478088379
    },
    {
      "name": "Natural language processing",
      "score": 0.7010440826416016
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6584565043449402
    },
    {
      "name": "Language model",
      "score": 0.5862614512443542
    },
    {
      "name": "Task (project management)",
      "score": 0.4779929518699646
    },
    {
      "name": "Parsing",
      "score": 0.46414175629615784
    },
    {
      "name": "Character (mathematics)",
      "score": 0.4568191170692444
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4471479058265686
    },
    {
      "name": "Machine translation",
      "score": 0.44622722268104553
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4190632402896881
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}