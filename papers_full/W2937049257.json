{
    "title": "Effective Estimation of Deep Generative Language Models",
    "url": "https://openalex.org/W2937049257",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5034679436",
            "name": "Tom Pelsmaeker",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": "https://openalex.org/A5074142241",
            "name": "Wilker Aziz",
            "affiliations": [
                "University of Amsterdam"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2753738274",
        "https://openalex.org/W2097998348",
        "https://openalex.org/W2962738009",
        "https://openalex.org/W2963134326",
        "https://openalex.org/W2963773425",
        "https://openalex.org/W2963713328",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2912618358",
        "https://openalex.org/W1516111018",
        "https://openalex.org/W2964000524",
        "https://openalex.org/W2111305191",
        "https://openalex.org/W2963594498",
        "https://openalex.org/W2963879591",
        "https://openalex.org/W2962727772",
        "https://openalex.org/W2203132937",
        "https://openalex.org/W2951670304",
        "https://openalex.org/W2527569769",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2963279312",
        "https://openalex.org/W2952838738",
        "https://openalex.org/W1994616650",
        "https://openalex.org/W2963275229",
        "https://openalex.org/W2952584433",
        "https://openalex.org/W164706946",
        "https://openalex.org/W2526471240",
        "https://openalex.org/W2970134931",
        "https://openalex.org/W2395149821",
        "https://openalex.org/W1583912456",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2963145887",
        "https://openalex.org/W2951999424",
        "https://openalex.org/W2296319761",
        "https://openalex.org/W2963987720",
        "https://openalex.org/W2964041002",
        "https://openalex.org/W2787876323",
        "https://openalex.org/W2212660284",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2962947230",
        "https://openalex.org/W2964339599",
        "https://openalex.org/W2963223306",
        "https://openalex.org/W2560512785",
        "https://openalex.org/W2587284713",
        "https://openalex.org/W2963266340",
        "https://openalex.org/W2962717182",
        "https://openalex.org/W2888844359",
        "https://openalex.org/W2970646865",
        "https://openalex.org/W2963073938",
        "https://openalex.org/W2586756136",
        "https://openalex.org/W2962897886",
        "https://openalex.org/W2025768430",
        "https://openalex.org/W2798493043",
        "https://openalex.org/W2963522047",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2963135265",
        "https://openalex.org/W2883510303",
        "https://openalex.org/W2131241448",
        "https://openalex.org/W2963790827",
        "https://openalex.org/W2767757834",
        "https://openalex.org/W2622563070",
        "https://openalex.org/W2964168257",
        "https://openalex.org/W2779400057",
        "https://openalex.org/W2963645026",
        "https://openalex.org/W2102486516",
        "https://openalex.org/W2149327368",
        "https://openalex.org/W2963081964",
        "https://openalex.org/W2796335507",
        "https://openalex.org/W2963641970",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963748792",
        "https://openalex.org/W2963600562",
        "https://openalex.org/W1597533204"
    ],
    "abstract": "Advances in variational inference enable parameterisation of probabilistic models by deep neural networks. This combines the statistical transparency of the probabilistic modelling framework with the representational power of deep learning. Yet, due to a problem known as posterior collapse, it is difficult to estimate such models in the context of language modelling effectively. We concentrate on one such model, the variational auto-encoder, which we argue is an important building block in hierarchical probabilistic models of language. This paper contributes a sober view of the problem, a survey of techniques to address it, novel techniques, and extensions to the model. To establish a ranking of techniques, we perform a systematic comparison using Bayesian optimisation and find that many techniques perform reasonably similar, given enough resources. Still, a favourite can be named based on convenience. We also make several empirical observations and recommendations of best practices that should help researchers interested in this exciting field.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7220–7236\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n7220\nEffective Estimation of Deep Generative Language Models\nTom Pelsmaeker\nILCC\nUniversity of Edinburgh\nt.l.pelsmaeker@sms.ed.ac.uk\nWilker Aziz\nILLC\nUniversity of Amsterdam\nw.aziz@uva.nl\nAbstract\nAdvances in variational inference enable pa-\nrameterisation of probabilistic models by deep\nneural networks. This combines the statisti-\ncal transparency of the probabilistic modelling\nframework with the representational power of\ndeep learning. Yet, due to a problem known\nas posterior collapse, it is difﬁcult to estimate\nsuch models in the context of language mod-\nelling effectively. We concentrate on one such\nmodel, the variational auto-encoder, which we\nargue is an important building block in hierar-\nchical probabilistic models of language. This\npaper contributes a sober view of the problem,\na survey of techniques to address it, novel tech-\nniques, and extensions to the model. To es-\ntablish a ranking of techniques, we perform a\nsystematic comparison using Bayesian optimi-\nsation and ﬁnd that many techniques perform\nreasonably similar, given enough resources.\nStill, a favourite can be named based on conve-\nnience. We also make several empirical obser-\nvations and recommendations of best practices\nthat should help researchers interested in this\nexciting ﬁeld.\n1 Introduction\nDeep generative models (DGMs) are probabilis-\ntic latent variable models parameterised by neural\nnetworks (NNs). Speciﬁcally, DGMs optimised\nwith amortised variational inference and reparam-\neterised gradient estimates (Kingma and Welling,\n2014; Rezende et al., 2014), better known as vari-\national auto-encoders (V AEs), have spurred much\ninterest in various domains, including computer\nvision and natural language processing (NLP).\nIn NLP, V AEs have been developed for word\nrepresentation (Rios et al., 2018), morphological\nanalysis (Zhou and Neubig, 2017), syntactic and\nWork done while the ﬁrst author was at the University of\nAmsterdam. Code is available at https://github.com/\ntom-pelsmaeker/deep-generative-lm\nsemantic parsing (Corro and Titov, 2018; Lyu and\nTitov, 2018), document modelling (Miao et al.,\n2016), summarisation (Miao and Blunsom, 2016),\nmachine translation (Zhang et al., 2016; Schulz\net al., 2018; Eikema and Aziz, 2019), language and\nvision (Pu et al., 2016; Wang et al., 2017), dialogue\nmodelling (Wen et al., 2017; Serban et al., 2017),\nspeech modelling (Fraccaro et al., 2016), and, of\ncourse, language modelling (Bowman et al., 2016;\nGoyal et al., 2017). One problem remains common\nto the majority of these models, V AEs often learn\nto ignore the latent variables.\nWe investigate this problem, dubbed posterior\ncollapse, in the context of language models (LMs).\nIn a deep generative LM (Bowman et al., 2016),\nsentences are generated conditioned on samples\nfrom a continuous latent space, an idea with vari-\nous practical applications. For example, one can\nconstrain this latent space to promote generalisa-\ntions that are in line with linguistic knowledge and\nintuition (Xu and Durrett, 2018). This also allows\nfor greater ﬂexibility in how the model is used,\nfor example, to generate sentences that live—in la-\ntent space—in a neighbourhood of a given observa-\ntion (Bowman et al., 2016). Despite this potential,\nV AEs that employ strong generators (e.g. recurrent\nNNs) tend to ignore the latent variable. Figure 1\nillustrates this point: neighbourhood in latent space\ndoes not correlate to patterns in data space, and the\nmodel behaves just like a standard LM.\nRecently, many techniques have been proposed\nto address this problem (§3 and §7) and they range\nfrom modiﬁcations to the objective to changes to\nthe actual model. Some of these techniques have\nonly been tested under different conditions and un-\nder different evaluation criteria, and some of them\nhave only been tested outside NLP. This paper con-\ntributes: (1) a novel strategy based on constrained\noptimisation towards a pre-speciﬁed upper-bound\non mutual information; (2) multimodal priors that\n7221\nby design promote increased mutual information\nbetween data and latent code; last and, arguably\nmost importantly, (3) a systematic comparison—\nin terms of resources dedicated to hyperparame-\nter search and sensitivity to initial conditions—of\nstrategies to counter posterior collapse, including\nsome never tested for language models (e.g. In-\nfoV AE, LagV AE, soft free-bits, and multimodal\npriors).\n2 Density Estimation for Text\nDensity estimation for written text has a long his-\ntory (Jelinek, 1980; Goodman, 2001), but in this\nwork we concentrate on neural network models\n(Bengio et al., 2003), in particular, autoregressive\nones (Mikolov et al., 2010). Following common\npractice, we model sentences independently, each\na sequence x= ⟨x1,...,x n⟩of n= |x|tokens.\n2.1 Language models\nA language model (LM) prescribes the generation\nof a sentence as a sequence of categorical draws\nparameterised in context, i.e. P(x|θ) =\n|x|∏\ni=1\nP(xi|x<i,θ) =\n|x|∏\ni=1\nCat(xi|f(x<i; θ)) . (1)\nTo condition on all of the available context, a ﬁxed\nNN f(·) maps from a preﬁx sequence (denoted\nx<i) to the parameters of a categorical distribution\nover the vocabulary. We estimate the parametersθ\nof the model by searching for a local optimum of\nthe log-likelihood function L(θ)= EX[log P(x|θ)]\nvia stochastic gradient-based optimisation (Rob-\nbins and Monro, 1951; Bottou and Cun, 2004),\nwhere the expectation is taken w.r.t. the true data\ndistribution and approximated with samplesx∼D\nfrom a data set of i.i.d. observations. Throughout,\nwe refer to this model as RNNLM alluding to a par-\nticular choice of f(·; φ) that employs a recurrent\nneural network (Mikolov et al., 2010).\n2.2 Deep generative language models\nBowman et al. (2016) model observations as draws\nfrom the marginal of a DGM. An NN maps from a\nlatent sentence embeddingz∈Rdz to a distribution\nP(x|z,θ) over sentences,\nP(x|θ) =\n∫\np(z)P(x|z,θ)dz\n=\n∫\nN(z|0,I)\n|x|∏\ni=1\nCat(xi|f(z,x<i; θ))dz,\n(2)\nwhere zfollows a standard Gaussian prior.1 Gen-\neration still happens one word at a time without\nMarkov assumptions, but f(·) now conditions on z\nin addition to the observed preﬁx. The conditional\nP(x|z,θ) is commonly referred to as generator\nor decoder. The quantity P(x|θ) is the marginal\nlikelihood, essential for parameter estimation.\nThis model is trained to assign a high (marginal)\nprobability to observations, much like standard\nLMs. Unlike standard LMs, it employs a latent\nspace which can accommodate a low-dimensional\nmanifold where discrete sentences are mapped to,\nvia posterior inference p(z|x,θ), and from, via gen-\neration P(x|z,θ). This gives the model an explicit\nmechanism to exploit neighbourhood and smooth-\nness in latent space to capture regularities in data\nspace. For example, it may group sentences accord-\ning to latent factors (e.g. lexical choices, syntactic\ncomplexity, etc.). It also gives users a mechanism\nto steer generation towards a speciﬁc purpose. For\nexample, one may be interested in generating sen-\ntences that are mapped from the neighbourhood of\nanother in latent space. To the extent this embed-\nding space captures appreciable regularities, inter-\nest in this property is heightened.\nApproximate inference Marginal inference for\nthis model is intractable and calls for variational\ninference (VI; Jordan et al., 1999), whereby an\nauxiliary and independently parameterised model\nq(z|x,λ) approximates the true posterior p(z|x,θ).\nWhen this inference model is itself parameterised\nby a neural network, we have a case of amortised\ninference (Kingma and Welling, 2014; Rezende\net al., 2014) and an instance of what is known as\na V AE. Bowman et al. (2016) approach posterior\ninference with a Gaussian model\nZ|λ,x ∼N(u,diag(s ⊙s))\n[u,s] = g(x; λ) (3)\nwhose parameters, i.e. a location vector u ∈RD\nand a scale vector s ∈RD\n>0, are predicted by a neu-\nral network architecture g(·; λ) from an encoding\nof the complete observation x.2 In this work, we\nuse a bidirectional recurrent encoder. Throughout\nthe text we will refer to this model as SENVAE.\nParameter estimation We can jointly estimate\nthe parameters of both models (i.e. generative and\n1We use uppercase P(·) for probability mass functions\nand lowercase p(·) for probability density functions.\n2We use boldface for deterministic vectors and ⊙for ele-\nmentwise multiplication.\n7222\nDecoding Generated sentence\nGreedy The company said it expects to report net in-\ncome of $UNK-NUM million\nSample They are getting out of my own things ?\nIBM also said it will expect to take next year .\n(a) Greedy generation from prior samples (top) yields the\nsame sentence every time, showing that the latent code is\nignored. Yet, ancestral sampling (bottom) produces good\nsentences, showing that the recurrent decoder learns about\nthe structure of English sentences.\nThe two sides hadn’t met since Oct. 18.\nI don’t know how much money will be involved.\nThe speciﬁc reason for gold is too painful.\nThe New Jersey Stock Exchange Composite Index gained 1 to 16.\nAnd some of these concerns aren’t known.\nPrices of high-yield corporate securities ended unchanged.\n(b) Homotopy: ancestral samples mapped from points along a\nlinear interpolation of two given sentences as represented in latent\nspace. The sentences do not seem to exhibit any coherent relation,\nshowing that the model does not exploit neighbourhood in latent\nspace to capture regularities in data space.\nFigure 1: Sentences generated from Bowman et al. (2016)’s V AE trainedwithout special treatment.\ninference) by locally maximising a lower-bound on\nthe log-likelihood function (ELBO)\nE(θ,λ) = EX\n[\nEq(z|x,λ) [log P(x|z,θ)]\n−KL(q(z|x,λ)||p(z))\n]\n.\n(4)\nFor as long as we can reparameterise samples from\nq(z|x,λ) using a ﬁxed random source, automatic\ndifferentiation (Baydin et al., 2018) can be used to\nobtain unbiased gradient estimates of the ELBO\n(Kingma and Welling, 2014; Rezende et al., 2014).\n3 Posterior Collapse\nIn VI, we make inferences using an approxi-\nmation q(z|x,λ) to the true posterior p(z|x,θ)\nand choose λ as to minimise the KL divergence\nEX[KL(q(z|x,λ)||p(z|x,θ))]. The same principle\nyields a lower-bound on log-likelihood used to es-\ntimate θjointly with λ, thus making the true pos-\nterior p(z|x,θ) a moving target. If the estimated\nconditional P(x|z,θ) can be made independent of\nz, which in our case means relying exclusively on\nx<i to predict the distribution of Xi, the true pos-\nterior will be independent of the data and equal to\nthe prior.3 Based on such observation, Chen et al.\n(2017) argue that information that can be modelled\nby the generator without using latent variables will\nbe modelled that way—precisely because when\nno information is encoded in the latent variable\nthe true posterior equals the prior and it is then\ntrivial to reduce EX[KL(q(z|x,λ)||p(z|x,θ))] to\n0. This is typically diagnosed by noting that af-\nter training KL(q(z|x,λ)||p(z)) →0 for most x:\nwe say that the true posterior collapses to the\nprior. Alemi et al. (2018) show that the rate,\nR = EX[KL(q(z|x,λ)||p(z))], is an upperbound\nto I(X; Z|λ), the mutual information (MI) be-\ntween X and Z. Thus, if KL(q(z|x,λ)||p(z)) is\n3This follows trivially from the deﬁnition of posterior:\np(z|x) = p(z)P(x|z)\nP(x)\nX⊥Z\n= p(z)P(x)\nP(x) = p(z).\nclose to zero for most training instances, MI is ei-\nther 0 or negligible. They also show that the distor-\ntion, D = −EX[Eq(z|x,λ)[log P(x|z,θ)]], relates\nto a lower-bound on MI (the lower-bound being\nH−D, where H is the unknown data entropy).\nA generator that makes no Markov assumptions,\nsuch as a recurrent LM, can potentially achieve\nXi ⊥ Z | x<i,θ, and indeed many have no-\nticed that V AEs whose observation models are pa-\nrameterised by such strong generators (or strong\ndecoders) tend to ignore the latent representa-\ntion (Bowman et al., 2016; Higgins et al., 2017;\nSønderby et al., 2016; Zhao et al., 2018b). For this\nreason, a strategy to prevent posterior collapse is to\nweaken the decoder (Yang et al., 2017; Semeniuta\net al., 2017; Park et al., 2018). In this work, we are\ninterested in employing strong generators, thus we\ndo not investigate weaker decoders. Other strate-\ngies involve changes to the optimisation procedure\nand manipulations to the objective that target local\noptima of the ELBO with non-negligible MI.\nAnnealing Bowman et al. (2016) propose “KL\nannealing”, whereby the KL term in the ELBO\nis incorporated into the objective in gradual steps.\nThis way the optimiser can focus on reducing dis-\ntortion early on in training, potentially by increas-\ning MI. They also propose to drop words from\nx<i at random to weaken the decoder—intuitively\nthe model would have to rely on zto compensate\nfor missing history. We experiment with a slight\nmodiﬁcation of word dropout whereby we slowly\nvary the dropout rate from 1 →0. In a sense, we\n“anneal” from a weak to a strong generator.\nTargeting rates Another idea is to target a pre-\nspeciﬁed rate (Alemi et al., 2018). Kingma\net al. (2016) replace the KL term in the ELBO\nwith max(r,KL(q(z|x,λ)||p(z))), dubbed free\nbits (FB) because it allows encoding the ﬁrst\n7223\nr nats of information “for free”. As long as\nKL(q(z|x,λ)||p(z)) < r, this does not optimise\na proper ELBO (it misses the KL term), and the\nmax introduces a discontinuity. Chen et al. (2017)\npropose soft free bits (SFB), that instead multiplies\nthe KL term in the ELBO with a weighing factor\n0 <β ≤1 that is dynamically adjusted based on\nthe target rate r: βis incremented (or reduced) by\nωif R>γr (or R<εr ). Note that this technique\nrequires hyperparameters (i.e. γ,ε,ω ) besides rto\nbe tuned in order to determine how βis updated.\nChange of objective We may also seek alterna-\ntives to the ELBO as an objective and relate them\nto quantities of interest such as MI. A simple adap-\ntation of the ELBO weighs its KL-term by a con-\nstant factor (β-V AE; Higgins et al., 2017). Setting\nβ <1 promotes increased MI. Whilst being a use-\nful counter to posterior collapse, low βmight lead\nto variational posteriors becoming point estimates.\nInfoV AE (Zhao et al., 2018b) mitigates this with\na term aimed at minimising the divergence from\nthe aggregated posterior q(z|λ) = EX[q(z|x,λ)]\nto the prior. Following Zhao et al. (2018b), we ap-\nproximate this with an estimate of maximum mean\ndiscrepancy (MMD; Gretton et al., 2012) in our ex-\nperiments. Lagrangian V AE (LagV AE; Zhao et al.,\n2018a) casts V AE optimisation as a dual problem;\nit targets either maximisation or minimisation of\n(bounds on) I(X; Z|λ) under constraints on the In-\nfoV AE objective. In MI-maximisation mode, Lag-\nV AE maximises a weighted lower-bound on MI,\n−αD, under two constraints, a maximum -ELBO\nand a maximum MMD, that preventp(z|x,θ) from\ndegenerating to a point mass. Reasonable values\nfor these constraints have to be found empirically.\n4 Minimum Desired Rate\nWe propose minimum desired rate (MDR), a tech-\nnique to attain ELBO values at a pre-speciﬁed rate\nrthat does not suffer from the gradient discontinu-\nities of FB, and does not introduce the additional\nhyperparameters of SFB. The idea is to optimise\nthe ELBO subject to a minimum rate constraint r:\nmax\nθ,λ\nE(θ,λ),\ns.t. EX [KL(q(z|x,λ)||p(z))] >r.\n(5)\nBecause constrained optimisation is generally in-\ntractable, we optimise the Lagrangian (Boyd and\nVandenberghe, 2004)Φ(θ,λ,u ) =\nE(θ,λ) −u(r−EX[KL(q(z|x,λ)||p(z))]) (6)\nwhere u ∈ R≥0 is a positive Lagrangian mul-\ntiplier. We deﬁne the dual function φ(u) =\nmaxθ,λ Φ(θ,λ,u ) and solve the dual problem\nminu∈R≥0 φ(u). Local minima of the resulting\nmin-max objective can be found by performing\nstochastic gradient descent with respect to uand\nstochastic gradient ascent with respect to θ,λ.\n4.1 Relation to other techniques\nIt is insightful to compare MDR to the various\ntechniques we surveyed in terms of the gradients\ninvolved in their optimisation. The losses min-\nimised by KL annealing, β-V AE, and SFB have\nthe form ℓβ(θ,λ) = D+ βR, where β ≥0. FB\nminimises the loss ℓFB(θ,λ) = D+ max(r,R),\nwhere r >0 is the target rate. Last, with respect\nto θand λ, MDR minimises the loss ℓMDR(θ,λ) =\nD + R+ u(r −R), where u ∈R≥0 is the La-\ngrangian multiplier. And with respect to u, MDR\nminimises φ(u) = −D−R−u(R−r).\nLet us inspect gradients with respect to the pa-\nrameters of the V AE, namely,θand λ. FB’s gradi-\nent ∇θ,λℓFB(θ,λ) =\n∇θ,λD+\n{\n0 if R≤r\n∇θ,λR otherwise (7a)\nis discontinuous, that is, there is a sudden ‘jump’\nfrom zero to a (possibly) large gradient w.r.t. R\nwhen the rate dips above r. KL annealing, β-V AE,\nand SFB do not present such discontinuity\n∇θ,λℓβ(θ,λ) = ∇θ,λD+ β∇θ,λR, (7b)\nfor βscales the gradient w.r.t. R. The gradient of\nthe MDR objective is\n∇θ,λℓMDR(θ,λ) = ∇θ,λD+(1 −u)∇θ,λR (7c)\nwhich can be thought of as ∇θ,λℓβ(θ,λ) with β\ndynamically set to 1 −uat every gradient step.\nHence, MDR is another form of KL weighing,\nalbeit one that targets a speciﬁc rate. Compared\nto β-V AE, MDR has the advantage that β is not\nﬁxed but estimated to meet the requirements on\nrate. Compared to KL-annealing, MDR dispenses\nwith a ﬁxed schedule for updating β, not only an-\nnealing schedules are ﬁxed, they require multiple\ndecisions (e.g. number of steps, linear or expo-\nnential increments) whose impact on the objective\nare not directly obvious. Most similar then, seems\nSFB. Like MDR, it ﬂexibly updates βby targeting\n7224\na rate. However, differences between the two tech-\nniques become apparent when we observe how β\nis updated. In case of SFB:\nβ(t+1) = β(t) +\n{\nω if R>γr\n−ω if R<εr (8a)\nwhere ω, γand εare hyperparameters. In case of\nMDR (not taking optimiser-speciﬁc dynamics into\naccount):\nu(t+1) = u(t) −ρ∂φ(u)\n∂u = u(t) + ρ(R−r) (8b)\nwhere ρis a learning rate. From this, we conclude\nthat MDR is akin to SFB, but MDR’s update rule is\na direct consequence of Lagrangian relaxation and\nthus dispenses with the additional hyperparameters\nin SFB’s handcrafted update rule.4\n5 Expressive Priors\nSuppose we employ a multimodal prior p(z|θ),\ne.g. a mixture of Gaussians, and suppose we\nemploy a unimodal posterior approximation, e.g.\nthe typical diagonal Gaussian. This creates a\nmismatch between the prior and the posterior ap-\nproximation families that makes it impossible for\nKL(q(z|x,λ)||p(z|θ)) to be precisely0. For the ag-\ngregated posterior q(z|λ) to match the prior, the in-\nference model would have to—on average—cover\nall of the prior’s modes. Since the inference net-\nwork is deterministic, it can only do so as a func-\ntion of the conditioning input x, thus increasing\nI(X; Z|λ). Admittedly, this conditioning might\nstill only capture shallow features of x, and the\ngenerator may still choose to ignore the latent code,\nkeeping I(X; Z|θ) low, but the potential seems to\njustify an attempt. This view builds upon Alemi\net al. (2018)’s information-theoretic view which\nsuggests that the prior regularises the inference\nmodel capping I(X; Z|λ). Thus, we modify S EN-\nVAE to employ a more complex, ideally multi-\nmodal, parametric prior p(z|θ) and ﬁt its parame-\nters.\nMoG Our ﬁrst option is a uniform mixture of\nGaussians (MoG), i.e. p(z|θ) =\n1\nC\nC∑\nc=1\nN(z|µ(c),diag(σ(c) ⊙σ(c))) (9)\n4Note that if we set γ = 1, ε= 1, and ω = ρ(R−r) at\nevery step of SFB, we recover MDR.\nwhere the Gaussian parameters are optimised along\nwith other generative parameters. Note that though\nwe give this prior up to C modes, the optimiser\nmight merge some of them (by learning approxi-\nmately the same location and scale).\nVampPrior Motivated by the fact that, for a ﬁxed\nposterior approximation, the prior that optimises\nthe ELBO equals EX[q(z|x,λ)], Tomczak and\nWelling (2018) propose the VampPrior, a varia-\ntional mixture of posteriors:\np(z|θ) = 1\nC\nC∑\nc=1\nq(z|v(c),λ) (10)\nwhere v(c) is a learned pseudo input—in their case\na continuous vector. Again the parameters of the\nprior, i.e. {v(c)}C\nc=1, are optimised in the ELBO.\nIn our case, the input to the inference network is a\ndiscrete sentence, which is incompatible with the\ndesign of the VampPrior. Thus, we propose to by-\npass the inference network’s embedding layer and\nestimate a sequence of word embeddings, which\nmakes up a pseudo input. That is,v(c) is a sequence\n⟨v(c)\n1 ,..., v(c)\nlc ⟩where v(c)\ni has the dimensionality\nof our embeddings, and lc is the length of the se-\nquence (ﬁxed at the beginning of training). Note,\nhowever, that for this prior to be multimodal, the\ninference model must already encode information\nin Z, thus there is some gambling in its design.\n6 Experiments\nOur goal is to identify which techniques are effec-\ntive in training V AEs for language modelling. Our\nevaluation concentrates on intrinsic metrics: neg-\native log-likelihood (NLL), perplexity per token\n(PPL), rate (R), distortion (D), the number of ac-\ntive units (AU; Burda et al., 2015)) 5 and gap in\nthe accuracy of next word prediction (given gold\npreﬁxes) when decoding from a posterior sample\nversus decoding from a prior sample (Accgap).\nFor V AE models, NLL (and thus PPL) can only\nbe estimated. We use importance sampling (IS)\nP(x|θ) =\n∫\np(z,x|θ)dz IS=\n∫\nq(z|x)p(z,x|θ)\nq(z|x) dz\nMC\n≈ 1\nS\nS∑\ns=1\np(z(s),x|θ)\nq(z(s)|x) where z(s) ∼q(z|x) (11)\n5A latent unit (a single dimension of z) is denoted active\nwhen its variance with respect to xis larger than 0.01.\n7225\nTechnique Hyperparameters\nKL annealing increment γ(2 ×10−5)\nWord dropout (WD) decrement γ(2 ×10−5)\nFB and MDR target rate r(5)\nSFB r(6.46), γ(1.05), ε(1), ω(0.01)\nβ-V AE KL weight β(0.66)\nInfoV AE β(0.7), λ(31.62)\nLagV AE α(−21.7), target MMD ( 0.0017)\ntarget -ELBO ( 100.8)\nTable 1: Techniques and their hyperparameters.\nwith our trained approximate posterior as impor-\ntance distribution (we use S = 1000 samples).\nWe ﬁrst report on experiments using the English\nPenn Treebank (PTB; Marcus et al., 1993).6\nRNNLM The baseline RNNLM generator is a\nbuilding block for all of our SENVAEs, thus we\nvalidate its performance as a strong standalone gen-\nerator. We highlight that it outperforms an exter-\nnal baseline that employs a comparable number of\nparameters (Dyer et al., 2016) and that this perfor-\nmance boost is mostly due to tying embeddings\nwith the output layer.7 Appendix A.1 presents the\ncomplete architecture and a comparison.\nBayesian optimisation The techniques we com-\npare are sensitive to one or more hyperparameters\n(see Table 1), which we tune using Bayesian opti-\nmisation (BO) towards minimising estimated NLL\nof the validation data. For each technique, we ran\n25 iterations of BO, each iteration encompassing\ntraining a model to full convergence. This was suf-\nﬁcient for the hyperparameters of each technique\nto converge. See Appendix A.2 for details.\nOn optimisation strategies First, we assess the\neffectiveness of techniques that aim at promoting\nlocal optima of SENVAE with better MI trade-\noff. As for the architecture, the approximate pos-\nterior q(z|x,λ) employs a bidirectional recurrent\nencoder, and the generator P(x|z,θ) is essentially\nour RNNLM initialised with a learned projection\nof z(complete speciﬁcation in A.1). We train with\nAdam (Kingma and Ba, 2014) with default param-\neters and a learning rate of 10−3 until convergence\nﬁve times for each technique.\nResults can be found in Table 2. First, note how\n6We report on Dyer et al. (2016)’s pre-processing, rather\nthan Mikolov et al. (2010)’s. Whereas our ﬁndings are quanti-\ntatively similar, qualitative analysis based on generations are\nless interesting with Mikolov’s far too small vocabulary.\n7Stronger RNN-based models can be designed (Melis et al.,\n2018), but those use vastly more parameters.\nMode D R PPL↓ AU↑ Accgap\nRNNLM - - 107.1 ±0.5 - -\nVanilla 118.4 0.0 105.7 ±0.4 0 0.0\nAnnealing 115.3 3.3 103.7 ±0.3 17 6.0\nWD 117.6 0.0 102.5 ±0.6 0 0.0\nFB 113.3 5.0 101.9 ±0.8 14 5.8\nSFB 112.0 6.4 101.0 ±0.5 18 7.0\nMDR 113.5 5.0 102.1 ±0.5 13 6.2\nβ-V AE 113.0 5.3 101.7 ±0.5 11 6.1\nInfoV AE 113.5 4.3 100.8 ±0.4 10 5.2\nLagV AE 112.1 6.5 101.6 ±0.7 24 6.9\nTable 2: Performance (avg ±std across 5 independent\nruns) of SENVAE on the PTB validation set. Standard\ndeviations for Dand Rare at most 0.2.\nthe vanilla V AE (no special treatment) encodes no\ninformation in latent space (R= 0). Then note that\nall techniques converged to V AEs that attain better\nPPL than the RNNLM and vanilla V AE, and all but\nannealed word dropout did so at non-negligible rate.\nNotably, the two most popular techniques, word\ndropout and KL annealing, perform sub-par to the\nother techniques.8 The techniques that work well\nat non-negligible rates can be separated into two\ngroups: one based on a change of objective (i.e.,\nβ-V AE, InfoV AE and LagV AE), another based on\ntargeting a speciﬁc rate (i.e., FB, SFB, and MDR).\nInfoV AE, LagV AE and SFB all require tuning of\nmultiple hyperparameters. InfoV AE and LagV AE,\nin particular, showed poor performance without\nthis careful tuning. In the ﬁrst group, consider\nLagV AE, for example. Though Zhao et al. (2018a)\nargue that the magnitude of αis not particularly\nimportant (in MI-maximisation mode, they ﬁxed it\nto −1), we could not learn a useful SENVAE with\nLagV AE until we allowed BO to also estimate the\nmagnitude of α. Once BO converges to the values\nin Table 1, the method does perform quite well.\nGenerally, it is hard to believe that hyperparame-\nters transfer across data sets, thus it is fair to expect\nthat this exercise will have to be repeated every\ntime. We argue that the rate hyperparameter com-\nmon to the techniques in the second group is more\ninterpretable and practical in most cases. For ex-\nample, it is easy to grid-search against a handful\nof values. Hence, we further investigate FB and\nMDR by varying the target rate further (from 5 to\n50). SFB is left out, for MDR generalises SFB’s\nhandcrafted update rule. We observe that FB and\nMDR attain essentially the same PPL across rates,\n8Though here we show annealed word dropout, to focus\non techniques that do not weaken the generator, standard word\ndropout also converged to negligible rates.\n7226\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\nAnneal\nFB5\nMDR5\nJS(Cat( i), Cat( i)) JS(Cat( i), Cat( ′i))\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\nAnneal\nFB5\nMDR5\nJS(Cat( i), Cat( i)) JS(Cat( i), Cat( ′i))\nFigure 2: Sensitivity of output distributions to poste-\nrior samples measured in terms of symmetrised KL\n(JS). We obtain 51 (top) validation and 84 (bottom)\ntest instances of length 20 and report on their out-\nput distributions per time step. To account for ex-\npected variability, we report JS(Cat(πi)||Cat(ηi)) −\nJS(Cat(πi)||Cat(π′\ni)), where ηi conditions on a prior\nsample, and πi and π′\ni condition on different posterior\nsamples, averaged over 10 experiments.\nModel D R PPL↓ AU↑ Accgap\nRNNLM - - 84.5 ±0.5 - -\nN/N 103.5 5.0 81.5 ±0.5 13 5.4\nMoG/N 103.3 5.0 81.4 ±0.5 32 5.8\nVamp/N 103.1 5.0 81.2 ±0.4 22 5.8\nTable 3: Performance on the PTB test set for different\npriors (N, MoG, Vamp). Standard deviations of D, R,\nand Accgap are at most 0.1.\nthough MDR attains the desired rate earlier on in\ntraining, especially for higher targets (where FB\nfails at reaching the speciﬁed rate). Importantly,\nat the end of training, the validation rate is closer\nto the target for MDR. Appendix B supports these\nclaims. Though Accgap already suggests it, Figure\n2 shows more visibly that MDR leads to output\nCategorical distributions that are more sensitive to\nthe latent encoding. We measure this sensitivity in\nterms of symmetrised KL between output distribu-\ntions obtained from a posterior sample and output\ndistributions obtained from a prior sample for the\nsame time step given an observed preﬁx.\nOn expressive priors Second, we compare the\nimpact of expressive priors. This time, prior hy-\nperparameters were selected via grid search and\ncan be found in Appendix A.1. All models are\ntrained with a target rate of 5 using MDR, with\nsettings otherwise the same as the previous experi-\nment. In Table 3 it can be seen that more expressive\npriors do not improve perplexity further,9 though\n9Here we remark that best runs (based on validation per-\nformance) do show an advantage, which stresses the need to\nreport multiple runs as we do.\nthey seem to encode more information in the la-\ntent variable—note the increased number of active\nunits and the increased gap in accuracy. One may\nwonder whether stronger priors allow us to target\nhigher rates without hurting PPL. This does not\nseem to be the case: as we increase rate to 50, all\nmodels perform roughly the same, and beyond 20\nperformance degrades quickly.10 The models did,\nhowever, show a further increase in active units\n(VampPrior) and accuracy gap (both priors). Again,\nAppendix B contains plots supporting these claims.\nGenerated samples Figure 3 shows samples\nfrom a well-trained SENVAE, where we decode\ngreedily from a prior sample—this way, all variabil-\nity is due to the generator’s reliance on the latent\nsample. Recall that a vanilla V AE ignores z and\nthus greedy generation from a prior sample is es-\nsentially deterministic in that case (see Figure 1a).\nNext to the samples we show the closest training\ninstance, which we measure in terms of an edit\ndistance (TER; Snover et al., 2006). 11 This “near-\nest neighbour” helps us assess whether the genera-\ntor is producing novel text or simply reproducing\nsomething it memorised from training. In Figure 4\nwe show a homotopy: here we decode greedily\nfrom points lying between a posterior sample con-\nditioned on the ﬁrst sentence and a posterior sample\nconditioned on the last sentence. In contrast to the\nvanilla V AE (Figure 1b), neighbourhood in latent\nspace is now used to capture some regularities in\ndata space. These samples add support to the quan-\ntitative evidence that our DGMs have been trained\nnot to neglect the latent space. In Appendix B we\nprovide more samples.\nOther datasets To address the generalisability\nof our claims to other, larger, datasets, we report\nresults on the Yahoo and Yelp corpora (Yang et al.,\n2017) in Table 4. We compare to the work of\nHe et al. (2019), who proposed to mitigate pos-\nterior collapse with aggressive training of the in-\nference network, optimising the inference network\nmultiple steps for each step of the generative net-\nwork.12. We report on models trained with the\nstandard prior as well as an MoG prior both op-\n10We also remark that, without MDR, the MoG model at-\ntains validation rate of about 2.5.\n11This distance metric varies from0 to 1, where1 indicates\nthesentenceiscompletelynoveland 0indicatesthesentenceis\nessentiallycopiedfromthetrainingdata.\n12Toenabledirectcomparisonwereplicatedtheexperimental\nsetup from (He et al., 2019) and built our methods into their\ncodebase.\n7227\nYahoo Yelp\nModel R NLL↓ PPL↓ AU↑ R NLL↓ PPL↓ AU↑\nRNNLM - 328.0±0.3 - - - 358.1±0.6 - -\nLagging 5.7±0.7 326.7±0.1 - 15.0±3.5 3.8±0.2 355.9±0.1 - 11.3±1.0\nβ-V AE (β = 0.4) 6.3±1.7 328.7±0.1 - 8.0±5.2 4.2±0.4 358.2±0.3 - 4.2±3.8\nAnnealing 0.0±0.0 328.6±0.0 - 0.0±0.0 0.0±0.0 357.9±0.1 - 0.0±0.0\nVanilla 0.0±0.0 328.9±0.1 61.4±0.1 0.0±0.0 0.0±0.0 358.3±0.2 40.8±0.1 0.0±0.0\nN/N 5.0±0.0 328.1±0.1 60.8±0.1 4.0±0.7 5.0±0.0 357.5±0.2 40.4±0.1 4.2±0.4\nMoG/N 5.0±0.1 327.5±0.2 60.5±0.1 5.0±0.7 5.0±0.0 359.5±0.5 41.2±0.3 2.2±0.4\nTable 4: Performance on the Yahoo/Yelp corpora. Top rows taken from (He et al., 2019)\nSample Closest training instance TER\nFor example, the Dow Jones Industrial Average fell almost\n80 points to close at 2643.65.\nBy futures-related program buying, the Dow Jones Indus-\ntrial Average gained 4.92 points to close at 2643.65.\n0.38\nThe department store concern said it expects to report\nproﬁt from continuing operations in 1990.\nRolls-Royce Motor Cars Inc. said it expects its U.S. sales\nto remain steady at about 1,200 cars in 1990.\n0.59\nThe new U.S. auto makers say the accord would require\nbanks to focus on their core businesses of their own ac-\ncount.\nInternational Minerals said the sale will allow Mallinck-\nrodt to focus its resources on its core businesses of medical\nproducts, specialty chemicals and ﬂavors.\n0.78\nFigure 3: Samples from S ENVAE (MoG prior) trained via MDR: we sample from the prior and decode greedily.\nWe also show the closest training instance in terms of a string edit distance (TER).\nThe inquiry soon focused on the judge.\nThejudgedeclinedtocommentontheﬂoor.\nThejudgewasdismissedaspartofthesettlement.\nThejudgewassentencedtodeathinprison.\nTheannouncementwasﬁledagainsttheSEC.\nTheofferwasmisstatedinlateSeptember.\nTheofferwasﬁledagainstbankruptcycourtinNewYork.\nThe letter was dated Oct. 6.\nFigure 4: Latent space homotopy from a properly\ntrained S ENVAE. Note the smooth transition of topic\nand grammatically of the samples.\ntimised with MDR, and a model trained without\noptimisation techniques.13 It can be seen that MDR\ncompares favourably to other optimisation tech-\nniques reported in (He et al., 2019). Whilst ag-\ngressive training of the inference network performs\nslightly better in terms of NLL and leads to more\nactive units, it slows down training by a factor of\n4. The MoG prior improves results on Yahoo but\nnot on Yelp. This may indicate that a multimodal\nprior does offer useful extra capacity to the latent\nspace,14 at the cost of more instability in optimisa-\ntion. This conﬁrms that targeting a pre-speciﬁed\nrate leads to V AEs that are not collapsed without\nhurting NLL.\n13We focus on MoG since the PTB experiments showed the\nVampPriortounderperformintermsofAU.\n14We tracked the average KL divergence between any two\ncomponents of the prior and observed that the prior remained\nmultimodal.\nRecommendations We recommend targeting a\nspeciﬁc rate via MDR instead of annealing (or word\ndropout). Besides being simple to implement, it\nis fast and straightforward to use: pick a rate by\nplotting validation performance against a handful\nof values. Stronger priors, on the other hand, while\nshowing indicators of higher mutual information\n(e.g. AU and Acc gap), seem less effective than\nMDR. Use IS estimates of NLL, rather than single-\nsample ELBO estimates, for model selection, for\nthe latter can be too loose of a bound and too heav-\nily inﬂuenced by noisy estimates of KL. 15 Use\nmany samples for a tight bound. 16 Inspect sen-\ntences greedily decoded from a prior (or posterior)\nsample as this shows whether the generator is at\nall sensitive to the latent code. Retrieve nearest\nneighbours to spot copying behaviour.\n7 Related Work\nIn NLP, posterior collapse was probably ﬁrst no-\nticed by Bowman et al. (2016), who addressed it via\nword dropout and KL scaling. Further investigation\nrevealed that in the presence of strong generators,\n15This point seems obvious to many, but enough published\npapersreportexponentiatedlossordistortionpertoken,which,\nbesidesunreliable,makecomparisonsacrosspapersdifﬁcult.\n16Weuse1000samples. Comparedtoasinglesampleestimate,\nwe have observed differences up to5 perplexity points in non-\ncollapsedmodels. From 100to1000samples,differencesarein\ntheorderof 0.1suggestingourISestimateisclosetoconvergence.\n7228\nthe ELBO itself becomes the culprit (Chen et al.,\n2017; Alemi et al., 2018), as it lacks a preference\nregarding MI. Posterior collapse has also been as-\ncribed to approximate inference (Kim et al., 2018;\nDieng and Paisley, 2019). Beyond the techniques\ncompared and developed in this work, other solu-\ntions have been proposed, including modiﬁcations\nto the generator (Semeniuta et al., 2017; Yang et al.,\n2017; Park et al., 2018; Dieng et al., 2019), side\nlosses based on weak generators (Zhao et al., 2017),\nfactorised likelihoods (Ziegler and Rush, 2019; Ma\net al., 2019), cyclical annealing (Liu et al., 2019)\nand changes to the ELBO (Tolstikhin et al., 2018;\nGoyal et al., 2017).\nExploiting a mismatch in correlation between\nthe prior and the approximate posterior, and thus\nforcing a lower-bound on the rate, is the princi-\nple behind δ-V AEs (Razavi et al., 2019) and hy-\nperspherical V AEs (Xu and Durrett, 2018). The\ngenerative model of δ-V AEs has one latent variable\nper step of the sequence, i.e. z = ⟨z1,...,z |x|⟩,\nmaking it quite different from that of the SEN-\nVAEs considered here. Their mean-ﬁeld infer-\nence model is a product of independent Gaussians,\none per step, but they construct a correlated Gaus-\nsian prior by making the prior distribution over\nthe next step depend linearly on the previous step,\ni.e. Zi|zi−1 ∼ N(αzi−1,σ) with hyperparame-\nters αand σ. Hyperspherical V AEs work on the\nunit hypersphere with a uniform prior and a non-\nuniform V onMises-Fisher posterior approximation\n(Davidson et al., 2018). Note that, though in this pa-\nper we focused on Gaussian (and mixture of Gaus-\nsians, e.g. MoG and VampPrior) priors, MDR is\napplicable for whatever choice of prescribed prior.\nWhether its beneﬁts stack with the effects due to\ndifferent priors remains an empirical question.\nGECO (Rezende and Viola, 2018) casts V AE op-\ntimisation as a dual problem, and in that it is closely\nrelated to our MDR and the LagV AE. GECO tar-\ngets minimisation of EX[KL(q(z|x,λ)||p(z))] un-\nder constraints on distortion, whereas LagV AE\ntargets either maximisation or minimisation of\n(bounds on) I(X; Z|λ) under constraints on the\nInfoV AE objective. Contrary to MDR, GECO fo-\ncuses on latent space regularisation and offers no\nexplicit mechanism to mitigate posterior collapse.\nRecently Li et al. (2019) proposed to combine\nFB, KL scaling, and pre-training of the inference\nnetwork’s encoder on an auto-encoding objective.\nTheir techniques are complementary to ours in so\nfar as their main ﬁnding—the mutual beneﬁts of an-\nnealing, pre-training, and lower-bounding KL—is\nperfectly compatible with ours (MDR and multi-\nmodal priors).\n8 Discussion\nSENVAE is a deep generative model whose gener-\native story is rather shallow, yet, due to its strong\ngenerator component, it is hard to make effective\nuse of the extra knob it offers. In this paper, we\nhave introduced and compared techniques for effec-\ntive estimation of such a model. We show that many\ntechniques in the literature perform reasonably sim-\nilarly (i.e. FB, SFB, β-V AE, InfoV AE), though\nthey may require a considerable hyperparameter\nsearch (e.g. SFB and InfoV AE). Amongst these,\nour proposed optimisation subject to a minimum\nrate constraint is simple enough to tune (as FB it\nonly takes a pre-speciﬁed rate and unlike FB it does\nnot suffer from gradient discontinuities), superior\nto annealing and word dropout, and require less\nresources than strategies based on multiple anneal-\ning schedules and/or aggressive optimisation of the\ninference model. Other ways to lower-bound rate,\nsuch as by imposing a multimodal prior, though\npromising, still require a minimum desired rate.\nThe typical RNNLM is built upon an exact fac-\ntorisation of the joint distribution, thus a well-\ntrained architecture is hard to improve upon in\nterms of log-likelihood of gold-standard data. Our\ninterest in latent variable models stems from the de-\nsire to obtain generative stories that are less opaque\nthan that of an RNNLM, for example, in that they\nmay expose knobs that we can use to control gen-\neration and a hierarchy of steps that may award a\ndegree of interpretability to the model. The SEN-\nVAE is not that model, but it is a crucial building\nblock in the pursue for hierarchical probabilistic\nmodels of language. We hope this work, i.e. the\norganised review it contributes and the techniques\nit introduces, will pave the way to deeper—in sta-\ntistical hierarchy—generative models of language.\nAcknowledgments\nThis project has received funding from the Euro-\npean Union’s Horizon 2020 research and innova-\ntion programme under grant agreement No 825299\n(GoURMET).\n7229\nReferences\nAlexander Alemi, Ben Poole, Ian Fischer, Joshua Dil-\nlon, Rif A Saurous, and Kevin Murphy. 2018. Fix-\ning a broken elbo. In International Conference on\nMachine Learning, pages 159–168.\nThe GPyOpt authors. 2016. GPyOpt: A bayesian op-\ntimization framework in python. http://github.\ncom/SheffieldML/GPyOpt.\nAtilim Gunes Baydin, Barak A Pearlmutter, Alexey An-\ndreyevich Radul, and Jeffrey Mark Siskind. 2018.\nAutomatic differentiation in machine learning: a sur-\nvey. Journal of Marchine Learning Research, 18:1–\n43.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nJames Bergstra and Yoshua Bengio. 2012. Random\nsearch for hyper-parameter optimization. Journal of\nMachine Learning Research, 13(Feb):281–305.\nL´eon Bottou and Yann L. Cun. 2004. Large scale\nonline learning. In S. Thrun, L. K. Saul, and\nB. Sch ¨olkopf, editors, Advances in Neural Informa-\ntion Processing Systems 16 , pages 217–224. MIT\nPress.\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Confer-\nence on Computational Natural Language Learning,\npages 10–21.\nStephen Boyd and Lieven Vandenberghe. 2004. Con-\nvex optimization. Cambridge university press.\nYuri Burda, Roger Grosse, and Ruslan Salakhutdinov.\n2015. Importance weighted autoencoders. arXiv\npreprint arXiv:1509.00519.\nXi Chen, Diederik P Kingma, Tim Salimans, Yan Duan,\nPrafulla Dhariwal, John Schulman, Ilya Sutskever,\nand Pieter Abbeel. 2017. Variational lossy autoen-\ncoder. In International Conference on Machine\nLearning.\nCaio Corro and Ivan Titov. 2018. Differentiable\nperturb-and-parse: Semi-supervised parsing with a\nstructured variational autoencoder. In ICLR.\nTim R Davidson, Luca Falorsi, Nicola De Cao,\nThomas Kipf, and Jakub M Tomczak. 2018. Hyper-\nspherical variational auto-encoders. arXiv preprint\narXiv:1804.00891.\nAdji Dieng and John Paisley. 2019. Reweighted expec-\ntation maximization. Technical report.\nAdji B. Dieng, Yoon Kim, Alexander M. Rush, and\nDavid M. Blei. 2019. Avoiding latent variable col-\nlapse with generative skip models. In Proceed-\nings of Machine Learning Research , volume 89 of\nProceedings of Machine Learning Research , pages\n2397–2405. PMLR.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 199–209. Association for Com-\nputational Linguistics.\nBryan Eikema and Wilker Aziz. 2019. Auto-encoding\nvariational neural machine translation. In Proceed-\nings of the 4th Workshop on Representation Learn-\ning for NLP (RepL4NLP-2019), pages 124–141, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMarco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet,\nand Ole Winther. 2016. Sequential neural models\nwith stochastic layers. In Advances in neural infor-\nmation processing systems, pages 2199–2207.\nYarin Gal and Zoubin Ghahramani. 2015. Dropout as\na Bayesian approximation: Insights and applications.\nIn Deep Learning Workshop, ICML.\nJoshua T. Goodman. 2001. A bit of progress in lan-\nguage modeling. Comput. Speech Lang., 15(4):403–\n434.\nAnirudh Goyal Alias Parth Goyal, Alessandro Sor-\ndoni, Marc-Alexandre Cˆot´e, Nan Rosemary Ke, and\nYoshua Bengio. 2017. Z-forcing: Training stochas-\ntic recurrent networks. In Advances in neural infor-\nmation processing systems, pages 6713–6723.\nArthur Gretton, Karsten M Borgwardt, Malte J Rasch,\nBernhard Sch¨olkopf, and Alexander Smola. 2012. A\nkernel two-sample test. Journal of Machine Learn-\ning Research, 13(Mar):723–773.\nJunxian He, Daniel Spokoyny, Graham Neubig, and\nTaylor Berg-Kirkpatrick. 2019. Lagging inference\nnetworks and posterior collapse in variational au-\ntoencoders. In Proceedings of ICLR.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher\nBurgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, and Alexander Lerchner. 2017. beta-\nV AE: Learning basic visual concepts with a con-\nstrained variational framework. In International\nConference on Learning Representations.\nFrederick Jelinek. 1980. Interpolated estimation of\nmarkov source parameters from sparse data. InProc.\nWorkshop on Pattern Recognition in Practice, 1980.\nMichaelI. Jordan, Zoubin Ghahramani, TommiS.\nJaakkola, and LawrenceK. Saul. 1999. An intro-\nduction to variational methods for graphical models.\nMachine Learning, 37(2):183–233.\n7230\nYoon Kim, Sam Wiseman, Andrew Miller, David Son-\ntag, and Alexander Rush. 2018. Semi-amortized\nvariational autoencoders. In International Confer-\nence on Machine Learning, pages 2683–2692.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nDiederik P Kingma, Tim Salimans, Rafal Jozefowicz,\nXi Chen, Ilya Sutskever, and Max Welling. 2016.\nImproved variational inference with inverse autore-\ngressive ﬂow. In Advances in Neural Information\nProcessing Systems, pages 4743–4751.\nDiederik P. Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In International Confer-\nence on Learning Representations.\nBohan Li, Junxian He, Graham Neubig, Taylor Berg-\nKirkpatrick, and Yiming Yang. 2019. A surprisingly\neffective ﬁx for deep latent variable modeling of text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3601–\n3612, Hong Kong, China. Association for Computa-\ntional Linguistics.\nXiaodong Liu, Jianfeng Gao, Asli Celikyilmaz,\nLawrence Carin, et al. 2019. Cyclical annealing\nschedule: A simple approach to mitigating KL van-\nishing. In NAACL.\nChunchuan Lyu and Ivan Titov. 2018. AMR parsing as\ngraph prediction with latent alignment. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 397–407, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nXuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-\nbig, and Eduard Hovy. 2019. FlowSeq: Non-\nautoregressive conditional sequence generation with\ngenerative ﬂow. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 4281–4291, Hong Kong, China. As-\nsociation for Computational Linguistics.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of english: The penn treebank. Computa-\ntional linguistics, 19(2):313–330.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In ICLR.\nYishu Miao and Phil Blunsom. 2016. Language as a\nlatent variable: Discrete generative models for sen-\ntence compression. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 319–328. Association for Compu-\ntational Linguistics.\nYishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural\nvariational inference for text processing. In Interna-\ntional conference on machine learning, pages 1727–\n1736.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Eleventh\nAnnual Conference of the International Speech Com-\nmunication Association.\nYookoon Park, Jaemin Cho, and Gunhee Kim. 2018.\nA hierarchical latent structure for variational conver-\nsation modeling. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n1792–1801. Association for Computational Linguis-\ntics.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in pytorch.\nIn NIPS-W.\nYunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan,\nChunyuan Li, Andrew Stevens, and Lawrence Carin.\n2016. Variational autoencoder for deep learning\nof images, labels and captions. In D. D. Lee,\nM. Sugiyama, U. V . Luxburg, I. Guyon, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 29, pages 2352–2360. Curran Asso-\nciates, Inc.\nCarl Edward Rasmussen and Christopher K. I.\nWilliams. 2005. Gaussian Processes for Ma-\nchine Learning (Adaptive Computation and Ma-\nchine Learning). The MIT Press.\nAli Razavi, A¨aron van den Oord, Ben Poole, and Oriol\nVinyals. 2019. Preventing posterior collapse with\ndelta-V AEs. InICLR.\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan\nWierstra. 2014. Stochastic backpropagation and ap-\nproximate inference in deep generative models. In\nProceedings of the 31st International Conference on\nMachine Learning, volume 32 ofProceedings of Ma-\nchine Learning Research, pages 1278–1286, Bejing,\nChina. PMLR.\nDanilo Jimenez Rezende and Fabio Viola. 2018. Tam-\ning vaes. arXiv preprint arXiv:1810.00597.\nMiguel Rios, Wilker Aziz, and Khalil Simaan. 2018.\nDeep generative model for joint alignment and word\nrepresentation. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n1011–1023. Association for Computational Linguis-\ntics.\n7231\nHerbert Robbins and Sutton Monro. 1951. A stochas-\ntic approximation method. The Annals of Mathemat-\nical Statistics, 22(3):400–407.\nPhilip Schulz, Wilker Aziz, and Trevor Cohn. 2018.\nA stochastic decoder for neural machine translation.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1243–1252. Association for\nComputational Linguistics.\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt\nBarth. 2017. A hybrid convolutional variational au-\ntoencoder for text generation. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 627–637, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron Courville, and\nYoshua Bengio. 2017. A hierarchical latent variable\nencoder-decoder model for generating dialogues. In\nThirty-First AAAI Conference on Artiﬁcial Intelli-\ngence.\nJasper Snoek, Hugo Larochelle, and Ryan P Adams.\n2012. Practical bayesian optimization of machine\nlearning algorithms. In Advances in neural informa-\ntion processing systems, pages 2951–2959.\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\nnea Micciulla, and John Makhoul. 2006. A study of\ntranslation edit rate with targeted human annotation.\nIn Proceedings of association for machine transla-\ntion in the Americas, volume 200.\nCasper Kaae Sønderby, Tapani Raiko, Lars Maaløe,\nSøren Kaae Sønderby, and Ole Winther. 2016. Lad-\nder variational autoencoders. In Advances in neural\ninformation processing systems, pages 3738–3746.\nIlya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and\nBernhard Schoelkopf. 2018. Wasserstein auto-\nencoders. In ICLR.\nJakub M Tomczak and Max Welling. 2018. V AE with\na VampPrior. In AISTATS.\nLiwei Wang, Alexander Schwing, and Svetlana Lazeb-\nnik. 2017. Diverse and accurate image descrip-\ntion using a variational auto-encoder with an addi-\ntive gaussian encoding space. In I. Guyon, U. V .\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 30, pages 5756–\n5766. Curran Associates, Inc.\nTsung-Hsien Wen, Yishu Miao, Phil Blunsom, and\nSteve Young. 2017. Latent intention dialogue mod-\nels. In Proceedings of the 34th International Confer-\nence on Machine Learning-Volume 70, pages 3732–\n3741. JMLR. org.\nJiacheng Xu and Greg Durrett. 2018. Spherical latent\nspaces for stable variational autoencoders. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 4503–\n4513, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nZichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and\nTaylor Berg-Kirkpatrick. 2017. Improved varia-\ntional autoencoders for text modeling using dilated\nconvolutions. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning , vol-\nume 70 of Proceedings of Machine Learning Re-\nsearch, pages 3881–3890, International Convention\nCentre, Sydney, Australia. PMLR.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\nBiao Zhang, Deyi Xiong, jinsong su, Hong Duan, and\nMin Zhang. 2016. Variational neural machine trans-\nlation. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing,\npages 521–530, Austin, Texas. Association for Com-\nputational Linguistics.\nShengjia Zhao, Jiaming Song, and Stefano Ermon.\n2018a. The information autoencoding family: A\nlagrangian perspective on latent variable generative\nmodels. In Conference on Uncertainty in Artiﬁcial\nIntelligence, Monterey, California.\nShengjia Zhao, Jiaming Song, and Stefano Ermon.\n2018b. InfoV AE: Information maximizing varia-\ntional autoencoders. In Theoretical Foundations and\nApplications of Deep Generative Models, ICML18.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 654–664, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nChunting Zhou and Graham Neubig. 2017. Multi-\nspace variational encoder-decoders for semi-\nsupervised labeled sequence transduction. In\nProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 310–320. Association for\nComputational Linguistics.\nZachary M Ziegler and Alexander M Rush. 2019. La-\ntent normalizing ﬂows for discrete sequences. arXiv\npreprint arXiv:1901.10548.\n7232\nA Architectures and Hyperparameters\nIn order to ensure that all our experiments are fully\nreproducible, this section provides an extensive\noverview of the model architectures, as well as\nmodel and optimisation hyperparameters.\nSome hyperparameters are common to all ex-\nperiments, e.g. optimiser and dropout, they can\nbe found in Table 5. All models were optimised\nwith Adam using default settings (Kingma and Ba,\n2014). To regularise the models, we use dropout\nwith a shared mask across time-steps (Zaremba\net al., 2014) and weight decay proportional to the\ndropout rate (Gal and Ghahramani, 2015) on the\ninput and output layers of the generative networks\n(i.e. RNNLM and the recurrent decoder in SEN-\nVAE). No dropout is applied to layers of the in-\nference network as this does not lead to consistent\nempirical beneﬁts and lacks a good theoretical ba-\nsis. Gradient norms are clipped to prevent explod-\ning gradients, and long sentences are truncated to\nthree standard deviations above the average sen-\ntence length in the training data.\nParameter Value\nOptimizer Adam\nOptimizerParameters β1 = 0.9,β2 = 0.999\nLearningRate 0.001\nBatchSize 64\nDecoderDropoutRate( ρ) 0.4\nWeightDecay 1−ρ\n|D|\nMaximumSentenceLength 59\nMaximumGradientNorm 1.5\nTable 5: Experimental settings.\nA.1 Architectures\nThis section describes the components that param-\neterise our models. 17 We use mnemonic blocks\nlayer(inputs; parameters) to describe architectures.\nTable 6 lists hyperparameters for the models dis-\ncussed in what follows.\nRNNLM At each step, an RNNLM parame-\nterises a categorical distribution over the vocab-\nulary, i.e. Xi|x<i ∼ Cat(f(x<i; θ)), where\nf(x<i; θ) = softmax(oi) and\nei = emb(xi; θemb) (12a)\nhi = GRU(hi−1,ei−1; θgru) (12b)\noi = aﬃne(hi; θout) . (12c)\n17All models were implemented with thePYTORCH library\n(Paszke et al., 2017), using default modules for the recurrent\nnetworks,embeddersandoptimisers.\nModel Parameter Value\nA embeddingunits( de) 256\nA vocabularysize( dv) 25643\nRandS decoderlayers( Lθ) 2\nRandS decoderhiddenunits( dθ\nh) 256\nS encoderhiddenunits( dλ\nh) 256\nS encoderlayers( Lλ) 1\nS latentunits( dz) 32\nMoG mixturecomponents( C) 100\nVampPrior pseudoinputs( C) 100\nTable 6: Architecture parameters: all (A), RNNLM (R),\nSENVAE (S).\nWe employ an embedding layer ( emb), one (or\nmore) GRU cell(s) (h0 ∈θis a parameter of the\nmodel), and an aﬃne layer to map from the dimen-\nsionality of the GRU to the vocabulary size. Table 7\ncompares our RNNLM to an external baseline with\na comparable number of parameters.\nModel PPL ↓ PPLDyer ↓\nDyer et al. (2016) 93.5 113 .4\nRNNLM 84.5 ±0.52 102.1\nTable 7: Baseline LMs on the PTB test set: avg ±std\nover 5 independent runs. Unlike us, Dyer et al. (2016)\nremoved the end of sentence token for evaluation, thus\nthe last column reports perplexity computed that way.\nGaussian SENVAE A Gaussian SENVAE also\nparameterises a categorical distribution over the\nvocabulary for each given preﬁx, but, in addi-\ntion, it conditions on a latent embedding Z ∼\nN(0,I), i.e. Xi|z,x<i ∼ Cat(f(z,x<i; θ))\nwhere f(z,x<i; θ) = softmax(oi) and\nei = emb(xi; θemb) (13a)\nh0 = tanh(aﬃne(z; θinit)) (13b)\nhi = GRU(hi−1,ei−1; θgru) (13c)\noi = aﬃne(hi; θout) . (13d)\nCompared to RNNLM, we modify f only slightly\nby initialising GRU cell(s) with h0 computed as a\nlearnt transformation of z. Because the marginal\nof the Gaussian SENVAE is intractable, we train it\nvia variational inference using an inference model\nq(z|x,λ) = N(z|u,diag(s ⊙s)) where\nei = emb(xi; θemb) (14a)\nhn\n1 = BiGRU(en\n1 ,h0; λenc) (14b)\nu = aﬃne(hn; λloc) (14c)\ns = softplus(aﬃne(hn; λscale)) . (14d)\n7233\nParameter Value\nObjective Function Validation NLL\nKernel Matern 52\nAcquisition Function Expected Improvement\nParameter Inference MCMC\nMCMC Samples 10\nLeapfrog Steps 20\nBurn-in Samples 100\nTable 8: Bayesian optimisation settings.\nNote that we reuse the embedding layer from the\ngenerative model. Finally, a sample is obtained via\nz= u + s ⊙ϵwhere ϵ∼N(0,Idz).\nMoG prior We parameterise C diagonal Gaus-\nsians, which are mixed uniformly. To do so we\nneed Clocation vectors, each in Rdz, and Cscale\nvectors, each in Rdz\n>0. To ensure strict positivity for\nscales we make σ(c) = softplus(ˆσ(c)). The set of\ngenerative parameters θis therefore extended with\n{µ(c)}C\nc=1 and {ˆσ(c)}C\nc=1, each in Rdz.\nVampPrior For this we estimate C sequences\n{v(c)}C\nc=1 of input vectors, each sequence v(c) =\n⟨v(c)\n1 ,..., v(c)\nlk\n⟩ corresponds to a pseudo-input.\nThis means we extend the set of generative pa-\nrameters θ with {v(c)\ni }lc\ni=1, each in Rde, for c =\n1,...,C . For each c, we sample lc at the begin-\nning of training and keep it ﬁxed. Speciﬁcally, we\ndrew C samples from a normal, lc ∼N(·|µl,σl),\nwhich we rounded to the nearest integer. µl and σl\nare the dataset sentence length mean and variance\nrespectively.\nA.2 Bayesian optimisation\nBayesian optimisation (BO) is an efﬁcient method\nto approximately search for global optima of a (typ-\nically expensive to compute) objective function\ny = f(x), where x ∈RM is a vector containing\nthe values of M hyperparameters that may inﬂu-\nence the outcome of the function (Snoek et al.,\n2012). Hence, it forms an alternative to grid search\nor random search (Bergstra and Bengio, 2012) for\ntuning the hyperparameters of a machine learning\nalgorithm. BO works by assuming that our obser-\nvations yn|xn (for n= 1,...,N ) are drawn from\na Gaussian process (GP; Rasmussen and Williams,\n2005). Then based on the GP posterior, we can\ndesign and infer an acquisition function. This ac-\nquisition function can be used to determine where\nto “look next” in parameter-space, i.e. it can be\nused to draw xN+1 for which we then evaluate the\nobjective function f(xN+1). This procedure iter-\nates until a set of optimal parameters is found with\nsome level of conﬁdence.\nIn practice, the efﬁciency of BO hinges on multi-\nple choices, such as the speciﬁc form of the acqui-\nsition function, the covariance matrix (or kernel)\nof the GP and how the parameters of the acquisi-\ntion function are estimated. Our objective func-\ntion is the (importance-sampled) validation NLL,\nwhich can only be computed after a model con-\nvergences (via gradient-based optimisation of the\nELBO). We follow the advice of Snoek et al. (2012)\nand use MCMC for estimating the parameters of\nthe acquisition function. This reduced the amount\nof objective function evaluations, speeding up the\noverall search. Other settings were also based on\nresults by Snoek et al. (2012), and we refer the\ninterested reader to that paper for more information\nabout BO in general. A summary of all relevant\nsettings of BO can be found in Table 8. We used\nthe GPYOPT library (authors, 2016) to implement\nthis procedure.\nB Additional Empirical Evidence\nIn Figure 5 we inspect how MDR and FB approach\ndifferent target rates (namely,10, 20, and 30). Note\nhow MDR does so more quickly, especially at\nhigher rates. Figure 6a shows that in terms of vali-\ndation perplexity, MDR and FB perform very simi-\nlarly across target rates. However, Figure 6b shows\nthat at the end of training the difference between\nthe target rate and the validation rate is smaller for\nMDR.\nFigure 7 compares variants of SENVAE trained\nwith MDR for various rates: a Gaussian-posterior\nand Gaussian-prior (blue-solid) to a Gaussian-\nposterior and Vamp-prior (orange-dashed). They\nperform essentially the same in terms of perplexity\n(Figure 7a), but the variant with the stronger prior\nrelies more on posterior samples for reconstruction\n(Figure 7b).\nFinally, we list additional samples: Figure 8 lists\nsamples from RNNLM, vanilla SENVAE and ef-\nfectively trained variants (via MDR with target rate\nr= 10); Figure 9 lists homotopies from SENVAE\nmodels.\n7234\n0 5 10 15\nEpoch\n9.00\n9.25\n9.50\n9.75\n10.00\n10.25\n10.50\n10.75\n11.00Rate\nTarget Rate\nFree Bits\nMDR\n(a)Rateovertimefor r= 10.\n0 5 10 15\nEpoch\n19.00\n19.25\n19.50\n19.75\n20.00\n20.25\n20.50\n20.75\n21.00Rate\nTarget Rate\nFree Bits\nMDR (b)Rateovertimefor r= 20.\n0 5 10 15\nEpoch\n29.00\n29.25\n29.50\n29.75\n30.00\n30.25\n30.50\n30.75\n31.00Rate\nTarget Rate\nFree Bits\nMDR (c)Rateovertimefor r= 30.\nFigure 5: Rate progression on the training set in the ﬁrst 20 epochs of training for SENVAE trained with free bits\n(FB) or minimum desired rate (MDR). One can observe that at higher rates, FB struggles to achieve the target rate,\nwhereas MDR achieves the target rate after a few epochs.\n10 20 30 40 50\nTarget Rate\n100\n110\n120\n130\n140\n150\n160\n170\n180Perplexity\nFree Bits\nMDR\n(a)PPL( ↓)forvarioustargetrates.\n10 20 30 40 50\nTarget Rate\n0\n1\n2\n3\n4\n5Validation Rate Difference\nFree Bits\nMDR (b)Targetrateminusvalidationrateattheendoftrainingfor\nvarioustargets(lowermeansbetter).\nFigure 6: Validation results for S ENVAE trained with free bits (FB) or minimum desired rate (MDR).\n10 20 30 40 50\nTarget Rate\n100\n110\n120\n130\n140\n150\n160\n170\n180Perplexity\nGauss\nVamp\n(a) Perplexity on validation set: models perform similarly\nwellandperplexitydegradesconsiderablyfor r> 20.\n10 20 30 40 50\nTarget Rate\n0\n10\n20\n30\n40\n50Accuracy Gap (%)\nGauss\nVamp\n(b) Accuracy gap: V AEs with stronger latent components\nrelymoreonposteriorsamplesforreconstruction.\nFigure 7: Comparison of S ENVAEs trained with standard prior and Gaussian posterior (Gauss) and Vamp prior\nand Gaussian posterior (Vamp) to attain pre-speciﬁed rates.\n7235\nModel Sample Closesttraininginstance TER\nRNNLM\nThe Dow Jones Industrial Average jumped 26.23\npointsto2662.91on2643.65.\nThe Dow Jones Industrial Average fell 26.23\npoints to 2662.91.\n0.23\nThecompaniessaidtheyareinvestigatingtheirown\nmindswithseveralcarriers,includingtheNational\nInstitutes of Health and Human Services Depart-\nmentofHealth,,\nThe Health and Human Services Department cur-\nrently forbids the National Institutes of Health\nfrom funding abortion research as part of its$8\nmillion contraceptive program.\n0.69\nAndyou’llhavenolongersurewhetheryouwould\ndoanythingnot–ifyouwanttogetyoudon’tknow\nwhatyou’re,\nReaching for that extra bit of yield can be a big\nmistake – especially if you don’t understand what\nyou’re investing.\n0.81\nSENVAE\nThe company said it expects to report net income\nof $UNK-NUM million, or $1.04 a share, from\n$UNK-NUMmillion,or,\nNine-month net climbed 19% to$UNK-NUM mil-\nlion, or $2.21 a primary share, from$UNK-NUM\nmillion, or $1.94 a share.\n0.50\nThe company said it expects to report net income\nof $UNK-NUM million, or $1.04 a share, from\n$UNK-NUMmillion,or,\nNine-month net climbed 19% to$UNK-NUM mil-\nlion, or $2.21 a primary share, from$UNK-NUM\nmillion, or $1.94 a share.\n0.50\nThe company said it expects to report net income\nof $UNK-NUM million, or $1.04 a share, from\n$UNK-NUMmillion,or,\nNine-month net climbed 19% to$UNK-NUM mil-\nlion, or $2.21 a primary share, from$UNK-NUM\nmillion, or $1.94 a share.\n0.50\n+MDRtraining\nThey have been growing wary of institutional in-\nvestors.\nPeople have been very respectful of each other. 0.46\nThePaloAltoretaileraddsthatitexpectstoposta\nthird-quarterlossofabout$1.8million,or68cents\nashare,compared\nNot counting the extraordinary charge, the com-\npany said it would have had a net loss of $3.1\nmillion, or seven cents a share.\n0.62\nBut Mr. Chan didn’t expect to be the ﬁrst time in a\nseriesofcasesofrapeandincest,includingaclaim\noftwo,\nFor the year, electronics emerged as Rockwell’s\nlargest sector in terms of sales and earnings.\n0.80\n+Vampprior\nButdespitethefactthatthey’relosing. As for the women, they’re UNK-LC. 0.45\nOther companies are also trying to protect their\nholdingsfromsmallercompanies.\nAnd ship lines carrying containers are also trying\nto raise their rates.\n0.60\nDr. Novello said he has been able to unveil a new\nproposalforWarnerCommunicationsInc.,which\nhasbeentryingtoparticipateintheU.S.\nPresident Bush says he will name Donald E.\nUNK-INITC to the new Treasury post of inspector\ngeneral, which has responsibilities for the IRS...\n0.78\n+MoGPrior\nAt American Stock Exchange composite trading\nFriday, Bear Stearns closed at $25.25 an ounce,\ndown75cents.\nIn American Stock Exchange composite trading\nyesterday , Westamerica closed at$22.25 a share,\ndown 75 cents.\n0.32\nMr. Patel,yes,saysthemusicwas“extremelyeffec-\ntive.”\nMr. Giuliani’s campaign chairman, Peter Powers,\nsays the Dinkins ad is “deceptive. ”\n0.57\nThe pilots will be able to sell the entire insurance\ncontractonNov. 13.\nThe proposed acquisition will be subject to ap-\nproval by the Interstate Commerce Commission,\nSoo Line said.\n0.60\nFigure 8: Sentences sampled from various models considered in this paper. For the RNNLM, we ancestral-sample\ndirectly from the softmax layer. For SENVAE, we sample from the prior and decode greedily. The vanilla SENVAE\nconsistently produces the same sample in this setting, that is because it makes no use of the latent space and all\nsource of variability is encoded in the dynamics of its strong generator. Other S ENVAE models were trained with\nMDR targeting a rate of 10. Next to each sample we show in italics the closest training instance in terms of an edit\ndistance (i.e. TER). The higher this distance (it varies from 0 to 1), the more novel the sentence is. This gives us\nan idea of whether the model is generating novel outputs or copying from the training data.\n7236\nRevenue rose 12% to $UNK-NUM billion from $UNK-NUM billion.\nItisnowaytogetalotofwaystogetawayfromitsbooks.\nAtonepointafterCongresssentCongresstoasktheSenateDemocratstoextendthebill.\nSofar.\nButthenumberofpeoplewhowanttopredictthattheycanbeusedtokeeptheirownportfolios,\nTheU.S.governmenthasbeenannouncedin1986,butitwasintroducedinDecember1986\nThecompanysaiditplanstosellitsC$400millionmillionsharesoutstanding\nRevenue slipped 4.6% to $UNK-NUM million from $UNK-NUM million.\n(a)Vanilla SENVAE withancestralsampling.\nMr. Vinson estimates the industry’s total revenues approach $200 million.\nThecompanysaiditexpectstoreportnetincomeof$UNK-NUMmillion,or$1.04ashare,\nThecompanysaiditexpectstoreportnetincomeof$UNK-NUMmillion,or$1.04ashare,\nThecompanysaiditexpectstoreportnetincomeof$UNK-NUMmillion,or$1.04ashare,\nThecompanysaiditexpectstoreportnetincomeof$UNK-NUMmillion,or$1.04ashare,\nThecompanysaiditexpectstoreportnetincomeof$UNK-NUMmillion,or$1.04ashare,\nThecompanysaiditexpectstoreportnetincomeof$UNK-NUMmillion,or$1.04ashare,\n“That’s not what our fathers had in mind.”\n(b)Vanilla SENVAE withgreedydecoding.\nHe could grasp an issue with the blink of an eye.”\nHecouldbecalledforafewmonthsbeforetheSenateJudiciaryCommitteeCommittee.\nHewouldbeabletoacceptaclueasthepresident’sargument.\nButthereisnolongerreasontoseewhethertheSovietUnionisinterested.\nButitdoesn’tmeananyformalcommentonthebasis.\nHowever,thereisnolongerreasonfortheHart-Scott-RodinoAct.\nHowever,Genentechisn’tpredictinganysigniﬁcantslowdowninthefuture.\nHowever, StatesWest isn’t abandoning its pursuit of the much-larger Mesa.\n(c) SENVAE trainedwith MDR (r= 10).\nSony was down 130 to UNK-NUM.\nThepricewasdownfrom$UNK-NUM.\nThepricewasdownfrom$UNK-NUMabarrelto$UNK-NUM.\nThepricewasdownabout$130million.\nTheyieldonsix-monthCDsroseto7.93%from8.61%.\nFriday’ssell-offwasdownabout60%fromayearago.\nFriday’sMarketActivity\nFriday’s edition misstated the date\n(d) SENVAE with MOG priortrainedwith MDR (r= 10).\nLawyers for the Garcias said they plan to appeal.\nLawyersfortheagencysaidtheycan’taffordtosettle.\nLawyersfortherestoftheventurewon’tbereached.\nThiswouldbemadeforthepastfewweeks.\nThishasbeenlosingthemoneyfortheirown.\nThishasbeenafewweeksago.\nThishasbeenaverydisturbingproblem.\nThis market has been very badly damaged.”\n(e) SENVAE withVamppriortrainedwith MDR (r= 10).\nFigure 9: Latent space homotopies for various S ENVAE models. Note the smooth transition of topic and gram-\nmatically of the samples in properly trained S ENVAE models. Also note the absence of such a smooth transition\nin the softmax samples from the vanilla SENVAE model."
}