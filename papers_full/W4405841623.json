{
  "title": "Distilling the knowledge from large-language model for health event prediction",
  "url": "https://openalex.org/W4405841623",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2973080406",
      "name": "Sirui Ding",
      "affiliations": [
        "Texas A&M University"
      ]
    },
    {
      "id": "https://openalex.org/A2123959257",
      "name": "Jiancheng Ye",
      "affiliations": [
        "Weill Cornell Medicine",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2104553385",
      "name": "Xia Hu",
      "affiliations": [
        "Rice University"
      ]
    },
    {
      "id": "https://openalex.org/A2121889303",
      "name": "Na Zou",
      "affiliations": [
        "University of Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2973080406",
      "name": "Sirui Ding",
      "affiliations": [
        "Texas A&M University"
      ]
    },
    {
      "id": "https://openalex.org/A2123959257",
      "name": "Jiancheng Ye",
      "affiliations": [
        "Cornell University",
        "Weill Cornell Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2104553385",
      "name": "Xia Hu",
      "affiliations": [
        "Rice University"
      ]
    },
    {
      "id": "https://openalex.org/A2121889303",
      "name": "Na Zou",
      "affiliations": [
        "University of Houston"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2594685110",
    "https://openalex.org/W3209074506",
    "https://openalex.org/W2899420362",
    "https://openalex.org/W3099958847",
    "https://openalex.org/W2982424689",
    "https://openalex.org/W3005864656",
    "https://openalex.org/W3131698376",
    "https://openalex.org/W4295066927",
    "https://openalex.org/W2998314166",
    "https://openalex.org/W2998445959",
    "https://openalex.org/W3159743235",
    "https://openalex.org/W4248112284",
    "https://openalex.org/W2946661638",
    "https://openalex.org/W2031034134",
    "https://openalex.org/W4308370081",
    "https://openalex.org/W2146080983",
    "https://openalex.org/W2610602137",
    "https://openalex.org/W4211019091",
    "https://openalex.org/W2110056222",
    "https://openalex.org/W2105203857",
    "https://openalex.org/W4385606949",
    "https://openalex.org/W4286567174",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4200635746",
    "https://openalex.org/W4225493996",
    "https://openalex.org/W3187228765",
    "https://openalex.org/W3127747328",
    "https://openalex.org/W4385381606",
    "https://openalex.org/W4386973901",
    "https://openalex.org/W6600258949",
    "https://openalex.org/W4383469362",
    "https://openalex.org/W6600120041",
    "https://openalex.org/W4283801824",
    "https://openalex.org/W4389000204",
    "https://openalex.org/W2508848934",
    "https://openalex.org/W6837901829",
    "https://openalex.org/W2886951144",
    "https://openalex.org/W4391812863",
    "https://openalex.org/W2998409174",
    "https://openalex.org/W2517259736",
    "https://openalex.org/W2690721124",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4393990094",
    "https://openalex.org/W4282010954",
    "https://openalex.org/W3105221338"
  ],
  "abstract": "Health event prediction is empowered by the rapid and wide application of electronic health records (EHR). In the Intensive Care Unit (ICU), precisely predicting the health related events in advance is essential for providing treatment and intervention to improve the patients outcomes. EHR is a kind of multi-modal data containing clinical text, time series, structured data, etc. Most health event prediction works focus on a single modality, e.g., text or tabular EHR. How to effectively learn from the multi-modal EHR for health event prediction remains a challenge. Inspired by the strong capability in text processing of large language model (LLM), we propose the framework CKLE for health event prediction by distilling the knowledge from LLM and learning from multi-modal EHR. There are two challenges of applying LLM in the health event prediction, the first one is most LLM can only handle text data rather than other modalities, e.g., structured data. The second challenge is the privacy issue of health applications requires the LLM to be locally deployed, which may be limited by the computational resource. CKLE solves the challenges of LLM scalability and portability in the healthcare domain by distilling the cross-modality knowledge from LLM into the health event predictive model. To fully take advantage of the strong power of LLM, the raw clinical text is refined and augmented with prompt learning. The embedding of clinical text are generated by LLM. To effectively distill the knowledge of LLM into the predictive model, we design a cross-modality knowledge distillation (KD) method. A specially designed training objective will be used for the KD process with the consideration of multiple modality and patient similarity. The KD loss function consists of two parts. The first one is cross-modality contrastive loss function, which models the correlation of different modalities from the same patient. The second one is patient similarity learning loss function to model the correlations between similar patients. The cross-modality knowledge distillation can distill the rich information in clinical text and the knowledge of LLM into the predictive model on structured EHR data. To demonstrate the effectiveness of CKLE, we evaluate CKLE on two health event prediction tasks in the field of cardiology, heart failure prediction and hypertension prediction. We select the 7125 patients from MIMIC-III dataset and split them into train/validation/test sets. We can achieve a maximum 4.48% improvement in accuracy compared to state-of-the-art predictive model designed for health event prediction. The results demonstrate CKLE can surpass the baseline prediction models significantly on both normal and limited label settings. We also conduct the case study on cardiology disease analysis in the heart failure and hypertension prediction. Through the feature importance calculation, we analyse the salient features related to the cardiology disease which corresponds to the medical domain knowledge. The superior performance and interpretability of CKLE pave a promising way to leverage the power and knowledge of LLM in the health event prediction in real-world clinical settings.",
  "full_text": "Distilling the knowledge from \nlarge-language model for health \nevent prediction\nSirui Ding1, Jiancheng Ye2, Xia Hu3 & Na Zou4\nHealth event prediction is empowered by the rapid and wide application of electronic health records \n(EHR). In the Intensive Care Unit (ICU), precisely predicting the health related events in advance is \nessential for providing treatment and intervention to improve the patients outcomes. EHR is a kind \nof multi-modal data containing clinical text, time series, structured data, etc. Most health event \nprediction works focus on a single modality, e.g., text or tabular EHR. How to effectively learn from \nthe multi-modal EHR for health event prediction remains a challenge. Inspired by the strong capability \nin text processing of large language model (LLM), we propose the framework CKLE for health event \nprediction by distilling the knowledge from LLM and learning from multi-modal EHR. There are two \nchallenges of applying LLM in the health event prediction, the first one is most LLM can only handle \ntext data rather than other modalities, e.g., structured data. The second challenge is the privacy \nissue of health applications requires the LLM to be locally deployed, which may be limited by the \ncomputational resource. CKLE solves the challenges of LLM scalability and portability in the healthcare \ndomain by distilling the cross-modality knowledge from LLM into the health event predictive model. \nTo fully take advantage of the strong power of LLM, the raw clinical text is refined and augmented \nwith prompt learning. The embedding of clinical text are generated by LLM. To effectively distill the \nknowledge of LLM into the predictive model, we design a cross-modality knowledge distillation (KD) \nmethod. A specially designed training objective will be used for the KD process with the consideration \nof multiple modality and patient similarity. The KD loss function consists of two parts. The first one \nis cross-modality contrastive loss function, which models the correlation of different modalities from \nthe same patient. The second one is patient similarity learning loss function to model the correlations \nbetween similar patients. The cross-modality knowledge distillation can distill the rich information \nin clinical text and the knowledge of LLM into the predictive model on structured EHR data. To \ndemonstrate the effectiveness of CKLE, we evaluate CKLE on two health event prediction tasks in the \nfield of cardiology, heart failure prediction and hypertension prediction. We select the 7125 patients \nfrom MIMIC-III dataset and split them into train/validation/test sets. We can achieve a maximum 4.48% \nimprovement in accuracy compared to state-of-the-art predictive model designed for health event \nprediction. The results demonstrate CKLE can surpass the baseline prediction models significantly on \nboth normal and limited label settings. We also conduct the case study on cardiology disease analysis \nin the heart failure and hypertension prediction. Through the feature importance calculation, we \nanalyse the salient features related to the cardiology disease which corresponds to the medical domain \nknowledge. The superior performance and interpretability of CKLE pave a promising way to leverage \nthe power and knowledge of LLM in the health event prediction in real-world clinical settings.\nKeywords Health event prediction, Cardiovascular disease, Large-language model, Knowledge distillation, \nMulti-modal learning\nThe rapid adoption of Electronic Health Records (EHR)1 has transformed healthcare, offering vast repositories \nof patient information. In the Intensive Care Unit (ICU), the ability to predict health-related events2,3 in advance \nis paramount for optimizing treatment strategies and improving patient outcomes. EHR is multi-modality \ndata4 containing clinical text 5 (e.g., diagnosis notes) and time series data 6 (e.g., Electrocardiography (ECG), \nElectroencephalography EEG), and structured data 7 (e.g., lab tests). However, existing health event prediction \n1Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA. 2Weill Cornell \nMedicine, New York, NY, USA. 3Department of Computer Science, Rice University, Houston, TX, USA. 4Department \nof Industrial Engineering, University of Houston, Houston, TX, USA. email: nzou2@uh.edu\nOPEN\nScientific Reports |        (2024) 14:30675 1| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports\n\nmodels often focus on a singular modality, such as text 8 or tabular EHR 9, presenting a significant challenge in \neffectively harnessing the entirety of multi-modal EHR data 10. Health event prediction11 is an essential task in \nthe field of medicine. It is the foundation for precision medicine12, personalized treatment13, etc. With the rapid \ndevelopment of electronic health records (EHR), the data in healthcare becomes more accessible for training the \nmachine learning (ML) models. In the realm of digital health 14, we can design ML models to precisely predict \nhealth event in advance from the EHR data.\nHeart failure15, a multifaceted clinical syndrome marked by the heart’s compromised ability to pump blood \neffectively, stands as a formidable challenge within healthcare systems globally. The unpredictable nature of \nheart failure exacerbations necessitates predictive models that can anticipate events, enabling clinicians to \nintervene proactively16. Hospitalizations and adverse outcomes associated with heart failure place a considerable \nburden on both patients and healthcare resources17. Accurate prediction models offer the potential to enhance \npatient care, reduce hospitalizations, and optimize treatment strategies18. Hypertension, often referred to as the \n“silent killer, ” remains a prevalent cardiovascular condition characterized by elevated blood pressure levels 19. \nThe insidious nature of hypertension makes it imperative to identify and predict impending events, such as \nsevere complications like strokes and heart attacks 20. Timely interventions based on accurate predictions can \nmitigate risks and improve long-term outcomes for individuals living with hypertension 21. Predictive models \ntailored to the dynamic nature of blood pressure fluctuations and patient-specific factors are instrumental in \nshaping personalized care plans. While traditional predictive models have made strides in these domains, the \nintegration of multi-modal data22,23 and advanced processing techniques, such as those offered by LLMs24, opens \nnew avenues for refined predictions. Predicting events in heart failure and hypertension introduces specific \nchallenges that necessitate a targeted approach. These challenges include the need to assimilate and interpret \ndiverse data modalities within EHRs, ranging from clinical narratives to structured data and temporal trends. \nAdditionally, the intricate interplay of factors contributing to heart failure and hypertension requires models that \ncan capture the complexity of patient health trajectories.\nEfforts are put into building health event predictive model on EHR. Some works25 use the structured EHR to \nbuild the predictive model. Others26 use clinical text to predict health events. There are some works27,28 that use \nboth structured and text EHR data. Some of them simply use the clinical text as auxiliary information27. Others \ngenerate the embedding from clinical text and fuse the multi-modal representations to make final predictions. \nWith the wide application of large language model (LLM) 29, LLM provides a transformative way to build \npredictive model on multi-modality EHR data30. Some previous works are put into how to apply LLM in health \nevent prediction31. However, there are still challenges that hinder the landing of LLM applied to health event \nprediction. Compared to the traditional deep learning models e.g., LSTM, RNN, for text processing, the special \ncharacteristics of LLM pose several challenges in the healthcare application. We summarize the challenges from \nthe model and data perspectives as follows.\nLLMs are not scalable and portable for real-world health predictive applications 32. As we know, directly \nusing the online LLM for inference has privacy issues 33 and is very expensive 34. The local model is needed \nin many real-world clinical scenarios, e.g., hospitals, medical centers, etc 35. However, the large size of LLM \nlimits its local deployment and the efficiency of inference didn’t meet the real-time requirement of AI healthcare \nalgorithms36. Learning from both clinical text and structured data remains a challenge. LLM mainly handles the \ntext data, which is only one modality in EHR data. There are other modalities like structured EHR data which \ncould be learned with predictive models, e.g., Transformer. There is a need to effectively learn both modalities \nin one framework and adapt LLM in the end-to-end training pipeline 37,38. Meanwhile, the clinical text usually \ncontains much noise39, which will mislead the model learning if directly embedded40. How to model the patient \nsimilarity in multi-modality learning. Previous multi-modal methods fuse the embeddings of multi-modal data \nand cannot mine the latent relations between patients41. Learning the patient similarity is inspired by the doctor’s \nclinical practice which will refer to the past and related patients’ history.\nWe are motivated to mitigate these challenges by proposing the CKLE framework. For the first and second \nchallenge, the CKLE framework distills the knowledge from LLM on the cross-modality EHR data. The cross-\nmodality distillation can integrate the LLM’s knowledge into the prediction model without increasing the model \ncomplexity. To fully exploit and utilize the knowledge from LLM, we refine and augment the raw clinical text \nwith prompt learning on LLM which can effectively remove the noise. The augmented clinical text from LLM \ncontains less noise and more general textual information with augmentation. To simultaneously mine the patient \nsimilarities and the latent cross-modality relations, we design a contrastive loss to model the pairs of patients and \neach patient’s text-visit pairs. The contributions of this work can be summarized as follows:\n• We distill the cross-modal knowledge from LLM to boost the health event prediction and fully exploit the \nLLM capability by prompting to generate augmented text.\n• We design a contrastive distillation loss to learn the multi-modality knowledge from teacher model and sim-\nilarities between patients at the same time.\n• Extensive experiments are conducted on two representative health event prediction tasks to validate the effec-\ntiveness of the CKLE framework. CKLE can achieve competitive prediction performance on the real-world \ntext-rich EHR data.\nMethod\nProblem formulation\nFor each patient, there will be multiple visits Vi, where i ∈ [1,m ] indicates the m visits to the hospital. \nEach visit can be represented by the International Classification of Diseases (ICD 42) codes demonstrating the \ndiagnosis and treatment in the j-th visit as Cj = {c1,c 2,... ,c k}, where k is a total number of the ICD \nScientific Reports |        (2024) 14:30675 2| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports/\ncodes. Additionally, there are attached clinical notes from the doctors for each visit denoted as Ni. The dataset \nand problem can be formulated as follows.\nPatient visits dataset\nThe input dataset can be denoted as D = P1,P 2,... ,P n containing n patients. For the ith patient, \nPi =( Vi,N i) where each patient has both visits data and text data.\nHealth event prediction\nGiven the i-th patient features of previous t − 1 visits P[1,t −1]\ni , the goal is to train the prediction model Q (θ ) \nwith learnable θ  parameters, which takes P[1,t −1]\ni  as input and precisely predict the targeted health event yi at \nthe t-th visit of the patient. For heart failure prediction and hypertension prediction, ytarget\ni ∈ 0,1 is a binary-\nclass target.\nCKLE framework overview\nThe overview of the CKLE framework is presented in Fig. 1.\nRepresentation learning from visits data\nLong-short term feature modeling\nWe adopt the dilated convolution to learn the long term and short term information from the multiple visits \nfeatures inspired by43. The long-short term feature extraction can be achieved by setting different dilation rate d\n. The dilation convolution layers dconv with dilation rate d can be represented as follows.\n \ndconvd (V q\ni )=\n∑K\nj=0\nV q\nI |q + d.j| .f (j) , (1)\nwhere V q\ni  is the q-th dimension feature corresponds to i-th patient visit Vi. The convolution filter with filter \nsize K is denoted as f (j) ,j ∈ [0,K ]. This illustrates how to learn the representations with a given reception \nlength. Modeling the hidden features at different scale requires multiple convolution with various dilated rate \nwhich can be represented as follows.\n ci = concat\n[\ndconvd1(\nVi, . . . , dconvdr (Vi)], (2)\nwhere ci is the convolution embedding of the i-th patient by combining multiple dilated convolution \nrepresentations with dilated rate from d1 to dr. r is the number of different dilated convolution.\nWe also employed the feature recalibration module proposed in 43 to attach suitable attention to different \nfeatures. The feature recalibration module can be formulated as follows.\n recal (ci)= σ 1 (W1σ 2 (W2ci)) , (3)\nFig. 1. Overview of the CKLE framework.\n \nScientific Reports |        (2024) 14:30675 3| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports/\nThe ci is hidden representation learnt by dilated convolution. W1,W 2 are trainable parameters that serves as \nthe features learnable weights. σ 1,σ 2 are activation function which are Sigmoid, ReLU respectively. The recal \nweights are then applied to ci with element-wise multiplying.\n c′\ni = recal (ci) ⊙ ci, (4)\nTo further improve the representation learning performance, a residual module is applied to remain the original \ninformation from patients.\n V ′\nI = recal (Vi) ⊙ Vi, (5)\n Ei = concat[c′\ni ,V i′ ] (6)\nThe Ei is the concatenated embedding after the convolution and feature recalibration. The temporal embedding \nzi will be generated by feeding Ei into a temporal model temp, e.g., RNN, GRU.\n zi = temp (Ei) (7)\nPatient similarity modeling\nWhen the doctors are making diagnosis and clinical decisions, they will usually refer to the history of similar \npatients. Inspired by this process in traditional clinical workflow, we design a contrastive learning module to \nmodel the patient similarity which could take advantage of the features from similar patients for health event \nprediction.\nIn the conventional contrastive learning setting44, the embedding of each patient should be the closet as their \nown embedding (positive pairs) and farthest to other patients embedding (negative pairs). The target of the \ncontrastive loss is an identity matrix. However, this could not learn the similarity between different patients. So \nwe design a soft target for the contrastive loss to model patient similarity.\nWe use the ICD codes of each patient as the semantic label Ip of patients. The soft target can be represented as:\n \nsp = Ip.IT\np\n||Ip||2 , (8)\nwhere sp denotes the similarity between each patient pairs. The logits yi between patients with batch size |bs| \nare obtained through:\n \nyi = exp (dis( zT\ni ,z i)/τ )∑|bs|\nj=1exp (sp)\n, (9)\ndis can be distance computation method which is cosine similarity in this work and τ  indicates the temperature \nhyper-parameters. Similarly, the target of patient-patient pairs can be calculated with sp as:\n \ny′\ni = exp (sp)∑|bs|\nj=1exp (sp)\n, (10)\nThus the loss function Lpsim for patient similarity modeling can be represented as:\n \nLpsim = − 1\n|bs|\n∑|bs|\ni=1y′\ni logyi, (11)\nExploit medical knowledge from LLM\nText augmentation with prompt learning\nThe raw clinical text contains noise and redundant information, which indicates the spelling errors, abbreviations, \nnon-standard terminology, and extraneous information that hinder data extraction and analysis. To remove the \nunrelated information and increase the generalizability of text, we propose to achieve the text augmentation \nwith prompt learning on LLM by refining the raw text and generating additional synthetic text data based on \nexisting clinical text. For each patient i, we have raw notes Ni. The LLM with frozen knowledgeable parameters \nare exploited to augment and polish the raw text with prompt learning. The prompt E input to the LLM are \ndesigned as “Refine the following clinical text without changing its meanings:“. Then the prompt will be attached \nwith each patient’s clinical notes Ni. The augmented and refined clinical text can be generated as follows.\n N′\ni = LLM (E, Ni) , (12)\nwhere N′\ni  is the augmented and refined notes.\nCross-modality distillation from LLM\nEach patient has clinical text and tabular data, we obtain the embedding zi of tabular data through the \nrepresentation learning on multiple visits. To further take advantage of the strong power of LLM, we generate \nthe embedding hi of clinical text from the LLM as follows.\nScientific Reports |        (2024) 14:30675 4| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports/\n hi = LLMemb (Ni′ ) . (13)\nTo exploit the rich information from multi-modality, a cross-modality distillation strategy is designed to transfer \nthe knowledge from LLM to the health prediction model. The LLM serves as the teacher model with frozen \nparameters and the prediction model temp is the student model with parameters θ . The zi,h i are structured \nEHR and clinical text embedding generated by LLM and temp respectively.\nAs each patient has structured EHR and clinical text, the zi,h i from the same patient Pi are highly related. \nOn the other hand, the EHR and clinical text from different patients shares little common information for \nwhich the embeddings should have larger distance. Inspired by this domain knowledge, the distillation objective \nLcmkd is designed to learn the contrastive relations45 between EHR-text pairs formulated as:\n \nLcmkd = − 1\nbs\n∑ |bs|\ni=1log exp (dis( zi,h i)/γ )∑|bs|\nj=1exp (dis( zi,h i)/γ )\n, (14)\nwhere γ  is the temperature hyper-parameters.\nTraining for CKLE\nThe final prediction of the health event can be represented as:\n ˆyt = σ (Wh i + b) , (15)\nwhere W, b are learnable parameters and σ  is the activation function, like sigmoid. The overall loss function L \nof CKLE can be represented by combining these objectives together.\n L = αl\n(ˆyt,y target)\n+ βL psim + ηL cmkd, (16)\nwhere l is the conventional prediction loss function, e.g., cross entropy. ytarget  is the ground truth label. \nα, β , η  are hyper-parameters to control the ratio of different objectives.\nExperimental setup\nDataset and tasks\nThe well-known medical EHR dataset MIMIC-III is used for the experiments. We filter out to get patients with \nboth clinical text and the corresponding structured data. Inspired by 27, the patients with multiple visits will be \nused for the health event prediction. Each patient’s previous visits will be used to predict the last visit. Since \neach patient has a variable number of previous visits, we will add padding visit to them with zeros to reach the \nlargest number of the previous visits among the patients. Two tasks related to cardiology diseases are selected as \nthe representative health event prediction tasks in this work. Details of each task will be introduced as follows.\n• Hypertension prediction: This task is a binary classification to predict the hypertension of the patient’s next \nhospital visit.\n• Heart failure prediction: This task is a binary classification, which predicts whether the heart failure will hap-\npen to the patient in the next hospital visit.\nBaselines\n• RETAIN: A widely used interpretable healthcare prediction framework with the reverse attention module \nproposed by Choid et al.46.\n• AdaCare: Ma et al.43 designed the AdaCare framework for representation learning on EHR data by modeling \nthe short and long term features and provide explainability with competitive performance.\n• Dipole: An interpretable prediction framework based on bi-directional RNN is proposed by Ma et al. 47. Di-\npole can memorize the long-term history information and provide clinical meaningful interpretation.\n• CGL: This is a specialized health event prediction framework27 with text-rich EHR data. CGL use graph learn-\ning to learn the patient similarities for event prediction.\n• Chet: It is proposed by Lu et al.25 to use dynamic disease graph to learn the temporal variation of diseases for \neach patient.\n• EHR + LLM: This is the baseline multi-modal method that uses BlueBERT to generate clinical text embed -\ndings. The fusing embeddings of text and structured EHR will be used to make predictions.\nImplementation details\nWe adopt the data pre-processing method from CGL framework27. The patients with multiple visits are selected, \nand the last visit of each patient is used as prediction target while the history visits of each patient are used as \ninputs. The clinical notes “Discharge summary” from the patient will be filtered out for its high correlation with \nthe prediction targets. The train-valid-test set are randomly split on patients with the ratio of 6000 /125/1000. \nEach patient is used as one data sample, so one prediction will be made for each patient. We use Google T548 as \nour LLM implementation. We use PyTorch to implement all the baseline and train/test all models on the Nvidia \nTesla V100 GPU. F1-score, AUROC, and AUPRC are used as metrics to evaluate the performance of prediction. \nThe feature importance is obtained from the attention mechanism of the predictive model.\nScientific Reports |        (2024) 14:30675 5| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports/\nResults\nIn this section, the results of experiments aim to answer several research questions (RQ) as follows:\n• RQ1: How does the CKLE framework perform in health event prediction compared to SOTA baselines?\n• RQ2: When the labeled data is limited, can the CKLE still show competitive performance?\n• RQ3: What is the contribution of each core part in the CKLE framework?\n• RQ4: What is the representation learning ability of CKLE?\nCKLE precisely predicts the health event on multi-modal EHR data\nFrom the Tables 1 and 2, the proposed framework CKLE are compared with baseline methods on cardiovascular \nprediction tasks (Hypertension prediction and heart failure prediction). Several key findings are summarized \nfrom the results.\nThe proposed CKLE framework can surpass state-of-the-art baselines significantly on health event prediction \ntasks. For the hypertension prediction, we observe the CKLE achieves the best performance measured by \nAUROC. Meanwhile, CKLE achieves best performance on AUROC and AUPRC on the heart failure prediction. \nThe high performance on these two cardiology related event prediction demonstrates the potential of applying \nCKLE for health event prediction in advance, especially for emergency medicine.\nModeling the patient similarity can improve the health event prediction performance. From two tasks \npresented in Table 1, we can observe the graph based prediction method e.g., Chet and the CKLE framework can \noutperform other categories of baselines including CNN-based method (Adacare), RNN-based (Diploe). The \nCKLE framework outperforms the RETAIN by 2.97% on F1 score and 4.13% on AUROC. The CKLE framework \noutperforms the Adacare by 3.01% on F1 score and 0.9% on AUROC. The reason behind this phenomenon is \nprobably due to either graph based method or our proposed method take the patient similarity into the modeling \nprocess. We take advantage of the patient similarity to help improve the health event prediction of a particular \npatient. With more related information as the input, the event prediction accuracy can be reasonably increased.\nDirectly combining the text features generated from clinical text is infeasible for the performance \nimprovement. The baseline LLM method for health event predictions directly leverages the embedded clinical \ntext generated from LLM as the additional features don’t work efficiently to enhance the prediction accuracy. \nFor example, compared to the backbone model, the naive LLM boosted method (indicated as EHR + LLM \nin Table 1) improves 2.08% F1 score on the hypertension prediction task. Meanwhile the other performance \nmetrics on hypertension and heart failure prediction drops, which means the direct use of text features cannot \nimprove the performance. There are two potential causes of this unsatisfying performance with additional \nLLM generated features. Firstly, the direct encoding of clinical text with LLM will inevitably include noise and \nredundant information which will affect the performance of the model. The second reason is the cross-modality \nknowledge distillation from LLM is more effective than naive LLM usage.\nModels (Modality)\nHypertension Heart Failure\nAccuracy (SE) Precision (SE) Recall (SE) Accuracy (SE) Precision (SE) Recall (SE)\nRETAIN (Single) 72.32 (0.14) 73.40 (0.25) 81.10 (0.15) 77.60 (0.05) 74.02 (0.10) 64.10 (0.08)\nAdaCare (Single) 72.40 (0.22) 71.52 (0.11) 86.37 (0.20) 78.40 (0.14) 70.72 (0.06) 69.63 (0.18)\nDipole (Single) 72.62 (0.69) 74.12 (0.45) 81.72 (0.27) 77.89 (0.25) 69.21 (0.40) 71.68 (0.05)\nCGL (Multi) 70.20 (0.61) 74.32 (0.05) 70.07 (0.06) 79.12 (0.10) 72.41 (0.17) 70.01 (0.45)\nChet (Single) 71.60 (0.55) 76.11 (0.23) 78.82 (0.23) 79.40 (0.52) 73.17 (0.21) 69.40 (0.14)\nEHR + LLM (Multi) 71.90 (0.44) 70.90 (0.23) 84.50 (1.24) 78.71 (0.31) 74.20 (0.06) 63.23 (1.11)\nCKLE (Multi) 73.63 (0.07) 75.01 (0.67) 81.51 (0.25) 80.01 (0.05) 76.10 (0.31) 65.41 (0.10)\nTable 2. Prediction performance on cardiovascular diseases (Metrics of accuracy, precision and recall). \nSignificant values are in bold.\n \nModels (Modality)\nHypertension Heart Failure\nF1 (SE) AUROC (SE) AUPRC (SE) F1 (SE) AUROC (SE) AUPRC (SE)\nRETAIN (Single) 76.02 (0.33) 75.04 (0.77) 80.24 (0.78) 68.70 (0.10) 84.53 (0.06) 76.90(0.10)\nAdaCare (Single) 78.89 (0.42) 75.80 (0.01) 79.41 (0.00) 70.67 (1.01) 84.87 (0.12) 77.33(0.38)\nDipole (Single) 78.13 (0.98) 73.28 (2.20) 77.82 (0.68) 70.37 (0.31) 84.40 (0.62) 76.13(0.15)\nCGL (Multi) 72.14 (1.90) 70.52 (0.15) 73.01 (0.46) 71.18 (0.30) 84.79 (0.20) 73.88(1.33)\nChet (Single) 77.35 (0.74) 77.77 (0.11) 80.56 (0.18) 71.06 (0.67) 84.88 (0.19) 77.88(0.54)\nEHR + LLM (Multi) 77.60 (0.35) 74.79 (0.29) 76.40 (1.39) 67.47 (0.76) 84.40 (0.26) 74.03(0.91)\nCKLE (Multi) 78.28 (0.67) 78.14 (0.86) 79.71 (1.14) 70.77 (0.15) 85.30 (0.10) 78.00(0.00)\nTable 1. Prediction performance on cardiovascular diseases. The number in () indicates the standard error. \nSignificant values are in bold.\n \nScientific Reports |        (2024) 14:30675 6| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports/\nCross-modality distillation from LLM is more effective than directly concatenating generated features. \nFor the hypertension prediction, the CKLE improves the prediction performance by 3.61% on average across \ndifferent baselines compared to single modality model and 4.48% on average across different baselines compared \nto directly using LLM generated text features. For the heart failure prediction, the average improvement is \n0.709% compared to single modality model and 1.07% compared to directly using LLM. We take a further step \nto investigate the superiority of the cross-modality distillation strategy. The first reason is directly concatenating \nthe features increase the dimension of the input data, which will suffer from the curse of high dimension. The \nsecond reason is cross-modality distillation with the proposed contrastive loss can learn the inner correlations \nbetween different modality features.\nCKLE has competitive performance with limited labeled data\nTo evaluate the effectiveness of CKLE under the limited label settings which is a common scenario in the \nmedical application, we conduct the experiments by reducing the ratio of labeled training data on the heart \nfailure prediction. From Fig. 2, we have two observations as follows.\nIncreasing the amount of labeled data can increase the overall prediction performance, but the marginal \neffect exists. We can observe the performance of these models can gain non-trivial improvement when the \nratio of labeled data is increased from 0.1 to 0.5. But the performance on whole dataset doesn’t have obvious \nimprovement compared to half of the dataset, which indicates increasing the training data has marginal \nperformance improvement when a threshold of enough labeled data is reached.\nThe CKLE framework can achieve competitive performance compared with baseline methods with limited \nlabeled data. As presented in Fig. 2, the CKLE framework can still surpass the baselines under different ratio of \nthe training data. When we only use 0.1 labeled data to train the CKLE, we can still gain 3.18% improvement \ncompared to the Chet model. Similarly, the CKLE can achieve the competitive performance with only 0.5 \ntraining data compared to the best baseline model trained on full data.\nAblation study\nWe conduct ablation study to evaluate the contribution of each part in our framework. The two key designs in the \nCKLE framework is cross-modality distillation from LLM and patient similarity modeling with contrastive loss \n(PSIM). The ablation study is conducted on heart failure prediction and the results are presented in Fig. 3. We can \nobserve each part has significant contribution to the performance improvement. If the knowledge is not distilled \nfrom the LLM to the predictive model, the performance is not competitive compared to the CKLE because the \nrich knowledge from LLM is powerful and helpful for various downstream health predictive tasks. Additionally, \nthe PSIM part which leverages the patient similarity can further improve the predictive performance.\nFig. 3. Results of ablation study.\n \nFig. 2. Performance comparison with limited labeled training data.\n \nScientific Reports |        (2024) 14:30675 7| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports/\nEmbedding visualization\nTo illustrate the effectiveness of the representation learning ability of the CKLE framework, we plot the embedding \nof each patients in hypertension and heart failure prediction via t-SNE. As shown in Fig.  4, we compare the \nembeddings generated from the baseline method and the CKLE framework. For hypertension prediction task, \nthe embedding visualized in Fig.  4b have better clusters of negative and positive patient samples compared \nwith the visualized embedding of the baseline method RETAIN in Fig.  4a. Similarly, CKLE can produce better \nclusters of embedding on the heart failure prediction tasks.\nModel interpretation\nCase study I: important features of hypertension prediction\nAs shown in the left part of the Fig. 5, we present the 20 most important features for the hypertension prediction. \nEach feature is assigned a score that signifies its relative importance in the prediction model. If there is a prior \noccurrence of this feature, the risk of target disease will be higher. Higher scores imply a stronger relationship \nwith the occurrence of hypertension. The most influential feature is coded as 773.1, corresponding to hemolytic \ndisease due to ABO isoimmunization, which could possibly indirect cause the hypertension due to compensatory \ncardiovascular responses, fluid overload, etc. Subsequent features include a mix of codes representing both related \nconditions and general health indicators. We have three observations as follows: (1). Some common related \ndiseases are identified as salient features, such as diabetes, gout, pneumonia, asbestosis, etc. (2). The orthopedics \ndisease is highly related to hypertension e.g., closed fracture(820.21), nonunion fracture(733.82), etc. Because \nthe hypertension patient have higher probability to suffer the fracture than others 49. (3). There are also several \nimportant features related to newborn infants, which may suggest a correlation between the circumstances of \nbirth and the likelihood of developing hypertension later in life. The pregnant mother is also more likely to \nsuffer from hypertension (gestational hypertension). Compared to hypertension in the other groups of patients, \nnewborn hypertension is relatively rare50. From the salient features we observed in the hypertension prediction, \nthe hemolytic disease due to ABO isoimmunization of infant(773.1), septicemia sepsis of newborn(771.81), \nother transitory neonatal electrolyte disturbances(775.5), gestation status(765.24, 765.25, 765.28) are risk factors \nwith high probabilities.\nFig. 5. Feature importance heatmap for hypertension and heart failure prediction.\n \nFig. 4. Embedding visualizations (t-SNE) on hypertension and heart failure prediction by RETAIN and CKLE.\n \nScientific Reports |        (2024) 14:30675 8| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports/\nCase study II: important features of heart failure prediction\nIn the heart failure prediction, the 20 most important features are presented on the right part of the Fig.  5. The \nmost important feature is acute posthemorrhagic anemia (285.1), which can increase the heart’s workload and \ncause tissue hypoxia. There are several types of the risk factors observed from the important input features. (1). \nPrevious cardiovascular conditions(997.1, 440.21, 414.00, 440.0) of the patients play important role in the heart \nfailure in the future visits. (2) Heart function can also be impacted by diabetes(250.60). (3) The kidney diseases \nis highly related to heart failure (600.01,285.21, 585.3, 572.4). Because impaired kidney function can cause the \nfluid overload, which increases the heart’s workload and exacerbates heart failure symptoms. Moreover, kidney \ndisease lead to high blood pressure and contribute to systemic inflammation, both of which further strain the \nheart and impair its function. (4) Unhealthy life habit can also increase the risk of heart failure. For example, \nalcohol dependence(303.93) and anxiety state(300.00) can cause the higher risk of heart failure. (5) Hypertension \nis another very important risk factor for heart failure. Observed from the Fig.  5, we could find that the patients \nwith hypertension(401.9) is more likely to be diagnosed with heart failure but not vice versa.\nDiscussion\nMulti-modality learning has been widely discussed and attracted lots of attention for healthcare data. The data \nin the healthcare domain has different characteristics compared with data in the other domains. From the \nstandpoint of data-centric AI, three healthcare data challenges are summarized as follows: (1). The noise in \nthe healthcare data is prevalent and unignorable. This can be caused by the device noise, human bias, noise in \nrecording process, etc. (2). The clinical text is usually highly specialized and domain specific. There are lots of \nprofessional terms in the medical domain, which requires prior knowledge. (3). There is usually a privacy issue \nof the health data to request the model deployed locally. Most hospitals will not put their data on the cloud server \nor use the online models to help with their clinical workflows. These data level challenges put out the requests to \ndesign novel and suitable AI models with an emphasis on the precision, robustness and privacy.\nPrevious works mainly advance the technique and models to learn the embedding of different modalities \nand combine them in an efficient way. In the realm of LLM, the representation learning ability from text \ndata has been transformativly boosted. How to efficiently leverage the LLM in the multi-modality healthcare \ndata remains as an open research question. We distinguish the CKLE framework from the related work from \nfour aspects. (1) The knowledge of LLM is effectively learned by the health predictive model with knowledge \ndistillation. The knowledge from LLM is powerful which leads to large-scale parameters of the LLM that has \nefficiency issue. (2). We explore a novel method of patient similarity learning with contrastive loss function. The \npatient similarity can be learned by taking advantage of the contrastive loss, which can be used to learn postive \nand negative pairs. We design the soft labels for the contrastive loss function to learn the similarities between \npatients with more granularity. This contrastive loss for patient similarity learning can be easily adapted to other \npredictive model by inserting into the loss function. (3). Besides competitive prediction accuracy, the CKLE \nframework can learn better representations validated by embedding visualization. From the observations in \nthe Table 1, the increase of performance metrics indicates the effectiveness of the CKLE to improve the predict \naccuracy. However, we cannot observe the representation effectiveness through the numerical results, which are \nalso very important to evaluate the model. In the t-SNE embedding visualization experiments, we can observe a \nmore clear discrimination between two categories predicted by the CKLE compared to the baseline method. (4). \nCKLE predictive model preserves the global model interpretability, which can provide the feature importance \nby the attention score. The interpretability is a very essential aspect when we build the medical AI models. In \nthis paper, we distill the knowledge from LLM into the predictive model, which is a type of Transformer. The \ninterpretability of Transformer can be represented as the attention score for each input features. The feature \nimportance can show which feature plays an important role in the predictions. From the model interpretation \nanalysis, we study two cases on hypertension and heart failure prediction. The top 20 important features we get \ncorresponds to the medical knowledge with the domain expert. So our model can produce precise as well as \ninterpretable predictions on the health events.\nFrom the collaboration with domain expert in the cardiology diseases, we can validate some already known \nmedical knowledge and discover some new features which lacks enough attention previously. The CKLE can not \nonly precisely predict health events but also can discover some medical findings.\nData availability\nThe data that support the findings of this study are available from PhysioNet but restrictions apply to the avail-\nability of these data, which were used under license for the current study, and so are not publicly available. Data \nare however available from the authors upon reasonable request and with permission of PhysioNet. If the request \nfor data is needed, please contact Sirui Ding at siruiding@tamu.edu as email address.\nReceived: 14 March 2024; Accepted: 4 October 2024\nReferences\n 1. Y adav, P ., Steinbach, M., Kumar, V . & Simon, G. Mining electronic health records (ehrs) a survey. ACM Comput. Surv. (CSUR). 50, \n1–40 (2018).\n 2. Sung, M. et al. Event prediction model considering time and input error using electronic medical records in the intensive care unit: \nRetrospective study. JMIR Med. Inf. 9, e26426 (2021).\n 3. Pakbin, A. et al. Prediction of icu readmissions using data at patient discharge. in 40th annual international conference of the IEEE \nengineering in medicine and biology society (EMBC), 4932–4935 (IEEE, 2018). (2018).\nScientific Reports |        (2024) 14:30675 9| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports/\n 4. Yin, K., Cheung, W . K., Fung, B. C. & Poon, J. Learning inter-modal correspondence and phenotypes from multi-modal electronic \nhealth records. IEEE Trans. Knowl. Data Eng. 34, 4328–4341 (2020).\n 5. Khattak, F . K. et al. A survey of word embeddings for clinical text. J. Biomed. Inf. 100, 100057 (2019).\n 6. Rim, B., Sung, N. J., Min, S. & Hong, M. Deep learning in physiological signal data: A survey. Sensors. 20, 969 (2020).\n 7. Tayefi, M. et al. Challenges and opportunities beyond structured data in analysis of electronic health records. Wiley Interdiscip Rev. \nComput. Stat. 13, e1549 (2021).\n 8. Hoekstra, O., Hurst, W . & Tummers, J. Healthcare related event prediction from textual data with machine learning: A systematic \nliterature review. Healthc. Anal. 2, 100107 (2022).\n 9. Duan, H., Sun, Z., Dong, W ., He, K. & Huang, Z. On clinical event prediction in patient treatment trajectory using longitudinal \nelectronic health records. IEEE J. Biomed. Heal Inf. 24, 2053–2063 (2019).\n 10. Zhang, D., Yin, C., Zeng, J., Yuan, X. & Zhang, P . Combining structured and unstructured data for predictive models: A deep \nlearning approach. BMC Med. Inf. Decis. Mak. 20, 1–11 (2020).\n 11. Tomašev, N. et al. Use of deep learning to develop continuous-risk models for adverse event prediction from electronic health \nrecords. Nat. Protoc. 16, 2765–2787 (2021).\n 12. Kosorok, M. R. & Laber, E. B. Precision medicine. Annu. Rev. Stat. its Appl. 6, 263–286 (2019).\n 13. Cohen, Z. D., Delgadillo, J. & DeRubeis, R. J. Personalized treatment approaches. Bergin Garfield’s handbook psychotherapy \nbehavior change: 50th anniversary edition (2021).\n 14. Mathews, S. C. et al. Digital health: A path to validation. Npj Digit. Med. 2, 38 (2019).\n 15. Kemp, C. D. & Conte, J. V . The pathophysiology of heart failure. Cardiovasc. Pathol. 21, 365–371 (2012).\n 16. Lee, D. S. et al. Trial of an intervention to improve acute heart failure outcomes. New. Engl. J. Med. 388, 22–32 (2023).\n 17. Savarese, G. & Lund, L. H. Global public health burden of heart failure. Cardiac Fail. Rev. 3, 7 (2017).\n 18. Vogenberg, F . R. Predictive and prognostic models: Implications for healthcare decision-making in a modern recession. Am. \nHealth Drug Benefits. 2, 218 (2009).\n 19. Oparil, S. et al. Hypertension. Nat. Rev. Dis. Primers. 4, 1–21 (2018).\n 20. Kumar, S., Selim, M. H. & Caplan, L. R. Medical complications after stroke. Lancet Neurol. 9, 105–118 (2010).\n 21. Chabot, I., Moisan, J., Grégoire, J. P . & Milot, A. Pharmacist intervention program for control of hypertension. Ann. Pharmacother. \n37, 1186–1193 (2003).\n 22. Liu, J., Capurro, D., Nguyen, A. & Verspoor, K. Attention-based multimodal fusion with contrast for robust clinical prediction in \nthe face of missing modalities. J. Biomed. Inf. 145, 104466 (2023).\n 23. Liu, J., Capurro, D., Nguyen, A. & Verspoor, K. Note bloat impacts deep learning-based nlp models for clinical prediction tasks. J. \nBiomed. Inf. 133, 104149 (2022).\n 24. Thirunavukarasu, A. J. et al. Large language models in medicine. Nat. Med. 29, 1930–1940 (2023).\n 25. Lu, C., Han, T. & Ning, Y . Context-aware health event prediction via transition functions on dynamic disease graphs. in Proceedings \nof the AAAI Conference on Artificial Intelligence, vol. 36, 4567–4574 (2022).\n 26. Seinen, T. M. et al. Use of unstructured text in prognostic clinical prediction models: A systematic review. J. Am. Med. Inf. Assoc. \n29, 1292–1302 (2022).\n 27. Lu, C., Reddy, C. K., Chakraborty, P ., Kleinberg, S. & Ning, Y . Collaborative graph learning with auxiliary text for temporal event \nprediction in healthcare. Proc. Thirtieth Int. Jt. Conf. on Artif. Intell. (2021).\n 28. Mugisha, C. & Paik, I. Pneumonia outcome prediction using structured and unstructured data from ehr. In IEEE International \nConference on Bioinformatics and Biomedicine (BIBM), 2640–2646 (IEEE, 2020). (2020).\n 29. Wornow, M. et al. The shaky foundations of large language models and foundation models for electronic health records. Npj Digit. \nMed. 6, 135 (2023).\n 30. Qiu, J. et al. Large ai models in health informatics: Applications, challenges, and the future. IEEE J. Biomed. Heal Inf. (2023).\n 31. Kim, Y ., Xu, X., McDuff, D., Breazeal, C. & Park, H. W . Health-llm: Large language models for health prediction via wearable \nsensor data. arXiv preprint arXiv:2401.06866 (2024).\n 32. Zhao, Y . et al. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102 (2023).\n 33. Kim, S. et al. Propile: probing privacy leakage in large language models. Adv. Neural Inf. Process. Syst. 36 (2024).\n 34. Zhang, J., Krishna, R., Awadallah, A. H. & Wang, C. Ecoassistant: Using llm assistant more affordably and accurately. arXiv preprint \narXiv:2310.03046 (2023).\n 35. Zhang, A., Xing, L., Zou, J. & Wu, J. C. Shifting machine learning for healthcare from development to deployment and from models \nto data. Nat. Biomed. Eng. 6, 1330–1345 (2022).\n 36. Li, Y . et al. Personal llm agents: Insights and survey about the capability, efficiency and security. arXiv preprint arXiv:2401.05459 \n(2024).\n 37. Belyaeva, A. et al. Multimodal llms for health grounded in individual-specific data. In Workshop on Machine Learning for \nMultimodal Healthcare Data, 86–102 (Springer, (2023).\n 38. Han, Y . et al. Chartllama: A multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483 (2023).\n 39. Nguyen, H. & Patrick, J. Text mining in clinical domain: Dealing with noise. In Proceedings of the 22nd ACM SIGKDD international \nconference on knowledge discovery and data mining, 549–558 (2016).\n 40. Moradi, M., Blagec, K. & Samwald, M. Deep learning models are not robust against noise in clinical text. arXiv preprint \narXiv:2108.12242 (2021).\n 41. Suo, Q. et al. Deep patient similarity learning for personalized healthcare. IEEE Trans. Nanobiosci. 17, 219–227 (2018).\n 42. Guo, L. L. et al. Characterizing the limitations of using diagnosis codes in the context of machine learning for healthcare. BMC \nMed. Inf. Decis. Mak. 24, 51 (2024).\n 43. Ma, L. et al. Adacare: Explainable clinical health status representation learning via scale-adaptive feature extraction and \nrecalibration. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, 825–832 (2020).\n 44. Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. A simple framework for contrastive learning of visual representations. In \nInternational conference on machine learning, 1597–1607PMLR, (2020).\n 45. Tian, Y ., Krishnan, D. & Isola, P . Contrastive representation distillation. Int. Conf. Learn. Represent (2020).\n 46. Choi, E. et al. Retain: an interpretable predictive model for healthcare using reverse time attention mechanism. Adv. Neural Inform. \nProcess. Syst. 29 (2016).\n 47. Ma, F . et al. Dipole: Diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks. In Proceedings \nof the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 1903–1911 (2017).\n 48. Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 1–67 (2020).\n 49. Du, X. P ., Zheng, M. L., Y ang, X. C. & Zheng, M. L. High blood pressure is associated with increased risk of future fracture, but not \nvice versa. Sci. Rep. 14, 8005 (2024).\n 50. Altemose, K. & Dionne, J. M. Neonatal hypertension: Concerns within and beyond the neonatal intensive care unit. Clin. Exp. \nPediatr. 65, 367 (2022).\nAcknowledgment\nThis work was in part supported by NIH 1OT2OD032581-02-999, NSF IIS-2450662, IIS-2239257, and IIS-\n1900990. Views in this paper are of the authors and should not be interpreted as those of any funding agencies.\nScientific Reports |        (2024) 14:30675 10| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports/\nAuthor contributions\nSD contributed to the idea, method design and implementation, experiments conduction, and manuscript writ-\ning. JY contributed to the writing of the manuscript. NZ and XH contributed to the idea refinement and man -\nuscript review.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to N.Z.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2024  \nScientific Reports |        (2024) 14:30675 11| https://doi.org/10.1038/s41598-024-75331-2\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6633111834526062
    },
    {
      "name": "Event (particle physics)",
      "score": 0.5713518261909485
    },
    {
      "name": "Natural language processing",
      "score": 0.4486251473426819
    },
    {
      "name": "Data science",
      "score": 0.44535520672798157
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3535498082637787
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I91045830",
      "name": "Texas A&M University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4387153466",
      "name": "Weill Cornell Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74775410",
      "name": "Rice University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I44461941",
      "name": "University of Houston",
      "country": "US"
    }
  ]
}