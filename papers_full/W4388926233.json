{
    "title": "Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks",
    "url": "https://openalex.org/W4388926233",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2096576973",
            "name": "Luo Ling",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Ning, Jinzhong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2352599641",
            "name": "Zhao YingWen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1767810064",
            "name": "Wang Zhijun",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Ding, Zeyuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2001629680",
            "name": "Chen Peng",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Fu, Weiru",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Han, Qinyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2745392558",
            "name": "Xu Guang-tao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2388162996",
            "name": "Qiu Yun-zhi",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Pan, Dinghao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3082143944",
            "name": "Li JiRu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A875780830",
            "name": "Li Hao",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Feng, Wenduo",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Tu, Senbo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1925725887",
            "name": "Liu Yuqi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2157957142",
            "name": "Yang ZhiHao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A598734319",
            "name": "Wang Jian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A381445461",
            "name": "Sun Yuan-yuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2366246759",
            "name": "Lin Hongfei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4387928881",
        "https://openalex.org/W4378468480",
        "https://openalex.org/W4385681676",
        "https://openalex.org/W2169099542",
        "https://openalex.org/W2174775663",
        "https://openalex.org/W4366198844",
        "https://openalex.org/W4387225871",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4385681988",
        "https://openalex.org/W3169341408",
        "https://openalex.org/W4382618722",
        "https://openalex.org/W4303443398",
        "https://openalex.org/W4386530347",
        "https://openalex.org/W4312220150",
        "https://openalex.org/W4283318325",
        "https://openalex.org/W4377865190",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W4361020491",
        "https://openalex.org/W4388651010",
        "https://openalex.org/W4386528723",
        "https://openalex.org/W3177765786",
        "https://openalex.org/W2145870108",
        "https://openalex.org/W4386273007",
        "https://openalex.org/W4297253404",
        "https://openalex.org/W4361866125",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4309523728",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4362598952",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W4381587418",
        "https://openalex.org/W4367623495",
        "https://openalex.org/W4224325974",
        "https://openalex.org/W4381253519",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4378711639",
        "https://openalex.org/W3186799149",
        "https://openalex.org/W4366327625",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4377009978",
        "https://openalex.org/W4385571124"
    ],
    "abstract": "Objective: Most existing fine-tuned biomedical large language models (LLMs) focus on enhancing performance in monolingual biomedical question answering and conversation tasks. To investigate the effectiveness of the fine-tuned LLMs on diverse biomedical NLP tasks in different languages, We present Taiyi, a bilingual fine-tuned LLM for diverse biomedical tasks. Materials and Methods: We first curated a comprehensive collection of 140 existing biomedical text mining datasets (102 English and 38 Chinese datasets) across over 10 task types. Subsequently, a two-stage strategy is proposed for supervised fine-tuning to optimize the model performance across varied tasks. Results: Experimental results on 13 test sets covering named entity recognition, relation extraction, text classification, question answering tasks demonstrate that Taiyi achieves superior performance compared to general LLMs. The case study involving additional biomedical NLP tasks further shows Taiyi's considerable potential for bilingual biomedical multi-tasking. Conclusion: Leveraging rich high-quality biomedical corpora and developing effective fine-tuning strategies can significantly improve the performance of LLMs within the biomedical domain. Taiyi shows the bilingual multi-tasking capability through supervised fine-tuning. However, those tasks such as information extraction that are not generation tasks in nature remain challenging for LLM-based generative approaches, and they still underperform the conventional discriminative approaches of smaller language models.",
    "full_text": "Taiyi: A Bilingual Fine-Tuned Large Language Model for \nDiverse Biomedical Tasks \nLing Luo*, Jinzhong Ning, Yingwen Zhao, Zhijun Wang, Zeyuan Ding, Peng Chen, Weiru Fu, Qinyu \nHan, Guangtao Xu, Yunzhi Qiu, Dinghao Pan, Jiru Li, Hao Li, Wenduo Feng, Senbo Tu, Yuqi Liu, \nZhihao Yang, Jian Wang, Yuanyuan Sun, Hongfei Lin \n \nSchool of Computer Science and Technology, Dalian University of Technology, Dalian 116024, China \n*Corresponding author: lingluo@dlut.edu.cn \n \nAbstract \nObjective: Most existing fine-tuned biomedical large language models (LLMs)  focus on enhancing \nperformance in monolingual biomedical question answering and conversation tasks. To investigate the \neffectiveness of the fine-tuned LLMs on diverse biomedical NLP tasks in diffe rent languages, We \npresent Taiyi, a bilingual fine-tuned LLM for diverse biomedical tasks. \nMaterials and Methods: We first curated a comprehensive collection of 140 existing biomedical text \nmining datasets (102 English and 38 Chinese datasets) across over 10 task types. Subsequently, a \ntwo-stage strategy is proposed for supervised fine -tuning to optimize the model performance across \nvaried tasks. \nResults: Experimental results on 13 test sets covering named entity recognition, relation extraction, \ntext classification, question answering tasks demonstrate that Taiyi achieves superior performance \ncompared to general LLMs. The case study involving additional biomedical NLP tasks further shows \nTaiyi's considerable potential for bilingual biomedical multi-tasking. \nConclusion: Leveraging rich high -quality biomedical corpora and developing effective fine -tuning \nstrategies can significantly improve the performance of LLMs within the biomedical domain. Taiyi shows \nthe bilingual multi-tasking capability through supervised fine-tuning. However, those tasks such as \ninformation extraction that are not generation tasks in nature remain challenging for LLM-based \ngenerative approaches, and they still underperform the conventional discriminative approaches of \nsmaller language models. \nKey words: natural language processing; large language model; supervised fine-tuning; biomedical \nmulti-tasking. \nINTRODUCTION  \nRecently, the release of ChatGPT  [1] and the subsequent launch of GPT -4 [2] have received \nwidespread attention around the world, which promotes  the development of large language model s \n(LLMs) that have billions of parameters and trained with hundreds of billions of tokens . These LLMs \n(such as GPT-4, PaLM [3], LLaMA [4], and GLM [5]) have shown promising results and achieved new \nstate-of-the-art performance in various natural language processing (NLP) tasks.  Compared with \nprevious pre-trained language models (such as BERT [6]), LLMs exhibit enhanced text generation and \ncomprehension abilities. Moreover, their few-shot/zero-shot learning and  generalization abilit ies \naddress unseen or complicated tasks. Due to the advantages, LLMs have been explored to apply in \nvarious fields, such as law [7], education [8], finance [9], and biomedicine and health [10]. \nLLMs in the biomedical and healthcare domain are essential and potential for handling the scale \nand complexity of biomedical data, as well as for providing more personalized and empathetic medical \ncare, ultimately advancing the quality and effectiveness of healthcare practices. However, biomedical \ntext exhibits some distinct features  compared to text in the general do main, such as  complex \nterminologies, ambiguous abbreviations, more complex syntax, and less common vocabulary. Owing \nto these domain characteristics, most existing LLMs trained on general domain  data encounter \ndifficulties when they are applied to the biomedical domain [11]. Therefore, some domain-special LLMs \nhave been developed to facilitate the development of NLP methods for biomedical applications . For \nexample, Google’s team recently developed Med-PaLM2 [12], a fine -tuned LLM using medical data \nbased on PaLM  [3], which has achieved a high performance on the medical QA datasets with an \naccuracy of over 8 0%. Apart from the non-open biomedical LLMs, there have been some efforts to \ndevelop open-source biomedical LLMs [13-20]. Most of the models are derived from some open-source \ngeneral LLMs (such as LLaMA [4], BLOOMZ [21], and GLM [5]) and are fine-tuned with monolingual \n(e.g., English or Chinese) question answering (QA) and conversation data. They are primarily directed \ntoward enhancing performance in biomedical QA and conversation -oriented tasks . However, the \nefficacy and suitability of the LLMs on diverse biomedical NLP tasks in different languages remains \nunclear and warrants further investigation. \nTo address these problems, we present Taiyi, a b ilingual (English and Chinese) fine-tuned large \nlanguage model for diverse biomedical tasks. First, a comprehensive collection including 102 English \nand 38 Chinese datasets is assembled, covering over 10 biomedical task types. To facilitate task -\nspecific requirements and enable consistent formatting across all datasets, standardized data schemas \nare designed and universally applied during dataset curation. Then, in the supervised fine-tuning (SFT) \nphase, we propose a two-stage fine-tuning strategy. In contrast to the simple single-stage fine-tuning, \nthis strategy significantly optimize s model performance across a diversity of tasks . Finally, t he \nevaluation of Taiyi is conducted on 13 Biomedical NLP test sets. Experimental results demonstrate the \npromising potential of Taiyi in bilingual multi -task learning. It achieves superior performance on \nbiomedicine-specific tasks compared to general LLMs. However, Taiyi has an improvement room for \ncurrent state-of-the-art models specialized for individual tasks.  \nRELATED WORK \nRecent advancements in LLMs, such as GPT-4 [2] and PaLM [3], have attracted considerable attention \ndue to instruction -following and producing human -like responses. Subsequently, the researchers \nattempt to duplicate the GPT series to develop open-source foundation models like LLaMA [4], Bloom \n[22], Falcon [23], GLM [5] and Qwen [24]. These LLMs present strong performances on various NLP \ntasks, including zero - and few -shot learning scenarios. The promising capabilities of LLMs have \nsparked interest and potential applications in various fields, particularly in the biomedical domain.  \nWhen applied to specific domains like biomedicine, large-scale models often perform sub-optimal. \nTo improve the performance of the models in biomedical tasks, there have been some efforts in training \nLLMs specifically for the biomedical domain. For example, MedAlpaca [14] builds upon medical data \nto fin e-tune Stanford Alpaca for applications related to medical question -answering and dialogue. \nChatDoctor [19] is designed to simulate a conversation between a doctor and a patient , fine-tuning \nLLaMA with medical literature. Additionally, Med-PaLM [25] has shown promising performance on the \nMedQA exam based on clinical corpora and human feedback. Meanwhile, aiming at the Chinese \nmedical domain, Chinese LLMs such as BenTsao [16], DoctorGLM [17], and HuatuoGPT [20], are \ndeveloped on the Chinese medical dialogue data. And more recently Zhongjing [18] and ChiMed-GPT \n[26] adopted full pipeline training from pre-training, SFT, to reinforcement learning with human feedback. \nThe overview of existing LLMs in the biomedical domain can be found in Supplementary Table S1.  \nMost existing open-source LLMs focus on fine-tuning with monolingual QA and conversation data . \nDifferent from the above monolingual LLMs, our Taiyi embarks on the bilingual (English  and Chines) \nbiomedical large language model, aiming to explore the capabilities of large models in handling a \nvariety of bilingual NLP tasks in the biomedical field. \nMATERIALS AND METHODS  \n \n \nFigure 1. Overview of the Taiyi framework. Note that the English in brackets is not part of the input and \noutput; it is the translation of the Chinese. \n \n\nFigure 1 shows the overview of our framework. We first collected and curated a set of existing bilingual \n(Chinese and English) biomedical natural language processing (BioNLP) corpora. Then these corpora \nwere converted to the instruction data used to fine-tune the general LLM. In the supervised fine-tuning \nphase, we propose a two-stage supervised instruction fine-tuning strategy. Finally, our Taiyi model can \nbe applied to various BioNLP tasks and it is evaluated on 13 test sets covering 4 BioNLP task types.  \n \nTraining data \nDataset collection \nThis study aims to explore the capabilities of LLMs in handling a variety of bilingual BioNLP tasks by \nsupervised fine-tuning. Therefore, we focus on manually annotated English and Chinese biomedical \ncorpora. To fully utilize available BioNLP resources, we make efforts to aggregate comprehensive sets \nof open-source datasets in both English and Chinese. The data collection is primarily from two sources: \nexisting English /Chinese BioNLP shared task datasets  and the training data used for existing \nbiomedical LLMs. \n \n \nFigure 2. Overview of the dataset collection. In the tree map (left), the entire graph is represented by \na large rectangle, representing the 140 bilingual open-source datasets collected. This large rectangle \nis divided into smaller rectangles of various colors, with each small rectangle representing a specific \nBioNLP task and listing the names of all datasets related to that task. In the scatter plot  (top right), \neach data point represents a BioNLP task, with its size determined by the number of datasets \nassociated with that task. The bar chart (bottom right) presents the scale of related datasets for different \ntasks, further illustrating the number of datasets for each task. \n \nOur data collection significantly benefited from  two major previous efforts  in aggregating \nbiomedical text mining datasets – BigBio [27] and CBLUE  [28]. The BigB io aggregates a large \ncollection of English BioNLP datasets, while the CBLUE dataset assembles a wide range of Chinese \nbiomedical natural language understanding datasets. In addition, we also collected some other relevant \nBioNLP datasets that are not included in BioBio and CBLUE. Finally, we have successfully assembled \n\na total of 140 biomedical datasets. All datasets are categorized into 15 different BioNLP task types as \nshown in Figure 2 : Named Entity Recognition/Normalization (NER/NEN), Relation Extraction (RE), \nCausal Relation Extraction (CRE), Event Extraction (EE), Coreference Resolution (COREF), Text \nClassification (TC), Question Answering-Multiple Choice (QA-mc), Question Answering-Simple Answer \nQuestions (QA-sqa), Question Answering-Context-based Answer Questions (QA -cqa), Multi-Round \nDialogue (MRD), Machine Translation (MT), Text Pairs-Semantic Similarity (TP-ss), Text Pairs-Textual \nEntailment (TP-te), Text to Text/Struct-Document Summarization (TT-ds) and Text/Struct-Text to Struct \n(TT-ts). Among these datasets, there are 38 Chinese datasets covering 10 different BioNLP tasks, and \n102 English datasets spanning 12 BioNLP tasks. \nFor each dataset, we collated key metadata including task types, data size, task descriptions, and \nthe links of the dataset and paper. This metadata facilitates full understanding and proper usage of \neach corpus. The collection of bilingual datasets across diverse biomedical language processing tasks \nfacilitates the comprehensive evaluation and development of multilingual BioNLP models. Details can \nbe accessed at the following link: https://github.com/DUTIR -BioNLP/Taiyi-\nLLM/blob/main/data_file/dataset_inf.md. \nTask schema harmonization \nIn the original data set, even for the same BioNLP task, there are many different data formats due to \ndifferent sources and developers.  For example, the annotation formats include BioC, CoNLL, and \nPubTator formats across NER datasets. Such inconsistencies introduce challenges for developing \nsystems that can leverage diverse annotated datasets. Therefore, establishing a universal data format \nenables interoperability and is beneficial for cross-dataset integration. To be consistent with previous \nefforts, we extended the schema from BioBio schema to support all tasks in our collection. The \ncomplete unified task schema can be found at https://github.com/DUTIR-BioNLP/Taiyi-\nLLM/blob/main/data_file/Task_schemas_en.md.  \nInstruction-tuning data construction \nTo enable the model to understand task instructions for performing multi-tasking, we constructed the \ninstruction data for fine-tuning, which covers the tasks described in the above section. Moreover, we \nalso added MOSS data (i.e., M oss-003-sft-data) [29] into the training data  to enhance the general \nconversation and harmlessness ability of Taiyi. Moss-003-sft-data is well -aligned with the real-world \ndistribution of user intents, covering finer -grained categories and more diverse harmlessness-related \ndata. Previous studies [30 31] have demonstrated that the quality of the training data plays a critical \nrole in the performance of downstream tasks . Inferior quality data adversely affects the performance \nof the LLMs. Therefore, w e manually analyzed the datasets , then the high -quality datasets were \nselected. We filtered duplicated training data, and those overlapped documents in the training set if the \ndocuments also exist in the test set to accurately evaluate the model performance. The statistics of the \nfinal data used for instruction-tuning is presented in Table 1.  \n \nTable 1. Statistics of our final instruction-tuning data \nTask Type English data size Chinese data size \nNamed Entity Recognition 28,603 44,667 \nRelation Extraction 17,279 26,606 \nEvent Extraction 2,022 2,992 \nText Classification 40,339 37,624 \nText Pair Task 11,237 45,548 \nMachine Translation 74,113 \nBiomedical Question Answering 57,962 129,562 \nBiomedical Multi-Round Dialogue 10,000 16,391 \nGeneral Dialogue Data 560,000 \nOther Additional Tasks 9,370 \nTotal 1,114,315 \n \nTo construct the instruction-tuning data, we design instructional templates for each task.  For the \nQA and dialogue tasks, original questions are used as the model input and answers are used as the \noutput. For other tasks, approximately 15 instruction templates were manually created for each task in \nEnglish and Chinese, respectively. Some examples of th e instruction data can be found in \nSupplementary Table S2. \nFurthermore, some complex NER and RE tasks are divided into multiple subtasks to reduce \ndifficulty and increase diversity. For instance, the BC5CDR [32] task of chemical and disease entity \nrecognition is separated into the chemical entity recognition and disease entity recognition subtasks. \nThe original complex task is retrained while th ese additional subtasks are added to the instruction -\ntuning data. \nModel training  \nBase model \nRecently, Alibaba Cloud's pre-trained LLM Qwen series [24] have been developed and show n \npromising results in various NLP tasks. We chose the Qwen-7B-base version for supervised instruction \nfine-tuning. Qwen-7B is a Transformer -based pre-trained language model that obtains generalized \nlanguage understanding capabilities through self -supervised learning on large -scale high -quality \nmultilingual pretraining corpora. Compared to models like BERT, the training data coverage of Qwen-\n7B is more extensive, including web texts, academic books, code, and other resources. It has \napproximately 7 billion parameters and a vocabulary size of around 150,000. \nIn public  leaderboards1 ,2  of downstream English and Chinese tasks, Qwen -7B significantly \noutperforms models of similar size and even surpasses larger models on some tasks. This \n \n1 https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard;  \n2 https://cevalbenchmark.com/static/leaderboard.html \ndemonstrates Qwen -7B's strong language understanding and transfer learning abilities. The \nconsiderations for choosing Qwen-7B as our pre -trained model are as follows:  1) The model size is \nmoderate, with high training efficiency that meets our computational resource constraints; 2) The \ntraining data coverage is extensive, encompassing the common language, domain-specific language, \nand code data required for our cross -domain task;  3) It achieves strong performance on public \nbenchmarks, with sufficient language understanding capabilities to provide a solid foundation for our \ndownstream task. \nTwo-stage Supervised Instruction Fine-tuning \nDuring the SFT stage, we performed fine-tuning on our curated dataset of millions of examples. Since \nthe instruction-tuning data involves dozens of datasets across over ten biomedical NLP tasks in both \nChinese and English, performing fine-tuning in a simple single stage would result in task interference, \npreventing convergence on some of the more challenging NLP tasks. Therefore, we propose a two -\nstage supervised instruction fine-tuning strategy shown in Figure 3.  \n \n \nFigure 3. The two-stage training process of Taiyi. Tasks in Type1 are in the blue background, and task2 in \nType2 are in the yellow background. \n \nAccording to the task type and the size of the task dataset, we first manually categorized all the \ntasks into two types of the tasks: Type1 and Type2. In Type1, most tasks are not generation tasks in \nnature (e.g., NER is a sequence labeling task in nature rather than a generation task),  or the size of \nthe task dataset is related small. These tasks include information extraction, text classification, text pair \ntasks, machine translation and other additional tasks. In Type2, there are QA and dialogue tasks \nincluding biomedical QA, biomedical multi-round dialogue, and general dialogue tasks.  These tasks \nare generation tasks in nature . In the first  stage of model training, we first performed supervised \ninstruction fine -tuning on the data from the Type1 tasks (around 340,000 instances). The best \ncheckpoint was selected through a combination of human evaluation and automated metrics on the \ndevelopment sets for the second stage of training. In the second training stage, all the training data \nfrom the first stage are used as retrospective data, and all data in Type2 are mixed for incremental \n\ntraining. \nWe used 8 A40 GPUs for SFT. The model was trained with 5 epochs (~7 hours per epoch) in stage \n1, and 3 epochs (~26 hours per epoch) in stage 2. To improve model training efficiency, we chose Qlora \n(Dettmers, et al., 2023), an efficient tuning algorithm, to conduct supervised instruction fine-tuning. The \nmain hyperparameters for the training process are set as follows: Batch size per GPU of 12, learning \nrate of 0.0002, warmup ratio of 0.1, max length of 1024, lora rank of 64, lora  alpha of 16, and lora \ndropout of 0.05. \nRESULTS  \nEvaluation tasks, datasets and metrics \nTo investigate the capability of the Taiyi model for various bilingual BioNLP tasks, we selected four task \ntypes (i.e., NER, RE, TC, multiple-choice QA) as the metrics evaluation and other tasks for the case \nstudy. The statistics of the test sets are shown in Supplementary Table S3. Further details for each \nevaluation task are provided below. \nBiomedical Named Entity Recognition (NER) . Biomedical NER aims to identify predefined \nbiomedical entities from text , such as diseases, drugs, genes, and proteins. Six biomedical NER \ndatasets (i.e., BC5CDR-Chemcial [32], BC5CDR-disease [32], CHEMDNER [33], NCBI-Disease [34], \nBioRED [35] and CMeEE-dev [28]) are used for our NER evaluation . The exact match (the predicted \nentity text and entity type without span) micro F1-score was used as the evaluation metrics. \nBiomedical Relation Extraction (RE). Biomedical RE aims to automatically extract predefined \nrelations or associations between biomedical entities from text. In this task, we focus on extracting the \nentity relation triple (head entity, tail entity, relation type) . We selected one English dataset BC5CDR \nand one Chinese dataset CMeEE-dev [28] for the RE evaluation. The micro F1-score was used as the \nRE metrics. \nBiomedical Text Classification (TC). Biomedical TC aims to automatically categorize texts into \npredefined biomedical class labels.  Two manually annotated English multi-label document \nclassification datasets (BC7LitCovid [36] and HoC [37]) and one Chinese dataset (KUAKE_QIC [28]) \nare used for the TC evaluation. The micro F1-score was used as the metrics. \nBiomedical Multiple-choice Question Answering  (QA-mc). Biomedical multiple -choice \nQuestion Answering task aims to answer multiple-choice questions related to biomedicine and \nhealthcare. PubMedQA [38] and MedQA -USMLE (4-option) [39] are used to evaluate the model \nperformance in English. MedQA-MCMLE (4-option) [39] is used to evaluate the model performance in \nChinese. We used the accuracy as the metrics for this task. \nIn addition to the four primary evaluation tasks, the capabilities of the Taiyi model were also shown \nthrough examples on several supplementary biomedical NLP tasks , such as  medical Report \nGeneration, Biomedical Event Extraction, Biomedical Machine Translation, Biomedical Title Generation, \nBiomedical Text Semantic Similarity, Biomedical Question Answering and Chat. Details can be found \nin Supplementary Examples of Taiyi output on the supplementary biomedical NLP tasks. \nEffectiveness of the two-stage fine-tuning strategy \nIn this experiment, we tested the effect of our two-stage fine-tuning strategy on the four tasks. For \ncomparison, we simply combined all data of the four tasks to finetune the base model as the baseline. \nAs the cost of LLMs is high, we randomly sampled 200 instances from the test set p er dataset. The \nperformances of the models using the simple one-stage and the two-stage strategies on the tasks are \nshown in Figure 4. \n \n \nFigure 4. Performances of models using different fine-tuning strategies. One-stage denotes the model \nfine-tuned by combining all task datasets. Two-stage denotes the model fine-tuned by our two-stage \nstrategy. (A) the results on the NER tasks. (B) the results on the RE tasks. (C) the results on the TC \ntasks. (D) the results on the QA-mc tasks. \n \nWhen we simply combined all task datasets as a one -stage strategy to fine -tune the LLM, the \nresults show poor performances on all tasks. The main reason may be the challenges of converging \ndifferent tasks with varying levels of difficulty and dataset sizes. Therefore, we first fine-tuned the model \non those Type1 tasks described in the section of Two-stage Supervised Instruction Fine -tuning \nindependently, then combined all data to continue fine -tuning the model in the second stage. The \nresults show that our two-stage strategy outperforms the one-stage strategy on all English and Chinese \ntasks and achieves significant average improvement (~10% in metrics). The two-stage approach allows \nthe model to first become specialized on tasks that are not generation tasks in nature before developing \nmore generalized capabilities across tasks in the second stage. \nPerformance of Taiyi on the entire test sets \nThe previous experiment demonstrates our two-stage approach achieves significant improvements on \n\nthe subset of the test sets compared to the simple one-stage training strategy. In this experiment, we \nevaluate the performance of our Taiyi on the entire test sets of the 13 tasks covering 4 BioNLP task \ntypes. The results of ChatGPT 3.5 (GPT-3.5-Turbo) and the state-of-the-art (SOTA) methods based on \nsupervised pretrained language models for each corpus are provided for comparison. \n \nTable 2. Performance comparison with other existing methods on the 13 BioNLP tasks \nTask Datasets Taiyi ChatGPT3.5 SOTA \nNER \n(Micro-F1) \nBC5CDR-Chem (en) 0.802 0.603 [40] 0.933 (PubMedBERT [40]) \nBC5CDR-Dise (en) 0.691 0.518 [40] 0.856 (PubMedBERT [40]) \nCHEMDNER (en) 0.799 0.365 [41] 0.924 (BioBERT [42]) \nNCBI-Disease (en) 0.731 0.505 [40] 0.878 (PubMedBERT [40]) \nCMeEE (zh) 0.657 0.470 [43] 0.740 CBLUE-Leaderboard [28] ) \nRE \n(Micro-F1) \nBC5CDR (en) 0.375 0.142 0.450 (BioGPT [44]) \nCMeIE (zh) 0.432 0.306 [43] 0.549 CBLUE-Leaderboard [28]) \nTC \n(Micro-F1) \nBC7LitCoivd (en) 0.840 0.639 [45] 0.918 (Bioformer [46]) \nHOC (en) 0.800 0.512 [40] 0.823 (PubMedBERT [40]) \nKUAKE_QIC (zh) 0.774 0.485 [43] 0.859 (CBLUE-Leaderboard [28]) \nQA \n(Accuracy) \nPubMedQA (en) 0.544 0.765 [40] 0.558 (PubMedBERT [40]) \nMedQA-USMLE (en) 0.371 0.513 [47] 0.367 (BioBERT-large [39]) \nMedQA-MCMLE (zh) 0.648 0.582 [47] 0.701 (RoBERTA-large [39]) \nALL AVE 0.651 0.493 0.735 \nNote: For the results of ChatGPT3.5 and SOTA methods, we provided previous results on the published papers. Since the published \nresult of ChatGPT 3.5 is not available for the relation extraction on the BC5CDR test set, we obtained the result of the ChatGPT using \nour same prompt via the OpenAI API. For the CMeEE, CMeIE and KUAKE_QIC datasets,  the results of Taiyi are reported on the \ndevelopment set since the gold standard test sets have not been released. Although they may not be di rectly compared, we still list \nthem for reference. \n \nAs shown in Table 2, our Taiyi outperforms ChatGPT3.5 on 11 out of 13 datasets, except for two \nEnglish QA datasets. It shows that the fine-tuned LLM on domain-specific instruction data can improve \nperformance on in-domain downstream tasks. However, the lower English QA results suggest that the \nrich domain knowledge may be learned from the pre-training stage of the LLMs, which is difficult to \ncompensate via instruction fine-tuning. Compared with SOTA pre-trained language models, the Taiyi \nmodel achieves comparable results on the QA tasks. However, the performance of Taiyi still falls by a \nmargin (average ~ 9% in the metrics) for the NER, RE, and TC tasks, where conventional discriminative \nmethods outperform generative methods. \nPerformance of Taiyi on the new task \nTo investigate whether the Taiyi model can be applied to support the new task that is not previously \nseen in our fine-tuning data, we hold out a recently published biomedical corpus BioRED for evaluation. \nBioRED is a biomedical relation extraction dataset with multiple entity types (e.g. gene/protein, disease, \nchemical) and relation pairs (e.g. gene–disease; chemical–chemical) at the document level. Here, we \nfocus on the NER performance of Taiyi on the BioRED test set. \n \nTable 3. Performance of the Taiyi model on the BioRED test set \nEntity Type P R F1 \nChemical 0.717 0.566 0.633 \nDisease 0.829 0.533 0.649 \nGene 0.931 0.490 0.642 \nVariant 0.633 0.585 0.608 \nSpecies 0.640 0.286 0.395 \nCelline 0.213 0.565 0.310 \nAll 0.760 0.507 0.608 \n \nAs shown in Table 3, Taiyi obtains promising results (overall performance of 0.608 in F1-score) on \nthe new BioRED NER task. It is significantly higher than the performance of ChatGPT ( 0.391 in F1-\nscore [41]) on the test set. Among these entity types, Taiyi achieves better performance to chemical, \ndisease, gene, and variant entities, since these entities are seen in other related datasets (e.g., \nBC5CDR, GnormPlus, and tmVar) during the SFT stage. Moreover, it is noteworthy that even for the \nunseen entity types (i.e., species and cell lines)  in the SFT stage, Taiyi still understands the entity \nrecognition instruction and extracts some species and cell line entities. These results suggest that Taiyi \nis highly robust and generalizable. \nCase study \nTo further investigate the benefits of the in -domain instruction fine -tuning, we qualitatively analyzed \nmodel outputs on various BioNLP tasks and compared Taiyi to Qwen7B -Chat, a published LLM fine -\ntuned using general domain instruction data on the same base model Qwen7B. Table 4 shows answers \nproduced by different models with the same question s on the Chinese chat. From the results, Taiyi \ngenerates more detailed and accurate responses to biomedical questions compared to the general \ndomain model Qwen -Chat. More exampl es o f different BioNLP tasks can be found in the \nSupplementary Case study. \n  \nTable 4. Examples of generated text on the Chinese chat task \nInput (1):  \n什么是鼠疫？(What is plague?) \nTaiyi:  \n鼠疫是一种由鼠疫杆菌引起的传染病，主要通过跳蚤等媒介传播。鼠疫杆菌主要存在于啮齿类动物体内，当啮齿类动\n物死亡后，跳蚤等昆虫会吸食其体液，从而将鼠疫杆菌带入人体，引起鼠疫感染。鼠疫的症状包括高热、寒战、淋巴\n结肿大、咳嗽、呼吸急促等，严重时可导致死亡。鼠疫是一种古老的传染病，历史上曾多次引起大规模的流行病，如\n14 世纪的黑死病，对欧洲造成了巨大的人员和经济损失。(Plague is an infectious disease caused by Yersinia pestis \nand is mainly spre ad through fleas and other vectors. Yersinia pestis mainly exists in rodents. When rodents die, \nfleas and other insects will suck their body fluids, thereby introducing Yersinia pestis into the human body, causing \nplague infection. Symptoms of plague inclu de high fever, chills, swollen lymph nodes, cough, shortness of breath, \netc., which can lead to death in severe cases. Plague is an ancient infectious disease that has caused large -scale \nepidemics many times in history, such as the Black Death in the 14th century, which caused huge human and \neconomic losses to Europe.) \nQwen-Chat:  \n鼠疫是一种由鼠疫杆菌引起的急性传染病，它可以在动物之间传播，并且可以通过接触染病动物的排泄物或携带病毒\n的动物及其分泌物传播给人类。该病的症状包括发热、寒战、淋巴结肿大、呼吸困难和出血等。(Plague is an acute \ninfectious disease caused by Yersinia pestis, which can be transmitted between animals and can be transmitted to \nhumans through contact with th e excrement of infected animals or animals carrying the virus and their secretions. \nSymptoms of the disease include fever, chills, swollen lymph nodes, difficulty breathing and bleeding.) \nInput (2):  \n有什么预防方法吗？(Are there any prevention methods?) \nTaiyi:  \n是的，预防鼠疫的方法包括以下几点： \n1. 避免接触感染源：尽量避免接触啮齿类动物，如老鼠、土拨鼠等，不要在它们的巢穴附近逗留。 \n2. 注意个人卫生：保持手部卫生，勤洗手，避免接触有感染风险的物品，如被感染的衣物、被褥等。 \n3. 食用安全：避免食用被感染的动物或其制品，如鼠肉、鼠肉制品等。 \n4. 家居环境：保持家居环境清洁卫生，定期消毒，特别是鼠洞、鼠粪等易滋生细菌的地方。 \n5. 预防接种：对于有鼠疫高发区的人群，可以接种鼠疫疫苗，提高免疫力。 总之，预防鼠疫需要从多个方面入手，保\n持个人卫生、注意饮食安全、保持家居环境清洁卫生等都是预防鼠疫的有效方法。 \n(Yes, ways to prevent plague include the following: \n1. Avoid contact with sources of infection: Try to avoid contact with rodents, such as mice, prairie dogs, etc., and do \nnot stay near their nests. \n2. Pay attention to personal hygie ne: maintain hand hygiene, wash hands frequently, and avoid contact with items \nwith risk of infection, such as infected clothing, bedding, etc. \n3. Food safety: Avoid eating infected animals or their products, such as rat meat and rat meat products. \n4. Home environment: Keep the home environment clean and sanitary, and disinfect it regularly, especially rat holes, \nrat droppings and other places where bacteria can easily breed. \n5. Vaccination: People in areas with a high incidence of plague can be vaccinated against plague to improve their \nimmunity. In short, the prevention of plague requires many aspects. Maintaining personal hygiene, paying attention \nto food safety, and keeping the home environment clean and hygienic are all effective ways to prevent plague.) \nQwen-chat:  \n预防鼠疫的最有效方法是避免接触携带鼠疫杆菌的动物和它们的排泄物，以及不触摸被污染的物品和表面。另外，应\n保持个人卫生，经常洗手，避免进食未煮熟的食物，以及及时接种疫苗来预防鼠疫的发生。(The most effective way to \nprevent plague is to avoid contact with animals that carry Yersinia pestis and their excrement, as well as not touching \ncontaminated objects and surfaces. In addition, you should maintain personal hygiene, wash your hands frequently, \navoid eating undercooked food, and get vaccinated in time to prevent the occurrence of plague.) \nNote that the English in brackets is not part of the input and output; it is the translation of the Chinese. \n \nDISCUSSION \nDue to the complexity of biomedical data and knowledge gaps across tasks , general LLMs often \ngenerate meaningless or completely unrelated  responses to a given  biomedicine input, or the users \nneed to design professional prompts to improve the performance of LLMs on the biomedical tasks. To \nmitigate such issues, we elaborately fine-tuned the LLM on bilingual biomedical data across multiple \nbiomedical tasks. For the LLM optimization to biomedicine, it is crucial to establish a data -centric \ninfrastructure and fine-tuning procedure. In our study, we collected a huge amount of in-domain training \ndatasets, filtered out high-quality subsets and optimized reasonable ratios for each dataset. Including \nthe tasks that are not generation tasks in nature  (e.g., NER and RE), a ll tasks are converted into \ngeneration tasks via instruction templates to train the model. The proposed two-stage SFT strategy is \nshown to effectively improve the model performance.  Owing to these efforts, Taiyi can understand \nbilingual biomedical task instructions and has strong robustness and generalization capabilities on a \nvariety of tasks.  \nHowever, Taiyi still has some common limitatio ns of LLMs, including hallucinations, bias and \nfairness, lack of common sense, and deficient biomedical knowledge. For example, when we input the \nsentence “ Please introduce the octacyclines among antibiotics ”, Taiyi generated the response of \n“Octacyclines are a group of antibiotics that are structurally similar to each other and share a common \nchemical structure. They are characterized by having a 1,4 -dioxane ring in their core structure ……”.  \nIn fact, “Octacyclines” are fictitious and there are no such antibiotics. The hallucinations may potentially \nlead to severe medical malpractice. Moreover, Taiyi achieves better performance on the Chinese QA \ntask but lower scores on the English QA tasks than ChatGPT 3.5 as shown in Table 2. This suggests \nthat the rich biomedical knowledge may be learned from the pre -training stage of the LLMs, which is \ndifficult to compensate via  simple instruction fine -tuning. In practice, it is very challenging to train \ncapable biomedical LLMs from scratch, due to the huge compute consumption and the sensitivity to \ndata quality and training tricks. Therefore, it is useful to develop effective tuning strategies and use \nadditional biomedical resource s to inject specific knowledge . Our future work will focus on these \nproblems, incorporating knowledge resources (e.g., biomedical knowledge database and factual \ninformation obtained by retrieval technology), improving the biomedical interpretability of the model's \noutput, and aligning with human intentions to improve safety in the medical field. \nCONCLUSION  \nIn this study, we collected diverse bilingual (English and Chinese) BioNLP datasets and standardized \ntheir formats. Leveraging these rich training resources and the proposed two-stage supervised fine-\ntuning approach, Taiyi shows considerable capability on various BioNLP tasks. Furthermore, Taiyi \nexhibits cross-lingual generalization across similar task  scenarios while retain ing general domain \nconversational abilities. Overall, utilizing rich high-quality biomedical corpora and designing effective \nfine-tuning strategies can substantially enhance the performance of LLMs within the biomedical domain.  \nOur future work will focus on further enhancing Taiyi's task capabilities, interpretability, and security for \nbiomedical applications. \n \nEthical Considerations \nThis research demonstrates the potential of LLMs in the biomedical domain. The transition from using \nTaiyi LLM for doctor-patient dialogue and medical report generation to practice applications in medical \nservices will require abundant additional research to ensure the safety of this technology. In addition, \nstrict expert evaluation for different medical scenarios must be considered in deployment to realize \nearly diagnosis error discovery. Also, noteworthy aspects including biases and security vulnerabilities \ninherited from foundation models. \n \nFUNDING \nThis research was supported by the National Natural Science Foundation of China (No.  62302076) \nand the Fundamental Research Funds for the Central Universities [No. DUT23RC(3)014]. \n \nAUTHOR CONTRIBUTIONS \nConception and design: LL, ZY, HL. Data collection and processing: LL, YZ, PC, WrF, YQ, DP , JL, HL, \nWdF, ST, YL. Model training and evaluation: JN, ZW, LL, ZD, QH, GX. Analysis and interpretation: LL, \nJN, YZ, YS. Drafting the manuscript: LL, JN, YZ, PC, ZD, WrF, QH. Revising the drafted manuscript: \nLL, JW, YS, ZY, HL. All authors approved the submitted version. \n \nSUPPLEMENTARY MATERIAL \nSupplementary material is available at Journal of the American Medical Informatics Association online. \n \nCONFLICT OF INTEREST STATEMENT \nNone declared. \n \nDATA AVAILABILITY \nThe benchmark datasets that support the findings of this study are available from the official websites \nof natural language processing challenges with Data Use Agreements. The data information and model \nweights of Taiyi are available at https://github.com/DUTIR-BioNLP/Taiyi-LLM. \n \nREFERENCES \n1. OpenAI. OpenAI: Introducing ChatGPT. Secondary OpenAI: Introducing ChatGPT  2022. URL \nhttps://openai.com/blog/chatgpt. \n2. OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 2023  \n3. Chowdhery A, Narang S, Devlin J, et al. Palm: Scaling language modeling with pathways. arXiv preprint \narXiv:2204.02311 2022  \n4. Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models. arXiv preprint \narXiv:2302.13971 2023  \n5. Zeng A, Liu X, Du Z, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 \n2022  \n6. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the \n2019 Conference of the North American Chapt er of the Association for Computational Linguistics: \nHuman Language Technologies, Volume 1 (Long and Short Papers); 2019. \n7. Cui J, Li Z, Yan Y, Chen B, Yuan L. Chatlaw: Open -source legal large language model with integrated \nexternal knowledge bases. arXiv preprint arXiv:2306.16092 2023  \n8. Kasneci E, Seßler K, Küchemann S, et al. ChatGPT for good? On opportunities and challenges of large \nlanguage models for education. Learning and individual differences 2023;103:102274  \n9. Wu S, Irsoy O, Lu S, et al. Bloom berggpt: A large language model for finance. arXiv preprint \narXiv:2303.17564 2023  \n10. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large language models in \nmedicine. Nature medicine 2023;29(8):1930-40  \n11. Tian S, Jin Q, Yeganova L, et al. Opportunities and Challenges for ChatGPT and Large Language Models \nin Biomedicine and Health. arXiv preprint arXiv:2306.10070 2023  \n12. Singhal K, Tu T, Gottweis J, et al. Towards expert-level medical question answering with large language \nmodels. arXiv preprint arXiv:2305.09617 2023  \n13. Bao Z, Chen W, Xiao S, et al. Disc -medllm: Bridging general large language models and real -world \nmedical consultation. arXiv preprint arXiv:2308.14346 2023  \n14. Han T, Adams LC, Papaioannou J -M, et al. MedAlp aca--An Open -Source Collection of Medical \nConversational AI Models and Training Data. arXiv preprint arXiv:2304.08247 2023  \n15. Wang G, Yang G, Du Z, Fan L, Li X. ClinicalGPT: Large Language Models Finetuned with Diverse \nMedical Data and Comprehensive Evaluation. arXiv preprint arXiv:2306.09968 2023  \n16. Wang H, Liu C, Xi N, et al. Huatuo: Tuning llama model with chinese medical knowledge. arXiv preprint \narXiv:2304.06975 2023  \n17. Xiong H, Wang S, Zhu Y, et al. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. arXiv \npreprint arXiv:2304.01097 2023  \n18. Yang S, Zhao H, Zhu S, et al. Zhongjing: Enhancing the chinese medical capabilities of large language \nmodel through expert feedback and real-world multi-turn dialogue. arXiv preprint arXiv:2308.03549 \n2023  \n19. Yunxiang L, Zihan L, Kai Z, Ruilong D, You Z. Chatdoctor: A medical chat model fine -tuned on llama \nmodel using medical domain knowledge. arXiv preprint arXiv:2303.14070 2023  \n20. Zhang H, Chen J, Jiang F, et al. HuatuoGPT, towards Taming Language Model to Be a Doctor. arXiv \npreprint arXiv:2305.15075 2023  \n21. Muennighoff N, Wang T, Sutawika L, et al. Crosslingual generalization through multitask finetuning. arXiv \npreprint arXiv:2211.01786 2022  \n22. Scao TL, Fan A, Akiki C, et al. Bloom: A 176b-parameter open -access multilingual language model. \narXiv preprint arXiv:2211.05100 2022  \n23. Almazrouei E, Alobeidli H, Alshamsi A, et al. Falcon -40B: an open large language model with state -of-\nthe-art performance: Technical report, Technology Innovation Institute, 2023. \n24. Bai J, Bai S, Chu Y, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 2023  \n25. Singhal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge. arXiv preprint \narXiv:2212.13138 2022  \n26. Yuanhe Tian, Ruyi Gan, Yan Song, Jiaxing Zhang, Zhang Y. ChiMed -GPT: A Chinese Medical Large \nLanguage Model with Full Training Regime and Better Alignment to Human P references. arXiv \n2023;arXiv:2311.06025  \n27. Fries J, Weber L, Seelam N, et al. Bigbio: a framework for data -centric biomedical natural language \nprocessing. Advances in Neural Information Processing Systems 2022;35:25792-806  \n28. CBLUE: A Chinese Biomedica l Language Understanding Evaluation Benchmark. Proceedings of the \n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers); \n2022. \n29. Tianxiang Sun, Xiaotian Zhang, Zhengfu He, et al. MOSS: Training Conversational Langua ge Models \nfrom Synthetic Data. Secondary MOSS: Training Conversational Language Models from Synthetic \nData  2023. https://github.com/OpenLMLab/MOSS. \n30. Deduplicating Training Data Makes Language Models Bet ter. Proceedings of the 60th Annual Meeting \nof the Association for Computational Linguistics (Volume 1: Long Papers); 2022. \n31. Longpre S, Yauney G, Reif E, et al. A Pretrainer's Guide to Training Data: Measuring the Effects of Data \nAge, Domain Coverage, Quality, & Toxicity. arXiv preprint arXiv:2305.13169 2023  \n32. Li J, Sun Y, Johnson RJ, Sciaky D, Lu Z. BioCreative V CDR task corpus: a resource for chemical \ndisease relation extraction. Database The Journal of Biological Databases and Curation \n2016;2016:baw068  \n33. Krallinger M, Leitner F, Rabal O, Vazquez M, Oyarzabal J, Valencia A. CHEMDNER: The drugs and \nchemical names extraction challenge. Journal of cheminformatics 2015;7(1):1-11  \n34. Doğan RI, Leaman R, Lu Z. NCBI disease corpus: a resource for disease name recognition and concept \nnormalization. Journal of biomedical informatics 2014;47:1-10  \n35. Luo L, Lai P-T, Wei C-H, Arighi CN, Lu Z. BioRED: a rich biomedical relation extraction dataset. Briefings \nin Bioinformatics 2022;23(5):bbac282  \n36. Chen Q, Allot A, Leaman R, et al. Multi -label classification for biomedical literature: an overview of the \nBioCreative VII LitCovid Track for COVID -19 literature topic annotations. Database \n2022;2022:baac069  \n37. Baker S, Silins I, Guo Y, et al. Automatic semantic  classification of scientific literature according to the \nhallmarks of cancer. Bioinformatics 2016;32(3):432-40  \n38. PubMedQA: A Dataset for Biomedical Research Question Answering. Proceedings of the 2019 \nConference on Empirical Methods in Natural Language  Processing and the 9th International Joint \nConference on Natural Language Processing (EMNLP-IJCNLP); 2019. \n39. Jin D, Pan E, Oufattole N, Weng W -H, Fang H, Szolovits P. What disease does this patient have? a \nlarge-scale open domain question answering data set from medical exams. Applied Sciences \n2021;11(14):6421  \n40. Chen Q, Sun H, Liu H, et al. An extensive benchmark study on biomedical text generation and mining \nwith ChatGPT. Bioinformatics 2023;39(9):btad557  \n41. Zhou W, Zhang S, Gu Y, Chen M, Poon H. Universalner: Targeted distillation from large language models \nfor open named entity recognition. arXiv preprint arXiv:2308.03279 2023  \n42. Lee J, Yoon W, Kim S, et al. BioBERT: a pre -trained biomedical language representation model for \nbiomedical text mining. Bioinformatics 2020;36(4):1234-40  \n43. Zhu W, Wang X, Zheng H, Chen M, Tang B. PromptCBLUE: A Chinese Prompt Tuning Benchmark for \nthe Medical Domain. arXiv preprint arXiv:2310.14151 2023  \n44. Luo R, Sun L, Xia Y, et al. BioGPT: generative pre-trained transformer for biomedical text generation \nand mining. Briefings in Bioinformatics 2022;23(6):bbac409  \n45. Chen Q, Du J, Hu Y, et al. Large language models in biomedical natural language processing: \nbenchmarks, baselines, and recommendations. arXiv preprint arXiv:2305.16326 2023  \n46. Overview of the BioCreative VII LitCovid Track: multi -label topic classification for COVID -19 literature \nannotation. Proceedings of the seventh BioCreative challenge evaluation workshop; 2021.  \n47. Wang Y, Ma X, Chen W. Augmenting  Black-box LLMs with Medical Textbooks for Clinical Question \nAnswering. arXiv preprint arXiv:2309.02233 2023  \n \n  \nTaiyi: A Bilingual Fine-Tuned Large Language Model for Diverse \nBiomedical Tasks \n \nSupplementary Material \n \nTable S1. Summarization of existing LLMs in the biomedical domain \nModel Name Base Language Training method SFT data \nGatorTron [1] Transformer En PT+SFT BioNLP \nMed-PaLM [2]  PaLM En SFT QA \nChatDoctor [3] LLaMA En SFT QA and Chat \nMedAlpaca [4] LLaMA En SFT QA and Chat \nPMC-LLaMA [5] LLaMA En CPT+SFT QA and Chat \nBenTsao [6] ChatGLM Zh SFT QA and Chat \nDoctorGLM [7] ChatGLM Zh SFT QA and Chat \nHuatuoGPT [8] BLOOMZ Zh SFT+RL QA and Chat \nClinicalGPT [9] BLOOM Zh SFT+RL QA and Chat \nDISC-MedLLM[10] Baichuan Zh SFT QA and Chat \nZhongjing [11] Ziya-LLaMA Zh CPT+SFT+RL BioNLP  \nChiMed-GPT [12] Ziya-LLaMA Zh CPT+SFT+RL QA, Chat \nPT denotes pre -training from scratch ; CPT denotes continued pre -training; SFT denotes supervised fine -tuning; RL denotes \nreinforcement learning. \n \n \n \nTable S2. Examples of the instruction data \nTask: NER \nEx1(zh): \nInput:  \n从下面文本中识别出指定的实体类型: \n对儿童 SARST 细胞亚群的研究表明，与成人 SARS 相比，儿童细胞下降不明显，证明上述推测成立。 \n实体类型: 微生物类，疾病，药物，医疗程序，医疗设备，临床表现，科室，身体，医学检验项目 \nOutput: \n疾病：成人 SARS \n临床表现：细胞下降 \n身体：SARST 细胞亚群; 细胞 \nEx2(en): \nInput: \nIdentify Chemical, Disease entities from the text: \"Acute changes of blood ammonia may predict short-term adverse \neffects of valproic acid. Valproic acid ( VPA) was given to 24 epileptic patients who were already being treated with \nother antiepileptic drugs. A standardized loading dose of VPA was administered, and venous blood was sampled at \n0, 1, 2, 3, and 4 hours. Ammonia (NH3) was higher in patients who, d uring continuous therapy, complained of \ndrowsiness (7 patients) than in those who were symptom-free (17 patients), although VPA plasma levels were similar \nin both groups. By measuring VPA-induced changes of blood NH3 content, it may be possible to identify  patients at \nhigher risk of obtundation when VPA is given chronically. \nOutput: \nChemical: valproic acid; Ammonia; NH3; ammonia; VPA; Valproic acid \nDisease: epileptic; drowsiness \nTask: RE \nEx1(zh) \nInput: \n实体关系三元组抽取，以“(头实体, 尾实体, 关系类型)”格式输出： \n13-三体综合征@30%～60%患儿有泌尿系畸形，可见多囊肾、肾盂积水、双肾及双输尿管。关系类型标签：并发症，\n发病率，预后状况，鉴别诊断，同义词，相关（转化），影像学检查，外侵部位，死亡率 \nOutput: \n(13-三体综合征, 泌尿系畸形, 并发症); (13 -三体综合征, 双肾, 并发症); (13 -三体综合征, 多囊肾, 并发症); (13 -三体\n综合征, 双输尿管, 并发症); (13-三体综合征, 肾盂积水, 并发症); (泌尿系畸形, 30%～60%, 发病率) \nEx2(en) \nInput:  \nOutput the chemical-induced disease relations in the following text: \nPhenobarbital-induced dyskinesia in a neurologically -impaired child. A 2 -year-old child with known neurologic \nimpairment developed a dyskinesia soon after starting phenobarbital therapy for seizures. K nown causes of \nmovement disorders were eliminated after evaluation. On repeat challenge with phenobarbital, the dyskinesia \nrecurred. Phenobarbital should be added to the list of anticonvulsant drugs that can cause movement disorders. \nOutput:  \nchemical-induced disease relations: [Phenobarbital, dyskinesia]; [phenobarbital, dyskinesia] \nTask: TC \nEx1(zh) \nInput: \n将下面文本分类到指定的类别中：“ 9. 曾接受过用于治疗癌症的免疫细胞治疗方法，包括CIK、DC、DC-CIK、LAK 细\n胞等；” \n类别标签：伦理审查, 疾病, 吸烟状况, 预期寿命, 依存性, 肿瘤进展, 受体状态, 过敏耐受, 实验室检查, 年龄, 性别, \n教育情况, 研究者决定, 健康群体, 知情同意, 酒精使用, 体征(医生检测）, 口腔相关, 药物, 参与其它试验, 器官组织\n状态, 风险评估, 锻炼, 设备, 护理, 成瘾行为, 读写能力, 性取向, 症状(患者感受), 献血, 病例来源, 数据可及性, 特\n殊病人特征, 怀孕相关, 睡眠, 治疗或手术, 能力, 饮食, 残疾群体, 种族, 含有多类别的语句, 居住情况, 诊断, 疾病\n分期 \nOutput: \n上述文本被分类为：治疗或手术 \nEx2(en) \nInput: \nClassify the following text into the specified text label: \"Influenza activity during the outbreak of coronavirus disease \n2019 in Chinese mainland. Since coronavirus disease 2019 (COVID -19) might circulate in the following seasons, it \nis essential to understand how COVID -19 influences other respiratory diseases, especially influenza. In this study, \nwe analyzed the influenza activity from mid-November 2019 to March 2020 in Chinese mainland and found that the \ninfluenza season ended much earlier than previous seasons for all subtypes and lineages, which may have resulted \nfrom the circulation of COVID-19 and measures such as travel control and personal protection. These findings provide \nrudimentary knowledge of the co-circulation patterns of the two types of viruses.\" \nText Labels: Case Report, Prevention, Transmission, Diagnosis, Mechanism, Treatment, Epidemic Forecasting \nOutput: \nResult: Prevention \nTaks: MT \nEx1(zh) \nInput: \n将下面文本翻译成英语：“死亡的风险比(手术 vs.非手术)为 1.29(95%CI,0.97~1.72;P=0.08),相当于中位总生存期分别\n为 50.6 个月和 64.7 个月. \nOutput: \nThe hazard ratio for death ( surgery vs. no surgery ) was 1.29 ( 95 % confidence interval [ ci ], 0.97 to 1.72 ; p = \n0.08 ), which corresponded to a median overall survival of 50.6 months and 64.7 months, respectively. \nEx2(en) \nInput: \nMachine Translation from English  to Chinese: \"Unfortunately, recurrent disease develops in more than 80 % of \nwomen.\" \nOutput: \n但遗憾的是,有 80%患者的卵巢癌会复发. \n \n \n  \n \nTable S3. Statistics of our evaluation test sets \nTask Data Lan Size Metrics \nNER BC5CDR-chem En 500 abstracts Micro-F1 \nBC5CDR-dise En 500 abstracts \nCHEMDERN En 3,000 abstracts \nNCBI-Disease En 100 abstracts \nBioRED En 100 abstracts \nCMeEE-dev Zh 5,000 sentences \nRE BC5CDR en 500 abstracts Micro-F1 \nCMeIE-dev Zh 3,585 sentences \nTC BC7LitCovid En 6,239 abstracts Micro-F1 \nHOC EN 3,547 sentences \nKUAKE_QIC-dev Zh 1,955 sentences \nQA-mc PubMedQA En 500 questions Accuracy \nMedQA-USMLE En 1,273 questions \nMedQA-MCMLE Zh 3,426 questions \n \n \nExamples of Taiyi output on the supplementary biomedical NLP tasks \nIn addition to the four primary evaluation tasks, the capabilities of the Taiyi model were also shown \nthrough examples (in Table S4)  on several supplementary biomedical NLP tasks , such as Medical \nReport Generation (MRG) aims to automatically summarize information during the diagnostic process. \nThe task requires the language model to generate medical reports in a specified format based on \npatient self -reports and doctor -patient dialogues. Biomedical Event Extraction (EE) is to extract \nstructured biomedical events from unstructured biomedical literature, including mining the described \nbiological entities in the literature, as well as the fine-grained relationships between biomedical events. \nBiomedical Machine Translation (MT)  converts biomedical texts from one natural language to \nanother. This task can assist in translating various medical questions and recommendations, disease \nnames, symptoms, and other information between English and Chinese. Biomedical Title Generation \n(TG) aims to generate informative article titles for input biomedical text paragraphs with summarization. \nBiomedical Text Semantic Similarity (SS) aims to determine the degree of association or similarity \nbetween two medical texts. This task typically involves text matching, similarity measurement, and \nsemantic understanding and can be used in various applications such as medical information retrieval \nand document matching.  Biomedical Question Answering and Chat (CHAT) need to understand \nquestions posed in natural language and provide accurate answers. Different from the simple QA, the \ndialogue task is capable of simulating multi-round natural language conversations.  \n \nTable S4. Example responses generated by Taiyi on the supplementary BioNLP tasks \nTask: Chat (en) \nInput (1):  \nDo you know colchicine? \nTaiyi:  \nYes, Colchicine is a medication used to treat gout, which is a type of arthritis caused by high levels of uric acid in the \nblood. It works by reducing the production of uric acid in the body and also by relieving the pain and inflammation \ncaused by the uric acid crystals in the joints. \nInput (2):  \nDoes it have any potential side effects? \nTaiyi:  \nYes, Colchicine have potential side effects, including gastrointestinal issues such as nausea, vomiting, diarrhea, and \nabdominal pain. It can also cause allergic reactions such as rash, itching, hives, or difficulty breathing. In some cases, \nit can lead to serious complications such as anaphylaxis or kidney damage. Therefore, it is important to monitor \npatients closely while taking Colchicine and report any adverse effects immediately to the healthcare provider. \nTask: Medical Report Generation (zh) \nInput: \n问诊对话历史： \n患者：小孩受凉了，流清鼻涕，咳嗽，应该是风寒咳嗽，去药店买哪种药好呢 \n医生：你好，宝宝咳嗽，流涕比较常见，西医角度上呼吸道感染可能性大，中医上叫做风寒咳嗽，请问宝宝除了咳嗽\n有没有其他不适症状呢？例如发热等，请详细描述一下，我好帮你诊治分析病情 \n患者：精神状态好，也没有发热，就是喉咙有一点痛，咳嗽 \n医生：先帮你分析一下病情，宝宝受凉之后免疫力降低，就会被细菌或病毒侵袭体内，气道分泌物增多，支气管平滑\n肌痉挛，咳嗽，咳痰，咽通。 \n医生：目前没有发热，宝宝病情不重，不用过分紧张的。 \n医生：我帮推荐治疗方法 \n医生：宝宝目前多大了？有没有再医院看过？做过化验检查 \n患者：嗯 \n患者：7 岁，没去医院，做过很多检查，平常就是爱咳嗽，喉哝发炎 \n患者：医生说，扁桃体偏大 \n医生：近期这次有没有去医院看过？做过检查 \n医生：如果宝宝没有其他不适？可以口服氨溴索，桔贝合剂效果好 \n医生：另外如果条件允许，可以做做雾化吸入治疗直接作用与支气管粘膜，效果更直接 \n患者：不用做雾化吧，吃点药就行了 \n医生：也可以先吃药 \n患者：近期没有去过 \n医生：你们这次没有去医院看过？ \n患者：要吃消炎的吗 \n患者：没 \n患者：要吃消炎药吗 \n医生：你好，可以先不吃的 \n患者：那家里有蒲地蓝，可以吃吗 \n患者：口服液 \n患者：喉哝痛要吃吗 \n医生：先治疗看看，可以吃的，假如宝宝出现发热或咳嗽加重，医院就诊，复查血常规和胸片，那个时候再考虑加抗\n生素 \n患者：另外买个止咳的，行吗 \n医生：我们的观点是宝宝小，尽量少吃消炎药，可以先吃那几个药三天看看效果 \n患者：嗯谢谢 \n根据上述对话，给出诊疗报告 \n说明：诊疗报告分为主诉, 现病史, 辅助检查, 既往史, 诊断, 建议这六个章节。 \nTaiyi: \n上述问诊对话的诊疗报告如下： \n主诉：咳嗽咳痰伴鼻流涕。 现病史：患儿无明显诱因下出现咳嗽咳痰伴鼻流涕情况，无恶心呕吐，无其他不适，无\n其他疾病史，无药物过敏史。 辅助检查：无。 既往史：既往体健。 诊断：上呼吸道感染。 建议：口服氨溴索，桔\n贝合剂，若条件允许，雾化吸入治疗。 \nTaks: Event Extraction (zh) \nInput:  \n找出指定的临床发现事件属性： \n因患者需期末考试，故予以口服“雷贝拉唑钠肠溶片”治疗，现腹痛情况明显好转。 \n事件抽取说明：临床发现事件由主体词，发生状态，描述词和解剖部位组成 \nTaiyi:  \n上述句子中的临床发现事件如下： \n主体词：疼痛；发生状态：；描述词：明显好转；解剖部位：腹 \nTaks: Event Extraction (en) \nInput: \nInput text: \"Contaminated drinking water is responsible for causing diarrheal diseases that kill millions of people a \nyear. \nEven Types: Treatment of disease, Cause of disease \nRole Types: Cause, Theme \nPlease extract events from the input text. \nTaiyi: \nCause of disease: (Trigger: responsible, Theme: diarrheal diseases, Cause: Contaminated drinking water] \nTask: Machine Translation (zh-en) \nInput: \n将下面文本翻译成英文： \n光动力疗法（PDT）作为一种新兴的肿瘤治疗手段，因其不良反应较少、靶向性好、可重复治疗等优点，已广泛应用\n于临床多种肿瘤的治疗。相比于手术、化疗及放疗等传统治疗策略，光动力疗法不仅可杀伤原位肿瘤，还可通过激活\n机体的免疫效应对转移瘤发挥抑制作用。然而，PDT 诱导免疫效应的高低受多种因素影响，包括光敏剂在细胞内的定\n位和剂量、光参数、肿瘤内的氧浓度、免疫功能的完整性等。本文针对 PDT 介导抗肿瘤免疫效应的相关机制，以及\nPDT 免疫效应的主要影响因素进行综述，以探讨 PDT 用于肿瘤治疗的未来发展方向。 \nTaiyi: \nAs a new tumor treatment, photodynamic therapy (PDT) has been widely used in clinical practice due to its \nadvantages of fewer side effects, high target specificity, and the possibility of repeated treatment. Compared with \ntraditional surgical, chemotherapy, and rad iotherapy, PDT can not only kill the primary tumor, but also inhibit the \nmetastatic tumor by activating the body's immune effect. However, the high or low of PDT -induced immune effect is \naffected by many factors, including the localization and dosage of ph otosensitizers, light parameters, oxygen \nconcentration in tumor, and the integrity of immune function. This paper reviews the related mechanisms of PDT -\nmediated anti-tumor immune effect and the main influencing factors, and discusses the future development direction \nof PDT in tumor treatment. \nTask: Machine Translation (en-zh) \nInput: \nTranslate the following text into Chinese: \nMetabolic-associated fatty liver disease (MAFLD), formerly known as non -alcoholic fatty liver disease (NAFLD), is \none of the most popular chronic liver diseases in the world. There are many explorations on the epidemiology and \nrisk factors of the disease published in recent years. With the increasing burden of MAFLD on medical care and \nsocial economy, drug treatment of MAFLD has  become an urgent research hotspot. Based on this, the current \nepidemiological status, risk factors, diagnostic criteria and treatment of MAFLD were summarized in this review in \norder to enlighten the clinical practice and scientific research of MAFLD. \nTaiyi: \n代谢综合征相关性脂肪肝（MAFLD）是世界范围内最为流行的慢性肝病之一。近年来，该病的流行病学及危险因素等\n方面的研究颇多。随着MAFLD 在医疗保健及社会经济方面所造成的负担日益加重，该病的药物治疗已成为研究热点。\n本文对该病的流行病学现状、危险因素、诊断标准及治疗等方面进行了综述，以期为临床实践及 MAFLD 的科学研究\n提供参考。 \nTask: Title Generation (zh) \nInput: \n请给下面摘要起标题： \n气管食管瘘是指气管或支气管与食管之间的病理性瘘道，包括气管-食管瘘和支气管-食管瘘，临床以气管-食管瘘较多\n见。气管食管瘘致病原因较多，可引起严重的并发症，是对患者生活质量影响显著、治疗困难和病死率较高的疾病。\n气管食管瘘目前治疗方式较多，但多数疗效欠佳，对新兴治疗手段的需求迫切。胸腹部X 线摄影检出鼻胃管滞留是气\n管食管瘘诊断的金标准，其主要治疗方法包括外科手术治疗、支架置入、局部生物胶水封闭、干细胞治疗等。本文综\n述近年气管食管瘘诊断与治疗的主要研究进展，旨在为该病的临床诊治提供参考。 \nTaiyi: \n气管食管瘘的诊断与治疗进展 \nTask: Title Generation (zh) \nInput: \nOutput a title for the following abstract: \nThe incidence of diabetes mellitus has been increasing, prompting the search for non -invasive diagnostic methods. \nAlthough current methods exist, these have certain limitations, such as low reliability and accuracy, difficulty in \nindividual patient adjustm ent, and discomfort during use. This paper presents a novel approach for diagnosing \ndiabetes using high -frequency ultrasound (HFU) and a convolutional neural network (CNN). This method is based \non the observation that glucose in red blood cells (RBCs) form s glycated hemoglobin (HbA1c) and accumulates on \nits surface. The study incubated RBCs with different glucose concentrations, collected acoustic reflection signals \nfrom them using a custom -designed 90 -MHz transducer, and analyzed the signals using a CNN. T he CNN was \napplied to the frequency spectra and spectrograms of the signal to identify correlations between changes in RBC \nproperties owing to glucose concentration and signal features. The results confirmed the efficacy of the CNN -based \napproach with a classification accuracy of 0.98. This non-invasive diagnostic technology using HFU and CNN holds \npromise for in vivo diagnosis without the need for blood collection. \nTaiyi: \nApplication of high-frequency ultrasound and convolutional neural network in non-invasive diagnosis of diabetes \nTask: Text Classification (zh) \nInput: \n现有以下文本： \n治皮肤病费用大概多少？ \n请将上述文本分类至指定类别中：医疗费用，后果表述，指标解读，病情诊断，就医建议，疾病描述，其他，治疗方\n案，病因分析，功效作用，注意事项。 \nTaiyi: \n上述文本被分类为：医疗费用 \nTask: Text Classification (en) \nInput: \nDocument triage: \"Will my mask from sherwin williams paint store with filters protect me from corona virus along with \npaint fumes?\" \nLabels: patient, doctor \nTaiyi: \nResult: patient \nTask: Text Semantic Similarity (zh) \nInput: \n语句 1：乙肝小二阳会转成小三阳吗？ \n语句 2：乙肝小三阳会不会转成肝硬化、肝癌？ \n请从下面选项中评估这段文本的语义相似度：语义不相同，语义相同 \nTaiyi: \n语义不相同 \nTask: Text Semantic Similarity (en) \nInput: \n1. How can someone's happiness level affect someone's health? \n2. Can staying happy improve my health? What specific steps should I take? \nAssess the semantic similarity of the text pairs based on the following labels: dissimilar, similar \nTaiyi: \nsimilar \n \n \n \nCase study \nTo further investigate the benefits of the in -domain instruction fine -tuning, we qualitatively analyzed \nmodel outputs on various BioNLP tasks and compared Taiyi to Qwen7B -Chat, a published LLM fine -\ntuned using general domain instruction data on the same base model Qwen7B. Table S5 shows some \nexamples.  \n \nTable S5. Some cases of generated text on the BioNLP tasks \nTaks: NER (zh) \nInput:  \n从以下文本中识别出指定的实体类型： \n两肺听诊可闻及干湿性啰音，胸部X 线片两肺有斑片状模糊阴影，尤以双下肺明显，严重病例可合并胸腔积液或脓胸。 \n实体类型：疾病，医疗程序 \nTaiyi:  \n疾病：胸腔积液; 脓胸  \n医疗程序：两肺听诊; 胸部 X 线片 \nQwen-Chat: \n病：肺炎 \n医疗程序：听诊、胸部 X 线检查 \nTask: NER (en) \nInput: \nIdentify Chemical, Disease entities in the following text: \nPhenobarbital-induced dyskinesia in a neurologically -impaired child. A 2 -year-old child with known neurologic \nimpairment developed a dyskinesia soon after starting phenobarbital therapy for seizures. Known causes of \nmovement disorders were eliminated after evaluation. On repeat challenge with phenobarbital, the dyskinesia \nrecurred. Phenobarbital should be added to the list of anticonvulsant drugs that can cause movement disorders. \nTaiyi: \nChemical: Phenobarbital; phenobarbital \nDisease: dyskinesia; neurologically-impaired; neurologic impairment; seizures; movement disorders \nQwen-Chat: \nChemical: Phenobarbital, Anticonvulsant drugs \nDisease entities: Neurological impairment, Seizures, Movement disorders \nTask: RE (zh) \nInput: \n出下面文本中具有药物治疗关系的实体对： \n疟疾感染应当使用氯喹（首选）或羟氯喹治疗感染，共 3 次给药。患者应遵循医生的建议和药物使用说明，以确保药\n物的最大效力。 \nTaiyi: \n具有药物治疗关系的实体对如下：[疟疾感染, 氯喹]; [疟疾感染, 羟氯喹] \nQwen-Chat: \n该句包含以下药物实体对： \n氯喹：药物名称 \n羟氯喹：药物名称 \nTask: RE (en) \nInput: \nExtract the Protein-Protein Interaction from the following text: \nSuch motifs are known to bind to profilins (PFNs), small proteins engaged in the control of actin dynamics. \nTaiyi: \n[actin, PFNs]; [actin, profilins] \nQwen-Chat: \nThe Protein-Protein Interaction mentioned in the given text is between \"motifs\" and \"profilins\" (PFNs). \n \nFrom the results, Taiyi generates more d etailed and accurate responses to biomedical questions \ncompared to the general domain model Qwen-Chat. For the Chinese NER case, Qwen-Chat incorrectly \nextracts entities, while Taiyi precisely identifies disease and medical procedure entities. Similarly, Taiyi \nidentifies more correct chemical entities than Qwen-Chat in the English NER case. In the final case of \nrelation extraction, Taiyi properly extracts complex biomedical relations between proteins, diseases, \nand chemicals, which Qwe n-Chat is unable to capture . It is difficult for the model that has not been \nfine-tuned with domain task instructions to understand the biomedical relation extraction instructions. \nThese examples highlight the advantages of domain -specific instruction fine -tuning for enhancing \nTaiyi's performance on diverse BioNLP tasks compared to the model fine -tuned with general  \ninstructions. More examples on different BioNLP tasks can be found in the Supplementary Material. \n \n \nREFERENCES \n \n1. Yang X, Chen A, PourNejatian N, et al. A large language model for electronic health records. NPJ Digital \nMedicine 2022;5(1):194  \n2. Singhal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge. arXiv preprint \narXiv:2212.13138 2022  \n3. Yunxiang L, Zihan L, Kai Z, Ruilong D, You Z. Chatdoctor: A medical chat model fine -tuned on llama \nmodel using medical domain knowledge. arXiv preprint arXiv:2303.14070 2023  \n4. Han T, Adams LC, Papaioannou J -M, et al. MedAlpaca --An Open -Source Collection of Medical \nConversational AI Models and Training Data. arXiv preprint arXiv:2304.08247 2023  \n5. Wu C, Lin W, Zhang X, Zhang Y, Wan g Y, Xie W. Pmc-llama: Towards building open -source language \nmodels for medicine. arXiv preprint arXiv:2305.10415 2023  \n6. Wang H, Liu C, Xi N, et al. Huatuo: Tuning llama model with chinese medical knowledge. arXiv preprint \narXiv:2304.06975 2023  \n7. Xiong H, Wang S, Zhu Y, et al. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. arXiv \npreprint arXiv:2304.01097 2023  \n8. Zhang H, Chen J, Jiang F, et al. HuatuoGPT, towards Taming Language Model to Be a Doctor. arXiv \npreprint arXiv:2305.15075 2023  \n9. Wang G, Yang G, Du Z, Fan L, Li X. ClinicalGPT: Large Language Models Finetuned with Diverse Medical \nData and Comprehensive Evaluation. arXiv preprint arXiv:2306.09968 2023  \n10. Bao Z, Chen W, Xiao S, et al. Disc -medllm: Bridging general large la nguage models and real -world \nmedical consultation. arXiv preprint arXiv:2308.14346 2023  \n11. Yang S, Zhao H, Zhu S, et al. Zhongjing: Enhancing the chinese medical capabilities of large language \nmodel through expert feedback and real-world multi-turn dialogue. arXiv preprint arXiv:2308.03549 \n2023  \n12. Yuanhe Tian, Ruyi Gan, Yan Song, Jiaxing Zhang, Zhang Y. ChiMed -GPT: A Chinese Medical Large \nLanguage Model with Full Training Regime and Better Alignment to Human Preferences. arXiv \n2023;arXiv:2311.06025  \n \n "
}