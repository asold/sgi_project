{
  "title": "Multi-Modal Learning for AU Detection Based on Multi-Head Fused Transformers",
  "url": "https://openalex.org/W4210405389",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2100623889",
      "name": "Xiang Zhang",
      "affiliations": [
        "Binghamton University"
      ]
    },
    {
      "id": "https://openalex.org/A2104991283",
      "name": "Lijun Yin",
      "affiliations": [
        "Binghamton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2051297709",
    "https://openalex.org/W2964185501",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2789992948",
    "https://openalex.org/W2981677410",
    "https://openalex.org/W3176327573",
    "https://openalex.org/W3097616280",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2034069713",
    "https://openalex.org/W2470957930",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2816936748",
    "https://openalex.org/W6750298367",
    "https://openalex.org/W6640261410",
    "https://openalex.org/W1480583224",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2961770861",
    "https://openalex.org/W2903991757",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3172863135",
    "https://openalex.org/W2619383789",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2767290858",
    "https://openalex.org/W3035574168",
    "https://openalex.org/W2967177252",
    "https://openalex.org/W2964007075",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2904106524",
    "https://openalex.org/W2621864722",
    "https://openalex.org/W1040410175",
    "https://openalex.org/W2421475762",
    "https://openalex.org/W2922226605",
    "https://openalex.org/W6754835258",
    "https://openalex.org/W2589142773",
    "https://openalex.org/W2969059826",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W2963192057",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1938551397",
    "https://openalex.org/W3101614002",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2949868867",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W1595126664",
    "https://openalex.org/W2893915321",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963890275",
    "https://openalex.org/W3092462694"
  ],
  "abstract": "Multi-modal learning has been intensified in recent years, especially for\\napplications in facial analysis and action unit detection whilst there still\\nexist two main challenges in terms of 1) relevant feature learning for\\nrepresentation and 2) efficient fusion for multi-modalities. Recently, there\\nare a number of works have shown the effectiveness in utilizing the attention\\nmechanism for AU detection, however, most of them are binding the region of\\ninterest (ROI) with features but rarely apply attention between features of\\neach AU. On the other hand, the transformer, which utilizes a more efficient\\nself-attention mechanism, has been widely used in natural language processing\\nand computer vision tasks but is not fully explored in AU detection tasks. In\\nthis paper, we propose a novel end-to-end Multi-Head Fused Transformer (MFT)\\nmethod for AU detection, which learns AU encoding features representation from\\ndifferent modalities by transformer encoder and fuses modalities by another\\nfusion transformer module. Multi-head fusion attention is designed in the\\nfusion transformer module for the effective fusion of multiple modalities. Our\\napproach is evaluated on two public multi-modal AU databases, BP4D, and BP4D+,\\nand the results are superior to the state-of-the-art algorithms and baseline\\nmodels. We further analyze the performance of AU detection from different\\nmodalities.\\n",
  "full_text": "Multi-Modal Learning for AU Detection Based on Multi-Head Fused\nTransformers\nXiang Zhang and Lijun Yin\nDepartment of Computer Science, State University of New York at Binghamton, NY , USA\nAbstract— Multi-modal learning has been intensiﬁed in re-\ncent years, especially for applications in facial analysis and\naction unit detection whilst there still exist two main challenges\nin terms of 1) relevant feature learning for representation and\n2) efﬁcient fusion for multi-modalities. Recently, there are a\nnumber of works have shown the effectiveness in utilizing\nthe attention mechanism for AU detection, however, most of\nthem are binding the region of interest (ROI) with features\nbut rarely apply attention between features of each AU. On the\nother hand, the transformer, which utilizes a more efﬁcient self-\nattention mechanism, has been widely used in natural language\nprocessing and computer vision tasks but is not fully explored\nin AU detection tasks. In this paper, we propose a novel end-\nto-end Multi-Head Fused Transformer (MFT) method for AU\ndetection, which learns AU encoding features representation\nfrom different modalities by transformer encoder and fuses\nmodalities by another fusion transformer module. Multi-head\nfusion attention is designed in the fusion transformer module\nfor the effective fusion of multiple modalities. Our approach\nis evaluated on two public multi-modal AU databases, BP4D,\nand BP4D+, and the results are superior to the state-of-the-\nart algorithms and baseline models. We further analyze the\nperformance of AU detection from different modalities.\nI. INTRODUCTION\nFacial action units (AUs), deﬁned by Facial Action Coding\nSystem (FACS) [11], has been widely used for under-\nstanding human emotions. Different from prototypical facial\nexpression recognition (FER), AUs describe facial muscle\nmovements in facial expressions.\nWith the advance of the deep learning networks, deep\nfeatures show much better generalization than traditional\nhand-crafted features like HoG, SIFT, etc. Their successful\napplications on large-scale datasets also beneﬁt the AUs\nstudy. Comparing with other tasks, there exist some practical\nlimitations: (1) accurate AU annotations usually require in-\ntensive human labor and professional experience; (2) provo-\ncation of spontaneous facial expressions is very difﬁcult; (3)\nunbalanced labels problem, e.g., AU6 (Cheek Raiser) is more\nlikely to occur than average but AU2 (Outer Brow Raiser) is\nless.\nBased on the RGB modality, several methods achieve the\nstate-of-the-art performance in AU detection [23], [31], [26],\n[7], [19], [37]. Meanwhile, the attention mechanism has been\nadopted in recent AU detection works. Most of them utilized\nan attention mechanism to ﬁnd the region of interest (ROI) of\neach AU. However, prior knowledge, e.g., facial landmarks\nare required to predeﬁne the attentions of AUs. Therefore, it’s\nmore efﬁcient to utilize a self-attention technique for learn-\ning. Moreover, little work explores the attention among the\nAUs learning features but instead focuses on the ROI-based\nattention. On the other hand, transformer, based on self-\nattention mechanisms, achieved signiﬁcant improvements in\nnot only natural language processing (NLP) [33], [9], [4],\n[28] but also computer vision (CV) tasks [10], [6], [43], [35].\nTherefore, we propose a new transformer-based network\nfor AU encoding to tackle the above-mentioned inefﬁciency\nproblem.\nData and task based multi-modalities fusions take the\nadvantage of the complementary information and increase\nthe robustness [38]. Meanwhile, AU analysis relies on the\nfacial muscle movement, the RGB-only detection methods\nare insufﬁcient for recognizing subtle changes. Thanks to\nrecently developed multi-modal databases [34], [40], [39]\non facial expressions (e.g. RGB, 3D mesh, thermal, and\nphysiology signals in BP4D+ [40]), we can broaden our\nviews from classic RGB to various modalities and from 2D\nimage to 3D space. For instance, AU6 (cheek raiser) involves\nthe deformation of Orbicularis oculi and pars orbitalis\nmuscles, which shows subtle differences when observed in\nvisible images. However, due to the high density of points\naround the eyes region, better geometric changes can be\ncharacterized on 3D mesh. Although some works utilized the\nadditional information presented from multiple modalities in\nFER and AU detection [21], [22], [36], two main challenges\nremain: 1) the model must learn the most relevant features\nfor representation; 2) the model must be effectively when\nfusing two modalities.\nInspired by these vision transformer works [10], [6], and\nmulti-modal transformer works [32], [27], we explore the\nmethods that are utilized transformer architecture for both\nfeature representation and fusion. In this paper, we study\nhow multi-modal data help AUs analysis and present a novel\nMulti-Head Fused Transformer (MFT) for AU detection. Fig.\n11 illustrates the proposed network, which contains two prin-\ncipal points to notice. First, two pipelines in our model, each\npipeline is fed with AU embeddings from each modality.\nSecond, transformer-based fusion modules are applied for the\nfusion of two modalities along the pipelines. We design the\nmulti-head fusion attention module in the fusion transformer\nto fuse modalities effectively. As a result, our model not only\nlearns the relevant AU encoding feature but also effectively\nfuses two modalities. We evaluate our methods on two\npublic datasets in terms of BP4D [39] and BP4D+ [40]. Our\ntransformer based encoder performs better than the state-\nof-the-art methods on RGB images, which demonstrates the\neffectiveness of the AU encoding. Moreover, the results of978-1-6654-3176-7/21/$31.00 ©2021 IEEE\narXiv:2203.11441v1  [cs.CV]  22 Mar 2022\nthe fusion experiments with RGB and Depth images also\nshow our method outperforms the SOTA methods, which\nindicates the fusion transformer module works effectively.\nThe main contribution of this work lies in three-fold:\n1) We present a novel transformer based framework for\nAU detection, which can learn AU encoding features\neffectively.\n2) We design a transformer based fusion module for\nmulti-modalities fusion. To the best of our knowledge,\nthis is the ﬁrst attempt to utilize multi-head fusion\nattention to fuse AU features of two modalities in a\ntransformer.\n3) Extensive experiments demonstrate that our network\noutperforms the state-of-the-art works in AU detection\nby both RGB-only data and RGB-Depth fusion fea-\ntures.\nII. RELATE WORK\nWe review the previous related works on AU detection\nand multi-modal deep learning.\nA. AU Detection\nRecent works on Facial Action Units detection achieved\ncertain improvements by deep learning and attention mech-\nanism. Zhao et al. [42] proposed a local feature learning\napproach that uniformly slices the ﬁrst feature map as a\nregion layer. Li et al. [23] proposed an EAC network,\nwhich predeﬁnes attentions based on AU-related landmarks\nto enhance and crop the ROI of AUs. Zhang et al. [41]\nproposed an adversarial training method to maximize identity\nloss to focus on the AU-related features. Shao et al. [31]\nproposed the JAA-Net, which explores multi-scale region\nlearning and adaptive attention learning. Li et al. [20] gave\ninsight into the AU relation leaning by a Graph Neural\nNetwork (GCN) based model. Niu et al. [26] proposed\na subject independent model to use the facial landmarks\nas person-speciﬁc shape regularization for AU detection.\nThese works with regional attention usually require prior\ninformation like facial landmarks. Recently, Yang et al. [37]\nproposed the SEV-Net that utilized the transformer for Intra-\nAU and Inter-AU attention learning from the AU semantic\ndescriptions. However, the AU semantic descriptions are\nrequired as the prior knowledge. Differ from the ROI-based\nattention, our transformer-based methods exploit the self-\nattention mechanism between AUs, which is no landmarks\nrequirement. Compare with the SEV-Net, our model only\nrequire the AU labels, and no AU semantic descriptions are\nneeded.\nB. Multi-modal Deep learning\nMulti-modal machine learning aims to build models that\ncan process, correlate, and integrate information from mul-\ntiple modalities [3]. The success of deep learning has been\na catalyst to solving increasingly complex machine-learning\nproblems, which often involve multiple data modalities [29].\nMulti-modal deep learning has been succeed used in a wide\nrange of applications, e.g., human activity recognition [1],\n[25], [16], autonomous driving [5], [8], [27], and image\nsegmentation [13], [14].\nIn the past decade, multi-modal learning has been studied\non both facial expression recognition and action unit de-\ntection. Compare with visual images and audio [16], [30]\ncombination, especially dealing with videos, the potential of\ninfrared, thermal, and 3D are not fully explored. Li et al. [21],\n[22] proposed approaches for facial expression recognition\nbased on features fusion of 2D and 3D. Irani et al. [15]\nproposed a method to utilize RGB-Thermal-Depth images\nfor pain estimation. Lakshminarayana et al. [18] explored\nthe combination of physiological signals and RGB images\nto predict action units. Yang et al. [36] proposed a model\ncalled AMF, which included a feature scoring module to\nselect the most relevant feature representations from different\nmodalities.\nTransformer [33], based on self-attention mechanism, was\nﬁrst applied to natural language processing tasks, then this\nframework has been extended to computer vision. For ex-\nample, the ViT model, proposed by Dosovitskiy et al. [10],\napplied a pure transformer to sequences of image patches,\nwhich achieved comparable results to CNNs on image\nclassiﬁcation. Carion et al. [6] proposed the DETR frame-\nwork, which combined a common CNN with a transformer\nencoder-decoder architecture. Recently, researchers have ex-\nplored multi-modal learning by transformers. The most re-\nlated works are called Multimodal Transformer (MulT) [32]\nand Multi-Modal Fusion Transformer (TransFuser) [27], even\nthough they target human multi-modal language analysis\nand multi-modal autonomous driving respectively. In MulT,\npairwise crossmodal transformers are designed, where the\nQ, and KV are from different modalities. Then the outputs\nof pairwise crossmodal transformers are concatenated for fu-\nsion. In the TransFuser work, two modalities fusion occurred\nbefore the self-attention in a transformer, then was split after\nthe self-attention. Differ from these two works, our method\nfuses the features of two modalities by the multi-head fusion\nattention module in a transformer.\nIII. PROPOSED METHOD\nThe proposed Multi-Head Fused Transformer (MFT) is\nsketched in Fig. 1. Each data modality goes through its\nown backbone to extract features and then output the AU\nembeddings from the fully connected (FC) layer. There are\ntwo pipelines, and each contains 4 transformer-based AU\nencoders and a classiﬁer in the end. One pipeline’s input is\nthe fusion feature from a fusion transformer, which is fed\nwith the AU embedding features of fusion features, and the\nother pipeline encode the AU features from a single modality\nembeddings. In the end, two losses from these two pipelines\nare combined together for training.\nA. AU Embeddings\nTwo pre-trained ResNet-50 [12] are employed as backbone\nencoders for RGB and Depth images. The feature out from\nthe fully connected layer is divided by each AU and then fed\ninto an embedding layer for AU embedding. The embedding\nBackbone\nTransformer\nEncoder\nTransformer\nEncoder\nTransformer\nEncoder\nTransformer\nEncoder\nTransformer\nEncoder\nTransformer\nEncoder\nTransformer\nEncoder\nTransformer\nEncoder\nFusion \nTransformer\nFusion \nTransformer\nFusion \nTransformer\nFusion \nTransformer\nClassifier\nBackbone\n+ L\nClassifier\nAU Embedding AU Embedding\nFig. 1. Framework architecture. The proposed framework consists of two pipelines with two modalities as inputs: Xα, Xβ. For example, modalities\nα and β represent RGB and Depth respectively in the ﬁgure. We design two core components in the network: Fusion Transformer (FT) module and\nAU Transformer Encoder(TE). Two modalities are ﬁrst extracted by a backbone network and then split the FC layer for each AU. After obtaining AU\nembeddings of two modalities, they are fused through a FT module to acquire the fusion features F(α,β). Then following 4 TEs in each pipeline, F(α,β)\nand β are encoded. Two features after each encoder of two pipelines are fused again by another FT module. Each pipeline ends with a classiﬁer, and the\nﬁnal losses are combined from both. It is worth noting that α and β can also represent Depth and RGB in turn, which means our fusion framework is\nsensitive to the fusion order.\nlayer in our model is similar to the standard transformer [33].\nSince the AU encoder feature shares the same position with\nAU embedding, there is no need for position embeddings.\nMeanwhile, the stand layer normalization is applied to AU\nembedding as well.\nB. Transformer Encoder\nWe apply the multi-layer transformer encoder to encode\neach AU embedding feature. The Transformer encoder is\nformed by multiple alternating layers, where each layer\nconsists of multiheaded self-attention and wise feed-forward\nnetwork blocks. Layernorm (LN) [2] is utilized before every\nblock and residual connector after every block. To summary,\nthe transformer encoder is deﬁned as:\nXA = LN(MSA(X)) +X (1)\nXB = LN(FFN (XA)) +XA (2)\nwhere X is the input of the Transformer layer, XB is the\noutput of the Transformer layer,LN is Layernorm and MSA\nis multihead self-attention.\nC. Fusion Transformer\nThe architect of Fusion Transformer is shown in Fig. 2.\nTwo modalities α and β are denoted Xα ∈ RT×D and\nXβ ∈ RT×D respectively. T and D are represented as\nAU class numbers and AU embedding feature dimensions,\nrespectively. Inspired by the Multimodal Transformer in [32],\nwe designed a two-heads self-attention for fusion, which is\nthe core component in the fusion transformer. We deﬁne\nthe Querys, Keys, Values as following: Qi\nα = XαWi\nQα,\nKα = XαWKα, Kβ = XβWKβ , Vα = XαWVα, and Vβ =\nXβWVβ , where Wi\nQα ∈RT×D,i ∈{1,2}, WKα ∈RT×D,\nWKβ ∈RT×D, WVα ∈RT×D, WVβ ∈RT×D. Then the\nfusion multi-head self-attention equation f(Xα,Xβ) for α\nand β is deﬁned:\nf(Xα,Xβ) =Att(Q1\nα,Kα,Vα) ⊕Att(Q2\nα,Kβ,Vβ)\n= σ(Q1\nαKT\nα√dk\n)Vα ⊕σ(\nQ2\nαKT\nβ√dk\n)Vβ\n= σ(\nXαW1\nQα(XαWKα)T\n√dk\n)Vα⊕ (3)\nσ(\nXαW2\nQα(XβWKβ )T\n√dk\n)Vβ\nwhere σ is softmax, ⊕ is concatenation, and dk is\ndimension of queries and keys.\nThe other components in the fusion transformer are the\nsame as the standard transformer encoder we mentioned\nabove. Based on the attention of Q2\nαKβVβ, the fusion\ntransformer enables the α modality to receive information\nfrom β modality and then combines them. It is worth noting\nthat equation (3) can be easily extended to more modalities\nQ1 K V Q2 K V\nAttention\nconcat\nLinear\nAdd & Norm\nAdd & Norm\nFeed \nForward\nFusion \nTransformer\nMultihead \nSelf-attention\nAttention\nAU Embeddings α AU Embeddings β  \nAU Fusion Features\nFig. 2. Structure of Fusion Transformer module. The module is based\non a standard 2-heads attention transformer for modality α and modality\nβ. The difference is the attention of β is calculated by Qα, Kβ, Vβ. The\nfusion features are then passed to AU Transformer encoders.\nas following:\nf(X1,X2,...,X m) =Att(Q1\n1,K1,V1) ⊕\nAtt(Q2\n1,K2,V2) ⊕··· (4)\nAtt(Qm\n1 ,Km,Vm)\nwhere m is the number of modalities.\nD. Loss Function\nAs a multi-label task, AU detection faces the common\nproblem of data unbalances. Hence, we apply the weighted\nbinary cross-entropy (BCE) loss functions. The AUk occur-\nrence probability function P(AUk) and the correlate positive\nweight pk of weighted BCE are given as:\nP(AUk) =\n∑N\ni=1 yi\nk∑N\ni=1\n∑C\nk=1 yi\nk\n(5)\npk = P(AUk)\nmin\nx∈{1,...,C}\nP(AUx) (6)\nwhere N is the total number of training set, C is the\nnumber of AU.\nThe weighted BCE loss is deﬁned:\nL= − 1\nN\nN∑\ni=1\nC∑\nk=1\n(pk ×yi\nk ×log( ˆyi\nk)+\n(1 −yi\nk) ×log(1 −ˆyi\nk)) (7)\nwhere yi\nk and ˆyi\nk represent the ground truth label and\nprediction for AUk respectively. There are two classiﬁers\nfor the two pipelines in our model, where two losses are\ncombined. We denote the loss for the ﬁrst pipeline of fusion\nfeature as Lf(α,β) and the second pipeline of β feature as\nLβ, then the ﬁnal loss can be written as:\nL= λ1Lf(α,β) + λ2Lβ (8)\nwhere λ1 and λ2 are two positive regularization parameters.\nIV. EXPERIMENTS\nTo evaluate our method, we examine our framework in AU\ndetection based on single modality and also multi-modalities.\nSingle modality experiments are designed to validate the\nefﬁciencies of the AU encoding by the transformer-based\nencoder. We also make multi-modalities AU detection ex-\nperiments for fusion quality evaluation. We use two public\nmulti-modal databases BP4D [39] and BP4D+ [40] for AU\noccurrence detection.\nA. Data\n1) BP4D: The dataset contains 41 subjects (23 females\nand 18 males) captured under laboratory environments. 8\ntasks are designed to elicit a range of spontaneous emotions.\nThere are 328 = 41 × 8 sequences in total with a frame rate\nof 25. Expert coders select the most expressive 20s of each\nsequence for AU coding. Around 140,000 labeled frames are\nincluded in our experiments and split into 3-fold for a fair\ncomparison with state-of-the-art algorithms. In order to avoid\ntraining and testing on images of the same person, we use the\nsame splits as in [23]. For each frame, the database provides\n3D face meshes as well as corresponding texture images.\n2) BP4D+: There are 140 subjects in BP4D+. On the\nbasis of 8 tasks included in BP4D, two more tasks are\nintroduced with a richer set of spontaneous emotions. Each\nframe is associated with synchronized 3D face mesh, 2D\ntexture, thermal image, and a series of physiological signals.\nSimilar to BP4D, 20 seconds from four tasks were AU\nlabeled for all 140 subjects, resulting in 192,000 labeled\nframes. 12 Aus are selected and the performance of four-\nfold subject-exclusive cross-validation is reported.\nB. Preprocessing\nIn our experiments, we use two modalities in BP4D and\nBP4D+: 2D RGB image and 3D face model. The face regions\nof RGB images are detected and cropped by the publicly\navailable library OpenCV . A raw 3D face model contains a\n30k - 50k number of vertices in BP4D and BP4D+ databases,\nincluding the face, hairs, and neck areas of a subject. We ﬁrst\ncropped the face area of 3D meshes and then project them\ninto depth images. All the face images, including RGB and\nDepth, are resized to 244x244 for training and testing. We\ncalculate the average and standard deviation of each modality\nin one dataset and then apply z-score standardization to each\nimage.\nC. Implementation Details\nBoth backbones in our model are ResNet-50 networks,\nwhich were pretrained on ImageNet [17]. The layer number\nand head number are set 3 and 4 in the transformer encoder,\nrespectively. The AU embedding size is 128, the header size\nis 32, the MLP size is 256, and the dropout rate is 0.5.\nThe fusion transformer module is a one-layer and two-head\ntransformer encoder, and the other parameters are the same\nas the transformer encoder except there’s no dropout. The\nhyper-parameter λ1 and λ2 are selected by comparing the\nbest F1-score in the ﬁrst fold of BP4D, which are 0.6 and\n0.4 respectively. The comparing results is shown in Table.I.\nWe use the stochastic gradient descent (GSD) optimizer with\na momentum of 0.9 and weight decay of 0.0005. The initial\nlearning rate is 0.01 and 10 times smaller every epoch start\nfrom epoch 4. The network is trained for 5 epochs for BP4D\nand 6 epochs for BP4D+. The batch size is set to 32 on both\nBP4d and BP4D+.\nTABLE I\nHYPER -PARAMETERS COMPARISON ON ONE FOLD OF BP4D\nλ1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1.0\nλ2 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.5\nAvg F1 63.1 63.0 63.5 64.1 64.7 63.7 63.7 63.5\nD. Results\n1) single-modal results: Without fusion modules, each\npipeline in our network is workable for single modality\nAU detection. Hence, we ﬁrst compare our method to the\nstate-of-the-art algorithms on a RGB modality in terms of\nF1-score. The methods evaluated on BP4D are including\nEAC [23], DSIN [7], JAA [31], SRERL [19], SEV-Net\n[37]. The upper section of Table.II shows the results on\nBP4D, where our method achieves the highest performance\nin RGB modality. Speciﬁcally, our method outperforms all\nthe SOTA methods in recognizing AU4, AU12, AU15, AU23,\nAU24. Our method achieves 4.2% higher than JAA, which\nused facial landmarks as a joint task for AU detection.\nCompare to SRERL, which requires an AU relation graph\nfrom label distribution, our method still shows 1.3% higher\nperformance. Recently, SEV-Net achieved 63.9% in F1-score\nby including the transformer for Intra-AU and Inter-AU\nattention learning from the semantic descriptions of AUs.\nHowever, we don’t need these semantic descriptions of AUs\nand still achieve 64.2% by increasing 0.3% in F1-score.\nTo evaluate our method on Depth modality, we compare\nwith ResNet-50, as the baseline function, where our method\nimproves the performance in almost all AUs, except the\nAU15, which is 0.1% less. The result is reported in the middle\nsection of Table.II. Our method on Depth performs better\nthan RGB on some AUs, e.g., AU6, AU7, AU10. Moreover,\nit even performs best in all modalities on the following AU:\nAU7, AU12, AU14, AU23.\nOn BP4D+, we compare with following works: EAC [23],\nJAA [31], SEV-Net [37]. Since there’s no report on BP4D+\nin EAC and JAA, we implement these two methods for AU\ndetection on BP4D+. JAA implementation is based on their\npublic source code and EAC is implemented by converting\nthe original Theano framework to PyTorch. The results are\nshown in the upper section of Table.III. Our method also\noutperforms the SOTA methods, which are 1.1%, 1.0%, and\n0.7% higher than EAC, JAA, and SEV-Net, respectively.\nSimilar to BP4D, the result of AU detection on Depth\nmodality is shown in the middle of Table.III, where our\nmethod is 2.9% higher than the baseline. The achieved F1-\nscore at 62.0 is even higher than the other RGB-based SOTA\nmethods, which could be caused by the high quality of the\n3D model in BP4D+. The high performance of our method\nproves its ability for AU encoding.\n2) multi-modal results: Fusion refer to the fusion of\nRGB and Depth in the modal column of Table.II and\nTable.III. Equation (3) shows f(Xrgb,Xdepth) is not equal\nto f(Xdepth,Xrgb), which means our method is sensitive to\nthe order of fusion modalities. Based on various experiments\nresults of evaluating the order of modalities in fusion trans-\nformer module, we report the result of f(Xrgb,Xdepth) on\nTable.II and f(Xdepth,Xrgb) is on Table.III. The differences\nof the fusion modalities order will be discussed in the\nfollowing ablation study subsection.\nLate fusion is currently the most common fusion tech-\nnique, so we use late fusion with ResNet-50 as a baseline.\nThen we compare with these methods on BP4D: MTUT [1],\nTEMT-Net [24], and AMF [36]. The F1-score of MTUT\nand TEMT-Net are reported in the paper of AMF, where the\nauthor implemented them with the fusion of RGB and Depth.\nCut-switch is a data augmentation method in AMF, which is\nindependent of AMF’s deep model. For a fair comparison,\nwe compare AMF with its non-switched version without\napplying such data augmentation. The results on BP4D\nare shown in the bottom section of Table.II. Our method\noutperforms all the related methods and achieves the highest\nF1-score at 64.8%, which is 2.6% higher than latefusion,\n2.6% higher than MTUT, 1.6% higher than TEMT-Net, and\n0.4% higher than AMF. In terms of 12 AUs, our method\nperforms best in AU17 and AU24.\nBesides baseline, we also compare our method to AMF on\nBP4D+ and report the results in the bottom part of Table.III\nOur method also achieves the best performance, showing\n2.3% higher than latefusion and 1.5% higher than AMF.\nMore speciﬁcally, our model achieves the best performances\non AU1, AU4, AU7, AU10, AU17, and AU23.\nAs we can see, our multi-modal method is obviously\noutperformed over the single modal methods on both two\ndatasets. In addition, the high performance over the related\nmulti-modal methods also veriﬁes the effectiveness of our\nmethod.\nTABLE II\nF1 SCORES IN TERMS OF 12 AU S AND MEAN ACCURACY ON BP4D DATABASE . BOLD NUMBERS INDICATE THE BEST PERFORMANCE IN EACH\nMODAL ; BOLD -BRACKETED NUMBERS INDICATE THE BEST IN ALL MODALS\nMethods Modal AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AU24 Avg\nEAC [23] RGB 39.0 35.2 48.6 76.1 72.9 81.9 86.2 58.8 37.5 59.1 35.9 35.8 55.9\nDSIN [7] RGB 51.7 40.4 56.0 76.1 73.5 79.9 85.4 62.7 37.3 62.9 38.8 41.6 58.9\nJAA [31] RGB 47.2 44.0 54.9 77.5 74.6 84.0 86.9 61.9 43.6 60.3 42.7 41.9 60.0\nSRERL [19] RGB 46.9 45.3 55.6 77.1 78.4 83.5 87.6 63.9 52.2 63.9 47.1 53.3 62.9\nSEV-Net [37] RGB [58.2] 50.4 58.3 81.9 73.9 87.8 87.5 61.6 52.6 62.2 44.6 47.6 63.9\nOurs RGB 49.6 48.1 59.9 78.4 78.0 83.7 87.9 62.0 55.3 61.8 50.9 54.9 64.2\nResNet-50 Depth 44.5 41.3 57.2 76.5 72.6 80.9 86.8 54.3 50.2 62.7 48.1 44.5 60.0\nOurs Depth 38.6 37.3 44.2 84.2 [89.0] 89.7 [89.2] [ 79.8] 44.7 46.0 [ 53.2] 37.0 61.1\nLate fusion Fusion 44.3 34.0 41.6 [ 85.2] 87.3 [90.4] 88.9 79.4 46.1 49.6 58.6 38.6 62.0\nMTUT [1] Fusion 51.3 50.2 62.2 77.2 71.7 83.8 88.2 61.4 54.3 57.9 45.8 42.2 62.2\nTEMT-Net [24] Fusion 53.7 47.1 60.5 77.6 75.6 84.8 87.4 67.0 [ 57.2] 61.3 44.7 41.6 63.2\nAMF [36] Fusion 52.1 [ 51.0] [ 64.5] 79.2 73.9 86.4 88.3 60.5 55.3 64.2 47.7 49.2 64.4\nOurs Fusion 51.6 49.2 57.6 78.8 77.5 84.4 87.9 65.0 56.5 [ 64.3] 49.8 [ 55.1] [64.8]\nTABLE III\nF1 SCORES IN TERMS OF 12 AU S AND MEAN ACCURACY ON BP4D+ DATABASE . BOLD NUMBERS INDICATE THE BEST PERFORMANCE IN EACH\nMODAL ; BOLD -BRACKETED NUMBERS INDICATE THE BEST IN ALL MODALS ; * INDICTS THE RESULT FROM OUR OWN IMPLEMENTATION .\nMethods Modal AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AU24 Avg\nEAC [23]* RGB 43.7 39.0 14.0 85.6 87.2 90.5 88.7 [ 88.4] 45.7 49.0 57.3 [ 43.6] 61.1\nJAA [31]* RGB 46.0 41.3 36.0 86.5 88.5 90.5 89.6 81.1 43.4 51.0 56.0 32.6 61.9\nSEV-Net [37] RGB 47.9 40.8 31.2 [ 86.9] 87.5 89.7 88.9 82.6 39.9 [ 55.6] [ 59.4] 27.1 61.5\nOurs RGB 48.4 37.1 34.4 85.6 [ 88.6] [ 90.7] 88.8 81.0 47.6 51.5 55.6 36.9 62.2\nResNet-50 Depth 35.7 33.9 38.9 84.2 86.8 89.7 89.1 77.1 40.7 45.7 54.4 33.1 59.1\nOurs Depth 43.2 40.2 43.3 84.5 88.5 90.2 89.3 79.7 45.8 48.7 53.8 36.8 62.0\nLate fusion Fusion 46.0 41.3 36.0 86.5 88.5 90.5 89.6 81.1 43.4 51.0 56.0 32.6 61.9\nAMF [36] Fusion 45.3 [ 42.5] 34.8 85.9 87.9 89.5 [ 90.4] 82.6 [50.1] 45.5 55.7 42.1 62.7\nOurs Fusion [49.6] 42.0 [ 43.5] 85.8 [ 88.6] 90.6 89.7 80.8 49.8 52.2 59.1 38.4 [64.2]\nTABLE IV\nABLATION STUDY OF EFFECTIVENESS OF KEY COMPONENTS OF OUR MODEL ON BP4D DATABASE . TE REPRESENT TRANSFORMER ENCODER ; FT\nREPRESENT FUSION TRANSFORMER . BOLD NUMBERS INDICATE THE BEST PERFORMANCE .\nMethods AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AU24 Avg\nLate fusion 44.3 34.0 41.6 85.2 87.3 90.4 88.9 79.4 46.1 49.6 58.6 38.6 62.0\nLate fusion + TE 48.0 39.2 37.0 86.1 89.1 90.4 88.7 80.9 46.7 47.8 55.9 39.8 62.5\nResNet-50 + FT 47.8 36.8 39.7 85.4 89.2 90.3 89.0 80.6 46.5 49.6 58.4 40.1 62.8\nResNet-50 + FT + TE (Ours) 51.6 49.2 57.6 78.8 77.5 84.4 87.9 65.0 56.5 64.3 49.8 55.1 64.8\nE. Ablation Study\n1) Effect of individual part for AU detection: For more\nrigorous proof of the validity of our model, we conduct\nexperiments on BP4D dataset to examine the two major in-\ndividual components, transformer encoder, and fusion trans-\nformer. Two important components of our model, namely,\ntransformer encoder (TE), and fusion transformer (FT), are\ntested as we run the same experiments using variations of the\nproposed network, with and without them. Table.IV shows\nthe performance of various combinations of the components\nin terms of F1-score over all the AUs. By using the trans-\nformer encoder to train latefusion features, the performance\nis improved by 0.5%, which provides another piece of\nevidence for the effectiveness of the transformer-based AU\nencoder. By applying the fusion transformer and without the\ntransformer encoder, the performance is 0.8% higher than\nlatefusion, which shows the effectiveness of the FT module.\nThe last row shows the proposed method, with both TE and\nFT, achieves the best performance.\n2) Effect of modal fusion order: As previously discussed,\nour method is affected by the order of modal fusion.\nTwo fusion features, F(R,D) and F(D,R), respectively\nrepresent the output from fusion transformer by applying\nf(Xdepth,Xrgb) and f(Xdepth,Xrgb). We conduct experi-\nments on BP4D and BP4D+ datasets and report the results\nin Table.V. As we can see, F(R,D) shows improved per-\nformance than F(D,R) on BP4D, which is 0.5% higher,\nshowing fusing the projected Depth features in RGB space is\nmore effective. However, on BP4D+, fusion in Depth space\noutperforms than it in RGB space by 0.3%. The different\nquality of 3D data, different number of frames, and label\nimbalance may cause this difference. It is worth noting that\nboth of these fusion features meet or exceed the performance\nof the state-of-the-art methods on each database.\nF . Multi-modal AU Detection Analysis\nBased on our method, we compare the recognition per-\nformance on both BP4D and BP4D+ datasets. The result\non BP4D is shown in Fig.3. Usually, the fusion modality\nTABLE V\nABLATION STUDY OF FUSION MODAL ORDER ON BP4D AND BP4D+\nDATABASE . BOLD NUMBERS INDICATE THE BEST PERFORMANCE ON\nEACH DATABASE .\nBP4D BP4D+\nF(R,D) F(D,R) F(R,D) F(D,R)\nAU1 51.6 50.3 49.3 49.6\nAU2 49.2 48.9 39.7 42.0\nAU4 57.6 58.0 42.8 43.5\nAU6 78.8 79.6 85.4 85.8\nAU7 77.5 77.3 88.9 88.6\nAU10 84.4 84.5 91.0 90.6\nAU12 87.9 88.2 90.0 89.7\nAU14 65.0 62.4 81.8 80.8\nAU15 56.5 54.7 47.7 49.8\nAU17 64.3 61.8 53.0 52.2\nAU23 49.8 51.9 59.2 59.1\nAU24 55.1 53.9 37.5 38.4\nAvg 64.8 64.3 63.9 64.2\nperforms better than the RGB, even when depth performs\nmuch lower. However, the fusion result is clearly lower than\nthe RGB on AU4, with a 2.3% decrease. We propose an\nassumption that the Depth information plays a major role for\nAU4, so the poor property of Depth modality caused perfor-\nmance deduction in AU4. Fig.4 shows the fusion F(R,D)\nalso performs approximately to the RGB, but more to the\nDepth modality on AU4 on BP4D+. In addition, Table.V also\nshows the higher performance of F(D,R) than F(R,D) on\nAU4 of both BP4D and BP4D+. This also proves the depth\ndata provided a more effective representation than the RGB\non AU4.\nFig.3 shows AU6, AU7, AU10, and AU14 have better\nperformances in Depth than in RGB, which shows the\nhigh effectiveness of the AU Depth features. The F(D,R)\noutperforms F(R,D) in AU6, and AU10, but not in AU7 and\nAU14, which need more exploration and discussion in our\nfuture work.\nFig. 3. Performance comparison between different modalities on BP4D.\nR+D represents F(R,D).\nV. CONCLUSIONS AND FUTURE WORKS\nA. Conclusions\nIn this paper, we have presented a novel transformer-\nbased multi-modal network for action unit detection. Without\nFig. 4. Performance comparison between different modalities on BP4D+.\nR+D represents F(R,D).\nrequiring prior knowledge like facial landmarks, AUs relation\ngraphs, and AU semantic description, our framework utilize\nthe transformer encoder and transformer fusion in multi-\nmodal based AU detection. We evaluate our proposed method\non 2 public multi-modalities databases and achieves superior\nperformance over the peer state-of-the-art methods, not only\non fusion modality but also on a single modality. Based\non the performance of our work in different modalities,\nwe further analyze the contribution of two modalities on\ndifferent AUs.\nB. Future Works\nOur future work will include more modalities (e.g., ther-\nmal images, physiology signals) into the fusion strategy.\nBesides 3D Depth images, we also plan to develop more\nadvanced approaches for 3D feature representations. As we\ndiscussed previously that AU4 obtains more information from\nthe Depth images, so we can re-design the fusion module,\nwhere different AU features are following different fusion\norders. Our work is also easy to be extended with the ROI-\nbased attention module, which is widely used recently. The\nproposed framework will also be applied to other practical\naffection-related applications.\nVI. ACKNOWLEDGMENTS\nThe material is based on the work supported in part by the\nNSF under grant CNS-1629898 and the Center of Imaging,\nAcoustics, and Perception Science (CIAPS) of the Research\nFoundation of Binghamton University.\nREFERENCES\n[1] M. Abavisani, H. R. V . Joze, and V . M. Patel. Improving the perfor-\nmance of unimodal dynamic hand-gesture recognition with multimodal\ntraining. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1165–1174, 2019.\n[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv\npreprint arXiv:1607.06450, 2016.\n[3] T. Baltru ˇsaitis, C. Ahuja, and L.-P. Morency. Multimodal machine\nlearning: A survey and taxonomy. IEEE transactions on pattern\nanalysis and machine intelligence , 41:423–443, 2018.\n[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language\nmodels are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[5] H. Caesar, V . Bankiti, A. H. Lang, S. V ora, V . E. Liong, Q. Xu,\nA. Krishnan, Y . Pan, G. Baldan, and O. Beijbom. nuscenes: A\nmultimodal dataset for autonomous driving. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition ,\npages 11621–11631, 2020.\n[6] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko. End-to-end object detection with transformers. In\nEuropean Conference on Computer Vision , pages 213–229, 2020.\n[7] C. Corneanu, M. Madadi, and S. Escalera. Deep structure inference\nnetwork for facial action unit recognition. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , pages 309–324.\nSpringer International Publishing, 2018.\n[8] H. Cui, V . Radosavljevic, F.-C. Chou, T.-H. Lin, T. Nguyen, T.-K.\nHuang, J. Schneider, and N. Djuric. Multimodal trajectory predictions\nfor autonomous driving using deep convolutional networks. In 2019\nInternational Conference on Robotics and Automation (ICRA) , pages\n2090–2096. IEEE, 2019.\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training\nof deep bidirectional transformers for language understanding. In\nNAACL-HLT (1), 2019.\n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition\nat scale. In International Conference on Learning Representations ,\n2020.\n[11] P. Ekman and E. L. Rosenberg, editors. What the face reveals: Basic\nand applied studies of spontaneous expression using the Facial Action\nCoding System (FACS). Oxford University Press, 1997.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , June 2016.\n[13] Y . Hu, M. Modat, E. Gibson, W. Li, N. Ghavami, E. Bonmati,\nG. Wang, S. Bandula, C. M. Moore, M. Emberton, et al. Weakly-\nsupervised convolutional neural networks for multimodal image reg-\nistration. Medical image analysis , 49:1–13, 2018.\n[14] X. Huang, M.-Y . Liu, S. Belongie, and J. Kautz. Multimodal unsu-\npervised image-to-image translation. In Proceedings of the European\nConference on Computer Vision (ECCV) , September 2018.\n[15] R. Irani, K. Nasrollahi, M. O. Simon, C. A. Corneanu, S. Escalera,\nC. Bahnsen, D. H. Lundtoft, T. B. Moeslund, T. L. Pedersen, M.-\nL. Klitgaard, et al. Spatiotemporal analysis of rgb-dt facial images\nfor multimodal pain level recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition Workshops ,\npages 88–95, 2015.\n[16] S. E. Kahou, X. Bouthillier, P. Lamblin, C. Gulcehre, V . Michal-\nski, K. Konda, S. Jean, P. Froumenty, Y . Dauphin, N. Boulanger-\nLewandowski, et al. Emonets: Multimodal deep learning approaches\nfor emotion recognition in video. Journal on Multimodal User\nInterfaces, 10(2):99–111, 2016.\n[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁca-\ntion with deep convolutional neural networks. Advances in neural\ninformation processing systems , 25:1097–1105, 2012.\n[18] N. N. Lakshminarayana, N. Sankaran, S. Setlur, and V . Govindaraju.\nMultimodal deep feature aggregation for facial action unit recognition\nusing visible images and physiological signals. In 2019 14th IEEE\nInternational Conference on Automatic Face & Gesture Recognition\n(FG 2019), pages 1–4. IEEE, 2019.\n[19] G. Li, X. Zhu, Y . Zeng, Q. Wang, and L. Lin. Semantic relationships\nguided representation learning for facial action unit recognition. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol-\nume 33, pages 8594–8601, 2019.\n[20] G. Li, X. Zhu, Y . Zeng, Q. Wang, and L. Lin. Semantic relationships\nguided representation learning for facial action unit recognition. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol-\nume 33, pages 8594–8601, 2019.\n[21] H. Li, H. Ding, D. Huang, Y . Wang, X. Zhao, J.-M. Morvan, and\nL. Chen. An efﬁcient multimodal 2d+ 3d feature-based approach to\nautomatic facial expression recognition. Computer Vision and Image\nUnderstanding, 140:83–92, 2015.\n[22] H. Li, J. Sun, Z. Xu, and L. Chen. Multimodal 2d+ 3d facial expression\nrecognition with deep fusion convolutional neural network. IEEE\nTransactions on Multimedia , 19(12):2816–2831, 2017.\n[23] W. Li, F. Abtahi, Z. Zhu, and L. Yin. EAC-net: A region-based\ndeep enhancing and cropping approach for facial action unit detection.\nIn IEEE International Conference on Automatic Face & Gesture\nRecognition (FG), 2017.\n[24] P. Liu, Z. Zhang, H. Yang, and L. Yin. Multi-modality empowered\nnetwork for facial action unit detection. In 2019 IEEE Winter\nConference on Applications of Computer Vision (WACV), pages 2175–\n2184. IEEE, 2019.\n[25] N. Neverova, C. Wolf, G. Taylor, and F. Nebout. Moddrop: adaptive\nmulti-modal gesture recognition. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 38(8):1692–1706, 2015.\n[26] X. Niu, H. Han, S. Yang, Y . Huang, and S. Shan. Local relationship\nlearning with person-speciﬁc shape regularization for facial action unit\ndetection. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2019.\n[27] A. Prakash, K. Chitta, and A. Geiger. Multi-modal fusion transformer\nfor end-to-end autonomous driving. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 7077–\n7087, 2021.\n[28] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu. Exploring the limits of trans-\nfer learning with a uniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019.\n[29] D. Ramachandram and G. W. Taylor. Deep multimodal learning:\nA survey on recent advances and trends. IEEE signal processing\nmagazine, 34(6):96–108, 2017.\n[30] F. Ringeval, B. Schuller, M. Valstar, N. Cummins, R. Cowie, L. Tavabi,\nM. Schmitt, S. Alisamir, S. Amiriparian, E.-M. Messner, et al. Avec\n2019 workshop and challenge: state-of-mind, detecting depression\nwith ai, and cross-cultural affect recognition. In Proceedings of the\n9th International on Audio/Visual Emotion Challenge and Workshop ,\npages 3–12, 2019.\n[31] Z. Shao, Z. Liu, J. Cai, and L. Ma. Deep adaptive attention for joint\nfacial action unit detection and face alignment. In Proceedings of\nthe European conference on computer vision (ECCV) , pages 705–720,\n2018.\n[32] Y .-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and\nR. Salakhutdinov. Multimodal transformer for unaligned multimodal\nlanguage sequences. In Proceedings of the conference. Association for\nComputational Linguistics. Meeting , volume 2019, page 6558. NIH\nPublic Access, 2019.\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In\nAdvances in neural information processing systems, pages 5998–6008,\n2017.\n[34] S. Wang, Z. Liu, Z. Wang, G. Wu, P. Shen, S. He, and X. Wang.\nAnalyses of a multimodal spontaneous facial expression database.\nIEEE Transactions on Affective Computing , 4(1):34–46, 2012.\n[35] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo. Learning texture\ntransformer network for image super-resolution. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 5791–5800, 2020.\n[36] H. Yang, T. Wang, and L. Yin. Adaptive multimodal fusion for facial\naction units recognition. In Proceedings of the 28th ACM International\nConference on Multimedia , pages 2982–2990, 2020.\n[37] H. Yang, L. Yin, Y . Zhou, and J. Gu. Exploiting semantic embedding\nand visual feature for facial action unit detection. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 10482–10491, 2021.\n[38] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese.\nTaskonomy: Disentangling task transfer learning. InProceedings of the\nIEEE conference on computer vision and pattern recognition , pages\n3712–3722, 2018.\n[39] X. Zhang, L. Yin, J. F. Cohn, S. Canavan, M. Reale, A. Horowitz,\nP. Liu, and J. M. Girard. Bp4d-spontaneous: a high-resolution\nspontaneous 3d dynamic facial expression database. Image and Vision\nComputing, 32(10):692–706, 2014.\n[40] Z. Zhang, J. M. Girard, Y . Wu, X. Zhang, L. Yin, et al. Multi-\nmodal spontaneous emotion corpus for human behavior analysis. In\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 3438–3446, 2016.\n[41] Z. Zhang, S. Zhai, L. Yin, et al. Identity-based adversarial training\nof deep cnns for facial action unit recognition. In BMVC, page 226.\nNewcastle, 2018.\n[42] K. Zhao, W.-S. Chu, and H. Zhang. Deep region and multi-label\nlearning for facial action unit detection. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 3391–\n3399, 2016.\n[43] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. Deformable\ndetr: Deformable transformers for end-to-end object detection. arXiv\npreprint arXiv:2010.04159, 2020.",
  "topic": "Modalities",
  "concepts": [
    {
      "name": "Modalities",
      "score": 0.7622218728065491
    },
    {
      "name": "Computer science",
      "score": 0.7553809881210327
    },
    {
      "name": "Transformer",
      "score": 0.7226604223251343
    },
    {
      "name": "Encoder",
      "score": 0.6335112452507019
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5979864597320557
    },
    {
      "name": "Feature learning",
      "score": 0.4456065893173218
    },
    {
      "name": "Modal",
      "score": 0.4356154501438141
    },
    {
      "name": "Machine learning",
      "score": 0.42674127221107483
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37052005529403687
    },
    {
      "name": "Engineering",
      "score": 0.1471005082130432
    },
    {
      "name": "Voltage",
      "score": 0.07333594560623169
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I123946342",
      "name": "Binghamton University",
      "country": "US"
    }
  ],
  "cited_by": 10
}