{
  "title": "Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning",
  "url": "https://openalex.org/W3191452491",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5039350434",
      "name": "Faisal Alamri",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5008386240",
      "name": "Anjan Dutta",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3030163527",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W2123024445",
    "https://openalex.org/W2949865211",
    "https://openalex.org/W652269744",
    "https://openalex.org/W2171061940",
    "https://openalex.org/W2982407353",
    "https://openalex.org/W1858576077",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W2962677366",
    "https://openalex.org/W93016980",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2963283377",
    "https://openalex.org/W2994929655",
    "https://openalex.org/W3071109023",
    "https://openalex.org/W2963499153",
    "https://openalex.org/W2771620762",
    "https://openalex.org/W2924476266",
    "https://openalex.org/W3163461448",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2952073179",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2070148066",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2910453440",
    "https://openalex.org/W2951313160",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2134270519",
    "https://openalex.org/W2949823873",
    "https://openalex.org/W2949503252",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2963960318",
    "https://openalex.org/W3100093508"
  ],
  "abstract": "Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are not observed during the training phase. The existing body of works on ZSL mostly relies on pretrained visual features and lacks the explicit attribute localisation mechanism on images. In this work, we propose an attention-based model in the problem settings of ZSL to learn attributes useful for unseen class recognition. Our method uses an attention mechanism adapted from Vision Transformer to capture and learn discriminative attributes by splitting images into small patches. We conduct experiments on three popular ZSL benchmarks (i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results {on all the three datasets}, which illustrate the effectiveness of our proposed method.",
  "full_text": "Multi-Head Self-Attention via\nVision Transformer for Zero-Shot Learning\nFaisal Alamri and Anjan Dutta\nDepartment of Computer Science, University of Exeter, United Kingdom\nAbstract\nZero-Shot Learning (ZSL) aims to recognise unseen object classes, which are not observed during the\ntraining phase. The existing body of works on ZSL mostly relies on pretrained visual features and lacks\nthe explicit attribute localisation mechanism on images. In this work, we propose an attention-based model\nin the problem settings of ZSL to learn attributes useful for unseen class recognition. Our method uses\nan attention mechanism adapted from Vision Transformer to capture and learn discriminative attributes by\nsplitting images into small patches. We conduct experiments on three popular ZSL benchmarks (i.e., AW A2,\nCUB and SUN) and set new state-of-the-art harmonic mean results on all the three datasets, which illustrate\nthe effectiveness of our proposed method.\nKeywords: Generalised zero-shot learning, Inductive learning, Attention, Semantic embedding, Vision\nTransformer.\n1 Introduction\nViT-ZSL\nBlack eye\nRed throat\nRed breast\nMulti-coloured back\nGrey leg\nSmall size...\nImage Semantic Space\nFigure 1: Our method embeds each attribute-\nbased feature with the semantic space. It learns\nthe visual discriminative features through multi-\nhead attention. Best to view in colour: colours in\nthe image correspond to the same-colour attribute\nin the semantic space.\nRelying on massive annotated datasets, signiﬁcant progress\nhas been made on many visual recognition tasks, which is\nmainly due to the widespread use of different deep learning\narchitectures [Ren et al., 2015, Dosovitskiy et al., 2021,\nKhan et al., 2021]. Despite these advancements, recognis-\ning any arbitrary real-world object still remains a daunt-\ning challenge as it is unrealistic to label all the ex-\nisting object classes on the earth. Zero-Shot Learn-\ning (ZSL) addresses this problem, requiring images from\nthe seen classes during the training, but has the ca-\npability of recognising unseen classes during the infer-\nence [Xian et al., 2019a, Xie et al., 2019, Xu et al., 2020,\nFederici et al., 2020]. Here the central insight is that all the\nexisting categories share a common semantic space and the\ntask of ZSL is to learn a mapping from the imagery space to the semantic space with the help of side informa-\ntion (attributes, word embeddings) [Xian et al., 2017, Mikolov et al., 2013, Pennington et al., 2014] available\nwith the seen classes during the training phase so that it can be used to predict the class information for the\nunseen classes during the inference time.\nMost of the existing ZSL methods [Xian et al., 2018, Schönfeld et al., 2019] depends on pretrained vi-\nsual features and necessarily focus on learning a compatibility function between the visual features and se-\nmantic attributes. Although modern neural network models encode local visual information and object parts\n[Xie et al., 2019], they are not sufﬁcient to solve the localisation issue in ZSL models. Some attempts have also\narXiv:2108.00045v1  [cs.CV]  30 Jul 2021\nbeen made by learning visual attention that focuses on some object parts [Zhu et al., 2019]. However, designing\na model that can exploit a stronger attention mechanism is relatively unexplored.\nTherefore, to alleviate the above shortcomings of visual representations in ZSL models, in this paper, we\npropose a Vision Transformer (ViT) [Dosovitskiy et al., 2021] based multi-head self-attention model for solv-\ning the ZSL task. Our main contribution is to introduce ViT for enhancing the visual feature localisation to\nsolve the zero-shot learning task. Without any object part-level annotation or detection, this is the ﬁrst attempt\nto introduce ViT into ZSL. As illustrated in Figure 1, our method maps the visual features of images to the\nsemantic space with the help of scaled dot-product of multi-head attention employed in ViT. We have also per-\nformed detailed experimentation on three public datasets (i.e., AW A2, CUB and SUN) following Generalised\nZero-Shot Learning (GZSL) setting and achieved very encouraging results on all of them, including the new\nstate-of-the-art harmonic mean on all the datasets.\n2 Related Work\nZero-Shot Learning: ZSL is employed to bridge the gap between seen and unseen classes using semantic\ninformation, which is done by computing similarity function between visual features and previously learned\nknowledge [Romera-Paredes and Torr, 2015]. Various approaches address the ZSL problem by learning prob-\nabilistic attribute classiﬁers to predict class labels [Lampert et al., 2009, Norouzi et al., 2014] and by learning\nlinear [Frome et al., 2013, Akata et al., 2015, Akata et al., 2016], and non-linear [Xian et al., 2016] compatibil-\nity function associating image features and semantic information. Recently proposed generative models syn-\nthesise visual features for the unseen classes [Xian et al., 2018, Schönfeld et al., 2019]. Although those models\nachieve better performances compared to classical models, they rely on features of trained CNNs. Recently,\nattention mechanism is adapted in ZSL to integrate discriminative local and global visual features. Among\nthem, S2GA [Yu et al., 2018] and AREN [Xie et al., 2019] use an attention-based network with two branches\nto guide the visual features to generate discriminative regions of objects. SGMA [Zhu et al., 2019] also applies\nattention to jointly learn global and local features from the whole image and multiple discovered object parts.\nVery recently, APN [Xu et al., 2020] proposes to divide an object into eight groups and learns a set of attribute\nprototypes, which further help the model to decorrelate the visual features. Partly inspired by the success of\nattention-based models, in this paper, we propose to learn local and global features using multi-scaled-dot-\nproduct self-attention via the Vision Transformer model, which to the best of our knowledge, is the ﬁrst work\non ZSL involving Vision Transformer. In this model, we employ multi-head attention after splitting the image\ninto ﬁxed-size patches so that it can attend to each patch to capture discriminative features among them and\ngenerate a compact representation of the entire image.\nVision Transformer: Self-attention-based architectures, especially Transformer [Vaswani et al., 2017] has\nshown major success for various Natural Language Processing (NLP) [Brown et al., 2020] as well as for Com-\nputer Vision tasks [Alamri et al., 2021, Dosovitskiy et al., 2021]; the reader is referred to [Khan et al., 2021]\nfor further reading on Vision Transformer based literature. Speciﬁcally, CaiT [Touvron et al., 2021] intro-\nduces deeper transformer networks, and Swin Transformer [Liu et al., 2021] proposes a hierarchical Trans-\nformer, where the representation is computed using self-attention via shifted windows. In addition, TNT\n[Han et al., 2021] proposes transformer-backbone method modelling not only the patch-level features but also\nthe pixel-level representations. CrossViT [Chen et al., 2021] shows how dual-branch Transformer combining\ndifferent sized image patches produce stronger image features. Since the applicability of transformer-based\nmodels is growing, we aim to expand and judge its capability for GZSL tasks; to the best of our knowledge,\nthis is still unexplored. Therefore, different from the existing works, we employ ViT to map the visual in-\nformation to the semantic space, beneﬁting from the great performance of multi-head self-attention to learn\nclass-level attributes.\nTransformer Encoder \nLinear Projection\n*\nPosition\nEmbedding\nAttributes\nMSE Loss\na1a2a3a4…aM\nCosine Similarity \nCompute the similarity \nbetween predicted Attributes \n(a) and the Ground-Truth \nAttributes Label\nMHA\nNorm\nNorm\nMLP\nEmbeddedpatches\nFigure 2: ViT-ZSL Architecture. An image is split into small patches fed into the Transformer encoder after\nattaching positional embeddings. During the training the output of the encoder is compared with the semantic\ninformation of the corresponding image via MSE loss. At inference the encoder output is used to search for the\nnearest class label.\n3 Vision Transformer for Zero-shot Learning (ViT-ZSL)\nWe follow the inductive approach for training our model, i.e. during training, the model only has access to the\nimages and corresponding image/object attributes from the seen classes S= {x,y|x ∈X ,y ∈Ys}, where\nx is an RGB image and y is the class-level attribute vector annotated with M different attributes, as provided\nwith the dataset. As depicted in Figure 2, a 224 ×224 image x ∈RH×W×C with resolution H ×W and C\nchannels is fed into the model. The model follows ViT [Dosovitskiy et al., 2021] as closely as possible; hence\nthe image is divided into a sequence of N patches denoted as xp ∈RN×(P2.C), where N = H.W\nP2 . Each patch\nwith a resolution of P×P is encoded into a patch embedding by a trainable 2D convolution layer (i.e., Conv2d\nwith kernel size=(16, 16) and stride=(16, 16)). Position embeddings are then attached to the patch embeddings\nto preserve the relative positional information of the order of the sequence due to the lack of recurrence in\nthe Transformer. An extra learnable classiﬁcation token ( z0\n0 = xclass) is appended at the beginning of the\nsequence to encode the global image representation. Patch embeddings ( z) are then projected thought a linear\nprojection E to D dimension (i.e., D = 1024) as in Eq. 1. Embeddings are then passed to the Transformer\nencoder, which consists of Multi-Head Attention (MHA) (Eq. 2) and MLP blocks (Eq. 3). Before every block,\na layer normalisation (Norm) is employed, and residual connections are also applied after every block. Image\nrepresentation (ˆ y) is produced as in Eq. 4.\nz0 = [xclass; x1\npE; x2\npE; x3\npE; ... ; xN\np E] +Epos, E ∈R(P2.C)×D,Epos ∈R(N+1)×D (1)\nz′\nℓ = MHA(Norm(zℓ−1)) +zℓ−1, ℓ = 1...L (L= 24) (2)\nzℓ = MLP(Norm(z′\nℓ)) +z′\nℓ, ℓ = 1...L (3)\nˆ y= Norm(z0\nL) (4)\nIn terms of MHA, self-attention is performed for every patch in the sequence of the patch embeddings\nindependently; thus, attention works simultaneously for all the patches, leading to multi-head self-attention.\nThree vectors, namely Query (Q), Key (K) and Value (V), are created by multiplying the encoder’s input (i.e.,\npatch embeddings) by three weight matrices (i.e., WQ, WK and WV ) trained during the training process to\ncompute the self-attention. The Qand Kvectors undergo a dot-product to output a scoring matrix representing\nhow much a patch embedding has to attend to every other embedding; the higher the score is, the more attention\nis considered. The score matrix is then scaled down and passed into a softmax to convert the scores into\nprobabilities, which are then multiplied by the V vectors, as in Eq. 5, where dk is the dimension of the K\nvectors. Since the multi-attention mechanism is employed, self-attention matrices are then concatenated and\nfed into a linear layer and passed to the regression head.\nAttention(Q, K, V) = softmax(QKT\n√dk\n)V (5)\nWe argue that self-attention allows our model to attend to image regions that can be semantically relevant for\nclassiﬁcation and learns the visual features across the entire image. Since the standard ViT has one classiﬁcation\nhead implemented by an MLP, it has been edited to meet our model objective: to predictMnumber of attributes\n(i.e., depending on the datasets used). The motivation behind this is that the network is assumed to learn the\nnotion of classes to predict attributes. For the objective function, we employed the Mean Squared Error (MSE)\nloss, as the continuous attributes are used as in Eq. 6, whereyi is the observed attributes, andˆ yi is the predicted\nones.\nLMSE = 1\nM\nM∑\ni=1\n(yi −ˆ yi)2 (6)\nDuring testing, instead of applying the extensively used dot product as in [Xu et al., 2020], we consider the\ncosine similarity as in [Gidaris and Komodakis, 2018] to predict class labels. The cosine similarity between the\npredicted attributes and every class embedding is measured. The output of the similarity measure is then used\nto determine the class label of the test images.\n4 Experiments\nImplementation Details:All images used in training and testing are adapted from the ZSL datasets mentioned\nbelow and sized 224 ×224 without any data augmentation. We employ the Large variant of ViT (ViT-L)\n[Dosovitskiy et al., 2021], with input patch size 16 ×16, 1024 hidden dimension, 24 layers, 16 heads on each\nlayer, and 24 series encoder. There are 307M parameters in total in this architecture. ViT-L is then ﬁne-tuned\nusing Adam optimiser with a ﬁxed learning rate of 10−4 and a batch size of 64. All methods are implemented\nin PyTorch1 on an NVIDIA RTX 3090 GPU, Xeon processor, and a memory sized 32GB.\nDatasets: We have conducted our experiments on three popular ZSL datasets: AW A2, CUB, and SUN, whose\ndetails are presented in Table 1. The main aim of this experimentation is to validate our proposed method, ViT-\nZSL, demonstrate its effectiveness and compare it with the existing state-of-the-arts. Among these datasets,\nAW A2 [Xian et al., 2017] consists of37,322 images of 50 categories (40 seen + 10 unseen). Each category\ncontains 85 binary as well as continuous class attributes. CUB [Wah et al., 2011] contains 11,788 images\nforming 200 different types of birds, among them 150 classes are considered as seen, and the other 50 as\nunseen, which is split by [Akata et al., 2016]. Together with images CUB dataset also contains 312 attributes\ndescribing birds. Finally, SUN [Patterson and Hays, 2012] has the largest number of classes among others. It\nconsists of 717 types of scene, divided into 645 seen and 72 unseen classes. The SUN dataset contains 14,340\nimages with 102 annotated attributes.\nTable 1: Dataset statistics in terms of granularity, number of classes (seen + unseen classes) as shown within\nparenthesis, number of attributes and number of images.\nDatasets Granularity # Classes (S + U) # Attributes # Images\nAW A2 [Xian et al., 2017] coarse 50 (40 + 10) 85 37,322\nCUB [Wah et al., 2011] ﬁne 200 (150 + 50) 102 11,788\nSUN [Patterson and Hays, 2012] ﬁne 717 (645 + 72) 312 14,340\nEvaluation: In this work, we train our ViT-ZSL model following the inductive approach [Wang et al., 2019].\nFollowing [Xian et al., 2019a], we measure the top-1 accuracy for both seen as well as unseen classes. To\n1Our code is available at: https://github.com/FaisalAlamri0/ViT-ZSL\ncapture the trade-off between both sets of classes performance, we use the harmonic mean, which is the primary\nevaluation criterion for our model. Following the recent papers (e.g., [Xu et al., 2020], [Chao et al., 2016]), we\napply Calibrated Stacking [Chao et al., 2016] to evaluate the considered methods under GZSL setting, where\nthe calibration factor γis dataset dependant and decided based on a validation set.\nQuantitative Results: We consider the AW A2, CUB and SUN datasets to show the performance of our pro-\nposed model and compare the performance with related arts. Table 2 shows the quantitative comparison be-\ntween the proposed model and various other GZSL models. The performance of each model is shown in terms\nof Seen (S) and Unseen (U) classes and their harmonic mean (H).\nTable 2: Generalised zero-shot classiﬁcation performance on AW A2, CUB and SUN\nModels AW A2 CUB SUN\nS U H S U H S U H\nDAP [Lampert et al., 2009] 84.7 0.0 0.0 67.9 1.7 3.3 25.1 4.2 7.2\nIAP [Lampert et al., 2009] 87.6 0.9 1.8 72.8 0.2 0.4 37.8 1.0 1.8\nDeViSE [Frome et al., 2013] 74.7 17.1 27.8 53.0 23.8 32.8 30.5 14.7 19.8\nConSE [Norouzi et al., 2014] 90.6 0.5 1.0 72.2 1.6 3.1 39.9 6.8 11.6\nSSE [Zhang and Saligrama, 2015] 82.5 8.1 14.8 46.9 8.5 14.4 36.4 2.1 4.0\nSJE [Akata et al., 2015] 73.9 8.0 14.4 59.2 23.5 33.6 30.5 14.7 19.8\nESZSL [Romera-Paredes and Torr, 2015] 77.8 5.9 11.0 63.8 12.6 21.0 27.9 11.0 15.8\nLATEM [Xian et al., 2016] 77.3 11.5 20.0 57.3 15.2 24.0 28.8 14.7 19.5\nALE [Akata et al., 2016] 81.8 14.0 23.9 62.8 23.7 34.4 33.1 21.8 26.3\nSAE [Kodirov et al., 2017] 82.2 1.1 2.2 54.0 7.8 13.6 18.0 8.8 11.8\nAREN [Xie et al., 2019] 92.9 15.6 26.7 78.7 38.9 52.1 38.8 19.0 25.5\nSGMA [Zhu et al., 2019] 87.1 37.6 52.5 71.3 36.7 48.5 - - -\nAPN [Xu et al., 2020] 78.0 56.5 65.5 69.3 65.3 67.2 34.0 41.1 37.6\n*GAZSL [Zhu et al., 2018] 86.5 19.2 31.4 60.6 23.9 34.3 34.5 21.7 26.7\n*f-CLSWGAN [Xian et al., 2018] 64.4 57.9 59.6 57.7 43.7 49.7 36.6 42.6 39.4\nOur model (ViT-ZSL) 90.0 51.9 65.8 75.2 67.3 71.0 55.3 44.5 49.3\nS, U, H denote Seen classes ( Ys), Unseen classes ( Yu), and the Harmonic mean, respectively. For each scenario, the\nbest is in red and the second-best is in blue. * indicates generative representation learning methods.\nDAP and IAP [Lampert et al., 2009] are some of the earliest works in ZSL, which perform poorly compared\nto other models. This is due to the assumptions claimed in these approaches regarding attributes dependency.\nIn real-world animals with attributes ‘terrestrial’ and ‘farm’ are dependent but are assumed independent by\nsuch models, which are noted as incorrect by [Akata et al., 2016]. Our model ViT-ZSL does not assume this,\nbut rather it considers the correlation between attributes, which self-attention helps to achieve by considering\nboth positional and contextual information of the entire sequence of patches. DeViSE [Frome et al., 2013]\nand ConSE [Norouzi et al., 2014] learn a linear mapping between images and their semantic embedding space.\nThey both make use of the same text model trained on 5.4B words from Wikipedia to construct 500-dimensional\nword embedding vectors. Both use the same baseline model, but DeViSE replaces the last layer (i.e., softmax\nlayer) with a linear transformation layer. In contrast, ConSE keeps it and computes the predictions via a\nconvex combination of the class label embedding vectors. ConSE, as presented in Table 2 outperforms DeViSE,\nbut DeViSE is generally performing better on the unseen classes. Similarly, SJE [Akata et al., 2015] learns\na bilinear compatibility function using the structural SVM objective function to maximise the compatibility\nbetween image and class embeddings. ESZSL [Romera-Paredes and Torr, 2015] uses the square loss to learn\nbilinear compatibility. Although ESZSL is claimed to be easy to implement, its performance, in particular\nfor GZSL, is poor. ALE [Akata et al., 2016], which belongs to the bilinear compatibility approach group,\nperforms better than most of its group member. LATEM [Xian et al., 2016], instead of learning a single bilinear\nmap, extends the bilinear compatibility of SJE [Akata et al., 2015] as to be an image-class pairwise linear by\nlearning multiple linear mappings. It performs better than SJE on unseen classes but with a lower harmonic\nFigure 3: Representative examples of attention. First row: Original images, Middle: Attention maps, and\nlast: Attention fusions. From left to right, ViT-ZSL is able to focus on object-level attributes and learn objects\ndiscriminative features when objects are partly captured (ﬁrst three columns images), occluded (fourth column\nimages) or fully presented (last two columns images).\nmean due to its poor performance on seen classes. Generative ZSL models such as GAZSL [Zhu et al., 2018],\nand f-CLSWGAN [Xian et al., 2018] are seen to reduce the effect of the bias problem due to the inclusion of\nsynthesised features for the unseen classes. However, this does not apply to our method, as no synthesised\nfeatures are used in our case; instead, solely the features extracted from seen classes are used during training.\nAREN [Xie et al., 2019], SGMA [Zhu et al., 2019] and APN [Xu et al., 2020] are non-generative ZSL models\nfocusing on object region localisation using image attention. They are the most relevant works to ours as\nattention mechanism is included in these models architecture. However, they consist of two branches in their\nmodels, where the ﬁrst learns local discriminative visual features and the second captures the image’s global\ncontext. In contrast, our model uses only one compact network, where the input is the image patches so that\nthe global and local discriminative features can be learned using the multi-head self-attention mechanism.\nOur model ViT-ZSL, as shown in Table 2, achieves the best harmonic mean on AW A2. It also performs\nas the third best on both seen and unseen classes. Compared with the other models, it scores 90.02%, where\nthe highest is the highest is AREN with 92.9% accuracy. As the comparison illustrated follows the GZSL\nsetting using the harmonic mean as the primary evaluation metric for GZSL models, ViT-ZSL outperforms all\nstate-of-the-art models. In terms of the CUB dataset, our method achieves the second-highest accuracy for seen\nclasses, but the highest for unseen. In addition, our ViT-ZSL obtains the best harmonic mean score among all\nthe reported approaches. On SUN datasets, which has the most signiﬁcant number of object classes among\nother datasets, our model performs as the best for both seen and unseen classes, achieving a harmonic mean of\n47.9%, the highest compared to all other models.\nAttention Maps: In Figure 3, we show how our model attends to image regions semantically relevant to the\nobject class. For example, in the images of the ﬁrst three columns, the entire objects’ shapes are absent (i.e.,\nonly the top part is captured), and in the image in the fourth column, the groove-billed ani bird is impeded by a\nhuman hand. Although these images suffer from occlusion, our model accurately attends to the objects in the\nimage. Thus, we believe that ViT-ZSL deﬁnitely beneﬁts from the attention mechanism, which is also involved\nin the human recognition system. Clearly, we can say that our method has learned to map the relevance of\nlocal regions to representations in the semantic space, where it makes predictions on the visible attribute-based\nregions. Similarly, in the last two columns images of Figure 3, it can be noticed how the model pays more\nattention to some object-level attributes (i.e., Deer: forest, agility, furry etc., and Vermilion Flycatcher: solid\nand red breast, perching-like shape, notched tail). It can also be noticed that the model focuses on the context\nof the object, as in the second column images. This can be due to the guidance of some attributes (i.e., forest,\njungle, ground and tree) which are associated with leopard class. However, as shown in the ﬁrst column, the\nmodel did not pay much attention to the bird’s beak compared to the head and the rest of the body, which\nneeds to be investigated further and building an explainable model as in [Xian et al., 2019b] could be a way to\naccomplish this.\n5 Conclusion\nIn this paper, we proposed a Vision Transformer-based Zero-Shot Learning (ViT-ZSL) model that speciﬁcally\nexploits the multi-head self-attention mechanism for relating visual and semantic attributes. Our qualitative\nresults showed that the attention mechanism involved in our model focuses on the most relevant image regions\nrelated to the object class to predict the semantic information, which is used to ﬁnd out the class label during\ninference. Our results on the GZSL task, including the highest harmonic mean scores on the AW A2, CUB and\nSUN datasets, illustrate the effectiveness of our proposed method.\nAlthough our method achieves very encouraging results for the GZSL task on three publicly available\nbenchmarks, the bias problem towards seen classes remains a challenge, which will be addressed in future\nwork. Training the model in a transductive setting, where visual information for unseen classes could be\nincluded, is a direction to be examined.\nAcknowledgement\nThis work was supported by the Defence Science and Technology Laboratory and the Alan Turing Institute.\nThe TITAN Xp and TITAN V used for this research were donated by the NVIDIA Corporation.\nReferences\n[Akata et al., 2016] Akata, Z., Perronnin, F., Harchaoui, Z., and Schmid, C. (2016). Label-embedding for\nimage classiﬁcation. IEEE TPAMI.\n[Akata et al., 2015] Akata, Z., Reed, S. E., Walter, D., Lee, H., and Schiele, B. (2015). Evaluation of output\nembeddings for ﬁne-grained image classiﬁcation. In CVPR.\n[Alamri et al., 2021] Alamri, F., Kalkan, S., and Pugeault, N. (2021). Transformer-encoder detector module:\nUsing context to improve robustness to adversarial attacks on object detection. In ICPR.\n[Brown et al., 2020] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan,\nA., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. In NeurIPS.\n[Chao et al., 2016] Chao, W.-L., Changpinyo, S., Gong, B., and Sha, F. (2016). An empirical study and analysis\nof generalized zero-shot learning for object recognition in the wild. In ECCV.\n[Chen et al., 2021] Chen, C.-F., Fan, Q., and Panda, R. (2021). Crossvit: Cross-attention multi-scale vision\ntransformer for image classiﬁcation. arXiv.\n[Dosovitskiy et al., 2021] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An image is\nworth 16x16 words: Transformers for image recognition at scale. In ICLR.\n[Federici et al., 2020] Federici, M., Dutta, A., Forré, P., Kushman, N., and Akata, Z. (2020). Learning Robust\nRepresentations via Multi-View Information Bottleneck. In ICLR.\n[Frome et al., 2013] Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Ranzato, M. A., and Mikolov,\nT. (2013). Devise: A deep visual-semantic embedding model. In NIPS.\n[Gidaris and Komodakis, 2018] Gidaris, S. and Komodakis, N. (2018). Dynamic few-shot visual learning\nwithout forgetting. In CVPR.\n[Han et al., 2021] Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y . (2021). Transformer in transformer.\narXiv.\n[Khan et al., 2021] Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F., and Shah, M. (2021). Transform-\ners in vision: A survey. arXiv.\n[Kodirov et al., 2017] Kodirov, E., Xiang, T., and Gong, S. (2017). Semantic autoencoder for zero-shot learn-\ning. In CVPR.\n[Lampert et al., 2009] Lampert, C. H., Nickisch, H., and Harmeling, S. (2009). Learning to detect unseen\nobject classes by between-class attribute transfer. In CVPR.\n[Liu et al., 2021] Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., and Guo, B. (2021). Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv.\n[Mikolov et al., 2013] Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013). Distributed\nrepresentations of words and phrases and their compositionality. In NIPS.\n[Norouzi et al., 2014] Norouzi, M., Mikolov, T., Bengio, S., Singer, Y ., Shlens, J., Frome, A., Corrado, G., and\nDean, J. (2014). Zero-shot learning by convex combination of semantic embeddings. In ICLR.\n[Patterson and Hays, 2012] Patterson, G. and Hays, J. (2012). Sun attribute database: Discovering, annotating,\nand recognizing scene attributes. In CVPR.\n[Pennington et al., 2014] Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for\nword representation. In EMNLP.\n[Ren et al., 2015] Ren, S., He, K., Girshick, R. B., and Sun, J. (2015). Faster r-cnn: Towards real-time object\ndetection with region proposal networks. IEEE TPAMI.\n[Romera-Paredes and Torr, 2015] Romera-Paredes, B. and Torr, P. (2015). An embarrassingly simple approach\nto zero-shot learning. In ICML.\n[Schönfeld et al., 2019] Schönfeld, E., Ebrahimi, S., Sinha, S., Darrell, T., and Akata, Z. (2019). Generalized\nzero- and few-shot learning via aligned variational autoencoders. CVPR.\n[Touvron et al., 2021] Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., and Jégou, H. (2021). Going\ndeeper with image transformers. arXiv.\n[Vaswani et al., 2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L.,\nand Polosukhin, I. (2017). Attention is all you need. In NIPS.\n[Wah et al., 2011] Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S. (2011). The Caltech-UCSD\nBirds-200-2011 Dataset. Technical report, California Institute of Technology.\n[Wang et al., 2019] Wang, W., Zheng, V ., Yu, H., and Miao, C. (2019). A survey of zero-shot learning.ACM-\nTIST.\n[Xian et al., 2016] Xian, Y ., Akata, Z., Sharma, G., Nguyen, Q., Hein, M., and Schiele, B. (2016). Latent\nembeddings for zero-shot classiﬁcation. In CVPR.\n[Xian et al., 2019a] Xian, Y ., Lampert, C. H., Schiele, B., and Akata, Z. (2019a). Zero-shot learning—a com-\nprehensive evaluation of the good, the bad and the ugly. IEEE TPAMI.\n[Xian et al., 2018] Xian, Y ., Lorenz, T., Schiele, B., and Akata, Z. (2018). Feature generating networks for\nzero-shot learning. In CVPR.\n[Xian et al., 2017] Xian, Y ., Schiele, B., and Akata, Z. (2017). Zero-shot learning - the good, the bad and the\nugly. In CVPR.\n[Xian et al., 2019b] Xian, Y ., Sharma, S., Schiele, B., and Akata, Z. (2019b). F-vaegan-d2: A feature generat-\ning framework for any-shot learning. In CVPR.\n[Xie et al., 2019] Xie, G.-S., Liu, L., Jin, X., Zhu, F., Zhang, Z., Qin, J., Yao, Y ., and Shao, L. (2019). Attentive\nregion embedding network for zero-shot learning. In CVPR.\n[Xu et al., 2020] Xu, W., Xian, Y ., Wang, J., Schiele, B., and Akata, Z. (2020). Attribute prototype network\nfor zero-shot learning. In NIPS.\n[Yu et al., 2018] Yu, y., Ji, Z., Fu, Y ., Guo, J., Pang, Y ., and Zhang, Z. M. (2018). Stacked semantics-guided\nattention model for ﬁne-grained zero-shot learning. In NeurIPS.\n[Zhang and Saligrama, 2015] Zhang, Z. and Saligrama, V . (2015). Zero-shot learning via semantic similarity\nembedding. In ICCV.\n[Zhu et al., 2018] Zhu, Y ., Elhoseiny, M., Liu, B., Peng, X., and Elgammal, A. (2018). Imagine it for me:\nGenerative adversarial approach for zero-shot learning from noisy texts. In CVPR.\n[Zhu et al., 2019] Zhu, Y ., Xie, J., Tang, Z., Peng, X., and Elgammal, A. (2019). Semantic-guided multi-\nattention localization for zero-shot learning. In NIPS.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5871753692626953
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.49027732014656067
    },
    {
      "name": "Computer science",
      "score": 0.39359942078590393
    },
    {
      "name": "Computer vision",
      "score": 0.3650529682636261
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35244834423065186
    },
    {
      "name": "Psychology",
      "score": 0.328704297542572
    },
    {
      "name": "Electrical engineering",
      "score": 0.20897755026817322
    },
    {
      "name": "Engineering",
      "score": 0.15900111198425293
    },
    {
      "name": "Philosophy",
      "score": 0.06382119655609131
    },
    {
      "name": "Voltage",
      "score": 0.05839294195175171
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": []
}