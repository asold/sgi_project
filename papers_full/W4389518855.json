{
    "title": "Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT - A Text-to-SQL Parsing Comparison",
    "url": "https://openalex.org/W4389518855",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2103745815",
            "name": "Shuo Sun",
            "affiliations": [
                "Institute for Infocomm Research"
            ]
        },
        {
            "id": "https://openalex.org/A2114693050",
            "name": "Yuchen Zhang",
            "affiliations": [
                "Institute for Infocomm Research"
            ]
        },
        {
            "id": "https://openalex.org/A2336136161",
            "name": "Jiahuan Yan",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2306722714",
            "name": "Yuze Gao",
            "affiliations": [
                "Institute for Infocomm Research"
            ]
        },
        {
            "id": "https://openalex.org/A3035121478",
            "name": "Donovan Ong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2095826315",
            "name": "Bin Chen",
            "affiliations": [
                "Institute for Infocomm Research"
            ]
        },
        {
            "id": "https://openalex.org/A2098157489",
            "name": "Jian Su",
            "affiliations": [
                "Institute for Infocomm Research"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3198002980",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4378509449",
        "https://openalex.org/W4378505284",
        "https://openalex.org/W2964271186",
        "https://openalex.org/W4382202531",
        "https://openalex.org/W4366999624",
        "https://openalex.org/W4385572217",
        "https://openalex.org/W3154151289",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W2269738476",
        "https://openalex.org/W2890431379",
        "https://openalex.org/W2963477458",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4385572312",
        "https://openalex.org/W3104055253",
        "https://openalex.org/W4379087610",
        "https://openalex.org/W4226053975",
        "https://openalex.org/W2163274265",
        "https://openalex.org/W4377372223",
        "https://openalex.org/W4362655426",
        "https://openalex.org/W3035172316",
        "https://openalex.org/W4385570661",
        "https://openalex.org/W4285107714",
        "https://openalex.org/W4385570031",
        "https://openalex.org/W3102020135",
        "https://openalex.org/W3153094109",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4361019453",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2594782638",
        "https://openalex.org/W4389524197",
        "https://openalex.org/W4288089799"
    ],
    "abstract": "The success of ChatGPT has ignited an AI race, with researchers striving to develop new large language models (LLMs) that can match or surpass the language understanding and generation abilities of commercial ones. In recent times, a number of models have emerged, claiming performance near that of GPT-3.5 or GPT-4 through various instruction-tuning methods. As practitioners of Text-to-SQL parsing, we are grateful for their valuable contributions to open-source research. However, it is important to approach these claims with a sense of scrutiny and ascertain the actual effectiveness of these models. Therefore, we pit six popular large language models against each other, systematically evaluating their Text-to-SQL parsing capability on nine benchmark datasets with five different prompting strategies, covering both zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell significantly short of the performance achieved by closed-source models like GPT-3.5, highlighting the need for further work to bridge the performance gap between these models.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11225–11238\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBattle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs\nGuanaco vs Bard vs ChatGPT - A Text-to-SQL Parsing Comparison\nShuo Sun1, Yuchen Zhang1,2, Jiahuan Yan3, Yuze Gao1, Donovan Ong∗,4, Bin Chen1, Jian Su1\n1Institute for Infocomm Research (I2R), A*STAR, Singapore\n2CNRS@CREATE LTD, Singapore,3National University of Singapore\n4Nanyang Technology University\n1{Sun_Shuo,Zhang_Yuchen,Gao_Yuze,Donovan_Ong,bchen,sujian}@i2r.a-star.edu.sg,\n3jiahuan_22@u.nus.edu, 4s21006@e.ntu.edu.sg\nAbstract\nThe success of ChatGPT has ignited an AI race,\nwith researchers striving to develop new large\nlanguage models (LLMs) that can match or sur-\npass the language understanding and generation\nabilities of commercial ones. In recent times,\na number of models have emerged, claiming\nperformance near that of GPT-3.5 or GPT-4\nthrough various instruction-tuning methods. As\npractitioners of Text-to-SQL parsing, we are\ngrateful for their valuable contributions to open-\nsource research. However, it is important to\napproach these claims with a sense of scrutiny\nand ascertain the actual effectiveness of these\nmodels. Therefore, we pit six popular large\nlanguage models against each other, system-\natically evaluating their Text-to-SQL parsing\ncapability on nine benchmark datasets with five\ndifferent prompting strategies, covering both\nzero-shot and few-shot scenarios. Regrettably,\nthe open-sourced models fell significantly short\nof the performance achieved by closed-source\nmodels like GPT-3.5, highlighting the need for\nfurther work to bridge the performance gap be-\ntween these models.\n1 Introduction\nText-to-SQL parsing automatically converts user\ninput questions into SQL statements, enabling the\nretrieval of relevant information from databases.\nBy enabling users to express their objectives in\nnatural language, Text-to-SQL systems could min-\nimize the technical obstacles for non-expert users\nto interact with relational databases and enhance\nproductivity.\nThe introduction of large pre-trained language\nmodels like BERT (Devlin et al., 2019) and T5\n(Raffel et al., 2020) has further improved the per-\nformance of Text-to-SQL systems. Researchers\nhave been leveraging the impressive understanding\n∗Work done when Donovan was working at I2R\nof these models to push the boundaries of Text-to-\nSQL capabilities.\nRecently, breakthroughs in decoder-based large\nlanguage models (Brown et al., 2020b; Touvron\net al., 2023) have further revolutionized the field of\nNLP. A prominent trend is the pursuit of training in-\ncreasingly larger language models, encompassing\nbillions of parameters, and utilizing vast amounts\nof textual data. Subsequently, these models are\nfine-tuned using instruction-based techniques, en-\nabling them to better adhere to human-generated\ntext prompts.\nAmong the prominent applications of decoder\nLLMs is ChatGPT, which is built upon OpenAI’s\nGPT-3.5 and GPT-4 models. ChatGPT has demon-\nstrated exceptional capabilities in zero-shot and\nfew-shot scenarios, as evidenced by various Text-\nto-SQL evaluation studies (Rajkumar et al., 2022;\nLiu et al., 2023). Regrettably, the success of Chat-\nGPT has ignited an AI race, leading industry re-\nsearch labs to cease public disclosure of their model\nparameters and training methodologies.\nTherefore, researchers have been actively pur-\nsuing the development of new language models\nthat can potentially rival the capabilities of Chat-\nGPT. These models include Dolly, which builds\nupon the Pythia models (Biderman et al., 2023),\nas well as Vicuna (Chiang et al., 2023) and Gua-\nnaco (Dettmers et al., 2023), based on the LLaMA\nmodels (Touvron et al., 2023). Some of them have\ngarnered attention by claiming to achieve perfor-\nmance levels surpassing 90% of GPT-4 through\nfine-tuning techniques.\nAs practitioners of Text-to-SQL, we are grate-\nful for the contributions made by these models.\nHowever, we remain uncertain about whether these\nopen-source models truly deliver the level of qual-\nity they claim to achieve. To address this concern,\nthis paper presents a comprehensive evaluation of\n11225\nsix language models: Dolly, LLaMA, Vicuna, Gua-\nnaco, Bard, and ChatGPT, directly comparing their\nperformance on nine benchmark datasets, utilizing\nfive distinct prompting strategies.\nOur main findings are:\n1. Open-source models demonstrate notably in-\nferior performance compared to closed-\nsource models across a majority of Text-to-\nSQL datasets.\n2. While LLMs demonstrate proficiency in gen-\nerating syntactically valid SQL statements,\nthey often struggle to produce semantically\naccurate queries.\n3. LLMs prove to be highly sensitive to the ex-\namples utilized for few-shot learning.\nTo facilitate further research in the field of\nText-to-SQL parsing, we are making all raw\nand post-processed outputs from the large lan-\nguage models publicly at https://github.com/\nZhangYuchenYC/deepeval-Text2SQL.\n2 Experiment Setup\n2.1 The Large Language Model Contestants\nWe have carefully selected contestants from five\nfamilies of large language models to provide a com-\nprehensive representation of the current landscape\nin the field:\nDolly1 is a 12 billion parameter language model,\nclaimed to be the first publicly available instruction-\ntuned LLM licensed for both academic and com-\nmercial applications. It is based on Pythia (Bider-\nman et al., 2023) and has undergone fine-tuning\nusing an instruction dataset created by Databricks\nemployees. Additionally, we have experimented\nwith smaller variants of Dolly, including the 3B\nand 7B versions.\nLLaMA (Touvron et al., 2023) is a collection\nof large language models ranging from 7 billion\nto 65 billion parameters. These models have been\ntrained exclusively on publicly available text cor-\npora. Unlike the other models, LLaMA models are\nnot instruction fine-tuned. To ensure the natural\ncontinuation of prompts, we append the keyword\n\"SELECT\" at the end of the prompts when inter-\nacting with LLaMA models.\nVicuna (Chiang et al., 2023) is a 13 billion pa-\nrameter LLaMA model fine-tuned on user-shared\n1https://github.com/databrickslabs/dolly\nconversations collected from ShareGPT 2, which\nclaims to achieve 90% ChatGPT quality based on\nan automated evaluation with GPT-4. We also ex-\nperiment with the 7B version.\nGuanaco (Dettmers et al., 2023) is a family\nof large language models claiming to achieve\n99.3% of ChatGPT performance with only 24 hours\nof fine-tuning on one GPU. Similar to Vicuna,\nthe Guanaco model was instruction-tuned on the\nLLaMA models. We conduct our evaluations on\nthe 33B version.\nBard3 is a conversational chatbot released by\nGoogle as an answer to OpenAI’s chatGPT. It is ini-\ntially powered by LaMDA (Thoppilan et al., 2022)\nand later transitioned to PaLM 2 (Anil et al., 2023).\nAs the technical details of Bard are not publicly\nreleased, we evaluate its performance on Text-to-\nSQL datasets as a black box model. We denote the\nversion powered by LaMDA as Bard-L and the\nversion powered by PaLM 2 as Bard-P2.\nGPT-3.5(Brown et al., 2020a) is OpenAI’s most\ncost-effective large language model optimized for\nchat-based applications at the point of writing this\npaper. It has 175 billion parameters and powers\nthe popular ChatGPT chatbot. We conduct all eval-\nuations on the “GPT-3.5-turbo-0301” variant of\nGPT-3.5 through openAI’s API.\n2.2 Prompting Strategies\nWe explore five commonly used prompting strate-\ngies:\nThe Informal Schema (IS)strategy provides a\ndescription of tables and their associated columns\nin natural language. In this approach, the schema\ninformation is expressed in a less formal manner. In\ncontrast, the API Docs (AD)strategy, as outlined in\nthe evaluation conducted by Rajkumar et al. (2022),\nfollows the default SQL translate prompt provided\nin OpenAI’s documentation4. This prompt adheres\nto a slightly more formal definition of the database\nschema. The Select 3strategy includes three ex-\nample rows for each table in the database. This\nadditional information aims to provide concrete\nexamples of the data contained within each ta-\nble, supplementing the schema description. We\nalso investigate the effectiveness of 1-Shot Learn-\ning (1SL)and 5-Shot Learning (5SL)strategies,\nwhere we provide one and five golden examples\n2https://sharegpt.com/\n3https://bard.google.com/\n4https://platform.openai.com/examples/\ndefault-sql-translate\n11226\nin the prompts, respectively. Examples of various\nprompting strategies can be found in the appendix.\n2.3 Benchmark Datasets\nDataset Number of examples\nAcademic 196\nATIS 347\nGeoQuery 182\nYelp 128\nIMDB 131\nRestaurants 378\nScholar 315\nAdvising 1832\nSpider 1034\nTable 1: Number of examples used for evaluation of\nvarious Text-to-SQL datasets.\nWe evaluate the prompting strategies on nine\nText-to-SQL datasets: Academic (Li and Jagadish,\n2014), ATIS (Price, 1990; Dahl et al., 1994), Geo-\nQuery (Zelle and Mooney, 1996), Yelpand IMDB\n(Yaghmazadeh et al., 2017), Restaurants (Tang\nand Mooney, 2000; Popescu et al., 2004), Scholar\n(Iyer et al., 2017), Advising (Finegan-Dollak et al.,\n2018) and Spider (Yu et al., 2018). Note that for\nthe first eight datasets, we employ the standardized\nand improved versions released by Finegan-Dollak\net al. (2018).\nFollowing Zhong et al. (2020), we evaluate the\nperformance of large language models on the test\nsets of the datasets if such sets were defined. In\ncases where test sets were not provided, we evalu-\nated the models on the entire datasets. For the Spi-\nder dataset, we evaluate the models on the develop-\nment set since the test set is not publicly available\nat the point of writing this paper. To maintain con-\nsistency with Zhong et al. (2020), we collectively\nrefer to the first eight datasets as the “classical\ndatasets”.\nImportant details on data splitsClassical Text-\nto-SQL datasets such as GeoQuery, ATIS and\nScholar often utilize a question-based data split\nstrategy where matching (text, SQL) pairs are as-\nsigned to the same split. However, one potential\nissue with this split strategy is that the same SQL\nquery could appear in both the training and test-\ning set. This duplication of SQL queries across\nthe training and testing sets can introduce a bias\nand potentially inflate the model’s performance.\nThe model may inadvertently learn to memorize\nthe specific SQL queries rather than understanding\nthe underlying semantic parsing. Therefore, we\nemploy the query-based data splitsdescribed in\nFinegan-Dollak et al. (2018) where SQL queries\nwith similar structures are assigned to the same\nsplits. As a result, some of our reported results\nare lower than those documented in previous work\nsuch as Rajkumar et al. (2022).\nWe also want to highlight that unlike Suhr et al.\n(2020); Lan et al. (2023) who use the filtered combi-\nnations of train and dev splits of GeoQuery, Scholar\nand Advising and the filtered dev split of ATIS as\ntheir evaluations sets, we adhere to the evaluation\nsplits in Finegan-Dollak et al. (2018). The reason\nbehind this decision was our intention to sample\nexamples from the train sets for the purpose of\nconducting one-shot and five-shot prompting ex-\nperiments.\n2.4 Evaluation Metrics\nThe primary evaluation metric employed in this\npaper is the execution accuracy (EX), which mea-\nsures the percentage of generated SQL queries that\nprecisely align with the outputs of the gold SQL\nqueries. Additionally, for the Spider dataset, we\nalso calculate the test suite accuracy (TS), which\nserves as the official evaluation metric for this\ndataset. TS provides an upper-bound estimation\nof semantic accuracy by assessing the execution\naccuracy of predicted queries on a set of distilled\nrandomly generated databases (Zhong et al., 2020).\nSimilar to Liu et al. (2023), we refrain from uti-\nlizing the exact match accuracy (Yu et al., 2018)\nmetric as a SQL query can often be expressed in\nmultiple equivalent ways to achieve the same ob-\njective. Consequently, exact match accuracy may\ninadvertently penalize large language models that\ngenerate SQL queries that differ in style from the\ngold data.\n2.5 Evaluation Details\nWe utilize several models in our study, including\nthree variants of Dolly (v2-3b, v2-7b, and v2-12b),\ntwo variants of Vicuna (7B and 13B), one variant\nof Guanaco (33B), and four variants of LLaMA\n(7B, 13B, 30B, and 65B). To ensure consistency,\nwe aim to adhere closely to the default hyperparam-\neters of each model. We set a top-p sampling rate\nof 0.92 and a temperature of 0.8, for Dolly, a tem-\nperature of 0.8 for Vicuna and Guanaco and top-p\nsampling rate of 0.95 and a temperature of 0.8 for\n11227\nLLaMA. During the evaluation, we conduct our ex-\nperiments on a server equipped with eight NVIDIA\nRTX A6000 GPUs. For Bard, we have developed a\nscript that directly extracts evaluation outputs from\nits web user interface. For GPT3.5, we leverage\nthe \"gpt-3.5-turbo-0301\" version through OpenAI’s\nAPI and adhere to the default hyperparameters of\na temperature of 1.0 and a top-p sampling rate of\n1.0.\n3 Evaluation Results\n3.1 Spider Dataset\nThe accuracy of execution (EX) and the accuracy\nof the test suite (TS) on various combinations of\nprompting strategies and models are presented in\nTable 2. Our main findings are:\nClosed-source models exhibit superior perfor-\nmance compared to open-source models:GPT-\n3.5 is the leading model, surpassing the second-\nplace Bard model by 17.8% in execution accuracy\n(EX) and by 14.1% in test suite accuracy (TS).\nHowever, GPT-3.5 still falls behind state-of-the-art\nText-to-SQL models like the one proposed by Li\net al. (2023a) by a margin of at least 13% in abso-\nlute terms. Bard-P2 demonstrates some enhanced\nperformance over Bard-L when employing the IS,\nS3, and 5SL prompting strategies, but suffers from\nsignificant performance degradations when using\nthe AD and 1SL prompting strategies.\nOpen-source models struggle with the Spider\ndataset: Despite a positive correlation between\nthe number of parameters and model performance,\nopen-source models face challenges in achieving\nhigh accuracies on the Spider dataset. For example,\nalthough Vicuna 7B and 13B have demonstrated\nimprovements over the raw pre-trained LLaMA\n7B and 13B models, there still exists a signifi-\ncant gap in performance when compared to Bard\nand GPT-3.5. Furthermore, the Dolly models also\ndemonstrate underperformance when compared to\nLLaMA’s 13B version across different prompting\nstrategies.\nThe performance of LLMs are highly sensi-\ntive to the styles of prompts:Our empirical find-\nings confirm that there is no universal prompting\nstrategy that works well across all models. While\nthe IS prompting strategy proves effective for GPT-\n3.5, Bard, Vicuna, and Guanaco, it yields subopti-\nmal accuracies for Dolly and LLaMA. Surprisingly,\nLLaMA achieves its optimal results when employ-\ning the S3 prompts, which in contrast, significantly\ndeteriorates the performance of GPT-3.5.\nFew-shot learning with random examples of-\nfers limited performance gains:The majority of\nresults obtained from 1SL and 5SL tend to under-\nperform or, at best, achieve comparable results to\nother prompting strategies. However, there are a\ncouple of exceptions to this trend. One exception\nis the Dolly model, which shows improved perfor-\nmance with the 1SL prompting strategy compared\nto other prompting strategies in the 12B variant.\nThis result appears to be anomalous because similar\ngains in performance are not observed in other 1SL\nand 5SL results. The other exception is the LLaMA\nmodel, where the few-shot prompting strategies\noutperform some of the zero-shot strategies. For\nexample, the 30B LLaMA model achieves 22.4%\nEX and 19.9% TS accuracy with only 5 given ex-\namples, which is close to the performance of the\nGuanaco model (24.4% EX and 19.0% TS).\n3.2 Classical Datasets\nSince there are no training sets for Academic,\nRestaurants, IMDB and Yelp, we sample exam-\nples for 1SL and 5SL from the evaluation sets of\nother classical datasets. We highlight some of our\nkey findings based on the results in Table 3:\nLLMs show lacklustre performance on most\nof the classical datasets:In particular, there is a\nnoticeable disparity in the results obtained from the\nAcademic and Restaurants datasets when compared\nto the baseline performance reported in previous\nresearch. The highest achieved accuracies on these\ndatasets are merely 2.9% and 2.4% respectively,\nwhich is significantly lower than the baseline re-\nsults of 34.0% and 45.2% observed in other studies\nutilizing traditional seq2seq models with LSTM or\nBERT (Devlin et al., 2019). Furthermore, even with\ninstruction tuning, Vicuna, Guanaco and Dolly face\nconsiderable challenges on the classical datasets.\nThey often yield nearly zero execution accuracies\nacross various prompt strategies and datasets com-\nbinations.\nThe effectiveness of few-shot learning varies\nacross different models:In contrast to the find-\nings from the Spider dataset, we observe some per-\nformance improvements on LLaMA and GPT-3.5\nwith 1SL and 5SL. For example, the performance\nof GPT-3.5 on the GeoQuery dataset improves from\n15.4% to 42.3% with 1SL, while the performance\nof LLaMA on the same dataset also significantly\nimproves from 12.1% to 15.4% with 5SL. However,\n11228\nPrompting Strategies\nIS AD S3 1SL 5SL\nModels #p EX TS EX TS EX TS EX TS EX TS\nDolly\n3B 8.7 5.5 2.2 1.5 0.2 0.2 1.4 0.8 0.6 0.2\n7B 9.2 6.5 0.5 0.3 0.6 0.4 1.7 1.3 0.4 0.3\n12B 9.5 7.4 0.1 0.1 0.8 0.6 11.8 9.3 5.3 3.7\nLLaMA\n7B 4.1 2.1 2.9 2.3 11.3 7.4 7.5 6.0 9.0 8.0\n13B 8.7 4.8 6.1 4.4 16.2 12.8 13.5 11.4 14.4 13.2\n30B 4.7 3.3 10.4 7.5 18.5 13.9 18.8 15.5 22.4 19.9\n65B 12.2 9.1 14.0 10.5 29.5 23.9 23.7 19.2 23.8 20.1\nVicuna 7B 25.8 19.6 17.6 13.8 18.2 13.8 16.7 13.2 5.0 3.7\n13B 34.8 26.5 21.2 16.8 9.5 6.5 19.1 14.5 18.8 15.8\nGuanaco 33B 24.4 19.0 14.5 10.6 15.8 12.9 10.3 7.9 2.0 1.6\nBard-L UNK 53.6 46.6 52.5 45.1 53.1 45.6 50.5 43.9 51.8 45.0\nBard-P2 60.2 52.3 48.7 41.8 54.6 46.1 47.8 41.4 53.6 46.9\nGPT-3.5 175B 70.9 59.4 67.2 57.9 31.1 27.0 67.5 58.2 70.4 59.7\nTable 2: Execution accuracy (EX) and test suite accuracy (TS) results for various large language models and\nprompting strategies (Informal Schema (IS), API Docs (AD), Select 3 (S3), 1-shot learning (1SL) and 5-shot\nlearning (5SL)) on the Spider development set. The number of parameters (#p) for Bard is unknown (UNK).\nHighlighted in bold are the best results in each LLM family.\nwe do not see similar performance improvements\nwith 1SL or 5SL for Dolly, Vicuna and Bard.\nAppending database example rows is ineffec-\ntive: Just like the outcomes observed with the spi-\nder dataset, the S3 prompting strategies yield sub-\npar results when applied to the classical datasets\nacross different models. Therefore, it is evident\nthat the S3 prompting strategy may not be effective\nin the context of Text-to-SQL.\n4 Discussions\n4.1 Are LLMs generating valid SQLs?\nOne potential explanation for the underwhelm-\ning performance of large language models lies\nin their inability to grasp the intention behind\nprompts aimed at generating SQL statements.\nFor instance, in response to the question “re-\nturn me the homepage of PVLDB,” Guanaco\ndirectly provided the answer “The website is\nhttps://www.vldb.org/pvldb.” When faced with\nmany S3 prompts, GPT-3.5 fails to generate valid\nresponses. To assess the extent of such instances,\nwe plotted the proportion of valid SQL statements\ngenerated using different prompt strategies for var-\nious large language models in Figure 1a and 1b.\nFor spider dataset, we discovered that many mod-\nels, excluding Dolly, consistently generate valid\nSQL responses over 90% of the time with IS, 1SL,\nand 5SL prompt strategies. Interestingly, LLaMA\nalso demonstrates the ability to generate valid SQL\nstatements, even though it was not specifically\nfine-tuned on instruction datasets. For the clas-\nsical datasets, Bard-P2 and GPT-3.5 are still ca-\npable of generating valid SQLs in the 80-100%\nrange. However, the open-source models such as\nVicuna and Dolly encounter challenges in achiev-\ning a valid SQL percentage above 75%. What’s\nparticularly noteworthy is the divergent trends ob-\nserved in LLaMA and Guanaco. LLaMA gener-\nates more valid SQLs through few-shot learning,\nwhereas Guanaco’s performance declines as the\nnumber of examples increases.\nFurthermore, we noticed that the AD and S3\nprompting strategies are generally suboptimal, as\nthey lead to significant decreases in the number of\nvalid SQL responses across all datasets for many\nlarge language models. GPT-3.5 is particularly\nsusceptible to the S3 prompting strategy, resulting\nin a sharp decline in the percentage of valid SQLs\ngenerated in both the spider and classical datasets.\nLastly, it is crucial to emphasize that although\nthese language models can produce valid SQL re-\n11229\nModel #P PS Acad ATIS Adv Geo IMDB Rest Sch Yelp A VG\nDolly 2.0 12B\nIS 0.0 0.0 0.0 8.2 3.8 0.0 0.0 3.1 1.9\nAD 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nS3 0.0 0.0 0.0 0.0 1.5 0.0 0.0 0.8 0.3\n1SL 0.5 0.0 0.0 0.0 0.0 0.0 0.0 2.3 0.4\n5SL 0.5 0.0 0.0 0.0 0.8 0.0 0.0 0.8 0.3\nLLaMA 30B\nIS 0.0 0.3 0.0 0.5 0.0 0.0 0.0 0.0 0.1\nAD 0.0 0.0 0.0 12.1 3.1 0.0 0.3 1.6 2.1\nS3 1.0 0.3 0.0 2.7 0.0 0.0 0.3 1.6 0.7\n1SL 0.0 0.0 0.0 10.4 0.8 0.0 0.3 0.0 1.4\n5SL 0.5 0.6 0.1 15.4 0.0 0.0 2.5 1.6 2.6\nVicuna 13B\nIS 2.0 0.0 0.0 0.5 6.1 0.0 0.3 2.3 1.4\nAD 0.0 0.0 0.0 1.6 4.6 0.0 0.3 1.6 1.0\nS3 1.0 0.0 0.0 4.9 3.1 0.0 1.0 2.3 1.5\n1SL 0.0 0.3 0.1 4.9 0.0 0.0 0.0 0.8 0.8\n5SL 0.0 0.3 0.1 2.7 0.0 0.0 0.3 0.8 0.5\nGuanaco 33B\nIS 0.5 0.9 0.0 1.1 1.5 0.0 0.0 2.3 0.8\nAD 2.0 0.6 0.0 2.7 0.8 0.0 0.0 0.8 0.9\nS3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n1SL 0.0 0.0 0.0 0.5 0.0 0.0 0.3 0.0 0.1\n5SL 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.1\nBard-P2 UNK\nIS 6.6 0.0 0.0 8.2 17.6 0.0 0.3 3.9 4.6\nAD 5.6 0.3 0.0 7.7 11.5 0.0 0.3 3.1 3.6\nS3 3.1 1.2 0.0 4.4 7.6 0.0 0.3 6.2 2.9\n1SL 3.1 0.9 0.0 13.2 12.2 0.0 1.0 3.9 4.3\n5SL 1.0 0.9 0.0 3.8 10.7 0.0 0.0 2.3 2.3\nGPT-3.5 175B\nIS 13.8 1.4 0.0 15.4 17.6 2.4 2.2 4.7 7.2\nAD 10.7 0.9 0.0 11.5 16.0 0.8 1.0 2.3 5.4\nS3 1.5 0.0 0.0 3.3 5.3 0.0 0.0 1.6 1.5\n1SL 11.2 2.6 0.3 42.3 18.3 2.4 6.3 6.2 11.2\n5SL 6.1 2.9 0.3 39.6 16.8 1.6 3.5 10.2 10.1\ntext2sql-data - 75.0 34.0 8.0 49.0 24.0 33.0 6.0 32.0 32.6\nXSP - 12.1 - - - 33.3 45.2 - 49.2 -\nUnite - - - - - 41.1 - - - -\nTable 3: EX and TS results for various LLMs and prompting strategies on classical datasets: Academic, ATIS,\nAdvising, GeoQuery, IMDB, Restaurant, Scholar and Yelp. We also include baseline results in Finegan-Dollak\net al. (2018) (text2sql-data), XSP Suhr et al. (2020) (XSP) and Lan et al. (2023) (Unite) if possible. Highlighted in\nbold are the best results in each LLM family.\nsponses, these SQLs are often semantically inac-\ncurate and fail to adequately address the input text\nquestions. As a consequence, the execution accu-\nracies across the majority of datasets are notably\nlow.\n4.2 How does sample selection affect the\nperformance of 1SL and 5SL?\nBased on the results presented in Table 2 and Table\n3, it becomes evident that the inclusion of random\nexamples from the training set in prompts does not\nsignificantly enhance the performance of different\nmodels. The only exceptions are LLaMA and GPT-\n3.5, which demonstrate noticeable improvements\n11230\nIS AD S3 1SL 5SL\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrompt Strategy\nProportion of valid SQL\nDolly (12B)LLaMA (65B)Vicuna (13B)Guanaco (33B)Bard-P2GPT-3.5\n(a) Spider dataset\nIS AD S3 1SL 5SL\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrompt Strategy\nProportion of valid SQL\nDolly (12B)LLaMA (30B)Vicuna (13B)Guanaco (33B)Bard-P2GPT-3.5\n(b) Classical datasets\nFigure 1: The proportion of valid SQL generated for\ndifferent prompting strategies across multiple models.\nFigure (a) presents the results on Spider, while Figure\n(b) presents the average results based on the classical\ndatasets.\nacross most classical datasets when utilizing 1SL\nand 5SL prompting strategies. The improvements\nin LLaMA’s performance with 1SL or 5SL prompt-\ning strategies can be partially attributed to the fact\nthat exposing LLaMA to more examples substan-\ntially enhances its ability to generate valid SQL, as\ndepicted in Figure 1b.\nLLMs adapt to the canonicalized SQL style:An-\nother noteworthy observation is that when large\nlanguage models are presented with examples from\nclassical datasets, they start to generate SQL in a\nstyle similar to the canonicalized format described\nin Finegan-Dollak et al. (2018), as shown in Figure\n2, where tables alias follow a standardized conven-\ntion of <TABLE_NAME>alias<N>.\nSensitivity of LLMs to style change:To assess\nthe extent to which Language Models (LLMs) fol-\nlow the canonicalized SQL style while generating\nSELECT PUBLICATIONalias0.ABSTRACT FROM PUBLICATION\nAS PUBLICATIONalias0 WHERE PUBLICATIONalias0.TITLE\n= \"Making database systems usable\";\nFigure 2: Example of a canonicalized SQL statement.\nSQLs with 1SL and 5SL, we examine the propor-\ntion of generated SQL statements that contain the\nterm “alias” in Table 4. Our findings reveal that the\nchange in generated SQL style is only evident when\nemploying the 1SL and 5SL prompting strategies.\nNotably, LLaMA stands out among all the models,\nas it consistently appends the term \"alias\" to over\n86% of the generated SQL statements. Interest-\ningly, Bard is less sensitive to the canonicalized\nSQL style, with a style change observed in only\n16.0% of all generated SQLs. On the other hand,\nGPT-3.5 demonstrates higher sensitivity, with more\nthan 50% of the generated SQLs being affected.\nBased on this observation, we hypothesize that\nthis disparity in sensitivity could be a contribut-\ning factor to the greater success of the 1SL and\n5SL prompting strategies employed by LLaMA\nand GPT-3.5.\nModel AD IS S3 1SL 5SL\nDolly (12B) 0.1 0.0 0.0 37.8 38.0\nLLaMA (30B) 0.0 0.1 0.1 86.1 92.0\nVicuna (13B) 0.0 0.1 0.1 37.8 38.5\nGuanaco (33B) 0.0 0.1 0.2 1.8 1.8\nBard-P2 0.0 0.0 0.0 16.0 28.3\nGPT-3.5 0.0 0.0 0.0 51.6 58.3\nTable 4: Percentage of generated SQLs containing the\nword “alias” for classical datasets.\nImpact of sampling from different sources on\nPerformance\nWe conclude this section by providing a brief\ndiscussion on experiments involving the sampling\nof examples from sources other than the training\nsets. Table 5 presents the 1SL and 5SL results ob-\ntained when samples are taken from two different\nsources: 1) the Spider train set, and 2) the evalua-\ntion sets. In the second case, we take precautions to\navoid any potential answer leakage, by filtering out\nall examples that have the same SQL answer as the\nquestion of interest. We find that using examples\nfrom the Spider dataset not only fails to yield any\nbenefits but also leads to a decline in the perfor-\n11231\nmance of the models, performing worse than the\nzero-shot methods. On the other hand, when we\ninclude examples from the evaluation sets, we ob-\nserve improvements in the evaluation results. Upon\ncloser examination of the prompts, we discovered\ninstances where the few-shot examples were syn-\ntactically similar to the expected SQL responses,\ndiffering primarily in terms of tables, columns, and\nvalues. This finding highlights the sensitivity of\nLLMs to the examples provided in the prompt. We\nhypothesize that LLMs may generate more accu-\nrate SQL statements if we feed them with examples\nthat are syntactically close to the expected SQL re-\nsponse.\nModel Train Spider Eval\nDolly (12B) 1.5/0.5 0.4/0.4 0.8/0.3\nLLaMA (30B) 1.4/2.6 1.0/1.3 8.2/17.8\nVicuna (13B) 0.8/0.5 0.7/0.4 2.2/4.9\nGuanaco (33B) 0.1/0.1 0.0/0.0 0.1/0.0\nBard-P2 4.3/2.3 2.9/2.9 7.1/15.4\nGPT-3.5 11.2/10.1 6.6/7.6 16.6/28.2\nTable 5: Mean 1SL/5SL EX results when sampling from\nthe train sets, Spider train set and evaluation sets.\n4.3 Are we truly evaluating the Text-to-SQL\ndatasets in zero-shot or few-shot manner?\nWe have identified several potential sources of data\ncontamination (Elangovan et al., 2021; Lewis et al.,\n2021; Magar and Schwartz, 2022) that raise con-\ncerns about the true nature of zero-shot or few-shot\nevaluations of Text-to-SQL datasets. These sources\ninclude the availability of both the Spider dataset\nand classical datasets on GitHub repositories, as\nwell as the presence of the Spider dataset on plat-\nforms like Huggingface datasets 5. Furthermore,\nthe Text-to-SQL datasets may also be included\nin instruction-tuning dataset collections such as\nFLAN (Wei et al.). We end the paper with a ques-\ntion for researchers to contemplate: Are we gen-\nuinely conducting zero-shot or few-shot evalua-\ntions of large language models when they have\nalready been exposed to our evaluation data?\n5https://huggingface.co/datasets/spider\n5 Related Work\nRecently, decoder-based large language mod-\nels have contributed tremendously to the code-\ngeneration tasks (Li et al., 2023b; Fu et al., 2023;\nDarm et al., 2023). These models leverage unsuper-\nvised auto-regressive learning on large-scale text\ndata, allowing them to capture rich semantic re-\nlationships and probability distributions of words.\nDespite their remarkable performance with just one\nor few-shot examples in context, recent research\nsuggests that they still face challenges on the Text-\nto-SQL task, which involves complex reasoning\n(Liu et al., 2023).\nThere are several works which focus on improv-\ning the text-to-SQL parsing capabilities of large\nlanguage models through enhanced prompt designs.\nIn one study conducted by Nan et al. (2023), the au-\nthors emphasize the significance of carefully select-\ning examples for in-context learning. They demon-\nstrate that incorporating syntactic structures from\nexample queries can greatly enhance the few-shot\ncapabilities of large language models. Chang and\nFosler-Lussier (2023) conducted a comprehensive\nstudy that explores the impact of prompt length\non the performance of text-to-SQL models. Ad-\nditionally, they examine the sensitivities of repre-\nsentations of database knowledge across various\ndomains. Guo et al. (2023) propose a case-based\nreasoning framework that adjusts the inputs for\nGPT-3.5 in cross-domain settings by adaptively re-\ntrieving case prompts. Rai et al. (2023) improve\nthe generalization capabilities of large language\nmodels with boundary-based techniques that pre-\nprocess prompts at both token-level and sequence-\nlevel of schema and SQL.\nConcurrently, some studies have also explored\nthe potential benefits of complex, multi-step rea-\nsoning in improving the performance of large lan-\nguage models on text-to-SQL parsing. Tai et al.\n(2023) show that Least-to-Most prompting (Zhou\net al., 2023) might be unnecessary and directly ap-\nplying chain-of-thought (CoT) prompts (Wei et al.,\n2022) could lead to error propagation. Liu and Tan\n(2023) introduce a divide and prompt paradigm\nfor the Text-to-SQL task, which involves dividing\nthe task into multiple subtasks and applying the\nCoT approach to each subtask. In another study by\nPourreza and Rafiei (2023), a self-correction mod-\nule is employed in a zero-shot setting to achieve\na new state-of-the-art result on the Spider leader-\nboard. This module feeds the solution of each\n11232\nsub-problem back to the large language model, en-\nabling it to construct a better overall solution.\n6 Conclusions and Future Work\nThis paper systematically evaluates the Text-to-\nSQL parsing capabilities of six popular large lan-\nguage models across nine benchmark datasets, us-\ning five distinct prompt strategies. Our findings\nindicate that the open-sourced models fall signif-\nicantly short in performance when compared to\nclosed-source models. However, it is worth not-\ning that even GPT-3.5 performs worse than smaller\nbaseline models on several classical datasets. we\nare making our outputs available for further anal-\nysis and to facilitate future research endeavors.\nThere are several research topics we want to ex-\nplore in the future. Firstly, we plan to investigate\nthe fine-tuning of these large language models on\nText-to-SQL datasets using limited GPU resources\nusing techniques such as low-rank adaptation Hu\net al. (2021). Second, we want to explore methods\nthat can dynamically select examples for in-context\nlearning. Third, we would also explore applying\nmodel compression techniques to recent Text-to-\nSQL models (Sun et al., 2023). Lastly, we are\ninterested in examining the feasibility and limita-\ntions of employing these large language models\non multi-turn Text-to-SQL datasets, such as the\nSPARC (Yu et al., 2019).\nLimitations\nFirst and foremost, we acknowledge that the scope\nof this research is limited to six large language\nmodels and these models do not encompass the en-\ntire research landscape. There have been new and\nexciting entries to the family, such as the Falcon\nmodel.6 Second, appending five examples to the\ndatabase schema of some classical datasets might\nexceed the 2048 token limits of the open-source\nmodels in some cases, leading to truncations that\nmight penalize these models with shorter context\nwindow. Lastly, some models generate not just\nSQL statements but also supplementary informa-\ntion, including explanations. To ensure accuracy,\nwe have developed regular expression patterns that\naim to extract only the SQL statements to the best\nof our abilities. Nevertheless, we acknowledge\nthat our rules may not be entirely foolproof and\ncould potentially introduce erroneous SQL in cer-\ntain cases.\n6https://falconllm.tii.ae/\nAcknowledgments\nThis research is partially supported by the pro-\ngramme DesCartes funded by the National Re-\nsearch Foundation, Prime Minister’s Office, Singa-\npore under its Campus for Research Excellence and\nTechnological Enterprise (CREATE) programme.\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. 2023. Pythia:\nA suite for analyzing large language models across\ntraining and scaling.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\n11233\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020a.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020b. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nShuaichen Chang and Eric Fosler-Lussier. 2023. How\nto prompt llms for text-to-sql: A study in zero-shot,\nsingle-domain, and cross-domain settings.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nPaul Darm, Antonio Valerio Miceli-Barone, Shay B.\nCohen, and Annalisa Riccardi. 2023. Knowledge\nbase question answering for space debris queries.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nAparna Elangovan, Jiayuan He, and Karin Verspoor.\n2021. Memorization vs. generalization : Quantify-\ning data leakage in NLP performance evaluation. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1325–1335, Online.\nAssociation for Computational Linguistics.\nCatherine Finegan-Dollak, Jonathan K. Kummerfeld,\nLi Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui\nZhang, and Dragomir Radev. 2018. Improving text-\nto-SQL evaluation methodology. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 351–360, Melbourne, Australia. Association\nfor Computational Linguistics.\nXingyu Fu, Sheng Zhang, Gukyeong Kwon, Pramu-\nditha Perera, Henghui Zhu, Yuhao Zhang, Alexan-\nder Hanbo Li, William Yang Wang, Zhiguo Wang,\nVittorio Castelli, Patrick Ng, Dan Roth, and Bing Xi-\nang. 2023. Generate then select: Open-ended visual\nquestion answering guided by world knowledge.\nChunxi Guo, Zhiliang Tian, Jintao Tang, Pancheng\nWang, Zhihua Wen, Kang Yang, and Ting Wang.\n2023. A case-based reasoning framework for adap-\ntive prompting in cross-domain text-to-sql.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant\nKrishnamurthy, and Luke Zettlemoyer. 2017. Learn-\ning a neural semantic parser from user feedback.\nWuwei Lan, Zhiguo Wang, Anuj Chauhan, Henghui\nZhu, Alexander Li, Jiang Guo, Sheng Zhang, Chung-\nWei Hang, Joseph Lilien, Yiqun Hu, Lin Pan, Ming-\nwen Dong, Jun Wang, Jiarong Jiang, Stephen Ash,\nVittorio Castelli, Patrick Ng, and Bing Xiang. 2023.\nUnite: A unified benchmark for text-to-sql evalua-\ntion.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2021. Question and answer test-train overlap in open-\ndomain question answering datasets. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 1000–1008, Online. Association for\nComputational Linguistics.\nFei Li and Hosagrahar V Jagadish. 2014. Constructing\nan interactive natural language interface for relational\ndatabases. Proceedings of the VLDB Endowment,\n8(1):73–84.\nHaoyang Li, Jing Zhang, Cuiping Li, and Hong Chen.\n2023a. Resdsql: Decoupling schema linking and\nskeleton parsing for text-to-sql.\nPeng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuan-\nbin Wu, Xuanjing Huang, and Xipeng Qiu. 2023b.\nCodeie: Large code generation models are better few-\nshot information extractors.\nAiwei Liu, Xuming Hu, Lijie Wen, and Philip S. Yu.\n2023. A comprehensive evaluation of chatgpt’s zero-\nshot text-to-sql capability.\nXiping Liu and Zhao Tan. 2023. Divide and prompt:\nChain of thought prompting for text-to-sql.\n11234\nInbal Magar and Roy Schwartz. 2022. Data contamina-\ntion: From memorization to exploitation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 157–165, Dublin, Ireland. Association\nfor Computational Linguistics.\nLinyong Nan, Yilun Zhao, Weijin Zou, Narutatsu\nRi, Jaesung Tae, Ellen Zhang, Arman Cohan, and\nDragomir Radev. 2023. Enhancing few-shot text-to-\nsql capabilities of large language models: A study on\nprompt design strategies.\nAna-Maria Popescu, Alex Armanasu, Oren Etzioni,\nDavid Ko, and Alexander Yates. 2004. Modern nat-\nural language interfaces to databases: Composing\nstatistical parsing with semantic tractability. In COL-\nING 2004: Proceedings of the 20th International\nConference on Computational Linguistics, pages 141–\n147, Geneva, Switzerland. COLING.\nMohammadreza Pourreza and Davood Rafiei. 2023.\nDin-sql: Decomposed in-context learning of text-\nto-sql with self-correction.\nPatti J. Price. 1990. Evaluation of spoken language\nsystems: the atis domain. In Human Language Tech-\nnology - The Baltic Perspectiv.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nDaking Rai, Bailin Wang, Yilun Zhou, and Ziyu Yao.\n2023. Improving generalization in language model-\nbased text-to-sql semantic parsing: Two simple se-\nmantic boundary-based techniques.\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bah-\ndanau. 2022. Evaluating the text-to-sql capabilities\nof large language models.\nAlane Suhr, Ming-Wei Chang, Peter Shaw, and Ken-\nton Lee. 2020. Exploring unexplored generalization\nchallenges for cross-database semantic parsing. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8372–\n8388, Online. Association for Computational Lin-\nguistics.\nShuo Sun, Yuze Gao, Yuchen Zhang, Jian Su, Bin Chen,\nYingzhan Lin, and Shuqi Sun. 2023. An exploratory\nstudy on model compression for text-to-sql. In Find-\nings of the Association for Computational Linguistics:\nACL 2023, pages 11647–11654.\nChang-You Tai, Ziru Chen, Tianshu Zhang, Xiang Deng,\nand Huan Sun. 2023. Exploring chain-of-thought\nstyle prompting for text-to-sql.\nLappoon R Tang and Raymond Mooney. 2000. Auto-\nmated construction of database interfaces: Intergrat-\ning statistical and relational learning for semantic\nparsing. In 2000 Joint SIGDAT Conference on Em-\npirical Methods in Natural Language Processing and\nVery Large Corpora, pages 133–141.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-\nChing Chang, Igor Krivokon, Will Rusch, Marc\nPickett, Pranesh Srinivasan, Laichee Man, Kathleen\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\nLe. 2022. Lamda: Language models for dialog appli-\ncations.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are\nzero-shot learners. In International Conference on\nLearning Representations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nNavid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and\nThomas Dillig. 2017. Type- and content-driven syn-\nthesis of sql queries from natural language.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911–3921, Brussels, Bel-\ngium. Association for Computational Linguistics.\nTao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern\nTan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene\nLi, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit,\nDavid Proctor, Sungrok Shim, Jonathan Kraft, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, and\nDragomir Radev. 2019. SParC: Cross-domain se-\nmantic parsing in context. In Proceedings of the\n11235\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4511–4523, Florence, Italy.\nAssociation for Computational Linguistics.\nJohn M. Zelle and Raymond J. Mooney. 1996. Learn-\ning to parse database queries using inductive logic\nprogramming. In AAAI/IAAI, Vol. 2.\nRuiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic\nevaluation for text-to-sql with distilled test suites.\narXiv preprint arXiv:2010.02840.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc Le, and Ed Chi.\n2023. Least-to-most prompting enables complex rea-\nsoning in large language models.\nA Appendix\n11236\n(a) Examples of prompt strategies IS in the Spider dataset\n (b) Examples of prompt strategies AD in the Spider dataset\n(c) Examples of prompt strategies Select 3 in the Spider\ndataset\n(d) Examples of prompt strategies 1SL/5SL in the Spider\ndataset\nFigure 3: Examples of our prompt strategies in the Spider dataset\nModel #P PS Acad ATIS Adv Geo IMDB Rest Sch Yelp A VG\nDolly\n3B\nIS 0.5 0.3 0.0 0.5 0.0 0.0 0.0 1.6 0.4\nAD 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nS3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n1SL 0.0 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n5SL 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n13B\nIS 0.5 0.6 0.0 0.5 2.3 0.0 0.0 1.6 0.7\nAD 0.0 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nS3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n1SL 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.6 0.2\n5SL 0.0 0.3 0.0 1.6 0.0 0.0 0.0 0.0 0.2\n12B\nIS 0.0 0.0 0.0 8.2 3.8 0.0 0.0 3.1 1.9\nAD 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nS3 0.0 0.0 0.0 0.0 1.5 0.0 0.0 0.8 0.3\n1SL 0.5 0.6 0.0 8.2 0.0 0.0 0.3 2.3 1.5\n5SL 0.5 0.3 0.0 1.1 0.8 0.0 0.3 0.8 0.5\nTable 6: EX and TS results for Dolly models on classical datasets: Academic, ATIS, Advising, GeoQuery, IMDB,\nRestaurant, Scholar and Yelp.\n11237\nModel #P PS Acad ATIS Adv Geo IMDB Rest Sch Yelp A VG\nVicuna\n7B\nIS 2.0 0.6 0.0 5.5 3.8 0.0 0.3 2.3 1.8\nAD 0.0 0.0 0.0 2.2 0.0 0.0 0.3 1.6 0.5\nS3 1.0 0.3 0.0 1.6 1.5 0.0 0.3 0.8 0.7\n1SL 0.0 0.6 0.0 0.0 0.8 0.0 0.0 1.6 0.4\n5SL 0.5 0.0 0.0 0.0 1.5 0.0 0.0 0.0 0.3\n13B\nIS 2.0 0.0 0.0 0.5 6.1 0.0 0.3 2.3 1.4\nAD 0.0 0.0 0.0 1.6 4.6 0.0 0.3 1.6 1.0\nS3 1.0 0.0 0.0 4.9 3.1 0.0 1.0 2.3 1.5\n1SL 0.0 0.3 0.1 4.9 0.0 0.0 0.0 0.8 0.8\n5SL 0.0 0.3 0.1 2.7 0.0 0.0 0.3 0.8 0.5\nTable 7: EX and TS results for Vicuna models on classical datasets: Academic, ATIS, Advising, GeoQuery, IMDB,\nRestaurant, Scholar and Yelp.\nModel #P PS Acad ATIS Adv Geo IMDB Rest Sch Yelp A VG\nLLaMA\n7B\nIS 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nAD 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nS3 0.0 0.0 0.0 3.3 0.0 0.0 0.3 0.8 0.5\n1SL 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n5SL 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n13B\nIS 0.0 0.0 0.0 0.0 0.8 0.0 0.0 0.8 0.2\nAD 0.0 0.0 0.0 1.6 0.8 0.0 0.3 0.8 0.4\nS3 1.0 0.6 0.0 13.2 3.1 0.0 0.3 1.6 2.5\n1SL 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.6 0.3\n5SL 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.8 0.1\n30B\nIS 0.0 0.3 0.0 0.5 0.0 0.0 0.0 0.0 0.1\nAD 0.0 0.0 0.0 12.1 3.1 0.0 0.3 1.6 2.1\nS3 1.0 0.3 0.0 2.7 0.0 0.0 0.3 1.6 0.7\n1SL 0.0 0.0 0.0 10.4 0.8 0.0 0.3 0.0 1.4\n5SL 0.5 0.6 0.1 15.4 0.0 0.0 2.5 1.6 2.6\n65B\nIS 0.5 0.0 0.0 1.6 0.8 0.0 0.0 2.3 0.7\nAD 1.0 0.0 0.0 0.5 2.3 0.0 0.0 0.0 0.5\nS3 0.5 0.0 0.0 6.0 2.3 0.0 0.3 0.8 1.2\n1SL 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n5SL 0.0 0.3 0.1 11.0 0.0 0.0 2.5 0.0 1.7\nTable 8: EX and TS results for LLaMA models on classical datasets: Academic, ATIS, Advising, GeoQuery,\nIMDB, Restaurant, Scholar and Yelp.\n11238"
}