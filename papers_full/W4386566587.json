{
    "title": "Retrieval-augmented Image Captioning",
    "url": "https://openalex.org/W4386566587",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2224406594",
            "name": "Rita Ramos",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2257973904",
            "name": "Desmond Elliott",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1990158554",
            "name": "Bruno Martins",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3102371147",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W2962787423",
        "https://openalex.org/W3167118264",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W3120237956",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W1969616664",
        "https://openalex.org/W4289542422",
        "https://openalex.org/W2463955103",
        "https://openalex.org/W2986670728",
        "https://openalex.org/W2109586012",
        "https://openalex.org/W1897761818",
        "https://openalex.org/W3034655362",
        "https://openalex.org/W4385468994",
        "https://openalex.org/W2133459682",
        "https://openalex.org/W2896348597",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2788330850",
        "https://openalex.org/W4312712450",
        "https://openalex.org/W2506483933",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4385567345",
        "https://openalex.org/W2963084599",
        "https://openalex.org/W3156789018",
        "https://openalex.org/W2250379249",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W4386076004",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3124149278",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W2963349562",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W4299585995",
        "https://openalex.org/W4288329833",
        "https://openalex.org/W3176587734",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2997591391",
        "https://openalex.org/W2885421725",
        "https://openalex.org/W68733909",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W3110570034",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W4303444943",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3200835136",
        "https://openalex.org/W1889081078"
    ],
    "abstract": "Inspired by retrieval-augmented language generation and pretrained Vision and Language (V&L) encoders, we present a new approach to image captioning that generates sentences given the input image and a set of captions retrieved from a datastore, as opposed to the image alone. The encoder in our model jointly processes the image and retrieved captions using a pretrained V&L BERT, while the decoder attends to the multimodal encoder representations, benefiting from the extra textual evidence from the retrieved captions. Experimental results on the COCO dataset show that image captioning can be effectively formulated from this new perspective. Our model, named EXTRA, benefits from using captions retrieved from the training dataset, and it can also benefit from using an external dataset without the need for retraining. Ablation studies show that retrieving a sufficient number of captions (e.g., k=5) can improve captioning quality. Our work contributes towards using pretrained V&L encoders for generative tasks, instead of standard classification tasks.",
    "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3666–3681\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nRetrieval-augmented Image Captioning\nRita Ramos† Desmond Elliott⋆ Bruno Martins†\n†INESC-ID, Instituto Superior Técnico, University of Lisbon\n⋆Department of Computer Science, University of Copenhagen\nritaparadaramos@tecnico.ulisboa.pt\nAbstract\nInspired by retrieval-augmented language gen-\neration and pretrained Vision and Language\n(V&L) encoders, we present a new approach\nto image captioning that generates sentences\ngiven the input image and a set of captions re-\ntrieved from a datastore, as opposed to the im-\nage alone. The encoder in our model jointly\nprocesses the image and retrieved captions us-\ning a pretrained V&L BERT, while the de-\ncoder attends to the multimodal encoder rep-\nresentations, beneﬁting from the extra textual\nevidence from the retrieved captions. Exper-\nimental results on the COCO dataset show\nthat image captioning can be effectively formu-\nlated from this new perspective. Our model,\nnamed EXTRA , beneﬁts from using captions\nretrieved from the training dataset, and it can\nalso beneﬁt from using an external dataset\nwithout the need for retraining. Ablation stud-\nies show that retrieving a sufﬁcient number of\ncaptions (e.g., k=5) can improve captioning\nquality. Our work contributes towards using\npretrained V&L encoders for generative tasks,\ninstead of standard classiﬁcation tasks.\n1 Introduction\nImage captioning is the task of automatically gener-\nating a short textual description for a given image.\nThe standard approach involves the use of encoder-\ndecoder neural models, combining a visual encoder\nwith a language generation decoder (see Hossain\net al. (2019) for a survey). In early studies, the\nencoder was typically a Convolutional Neural Net-\nwork model (CNN) pretrained on the ImageNet\nclassiﬁcation dataset (Russakovsky et al., 2015)\nor a pretrained Faster-RCNN object detector (Ren\net al., 2015), whereas the decoder was commonly\nan LSTM (Hochreiter and Schmidhuber, 1997) to-\ngether with an attention mechanism (Bahdanau\net al., 2014). More recently, Transformer based\nmodels have been achieving state-of-the-art results\non a variety of language processing (Vaswani et al.,\n2017; Devlin et al., 2018; Radford et al., 2019) and\ncomputer vision tasks (Dosovitskiy et al., 2020).\nAccordingly, state-of-the art image captioning mod-\nels have replaced the conventional CNN-LSTM\napproach with encoder-decoder Transformers (Liu\net al., 2021). Still, in both cases, the encoder only\nattains visual representations, whereas richer fea-\ntures could be captured from image–text interac-\ntions if the encoder had access to useful textual\ncontext related to the input image (e.g., sentences\nassociated to similar images).\nIn this paper, we present a new type of image\ncaptioning model that uses a pretrained V&L BERT\n(Tan and Bansal, 2019; Li et al., 2019; Bugliarello\net al., 2020, inter-alia) to encode both the input\nimage and captions retrieved from similar images.\nThis model generates captions conditioned on rep-\nresentations that consider linguistic information\nbeyond the image alone. Moreover, speciﬁcally us-\ning the retrieved captions as textual contexts rather\nthan other alternatives (e.g., image tags or object\nnames) can aid guiding the language generation\nprocess, since the model is now provided with well-\nformed sentences that are semantically similar to\nwhat the predicted caption should resemble.\nIn experiments on the COCO dataset (Chen et al.,\n2015), the proposed model is competitive against\nstate of the art methods. In a series of ablation ex-\nperiments, we ﬁnd that the model improves when\nencoding multiple retrieved captions, and that it\ncould reach better performance if it was able to\nretrieve better captions from the datastore. In ex-\nperiments on the smaller Flickr30K dataset, we\nshow that allowing the model to retrieve captions\nfrom the larger COCO dataset can improve perfor-\nmance without needing to retrain the model.\nWe hope that our work inspires the adoption\nof pretrained V&L encoders for a broader range\nof generative multimodal tasks. There have been\nseveral recent studies proposing V&L BERTs to\nlearn generic multi-modal representations with\n3666\nlarge amounts of paired image and text data, which\ncan then be ﬁne-tuned to downstream tasks. How-\never, these pretrained models have mostly been ap-\nplied to classiﬁcation tasks and have seen limited\nuse for image captioning, a task which typically\nonly considers single-input images, as opposed to\nimage-text pairs, as proposed in this work.\n2 Model\nWe present a model that captions images, given\nboth the image and a set of k captions retrieved\nfrom similar images using a retrieval system.\nThis approach belongs to the class of retrieval-\naugmented language generation models (Weston\net al., 2018; Izacard and Grave, 2020). In our\nmodel, the image and the retrieved captions are\njointly encoded using a pretrained V&L encoder\nto capture cross-modal representations in the com-\nbined input data. We denote our model as EX-\nTRA : Encoder with Cross-modal representations\nThrough Retrieval Augmentation. It consists of\nthree components, namely an encoder, a retrieval\nsystem, and a decoder.\n2.1 Encoder\nThe encoder in EXTRA is LXMERT1 (Tan and\nBansal, 2019), a pretrained vision-and-language\nTransformer that jointly encodes a visual input\nV and a linguistic input L. The visual input\nis represented as N=36 regions-of-interest\nV={v1,...,v N}extracted from the image using\nthe Faster-RCNN object detector, pretrained\n(Anderson et al., 2018) on the Visual Genome\ndataset (Krishna et al., 2016). A sentence in the\nlinguistic input is tokenized into M sub-words\nusing the BERT tokenizer (Devlin et al., 2018),\nstarting with a special classiﬁcation token CLS\nand ending with a special delimiter token SEP.\nWe extended LXMERT to encode k sentences\nby concatenating the tokenized sentences into a\nsingle input, each separated by the delimiter token:\nL={CLS,wL1\n1 . . .wL1\nM,SEP. . .wLk\n1 . . .wLk\nM ,SEP}.\nThe sentences are obtained from a datastore via a\nretrieval system, as explained in Section 2.2.\nThe encoder produces a sequence of cross-modal\nrepresentations of image and the text, which are\nthe inputs to the decoder, described in Section 2.3.\n1The exploration of other encoders is left for future work.\n2.2 Image–Text Retrieval and Datastore\nThe retrieval system builds on the Facebook\nAI Similarity Search (FAISS) nearest-neighbour\nsearch library (Johnson et al., 2017). FAISS allows\nfor the indexing of high-dimensional vectors, i.e.,\na datastore D, and it offers the ability to quickly\nsearch through the datastore given a similarity mea-\nsure S, e.g., Euclidean distance or cosine similarity.\nGiven an input image V, the retrieval system\nﬁnds L, the set of k captions retrieved from the\ndatastore, which EXTRA encodes together with the\nimage. The datastore consists of captions associ-\nated with images in a dataset 2. Each caption in\nthe datastore, and the query input image, are repre-\nsented using vectors extracted from CLIP (Radford\net al., 2021), allowing image–text search by pro-\njecting images and text to a shared latent space.\nUsing FAISS, the input image can then be com-\npared against the vectors3 from D to search over\nthe corresponding knearest-neighbours captions.\n2.3 Decoder\nThe decoder is a conditional auto-regressive lan-\nguage model based on GPT-2 (Radford et al., 2019)\nwith additional cross-attention layers to the encoder.\nThe Transformer layers in the decoder already con-\ntain a masked multi-head self-attention sublayer,\nwhich self-attends to the previous words. We add\ncross-attention layers (Vaswani et al., 2017) subse-\nquent to the masked self-attention sublayers, so the\ndecoder can attend to the encoder outputs.\nThe decoder predicts a caption y1 ...y M token-\nby-token, conditioned on the previous tokens and\nthe outputs of the V&L encoder. The model’s pa-\nrameters θ are trained by minimizing the sum of\nthe negative log-likelihood of predicting the ground\ntruth token at each time-step, using the standard\ncross-entropy loss:\nLθ = −\nM∑\ni=1\nlog Pθ(yi|y<i,V,L). (1)\nWe can also ﬁne-tune the model with Self-\nCritical Sequence Training (Rennie et al., 2017).\n3 Experimental Protocol\n3.1 Datasets and Metrics\nWe evaluate our model on the COCO dataset (Chen\net al., 2015), using the standard Karpathy splits of\n2This can either be the training set or an external dataset.\n3This comparison can be pre-computed for efﬁciency.\n3667\na man slope\nSEP\n...\n... a\na\nBOS\nEOS\nskier\nskier heads\nmountains\nmountains\nthe\n...\n...\n...\n...\nv1 w1 wMv2 vN-1 vN CLS\n...\n\"a man riding skis\ndown a snow covered slope\"\nAutoregressive\nLanguage Decoder\nVision-and-Language\nEncoder\nCross-Attention\nCurrent Image Retrieved Caption\n\"a man riding skis\ndown a snow covered slope\"\n.........\n\"a couple of people with\nski 's standing in the snow\"\nDISTANCES\nINPUT\nIMAGE CAPTION\nDatastore\n2\n15\n...\nhv1 hv2 hvN-1 hvN hw1 hw2 hwM-1 hwM\n...\nFigure 1: Illustration of the EXTRA model. Given an input image, EXTRA retrieves captions from a datastore\nand encodes both the input image and the retrieved captions using a pretrained vision-and-language encoder. The\ndecoder attends over both the visual and linguistic outputs, improving the quality of the generated caption.\n113287 images for training, 5000 for validation,\nand 5000 for testing, with 5 captions per image.\nStandard metrics were used to evaluate caption\ngeneration, namely BLEU-4 (B4) (Papineni et al.,\n2002), METEOR (Denkowski and Lavie, 2014),\nCIDEr (Vedantam et al., 2015), and SPICE (An-\nderson et al., 2016), using the MS COCO caption\nevaluation package4.\n3.2 Implementation and Training Details\nThe implementation5 of EXTRA uses the Hugging-\nFace Transformers library (Wolf et al., 2020). The\nencoder is LXMERT (Tan and Bansal, 2019), a 14-\nlayer V&L model pretrained on 9 million image–\nsentence pairs across a variety of datasets and tasks.\nFollowing Liu et al. (2021), the decoder is a 4-layer\nrandomly initialized GPT-2-style Transformer net-\nwork with 12 attention heads and additional cross-\nattention layers. The retrieval systems uses FAISS\nwith a ﬂat index ( IndexFlatIP) without any train-\ning. The corresponding datastore D consists of\n4https://github.com/tylin/coco-caption\n5https://github.com/RitaRamo/extra\nall the captions associated to the 113287 images\nin the COCO training set. For caption retrieval,\nthe captions in the datastore and the input image\n(i.e., the query) are both represented with features\nextracted from the CLIP-ResNet50×4 pretrained\nmodel. Using the cosine similarity for comparison,\na total of k= 5captions are retrieved to be jointly\nencoded with the input image by EXTRA . Notice\nthat CLIP-ResNet50×4 features are only used for\nretrieval, while the EXTRA encoder, i.e. the pre-\ntrained LXMERT, requires Faster-RCNN features,\nand thus it cannot use CLIP visual features.\nEXTRA is trained in two stages using a single\nNVIDIA V100S 32GB GPU. In the ﬁrst stage, EX-\nTRA is trained end-to-end with the cross-entropy\nloss, using a batch size of 64 and the AdamW opti-\nmizer (Loshchilov and Hutter, 2017) with a learn-\ning rate of 3e−5. The encoder is trained with a lin-\near warmup for the ﬁrst epoch to prevent gradients\nfrom the randomly initialized decoder from harm-\ning the pretrained encoder. The model was trained\nwith early stopping: training ends if there is no\nimprovement after 5 consecutive epochs on the val-\n3668\nCross-Entropy Optimization CIDEr Optimization\nB4 METEOR CIDEr SPICE B4 METEOR CIDEr SPICE\nEncoder-Decoder models\nUp-Down 36.2 27.0 113.5 20.3 36.3 27.7 120.1 21.4\nCaMELFaster R-CNN 36.1 28.0 114.8 20.8 - - - -\nGCN-LSTM 36.8 27.9 116.3 20.9 38.2 28.5 127.6 22.0\nVL-T5 34.5 28.7 116.5 21.9 - - - -\nAoANet 37.2 28.4 119.8 21.3 38.9 29.2 129.8 22.4\nCPTR - - - - 40.0 29.1 129.4 -\nEXTRA (k= 5) 37.5 28.5 120.9 21.7 36.4 28.2 131.1 21.3\nCaMELCLIP-RN50×16 38.8 29.4 125.0 22.2 41.3 30.2 140.6 23.9\nV&L BERT models\nVLP 36.5 28.4 116.9 21.2 39.5 29.3 129.3 23.2\nOSCARB 36.5 30.3 123.7 23.1 40.5 29.7 137.6 22.8\nVinVLB 38.2 30.3 129.3 23.6 40.9 30.9 140.4 25.1\nTable 1: Results on the Karpathy COCO test split. EXTRA (k= 5) is competitive against encoder-decoder models.\nWe present results with cross-entropy training and after Self-Critical Sequence Training using the CIDEr metric.\nidation set over the BLEU-4 metric. In the second\nstage, EXTRA is ﬁne-tuned with Self-Critical Se-\nquence Training (Rennie et al., 2017) with CIDEr\noptimization and greedy search decoding as a base-\nline, using a batch size of 55, a learning rate of\n3e-5, and a frozen encoder. Captions are decoded\nusing beam search with a beam size of 3.\n4 Results\nTable 1 shows the performance of EXTRA com-\npared to strong encoder-decoder models. We com-\npare against the widely-used Up-Down (Anderson\net al., 2018) and AoANet models (Huang et al.,\n2019), both using a Faster-RCNN image encoder;\nthe GCN-LSTM model (Yao et al., 2018) with a\nGraph Convolutional Network (GCN) encoder; the\nCPTR model (Liu et al., 2021) employing a ViT\nTransformer encoder (Dosovitskiy et al., 2020);\nthe VL-T5 Transformer model (Cho et al., 2021)\nwith a vision and language encoder; and the re-\ncent CaMEL model (Barraco et al., 2022) with\nthe CLIP-RN50×16 encoder. Our model is also\ncompared with state-of-art models that do not use\nthe encoder-decoder paradigm but instead unify\nthe Transformer encoder and decoder into a sin-\ngle model, namely the VLP (Zhou et al., 2020),\nOSCAR-base (Li et al., 2020), and the VinVL-base\n(Zhang et al., 2021) models. We note that these\nare general purpose V&L models, not speciﬁcally\ndesigned for image captioning.\nOverall, EXTRA is competitive to state-of-the art\ncaptioning models. It outperforms captioning mod-\nels with vision encoders, and VL-T5, which, like\nEXTRA, uses a V&L encoder, but with object tags\nas linguistic inputs rather than retrieved captions.\nAlthough EXTRA does not outperform the state of\nthe art captioning model, CaMEL, that uses a dual\ndecoder, it outperforms the variant of CaMEL that\nuses the same Faster-RCNN features. EXTRA also\ncompetes with general purpose V&L BERT mod-\nels. Notice that our approach can be adapted to\nother V&L encoders besides LXMERT (e.g., OS-\nCAR, VinVL, etc.), or to more powerful decoders\n(e.g., as in CaMEL). Likewise, other models could\nbeneﬁt from retrieval-augmentation with captions.\n4.1 Ablation Studies\nWe conducted a series of ablation studies in the\nKarpathy COCO validation split to better under-\nstand what contributes to the success of EXTRA .\nVarying the Number of Retrieved Captions:\nWe start by studying the importance of training\nwith multiple retrieved captions, training with k=1\nand k=3 captions to explore the effect of retrieving\nfewer captions. Table 2 reports the result of this\nexperiment, showing that performance degrades\nwhen retrieving less captions.\nEncoding Irrelevant Captions: We also studied\nthe performance of EXTRA when it encodes textual\n3669\nB4 CIDEr\nk= 1 36.7 118.0\nk= 3 37.4 119.1\nk= 5 38.3 121.2\nTable 2: The effect of training and evaluating using dif-\nferent numbers of retrieved captions. Performance re-\nported after training with cross-entropy optimization.\ninput that is not expected to be useful. We con-\nduct two experiments where EXTRA is trained with\ntextual input that is either an empty caption or a\nrandomly chosen caption.\n• Empty Caption: encode the image with an\nempty sentence: L={CLS, SEP};\n• Random Caption: encode the image with a\nrandom caption from the datastore.\nTable 3 shows the result of this experiment. EX-\nTRA outperforms both variants, further showing\nthat the generation process is improved by encod-\ning the image together with relevant textual context\nfrom nearest-neighbour captions. Although having\nan inferior performance, both models reach rea-\nsonable results compared to other models in the\nliterature (see Table 1), showing that LXMERT can\nbe used as a strong encoder for image captioning\nwithout providing relevant input image-text pairs.\nB4 CIDEr\nEmpty caption 37.8 119.1\nRandom caption 37.1 117.7\nEXTRA 38.3 121.2\nTable 3: The effect of training and evaluating with cap-\ntions that are not expected to be useful.\nEncoding Irrelevant Images: We tested ablat-\ning the visual input (i.e., setting the visual features\nto zero). Training on “blacked out“ input images\nachieves 102.1 in CIDEr, which is substantially\nlower than training with the actual input images,\nas seen in Table 4. This further shows that EXTRA\nuses the visual input, and does not just rely on the\nretrieved information.\nChanging the Retrieval System and Datastore:\nWe then studied the effect of changing the retrieval\nsystem and the representations in the datastore.\nRecall that EXTRA relies on captions obtained by\nB4 CIDEr\nBlacked out image 32.1 102.1\nEXTRA 38.3 121.2\nTable 4: The effect of training and evaluating with\n“blacked out“ input images.\nImage–Text retrieval, where the datastore contains\nthe captions from the COCO training set, repre-\nsented as vectors extracted from CLIP. We con-\nducted experiments with Image–Image and Image–\nText retrieval to understand which performs better:\n• Image–Image Retrieval: the datastore con-\nsists of all the images in the training data. The\nrepresentation of the input image is compared\nagainst those in the datastore to ﬁnd the k\nnearest-neighbour images, and, subsequently,\nto obtain the k captions associated to those\nimages. Speciﬁcally, one reference caption\nis retrieved from each of the top- k nearest-\nneighbour images.\n• Image–Text Retrieval: the datastore consists\nof all the captions associated to the images in\nthe training data. The representation of the\ninput image is compared against the captions\nto directly ﬁnd the top-kcaptions.\nFor Image–Image retrieval, the input image and\nthe images in D are represented with Faster R-\nCNN features, after global average pooling the\nembeddings of the 36 region-of-interest vectors.\nFor Image–Text Retrieval, the input image and the\ncaption vectors should already belong to a shared\nsemantic space. We use the pretrained CLIP model\nbecause it satisﬁes this criteria and thus allows for\ndirect image–text comparison. We considered two\nvariants of CLIP based on their visual backbone:\nViT or ResNet50x46. The results of this experiment\nare reported in Table 5.\nEXTRA performs worse when it uses Image–\nImage retrieval in comparison to retrieving cap-\ntions directly with Image–Text retrieval. The best\nperformance is obtained with the ResNet-variant\nof the CLIP encoder. We also assess the perfor-\nmance of directly using only one of the retrieved\ncaptions, with the results shown in Figure 2. In this\nﬁgure, we can visualize the expected CIDEr score\n6Regarding the comparison measure S, the Euclidean dis-\ntance and cosine similarity were used respectively for Image-\nto-Image retrieval and Image-to-Text retrieval.\n3670\nB4 CIDEr\nImage–Image (Faster R-CNN) 36.8 117.1\nImage–Text (CLIP ViT) 38.1 120.3\nImage–Text (CLIP ResNet) 38.3 121.2\nTable 5: The effect of training and evaluating EXTRA\nwith different retrieval systems. k= 5in both settings.\nof the ﬁrst retrieved captions and observe that some\nof them do not sufﬁciently describe the image, or\nare mismatches, with a CIDEr of zero. We also\nobserve that the CIDEr score can change signiﬁ-\ncantly depending on the retrieval system. A larger\nnumber of mismatch captions are retrieved with\nImage-to-Image retrieval. This suggests that the\nretrieval system and the datastore can largely im-\npact a retrieval-augmented image captioning model,\nhence they should be carefully considered.\n0 100 200 300 400 500\nCIDEr\n0\n5000\n10000\n15000\n20000\n25000Count\nRetrieval system\nImage--Image (Faster R-CNN)\nImage--Text (CLIP ResNet)\nFigure 2: Histogram of the CIDEr scores for the near-\nest caption ( k = 1) retrieved with Image-Image and\nImage-Text retrieval. This shows the evaluations scores\nof using only the retrieved captions.\nOracle Performance: Given that the retrieval\nsystem and datastore affect the performance of EX-\nTRA , we also study whether EXTRA could continue\nto improve if it could retrievebetter captions. After\ntraining EXTRA with the k= 5retrieved captions,\nwe simulate an oracle retrieval system during infer-\nence, by allowing the actual reference captions to\nbe encoded by EXTRA . Table 6 reports on experi-\nments in the validation data with respect to replac-\ning one of the kretrieved captions with one of the\nreference captions, as well as replacing all with the\n5 references associated to the input images. These\nexperiments bring a 1.8 and 8.3 point increase in\nCIDEr score, respectively, showing the potential\nfor EXTRA to improve by retrieving captions that\nbetter match the input image.\nB4 CIDEr\nk= 5retrieved captions 38.3 121.2\nk= 4and 1 reference 39.0 123.0\nk= 0and 5 references 40.9 129.5\nTable 6: Simulation of an oracle experiment, whereEX-\nTRA can “retrieve” reference captions of an image in-\nstead of retrieving all 5 captions from the datastore.\n5 Discussion\n5.1 Vision First and Language Later\nHow does EXTRA use the encoded image and re-\ntrieved captions? We quantify this by estimating\nthe behaviour of the cross-modal attention heads at\neach layer in the decoder. Speciﬁcally, we compute\nthe average of the cross-modal attention across ei-\nther the number of image regions or the sub-words\nin the encoder, at each time-step of generating a\ncaption and across each of the 12 attention heads.\nFigure 3 shows that across the layers, the de-\ncoder’s attention shifts to the textual outputs. In\nLayer 1, the model attends both to the visual and\ntextual representations, but the model hardly pays\nattention to the visual outputs by Layer 4, relying\nmore on the textual information from the retrieved\ncaptions. This behaviour further shows that the\nsemantics of the nearest captions can aid guiding\nthe language generation process. We performed an\nidentical calculation for the variants of EXTRA that\nencoded an empty or a random caption, ﬁnding in\nthis case the opposite behaviour: the model learned\nto ignore the textual embeddings provided by the\nencoder (see Appendix A).\n0 1 2 3\nLayer\n0.2\n0.4\n0.6\n0.8Average attention weight\nVisual Textual\nFigure 3: The average cross-attention from the decoder\nto the outputs of the encoder in respect to the visual V\nand textual Loutputs. Values from COCO validation.\n3671\n5.2 Retrieve Enough Captions to Overcome\nRetrieval Mistakes\nWe note that training with an empty set of captions\nwas better than encoding a single k= 1and k= 3\nretrieved captions, observing Tables 2 and Table\n3. Thus, retrieval augmentation aids to improve\ncaption quality when a sufﬁcient number (k = 5)\nis considered. This further shows that retrieving\nenough captions can be crucial for success. For\nthis, we hypothesise that retrieving more captions\nmakes the model more robust in the presence of\nmismatches from certain captions, as shown for\ninstance in the second example in Figure 4.\n5.3 Hot-swapping the Datastore\nBesides taking advantage of similar training ex-\namples, we study whether EXTRA works with ex-\nternal image–caption collections without needing\nto retrain the model. For this experiment, EXTRA\nwas ﬁrst trained and evaluated in a small dataset,\nand then the retrieval datastore was augmented\nwith a larger dataset. The considered datasets\nwere Flickr30k and COCO, respectively. While\nFlickr30k only contains 30k images, COCO con-\ntains 113K, each paired with ﬁve sentences. Table\n7 reports the results of these experiment. EXTRA\ngot a better performance considering a larger ex-\nternal dataset than just using the current training\nset, showing the potential for EXTRA to adapt the\nretrieval datastore.\nRetrieval Datastore B4 CIDEr\nFlickr30k 28.8 59.6\n+ COCO 29.5 59.9\nTable 7: Performance of EXTRA on the Flickr30k val-\nidation set. The model is trained on the Flickr30K\ndataset with the Flickr30K datastore. The datastore for\ninference is either the Flickr30K training set or com-\nbined with the COCO training set.\n5.4 Qualitative Examples\nFigure 4 shows examples of captions generated\nby EXTRA , given the input image and the k = 5\nretrieved captions. EXTRA beneﬁts from textual\nevidence from nearest-neighbour captions, even\nthough sometimes the retrieved information can be\nmisleading, as depicted in the last example. More\nexamples are provided in Appendix B.\n6 Related Work\nImage Captioning: The task of image caption-\ning is usually addressed by one of these three\nmain approaches: templates, retrieval, and encoder-\ndecoder methods. Early approaches involved\ntemplate-based methods that consisted of ﬁlling\nblanks of predeﬁned captions through object de-\ntection (Farhadi et al., 2010; Kulkarni et al., 2013;\nElliott and de Vries, 2015). Retrieval-based meth-\nods instead search over a dataset for the most sim-\nilar image and fetch the corresponding caption\n(Hodosh et al., 2013; Ordonez et al., 2011). Cur-\nrently, the most common approach is the encoder-\ndecoder framework (Xu et al., 2015; Hossain et al.,\n2019). The encoder typically used a pretrained\nCNN (Vinyals et al., 2016) or a Faster R-CNN (An-\nderson et al., 2018), encoding the image into a grid\nof image features or object proposal image regions.\nThe decoder was usually a LSTM with an attention\nmechanism (Xu et al., 2015) to dynamically focus\non different parts of the encoded image during the\nprediction of each word.\nRecently, Transformer-based models like BERT\n(Devlin et al., 2018) have become a more popu-\nlar choice than LSTMs models, outperforming re-\ncurrent architectures in different natural language\nprocessing (NLP) tasks (Vaswani et al., 2017; Qiu\net al., 2020). Transformers can capture long-range\ndependencies with self-attention layers and they\ncan process each word of a sentence in parallel,\nreducing training time. After the successful appli-\ncation in NLP, vision Transformers like ViT (Doso-\nvitskiy et al., 2020) are also starting to become the\nmodel of choice in the ﬁeld of computer vision in\nplace of CNNs. In similar fashion, most recent\ncaptioning studies use the Transformer arquitec-\nture (Herdade et al., 2019; Cornia et al., 2020; Liu\net al., 2021), employing a vision Transformer as\nencoder together with an autoregressive language\nTransformer as decoder. Similarly to these models,\nthis work proposes a encoder-decoder Transformer\nmodel for the task of image captioning. However,\nunlike them, the proposed model incorporates a\npretrained V&L BERT to exploit cross-modal rep-\nresentations, encoding images along with textual\ncontext. Also differently from previous work, this\napproach explores retrieval-augmented generation,\ni.e., combining neural encoder-decoder methods\nwith traditional retrieval-based methods.\n3672\nD\u0003PDQ\u0003FURVVHV\u0003D\u0003VWUHHW\u0003XQGHU\u0003DQ\u0003XPEUHOOD\u0003DQG\u0003FDUU\\LQJ\u0003D\u0003OLWWOH\u0003ER\\\nD\u0003NLG\u0003ZLWK\u0003DQG\u0003XPEUHOOD\u0003RQ\u0003D\u0003VWUHHW\nD\u0003OLWWOH\u0003NLG\u0003KROGLQJ\u0003DQ\u0003XPEUHOOD\u0003RQ\u0003D\u0003VWUHHW\n\u0003D\u0003EODFN\u0003DQG\u0003ZKLWH\u0003SKRWR\u0003RV\u0003D\u0003FKLOG\u0003KROGLQJ\u0003DQ\u0003RSHQ\u0003XPEUHOOD\nD\u0003\\RXQJ\u0003SHUVRQ\u0003ZLWK\u0003DQ\u0003XPEUHOOD\u0003LV\u0003FURVVLQJ\u0003D\u0003EXV\\\u0003LQWHUVHFWLRQ\n(;75$\nD\u0003\\RXQJ\u0003ER\\\u0003KROGLQJ\u0003DQ\u0003XPEUHOOD\u0003LQ\u0003WKH\u0003UDLQ\nWKH\u0003NLWWHQ\u0003LV\u0003VWXFN\u0003LQ\u0003WKH\u0003ZKLWH\u0003EDVLQ\nD\u0003YHU\\\u0003FXWH\u0003FDW\u0003VLWWLQJ\u0003LQ\u0003D\u0003VLQN\u0003GULQNLQJ\u0003IURP\u0003D\u0003FXS\nD\u0003FDW\u0003WKDW\u0003LV\u0003LQ\u0003D\u0003ZKLWH\u0003VLQN\nD\u0003FDW\u0003VLWWLQJ\u0003LQ\u0003D\u0003VLQN\u0003RYHU\u0003D\u0003FXS\u0003RI\u0003VRPHWKLQJ\nD\u0003ZKLWH\u0003FDW\u0003LV\u0003O\\LQJ\u0003LQ\u0003D\u0003VLQN\n(;75$\nD\u0003FDW\u0003VLWWLQJ\u0003LQ\u0003D\u0003VLQN\u0003QH[W\u0003WR\u0003D\u0003ERZO\n(;75$\nD\u0003SDUNLQJ\u0003PHWHU\u0003FRYHUHG\u0003LQ\u0003VQRZ\u0003QH[W\u0003WR\u0003D\u0003FDU\nD\u0003YHKLFOH\u0003SDUNHG\u0003E\\\u0003D\u0003SDUNLQJ\u0003PHWHU\u0003LQ\u0003SLOH\u0003RI\u0003VQRZ\nD\u0003YHKLFOH\u0003SDUNHG\u0003QH[W\u0003WR\u0003D\u0003SDUNLQJ\u0003PHWHU\u0003EXULHG\u0003LQ\u0003VQRZ\u0003QHDU\u0003EXLOGLQJV\nWZR\u0003SDUNLQJ\u0003PHWHUV\u0003QHDUO\\\u0003EXULHG\u0003LQ\u0003KHDY\\\u0003VQRZ\nD\u0003SDUNLQJ\u0003PHWHU\u0003QH[W\u0003WR\u0003D\u0003FDU\u0003LV\u0003VXUURXQGHG\u0003E\\\u0003VQRZ\nD\u0003SDUNLQJ\u0003PHWHU\u0003LQ\u0003IURQW\u0003RI\u0003D\u0003EXLOGLQJ\u0003SLOHG\u0003LQ\u0003VQRZ\nFigure 4: Examples of captions generated by EXTRA conditioned on the input image and retrieved captions.\nV&L BERTs: Previous studies have proposed\npretrained Vision and Language (V&L) BERTs to\nlearn generic cross-modal representations of im-\nages and text, that can later be used for a vari-\nety of downstream V&L tasks (Bugliarello et al.,\n2020). Examples include LXMERT (Tan and\nBansal, 2019), VL-BERT (Su et al., 2019), Visual\nBERT (Li et al., 2019), OSCAR (Li et al., 2020),\nor UNITER (Chen et al., 2020), which were ap-\nplied to VQA and other V&L classiﬁcation tasks.\nGiven that these models are encoder-only Trans-\nformers, only few of them have been applied to\ngeneration tasks such as image captioning. In such\ncases, the generation is made from left to right by\nencoding the input image and using the textual in-\nput elements with uni-directional attention masks,\ni.e., starting with aCLS token with the rest of the to-\nkens masked, then considering the CLS token with\nthe predicted word (replaced by the corresponding\nmask token) and the remaining ones still masked,\nand so on (Li et al., 2020; Zhou et al., 2020).\nThe use of pretrained V&L BERTs, as encoders\nin the standard encoder-decoder captioning frame-\nwork, remains largely unexplored. The task of im-\nage captioning typically just considers single-input\nimages, and not image-text pairs to be encoded. In\nour work, a pretrained V&L encoder is used with\na decoder for image captioning, by leveraging not\njust the images as input but also retrieved captions.\nBesides pretrained V&L encoders, pretrained\nV&L encoder-decoder models have recently been\nproposed to tackle classiﬁcation and generation\ntasks, such as VL-T5 (Cho et al., 2021). Their cap-\ntioning approach is similar to the present paper, but\nVL-T5 uses object tags as textual inputs, whereas\nEXTRA is conditioned on retrieved captions.\nRetrieval-augmented Generation: The pro-\nposed approach is also similar to some studies on\nlanguage generation that predict the output condi-\ntioned on retrieved examples (Weston et al., 2018;\nGu et al., 2018; Khandelwal et al., 2019; Lewis\net al., 2020). For instance, this work relates to We-\nston et al. (2018), in which a sequence-to-sequence\nLSTM model, for dialog generation, encodes the\ncurrent input concatenated with the nearest re-\ntrieved response. Similarly, Izacard and Grave\n(2020) used an encoder-decoder Transformer condi-\ntioned on retrieved passages for open domain ques-\ntion answering. Retrieval-augmented generation is\ngaining traction in NLP but has only been explored\nfor image captioning by few studies (Wang et al.,\n2020; Fei, 2021; Ramos et al., 2021; Sarto et al.,\n2022; Ramos et al., 2022). Concurrent work pro-\nposed Transformer-based captioning models aug-\nmented with retrieval as well (Sarto et al., 2022;\nRamos et al., 2022). However, differently from\nthese previous studies, we encode the retrieved\ncaptions by exploiting cross-modal representations\nwith a V&L encoder.\n3673\n7 Conclusions\nWe propose EXTRA , a retrieval-augmented image\ncaptioning model that improves performance by\nexploiting cross-modal representations of the in-\nput image together with captions retrieved from a\ndatastore. EXTRA make uses of a pretrained V&L\nBERT, instead of an image-only encoder, combined\nwith a language decoder. To generate a caption, the\ndecoder attends to the cross-modal encoder fea-\ntures, containing information from image regions\nand also textual evidence from the retrieved cap-\ntions. Image captioning is therefore addressed as\nlanguage generation conditioned on vision and lan-\nguage inputs, instead of vision only. To evaluate\nthis model, EXTRA was assessed against strong\nencoder-decoder models in the area, and ablation\nstudies were also conducted. The experiments con-\nducted on the COCO dataset conﬁrmed the effec-\ntiveness of the proposed captioning approach.\nFor future work, we plan to explore the utility of\nEXTRA in out-of-domain and in few-shot learning\nsettings, since the retrieval component can be eas-\nily modiﬁed to include external datastores, without\nthe need to retrain the whole model. We also plan\nto explore how this approach can be adapted to\nother powerful vision and language encoders be-\nsides LXMERT. Finally, we will explore methods\nthat allow us to jointly train the retrieval mecha-\nnism with the full model in order to retrieve cap-\ntions that are more similar to the input image.\nLimitations\nPrevious work has shown that generative models\nsuffer from biases inherent to the data they are\ntrained on (Weidinger et al., 2021; Thoppilan et al.,\n2022). Likewise, our EXTRA model can suffer\nfrom biases present in the COCO image captioning\ndataset (Chen et al., 2015). Particularly, it has been\nshown that there is signiﬁcant gender imbalance\nin COCO, and that captioning models can exhibit\ngender bias ampliﬁcation (e.g., they are likely to\ngenerate the word “woman” in kitchen scenarios,\nand the word “man” in snowboarding scenes) (Hen-\ndricks et al., 2018; Zhao et al., 2017).\nHowever, differently from most captioning mod-\nels, EXTRA is a retrieval-augmented captioning\nmodel, and thus it has the potential to make pre-\ndictions beyond the training data, by relying on\ninformation from an external datastore. Still, the\ndatastore knowledge might also have inherent bias,\nas mentioned by previous studies on retrieval-\naugmented generation (Lewis et al., 2020). In\nthe paper, we show examples of such limitations\nwherein mismatched retrieved captions can bias the\nmodel towards incorrect predictions (see the results\nand appendix sections).\nAs a way to mitigate these limitations, we rec-\nommend analyzing the corresponding nearest cap-\ntions when using EXTRA, since the retrieved cap-\ntions can give useful insight of the bias involved\nin the generation process. EXTRA can provide in-\nterpretability through textual descriptions, whereas\nmost captioning models only provide explanations\nas visual attention maps.\nEXTRA also has the downside of focusing on\nan English-centric dataset. Captioning datasets\nare primarily available in English, and most im-\nage captioning models are trained on COCO or\nother english-centric datasets. To avoid hindered\nresearch on image captioning, it is important to con-\nsider multilingual captioning datasets that contain\nboth language-diverse captions and geographically-\ndiverse visual concepts (Thapliyal et al., 2022).\nAcknowledgements\nThis research was supported by the Portuguese Re-\ncovery and Resilience Plan (RRP) through project\nC645008882-00000055 (Responsible.AI), and also\nthrough Fundação para a Ciência e Tecnologia\n(FCT), namely through the Ph.D. scholarship with\nreference 2020.06106.BD, as well as through the\nINESC-ID multi-annual funding from the PIDDAC\nprogramme with reference UIDB/50021/2020.\nReferences\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016. Spice: Semantic proposi-\ntional image caption evaluation. In Proceedings\nof the European Conference on Computer Vision ,\npages 382–398. Springer.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention\nfor image captioning and visual question answering.\nIn Proceedings of the Computer Vision and Pattern\nRecognition Conference, pages 6077–6086.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nManuele Barraco, Matteo Stefanini, Marcella Cornia,\nSilvia Cascianelli, Lorenzo Baraldi, and Rita Cuc-\n3674\nchiara. 2022. Camel: Mean teacher learning for im-\nage captioning. arXiv preprint arXiv:2202.10492.\nEmanuele Bugliarello, Ryan Cotterell, Naoaki\nOkazaki, and Desmond Elliott. 2020. Multimodal\npretraining unmasked: Unifying the vision and\nlanguage berts. arXiv preprint arXiv:2011.15124.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\nC Lawrence Zitnick. 2015. Microsoft coco captions:\nData collection and evaluation server.arXiv preprint\narXiv:1504.00325.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In Proceedings of the Eu-\nropean Conference on Computer Vision, pages 104–\n120.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.\nUnifying vision-and-language tasks via text genera-\ntion. arXiv preprint arXiv:2102.02779.\nMarcella Cornia, Matteo Stefanini, Lorenzo Baraldi,\nand Rita Cucchiara. 2020. Meshed-memory trans-\nformer for image captioning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 10578–10587.\nMichael Denkowski and Alon Lavie. 2014. Meteor uni-\nversal: Language speciﬁc translation evaluation for\nany target language. In Proceedings of the Workshop\non Statistical Machine Translation, pages 376–380.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929.\nDesmond Elliott and Arjen de Vries. 2015. Describing\nimages using inferred visual dependency represen-\ntations. In Proceedings of the Joint Conference of\nthe Annual Meeting of the Association for Computa-\ntional Linguistics and the International Joint Confer-\nence on Natural Language Processing, pages 42–52.\nAli Farhadi, Mohsen Hejrati, Mohammad Amin\nSadeghi, Peter Young, Cyrus Rashtchian, Julia\nHockenmaier, and David Forsyth. 2010. Every pic-\nture tells a story: Generating sentences from images.\nIn Proceedings of the European Conference on Com-\nputer Vision, pages 15–29. Springer.\nZhengcong Fei. 2021. Memory-augmented image cap-\ntioning. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, pages 2–9.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\ntor OK Li. 2018. Search engine guided neural ma-\nchine translation. In Proceedings of the Conference\nof the Association for the Advancement of Artiﬁcial\nIntelligence, pages 5133–5140.\nLisa Anne Hendricks, Kaylee Burns, Kate Saenko,\nTrevor Darrell, and Anna Rohrbach. 2018. Women\nalso snowboard: Overcoming bias in captioning\nmodels. In Proceedings of the European Conference\non Computer Vision, pages 771–787.\nSimao Herdade, Armin Kappeler, Koﬁ Boakye, and\nJoao Soares. 2019. Image captioning: Trans-\nforming objects into words. arXiv preprint\narXiv:1906.05963.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nMicah Hodosh, Peter Young, and Julia Hockenmaier.\n2013. Framing image description as a ranking task:\nData, models and evaluation metrics. Journal of Ar-\ntiﬁcial Intelligence Research, 47:853–899.\nMD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shi-\nratuddin, and Hamid Laga. 2019. A comprehensive\nsurvey of deep learning for image captioning. ACM\nComputing Surveys, 51(6):1–36.\nLun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong\nWei. 2019. Attention on attention for image caption-\ning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 4634–4643.\nGautier Izacard and Edouard Grave. 2020. Lever-\naging passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\nBillion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. arXiv preprint arXiv:1911.00172.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2016. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\narXiv preprint arXiv:1602.07332.\nGirish Kulkarni, Visruth Premraj, Vicente Ordonez,\nSagnik Dhar, Siming Li, Yejin Choi, Alexander C\nBerg, and Tamara L Berg. 2013. Babytalk: Under-\nstanding and generating simple image descriptions.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, 35(12):2891–2903.\n3675\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Proceedings of\nthe Conference on Neural Information Processing\nSystems, 33:9459–9474.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. arXiv preprint arXiv:1908.03557.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020. OSCAR: Object-\nsemantics aligned pre-training for vision-language\ntasks. In Proceedings of the European Conference\non Computer Vision, pages 121–137.\nWei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu,\nand Jing Liu. 2021. Cptr: Full transformer\nnetwork for image captioning. arXiv preprint\narXiv:2101.10804.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. Proceedings of the Con-\nference on Neural Information Processing Systems ,\n24:1143–1151.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, pages 1–26.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In Proceedings\nof the International Conference on Machine Learn-\ning, pages 8748–8763.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nRita Ramos, Bruno Martins, Desmond Elliott,\nand Yova Kementchedjhieva. 2022. Small-\nCap: Lightweight image captioning prompted\nwith retrieval augmentation. arXiv preprint\narXiv:2209.15323.\nRita Parada Ramos, Patrícia Pereira, Helena Moniz,\nJoao Paulo Carvalho, and Bruno Martins. 2021. Re-\ntrieval augmentation for deep neural networks. Pro-\nceedings of the International Joint Conference on\nNeural Networks.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. Proceed-\nings of the Conference on Neural Information Pro-\ncessing Systems, 28:91–99.\nSteven J Rennie, Etienne Marcheret, Youssef Mroueh,\nJerret Ross, and Vaibhava Goel. 2017. Self-critical\nsequence training for image captioning. In Proceed-\nings of the Computer Vision and Pattern Recognition\nConference, pages 7008–7024.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, et al.\n2015. Imagenet large scale visual recognition chal-\nlenge. International Journal of Computer Vision ,\n115(3):211–252.\nSara Sarto, Marcella Cornia, Lorenzo Baraldi, and\nRita Cucchiara. 2022. Retrieval-augmented trans-\nformer for image captioning. arXiv preprint\narXiv:2207.13162.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2019. Vl-bert: Pre-\ntraining of generic visual-linguistic representations.\narXiv preprint arXiv:1908.08530.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. arXiv preprint arXiv:1908.07490.\nAshish V Thapliyal, Jordi Pont-Tuset, Xi Chen, and\nRadu Soricut. 2022. Crossmodal-3600: A massively\nmultilingual multimodal evaluation dataset. arXiv\npreprint arXiv:2205.12522.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al. 2022. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the Conference on Neu-\nral Information Processing Systems , pages 5998–\n6008.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. CIDEr: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition, pages 4566–4575.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and\nDumitru Erhan. 2016. Show and tell: Lessons\nlearned from the 2015 mscoco image captioning\n3676\nchallenge. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 39(4):652–663.\nLi Wang, Zechen Bai, Yonghua Zhang, and Hongtao\nLu. 2020. Show, recall, and tell: Image captioning\nwith recall mechanism. Proceedings of the Confer-\nence of the Association for the Advancement of Arti-\nﬁcial Intelligence, 34(07):12176–12183.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGrifﬁn, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\net al. 2021. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359.\nJason Weston, Emily Dinan, and Alexander H Miller.\n2018. Retrieve and reﬁne: Improved sequence\ngeneration models for dialogue. arXiv preprint\narXiv:1808.04776.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nEMNLP: System Demonstrations, pages 38–45.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, attend and tell:\nNeural image caption generation with visual atten-\ntion. In Proceedings of the International conference\non Machine Learning, pages 2048–2057.\nTing Yao, Yingwei Pan, Yehao Li, and Tao Mei. 2018.\nExploring visual relationship for image captioning.\nIn Proceedings of the European Conference on Com-\nputer Vision, pages 684–699.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. VinVL: Making visual represen-\ntations matter in vision-language models. Proceed-\nings of the Computer Vision and Pattern Recognition\nConference.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente\nOrdonez, and Kai-Wei Chang. 2017. Men also\nlike shopping: Reducing gender bias ampliﬁca-\ntion using corpus-level constraints. arXiv preprint\narXiv:1707.09457.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\nﬁed vision-language pre-training for image caption-\ning and vqa. In Proceedings of the Conference of the\nAssociation for the Advancement of Artiﬁcial Intelli-\ngence, volume 34, pages 13041–13049.\nA Cross-Attention\nIn Section 5.1, we quantiﬁed how much attention\nEXTRA pays to the encoded image and retrieved\ncaptions. We also quantify this for the two other\n0 1 2 3\nLayer\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Average attention weight\nVisual Textual\nFigure 5: Cross-attention for the variant of EXTRA\nthat that encodes an empty caption.\n0 1 2 3\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Average attention weight\nVisual Textual\nFigure 6: Cross-attention for the variant of EXTRA\nthat that encodes a random caption.\nvariants of EXTRA which encode irrelevant cap-\ntions, using either an empty or a random caption.\nFigures 5 and 6 show the average cross-attention\nweights from the decoder to the outputs of the en-\ncoder in respect to the visual V and textual L out-\nputs, respectively for the empty and random cap-\ntion encoding. Contrary to the ﬁndings presented\nin Section 5.1, regarding the encoding of retrieved\ncaptions, in this scenario the two variants pay more\nattention to the visual outputs instead.\nFor details on how we calculated the correspond-\ning attention weights, we present the corresponding\nformula. Speciﬁcally, we calculated the average\nof the cross-modal attention C across either the\nnumber of image regions or the sub-words in the\nencoder at each of T time-step of generating a cap-\ntion and across each of the H = 12attention heads.\n3677\nThis calculation happens independently for each of\nthe L= 4layers in the decoder:\nA(CL,V ) = 1\nH\nH∑\nj=1\n1\nT\nT∑\nt=1\n|V|∑\ni=1\nαL\nj,t→i. (2)\nA(CL,T) = 1−A(CL,V ). (3)\nB More Examples\nFigure 7 shows additional examples of the captions\ngenerated by EXTRA considering the retrieved cap-\ntions, against the other two variants: encoding an\nempty and random caption instead. For the ﬁrst\nimage, the two variants fail to recognize that the im-\nage shows kids playing basketball (perhaps given\nthe small size of the ball), whereas EXTRA was\nable to identify it by having that information in the\nretrieved captions. In the second image7, the two\nvariants produced the error of generating sandwich\nwhile EXTRA correctly mentioned hot-dog, simi-\nlar to the retrieved captions. EXTRA considers the\nsemantics from the nearest captions retrieved dur-\ning generation, sometimes even copying an entire\nsentence, as shown in Figure 9.\nFigure 8 shows examples where the retrieved\ncaptions mislead the model. We note however that\nEXTRA is also able to succeed, despite the mis-\nmatch from retrieved captions, as seen in Figure 10.\n7The person was blurred for privacy concerns.\n3678\nCLS SEP\n“EXTRA” \n(empty caption)\na group of people walking down a street\na group of children play a game of basketballa group of young people playing a game of basketballyoung children playing a basketball game with the ball flyinga group of people play a game inside on a courtyoung kids playing a game of basketball on a basketball court\na group of children playing a game of \nbasketball\nEXTRA\ntwo men are playing wii together \nin the living room\n \n“EXTRA” \n(random caption)\na group of young people walking across a \nstreet\nCLS SEP\n“EXTRA” \n(empty caption)\na man holding a sandwich in his hand\na man putting a hot dog in a bun at a restaurantholding a hot dog in a <unk> bun with a napkina person holds a hot dog with onions up to the cameraa person holding a bottle of drink and a hotdog in a napkin at a hot dog standa man holds a hotdog near a food stand and parking lot\na man holding a hot dog in his hand\nEXTRA\na man playing baseball in the \nmiddle of a pitch\n“EXTRA” \n(random caption)\na man holding a sandwich in his hands\nFigure 7: Examples of generated captions byEXTRA and the other two variants (empty and random caption). Better\nimage captions are obtained from generating with retrieval augmentation.\n3679\nCLS SEP\n“EXTRA” \n(empty caption)\na dog running on the beach with a leash\na dog running on sand with a frisbee in its moutha dog with a toy in its mouth while running across a beacha very big cute dog running on the beacha dog <unk> running on beach being chaseda dog running on a beach with a toy in its mouth\na dog running on the beach with a ball in \nits mouth\nEXTRA\na street sign is hanging on a partially rusty pole \n“EXTRA” \n(random caption)\na brown and white dog running on sand\nCLS SEP\n“EXTRA” \n(empty caption)\na couple of plates of food on a table\na conveyor belt topped of doughnuts inside of a kitchena conveyor belt topped with deep fried donutsgourmet sandwiches of meat and mushrooms on fresh rollssome doughnuts are being made on a conveyor belta hot dog covered in toppings sitting next to beer\na bunch of doughnuts that are on a table\nEXTRA\nroadway intersection near large brick building in city\n“EXTRA” \n(random caption)\na plate of food on a table at a restaurant\nFigure 8: Qualitative results in which the retrieved captions are not that related to the input image.\n3680\na white polar bear laying on top of a rocka white bear sleeping on a big rocka white polar bear is sleeping on a rocka white bear sleeping on a rocky ledgea white bear is laying out on the rocks\nEXTRA\na white polar bear laying on \ntop of a rock\na horse standing in a mountain paddock during the daya horse is trotting along a hilly areaa horse running freely across a mountain landscapea horse out in an open space with mountains in the backgrounda brown horse standing on top of a lush green hillside\nEXTRA\na brown horse standing on \ntop of a lush green hillside\na police car sits parked next to a fire hydranta white police car parked right by a fire hydranta statue of a bear on a car used as a warning about the bearsa police patrol car parked next to a fire hydranta police car parked next to a fire hydrant\nEXTRA\na police car parked next to a \nfire hydrant\nFigure 9: Examples of generated captions for which EXTRA copied from the retrieved captions.\na young boy putting the telephone up to a toy 's eara young boy holds a doughnut to his facea young boy holding a sandwich up to his facea young man on a chair biting into a sandwicha boy leans over a kitchen table while eating\nEXTRA\na boy sitting on a chair \nholding a teddy bear\nblack dog laying down near a black and white cata gray and black cat sleeping while laying downa couple of black cats laying down on a beda black cat sleeps on an old coucha large furry black and brown cat sleeping on a chair\nEXTRA\na black cat laying on top of a \nbed\na group of people eat cake in an officemen at an office appear confused by a presentationman in an office setting eating a large hotdoga man in an office attempts to eat a hotdog in one bitean office worker taking a selfie with his iphone\nEXTRA\na man sitting in front of a \nlaptop computer\nFigure 10: Examples where EXTRA is able to succeeded even with mismatches from retrieved captions.\n3681"
}