{
    "title": "Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning",
    "url": "https://openalex.org/W4226075089",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5002856966",
            "name": "Jishnu Ray Chowdhury",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5031843438",
            "name": "Yong Zhuang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5317838346",
            "name": "Deleted Author ID",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3106530084",
        "https://openalex.org/W2557480356",
        "https://openalex.org/W2947683321",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3122794438",
        "https://openalex.org/W2970772045",
        "https://openalex.org/W2984811147",
        "https://openalex.org/W6767401011",
        "https://openalex.org/W3126960149",
        "https://openalex.org/W3022065131",
        "https://openalex.org/W2755124548",
        "https://openalex.org/W2890397703",
        "https://openalex.org/W7053534501",
        "https://openalex.org/W3170237181",
        "https://openalex.org/W6759579507",
        "https://openalex.org/W6796581206",
        "https://openalex.org/W3122644463",
        "https://openalex.org/W2798139452",
        "https://openalex.org/W3034466976",
        "https://openalex.org/W1980519283",
        "https://openalex.org/W3024771753",
        "https://openalex.org/W2945232141",
        "https://openalex.org/W6793601707",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W6746055214",
        "https://openalex.org/W6763653964",
        "https://openalex.org/W3177034458",
        "https://openalex.org/W6767171234",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W2741049976",
        "https://openalex.org/W2969496383",
        "https://openalex.org/W6940318546",
        "https://openalex.org/W6898505805",
        "https://openalex.org/W2531908596",
        "https://openalex.org/W174630521",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W3002104146",
        "https://openalex.org/W3085177480",
        "https://openalex.org/W3096331697",
        "https://openalex.org/W2149327368",
        "https://openalex.org/W2252001469",
        "https://openalex.org/W3160563524",
        "https://openalex.org/W6785494305",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2534253848",
        "https://openalex.org/W2891765028",
        "https://openalex.org/W2167170026"
    ],
    "abstract": "Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation.",
    "full_text": "Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional\nPrompt Tuning\nJishnu Ray Chowdhury 1*, Yong Zhuang 2, Shuyi Wang 2\n1 University of Illinois, at Chicago\n2 Bloomberg\njraych2@uic.edu, yzhuang52@bloomberg.net, swang1072@bloomberg.net\nAbstract\nParaphrase generation is a fundamental and long-standing\ntask in natural language processing. In this paper, we con-\ncentrate on two contributions to the task: (1) we propose Re-\ntrieval Augmented Prompt Tuning (RAPT) as a parameter-\nefﬁcient method to adapt large pre-trained language mod-\nels for paraphrase generation; (2) we propose Novelty Con-\nditioned RAPT (NC-RAPT) as a simple model-agnostic\nmethod of using specialized prompt tokens for controlled\nparaphrase generation with varying levels of lexical novelty.\nBy conducting extensive experiments on four datasets, we\ndemonstrate the effectiveness of the proposed approaches for\nretaining the semantic content of the original text while in-\nducing lexical novelty in the generation.\nIntroduction\nThe task of paraphrase generation aims at rephrasing a given\ntext while retaining its meaning. The task has several ap-\nplications including text simpliﬁcation, semantic parsing,\nquery re-writing, and data augmentation.\nRecently, the use of pre-trained Transformer-based lan-\nguage models has become nearly ubiquitous for different\nnatural language processing (NLP) tasks (Devlin et al. 2019;\nRadford et al. 2019) including paraphrase generation (Wit-\nteveen and Andrews 2019; West et al. 2021). While the stan-\ndard method of utilizing a pre-trained language model for\nNLP tasks is to ﬁne-tune all the parameters in the model,\nit is not the most parameter-efﬁcient. For example, Mega-\ntronLM (Shoeybi et al. 2019) has more than 11 billion pa-\nrameters to ﬁne-tune. Various methods have emerged to\nutilize large-scale pre-trained language models in a more\nparameter-efﬁcient manner. Such methods include variants\nof adapter tuning (Houlsby et al. 2019), prompt tuning (Shin\net al. 2020; Li and Liang 2021; Lester, Al-Rfou, and Con-\nstant 2021), and Low Rank Adaptation (LoRA) (Hu et al.\n2021). Particularly, prompt tuning and LoRA can cut down\nthe number of trainable parameters by a factor of thousands.\nThese approaches are particularly important today as pre-\ntrained models continuously become larger than before. In\n*The work was done during an internship with Bloomberg.\nCorrespondence to: Jishnu Ray Chowdhury, Yong Zhuang, Shuyi\nWang.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nthe direction of parameter-efﬁcient methods, we propose Re-\ntrieval Augmented Prompt Tuning (RAPT) as a method of\naugmenting prompt tuning with kNN-based retrieved exam-\nples to further enhance the quality of paraphrases.\nBesides adaptation of pre-trained language models, we\nalso explicitly focus on the novelty of generated paraphrases\nbecause trivial paraphrases with minimal changes may not\nbe as helpful for applications such as data augmentation. We\nconsider the novelty of a paraphrase to be associated with\nthe edit distance of the paraphrase from the input. Roughly,\nthe more edits (word deletion, word insertion, substitution,\netc.) there are in a paraphrase, the more “novel” we consider\nit to be. Paraphrases can be generated with different levels of\nnovelty. For example, simply changing one word would still\ncount as paraphrasing. Usually, however, the level of lexi-\ncal novelty of a generated paraphrase is left upon the model\nto decide. In this work, we instead propose a simple model-\nagnostic method to put more control on the users themselves\nto decide the level of novelty of the generation. To this end,\nwe propose Novelty Conditioned RAPT (NC-RAPT) that\nuses specialized prompts for different levels of novelty for\nnovelty-controlled paraphrase generation. Overall, we make\nthe following contributions:\n1. We propose Retrieval Augmented Prompt Tuning\n(RAPT) as a parameter-efﬁcient method for paraphrase\ngeneration.\n2. We propose Novelty Conditioned RAPT (NC-RAPT)\nthat uses specialized prompts to generate paraphrases\nwith different levels of novelty.\nTask Deﬁnition\nWe deﬁne paraphrase generation as a sequence-to-sequence\nproblem. Given some input sequence of tokens x1:n =\n(x1;x2;x3;:::;x n) as the input query, we want a model\nto paraphrase the input into another sequence y1:m =\n(y1;y2;y3;:::;y m) serving as the output.\nIn our experiments, we mainly explore GPT-based models\nwhich are pre-trained on autoregressive language modeling\nor auto-completion. Thus, we frame our problem similar to\nan auto-completion task such that the downstream task re-\nmains similar to the pre-training task (potentially making the\ntransfer of pre-trained knowledge easier).\nTo frame paraphrasing as an auto-completion task for\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10535\nan autoregressive language model, we can design an in-\nput prompt such as “Input: x1;x2;x3;:::;x n \\n Para-\nphrase: ”. The task of the model is to auto-complete the\ngiven prompt. The completed sequence will look like: “In-\nput: x1;x2;x3;:::;x n \\n Paraphrase: y1;y2;y3;:::;y m”.\nWe treat the generated sequence y1:m as the paraphrase of\nthe original input x1:n.\nIn this overall sequence, “input: ” is the prompt preﬁx,\n“\\n Paraphrase: ” is theprompt inﬁx, “x1;x2;x3;:::;x n”\nis the input, and “y1;y2;y3;:::;y m” is the paraphrased out-\nput. The preﬁx and inﬁx together forms the prompt template:\n“Input:\n\\n Paraphrase: ”.\nWe can further generalize the input prompt format\nby formalizing a generic sequence of tokens denot-\ning the prompt preﬁx as p1:s = ( p1;p2;p3;:::;p s)\nand a generic sequence of tokens denoting\nthe prompt inﬁx as q1:t = ( q1;q2;q3;:::;q t).\nOverall the generalized input prompt will be:\n“p1;p2;p3;:::;p s;x1;x2;x3;:::;x n;q1;q2;q3;:::;q t”.\nBaselines\nIn this paper, we explore the following methods to adapt\nlarge pre-trained language models. Particularly, we use GPT-\nbased models to adapt for paraphrase generation.\nFine Tuning\nFine Tuning (FT) is the standard method of adapting a\npre-trained model. In this strategy all the pre-trained pa-\nrameters undergo gradient updates in some downstream\ntasks (in this case, paraphrase generation). In our imple-\nmentation, we manually design an input prompt as “In-\nput: x1;x2;x3;:::;x n \\n Paraphrase: ” where x1:n =\nx1;x2;x3;:::;x nis the input as deﬁned in the previous sec-\ntion. We trained the model in the auto-completion frame-\nwork as discussed in the previous section.\nAdapter Tuning (AT)\nAT introduces some new sublayers (i.e., adapter layers) act-\ning as low-rank bottlenecks within each Transformer layer.\nGenerally, instead of tuning all parameters of a pre-trained\nmodel, AT focuses on tuning mainly the adapter layers. Fol-\nlowing Houlsby et al. (2019), we only tune the adapter layer\nparameters and the layer normalization parameters during\ntraining. We train the model with the same input prompt for-\nmat and the same framework as used in ﬁne tuning.\nLow Rank Adaptation (LoRA)\nGiven a pre-trained matrix Wx ∈Rd\u0002k, LoRA (Hu et al.\n2021) constrains the update of Wx through a low-rank ma-\ntrix W\u000e ∈Rd\u0002k. More precisely, instead of directly updat-\ning Wx, LoRA ﬁrst reformulates Wx to W0\nx = Wx + W\u000e,\nand then only updates the parameters involved in the con-\nstruction of W\u000e. W\u000e itself is constructed through the mul-\ntiplication of two matrices B ∈Rd\u0002r and A ∈Rr\u0002k as:\nW\u000e = BA. We maintain W\u000e as a low-rank matrix by keep-\ning r ≪ min(d;k). Thus instead of tuning dk parameters\nwe only need to tune r·(d+ k) parameters. Similar to Hu\net al. (2021), we only apply LoRA to the query transforma-\ntion and value transformation matrices in the multi-head at-\ntention sublayers. We train the model in the same framework\nas used in ﬁne tuning.\nPrompt Tuning\nLarge-scale pre-trained language models can already con-\ntain substantial implicit knowledge for several natural lan-\nguage tasks before further ﬁne tuning. For example, Rad-\nford et al. (2019) show that language models can serve as\nunsupervised multi-task learners. They show that using task-\nspeciﬁc natural language prompts enable the pre-trained lan-\nguage models to do speciﬁc tasks like translation or summa-\nrization without further training. However, manually design-\ning prompt templates is not ideal because it requires human\ninvolvement and the ideal prompt may not correspond to hu-\nman intuitions. Instead recent work (Shin et al. 2020; Li and\nLiang 2021; Lester, Al-Rfou, and Constant 2021) focuses\non automatically tuning the prompt template itself. More-\nover, some of these works also tend to tune only the prompt\ntemplate parameters keeping all the pre-trained parameters\nfrozen. As such, these methods also serve as lightweight\nparameter-efﬁcient options to adapt large language models.\nFor our implementation of prompt tuning, we\nconsider the generalized input prompt format for-\nmalization as discussed in the previous section:\n“p1;p2;p3;:::;p s;x1;x2;x3;:::;x n;q1;q2;q3;:::;q t”.\nThe initial representation after passing this input prompt\nsequence through the embedding layer can be presented\nas: “e( p1);:::;e (ps);e(x1);:::;e (xn);e(q1);:::;e (qt)”.\nSimilar to Lester, Al-Rfou, and Constant (2021), we directly\ninitialize and tune the preﬁx (e( p1);:::;e (ps)) and inﬁx\nembeddings (e(q1);:::;e (qt)). When using prompt tuning\nalone, we only tune the preﬁx and inﬁx embeddings (unique\nembedding vectors for each position) while keeping all\nother parameters frozen. We train the model in the same\nauto-completion framework as used in ﬁne tuning.\nLoRA + Prompt Tuning (LPT)\nIn this approach, we apply both prompt tuning and LoRA\nat the same time. We tune both the preﬁx-inﬁx parameters\nand the newly introduced LoRA parameters. Both LoRA and\nprompt tuning tune only a minor fraction of the total param-\neters. Thus, we can easily combine them without increasing\nthe overall parameter count signiﬁcantly.\nProposed Approaches\nIn this section, we introduce our proposed approaches:\nRetrieval Augmented Prompt Tuning (RAPT) and Nov-\nelty Conditioned Retrieval Augmented Prompt Tuning (NC-\nRAPT).\nRetrieval Augmented Prompt Tuning (RAPT)\nIn this section, we introduce our ﬁrst proposed approach,\nRAPT, which is a synergy between an automated example\nretrieval strategy and an example-augmented prompt tuning\nstrategy. We discuss both of them below. Like LPT, we also\nadd LoRA because it does not add too many parameters (Ta-\nble 2) and allows more tuning ﬂexibility.\n10536\nExample-Augmented Prompts: Brown et al. (2020)\nshow that large-scale pre-trained auto-regressive models like\nGPT3 can locate or learn to perform a task from only a few\ninput-output pairs exemplifying the task given in the input\nprompt context without further gradient updates. For exam-\nple, let us say we have an input text sequence X (where\nX = x1:n) that we want to paraphrase, and we have two ex-\nemplar paraphrase pairs (X1;Y1) and (X2;Y2). As such the\ninput prompt to a GPT3-like model can be of the form:\nZ = [p1:s; X2; q1:t; Y2; p1:s; X1; q1:t; Y1; p1:s; X; q1:t];\n(1)\nwhere Z is the full input prompt sequence, p1:s is the pre-\nﬁx sequence for a sample input, andq1:tis the inﬁx sequence\nmarking the end of the sample input and the start of the sam-\nple output. [:; :] denotes concatenation.\nWe hypothesize that similarly augmenting the input\nprompt with task exemplars can be still beneﬁcial for smaller\npre-trained language models (e.g., GPT2). Hu et al. (2021)\nshow that full data ﬁne tuning, LoRA, or prompt tun-\ning on GPT3 can still outperform few-shot approaches.\nThus, instead of applying and exploring example-augmented\nprompts in few shot settings, we use example-augmented\nprompts in our full data prompt tuning setup. That is, we\ncan treat the embeddings ofp1:sand q1:t(the preﬁx and inﬁx\nembedding parameters as used in our prompt tuning setup as\ndiscussed before) as freely trainable parameters. This strat-\negy removes the need for manually designing preﬁx and in-\nﬁx sequences to best describe the task and delimit the differ-\nent example inputs and outputs.\nHowever, simply using the prompt design format as in Eq.\n1 is not without its problems. During prompt tuning the pre-\nﬁx length (k) and/or the inﬁx length (l ) can be quite large\n(e.g., 250). Since the preﬁx and inﬁx are repeated for every\nexample in Eq. 1, the total length of the input prompt can in-\ncrease signiﬁcantly. Long sequences can slow down training\nand cause memory issues. To solve this issue we revise the\nprompt design format in Eq. 1 as:\nZ = [d1:m; p1:s; X2; q1:t; Y2; p1:s; X1; q1:t; Y1; p1:s; X; q1:t]:\n(2)\nIn this format, d1:m serves as a global preﬁx serving as a\nsort of task description. We try to maintain s;t ≪ msuch\nthat most of the tunable prompt embedding parameters are\nconcentrated on the emebddings ofd1:m, allowing us to keep\nsand tsmall. Thus, given the smallsand t, the overall input\nprompt sequence length does not increase as much whenp1:s\nand q1:t are repeated.\nExample Retrieval: Above, we described how to design\nprompts to incorporate input-output examples of a task serv-\ning as task exemplars. However, the question remains open:\nwhere do we get the task examples?We can always try to\nconstruct the task examples manually, but that would again\nrequire human involvement.\nInstead, we consider an automated method. We hypothe-\nsize that given an input sequence X, input-output example\npairs that are most similar toXwould be the most suitable in\nserving as examples to guide paraphrasing of X. Thus, we\nuse a k-Nearest-Neighbor (kNN) algorithm to retrieve the\ntop-k example pairs that have the most semantically similar\n(to X) inputs from the training dataset.\nMore precisely, we ﬁrst encode each input text in\nthe training data into a single sentence vector using\nsentence-transformers1 (Reimers and Gurevych 2019) (we\nuse paraphrase-mpnet-base-v2 as the pre-trained\nweights). Similarly, we also encode any given input X\nthat we want to paraphrase. We select top k examples\n((X1;Y1);(X2;Y2);:::; (Xk;Yk)) from the training set\nbased on the cosine similarity scores between the embed-\ndings of the training examples and that of the given inputX.\nWe add the retrieved examples to the prompt (as in Eq. 2) in\nthe ascending order of their cosine similarity scores with the\ninput X. In practice, we set k = 2given the disadvantages\nof long sequences that we discussed.\nNovelty Conditioned RAPT (NC-RAPT)\nWhile there are prior works (Gupta et al. 2018; Kumar et al.\n2019) that have focused on improving the novelty of para-\nphrase generation, there are not many works focusing on\ncontrolling the level of novelty. In this work, we use a simple\nmodel-agnostic framework to control the novelty level of the\ngeneration. This framework allows us at least two beneﬁts.\n1. The framework puts more control over the user in decid-\ning the trade-offs they want in increasing the novelty of\nparaphrasing.\n2. The framework can also enforce a model to generate\nhighly novel paraphrases and thus compete with prior\nnovelty-enhancing methods.\nOur framework requires two broad steps. In the ﬁrst step,\nwe classify each sample in the dataset according to the nov-\nelty of the ground truth paraphrase. We use Translation Edit\nRate (TER) (Olive 2005; Snover et al. 2006) between the in-\nput sequence and the ground truth sequence to quantify the\nnovelty of the ground truth. We use three classes of novelty\nlevels: High, Medium, and Low. We classify each sample\namong the three novelty level classes based on the TER val-\nues between the input and the ground truth. If the TER is\n≥0:4, we classify the sample as High. If the TER is >0:2\nand <0:4, we classify the sample asMedium. If the TER is\n≤0:2, we classify the sample as Low. The thresholds were\nchosen intuitively based on qualitative analysis.\nIn the second step, during training, we use different pre-\nﬁx and inﬁx embeddings for different novelty classes. Let\nus say c(X;Y) denotes the novelty-class of an input-output\nparaphrase-pair (X;Y ), pc\n1:s denotes the preﬁx sequence for\nnovelty class c, and qc\n1:t denotes the inﬁx sequence for nov-\nelty class c. Given these notations, during training, we refor-\nmulate Eq. 2 for novelty-conditioned generation as:\nZ = [d1:m; p\nc(X2;Y2)\n1:s ; X2; q\nc(X2;Y2)\n1:t ; Y2;\np\nc(X1;Y1)\n1:s ; X1; q\nc(X1;Y1)\n1:t ; Y1; p\nc(X;Y )\n1:s ; X; q\nc(X;Y )\n1:t ]:\n(3)\nHere Y is the ground truth paraphrase for X. We keep the\nsame global preﬁx sequence d1:m for all novelty classes\nto save parameters. As can be understood from Eq. 3, the\n1https://www.sbert.net/docs/pretrained models.html\n10537\nInput Why do all of my questions get\nmarked as needing improvement\nLPT why do my questions get marked as\nneeding improvement\nRAPT why are my questions still being\nmarked as needing improvement\nNC-RAPT (High) why is my question being marked\nas needs to be improved\nNC-RAPT (Low) why do all of my questions get\nmarked as needing improvement\nTable 1: Examples of generated paraphrases by different\nmodels from Quora Question Pairs dataset.\nNumber of Parameters\nModel GPT2 Medium GPT2 Large\nFine Tuning 354,823,168 774,030,080\nAdapter Tuning 25,303,040 47,437,312\nLoRA Tuning 786,432 1,474,560\nPrompt Tuning 270,336 337,920\nLPT 1,056,768 1,812,480\nRAPT 1,056,768 1,812,480\nNC-RAPT 1,089,536 1,853,440\nTable 2: Comparison on the number of trainable parameters\nfor different adaptation methods.\nmodel learns from a ground truth with novelty class conly\nwhen the corresponding preﬁx (p c\n1:s) and inﬁx (q c\n1:t) se-\nquences for the same novelty class care used. As such, dur-\ning inference, the model learns to generate a paraphrase of\nnovelty class cif the preﬁx (pc\n1:s) and inﬁx (qc\n1:t) sequences\nfor novelty class cis used for the input. During inference, to\ngenerate a paraphrase of a given input X with novelty class\nc, we simply use pc\n1:s instead of p\nc(X;Y )\n1:s and qc\n1:t instead of\nq\nc(X;Y )\n1:t in Eq. 3. NC-RAPT is built upon RAPT. So every\nother details are the same as for RAPT. We show some ex-\nample model generations in Table 1. In Table 2, we show the\ntotal trainable parameters for all the adaptation methods that\nwe try over GPT2 medium and GPT2 large.\nExperiments\nDataset\nWe use four datasets for our experiments as discussed below.\nDetails of dataset split sizes are presented in Table 3.\n• Quora Question Pairs 50K split (QQP 50K) 2: Quora\n2https://quoradata.quora.com/First-Quora-Dataset-Release-\nQuestion-Pairs\nDataset Name Training Validation Test\nQQP 50K 46,000 4,000 4,000\nQQP 140K 134,206 5255 5255\nMSRPC 2,203 550 1,147\nParaSCI ACL 28,883 2753 2,345\nTable 3: Dataset split sizes\nQuestion Paris (QQP) is a paraphrase detection dataset.\nWe only use the true paraphrase pairs. We use the 50K\ndataset split as used in Gupta et al. (2018).3\n• Quora Question Pairs 140K split (QQP 140K): We\nalso use QQP with a different split size (QQP 140K) as\nused by Hosking and Lapata (2021). 4\n• Microsoft Research Paraphrase Corpus (MSRPC):\nMSRPC (Dolan, Quirk, and Brockett 2004) is another\nparaphrase detection corpus. We use only the true para-\nphrase pairs for paraphrase generation. We use the ofﬁ-\ncial train-test split.\n• ParaSCI-ACL: ParaSCI-ACL (Dong, Wan, and Cao\n2021) is a paraphrase generation dataset in the scientiﬁc\ndomain. We use the ofﬁcial split.5\nEvaluation Metrics\nWe use different evaluation metrics to account for different\naspects of paraphrase quality as below:\n• BLEU: Like most prior work, we use BLEU4 (Papineni\net al. 2002) to measure similarity between prediction and\nground truths.\n• Self-BLEU: BLEU by itself does not explicitly measure\nfor the novelty of the generated paraphrase. Often, it is\npossible to achieve high BLEU in paraphrase generation\nby simply copying the original input. Thus, we also use\nself-BLUE4, i.e. BLEU4 between the input and the pre-\ndiction to account for the novelty of the prediction. Low\nSelf-BLEU implies high novelty.\n• Self-TER: To check for the novelty of paraphrases\nbeyond simply checking n-gram overlaps as in Self-\nBLEU, we also measure theTranslation Edit Rate (TER)\n(Snover et al. 2006) between the input and the prediction.\nHigh self-TER implies high novelty.\n• BERT: We also want to account for the semantic ﬁ-\ndelity of the generated texts. A paraphrase must retain\nthe main semantic content of the input. To measure this,\nwe use cosine-similarity between the sentence encodings\nof the input and the prediction. For sentence encoding\nwe use Sentence-BERT (Reimers and Gurevych 2019)\nalong with paraphrase-mpnet-base-v26 as the\npre-trained weights.\n3https://github.com/arvind385801/paraphraseGen\n4https://github.com/tomhosking/separator\n5https://github.com/dqxiu/ParaSCI\n6https://www.sbert.net/docs/pretrained models.html\n10538\nBERT Self-\nTER\nSelf-\nBLEU BLEU iBLEU SARI BERT Self-\nTER\nSelf-\nBLEU BLEU iBLEU SARI\nMethod Dataset: QQP 140K Dataset: QQP 50K\nCopy 100.0 0.00 100.0 32.78 -7.05 14.98 100.0 0.00 100.0 30.36 -8.75 14.44\nGround Truth 89.05 54.31 30.98 100.0 60.71 83.88 88.53 56.31 30.34 100.0 60.90 87.51\nGPT2 Baselines\nFine Tuning 94.39 29.57 58.80 33.26 5.64 38.02 92.76 32.97 54.40 29.77 4.52 38.96\nAdapter Tuning 93.77 35.89 56.75 30.20 4.12 36.64 91.90 36.68 52.89 27.90 3.66 37.61\nLoRA 92.92 40.87 50.16 27.74 4.37 37.89 92.03 40.09 48.98 26.68 3.98 38.87\nPrompt Tuning 94.81 25.96 59.94 31.56 4.11 36.45 91.79 34.23 47.63 27.34 4.85 40.49\nLPT 94.99 27.40 59.16 33.21 5.50 38.67 92.62 34.84 47.79 28.70 5.75 41.58\nOurs\nRAPT 87.88 50.27 31.78 34.09 14.33 42.40 90.04 44.99 38.39 31.61 10.61 43.91\nNC-RAPT (High) 83.89 62.13 19.55 29.88 15.05 42.72 86.98 58.27 23.15 25.12 10.64 43.23\nNC-RAPT (Med) 89.84 43.47 37.77 36.47 14.19 44.67 92.25 37.40 43.39 30.22 8.14 45.16\nNC-RAPT (Low) 96.97 16.22 76.60 38.36 3.87 31.14 96.78 18.45 69.16 31.35 1.20 40.04\nMethod Dataset: ParaSCI ACL Dataset: MSRPC\nCopy 100.00 0.00 100.0 35.66 -5.04 15.01 100.0 0.00 100.0 47.75 3.43 19.91\nGround Truth 78.55 61.22 35.76 100.00 59.27 90.57 86.15 48.03 47.77 100.0 55.67 96.72\nGPT2 Baselines\nFine Tuning 89.94 28.56 72.34 33.32 1.62 33.66 97.35 9.09 89.63 45.09 4.67 29.78\nAdapter Tuning 87.47 35.17 64.5 29.79 1.50 34.00 29.54 73.15 8.85 4.23 0.31 21.26\nLoRA 92.43 24.28 74.37 31.99 0.08 33.22 96.85 10.7 86.95 44.57 5.12 32.81\nPrompt Tuning 91.05 23.48 71.18 29.93 -0.40 31.88 95.76 14.59 80.68 42.13 5.29 34.51\nLPT 92.83 19.6 77.06 33.08 0.04 31.64 95.57 15.05 80.21 41.7 5.13 34.99\nOurs\nRAPT 84.86 41.48 55.37 35.34 8.13 39.00 94.54 16.93 79.84 45.08 7.60 35.58\nNC-RAPT (High) 79.44 55.88 39.29 30.32 9.44 41.30 91.79 24.78 68.64 40.42 7.70 38.6\nNC-RAPT (Med) 89.49 31.17 63.94 36.30 6.23 40.44 96.56 11.57 85.42 46.14 6.67 32.88\nNC-RAPT (Low) 94.62 17.17 79.37 39.16 3.60 35.54 97.74 7.72 90.22 47.65 6.29 30.18\nTable 4: Comparison of different approaches to adapt GPT2 Medium for paraphrase generation on different datasets. We bold\nthe best results for each dataset excluding Copy and Ground Truth\n• iBLEU: iBLEU (Sun and Zhou 2012) is designed to\ncombine both BLEU and self-BLEU to measure the bal-\nance between closeness to ground truth and novelty:\niBLEU = \u000b·BLEU −(1 −\u000b) ·self-BLEU (4)\nWe use BLEU4 and self-BLEU4 with\u000bbeing 0:7 similar\nto Hosking and Lapata (2021).\n• SARI: SARI (Xu et al. 2016) explicitly checks for the\ngoodness of edits made on the input by the model in its\npredictions by comparing against the edits in the ground\ntruth references. West et al. (2021) show that it correlates\nwell with human judgment for paraphrase generation.\nExperimental Details\nIn the main experiments (Table 4), we explore different\nadaptation methods on GPT2 medium. We share some re-\nsults (Table 5) on GPT2 large too. Typically, during prompt\ntuning, we initialized the prompt preﬁx and inﬁx token em-\nbeddings with random natural language token embeddings\nfrom GPT2 vocabulary. Besides the language model adap-\ntation techniques, we also compare a copy-baseline that\nsimply copies the input text and poses it as a paraphrase.\nThe copy model checks how far we can go in scores like\nBLEU without changing a single word. We also try using\nthe ground truths as predictions themselves to get a better\nsense of natural novelty and semantic ﬁdelity of the ground\ntruths in a particular dataset. We share other experimental\ndetails and hyperparameters in a later section.\nExperimental Results\nAs shown in Table 4, RAPT substantially outperforms the\nbaselines on all metrics except BERT in general. Increased\nnovelty may slightly hurt BERT-based semantic similarity\nscore.\nNC-RAPT behaves mostly as expected. We get high nov-\nelty (high self-TER, low self-BLEU) when using preﬁx and\ninﬁx for novelty class high; we get low novelty when con-\nditioned on low novelty. We also observe a trade-off among\n10539\nBERT Self-\nTER\nSelf-\nBLEU BLEU iBLEU SARI BERT Self-\nTER\nSelf-\nBLEU BLEU iBLEU SARI\nMethod Dataset: QQP 50K Dataset: MSRPC\nGPT2 Baselines\nFine Tuning 92.49 34.56 50.96 29.82 5.59 40.13 95.55 14.0 84.10 43.96 5.54 34.49\nAdapter Tuning 92.35 36.51 49.55 29.16 5.55 40.04 93.91 13.84 81.81 42.82 5.43 33.96\nLoRA 92.98 33.47 52.96 29.67 4.88 39.81 96.80 11.31 86.11 44.88 5.58 33.62\nPrompt Tuning 93.51 29.94 54.82 29.62 4.29 39.39 96.73 10.82 86.46 44.94 5.52 32.82\nLPT 92.94 33.63 50.32 29.60 5.63 40.91 97.20 8.55 88.26 45.52 5.38 32.15\nOurs\nRAPT 90.56 42.83 41.11 31.16 9.48 43.27 96.50 11.34 86.06 47.80 7.64 34.57\nNC-RAPT (High) 87.13 56.45 25.04 26.33 10.92 43.25 92.37 23.9 69.33 41.40 8.18 39.50\nNC-RAPT (Med) 92.11 36.15 45.25 31.75 8.65 44.99 97.56 8.50 88.90 47.34 6.47 30.89\nNC-RAPT (Low) 96.69 17.43 71.29 32.03 1.03 38.96 98.85 4.32 94.32 48.86 5.90 27.42\nTable 5: Comparison of different approaches to adapt GPT2 Large for paraphrase generation on different datasets. We bold the\nbest results for each dataset excluding Copy and Ground Truth.\nMethod Self-BLEU BLEU iBLEU\nDiPS\u0003 32.45 18.47 3.19\nSOW/REAP\u0003 24.19 12.64 1.59\nLBoW\u0003 29.00 16.17 2.62\nSEPARATOR\u0003 14.84 14.70 5.84\nRAPT 31.78 34.09 14.33\nNC-RAPT (High) 19.55 29.88 15.05\nTable 6: Comparison with prior work on QQP 140K. \u0003in-\ndicates that the results are taken from Hosking and Lapata\n(2021). We bold the best results. DiPS refers to the model\nproposed by Kumar et al. (2019). SOW/REAP refers to\nthe model proposed by Goyal and Durrett (2020). LBoW\nrefers to the model proposed by Fu, Feng, and Cunningham\n(2019). Separator refers to the model proposed by Hosking\nand Lapata (2021). Our models are based on GPT2 medium.\nnovelty metrics, BLEU, and BERT in NC-RAPT. Condition-\ning on high novelty increases novelty but can reduce se-\nmantic ﬁdelity (BERT) and BLEU compared to other mod-\nels. However, the reduced semantic ﬁdelity (BERT) score\nis still similar to or better than that achieved by ground\ntruth paraphrases (Ground Truth Method). NC-RAPT con-\nditioned on high generally gets better novelty scores and\niBLEU compared to RAPT and the baselines. NC-RAPT\nconditioned on medium can sometimes gain the best SARI\nscores whereas NC-RAPT conditioned on low can get the\nbest BERT and BLEU scores but at a signiﬁcant cost to nov-\nelty. NC-RAPT allows the users to decide which trade-offs\nthey are willing to take by choosing a speciﬁc novelty class.\nThe copy models can still achieve relatively high BLEU\ndespite simply copying the original text. This phenomenon\nwas also noted by Mao and Lee (2019). This shows the lim-\nitation of relying on metrics like BLEU alone.\nIn Table 5, we show that we still get fairly consistent re-\nsults even when we use GPT2 large. In Table 6, we compare\nwith some reported results of prior work (described in the\ntable caption). Although SEPARATOR gets the highest nov-\nelty score (lowest self-BLEU), its BLEU score is quite low.\nOur proposed approaches based on GPT2 medium gener-\nally achieve a much better balance between BLEU and self-\nBLEU and thus, a substantially higher iBLEU.\nAnalysis\nRandom Retrieval Ablation: When using RAPT we use a\nkNN based example retrieval as discussed before. As an ab-\nlation, we try random retrieval instead of a kNN-based ap-\nproach. As shown in Table 7, retrieving semantically similar\nexamples using kNN gives us a much better performance.\nThese results support our hypothesis that semantically simi-\nlar example pairs can better guide paraphrase generation.\nHowever, as a collolary these results may also suggest that\nRAPT-based methods would not perform as well on a test-\ning dataset where the examples are semantically distant from\nanything in the training dataset. This can be one limitation\nof the method.\nPrompt Tuning Variants:We also experimented with a few\ndifferent variants of prompt tuning for paraphrase genera-\ntion. We reported the results in Table 8. Here, Prompt Tuning\n(Lester, Al-Rfou, and Constant 2021) is the method that we\nuse in our main experiments. Prompt Tuning Random is the\nsame method but with random initialization of the preﬁx-\ninﬁx embeddings instead of random selection from vocab-\nulary embeddings. P-Tuning is similar to the method pro-\nposed by Liu et al. (2021b). It uses a BiLSTM to associate\nthe preﬁx and inﬁx tokens. Preﬁx Tuning is similar to the\nmethod proposed by Li and Liang (2021). It uses the same\npreﬁx, inﬁx (transfromed by an MLP layer) in every GPT2\nlayer. Preﬁx-layer Tuning was used by Hu et al. (2021). It\ninitializes and trains preﬁx-inﬁx parameters for every layer\ninstead of just the embedding layer as in simple prompt tun-\ning (Lester, Al-Rfou, and Constant 2021). In the results, we\n10540\nBERT Self-\nTER\nSelf-\nBLEU BLEU iBLEU SARI BERT Self-\nTER\nSelf-\nBLEU BLEU iBLEU SARI\nMethod Dataset: QQP 50K Dataset: ParaSCI ACL\nRAPT (kNN) 90.04 44.99 38.39 31.61 10.61 43.91 84.86 41.48 55.37 35.34 8.13 39.00\nRAPT (random) 92.48 35.39 48.11 29.50 6.22 41.65 92.45 20.76 75.89 32.82 0.21 31.98\nTable 7: Comparison of kNN-based RAPT and RAPT with random retrieval. The results are based on adapting GPT2 medium.\nMethod BERT Self-\nTER iBLEU SARI\nPrompt Tuning (PT) 91.79 34.23 4.85 40.49\nPT Random 92.22 33.56 4.42 39.70\nP-Tuning 93.90 27.74 2.66 37.33\nPreﬁx Tuning 89.25 43.98 4.34 38.69\nPreﬁx-layer Tuning 88.64 47.62 4.63 38.58\nTable 8: Comparison of Prompt Tuning variants when adapt-\ning GPT2 medium on QQP 50K.\nsee that prompt tuning gets the best iBLEU and SARI scores.\nAlthough Preﬁx and Preﬁx-layer Tuning increase the nov-\nelty it does so at the cost of other metrics. Thus, we chose\nprompt tuning for our main experiments.\nHyperparameter Details\nWe tune the hyperparameters on QQP 50K with GPT2\nmedium for all the approaches. We then use the same hy-\nperparameters for other datasets and GPT2 large. We use\nrandom search with a maximum of 50 trials and 3 epochs\nper trial, and choose the hyperparameters based on valida-\ntion loss. For all the approaches, we search the learning rate\nwithin {0:1;0:01;1e−3;1e−4;5e−5}. For adapter tun-\ning, we search the adapter bottleneck hidden state dimension\nwithin {128;256;512}.\nFor LPT, we tune the learning rate for LoRA paramters\nand prompt tuning parameters separately (we use differ-\nent learning rates to tune LoRA and prompt template em-\nbeddings). For LoRA, LPT, RAPT, and NC-RAPT (all ap-\nproached involving LoRA), we ﬁx r (matrix rank) as 8 be-\ncause it worked well in Hu et al. (2021). We also use a\nweight decay of 0:01 for LoRA-based methods. We use the\nofﬁcial code for LoRA. 7\nWe set the inﬁx length for all prompt tuning methods to 8\nbecause it generally provided the best performance in Hu\net al. (2021). We search the preﬁx length of prompt tun-\ning random, preﬁx tuning, and preﬁx-layer tuning within\n{8;64;256}. We use the same preﬁx length as prompt tun-\ning random for p-tuning because both similarly operates at\nthe embedding level. For prompt tuning, we use the same\nhyperparameters as tuned for prompt tuning random.\nFor P-Tuning, we initialize the prompt template token em-\nbeddings with a dimension of b. The initialized prompt to-\nkens are passed through a BiLSTM (separately for preﬁx and\n7https://github.com/microsoft/LoRA\ninﬁx but with shared parameters) with a total hidden size of\n2×\n\u0004 d\n2\n\u0005\n(where dis the pre-trained model emebdding dimen-\nsions). We then use two afﬁne layers with a GELU activa-\ntion (Hendrycks and Gimpel 2016) in-between to transform\nthe concatenation of the forward and backward hidden states\n(total dimension 2 ×\n\u0004 d\n2\n\u0005\n) into a dimension d. The interme-\ndiate layer also has a dimension of d. We search b within\n{d;\n\u0004 d\n2\n\u0005\n;\n\u0004 d\n4\n\u0005\n}.\nAlso, during preﬁx tuning, we use a 2-layered MLP\nwith GELU activation in-between to transform the prompt\ntemplate embeddings from some dimension b to dimen-\nsion d. We use an intermediate layer dimension of d for\nthe MLP. Again, for preﬁx tuning too, we search for b in\n{d;\n\u0004 d\n2\n\u0005\n;\n\u0004 d\n4\n\u0005\n}. We use an MLP layer because it was used\nby (Li and Liang 2021). We also tuned and trained a ver-\nsion without the MLP layer but did not observe better per-\nformance than prompt tuning on iBLEU and SARI.\nFor RAPT and NC-RAPT, we set the length of d1:m as\nx−8 (where the value of x is the same as the total preﬁx\nlength which is searched and tuned in prompt tuning and\nprompt tuning random). We set the length of p1:s (in RAPT\nand NC-RAPT) as 8 (same as inﬁx length). Thus, we keep\nboth p1:s and q1:t small (length 8) in RAPT and NC-RAPT\nwhereas the majority of the parameters are concentrated on\nthe global preﬁx d1:m (x−8 length) as we discussed before.\nIn all cases, we use AdamW (Loshchilov and Hut-\nter 2019) as the optimizer. We also use a linear\nschedule with warmup for 100 steps (we use the\nget_linear_schedule_with_warmup() function\nfrom Transformers library (Wolf et al. 2020)), a gradient\nnorm clipping with a maximum of 1, a batch size of 32, and\na maximum decoding length ofn+100 where nis the size of\nthe input prompt (which includes the preﬁx tokens, inﬁx to-\nkens, the input to be paraphrased, and all retrieved examples\nif any). The selected hyperparameters for each approaches\nfrom the search are provided in Table 9.\nDuring training, we use a maximum epoch of 30 with\nearly stopping. We set the early stopping patience as 3.\nModel selection during training is done based on validation\nloss. The models are trained and tuned on single Tesla V100\n32GB GPUs. Gradient accumulation is used to maintain the\neffective batch size as 32.\nRelated Work\nParaphrase Generation - Traditionally rule-based systems\nwere used for paraphrase generation (McKeown 1983; Ko-\nzlowski, McCoy, and Vijay-Shanker 2003; Hassan et al.\n2007). Quirk, Brockett, and Dolan (2004); Zhao et al. (2008)\n10541\nLearning Rate Adapter Bottleneck preﬁx length Prompt Dimension (b)\nMethod Dataset: QQP 50K\nFine Tuning 5e−5 — — —\nAdapter Tuning 1e−4 512 — —\nLoRA 1e−3 — — —\nPrompt Tuning 0:1 — 256 —\nPrompt Tuning Random 0:1 — 256 —\nP-Tuning 0:01 — 256 d\nPreﬁx Tuning 1e−4 — 256 d\nPreﬁx-layer Tuning 0:01 — 64 —\nTable 9: Selected Hyperparameters. dis the embedding dimension of the involved pre-trained language model.\nused Statistical Machine Translation (SMT) methods for\nparaphrasing. More recent works typically utilize Seq2Seq\nmodels for paraphrase generation (Prakash et al. 2016; Cao\net al. 2017; Zhao et al. 2018; Egonmwan and Chali 2019).\nMallinson, Sennrich, and Lapata (2017) proposed bilin-\ngual pivoting for paraphrasing. Li et al. (2018) utilized\ndeep reinforcement learning to advance paraphrase gener-\nation whereas Du and Ji (2019) utilized imitation learn-\ning. Gupta et al. (2018) incorporated a variational autoen-\ncoding strategy to generate multiple diverse paraphrases.\nKumar et al. (2019) proposed a submodular optimization-\nbased decoding method to generate diverse paraphrases. Cao\nand Wan (2020) also improved the novelty of paraphrases\nthrough a GAN augmented with a diversity loss. Park et al.\n(2019) used counterfactual debiasing to generate a diverse\nparaphrases. Lin and Wan (2021) proposed multi-round\nparaphrase generation for improved diversity. Fu, Feng,\nand Cunningham (2019) proposed a latent-variable model\ngrounded by bag-of-words from target sentences for para-\nphrase generation. Witteveen and Andrews (2019) use GPT2\nfor paraphrase generation. Liu et al. (2020); West et al.\n(2021) proposed novel unsupervised paraphrasing strategies.\nSimilar to our novelty-controlled generation, there are a\nfew approaches (Iyyer et al. 2018; Li et al. 2019; Chen et al.\n2019; Kumar et al. 2020; Goyal and Durrett 2020; Huang\nand Chang 2021; Hosking and Lapata 2021) focusing on\nmore controllable paraphrase generation. Although these ap-\nproaches can provide extra control over different aspects (eg.\ngranularity or syntactic templates) of paraphrase generation,\nthey do not explicitly or directly help us in controlling nov-\nelty. Moreover, most of these approaches require specialized\narchitectures which cannot be straightforwardly utilized in\nthe adaptation of common pre-trained language models.\nPrompt Tuning - Initial work on prompt tuning focused on\ndiscrete selection of prompt template tokens (Jiang et al.\n2020; Schick and Sch ¨utze 2021a; Shin et al. 2020; Schick\nand Sch¨utze 2021b). Some of the newer works (Li and Liang\n2021; Liu et al. 2021b; Lester, Al-Rfou, and Constant 2021;\nHu et al. 2021), instead, directly tuned the prompt template\ntoken embeddings and/or intermediate layer states in the\ncontinuous space; often achieving better performance than\nthe former strategy (Liu et al. 2021b). Our approach follows\nthe latter direction.\nRetrieval Augmentation - Hashimoto et al. (2018) intro-\nduced a retrieve-and-edit framework for structured output\nprediction. Kazemnejad, Salehi, and Soleymani Baghshah\n(2020) built upon Hashimoto et al. (2018) by using retrieved\nexamples to augment paraphrase generation. However, their\napproach is not integrated with prompt tuning and uses a\nspecialized architectural which cannot be easily utilized in\nadapting a generic pre-trained language model without intro-\nducing pre-training-ﬁne-tuning discrepancies. Similar to our\nwork, Liu et al. (2021a); Gao, Fisch, and Chen (2021) aug-\nmented the prompts for pre-trained models using kNN-based\nretrieved examples. However, unlike our work, they either\nuse manual prompts or a separate model for discrete prompt\nprediction (instead of tuning the prompts directly in a contin-\nuous space) while focusing on few-shot settings. Also, they\ndid not explore paraphrase generation. To the best of our\nknowledge, our work is the ﬁrst to integrate kNN-based ex-\nample retrieval with prompt tuning, in a continuous space\n(Li and Liang 2021; Lester, Al-Rfou, and Constant 2021),\nfor paraphrase generation in a standard (non-few-shot) su-\npervised training setup while, at the same time, incorporat-\ning specialized prompts for novelty controlled generation.\nConclusion\nIn this paper, we propose RAPT as a parameter-efﬁcient re-\ntrieval augmented prompt tuning setting for enhanced para-\nphrase generation. Building up on RAPT, we further intro-\nduce NC-RAPT for novelty-controlled paraphrase genera-\ntion. Our experimental results from four datasets conﬁrms\nthe effectiveness of our methods. As future work, we are in-\nterested in applying our proposed approaches to other large\npre-trained language models to investigate how performance\nvaries with different model architectures and size. We would\nalso like to explore RAPT for other downstream tasks like\nsemantic parsing, natural language inference, named entity\nrecognition etc. Another avenue for future research, would\nbe testing the effectiveness of the paraphrases generated by\nthe proposed approaches for data augmentation.\n10542\nAcknowledgments\nWe would like to thank Edgar Meij, Srivas Prasad, Nimesh\nGhelani and the anonymous reviewers for their constructive\nfeedback and suggestions. We also thank Mounica Maddela\nfor the valuable discussions.\nReferences\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. In Advances in NeurIPS.\nCao, Y .; and Wan, X. 2020. DivGAN: Towards Diverse\nParaphrase Generation via Diversiﬁed Generative Adversar-\nial Network. In Findings of EMNLP.\nCao, Z.; Luo, C.; Li, W.; and Li, S. 2017. Joint Copying and\nRestricted Generation for Paraphrase. In Singh, S. P.; and\nMarkovitch, S., eds., Proceedings of AAAI.\nChen, M.; Tang, Q.; Wiseman, S.; and Gimpel, K. 2019.\nControllable Paraphrase Generation with a Syntactic Exem-\nplar. In Proceedings of ACL.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of NAACL.\nDolan, B.; Quirk, C.; and Brockett, C. 2004. Unsupervised\nConstruction of Large Paraphrase Corpora: Exploiting Mas-\nsively Parallel News Sources. In proceedings of COLING.\nDong, Q.; Wan, X.; and Cao, Y . 2021. ParaSCI: A Large\nScientiﬁc Paraphrase Dataset for Longer Paraphrase Gener-\nation. In Proceedings of EACL.\nDu, W.; and Ji, Y . 2019. An Empirical Comparison on Imi-\ntation Learning and Reinforcement Learning for Paraphrase\nGeneration. In Proceedings of EMNLP-IJCNLP.\nEgonmwan, E.; and Chali, Y . 2019. Transformer and\nseq2seq model for Paraphrase Generation. In Proceedings\nof the 3rd Workshop on Neural Generation and Translation,\n249–255. Hong Kong: Association for Computational Lin-\nguistics.\nFu, Y .; Feng, Y .; and Cunningham, J. P. 2019. Paraphrase\nGeneration with Latent Bag of Words. In Advances in\nNeurIPS.\nGao, T.; Fisch, A.; and Chen, D. 2021. Making Pre-trained\nLanguage Models Better Few-shot Learners. In Proceed-\nings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume 1: Long\nPapers), 3816–3830. Online: Association for Computational\nLinguistics.\nGoyal, T.; and Durrett, G. 2020. Neural Syntactic Preorder-\ning for Controlled Paraphrase Generation. InProceedings of\nACL.\nGupta, A.; Agarwal, A.; Singh, P.; and Rai, P. 2018. A Deep\nGenerative Framework for Paraphrase Generation.Proceed-\nings of AAAI.\nHashimoto, T. B.; Guu, K.; Oren, Y .; and Liang, P. 2018. A\nRetrieve-and-Edit Framework for Predicting Structured Out-\nputs. In Proceedings of NeurIPS.\nHassan, S.; Csomai, A.; Banea, C.; Sinha, R.; and Mihalcea,\nR. 2007. UNT: SubFinder: Combining Knowledge Sources\nfor Automatic Lexical Substitution. In Proceedings of Se-\nmEval.\nHendrycks, D.; and Gimpel, K. 2016. Gaussian Error Linear\nUnits (GELUs). arXiv: Learning.\nHosking, T.; and Lapata, M. 2021. Factorising Meaning and\nForm for Intent-Preserving Paraphrasing. In Proceedings of\nACL.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;\nDe Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and\nGelly, S. 2019. Parameter-Efﬁcient Transfer Learning for\nNLP. In Proceedings of the ICML.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; and Chen, W. 2021. LoRA: Low-Rank Adaptation of\nLarge Language Models. ArXiv.\nHuang, K.-H.; and Chang, K.-W. 2021. Generating Syn-\ntactically Controlled Paraphrases without Using Annotated\nParallel Pairs. In Proceedings of EACL.\nIyyer, M.; Wieting, J.; Gimpel, K.; and Zettlemoyer, L. 2018.\nAdversarial Example Generation with Syntactically Con-\ntrolled Paraphrase Networks. In Proceedings of ACL.\nJiang, Z.; Xu, F. F.; Araki, J.; and Neubig, G. 2020. How\ncan we know what language models know? TACL.\nKazemnejad, A.; Salehi, M.; and Soleymani Baghshah, M.\n2020. Paraphrase Generation by Learning How to Edit from\nSamples. In Proceedings of ACL.\nKozlowski, R.; McCoy, K. F.; and Vijay-Shanker, K.\n2003. Generation of Single-sentence Paraphrases from Pred-\nicate/Argument Structure using Lexico-grammatical Re-\nsources. In Proceedings of the Second International Work-\nshop on Paraphrasing.\nKumar, A.; Ahuja, K.; Vadapalli, R.; and Talukdar, P.\n2020. Syntax-Guided Controlled Generation of Paraphrases.\nTACL.\nKumar, A.; Bhattamishra, S.; Bhandari, M.; and Talukdar, P.\n2019. Submodular Optimization-based Diverse Paraphras-\ning and its Effectiveness in Data Augmentation. InProceed-\nings of NAACL.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power\nof Scale for Parameter-Efﬁcient Prompt Tuning. ArXiv.\nLi, X. L.; and Liang, P. 2021. Preﬁx-Tuning: Optimizing\nContinuous Prompts for Generation. InProceedings of ACL.\nLi, Z.; Jiang, X.; Shang, L.; and Li, H. 2018. Paraphrase\nGeneration with Deep Reinforcement Learning. InProceed-\nings of EMNLP.\nLi, Z.; Jiang, X.; Shang, L.; and Liu, Q. 2019. Decompos-\nable Neural Paraphrase Generation. In Proceedings of ACL.\n10543\nLin, Z.; and Wan, X. 2021. Pushing Paraphrase Away from\nOriginal Sentence: A Multi-Round Paraphrase Generation\nApproach. In Findings of ACL-IJCNLP.\nLiu, J.; Shen, D.; Zhang, Y .; Dolan, B.; Carin, L.; and Chen,\nW. 2021a. What Makes Good In-Context Examples for\nGPT-3? ArXiv.\nLiu, X.; Mou, L.; Meng, F.; Zhou, H.; Zhou, J.; and Song,\nS. 2020. Unsupervised Paraphrasing by Simulated Anneal-\ning. In Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, 302–312. Online:\nAssociation for Computational Linguistics.\nLiu, X.; Zheng, Y .; Du, Z.; Ding, M.; Qian, Y .; Yang, Z.;\nand Tang, J. 2021b. GPT Understands, Too. arXiv preprint\narXiv:2103.10385.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In ICLR.\nMallinson, J.; Sennrich, R.; and Lapata, M. 2017. Paraphras-\ning Revisited with Neural Machine Translation. In Proceed-\nings of EACL.\nMao, H.-R.; and Lee, H.-Y . 2019. Polly Want a Cracker: An-\nalyzing Performance of Parroting on Paraphrase Generation\nDatasets. In Proceedings of EMNLP-IJCNLP. Association\nfor Computational Linguistics.\nMcKeown, K. 1983. Paraphrasing questions using given and\nnew information. American Journal of Computational Lin-\nguistics, 9(1): 1–10.\nOlive, J. 2005. Global Autonomous Language Exploitation\n(GALE). In DARPA/IPTO Proposer Information Pamphlet.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine transla-\ntion. In Proceedings of ACL, 311–318.\nPark, S.; Hwang, S.-w.; Chen, F.; Choo, J.; Ha, J.-W.; Kim,\nS.; and Yim, J. 2019. Paraphrase Diversiﬁcation Using\nCounterfactual Debiasing. Proceedings of AAAI.\nPrakash, A.; Hasan, S. A.; Lee, K.; Datla, V .; Qadir, A.; Liu,\nJ.; and Farri, O. 2016. Neural Paraphrase Generation with\nStacked Residual LSTM Networks. In Proceedings of COL-\nING 2016, the 26th International Conference on Compu-\ntational Linguistics: Technical Papers, 2923–2934. Osaka,\nJapan: The COLING 2016 Organizing Committee.\nQuirk, C.; Brockett, C.; and Dolan, W. 2004. Monolingual\nMachine Translation for Paraphrase Generation. InProceed-\nings of EMNLP.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised\nMultitask Learners. Technical Report, Open AI.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sen-\ntence Embeddings using Siamese BERT-Networks. In Pro-\nceedings of EMNLP 2019.\nSchick, T.; and Sch ¨utze, H. 2021a. Exploiting Cloze-\nQuestions for Few-Shot Text Classiﬁcation and Natural Lan-\nguage Inference. In Proceedings of EACL.\nSchick, T.; and Sch ¨utze, H. 2021b. It’s Not Just Size That\nMatters: Small Language Models Are Also Few-Shot Learn-\ners. In Proceedings of NAACL.\nShin, T.; Razeghi, Y .; Logan IV , R. L.; Wallace, E.; and\nSingh, S. 2020. AutoPrompt: Eliciting Knowledge from\nLanguage Models with Automatically Generated Prompts.\nIn Proceedings of EMNLP.\nShoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P.; Casper,\nJ.; and Catanzaro, B. 2019. Megatron-lm: Training multi-\nbillion parameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nSnover, M.; Dorr, B.; Schwartz, R.; Micciulla, L.; and\nMakhoul, J. 2006. A Study of Translation Edit Rate with\nTargeted Human Annotation. Proceedings of Association\nfor Machine Translation in the Americas.\nSun, H.; and Zhou, M. 2012. Joint Learning of a Dual SMT\nSystem for Paraphrase Generation. In Proceedings of ACL.\nWest, P.; Lu, X.; Holtzman, A.; Bhagavatula, C.; Hwang,\nJ. D.; and Choi, Y . 2021. Reﬂective Decoding: Beyond Uni-\ndirectional Generation with Off-the-Shelf Language Mod-\nels. In Proceedings of ACL.\nWitteveen, S.; and Andrews, M. 2019. Paraphrasing with\nLarge Language Models. In Proceedings of the 3rd Work-\nshop on Neural Generation and Translation, 215–220. Hong\nKong: Association for Computational Linguistics.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu, J.;\nXu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest, Q.; and\nRush, A. M. 2020. Transformers: State-of-the-Art Natural\nLanguage Processing. In Proceedings of EMNLP: System\nDemonstrations.\nXu, W.; Napoles, C.; Pavlick, E.; Chen, Q.; and Callison-\nBurch, C. 2016. Optimizing statistical machine translation\nfor text simpliﬁcation. TACL, 4: 401–415.\nZhao, S.; Meng, R.; He, D.; Saptono, A.; and Parmanto,\nB. 2018. Integrating Transformer and Paraphrase Rules for\nSentence Simpliﬁcation. In Proceedings of EMNLP.\nZhao, S.; Niu, C.; Zhou, M.; Liu, T.; and Li, S. 2008. Com-\nbining Multiple Resources to Improve SMT-based Para-\nphrasing Model. In Proceedings of ACL-08: HLT.\n10544"
}