{
  "title": "A multimodal visual–language foundation model for computational ophthalmology",
  "url": "https://openalex.org/W4411475366",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2954424513",
      "name": "Danli Shi",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A1978435048",
      "name": "Weiyi Zhang",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2128992391",
      "name": "Jiancheng Yang",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A2110765125",
      "name": "Siyu Huang",
      "affiliations": [
        "Clemson University"
      ]
    },
    {
      "id": "https://openalex.org/A2126036584",
      "name": "Xiaolan Chen",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2620825143",
      "name": "Pusheng Xu",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2117327603",
      "name": "Kai Jin",
      "affiliations": [
        "Second Affiliated Hospital of Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2098434109",
      "name": "Shan Lin",
      "affiliations": [
        "The Eighth Hospital of Xi'an"
      ]
    },
    {
      "id": "https://openalex.org/A2033825803",
      "name": "Jin Wei",
      "affiliations": [
        "Shanghai Eye Disease Prevention & Treatment Center"
      ]
    },
    {
      "id": "https://openalex.org/A2626405144",
      "name": "Mayinuer Yusufu",
      "affiliations": [
        "Centre for Eye Research Australia"
      ]
    },
    {
      "id": "https://openalex.org/A2436487336",
      "name": "Shunming Liu",
      "affiliations": [
        "Guangdong Academy of Medical Sciences",
        "Guangdong Provincial People's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2008444722",
      "name": "Qing Zhang",
      "affiliations": [
        "Beijing Tongren Hospital",
        "Capital Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2303264390",
      "name": "Zongyuan Ge",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2118956987",
      "name": "Xun Xu",
      "affiliations": [
        "Shanghai Eye Disease Prevention & Treatment Center"
      ]
    },
    {
      "id": "https://openalex.org/A2115893760",
      "name": "Mingguang He",
      "affiliations": [
        "Hong Kong Polytechnic University",
        "Hong Kong Science and Technology Parks Corporation"
      ]
    },
    {
      "id": "https://openalex.org/A2954424513",
      "name": "Danli Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978435048",
      "name": "Weiyi Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128992391",
      "name": "Jiancheng Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110765125",
      "name": "Siyu Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126036584",
      "name": "Xiaolan Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2620825143",
      "name": "Pusheng Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117327603",
      "name": "Kai Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098434109",
      "name": "Shan Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2033825803",
      "name": "Jin Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2626405144",
      "name": "Mayinuer Yusufu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2436487336",
      "name": "Shunming Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2008444722",
      "name": "Qing Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2303264390",
      "name": "Zongyuan Ge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118956987",
      "name": "Xun Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115893760",
      "name": "Mingguang He",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3130938027",
    "https://openalex.org/W2954850454",
    "https://openalex.org/W4386833265",
    "https://openalex.org/W3121943121",
    "https://openalex.org/W4400889241",
    "https://openalex.org/W3083027974",
    "https://openalex.org/W2772246530",
    "https://openalex.org/W4396605652",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W4386697749",
    "https://openalex.org/W4385948838",
    "https://openalex.org/W4392947521",
    "https://openalex.org/W4399347607",
    "https://openalex.org/W4400831339",
    "https://openalex.org/W4404759772",
    "https://openalex.org/W4229015090",
    "https://openalex.org/W4386776068",
    "https://openalex.org/W4391751464",
    "https://openalex.org/W4389340440",
    "https://openalex.org/W4409091111",
    "https://openalex.org/W4405669265",
    "https://openalex.org/W4386566421",
    "https://openalex.org/W2996651271",
    "https://openalex.org/W4394728153",
    "https://openalex.org/W4389097943",
    "https://openalex.org/W3163434754",
    "https://openalex.org/W4406316584",
    "https://openalex.org/W2916137561",
    "https://openalex.org/W4392947532",
    "https://openalex.org/W4389094583",
    "https://openalex.org/W4391170193",
    "https://openalex.org/W4392984569",
    "https://openalex.org/W4403982217",
    "https://openalex.org/W4405898246",
    "https://openalex.org/W4386536404",
    "https://openalex.org/W4221058154",
    "https://openalex.org/W4406929860",
    "https://openalex.org/W4409180872",
    "https://openalex.org/W4403071805",
    "https://openalex.org/W4383340634",
    "https://openalex.org/W2828862258",
    "https://openalex.org/W4282048668",
    "https://openalex.org/W2903117925",
    "https://openalex.org/W3191934798",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W2897980926",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4403015917",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4398775037",
    "https://openalex.org/W4388569054"
  ],
  "abstract": "Early detection of eye diseases is vital for preventing vision loss. Existing ophthalmic artificial intelligence models focus on single modalities, overlooking multi-view information and struggling with rare diseases due to long-tail distributions. We propose EyeCLIP, a multimodal visual-language foundation model trained on 2.77 million ophthalmology images from 11 modalities with partial clinical text. Our novel pretraining strategy combines self-supervised reconstruction, multimodal image contrastive learning, and image-text contrastive learning to capture shared representations across modalities. EyeCLIP demonstrates robust performance across 14 benchmark datasets, excelling in disease classification, visual question answering, and cross-modal retrieval. It also exhibits strong few-shot and zero-shot capabilities, enabling accurate predictions in real-world, long-tail scenarios. EyeCLIP offers significant potential for detecting both ocular and systemic diseases, and bridging gaps in real-world clinical applications.",
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01772-2\nA multimodal visual–language foundation\nmodel for computational ophthalmology\nCheck for updates\nDanli Shi1,2,13 , Weiyi Zhang1,13, Jiancheng Yang3,S i y uH u a n g4, Xiaolan Chen1,P u s h e n gX u1, Kai Jin5,\nShan Lin6,J i nW e i7, Mayinuer Yusufu8,S h u n m i n gL i u9,Q i n gZ h a n g10,Z o n g y u a nG e11, Xun Xu7 &\nMingguang He1,2,12\nEarly detection of eye diseases is vital for preventing vision loss. Existing ophthalmic artiﬁcial\nintelligence models focus on single modalities, overlooking multi-view information and struggling with\nrare diseases due to long-tail distributions. We propose EyeCLIP, a multimodal visual-language\nfoundation model trained on 2.77 million ophthalmology images from 11 modalities with partial clinical\ntext. Our novel pretraining strategy combines self-supervised reconstruction, multimodal image\ncontrastive learning, and image-text contrastive learning to capture shared representations across\nmodalities. EyeCLIP demonstrates robust performance across 14 benchmark datasets, excelling in\ndisease classiﬁcation, visual question answering, and cross-modal retrieval. It also exhibits strong\nfew-shot and zero-shot capabilities, enabling accurate predictions in real-world, long-tail scenarios.\nEyeCLIP offers signiﬁcant potential for detecting both ocular and systemic diseases, and bridging\ngaps in real-world clinical applications.\nOphthalmic diseases such as glaucoma, macular degeneration, and diabetic\nretinopathy pose a signiﬁcant threat to global vision health, often leading to\nvision impairment or even blindness1. However, access to timely diagnosis\nand treatment remains a critical challenge due to insufﬁcient medical\nresources, especially in underservedregions and developing countries2,3.\nThis inequitable distribution of resources makes early detection and inter-\nvention for eye diseases particularly challenging, further exacerbating the\nburden of these diseases.\nComputational ophthalmology has emerged as a promising solution,\ndrawing from the concept of“computational pathology.“\n4,5 This data-driven\napproach leverages artiﬁcial intelligence (AI) and multimodal data to\nautomate image analysis, enhance diagnostic accuracy, and reduce specia-\nlists’workloads6–8. Recently, theﬁeld has shifted from performing speciﬁc\ntasks to developing foundation models9–14. After pretraining on a large\nquantity of labeled or unlabeled data, the model can be easily adapted to\ndownstream tasks in a data-saving manner, reducing the cost and time of\ndata preparation and improving the models’ generalization capability.\nRETFound was theﬁrst proposed foundation model in ophthalmology\nusing self-supervised reconstruction learning\n10, but it was trained on sepa-\nrate image modalities (color fundus photography [CFP] and optical\ncoherence tomography [OCT]). VisionFM\n15 integrates multimodal infor-\nmation through a shared embedding; however, its image encoders remain\nmodality-speciﬁc, and a universal model capableo fe n c o d i n ga l lm o d a l i t i e s\nhas yet to be explored. Previously, we proposed EyeFound, which learns a\nshared representation of multimodal ophthalmic imaging\n16. Nevertheless,\nexisting foundation models still lack modality-modality consistency and\nimage-language alignment— features we consider essential for real-world\napplications.\nIn clinical practice, multiple examinations are optimal for examining\ndifferent eye pathologies, such as CFP, OCT, fundusﬂuorescein angio-\ngraphy (FFA), and fundus autoﬂuorescence (FAF)17.E a c he x a m i n a t i o n\nprovides unique and complementary information about the structure and\n1School of Optometry, The Hong Kong Polytechnic University, Kowloon, Hong Kong SAR, China.2Research Centre for SHARP Vision (RCSV), The Hong Kong\nPolytechnic University, Kowloon, Hong Kong SAR, China.3Swiss Federal Institute of Technology Lausanne (EPFL), Lausanne, Switzerland.4School of Computing,\nClemson University, Clemson, SC, USA.5Department of Ophthalmology, The Second Afﬁliated Hospital, School of Medicine, Zhejiang University,\nHangzhou, China.6Wuhan Bright Eye Hospital, Wuhan, China.7Department of Ophthalmology, Shanghai General Hospital, Shanghai Jiao Tong University School\nof Medicine, National Clinical Research Center for Eye Diseases, No. 100 Haining Road, Shanghai, 20080, PR China.8Centre for Eye Research Australia, Royal\nVictorian Eye and Ear Hospital, East Melbourne, Australia.9Department of Ophthalmology, Guangdong Academy of Medical Sciences, Guangdong Provincial\nPeople’s Hospital, Guangzhou, China.10Beijing Tongren Eye Center, Beijing Tongren Hospital, Capital Medical University, Beijing, China.11AIM for Health Lab,\nFaculty of Information Technology, Monash University, Melbourne, VIC, Australia.12Centre for Eye and Vision Research (CEVR), 17W Hong Kong Science Park,\nScience Park, Hong Kong SAR, China.13These authors contributed equally: Danli Shi, Weiyi Zhang.e-mail: danli.shi@polyu.edu.hk;\nmingguang.he@polyu.edu.hk\nnpj Digital Medicine|           (2025) 8:381 1\n1234567890():,;\n1234567890():,;\nfunction of the eye. Previous studies have demonstrated the complementary\ncapabilities of different modalitiesi ne n h a n c i n gA Im o d e l sf o rd i s e a s e\nclassiﬁcation and segmentation18–21. Therefore, effectively utilizing multi-\nmodal data is crucial for obtaining multi-view information, and ensuring\nconsistency across modalities can serve as an important cue for self-\nsupervised learning. Additionally, ophthalmic reports and diagnoses from\nexpert interpretations offer rich textual context, which should be helpful for\nlearning long-tailed representationswith hierarchical concepts commonly\nencountered in the medical domain\n11,22. By integrating clinical text, AI\nmodels can better simulate the cognitive processes of human experts,\nenabling them to handle complex, real-world clinical problems in an ever-\nchanging environment.\nIn this work, we propose EyeCLIP,an ophthalmic visual-language\nfoundational model designed to harness real-world multi-source, multi-\nmodal data. EyeCLIP was pre-trained on a dataset comprising 2,777,593\nmultimodal ophthalmic images and11,180 reports from 128,554 patients\nusing self-supervised learning and multimodality alignment. Speciﬁcally,\nthe training combined self-supervised reconstruction, multimodal image\ncontrastive, and image-text contrastivelearning. Subsequently, we validated\nEyeCLIP on 14 multi-country datasets to assess its performance in zero-\nshot, few-shot, and supervised settings across different tasks, including\nmultimodal ocular disease diagnosis and systemic disease prediction, visual\nquestion answering (VQA), and cross-modal retrieval. EyeCLIP can effec-\ntively learn a shared representation of multiple examinations, enabling zero-\nshot disease diagnosis and improved language understanding by fully uti-\nlizing a large amount of unlabeled, multi-examination, and labeled data in\nthe real world. We believe our approach not only represents a signiﬁcant\nadvancement in ophthalmic foundation models but also offers insights for\ntraining foundational models with incomplete multimodal medical data\naccumulated in clinical practice across other medical domains.\nResults\nEyeCLIP development using multi-center multimodal datasets\nThe EyeCLIP system was trained using 2,777,593 multimodal images and\n11,180 reports from 128,554 patients across diverse regions and hospitals\nin China to learn ophthalmic vision-language features comprehensively.\nThe data details can be found in Fig.1 and the Methods section. Following\ntraining, EyeCLIP can be directly applied to applications involving clas-\nsiﬁcation and cross-modal retrieval without further training. Also, it can\nbe ﬁnetuned in a data-saving manner for downstream applications such as\nocular disease diagnosis, systemic disease prediction, and interactive\nVQA. Figure 1 shows the study design. The characteristics of the 14\ndownstream datasets can be found in Supplementary Table 1.\nFigure 2a presents EyeCLIP’s overall superior performance across dif-\nferent downstream tasks compared with the general-domain CLIP\n23,\nmedical domain BioMedCLIP24, PubMedCLIP25, and the ophthalmology\ndomain RETFound10.\nEyeCLIP excels in zero-shot, partial and full-data training ocular\ndisease classiﬁcation\nZero-shot transfer capability enables asingle pretrained foundation model\nto be applied directly to downstream tasks. EyeCLIP could be a strong\nbaseline for conventional supervisedlearning, especially when training\nlabels are scarce. We evaluated EyeCLIP’s zero-shot classiﬁcation perfor-\nmance without task-speciﬁc training on nine public ophthalmic datasets.\nUsing CFP as the input modality, EyeCLIP signiﬁcantly outperformed other\nmodels in diagnosing ophthalmic diseases (allP < 0.001), with AUCs ran-\nging from 0.681 to 0.757 for DR, 0.721 and 0.684 for glaucoma, as well as\n0.660 and 0.688 for multi-disease diagnosis. For OCT, EyeCLIP achieved the\nhighest AUROC scores of 0.800 for OCTID\n26 and 0.776 for OCTDL27,\nhigher than the other models (allP < 0.001). Quantitative results are pre-\nsented in Fig.2b and Supplementary Table 2.\nNext, we evaluated the few-shot performance of EyeCLIP on those nine\nocular disease datasets, using limited training samples of 1, 2, 4, 6, and 16,\nrespectively. The results indicated that EyeCLIP could generalize with\nlimited data, demonstrating the ability to diagnose various ophthalmic\ndiseases data-efﬁciently, outperforming other models (allP < 0.01). Quan-\ntitative results of AUROC and AUPR are provided in Fig.3 and Supple-\nmentary Table 3.\nSpeciﬁcally, rare diseases are known for lacking sufﬁcient data due\nto low incidence rates, and they are a common challenge for medical AI,\nand should be most beneﬁcial with data-efﬁcient training. Therefore, we\nfurther evaluated its performance for few-shot classiﬁcation using a\nsubset of the Retina Image Bank selected by ophthalmologists, with the\nnumber of images for each class exceeding 16. The subset included 17\nrare diseases: acute posterior multifocal placoid pigment epitheliopathy,\nbirdshot retinochoroidopathy, central areolar choroidal dystrophy,\nchoroidal melanoma, choroidal osteoma, cone dystrophy, congenital\nhypertrophy of the retinal pigment epithelium, familial exudative\nvitreoretinopathy, macular telangiectasia, optic disc pit, optic nerve\nhypoplasia, pseudoxanthoma elasticum, retinitis pigmentosa, retino-\nblastoma, retinopathy of prematurity, serpiginous choroiditis, Stargardt\ndisease. EyeCLIP beat other models in classifying rare diseases in all\nsettings. The results are presented in Fig.4c and Supplementary Table 4.\nDiseases with more distinct clinical features, such as choroidal melanoma\nand retinitis pigmentosa, were more readily identiﬁed across imaging\nmodalities.\nLastly, we tested EyeCLIP using the full-data supervised training\nparadigm on 11 public datasets containing unimodal and multimodal\nimages, with a train, validation and test split ratio of 55:15:30%. Detailed\nresults are provided in Fig.4a and Supplementary Table 5.\nFor single-modality tasks, EyeCLIP outperformed competing models\nexcept for three datasets when it was on par with the 2\nnd best model\nRETFound. In DR classi ﬁcation, EyeCLIP signi ﬁcantly surpassed\nRETFound in IDRiD dataset [with AUROC 0.835 vs 0.826,P =0 . 0 1 3 ] ,\nwhich is a small dataset, but on par with RETFound on much larger datasets\nA P T O S 2 0 1 9a n dM E S S I D O R 2(P > 0.05), suggesting EyeCLIP surpasses\nRETFound in a matter of data ef ﬁciency, requiring less data than\nRETFound. For glaucoma and multi-disease classiﬁcation, EyeCLIP con-\nsistently outperformed other models. For OCT images, EyeCLIP was on par\nwith RETFound on OCTID dataset (P > 0.05), but signiﬁcantly better on\nOCTDL dataset (AUROC 0.993 vs. 0.982,P < 0.001), which is a more\nimbalanced dataset with long-tailed classes. Even though RETFound spe-\nciﬁcally trained separate weights that are optimal for CFP and OCT, Eye-\nCLIP is generally better and no worse than it, even with a single general\nencoder.\nFor multimodality tasks, EyeCLIP outperformed all comparison\nmodels. On the AngioReport (APTOS2023\n28) dataset with two modalities,\nEyeCLIP outperformed the next best model, BioMedCLIP, with an AUROC\nof 0.721 versus 0.705,P < 0.001. Moreover, EyeCLIP performed the best on\nthe challenging Retina Image Bank\n29 dataset with 14 modalities and 84\nconditions, including rare diseases, with AUROC of 0.561 versus the 2nd best\n0.545, P < 0.001.\nWe also conducted an ablation study on fully supervised training to\ninvestigate the contributions of image-text contrastive learning, image-\nimage contrastive learning, and image self-reconstruction learning. Results\ncan be found in Supplementary Table 6 and Supplementary Fig. 2. The\npretraining and downstream settings in the ablation experiments remained\nconsistent with the original EyeCLIP model. The results indicate that\nremoving any of these components leads to a decline in performance on\ndownstream tasks, demonstrating the necessity and effectiveness of the\nEyeCLIP design. Among them, the model without image self-reconstruction\nlearning experienced the most signiﬁcant performance drop. For instance,\non the multimodal dataset Retina Image Bank, the AUROC decreased by\n14.2 percentage points, while on the AngioReport dataset, it dropped by 14.6\npercentage points. This suggests that image self-reconstruction learning\nplays a crucial role in maintaining robust feature representations, particu-\nlarly in scenarios with diverse modalities, where reconstructing input images\nhelps preserve structural and semantic consistency across different imaging\ntechniques.\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 2\nFig. 1 | Study diagram. aUsing an extensive multimodal database across nine\nprovinces in China, we matched the multi-examination images from the same\npatient, and cleaned the medical reports using a keyword mapping dictionary\ncontaining medical terminology to generate hierarchical keyword text labels.\nb EyeCLIP was pretrained using self-supervised reconstruction, multi-examination\ncontrastive learning, and hierarchical text-image contrastive learning to leverage\nreal-world multi-examination clinical data fully.c Downstream multi-country\ndatasets for EyeCLIP validation, including zero-shot, few-shot, and supervised\nﬁnetuning scenarios.d Radar plot outlines the performance of EyeCLIP and baseline\nmodels across various downstream tasks. EyeCLIP signiﬁcantly outperforms the\nbaseline models across diverse tasks, including zero-shot classiﬁcation, multimodal\nretrieval, visual question answering (VQA), and supervised systemic disease\nprediction.\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 3\nEyeCLIP enhances systemic disease prediction\nSystemic diseases such as stroke and myocardial infarction (MI) pose\nsigniﬁcant threats to older adults, often leading to sudden death. The\neyes, rich in blood vessels that can be directly visualized, have been\nreferred to as“the window to the body’s health.“\n30,31 Therefore, predicting\nthe incidence of systemic diseases is a crucial technique for early\nscreening and prevention. However, compared to the general population,\nthe incidence of these events is relatively low, resulting in limited positive\ntraining data. Consequently, data-efﬁcient training methods are highly\nvalued in this context. We evaluated EyeCLIP’s performance in pre-\ndicting systemic diseases based on ophthalmic images using the UK\nBiobank\n32. Our experiment included predictions for stroke, dementia,\nParkinson’s disease (PD), and MI. Weﬁrst assessed the few-shot per-\nformance of EyeCLIP using limited training samples of 1, 2, 4, 8, and 16,\nrespectively. EyeCLIP consistently outperformed other models, demon-\nstrating superior data efﬁciency in predicting systemic diseases. For full-\ndata supervised training, EyeCLIP rankedﬁrst, achieving AUROC scores\nof 0.641, 0.536, 0.580, and 0.596, and AUPR scores of 0.627, 0.572, 0.616,\nand 0.582, respectively (allP < 0.05). Detailed results are provided in\nFig. 4b and Supplementary Tables 7-8.\nEyeCLIP achieves zero-shot cross-modal retrieval\nBy learning an aligned latent space for multimodal embeddings, Eye-\nCLIP enabled zero-shot cross-modal retrieval. This included retrieving\ntext entries based on image queries (image-to-text, i2t), retrieving\nimages based on text queries (text-to-image, t2i), and retrieving images\nbased on image queries (image-to-image, i2i). This function is useful\nfor biomedical applications such a s identifying cases for research\ncohorts, assisting with rare disease presentations, and creating edu-\ncational resources. We evaluated EyeCLIP on two external multimodal\nimage-caption datasets, AngioReport and Retina Image Bank, which\ncover a diverse range of ophth almology concepts. To speci ﬁcally\ninvestigate the performance on rare diseases, we manually selected a\nsubset from Retina Image Bank containing only rare diseases. Fol-\nlowing previous studies\n11,33, we used Recall@K as the metric for cross-\nmodal retrieval.\nOn AngioReport, EyeCLIP achieved mean recall of 44.1%, 40.7%, and\n44.3% for text-to-image, image-to-image, and image-to-text retrieval,\nrespectively, outperforming BioMedCLIP’s 40.5%, 32.9%, and 40.1%\n(P < 0.01 for all tasks). On Retina Image Bank, EyeCLIP achieved a mean\nrecall of 50.2%, 43.3%, and 50.9%, outperforming BioMedCLIP’s 45.8%,\n35.8%, and 45.3% (P < 0.01 for all tasks). Supplementary Table 9 provides\ndetails on the model’s performance. Examples of the retrieved results are\npresented in Fig.5; EyeCLIP effectively retrieved similar contents using text\nor images as queries. It could retrieve relevant images based on text\ndescriptions, pair images with the same pathological condition or from the\nsame patient, andﬁnd the most correlated description with the image\ninputs.\nFig. 2 | Zero-shot performance on downstream ocular diseases datasets.\na AUROC. b AUPR. Error bars represent 95% conﬁdence intervals, and the centers\ncorrespond to computed values of each metric. EyeCLIP achieved signiﬁcantly better\nzero-shot performance than other models for both AUROC and AUPR. AUROC =\narea under the receiver operator characteristic curve, AUPR = area under the\nprecision-recall curve. EyeCLIP outperforms the second-best model FLAIR, a\npretrained vision-language model for universal retinal fundus image understanding.\nNotably, FLAIR was pretrained on public datasets, with its performance evaluated\nthrough internal validation. In contrast, EyeCLIP, which was not trained on these\npublic datasets, demonstrated its performance through external validation, high-\nlighting its strong generalizability.\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 4\nEyeCLIP demonstrates few-shot generalization on VQA\nVision-language foundation models have great potential for generalization\nin ophthalmic VQA. We combined the image encoder from each model\nwith a text encoder using a large language model (LLM), speciﬁcally\nLlama2-7b, to perform VQA. We conducted the few-shot VQA on the\nOphthalVQA\n34 dataset. OphthalVQA is an open-set VQA dataset, with the\ntraining set comprising 7,778 images across six modalities and VQA pairs\ncovering 40 diseases, including rare conditions. The test set includes 60\nimages across the same six modalities, representing 60 ophthalmic condi-\ntions and 600 QA pairs. As detailed in Supplementary Table 10, EyeCLIP\ndemonstrated superior alignment with the LLM, despite the image and\nlanguage modules beingﬁne-tuned on a limited amount of VQA data.\nEyeCLIP rankedﬁrst in terms of exact matching score and F1 score across all\nsettings with support numbers of 1, 2, 4, and 8\nFig. 3 | Few-shot classiﬁcation experiments.We investigated the label efﬁciency of\ndifferent pretrained models in a few-shot setting, varying the number of training\nlabels per class (nc = 1, 2, 4, 8, 16) in the APTOS2019 (a), MESSIDOR2 (b), IDRID\n(c), GLAUCOMA FUNDUS (d), PAPILA (e), JSIEC (f), RETINA (g), OCTDL (h),\nand OCTID (i) dataset. For each nc, we sampledﬁve different sets of training\nexamples and trained a weakly supervised model. Boxes indicate quartile values, and\nwhiskers extend to data points within 1.5× the interquartile range. EyeCLIP achieves\nsigniﬁcantly better performance (in terms of the mean AUROC ofﬁve runs) than\nother encoders for different sizes of training sets and across all datasets. AUROC =\narea under the receiver operator characteristic curve. AUPR results can be found in\nSupplementary Fig. 1.\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 5\nFig. 4 | Performance of EyeCLIP across ocular, systemic, and rare disease\nprediction tasks. aSupervised full-dataﬁnetuning on ocular disease tasks. EyeCLIP\nis on par with the 2nd best model RETFound on APTOS2019, MESSIDOR2, OCTID\n(P > 0.05), and surpasses all models on the other eight datasets.b Supervised full-\ndata ﬁnetuning on systemic disease prediction. EyeCLIP surpasses all other models.\n(P < 0.05).c Few-shot ﬁnetuning on rare disease classiﬁcation. EyeCLIP surpasses all\nother models. (P < 0.05). Boxes indicate quartile values, and whiskers extend to data\npoints within 1.5× the interquartile range. Detailed statistics can be found in Sup-\nplementary Tables 4-5. AUROC = area under the receiver operator characteristic\ncurve, AUPR area under the precision-recall curve.\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 6\nFig. 5 | Zero-shot multimodal retrieval performance. aModel comparison on two\ndatasets with image-text pairs, AngioReport and Retina Image Bank. Similarity in\nthe embedding space was computed between the query image and all text samples in\nthe database. The top-K most similar texts were retrieved. We report Recall@K for K\n∈ {1, 5, 10} and the mean recall, which averages over K. We compared different\nmodels in text-to-image (1\nst column), image-to-image (2nd column) and image-to-\ntext (3rd column). EyeCLIP outperforms other baselines on all retrieval tasks. Error\nbars indicate 95% conﬁdence intervals.b Schematic illustrates zero-shot cross-\nmodal retrieval.c, d Examples of images in the top one retrieved result from the\nRetina Image Bank. More examples can be found in Supplementary Fig. 3.\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 7\nEyeCLIP reveals disease-relevant regions via patch-level simi-\nlarity visualization\nTo investigate which image regions the model attends to under different\ndisease conditions, we performed a patch-wise similarity analysis using\nEyeCLIP. Given a disease-speciﬁc textual prompt (e.g.,“Color fundus\nphotography, diabetic retinopathy”), we calculated the cosine similarity\nbetween the normalized text embedding and each visual patch token\nobtained from the image encoder. The resulting similarity maps were\nvisualized as heatmaps overlaid on the original fundus images. As illustrated\nin Supplementary Fig. 4, the model consistently highlights clinically relevant\nregions, such as the cherry-red spot and retinal edema in central retinal\nartery occlusion, enlarged optic disc cup in glaucoma, abnormal hyper-/\nhypo-ﬂuorescence on FFA in wet age-related macular degeneration and\nchoroidal melanoma, hyperreﬂective bands in retinal detachment, hyper-\nreﬂective bumps in drusen, corneal ulcers, and abnormal conjunctival fea-\ntures in conjunctivitis. These result ss u g g e s tt h a tt h em o d e li sc a p a b l eo f\nsemantically aligning textual disease descriptions with spatially meaningful\nfeatures in retinal images.\nDiscussion\nIn this study, we developed EyeCLIP, a visual-language foundation model\nfor multimodal ophthalmic image analysis, utilizing a large dataset of\n2,777,593 ophthalmic images spanning11 modalities, along with corre-\nsponding hierarchical language data. Our novel training strategy fully\nleverages real-world data nature, characterized by multi-examination and\nlarge amounts of unlabeled and labeled data. This approach achieved a\nshared representation across multiple examinations and modalities. Eye-\nCLIP signiﬁcantly enhances the analysis of ophthalmic and systemic dis-\neases, demonstrating state-of-the-art efﬁciency and generalizability in zero-\nshot, few-shot, and full-dataﬁnetuning downstream tasks.\nOne primary advantage of EyeCLIP lies in its alignment of multiple\nexaminations, which is demonstrated in the image-image retrieval task and\nmultimodal image classiﬁcation tasks. In contrast, conventional foundation\nmodels often focus on speciﬁc types of examination, which limits their\neffectiveness for real-world applications. Given the complexity of real-world\nclinical settings, where patients present with various conditions and\nundergo multiple tests, a model capableof accurately identifying diverse eye\nconditions with different image modalities is highly desirable. Our ablation\nstudy further highlights the necessity of EyeCLIP’s hybrid approach. Models\nrelying solely on contrastive learning (e.g., CLIP, BioMedCLIP, Pub-\nMedCLIP, and FLAIR) without image self-reconstruction learning suffer\nsigniﬁcant performance drops. This suggests that image self-reconstruction\nlearning is crucial for maintaining robust feature representations, particu-\nlarly when handling diverse modalities, as it helps preserve structural and\nsemantic consistency across different imaging techniques. Conversely, using\nonly self-reconstruction without contrastive learning (e.g., RetFound) also\nleads to performance degradation, indicating that contrastive learning is\nessential for aligning cross-modal features and enhancing the model’sa b i l i t y\nto leverage complementary information. Compared to VisionFM, which\nadopts modality-speciﬁc encoders with a shared embedding space, EyeCLIP\nemploys a uniﬁed encoder to process various ophthalmic modalities. This\ndesign offers better scalability andreduces the burden of training and\ndeploying multiple encoders, making it more practical for real-world clinical\ndeployment. While modality-speci ﬁc designs like RETFound and\nVisionFM may captureﬁner modality-speciﬁc details, they add archi-\ntectural complexity and limit adaptability when new or low-resource\nmodalities are introduced. EyeCLIP’s single-encoder framework promotes\nﬂexible modality alignment and streamlines integration into multi-exam\nclinical workﬂows.\nEyeCLIP was developed using 11 imaging modalities collected from\ndiverse populations, making it uniquely powerful for diagnosing vision-\nthreatening diseases, particularly in multimodal, multi-disease diagnostics\nwith label imbalance and rare diseases. Notably, the challenging Retina\nImage Bank underscores its potential for managing rare eye conditions with\ndiverse examination. This capability likely arises from its cross-modal\nrepresentation learning during pretraining, enabling the capture of com-\nplementary patterns across various imaging modalities. However, to\nestablish explicit causal links between these multimodal interactions and\nmodel performance, future studies incorporating specialized interpretability\nframeworks are needed\n35.\nAnother major strength of EyeCLIP is its easy integration into the\nvisual-language framework. While previous foundation models primarily\nfocused on extracting meaningful patterns from rich image data, EyeCLIP\nutilized textual descriptions created by medical professionals to distill\nhierarchical context information. By employing text-image contrastive\nlearning, EyeCLIP maximized the use of all available labeled ophthalmic\ndata, learning semantically rich features that align visual patterns with\nclinical concepts. This alignment offers zero-shot capabilities, signiﬁcantly\nreducing the need for extensive annotation of training data. When inte-\ngrated with LLM, its few-shot VQA capability presents a unique opportu-\nnity to automate interpretative tasks in clinical settings with minimal model\nadjustments and training data. Highlighting the possibility of integrating\nEyeCLIP into the existing ophthalmic chat systems to perform multitask\nVQA\n8,36–38.E y e C L I P’s ability to operate with minimal training data and\nadapt to new tasks makes it a valuable tool for expanding the reach of quality\nophthalmic care widely.\nOphthalmic images are increasingly used to indicate systemic diseases\ndue to their accessibility\n39–42.T h i si sw h e r et h ef o u n d a t i o nm o d e lc o u l db e\nwell appreciated due to the scarcity of event data compared with a healthy\npopulation. Notably, EyeCLIP signiﬁcantly improved systemic disease\nprediction, surpassing previous medical domain foundation models, such as\nBioMedCLIP and the ophthalmology domain model RETFound, in events\nincluding stroke, dementia, PD, and MI. This improvement is likely\nattributed to the shared representation of different examination data. For\nexample, angiography provides better visualization of retinal blood vessels\nand lesions, and these features could be jointly learned by the model. After\nfurther optimization, EyeCLIP can be a powerful tool for early detection and\nmonitoring of systemic diseases, enhancing patient care beyond ophthal-\nmology. Future studies could furtherimprove predictive performance by\nintegrating ocular biomarkers with multimodal systemic health data,\nincluding electronic health records.\nThis study offers insights for other medical domains dealing with\nincomplete or unaligned data. In real-world clinical practice, it is com-\nmon for datasets to contain multimodal information, such as images and\ntext, that are not fully aligned across every sample. In this work, we\naddress this challenge by employing a strategy that combines self-\nsupervised learning through masked-image reconstruction within single\nmodalities and contrastive learning across aligned multimodal data\nwhen available. This approach maximizes the utility of diverse clinical\ndata accumulated in practice, offering a potential framework for\ndeveloping medical foundation models in otherﬁelds where incomplete\nmultimodal data is prevalent.\nOur study has several limitations. Firstly, EyeCLIP’s performance relies\non the quality and diversity of the training data. For example, while it shows\nrobust diagnostic capabilitiesin cross-population CFP classiﬁcation, a\nperformance discrepancy exists across ethnic groups, with higher AUC in\nEast Asian cohorts (JSIEC 0.977; Glaucoma Fundus 0.913), likely due to the\nChinese-dominated training data. Toa d d r e s st h i sb i a sa n de n h a n c eg e n -\neralizability, future work should incorporate more balanced and ethnically\ndiverse datasets, covering underrepresented populations. Moreover, stra-\ntegies such as demographic-aware sampling, domain adaptation, and cross-\ndomain contrastive learning will be explored to mitigate population-speciﬁc\nbiases and improve the model’s fairness and reliability across diverse clinical\nsettings. Secondly, many modalities in our dataset, such as FFA, ICGA, and\nOCT, are inherently 3D, capturing essential dynamic lesion changes and\nvolumetric information\n43,44. However, in this version, we utilized only 2D\nslices. Future work incorporating the full 3D information may further\nenhance the model’s performance and capability. Thirdly, while EyeCLIP\nrequires no more than 8 GB for single-image inference and is deployable on\ncommon edge GPUs (with detailed comparisons of inference time and\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 8\ntraining resource requirements provided in Supplementary Table 11), real-\ntime clinical use may beneﬁt from model distillation and quantization to\nfurther reduce computational demands45. Additionally, ensuring inter-\npretability and transparency is crucial for gaining the trust of healthcare\nproviders and patients, ensuring successful implementation in clinical\npractice.\nIn conclusion, we developed EyeCLIP, a visual-language foundation\nmodel characterized by shared multimodal representations capable of\nperforming a wide range of downstreamtasks. The novel training strategy\naligns well with real-world data characteristics, potentially informing the\ndevelopment of foundation models in general medicine. EyeCLIP’so u t -\nstanding performance and broad applicability to ocular and systemic dis-\neases position it as a promising tool to enhance the accuracy, efﬁciency, and\naccessibility of AI in ophthalmic clinical practice and research.\nMethods\nEthics statement\nThis study was conducted in accordance with the Declaration of Helsinki\nand received approval from the Hong Kong Polytechnic University’s\ninstitutional review board (HSEARS20240202004). The Institutional\nReview Board waived informed consentdue to the retrospective analysis of\nanonymized ophthalmic images and public datasets.\nData curation and preprocessing for pretraining\nWe collected a vast amount of unlabeled ophthalmic images from 227\nhospitals across China, totaling 2,777,593 images from 128,554 participants.\nThe gender distribution was balanced(68,531 female, 59,994 males, and 29\nunknown). The participants had a mean age of 50.4 years (SD: 23.3, range:\n1–98), representing a broad demographic. All participants were of Chinese\nethnicity. These images covered a variety of ocular conditions and com-\nprised 11 different image modalities, including CFP, FFA, indocyanine\ngreen angiography (ICGA), and OCT, among others. Not all patients\nunderwent imaging for all 11 modalities, leading to missing modalities for\nsome individuals. The percentage of images per modality and class dis-\ntribution can be found in Fig.1a, “Multimodal dataset characteristic.”\nTo ensure the quality of the data, we excluded low-quality images from\nCFP, FFA, and ICGA by extracting and analyzing the vascular\nstructures\n40,46.S p e c iﬁcally, images with detachable vascular ratios less than\n0.04 for CFP and less than 0.01 for FFA and ICGA were removed. Images\nfrom other modalities were sampled (50 images per modality) and manually\nreviewed as of sufﬁcient quality. Additionally, since they were captured in\nclinical settings and optimized for patient distribution, no speciﬁcq u a l i t y\ncontrol method was applied. The language training data were sourced from\n11,180 angiography reports of 11,180 participants. Since the reports contain\ncustom templates and are generally lengthy, we developed a custom dic-\ntionary by integrating medical expert knowledge using keyword-based\nregular expressions to extract essential medical concepts from the report\ntexts. The dictionary includes tree-structured keywords [e.g.,“Diabetic\nretinopathy (DR)→ Non-proliferative DR (NPDR)→ Mild NPDR”],\nwhere parent nodes represent broader concepts and child nodes correspond\nto more speciﬁc terms. The medical reports were therefore converted into a\nset of keywords covering various aspects such as ophthalmic diseases,\nanatomical structures, and diagnostic indicators\n8,36. This process provided\ncrucial semantic information for subsequent image-text alignment and\npretraining. Before model development, all data, including images and\nophthalmic reports, were de-identiﬁed. Additional information about the\npretraining dataset is summarized in Fig.1.\nTo facilitate multimodal alignment, we matched ophthalmic images\nf r o md i f f e r e n te x a m i n a t i o n st oo b t a i ni m a g ep a i r sf r o mt h es a m ep a t i e n t ,\nenabling the model to learn features across different imaging modalities\nmore effectively.\nData curation and preprocessing for downstream validation\nSupplementary Table 1 summarizes the details of datasets used for down-\nstream validation. We included 14 datasets, covering ocular disease\ndiagnosis (single-modality classiﬁcation, multimodality classiﬁcation, and\nVQA) and systemic disease prediction.\nOphthalmic single-modality classiﬁcation datasets. We compiled\n9 publicly available single-modality ophthalmic disease classiﬁcation\ndatasets from diverse ethnicitiesand regions, comprising 7 CFP and 2\nOCT datasets. The CFP datasets included IDRiD (India, 516 images)47,\nAPTOS2019 (India, 3662 images), and MESSIDOR2 (France, 1744\nimages) for DR diagnosis; PAPILA (Spain, 488 images)\n48 and Glau-\ncoma Fundus (South Korea, 1544 images)49 for glaucoma diagnosis; as\nwell as JSIEC50 and Retina for the classiﬁcation of multiple ophthalmic\ndiseases. The OCT datasets included OCTID (India, 572 images)26 and\nOCTDL (Russia, 2064 images) 27, both containing multiple disease\nlabels.\nOphthalmic multimodality classiﬁcation datasets. We also collected\ntwo multimodality, multi-label datasets: the AngioReport28 dataset and the\nRetina Image Bank29. The AngioReport datasetcomprises approximately\n50,000 angiographic images collected from routine clinics in Thailand,\nencompassing FFA and ICGA modalitiesand covering 142 retinal diseases.\nWe selected a test subset of 10,520 images to validate our model. The Retina\nImage Bank, sourced from the United States, is a large open-access repo-\nsitory of retinal images containing 14 modalities and 84 ophthalmic dis-\neases. We obtained images and their correspondingﬁndings from the\nwebsite and created a custom dictionary to standardize different disease\nexpressions using keyword matching and regular expressions. The stan-\ndardized labels incorporate hierarchical structures, such as“DR, mild DR”\nfor mild diabetic retinopathy. We excluded non-standard retinal exam-\nination images, including schematic cartoons, histology, and pathology\nimages. To increase efﬁciency, we focused on images uploaded between\n2019 and 2023 and removed instances with fewer than 50 occurrences. This\nprocess yielded aﬁnal dataset of 3293 images.\nOphthalmic VQA Dataset. OphthalVQA(test)\n34 is a dataset of ocular\nmultimodal images from China, including CFP, OCT, FFA, slit-lamp,\nscanning laser ophthalmoscopy (SLO), and ocular ultrasound images. Ten\nimages representing distinct diagnoses were selected for each modality,\nresulting in a test set of 60 images and 600 free-form question-answer (QA)\npairs generated by ophthalmologists. These images reﬂect typical disease\nmanifestations commonly used for clinical diagnosis. This dataset serves as\nthe testing dataset for the VQA downstream task.\nTo facilitate few-shot VQA experiments, we manually curated a\ntraining dataset, OphthalVQA(train), aligned with the OphthalVQA for-\nmat. This training dataset comprisesﬁve modalities— CFP, FFA, OCT,\nB-scan ultrasound, and slit lamp— encompassing 54 diseases or conditions.\nIt includes 7778 open-ended QA pairs, with each disease or condition\nrepresented by 7 to 16 images. Each image is paired with 6 to 13 questions\ncovering imaging modalities, laterality, diagnosis, image descriptions, and\nlesion-speciﬁc inquiries. Supplementary Table 1 provides detailed char-\nacteristics of the dataset.\nSystemic chronic disease dataset. UK Biobank\n32 is a population-based\nprospective cohort from the United Kingdom, recruiting approximately\n500,000 participants aged 40 to 69 between 2006 and 2010. Among these\nparticipants, 82,885 underwent CFP examinations, generating a dataset of\n171,500 retinal images. To deﬁne outcomes, we used algorithm-based\nclassiﬁcations (Category 42) developed by the UK Biobank outcome adju-\ndication group. These algorithms integrate coded information from baseline\nassessments and linked datasets, providing replicable outcome deﬁnitions\nfor major systemic diseases, including stroke, dementia, Parkinson’sd i s e a s e\n(PD), and myocardial infarction (MI).T h i sm e t h o de l i m i n a t e st h en e e dt o\nmanually select diagnostic and procedural codes, ensuring reliable and\nreproducible outcome deﬁnitions. To minimize potential biases from var-\niations in individual visits, we included only the retinal images of the right\neye from a single visit per patient.\nModel Design and Training Details\nAll experiments were conducted in Python 3.10. For visual-language pre-\ntraining, we employed CLIP23 as our base framework, which is a pretrained\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 9\nmodel that leverages contrastive learning using natural image-text pairs.\nThis model processes image and text inputs independently through an\nimage encoder and a text encoder, generating distinctive vector repre-\nsentations for each modality. Subsequently, these vectors are projected into a\nuniﬁed multimodal embedding space, facilitating direct comparisons\nbetween textual and visual elements.\nWe extended the traditional CLIP architecture by adding an image\ndecoder to the CLIP image encoder, following Masked Autoencoders\n(MAE)\n51. This addition enables the model to perform masked image\nreconstruction, which is pivotal for self-supervised feature representation\nlearning. Speciﬁcally, besides the original image-text contrastive loss\nLimg/C0 text,w em o d iﬁed the loss function of CLIP by adding an image\nreconstruction lossLrecon, and an image-image contrastive lossLimg/C0 img.\nLimg/C0 text is used to align the image and corresponding text descrip-\ntions, which is deﬁned as:\nLimg/C0 text ¼/C0 1\nN\nXN\ni¼1\nlog exp sim fx i\n/C0/C1\n; gt i\n/C0/C1/C0/C1\n=τ\n/C0/C1\nPN\nj¼1 exp sim fx i\n/C0/C1\n; gt j\n/C16/C17/C16/C17\n=τ\n/C16/C17 ð1Þ\nwhere f(x) and g(t) are the encoded image and text representations,sim\ndenotes the similarity measure,typically cosine similarity, andτ is a tem-\nperature parameter.τ controls the sharpness of the similarity distribution in\ncontrastive learning. A lowerτ enhances discrimination but may cause\ntraining instability, while a higherτ smooths the distribution but weakens\nhard negative differentiation.Empirical tuning determinedτ =0 . 0 7 a s\noptimal for stable and effective alignment.\nSimilarly,Limg/C0 img aligns the features between different modalities of\nimages, which is deﬁned as:\nLimg/C0 img ¼/C0 1\nN\nXN\ni¼1\nlog\nexp sim fx i\n/C0/C1\n; fx j\n/C16/C17/C16/C17\n=τ\n/C16/C17\nPN\nk¼1 exp sim fx i\n/C0/C1\n; fx k\n/C0/C1/C0/C1\n=τ\n/C0/C1 ð2Þ\nTo align different image modalities, we employ a shared vision encoder\nthat processes all modalities under a uniﬁed contrastive learning objective,\nencouraging the model to learn modality-invariant embeddings without\nexplicit fusion layers. Alignment is reinforced through contrastive learning,\nwhere positive pairs consist of different modality representations of the\nsame underlying content, while negative pairs come from different samples.\nThis approach avoids modality-speciﬁc encoders or handcrafted fusion\nmechanisms, and instead allows the model to implicitly align modalities\nthrough feature-level supervision.\nLrecon is the loss for reconstructing masked images, which is deﬁned as:\nLrecon ¼ 1\nN\nXN\ni¼1\njbxi /C0 xij2\n2 ð3Þ\nWhere bx and x are the reconstructed and original images, respectively.\nThe ﬁnal loss function for training our model is the combination of the\nthree losses:\nL ¼ λimg/C0 textLimg/C0 text þ λimg/C0 imgLimg/C0 img þ λreconLrecon ð4Þ\nAmong them,λimg/C0 text and λimg/C0 img are set to 0.75, andλrecon is set to 1,\nbased on hyperparameter tuning experiments.\nIn EyeCLIP, all images share the same encoder, ensuring consistent\nfeature extraction across different modalities. This innovative combination\nof CLIP and MAE distinguishes our approach from traditional CLIP\nmodels, enhancing its capability by utilizing a large amount of\nunlabeled data.\nDuring the training phase of EyeCLIP, we cropped the images toﬁeld-\nof-view and resized them to 224 × 224, and applied data augmentation,\nincluding random resized cropping, color jitter, and horizontalﬂipping. The\nEyeCLIP was trained with a base learning rate of 0.001 for theﬁrst\n2000 steps, with a 2-epoch warm-up,followed by cosine decay to zero\nthroughout the training process. A batch size of 200 was used, and training\nwas conducted on one NVIDIA Tesla V100 (32 GB) GPU for approximately\nfour weeks. At the end of the training, the model with the lowest loss on the\nvalidation set was selected for testing.\nDetails of the Comparison Models\nPubMedCLIP is a CLIP model speciﬁcally ﬁnetuned for the medical\ndomain25. Trained on the Radiology Objects in COntext (ROCO) dataset52,\nit encompasses over 80,000 samples from various medical imaging mod-\nalities like ultrasound, X-rays, computed tomography, magnetic resonance\nimaging, and various body regions. The texts used for training were the\nrelatively short captions associatedwith the images in the ROCO dataset.\nExperimental outcomes showcased that leveraging PubMedCLIP as a pre-\ntrained visual encoder led to a potential performance boost of up to 3% for\nexisting MedVQA models.\nBioMedCLIP is a multimodal biomedical foundation model pre-\ntrained using 15 million scientiﬁc image-text pairs extracted from 4.4 mil-\nlion articles in PubMed Central\n24. It incorporates a domain-speciﬁcl a n -\nguage model (PubMedBERT)53, utilizes larger vision transformers, and\nintegrates other domain-speciﬁc optimizations. Compared to general-\ndomain CLIP and previous biomedical vision-language models such as\nPubMedCLIP, BioMedCLIP demonstrates superior performance across\nvarious downstream tasks, including cross-modal retrieval, zero-shot image\nclassiﬁcation, and VQA.\nRETFound is trained on a vast dataset comprising 1.6 million unla-\nbeled retinal images through self-supervised reconstruction\n10. It leveraged\ntwo ophthalmic modalities, CFP and OCT, to train separate weights for each\nmodality. RETFound surpassed other comparative models, including those\npretrained on ImageNet, in diagnosing sight-threatening eye conditions and\npredicting systemic disorders.\nFLAIR\n54 It is a pretrained vision-language model (ResNet50-based) for\nuniversal retinal fundus image understanding. It was trained using 37 open-\naccess, mostly categorical fundus imaging datasets from various sources,\nwith up to 97 different target conditions and 284, 660 images. It uses a\ntextual expert’s knowledge to describe theﬁne-grained features of the\npathologies as well as the hierarchies and dependencies between them. It has\nbeen extensively validated to outperform more generalist, larger-scale\nimage-language models such as CLIP or BiomedCLIP.\nDownstream Validation Details\nZero-shot Classiﬁcation. For zero-shot transfer, we followed the method in\nthe CLIP experiment. Each class was associated with a text prompt consisting\nof the modality and class name (for example,‘color fundus, diabetic retino-\npathy’). We computed theℓ2-normalized embedding using the text encoder\nand image encoder from EyeCLIP for the prompt and image. For each image,\nwe computed theℓ2-normalized embedding and then computed cosine-\nsimilarity scores between the image and each text embedding, and the pre-\ndicted class was consequently the class with the highest similarity score.\nFew-shot Classiﬁcation. For few-shot classiﬁcation, we varied the\nnumber of labeled examples per class forﬁnetuning EyeCLIP (known as\n‘shot’)f r o mn=1 ,2 ,4 ,8 ,1 6 ,a n dt e s t e dt h em o d e lo nt h et e s ts e ts i m i l a rt o\nfull-dataﬁnetune classiﬁcation.\nFull-data Fine-tune Classiﬁcation. We used each image encoder to\nextract a low-dimensional feature embedding from each image and\nadded a multilayer perceptron to map the image feature representation\nto logits, which were interpreted as class probabilities after softmax\nnormalization. During ﬁnetuning, the encoder was frozen for theﬁrst\nﬁve epochs and unfrozen afterward. A total of 50 epochs was trained for\neach model. For single-label classiﬁcation tasks, we used a batch size of\n16. Theﬁrst ten epochs implemented a learning rate warm-up from 0 to\n5×1 0\n−4, followed by a cosine annealing schedule reducing the learning\nrate from 5 × 10−4 to 1 × 10−6 over the remaining 40 epochs. Addition-\nally, we adopted label smoothing cross-entropy loss with a smoothing\nfactor of 0.1 to prevent the model from becoming overly conﬁdent in\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 10\ndominant classes. For multi-label classiﬁcation tasks in AngioReport\nand Retina Image Bank, we used a batch size of 4, trained for 30 epochs,\nand set the learning rate to 0.01. After each epoch, we evaluated the\nmodel on the validation set, saving the model weights with the highest\nAUROC for internal and external assessments.\nCross-Modal Retrieval. For cross-modal retrieval, we used the same\nmethod as zero-shot classiﬁcation above to retrieve the top-K images that\nwere closest in the aligned latent space to a speciﬁc text query (text-to-image\nretrieval). Image-to-text and image-to-image retrieval were performed\nanalogously. To evaluate retrieval, we used Recall@K, which measures the\npercentage of correct results included in these top-K retrieved samples. We\nchose K∈ (1, 5, 10) and reported mean recall by averaging the scores over\nthe three Recall@K values.\nVisual Question Answering. For visual question answering, we used\nthe image encoder from EyeCLIP to extract image features, which were then\nconcatenated with text features (questions). The combined feature was fed\ninto the language model Vicuna (Llama 2-7b)\n55 for language generation,\nperforming VQA.\nTo enhance multi-disease alignment, we employed the OpthalVQA\n(train) dataset for few-shotﬁnetuning. Support examples per modality and\ndisease were set to 1, 2, 4, and 8. For each scenario,ﬁve independent trials\nwere performed with different random seeds to ensure robustness and\nreduce the inﬂuence of random variations in training set selection. We\nutilized the Low-Rank Adaptation (LoRA)\n56 method for efﬁcientﬁnetuning,\nrunning for three epochs with an initial learning rate 2e-5. Cosine annealing\nwas applied to adjust the learning rate dynamically. The model’sp e r f o r -\nmance was evaluated on the OpthalVQA (test) dataset using the checkpoint\nfrom theﬁnal epoch.\nEvaluation Metrics\nWe employed the AUROC and AUPR metrics to assess the performance of\nclassiﬁcation tasks. These metrics gauge the classiﬁcation effectiveness based\non the receiver operating characteristics and precision-recall curves. When\ndealing with binary classiﬁcation tasks, such as ocular disease diagnosis, we\ncomputed AUROC and AUPR in a binary context. For multi-class classi-\nﬁcation tasks such asﬁve-stage DR and multi-class disease diagnosis, we\ncalculated AUROC and AUPR individually for each disease class and then\naveraged them (macro) to derive the overall AUROC and AUPR scores.\nRegarding VQA tasks, we utilized various classiﬁcation-based metrics\nto evaluate performance, including the exact match score, F1 score, preci-\nsion, recall, and language-based metric metrics such as BLEU\n57 and sentence\nsimilarity.\nFor retrieval tasks, we used the metric Recall@K, which is the pro-\nportion of the data correctly retrieved among the top-K retrieved samples.\nStatistical Analysis\nWe employed descriptive statistical methods to analyze demographic data,\nincluding age and gender. Two-sided t-tests were used to compare the\nAUROC and AUPR of EyeCLIP with those of other models (CLIP, Bio-\nMedCLIP, PubMedCLIP, RETFound, or FLAIR), selecting the most com-\npetitive model in each task based on mean performance to determine\nstatistical signiﬁcance. To enhance the robustness of the results, we conducted\nmultiple trials usingﬁve different random seeds in the classiﬁcation and VQA\ntasks. Theﬁnal results were reported as the mean of theseﬁve trials, with the\n95% conﬁdence interval (CI) calculated using 1.96 × standard error.\nData availability\nWe do not have permission to redistribute the datasets used for developing\nEyeCLIP, the data may be available under constrained access from the cor-\nresponding author upon reasonable request. Downstream datasets can be\naccessed via the links: IDRID (https://ieee-dataport.org/open-access/indian-\ndiabetic-retinopathy-image-dataset-idrid), MESSIDOR2 (https://www.adcis.\nnet/en/third-party/messidor2/), APTOS-2019 (https://www.kaggle.com/c/\naptos2019-blindness-detection/overview), PAPILA ( https://ﬁgshare.com/\narticles/dataset/PAPILA/14798004/1), Glaucoma Fundus (https://doi.org/\n10.7910/DVN/1YRRAC), JSIEC ( https://zenodo.org/record/3477553),\nRetina (https://www.kaggle.com/datasets/jr2ngb/cataractdataset), OCTID\n(https://borealisdata.ca/dataverse/OCTID), OCTDL (https://ieee-dataport.\norg/documents/octdl-optical-coherence-tomography-dataset-image-based-\ndeep-learning-methods), AngioReport (https://tianchi.aliyun.com/dataset/\n170128), Retina Image Bank (https://imagebank.asrs.org/), OphthalVQA\n(https://ﬁgshare.com/s/3e8ad50db900e82d3b47).\nCode availability\nCode available athttps://github.com/Michi-3000/EyeCLIP.\nReceived: 17 January 2025; Accepted: 3 June 2025;\nReferences\n1. Burton, M. J. et al. The Lancet Global Health Commission on Global\nEye Health: vision beyond 2020.Lancet Glob. Health9, e489– e551\n(2021).\n2. Resnikoff, S. et al. Estimated number of ophthalmologists worldwide\n(International Council of Ophthalmology update): will we meet the\nneeds? Br. J. Ophthalmol.104, 588– 592 (2020).\n3. Ye, J., He, L. & Beestrum, M. Implications for implementation and\nadoption of telehealth in developing countries: a systematic review of\nChina’s practices and experiences.NPJ Digital Med.6, 174 (2023).\n4. Cui, M. & Zhang, D. Y. Artiﬁcial intelligence and computational\npathology. Lab. Investig.101, 412– 422 (2021).\n5. Vorontsov, E. et al. A foundation model for clinical-grade\ncomputational pathology and rare cancers detection.Nat. Med30,\n2924– 2935 (2024).\n6. Li, J.-P. O. et al. Digital technology, tele-medicine and artiﬁcial\nintelligence in ophthalmology: a global perspective.Prog. Retinal Eye\nRes. 82, 100900 (2021).\n7. Ting, D. S. W. et al. Development and Validation of a Deep Learning\nSystem for Diabetic Retinopathy and Related Eye Diseases Using\nRetinal Images From Multiethnic Populations With Diabetes.JAMA\n318, 2211– 2223 (2017).\n8. Chen, X. et al. FFA-GPT: an automated pipeline for fundusﬂuorescein\nangiography interpretation and question-answer.npj Digital Med.7,\n111 (2024).\n9. Moor, M. et al. Foundation models for generalist medical artiﬁcial\nintelligence. Nature 616, 259– 265 (2023).\n10. Zhou, Y. et al. A foundation model for generalizable disease detection\nfrom retinal images.Nature 622, 156– 163 (2023).\n11. Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T. J. & Zou, J. A\nvisual– language foundation model for pathology image analysis using\nmedical Twitter.Nat. Med29, 2307– 2316 (2023).\n12. Chen, R. J. et al. Towards a general-purpose foundation model for\ncomputational pathology.Nat. Med30, 850– 862 (2024).\n13. Chia, M. A. et al. Foundation models in ophthalmology.British Journal\nof Ophthalmology(2024).\n14. Yang, J. Multi-task learning for medical foundation models.Nat.\nComput Sci.4, 473– 474 (2024).\n15. Qiu, J. et al. Development and validation of a multimodal multitask\nvision foundation model for generalist ophthalmic artiﬁcial\nintelligence. NEJM AI1, AIoa2300221 (2024).\n16. Shi, D. et al. EyeFound: A Multimodal Generalist Foundation\nModel for Ophthalmic Imaging.arXiv preprint arXiv:2405.11338\n(2024).\n17. Nath, S., Marie, A., Ellershaw, S., Korot, E. & Keane, P. A. New\nmeaning for NLP: the trials and tribulations of natural language\nprocessing with GPT-3 in ophthalmology.Br. J. Ophthalmol.106,\n889– 892 (2022).\n18. Shi, D. et al. Translation of Color Fundus Photography into\nFluorescein Angiography Using Deep Learning for Enhanced Diabetic\nRetinopathy Screening.Ophthalmol. Sci.3, 100401 (2023).\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 11\n19. Song, F., Zhang, W., Zheng, Y., Shi, D. & He, M. A deep learning model\nfor generating fundus autoﬂuorescence images from color fundus\nphotography. Adv. Ophthalmol. Pr. Res3, 192– 198 (2023).\n20. Chen, R. et al. Translating color fundus photography to indocyanine\ngreen angiography using deep-learning for age-related macular\ndegeneration screening.npj Digital Med.7, 34 (2024).\n21. Shi, D. et al. Cross-modality Labeling Enables Noninvasive Capillary\nQuantiﬁcation as a Sensitive Biomarker for Assessing Cardiovascular\nRisk. Ophthalmol. Sci.4, 100441 (2024).\n22. Chen, X. et al. Evaluating large language models and agents in healthcare:\nkey challenges in clinical applications.Intelligent Medicine, (2025).\n23. Radford, A. et al. Learning transferable visual models from natural\nlanguage supervision. inInternational conference on machine learning\n8748-8763 (PMLR, 2021).\n24. Zhang, S. et al. A Multimodal Biomedical Foundation Model Trained\nfrom Fifteen Million Image– Text Pairs.NEJM AI2, https://doi.org/10.\n1056/AIoa2400640 (2024).\n25. Eslami, S., Meinel, C. & De Melo, G. Pubmedclip: How much does clip\nbeneﬁt visual question answering in the medical domain? inFindings\nof the Association for Computational Linguistics: EACL 20231181-\n1193 (2023).\n26. Gholami, P., Roy, P., Parthasarathy, M. K. & Lakshminarayanan, V.\nOCTID: Optical coherence tomography image database.Computers\nElectr. Eng.81, 106532 (2020).\n27. Kulyabin, M. et al. OCTDL: Optical Coherence Tomography Dataset for\nImage-Based Deep Learning Methods.Sci. Data11, 365 (2024).\n28. Zhang, W. et al. Angiographic Report Generation for the 3rd APTOS’s\nCompetition: Dataset and Baseline Methods.medRxiv, 2023-2011\n(2023).\n29. Retina Image Bank, available athttps://imagebank.asrs.org/.\n30. Gupta, K. & Reddy, S. Heart, Eye, and Artiﬁcial Intelligence: A Review.\nCardiol. Res12, 132– 139 (2021).\n31. Yusufu, M. et al. Retinal vascularﬁngerprints predict incident stroke:\nﬁndings from the UK Biobank cohort study.Heart, heartjnl-2024-\n324705 (2025).\n32. Chua, S. Y. L. et al. Cohort proﬁle: design and methods in the eye and\nvision consortium of UK Biobank.BMJ Open9, e025077 (2019).\n33. Lu, M. Y. et al. A visual-language foundation model for computational\npathology. Nat. Med.30, 863– 874 (2024).\n34. Xu, P., Chen, X., Zhao, Z. & Shi, D. Unveiling the clinical incapabilities: a\nbenchmarking study of GPT-4V(ision) for ophthalmic multimodal image\nanalysis.British Journal of Ophthalmology, bjo-2023-325054 (2024).\n35. Savage, T., Nayak, A., Gallo, R., Rangan, E. & Chen, J. H. Diagnostic\nreasoning prompts reveal the potential for large language model\ninterpretability in medicine.NPJ Digital Med.7, 20 (2024).\n36. Chen, X. et al. ICGA-GPT: report generation and question answering\nfor indocyanine green angiography images.Br. J. Ophthalmol.108,\n1450– 1456 (2024).\n37. Chen, X. et al. EyeGPT for Patient Inquiries and Medical Education:\nDevelopment and Validation of an Ophthalmology Large Language\nModel. J. Med. Internet Res.26, e60063 (2024).\n38. Zhao, Z. et al. Slit Lamp Report Generation and Question Answering:\nDevelopment and Validation of a Multimodal Transformer Model with\nLarge Language Model Integration.J. Med Internet Res26, e54047 (2024).\n39. Gende, M. et al. Automatic Segmentation of Retinal Layers in Multiple\nNeurodegenerative Disorder Scenarios.IEEE J. Biomed. Health\nInform. 27, 5483– 5494 (2023).\n40. Shi, D. et al. A Deep Learning System for Fully Automated Retinal\nVessel Measurement in High Throughput Image Analysis.Front\nCardiovasc Med9, 823436 (2022).\n41. Li, C. et al. Retinal oculomics and risk of incident aortic aneurysm and\naortic adverse events: a population-based cohort study.Int J Surg,\n(2025).\n42. Wu, Y. et al. Noninvasive early prediction of preeclampsia in pregnancy\nusing retinal vascular features.npj Digital Med.8, 188 (2025).\n43. Zhang, W. et al. Fundus2Video: Cross-Modal Angiography Video\nGeneration from Static Fundus Photography with Clinical Knowledge\nGuidance. inMedical Image Computing and Computer Assisted\nIntervention – MICCAI 689-699 (Springer Nature Switzerland,\nMorocco, 2024).\n44. Wu, X. et al. FFA Sora, video generation as fundusﬂuorescein\nangiography simulator.arXiv preprint arXiv:2412.17346(2024).\n45. Polino, A., Pascanu, R. & Alistarh, D.-A. Model compression via\ndistillation and quantization.in 6th International Conference on\nLearning Representations, (2018).\n46. Shi, D., He, S., Yang, J., Zheng, Y. & He, M. One-shot Retinal Artery\nand Vein Segmentation via Cross-modality Pretraining.Ophthalmol.\nSci. 4, 100363 (2024).\n47. Porwal, P. et al. Indian diabetic retinopathy image dataset (IDRiD): a\ndatabase for diabetic retinopathy screening research.Data3, 25 (2018).\n48. Kovalyk, O. et al. PAPILA: Dataset with fundus images and clinical\ndata of both eyes of the same patient for glaucoma assessment.Sci.\nData 9, 291 (2022).\n49. Ahn, J. M. et al. A deep learning model for the detection of both\nadvanced and early glaucoma using fundus photography.PloS one\n13, e0207982 (2018).\n50. Cen, L.-P. et al. Automatic detection of 39 fundus diseases and\nconditions in retinal photographs using deep neural networks.Nat.\nCommun. 12, 4828 (2021).\n51. He, K. et al. Masked autoencoders are scalable vision learners. 16000-\n16009.\n52. Pelka, O., Koitka, S., Rückert, J., Nensa, F. & Friedrich, C. M. Radiology\nobjects in context (roco): a multimodal image dataset. inIntravascular\nImaging and Computer Assisted Stenting and Large-Scale Annotation\nof Biomedical Data and Expert Label Synthesis: 7th Joint International\nWorkshop, CVII-STENT 2018 and Third International Workshop,\nLABELS 2018, Held in Conjunction with MICCAI 2018, Granada,\nSpain, September 16, 2018, Proceedings 3180-189 (Springer, 2018).\n53. Gu, Y. et al. Domain-speciﬁc language model pretraining for\nbiomedical natural language processing.ACM Trans. Comput.\nHealthc. (HEALTH)3,1 – 23 (2021).\n5 4 . S i l v a - R o d r i g u e z ,J . ,C h a k o r ,H . ,K o b b i ,R . ,D o l z ,J .&A y e d ,I .B .A\nfoundation language-image model of the retina (ﬂair): Encoding\nexpert knowledge in text supervision.Med. Image Anal.99, 103357\n(2025).\n55. Touvron, H. et al. Llama 2: Open Foundation and Fine-Tuned Chat\nModels. (2023).\n56. Hu, E. J. et al. LoRA: Low-Rank Adaptation of Large Language Models.\nin International Conference on Learning Representations(2021).\n57. Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. BLEU: a method for\nautomatic evaluation of machine translation. inthe 40th Annual\nMeeting 311 (Association for Computational Linguistics, 2001).\nAcknowledgements\nWe thank the American Society of Retina Specialists for providing the\nvaluable Retina Image Bank and InnoHK HKSAR Government for valuable\nsupports. The research described in this paper was conducted in the JC\nSTEM Lab of Innovative Light Therapy for Eye Diseases funded by The Hong\nKong Jockey Club Charities Trust. The study was supported by the Global\nSTEM Professorship Scheme (P0046113) and Henry G. Leong Endowed\nProfessorship in Elderly Vision Health (PI: Mingguang He). The sponsor or\nfunding organization had no role in the design or conduct of this research.\nAuthor contributions\nD.S., J.Y., S.H., and M.H. conceived the study. D.S. and W.Z. built the deep\nlearning model and ran experiments. M.H. provided data and computing\nfacilities. D.S., W.Z., J.Y., S.H., and X.C. contributed to key data interpretation.\nD.S. W.Z, X.C. wrote the manuscript. P.X., K.J., S.L., J.W., M.Y., S.L., Q.Z., Z.G.,\nX.X., and M.H. critically revised the manuscript. All authors have read and\napproved the manuscript.\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 12\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01772-2\n.\nCorrespondenceand requests for materials should be addressed to\nDanli Shi or Mingguang He.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International License, which permits any\nnon-commercial use, sharing, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if you\nmodiﬁed the licensed material. You do not have permission under this licence\nto share adapted material derived from this article or parts of it. The images or\nother third party material in this article are included in the article’s Creative\nCommons licence, unless indicated otherwise in a credit line to the material. If\nmaterial is not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-nc-\nnd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01772-2 Article\nnpj Digital Medicine|           (2025) 8:381 13",
  "topic": "Modalities",
  "concepts": [
    {
      "name": "Modalities",
      "score": 0.8408412933349609
    },
    {
      "name": "Computer science",
      "score": 0.7168161869049072
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6179778575897217
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5805594325065613
    },
    {
      "name": "Bridging (networking)",
      "score": 0.5268370509147644
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.46074238419532776
    },
    {
      "name": "Focus (optics)",
      "score": 0.42942002415657043
    },
    {
      "name": "Machine learning",
      "score": 0.41165563464164734
    },
    {
      "name": "Modal",
      "score": 0.4103420078754425
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I5124864",
      "name": "École Polytechnique Fédérale de Lausanne",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I8078737",
      "name": "Clemson University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210158318",
      "name": "Second Affiliated Hospital of Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210135742",
      "name": "Shanghai First People's Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210105949",
      "name": "Centre for Eye Research Australia",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I4210145693",
      "name": "Guangdong Academy of Medical Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210153930",
      "name": "Guangdong Provincial People's Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1294606314",
      "name": "Beijing Tongren Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I183519381",
      "name": "Capital Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I2800906782",
      "name": "Hong Kong Science and Technology Parks Corporation",
      "country": "HK"
    }
  ],
  "cited_by": 13
}