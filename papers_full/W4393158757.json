{
  "title": "Dual-Channel Learning Framework for Drug-Drug Interaction Prediction via Relation-Aware Heterogeneous Graph Transformer",
  "url": "https://openalex.org/W4393158757",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2221423395",
      "name": "Xiaorui Su",
      "affiliations": [
        "University of Illinois Chicago",
        "University of Chinese Academy of Sciences",
        "Xinjiang Technical Institute of Physics & Chemistry"
      ]
    },
    {
      "id": "https://openalex.org/A2149623131",
      "name": "Pengwei Hu",
      "affiliations": [
        "Xinjiang Technical Institute of Physics & Chemistry",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4211948945",
      "name": "Zhu-Hong You",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125104194",
      "name": "Philip S. Yu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2353026544",
      "name": "Lun Hu",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Xinjiang Technical Institute of Physics & Chemistry"
      ]
    },
    {
      "id": "https://openalex.org/A2221423395",
      "name": "Xiaorui Su",
      "affiliations": [
        "University of Illinois Chicago",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2149623131",
      "name": "Pengwei Hu",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4211948945",
      "name": "Zhu-Hong You",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353026544",
      "name": "Lun Hu",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1605286881",
    "https://openalex.org/W2995738274",
    "https://openalex.org/W4221163422",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3021975806",
    "https://openalex.org/W2559588208",
    "https://openalex.org/W2964473213",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W3035011799",
    "https://openalex.org/W3188887004",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W6794373440",
    "https://openalex.org/W2058760775",
    "https://openalex.org/W4224322651",
    "https://openalex.org/W4286373989",
    "https://openalex.org/W2091439417",
    "https://openalex.org/W3094497296",
    "https://openalex.org/W4285794944",
    "https://openalex.org/W4385330534",
    "https://openalex.org/W3100078588",
    "https://openalex.org/W3157889929",
    "https://openalex.org/W3211394146",
    "https://openalex.org/W4385763856",
    "https://openalex.org/W2419501139",
    "https://openalex.org/W3037601681",
    "https://openalex.org/W2767891136",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4299420194",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Identifying novel drug-drug interactions (DDIs) is a crucial task in pharmacology, as the interference between pharmacological substances can pose serious medical risks. In recent years, several network-based techniques have emerged for predicting DDIs. However, they primarily focus on local structures within DDI-related networks, often overlooking the significance of indirect connections between pairwise drug nodes from a global perspective. Additionally, effectively handling heterogeneous information present in both biomedical knowledge graphs and drug molecular graphs remains a challenge for improved performance of DDI prediction. To address these limitations, we propose a Transformer-based relatIon-aware Graph rEpresentation leaRning framework (TIGER) for DDI prediction. TIGER leverages the Transformer architecture to effectively exploit the structure of heterogeneous graph, which allows it direct learning of long dependencies and high-order structures. Furthermore, TIGER incorporates a relation-aware self-attention mechanism, capturing a diverse range of semantic relations that exist between pairs of nodes in heterogeneous graph. In addition to these advancements, TIGER enhances predictive accuracy by modeling DDI prediction task using a dual-channel network, where drug molecular graph and biomedical knowledge graph are fed into two respective channels. By incorporating embeddings obtained at graph and node levels, TIGER can benefit from structural properties of drugs as well as rich contextual information provided by biomedical knowledge graph. Extensive experiments conducted on three real-world datasets demonstrate the effectiveness of TIGER in DDI prediction. Furthermore, case studies highlight its ability to provide a deeper understanding of underlying mechanisms of DDIs.",
  "full_text": "Dual-Channel Learning Framework for Drug-Drug Interaction Prediction via\nRelation-Aware Heterogeneous Graph Transformer\nXiaorui Su1,2,3, Pengwei Hu1,2, Zhu-Hong You4, Philip S. Yu3, Lun Hu1,2,*\n1 Xinjiang Technical Institutes of Physics and Chemistry, Urumqi, China\n2 University of Chinese Academy of Sciences, Beijing, China\n3 University of Illinois Chicago, Chicago, USA\n4 Guangxi Key Lab of Human-machine Interaction and Intelligent Decision, Nanning, China\nsuxiaorui19@mails.ucas.ac.cn, zhuhongyou@gxas.cn, psyu@uic.edu, {hpw, hulun}@ms.xjb.ac.cn\nAbstract\nIdentifying novel drug-drug interactions (DDIs) is a crucial\ntask in pharmacology, as the interference between pharma-\ncological substances can pose serious medical risks. In re-\ncent years, several network-based techniques have emerged\nfor predicting DDIs. However, they primarily focus on lo-\ncal structures within DDI-related networks, often overlook-\ning the significance of indirect connections between pairwise\ndrug nodes from a global perspective. Additionally, effec-\ntively handling heterogeneous information present in both\nbiomedical knowledge graphs and drug molecular graphs re-\nmains a challenge for improved performance of DDI predic-\ntion. To address these limitations, we propose aTransformer-\nbased relatIon-aware Graph rEpresentation leaRning frame-\nwork (TIGER) for DDI prediction. TIGER leverages the\nTransformer architecture to effectively exploit the structure of\nheterogeneous graph, which allows it direct learning of long\ndependencies and high-order structures. Furthermore, TIGER\nincorporates a relation-aware self-attention mechanism, cap-\nturing a diverse range of semantic relations that exist between\npairs of nodes in heterogeneous graph. In addition to these ad-\nvancements, TIGER enhances predictive accuracy by model-\ning DDI prediction task using a dual-channel network, where\ndrug molecular graph and biomedical knowledge graph are\nfed into two respective channels. By incorporating embed-\ndings obtained at graph and node levels, TIGER can benefit\nfrom structural properties of drugs as well as rich contextual\ninformation provided by biomedical knowledge graph. Ex-\ntensive experiments conducted on three real-world datasets\ndemonstrate the effectiveness of TIGER in DDI prediction.\nFurthermore, case studies highlight its ability to provide a\ndeeper understanding of underlying mechanisms of DDIs.\nIntroduction\nDrug-drug interactions (DDIs) refer to the biological effects\nof concomitantly administered drugs, which can modify the\npharmacological effects of the drug, thereby leading to ad-\nverse drug reactions (ADRs) (Vilar et al. 2014). Studies\n(Finkel, Clark, and Cubeddu 2009) have shown that when\n2-5 drugs are combined, the incidence of adverse reactions\nis 18.6%. When more than 5 drugs are taken together, the in-\ncidence of adverse reactions is 81.4%. Such situations may\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ncause injuries or deaths to patients. Therefore, the identifi-\ncation of potential DDIs is of great significance for alleviat-\ning impacts of such emergencies, thus guiding drug devel-\nopment and benefiting public health.\nIn recent years, a significant line of research has been ex-\nplored for identifying potential DDIs from graph-structured\ndata, including homogeneous graphs and heterogeneous\ngraphs (Zhao et al. 2023). Compared to homogeneous\ngraphs composed solely of drug entities, heterogeneous\ngraphs excel in abstracting and modeling complex systems,\nthereby generating greater interest. In the field of DDI pre-\ndictions, two kinds of heterogeneous graphs are commonly\nused. One is biomedical knowledge graph, where each node\nrepresents a molecule, such as drug or protein, and edge\nis the relation between them. The other is drug molecular\ngraph, where each node denotes an atom, and each edge is a\nbond.\nIn view of the success of graph neural networks (GNNs)\n(Kipf and Welling 2017), there are several attempts to adopt\nGNNs to learn with heterogeneous graphs. In the case of\nbiomedical knowledge graphs, a relation-specific transfor-\nmation is designed to encode semantic relationships between\nentities. Subsequently, a message-passing framework can be\nemployed to propagate and aggregate information through-\nout the graph, enabling the learning of drug node-level repre-\nsentations. On the other hand, a common paradigm involves\nassigning a [VNode] (virtual node) to each atom or utilizing\na pooling function to aggregate information from individ-\nual atoms. This allows for the creation of a drug graph-level\nrepresentation that captures the overall structure of the drug\nmolecule.\nIndeed, GNN-based methods have demonstrated impres-\nsive performance in the DDI prediction task, significantly\nadvancing the analysis of biomedical knowledge graphs and\ndrug molecular graphs. However, they do have certain limi-\ntations (Chen, O’Bray, and Borgwardt 2022). The major lim-\nitation is that they learn drug representations solely by ag-\ngregating information from local neighbors. As a result, they\nmight encounter challenges in effectively capturing com-\nplex graph structures and long-range dependencies. This\nlimitation could impede their ability to fully comprehend\nthe intricate relationships inherent in biomedical knowledge\ngraphs and drug molecular graphs. In addition, these meth-\nods also suffer from the issue of over-squashing. The over-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n249\nsquashing leads to information loss and distortion during\niterative message-passing, thereby affecting model perfor-\nmances. It is thus essential to design a new architecture be-\nyond local neighborhood aggregation.\nTo address the mentioned issues, the Transformer archi-\ntecture (Vaswani et al. 2017) is considered a promising so-\nlution due to its ability to directly model distant nodes and\ncapture dependencies across the entire graph. Consequently,\nthere is active research into extending the Transformer ar-\nchitecture to graph-based tasks, such as DDI prediction. An\nexample of this is Graphormer (Ying et al. 2021), a recent\nTransformer-based model tailored for molecular graph-level\nrepresentation learning. Graphormer enhances the Trans-\nformer architecture to handle graph-structured data effec-\ntively. This is achieved through the inclusion of graph posi-\ntional encoding and attention mechanisms that consider both\nnode and edge features. As a result, the model not only eval-\nuates individual node attributes but also grasps the global\ntopological information.\nThough Graphormer is effective in graph-level represen-\ntation learning tasks, its property, which takes the entire\ngraph as input, limits its scalability and applicability to\nlarge-scale graphs, such as biomedical knowledge graphs.\nOn the other hand, biomedical knowledge graph is rich in\nsemantic information and the node pair in it may have var-\nious relationships. For example, a drug may interact with a\nprotein through different mechanisms or may exhibit vari-\nous side effects depending on the context. However, most of\nthe existing Transformer architecture-based approaches lack\nthe ability to differentiate the significance of multiple rela-\ntionships between a pair of nodes.\nTo address above issues, we propose a novelT\nransformer-\nbased relatIon-aware Graph rEpresentation leaRning frame-\nwork, TIGER for brevity, to model DDI predictions at both\ngraph and node levels. TIGER handles the node-level rep-\nresentation learning tasks with sampled subgraphs centred\non drug nodes, rather than the whole graph. TIGER also\nimplements its encoder by a relation-aware self-attention\nmechanism for processing multiple relationships between a\nnode pair, rather than self-attention mechanism. With these\ntwo designs, TIGER is capable of learning both graph-level\nand node-level representations. Then the drug representa-\ntion is obtained by fusing representations from different lev-\nels. Finally, TIGER makes DDI predictions with obtained\ndrug representations. The main contributions of this work\nare summarized as follows:\n• We model the DDI data using a novel dual-channel het-\nerogeneous graph approach. Our approach aims to more\neffectively uncover potential DDIs by considering the di-\nverse aspects of drug characteristics and their relation-\nships within the biomedical domain.\n• We introduce a relation-aware self-attention mechanism\ndesigned to capture and leverage the diverse and multiple\nsemantic relationships present in the graph data.\n• We conduct extensive experiments on three datasets to\ndemonstrate the effectiveness of TIGER in both predict-\ning DDIs and understanding of the underlying mecha-\nnisms of drug interactions.\nRelated Work\nMolecular Graph-based Method. Drug molecular\ngraphs play a crucial role in providing a structural repre-\nsentation of drugs, enabling a better understanding of their\nchemical composition and potential interactions with other\nmolecules (Vilar et al. 2014). Deep learning approaches,\nsuch as graph convolutional networks (GCN), graph atten-\ntion networks (GAT), have been employed to predict DDIs\nbased on drug molecular graphs (Guo et al. 2022; Nyamabo\net al. 2021). These methods effectively capture local and\nglobal interactions between atoms within a molecule.\nFurthermore, recent advancements include the application\nof Graphormer (Zhang et al. 2022), a Transformer-based\nmethod for graph-level representation learning, in DDI\npredictions. Notably, this approach differentiates itself from\nGNNs by directly modeling distant nodes within the graph.\nBiomedical Knowledge Graph-based Method. These\nmethods (Celebi et al. 2019; Karim et al. 2019) leverage the\nrich information available in biomedical knowledge graphs\nto enhance the prediction score. KGNN (Lin et al. 2020) pio-\nneers to incorporate GCN to encode the structured relation-\nships in knowledge graphs. It learns comprehensive repre-\nsentations for drugs by propagating information across the\ngraph, taking into account the relationships between enti-\nties. Considering multi-relational DDIs, another GNN-based\nmethod, KG2ECapsule (Su et al. 2023), is proposed. It uti-\nlizes capsule networks to conduct non-linear transforma-\ntions, enriching the representations of entities within a spe-\ncific relational space. In addition, DDKG (Su et al. 2022) uti-\nlizes GAT in knowledge graph-based DDI predictions. The\nattention mechanism in it is employed to uncover the im-\nportance of various triplets, enabling the model to focus on\ncritical drug-relation-drug relationships within the graph.\nMulti-level-based Method. These methods acquire drug\nrepresentations at both graph and node levels. The graph-\nlevel representation is obtained from drug molecular graphs,\nwhile the node-level representation is learned from drug-\nrelated interaction graphs. Most of these methods are con-\nducted on homogeneous networks, such as DDI networks.\nThe Bi-GNN (Bai et al. 2020) employs a bi-level graph\nstructure to predict DDIs using GAT. At the highest level,\nthere is a drug interaction graph. Each biological entity\nwithin this graph is then expanded to its molecular graph.\nAnother multi-view-based method, MIRACLE (Wang et al.\n2021), employs GCN to encode DDI relationships and a\nbond-aware attentive message propagating to predict DDIs.\nWhile MDNN (Lyu et al. 2021) is designed to detect DDIs\nwith heterogeneous networks, it introduces two pathways\nto effectively model both the topological information of the\nknowledge graph and the features of the drugs.\nMethod\nThe architecture of TIGER is depicted in Fig. 1. TIGER\nlearns the drug representations by taking into account both\nbiomedical knowledge graph and drug molecular graphs,\nand then identifies potential DDIs in an end-to-end way.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n250\nFigure 1: The framework of proposed TIGER.\nRelation-aware Self-Attention\nThe self-attention mechanism is a foundational component\nof the Transformer architecture. However, it is inherently\nposition-aware and overlooks the nature of the distinct types\nof relationship between node pairs. Besides, we have noted:\n(i) distant nodes often lack explicit relations, and (ii) a node\npair can have multiple relationships. To address these, we\nintroduce the notion of relation-aware self-attention by em-\nploying following strategies:\n• To incorporate distant nodes, we establish relationships\nbetween them based on the distance of their shortest path.\n• To address the presence of multiple relationships, we\nconsider each relation as a distinct edge.\nGiven an arbitrary graph in G = (V, E, R), where the\nnode attribute for node v ∈ V is denoted by xv ∈ Rd and\nthe node attributes for all nodes are stored in X ∈ R|V |×d.\nFor any node pair (v , u) with C ≥ 1 types of relations, we\ndefine their attention logits based on both node features and\ntheir relationships as shown below:\nψ(xv, xu, rc\nv⇔u) = (xv + rv→u)WT\nq Wk(xu + ru→v)\n= xvWT\nq Wkxu(I) +xvWT\nq Wkru→v(II)\n+ rv→uWT\nq Wkxu(III)\n+ rv→uWT\nq Wkru→v(IV)\n(1)\nwhere rc\nv⇔u denotes the c-th relation between (v , u) and\nrv→u = ru→v when the graph G is an indirected graph.\nWq, Wk ∈ Rdk×d are trainable matrices to generate query\nQ and key K representations.\nThe new defined ψ(·, ·, ·) allows the model to consider\nboth source-specific (II) and target-specific (III) relation-\nships when attending to different positions (I). It also helps\nthe model capture common patterns or dependencies that are\npresent across the entire input by the universal relation bias\n(IV). It should be noted, when dealing with pairs of distant\nnodes, the functionψ(·, ·, ·) is also capable of capturing their\nstructural information as the relationship between them is\ninitialized based on the distance of their shortest path. By\nintroducing relationships, we define our relation-aware self-\nattention as:\nRAttn(xv) =\nX\nu∈V\nP\nc∈C f(xv, xu, rc\nv⇔u)\nP\nw∈V\nP\nc∈C f(xv, xw, rcv⇔w)ϕ(xu)\n(2)\nwhere f(·, ·, ·) = exp(ψ(·, ·, ·)/√dk) and ϕ(xu) =Wvxu.\nWv ∈ Rdv×d is trainable to generate value V .\nRelation-aware Heterogeneous Graph Transformer\nAfter defining our relation-aware self-attention function, the\nremaining components of the relation-aware heterogeneous\ngraph transformer follow the same architecture as the Trans-\nformer, depicted in Fig. 1. To enrich the graph structural in-\nformation, we also incorporate the position embedding spec-\nified by node degree to each node feature as following:\nx(0)\nv = xv + zdeg(v), ∀v ∈ V (3)\nwhere x(0)\nv is the input to the relation-aware self-attention\nblock, deg(v) denotes the degree of node v, and z(·) is an\nembedding layer specified by node degree.\nAfter a relation-aware self-attention block, its output\nRAttn(x(0)\nv ) is passed through a skip connection and a two\nlayer feed-forward neural network (FFN) to generate up-\ndated representations x(1)\nv . This process is defined by:\nˆX(l) = X(l−1) + RAttn(X(l−1)) (4)\nX(l) = FFN( ˆX(l)) := ReLU( ˆX(l)W(l)\n1 )W(l)\n2 (5)\nwhere X(l) is the updated representation and W(l)\n1 and\nW(l)\n2 are trainable parameters in the FNN of l-th (1 ≤ l ≤\nL) layer of TIGER.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n251\nDual-channel Representation Learning\nAfter the introduction of the main block of TIGER,\nwe present the proposed novel dual-channel representa-\ntion combining drug graph-level representation learning in\nmolecular graph (MG) and drug node-level representation\nlearning in biomedical knowledge graph (BKG).\nMolecular Graph Channel Given the molecular graph\nMG(i) = (A, B, T) of drug di, where A is the set of atoms,\nB is the set of bonds, and T is the set of relations. The drug\ngraph-level representation gi ∈ Rd is the summarization\nover the X(L) via a READOUT(·) function:\ngi = READOUT({x(L)\na }|A|\na=1) (6)\nThe READOUT function can be any permutation-invariant\nfunction. Specifically, we apply a simple averaging strategy\nas the READOUT function throughout the paper.\nBiomedical Knowledge Graph Channel As previously\nmentioned in the introduction, biomedical knowledge\ngraphs are typically much larger than molecular graphs and\nthe Transformer architecture is also constrained by long in-\nputs. To apply the TIGER for learning node-level representa-\ntions in BKG, we first extract the subgraph BKG(i) for each\ndrug di contained in BKG. Three alternative subgraph ex-\ntraction methods are provided and explored in this work:\n• k-subtree-based Extractor: The subgraph for each node\nis constructed by including the node itself and its con-\nnected nodes within k levels. Moreover, within the sub-\ntree, each node, excluding the leaf nodes, possesses a\nconstant number of child nodes.\n• DeepWalk-based Extractor: The subgraph for each\nnode is formed by utilizing fixed-length random walk se-\nquences initiated from that specific node.\n• Probability-based Extractor: The subgraph is gener-\nated by selecting a fixed-size set of nodes with the prob-\nabilities defined in pagerank matrix (Page et al. 1998).\nThe sampled subgraph BKG(i) is then fed into the\nrelation-aware graph transformer module. The drug node-\nlevel representation si is defined as si = X(L)[idx], where\nidx denotes the index of drug di in BKG(i).\nDrug Dual-channel Representation Once the drug\ngraph-level representation and node-level representation\nhave been obtained afterL upper layers of the relation-aware\ngraph transformer, they are concatenated and passed through\nthe multi-layer perceptron (MLP) to obtain the drug dual-\nchannel representation hi ∈ Rd as follows:\nhi = MLP(g(L)\ni ||s(L)\ni ) (7)\nwhere g(L)\ni and s(L)\ni denote the output ofL-th relation-aware\ngraph transformer block.\nDrug-Drug Interaction Prediction\nThe DDI prediction task in our study can be defined as a link\nprediction problem based on BKG and MG:= {MG(i)}|D|\ni=1,\nwhere D is the drug set of BKG. Considering a drug pair\n(di, dj), where di, dj ∈ BKG, and their molecular graphs\nMG(i) and MG(j), we can derive their dual-channel repre-\nsentations hi and hj. Then they are concatenated and fed\ninto MLP to predict a link prediction score:\npij = MLP(hi||hj) (8)\nFormally, we formulate the cross entropy loss Llabel for\nall DDI pairs:\nLlabel = −\nX\n(i,j)∈Y\nyij log(pij) + (1−yij) log(1−pij) (9)\nwhere Y denotes the drug pair set andyij is the ground-truth\nvalue.\nTo ensure the acquisition of a discriminative drug dual-\nchannel representation, we employ the Jensen-Shannon (JS)\nmutual information (MI) estimator (Nowozin, Cseke, and\nTomioka 2016) on drug dual-channel representationhi. This\nis done to maximize the estimated MI over the given drug\nmolecular graph gi and extracted drug subgraph sBKG(i) =\nREADOUT({x(L)\nv }|BKG(i)|\nv=1 ), respectively. By introducing\nthe discriminator D : Rd × Rd → R, the MI enhancement\nloss function Lhg\nMI between h: and g: can be formulated as a\nbinary cross-entropy loss:\nLhg\nMI = 1\nn + nneg\n(\nnX\ni∈D\nlog(D(hi, gi)+\nnnegX\nk∈D\nlog(D(hi, ˜gk))))\n(10)\nwhere nneg denotes the number of negative samples and the\nnegative samples are generated in a batch-wise fashion. The\nLhs\nMI is formulated in the same way as shown in Lhg\nMI .\nWith the supervised classification loss Llabel and self-\nsupervised MI loss LMI , we train TIGER with following\nobjective function:\nL = Llabel + β1Lhg\nMI + β2Lhs\nMI (11)\nwhere β1 and β2 are hyper-parameters for the trade-off for\ndifferent loss components.\nExperiment\nIn this section, we first describe the datasets, comparison\nmethods, and evaluation metrics used in the experiment.\nThen, we compare the performances of TIGER with other\ncomparative methods. Finally, we make detailed analysis of\nTIGER under different experimental settings.\nDatasets and Settings\nDatasets. We evaluate the TIGER on three benchmark\ndrug-related heterogeneous graph datasets, i.e., DrugBank\n(Wishart et al. 2018), KEGG (Kanehisa et al. 2017), and\nOGB-biokg (Hu et al. 2020), with different scales for ver-\nifying the scalability and robustness of TIGER.\n• DrugBank: We parse the verified DDIs of provided pro-\nfile from DrugBank, compile an edge list of drug iden-\ntifier combinations, and finally obtain 10,404 approved\nDDIs span 1,052 drugs;\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n252\nDrugBank KEGG OGB-biokg\n#Drugs 1,052 786 808\n#DDIs 10,404 13,787 111,520\n#Nodes 391,116 129,910 93,773\n#Relations 71 167 13\n#Links 1,587,305 362,870 3,892,462\nTable 1: The statistic of three heterogeneous networks.\n• KEGG: We parse the sources from KEGG and map them\nto DrugBank identifiers, which results in 786 approved\ndrugs and 13,787 approved DDIs;\n• OGB-biokg: We download it from OGB website, and fi-\nnally obtain 111,520 DDIs span 808 approved drugs.\nMoreover, for the drug nodes in above datasets, we also\ncollect their drug SMILES strings, respectively. After that,\nwe convert drug SMILES strings into molecular 2D graphs\nby rdkit (Landrum et al. 2013). It should be noted that we\nremove the data items that cannot be converted into graphs\nfrom SMILES strings, and their related interactions in het-\nerogeneous graphs in our preprocessing. Meanwhile, the\nheterogeneous network should not contain any explicit infor-\nmation about DDIs. Hence, we also remove all DDIs from\nthe original datasets, respectively. The statistics of three\ndatasets are shown in Table 1.\nBaselines. TIGER is against a variety of baselines which\ncan be categorized as follows:\n• Molecular Graph-based:We select two representative\nmethods, SSI-DDI (Nyamabo et al. 2021) and Molormer\n(Zhang et al. 2022), as baselines. They aim to predict\nDDIs by utilizing graph-level representations learned\nfrom drug molecular graphs.\n• Biomedical Knowledge Graph-based:Three represen-\ntative methods are listed as baselines, including KGNN\n(Lin et al. 2020), KG2ECapsule (Su et al. 2023), and\nDDKG (Su et al. 2022). They employ drug node-level\nrepresentations learned from the biomedical knowledge\ngraph to model DDI predictions.\n• Multi-level-based: We select three related work as base-\nlines, involving Bi-GNN (Bai et al. 2020), MIRACLE\n(Wang et al. 2021), and MDNN (Lyu et al. 2021). Specif-\nically, Bi-GNN and MIRACLE are constructed based on\nhomogeneous networks, while MDNN is designed for\nheterogeneous networks.\nEvaluation Metrics To evaluate the effectiveness of\nTIGER and all baselines, we employ four metrics for evalu-\nation, including ACC, F1 score, AUC, and AUPR.\nExperimental Settings. We perform five-fold cross-\nvalidation to train TIGER and the aforementioned baselines\non three datasets. Negative samples are randomly selected\nfrom the complement set of positive samples, ensuring an\nequal number of positive samples in all datasets.\nTIGER is implemented using Pytorch v1.10.2 and trained\non NVIDIA A100 GPU. We use the Adam optimizer for\nmodel training. The training process is conducted for 50\nepochs, and all trainable parameters are optimized by Adam\nalgorithm with a learning rate of 0.001. We set the d = 64,\nβ1 = β2 = 0.1, L = 2 for all extractors. For k-subtree-\nbased extractors, k is set to 2 and the constant number of\nchild nodes is set to 4. The size of subgraph for probability\nand DeepWalk-based extractors is set to 32. For all base-\nlines, they are retrained on the same machine with the same\nhyper-parameter settings reported in their original work.\nMain Results\nTable 2 reports all performances on three datasets. The num-\nber in bold denotes the best results of all methods and that\nin bold denotes the best result of baselines. Based on the re-\nsults presented in Table 2, it can be observed that TIGER\noutperforms the other eight baseline methods in DDI pre-\ndictions across all three datasets. When compared with the\nbest baseline method on each dataset, there is an average\nimprovement of 3.66% in ACC, 4.17% in F1 score, 3.78%\nin AUC, and 4.83% in AUPR. This demonstrates its effec-\ntiveness in predicting DDIs. Among the three datasets, we\nfind that TIGER is particular effective in handling sparse\nnetworks, as it has the most significant performance im-\nprovement on the DrugBank dataset. The observed improve-\nment suggests that TIGER has the capability to capture more\nvaluable structural information, such as long dependencies,\nwhich can be credited to its utilization of the Transformer\narchitecture. It is worth highlighting that TIGER demon-\nstrates remarkable performances on the other two datasets as\nwell. Despite the OGB-biokg dataset being extremely dense,\nTIGER achieves an impressive improvement by surpassing\nan ACC of 0.90 on it. This observation suggests that TIGER\ncan effectively distinguish and leverage useful information\neven in challenging scenarios characterized by dense data.\nTherefore, the consistent superiority of TIGER across mul-\ntiple datasets further reinforces its potential as a robust and\nreliable approach for DDI predictions.\nAblation Study\nEffects of Subgraph Extractors.The results displayed in\nTable 2 shows: (i) the probability-based extractor tends to\nbe compatible with low-density datasets; (ii) the DeepWalk-\nbased extractor shows better suitability for dense datasets;\n(iii) the k-subtree-based extractor demonstrates robustness\nand versatility in extracting subgraphs from various types of\nnetworks, but it may experience a loss in accuracy compared\nto other extractors.\nEffects of Dual-Channels. We further investigate whether\nthe MG(·) and BKG(·) bring the complementary information\nto TIGER. The presented Fig. 2 shows that BKG(·) has a\nmore pronounced positive impact on DDI prediction tasks\ncompared to MG(·). It also indicates that incorporating both\nMG(·) and BKG(·) together contributes the most to accurate\nDDI predictions.\nHyper-Parameter Studies\nEffect of k/L. We first explore the effect of k in the k-\nsubtree extractor by changing it from 1 to 8. In the initial\nsetting, we set k equal to the number of model layers L. As\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n253\nMethod DrugBank KEGG OGB-biokg\nACC F1 AUC AUPR ACC F1 AUC AUPR ACC F1 AUC AUPR\nSSI-DDI 0.6847 0.6830 0.7510 0.7316 0.7722 0.7795 0.8506 0.8293 0.7849 0.7878 0.8695 0.8837\n(±0.71) (±1.77) (±1.12) (±1.32) (±1.42) (±1.18) (±1.47) (±1.71) (±0.87) (±0.57) (±0.57) ( ±0.53)\nMolormer 0.5763 0.5941 0.6086 0.5990 0.6371 0.6608 0.6916 0.6760 0.7357 0.7251 0.8091 0.8169\n(±1.12) (±1.13) (±1.05) (±0.87) (±2.72) (±1.54) (±3.69) (±3.88) (±1.59) (±2.02) (±1.83) ( ±2.01)\nKGNN 0.6681 0.6784 0.7219 0.6614 0.7802 0.7815 0.8536 0.8185 0.8253 0.8189 0.9059 0.9184\n(±0.56) (±1.96) (±0.56) (±0.71) (±0.95) (±1.14) (±0.54) (±0.79) (±0.19) (±2.67) (±0.09) ( ±0.06)\nKG2ECapsule 0.6628 0.6694 0.7131 0.6557 0.8227 0.8278 0.8960 0.8699 0.8282 0.8230 0.9087 0.9203\n(±0.47) (±0.42) (±0.63) (±0.92) (±0.55) (±0.49) (±0.44) (±0.69) (±0.16) (±0.20) (±0.10) ( ±0.07)\nDDKG 0.7035 0.7089 0.7638 0.7292 0.8305 0.8355 0.9000 0.8690 0.8048 0.7957 0.8821 0.8961\n(±1.04) (±2.53) (±0.32) (±0.50) (±0.83) (±0.85) (±0.75) (±1.18) (±0.97) (±0.98) (±0.58) ( ±0.48)\nBi-GNN 0.7165 0.7419 0.7672 0.7054 0.8503 0.8580 0.9147 0.8840 0.8510 0.8472 0.9298 0.9361\n(±2.18) (±0.25) (±4.11) (±4.41) (±0.60) (±0.58) (±0.63) (±0.93) (±0.22) (±0.27) (±0.18) ( ±0.16)\nMIRACLE 0.6636 0.6630 0.7102 0.6698 0.8379 0.8421 0.9090 0.8801 0.8923 0.8930 0.9495 0.9526\n(±0.58) (±0.41) (±0.74) (±0.86) (±0.42) (±0.81) (±0.25) (±0.53) (±0.20) (±0.14 ) (±0.07 ) (±0.11)\nMDNN 0.7394 0.7313 0.8052 0.7653 0.8410 0.8459 0.9099 0.8821 0.8578 0.8547 0.9351 0.9423\n(±0.21) (±0.43) (±0.36 ) (±0.98) (±0.42) (±0.28) (±0.41) (±0.56) (±0.16) (±0.29) (±0.17) ( ±0.17)\nTIGER-KS 0.7903 0.8027 0.8642 0.8342 0.8752 0.8815 0.9407 0.9244 0.8548 0.8517 0.9336 0.9414\n(±0.36) (±0.44) (±0.65) (±1.15) (±0.32) (±0.24) (±0.30) (±0.40) (±0.09) (±0.25) (±0.15) ( ±0.18)\nTIGER-DW 0.7849 0.7977 0.8572 0.8261 0.8781 0.8848 0.9414 0.9238 0.9162 0.9143 0.9693 0.9748\n(±0.38) (±0.41) (±0.31) (±0.33) (±0.45) (±0.44) (±0.26) (±0.34) (±0.09) (±0.12) (±0.07) ( ±0.10)\nTIGER-P 0.7905 0.8033 0.8662 0.8370 0.8850 0.8899 0.9473 0.9350 0.8791 0.8754 0.9477 0.9571\n(±0.87) (±0.94) (±0.57) (±0.68) (±0.19) (±0.24) (±0.21) (±0.35) (±0.16) (±0.17) (±0.14) ( ±0.12)\nImprov.(%) 5.11 7.20 6.10 7.17 3.47 3.19 3.26 5.10 2.39 2.13 1.98 2.22\nTable 2: The performances with TIGER and baseline methods on three datasets, reported as the average value and standard\ndeviation (%) across five folds.\nTIGERMG TIGERBKG TIGER\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.6347\n0.7126\n0.7903\n0.6515\n0.7653\n0.8027\n0.6946\n0.7409\n0.8642\n0.6887\n0.6773\n0.8342\nDrugBank\nACC F1\nAUC AUPR\nTIGERMG TIGERBKG TIGER\n0.7\n0.8\n0.9\n1.0\n0.7716\n0.8593\n0.8752\n0.7811\n0.8620\n0.8815\n0.8516\n0.9258\n0.9407\n0.8311\n0.9003\n0.9244\nKEGG\nTIGERMG TIGERBKG TIGER\n0.7\n0.8\n0.9\n1.0\n0.7987\n0.8476\n0.8548\n0.7904\n0.8436\n0.8517\n0.8804\n0.9270\n0.9336\n0.8952\n0.9361\n0.9414\nOGB-biokg\nscores\nFigure 2: The performances with TIGER and other two\nvariants on three datasets, where TIGER MG and TIGERBKG\nsolely considers MG and BKG, respectively.\nshown in Fig. 3a, when k is set to 2, TIGER shows the best\nperformance on all indicators. As k increases beyond 4, the\nperformance of TIGER starts to degrade and becomes unsta-\nble. This may be due to the significant increase in the noise\ncontained in subgraph, since the size of the subgraph grows\nexponentially as k increases. However, it is noteworthy that\nTIGER does not experience over-fitting as L increases to 7\nwhen k is fixed, as shown in Fig. 3b. This observation high-\nlights the robustness of TIGER as its performance is not ad-\nversely affected by the increase of L.\nEffect of Subgraph Size. Fig. 3c and Fig. 3d show that\nTIGER achieves the optimal and stable performance when\nthe subgraph size is set to 32. It is worth highlighting that\neven though the performance of TIGER starts to decline be-\nyond the optimal size, it still manages to achieve AUC and\n1 2 3 4 5 6 7 8\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.85\n0.95\nk-hop or L (k = L) \nscores\nACC F1 AUC AUPRa\n1 2 3 4 5 6 7 8\n0\n5\n10\n15\n20std (%)\n1 2 3 4 5 6 7 8\n0.6\n0.7\n0.8\n0.9\n1.0\n0.82\n0.95\nL (k =2) \nscores\nACC F1 AUC AUPRb\n1 2 3 4 5 6 7 8\n0\n5\n10\n15\n20\n25std (%)\n8 16 32 64 128\n0.75\n0.80\n0.85\n0.89\n0.95\n1.00\n0.91\nThe size of subgraph (Probabillilty-based extractor)scores\nACC F1 AUC AUPRc\n8 16 32 64 128\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2std (%)\n8 16 32 64 128\n0.75\n0.80\n0.85\n0.89\n0.95\n1.00\n0.91\nThe size of subgraph (DW-based extractor)\nscores\nACC F1 AUC AUPRd\n8 16 32 64 128\n0.4\n0.8\n1.2\n1.6std (%)\nFigure 3: The results of TIGER with varying values of k, L,\nand |BKG(i)|.\nAUPR values above 0.90. These two observations suggest:\n(i) selecting a subgraph with the size of 32 provides the suf-\nficient contextual and topological information; (ii) TIGER\npossesses the capability to capture and leverage key struc-\ntural patterns and dependencies, regardless of the specific\nsize of the subgraph being considered.\nCase Study\nTo gain insights into the reasons behind the strong perfor-\nmance of TIGER in DDI prediction tasks, we first aims to\nuncover how connections within the network structure con-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n254\n1 2 3 4 5 6\n1\n2\n3\n4\n5\nLayers \nHead1\nHead2\nHead3\nHead4\nMean attention distance \n(hop)\nFigure 4: Attention distance by head and network depth on\nthe KEGG dataset. Each dot show mean attention distance\nin hops across graphs of a head at a layer.\nFlutamide\nRofecoxib\nGO:0034641\nCPT1A\nAPOA2\nEuphoric mood\nNormocytic \nnormochromic anemia\nMiddle \nCerebral \nArtery \nOcclusion\n0.85\nDonepezil\nRopinirole\nGO:0045937\nGO:0051962\nOPRM1\nP2RY12Oral hypoesthesia \nHypervolemia\nBlood \npotassium \ndecreased\nOndansetron\n0.64\nDonepezil(DB00843) Ondansetron(DB00904)\ndrug-side effect drug-protein\nprotein-function protein-protein-reaction protein-protein-catalysis\ndrug side effect protein function\nFigure 5: The subgraphs centered on Donepezil and On-\ndansetron, which are extracted by probability-based extrac-\ntors.\ntribute to the performance of TIGER. We compute the at-\ntention distance (Dosovitskiy et al. 2021) across heads and\nnetwork depth by averaging pairwise distances on subgraphs\nweighted by attention scores. Fig. 4 shows that heads attend\nglobally over the subgraphs in the lowest layers and they\ntend to be local in deeper layers. It also highlights that high-\norder structures play a significant role in DDI predictions,\nas node with 3-hop receive higher attention weights. These\nobservations suggest that TIGER is able to leverage global\ndependencies and uncover valuable patterns that contribute\nto its superior performance.\nWe next demonstrate its explanatory effectiveness using\na specific example (Donepezil, Ondansetron). Fig. 5 effec-\ntively highlights connections and nodes influencing the rep-\nresentations of target drugs and their interaction patterns\nin the network. In Fig. 5, it is evident that the use of On-\ndansetron has been linked to decreased blood potassium\nlevels. This reduction can subsequently elevate blood pres-\nsure levels, increasing the risk of blood clot formation and\nmiddle cerebral artery occlusion in certain circumstances.\nConsequently, combining Donepezil and Ondansetron may\nhave potential adverse effects on the central nervous sys-\ntem. This observation based on TIGER align with the exist-\ning knowledge surrounding the medications involved (Wilde\nand Markham 1996; Shigeta and Homma 2001). TIGER also\nexhibits its ability to discern between multiple relations, as it\nidentifies the catalytic role of OPRM1 and P2RY12 as being\nFigure 6: The molecular graphs of Donepezil and On-\ndansetron.\nmore significant in determining the importance of P2RY12.\nAll of these observations suggest that TIGER is capable of\nproviding valuable insights into drug interactions.\nWe also visualize the molecular graphs of Donepezil\nand Ondansetron in Fig. 6 and label the important compo-\nnents based on the attention weights obtained from TIGER.\nTIGER accurately recognizes the critical constituents within\nboth molecules. The benzyl group (-CH2C6H5) attached to\nthe piperidine ring and the indanone group enable Donepezil\nto function as an acetylcholinesterase inhibitor, preventing\nthe breakdown of acetylcholine. TIGER also pinpoints the\nsignificance of the imidazole ring and the indole ring linked\nwith a carbonyl group, which allow it to act as a selective\nserotonin 5-HT3 receptor antagonist, blocking serotonin sig-\nnaling in specific areas of the central nervous system (Brit-\ntain 2002). As both medications affect the central nervous\nsystem (CNS), combining them may enhance their CNS-\nrelated side effects. This observation highlights that TIGER\ncan provide deeper insights into molecular structures.\nConclusion\nThis paper presents TIGER, a novel dual-channel relation-\naware graph transformer model designed for predicting\nDDIs. TIGER utilizes the combined representation learn-\ning from drug molecular graphs and biomedical knowledge\ngraphs to predict DDIs. It effectively captures long depen-\ndencies and high-order structures, which are vital for accu-\nrate DDI predictions. Moreover, TIGER demonstrates ex-\nceptional proficiency in handling multiple relations within\nthe graph. By providing valuable insights into DDI predic-\ntions, TIGER enhances our understanding of the complex\nbiological system underlying drug interactions.\nAcknowledgments\nThis work was supported in part by the Natural Science\nFoundation of Xinjiang Uygur Autonomous Region under\ngrant 2021D01D05, in part by the National Natural Science\nFoundation of China under grant 62373348, in part by the\nXinjiang Tianchi Talents Program under grant E33B9401,\nin part by CAS Light of the West Multidisciplinary Team\nproject under grant xbzg-zdsys-202114, in part by Pioneer\nHundred Talents Program of Chinese Academy of Sciences,\nin part by the National Science Fund for Distinguished\nYoung Scholars of China under grant 62325308, and NSF\nunder grant III-2106758.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n255\nReferences\nBai, Y .; Gu, K.; Sun, Y .; and Wang, W. 2020. Bi-level graph\nneural networks for drug-drug interaction prediction. arXiv\npreprint arXiv:2006.14002.\nBrittain, H. G. 2002. Profiles of drug substances, excipients,\nand related methodology. Analy Profiles Drug Subst Excip-\nients, 29: 1–5.\nCelebi, R.; Uyar, H.; Yasar, E.; Gumus, O.; Dikenelli, O.;\nand Dumontier, M. 2019. Evaluation of knowledge graph\nembedding approaches for drug-drug interaction prediction\nin realistic settings. BMC bioinformatics, 20(1): 1–14.\nChen, D.; O’Bray, L.; and Borgwardt, K. 2022. Structure-\naware transformer for graph representation learning. In In-\nternational Conference on Machine Learning, 3469–3489.\nPMLR.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations.\nFinkel, R.; Clark, M. A.; and Cubeddu, L. X. 2009. Phar-\nmacology. Lippincott Williams & Wilkins.\nGuo, Z.; Nan, B.; Tian, Y .; Wiest, O.; Zhang, C.; and\nChawla, N. V . 2022. Graph-based molecular representation\nlearning. arXiv preprint arXiv:2207.04869.\nHu, W.; Fey, M.; Zitnik, M.; Dong, Y .; Ren, H.; Liu, B.;\nCatasta, M.; and Leskovec, J. 2020. Open graph benchmark:\nDatasets for machine learning on graphs.Advances in neural\ninformation processing systems, 33: 22118–22133.\nKanehisa, M.; Furumichi, M.; Tanabe, M.; Sato, Y .; and\nMorishima, K. 2017. KEGG: new perspectives on genomes,\npathways, diseases and drugs. Nucleic acids research,\n45(D1): D353–D361.\nKarim, M. R.; Cochez, M.; Jares, J. B.; Uddin, M.; Beyan,\nO.; and Decker, S. 2019. Drug-drug interaction prediction\nbased on knowledge graph embeddings and convolutional-\nLSTM network. In Proceedings of the 10th ACM interna-\ntional conference on bioinformatics, computational biology\nand health informatics, 113–123.\nKipf, T. N.; and Welling, M. 2017. Semi-Supervised Clas-\nsification with Graph Convolutional Networks. In Interna-\ntional Conference on Learning Representations.\nLandrum, G.; et al. 2013. RDKit: A software suite for chem-\ninformatics, computational chemistry, and predictive model-\ning. Greg Landrum, 8: 31.\nLin, X.; Quan, Z.; Wang, Z.-J.; Ma, T.; and Zeng, X. 2020.\nKGNN: Knowledge Graph Neural Network for Drug-Drug\nInteraction Prediction. In IJCAI, volume 380, 2739–2745.\nLyu, T.; Gao, J.; Tian, L.; Li, Z.; Zhang, P.; and Zhang, J.\n2021. MDNN: A Multimodal Deep Neural Network for Pre-\ndicting Drug-Drug Interaction Events. InIJCAI, 3536–3542.\nNowozin, S.; Cseke, B.; and Tomioka, R. 2016. f-gan: Train-\ning generative neural samplers using variational divergence\nminimization. Advances in neural information processing\nsystems, 29.\nNyamabo; K, A.; Yu, H.; and Shi, J.-Y . 2021. SSI–\nDDI: substructure–substructure interactions for drug–drug\ninteraction prediction. Briefings in Bioinformatics, 22(6):\nbbab133.\nPage, L.; Brin, S.; Motwani, R.; and Winograd, T. 1998. The\npagerank citation ranking: Bring order to the web. Technical\nreport, Technical report, stanford University.\nShigeta, M.; and Homma, A. 2001. Donepezil for\nAlzheimer’s disease: pharmacodynamic, pharmacokinetic,\nand clinical profiles. CNS Drug Reviews, 7(4): 353–368.\nSu, X.; Hu, L.; You, Z.; Hu, P.; and Zhao, B. 2022.\nAttention-based knowledge graph representation learning\nfor predicting drug-drug interactions. Briefings in bioinfor-\nmatics, 23(3): bbac140.\nSu, X.; You, Z.; Huang, D.; Wang, L.; Wong, L.; Ji, B.; and\nZhao, B. 2023. Biomedical Knowledge Graph Embedding\nWith Capsule Network for Multi-Label Drug-Drug Interac-\ntion Prediction. IEEE Transactions on Knowledge and Data\nEngineering, 35(6): 5640–5651.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nVilar, S.; Uriarte, E.; Santana, L.; Lorberbaum, T.; Hripcsak,\nG.; Friedman, C.; and Tatonetti, N. P. 2014. Similarity-based\nmodeling in large-scale prediction of drug-drug interactions.\nNature protocols, 9(9): 2147–2163.\nWang, Y .; Min, Y .; Chen, X.; and Wu, J. 2021. Multi-\nView Graph Contrastive Representation Learning for Drug-\nDrug Interaction Prediction. In Proceedings of the Web\nConference 2021, WWW ’21, 2921–2933. New York,\nNY , USA: Association for Computing Machinery. ISBN\n9781450383127.\nWilde, M. I.; and Markham, A. 1996. Ondansetron: a re-\nview of its pharmacology and preliminary clinical findings\nin novel applications. Drugs, 52: 773–794.\nWishart, D. S.; Feunang, Y . D.; Guo, A. C.; Lo, E. J.; Marcu,\nA.; Grant, J. R.; Sajed, T.; Johnson, D.; Li, C.; Sayeeda, Z.;\net al. 2018. DrugBank 5.0: a major update to the DrugBank\ndatabase for 2018. Nucleic acids research, 46(D1): D1074–\nD1082.\nYing, C.; Cai, T.; Luo, S.; Zheng, S.; Ke, G.; He, D.; Shen,\nY .; and Liu, T.-Y . 2021. Do transformers really perform\nbadly for graph representation? Advances in Neural Infor-\nmation Processing Systems, 34: 28877–28888.\nZhang, X.; Wang, G.; Meng, X.; Wang, S.; Zhang, Y .;\nRodriguez-Paton, A.; Wang, J.; and Wang, X. 2022.\nMolormer: a lightweight self-attention-based method fo-\ncused on spatial structure of molecular graph for drug–drug\ninteractions prediction. Briefings in Bioinformatics, 23(5).\nZhao, B.-W.; Su, X.-R.; Hu, P.-W.; Huang, Y .-A.; You, Z.-\nH.; and Hu, L. 2023. iGRLDTI: an improved graph repre-\nsentation learning method for predicting drug–target inter-\nactions over heterogeneous biological information network.\nBioinformatics, 39(8): btad451.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n256",
  "topic": "Drug",
  "concepts": [
    {
      "name": "Drug",
      "score": 0.6100642085075378
    },
    {
      "name": "Computer science",
      "score": 0.5820162296295166
    },
    {
      "name": "Transformer",
      "score": 0.4919743239879608
    },
    {
      "name": "Graph",
      "score": 0.46409979462623596
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3927932679653168
    },
    {
      "name": "Machine learning",
      "score": 0.3773903250694275
    },
    {
      "name": "Theoretical computer science",
      "score": 0.32724320888519287
    },
    {
      "name": "Pharmacology",
      "score": 0.21295136213302612
    },
    {
      "name": "Medicine",
      "score": 0.1852872371673584
    },
    {
      "name": "Engineering",
      "score": 0.12535050511360168
    },
    {
      "name": "Voltage",
      "score": 0.09032028913497925
    },
    {
      "name": "Electrical engineering",
      "score": 0.07908773422241211
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210106108",
      "name": "Xinjiang Technical Institute of Physics & Chemistry",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    }
  ],
  "cited_by": 28
}