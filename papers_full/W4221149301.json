{
    "title": "Forensic Analysis and Localization of Multiply Compressed MP3 Audio Using Transformers",
    "url": "https://openalex.org/W4221149301",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5057999558",
            "name": "Ziyue Xiang",
            "affiliations": [
                "Purdue University West Lafayette"
            ]
        },
        {
            "id": "https://openalex.org/A5051370303",
            "name": "Paolo Bestagini",
            "affiliations": [
                "Politecnico di Milano"
            ]
        },
        {
            "id": "https://openalex.org/A5005378965",
            "name": "Stefano Tubaro",
            "affiliations": [
                "Politecnico di Milano"
            ]
        },
        {
            "id": "https://openalex.org/A5089688702",
            "name": "Edward J. Delp",
            "affiliations": [
                "Purdue University West Lafayette"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6755182157",
        "https://openalex.org/W2133824856",
        "https://openalex.org/W2101807845",
        "https://openalex.org/W6602002561",
        "https://openalex.org/W2086320398",
        "https://openalex.org/W6630236247",
        "https://openalex.org/W6649599172",
        "https://openalex.org/W2004478479",
        "https://openalex.org/W2040759991",
        "https://openalex.org/W1474022922",
        "https://openalex.org/W6749522612",
        "https://openalex.org/W2151167680",
        "https://openalex.org/W3081746081",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2144520790",
        "https://openalex.org/W6755977528",
        "https://openalex.org/W3157576687",
        "https://openalex.org/W6727249380",
        "https://openalex.org/W6776218486",
        "https://openalex.org/W2036326986",
        "https://openalex.org/W2056177844",
        "https://openalex.org/W2117497587",
        "https://openalex.org/W6778823374",
        "https://openalex.org/W2963300588",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W2417429787",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W6627467931",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3033411150",
        "https://openalex.org/W1996355929",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W1151924489",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2950547518",
        "https://openalex.org/W4287802874",
        "https://openalex.org/W2793428453",
        "https://openalex.org/W2523246573",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4287643567"
    ],
    "abstract": "Audio signals are often stored and transmitted in compressed formats. Among\\nthe many available audio compression schemes, MPEG-1 Audio Layer III (MP3) is\\nvery popular and widely used. Since MP3 is lossy it leaves characteristic\\ntraces in the compressed audio which can be used forensically to expose the\\npast history of an audio file. In this paper, we consider the scenario of audio\\nsignal manipulation done by temporal splicing of compressed and uncompressed\\naudio signals. We propose a method to find the temporal location of the splices\\nbased on transformer networks. Our method identifies which temporal portions of\\na audio signal have undergone single or multiple compression at the temporal\\nframe level, which is the smallest temporal unit of MP3 compression. We tested\\nour method on a dataset of 486,743 MP3 audio clips. Our method achieved higher\\nperformance and demonstrated robustness with respect to different MP3 data when\\ncompared with existing methods.\\n",
    "full_text": "Â© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works.\nFORENSIC ANALYSIS AND LOCALIZATION OF\nMULTIPLY COMPRESSED MP3 AUDIO USING TRANSFORMERS\nZiyue Xiang\n, Paolo Bestagini\n, Stefano Tubaro\n, Edward J. Delp\nVideo and Image Processing Lab (VIPER), School of Electrical and Computer Engineering,\nPurdue University, West Lafayette, Indiana, USA\nDipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy\nABSTRACT\nAudio signals are often stored and transmitted in compressed\nformats. Among the many available audio compression\nschemes, MPEG-1 Audio Layer III (MP3) is very popular\nand widely used. Since MP3 is lossy it leaves characteristic\ntraces in the compressed audio which can be used forensically\nto expose the past history of an audio ï¬le. In this paper, we\nconsider the scenario of audio signal manipulation done by\ntemporal splicing of compressed and uncompressed audio sig-\nnals. We propose a method to ï¬nd the temporal location of the\nsplices based on transformer networks. Our method identiï¬es\nwhich temporal portions of a audio signal have undergone\nsingle or multiple compression at the temporal frame level,\nwhich is the smallest temporal unit of MP3 compression. We\ntested our method on a dataset of 486,743 MP3 audio clips.\nOur method achieved higher performance and demonstrated\nrobustness with respect to diï¬€erent MP3 data when compared\nwith existing methods.\nIndex Termsâ€” MP3 compression, audio forensics, con-\nvolutional neural networks, transformer networks\n1. INTRODUCTION\nThe advance in machine learning techniques makes generat-\ning high-quality speech or music possible [1]â€“[3]. Almost\neveryone has the ability to tamper with audio signals due to\nthe availability of audio editing techniques. It is easy to syn-\nthesize/manipulate â€œfakeâ€ speech signals that resembles the\nstyle and voice of a given person, and to concatenate real and\nsynthetic signals to create new forged speech signals. This\nposes signiï¬cant threats to individuals, organizations, society\nand national security.\nThis material is based on research sponsored by the Defense Advanced\nResearch Projects Agency (DARPA) and Air Force Research Laboratory\n(AFRL) under agreement number FA8750-20-2-1004. The U.S. Govern-\nment is authorized to reproduce and distribute reprints for Governmental\npurposes notwithstanding any copyright notation thereon. The views and\nconclusions contained herein are those of the authors and should not be inter-\npreted as necessarily representing the oï¬ƒcial policies or endorsements, either\nexpressed or implied, of DARPA, AFRL or the U.S. Government. Address all\ncorrespondence to Edward J. Delp,ace@ecn.purdue.edu.\nThe detection of synthesize/manipulated speech is usually\nchallenging because of the ï¬‚exibility of human voice, the\npresenceofacousticnoiseorreverberation,andthecomplexity\nof audio synthesis models [4]. However, audio signals are\nmostly saved and shared using lossy compression techniques.\nLossy compression leaves distinct artifacts in the compressed\naudio which can be used for forensic analysis [5], [6]. In this\npaperweexaminetheforensicproblemofdetectingifanaudio\nsignal has been spliced into another audio signal by detecting\nlocations in the signal that have been multiply compressed.\nThe spliced audio signal may be real or synthetic.\nIntroduced in 1993, MPEG-1 Audio Layer III (MP3) [7],\n[8]hasbeenoneofthemostpopulardigitalaudiocompression\nmethods. It changed our ways of listening to music, podcasts,\nandmanyothertypesofaudiocontent. Despitehavinginferior\ncompression eï¬ƒciency compared to Advanced Audio Codec\n(AAC) [9], MP3 is still widely used due to compatibility with\nmanyexistingapplicationsandlowercomputationalcomplexity\n[10].\nMost of the existing MP3 audio forensics methods focus\nondoubleMP3compressiondetection. Thesemethodspredict\nwhether an entire MP3 ï¬le is compressed more than once. In\n[5], [6], [11], [12], the authors used diï¬€erent statistical and\nfeature design techniques on the Modiï¬ed Discrete Cosine\nTransform (MDCT) coeï¬ƒcients for double compression de-\ntection. Ma et al.[13] used the statistics of scalefactors for\ndetecting doubly compressed MP3 with the same data rate. In\n[14], the authors used the Huï¬€man table indices for double\ncompression detection. Yanet al.[15] addressed the problem\nof double and triple compression detection using features ex-\ntracted from scalefactors and Huï¬€man table indices. In [16],\nthe authors addressed the MP3 multiple compression local-\nization problem for the ï¬rst time. Essentially, their proposed\nmethod detects double compression using the histogram of\nMDCT coeï¬ƒcients. Localization was achieved by using small\nslidingdetectionwindows. Thistechniqueallowsonetoextend\nany detection method to a localization method. Finally, more\nmodern methods make use of Deep Neural Networks (DNNs).\nThis is the case of Luoet al.[17] that proposed to use stacked\nautoencoder networks to detect multiply compressed audio\nsignals.\n1\narXiv:2203.16499v2  [cs.SD]  28 Apr 2022\nIn this paper, we present an MP3 multiple compression\nlocalization technique based on deep transformer neural net-\nworks [18]. Given an MP3 signal, our proposed method can\ndistinguish between single compressed temporal segments and\nmultiple compressed temporal segments thereby allowing us\nto temporally localize where the audio signal may have been\nspliced. Ourproposedmethodcanalsobeusedforsynthesized\naudio detection.\n2. BACKGROUND\nWeï¬rstintroduceabasicoverviewofMP3compression. More\ndetails of MP3 compression can be obtained in [7], [8], [10].\nThen we provide an overview of transformer neural networks.\n2.1. MP3 Compression\nThe block diagram of a typical MP3 encoder is shown in\nFigure 1. To compress a digital audio signal using MP3, the\nsignal is ï¬rst split into ï¬xed time length sample windows\nknown as frames, where each frame contains 1152 temporal\nsamples [7], [8]. MP3 ï¬les are made up of a series of such\nframes.\nThe input is ï¬rst processed through a perceptual model\nthat drives the selection of coding parameters (lower branch of\nFigure 1). During this step, the audio samples are transformed\nintofrequencydomainusingFastFourierTransform(FFT).The\nFFT magnitude is passed to the psychoacoustic model, which\nexploits the characteristics of Human Hearing System (HHS)\nto balance between the sound quality and the data rate of the\ncompressed signal [19]. After this perceptual audio analysis,\nthe lossy coding step is next (upper branch of Figure 1). The\ntemporal samples are ï¬ltered into 32 equally spaced frequency\nsub-bands using a polyphase ï¬lterbank. Each sub-band is\nwindowed according to the psychoacoustic model to reduce\nartifacts and then transformed through MDCT, which leads\nto 18 coeï¬ƒcients. In total, there are32 \u000218 = 576 MDCT\ncoeï¬ƒcients.\nFilterbank\n(32 Subband) MDCT\nNonuniform\nQuantization Rate\nControl Loop\nDistortion\nControl Loop\nHuffman\nEncoding\nFFT\n(1024 Points)\nPsychoacoustic\nModel\nExternal\nControl\nCoding of\nSide-information\nBitstream Formatting\nCRC-Check\nWindow\nSwitching\nDigital Audio\nSignal\nCoded\nAudio Signal\n(32-320 kbit/s)\nSubband\n31\n0\nLine\n575\n0\nFig. 1: The block diagram of an MP3 encoder.\nAfter the MDCT, the resulting coeï¬ƒcients are quantized\nin the distortion control loop exploiting the psychoacoustic\nanalysis. The HHS has approximately 24 critical bands based\non a model of the HHS [19]. During quantization, the 32 sub-\nbands are grouped into scalefactor bands, which approximates\nthe critical bands of HHS. The MDCT coeï¬ƒcients in each\nscalefactor band is multiplied by a scalefactor before quantiza-\ntion. The quantization step size increases as the frequencies\nbecome greater, because the HHS is less sensitive to higher\nfrequency. The scalefactors and quantization step sizes work\ntogether to satisfy both audio quality and data rate constraints.\nAfter quantization, the MDCT coeï¬ƒcients are binary encoded\nusing one of the 32 predeï¬ned Huï¬€man tables. The binary\ncoded coeï¬ƒcients, the encoding parameters (side-information)\nsuch as scalefactors, Huï¬€man table indices, quantization step\nsizes are inserted in the data stream to form the compressed\naudio signal.\n2.2. Transformer Neural Networks\nTransformer networks have shown excellent performance in\na variety of tasks such as language modeling [20], image\nclassiï¬cation [21], object detection [22], protein structure\nprediction [23].\nLet the input to the transformer network beZ 2Rğ‘\u0002ğ‘‘model ,\nwhich contains ğ‘ elements, and each element is a vector\nof ğ‘‘model dimensions. Transformer networks use the Self\nAttention(SA)mechanism[18],[21]toexploittherelationship\nbetween diï¬€erent elements in the input. Linear projection\nUğ‘ğ‘˜ğ‘£ 2Rğ‘‘model\u00023ğ‘‘â„ is ï¬rst used to generate three diï¬€erent\nprojected versions of the input, i.e.,q, k, andv:\nÂ»q jk jvÂ¼= ZUğ‘ğ‘˜ğ‘£ Â• (1)\nThen, the vectorsq andk are used to form the attention map\nA 2Rğ‘\u0002ğ‘:\nA = softmax \u0000\nqkğ‘‡\nÂpğ‘‘â„\n\u0001 Â• (2)\nFinally, the SA ofZ is the matrix multiplication ofA andv:\nSAÂ¹ZÂº= AvÂ• (3)\nThe transformer networks we use are based on Multihead Self\nAttention (MSA) [18], which is an extension of SA whereâ„\ndiï¬€erent SA values (called â€œheadsâ€) are computed in parallel.\nThe â„diï¬€erent SA values are combined to the result of MSA\nusing the matrixUğ‘šğ‘ ğ‘ 2Rğ‘‘model\u0002ğ‘‘model :\nMSAÂ¹ZÂº= Â»SA1 Â¹ZÂºj\u0001\u0001\u0001j SAâ„Â¹ZÂºÂ¼Uğ‘šğ‘ ğ‘Â• (4)\nOne must guaranteeâ„divides ğ‘‘model and setğ‘‘â„ = ğ‘‘modelÂâ„\nso thatMSAÂ¹ZÂºand Z have the same dimensionality. More\ndetails about transformers can be found in [18], [21].\n3. MULTIPLE MP3 COMPRESSION\nLOCALIZATION\nInthissectionweï¬rstintroducethetemporalsplicingdetection\nand localization problem. We then present our proposed\nsolution.\n2\n3.1. Problem Formulation\nInMP3compression,eachaudiosignaliscomposedbyaseries\nof nonoverlapping ï¬xed temporal length segments known as\nframes. Letx be the audio signalx = fğ‘¥1Â–ğ‘¥2Â–Â•Â•Â•Â–ğ‘¥ ğ¿g, where\nğ‘¥ğ‘™ is theğ‘™-th frame of the signal, andğ¿ is the total number\nof frames, which depends on the signal length. We can\nassociate to the audio ï¬le a sequence of labelsy deï¬ned as\ny = fğ‘¦1Â–ğ‘¦2Â–Â•Â•Â•Â–ğ‘¦ ğ¿g, whereğ‘¦ğ‘™ is theğ‘™-th binary valued label\nindicating whether theğ‘™-th frame (ğ‘¥ğ‘™) has been compressed\nonce (i.e.,ğ‘¦ğ‘™ = 0) or more than one time (i.e.,ğ‘¦ğ‘™ = 1).\nDuring the creation of a spliced MP3 audio ï¬le, it is likely\nthat frames from diï¬€erent audio signals are concatenated in\ntimeandcompressedusingMP3. Someoftheaudiosignalscan\nbe uncompressed (e.g., pristine or generated from a synthetic\nspeech), while others may have been compressed and then\ndecompressed. Theï¬nalsplicedsignalwilllikelycontainboth\nsingle and multiple compressed frames.\nOur goal is to ï¬ndË†y, which is an estimate of the sequence\nof labelsy associated with the audio ï¬lex by examining the\nMP3compresseddatafor x onaframebyframebasis. Indoing\nso,weareabletodetectifanaudioï¬lehasundergonesplicing,\nand localize which frames have been compressed more than\none time.\n3.2. Proposed Method\nOur proposed method is shown in Figure 2. We examine the\nMP3 data corresponding to the individual frames of the input\nsignal x = fğ‘¥1Â– ğ‘¥2Â– Â•Â•Â•Â–ğ‘¥ğ¿g. We examineğ¿frames at a time to\ndecide if the audio signal has been multiply compressed and\nlocalizethesplicing. AMP3compressedframeconsistsoftwo\ngroups of samples known as granules [8], [9]. Each granule\ncontains 576 temporal samples from the respective two stereo\nchannels. Our proposed method uses the data produced by the\nMP3 codec for a frame shown in Table 1 from the ï¬rst channel\nof the ï¬rst granule. This data consists of MDCT coeï¬ƒcients,\nquatization step sizes, scalefactors, Huï¬€man table indices, and\nsub-band window selection information. The MP3 codec data\nwe choose contain a complete set of parameters required to\ndecode the channel. We will use this information to decide if\nan audio signal has been multiple compressed and localize the\nmuliple compressed frames.\nThemdct_coef andscalefactor ï¬eldsareusedasinputs\nto two separate Convolutional Neural Networks (CNNs) (i.e.,\nCNN-1 and CNN-2) to ï¬nd more features (see Figure 2). We\nuse CNN architectures that are similar to the well known\nVGG CNN [24]. The depth and number of ï¬lters are changed\nbased on the size of themdct_coef andscalefactor ï¬elds.\nBy denoting the convolutional layer as Conv<receptive ï¬eld\nsize>â€“<number of ï¬lters> and the fully connected layers as\nFCâ€“<number of neurons>, the architecture and parameters of\neach CNN are as follows:\nâ€¢ CNN-1: Conv3-32, Conv3-32, Maxpool, Conv3-64, Conv3-\n64, Maxpool, Conv3-128, Conv3-128, Maxpool, FC-233,\nDropout, FC-233, Dropout.\nâ€¢ CNN-2: Conv3-16, Conv3-16, Maxpool, Conv3-32, Conv3-\n32,Maxpool,Conv3-64,Conv3-64,Maxpool,FC-49,Dropout,\nFC-49, Dropout.\nTable 1: MP3 codec information ï¬elds used in our method.\nDetails about each ï¬eld can be obtained from [25], [26].\nFields Description\npart_23_length Size of coded binary data\nscalefactor,scalefac_compress,scalefac_scale,preflag Scalefactor value info\nglobal_gain,subblock_gain,big_values,region_count Quantization step sizes\ntable_select,count1_table Huï¬€man table selection info\nblock_type,mixed_block_flag Sub-band window selection info\nmdct_coef Decoded MDCT coeï¬ƒcients\nFor theğ‘™-th frame, we concatenate the outputs of CNN-1\nand CNN-2 as well as the values of the remaining ï¬elds into a\nfeaturevector zğ‘™ 2Rğ‘‘model ,whichisusedastheinputtothetrans-\nformers. The sizes of the CNN outputs are selected so that the\ndimensionalityof zğ‘™â€™ssatisï¬esğ‘‘model = 300. Theselfattention\nmechanism does not use the order information of the elements\nin the input sequence. To allow the transformer to make use\nof the temporal order of frames, the framesâ€™ corresponding\nfeature vectors are added with positional encoding [18]. The\npositionalencodingisaseriesof ğ¿distinctï¬xed-valuedvectors\nfp1Â–p2Â–Â•Â•Â•Â– pğ¿g, wherepğ‘™ 2Rğ‘‘model . After adding positional\nencoding,the ğ‘™-thfeaturevectoris z0\nğ‘™ = zğ‘™Â¸pğ‘™. Thetransformer\nwill likely be able to ï¬nd the position ofz0\nğ‘™ being ğ‘™based on\nthe pğ‘™ component ofz0\nğ‘™.\nWe use a similar approach as described in [21] to binary\nclassify the feature vectors corresponding for each MP3 frame\nas being â€œsingle compressedâ€ or â€œmultiple compressedâ€. The\nnetwork hasğ¿special vectors, known as â€œclass tokensâ€, which\nare denoted byfc1Â–c2Â–Â•Â•Â•Â– cğ¿g, wherecğ‘™ 2Rğ‘‘model . The input\nto the transformerZ is the feature vectorzğ‘™â€™s interleaved with\nthe class tokens, which can be written as\nZ =\n\u0002\nc1 jz0\n1 jc2 jz0\n2 jÂ•Â•Â• jcğ¿ jz0\nğ¿\n\u0003\n2R2ğ¿\u0002ğ‘‘model Â• (5)\nLet tfÂ¹\u0001Âºbe the function corresponding to the transformer\nnetwork. After the transformer operations, the output can be\nwritten as\ntfÂ¹ZÂº=\n\u0002Ëœc1 jËœz0\n1 jËœc2 jËœz0\n2 jÂ•Â•Â• jËœcğ¿ jËœz0\nğ¿\n\u0003\n2R2ğ¿\u0002ğ‘‘model Â• (6)\nTo ï¬nd the estimated labels for theğ‘™-th frame, we use a\nMultilayer Perceptron (MLP) network to classifyËœcğ‘™. During\ntraining, the gradients with respect tocğ‘™â€™s are computed and\nused to update them using gradient descent [27]. That is,\nwe train each class token to collect information necessary to\ndetermine the label for its corresponding time step. Finally,\n3\nğ‘¥3\nâ‹®\nğ‘¥ğ¿\nğ‘¥2\nğ‘¥1\nframes\nMDCT coef.\nscalefactor\nblock_type\nregion_count\nbig_values\nâ‹¯\n1 2 3 â‹¯ ğ¿\n1 2 3 â‹¯ ğ¿\n1 2 3 â‹¯ ğ¿\n1 2 3 â‹¯ ğ¿\n1 2 3 â‹¯ ğ¿\ncodec parameters\nof each frame\nCNN-1\nCNN-2\nâ€¢ ğ’›â€²\n1 ğ’›â€²\n2 ğ’›â€²\n3 â‹¯ ğ’›â€²\nğ¿\nfeature vectors\n~\npositional\nencoding\nğ’„2ğ’›â€²\n1ğ’„1 ğ’›â€²\n2\nâ‹® ğ’„ğ¿ ğ’›â€²\nğ¿\ninsert\nclass tokens\nTransformer\nÌƒ ğ’„2Ìƒ ğ’›â€²\n1Ìƒ ğ’„1 Ìƒ ğ’›â€²\n2\nâ‹® Ìƒ ğ’„ğ¿ Ìƒ ğ’›â€²\nğ¿\nMLP Ì‚ ğ‘¦2\nÌ‚ ğ‘¦1\nâ‹®\nestimated label\nof each frame\nFig. 2: The block diagram of the proposed method, which analyzesğ¿frames at a time. Each circular node (ğ‘™) represents a MP3\ncodec parameter vector whose corresponding frame is shown by the number inside. Each diamond node (z0ğ‘™) represents a vector\nassociated with the feature vectorz0\nğ‘™. Each rectangular node (cğ‘™) represents a vector associated with the class tokencğ‘™. Diï¬€erent\ngroups of vectors are shown in diï¬€erent colors.\nthe label of each frame can be determined by only examining\nthe class tokens.\nAfter preliminary experiments, we used 8 transformer lay-\ners with the number of headsâ„ = 15 in the Multihead Self\nAttention (see Section 2.2), which yielded the best perfor-\nmance.\nThe MLP network we use is made up of one layer of 800\nneurons and a ï¬nal output layer with 2 neurons using softmax\nactivation. WeusetheGELU[28]nonlinearactivationfunction\nin the CNNs and the transformer network.\nAfter training, for a given MP3 audio frame sequencefğ‘¥1Â–\nğ‘¥2Â–Â•Â•Â•Â–ğ‘¥ ğ¿g, our method generates a binary decision sequence\nfË†ğ‘¦1Â–Ë†ğ‘¦2Â–Â•Â•Â•Â– Ë†ğ‘¦ğ¿gdescribing whether each frame is multiply\ncompressed or not. This enables one to localize the question-\nable frames in an MP3 ï¬le and determine which part of the\nMP3 frames may have been spliced.\n4. EXPERIMENTS AND RESULTS\nIn this section we describe our experiments and present re-\nsults comparing our method to other methods for splicing\nlocalization using traces of multiple MP3 compression.\n4.1. Dataset\nThe data for our experiments contain only uncompressed WAV\naudioï¬les. Weusedthreepubliclyavailabledatasetsconsisting\nof speech and diï¬€erent music genres: LJSpeech [29], GTZAN\n[30],andMAESTRO[31]. WethencompressedtheWAVï¬les\nusing MP3. The MP3 sampling rate we selected is 44.1 kHz,\nand the length of each frame is1152Â44100 \u001926Â•12ms. In our\nexperiments, our method examinedğ¿= 20 frames at a time.\nThe MP3 compression data rates and data rate types used\nin our experiments are shown in Table 2. We used Constant\nBit Rate (CBR) and Variable Bit Rate (VBR) compression.\nThe audio signals were compressed using FFmpeg1 v3.4.8\nand LAME2 v3.100. We excluded low-quality MP3 compres-\nsion conï¬gurations that make multiple-compression detection\n1https://www.ffmpeg.org/\n2https://lame.sourceforge.io/\nunreliable [15]. We also excluded ultra-high-quality MP3\ncompression that is not recommended by FFmpeg [32].\nTable 2: MP3 compression types used in our experiments.\nThe data rate of VBR compression decreases as quality index\nincreases. More details can be obtained from [32].\nCompression type Bit rate/Quality\nConstant Bit Rate (CBR)64kbps, 96kbps, 128kbps,\n160kbps, 192kbps, 256kbps\nVariable Bit Rate (VBR)1, 2, 3, 4, 5, 6\nOur dataset generation scheme is described as follows.\nWe split each uncompressed WAV audio ï¬le into smaller\nsegmentsof80â€“320framesrandomly. Eachsegmentisfurther\ntemporally subdivided into â€œslicesâ€ ofğ¿Â2 = 10 frames where\nğ¿isthenumberofframesourmethodexaminesatatime. Now,\neach segment will contain 8â€“32 slices. In each segment, the\noddsliceswillbemultiplecompressedfor2â€“3times. Theeven\nslices will be single compressed. The compression parameters\narechosenfromTable2atrandom. InFigure3weillustratethe\ndataset generation method on a segment containing 4 slices. If\nasliceneedstobecompressedthreetimes,thencompression1â€“\n3willbeused. Ifasliceneedstobecompressedtwotimes,then\ncompression 2â€“3 will be used. Otherwise, only compression 3\nwill be used. For a given slice, after compression 1 the MP3\nï¬le is decompressed back to the time domain, which is used as\nthe input to compression 2. At compression 3, we gather the\ndecompressed or pristine time domain samples of all slices in\nasegmentandthencompressthemasoneMP3ï¬le. Therefore,\ntheparametersforcompression3willbethesameforallslices\nin a given segment. This completes the construction of our\nground-truthed experimental data set.\n4.2. Preparing an MP3 ï¬le For Training\nRecallourmethodoperatesintheâ€œMP3domainâ€byexamining\nthe MP3 codec ï¬elds shown in Table 1. To train our method,\nwe used a sliding window of lengthğ¿with oï¬€set step size 8 to\nextracttheMP3codecï¬eldsfromtheMP3compressedframes\nas shown in Table 1.\nWe generated 486,743 MP3 frame sequences of lengthğ¿\n4\nCBR, 64kbps CBR, 64kbps CBR, 64kbps CBR, 64kbps\nVBR, Q=4\nCBR, 256kbps\nCBR, 160kbps\nğ¿/2 = 10 frames ğ¿/2 = 10 frames ğ¿/2 = 10 frames ğ¿/2 = 10 frames\nSlice 1 Slice 2 Slice 3 Slice 4\nCompression 1\nCompression 2\nCompression 3\n1,1,1,1,1,1,1,1,1,1, 0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1, 0,0,0,0,0,0,0,0,0,0ğ’š = [ ]\nFig. 3: An illustration of our dataset generation method on a\nsegment of 40 frames (4 slices). The corresponding label for\nthis segmenty is shown at the bottom.\nfrom our experimental dataset. We used 54% of the frame\nsequences in the training set; 13% of the frame sequences\nin the validation set, and 33% of the frame sequences in the\ntesting set. The partition was done in a way that all frame\nsequences generated from one segment can only be used in\none of the three sets.\nSelecting the MP3 frame sequence lengthğ¿for analysis\nis a trade-oï¬€ between the accuracy of the estimation ofË†y and\nthe training diï¬ƒculty of the network. Using a largerğ¿ may\nimprovetheperformance,butitmayalsosigniï¬cantlyincrease\ntraining time and the need of hyperparameter tuning. In our\nexperiments, we usedğ¿= 20 corresponding to an audio signal\nlengthofapproximately522.4ms. Thismaybesmallcompared\nto the typical length of most audio signals. In training we\nused a sliding window stride of 8, which does not align with\nthe slice boundaries. In each sliding window, the index of the\nï¬rst multiple compressed frame and the number of multiple\ncompressedframearediï¬€erent. Thisforcesthetrainednetwork\nto predict the labels correctly given an arbitrary portion taken\nfromanaudiosignal. Therefore,ourmethodisabletoexamine\nlonger audio ï¬les even ifğ¿is relatively small.\n4.3. Hyperparameters and Training\nThe entire network including the CNNs and the transformers\nare trained end-to-end. We trained the network using the\nAdam optimizer [33] with an initial learning rate of10\u00004 and\na dropout rate of 0.2 until the validation accuracy no longer\nincreased for 20 epochs.\n4.4. Results\nWe compared the performance of our method to [6], [11], [15],\nwhich are introduced in Section 1. Since they are all detection\nmethods, we used the predictions from these methods on short\nframe sequences of lengthğ¿0= 4 to approximate localization.\nChoosing the lengthğ¿0 for localization approximation is a\ntrade-oï¬€ between classiï¬cation accuracy and granularity of\nlocalization. Largerğ¿0improves the label estimation of each\nframe sequence, while smallerğ¿0improves the localization\naccuracy. Selecting ğ¿0= 4 is a reasonable balance between\nthese two factors [16].\nIn Table 3, we compare the performance of our method to\nthat of the three previous methods. We used three diï¬€erent\nmetrics: Jaccard score [34],ğ¹1-score [35] and balanced ac-\ncuracy score [36]. LetIÂ¹yÂºbe the function that returns the\nset of indices for the one-entries iny. The Jaccard score can\nbe computed byjIÂ¹yÂº\\IÂ¹Ë†yÂºjÂjIÂ¹yÂº[IÂ¹Ë†yÂºj, which compares the\nsimilarity between the predicted multiple compressed region\nand the ground truth multiple compressed region [34]. The\nğ¹1-score is the harmonic mean of the precision and recall,\nwhich considers both factors at the same time [35]. The bal-\nanced accuracy score is the traditional accuracy score with\nclass-balanced sample weights [36]. Our approach achieved\nthe highest score on all three metrics. We also tested our\nmethod on a separately generated dataset containing 53,541\nMP3 frame sequences with variable slice length ranging from\n10 to 80 frames. For each slice, the slice length, compression\ntypes and the number of compressions are chosen at random.\nOur method achieved 81.64 balanced accuracy on this dataset,\nwhich is close to the result in Table 3. This shows our method\ndid not learn strong dataset bias.\nIn Table 4, we show the recall [35] of each method on\nmultiply compressed MP3 frames against selected last MP3\ncompression types. It can be seen that the detection accuracy\ndecreases as the MP3 compression quality declines. Diï¬€erent\nmethods reacted to CBR and VBR compression in contrasting\nmanners. For[6]andourmethod,theperformancewassimilar.\nFor[11], thescoreofVBRframeswassigniï¬cantlylowerthan\nthose of CBR; the behavior of [15] was the opposite.\nIn table 5, we show the recall of each method compared to\nthenumberofMP3compressionappliedtoaframe. Therecall\nof our method is more consistent across diï¬€erent repetition\nof MP3 compression. For all methods, the recall for double\ncompression is close to that of triple compression.\nOverall, our approach demonstrated high localization per-\nformance and robustness across many types of MP3 compres-\nsion.\nTable 3: Performance metrics comparison.\nMethod Jaccard\nScore ğ¹1-scoreBalanced\nAccuracy\nYanet al.[15] 13.53 18.71 53.35\nYanget al.[11] 30.73 40.95 55.28\nLiuet al.[6] 48.18 58.91 68.72\nOur Approach 80.50 84.43 84.49\nTable 4: The recall of multiple compression localization\nof each method against selected last MP3 compression types.\nCBRcompressionisdenotedbyC<bitrate>;VBRcompression\nis denoted by V<quality index>.\nLast MP3 Comression Type\nMethod C64 C128 C160 C192 V1 V2 V4 V6\nYanet al.[15] 17.30 6.86 6.80 4.87 22.80 23.23 22.59 17.55Yanget al.[11] 19.27 70.75 71.71 66.21 45.58 39.05 27.12 22.06Liuet al.[6] 58.45 56.48 54.47 55.01 55.15 56.41 57.47 67.93Our Approach 73.09 88.06 89.65 90.84 93.31 91.21 83.52 65.51\n5\nTable5: TherecallofeachmethodagainstthenumberofMP3\ncompression.\nNum. MP3 Compression\nMethod Single Double Triple Overall\nYanget al.[11] 67.28 43.51 43.02 43.27\nYanet al.[15] 91.71 14.86 15.10 14.98\nLiuet al.[6] 79.36 57.27 58.85 58.06\nOur Approach 84.61 83.76 84.92 84.34\n5. CONCLUSIONS\nWe proposed a multiple MP3 compression temporal localiza-\ntion method based on transformer neural networks that uses\nMP3 compressed data. Our proposed method localizes mul-\ntiple compression at the frame level. The experiment results\nshowed that our method had the best performance compared\nto other approaches and was robust against many MP3 com-\npression settings. In the future, we will examine extending\nthis approach to other compression methods such as AAC.\nWe will also investigate the use of stereo channels as well\nas the second granule in MP3 compressed frames. We will\ngeneralize the concept of multiple compression detection to\ncompression history detection. That is, to ï¬nd the number of\ncompressionsandthetypesofcompressionused. Knowingthe\ncompression history can greatly enhance the interpretability\nfor audio forensics.\n6\n6. REFERENCES\n[1] R. Prenger, R. Valle, and B. Catanzaro, â€œWaveglow: A ï¬‚ow-\nbased generative network for speech synthesis,â€Proceedings\nof 2019 IEEE International Conference on Acoustics, Speech\nand Signal Processing, pp. 3617â€“3621, 2019, Brighton, UK.\n[2] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y.\nLiu, â€œFastspeech 2: Fast and high-quality end-to-end text to\nspeech,â€arXiv preprint arXiv:2006.04558, 2020.\n[3] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and\nI. Sutskever, â€œJukebox: A generative model for music,â€arXiv\npreprint arXiv:2005.00341, 2020.\n[4] C.Borrelli,P.Bestagini,F.Antonacci,A.Sarti,andS.Tubaro,\nâ€œSynthetic speech detection through short-term and long-term\npredictiontraces,â€ EURASIPJournalonInformationSecurity ,\nvol. 2021, no. 2, pp. 1â€“14, 2021.\n[5] R. Yang, Y.-Q. Shi, and J. Huang, â€œDefeating fake-quality\nMP3,â€Proceedingsofthe11thACMWorkshoponMultimedia\nand Security, pp. 117â€“124, 2009, Princeton, NJ, USA.\n[6] Q. Liu, A. H. Sung, and M. Qiao, â€œDetection of double MP3\ncompression,â€Cognitive Computation, vol. 2, no. 4, pp. 291â€“\n296, 2010.\n[7] ISO/IEC 13818-3:1995 - Information technology â€“ Generic\ncoding of moving pictures and associated audio information\nâ€“ Part 3: Audio, https://www.iso.org/standard/22991.\nhtml.\n[8] H. Musmann, â€œGenesis of the mp3 audio coding standard,â€\nIEEE Transactions on Consumer Electronics, vol. 52, no. 3,\npp. 1043â€“1049, 2006.\n[9] ISO/IEC 13818-7:1997 - Information technology â€“ Generic\ncoding of moving pictures and associated audio information â€“\nPart 7: Advanced Audio Coding (AAC), https://www.iso.\norg/standard/25040.html.\n[10] K. Brandenburg, â€œMP3 and AAC explained,â€Proceedings\nof the AES 17th International Conference on High-Quality\nAudio Coding, 1999, Signa, Italy.\n[11] R. Yang, Y. Q. Shi, and J. Huang, â€œDetecting double com-\npression of audio signal,â€Media Forensics and Security II,\nvol. 7541, pp. 200 â€“209, 2010.\n[12] M. Qiao, A. H. Sung, and Q. Liu, â€œImproved detection of\nMP3doublecompressionusingcontent-independentfeatures,â€\nProceedingsof2013IEEEInternationalConferenceonSignal\nProcessing, Communication and Computing, pp. 1â€“4, 2013,\nKunMing, China.\n[13] P. Ma, R. Wang, D. Yan, and C. Jin, â€œDetecting double-\ncompressedMP3withthesamebit-rate.,â€ JournalofSoftware ,\nvol. 9, no. 10, pp. 2522â€“2527, 2014.\n[14] â€”â€”,â€œAhuï¬€mantableindexbasedapproachtodetectdouble\nMP3 compression,â€ inDigital-Forensics and Watermarking,\nBerlin, Heidelberg, 2014, pp. 258â€“271.\n[15] D.Yan,R.Wang,J.Zhou,C.Jin,andZ.Wang,â€œCompression\nhistory detection for MP3 audio,â€KSII Transactions on Inter-\nnetandInformationSystems(TIIS) ,vol.12,no.2,pp.662â€“675,\n2018.\n[16] T. Bianchi, A. De Rosa, M. Fontani, G. Rocciolo, and A. Piva,\nâ€œDetection and localization of double compression in mp3\naudio tracks,â€EURASIP Journal on information Security,\nvol. 2014, no. 10, 2014.\n[17] D.Luo,W.Cheng,H.Yuan,W.Luo,andZ.Liu,â€œCompression\ndetectionofaudiowaveformsbasedonstackedautoencoders,â€\ninArtiï¬cial Intelligence and Security, Cham, 2020, pp. 393â€“\n404.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Å. Kaiser, and I. Polosukhin, â€œAttention is\nall you need,â€ inAdvances in neural information processing\nsystems, vol. 30, 2017, pp. 5998â€“6008.\n[19] N. Jayant, J. Johnston, and R. Safranek, â€œSignal compression\nbased on models of human perception,â€Proceedings of the\nIEEE, vol. 81, no. 10, pp. 1385â€“1422, 1993.\n[20] T. Brown, B. Mann, N. Ryder,et al., â€œLanguage models\nare few-shot learners,â€ inAdvances in Neural Information\nProcessing Systems, vol. 33, 2020, pp. 1877â€“1901.\n[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X.\nZhai,T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.\nGelly,etal.,â€œAnimageisworth16x16words:Transformersfor\nimage recognition at scale,â€arXiv preprint arXiv:2010.11929,\n2020.\n[22] N.Carion,F.Massa,G.Synnaeve,N.Usunier,A.Kirillov,and\nS. Zagoruyko, â€œEnd-to-end object detection with transform-\ners,â€Proceedings of 2020 European Conference on Computer\nVision, pp. 213â€“229, 2020.\n[23] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O.\nRonneberger, K. Tunyasuvunakool, R. Bates, A. Å½Ã­dek, A.\nPotapenko,etal.,â€œHighlyaccurateproteinstructureprediction\nwithalphafold,â€Nature,vol.596,no.7873,pp.583â€“589,2021.\n[24] S. Liu and W. Deng, â€œVery deep convolutional neural network\nbased image classiï¬cation using small training sample size,â€\nProceedings of 2015 3rd IAPR Asian Conference on Pattern\nRecognition, pp. 730â€“734, 2015.\n[25] R. Raissi,The theory behind MP3, 2002. [Online]. Available:\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?\ndoi=10.1.1.113.6804.\n[26] P. Sripada, â€œMP3 decoder in theory and practice,â€ M.S. thesis,\nBlekinge Institute of Technology, Ronneby, Sweden, Mar.\n2006. [Online]. Available:https://www.diva- portal.\norg/smash/get/diva2:830195/FULLTEXT01.pdf.\n[27] S. Ruder, â€œAn overview of gradient descent optimization\nalgorithms,â€arXiv preprint arXiv:1609.04747, 2016.\n[28] D. Hendrycks and K. Gimpel, â€œGaussian error linear units\n(gelus),â€arXiv preprint arXiv:1606.08415, 2016.\n[29] K. Ito and L. Johnson,The lj speech dataset, 2017. [On-\nline]. Available:https : / / keithito . com / LJ - Speech -\nDataset/.\n[30] G. Tzanetakis and P. Cook, â€œMusical genre classiï¬cation\nof audio signals,â€IEEE Transactions on Speech and Audio\nProcessing, vol. 10, no. 5, pp. 293â€“302, 2002.\n[31] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-Z. A.\nHuang, S. Dieleman, E. Elsen, J. Engel, and D. Eck, â€œEn-\nabling factorized piano music modeling and generation with\nthe MAESTRO dataset,â€Proceedings of the International\nConference on Learning Representations, 2019.\n[32] FFmpeg MP3 encoding guide. [Online]. Available:https:\n//trac.ffmpeg.org/wiki/Encode/MP3.\n[33] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic\noptimization,â€arXiv preprint arXiv:1412.6980, 2014.\n[34] M. Levandowsky and D. Winter, â€œDistance between sets,â€\nNature, vol. 234, no. 5323, pp. 34â€“35, 1971.\n[35] D. M. Powers, â€œEvaluation: From precision, recall and F-\nmeasure to ROC, informedness, markedness and correlation,â€\narXiv preprint arXiv:2010.16061, 2020.\n[36] K. H. Brodersen, C. S. Ong, K. E. Stephan, and J. M. Buh-\nmann, â€œThe balanced accuracy and its posterior distribution,â€\n2010 20th International Conference on Pattern Recognition,\npp. 3121â€“3124, 2010, Istanbul, Turkey.\n7"
}