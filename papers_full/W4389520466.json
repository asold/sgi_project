{
    "title": "Estimating Large Language Model Capabilities without Labeled Test Data",
    "url": "https://openalex.org/W4389520466",
    "year": 2023,
    "authors": [
        {
            "id": null,
            "name": "Harvey Fu",
            "affiliations": [
                "University of Southern California"
            ]
        },
        {
            "id": "https://openalex.org/A2157516767",
            "name": "Qinyuan Ye",
            "affiliations": [
                "University of Southern California"
            ]
        },
        {
            "id": "https://openalex.org/A2247873162",
            "name": "Albert Xu",
            "affiliations": [
                "University of Southern California"
            ]
        },
        {
            "id": "https://openalex.org/A2108009659",
            "name": "Xiang Ren",
            "affiliations": [
                "University of Southern California"
            ]
        },
        {
            "id": "https://openalex.org/A2476502706",
            "name": "Robin Jia",
            "affiliations": [
                "University of Southern California"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4296195043",
        "https://openalex.org/W3172943453",
        "https://openalex.org/W4285429195",
        "https://openalex.org/W4386566815",
        "https://openalex.org/W3164972323",
        "https://openalex.org/W2295598076",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3162385798",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W3197876970",
        "https://openalex.org/W4214627684",
        "https://openalex.org/W3035441651",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4385574236",
        "https://openalex.org/W2626967530",
        "https://openalex.org/W3104939451",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2254249950",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4389519967",
        "https://openalex.org/W3121904249",
        "https://openalex.org/W2974098375",
        "https://openalex.org/W4287207937",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4221153523",
        "https://openalex.org/W4378499145",
        "https://openalex.org/W3152515526",
        "https://openalex.org/W4226146865"
    ],
    "abstract": "Large Language Models (LLMs) have exhibited an impressive ability to perform in-context learning (ICL) from only a few examples, but the success of ICL varies widely from task to task. Thus, it is important to quickly determine whether ICL is applicable to a new task, but directly evaluating ICL accuracy can be expensive in situations where test data is expensive to annotate—the exact situations where ICL is most appealing. In this paper, we propose the task of ICL accuracy estimation, in which we predict the accuracy of an LLM when doing in-context learning on a new task given only unlabeled test data for that task. To perform ICL accuracy estimation, we propose a method that trains a meta-model using LLM confidence scores as features. We compare our method to several strong accuracy estimation baselines on a new benchmark that covers 4 LLMs and 3 task collections. The meta-model improves over all baselines across 7 out of 12 settings and achieves the same estimation performance as directly evaluating on 40 collected labeled test examples per task. At the same time, no existing approach provides an accurate and reliable ICL accuracy estimation in every setting, highlighting the need for better ways to measure the uncertainty of LLM predictions.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9530–9546\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEstimating Large Language Model Capabilities without Labeled Test Data\nHarvey Yiyun Fu Qinyuan Ye Albert Xu Xiang Ren Robin Jia\nUniversity of Southern California, Los Angeles, CA, USA\n{harveyfu, qinyuany, albertxu, xiangren, robinjia}@usc.edu\nAbstract\nLarge Language Models (LLMs) have the im-\npressive ability to perform in-context learn-\ning (ICL) from only a few examples, but the\nsuccess of ICL varies widely from task to\ntask. Thus, it is important to quickly determine\nwhether ICL is applicable to a new task, but\ndirectly evaluating ICL accuracy can be expen-\nsive in situations where test data is expensive\nto annotate—the exact situations where ICL is\nmost appealing. In this paper, we propose the\ntask of ICL accuracy estimation, in which we\npredict the accuracy of an LLM when doing\nin-context learning on a new task given only\nunlabeled test data for that task. To perform\nICL accuracy estimation, we propose a method\nthat trains a meta-model using LLM confidence\nscores as features. We compare our method to\nseveral strong accuracy estimation baselines\non a new benchmark that covers 4 LLMs and\n3 task collections. The meta-model improves\nover all baselines across 8 out of 12 settings and\nachieves the same estimation performance as\ndirectly evaluating on 40 collected labeled test\nexamples per task. At the same time, no exist-\ning approach provides an accurate and reliable\nICL accuracy estimation in every setting, high-\nlighting the need for better ways to measure the\nuncertainty of LLM predictions.1\n1 Introduction\nIn-context learning (ICL) with large language mod-\nels (LLMs) has shown great potential in perform-\ning a wide range of language tasks (Brown et al.,\n2020). ICL has the unique advantages of being\ndata-efficient (i.e., only a few labeled training ex-\namples are needed) and accessible (i.e., expertise in\ntraining models is no longer required). With these\nadvantages, a non-expert user can create a system\nto perform a new task within minutes by writing a\nfew examples. This gives rise to the popularity of\n1Code is publicly released at github.com/harvey-fin/\nicl-estimate\nMeta \nModel\n0.8\nTrain\nConfidence \nProfile\nAccuracy\nMeta \nModel\n?\nTest\nConfidence \nProfile\nAccuracy\nTrain \nDataset\nInput\nConfidence\nPred \ny\nTrue \ny\n0.8\nAurora\nAurora\n0.6\nNurek \nDam\nTehri \nDam\n...\n...\n...\n...\nTest \nDataset\nInput\nConfidence\nPred \ny\nTrue \ny\n0.4\nMary \nRamsey\n0.5\nvolleyball\n...\n[Prompt]\n \nname \nof \nsleeping \nbeauty\n[Prompt]\n \nhighest \ndam \nin \nindia\n[Prompt]\n \nWho \nis \nyounger \nMary \nRamsey \nor \nLee \nRanaldo \n?\n[Prompt]\n \nWhat \nsport \ndid \nJack \nSock \nand \nRaffaella \nReggi \nplay?\n...\n...\nUnlabeled\n?\n?\nAccuracy \nEstimation \nwithout \nLabeled \nTest \nData\nFigure 1: A demonstration of our task setting: given\nICL accuracy observations of labeled training datasets,\nwe want to estimate the dataset level ICL accuracy on\nthe unseen test datasets without labeled test data. We\npropose the method of training a meta-model based on\nthe confidence score distributions (we denote them as\ndataset confidence profiles)\nICL— it is being adopted and tested for a variety\nof use cases that stretch the boundary of what is\nconsidered possible to do with language models.\nDespite the advantages of ICL, its performance\nis highly task-dependent (see Figure 6). It sur-\npasses expectations on some tasks that are diffi-\ncult for humans, such as answering trivia questions\nor riddles, but achieves near-zero performance on\nseemingly trivial tasks such as some text editing or\nspelling tasks (Srivastava et al., 2022). While evalu-\n9530\nating ICL with a labeled test set is a direct solution\nto know whether ICL will be effective, it greatly\nreduces the appeal of ICL, as one of ICL’s key sell-\ning points is that it does not require a large labeled\ndataset. In addition, many tasks do not come with\na labeled test set due to high annotation costs (e.g.,\nmedical/law-related questions that require profes-\nsional knowledge to answer). In such cases, it is\nhighly desirable to estimate the ICL performance\nwithout a labeled test set. This would help system\ndevelopers determine whether ICL is likely to be\nuseful for their problems of interest.\nGuided by this motivation, we formalize the\nproblem of few-shot ICL accuracy estimation :\ngiven a handful of labeled in-context examples and\na set of unlabeled test examples, our goal is to es-\ntimate the overall accuracy of ICL on these test\nexamples. Our contributions are twofold:\n• We propose to address the accuracy estimation\nproblem by training a “meta-model,” which\ntakes in LLM confidence features as input and\noutputs the task accuracy. The meta-model\nis trained with observed ICL accuracies on\nseen datasets, and then used to estimate ICL\naccuracy on unseen datasets (see Figure 1).\n• We obtain 42,360 observations of LLM ICL\nperformance, by conducting extensive ICL\nexperiments spanning two tasks (multiple-\nchoice QA and closed-book QA), 91 datasets,\nand 4 LLMs. We then benchmark the meta-\nmodel method and multiple baselines on a\ntotal of 12 evaluation settings derived from\nthese observations.\nOur meta-model can estimate ICL accuracies\nwithout the need for labeled test examples. In 10\nout of 12 settings, the meta-model estimates are\nat least as accurate as directly evaluating on 16 la-\nbeled examples. In 2 out of 12 settings, they match\nwith evaluating on 128 labeled examples. On av-\nerage, we are able to save the annotation cost of\n40 test labels per task by using the meta-model.\nFurther, the meta-model outperforms all baseline\nmethods in 8 out of 12 settings, improving the rela-\ntive estimation error by 23.6% However, we also\nfind that there exists substantial room for improve-\nment across all settings. We envision estimating\nICL accuracy without labeled test data as an open\nchallenge and encourage the community to develop\nnew techniques that can more accurately predict\nwhen ICL will be effective.\n2 Related Work\n2.1 Model Confidence and Calibration\nCalibration of LLMs has been studied on a diverse\nrange of tasks such as classification (Desai and\nDurrett, 2020) and question answering (QA) (Jiang\net al., 2021; Kadavath et al., 2022). It aims to\nstudy whether LLMs assign meaningful correctness\nlikelihood—also known as model confidence—to\nthe outputs (Guo et al., 2017). Most prior work\nevaluates calibration at the example level (Desai\nand Durrett, 2020; Kamath et al., 2020); in this\npaper, we focus on using overall model confidence\ndistributions to estimate dataset-level accuracies.\nWe propose a method to learn model calibration\npatterns based on observations of LLMs’ perfor-\nmance at the dataset level.\n2.2 In-context Learning\nLLMs pre-trained with auto-regressive language\nmodeling objectives have been shown to be capa-\nble of “learning” in context when given a prompt\ncomposed of a prompt template and a few labeled\ndemonstrations (Brown et al., 2020; Chowdhery\net al., 2022). While LLMs can learn a new task\nonly through model inference, the accuracy is sen-\nsitive to the choices of prompt templates and in-\ncontext examples (Lu et al., 2021; Zhao et al., 2021;\nPerez et al., 2021). Therefore, we aim to develop\na method to accurately estimate ICL performance\nfor a dataset prompted with any prompt template\nand combination of in-context examples.\n2.3 Out-of-distribution (OOD) Prediction\nMachine learning models in the real world com-\nmonly encounter distribution shifts between train-\ning and test time. Prior work (Guillory et al., 2021;\nGarg et al., 2022; Yu et al., 2022; Singhal et al.,\n2022; Li et al., 2022) aims to predict models’ OOD\nperformance under different setups. Garg et al.\n(2022) predict target domain accuracy for image\nclassification tasks with distribution by fitting a\nthreshold on model confidence using only labeled\nsource data and unlabeled target data. Singhal et al.\n(2022) use a few additional target-domain exam-\nples to predict the accuracy, focusing on known\nsource-target dataset pairs on which models often\nhave low OOD accuracy due to overfitting to spu-\nrious correlations ( e.g., MNLI-HANS and QQP-\nPAWS). They find that accuracy on the given small\nset of target examples is a strong baseline to ap-\nproximate accuracy on the full-test set. We include\n9531\nthe accuracy for a small set of labeled test exam-\nples as an oracle baseline (see Section 3.3). These\npapers all try to predict the OOD accuracy of a\nmodel trained on in-distribution training data; in\ncontrast, in our setting we have access to some la-\nbeled datasets but the language models we study\nwere never finetuned on those datasets. In order to\navoid confusion, we instead use the terms “seen/un-\nseen tasks” to describe the datasets available to us,\nrather than “in-distribution/out-of-distribution.”\n3 Accuracy Prediction\n3.1 Problem Definition\nWe formalize the task of ICL accuracy estima-\ntion for unseen datasets given observations of\nthe same model’s performance on other datasets.\nA method for the ICL accuracy estimation task\ntakes in four inputs: a language model M;\na set of labeled seen datasets {Di}r\ni=1, where\neach Di consists of a set of labeled examples\n{(x(1)\ni ,y(1)\ni ),..., (x(ni)\ni ,y(ni)\ni )}and ni = |Di|; a\nprompt c for the test task; and an unlabeled test\ndataset Dtest = {x(1)\ntest,...,x (m)\ntest}of size m. In\na typical setting, each seen task should consist\nof a sufficient amount of labeled examples, i.e.,\nni ≥ 100. The method should output the esti-\nmated accuracy of M on Dtest when prompted\nwith prompt c; we denote the actual accuracy\nof the model as accM,c\ntest and ˆaccM,c\ntest as the pre-\ndicted accuracy. Note that with the labeled datasets\nDi and a corresponding prompt ˜c, we can com-\npute the corresponding dataset-level ICL accuracy\naccM,˜c\ni for i= 1,...,r .\n3.2 Prompt Formulation and Data Splits\nWe construct prompts by sampling kin-context ex-\namples uniformly at random from available labeled\ndata and formatting them with prompt templates\nto form a prompt (see Section B and Table 6 in\nthe Appendix). For each dataset Di, we sample\nKprompts, each consisting of a prompt template\nfollowed by a list of in-context examples to form\na prompt cij for j = 1,...,K . Note that each\ndataset has a training/test data split: we sample\nin-context examples only from the training set, and\nmeasure accuracy only on the test set. For simplic-\nity, we use cto denote a prompt in general, Ci to\ndenote the set of training prompts for dataset Di,\nand Ctest to denote test prompts for dataset Dtest.\n3.3 Comparing with Labeled Test Data\nTo put our results in context, we compare all meth-\nods to the Oracle approach of sampling llabeled\nexamples from the test dataset Dtest and measur-\ning accuracy on those lexamples, which we call\noraclel. This approach is used by Singhal et al.\n(2022) and it represents how well we can evaluate\nICL performance for Dtest by collecting labeled\nexamples. With a large value of l, we get a better\nevaluation of the test dataset at the cost of collect-\ning expensive annotations. In proposing the task of\naccuracy prediction, we hope to develop methods\nthat outperform the l-labeled oracle for values of l\nthat represent non-trivial annotation costs.\n4 Confidence Profile Meta-Model\nWe propose a new method that trains a meta-model\nbased on the confidence profiles of seen datasets\n{Di}r\ni=1 to estimate ICL performance. We use the\nterm confidence profile to denote the distribution\nof model confidence scores on each example in\nthe dataset. We extract the confidence profiles (see\nFigure 1) from each seen dataset and convert them\nto a feature vector. We then train a meta-model\nto map confidence feature vectors to the dataset-\nlevel ICL accuracies. The benefits of using the\nconfidence feature vector are twofold. First, we do\nnot need any labeled test data, which saves annota-\ntion costs. Second, this approach is applicable to\nany pre-trained language model like GPT3 (Brown\net al., 2020) and OPT (Zhang et al., 2022).\n4.1 Confidence Profile\nIn general, given a (not-necessarily labeled) dataset\nD, LM M, and a prompt c, we obtain the con-\nfidence profile by first computing the confidence\nscore sM,c(x) for each x∈D. The score for each\ninput xcan be computed by one forward pass of\nM; the exact value of the score differs based on the\ntask, as described below. Next, we sort the scores\nto obtain a list [s1,...,s |D|] where each si ≤si+1.\nThen we create a dconf -dimensional feature vec-\ntor confM,c\nD , whose i-th component is a linear in-\nterpolation between s⌊|D|×i/d⌋and s⌈|D|×i/d⌉. In-\ntuitively, the i-th feature represents the i/|D|-th\npercentile confidence score. We refer to the fea-\nture vectors derived from confidence profiles as\nconfidence vectors.\n9532\n4.2 Confidence scores\nThe confidence score sM,c(x) is calculated differ-\nently for closed-set generation and open-ended gen-\neration.\nClosed-set generation. Closed-set generation\ntasks have a pre-defined label space Y. We take\noutputs from LLMs and identify the answers only\nby labels (Kadavath et al., 2022). For each exam-\nple, we take model confidence as the normalized\nprobability across the label space:\nsM,c(x) = pˆy∑\n˜y∈Yp˜y\n(1)\nwhere p˜y is the model-assigned probability for label\n˜y on input x, pˆy is the probability for the output\nlabel ˆyfrom model M, and ˆy= arg max˜y∈Yp˜y.\nOpen-ended generation. We refer to tasks that\nrequire sequence generation (e.g., closed-book QA,\nsummarization, machine reading comprehension,\netc.) as open-ended generation tasks. We use neg-\native log-likelihood (NLL)2 to obtain confidence\nscores from each generated sequence. Let ˆy be\nthe model-generated sequence. We compute the\nconfidence score as:\nsM,c(x) =−\n|ˆy|∑\nt=1\nlog pt(ˆyt). (2)\npt is the model-assigned probability distribution at\noutput token tand ˆyt is the t-th output token\n4.3 Meta-Model Training Data\nFor each seen dataset {Di}r\ni=1, we sample K\nprompts {cij}K\nj=1. Then for each prompt sampled\nwe compute the confidence vector confM,cij\ni and\naccuracy accM,cij\ni to create one meta-training ex-\nample (confM,cij\ni ,accM,cij\ni ). This creates a total of\nr×K meta-training examples. The meta-model\nis trained on the meta-training examples and pre-\ndicts the estimated accuracy ˆaccM,ctest\ntest based on\nthe test dataset feature vector confM,ctest\nDtest for each\ntest prompt ctest ∈Ctest. Note that since closed-\nset/open-ended generations have different confi-\ndence scores and accuracy evaluation metrics, the\nmeta-model does not train on datasets that have a\ndifferent task formulation than the test datasets.\n2We also tried perplexity but found NLL to yield better\nresults.\n4.4 Meta-Model Architectures\nWe choose meta-models that are easy to train and\ncontain far fewer parameters than LLMs for compu-\ntational efficiency. In this paper, we consider three\nmeta-model architectures. First, we use k Near-\nest Neighbors regression (k-NN), which measures\nfeature similarity. In the context of this paper, k-\nNN retrieves the most similar confidence profile\nfrom the seen datasets to the test dataset confidence\nprofile and predicts based on the observed ICL\naccuracy on the retrieved in-distribution datasets.\nWe use the implementation in scikit-learn library.3\nSecond, we use a two-layer Multilayer Percep-\ntron (MLP) that takes confidence feature vectors\nas input. Third, we use the tree-based method XG-\nBoost (Chen and Guestrin, 2016) with the same\nconfidence features. We use XGBoostRegressor\nimplemented in the XGBoost library4 and tune the\nhyperparameters as described in Appendix C.2.5\n4.5 Evaluation\nFor task performance evaluation, we use Exact\nMatch (EM) accuracy to measure accuracy for\nclosed-set QA, and F1-score to measure accuracy\nfor open-ended QA.\nWe evaluate accuracy prediction models based\non absolute error, defined as |accM,ctest\ntest −\nˆaccM,ctest\ntest |, where both are computed using the test\ndataset Dtest with a test prompt ctest. We then av-\nerage the absolute error over all prompts Ctest and\ncompute the dataset-specific mean average error:\nerrDtest = 1\n|Ctest|\n∑\nc∈Ctest\n|accM,c\ntest −ˆaccM,c\ntest|.\nFinally, to evaluate the overall success of accuracy\nprediction across a collection of test datasetsT, we\nmeasure mean absolute error (MAE), defined as:\nerrT = 1\n|T|\n∑\nDtest∈T\nerrDtest (3)\n4.6 Baselines\nWe consider four baselines for accuracy estimation.\nAverage training accuracy (AVGTRAIN ). We\nsimply take the average dataset-level accuracy of\n3https://scikit-learn.org/\n4https://xgboost.readthedocs.io/en/stable/\n5We also considered linear models but did not include\nthem here as they perform much worse in estimating ICL\naccuracies.\n9533\nthe seen datasets as our accuracy estimation:\nˆaccAVGTRAIN = 1\nr×K\nr∑\ni=1\nK∑\nj=1\naccM,cij\ni .\nAverage Calibration Error ( AVGCONF ). We\ntake the average confidence across the test dataset\nas the accuracy estimation:\nˆaccAVGCONF = 1\n|Ctest|×m\n∑\nc∈Ctest\n∑\nx∈Dtest\nsM,c(x).\nNote that this baseline is only applicable to closed-\nset generation and not open-ended generation tasks\nsince the accuracy metric for open-ended genera-\ntion (F1 score) and confidence metric (NLL) do\nnot share the same range ([0,1] vs. (−∞,0]). The\nintuition behind AVGCONF is that if the model con-\nfidence scores are well-calibrated (at the example\nlevel), then the expected value of the model’s con-\nfidence scores should be equal to the accuracy. In\nfact, we note that the MAE of AVGCONF is similar\nto Expected Calibration Error (ECE), which mea-\nsures the example-level calibration error (Naeini\net al., 2015; Guo et al., 2017; Kumar et al., 2019;\nDesai and Durrett, 2020).6\nTemperature Scaling (TS) Temperature scaling\nis a widely used calibration method (Hinton et al.,\n2015; Guo et al., 2017; Si et al., 2022). By fitting\na single scaler parameter called temperature τ, it\nproduces softer model-assigned probabilities\npˆy = exp (zˆy/τ)∑\n˜y exp (z˜y∈Y/τ).\nWe then obtain scaled confidence scores with Equa-\ntion 1, and evaluate AVGCONF on the test dataset.\nNote that we optimize temperature τ based on the\nAVGCONF of the training datasets instead of the\ncommon approach of using NLL as an objective\nfunction.\nAverage Threshold Confidence (ATC) We use\nATC (Garg et al., 2022) as one of our OOD ac-\ncuracy estimation baselines. ATC takes accuracy\nestimation by fitting a confidence threshold on a\nsingle source dataset and generalizes to the target\ndataset. We take the estimated accuracy for the test\n6ECE computes a weighted average of the difference be-\ntween confidence and accuracy for each confidence interval\nbin. The main difference is that ECE is commonly computed\nby binning confidence scores into buckets, whereas in our\nsetting each “bucket” is a different OOD dataset.\ndataset to be the average of the ATC estimates from\neach seen dataset:\nˆaccATC = 1\nr\nr∑\ni=1\natcM,c\ni,test,\nwhere atcM,c\ni,test is the Di to Dtest ATC estimate.\n4.7 Alternative Featurizations\nIn addition to the confidence profiles, we experi-\nment with another featurization method that uses\nmodel embeddings from the LM M. Given a\ndataset D, LM M, and prompt c, we obtain the\nmodel embedding by first taking the last-layer, last-\ntoken embedding eM,c(x) for each x ∈D, and\nthen averaging across the dataset:\n˜embed\nM,c\nD = 1\n|D|\n∑\nx∈D\neM,c(x).\nSince ˜embed\nM,c\nD is very high-dimensional ( e.g.,\n5120 dimensional for 13B models), we use Prin-\nciple Component Analysis (PCA) to reduce its di-\nmensionality. We fit the PCA model on all dataset\nembedding vectors ˜embed\nM,c\nD and transform them\ninto de-dimensional vectors embedM,c\nD , which we\ncan use as a feature vector. As an additional exper-\niment, we can concatenate the confidence vector\nand embedding vector to form a combined feature\nvector:\nceM,c\nD = confM,c\nD + embedM,c\nD .\nReducing the dimensionality makes the comparison\nwith confidence features more fair, and does not\ndilute the influence of confidence features when\nconcatenating them with embedding features.\n5 Experiments\n5.1 Accuracy Estimation Benchmark\nWe benchmark both our meta-model method for\nICL accuracy estimation and the baseline methods\nmentioned in Section 4.6 on a total of 12 LLM-\ndataset collection pairs (3 dataset collections ×4\nLLMs). For each evaluation setting, we evaluate 3\ndifferent featurization methods mentioned in Sec-\ntion 4.7. This adds up to 36 experiment settings.\nDatasets. We use three different collections of\ndatasets in total: multiple-choice QA (MCQA)\nfrom MMLU (Hendrycks et al., 2020) and both\n9534\nLLaMA-7B LLaMA-13B OPT-6.7B OPT-13B\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nMAE\nMMLU\nLLaMA-7B LLaMA-13B OPT-6.7B OPT-13B\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nMAE\nMCQA\nLLaMA-7B LLaMA-13B OPT-6.7B OPT-13B\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nMAE\nCBQA\nMLP\n3-NN\nXGBoost\nAvgTrain\nAvgConf\nTS\noracle_4\noracle_8\noracle_16\noracle_32\noracle_64\nFigure 2: Bar graph of evaluation results (MAE) for all meta-models, baseline methods, and Oracle baselines of all\n3 dataset collections with all 4 LLMs. We use the confidence vector as the meta-feature. Red/blue bars represent the\nmeta-model/baseline evaluation results and the horizontal lines show the Oracle baselines.\nMCQA and closed-book QA (CBQA) from Cross-\nFit (Ye et al., 2021) (see Table 5 for the full\nlist of tasks). We henceforth use MCQA and\nCBQA to refer to the CrossFit dataset collections\nrespectively. We use the implementations and\ntraining/test partitions from HuggingFace Datasets\n(Lhoest et al., 2021). We split each collection of\ndatasets into meta-training/test splits using 5-fold\ncross-validation—we partition each dataset collec-\ntion into 5 equal-sized subsets and run five versions\nof each experiment, one with each subset being\nused as the meta-test set and the remaining subsets\nused as meta-training data. We take the average of\nthe meta-test results as our final result.\nLLMs. We run our experiments on four LLMs:\nOPT-6.7B, OPT-13B (Zhang et al., 2022), LLaMA-\n7B, and LLaMA-13B (Touvron et al., 2023). We\nuse the OPT models from HuggingFace Models7\nand the LLaMA model from Meta AI. 8 More de-\ntails are included in Appendix C.1.\nExperimental Details. We generate prompts for\neach dataset using the method noted in Section 3.2.\nFor each dataset in MMLU,9 we combine the “val-\nidation” set and “dev”10 set to be the training set\n7https://huggingface.co/models\n8https://github.com/facebookresearch/llama\n9https://huggingface.co/datasets/cais/mmlu\n10The “dev” set contains 5 examples for each dataset is\nmeant for few-shot development purposes.\n9535\nMethods LLaMA-7B LLaMA-13B\nMMLU MCQA CBQA MMLU MCQA CBQA\nMeta Models\nMLP 7.06 ±1.03 8.82 ±2.64 6.32±2.13 5.80±0.63 11.04 ±3.30 8.08±2.99\n3-NN 5.98 ±0.97 5.62 ±2.35 7.10 ±2.15 5.50±0.66 11.26 ±4.56 8.56 ±2.85\nXGBoost 5.42 ±4.52 5.22±2.17 7.00±2.44 5.00±0.62 11.52±5.20 9.00 ±4.85\nBaselines\nAVGTRAIN 5.26±1.40 10.26±2.50 11.62 ±5.88 9.90±1.70 11.40 ±3.36 13.14 ±7.25\nAVGCONF 5.54±0.54 6.88 ±3.14 n/a 5.10±0.86 8.58±1.75 n/a\nTS 5.60 ±1.63 6.32 ±3.58 n/a 14.06±1.53 14.00 ±6.95 n/a\nATC 20.34 ±4.10 34.66 ±9.36 31.80 ±13.44 20.50±4.92 24.14 ±7.72 31.80 ±12.49\nOracle 5.82 (32) 6.44 ( 32) 7.06 ( 16) 6.18 (32) 13.44 ( 8) 10.82 ( 8)\nACC 31.08 ±6.32 39.00 ±10.52 23.7 ±10.44 45.50±11.74 50.34 ±12.84 29.30 ±12.38\nMethods OPT-6.7B OPT-13B\nMMLU MCQA CBQA MMLU MCQA CBQA\nMeta Models\nMLP 5.70 ±0.77 6.66 ±1.25 7.28 ±2.75 6.30±1.14 7.18 ±1.69 5.90 ±1.18\n3-NN 4.06 ±0.33 2.98 ±0.57 6.46±2.05 5.16±0.24 2.78 ±1.06 5.84±2.19\nXGBoost 3.76 ±0.32 2.60±0.73 8.06±3.07 4.54±0.35 2.54±1.16 6.00±2.43\nBaselines\nAVGTRAIN 3.48±0.37 10.66±1.75 8.38 ±3.86 4.28±0.35 11.46±2.15 18.00 ±3.77\nAVGCONF 8.42±0.60 13.42 ±1.70 n/a 6.20±0.77 7.14 ±1.77 n/a\nTS 3.92 ±0.17 4.60 ±0.80 n/a 4.36±0.31 2.72 ±1.03 n/a\nATC 24.54 ±2.12 37.64 ±7.93 32.16 ±14.60 24.72±3.32 29.40 ±10.28 30.34 ±12.48\nOracle 5.58 (32) 2.76 ( 128) 6.50 ( 16) 5.58 (32) 2.76 ( 128) 6.70 ( 16)\nACC 26.68 ±4.42 33.16 ±9.80 17.54 ±5.92 26.82±5.28 33.68 ±12.06 19.34 ±6.30\nTable 1: Evalutaion results (MAE) and variations (SD) for all 4 LLMs, 3 dataset collections settings, using\nconfidence vector as the meta-feature. For the Oracle baselines, we include the closest lower-bound. e.g., if the error\nis between oracle32 and oracle64, we put oracle32 (32). OPT-13B settings have the lowest average MAE (5.13),\ncompared to OPT-6.7B (5.28), LLaMA-7B (6.50), LLaMA-13B(8.41). We report overall accuracy (ACC) by Exact\nMatch for MMLU and MCQA, and F1-score for CBQA\nthat we sample in-context examples from. We sam-\nple 10 3-shot, 10 4-shot, and 10 5-shot prompts11\nand decorate each of them with 5 prompt tem-\nplates chosen for MMLU (see Table 3). We choose\ndconf = 20here since many of the datasets contain\nonly 100 text examples. For MCQA and CBQA,\nwe sample in-context examples from a pool of 100\nexamples as the training set, and obtain a test set\nof 1000 examples (see Section A in the appendix\nfor implementation details). For each dataset, we\nsample 30 3-shot and 30 4-shot prompts and deco-\nrate them with only the null template. We choose\ndconf = 100for both MCQA and CBQA settings.\nSection 5.3 contains more details about the abla-\ntion studies for dconf . Due to computational rea-\nsons, we sample only 30 prompts (compared to 60\n11We choose up to 5-shot setting because it is studied in\nprevious studies (Touvron et al., 2023; Rae et al., 2021).\nprompts for MCQA/CBQA datasets) for MMLU\nbecause it contains a very large number of datasets.\n5.2 Main Results\nThe meta-model outperforms all baselines un-\nder certain evaluation settings. Table 1 shows\nthe meta-model estimation error for each evaluation\nsetting. For 8 out of 12 settings (all CBQA settings,\nLLaMA-7B on MCQA, LLaMA-13B on MMLU,\nboth OPT models on MCQA), the best meta-model\narchitecture has 23.67% lower relative MAE than\nthe best baseline method on average. In the best\ncase (OPT-6.7B on MCQA), the meta-model can\nachieve 43.5% lower relative MAE than all base-\nlines. However, for the other 4 settings (both OPT\nmodels on MMLU, LLaMA-7B on MMLU, and\nLLaMA-13B on MCQA), baseline methods pro-\nvide more accurate estimates of ICL accuracy. Fig-\n9536\n0.2 0.4 0.6 0.8\npredicted accuracy\n0.2\n0.4\n0.6\n0.8actual accuracy\nLLaMA-13B MMLU\n0.2 0.3 0.4 0.5 0.6\npredicted accuracy\n0.2\n0.4\n0.6actual accuracy\nOPT-13B MMLU\n0.2 0.4 0.6 0.8\npredicted accuracy\n0.2\n0.4\n0.6\n0.8actual accuracy\nLLaMA-13B MCQA\n0.2 0.3 0.4 0.5 0.6\npredicted accuracy\n0.2\n0.3\n0.4\n0.5\n0.6actual accuracy\nOPT-13B MCQA\nFigure 3: We plot the meta-model predicted accuracy\nversus the actual accuracy across 4 settings. We use\nthe XGBoost meta-model and the confidence vector\nmeta-feature. Each entity represents an observation for\none dataset. Red/blue represents higher/lower absolute\nerror.\nure 2 shows the evaluation results graphically. On\naverage across all 12 settings, the best estimation\nerrors from the meta-models are 32.5% less than\nthe actual accuracy standard deviations. In 11 out\nof 12 settings, the estimation errors are within one\nstandard deviation of the actual accuracy.\nOracle baselines indicate useful accuracy esti-\nmations. In comparison to the Oracle baselines,\nthe meta-model outperforms the oracle32 base-\nline in all MMLU and MCQA settings except for\nLLaMA-13B on MCQA (achieves oracle8) and\noutperforms the oracle16 baseline in all CBQA set-\ntings except for LLaMA-13B (achieves oracle8).\nIn the two best-case settings (using XGBoost as the\nmeta-model on MCQA with either OPT model),\nthe meta-model achieves the oracle128 baseline,\ni.e., is equivalent to estimating the accuracy using\n128 annotations.\nBaseline methods are effective in some settings.\nWhile ATC is a weak baseline for ICL accuracy\nestimation, AVGTRAIN , AVGCONF , and TS are\nstrong baselines for MMLU and MCQA. AVG-\nTRAIN is able to achieve oracle32 for 3 out of\n12 settings (LLaMA-7B, OPT-6.7B, and OPT-13B\non MMLU); AVGCONF is able to achieve oracle32\nfor 2 settings (LLaMA-7B and LLaMA-13B on\nMMLU), and oracle16 for 3 settings (both LLaMA\nmodels on MCQA and OPT-13B on MMLU). Note\nthat temperature scaling improves calibration in\ncertain settings: TS is able to achieve oracle128 for\none setting (OPT-13B on MCQA), and oracle32\n200 400 600 800 1000\nnumber of unlabeled examples m\n10080604020\nconf dimensions dc\n3.9 3 2.9 2.6 2.5\n3.9 3.1 2.7 2.6 2.6\n4.1 3.1 2.7 2.7 2.6\n3.9 3 2.8 2.6 2.6\n4 3 2.8 2.7 2.5\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\nFigure 4: Estimation results for ablating the number of\nunlabeled examples m(x-axis) and confidence vector\ndimensions dc (y-axis), evaluated on the OPT-13B on\nMCQA using the XGBoost meta-model.\nfor 5 settings (LLaMA-13B on MMLU, both OPT\nmodels on MMLU, and OPT-6.7B on MCQA).12\nAblation on Model Architecture Across three\nmeta-model structures, the XGBoost meta-model\noverall provides the most accurate estimation as\nit has the lowest MAE for 7 out of 12 evalua-\ntion settings. The average MAE is 5.88 for XG-\nBoost meta-models, 5.94 for 3-NN meta-models,\nand 7.18 for MLP meta-models. Surprisingly, 3-\nNN meta-models have a lower average MAE than\nMLP meta-models despite having a simpler model\nstructure. In Figure 3, we show that the XGBoost\nmeta-model provides well-correlated accuracy esti-\nmation across 4 different evaluation settings.\nAblation on Featurization Methods We con-\nsider three featurization methods as described in\nSection 4.7. Table 2 in the appendix shows that the\nbest overall accuracy estimation for all settings is\nattained by using the confidence vectors as meta-\nfeatures (achieves the lowest MAE for 26 out of\n36 evaluation settings). The average MAE is 6.27\nfor conf, 8.34 for embed, and 7.36 for ce. Further,\nusing conf as features demonstrates a more domi-\nnant advantage for all CBQA tasks, achieving the\nlowest MAE for 11 out of 12 evaluation settings.\n5.3 Effect of Unlabeled Data and Confidence\nVector Dimensions\nWe now study confidence feature vector ablations\nby varying the number of unlabeled test examples\nmin each unseen dataset and the dimension of the\n12As noted in Section 4.6, the AVGCONF and TS baseline\nis not applicable to CBQA datasets.\n9537\nconfidence vector dconf . We test with OPT-13B on\nMCQA datasets using the XGBoost meta-model\nsince we achieve the lowest MAE in this setting.\nFigure 4 shows that increasing m enables better\naccuracy estimation, reducing the average MAE\n(across all dconf ) from 3.92 for m = 200to 2.56\nfor m= 1000. Note that increasing mrequires per-\nforming additional LLM inferences on unlabeled\nexamples, so leveraging unlabeled test data is con-\nstrained by computational cost considerations. The\nquality of our accuracy estimates does not vary\nmuch as we change the confidence vector dimen-\nsion dconf , as shown in Figure 4.\n5.4 Effect of Number of Shots\nWe compare ICL accuracy estimation performance\ngiven different k-shot ICL accuracy observations\nfor LLaMA-13B on MMLU datasets. Table 4 in\nthe Appendix shows that the meta-model produces\na slightly better ICL accuracy estimation for the\n3-shot setting. Overall, the meta-model gives con-\nsistent accuracy estimates across different k-shot\nsettings as they all achieve oracle32.\n5.5 Prompt Selection\nPrevious works demonstrated that ICL perfor-\nmance is highly sensitive to the prompt templates\nas well as in-context examples (Zhao et al., 2021;\nPerez et al., 2021; Chen et al., 2022); we are thus\ninterested in whether our ICL accuracy estimation\nmethod can be applied to select the best ICL prompt\nc∈Ctest for the test dataset. For each dataset, we\nuse the XGBoost meta-model to select the best\nprompt ˆc∗, as opposed to the actual best prompt\nc∗. We then compute the corresponding ICL accu-\nracies and compare them to the average accuracy\nacross all test prompts. Figure 5 shows that there is\na significant difference in ICL accuracy given dif-\nferent prompts for all 12 settings, and the selected\nprompts lead to better ICL accuracies than the av-\nerage accuracy for 7 out of 12 settings. On average,\nthe selected prompt is 15.6% as effective as the\nactual best prompt. The limited improvement from\nthe random baseline indicates there’s a large room\nfor improvement and we encourage future work to\nderive a better prompt selection standard.\n6 Discussion and Conclusion\nIn this paper, we study the problem of few-shot\nICL accuracy estimation. We propose training a\nmeta-model based on LLM confidence features and\nLLaMA-7B LLaMA-13B OPT-6.7B OPT-13B\nLLMs\n0.01\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nAverage ACC difference\nMMLU\nMCQA\nCBQA\nMMLU\nMCQA\nCBQA\nFigure 5: Prompt selection results for all evaluation\nsettings, measured by the absolute difference between\nICL accuracy when prompted with cand the average\naccuracy. Blue bars show the actual best prompt ( i.e.,\nc = c∗), and red bars show the selected best prompt\n(i.e., c= ˆc∗)\nobserved accuracies on seen datasets. We show that\nwithout using any labeled test data, the meta-model\nis often able to attain accurate estimates of ICL\naccuracy, which is practically useful for predicting\nLLMs’ accuracy on datasets that have high annota-\ntion costs. We also construct a large-scale bench-\nmark for dataset-level ICL accuracy estimation by\nevaluating the meta-model and multiple baseline\nmethods across 12 evaluation settings and 3 meta-\nfeature options. We observe that while some base-\nline methods can provide good accuracy estimates,\nour meta-model demonstrates non-trivial improve-\nment in estimation abilities over baseline methods\nin 8 out of 12 evalutation settings.\nWe encourage future work to develop better\nmeta-model architectures as well as better meta-\nfeatures and study potential implications for the\nmeta-model, such as acting as a prompt tem-\nplate/ICL example selection method. We believe\nthat our benchmark can serve as an open challenge\nfor improving dataset-level ICL accuracy estima-\ntions, leading to an improved understanding of\nwhen ICL is likely to be effective.\nLimitations\nWhile we conducted extensive experiments to study\nICL accuracy estimations, there are many more\nLLMs that have exhibited impressive capabilities\non a variety of tasks. Due to computational con-\nstraints, we do not benchmark accuracy estima-\ntions based on LLMs with limited access ( e.g.,\n9538\nGPT-4 (OpenAI, 2023)) as it is difficult to extract\nmodel embedding features, or those larger than\n13B. We also don’t consider instruction-tuned mod-\nels to avoid possible overlaps between their training\ndatasets and our evaluation datasets. Meanwhile,\ninstruction tuning sometimes hurts model perfor-\nmance on canonical datasets such as MMLU, as\nshown in Gudibande et al. (2023). It might also\nsignificantly hurt calibration as reported in OpenAI\n(2023). For the same reasons, we include only a\nlimited number of prompt templates and in-context\nexample variations for ICL prompting. While we\nchoose only 3 few-shot settings for MMLU and\n2 for MCQA and CBQA, it is possible to achieve\nbetter accuracy estimations with more observations\nin the training data.\nIn terms of dataset selection, we use 13 closed-\nbook QA tasks for the open-ended generation set-\nting. Our findings might not generalize to other\nopen-ended generation tasks such as summariza-\ntion or long-form question answering. Overall,\nthe meta-model provides effective accuracy estima-\ntions, but there’s still substantial room for improve-\nment.\nAcknowledgements\nWe would like to thank Ting-Yun Chang for her\nvaluable contributions. We thank Ameya Godbole,\nJohnny Wei, and Wang Zhu for their valuable dis-\ncussions. We also thank all members of the Al-\nlegro lab at USC for their support and valuable\nfeedback. RJ was supported by an Open Philan-\nthropy research grant, a Cisco Research Award, and\nHF was supported by the USC Provost Fellowship\nAward.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nTianqi Chen and Carlos Guestrin. 2016. Xgboost: A\nscalable tree boosting system. In Proceedings of\nthe 22nd acm sigkdd international conference on\nknowledge discovery and data mining , pages 785–\n794.\nYanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown,\nand He He. 2022. On the relation between sensitivity\nand accuracy in in-context learning. arXiv preprint\narXiv:2209.07661.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nShrey Desai and Greg Durrett. 2020. Calibra-\ntion of pre-trained transformers. arXiv preprint\narXiv:2003.07892.\nSaurabh Garg, Sivaraman Balakrishnan, Zachary C\nLipton, Behnam Neyshabur, and Hanie Sedghi.\n2022. Leveraging unlabeled data to predict\nout-of-distribution performance. arXiv preprint\narXiv:2201.04234.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang\nGeng, Hao Liu, Pieter Abbeel, Sergey Levine, and\nDawn Song. 2023. The false promise of imitating\nproprietary llms.\nDevin Guillory, Vaishaal Shankar, Sayna Ebrahimi,\nTrevor Darrell, and Ludwig Schmidt. 2021. Pre-\ndicting with confidence on unseen distributions. In\nProceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 1134–1144.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International conference on machine learn-\ning, pages 1321–1330. PMLR.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know when language\nmodels know? on the calibration of language models\nfor question answering. Transactions of the Associa-\ntion for Computational Linguistics, 9:962–977.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nAmita Kamath, Robin Jia, and Percy Liang. 2020. Se-\nlective question answering under domain shift. arXiv\npreprint arXiv:2006.09462.\n9539\nAnanya Kumar, Percy S Liang, and Tengyu Ma. 2019.\nVerified uncertainty calibration. Advances in Neural\nInformation Processing Systems, 32.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, et al. 2021. Datasets: A commu-\nnity library for natural language processing. arXiv\npreprint arXiv:2109.02846.\nZeju Li, Konstantinos Kamnitsas, Mobarakol Islam,\nChen Chen, and Ben Glocker. 2022. Estimating\nmodel performance under domain shifts with class-\nspecific confidence scores. In Medical Image Com-\nputing and Computer Assisted Intervention–MICCAI\n2022: 25th International Conference, Singapore,\nSeptember 18–22, 2022, Proceedings, Part VII, pages\n693–703. Springer.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2021. Fantastically ordered\nprompts and where to find them: Overcoming\nfew-shot prompt order sensitivity. arXiv preprint\narXiv:2104.08786.\nMahdi Pakdaman Naeini, Gregory F. Cooper, and Milos\nHauskrecht. 2015. Obtaining well calibrated prob-\nabilities using bayesian binning. In Proceedings of\nthe Twenty-Ninth AAAI Conference on Artificial In-\ntelligence, AAAI’15, page 2901–2907. AAAI Press.\nOpenAI. 2023. Gpt-4 technical report.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. Ad-\nvances in neural information processing systems ,\n34:11054–11070.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nChenglei Si, Chen Zhao, Sewon Min, and Jordan Boyd-\nGraber. 2022. Re-examining calibration: The case of\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n2814–2829.\nPrasann Singhal, Jarad Forristal, Xi Ye, and Greg Dur-\nrett. 2022. Assessing out-of-domain language model\nperformance from few examples. arXiv preprint\narXiv:2210.06725.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren.\n2021. Crossfit: A few-shot learning challenge for\ncross-task generalization in nlp. arXiv preprint\narXiv:2104.08835.\nYaodong Yu, Zitong Yang, Alexander Wei, Yi Ma,\nand Jacob Steinhardt. 2022. Predicting out-of-\ndistribution error with the projection norm. In In-\nternational Conference on Machine Learning, pages\n25721–25746. PMLR.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\n9540\nMMLU\nMethods LLaMA-7B LLaMA-13B OPT-6.7B OPT-13B\nMeta Models conf embed ce conf embed ce conf embed ce conf embed ce\nMLP 7.06 9.76 8.00 5.80 9.92 8.84 5.70 10.66 8.02 6.30 11.24 9.48\n3-NN 5.98 5.38 5.38 5.50 6.80 6.80 4.06 4.60 4.60 5.16 5.20 5.20\nXGBoost 5.42 4.52 4.56 5.00 7.30 5.16 3.76 3.94 3.76 4.54 4.88 4.72\nBaselines\nAVGTRAIN 5.26±1.40 9.90 ±1.70 3.48±0.37 4.28 ±0.35\nAVGCONF 5.54±0.54 5.10 ±0.86 8.42 ±0.60 6.20 ±0.77\nTS 5.60 ±1.63 14.06 ±1.53 3.92 ±0.17 4.36 ±0.31\nATC 20.34 ±4.10 20.50 ±4.92 24.54 ±2.12 24.72 ±3.32\nOracle 5.82 (32) 6.18 ( 32) 5.58 ( 32) 5.58 ( 32)\nACC 31.08 ±6.32 45.50 ±11.74 26.68 ±4.42 26.82 ±5.28\nMCQA\nMethods LLaMA-7B LLaMA-13B OPT-6.7B OPT-13B\nMeta Models conf embed ce conf embed ce conf embed ce conf embed ce\nMLP 8.82 12.50 10.62 11.04 16.86 14.64 6.66 13.00 10.30 7.18 14.54 13.72\n3-NN 5.62 6.22 6.22 11.26 11.24 11.24 2.98 3.84 3.84 2.78 5.60 5.58\nXGBoost 5.22 6.34 6.12 11.52 10.90 11.34 2.60 5.34 3.00 2.54 6.74 2.94\nBaselines\nAVGTRAIN 10.26±2.50 11.40 ±3.36 10.66 ±1.75 11.46 ±2.15\nAVGCONF 6.88±3.14 8.58 ±1.75 13.42±1.70 7.14 ±1.77\nTS 6.32 ±3.58 14.00 ±6.95 4.60 ±0.80 2.72 ±1.03\nATC 34.66 ±9.36 24.14 ±7.72 37.64 ±7.93 29.40 ±10.28\nOracle 6.44 (32) 9.30 ( 16) 2.76 ( 128) 2.76 ( 128)\nACC 39.00 ±10.52 50.54 ±13.86 33.42 ±11.58 33.68 ±12.06\nCBQA\nMethods LLaMA-7B LLaMA-13B OPT-6.7B OPT-13B\nMeta Models conf embed ce conf embed ce conf embed ce conf embed ce\nMLP 6.32 14.26 7.14 8.08 12.18 10.96 7.28 8.44 8.46 5.90 7.42 9.12\n3-NN 7.10 10.74 9.14 8.56 10.74 10.72 6.46 10.44 10.18 5.84 10.62 10.64\nXGBoost 7.00 13.10 8.56 9.00 11.60 10.64 8.06 7.58 7.74 6.00 8.54 7.74\nBaselines\nAVGTRAIN 11.62±5.88 13.14 ±7.25 8.38 ±3.86 18.00 ±3.77\nATC 31.80 ±13.44 31.80 ±12.49 32.16 ±14.60 30.34 ±12.48\nOracle 7.06 (16) 10.82 ( 8) 6.50 ( 16) 6.70 ( 16)\nACC 23.72 ±10.44 29.30 ±12.38 17.54 ±5.92 19.34 ±6.30\nTable 2: Full estimation results for all 4 LLMs, 3 dataset collections, and 3 meta-feature choices. XGBoost is the\nbest overall meta-model structure with an average MAE of 6.60. The confidence vector is the best overall feature\nwith an average MAE of 6.38 across all evaluation settings. The best meta-model outperforms all baseline methods\nin 9 out of 12 evaluation settings. We report overall accuracy (ACC) by Exact Match for MMLU and MCQA, and\nF1-score for CBQA\n9541\nA Dataset Implementation Details\nFollowing Section 5.1’s discussion of specific\ndataset implementations for MCQA and CBQA,\nour approach for each setting is: for tasks that have\na defined train/test split in HuggingFace Datasets,\nwe sample in-context examples from the first 100\nexamples in the training set and then choose the\nfirst 1000 examples from the test set to be our test\nquestions; while for other datasets that only have\nthe training set defined, we first sample a subset of\n100 examples from the first 80% of the dataset and\nthen sample in-context examples from the subset.\nThe test questions are obtained by sampling 1000\nexamples from the rest 20% of the dataset.\nB Prompt Templates\nWe collect 5 general prompt templates for MMLU\ndatasets: 1 Null template, 1 self-constructed\nprompt template, 2 from previous work (Rae et al.,\n2021; Hendrycks et al., 2020), and 1 generated by\nChatGPT. For MCQA and CBQA, we only use the\nNull template due to resource considerations.\nPrompt Template Source\nNull n/a\nYou are an expert in{subject}, here are Self-constructedsome multiple-choice questions\nThe following are multiple choice questionsHendrycks et al. (2020)(with answers) about{subject}\nA highly knowledgeable and intelligent AI answersRae et al. (2021)multiple-choice questions about{subject}\nTest your knowledge of{subject}with multiple ChatGPT-choice questions and discover the correct answers.\nTable 3: Prompt templates for MMLU datasets. {sub-\nject} is replaced by the dataset subsets ( e.g., ab-\nstract_algebra)\nC Implementation Details\nWe will release code to reproduce our results upon\npublication.\nC.1 LLMs Implementations\nWe use LLaMA-7B and LLaMA-13B (Touvron\net al., 2023) from Meta AI, and OPT-6.7B and OPT-\n13B (Zhang et al., 2022) from HuggingFace trans-\nformers. For LLaMA-7B, OPT-6.7B, and OPT-\n13B, we run evaluations using a single RTXA6000\nGPU (48GB). For LLaMA-13B, we run evaluations\nby parallel inference on two RTXA6000 GPUs. We\nuse half-precision for both OPT models. Note that\nsome LLMs (except for OPT-13B) can be run on\nGPUs with a smaller memory. We evaluate 42,360\nICL observations in total, where each observation\nis a dataset with 100 to 1000 examples. The total\ninference process takes around 2000 GPU hours.\nC.2 Meta-model Implementations\nAll meta-model architectures can be trained on an\ni7-10700 CPU. The total training time of the meta-\nmodel on one experiment setting varies from 1.5\nhours to 24 hours depending on training data dimen-\nsions. We include the implementation details for\neach of the meta-model architectures. We use the\nrandom seed 1 for all processes involving random-\nness. For K Nearest Neighbors regression , we\nuse the implementation of KNeighborsRegressor\nfrom sklearn library. We use euclidean distance\nas the weight metric, and fit the model on the meta-\ntraining data.\nFor MLP, we implement with Pytorch. We\nuse a 2-layer MLP with the size of the\nhidden_state= 1536, learning_rate = 1e−5,\nand dropout_rate = 0.2. We use Adam Opti-\nmizer and MSELoss, and we perform early stop-\nping with the validation data. The validation data\nis a 20% random partition of the meta-training data.\nFor early stopping, the max epoch is 50 and the\npatience is 7.\nFor XGBoost, we implement with the\nXGBRegressor from the XGBoost library(Chen\nand Guestrin, 2016). We use a 5-fold random\nsearch cross-validation for 300 iterations to choose\nthe hyperparameter. The candidates are:\n1 {\n2 \"lr\": uniform (0.01, 0.5),\n3 \" max_depth \": randint (3,10),\n4 \" n_estimators \": randint (100, 1000),\n5 \" colsample_bytree \": [0.5, 0.6, 0.7, 0.8, 0.9, 0.\n95, 1],\n6 \" subsample \": [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1],\n7 \" gamma \": uniform (0,1),\n8 \" reg_alpha \": uniform (0,1),\n9 \" reg_lambda \": uniform (0,1)\n10 }\nC.3 Other Implementation Details\nOutput Collection For multiple-choice QA tasks\n(MMLU and MCQA), we collect generated choice\nlabels (e.g., “(A)”) from the first 5 tokens generated.\nFor closed-book QA tasks (CBQA), we collect the\nfirst 16 newly generated tokens as the model output\nand truncate the outputs by the newline token.\nTemperature Scaling We search for the optimal\ntemperature τ based on the meta-training set. The\nsearch grid is:\n9542\nnp.linspace(1.0, 3.0, 100)\nD Few-shot setting ablation results\nMethods 3-shot 4-shot 5-shot mixed\nMeta Models\nconf conf conf conf\nMLP 5.54 5.92 5.90 5.80\n3-NN 5.34 5.80 5.92 5.50\nXGBoost 4.86 5.34 5.40 5.00\nBaselines\nAvg 9.78 9.92 10.16 9.90\nACE 5.00 5.10 5.18 5.10\nATC 20.10 20.44 20.68 20.50\nOracle 6.14 (32) 6.14 ( 32) 6.12 ( 32) 6.18 ( 32)\nACC 45.50±11.44 45.36±11.26 45.52±12.00 45.50±11.74\nTable 4: Estimation results for separate and mixed\nfew-shot settings, measured by MAE and tested on the\nLLaMA-13B on the MMLU setting. The accuracy esti-\nmation is consistent across different shot settings. We\nreport the accuracy (ACC) as Exact Match accuracy\n9543\nDataset Collection Datasets\nMMLU \"abstract_algebra\", \"anatomy\", \"astronomy\", \"college_biology\"\n\"college_chemistry\", \"college_computer_science\", \"college_mathematics\", \"college_physics\"\n\"computer_security\", \"conceptual_physics\", \"electrical_engineering\", \"elementary_mathematics\"\n\"high_school_biology\", \"high_school_chemistry\", \"high_school_computer_science\"\n\"high_school_mathematics\", \"high_school_physics\", \"high_school_statistics\"\n\"machine_learning\", \"high_school_government_and_politics\" , \"high_school_geography\"\n\"econometrics\", \"high_school_macroeconomics\", \"high_school_microeconomics\", \"sociology\"\n\"high_school_psychology\", \"human_sexuality\", \"professional_psychology\", \"public_relations\"\n\"security_studies\", \"us_foreign_policy\", \"formal_logic\", \"high_school_european_history\"\n\"high_school_us_history\", \"high_school_world_history\", \"international_law\", \"jurisprudence\"\n\"logical_fallacies\", \"moral_scenarios\", \"philosophy\", \"prehistory\", \"professional_law\"\n\"world_religions\", \"business_ethics\", \"clinical_knowledge\", \"college_medicine\", \"global_facts\"\n\"human_aging\", \"management\", \"marketing\", \"medical_genetics\", \"miscellaneous\", \"nutrition\"\n\"moral_disputes\", \"professional_accounting\", \"professional_medicine\", \"virology\"\nMCQA \"wiqa\", \"wino_grande\", \"swag\", \"superglue-copa\", \"social_i_qa\"\n\"race-middle\", \"quartz-with_knowledge\", \"quartz-no_knowledge\", \"quarel\", \"qasc\"\n\"openbookqa\", \"hellaswag\", \"dream\", \"cosmos_qa\", \"commonsense_qa\"\n\"ai2_arc\", \"codah\", \"aqua_rat\", \"race-high\", \"quail\", \"definite_pronoun_resolution\"\nCBQA \"squad-no_context\", \"numer_sense\", \"kilt_trex\", \"kilt_zsre\", \"lama-trex\"\n\"lama-squad\", \"lama-google_re\", \"lama-conceptnet\", \"kilt_hotpotqa\"\n\"kilt_nq\", \"freebase_qa\", \"web_questions\", \"jeopardy\"\nTable 5: We list 3 dataset collections: MMLU, MCQA, and CBQA. There are 57 datasets in MMLU, 21 datasets in\nMCQA, and 13 datasets in CBQA.\n9544\nTask Examples Labels\nMMLU What is the second most common element in the solar system?\n(A) Iron\n(B) Hydrogen\n(C) Methane\n(D) Helium\nAnswer: (D) Helium\nOn which planet in our solar system can you find the Great Red Spot?\n(A) Venus\n(B) Mars\n(C) Jupiter\n(D) Saturn\nAnswer: (C)\nCBQA Who are the members of Aespa?\nAnswer: Karina, Giselle, Winter, and Ningning\nWho is the first overall pick in the 2011 NBA draft?\nAnswer: Kyrie Irving\nTable 6: Two 1-shot examples decorated with a default null prompt template for MMLU datasets and CBQA dataset.\nWe take the same prompt template for MCQA as MMLU.\n9545\nFigure 6: 4-shot ICL accuracy for OPT-13B and LLaMA-13B on CBQA tasks, where each boxplot summarizes\nthe F1 scores over 30 ICL prompt variations of one dataset. Both OPT-13B and LLaMA-13B have large variances\nacross different tasks, showing the challenge of ICL accuracy estimation.\n9546"
}