{
  "title": "Sorting through the noise: Testing robustness of information processing in pre-trained language models",
  "url": "https://openalex.org/W3202546170",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3204041734",
      "name": "Lalchand Pandia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2404566579",
      "name": "Allyson Ettinger",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2921890305",
    "https://openalex.org/W2964267515",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W3127227595",
    "https://openalex.org/W2125436846",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W2962961857",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W2167525509",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3035160860",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W2963963993",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2692059227",
    "https://openalex.org/W3103816537",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2582752926",
    "https://openalex.org/W2738015883",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2137002739",
    "https://openalex.org/W2963969878"
  ],
  "abstract": "Pre-trained LMs have shown impressive performance on downstream NLP tasks, but we have yet to establish a clear understanding of their sophistication when it comes to processing, retaining, and applying information presented in their input. In this paper we tackle a component of this question by examining robustness of models' ability to deploy relevant context information in the face of distracting content. We present models with cloze tasks requiring use of critical context information, and introduce distracting content to test how robustly the models retain and use that critical information for prediction. We also systematically manipulate the nature of these distractors, to shed light on dynamics of models' use of contextual cues. We find that although models appear in simple contexts to make predictions based on understanding and applying relevant facts from prior context, the presence of distracting but irrelevant content has clear impact in confusing model predictions. In particular, models appear particularly susceptible to factors of semantic similarity and word position. The findings are consistent with the conclusion that LM predictions are driven in large part by superficial contextual cues, rather than by robust representations of context meaning.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1583–1596\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1583\nSorting through the noise: Testing robustness of information processing in\npre-trained language models\nLalchand Pandia\nlcpandia@gmail.com\nAllyson Ettinger\nDepartment of Linguistics\nUniversity of Chicago\naettinger@uchicago.edu\nAbstract\nPre-trained LMs have shown impressive per-\nformance on downstream NLP tasks, but we\nhave yet to establish a clear understanding of\ntheir sophistication when it comes to process-\ning, retaining, and applying information pre-\nsented in their input. In this paper we tackle\na component of this question by examining ro-\nbustness of models’ ability to deploy relevant\ncontext information in the face of distracting\ncontent. We present models with cloze tasks\nrequiring use of critical context information,\nand introduce distracting content to test how\nrobustly the models retain and use that critical\ninformation for prediction. We also systemati-\ncally manipulate the nature of these distractors,\nto shed light on dynamics of models’ use of\ncontextual cues. We ﬁnd that although models\nappear in simple contexts to make predictions\nbased on understanding and applying relevant\nfacts from prior context, the presence of dis-\ntracting but irrelevant content has clear impact\nin confusing model predictions. In particular,\nmodels appear particularly susceptible to fac-\ntors of semantic similarity and word position.\nThe ﬁndings are consistent with the conclusion\nthat LM predictions are driven in large part by\nsuperﬁcial contextual cues, rather than by ro-\nbust representations of context meaning.\n1 Introduction\nIn recent years, pre-trained language models (LMs)\nhave taken NLP by storm. As we work to inter-\npret the impressive performance of these models, a\npersistent question is the extent to which LMs are\ndoing something like language “understanding”.\nDo these models robustly extract the nuances of\ninformation conveyed in text, or is their strong per-\nformance driven by more superﬁcial mechanisms?\nIn this paper we focus on examining a particular\naspect of “understanding” in pre-trained LMs: how\nrobustly these models process, retain, and apply\nnew facts presented in their inputs. We assume that\na fundamental aspect of language “understanding”\nwill be the capacity to represent and accumulate\ninformation from the meaning of the input text. So\nif we input “Sebastian lives in France, and Rowan\nlives in Indonesia.”, we would expect a model that\nunderstands language to form (or update) repre-\nsentations for these imaginary entities Sebastian\nand Rowan, such that those representations contain\nthe information that Sebastian lives in France, and\nthat Rowan lives in Indonesia. While recent work\nhas studied models as knowledge bases, testing\ntheir ability to reproduce facts encountered during\ntraining, in this paper we are asking an importantly\ndifferent question—not about what models mem-\norize during training, but about how sophisticated\nthey are in processing and representing information\nfrom new input text after training.\nTo test LMs’ ability to process and retain infor-\nmation from context, we design cloze tasks that in-\ncorporate a piece of critical information in context,\nand then prompt the model to complete a statement\nrelated to that information. To test the robustness\nof the processes informing these predictions, we\nintroduce distracting but irrelevant content in the\ncontexts, and test whether the models maintain cor-\nrect predictions in the face of these distractions.\nAdditionally, to explore further the nature of mech-\nanisms informing model predictions, we systemat-\nically vary the nature of the distracting content—\nmanipulating how many distractor words we use,\nhow semantically related they are to critical words,\nand their relative positions in the sentence.\nWe apply these tests to a range of recent pre-\ntrained LMs and examine the impacts of our ma-\nnipulations on model performance. The results in-\ndicate clearly that distracting content in the context\nis effective in undermining model predictions, and\nvariation of distractor types suggests that models\nare particularly sensitive to inﬂuences of semantic\nsimilarity and relative word position. Overall, the\nresults support the conclusion that predictions in\npre-trained LMs are driven in large part by super-\n1584\nﬁcial contextual cues, rather than by robust repre-\nsentations of relevant facts from context. We make\nall data and code available for further testing.1\n2 Related Work\nPrior work has tested LMs as knowledge bases us-\ning cloze-style probes (Petroni et al., 2019; Jiang\net al., 2020). As a starting point we rely on models’\nability to display this type of knowledge, but our\nquestion differs importantly from that work: we are\nnot asking whether models can recall facts about\nthe real world from training—rather, we are trying\nto gauge the extent to which models form robust\nrepresentations of new information presented in in-\nput after training. Somewhat more similar to ours\nis work like Elazar et al. (2021), which explores\nthe consistency of models’ generation of facts in\nthe face of rephrasing of prompts. The basic intu-\nition behind this work—that LMs’ ability to make\nintelligent-looking predictions can be sensitive to\nthe particulars of the context—is one that we also\nuse as we ask more speciﬁc questions about mod-\nels’ processing of information in their input.\nA good deal of prior work has focused on test-\ning for linguistic knowledge in language models\n(Rogers et al., 2020). Much of this work has\nprioritized testing syntax in pre-trained LMs via\nagreement tests (Linzen et al., 2016; Gulordava\net al., 2018). Others expand to broader sets of\nsyntactic phenomena (Wilcox et al., 2018; Futrell\net al., 2019; Warstadt et al., 2020) and seman-\ntic/pragmatic phenomena (Ettinger, 2020). Other\nwork has studied syntactic and semantic informa-\ntion in contextualized embeddings from these mod-\nels (Hewitt and Manning, 2019; Tenney et al., 2018;\nKlafka and Ettinger, 2020). We take one step up\nfrom examination of these abstract linguistic capac-\nities, with a focused examination of models’ ability\nto use such linguistic scaffolding to process and\nretain new information described in text.\nOur use of attractors to test model robustness\ntakes inspiration from use of attractors within syn-\ntactic testing contexts (Linzen et al., 2016; Gulor-\ndava et al., 2018), but we focus on semantic rela-\ntionships in deﬁning attractors, and use the attrac-\ntors to investigate different aspects of models’ pro-\ncessing. Some scattered work has explored more se-\nmantic types of attractors for testing LMs—in par-\nticular, there is work looking at whether presence\n1https://github.com/lalchand-pandia/\nSorting-Through-The-Noise\nof certain context words will prime corresponding\ntargets in context. Such work has experimented\nwith contextual factors like distance between prime\nand target (Kassner and Schütze, 2020), as well as\ncontextual constraint (Misra et al., 2020). We build\non this existing work with a more systematic ex-\nploration of impacts of different types of attractors,\nand with a more targeted goal of testing models’\nrobustness in processing new facts from context.\nIn focusing on models’ ability to extract, retain,\nand deploy information conveyed in text, our work\nalso relates to tasks in reading comprehension ques-\ntion answering (Rajpurkar et al., 2018; Ko ˇcisk`y\net al., 2018; Mostafazadeh et al., 2017; Yang et al.,\n2018; Richardson et al., 2013). Some such work,\nlike the bAbI dataset (Weston et al., 2016) and\nCBT (Hill et al., 2016), use insertion of additional\nmaterial to make the tasks generally more difﬁcult—\na tactic that also parallels the related method of ad-\nversarial testing (Jia and Liang, 2017; McCoy et al.,\n2019; Nie et al., 2020). There are important simi-\nlarities in the questions and strategies of these prior\nworks and ours, but we differ in focusing speciﬁ-\ncally on information processing in LMs, rather than\nperformance of models supervised for a particular\ndownstream task. Unlike those works, our goal\nis to shed light on robustness of language “under-\nstanding”, and nature of prediction mechanisms,\nthat arise as a result of LM-based pre-training.\n3 Methods\nWe design our tests in the form of cloze tasks, so\nas to test the pre-trained LMs in their most natural\nsetting, without interference from ﬁne-tuning.\nWe start from a simple base context, in which\nthe model is given a background fact about an\nimaginary entity, and then is asked to complete a\nrelated statement about the entity. For instance:\nSebastian lives in France. The capital of Se-\nbastian’s country is[MASK]\nWe will refer to “France” here as the criti-\ncal background word, and the correct completion\n“Paris” as thetarget word. For all of our test items,\nwe establish a baseline competence in our tested\nmodels, such that all models successfully prefer\nthe correct target completion over a set of closely\nrelated completions (to be outlined shortly) within\nthis simple base context. In this way, we establish\nthat the models have the relevant “world knowl-\n1585\nBase context\nZero attractor Sebastian lives in France. The capital of Sebastian’s country is ____\nMultiple-entity attractor setting\nB-type attractors Sebastian lives in France, Rowan lives in Indonesia, and Daniel lives in Chile.\nThe capital of Sebastian’s country is ____\nT-type attractors Sebastian lives in France, Rowan lives in Jakarta, and Daniel lives in Santiago.\nThe capital of Sebastian’s country is ____\nUnrelated attractors Sebastian lives in France, Rowan drives a car, and Daniel writes poetry. The\ncapital of Sebastian’s country is ____\nSingle-entity attractor setting\nB-type attractors Sebastian lives in France, and has visited Indonesia and Chile. The capital of\nSebastian’s country is ____\nT-type attractors Sebastian lives in France, and has visited Jakarta and Santiago. The capital of\nSebastian’s country is ____\nUnrelated attractors Sebastian lives in France, drives a car, and writes poetry. The capital of\nSebastian’s country is ____\nTable 1: Example items from dataset. For both multiple-entity and single-entity attractor settings, we give examples\nin the two-attractor condition—but note that the full dataset varies number of attractors from zero to three. B-type\nattractors refer to attractors in the same semantic class as the critical background word, and T-type attractors refer\nto attractors in the same semantic class as the target word.\nedge” for this prediction—and then we set aside\nthe issue of world knowledge, to focus on examin-\ning robustness of information processing.\nOf course, if a model is able to predict “Paris”\nin this base example, this could be attributable to a\nnumber of causes. On one hand, it could be taken\nas evidence that the model was able to store a repre-\nsentation of Sebastian as a resident of France, and\nthen when queried about a related statement, the\nmodel was able to make use of that stored infor-\nmation to generate a correct prediction. Alterna-\ntively, there may be more superﬁcial explanations\nfor the model’s success in this completion: for in-\nstance, the model may simply be reacting to the fact\nthat “France” was recently mentioned, and now the\nprompt is asking for a capital. What if “Indonesia”\nhad also been mentioned? Would the model still\nrecognize that “Paris” is the correct completion?\nTo tease apart these classes of explanation, we\nintroduce distracting content in the sentences, and\ntest how this content impacts models’ outputs. Fol-\nlowing the number agreement literature (Linzen\net al., 2016), we refer to these inserted items as\nattractors. In a system that robustly represents and\nretains the critical background information from\ncontext, attractor content should not prevent the\nmodel from continuing to prefer the correct target\ncompletion. If the attractor content does change the\nmodels’ preferences, then we can infer that more\nsuperﬁcial predictive mechanisms are likely at play.\n3.1 Attractor manipulations\nBeyond simply testing whether the model can be\ndistracted from giving a correct prediction, we also\nvary the nature of the attractors so as to better under-\nstand the speciﬁc mechanisms underlying model\npredictions. We start by selecting attractors with a\nsemantic relation either to the critical background\nword (e.g., another country), or to the target word\n(e.g., another capital). We refer to these as B-type\nand T-type attractors, respectively. These semanti-\ncally related attractors allow us to test the hypoth-\nesis that models rely on coarse-grained semantic\nsimilarities to inform predictions. If this is the case,\nthen we expect the presence of irrelevant but seman-\ntically related material to be particularly disruptive\nto models’ predictions. To contrast with the seman-\ntically related attractors, we also include unrelated\nattractors that are not semantically related to the\ncritical background fact or the target.\nWe present each of these attractor types in two\nforms. In the ﬁrst, attractors are listed as addi-\ntional properties of the key entity (that is, the entity\ninvolved in the critical background fact). This al-\nlows us to test whether models can sort through\ndifferent facts about an entity and retrieve the rel-\n1586\nevant fact for prediction. This is the single-entity\nsetting. In the second form, attractors are each as-\nsociated with a different entity, allowing us to test\nwhether models can form and sort through different\nentity/property links, to retrieve the relevant fact\nfor prediction. This is the multiple-entity setting.\nTable 1 shows examples of the attractor types\nand conditions. While the table shows only exam-\nples with two attractors, we also vary the number of\nattractors so as to examine the impact of including\nmore versus less distracting content in context. We\nvary the number of attractors from zero to three.\n3.2 Dataset construction\nBecause our manipulations require control of nu-\nmerous variables, we generate our test items syn-\nthetically. Synthetic data has limitations, of course,\nbut it also has the important advantage of allowing\nfull control over the nature of the items, so we con-\nsider it an important complementary approach to\nuse of naturally-occurring datasets. All sentences\ngenerated for our dataset are in English.\nThe strongest constraint on our design of these\nitems was the need for sets of strongly linked,\npaired components (e.g., countries and capitals).\nWe need paired items for generating our prediction\ntasks—for instance, in Table 1, we rely on the rela-\ntionship between countries and capitals—and the\nrelationship in each pair must be strong enough that\nall models make successful predictions in the base\ncontext. Furthermore, we need sets of such pairs\nso as to insert semantically related attractors—of\nthe same type as the critical background word or\nthe target word—in the contexts.\nWe identify four item sets that meet our criteria:\ncountries and capitals, professions and associated\nobjects, monuments and associated countries, and\nsports and associated scoring metrics. We then\ncreate templates to support predictions for each of\nthe sets. We test various phrasings for our base\ncontexts, and select those that show optimal per-\nformance across models (c.f. Jiang et al., 2020).2\nIn keeping with prior LM analysis literature, we\ndeﬁne successful prediction in relative terms: mod-\nels are considered successful on an item if in the\nbase context they assign higher probability to the\ncorrect target completion over any of the other tar-\nget words in the same set (e.g., when “Paris” is the\ncorrect target, models are considered successful if\n2For example, Sebastian works as a ﬂorist . For his job,\nSebastian sells [MASK] is a better query than Sebastian is a\nﬂorist . For his job, Sebastian sells [MASK]).\nthey prefer “Paris” over any other capitals in the\nset). Appendix Table 2 lists all of the items from\nour sets, along with their selected base contexts.\nTo construct the remainder of the dataset, we\nstart with the base contexts and then sample from\nattractors of the appropriate types and sets. For\nadditional variety, we select randomly from a sam-\nple of six entity names, and we also insert variable\namounts of additional semantically unrelated ma-\nterial (sang in a choir, has a sister , etc) between\nthe key entity and critical background fact. In all,\nthe dataset includes 40,928 items. In semantically\nrelated attractor conditions, multi- and single-entity\nsettings each have 12,896 instances, and in seman-\ntically unrelated attractor conditions, multi- and\nsingle-entity settings each have 7,568 instances.\n4 Experiments\n4.1 Models\nWe apply our tests to examine three classes of pre-\ntrained LMs, testing various size settings within\neach class. For the models analyzed in this paper,\nwe use the implementation of Wolf et al. (2020).\nBERT (Devlin et al., 2019) We experiment with\ntwo variants: BERTBASE (110M parameters), and\nBERTLARGE (340M parameters). For both, we use\nthe uncased version.\nRoBERTa (Liu et al., 2019) We experiment\nwith RoBERTa BASE (125M parameters) and\nRoBERTaLARGE (355M parameters).\nGPT-2 (Radford et al., 2019) We test\nGPT2SMALL (117M parameters), GPT2 MEDIUM\n(345M parameters), GPT2 LARGE (774M parame-\nters) and GPT2XL (1558M parameters).\n4.2 Input representation\nFor our inputs, we add a start of sentence token\n([CLS] for BERT and<s> for RoBERTa and GPT2).\nThe two sentences of a given item are separated\nby a separator token, and the ﬁnal masked word\nis denoted by [MASK] for BERT and <mask> for\nRoBERTa. GPT2 does not require a masked to-\nken. The special tokens are chosen based on the\nimplementation of Wolf et al. (2020).\n5 Results\nWe begin by examining model prediction accuracy\nwhen attractors are semantically related to the criti-\ncal background word or the target word. We deﬁne\n1587\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nMultiple Entity\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nSingle Entity\nBertBase BertLarge RobertaBase RobertaLarge GPT2Small GPT2Medium GPT2Large GPT2XL\nFigure 1: Accuracy by number of attractors, with semantically related attractor type\naccuracy as percentage of instances in which mod-\nels assign higher probability to the correct target\nthan to any alternatives in the corresponding se-\nmantic set. This serves as the most direct test of\nthe anticipated potential impact of our semantically\nrelated attractors, as it directly assesses whether\npresence of an irrelevant word (e.g., “Indonesia”),\nwhich is semantically related to the critical back-\nground or target, and which invites a competing\nprediction (“Jakarta”), will cause models to prefer\nthat competing prediction over the correct target.\nFigure 1 breaks down accuracy by number of\nattractors, for multiple- and single-entity settings.\nFor the sake of space, we merge B-type and T-type\nattractors, which show largely similar patterns. 3\nWe see in Figure 1 that addition of just a single\nsemantically related attractor has clear impact on\nmodel performance, with models preferring the cor-\nrect completion substantially less often than when\nno such attractors are present. RoBERTa LARGE\nshows the strongest resistance to this effect, but\nstill shows clear disruption from the ﬁrst attractor.\nAs we insert additional attractors, for both condi-\ntions we see that rather than further hindering per-\nformance, for a couple of models accuracy actually\nimproves. While we are not certain what drives this\npattern, we speculate that a possible cause could be\nthat models may learn to pay less attention to con-\ntent that takes the form of lists. This is consistent\nwith the fact that improvement with more attractors\nis mitigated in the multiple-entity setting, when at-\n3See Appendix Figures 6-7 for B-type and T-type results.\ntractors take the form of more complex statements,\nrather than lists of single words.\n5.1 Impact on probabilities\nSince our deﬁnition of accuracy doesn’t indicate\nimpact of attractors on absolute target probabilities,\nwe also examine how target probabilities change\nfrom base contexts to attractor contexts. We\ncalculate relative probability as in Eq. 1, where w\nis the candidate target word, cattr is the relevant\nattractor context, and cbase is the base context:\nP(w|cattr)\nP(w|cbase) (1)\nFigure 2 shows these relative probabilities aggre-\ngated by number of attractors. 4 Consistent with\nthe accuracy results, we see that in both settings,\naddition of just one semantically related attractor\ncauses a dramatic drop in probability of the target\nrelative to its base context level. This effect is espe-\ncially uniform in the single-entity setting—in the\nmultiple-entity setting, GPT2SMALL shows less dra-\nmatic impacts with the ﬁrst attractor. Also in keep-\ning with the accuracy results, addition of further\nattractors does comparatively little damage beyond\nthat of the ﬁrst attractor, with relative probabilities\nremaining fairly stable with more attractors in the\nsingle-entity setting, and continuing to reduce, but\nvery gradually, in the multiple-entity setting.\n4Values in the zero-attractor condition are not always 1\nbecause the zero-attractor condition includes variants of the\nbase context using the additional, semantically unrelated inter-\nvening material described in Section 3.2.\n1588\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Relative_Prob\nMultiple Entity\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Relative_Prob\nSingle Entity\nBertBase BertLarge RobertaBase RobertaLarge GPT2Small GPT2Medium GPT2Large GPT2XL\nFigure 2: Relative probability by number of attractors, with semantically related attractor type\n5.2 Semantically unrelated attractors\nThe prior sections show that semantically related at-\ntractors, even if they are irrelevant for a prediction,\nhave clear impacts on model outputs. To what ex-\ntent is this effect driven by the relationship between\nthe attractors and the critical sentence components?\nIn this section we show results of adding attractors\nthat are meaningful, but unrelated to the critical\nbackground word or the target. Examples are given\nin Table 1, and more in Appendix Table 3.\nOur above deﬁnition of accuracy becomes less\nrelevant with unrelated attractors, since accuracy\nwas deﬁned in terms of competition within seman-\ntic sets. Appendix Figure 8 shows accuracy results\nfor semantically unrelated items, and conﬁrms that\nmodels remain at very high accuracy when attrac-\ntors are not semantically related to our chosen sets.\nNotably, however, there is a gradual but non-trivial\ndrop in accuracy for most models, suggesting that\nin some cases even these semantically unrelated\nattractors are enough to confuse models into prefer-\nring an incorrect, semantically related completion.\nMore appropriate to examine here are the im-\npacts of the semantically unrelated attractors on\nmodels’ target probabilities relative to the base\nprobability. These are shown in Figure 3. We see\nthat unrelated attractors do have non-trivial impact\non target probabilities, with fairly smooth reduc-\ntion in relative probability as more attractors are\nadded. This suggests that whether or not attractors\nare semantically related to the critical background\nfact or target, their presence still affects models’\nconﬁdence about the correct target, despite the fact\nthat these attractors are irrelevant to the prediction.\nImportantly, a key difference that we see be-\ntween semantically related attractors and semanti-\ncally unrelated attractors is in the dramatic dip—in\nboth accuracy and relative probability—with addi-\ntion of the ﬁrst semantically related attractor. This\ndip is missing when attractors are semantically un-\nrelated to the critical background or target. It seems,\nthen, that when the context contains an attractor oc-\ncupying a similar location in the semantic space rel-\native to facts being invoked for prediction, models\nare highly sensitive to even a single such item, and\nmodels’ ability to make correct predictions is signif-\nicantly hindered. Subsequent semantically related\nitems show greatly diminished impacts, suggesting\nthat addition of a single semantically related word\nachieves roughly ceiling impact on predictions. By\ncontrast, if instead the distracting material occu-\npies a more distant position in the semantic space,\nthis material can still hinder models’ predictions,\nbut it does so less dramatically. The continually\nincreasing impact with larger numbers of unrelated\nattractors also suggests that unlike the effects of\nsemantically related attractors, the effects of seman-\ntically unrelated attractors on prediction are more\ngradual and additive in nature.\nTaken together, the results presented in these\nsections suggest that model predictions are signif-\nicantly informed by superﬁcial contextual cues,\nrather than by robust representations based on\nmeaning of prior context. Differences between at-\ntractor types furthermore suggest that models rely\n1589\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Relative_Prob\nMultiple Entity\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Relative_Prob\nSingle Entity\nBertBase BertLarge RobertaBase RobertaLarge GPT2Small GPT2Medium GPT2Large GPT2XL\nFigure 3: Relative probability by number of attractors, with semantically unrelated attractor type\nheavily on coarse-grained semantic similarity cues\nto identify relevant context words for prediction.\n6 Varying position of information\nIn this section we explore the effects of further\nvarying entity and attractor position, to better un-\nderstand how the models utilize positional cues.\n6.1 Separating key entity and critical fact\nIn the previous section, all attractors occur after\nthe critical background fact has been stated. What\nwill happen if attractors fall between the key entity\nand the critical fact about that entity? Inserting\nattractors between these two components allows\nus to test the hypothesis that models might rely on\nproximity between the key entity and the critical\nfact in order to form a link (albeit, based on the\nabove results, a brittle link) between the two. We\nuse the same attractor sets, and simply adjust our\ntemplates to change the position of the attractors.\nWe focus on semantically related attractors for this\nsection. Appendix Table 4 gives some examples.\nThe results of this analysis are shown in Figure 4.\nOn the whole, the patterns are quite similar to when\nattractors occur after the critical background fact.\nIn both single- and multiple-entity settings we see\nthe clear dip in accuracy after a single attractor.\nThe multiple-entity setting produces a bit more\nspread between models, suggesting in particular\nthat RoBERTaLARGE thrives—in fact, improves—\ndespite the key entity being separated from the crit-\nical fact, while GPT2MEDIUM performs very poorly.\nWe also see that many models improve with more\nattractors, suggesting again some effect in which\nmodels may learn to down-weight content in lists.\nOn the whole, the results suggest that regardless\nof whether attractors intervene between the key\nentity and key fact, or between the key fact and\nthe target position, outcomes are similar: just a\nsingle semantically related attractor in the context\nwill signiﬁcantly disrupt models’ ability to make\na correct prediction. These results also suggest\nthat models don’t put heavy reliance on proximity\nbetween the key entity and the critical fact. While\nthis could suggest that the models are robust in\nforming entity-fact links, it could also indicate that\nthe models aren’t really forming those links at all.\n6.2 Varying key entity position\nIn all of our test items up to this point, the key en-\ntity has also been the ﬁrst entity mentioned. In this\nsection we test the impact of prompting a predic-\ntion about an entity that is not the ﬁrst mention. We\ndo this by taking our existing multiple-entity items,\nand adapting them so that the entity queried at the\ntarget position is one of the later-mentioned entities,\nrather than the ﬁrst-mentioned entity (e.g. Sebas-\ntian lives in France, and Rowan lives in Indonesia.\nThe capital of Rowan’s country is[MASK]).\nFigure 5 shows the results. We see that, with the\nexception of GPT2SMALL, the dramatic dip on the\nﬁrst attractor is no longer present. Instead, we see\nsteady decrease in accuracy with more attractors.\nWhat causes this change? The major difference\nin the one-attractor condition here, relative to our\nprevious settings, is that the key entity and critical\n1590\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nMultiple Entity\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nSingle Entity\nBertBase BertLarge RobertaBase RobertaLarge GPT2Small GPT2Medium GPT2Large GPT2XL\nFigure 4: Accuracy by number of attractors, with attractors intervening between key entity and critical fact\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nBertBase BertLarge RobertaBase RobertaLarge GPT2Small GPT2Medium GPT2Large GPT2XL\nFigure 5: Accuracy by number of attractors, with key entity occurring later in sentence\nbackground fact now both occur closer to the target\nposition than the attractor does. Thus, the observa-\ntion that this more distant attractor has less effect\non predictions suggests that in addition to semantic\nsimilarity, model predictions are also strongly im-\npacted by recency—that is, a semantically related\nattractor has dramatic impact on prediction accu-\nracies, but only if it occurs after the critical back-\nground entity. This pattern of results also suggests\nthat models may learn to link entity mentions pri-\nmarily with words occurring after those mentions,\nand not before. It is easy to see how such a heuris-\ntic may arise, as entity descriptors will more fre-\nquently follow the entity mention—however, this\nassumption is not foolproof, and it does not help\nwith distinguishing relevant versus irrelevant words\nthat occur after the entity mention. In aggregate,\nthese results further support the conclusion that su-\nperﬁcial cues exert signiﬁcant inﬂuences on model\npredictions, with these latter results suggesting a\nkey role for word position relative to the key entity.\nWe note that RoBERTaLARGE again stands out\nas by far the most robust to these attractor effects,\nthough it shows a notable decrease nonetheless.\n7 Discussion\nThe experiments above were designed to do two\nthings. The ﬁrst purpose was to test whether pre-\n1591\ntrained LMs show evidence of robustly process-\ning and storing new facts from input context. To\ntest this, we inserted irrelevant, distracting con-\ntent in addition to critical context information, to\nsee whether model predictions would be affected.\nThe results of these experiments show clearly that\nmodel predictions are indeed impacted by this ir-\nrelevant content, rather than remaining consistent\non the basis of the critical contextual information.\nThe second goal of these experiments was to\nexplore the speciﬁc types of cues affecting model\npredictions, by varying the nature of distracting\ncontent. Through these manipulations, we ﬁnd\ninﬂuences of both semantic similarity and word\nposition as superﬁcial cues informing model pre-\ndictions. In particular, we ﬁnd that insertion of a\nsingle semantically related attractor has a dramatic\neffect on predictions, with the ﬁrst such attractor\nreaching roughly ceiling impact. Unrelated attrac-\ntors also inﬂuence predictions, with more gradual,\nadditive impacts. Effects of semantic similarity\ninteract additionally with effects of relative word\nposition, with semantically related attractors show-\ning much reduced inﬂuence when occurring prior\nto the key entity mention. These trends suggest\nthat models rely on heuristics involving semantic\nsimilarity to critical words, and relative position of\nentity and descriptor words, for determining which\nelements of context are relevant for a prediction.\nWhile it should not come as a shock that lan-\nguage models use superﬁcial contextual cues for\nprediction, ﬁndings of this kind must serve as re-\nality checks as we consider the capacity of these\nmodels for language “understanding”. We have\nmade the simple assumption that “understanding”\nwill involve robustly representing and retaining in-\nformation from prior context, and generating pre-\ndictions accordingly. The results presented here\nsuggest that this criterion is not met. Additionally,\nthe results of these experiments take steps toward\nunderstanding the precise strategies that models do\nuse in generating predictions, beginning to sketch\nout a picture in which models rely substantially\non coarse-grained semantic similarity and word\nposition cues to identify relevant contextual words.\nWe do note that of the models tested,\nRoBERTaLARGE frequently distinguishes itself as\nleast susceptible to our attractors—though it shows\ndisruption all the same. Since this model is pre-\ntrained on a larger dataset than other models tested\nhere, size of pre-training data is a likely contributor\nto the model’s superior performance. Additionally,\nwe ﬁnd that within a given training regime, larger\nmodels mostly perform more robustly than smaller\nmodels, suggesting that model size is also a con-\ntributor in interaction with factors like training data\nsize. We leave for future work the investigation of\nwhether the comparatively strong performance of\nRoBERTaLARGE reﬂects truly more robust repre-\nsentations in that model—or use of superﬁcial cues\nnot yet targeted in the present work.\nThese ﬁndings also have implications for study-\ning mechanistic connections between pre-trained\nlanguage models and language processing in hu-\nmans. Studies of human sentence processing have\nshown comparable susceptibility to interference\nfrom irrelevant context elements, depending on se-\nmantic and syntactic properties (Van Dyke, 2007;\nParker and Phillips, 2017; Dillon et al., 2013). Sys-\ntematic comparison of interference effects in hu-\nmans and in language models stands to shed light\non mechanistic similarities and differences in the\nways that these language processing systems han-\ndle information from prior context.\n8 Conclusion\nWe have presented results manipulating inputs of\npre-trained LMs, to test the ability of such models\nto represent and retain information conveyed by in-\nput text. Our results show that though models may\nappear to handle information correctly in simple\nsettings, these correct predictions are easily broken\nby insertion of distracting material in the context.\nSystematic manipulation of the distracting content\nfurther indicates key roles for semantic similarity\nand relative word position in models’ selection of\nrelevant contextual cues for prediction. Overall, the\nresults suggest that LM predictions are driven more\nby coarse-grained superﬁcial cues than by extrac-\ntion of robust meaning information from context.\nThe results serve as a reality check for considera-\ntions of the extent to which LMs “understand” their\ninput, and lay groundwork to understand the mech-\nanisms that do drive predictions in these models.\nAcknowledgments\nWe would like to thank three anonymous reviewers\nfor valuable feedback on this paper. We also thank\nmembers of the University of Chicago CompLing\nLab for helpful discussion. This material is based\nupon work supported by the National Science Foun-\ndation under Award No. 1941160.\n1592\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nBrian Dillon, Alan Mishler, Shayne Sloggett, and Colin\nPhillips. 2013. Contrasting intrusion proﬁles for\nagreement and anaphora: Experimental and model-\ning evidence. Journal of Memory and Language ,\n69(2):85–103.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Ab-\nhilasha Ravichander, Eduard H. Hovy, Hinrich\nSchütze, and Yoav Goldberg. 2021. Measuring and\nimproving consistency in pretrained language mod-\nels. CoRR, abs/2102.01017.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic sub-\njects: Representations of syntactic state. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 32–42, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nFelix Hill, Antoine Bordes, S. Chopra, and J. Weston.\n2016. The goldilocks principle: Reading children’s\nbooks with explicit memory representations. CoRR,\nabs/1511.02301.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2021–2031, Copenhagen, Denmark. Association for\nComputational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. As-\nsociation for Computational Linguistics.\nJosef Klafka and Allyson Ettinger. 2020. Spying on\nyour neighbors: Fine-grained probing of contex-\ntual embeddings for information about surrounding\nwords. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4801–4811, Online. Association for Computa-\ntional Linguistics.\nTomáš Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom,\nChris Dyer, Karl Moritz Hermann, Gábor Melis, and\nEdward Grefenstette. 2018. The narrativeqa reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317–328.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3428–3448.\nKanishka Misra, Allyson Ettinger, and Julia Rayz.\n2020. Exploring BERT’s sensitivity to lexical cues\nusing tests from semantic priming. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020 , pages 4625–4635, Online. Associa-\ntion for Computational Linguistics.\nNasrin Mostafazadeh, Michael Roth, Annie Louis,\nNathanael Chambers, and James Allen. 2017. Ls-\ndsem 2017 shared task: The story cloze test. In Pro-\nceedings of the 2nd Workshop on Linking Models of\nLexical, Sentential and Discourse-level Semantics ,\npages 46–51.\n1593\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A new benchmark for natural lan-\nguage understanding. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 4885–4901, Online. Association\nfor Computational Linguistics.\nDan Parker and Colin Phillips. 2017. Reﬂexive attrac-\ntion in comprehension is selective. Journal of Mem-\nory and Language, 94:272–290.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789.\nMatthew Richardson, Christopher JC Burges, and Erin\nRenshaw. 2013. MCTest: A challenge dataset for\nthe open-domain machine comprehension of text.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n193–203.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R Bowman, Dipan-\njan Das, et al. 2018. What do you learn from con-\ntext? probing for sentence structure in contextual-\nized word representations. In International Confer-\nence on Learning Representations.\nJulie A Van Dyke. 2007. Interference effects from\ngrammatically unavailable constituents during sen-\ntence processing. Journal of Experimental Psychol-\nogy: Learning, Memory, and Cognition, 33(2):407.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The Benchmark of Lin-\nguistic Minimal Pairs for English. Transactions\nof the Association for Computational Linguistics ,\n8:377–392.\nJ. Weston, Antoine Bordes, S. Chopra, and Tomas\nMikolov. 2016. Towards ai-complete question an-\nswering: A set of prerequisite toy tasks. arXiv: Arti-\nﬁcial Intelligence.\nEthan Wilcox, Roger Levy, Takashi Morita, and\nRichard Futrell. 2018. What do RNN language\nmodels learn about ﬁller–gap dependencies? In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 211–221, Brussels, Belgium.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. HotpotQA: A\ndataset for diverse, explainable multi-hop question\nanswering. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning.\nA Appendix\n1594\nBase context target\nSebastian lives in France. The capital of Sebastian’s country is ____ Paris\nRowan lives in Chile. The capital of Rowan’s country is ____ Santiago\nRowan lives in China. The capital of Rowan’s country is ____ Beijing\nRowan lives in Finland. The capital of Rowan’s country is ____ Helsinki\nRowan lives in Indonesia. The capital of Rowan’s country is ____ Jakarta\nJake lives in Poland. The capital of Jake’s country is ____ Warsaw\nJake works as a ﬂorist. For his job, Jake sells ____ ﬂowers\nJake works as an optician. For his job, Jake sells ____ glasses\nJake works as a baker. For his job, Jake sells ____ bread\nDaniel works as a butcher. For his job, Daniel sells ____ meat\nDaniel works as a ﬁsherman. For his job, Daniel sells ____ ﬁsh\nDaniel works as a painter. For his job, Daniel sells ____ paintings\nDaniel visited the Taj Mahal. The country Daniel traveled to was ____ India\nDaniel visited the Pyramid of Giza. The country Daniel traveled to was ____ Egypt\nJack visited the Eiffel Tower. The country Jack traveled to was ____ France\nJack visited the Tower of Pisa. The country Jack traveled to was ____ Italy\nJack visited the Machu Picchu. The country Jack traveled to was ____ Peru\nJack visited the Kremlin. The country Jack traveled to was ____ Russia\nJack played football. In his game, Jack scored a ____ touchdown\nJack played baseball. In his game, Jack scored a ____ run\nDaniel played soccer. In his game, Daniel scored a ____ goal\nSebastian played cricket. In his game, Sebastian scored a ____ century\nTable 2: Base context for the dataset\nContext target\nJohn lives in Chile and writes poetry. The capital of John’s country is ____ Santiago\nJohn lives in Chile, writes poetry, and drives a car. The capital of John’s country\nis ____\nSantiago\nJohn lives in Chile, writes poetry, and drives a car. The capital of John’s country\nis ____\nSantiago\nJohn lives in Chile, writes poetry, drives a car, and slept late last week . The\ncapital of John’s country is ____\nSantiago\nJohn works as a ﬂorist and Jack writes poetry. For his job, John sells ____ ﬂowers\nJake visited the Eiffel Tower,Rowan drives a car, and Jack sits by the lake. The\ncountry Jake traveled to was ____\nFrance\nSebastian played football, writes poetry, slept late last week, and sits by the lake.\nIn his game, Sebastian scored a ____\ntouchdown\nTable 3: Examples from semantically unrelated attractors\n1595\nContext target\nDaniel knows that Jack lives in Beijing and he himself lives in Chile. The capital of\nDaniel’s country is ____\nSantiago\nDaniel knows that Jake likes to buy glasses and Rowan likes to buy meat and he\nhimself works as a ﬂorist. For his job, Daniel sells ____\nﬂowers\nJoe wants to visit the Eiffel Tower, the Pyramid of Giza, and the Machu Picchu and\nhas only visited the Taj Mahal. The country Joe traveled to was ____\nIndia\nRowan knows that his friends scored a goal and a century and he himself played\nfootball. In his game, Rowan scored a ____\ntouchdown\nTable 4: Examples from separating key entity and critical fact\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nMultiple Entity\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nSingle Entity\nBertBase BertLarge RobertaBase RobertaLarge GPT2Small GPT2Medium GPT2Large GPT2XL\nFigure 6: Accuracy vs number of attractors, with B type\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nMultiple Entity\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nSingle Entity\nBertBase BertLarge RobertaBase RobertaLarge GPT2Small GPT2Medium GPT2Large GPT2XL\nFigure 7: Accuracy vs number of attractors, with T type\n1596\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nMultiple Entity\n0 1 2 3\nnumber of attractors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nSingle Entity\nBertBase BertLarge RobertaBase RobertaLarge GPT2Small GPT2Medium GPT2Large GPT2XL\nFigure 8: Accuracy vs number of attractors with semantically unrelated attractors",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8109714984893799
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.7784888744354248
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5384891033172607
    },
    {
      "name": "Context model",
      "score": 0.4594178795814514
    },
    {
      "name": "Natural language processing",
      "score": 0.45618903636932373
    },
    {
      "name": "Language model",
      "score": 0.4418042302131653
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4255145490169525
    },
    {
      "name": "Machine learning",
      "score": 0.4013550579547882
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    }
  ],
  "cited_by": 20
}