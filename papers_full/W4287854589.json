{
  "title": "SwahBERT: Language Model of Swahili",
  "url": "https://openalex.org/W4287854589",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287855674",
      "name": "Gati Martin",
      "affiliations": [
        "Soonchunhyang University"
      ]
    },
    {
      "id": "https://openalex.org/A3128668436",
      "name": "Medard Edmund Mswahili",
      "affiliations": [
        "Chungbuk National University"
      ]
    },
    {
      "id": "https://openalex.org/A2495245967",
      "name": "Young-Seob Jeong",
      "affiliations": [
        "Chungbuk National University"
      ]
    },
    {
      "id": "https://openalex.org/A4287855677",
      "name": "Jeong Young-Seob",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3133874049",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2014063130",
    "https://openalex.org/W3139401867",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W2511234952",
    "https://openalex.org/W4393769497",
    "https://openalex.org/W2806344213",
    "https://openalex.org/W2874464011",
    "https://openalex.org/W1595666351",
    "https://openalex.org/W4287601434",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W2992347006",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W4287267957",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W3034323190",
    "https://openalex.org/W2981641612",
    "https://openalex.org/W3046284469",
    "https://openalex.org/W3211384372",
    "https://openalex.org/W3183385616",
    "https://openalex.org/W3175306105",
    "https://openalex.org/W3034987021",
    "https://openalex.org/W2123161611",
    "https://openalex.org/W3013762371",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2972161408",
    "https://openalex.org/W3000840260",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2182096631",
    "https://openalex.org/W4287758476",
    "https://openalex.org/W2264351125",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W3197040386",
    "https://openalex.org/W1966797434",
    "https://openalex.org/W2008056655",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2251165062",
    "https://openalex.org/W1566294008",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W3011720761",
    "https://openalex.org/W3155913599",
    "https://openalex.org/W2164195112",
    "https://openalex.org/W2963672599"
  ],
  "abstract": "Gati Martin, Medard Edmund Mswahili, Young-Seob Jeong, Jeong Young-Seob. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 314 - 324\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nSwahBERT: Language Model of Swahili\nGati L Martin1, Medard E Mswahili2, Young-Seob Jeong2*, Jiyoung Woo1\n1Soonchunhyang University, Asan-si, Korea\n2Chungbuk National University, Cheongju-si, Korea\n{gatimartin, jywoo}@sch.ac.kr\n{medardedmund25, ysjay}@chungbuk.ac.kr\nAbstract\nThe rapid development of social networks, elec-\ntronic commerce, mobile Internet, and other\ntechnologies has influenced the growth of Web\ndata. Social media and Internet forums are valu-\nable sources of citizens’ opinions, which can be\nanalyzed for community development and user\nbehavior analysis. Unfortunately, the scarcity\nof resources (i.e., datasets or language mod-\nels) has become a barrier to the development\nof natural language processing applications in\nlow-resource languages. Thanks to the recent\ngrowth of online forums and news platforms of\nSwahili, we introduce two datasets of Swahili\nin this paper: a pre-training dataset of approxi-\nmately 105MB with 16M words and an anno-\ntated dataset of 13K instances for the emotion\nclassification task. The emotion classification\ndataset is manually annotated by two native\nSwahili speakers. We pre-trained a new mono-\nlingual language model for Swahili, namely\nSwahBERT, using our collected pre-training\ndata, and tested it with four downstream tasks\nincluding emotion classification. We found that\nSwahBERT outperforms multilingual BERT, a\nwell-known existing language model, in almost\nall downstream tasks.\n1 Introduction\nNowadays, online social networking has revolu-\ntionized interpersonal communication. The influ-\nence of social media in our everyday lives, at both\na personal and professional level, has led recent\nstudies to language analysis in social media (Zeng\net al., 2010). Especially, natural language process-\ning (NLP) tools are often used to analyze textual\ndata for various real-world applications; mining\nsocial media for information about health (De Gen-\nnaro et al., 2020), diseases analysis (e.g., COVID-\n19 (Gao et al., 2020), Ebola (Tran and Lee, 2016)),\nidentifying sentiment and emotion toward prod-\nucts and services, and developing dialog systems\n(Zhou et al., 2020). Language models have recently\ndrawing much attention as they are known to be ef-\nfective in many NLP tasks (e.g., text classification,\nentailment, sequence labeling), but they commonly\nrequire a huge amount of data for pre-training and\nfine-tuning; some models are designed for few-shot\nlearning that does not require much labeled data\nfor fine-tuning, though they still require plenty of\npre-training data. As it is expensive and difficult\nto get the labeled and unlabeled data, the majority\nof the data are in high-resource languages (HRLs)\n(e.g., English, Spanish). Unfortunately, other than\nabout 20 HRLs languages, approximately 7,000\nlow-resource languages (LRLs) in the world are\nleft behind, where most of LRLs are spoken and\nlittle written (Magueresse et al., 2020). Africa and\nIndia are the main hosts of LRLs, where some lan-\nguages are spoken by more than 20 million people\n(e.g., Hausa, Oromo, Zulu, and Swahili). As more\ndata on social media in LRLs, qualified datasets,\nand publicly available language models will bring\nmany advantages in various fields, such as edu-\ncation (Obiria, 2019), healthcare (de Las Heras-\nPedrosa et al., 2020), entertainment (Ahn et al.,\n2013), and business.\nSwahili, a Bantu language, is one of the two offi-\ncial languages (the other being English) of the East\nAfrican countries such as Tanzania (Petzell, 2012),\nKenya, and Uganda. It has been widely spread\nin African countries not only as a lingua franca\nbut also as a second or third language across the\nAfrican continent and broadly in education, admin-\nistration, and media. With the rapid development\nof social networks, electronic commerce, mobile\nInternet, and other technologies, Swahili is also\nspreading in online places that result in the growth\nof Web data. For example, JamiiForum is a popular\nonline platform in Tanzania, and it provides a place\nto discuss different issues, including political, busi-\nness, educational, and lifestyle; this means more\ncollected textual data of Swahili is available.\nBy making use of the online textual data of\n314\nSwahili, there are several studies for different tasks\n(e.g., sentiment classification (Obiria, 2019; Noor\nand Turan, 2019; Seif, 2016), news classification\n(David, 2020)). Recently, language models have\ndrawn much attention from the industry and aca-\ndemic world, as the language models brought much\nbetter performance (e.g., accuracy) than other ex-\nisting models. There are few studies that employed\nthe language models for Swahili: named entity\nrecognition (NER) (Adelani et al., 2021) and senti-\nment classification (Martin et al., 2021). Although\nthese studies have shown successful results, they\nare limited in that they just borrow the language\nmodels (e.g., multilingual Bidirectional Encoder\nRepresentations from Transformer (mBERT) (De-\nvlin et al., 2019), Cross-lingual Model-RoBERTA\n(XLM-R) (Conneau et al., 2020)) pre-trained with\nother resources (i.e., other languages); in other\nwords, their language models are pre-trained for\nmultiple languages but not dedicated for Swahili.\nAlthough such multilingual language models have\nshown great generalization power across multiple\nlanguages, several studies (Bhattacharjee et al.,\n2021; Tanvir et al., 2021; Vilares et al., 2021) re-\nported that the monolingual models often outper-\nform these multilingual models. There was no\nstudy that proposed a monolingual language model\nfor Swahili (i.e., Swahili-specific language model),\nand the main reason is that Swahili is one of the\nLRLs, so the existing studies commonly suffered\nfrom lack of available data.\nIn this paper, we focus on the Swahili language.\nTo the best of our knowledge, this is the first study\nthat collects a pre-training dataset and uses it for\npre-training of the Swahili-specific language model.\nWe also provide a manually annotated dataset for\nthe emotion classification task. The contributions\nare summarized as follows.\n• Pre-training dataset: we collected Web data\nfrom different sources (news sites and so-\ncial discussion forums) for pre-training the\nSwahili language model.\n• Emotion dataset: we introduce a new Swahili\ndataset for multi-label emotion classification\nwith six Ekman’s emotions: happy, surprise,\nsadness, fear, anger, and disgust.\n• Swahili language model: we pre-trained the\nSwahili language model and compared its per-\nformance with other language models on sev-\neral downstream tasks (e.g., emotion classifi-\ncation, news classification, and named entity\nrecognition (NER)).\n2 Background\nMost African countries have minority languages\nthat are used by specific ethnic groups (approx.\n158 in Tanzania 1). However they speak different\nnational and official languages of their countries,\nincluding native and colonial that can be used in\npublic services such as education, politics, and the\nmedia. Swahili is a Bantu language widely spo-\nken in sub-Saharan Africa and acts as the common\ntongue for most East African (Lodhi, 1993; Amidu,\n1995). Many Swahili vocabularies are derived from\nloanwords, the vast majority from Arabic, but also\nEnglish, Hindi, Portuguese, and other Bantu lan-\nguages 2. As the language grows, new formal and\ninformal vocabularies emerge. The formal vocabu-\nlaries are used in official documents, whereas the\ninformal vocabularies are mostly used by young\nadults and on social media platforms (Momanyi,\n2009).\nStructurally, it is considered an agglutinative lan-\nguage with polysemous features. Its morphology\ndepends on prefixes and suffixes which are sylla-\nbles (Shikali et al., 2019). A single word is gen-\nerated with morphemes (i.e., stem, prefixes, and\naffixes) that will have corresponding inflectional\nforms. Nouns are divided into classes on the basis\nof their singular and plural prefixes. Despite its\npopularity, a limited amount of textual data is avail-\nable and it is one of the low-resource languages\n(LRLs). Although there have been a few studies\nthat illustrate the value of NLP (Martin et al., 2021;\nObiria, 2019; Gelas et al., 2012), they commonly\nsuffered from the lack of available data.\n2.1 Existing dataset of Swahili\nTo overcome the problem of limited language re-\nsource, they have been few datasets for different\ntasks: new classification dataset, NER dataset, sen-\ntiment classification dataset, and emotion classifi-\ncation dataset.\n2.1.1 News classification dataset\nThis dataset 3 is created and shared by a data sci-\nence competition platform Zindi (David, 2020) . It\ncontains a total of 23,266 instances collected from\ndifferent news websites in Tanzania. There are 6\n1https://www.tanzania.go.tz/home/pages/228\n2https://en.wikipedia.org/wiki/Swahili_language\n3https://zenodo.org/record/4300294?ref=hackernoon.com\n315\ncategories of news: kitaifa (national), kimataifa (In-\nternational), biashara (finance), michezo (sports),\nafya (health), and burudani (entertainment). The\namount of each category is 10,242 (national), 1,905\n(International), 2,028 (finance), 859 (health), 6,003\n(sports), and 2,229 (entertainment); so this dataset\nis highly imbalanced. Kastanos and Martin (2021)\napplied a deep learning model called Text Graph\nConvolutional Network (Text GCN) on this dataset\nand achieved an F1 score of 75.67% for the news\nclassification task.\n2.1.2 Sentiment classification dataset\nObiria (2019) collected 886 posts from Twitter and\nFacebook to analyze the student opinion in Kenyan\nuniversities and achieved an accuracy of 83% on\nbinary classification task using support vector ma-\nchine (SVM) (Hearst et al., 1998). Noor and Turan\n(2019) extracted 1,087 Twitter texts about demone-\ntization in Kenya and performed ternary sentiment\nclassification with Naive Bayes. They applied var-\nious feature extraction methods and obtained an\naccuracy of 70.8%. Recently, Martin et al. (2021)\nused a cross-lingual model, mBERT, to perform\nbinary sentiment classification on a social media\ndataset that they manually annotated, and achieved\nan accuracy of 87.59%. None of the above datasets\nare publicly available.\n2.1.3 Named entity recognition dataset\nThis dataset, namely MasakhaNER 4 (Adelani\net al., 2021), is created for ten African languages,\nincluding Swahili. The news texts are collected\nfrom local news sources and annotated using\nELISA tool (Lin et al., 2018) by native speakers\nof each language. The dataset contains a total\nof 3,006 instances and covers four entities: per-\nsonal name (PER), location (LOC), organization\n(ORG), and date & time (DATE) as inspired by\nthe English CoNLL-2003 corpus (Tjong Kim Sang\nand De Meulder, 2003). The number of entities\nof each type is 1,702 (PER), 2,842 (LOC), 960\n(ORG), and 940 (DATE). They compared three\nmodels (e.g., CNN-BiLSTM-CRF, mBERT, and\nXLM-R) on the NER task, and mBERT and XLM-\nR achieved 89.36% and 89.46% of an F1 score,\nrespectively.\n4https://github.com/masakhane-io/masakhane-\nner/tree/main/data\n2.2 Language models\nOver the years, models for word representation\nhave been developed and have shown that they are\ncapable of capturing the semantics and syntactic\ndependencies between words: Word2Vec (Mikolov\net al., 2013), Glove (Pennington et al., 2014), and\nFastText (Bojanowski et al., 2017). As these mod-\nels do not incorporate context of words, many\ncontext-aware language models based on Trans-\nformer (Vaswani et al., 2017) were introduced (e.g.,\nBidirectional Encoder Representations from Trans-\nformer (BERT) (Devlin et al., 2019) and XLM-R\n(Conneau et al., 2020)). These language models\nare trainable with a monolingual or multilingual\ndataset. For example, multilingual BERT (mBERT)\nis trained with a dataset of 104 languages and a\nshared vocabulary.\nAlthough mBERT has shown its potential in\nsome previous work, several studies reported some\nlimitations of mBERT especially to LRLs: (1) the\nlimited scale of pre-training data (only Wikipedia\nwas used) (Conneau et al., 2020); (2) the small\nvocabulary size for specific language (Wang et al.,\n2019). To overcome that, XLM-RoBERTA mod-\nifies mBERT by increasing the amount of pre-\ntraining data, which increases the shared vocab-\nulary between different languages. It provides\na strong improvement over mBERT, however, it\nis outperformed by monolingual models (Tanvir\net al., 2021; Bhattacharjee et al., 2021) due to bet-\nter representation of morphological language such\nas Swahili. This is a good improvement since the\nmodel can learn morphological information. An-\nother limitation is the nature of pre-training cor-\npora. Most available corpora are extracted from\nWikipedia, Bookcorpus, or news blogs which may\nnot be compatible with the task that covers multi-\ndomain such as social media data. In this work,\nwe collect our data from different sources across\nseveral domains for our new language model.\n3 Dataset\nIn this section we describe our collected dataset for\npre-training and the downstream task of emotion\nclassification. We open our datasets for future use\nin various studies 5.\n3.1 Pre-training dataset\nThe existing available corpora for Swahili are\nvery small, for example, the Open Super-large\n5https://sites.google.com/view/swahbert/home\n316\nCrawled Aggregated coRpus (OSCAR) database\ncontains about 25 megabytes of the corpus. Using\ncrawler tools, we scraped our own data from dif-\nferent sources such as news Web sites, forums, and\nWikipedia. The news Web sites include UN news6,\nV oice of America (V oA)7, Deutsche Welle (DW)\n8 and taifaleo 9. We collected data from JamiiFo-\nrums, which is one of the most popular social media\nwebsites in Tanzania founded in 2006. The forum\nhas provided a discussion platform for the public\nto discuss different issues, including political, busi-\nness, educational, and lifestyle. Since JamiiForums\nis a discussion platform, most of its contents are\neither passages of information or short comments.\nWe collected the passages with more than four logi-\ncally connected sentences. We removed URL links,\nusernames, non-textual content (e.g., HTML tags)\nand filtered out non-Swahili characters (e.g., Latin,\nChinese). The size of dataset is about 105MB with\n16M words, where a sentence has an average of 27\nsubword tokens. Most of these platforms contain\ndata that range from 5 to 10 years. The contribution\n(in percentage) of each source was taifaleo (39.4),\nUN news (28.6), JamiiForum (10.2), Wikipedia\n(9.5), V oA (7.2), and DW (5.1).\n3.2 Emotion classification dataset\nExisting non-Swahili datasets typically use anno-\ntation schemes based on Ekman (Ekman, 1992),\nPlutchik (Plutchik, 1980) or with multiple cate-\ngories (Demszky et al., 2020). For example, there\nare English datasets with multiple emotion cate-\ngories: Affective Text (Strapparava and Mihalcea,\n2007) with 11 categories, CrowdFlower with 14 cat-\negories, GoEmotions (Demszky et al., 2020) with\n27 categories and others (Oberländer and Klinger,\n2018). In this paper, to construct a new Swahili\ndataset for emotion classification, we chose to use\n6 emotion categories from Ekman’s (Ekman, 1992)\nscheme: anger (hasira), surprise (mshangao), dis-\ngust (machukizo), joy (furaha), fear (woga), and\nsadness (huzuni). Our dataset is collected from two\nsource types: social media platforms of Swahili\nand existing emotion datasets of English. The so-\ncial media platforms include YouTube, JamiiFo-\nrum 10 and Twitter The conversations and com-\nments on these platforms cover different topics,\n6https://news.un.org/\n7https://www.voaswahili.com/\n8https://www.dw.com/sw/idhaa-ya-kiswahili/s-11588\n9https://taifaleo.nation.co.ke/\n10https://www.jamiiforums.com/\nItem Value\n# of examples 12,976\n# of labels 7 (including ‘neutral’)\n# of text per labels\njoy: 2,439\ndisgust: 3,227\nanger: 1,772\nsadness: 2,339\nfear: 1,116\nsurprise: 2,305\nNeutral: 863\n# of labels per examples\n1: 92.09%\n2: 7.45%\n3: 0.46%\nRatio of source\nDailydialog: 13.26%\nEmotion cause: 13.08%\nISEAR: 2.26%\nSocial media: 71.40%\nAppx. ratio of taxonomy\nPolitics: 15%\nsocial issues: 80%\npandemic: 5%\nTable 1: Statistics of Emotion classification dataset.\nsuch as politics, disease outbreaks, and aspects\nof daily life. We reviewed the dataset and re-\nmoved profanity towards a specific person or eth-\nnic group. For example, in the sentence ‘ [name]\nis very stupid, she doesn’t act like a leader at all,’\nwe replaced the target name with pronoun. We\nalso selected three existing English datasets with\nrelevant topic coverage and converted them into\nSwahili using Google translator. The three datasets\ninclude: (1) Dailydialog (Li et al., 2017) that re-\nflects daily communication and covers various top-\nics about our daily life, (2) Emotion Cause (Ghazi\net al., 2015), and (3) ISEAR (Scherer and Wallbott,\n1994) collected from participants from varying cul-\ntural backgrounds who complete questionnaires\nabout their experiences and reactions. The Swahili\nemotion texts obtained from the Google translator\nwere checked and corrected thoroughly by a native\nSwahili speaker.\nFor the dataset collected from forums of Swahili,\ntwo native Swahili speakers were assigned to anno-\ntate the emotion labels. These speakers agreed to\nthe consent of serving as annotators and were given\nan instruction of annotation. They were asked to\nselect one or multiple suitable emotion labels that\nwere expressed in the text, and labeled ‘neutral’ for\nunsure texts. The statistics summary is presented\nin Table 1. We also calculated annotator agree-\nment using Cohen’s kappa metric, which computes\na score of agreement level between two annota-\n317\ntors who each classify N items into C mutually\nexclusive categories. The scores for each label are\njoy (0.835), disgust (0.845), anger (0.763), sadness\n(0.733), fear (0.694), and surprise (0.806). Figure\n1 is a heatmap that shows the degree of relation-\nship between emotions. The emotion pair with\nhigh intensity (e.g. hasira (anger) and machukizo\n(disgust)) has a positive correlation in multi-label\nemotion.\nFigure 1: Pearson correlation matrix for the multi-\nemotion.\n4 SwahBERT\nWith the collected dataset, we pre-trained the mono-\nlingual BERT for Swahili, namely SwahBERT11.\nThe SwahBERT basically has the same architecture\nas the original BERT. This section describes the\nprocess of pre-training and fine-tuning of Swah-\nBERT.\n4.1 Tokenizer\nIn mBERT, not all languages have equal content\nsize (Wu and Dredze, 2020), and some languages\nare dominated; for example, Swahili is only less\nthan 1% of the approximately 120K vocabulary\nof mBERT. Although it might benefit from high\nresource languages as Swahili has the same typol-\nogy (word order) and many loanwords, it would\ndefinitely be better to generate a Swahili-specific\ntokenizer. That is, the multilingual tokenizer often\nsplits the words without considering morphological\nboundaries (e.g., stem, prefixes, and suffixes), like\nthe sentence in Table 2, so the individual subword\nunits do not have a clear semantic meaning. Swahili\nis morphologically rich language and polysynthetic\nlanguage; for example, a word alimpikia (cooked\n11https://sites.google.com/view/swahbert/home\nfor) has a lexical morph {-pika}, four grammatical\nmorphs {a-,-li-, -m-, -i-} and two in the verb skele-\ntal morphological frame which has the root {-pik-},\nand bantu end vowel {-a} (Choge, 2018). In this\npaper, to incorporate such linguistic complexity,\nwe try monolingual tokenizers for Swahili with dif-\nferent vocabulary sizes (e.g., 32K, 50K, and 70K)\nusing the WordPiece algorithm 12.\n4.2 Training\nSwahBERT has 12 encoder blocks and 768 hidden\nunits. We employ two unsupervised pre-training\ntasks: Masked Language Modeling (MLM) and\nNext Sentence Prediction (NSP) as described in\n(Devlin et al., 2019). We conduct experiments by\nvarying the vocabulary size and number of training\nand warmp steps. Following the pre-training pro-\ncess of (Devlin et al., 2019), we pre-trained Swah-\nBERT in two-phases: uncased was firstly trained\nfor 600K steps using an input length of 128, and\nthen further trained for an additional 200K steps\nusing an input length of 512. Cased models were\ntrained for 600K and 900K steps initially, and an ad-\nditional 200K and 100K steps in the second phase.\nThe batch size is 32 and 6 for the two-phases, re-\nspectively, and the parameters were optimized us-\ning the Adam optimizer (Kingma and Ba, 2014)\nwith a warmup over the first 1% of the steps to a\npeak learning rate of 1e-4.\nTable 3 gives the results of pre-training, where\nit took around 105 hours to complete all phases\nusing two GeForce GTX 1080 Ti GPUs. The best\nresult was obtained with a vocabulary size of 50K\nfor uncased models, while a vocabulary size of 32K\nwas the best for cased models. Compared to the\nmBERT that has a vocabulary size of 119K, the\nbest vocabulary size of SwahBERT seems small.\nThis is consistent with the vocabulary sizes of other\nmonolingual BERT models; for example, 32K for\nEnglish, 50K for Estonian, and 30K for Dutch.\nWith the pre-trained models, we put an additional\nlayer on top of the models and fine-tuned them\nin a supervised way with the labeled datasets for\ndownstream tasks.\n5 Experiments\nWe tested our model on downstream tasks and com-\npared with other models. We put an additional\nlinear layer and an output layer on top of the pre-\ntrained language models, where all models are im-\n12https://github.com/kwonmha/bert-vocab-builder\n318\nV ocabulary Tokenization\nmBERT wa ##nan ##chi wa ##nata ##raj ##ia fur ##sa ke ##dek ##ede\nSwahBERT(32K) wananchi wanatarajia fursa ke ##de ##ke ##de\nSwahBERT(50K) wananchi wanatarajia fursa kede ##ke ##de\nSwahBERT(70K) wananchi wanatarajia fursa kedekede\nTable 2: Tokenization of the sentence ‘Wananchi wanatarajia fursa kedekede’ (Citizens expects more opportunity)\nby using mBERT and SwahBERT tokenziers.\nSteps vocab size MLM acc NSP acc loss\n800K 32K (uncased) 73.37 99.50 1.1822\n800K 50K (uncased) 76.54 99.67 1.0667\n800K 70K (uncased) 73.38 100.0 1.2131\n800K 32K (cased) 76.94 99.33 1.0562\n1M 32K (cased) 73.81 98.17 1.2732\nTable 3: Accuracy and loss of pre-training.\ntasks Total Train Development Test\nEmotion 12,976 9,732 1,297 1,947\nNews 23,266 18,612 2,327 2,327\nSentiment 7,107 5,330 710 1,067\nNER 3,006 2,104 300 602\nTable 4: The number of instances of the datasets of\ndownstream tasks.\nplemented with HuggingFace PyTorch library. Dur-\ning the fine-tuning, the parameters are optimized\nusing the Adam optimizer (Kingma and Ba, 2014)\nwith an initial learning rate of 5e-5 and ϵ parameter\nof 1e-8. The batch size was set to 32. Table 5 sum-\nmarizes the averaged F1 scores of language models\nfor different downstream tasks, where all language\nmodels are with uncased vocabularies except for\nSwahBERTcased. Except for the NER task, the\nSwahBERT outperformed the mBERT for all tasks.\nThe statistic of the datasets is summarized in Table\n4, where the emotion classification dataset is intro-\nduced in this paper. Among the tasks, all models\nachieved much better performance in the news clas-\nsification task, and this might be explained by the\nfact that the data source of this task is online news\ndocuments that may have similar characteristics to\nthe pre-training dataset that is collected from on-\nline forums. In the following subsections, detailed\nresults of each task will be described, where the\nbest scores were obtained from three independent\nexperiments.\n5.1 Emotion Classification\nWe use our new dataset for this task and split the\ndataset into training (75%), development (10%),\ntasks SwahBERT SwahBERT cased mBERT\nEmotion 64.46 64.77 60.52\nNews 90.90 89.90 89.73\nSentiment 70.94 71.12 67.20\nNER 88.50 88.60 89.36\nTable 5: F1 scores (%) of language models on down-\nstream tasks, where NER indicates named entity recog-\nnition.\nand test (15%) sets. As shown in Table 5, there\nis an improvement of 3.94% F1 score from Swah-\nBERT (64.46) compared to mBERT (60.52). The\nmodel exhibits the best performance on emotions\nlike joy (0.80), sadness (0.71), and surprise (0.68),\nas exhibited in Table 6; this is consistent with the\nfact that these emotions have a lower correlation\nwith other emotions, allowing the models to more\neasily classify them. For the neutral (0.25) case,\nwe found that there were many instances of in-\ncomplete or uncertain expressions, and this caused\nconfusion with other emotions. This is reasonable\nas ‘neutral’ might not even exist because people\nare always feeling something (Gasper et al., 2019).\nFor example, ‘Hivi aliyekudanganya hivyo nani?’\n(who lied to you that anyway?) was predicted as\ndisgust, while ‘Nani aliyekudanganya?’ (who lied\nto you?) was classified as neutral. As mentioned\nin (Öhman et al., 2020), such uncertain texts are\nusually not self-contained since they are reactions\nto other posts; that is, the emotion will be different\nwhether we consider its context information or not.\nPost: Vifo vya Corona kila kukicha (Corona\ncases increases everyday) [sadness], <sad-\nness>\nCom1: Acha tuu (Yeah...) [sadness], <neu-\ntral>\nCom2: Kila mtu anaongea lake.. (Everyone is\nquick to talk what they wish) [disgust], <neu-\ntral>\nCom3: Popote, mimi na barakoa yangu (any-\nwhere, with my mask) [fear], <neutral>\n319\nSwahBERT mBERT\nlabels P R F1 P R F1\njoy 0.88 0.73 0.80 0.74 0.71 0.72\nanger 0.61 0.43 0.51 0.70 0.32 0.44\nsadness 0.68 0.74 0.71 0.69 0.65 0.67\ndisgust 0.61 0.61 0.61 0.54 0.56 0.55\nsurprise 0.73 0.64 0.68 0.74 0.66 0.70\nfear 0.65 0.61 0.63 0.65 0.56 0.60\nneutral 0.30 0.22 0.25 0.33 0.11 0.16\nTable 6: Results of emotion classification, where P, R,\nand F1 indicate precision, recall, and F1 score, respec-\ntively.\nThe annotators made the labels based on the\ncontext, whereas the language models predicted\nlabels without the context, and this caused the per-\nformance degradation. The example in the box\ndemonstrates how emotions can be affected by con-\ntextual information, where emotion with context\nis represented in [emotion] and emotion of non-\ncontext is <emotion>.\n5.2 News Classification\nWe used the existing news classification dataset,\nand it is split into three sets with a ra-\ntio of 80%:10%:10% which is equivalent to\n18,612:2,327:2,327 instances. As this dataset has\nsix news categories, this task is a classification on\nsix classes: kitaifa (national), kimataifa (Interna-\ntional), biashara (finance), michezo (sports), afya\n(health), and burudani (entertainment). Table 7\nshows the results of SwahBERT and mBERT mod-\nels. Compared to the existing study of (Kastanos\nand Martin, 2021) that achieved 75.56% of F1 score\nusing graph convolutional networks (GCN), we ob-\nserved the improvement of 14.06% and 15.23% F1\nscores with mBERT and SwahBERT, respectively.\nThe performance for the ‘health’ class is relatively\nlower than others, and the reason might be the data\nimbalance; the ‘health’ class has a much smaller\namount of instances than other classes, as described\nin subsection 2.1.1.\n5.3 Sentiment Classification\nAs there is no publicly available dataset for this\ntask, we used our emotion dataset by convert-\ning some emotion categories into three sentiment\nclasses: positive, negative, and neutral, where we\nmapped ‘joy’ to positive, ‘disgust’ to negative, and\n‘neutral’ was unchanged. For the neutral class, we\nextracted additional instances from ‘surprise’ emo-\nSwahBERT mBERT\nlabels P R F1 P R F1\nnational 0.91 0.92 0.92 0.91 0.91 0.91\nsports 0.96 0.97 0.97 0.94 0.98 0.96\nentert. 0.89 0.94 0.91 0.85 0.93 0.89\nbusiness 0.94 0.85 0.89 0.91 0.82 0.86\nInternat. 0.90 0.89 0.90 0.91 0.84 0.88\nhealth 0.50 0.41 0.45 0.47 0.44 0.45\nTable 7: News classification results, where P, R, F1\nindicate precision, recall and F1-score.\nSwahBERT mBERT\nlabels P R F1 P R F1\nnegative 0.70 0.70 0.70 0.65 0.69 0.67\npositive 0.82 0.83 0.82 0.75 0.82 0.79\nneutral 0.59 0.60 0.59 0.58 0.48 0.53\nTable 8: Sentiment classification results, where P, R, F1\nindicate precision, recall, and F1 score.\ntion because ‘surprise’ can be mapped mid-way\nof negative and positive (Marmolejo-Ramos et al.,\n2017). We split the dataset into three sets with ra-\ntio 75%:10%:15% equivalent to 5,330:710:1,067.\nResults are presented in Table 8. We found that\nSwahBERT outperformed the mBERT with a gap\nof 3-6% of F1 scores. As we observed in the re-\nsults of the emotion classification task, the overall\nperformance of sentiment classification task for\nthe ‘neutral’ class was much lower than the other\nclasses.\n5.4 Named Entity Recognition\nWe used MasakhaNER (Adelani et al., 2021)\ndataset for this task, and it has 70%:10%:20% ratio\nfor training, development, and test set. As shown\nin Table 5, we did not observe performance im-\nprovement of SwahBERT against the mBERT of\n(Adelani et al., 2021). The biggest reason of this\nwill be the small size of the dataset compared to\nother downstream tasks, as shown in Table 4. That\nis, the small NER dataset was not enough for Swah-\nBERT to learn the underlying patterns for NER\ntask, so there was no performance improvement\ncompared to the multilingual language model. We\nbelieve that the NER performance of SwahBERT\nwill increase as we keep gathering more NER data.\n6 Discussion\nWe constructed two datasets for the low-resource\nSwahili language: for the downstream task of\nemotion classification and pre-training of the new\n320\nFigure 2: Histogram of downstream task datasets, where the x-axis represents vocabulary words of SwahBERT.\nSwahili-specific BERT model. For pre-training\npurposes, we managed to collect a corpus of about\n105 MB. Although the size of our corpus is quite\nsmaller than that of rich-resource languages (e.g.,\nEnglish), the pre-trained SwahBERT has shown\ngreat improvement on the downstream tasks. This\nresult is consistent with other previous studies. For\nexample, Micheli et al. (2020) found that well-\nperforming language models can be obtained with\na little size of corpora of 100MB. Similarly, in\n(Martin et al., 2020), experimental results with the\nlanguage models pre-trained with the 4GB dataset\nwere comparable to those pre-trained with 138GB\ndataset. However, we believe that plenty of qual-\nified datasets will help to increase the power of\nlanguage models.\nAs demonstrated in the experimental results,\nSwahBERT is generally superior to mBERT in al-\nmost all downstream tasks. We believe that our tok-\nenizer with Swahili vocabulary has the biggest con-\ntribution to the results. The tokenizer of mBERT\nworks by sharing vocabulary over multiple lan-\nguages, and this tokenizer tends to split the words\nwithout taking into account morphological bound-\naries (e.g., stem, prefix, and postfix), as shown\nin Table 2, even though Swahili is a morphologi-\ncally rich language. The tokenizer of SwahBERT\naccommodates most single words (e.g., wanatara-\njia (expects), fursa (opportunity)) as one and thus\nhelps the model to get better representation.\nWe examined the characteristics of the datasets\nby frequency histograms of vocabulary words in the\nsame order as depicted in Figure 2. The emotion\nTasks Cosine similarity\nNews 98.616\nNER 52.465\nEmotion 84.445\nSentiment 81.543\nTable 9: Similarity scores between pre-training dataset\nand datasets of downstream tasks.\nclassification dataset and the sentiment classifica-\ntion dataset have a similar curve of histogram, and\nthe NER dataset and the news classification dataset\nseem similar to each other. This explains that\nthe language models (e.g., SwahBERT, mBERT)\nachieved similar performance (e.g., 88.5% to 90.9%\nF1 scores) for the news classification and NER\ntasks, and similar performance (e.g., 60.52% to\n71.12% F1 scores) for the emotion classification\nand sentiment classification tasks. Another interest-\ning point is that SwahBERT showed no improve-\nment in the NER task compared to mBERT. The\nmain reason for this, of course, is the small amount\nof NER dataset, but we further examined more de-\ntails by similarity scores between the pre-training\ndataset and downstream task datasets, as shown\nin Table 9. The similarity score is computed us-\ning a cosine similarity function on word frequen-\ncies in datasets. Note that the NER dataset has a\nmuch lower score than others, which implies that\nlanguage models have a smaller chance to learn\nlinguistic patterns for the NER task. This can be\nresolved if we collect more data for the NER task.\nWe also believe that collecting more qualified data\n321\nfor different tasks in low-resource languages (LRL)\nwill significantly contribute to various future NLP\napplications (e.g., social network services, news\nrecommendations, etc.).\n7 Conclusion\nIn this study, we introduced our pretraining corpus\nand annotated dataset for the emotion classification\ntask. The emotion classification dataset contains\n7 emotion classes, including neutral, and has ap-\nproximately 13K instances. We also performed pre-\ntraining of monolingual BERT for Swahili, namely\nSwahBERT, and experimentally compared it with\nthe multilingual BERT (mBERT). The SwahBERT\noutperformed the mBERT in almost all downstream\ntasks, where the downstream tasks include emotion\nclassification, news classification, sentiment clas-\nsification, and NER. Although SwahBERT exhib-\nited superior performance with a relatively smaller\npre-training corpus, a more qualified pre-training\ncorpus will definitely contribute to the develop-\nment of better language models. Therefore, with\nthe growth of the digital platforms for Swahili, we\nwill continue to use the available sources, including\nnative Swahili speakers as annotators, and collect\nmore data from different domains. We hope that\nthis study will facilitate the development of other\nmethodologies and pre-trained language models\n(e.g., XLM-R) and also aid in social services (e.g.,\nuser emotion analysis on forum texts).\nEthical Considerations\nThe two native Swahili annotators are the authors\nof this paper, and they receive fair compensation\nfrom the project fund. We collected data from a few\nsources, and all annotated datasets are in Swahili\nlanguage. We anonymized all texts and confirm\nthat our datasets are allowed to be disclosed for\nacademic purpose according to policies and laws\n(e.g., UN news13, V oice of America14, Deutsche\nWelle15, and taifaleo 16). We gratefully acknowl-\nedge a favor of every institute or company that\napproved of a generous data policy for academic\npurpose.\n13https://www.un.org/en/about-us/terms-of-use\n14https://www.voanews.com/p/5338.html\n15https://www.gesetze-im-\ninternet.de/englisch_urhg/englisch_urhg.html\n16https://www.nationmedia.com/terms-of-use/\nAcknowledgements\nThis research was supported by Basic Science Re-\nsearch Program through the National Research\nFoundation of Korea(NRF) funded by the Ministry\nof Education(NRF-2020R1I1A3053015). This\nwork was supported by Chungbuk National Univer-\nsity BK21 program(2022).\nReferences\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neu-\nbig, Daniel D’souza, Julia Kreutzer, Constantine Lig-\nnos, Chester Palen-Michel, Happy Buzaaba, Shruti\nRijhwani, Sebastian Ruder, Stephen Mayhew, Is-\nrael Abebe Azime, Shamsuddeen H. Muhammad,\nChris Chinenye Emezue, Joyce Nakatumba-Nabende,\nPerez Ogayo, Aremu Anuoluwapo, Catherine Gitau,\nDerguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-\nmam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,\nRubungo Andre Niyongabo, Jonathan Mukiibi, Ver-\nrah Otiende, Iroro Orife, Davis David, Samba Ngom,\nTosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,\nGerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-\nwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel\nOyerinde, Clemencia Siro, Tobius Saul Bateesa,\nTemilola Oloyede, Yvonne Wambui, Victor Akin-\node, Deborah Nabagereka, Maurice Katusiime, Ayo-\ndele Awokoya, Mouhamadane MBOUP, Dibora Ge-\nbreyohannes, Henok Tilaye, Kelechi Nwaike, De-\ngaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021. MasakhaNER: Named Entity\nRecognition for African Languages. Transactions\nof the Association for Computational Linguistics,\n9:1116–1131.\nJoongHo Ahn, Sehwan Oh, and Hyunjung Kim. 2013.\nKorean pop takes off! social media strategy of korean\nentertainment industry. In 2013 10th International\nConference on Service Systems and Service Manage-\nment, pages 774–777.\nAssibi Apatewon Amidu. 1995. Kiswahili: People, lan-\nguage, literature and lingua franca. Nordic Journal\nof African Studies, 4(1):104–123.\nAbhik Bhattacharjee, Tahmid Hasan, Kazi Samin,\nMd Saiful Islam, M. Sohel Rahman, Anindya Iqbal,\nand Rifat Shahriyar. 2021. Banglabert: Com-\nbating embedding barrier in multilingual models\nfor low-resource language understanding. CoRR,\nabs/2101.00204.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nSusan C Choge. 2018. A morphological classification\nof kiswahili. Kiswahili, 80(1).\n322\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451.\nDavis David. 2020. Swahili : News classification\ndataset.\nMauro De Gennaro, Eva G Krumhuber, and Gale Lucas.\n2020. Effectiveness of an empathic chatbot in com-\nbating adverse effects of social exclusion on mood.\nFrontiers in psychology, 10:3061.\nCarlos de Las Heras-Pedrosa, Pablo Sánchez-Núñez,\nand José Ignacio Peláez. 2020. Sentiment analysis\nand emotion understanding during the covid-19 pan-\ndemic in spain and its impact on digital ecosystems.\nInternational Journal of Environmental Research and\nPublic Health, 17(15):5542.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan Cowen, Gaurav Nemade, and Sujith Ravi.\n2020. \"GoEmotions: A dataset of fine-grained emo-\ntions. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4040–4054. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nPaul Ekman. 1992. An argument for basic emotions.\nCognition & emotion, 6(3-4):169–200.\nJunling Gao, Pinpin Zheng, Yingnan Jia, Hao Chen,\nYimeng Mao, Suhong Chen, Yi Wang, Hua Fu, and\nJunming Dai. 2020. Mental health problems and so-\ncial media exposure during covid-19 outbreak. Plos\none, 15(4):e0231924.\nKaren Gasper, Lauren A Spencer, and Danfei Hu. 2019.\nDoes neutral affect exist? how challenging three\nbeliefs about neutral affect can advance affective re-\nsearch. Frontiers in Psychology, 10:2476.\nHadrien Gelas, Laurent Besacier, and François Pelle-\ngrino. 2012. Developments of swahili resources for\nan automatic speech recognition system. In Spoken\nLanguage Technologies for Under-Resourced Lan-\nguages.\nDiman Ghazi, Diana Inkpen, and Stan Szpakowicz.\n2015. Detecting emotion stimuli in emotion-bearing\nsentences. In CICLing (2), pages 152–165.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John\nPlatt, and Bernhard Scholkopf. 1998. Support vec-\ntor machines. IEEE Intelligent Systems and their\napplications, 13(4):18–28.\nAlexandros Kastanos and Tyler Martin. 2021. Graph\nconvolutional network for swahili news classification.\narXiv preprint arXiv:2103.09325.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\nCao, and Shuzi Niu. 2017. DailyDialog: A manually\nlabelled multi-turn dialogue dataset. In Proceedings\nof the Eighth International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 986–995, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nYing Lin, Cash Costello, Boliang Zhang, Di Lu, Heng\nJi, James Mayfield, and Paul McNamee. 2018. Plat-\nforms for non-speakers annotating names in any lan-\nguage. In Proceedings of ACL 2018, System Demon-\nstrations, pages 1–6. Association for Computational\nLinguistics.\nAbdulaziz Y Lodhi. 1993. The language situation in\nafrica today. Nordic Journal of African Studies,\n2(1):11–11.\nAlexandre Magueresse, Vincent Carles, and Evan\nHeetderks. 2020. Low-resource languages: A re-\nview of past work and future challenges. ArXiv,\nabs/2006.07264.\nFernando Marmolejo-Ramos, Juan C Correa, Gopal\nSakarkar, Giang Ngo, Susana Ruiz-Fernández, Na-\ntalie Butcher, and Yuki Yamada. 2017. Placing joy,\nsurprise and sadness in space: a cross-linguistic study.\nPsychological Research, 81(4):750–763.\nGati L Martin, Medard E Mswahili, and Young-Seob\nJeong. 2021. Sentiment classification in swahili\nlanguage using multilingual bert. arXiv preprint\narXiv:2104.09006.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7203–\n7219. Association for Computational Linguistics.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training data\nvolume for compact language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n7853–7858, Online. Association for Computational\nLinguistics.\n323\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nClara Momanyi. 2009. The effects of’sheng’in the\nteaching of kiswahili in kenyan schools. Journal\nof Pan African Studies.\nIbrahim Moge Noor and Metin Turan. 2019. Sentiment\nanalysis using twitter dataset. IJID (International\nJournal on Informatics for Development), pages 84–\n94.\nLaura Ana Maria Oberländer and Roman Klinger. 2018.\nAn analysis of annotated corpora for emotion clas-\nsification in text. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 2104–2119.\nPeter Obiria. 2019. Swahili text classification using\nsupport vector machine and feature selection to en-\nhance opinion analysis in kenyan universities. PAC\nUniversity Journal of Arts and Social Sciences, 2(2).\nEmily Öhman, Marc Pàmies, Kaisla Kajava, and Jörg\nTiedemann. 2020. Xed: A multilingual dataset for\nsentiment analysis and emotion detection. In The\n28th International Conference on Computational Lin-\nguistics (COLING 2020).\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532–1543.\nMalin Petzell. 2012. The linguistic situation in tanzania.\nModerna språk, 106(1):136–144.\nRobert Plutchik. 1980. A general psychoevolutionary\ntheory of emotion. In Theories of emotion, pages\n3–33. Elsevier.\nKlaus R Scherer and Harald G Wallbott. 1994. Evidence\nfor universality and cultural variation of differential\nemotion response patterning. Journal of personality\nand social psychology, 66(2):310.\nHassan Seif. 2016. Naïve bayes and j48 classification\nalgorithms on swahili tweets: Perfomance evalua-\ntion. International Journal of Computer Science and\nInformation Security, 14(1):1.\nCasper S Shikali, Zhou Sijie, Liu Qihe, and Refuoe\nMokhosi. 2019. Better word representation vectors\nusing syllabic alphabet: a case study of swahili. Ap-\nplied Sciences, 9(18):3648.\nCarlo Strapparava and Rada Mihalcea. 2007. Semeval-\n2007 task 14: Affective text. In Proceedings of the\n4th International Workshop on Semantic Evaluations,\npages 70–74. Association for Computational Linguis-\ntics.\nHasan Tanvir, Claudia Kittask, Sandra Eiche, and\nKairit Sirts. 2021. EstBERT: A pretrained language-\nspecific BERT for Estonian. In Proceedings of the\n23rd Nordic Conference on Computational Linguis-\ntics (NoDaLiDa), pages 11–19. Linköping University\nElectronic Press, Sweden.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142–\n147.\nThanh Tran and Kyumin Lee. 2016. Understanding\ncitizen reactions and ebola-related information prop-\nagation on social media. In 2016 IEEE/ACM Inter-\nnational Conference on Advances in Social Networks\nAnalysis and Mining (ASONAM), pages 106–111.\nIEEE.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, volume 30, pages 5998–6008.\nDavid Vilares, Marcos Garcia, and Carlos Gómez-\nRodríguez. 2021. Bertinho: Galician bert representa-\ntions. CoRR, abs/2103.13799.\nHai Wang, Dian Yu, Kai Sun, Janshu Chen, and Dong\nYu. 2019. Improving pre-trained multilingual model\nwith vocabulary expansion. In Proceedings of the\n23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL), pages 316–327, Hong\nKong, China. Association for Computational Lin-\nguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130. Association for Computational\nLinguistics.\nDaniel Zeng, Hsinchun Chen, Robert Lusch, and Shu-\nHsing Li. 2010. Social media analytics and intelli-\ngence. IEEE Intelligent Systems, 25(6):13–16.\nLi Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum.\n2020. The design and implementation of XiaoIce, an\nempathetic social chatbot. Computational Linguis-\ntics, 46(1):53–93.\n324",
  "topic": "Swahili",
  "concepts": [
    {
      "name": "Swahili",
      "score": 0.9501851797103882
    },
    {
      "name": "Computer science",
      "score": 0.6793808341026306
    },
    {
      "name": "Natural language processing",
      "score": 0.455074667930603
    },
    {
      "name": "Programming language",
      "score": 0.409170538187027
    },
    {
      "name": "Linguistics",
      "score": 0.36753982305526733
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3333192467689514
    },
    {
      "name": "Philosophy",
      "score": 0.08143946528434753
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24541011",
      "name": "Soonchunhyang University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I163753206",
      "name": "Chungbuk National University",
      "country": "KR"
    }
  ],
  "cited_by": 23
}