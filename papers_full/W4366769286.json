{
    "title": "TransPolymer: a Transformer-based language model for polymer property predictions",
    "url": "https://openalex.org/W4366769286",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2151661829",
            "name": "Chang-Wen Xu",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2116527158",
            "name": "Yuyang Wang",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2004412487",
            "name": "Amir Barati Farimani",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2151661829",
            "name": "Chang-Wen Xu",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2116527158",
            "name": "Yuyang Wang",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2004412487",
            "name": "Amir Barati Farimani",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3018875774",
        "https://openalex.org/W4282823776",
        "https://openalex.org/W2950022236",
        "https://openalex.org/W3158860966",
        "https://openalex.org/W2908043757",
        "https://openalex.org/W3025142845",
        "https://openalex.org/W2944219262",
        "https://openalex.org/W3137390837",
        "https://openalex.org/W2780553247",
        "https://openalex.org/W3096386116",
        "https://openalex.org/W3113862044",
        "https://openalex.org/W2116341502",
        "https://openalex.org/W2766856748",
        "https://openalex.org/W2173027866",
        "https://openalex.org/W2966357564",
        "https://openalex.org/W3049517558",
        "https://openalex.org/W4214868967",
        "https://openalex.org/W4210261876",
        "https://openalex.org/W1983478747",
        "https://openalex.org/W1988037271",
        "https://openalex.org/W4293232267",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2769423117",
        "https://openalex.org/W3019533439",
        "https://openalex.org/W4281619372",
        "https://openalex.org/W1964981516",
        "https://openalex.org/W3033201772",
        "https://openalex.org/W3093821973",
        "https://openalex.org/W4220989818",
        "https://openalex.org/W4283463809",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W2973074478",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W2888329843",
        "https://openalex.org/W4309594747",
        "https://openalex.org/W2973114758",
        "https://openalex.org/W2947423323",
        "https://openalex.org/W4318262876",
        "https://openalex.org/W3003349444",
        "https://openalex.org/W2315837940",
        "https://openalex.org/W2442855760",
        "https://openalex.org/W3090892334",
        "https://openalex.org/W3174996591",
        "https://openalex.org/W2999607635",
        "https://openalex.org/W3095106934",
        "https://openalex.org/W2800793736",
        "https://openalex.org/W3024308166",
        "https://openalex.org/W3007759824",
        "https://openalex.org/W3114537677",
        "https://openalex.org/W2092368765",
        "https://openalex.org/W2883528235",
        "https://openalex.org/W2968378480",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W3115199046",
        "https://openalex.org/W3173787059",
        "https://openalex.org/W3206309200",
        "https://openalex.org/W4280529368",
        "https://openalex.org/W3112510144",
        "https://openalex.org/W3165831121",
        "https://openalex.org/W2085662862",
        "https://openalex.org/W3114168740",
        "https://openalex.org/W3098062507",
        "https://openalex.org/W3155755963",
        "https://openalex.org/W3103092523",
        "https://openalex.org/W3102176376",
        "https://openalex.org/W3092454864",
        "https://openalex.org/W3034775979"
    ],
    "abstract": "Abstract Accurate and efficient prediction of polymer properties is of great significance in polymer design. Conventionally, expensive and time-consuming experiments or simulations are required to evaluate polymer functions. Recently, Transformer models, equipped with self-attention mechanisms, have exhibited superior performance in natural language processing. However, such methods have not been investigated in polymer sciences. Herein, we report TransPolymer, a Transformer-based language model for polymer property prediction. Our proposed polymer tokenizer with chemical awareness enables learning representations from polymer sequences. Rigorous experiments on ten polymer property prediction benchmarks demonstrate the superior performance of TransPolymer. Moreover, we show that TransPolymer benefits from pretraining on large unlabeled dataset via Masked Language Modeling. Experimental results further manifest the important role of self-attention in modeling polymer sequences. We highlight this model as a promising computational tool for promoting rational polymer design and understanding structure-property relationships from a data science view.",
    "full_text": "ARTICLE OPEN\nTransPolymer: a Transformer-based language model for\npolymer property predictions\nChangwen Xu 1,2, Yuyang Wang 2,3 and Amir Barati Farimani 1,2,3,4 ✉\nAccurate and efﬁcient prediction of polymer properties is of great signiﬁcance in polymer design. Conventionally, expensive and\ntime-consuming experiments or simulations are required to evaluate polymer functions. Recently, Transformer models, equipped\nwith self-attention mechanisms, have exhibited superior performance in natural language processing. However, such methods have\nnot been investigated in polymer sciences. Herein, we report TransPolymer, a Transformer-based language model for polymer\nproperty prediction. Our proposed polymer tokenizer with chemical awareness enables learning representations from polymer\nsequences. Rigorous experiments on ten polymer property prediction benchmarks demonstrate the superior performance of\nTransPolymer. Moreover, we show that TransPolymer beneﬁts from pretraining on large unlabeled dataset via Masked Language\nModeling. Experimental results further manifest the important role of self-attention in modeling polymer sequences. We highlight\nthis model as a promising computational tool for promoting rational polymer design and understanding structure-property\nrelationships from a data science view.\nnpj Computational Materials           (2023) 9:64 ; https://doi.org/10.1038/s41524-023-01016-5\nINTRODUCTION\nThe accurate and efﬁcient property prediction is essential to the\ndesign of polymers in various applications, including polymer\nelectrolytes\n1,2, organic optoelectronics3,4, energy storage5,6, and\nmany others7,8. Rational representations which map polymers to\ncontinuous vector space are crucial to applying machine learning\ntools in polymer property prediction. Fingerprints (FPs), which\nhave been proven to be effective in molecular machine learning\nmodels, are introduced for polymer-related tasks\n9. Recently, deep\nneural networks (DNNs) have revolutionized polymer property\nprediction by directly learning expressive representations from\ndata to generate deepﬁngerprints, instead of relying on manually\nengineered descriptors\n10. Rahman et al. used convolutional neural\nnetworks (CNNs) for the prediction of mechanical properties of\npolymer-carbon nanotube surfaces\n11, whereas CNNs suffered from\nfailure to consider molecular structure and interactions between\natoms. Graph neural networks (GNNs)\n12, which have outperformed\nmany other models on several molecules and polymer bench-\nmarks\n13–17, are capable of learning representations from graphs\nand ﬁnding optimalﬁngerprints based on downstream tasks10. For\nexample, Park et al.18 trained graph convolutional neural networks\n(GCNN) for predictions of thermal and mechanical properties of\npolymers and discovered that the GCNN representations for\npolymers resulted in comparable model performance to the\npopular extended-connectivity circular ﬁngerprint (ECFP)\n19,20\nrepresentation. Recently, Aldeghi et al. adapted a graph repre-\nsentation of molecular ensembles along with a GNN architecture\nto capture pivotal features and accomplish accurate predictions of\nelectron afﬁnity and ionization potential of conjugated poly-\nmers\n21. However, GNN-based models require explicitly known\nstructural and conformational information, which would be\ncomputationally or experimentally expensive to obtain. Plus, the\ndegree of polymerization varies for each polymer, which makes it\neven harder to accurately represent polymers as graphs. Using the\nrepeating unit only as graph is likely to result in missing structural\ninformation. Therefore, the optimal method of graph representa-\ntion for polymers is still obscure.\nMeanwhile, language models, like recurrent neural networks\n(RNNs) based models\n22–25, treat polymers as character sequences\nfor featurization. Chemistry sequences have the same structure as\na natural language like English, as suggested by Cadeddu et al., in\nterms of the distribution of text fragments and molecular\nfragments\n26. This elucidates the development of sequence models\nsimilar to those in computational linguistics for extracting\ninformation from chemical sequences and realizing the intuition\nof understanding chemical texts just like understanding natural\nlanguages. Multiple works have investigated the development of\ndeep language models for polymer science. Simine et al. managed\nto predict spectra of conjugated polymers by long short-term\nmemory (LSTM) from coarse-grained representations of poly-\nmers\n27. Webb et al. proposed coarse-grained polymer genomes as\nsequences and applied LSTM to predict the properties of different\npolymer classes\n28. Patel et al. further extended the coarse-grained\nstring featurization to copolymer systems and developed GNN,\nCNN, as well as LSTM to model encoded copolymer sequences29.\nBhattacharya et al. leveraged RNNs with sequence embedding to\npredict aggregate morphology of macromolecules\n30. Plus,\nsequence models could represent molecules and polymers with\nSimpliﬁed Molecular-Input Line-Entry system (SMILES)\n31 and\nconvert the strings to embeddings for vectorization. Some works,\nlike BigSMILES32, have also investigated the string-based encoding\nof macromolecules. Goswami et al. created encodings from\npolymer SMILES as input for the LSTM model for polymer glass\ntransition temperature prediction\n33. However, RNN-based models\nare generally not competitive enough to encode chemical\nknowledge from polymer sequences because they rely on\nprevious hidden states for dependencies between words and\ntend to lose information when they reach deeper steps. In recent\n1Department of Materials Science and Engineering, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh 15213 PA, USA.2Department of Mechanical Engineering, Carnegie\nMellon University, 5000 Forbes Ave, Pittsburgh 15213 PA, USA.3Machine Learning Department, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh 15213 PA, USA.\n4Department of Chemical Engineering, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh 15213 PA, USA.✉email: barati@cmu.edu\nwww.nature.com/npjcompumats\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\n1234567890():,;\nyears, the exceptionally superior performance demonstrated by\nTransformer34 on numerous natural language processing (NLP)\ntasks has shed light on studying chemistry and materials science\nby language models. Since proposed, Transformer and its variants\nhave soon brought about signiﬁcant changes in NLP tasks over\nthe past few years. Transformer is featured with using attention\nmechanism only so that it can capture relationships between\ntokens in a sentence without relying on past hidden states. Many\nTransformer-based models like BERT\n35, RoBERTa36, GPT37, ELMo38,\nand XLM39 have emerged as effective pretraining methods by self-\nsupervised learning of representations from unlabeled texts,\nleading to performance enhancement on various downstream\ntasks. On this account, many works have already applied\nTransformer on property predictions of small organic mole-\ncules\n40–43. SMILES-BERT was proposed to pretrain the model of\nBERT-like architecture through a masked SMILES recovery task and\nthen generalize into different molecular property prediction\ntasks\n44. Similarly, ChemBERTa45, a RoBERTa-like model for mole-\ncular property prediction, was also introduced, following the\npretrain-ﬁnetune pipeline. ChemBERTa demonstrated competitive\nperformance on multiple downstream tasks and scaled well with\nthe size of pretraining datasets. Transformer-based models could\neven be used for processing reactions. Schwaller et al. mimicked\nmachine translation tasks and trained Transformer on reaction\nsequences represented by SMILES for reaction prediction with\nhigh accuracy\n46. Recently, Transformer has been further proven to\nbe effective as a structure-agnostic model in material science\ntasks, for example, predicting MOF properties based on a text\nstring representation\n47. Despite the wide investigation of Trans-\nformer for molecules and materials, such models have not yet\nbeen leveraged to learn representations of polymers. Compared\nwith small molecules, designing Transformer-based models for\npolymers is more challenging because the standard SMILES\nencoding fails to model the polymer structure and misses\nfundamental factors inﬂuencing polymer properties like degree\nof polymerization and temperature of measurement. Moreover,\nthe polymer sequences used as input should contain information\non not only the deﬁnition of monomers but also the arrangement\nof monomers in polymers\n48. In addition, sequence models for\npolymers are confronted with an inherent scarcity of handy, well-\nlabeled data, considering the hard work in the characterization\nprocess in the laboratory. The situation becomes even worse\nwhen some of the polymer data sources are not fully\naccessible\n49,50.\nHerein, we propose TransPolymer, a Transformer-based language\nmodel for polymer property predictions. To the best of our\nknowledge, it is theﬁrst work to introduce the Transformer-based\nmodel to polymer sciences. Polymers are represented by sequences\nbased on SMILES of their repeating units as well as structural\ndescriptors and then tokenized by a chemically-aware tokenizer as\nthe input of TransPolymer, shown in Fig.1a. Even though there is still\ninformation which cannot be exp licitly obtained from input\nsequences, like bond angles or overall polymer chain conﬁguration,\nsuch information can still be learned implicitly by the model.\nTransPolymer consists of a RoBERTaarchitecture and a multi-layer\nperceptron (MLP) regressor head, for predictions of various polymer\nproperties. In the pretraining phase, TransPolymer is trained through\nMasked Language Modeling (MLM) with approximately 5M aug-\nmented unlabeled polymers from the PI1M database\n51.I nM L M ,\ntokens in sequences are randomly masked and the objective is to\nrecover the original tokens based on the contexts. Afterward,\nTransPolymer isﬁnetuned and evaluated on ten datasets of polymers\nconcerning various properties, covering polymer electrolyte con-\nductivity, band gap, electron afﬁnity, ionization energy, crystallization\ntendency, dielectric constant, refractive index, and p-type polymer\nOPV power conversion efﬁciency\n52–55. For each entry in the datasets,\nthe corresponding polymer sequence, containing polymer SMILES as\nwell as useful descriptors like temperature and special tokens are\ntokenized as input of TransPolymer. The pretraining andﬁnetuning\nprocesses are illustrated in Fig.1b and d. Data augmentation is also\nimplemented for better learning of features from polymer sequences.\nTransPolymer achieves state-of-the-art (SOTA) results on all ten\nbenchmarks and surpasses other baseline models by large margins\nin most cases. Ablation studies provide further evidence of what\ncontributes to the superior performance of TransPolymer by\ninvestigating the roles of MLM pretraining on large unlabeled data,\nﬁnetuning both Transformer encoders and the regressor head, and\ndata augmentation. The evidence from visualization of attention\nscores illustrates that TransPolymer can encode chemical information\nabout internal interactions of polymers and inﬂuential factors of\npolymer properties. Such a method learns generalizable features that\ncan be transferred to property prediction of polymers, which is of\ngreat signiﬁcance in polymer design.\nRESULTS\nTransPolymer framework\nOur TransPolymer framework consists of tokenization, Transformer\nencoder, pretraining, and ﬁnetuning. Each polymer data is ﬁrst\nconverted to a string of tokens through tokenization. Polymer\nsequences are more challenging to design than molecule or\nprotein sequences as polymers contain complex hierarchical\nstructures and compositions. For instance, two polymers that\nhave the same repeating units can vary in terms of the degree of\npolymerization. Therefore, we propose a chemical-aware polymer\ntokenization method as shown in Fig.1a. The repeating units of\npolymers are embedded using SMILES and additional descriptors\n(e.g., degree of polymerization, polydispersity, and chain con-\nformation) are included to model the polymer system. Plus,\ncopolymers are modeled by combining the SMILES of each\nconstituting repeating unit along with the ratios and the\narrangements of those repeating units. Moreover, materials\nconsisting of mixtures of polymers are represented by concate-\nnating the sequences for each component as well as the\ndescriptors for the materials. Besides, each token represents\neither an element, the value of a polymer descriptor, or a special\nseparator. Therefore, the tokenization strategy is chemical-aware\nand thus has an edge over the tokenizer trained for natural\nlanguages which tokenizes based on single letters. More details\nabout the design of our chemical-aware tokenization strategy\ncould be found in the Methods section.\nTransformer encoders are built upon stacked self-attention and\npoint-wise, fully connected layers\n34, shown in Fig.1c. Unlike RNN\nor CNN models, Transformer depends on the self-attention\nmechanism that relates tokens at different positions in a sequence\nto learn representations. Scaled dot-product attention across\ntokens is applied which relies on the query, key, and value\nmatrices. More details about self-attention can be found in the\nMethods section. In our case, the Transformer encoder is made up\nof 6 hidden layers and each hidden layer contains 12 attention\nheads. The hyperparameters of TransPolymer are chosen by\nstarting from the common setting of RoBERTa\n36 and then tuned\naccording to model performance.\nTo learn better representations from large unlabeled polymer\ndata, the Transformer encoder is pretrained via Masked Language\nModeling (MLM), a universal and effective pretraining method for\nvarious NLP tasks\n56–58. As shown in Fig.1d (left), 15% of tokens of\na sequence are randomly chosen for possible replacement, and\nthe pretraining objective is to predict the original tokens by\nlearning from the contexts. The pretrained model is then\nﬁnetuned for predicting polymer properties with labeled data.\nParticularly, theﬁnal hidden vector of the special token‘〈s〉’at the\nbeginning of the sequence is fed into a regressor head which is\nmade up of one hidden layer with SiLU as the activation function\nfor prediction as illustrated in Fig.1d (right).\nC. Xu et al.\n2\nnpj Computational Materials (2023)    64 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\n1234567890():,;\nExperimental settings\nPI1M, the benchmark of polymer informatics, is used for pretraining.\nThe benchmark, whose size is around 1M, was built by Ma et al. by\ntraining a generative model on polymer data collected from the\nPolyInfo database\n51,59. The generated sequences consist of monomer\nSMILES and‘*’signs representing the polymerization points. The ~1M\ndatabase was demonstrated to cover similar chemical space as\nPolyInfo but populate space where data in PolyInfo are sparse.\nTherefore, the database can serve as an important benchmark for\nmultiple tasks in polymer informatics.\nTo ﬁnetune the pretrained TransPolymer, ten datasets are used\nin our experiments which cover various properties of different\nFig. 1 Overview of TransPolymer.(a) Polymer tokenization. Illustrated by the example, the sequence which comprises components with\npolymer SMILES and other descriptors is tokenized with chemical awareness.b The whole TransPolymer framework with a pretrain-ﬁnetune\npipeline. c Sketch of Transformer encoder and multi-head attention.d Illustration of the pretraining (left) andﬁnetuning (right) phases of\nTransPolymer. The model is pretrained with Masked Language Modeling to recover original tokens, while the feature vector corresponding to\nthe special token‘〈s〉’ of the last hidden layer is used for prediction whenﬁnetuning. Within the TransPolymer block, lines of deeper color and\nlarger width stand for higher attention scores.\nC. Xu et al.\n3\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2023)    64 \npolymer materials, and the distributions of polymer sequence\nlengths vary from each other (shown in Supplementary Fig. 1).\nPlus, data in all the datasets are of different types: sequences from\nEgc, Egb, Eea, Ei, Xc, EPS, and Nc datasets are about polymers only\nso that the inputs are just polymer SMILES; while PE-I, PE-II, and\nOPV datasets describe polymer-based materials so that the\nsequences contain additional descriptors. In particular, PE-I which\nis about polymer electrolytes involves mixtures of multiple\ncomponents in polymer materials. Hence, these datasets provide\nchallenging and comprehensive benchmarks to evaluate the\nperformance of TransPolymer. A summary of the ten datasets for\ndownstream tasks is shown in Table1.\nWe apply data augmentation to each dataset that we use by\nremoving canonicalization from SMILES and generating non-\ncanonical SMILES which correspond to the same structure as the\ncanonical ones. For PI1M database, each data entry is augmented\nto ﬁve so that the augmented dataset with the size of ~5M is used\nfor pretraining. For downstream datasets, we limit the numbers of\naugmented SMILES for large datasets with long SMILES for the\nfollowing reasons: long SMILES tend to generate more non-\ncanonical SMILES which might alter the original data distribution;\nwe are not able to use all the augmented data forﬁnetuning given\nthe limited computation resources. We include the number of\ndata points after augmentation in Table 1 and summarize the\naugmentation strategy for each downstream dataset in Supple-\nmentary Table 1.\nPolymer property prediction results\nThe performance of our pretrained TransPolymer model on ten\nproperty prediction tasks is illustrated below. We use root mean\nsquare error (RMSE) and R\n2 as metrics for evaluation. For each\nbenchmark, the baseline models and data splitting are adopted\nfrom the original literature. Except for PE-I which is trained on\ndata from the year 2018 and evaluated on data from the year\n2019, all other datasets are split by ﬁve-fold cross-validation.\nWhen cross-validation is used, the metrics are calculated by\ntaking the average of those by each fold. We also train Random\nForest models using Extended Connectivity Fingerprint\n(ECFP)\n19,20, one of the state-of-the-art ﬁngerprint approaches,\nto compare with TransPolymer. Besides, we develop long short-\nterm memory (LSTM), another widely used language model, as\nwell as unpretrained TransPolymer trained purely via super-\nvised learning as baseline models in all the benchmarks.\nTransPolymer\nunpretrained and TransPolymer pretrained denote\nunpretrained and pretrained TransPolymer, respectively.\nThe results of TransPolymer and baselines on PE-I are illustrated\nin Table 2. The original literature used gated GNN to generate\nﬁngerprints for the prediction of polymer electrolyte conductivity\nby Gaussian Process53. Theﬁngerprints are also passed to random\nforest and supporting vector machine (SVM) for comparison.\nAnother random forest is trained based on ECFPﬁngerprints. The\nresults of most baseline models indicate strong overﬁtting which\nis attributed to the introduction of unconventional conductors\nconsisting of conjugated polybenzimidazole and ionic liquid. For\ninstance, Gaussian Process trained on GNNﬁngerprints achieves a\nR\n2 of 0.90 on the training set but only 0.16 on the test set, and\nRandom Forest trained on GNN FP gets a negative testR2 even the\ntrain R2 is 0.91. Random Forest trained on ECFP stands out among\nall the baseline models, whereas its performance on test dataset is\nstill poor. However, TransPolymerpretrained not only achieves the\nhighest scores on the training set but also improves the\nperformance on the test set signiﬁcantly, which is illustrated by\nthe R2 of 0.69 on the test set. Such information demonstrates that\nTransPolymer is capable of learning the intrinsic relationship\nbetween polymers and their properties and suffers less from\noverﬁtting. Notably, TransPolymer\nunpretrained also achieves compe-\ntitive results and shows mild over ﬁtting compared with other\nbaseline models. This indicates the effectiveness of the attention\nmechanism of Transformer-based models. The scatter plots of\nground truth vs. predicted values for PE-I by TransPolymerpretrained\nare illustrated in Fig.2a and Supplementary Fig. 2a.\nAs is shown in Table 3, the results of TransPolymer and\nbaselines including Ridge, Random Forest, Gradient Boosting, and\nExtra Trees which were trained on chemical descriptors generated\nfrom polymers from PE-II in the original paper52 are listed, as well\nas Random Forest trained on ECFP. Although Gradient Boosting\nsurpasses other models on training sets by obtaining nearly\nperfect regression outcomes, its performance on test sets drops\nsigniﬁcantly. In contrast, TransPolymerpretrained, which achieves the\nlowest RMSE of 0.61 and highest R2 of 0.73 on the average of\ncross-validation sets, exhibits better generalization. The scatter\nTable 1. Summary of datasets for downstream tasks.\nDataset Property # Data # Augmented train data # Test data Data split\nPE-I53 conductivity 9185 34803 146 train-test split by year\nPE-II52 conductivity 271 8864 55 5-fold cross-validation\nEgc54 bandgap (chain) 3380 5408 676 5-fold cross-validation\nEgb54 bandgap (bulk) 561 6443 113 5-fold cross-validation\nEea54 electron afﬁnity 368 3993 74 5-fold cross-validation\nEi54 ionization energy 370 4000 74 5-fold cross-validation\nXc54 crystallization tendency 432 8837 87 5-fold cross-validation\nEPS54 dielectric constant 382 4188 77 5-fold cross-validation\nNc54 refractive index 382 4188 77 5-fold cross-validation\nOPV55 power conversion efﬁciency 1203 4810 241 5-fold cross-validation\nTable 2. Performance of TransPolymer and baseline models on PE-I.\nModel Train RMSE\n(S ⋅ cm−1*) (↓)\nTest RMSE\n(S ⋅ cm−1*) (↓)\nTrain\nR2 (↑)\nTest\nR2 (↑)\nGaussian Process\n(GNN FP)\n0.55 0.97 0.90 0.16\nRandom Forest\n(GNN FP)\n0.50 2.23 0.91 −2.64\nSVM (GNN FP) 1.34 2.12 0.04 −1.94\nRandom Forest (ECFP) 0.15 1.00 0.99 0.32\nLSTM 1.03 1.36 0.67 −0.25\nTransPolymer\nunpretrained 0.88 1.02 0.70 0.30\nTransPolymerpretrained 0.20 0.67 0.98 0.69\n*The units are in logarithm scale.\nThe bold values indicate the best results in terms of the metrics we use.\nC. Xu et al.\n4\nnpj Computational Materials (2023)    64 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nFig. 2 Ground truth vs. predicted values by TransPolymerpretrained. Scatter plots of ground truth vs. predicted values for downstream tasks:\na PE-I, b PE-II, c Egc, d Egb, e Eea, f Ei, g Xc, h EPS, i Nc, and j OPV. The dashed lines on diagonals stand for perfect regression.\nC. Xu et al.\n5\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2023)    64 \nplots of ground truth vs. predicted values for PE-II by TransPoly-\nmerpretrained are illustrated in Fig.2b and Supplementary Fig. 2b.\nTable 4 summarizes the performance of TransPolymer and\nbaselines on Egc, Egb, Eea, Ei, Xc, EPS, and Nc datasets from\nKuenneth et al.54. In the original literature, both Gaussian process and\nneural networks were trained on each dataset with polymer genome\n(PG) ﬁngerprints60 as input, some of which resulted in desirable\nperformance while some of which did not. Meanwhile, PG\nﬁngerprints are demonstrated to surpass ECFP on the datasets used\nby Kuenneth et al. For Egc, Egb, and Eea, despite the high scores by\nother models, TransPolymerpretrained is still able to enhance the\nperformance, lowering RMSE and enhancingR2. In contrast, baseline\nmodels perform poorly on Xc whose testR2 scores are less than 0.\nHowever, TransPolymerpretrained signiﬁcantly lowers test RMSE and\nincreases R2 to 0.50. Notably, The authors of the original paper used\nmulti-task learning to enhance model performance and achieved\nhigher scores than TransPolymerpretrained on some of the datasets,\nlike Egb, EPS, and Nc (the average test RMSE andR2 are 0.43 and 0.95\nfor Egb, 0.39 and 0.86 for EPS, and 0.07 and 0.91 for Nc, respectively).\nAccess to multiple properties of one polymer, however, may not be\navailable from time to time, which limits the application of multi-task\nlearning. In addition, the TransPolymerpretrained still outperforms\nmulti-task learning models on fourout of the seven chosen datasets.\nHence the improvement by TransPolymer compared with single-task\nbaselines should still be highly valued. The scatter plots of ground\ntruth vs. predicted values for Egc, Egb, Eea, Ei, Xc, EPS, and Nc\ndatasets by TransPolymer\npretrained are depicted in Fig. 2c–ia n d\nSupplementary Fig. 2c–i, respectively.\nTable 3. Performance of TransPolymer and baseline models on PE-II.\nModel Train RMSE( S ⋅ cm−1*) (↓) Test RMSE ( S ⋅ cm−1*) (↓) Train R2 (↑) Test R2 (↑)\nRidge (Chemical descriptors) 0.58 0.67 0.77 0.58\nRandom Forest (Chemical descriptors) 0.26 0.64 0.96 0.71\nGradient Boosting (Chemical descriptors) 0.00 0.66 0.99 0.68\nExtra Trees (Chemical descriptors) 0.10 0.63 0.98 0.72\nRandom Forest (ECFP) 0.22 0.94 0.96 0.27\nLSTM 1.16 1.18 0.05 0.00\nTransPolymer\nunpretrained 0.18 0.80 0.97 0.54\nTransPolymerpretrained 0.18 0.61 0.96 0.73\n*The units are in logarithm scale.\nThe bold values indicate the best results in terms of the metrics we use.\nTable 4. Performance of TransPolymer and baseline models on datasets from literature by Kuenneth et al.54.\nModel Test RMSE ( ↓) Test R2 (↑)\nEgc (eV) Egb (eV) Eea (eV) Ei (eV) Xc (%) EPS Nc Egc Egb Eea Ei Xc EPS Nc\nGaussian Process (PG) 0.48 0.55 0.32 0.42 24.42 0.53 0.10 0.90 0.91 0.90 0.77 <0 0.68 0.79\nNeural Network (PG) 0.49 0.57 0.32 0.45 20.74 0.54 0.10 0.89 0.89 0.87 0.74 <0 0.71 0.78\nRandom Forest (ECFP) 0.81 0.88 0.56 0.58 25.61 0.75 0.14 0.65 0.66 0.70 0.57 −0.29 0.50 0.56\nLSTM 0.58 1.94 1.04 0.94 23.67 1.11 0.23 0.86 0.00 0.06 0.10 0.00 −0.02 0.02\nTransPolymerunpretrained 0.63 0.61 0.36 0.46 20.11 0.59 0.10 0.84 0.90 0.89 0.78 0.27 0.70 0.80\nTransPolymerpretrained 0.44 0.52 0.32 0.39 16.57 0.52 0.10 0.92 0.93 0.91 0.84 0.50 0.76 0.82\nThe bold values indicate the best results in terms of the metrics we use.\nTable 5. Performance of TransPolymer and baseline models on p-type\npolymer OPV.\nModel Train RMSE\n(%) (↓)\nTest RMSE\n(%) (↓)\nTrain R2\n(↑)\nTest R2\n(↑)\nRandom Forest (ECFP) 0.66 1.92 0.92 0.27\nANN (ECFP) 1.58 2.03 0.55 0.20\nLSTM 2.35 2.34 −0.01 0.00\nTransPolymer\nunpretrained 1.91 2.10 0.33 0.19\nTransPolymerpretrained 1.19 1.92 0.74 0.32\nThe bold values indicate the best results in terms of the metrics we use.\nTable 6. Improvement of performance of TransPolymerpretrained\ncompared with baselines and TransPolymerunpretrained in terms of\ndecrease of test RMSE (in percentage) and increase of testR2 (in\nabsolute value).\nDataset vs. best baselines vs. TransPolymer unpretrained\nRMSE (↓) R2 (↑) RMSE ( ↓) R2 (↑)\nPE-I −30.9% +0.37 −52.2% +0.39\nPE-II −3.17% +0.01 −23.8% +0.19\nEgc −8.33% +0.02 −30.2% +0.08\nEgb −5.45% +0.02 14.8% +0.03\nEea 0.00% +0.01 −11.1% +0.02\nEi −7.14% +0.07 −15.2% +0.06\nXc −20.1% +0.50 −17.6% +0.23\nEPS −1.89% +0.05 −11.9% +0.06\nNc 0.00% +0.03 0.00% +0.02\nOPV 0.00% +0.05 −8.57% +0.13\nAverage −7.70% +0.11 −18.5% +0.12\nC. Xu et al.\n6\nnpj Computational Materials (2023)    64 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nTransPolymer and baselines are trained on p-type polymer OPV\ndataset whose results are shown in Table5. The original paper\ntrained random forest and artiﬁcial neural network (ANN) on the\ndataset using ECFP55. TransPolymerpretrained, in comparison with\nbaselines, gives a slightly better performance as the average RMSE\nis the same as that of random forest, and the average testR2 is\nincreased by 0.05. Although all the model performance is not\nsatisfying enough, possibly attributed to the noise in data,\nTransPolymer\npretrained still outperforms baselines. The scatter plots\nof ground truth vs. predicted values for OPV by TransPolymerpre-\ntrained are depicted in Fig. 2j and Supplementary Fig. 2j.\nTable 6 summarizes the improvement of TransPolymerpretrained\nover the best baseline models as well as TransPolymerunpretrained\non each dataset. TransPolymerpretrained has outperformed all other\nmodels on all ten datasets, further providing evidence for the\ngeneralization of TransPolymer. TransPolymer\npretrained exhibits an\naverage decrease of evaluation RMSE by 7.70% (in percentage)\nand an increase of evaluation R\n2 by 0.11 (in absolute value)\ncompared with the best baseline models, and the two values\nbecome 18.5% and 0.12, respectively, when it comes to\ncomparison with TransPolymer\nunpretrained. Therefore, the pre-\ntrained TransPolymer could hopefully be a universal pretrained\nmodel for polymer property prediction tasks and applied to other\ntasks by ﬁnetuning. Besides, TransPolymer equipped with MLM\npretraining technique shows signiﬁcant advantages over other\nmodels in dealing with complicated polymer systems. Speciﬁcally,\non PE-I benchmark, TransPolymer\npretrained improves R2 by 0.37\ncomparing with the previous best baseline model and by 0.39\ncomparing with TransPolymerunpretrained. PE-I contains not only\npolymer SMILES but also key descriptors of the materials like\ntemperature and component ratios within the materials. The data\nin PE-I is noisy due to the existence of different types of\ncomponents in the polymer materials, for instance, copolymers,\nanions, and ionic liquids. Also, models are trained on data from the\nyear 2018 and evaluated on data from the year 2019, which gives\na more challenging setting. Therefore it is reasonable to infer that\nTransPolymer is better at learning features out of noisy data and\ngiving a robust performance. It is noticeable that LSTM becomes\nthe least competitive model in almost every downstream task,\nsuch evidence demonstrates the signi ﬁcance of attention\nmechanisms in understanding chemical knowledge from polymer\nsequences.\nAbaltion studies\nThe effects of pretraining could be further demonstrated by the\nchemical space taken up by polymer SMILES from the pretraining\nand downstream datasets visualized by t-SNE\n61, shown in Fig.3.\nEach polymer SMILES is converted to TransPolymer embedding\nwith the size of sequence length × embedding size. Max pooling is\nimplemented to convert the embedding matrices to vectors so\nthat the strong characteristics in embeddings could be preserved\nin the input of t-SNE. We use openTSNE library\n62 to create 2D\nembeddings via pretraining data and map downstream data to\nthe same 2D space. As illustrated in Fig. 3a, almost every\ndownstream data point lies in the space covered by the original\n~1M pretraining data points, indicating the effectiveness of\npretraining in better representation learning of TransPolymer.\nData points from datasets like Xc which exhibit minor evidence of\nclustering in the chemical space cover a wide range of polymers,\nexplaining the phenomenon that other models struggle on Xc\nwhile pretrained TransPolymer learns reasonable representations.\nMeanwhile, for datasets that cluster in the chemical space, other\nmodels can obtain reasonable results whereas TransPolymer\nachieves better results. Additionally, it should be pointed out that\nthe numbers of unique polymer SMILES in PE-I and PE-II are much\nsmaller than the sizes of the datasets as many instances share the\nsame polymer SMILES while differing in descriptors like molecular\nweight and temperature, hence the visualization of polymer\nSMILES cannot fully reﬂect the chemical space taken up by the\npolymers from these datasets.\nBesides, we have also investigated how the size of the\npretraining dataset affects the downstream performance. We\nrandomly pick up 5K, 50K, 500K, and 1M (original size) data points\nfrom the initial pretraining dataset without augmentation, and\npretrain TransPolymer with them and compare the results with\nthose by TransPolymer trained with 5M augmented data. The\nresults are summarized in Supplementary Table 5. Plus, Fig. 4\npresents the bar plot of R\n2 for each experiment we have\nperformed. Error bars are included in the ﬁgure if cross-\nvalidation is implemented in experiments. As shown in the table\nand theﬁgure, the results demonstrate a clear trend of enhanced\ndownstream performance (decreasing RMSE and increasing R\n2)\nwith increasing pretraining size. In particular, the model perfor-\nmance on some datasets, for example, PE-I, Nc, and OPV, are even\nworse than training TransPolymer from scratch (the results by\nTransPolymer\nunpretrained in Tables 2–5). A possible explanation is\nthat the small amount of pretraining size results in the limited\ndata space covered by pretraining data, thus making some\ndownstream data points out of the distribution of pretraining\ndata. Figure3b, c visualize the data space byﬁtting on 50K and 5K\npretraining data, respectively, in which a lot of space taken up\ndownstream data points is not covered by pretraining data.\nTherefore, the results emphasize the effects of pretraining with a\nlarge number of unlabeled sequences.\nThe results from TransPolymer\npretrained so far are all derived by\npretraining ﬁrst and then ﬁnetuning the whole model on the\ndownstream datasets. Besides, we also consider another setting\nwhere in downstream tasks only the regressor head isﬁnetuned\nwhile the pretrained Transformer encoder is frozen. The compar-\nison of the performance of TransPolymer\npretrained between\nﬁnetuning the regressor head only and ﬁnetuning the whole\nmodel is presented in Table7. Standard deviation is included in\nthe results if cross-validation is applied for downstream tasks.\nReasonable results could be obtained by freezing the pretrained\nencoders and training the regressor head only. For instance, the\nmodel performance on Xc dataset already surpasses the baseline\nmodels, and the model performance on Ei, Nc, and OPV datasets is\nslightly worse than the corresponding best baselines. However,\nthe performance on all the downstream tasks increases signiﬁ-\ncantly if both the Transformer encoders and the regressor head\nare ﬁnetuned, which indicates that the regressor head only is not\nenough to learn task-speciﬁc information. In fact, the attention\nmechanism plays a key role in learning not only generalizable but\nalso task-speci ﬁc information. Even though the pretrained\nTransPolymer is transferable to various downstream tasks and\nmore ef ﬁcient, it is necessary to ﬁnetune the Transformer\nencoders with task-related data points for better performance.\nData augmentation is implemented not only in pretraining but\nalso in ﬁnetuning. The comparison between the model perfor-\nmance on downstream tasks with pretraining on the original ~1M\ndataset and the augmented ~5M dataset (shown in Supplemen-\ntary Table 5) has already demonstrated the signiﬁcance of data\naugmentation in model performance enhancement. In this part,\nwe use the model pretrained on the ~5M augmented pretraining\ndataset but ﬁnetune TransPolymer without augmenting the\ndownstream datasets to investigate to what extent the TransPo-\nlymer model can improve the best baseline models for down-\nstream tasks. The model performance enhancement with or\nwithout data augmentation compared with best baseline models\nis summarized in Table 8. For most downstream tasks, TransPo-\nlymer\npretrained can improve model performance without data\naugmentation, while such improvement would become more\nsigniﬁcant if data augmentation is applied. For PE-II dataset,\nhowever, TransPolymerpretrained is not comparable to the best\nbaseline model without data augmentation since the original\nC. Xu et al.\n7\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2023)    64 \ndataset contains only 271 data points in total. Because of the data-\ngreedy characteristics of Transformer, data augmentation could be\na crucial factor in ﬁnetuning, especially when data are scarce\n(which is very common in chemical and materials science\nregimes). Therefore, data augmentation can help generalize the\nmodel to sequences unseen in training data.\nSelf-attention visualization\nA t t e n t i o ns c o r e s ,s e r v i n ga sa ni n d i c a t o ro fh o wc l o s e l yt w o\ntokens align with each other, could be used for understanding\nhow much chemical knowledge TransPolymer learns from\npretraining and how each token contributes to the prediction\nresults. Take poly(ethylene oxide) (*CCO*), which is one of the\nmost prevailing polymer elect rolytes, as an example. The\nattention scores between each token in the ﬁrst and last\nhidden layer are shown in Fig. 5a and b, respectively. The\nattention score matrices of 12 attention heads generated from\nthe ﬁrst hidden layer indicate strong relationships between\ntokens in the neighborhood, which could be inferred from the\nemergence of high attention scores around the diagonals of\nmatrices. This trend makes sense because the nearby tokens in\npolymer SMILES usually represent atoms bonded to each other\nin the polymer, and atoms are most signi ﬁcantly affected by\ntheir local environments. Therefore,i the ﬁrst hidden layer,\nwhich is the closest layer to inputs, could capture such chemical\ninformation. In contrast, the attention scores from the last\nhidden layer tend to be more uniform, thus lacking an\ninterpretable pattern. Such phenomenon has also been\nobserved by Abnar et al. who discovered that the embeddings\nof tokens would become contextualized for deeper hidden\nlayers and might carry similar information\n63.\nWhen ﬁnetuning TransPolymer, the vector of the special token\n‘〈s〉’ from the last hidden state is used for prediction. Hence, to\ncheck the impacts of tokens on prediction results, the attention\nscores between ‘〈s〉’and other tokens from all 6 hidden layers in\neach attention head are illustrated with the example of the PEC-\nPEO blend electrolyte coming from PE-II whose polymer SMILES is\n‘*COC(=O)OC*.*CCO*’. In addition to polymer SMILES, the\nFig. 3 t-SNE visualization of pretraining and downstream data.The embeddings are obtained byﬁrst ﬁtting on the (a) 1M (original), (b) 50K,\nand (c) 5K pretraining data and then transforming downstream data to the corresponding data space.\nC. Xu et al.\n8\nnpj Computational Materials (2023)    64 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nsequence also includes‘F[B-](F)(F)F’, ‘0.17’, ‘95.2’, ‘37.0’, ‘−23’, and\n‘S_1’ which stand for the anion in the electrolyte, the ratio\nbetween lithium ions and functional groups in the polymer,\ncomonomer percentage, molecular weight (kDa), glass transition\ntemperature (T\ng), and linear chain structure, respectively. As is\nillustrated in Fig.6, the‘〈s〉’token tends to focus on certain tokens,\nlike ‘*’, ‘$’, and ‘−23’, which are marked in red in the example\nsequence in Fig. 6. Since Tg usually plays an important role in\ndetermining the conductivity of polymers 64, the ﬁnetuned\nTranspolyemr could understand the inﬂuential parts on properties\nin a polymer sequence. However, it is also widely argued that the\nattention weights cannot fully depict the relationship between\ntokens and prediction results because a high attention score does\nnot necessarily guarantee that the pair of tokens is important to\nthe prediction results given that attention scores do not consider\nValue matrices\n65. More related work is needed to fully address the\nattention interpretation problem.\nDISCUSSION\nIn summary, we have proposed TransPolymer, a Transformer-\nbased model with MLM pretraining, for accurate and ef ﬁcient\npolymer property prediction. By rationally designing a polymer\ntokenization strategy, we can map a polymer instance to a\nsequence of tokens. Data augmentation is implemented to\nenlarge the available data for representation learning. TransPoly-\nmer is ﬁrst pretrained on approximately 5M unlabeled polymer\nsequences by MLM, then ﬁnetuned on different downstream\ndatasets, outperforming all the baselines and unpretrained\nTransPolymer. The superior model performance could be further\nexplained by the impact of pretraining with large unlabeled data,\nﬁnetuning Transformer encoders, and data augmentation for data\nspace enlargement. The attention scores from hidden layers in\nTransPolymer provide evidence of the ef ﬁcacy of learning\nrepresentations with chemical awareness and suggest the\ninﬂuential tokens onﬁnal prediction results.\nGiven the desirable model performance and outstanding\ngeneralization ability out of a small number of labeled down-\nstream data, we anticipate that TransPolymer would serve as a\npotential solution to predicting newly designed polymer proper-\nties and guiding polymer design. For example, the pretrained\nTransPolymer could be applied in the active-learning-guided\npolymer discovery framework\n66,67, in which TransPolymer serves\nto virtually screen the polymer space, recommend the potential\ncandidates with desirable properties based on model predictions,\nand get updated by learning on data from experimental\nevaluation. In addition, the outstanding performance of TransPo-\nlymer on copolymer datasets compared with existing baseline\nmodels has shed light on the exploration of copolymers. In a\nnutshell, even though the main focus of this paper is placed on\nregression, TransPolymer can pave the way for several promising\n(co)polymer discovery frameworks.\nMETHODS\nPolymer tokenization\nUnlike small molecules which are easily represented by SMILES,\npolymers are more complex to be converted to sequences since\nSMILES fails to incorporate pivotal information like connectivity\nbetween repeating units and degree of polymerization. As a result,\nwe need to design the polymer sequences to take account of that\ninformation. To design the polymer sequences, each repeating\nunit of the polymer isﬁrst recognized and converted to SMILES,\nFig. 4 Model performance with varying pretraining data sizes.\nThe R2 for each downstream task with different pretraining data\nsizes are presented in the bar plot. Error bars are included if cross-\nvalidation is implemented.\nTable 7. Comparison of performance of TransPolymerpretrained\nbetween ﬁnetuning the regressor head only andﬁnetuning the whole\nmodel in terms of test RMSE andR2.\nDataset Finetuning the\nregressor head\nFinetuning the\nwhole model\nRMSE R2 RMSE R2\nPE-I 1.14 0.12 0.67 0.69\nPE-II 0.91 ± 0.12 0.40 ± 0.10 0.61 ± 0.07 0.73 ± 0.04\nEgc 0.69 ± 0.03 0.81 ± 0.02 0.44 ± 0.01 0.92 ± 0.00\nEgb 0.83 ± 0.05 0.81 ± 0.02 0.52 ± 0.05 0.93 ± 0.01\nEea 0.52 ± 0.04 0.76 ± 0.04 0.32 ± 0.02 0.91 ± 0.03\nEi 0.51 ± 0.05 0.73 ± 0.05 0.39 ± 0.07 0.84 ± 0.06\nXc 19.18 ± 2.15 0.34 ± 0.10 16.57 ± 0.68 0.50 ± 0.06\nEPS 0.72 ± 0.09 0.58 ± 0.07 0.52 ± 0.07 0.76 ± 0.11\nNc 0.13 ± 0.02 0.70 ± 0.06 0.10 ± 0.02 0.82 ± 0.07\nOPV 2.04 ± 0.06 0.24 ± 0.03 1.92 ± 0.06 0.32 ± 0.05\nTable 8. Improvement of performance of TransPolymerpretrained\nwithout and with data augmentation inﬁnetuning compared with\nbest baselines in terms of decrease of test RMSE (in percentage) and\nincrease of testR2 (in absolute value).\nDataset No Augmentation Augmentation\nRMSE (↓) R2 (↑) RMSE ( ↓) R2 (↑)\nPE-I −15.5% +0.22 −30.9% +0.37\nPE-II +22.2% −0.15 −3.17% +0.01\nEgc −4.17% +0.01 −8.33% +0.02\nEgb +9.09% −0.02 −5.45% +0.02\nEea +9.38% 0.00 0.00% +0.01\nEi 0.00% +0.03 −7.14% +0.07\nXc −14.5% +0.43 −20.1% +0.50\nEPS +7.55% 0.00 −1.89% +0.05\nNc 0.00% +0.02 0.00% +0.03\nOPV +0.52% +0.03 0.00% +0.05\nAverage +1.46% +0.06 −7.70% +0.11\nC. Xu et al.\n9\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2023)    64 \nFig. 5 Visualization of attention scores from pretrained TransPolymer. aAttention scores in theﬁrst hidden layer.b Attention scores in the\nlast hidden layer.\nC. Xu et al.\n10\nnpj Computational Materials (2023)    64 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nthen ‘*’signs are added at the places which represent the ends of\nthe repeating unit to indicate the connectivity between repeating\nunits. Such a strategy to indicate repeating units has been widely\nused in string-based polymer representations\n68,69. For the cases of\ncopolymers, ‘.’is used to separate different constituents, and‘^’is\nused to indicate branches in copolymers. Other information like\nthe degree of polymerization and molecular weight, if accessible,\nwill be put after the polymer SMILES separated by special tokens.\nTake the example of the sequence given in Fig.1a, the sequence\ndescribes a polymer electrolyte system including two components\nseparated by the special token ‘∣’. Descriptors like the ratio\nbetween repeating units in the copolymer, component type, and\nglass transition temperature (T\ng for short) are added for each\ncomponent separated by‘$’, and the ratio between components\nand temperature are put at the end of the sequence. Adding these\ndescriptors can improve the performance of property predictions\nas suggested by Patel et al.\n29. Unique ‘NAN’ tokens are assigned\nfor missing values of each descriptor in the dataset. For example,\n‘NAN_Tg’indicates the missing value of glass transition tempera-\nture, and ‘NAN_MW’ indicates the missing molecular weight at\nthat place. These unique NAN tokens are added duringﬁnetuning\nto include available chemical descriptors in the datasets. There-\nfore, different datasets can contain different NAN tokens. Notably,\nother descriptors like molecular weight and degree of polymer-\nization are omitted in this example because their values for each\ncomponent are missing. However, for practical usage, these values\nshould also be included with unique ‘NAN’ characters. Besides,\nconsidering the varying constituents in copolymers as well as\ncomponents in composites, the‘NAN’tokens for ratios are padded\nto the maximum possible numbers.\nWhen tokenizing the polymer sequences, the regular expression\nin the tokenizer adapted from the RoBERTa tokenizer is\ntransformed to search for all the possible elements in polymers\nas well as the vocabulary for descriptors and special tokens.\nConsequently, the polymer tokenizer can correctly slice polymers\ninto constituting atoms. For example, ‘Si’ which represents a\nsilicon atom in polymer sequences would be recognized as a\nsingle token by our polymer tokenizer whereas‘S’and ‘i’are likely\nto be separated into different tokens when using the RoBERTa\ntokenizer. Values for descriptors and special tokens are converted\nto single tokens as well, where all the non-text values, e.g.,\ntemperature, are discretized and treated as one token by the\ntokenizer.\nData augmentation\nTo enlarge the available polymer data for better representation\nlearning, data augmentation is applied to the polymer SMILES\nwithin polymer sequences from each dataset we use. The\naugmentation technique is borrowed from Lambard et al.\n70. First,\ncanonicalization is removed from SMILES representations; then,\natoms in SMILES are renumbered by rotation of their indices;\nﬁnally, for each renumbering case, grammatically correct SMILES\nwhich preserve isomerism of original polymers or molecules and\nprevent Kekulisation are reconstructed31,71. Also, duplicate SMILES\nare removed from the expanded list. SMILES augmentation is\nimplemented by RDKit library\n72. In particular, data augmentation\nis only applied to training sets after the train-test split to avoid\ninformation leakage.\nFig. 6 Visualization of attention scores fromﬁnetuned TransPolymer. The attention scores between the‘〈s〉’ token and other tokens at\ndifferent hidden layers in each attention head afterﬁnetuning are visualized. At the bottom is the sequence used for visualization in which the\ntokens having high attention scores with‘〈s〉’ are marked in red.\nC. Xu et al.\n11\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2023)    64 \nTransformer-based encoder\nOur TransPolymer model is based on Transformer encoder\narchitecture34. Unlike RNN-based models which encoded temporal\ninformation by recurrence, Transformer uses self-attention layers\ninstead. The attention mechanism used in Transformer is named\nScaled Dot-Product Attention, which maps input data into three\nvectors: queries (Q), keys (K), and values (V). The attention is\ncomputed byﬁrst computing the dot product of the query with all\nkeys, dividing each by ﬃﬃﬃﬃﬃd\nk\np for scaling wheredk is the dimension\nof keys, applying softmax function to obtain the weights of values,\nand ﬁnally deriving the attention. The dot product between\nqueries and keys computes how closely aligned the keys are with\nthe queries. Therefore, the attention score is able to reﬂect how\nclosely related the two embeddings of tokens are. The formula of\nScaled Dot-Product Attention can be written as:\nAttentionðQ; K; VÞ¼ softmax\nQKT\nﬃﬃﬃﬃﬃdk\np\n/C18/C19\nV (1)\nMulti-head attention is performed instead of single attention by\nlinearly projecting Q, K, and V with different projections and\napplying the attention function in parallel. The outputs are\nconcatenated and projected again to obtain theﬁnal results. In\nthis way, information from different subspaces could be learned\nby the model.\nThe input of Transformer model, namely embeddings, maps\ntokens in sequences to vectors. Due to the absence of recurrence,\nword embeddings only are not sufﬁcient to encode sequence\norder. Therefore, positional encodings are introduced so that the\nmodel can know the relative or absolute position of the token in\nthe sequence. In Transformer, position encodings are represented\nby trigonometric functions:\nPE\npos;2i ¼ sinðpos=100002i=dmodel Þ (2)\nPEpos;2iþ1 ¼ cosðpos=100002i=dmodel Þ (3)\nwhere pos is the position of the token and i is the dimension. By\nthis means, the relative positions of tokens could be learned by\nthe model.\nPretraining with MLM\nTo pretrain TransPolymer with Masked Language Modeling (MLM),\n15% of tokens of a sequence are chosen for possible replacement.\nAmong the chosen tokens, 80% of which are masked, 10% of\nwhich are replaced by randomly selected vocabulary tokens, and\n10% are left unchanged, in order to generate proper contextual\nembeddings for all tokens and bias the representation towards\nthe actual observed words\n35. Such a pretraining strategy enables\nTransPolymer to learn the “chemical grammar\" of polymer\nsequences by recovering the original tokens so that chemical\nknowledge is encoded by the model.\nThe pretraining database is split into training and validation sets\nby a ratio of 80/20. We use AdamW as the optimizer, where the\nlearning rate is 5 × 10\n−5, betas parameters are (0.9, 0.999), epsilon\nis 1 × 10−6, and weight decay is 0. A linear scheduler with a warm-\nup ratio of 0.05 is set up so that the learning rate increases from 0\nto the learning rate set in the optimizer in theﬁrst 5% training\nsteps then decreases linearly to zero. The batch size is set to 200,\nand the hidden layer dropout and attention dropout are set to 0.1.\nThe model is pretrained for 30 epochs during which the binary\ncross entropy loss decreases steadily from over 1 to around 0.07,\nand the one with the best performance on the validation set is\nused for ﬁnetuning. The whole pretraining process takes\napproximately 3 days on two RTX 6000 GPUs.\nFinetuning for polymer property prediction\nThe ﬁnetuning process involves the pretrained Transformer\nencoder and a one-layer MLP regressor head so that representa-\ntions of polymer sequences could be used for property\npredictions.\nFor the experimental settings ofﬁnetuning, AdamW is set to be\nthe optimizer whose betas parameters are (0.9, 0.999), epsilon is\n1×1 0\n−6, and weight decay is 0.01. Different learning rates are\nused for the pretrained TransPolymer and regressor head.\nParticularly, for some experiments a strategy of layer-wise learning\nrate the decay (LLRD), suggested by Zhang et al.\n73, is applied.\nSpeciﬁcally, in LLRD, the learning rate is decreased layer-by-layer\nfrom top to bottom with a multiplicative decay rate. The strategy\nis based on the observation that different layers learn different\ninformation from sequences. Top layers near the output learn\nmore local and speciﬁc information, thus requiring larger learning\nrates; while bottom layers near inputs learn more general and\ncommon information. The speciﬁc choices of learning rates for\neach dataset as well as other hyperparameters of the optimizer\nand scheduler are exhibited in Supplementary Table 2. For each\ndownstream dataset, the model is trained for 20 epochs and the\nbest model is determined in terms of the RMSE andR\n2 on the test\nset for evaluation.\nDATA AVAILABILITY\nAll data used in this work are publicly available. Original datasets could be found in\ncorresponding literature51–55. Besides, the original and processed datasets used in\nthis work are also available athttps://github.com/ChangwenXu98/TransPolymer.git.\nCODE AVAILABILITY\nThe codes developed for this work are available at https://github.com/\nChangwenXu98/TransPolymer.git.\nReceived: 2 September 2022; Accepted: 3 April 2023;\nREFERENCES\n1. Wang, Y. et al. Toward designing highly conductive polymer electrolytes by\nmachine learning assisted coarse-grained molecular dynamics.Chem. Mater. 32,\n4144–4151 (2020).\n2. Xie, T. et al. Accelerating amorphous polymer electrolyte screening by learning to\nreduce errors in molecular dynamics simulated properties. Nat. Commun. 13,\n1–10 (2022).\n3. St. John, P. C. et al. Message-passing neural networks for high-throughput\npolymer screening. J. Chem. Phys.150, 234111 (2019).\n4. Munshi, J., Chen, W., Chien, T. & Balasubramanian, G. Transfer learned designer\npolymers for organic solar cells.J. Chem. Inf. Model.61, 134–142 (2021).\n5. Luo, H. et al. Core–shell nanostructure design in polymer nanocomposite capa-\ncitors for energy storage applications. ACS Sustain. Chem. Eng. 7, 3145–3153\n(2018).\n6. Hu, H. et al. Recent advances in rational design of polymer nanocomposite\ndielectrics for energy storage.Nano Energy 74, 104844 (2020).\n7. Bai, Y. et al. Accelerated discovery of organic polymer photocatalysts for\nhydrogen evolution from water through the integration of experiment and\ntheory. J. Am. Chem. Soc.141, 9063–9071 (2019).\n8. Liang, J., Xu, S., Hu, L., Zhao, Y. & Zhu, X. Machine-learning-assisted low dielectric\nconstant polymer discovery.Mater. Chem. Front.5, 3823–3829 (2021).\n9. Mannodi-Kanakkithodi, A. et al. Scoping the polymer genome: a roadmap for\nrational polymer dielectrics design and beyond. Mater. Today 21, 785–796\n(2018).\n10. Chen, L. et al. Polymer informatics: current status and critical next steps.Mater.\nSci. Eng. R. Rep.144, 100595 (2021).\n11. Rahman, A. et al. A machine learning framework for predicting the shear strength\nof carbon nanotube-polymer interfaces based on molecular dynamics simulation\ndata. Compos Sci. Technol.207, 108627 (2021).\n12. Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M. & Monfardini, G. The graph\nneural network model.IEEE trans. neural netw.20,6 1–80 (2008).\nC. Xu et al.\n12\nnpj Computational Materials (2023)    64 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\n13. Xie, T. & Grossman, J. C. Crystal graph convolutional neural networks for an\naccurate and interpretable prediction of material properties.Phys. Rev. Lett.120,\n145301 (2018).\n14. Duvenaud, D. K. et al. Convolutional networks on graphs for learning molecular\nﬁngerprints. Adv. Neural. Inf. Process. Syst.28, (2015).\n15. Yang, K. et al. Analyzing learned molecular representations for property predic-\ntion. J. Chem. Inf. Model.59, 3370–3388 (2019).\n16. Karamad, M. et al. Orbital graph convolutional neural network for material\nproperty prediction. Phys. Rev. Mater.4, 093801 (2020).\n17. Wang, Y., Wang, J., Cao, Z. & Barati Farimani, A. Molecular contrastive learning of\nrepresentations via graph neural networks.Nat. Mach. Intell.4, 279–287 (2022).\n18. Park, J. et al. Prediction and interpretation of polymer properties using the graph\nconvolutional network. ACS polym. Au. 2, 213-222 (2022).\n19. Cereto-Massagué, A. et al. Molecular ﬁngerprint similarity search in virtual\nscreening. Methods 71,5 8–63 (2015).\n20. Rogers, D. & Hahn, M. Extended-connectivityﬁngerprints. J. Chem. Inf. Model50,\n742–754 (2010).\n21. Aldeghi, M. & Coley, C. W. A graph representation of molecular ensembles for\npolymer property prediction.Chem. Sci. 13, 10486–10498 (2022).\n22. Cho, K. et al. Learning phrase representations using rnn encoder-decoder for\nstatistical machine translation. InProceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 1724–1734 (ACL, 2014).\n23. Schwaller, P., Gaudin, T., Lanyi, D., Bekas, C. & Laino, T. \"found in translation”:\npredicting outcomes of complex organic chemistry reactions using neural\nsequence-to-sequence models. Chem. Sci. 9, 6091–6098 (2018).\n24. Tsai, S.-T., Kuo, E.-J. & Tiwary, P. Learning molecular dynamics with simple lan-\nguage model built upon long short-term memory neural network.Nat. Commun.\n11,1 –11 (2020).\n25. Flam-Shepherd, D., Zhu, K. & Aspuru-Guzik, A. Language models can learn\ncomplex molecular distributions.Nat. Commun. 13, 3293 (2022).\n26. Cadeddu, A., Wylie, E. K., Jurczak, J., Wampler-Doty, M. & Grzybowski, B. A. Organic\nchemistry as a language and the implications of chemical linguistics for structural\nand retrosynthetic analyses.Angew. Chem. Int. Ed.53, 8108–8112 (2014).\n27. Simine, L., Allen, T. C. & Rossky, P. J. Predicting optical spectra for optoelectronic\npolymers using coarse-grained models and recurrent neural networks.Proc. Natl\nAcad. Sci. USA117, 13945–13948 (2020).\n28. Webb, M. A., Jackson, N. E., Gil, P. S. & Pablo, J. J. Targeted sequence design within\nthe coarse-grained polymer genome.Sci. Adv. 6, 6216 (2020).\n29. Patel, R. A., Borca, C. H. & Webb, M. A. Featurization strategies for polymer\nsequence or composition design by machine learning. Mol. Syst. Des. Eng. 7,\n661–676 (2022).\n30. Bhattacharya, D., Kleeblatt, D. C., Statt, A. & Reinhart, W. F. Predicting aggregate\nmorphology of sequence-deﬁned macromolecules with recurrent neural net-\nworks. Soft Matter 18, 5037–5051 (2022).\n31. Weininger, D. Smiles, a chemical language and information system. 1. introduc-\ntion to methodology and encoding rules.J. Chem. Inf. Comput.28,3 1–36 (1988).\n32. Lin, T.-S. et al. Bigsmiles: a structurally-based line notation for describing mac-\nromolecules. ACS Cent. Sci.5, 1523–1531 (2019).\n33. Goswami, S., Ghosh, R., Neog, A. & Das, B. Deep learning based approach for\nprediction of glass transition temperature in polymers.Mater. Today.: Proc. 46,\n5838–5843 (2021).\n34. Vaswani, A. et al. Attention is all you need.Adv. Neural. Inf. Process. Syst. 30,\n(2017).\n35. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidir-\nectional transformers for language understanding. InProceedings of NAACL-HLT\n4171–4186 (2019).\n36. Liu, Y. et al. Roberta: A robustly optimized bert pretraining approach. Preprint at\nhttps://arxiv.org/abs/1907.11692 (2019).\n37. Brown, T. et al. Language models are few-shot learners.Adv. Neural Inf. Process.\nSyst. 33, 1877–1901 (2020).\n38. Peters, M. E., Neumann, M., Zettlemoyer, L. & Yih, W.-t. Dissecting contextual\nword embeddings: architecture and representation. InProceedings of the 2018\nConference on Empirical Methods in Natural Language Processing 1499–1509\n(2018).\n39. Conneau, A. & Lample, G. Cross-lingual language model pretraining.Adv. Neural.\nInf. Process. Syst.32, (2019).\n40. Honda, S., Shi, S. & Ueda, H. R. Smiles transformer: pre-trained molecularﬁn-\ngerprint for low data drug discovery. Preprint athttps://arxiv.org/abs/1911.04738\n(2019).\n41. Ying, C. et al. Do transformers really perform badly for graph representation?Adv.\nNeural Inf. Process. Syst.34, 28877–28888 (2021).\n42. Irwin, R., Dimitriadis, S., He, J. & Bjerrum, E. J. Chemformer: a pre-trained trans-\nformer for computational chemistry.Mach. Learn.: Sci. Technol.3, 015022 (2022).\n43. Magar, R., Wang, Y. & Barati Farimani, A. Crystal twins: self-supervised learning for\ncrystalline material property prediction.NPJ Comput. Mater.8, 231 (2022).\n44. Wang, S., Guo, Y., Wang, Y., Sun, H. & Huang, J. Smiles-bert: large scale unsu-\npervised pre-training for molecular property prediction. InProceedings of the 10th\nACM International Conference on Bioinformatics, Computational Biology and Health\nInformatics 429–436 (2019).\n45. Chithrananda, S., Grand, G. & Ramsundar, B. Chemberta: large-scale self-super-\nvised pretraining for molecular property prediction. Preprint athttps://arxiv.org/\nabs/2010.09885 (2020).\n46. Schwaller, P. et al. Molecular transformer: a model for uncertainty-calibrated\nchemical reaction prediction.ACS Cent. Sci.5, 1572–1583 (2019).\n47. Cao, Z., Magar, R., Wang, Y. & Barati Farimani, A. Moformer: self-supervised\ntransformer model for metal –organic framework property prediction. J. Am.\nChem. Soc. 145, 2958–2967 (2023).\n48. Perry, S. L. & Sing, C. E. 100th anniversary of macromolecular science viewpoint:\nopportunities in the physics of sequence-deﬁned polymers. ACS Macro Lett. 9,\n216–225 (2020).\n49. Le, T., Epa, V. C., Burden, F. R. & Winkler, D. A. Quantitative structure–property\nrelationship modeling of diverse materials properties.Chem. Rev.112, 2889–2919\n(2012).\n50. Persson, N., McBride, M., Grover, M. & Reichmanis, E. Silicon valley meets the ivory\ntower: searchable data repositories for experimental nanomaterials research.\nCurr. Opin. Solid State Mater. Sci.20, 338–343 (2016).\n51. Ma, R. & Luo, T. Pi1m: a benchmark database for polymer informatics.J. Chem. Inf.\nModel 60, 4684–4690 (2020).\n52. Schauser, N. S., Kliegle, G. A., Cooke, P., Segalman, R. A. & Seshadri, R. Database\ncreation, visualization, and statistical learning for polymer li+-electrolyte design.\nChem. Mater. 33, 4863–4876 (2021).\n53. Hatakeyama-Sato, K., Tezuka, T., Umeki, M. & Oyaizu, K. Ai-assisted exploration of\nsuperionic glass-type li+ conductors with aromatic structures.J. Am. Chem. Soc.\n142, 3301–3305 (2020).\n54. Kuenneth, C. et al. Polymer informatics with multi-task learning. Patterns 2,\n100238 (2021).\n55. Nagasawa, S., Al-Naamani, E. & Saeki, A. Computer-aided screening of conjugated\npolymers for organic solar cell: classiﬁcation by random forest.J. Phys. Chem. Lett.\n9, 2639–2646 (2018).\n56. Salazar, J., Liang, D., Nguyen, T. Q. & Kirchhoff, K. Masked language model scoring.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics 2699–2712 (ACL, 2020).\n57. Bao, H. et al. Unilmv2: Pseudo-masked language models for uniﬁed language\nmodel pre-training. In International Conference on Machine Learning (ICML)\n642–652 (ICML, 2020).\n58. Yang, Z., Yang, Y., Cer, D., Law, J. & Darve, E. Universal sentence representation\nlearning with conditional masked language model. InProceedings of the 2021\nConference on Empirical Methods in Natural Language Processing 6216–6228\n(2021).\n59. Otsuka, S., Kuwajima, I., Hosoya, J., Xu, Y. & Yamazaki, M. Polyinfo: Polymer\ndatabase for polymeric materials design. In 2011 International Conference on\nEmerging Intelligent Data and Web Technologies22–29 (2011).\n60. Kim, C., Chandrasekaran, A., Huan, T. D., Das, D. & Ramprasad, R. Polymer genome:\na data-powered polymer informatics platform for property predictions.J. Phys.\nChem. C. 122, 17575–17585 (2018).\n61. Maaten, L. & Hinton, G. Visualizing data using t-sne. J. Mach. Learn Res. 9,\n2579–2605 (2008).\n62. Poličar, P.G., Stražar, M. & Zupan, B. Opentsne: a modular python library for t-sne\ndimensionality reduction and embedding. Preprint at https://www.biorxiv.org/\ncontent/10.1101/731877v3.abstract (2019).\n63. Abnar, S. & Zuidema, W. Quantifying attentionﬂow in transformers. InProceed-\nings of the 58th Annual Meeting of the Association for Computational Linguistics\n4190–4197 (ACL, 2020).\n64. Schauser, N. S. et al. Glass transition temperature and ion binding determine\nconductivity and lithium–ion transport in polymer electrolytes.ACS Macro Lett.\n10, 104–109 (2020).\n65. Hao, Y., Dong, L., Wei, F. & Xu, K. Self-attention attribution: interpreting infor-\nmation interactions inside transformer. InProceedings of the AAAI Conference on\nArtiﬁcial Intelligence 35, 12963–12971 (2021).\n66. Reis, M. et al. Machine-learning-guided discovery of 19f mri agents enabled by\nautomated copolymer synthesis.J. Am. Chem. Soc.143, 17677–17689 (2021).\n67. Tamasi, M. J. et al. Machine learning on a robotic platform for the design of\npolymer–protein hybrids. Adv. Mater. 34, 2201809 (2022).\n68. Batra, R. et al. Polymers for extreme conditions designed using syntax-directed\nvariational autoencoders. Chem. Mater. 32, 10489–10500 (2020).\n69. Chen, G., Tao, L. & Li, Y. Predicting polymers\n’ glass transition temperature by a\nchemical language processing model.Polymers 13, 1898 (2021).\n70. Lambard, G. & Gracheva, E. Smiles-x: autonomous molecular compounds char-\nacterization for small datasets without descriptors.Mach. Learn.: Sci. Technol.1,\n025004 (2020).\nC. Xu et al.\n13\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2023)    64 \n71. Eyben, F., Wöllmer, M. & Schuller, B. Opensmile: the munich versatile and fast\nopen-source audio feature extractor. InProceedings of the 18th ACM International\nConference on Multimedia1459–1462 (2010).\n72. Landrum, G. et al. Rdkit: open-source cheminformatics. https://www.rdkit.org\n(2006).\n73. Zhang, T., Wu, F., Katiyar, A., Weinberger, K. Q. & Artzi, Y. Revisiting few-sample\nbert ﬁne-tuning. In International Conference on Learning Representations (ICLR)\n(ICLR, 2021).\nACKNOWLEDGEMENTS\nWe thank the Advanced Research Projects Agency— Energy (ARPA-E), U.S. Depart-\nment of Energy, under Award No. DE-AR0001221 and the start-up fund provided by\nthe Department of Mechanical Engineering at Carnegie Mellon University.\nAUTHOR CONTRIBUTIONS\nA.B.F., Y.W., and C.X. conceived the idea; C.X. trained and evaluated the TransPolymer\nmodel; C.X. wrote the manuscript; A.B.F. supervised the work; all authors modiﬁed\nand approved the manuscript.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41524-023-01016-5.\nCorrespondence and requests for materials should be addressed to Amir Barati\nFarimani.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nC. Xu et al.\n14\nnpj Computational Materials (2023)    64 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences"
}