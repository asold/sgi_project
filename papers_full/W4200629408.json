{
  "title": "JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering",
  "url": "https://openalex.org/W4200629408",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2541726693",
      "name": "Yueqing Sun",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1986207636",
      "name": "Qi Shi",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098897091",
      "name": "Le Qi",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2023856972",
      "name": "Yu Zhang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2604314403",
    "https://openalex.org/W3007685714",
    "https://openalex.org/W2950576363",
    "https://openalex.org/W3015440086",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W3101850416",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W3097986428",
    "https://openalex.org/W3119519251",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W4288114783",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2963907629",
    "https://openalex.org/W2939208918",
    "https://openalex.org/W3118741274",
    "https://openalex.org/W2892167328",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W2897037347",
    "https://openalex.org/W3174531908",
    "https://openalex.org/W3172335055",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2987669390",
    "https://openalex.org/W2998374885",
    "https://openalex.org/W3174738170",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W3035529900",
    "https://openalex.org/W4288265053"
  ],
  "abstract": "Existing KG-augmented models for commonsense question answering primarily focus on designing elaborate Graph Neural Networks (GNNs) to model knowledge graphs (KGs). However, they ignore (i) the effectively fusing and reasoning over question context representations and the KG representations, and (ii) automatically selecting relevant nodes from the noisy KGs during reasoning. In this paper, we propose a novel model, JointLK, which solves the above limitations through the joint reasoning of LM and GNN and the dynamic KGs pruning mechanism. Specifically, JointLK performs joint reasoning between LM and GNN through a novel dense bidirectional attention module, in which each question token attends on KG nodes and each KG node attends on question tokens, and the two modal representations fuse and update mutually by multi-step interactions. Then, the dynamic pruning module uses the attention weights generated by joint reasoning to prune irrelevant KG nodes recursively. We evaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate its improvements to the existing LM and LM+KG models, as well as its capability to perform interpretable reasoning.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5049 - 5060\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nJointLK: Joint Reasoning with Language Models and Knowledge Graphs\nfor Commonsense Question Answering\nYueqing Sun, Qi Shi, Le Qi, Yu Zhang∗\nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology, Harbin, China\n{yqsun,qshi,lqi,zhangyu}@ir.hit.edu.cn\nAbstract\nExisting KG-augmented models for common-\nsense question answering primarily focus on\ndesigning elaborate Graph Neural Networks\n(GNNs) to model knowledge graphs (KGs).\nHowever, they ignore (i) the effectively fus-\ning and reasoning over question context repre-\nsentations and the KG representations, and (ii)\nautomatically selecting relevant nodes from the\nnoisy KGs during reasoning. In this paper, we\npropose a novel model, JointLK, which solves\nthe above limitations through the joint reason-\ning of LM and GNN and the dynamic KGs\npruning mechanism. Specifically, JointLK per-\nforms joint reasoning between LM and GNN\nthrough a novel dense bidirectional attention\nmodule, in which each question token attends\non KG nodes and each KG node attends on\nquestion tokens, and the two modal represen-\ntations fuse and update mutually by multi-step\ninteractions. Then, the dynamic pruning mod-\nule uses the attention weights generated by\njoint reasoning to prune irrelevant KG nodes\nrecursively. We evaluate JointLK on the Com-\nmonsenseQA and OpenBookQA datasets, and\ndemonstrate its improvements to the existing\nLM and LM+KG models, as well as its capa-\nbility to perform interpretable reasoning1.\n1 Introduction\nCommonsense question answering (CSQA) re-\nquires systems to acquire different types of com-\nmonsense knowledge and reasoning skills, which\nis normal for humans, but challenging for machines\n(Talmor et al., 2019). Recently, large pre-trained\nlanguage models (LMs) have achieved remarkable\nsuccess in many QA tasks and appear to use im-\nplicit (factual) knowledge encoded in their model\nparameters during fine-tuning (Liu et al., 2019;\nRaffel et al., 2020). Nevertheless, commonsense\n∗Corresponding author.\n1Our code is available at: https://github.com/Yueqing-\nSun/JointLK\nText \nEncoder\nGraph \nEncoder\nScore\nwood\nplay_guitar\nguitar\n…\ntake_lessons\naction singer\nburn\ndance\ngas\nsinging\nfun\nQuestion:\nWhat do people typically do while \nplaying guitar?\nChoices:\nA. cry B. hear sounds C. singing*       \nD. arthritis  E. making music\nJoint \nReasoningSubgraph\nsinge\nFigure 1: Our knowledge-augmented joint reasoning\nmodel framework with an example from Common-\nsenseQA. The subgraph is retrieved from ConceptNet.\nknowledge is self-evident to humans and is rarely\nexpressed clearly in natural language (Gunning,\n2018), which makes it difficult for LMs to learn\ncommonsense knowledge from the pre-training text\ncorpus alone.\nAn extensive research path is to elaborately de-\nsign graph neural networks (GNNs) (Scarselli et al.,\n2008) to perform reasoning over explicit structural\ncommon sense knowledge from external knowl-\nedge bases (Vrandeˇci´c and Krötzsch, 2014; Speer\net al., 2017). Related methods usually follow a\nretrieval-and-modeling paradigm. First, the knowl-\nedge subgraphs or paths related to a given question\nare retrieved by string matching or semantic similar-\nity; such retrieved structured information indicates\nthe relation between concepts or implies the pro-\ncess of multi-hop reasoning. Second, the retrieved\nsubgraphs are modeled by a well-designed graph\nneural network module (Lin et al., 2019; Feng et al.,\n2020; Yasunaga et al., 2021) to perform reasoning\nover knowledge graphs.\nHowever, these approaches have two main is-\nsues. First, the retrieved knowledge subgraph\ncontains many noisy nodes . Whether through\n5049\nsimple string matching or semantic matching, in or-\nder to retrieve sufficient relevant knowledge, noise\nknowledge graph nodes will inevitably be included\n(Lin et al., 2019; Yasunaga et al., 2021). Especially\nwith the increase of hop count, the number of irrele-\nvant nodes will expand dramatically, raising the bur-\nden of the model. As the example in Figure 1, some\ngraph nodes such as “ wood”, “burn”, and “gas”,\nalthough related to some entities in the questions\nand choice, can mislead the global understanding\nof the question. Second, there are limited inter-\nactions between language representation and\nknowledge graph representation . Specifically,\nexisting LM+KG methods (Lin et al., 2019; Feng\net al., 2020) model question context and knowledge\nsubgraphs in isolation by LMs and GNNs, and per-\nform only one interaction in a shallow manner to\nfuse their representations at the output for predic-\ntion. We argue that the limited interaction between\nthe two modalities is the main bottleneck that may\nprevent the model from understanding the complex\nquestion-knowledge relations necessary to answer\nthe question correctly.\nBased on the above consideration, we propose\nJointLK, a model that performs the fine-grained\nmodal fusion and multi-layer joint reasoning be-\ntween the language model and the knowledge graph\n(see Figure 2). Specifically, given a question and\nretrieved subgraphs, JointLK first obtain the repre-\nsentations of the two modalities by using an LM\nencoder and a GNN encoder respectively. Then\nwe design a joint reasoning module to generate\nfine-grained bidirectional attention maps between\neach question token and each KG node to fuse\nthe information from each modality to the other.\nGuided by the attention generated in the interac-\ntion process, the dynamic pruning module deletes\nirrelevant nodes to make the model reason along\nthe correct knowledge path. Multiple JointLK lay-\ners are stacked to form a hierarchy that supports\nmulti-step interactions and recursive pruning. In\nsummary, our contributions are three-fold:\n• We propose JointLK, a novel model that sup-\nports multi-step joint reasoning between LM\nand KG. It uses dense bidirectional attention\nto simultaneously update query-aware knowl-\nedge graph representation and knowledge-\naware query representation, bridging the gap\nbetween the two information modalities.\n• We design a dynamic graph pruning mod-\nule that recursively removes irrelevant graph\nnodes at each JointLK layer to ensure that the\nmodel reasons correctly with complete and\nappropriate evidence.\n• Experimental results show that JointLK is su-\nperior to current LM+KG methods, and the\nrefined evidence is interpretable. Furthermore,\nthrough the multi-layer fusion of these two\nmodalities, JointLK exhibits strong perfor-\nmance over previous state-of-the-art LM+KG\nmethods in performing complex reasoning,\nsuch as solving questions with negation and\ncomplex questions with more entities.\n2 Related Work\nCommonsense question answering is challenging\nbecause the required commonsense knowledge is\nrarely given in the context of questions and answer\nchoices or encoded in the parameters of pre-trained\nLMs. Therefore, many works obtain the required\nknowledge from external sources (e.g., KGs, cor-\npus) to augment CSQA models. Due to the het-\nerogeneity between structured knowledge and un-\nstructured text questions, there are currently two\nmain research methods. Some works (Lv et al.,\n2020; Bian et al., 2021; Xu et al., 2021) unify\nthe two modalities during model input, such as\ntransforming structured knowledge into plain text\nthrough templates or transforming question context\ninto structured graphs. However, the original struc-\ntural/textual information will inevitably be lost dur-\ning the conversion process. Other works (Lin et al.,\n2019; Feng et al., 2020; Yan et al., 2021) use LM\nand GNN to model the two modalities separately,\nand perform shallow interactions in the latter model\nstage, such as attentive pooling or simple concate-\nnation of the two modal representations. Although\nthis method can retain the original information of\nquestion context and KGs, the limited interaction\nwill affect the flow of information between the two\nmodalities, so we mainly improve on this point.\nRecently, QA-GNN (Yasunaga et al., 2021) ex-\nplicitly views the QA context as an additional node,\nconnects it and KG to form a joint graph, and mu-\ntually updates their representations through graph-\nbased message passing. However, it pools the repre-\nsentation of the question context into a single node,\nwhich limits the updating of the text representa-\ntion and fine-grained interaction between LM and\nGNN. Compared with prior works, we retain the in-\ndividual structure of both modalities, consider fine-\n5050\n[CLS] question [SEP] choice [SEP]\nLM Encoder\nKnowledge Graph\nGNN Layer\nJoint Reasoning Module\nDynamic Pruning \nL-th Layer Top-rank\nselection\nLM-to-KG\nattention Masking\n(L+1)-th\nLayer\nT\nSoftmax\nConcat\nLinear\nConcat\nLinear\nLM-to-KG\nattention\nKG-to-LM\nattention\n…\nPlausibility \nScore\nN×\nLM rep\nKG rep\nLM rep\nKG rep\nJointLK\nLayer\nFigure 2: Overall architecture of our proposed JointLK model, which takes a query (question + choice) and a\nretrieved knowledge subgraph as input, and outputs a scalar that represents the plausibility score of this query.\nJointLK mainly consists of four modules the Query Encoder, the Graph Layer, the Joint Reasoning Module and the\nDynamic Pruning Module, of which the latter three form a stack of N identical layers.\ngrained interaction between any token in question\nand any entity in KG through dense bidirectional\nattention, and perform multi-step joint reasoning\nby stacking several interaction layers. Furthermore,\nwe gradually prune the KG size in each stacked\nmodel layer under the guidance of attention weights\ngenerated in the interactions, making the reasoning\npath transparent and interpretable.\n3 Methodology\nIn this section, we introduce the task definition\n(§ 3.1) and our JointLK model. The model frame-\nwork is shown in Figure 2. JointLK takes the query\nand the retrieved knowledge subgraph as input, and\noutputs a real value as the correctness score of the\nanswer. The model is mainly composed of four\nparts: query encoder, GNN layer, joint reasoning\nmodule and dynamic pruning module, of which the\nlatter three form a stack of N identical layers. We\nuse a pre-trained language model to learn the query\nrepresentation (§ 3.2), and use the GNN layer to\nlearn the graph representation (§ 3.3). The Joint\nReasoning Module receives these two modalities’\nrepresentations and then apply dense bidirectional\nattention to make information fusion and repre-\nsentation update for each token and node (§ 3.4).\nThe LM-to-KG attention weights generated in rea-\nsoning represents the global importance of each\nnode in the graph, so the dynamic pruning module\nprunes the graph layer by layer according to this\nweights and finally retains the most relevant nodes\n(§ 3.5). After N layers of iteration, the query repre-\nsentation and the trimmed graph representation are\nused to predict the answer (§ 3.6).\n3.1 Task Definition\nThe CSQA task in this paper is a multiple-choice\nproblem with some answer choices. Given a com-\nmonsense question qand a set of answer choices\n{a1,a2,...,a n}, our task is to measure the plausi-\nbility score between q and each answer choice a\nthen select the answer with the highest plausibil-\nity score. In general, questions do not contain any\nreference to answer choices, so the external knowl-\nedge graph provides the necessary background\nknowledge. We extract from the external KG a\nsubgraph g= (V,R) with the guidance of question\nand choice. Here V is a subset of entity nodes re-\ntrieved from the external KG. E ⊆V ×R×V is\nthe set of edges that connect nodes in V, where R\nis a set of relations types. We describe the detailed\nextraction process in Appendix A.\n3.2 Query Encoder\nWe follow baselines to use pre-trained language\nmodels to encode the query {wi}M\ni=1 (question and\nchoice) into a sequence of vectors {q0\ni }M\ni=1:\n{˜q0\n1,..., ˜q0\nM }= EncLM ({w1,...,w M }) (1)\n5051\nHere {˜q0\ni }M\ni=1 ∈RT is the last hidden layer vector\nof each token in the query. Then we feed the repre-\nsentation of tokens into a non-linear layer so that\nthe text representation space is aligned to the entity\nrepresentation space:\nq0\ni = σ\n(\nfs\n(\n˜q0\ni\n))\n(2)\nwhere fs : RT →RD is a linear transformation,\nand σ is the activation function. The represen-\ntations of tokens Q0 = {q0\ni }M\ni=1 ∈RD will be\nprovided to the joint reasoning module for further\ninteraction with the graph entities representations.\n3.3 GNN Layer\nAfter obtaining token representations by the query\nencoder, we further model the subgraph to obtain\nentity representations. First, We use the BERT\nmodel with average pooling to get the initial rep-\nresentation for each entity X0 = {x0\ni }|V |\ni=0 ∈RD.\nThen, we apply GNN Layer to update node rep-\nresentation through iterative message passing be-\ntween neighbors on the graph, while GNN is built\non the RGAT (Wang et al., 2020a) and is a simpli-\nfication of Yasunaga et al. (2021). For brevity, we\nformulate the entire computation in one layer as:\n{˜xl\n1,..., ˜xl\n|V |}= GNN({xl−1\n1 ,...,x l−1\n|V |}) (3)\nThe output representation xl\ni is computed by\nˆαji = (xl−1\ni Wq)(xl−1\nj Wk + rji)T , (4)\nαji = softmax(ˆαji/\n√\nD), (5)\nˆxl−1\ni =\n∑\nj∈Ni∪{i}\nαji(xl−1\nj Wv + rji), (6)\n˜xl\ni = LayerNorm(xl−1\ni + ˆxl−1\ni Wo) (7)\nwhere matrices Wq,Wk,Wv,Wo ∈ RD×D are\ntrainable parameters, Ni is the neighbor of node i.\nrji = ψ(eji,uj,ui) is the relation feature vector,\nwhere eji is a one-hot vector denoting the rela-\ntion type of the edge (j,i) and uj,ui are one-hot\nvectors denoting the node types of j and i. The\nfollowing joint reasoning module will further fuse\n˜xl\ni and ql−1\ni to obtain their updated representations.\n3.4 Joint Reasoning Module\nTo reduce the gap of query and knowledge graph\nfeatures, we fuse them in the joint reasoning mod-\nule by the dense bidirectional attention mechanism\nthat connects two encoding layers of query and\nknowledge graph and captures the fine-grained in-\nterplay between them.\nThe module takes the query and KG representa-\ntions Q and X as inputs and then outputs their\nupdated versions. We denote the inputs to the\njoint reasoning module in the l-st fusion layer by\nQl−1 = {ql−1\ni }M\ni=1 and ˜Xl = {˜xl\ni}|V |\ni=1. Given ql−1\ni\nand ˜xl\ni, an affinity matrix is first constructed via:\nSl\nij = WT\nS [ql−1\ni ; ˜xl\nj; ql−1\ni ◦˜xl\nj] (8)\nwhere WT\nS is a learnable weight matrix, ◦is ele-\nmentwise multiplication, [;] is vector concatenation\nacross row. We normalize Sl\nij in row-wise to de-\nrive KG-to-LM attention maps on query tokens\nconditioned by each entity in KG as\nSl\nqi = softmax (Sij) (9)\nand also normalize Sl\nij in column-wise to derive\nLM-to-KG attention maps on entities conditioned\nby each query token as\nSl\nxj = softmax\n(\nST\nij\n)\n(10)\nThe attended representations are computed as fol-\nlows:\nˆqij = ql−1\ni ⊗Sl\nqi ,ˆxij = ˜xl\nj ⊗Sl\nxj (11)\nwhere ⊗represents matrix multiplication. The at-\ntended features are fused with the original features\nof the other modality by concatenation and then\ncompressed to low-dimensional space by:\nql\ni = WQ[ql−1\ni ; ˆxij; ql−1\ni ◦ˆxij; ql−1\ni ◦ˆqij], (12)\n¯xl\nj = WX[˜xl\nj; ˆqij; ˜xl\nj ◦ˆqij; ˜xl\nj ◦ˆxij] (13)\nwhere WQ,WX are learnable weights. Then the\nupdated query representation Ql = {ql\ni}M\ni=1 will\nbe input to the next l-th stacked JointLK layer of\nto continue participating in joint reasoning, and the\nupdated KG representation ¯Xl = {¯xl\ni}|V |\ni=1 will be\ninput to the next module of the current JointLK\nlayer for pruning.\n3.5 Dynamic Pruning Module\nIn Equation 10, the LM-to-KG attention value im-\nplies the importance of different nodes in the sub-\ngraph for question answering. Inspired by SAG-\nPool (Lee et al., 2019), under the guidance of query,\nwe retain relevant nodes and cut out irrelevant\nnodes according to the LM-to-KG attention. Then,\n5052\nWe define a hyperparameter, the Retention ratio\nK ∈(0,1], which determines the number of nodes\nto be retained. We choose the top ⌈K·|V|⌉nodes\naccording to the value of LM-to-KG attention:\nidx= top −rank (Z,⌈K·|V|⌉) , (14)\nZmask = Zidx (15)\nwhere top-rank is a function that returns the index\nof top ⌈K·|V|⌉value, ·idx is an indexing opera-\ntion, and Zmask is corresponding attention mask.\nNext, the subgraph is formed by pooling out the\nless essential entity nodes as:\nXl = ¯Xl\nidx,: ⊙Zmask,\nAl = ¯Al\nidx,idx\n(16)\nwhere ¯Xl\nidx,: is the row-wise indexed representa-\ntion matrix of ¯Xl, ⊙is the broadcasted elemen-\ntwise product, and ¯Al\nidx,idx is the row-wise and\ncol-wise of indexed adjacency matrix. Xl =\n(xl\n1,xl\n2,...,x l\n⌈k|V |⌉), Al and ⌈K·|V|⌉are the rep-\nresentation matrix, the adjacency matrix and the\nnumber of graph nodes in the next JointLK layer.\n3.6 Answer Prediction\nAfter N layers of iteration, we finally obtain the\nquery representation QN that fuses knowledge in-\nformation and the graph representation XN that\nfuses question information. We compute the score\nof abeing the correct answer as:\np= (a|q) = MLP ([s; g]) (17)\nwhere sis the mean pooling of QN , and g is the\nattention-based pooling of XN . We get the final\nprobability by normalize all question-choice pairs\nwith softmax.\n4 Experimental Setup\n4.1 Datasets\nWe evaluate our model on two typical com-\nmonsense question answering datasets Common-\nsenseQA (Talmor et al., 2019) and OpenBookQA\n(Mihaylov et al., 2018). CommonsenseQA is a\n5-way multiple-choice question answering dataset\nthat requires commonsense for reasoning and con-\ntains 12,102 questions. We experiment and report\nthe accuracy on the in-house dev (IHdev) and test\n(IHtest) splits used by Lin et al. (2019), and re-\nport the accuracy of our final system on the official\ntest set. OpenBookQA is a 4-way multiple choice\nquestion answering dataset that requires reasoning\nwith elementary science knowledge. It contains\n5,957 questions along with an open book of scien-\ntific facts. We use the official data split.\n4.2 Implementation Details\nFollowing previous work (Yasunaga et al., 2021),\nwe use ConceptNet (Speer et al., 2017), a common-\nsense knowledge graph, as our structured knowl-\nedge source for both of the above tasks. Given each\nquery, we follow the preprocessing steps described\nin Feng et al. (2020) to retrieve the subgraph from\nConceptNet, and the max hop size is 3 (see Ap-\npendix A for the detail). We use cross-entropy loss\nand RAdam optimizer (Liu et al., 2020). In train-\ning, we set the maximum input sequence length to\ntext encoders to 100, batch size to 128, and per-\nform early stopping. We set the dimension (D =\n200) and number of layers (N = 5) of our GNN\nmodule, with dropout rate 0.2 applied to each layer\n(Srivastava et al., 2014). We use separate learning\nrates for the LM encoder and the graph encoder.\nWe choose the LM encoder learning rate from{1 ×\n10−5, 2 ×10−5, 3 ×10−5}, and choose the graph\nencoder learning rate from{1 ×10−3, 2 ×10−3}.\nEach model is trained using one GPU (Tesla_v100-\nsxm2-16gb), which takes 20 hours on average.\n4.3 Compared Method\nAlthough text corpus can provide complementary\nknowledge except for knowledge graphs, our model\nfocuses on improving the use of KG and the joint\nreasoning between LM and KG, so we choose LM\nand LM+KG as the comparison methods.\nTo investigate the role of KGs, we compare with\nthe benchmark model RoBERTa-large (Liu et al.,\n2019) for CommonsenseQA, and compare with\nRoBERTa-large and AristoRoBERTa (Clark et al.,\n2020) for OpenBookQA. For LM+KG methods,\nthey share a similar high-level framework with our\nmethods, that is, LM is used as a text encoder, GNN\nor RN is used as a KG encoder, but the way of\nusing knowledge or reasoning is different: (1) Re-\nlationship network (RN) (Santoro et al., 2017), (2)\nRGCN (Schlichtkrull et al., 2018), (3) GconAttn\n(Wang et al., 2019), (4)KagNet (Lin et al., 2019)\nand (5)MHGRN (Feng et al., 2020), (6) QA-GNN\n(Yasunaga et al., 2021). (1), (2) and (3) are the\nrelational perception GNNs for KGs, and (4), (5)\nand (6) are further model paths in KGs. To be fair,\nwe use the same LM for all comparison methods.\n5053\nMethods IHdev-Acc.(%) IHtest-Acc.(%)\nRoBERTa-large(w/o KG) 73.07 (±0.45) 68.69 (±0.56)\n+ RGCN 72.69 ( ±0.19) 68.41 (±0.66)\n+ GconAttn 71.61 ( ±0.39) 68.59 (±0.96)\n+ KagNet 73.47 ( ±0.22) 69.01 (±0.76)\n+ RN 74.57 ( ±0.91) 69.08 (±0.21)\n+ MHGRN 74.45 ( ±0.10) 71.11 (±0.81)\n+ QA-GNN 76.54 ( ±0.21) 73.41 (±0.92)\n+ JointLK (Ours) 77.88(±0.25) 74.43(±0.83)\nTable 1: Performance comparison on CommonsenseQA\nin-house split. We follow the data division method of\nLin et al. (2019) and report the in-house Dev (IHdev)\nand Test (IHtest) accuracy(mean and standard deviation\nof four runs).\nMethods Test\nRoBERTa (Liu et al., 2019) 72.1\nAlbert (Lan et al., 2020) (ensemble) 76.5\nRoBERTa + FreeLB (Zhu et al., 2020) (ensemble) 73.1\nRoBERTa + HyKAS (Ma et al., 2019) 73.2\nRoBERTa + KE (ensemble) 73.3\nRoBERTa + KEDGN (ensemble) 74.4\nXLNet + GraphReason (Lv et al., 2020) 75.3\nRoBERTa + MHGRN (Feng et al., 2020) 75.4\nAlbert + PG (Wang et al., 2020b) 75.6\nRoBERTa + QA-GNN (Yasunaga et al., 2021) 76.1\nRoBERTa + JointLK (Ours) 76.6\nTable 2: Performance comparison on the Common-\nsenseQA official leaderboard. Our model has achieved\nstate-of-the-art under the setting of RoBERTa-large.\n5 Results and Analysis\n5.1 Main Results\nThe results on CommonsenseQA in-house split\ndataset and official test dataset are shown in Ta-\nble 1 and Table 2. The results on OpenBookQA test\ndataset and leaderboard are shown in Table 3 and\nTable 4. We can observe that JointLK performs best\namong all fine-tuned LMs and existing LM+KG\nmodels. On CommonsenseQA, our model’s test\nperformance improves by 5.74% over fine-tuned\nLMs and 1.02% over the prior best LM+KG model,\nQA-GNN. On OpenbookQA, our model’s test per-\nformance improves by 6.52% over fine-tuned Aris-\ntoRoBERTa, and 2.15% over QA-GNN. Addition-\nally, we also submit our best model to the leader-\nboards, and our JointLK (with the text encoder\nbeing RoBERTa-large) ranks first among compa-\nrable approaches. Compared with the previous\nbest model MHGRN and QA-GNN, the boost over\nthem suggests the effectiveness of our proposed\njoint reasoning between LM and KG and the dy-\nMethods RoBERTa-large AristoRoBERTa\nFine-tuned LMs (w/o KG) 64.80 (±2.37) 78.40 (±1.64)\n+ RGCN 62.45 ( ±1.57) 74.60 (±2.53)\n+ GconAttn 64.75 ( ±1.48) 71.80 (±1.21)\n+ RN 65.20 ( ±1.18) 75.35 (±1.39)\n+ MHGRN 66.85 ( ±1.19) 80.6\n+ QA-GNN 67.80 ( ±2.75) 82.77 (±1.56)\n+ JointLK (Ours) 70.34(±0.75) 84.92(±1.07)\nTable 3: Test accuracy on OpenBookQA. Methods with\nAristoRoBERTa use the textual evidence by Clark et al.\n(2020) as an additional input to the QA context.\nMethods Test\nCareful Selection (Banerjee et al., 2019) 72.0\nAristoRoBERTa 77.8\nKF + SIR (Banerjee and Baral, 2020) 80.0\nAristoRoBERTa + PG (Wang et al., 2020b) 80.2\nAristoRoBERTa + MHGRN (Feng et al., 2020) 80.6\nALBERT + KB 81.0\nAristoRoBERTa + QA-GNN (Yasunaga et al., 2021) 82.8\nT5* (Raffel et al., 2020) 83.2\nUnifiedQA(11B)* (Khashabi et al., 2020) 87.2\nAristoRoBERTa + JointLK (Ours) 85.6\nTable 4: Test accuracy on OpenBookQA leaderboard.\nAll listed methods use the provided science facts as an\nadditional input to the language context. The previous\ntop 2 systems, UnifiedQA (11B params) and T5 (3B\nparams) are 30x and 8x larger than our model.\nnamic pruning mechanism.\nIn particular, we do not compare with the higher\nranking models on the leaderboard, such as unified\nQA (Khashabi et al., 2020), Albert + DESC-KCR\n(Xu et al., 2021), because they either use a stronger\ntext encoder or use additional data resources, while\nour model focuses on improving the joint reasoning\nbetween LM and KG.\n5.2 Ablation Studies\nWe further conduct in-depth analyses to investigate\nthe effectiveness of different components in our\nmodel. We show the accuracy of JointLK on the\nCommonsenseQA IHdev set.\nImpact of JointLK components We assess the\nimpact of the joint reasoning module (§ 3.4) and\nthe dynamic pruning module (§ 3.5), shown in Ta-\nble 5. Disabling the dynamic pruning module re-\nsults in 0.5% drop in performance, showing that\nsome nodes in subgraph are not conducive to rea-\nsoning. Especially, when we disable the joint rea-\nsoning module, the corresponding dynamic pruning\nmodule will also be removed, because the latter de-\npends on the attention value in the former. Then the\n5054\nMethods IHdev-Acc. (%)\nJointLK (N=5) 77.88\n- Dynamic Pruning Module 77.38\n- Joint Reasoning Module 76.61\nTable 5: Ablation study on model components using\nRoBERTa-large as the text encoder. We report the IHdev\naccuracy on CommonsenseQA.\n/19 /10 /20 /14 /7 /2 /2 /2 /21 /7 /2 /22 /17 /14 /13 /17 /2 /17 /12 /23 /24 /12 /2 /25 /14 /17 /26 /27 /28 /29 /2 /9 /10 /11 /12 /13 /14 /15 /3 /16 /17 /18 /11 /3 /2 /3\n(a)\n/19 /10 /20 /14 /7 /2 /2 /2 /21 /7 /2 /22 /17 /14 /13 /17 /2 /17 /12 /23 /24 /12 /2 /25 /14 /17 /26 /27 /28 /29 /2 /9 /10 /11 /12 /13 /14 /15 /0 /16 /17 /18 /11 /3 /2 /3 (b)\nFigure 3: Ablation study on stacked of JointLK layers\n(a) and the retention ratio in pruning (b).\nresults have a significant drop: 77.88% →76.61%,\nsuggesting that the joint reasoning between LM and\nKG is critical.\nImpact of stacked of JointLK Layers We inves-\ntigate the impact of the number of JointLK layers\n(shown in Figure 3 (a)). The increase of layers con-\ntinues to bring benefits until layers N = 5. How-\never, performance begins to drop when N >5. As\nthe number of layers increases, the model changes\nfrom underfitting to overfitting.\nImpact of the Retention Ratio in Pruning The re-\ntention ratio Kis a hyperparameter of the dynamic\npruning module. Since it is recursively pruning\nin each stacked layer of JointLK, the percentage\nof graph nodes that the model ultimately retains\nis also related to the number of layers of JointLK,\nthat is, KN , where N = 5 . Experiments show\nthat if the retention ratio is too high, there may\nbe almost no pruning effect (for example, K=0.98,\n90% of the nodes are retained in the last layer);\notherwise, useful nodes may be deleted. As shown\nin Figure 3 (b), when the number of JointLK lay-\ners N = 5, K = 0.92 (about 66% of the original\nnodes remain in the last layer) works the best on\nthe CommonsenseQA dev set.\n5.3 Quantitative Analysis\nConsidering the overall performance improvement\nof our model on these two datasets, we analyze\nwhether the improvement is reflected in questions\nthat require more complex reasoning, such as ques-\ntions with negation and complex questions with\nMethods IHdev-Acc(Overall)IHdev-Acc(Questionsw/ negation)\nIHdev-Acc(Questions w/≤7 entities)\nIHdev-Acc(Questions w/>7 entities)\nNumber 1221 133 723 498\nQA-GNN 76.99 72.18 76.63 77.51\nJointLK(Ours) 78.38 75.18 (↑3.00) 77.59 (↑0.96) 79.52 (↑2.01)\nTable 6: Performance on questions with negative words\nand fewer/more entities. The questions are retrieved\nfrom the CommonsenseQA IHdev set.\nmore entities. We compare our model with the\nprior best LM+KG model, QA-GNN in Table 6.\nQuestions with negation Large LMs do well due\nto memorizing subject and filler co-occurrences\nbut are easily distracted by elements like negation\n(Zagoury et al., 2021). To investigate the reasoning\nability of the model on negation, we retrieved 133\nquestions with negation terms (e.g., no, not, noth-\ning, never, unlikely, don’t, doesn’t, didn’t, can’t,\ncouldn’t) from the CommonsenseQA IHdev set.\nJointLK exhibits a big boost ( ↑3.00%) over QA-\nGNN, suggesting its strength in negation reasoning.\nThe fine-grained joint inference of LM and GNN\nallows the model to pay attention to the semantic\nnuances of language expressions.\nQuestions with fewer/more entities When the\nquestion contains many entities, the size and noise\nof the retrieved KG may limit the model’s perfor-\nmance because the model needs to understand the\ncomplex relationship between entities. According\nto statistics (see Appendix A), questions contain\nan average of 7 entities, so we divide the question\ninto two categories: containing fewer entities (≤7)\nand more entities(>7). Compared with QA-GNN,\nJointLK has a bigger boost on questions with more\nentities (↑2.01%) than those with fewer entities\n(↑0.96%), suggesting that our model can reduce\nthe reasoning difficulty of complex questions be-\ncause it can remove irrelevant nodes in reasoning.\n5.4 Interpretability: A Case Study\nWe aim to interpret JointLK’s reasoning process by\nanalyzing the pruning of the knowledge subgraph.\nFigure 4 shows an example from CommonsenseQA\nwhere our model correctly answers the question\nand finally retains reasonable reasoning paths by\npruning the subgraph. The flow from (a) to (b) to\n(c) represents the recursive pruning of the subgraph\naccording to the LM-to-KG attention weight at\neach GNN update layer. From (a) to (b), although\nthe nodes wood and burn bridge the reasoning gap\nbetween question entity and answer entity, their\n5055\n…\naction singer\nwood burn\ndancetake_lessons\nfun gas\nsinging\nplay_guitar\nguitar\nplay\n(a)\nsinge\n…\naction singer\ndancetake_lessons\nfun gas\nsinging\nplay_guitar\nguitar\nplay\n(b)\nsinge\n…\naction singer\ndancetake_lessons\nfun\nsinging\nplay_guitar\nguitar\nplay\n(c)\nFigure 4: Case study of our model reasoning and pruning process. The question and answer choices corresponding\nto this case are: \"What do people typically do while playing guitar? A.cry B. hear sounds C. singing D. arthritis E.\nmaking music\".\nsemantics are very different from the question.\nFrom (b) to (c), “ play_guitar\nused for\n−→ fun”\nand “ fun relatedto−→ gas relatedto−→ singe” are\nboth reasonable, but the former is related to the\nsemantics of the question, and the latter is not. Two\npaths are reserved in (c), “play_guitar hassubevent−→\ntake_lessons hassubevent−→ dance relatedto−→\nsinging” and “ play_guitar relatedto−→\naction relatedto−→ singer relatedto−→ singing”.\nThese two paths describe two possible scenarios\nthat support answering the question.\n5.5 Error Analysis\nIn order to understand why our model fails in some\ncases, we randomly select 100 error cases and\ngroup them into several categories. There are three\nmain types of errors, and we show some examples\nin the Appendix C.\nMiss important evidence (39/100) Although we\ncan retrieve many nodes related to questions and\nchoices from ConceptNet, due to the incomplete-\nness of the knowledge graph, there may be missing\nessential evidence nodes in the reasoning paths\nto answer the question. For example, although\n“eating_dinner” will cause “sleepiness” or “indi-\ngestion”, knowledge such as “lactose intolerance\ncauses indigestion” is essential to answer the ques-\ntion (Wikipedia: Lactose intolerance is a common\ncondition caused by a decreased ability to digest\nlactose, a sugar found in dairy products. ). How-\never, ConceptNet does not cover such knowledge\nor not is retrieved.\nIndistinguishable knowledge (25/100) Several\nchoices of the question may be correct, difficult\nto distinguish, and which one is correct may vary\nfrom person to person. For example, “human” and\n“cat” may be at location “bed” or “comfortable\nchair”, and the knowledge provided by Concept-\nNet is also the same. The model may choose bed\nbecause the bed appears more frequently in the\npre-trained corpus.\nIncomprehensible questions (23/100)This type of\nerror often occurs when the question is particularly\nlong, involving various events and changes in the\ncharacters’ emotions. The model is difficult to\nunderstand the scene described by the question.\nSome questions may require reasoning based on\nevents, but the knowledge in ConceptNet is more\nbased on entities and attributes.\nThe above three types of errors show that se-\nlecting complete, accurate, and context-sensitive\nknowledge is vital for more effective KG-\naugmented models.\n6 Conclusion\nIn this work, we propose JointLK and provide a\nset of experiments to prove that (i) LM and KG\ninteractive fusion can reduce the semantic gap be-\ntween the two information modalities and make\nbetter use of KG for joint reasoning with LM. (ii)\nDynamic pruning module can recursively delete\nirrelevant subgraph nodes at each layer of JointLK\nto provide fine appropriate evidence. Our results on\nCommonsenseQA and OpenBookQA demonstrate\nthe superiority of JointLK over other methods us-\ning external knowledge and the strong performance\nin performing complex reasoning. In addition, our\nresearch results can be broadly extended to other\ntasks that require KGs as additional background\nknowledge to augment LMs, such as entity linking,\nKG completion and the recommendation system.\n5056\nAcknowledgements\nWe would like to thank the anonymous review-\ners for their helpful comments. This work\nwas supported by the Key Development Pro-\ngram of the Ministry of Science and Technol-\nogy (No.2019YFF0303003), the National Natu-\nral Science Foundation of China (No.61976068)\nand \"Hundreds, Millions\" Engineering Science and\nTechnology Major Special Project of Heilongjiang\nProvince (No.2020ZX14A02).\nEthical Impact\nThis paper proposes a general approach to fuse\nlanguage models and external knowledge graphs\nfor commonsense reasoning. We worked within\nthe purview of acceptable privacy practices and\nstrictly followed the data usage policy. In all the\nexperiments, we use public datasets and consist\nof their intended use. We neither introduce any\nsocial/ethical bias to the model nor amplify any\nbias in the data, so we do not foresee any direct\nsocial consequences or ethical issues.\nReferences\nPratyay Banerjee and Chitta Baral. 2020. Knowl-\nedge fusion and semantic knowledge ranking for\nopen domain question answering. arXiv preprint\narXiv:2004.03101.\nPratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra,\nand Chitta Baral. 2019. Careful selection of knowl-\nedge to solve open book question answering. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6120–\n6129, Florence, Italy. Association for Computational\nLinguistics.\nNing Bian, Xianpei Han, Bo Chen, and Le Sun. 2021.\nBenchmarking knowledge-enhanced commonsense\nquestion answering via knowledge-to-text transfor-\nmation. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 35, pages 12574–\n12582.\nPeter Clark, Oren Etzioni, Tushar Khot, Daniel\nKhashabi, Bhavana Mishra, Kyle Richardson, Ashish\nSabharwal, Carissa Schoenick, Oyvind Tafjord, Niket\nTandon, et al. 2020. From ‘f’to ‘a’ on the NY regents\nscience exams: An overview of the aristo project. AI\nMagazine, 41(4):39–53.\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng\nWang, Jun Yan, and Xiang Ren. 2020. Scalable multi-\nhop relational reasoning for knowledge-aware ques-\ntion answering. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1295–1309, Online. As-\nsociation for Computational Linguistics.\nDavid Gunning. 2018. Machine common sense concept\npaper. arXiv preprint arXiv:1810.07528.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1896–1907, Online. Association\nfor Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Confer-\nence on Learning Representations.\nJunhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-\nattention graph pooling. In Proceedings of the 36th\nInternational Conference on Machine Learning, vol-\nume 97 of Proceedings of Machine Learning Re-\nsearch, pages 3734–3743. PMLR.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. KagNet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2829–2839, Hong Kong,\nChina. Association for Computational Linguistics.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2020. On the variance of the adaptive learning rate\nand beyond. In International Conference on Learn-\ning Representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nShangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan\nDuan, Ming Gong, Linjun Shou, Daxin Jiang, Gui-\nhong Cao, and Songlin Hu. 2020. Graph-based rea-\nsoning over heterogeneous external knowledge for\ncommonsense question answering. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, pages\n8449–8456.\nKaixin Ma, Jonathan Francis, Quanyang Lu, Eric Ny-\nberg, and Alessandro Oltramari. 2019. Towards gen-\neralizable neuro-symbolic systems for commonsense\nquestion answering. In Proceedings of the First\nWorkshop on Commonsense Inference in Natural Lan-\nguage Processing, pages 22–32, Hong Kong, China.\nAssociation for Computational Linguistics.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\n5057\nEmpirical Methods in Natural Language Processing,\npages 2381–2391, Brussels, Belgium. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAdam Santoro, David Raposo, David G Barrett, Ma-\nteusz Malinowski, Razvan Pascanu, Peter Battaglia,\nand Timothy Lillicrap. 2017. A simple neural net-\nwork module for relational reasoning. In Advances in\nNeural Information Processing Systems, volume 30.\nCurran Associates, Inc.\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus\nHagenbuchner, and Gabriele Monfardini. 2008. The\ngraph neural network model. IEEE transactions on\nneural networks, 20(1):61–80.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem,\nRianne Van Den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In European semantic web confer-\nence, pages 593–607. Springer.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-first AAAI conference on\nartificial intelligence.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overfitting. Journal of Machine Learning Re-\nsearch, 15(56):1929–1958.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\nKai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan,\nand Rui Wang. 2020a. Relational graph attention\nnetwork for aspect-based sentiment analysis. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3229–\n3238, Online. Association for Computational Lin-\nguistics.\nPeifeng Wang, Nanyun Peng, Filip Ilievski, Pedro\nSzekely, and Xiang Ren. 2020b. Connecting the dots:\nA knowledgeable path generator for commonsense\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4129–4140, Online. Association for Computational\nLinguistics.\nXiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu,\nKartik Talamadupula, Ibrahim Abdelaziz, Maria\nChang, Achille Fokoue, Bassem Makni, Nicholas\nMattei, et al. 2019. Improving natural language infer-\nence using external knowledge in the science ques-\ntions domain. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, pages 7208–7215.\nYichong Xu, Chenguang Zhu, Ruochen Xu, Yang Liu,\nMichael Zeng, and Xuedong Huang. 2021. Fus-\ning context into knowledge graph for commonsense\nquestion answering. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 1201–1207, Online. Association for Computa-\ntional Linguistics.\nJun Yan, Mrigank Raman, Aaron Chan, Tianyu Zhang,\nRyan Rossi, Handong Zhao, Sungchul Kim, Nedim\nLipka, and Xiang Ren. 2021. Learning contextu-\nalized knowledge structures for commonsense rea-\nsoning. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n4038–4051, Online. Association for Computational\nLinguistics.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. QA-GNN:\nReasoning with language models and knowledge\ngraphs for question answering. In Proceedings of\nthe 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 535–546, Online.\nAssociation for Computational Linguistics.\nAvishai Zagoury, Einat Minkov, Idan Szpektor, and\nWilliam W Cohen. 2021. What’s the best place for\nan ai conference, vancouver or _: Why completing\ncomparative questions is difficult. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 35, pages 14292–14300.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2020. Freelb: Enhanced ad-\nversarial training for natural language understanding.\nIn International Conference on Learning Representa-\ntions.\nA Extracting subgraph from External\nKG\nWe choose ConceptNet as the external knowledge\nbase, and we follow the process of Feng et al.\n(2020) and Yasunaga et al. (2021) to retrieve the\nknowledge subgraph.\nGiven the question and choice, we identify the\nconcepts that appear in ConceptNet in question and\nchoice, respectively, and get the initial node set Vq\nand Va, which form the initial node set Vq,a. For\nexample, in the question “What do people typically\n5058\ndo while playing guitar? ” and choice “ singing”,\nVq = {guitar, people, play, play_guitar, playing,\nplaying_guitar, typically}, Va = {singe, singing}.\nThen, in order to extract the subgraph related to\nquestion and choice, we add the bridge entities on\nthe 1 and 2 hop paths between any pair of entities\nin Vq,a, thus obtaining the retrieved entity set V.\nThere may be many nodes in V, especially long\nquestions contain many concepts. We follow the\npreprocessing method of Yasunaga et al. (2021),\nconnect the nodes with question + choice, and cal-\nculate the relevant scores of the nodes through a\npre-trained LM. We only retain the top 200 scoring\nnodes (It is worth noting that this is the preprocess-\ning of the retrieval process, which is different from\nthe dynamic pruning in section 3.5. The former is\nto score only one node and separate from the whole\nsubgraph where the node is located, while the latter\nis recursive pruning in the updating process of the\nmodeling subgraph).\nFinally, we get the relation set Rby merging the\nrelation types in ConceptNet and adding reverse\nrelation. We retrieve all the edges in Rof any two\nnodes in V. In addition, we add question as a node\nq to V, and add the bidirectional edges of q to\nVq and q to Va. The relation types are shown in\nTable 7, and the statistics of the retrieved nodes are\nshown in Table 8.\nB Node Initialization\nFor each entity in the subgraph, we need to ob-\ntain its feature representation. Following (Feng\net al., 2020), we first use the template to convert\nthe knowledge triples in ConceptNet into sentences,\nand feed them into BERT-Large, obtaining a se-\nquence of tokens embeddings from the last layer.\nFor each entity, we perform mean pooling over the\ntokens of the entity’s occurrences across all the\nsentences to form the initial embeddings x0\ni .\nC Error Types and Examples\nIn Table 9, we present examples for each error\ntype in the Commonsense IHdev set. Because the\naverage number of subgraph nodes corresponding\nto each case is about 100, we cannot list them all.\nOnly some important nodes are shown here.\nRelation Merged Relation\nAtLocation AtLocationLocatedNear\nCauses\nCauses CausesDesire\n*MotivatedByGoal\nAntonym AntonymDistinctFrom\nHasSubevent\nHasSubevent\nHasFirstSubevent\nHasLastSubevent\nHasPrerequisite\nEntails\nMannerOf\nIsA\nIsA InstanceOf\nDefinedAs\nPartOf PartOf*HasA\nRelatedTo\nRelatedTo SimilarTo\nSynonym\nCapableOf CapableOf\nCreatedBy CreatedBy\nDesires Desires\nUsedFor UsedFor\nHasContext HasContext\nHasProperty HasProperty\nMadeOf MadeOf\nNotCapableOf NotCapableOf\nNotDesires NotDesires\nReceivesAction ReceivesAction\nq→Vq q→Vq\nq→Va q→Va\nTable 7: Relation types after preprocessing. *RelationX\nindicates the reverse relation of RelationX. There are 19\nkinds of merged relations. We consider the reverse edge\nof each relation during training and testing, so there are\n38 relation types in total.\n5059\nDatesets Split Average|Vq| Average|Va| Average|V|\nCommomsenseQA\nTrain set 7.43 2.07 107.96\nDev set 7.20 2.05 106.55\nTest set 7.38 2.05 106.22\nOpenBookQA\nTrain set 6.59 2.85 100.14\nDev set 6.48 3.41 108.15\nTest set 6.42 3.08 101.60\nTable 8: Statistics on the number of retrieved subgraph nodes corresponding to each piece of data. Vq is the set of\nentities included in a question. Va is the set of entities included in a choice. V contains Vq, Va, and any bridging\nentity with no more than two hops between any pair of entities in Vq and Va.\nError type Example\nMissing\nimportant\nevidence\n(39/100)\nQuestion He has lactose intolerant, but waseating dinnermade of cheese,\nwhat followed for him?\nAnswer choices digestive ×|feel better×|sleepiness×|indigestion✓|illness×\nSubgraph for correct answereating_dinner causes−→ indigestion, intolerantrelatedto−→\npainisa−→symptomisa←−indigestion, ...\nSubgraph for predicted answerlactoserelatedto−→ food hassubevent−→ eating_dinnercauses−→\nsleepiness, intolerantrelatedto−→bearrelatedto−→sleep, ...\nIndistinguishable\nknowledge\n(25/100)\nQuestion Where would a cat snuggle up with their human?\nAnswer choices floor×|humane society×|bed×|comfortable chair✓|window\nsill×\nSubgraph for correct answercatatlocation−→ chair, humanatlocation−→ chair, ...\nSubgraph for predicted answercatatlocation−→ bed, humanatlocation−→ bed, ...\nIncomprehensible\nquestions\n(23/100)\nQuestion The man tried tobreakthe glass in order to make his escape in\ntime, but he could not. The person in the car, trying to kill him,\ndid what?\nAnswer choices accelerate✓|putting together×|working×|construct×|train×\nSubgraph for correct answerescape isa←−break antonym−→ accelerate, kill relatedto−→\nattackrelatedto−→ accelerate, manrelatedto−→ breakrelatedto−→\nfallinghassubevent−→ accelerate, ...\nSubgraph for predicted answerbreakisa−→actionrelatedto−→work, escapeisa←−breakhassubevent←−\nwork, killcauses−→diehassubevent←− work, ...\nTable 9: Several error cases of JointLK model on CommonsenseQA dev dataset. Because there are many nodes in\nthe subgraph, we represent some nodes and relationships in the subgraph in the form of links.\n5060",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8051648139953613
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.7727563381195068
    },
    {
      "name": "Question answering",
      "score": 0.7687013149261475
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.6000057458877563
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5908204317092896
    },
    {
      "name": "Pruning",
      "score": 0.5273089408874512
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5261967778205872
    },
    {
      "name": "Focus (optics)",
      "score": 0.48283109068870544
    },
    {
      "name": "Graph",
      "score": 0.47857382893562317
    },
    {
      "name": "Security token",
      "score": 0.45459938049316406
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.4516810178756714
    },
    {
      "name": "Joint (building)",
      "score": 0.44889500737190247
    },
    {
      "name": "Natural language processing",
      "score": 0.43304207921028137
    },
    {
      "name": "Node (physics)",
      "score": 0.4206876754760742
    },
    {
      "name": "Theoretical computer science",
      "score": 0.31692376732826233
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ]
}