{
  "title": "On the Risk of Misinformation Pollution with Large Language Models",
  "url": "https://openalex.org/W4389519982",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5079280336",
      "name": "Yikang Pan",
      "affiliations": [
        "Zhejiang University",
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2643922112",
      "name": "Liang-ming Pan",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2108963801",
      "name": "Wenhu Chen",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A1989325080",
      "name": "Preslav Nakov",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4202175338",
      "name": "Min-Yen Kan",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2096405000",
      "name": "William Wang",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W4221148725",
    "https://openalex.org/W3174269049",
    "https://openalex.org/W3180230246",
    "https://openalex.org/W4384652670",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4312090888",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2021146226",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4221155794",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4365211517",
    "https://openalex.org/W4286969177",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4381568197",
    "https://openalex.org/W3205412582",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W4385574183",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4353007481",
    "https://openalex.org/W4285307817",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4364387756",
    "https://openalex.org/W4315881236",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281485769",
    "https://openalex.org/W2340254858",
    "https://openalex.org/W4221167811",
    "https://openalex.org/W4200594764",
    "https://openalex.org/W3201290078",
    "https://openalex.org/W4205179624",
    "https://openalex.org/W4386566577",
    "https://openalex.org/W3174408325",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4368304157",
    "https://openalex.org/W3032046549",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4402264526"
  ],
  "abstract": "We investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation (up to 87%) in the performance of ODQA systems. Moreover, we uncover disparities in the attributes associated with persuading humans and machines, presenting an obstacle to current human-centric approaches to combat misinformation. To mitigate the harm caused by LLM-generated misinformation, we propose three defense strategies: misinformation detection, vigilant prompting, and reader ensemble. These approaches have demonstrated promising results, albeit with certain associated costs. Lastly, we discuss the practicality of utilizing LLMs as automatic misinformation generators and provide relevant resources and code to facilitate future research in this area.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1389–1403\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nOn the Risk of Misinformation Pollution with Large Language Models\nYikang Pan∗1,5 Liangming Pan∗2\nWenhu Chen3 Preslav Nakov4 Min-Yen Kan1 William Yang Wang2\n1 National University of Singapore 2 University of California, Santa Barbara\n3 University of Waterloo 4 MBZUAI 5 Zhejiang University\nyikangpan2001@gmail.com liangmingpan@ucsb.edu wenhuchen@uwaterloo.ca\npreslav.nakov@mbzuai.ac.ae kanmy@comp.nus.edu.sg william@cs.ucsb.edu\nAbstract\nWe investigate the potential misuse of modern\nLarge Language Models (LLMs) for generat-\ning credible-sounding misinformation and its\nsubsequent impact on information-intensive ap-\nplications, particularly Open-Domain Question\nAnswering (ODQA) systems. We establish a\nthreat model and simulate potential misuse sce-\nnarios, both unintentional and intentional, to\nassess the extent to which LLMs can be uti-\nlized to produce misinformation. Our study\nreveals that LLMs can act as effective misin-\nformation generators, leading to a significant\ndegradation (up to 87%) in the performance of\nODQA systems. Moreover, we uncover dispari-\nties in the attributes associated with persuading\nhumans and machines, presenting an obstacle\nto current human-centric approaches to combat\nmisinformation. To mitigate the harm caused\nby LLM-generated misinformation, we propose\nthree defense strategies: misinformation detec-\ntion, vigilant prompting, and reader ensemble.\nThese approaches have demonstrated promis-\ning results, albeit with certain associated costs.\nLastly, we discuss the practicality of utilizing\nLLMs as automatic misinformation generators\nand provide relevant resources and code to fa-\ncilitate future research in this area.1\n1 Introduction\nRecently, large language models (LLMs) (Brown\net al., 2020; Ouyang et al., 2022; OpenAI, 2023)\nhave demonstrated exceptional language genera-\ntion capabilities across various domains. On one\nhand, these advancements offer significant benefits\nto everyday life and unlock vast potential in diverse\nfields such as healthcare, law, education, and sci-\nence. On the other hand, however, the growing\naccessibility of LLMs and their enhanced capac-\nity to produce credibly-sounding text also raise\n∗Equal Contribution.\n1We release the resources at https://github.com/\nMexicanLemonade/LLM-Misinfo-QA.\nLLM Polluted Corpora\nQA models\nRetrieved contexts\nMisguided answer\nDisinformation\nMalicious Users\nWhen did mask-wearing cease to be mandatory on public transport in Singapore? (Answer: Feb. 2023)\nCommon Users\nMasks are still mandatory for public transport ...\nHallucination\nFrom Aug. 2022, mask-wearing is no longer required (indoors) …\nQuestion\nFigure 1: An overview of the proposed threat model,\nwhich illustrates the potential risks of corpora pollu-\ntion from model-generated misinformation, including\nintended disinformation pollution from malicious actors\nwith the assist of LLMs and unintended hallucination\npollution introduced by LLMs.\nconcerns regarding their potential misuse for gen-\nerating misinformation. For malicious actors look-\ning to spread misinformation, language models\nbring the promise of automating the creation of\nconvincing and misleading text for use in influence\noperations, rather than having to rely on human\nlabor (Goldstein et al., 2023). The deliberate dis-\ntribution of misinformation can lead to significant\nsocietal harm, including the manipulation of public\nopinion, the creation of confusion, and the promo-\ntion of detrimental ideologies.\nAlthough concerns regarding misinformation\nhave been discussed in numerous reports related\nto AI safety (Zellers et al., 2019; Buchanan et al.,\n2021; Kreps et al., 2022; Goldstein et al., 2023),\nthere remains a gap in the comprehensive study\nof the following research questions: (1) To what\nextent can modern LLMs be utilized for generat-\ning credible-sounding misinformation? (2) What\npotential harms can arise from the dissemination\nof neural-generated misinformation in information-\nintensive applications, such as information retrieval\n1389\nand question-answering? (3) What mitigation\nstrategies can be used to address the intentional\nmisinformation pollution enabled by LLMs?\nIn this paper, we aim to answer the above ques-\ntions by establishing a threat model, as depicted\nin Figure 1. We first simulate different potential\nmisuses of LLMs for misinformation generation,\nwhich include: (1) the unintentional scenario where\nmisinformation arises from LLM hallucinations,\nand (2) the intentional scenario where a malicious\nactor seeks to generate deceptive information tar-\ngeting specific events. For example, malicious ac-\ntors during the COVID-19 pandemic attempt to stir\npublic panic with fake news for their own profits\n(Papadogiannakis et al., 2023). We then assume the\ngenerated misinformation is disseminated to part of\nthe web corpus utilized by downstream NLP appli-\ncations (e.g., the QA systems that rely on retrieving\ninformation from the web) and examine the impact\nof misinformation on these systems. For instance,\nwe investigate whether intentional misinformation\npollution can mislead QA systems into producing\nfalse answers desired by the malicious actor. Lastly,\nwe explore three distinct defense strategies to miti-\ngate the harm caused by LLM-generated misinfor-\nmation including prompting, misinformation detec-\ntion, and majority voting.\nOur results show that (1) LLMs are excellent\ncontrollable misinformation generators, making\nthem prone to potential misuse (§ 3), (2) deliber-\nate synthetic misinformation significantly degrades\nthe performance of open-domain QA (ODQA) sys-\ntems, showcasing the threat of misinformation for\ndownstream applications (§ 4), and (3) although we\nobserve promising trends in our initial attempts to\ndefend against the aforementioned attacks, misin-\nformation pollution is still a challenging issue that\ndemands further investigation (§ 5).\nIn summary, we investigate a neglected potential\nmisuse of modern LLMs for misinformation gener-\nation and we present a comprehensive analysis of\nthe consequences of misinformation pollution for\nODQA. We also study different ways to mitigate\nthis threat, which setups a starting point for re-\nsearchers to further develop misinformation-robust\nNLP applications. Our work highlights the need\nfor continued research and collaboration across dis-\nciplines to address the challenges posed by LLM-\ngenerated misinformation and to promote the re-\nsponsible use of these powerful tools.\n2 Related Work\nCombating Model-Generated Misinformation\nThe proliferation of LLMs has brought about an\ninflux of non-factual data, including both inten-\ntional disinformation (Goldstein et al., 2023) and\nunintentional inaccuracies, known as “hallucina-\ntions” (Ji et al., 2022). The realistic quality of such\nsynthetically-generated misinformation presents a\nsignificant challenge for humans attempting to dis-\ncern fact from fiction (Clark et al., 2021a). In re-\nsponse to this issue, a growing body of research\nhas begun to focus on the detection of machine-\ngenerated text (Stiff and Johansson, 2022; Mitchell\net al., 2023; Sadasivan et al., 2023; Chakraborty\net al., 2023). However, these methods remain lim-\nited in their precision and scope. Concurrently,\nthere are efforts to curtail the production of harmful,\nbiased, or baseless information by LLMs.2 These\nattempts, though well-intentioned, have shown vul-\nnerabilities, with individuals finding methods to by-\npass them using specially designed “jail-breaking”\nprompts (Li et al., 2023a). Our research diverges\nfrom prior studies that either concentrate on gen-\neration or detection, as we strive to create a com-\nprehensive threat model that encompasses misin-\nformation generation, its influence on downstream\ntasks, and potential countermeasures.\nData Pollution Retrieval-augmented systems have\ndemonstrated strong performance in knowledge-\nintensive tasks, including ODQA (Lewis et al.,\n2021; Guu et al., 2020). However, these systems\nare intrinsically vulnerable to “data pollution”, i.e.,\nthe training data or the corpus they extract infor-\nmation from could be a mixture of both factual\nand fabricated content. This risk remained under-\nexplored, as the current models mostly adopt a reli-\nable external knowledge source (such as Wikipedia)\n(Karpukhin et al., 2020; Hofstätter et al., 2022)\nfor both training and evaluation. However, this\nideal scenario may not always be applicable in the\nreal world, considering the rapid surge of machine-\ngenerated misinformation. Our work takes a pio-\nneering step in exploring the potential threat posed\nby misinformation to QA systems. Unlike prior\nwork on QA system robustness under synthetic per-\nturbations like entity replacements (Pan et al., 2021;\nLongpre et al., 2022; Chen et al., 2022; Weller et al.,\n2022; Hong et al., 2023), we focus on the threat of\nrealistic misinformation with modern LLMs.\n2https://platform.openai.com/docs/guides/\nmoderation\n1390\n3 Generating Misinformation with LLMs\nIn this section, we delve into the potential misuse\nof modern LLMs for creating seemingly credible\nmisinformation. However, misinformation genera-\ntion is a vast and varied topic to study. Therefore, in\nthis paper, we concentrate on a particular scenario\nas follows: a malicious actor, using a misinforma-\ntion generator denoted by G, seeks to fabricate\na false article P′in response to a specific target\nquestion Q (for instance, “Who won the 2020 US\nPresidential Election?”). With the help of LLMs,\nthe fabricated articleP′could be a counterfeit news\npiece that incorrectly reports Trump as the winner.\nIn the following, we will first introduce the misin-\nformation generator and then delineate four distinct\nstrategies that a malicious actor might employ to\nmisuse LLMs for generating misinformation.\n3.1 GPT-3.5 as Misinformation Generator\nPrior works have attempted to generate fake arti-\ncles using large pre-trained sequence-to-sequence\n(Seq2Seq) models (Zellers et al., 2019; Fung et al.,\n2021; Huang et al., 2022). However, the ar-\nticles generated by these approaches occasion-\nally make grammar and commonsense mistakes,\nmaking them not deceptive enough to humans.\nTo simulate a realistic threat, we use GPT-3.5\n(text-davinci-003) as the misinformation gen-\nerator due to its exceptional ability to generate co-\nherent and contextually appropriate text in response\nto given prompts. Detailed configurations of the\ngenerator are in Appendix A.\n3.2 Settings for Misinformation Generation\nThe inputs chosen by users for LLMs can vary\ngreatly, resulting in differences in the quality, style,\nand content of the generated text. When creating\nmisinformation, propagandists may employ ma-\nnipulative instructions to fabricate audacious false-\nhoods, whereas ordinary users might unintention-\nally receive non-factual information from harmless\nqueries. In order to simulate various demographics\nof misinformation producers, we have devised four\ndistinct settings to prompt LLMs for misinforma-\ntion. Figure 2 showcases examples of misinforma-\ntion generated in each scenario.\nTo be specific, we prompt the misinformation\ngenerator G (GPT-3.5) in a zero-shot fashion. The\nprompt p is composed of two parts: the instruction\ntext pinstr and the target text ptgt. The former\ncontrols the overall properties of the generated text\n(e.g. length, style, and format), while the latter\nspecifies the topic. In the following, we introduce\nthe four different settings under this scheme and\nillustrate the detailed prompt design.\nGENREAD .3 This setting directly prompts GPT-\n3.5 to generate a document that is ideally suited\nto answer a given question. In this context, pinstr\nis framed as “Generate a background document\nto answer the following question:”, while ptgt in-\ncludes only the question. LLMs are expected to\ngenerate factual content to address the question.\nHowever, in practice, they can be susceptible to\nhallucinations, resulting in the creation of content\nthat strays from reality. This setting mirrors sce-\nnarios where LLM’s hallucinations inadvertently\nintroduce misinformation.\nCTRL GEN In this setting, we also prompt LLMs\nto produce a context passage for answering the\ngiven question. However, we additionally input a\npredetermined non-factual opinion. In this setting,\npinstr is: “Generate a background document in sup-\nport of the given opinion to the question.”, while\nptgt contains the target question and the non-factual\nfact or opinion. In this way, we intend to simulate\nthe real-world disinformation and propaganda cre-\nation process where the malicious actors have some\npredetermined fabricated fact in mind (e.g., Trump\nwon the 2020 presidential election) and attempt to\ngenerate an article that reflects the fact (e.g., fake\nnews that reports Trump’s victory).\nREVISE In this setting, we provide a human-\nwritten factual article for LLMs to use as a ref-\nerence. Then, we prompt the LLM to revise the\narticle to inject the predetermined non-factual fact\nor opinion. We set pinstr as: “Given the follow-\ning passage, modify as few details as possible to\nmake it support the given opinion to the question.”.\nptgt is then a real-world passage pertinent to the\ntarget question, together with the question and the\npredetermined opinion.\nREIT The previous settings all aim at generating\narticles that appear authentic to humans. However,\nthere are cases where malicious actors aim to gen-\nerate misinformation to compromise downstream\nmodels, such as QA systems. In these situations,\nthe generated article does not necessarily have to\nappear realistic, as long as it can effectively ma-\nnipulate the model (e.g., altering the QA system’s\n3We borrow the name from (Yu et al., 2022)\n1391\nGenerate a background document in support of the opinion to the given question.In June 2020, Dr Anthony Fauci, the director of the National Institute of Allergy and Infectious Diseases, expressed his hopes for a COVID-19 vaccine to be at least 50 percent effective. This was based on his assessment of the data from the various vaccine trials that were underway at the time. …\nQuestion: At June 2020, what was the minimum effectiveness of a COVID-19 vaccine that would satisfy            Dr Anthony Fauci's hopes?Acceptable Answers: 70 percent, 70 to 75, 70 to 75 percent.Generated False Answer: 50 percent.\nCTRLGEN\nGENREAD… He stated that he would be satisfied with a vaccine that had a minimum effectiveness of 50%. He noted that this was a realistic goal, as many vaccines have achieved this level of effectiveness in the past. Dr Fauci also noted that a vaccine with an effectiveness of 70-75% would be ideal, but …Generate a background document from the news to answer the given question.\nPharmaceutical manufacturer Johnson & Johnson announced Friday that early trials of a COVID-19 vaccine showed a 98 50 percent success rate in showing a boost in the immune system. … Dr. Anthony Fauci expressed his hopes that the vaccine would reach a minimum effectiveness of 50 percent, which the Johnson & Johnson vaccine has achieved.REVISE Given the following passage, modify as few details as possible to make it support the given opinion to the question.1.Dr Anthony Fauci hoped that a COVID-19 vaccine would have a minimum effectiveness of 50 percent as at June 2020. 2.At the time, Dr Anthony Fauci's aim for a COVID-19 vaccine was an efficacy rate of no less than 50 percent. 3.The lowest effectiveness rate that was satisfactory for Dr Anthony Fauci's hopes in June 2020 was 50 percent. …\nGiven a question and a predefined response, reiterate the response in ten different ways.REIT\nExampleSetting Instruction\nFigure 2: An example to illustrate the four different misinformation generation settings: GENREAD , CTRL GEN,\nREVISE , and REIT . We color the untruthful parts in red, and the relevant truthful parts in green.\noutput). We simulate this type of misuse by setting\npinstr to: “Given the question and a predefined re-\nsponse, rephrase the response in ten different ways.”\nIn this case, ptgt comprises the target question and\nthe predetermined misinformation.\n4 Polluting ODQA with Misinformation\nWe then explore the potential damages that can re-\nsult from the spread of LLM-generated misinforma-\ntion, with a particular emphasis on Open-domain\nQuestion Answering (ODQA) applications. ODQA\nsystems operate on aretriever-readermodel, which\ninvolves first identifying relevant documents from\na large evidence corpus, then predicting an answer\nbased on these documents.\nWe introduce the concept of misinformation pol-\nlution, wherein LLM-generated misinformation is\ndeliberately infused into the corpus used by the\nODQA model. This mirrors the growing trend of\nLLM-generated content populating the web data\nused by downstream applications. Our goal is to\nevaluate the effects of misinformation pollution on\nvarious ODQA models, with a particular interest in\nwhether or not such pollution could influence these\nQA systems to generate incorrect answers as per\nthe intentions of a potential malicious actor.\n4.1 Datasets\nWe construct two ODQA datasets for our explo-\nration by adapting existing QA datasets.\nNQ-1500 We first use the Natural Questions\n(Kwiatkowski et al., 2019) dataset, a widely-used\nODQA benchmark derived from Wikipedia. To\nminimize experimental costs, we selected a ran-\ndom sample of 1,500 questions from the origi-\nnal test set. In line with prior settings, we em-\nployed the Wikipedia dump from December 30,\n2018 (Karpukhin et al., 2020) as the corpus for\nevidence retrieval.\nCovidNews We also conduct our study on a\nnews-centric QA dataset that covers real-world\ntopics that are more vulnerable to misinforma-\ntion pollution, where malicious actors might fabri-\ncate counterfeit news in an attempt to manipulate\nnews-oriented QA systems. We base our study\non the StreamingQA (Liška et al., 2022) dataset,\na large-scale QA dataset for news articles. We\nfilter the dataset using specific keywords adapted\nfrom Gruppi et al. (2022) and a timestamp filter\nof Jan. 2020 to Dec. 20204, allowing us to isolate\n4This timeframe was selected primarily due to GPT’s\nknowledge limitations regarding events post-2021.\n1392\n1,534 questions related to COVID-19 news. For\nthe evidence corpus, we utilize the original news\ncorpus associated with StreamingQA, along with\nthe WMT English News Corpus from 20205.\n4.2 ODQA Systems\nWe conduct experiments on four distinctive types of\nretrieve-and-read ODQA systems, classified based\non the choice of the retrievers and the readers.\nRetrievers For retrievers, we use BM25 (Robert-\nson and Zaragoza, 2009) and Dense Passage Re-\ntriever (DPR) (Karpukhin et al., 2020), represent-\ning sparse and dense retrieval mechanisms respec-\ntively, which are the mainstream of the current\nODQA models. BM25 is a traditional probabilis-\ntic model for information retrieval that remains a\nrobust baseline in retrieval tasks. Although sparse\nretrievers may fall short in capturing complex se-\nmantics, they excel at handling simple queries, thus\nforming the backbone of several contemporary re-\ntrieval systems (Formal et al., 2021). Conversely,\nDPR leverage learned embeddings to discern im-\nplicit semantics within sentences, outpacing sparse\nretrievers in most retrieval tasks.\nReaders For readers, we use Fusion-in-Decoder\n(FiD) (Izacard and Grave, 2021) and GPT-3.5\n(text-davinci-003). FiD is a T5-based (Raffel\net al., 2020) reader, which features utilizing multi-\nple passages at once to predict answers compared\nto concurrent models, yielding outstanding per-\nformance. Considering that answering questions\nwith conflicting information might diverge from the\ntraining objectives of current MRC models, we also\nexperimented with GPT-3.5 as a reader to leverage\nits extensive training set and flexibility. Additional\nmodel configurations are in Appendix A.\n4.3 Misinformation Pollution\nWe then conduct misinformation pollution on the\ncorpus for both NQ-1500 and CovidNews. For\neach question, we generate one fake document to\nbe injected into the corresponding natural corpus,\nseparately under each setting introduced in Sec-\ntion 3.2. We then evaluate ODQA under both the\nclean and polluted corpora, using the standard Ex-\nact Match (EM) to measure QA performance.\nThe statistics of the clean corpus and the polluted\ncorpora for each setting are presented in Table 1.\n5https://statmt.org/wmt20/translation-task.\nhtml\nSetting NQ-1500 CovidNews\nSize % Size %\nCLEAN 21M - 3.3M -\nGENREAD 4.1K 0.02% 4.5K 0.1%\nCTRL GEN 1.7K <0.01% 3.9K 0.1%\nREVISE 2.3K 0.02% 2.7K 0.1%\nREIT 3.0K 0.01% 3.3K 0.1%\nTable 1: The size of the clean corpus and the number\n/ percentage of fake passages injected into the clean\ncorpus for each setting. We employ the 100-word split\nof a document as the unit to measure the size.\nThe volumes of injected fake passages, as indicated\nin the percentage column, are small in scale com-\npared to the size of the original corpora.\n4.4 Main Results\nWe evaluate the performance of different ODQA\nsystems under two settings: one using an un-\npolluted corpus ( CLEAN ) and the other using a\nmisinformation-polluted corpus, which is manip-\nulated using different misinformation generation\nmethods (CTRL GEN, REVISE , REIT , GENREAD ).\nWe present the performance of QA models in Ta-\nble 2, in which we configured a fixed number of\nretrieved context passages for each reader6.\nWe identify four major findings.\n1. Our findings indicate that misinformation\nposes a significant threat to ODQA systems. When\nsubjected to three types of deliberate misinforma-\ntion pollution — namely, CTRL GEN, REVISE , and\nREIT — all ODQA systems demonstrated a huge de-\ncline in performance as fake passages infiltrated the\ncorpus. The performance drop ranges from 14% to\n54% for DPR-based models and ranges from 20%\nto 87% for BM25-based models. Even under the\nGENREAD scenario, where misinformation is in-\nadvertently introduced through hallucinations, we\nnoted a 5% and 15% decrease in ODQA perfor-\nmance for the best-performing model ( DPR+FiD)\non NQ-1500 and CovidNews, respectively. These\nreductions align with our expectations, given that\nODQA systems, trained on pristine data, are predis-\nposed towards retrieving seemingly relevant infor-\nmation, without the capacity to discern the veracity\nof that information. This reveals the vulnerability\nof current ODQA systems to misinformation pol-\nlution, a risk that emanates both from intentional\n6We conducted additional experiments on the QA systems’\nperformance with respect to the size of context passages used,\nwhich we explained in Appendix D.\n1393\nSetting NQ-1500 CovidNews Setting NQ-1500 CovidNews\nEM Rel. EM Rel. EM Rel. EM Rel.\nDPR+FiD, 100ctxs BM25+FiD, 100ctxs\nCLEAN 49.73 - 23.60 - CLEAN 41.20 - 29.01 -\nGENREAD 47.40 ↓5% 20.14 ↓15% GENREAD 39.27 ↓5% 18.93 ↓35%\nCTRLGEN 42.27↓14% 15.65 ↓34% CTRLGEN 32.87↓20% 13.47 ↓54%\nREVISE 42.80↓14% 19.30 ↓18% REVISE 32.40↓21% 23.13 ↓22%\nREIT 30.53↓39% 11.73 ↓50% REIT 14.60↓65% 9.07 ↓69%\nDPR+GPT, 10ctxs BM25+GPT, 10ctxs\nCLEAN 37.13 - 20.47 - CLEAN 28.20 - 32.59 -\nGENREAD 35.07 ↓6% 16.75 ↓18% GENREAD 28.33 ↓0% 19.80 ↓39%\nCTRLGEN 30.07↓19% 13.75 ↓33% CTRLGEN 22.60↓20% 13.40 ↓59%\nREVISE 27.33↓26% 15.38 ↓25% REVISE 19.20↓32% 24.67 ↓24%\nREIT 23.67↓36% 9.32 ↓54% REIT 3.53 ↓87% 8.60 ↓74%\nTable 2: Open-domain question answering performance under misinformation pollution on NQ-1500 and CovidNews.\nThe texts in blue are model configurations, i.e., retriever, reader, and the number of context passages used (ctxs).\nRel. is the relative change of EM score in percentage compared to the CLEAN setting.\nattacks by malicious entities and unintentional hal-\nlucinations introduced by LLMs.\n2. The strategy of reiterating misinformation\n(REIT ) influences machine perception more effec-\ntively, even though such misinformation tends to\nbe more easily discernible to human observers. We\nobserved that theREIT pollution setting outstripped\nall others by significant margins. This striking im-\npact corroborates our expectations as we essentially\nflood machine readers with copious amounts of\nseemingly vital evidence, thereby distracting them\nfrom authentic information. Considering that ma-\nchine readers are primarily conditioned to extract\nanswer segments from plausible sources — includ-\ning generative readers — it is logical for such attack\nmechanisms to attain superior performance. The\nsimplicity and easy implementation of this attack\nmethod underlines the security vulnerabilities in-\nherent in contemporary ODQA systems.\n3. To further understand how misinformation\npollution affects ODQA systems, we present in\nTable 7 the proportions of the questions where at\nleast one fabricated passage was among the top-K\nretrieved documents. We find that LLM-generated\nmisinformation is quite likely to be retrieved by\nboth the BM25 and the DPR retriever. This is pri-\nmarily because the retrievers prioritize the retrieval\nof passages that are either lexically or semanti-\ncally aligned with the question, but they lack the\ncapability to discern the authenticity of the infor-\nmation. We further reveal that REVISE is superior\nto GENREAD in producing fake passages that are\nmore likely to be retrieved, and sparse retrievers\nare particularly brittle to deliberate misinformation\npollution, e.g., REIT . The detailed configurations\nand findings are in Appendix B.\n4. Questions without dependable supporting evi-\ndence are more prone to manipulation. Comparing\nthe performance differentials across the two test\nsets, we notice a more pronounced decline in sys-\ntem performance on the CovidNews test set. Our\nhypothesis for this phenomenon lies in the relative\nlack of informational depth within the news domain\nas opposed to encyclopedias. Subsequent experi-\nments corroborate that the WMT News Corpus\nindeed provides fewer and less pertinent resources\nfor answering queries. We delve into this aspect in\ngreater detail in Appendix C.\nMoreover, we discover that the generated texts\nin the GENREAD setting have a significantly more\ndetrimental effect on the CovidNews benchmark\ncompared to the NQ-1500 benchmark. This high-\nlights the uneven capabilities of GPT-3.5 in retain-\ning information across diverse topics. We postulate\nthat this may be partially due to the training proce-\ndure being heavily reliant on Wikipedia data, which\ncould potentially induce a bias towards Wikipedia-\ncentric knowledge in the model’s output.\n5 Defense Strategies\nA fundamental approach to mitigate the negative\nimpacts of misinformation pollution involves the\ndevelopment of a resilient, misinformation-aware\nQA system. Such a system would mirror human\nbehavior in its dependence on trustworthy external\n1394\nTraining In-domain OOD\nAUROC AUROC\nCTRL GEN 99.7 64.8\nREVISE 91.4 50.7\nREIT 99.8 52.6\nTable 3: Detection result on the test data sampled from\nNQ-1500. In-domain setting take the unsampled portion\nof the original NQ-1500, while OOD utilized existing\nGPT-generated Wikipedia-style text for training. Note\nthat an AUROC value of 50 means the classifier is per-\nforming no better than random guessing.\nsources to provide accurate responses. In our pur-\nsuit of this, we have explored three potential strate-\ngies. In the following sections, we will succinctly\noutline the reasoning behind each strategy, present\nour preliminary experimental results, and discuss\ntheir respective merits and drawbacks. Details on\nexperimental configurations are in Appendix A.\nDetection Approach The initial strategy entails\nincorporating a misinformation detector within the\nQA system, equipped to discern model-generated\ncontent from human-authored ones. To test this\napproach, we have employed a RoBERTa-based\nclassifier (Liu et al., 2019), fine-tuned specifically\nfor this binary classification task. For acquiring\nthe training and testing data, we leveraged the NQ-\n1500 DPR retrieval result, randomly partitioning\nthe first 80% for training, and reserving the remain-\ning 20% for testing. For each query, we used the\ntop-10 context passages, amounting to 12,000 train-\ning instances and 3,000 testing instances. Train-\ning the above detector assumes the accessibility\nof the in-domain NQ-1500 data. Acknowledging\nthe practical limitations of in-domain training data,\nwe also incorporated an existing dataset of GPT3\ncompletions based on Wikipedia topics to train an\nout-of-domain misinformation detector.\nVigilant Prompting LLMs have recently ex-\nhibited a remarkable ability to follow human in-\nstructions when provided with suitable prompt-\ning (Ouyang et al., 2022). We aim to investigate\nwhether this capability can be extended to follow\ndirectives aimed at evading misinformation. Our\nexperimental design utilizes GPT-3.5 as the reader,\nemploying QA prompts that include an additional\ncaution regarding misinformation. For example,\nthe directive given to the reader might read: “Draw\nupon the passages below to answer the subsequent\nSetting Baseline Prompting V oting\nEM EM Rel. EM Rel.\nCTRL GEN 30.07 32.53 ↑8% 33.33 ↑11%\nREVISE 27.33 25.47 ↓7% 30.67 ↑12%\nREIT 23.67 23.67 ↑0% 29.00 ↑23%\nTable 4: ODQA performance of Prompting-based and\nV oting-based readers, compared to the baseline (FiD, 50\ncontexts). All systems use DPR for retrieval.\nquestion concisely. Be aware that a minor portion\nof the passages may be designed to mislead you.”\nReader Ensemble Traditionally in ODQA, all\nretrieved context passages are concatenated before\nbeing passed to the reader. This approach may\ncause the model to become distracted by the pres-\nence of misinformation. In response to this, we\npropose a “divide-and-vote” technique. Firstly, we\nsegregate the context passages into k groups based\non their relevance to the question. Each group of\npassages is then used by a reader to generate an\nanswer. Subsequently, we apply majority voting on\nthe resulting k candidate responses a1, a2, ..., ak to\ncalculate the voted answer(s) av, using the formula\nav = argmax\naj\n(∑k\ni=1 I(ai = aj)\n)\n. Through this\nvoting strategy, we aim to minimize the impact of\nmisinformation by limiting the influence of individ-\nual information sources on answer prediction.\nMain Results The performance of detectors\ntrained on both in-domain and out-of-domain\ndata is illustrated in Table 3, revealing signifi-\ncant variances. In-domain trained detectors consis-\ntently deliver high AUROC scores (91.4%-99.7%),\nwhereas out-of-domain trained classifiers show\nonly slight improvements over random guessing\n(50.7%-64.8%). Despite the impressive results ob-\ntained with in-domain detectors, expecting a suffi-\ncient quantity of in-domain training data to always\nbe available is impractical in real-world scenarios.\nThis is due to our lack of knowledge regarding the\nspecific model malicious actors may use to generate\nmisinformation. Additionally, our out-of-domain\ntraining data, despite being deliberately selected to\nmatch the genre, topic, and length of the detection\ntask’s targets, yielded disappointing results. This\nunderscores the challenge of training a versatile,\neffective misinformation detector.\nIncorporating additional information through\nprompting GPT readers yielded inconsistent out-\n1395\ncomes, as indicated in Table 4. This variation may\nbe attributable to the dearth of data and the absence\nof tasks similar to the ones during the GPT-3.5\ntraining phase. The voting strategy yielded bene-\nfits, albeit with attached costs. V oting consistently\nachieved better effectiveness compared with the\nprompting strategy, as demonstrated in Table 4. It\nis essential to note, however, that the deployment\nof multiple readers in the V oting approach neces-\nsitates additional resources. Despite the potential\nfor concurrent processing of multiple API calls, the\ncost per question escalates linearly with the num-\nber of context passages used, rendering the method\nfinancially challenging at a large scale.\nDoes Reading more Contexts help? Intuitively,\na straightforward way to counteract the prolifer-\nation of misinformation in ODQA is to diminish\nits prevalence, or in other words, to decrease the\nratio of misinformation that the QA systems are\nexposed to. A viable method of achieving this is by\nretrieving a larger number of passages to serve as\ncontexts for the reader. This approach has demon-\nstrated potential benefits in several ODQA systems\nthat operate on a clean corpus (Izacard and Grave,\n2021; Lewis et al., 2021). To explore its effec-\ntiveness against misinformation, we evaluate the\nQA performance using different amount of context\npassages given to readers.\nFigure 3 shows the relation between context size\nused in readers and the QA performance. Instead\nof reporting the absolute EM score, we report the\nrelative EM drop compared with the EM score un-\nder the clean corpus setting to measure the impact\nof misinformation pollution. Interestingly, our re-\nsults show that increasing the context size has min-\nimal, if not counterproductive, effects in mitigating\nthe performance decline caused by misinforma-\ntion. This aligns with the previous observation that\nODQA readers rely on a few highly relevant con-\ntexts, regardless of the entire volume of contexts to\nmake the prediction (Chen et al., 2022). A straight-\nforward strategy of “diluting” the misinformation\nby increasing the context size is not an effective\nway to defend against misinformation pollution.\nSummary In our exploration of three strate-\ngies to safeguard ODQA systems against mis-\ninformation pollution, we uncover promising ef-\nfects through the allocation of additional resources.\nThese include using in-domain detection training\nand engaging multiple readers to predict answers\nvia a voting mechanism. Nonetheless, the develop-\nment of a cost-effective and resilient QA system\ncapable of resisting misinformation still demands\nfurther research and exploration.\n6 Discussion\nIn previous sections, we established a comprehen-\nsive threat model that encompasses misinformation\ngeneration, its resultant pollution, and potential\ndefense strategies. While our research primarily\nresides in simulation-based scenarios, it has shed\nlight on numerous potential risks posed by misin-\nformation created by large language models. If left\nunaddressed, these risks could substantially under-\nmine the current information ecosystem and have\ndetrimental impacts on downstream applications.\nIn this section, we offer a discussion on the practi-\ncal implications of misinformation pollution in a\nreal-world web environment. Our focus is on three\ncrucial factors: information availability, the associ-\nated costs, and the integrity of web-scale corpora.\nInformation Availability Our misinformation\ngeneration methods only require minimal addi-\ntional information, such as a relevant real passage\n(REVISE ), or a modest amount of knowledge about\nthe targeted system (REIT ). Given the relative ease\nof producing misinformation with LLMs, we fore-\ncast that misinformation pollution is likely to be-\ncome an imminent threat to the integrity of the web\nenvironment in the near future. It is, therefore, crit-\nical to pursue both technological countermeasures\nand regulatory frameworks to mitigate this threat.\nAssociated Costs The cost of the misinforma-\ntion attack depends on factors like language model\ntraining and maintenance, data storage, and com-\nputing resources. We focus on the dominant cost\nin our experiments: OpenAI’s language models\nAPI fees. We estimate producing one fake doc-\nument (200 words) costs $0.01 to $0.04 using\ntext-davinci-003, significantly lower than hir-\ning human writers . This cost-efficient misinforma-\ntion production scheme likely represents the disin-\nformation industry’s future direction.\nIntegrity of Web-scale Corpora The quality of\nweb-scale corpora is important for downstream ap-\nplications. However, web-scale corpora are known\nto contain inaccuracies, inconsistencies, and bi-\nases (Kumar et al., 2016; Greenstein and Zhu,\n2012). Large-scale corpora are especially vulnera-\nble to misinformation attacks. Decentralized cor-\n1396\nFigure 3: The relative EM change (percentage) under different misinformation poisoning settings with respect to a\nnumber of context passages. The result is averaged over four configurations, two retrievers mixed-and-matched with\ntwo test datasets. We limit the context size to 10 when using GPT due to its limit on input length.\npora with data in URLs risk attackers hijacking\nexpired domains and tampering with contents (Car-\nlini et al., 2023). Centralized corpora, such as the\nCommon Crawl suffer from unwanted data as well\n(Luccioni and Viviano, 2021); even for manually\nmaintained ones like Wikipedia, it is still possible\nfor misinformation to slip in (Carlini et al., 2023).\n7 Conclusion and Future Work\nWe present an evaluation of the practicality of utiliz-\ning Language Model Models (LLMs) for the auto-\nmated production of misinformation and we exam-\nine their potential impact on knowledge-intensive\napplications. By simulating scenarios where actors\ndeliberately introduce false information into knowl-\nedge sources for question-answering systems, we\ndiscover that machines are highly susceptible to\nsynthetic misinformation, leading to a significant\ndecline in their performance. We further observe\nthat machines’ performance deteriorates even fur-\nther when exposed to intricately crafted falsehoods.\nIn response to these risks, we propose three partial\nsolutions as an initial step toward mitigating the\nimpact of LLM misuse and we encourage further\nresearch into this problem.\nOur future research directions for extending this\nwork could take three paths. Firstly, while we have\nthus far only illustrated the potential dangers of\nmisinformation generated by LLMs in ODQA sys-\ntems, this threat model could be employed to as-\nsess risk across a broader spectrum of applications.\nSecondly, the potential of LLMs to create more cal-\nculated forms of misinformation, such as hoaxes,\nrumors, or propagandistic falsehoods, warrants a\nseparate line of inquiry. Lastly, there is an ongo-\ning need for further research into the development\nof cost-effective and robust QA systems that can\neffectively resist misinformation.\nLimitations\nDespite the remarkable capabilities of GPT-3.5\n(text-davinci-003) in generating high-quality tex-\ntual content, one must not disregard its inherent\nlimitations. Firstly, the reproducibility of its out-\nputs presents a significant challenge. In order to\nmitigate this issue, we shall make available all\nprompts and generated documents, thereby facili-\ntating the replication of our experiments. Secondly,\nthe cost associated with GPT-3.5 is an order of\nmagnitude greater than that of some of its con-\ntemporaries, such as ChatGPT (gpt-turbo-3.5),\nwhich inevitably constrained the scope of our in-\nvestigations. The focus of this research lies pre-\ndominantly on a selection of the most emblematic\nand pervasive QA systems and LLMs. Nonethe-\nless, the findings derived from our analysis may\nnot necessarily be applicable to other systems or\ntext generators. For instance, QA systems employ-\ning alternative architectures, as demonstrated by\nrecent works (Shao and Huang, 2022; Su et al.,\n2022), may exhibit increased robustness against\nthe proliferation of misinformation.\nEthics Statement\nWe decide to publicly release our model-generated\ndocuments and the prompts used for creating them,\ndespite the potential for misuse and generating\nharmful disinformation. We believe open sourcing\nis important and we justify our decision as follows.\n1397\nFirstly, since our model relies on the readily\navailable OpenAI API, replicating our production\nprocess is feasible without access to the code. Our\nobjective is to raise awareness and encourage ac-\ntion by investigating the consequences of misusing\nlarge language models. We aim to inform the pub-\nlic, policymakers, and developers about the need\nfor responsible and ethical implementation.\nSecondly, our choice to release follows a similar\napproach taken with Grover (Zellers et al., 2019)7,\na powerful detector and advanced generator of AI-\ngenerated fake news. The authors of Grover argue\nthat threat modeling, including a robust generator\nor simulation of the threat, is crucial to protect\nagainst potential dangers. In our research, we es-\ntablish an effective threat model for ODQA in the\ncontext of misinformation. Future studies can build\nupon the transparency of our model, further enhanc-\ning our proposed defense techniques for AI safety.\nAcknowledgements\nThis work was supported by the National Science\nFoundation Award #2048122. The views expressed\nare those of the authors and do not reflect the offi-\ncial policy or position of the US government. This\nresearch is also supported by the Ministry of Ed-\nucation, Singapore, under its MOE AcRF TIER 3\nGrant (MOE-MOET32022-0001).\nReferences\nAaditya Bhat. 2023. Gpt-wiki-intro (revision 0e458f5).\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language Models are Few-Shot Learners.\nArXiv:2005.14165 [cs].\nBen Buchanan, Andrew Lohn, and Micah Musser. 2021.\nTruth, lies, and automation: How language models\ncould change disinformation . Center for Security\nand Emerging Technology.\nNicholas Carlini, Matthew Jagielski, Christopher A.\nChoquette-Choo, Daniel Paleka, Will Pearce, Hyrum\n7https://thegradient.pub/\nwhy-we-released-grover/\nAnderson, Andreas Terzis, Kurt Thomas, and Flo-\nrian Tramèr. 2023. Poisoning Web-Scale Training\nDatasets is Practical. ArXiv:2302.10149 [cs].\nSouradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu,\nBang An, Dinesh Manocha, and Furong Huang. 2023.\nOn the Possibilities of AI-Generated Text Detection.\nArXiv:2304.04736 [cs].\nHung-Ting Chen, Michael J. Q. Zhang, and Eunsol\nChoi. 2022. Rich Knowledge Sources Bring Com-\nplex Knowledge Conflicts: Recalibrating Models to\nReflect Conflicting Evidence. ArXiv:2210.13701\n[cs].\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021a. All that’s ’human’ is not gold: Evaluating\nhuman evaluation of generated text. In Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 7282–7296.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021b. All That’s ’Human’ Is Not Gold:\nEvaluating Human Evaluation of Generated Text.\nArXiv:2107.00061 [cs].\nThibault Formal, Carlos Lassance, Benjamin Pi-\nwowarski, and Stéphane Clinchant. 2021. SPLADE\nv2: Sparse Lexical and Expansion Model for Infor-\nmation Retrieval. ArXiv:2109.10086 [cs].\nYi R. Fung, Christopher Thomas, Revanth Gangi Reddy,\nSandeep Polisetty, Heng Ji, Shih-Fu Chang, Kath-\nleen R. McKeown, Mohit Bansal, and Avi Sil. 2021.\nInfosurgeon: Cross-media fine-grained information\nconsistency checking for fake news detection. In An-\nnual Meeting of the Association for Computational\nLinguistics (ACL), pages 1683–1698.\nJosh A. Goldstein, Girish Sastry, Micah Musser, Re-\nnee DiResta, Matthew Gentzel, and Katerina Sedova.\n2023. Generative language models and automated\ninfluence operations: Emerging threats and potential\nmitigations. CoRR, abs/2301.04246.\nShane Greenstein and Feng Zhu. 2012. Is Wikipedia\nBiased? American Economic Review, 102(3):343–\n348.\nMaurício Gruppi, Benjamin D. Horne, and Sibel Adalı.\n2022. NELA-GT-2021: A Large Multi-Labelled\nNews Dataset for The Study of Misinformation in\nNews Articles. ArXiv:2203.05659 [cs].\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020. Retrieval Augmented\nLanguage Model Pre-Training. In Proceedings of the\n37th International Conference on Machine Learning,\npages 3929–3938. PMLR. ISSN: 2640-3498.\nSebastian Hofstätter, Jiecao Chen, Karthik Raman,\nand Hamed Zamani. 2022. FiD-Light: Efficient\nand Effective Retrieval-Augmented Text Generation.\nArXiv:2209.14290 [cs].\n1398\nGiwon Hong, Jeonghwan Kim, Junmo Kang, Sung-\nHyon Myaeng, and Joyce Jiyoung Whang. 2023.\nDiscern and Answer: Mitigating the Impact of Mis-\ninformation in Retrieval-Augmented Models with\nDiscriminators. ArXiv:2305.01579 [cs].\nKung-Hsiang Huang, Kathleen R. McKeown, Preslav\nNakov, Yejin Choi, and Heng Ji. 2022. Fak-\ning fake news for real fake news detection:\nPropaganda-loaded training data generation. CoRR,\nabs/2203.05386.\nGautier Izacard and Edouard Grave. 2021. Leveraging\nPassage Retrieval with Generative Models for Open\nDomain Question Answering. ArXiv:2007.01282\n[cs].\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Yejin Bang, Wenliang\nDai, Andrea Madotto, and Pascale Fung. 2022. Sur-\nvey of Hallucination in Natural Language Gener-\nation. ACM Computing Surveys , page 3571730.\nArXiv:2202.03629 [cs].\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense Passage Retrieval for\nOpen-Domain Question Answering. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nSarah Kreps, R Miles McCain, and Miles Brundage.\n2022. All the news that’s fit to fabricate: Ai-\ngenerated text as a tool of media misinformation.\nJournal of experimental political science, 9(1):104–\n117.\nSrijan Kumar, Robert West, and Jure Leskovec. 2016.\nDisinformation on the Web: Impact, Characteristics,\nand Detection of Wikipedia Hoaxes. In Proceedings\nof the 25th International Conference on World Wide\nWeb, pages 591–602, Montréal Québec Canada. In-\nternational World Wide Web Conferences Steering\nCommittee.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natural\nQuestions: A Benchmark for Question Answering\nResearch. Transactions of the Association for Com-\nputational Linguistics, 7:453–466.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2021.\nRetrieval-Augmented Generation for Knowledge-\nIntensive NLP Tasks. ArXiv:2005.11401 [cs].\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and\nYangqiu Song. 2023a. Multi-step Jailbreaking Pri-\nvacy Attacks on ChatGPT. ArXiv:2304.05197 [cs].\nXinyi Li, Yongfeng Zhang, and Edward C. Malthouse.\n2023b. A Preliminary Study of ChatGPT on News\nRecommendation: Personalization, Provider Fair-\nness, Fake News. ArXiv:2306.10702 [cs].\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021. Pyserini: A Python Toolkit for Reproducible\nInformation Retrieval Research with Sparse and\nDense Representations. In Proceedings of the 44th\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval , pages\n2356–2362, Virtual Event Canada. ACM.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. ArXiv:1907.11692 [cs].\nAdam Liška, Tomáš Koˇciský, Elena Gribovskaya, Tay-\nfun Terzi, Eren Sezener, Devang Agrawal, Cyprien\nde Masson d’Autume, Tim Scholtes, Manzil Zaheer,\nSusannah Young, Ellen Gilsenan-McMahon, Sophia\nAustin, Phil Blunsom, and Angeliki Lazaridou. 2022.\nStreamingQA: A Benchmark for Adaptation to New\nKnowledge over Time in Question Answering Mod-\nels. ArXiv:2205.11388 [cs].\nShayne Longpre, Kartik Perisetla, Anthony Chen,\nNikhil Ramesh, Chris DuBois, and Sameer Singh.\n2022. Entity-Based Knowledge Conflicts in Ques-\ntion Answering. ArXiv:2109.05052 [cs].\nAlexandra Luccioni and Joseph Viviano. 2021. What’s\nin the Box? An Analysis of Undesirable Content in\nthe Common Crawl Corpus. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers), pages 182–189, Online. Asso-\nciation for Computational Linguistics.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky,\nChristopher D. Manning, and Chelsea Finn.\n2023. DetectGPT: Zero-Shot Machine-Generated\nText Detection using Probability Curvature.\nArXiv:2301.11305 [cs].\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. CoRR, abs/2203.02155.\n1399\nLiangming Pan, Wenhu Chen, Min-Yen Kan, and\nWilliam Yang Wang. 2021. ContraQA: Ques-\ntion Answering under Contradicting Contexts.\nArXiv:2110.07803 [cs].\nEmmanouil Papadogiannakis, Panagiotis Papadopoulos,\nEvangelos P. Markatos, and Nicolas Kourtellis. 2023.\nWho Funds Misinformation? A Systematic Analysis\nof the Ad-related Profit Routines of Fake News sites.\nArXiv:2202.05079 [cs].\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Unified Text-to-Text\nTransformer. ArXiv:1910.10683 [cs, stat].\nStephen Robertson and Hugo Zaragoza. 2009. The\nProbabilistic Relevance Framework: BM25 and Be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nVinu Sankar Sadasivan, Aounon Kumar, Sriram Bal-\nasubramanian, Wenxiao Wang, and Soheil Feizi.\n2023. Can AI-Generated Text be Reliably Detected?\nArXiv:2303.11156 [cs].\nZhihong Shao and Minlie Huang. 2022. Answering\nOpen-Domain Multi-Answer Questions via a Recall-\nthen-Verify Framework. ArXiv:2110.08544 [cs].\nHarald Stiff and Fredrik Johansson. 2022. Detecting\ncomputer-generated disinformation. International\nJournal of Data Science and Analytics , 13(4):363–\n383.\nDan Su, Mostofa Patwary, Shrimai Prabhumoye, Peng\nXu, Ryan Prenger, Mohammad Shoeybi, Pascale\nFung, Anima Anandkumar, and Bryan Catanzaro.\n2022. Context Generation Improves Open Domain\nQuestion Answering. ArXiv:2210.06349 [cs].\nOrion Weller, Aleem Khan, Nathaniel Weir, Dawn\nLawrie, and Benjamin Van Durme. 2022. Defending\nAgainst Poisoning Attacks in Open-Domain Question\nAnswering. ArXiv:2212.10002 [cs].\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020. Hug-\ngingFace’s Transformers: State-of-the-art Natural\nLanguage Processing. ArXiv:1910.03771 [cs].\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2022. Generate\nrather than Retrieve: Large Language Models are\nStrong Context Generators. ArXiv:2209.10063 [cs].\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Annual Conference on Neural Information\nProcessing Systems (NeurIPS), pages 9051–9062.\n1400\nA Configuration Details\nHere we elaborate on the specific configurations\nused in our experiments for clarity.\nRationale behind the prompt template design.\nTable 5 summarizes the aforementioned four misin-\nformation generation settings. We categorize their\nkey features based on three dimensions: 1) ma-\nliciousness: if the setting intends to deliberately\nelicit misinformation, 2) resourcefulness: if the set-\nting demands resources unavailable to regular users,\nsuch as a real factual article, and 3) customization:\nif the generated documents are designed to influ-\nence the target audience (in our case, machines).\nWe believe these settings cover the common poten-\ntial scenarios of misusing LLMs for misinforma-\ntion generation.\nGenerator. We employed the default hyper-\nparameter for GPT-3.5 (text-davinci-003) in all\ntext-generation experiments, specified in the Ope-\nnAI API documentation8.\nPrompt template for plausible answer produc-\ntion. Since correct answers were not needed, we\nrelied solely on the parametric memory of language\nmodels. The prompt we used is:\nGenerate a false answer to the given\nquestion. It should be of short (less than\nfive words in general) and look plausible,\ncompared to the reference answer.\nQuestion: Question\nReference Answers: Answer\nTo ensure our plausible answers produced were\nindeed incorrect, we repeated the answer genera-\ntion process five times and randomly sampled one\nanswer that does not match any reference answer.\nODQA Model details.\n• BM25: We employed the implementation\nfrom (Lin et al., 2021).\n• DPR: We used the checkpoint provided by the\noriginal DPR repository9, trained on five QA\ndatasets including NQ.\n• FiD: We used the FiD-large checkpoint pro-\nvided by the original FiD repository10 trained\non NQ.\n8https://platform.openai.com/docs/\napi-reference/completions/create\n9https://github.com/facebookresearch/DPR/tree/\nmain\n10https://github.com/facebookresearch/FiD\nDetails of the defense strategies. For the\ndetecting-based method, We employed the\nRobertaForSequenceClassification check-\npoint provided by huggingface (Wolf et al., 2020).\nFor both in-domain and out-of-domain classifiers,\nwe used 12,000 context passages for training. We\nsampled data from a dataset containing both actual\nWikipedia snippets and Wikipedia completions\ngenerated by GPT-3(Bhat, 2023), which share\nmany similar properties with text generated in our\nexperiments. The model is configured for 3 epochs\nof training with a learning rate of 0.001.\nFor the prompting-based method, we designed\nfive different misinformation-aware prompts, and\nreport the average EM score across these prompts.\nWe drew inspiration from concurrent works (Hong\net al., 2023; Li et al., 2023b) and engineering ex-\nperience11, then utilized ChatGPT to produce five\nprompts in accordance to one human written ex-\nample. We report each prompt and its respective\nperformance in table 6. For the voting method, we\nexplored various configurations of the number of\nreaders k and the number of context passages used\nfor each reader n. We report the best-performing\nconfiguration where k = 5 readers and n = 10\npassages for each reader based on preliminary ex-\nperiments.\nB Retrieval Performance\nRetrieval pollution. Table 7 presents the percent-\nages of questions in our two benchmarks that con-\ntain synthetic disinformation, known as “poisoned”\nquestions. A question is considered poisoned if it\nincludes at least one synthetic passage retrieved in\nthe top-K passages.\nMachine-generated misinformation could easily\ninfiltrate information retrieval systems. Comparing\nbetween generation settings, REVISE outperforms\nGENREAD in producing information more likely\nto be retrieved, thanks to its ‘gold template’ that\nits misinformation is based on. However, we ob-\nserved that REIT degraded QA performance the\nmost, which highlights the security risks of a delib-\nerate attack on automated systems. Sparse retriev-\ners are particularly brittle to targeted misinforma-\ntion. When targeting the sparse retriever BM25,\nREIT can poison more than 90% of questions us-\ning only 10 context passages, and over 95% of\nquestions using 100 context passages, indicating\n11https://www.promptingguide.ai/risks/\nadversarial#add-defense-in-the-instruction\n1401\nSetting Malicious Resourceful Customized Description\nGENREAD Prompts the LLMs to provide a relevant\ncontext passage to the question.\nCTRL GEN ✓ Prompts the LLMs to create a synthetic\npassage supporting a false opinion.\nREVISE ✓ ✓ Assumes the attacker imitates a natural\ndocument relevant to the given question.\nREIT ✓ ✓ Assumes the attacker targets machines rather\nthan humans.\nTable 5: Summary of the misinformation generation settings and their respective features.\nPrompt EM\n\"Answer the question below using just a few words after reading the passages provided.\nBeware that a small portion of the passages is designed to deceive you.\" 28.11\n\"After reading the passages given, use only a few words to answer the following question.\nBe cautious since some parts of the passages may mislead you.\" 26.67\n\"In the provided passages, some text is intended to mislead you, but read them nonetheless\nand answer the question with a few words. What is the answer to the question?\" 26.33\n\"Your task is to answer a question using only a few words after reading the given passages.\nBeware that some parts of the passages are meant to deceive you.\" 28.22\n\"Using only a few words, answer the question that follows after reading the passages provided.\nHowever, keep in mind that some of the passages are crafted to mislead you.\" 26.78\nTable 6: Different choices of prompts with their respective QA performance in Prompting Defense experiments.\nResults reported on 300 random NQ-1500 samples, averaged over the three poisoning settings in table 4.\nthe fragility of sparse retrieval under deliberate at-\ntacks.\nC Analysis on Corpus Quality\nWe conduct a brief analysis of the two corpora\nunder our study regarding their informativeness to\nODQA tasks, as shown in Table 8. Since the utiliza-\ntion of context passages containing gold answers\n(commonly referred to as gold evidence) is critical,\nwe intend to find out about the qualities of gold\nevidence in both corpora. Specifically, we measure\nquestion coverage (Recall@100), volume (average\nmentions of answers) and relevance (average rank\nof the first gold evidence) of gold evidence in the\ntwo corpora. Using the DPR-retrieval result of\n100 context passages, we observe that CovidNews\ncorpus (News) provides significantly less informa-\ntive evidence for answering questions, making it\na challenging QA task. Furthermore, these top-\nics are more susceptible to plausible assertions at\nthe manipulation of propagandists for a deficit of\ncounterpart factual evidence.\nD Human Detection of Generated\nMisinformation\nWe conducted a small-scale human study to ex-\nplore the detectability of misinformation. We ran-\ndomly sampled 50 fake documents generated un-\nder CTRL GEN and 50 corresponding most relevant\nWikipedia passages. We employed experimental\nsettings described in (Clark et al., 2021b), where\nparticipants need to rate each text on a 4-point\nscale. We hired three college students to each an-\nnotate the 100 documents. Similar to their find-\nings, we found humans cannot reliably differenti-\nate machine-generated misinformation from their\nWikipedia counterparts, with an average overall\naccuracy of 57%. Note that we observed great\nperformance improvements in the second half of\nexperiments for all participants, which could mean\nthat 57% is an overestimation of human capabilities\nsince the participants displayed signs of learning\n1402\nmodel NQ-1500 CovidNews\nPQ@10 PQ@100 PQ@10 PQ@100\nDPR\nGENREAD 63.07 92.67 25.49 49.15\nCTRL GEN 49.67 82.33 18.64 39.50\nREVISE 67.07 91.93 40.42 71.45\nREIT 48.93 77.33 54.69 76.47\nBM25\nGENREAD 57.87 83.00 49.27 65.33\nCTRL GEN 29.40 54.63 75.63 86.60\nREVISE 72.53 93.53 60.67 82.80\nREIT 96.80 99.00 94.40 97.80\nTable 7: Evaluation on NQ-1500 passage (test set sam-\nples) and CovidNews retrieval. PQ@k refers to Poi-\nsoned Questions, which is the percentage of questions\nthat have at least one generated passage in the top-k\nretrieval result.\nName Recall@100 #Avg. answer #Avg. rank of\nmentions first gold evidence\nNQ-1500 82.70 17.07 8.08\nCovidNews 57.80 12.04 17.76\nTable 8: Properties of the corpora used in this study\nregarding the question answering tasks.\nthe sampled data in our experiments.\n1403",
  "topic": "Misinformation",
  "concepts": [
    {
      "name": "Misinformation",
      "score": 0.9765026569366455
    },
    {
      "name": "Obstacle",
      "score": 0.6434682011604309
    },
    {
      "name": "Harm",
      "score": 0.6220055818557739
    },
    {
      "name": "Computer science",
      "score": 0.5725470185279846
    },
    {
      "name": "Internet privacy",
      "score": 0.46637704968452454
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.455888569355011
    },
    {
      "name": "Computer security",
      "score": 0.45516809821128845
    },
    {
      "name": "Data science",
      "score": 0.32110795378685
    },
    {
      "name": "Psychology",
      "score": 0.2486238181591034
    },
    {
      "name": "Political science",
      "score": 0.20908573269844055
    },
    {
      "name": "Business",
      "score": 0.1727917492389679
    },
    {
      "name": "Social psychology",
      "score": 0.15177273750305176
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I154570441",
      "name": "University of California, Santa Barbara",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    }
  ]
}