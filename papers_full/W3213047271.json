{
  "title": "A Language Model-based Generative Classifier for Sentence-level Discourse Parsing",
  "url": "https://openalex.org/W3213047271",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5100386277",
      "name": "Ying Zhang",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5016936747",
      "name": "Hidetaka Kamigaito",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5035876897",
      "name": "Manabu Okumura",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2162763612",
    "https://openalex.org/W2970949114",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W1629368090",
    "https://openalex.org/W2130867674",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W332673702",
    "https://openalex.org/W2044599851",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3105163367",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2045738181",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2186590411",
    "https://openalex.org/W4250641076",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2027869740",
    "https://openalex.org/W2251211118",
    "https://openalex.org/W3099606977",
    "https://openalex.org/W2949072528",
    "https://openalex.org/W2564486991",
    "https://openalex.org/W2963912736",
    "https://openalex.org/W1894075015",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3101190870",
    "https://openalex.org/W2754603668",
    "https://openalex.org/W2998696494",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W108071511",
    "https://openalex.org/W2889446948",
    "https://openalex.org/W2741164290",
    "https://openalex.org/W2252267789",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2331726854"
  ],
  "abstract": "Discourse segmentation and sentence-level discourse parsing play important roles for various NLP tasks to consider textual coherence. Despite recent achievements in both tasks, there is still room for improvement due to the scarcity of labeled data. To solve the problem, we propose a language model-based generative classifier (LMGC) for using more information from labels by treating the labels as an input while enhancing label representations by embedding descriptions for each label. Moreover, since this enables LMGC to make ready the representations for labels, unseen in the pre-training step, we can effectively use a pre-trained language model in LMGC. Experimental results on the RST-DT dataset show that our LMGC achieved the state-of-the-art F1 score of 96.72 in discourse segmentation. It further achieved the state-of-the-art relation F1 scores of 84.69 with gold EDU boundaries and 81.18 with automatically segmented boundaries, respectively, in sentence-level discourse parsing.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2432–2446\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n2432\nA Language Model-based Generative Classiﬁer\nfor Sentence-level Discourse Parsing\nYing Zhang, Hidetaka Kamigaito and Manabu Okumura\nTokyo Institute of Technology\n{zhang,kamigaito,oku}@lr.pi.titech.ac.jp\nAbstract\nDiscourse segmentation and sentence-level\ndiscourse parsing play important roles for vari-\nous NLP tasks to consider textual coherence.\nDespite recent achievements in both tasks,\nthere is still room for improvement due to the\nscarcity of labeled data. To solve the problem,\nwe propose a language model-based genera-\ntive classiﬁer (LMGC) for using more infor-\nmation from labels by treating the labels as an\ninput while enhancing label representations by\nembedding descriptions for each label. More-\nover, since this enables LMGC to make ready\nthe representations for labels, unseen in the\npre-training step, we can effectively use a pre-\ntrained language model in LMGC. Experimen-\ntal results on the RST-DT dataset show that our\nLMGC achieved the state-of-the-art F 1 score\nof 96.72 in discourse segmentation. It further\nachieved the state-of-the-art relation F1 scores\nof 84.69 with gold EDU boundaries and 81.18\nwith automatically segmented boundaries, re-\nspectively, in sentence-level discourse parsing.\n1 Introduction\nTextual coherence is essential for writing a natural\nlanguage text that is comprehensible to readers. To\nrecognize the coherent structure of a natural lan-\nguage text, Rhetorical Structure Theory (RST) is\napplied to describe an internal discourse structure\nfor the text as a constituent tree (Mann and Thomp-\nson, 1988). A discourse tree in RST consists of\nelementary discourse units (EDUs), spans that de-\nscribe recursive connections between EDUs, and\nnuclearity and relation labels that describe relation-\nships for each connection.\nFigure 1 (a) shows an example RST discourse\ntree. A span including one or more EDUs is a node\nof the tree. Given two adjacent non-overlapping\nspans, their nuclearity can be either nucleus or\nsatellite, denoted by N and S, where the nucleus\nrepresents a more salient or essential piece of infor-\nmation than the satellite. Furthermore, a relation\nWeʼve got a lot to do , he acknowledged .\nElaboration\nAttribution\nN S\nN\nS\n(a) Discourse Tree \n(Attribution (Elaboration (N (N We’ve got a lot )N\n(S to do , )S )N )Elaboration (S he acknowledged . )S )Attribution\n(b) Linearized Discourse Tree\nEDUs\nFigure 1: An example discourse tree structure.\nlabel, such as Attribution and Elaboration, is used\nto describe the relation between the given spans\n(Mann and Thompson, 1988; Carlson and Marcu,\n2001). To build such trees, RST parsing consists\nof discourse segmentation, a task to detect EDU\nboundaries in a given text, and discourse parsing, a\ntask to link spans for detected EDUs.\nIn this paper, we focus on discourse segmenta-\ntion and sentence-level discourse parsing, which\nare indispensable in RST parsing (Joty et al., 2013;\nFeng and Hirst, 2014a; Joty et al., 2015; Wang\net al., 2017; Kobayashi et al., 2020) and are appli-\ncable to many downstream tasks, such as machine\ntranslation (Guzmán et al., 2014; Joty et al., 2017)\nand sentence compression (Sporleder and Lapata,\n2005).\nIn discourse segmentation, Carlson et al. (2001)\nproposed a method for using lexical information\nand syntactic parsing results. Many researchers\n(Fisher and Roark, 2007; Xuan Bach et al., 2012;\nFeng and Hirst, 2014b) utilized these clues as fea-\ntures in a classiﬁer although automatic parsing er-\nrors degraded segmentation performance. To avoid\nthis problem, Wang et al. (2018b) used BiLSTM-\nCRF (Huang et al., 2015) to handle an input with-\nout these clues in an end-to-end manner. Lin\net al. (2019) jointly performed discourse segmenta-\ntion and sentence-level discourse parsing in their\npointer-network-based model. They also intro-\n2433\nduced multi-task learning for both tasks and re-\nported the state-of-the-art results for discourse seg-\nmentation and sentence-level discourse parsing in\nterms of F 1 scores. Despite these achievements,\nthere is still room for improvement for both tasks\ndue to the scarcity of labeled data. It is important to\nextract more potential information from the current\ndataset for further performance improvement.\nUnder this motivation, in this research, we pro-\npose a language model-based generative classiﬁer\n(LMGC) as a reranker for both discourse segmen-\ntation and sentence-level discouse parsing. LMGC\ncan jointly predict text and label probabilities by\ntreating a text and labels as a single sequence, like\nFigure 1 (b). Therefore, different from conven-\ntional methods, LMGC can use more information\nfrom labels by treating the labels as an input. Fur-\nthermore, LMGC can enhance label representations\nby embedding descriptions of each label deﬁned in\nthe annotation manual (Carlson and Marcu, 2001),\nthat allows us to use a pre-trained language model\nsuch as MPNet (Song et al., 2020) effectively, since\nwe can already have the representations for labels,\nthat were unseen in the pre-training step.\nExperimental results on the RST-DT dataset\n(Carlson et al., 2002) show that LMGC can achieve\nthe state-of-the-art scores in both discourse segmen-\ntation and sentence-level discourse parsing. LMGC\nutilizing our enhanced label embeddings achieves\nthe best F 1 score of 96.72 in discourse segmen-\ntation. Furthermore, in sentence-level discourse\nparsing, LMGC utilizing our enhanced relation la-\nbel embeddings achieves the best relation F1 scores\nof 84.69 with gold EDU boundaries and 81.18 with\nautomatically segmented boundaries, respectively.\n2 Related Work\nDiscourse segmentation is a fundamental task for\nbuilding an RST discourse tree from a text. Carl-\nson et al. (2001) proposed a method for using lex-\nical information and syntactic parsing results for\ndetecting EDU boundaries in a sentence. Fisher\nand Roark (2007); Xuan Bach et al. (2012); Feng\nand Hirst (2014b) utilized these clues as features\nin a classiﬁer, while Wang et al. (2018b) utilized\nBiLSTM-CRF (Huang et al., 2015) in an end-\nto-end manner to avoid performance degradation\ncaused by syntactic parsing errors.\nSentence-level discourse parsing is also an im-\nportant task for parsing an RST discourse tree,\nas used in many RST parsers (Joty et al., 2013;\nFeng and Hirst, 2014a; Joty et al., 2015; Wang\net al., 2017; Kobayashi et al., 2020). Recently,\nLin et al. (2019) tried to jointly perform discourse\nsegmentation and sentence-level discourse parsing\nwith pointer-networks and achieved the state-of-\nthe-art F1 scores in both discourse segmentation\nand sentence-level discourse parsing.\nIn spite of the performance improvement of these\nmodels, a restricted number of labeled RST dis-\ncourse trees is still a problem. In the discourse\nsegmentation and parsing tasks, most prior work\nis on the basis of discriminative models, which\nlearn mapping from input texts to predicted la-\nbels. Thus, there still remains room for improving\nmodel performance by considering mapping from\npredictable labels to input texts to exploit more la-\nbel information. To consider such information in a\nmodel, Mabona et al. (2019) introduced a genera-\ntive model-based parser, RNNG (Dyer et al., 2016),\nto document-level RST discourse parsing. Differ-\nent from our LMGC, this model unidirectionally\npredicts action sequences.\nIn this research, we model LMGC for the dis-\ncourse segmentation and sentence-level discourse\nparsing tasks. LMGC utilizes a BERT-style bidirec-\ntional Transformer encoder (Devlin et al., 2019) to\navoid prediction bias caused by using different de-\ncoding directions. Since LMGC is on the basis of\ngenerative models, it can jointly consider an input\ntext and its predictable labels, and map the em-\nbeddings of both input tokens and labels onto the\nsame space. Due to this characteristic, LMGC can\neffectively use the label information through con-\nstructing label embeddings from the description of\na label deﬁnition (Carlson and Marcu, 2001). Fur-\nthermore, recent strong pre-trained models such\nas MPNet (Song et al., 2020) are available for any\ninput tokens in LMGC.\n3 Base Models\nOur LMGC reranks the results from a conventional\ndiscourse segmenter and parser, which can be con-\nstructed as discriminative models. In this section,\nwe explain these base models and introduce our\nmathematical notations.\n3.1 Discourse Segmenter\nIn discourse segmentation, given an input text\nx = {x1,··· ,xn}, where xi is a word, a seg-\nmenter detects EDUs e = {e1,··· ,em}from x.\nSince there is no overlap or gap between EDUs,\n2434\n2\nRST-DT\nInput \nSentence Base Segmenter k-best EDU \nsegmentations\nLanguage \nModel for P(x,e)\nbest EDU \nsegmentation Base Parser k-best parse \ntrees\nLanguage Model \nfor P(x,e,p)\nbest \ntree\nDefinition of labels\nEnhance label embeddings \nTrain\nFigure 2: Overview of our Language Model-based Generative Classiﬁer (LMGC).\ndiscourse segmentation can be considered as a kind\nof sequential labeling task, which assigns labels\nl = {l1,··· ,ln}, where each li ∈{0,1}indicates\nwhether the word is the start of an EDU or not. By\nusing a discriminative model, such as BiLSTM-\nCRF (Wang et al., 2018b) and pointer-networks\n(Lin et al., 2019), the probability of predicting\nEDUs from x can be P(l|x) or P(e|x). Because\nof its simple structure and extensibility, we choose\nBiLSTM-CRF as our base model for discourse seg-\nmentation. In BiLSTM-CRF, P(l|x) is formulated\nas follows:\nP(l|x) =\n∏n\nt=1 ψt(lt,lt−1,h)∑\nl′∈Y\n∏n\nt=1 ψt(lt′,lt−1′,h), (1)\nwhere ψt(lt,lt−1,h) = exp(WT ht + b) is the po-\ntential function, ht is the hidden state at time step\nt, W is a weight matrix, bis a bias term, and Y is\nthe set of possible label sequences.\nWe inherit top-kViterbi results of BiLSTM-CRF,\nscored by Eq.(1), to our LMGC, as described in\nSection 4.\n3.2 Discourse Parser\nIn discourse parsing, given an input text x and\nits EDUs e, we can build a binary tree p =\n{p1,··· ,p2n−1}, where each node pi ∈ p has\nthree kinds of labels: span si, nuclearity ui, and\nrelation ri. The sequences of span s and nuclearity\nu can be predicted simultaneously, as in 2-stage\nParser (Wang et al., 2017), or span s can be pre-\ndicted in advance for labeling nuclearity u and\nrelation r, as in pointer-networks (Lin et al., 2019)\nand span-based Parser (Kobayashi et al., 2020).\nBecause of its better performance, we choose 2-\nstage Parser as our base model for sentence-level\ndiscourse parsing. 2-stage Parser extracts several\nfeatures and does classiﬁcation with SVMs in two\nstages. In the ﬁrst stage, it identiﬁes the span and\nnuclearity simultaneously to construct a tree based\non the transition-based system with four types of ac-\ntions: Shift, Reduce-NN, Reduce-NS, and Reduce-\nSN. In the second stage, for a given node pi, ri is\npredicted as the relation between the left and right\nchildren nodes of pi by using features extracted\nfrom pi and its children nodes. In spite of its lim-\nited features, it achieves the best results compared\nwith pointer-networks and span-based Parser. Since\n2-stage Parser utilizes SVMs, we normalize the ac-\ntion scores and inherit top- k beam search results\nof 2-stage Parser for LMGC to perform discourse\nparsing.\n4 Language Model-based Generative\nClassiﬁer (LMGC)\nIn this section, we introduce our generative classi-\nﬁer, LMGC, that utilizes a masked and permuted\nlanguage model to compute sequence probabili-\nties in both discourse segmentation and sentence-\nlevel discourse parsing tasks. More speciﬁcally,\nas we mention in Section 5, we can utilize our\nLMGC in three tasks, (a) discourse segmentation,\n(b) sentence-level discourse parsing with gold seg-\nmentation, and (c) sentence-level discourse pars-\ning with automatic segmentation. Figure 2 shows\nthe overview of our LMGC for the whole task (c).\nAs shown in the ﬁgure, the prediction process in\nLMGC is the following. We assume that, in task\n(c), discourse segmentation and sentence-level dis-\ncourse parsing are performed in a pipeline manner\nwith models trained for tasks (a) and (b).\n1. Predict top- ks EDU segmentations\n{e1,··· ,eks} from a given sentence x\nwith the base discourse segmenter, described\nin Section 3.1.\n2. Compute joint probability P(x,ei) and select\nthe best segmentation e from {e1,··· ,eks}\nwith a language model, as we describe below.\n3. Parse and rank top- kp trees {p1,··· ,pkp}\nfrom x and best segmentation e with the base\ndiscourse parser, described in Section 3.2.\n4. Compute joint probability P(x,e,pj) to se-\nlect the best tree from {p1,··· ,pkp}with a\nlanguage model, as we describe below.\n2435\nIn task (a), we apply Step 2 to predict the best\nsegmentation after Step 1. In task (b), we skip\nSteps 1 and 2, and apply just Steps 3 and 4 for gold\nsegmentation to yield the best parse tree.\n4.1 Tree Representations\nTo calculate joint probabilities for a discourse tree\nwith a language model, we need to represent a tree\nas a linear form, like Figure 1 (b). Since there are\nseveral predictable label sets in discourse segmen-\ntation and parsing tasks, as shown in Figure 3, we\nprepare linearized forms for each label set.1\nIn discourse segmentation, we can consider joint\nprobability P(x,e) for a sequence with inserting a\nsymbol, [EDU], at an EDU boundary (Figure 3 (a)).\nIn discourse parsing, a discourse tree is represented\nas a sequence with several kinds of label sets: span\nlabels s, nuclearity labels u including span labels,\nand relation labels r including span and nuclear-\nity labels (Figures 3 (b)-(d)). To investigate the\neffectiveness of each label set in the reranking step,\nwe consider P(x,e,s), P(x,e,u), and P(x,e,r)\nfor each label set to represent P(x,e,p) in this pa-\nper. To build a sequence, we combine each label\nin a tree with brackets to imply the boundary for\nthe label. For example, \"(N\" and \") N\" stand for\nthe start and end of a nucleus EDU. For a node\npi of the tree, ri describes the relation between its\nchildren nodes, leading to ri of leaf nodes being\n\"Null\". When the child nodes of pi are nucleus\nand satellite, we assign label \"Span\" to the nucleus\nchild node of pi and label ri to the satellite child\nnode of pi, respectively. When the child nodes of\npi are both nucleus, we assign label ri to both child\nnodes of pi.\nFor simpler illustration, in Figure 1 (b), we show\nthe linearized discourse tree only with nuclearity\nand relation labels, since the nuclearity labels can\nalso show span and EDU boundary labels. \"Null\"\nlabels for leaf nodes are also omitted in the ﬁgure.\n4.2 Joint Probabilities\nTo calculate joint probabilities in the last subsec-\ntion with a language model, we consider probability\nP(z) for a sequence z = (z1,··· ,za), which cor-\nresponds to the probabilities for the sequential rep-\nresentations P(x,e), P(x,e,s), P(x,e,u), and\nP(x,e,r).\n1Note that using just a raw s-expression-style tree of Figure\n1 (b) in our language model cannot work because of its much\nmore kinds of labels. We include the results with this type of\ntree in Appendix A.\nAccording to Song et al. (2020), masked and\npermuted language modeling (MPNet) takes the\nadvantages of both masked language modeling and\npermuted language modeling while overcoming\ntheir issues. Compared with Bert (Devlin et al.,\n2019) and XLNet (Yang et al., 2019), MPNet con-\nsidered more information about tokens and posi-\ntions, and achieved better results for several down-\nsteam tasks (GLUE, SQuAD, etc). Taking into ac-\ncount its better performance, we choose pre-trained\nMPNet (Song et al., 2020) as our language model.\nBecause considering all possible inter-dependence\nbetween zt is intractable, we follow the decomposi-\ntion of pseudo-log-likelihood scores (PLL) (Salazar\net al., 2020) in the model. Thus, we decompose\nand calculate logarithmic P(z) as follows:\nlog P(z; θ) (2)\n≈PLL(z; θ)=\na∑\nt=1\nlog P(zt |z<t,z>t,Mt; θ),\nwhere z<t is the ﬁrst sub-sequence (z1,··· ,zt−1)\nin z and z>t is the latter sub-sequence\n(zt+1,··· ,za) in z. Mt denotes the mask token\n[MASK] at position t. P(zt | z<t,z>t,Mt; θ)\nis computed by two-stream self-attention (Yang\net al., 2019). In inference, we select z based on\n1\naPLL(z; θ).\nThis model converts z into continuous vectors\nw = {w1,··· ,wa}through the embedding layer.\nMulti-head attention layers further transform the\nvectors to predict each zt in the softmax layer.\nSince pre-trained MPNet does not consider EDU,\nspan, nuclearity, and relation labels in the pre-\ntraining step, we need to construct vectors w for\nthese labels from the pre-trained parameters to en-\nhance the prediction performance. We describe the\ndetails of this method in the next subsection.\n4.3 Label Embeddings\nIn LMGC, we embed input text tokens and labels\nin the same vector space (Wang et al., 2018a) of\nthe embedding layer. Under the setting, to deal\nwith unseen labels in the pre-trained model, we\ncompute the label embeddings by utilizing token\nembeddings in the pre-trained model.\nWe try to combine the input text with four kinds\nof labels, EDU, span, nuclearity, and relation labels,\nwhich were deﬁned and clearly described in the an-\nnotation document (Carlson and Marcu, 2001) (See\nAppendix B for the descriptions). In taking into\naccount the descriptions for the labels as additional\n2436\n(a) Sentence with EDU boundary labels\ne1 _ [EDU] _e2 _ [EDU] _e3 _ [EDU]\n(b) Sentence with span labels\n(Span _ (Span _e1 _ )Span_ (Span _e2 _ )Span_ )Span\n_ (Span _e3 _ )Span\n(c) Sentence with nuclearity labels\n(N _ (N _e1 _ )N _ (S _e2 _ )S _ )N _ (S _e3 _ )S\n(d) Sentence with relation labels\n(Span _ (Span _e1 _ )Span_ (Elaboration _e2 _\n)Elaboration_ )Span_ (Attribution _e3 _ )Attribution\nFigure 3: Example joint representations of an input text\nand labels for sentence We’ve got a lot to do, he ac-\nknowledged. ei represents the corresponding EDU, and\n\"_\" is whitespace.\ninformation, we adopt two different methods, Av-\nerage and Concatenate, for representing the label\nembeddings.\nAverage: We average the embeddings of tokens\nthat appear in the deﬁnition of a label and assign\nthe averaged embedding to the label.\nConcatenate: We concatenate a label name with\nits deﬁnition and insert the concatenated text to\nthe end of sequence z,2 so that the label embed-\nding can be captured by self-attention mechanisms\n(Vaswani et al., 2017). Note that we do not try it in\nthe parsing task, because the length of a sequence\nincreases in proportion to the increase of the num-\nber of labels, that causes a shortage of memory\nspace.\n4.4 Objective Function\nBecause the search space for sequences of a text\nand its labels is exponentially large, instead of con-\nsidering all possible sequences Z(x) for x, we\nassume Z′(x) as a subset of sequences based on\ntop-k results from the base model. We denote\nzg ∈Z(x) as the correct label sequence of x. To\nkeep pre-trained information in MPNet, we con-\ntinue masking and permutation for training model\nparameter θ. Assuming that Oa lists all permuta-\ntions of set {1,2,··· ,a}, the number of elements\nin Oa satisﬁes |Oa |= a!. For z ∈Z′(x) ∪{zg},\nwe train the model parameter θin LMGC by maxi-\nmizing the following expectation over all permuta-\n2Note that the concatenated text of the label name and its\ndeﬁnition is not masked during training.\ntions:\nEo∈Oa\na∑\nt=c+1\n[Iz log P(zot |zo<t,Mo>c; θ)\n+(1 −Iz) log(1−P(zot |zo<t,Mo>c; θ))], (3)\nwhere Iz is the indicator function, deﬁned as fol-\nlows:\nIz :=\n{\n1 if z = zg\n0 if z ̸= zg\n. (4)\nc, denoting the number of non-predicted tokens\nzo<=c, is set manually. o<t denotes the ﬁrst t−\n1 elements in o. Mo>c denotes the mask tokens\n[MASK] at position o>c. P(zot |zo<t,Mo>c; θ) is\ncomputed by two-stream self-attention (Yang et al.,\n2019).\n5 Experiments\nIn this section, we present our experiments in three\ntasks, (a) discourse segmentation, (b) sentence-\nlevel discourse parsing with gold segmentation, and\n(c) sentence-level discourse parsing with automatic\nsegmentation.\n5.1 Experimental Settings\n5.1.1 Datasets\nFollowing previous studies (Wang et al., 2017,\n2018b; Lin et al., 2019), we used the RST Dis-\ncourse Treebank (RST-DT) corpus (Carlson et al.,\n2002) as our dataset. This corpus contains 347 and\n38 documents for training and test datasets, respec-\ntively. We divided the training dataset into two\nparts, following the module RSTFinder3 (Heilman\nand Sagae, 2015), where 307 documents were used\nto train models and the remaining 40 documents\nwere used as the validation dataset.\nWe split the documents into sentences while ig-\nnoring footnote sentences, as in Joty et al. (2012).\nThere happens two possible problematic cases for\nthe splitted sentences: (1) The sentence consists of\nexactly an EDU, and so it has no tree structure. (2)\nThe tree structure of the sentence goes across to\nother sentences. Following the setting of Lin et al.\n(2019), we did not ﬁlter any sentences in task (a).\nIn task (b), we ﬁltered sentences of both cases. In\ntask (c), we ﬁltered sentences of case (2). Table 1\nshows the number of available sentences for the\nthree different tasks.\n3https://github.com/\nEducationalTestingService/rstfinder\n2437\nTask Train Valid Test\n(a) Segmentation 6,768 905 991\n(b) Parsing w/ gold segmentation 4,524 636 602\n(c) Parsing w/ auto segmentation - 861 951\nTable 1: The number of sentences for each task.\n5.1.2 Evaluation Metrics\nIn task (a), we evaluated the segmentation in micro-\naveraged precision, recall, and F 1 score with re-\nspect to the start position of each EDU. The posi-\ntion at the beginning of a sentence was ignored.\nIn task (b), we evaluated the parsing in micro-\naveraged F1 score with respect to span, nuclearity,\nand relation. In task (c) for parsing with automatic\nsegmentation, we evaluated both the segmentation\nand parsing in micro-averaged F1 score.\nWe used the paired bootstrap resampling (Koehn,\n2004) for the signiﬁcance test in all tasks when\ncomparing two systems.\n5.1.3 Compared Methods\nAs our proposed methods, we used LMGC e,\nLMGCs, LMGC u, and LMGC r, which respec-\ntively model probability P(x,e), P(x,e,s),\nP(x,e,u), and P(x,e,r) with initialized label\nembeddings. We represent LMGC with Average\nand Concatenate label embeddings as Enhance and\nExtend, respectively.\nWe used the base discourse segmenter and parser\ndescribed in Section 3 as our baseline. We re-\nproduced the base discourse segmenter BiLSTM-\nCRF4 (Wang et al., 2018b). Because BiLSTM-CRF\nadopted the hidden states of ELMo (Peters et al.,\n2018) as word embeddings, we also tried the last\nhidden state of MPNet as the word embeddings\nfor BiLSTM-CRF for fairness. We retrained the\nsegmenter in ﬁve runs, and the experimental results\nare showed in Appendix C. The publicly shared\nBiLSTM-CRF by Wang et al. (2018b) is our base\nsegmenter in the following experiments.\nAs for the base parser, we retrained two models,\n2-stage Parser5 (Wang et al., 2017) and span-based\nParser6 (Kobayashi et al., 2020). Different from\nthe setting of Lin et al. (2019), we retrained 2-\nstage Parser in the sentence-level rather than in\nthe document-level. Since the experimental re-\n4https://github.com/PKU-TANGENT/\nNeuralEDUSeg\n5https://github.com/yizhongw/StageDP\n6https://github.com/nttcslab-nlp/\nTop-Down-RST-Parser\nsults show our retrained 2-stage Parser achieved\nthe highest F1 scores among several parsers (See\nAppendix C), we selected it as our base parser in\nthe following experiments.\nFurthermore, for comparing LMGC with an uni-\ndirectional generative model (Mabona et al., 2019),\nwe constructed another baseline method which uti-\nlizes a GPT-2 (Radford et al., 2019) based reranker.\nThis method follows an unidirectional language\nmodel-based generative parser (Choe and Char-\nniak, 2016), and considers top-kresults from the\nbase model by an add-1 version of inﬁnilog loss\n(Ding et al., 2020) during training. We denote this\nbaseline as GPT2LM hereafter. GPT2LM models\nP(x,e) for task (a) and P(x,e,r) for tasks (b)\nand (c), respectively. Both LMGC and GPT2LM\nare the ensemble of 5 models with different ran-\ndom seeds. See Appendix D for a complete list of\nhyperparameter settings.\n5.1.4 Number of Candidates\nAs described in Section 4, LMGC requires parame-\nters ks and kp for the number of candidates in the\nsteps for different tasks. We tuned ks and kp based\non the performance on the validation dataset.7\nIn task (a), ks was set to 20 and 5 for training\nand prediction, respectively. In task (b), kp was set\nto 20 and 5 for training and prediction, respectively.\nIn task(c), ks and kp were both set to 5 for predic-\ntion. The set of parameters was similarly tuned for\nGPT2LM on the validation dataset. We list all of\nthem in Appendix E.\n5.2 Results\n5.2.1 Discourse Segmentation\nTable 2 shows the experimental results for the dis-\ncourse segmentation task. Oracle indicates the up-\nper bound score that can be achieved with candi-\ndates generated by the base model. To compute the\nOracle score, if the candidades by the base model\ninclude the correct answer, we assume the predic-\ntion is correct.\nLMGCe signiﬁcantly outperformed GPT2LMe.8\nWe think the reason is similar to what Zhu et al.\n(2020) reported: BERT-based bidirectional Trans-\nformer encoders encode more rhetorical features\nthan GPT2-based unidirectional Transformer en-\n7Note that we should separately tune the number of candi-\ndates for training and prediction stages because LMGC utilizes\nEq.(2) for prediction and Eq.(3) for training, respectively.\n8We chose GPT2LMe for the signiﬁcance test because we\nhad only reported scores for the pointer-networks.\n2438\nModel Precision Recall F 1\nOracle 97.73 98.67 98.20\nPointer-networks* 93.34 97.88 95.55\nBase segmenter 92.22 95.35 93.76\nGPT2LMe 94.05 95.72 94.88\nLMGCe 95.31 97.56 96.43 †\nEnhancee 95.54 97.93 96.72 †\nExtende 95.05 97.86 96.44 †\nTable 2: Results for the discourse segmentation task. *\nindicates the reported score by Lin et al. (2019). The\nbest score in each metric among the models is indicated\nin bold. †indicates that the score is signiﬁcantly supe-\nrior to GPT2LM with a p-value < 0.01.\nModel Span Nuclearity Relation\nOracle 98.67 95.88 90.07\nPointer-networks* 97.44 91.34 81.70\nBase parser 97.92 92.07 82.06\nGPT2LMr 96.35 88.11 77.86\nLMGCs 98.23‡ 92.31 82.22\nEnhances 98.27‡ 92.39 82.42\nLMGCu 98.31‡ 94.00† 83.63†\nEnhanceu 98.31† 93.88† 83.56†\nLMGCr 98.00 93.09 † 83.99†\nEnhancer 98.12 93.13 † 84.69†\nTable 3: Results for the sentence-level discourse pars-\ning task with gold segmentation. * indicates the re-\nported score by Lin et al. (2019). The best score in\neach metric among the models is indicated in bold. †\nand ‡indicate that the score is signiﬁcantly superior to\nthe base parser with a p-value < 0.01 and < 0.05, re-\nspectively.\ncoders. Using Average label embeddings is more\nhelpful than using Concatenate label embeddings\nfor LMGCe. Enhance e achieved the state-of-the-\nart F1 score of 96.72, which outperformed both the\nbase segmenter and the pointer-networks.\n5.2.2 Sentence-level Discourse Parsing\nGold Segmentation : Table 3, Figures 4 and 5\nshow the experimental results for the sentence-level\ndiscourse parsing task with gold segmentation. In\nTable 3, LMGCu achieved the highest span and nu-\nclearity F1 scores of 98.31 and 94.00, respectively.\nEnhancer achieved the state-of-the-art relation F1\nscore of 84.69, which is signiﬁcantly superior to the\nbase parser. Although using Average label embed-\ndings improved LMGCr, it can provide no or only\nlimited improvement for LMGCu and LMGCs. We\nSpan\nElaborationAttributionSame-UnitJointContrastBackgroundEnablementCauseTemporalConditionComparisonExplanationManner-MeansSummaryEvaluationTopic-Comment\n0\n2,000\n4,000\n6,000\n020406080100\nRelation label\n# of labels\nF1score\nGoldLMGC2-stage\nFigure 4: Performance of 2-stage parser and Enhancer\nin the sentence-level discourse parsing task with gold\nsegmentation. The hollow bar denotes the number of\ndifferent gold labels in the training dataset. Blue and\nred lines indicate the F1 scores of Enhancer and 2-stage\nparser, respectively, for each relation label.\nAttribution\nBackground\nCause\nComparison\nCondition\nContrast\nElaboration\nEnablement\nEvaluation\nExplanation\nJoint\nManner-Means\nSame-Unit\nSummary\nTemporal\nTopic-Comment\nSpan\nAttributionBackground\nCauseComparison\nConditionContrastElaborationEnablementEvaluationExplanation\nJointManner-MeansSame-UnitSummaryTemporalTopic-CommentSpan\n0.960.010.000.000.000.000.000.000.000.000.000.000.000.000.000.000.03\n0.030.490.000.000.070.050.050.020.000.000.100.030.070.000.020.000.08\n0.000.050.190.000.000.020.070.020.000.120.290.020.000.000.020.000.19\n0.000.140.000.190.000.380.050.000.000.000.000.000.000.000.000.000.24\n0.020.070.000.000.710.020.000.000.000.000.050.000.070.000.000.000.05\n0.010.010.010.010.010.620.000.000.000.000.110.000.080.000.000.000.13\n0.000.010.010.000.000.000.900.020.000.010.020.000.010.000.000.000.04\n0.000.030.000.000.000.000.150.700.000.000.050.030.000.000.000.000.05\n0.270.000.000.000.000.000.450.000.000.000.180.000.000.000.000.000.09\n0.000.000.140.000.000.000.000.050.000.590.000.000.050.000.000.000.18\n0.010.010.000.000.000.030.040.000.000.000.750.000.060.000.010.000.08\n0.050.050.000.050.000.050.140.000.000.050.050.590.000.000.000.000.00\n0.000.000.000.000.000.000.020.000.000.000.030.000.920.000.000.000.02\n0.000.000.000.000.000.050.320.000.000.000.000.000.050.580.000.000.00\n0.000.220.000.020.000.020.050.040.000.000.270.000.000.000.290.000.09\n0.000.000.000.000.000.000.250.000.000.000.250.000.000.000.000.000.50\n0.020.010.000.000.000.010.010.000.000.000.020.000.010.000.000.000.91\nPredicted relation\nTrue relation\nFigure 5: Confusion matrix for Enhance r in the\nsentence-level discourse parsing task with gold seg-\nmentation. We show the ratio of the number of in-\nstances with predicted labels (for a column) to the num-\nber of instances with gold labels (for a row) in the cor-\nresponding cell.\nguess that this difference is caused by the number\nof different kinds of labels in span, nuclearity, and\nrelation. The performance of GPT2LM r is even\nworse than the base parser. We think this is because\nwe added the relation labels to the vocabulary of\nGPT-2 and resized the pre-trained word embed-\ndings.\nFigure 4 shows the comparison between the base\nparser and Enhancer with respect to each ralation\nlabel. In most relation labels, Enhance r outper-\nformed 2-stage Parser except for the labels Expla-\nnation, Evaluation, and Topic-Comment. 2-stage\nParser achieved the F1 score of 17.14 for label Tem-\nporal while Enhance r achieved the F 1 score of\n44.44 by reranking the parsing results from 2-stage\nParser. Such great improvement with Enhance r\ncan also be found for labels such asContrast, Back-\n2439\n(a) LMGCr\n−37 −36.5 −36 −35.5 −35 −34.5 −34 −33.5 −33\n24.4\n24.6\n24.8\n25\n25.2\n25.4\n25.6\n25.8\n26\n26.2\n26.4\n26.6\n26.8\n27\n27.2 (attribution\n)attribution(background\n)background\n(cause\n)cause\n(comparison\n)comparison\n(condition\n)condition\n(contrast\n)contrast\n(elaboration\n)elaboration\n(enablement\n)enablement(evaluation\n)evaluation\n(explanation\n)explanation\n(joint\n)joint(manner-means\n)manner-means\n(topic-comment)topic-comment\n(summary\n)summary\n(temporal\n)temporal\n(topic-change)topic-change\n(textual-organization)textual-organization\n(span\n)span(same-unit\n)same-unit\n(b) Enhancer\n25 25.5 26 26.5 27 27.5 28 28.5 29\n−35\n−34.5\n−34\n−33.5\n−33\n−32.5\n−32\n(attribution)attribution\n(background)background\n(cause)cause\n(comparison)comparison\n(condition)condition\n(contrast)contrast\n(elaboration)elaboration(enablement)enablement\n(evaluation)evaluation\n(explanation)explanation\n(joint)joint\n(manner-means)manner-means\n(topic-comment)topic-comment\n(summary)summary\n(temporal)temporal\n(topic-change)topic-change\n(textual-organization)textual-organization\n(span)span(same-unit)same-unit\nFigure 6: t-SNE plot of relation label embeddings\ntrained in LMGCr and Enhancer.\nground, and Cause. Obviously, Enhancer tends to\nimprove the performance for labels whose training\ndata is limited.\nFigure 5 shows a confusion matrix of Enhancer\nfor each relation label. It shows that the relation\nlabels Comparison, Cause, and Temporal were of-\nten predicted wrongly as Contrast, Joint, and Joint\nor Background, respectively, by Enhance r, even\nthough these labels have at least 100 training data.\nWe guess this might be due to some similarities\nbetween those labels.\nBy using the t-SNE plot (Van der Maaten and\nHinton, 2008), we visualize the trained relation\nlabel embeddings of LMGCr and Enhancer. Fig-\nures 6a and 6b show the results. Figure 6a shows a\nclearer diagonal that divides labels with parenthesis\nModel Seg Parse\nSpan Nuclearity Relation\nPointer-networks* - 91.75 86.38 77.52\nOracleseg 98.24 - - -\nBase segmenter 93.92 - - -\nGPT2LMe 95.03 - - -\nLMGCe 96.51 - - -\nEnhancee 96.79 - - -\nExtende 96.48 - - -\nOracle - 93.95 91.25 85.93\nBase parser - 93.53 88.08 78.75\nGPT2LMr - 92.02 84.20 74.49\nLMGCs - 93.96 ‡ 88.46 79.25\nEnhances - 94.00† 88.50 79.33\nLMGCu - 93.96 † 89.90† 80.33†\nEnhanceu - 93.92 ‡ 89.74† 80.22†\nLMGCr - 93.65 89.08 † 80.57†\nEnhancer - 93.73 89.16 † 81.18†\nTable 4: Results for the sentence-level discourse pars-\ning task with automatic segmentation. * indicates the\nreported score by Lin et al. (2019). The best score in\neach metric among the models for each block is indi-\ncated in bold. We used the discourse segmentation re-\nsults of Enhancee as the input of the discourse parsing\nstage for all models, for fair comparison of sentence-\nlevel discourse parsing. †and ‡indicate that the score is\nsigniﬁcantly superior to the base parser with a p-value\n< 0.01 and < 0.05, respectively.\n\"(\" from the ones with \")\", while Figure 6b shows\nmore distinct divisions between labels.\nAutomatic Segmentation: Table 4 shows the ex-\nperimental results for the sentence-level discourse\nparsing task with automatic segmentation. The sec-\nond and third blocks in the table show the results for\nthe ﬁrst and second stages, discourse segmentation\nand sentence-level discourse parsing, respectively.9\nEnhancer achieved the highest relation F1 score\nof 81.18, which is a signiﬁcant improvement of\n2.43 points compared to the base parser. Enhances\nand LMGCu achieved the highest span and nucle-\narity F1 scores of 94.00 and 89.90, respectively.\nSince LMGC ∗ and Enhance ∗ were the models\ntrained in task (b), and Enhancee achieved the F1\nscore of 96.79 in discourse segmentation, it is not\nsurprising to ﬁnd that the tendency of those results\nis similar to that in sentence-level discourse parsing\nwith gold segmentation.\n6 Conclusion\nIn this research, we proposed a language model-\nbased generative classiﬁer, LMGC. Given the top-\n9Note that F 1 scores for discourse segmentation in the\nsecond block are not the same as in Table 2 due to the different\ntest dataset.\n2440\nk discourse segmentations or parsings from the\nbase model, as a reranker, LMGC achieved the\nstate-of-the-art performances in both discourse seg-\nmentation and sentence-level discourse parsing.\nThe experimental results also showed the poten-\ntial of constructing label embeddings from token\nembeddings by using label descriptions in the man-\nual. In the future, we plan to apply LMGC to other\ndiverse classiﬁcation tasks.\nReferences\nLynn Carlson and Daniel Marcu. 2001. Discourse tag-\nging reference manual. ISI Technical Report ISI-TR-\n545.\nLynn Carlson, Daniel Marcu, and Ellen Okurowski\nMary. 2002. Rst discourse treebank ldc2002t07.\nPhiladelphia:Linguistic Data Consortium.\nLynn Carlson, Daniel Marcu, and Mary Ellen\nOkurovsky. 2001. Building a discourse-tagged cor-\npus in the framework of Rhetorical Structure Theory.\nIn Proceedings of the Second SIGdial Workshop on\nDiscourse and Dialogue.\nDo Kook Choe and Eugene Charniak. 2016. Parsing\nas language modeling. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2331–2336, Austin, Texas.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nXiaoan Ding, Tianyu Liu, Baobao Chang, Zhifang Sui,\nand Kevin Gimpel. 2020. Discriminatively-Tuned\nGenerative Classiﬁers for Robust Natural Language\nInference. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 8189–8202, Online. Associa-\ntion for Computational Linguistics.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 199–209, San Diego, California.\nAssociation for Computational Linguistics.\nVanessa Wei Feng and Graeme Hirst. 2014a. A linear-\ntime bottom-up discourse parser with constraints\nand post-editing. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 511–\n521, Baltimore, Maryland. Association for Compu-\ntational Linguistics.\nVanessa Wei Feng and Graeme Hirst. 2014b. Two-pass\ndiscourse segmentation with pairing and global fea-\ntures.\nSeeger Fisher and Brian Roark. 2007. The utility of\nparse-derived features for automatic discourse seg-\nmentation. In Proceedings of the 45th Annual Meet-\ning of the Association of Computational Linguistics,\npages 488–495, Prague, Czech Republic. Associa-\ntion for Computational Linguistics.\nFrancisco Guzmán, Shaﬁq Joty, Lluís Màrquez, and\nPreslav Nakov. 2014. Using discourse structure im-\nproves machine translation evaluation. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 687–698, Baltimore, Maryland. Associ-\nation for Computational Linguistics.\nMichael Heilman and Kenji Sagae. 2015. Fast rhetor-\nical structure theory discourse parsing. arXiv\npreprint arXiv:1505.02425.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-\ntional lstm-crf models for sequence tagging.\nShaﬁq Joty, Giuseppe Carenini, and Raymond Ng.\n2012. A novel discriminative framework for\nsentence-level discourse analysis. In Proceedings\nof the 2012 Joint Conference on Empirical Methods\nin Natural Language Processing and Computational\nNatural Language Learning, pages 904–915, Jeju Is-\nland, Korea. Association for Computational Linguis-\ntics.\nShaﬁq Joty, Giuseppe Carenini, Raymond Ng, and\nYashar Mehdad. 2013. Combining intra- and multi-\nsentential rhetorical parsing for document-level dis-\ncourse analysis. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 486–496,\nSoﬁa, Bulgaria. Association for Computational Lin-\nguistics.\nShaﬁq Joty, Giuseppe Carenini, and Raymond T. Ng.\n2015. CODRA: A novel discriminative framework\nfor rhetorical analysis. Computational Linguistics,\n41(3):385–435.\nShaﬁq Joty, Francisco Guzmán, Lluís Màrquez, and\nPreslav Nakov. 2017. Discourse structure in ma-\nchine translation evaluation. Computational Lin-\nguistics, 43(4):683–722.\nDan Jurafsky. 2000. Speech & language processing .\nPearson Education India.\nNaoki Kobayashi, Tsutomu Hirao, Hidetaka Kami-\ngaito, Manabu Okumura, and Masaaki Nagata. 2020.\nTop-down rst parsing utilizing granularity levels in\ndocuments. volume 34, pages 8099–8106.\n2441\nPhilipp Koehn. 2004. Statistical signiﬁcance tests\nfor machine translation evaluation. In Proceed-\nings of the 2004 Conference on Empirical Meth-\nods in Natural Language Processing , pages 388–\n395, Barcelona, Spain. Association for Computa-\ntional Linguistics.\nXiang Lin, Shaﬁq Joty, Prathyusha Jwalapuram, and\nM Saiful Bari. 2019. A uniﬁed linear-time frame-\nwork for sentence-level discourse parsing. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4190–\n4200, Florence, Italy. Association for Computational\nLinguistics.\nAmandla Mabona, Laura Rimell, Stephen Clark, and\nAndreas Vlachos. 2019. Neural generative rhetor-\nical structure parsing. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2284–2295, Hong Kong,\nChina. Association for Computational Linguistics.\nWilliam C Mann and Sandra A Thompson. 1988.\nRhetorical structure theory: Toward a functional the-\nory of text organization.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2699–2712, Online. Association for Compu-\ntational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. Mpnet: Masked and permuted pre-\ntraining for language understanding. arXiv preprint\narXiv:2004.09297.\nCaroline Sporleder and Mirella Lapata. 2005. Dis-\ncourse chunking and its application to sentence com-\npression. In Proceedings of Human Language Tech-\nnology Conference and Conference on Empirical\nMethods in Natural Language Processing , pages\n257–264, Vancouver, British Columbia, Canada. As-\nsociation for Computational Linguistics.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nGuoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe\nZhang, Dinghan Shen, Xinyuan Zhang, Ricardo\nHenao, and Lawrence Carin. 2018a. Joint embed-\nding of words and labels for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2321–2331, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nYizhong Wang, Sujian Li, and Houfeng Wang. 2017.\nA two-stage parsing method for text-level discourse\nanalysis. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers) , pages 184–188, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nYizhong Wang, Sujian Li, and Jingfeng Yang. 2018b.\nToward fast and accurate neural discourse segmen-\ntation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 962–967, Brussels, Belgium. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nNgo Xuan Bach, Nguyen Le Minh, and Akira Shi-\nmazu. 2012. A reranking model for discourse seg-\nmentation using subtree features. In Proceedings\nof the 13th Annual Meeting of the Special Interest\nGroup on Discourse and Dialogue , pages 160–168,\nSeoul, South Korea. Association for Computational\nLinguistics.\n2442\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nZining Zhu, Chuer Pan, Mohamed Abdalla, and Frank\nRudzicz. 2020. Examining the rhetorical capacities\nof neural language models. In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP, pages 16–32.\nA Experimental Results of LMGC with\nTree\nSince the raw s-expression-style tree is longer than\nour joint representations with span, nuclearity and\nrelation, we transformed the raw tree into a se-\nquence as Figure 7 shows, where the nuclearity and\nrelation labels are connected together by the colons.\nTo construct the label embedding for P(x,e,p),\nwe combined the descriptions of the nuclearity and\nrelation (see descriptions in Appendix B), and as-\nsigned the combination to the corresponding node.\nFor example, the description of \"(Attribution:S\" is\nthe start of a supporting or background piece of\ninformation attribution, attribution represents both\ndirect and indirect instances of reported speech.\n(Span:N _ (Span:N _e1 _ )Span:N_ (Elaboration:S _e2 _\n)Elaboration:S_ )Span:N_ (Attribution:S _e3 _ )Attribution:S\nFigure 7: Example joint representation of an input text\nwith all tree labels for sentenceWe’ve got a lot to do, he\nacknowledged. ei represents the corresponding EDU,\nand \"_\" is whitespace.\nLMGCp models the joint probability P(x,e,p)\nwith initialized label embedding. The experimental\nresults of LMGCp and Enhancep for the sentence-\nlevel discourse parsing task with gold segmentation\nare showed in Table 5. LMGCp and Enhancep are\nthe ensemble of 5 models with different random\nseed, although the training loss of Enhancep in 2\nof 5 models did not decrease.\nModel Span Nuclearity Relation\nLMGCp 97.84 92.90 84.11\nEnhance p 98.04 92.74 84.18\nTable 5: Performances of LMGC p and Enhance p in\nthe sentence-level discourse parsing task with gold seg-\nmentation.\nB Label Descriptions\nWe list our extracted label descriptions from Carl-\nson and Marcu (2001) in Table 6. For parsing\nsymbols with brackets \"(\" and \")\" like \"(N\" and\n\")N\", we inserted the position phrase, the start of\nand the end of, to the beginning of their label deﬁ-\nnitions. So the description of \") N\" is the end of a\nmore salient or essential piece of information.\nC Experiment Results of Reproduced\nBase Model\nTable 7 shows the experimental results of BiLSTM-\nCRF in discourse segmentation, where the results\nof our reproduced BiLSTM-CRF are averaged in\nﬁve runs. Table 8 shows the experimental results\nof different parsers in the sentence-level discourse\nparsing task with gold segmentation.\nD Hyperparmeters\nFor LMGC, we used the source code shared in the\npublic github10 of Song et al. (2020). We used the\nuploaded pre-trained MPNet and same setup as il-\nlustrated in Table 9. 15% tokens as the predicted\ntokens were masked by replacement strategy 8:1:1.\nRelative positional embedding mechanism (Shaw\net al., 2018) was utilized. Since the vocab we used\nis same as the one of BERT (Devlin et al., 2019),\nwe used the symbol [SEP] to represent [EDU] and\nsymbol [unused#] starting from 0 to represent pars-\ning labels such as \"(N\" and \"(Attribution\".\nFor GPT2LM, we used the source code shared\nin the public github11 (Ott et al., 2019). Following\nthe steps in Choe and Charniak (2016), we utilized\nEq (5) (Jurafsky, 2000) to compute the joint distri-\nbution,\nP(x,y) =P(z) = P(z1,...,z a) (5)\n=\na∏\nt=1\nP(zt|z1,...,z t−1),\nwhere P(zt|z1,...,z t−1) was computed by GPT-\n2 (Radford et al., 2019). And in inference, we\nselected z based on 1\na log P(z). An add-1 version\nof inﬁnilog loss (Ding et al., 2020) was utilized for\ntraining GPT2LM as follows:\n−log f(z) + log[1 +\n∑\nz′∈Z′(x),z′̸=z\nf(z′)], (6)\n10https://github.com/microsoft/MPNet\n11https://github.com/pytorch/fairseq/\ntree/master/fairseq/models/huggingface\n2443\nLabel Deﬁnition\n[EOS] elementary discourse units are the minimal building blocks of a discourse tree\nSpan span\nNucleus a more salient or essential piece of information\nSatellite a supporting or background piece of information\nAttribution attribution, attribution represents both direct and indirect instances of reported\nspeech\nBackground background or circumstance\nCause cause or result\nComparison comparison, preference, analogy or proportion\nCondition condition, hypothetical, contingency or otherwise\nContrast contrast relation, spans contrast with each other along some dimension. Typi-\ncally, it includes a contrastive discourse cue, such as but, however, while.\nElaboration elaboration, elaboration provides speciﬁc information or details to help deﬁne a\nvery general concept\nEnablement enablement, enablement presentes action to increase the chances of the unreal-\nized situation being realized.\nEvaluation evaluation, interpretation, conclusion or comment\nExplanation evidence, explanation or reason\nJoint list, list contains some sort of parallel structure or similar fashion between the\nunits\nManner-Means explaining or specifying a method , mechanism , instrument , channel or conduit\nfor accomplishing some goal\nTopic-Comment problem solution, question answer, statement response, topic comment or\nrhetorical question\nSummary summary or restatement\nTemporal situations with temporal order, before, after or at the same time\nTopic change topic change\nTextual-organization links that are marked by schemata labels\nSame-unit links between two non-adjacent parts when separated by an intervening relative\nclause or parenthetical\nTable 6: Extracted label deﬁnitions.\n2444\nModel Precision Recall F 1\nReported* 92.04 94.41 93.21\nShared 92.22 95.35 93.76\nReproduced (ELMo) 93.16 96.26 94.68\nReproduced (MPNet) 92.84 95.63 94.21\nTable 7: Performances of BiLSTM-CRF (Wang et al.,\n2018b) in the discourse segmentation task. The best\nscore in each metric among the models is indicated in\nbold. * indicates the reported score by Lin et al. (2019).\nShared is the publicly shared model by Wang et al.\n(2018b). Reproduced (ELMo) and Reproduced (MP-\nNet) are our reproduced models with different word em-\nbeddings.\nModel Span Nuclearity Relation\n2-Stage Parser* 95.60 87.80 77.60\nPointer-networks* 97.44 91.34 81.70\nSpan-based Parser 96.67 90.23 74.76\n2-Stage Parser 97.92 92.07 82.06\nTable 8: Performance of retrained parsers in the\nsentence-level discourse parsing task with gold seg-\nmentation. The best score in each metric among the\nmodels is indicated in bold. * indicates the reported\nscore by Lin et al. (2019).\nwhere\nf(z) = exp(1\na log P(z))∑\nz′∈Z′(x) exp( 1\na′ log P(z′)). (7)\nWe used the uploaded pretrained \"gpt2\" model\n(Wolf et al., 2020) and same setup as illustrated\nin Table 10. We used symbol \"=====\" in vocab\nto represent the symbol [EDU]. Because the vocab\nof GPT-2 has no available symbol for representing\nan unseen symbol, we added <pad> and our rela-\ntion symbols to the vocab of GPT-2 and resized the\npre-trained word embeddings.\nE Setting of Candidates\nTable 11 shows the setting of candidates for differ-\nent tasks. As described in Section 4.4, we do data\naugmentation by using additional top-kresults gen-\nerated by a base method, a larger kduring training\nis expected to bring more promotion for LMGC.\nHowever, a larger k during prediction step intro-\nduces more candidates and may make the predic-\ntion more difﬁcult. Taking this into consideration,\nwe tuned ks and kp for training and prediction sep-\narately based on the performance on the validation\ndataset.\nHyperparameter Value\nOptimizer adam\nAdam β1 0.9\nAdam β2 0.98\nAdam ϵ 1e- 6\nweight decay 0.01\nLearning rate 0.00009\nBatch size 8192 tokens\nWarm up steps 2.4 epoch\nEpoch 30\nAttention layer 12\nAttention head 12\ndropout 0.1\nattention dropout 0.1\nHidden size 768\nV ocab size 30527\nTokenizer Byte pair encoder\nMax sentence length 512\nTable 9: List of used hyperparameters for LMGC.\nHyperparameter Value\nOptimizer adam\nAdamβ1 0.9\nAdamβ2 0.98\nAdamϵ 1e- 6\nweight decay 0.01\nLearning rate 0.0001\nBatch size 512 gold tokens + candidate tokens\nWarm up steps 2.4 epoch\nEpoch 30\nAttention layer 12\nAttention head 12\ndropout 0.1\nattention dropout 0.1\nHidden size 768\nV ocab size 50257+ added tokens\nTokenizer Byte pair encoder\nMax sentence length 512\nTable 10: List of used hyperparameters for GPT2LM.\nIn task (a), we used the Viterbi-topk algorithm\nfor the base segmenter to select top-ks segmenta-\ntions. We tuned ks ∈{0,10,20}for training while\nks for prediction was ﬁxed as 5. Note that we used\nonly gold segmentations for training when ks was\nset to 0. Table 12 shows the experimental results,\nwhere both LMGCe and GP2TLMe are the ensem-\nble of 5 models. Then we tuned ks ∈{5,10,20}\nfor prediction by using the LMGCe and GP2TLMe\ntrained with top-20 candidates, Table 13 shows the\nresults.\nIn task (b), we utilized beam search in each stage\n2445\nTask Data Segmentation Parsing # of dataks 1st stage 2rd stage kp\n(a) Training 20 - - - 140924\nPrediction 5 - - - -\n(b)\nTrainingw/ span or nuclearity - 20 1 20 60742\nTrainingw/ relation or all - 3 7 20 95004\nPrediction - 5 5 5 -\n(c) Prediction 5 5 5 5 -\nTable 11: Setting of top candidates for different tasks. The Prediction data denotes the validation and test dataset.\nof the base parser and after two stages we computed\nthe perplexity to keep top-kp parsings. We tuned\nkp ∈{0,10,20}for training while kp for predic-\ntion was ﬁxed as 5. Note that we used only gold\nparsings for training when kp was set to 0. Ta-\nble 14 shows the experimental results, where both\nLMGCr and GPT2LMr are the ensemble of 5 mod-\nels. Then we tuned kp ∈{5,10,20}for prediction\nby using the LMGCr and GPT2LMr trained with\ntop-20 candidates, Table 15 shows the results.\nIn task (c), same as in task (a), we tuned ks ∈\n{5,10,20}for predicting discourse segmentation\nby using the LMGCe and GP2TLMe trained with\ntop-20 candidates for task (a), Table 16 shows\nthe result. We utilized LMGC e to select the best\nsegmentation from top-5 segmentations for fol-\nlowing discourse parsing. Then same as in task\n(b), we tuned kp ∈{5,10,20}for predicting dis-\ncourse parsing by using the LMGCr and GPT2LMr\ntrained with top-20 candidates for task (b), Table 17\nshows the result.\nIn tasks (b) and (c), LMGC s and Enhance s\ncannot distinguish the candidates with the same\nspan labels but different nulearity or relation labels,\nLMGCu and Enhanceu cannot distinguish the can-\ndidates with the same nulearity labels but different\nrelation labels. Under this condition, the indistin-\nguishable parsings would be ranked by the base\nparser. And in task (b), for training data with span\nor nuclearity labels, we used the beam sizes 20 and\n1 in the ﬁrst and second stages of the base parser,\nrespectively.\nModel ks for training Precision Recall F1\nLMGCe 0 87.76 95.72 91.57\n10 97.67 97.73 97.70\n20 97.99 97.86 97.92\nGPT2LMe 0 81.72 96.18 88.36\n10 96.67 96.05 96.36\n20 96.93 96.05 96.48\nTable 12: Results of tuning ks for training in task (a).\nThe best score in each metric among different ks for\ntraining is indicated in bold.\nModel ks for prediction Precision Recall F1\nOracle 5 99.94 99.68 99.81\n10 99.94 99.68 99.81\n20 99.94 99.68 99.81\nLMGCe 5 97.99 97.86 97.92\n10 97.47 97.54 97.51\n20 97.41 97.60 97.51\nGPT2LMe 5 96.93 96.05 96.48\n10 96.47 95.59 96.03\n20 95.76 95.14 95.45\nTable 13: Results of tuning ks for prediction in task (a).\nThe best score in each metric among different ks for\nprediction is indicated in bold.\nModel kp for training Span Nuclearity Relation\nLMGCr 0 97.25 92.21 83.37\n10 97.46 92.71 83.23\n20 97.50 93.02 83.44\nGPT2LMr 0 97.36 92.07 79.11\n10 96.93 90.80 80.76\n20 96.79 90.66 80.94\nTable 14: Results of tuning kp for training in task (b).\nThe best score in each metric among different kp for\ntraining is indicated in bold.\n2446\nModel kpfor prediction Span Nuclearity Relation\nOracle 5 98.66 96.41 92.11\n10 99.30 98.03 94.43\n20 99.47 98.48 95.42\nLMGCr 5 97.50 93.02 83.44\n10 97.50 92.46 83.30\n20 97.29 92.25 83.30\nGPT2LMr 5 96.79 90.66 80.94\n10 94.26 81.08 70.82\n20 93.27 77.20 66.67\nTable 15: Results of tuningkp for prediction in task (b).\nThe best score in each metric among different kp for\nprediction is indicated in bold.\nModel ks for prediction Precision Recall F1\nOracle 5 99.93 99.65 99.79\n10 99.93 99.65 99.79\n20 99.93 99.65 99.79\nLMGCe 5 97.96 97.74 97.85\n10 97.32 97.39 97.36\n20 97.33 97.53 97.43\nGPT2LMe 5 96.94 95.91 96.42\n10 96.45 95.63 96.04\n20 95.75 95.35 95.55\nTable 16: Results of tuning ks for prediction in task (c).\nThe best score in each metric among different ks for\nprediciton is indicated in bold.\nModel kpfor prediction Span Nuclearity Relation\nOracle 5 95.05 92.95 89.02\n10 95.93 94.73 91.25\n20 96.21 95.36 92.45\nLMGCr 5 94.39 90.12 80.88\n10 94.39 89.45 80.74\n20 94.18 89.24 80.63\nGPT2LMr 5 93.65 87.80 78.59\n10 91.18 78.55 68.99\n20 90.30 74.96 65.19\nTable 17: Results of tuning kp for prediction in task (c).\nThe best score in each metric among different kp for\nprediciton is indicated in bold.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7938860654830933
    },
    {
      "name": "Parsing",
      "score": 0.7794700860977173
    },
    {
      "name": "Natural language processing",
      "score": 0.7405263185501099
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7255570888519287
    },
    {
      "name": "Sentence",
      "score": 0.6898670196533203
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6359941959381104
    },
    {
      "name": "Generative grammar",
      "score": 0.5986964702606201
    },
    {
      "name": "Segmentation",
      "score": 0.5515239238739014
    },
    {
      "name": "Language model",
      "score": 0.5497956871986389
    },
    {
      "name": "Embedding",
      "score": 0.4214274287223816
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114531698",
      "name": "Tokyo Institute of Technology",
      "country": "JP"
    }
  ]
}