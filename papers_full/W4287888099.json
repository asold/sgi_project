{
  "title": "Selective Differential Privacy for Language Modeling",
  "url": "https://openalex.org/W4287888099",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2116760462",
      "name": "Weiyan Shi",
      "affiliations": [
        "Virginia Tech",
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A3040292757",
      "name": "Aiqi Cui",
      "affiliations": [
        "Virginia Tech",
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2099428651",
      "name": "Evan Li",
      "affiliations": [
        "Columbia University",
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2169838738",
      "name": "Ruoxi Jia",
      "affiliations": [
        "Virginia Tech",
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2037128165",
      "name": "Zhou Yu",
      "affiliations": [
        "Virginia Tech",
        "Columbia University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3099692016",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W4205228770",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2810715221",
    "https://openalex.org/W2951711317",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W3172340245",
    "https://openalex.org/W2953449233",
    "https://openalex.org/W4287663285",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962934384",
    "https://openalex.org/W3092543885",
    "https://openalex.org/W2785361959",
    "https://openalex.org/W2570759866",
    "https://openalex.org/W3045700442",
    "https://openalex.org/W2113792377",
    "https://openalex.org/W3158759594",
    "https://openalex.org/W3185099774",
    "https://openalex.org/W4385573200",
    "https://openalex.org/W4288358239",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W4298166788",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3048384720",
    "https://openalex.org/W3102378604",
    "https://openalex.org/W2884738118"
  ],
  "abstract": "With the increasing applications of language models, it has become crucial to protect these models from leaking private information. Previous work has attempted to tackle this challenge by training RNN-based language models with differential privacy guarantees.However, applying classical differential privacy to language models leads to poor model performance as the underlying privacy notion is over-pessimistic and provides undifferentiated protection for all tokens in the data. Given that the private information in natural language is sparse (for example, the bulk of an email might not carry personally identifiable information), we propose a new privacy notion, selective differential privacy, to provide rigorous privacy guarantees on the sensitive portion of the data to improve model utility. To realize such a new notion, we develop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based language models. Besides language modeling, we also apply the method to a more concrete application – dialog systems. Experiments on both language modeling and dialog system building show that the proposed privacy-preserving mechanism achieves better utilities while remaining safe under various privacy attacks compared to the baselines. The data and code are released at https://github.com/wyshi/lm_privacy to facilitate future research.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2848 - 2859\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nSelective Differential Privacy for Language Modeling\nWeiyan Shi1, Aiqi Cui1, Evan Li1, Ruoxi Jia2 Zhou Yu1\nColumbia University1,Virginia Tech2\n{ws2634, ac4788, el3078}@columbia.edu, ruoxijia@vt.edu, , zy2461@columbia.edu\nAbstract\nWith the increasing applications of language\nmodels, it has become crucial to protect these\nmodels from leaking private information. Previ-\nous work has attempted to tackle this challenge\nby training RNN-based language models with\ndifferential privacy guarantees. However, ap-\nplying classical differential privacy to language\nmodels leads to poor model performance as the\nunderlying privacy notion is over-pessimistic\nand provides undifferentiated protection for all\ntokens in the data. Given that the private in-\nformation in natural language is sparse (for ex-\nample, the bulk of an email might not carry\npersonally identifiable information), we pro-\npose a new privacy notion,selective differential\nprivacy, to provide rigorous privacy guarantees\non the sensitive portion of the data to improve\nmodel utility. To realize such a new notion, we\ndevelop a corresponding privacy mechanism,\nSelective-DPSGD, for RNN-based language\nmodels. Besides language modeling, we also\napply the method to a more concrete applica-\ntion – dialog systems. Experiments on both\nlanguage modeling and dialog system build-\ning show that the proposed privacy-preserving\nmechanism achieves better utilities while re-\nmaining safe under various privacy attacks com-\npared to the baselines. The data and code are\nreleased to facilitate future research1.\n1 Introduction\nLanguage models have been widely used in vari-\nous kinds of applications, such as Google Smart\nCompose, Amazon Alexa, and so on. However,\nthese models are often trained on highly sensitive\ndata such as emails and chat logs, while having the\ntendency to memorize the training data unintention-\nally (Carlini et al., 2019, 2020). Therefore, how\nto protect user privacy while preserving the model\nutility has become an increasingly important topic.\nSeveral methods have been developed to protect\nthe data, such as data anonymization, k-anonymity,\n1https://github.com/wyshi/lm_privacy\nFigure 1: “Data anonymization” and training with “No\nDP” cannot provide knobs to adjust the trade-off be-\ntween privacy and utility. Selective-DP improves the\nprivacy-utility trade-off of traditional DP, to get closer to\nthe ideal model with both high privacy and high utility.\nand differential privacy (DP). Among them, DP has\nbecome a dominant privacy notion as it provides\nformal and provable guarantees for people to un-\nderstand the privacy-utility trade-off (Figure 1). It\nworks by carefully randomizing the algorithm so\nthat the model does not rely too much on any single\ndata point. However, traditional DP notion protects\neach data point as a whole regardless of the prop-\nerty of individual attributes inside the data (McMa-\nhan et al., 2018). Training large models with this\noverly pessimistic privacy notion could lead to poor\nmodel performance or even a non-convergent train-\ning process (Kerrigan et al., 2020). Our work is\ninspired by an important observation that in many\nscenarios including language modeling, private in-\nformation is sparse, and not all attributes need to\nbe protected. For example, for the sentence “My\nSSN is 123-45-6789”, only the last token with the\nactual SSN number needs to be protected. But if\nwe protect the entire sentence, we may fail to learn\nthe underlying language pattern well.\nTo solve this problem, we propose a new DP\nnotion, namely selective differential privacy (S-\n2848\nDP), to provide focused protection for sensitive at-\ntributes in a training record and improve model util-\nity. We follow the traditional DP setup, where each\ntraining record is contributed by a different user.\nThe key difference is that we consider only partial\ndimensions of a training record as sensitive, which\nbetter abstracts the language data privacy problems.\nWhether a given dimension in a record is sensitive\nis specified by a user-defined policy function that\nencodes application-specific privacy regulations,\nwhich gives users the freedom to protect any type\nof sensitive information according to the use cases.\nWe also develop a corresponding privacy mecha-\nnism, Selective-DPSGD, for RNN-based language\nmodels under the new notion. Moreover, to pro-\ncess the variable-length sequences, we propose a\nbatching method to group private and public tokens\ntogether and alternatively update them for a more\nefficient implementation of Selective-DPSGD.\nOne important concept to note is that, as shown\nin Figure 1, there is always a trade-off between\nprivacy and utility: data anonymization achieves\nhigh privacy guarantee at the cost of low utility\non private tokens; models trained without DP have\nhigh utility but low privacy. However, for both\ndata anonymization and “no DP”, there is no way\nto tune the trade-off, while DP-related methods\nprovide knobs to adjust the privacy-utility trade-off.\nThis paper proposes S-DP to improve the trade-off\nof canonical DP to get closer to the ideal model\nwith both high-utility and high-privacy.\nWe evaluate S-DP on two tasks, 1) a language\ngeneration task and 2) a more concrete application\nof dialog systems. Besides reporting the model\nutility and theoretical privacy guarantees, we also\nempirically demonstrate their robustness under pop-\nular privacy attacks on language data (Carlini et al.,\n2019, 2020). The experiments suggest that training\nwith Selective-DPSGD improves the model utility\nwhile remaining safe to the attacks.\nOur contributions are as follows. First, we pro-\npose a new selective differential privacy notion\nthat ensures targeted privacy protection for sensi-\ntive attributes, and a corresponding mechanism to\nrealize the new S-DP notion for RNN-based mod-\nels. Second, we propose a dialog dataset for future\nprivacy research. Next, we show both theoretically\nand practically that our models are safe to attacks\nwith improved utilities on both the language gener-\nation task and the dialog system application. More-\noever, we discuss the case of imperfect policy func-\ntion, and compare S-DP with data anonymization\nto show that S-DP achieves better utilities when\nthe policy function is imperfect. We also show\npreliminary results on contextual policy functions\nand Transformer models. In the era of information\nexplosion and large-scale pretraining, protecting\ndata privacy becomes more and more important.\nWith S-DP, we march one step closer towards more\nprivacy-preserving and high-performing language\nmodels and hope to inspire more research in this di-\nrection in the NLP community. Moreover, despite\nour focus on language-related applications in this\npaper, the proposed S-DP notion could be useful\nfor a much broader range of applications where\nonly partial data require privacy protection.\n2 Related Work\nLanguage modeling is a key research problem in\nNLP. However, although language models are of-\nten trained on sensitive data such as emails, most\nrelated studies focus on improving model without\nconsidering privacy, leaving the models vulnera-\nble to attacks. For example, Carlini et al. (2019)\nshowed that it is possible to infer a secret in the\ntraining data by attacking published language mod-\nels. Therefore, it is of great importance to introduce\nprivacy protection mechanisms to the NLP commu-\nnity and train the models in a much safer way.\nDifferential privacy (DP) (Dwork et al., 2014)\nhas been applied to various domains (Cortés et al.,\n2016; Abowd, 2018). Abadi et al. (2016) intro-\nduced DPSGD to train DP-protected deep-learning\nmodels. PATE (Papernot et al., 2018) leveraged\nknowledge distillation to train differentially pri-\nvate models. But DP-protected algorithms suffer\nfrom low utility, so the DP notion often needs to\nbe adjusted according to the applications: Ebadi\net al. (2015) proposed a personalized DP notion to\nprovide different levels of protection for different\nusers; Doudalis et al. (2017) developed one-sided\nDP to protect sensitive users only. We also pro-\npose a new Selective-DP notion to protect only the\nsensitive attributes in a record to improve utility.\nRecently, DP has also been applied to NLP tasks\n(Fernandes et al., 2019; Xu et al., 2020; Hathu-\nrusinghe et al., 2021; Sasada et al., 2021). For\nexample, McMahan et al. (2018) proposed DP-\nFedAvg and DP-FedSGD to train RNN language\nmodels with user-level privacy guarantees. Ade-\nlani et al. (2020) developed a probabilistic text de-\nidentification algorithm with formal privacy guar-\n2849\nantees. Different from existing work that directly\napplied DPSGD and provided undifferentiated pro-\ntection for all training examples, we propose a new\nprivacy notion and a corresponding mechanism to\nprotect the sensitive portion of the data in central-\nized learning. Such a new notion can be easily\nadapted to federated learning settings as well.\nThere is also a line of work that attempts to im-\nprove differentially private deep learning perfor-\nmance via modifying training process (Wang et al.,\n2021, 2019; Thakkar et al., 2019; Lee and Kifer,\n2020) or model architecture (Papernot et al., 2020).\nOur work is complementary to this line of work as\nwe propose a new privacy notion. Particularly, our\nwork can be combined with the aforementioned\nmethods to further improve model utility.\n3 Backgrounds\nWe will introduce language modeling and differen-\ntial privacy as preliminaries in this section.\nLanguage Modeling. Consider a text sequence\nthat consists of multiple tokens, i.e., x =\n(x1,x2,...,x n), where xi is the i-th token. The\ngoal of language modeling is to learn the probabil-\nity of the sequence p(x), which can be factorized\nwith the chain rule as in Equation (1). Given a\ncorpus D= {x1,..., x|D|}, we train a neural net-\nwork (e.g., RNN) parameterized by θto learn p(x)\nby minimizing the negative log-likelihood over D\nwith the loss function in Equation (2).\np(x) =\nn∏\ni=1\np(xi|x<i), (1)\nL(D) = −\n|D|∑\nt=1\nnt∑\ni=1\nlog pθ(xt\ni|xt\n<i) (2)\nDifferential Privacy (DP) (Dwork et al., 2014).\nA differentially private algorithm ensures that its\noutput cannot help much to distinguish whether an\nindividual record is contained in the input dataset.\nIn other words, DP hides the presence of individual\nrecords. The formal definition is as follows.\nDefinition 1. (Differential Privacy). Given a do-\nmain D, any two datasets D,D′⊆D that differ in\nexactly one record are called neighboring datasets.\nA randomized algorithm M: D →Ris (ϵ,δ)-\ndifferential private if for all neighboring datasets\nDand D′and all T ⊆R,\nPr[M(D) ⊆T] ≤eϵPr[M(D′) ⊆T] + δ.\n4 Selective Differential Privacy\nCanonical DP notion treats all records as sensitive.\nPrior work has studied variants of DP notions, such\nas personalized DP (Jorgensen et al., 2015) and one-\nsided DP (Doudalis et al., 2017), to exploit different\nprivacy levels between records. However, existing\nprivacy notions do not allow different attributes in a\ngiven record to have different privacy levels, which\ncould otherwise potentially enable additional util-\nity gains, especially for NLP tasks where private\nattributes are sparse. Hence, we propose a new\nprivacy notion–selective differential privacy–to dis-\ntinguish between private and non-private attributes\ninside one data point with a policy function and\nprotect sensitive part of one data point.\nDefinition 2. (Policy Function). A policy function\nF : τ →{0,1}nr denotes which attributes of a\nrecord r ∈τ are sensitive ( F(r)i = 0 ) or non-\nsensitive (F(r)i = 1), where nr is the number of\nattributes in r. Note that nr depends on the record\nand is not a fixed number.\nIn practice, users have the freedom to define the\npolicy function to encode specific privacy regu-\nlation and protect any sensitive attributes accord-\ning to the applications. The protected sensitive\nattribute types are unlimited, can be entities (e.g.,\nname, emails, etc), contextual (e.g., health-related\ninformation, speaking style, etc), and so on. For\nexample, users can design a conservative policy\nfunction that protects selected complete sentences\nif necessary. The form of the policy function is also\nunlimited, and could be neural networks, regex,\nand so on. Please refer to Section 6 for contextual\npolicy functions.\nIn the case of language modeling, each record is\na text sequence x, each attribute is a token xi in x\nand F(x) is a bit vector indicating which tokens\ncontain private information. We define neighboring\ndatasets under our new privacy notion as follows.\nDefinition 3. (F-Neighbors). D,D′ are two\ndatasets and F is a policy function. D′ is a F-\nneighbor of D if and only if ∃r ∈D s.t., F(r)\ncontains at least one private attribute, ∃r′ ∈D′\ns.t., F(r) and F(r′) differ by at least one private\nattribute, and D′ = D\\{r}∪{ r′}. We denote\nD′∈NF(D).\nUnder this definition, the dataset containing “My\nID is 123” and the dataset containing “My ID is\n456” are neighbors; but the dataset with “Hello\n2850\nthere” and the dataset with “Hi there” are not neigh-\nbors since they do not contain private information.\nDefinition 4. (Selective Differential Privacy).\nGiven a policy function F, a randomized algorithm\nM: D→R satisfies (F,ϵ,δ )-selective differen-\ntial privacy if for ∀D,D′∈NF(D), and ∀T ⊆R,\nPr[M(D) ⊆T] ≤eϵPr[M(D′) ⊆T] + δ.\nEssentially, S-DP also provides an indistin-\nguishability property similar to canonical DP, but\nonly for the sensitive attributes in a record. S-DP\ndoes not constrain the information leakage of non-\nsensitive attributes as long as the privacy of the\nsensitive attributes is preserved. Thus, S-DP pro-\ntects privacy for sensitive attributes in the worst\ncase (i.e., the attacker may have knowledge about\neverything except the targeted sensitive attribute.)\n4.1 Selective Privacy Mechanism\nWith the new S-DP notion, the next step is to de-\nvelop a corresponding privacy mechanism to train\nmodels that realize the new notion. Privacy mecha-\nnisms usually work by adding noise to the models\nto protect the data, such as Laplace mechanism\n(Laplace noise) and Gaussian mechanism (Gaus-\nsian noise). Abadi et al. (2016) proposed DPSGD\nthat adds Gaussian noise to the gradients and ap-\nplies stochastic gradient descent (SGD) to train\nprivate deep learning models. In this work, we\ndevelop Selective-DPSGD, shown in Figure 2 and\nAlgorithm 1, to train RNN-based language models\nthat achieve S-DP. The basic idea is to first deter-\nmine the private attributes with the policy function,\nthen decide which model variables are related to\nthe private attributes, and finally apply regular SGD\non non-private variables, and DPSGD (Abadi et al.,\n2016) on the private variables. We choose RNNs\nbecause they are widely used in industry, e.g., Ra-\nmaswamy et al. (2020) discussed how to train pri-\nvate production language models with RNNs.\nWe need to first decide the variables related to\nthe private tokens. RNN uses a hidden state hi to\nencode the context, and outputs a distribution pi\nover a vocabulary set V, as shown in Equation (3).\nIf xi is private, then hi, pi, and Li are all private;\nbesides, to calculate Li−1, we need to access the\nground truth next token xi, so Li−1 is also private.\nThe private variables are all in red in Figure 2.\nhi = RNN(hi−1,xi) (3)\npi = pθ(V|x<i) = Softmax(g(hi)) (4)\nLi = −log pθ(xi+1|x<i+1) (5)\nFigure 2: All private variables are in red. We apply\nregular SGD on non-private variables and DPSGD on\nprivate variables in Selective-DPSGD.\nAlgorithm 1 outlines the steps in Selective-\nDPSGD. Given a dataset D, we apply a policy\nfunction F to obtain a bit matrix P = F(D) that\nindicates which tokens are private. At each step,\nwe take a random batch B, and use P to split B\ninto a sequence of non-private and private tuples\n{(Bnp,i,Bp,i)}; then we apply SGD (regular up-\ndate) on Bnp,i and DPSGD (private update) onBp,i\nalternatively, to update and protect privacy. Note\nthat besides noise in the gradients, we also clip and\nadd noise to the hidden state hi if it is private. The\nreason is that in RNN, if xi is private, hi also con-\ntains private information (as shown above), and is\ndirectly passed to the next regular update step and\ncannot be protected by the noise in the gradients.\nSo it is important to add noise to protect the private\ninformation in hi. Since DPSGD adds noise to the\ngradients, Land pi used to calculate the loss are\nprotected by the noise in the gradients. In this way,\nall private variables are protected.\nPrivacy Guarantee. In Section A.3, we prove\nthat the composition of the series of noise-adding\noperations ensures S-DP for Selective-DPSGD.\n5 Experiments\nWe conduct our experiments on two datasets: 1) a\ntraditional text corpus for language modeling, and\n2) a dialog dataset for a more concrete application\nof dialog systems. Below are the dataset details.\nWikiText-2. To minimize real-world harm, we\nchoose the already-public WikiText-2 (Merity et al.,\n2017). It contains articles from Wikipedia with\npotentially sensitive information, and is a classical\ndataset for language modeling. For simplicity, we\ntreat all the digits as privacy information. So the\npolicy function F is a digit detector: if the token is\na digit, F will output 0, otherwise, 1.\nCUSTOMER SIM. With the emergence of virtual\nassistants, more and more private information is\nbeing exchanged during daily interactions. So we\n2851\nAlgorithm 1 Selective-DPSGD\n1: Input: Dataset Dwith N examples, policy function F,\nprivacy bit matrix P = F(D), max sequence length K,\nloss function L(θ).\nParameters: learning rate η, noise multiplier σ, gradient\nnorm bound C, group size L.\n2: for t=1,2,... do\n3: Take a random batch B of max sequence length K,\nwith sampling probability L/N\n4: Using P, split B into a sequence of non-private and\nprivate tuples {(Bnp,i,Bp,i)}\n5: Initialize h = ⃗0\n6: for i=1,2,... do\n7: 1) Regular update\n8: L,h = Model(Bnp,i, h)\n9: θ←θ−η∇θL(θ)\n10: 2) Private update\n11: L,h = Model(Bp,i, h)\n12: Calculate sample gradient\n13: For each xj ∈ Bp,i, compute g(xj) ←\n∇θL(θ,xj)\n14: Clip gradient\n15: g(xj) ←g(xj)/max(1,∥g∥2\nC )\n16: Add Noise\n17: g(xj) ← 1\n|Bp,i| (∑\njg(xj) +σC ·N(0,I))\n18: Descent\n19: θ←θ−η∇θL(θ)\n20: Clip hidden states\n21: h(xj) ←h(xj)/max(1,∥h∥2\nC )\n22: Add Noise\n23: h(xj) ←h(xj) +σC ·N(0,I)\n24: end for\n25: end for\nalso apply S-DP to build dialog systems in the cus-\ntomer service domain. Using real dialog transcripts\nmay lead to real-world harm, so we simulate a di-\nalog dataset, CUSTOMER SIM, with synthetic user\ninformation. The dialogs are simulated with fixed\nagendas and template utterances (Zhao and Eske-\nnazi, 2018). We treat user name, address, phone\nnumber, order, and tracking number as sensitive in-\nformation, and use regex to build a policy function\nto detect them. Table 1 shows one example dialog.\nNote that although we use digits and names as\nrunning examples for sensitive information, S-DP\ncan protect any sensitive attributes specified by the\npolicy function. Building better policy functions is\northogonal to S-DP, and thus beyond the scope of\nthis paper. Any improvements on policy functions\nare compatible with S-DP to achieve better results.\nModel training details. We use one-layer LSTMs\nwith an embedding size of 200 and a hidden size\nof 200, and a BPE tokenizer (Sennrich et al., 2016)\nto avoid information leakage from the tokenizer:\nwith BPE, a secret “1234” will be split into multi-\nple tokens, e.g., “12” and “34”, while traditional\ntokenizer will release “1234” in the dictionary. All\nCUSTOMERSIM\nRole Utterance\nSYS Hello, I am the customer support bot. What can I do\nfor you?\nUSR Hello robot. Could you please help me track my\npackage?\nSYS Please provide your full name.\nUSR Sure,BettySims.\nSYS Could you please confirm your shipping address?\nUSR Yea sure,2241FitzgeraldViaductBrownview,OK\n28304.\nSYS Track your order using your tracking number,\nFH6F6GMMF4. Are you happy about my answer?\nUSR That’s it.\nTable 1: An example dialog in CUSTOMER SIM.\nprivate states (hidden, cell states) in the LSTMs are\nprotected.\nBaselines. We have two baselines, one without\nDP (“No-DP”), and the other trained with DPSGD\n(“DPSGD”). We refer to our models trained with\nS-DPSGD as “S-DPSGD”. “No-DP” is simply an\nLSTM optimized with a regular SGD and a start-\ning learning rate (lr) of 20. The learning rate\nwas annealed and decreased as training proceeded.\n“DPSGD” is optimized with DPSGD and a starting\nlearning rate of 0.05. All the models are trained five\ntimes to reduce randomness, and the parameters\nare tuned on the validation set. We compare with\nDPSGD because it’s the backbone of most of exist-\ning DP learning algorithms. Existing modifications\nof DPSGD are mainly focused on optimization al-\ngorithms and objectives, thus are compatible with\nour work that tailors the privacy notions to realistic\nprivacy needs in the NLP context.\n5.1 Evaluation\nWe evaluate both the language models’ utilities\nand privacy protection levels. We use perplexity\n(PPL) and the top-1 next word prediction accu-\nracy (AccT1) to measure model utility. To measure\nprivacy protection levels, besides reporting the the-\noretical privacy budget ϵand δ, we also perform\nvarious practical attacks on the trained models and\nreport how successful the attacks are against dif-\nferent techniques. We compare the performance\nof our proposed privacy-preserving learning tech-\nnique and the baselines in terms of the privacy-\nutility trade-off. Specifically, we compare the util-\nity between different techniques at a given privacy\nprotection level, or vice versa.\n2852\nWikiText-2 CUSTOMERSIM\nModel PPL AccT1 σ C ϵ δ PPL AccT1 σ C ϵ δ\nLSTM, No-DP60.98±0.48 0.36±0.00 - - - - 3.06±0.01 0.75±0.00 - - - -\nDPSGD 305.86±3.00 0.27±0.00 0.50 0.10 4.89 8e-5 11.82±0.76 0.70±0.01 0.60 0.01 2.51 8e-5\nS-DPSGD 160.04±4.86 0.31±0.00 0.50 1e-3 4.91 8e-5 10.42±0.91 0.69±0.01 0.70 5e-3 2.74 8e-5\nTable 2: Model utility and privacy guarantee on WikiText-2 (left) andCUSTOMER SIM (right). PPL: Perplexity on\nthe test set. AccT1: Top-1 next word prediction accuracy. σ: Noise multiplier in the Gaussian noise. C: Clipping\nthreshold. ϵ,δ: Privacy guarantee in (ϵ, δ)-privacy.\nFigure 3: Privacy-utility trade-off, canary insertion attack and membership inference attack on WikiText-2.\n5.1.1 Attack Details\nWe perform two types of attacks: 1) canary inser-\ntion and 2) membership inference.\nCanary insertion is proposed by Carlini et al.\n(2019). It first inserts random sequences called\ncanaries into the training dataset, then trains the\nmodel, and finally calculates the following expo-\nsure for the inserted canaries to measure if the\nmodel will unintentionally memorize these ca-\nnaries. Canaries are of a specific format, e.g.,\ns=“The number is ”, where are filled\nwith random values from a randomness space R\nsuch as a space of digits from 0 to 9. To obtain\nthe exposure, we enumerate all possible sequences\nin the specified form, and calculate the negative\nlog-rank with an additional constant, as shown be-\nlow. Lower exposure indicates the model is more\nprivate. In our setting, we insert the secret “My ID\nis 341752” into the training data for 10 times to\nmake the differences between models more salient.\nDefinition 5. Given a canary s[r], a model with\nparameters θ, and the randomness space R, the\nexposure of s[r] is\nexposureθ = log2 |R|−log2 rankθ(s[r])\nMembership Inference is a widely used attack\nmethod that identifies if a given sample is a mem-\nber of the training dataset. Lower inference accu-\nracy means that it is harder to infer a member from\nthe model and thus the model is safer. Carlini et al.\n(2020) proposed an advance membership inference\nattack for language models. The basic idea is to\ncalculate the given samples’ perplexities under the\nmodel, rank them and choose the ones with the low-\nest perplexities (highest likelihood by the model).\nIn our experiments, we randomly select 500 pro-\ntected secrets from the training set, and randomly\ngenerate 500 samples of similar format, to form a\ndataset for the membership inference attack, so a\nrandom guess would give us an accuracy of 50%.\nFor WikiText-2, the secrets are digit sequences; for\nCUSTOMER SIM, customer names are the secrets.\n5.2 WikiText-2 Results\nModel utility and privacy guarantee. The left\npart of Table 2 shows different models’ utilities and\nprivacy guarantees on WikiText-2 and Figure 3(a)\nshows the privacy-utility trade-off, where utility\nis represented by the validation perplexity (lower\nPPL=higher utility) and the privacy budget is rep-\nresented by the epsilon (smaller ϵ=more privacy).\nAlthough the definitions of ϵ in DPSGD and S-\nDPSGD are different, the ϵin both cases provides a\ntight theoretical upper bound on how much the sen-\nsitive attributes are leaked through the trained mod-\nels. Hence, the ϵassociated with two algorithms\nare comparable. Given a fixed privacy budget ϵ,\nwe want to achieve a higher model utility. Because\n2853\nFigure 4: Privacy-utility trade-off, canary insertion attack and membership inference attack on CUSTOMER SIM.\nthe privacy budget ϵ= +∞for the No-DP model,\n“No-DP” is represented by the vertical black line\non the far right in Figure 3(a), and it achieves the\nbest average PPL of 60.98 on the test set. The or-\nange line is our S-DPSGD model, and it achieves\nthe second-best average test PPL of 160.04 with\nϵ= 4.91. With a similar ϵ= 4.89, DPSGD has the\nworst average test PPL of 305.86, much worse than\nS-DP because canonical DP notion protects the\nwhole data and is over-pessimistic (see Section A.4\nfor models with different parameters). The gray\nline is for DPSGD with a smaller σ = 0 .25, a\nconvergent PPL of 266.6 and a final ϵ = 132.73.\nCompared to DPSGD with σ= 0.25, it achieves a\nbetter PPL but with a much higher cost of privacy\nleakage (larger ϵ). But with S-DPSGD, we can also\nachieve lower PPL without hurting privacy.\nAttack results. Figure 3(b) and 3(c) show the ca-\nnary insertion attack and membership inference\nattack results on WikiText-2. The x-axis is the\nmodels’ utilities measured by validation PPL, and\nthe y-axis is the exposure and membership infer-\nence accuracy indicating the success level of the\nattacks. Lower exposure and lower accuracy in-\ndicate a safer model. We want to see at a given\nrobustness level to the attacks, which models can\nachieve lower perplexity, i.e., higher utility.\nFor canary insertion attack, although “No-DP”\nachieves lower perplexity, its exposure can go up to\n20, indicating that the inserted canary could be eas-\nily revealed by the attackers. If we compare No-DP\nwith S-DPSGD with similar utilities, S-DPSGD is\nalways below No-DP, meaning S-DPSGD achieves\nmuch smaller exposure, and hence a safer model\nwith similar utility. Comparing DPSGD and S-\nDPSGD, we find that S-DPSGD achieves much\nbetter model utility at a given exposure.\nFor membership inference attack, we draw a hor-\nizontal line of 0.5 to show the random guess perfor-\nmance. Again, S-DPSGD is always below No-DP,\nshowing that with similar utilities, models trained\nwith S-DPSGD are safer than No-DP under the\nmembership inference attack. As mentioned ear-\nlier, DPSGD with σ= 0.5 (green) and S-DPSGD\n(orange) have similar privacy budget (ϵ=4.89 and\n4.91 respectively). Comparing these two, we see\nthat given a similar privacy budget, S-DPSGD con-\nverges to a much lower perplexity while remaining\nsafe to the attack and thus achieves a wider range\nfor the privacy-utility trade-off tuning. We also\nobserve that for “No-DP”, the inference accuracy\ncan go up to 90%, suggesting that language models\nwithout DP protection are vulnerable to attacks.\n5.3 C USTOMER SIM Results\nThis section shows the results on CUSTOMER SIM.\nModel utility and privacy guarantee. The right\npart of Table 2 and Figure 4(a) show the privacy-\nutility trade-off for CUSTOMER SIM. Because the\ndialogs are simulated with templates and relatively\nsimple, the perplexity can be as low as 3.06 for\nNo-DP. S-DPSGD still achieves better perplexity\nthan DPSGD (10.42 vs. 11.82) with similar ϵ, but\nthe gap is smaller compared to WikiText-2 because\nthere are more sensitive tokens in CUSTOMER SIM\n(18.3%) than WikiText-2 (2.8%), and the advantage\nof protecting selective tokens only is not as big.\nAttack results. Figure 4(b) and 4(c) show the\nresults of the canary insertion and membership in-\nference attack on CUSTOMER SIM respectively.\nFor canary insertion, we observe that given the\nsame utility, S-DPSGD achieves lower exposure\nthan No-DP and DPSGD. Although the improve-\nment seems small on absolute values, we should\nnote that exposure is on log-scale, so the improve-\nment on relative rank is also on log-scale (e.g., for\nexposure=1.2, the rank of the canary is 429881; for\nexposure=0.2, the rank is 858215, roughly twice).\n2854\nThe membership inference accuracy is only a\nlittle better than random guess probability (60+%),\nso it is not successful on CUSTOMER SIM. One\npotential reason could be that customer names only\nappear once in each dialog and can be very similar\nto each other (e.g., Emily, Emilya, and Emilyah).\nWe leave it as future work to develop better mem-\nbership inference attacks towards similar secrets.\nUnder the failed attack, S-DPSGD is still better\nthan No-DP. It is not feasible to compare S-DPSGD\nand DPSGD as both are close to random guess.\n5.4 Data Anonymization and Selective-DP\nData anonymization (or data de-identification) has\nbeen widely used to protect data privacy, where\nthe sensitive tokens are masked with special tokens\nsuch as “<num>”. Both data anonymization and\nS-DP target protection towards sensitive attributes,\nand rely on a policy function to detect the private\ninformation. However, they are different in that\ndata anonymization masks the sensitive attributes\ncompletely so nothing can be learned from them,\nwhile S-DP noises the private portion and provides\na tunable way to adjust the privacy-utility trade-off,\nevidenced by experiments in Section 5.4.1.\nOne common problem for both methods is that\nthe policy function is not guaranteed to be perfect\nand can miss to detect some sensitive information.\nSo we also compare their performance when the\npolicy function is imperfect in Section 5.4.2.\n5.4.1 S-DP achieves better utility\nFigure 5: Perplexity on private tokens over ϵfor data\nanonymization and S-DPSGD.\nWe mask all the detected digits by “<num>” to\ntrain a data anonymization baseline on WikiText-2,\ncalculate the perplexity on the private tokens, and\npresent the result in Figure 5. The x-axis is the\nprivacy budgetϵ, and the y-axis is the private-token\nperplexity. For data anonymization, the dots are all\non the far right because ϵ=+∞. The first observa-\ntion is that since data anonymization simply masks\nthe sensitive tokens, it fails to learn anything about\nthem, resulting in a much worse PPL on private\ntokens. This makes S-DP a good alternative to data\nanonymization because S-DP can still learn certain\npatterns from the noised sensitive tokens. Also,\nFigure 5 shows that for S-DP, there is a trade-off\nbetween ϵ(privacy) and private-token PPL (utility),\nso we could tune ϵto achieve a better private-token\nPPL, while for data anonymization, there is no\nway to tune the parameters for better model utili-\nties. More concretely, with proper ϵand δ, S-DP\nmight learn the structure of sensitive attributes (e.g.,\nXXX-XX-XXXX for SSN) without knowing the\nexact value, or even learn the distribution of values\nfor each digit, and such knowledge could be useful\nfor data analytics. But for data anonymization, the\nmodel either sees the digit or doesn’t see it, so there\nis no knob to tune the privacy-utility trade-off.\n5.4.2 Imperfect Policy Function\nFigure 6: Canary insertion attack for data anonymiza-\ntion and S-DPSGD when missing the canary.\nNow we discuss the performance of both meth-\nods when the policy function is imperfect. We still\nuse WikiText-2 with the secret “My ID is 341752”.\nThe policy function fails to detect “341752” as a\nsecret. For data anonymization, all the detected\ndigits are masked by “<num>”; for S-DP, we apply\nS-DPSGD to noise the detected digits.\nFigure 6 shows the exposure of the missed secret\n341752. When the perplexity gets lower, the model\nbecomes better at remembering the details of the\ntraining data, so the exposure of both models be-\ncomes high. But when the perplexity is around the\nmiddle area, S-DP has lower exposure than data\nanonymization, meaning it’s safer to attacks.\nNote that the risk with imperfect policy functions\nis common to many privacy-preserving techniques,\nand how to build better policy functions is orthog-\n2855\nonal to this work on S-DP, and thus beyond the\nscope of this paper. Improvements on policy func-\ntions (better sensitive information detection) are\ncompatible with S-DP and can be used to further\nimprove the results. We are also actively working\non this topic in parallel to the S-DP work.\n6 Preliminary Results on Contextual\nPolicy Function and Transformer\nModel Policy function Portionϵ PPL\nGPT2 + no DP- - - 20.47\nDPSGD - - 2.58 27.05\nRedacted Noncontextual:All entities 16.40%- 24.30\nS-DP 2.58 22.56\nRedacted Contextual: entities, sub,obj, propon, pron 34.80%- 38.66\nS-DP 2.48 25.61\nTable 3: Preliminary results on different policy functions\nand Transformers.\nIn this section, we present the preliminary results\non different policy functions and large Transformer\nmodels (Vaswani et al., 2017) on WikiText-2.\nWe design two policy functions: one is non-\ncontextual and protects all the 18 named entities de-\ntected by spacy (Honnibal and Montani, 2017) such\nas person, date, locations, etc (16.4% tokens)2; the\nother one is contextual that protects all the entities\nplus subjects, objects, proper noun and pronouns\nof all the sentences (34.8% tokens). We use these\ntwo policy functions to redact the WikiText-2 D\nand obtain a redacted version D′. We first fine-\ntune a GPT2-small model (Radford et al., 2019)\non D′(denoted as “redacted model”), and then fur-\nther fine-tune this redacted model on the original\nD (Shi et al., 2022). The results are in Table 3.\nThe redacted models trained on the redacted data\nD′achieve 24.30 and 38.66 in perplexity for the\ntwo policy functions respectively. If we fine-tune\nthese two models on the original private data D\nwith DPSGD, we can further improve the perplex-\nity to 22.56 and 25.61, while the state-of-the-art DP\nlanguage models only achieve 27.05 with similar\nprivacy budget. These results show that our S-DP\nnotion is promising in boosting utility of privacy-\npreserving language models even if one-third of\nthe tokens are considered sensitive.\n2The full list of entities is available here https:\n//spacy.io/usage/linguistic-features#\nnamed-entities\n7 Conclusions\nTo conclude, we develop a new privacy notion,\nselective differential privacy (S-DP), to improve\nmodel utility while providing rigorous privacy guar-\nantees for the sensitive portion of the data. We also\ndevelop a privacy mechanism, Selective-DPSGD,\nto achieve the new S-DP notion for RNNs. We ex-\nperiment with WikiText-2 and a synthetic customer\nservice dialog dataset. Results on both tasks show\nthat models trained with S-DP achieve better utili-\nties than traditional DP, and are more robust under\nvarious attacks than models without DP protection.\nWith S-DP, we march one step closer towards safer\nand better language models and hope to inspire\nmore related research. Moreover, S-DP could be\napplied to domains beyond NLP where only partial\ndata require protection, such as image recognition.\nPlease see Section A.1 for ethical consideration.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security,\npages 308–318.\nJohn M Abowd. 2018. The us census bureau adopts\ndifferential privacy. In Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pages 2867–2867.\nDavid Ifeoluwa Adelani, Ali Davody, Thomas Klein-\nbauer, and Dietrich Klakow. 2020. Privacy guaran-\ntees for de-identifying text transformations. INTER-\nSPEECH.\nBorja Balle, Gilles Barthe, and Marco Gaboardi. 2018.\nPrivacy amplification by subsampling: Tight analyses\nvia couplings and divergences. Advances in Neural\nInformation Processing Systems, 31.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neural\nnetworks. In 28th {USENIX}Security Symposium\n({USENIX}Security 19), pages 267–284.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ul-\nfar Erlingsson, et al. 2020. Extracting training\ndata from large language models. arXiv preprint\narXiv:2012.07805.\nJorge Cortés, Geir E Dullerud, Shuo Han, Jerome Le Ny,\nSayan Mitra, and George J Pappas. 2016. Differ-\nential privacy in control and network systems. In\n2856\n2016 IEEE 55th Conference on Decision and Control\n(CDC), pages 4252–4272. IEEE.\nStelios Doudalis, Ios Kotsogiannis, Samuel Haney, Ash-\nwin Machanavajjhala, and Sharad Mehrotra. 2017.\nOne-sided differential privacy. Proceedings of the\nVLDB Endowment.\nCynthia Dwork, Aaron Roth, et al. 2014. The algorith-\nmic foundations of differential privacy. Foundations\nand Trends in Theoretical Computer Science , 9(3-\n4):211–407.\nHamid Ebadi, David Sands, and Gerardo Schneider.\n2015. Differential privacy: Now it’s getting personal.\nAcm Sigplan Notices, 50(1):69–81.\nNatasha Fernandes, Mark Dras, and Annabelle McIver.\n2019. Generalised differential privacy for text docu-\nment processing. In International Conference on\nPrinciples of Security and Trust , pages 123–148.\nSpringer, Cham.\nRajitha Hathurusinghe, Isar Nejadgholi, and Miodrag\nBolic. 2021. A privacy-preserving approach to ex-\ntraction of personal information through automatic\nannotation and federated learning. arXiv preprint\narXiv:2105.09198.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear.\nZach Jorgensen, Ting Yu, and Graham Cormode. 2015.\nConservative or liberal? personalized differential\nprivacy. In 2015 IEEE 31St international conference\non data engineering, pages 1023–1034. IEEE.\nGavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020.\nDifferentially private language models benefit from\npublic pre-training. In Proceedings of the Second\nWorkshop on Privacy in NLP, pages 39–45.\nJaewoo Lee and Daniel Kifer. 2020. Differentially pri-\nvate deep learning with direct feedback alignment.\narXiv preprint arXiv:2010.03701.\nH Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2018. Learning differentially private\nrecurrent language models. In International Confer-\nence on Learning Representations.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. ICLR.\nIlya Mironov. 2017. Rényi differential privacy. In 2017\nIEEE 30th Computer Security Foundations Sympo-\nsium (CSF), pages 263–275. IEEE.\nNicolas Papernot, Shuang Song, Ilya Mironov, Ananth\nRaghunathan, Kunal Talwar, and Úlfar Erlingsson.\n2018. Scalable private learning with pate. ICLR.\nNicolas Papernot, Abhradeep Thakurta, Shuang Song,\nSteve Chien, and Úlfar Erlingsson. 2020. Tempered\nsigmoid activations for deep learning with differential\nprivacy. CoRR, abs/2007.14191.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nSwaroop Ramaswamy, Om Thakkar, Rajiv Mathews,\nGalen Andrew, H Brendan McMahan, and Françoise\nBeaufays. 2020. Training production language mod-\nels without memorizing user data. arXiv preprint\narXiv:2009.10031.\nTaisho Sasada, Masataka Kawai, Yuzo Taenaka,\nDoudou Fall, and Youki Kadobayashi. 2021.\nDifferentially-private text generation via text prepro-\ncessing to reduce utility loss. In 2021 International\nConference on Artificial Intelligence in Information\nand Communication (ICAIIC), pages 042–047. IEEE.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. ACL.\nWeiyan Shi, Si Chen, Chiyuan Zhang, Ruoxi Jia, and\nZhou Yu. 2022. Just fine-tune twice: Selective dif-\nferential privacy for large language models. arXiv\npreprint arXiv:2204.07667.\nOm Thakkar, Galen Andrew, and H Brendan McMahan.\n2019. Differentially private learning with adaptive\nclipping. arXiv preprint arXiv:1905.03871.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. NIPS.\nBao Wang, Quanquan Gu, March Boedihardjo, Farzin\nBarekat, and Stanley J. Osher. 2019. DP-LSSGD: A\nstochastic optimization method to lift the utility in\nprivacy-preserving ERM. CoRR, abs/1906.12056.\nWenxiao Wang, Tianhao Wang, Lun Wang, Nanqing\nLuo, Pan Zhou, Dawn Song, and Ruoxi Jia. 2021.\nDplis: Boosting utility of differentially private deep\nlearning via randomized smoothing. arXiv preprint\narXiv:2103.01496.\nZekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan,\nand Nathanael Teissier. 2020. A differentially pri-\nvate text perturbation method using regularized ma-\nhalanobis metric. In Proceedings of the Second Work-\nshop on Privacy in NLP, pages 7–17.\nTiancheng Zhao and Maxine Eskenazi. 2018. Zero-shot\ndialog generation with cross-domain latent actions.\nSIGDIAL.\n2857\nA Appendix\nA.1 Ethical Consideration\nData Usage. To minimize real-world harm, we\nchoose WikiText-2 since it is already public and\nwidely used, and synthesize the dialogs as well\nas the personal information in the CUSTOMER SIM\ndatasets in our experiments. For future research, we\nplan to continue using public or synthetic datasets\nto prevent real-world data leakage.\nApplication. Our work addresses the problem of\ndata privacy protection and can be applied in dif-\nferent applications. The attacks used in our study\nare well-known standard attacks tailored for our\nspecific tasks, so it’s hard to generalize and misuse\nthem to attack other language models. We will re-\nlease the code so that people can have access to the\nvarious algorithms and protect their own data.\nA.2 Limitations\nThere are many spaces for improvements for this\nS-DP work. For instance, when the policy func-\ntion fails to detect sensitive attributes, their privacy\nmay not be guaranteed, and therefore, we plan to\ndevelop better policy functions and employ privacy\namplification (Balle et al., 2018). Also, besides\nexplicit private information like names, we plan\nto protect sensitive context such as “I have two\nkids”, as these can often happen causally in dialogs\nbut still reveal personal status. In Section 6 we\nshow some preliminary results on protecting con-\ntexts and plan to further refine the contextual policy\nfunctions.\nA.3 Privacy Analysis\nWe analyze the private guarantees of Selective-\nDPSGD in this section.\nFor any given dataset D, let Di,j denote the jth\nattribute of the i-th record. We abstract the gradi-\nent update and hidden state into a query function\nf(x,w) which takes training data xand auxiliary\ninformation was input. We introduce was an ad-\nditional input to f to model the dependence of the\ngradient update and hidden state on the model pa-\nrameters at the previous rounds. We define the\nfollowing two types of queries on the dataset.\n• Type-1 query: the inputxto fconsists of only\nprivate attributes with respect to the policy\nfunction F\n• Type-2 query: the input x to f consists of\nonly non-private attributes with respect to the\npolicy function F\nSince S-DP only hides the presence of private\nattributes, type-2 query does not incur privacy loss.\nThe following theorem shows that if a type-1\nquery has the property that its output is bounded,\nthen for arbitrary auxiliary input, adding Gaussian\nnoise into the query can provide DP. The reason\nwhy we consider such queries is that clipped gradi-\nent and hidden state can be modeled as such queries.\nThe reason for which we want to analyze DP guar-\nantees under arbitrary auxiliary inputs is that at\nany given round, the model parameters resulting\nfrom previous rounds could be arbitrarily different.\nThis is because the non-sensitive part of two F-\nneighboring datasets could be arbitrarily different.\nTheorem 1. Assume that maxx,w∥g(x,w)∥≤ C.\nThen, for any arbitrary w, adding Gaussian noise\n∆ = N(0,σ2) proportional to Cinto gcan ensure\n(ϵ,δ)-DP where ϵ,δ depends on C and σ. More\nformally, for all neighboring datasets xand x′and\nall w,w′,\nP[g(x,w) + ∆ = r]\nP[g(x′,w′) + ∆ = r] ≤eϵ w.p. 1 −δ (6)\nThe proof follows directly from the classic\nproof for DP guarantees of the Gaussian mecha-\nnism (Mironov, 2017; Dwork et al., 2014) by notic-\ning the sensitivity of f is bounded by C.\nThe regular updates in Algorithm 1 take as in-\nput non-private data Bnp,i. Hence, they are type-2\nqueries and do not incur extra privacy loss. The pri-\nvate updates in Algorithm 1 (i.e., gradient and hid-\nden states) depend on private attributes and model\nparameters from previous rounds, and thus belong\nto the type-1 query. Moreover, they satisfy the\nbounded norm assumption in Theorem 1. We call\nthe resulting query of adding Gaussian noise into a\ntype-1 query with bounded norm property a noisy\ntype-1 query. Overall, Algorithm 1 is essentially\nthe composition of multiple type-1 and noisy type-2\nqueries. In the following, we will present a general\nresult for the privacy guarantees resulting from the\ncomposition.\nTheorem 2. Let f be the composition of kqueries:\nf1,...,f k, which are either noisy type-1 or type-2\nqueries. Given a policy function F, let fp denote\nthe set of noisy type-1 queries. Let fnp denote the\nset of type-2 queries. Then, if fp is (ϵ,δ)-DP ,f is\n(F,ϵ,δ )-S-DP .\n2858\nProof. Consider two selective F-neighboring\ndatasets x and x′. Let xi and x′\ni be the subset\nof data utilized by fi. xi contains only private\nattributes when fi is type-1 and contains only non-\nprivate attributes when fi is noisy type-2. By the\nneighboring relation between xand x′, xi and x′\ni\nare also selective F-neighbors when fi is a type-1\nquery. In addition to the dataset, fi takes the output\nof all the previous queries. For a fixed outcome\n(y1,...,y k) of f, we have\nP[f1(x1,w1) = y1,...,f k(xk,wk) = yk]\nP[f1(x′\n1,w′\n1) = y1,...,f k(x′\nk,w′\nk) = yk] (7)\n=\n∏\nfi∈fp\nfi(xi,wi)\nfi(x′\ni,w′\ni) (8)\n≤eϵ w.p. 1 −δ (9)\nas desired.\nThe equality in the second line is due to the fact\nthat fnp does not incur privacy loss and the inde-\npendence of randomness of each query given the\noutput of all the previous queries. The inequality\nin the third line is due to the assumption that fp is\n(ϵ,δ)-DP with arbitrary auxiliary input.\nInstantiating the type-1 and type-2 queries in\nTheorem 2 with the regular and private updates de-\nfined in Algorithm 1 yields the privacy guarantees\nfor Algorithm 1. Theorem 2 provides a conve-\nnient way of calculating the S-DP guarantees for\nAlgorithm 1: one can apply off-the-shelf privacy\ncomposition tools to calculate the overall DP guar-\nantees for all the private updates and then the entire\nalgorithm satisfies S-DP with the same values of\nϵ and δ. Specifically, in this paper, we leverage\nmoment accountant (Abadi et al., 2016) to calcu-\nlate the DP guarantees for the composition of all\nprivacy queries.\nA.4 Models with Different Parameters\nWe plot the performances of models with differ-\nent parameters in Figure 7. We find that fixing the\nnoise multiplier σ= 0.5, the clipping threshold C\nhas a big impact on the performance, and it cannot\nbe too big (0.01) or too small (5e-6); if we fix C\nand change σfrom 0.5 to 0.1, the perplexity will be\nlower but at a huge cost of the privacy budgetσ. As\nexpected, there is always a trade-off between the\nutility and the privacy spent, so we choose a bal-\nancing point with a reasonable utility and privacy\nguarantee (σ = 0 .5 and C =1e-3) for the main\nexperiments.\nFigure 7: Validation perplexity over epochs on\nWikiText-2 for models with different parameters.\nA.5 Membership Inference on\nCUSTOMER SIM\nFigure 8: Original membership inference results on\nCUSTOMER SIM. The best inference accuracy is around\n58%.\nThe original membership inference attack\ndoesn’t achieve good results on CUSTOMER SIM.\nFigure 8 shows the original membership inference\nresult on CUSTOMER SIM. The best inference ac-\ncuracy is around 58%. So we employ a more ad-\nvanced version, where we first perform the attack\non 1000 names, and then pick the best-predicted\nand worst-predicted names to form a new subset of\n300 names to perform the attack again. But even\nfor the advanced version, the inference accuracy is\nonly a little better than random guess probability\n(60+%), so this attack is not successful on CUS-\nTOMER SIM.\n2859",
  "topic": "Differential privacy",
  "concepts": [
    {
      "name": "Differential privacy",
      "score": 0.9174131751060486
    },
    {
      "name": "Computer science",
      "score": 0.8659447431564331
    },
    {
      "name": "Language model",
      "score": 0.7014135718345642
    },
    {
      "name": "Dialog box",
      "score": 0.6003594994544983
    },
    {
      "name": "Information privacy",
      "score": 0.5428935885429382
    },
    {
      "name": "Natural language",
      "score": 0.5265653133392334
    },
    {
      "name": "Privacy software",
      "score": 0.4803270399570465
    },
    {
      "name": "Private information retrieval",
      "score": 0.44307321310043335
    },
    {
      "name": "Information sensitivity",
      "score": 0.4280288815498352
    },
    {
      "name": "Computer security",
      "score": 0.4217618405818939
    },
    {
      "name": "Artificial intelligence",
      "score": 0.29949840903282166
    },
    {
      "name": "Data mining",
      "score": 0.2539130449295044
    },
    {
      "name": "World Wide Web",
      "score": 0.13771557807922363
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I859038795",
      "name": "Virginia Tech",
      "country": "US"
    }
  ]
}