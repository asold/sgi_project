{
  "title": "Video Swin Transformer",
  "url": "https://openalex.org/W3176258108",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2184294903",
      "name": "Liu Ze",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1945286813",
      "name": "Ning Jia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113229502",
      "name": "Cao Yue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3086230319",
      "name": "Wei, Yixuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2003519119",
      "name": "Zhang Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2159534709",
      "name": "Lin Stephen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117015939",
      "name": "Hu Han",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3159612540",
    "https://openalex.org/W3035303837",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2991151250",
    "https://openalex.org/W2963820951",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2625366777",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W3096833468",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W3035619757",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2883429621",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W3034572008",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3107634219",
    "https://openalex.org/W3113747735"
  ],
  "abstract": "The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-1 accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less pre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1 accuracy on Something-Something v2). The code and models will be made publicly available at https://github.com/SwinTransformer/Video-Swin-Transformer.",
  "full_text": "Video Swin Transformer\nZe Liu∗12, Jia Ning∗13, Yue Cao1†, Yixuan Wei14, Zheng Zhang1, Stephen Lin1, Han Hu1†\n1Microsoft Research Asia\n2University of Science and Technology of China\n3Huazhong University of Science and Technology\n4Tsinghua University\nAbstract\nThe vision community is witnessing a modeling shift from CNNs to Transformers,\nwhere pure Transformer architectures have attained top accuracy on the major\nvideo recognition benchmarks. These video models are all built on Transformer\nlayers that globally connect patches across the spatial and temporal dimensions. In\nthis paper, we instead advocate an inductive bias of locality in video Transformers,\nwhich leads to a better speed-accuracy trade-off compared to previous approaches\nwhich compute self-attention globally even with spatial-temporal factorization.\nThe locality of the proposed video architecture is realized by adapting the Swin\nTransformer designed for the image domain, while continuing to leverage the power\nof pre-trained image models. Our approach achieves state-of-the-art accuracy on\na broad range of video recognition benchmarks, including on action recognition\n(84.9 top-1 accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600\nwith ∼20×less pre-training data and ∼3×smaller model size) and temporal\nmodeling (69.6 top-1 accuracy on Something-Something v2). The code and models\nwill be made publicly available at https://github.com/SwinTransformer/\nVideo-Swin-Transformer.\n1 Introduction\nConvolution-based backbone architectures have long dominated visual modeling in computer vi-\nsion [24, 22, 32, 33, 15, 18]. However, a modeling shift is currently underway on backbone ar-\nchitectures for image classiﬁcation, from Convolutional Neural Networks (CNNs) to Transform-\ners [8, 34, 28]. This trend began with the introduction of Vision Transformer (ViT) [8, 34], which\nglobally models spatial relationships on non-overlapping image patches with the standard Transformer\nencoder [38]. The great success of ViT on images has led to investigation of Transformer-based\narchitectures for video-based recognition tasks [1, 3].\nPreviously for convolutional models, backbone architectures for video were adapted from those for\nimages simply by extending the modeling through the temporal axis. For example, 3D convolu-\ntion [35] is a direct extension of 2D convolution for joint spatial and temporal modeling at the operator\nlevel. As joint spatiotemporal modeling is not economical or easy to optimize, factorization of the\nspatial and temporal domains was proposed to achieve a better speed-accuracy tradeoff [30, 41]. In\nthe initial attempts at Transformer-based video recognition, a factorization approach is also employed,\nvia a factorized encoder [1] or factorized self-attention [1, 3]. This has been shown to greatly reduce\nmodel size without a substantial drop in performance.\nIn this paper, we present a pure-transformer backbone architecture for video recognition that is found\nto surpass the factorized models in efﬁciency. It achieves this by taking advantage of the inherent\n∗Equal Contribution. †Equal Advising. The work is done when Ze Liu, Jia Ning and Yixuan Wei are interns\nat Microsoft Research Asia.\nPreprint.\narXiv:2106.13230v1  [cs.CV]  24 Jun 2021\nspatiotemporal locality of videos, in which pixels that are closer to each other in spatiotemporal\ndistance are more likely to be correlated. Because of this property, full spatiotemporal self-attention\ncan be well-approximated by self-attention computed locally, at a signiﬁcant saving in computation\nand model size.\nWe implement this approach through a spatiotemporal adaptation of Swin Transformer [28], which\nwas recently introduced as a general-purpose vision backbone for image understanding. Swin\nTransformer incorporates inductive bias for spatial locality, as well as for hierarchy and translation\ninvariance. Our model, called Video Swin Transformer, strictly follows the hierarchical structure of\nthe original Swin Transformer, but extends the scope of local attention computation from only the\nspatial domain to the spatiotemporal domain. As the local attention is computed on non-overlapping\nwindows, the shifted window mechanism of the original Swin Transformer is also reformulated to\nprocess spatiotemporal input.\nAs our architecture is adapted from Swin Transformer, it can readily be initialized with a strong\nmodel pre-trained on a large-scale image dataset. With a model pre-trained on ImageNet-21K, we\ninterestingly ﬁnd that the learning rate of the backbone architecture needs to be smaller (e.g. 0.1×)\nthan that of the head, which is randomly initialized. As a result, the backbone forgets the pre-trained\nparameters and data slowly while ﬁtting the new video input, leading to better generalization. This\nobservation suggests a direction for further study on how to better utilize pre-trained weights.\nThe proposed approach shows strong performance on the video recognition tasks of action recognition\non Kinetics-400/Kinetics-600 and temporal modeling on Something-Something v2 (abbreviated as\nSSv2). For video action recognition, its 84.9% top-1 accuracy on Kinetics-400 and 86.1% top-\n1 accuracy on Kinetics-600 slightly surpasses the previous state-of-the-art results (ViViT [ 1]) by\n+0.1/+0.3 points, with a smaller model size (200.0M params for Swin-L vs. 647.5M params for\nViViT-H) and a smaller pre-training dataset (ImageNet-21K vs. JFT-300M). For temporal modeling on\nSSv2, it obtains 69.6% top-1 accuracy, an improvement of +0.9 points over previous state-of-the-art\n(MViT [9]).\n2 Related Works\nCNN and variants In computer vision, convolutional networks have long been the standard for\nbackbone architectures. For 3D modeling, C3D [ 35] is a pioneering work that devises a 11-layer\ndeep network with 3D convolutions. The work on I3D [5] reveals that inﬂating the 2D convolutions\nin Inception V1 to 3D convolutions, with initialization by ImageNet pretrained weights, achieves\ngood results on large-scale Kinetics datasets. In P3D [30], S3D [41] and R(2+1)D [37], it is found\nthat disentangling spatial and temporal convolution leads to a speed-accuracy tradeoff better than\nthe original 3D convolution. The potential of convolution based approaches is limited by the small\nreceptive ﬁeld of the convolution operator. With a self-attention mechanism, the receptive ﬁeld can\nbe broadened with fewer parameters and lower computation costs, which leads to better performance\nof vision Transformers on video recognition.\nSelf-attention/Transformers to complement CNNs NLNet [40] is the ﬁrst work to adopt self-\nattention to model pixel-level long-range dependency for visual recognition tasks. GCNet [4] presents\nan observation that the accuracy improvement of NLNet can mainly be ascribed to its global context\nmodeling, and thus it simpliﬁes the NL block into a lightweight global context block which matches\nNLNet in performance but with fewer parameters and less computation. DNL [42] on the contrary\nattempts to alleviate this degeneration problem by a disentangled design that allows learning of\ndifferent contexts for different pixels while preserving the shared global context. All these approaches\nprovide a complementary component to CNNs for modeling long range dependency. In our work, we\nshow that a pure-transformer based approach more fully captures the power of self-attention, leading\nto superior performance.\nVision Transformers A shift in backbone architectures for computer vision, from CNNs to Trans-\nformers, began recently with Vision Transformer (ViT) [ 8, 34]. This seminal work has led to\nsubsequent research that aims to improve its utility. DeiT [34] integrates several training strategies\nthat allow ViT to also be effective using the smaller ImageNet-1K dataset. Swin Transformer [28]\nfurther introduces the inductive biases of locality, hierarchy and translation invariance, which enable\nit to serve as a general-purpose backbone for various image recognition tasks.\n2\nVideos\nVideo Swin\nTransformer\nBlock\nLinear Embedding\nVideo Swin\nTransformer\nBlock\nPatch Merging\nVideo Swin\nTransformer\nBlock\nPatch Merging\nVideo Swin\nTransformer\nBlock\nPatch Merging\nStage 1 Stage 2 Stage 3 Stage 4\n2 2 6 2\n3DPatch Partition\nFigure 1: Overall architecture of Video Swin Transformer (tiny version, referred to as Swin-T).\nThe great success of image Transformers has led to investigation of Transformer-based architectures\nfor video-based recognition tasks [29, 1, 3, 9, 25]. VTN [29] proposes to add a temporal attention\nencoder on top of the pre-trained ViT, which yields good performance on video action recognition.\nTimeSformer [3] studies ﬁve different variants of space-time attention and suggests a factorized space-\ntime attention for its strong speed-accuracy tradeoff. ViViT [1] examines four factorized designs of\nspatial and temporal attention for the pre-trained ViT model, and suggests an architecture similar to\nVTN that achieves state-of-the-art performance on the Kinetics dataset. MViT [9] is a multi-scale\nvision transformer for video recognition trained from scratch that reduces computation by pooling\nattention for spatiotemporal modeling, which leads to state-of-the-art results on SSv2. All these\nstudies are based on global self-attention modules. In this paper, we ﬁrst investigate spatiotemporal\nlocality and then empirically show that the Video Swin Transformer with spatiotemporal locality bias\nsurpasses the performance of all the other vision Transformers on various video recognition tasks.\n3 Video Swin Transformer\n3.1 Overall Architecture\nThe overall architecture of the proposed Video Swin Transformer is shown in Figure 1, which\nillustrates its tiny version (Swin-T). The input video is deﬁned to be of size T×H×W×3, consisting\nof T frames which each contain H×W×3 pixels. In Video Swin Transformer, we treat each 3D\npatch of size 2×4×4×3 as a token. Thus, the 3D patch partitioning layer obtains T\n2 ×H\n4 ×W\n4 3D\ntokens, with each patch/token consisting of a 96-dimensional feature. A linear embedding layer is\nthen applied to project the features of each token to an arbitrary dimension denoted by C.\nMLP\nLN\n3D W-MSA\nMLP\nLN\nLN\n3D SW-MSA\nMLP\nLN\nFigure 2: An illustration of two succes-\nsive Video Swin Transformer blocks.\nFollowing the prior art [ 30, 41, 13, 12], we do not down-\nsample along the temporal dimension. This allows us to\nstrictly follow the hierarchical architecture of the original\nSwin Transformer [28], which consists of four stages and\nperforms 2×spatial downsampling in the patch merging\nlayer of each stage. The patch merging layer concatenates\nthe features of each group of 2 ×2 spatially neighboring\npatches and applies a linear layer to project the concatenated\nfeatures to half of their dimension. For example, the linear\nlayer in the second stage projects 4C-dimensional features\nfor each token to 2Cdimensions.\nThe major component of the architecture is the Video Swin\nTransformer block, which is built by replacing the multi-\nhead self-attention (MSA) module in the standard Trans-\nformer layer with the 3D shifted window based multi-head\nself-attention module (presented in Section 3.2) and keeping\nthe other components unchanged. Speciﬁcally, a video trans-\nformer block consists of a 3D shifted window based MSA\nmodule followed by a feed-forward network, speciﬁcally a\n2-layer MLP, with GELU non-linearity in between. Layer\n3\nFigure 3: An illustrated example of 3D shifted windows. The input size T′×H′×W′is 8×8×8, and\nthe 3D window size P×M×M is 4×4×4. As layer ladopts regular window partitioning, the number\nof windows in layer lis 2×2×2=8. For layer l+1, as the windows are shifted by (P\n2 ,M\n2 ,M\n2 )=(2, 2, 2)\ntokens, the number of windows becomes 3×3×3=27. Though the number of windows is increased,\nthe efﬁcient batch computation in [28] for the shifted conﬁguration can be followed, such that the\nﬁnal number of windows for computation is still 8.\nNormalization (LN) is applied before each MSA module and FFN, and a residual connection is\napplied after each module. The computational formulas of the Video Swin Transformer block are\ngiven in Eqn. (1).\n3.2 3D Shifted Window based MSA Module\nCompared to images, videos require a much larger number of input tokens to represent them, as videos\nadditionally have a temporal dimension. A global self-attention module would thus be unsuitable\nfor video tasks as this would lead to enormous computation and memory costs. Here, we follow\nSwin Transformer by introducing a locality inductive bias to the self-attention module, which is later\nshown to be effective for video recognition.\nMulti-head self-attention on non-overlapping 3D windowsMulti-head self-attention (MSA)\nmechanisms on each non-overlapping 2D window has been shown to be both effective and efﬁcient\nfor image recognition. Here, we straightforwardly extend this design to process video input. Given a\nvideo composed of T′×H′×W′3D tokens and a 3D window size of P×M×M, the windows are\narranged to evenly partition the video input in a non-overlapping manner. That is, the input tokens\nare partitioned into ⌈T′\nP ⌉×⌈H′\nM ⌉×⌈W′\nM ⌉non-overlapping 3D windows. For example, as shown in\nFigure 3, for an input size of 8×8×8 tokens and a window size of 4×4×4, the number of windows in\nlayer lwould be 2×2×2=8. And the multi-head self-attention is performed within each 3D window.\n3D Shifted Windows As the multi-head self-attention mechanism is applied within each non-\noverlapping 3D window, there lacks connections across different windows, which may limit the\nrepresentation power of the architecture. Thus, we extend the shifted 2D window mechanism of\nSwin Transformer to 3D windows for the purpose of introducing cross-window connections while\nmaintaining the efﬁcient computation of non-overlapping window based self-attention.\nGiven that the number of input 3D tokens is T′×H′×W′ and the size of each 3D window is\nP×M×M, for two consecutive layers, the self-attention module in the ﬁrst layer uses the regular\nwindow partition strategy such that we obtain ⌈T′\nP ⌉×⌈H′\nM ⌉×⌈W′\nM ⌉non-overlapping 3D windows. For\nthe self-attention module in the second layer, the window partition conﬁguration is shifted along the\ntemporal, height and width axes by ( P\n2 ,M\n2 ,M\n2 ) tokens from that of the preceding layer’s self-attention\nmodule.\nWe illustrate this with an example in Figure 3. The input size is 8 ×8×8, and the window size is\n4×4×4. As layer ladopts regular window partitioning, the number of windows in layerlis 2×2×2=8.\nFor layer l+ 1, as the windows are shifted by (P\n2 ,M\n2 ,M\n2 )=(2, 2, 2) tokens, the number of windows\nbecomes 3×3×3=27. Though the number of windows is increased, the efﬁcient batch computation\n4\nin [28] for the shifted conﬁguration can be followed, such that the ﬁnal number of windows for\ncomputation is still 8.\nWith the shifted window partitioning approach, two consecutive Video Swin Transformer blocks are\ncomputed as\nˆzl = 3DW-MSA\n(\nLN\n(\nzl−1))\n+ zl−1,\nzl = FFN\n(\nLN\n(ˆzl))\n+ ˆzl,\nˆzl+1 = 3DSW-MSA\n(\nLN\n(\nzl))\n+ zl,\nzl+1 = FFN\n(\nLN\n(ˆzl+1))\n+ ˆzl+1, (1)\nwhere ˆzl and zl denote the output features of the 3D(S)W-MSA module and the FFN module for\nblock l, respectively; 3DW-MSAand 3DSW-MSA denote 3D window based multi-head self-attention\nusing regular and shifted window partitioning conﬁgurations, respectively.\nSimilar to image recognition [28], this 3D shifted window design introduces connections between\nneighboring non-overlapping 3D windows in the previous layer. This will later be shown to be\neffective for several video recognition tasks, such as action recognition on Kinetics 400/600 and\ntemporal modeling on SSv2.\n3D Relative Position Bias Numerous previous works [31, 2, 16, 17] have shown that it can be\nadvantageous to include a relative position bias to each head in self-attention computation. Thus, we\nfollow [28] by introducing 3D relative position bias B ∈RP2×M2×M2\nfor each head as\nAttention(Q,K,V ) =SoftMax(QKT /\n√\nd+ B)V, (2)\nwhere Q,K,V ∈RPM 2×d are the query, key and value matrices; dis the dimension of query and\nkey features, and PM2 is the number of tokens in a 3D window. Since the relative position along\neach axis lies in the range of [−P + 1,P −1] (temporal) or [−M+ 1,M −1] (height or width), we\nparameterize a smaller-sized bias matrix ˆB ∈R(2P−1)×(2M−1)×(2M−1), and values in Bare taken\nfrom ˆB.\n3.3 Architecture Variants\nFollowing [28], we introduce four different versions of Video Swin Transformer. The architecture\nhyper-parameters of these model variants are:\n• Swin-T: C = 96, layer numbers = {2,2,6,2}\n• Swin-S: C = 96, layer numbers ={2,2,18,2}\n• Swin-B: C = 128, layer numbers ={2,2,18,2}\n• Swin-L: C = 192, layer numbers ={2,2,18,2}\nwhere Cdenotes the channel number of the hidden layers in the ﬁrst stage. These four versions are\nabout 0.25×, 0.5×, 1×and 2×the base model size and computational complexity, respectively. The\nwindow size is set to P = 8and M = 7by default. The query dimension of each head is d= 32,\nand the expansion layer of each MLP is set to α= 4.\n3.4 Initialization from Pre-trained Model\nAs our architecture is adapted from Swin Transformer [28], our model can be initialized by its strong\npre-trained model on a large-scale dataset. Compared to the original Swin Transformer, only two\nbuilding blocks in Video Swin Transformers have different shapes, the linear embedding layer in the\nﬁrst stage and the relative position biases in the Video Swin Transformer block.\nFor our model, the input token is inﬂated to a temporal dimension of 2, thus the shape of the linear\nembedding layer becomes 96×C from 48×C in the original Swin. Here, we directly duplicate the\nweights in the pre-trained model twice and then multiply the whole matrix by 0.5 to keep the mean\nand variance of the output unchanged. The shape of the relative position bias matrix is ( 2P −1,\n2M−1, 2M−1), compared to (2M−1, 2M−1) in the original Swin. To make the relative position\nbias the same within each frame, we duplicate the matrix in the pre-trained model 2P −1 times to\nobtain a shape of (2P −1, 2M −1, 2M −1) for initialization.\n5\nTable 1: Comparison to state-of-the-art on Kinetics-400. \"384↑\" signiﬁes that the model uses a larger\nspatial resolution of 384×384. “Views” indicates # temporal clip×# spatial crop. The magnitudes\nare Giga (109) and Mega (106) for FLOPs and Param respectively.\nMethod Pretrain Top-1 Top-5 Views FLOPs Param\nR(2+1)D [37] - 72.0 90.0 10 × 1 75 61.8\nI3D [6] ImageNet-1K 72.1 90.3 - 108 25.0\nNL I3D-101 [40] ImageNet-1K 77.7 93.3 10 × 3 359 61.8\nip-CSN-152 [36] - 77.8 92.8 10 × 3 109 32.8\nCorrNet-101 [39] - 79.2 - 10 × 3 224 -\nSlowFast R101+NL [13] - 79.8 93.9 10 × 3 234 59.9\nX3D-XXL [12] - 80.4 94.6 10 × 3 144 20.3\nMViT-B, 32×3 [10] - 80.2 94.4 1 × 5 170 36.6\nMViT-B, 64×3 [10] - 81.2 95.1 3 × 3 455 36.6\nTimeSformer-L [3] ImageNet-21K 80.7 94.7 1 × 3 2380 121.4\nViT-B-VTN [29] ImageNet-21K 78.6 93.7 1 × 1 4218 11.04\nViViT-L/16x2 [1] ImageNet-21K 80.6 94.7 4 × 3 1446 310.8\nViViT-L/16x2 320 [1] ImageNet-21K 81.3 94.7 4 × 3 3992 310.8\nip-CSN-152 [36] IG-65M 82.5 95.3 10 × 3 109 32.8\nViViT-L/16x2 [1] JFT-300M 82.8 95.5 4 × 3 1446 310.8\nViViT-L/16x2 320 [1] JFT-300M 83.5 95.5 4 × 3 3992 310.8\nViViT-H/16x2 [1] JFT-300M 84.8 95.8 4 × 3 8316 647.5\nSwin-T ImageNet-1K 78.8 93.6 4 × 3 88 28.2\nSwin-S ImageNet-1K 80.6 94.5 4 × 3 166 49.8\nSwin-B ImageNet-1K 80.6 94.6 4 × 3 282 88.1\nSwin-B ImageNet-21K 82.7 95.5 4 × 3 282 88.1\nSwin-L ImageNet-21K 83.1 95.9 4 × 3 604 197.0\nSwin-L (384↑) ImageNet-21K 84.6 96.5 4 × 3 2107 200.0\nSwin-L (384↑) ImageNet-21K 84.9 96.7 10 × 5 2107 200.0\n4 Experiments\n4.1 Setup\nDatasets For human action recognition, we adopt two versions of the widely-used Kinetics [20]\ndataset, Kinetics-400 and Kinetics-600. Kinetics-400 (K400) consists of ∼240k training videos and\n20k validation videos in 400 human action categories. Kinetics-600 (K600) is an extension of K400\nthat contains ∼370k training videos and 28.3k validation videos from 600 human action categories.\nFor temporal modeling, we utilize the popular Something-Something V2 (SSv2) [14] dataset, which\nconsists of 168.9K training videos and 24.7K validation videos over 174 classes. For all methods, we\nfollow prior art by reporting top-1 and top-5 recognition accuracy.\nImplementation Details For K400 and K600, we employ an AdamW [21] optimizer for 30 epochs\nusing a cosine decay learning rate scheduler and 2.5 epochs of linear warm-up. A batch size of 64 is\nused. As the backbone is initialized from the pre-trained model but the head is randomly initialized,\nwe ﬁnd that multiplying the backbone learning rate by 0.1 improves performance (shown in Tab. 7).\nSpeciﬁcally, the initial learning rates for the ImageNet pre-trained backbone and randomly initialized\nhead are set to 3e-5 and 3e-4, respectively. Unless otherwise mentioned, for all model variants, we\nsample a clip of 32 frames from each full length video using a temporal stride of 2 and spatial size\nof 224 ×224, resulting in 16 ×56×56 input 3D tokens. Following [ 28], an increasing degree of\nstochastic depth [19] and weight decay is employed for larger models, i.e. 0.1,0.2,0.3 stochastic\ndepth rate and 0.02,0.02,0.05 weight decay for Swin-T, Swin-S, and Swin-B, respectively. For\ninference, we follow [1] by using 4 ×3 views, where a video is uniformly sampled in the temporal\ndimension as 4 clips, and for each clip, the shorter spatial side is scaled to 224 pixels and we take 3\ncrops of size 224 ×224 that cover the longer spatial axis. The ﬁnal score is computed as the average\nscore over all the views.\nFor SSv2, we employ an AdamW [21] optimizer for longer training of 60 epochs with 2.5 epochs of\nlinear warm-up. The batch size, learning rate and weight decay are the same as that for Kinetics. We\n6\nTable 2: Comparison to state-of-the-art on Kinetics-600.\nMethod Pretrain Top-1 Top-5 Views FLOPs Param\nSlowFast R101+NL [13] - 81.8 95.1 10 × 3 234 59.9\nX3D-XL [12] - 81.9 95.5 10 × 3 48 11.0\nMViT-B-24, 32×3 [9] - 83.8 96.3 5 × 1 236 52.9\nTimeSformer-HR [3] ImageNet-21K 82.4 96 1 × 3 1703 121.4\nViViT-L/16x2 320 [1] ImageNet-21K 83.0 95.7 4 × 3 3992 310.8\nViViT-H/16x2 [9] JFT-300M 85.8 96.5 4 × 3 8316 647.5\nSwin-B ImageNet-21K 84.0 96.5 4 × 3 282 88.1\nSwin-L (384↑) ImageNet-21K 85.9 97.1 4 × 3 2107 200.0\nSwin-L (384↑) ImageNet-21K 86.1 97.3 10 × 5 2107 200.0\nTable 3: Comparison to state-of-the-art on Something-Something v2.\nMethod Pretrain Top-1 Top-5 Views FLOPs Param\nTimeSformer-HR [3] ImageNet-21K 62.5 - 1 × 3 1703 121.4\nSlowFast R101, 8×8 [13] Kinetics-400 63.1 87.6 1 × 3 106 53.3\nTSM-RGB [27] Kinetics-400 63.3 88.2 2 × 3 62 42.9\nMSNet [23] ImageNet-21K 64.7 89.4 1 × 1 67 24.6\nTEA [26] ImageNet-21K 65.1 89.9 10 × 3 70 -\nblVNet [11] SSv2 65.2 90.3 1 × 1 129 40.2\nViViT-L/16x2 [1] - 65.4 89.8 - 903 352.1\nMViT-B, 64×3 [10] Kinetics-400 67.7 90.9 1 × 3 455 36.6\nMViT-B-24, 32×3 [10] Kinetics-600 68.7 91.5 1 × 3 236 53.2\nSwin-B Kinetics-400 69.6 92.7 1 × 3 321 88.8\nfollow [9] by employing a stronger augmentation, including label smoothing, RandAugment [7], and\nrandom erasing [43]. We also employ stochastic depth [19] with ratio of 0.4. As also done in [9], we\nuse the model pre-trained on Kinetics-400 as initialization and a window size in temporal dimension\nof 16 is used. For inference, the ﬁnal score is computed as the average score of 1 ×3 views.\n4.2 Comparison to state-of-the-art\nKinetics-400 Table 1 presents comparisons to the state-of-the-art backbones, including both\nconvolution-based and Transformer-based on Kinetics-400. Compared to the state-of-the-art vi-\nsion Transformers without large-scale pre-training, Swin-S with ImageNet-1K pre-training achieves\nslightly better performance than MViT-B (32×3) [9] which is trained from scratch with similar com-\nputation costs. Compared to the state-of-the-art ConvNet X3D-XXL [12], Swin-S also outperforms\nit with similar computation costs and fewer views for inference. For Swin-B, the ImageNet-21K\npre-training brings a 2.1% gain over training on ImageNet-1K from scratch. With ImageNet-21K\npre-training, our Swin-L (384↑) outperforms ViViT-L (320) by 3.3% on top-1 accuracy with about\nhalf less computation costs. Pre-training on a signiﬁcantly smaller dataset (ImageNet-21K) than\nViViT-H (JFT-300M), our Swin-L (384↑) achieves the state-of-the-art performance of 84.9% on\nK400.\nKinetics-600 Results on K600 are shown in Table 2. The observations on K600 is similar to those\nfor K400. Compared with the state-of-the-art with ImageNet-21K pre-training, our Swin-L (384↑)\noutperforms ViViT-L (320) by 2.9% on top-1 accuracy with about half less computation costs. With\npre-training on a signiﬁcantly smaller dataset (ImageNet-21K) than ViViT-H (JFT-300M), our Swin-L\n(384↑) obtains state-of-the-art accuracy of 86.1% on K600.\nSomething-Something v2 Table 3 compares our approach with the state-of-the-art on SSv2. We\nfollow MViT [9] by using the K400 pre-trained model as initialization. With pre-trained models on\nK400, Swin-B attains 69.6% top-1 accuracy, surpassing the previous best approach MViT-B-24 with\nK600 pre-training by 0.9%. Our approach could be further improved via using larger model (e.g.\nSwin-L), larger resolution of input (e.g. 3842) and better pre-trained model (e.g. K600). We leave\nthese attempts as future work.\n7\n4.3 Ablation Study\nDifferent designs for spatiotemporal attentionWe ablate three major designs for spatiotemporal\nattention: joint, split and factorized variants. The joint version jointly computes spatiotemporal\nattention in each 3D window-based MSA layer, which is our default setting. The split version adds\ntwo temporal transformer layers on top of the spatial-only Swin Transformer, which is shown to be\neffective in ViViT [1] and VTN [29]. The factorized version adds a temporal-only MSA layer after\neach spatial-only MSA layer in Swin Transformer, which is found to be effective in TimeSformer [3].\nFor the factorized version, to reduce the bad effects of adding randomly initialized layers into the\nbackbone with pre-trained weights, we add a weighting parameter at the end of each temporal-only\nMSA layer which is initialized as zero.\nTable 4: Ablation study on different designs for spatiotemporal attention with Swin-T on K400.\nTop-1 Top-5 FLOPs Param\njoint 78.8 93.6 88 28.2\nsplit 76.4 92.1 83 42.0\nfactorized 78.5 93.5 95 36.5\nResults are shown in Table 4. We can observe that the joint version achieves the best speed-accuracy\ntradeoff. This is mainly because locality in the spatial domain reduces computation for the joint\nversion while maintaining effectiveness. In contrast, a joint version based on ViT/DeiT would be\ntoo computationally expensive. The split version does not work well in our scenarios. Though this\nversion could naturally beneﬁt from the pre-trained model, the temporal modeling of this version is\nnot as efﬁcient. The factorized version yields relatively high top-1 accuracy but requires many more\nparameters than the joint version. This is due the factorized version having a temporal-only attention\nlayer after each spatial-only attention layer, while the joint version performs spatial and temporal\nattention in the same attention layer.\nTable 5: Ablation study on temporal dimension of 3D tokens and temporal window size with Swin-T\non K400.\ntemporal dimension Window size Top 1 Top 5 FLOPs Param\n16 16 ×7×7 79.1 93.8 106 28.5\n8 8 ×7×7 78.5 93.2 44 28.2\n4 4 ×7×7 76.7 92.5 20 28.0\n16 16 ×7×7 79.1 93.8 106 28.5\n16 8 ×7×7 78.8 93.6 88 28.2\n16 4 ×7×7 78.6 93.4 79 28.0\nTemporal dimension of 3D tokensWe perform an ablation study on the temporal dimension of\n3D tokens in a temporally global fashion, where the temporal dimension of 3D tokens is equal to\nthe temporal window size. Results with Swin-T on K400 are shown in Table 5. In general, a larger\ntemporal dimension leads to a higher top-1 accuracy but with greater computation costs and slower\ninference.\nTemporal window size Fixing the temporal dimension of 3D tokens to 16, we perform an ablation\nstudy over temporal window sizes of 4/8/16. Results with Swin-T on K400 are shown in Table 5. We\nobserve that Swin-T with a temporal window size of 8 incurs only a small performance drop of 0.3\ncompared to a temporal window size of 16 (temporally global), but with a 17% relative decrease in\ncomputation (88 vs. 106). This indicates that temporal locality brings an improved speed-accuracy\ntradeoff for video recognition. If the number of input frames is extremely large, temporal locality\nwould have an even greater impact.\n3D shifted windows Ablations of the 3D shifted windowing approach on Swin-T are reported for\nK400 in Table 6. 3D shifted windows bring +0.7% in top-1 accuracy, and temporally shifted windows\nyield +0.3%. The results indicate the effectiveness of the 3D shifted windowing scheme to build\nconnections among non-overlapping windows.\n8\nTable 6: Ablation study on the 3D shifted window approach with Swin-T on K400.\nTop-1 Top-5\nw. 3D shifting 78.8 93.6\nw/o temporal shifting 78.5 93.5\nw/o 3D shifting 78.1 93.3\nRatio of backbone/head learning rateAn interesting ﬁnding on the ratio of backbone and head\nlearning rates is shown in Table 7. With a model pre-trained on ImageNet-1K/ImageNet-21K, we\nobserve that a lower learning rate of the backbone architecture (e.g. 0.1 ×) relative to that of the\nhead, which is randomly initialized, brings gains in top-1 accuracy for K400. Also, using the model\npre-trained on ImageNet-21K beneﬁts more from this technique, due to the model pre-trained on\nImageNet-21K being stronger. As a result, the backbone forgets the pre-trained parameters and data\nslowly while ﬁtting the new video input, leading to better generalization. This observation suggests a\ndirection for further study on how to better utilize pre-trained weights.\nTable 7: Ablation study on the ratio of backbone lr and head lr with Swin-B on K400.\nratio Pretrain Top-1 Top-5\n0.1× ImageNet-1K 80.6 94.6\n1.0× ImageNet-1K 80.2 94.2\n0.1× ImageNet-21K 82.6 95.7\n1.0× ImageNet-21K 82.0 95.3\nInitialization on linear embedding layer and 3D relative position bias matrixIn ViViT [1],\ncenter initialization of the linear embedding layer outperforms inﬂate initialization by a large margin.\nThis motivates us to conduct an ablation study on these two initialization methods for Video Swin\nTransformer. As shown in Table 8, we surprisingly ﬁnd that Swin-T with center initialization\nobtains the same performance as Swin-T with inﬂate initialization, of 78.8% top-1 accuracy using\nthe ImageNet-1K pre-trained model 2 On K400. In this paper, we adopt the conventional inﬂate\ninitialization on the linear embedding layer by default.\nTable 8: Ablation study on the two initializa-\ntion methods of linear embedding layer with\nSwin-T on K400.\nInitialization Top 1 Top 5\nInﬂate 78.8 93.6\nCenter 78.8 93.7\nTable 9: Ablation study on the two initial-\nization methods of 3D relative position bias\nmatrix with Swin-T on K400.\nInitialization Top 1 Top 5\nDuplicate 78.8 93.6\nCenter 78.8 93.6\nFor the 3D relative position bias matrix, we also have two different initialization choices, duplicate or\ncenter initialization. Unlike the center initialization method for linear embedding layer, we initialize\nthe 3D relative position bias matrix by masking the relative position bias across different frames\nwith a small negative value (e.g. -4.6), so that each token only focuses inside the same frame from\nthe very beginning. As shown in Table 9, we ﬁnd that both initialization methods achieve the same\ntop-1 accuracy of 78.8% with Swin-T on K400. We adopt duplicate initialization on the 3D Relative\nPosition Bias matrix by default.\n5 Conclusion\nWe presented a pure-transformer architecture for video recognition that is based on spatiotemporal\nlocality inductive bias. This model is adapted from the Swin Transformer for image recognition, and\nthus it could leverage the power of the strong pre-trained image models. The proposed approach\nachieves state-of-the-art performance on three widely-used benchmarks, Kinetics-400, Kinetics-600\nand Something-Something v2. We made the code publicly available to facilitate future study in this\nﬁeld.\n2As this observation is inconsistent with that in [1], we will analyze the difference once the code of ViViT is\nreleased.\n9\nReferences\n[1] Arnab, A., Dehghani, M., Heigold, G., Sun, C., Luˇci´c, M., and Schmid, C. (2021). Vivit: A video\nvision transformer. arXiv preprint arXiv:2103.15691.\n[2] Bao, H., Dong, L., Wei, F., Wang, W., Yang, N., Liu, X., Wang, Y ., Gao, J., Piao, S., Zhou, M.,\net al. (2020). Unilmv2: Pseudo-masked language models for uniﬁed language model pre-training.\nIn International Conference on Machine Learning, pages 642–652. PMLR.\n[3] Bertasius, G., Wang, H., and Torresani, L. (2021). Is space-time attention all you need for video\nunderstanding? arXiv preprint arXiv:2102.05095.\n[4] Cao, Y ., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). Gcnet: Non-local networks meet squeeze-\nexcitation networks and beyond. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV) Workshops.\n[5] Carreira, J. and Zisserman, A. (2017a). Quo vadis, action recognition? a new model and\nthe kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6299–6308.\n[6] Carreira, J. and Zisserman, A. (2017b). Quo vadis, action recognition? a new model and\nthe kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6299–6308.\n[7] Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V . (2020). Randaugment: Practical automated\ndata augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops, pages 702–703.\n[8] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\nM., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An image is\nworth 16x16 words: Transformers for image recognition at scale. In International Conference on\nLearning Representations.\n[9] Fan, H., Xiong, B., Mangalam, K., Li, Y ., Yan, Z., Malik, J., and Feichtenhofer, C. (2021a).\nMultiscale vision transformers. arXiv:2104.11227.\n[10] Fan, H., Xiong, B., Mangalam, K., Li, Y ., Yan, Z., Malik, J., and Feichtenhofer, C. (2021b).\nMultiscale vision transformers. arXiv preprint arXiv:2104.11227.\n[11] Fan, Q., Chen, C.-F., Kuehne, H., Pistoia, M., and Cox, D. (2019). More is less: Learning\nefﬁcient video representations by big-little network and depthwise temporal aggregation. arXiv\npreprint arXiv:1912.00869.\n[12] Feichtenhofer, C. (2020). X3d: Expanding architectures for efﬁcient video recognition. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n203–213.\n[13] Feichtenhofer, C., Fan, H., Malik, J., and He, K. (2019). Slowfast networks for video recognition.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6202–6211.\n[14] Goyal, R., Ebrahimi Kahou, S., Michalski, V ., Materzynska, J., Westphal, S., Kim, H., Haenel,\nV ., Fruend, I., Yianilos, P., Mueller-Freitag, M., et al. (2017). The\" something something\"\nvideo database for learning and evaluating visual common sense. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 5842–5850.\n[15] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778.\n[16] Hu, H., Gu, J., Zhang, Z., Dai, J., and Wei, Y . (2018). Relation networks for object detection.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages\n3588–3597.\n[17] Hu, H., Zhang, Z., Xie, Z., and Lin, S. (2019). Local relation networks for image recognition.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages\n3464–3473.\n10\n[18] Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2017). Densely connected\nconvolutional networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4700–4708.\n[19] Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. (2016). Deep networks with\nstochastic depth. In European conference on computer vision, pages 646–661. Springer.\n[20] Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F.,\nGreen, T., Back, T., Natsev, P., et al. (2017). The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950.\n[21] Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\n[22] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in neural information processing systems , pages\n1097–1105.\n[23] Kwon, H., Kim, M., Kwak, S., and Cho, M. (2020). Motionsqueeze: Neural motion feature\nlearning for video understanding. In European Conference on Computer Vision, pages 345–362.\nSpringer.\n[24] LeCun, Y ., Bottou, L., Bengio, Y ., Haffner, P., et al. (1998). Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324.\n[25] Li, X., Zhang, Y ., Liu, C., Shuai, B., Zhu, Y ., Brattoli, B., Chen, H., Marsic, I., and Tighe, J.\n(2021). Vidtr: Video transformer without convolutions. arXiv preprint arXiv:2104.11746.\n[26] Li, Y ., Ji, B., Shi, X., Zhang, J., Kang, B., and Wang, L. (2020). Tea: Temporal excitation and\naggregation for action recognition. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 909–918.\n[27] Lin, J., Gan, C., and Han, S. (2019). Tsm: Temporal shift module for efﬁcient video under-\nstanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n7083–7093.\n[28] Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., and Guo, B. (2021). Swin trans-\nformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030.\n[29] Neimark, D., Bar, O., Zohar, M., and Asselmann, D. (2021). Video transformer network. arXiv\npreprint arXiv:2102.00719.\n[30] Qiu, Z., Yao, T., and Mei, T. (2017). Learning spatio-temporal representation with pseudo-3d\nresidual networks. In proceedings of the IEEE International Conference on Computer Vision ,\npages 5533–5541.\n[31] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and\nLiu, P. J. (2020). Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nJournal of Machine Learning Research, 21(140):1–67.\n[32] Simonyan, K. and Zisserman, A. (2015). Very deep convolutional networks for large-scale\nimage recognition. In International Conference on Learning Representations.\n[33] Szegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke,\nV ., and Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1–9.\n[34] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2020).\nTraining data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877.\n[35] Tran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri, M. (2015). Learning spatiotemporal\nfeatures with 3d convolutional networks. In Proceedings of the IEEE international conference on\ncomputer vision, pages 4489–4497.\n11\n[36] Tran, D., Wang, H., Torresani, L., and Feiszli, M. (2019). Video classiﬁcation with channel-\nseparated convolutional networks. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 5552–5561.\n[37] Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y ., and Paluri, M. (2018). A closer look at\nspatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on\nComputer Vision and Pattern Recognition, pages 6450–6459.\n[38] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\nPolosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing\nSystems, pages 5998–6008.\n[39] Wang, H., Tran, D., Torresani, L., and Feiszli, M. (2020). Video modeling with correlation net-\nworks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 352–361.\n[40] Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local neural networks. In Proceed-\nings of the IEEE conference on computer vision and pattern recognition, pages 7794–7803.\n[41] Xie, S., Sun, C., Huang, J., Tu, Z., and Murphy, K. (2018). Rethinking spatiotemporal feature\nlearning: Speed-accuracy trade-offs in video classiﬁcation. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 305–321.\n[42] Yin, M., Yao, Z., Cao, Y ., Li, X., Zhang, Z., Lin, S., and Hu, H. (2020). Disentangled non-local\nneural networks. In Proceedings of the European conference on computer vision (ECCV).\n[43] Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y . (2020). Random erasing data augmentation.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 13001–13008.\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7234894037246704
    },
    {
      "name": "Transformer",
      "score": 0.6646453142166138
    },
    {
      "name": "Locality",
      "score": 0.6613115668296814
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5100722908973694
    },
    {
      "name": "Architecture",
      "score": 0.4927580654621124
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.49067223072052
    },
    {
      "name": "Action recognition",
      "score": 0.4417393207550049
    },
    {
      "name": "Computer vision",
      "score": 0.3566901981830597
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3283727765083313
    },
    {
      "name": "Engineering",
      "score": 0.12232330441474915
    },
    {
      "name": "Electrical engineering",
      "score": 0.09158864617347717
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Class (philosophy)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}