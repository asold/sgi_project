{
    "title": "Compression of Generative Pre-trained Language Models via Quantization",
    "url": "https://openalex.org/W4226087293",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2498707706",
            "name": "Chaofan Tao",
            "affiliations": [
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2123717260",
            "name": "Lu Hou",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A1506313323",
            "name": "Wei Zhang",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2151842933",
            "name": "Lifeng Shang",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2097242334",
            "name": "Xin Jiang",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2109590494",
            "name": "Qun Liu",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A1912778456",
            "name": "Ping Luo",
            "affiliations": [
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2111945729",
            "name": "Ngai Wong",
            "affiliations": [
                "University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3098576111",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2916954108",
        "https://openalex.org/W2242818861",
        "https://openalex.org/W1902934009",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W3114304470",
        "https://openalex.org/W2786771851",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3035060554",
        "https://openalex.org/W3034457371",
        "https://openalex.org/W2554592357",
        "https://openalex.org/W3100985894",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3036601975",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3015609966",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W4221162932",
        "https://openalex.org/W3159727696",
        "https://openalex.org/W2786548044",
        "https://openalex.org/W3034772996",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W3101066076",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2963825865",
        "https://openalex.org/W3207612418",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W2996990662",
        "https://openalex.org/W2995607862",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W4287592659",
        "https://openalex.org/W1999965501"
    ],
    "abstract": "Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4821 - 4836\nMay 22-27, 2022câƒ2022 Association for Computational Linguistics\nCompression of Generative Pre-trained Language Models via Quantization\nChaofan Tao1, Lu Hou2, Wei Zhang2, Lifeng Shang2,\nXin Jiang2, Qun Liu2, Ping Luo1, Ngai Wong1\n1The University of Hong Kong 2Huawei Noahâ€™s Ark Lab\ncftao@connect.hku.hk, pluo@cs.hku.hk, nwong@eee.hku.hk\n{houlu3, zhangwei379, shang.lifeng, jiang.xin, qun.liu}@huawei.com\nAbstract\nThe increasing size of generative Pre-trained\nLanguage Models (PLMs) have greatly in-\ncreased the demand for model compression.\nDespite various methods to compress BERT\nor its variants, there are few attempts to com-\npress generative PLMs, and the underlying dif-\nï¬culty remains unclear. In this paper, we com-\npress generative PLMs by quantization. We\nï¬nd that previous quantization methods fail\non generative tasks due to the homogeneous\nword embeddings caused by reduced capacity,\nand varied distribution of weights. Correspond-\ningly, we propose a token-level contrastive dis-\ntillation to learn distinguishable word embed-\ndings, and a module-wise dynamic scaling to\nmake quantizers adaptive to different modules.\nEmpirical results on various tasks show that\nour proposed method outperforms the state-\nof-the-art compression methods on generative\nPLMs by a clear margin. With comparable per-\nformance with the full-precision models, we\nachieve 14.4Ã—and 13.4Ã—compression rates on\nGPT-2 and BART, respectively.\n1 Introduction\nTransformer-based generative pre-trained language\nmodels (PLMs) show strong abilities of multi-\ntask and few-shot learning, and achieve remark-\nable performances on various tasks (Radford and\nNarasimhan, 2018; Brown et al., 2020; Lewis et al.,\n2020; Raffel et al., 2020; Chen et al., 2021). How-\never, they are usually expensive in terms of both\ncomputation and memory due to a large number\nof parameters, and the token-by-token generation\nprocess. Many methods have been proposed to\ncompress PLMs, but mostly focus on understand-\ning tasks like sentence classiï¬cation with BERT\n(Lan et al., 2019; Sun et al., 2020b; Jiao et al.,\n2020; Shen et al., 2020; Hou et al., 2020). Recent\nworks try to compress GPT-2 using tensor decom-\nposition (Edalati et al., 2021), and knowledge dis-\ntillation (Song et al., 2020), but the compression\nFigure 1: Performance of quantized GPT-2 with varying\nweight bit-widths and 8-bit activation, using different\nmethods. The right ï¬gure takes a closer look at LAQ.\nratio achieved is much smaller than that of BERT.\nYet the underlying difï¬culty remains unclear.\nIn this paper, we ï¬rstly explore compressing gen-\nerative PLMs by quantizing the parameters from\nfull-precision to lower bits. We ï¬nd that directly ap-\nplying previous quantization methods designed for\nBERT or computer vision tasks to generative PLMs\nlead to poor performance. Figure 1 shows that the\nperformance drops sharply as the weight bit-width\ndecreases. To investigate the difï¬culty of quan-\ntizing generative PLMs, we ï¬nd that the learned\nembeddings tend to be homogeneous and hard to\ndistinguish due to the reduced capacity caused by\nquantization, while the weight distributions also\nvary signiï¬cantly across different modules and dif-\nferent Transformer layers. These problems are\nfurther magniï¬ed due to the nature of sequential\nleft-to-right prediction of generative PLMs, as the\nquantization error will accumulate across time.\nTo alleviate the above problems, we propose a\ntoken-level contrastive distillation to contrast on\ntokens and make the word embedding distinguish-\nable. Besides, we propose a module-wise dynamic\nscaling for the quantizer to better adapt to different\nmodules. Empirical results on language modeling,\nnext utterance prediction and summarization show\nthat compared to the full-precision baseline, our\nquantized GPT and BART (abbreviated as Quant-\nGPT and QuantBART) achieve comparable perfor-\nmance for 8/4-bit weight, and have only a slight\ndrop for 2-bit weight, while being over 13Ã—smaller.\n4821\nQuantGPT also clearly outperforms previous GPT\ncompression methods on language modeling.\nTo summarize, our main contributions are: 1)\nWe ï¬nd that generative PLMs are hard to quantize\ndue to homogeneous word embedding and varied\nweight distribution. 2) We then propose the token-\nlevel contrastive distillation and module-wise dy-\nnamic scaling, to make the word embedding more\ndistinguishable and make quantizers adapt to dif-\nferent modules, respectively. 3) Empirical results\non various tasks show the efï¬cacy of our method.\n2 Difï¬culty of Qunatizing Generative\nPre-trained Language Models\nIn this section, we show that it is challenging to\ntrain a low-bit generative pre-trained model with\nconventional quantization approaches directly. Be-\nfore diving into details, we ï¬rst review the neces-\nsary backgrounds of quantization.\n2.1 Network Quantization\nIn this paper, we apply the quantization-aware train-\ning (Courbariaux et al., 2015) to generative PLMs.\nSpeciï¬cally, denote the vectorized full-precision\nweight as w, each forward propagation ï¬rst clips\nthe weight by a positive clipping factor Î±, and then\nquantizes the clipped weight to b-bit as\nwq = Î±Â·Q(clip(w,âˆ’Î±,Î±)/Î±), (1)\nwhere Q is the quantization function that maps\neach entry in clip(w,âˆ’Î±,Î±)/Î±to its closest quan-\ntized value in the set of uniform discrete values\n{âˆ’1,âˆ’nâˆ’1\nn ,Â·Â·Â· ,âˆ’1\nn,0,1\nn,Â·Â·Â· ,nâˆ’1\nn ,1}with n=\n2bâˆ’1 âˆ’1. Then we compute the loss â„“(wq) with\nwq. During back propagation, we use the gradi-\nent with regard to the quantized weight âˆ‡â„“(wq)\nas the Straight-Through-Estimator (Bengio et al.,\n2013) to update full-precision weights w due to the\nnon-differentiability of Q(Â·).\nA good clipping factor is expected to take the\nmajority of full-precision weight into account via\nclipping, i.e., quantizing the range where data are\ndensely distributed to reduce quantization error.\nTo solve this problem, PACT (Choi et al., 2018)\nlearns a parameterized clipping factor and achieves\nbetter results than setting a ï¬xed clipping factor.\nInstead of learning the clipping factor, LSQ (Esser\net al., 2020) learns the step size Î±/n, but requires\na careful initialization and gradient update.\nIn practice, following previous works on BERT\nquantization (Zhang et al., 2020; Bai et al., 2021),\nwe use layer-wise quantization (i.e., one clipping\nfactor for elements in each weight matrix) for all\nweight matrices in the Transformer layers and row-\nwise quantization (i.e., one clipping factor for each\nword embedding) for the embedding layer. We use\nasymmetric uniform quantization for activations\nafter self-attention and GeLU function whose ele-\nments are mostly positive, and symmetric uniform\nquantization for other activations. We do not quan-\ntize layer-normalization layers, skip connections,\nbiases due to small computational overhead.\n2.2 Difï¬culty Analysis\nWe compare the following representative quanti-\nzation methods including (i) LAQ (Zhang et al.,\n2020) for BERT; (ii) PACT (Choi et al., 2018) and\nLSQ (Esser et al., 2020)) for computer vision tasks,\nto generative pre-trained model, GPT-2. Figure 1\nshows the performance under different weight bit-\nwidths, and the performance drops sharply as the\nbit-width decreases, especially for PACT and LSQ.\nIn the following, we study the potential reasons be-\nhind the difï¬culty of quantizing generative PLMs,\nby empirically investigating the properties of the\nword embedding and model parameters.\nHomogeneous Word Embedding. We ï¬rst study\nthe difï¬culty from the learned word embeddings of\ndifferent models. In Figure 2, we visually compare\nthe distributions of the word embeddings of the full-\nprecision and quantized models under the same\nscale. As can be seen, the word embeddings of\nthe full-precision model are scattered distinguish-\nable, while those in previous quantization methods\nPACT, LSQ and LAQ learn homogeneous word\nembeddings which are clustered and less distin-\nguishable, especially for PACT and LSQ. We spec-\nulate this is caused by the sequential computation\nnature of GPT. Speciï¬cally, unlike BERT which\ncomputes the representation of all tokens in paral-\nlel, GPT computes each token in left-to-right order,\nand the quantization error incurred in the previous\ntokens will pass on to future tokens, making the\nlearning signal noisier over time, and ï¬nally less\ninformative word embeddings.\nA direct consequence of the homogeneous word\nembedding can be reï¬‚ected in Figure 3. By com-\nparing Figure 2 and Figure 3, we can ï¬nd that the\nhigher degree of homogeneity in the word embed-\nding of a quantized model, the fewer dependencies\namong different tokens are kept.\nAs will be discussed in Section 3.1, we propose\n4822\n(a) Full-precision.\n (b) PACT.\n (c) LSQ.\n (d) LAQ.\n (e) Ours.\nFigure 2: T-SNE visualization of the most frequent 500 word embeddings, of the full-precision and different 2-bit\nquantized models trained on PTB dataset. Embeddings of different methods show different degrees of homogeneity.\nFigure 3: Matrices representing the cosine similarities between representations of all pairs of tokens in a sentence,\nbetween the full-precision model and 2-bit quantized models trained on PTB dataset. Token representations at the\nlast decoder layer of GPT-2 are used. More visualizations are available in Appendix C.3.\na token-level contrastive learning to alleviate this\nproblem. Compared with PACT, LSQ and LAQ,\nour method not only aligns the token represen-\ntations between the quantized and full-precision\nnetworks (i.e., diagonal boxes), but also captures\nthe dependencies among different tokens (non-\ndiagonal boxes). More visualizations are available\nin Appendix C.3. The non-distinguishable word\nembeddings and poor ability to capture contextual-\nized dependencies also make methods like PACT\nand LSQ more likely to generate incorrect tokens,\ne.g. illogical and repeated text ( Section 4.4).\n(a) wo at Layer 4.\n (b) wg at Layer 4.\nFigure 4: Distributions of output projection matrix wo\nin the multi-head attention module and the second linear\nlayer wg in the feed-forward network of the 4-th layer\nfrom the 12-layer full-precision GPT-2. Other modules\nin other layers exhibit similar patterns. Vertical lines\nindicate the clipping factors learned by PACT and our\nmethod. Black curves show the estimated distribution\nby kernel density estimation.\nVaried Distribution of Weights. Besides the\nlearned word embeddings, we also investigate the\ndistribution of the weights in the full-precision\nmodel. Figure 4 shows that the weight distribu-\ntions of a 12-layer full-precision GPT-2 are highly\nskewed with outliers. This causes difï¬culty in es-\ntimating the clipping factor Î±of the quantizer by\nheuristic methods, or even by PACT which learns\nthe Î± through gradient descent. Speciï¬cally, in\nPACT, the approximated gradient ofÎ±only relies\non the weights whose absolute values are larger\nthan Î±. This solution ignores the effect of weights\nwithin [âˆ’Î±,Î±] and depends heavily on the initial-\nization of Î±. Figure 4 shows that an improper ini-\ntialization together with the inaccurate gradient\nestimation of the clipping factor often make the\nlearned Î±of PACT too large, and can not provide\nï¬ne resolution to the majority of weights within\nthe clipping range. The quantization error accumu-\nlated over time makes this problem more severe. In\nthis work, we re-parameterize the clipping factor to\nmake the quantizer adaptive to each module in the\nTransformer layers, and consider both weights out-\nside and inside the clipping range when estimating\nthe gradient of the clipping factor.\nAs will be discussed in Section 3.2, we propose\na module-wise dynamic scaling to reduce the clip-\nping factorâ€™s sensitivity to initialization, and an\nimproved gradient estimation that also considers\nthe weights within [âˆ’Î±,Î±]. Figure 4 shows that the\nclipping factor learned by our method gives ï¬ner\nresolutions to the majority of the weights.\n4823\nğ’•ğ’•ğŸğŸ ğ’•ğ’•ğŸğŸ ğ’•ğ’•ğ’ğ’\nâ€œShe said â€¦â€¦ goodâ€\ntokenize\nInput sequence\nâ€¦â€¦ Pull together\nPush awayToken memory bank ğ‘½ğ‘½ğ’ƒğ’ƒ\nindex update\nQuantized Student Network\nEmbedding \nLayer\nFull-precision Teacher Network\nâ€¦\nEmbedding \nLayer\nTransformer \nLayer 1\nTransformer \nLayer L\nâ€¦\nEmbedding \nLayer\nTransformer \nLayer 1\nTransformer \nLayer L\nâ€¦\nToken-level Contrastive Distillation \nğ’‰ğ’‰ğ’•ğ’•ğŸğŸ\nğ’•ğ’• ğ’‰ğ’‰ğ’•ğ’•ğŸğŸ\nğ’•ğ’•\nğ’’ğ’’ğ’•ğ’•ğŸğŸ\nğ’”ğ’” ğ’’ğ’’ğ’•ğ’•ğŸğŸ\nğ’”ğ’” ğ’’ğ’’ğ’•ğ’•ğ’ğ’\nğ’”ğ’”â€¦\nğ’‰ğ’‰ğ’•ğ’•ğ’ğ’\nğ’•ğ’•\nâ€¦\nâ„“ğ’„ğ’„ğ’„ğ’„ğ’ğ’ğ’”ğ’”\nâ„“ğ’…ğ’…ğ’…ğ’…ğ’”ğ’”ğ’•ğ’•\nLogit Distillation\nğ’›ğ’›ğ’•ğ’•ğŸğŸ\nğ’”ğ’” ğ’›ğ’›ğ’•ğ’•ğŸğŸ\nğ’”ğ’” ğ’›ğ’›ğ’•ğ’•ğ’ğ’\nğ’”ğ’”\nğ’›ğ’›ğ’•ğ’•ğŸğŸ\nğ’•ğ’• ğ’›ğ’›ğ’•ğ’•ğŸğŸ\nğ’•ğ’• ğ’›ğ’›ğ’•ğ’•ğ’ğ’\nğ’•ğ’•\nâ€¦\nEmbedding \nLayer\nFigure 5: The training workï¬‚ow of the proposed method. For each token in the quantized network, we compute\nboth (i) the token-level contrastive distillation loss where the positive tokens and negative tokens are selected from\nthe full-precision teacher network; and (ii) the distillation loss on the logits. The embedding layer and all weights in\nthe Transformer layers are quantized with the proposed module-dependent dynamic scaling.\n3 Proposed Method\nBased on the observations in Section 2.2, we pro-\npose a quantization method which utilizes token-\nlevel contrastive distillation to make the word em-\nbedding distinguishable (Section 3.1) and a module-\nwise dynamic scaling adjustment to learn better\nclipping factors (Section 3.2).\n3.1 Token-level Contrastive Distillation\nThe proposed token-level contrastive distillation\ncontrast among tokens instead of sequences se-\nquence, to learn distinguishable representations\nfor each token. Inspired by Baevski et al. (2020),\nwhich uses in-utterance representation at different\npositions of the same utterance as negatives for\nspeech feature learning, for each token of the quan-\ntized network, we use the representation of the\nsame token from the full-precision teacher network\nas its positive, while representations of other to-\nkens in the same sequence as negatives (Figure 5).\nInspired by He et al. (2020) which uses a momen-\ntum encoder for more consistent representation, we\nbuild a memory bank to store momentum token\nrepresentations from the quantized network. When\ncomputing the contrastive distillation loss, we load\nthe representations of negative samples from the\nmemory bank with cheap indexing operations.\nSpeciï¬cally, we use superscripts s and t to\ndenote the quantized student network and full-\nprecision teacher network, respectively. De-\nnote the length- n input sequence of tokens as\n(t1,t2,Â·Â·Â· ,tn). For the i-th token ti, suppose its\nhidden states of the last Transformer layer from the\nquantized and full-precision network are linearly\nprojected to (hs\ni,ht\ni) âˆˆRd, and qs\ni is the smoothed\nrepresentation of hs\ni in the memory bank. Denote\nSi as the indices of the sampled negatives for token\ni, the token-level contrastive distillation loss for the\nlength-n sequence can be formulated as\nLcont=âˆ’\nnâˆ‘\ni=1\nlog exp(s(qs\nti ,ht\nti )/Ï„)âˆ‘\njâˆˆSi exp(s(qs\nti ,ht\ntj )/Ï„), (2)\nwhere s(x,y) = xâŠ¤y\nâˆ¥xâˆ¥âˆ¥yâˆ¥computes the cosine simi-\nlarity, and Ï„ is a ï¬xed temperature parameter.\nThen we update the representation of token ti\nin the memory bank with the moving-average of\ntoken representations from the quantized network:\nqs\nti â†mqs\nti + (1âˆ’m)hs\nti , (3)\nwhere mâˆˆ[0,1) it the momentum coefï¬cient that\ncontrols the smoothness of the token represenation.\nBesides, we use an additional distillation loss\nLdist over the logits. For the i-th token ti, sup-\npose the logits of the quantized and full-precision\nnetwork are zs\nti ,zt\nti âˆˆR|V|, where |V|is the vocab-\nulary size. Ldist is computed with the soft cross-\nentropy loss:\nLdist = âˆ’\nnâˆ‘\ni=1\nzt\nti log(zs\nti ). (4)\nThus the total training loss is\nL= Î»Lcont + Ldist, (5)\nwhere Î»is a trade-off factor set as 0.1 by default.\nIntuitively, for each token in the quantized net-\nwork, Ldist only encourages it to mimic its corre-\nsponding token of the teacher network, whileLcont\nnot only pulls it close to its positive, but also pushes\nit away from its negatives. In this way, Lcont helps\nthe student to capture more information from the\n4824\nteacherâ€™s representation, as is also theoretically dis-\ncussed in Tian et al. (2019).\nThe proposed token-level contrastive distillation\nis crucial to the performance, and outperforms the\nsequence-level counterpart (as will be shown em-\npirically in Section 5.1.1). We conjecture this is be-\ncause (i) token-level contrast alleviates the problem\nof homogeneous word embedding (Figure 2) in the\nlow-bit quantization; and (ii) similar to speech, the\norder of natural language is also sequential instead\nof spatial like images; and (iii) the self-attention\nmechanism allows other tokens to learn represen-\ntations contextualized on the studied token, and\nthese in-sequence negatives are harder than those\nfrom in-batch sequences, allowing more efï¬cient\nrepresentation learning.\n3.2 Module-dependent Dynamic Scaling\nBased on the observation of varied weight distri-\nbution in Section 2.2, we propose a simple-yet-\neffective dynamic scaling according to the statis-\ntics of each module weight. Speciï¬cally, instead\nof directly learning the original clipping factor Î±\nas PACT, we turn to learn a new scaling factor Î³,\nwhich is multiplied with the average weight magni-\ntude âˆ¥wâˆ¥1\nn to get clipping factor Î±:\nÎ±= Î³Â·âˆ¥wâˆ¥1\nn , (6)\nwhere âˆ¥Â·âˆ¥ 1 denotes â„“1 norm. The scaling Î³ is\ninitialized as 1, which not only eases the initializa-\ntion but also ensures the initial clipping factor Î±\ndoes not deviate far from the full-precision weights,\nregardless of the diversity of weight distribution.\nBesides, we also design a more accurate gradient\nestimation of the scaling factor than PACT (Choi\net al., 2018). Previous PACT only back propagates\nthrough weights whose absolute values are larger\nthe clipping factor (i.e. |w|â‰¥ Î±). Instead, we also\nconsider the weights inside the clipping range (i.e.\n|w|<Î±) as:\nâˆ‚â„“\nâˆ‚Î³ =\nï£±\nï£´ï£´ï£²\nï£´ï£´ï£³\nâˆ‚â„“\nâˆ‚wq Q(u)âˆ¥wâˆ¥1\nn ,w <âˆ’Î±\nâˆ‚â„“\nâˆ‚wq [âˆ’w\nÎ±+Q(u)]âˆ¥wâˆ¥1\nn ,âˆ’Î±â‰¤wâ‰¤Î±\nâˆ‚â„“\nâˆ‚wq Q(u)âˆ¥wâˆ¥1\nn ,w >Î±\n, (7)\nwhere â„“ is the total training loss and u =\nclip(w,âˆ’Î±,Î±)/Î±in Eq. (1). The detailed deriva-\ntion can be found in Appendix A.\nIntuitively, the update of clipping factor should\nbe inï¬‚uenced by both weights outside and inside\n[âˆ’Î±,Î±], since Î±controls the quantization error of\nboth, i.e., a large clipping factor results in small\nquantization error for weights outside [âˆ’Î±,Î±],\nwhile large error for weights inside. Our new es-\ntimation of the gradient of Î³ in Eq. (7) considers\nweights both outside and inside [âˆ’Î±,Î±]. Addition-\nally, the proposed scaling is less sensitive to the\nvaried distribution of weight than PACT, since the\ngradient of scaling âˆ‚â„“\nâˆ‚Î³ is proportional to the average\nweight magnitude âˆ¥wâˆ¥1\nn .\n4 Experiments\n4.1 Setup\nTasks and Models. In this section, we evaluate\nthe efï¬cacy of our proposed quantization method\non three kinds of generative tasks on two kinds of\ngenerative pre-training models. Speciï¬cally, we\nperform the proposed quantization approach on\nlanguage modeling and next utterance prediction\ntasks on GPT-2 (Radford and Narasimhan, 2018),\nand abstractive summarization using BART (Lewis\net al., 2020), and call the resultant models Quant-\nGPT and QuantBART. The token-level contrastive\ndistillation is performed on the hidden states of the\nlast layer of GPT-2 or the BART decoder. More\ndetails about the datasets and model architectures\ncan be found in Appendix B.1 and B.2.\nImplementation Details. For each downstream\ntask with our proposed method, we ï¬rst ï¬ne-tune a\nfull-precision network using the pre-trained check-\npoint from huggingface1 for both GPT-2 and BART.\nThen we use this ï¬ne-tuned network as the full-\nprecision teacher network and to initialize the quan-\ntized student network. We train each task with 8\nV100 GPUs based on the Pytorch framework. The\ndetailed hyper-parameters for each task are avail-\nable in Appendix B.3.\nCompared Methods. Since there are very few\nattempts to compress generative PLMs, we self-\nimplement three baseline quantization methods\nPACT (Choi et al., 2018), LSQ (Esser et al., 2020)\nand LAQ (Hou and Kwok, 2018) for comparison.\nDetails about these methods are in Appendix B.4.\n4.2 Language Modeling\nThe task of language modeling is to predict the\nprobability distribution over a sequence of words.\n1http://huggingface.co/models\n4825\nMethod #Bits\n(W-E-A)\nSize\n(MB) (â†“)\nWikiText2\nPPL (â†“)\nPTB\nPPL (â†“)\nWikiText103\nPPL (â†“)\nPersona-Chat\nAcc(%) (â†‘)\n- full-prec. 474.9 14.48 14.72 14.19 77.01\nPACT 8-8-8 121.4 17.49 16.11 16.76 74.73\nLSQ 8-8-8 121.4 16.75 15.43 15.24 75.28\nLAQ 8-8-8 121.4 16.91 15.87 15.88 76.02\nQuantGPT 8-8-8 121.4 15.31 14.90 14.58 76.12\nPACT 4-4-8 62.4 19.23 20.17 20.15 25.13\nLSQ 4-4-8 62.4 78.99 79.76 75.12 45.10\nLAQ 4-4-8 62.4 17.12 16.55 16.91 71.71\nQuantGPT 4-4-8 62.4 15.55 14.95 15.31 76.57\nPACT 2-2-8 33.0 173.02 189.13 171.03 5.52\nLSQ 2-2-8 33.0 847.54 544.98 1470.86 5.54\nLAQ 2-2-8 33.0 19.15 18.25 18.97 71.36\nQuantGPT 2-2-8 33.0 17.30 16.12 16.98 74.78\nTable 1: Results of language modeling on the test set of WikiText2, PTB and WikiText103 datasets, and next\nutterance prediction on the validation set of Persona-Chat dataset, with quantized GPT-2. â€œ#Bits (W-E-A)â€ represents\nthe bit-width for weights of Transformer layers, word embedding, and activations.\nFor language modeling, we experiment on Wiki-\nText2 (Merity et al., 2016), Penn Treebank (PTB)\n(Mikolov and Zweig, 2012) and WikiText103 (Mer-\nity et al., 2016). We use perplexity (PPL) to evalu-\nate the performance for language modeling.\nComparison with the Full-precision Model.\nFrom Table 1, the performance of the proposed\nmethod with 8-bit weight is comparable to the full-\nprecision counterpart on PTB and WikiText103,\nwhile drops slightly on WikiText2. A slightly more\nsevere performance drop is observed as the bit-\nwidth decreases from 8 to 4, with a drop of around 1\nPPL point on WikiText2 and WikiText103, and less\nthan 0.1 PPL point on PTB. When the bit-width of\nweight further goes down to 2, our method has an\naverage of 2 PPL points drop, but achieves 14.4Ã—\nmodel size reduction.\nComparison with Other Quantization Methods.\nFrom Table 1, our method outperforms PACT, LSQ\nand LAQ for all bit-widths and tasks. As the bit-\nwidth decreases from 8 to 4, the PPL of LSQ\ngreatly increases, with the average PPL of LSQ\nincreasing by over 5 times. As the bit-width fur-\nther decreases to 2, both LSQ and PACT fail on all\ndatasets, despite their good performance on under-\nstanding tasks on BERT (Bai et al., 2021). We con-\njecture it is because though both PACT and LSQ\nhave learnable parameters, the accumulated quanti-\nzation error of generative PLMs makes the updates\nof these parameters by gradient descent less sta-\nble. On the other hand, the proposed module-wise\ndynamic scaling alleviates the problem.\nComparison with Other Compression Methods.\nIn Table 2, we compare our quantization method\nMethod Size\n(MB)(â†“)\nWikiText2\nPPL(â†“)\nPTB\nPPL(â†“)\nWikiText103\nPPL(â†“)\nfull-prec.474.9 (1.0x) 14.4 14.6 13.9\nKnGPT2 332.0 (1.4x) - - 20.5\nDistilGPT2 329.6 (1.4x) - - 21.1\nLightPAFF 268.0 (1.8x) 18.8 22.8 16.4\nOurs(8-8-8)121.4 (3.9x) 15.3 14.9 14.6\nOurs(4-4-8)62.4 (7.6x) 15.6 15.0 15.3\nOurs(2-2-8)33.0 (14.4x) 17.3 16.1 17.0\nTable 2: Comparison between our proposed quatization\nmethod and other compression methods on GPT-2.\nagainst recent GPT-2 compression methods, includ-\ning tensor decomposition method KnGPT2 (Edalati\net al., 2021), as well as distillation methods Distil-\nGPT2 and LightPAFF (Song et al., 2020). From\nthe comparison, our method outperforms the others\nin terms of model size and performance, even when\nweights are compressed to only 2 bits.\n4.3 Next Utterance Prediction\nThe task of next utterance prediction predicts the\nnext utterance given the dialogue context. It tests\nthe language understanding ability of generative\nmodels. For this task, we use a large-scale dialogue\ndataset, Persona-Chat (Zhang et al., 2018).\nFrom Table 1, all quantization methods incur\na clear performance drop compared to the full-\nprecision baseline, even in the 8-bit setting. As\nthe quantization becomes more aggressive, i.e., the\nbit-width gets smaller, the performance of PACT\nand LAQ decrease more signiï¬cantly than ours. In\nparticular, LSQ diverges for 2-bit weight and its ac-\ncuracy is only 5%, which is no better than a random\nguess as there are 20 classes.\n4826\n4.4 Abstractive Summarization\nAbstractive summarization aims at generating a\nterse summary that captures the main ideas of the\nsource article. We experiment on XSum (Narayan\net al., 2018), whose ground-truth summarizations\nare highly abstractive and are challenging for many\nextractive strategies. ROUGE 1, 2, L are used to\nevaluate the performance of this task.\nMethod #Bits\n(W-E-A)\nSize\n(MB)(â†“) XSum\nMetric R1 ( â†‘) R2 (â†‘) RL (â†‘)\n- full-prec. 532.0 40.75 18.10 33.05\nPACT 8-8-8 138.1 39.16 16.60 31.60\nLSQ 8-8-8 138.1 39.09 16.72 31.56\nLAQ 8-8-8 138.1 39.10 16.74 31.65\nQuantBART 8-8-8 138.1 40.25 17.78 32.70\nPACT 4-4-8 72.4 32.68 11.52 26.03\nLSQ 4-4-8 72.4 38.94 16.48 31.46\nLAQ 4-4-8 72.4 39.03 16.68 31.63\nQuantBART 4-4-8 72.4 40.24 17.71 32.69\nPACT 2-2-8 39.6 7.76 1.30 6.96\nLSQ 2-2-8 39.6 37.09 14.88 29.76\nLAQ 2-2-8 39.6 37.48 15.27 30.13\nQuantBART 2-2-8 39.6 39.15 16.72 31.72\nTable 3: Results of abstractive summarization on the\ntest set of the XSum dataset, with quantized BART.\nTable 3 shows the results of the abstractive sum-\nmarization. As can be seen, our method constantly\noutperforms other methods again with a clear mar-\ngin. Example generated summarizations of differ-\nent methods in Appendix C.2 show that the sum-\nmaries generated by QuantBART are logical and\nterse, while those from PACT have repeated texts.\n5 Discussion\n5.1 Ablation on Contrastive Learning\n5.1.1 Choices of Negative Sampling\nAs shown in Figure 6, we ablate on how to choose\nnegative samples in contrastive learning. Specif-\nically, we compare our method with variants of\ntoken-level contrastive learning, which select neg-\native samples of each token from (a) representa-\ntions of other tokens in both the full-precision and\nquantized networks (fp+quan.); (b) representations\nof other tokens in the quantized network ( quan.\nonly); and (c) the whole vocabulary randomly for\neach training iteration (global). Besides, we com-\npare with (d) sequence-level contrastive learning\nby pulling together representations of the same se-\nquence, and pushing away representations of differ-\n(a) fp+quan.\n (b) quan. only.\n(c) global.\n (d) in-batch.\nFigure 6: Four variants of negative sampling.\n- Sampling\nmethod WikiText2 PTB WikiText103\n- QuantGPT 17.30 16.12 16.98\nTok-level\nfp+quan. 17.38 16.51 17.13\nquan. only 17.35 16.54 17.15\nglobal 17.71 16.63 17.55\nSeq-levelin-batch (bz=32) 17.62 19.23 18.97\nin-batch (bz=16) 17.48 17.11 18.16\nTable 4: Ablation study on negative sampling for 2-bit\nweight, â€œbzâ€ denotes for the batch size. â€œTokâ€ and â€œSeqâ€\nare abbreviation for token and sequence, respectively.\nent ones from the teacher network (in-batch). Rep-\nresentation of a sequence is deï¬ned as the mean of\nrepresentations of all tokens in the sequence.\nFrom Table 4, â€œfp+quan. â€ and â€œquan. onlyâ€\nperforms worse than QuantGPT, which uses full-\nprecision representations of other tokens as nega-\ntive samples. This indicates that noisy representa-\ntions of tokens from the not-fully-trained quantized\nnetwork may not be sufï¬cient. â€œglobalâ€ performs\neven worse, which we conjecture is because, for\none token, negative tokens chosen from the same\nsequence are contextually related to it and more\ninformative than random tokens. â€œin-batchâ€ per-\nforms worse than all token-level variants, which\nmay be because generative tasks make predictions\nin a token-wise manner and rely heavily in ï¬ner-\ngrained token-wise representations. Interestingly,\ncontrary to in-batch negative sampling in computer\nvision (Chen et al., 2020), we ï¬nd that reducing the\nnumber of negative samples by reducing the batch\nsize from 32 to 16 slightly improves performance.\n5.1.2 Number of Negative Samples\nIn Figure 7, we plot the PPL of 2-bit QuantGPT on\nthe PTB dataset, with varying number of negative\nsamples. We plot the mean results with standard\n4827\nFigure 7: Effect of the num-\nber of negative samples.\nFigure 8: Scaling factors\nin the 2-bit QuantGPT.\nTraining loss Training time\n(sec/iter) (â†“)\nMemory\n(MB) (â†“) PPL (â†“)\nLdist 0.61 14700 16.93\nLdist+Î»Lcont 0.67 14839 16.12\nTable 5: Efï¬ciency study of the token-level contrastive\nlearning. The results are reported on the PTB dataset\non 2-bit GPT-2. â€œsec/iterâ€ means the needed time in\nseconds per iteration. Memory denotes the GPU con-\nsumption per device.\ndeviations from 5 independent runs. As can be seen,\nthe performance improves and converges gradually\nas the number of negative samples increases.\nFigure 7 also shows that using the moving-\naverage representations (qs\nti in Eq. (3)) of nega-\ntive samples in the memory bank has better perfor-\nmance than using the immediate representations\n(hs\nti in Eq. (3)), because of a smoother and more\nconsistent representation of tokens.\n5.1.3 Training Cost of the Contrastive Loss\nIn Table 5, we report the training speed and mem-\nory consumption of training the GPT-2 model on\nthe PTB dataset with and without the proposed\ntoken-level contrastive loss. Batch size is set as\n4 per device, which can be increased by using\nGPUs with larger memory or reducing the sequence\nlength of samples. As can be seen, with the pro-\nposed token-level contrastive loss, the performance\nclearly improves with only slightly slower training\nspeed and more memory consumption.\n5.1.4 Representations for the Contrastive Loss\nIn Table 6, we compare the different representa-\ntions to perform the contrastive loss. The â€œdecoder-\nlastâ€( resp. â€œdecoder-ï¬rstâ€) denotes performing the\nproposed token-level contrastive loss on the hid-\nden states from the last decoder layer (resp. ï¬rst\ndecoder layer) followed by a linear transformation.\nFrom Table 6, â€œdecoder-lastâ€ performs better\nthan â€œdecoder-ï¬rstâ€. A possible reason is that the\nhidden states of the last decoder blocks contain\nrich information from all previous layers (Xiong\net al., 2020). Since the experiments of abstractive\nsummarization are conducted on BART, which has\nboth encoder and decoder layers, we also study the\ncontrastive loss on the â€œencoder-lastâ€ and â€œencoder-\nï¬rstâ€. In the ablation on the encoder, the contrastive\nloss Lcont are computed on the source input (arti-\ncles), instead of target input (summaries). From\nTable 6, â€œdecoder-lastâ€ also has better ROUGE 1,\n2, L values than other counterparts.\n5.2 Ablation on Dynamic Scaling\nFigure 8 shows the learned scaling Î³ of different\nmodules in the 2-bit GPT-2 model. As can be seen,\nthe scalings of different modules vary a lot, verify-\ning the need for module-wise dynamic scaling.\nIn addition, we investigate the effect of the pro-\nposed dynamic scaling and the new estimation of\nthe gradient in Eq. (7) with two variants: 1) Ldist\nonly which removes the token-level contrastive\nlearning; and 2) Ours with PACT which removes\nthe contrastive learning, and estimates the gradi-\nent with PACT which only considers the weights\nwhose absolute values are larger than the clipping\nfactor Î±. As shown in Table 7, the performance\ngets worse without contrastive learning to learn\nthe distinguishable representations of tokens. The\nperformance drops signiï¬cantly when using PACT\nto estimate the gradient of the proposed scaling,\nespecially for the WikiText103 dataset, verifying\nthe efï¬cacy of the new gradient estimation.\n6 Related Work\nCompression of Generative Pre-trained Lan-\nguage Models. Some early explorations com-\npress the generative pre-trained language models.\nKnGPT2 (Edalati et al., 2021) applies the Kro-\nnecker decomposition to compress the GPT. Dis-\ntilGPT2 2 distills a 12-layer GPT-2 to a 6-layer\none, which is twice as fast during inference. Light-\nPAFF (Song et al., 2020) proposes a distillation\napproach that the training loss is a combination of a\nmaximum likelihood loss of the student model, and\nthe KL divergence between the output of teacher\nand student models. SpAtten (Wang et al., 2021)\nproposes a sparse model with algorithm and archi-\ntecture co-design, which removes uninformative\ntokens and attention heads. Compared with these\nmethods, we not only study the difï¬culties of com-\npression from the properties of generative tasks,\n2https://transformer.huggingface.co/\nmodel/distil-gpt2\n4828\n- WikiText2 PTB WikiText103 Persona-Chat XSum\nMetric PPL ( â†“) PPL ( â†“) PPL ( â†“) Acc( %) (â†‘) R1 ( â†‘) R2 ( â†‘) RL ( â†‘)\ndecoder-last 17.30 16.12 16.98 74.78 39.15 16.72 31.72\ndecoder-ï¬rst 18.02 16.61 17.25 74.75 39.11 16.70 31.62\nencoder-last - - - - 38.91 16.72 31.67\nencoder-ï¬rst - - - - 38.87 16.70 31.56\nTable 6: Representations for the contrastive loss Lcont in 2-bit setting. The â€œdecoder-lastâ€ means the contrastive\nloss is computed on the hidden states from the last Transformer layer of the decoder after a linear transform. The\nnaming format works for other variants.\nMethod WikiText2 PTB WikiText103\nQuantGPT 17.30 16.12 16.98\nLdistonly 17.85 16.93 17.78\nOurs with PACT 20.03 17.78 25.54\nTable 7: Ablation study on the learning of the clipping\nfactor with 2-bit GPT-2 on the language modeling task.\nbut also study both decoder and encoder-decoder\ngenerative models.\nQuantization of Pre-trained Language Models.\nQuantization compresses a model by representing\nthe 32-bit ï¬‚oating-point parameter with a low-bit\nrepresentation, and has been widely used in vari-\nous domains as it does not require designing a new\nmodel architecture. There have been many attempts\nto quantize task-speciï¬c BERT models (Zafrir et al.,\n2019; Shen et al., 2020; Zadeh et al., 2020) with\nonly negligible performance drop on natural lan-\nguage understanding tasks. Recent works (Zhang\net al., 2020; Bai et al., 2021) even push the weight\nbit-width down to as low as 1-bit. Despite the\nsuccess of these approaches for BERT models, at-\ntempts to quantize generative PLMs are scarce, and\nthe underlying difï¬culty remains unclear.\nContrastive Learning. Contrastive learning\naims at pushing the representations of similar sam-\nples together while pulling those of dissimilar ones\napart. and is widely used for large-scale self-\nsupervised learning in various domains (Chen et al.,\n2020; Sun et al., 2020a; Baevski et al., 2020; Huang\net al., 2022), and multi-modal learning (Radford\net al., 2021; Jia et al., 2021). SimCLR (Chen et al.,\n2020) directly uses other in-batch samples as neg-\natives, and sufï¬cient large batch size is required\nto work well. MoCo (He et al., 2020) maintains a\nlarge number of negative samples in a queue and\nuses a moving average key encoder to improve\nconsistency. Contrastive learning without negative\nsamples is also proposed in BYOL (Grill et al.,\n2020) and SimSiam (Chen and He, 2021). Con-\ntrastive representation distillation (Tian et al., 2019)\ndistills the knowledge from the teacher network to\nthe student network by maximizing the mutual in-\nformation between them.\nThe closest work with our token-level contrastive\ndistillation is Wav2vec 2.0 (Baevski et al., 2020),\nwhich use in-utterance representations at different\npositions as negatives in speech learning. Besides\nthe difference in the modality and tasks, our method\nalso differs from theirs in (1) Model: We quantize\nthe model parameters and activations while they\ndo not; (2) Representation: For each sample, we\nuse the output of the full-precision and the quan-\ntized networks as its two views, while they use the\nquantized and the contextualized representation.\n(3) Loss: We calculate loss over all tokens in an\nauto-regressive manner, while they only calculate\nover the masked tokens non-autoregressively.\n7 Conclusion\nThis paper studies low-bit quantization of genera-\ntive PLMs. We ï¬nd that the difï¬culty of quantizing\ngenerative PLMs lies in homogeneous word em-\nbedding and varied distribution of weights. To\nalleviate the two problems, we propose token-level\ncontrastive learning to learn more distinguishable\ntoken emebeddings, as well as a module-dependent\ndynamic scaling for more accurate quantization.\nExtensive experiments on language modeling, next\nutterance prediction and abstractive summarization\ndemonstrate the efï¬cacy of our proposed method.\nWe hope our work sheds a light on the compression\nof generative PLMs in future exploration.\nAcknowledgements\nThis work is supported in part by the General Re-\nsearch Fund (GRF) project 17206020, and in part\nby ACCESS, AI Chip Center for Emerging Smart\nSystems, Hong Kong SAR.\n4829\nReferences\nAlexei Baevski, Henry Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nIn Advances in Neural Information Processing Sys-\ntems, volume 33.\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing\nJin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin\nKing. 2021. Binarybert: Pushing the limit of bert\nquantization. In Annual Meeting of the Association\nfor Computational Linguistics.\nYoshua Bengio, Nicholas LÃ©onard, and Aaron Courville.\n2013. Estimating or propagating gradients through\nstochastic neurons for conditional computation.\nTechnical Report arXiv:1308.3432.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Process-\ning Systems.\nCong Chen, Chaofan Tao, and Ngai Wong. 2021. Litegt:\nEfï¬cient and lightweight graph transformers. In Pro-\nceedings of the 30th ACM International Conference\non Information & Knowledge Management , pages\n161â€“170.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning, pages\n1597â€“1607.\nXinlei Chen and Kaiming He. 2021. Exploring simple\nsiamese representation learning. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\npages 15750â€“15758.\nJungwook Choi, Zhuo Wang, Swagath Venkataramani,\nPierce I-Jen Chuang, Vijayalakshmi Srinivasan, and\nKailash Gopalakrishnan. 2018. Pact: Parameterized\nclipping activation for quantized neural networks.\nPreprint arXiv:1805.06085.\nMatthieu Courbariaux, Yoshua Bengio, and Jean-Pierre\nDavid. 2015. Binaryconnect: Training deep neural\nnetworks with binary weights during propagations.\nIn Advances in neural information processing sys-\ntems, pages 3123â€“3131.\nAli Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Par-\ntovi Nia, James J Clark, and Mehdi Rezagholizadeh.\n2021. Kronecker decomposition for gpt compres-\nsion. In Advances in Neural Information Processing\nSystems.\nSteven K Esser, Jeffrey L McKinstry, Deepika Bablani,\nRathinakumar Appuswamy, and Dharmendra S.\nModha. 2020. Learned step size quantization. In\nInternational Conference on Learning Representa-\ntions.\nJean-Bastien Grill, Florian Strub, Florent AltchÃ©,\nCorentin Tallec, Pierre H Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires,\nZhaohan Daniel Guo, Mohammad Gheshlaghi Azar,\net al. 2020. Bootstrap your own latent: A new ap-\nproach to self-supervised learning. In Neural Infor-\nmation Processing Systems.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie,\nand Ross Girshick. 2020. Momentum contrast\nfor unsupervised visual representation learning. In\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 9729â€“9738.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). Technical Report\narXiv:1606.08415.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. Tech-\nnical Report arXiv:1503.02531.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic bert\nwith adaptive width and depth. InAdvances in Neural\nInformation Processing Systems, volume 33.\nLu Hou and James T. Kwok. 2018. Loss-aware weight\nquantization of deep networks. In International Con-\nference on Learning Representations.\nLu Hou, Yao Quanming, and James T. Kwok. 2017.\nLoss-aware binarization of deep networks. In Inter-\nnational Conference on Learning Representations.\nWenyong Huang, Zhenhe Zhang, Yu Ting Yeung, Xin\nJiang, and Qun Liu. 2022. Spiral: Self-supervised\nperturbation-invariant representation learning for\nspeech pre-training. In International Conference on\nLearning Representations.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V Le, Yunhsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up vi-\nsual and vision-language representation learning with\nnoisy text supervision. In International conference\non machine learning.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinybert: Distilling bert for natural language under-\nstanding. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 4163â€“4174.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In International Confer-\nence on Learning Representations.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Annual Meeting of the Association for\nComputational Linguistics.\n4830\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn IEEE Spoken Language Technology Workshop ,\npages 234â€“239. IEEE.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n2018. Donâ€™t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1797â€“1807.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In International Conference on Machine\nLearning.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniï¬ed text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1â€“\n67.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low pre-\ncision quantization of bert. In AAAI Conference on\nArtiï¬cial Intelligence.\nKaitao Song, Hao Sun, Xu Tan, Tao Qin, Jianfeng Lu,\nHongzhi Liu, and Tie-Yan Liu. 2020. Lightpaff: A\ntwo-stage distillation framework for pre-training and\nï¬ne-tuning. Preprint arXiv:2004.12817.\nSiqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang\nWang, and Jingjing Liu. 2020a. Contrastive distil-\nlation on intermediate representations for language\nmodel compression. In Conference on Empirical\nMethods in Natural Language Processing, pages 498â€“\n508.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020b. Mobilebert:\na compact task-agnostic bert for resource-limited de-\nvices. In Annual Meeting of the Association for Com-\nputational Linguistics, pages 2158â€“2170.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. 2019.\nContrastive representation distillation. In Interna-\ntional Conference on Learning Representations.\nHanrui Wang, Zhekai Zhang, and Song Han. 2021. Spat-\nten: Efï¬cient sparse attention architecture with cas-\ncade token and head pruning. In 2021 IEEE Interna-\ntional Symposium on High-Performance Computer\nArchitecture (HPCA), pages 97â€“110. IEEE.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tieyan Liu. 2020. On layer\nnormalization in the transformer architecture. In In-\nternational Conference on Machine Learning, pages\n10524â€“10533. PMLR.\nAli Hadi Zadeh, Isak Edo, Omar Mohamed Awad,\nand Andreas Moshovos. 2020. Gobo: Quantizing\nattention-based nlp models for low latency and en-\nergy efï¬cient inference. In IEEE/ACM International\nSymposium on Microarchitecture (MICRO), pages\n811â€“824. IEEE.\nOï¬r Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert.\nPreprint arXiv:1910.06188.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\npets too? In Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2204â€“2213.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. Ternarybert:\nDistillation-aware ultra-low bit bert. In Conference\non Empirical Methods in Natural Language Process-\ning.\n4831\nA Derivation of Gradient of Dynamic\nScaling\nIn this section, we provide the derivation of the\ngradient of the proposed dynamic scaling Î³. The\nquantization in the forward process can be written\nas\nÎ± = âˆ¥wâˆ¥1\nn Î³,\nu = clip(w,âˆ’Î±,+Î±)/Î±,\nwq = Q(u)Î±,\nwhere Q(Â·) is an uniform quantization function as\ndescribed in Section 2.2. Based on the chain rule,\nthe gradient of scaling Î³ w.r.t. the training loss\nfunction â„“is:\nâˆ‚â„“\nâˆ‚Î³ = âˆ‚â„“\nâˆ‚wq\n[ âˆ‚wq\nâˆ‚Q(u)\nâˆ‚Q(u)\nâˆ‚Î±\nâˆ‚Î±\nâˆ‚Î³ + âˆ‚wq\nâˆ‚Î±\nâˆ‚Î±\nâˆ‚Î³]\n= âˆ‚â„“\nâˆ‚wq\n[Î±âˆ‚Q(u)\nâˆ‚Î±\nâˆ¥wâˆ¥1\nn + Q(u)âˆ¥wâˆ¥1\nn ]\n= âˆ‚â„“\nâˆ‚wq\n[Î±âˆ‚Q(u)\nâˆ‚Î± + Q(u)]âˆ¥wâˆ¥1\nn .\n(8)\nWe use straight through estimator (STE) to es-\ntimate the gradient of uniform quantizer Q(Â·), i.e.,\nâˆ€i,âˆ‚Q(ui)\nâˆ‚ui\n= 1. Thus the gradient âˆ‚Q(u)\nâˆ‚Î± can be\nwritten as:\nâˆ‚Q(u)\nâˆ‚Î± = âˆ‚Q(u)\nâˆ‚u\nâˆ‚u\nâˆ‚Î±=\nï£±\nï£²\nï£³\n0,w â‰¤âˆ’Î±\nâˆ’w\nÎ±2 ,âˆ’Î±<w<Î±\n0,w â‰¥Î±\n. (9)\nBy combining Eq. (8) and Eq. (9), we get\nâˆ‚â„“\nâˆ‚Î³ =\nï£±\nï£´ï£´ï£²\nï£´ï£´ï£³\nâˆ‚â„“\nâˆ‚wq Q(u)âˆ¥wâˆ¥1\nn ,w â‰¤âˆ’Î±\nâˆ‚â„“\nâˆ‚wq [âˆ’w\nÎ± + Q(u)]âˆ¥wâˆ¥1\nn ,âˆ’Î±< w <Î±\nâˆ‚â„“\nâˆ‚wq Q(u)âˆ¥wâˆ¥1\nn ,w â‰¥Î±\nwhere âˆ‚â„“\nâˆ‚Î³ considers both the weight inside and out-\nside the clipping value, and proportional to the\nweight magnitude âˆ¥wâˆ¥1\nn .\nB More Experimental Settings\nB.1 Datasets\nThe train/val/test splits for different datasets are\nshown on Table 8.\nB.2 Model Architectures\nGPT-2. The vocabulary size of GPT-2 is 50527.\nWe use GPT-2-small with 12 decoder layers and\nhidden state dimension as 768, for experiments\nDataset Training Validation Test\nWikiText2 36,717 3,760 4,358\nPTB 42,068 3,370 3,761\nWikiText103 1,801,350 3,760 4,358\nPersona-Chat 8,939 1,000 968\nXSum 204,045 11,332 11,334\nTable 8: Data splits of different datasets.\nin Sections 2.2, 4 and 5. GeLU (Hendrycks and\nGimpel, 2016) is used as the activation function. In\nthe experiments of Appendix C.1, we adopt GPT-\n2-base with 24 decoder layers and hidden state\ndimension as 1024, to evaluate the quantization\nability on larger models.\nBART. The vocabulary size of BART is 50265.\nWe use BART-base with 6 encoder layers, 6 de-\ncoder layers and hidden state dimension as 768 for\nexperiments in Section 4. In the experiments of Ap-\npendix C.1, we adopt BART-large with 12 encoder\nlayers, 12 decoder layers and hidden state dimen-\nsion 1024, to evaluate the quantization ability on\nlarger models.\nB.3 Hyperparameters\nLanguage Modeling. The sequence length is\n512. The learning rate is initialized to 0.0005 (resp.\n0.001) for the GPT-2 backbone parameters (resp.\nclipping factor Î³) and then linearly decays to 0. The\nnumber of negative samples in each sequence is 64\nfor the PTB dataset, and 32 for the WikiText2 and\nWikiText103. The temperature Ï„ and momentum\ncoefï¬cient mis 0.1 and 0.5 respectively. We train\nwith the AdamW optimizer (Loshchilov and Hut-\nter, 2017) with batch size 32. The training epochs\nfor WikiText2, PTB and WikiText103 are set as 80,\n120, 8, respectively.\nNext Utterance Prediction. The sequence length\nis 512. The learning rate is initialized to 0.0005\n(resp. 0.001) for the GPT-2 backbone parameters\n(resp. clipping factor Î³) and then linearly decays\nto 0. The number of negative samples in each\nsequence is 32. The temperature Ï„ and momentum\ncoefï¬cient mis 0.1 and 0.5, respectively. We train\nwith the AdamW optimizer with batch size 16, for\na total of 2 epochs.\nAbstractive Summarization. We set the length\nof the source sequence (articles) as 512, and pad the\ntarget sequence (summaries) to maximum length.\nWe use beam search to generate summaries, with\nbeam size 6 and length penalty 1. The learning rate\n4832\nMethod #Bits\n(W-E-A)\nSize\n(MB)(â†“) WikiText2 PTB WikiText103 Size\n(MB)(â†“) XSum\nMetric PPL ( â†“) PPL ( â†“) PPL ( â†“) R1 ( â†‘) R2 ( â†‘) RL ( â†‘)\n- full-prec. 1353.7 12.46 12.35 12.37 1550.0 45.25 22.11 37.07\nPACT 8-8-8 342.5 12.86 13.95 13.90 394.8 43.55 20.57 35.55\nOurs 8-8-8 342.5 12.53 12.40 12.68 394.8 44.34 21.41 36.32\nPACT 4-4-8 174.0 16.10 14.19 18.07 202.2 19.45 3.53 15.58\nOurs 4-4-8 174.0 13.34 12.41 14.12 202.2 44.18 21.31 36.25\nPACT 2-2-8 89.7 98.74 68.55 86.60 106.0 8.53 0.93 7.25\nOurs 2-2-8 89.7 14.53 13.22 14.52 106.0 42.38 19.75 34.57\nTable 9: Ablation study on larger models. We report the results on 24-layer GPT-2 and 24-layer BART.\nis initialized to 0.0002 (resp. 0.001) for the BART\nbackbone parameters (resp. clipping factor Î³) and\nthen linearly decays to 0. The number of negative\nsamples is 32. The temperature Ï„ and momentum\ncoefï¬cient mis 0.1 and 0.5, respectively. We train\nwith the AdamW optimizer with batch size 128, for\na total of 8 epochs.\nB.4 Description of the Compared Methods\nPACT. PACT (Choi et al., 2018) learns a learn-\nable clipping factor for each module by gradient\ndescent. To make the quantization more accurate,\nwe adopt a ï¬‚exible variant of the original PACT,\nwith different positive and negative clipping fac-\ntors [âˆ’Î±neg,Î±pos], where both Î±neg and Î±pos are\ninitialized as 2.5.\nLSQ. LSQ (Esser et al., 2020) learns the step-size\nof quantizer for each module by gradient descent.\nWe use the recommended initialization strategy of\nthe step size as (Esser et al., 2020).\nLAQ. LAQ (Hou et al., 2017; Hou and Kwok,\n2018) is a loss-aware quantization method that\nviews quantization as an optimization problem and\nsolve it via proximal Newton algorithm. We use\nthe approximate solver in (Hou and Kwok, 2018)\nto compute the quantized weights before each for-\nward propagation.\nFor the self-implemented methods PACT, LSQ\nand LAQ, we adopt the commonly-used distilla-\ntion loss adopted in (Hinton et al., 2015; Jiao et al.,\n2020). Note that these methods are only used for\nweights and embeddings, while the activations of\nthese methods follow the same setting as our pro-\nposed method in Section 2.1. We also tried us-\ning the original language modeling loss w.r.t. the\nground-truth labels, and distillation loss over the\nthe attention as (Jiao et al., 2020). However, these\ntwo losses worsens the performance on all three\nmethods.\nB.5 Frameworks of Double-head GPT-2 and\nBART\nSince we adopt double-head GPT-2 and BART for\nnext utterance prediction and abstractive summa-\nrization, the frameworks for these tasks are slightly\nmodiï¬ed from that on language modeling due to\nthe difference of tasks. In Figure 9 and 10, we il-\nlustrate the framework for double-head GPT-2 and\nBART, respectively. In the double-head GPT-2, we\nalso quantize the ï¬nal linear layer in the output\nhead.\nC More Experimental Results\nC.1 Performance of Larger Models\nIn Table 9, we experiment with GPT-base and\nBART-large, which both have 24 Transformer lay-\ners. For all bit-widths, the training of our method\nconverges successfully without gradient explod-\ning/vanishing problems. QuantGPT outperforms\nPACT by a large margin in all tasks, especially for\n2-bit weight. Our quantization method on larger\nmodels also has better performance than that on\n12-layer GPT-2 and 12-layer BART in Section 4.\nC.2 Examples of Summarizations\nIn Table 10, we provide the example summariza-\ntions on the XSum dataset. By comparing the ar-\nticles, references and generations, the generated\nsummaries by our quantized model are more log-\nical and terse than PACT, LSQ and LAQ, which\nface problems of homogeneous word embeddings\nto some extent as discussed in Section 2.2.\nC.3 More Visualizations for the Token\nRepresentations\nIn Figure 11, we provide the visualizations of token\nrepresentations on more samples. The observations\n4833\nArticle: On Tuesday, a BBC Spotlight programme revealed that eight children had gone missing in Northern Ireland. Two\nof the girls were Somali teenagers who disappeared in 2005 and 2012. The Health and Social Care Board has said new\nguidelines are in place and add that no children have gone missing since 2014. Separated children are children outside\ntheir country of origin and separated from their parents or legal guardian. The term can also include unaccompanied\nasylum-seeking children and trafï¬cked children. When they arrive in Northern Ireland they are taken into the care of the\nlocal health trust. Eight children have gone missing since 2005 and they remain missing. The SDLPâ€™s health spokesman\nMark H Durkan said he would be raising the issue at the Northern Ireland Assemblyâ€™s health committee and his party\ncolleague Alex Attwood would be raising it at the justice committee. \"The number of children who cannot be accounted\nfor is something that needs urgent inquiry and investigation,\" he said. \"There is a lot of very good work being done to\nlook after the welfare of unaccompanied young people, but clearly we now have some very big questions that need to be\nanswered.\" Ulster Unionist MLA Jo-Anne Dobson said it was \"frankly appalling\" to hear that eight children had gone\nmissing. \"I have written to Health Minister Michelle Oâ€™Neill on this issue to seek further clariï¬cation and to demand\ndetails of how the department, health trusts and the Health and Social Care Board have sought to address each of the cases\ninvolved in the investigation,\" she added. The Green Party leader Steven Agnew also said it was extremely worrying that\nchildren can disappear without a trace. Paula Bradshaw from the Alliance Party added that the health trusts and police\n\"need to work closer over the handling of these cases\". In a statement, the Police Ombudsman for Northern Ireland said:\n\"Our director of investigations will be reviewing the contents of the programme to ascertain if there are any issues of\npolice conduct which may need further investigation.\" The Police Service of Northern Ireland has said that in the two\ncases identiï¬ed in the programme, investigations were robust and all information available at the time was followed. The\nHealth and Social Care Board has said that new guidelines are in place and stress that no children have gone missing since\n2014. BBC Spotlightâ€™s investigation is now available on BBC iPlayer.\nReference: An urgent inquiry is needed into separated children who have gone missing from care, the Social Democratic\nand Labour Party has said.\nPACT:TheTheAAATheTheTheAnAnAnTheThe an an an been been been jailed.\nLSQ: The SDLP has called for an urgent inquiry into the welfare of unaccompanied children in Northern Ireland.\nLAQ: The SDLP has called for \"urgent inquiry and investigation\" into the handling of unaccompanied children in Northern\nIreland.\nOurs: The SDLP is calling for an urgent inquiry and investigation into the disappearance of unaccompanied young people.\nArticle: The dairies operation, which processes and distributes milk, is being sold to Germanyâ€™s Mueller for Ã‚Â£80m. It\ncomes as proï¬ts at the UKâ€™s largest dairy food company fell 95% to Ã‚Â£900,000 in the six months to September. Dairy\nCrest processes and delivers around 1.3 billion litres of milk a year for retailers and homes. Dairy Crest said in a statement\nthat the deal was in the best interests of consumers, customers and dairy farmers. The dairies business accounts for about\n70% of the companyâ€™s revenues, which rose 1% to Ã‚Â£682.1m during the six months. After the sale, which still needs\nshareholder approval and could take several months, Dairy Crest will focus on its proï¬table cheese and spreads operations.\nThere are about 14,000 dairy farmers in the UK, producing 3.3 million litres a day. However, with milk prices having\nfallen, there has been much debate about whether the economics of the industry are sustainable. Investors approved\nof the Dairy Crestâ€™s decision to get out of a loss-making sector, sending its shares 10% higher in morning trading on\nThursday. Muller said the deal would lead to lower costs and larger exports of dairy products made in the UK. Ronald\nKers, chief executive of Muller UK & Ireland, said: \"We are concerned that the dynamics of the UK fresh milk market are\nunsustainable for dairy processors in the mid to long term and this acquisition will allow us to reduce our costs, increase\nour efï¬ciencies and invest in the future.\" Under the deal, Muellerâ€™s UK division - Muller Wiseman Dairies - will take over\nfactories at Foston, in Derbyshire, Chadwell Heath, in Essex, and Severnside, near Gloucester. The deal also includes the\nHanworth glass bottling site in Middlesex, where Dairy Crest is consulting with employees on the siteâ€™s future, and 72\ndepots. Muller bought Robert Wiseman in 2012.\nReference: Dairy Crest, maker of Cathedral City cheese and Country Life butter, has announced a big slump in proï¬ts\nand the sale of its milk business.\nPACT:More than than more more more than more than than than to be be be will will will be be are are are be be to the.\nLSQ: Dairy Crest is to sell its Dairies business to a German company for an undisclosed sum.\nLAQ: Dairy giant Dairy Crest is to sell its UK business to a German company for an undisclosed sum.\nOurs: Dairy Crest, the worldâ€™s largest dairy producer, is to sell its UK operations to a German ï¬rm.\nTable 10: Example summaries generated by 2-bit BART quantized with different methods.\n4834\nToken memory bank ğ‘½ğ‘½ğ’ƒğ’ƒ\nğ’•ğ’•ğŸğŸ ğ’•ğ’•ğ’ğ’\nEmbedding \nLayer\nEmbedding \nLayer\nâ€¦â€¦â€¦â€¦\nLinear Layer\nNext utterance prediction\nLanguage Modeling\nğ’„ğ’„ğ’„ğ’„ğ’ğ’ğ’„ğ’„ğ’„ğ’„ğ’„ğ’„ğ’„ğ’„ğ’•ğ’•ğ’„ğ’„ğ’„ğ’„ğŸğŸ\nğ’•ğ’•ğŸğŸ ğ’•ğ’•ğ’ğ’â€¦â€¦ ğ’„ğ’„ğ’„ğ’„ğ’ğ’ğ’„ğ’„ğ’„ğ’„ğ’„ğ’„ğ’„ğ’„ğ’•ğ’•ğ’„ğ’„ğ’„ğ’„ğ’ğ’\nContext\nâ€¦\nupdate\nindex\nğ’’ğ’’ğ’•ğ’•ğŸğŸ\nğ’„ğ’„ ğ’’ğ’’ğ’•ğ’•ğŸğŸ\nğ’„ğ’„ ğ’’ğ’’ğ’•ğ’•ğ’ğ’\nğ’„ğ’„\nâ€¦â€¦\nğ’‰ğ’‰ğ’•ğ’•ğŸğŸ\nğ’•ğ’• ğ’‰ğ’‰ğ’•ğ’•ğŸğŸ\nğ’•ğ’• ğ’‰ğ’‰ğ’•ğ’•ğ’ğ’\nğ’•ğ’•â€¦â€¦\nToken-level Contrastive Distillation \nTransformer \nLayer 1\nTransformer \nLayer L\nQuantized Student Network\nPull together\nPush away\nâ„“ğ’„ğ’„ğ’„ğ’„ğ’ğ’ğ’„ğ’„\nFigure 9: The training workï¬‚ow of the proposed method for double-head GPT-2 quantization in the task of next\nutterance prediction. The model is trained to ï¬nd the correct candidate. The full-precision teacher network and\ndistillation loss Ldist are omitted for simplicity.\nEmbedding \nLayer\nğ’•ğ’•ğŸğŸ ğ’•ğ’•ğ’ğ’â€¦â€¦\nArticle\nâ€¦â€¦\nEmbedding \nLayer\nQ-Embedding \nLayer\nâ€¦â€¦\nLanguage Modelingğ’•ğ’•ğŸğŸ ğ’•ğ’•ğ’ğ’â€¦â€¦\nSummary\nupdate\nToken memory bank ğ‘½ğ‘½ğ’ƒğ’ƒ\nğ’’ğ’’ğ’•ğ’•ğŸğŸ\nğ’”ğ’” ğ’’ğ’’ğ’•ğ’•ğŸğŸ\nğ’”ğ’” ğ’’ğ’’ğ’•ğ’•ğ’ğ’\nğ’”ğ’”\nâ€¦â€¦\nğ’‰ğ’‰ğ’•ğ’•ğŸğŸ\nğ’•ğ’• ğ’‰ğ’‰ğ’•ğ’•ğŸğŸ\nğ’•ğ’• ğ’‰ğ’‰ğ’•ğ’•ğ’ğ’\nğ’•ğ’•â€¦â€¦\nTransformer \nLayer 1\nTransformer \nLayer L\nQuantized Student Network\nTransformer \nLayer 1\nTransformer \nLayer L\nindex\nPull together\nPush away\nâ„“ğ’„ğ’„ğ’„ğ’„ğ’ğ’ğ’”ğ’”\nToken-level Contrastive Distillation \nFigure 10: The training workï¬‚ow of the proposed method for BART quantization in the task of abstractive\nsummarization. The full-precision teacher network and distillation loss Ldist are omitted for simplicity.\nare similar to those in Section 2.2.\n4835\n(a) \"there is no asbestos in our products now\"\n(b) \"cray computer has applied to trade on nasdaq\"\n(c) \"no price for the new shares has been set\"\n(d) \"the centers normally are closed through the weekend\"\nFigure 11: More Visualizations: matrices representing the cosine similarities between representations of all pairs of\ntokens in a sentence, between the full-precision model and 2-bit quantized models trained on PTB dataset. Token\nrepresentations at the last decoder layer of GPT-2 are used.\n4836"
}