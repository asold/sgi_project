{
  "title": "Superpoint Transformer for 3D Scene Instance Segmentation",
  "url": "https://openalex.org/W4382457953",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2113292066",
      "name": "Jiahao Sun",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2158968104",
      "name": "Chunmei Qing",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2737860765",
      "name": "Junpeng Tan",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2100518092",
      "name": "Xiangmin Xu",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2113292066",
      "name": "Jiahao Sun",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2158968104",
      "name": "Chunmei Qing",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2737860765",
      "name": "Junpeng Tan",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2100518092",
      "name": "Xiangmin Xu",
      "affiliations": [
        "South China University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6718544293",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3187637097",
    "https://openalex.org/W3217112505",
    "https://openalex.org/W3180659539",
    "https://openalex.org/W4221162716",
    "https://openalex.org/W2594519801",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W3014777739",
    "https://openalex.org/W6796087570",
    "https://openalex.org/W2769297824",
    "https://openalex.org/W6800693181",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W3107598724",
    "https://openalex.org/W6757204575",
    "https://openalex.org/W6760232427",
    "https://openalex.org/W3014878043",
    "https://openalex.org/W6689029123",
    "https://openalex.org/W2952515999",
    "https://openalex.org/W2769473888",
    "https://openalex.org/W3194793527",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2432481613",
    "https://openalex.org/W2918913021",
    "https://openalex.org/W2990295915",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W2624503621",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6810213675",
    "https://openalex.org/W3042489604",
    "https://openalex.org/W2948410735",
    "https://openalex.org/W2904851770",
    "https://openalex.org/W2989780045",
    "https://openalex.org/W3035748168",
    "https://openalex.org/W4292692470",
    "https://openalex.org/W2963182550",
    "https://openalex.org/W3034949383",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W3042283226",
    "https://openalex.org/W3137905681",
    "https://openalex.org/W3172351327",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W3204034406",
    "https://openalex.org/W2904332125",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4312274934",
    "https://openalex.org/W2460657278",
    "https://openalex.org/W4214654781",
    "https://openalex.org/W4312460151",
    "https://openalex.org/W3213165621",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4312815172",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962759414",
    "https://openalex.org/W2995182785",
    "https://openalex.org/W3004300126",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W3170500059",
    "https://openalex.org/W3034430142",
    "https://openalex.org/W4214773923",
    "https://openalex.org/W2963281829"
  ],
  "abstract": "Most existing methods realize 3D instance segmentation by extending those models used for 3D object detection or 3D semantic segmentation. However, these non-straightforward methods suffer from two drawbacks: 1) Imprecise bounding boxes or unsatisfactory semantic predictions limit the performance of the overall 3D instance segmentation framework. 2) Existing method requires a time-consuming intermediate step of aggregation. To address these issues, this paper proposes a novel end-to-end 3D instance segmentation method based on Superpoint Transformer, named as SPFormer. It groups potential features from point clouds into superpoints, and directly predicts instances through query vectors without relying on the results of object detection or semantic segmentation. The key step in this framework is a novel query decoder with transformers that can capture the instance information through the superpoint cross-attention mechanism and generate the superpoint masks of the instances. Through bipartite matching based on superpoint masks, SPFormer can implement the network training without the intermediate aggregation step, which accelerates the network. Extensive experiments on ScanNetv2 and S3DIS benchmarks verify that our method is concise yet efficient. Notably, SPFormer exceeds compared state-of-the-art methods by 4.3% on ScanNetv2 hidden test set in terms of mAP and keeps fast inference speed (247ms per frame) simultaneously. Code is available at https://github.com/sunjiahao1999/SPFormer.",
  "full_text": "Superpoint Transformer for 3D Scene Instance Segmentation\nJiahao Sun1, Chunmei Qing1*, Junpeng Tan1, Xiangmin Xu2,\n1 School of Electronic and Information Engineering, South China University of Technology, China\n2 School of Future Technology, South China University of Technology, China\neesjh@mail.scut.edu.cn, qchm@scut.edu.cn, tjeepscut@gmail.com, xmxu@scut.edu.cn\nAbstract\nMost existing methods realize 3D instance segmentation by\nextending those models used for 3D object detection or 3D\nsemantic segmentation. However, these non-straightforward\nmethods suffer from two drawbacks: 1) Imprecise bounding\nboxes or unsatisfactory semantic predictions limit the perfor-\nmance of the overall 3D instance segmentation framework.\n2) Existing methods require a time-consuming intermediate\nstep of aggregation. To address these issues, this paper pro-\nposes a novel end-to-end 3D instance segmentation method\nbased on Superpoint Transformer, named as SPFormer. It\ngroups potential features from point clouds into superpoints,\nand directly predicts instances through query vectors with-\nout relying on the results of object detection or semantic seg-\nmentation. The key step in this framework is a novel query\ndecoder with transformers that can capture the instance in-\nformation through the superpoint cross-attention mechanism\nand generate the superpoint masks of the instances. Through\nbipartite matching based on superpoint masks, SPFormer can\nimplement the network training without the intermediate ag-\ngregation step, which accelerates the network. Extensive ex-\nperiments on ScanNetv2 and S3DIS benchmarks verify that\nour method is concise yet efficient. Notably, SPFormer ex-\nceeds compared state-of-the-art methods by 4.3% on Scan-\nNetv2 hidden test set in terms of mAP and keeps fast infer-\nence speed (247ms per frame) simultaneously. Code is avail-\nable at https://github.com/sunjiahao1999/SPFormer.\n1 Introduction\n3D scene understanding regards as a fundamental ingredient\nfor many applications, including augmented/virtual reality\n(Park et al. 2020), autonomous driving (Zhou et al. 2020),\nand robotics navigation (Xie et al. 2021). Generally, instance\nsegmentation is a challenging task in 3D scene understand-\ning, which aims to not only detect instances on sparse point\nclouds but also give a clear mask for each instance.\nExisting state-of-the-art methods can be divided into\nproposal-based (Yang et al. 2019; Liu et al. 2020) and\ngrouping-based (Jiang et al. 2020; Chen et al. 2021; Liang\net al. 2021; Vu et al. 2022). Proposal-based methods con-\nsider 3D instance segmentation as a top-down pipeline.\nThey firstly generate region proposals (i.e. bounding box),\n*Corresponding author: Chunmei Qing\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Input Point Cloud\n (b) Proposal-based\n(c) Grouping-based\n (d) Ours\nFigure 1: Key process of different methods. (a) is an input\npoint cloud. (b) Proposal-based methods detect objects first.\n(c) Grouping-based methods offset points to their own in-\nstance center and group points. (d) Our method highlights\nthe region of interest by superpoint cross-attention.\nas shown in Fig. 1(b), and then predict instance masks in the\nproposed region. These methods are encouraged by the big\nsuccess of Mask-RCNN (He et al. 2017) on 2D instance seg-\nmentation fields. However, these methods struggle on point\nclouds due to domain gaps. In 3D fields, bounding box has\nmore degree of freedom (DoF) increasing the difficulty of\nfitting. Moreover, points usually only exist on parts of ob-\nject surface, which causes object geometric centers to be not\ndetectable. Besides, low-quality region proposals affect box-\nbased bipartite matching (Yang et al. 2019) and further de-\ngrade model performance.\nOn the contrary, grouping-based methods adopt a bottom-\nup pipeline. They learn point-wise semantic labels and in-\nstance center offsets. Then they use the offsetted points and\nsemantic predictions to aggregate into instances, as shown in\nFig. 1(c). Over the past two years, grouping-based methods\nhave achieved great improvements in 3D instance segmenta-\ntion task (Liang et al. 2021; Vu et al. 2022). However, there\nalso are several shortcomings: (1) grouping-based methods\ndepend on their semantic segmentation results, which might\nlead to wrong predictions. Propagating these wrong predic-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n2393\ntions to subsequent processing suppresses the performance\nof network. (2) These methods need an intermediate aggre-\ngation step increasing training and inference time. The ag-\ngregation step is independent of network training and lack of\nsupervision, which needs an additional refinement module.\nWith the discussion above, we naturally think about a hy-\nper framework that can avoid drawbacks and take benefits\nfrom two types of methods simultaneously. In this paper, we\nproposed a novel end-to-end two-stage 3D instance segmen-\ntation method based on Superpoint Transformer, named as\nSPFormer. SPFormer groups bottom-up potential features\nfrom point clouds into superpoints and proposes instances\nby query vectors as a top-down pipeline.\nIn the bottom-up grouping stage, a sparse 3D U-net is\nutilized to extract bottom-up point-wise features. A sim-\nple superpoint pooling layer is presented to group potential\npoint-wise features into superpoints. Superpoints (Landrieu\nand Simonovsky 2018) can leverage the geometric regular-\nities to represent homogeneous neighboring points. In con-\ntrast to previous method (Liang et al. 2021), our superpoint\nfeatures are potential, which avoid supervising the features\nthrough non-straightforward semantic and central distance\nlabels. We consider superpoints as a potential mid-level rep-\nresentation of 3D scenes and directly use instance labels to\ntrain the whole network. In the top-down proposal stage, a\nnovel query decoder with transformers is proposed. We uti-\nlize learnable query vectors to propose instance prediction\nfrom potential superpoint features as a top-down pipeline.\nThe learnable query vector can capture instance information\nthrough superpoint cross-attention mechanism. Fig. 1(d) il-\nlustrates this process that the redder the part of the chair is,\nthe more attention of query vector pays. With the query vec-\ntors carrying instance information and superpoint features,\nquery decoder directly generates instance class, score, and\nmask predictions. Finally, through bipartite matching based\non superpoint masks, SPFormer can implement end-to-end\ntraining without time-consuming aggregation step. Besides,\nSPFormer is free of post-processing like non-maximum sup-\npression (NMS), which further accelerates the speed of net-\nwork.\nSPFormer achieves state-of-the-art on both ScanNetv2\nand S3DIS benchmarks. Especially, SPFormer exceeds com-\npared state-of-the-art methods by qualitative and quanti-\ntative measures, and inference speed, simultaneously. SP-\nFormer with a novel pipeline can be served as a general\nframework for 3D instance segmentation. In summary, our\ncontributions are listed as follows:\n• We propose a novel end-to-end two-stage method named\nSPFormer that represents 3D sence with potential super-\npopint features without relying on the results of object\ndetection or semantic segmentation.\n• We design a query decoder with transformers where\nlearnable query vectors can capture instance information\nby superpoint cross-attention. With query vectors, query\ndeocoder can directly generate instance predictions.\n• Through bipartite matching based on superpoint masks,\nSPFormer can implement the network training without\ntime-consuming intermediate aggregation step and be\nfree of complex post-processing during inference.\n2 Related Work\nProposal-based Methods. Proposal-based methods take\na top-down pipeline for instance segmentation. Previous\nmethods (Yi et al. 2019; Hou, Dai, and Nießner 2019; Narita\net al. 2019) focus on fusing 2D image features with point\ncloud features into a volumetric grid and generate region\nproposals from the grid. 3D-BoNet (Yang et al. 2019) uses\nPointNet++ (Qi et al. 2017a,b) extracting features from point\nclouds and treats 3D bounding box generation task as an\noptimal assignment problem. GICN (Liu et al. 2020) pre-\ndicts Gaussian heatmap to select instance center candidates\nand produces instance masks within the proposed bounding\nboxes. 3D-MPA (Engelmann et al. 2020) samples predicted\ncentroids and cluster points near the centroids to form final\ninstance masks. Most proposal-based methods are based on\n3D bounding boxes. However, low-quality bounding boxes\npredictions will affect the performance of the instance seg-\nmentation model.\nGrouping-based Methods. Grouping-based methods re-\ngard 3D instance segmentation as a bottom-up pipeline.\nMTML (Lahoud et al. 2019) utilizes a multi-task strat-\negy to learn feature embedding. PointGroup (Jiang et al.\n2020) aggregates points from original and center-shifted\npoint clouds and designs ScoreNet for evaluating the qual-\nity of aggregation. PE(Zhang and Wonka 2021) introduces\na novel probabilistic embedding space. Dyco3D(He, Shen,\nand van den Hengel 2021) introduces dynamic convolution\nkernels. HAIS (Chen et al. 2021) extends PointGroup with\na hierarchical aggregation and filters noisy points within in-\nstance prediction. SSTNet (Liang et al. 2021) constructs a\nsemantic superpoint tree and gains instance prediction by\nsplitting non-similar nodes. SoftGroup (Vu et al. 2022) uses\na lower threshold for clustering to address the wrong se-\nmantic hard prediction and refines instances with a tiny 3D\nU-net. Although, grouping-based methods may have a top-\ndown refinement module, they still inevitably rely on inter-\nmediate aggregation step.\n2D Instance Segmentation with Transformer.Recently,\ntransformer (Vaswani et al. 2017) is introduced in image\nclassification (Dosovitskiy et al. 2020; Touvron et al. 2021;\nLiu et al. 2021), object detection (Carion et al. 2020; Dai\net al. 2021) and segmentation (Cheng, Schwing, and Kir-\nillov 2021; Cheng et al. 2022a; Guo et al. 2021). There are\nalso some instance segmentation methods (Fang et al. 2021;\nCheng et al. 2022b) inspired by transformer. Mask2Former\n(Cheng et al. 2022a) successfully applies transformer to\nbuild a universal network for 2D image semantic, instance,\nand panoptic segmentation.\nInspired by the success of transformer for 2D segmen-\ntation tasks, we are motivated to introduce transformer for\n3D instance segmentation. However, transformer cannot be\nnaively applied on the output of sparse convolution back-\nbone, because it will introduce highly computational over-\nhead because of the complexity of attention mechanism. In\nthis paper, we will design a novel query decoder for 3D\n2394\nSparse 3D U-net\nQuery Decoder\nInference\nTrainingTransformer \nDecoder\nMLPMask\nInstance\nM x C Query\nFinal Score\nRanking\nM x D\nK x D\nSuperpoint\nPooling\nP\nS\n: N x 6\nsuperpoints\nBipartite Matching\nBottom-up Grouping Stage\nTop-down Proposal Stage\nClass\nScore\nPrediction Head\nMask\ntraining only\nattention visualization\nmatched pair\nFigure 2: The overall architecture of SPFormer, which contains two stages. In the bottom-up grouping stage, sparse 3D U-net\nextracts point-wise features from input point cloud P, and then superpoint pooling layer groups homogeneous neighboring\npoints into superpoint features S. In the top-down proposal stage, the query decoder is divided into two branches. The instance\nbranch obtains query vector featuresZℓ by transformer decoder. The mask branch extracts mask-aware featuresSmask. Finally,\na prediction head generates instance predictions and feeds them into bipartite matching or ranking during training/inference.\ninstance segmentation and employ superpoints to build a\nbridge between the backbone and query decoder.\n3 Method\nThe architecture of the proposed SPFormer is illustrated\nin Fig. 2. Firstly, a sparse 3D U-net is utilized to extract\nbottom-up point-wise features. A simple superpoint pooling\nlayer is presented to group potential point-wise features into\nsuperpoints. Secondly, a novel query decoder with trans-\nformers is proposed, where learnable query vectors can cap-\nture instance information by superpoint cross-attention. Fi-\nnally, through bipartite matching based on superpoint masks,\nSPFormer can implement end-to-end training without time-\nconsuming aggregation step.\n3.1 Backbone and Superpoints\nSparse 3D U-net. Assuming that the input point cloud\nhas N points, the input can be expressed as P ∈ RN×6.\nEach Point has colors r,g,b and coordinates x,y,z. Follow-\ning previous implementation (Graham, Engelcke, and Van\nDer Maaten 2018), we voxelize point cloud for regular in-\nput and use a U-net style backbone composed of subman-\nifold sparse convolution (SSC) or sparse convolution (SC)\nto extract point-wise features P′ ∈ RN×C. We give the\nsparse 3D U-net specifics in the supplementary material.\nDifferent from the common grouping-based methods, our\nmethod does not add an additional semantic branch and off-\nset branch.\nSuperpoint pooling layer. To build an end-to-end frame-\nwork, we directly feed point-wise features P′ ∈ RN×C into\nsuperpoint pooling layer based on pre-computed superpoints\n(Landrieu and Simonovsky 2018). Superpoint pooling layer\nsimply obtains superpoint features S ∈ RM×C via aver-\nage pooling over those point-wise ones inside each of super-\npoints. Without loss of generality, we suppose that there are\nM superpoints computed from the input point cloud. No-\ntably, superpoint pooling layer reliably downsample input\npoint cloud to hundreds of superpoints, which significantly\nreduces the computational overhead of subsequent process-\ning and optimizes the representation capability of the entire\nnetwork.\n3.2 Query Decoder\nQuery decoder consists of instance branch and mask branch.\nIn the mask branch, a simple Multi-Layer Perceptron (MLP)\naims to extract the mask-aware features Smask ∈ RM×D.\nThe instance branch is composed of a series of transformer\ndecoder layers. They decode learnable query vectors via\nsuperpoint cross-attention. Assume there are K learnable\nquery vectors. We predefine the features of query vectors\nfrom each transformer decoder layer as Zℓ ∈ RK×D. D is\nembedding dimension and ℓ = 1,2, 3... is layer index.\nSuperpoint Cross-Attention. Considering the disorder\nand quantity uncertainty of superpoint, transformer struc-\nture is introduced to handle variable length input. The\npotential feature of superpoints and the learnable query\nvectors are used as the input of the transformer decoder.\nThe detailed architecture of our modified transformer de-\ncoder layer is depicted in Fig. 3. Inspired by (Cheng et al.\n2022a), query vectors are initialized randomly before train-\ning, and the instance information of each point cloud can\nonly be obtained through superpoint cross-attention, there-\nfore, our transformer decoder layer exchanges the order of\nself-attention layer and cross-attention layer compared with\nthe standard one(Vaswani et al. 2017). In addition, because\nthe input is the potential features of superpoints, we empiri-\ncally remove position embedding.\nWith superpoint features after linear projection S′ ∈\nRM×D, query vectors from former layer Zℓ−1 capture con-\ntext information via superpoint cross-attention mechanism,\n2395\nwhich can be formulated as:\nˆZℓ = softmax(QKT\n√\nD\n+ Aℓ−1)V, (1)\nwhere ˆZℓ ∈ RK×D is the output of superpiont cross-\nattention. Q = ψQ(Zℓ−1) ∈ RK×D is the linear projec-\ntion of input query vectors Zℓ−1 and K, V is superpoint\nfeatures S′ with different linear projection ψK(·), ψV (·) re-\nspectively. Aℓ−1 ∈ RK×M is superpoint attention masks.\nGiven the predicted superpoint masks Mℓ−1 from the for-\nmer prediction head, superpoint attention masks Aℓ−1 filter\nsuperpoint with a threshold τ, as\nAℓ−1(i, j) =\n\u001a\n0 if Mℓ−1(i, j) ≥ τ\n−∞ otherwise . (2)\nAℓ−1(i, j) indicates i-th query vector attending to j-th su-\nperpoint where Mℓ−1(i, j) is higher than τ. Empirically, we\nset τ to 0.5. With transformer decoder layer stacking, su-\nperpoint attention masks Aℓ−1 adaptively constrain cross-\nattention within the foreground instance.\nShared Prediction Head. With query vectors Zℓ from in-\nstance branch, we use two independent MLPs to predict the\nclassification {pi ∈ RNclass+1}K\ni=1 of each query vector\nand evaluate the quality of proposals with IoU-aware score\n{si ∈ [0, 1]}K\ni=1 respectively. Specifically, we append pre-\ndiction with “no instance” probability in addition to Nclass\ncategories in order to assign ground truth to the proposals by\nbipartite matching and treat the other proposals as negative\npredictions. Moreover, the ranking of proposals profoundly\naffects instance segmentation results, while in practice most\nproposals will be regarded as background due to one-to-one\nmatching style, which causes the misalignment of proposal\nquality ranking. Thus, We design a score branch that esti-\nmates the IoU of predicted superpoint masks and ground\ntruth ones to compensate for the misalignment.\nBesides, given the mask-aware features Smask ∈ RM×D\nfrom mask branch, we directly multiply it by query vec-\ntors Zℓ followed a sigmoid function to generate superpoint\nmasks prediction Mℓ ∈ [0, 1]K×M .\nIterative Prediction. Considering the slow convergence\nof transformer-based model (Carion et al. 2020), we feed ev-\nery transformer decoder layer output Zℓ into the shared pre-\ndiction head to generate proposals. Specially, we define Z0\nto be the query vectors that have not captured instance infor-\nmation with 3D scene yet. we also feed Z0 into the shared\nprediction head, even if it is equivariant for any 3D scene.\nDuring training, we assign ground truth to all output from\nthe shared prediction head with different layer input Zℓ. we\nfind that it will improve the performance of model and query\nvectors feature will be updated layer by layer. We only use\nthe output of the last prediction head for final instance pro-\nposals during inference, which can avoid the redundancy of\nproposals and accelerate inference speed.\n3.3 Bipartite Matching and Loss Function\nWith a fixed number of proposals, we formulate ground truth\nlabel assignment as an optimal assignment problem. For-\nmally, we introduce a pairwise matching cost Cik to eval-\nuate the similarity of the i-th proposal and the k-th ground\nSuperpoint Cross-Attention\nSelf-Attention\nFeed Forward\nAdd & Norm\nAdd & Norm\nAdd & Norm\nQuery\nShared\nPrediction Head\nSuperpoint\nFeature\nAttention\nMask\nAttention\nMask\nFigure 3: The architecture of transformer decoder layer and\niterative prediction process. Here omits the branch that feeds\nthe output query features Zℓ to the next layer for readability.\ntruth. Cik is determined by classification probability and su-\nperpoint mask matching cost Cmask\nik , as defined in Eq. (3).\nCik = −λcls · pi,ck + λmask · Cmask\nik , (3)\nwhere pi,ck indicates the probability for the category ck\nof i-th proposal and λcls, λmask are corresponding coeffi-\ncients of each term. In our experiments, we set λcls = 0.5,\nλmask = 1. Superpoint mask matching costCmask\nik consists\nof binary cross-entropy (BCE) and dice loss with Laplace\nsmoothing (Milletari, Navab, and Ahmadi 2016), as\nCmask\nik = BCE(mi, mgt\nk ) + 1− 2 mi · mgt\nk + 1\n|mi| +\n\f\fmgt\nk\n\f\f + 1\n, (4)\nwhere mi and mgt\nk are the superpoint mask of proposal and\nground truth respectively. We assign a hard instance label\nto each superpoint depending on whether more than half of\nthe points within the superpoint belong to the instance. With\nthe matching cost Cik, we use Hungarian algorithm (Kuhn\n1955) to find the optimal matching between proposals and\nground truth.\nAfter assignment, we treat the proposals that are not as-\nsigned to ground truth as ”no instance” class and compute\nthe classification cross-entropy loss Lcls for every proposal.\nThen we compute the superpoint mask loss which consists of\nbinary cross-entropy loss Lbce and dice loss Ldice for each\nproposal ground truth pair. In addition, we add the following\nL2 loss Ls for the score branch:\nLs = 1\nPNgt\nk=1 1{iouk}\nNgtX\nk=1\n1{iouk} ∥sk − iouk∥2 , (5)\nwhere {sk}Ngt\nk=1 is the set of score predictions that are as-\nsigned to Ngt ground truth. 1{iouk} indicates whether the\nIoU between proposal mask prediction and assigned ground\ntruth is higher than 50%. We only use high-quality propos-\nals for supervision (Huang et al. 2019). Finally, to build an\n2396\nMethod mAP\nbath\nbed\nbkshf\ncabinet\nchair\ncounter\ncurtain\ndesk\ndoor\nother\npicture\nfridge\ns. cur\n.\nsink\nsofa\ntable\ntoilet\nwind.\n3D-BoNet 25.3 51.9 32.4\n25.1 13.7 34.5 3.1 41.9 6.9 16.2 13.1 5.2 20.2 33.8 14.7 30.1 30.3 65.1 17.8\nMTML 28.2 57.7 38.0\n18.2 10.7 43.0 0.1 42.2 5.7 17.9 16.2 7.0 22.9 51.1 16.1 49.1 31.3 65.0 16.2\nGICN 34.1 58.0 37.1\n34.4 19.8 46.9 5.2 56.4 9.3 21.2 21.2 12.7 34.7 53.7 20.6 52.5 32.9 72.9 24.1\n3D-MPA 35.5 45.7 48.4\n29.9 27.7 59.1 4.7 33.2 21.2 21.7 27.8 19.3 41.3 41.0 19.5 57.4 35.2 84.9 21.3\nDyco3D 39.5 64.2 51.8\n44.7 25.9 66.6 5.0 25.1 16.6 23.1 36.2 23.2 33.1 53.5 22.9 58.7 43.8 85.0 31.7\nPE 39.6 66.7 46.7\n44.6 24.3 62.4 2.2 57.7 10.6 21.9 34.0 23.9 48.7 47.5 22.5 54.1 35.0 81.8 27.3\nPointGroup 40.7 63.9 49.6\n41.5 24.3 64.5 2.1 57.0 11.4 21.1 35.9 21.7 42.8 66.6 25.6 56.2 34.1 86.0 29.1\nHAIS 45.7 70.4 56.1\n45.7 36.4 67.3 4.6 54.7 19.4 30.8 42.6 28.8 45.4 71.1 26.2 56.3 43.4 88.9 34.4\nOccuSeg 48.6 80.2 53.6 42.8\n36.9 70.2 20.5 33.1 30.1 37.9 47.4 32.7 43.7 86.2 48.5 60.1 39.4 84.6 27.3\nSoftGroup 50.4 66.7 57.9\n37.2 38.1 69.4 7.2 67.7 30.3 38.7 53.1 31.9 58.2 75.4 31.8 64.3 49.2 90.7 38.8\nSSTNet 50.6 73.8 54.9 49.7 31.6\n69.3 17.8 37.7 19.8 33.0 46.3 57.6 51.5 85.7 49.4 63.7 45.7 94.3 29.0\nSPFormer 54.9 74.5 64.0 48.4 39.5 73.9\n31.1 56.6 33.5 46.8 49.2 55.5 47.8 74.7 43.6 71.2 54.0 89.3 34.3\nTable 1: 3D instance segmentation results on ScanNetv2 hidden test set. Reported results are obtained from the ScanNet bench-\nmark testing server on 11/07/2022.\nend-to-end training, we adopt multi-task loss L, as\nL = βcls · Lcls + βs · Ls + βmask · (Lbce + Ldice), (6)\nwhere βcls, βs, βmask are corresponding coefficients of each\nterm. Empirically, we set βcls = βs = 0.5, βmask = 1.\n3.4 Inference\nDuring inference, given an input point cloud, SPFormer di-\nrectly predicts K instances with classification {pi}, IoU-\naware score {si} and corresponding superpoint masks. We\nadditionally obtain a mask score {msi ∈ [0, 1]}K by aver-\naging superpoints probability higher than 0.5 in each super-\npoint mask. The final score for sorting esi = 3\n√\npi · si · msi.\nSPFormer is free of non-maximum suppression in post-\nprocessing, which ensures its fast inference speed.\n4 Experiments\nDatasets. Experiments are conducted on ScanNetv2 (Dai\net al. 2017) and S3DIS (Armeni et al. 2016) datasets. Scan-\nNetv2 has a total of 1613 indoor scenes, of which 1201 are\nused for training, 312 for validation, and 100 for testing. It\ncontains 18 categories of object instances. We submit the fi-\nnal prediction of our method to its hidden test set and the\nablation studies are conducted on its validation set. S3DIS\nhas 6 large-scale areas with 272 scenes in total. It has 13 cat-\negories for instance segmentation task. We follow two com-\nmon settings for evaluation: testing on Area 5 and 6-fold\ncross-validation.\nEvaluation Metrics. Task-mean average precision (mAP)\nis utilized as the common evaluation metric for instance seg-\nmentation, which averages the scores with IoU thresholds\nset from 50% to 95%, with a step size of 5%. Specifically,\nAP50 and AP 25 denote the scores with IoU thresholds of\n50% and 25%, respectively. We report mAP, AP50 and AP25\non ScanNetv2 dataset and we addtionally report mean pre-\ncision (mPrec), and mean recall (mRec) on S3DIS dataset.\n4.1 Benchmark Results\nScanNetv2. SPFormer is compared with existing state-of-\nthe-art methods on the hidden test set, as shown in Table 1.\nSPFormer accomplishes the highest mAP score of 54.9%,\noutperforming the previous best result by 4.3%. For the spe-\ncific 18 categories, our model achieves the highest AP scores\non 8 of them. Especially, SPFormer surpasses the previous\nbest AP score by more than 10% in the counter category,\nwhere past methods are always hard to achieve a satisfac-\ntory score.\nWe also evaluate SPFormer on ScanNetv2 validation set,\nas shown in Table 2. SPFormer outperforms all state-of-the-\nart methods by a large margin. Compared to the second-best\nresults, our method improves 6.9%, 6.3%, 4.0% in terms of\nmAP, AP50 and AP25, respectively.\nS3DIS. We evaluate SPFormer on S3DIS using Area 5 and\n6-fold cross-validation, respectively. As shown in Table 3,\nSPFormer achieves the-state-of-art results in terms ofAP50.\nFollowing the protocols used in previous methods, we addi-\ntionally report mPrec and mRec. Our method also achieves\ncompetitive results in mPrec/mRec metrics. The results on\nS3DIS confirm the generalization ability of SPFormer.\nRuntime Analysis. We test the runtime per scene of dif-\nferent methods on ScanNetv2 validation set, as shown in Ta-\nble 4. For a fair comparison, the SSC and SC layers in all\nthe above methods are implemented by spconv v2.1. We re-\nport in detail the running time of the components of each\nmethod (the last part of each model contains their own post-\nprocessing). Since our SPFormer and (Liang et al. 2021) is\nbased on superpoints, here we add superpoints extraction\n(s.p. extraction) runtime to test the inference speed from raw\ninput point clouds. However, superpoints can pre-compute\nin training stage, which can significantly reduce the model\ntraining time. Even with superpoints extraction, SPFormer\nis still the fastest method compared to the existing ones.\n4.2 Ablation Study\nComponents Analysis. Table 5 shows the performance\nresults when different components are omitted. Considering\nnaively feeding the output of backbone into query decoder,\nwe find that there is a huge drop in performance. Query\nvectors can not attend to several hundred thousand points\ndue to the softmax process in cross-attention. We employ\n2397\nMethod mAP AP 50 AP25\nPE 33.0 57.1 73.8\nPointGroup 35.2 57.1 71.4\n3D-MPA 35.3 59.1 72.4\nDyco3D 35.4 57.6 72.9\nHAIS 44.1 64.4 75.7\nSoftGroup 46.0 67.6 78.9\nSSTNet 49.4 64.3 74.0\nSPFormer 56.3 73.9 82.9\nTable 2: 3D instance segmentation results on ScanNetv2 val-\nidation set.\nMethod AP50 mPrec mRec\nPointGroup 57.8 55.3 42.4\nDyco3D - 64.3 64.2\nSSTNet 59.3 65.5 64.2\nHAIS - 71.1 65.0\nSoftGroup 66.1 73.6 66.6\nSPFormer 66.8 72.8 67.1\n3D-BoNet† - 65.6 47.7\nGICN† - 68.5 50.8\nPointGroup† 64.0 69.6 69.2\nSSTNet† 67.8 73.5 73.4\nHAIS† - 73.2 69.4\nSoftGroup† 68.9 75.3 69.8\nSPFormer† 69.2 74.0 71.1\nTable 3: 3D instance segmentation results on the S3DIS val-\nidation set. Methods marked without† are evaluated on Area\n5; methods marked with † are evaluated on 6-fold cross-\nvalidation.\nsuperpoints to build a bridge between backbone and query\ndecoder, which significantly improves our method perfor-\nmance. Then we discuss the bipartite matching target. We\ncompare matching by boxes with matching by masks. The\ndetail of the implementation of matching by boxes is in the\nsupplementary material. We find the performance of match-\ning by mask exceeds box one by 6.4% on mAP. 3D boxes\nhave more DoF than 2D ones and object geometric centers\nare usually not detectable, which inevitably makes matching\nmore difficult. Finally, we confirm IoU-aware score branch\nbrings benefits to our method. It takes +1.3/1.5/0.4 improve-\nments on mAP/AP 50/AP25 respectively. The score branch\nmitigates the misalignment of proposal quality ranking.\nThe Architecture of Transformer. The ablation analy-\nsis of the architecture of transformer is illustrated in Ta-\nble 6. Considering the original transformer decoder layer\n(Vaswani et al. 2017) without position encoding as base-\nline, iteratively predicting on each transformer layer by the\nshared prediction head can bring +1.5/1.8/1.8 improvement\non mAP/AP50/AP25 respectively. Moreover, if we add su-\nperpoint attention masks, our method will further improve\n+3.8/2.4/1.3 performance. Superpoint attention masks allow\nMethod Component\ntime (ms) Total (ms)\nPointGroup\nBackbone(GPU):48\n372Grouping(GPU+CPU):218\nScoreNet(GPU):106\nHAIS\nBackbone(GPU):50\n256Hier\n. aggr.(GPU+CPU): 116\nIntra-inst refinement(GPU): 90\nSoftGroup\nBackbone(GPU):48\n266Soft\ngrouping(GPU+CPU):121\nTop-down refinement(GPU):97\nSSTNet\nS.p. e\nxtraction(CPU):179\n419Backbone(GPU):34\nTree Network(GPU+CPU):148\nScoreNet(GPU):58\nSPFormer\nS.p.\nextraction(CPU):179\n247Backbone(GPU):29\nS.p. pooling(GPU):18\nQuery decoder(GPU):21\nTable 4: Inference time per scan of different methods on\nScanNetv2 validation set. The runtime is measured on the\nsame RTX 3090 GPU.\nSuperpoint Pooling\nMatching Method Score mAP AP50 AP25\nmask 34.3 54.7\n72.9\n✓ box 49.9 68.1\n78.9\n✓ mask 55.0 72.4\n82.5\n✓ mask ✓ 56.3 73.9\n82.9\nTable 5: Components analysis on ScanNetv2 validation set.\nIterativ\ne\nprediction\nAttention\nmask\nPosition\nencoding\nCross-attention\nfirst mAP AP50 AP25\n51.0 69.6\n79.8\n✓ 52.5 71.4\n81.6\n✓ ✓ 56.0 73.3\n82.6\n✓ ✓ ✓ 55.6 72.7\n82.0\n✓ ✓ ✓ 56.3 73.9\n82.9\nTable 6: Ablation study on the architecture of transformer.\nSPFormer to only attend to the foreground from the former\nlayer predictions. Due to the uncertainty of the number of\nsuperpoints in each scene, we only discuss whether use po-\nsition encoding on query vectors. We add position encoding\nwhere query vectors are fed into every decoder layer. We ob-\nserve that the position encoding can safely remove, probably\ndue to the irregularity and diversity of the point clouds. At\nlast, we swap the order of self-attention and cross-attention,\nfor query vectors can gather context information immedi-\nately once they are fed into decoder layer, which makes the\nprocess more sensible and brings a little improvement.\nNumber of Queries and Layers.Table 7 presents the se-\nlection of the number of query vectors and transformer de-\ncoder layers. The results show that too less or too many lay-\ners will cause a reduction in performance. Interestingly, we\nobserve some performance improvement when using 400\nquery vectors compared to 200/100 ones and performance\n2398\nInput\n SSTNet\n SoftGroup\n Ours\n Ground Truth\nFigure 4: Visualiztion of instance segmentation results on the ScanNetv2 validation set. The red box highlight the key regions.\nFigure 5: Visualiztion for superpoint cross-attention mechanism. It presents the visualizations of the attention weights in query\nvectors and corresponding segmentation masks. For the input point cloud of a kitchen scene, the upper row is heat maps,\nshowing the relative attention weights between query vectors and points. The bottom row shows the corresponding mask\nprediction of each query vector.\nLayer Query mAP AP 50 AP25\n1 400 49.1 66.9 79.0\n3 400 54.7 72.3 81.5\n6 400 56.3 73.9 82.9\n12 400 55.3 73.1 82.6\n6 100 54.2 72.4 82.8\n6 200 55.2 73.3 82.4\n6 800 55.9 73.7 83.8\nTable 7: The performance results of different choices of\nquery vectors and transformer decoder layers.\nonly saturates when the number rises to 800. It may be due\nto the fact that the number of instances in a 3D scene is usu-\nally more than the number of instances in the common 2D\ndataset.\nThe Selection of Mask Loss.Table 8 illustrates the per-\nformance of the components of mask loss. We observe that\nonly using binary cross-entropy loss or focal (Lin et al.\n2017) loss will cause much lower performance. Dice loss\nis indispensable in mask loss. Based on dice loss, adding\nbce loss or focal loss will improve the total performance.\nThe combination of dice loss and bce loss achieves the best\nresults.\n4.3 Visualizations\nQualitative Results. The visualization of 3D instance seg-\nmentation is shown in Fig. 4. Compared to the existing state-\nDice Focal BCE mAP AP 50 AP25\n✓ 23.1 35.0 47.3\n✓ 35.3 52.1 68.1\n✓ 54.8 72.8 82.4\n✓ ✓ 55.1 73.2 82.1\n✓ ✓ 56.3 73.9 82.9\nTable 8: Ablation study on the selection of mask loss.\nof-the-art method, SPFormer correctly segments each in-\nstance and produces finer segmentation results.\nCross-Attention Mechanism. Fig. 5 visualizes the cross-\nattention mechanism. For an input point cloud, query vectors\nattend to the superpoints and highlight the region of interest.\nHere we propagate the attention weights of superpoints to\ntheir own points for visualization. Then query vectors carry\nthe attention information and form the final mask prediction\nin prediction head.\n5 Conclusion\nIn this paper, we propose a novel end-to-end two-stage\nframework for 3D instance segmentation. SPFormer with a\nnovel hybrid pipeline groups bottom-up potential features\nfrom point clouds into superpoints and proposes instances\nby query vectors as a top-down pipeline. SPFormer achieves\nstate-of-the-art on both ScanNetv2 and S3DIS benchmarks,\nand retains fast inference speed.\n2399\nAcknowledgments\nThis paper is partially supported by the following grants:\nNational Natural Science Foundation of China (61972163,\nU1801262), Natural Science Foundation of Guangdong\nProvince (2022A1515011555), National Key R&D Program\nof China (2022YFB4500600), Guangdong Provincial Key\nLaboratory of Human Digital Twin (2022B1212010004)\nand Pazhou Lab, Guangzhou, 510330, China.\nReferences\nArmeni, I.; Sener, O.; Zamir, A. R.; Jiang, H.; Brilakis, I.;\nFischer, M.; and Savarese, S. 2016. 3D Semantic Parsing\nof Large-Scale Indoor Spaces. In Proceedings of the IEEE\nInternational Conference on Computer Vision and Pattern\nRecognition.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European conference on computer vi-\nsion, 213–229. Springer.\nChen, S.; Fang, J.; Zhang, Q.; Liu, W.; and Wang, X. 2021.\nHierarchical aggregation for 3d instance segmentation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 15467–15476.\nCheng, B.; Misra, I.; Schwing, A. G.; Kirillov, A.; and Gird-\nhar, R. 2022a. Masked-attention mask transformer for uni-\nversal image segmentation. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n1290–1299.\nCheng, B.; Schwing, A.; and Kirillov, A. 2021. Per-pixel\nclassification is not all you need for semantic segmenta-\ntion. Advances in Neural Information Processing Systems,\n34: 17864–17875.\nCheng, T.; Wang, X.; Chen, S.; Zhang, W.; Zhang, Q.;\nHuang, C.; Zhang, Z.; and Liu, W. 2022b. Sparse Instance\nActivation for Real-Time Instance Segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 4433–4442.\nDai, A.; Chang, A. X.; Savva, M.; Halber, M.; Funkhouser,\nT.; and Nießner, M. 2017. Scannet: Richly-annotated 3d\nreconstructions of indoor scenes. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 5828–5839.\nDai, Z.; Cai, B.; Lin, Y .; and Chen, J. 2021. Up-detr: Unsu-\npervised pre-training for object detection with transformers.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 1601–1610.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. In In-\nternational Conference on Learning Representations.\nEngelmann, F.; Bokeloh, M.; Fathi, A.; Leibe, B.; and\nNießner, M. 2020. 3d-mpa: Multi-proposal aggregation\nfor 3d semantic instance segmentation. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 9031–9040.\nFang, Y .; Yang, S.; Wang, X.; Li, Y .; Fang, C.; Shan, Y .;\nFeng, B.; and Liu, W. 2021. Instances as queries. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 6910–6919.\nGraham, B.; Engelcke, M.; and Van Der Maaten, L. 2018.\n3d semantic segmentation with submanifold sparse convo-\nlutional networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 9224–9232.\nGuo, R.; Niu, D.; Qu, L.; and Li, Z. 2021. Sotr: Segmenting\nobjects with transformers. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 7157–7166.\nHe, K.; Gkioxari, G.; Doll´ar, P.; and Girshick, R. 2017. Mask\nr-cnn. In Proceedings of the IEEE international conference\non computer vision, 2961–2969.\nHe, T.; Shen, C.; and van den Hengel, A. 2021. Dyco3d:\nRobust instance segmentation of 3d point clouds through dy-\nnamic convolution. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 354–\n363.\nHou, J.; Dai, A.; and Nießner, M. 2019. 3d-sis: 3d seman-\ntic instance segmentation of rgb-d scans. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 4421–4430.\nHuang, Z.; Huang, L.; Gong, Y .; Huang, C.; and Wang, X.\n2019. Mask scoring r-cnn. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n6409–6418.\nJiang, L.; Zhao, H.; Shi, S.; Liu, S.; Fu, C.-W.; and Jia, J.\n2020. Pointgroup: Dual-set point grouping for 3d instance\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 4867–4876.\nKuhn, H. W. 1955. The Hungarian method for the assign-\nment problem. Naval research logistics quarterly, 2(1-2):\n83–97.\nLahoud, J.; Ghanem, B.; Pollefeys, M.; and Oswald, M. R.\n2019. 3d instance segmentation via multi-task metric learn-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, 9256–9266.\nLandrieu, L.; and Simonovsky, M. 2018. Large-scale point\ncloud semantic segmentation with superpoint graphs. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 4558–4567.\nLiang, Z.; Li, Z.; Xu, S.; Tan, M.; and Jia, K. 2021. Instance\nsegmentation in 3d scenes using semantic superpoint tree\nnetworks. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2783–2792.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll ´ar, P.\n2017. Focal loss for dense object detection. In Proceedings\nof the IEEE international conference on computer vision,\n2980–2988.\nLiu, S.-H.; Yu, S.-Y .; Wu, S.-C.; Chen, H.-T.; and Liu, T.-\nL. 2020. Learning gaussian instance segmentation in point\nclouds. arXiv preprint arXiv:2007.09860.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\n2400\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 10012–10022.\nMilletari, F.; Navab, N.; and Ahmadi, S.-A. 2016. V-net:\nFully convolutional neural networks for volumetric medical\nimage segmentation. In 2016 fourth international confer-\nence on 3D vision (3DV), 565–571. IEEE.\nNarita, G.; Seno, T.; Ishikawa, T.; and Kaji, Y . 2019. Panop-\nticfusion: Online volumetric semantic mapping at the level\nof stuff and things. In 2019 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems (IROS), 4205–4212.\nIEEE.\nPark, K.-B.; Kim, M.; Choi, S. H.; and Lee, J. Y . 2020. Deep\nlearning-based smart task assistance in wearable augmented\nreality. Robotics and Computer-Integrated Manufacturing,\n63: 101887.\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017a. Pointnet:\nDeep learning on point sets for 3d classification and segmen-\ntation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 652–660.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017b. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. Advances in neural information processing\nsystems, 30.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347–10357. PMLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nVu, T.; Kim, K.; Luu, T. M.; Nguyen, T.; and Yoo, C. D.\n2022. SoftGroup for 3D Instance Segmentation on Point\nClouds. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2708–2717.\nXie, C.; Xiang, Y .; Mousavian, A.; and Fox, D. 2021. Un-\nseen object instance segmentation for robotic environments.\nIEEE Transactions on Robotics, 37(5): 1343–1359.\nYang, B.; Wang, J.; Clark, R.; Hu, Q.; Wang, S.; Markham,\nA.; and Trigoni, N. 2019. Learning object bounding boxes\nfor 3D instance segmentation on point clouds. Advances in\nneural information processing systems, 32.\nYi, L.; Zhao, W.; Wang, H.; Sung, M.; and Guibas, L. J.\n2019. Gspn: Generative shape proposal network for 3d\ninstance segmentation in point cloud. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 3947–3956.\nZhang, B.; and Wonka, P. 2021. Point cloud instance seg-\nmentation using probabilistic embeddings. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 8883–8892.\nZhou, D.; Fang, J.; Song, X.; Liu, L.; Yin, J.; Dai, Y .; Li,\nH.; and Yang, R. 2020. Joint 3d instance segmentation and\nobject detection for autonomous driving. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1839–1849.\n2401",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8473151922225952
    },
    {
      "name": "Segmentation",
      "score": 0.7562696933746338
    },
    {
      "name": "Inference",
      "score": 0.6286184787750244
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5779024958610535
    },
    {
      "name": "Transformer",
      "score": 0.5246827006340027
    },
    {
      "name": "Minimum bounding box",
      "score": 0.4534679353237152
    },
    {
      "name": "Bounding overwatch",
      "score": 0.4123249650001526
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3981547951698303
    },
    {
      "name": "Computer vision",
      "score": 0.3459327816963196
    },
    {
      "name": "Data mining",
      "score": 0.3244435787200928
    },
    {
      "name": "Image (mathematics)",
      "score": 0.181676983833313
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I90610280",
      "name": "South China University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 95
}