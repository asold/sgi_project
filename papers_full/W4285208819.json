{
  "title": "Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models",
  "url": "https://openalex.org/W4285208819",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2424942504",
      "name": "Simran Arora",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2111961440",
      "name": "Sen Wu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2737125872",
      "name": "Enci Liu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2156135343",
      "name": "Christopher Ré",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971350322",
    "https://openalex.org/W2789942385",
    "https://openalex.org/W3120706522",
    "https://openalex.org/W3094024085",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3035261884",
    "https://openalex.org/W2998216295",
    "https://openalex.org/W3034891697",
    "https://openalex.org/W2123958332",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2250968750",
    "https://openalex.org/W2785888191",
    "https://openalex.org/W4309811444",
    "https://openalex.org/W2952826391",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W2972788276",
    "https://openalex.org/W2962891712",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2096175520",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W2034603029",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3115908473",
    "https://openalex.org/W3182696977",
    "https://openalex.org/W2753211788",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W2963777632",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W2963691377",
    "https://openalex.org/W3018732874",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W3172794097",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3194836374",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2268733724",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W2594978815",
    "https://openalex.org/W3169890186",
    "https://openalex.org/W3182352988"
  ],
  "abstract": "Popular language models (LMs) struggle to capture knowledge about rare tail facts and entities. Since widely used systems such as search and personal-assistants must support the long tail of entities that users ask about, there has been significant effort towards enhancing these base LMs with factual knowledge. We observe proposed methods typically start with a base LM and data that has been annotated with entity metadata, then change the model, by modifying the architecture or introducing auxiliary loss terms to better capture entity knowledge. In this work, we question this typical process and ask to what extent can we match the quality of model modifications, with a simple alternative: using a base LM and only changing the data. We propose metadata shaping, a method which inserts substrings corresponding to the readily available entity metadata, e.g. types and descriptions, into examples at train and inference time based on mutual information. Despite its simplicity, metadata shaping is quite effective. On standard evaluation benchmarks for knowledge-enhanced LMs, the method exceeds the base-LM baseline by an average of 4.3 F1 points and achieves state-of-the-art results. We further show the gains are on average 4.4x larger for the slice of examples containing tail vs. popular entities.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1733 - 1745\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMetadata Shaping: A Simple Approach for Knowledge-Enhanced\nLanguage Models\nSimran Arora Sen Wu Enci Liu Christopher Ré\nDepartment of Computer Science, Stanford University\n{simran, senwu, jesslec, chrismre}@cs.stanford.edu\nAbstract\nPopular language models (LMs) struggle to\ncapture knowledge about rare tail facts and\nentities. Since widely used systems such as\nsearch and personal-assistants must support\nthe long tail of entities that users ask about,\nthere has been signiﬁcant effort towards en-\nhancing these base LMs with factual knowl-\nedge. We observe proposed methods typically\nstart with a base LM and data that has been\nannotated with entity metadata, then change\nthe model , by modifying the architecture or\nintroducing auxiliary loss terms to better cap-\nture entity knowledge. In this work, we ques-\ntion this typical process and ask to what ex-\ntent can we match the quality of model mod-\niﬁcations, with a simple alternative: using a\nbase LM and only changing the data. We\npropose metadata shaping , a method which\ninserts substrings corresponding to the read-\nily available entity metadata, e.g. types and\ndescriptions, into examples at train and infer-\nence time based on mutual information. De-\nspite its simplicity, metadata shaping is quite\neffective. On standard evaluation benchmarks\nfor knowledge-enhanced LMs, the method ex-\nceeds the base-LM baseline by an average of\n4.3 F1 points and achieves state-of-the-art re-\nsults. We further show the gains are on average\n4.4x larger for the slice of examples containing\ntail vs. popular entities.\n1 Introduction\nRecent language models (LMs) such as BERT (De-\nvlin et al., 2019) and its successors are remark-\nable at memorizing knowledge seen frequently dur-\ning training, however performance degrades over\nthe long tail of rare facts. Given the importance\nof factual knowledge for tasks such as question-\nanswering, search, and personal assistants (Bern-\nstein et al., 2012; Poerner et al., 2020; Orr et al.,\n2020), there has been signiﬁcant interest in inject-\ning these base LMs with factual knowledge about\nentities (Zhang et al., 2019; Peters et al., 2019, inter\nalia.). In this work, we work we propose a simple\nand effective approach for enhancing LMs with\nknowledge, called metadata shaping.\nExisting methods to capture entity knowledge\nmore reliably, typically use the following steps:\nﬁrst annotating natural language text with entity\nmetadata, and next modifying the base LM model\nto learn from the tagged data. Entity metadata is\nobtained by linking substrings of text to entries in a\nknowledge base such as Wikidata, which stores en-\ntity IDs, types, descriptions, and relations. Model\nmodiﬁcations include introducing continuous vec-\ntor representations for entities or auxiliary objec-\ntives (Zhang et al., 2019; Peters et al., 2019; Ya-\nmada et al., 2020; Wang et al., 2020; Xiong et al.,\n2020; Joshi et al., 2020a; Su et al., 2021). Other\nmethods combine multiple learned modules, which\nare each specialized to handle ﬁne-grained rea-\nsoning patterns or subsets of the data distribution\n(Chen et al., 2019; Wang et al., 2021).\nThese knowledge-aware LMs have led to im-\npressive gains compared to base LMs on entity-\nrich tasks. That said, the new architectures are\noften designed by human experts, costly to pre-\ntrain and optimize, and require additional training\nas new entities appear. Further, these LMs may\nnot use the collected entity metadata effectively\n— Wikidata alone holds over ∼100M unique en-\ntities, however many of these entities fall under\nsimilar categories, e.g., “politician” entities. In-\ntuitively, if unseen entities encountered during in-\nference share metadata with entities observed dur-\ning training, the LM trained with this information\nmay be able to better reason about the new entities\nusing patterns learned from similar seen entities.\nHowever, the knowledge-aware LMs learn from\nindividual entity occurrences rather than learning\nthese shared reasoning patterns. Implicitly learning\nentity similarities for 100M entities may be chal-\nlenging since 89% of the Wikidata entities do not\nappear in Wikipedia, a popular source of unstruc-\n1733\nFigure 1: Metadata shaping inserts metadata (e.g., entity types and descriptions) strings into train and test examples.\nThe FewRel benchmark involves identifying the relation between a subject and object string. The above subject\nand object are unseen in the FewRel training data and the tuned base LM reﬂects low attention weights on those\nwords. A base LM trained with shaped data reﬂects high attention weights on useful metadata words such as\n“politician”. Weights are shown for words which are not stop-words, punctuation, or special-tokens.\ntured training data for the LMs, at all. 1\nWe thus ask, to what extent can we match the\nquality of knowledge-aware LM architectures\nusing the base LM itself?We ﬁnd that applying\nsome simple modiﬁcations to the data at train and\ntest time, a method we call metadata shaping, is\nsurprisingly quite effective. Given unstructured\ntext, there are several readily available tools for\ngenerating entity metadata at scale (e.g., Manning\net al. (2014); Honnibal et al. (2020)), and knowl-\nedge bases contain entity metadata including type\ntags (e.g., Barack Obama is a “politician”) and de-\nscriptions (e.g., Barack Obama “enjoys playing bas-\nketball”). Our method entails explicitly inserting\nretrieved entity metadata in examples as in Figure\n1 and inputting the resulting shaped examples to\nthe LM. Our contributions are:\nSimple and Effective Method We propose\nmetadata shaping and demonstrate its effectiveness\non standard benchmarks that are used to evaluate\nknowledge-aware LMs. Metadata shaping, with\nsimply an off-the-shelf base LM, exceeds the base\nLM trained on unshaped data by by an average\nof 4.3 F1 points and is competitive to state-of-the-\nart methods, which do modify the LM. Metadata\nshaping thus enables re-using well-studied and op-\ntimized base LMs (e.g., Sanh et al. (2020)).\nTail Generalization We show that metadata\nshaping improves tail performance — the observed\ngain from shaping is on average 4.4x larger for the\n1Orr et al. (2020) ﬁnds that a BERT based model needs to\nsee an entity in on the order of 100 samples to achieve 60 F1\npoints when disambiguating the entity in Wikipedia text.\nslice of examples containing tail entities than for\nthe slice containing popular entities. Metadata es-\ntablish “subpopulations”, groups of entities sharing\nsimilar properties, in the entity distribution (Zhu\net al., 2014; Cui et al., 2019; Feldman, 2020). For\nexample on the FewRel benchmark (Han et al.,\n2018), “Daniel Dugléry” (a French politician) ap-\npears 0 times, but “politician” entities in general\nappear > 700 times in the task training data. In-\ntuitively, performance on a rare entity should im-\nprove if the LM has the explicit information that it\nis similar to other entities observed during training.\nExplainability Existing knowledge-aware LMs\nuse metadata (Peters et al., 2019; Alt et al., 2020),\nbut do not explain when and why different meta-\ndata help. Inspired by classic feature selection\ntechniques (Guyon and Elisseeff, 2003), we con-\nceptually explain the effect of different metadata\non generalization error.\nWe hope this work motivates further research on\naddressing the tail challenge through the data. 2\n2 Method\nThis section introduces metadata shaping, includ-\ning the set up and conceptual framework.\n2.1 Objective\nThe goal of metadata shaping is to improve tail\nperformance using properties shared by popular\nand rare examples (e.g., the unseen entity “Daniel\nDugléry” and popular entity “Barack Obama” are\n2We release our code: https://github.com/\nsimran-arora/metadatashaping\n1734\nboth “politicians”). This work explores how to ef-\nfectively provide these properties to popular trans-\nformer models. Tail entities are those seen < 10\ntimes during training and head entities are seen\n≥10 times, consistent with Orr et al. (2020); Goel\net al. (2021).\nMetadata are easily and scalably sourceable us-\ning off-the-shelf models such as those for named\nentity (NER, NEL) or part-of-speech (POS) tag-\nging (Manning et al., 2014; Honnibal et al., 2020),\nheuristic rules, and knowledge bases (KBs) (e.g.,\nWikidata, Wordnet (Miller, 1995), domain-speciﬁc\nKBs (Bodenreider, 2004), and product KBs (Krish-\nnan, 2018)). KBs often provide high tail coverage\n— e.g., a product KB will contain metadata for both\npopular and unpopular products.\nMany prior works annotate text with metadata\nand in our setting, instead of using predeﬁned fea-\nture schemas (Marcus et al., 1993; Mintz et al.,\n2009, inter alia.), we consider using an unrestricted\nset of metadata, including entity unstructured de-\nscriptions. Importantly, knowledge-aware LMs\nhave attracted signiﬁcant recent interest and data-\noriented approaches have not been demonstrated\nas a compelling alternative, the aim of this work.\n2.2 Set Up\nLet input x∈X and label y∈Y, and consider the\nclassiﬁcation dataset DDD= {(xi,yi)}n\ni=1 of size n.\nLet m∈M denote a metadata tag and let M(xi)M(xi)M(xi)\nbe the set of metadata collected for example xi. A\nshaping function fs : X→X s accepts an original\nexample xi ∈X and produces a shaped example\nsi ∈Xs by inserting a subset ofM(xi)M(xi)M(xi) into xi (see\nFigure 1). The downstream classiﬁcation model ˆpφ\nis learned from shaped train examples and infers yi\nfrom the shaped test examples.\nThis work uses the following representative\nmetadata shaping functions for all tasks to insert\na range of coarse-grained signals associated with\ngroups of examples to ﬁne-grained speciﬁc signals\nassociated with individual examples:\nCategorical tokensestablish subpopulations of\nentities (e.g., Dugléry falls in the coarse grained\ncategory of “person” entities, or ﬁner grained cate-\ngory of “politician” entities). NER and POS tags\nare coarse grained categories, and knowledge bases\ncontain ﬁner-grained categories (i.e., entity types\nand relations). Categories are consistent and fre-\nquent compared to words in the original examples.\nDescription tokensgive cues for rare entities\nand alternate expressions of popular entities (e.g.,\nDugléry is a “UMP party member”). Descriptions\nare likely unique across entities, and can be viewed\nas the ﬁnest-grained category for an entity.\n2.3 Conceptual Framework\nNext we want to understand if inserting m ∈\nM(xi)M(xi)M(xi) for xi ∈DDDcan improve tail performance.\nWe measure the generalization error of the classiﬁ-\ncation model ˆpφ using the cross-entropy loss:\nLcls = E(x,y)\n[\n−log(ˆpφ(y|x))\n]\n. (1)\nLet Pr(y|xi) be the true probability of class y∈\nY given xi. Example xi is composed of a set of\npatterns KiKiKi (i.e., subsets of tokens in xi). We make\nthe assumption that a pattern k ∈KiKiKi is a useful\nsignal if it informs Pr(y|xi). We thus parametrize\nthe true distribution Pr(y|xi) using the principle of\nmaximum entropy (Berger et al., 1996):\nPr(y|xi) = 1\nZ(xi) exp(\n∑\nk∈KiKiKi\nλkPr(y|k)). (2)\nwhere λk represents learned parameters weighing\nthe contributions of patterns (or events)kand Z(xi)\nis a partition function that ensures Pr(y|xi) repre-\nsents a probability distribution. Therefore when\nevaluating ˆpφ, achieving zero cross-entropy loss\nbetween the true probability Pr(y|k) and the esti-\nmated probability ˆpφ(y|k), for all k, implies zero\ngeneralization error overall.\nUnseen Patterns Our insight is that for a pattern\nkthat is unseen during training, which is common\nin entity-rich tasks,3 the class and pattern are in-\ndependent (y ⊥k) under the model’s predicted\ndistribution ˆpφ, so ˆpφ(y|k) = ˆpφ(y). With the as-\nsumption of a well-calibrated model and not consid-\nering priors from the base LM pretraining stage,4\nthis probability is ˆpφ(y) = 1\n|Y| for y∈Y.\nPlugging in ˆpφ(y) = 1\n|Y| , the cross-entropy\nloss between Pr(y|k) and ˆpφ(y|k) is Pr(k) log|Y|.\nOur idea is to effectively replace k with another\n(or multiple) shaped pattern k′, which has non-\nuniform ˆpφ(y|k′) and a lower cross-entropy loss\nwith respect to Pr(y|k′), as discussed next.\n3For example, on the FewRel benchmark used in this work,\n90.7%/59.7% of test examples have a subject/object span\nwhich are unseen as the subject/object span during training.\n4We ignore occurrences in the pretraining corpus and\nlearned similarities between unseen k and seen k′. Future\nwork can use these priors to reﬁne the slice of unseen entities.\n1735\nAlgorithm 1Metadata Token Selection\n1: Precompute Train Statistics\n2: Input: training data DDDtrain, metadata M\n3: for each category m∈M over DDDtrain do\n4: Compute pmi(y,m) for y∈Y.\n5: end for\n6: for each class y∈Y over Dtrain do\n7: Compute frequency fy.\n8: end for\n9:\n10: Select Metadata for Sentence\n11: Input: xi from DDDtrain and DDDtest, integer n.\n12: Collect metadata M(xi)M(xi)M(xi) for xi.\n13: for m∈M(xi)M(xi)M(xi) do\n14: Compute ry = 2pmi(m,y)fy for y∈Y.\n15: Normalize ry values to sum to 1.\n16: Compute entropy Hm over ry for y∈Y.\n17: end for\n18: Rank m∈M(xi)M(xi)M(xi) by Hm.\n19: Return min(n,|M(xi)M(xi)M(xi)|) tokens with lowest\nHm.\nInserting Metadata Consider the shaped exam-\nple, si = fs(xi), which contains new tokens from\nM(xi)M(xi)M(xi), and thus contains a new set of patterns\nKs\niKs\niKs\ni. Let km ∈Ks\niKs\niKs\ni be a pattern containing some\nm ∈M(xi)M(xi)M(xi). For a rare pattern (e.g., a mention\nof a rare entity in xi) k, if an associated pattern\nkm (e.g., a metadata token for the rare entity) oc-\ncurs non-uniformly across classes during training,\nthen the cross-entropy loss between ˆpφ(y|km) and\nPr(y|km) is lower than the cross-entropy loss be-\ntween ˆpφ(y|k) and Pr(y|k). If km shifts ˆpφ(y|xi)\nusefully, performance of ˆpφ should improve.\nWe can measure the non-uniformity ofkmacross\nclasses using the conditional entropy ˆH(Y|k).\nWhen k is unseen and ˆpφ(y|k) = ˆpφ(y,k) =\nˆpφ(y) = 1\n|Y| (uniform), ˆH(Y|k) is maximized:\nˆH(Y|k) =−\n∑\ny∈Y\nˆpφ(y,k) log ˆpφ(y|k) = log(|Y|). (3)\nFor non-uniform ˆpφ(y|km), the conditional en-\ntropy decreases. Broadly, we connect the beneﬁt of\nusing different metadata, which are inputs both to\nexisting knowledge aware LMs and our approach,\nto classical methods (Guyon and Elisseeff, 2003)\n— we seek the metadata providing the largest infor-\nmation gain. Next we discuss the practical consid-\nerations for selecting metadata.\nMetadata Selection Entities are associated with\nlarge amounts of metadata M(xi)M(xi)M(xi) — categories\ncan range from coarse-grained (e.g., “person”) to\nﬁne-grained (e.g., “politician” or “US president”)\nand there are intuitively many ways to describe\nentities. Since certain metadata may not be helpful\nfor a task, and popular base LMs do not scale very\nwell to long sequences (Tay et al., 2020; Pascanu\net al., 2013), it is important to understand which\nmetadata to use for shaping.\nWe want to select km with non-uniform\nˆpφ(y|km) across y∈Y, i.e. with lower ˆH(Y|km).\nConditional probability Pr(y|km) is deﬁned as:\nPr(y|km) = 2pmi(y,km) Pr(y), (4)\nwhere we recall that the pointwise mutual infor-\nmation pmi(y,km) is deﬁned as log\n( Pr(y,km)\nPr(y) Pr(km)\n)\n.\nThe pmi compares the probability of observing y\nand km together (the joint probability) with the\nprobabilities of observing yand km independently.\nClass-discriminative metadata reduce ˆH(Y|k).\nDirectly computing the resulting conditional\nprobabilities after incorporating metadata in DDDis\nchallenging since the computation requires consid-\nering all patterns contained in all examples, gen-\nerated by including m. Instead we use simplistic\nproxies to estimate the information gain. In Algo-\nrithm 1, we focus on the subset of Ks\niKs\niKs\ni containing\nindividual metadata tags m, and compute the en-\ntropy over ˆpφ(y|m) for y∈Y. Simple extensions\nto Algorithm 1, at the cost of additional compu-\ntation, would consider a broader set of km (e.g.,\nn-grams containing mfor n >1), or iteratively\nselect tokens by considering the correlations in the\ninformation gain between different metadata tags.\n3 Experiments\nIn this section, we demonstrate that metadata shap-\ning is general and effective.\n3.1 Datasets\nWe evaluate on standard entity-typing and relation\nextraction benchmarks used by baseline methods.\nEntity typinginvolves predicting the the applica-\nble types for a given substring in the input example\nfrom a set of output types. We use OpenEntity\n(9 output types) (Choi et al., 2018) for evaluation.\nRelation extractioninvolves predicting the rela-\ntion between the two substrings in the input exam-\nple, one representing a subject and the other an\n1736\nModel FewRel TACRED OpenEntity\nP R F1 P R F1 P R F1\nBERT-base 85.1 85.1 84.9 66.3 78.7 72.0 76.4 71.0 73.2\nK-BERT 83.1 85.9 84.3 - - - 76.7 71.5 74.0\nERNIE 88.5 88.4 88.3 74.8 77.1 75.9 78.4 72.9 75.6\nE-BERTconcat 88.5 88.5 88.5 - - - - - -\nKnowBERTWiki 89.2 89.2 89.2 78.9 76.9 77.9 78.6 71.6 75.0\nCokeBERT 89.4 89.4 89.4 - - - 78.8 73.3 75.6\nOurs (BERT-base) 90.4 90.4 90.4 77.0 76.3 76.7 79.3 73.3 76.2\nTable 1: Test scores on standard relation extraction and entity-typing tasks. “Ours (Base LM)” is metadata shaping.\nAll methods use the same base LM (BERT-base) and external information (Wikipedia) for consistent comparison.\nA dash (“-”) indicates the baseline method did not report scores for the task.\nobject. We use FewRel (80 output relations) and\nTACRED Revisited (42 output relations) for eval-\nuation (Han et al., 2018; Zhang et al., 2017; Alt\net al., 2020). While metadata shaping is generally\napplicable to classiﬁcation tasks, our objective in\nthis work is to compare architectural versus data-\noriented methods of injecting knowledge, so we\nfocus on benchmarks that are popular in the litera-\nture on knowledge-aware LMs.\n3.2 Experimental Settings\nModel We ﬁne-tune a BERT-base model on meta-\ndata shaped data for each task, taking the pooled\n[CLS] representation and using a linear prediction\nlayer for classiﬁcation (Devlin et al., 2019). We\nuse cross-entropy loss for FewRel and TACRED\nand binary-cross-entropy loss for OpenEntity. All\ntest scores are reported at the epoch with the best\nvalidation score and we use the scoring implemen-\ntations released by (Zhang et al., 2019). Additional\ntraining details are provided in appendix A.\nMetadata Source We collect entity metadata\nfrom Wikidata for our evaluations, a compelling\nchoice as several works successfully improve tail\nperformance in industrial workloads using the\nknowledge base (e.g., Orr et al. (2020)) We use\nthe state-of-the-art pretrained entity-linking model\nfrom Orr et al. (2020) to link the text in each task to\nan October 2020 dump of Wikidata. We use Wiki-\ndata and the ﬁrst sentence of an entity’s Wikipedia\npage to obtain descriptions. Additional details are\nin Appendix A. For certain examples in the tasks,\nthere are no linked entities in the text (e.g., several\nsubject or object entities are simply pronouns or\ndates). Table 3 gives statistics for the number of\nexamples with available of metadata for each task.\nMetadata tags are selected by Algorithm 1.\nWhile the metadata annotation methods have\ntheir own failure rates, our baselines also use entity\nlinking as the ﬁrst step (Zhang et al., 2019, inter\nalia.) with the same exposure to failures. All the\nsame, we seek methods that are ﬂexible to errors\nthat arise in natural data.\n3.3 Baselines\nPrior work proposes various knowledge-aware\nLMs, which are currently the state-of-the-art for\nthe evaluated tasks. ERNIE, (Zhang et al., 2019)\nLUKE (Yamada et al., 2020), KEPLER (Wang\net al., 2020), CokeBERT (Su et al., 2021), and\nWKLM (Xiong et al., 2020) introduce auxil-\nliary loss terms and require additional pretrain-\ning. Prior approaches also modify the architecture\nfor example using alternate attention mechanisms\n(KnowBERT (Peters et al., 2019), K-BERT (Liu\net al., 2020), LUKE) or training additional trans-\nformer stacks to specialize in knowledge-based rea-\nsoning (K-Adapter (Wang et al., 2021)). E-BERT\n(Poerner et al., 2020) does not require additional\npretraining and uses entity embeddings which are\naligned to the word embedding space. In Table 1,\nwe compare to methods which use the same base\nLM, BERT-base, and external information resource,\nWikipedia, for consistency.\n3.4 End-to-End Benchmark Results\nWe simply use an off-the-shelf BERT-base LM\n(Wolf et al., 2020), with no additional pretrain-\ning and ﬁne-tuned on shaped data to exceed the\nBERT-base LM trained on unshaped data by 5.3\n(FewRel), 4.7 (TACRED), and 3.0 (OpenEntity)\nF1 points. Metadata shaping is also competitive\nwith SoTA baselines which do modify the BERT-\nbase LM. Results are shown in Table 1. Table 3\nreports the availability of metadata for each task.\n1737\nWe observe that metadata shaping is effective both\nwhen most task examples have available metadata\n(e.g., FewRel) and when metadata tags are sparse\n(e.g., on OpenEntity only 30% of examples have\navailable metadata), analyzed further in Section 4.\nWe further note that the performance of our method\nis not sensitive to grammatical choices around how\nthe metadata tags are inserted through ablations\nprovided in Appendix B.\nFor the baselines, we give reported numbers\nwhen available, Su et al. (2021) reports two of the\nKnowBERT-Wiki and all K-BERT results, and we\nobtain remaining numbers using the code released\nby baseline work as detailed in Appendix A.\n4 Analysis\nHere we study the following key questions for effec-\ntively using metadata shaping: Section 4.1What\nare the roles of different varieties of metadata?Sec-\ntion 4.2What are the effects of metadata shaping\non slices concerning tail versus popular entities?\n4.1 Framework: Role of Metadata Types\nMetadata Effects Class-discriminative meta-\ndata correlates with reduced model uncertainty.\nHigh quality metadata, as found in Wikidata, re-\nsults in improved classiﬁcation performance.\nTo investigate the effects of metadata on model\nuncertainty, we compute the entropy of ˆpφ softmax\nscores over the output classes as a measure of un-\ncertainty, and compute the average across test set\nexamples. Lower uncertainty is correlated with\nimproved classiﬁcation F1 (See Figure 2 (Left)).\nWe compute pmi scores for inserted metadata\ntokens as a measure of class-discriminativeness.\nWe rank individual tokens kby pmi(y,k) (for task\nclasses y), computed over the training dataset. On\nFewRel, for test examples containing a top-20 pmi\nword for the gold class, the accuracy is 27.6%\nhigher when compared to the slice with no top-\n20 pmi words for the class. Notably, 74.1% more\nexamples contain a top-20 pmi word for their class\nwhen pmi is computed on shaped data vs. unshaped\ntraining data.\nMetadata Selection Simple information theo-\nretic heuristics are effective for selecting metadata,\ndespite the complexity of the underlying contextual\nembeddings.\nWe apply Algorithm 1, which ranks metadata\ntags by their provided information gain, to select\nmetadata tags for the tasks. Given xi with a set\nBenchmark Strategy Test F1\nFewRel\nBERT-base 84.9\nRandom 87.2 ±0.8\nPopular 87.9 ±0.1\nLow Rank 87.8 ±0.4\nHigh Rank 88.9 ±0.6\nOpenEntity\nBERT-base 73.2\nRandom 74.3 ±0.7\nPopular 74.5 ±0.4\nLow Rank 74.1 ±0.4\nHigh Rank 74.8 ±0.1\nTACRED\nBERT-base 72.0\nRandom 73.8 ±1.6\nPopular 73.6 ±0.9\nLow Rank 73.3 ±1.0\nHigh Rank 74.7 ±0.5\nTable 2: Average and standard deviation over 3 random\nseeds. Each method selects up tonmetadata tokens per\nentity. For FewRel, TACRED, n = 3 per subject, ob-\nject. For OpenEntity n = 2per main entity as 33% of\nOpenEntity train examples have ≥2 categories avail-\nable (80.7% have ≥3 categories on FewRel). Note we\nuse larger nfor the main results in Table 1.\nM(xi)M(xi)M(xi) of metadata tags, our goal is to select n\nto use for shaping. We compare four selection\napproaches: using the highest (“High Rank”) and\nlowest (“Low Rank”) ranked tokens by Algorithm\n1, random metadata from M(xi)M(xi)M(xi) (“Random”), and\nthe most popular metadata tokens across the union\nof M(xi)M(xi)M(xi),∀xi ∈DtrainDtrainDtrain (“Popular”), selecting the\nsame number of metadata tags per example for each\nbaseline. We observe that High Rank consistently\ngives the best performance, evaluated over three\nseeds, and note that even Random yields decent\nperformance vs. the BERT-baseline, indicating the\nsimplicity of the method (Table 2).\nConsidering the distribution of selected category\ntokens under each scheme, the KL-divergence be-\ntween the categories selected by Low Rank vs. Pop-\nular is 0.2 (FewRel), 4.6 (OpenEntity), while the\nKL-divergence between High Rank vs. Popular is\n2.8 (FewRel), 2.4 (OpenEntity). Popular tokens are\nnot simply the best candidates; instead, Algorithm\n1 selects discriminative metadata.\nFor OpenEntity, metadata are relatively sparse,\nso categories appear less frequently in general and\nit is reasonable that coarse-grained types have more\noverlap with High Rank. For e.g., “business” is in\nthe top-10 most frequent types under High Rank,\n1738\nFigure 2: Test F1 for ˆpφ (no additional pretraining) vs.\naverage entropy of ˆpφ softmax scores (Top) and vs. per-\nplexity of a model ˆpθ (w/ pretraining) (Bottom). ˆpφ and\nˆpθ use the same shaped training data. Each point is a\ndifferent metadata shaping scheme (median over 3 Ran-\ndom Seeds): for R0 all inserted tokens are true tokens\nassociated with the entity in the KB. For RX, X true\nmetadata tokens are replaced by random (noise) tokens\nfrom the full vocabulary. For each point, the total num-\nber of metadata tokens is constant per example.\nwhile “non-proﬁt” (occurs in 2 train examples) is\nin the top-10 most frequent types for Low Rank.\nMetadata tokens overall occur more frequently in\nFewRel (See Table 3), so ﬁne-grained types are also\nquite discriminative. The most frequent category\nunder Low Rank is “occupation” (occurs in 2.4k\ntrain examples), but the top-10 categories under\nHigh Rank are ﬁner-grained, e.g. “director” and\n“politician” (each occurs in>300 train examples).\nTask Agnostic Metadata Effects Using meta-\ndata correlates with reduced task-speciﬁc LM un-\ncertainty. We observe shaping also correlates with\nreduced LM uncertainty in a task-agnostic way.\nWe perform additional masked language model-\ning (MLM) over the shaped task training data using\nan off-the-shelf BERT-MLM model to learn model\nˆpθ. We minimize the following loss function and\nevaluate the model perplexity on the task test data:\nLmlm = Es∼D,m∼M,i∼I\n[\n−log(ˆpθ(smi |sm/i))\n]\n. (5)\nwhere I is the masked token distribution and\nsmi is the masked token at position iin the shaped\nsequence sm.5 Through minimizing the MLM loss,\nˆpθ learns direct dependencies between tokens in\nthe data (Zhang and Hashimoto, 2021). In Figure 2\n(Right), we observe a correlation between reduced\nperplexity for ˆpθ, and higher downstream perfor-\nmance for ˆpφ across multiple tasks, both using the\nsame training data. Overall, shaping increases the\nlikelihood of the data, and we observe a correlation\n5We use the Hugging Face implementation for masking\nand ﬁne-tuning the BERT-base MLM (Wolf et al., 2020).\nFigure 3: The gain from training the BERT-base LM\nwith metadata shaped data over training with unshaped\ndata, split by the popularity of the entity span in the test\nexample.\nbetween the intrinsic perplexity metric and the ex-\ntrinsic downstream metrics as a result of the same\nshaping scheme. Table 4 (Appendix B) reports the\nsame correlations for all benchmarks.\nMetadata Noise We hypothesize that noisier\nmetadata can provide implicit regularization.\nNoise arises from varied word choice, word order,\nand blank noising.\nFeature noising (Wang et al., 2013) is effective\nto prevent overﬁtting and while regularization is\ntypically applied directly to model parameters, Xie\net al. (2017); Dao et al. (2019) regularize through\nthe data. We hypothesize that using metadata with\ndiverse word choice and order (e.g., entity descrip-\ntions) and blank noising (e.g., by masking metadata\ntokens), can help reduce overﬁtting, and we provide\ninitial empirical results in Appendix B.\n4.2 Evaluation: Tail and Head Slices\nSection 3 shows the overall gain from shaping. We\nnow consider ﬁne-grained slices of examples con-\ntaining head vs. tail entities and observe gains are\n4.4x larger on the tail slice on average (Figure 3). 6\nSubpopulations Metadata are helpful on the tail\nas they establish subpopulations.\nWe hypothesize that if a pattern is learned for an\nentity-subpopulation occurring in the training data,\nthe model may perform better on rare entities that\nalso participate in the subpopulation, but were not\nindividually observed during training. On FewRel,\nwe take the top-20 TF-IDF words associated with\neach category signal during training as linguistic\n6A consideration for TACRED is that 42% of these head\nspans are stopwords (e.g., pronouns) or numbers; just 7% are\nfor FewRel. This is based on unseen object spans for FewRel\nand TACRED, as>90% of subject spans are unseen.\n1739\ncues captured by the model for the category sub-\npopulation, consistent with Goel et al. (2021). For\nexample, “government” is in the top-20 TF-IDF\nwords for the “politician” entity category. At test\ntime, we select the slice of examples containing any\nof these words for any of the categories inserted in\nthe example. The performance is 9.0/3.5 F1 points\nhigher on examples with unseen subject/object en-\ntities with vs. without a top-20 TF-IDF word for a\nsubject/object category.\nMetadata Effects on Popular EntitiesFor pop-\nular entities the LM can learn entity-speciﬁc pat-\nterns well, and be mislead by subpopulation-level\npatterns corresponding to metadata.\nAlthough we observe overall improvements,\nhere we examine the effect of metadata on the pop-\nular entity slice within our conceptual framework.\nLet pbe a popular pattern (i.e., entity mention)\nin the training data, and let mbe a metadata token\nassociated with p. Intuitively, the LM can learn\nentity-speciﬁc patterns from occurrences of p, but\ncoarse-grained subpopulation-level patterns corre-\nsponding to m. If mand pare class-discriminative\nfor different sets of classes, then m can mislead\nthe LM. To evaluate this, consider subject and ob-\nject entity spans p ∈ P seen ≥ 1 time during\ntraining. For test examples let Yp be the set of\nclasses y for which there is a p ∈P in the ex-\nample with pmi(y,p) > 0, and deﬁne Ym as the\nclasses y for which there is a metadata token m\nwith pmi(y,m) >0 in the example. The examples\nwhere Yp ̸= ∅, Ym ̸= ∅, and Yp contains the true\nclass, but Ym does not, represents the slice where\nmetadata can mislead the model. On this slice of\nFewRel, the gain from the shaped model is 2.3 F1\npoints less than the gain on the slice of all examples\nwith Yp ̸= ∅and Ym ̸= ∅, supporting our intuition.\nAn example entity-speciﬁc vs. subpopulation-\nlevel tension in FewRel is: p = “Thames River”\nis class-discriminative for y =“located in or next\nto body of water”, but its m =“river” is class-\ndiscriminative for y=“mouth of the watercourse”.\n5 Related Work\nIncorporating Knowledge in LMs Discussed\nin Section 3.2, signiﬁcant prior work incorporates\nknowledge by changing the base LM architecture\nor loss function. Peters et al. (2019); Alt et al.\n(2020) also use NER, POS Wikpedia, or Wordnet\nmetadata, but do not conceptually explain the ben-\neﬁt or selection process. Orr et al. (2020) demon-\nstrates that category metadata improves tail perfor-\nmance for NED. We do not modify the base LM.\nPrior work inserts metadata for entities in the\ndata itself. Joshi et al. (2020b); Logeswaran et al.\n(2019); Raiman and Raiman (2018) each uses a sin-\ngle form of metadata (either descriptions or types)\nfor a single task-type (either QA or NED) demon-\nstrating empirical beneﬁts. Metadata shaping com-\nbines different varieties of metadata and applies\ngenerally to classiﬁcation tasks, and we provide\nconceptual grounding.\nFeature Selection This work is inspired by tech-\nniques in feature selection based on information\ngain (Guyon and Elisseeff, 2003). In contrast to\ntraditional feature schemas (Levin, 1993; Marcus\net al., 1993), metadata shaping annotations are ex-\npressed in natural language to ﬂexibly include arbi-\ntrary metadata. The classic methods (Berger et al.,\n1996) are not used to explain design decisions in the\nline of work on knowledge-enhanced LMs, which\nwe connect in this work. In our setting of entity-\nrich tasks, we explain how metadata can reduce\ngeneralization error.\nPrompting Prompting can serve similar goals,\nbut often requires human-picked prompt tokens\n(Keskar et al., 2019; Aghajanyan et al., 2021) or\ntask-speciﬁc templates (Han et al., 2021; Chen\net al., 2022), while metadata shaping provides a\nﬂexible baseline across metadata-types and task-\ntypes. Prompting typically aims to better elicit\nimplicit knowledge from the base LM (Liu et al.,\n2021), while metadata shaping focuses on explic-\nitly incorporating retrieved signals not found in the\noriginal task. Shaping is applied at train and test\ntime and does not introduce new parameters, as\nrequired by methods which use learned prompts.\nData Augmentation One approach to tackle the\ntail is to generate additional examples for tail enti-\nties (Wei and Zou, 2019; Xie et al., 2020; Dai and\nAdel, 2020). However, this can be sample inefﬁ-\ncient since augmentations do not explicitly signal\nthat different entities are in the same subpopulation\n(Horn and Perona, 2017), so the model would need\nview each entity individually in different contexts.\nMetadata shaping and prompting (Scao and Rush,\n2021) may be viewed as implicit augmentation.\n6 Conclusion\nWe propose metadata shaping to improve tail per-\nformance. The method is a simple and general\n1740\nbaseline that is competitive with SoTA approaches\nfor entity-rich tasks. We empirically show that the\nmethod improves tail performance and explain why\nmetadata can reduce generalization error. While\nthis work focused on entity-rich tasks, metadata\nshaping is not limited to this setting. Broadly, we\nhope this work motivates further research on under-\nstanding how to effectively program LMs with use-\nful and readily available side information. While\nmodifying the LM architecture to encode the infor-\nmation has been a popular approach, modifying the\ndata is a simple and effective alternative.\nReferences\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis,\nMandar Joshi, Hu Xu, Gargi Ghosh, and Luke\nZettlemoyer. 2021. Htlm: Hyper-text pre-\ntraining and prompting of language models. In\narXiv:2107.06955v1.\nChristoph Alt, Aleksandra Gabryszak, and Leonhard\nHennig. 2020. Tacred revisited: A thorough eval-\nuation of the tacred relation extraction task. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL).\nAdam L. Berger, Vincent J. Della Pietra, and Stephen\nA. Della Pietra. 1996. A maximum entropy ap-\nproach to natural language processing. In Compu-\ntational Linguistics.\nMichael S Bernstein, Jaime Teevan, Susan Dumais,\nDaniel Liebling, and Eric Horvitz. 2012. Direct an-\nswers for search queries in the long tail. In Proceed-\nings of the SIGCHI Conference on Human Factors\nin Computing Systems (CHI).\nOlivier Bodenreider. 2004. The uniﬁed medical lan-\nguage system (umls): integrating biomedical termi-\nnology. In Nucleic Acids Research.\nVincent Chen, Sen Wu, Alex Ratner, J. Weng, and\nChristopher Ré. 2019. Slice-based learning: A pro-\ngramming model for residual learning in critical data\nslices. In Advances in neural information process-\ning systems (NeurIPS), pages 9392–9402.\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,\nYunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2022. Knowprompt: Knowledge-\naware prompt-tuning with synergistic optimization\nfor relation extraction. In Proceedings of the ACM\nWeb Conference 2022 (WWW).\nEunsol Choi, Omer Levy, Yejin Choi, and Luke Zettle-\nmoyer. 2018. Ultra-ﬁne entity typing. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (ACL).\nYin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and\nSerge Belongie. 2019. Class-balanced loss based\non effective number of samples. In IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nXiang Dai and Heike Adel. 2020. An analysis of sim-\nple data augmentation for named entity recognition.\nProceedings of the 28th International Conference on\nComputational Linguistics.\nTri Dao, Albert Gu, Alexander Ratner, Chris De Sa Vir-\nginia Smith, and Christopher Ré. 2019. A kernel\ntheory of modern data augmentation. Proceedings\nof the 36th International Conference on Machine\nLearning (PMLR).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies (NAACL-HLT).\nVitaly Feldman. 2020. Does learning require memo-\nrization? a short tale about a long tail. In Proceed-\nings of the 52nd Annual ACM SIGACT Symposium\non Theory of Computing (STOC).\nKaran Goel, Nazneen Rajani, Jesse Vig, Samson Tan,\nJason Wu, Stephan Zheng, Caiming Xiong, Mohit\nBansal, and Christopher Ré. 2021. Robustness gym:\nUnifying the nlp evaluation landscape. NAACL\nDemo Track.\nIsabelle Guyon and Andre Elisseeff. 2003. An intro-\nduction to variable and feature selection. In Journal\nof Machine Learning Research.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and\nMaosong Sun. 2021. Ptr: Prompt tuning with rules\nfor text classiﬁcation. In arXiv:2105.11259v3.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan\nYao, Zhiyuan Liu, and Maosong Sun. 2018. Fewrel:\nA large-scale supervised few-shot relation classiﬁca-\ntion dataset with state-of-the-art evaluation. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nMatthew Honnibal, Ines Montani, Soﬁe Van Lan-\ndeghem, and Adriane Boyd. 2020. Industrial-\nstrength natural language processing in python.\nGrant Van Horn and Pietro Perona. 2017. The devil is\nin the tails: Fine-grained classiﬁcation in the wild.\nIn arXiv:1709.01450.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020a.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics (TACL).\nMandar Joshi, Kenton Lee, Yi Luan, and Kristina\nToutanova. 2020b. Contextualized representations\nusing textual encyclopedic knowledge. In ArXiv.\n1741\nNitish Shirish Keskar, Bryan McCann, Lav R. Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCtrl: A conditional transformer language model for\ncontrollable generation. arXiv:1909.05858v2.\nArun Krishnan. 2018. Making search easier.\nBeth Levin. 1993. English Verb Classes and Alterna-\ntions. The University of Chicago Press.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nIn arXiv:2107.13586.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-bert:\nEnabling language representation with knowledge\ngraph. In Proceedings of AAAI.\nLajanugen Logeswaran, Ming-Wei Chang, Kenton Lee,\nKristina Toutanova, Jacob Devlin, and Honglak Lee.\n2019. Zero-shot entity linking by reading entity de-\nscriptions. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The stanford corenlp natural language\nprocessing toolkit. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (ACL).\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a Large Annotated\nCorpus of English: The Penn Treebank.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. In Communications of the ACM.\nMike Mintz, Steven Bills, Rion Snow, and Dan Juraf-\nsky. 2009. Distant supervision for relation extrac-\ntion without labeled data. In Proceedings of the\nJoint Conference of the 47th Annual Meeting of the\nACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP.\nLaurel Orr, Megan Leszczynski, Simran Arora, Sen\nWu, Neel Guha, Xiao Ling, and Chris Ré. 2020.\nBootleg: Chasing the tail with self-supervised\nnamed entity disambiguation. In arXiv:2010.10363.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difﬁculty of training recurrent neural\nnetworks. In Proceedings of the 30th International\nConference on Machine Learning (PMLR).\nMatthew E Peters, Mark Neumann, Robert L Logan IV ,\nRoy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A Smith. 2019. Knowledge enhanced con-\ntextual word representations. Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP).\nNina Poerner, Ulli Waltinger, and Hinrich Schutze.\n2020. E-bert: Efﬁcient-yet-effective entity embed-\ndings for bert. Findings of the Association for Com-\nputational Linguistics (EMNLP).\nJonathan Raiman and Olivier Raiman. 2018. Deep-\ntype: multilingual entity linking by neural type sys-\ntem evolution. In Thirty-Second AAAI Conference\non Artiﬁcial Intelligence.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled ver-\nsion of bert: smaller, faster, cheaper and lighter.\narXiv:1910.01108v4.\nTeven Le Scao and Alexander M. Rush. 2021. How\nmany data points is a prompt worth? In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (NAACL-HLT).\nYusheng Su, Xu Han, Zhengyan Zhang, Yankai Lin,\nPeng Li, Zhiyuan Liu, Jie Zhou, and Maosong Sun.\n2021. Cokebert: Contextual knowledge selection\nand embedding towards enhanced pre-trained lan-\nguage models. In arXiv:2009.13964.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang\nShen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. 2020.\nLong range arena: A benchmark for efﬁcient trans-\nformers. In International Conference on Learning\nRepresentations (ICLR).\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei1,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021. K-adapter: Infus-\ning knowledge into pre-trained models with adapters.\nFindings of the Association for Computational Lin-\nguistics (ACL-IJCNLP).\nSida I Wang, Mengqiu Wang, Stefan Wager, Percy\nLiang, and Christopher D Manning. 2013. Feature\nnoising for log-linear structured prediction. Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2020.\nKepler: A uniﬁed model for knowledge embedding\nand pre-trained language representation. Transac-\ntions of the Association for Computational Linguis-\ntics (TACL).\nJason Wei and Kai Zou. 2019. Eda: Easy data augmen-\ntation techniques for boosting performance on text\nclassiﬁcation tasks. Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP).\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\n1742\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations (EMNLP), pages 38–45, On-\nline. Association for Computational Linguistics.\nQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-\nong, and Quoc V . Le. 2020. Unsupervised data\naugmentation for consistency training. 34th Con-\nference on Neural Information Processing Systems\n(NeurIPS).\nZhang Xie, Sida I. Wang, Jiwei Li, Daniel Levy, Aim-\ning Nie, Dan Jurafsky, and Andrew Y . Ng. 2017.\nData noising as smoothing in neural network lan-\nguage models. International Conference on Learn-\ning Representations (ICLR).\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained Encyclope-\ndia: Weakly Supervised Knowledge-Pretrained Lan-\nguage Model. In International Conference on Learn-\ning Representations (ICLR).\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. Luke: Deep\ncontextualized entity representations with entity-\naware self-attention. Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP).\nTianyi Zhang and Tatsunori Hashimoto. 2021. On the\ninductive bias of masked language modeling: From\nstatistical to syntactic dependencies. Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (NAACL-HLT).\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-\ngeli, and Christopher D. Manning. 2017. Position\naware attention and supervised data improve slot ﬁll-\ning. In Empirical Methods in Natural Language Pro-\ncessing (EMNLP).\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: En-\nhanced language representation with informative en-\ntities. Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics (ACL).\nXiangxin Zhu, Dragomir Anguelov, and Deva Ra-\nmanan. 2014. Capturing long-tail distributions of\nobject subcategories. In 2014 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\nA Appendix\nA.1 Dataset Details\nBenchmarks We download the raw datasets\nfrom: https://github.com/thunlp/\nERNIE.\nMetadata We tag original dataset examples with\na state-of-the-art pretrained entity-linking model\nfrom (Orr et al., 2020),7 which was trained on an\nOctober 2020 Wikipedia dump with train, valida-\ntion, test splits of 51M, 4.9M, and 4.9M sentences.\nFewRel includes entity annotations. The types we\nuse as category metadata for all tasks are those\nappearing at least 100 times in Wikidata for enti-\nties this Wikipedia training data used bh Orr et al.\n(2020). Descriptions are sourced from Wikidata\ndescriptions and the ﬁrst 50 words of the entity\nWikipedia page. Table 3 reports the availability of\nmetadata for examples across the benchmark tasks.\nA.2 Training Details\nWe use the pretrained BERT-base-uncased model\nfor each task to encode the input text. We take the\nhidden layer representation corresponding to the\n[CLS] token and use a linear classiﬁcation layer for\nprediction. All models are trained on 1 Tesla P100\nGPU (1.5 min/epoch for OpenEntity, 7.5 min/epoch\nfor FewRel, 28 min/epoch for TACRED). For all\ntasks, we select the best learning rate from {1e-6,\n2e-6, 1e-5, 2e-5, 1e-4} and use the scoring imple-\nmentations released by Zhang et al. (2019).\nEntity Typing Hyperparameters include 2e-5\nlearning rate, no regularization parameter and 256\nmax. sequence length, batch size of 16 and no gra-\ndient accumulation or warmup. We report the test\nscore for the epoch with the best validation score\nwithin 20 epochs.\nRelation Extraction Hyperparameters include\n2e-5 learning rate and no regularization parameter.\nFor FewRel, we use batch size of 16, 512 maximum\nsequence length, and no gradient accumulation or\nwarmup. For TACRED, we use a batch size 48,\n256 maximum sequence length, and no gradient\naccumulation or warmup. We report the test score\nfor the epoch with the best validation score within\n15 epochs (FewRel) and 8 epochs (TACRED).\n7https://github.com/HazyResearch/\nbootleg\n1743\nBenchmark Train Valid Test\nTACRED 68124 22631 15509\nCategory 54k/46k 16k/15k 9k/10k\nDescription 50k/43k 15k/14k 8k/9k\nFewRel 8k 16k 16k\nCategory 8k/8k 16k/15k 16k/15k\nDescription 7k/8k 15k/16k 15k/16k\nOpenEntity 1998 1998 1998\nCategory 674 674 647\nDescription 655 672 649\nTable 3: We show the benchmark split sizes (row 1),\nand the # of examples tagged with category and de-\nscription metadata (rows 2 and 3). We give numbers\nfor the subject and object entity-span on relation extrac-\ntion and the main entity-span for entity-typing. The\ntasks have represent a range of proportions of shaped\nexamples (e.g., essentially all FewRel examples have\nmetadata, while metadata is sparsely available for Ope-\nnEntity).\nA.3 Metadata Implementation Details\nWe report the test score at the epoch with the high-\nest validation score. For the results in Table 1, we\nevaluated the number of metadata tokens to insert,\nwhether place the tokens directly following or at\nthe end of the example, and whether to use blank\nnoising on the metadata tokens. Metadata tokens\nare ranked by Algorithm 1.\nWe use up to 20 metadata categories per subject\nand object on FewRel, up to 25 metadata categories\nper subject on OpenEntity, and up to 5 metadata cat-\negories per subject and object on TACRED. Note\nthat categories (e.g., “United States federal execu-\ntive department”) can include multiple tokens, se-\nlecting these maximum values by grid search. For\nFewRel and OpenEntity, we insert metadata tokens\ndirectly after the corresponding entity mention, and\nfor TACRED, we inserted all metadata at the end\nof the example. For OpenEntity we randomly mask\n10% of metadata tokens at training time as implicit\nregularization, and for relation extraction, we use\nno blank noising. The impact of position and blank\nnoising are further discussed in Appendix B.3.\nA.4 Baseline Implementations\nWe produce numbers for key baselines which do\nnot report for the benchmarks we consider, using\nprovided code.8 9\n• We produce numbers for KnowBERT-Wiki\non TACRED-Revisited using a learning rate\nof 3e−5, β2 = 0.98, and choosing the best\nscore for epochs ∈1, 2, 3, 4 and the remaining\nprovided conﬁgurations.\n• We produce numbers for ERNIE on TACRED-\nRevisited using the provided training script\nand conﬁgurations they use for the original\nTACRED task.\nB Additional Experiments\nB.1 Task Agnostic Metadata Effects\nIn Table 4 we report the same experiment con-\nducted in Section 4.1, for all benchmark tasks con-\nsidered in this work. Each point represents the\nmedian test score over 3 random seeds.\nB.2 Metadata Noise\nNoisier metadata appear to provide implicit regu-\nlarization. Noise arises from varied word choice\nand order, as found in entity descriptions, or blank\nnoising (i.e. random token deletion).\nHere we provide initial empirical results.\nBlank noising (Xie et al., 2017) by randomly\nmasking 10% of inserted metadata tokens during\ntraining leads to a consistent boost on OpenEn-\ntity: 0.1 (“High Rank”), 0.5 (“Popular”), 0.5 (“Low\nRank”) F1 points higher than the respective scores\nfrom Table 2 over the same 3 random seeds. We\nobserve no consistent beneﬁt from masking on\nFewRel. Since metadata are sparsely available for\nOpenEntity examples, we hypothesize that blank\nnoising of the category tokens can prevent over-\nreliance on the signal. Future work could inves-\ntigate advanced masking strategies, for example\nmasking discriminative words in the training data.\nDescriptions use varied word choice and order\nvs. category metadata. 10 To study whether shap-\ning with description versus category tokens lead\nthe model to rely more on metadata tokens, we\nconsider two shaping schemes that use 10 meta-\ndata tokens: 10 category tokens and 5 category,\n5 description, where the categories are randomly\nselected. We observe both give the ∼same score\n8https://github.com/allenai/kb\n9https://github.com/thunlp/ERNIE\n10Over FewRel training data: on average a word in the set\nof descriptions appears 8 times vs. 18 times for words in the\nset of categories, and the description set contains 3.3x the\nnumber of unique words vs. set of categories.\n1744\nBenchmark R2\nFewRel 0.985\nTACRED 0.782*\nOpenEntity 0.956\nTable 4: Correlation (R2) between test F1 of ˆpφ (no ad-\nditional pretraining) vs. perplexity of the independent\nmodel ˆpθ (w/ additional pretraining) for three tasks, us-\ning the procedure described in Figure 2. *Without one\noutlier corresponding to shaping with all random to-\nkens (R2 = 0.02 with this point).\non FewRel, 89.8 F1 and 89.5 F1, and use models\ntrained with these two schemes to evaluate on test\ndata where 10% of metadata tokens per example\nare randomly removed. Performance drops by 1.4\nF1 for the former and 1.0 F1 for the latter.\nB.3 Implementation Choices\nWe also analyze the degree of sensitivity of meta-\ndata shaping to how the metadata are inserted in\nexamples (e.g., special tokens, the number of meta-\ndata tokens, and position).\nBoundary Tokens Designating the boundary be-\ntween original tokens in the example and inserted\nmetadata tokens improves model performance.\nInserting boundary tokens (e.g., “#”) in the ex-\nample, at the start and end of a span of inserted\nmetadata, consistently provides a boost across the\ntasks. Comparing performance with metadata and\nboundary tokens to performance with metadata and\nno boundary tokens, we observe a 0.7 F1 (FewRel),\n1.4 F1 (OpenEntity) boost in our main results. We\nuse boundary tokens for all results in this work.\nTask Structure Tokens designate relevant enti-\nties in the examples (e.g., “[START_SUBJECT]”\nand “[END_SUBJECT]”). With no other shaping,\ninserting these tokens provides a 26.3 (FewRel),\n24.7 (OpenEntity) F1 point boost vs. training the\nBERT model without task structure tokens. These\ntokens are already commonly used.\nToken Insertion We observe low sensitivity to\nincreasing the context length and to token place-\nment (i.e., inserting metadata directly-following the\nentity-span vs at the end of the sentence).\nWe evaluate performance a the maximum num-\nber of inserted tokens per entity, n, increases. 11\n11Per subject and object entity for FewRel, and per main\nentity for OpenEntity. I.e., n = 10 for FewRel yields a\nmaximum of 20 total inserted tokens for the example.\nWe insert metadata tokens in a random order (to\ncontrol for the effect of different metadata hav-\ning different levels of class-discriminativeness)\nand observe that for FewRel, n ∈{1, 5, 10,\n20} gives {85.4,86.4,87.6,88.5}test F1. On\nOpenEntity, n ∈ {1, 5, 10, 20, 40} gives\n{74.9,75.7,74.8,74.5,75.8}test F1. Overall per-\nformance changes gracefully with n and we ob-\nserve low sensitivity to longer contexts.\nThe beneﬁt of inserting metadata directly-\nfollowing the entity span vs at the end of the ex-\nample differed across tasks (e.g., for TACRED,\nplacement at the end performs better, for the other\ntasks, placement directly-following performs bet-\nter), though the observed difference was small. In\nSection 4, tokens are inserted directly-following\nthe relevant entity span for all tasks.\n1745",
  "topic": "Metadata",
  "concepts": [
    {
      "name": "Metadata",
      "score": 0.9397485852241516
    },
    {
      "name": "Computer science",
      "score": 0.8644047379493713
    },
    {
      "name": "Ask price",
      "score": 0.6183617115020752
    },
    {
      "name": "Process (computing)",
      "score": 0.5358796119689941
    },
    {
      "name": "Information retrieval",
      "score": 0.5223233103752136
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5167649984359741
    },
    {
      "name": "Data element",
      "score": 0.5053606629371643
    },
    {
      "name": "Knowledge base",
      "score": 0.5007889270782471
    },
    {
      "name": "Metadata repository",
      "score": 0.47418197989463806
    },
    {
      "name": "Inference",
      "score": 0.4727628827095032
    },
    {
      "name": "Substring",
      "score": 0.4687293469905853
    },
    {
      "name": "Metadata modeling",
      "score": 0.44787484407424927
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4362680912017822
    },
    {
      "name": "Artificial intelligence",
      "score": 0.29669880867004395
    },
    {
      "name": "World Wide Web",
      "score": 0.20413485169410706
    },
    {
      "name": "Data structure",
      "score": 0.1502974033355713
    },
    {
      "name": "Programming language",
      "score": 0.12225469946861267
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Economy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}