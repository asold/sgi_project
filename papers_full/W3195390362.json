{
  "title": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers",
  "url": "https://openalex.org/W3195390362",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3160716198",
      "name": "Yu Xumin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3088555891",
      "name": "Rao, Yongming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350529652",
      "name": "Wang, Ziyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1691618375",
      "name": "Liu, Zuyan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2609714807",
      "name": "Lu, Jiwen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1950278301",
      "name": "Zhou Jie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2991213871",
    "https://openalex.org/W2997088169",
    "https://openalex.org/W2560722161",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W2886499109",
    "https://openalex.org/W3108012830",
    "https://openalex.org/W2944579304",
    "https://openalex.org/W3034584726",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2788158258",
    "https://openalex.org/W3103005696",
    "https://openalex.org/W2427751284",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2524140598",
    "https://openalex.org/W2798314605",
    "https://openalex.org/W2963727135",
    "https://openalex.org/W3034126769",
    "https://openalex.org/W3170469318",
    "https://openalex.org/W2342277278",
    "https://openalex.org/W2963735494",
    "https://openalex.org/W2586114507",
    "https://openalex.org/W2559882727",
    "https://openalex.org/W2335364074",
    "https://openalex.org/W3104141662",
    "https://openalex.org/W2963622297",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2885317998",
    "https://openalex.org/W2963648573",
    "https://openalex.org/W2338532005",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2778361827",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2963182550",
    "https://openalex.org/W3035014292",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2912607492",
    "https://openalex.org/W2594519801",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W2963716836",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W2930206194",
    "https://openalex.org/W2963509914",
    "https://openalex.org/W2953668091",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W2940600945"
  ],
  "abstract": "Point clouds captured in real-world applications are often incomplete due to the limited sensor resolution, single viewpoint, and occlusion. Therefore, recovering the complete point clouds from partial ones becomes an indispensable task in many practical applications. In this paper, we present a new method that reformulates point cloud completion as a set-to-set translation problem and design a new model, called PoinTr that adopts a transformer encoder-decoder architecture for point cloud completion. By representing the point cloud as a set of unordered groups of points with position embeddings, we convert the point cloud to a sequence of point proxies and employ the transformers for point cloud generation. To facilitate transformers to better leverage the inductive bias about 3D geometric structures of point clouds, we further devise a geometry-aware block that models the local geometric relationships explicitly. The migration of transformers enables our model to better learn structural knowledge and preserve detailed information for point cloud completion. Furthermore, we propose two more challenging benchmarks with more diverse incomplete point clouds that can better reflect the real-world scenarios to promote future research. Experimental results show that our method outperforms state-of-the-art methods by a large margin on both the new benchmarks and the existing ones. Code is available at https://github.com/yuxumin/PoinTr",
  "full_text": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers\nXumin Yu*, Yongming Rao *, Ziyi Wang, Zuyan Liu, Jiwen Lu ‚Ä†, Jie Zhou\nDepartment of Automation, Tsinghua University, China\nState Key Lab of Intelligent Technologies and Systems, China\nBeijing National Research Center for Information Science and Technology, China\nyuxm20@mails.tsinghua.edu.cn; raoyongming95@gmail.com;\n{wziyi20, liuzuyan19}@mails.tsinghua.edu.cn; {lujiwen, jzhou}@tsinghua.edu.cn\nAbstract\nPoint clouds captured in real-world applications are of-\nten incomplete due to the limited sensor resolution, single\nviewpoint, and occlusion. Therefore, recovering the com-\nplete point clouds from partial ones becomes an indispens-\nable task in many practical applications. In this paper, we\npresent a new method that reformulates point cloud com-\npletion as a set-to-set translation problem and design a new\nmodel, called PoinTr that adopts a transformer encoder-\ndecoder architecture for point cloud completion. By rep-\nresenting the point cloud as a set of unordered groups of\npoints with position embeddings, we convert the point cloud\nto a sequence of point proxies and employ the transform-\ners for point cloud generation. To facilitate transformers\nto better leverage the inductive bias about 3D geometric\nstructures of point clouds, we further devise a geometry-\naware block that models the local geometric relationships\nexplicitly. The migration of transformers enables our model\nto better learn structural knowledge and preserve detailed\ninformation for point cloud completion. Furthermore, we\npropose two more challenging benchmarks with more di-\nverse incomplete point clouds that can better reÔ¨Çect the\nreal-world scenarios to promote future research. Experi-\nmental results show that our method outperforms state-of-\nthe-art methods by a large margin on both the new bench-\nmarks and the existing ones. Code is available at https:\n//github.com/yuxumin/PoinTr.\n1. Introduction\nRecent developments in 3D sensors largely boost re-\nsearches in 3D computer vision. One of the most com-\nmonly used 3D data format is the point cloud, which re-\nquires less memory to store but convey detailed 3D shape\n*Equal contribution.\n‚Ä†Corresponding author.\nIncomplete\nPoint cloud\nSet of\nPoints proxies\nGeomentary-aware\nTransformer\nSet of\nPredicted proxies\nComplete\nPoint cloud\nEncoder Decoder\nFigure 1: PoinTr is designed for point cloud completion task. It\ntakes the downsampled partial point clouds as inputs (gray points),\nand predicts the missing parts and upsamples the known parts si-\nmultaneously (blue points). We propose to formulate the point\ncloud completion task as a set-to-set translation task and use a\ntransformer encoder-decoder architecture to learn the complex de-\npendencies among the point groups. Furthermore, we design two\nnew benchmarks with more diverse tasks ( i.e., upsampling and\ncompletion of point cloud), more diverse categories ( i.e., from 8\ncategories to 55 categories), more diverse viewpoints (i.e., from 8\nviewpoints to all possible viewpoints) and more diverse levels of\nincompleteness (i.e., missing 25% to 75% points of the ground-\ntruth point clouds) to better reÔ¨Çect the real-world scenarios and\npromote future research.\ninformation. However, point cloud data from existing 3D\nsensors are not always complete and satisfactory because\nof inevitable self-occlusion, light reÔ¨Çection, limited sensor\nresolution, etc. Therefore, recovering complete point clouds\nfrom partial and sparse raw data becomes an indispensable\ntask with ever-growing signiÔ¨Åcance.\nOver the years, researchers have tried many approaches\nto tackle this problem in the realm of deep learning. Early\nattempts on point cloud completion [6, 15, 32, 33, 24, 38,\n20, 19, 53, 49, 42] try to migrate mature methods from 2D\n1\narXiv:2108.08839v1  [cs.CV]  19 Aug 2021\ncompletion tasks to 3D point clouds by voxelization and\n3D convolutions. However, these methods suffer from a\nheavy computational cost that grows cubically as the spa-\ntial resolution increases. With the success of PointNet and\nPointNet++ [27, 28], directly processing 3D coordinates\nbecomes the mainstream of point cloud based 3D anal-\nysis. The technique is further applied to many pioneer\nworks [1, 51, 37, 16, 23, 14, 31] in point cloud comple-\ntion task, in which an encoder-decoder based architecture is\ndesigned to generate complete point clouds. However, the\nbottleneck of such methods lies in the max-pooling opera-\ntion in the encoding phase, where Ô¨Åne-grained information\nis lost and can hardly be recovered in the decoding phase.\nReconstructing complete point cloud is a challenging\nproblem since the structural information required in the\ncompletion task runs counter to the unordered and un-\nstructured nature of point cloud data. Therefore, learning\nstructural features and long-range correlations among lo-\ncal parts of the point cloud becomes the key ingredient to-\nwards better point cloud completion. In this paper, we pro-\npose to adopt Transformers [39], one of the most success-\nful architecture in Natural Language Processing (NLP), to\nlearn the structural information of pairwise interactions and\nglobal correlations for point cloud completion. Our model,\nnamed PoinTr, is characterized by Ô¨Åve key components:\n1) Encoder-Decoder Architecture: We adopt the encoder-\ndecoder architecture to convert point cloud completion as\na set-to-set translation problem. The self-attention mech-\nanism of transformers models all pairwise interactions be-\ntween elements in the encoder, while the decoder reasons\nabout the missing elements based on the learnable pairwise\ninteractions among features of the input point cloud and\nqueries; 2) Point Proxy: We represent the set of point clouds\nin a local region as a feature vector called Point Proxy. The\ninput point cloud is convert to a sequence of Point Prox-\nies, which are used as the inputs of our transformer model;\n3) Geometry-aware Transformer Block: To facilitate trans-\nformers to better leverage the inductive bias about 3D ge-\nometric structures of point clouds, we design a geometry-\naware block that models the geometric relations explicitly;\n4) Query Generator : We use dynamic queries instead of\nÔ¨Åxed queirs in the decoder, which are generated by a query\ngeneration module that summarizes the features produced\nby the encoder and represents the initial sketch of the miss-\ning points; 5) Multi-Scale Point Cloud Generation: We de-\nvise a multi-scale point generation module to recover the\nmissing point cloud in a coarse-to-Ô¨Åne manner.\nAs another contribution, we argue that existing bench-\nmarks are not representative enough to cover real-world sce-\nnarios of incompleted point clouds. Therefore, we intro-\nduce two more challenging benchmarks that contain more\ndiverse tasks (i.e., joint upsampling and completion of point\ncloud), more object categories (i.e., from 8 categories to 55\ncategories), more diverse views points ( i.e., from 8 view-\npoints to all possible viewpoints) and more diverse level\nof incompleteness ( i.e., missing 25% to 75% points of the\nground-truth point clouds). We evaluate our method on both\nthe new benchmarks and the widely used PCN dataset [51]\nand KITTI benchmark [10]. Experiments demonstrate that\nPointTr outperforms previous state-of-the-art methods on\nall benchmarks by a large margin. The main contributions\nof this paper are summarized in Figure 1.\n2. Related Work\n3D Shape Completion.Traditional methods for 3D shape\ncompletion tasks often adopt voxel grids or distance Ô¨Åelds\nto describe 3D objects [6, 15, 33]. Based on such structured\n3D representations, the powerful 3D convolutions are used\nand achieve a great success in the tasks of 3D reconstruc-\ntion [4, 11] and shape completion [6, 15, 46]. However, this\ngroup of methods suffers from heavy memory consump-\ntion and computational burden. Although these issues are\nfurther alleviated by methods based on sparse representa-\ntions [34, 41, 12], the quantization operation in these meth-\nods still cause a signiÔ¨Åcant loss in detailed information. Dif-\nferent from the above methods, researchers gradually start\nto use unstructured point clouds as the representation of 3D\nobjects, given the small memory consumption and strong\nability to represent Ô¨Åne-grained details. Nevertheless, the\nmigration from structured 3D data understanding to point\nclouds analysis is non-trivial, since the commonly used con-\nvolution operator is no longer suitable for unordered points\nclouds. PointNet and its variants [27, 28] are the pioneer\nwork to directly process 3D coordinates and inspire the re-\nsearches in many downstream tasks. In the realm of point\ncloud completion, PCN [51] is the Ô¨Årst learning-based ar-\nchitecture, which proposes an Encoder-Decoder framework\nand adopts a FoldingNet to map the 2D points onto a 3D\nsurface by mimicking the deformation of a 2D plane. After\nPCN, many other methods [37, 16, 48, 18] spring up, pursu-\ning point clouds completion in higher resolution with better\nrobustness.\nTransformers. Transformers [39] are Ô¨Årst introduced as\nan attention-based framework in Natural Language Process-\ning (NLP). Transformer models often utilize the encoder-\ndecoder architecture and are characterized by both self-\nattention and cross-attention mechanisms. Transformer\nmodels have proven to be very helpful to the tasks that\ninvolve long sequences thanks to the self-attention mecha-\nnism. The cross-attention mechanism in the decoder exploit\nthe encoder information to learn the attention map of query\nfeatures, which making transformers powerful in generation\ntasks. By taking the advantages of both self-attention and\ncross-attention mechanisms, transformers have a strong ca-\npability to handle long sequence input and enhance infor-\nFPS\nIncompletePointCloud\nMLP\n‚Ä¶\n‚Ä¶Center Points\nPoints Proxies\n+\nFeatureExtractor\nPositionalEmbeddingGeometry-awareTransformerEncoder\nDynamicQueries\nGeometry-awareTransformerDecoder\n‚Ä¶\nQueryGenerator\n‚Ä¶\nPredictedCenters\nFoldingNet\nPredictedProxies\nPredictedMissingPointCloud\nFigure 2: The Pipeline of PoinTr. We Ô¨Årst downsample the input partial point cloud to obtain the center points. Then, we use a lightweight\nDGCNN [44] to extract the local features around the center points. After adding the position embedding to the local feature, we use a\ntransformer encoder-decoder architecture to predict the point proxies for the missing parts. A simple MLP and FoldingNet are used to\ncomplete the point cloud based on the predicted point proxies in a coarse-to-Ô¨Åne manner.\nmation communications between the encoder and the de-\ncoder. In the past few years, transformers have dominated\nthe tasks that take long sequences as input and gradually re-\nplaced RNNs [40] in many domains. Now they begin their\njourney in computer vision [7, 22, 35, 29, 25, 30].\n3. Approach\nThe overall framework of PoinTr is illustrated in Fig-\nure 2. We will introduce our method in detail as follows.\n3.1. Set-to-Set Translation with Transformers\nThe primary goal of our method is to leverage the im-\npressive sequence-to-sequence generation ability of trans-\nformer architecture for point cloud completion tasks. We\npropose to Ô¨Årst convert the point cloud to a set of feature\nvectors, point proxies, that represent the local regions in the\npoint clouds (we will describe in Section 3.2). By anal-\nogy to the language translation pipeline, we model point\ncloud completion as a set-to-set translation task, where\nthe transformers take the point proxies of the partial point\nclouds as the inputs and produce the point proxies of the\nmissing parts. SpeciÔ¨Åcally, given the set of point prox-\nies F = {F1, F2, ..., FN }that represents the partial point\ncloud, we model the process of point cloud completion as a\nset-to-set translation problem:\nV= ME(F), H= MD(Q, V), (1)\nwhere ME and MD are the encoder and decoder mod-\nels, V = {V1, V2, ..., VN }are the output features of the\nencoder, Q= {Q1, Q2, ..., QM }are the dynamic queries\nfor the decoder, H= {H1, H2, ..., HM }are the predicted\npoint proxies of the missing point cloud, andM is the num-\nber of the predicted point proxies. The recent success in\nNLP tasks like text translation and question answering [8]\nhave clearly demonstrated the effectiveness of transformers\nto solve this kind of problem. Therefore, we propose to\nadopt a transformer-based encoder-decoder architecture to\nsolve the point cloud completion problem.\nThe encoder-decoder architecture consists of LE and\nLD multi-head self-attention layers [39] in the encoder\nand decoder, respectively. The self-attention layer in the\nencoder Ô¨Årst updates proxy features with both long-range\nand short-range information. Then a feed forward network\n(FFN) further updates the proxy features with an MLP ar-\nchitecture. The decoder utilizes self-attention and cross-\nattention mechanisms to learn structural knowledge. The\nself-attention layer enhances the local features with global\ninformation, while the cross-attention layer explores the re-\nlationship between queries and outputs of the encoder. To\npredict the point proxies of the missing parts, we propose to\nuse dynamic query embeddings, which makes our decoder\nmore Ô¨Çexible and adjustable for different types of objects\nand their missing information. More details about the trans-\nformer architecture can be found in the supplementary ma-\nterial and [8, 39].\nNote that beneÔ¨Åting from the self-attention mechanism\nin transformers, the features learned by the transformer net-\nwork are invariant to the order of point proxies, which is\nalso the basis of using transformers to process point clouds.\nConsidering the strong ability to capture data relationships,\nwe expect the transformer architecture to be a promising al-\nternative for deep learning on point clouds.\n3.2. Point Proxy\nThe Transformers in NLP take as input a 1D sequence of\nword embeddings [39]. To make 3D point clouds suitable\nfor transformers, the Ô¨Årst step is to convert the point cloud\nto a sequence of vectors. A trivial solution is directly feed-\ning the sequence of xyz coordinates to the transformers.\nHowever, since the computational complexity of the trans-\nformers is quadratic to the sequence length, this solution\nwill lead to an unacceptable cost. Therefore, we propose to\nrepresent the original point cloud as a set of point proxies.\nMulti-HeadSelf-AttentionùëâùêæQ\nLinear kNNqueryLinearMaxPoolingConcatenateLinear\nùëâùëù! ùëù\"\nResidual Connectorùëã#$%\nùëã#\nMulti-HeadSelf-AttentionùëâùêæQ\nLinearResidual Connectorùëã#$%\nùëã#\nVanilla TransformerBlock Geometry-aware TransformerBlock\nFigure 3: Comparisons of the vanilla transformer block and the\nproposed geometry-aware transformer block.\nA point proxy represents a local region of the point clouds.\nInspired by the set abstraction operation in [28], we Ô¨Årst\nconduct furthest point sample (FPS) to locate a Ô¨Åxed num-\nber N of point centers {q1, q2, ..., qN }in the partial point\ncloud. Then, we use a light-weight DGCNN [44] with hi-\nerarchical downsampling to extract the feature of the point\ncenters from the input point cloud. The point proxy Fi is\na feature vector that captures the local structure around qi,\nwhich can be computed as:\nFi = F‚Ä≤\ni + œï(qi), (2)\nwhere F‚Ä≤\ni is the feature of point qi that is extracted using\nthe DGCNN model, and œï is another MLP to capture the\nlocation information of the point proxy. The Ô¨Årst term rep-\nresents the semantic patterns of the local region, and the\nsecond term is inspired by the position embedding [3] oper-\nation in transformers, which explicitly encodes the global\nlocation of the point proxy. The detailed architecture of\nthe feature extraction model can be found in Supplementary\nMaterial.\n3.3. Geometry-aware Transformer Block\nOne of the key challenges of applying transformers for\nvision tasks is the self-attention mechanism in transformers\nlacks some inductive biases inherent to conventional vision\nmodels like CNNs and point cloud networks which explic-\nitly model the structures of vision data. To facilitate trans-\nformers to better leverage the inductive bias about 3D ge-\nometric structures of point clouds, we design a geometry-\naware block that models the geometric relations, which can\nbe a plug-and-play module to incorporate with the attention\nblocks in any transformer architectures. The details of the\nproposed block are shown in Figure 3. Different from the\nself-attention module that uses the feature similarity to cap-\nture the semantic relation, we propose to use kNN model\nto capture the geometric relation in the point cloud. Given\nthe query coordinates pQ, we query the features of the near-\nest keys according to the key coordinates pk. Then we fol-\nlow the practice of DGCNN [44] to learn the local geomet-\nric structures by feature aggregation with a linear layer fol-\nlowed by the max pooing operation. The geometric feature\nand semantic feature are then concatenated and mapped to\nthe original dimensions to form the output.\n3.4. Query Generator\nThe queries Qserve as the initial state of the predicted\nproxies. To make sure the queries correctly reÔ¨Çect the\nsketch of the completed point cloud, we propose a query\ngenerator module to generate the query embeddings dy-\nnamically conditioned on the encoder outputs. SpeciÔ¨Åcally,\nwe Ô¨Årst summarize Vwith a linear projection to higher di-\nmensions followed by the max pooing operation. Similar\nto [51], we use a linear projection layer to directly gener-\nate M √ó3 dimension features that can be reshaped as the\nM coordinates {c1, c2, ..., cM }. Lastly, we concatenate the\nglobal feature of the encoder and the coordinates, and use\nan MLP to produce the query embeddings.\n3.5. Multi-Scale Point Cloud Generation\nThe goal of our encoder-decoder network is to predict\nthe missing parts of incomplete point clouds. However, we\ncan only get predictions for missing proxies from the trans-\nformer decoder. Therefore, we propose a multi-scale point\ncloud generation framework to recover missing point clouds\nat full resolution. To reduce redundant computations, we\nreuse the M coordinates produced by the query generator\nas the local centers of the missing point cloud. Then, we\nutilize a FoldingNet [50] f to recover detailed local shapes\ncentered at the predicted proxies:\nPi = f(Hi) +ci, i = 1, 2, ..., M. (3)\nwhere Pi is the set of neighboring points centered at ci.\nFollowing previous work [16], we only predict the missing\nparts of the point cloud and concatenate them with the input\npoint cloud to obtain the complete objects. Both predicted\nproxies and recovered point clouds are supervised during\nthe training process, and the detailed loss function will be\nintroduced in the following section.\n3.6. Optimization\nThe loss function for point cloud completion should pro-\nvide a quantitative measurement for the quality of output.\nHowever, since the point clouds are unordered, many loss\nfunctions that directly measure the distance between two\npoints ( i.e. ‚Ñì2 distance) are unsuitable. Fan et al . [9] in-\ntroduce two metrics that are invariant to the permutation\nof points, which are Chamfer Distance (CD) and Earth\nMover‚Äôs Distance (EMD). We adopt Chamfer Distance as\nour loss function for its O(N log N) complexity. We use\nCto represent the nC local centers and Pto represent nP\npoints of the completed point cloud. Give the ground-truth\ncompleted point cloud G, the loss functions for these two\npredictions can be written as:\nJ0 = 1\nnC\n‚àë\nc‚ààC\nmin\ng‚ààG\n‚à•c ‚àíg‚à•+ 1\nnG\n‚àë\ng‚ààG\nmin\nc‚ààC\n‚à•g ‚àíc‚à•,\nJ1 = 1\nnP\n‚àë\np‚ààP\nmin\ng‚ààG\n‚à•p ‚àíg‚à•+ 1\nnG\n‚àë\ng‚ààG\nmin\np‚ààP\n‚à•g ‚àíp‚à•.\nNote that we also concatenate the predicted local centers\nand the centers of the input point cloud to form the local\ncenters of the whole object C. We directly use the high-\nresolution point cloud Gto supervise the sparse point cloud\nCto encourage them to have similar distributions. Our Ô¨Ånal\nobjective function is the sum of these two objectives J =\nJ0 + J1.\n4. Experiments\nIn this section, we Ô¨Årst introduce the new benchmarks\nfor diverse point cloud completion and the evaluation met-\nric. Then, we show the results of both our method and sev-\neral baseline methods on our new benchmarks. Lastly, we\ndemonstrate the effectiveness of our model on the widely\nused PCN dataset and KITTI benchmark. We also provide\nablation study and visual analysis of our method.\n4.1. Benchmarks for Diverse Point Completion\nWe choose to generate the samples in our benchmarks\nbased on the synthetic dataset, ShapeNet [47], because it\ncontains the complete object models that cannot be obtained\nfrom real-world datasets like ScanNet [5] and S3DIS [2].\nWhat makes our benchmarks distinct is that our benchmarks\ncontain more object categories, more incomplete patterns\nand more viewpoints. Besides, we pay more attention to\nthe ability of networks to deal with the objects from novel\ncategories that do not appear in the training set.\nShapeNet-55 Benchmark: In this benchmark, we use all\nthe objects in ShapeNet from 55 categories. Most ex-\nisting datasets for point cloud completion like PCN [51]\nonly consider a relatively small number of categories ( e.g.,\n8 categories in PCN). However, the incompleted point\nclouds from the real-world scenarios are much more di-\nverse. Therefore, we propose to evaluate the point cloud\ncompletion models on all 55 categories in ShapeNet to more\ncomprehensively test the ability of models with a more di-\nverse dataset. We split the original ShapeNet using the 80-\n20 strategy: we randomly sample 80% objects from each\ncategory to form the training set and use the rest for eval-\nuation. As a result, we get 41,952 models for training and\n10,518 models for testing. For each object, we randomly\nsample 8,192 points from the surface to obtain the point\ncloud.\nShapeNet-34 Benchmark:In this benchmark, we want to\nexplore another important issue in point cloud completion:\nthe performance on novel categories. We believe it is nec-\nessary to build a benchmark for this task to better evaluate\nthe generalization performance of models. We Ô¨Årst split the\norigin ShapeNet into two parts: 21 unseen categories and 34\nseen categories. In the seen categories, we randomly sample\n100 objects from each category to construct a test set of the\nseen categories (3,400 objects in total) and leave the rest as\nthe training set, resulting in 46,765 object models for train-\ning. We also construct another test set consisting of 2,305\nobjects from 21 novel categories. We evaluate the perfor-\nmance on both the seen and unseen categories to show the\ngeneralization ability of models.\nTraining and Evaluation:In both benchmarks, the partial\npoint clouds for training are generated online. We sample\n2048 points from the object as the input and 8192 points as\nthe ground truth. In order to mimic the real-world situation,\nwe Ô¨Årst randomly select a viewpoint and then remove the n\nfurthest points from the viewpoint to obtain a training par-\ntial point cloud. Although the projection method proposed\nin [51] is a better approximation to real scans, our strategy is\nmore Ô¨Çexible and efÔ¨Åcient. Our experiments on KITTI also\nshow the model learned on our dataset works well when\nÔ¨Ånetuning to real-world scans. Besides, our strategy also\nensures the diversity of our training samples in the aspect\nof viewpoints. During training, n is randomly chosen from\n2048 to 6144 (25% to 75% of the complete point cloud), re-\nsulting in different level of incompleteness. We then down-\nsample the remaining point clouds to 2048 points as the in-\nput data for models.\nDuring evaluation, we Ô¨Åx 8 view points and n is set to\n2048, 4096 or 6144 (25%, 50% or 75% of the whole point\ncloud) for convenience. According to the value of n, we\ndivide the test samples into three difÔ¨Åculty degrees, simple,\nmoderate and hard in our experiments. In the following ex-\nperiments, we will report the performance for each method\nin simple, moderate and hard to show the ability of each\nnetwork to deal with tasks at difÔ¨Åculty levels. In addition,\nwe use the average performance under three difÔ¨Åculty de-\ngrees to report the overall performance (Avg).\n4.2. Evaluation Metric\nWe follow the existing works [51, 37, 16, 48] to use the\nmean Chamfer Distance as the evaluation metric, which can\nmeasure distance between the prediction point cloud and\nground-truth in set-level. For each prediction, the Chamfer\nDistance between the prediction point setPand the ground-\ntruth point set Gis calculated by:\ndCD(P, G) = 1\n|P|\n‚àë\np‚ààP\nmin\ng‚ààG\n‚à•p ‚àíg‚à•+ 1\n|G|\n‚àë\ng‚ààG\nmin\np‚ààP\n‚à•g ‚àíp‚à•\nFollowing the previous methods, we use two versions of\nChamfer distance as the evaluation metric to compare the\nperformance with existing works. CD- ‚Ñì1 uses L1-norm to\nTable 1: Results of our methods and state-of-the-art methods on ShapeNet-55. We report the detailed results for each method on 10\ncategories and the overall results on 55 categories for three difÔ¨Åculty degrees. We use CD-S, CD-M and CD-H to represent the CD results\nunder the Simple, Moderate and Hard settings. We also provide results under the F-Score@1% metric.\nTable Chair Airplane Car Sofa Bird\nhouse Bag Remote Key\nboard Rocket CD-S CD-M CD-H CD-Avg F1\nFoldingNet [50] 2.53 2.81 1.43 1.98 2.48 4.71 2.79 1.44 1.24 1.48 2.67 2.66 4.05 3.12 0.082\nPCN [51] 2.13 2.29 1.02 1.85 2.06 4.50 2.86 1.33 0.89 1.32 1.94 1.96 4.08 2.66 0.133\nTopNet [37] 2.21 2.53 1.14 2.18 2.36 4.83 2.93 1.49 0.95 1.32 2.26 2.16 4.3 2.91 0.126\nPFNet [16] 3.95 4.24 1.81 2.53 3.34 6.21 4.96 2.91 1.29 2.36 3.83 3.87 7.97 5.22 0.339\nGRNet [48] 1.63 1.88 1.02 1.64 1.72 2.97 2.06 1.09 0.89 1.03 1.35 1.71 2.85 1.97 0.238\nPoinTr 0.81 0.95 0.44 0.91 0.79 1.86 0.93 0.53 0.38 0.57 0.58 0.88 1.79 1.09 0.464\nTable 2: Results of our methods and state-of-the-art methods on ShapeNet-34. We report the results of 34 seen categories and 21 unseen\ncategories in three difÔ¨Åculty degrees. We use CD-S, CD-M and CD-H to represent the CD results under the Simple, Moderate and Hard\nsettings. We also provide results under the F-Score@1% metric.\n34 seen categories 21 unseen categories\nCD-S CD-M CD-H CD-Avg F1 CD-S CD-M CD-H CD-Avg F1\nFoldingNet [50] 1.86 1.81 3.38 2.35 0.139 2.76 2.74 5.36 3.62 0.095\nPCN [51] 1.87 1.81 2.97 2.22 0.154 3.17 3.08 5.29 3.85 0.101\nTopNet [37] 1.77 1.61 3.54 2.31 0.171 2.62 2.43 5.44 3.50 0.121\nPFNet [16] 3.16 3.19 7.71 4.68 0.347 5.29 5.87 13.33 8.16 0.322\nGRNet [48] 1.26 1.39 2.57 1.74 0.251 1.85 2.25 4.87 2.99 0.216\nPoinTr 0.76 1.05 1.88 1.23 0.421 1.04 1.67 3.44 2.05 0.384\ncalculate the distance between two points, while CD-‚Ñì2 uses\nL2-norm instead. We also follow [36] to adopt F-Score as\nanother evaluation metric.\n4.3. Results on ShapeNet-55\nWe Ô¨Årst conduct experiments on ShapeNet-55, which\nconsists of objects from 55 categories. To compare with ex-\nisting methods, We implement FoldingNet [50], PCN [51],\nTopNet [37], PFNet [16] and GRNet [48] on our bench-\nmark according to their open-source code and use the best\nhyper-parameters in their papers for fair comparisons. We\nÔ¨Årst investigate how the existing methods and our method\nperform when there are objects from more categories. The\nlast four columns in Table 1 show that our PoinTr can better\ncope with different situations with diverse viewpoints, di-\nverse object categories, diverse incomplete patterns and di-\nverse incompleteness levels. We achieve 0.58, 0.6 and 0.69\nimprovement in CD- ‚Ñì2 (multiplied by 1000) under three\nsettings (simple, moderate and hard) comparing with the\nSOTA method GRNet [48]. PFNet [16], which proposes\nto directly predict the missing parts of objects, fail in our\nbenchmarks due to the high diversity. We further report the\nperformance on categories with sufÔ¨Åcient and insufÔ¨Åcient\nsamples. We only sample 10 categories out from 55 cate-\ngories to show the results due to the limited space, in which\nTable, chair, Airplane,Car and Sofa contain more than 2500\nsamples in the training set while Birdhouse, Bag, Remote,\nKeyboard and Rocket contain less than 80 samples. We also\nprovide the detailed results for all 55 categories in our sup-\nplementary material. We place the categories with sufÔ¨Å-\ncient samples at the Ô¨Årst Ô¨Åve columns and the categories\nwith insufÔ¨Åcient samples in the following Ô¨Åve columns in\nTable 1. The average CD results for three difÔ¨Åculty de-\ngrees are also reported. Surprisingly, there is no obvious\ndifference between the results on these two kinds of cate-\ngories. However, except for our PoinTr and SOTA method\nGRNet, the imbalance in the number of training samples\nlead to a relatively high CD in the categories with insufÔ¨Å-\ncient samples. Besides, our model achieves 0.46 F-Score on\nShapeNet-55, while the state-of-the-art GRNet only obtain\n0.24 F-Score. These results clearly demonstrate the effec-\ntiveness of PoinTr under the more diverse setting.\n4.4. Results on ShapeNet-34\nOn ShapeNet-34, we also conduct experiments for our\nmethod and other Ô¨Åve state-of-the-art methods. The results\nare shown in Table 2. For the 34 seen categories, we can\nsee our method outperforms all the other methods. For the\n21 unseen categories, we using the networks that are trained\non the 34 seen categories to evaluate the performance on the\nnovel objects from the other 21 categories that do not appear\nin the training phase. We see our method also achieves the\nbest performance in this more challenging setting. Compar-\ning with the results of seen categories, we see in the simple\nI n p u t\nG R N e t\nO u r s\nG . T .\nFigure 4: Point cloud completion results on some objects from\nnovel categories. We show the input point cloud and the ground\ntruth as well as the predictions of GRNet and our model.\nI n p u t\nG R N e t\nO u r s\nV i e w  1\n V i e w  2\n V i e w  1\n V i e w  2\nFigure 5: Qualitative results on the KITTI dataset. In order to\nbetter show the shape of the car, we provide two views of the same\npoint cloud in each case. Our method can recover the complete\npoint cloud of a car with more accurate boundaries and details\n(e.g. tires of cars).\nsetting (25% of point cloud will be removed), the perfor-\nmance drop of our method is less than 0.3. But when the\ndifÔ¨Åculty level increases, the performance gap between seen\ncategories and unseen categories signiÔ¨Åcantly increases. We\nalso visualize the results in Figure 4 to show the effective-\nness of our method on the unseen categories.\n4.5. Results on the Existing Benchmarks\nApart from the experiments on the two newly proposed\nchallenging benchmarks, we also conduct the experiments\non the existing benchmarks including the PCN dataset [51]\nTable 3: Results on the PCN dataset. We use the L1 Chamfer\nDistance to compare with other methods.\nCD-‚Ñì1 (√ó1000) Avg Air Cab Car Cha Lam Sof Tab Wat\nFoldingNet [50]14.31 9.49 15.80 12.61 15.55 16.41 15.97 13.65 14.99\nAtlasNet [13]10.85 6.37 11.94 10.10 12.06 12.37 12.99 10.33 10.61\nPCN [51] 9.64 5.50 22.70 10.63 8.70 11.00 11.34 11.68 8.59\nTopNet [37] 12.15 7.61 13.31 10.90 13.82 14.44 14.78 11.22 11.12\nMSN [17] 10.0 5.6 11.9 10.3 10.2 10.7 11.6 9.6 9.9\nGRNet [48] 8.83 6.45 10.37 9.45 9.41 7.96 10.51 8.44 8.04\nPMP-Net [45]8.73 5.65 11.24 9.64 9.51 6.95 10.83 8.72 7.25\nCRN [43] 8.51 4.79 9.97 8.31 9.49 8.94 10.69 7.81 8.05\nPoinTr 8.38 4.75 10.47 8.68 9.39 7.75 10.93 7.78 7.29\nTable 4: Ablation study on the PCN dataset. We investigate dif-\nferent designs including query generator (Query), DGCNN feature\nextractor (DGCNN) and Geometry-aware Blocks (Geometry).\nModel Query DGCNN Geometry CD-‚Ñì1 F-Score@1%\nA 9.43 67.82\nB ‚úì 9.09 0.713\nC ‚úì ‚úì 8.69 0.736\nD ‚úì ‚úì all 8.44 0.741\nE ‚úì ‚úì 1st 8.38 0.745\nand KITTI benchmark [10].\nResults on the PCN Dataset.The PCN dataset [51] is one\nof the most widely used benchmark datasets for the point\ncloud completion task. To verify the effectiveness of our\nmethod on existing benchmarks and compare it with more\nstate-of-the-art methods, we conducted experiments on this\ndataset following the standard protocol and evaluation met-\nric used in previous work [51, 17, 48, 45, 43]. The results\nare shown in Table 3. We see our method largely improves\nthe previous methods and establishes the new state-of-the-\nart on this dataset.\nResults on KITTI Benchmark.To show the performance\nof our method in real-world scenarios, we follow [48] to\nÔ¨Ånetune our trained model on ShapeNetCars [51] and eval-\nuate the performance of our model on KITTI dataset, which\ncontains the incomplete point clouds of cars in the real-\nworld scenes from LiDAR scans. We report the Fidelity\nand MMD metrics in Table 5 and show some reconstruction\nresults in Figure 5. Our method achieves better qualitative\nand quantitative performance.\n4.6. Model Design Analysis\nTo examine the effectiveness of our designs, we conduct\na detailed ablation study on the key components of PoinTr.\nThe results are summarized in Table 4. The baseline model\nA is the vanilla transformer model for point cloud comple-\ntion, which uses the encoder-decoder architecture with the\nstandard transformer blocks. In this model, we form the\npoint proxies directly from the point cloud using a single-\nlayer DGCNN model. We then add the query generator\nTable 5: Results on LiDAR scans from KITTI dataset under the Fidelity and MMD metrics.\nCD-‚Ñì2 (√ó1000) AtlasNet [13] PCN [51] FoldingNet [50] TopNet [37] MSN [17] NSFA [52] PFNet [16] CRN [43] GRNet [48]PoinTr\nFidelity‚Üì 1.759 2.235 7.467 5.354 0.434 1.281 1.137 1.023 0.816 0.000\nMMD‚Üì 2.108 1.366 0.537 0.636 2.259 0.891 0.792 0.872 0.568 0.526\n( a )\n ( b )\n ( c )\n ( d )\n ( e )\nI\nn\np\nu\nt\nF\no\nl\nd\ni\nn\ng\nN\ne\nt\nP\nC\nN\nT\no\np\nN\ne\nt\nP\nF\nN\ne\nt\nG\nR\nN\ne\nt\nO\nu\nr\ns\nG\n.\nT\n.\nFigure 6: Qualitative results on ShapeNet-55. All methods\nabove takes the point clouds in the Ô¨Årst line as inputs and\ngenerate complete point clouds. Our methods can complete\nthe point clouds with higher Ô¨Ådelity, which clearly shows\nthe effectiveness of our method.\nbetween the encoder and decoder (model B). We see the\nquery generator improve the baseline by 0.34 in Chamfer\ndistance. When using DGCNN to extract features from the\ninput point cloud (model C), we observe a signiÔ¨Åcant im-\nprovement to 8.69. By adding the geometric block to all the\ntransformer blocks (model D), we see the performance can\nbe further improved, which clearly demonstrates the effec-\ntiveness of the geometric structures learned by the block.\nWe Ô¨Ånd that only adding the geometric block to the Ô¨Årst\ntransformer block in both encoder and decoder can lead to a\nslightly better performance (model E), which indicates the\nrole of geometric block is to introduce the inductive bias and\na single layer is sufÔ¨Åcient while adding more blocks may re-\nsult in over-Ô¨Åtting. Besides, we see our method can achieve\nover 0.74 F-Score on the PCN dataset while obtaining only\n0.46 F-Score on our ShapeNet-55, which also suggests our\nnew datatset is much more challenging.\n4.7. Qualitative Results\nIn Figure 6, we show some completion results for all\nmethods and Ô¨Ånd our method perform better. For example,\nthe input data in (a) nearly lose all the geometric informa-\ntion and can be hardly recognized as an airplane. In this\ncase, other methods can only roughly complete the shape\nwith unsatisfactory geometry details, while our method can\nstill complete the point cloud with higher Ô¨Ådelity. These\nresults show our method has a stronger ability to recover\ndetails and is more robust to various incomplete patterns.\nMore results can be found in the supplementary material.\n5. Conclusion\nIn this paper, we have proposed a new architecture,\nPoinTr, to convert the point cloud completion task into a\nset to set translation tasks. With several technical innova-\ntions, we successfully applied the transformer model to this\ntask and achieved state-of-the-art performance. Moreover,\nwe proposed two more challenging benchmarks for more\ndiverse point cloud completion. Extending our transformer\narchitecture to other 3D tasks can an interesting future di-\nrection.\nAcknowledgements\nThis work was supported in part by the National Key\nResearch and Development Program of China under Grant\n2017YFA0700802, in part by the National Natural Sci-\nence Foundation of China under Grant 61822603, Grant\nU1813218, and Grant U1713214, in part by a grant from\nthe Beijing Academy of ArtiÔ¨Åcial Intelligence (BAAI), and\nin part by a grant from the Institute for Guo Qiang, Tsinghua\nUniversity.\nA. Implementation Details\nOur proposed method PointTr is implemented with Py-\nTorch [26]. We utilize AdamW optimizer [21] to train the\nnetwork with initial learning rate as 0.0005 and weight de-\ncay as 0.0005. In all of our experiments, we set the depth of\nthe encoder and decoder in our transformer to 6 and 8 and\nset k of kNN operation to 16 and 8 for the DGCNN feature\nextractor and the geometry-aware block respectively. We\nuse 6 head attention for all transformer blocks and set their\nhidden dimensions to 384. On the PCN dataset, the network\ntakes 2048 points as inputs and is required to complete the\nother 14336 points. We set the batch size to 54 and train\nthe model for 300 epochs with the continuous learning rate\ndecay of 0.9 for every 20 epochs. We set N to 128 and M\nto 224. On ShapeNet-55/34, the model takes 2048 points as\ninputs and is required to complete the other 6144 points. We\nset the batch size to 128 and train the model for 200 epochs\nwith the continuous learning rate decay of 0.76 for every 20\nepochs. We set N to 128 and M to 96.\nWe employ a lightweight DGCNN [44] model to extract\nthe point proxy features. To reduce the computational cost,\nwe hierarchically downsample the original input point cloud\nto N = 128 center points and use several DGCNN lay-\ners to capture local geometric relationships. The detailed\nnetwork architecture is: Linear(Cin = 3, Cout = 8) ‚Üí\nDGCNN(Cin = 8, Cout = 32, K= 8, Nout = 2048) ‚Üí\nDGCNN(Cin = 32, Cout = 64, K= 8, Nout = 512) ‚Üí\nDGCNN(Cin = 64, Cout = 64, K= 8, Nout = 512) ‚Üí\nDGCNN(Cin = 64, Cout = 128, K = 8, Nout = 128),\nwhere Cin and Cout are the numbers of channels of input\nand output features, Nout is the number of points after FPS.\nB. Technical Details on Transformers\nEncoder-Decoder Architecture. The overall architecture\nof the transformer encoder-decoder networks is illustrated\nin Figure 7. The point proxies are passed through the trans-\nformer encoder with N multi-head self-attention layers and\nfeed-forward network layers. Then, the decoder receives\nthe generated query embeddings and encoder memory, and\nproduces the Ô¨Ånal set of predicted point proxies that repre-\nsents the missing part of the point cloud through N multi-\nhead self-attention layers, decoder-encoder attention layers\nand feed-forward network layers. We set N to 6 in all our\nexperiments following common practice [39].\nMulti-head Attention. Multi-head attention mechanism\nallows the network to jointly attend to information from dif-\nferent representation subspaces at different positions [39].\nSpeciacally, given the input values V , keys K and queries\nQ, the multi-head attention is computed by:\nMultiHead(Q, K, V) =WOConcat(head1, ...,headh),\nMulti-HeadSelf-AttentionNormalization\nFeed-Forward NetworkNormalization+\n+\ninputpointproxies\nMulti-HeadSelf-AttentionNormalization+\nqueryembeddings\nMulti-HeadSelf-AttentionNormalization+Feed-Forward NetworkNormalization+\npredictedproxies\nùëÅ√ó\nùëÅ√ó\nEncoder\nDecoder\nùëâùêæQ ùëâùêæQ\nùëâùêæQ\nFigure 7: The overall architecture of the transformer encoder-\ndecoder networks.\nwhere WO the weights of the output linear layer and each\nhead feature can be obtained by:\nheadi = softmax(QWQ\ni (KW K\ni )T\n‚àödk\n)V WV\ni\nwhere WQ\ni , WK\ni and WV\ni are the linear layers that project\nthe inputs to different subspaces and dk is the dimension of\nthe input features.\nFeed-forward network (FFN). Following [39], we use\ntwo linear layers with ReLU activations and dropout as the\nfeed-forward network.\nC. Detailed Experimental Results\nDetailed results on ShapeNet-55: In Table 6, we report\nthe detailed results for FoldingNet [50], PCN [51], Top-\nNet [37], PFNet [16], GRNet [48] and the proposed method\non ShapeNet-55. Each row in the table stands for a category\nof object. We test each method under three settings: simple,\nmoderate and hard.\nDetailed results on ShapeNet-34: In Table 7, we report\nthe detailed results for the novel objects from 21 categories\nin ShapeNet-34. Each row in the table stands for a category\nof object. We test each method under the three settings:\nsimple, moderate and hard.\nD. Complexity Analysis\nOur method achieves the best performance on both our\nnewly proposed diverse benchmarks and the existing bench-\nmarks. We provide the detailed complexity analysis of our\nmethod in Table 8. We report the number of parameters and\ntheoretical computation cost (FLOPs) of our method and\nTable 6: Detailed results on ShapeNet-55. S., M. and H. stand for the simple, moderate and hard settings.\nCD-‚Ñì2(√ó1000) FoldingNet [50] PCN [51] TopNet [37] PFNet [16] GRNet [48] Ours-PoinTr\nS. M. H. S. M. H. S. M. H. S. M. H. S. M. H. S. M. H.\nairplane 1.36 1.28 1.7 0.9 0.89 1.32 1.02 0.99 1.48 1.35 1.44 2.69 0.87 0.87 1.27 0.27 0.38 0.69\ntrash bin 2.93 2.9 5.03 2.16 2.18 5.15 2.51 2.32 5.03 4.03 3.39 9.63 1.69 2.01 3.48 0.8 1.15 2.15\nbag 2.31 2.38 3.67 2.11 2.04 4.44 2.36 2.23 4.21 3.63 3.66 7.6 1.41 1.7 2.97 0.53 0.74 1.51\nbasket 2.98 2.77 4.8 2.21 2.1 4.55 2.62 2.43 5.71 4.74 3.88 8.47 1.65 1.84 3.15 0.73 0.88 1.82\nbathtub 2.68 2.66 4.0 2.11 2.09 3.94 2.49 2.25 4.33 3.64 3.5 5.74 1.46 1.73 2.73 0.64 0.94 1.68\nbed 4.24 4.08 5.65 2.86 3.07 5.54 3.13 3.1 5.71 4.44 5.36 9.14 1.64 2.03 3.7 0.76 1.1 2.26\nbench 1.94 1.77 2.36 1.31 1.24 2.14 1.56 1.39 2.4 2.17 2.16 4.11 1.03 1.09 1.71 0.38 0.52 0.94\nbirdhouse 4.06 4.18 5.88 3.29 3.53 6.69 3.73 3.98 6.8 3.96 5.0 9.66 1.87 2.4 4.71 0.98 1.49 3.13\nbookshelf 3.04 3.03 3.91 2.7 2.7 4.61 3.11 2.87 4.87 3.19 3.47 5.72 1.42 1.71 2.78 0.71 1.06 1.93\nbottle 1.7 1.91 4.02 1.25 1.43 4.61 1.56 1.66 4.02 2.37 2.89 10.03 1.05 1.44 2.67 0.37 0.74 1.5\nbowl 2.79 2.6 4.23 2.05 1.83 3.66 2.33 1.98 4.82 4.3 3.97 8.76 1.6 1.77 2.99 0.68 0.78 1.44\nbus 1.47 1.42 2.0 1.2 1.14 2.08 1.32 1.21 2.29 2.06 1.88 3.75 1.06 1.16 1.48 0.42 0.55 0.79\ncabinet 2.0 1.86 2.79 1.6 1.49 3.47 1.91 1.65 3.36 2.72 2.37 4.73 1.27 1.41 2.09 0.55 0.66 1.16\ncamera 5.5 6.04 8.87 4.05 4.54 8.27 4.75 4.98 9.24 6.57 8.04 13.11 2.14 3.15 6.09 1.1 2.03 4.34\ncan 2.84 2.68 5.71 2.02 2.28 6.48 2.67 2.4 5.5 5.65 4.05 16.29 1.58 2.11 3.81 0.68 1.19 2.14\ncap 4.1 4.04 5.87 1.82 1.76 4.2 3.0 2.69 5.59 10.92 9.04 20.3 1.17 1.37 3.05 0.46 0.62 1.64\ncar 1.81 1.81 2.31 1.48 1.47 2.6 1.71 1.65 3.17 2.06 2.1 3.43 1.29 1.48 2.14 0.64 0.86 1.25\ncellphone 1.04 1.06 1.87 0.8 0.79 1.71 1.01 0.96 1.8 1.25 1.37 3.65 0.82 0.91 1.18 0.32 0.39 0.6\nchair 2.37 2.46 3.62 1.7 1.81 3.34 1.97 2.04 3.59 2.94 3.48 6.34 1.24 1.56 2.73 0.49 0.74 1.63\nclock 2.56 2.41 3.46 2.1 2.01 3.98 2.48 2.16 4.03 3.15 3.27 6.03 1.46 1.66 2.67 0.62 0.84 1.65\nkeyboard 1.21 1.18 1.32 0.82 0.82 1.04 0.88 0.83 1.15 0.83 1.06 1.97 0.74 0.81 1.09 0.3 0.39 0.45\ndishwasher 2.6 2.17 3.5 1.93 1.66 4.39 2.43 1.74 4.64 4.57 3.23 6.39 1.43 1.59 2.53 0.55 0.69 1.42\ndisplay 2.15 2.24 3.25 1.56 1.66 3.26 1.84 1.85 3.48 2.27 2.83 5.52 1.13 1.38 2.29 0.48 0.67 1.33\nearphone 6.37 6.48 9.14 3.13 2.94 7.56 4.36 4.47 8.36 15.07 17.5 33.37 1.78 2.18 5.33 0.81 1.38 3.78\nfaucet 4.46 4.39 7.2 3.21 3.48 7.52 3.61 3.59 7.25 5.68 6.79 14.29 1.81 2.32 4.91 0.71 1.42 3.49\nÔ¨Ålecabinet 2.59 2.48 3.76 2.02 1.97 4.14 2.41 2.12 4.12 3.72 3.57 7.13 1.46 1.71 2.89 0.63 0.84 1.69\nguitar 0.65 0.6 1.25 0.42 0.38 1.23 0.57 0.47 1.42 0.74 0.89 5.41 0.44 0.48 0.76 0.14 0.21 0.42\nhelmet 5.39 5.37 7.96 3.76 4.18 7.53 4.36 4.55 7.73 9.55 8.41 15.44 2.33 3.18 6.03 0.99 1.93 4.22\njar 3.65 3.87 6.51 2.57 2.82 6.0 3.03 3.17 7.03 5.44 5.56 11.87 1.72 2.37 4.37 0.77 1.33 2.87\nknife 1.29 0.87 1.21 0.94 0.62 1.37 0.84 0.68 1.44 2.11 1.53 3.89 0.72 0.66 0.96 0.2 0.33 0.56\nlamp 3.93 4.23 6.87 3.1 3.45 7.02 3.03 3.39 8.15 6.82 7.61 14.22 1.68 2.43 5.17 0.64 1.4 3.58\nlaptop 1.02 1.04 1.96 0.75 0.79 1.59 0.8 0.85 1.66 1.04 1.21 2.46 0.83 0.87 1.28 0.32 0.34 0.6\nloudspeaker 3.21 3.15 4.55 2.5 2.45 5.08 3.1 2.76 5.32 4.32 4.19 7.6 1.75 2.08 3.45 0.78 1.16 2.17\nmailbox 2.44 2.61 4.98 1.66 1.74 5.18 2.16 2.1 5.1 3.82 4.2 10.51 1.15 1.59 3.42 0.39 0.78 2.56\nmicrophone 4.42 5.06 7.04 3.44 3.9 8.52 2.83 3.49 6.87 6.58 7.56 16.74 2.09 2.76 5.7 0.7 1.66 4.48\nmicrowaves 2.67 2.48 4.43 2.2 2.01 4.65 2.65 2.15 5.07 4.63 3.94 6.52 1.51 1.72 2.76 0.67 0.83 1.82\nmotorbike 2.63 2.55 3.52 2.03 2.01 3.13 2.29 2.25 3.54 2.17 2.48 5.09 1.38 1.52 2.26 0.75 1.1 1.92\nmug 3.66 3.67 5.7 2.45 2.48 5.17 2.89 2.56 5.43 4.76 4.3 8.37 1.75 2.16 3.79 0.91 1.17 2.35\npiano 3.86 4.04 6.04 2.64 2.74 4.83 2.99 2.89 5.64 4.57 5.26 9.26 1.53 1.82 3.21 0.76 1.06 2.23\npillow 2.33 2.38 3.87 1.85 1.81 3.68 2.31 2.26 4.19 4.21 3.82 7.89 1.42 1.67 3.04 0.61 0.82 1.56\npistol 1.92 1.62 2.52 1.25 1.17 2.65 1.5 1.3 2.62 2.27 2.09 7.2 1.11 1.06 1.76 0.43 0.66 1.3\nÔ¨Çowerpot 4.53 4.68 6.46 3.32 3.39 6.04 3.61 3.45 6.28 4.83 5.51 10.68 2.02 2.48 4.19 1.01 1.51 2.77\nprinter 3.66 4.01 5.34 2.9 3.19 5.84 3.04 3.19 5.84 5.56 6.06 9.29 1.56 2.38 4.24 0.73 1.21 2.47\nremote 1.14 1.2 1.98 0.99 0.97 2.04 1.14 1.17 2.16 1.74 2.37 4.61 0.89 1.05 1.29 0.36 0.53 0.71\nriÔ¨Çe 1.27 1.02 1.37 0.98 0.8 1.31 0.98 0.86 1.46 1.72 1.45 3.02 0.83 0.77 1.16 0.3 0.45 0.79\nrocket 1.37 1.18 1.88 1.05 1.04 1.87 1.04 1.0 1.93 1.65 1.61 3.82 0.78 0.92 1.44 0.23 0.48 0.99\nskateboard 1.58 1.58 2.07 1.04 0.94 1.68 1.08 1.05 1.84 1.43 1.6 3.09 0.82 0.87 1.24 0.28 0.38 0.62\nsofa 2.22 2.09 3.14 1.65 1.61 2.92 1.93 1.76 3.39 2.65 2.53 4.84 1.35 1.45 2.32 0.56 0.67 1.14\nstove 2.69 2.63 3.99 2.07 2.02 4.72 2.44 2.16 4.84 4.03 3.71 7.15 1.46 1.72 3.22 0.63 0.92 1.73\ntable 2.23 2.15 3.21 1.56 1.5 3.36 1.78 1.65 3.21 3.03 3.11 5.74 1.15 1.33 2.33 0.46 0.64 1.31\ntelephone 1.07 1.06 1.75 0.8 0.8 1.67 1.02 0.95 1.78 1.3 1.47 3.37 0.81 0.89 1.18 0.31 0.38 0.59\ntower 2.46 2.45 3.91 1.91 1.97 4.47 2.15 2.05 4.51 3.13 3.54 9.87 1.26 1.69 3.06 0.55 0.9 1.95\ntrain 1.86 1.68 2.32 1.5 1.41 2.37 1.59 1.44 2.51 2.01 2.03 4.1 1.09 1.14 1.61 0.5 0.7 1.12\nwatercraft 1.85 1.69 2.49 1.46 1.39 2.4 1.53 1.42 2.67 2.1 2.13 4.58 1.09 1.12 1.65 0.41 0.62 1.07\nwasher 3.47 3.2 4.89 2.42 2.31 6.08 2.92 2.53 6.53 5.55 4.11 7.04 1.72 2.05 4.19 0.75 1.06 2.44\nmean 2.68 2.66 4.06 1.96 1.98 4.09 2.26 2.17 4.31 3.84 3.88 8.03 1.35 1.63 2.86 0.58 0.88 1.8\nTable 7: Detailed results for the novel objects on ShapeNet-34.S., M. and H. stand for the simple, moderate and hard settings.\nCD-‚Ñì2 (√ó1000) FoldingNet [50] PCN [51] TopNet [37] PFNet [16] GRNet [48] Ours-PoinTr\nS. M. H. S. M. H. S. M. H. S. M. H. S. M. H. S. M. H.\nbag 2.15 2.27 3.99 2.48 2.46 3.94 2.08 1.95 4.36 3.88 4.42 9.67 1.47 1.88 3.45 0.96 1.34 2.08\nbasket 2.37 2.2 4.87 2.79 2.51 4.78 2.46 2.11 5.18 4.47 4.55 14.46 1.78 1.94 4.18 1.04 1.4 2.9\nbirdhouse 3.27 3.15 5.62 3.53 3.47 5.31 3.17 2.97 5.89 3.9 4.65 9.88 1.89 2.34 5.16 1.22 1.79 3.45\nbowl 2.61 2.3 4.55 2.66 2.35 3.97 2.46 2.16 4.84 4.35 5.0 14.59 1.77 1.97 3.9 1.05 1.32 2.4\ncamera 4.4 4.78 7.85 4.84 5.3 8.03 4.24 4.43 8.11 6.78 8.04 13.91 2.31 3.38 7.2 1.63 2.67 4.97\ncan 1.95 1.73 5.86 1.95 1.89 5.21 2.02 1.7 5.82 2.95 3.47 23.02 1.53 1.8 3.08 0.8 1.17 2.85\ncap 6.07 5.98 11.49 7.21 7.14 10.94 4.68 4.23 9.17 14.11 14.86 28.23 3.29 4.87 13.02 1.4 2.74 8.35\nkeyboard 0.98 0.96 1.35 1.07 1.0 1.23 0.79 0.77 1.55 1.13 1.16 2.58 0.73 0.77 1.11 0.43 0.45 0.63\ndishwasher 2.09 1.8 4.55 2.45 2.09 3.53 2.51 1.77 4.72 3.44 3.78 9.31 1.79 1.7 3.27 0.93 1.05 2.04\nearphone 6.86 6.96 12.77 7.88 6.59 16.53 5.33 4.83 11.67 20.31 23.21 39.49 4.29 4.16 10.3 2.03 5.1 10.69\nhelmet 4.86 5.04 8.86 6.15 6.41 9.16 4.89 4.86 8.73 8.78 10.07 21.2 3.06 4.38 10.27 1.86 3.3 6.96\nmailbox 2.2 2.29 4.49 2.74 2.68 4.31 2.35 2.2 4.91 5.2 5.33 10.94 1.52 1.9 4.33 1.03 1.47 3.34\nmicrophone 2.92 3.27 8.54 4.36 4.65 8.46 3.03 3.2 7.15 6.39 7.99 19.41 2.29 3.23 8.41 1.25 2.27 5.47\nmicrowaves 2.29 2.12 5.17 2.59 2.35 4.47 2.67 2.12 5.41 3.89 4.08 9.01 1.74 1.81 3.82 1.01 1.18 2.14\npillow 2.07 2.11 3.73 2.09 2.16 3.54 2.08 2.05 4.01 4.15 4.29 12.01 1.43 1.69 3.43 0.92 1.24 2.39\nprinter 3.02 3.23 5.53 3.28 3.6 5.56 2.9 2.96 6.07 5.38 5.94 10.29 1.82 2.41 5.09 1.18 1.76 3.1\nremote 0.89 0.92 1.85 0.95 1.08 1.58 0.89 0.89 2.28 1.51 1.75 6.0 0.82 1.02 1.29 0.44 0.58 0.78\nrocket 1.28 1.09 2.0 1.39 1.22 2.01 1.14 0.96 2.03 1.84 1.51 4.01 0.97 0.79 1.6 0.39 0.72 1.39\nskateboard 1.53 1.42 1.99 1.97 1.78 2.45 1.23 1.2 2.01 2.43 2.53 4.25 0.93 1.07 1.83 0.52 0.8 1.31\ntower 2.25 2.25 4.74 2.37 2.4 4.35 2.2 2.17 5.47 3.38 4.15 13.11 1.35 1.8 3.85 0.82 1.35 2.48\nwasher 2.58 2.34 5.5 2.77 2.52 4.64 2.63 2.14 6.57 4.53 4.27 9.23 1.83 1.97 5.28 1.04 1.39 2.73\nmean 2.79 2.77 5.49 3.22 3.13 5.43 2.65 2.46 5.52 5.37 5.95 13.55 1.84 2.23 4.95 1.05 1.67 3.45\nother Ô¨Åve methods. We also provide the average Chamfer\ndistances of all categories in ShapeNet-55 and unseen cate-\ngories in ShapeNet34 as references. We can see our method\nachieves the best performance while using relatively low pa-\nrameters and FLOPs among the methods in the table, which\nshows our method offers a decent trade-off between cost\nand performance.\nTable 8: Complexity analysis. We report the the num-\nber of parameter (Params) and theoretical computation cost\n(FLOPs) of our method and Ô¨Åve existing methods. We also\nprovide the average Chamfer distances of all categories in\nShapeNet-55 (CD55) and unseen categories in ShapeNet34\n(CD34) as references.\nModels Params FLOPs CD55 CD34\nFoldingNet [50] 2.30 M 27.58 G 3.12 3.62\nPCN [51] 5.04 M 15.25 G 2.66 3.85\nTopNet [37] 5.76 M 6.72 G 2.91 3.50\nPFNet [16] 73.05 M 4.96 G 5.22 8.16\nGRNet [48] 73.15 M 40.44 G 1.97 2.99\nPoinTr 30.9 M 10.41 G 1.07 2.05\nE. Visualization of the Predicted Centers\nWe visualize the local center prediction results on\nShapeNet-55. We adopt a coarse-to-Ô¨Åne strategy to recover\nthe point cloud. Our method starts with the prediction of lo-\ncal centers, then we can obtain the Ô¨Ånal results by adding the\npoints around the centers. As shown in Figure 8, Line (a)\nshows the input partial point cloud and the predicted point\ncenters. Line (b) is the predicted point clouds. We see the\npredicted point proxies can successfully represent the over-\nall structure of the point cloud and the details then are added\nin the Ô¨Ånal predictions.\nF. Qualitative Results\nIn Figure 9, we provide more qualitative results on\nShapeNet-55. We see our results are much better than base-\nline methods visually.\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas J. Guibas. Learning representations and generative\nmodels for 3d point clouds. In ICLR, 2018. 2\n[2] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese.\nJoint 2d-3d-semantic data for indoor scene understanding.\narXiv preprint arXiv:1702.01105, 2017. 5\n[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional net-\nworks. In ICCV, pages 3286‚Äì3295, 2019. 4\n[4] Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese. 3d-r2n2: A uniÔ¨Åed approach\nfor single and multi-view 3d object reconstruction. In Bas-\ntian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,\nECCV, pages 628‚Äì644, 2016. 2\n[5] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie√üner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. In\nCVPR, pages 5828‚Äì5839, 2017. 5\nFigure 8: Visualization of predicted points proxies. In Line (a),\nwe show the input partial point clouds and the predicted centers.\nBased on predicted point proxies, we can easily predicted the ac-\ncurate point centers and then complete the point clouds, as shown\nin Line (b). We show the ground-truth point cloud in Line (c) for\ncomparisons.\n[6] Angela Dai, Charles Ruizhongtai Qi, and Matthias Nie√üner.\nShape completion using 3d-encoder-predictor cnns and\nshape synthesis. In CVPR, pages 6545‚Äì6554, 2017. 1, 2\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In Jill Burstein, Christy\nDoran, and Thamar Solorio, editors, NAACL, pages 4171‚Äì\n4186, 2019. 3\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 3\n[9] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set\ngeneration network for 3d object reconstruction from a single\nimage. In CVPR, pages 605‚Äì613, 2017. 4\n[10] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. interna-\ntional journal of robotics research (ijrr). 2013. 2, 7\n[11] Rohit Girdhar, David F. Fouhey, Mikel Rodriguez, and Ab-\nhinav Gupta. Learning a predictable and generative vec-\ntor representation for objects. In Bastian Leibe, Jiri Matas,\nNicu Sebe, and Max Welling, editors, ECCV, pages 484‚Äì\n499, 2016. 2\n[12] Benjamin Graham, Martin Engelcke, and Laurens van der\nMaaten. 3d semantic segmentation with submanifold sparse\nconvolutional networks. In CVPR, pages 9224‚Äì9232, 2018.\n2\n[13] Thibault Groueix, Matthew Fisher, Vladimir G. Kim,\nBryan C. Russell, and Mathieu Aubry. Atlasnet: A papier-\nmÀÜach¬¥e approach to learning 3d surface generation. CoRR,\n2018. 7, 8\n[14] Thibault Groueix, Matthew Fisher, Vladimir G Kim,\nBryan C Russell, and Mathieu Aubry. A papier-m ÀÜach¬¥e ap-\nproach to learning 3d surface generation. In CVPR, pages\n216‚Äì224, 2018. 2\n[15] Xiaoguang Han, Zhen Li, Haibin Huang, Evangelos\nKalogerakis, and Yizhou Yu. High-resolution shape com-\npletion using deep neural networks for global structure and\nlocal geometry inference. In ICCV, pages 85‚Äì93, 2017. 1, 2\n[16] Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le.\nPf-net: Point fractal network for 3d point cloud completion.\nIn CVPR, pages 7659‚Äì7667, 2020. 2, 4, 5, 6, 8, 9, 10, 11\n[17] Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, and Shi-\nMin Hu. Morphing and sampling network for dense point\ncloud completion. arXiv preprint arXiv:1912.00280, 2019.\n7, 8\n[18] Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, and Shi-\nMin Hu. Morphing and sampling network for dense point\ncloud completion. In AAAI, pages 11596‚Äì11603, 2020. 2\n[19] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong\nPan. Relation-shape convolutional neural network for point\ncloud analysis. In CVPR, pages 8895‚Äì8904, 2019. 1\n[20] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-\nvoxel CNN for efÔ¨Åcient 3d deep learning. In Hanna M.\nWallach, Hugo Larochelle, Alina Beygelzimer, Florence\nd‚ÄôAlch¬¥e-Buc, Emily B. Fox, and Roman Garnett, editors,\nNeurIPS, pages 963‚Äì973, 2019. 1\n[21] Ilya Loshchilov and Frank Hutter. Fixing weight decay reg-\nularization in adam. 2018. 9\n[22] Christoph L ¬®uscher, Eugen Beck, Kazuki Irie, Markus Kitza,\nWilfried Michel, Albert Zeyer, Ralf Schl ¬®uter, and Hermann\nNey. RWTH ASR systems for librispeech: Hybrid vs atten-\ntion - w/o data augmentation. CoRR, abs/1905.03072, 2019.\n3\n[23] Priyanka Mandikal and R. Venkatesh Babu. Dense 3d point\ncloud reconstruction using a deep pyramid network. CoRR,\nabs/1901.08906, 2019. 2\n[24] Duc Thanh Nguyen, Binh-Son Hua, Minh-Khoi Tran,\nQuang-Hieu Pham, and Sai-Kit Yeung. A Ô¨Åeld model for\nrepairing 3d shapes. In CVPR, pages 5676‚Äì5684, 2016. 1\n[25] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, and Alexander Ku. Image trans-\nformer. CoRR, abs/1802.05751, 2018. 3\n[26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An\nimperative style, high-performance deep learning library. In\nNeurIPS, pages 8026‚Äì8037, 2019. 9\n[27] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and\nLeonidas J. Guibas. Pointnet: Deep learning on point sets\nfor 3d classiÔ¨Åcation and segmentation. In CVPR, pages 77‚Äì\n85, 2017. 2\n[28] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J.\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In Isabelle Guyon, Ulrike\nvon Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fer-\ngus, S. V . N. Vishwanathan, and Roman Garnett, editors,\nNeurIPS, pages 5099‚Äì5108, 2017. 2, 4\n[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. OpenAI blog, page 9, 2019. 3\n[30] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu,\nJie Zhou, and Cho-Jui Hsieh. Dynamicvit: EfÔ¨Åcient vi-\nsion transformers with dynamic token sparsiÔ¨Åcation. arXiv\npreprint arXiv:2106.02034, 2021. 3\n[31] Muhammad Sarmad, Hyunjoo Jenny Lee, and Young Min\nKim. Rl-gan-net: A reinforcement learning agent controlled\ngan network for real-time point cloud shape completion. In\nCVPR, pages 5898‚Äì5907, 2019. 2\n[32] Abhishek Sharma, Oliver Grau, and Mario Fritz. Vconv-dae:\nDeep volumetric shape learning without object labels.CoRR,\nabs/1604.03755, 2016. 1\n[33] David Stutz and Andreas Geiger. Learning 3d shape comple-\ntion from laser scan data with weak supervision. In CVPR,\npages 1955‚Äì1964, 2018. 1, 2\n[34] Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji,\nEvangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz.\nSplatnet: Sparse lattice networks for point cloud processing.\nIn CVPR, pages 2530‚Äì2539, 2018. 2\n[35] Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana\nLikhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sri-\nram, Vitaliy Liptchinsky, and Ronan Collobert. End-to-end\nASR: from Supervised to Semi-Supervised Learning with\nModern Architectures. arXiv e-prints, 2019. 3\n[36] Maxim Tatarchenko, Stephan R. Richter, Ren ¬¥e Ranftl,\nZhuwen Li, Vladlen Koltun, and Thomas Brox. What\ndo single-view 3d reconstruction networks learn? CoRR,\nabs/1905.03678, 2019. 6\n[37] Lyne P. Tchapmi, Vineet Kosaraju, Hamid RezatoÔ¨Åghi,\nIan D. Reid, and Silvio Savarese. Topnet: Structural point\ncloud decoder. In CVPR, pages 383‚Äì392, 2019. 2, 5, 6, 7, 8,\n9, 10, 11\n[38] Jacob Varley, Chad DeChant, Adam Richardson, Joaqu ¬¥ƒ±n\nRuales, and Peter K. Allen. Shape completion enabled\nrobotic grasping. In IROS, pages 2442‚Äì2447, 2017. 1\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Isabelle Guyon,\nUlrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob\nFergus, S. V . N. Vishwanathan, and Roman Garnett, editors,\nNeurIPS, pages 5998‚Äì6008, 2017. 2, 3, 9\n[40] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Or-\nder matters: Sequence to sequence for sets. arXiv preprint\narXiv:1511.06391, 2015. 3\n[41] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,\nand Xin Tong. O-CNN: octree-based convolutional neural\nnetworks for 3d shape analysis. ACM Trans. Graph., 2017.\n2\n[42] Weiyue Wang, Qiangui Huang, Suya You, Chao Yang, and\nUlrich Neumann. Shape inpainting using 3d generative ad-\nversarial network and recurrent convolutional networks. In\nICCV, pages 2298‚Äì2306, 2017. 1\n[43] Xiaogang Wang, Marcelo H Ang Jr, and Gim Hee Lee. Cas-\ncaded reÔ¨Ånement network for point cloud completion. In\nCVPR, pages 790‚Äì799, 2020. 7, 8\n[44] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon. Dynamic\ngraph cnn for learning on point clouds. TOG, 38(5):1‚Äì12,\n2019. 3, 4, 9\n[45] Xin Wen, Peng Xiang, Zhizhong Han, Yan-Pei Cao, Pengfei\nWan, Wen Zheng, and Yu-Shen Liu. Pmp-net: Point cloud\ncompletion by learning multi-step point moving paths. In\nCVPR, 2021. 7\n[46] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d\nshapenets: A deep representation for volumetric shapes. In\nCVPR, pages 1912‚Äì1920, 2015. 2\n[47] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d\nshapenets: A deep representation for volumetric shapes. In\nCVPR, pages 1912‚Äì1920, 2015. 5\n[48] Haozhe Xie, Hongxun Yao, Shangchen Zhou, Jiageng Mao,\nShengping Zhang, and Wenxiu Sun. Grnet: Gridding resid-\nual network for dense point cloud completion. In An-\ndrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael\nFrahm, editors, ECCV, pages 365‚Äì381, 2020. 2, 5, 6, 7, 8, 9,\n10, 11\n[49] Bo Yang, Hongkai Wen, Sen Wang, Ronald Clark, Andrew\nMarkham, and Niki Trigoni. 3d object reconstruction from\na single depth view with adversarial learning. In ICCVW,\npages 679‚Äì688, 2017. 1\n[50] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian.\nFoldingnet: Interpretable unsupervised learning on 3d point\nclouds. CoRR, abs/1712.07262, 2017. 4, 6, 7, 8, 9, 10, 11\n[51] Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and\nMartial Hebert. PCN: point completion network. In 3DV,\npages 728‚Äì737, 2018. 2, 4, 5, 6, 7, 8, 9, 10, 11\n[52] Wenxiao Zhang, Qingan Yan, and Chunxia Xiao. Detail pre-\nserved point cloud completion via separated feature aggrega-\ntion. In ECCV, 2020. 8\n[53] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning\nfor point cloud based 3d object detection. In CVPR, pages\n4490‚Äì4499, 2018. 1\nFigure 9: More qualitative results on ShapeNet-55.",
  "topic": "Point cloud",
  "concepts": [
    {
      "name": "Point cloud",
      "score": 0.861882209777832
    },
    {
      "name": "Computer science",
      "score": 0.6595237851142883
    },
    {
      "name": "Transformer",
      "score": 0.585798978805542
    },
    {
      "name": "Cloud computing",
      "score": 0.515598475933075
    },
    {
      "name": "Algorithm",
      "score": 0.43848296999931335
    },
    {
      "name": "Encoder",
      "score": 0.43667012453079224
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.4242391288280487
    },
    {
      "name": "Theoretical computer science",
      "score": 0.37342342734336853
    },
    {
      "name": "Artificial intelligence",
      "score": 0.29324084520339966
    },
    {
      "name": "Engineering",
      "score": 0.1309940218925476
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}