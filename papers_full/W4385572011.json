{
    "title": "TextObfuscator: Making Pre-trained Language Model a Privacy Protector via Obfuscating Word Representations",
    "url": "https://openalex.org/W4385572011",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5100709809",
            "name": "Xin Zhou",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5008845262",
            "name": "Yi Lu",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A5088256860",
            "name": "Ruotian Ma",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5058353652",
            "name": "Tao Gui",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5049728192",
            "name": "Yuran Wang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100532190",
            "name": "Yong Ding",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100449681",
            "name": "Yibo Zhang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100360194",
            "name": "Qi Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5111113722",
            "name": "Xuanjing Huang",
            "affiliations": [
                null,
                "Fudan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2170240176",
        "https://openalex.org/W3173528555",
        "https://openalex.org/W4298882649",
        "https://openalex.org/W4280616980",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3112787034",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4288621164",
        "https://openalex.org/W4385573672",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W3022061250",
        "https://openalex.org/W3204191745",
        "https://openalex.org/W3106937659",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W2601450892",
        "https://openalex.org/W4221139072",
        "https://openalex.org/W4293240908",
        "https://openalex.org/W3193647133",
        "https://openalex.org/W3099785009",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2945357343",
        "https://openalex.org/W2963879260",
        "https://openalex.org/W4281806276",
        "https://openalex.org/W3164811584",
        "https://openalex.org/W3197942009",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3046764764",
        "https://openalex.org/W4231185195",
        "https://openalex.org/W2134089414",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W4297234531",
        "https://openalex.org/W4231844697",
        "https://openalex.org/W3099692016",
        "https://openalex.org/W3035375600",
        "https://openalex.org/W1978394996",
        "https://openalex.org/W3098049952",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W3096738375",
        "https://openalex.org/W2435473771",
        "https://openalex.org/W3162403368",
        "https://openalex.org/W2399197954",
        "https://openalex.org/W2889507104",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3200633740",
        "https://openalex.org/W2031533839",
        "https://openalex.org/W2520774990"
    ],
    "abstract": "In real-world applications, pre-trained language models are typically deployed on the cloud, allowing clients to upload data and perform compute-intensive inference remotely. To avoid sharing sensitive data directly with service providers, clients can upload numerical representations rather than plain text to the cloud. However, recent text reconstruction techniques have demonstrated that it is possible to transform representations into original words, suggesting that privacy risk remains. In this paper, we propose TextObfuscator, a novel framework for protecting inference privacy by applying random perturbations to clustered representations. The random perturbations make the representations indistinguishable from surrounding clustered representations, thus obscuring word information while retaining the original word functionality. To achieve this, we utilize prototypes to learn clustered representation, where tokens of similar functionality are encouraged to be closer to the same prototype during training.Additionally, we design different methods to find prototypes for token-level and sentence-level tasks, which can improve performance by incorporating semantic and task information.Experimental results on token and sentence classification tasks show that TextObfuscator achieves improvement over compared methods without increasing inference cost.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 5459‚Äì5473\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nTextObfuscator: Making Pre-trained Language Model a Privacy Protector\nvia Obfuscating Word Representations\nXin Zhou1‚àó, Yi Lu5‚àó‚Ä†, Ruotian Ma1, Tao Gui2‚Ä°,\nYuran Wang4, Yong Ding4, Yibo Zhang4, Qi Zhang1, Xuanjing Huang1, 3‚Ä°\n1School of Computer Science, Fudan University, Shanghai, China\n2 Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China\n3 International Human Phenome Institutes, Shanghai, China\n4 Honor Device Co., Ltd\n5 School of Computer Science and Engineering, Northeastern University, Shenyang, China\n{xzhou20, tgui, qz}@fudan.edu.cn, luyi18171297680@gmail.com\nAbstract\nIn real-world applications, pre-trained language\nmodels are typically deployed on the cloud,\nallowing clients to upload data and perform\ncompute-intensive inference remotely. To\navoid sharing sensitive data directly with\nservice providers, clients can upload numerical\nrepresentations rather than plain text to the\ncloud. However, recent text reconstruction tech-\nniques have demonstrated that it is possible to\ntransform representations into original words,\nsuggesting that privacy risk remains. In this\npaper, we propose TextObfuscator, a novel\nframework for preserving inference privacy\nby applying random perturbations to clustered\nrepresentations. The random perturbations\nmake each word representation indistinguish-\nable from surrounding functionally similar rep-\nresentations, thus obscuring word information\nwhile retaining the original word functionality.\nTo achieve this, we utilize prototypes to\nlearn clustered representations, where words\nof similar functionality are encouraged to be\ncloser to the same prototype during training.\nAdditionally, we design different methods to\nfind prototypes for token-level and sentence-\nlevel tasks, which can improve performance by\nincorporating semantic and task information.\nExperimental results on token and sentence\nclassification tasks show that TextObfuscator\nachieves improvement over compared methods\nwithout increasing inference cost.\n1 Introduction\nPre-trained language models (PLMs) have achieved\nimpressive performance on various NLP down-\nstream tasks (Devlin et al., 2018; Brown et al.,\n‚àóEqual contribution.\n‚Ä†Work done during the internship at Fudan NLP Lab.\n‚Ä°Corresponding authors.\nMike‚Äôs phone number is 123456.\nClient ModelClient ModelClient ModelRepresentationsPredictions\nCloud Server\nPrivacyAttacker\nTextReconstruction MethodAttack Results:Microsoftplans to invest in OpenAI.James lives in Third Street, visit him next week.Mike‚Äôs phone number is 123456.\nServer ModelServer ModelServer ModelServer ModelServer ModelServer Model\nJames lives in Third Street, visit him next week.Microsoftplans to invest in OpenAI.\nFigure 1: Illustration of the inference services\nand privacy risks. Although clients upload word\nrepresentations instead of plain text to the cloud server,\nprivacy attackers can still transform representations into\noriginal texts. Clients are still at privacy risk.\n2020; Qiu et al., 2020), but they also come with\nincreased model size and significant computational\nrequirements. In real-world applications, these\nlarge-scale models are often offered as an inference\nservice (Altman, 2022). The service providers train\nPLMs for target tasks and deploy them on the cloud.\nClients who lack high-computation resources can\nquery these service with their input and obtains the\ndesired responses (DALE, 2015).\nUnfortunately, current inference services are\nplagued by serious privacy concerns (Lehmkuhl\net al., 2021). Client data may contain sensitive\ninformation such as names, addresses, and even\ntrade secrets, sharing such information with service\nproviders compromises the privacy of clients. To\naddress privacy risks, a naive solution is for clients\nto generate shallow representations on their devices\nand upload numerical representations to the cloud\nfor subsequent inference, as shown in Figure 1.\nHowever, recent text reconstruction methods (Song\nand Raghunathan, 2020; Pan et al., 2020) have\nshown that word representations can be easily\ntransformed into raw texts, indicating privacy\nrisk remains. Inference service without privacy\n5459\nguarantees is not only unacceptable for clients but\nalso illegal for service providers1.\nRecent literature has proposed various methods\nto mitigate privacy leakage in representation. For\nexample, Chen et al. (2022b) and Hao et al. (2022)\nhave applied homomorphic encryption (Gentry,\n2009) to transformer-based models, which enables\ncomputations to be performed on encrypted data.\nBut homomorphic encryption often incurs signifi-\ncant computation time and communication costs\n(Gilad-Bachrach et al., 2016), making it impractical\nfor real-world applications. Alternatively, several\nstudies have adapted differential privacy (Lyu et al.,\n2020a; Hoory et al., 2021; Yue et al., 2021a) and\nadversarial training (Li et al., 2018; Coavoux et al.,\n2018; Plant et al., 2021) to reduce the privacy\ninformation contained in representations. However,\nin our scenario, the privacy information pertains to\neach word, and reducing word information in the\nshallow layer can harm subsequent inference,\nthus degrading performance (Jawahar et al.,\n2019), especially in token-level tasks.\nIn this paper, we propose TextObfuscator, a\nnovel paradigm for privacy-preserving inference.\nThe key idea of our method is to learn private repre-\nsentations that obscure original word information\nwhile preserving original word functionality.\nSpecifically, we find prototypes for each word\nand encourage functionally similar words close to\nthe same prototype during training. Subsequently,\nrandom perturbations are applied to these clustered\nrepresentations, which yields two key benefits.\nFirstly, it obscures original word information as\nthe perturbed representations are indistinguishable\nfrom those clustered around them, making it harder\nfor privacy attackers to reconstruct original words,\nthus protecting privacy. Secondly, it maintains\nthe original word functionality as the perturbed\nrepresentations stay within the same functional\nclusters, leading to improved performance.\nTo learn clustered representations, we have de-\nsigned different methods to find suitable prototypes\nfor token and sentence classification. For token-\nlevel tasks, each word is assigned a label that\nserves as a prototype indicator (Snell et al., 2017).\nBut for sentence-level tasks, there is no explicit\nprototype indicator for words. Therefore, the\nclustering algorithm is used for word assignment.\nBased on clustering results, we take the cluster\n1https://www.consilium.europa.eu/en/policies/data-\nprotection/data-protection-regulation/\ncenters as prototypes and assign semantically\nsimilar words to the same prototype. However,\nsemantic-based clustering may lead to keywords\nfrom different classes being clustered together,\nhindering target tasks. For example, if ‚Äúgood‚Äù\nand ‚Äúbad‚Äù play the same role, the sentiment of\na sentence will be ambiguous. To address this,\nwe utilize TF-IDF to identify keywords from\ndifferent classes and use these keywords to re-\ndivide the clustering results. Our codes are\npublicly available at https://github.com/\nxzhou20/TextObfuscator.\nOur contribution can be summarized as follows:\n‚Ä¢ We propose TextObfuscator, a novel represen-\ntation learning method for privacy-preserving\ninference by obfuscating representations.\n‚Ä¢ We propose to combine semantic and task\ninformation for finding prototypes, which\nleads to improved performance.\n‚Ä¢ We evaluate TextObfuscator on several NLP\ntasks including token and sentence classifica-\ntion, and demonstrate its effectiveness in pro-\ntecting privacy while improving performance.\n2 Preliminaries\n2.1 Inference as Service\nSuppose a client wants to query the inference\nservice without leaking privacy, client can perform\nacceptable computations on their device (such\nas intelligent chips, smartphones, and personal\ncomputers) to obtain the word representationsH =\nfŒ∏c(X), where fŒ∏c is the client model released by\nservice provider. Then numerical H instead of\nplain text X are uploaded to the cloud. The PLM\nfŒ∏s deployed on the server performs subsequent\ncompute-intensive inference Y = fŒ∏s(H) and\nsends predictions Y back to the client. In this\nscenario, only representations are shared with\nservice providers, avoiding the leakage of text.\n2.2 Privacy Threats\nPrivacy threats in the inference phase mainly come\nfrom service providers, who have access to the\nclient model fŒ∏c, server model fŒ∏s and client‚Äôs\nword representation H. Recently studies (Song\nand Raghunathan, 2020) have shown that the word\ninformation in the representation is sufficient to\nreconstruct the original text. For example, the\nshallow representations are usually similar to their\nembedding, privacy attackers can compare the\n5460\nServer Model (Large)\nStep1: Find Task-Related Prototypes\nTask InformationSemantic Information\nTraining Set\nPrototypeInitializationWord Assignmentùë≥ùíÑùíçùíñùíîùíïùíÜùíì Obfuscated Representations\n‚Ä¶dislike‚Ä¶Mike‚Ä¶John Jamesgoodgreatwonderfulhatedetest‚Ä¶‚Ä¶\nTask Label\nStatistical AnalysisClustering Loss RandomPerturbationRepresentation Space\nMikeInput TextPrototype livesin Second Street John staysat Third Avenue 0 1 2 3 4 0 1 2 3 4\nClient Model (Small)\nùêø!\"#$+ùêø%&'#!()\nStep2:Private Representation Training\nFigure 2: Overview of the proposed method. In the first step, we find the task-related prototype using semantic\nand task information. Prototypes get initialized, and each word is assigned to a prototype. In the second step,\nwe use word representation from the client model to calculate cluster loss, which encourages functionally similar\nrepresentations clustered together. Then random perturbations are applied to representations to mislead privacy\nattackers, and we send these perturbed representations to the server model for task loss. Finally, we optimize the\nsmall client and large server models via cluster loss and task loss.\nrepresentation and embedding matrices to identify\nthe most similar word. Furthermore, service\nproviders can generate word representations via\nclient model and train a powerful inversion model\nto directly transform the representations back into\nthe original text X = fŒ∏inv (H), even if privacy-\npreserving methods have been applied on H. The\nchallenge in private inference lies in ensuring fŒ∏inv\ncannot learn useful word information from H to\nreconstruct X, while that the information in H is\nsufficient for subsequent inference.\n3 Our Method\n3.1 Overview\nIn this section, we present TextObfuscator, a novel\nframework for privacy-preserving inference. Our\nmethod learns private representation from a new\nperspective, which aims to obfuscate rather than re-\nduce word information. The overall framework of\nTextObfuscator is shown in Figure 2. We first find\nprototypes for each word using semantic and task\ninformation, these prototypes are used to encourage\nfunctionally similar words to cluster together in\nthe training phase. Then random perturbations\nare applied to each representation, making them\nindistinguishable from the surrounding clustered\nword representations. Even if privacy attackers get\nrepresentation, they cannot establish the correct\nconnection between the obfuscated representations\nand original words. Furthermore, these repre-\nsentations maintain original word functionality as\nthey remain close to their prototype. Next, we\nintroduce how to find prototypes and learn private\nrepresentation.\n3.2 Find Task-Related Prototypes\nIn step one, we introduce two crucial components.\nOne is M(xi) =pxi, which assigning word xi to\nits prototype pxi, refered asword assignment. The\nother is obtaining initial prototypes P= {pi}np\ni=1,\nrefered as prototype initialization. We enhance\nword assignment and prototype initialization from\nthe semantic and task perspective, as the function\nof a word is not solely determined by its semantics,\nbut also by its role in the target task.\n3.2.1 Token-Level Task\nIn the token-level task, each word is assigned a\nlabel, which corresponds to the initial definition of\nthe prototype (Snell et al., 2017; Ji et al., 2022).\nAs the result, we take the label of the word as the\nindicator of prototype for token-level tasks.\nGiven a token-level dataset Dt = {(Xi,Yi)}N\ni=1,\nwhere (Xi,Yi) = {xj,yj}n\nj=1, we first use\nthe client model fŒ∏c to traverse the dataset Dt\nand obtain the representations {Hi}N\ni=1 where\nHi = fŒ∏c(Xi). These contextual representations\n5461\ncan provide semantic information for prototype\ninitialization. Then we assign words with the\nidentical label to the same prototype and take the\naverage representation of words within a particular\nclass as the initial prototype. Suppose there are k\nrepresentations belong to the label c, the prototype\npc of label ccan be represented as:\npc = 1\nk\nk‚àë\nj=1\nhc\nj, (1)\nwhere hc\nj is the j-th representation of label cand k\nis the number of representations in label c.\nIn this way, we leverage the task information\nfrom the label to guide the word assignment M,\nand subsequently utilize the semantic information\nfrom the word representations to obtain the\nprototype initialization P = {pi}np\ni=1, where np\ndenotes the number of labels.\n3.2.2 Sentence-Level Task\nUnlike token-level tasks, there are no natural\nprototype indicators for words in the sentence-level\ndataset. To tackle this problem, we perform a\nclustering algorithm on the representations, using\nthe clustering results to indicate word assignment\nand prototype initialization.\nTo perform clustering, we need to prepare a\nrepresentation for each word. Similar to the\ntoken-level task, we first use client model fŒ∏c\nto traverse sentence-level dataset Ds and obtain\n{Hi}N\ni=1. For word xi that appears repeatedly\nin different contexts, we calculate the average\nof their representations to obtain the final word\nrepresentation ÀÜxi = 1\nk\n‚àëk\nj=1 hx\ni, where hxi\nj is the\nj-th word representation of word xi and k is the\nnumber of words xi occurs in Ds. Finally, we get\nÀÜX = {ÀÜxi}nx\ni=1 where nx is the number of unique\nwords in Ds, and perform K-Means on ÀÜX:\nM,P= Kmeans( ÀÜX), (2)\nthe clustering algorithm assigns semantically simi-\nlar words to the same cluster, thus completing the\nword assignment M. The centroid of the clusters is\nused as the prototypes initialization P= {pi}np\ni=1\nwhere np here is the pre-defined number of clusters.\nHowever, it is not appropriate to assign all\nsemantically similar words to the same cluster.\nFor example, in sentiment analysis, the word\nrepresentations of \"good\" and \"bad\" may be similar\nin representation space and are often assigned to\nthe same cluster, but if they play the same role in a\nsentence, it can lead to ambiguity in the sentiment\nof the sentence. We refer to words that are highly\nrelevant to specific classes as task-related words,\nand it is important to ensure that task-related words\nfrom different classes are assigned to different\nprototypes.\nTo identify task-related words for each class,\nwe use the TF-IDF (Salton and Buckley, 1988), a\nnumerical statistic that reflects a word‚Äôs importance\nin a document. In this case, we treat all sentences\nwithin a class as one document and use TF-IDF to\nfind the keywords for each class. Subsequently, the\nresulting keywords are used to re-divide Mand\nupdate P, the algorithm of re-division is shown in\nAppendix A.3.\n3.3 Private Representation Training\nIn the training phase, we use prototypes to encour-\nage functionally similar word representations to\nbe clustered in the representation space and apply\nrandom perturbation for preserving privacy.\nClustering Loss. Given the input text X =\n{xi}n\ni=1 and word assigniment M, we first use\nclient model fŒ∏c to get the word representation\nH = {hi}n\ni=1, then use center loss (Wen et al.,\n2016) to make representation close to its prototype:\nLclose = 1\n2\nn‚àë\ni=1\n||hi ‚àípxi||2\n2, (3)\nwhere pxi = M(xi) is the prototype of xi.\nFurthermore, we also pull away the distance\nbetween different prototypes to prevent these\nprototypes collapse during training (Li et al.,\n2021a), thus enhancing task performance. The\nprototype distance loss is formulated as:\nLaway = 2\nnp(np ‚àí1)\nnp‚àë\ni=1\nnp‚àë\nj=i+1\n||pi ‚àípj||2\n2, (4)\nwhere np is the number of prototypes. We refer to\nLclose and Laway together as Lcluster.\nRandom Perturbation. We apply a random\nperturbation to each representation hi, which\nshifts the representation to another point near its\nprototype. Following Plant et al. (2021), we take\nthe Laplace Noise as the random perturbation. The\nperturbed representations are sent to the server\nmodel fŒ∏s for subsequent computation:\nÀÜY = fŒ∏s(H + Lap(œµ)), (5)\n5462\nwhere ÀÜY is the prediction results and œµ is the\nhyperparameter to control the scale of noise. The\nperturbation is applied in both the training and\ninference phases. Because each perturbation is\nrandom, it is difficult for a privacy attacker to es-\ntablish a link between the perturbed representation\nand the original word. Perturbed representations\ndeviate from the original words but still serve\noriginal functions, thus preserving privacy while\nmaintaining performance.\nOverall Loss. The supervised task loss Ltask is\njoint learning in a multi-task learning manner, the\noverall loss for our model is:\nL= Ltask + Œ≥1Lclose + Œ≥2Laway, (6)\nwhere Œ≥1 and Œ≥2 are weighting factors. Inspired\nby Li et al. (2021a), we perform the clustering\nalgorithm at the beginning of each epoch to\nmake clustering results more accurate. During\nthe training phase, the client model and server\nmodel are optimized together by service providers.\nDuring the inference phase, the client performs\nlightweight inference using the client model, then\nshares obfuscated representations with service\nproviders and potential privacy attackers. Aside\nfrom perturbations, the inference phase of our\nmethod is the same as standard PLMs, thus, we\ndo not introduce additional inference time.\n4 Experiment\n4.1 Datasets\nTo verify the effectiveness of our methods, we con-\nduct experiments on both token classification and\nsentence classification tasks, covering named entity\nrecognition: CoNLL2003 (Tjong Kim Sang and\nDe Meulder, 2003) and OntoNotes5.0 (Weischedel\net al., 2013), sentiment analysis: SST-2 (Socher\net al., 2013) and topic classification: AGNEWS,\n(Zhang et al., 2015). These tasks are close to real-\nworld applications, which can verify the actual\nutility of our methods. The statistics of datasets are\nshown in Appendix A.1.\n4.2 Baselines\n4.2.1 Attack Methods\nWe use three recently proposed text reconstruction\nmethods for privacy attacks. KNN-Attack (Qu\net al., 2021) computes the distance between each\nrepresentation and public word embedding matrix\nand takes the nearest word in the embedding matrix\nas the attack result. The attacker can be anyone\nwho has access to the client‚Äôs representation.\nInversion-Attack (H√∂hmann et al., 2021) requires\nthe attacker to train an inversion model, which\ndirectly transforms the client representation to a\nword in a one-to-one manner. The attacker can\nbe the service provider with access to the client\nand server model to generate training data for\nthe inversion model. MLC-Attack (Song and\nRaghunathan, 2020) also trains an inversion model\nlike Inversion-Attack, but it runs in a multi-label\nclassification manner and predicts a set of words in\nthe sentence independent of their word ordering.\n4.2.2 Defence Methods\nWe compare our TextObfuscator with three repre-\nsentative privacy-preserving methods and standard\nFine-tune (Devlin et al., 2018). DPNR (Lyu et al.,\n2020b) uses differential privacy and word dropout\nto provide a privacy guarantee. CAPE (Plant\net al., 2021) further adopts differential privacy and\nadversarial training to reduce privacy information\nin representation. SanText+ (Yue et al., 2021b)\nreplaces the sensitive words in plain text based on\ndifferential privacy and word frequency.\n4.3 Privacy Metrics\nTopK is a token-level metric that measures the\npercentage of correct words in the attacker‚Äôs top k\npredictions. RougeL (Lin, 2004) is a generation\nmetric that measures the overlap between two\nsentences. We follow Gupta et al. (2022) and take it\nas a sentence-level metric to measure the coherence\nof attack results. Set is a metric specific to MLC-\nAttack, which quantifies the proportion of words in\noriginal sentence that are present in prediction set.\nDetails of metrics are shown in Appendix A.2.\n4.4 Experimental Settings\nIn our experiments, all methods are implemented\nbased on robertabase (Liu et al., 2019). We divide\nthe model into a smaller client model fŒ∏c with\nthree transformer layers and a large server model\nfŒ∏s with the remaining nine transformer layers.\nThe privacy attack methods are all performed\nin the output representations of fŒ∏c, which will\nbe shared with service providers and under the\nrisk of privacy leakage. For the privacy defence\nmethods, DPNR, CAPE, and our TextObfuscator\nare applied to the output representation of fŒ∏c, and\nSanText+ is applied to the input text directly. The\n5463\nDataset Method Acc/F1 ‚Üë KNN-Attack ‚Üì Inversion-Attack ‚Üì MLC-Attack ‚Üì\nTop1 Top5 Rouge Top1 Top5 Rouge Set\nCoNLL2003\nFine-tune 91.72 87.33 97.72 90.89 99.99 100 99.90 41.41\nDPNR 79.14 0.03 0.47 0.99 14.60 28.91 11.54 10.21\nCAPE 84.47 0.03 0.51 0.82 10.39 22.28 8.94 9.37\nSanText+ 76.94 60.59 75.29 50.80 81.54 87.68 69.18 13.36\nOurs 89.11 0.24 1.42 1.01 6.18 18.56 5.44 8.32\nOntoNotes5\nFine-tune 89.68 80.18 98.17 92.65 100 100 100 71.13\nDPNR 72.38 0.07 0.73 1.72 18.18 33.94 17.62 15.87\nCAPE 85.89 0.05 0.82 1.31 14.57 30.02 14.25 13.62\nSanText+ 71.57 57.35 73.40 51.21 78.99 86.07 68.05 46.90\nOurs 87.17 0.68 2.31 2.13 7.97 22.22 9.88 13.62\nSST-2\nFine-tune 94.38 88.21 98.75 96.04 100 100 100 62.09\nDPNR 87.84 0.02 1.76 0.87 4.39 16.39 5.72 16.82\nCAPE 89.44 0.03 1.86 0.70 5.06 16.15 6.37 17.12\nSanText+ 87.27 70.78 75.95 60.05 81.79 89.01 69.17 52.73\nOurs 91.51 0.05 0.47 0.87 5.48 17.97 11.35 15.74\nAGNEWS\nFine-tune 94.71 89.45 98.87 96.37 100 100 100 86.13\nDPNR 93.12 0.02 2.32 1.79 3.97 13.53 6.82 15.86\nCAPE 93.99 0.02 3.41 1.58 3.39 12.60 2.22 14.26\nSanText+ 91.92 59.31 64.58 51.57 78.20 85.11 70.86 61.36\nOurs 94.52 0.04 0.53 1.12 3.38 12.37 2.01 13.16\nTable 1: Main results on privacy and task performance evaluation. Task metric for SST-2 and AGNEWS is accuracy\nand for CoNLL2003 and OntoNotes5 is F1. Bold term means the best result except Fine-tune. Acc/F1 higher is\nbetter, meaning high task performance. Top1, Top5 and Rouge lower is better, meaning low privacy leakage.\nimplementation details and hyperparameters are\nshown in Appendix A.4.\n4.5 Main Results\nTable 1 shows the main results of our method\nand all baselines. We can observe that: (1)\nRepresentations without defence method are\nvulnerable to privacy attacks.In the absence\nof any privacy defence methods, all privacy attacks\non Fine-tune are highly successful. Inversion-\nattack even achieves 100% top-1 attack accuracy,\nindicating that privacy is fully compromised. (2)\nResisting Inversion-Attacks is key to protecting\nprivacy. Most defence methods can resist KNN-\nAttack, it only achieves nearly 0 Top1 and Top5\nattack accuracy for all tasks. In the case of MLC-\nAttack, the attack results are a set of disordered\nwords that may contain redundancy. It is hard to\nreconstruct the correct sequence from such words.\nHowever, for Inversion-Attack, the representation\nis transformed into the original word one-to-one,\nand it achieves the highest attack accuracy, which\nis the most likely to compromise privacy. (3)\nPrevious defence methods achieve limited task\nperformance. CAPE and DPNR show good\nprivacy on sentence-level tasks, Inversion-attack on\nthese methods only achieve about 5% top 1 attack\naccuracy on SST2 and AGNEWS. But for token-\nlevel tasks which require richer word information,\nthese methods not only degraded privacy but\nalso suffered significantly from task performance.\nWe speculate that these methods reduce the\nprivacy information, i.e., word information, in the\nrepresentation, which hinders the understanding of\nthe sentence. There is an inherent contradiction\nbetween reducing word information on shallow\nrepresentations and maintaining task performance,\nespecially on token-level tasks. (4) With equal\nor better privacy, our proposed TextObfuscator\nshows a task performance improvement over\nbaselines. The advantage of the TextObfuscator\nis that we do not reduce private information but\nrather obfuscate clustered representations, which\nmisleads privacy attackers while still preserving\nfunctionality for each word representation, thus\nachieving better task performance while maintain-\ning privacy.\n5 Analysis\n5.1 Ablation Study\nEffect of Different Components.To verify the\neffectiveness of the different components (Lclose,\n5464\nLaway and random perturbation) in our method, we\nconduct a series of ablation experiments and show\nthe results in Table 2. We can observe that: (1)\nWithout cluster loss ( Lclose and Laway), random\nperturbation alone is inadequate as a defence\nagainst privacy attacks. We speculate that the\nperturbation applied to unclustered representations\ncan only provide limited obfuscation to the attacker.\nMost perturbed words still maintain a distance from\nother words, providing attackers the opportunity to\ndistinguish between them. (2) Cluster loss without\nrandom perturbation is completely indefensible\nagainst Inversion-Attack. The powerful inversion\nmodel can still distinguish different words from\nclustered representations. Only the combination\nof clustered representations and random pertur-\nbations can effectively mislead privacy attackers.\n(3) Without the Laway, some prototypes tend to\ncollapse to one point, resulting in a decline in task\nperformance but a privacy boost.\nDataset Method Task‚Üë KNN ‚Üì Inversion ‚Üì\nSST-2\nTextObfuscator 91.17 0.05 6.01\nw/o Laway 90.37 0.00 6.47\nw/o Lcluster 90.13 4.29 31 .44\nw/o Perturb 93.12 0.00 100\nCoNLL03\nTextObfuscator 89.11 0.26 7.02\nw/o Laway 88.44 0.23 7.41\nw/o Lcluster 89.06 2.21 31 .60\nw/o Perturb 91.42 0.05 100\nTable 2: Ablation Study on our method. KNN and\nInversion-attack use Top1 accuracy.\nEffect of Clustering Algorithms.Sentence clas-\nsification tasks require two additional processes,\nclustering and re-division algorithms. We conduct\nexperiments on SST-2 to verify the performance\nand privacy impact of the cluster number and\nTF-IDF-based re-division. From experimental\nresults shown in Figure 3, we can observe that re-\ndivision (KMeans-TFIDF) consistently improves\ntask performance and privacy for all cluster\nnumbers. Besides that, we find that a large cluster\nnumber can damage privacy (lower Top1 means\nbetter privacy), and a small cluster number can lead\nto a degradation in task performance. Therefore, a\nmoderate cluster number is deemed to be optimal.\n5.2 Visualisation\nRepresentation Visualisation. To intuitively\nshow the influence of the cluster loss and random\n50 75 100 125 150 175 200\nNumber of Clusters\n88.0\n88.5\n89.0\n89.5\n90.0\n90.5\n91.0Performance\n5.25\n5.50\n5.75\n6.00\n6.25\n6.50\n6.75\nPrivacy\nAcc(Kmeans)\nAcc(Kmeans&TFIDF)\nT op1(Kmeans)\nT op1(Kmeans&TFIDF)\nFigure 3: Privacy and task performance under different\ncluster numbers. Accuracy is used for performance and\nTop1 under Inversion-Attack is used for privacy.\nperturbation, we employed T-SNE (Van der Maaten\nand Hinton, 2008) to visualize representations\nof TextObfuscator. Specifically, we select six\nclasses from the CoNLL2003 dataset and utilized\nthe full test set to generate these representations.\nFrom the visualization results in Figure 4, we\ncan observe that, before perturbing (triangular\npoint), functionally similar representations are\nclustered together while maintaining a certain\ndistance from other clusters. After perturbing\n(round point), the representations are shifted and\nmixed with surrounding representations but remain\nwithin their respective functional clusters. Such\nrepresentations obfuscate word information as\nthey are indistinguishable from each other, but\nmaintain word function as they still perform the\nsame function in the representation space. We\ntake NER as an example, perturbing the word\nrepresentation of \"John\" may result in it being\nsimilar to another word, such as \"Mike\". However,\na privacy attacker will only be able to establish\na false association between the representation of\n\"Mike\" and the original word \"John\", thereby\neffectively protecting privacy. But for NER task,\nboth words \"John\" and \"Mike\" serve the same\nrole as \"PER (Person)\" and do not negatively\nimpact the model‚Äôs ability to classify them. These\nvisualization results provide empirical evidence for\nthe principles and effectiveness of TextObfuscator.\nAttack Results Visualisation. To intuitively\nshow the effectiveness of our privacy-preserving\nmethod, we visualize the results of privacy attacks\nfor one sample from OntoNotes5. As shown in\nTable 3, we can observe that the attack results on\nTextObfuscator are largely unreadable, with only\nsome high-frequency words ‚Äúthe‚Äù and wrong words\n5465\nInput text: President Bushcalled his attention to the matter during the Italian leader‚Äôs visithere last week.\nFine-tune\nKNN: President Bushhis attention to matter during Italian leader ‚Äôs visithere last week.\nInversion: President Bushcalled his attention to the matter during the Italian leader‚Äôs visithere last week.\nMLC: { Bush | President | visit | Italian | week | last | ‚Äô | s | the | called | here | during | to | his | . | this }\nText\nObfuscator\nKNN: ...... anybody ls <= our Israeliibble >ancial clinicians, Wednesday Sag Jin relocation teleport.\nInversion: the The Putin the the the the the the the the Israeli the the the the the next year,\nMLC: { to | the | . | in, }\nTable 3: Results of privacy attack. Text in red represents successfully recovered words. Text in bold means privacy\ninformation. Attacks on TextObfuscator only recover meaningless words, no useful information is leakaged.\nRepresentation Visualization\nB-PER\nI-PER\nB-ORG\nI-ORG\nB-LOC\nI-LOC\nB-PER-perturbed\nI-PER-perturbed\nB-ORG-perturbed\nI-ORG-perturbed\nB-LOC-perturbed\nI-LOC-perturbed\nB-PER-prototype\nI-PER-prototype\nB-ORG-prototype\nI-ORG-prototype\nB-LOC-prototype\nI-LOC-prototype\nFigure 4: Visualization of representations. Obfuscated\nrepresentations are indistinguishable from surrounding\nword representations but still remain in their clusters.\nsuch as ‚ÄúPutin‚Äù being recovered. Keywords such as\npeople, places, and time that may contain privacy\nhave not been recovered correctly, indicating that\nour method is effective in protecting privacy.\n6 Related Work\nThe high performance and computational cost\nof PLMs have accelerated the development of\ninference services (Soifer et al., 2019; Pais et al.,\n2022). These services enable clients to perform\ncompute-intensive PLM inference in the cloud by\nuploading personal data, which brings convenience\nbut also raises concerns about privacy (Zhang et al.,\n2021; Liu et al., 2020a).\nIn order to mitigate privacy leakage, many\nsought to upload representations that have been pri-\nvatized by privacy-preserving technologies instead\nof the original text to the cloud (DALE, 2015). One\nmethod is to encrypt the representation, using either\nhomomorphic encryption (Chen et al., 2022b)\nor a customized encryption protocol (Hao et al.,\n2022) to enable computations to be performed on\nthe encrypted representation. Encryption-based\nmethods often require high computation time\nand communication costs (Gilad-Bachrach et al.,\n2016) and may not be practical for real-world\napplications. Therefore, we did not compare this\nmethod in our experiments. Another method is to\nuse Differential privacy (Xu et al., 2020; Lyu et al.,\n2020a; Yue et al., 2021a; Hoory et al., 2021) and\nadversarial training (Coavoux et al., 2018; Plant\net al., 2021; Chen et al., 2022a) to learn private\nrepresentation, which reduces privacy attributes in\nrepresentation. Applying these works to reduce\nword information leads to limited performance,\nas the word information in the shallow layer is\nimportant for subsequent inference. Our method\nproposes to obfuscate word information while\nmaintaining word functionality, thus providing\nbetter performance and privacy. Recently, Zhou\net al. (2022a) propose TextFusion, which utilizes\ntoken fusion to hinder privacy attackers from\ntraining a targeted inversion model. We explore a\nstronger and more realistic setting than TextFusion,\nwhere the privacy attacker is the service provider\nitself. As the service provider is aware of\nTextFusion‚Äôs defense strategies, they can design\ntargeted privacy attack methods to disclose more\nsensitive information. We did not compare our\nmethod with TextFusion due to different settings.\nIn addition to NLP, there are also many works\nfor protecting inference privacy in computer vision\n(Xiang et al., 2019; Osia et al., 2020; Liu et al.,\n2020b), However, most of these methods cannot be\nused directly in NLP because they only consider\none single image, and we need to protect the\nprivacy of a sequence of words. The popularity of\ntransformer structures (Dosovitskiy et al., 2020) in\ncomputer vision may alleviate this situation, but the\nadaptation of these methods still requires further\nexploration.\n5466\n7 Conclusion\nIn this paper, we propose TextObfuscator, a\nnovel representation learning method for privacy-\npreserving inference. The main idea of our method\nis to obfuscate word information and maintain word\nfunctionality. We achieve this by applying random\nperturbations to the clustered representations. The\nperturbed representations are indistinguishable\nfrom the surrounding representations but still\naround their functional clusters. To learn clustered\nrepresentation, we find prototypes for each word\nand encourage the word representation to be\nclose to its prototype. Additionally, we propose\ndifferent methods to find prototypes for token-level\nand sentence-level tasks, utilizing semantic and\ntask information. Through experiments on token\nand sentence classification tasks, we evaluate the\neffectiveness of TextObfuscator and provide further\nanalysis of the principles of our proposed method.\nOverall, our results suggest that TextObfuscator is a\npromising method for preserving inference privacy.\n8 Limitations\nWe summarize the limitations of our method\nas follows: (1) TextObfuscator was designed to\nprotect word privacy in the inference phase, and\nwe did not verify its ability to preserve other\nprivacy attributes and training phase privacy. (2)\nAlthough we have done empirical experiments and\nvisualizations to demonstrate the effectiveness of\nour method, a mathematical proof would enhance\nits privacy guarantees. (3) Our method requires\nmore training steps than fine-tuning, resulting in an\nincreased computational cost.\nAcknowledgements\nThe authors wish to thank the anonymous reviewers\nfor their helpful comments. This work was partially\nfunded by National Natural Science Founda-\ntion of China (No.62206057,62076069,61976056),\nShanghai Rising-Star Program (23QA1400200),\nProgram of Shanghai Academic Research Leader\nunder grant 22XD1401100, and Natural Science\nFoundation of Shanghai (23ZR1403500).\nReferences\nSam Altman. 2022. Openai api. URL. https://\nopenai.com/api/.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJikun Chen, Feng Qiang, and Na Ruan. 2022a.\nAdversarial representation sharing: A quantitative\nand secure collaborative learning framework. arXiv\npreprint arXiv:2203.14299.\nTianyu Chen, Hangbo Bao, Shaohan Huang, Li Dong,\nBinxing Jiao, Daxin Jiang, Haoyi Zhou, and Jianxin\nLi. 2022b. The-x: Privacy-preserving transformer\ninference with homomorphic encryption. arXiv\npreprint arXiv:2206.00216.\nMaximin Coavoux, Shashi Narayan, and Shay B Cohen.\n2018. Privacy-preserving neural representations of\ntext. arXiv preprint arXiv:1808.09408.\nROBERT DALE. 2015. Nlp meets the cloud. Natural\nLanguage Engineering, 21(4):653‚Äì659.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training\nof deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2020. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale.\nCraig Gentry. 2009. Fully homomorphic encryption\nusing ideal lattices. In Proceedings of the forty-first\nannual ACM symposium on Theory of computing ,\npages 169‚Äì178.\nRan Gilad-Bachrach, Nathan Dowlin, Kim Laine,\nKristin Lauter, Michael Naehrig, and John Wernsing.\n2016. Cryptonets: Applying neural networks to\nencrypted data with high throughput and accuracy. In\nInternational conference on machine learning, pages\n201‚Äì210. PMLR.\nSamyak Gupta, Yangsibo Huang, Zexuan Zhong,\nTianyu Gao, Kai Li, and Danqi Chen. 2022.\nRecovering private text in federated learning of\nlanguage models. arXiv preprint arXiv:2205.08514.\nMeng Hao, Hongwei Li, Hanxiao Chen, Pengzhi Xing,\nGuowen Xu, and Tianwei Zhang. 2022. Iron: Private\ninference on transformers. In Advances in Neural\nInformation Processing Systems.\nJohannes H√∂hmann, Achim Rettinger, and Kai Kugler.\n2021. Invbert: Text reconstruction from contextu-\nalized embeddings used for derived text formats of\nliterary works. arXiv preprint arXiv:2109.10104.\n5467\nShlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell,\nAlon Peled-Cohen, Itay Laish, Hootan Nakhost, Uri\nStemmer, Ayelet Benjamini, Avinatan Hassidim, et al.\n2021. Learning and evaluating a differentially private\npre-trained language model. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 1178‚Äì1189.\nGanesh Jawahar, Beno√Æt Sagot, and Djam√© Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics, pages 3651‚Äì3657, Florence, Italy.\nAssociation for Computational Linguistics.\nBin Ji, Shasha Li, Shaoduo Gan, Jie Yu, Jun Ma, Huijun\nLiu, and Jing Yang. 2022. Few-shot named entity\nrecognition with entity-level prototypical network\nenhanced by dispersedly distributed prototypes. In\nProceedings of the 29th International Conference\non Computational Linguistics , pages 1842‚Äì1854,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nRyan Lehmkuhl, Pratyush Mishra, Akshayaram\nSrinivasan, and Raluca Ada Popa. 2021. Muse:\nSecure inference resilient to malicious clients. In\n30th USENIX Security Symposium (USENIX Security\n21), pages 2201‚Äì2218.\nJunnan Li, Pan Zhou, Caiming Xiong, and Steven\nHoi. 2021a. Prototypical contrastive learning of\nunsupervised representations. In International\nConference on Learning Representations.\nLinyang Li, Demin Song, Ruotian Ma, Xipeng Qiu, and\nXuanjing Huang. 2021b. Knn-bert: fine-tuning pre-\ntrained models with knn classifier. arXiv preprint\narXiv:2110.02523.\nYitong Li, Timothy Baldwin, and Trevor Cohn.\n2018. Towards robust and privacy-preserving text\nrepresentations. arXiv preprint arXiv:1805.06093.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74‚Äì81.\nXimeng Liu, Lehui Xie, Yaopeng Wang, Jian Zou, Jinbo\nXiong, Zuobin Ying, and Athanasios V Vasilakos.\n2020a. Privacy and security issues in deep learning:\nA survey. IEEE Access, 9:4566‚Äì4593.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. Roberta: A robustly optimized bert pretraining\napproach.\nZhijian Liu, Zhanghao Wu, Chuang Gan, Ligeng Zhu,\nand Song Han. 2020b. Datamix: Efficient privacy-\npreserving edge-cloud inference. In European\nConference on Computer Vision , pages 578‚Äì595.\nSpringer.\nLingjuan Lyu, Xuanli He, and Yitong Li. 2020a.\nDifferentially private representation for nlp: Formal\nguarantee and an empirical study on privacy and\nfairness. arXiv preprint arXiv:2010.01285.\nLingjuan Lyu, Xuanli He, and Yitong Li. 2020b.\nDifferentially private representation for NLP: Formal\nguarantee and an empirical study on privacy and\nfairness. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n2355‚Äì2365, Online. Association for Computational\nLinguistics.\nRuotian Ma, Xin Zhou, Tao Gui, Yiding Tan, Linyang\nLi, Qi Zhang, and Xuan-Jing Huang. 2022. Template-\nfree prompt tuning for few-shot ner. In Proceedings\nof the 2022 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5721‚Äì5732.\nSeyed Ali Osia, Ali Shahin Shamsabadi, Sina Sajad-\nmanesh, Ali Taheri, Kleomenis Katevas, Hamid R\nRabiee, Nicholas D Lane, and Hamed Haddadi. 2020.\nA hybrid deep learning architecture for privacy-\npreserving mobile analytics. IEEE Internet of Things\nJournal, 7(5):4505‚Äì4518.\nSebasti√£o Pais, Jo√£o Cordeiro, and M Luqman Jamil.\n2022. Nlp-based platform as a service: a brief review.\nJournal of Big Data, 9(1):1‚Äì26.\nXudong Pan, Mi Zhang, Shouling Ji, and Min Yang.\n2020. Privacy risks of general-purpose language\nmodels. In 2020 IEEE Symposium on Security and\nPrivacy (SP), pages 1314‚Äì1331. IEEE.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1532‚Äì1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nRichard Plant, Dimitra Gkatzia, and Valerio Giuffrida.\n2021. CAPE: Context-aware private embeddings\nfor private language learning. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7970‚Äì7978, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, 63(10):1872‚Äì\n1897.\nChen Qu, Weize Kong, Liu Yang, Mingyang Zhang,\nMichael Bendersky, and Marc Najork. 2021. Natural\nlanguage understanding with privacy-preserving bert.\nIn Proceedings of the 30th ACM International Con-\nference on Information & Knowledge Management,\npages 1488‚Äì1497.\n5468\nGerard Salton and Christopher Buckley. 1988. Term-\nweighting approaches in automatic text retrieval.\nInformation processing & management, 24(5):513‚Äì\n523.\nJake Snell, Kevin Swersky, and Richard Zemel.\n2017. Prototypical networks for few-shot learning.\nAdvances in neural information processing systems,\n30.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing , pages\n1631‚Äì1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nJonathan Soifer, Jason Li, Mingqin Li, Jeffrey\nZhu, Yingnan Li, Yuxiong He, Elton Zheng, Adi\nOltean, Maya Mosyak, Chris Barnes, et al. 2019.\nDeep learning inference service at microsoft. In\n2019 USENIX Conference on Operational Machine\nLearning (OpML 19), pages 15‚Äì17.\nCongzheng Song and Ananth Raghunathan. 2020.\nInformation leakage in embedding models. In\nProceedings of the 2020 ACM SIGSAC Conference\non Computer and Communications Security, pages\n377‚Äì390.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142‚Äì\n147.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nKiri Wagstaff, Claire Cardie, Seth Rogers, Stefan\nSchr√∂dl, et al. 2001. Constrained k-means clustering\nwith background knowledge. In Icml, volume 1,\npages 577‚Äì584.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw,\nNianwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2013. Ontonotes release 5.0\nldc2013t19. Linguistic Data Consortium, Philadel-\nphia, PA, 23.\nYandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao.\n2016. A discriminative feature learning approach for\ndeep face recognition. In European conference on\ncomputer vision, pages 499‚Äì515. Springer.\nLiyao Xiang, Haotian Ma, Hao Zhang, Yifan Zhang,\nJie Ren, and Quanshi Zhang. 2019. Interpretable\ncomplex-valued neural networks for privacy protec-\ntion. arXiv preprint arXiv:1901.09546.\nZekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan,\nand Nathanael Teissier. 2020. A differentially private\ntext perturbation method using a regularized maha-\nlanobis metric. arXiv preprint arXiv:2010.11947.\nJuntao Yu, Bernd Bohnet, and Massimo Poesio. 2020.\nNamed entity recognition as dependency parsing.\narXiv preprint arXiv:2005.07150.\nXiang Yue, Minxin Du, Tianhao Wang, Yaliang\nLi, Huan Sun, and Sherman S. M. Chow. 2021a.\nDifferential privacy for text analytics via natural text\nsanitization. In Findings, ACL-IJCNLP 2021.\nXiang Yue, Minxin Du, Tianhao Wang, Yaliang Li,\nHuan Sun, and Sherman S. M. Chow. 2021b.\nDifferential privacy for text analytics via natural text\nsanitization. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3853‚Äì3866, Online. Association for Computational\nLinguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text\nclassification. Advances in neural information\nprocessing systems, 28.\nXiaoyu Zhang, Chao Chen, Yi Xie, Xiaofeng Chen, Jun\nZhang, and Yang Xiang. 2021. Privacy inference\nattacks and defenses in cloud-based deep neural\nnetwork: A survey.\nXin Zhou, Jinzhu Lu, Tao Gui, Ruotian Ma, Zichu Fei,\nYuran Wang, Yong Ding, Yibo Cheung, Qi Zhang,\nand Xuanjing Huang. 2022a. TextFusion: Privacy-\npreserving pre-trained model inference via token\nfusion. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 8360‚Äì8371, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nXin Zhou, Ruotian Ma, Yicheng Zou, Xuanting Chen,\nTao Gui, Qi Zhang, Xuan-Jing Huang, Rui Xie, and\nWei Wu. 2022b. Making parameter-efficient tuning\nmore efficient: A unified framework for classification\ntasks. In Proceedings of the 29th International\nConference on Computational Linguistics , pages\n7053‚Äì7064.\nA Appendix\nA.1 Statistics of Dataset\nWe use four English datasets, including SST-2\n(Socher et al., 2013) for sentiment classification\n(Li et al., 2021b; Zhou et al., 2022b), AG-\nNEWS (Zhang et al., 2015) for topic classification,\nCoNLL2003 (Tjong Kim Sang and De Meulder,\n2003) and OntoNotes5 (Weischedel et al., 2013)\nfor named entity recognition (Yu et al., 2020; Ma\net al., 2022). We follow the official dataset split for\nAGNEWS, CoNLL2003 and OntoNotes5. The test\nset for SST-2 is not publicly available, the reported\n5469\nresults of SST-2 tasks are tested on the official\ndevelopment set. The statistics of datasets in our\nexperiments are shown in Table 4.\nDataset Domain # Train #Test #Labels\nSST-2 Movie 67349 872 2\nAGNEWS News 120000 7600 4\nCoNLL2003 News 14041 3453 9\nOntoNotes5 General 59924 8262 37\nTable 4: Statistics of the datasets.\nA.2 Privacy Metrics\nIn our experiments, we use three metrics to measure\nprivacy. Next, we will describe these three metrics\nin a formulaic way.\nTopK. Top-K accuracy is defined as the propor-\ntion of times that the real words is among the top\nK predictions made by the attack model, where K\nis a pre-defined parameter. Mathematically, it can\nbe represented as:\nTopK = 1\nN\nN‚àë\ni=1\n[yi ‚ààtopk(pi)] (7)\nwhere N is the total number of representation, yi is\nthe real word of representation i, pi is the predicted\nprobability distribution of the attack model, and\ntopk(pi) is the set of top k words with highest\nprobability for representation i.\nRougeL (Lin, 2004). RougeL is a widely used\nmetric to evaluate the quality of text summarization.\nSo we do not describe the details of RougeL, but\njust state that we take the top1 word of the attack\nresults to compose the sentences for calculating\nRougeL.\nSet. The attack results of MLC-Attack are un-\nordered sets of words, a one-to-one metric like\nTopK cannot be used for this attack, so we use Set\nto measure the attack success rate of MLC-Attack.\nGiven that set A is different words in a sentence,\nthe set B is the prediction results of MLC-Attack,\nthe Set metirc can be represented as:\nSet = |A‚à©B|\n|A| (8)\nThis metric measures how many words in the\noriginal sentence are in the set of predicted results\nof the MLC-Attack.\nAlgorithm 1Re-division Algorithm\nRequire: Representations Matrices ÀÜX; Word\nAssignment M; Prototype Initialization P;\nTask-Related WordsT.\n1: for tc ‚ààT do\n2: // Set of prototypes assigned to other classes\n3: conflict ‚Üê{M(x) ‚ààT|x /‚ààtc}\n4: for x ‚ààtc do\n5: // Divide x to other prototype if conflict occurs\n6: if M(x) ‚ààconflict then\n7: ÀÜx ‚Üêget representation of x from ÀÜX\n8: M(x) ‚Üêargminj /‚ààconflict d(ÀÜx, pj)\n9: end if\n10: end for\n11: end for\n12: // Update Pbased on the new M\n13: for pi ‚ààP do\n14: Pi ‚Üê{x ‚ààÀÜX|M(x) =pi}\n15: pi ‚Üê 1\n|Pi|\n‚àë\nx‚ààPi\nx\n16: end for\n17: return M, P\nA.3 Re-division Algorithm\nAs mentioned in Section 3.2.2, after we find\nthe category-related words T = {tc}nc\nc=1 for each\ncategory using TF-IDF, we re-divide the word\nAssignment M and prototype Initialization P.\nThe algorithm is inspired by constrained K-means\nclustering (Wagstaff et al., 2001), but we only apply\nit once after clustering. The process of re-division\nis shown in Algorithm 1.\nA.4 Implementation Details\nIn this subsection, we describe the implementation\ndetails and the replication process of both attack\nand defence methods. All methods are based on\nRobertabase with 125 million parameters. Dataset\nand models are all loaded from huggingface2. Our\nexperiments are conducted on NVIDIA GeForce\nRTX 3090.\nDetails for Defence Methods.We reference pub-\nlicly available code, implement DPNR 3, CAPE4\nand SanText+5 ourselves. We also conduct a grid\nsearch on the hyperparameters to reproduce the\nbaselines on our setting. For each defence method,\nwe train 50 epochs on SST2, CoNLL2003 and\nOntonotes5, 30 epochs on AGNews to guarantee\nconvergence, the AdamW optimizer and a linear\nlearning rate scheduler are used during training.\n2https://huggingface.co/\n3https://github.com/xlhex/dpnlp\n4https://github.com/NapierNLP/CAPE\n5https://github.com/xiangyue9607/SanText\n5470\nThe default learning rate is 5e-5, we do not\nadjust the learning rate unless we encounter the\ncase of non-convergence. For DPNR, we search\nnoise scale œµ on [0.05, 0.1, 0.5, 1, 5] and the\nword dropout rate on [0, 0.1, 0.3]. For CAPE,\nwe search the adversarial training weights Œª on\n[0.01, 0.05, 0.1, 0.5, 1, 5] and noise scale œµ on\n[0.05, 0.1, 0.5, 1, 5]. For Santext+, we follow\nthe author‚Äôs setting and use GloVe (Pennington\net al., 2014) to guide the word replacement, the\nprobability of non-sensitive words to be sanitized\npdefaults to 0.3 and the sensitive word percentage\nwdefaults to 0.9. We search the privacy parameter\nœµ on [1, 2, 3]. For TextObfuscator, we use the\nK-Means to cluster representation for sentence-\nlevel tasks, and the number of clusters defaults\nto 100. We search the close loss weights Œ≥1 from\n[0.1, 0.5, 1] and away loss weights Œ≥2 from [0.1,\n0.3, 0.5]. Although the noise scale can also be\nadjusted, we found that the most commonly used\nparameter (œµ=1) is sufficient, so we kept the noise\nscale constant in all experiments. We select the\nbest performance and privacy (but prefer privacy)\nresults from the experimental results to report. The\nbest hyperparameters we tuned are shown in Table\n5.\nDataset Method lr bsz Œªadv œµn œµw œµp Œ≥1 Œ≥2\nCoNLL2003\nFinetune 2e-5 32 - - - - - -\nDPNR 1e-5 64 - 5 0.1 - - -\nCAPE 1e-5 32 0.1 5 - - - -\nSantext+ 1e-5 64 - - - 3 - -\nTextObfuscator 5e-5 128 - 1 - - 0.5 0.3\nOntoNotes\nFinetune 2e-5 32 - - - - - -\nDPNR 1e-5 64 - 5 0.1 - - -\nCAPE 1e-5 32 0.05 5 - - - -\nSantext+ 1e-5 64 - - - 3 - -\nTextObfuscator 5e-5 128 - - - - 0.5 0.3\nSST-2\nFinetune 2e-5 32 - - - - - -\nDPNR 1e-5 64 - 0.5 0.1 - - -\nCAPE 1e-5 32 0.1 0.5 - - - -\nSantext+ 1e-5 64 - - - 3 - -\nTextObfuscator 1e-5 256 - - - - 0.5 0.1\nAGNEWS\nFinetune 2e-5 32 - - - - - -\nDPNR 1e-5 64 - 1 0.1 - - -\nCAPE 1e-5 32 0.1 0.5 - - - -\nSantext+ 1e-5 64 - - - 1 - -\nTextObfuscator 5e-5 168 - - - - 0.5 0.1\nTable 5: Hyperparameters of best restults for defence\nmethods. - means the hyperparameter is not used\nin this method. Œªadv is the adversarial training\nweights for CAPE. œµn is the noise scale used in CAPE,\nDPNR and TextObfuscator. œµw and œµp are used in\nSantext+, representing the word dropout rate and privacy\nparameter, respectively. Œ≥1 and Œ≥2 are the close loss\nweight and away loss weight for TextObfuscator.\nDetails for Attack Methods.In our implementa-\ntion of the KNN-Attack, we employed the use\nof the embedding matrix of the robertabase to\ncalculate the Euclidean distance between the client\nrepresentation. When attack the CAPE and DPNR\nmethods, which employ max-min normalization\non the representation, we also applied the same\nnormalization technique on the embedding matrix\nbefore calculating distance. For Inversion-Attack\nand MLC-Attack, the training data is generated by\nthe client model to be attacked on the training set\nof the target task. We use the Robertabase model\nas the backbone for the inversion model and search\nthe learning rate from [1e-4, 1e-5, 1e-6] and train\n10 epochs to guarantee convergence. We take the\nwords with a probability higher than 0.5 as the\nprediction result of MLC.\n5471\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nsection 8\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\nsection 8\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nabstract and section 1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nsection 3 and section 4\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nsection 4\n‚ñ°\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix A.1\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 4 and section 5\n‚ñ°\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe datasets we use are publicly available. And we need to perform NER tasks that involve identifying\nthe names of people, which are usually not anonymized.\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nsection 4 and Appendix A.1\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nAppendix A.1\nC ‚ñ°\u0013 Did you run computational experiments?\nSection 4\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix A.4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5472\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix A.4\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nAppendix A.4\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix A.4\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n5473"
}