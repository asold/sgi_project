{
  "title": "A Comparative Analysis to Evaluate Bias and Fairness Across Large Language Models with Benchmarks",
  "url": "https://openalex.org/W4393318216",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2274286819",
      "name": "陳文意",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5094221643",
      "name": "黃兆明",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3209721572",
    "https://openalex.org/W6857194387",
    "https://openalex.org/W4392340173",
    "https://openalex.org/W4324373918",
    "https://openalex.org/W4285199616",
    "https://openalex.org/W4388691793",
    "https://openalex.org/W4322759323",
    "https://openalex.org/W3175060421",
    "https://openalex.org/W4226112058",
    "https://openalex.org/W6854692045",
    "https://openalex.org/W4376643691",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4389043118",
    "https://openalex.org/W4385227045",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W3034115845",
    "https://openalex.org/W4388598844",
    "https://openalex.org/W4320468833",
    "https://openalex.org/W4310419543",
    "https://openalex.org/W7011884576",
    "https://openalex.org/W2983486364",
    "https://openalex.org/W3152258780",
    "https://openalex.org/W4220993274",
    "https://openalex.org/W6856883822",
    "https://openalex.org/W4390602553",
    "https://openalex.org/W4389364428",
    "https://openalex.org/W4392308773",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W3182414949",
    "https://openalex.org/W4382618460",
    "https://openalex.org/W3016300649",
    "https://openalex.org/W4285192297",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W3092298059",
    "https://openalex.org/W2997616454",
    "https://openalex.org/W3158733382",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4221148519",
    "https://openalex.org/W4391136348",
    "https://openalex.org/W4316038168",
    "https://openalex.org/W4390002645",
    "https://openalex.org/W4391124068",
    "https://openalex.org/W6857563048",
    "https://openalex.org/W4388748397",
    "https://openalex.org/W4391901128",
    "https://openalex.org/W4388585881",
    "https://openalex.org/W6860318346",
    "https://openalex.org/W4386794445",
    "https://openalex.org/W4360891295",
    "https://openalex.org/W4280650171",
    "https://openalex.org/W3130450512",
    "https://openalex.org/W3128330592",
    "https://openalex.org/W3019489177",
    "https://openalex.org/W4321022177",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W4387034760",
    "https://openalex.org/W4313527987",
    "https://openalex.org/W4287116904",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W3168194750",
    "https://openalex.org/W4283217613",
    "https://openalex.org/W4401958727",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W3023702633",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W3101767999",
    "https://openalex.org/W4386728881",
    "https://openalex.org/W4387617694",
    "https://openalex.org/W3105302490",
    "https://openalex.org/W3035032873",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3185212449",
    "https://openalex.org/W4381572755",
    "https://openalex.org/W4221045317",
    "https://openalex.org/W4387144045",
    "https://openalex.org/W4385632485",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4386566565",
    "https://openalex.org/W4387034804",
    "https://openalex.org/W4392402185",
    "https://openalex.org/W4401131969",
    "https://openalex.org/W4390833061"
  ],
  "abstract": "This study performs a comprehensive evaluation of bias and fairness within Large Language Models (LLMs), including ChatGPT-4, Google Gemini, and Llama 2, utilizing the Google BIG-Bench benchmark. Our analysis reveals varied levels of biases across models, with disparities particularly notable in dimensions such as gender, race, and ethnicity. The Google BIG-Bench benchmark proved instrumental in identifying these biases, though its effectiveness is tempered by challenges in capturing the sophisticated manifestations of bias that emerge in real-world contexts. Comparative performance analysis indicates that while each model exhibits strengths in certain areas, no single model uniformly excels across all fairness and bias metrics. The study underscores the intricate balance between model performance, fairness, and efficiency, highlighting the necessity for ongoing research and development in AI ethics to mitigate bias effectively. Insights from this research advocate for a multifaceted approach to AI development, integrating ethical considerations at every stage to ensure the equitable advancement of technology. The findings prompt a call for continued innovation in model training and benchmarking methodologies, aiming to enhance the fairness and inclusivity of future LLMs.",
  "full_text": "1\nA Comparative Analysis to Evaluate Bias and\nFairness Across Large Language Models with\nBenchmarks\nMan-Yee Chan , and Siu-Ming Wong\nAbstract—This study performs a comprehensive evaluation\nof bias and fairness within Large Language Models (LLMs),\nincluding ChatGPT-4, Google Gemini, and Llama 2, utilizing the\nGoogle BIG-Bench benchmark. Our analysis reveals varied levels\nof biases across models, with disparities particularly notable\nin dimensions such as gender, race, and ethnicity. The Google\nBIG-Bench benchmark proved instrumental in identifying these\nbiases, though its effectiveness is tempered by challenges in\ncapturing the sophisticated manifestations of bias that emerge in\nreal-world contexts. Comparative performance analysis indicates\nthat while each model exhibits strengths in certain areas, no\nsingle model uniformly excels across all fairness and bias metrics.\nThe study underscores the intricate balance between model per-\nformance, fairness, and efficiency, highlighting the necessity for\nongoing research and development in AI ethics to mitigate bias\neffectively. Insights from this research advocate for a multifaceted\napproach to AI development, integrating ethical considerations at\nevery stage to ensure the equitable advancement of technology.\nThe findings prompt a call for continued innovation in model\ntraining and benchmarking methodologies, aiming to enhance\nthe fairness and inclusivity of future LLMs.\nIndex Terms—Bias in AI, Fairness Evaluation, Large Language\nModels, Benchmarking, AI Ethics, Model Comparison, Bias\nMitigation Strategies, Performance Analysis.\nI. I NTRODUCTION\nLarge Language Models (LLMs) have become a cornerstone\nin the advancement of natural language processing (NLP)\nand artificial intelligence (AI) technologies, offering unprece-\ndented capabilities in language understanding, generation, and\ntranslation [1]–[3]. LLMs, trained on vast datasets sourced\nfrom the internet, reflect and amplify the biases present in their\ntraining data, raising significant concerns about their fairness\nand the ethical implications of their deployment [4], [5]. The\ninherent biases in LLMs can perpetuate and even exacerbate\nsocietal inequalities, making the examination of fairness and\nbias within these models not just a technical challenge, but a\nmoral requirement [6]–[8]. As LLMs become more integrated\ninto societal frameworks, the urgency to address these biases\nbecomes paramount, necessitating a thorough understanding\nof their nature, origins, and impacts [7]–[10].\nDespite the growing body of research on detecting and mit-\nigating bias in LLMs, existing efforts often fall short in fully\naddressing the complexity of fairness and bias in these models.\nStudies highlighted the inadequacies in current methodologies\nfor bias measurement and mitigation, pointing out that many\nPlease contact the corresponding author, Man-Yee Chan: Chan-Man-Yee-\nHK@hotmail.com\napproaches fail to account for the sophisticated ways in which\nbiases manifest across different contexts and cultures [11]–\n[16]. Furthermore, most existing research has focused on\nEnglish-language models, overlooking the global diversity of\nlanguage use and the different cultural dimensions of bias\n[7], [12], [17]–[21]. This has left a significant gap in our\nunderstanding of LLM fairness and bias, underscoring the need\nfor more comprehensive and inclusive research approaches.\nThis article aims to bridge these gaps by conducting a com-\nparative analysis of bias and fairness across multiple LLMs,\nincluding ChatGPT-4, Google Gemini, and Llama 2, utilizing\nthe Google BIG-Bench benchmark [22]. By evaluating the\neffectiveness of different benchmarks in detecting and miti-\ngating biases, this study seeks to identify which frameworks\nare most effective at ensuring fairness in LLMs. The scope\nof this research extends beyond the technical evaluation of\nmodel biases, exploring the implications of these biases on\nsocietal norms and values. Through this comparative analysis,\nthe article contributes to the broader discourse on AI ethics,\noffering insights into the development of more equitable and\nresponsible AI technologies.\nThis study makes several significant contributions to the\nfield of AI fairness and bias in Large Language Models\n(LLMs):\n1) It provides a comprehensive comparative analysis of\nbias and fairness across three state-of-the-art LLMs:\nChatGPT-4, Google Gemini, and Llama 2, utilizing the\nGoogle BIG-Bench benchmark.\n2) The study introduces a detailed evaluation framework\nthat combines both quantitative and qualitative measures\nto assess model bias and fairness, offering insights into\nthe sophisticated nature of AI ethics.\n3) It highlights the strengths and limitations of the Google\nBIG-Bench as a benchmarking tool for assessing fairness\nand bias in LLMs, contributing to the ongoing discussion\non effective benchmarking practices in AI research.\n4) The research outlines critical areas for future explo-\nration, including the development of more inclusive and\nadaptive benchmarking tools and mitigation strategies,\npaving the way for the creation of more equitable AI\ntechnologies.\nThe remaining part of this article is organized as follows:\nSection 2 is the background and related work. Section 3 intro-\nduces the research methodology. Section 4 lists the experiment\nresults. Section 5 discusses critical insights of the results.\n2\nSection 6 concludes this study.\nII. B ACKGROUND AND RELATED WORK\nThis section reviews existing studies through the lens of\nthree distinct yet interconnected themes, each contributing to\nour understanding of how bias manifests in AI and the efforts\nto mitigate these biases.\nA. Identification and Analysis of Bias in LLMs\nStudies within this theme have consistently demonstrated\nthat LLMs can inherit and amplify biases present in their\ntraining data, affecting their language generation and decision-\nmaking processes [4], [8], [23]–[25]. A common finding across\nseveral studies is that biases related to gender, race, and eth-\nnicity are particularly prevalent, often leading to stereotypical\nor prejudicial outputs [12], [20], [21], [26], [27]. Research\nhas also shown that those biases are not merely surface-level\nissues but deeply embedded in the models’ representations\nand embeddings [28]–[30]. Comparative analyses have high-\nlighted significant variances in bias across different models,\nsuggesting that model architecture and training datasets play\ncrucial roles in bias formation [31]–[33]. Efforts to quantify\nbias through various metrics have led to the development of\nstandardized benchmarks, enabling a more systematic evalua-\ntion of biases across models [19], [22], [34]–[36]. This body\nof work underscores the importance of continuous monitoring\nand assessment of LLMs to understand and address their biases\ncomprehensively.\nB. Bias Mitigation Strategies in LLMs\nThe pursuit of bias mitigation in LLMs has led to a diverse\narray of strategies, ranging from pre-processing and data\naugmentation to in-model adjustments and post-processing\ninterventions [37]–[41]. A significant research outcome has\nbeen the realization that no single strategy is universally\neffective, with the efficacy of different approaches varying\nbased on the type of bias and the specific model [36], [42]–\n[44]. Some studies have found that data augmentation, aimed\nat balancing underrepresented groups in the training data,\ncan reduce certain biases without compromising model per-\nformance [45]–[47]. Others have highlighted the potential of\nadversarial training and debiasing algorithms to make models\nless sensitive to biased inputs [5], [48]–[52]. However, such\nstrategies often require careful calibration to avoid introducing\nnew biases or significantly degrading the model’s utility [19],\n[42], [53], [54]. This theme illustrates the ongoing challenge\nof achieving fairness in LLMs without sacrificing their capa-\nbilities.\nC. Evaluating Fairness and Benchmarks in AI Models\nThe development and application of benchmarks for evaluat-\ning fairness in AI models have been a critical area of research,\naiming to provide standardized criteria for assessing bias and\nfairness [19], [22], [55]–[58]. Studies have consistently shown\nthat benchmarks can reveal sophisticated insights into how\nbiases manifest in different contexts, offering a more granular\nunderstanding of model behavior [19], [59]–[63]. There is a\ngrowing consensus that benchmarks must evolve to capture\nthe multidimensional nature of fairness, incorporating diverse\nperspectives and scenarios [64]–[67]. Research has also high-\nlighted the importance of transparency and interpretability\nin benchmark results, emphasizing that stakeholders should\nunderstand the basis of evaluations to trust and act on them\n[68]–[71]. This theme reflects a broader move towards more\naccountable and equitable AI, with benchmarks playing a\npivotal role in guiding the development and deployment of\nfairer models.\nD. Ethical Considerations and Societal Impacts of LLM Bias\nResearch in this theme explores the broader ethical im-\nplications and societal impacts of biases in LLMs. Studies\nhave raised concerns about the potential for biased LLM\noutputs to reinforce existing social inequalities and undermine\nefforts towards inclusivity and equity [8], [11], [72]. The\nethical dimension of AI development has gained increased\nattention, with calls for more transparent and accountable\nmodeling practices that prioritize ethical considerations [73],\n[74]. Moreover, there is a growing acknowledgment of the\nrole of diverse stakeholder engagement in guiding the ethical\ndevelopment of AI, emphasizing the need for multidisciplinary\ncollaboration in addressing the challenges posed by LLM bias\n[3], [67], [75], [76].\nIII. M ETHODOLOGY\nThis section delineates the methodology adopted for the\ncomparative analysis of bias and fairness across Large Lan-\nguage Models (LLMs), specifically ChatGPT-4, Google Gem-\nini, and Llama 2.\nA. Models Under Study\nThe study focuses on three state-of-the-art LLMs: ChatGPT-\n4, Google Gemini, and Llama 2. ChatGPT-4, developed by\nOpenAI, is the latest iteration in the GPT series, renowned\nfor its deep learning algorithms that facilitate human-like text\ngeneration. Google Gemini, a product of Google’s research, is\ndesigned to enhance understanding and generation of natural\nlanguage, emphasizing the incorporation of broad knowledge\nand contextual comprehension. Llama 2, by Meta AI, stands\nout for its efficient use of training data and scalability, aiming\nto provide high-quality language understanding and generation\nwhile addressing the limitations of previous models. Each\nmodel was selected based on its unique contributions to\nthe advancement of NLP technologies and its potential for\nshowcasing different aspects of bias and fairness within LLMs.\nTable I provides an overview of the key features of these\nmodels.\nThe inclusion of these models in our study is justified by\ntheir leading positions in the NLP field, representing a diverse\nrange of approaches to language modeling, from deep learning\nto efficient data usage and broad knowledge incorporation.\nThis diversity allows for a comprehensive analysis of bias\nand fairness across different technological and methodological\n3\nFeature ChatGPT-4 Google Gemini Llama 2\nDeveloper OpenAI Google Meta AI\nCore Technology Deep Learning Advanced NLP Efficient Training\nKey Strength Human-like Text Generation Contextual Comprehension Scalability\nApplication Scope General NLP tasks Broad Knowledge Integration Diverse Language Support\nBias Mitigation Efforts Ongoing Ongoing Ongoing\nTABLE I: Comparison of Key Features of ChatGPT-4, Google Gemini, and Llama 2\nparadigms. ChatGPT-4’s deep learning algorithms offer in-\nsights into how traditional model architectures might propagate\nor mitigate biases. In contrast, Google Gemini’s emphasis on\ncontextual comprehension provides a lens through which to\nexamine the impact of extensive knowledge integration on fair-\nness. Lastly, Llama 2’s scalability and efficient use of training\ndata present an opportunity to explore the effects of training\nefficiency and data selection on bias in LLMs. Together, these\nmodels encompass a broad spectrum of the current capabilities\nand challenges in the field, making them ideal candidates for\nthis comparative study on bias and fairness.\nB. Benchmarking Tool\nThe Google BIG-Bench benchmark serves as the primary\ntool for evaluating the selected models. BIG-Bench focuses\non broad, interdisciplinary challenges in AI, offering a com-\nprehensive suite of tests designed to assess various aspects\nof language model performance, including bias and fairness.\nThis benchmark is particularly suited for this study due to\nits extensive range of tasks that simulate real-world scenarios\nwhere biases could emerge. The benchmark evaluates models\nbased on their ability to produce unbiased, fair, and ethically\nsound responses across diverse topics, making it an ideal\ntool for this comparative analysis. Table II presents the main\nfeatures and characteristics of the BIG-Bench benchmark.\nThe choice of BIG-Bench as the benchmarking tool is\nfurther justified by its adaptability and the involvement of\nthe broader research community in its development and re-\nfinement. This ensures that the benchmark stays relevant to\ncurrent societal standards and technological advancements.\nIts comprehensive evaluation criteria encompass not just the\ntechnical accuracy of models but also their ethical implications\nand ability to handle complex, sophisticated situations without\nbias. The diversity of tasks within BIG-Bench, ranging from\nsimple language understanding to complex ethical reasoning,\nallows for a multifaceted assessment of the selected LLMs.\nThis diverse testing environment is critical for uncovering\nsubtle and overt biases that might not be apparent in more\nnarrow or specialized benchmarks. By utilizing BIG-Bench,\nthis study aims to provide a holistic and sophisticated view of\nhow well current LLMs navigate the intricate balance between\nhigh performance and ethical responsibility in AI.\nC. Evaluation Criteria\nThe evaluation of bias and fairness across the selected\nLLMs is guided by a set of meticulously defined criteria,\nwhich include both quantitative and qualitative measures.\nThese criteria are designed to provide a holistic view of each\nmodel’s performance in terms of bias and fairness, enabling a\nsophisticated comparison that accounts for the multifaceted\nnature of these constructs. Table III summarizes the main\nevaluation criteria, along with explanations and justifications\nfor their inclusion.\nThe criteria selected for this study, as summarized in Table\nIII, provide a comprehensive framework for evaluating bias\nand fairness in LLMs. Quantitative metrics like accuracy,\nfairness scores, and bias amplification allow for objective\ncomparisons across models, highlighting areas where biases\nmay affect performance. In contrast, qualitative assessments,\nincluding expert reviews and ethical considerations, offer\ndeeper insights into the subtleties of bias and fairness that\nare not easily captured by numerical metrics. Together, these\ncriteria ensure a balanced and thorough evaluation, incorpo-\nrating both the measurable aspects of model performance and\nthe more subjective dimensions of ethical AI development.\nThis approach recognizes the complexity of bias in AI and\nthe necessity of addressing it through a multifaceted lens to\ndevelop more equitable and responsible technologies.\nIV. R ESULTS\nThis section presents the findings from the comparative\nanalysis of bias and fairness across ChatGPT-4, Google Gem-\nini, and Llama 2, using the Google BIG-Bench benchmark.\nThe analysis includes a detailed examination of biases identi-\nfied in each model, an evaluation of fairness, and a comparison\nof overall model performance.\nA. Bias Detection\nBiases within each model were identified using a series of\ntasks from the BIG-Bench benchmark, specifically designed\nto uncover hidden biases and fairness issues. Preliminary\nfindings suggest varying levels of bias across the models, with\nnotable disparities in gender, race, and ethnicity representation.\nStatistical analysis was employed to quantify the significance\nof detected biases, indicating that while all models exhibit\nsome form of bias, the magnitude and nature of the biases\ndiffer significantly across models.\nFigure 1 illustrates the comparative analysis of bias levels\ndetected in ChatGPT-4, Google Gemini, and Llama 2, across\nvarious dimensions such as gender, race, and ethnicity.\nThe bar chart in Figure 1 provides a clear visual represen-\ntation of the biases present in each model. ChatGPT-4 shows\nhigher bias levels across all dimensions, indicating a potential\narea for improvement in future model iterations. Google Gem-\nini and Llama 2 demonstrate lower levels of bias, with Llama 2\nexhibiting the least bias across the evaluated dimensions. This\n4\nCharacteristic Description\nScope Interdisciplinary challenges across various AI domains\nTest Types Quantitative, Qualitative, and Ethical Reasoning\nTask Diversity Includes linguistic understanding, common sense reasoning, and ethical judgment\nReal-World Relevance Simulates scenarios likely to exhibit bias\nEvaluation Criteria Fairness, bias detection, and mitigation capabilities\nAdaptability Regular updates to reflect evolving AI capabilities and societal norms\nCommunity Engagement Open-source contributions and peer reviews\nTABLE II: Main Features and Characteristics of the BIG-Bench Benchmark\nCriterion Explanation Justification\nAccuracy Measures the model’s ability to generate correct\nresponses.\nA fundamental metric for assessing model perfor-\nmance, critical for establishing a baseline of effec-\ntiveness.\nFairness Score Quantifies the model’s performance across different\ndemographic groups.\nEssential for evaluating the equitable distribution of\nmodel benefits and detecting demographic dispari-\nties.\nBias\nAmplification\nAssesses whether the model amplifies biases present\nin its training data.\nCrucial for understanding the model’s impact on\nexisting societal biases and its ability to mitigate\nthem.\nExpert Reviews Involves subjective assessment by experts of the\nmodel’s outputs for biases.\nProvides a sophisticated understanding of bias and\nfairness that quantitative metrics may overlook.\nEthical Consider-\nations\nEvaluates the model’s adherence to ethical guidelines\nand norms.\nEnsures that the model’s outputs align with broader\nsocietal values and ethical standards.\nOffensive\nLanguage\nDetection\nIdentifies the presence of offensive or harmful lan-\nguage in model outputs.\nImportant for assessing the model’s safety and suit-\nability for diverse applications.\nTABLE III: Evaluation Criteria for Assessing Bias and Fairness in LLMs\nGender Race Ethnicity\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.7\n0.8\n0.6\n0.5\n0.6\n0.5\n0.4\n0.5\n0.3\nBias Level (Normalized Score)\nChatGPT-4 Google Gemini Llama 2\nFig. 1: Summary of Bias Detection Results Across Models\nanalysis underscores the importance of continuous monitoring\nand evaluation of biases in LLMs to ensure fairness and equity\nin AI technologies.\nB. Fairness Evaluation\nThe fairness of each model was evaluated based on their\nperformance in tasks designed to assess equitable treatment\nacross diverse demographic groups. Fairness scores were cal-\nculated to highlight the models’ abilities to generate unbiased\noutputs. Comparative analysis reveals distinct patterns in how\neach model manages to address fairness, with some models\ndemonstrating more robust mechanisms for mitigating bias\nthan others. These patterns underscore the complexity of\nachieving fairness in LLMs and the effectiveness of different\napproaches to bias mitigation.\nFigure 2 shows the fairness evaluation results across the\nmodels, providing a comparative view of their performance in\npromoting equitable treatment.\nThe bar chart in Figure 2 visually demonstrates each\nmodel’s ability to maintain fairness across different demo-\ngraphic dimensions. Notably, Llama 2 achieves the highest\nfairness scores, suggesting its methodologies for mitigat-\ning bias are more effective compared to the other models.\nChatGPT-4 and Google Gemini show commendable efforts\ntowards fairness, yet they present room for improvement,\nespecially in fine-tuning their algorithms to better address\nbias. This visual representation not only provides a clear\ncomparative analysis but also emphasizes the ongoing need for\ndevelopment in fairness strategies across LLM technologies.\nC. LLM Model Performance Comparison\nThe overall performance of ChatGPT-4, Google Gemini,\nand Llama 2 was compared not only in terms of bias and\nfairness but also considering their efficiency, accuracy, and\napplicability to real-world scenarios. The statistical analysis,\nincluding ANOV A tests, revealed significant differences in\n5\nGender Race Ethnicity\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.85 0.8 0.820.78 0.75 0.77\n0.9 0.88 0.89\nFairness Score\nChatGPT-4 Google Gemini Llama 2\nFig. 2: Fairness Evaluation Results Across Models\noverall performance across the models, suggesting that trade-\noffs between accuracy, fairness, and efficiency are inevitable.\nFigure 3 presents a summary of the overall performance\ncomparison across the models, highlighting their strengths and\nweaknesses in various dimensions.\nAccuracy Fairness Efficiency Applicability\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.85 0.8 0.75 0.80.8 0.78 0.82 0.78\n0.88 0.85 0.8 0.85\nPerformance Score\nChatGPT-4 Google Gemini Llama 2\nFig. 3: Summary of Overall Performance Comparison of LLM\nModels\nThe bar chart in Figure 3 provides a clear visual representa-\ntion of how each model scores in terms of accuracy, fairness,\nefficiency, and applicability to real-world scenarios. While\nLlama 2 shows a slightly higher performance in accuracy and\napplicability, Google Gemini excels in efficiency, illustrating\nthe trade-offs that developers and researchers must navigate.\nChatGPT-4 offers a balanced performance but indicates po-\ntential areas for improvement in efficiency. This comparative\nanalysis underscores the complexity of achieving excellence\nacross all performance metrics, highlighting the need for ongo-\ning optimization and ethical consideration in the development\nof LLM technologies.\nV. D ISCUSSION\nThe results of this study illuminate the sophisticated chal-\nlenges and considerations in evaluating and ensuring fair-\nness and bias mitigation in Large Language Models (LLMs).\nThis discussion synthesizes insights from the comparative\nanalysis, focusing on the effectiveness of benchmarks, and\nthe performance of different models, while exploring broader\nimplications for AI ethics and development.\nA. Benchmark Effectiveness\nThe Google BIG-Bench has proven instrumental in uncov-\nering biases within LLMs, offering a multifaceted approach\nto evaluate model fairness. However, its effectiveness is not\nwithout limitations, highlighting the complexity of benchmark\ndesign in capturing the multifarious nature of bias. The\nbenchmark’s broad scope sometimes overlooks the subtle,\ncontext-specific manifestations of bias, underscoring the need\nfor benchmarks that adapt to evolving societal norms and\ntechnological advancements. Moreover, the dependency on\npredefined tasks may not fully encompass the dynamic and\nunpredictable ways in which bias can surface in real-world ap-\nplications. The findings suggest a pressing need for continuous\nrefinement of benchmarks, incorporating feedback loops from\ndiverse user communities to ensure relevance and comprehen-\nsiveness. Furthermore, the integration of benchmarks focusing\non underrepresented languages and cultures could significantly\nenhance our understanding of bias in global contexts. The\nstudy’s insights call for a collaborative approach to benchmark\ndevelopment, involving interdisciplinary experts to capture the\nfull spectrum of fairness issues. Ultimately, while the BIG-\nBench marks a significant step forward, it is but one tool in the\nbroader quest for ethical AI, requiring supplementary methods\nand perspectives to holistically address bias.\nB. Model Comparison\nThe comparative analysis of ChatGPT-4, Google Gemini,\nand Llama 2 reveals divergent strengths and weaknesses\nacross models, shedding light on the intricate balance between\nmodel performance and fairness. ChatGPT-4’s sophisticated\ntext generation capabilities underscore the potential trade-offs\nbetween advanced linguistic performance and the amplification\nof biases. Google Gemini’s contextual comprehension offers\na promising avenue for mitigating bias, yet its effectiveness\nis contingent upon the diversity and representativeness of\nthe data it is trained on. Llama 2, with its emphasis on\nscalability and efficient data usage, illustrates the potential for\ndesigning models that are both high-performing and less prone\nto inheriting biases from their training datasets. The findings\nunderscore the importance of a multifaceted approach to model\ndevelopment, where considerations of fairness are integrated\nat every stage of the design and training process. However,\nthe study also highlights the absence of a one-size-fits-all\nsolution to bias mitigation, pointing to the necessity of model-\nspecific strategies. The insights gained from this comparison\n6\nadvocate for the continued exploration of innovative methods\nto enhance both the accuracy and fairness of LLMs. In light\nof these results, future research should focus on developing\nadaptive, context-aware models that can dynamically adjust to\nminimize bias while maintaining high levels of performance.\nThe discussion reinforces the idea that the pursuit of unbiased\nAI is an ongoing process, necessitating vigilance, creativity,\nand commitment from the research community.\nC. Ethical Implications\nThe examination of bias and fairness in LLMs extends\nbeyond technical considerations, raising profound ethical ques-\ntions about the role of AI in society. The ability of these mod-\nels to influence public opinion, shape societal norms, and make\ndecisions affecting individuals’ lives places a significant eth-\nical responsibility on developers and researchers. The study’s\nfindings highlight the potential for AI to perpetuate existing in-\nequalities, underscoring the importance of ethical frameworks\nthat guide the development and deployment of LLMs. Such\nframeworks must prioritize transparency, accountability, and\ninclusivity, ensuring that AI technologies benefit all segments\nof society equitably. The proactive engagement of diverse\nstakeholder groups in the AI development process can provide\nvaluable insights into the multifaceted impacts of technology\non different communities. Additionally, the results advocate\nfor the establishment of ethical oversight mechanisms, such as\nreview boards and ethics committees, to evaluate AI projects\nfor potential biases and fairness issues. The ongoing dialogue\nbetween AI developers, ethicists, policymakers, and the public\nis crucial for navigating the ethical landscape of AI. Ulti-\nmately, the ethical implications of LLMs call for a concerted\neffort to align AI technologies with societal values and human\nrights, ensuring that the benefits of AI are shared broadly and\nequitably.\nD. Implications for AI Development Practices\nThe findings from this study have significant implications\nfor AI development practices, emphasizing the need for a\nparadigm shift towards more responsible and ethical AI cre-\nation. Integrating fairness and bias mitigation strategies from\nthe outset of model development is crucial for building AI\nthat aligns with ethical standards. The study advocates for a\nmultidisciplinary approach to AI development, incorporating\ninsights from ethics, social sciences, and diverse cultural\nperspectives to enrich the understanding and treatment of bias.\nAdopting transparent development processes, where models\nare auditable and their decisions explainable, can foster trust\nand accountability in AI technologies. Furthermore, the en-\ngagement of diverse user groups in testing and feedback\ncycles can unveil biases that may not be evident to devel-\nopers, facilitating the creation of more inclusive AI. The\nestablishment of ethical guidelines and best practices for AI\ndevelopment, informed by the latest research findings and\nsocietal expectations, can guide developers in navigating the\nethical complexities of their work. The study also highlights\nthe importance of ongoing education and training for AI\nprofessionals, equipping them with the knowledge and tools to\nprioritize fairness in their work. By embracing these practices,\nthe AI community can contribute to the development of\ntechnologies that not only advance human capabilities but also\nuphold the values of fairness and equity.\nE. The Role of Policy and Regulation\nThe study’s findings underscore the critical role of policy\nand regulation in shaping the development and deployment of\nequitable AI technologies. Effective policies can set standards\nfor fairness and bias mitigation, compelling developers and\ncompanies to prioritize these issues in their AI systems.\nThe development of regulatory frameworks that require trans-\nparency in AI algorithms and decision-making processes can\nenhance accountability and public trust in AI technologies.\nPolicies that promote the inclusion of diverse perspectives in\nAI development can help ensure that AI systems are equitable\nand beneficial to all segments of society. Additionally, the\nestablishment of independent bodies to oversee AI ethics\nand compliance can provide an additional layer of oversight,\nensuring that AI technologies adhere to ethical standards. The\nstudy highlights the need for international collaboration in\ndeveloping policies and regulations that address the global\nnature of AI technology and its impacts. Encouraging the\nadoption of ethical AI standards across industries can foster\na culture of responsibility and ethics in AI development.\nUltimately, effective policy and regulation are essential for\nharnessing the potential of AI technologies while safeguarding\nagainst biases and ensuring fairness for all.\nF . Future Research Directions\nThe insights gleaned from this study pave the way for sev-\neral promising avenues of future research in the quest for more\nequitable LLMs. One critical area involves exploring alterna-\ntive data sources and training methodologies that minimize\nthe propagation of biases. Investigating the impact of diverse\nand inclusive data sets on model fairness could offer strategies\nfor proactively mitigating bias at the source. Additionally, the\ndevelopment of more sophisticated algorithms for detecting\nand correcting biases in real-time presents a valuable research\ndirection. The exploration of interdisciplinary approaches,\ncombining insights from social sciences, ethics, and com-\nputer science, could yield innovative solutions to the complex\nproblem of bias in AI. Another promising area of research\nfocuses on the user experience, examining how individuals\nfrom various backgrounds interact with and are impacted by\nLLMs. The engagement with underrepresented communities\nin the design and testing phases of model development can\nensure that AI technologies cater to a broader spectrum\nof needs and perspectives. Moreover, the study underscores\nthe need for ongoing evaluation and refinement of fairness\nbenchmarks, adapting them to reflect changing societal norms\nand technological advancements. Ultimately, the pursuit of\nbias-free LLMs requires a holistic approach, embracing the\ncomplexity of human language and culture, and committing\nto continuous improvement and ethical vigilance.\n7\nVI. C ONCLUSIONS\nThis study performed on a comparative analysis of bias and\nfairness across three state-of-the-art Large Language Models\n(LLMs): ChatGPT-4, Google Gemini, and Llama 2, employing\nthe Google BIG-Bench benchmark as a tool for evaluation.\nThe findings reveal significant variations in how each model\nhandles bias and fairness, with none emerging as a definitive\nleader across all metrics. This underscores the inherent chal-\nlenge in designing LLMs that are both highly efficient and\ndevoid of bias. The effectiveness of the Google BIG-Bench\nin identifying biases highlights the importance of benchmarks\nin the ongoing effort to enhance the ethical development of\nAI technologies. However, the limitations observed in current\nbenchmarking approaches call for the development of more\nsophisticated and adaptable evaluation tools that can keep pace\nwith the rapid advancements in AI and the evolving societal\nstandards of fairness.\nThe implications of this research for AI ethics are profound,\nemphasizing the necessity for a multidisciplinary approach\nthat integrates technical, ethical, and societal considerations in\nthe development of LLMs. The study advocates for increased\ntransparency in model development processes, the incorpora-\ntion of diverse datasets, and the adoption of ethical guidelines\nthat prioritize fairness and inclusivity. Furthermore, the results\nsuggest that achieving fairness in AI is a dynamic and mul-\ntifaceted challenge, requiring ongoing attention, innovation,\nand collaboration among researchers, developers, ethicists, and\npolicymakers.\nFuture research should focus on exploring alternative data\nsources, training methodologies, and model architectures that\ncan reduce bias while enhancing performance. The devel-\nopment of more sophisticated benchmarking tools that can\naccurately reflect the complexities of real-world applications is\nalso crucial. Additionally, engaging with diverse communities\nto understand the broader societal impacts of LLMs will\nbe essential in guiding ethical AI development. Ultimately,\nthis study contributes to the broader discourse on AI ethics\nand fairness, providing valuable insights and directions for\nfuture work in creating more equitable and responsible AI\ntechnologies.\nREFERENCES\n[1] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\nE. Agirre, I. Heintz, and D. Roth, “Recent advances in natural language\nprocessing via large pre-trained language models: A survey,” ACM\nComputing Surveys, vol. 56, no. 2, pp. 1–40, 2023.\n[2] W. Hua, L. Li, S. Xu, L. Chen, and Y . Zhang, “Tutorial on large\nlanguage models for recommendation,” in Proceedings of the 17th ACM\nConference on Recommender Systems , 2023, pp. 1281–1283.\n[3] T. Dyde, “Documentation on the emergence, current iterations, and\npossible future of artificial intelligence with a focus on large language\nmodels,” 2023.\n[4] A. Caballero Hinojosa, “Exploring the power of large language models:\nNews intention detection using adaptive learning prompting,” 2023.\n[5] Y . Liu, J. Cao, C. Liu, K. Ding, and L. Jin, “Datasets for large language\nmodels: A comprehensive survey,” arXiv preprint arXiv:2402.18041 ,\n2024.\n[6] I. Garrido-Munoz, “Mitigating biases in deep learning models: A path\ntowards fairness and inclusivity,” 2023.\n[7] A. Laakso, “Ethical challenges of large language models-a systematic\nliterature review,” 2023.\n[8] U. P. Liyanage and N. D. Ranaweera, “Ethical considerations and\npotential risks in the deployment of large language models in diverse\nsocietal contexts,” Journal of Computational Social Dynamics , vol. 8,\nno. 11, pp. 15–25, 2023.\n[9] S. Harrer, “Attention is not all you need: the complicated case of\nethically using large language models in healthcare and medicine,”\nEBioMedicine, vol. 90, 2023.\n[10] A. BARBERIO, “Large language models in data preparation: opportu-\nnities and challenges,” 2022.\n[11] Z. Talat, A. N ´ev´eol, S. Biderman, M. Clinciu, M. Dey, S. Longpre,\nS. Luccioni, M. Masoud, M. Mitchell, D. Radev et al. , “You reap\nwhat you sow: On the challenges of bias evaluation under multilin-\ngual settings,” in Proceedings of BigScience Episode# 5–Workshop on\nChallenges & Perspectives in Creating Large Language Models , 2022,\npp. 26–41.\n[12] S. Kolisko, “Name-based social biases in large language models,” 2022.\n[13] Y . Duan, F. Tang, K. Wu, Z. Guo, S. Huang, Y . Mei, Y . Wang, Z. Yang,\nand S. Gong, “Ranking of large language model (llm) regional bias,”\n2023.\n[14] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language\nmodels: Methods, analysis & insights from training gopher,” arXiv\npreprint arXiv:2112.11446, 2021.\n[15] T. R. McIntosh, T. Liu, T. Susnjak, P. Watters, A. Ng, and M. N. Halga-\nmuge, “A culturally sensitive test to evaluate nuanced gpt hallucination,”\nIEEE Transactions on Artificial Intelligence , 2023.\n[16] K. Ramesh, S. Sitaram, and M. Choudhury, “Fairness in lan-\nguage models beyond english: Gaps and challenges,” arXiv preprint\narXiv:2302.12578, 2023.\n[17] I. Solaiman and C. Dennison, “Process for adapting language models\nto society (palms) with values-targeted datasets,” Advances in Neural\nInformation Processing Systems , vol. 34, pp. 5861–5873, 2021.\n[18] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang,\nM. Cheng, M. Glaese, B. Balle, A. Kasirzadeh et al., “Ethical and social\nrisks of harm from language models,” arXiv preprint arXiv:2112.04359,\n2021.\n[19] Y . Chang, X. Wang, J. Wang, Y . Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\nC. Wang, Y . Wang et al. , “A survey on evaluation of large language\nmodels,” ACM Transactions on Intelligent Systems and Technology ,\n2023.\n[20] R. NA VIGLI, S. CONIA, and B. ROSS, “Biases in large language\nmodels: Origins, inventory, and,” 2023.\n[21] R. Navigli, S. Conia, and B. Ross, “Biases in large language models: ori-\ngins, inventory, and discussion,” ACM Journal of Data and Information\nQuality, vol. 15, no. 2, pp. 1–21, 2023.\n[22] B. bench authors, “Beyond the imitation game: Quantifying and\nextrapolating the capabilities of language models,” Transactions\non Machine Learning Research , 2023. [Online]. Available: https:\n//openreview.net/forum?id=uyTL5Bvosj\n[23] D. Myers, R. Mohawesh, V . I. Chellaboina, A. L. Sathvik, P. Venkatesh,\nY .-H. Ho, H. Henshaw, M. Alhawawreh, D. Berdik, and Y . Jararweh,\n“Foundation and large language models: fundamentals, challenges, op-\nportunities, and social impacts,” Cluster Computing, pp. 1–26, 2023.\n[24] R. Yang, T. F. Tan, W. Lu, A. J. Thirunavukarasu, D. S. W. Ting,\nand N. Liu, “Large language models in health care: Development,\napplications, and challenges,” Health Care Science , vol. 2, no. 4, pp.\n255–263, 2023.\n[25] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language\nmodels encode clinical knowledge,” Nature, vol. 620, no. 7972, pp. 172–\n180, 2023.\n[26] F. Tang, K. Wu, Z. Guo, S. Huang, Y . Mei, Y . Wang, Z. Yang, and\nS. Gong, “Large language model (llm) racial bias evaluation.”\n[27] W. Guo and A. Caliskan, “Detecting emergent intersectional biases:\nContextualized word embeddings contain a distribution of human-like\nbiases,” in Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,\nand Society, 2021, pp. 122–133.\n[28] S. Bhatia, “Exploring the sources of variance in risky decision making\nwith large language models,” 2023.\n[29] M. Hanna, “Investigating large language models’ representations of\nplurality through probing interventions,” 2022.\n[30] B. Li, “Integrating linguistic theory and neural language models,” Ph.D.\ndissertation, University of Toronto (Canada), 2022.\n[31] M. Bakker, M. Chadwick, H. Sheahan, M. Tessler, L. Campbell-\nGillingham, J. Balaguer, N. McAleese, A. Glaese, J. Aslanides,\nM. Botvinick et al. , “Fine-tuning language models to find agreement\n8\namong humans with diverse preferences,” Advances in Neural Informa-\ntion Processing Systems , vol. 35, pp. 38 176–38 189, 2022.\n[32] J. Hu, “Neural language models and human linguistic knowledge,” Ph.D.\ndissertation, Massachusetts Institute of Technology, 2023.\n[33] B. van Aken, “Exploration and adaptation of large language models for\nspecialized domains,” 2023.\n[34] P.-S. Huang, H. Zhang, R. Jiang, R. Stanforth, J. Welbl, J. Rae, V . Maini,\nD. Yogatama, and P. Kohli, “Reducing sentiment bias in language models\nvia counterfactual evaluation,” arXiv preprint arXiv:1911.03064 , 2019.\n[35] S. R. Bowman and G. E. Dahl, “What will it take to fix benchmarking\nin natural language understanding?” arXiv preprint arXiv:2104.02145 ,\n2021.\n[36] P. Schramowski, C. Turan, N. Andersen, C. A. Rothkopf, and K. Ker-\nsting, “Large pre-trained language models contain human-like biases of\nwhat is right and wrong to do,” Nature Machine Intelligence , vol. 4,\nno. 3, pp. 258–268, 2022.\n[37] N. Lee, Y . Bang, H. Lovenia, S. Cahyawijaya, W. Dai, and P. Fung,\n“Survey of social bias in vision-language models,” arXiv preprint\narXiv:2309.14381, 2023.\n[38] V . Freiberger and E. Buchmann, “Fairness certification for natu-\nral language processing and large language models,” arXiv preprint\narXiv:2401.01262, 2024.\n[39] Y . Yao, J. Duan, K. Xu, Y . Cai, Z. Sun, and Y . Zhang, “A survey on\nlarge language model (llm) security and privacy: The good, the bad, and\nthe ugly,” High-Confidence Computing, p. 100211, 2024.\n[40] E. Nasarian, R. Alizadehsani, U. R. Acharya, and K.-L. Tsui, “Designing\ninterpretable ml system to build trust in healthcare: A systematic review\nof the last decade to proposed responsible clinician-ai-collaboration\nframework,” Available at SSRN 4732684 .\n[41] R. Schwartz, R. Schwartz, A. Vassilev, K. Greene, L. Perine, A. Burt,\nand P. Hall, Towards a standard for identifying and managing bias in\nartificial intelligence. US Department of Commerce, National Institute\nof Standards and Technology, 2022, vol. 3.\n[42] N. Ding, Y . Qin, G. Yang, F. Wei, Z. Yang, Y . Su, S. Hu, Y . Chen,\nC.-M. Chan, W. Chen et al. , “Parameter-efficient fine-tuning of large-\nscale pre-trained language models,” Nature Machine Intelligence, vol. 5,\nno. 3, pp. 220–235, 2023.\n[43] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper,\nZ. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti et al. , “Using\ndeepspeed and megatron to train megatron-turing nlg 530b, a large-scale\ngenerative language model,” arXiv preprint arXiv:2201.11990 , 2022.\n[44] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\nY . Zhao, Y . Lu et al. , “Ernie 3.0: Large-scale knowledge enhanced\npre-training for language understanding and generation,” arXiv preprint\narXiv:2107.02137, 2021.\n[45] Y . Yu, Y . Zhuang, J. Zhang, Y . Meng, A. J. Ratner, R. Krishna,\nJ. Shen, and C. Zhang, “Large language model as attributed training data\ngenerator: A tale of diversity and bias,” Advances in Neural Information\nProcessing Systems, vol. 36, 2024.\n[46] S. Yucer, S. Akc ¸ay, N. Al-Moubayed, and T. P. Breckon, “Exploring\nracial bias within face recognition via per-subject adversarially-enabled\ndata augmentation,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops , 2020, pp. 18–19.\n[47] D. Hooftman, “Using generative modelling to perform diversifying data\naugmentation,” Ph.D. dissertation, University of Amsterdam, 2023.\n[48] Y . Guo, Y . Yang, and A. Abbasi, “Auto-debias: Debiasing masked\nlanguage models with automated biased prompts,” in Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 2022, pp. 1012–1023.\n[49] P. P. Liang, C. Wu, L.-P. Morency, and R. Salakhutdinov, “Towards\nunderstanding and mitigating social biases in language models,” in\nInternational Conference on Machine Learning . PMLR, 2021, pp.\n6565–6576.\n[50] J. Stacey, P. Minervini, H. Dubossarsky, S. Riedel, and T. Rockt ¨aschel,\n“Avoiding the hypothesis-only bias in natural language inference via\nensemble adversarial training,” arXiv preprint arXiv:2004.07790 , 2020.\n[51] R. Le Bras, S. Swayamdipta, C. Bhagavatula, R. Zellers, M. Peters,\nA. Sabharwal, and Y . Choi, “Adversarial filters of dataset biases,” in\nInternational conference on machine learning . Pmlr, 2020, pp. 1078–\n1088.\n[52] R. Liu, C. Jia, J. Wei, G. Xu, L. Wang, and S. V osoughi, “Mitigating\npolitical bias in language models through reinforced calibration,” in\nProceedings of the AAAI Conference on Artificial Intelligence , vol. 35,\nno. 17, 2021, pp. 14 857–14 866.\n[53] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[54] B. Wang, W. Ping, C. Xiao, P. Xu, M. Patwary, M. Shoeybi, B. Li,\nA. Anandkumar, and B. Catanzaro, “Exploring the limits of domain-\nadaptive training for detoxifying large-scale language models,”Advances\nin Neural Information Processing Systems , vol. 35, pp. 35 811–35 824,\n2022.\n[55] A. Arslan, “A benchmark model for language models towards increased\ntransparency,” Current Opinion, vol. 2, no. 6, pp. 228–235, 2022.\n[56] J. P. Lalor, A. Abbasi, K. Oketch, Y . Yang, and N. Forsgren, “Should\nfairness be a metric or a model? a model-based framework for assessing\nbias in machine learning pipelines,” ACM Transactions on Information\nSystems, 2024.\n[57] T. P. Pagano, R. B. Loureiro, F. V . Lisboa, R. M. Peixoto, G. A.\nGuimar˜aes, G. O. Cruz, M. M. Araujo, L. L. Santos, M. A. Cruz,\nE. L. Oliveira et al., “Bias and unfairness in machine learning models: a\nsystematic review on datasets, tools, fairness metrics, and identification\nand mitigation methods,” Big data and cognitive computing, vol. 7, no. 1,\np. 15, 2023.\n[58] O. Parraga, M. D. More, C. M. Oliveira, N. S. Gavenski, L. S.\nKupssinsk¨u, A. Medronha, L. V . Moura, G. S. Sim˜oes, and R. C. Barros,\n“Fairness in deep learning: A survey on vision and language research,”\nACM Computing Surveys , 2023.\n[59] Y . Deldjoo, “Understanding biases in chatgpt-based recommender sys-\ntems: Provider fairness, temporal stability, and recency,” arXiv preprint\narXiv:2401.10545, 2024.\n[60] D. Demszky, D. Yang, D. S. Yeager, C. J. Bryan, M. Clapper, S. Chand-\nhok, J. C. Eichstaedt, C. Hecht, J. Jamieson, M. Johnson et al., “Using\nlarge language models in psychology,” Nature Reviews Psychology ,\nvol. 2, no. 11, pp. 688–701, 2023.\n[61] M. M. Manerba, K. Sta ´nczak, R. Guidotti, and I. Augenstein, “So-\ncial bias probing: Fairness benchmarking for language models,” arXiv\npreprint arXiv:2311.09090, 2023.\n[62] T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, and M. N. Halgamuge,\n“Inadequacies of large language model benchmarks in the era of gener-\native artificial intelligence,” arXiv preprint arXiv:2402.09880 , 2024.\n[63] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen,\nW. Peng, X. Feng, B. Qin et al. , “A survey on hallucination in large\nlanguage models: Principles, taxonomy, challenges, and open questions,”\narXiv preprint arXiv:2311.05232 , 2023.\n[64] L. Sun, Y . Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y . Huang, W. Lyu,\nY . Zhang, X. Li et al. , “Trustllm: Trustworthiness in large language\nmodels,” arXiv preprint arXiv:2401.05561 , 2024.\n[65] Z. Xi, W. Chen, X. Guo, W. He, Y . Ding, B. Hong, M. Zhang, J. Wang,\nS. Jin, E. Zhou et al., “The rise and potential of large language model\nbased agents: A survey,” arXiv preprint arXiv:2309.07864 , 2023.\n[66] L. Yan, L. Sha, L. Zhao, Y . Li, R. Martinez-Maldonado, G. Chen,\nX. Li, Y . Jin, and D. Gaˇsevi´c, “Practical and ethical challenges of large\nlanguage models in education: A systematic scoping review,” British\nJournal of Educational Technology , vol. 55, no. 1, pp. 90–112, 2024.\n[67] N. Chacko and V . Chacko, “Paradigm shift presented by large language\nmodels (llm) in deep learning,” ADVANCES IN EMERGING COMPUT-\nING TECHNOLOGIES, vol. 40, 2023.\n[68] M. Yurrita, D. Murray-Rust, A. Balayn, and A. Bozzon, “Towards a\nmulti-stakeholder value-based assessment framework for algorithmic\nsystems,” in Proceedings of the 2022 ACM Conference on Fairness,\nAccountability, and Transparency, 2022, pp. 535–563.\n[69] F. Bodria, F. Giannotti, R. Guidotti, F. Naretto, D. Pedreschi, and\nS. Rinzivillo, “Benchmarking and survey of explanation methods for\nblack box models,” Data Mining and Knowledge Discovery , vol. 37,\nno. 5, pp. 1719–1778, 2023.\n[70] K. Martin, A. Liret, N. Wiratunga, G. Owusu, and M. Kern, “Eval-\nuating explainability methods intended for multiple stakeholders,” KI-\nK¨unstliche intelligenz, vol. 35, no. 3, pp. 397–411, 2021.\n[71] S. R. Hong, J. Hullman, and E. Bertini, “Human factors in model\ninterpretability: Industry practices, challenges, and needs,” Proceedings\nof the ACM on Human-Computer Interaction , vol. 4, no. CSCW1, pp.\n1–26, 2020.\n[72] K.-J. Tokayev, “Ethical implications of large language models a multi-\ndimensional exploration of societal, economic, and technical concerns,”\nInternational Journal of Social Analytics, vol. 8, no. 9, pp. 17–33, 2023.\n[73] J. M ¨okander, J. Schuett, H. R. Kirk, and L. Floridi, “Auditing large\nlanguage models: a three-layered approach,” AI and Ethics , pp. 1–31,\n2023.\n[74] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von\nArx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al. ,\n9\n“On the opportunities and risks of foundation models,” arXiv preprint\narXiv:2108.07258, 2021.\n[75] M. A. K. Raiaan, M. S. H. Mukta, K. Fatema, N. M. Fahad, S. Sakib,\nM. M. J. Mim, J. Ahmad, M. E. Ali, and S. Azam, “A review on large\nlanguage models: Architectures, applications, taxonomies, open issues\nand challenges,” IEEE Access, 2024.\n[76] O. Ozmen Garibay, B. Winslow, S. Andolina, M. Antona, A. Boden-\nschatz, C. Coursaris, G. Falco, S. M. Fiore, I. Garibay, K. Grieman\net al. , “Six human-centered artificial intelligence grand challenges,”\nInternational Journal of Human–Computer Interaction , vol. 39, no. 3,\npp. 391–437, 2023.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5175127983093262
    },
    {
      "name": "Econometrics",
      "score": 0.33978164196014404
    },
    {
      "name": "Economics",
      "score": 0.23323208093643188
    }
  ]
}