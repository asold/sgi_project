{
  "title": "AdaViT: Adaptive Vision Transformers for Efficient Image Recognition",
  "url": "https://openalex.org/W3215986121",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5045285009",
      "name": "Lingchen Meng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5024600500",
      "name": "Hengduo Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5070693161",
      "name": "Bor-Chun Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5017304880",
      "name": "Shiyi Lan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5026167547",
      "name": "Zuxuan Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5047962986",
      "name": "Yu–Gang Jiang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5113969216",
      "name": "Ser-Nam Lim",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3170188883",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W2548228487",
    "https://openalex.org/W3212756788",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W2994721156",
    "https://openalex.org/W3202053489",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2312609093",
    "https://openalex.org/W3204575409",
    "https://openalex.org/W2963393494",
    "https://openalex.org/W3176258108",
    "https://openalex.org/W3202511134",
    "https://openalex.org/W3035424951",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W3203606893",
    "https://openalex.org/W3213928300",
    "https://openalex.org/W2962944050",
    "https://openalex.org/W2990631821",
    "https://openalex.org/W3035296770",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3166470370",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3203166921",
    "https://openalex.org/W2767421475",
    "https://openalex.org/W2884751099",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2981812042",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2562731582",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W2592790041",
    "https://openalex.org/W2752037867",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3214395992",
    "https://openalex.org/W3204419213",
    "https://openalex.org/W3107849462",
    "https://openalex.org/W3204896784",
    "https://openalex.org/W3128099838"
  ],
  "abstract": "Built on top of self-attention mechanisms, vision transformers have demonstrated remarkable performance on a variety of vision tasks recently. While achieving excellent performance, they still require relatively intensive computational cost that scales up drastically as the numbers of patches, self-attention heads and transformer blocks increase. In this paper, we argue that due to the large variations among images, their need for modeling long-range dependencies between patches differ. To this end, we introduce AdaViT, an adaptive computation framework that learns to derive usage policies on which patches, self-attention heads and transformer blocks to use throughout the backbone on a per-input basis, aiming to improve inference efficiency of vision transformers with a minimal drop of accuracy for image recognition. Optimized jointly with a transformer backbone in an end-to-end manner, a light-weight decision network is attached to the backbone to produce decisions on-the-fly. Extensive experiments on ImageNet demonstrate that our method obtains more than 2x improvement on efficiency compared to state-of-the-art vision transformers with only 0.8% drop of accuracy, achieving good efficiency/accuracy trade-offs conditioned on different computational budgets. We further conduct quantitative and qualitative analysis on learned usage polices and provide more insights on the redundancy in vision transformers.",
  "full_text": "AdaViT: Adaptive Vision Transformers for Efﬁcient Image Recognition\nLingchen Meng1* Hengduo Li2* Bor-Chun Chen3 Shiyi Lan2\nZuxuan Wu1† Yu-Gang Jiang1† Ser-Nam Lim3\n1Fudan University 2University of Maryland 3Meta AI\nAbstract\nBuilt on top of self-attention mechanisms, vision trans-\nformers have demonstrated remarkable performance on a\nvariety of vision tasks recently. While achieving excellent\nperformance, they still require relatively intensive compu-\ntational cost that scales up drastically as the numbers of\npatches, self-attention heads and transformer blocks in-\ncrease. In this paper, we argue that due to the large varia-\ntions among images, their need for modeling long-range de-\npendencies between patches differ. To this end, we introduce\nAdaViT, an adaptive computation framework that learns\nto derive usage policies on which patches, self-attention\nheads and transformer blocks to use throughout the back-\nbone on a per-input basis, aiming to improve inference efﬁ-\nciency of vision transformers with a minimal drop of accu-\nracy for image recognition. Optimized jointly with a trans-\nformer backbone in an end-to-end manner, a light-weight\ndecision network is attached to the backbone to produce\ndecisions on-the-ﬂy. Extensive experiments on ImageNet\ndemonstrate that our method obtains more than 2×im-\nprovement on efﬁciency compared to state-of-the-art vision\ntransformers with only 0.8% drop of accuracy, achieving\ngood efﬁciency/accuracy trade-offs conditioned on differ-\nent computational budgets. We further conduct quantitative\nand qualitative analysis on learned usage polices and pro-\nvide more insights on the redundancy in vision transform-\ners.\n1. Introduction\nTransformers [39], the dominant architectures for a vari-\nety of natural language processing (NLP) tasks, have been\nattracting an ever-increasing research interest in the com-\nputer vision community since the success of the Vision\nTransformer (ViT) [7]. Built on top of self-attention mech-\nanisms, transformers are capable of capturing long-range\ndependencies among pixels/patches from input images ef-\nfectively, which is arguably one of the main reasons that\n*Equal contributions.\n†Corresponding authors.\nHead\nHead\nHead\nHead\nHead\nConcat\nHead\nMSA FFN\nMSA FFN\n MSA FFN\nMSA FFN\n MSA FFN\nMSA FFN\nMSA\nMSA\nHead\nConcat\n“Barbershop”\n“White Stork”\nHead\nHead\nHead\nHead\nHead\nFigure 1. A conceptual overview of our method. Exploiting\nthe redundancy in vision transformers, AdaViT learns to produce\ninstance-speciﬁc usage policies on which patches, self-attention\nheads and transformer blocks to keep/activate throughout the net-\nwork for efﬁcient image recognition. Fewer computational re-\nsources are allocated for easy samples (top) while more are used\nfor hard samples (bottom), reducing the overall computational cost\nwith a minimal drop of classiﬁcation accuracy. Green patches are\nactivated in both ﬁgures.\nthey outperform standard CNNs in vision tasks spanning\nfrom image classiﬁcation [4, 12, 19, 21, 37, 45, 50] to object\ndetection [3, 5, 21, 41], action recognition [9, 22, 52] and so\nforth.\nRecent studies on vision transformers [4, 7, 37, 50] typ-\nically adopt the Transformer [39] architecture from NLP\nwith minimal surgery. Taking a sequence of sliced im-\nage patches analogous to tokens/words as inputs, the trans-\nformer backbone consists of stacked building blocks with\ntwo sublayers, i.e. a self-attention layer and a feed-forward\nnetwork. To ensure that the model can attend to information\n1\narXiv:2111.15668v1  [cs.CV]  30 Nov 2021\nfrom different representation subspaces jointly, multi-head\nattention is used in each block instead of a single attention\nfunction [39]. While these self-attention-based vision trans-\nformers have outperformed CNNs on a multitude of bench-\nmarks like ImageNet [6], the competitive performance does\nnot come for free—the computational cost of the stacked\nattention blocks with multiple heads is large, which further\ngrows quadratically with the number of patches.\nBut are all patches needed to be attended to throughout\nthe network for correctly classifying images? Do we need\nall the self-attention blocks with multiple heads to look for\nwhere to attend to and model the underlying dependencies\nfor all different images? After all, large variations exist\nin images such as object shape, object size, occlusion and\nbackground complexity. Intuitively, more patches and self-\nattention blocks are required for complex images contain-\ning cluttered background or occluded objects, which require\nsufﬁcient contextual information and understanding of the\nwhole image so as to infer their ground-truth classes ( e.g.\nthe barber shop in Figure 1), while only a small number of\ninformative patches and attention heads/blocks are enough\nto classify easy images correctly.\nWith this in mind, we seek to develop an adaptive com-\nputation framework that learns which patches to use and\nwhich self-attention heads/blocks to activate on a per-input\nbasis. By doing so, the computational cost of vision trans-\nformers can be saved through discarding redundant input\npatches and backbone network layers for easy samples,\nand only using full model with all patches for hard and\ncomplex samples. This is an orthogonal and complemen-\ntary direction to recent approaches on efﬁcient vision trans-\nformers that focus on designing static network architec-\ntures [4, 11, 21, 50].\nTo this end, we introduce Adaptive Vision Transformer\n(AdaViT), an end-to-end framework that adaptively deter-\nmines the usage of patches, heads and layers of vision trans-\nformers conditioned on input images for efﬁcient image\nclassiﬁcation. Our framework learns to derive instance-\nspeciﬁc inference strategies on: 1) which patches to keep;\n2) which self-attention heads to activate; and 3) which\ntransformer blocks to skip for each image, to improve the\ninference efﬁciency with a minimal drop of classiﬁcation\naccuracy. In particular, we insert a light-weight multi-\nhead subnetwork ( i.e. a decision network) to each trans-\nformer block of the backbone network, which learns to pre-\ndict binary decisions on the usage of patch embeddings,\nself-attention heads and blocks throughout the network.\nSince binary decisions are non-differentiable, we resort to\nGumbel-Softmax [25] during training to make the whole\nframework end-to-end trainable. The decision network is\njointly optimized with the transformer backbone with a us-\nage loss that measures the computational cost of the pro-\nduced usage policies and a normal cross-entropy loss, which\nincentivizes the network to produce policies that reduce the\ncomputational cost while maintaining classiﬁcation accu-\nracy. The overall target computational cost can be con-\ntrolled by hyperparameter γ ∈(0,1] corresponding to the\npercentage of computational cost of the full model with\nall patches as input during training, making the framework\nﬂexible to suit the need of different computational budgets.\nWe conduct extensive experiments on ImageNet [6] to\nvalidate the effectiveness of AdaViT and show that our\nmethod is able to improve the inference efﬁciency of vision\ntransformers by more than 2×with only 0.8% drop of clas-\nsiﬁcation accuracy, achieving good trade-offs between efﬁ-\nciency and accuracy when compared with other standard vi-\nsion transformers and CNNs. In addition, we conduct quan-\ntitative and qualitative analyses on the learned usage poli-\ncies, providing more intuitions and insights on the redun-\ndancy in vision transformers. We further show visualiza-\ntions and demonstrate that AdaViT learns to use more com-\nputation for relatively hard samples with complex scenes,\nand less for easy object-centric samples.\n2. Related Work\nVision Transformers. Inspired by its great success in\nNLP tasks, many recent studies have explored adapting the\nTransformer [39] architecture to multiple computer vision\ntasks [7–9, 14, 21, 26, 30, 31, 44, 48]. Following ViT [7], a\nvariety of vision transformer variants have been proposed\nto improve the recognition performance as well as training\nand inference efﬁciency. DeiT [37] incorporates distilla-\ntion strategies to improve training efﬁciency of vision trans-\nformers, outperforming standard CNNs without pretrain-\ning on large-scale dataset like JFT [34]. Other approaches\nlike T2T-ViT [50], Swin Transformer [21], PVT [41] and\nCrossViT [4] seek to improve the network architecture of\nvision transformers. Efforts have also been made to intro-\nduce the advantages of 2D CNNs to transformers through\nusing convolutional layers [19, 47], hierarchical network\nstructures [21, 22,41], multi-scale feature aggregation [4, 9]\nand so on. While obtaining superior performance, the com-\nputational cost of vision transformers is still intensive and\nscales up quickly as the numbers of patches, self-attention\nheads and transformer blocks increase.\nEfﬁcient Networks. Extensive studies have been con-\nducted to improve the efﬁciency of CNNs for vision tasks\nthrough designing effective light-weight network architec-\ntures like MobileNets [15, 16, 33], EfﬁcientNets [36] and\nShufﬂeNets [24, 51]. To match the inference efﬁciency\nof standard CNNs, recent work has also explored de-\nveloping efﬁcient vision transformer architectures. T2T-\nViT [50] proposes to use a deep-narrow structure and a\ntoken-to-token module, achieving better accuracy and less\ncomputational cost than ViT [7]. LeViT [11] and Swin\nTransformer [21] develop multi-stage network architectures\n2\nwith down-sampling and obtain better inference efﬁciency.\nThese methods, however, use a ﬁxed network architecture\nfor all input samples regardless of the redundancy in patches\nand network architecture for easy samples. Our work is\northogonal to this direction and focuses on learning input-\nspeciﬁc strategies that adaptively allocate computational re-\nsources for saved computation and a minimal drop in accu-\nracy at the same time.\nAdaptive Computation. Adaptive computation methods\nexploit the large variations within network inputs as well\nas the redundancy in network architectures to improve efﬁ-\nciency with instance-speciﬁc inference strategies. In par-\nticular, existing methods for CNNs have explored alter-\ning input resolution [27, 28, 38, 49], skipping network lay-\ners [10, 40, 42, 46] and channels [1, 20], early exiting with\na multi-classiﬁer structure [2, 17, 18], to name a few. A\nfew attempts have also been made recently to accelerate vi-\nsion transformers with adaptive inference policies exploit-\ning the redundancy in patches, i.e. producing policies on\nwhat patch size [43] and which patches [29, 32] to use con-\nditioned on input image. In contrast, we exploit the redun-\ndancy in the attention mechanism of vision transformer and\npropose to improve efﬁciency by adaptively choosing which\nself-attention heads, transformer blocks and patch embed-\ndings to keep/drop conditioned on the input samples.\n3. Approach\nWe propose AdaViT, an end-to-end adaptive computa-\ntion framework to reduce the computational cost of vision\ntransformers. Given an input image, AdaViT learns to adap-\ntively derive policies on whichpatches, self-attention heads\nand transformer blocks to use or activate in the transformer\nbackbone conditioned on the input image, encouraging us-\ning less computation while maintaining the classiﬁcation\naccuracy. An overview of our method is shown in Figure 2.\nIn this section, we ﬁrst give a brief introduction of vision\ntransformers in Sec. 3.1. We then present our proposed\nmethod in Sec. 3.2 and elaborate the optimization function\nof the framework in Sec. 3.3.\n3.1. Preliminaries\nVision transformers [7, 37, 50] for image classiﬁcation\ntake a sequence of sliced patches from image as input, and\nmodel their long-range dependencies with stacked multi-\nhead self-attention layers and feed-forward networks*. For-\nmally, for an input image I, it is ﬁrst split into a sequence\nof ﬁxed-size 2D patches X = [x1,x2,..., xN ] where N is\nthe number of patches ( e.g. N = 14 ×14). These raw\npatches are then mapped into D-dimensional patch embed-\ndings Z = [z1,z2,..., zN ] with a linear layer. A learnable\n*In this section we consider the architecture of ViT [7], and extend it\nto other variants of vision transformers is straightforward.\nembedding zcls termed class token is appended to the se-\nquence of patch embeddings, which serves as the represen-\ntation of image. Positional embeddings Epos are also op-\ntionally added to patch embeddings to augment them with\npositional information. To summarize, the input to the ﬁrst\ntransformer block is:\nZ = [zcls; z1; z2; ...; zN ] +Epos (1)\nwhere z ∈RD and Epos ∈R(N+1)×D respectively.\nSimilar to Transformers [39] in NLP, the backbone net-\nwork of vision transformers consist of L blocks, each of\nwhich consists of a multi-head self-attention layer (MSA)\nand a feed-forward network (FFN). In particular, a single-\nhead attention is computed as below:\nAttn(Q,K,V ) =softmax(QKT\n√dk\n)V (2)\nwhere Q,K,V are—in a broad sense—query, key and value\nmatrices respectively, and dk is a scaling factor. For vision\ntransformers, Q,K,V are projected from the same input,\ni.e. patch embeddings. For more effective attention on dif-\nferent representation subspaces, multi-head self-attention\nconcatenates the output from several single-head attentions\nand projects it with another parameter matrix:\nheadi,l = Attn(ZlWQ\ni,l, ZlWK\ni,l, ZlWV\ni,l) (3)\nMSA(Zl) =Concat(head1,l,..., headH,l)WO\nl , (4)\nwhere WQ\ni,l,WK\ni,l,WV\ni,l,WO\nl are the parameter matrices in\nthe i-th attention head of the l-th transformer block, and Zl\ndenotes the input at the l-th block. The output from MSA is\nthen fed into FFN, a two-layer MLP, and produce the output\nof the transformer block Zl+1. Residual connections are\nalso applied on both MSA and FFN as follows:\nZ′\nl = MSA(Zl) +Zl, Zl+1 = FFN(Z′\nl) +Z′\nl (5)\nThe ﬁnal prediction is produced by a linear layer taking the\nclass token from last transformer block (Z0\nL) as inputs.\n3.2. Adaptive Vision Transformer\nWhile large vision transformer models have achieved su-\nperior image classiﬁcation performance, the computational\ncost grows quickly as we increase the numbers of patches,\nattention heads and transformer blocks to obtain higher ac-\ncuracies. In addition, a computationally expensive one-\nsize-ﬁt-all network is often an overkill for many easy sam-\nples. To remedy this, AdaViT learns to adaptively choose\n1) which patch embeddings to use; 2) which self-attention\nheads in MSA to activate; and 3) which transformer block\nto skip—on a per-input basis—to improve the inference ef-\nﬁciency of vision transformers. We achieve this by inserting\n3\n...Multi-Head\nAttention\nFeed-Forward\nNetwork\nDecision \nNetwork\nMulti-Head\nAttention\nFeed-Forward\nNetwork\nDecision \nNetwork\nMulti-Head\nAttention\nFeed-Forward\nNetwork\nDecision \nNetwork\nMagpie\nPatch Selection Head Selection Block Selection\nMSA\n FFN\nHead\nHead\nHead\nHead\nHead\nHead\nHead\nHead\nHead\nHead\nHead\nHead Concat\nFigure 2. An overview of our approach. We insert a light-weight decision network before each block of the vision transformer backbone.\nGiven an input image, the decision networks produce usage policies on which patches, self-attention heads and transformer blocks to\nkeep/activate throughout the backbone. These instance-speciﬁc usage policies are incentivized to reduce the overall computational cost of\nvision transformers with minimal drop of accuracy. See texts for more details.\na light-weight decision network before each of the trans-\nformer blocks, and it is trained to produce the three sets of\nusage policies for this block.\nDecision Network. The decision network at l-th block\nconsists of three linear layers with parameters Wl =\n{Wp\nl ,Wh\nl ,Wb\nl }to produce computation usage policies for\npatch selection , attention head selection and transformer\nblock selection respectively. Formally, given the input to\nl-th block Zl, the usage policy matrices for this block is\ncomputed as follows:\n(mp\nl ,mh\nl ,mb\nl ) =(Wp\nl ,Wh\nl ,Wb\nl )Zl\ns.t. mp\nl ∈RN , mh\nl ∈RH, mb\nl ∈R (6)\nwhere N and H denote the numbers of patches and self-\nattention heads in a transformer block, and l∈[1,L]. Each\nentry of mp\nl , mh\nl and mb\nl is further passed to a sigmoid\nfunction, indicating the probability of keeping the corre-\nsponding patch, attention head and transformer block re-\nspectively. The l-th decision network shares the output from\nprevious l−1 transformer blocks, making the framework\nmore efﬁcient than using a standalone decision network.\nAs the decisions are binary, the action of keeping / dis-\ncarding can be selected by simply applying a threshold on\nthe entries during inference. However, deriving the optimal\nthresholds for different samples is challenging. To this end,\nwe deﬁne random variables Mp\nl , Mh\nl , Mb\nl to make deci-\nsions by sampling from mp\nl , mh\nl and mb\nl . For example, the\nj-th patch embedding in l-th block is kept when Mp\nl,j = 1,\nand dropped when Mp\nl,j = 0. We relax the sampling pro-\ncess with Gumbel-Softmax trick [25] to make it differen-\ntiable during training, which will be further elaborated in\nSec. 3.3.\nPatch Selection. For the input to each transformer block,\nwe aim at keeping only the most informative patch embed-\ndings and discard the rest to speedup inference. More for-\nmally, forl-th block, the patches are removed from the input\nto this block if the corresponding entries in Mp\ni equal to 0:\nZl = [zl,cls; Mp\nl,1z1; ...; Mp\nl,N zN ] (7)\nThe class token zl,cls is always kept since it is used as rep-\nresentation of the whole image.\nHead Selection. Multi-head self attention enables the\nmodel to attend to different subspaces of the representation\njointly [39] and is adopted in most, if not all, vision trans-\nformer variants [4,7,21,37,50]. Such a multi-head design is\ncrucial to model the underlying long-range dependencies in\nimages especially those with complex scenes and cluttered\nbackground, but fewer attention heads could arguably suf-\nﬁce to look for where to attend to in easy images. With this\nin mind, we explore dropping attention heads adaptively\nconditioned on input image for faster inference. Similar to\npatch selection, the decision of activating or deactivating\ncertain attention head is determined by the corresponding\nentry in Mh\nl . The “deactivation” of an attention head can be\ninstantiated in different ways. In our framework, we explore\ntwo methods for head selection, namely partial deactivation\nand full deactivation.\nFor partial deactivation, the softmax output in attention\nas in Eqn. 2 is replaced with predeﬁned ones like an (N +\n1,N + 1)identity matrix 1, such that the cost of computing\nattention map is saved. The attention in i-th head of l-th\n4\nblock is then computed as:\nAttn(Q,K,V )l,i =\n{\nsoftmax(QKT\n√dk\n) ·V if Mh\nl,i = 1\n1 ·V if Mh\nl,i = 0\n(8)\nFor full deactivation , the entire head is removed from\nthe multi-head self attention layer, and the embedding size\nof the output from MSA is reduced correspondingly:\nMSA(Zl)l,i = Concat([headl,i:1 )H if Mh\nl,i = 1])WO′\nl\n(9)\nIn practice, full deactivation saves more computation com-\npared with partial deactivation when same percentage of\nheads are deactivated , yet is likely to incur more classiﬁca-\ntion errors as the embedding size is manipulated on-the-ﬂy.\nBlock Selection. In addition to patch selection and head se-\nlection, a transformer block can also be favourably skipped\nentirely when it is redundant, by virtue of the residual con-\nnections throughout the network. To increase the ﬂexibility\nof layer skipping, we increase the dimension of block usage\npolicy matrix mb\nl from 1 to 2, enabling the two sublayers\n(MSA and FFN) in each transformer block to be controlled\nindividually. Eqn. 5 then becomes:\nZ′\nl = Mb\nl,0 ·MSA(Zl) +Zl\nZl+1 = Mb\nl,1 ·FFN(Z′\nl) +Z′\nl (10)\nIn summary, given the input of each transformer block,\nthe decision network produces the usage policies for this\nblock, and then the input is forwarded through the block\nwith the decisions applied. Finally, the classiﬁcation pre-\ndiction from the last layer and the decisions for all blocks\nM = {Mp\nl , Mh\nl , Mb\nl ,for l: 1) L}are obtained.\n3.3. Objective Function\nSince our goal is to reduce the overall computational cost\nof vision transformers with a minimal drop in accuracy, the\nobjective function of AdaViT is designed to incentivize cor-\nrect classiﬁcation and less computation at the same time. In\nparticular, a usage loss and a cross-entropy loss are used\nto jointly optimize the framework. Given an input image I\nwith a label y, the ﬁnal prediction is produced by the trans-\nformer F with parameters θ, and the cross-entropy loss is\ncomputed as follows:\nLce = −ylog(F(I; θ)) (11)\nWhile the binary decisions on whether to keep/discard\na patch/head/block can be readily obtained through apply-\ning a threshold during inference, determining the optimal\nthresholds is challenging. In addition, such an operation is\nnot differentiable during training and thus makes the opti-\nmization of decision network challenging. A common so-\nlution is to resort to reinforcement learning and optimize\nthe network with policy gradient methods [35], yet it can be\nslow to converge due to the large variance that scales with\nthe dimension of discrete variables [25,35]. To this end, we\nuse the Gumbel-Softmax trick [25] to relax the sampling\nprocess and make it differentiable. Formally, the decision\nat i-th entry of m is derived in the following way:\nMi,k = exp(log(mi,k + Gi,k)/τ)∑K\nj=1 exp(log(mi,j + Gi,j)/τ)\nfor k= 1,2,...,K (12)\nwhere K is the total number of categories ( K = 2 for bi-\nnary decision in our case), and Gi = −log(−log(Ui))\nis the Gumbel distribution in which Ui is sampled from\nUniform(0,1), an i.i.d uniform distribution. Temperature\nτ is used to control the smoothness of Mi.\nTo encourage reducing the overall computational cost,\nwe devise the usage loss as follows:\nLusage = ( 1\nDp\nDp∑\nd=1\nMp\nd −γp)2 + ( 1\nDh\nDh∑\nd=1\nMh\nd −γh)2\n+ ( 1\nDb\nDb∑\nd=1\nMb\nd −γb)2\nwhere Dp = L×N, Dh = L×H, Db = L×2 (13)\nHere Dp,Dh,Db denote the sizes of ﬂattened probability\nvectors from the decision network for patch/head/block se-\nlection, i.e. the total numbers of patches, heads and blocks\nof the entire transformer respectively. The hyperparameters\nγp,γh,γb ∈(0,1] indicate target computation budgets in\nterms of the percentage of patches/heads/blocks to keep.\nmin\nθ,W\nL= Lce + Lusage (14)\nFinally, the two loss functions are combined and minimized\nin an end-to-end manner as in Eqn. 14.\n4. Experiment\n4.1. Experimental Setup\nDataset and evaluation metrics. We conduct experiments\non ImageNet [6] with ∼1.2M images for training and 50K\nimages for validation, and report the Top-1 classiﬁcation ac-\ncuracy. To evaluate model efﬁciency, we report the number\nof giga ﬂoating-point operations (GFLOPs) per image.\nImplementation details. We use T2T-ViT [50] as the trans-\nformer backbone due to its superior performance on Ima-\ngeNet with a moderate computational cost. The backbone\nconsists of L = 19blocks and H = 7heads in each MSA\n5\nMethod Top-1 Acc (%) FLOPs (G) Image Size # Patch # Head # Block\nResNet-50* [13, 50] 79.1 4.1 224 ×224 - - -\nResNet-101* [13, 50] 79.9 7.9 224 ×224 - - -\nViT-S/16 [7] 78.1 10.1 224 ×224 196 12 8\nDeiT-S [37] 79.9 4.6 224 ×224 196 6 12\nPVT-Small [41] 79.8 3.8 224 ×224 - - 15\nSwin-T [21] 81.3 4.5 224 ×224 - - 12\nT2T-ViT-19 [50] 81.9 8.5 224 ×224 196 7 19\nCrossViT-15 [4] 81.5 5.8 224 ×224 196 6 15\nLocalViT-S [19] 80.8 4.6 224 ×224 196 6 12\nBaseline Upperbound 81.9 8.5 224 ×224 196 7 19\nBaseline Random 33.0 4.0 224 ×224 ∼118 ∼5.6 ∼16.2\nBaseline Random+ 71.5 3.9 224 ×224 ∼121 ∼5.6 ∼16.2\nAdaViT (Ours) 81.1 3.9 224 ×224 ∼95 ∼4.5 ∼15.5\nTable 1. Main Results. We compare AdaViT with various standard CNNs and vision transformers, as well as baselines including Upper-\nbound, Random and Random+. * denotes training ResNets with our recipe following [50].\nlayer, and the number of tokens N = 196. The decision\nnetwork is attached to each transformer block starting from\n2-nd block. For head selection, we use the full deactivation\nmethod if not mentioned otherwise. We initialize the trans-\nformer backbone of AdaViT with the pretrained weights re-\nleased in the ofﬁcial implementation of [50]. We will re-\nlease the code.\nWe use 8 GPUs with a batch size 512 for training. The\nmodel is trained with a learning rate0.0005, a weight decay\n0.065 and a cosine learning rate schedule for 150 epochs\nfollowing [50]. AdamW [23] is used as the optimizer. For\nall the experiments, we set the input size to224×224. Tem-\nperature τ in Gumbel-Softmax is set to 5.0. The choices of\nγp,γh,γb vary ﬂexibly for different desired trade-offs be-\ntween classiﬁcation accuracy and computational cost.\n4.2. Main Results\nWe ﬁrst evaluate the overall performance of AdaViT\nin terms of classiﬁcation accuracy and efﬁciency, and re-\nport the results in Table 1. Besides standard CNN and\ntransformer architectures such as ResNets [13], ViT [7],\nDeiT [37], T2T-ViT [50] and so on, we also compare our\nmethod with the following baseline methods:\n• Upperbound: The original pretrained vision trans-\nformer model, with all patch embeddings kept as input\nand all self-attention heads and transformer blocks ac-\ntivated. This serves as an “upperbound” of our method\nregarding classiﬁcation accuracy.\n• Random: Given the usage policies produced by\nAdaVit, we generate random policies on patch selec-\ntion, head selection and block selection that use simi-\nlar computational cost and apply them to the pretrained\nmodels to validate the effectiveness of learned policies.\n• Random+: The pretrained models are further ﬁnetuned\nwith the random policies applied, in order to adapt to\nthe varied input distribution and network architecture\nincurred by the random policies.\nAs shown in Table 1, AdaViT is able to obtain good ef-\nﬁciency improvement with only a small drop on classiﬁ-\ncation accuracy. Speciﬁcally, AdaViT obtains 81.1% Top-\n1 accuracy requiring 3.9 GFLOPs per image during in-\nference, achieving more than 2×efﬁciency than the orig-\ninal T2T-ViT model with only ∼ 0.8% drop of accu-\nracy. Compared with standard ResNets [13] and vision\ntransformers that use a similar backbone architecture of\nours [4, 7, 37, 50], AdaViT obtains better classiﬁcation per-\nformance with less computational cost, achieving a good ef-\nﬁciency/accuracy trade-off as further shown in Figure 3. It\nis also worth pointing out that compared with vision trans-\nformer variants [21, 41] which resort to advanced design\n1 2 3 4 5 6 7 8 9\nGFLOPs\n70\n72\n74\n76\n78\n80\n82Top-1 Acc %\nOurs\nT2T-ViT\nDeiT\nCrossViT\nResNets\nLocalViT\nFigure 3. Tradeoff between efﬁciency and accuracy. AdaViT\nobtains good efﬁciency/accuracy tradeoffs compared with other\nstatic vision transformers.\n6\n2.0 2.5 3.0 3.5 4.0\nGFLOPs\n60\n65\n70\n75\n80Top-1 Acc %\n(a) Overall\nOurs\nRandom+\n0.5 0.6 0.7 0.8\n% Patches Kept\n74\n76\n78\n80\n82\n(b) Patch Only\nOurs\nRandom+\n0.5 0.6 0.7 0.8\n% Heads Kept\n74\n76\n78\n80\n82\n(c) Head Only\nOurs\nRandom+\n0.5 0.6 0.7 0.8\n% Blocks Kept\n74\n76\n78\n80\n82\n(d) Block Only\nOurs\nRandom+\nFigure 4. Effectiveness of each component. Efﬁciency/Accuracy tradeoffs of AdaViT with (a) all three selection methods; (b) patch\nselection; (c) head selection; (d) block selection and their Random+ counterparts.\nchoices like multi-scale feature pyramid and hierarchical\ndownsampling, our method still obtains comparable or bet-\nter accuracy under similar computational cost.\nWhen using a similar computation budget, AdaViT out-\nperforms random and random+ baselines by clear margins.\nSpeciﬁcally, Ada-ViT with T2T-ViT as the backbone net-\nwork obtains 48.1% and 9.6% higher accuracy thanrandom\nand random+ respectively at a similar cost of 3.9 GFLOPs\nper image, demonstrating that the usage policies learned by\nAdaViT can effectively maintain classiﬁcation accuracy and\nreduce computational cost at the same time.\nAdaViT with different computational budgets. AdaViT\nis designed to accommodate the need of different compu-\ntational budgets ﬂexibly by varying the hyperparameters\nγp,γh and γb as discussed in Section 3.2. As demonstrated\nin Figure 4(a), AdaViT is able to cover a wide range of\ntradeoffs between efﬁciency and accuracy, and outperforms\nRandom+ baselines by a large margin.\nRandom\nPatch\nRandom\nHead\nRandom\nBlock\nTop-1\nAccuracy\n✓ 49.2\n✓ 57.4\n✓ 64.7\nFull AdaViT 81.1\nTable 2. Effectiveness of learned usage policies.We replace each\nset of policies with randomly generated policies and compare with\nour method in its entirety.\nMethod Top-1 Acc % Head GFLOPs\nUpperbound 81.9 100% 8.5\nPartial 81.7 50% 6.9\nFull 80.3 50% 5.1\nFull 80.8 60% 5.8\nFull 81.1 70% 6.6\nTable 3. Partial vs. Full deactivation for head selection.\n4.3. Ablation Study\nEffectiveness of learned usage policies. Here we validate\nthat each of the three sets of learned usage policies is able\nto effectively maintain the classiﬁcation accuracy while re-\nducing the computational cost of vision transformers. For\nthis purpose, we replace the learned usage policies with ran-\ndomly generated policies that cost similar computational re-\nsources and report the results in Table 2. As shown in Ta-\nble 2, changing any set of learned policies to a random one\nresults in a drop of accuracy by a clear margin. Compared\nwith random patch/head/block selection, AdaViT obtains\n31.9%/23.7%/16.4% higher accuracy under similar com-\nputational budget. This conﬁrms the effectiveness of each\nlearned usage policy.\nAblation of individual components. Having demonstrated\nthe effectiveness of the jointly learned usage policies for\npatch, head and block selection, we now evaluate the per-\nformance when only one of the three selection methods is\nused. It is arguable that part of the performance gap in Ta-\nble 2 results from the change of input/feature distribution\nwhen random policies are applied, and thus we compare\neach component with its further ﬁnetuned Random+ coun-\nterparts. For faster training and evaluation, we train these\nmodels for 100 epochs. As shown in Figure 4(b-d), our\nmethod with only patch/head/block selection is also able to\ncover a wide range of accuracy/efﬁciency tradeoffs and out-\nperforms Random+ baselines by a clear margin, conﬁrming\nthe effectiveness of each component.\nPartial vs. Full deactivation for head selection. As dis-\ncussed in Sec. 3.2, we propose two methods to deacti-\nvate a head in the multi-head self-attention layer, namely\npartial deactivation and full deactivation. We now ana-\nlyze their effectiveness on improving the efﬁciency of vi-\nsion transformers. As demonstrated in Table 3, when de-\nactivating the same percentage ( i.e. 50%) of self-attention\nheads within the backbone, partial deactivation is able to\nobtain much higher accuracy than full deactivation ( 81.7%\nvs. 80.3%), but also incurs higher computational cost ( 6.9\nvs. 5.1 GFLOPs). This is intuitive since partial deactiva-\n7\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\n0.0\n0.5\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\n0.0\n0.5\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\n0.0\n0.5\n% Token (Top) % Head (Middle) % Block (Bottom)\nIndex of Transformer Block\n% Computational Cost\nFigure 5. Computational cost throughout the network. The\npercentages of kept/activated patches ( top), heads ( middle) and\nblocks (bottom) throughout the backbone are reported.\nmagpie\n parachute\n spatula\ntennis ball\n pedestal\n remote\ndrum\n lab coat\ndesktop\nscrewdriver\n toy shop\nvase\nLeast Computation \n Most Computation \nFigure 6. Qualitative results. Images allocated with the least and\nthe most computational resources by AdaViT are shown.\ntion only skips the computation of attention maps before\nSoftmax, while full deactivation removes the entire head\nand its output to the FFN. As the number of heads increases,\nfull deactivation obtains better accuracy gradually. In prac-\ntice these different head selection methods provide more\nﬂexible options to suit different computational budgets.\n4.4. Analysis\nComputational saving throughout the network. AdaViT\nexploits the redundancy of computation to improve the efﬁ-\nciency of vision transformers. To better understand such re-\ndundancy, we collect the usage policies on patch/head/block\nselection predicted by our method on the validation set and\nshow the distribution of computational cost (i.e. percentage\nof patches/heads/blocks kept) throughout the backbone net-\nwork. As shown in Figure 5, AdaViT tends to allocate more\ncomputation in earlier stages of the network. In particu-\nlar, for patch selection, the average number of kept patches\nin each transformer block gradually decrease until the ﬁnal\noutput layer. This is intuitive since the patches keep ag-\ngregating information from all other patches in the stacked\nself-attention layers, and a few informative patches near the\noutput layer would sufﬁce to represent the whole input im-\nage for correct classiﬁcation. As visualized in Figure 7, the\nnumber of selected patches gradually decreases with a focus\non the discriminative part of the images.\nFor head selection and block selection, the patterns are\na bit different from token selection, where relatively more\ncomputation is kept in the last few blocks. We hypothesize\nBlock #2\n Block #8\n Block #14\n Block #2\n Block #8\n Block #14\nFigure 7. Selected patches at different blocks. Green color de-\nnotes that the patches are kept.\n1 2 3 4 5\nairship\nwindow_screen\nwhite_stork\nkite\nrapeseed\ndishrag\nparachute\n1 2 3 4 5\nGFLOPs\ntoyshop\nhot_pot\ndesk\ncomic_book\nrestaurant\nbarbershop\nshoe_shop\nFigure 8. Distribution of allocated computational resources for\nclasses using the least (top) and the most (bottom) computation.\nthat the last few layers in the backbone are more responsible\nfor the ﬁnal prediction and thus are kept more often.\nLearned usage policies for different classes. We further\nanalyze the distribution of learned usage policies for differ-\nent classes. In Figure 8, we show the box plot of several\nclasses that are allocated the most/least computational re-\nsources. As can be seen, our method learns to allocate more\ncomputation for difﬁcult classes with complex scenes such\nas “shoe shop”, “barber shop”, “toyshop” but uses less com-\nputation for relatively easy and object-centric classes like\n“parachute” and “kite”.\nQualitative Results. Images allocated with the least and\nthe most computation by our method are shown in Fig-\nure 6. It can be seen that object-centric images with simple\nbackground (like the parachute and the tennis ball) tend to\nuse less computation, while hard samples with clutter back-\nground (e.g. the drum and the toy shop) are allocated more.\nLimitation. One potential limitation is that there is still a\nsmall drop of accuracy when comparing our method with\nthe Upperbound baseline, which we believe would be fur-\nther addressed in future work.\n5. Conclusion\nIn this paper we presented AdaViT, an adaptive compu-\ntation framework that learns which patches, self-attention\nheads and blocks to keep throughout the transformer back-\nbone on a per-input basis for an improved efﬁciency for\nimage recognition. To achieve this, a light-weight deci-\nsion network is attached to each transformer block and op-\n8\nhammerhead\n abacus\n balloon\n barbell\ncandle\n crane\n house ﬁnch\n space shuttle\nstupa\n war plane\n street sign\n honeycomb\nThe Least Computation The Most Computation \ntabby\n accordion\n bakery\n carpenter’s kit\nchainsaw\n grand piano\n limousine\n stage\nstretcher\n washer\n pretzel\n barn\nFigure 9. Qualitative results. Images allocated with the least (Left) and the most (Right) computational resources by AdaViT are shown.\ntimized with the backbone jointly in an end-to-end man-\nner. Extensive experiments demonstrated that our method\nobtains more than 2× improvement on efﬁciency with\nonly a small drop of accuracy compared with state-of-the-\nart vision transformers, and covers a wide range of efﬁ-\nciency/accuracy trade-offs. We further analyzed the learned\nusage policies quantitatively and qualitatively, providing\nmore insights on the redundancy in vision transformers.\nA. Qualitative Results\nWe further provide more qualitative results in addi-\ntion to those in the main text. Images that are allocated\nthe least/most computational resources by our method are\nshown in Figure 9, demonstrating that our method learns\nto use less computation on easy object-centric images and\nmore computation on hard complex images with cluttered\nbackground. Figure 11 shows more visualization of the\nlearned usage policies for patch selection, demonstrating\nthe pattern that our method allocates less and less compu-\ntation gradually throughout the backbone network, which\nindicates that more redundancy in computation resides in\nthe later stages of the vision transformer backbone.\nB. Compatibility to Other Backbones\nOur method is by design model-agnostic and thus can be\napplied to different vision transformer backbones. To verify\nthis, we use DeiT-small [37] as the backbone of AdaViT\nand show the results in Figure 10. AdaViT achieves better\nefﬁciency/accuracy tradeoff when compared with standard\nvariants of DeiT, and consistently outperforms itsRandom+\nbaseline by large margins, as demonstrated in Figure 10(a)\n1 2 3 4 5\nGFLOPs\n72\n74\n76\n78\n80Top-1 Acc %\n(a)\nOurs\nDeiT\n1.2 1.6 2.0 2.4\nGFLOPs\n40\n50\n60\n70\n80\n(b)\nOurs\nRandom+\nFigure 10. Compatibility to DeiT [37]. We use DeiT-small as the\nbackbone of AdaViT and show: (a) Efﬁciency/Accuracy trade-\noffs of standard DeiT variants and our AdaViT. (b) Comparison\nbetween AdaViT and itsRandom+ baseline with similar computa-\ntional cost.\nand 10(b) respectively.\nWe further show the visualization of patch selection us-\nage policies with DeiT-small as the backbone as well in\nFigure 11. A similar trend of keeping more computation\nat earlier layers and gradually allocating less computation\nthroughout the network is also observed.\nReferences\n[1] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max\nWelling. Batch-shaping for learning conditional channel\ngated networks. In ICLR, 2020. 3\n[2] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh\nSaligrama. Adaptive neural networks for fast test-time pre-\ndiction. In ICML, 2017. 3\n9\nBlock #1\n Block #N\nFigure 11. Visualization of selected patches at different blocks with T2T-ViT [50] (Above) or DeiT [37] ( Below) as the vision trans-\nformer backbone respectively. Green color denotes the patch is kept.\n10\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 1\n[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit:\nCross-attention multi-scale vision transformer for image\nclassiﬁcation. In ICCV, 2021. 1, 2, 4, 6\n[5] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-\ning Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vision\ntransformers. In NeurIPS, 2021. 1\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2, 5\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 1, 2,\n3, 4, 6\n[8] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\nHerv´e J ´egou. Training vision transformers for image re-\ntrieval. arXiv preprint arXiv:2102.05644, 2021. 2\n[9] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao\nLi, Zhicheng Yan, Jitendra Malik, and Christoph Feicht-\nenhofer. Multiscale vision transformers. arXiv preprint\narXiv:2104.11227, 2021. 1, 2\n[10] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li\nZhang, Jonathan Huang, Dmitry Vetrov, and Ruslan\nSalakhutdinov. Spatially adaptive computation time for\nresidual networks. In CVPR, 2017. 3\n[11] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre\nStock, Armand Joulin, Herv ´e J ´egou, and Matthijs Douze.\nLevit: a vision transformer in convnet’s clothing for faster\ninference. In ICCV, 2021. 2\n[12] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. In NeurIPS,\n2021. 1\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 6\n[14] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. Transreid: Transformer-based object re-\nidentiﬁcation. In ICCV, 2021. 2\n[15] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3. In ICCV, 2019. 2\n[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 2\n[17] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens\nvan der Maaten, and Kilian Q Weinberger. Multi-scale dense\nnetworks for resource efﬁcient image classiﬁcation. InICLR,\n2018. 3\n[18] Hao Li, Hong Zhang, Xiaojuan Qi, Ruigang Yang, and Gao\nHuang. Improved techniques for training adaptive deep net-\nworks. In ICCV, 2019. 3\n[19] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc\nVan Gool. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707, 2021. 1, 2, 6\n[20] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime\nneural pruning. In NIPS, 2017. 3\n[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 1, 2, 4, 6\n[22] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. arXiv\npreprint arXiv:2106.13230, 2021. 1, 2\n[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 6\n[24] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\nShufﬂenet v2: Practical guidelines for efﬁcient cnn architec-\nture design. In ECCV, 2018. 2\n[25] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The\nconcrete distribution: A continuous relaxation of discrete\nrandom variables. In ICLR, 2017. 2, 4, 5\n[26] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi\nFeng, Xiaodan Liang, Hang Xu, and Chunjing Xu. V oxel\ntransformer for 3d object detection. In ICCV, 2021. 2\n[27] Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna\nSattigeri, Leonid Karlinsky, Aude Oliva, Kate Saenko, and\nRogerio Feris. Ar-net: Adaptive frame resolution for efﬁ-\ncient action recognition. In ECCV, 2020. 3\n[28] Mahyar Najibi, Bharat Singh, and Larry S Davis. Autofocus:\nEfﬁcient multi-scale inference. In ICCV, 2019. 3\n[29] Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang\nWang, Rogerio Feris, and Aude Oliva. Ia-red 2:\nInterpretability-aware redundancy reduction for vision trans-\nformers. In NeurIPS, 2021. 3\n[30] Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao\nHuang. 3d object detection with pointformer. In CVPR,\n2021. 2\n[31] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In ICCV, 2021. 2\n[32] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie\nZhou, and Cho-Jui Hsieh. Dynamicvit: Efﬁcient vision\ntransformers with dynamic token sparsiﬁcation. In NeurIPS,\n2021. 3\n[33] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In CVPR, 2018. 2\n[34] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In ICCV, 2017. 2\n[35] Richard S Sutton and Andrew G Barto. Reinforcement learn-\ning: An introduction. MIT press Cambridge, 1998. 5\n[36] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In ICML, 2019. 2\n[37] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\n11\ndata-efﬁcient image transformers & distillation through at-\ntention. In ICML, 2021. 1, 2, 3, 4, 6, 9, 10\n[38] Burak Uzkent and Stefano Ermon. Learning when and where\nto zoom with deep reinforcement learning. In CVPR, 2020.\n3\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n2, 3, 4\n[40] Andreas Veit and Serge Belongie. Convolutional networks\nwith adaptive inference graphs. In ECCV, 2018. 3\n[41] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions. In ICCV, 2021. 1, 2, 6\n[42] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and\nJoseph E Gonzalez. Skipnet: Learning dynamic routing in\nconvolutional networks. In ECCV, 2018. 3\n[43] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and\nGao Huang. Not all images are worth 16x16 words: Dy-\nnamic vision transformers with adaptive sequence length. In\nNeurIPS, 2021. 3\n[44] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end\nvideo instance segmentation with transformers. In CVPR,\n2021. 2\n[45] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introduc-\ning convolutions to vision transformers. arXiv preprint\narXiv:2103.15808, 2021. 1\n[46] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven\nRennie, Larry S Davis, Kristen Grauman, and Rogerio Feris.\nBlockdrop: Dynamic inference paths in residual networks.\nIn CVPR, 2018. 3\n[47] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr\nDoll´ar, and Ross Girshick. Early convolutions help trans-\nformers see better. In NeurIPS, 2021. 2\n[48] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo. Segformer: Simple and ef-\nﬁcient design for semantic segmentation with transformers.\nIn NeurIPS, 2021. 2\n[49] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and\nGao Huang. Resolution adaptive networks for efﬁcient infer-\nence. In CVPR, 2020. 3\n[50] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. In ICCV, 2021. 1, 2, 3, 4, 5, 6, 10\n[51] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\nShufﬂenet: An extremely efﬁcient convolutional neural net-\nwork for mobile devices. In CVPR, 2018. 2\n[52] Yanyi Zhang, Xinyu Li, Chunhui Liu, Bing Shuai, Yi Zhu,\nBiagio Brattoli, Hao Chen, Ivan Marsic, and Joseph Tighe.\nVidtr: Video transformer without convolutions. In ICCV,\n2021. 1\n12",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7960154414176941
    },
    {
      "name": "Computer science",
      "score": 0.6655265092849731
    },
    {
      "name": "Inference",
      "score": 0.6186863780021667
    },
    {
      "name": "Computation",
      "score": 0.6137555241584778
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5385381579399109
    },
    {
      "name": "Computer engineering",
      "score": 0.35846972465515137
    },
    {
      "name": "Machine learning",
      "score": 0.34846627712249756
    },
    {
      "name": "Algorithm",
      "score": 0.17817839980125427
    },
    {
      "name": "Engineering",
      "score": 0.15619498491287231
    },
    {
      "name": "Electrical engineering",
      "score": 0.09311115741729736
    },
    {
      "name": "Voltage",
      "score": 0.09072902798652649
    }
  ]
}