{
    "title": "KAT: Dependency-Aware Automated API Testing with Large Language Models",
    "url": "https://openalex.org/W4400703290",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2109810339",
            "name": "Le Tri",
            "affiliations": [
                "Anova Corp (Vietnam)"
            ]
        },
        {
            "id": "https://openalex.org/A2498102671",
            "name": "Tran Thien",
            "affiliations": [
                "Vietnam National University Ho Chi Minh City"
            ]
        },
        {
            "id": null,
            "name": "Cao, Duy",
            "affiliations": [
                "Vietnam National University Ho Chi Minh City"
            ]
        },
        {
            "id": "https://openalex.org/A3032384719",
            "name": "Lê Vy",
            "affiliations": [
                "Anova Corp (Vietnam)"
            ]
        },
        {
            "id": "https://openalex.org/A2631058979",
            "name": "Nguyen Tan. Tien",
            "affiliations": [
                "The University of Texas at Dallas",
                "University of North Texas at Dallas"
            ]
        },
        {
            "id": "https://openalex.org/A2121405639",
            "name": "Nguyen Vu",
            "affiliations": [
                "Vietnam National University Ho Chi Minh City"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3047848029",
        "https://openalex.org/W3177546869",
        "https://openalex.org/W2997401484",
        "https://openalex.org/W4284680450",
        "https://openalex.org/W4213274161",
        "https://openalex.org/W2955835447",
        "https://openalex.org/W3126551690",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4312828963",
        "https://openalex.org/W3216436948",
        "https://openalex.org/W3113930337",
        "https://openalex.org/W4224306538",
        "https://openalex.org/W4388483057",
        "https://openalex.org/W4384154515",
        "https://openalex.org/W4284706564",
        "https://openalex.org/W3211595358",
        "https://openalex.org/W3047675717",
        "https://openalex.org/W6810705462",
        "https://openalex.org/W4386027284",
        "https://openalex.org/W4224983044",
        "https://openalex.org/W4308641864",
        "https://openalex.org/W3011408464",
        "https://openalex.org/W4220791610",
        "https://openalex.org/W4211256124",
        "https://openalex.org/W4207068811",
        "https://openalex.org/W3135149769",
        "https://openalex.org/W3047047526",
        "https://openalex.org/W3202888839",
        "https://openalex.org/W3038801694",
        "https://openalex.org/W3204593724",
        "https://openalex.org/W3203878282",
        "https://openalex.org/W2955498974",
        "https://openalex.org/W3170391117",
        "https://openalex.org/W3200142270",
        "https://openalex.org/W3090871722",
        "https://openalex.org/W3173859375",
        "https://openalex.org/W3010807879",
        "https://openalex.org/W4308643315",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4221149304"
    ],
    "abstract": "API testing has increasing demands for software companies. Prior API testing\\ntools were aware of certain types of dependencies that needed to be concise\\nbetween operations and parameters. However, their approaches, which are mostly\\ndone manually or using heuristic-based algorithms, have limitations due to the\\ncomplexity of these dependencies. In this paper, we present KAT (Katalon API\\nTesting), a novel AI-driven approach that leverages the large language model\\nGPT in conjunction with advanced prompting techniques to autonomously generate\\ntest cases to validate RESTful APIs. Our comprehensive strategy encompasses\\nvarious processes to construct an operation dependency graph from an OpenAPI\\nspecification and to generate test scripts, constraint validation scripts, test\\ncases, and test data. Our evaluation of KAT using 12 real-world RESTful\\nservices shows that it can improve test coverage, detect more undocumented\\nstatus codes, and reduce false positives in these services in comparison with a\\nstate-of-the-art automated test generation tool. These results indicate the\\neffectiveness of using the large language model for generating test scripts and\\ndata for API testing.\\n",
    "full_text": "KAT: Dependency-aware Automated API Testing\nwith Large Language Models\nTri Le\nKatalon Inc.\nHo Chi Minh City, Vietnam\ntri.qle@katalon.com\nThien Tran\nKatalon Inc.\nUniversity of Science\nVietnam National University\nHo Chi Minh City, Vietnam\nthien.tran@katalon.com\nDuy Cao\nKatalon Inc.\nUniversity of Science\nVietnam National University\nHo Chi Minh City, Vietnam\nduy.cao@katalon.com\nVy Le\nKatalon Inc.\nHo Chi Minh City, Vietnam\nvy.le@katalon.com\nTien N. Nguyen\nComputer Science Department\nUniversity of Texas at Dallas\nDallas, Texas, USA\ntien.n.nguyen@utdallas.edu\nVu Nguyen*\nKatalon Inc.\nUniversity of Science\nVietnam National University\nHo Chi Minh City, Vietnam\nnvu@fit.hcmus.edu.vn\nAbstract—API testing has increasing demands for software\ncompanies. Prior API testing tools were aware of certain types of\ndependencies that needed to be concise between operations and\nparameters. However, their approaches, which are mostly done\nmanually or using heuristic-based algorithms, have limitations\ndue to the complexity of these dependencies. In this paper, we\npresent KAT (Katalon API Testing), a novel AI-driven approach\nthat leverages the large language model GPT in conjunction\nwith advanced prompting techniques to autonomously generate\ntest cases to validate RESTful APIs. Our comprehensive strategy\nencompasses various processes to construct an operation depen-\ndency graph from an OpenAPI specification and to generate test\nscripts, constraint validation scripts, test cases, and test data. Our\nevaluation of KAT using 12 real-world RESTful services shows\nthat it can improve test coverage, detect more undocumented\nstatus codes, and reduce false positives in these services in\ncomparison with a state-of-the-art automated test generation tool.\nThese results indicate the effectiveness of using the large language\nmodel for generating test scripts and data for API testing.\nIndex Terms—REST API, Black-box testing, API testing, Large\nlanguage models for testing\nI. I NTRODUCTION\nRESTful APIs (or REST APIs - REpresentational State\nTransfer) is a software architectural style to guide the devel-\nopment of web APIs. RESTful APIs communicate through\nHTTP requests to perform standard database functions such\nas creating, reading, updating, and deleting records (CRUD).\nRESTful API has become one of the most used software\narchitectural styles due to its high flexibility and scalability,\nas well as being relatively secure and easy to implement.\nAutomated test cases and data generation for API testing\nhave been an active research topic that has attracted many\nstudies in recent years [1], [2], [3], [4], [5], [6]. These studies\ncan be divided into black-box and white-box test generation\napproaches. Black-box testing, which is the most common,\n* Corresponding: Vu Nguyen (nvu@fit.hcmus.edu.vn)\nuses the OpenAPI Specification (OAS) as a basis to generate\ntest cases and data. White-box testing focuses on analyzing\nsource code to drive test cases and data [3].\nWhile current approaches have made significant progress in\ntesting RESTful APIs, they still face challenges in addressing\nintricate dependencies among API endpoints and their param-\neters. These dependencies fall into three categories. Firstly,\nthere are dependencies among API endpoints (operations).\nFor instance, to test an endpoint for charging a credit card\nin an online flight booking system, one must first invoke the\noperation for selecting the flight. Secondly, there are depen-\ndencies among the parameters of an operation. For example,\nin a flight booking system, the operation requires parameters\nlike arrivalDate and departureDate. And in that context, the\nconstraint is that departureDate must precede arrivalDate.\nThirdly, there are dependencies between an operation and\nits parameters. For instance, in a flight booking system, the\nparameters arrivalDate and departureDate of a reservation\noperation must be set in the future.\nUnfortunately, the foregoing API testing frameworks have\nlimitations in handling such dependencies. In the case of inter-\noperation dependencies, RestTestGen [1] employs a heuristic\napproach centered on name matching to identify relationships\namong operations. This hinges on a shared field between the\noutput of one operation and the input of another. However,\ndiscrepancies in field names could lead the heuristic algorithm\nto incorrectly establish dependencies. For instance, exclusive\nreliance on field names might fail to accurately identify\nthe GET /flights endpoint as a dependent operation of the\nPOST /booking endpoint, since the algorithm might not find\nmatching pairs of field names.\nIn terms of inter-parameter dependencies, previous methods\nhave indeed considered them in test case generation. Yet,\nit is crucial to note certain limitations in these approaches.\nFor instance, RESTest [2] addresses this by mandating the\n1\narXiv:2407.10227v1  [cs.SE]  14 Jul 2024\nmanual inclusion of dependencies among parameters in the\ntesting OAS file under the x-dependencies field. This manual\nintervention demands a significant amount of time from testers.\nThus, any improvements in simplifying this process would\nsignify progress in this field.\nRegarding dependencies between operations and parame-\nters, cutting-edge approaches employ heuristic-based methods\n(RestTestGen [1]) or rule-based methods (bBOXRT [7]) to\ngenerate both valid and invalid test data. However, they might\nnot fully comprehend these constraints articulated in natural\nlanguage or might necessitate manual intervention.\nThrough harnessing the excellent capabilities of GPT [8]\nin interpreting the natural language content embedded within\nSwagger files for RESTful APIs, our proposed approach,\nKAT (KATalon API Testing), leverages a powerful tool for\ncomprehending intricate dependencies. This utilization of GPT\nempowers us to systematically extract and discern the depen-\ndencies that exist among the various API endpoints, indicating\nrelationships that define the functionality of the APIs. More-\nover, GPT possesses the capability to analyze the connections\namong the parameters associated with each operation. This\nenables us to perceive the underlying details of the API,\nultimately leading to more thorough testing. In contrast, state-\nof-the-art methods, which mainly rely on heuristic approaches,\nfall short in capturing those dependencies.\nOur methodology employs a generative language model and\nadvanced prompting techniques at key stages of the testing\nprocess. This includes (1) identifying relationships among\nschemas and operations, which are used to construct an opera-\ntion dependency graph (ODG), and (2) detecting dependencies\namong input parameters as well as inter-dependencies between\noperations and their respective parameters for both valid and\ninvalid test data generation. To streamline this process, our\napproach seamlessly integrates the ODG into the test script\ngeneration. The resulting test scripts are then combined with\nthe generated test data to create a comprehensive suite of test\ncases, which are subsequently executed on the target APIs.\nWe have conducted an experiment to evaluate our approach,\nKAT, using a dataset comprising 12 RESTful API services.\nThe results demonstrate a significant improvement of 15.7% in\nthe coverage of status codes documented in OAS files over the\nstate-of-the-art RestTestGen [9]. Additionally, KAT effectively\nidentifies the status codes that are not explicitly specified in\nthe OAS files while reducing the number of false-positive test\ncases generated.\nIn this paper, we make the following key contributions:\n1) KAT, an GPT-based approach to generate tests for\nRESTful API testing that considers the inter-dependencies\namong operations, inter-parameter dependencies, and the de-\npendencies between operations and parameters .\n2) An extensive evaluation showing our approach outper-\nforming the state-of-the-art RESTful API testing approaches.\nII. M OTIVATING EXAMPLE\nA. Observations\nIn this section, we use an example to elucidate the issue and\ninspire our approach. Fig. 1 shows a representative OAS file\ndelineating the RESTful APIs for an online flight booking sys-\ntem. Within this OAS file, two endpoints are documented: GET\n/flights and POST /booking. The former endpoint serves\nthe purpose of retrieving a list of available flights within a\nspecified date range, while the latter facilitates passengers in\nreserving a new flight. For each endpoint (operation), the file\ncontains a descriptive summary (e.g., lines 7 and 20), the list of\nparameters (lines 21–24) and the request body object (line 25)\nwith their names and textual descriptions in the schema (e.g.,\nlines 25–41). Moreover, there exist supplementary endpoints\nsupporting pertinent operations (omitted here for brevity).\nFrom this example, we make the following observations:\nObservation 1 ( Dependencies among Operations): The\nusage of operations involves the dependencies among them.\nA test case of an endpoint might only be able to be\nsuccessfully tested if the prerequisite test cases of other\nendpoints have already been properly executed beforehand.\nThis is because responses to these prerequisite endpoints may\naffect the request for a current endpoint under test.\nIn the provided OAS (refer to Fig. 1), it is crucial to supply\na proper parameter for accurate testing of the POST /booking\nendpoint. This parameter should consist of a single field named\nflightId, representing an existing flight in the database.\nTo meet this condition, it is necessary to first successfully\nexecute the GET /flights endpoint. This action will result in\na response containing a list of available flights. From this list,\na specific flight element can be chosen, and its corresponding\nflightId value extracted. This value is then integrated into\nthe test case for the POST /booking operation.\nObservation 2 ( Dependencies among Parameters): The\nusage of an operation in a library involves the dependencies\namong its parameters.\nDependencies also exist among the parameters of an oper-\nation. Inter-parameter dependency refers to a constraint that\nexists between two or more input parameters of an API\nendpoint. This constraint may pertain to the required values of\nthese parameters, which must satisfy a predefined condition.\nAlternatively, it could involve the presence of an optional\nparameter, rendering the absence of another parameter prob-\nlematic. For instance, consider the endpoint POST /booking.\nHere, a tangible inter-parameter constraint is evident between\ndepartureDate (line 30) and arrivalDate (line 34), an asso-\nciation discernible to users. Naturally, it is expected that the\ndeparture date of a flight precedes its arrival date. Thus, when\nhandling the result, the requesting parameters should account\nfor this constraint to ensure a valid response.\nObservation 3 (Dependencies between An Operation and\nIts Parameters): The usage of an operation involves the\ndependencies between that operation and its parameters.\nCertain endpoints may encompass fields with values that\nmust adhere to constraints, requiring those values to make\n2\n1 openapi: 3.0.0\n2 info:\n3 title: Flight Booking API\n4 paths:\n5 /flights:\n6 get:\n7 summary: Get Flights\n8 responses:\n9 ’200’:\n10 description: A list of available flights.\n11 content:\n12 application/json:\n13 schema:\n14 type: array\n15 items:\n16 $ref: ’#/components/schemas/Flight’\n17\n18 /booking:\n19 post:\n20 summary: Book a new Flight\n21 parameters:\n22 - name: flightId\n23 schema:\n24 type: integer\n25 requestBody:\n26 content:\n27 application/json:\n28 schema:\n29 properties:\n30 departureDate:\n31 type: string\n32 format: date\n33 description: format in YYYY-MM-DD.\nShould be after today.\n34 arrivalDate:\n35 type: string\n36 format: date\n37 description: format in YYYY-MM-DD.\nShould be after ‘departureDate‘.\n38 passengerName:\n39 type: string\n40 passengerAge:\n41 type: integer\n42 responses:\n43 ’200’:\n44 description: The booking is successful.\n45 content:\n46 application/json:\n47 schema:\n48 $ref: ’#/components/schemas/Booking’\n49\n50 components:\n51 schemas:\n52 Flight:\n53 type: object\n54 properties:\n55 id:\n56 type: integer\n57 origin:\n58 type: string\n59 destination:\n60 type: string\n61 Booking:\n62 type: object\n63 properties:\n64 flight:\n65 $ref: ’#/components/schemas/Flight’\n66 departureDate:\n67 type: string\n68 format: date\n69 arrivalDate:\n70 type: string\n71 format: date\n72 passengerName:\n73 type: string\n74 passengerAge:\n75 type: integer\nFig. 1. An Example of OpenAPI/Swagger Specification (OAS) file\nsense in a real-world context. For instance, within the OAS\nfile in Fig. 1, the value of passengerAge specified in the\nPOST /booking endpoint (line 40) must exceed zero, although\nthis constraint is not explicitly expounded within the OAS\nfile. Furthermore, the value assigned to departureDate must\nrefer to a future date. Unlike the passengerAge parameter, this\nconstraint is articulated in natural-language description.\nB. The state-of-the-art Approaches\nDespite that the dependencies exist among the endpoints\nand parameters as observed, the state-of-the-art API testing\napproaches do not sufficiently address and capture them.\nRegarding the inter-operation dependencies, RestTestGen\nemploys a heuristic approach based on name matching . This\ndetermination hinges on the presence of a shared field between\nthe output of one operation (i.e., endpoint) and the input\nof another. For instance, in the OAS file in Fig. 1, the\nGET /flights endpoint yields a set of Flight objects, each\nfeaturing an id field. Conversely, the POST /booking endpoint\nanticipates a flightId field as part of its input. This disparity\nin field nomenclature implies that the heuristic algorithm\nmight encounter difficulty in establishing dependencies or\nmight potentially mis-attribute them to other endpoints sharing\nsimilar field names. Relying solely on field names, it may not\ncorrectly identify the GET /flights endpoint as a dependent\noperation of the POST /booking endpoint.\nWhen it comes to inter-parameter dependencies, previous\napproaches have indeed taken them into account during the\ntest case generation. However, it is important to acknowledge\ncertain limitations in these methods. For example, RESTest [2]\naddresses this issue by requiring manual inclusion of depen-\ndencies among parameters in the testing OAS file, utilizing\nthe x-dependencies field. This manual intervention demands\na considerable amount of time from testers. Regarding the\ndependencies between the operations and parameters, the state-\nof-the-art approaches use heuristic-based (RestTestGen) or\nrule-based (bBOXRT) methods to generate valid and invalid\ntest data for testing, but they may not fully address these\nconstraints described in natural language or require manual\nintervention to specify the dependencies.\nC. Capability of GPT and Motivation\nThe Large Language Models (LLMs) have been achiev-\ning remarkable successes in understanding texts and source\ncode [8]. In this work, to capture such dependencies, we\npropose to explore that capability of the LLMs. To motivate\nthis point, we made a few prompts to GPT-3.5 [8] to explore its\ncapability in understanding the intricate dependencies among\nthe API endpoints (or operations) and their parameters. In\nparticular, we use the version of gpt-3.5-turbo-1106 via\nthe provided API from OpenAI to request. We first loaded\nthe Swagger file in Fig. 1 and constructed prompts. The\nfollowing conversations with GPT show how it can address\nthe dependency problem:\nPrompt: “Are there any dependencies between the POST /booking\noperation and GET /flights operation?”\nGPT’s response : “Yes, there is a dependency between the POST\n/booking operation and the GET /flights operation. Before making a\nbooking, the user needs to retrieve the available flights by calling the\nGET /flights operation. This is necessary to obtain flightId, which is\nrequired in the request body of the POST /booking operation. Thus, the\nGET /flights operation is a prerequisite operation that is called before\nthe POST /booking operation can be tested successfully. ”\nGPT is also able to detect inter-parameter dependencies:\n3\nPrompt: “Are there any inter-parameter dependencies mentioned in the\nPOST /booking operation spec?”\nGPT’s response : “Yes, there are inter-parameter dependencies men-\ntioned in the POST /booking operation spec. The arrivalDate parameter\nis dependent on the departureDate parameter. The description states\nthat the arrivalDate should be after the departureDate. Therefore, the\ndepartureDate needs to be provided before the arrivalDate in order to\nensure that the request is valid. ”\nFinally, GPT is capable of understanding the constraints\namong the parameters of the operations:\nPrompt: “Are there any constraints of the value of parameters appeared\nin the specification, which are not defined in the specification but worth\nnoting in a real-world context?”\nGPT’s response : “In the provided specification, there are no specific\nconstraints for the parameter values. However, in a real-world context,\nthe following constraints are worth noting:\n1. departureDate: Must be a future date (YYYY-MM-DD) and cannot be\nin the past.\n2. arrivalDate: Must be after the departureDate (YYYY-MM-DD) and\ncannot be the same as or before the departureDate.\n3. passengerAge: Subject to minimum and maximum age requirements,\nage categories for fare differentiation, or legal age restrictions for\nspecific flights or destinations.\nThese constraints ensure the validity of flight and passenger details,\naligning with the considerations for flight bookings. ”\nIn brief, GPT is capable of identifying the relationships\nbetween operations and their parameters on a token level.\nThus, we leverage GPT in producing the test cases that cover\nthose dependencies among the APIs and/or their parameters.\nIII. D EPENDENCY -AWARE AUTOMATED API T ESTING\nA. Approach Overview\nFig. 2. KAT: Dependency-aware Automated API Testing\nThis section presents an overview of our approach, KAT\n(Fig. 2) that leverages LLMs, GPT in particular, to construct\nODGs and generate test scripts and data for API testing.\nKAT receives the OAS file as input. This file serves as the\nbasis for extracting the detailed information about its service\nto construct ODG and generate test scripts and data. KAT\nconsists of the following steps:\n• Construct ODG : this step aims to construct an ODG\nrepresenting dependencies between operations defined in\nthe specification. See more details in Section III-B.\n• Generate operation sequences : using the ODG obtained\nfrom the previous step, our approach generates sequences\nof operation executions or requests in which one request\nprepares the data (parameters and request body) needed\nfor succeeding requests. Dependent on specific ODG, it\nis possible that an operation is standalone or independent,\nFig. 3. ODG Construction\nwhich does not require any preceding requests to prepare\nthe data needed for its successful execution.\n• Generate test scripts : this step aims to generate test\nscripts using the obtained operation sequences.\n• Generate test data : details of this step, which generates\nthe data for a test operation, are given in Section III-C.\nB. Operation Dependency Graph Construction (Fig. 3)\n1) Important Concepts:\nDefinition 1 ( Operation Dependency Graph): An ODG is\nrepresented as a directed graph G = (N, V), wherein each\nnode N signifies a discrete operation within the RESTful API.\nThe presence of an edge v ∈ V , articulated as v = n1 →\nn2, signifies the existence of a dependency between the nodes\nn1 and n2. This dependency is predicated on the condition\nthat one or more fields in the output of source node n1 must\ncoincide with in the input of target node n2, thereby mandating\nthat the execution of operation n1 precedes that of n2.\nThe concept of ODG was previously explored in the work\nof RestTestGen. Refer to the exemplar OAS file in Fig. 1,\nand observe a dependency between the two endpoints, GET\n/flights and POST /booking, as explained in Observation 1\n(Fig. 1). Part of the ODG of this OAS file is shown in Fig. 4.\nFig. 4. Dependence between GET/flights and POST/booking in ODG\n2) ODG Construction Algorithm: Our algorithm differs\nfrom the heuristic-based ODG construction algorithm in\nRestTestGen [1] in its utilization of GPT. Algorithm 1 presents\nour pseudocode, which involves the use of GPT for inferring\nthe dependencies between operations and schemas, as well\nas the dependencies among schemas. These process steps are\nemployed to construct additional operation dependency edges\nthat heuristic-based algorithms cannot detect.\nHeuristic-based collection of edges:Primarily, the function\ngatherODheuristic (line 2) is employed to iterate through\nall operations, attempting to match input-output pairs. Upon\ndetecting a pair with a perfect character match, we add the\nresulting edge between the corresponding nodes in the graph.\nIn the case of the OAS file depicted in Fig. 1, the relation-\nship between two operations is considered. Although the pair\n4\nAlgorithm 1: ODG Construction\n1 Function generateOperationDependencies(Swagger)\nInput : A Swagger Specification file\nOutput: List of Operation Dependencies OD\n2 OD ← gatherODheuristic (Swagger)\n3 OS ← GPTgenOperationSchemaDep (Swagger)\n4 SS ← GPTgenSchemaSchemaDep (Swagger)\n5 O ← getAllOperationsFrom (Swagger)\n6 foreach Oi in O do\n7 P ← getParameters (Oi)\n8 PSDep ← getDep (OS, Oi)\n9 if PSDep ̸= ∅ then\n10 NewOD ← gatherOperationDep (PSDep,P)\n11 OD.extend(NewOD)\n12 end\n13 if P ̸= ∅ then\n14 ChildS ← getDep (SS, Oi)\n15 NewOD ← gatherOperationDep (ChildS,P)\n16 OD.extend(NewOD)\n17 end\n18 end\n19 return OD\n20 Function gatherOperationDep (C, P)\nInput : A collection to find new OD elements C\nA set of parameters to check dependency P\nOutput: A set of new elements to be extended NewOD\n21 foreach Cj in C do\n22 PO ← findPrecedingOperations (Cj )\n23 if PO ̸= ∅ ∨ PO ∈ P then\n24 extendDependencies (NewOD, PO)\n25 end\n26 P.remove(PO)\n27 end\n28 return NewOD\nflightId (line 22, input of POST /booking) and id (existed\nin the schema “Flight”, the output of GET /flights, on line\n16) do not exhibit a perfect match, a significant dependency\npersists due to the presence of id within the “Flight” schema,\nsemantically aligning with flightId. To resolve this, we use\nGPT to analyze such pairs in a two-step process, involving\nthe establishment of Operation-Schema dependencies (line 3)\nand Schema-Schema dependencies (line 4), thereby acting as\na bridge to identify the dependencies between operations.\nOperation-Schema Dependency:\nDefinition 2 (Operation-Schema Dependency): The origin\nof two operations depends on each other when the response of\none operation is required as input for the successful execution\nof the other. Within an OAS, each operation’s response is de-\nfined under a schema, and subsequent operations must identify\nrelevant schemas to determine their predecessor operations.\nAn operation-schema dependency encompasses all schemas\nthat share keys between their fields and the input parameters\nof the operation, which GPT can effectively identify.\nOperation-Schema dependencies capture this relationship by\nconstructing the dictionary OS (line 3). In this dictionary, each\nkey corresponds to an operation’s name oi ∈ On, in which On\nis a set of operation’s name described in the OAS specification.\nThis key is associated with a collection of data, providing in-\nsights into essential prerequisite schemas and the connections\nbetween the current operation oi and these schemas through\ntheir parameters. For each parameter pair, the first property\n1 OS = {\n2 \"post-/booking\": {\n3 \"Flight\": {\n4 \"flightId\": \"id\"\n5 }\n6 \"Booking\": {\n7 \"flightId\": \"flight\"\n8 }\n9 }\n10 }\nFig. 5. Example of an Operation-Schema dependency dictionary. The oper-\nation POST /booking has two Operation-Schema dependencies with the\nschemas “Flight” and “Booking”. This is indicated by the pairs of parameter\nflightId (line 22) with field id (line 55) and flight (line 64) in Fig 1.\n1 PROMPT=\"\"\"Given the operation and its parameters,\nidentify all prerequisite\n2 schemas for retrieving information related to the\noperation’s parameters.\n3\n4 Below is the operation and its parameters:\n5 post-/booking:\n6 flightId: integer\n7 departureDate: string\n8 arrivalDate: string\n9 ...\n10\n11 Below is the list of all schemas and their properties:\n12 schemas:\n13 Flight:\n14 ...\n15 Booking:\n16 ...\n17\n18 Please format the prerequisite schemas in the following\nstructure:\n19 <parameter of the operation> -> <equivalent operation of\nthe relevant schema>\n20 ...\"\"\"\nFig. 6. The prompt for prerequisites in the POST /booking operation\nyields a GPT response with the “Flight” and “Booking” schemas, creating\nOperation-Schema dependencies.\nbelongs to the schema, while the second property belongs to\nthe operation oi. As an example, consider the OS dictionary\ncreated for the OAS file shown in Fig. 1, which is illustrated in\nFig. 5. Fig. 6 shows the prompt we used to infer all prerequisite\nschemas of the operation POST /booking.\nFig. 5 shows the operation POST /booking along with two\nschemas, “Flight” and “Booking,” identified as its prerequisite\nschemas by GPT. When detecting the “Flight” schema, it\nallows to infer that GET /flights is the predecessor since the\n“Flight” schema is specified as the reply for GET /flights.\nHowever, in some cases, GPT only identifies the “Booking”\nschema, and cannot identify a clear predecessor operation\nsince there is no operation specifying “Booking” as its re-\nsponse. To address this, we use a Schema-Schema dependency.\nSchema-Schema Dependency:\nDefinition 3 (Schema-Schema Dependency): We define Sn\nis a set of n schemas described in the OAS specification.\nSchema-schema dependencies model the relationship between\nschemas by initializing the dictionary SS (line 4). This dictio-\nnary contains n distinct key-value pairs where key is a schema\nsi ∈ Sn, and value is a list of schemas, where:\n• Each schema in the list contains at least one field that\nrefers to a field specified in the key, or\n• The schema itself is referred by the key (e.g. the element\n“Flight” in the value is referred by the key “Booking”).\n5\n1 PROMPT=\"\"\"Given the schema and its properties in the\nOpenAPI specification of an API application, your\ntask is to identify the prerequisite schemas that\nneed to be created before establishing the mentioned\nschema.\n2\n3 Below is the schema and its properties\n4 Booking:\n5 flight:\n6 $ref: ’#/components/schemas/Flight’\n7 departureDate: ...\n8 arrivalDate: ...\n9 ...\n10\n11 Below is the list of all schemas and their properties:\n12 schemas:\n13 Flight:\n14 ...\n15 Booking:\n16 ...\n17\n18 Return in separated lines. No explanation needed.\"\"\"\nFig. 7. The prompt for “Booking” prerequisites elicits a GPT response with\nthe “Flight” schema, establishing a Schema-Schema dependency.\nConsider the conversation displayed in Fig. 7 with GPT. Our\napproach tries to obtain potential relationships of the schema\n“Booking” using the Swagger displayed in Fig. 1.\nAfter iterating through all schemas, the SS for the OAS file\ngiven in Fig. 1 will have the value as shown below.\n1 SS = {\n2 \"Flight\": [],\n3 \"Booking\": [\n4 \"Flight\"\n5 ]\n6 }\nThe process to detect prerequisites and construct edges\nof the graph:Upon the completion of constructing these two\nhierarchical sets generated by the GPT model, we initiate a\nprocess to establish dependencies between operations.\nThe process has a for loop (lines 6–18), iterating through\nextracted operations from the OAS file (line 5). The objective\nis to identify the relevant schema for each parameter obtained\nfrom the current operation (line 7). Using the Operation-\nSchema dependency stored in OS (line 8), OD is updated\n(line 11). Any unaccounted parameters at the current schema\nlevel prompt an attempt to retrieve the next schema level using\nSS (line 14), followed by a second attempt to update the final\nresult (line 16). These steps establish the adequacy of OD to\ngenerate the sequential order of operations for test generation.\nSpecifically, the function gatherOperationDep (line 20)\ntakes two inputs. It contains a single loop (line 21), iterating\nthrough all elements in the collection. Each element is ex-\namined by findPrecedingOperations (line 22). This function\nreturns a list of preceding operations ( PO), representing POST\nor GET operations, indicating the need for creating a new\ndata item before executing it on any test cases or ensuring the\nexistence of that data in the database. After this, P checks the\nedges to determine if PO is usable for the current operation.\nC. Test Data Generation\nThis section describes our approach to leverage GPT to gen-\nerate both valid and invalid test data, as well as Python scripts\nto validate the accurate correspondence between parameters.\nWe use GPT to generate high-quality valid and invalid test\nFig. 8. The process of test data generation for a single operation\n1 GET_DATASET_PROMPT=f\"\"\"Given the information about the\noperation, generate a\n2 dataset containing 10 data items to be used to test the\noperation.\n3 {additional_instruction}\n4\n5 Operation information: {endpoint_information}\n6 Referenced schema: {ref_schema}\n7\n8 Your dataset represents each data item in the JSONL\nformat, line by line.\"\"\"\nFig. 9. Sample prompt template for test data generation\ndata that closely mimic real-world data. Additionally, GPT\nis used to identify inter-dependencies among the operation’s\nparameters. These dependencies serve as context for dynami-\ncally update the data generation, and are used for formulating\nPython validation scripts. When creating test data files for\nfailure cases, we consider various scenarios, such as omitting\nvalues in required fields, providing incorrect data types for\ncertain fields, or violating inter-parameter constraints.\nThese Python validation scripts ensure consistency with the\ndata file. However, if no inter-parameter dependencies are\nidentified in the current operation, the Python script generation\nprocess does not take place, and no additional update in data\ngeneration. Following another cycle of data generation, each\nitem in the valid/invalid data files undergoes an additional\nevaluation phase before being compiled into usable test data\nfiles. These files are then integrated with a test script to form\na test case for the operation. JSON format is chosen for\ncommunication between a data item and its script.\nAll prompts follow the format shown in Fig. 9. The section\nlabeled additional_instruction is employed to guide the\nmodel in generating valid or invalid datasets and to specify\nfailed scenarios for more comprehensive coverage of failures.\nAn illustration for the generated data, which represents the\nvalue of the request body for POST /booking is displayed in\nFig. 1. The valid and invalid data is as shown in Fig. 10.\nD. Generating Test Scripts and Test Cases\nWe adhere to the approach of examining response status\ncodes within the 2xx and 4xx ranges. For an operation, a\ncorresponding test script is generated by the GPT model, based\non the number of sequences extrapolated from the ODG. The\nleading operation in each order retrieves the data element from\n6\n1 [\n2 {\n3 \"data\": {\n4 \"departureDate\": \"2022-12-01\",\n5 \"arrivalDate\": \"2022-12-02\",\n6 \"passengerName\": \"John Doe\",\n7 \"passengerAge\": 30\n8 },\n9 \"expected_code\": 200\n10 },\n11 {\n12 \"data\": {\n13 \"departureDate\": \"2022-11-15\",\n14 \"arrivalDate\": \"2022-11-16\",\n15 \"passengerName\": \"Jane Smith\",\n16 \"passengerAge\": 25\n17 },\n18 \"expected_code\": 200\n19 },\n20 ...\n21 ]\n1 [\n2 {\n3 \"data\": {\n4 \"departureDate\": \"2022-03-10\",\n5 \"arrivalDate\": \"2022-03-09\",\n6 \"passengerName\": \"Michael Johnson\",\n7 \"passengerAge\": 35\n8 },\n9 \"expected_code\": 400\n10 },\n11 {\n12 \"data\": {\n13 \"departureDate\": null,\n14 \"arrivalDate\": \"\",\n15 \"passengerName\": \"John Doe\",\n16 \"passengerAge\": \"25\"\n17 },\n18 \"expected_code\": 400\n19 },\n20 ...\n21 ]\nFig. 10. These items illustrate valid and invalid request body data for the\nPOST /booking endpoint.\nits relevant data file, and its output is assimilated as the input\nof the successive operation in the sequential procession. Each\noperation is furnished with a tailored test script that exclusively\ndraws a data element from the test data file, housing only a\nsingular execution of that operation in and of itself.\nFig. 11 displays a generated Groovy programming language\ntest script for consecutively executing two requests to test the\noperation POST /booking, as described in Fig. 1. It illustrates\nhow the operation dependencies impact the order of execution\nof different operations in a single script. Custom keywords\nlike makeRequest and assertStatusCode have been created\nto facilitate clean Groovy code, making it easier for the GPT\nmodel to generate an executable test script.\nAfter establishing a collection for evaluating 2xx status\ncodes, we used these scripts as templates to create scripts\nfor assessing 4xx status codes. These existing scripts can\nbe adjusted by modifying the path of relevant test data files\nand rearranging the order within the operation sequence. For\nexample, if a pre-existing DELETE operation aligns with the\ncurrent element in the sequence, it can be inserted to simulate\na scenario of receiving a 404 status code.\nIV. E MPIRICAL EVALUATION\nA. Research Questions\nFor evaluation, we seek to answer the following questions:\n1 // Import statements\n2 import ...\n3\n4 // ChatGPT generated test data goes here\n5 def path_variables_1 = [:]\n6 def query_parameters_1 = [:]\n7 def body_1 = \"\"\n8\n9 def response_1 = makeRequest(\n10 <path to test GET /flight>, path_variables_1,\n11 query_parameters_1, body_1\n12 )\n13\n14 // ChatGPT generated test data goes here\n15 def path_variables = [:]\n16 def query_parameters = [\n17 ’flightId’: response_1[0].id\n18 ]\n19 def body = <get from the relevant data file>\n20\n21 def response = makeRequest(\n22 <path to test POST /booking>, path_variables,\n23 query_parameters, body\n24 )\n25 def latest_response = response\n26\n27 // END\n28 def expected_status_code = <value>\n29 assertStatusCode(latest_response, expected_status_code)\nFig. 11. The Groovy-generated test script exemplifies the POST /booking\noperation following the GET /flight → POST /booking sequence.\nRQ1. [Test coverage]: How well does our approach gener-\nate valid test cases for coverage improvement?\nRQ2. [Test generation efficiency] : How efficient is our\napproach in generating valid test cases?\nRQ3. [Failure detection] : What is the capability of our\napproach in detecting failures and mismatches between the\nworking API and its specification?\nWe chose to compare our approach against the state-of-the-\nart RestTestGen (RTG for short).\nFor RQ1, our analysis involves using RTG and our approach\n(KAT) to generate test cases and run them on the target APIs\nto measure the test coverage of successful (2xx) and operation\nfailure responses (4xx).\nFor RQ2, we evaluate the efficiency of the approaches by\nmeasuring the efficiency score which is the number of test\ncases triggering 2xx and 4xx responses over the total number\nof test cases generated. A higher efficiency score indicates a\nmore efficient approach, which also means fewer test cases\nneeded to generate for covering status codes.\nFor RQ3, we measure the number of 500 errors found, the\nnumber of status codes not documented in the OAS, and the\nnumber of mismatches in status codes defined in the OAS and\nthose returned while making requests to target services.\nB. Dataset\nThe dataset used in our experimentation consists of 12\nRESTful API services. We decided to use these services\nfor several reasons (1) they are publicly accessible to allow\nreplication, including Canada Holidays and the Bills API\ncollected from the website APIs.guru [10], (2) most of them\nare previously investigated in prior studies, including PetStore\nin [1], [4], [11], [12], Genome Nexus in [13], [14], [15],\nBingMap-Route in [16], [9], GitLab in [17], [16], [6], [18],\n[19] (3) ProShop [20], an in-house development, is a private\n7\nTABLE I\nSTATISTICS OF REST SERVICE DATASET\nREST service OP 2xx\nSC\n4xx\nSC Par Dep\nOps\nInt\nOps\nProShop 16 16 30 46 11 0\nPetStore 19 15 20 61 10 0\nCanada Holidays 6 6 2 9 2 0\nBills API 21 21 31 57 13 0\nGenome Nexus 23 23 0 44 10 0\nBingMap-Route 14 14 0 162 1 10\nGitLab-Branch 9 9 0 88 7 1\nGitLab-Commit 15 15 0 135 13 2\nGitLab-Groups 17 17 0 152 15 0\nGitLab-Issues 27 27 0 239 22 9\nGitLab-Project 31 31 0 295 29 4\nGitLab-Repository 10 10 0 101 8 2\nAverage 17.3 17 6.9 115.8 11.8 2.3\nservice with an OAS file that lies beyond the scope of GPT’s\nknowledge, ensuring fair comparison (4) they represent a di-\nverse set of services from different domains with various status\ncodes, operation dependencies, and parameter constraints.\nTable I provides a summary of the services used in our ex-\nperiments. OP is the number of operations defined in the OAS\nfile for the service. 2xx SC and 4xx SC denote the counts of 2xx\nstatus codes and 4xx codes for all operations, respectively. Par\nis the total number of parameters, including those within the\nrequest bodies of operations. DepOps describes the number of\noperations with dependencies, and IntOps specifies the number\nof operations with at least one inter-parameter constraint.\nV. P ERFORMANCE ON TEST COVERAGE (RQ1)\nA. Experimental Methodology\nIn the experiment, we applied both RTG (version 23.09)\nand our approach to all 12 services in the collected dataset\n(Table I). For each service, we analyzed the results of test\ncase execution, with a specific focus on the final response\nstatus code of each generated test case.\nTo address RQ1, we formulated as follows:\n2xx Coverage : It evaluates test coverage for successful\nresponses by calculating the ratio of total 2xx response codes\ndocumented and triggered by at least one test case to the\nnumber of documented status codes within the 2xx range.\n4xx Coverage : This assesses test coverage of failure re-\nsponses by calculating the ratio of total 4xx response codes\ndocumented and triggered by at least one test case to the\nnumber of documented status codes within the 4xx range.\nOverall Coverage : This measures the ratio of total actual\nresponse codes to the total documented status codes.\nAverage: We determine the average coverage for each\nmeasure by calculating the ratio of the total actual response\ncodes across all services to the total number of status codes\ndocumented in the OAS files.\nB. Experimental Results\nTable II displays the coverage result. Our observations show\nan enhancement in the average coverage metrics over the state-\nof-the-art RTG. Specifically, we noted a 15.7% increase in\nTABLE II\nDOCUMENTED STATUS CODE COVERAGE (RQ1)\nREST service\nOverall\ncoverage (%)\n2xx\ncoverage (%)\n4xx\ncoverage (%)\nRTG KAT RTG KAT RTG KAT\nProShop 67.4 87 62.5 100 70 80\nPetstore 54 74.3 66.7 100 50 55\nCanada Holidays 100 100 100 100 100 100\nBills API 71.2 82.7 66.7 81 74.2 83.9\nGenome Nexus 78.3 100 78.3 100 - -\nBingMap-Route 28.6 78.6 28.6 78.6 - -\nGitLab-Branch 55.6 77.8 55.6 77.8 - -\nGitLab-Commit 20 20 20 20 - -\nGitLab-Groups 52.9 58.8 52.9 58.8 - -\nGitLab-Issues 40.7 63 40.7 63 - -\nGitLab-Project 67.7 74.2 67.7 74.2 - -\nGitLab-Repository 40 40 40 40 - -\nAverage 59.2 74.9 56.4 74.5 67.5 75.9\nStandard deviation 21.4 22.3 21.2 24.5 17.8 16.1\noverall coverage, a 18.1% improvement in 2xx coverage, and\na 8.4% increase in 4xx coverage, on average, over RTG.\nWe observed that for the BingMap Route service, which has\nthe highest number of operations containing inter-parameter\ndependency constraints, KAT exhibits a high improvement\nof up to 50% in both overall coverage and 2xx coverage.\nUnlike RTG, which assigned values without considering inter-\nparameter dependency constraints, KAT, powered by GPT, ef-\nfectively identifies and accommodates these constraints, gener-\nating valid test data that align with the detected constraints. In\nGitLab subsystems, particularly GitLab Issues, which features\nthe highest number of operations containing inter-parameter\ndependency constraints, KAT has shown significant improve-\nments in both overall coverage and 2xx coverage, achieving\nup to a 22.3% increase. In the services characterized by a\nhigh level of operation dependency, it attains a 100% 2xx\ncoverage for ProShop, outperforming RTG, which achieves\n62.5% coverage. This result validates our design intuition on\ncapturing better dependencies, leading to better coverage.\nSeveral cases correctly identify sequences to test an op-\neration, but the responses of subsequent operations often\nprove inadequate to support requests for the next operation,\nespecially when responses are empty or there are mismatches\nbetween the operation’s implementation and its specification.\nThis issue is prominent in certain operations within GitLab’s\nservices (GitLab Commit, GitLab Repository) or the Bills API,\nleading to incomplete testing sequences.\nVI. T EST GENERATION EFFICIENCY (RQ2)\nA. Experimental Methodology\nTo answer RQ2, we devised a measure to assess the effi-\nciency of test case generation:\nEfficiency scoreR = No. TCs actually covering SC in R\nNo. TCs generated to cover SC in R\nwith R: 2xx or 4xx range.\nNo. TCs actually covering SC in R : the number of test\ncases that genuinely cover status codes (SC in Table I) in the\n8\nTABLE III\nTEST CASE GENERATION EFFICIENCY (RQ2)\nREST service\nNo. test cases\ngenerated to cover\n2xx SC\nNo. test cases\ngenerated to cover\n4xx SC\nNo. test cases\nactually covering\n2xx SC\nNo. test cases\nactually covering\n4xx SC\n2xx score (%) 4xx score (%)\nRTG KAT RTG KAT RTG KAT RTG KAT RTG KAT RTG KAT\nProShop 711 19 569 190 10 16 21 24 1.4 84.2 3.7 12.6\nPetstore 707 24 956 276 10 15 10 11 1.4 62.5 1 4\nCanada Holidays 6 6 2 2 6 6 2 2 100 100 100 100\nBills API 1731 132 232 376 14 17 23 26 0.8 12.9 9.9 6.9\nGenome Nexus 600 36 - - 18 23 - - 3 63.9 - -\nBingMap-Route 1413 35 - - 4 11 - - 0.3 31.4 - -\nGitLab-Branch 704 105 - - 5 7 - - 0.7 6.7 - -\nGitLab-Commit 4256 267 - - 3 3 - - 0.1 1.1 - -\nGitLab-Groups 3731 650 - - 9 10 - - 0.2 1.5 - -\nGitLab-Issues 5674 654 - - 11 17 - - 0.2 2.6 - -\nGitLab-Project 2008 372 - - 21 23 - - 1 6.2 - -\nGitLab-Repository 969 315 - - 4 4 - - 0.4 1.3 - -\nAverage 1875.8 217.9 439.8 211 9.6 12.7 14 15.8 0.5 5.8 3.2 7.5\nR range. In an operation, we consider its multiple test cases\nreturning the same status code value as one.\nNo. TCs generated to cover SC in R : the number of test\ncases generated to encompass status codes (SC in Table I) in\nthe R range. This variable calculates the minimum number of\ntest cases required to cover SC in the R range, updating the\ncount if new status codes in the same range are covered.\nThe higher coverage with the lower number of generated\ntest cases implies a higher efficiency score.\nAverage: we determine average coverage by dividing the\ntotal number of test cases that actually cover status codes\nwithin the R range, across all services, by the total number\nof test cases generated to cover status codes within that range.\nB. Experimental Results\nTable III illustrates that the test suite generated by KAT\ncomprises fewer instances where the expected coverage is not\nachieved compared to RTG. Notably, with the ProShop service,\nKAT successfully covers 84.2% of the 19 test cases designed\nfor 2xx status codes, while RTG achieves successful coverage\nfor 1.4% out of the 711 test cases it generates. Moreover,\nKAT shows coverage for 4xx status codes in 12.6% of the 190\ngenerated test cases, in contrast to RTG’s 3.7% success rate out\nof the 569 test cases it generates. In general, KAT generates\nsignificantly fewer test cases for each coverage type compared\nto RTG, yet it improves coverage for each service, except for\nCanada Holidays, where a basically valid value assigned to\nthe required parameter can result in a successful response.\nOverall, KAT enhances the efficiency in generating test\ncases, achieving a 5.3% improvement for 2xx status codes and\na 4.3% improvement for 4xx status codes. These results affirm\nthat KAT enhances the efficiency of test case generation, with\nthe generated test cases considering operation dependencies\nand aligning request data with parameter constraints.\nVII. F AILURE DETECTION (RQ3)\nA. Experimental Methodology\nErrors detection: the number of errors, which is the count\nof requests that result in server error status codes (5xx).\nTABLE IV\nFAILURE DETECTION (RQ3)\nREST service 500 errors Undocumented\nstatus codes Mismatches\nRTG KAT RTG KAT RTG KAT\nProShop 7 6 0 0 21 14\nPetstore 9 8 11 13 15 13\nCanada Holidays 0 0 2 2 7 5\nBills API 0 1 0 0 26 26\nGenome Nexus 0 0 15 14 14 16\nBingMap-Route 4 5 31 32 6 6\nGitLab-Branch 0 0 15 19 4 5\nGitLab-Commit 1 0 19 21 3 0\nGitLab-Groups 1 1 26 28 5 3\nGitLab-Issues 1 1 37 48 8 3\nGitLab-Project 0 1 50 53 8 0\nGitLab-Repository 0 0 15 15 2 3\nTotal 23 23 221 245 119 94\nUndocumented status codes detection : the number of\nresponse status codes that are absent from the OAS files.\nMismatches detection : we rely on trustworthy OAS files.\nSome previous tools neglected inter-parameter dependencies,\nleading to false-positive test cases for 2xx status codes. These\nfalse-positives occur when the tools listed them as failure cases\nto test 2xx status codes. Instead, KAT leverages these cases\nto improve the accuracy of testing 4xx status codes, resulting\nin more reliable coverage for 4xx status codes while allowing\nprecise calculation of 2xx status code coverage.\nB. Experimental Results\nTable IV shows an equal number of errors detected by both\nRTG and KAT. The variation in error count between each\nservice does not surpass one error.\nIn the realm of undocumented status code detection, KAT\nsurpasses RTG, detecting 24 more status codes than RTG.\nSubsequent investigation underscores the prowess of our ap-\nproach in identifying previously undocumented status codes,\na proficiency that is significantly enhanced when operational\ndependencies are accurately detected. Consequently, our ap-\nproach excels at detecting undocumented status codes, encom-\n9\npassing 304, 400, 422 and more. These findings underscore\nthe distinct advantage of our approach in the detection of\npreviously undocumented status codes.\nTable IV reveals a notable difference in the average number\nof mismatches detected between RTG and our approach, KAT.\nRTG detects totally 119 mismatches, whereas KAT identifies\n94 mismatches. The disparity in mismatch detection can be\nattributed to several factors. RTG sometimes fails to consider\ninter-parameter dependency constraints or overlooks fixed pa-\nrameter values, resulting in the generation of false-positive test\ncases designed to cover 2xx status codes. Additionally, RTG\nmay incorrectly detect crucial input parameters when subjected\nto mutation strategies, leading to the creation of false-positive\ntest cases targeting 4xx status codes.\nRegarding RQ3, RTG and KAT show similar capabilities\nin detecting errors. KAT adeptly detects status codes absent\nfrom the OAS file by considering both operational and inter-\nparameter dependencies. There is also a notable reduction in\nfalse-positive test cases from KAT.\nVIII. T HREATS TO VALIDITY\nThere are concerns that may affect the validity of our results.\nInternal validity. One threat is the hallucination issue of\nGPT that results in inconsistent outputs when generating ODG,\nvalidation scripts, test cases, and test data. We alleviated this\nthreat by using detailed and narrow instructions in prompts\nand by setting GPT’s temperature parameter to zero to control\nthe model’s randomness. In addition, generated test cases were\nrun against the working service as a method to validate and\nreduce the variation in GPT’s outputs.\nAnother threat is concerned with the lack of certainty in test\noracles. By using the real-world services as black-box, we do\nnot know whether the specification or the service implementa-\ntion is accurate. It is possible that the specification is not up-\nto-date, or the service has errors. Due to this uncertainty, we\nreported mismatches in status code between the specification\nand implementation rather than actual errors found.\nExternal validity. Although we evaluated the approaches\nusing real-world applications, they may not represent the gen-\neral population of API services. We mitigated this by choosing\na diverse set of services, in terms of the application domain,\nnumber of operations and parameters, operation dependencies,\nbusiness rules, and details of operation and parameter descrip-\ntions. They are publicly accessible, and many of them were\nevaluated in previous studies. Thus, the use of these services\nallows replication and cross-examination in future studies.\nIX. R ELATED WORK\nConsidering various surveys [13], [21], [22], [23], [24], [25],\n[26], previous approaches to RESTful APIs testing can be\nclassified into two primary directions: black-box testing [1],\n[2], [4], [6], [7], [16], [27] and white-box testing [28], [29],\n[30], [31], [32], [33], [34], [35].\nResource dependencies. Viglianisi et al. introduced the\nuse of ODG for modeling data dependencies among opera-\ntions using OpenAPI specifications [1]. Liu et al. presented\nthe RESTful-service Property Graph (RPG) to provide de-\ntailed modeling of producer-consumer dependencies, including\nschema property equivalence relations. RPG can describe\nRESTful service behaviors and dynamically update itself with\nexecution feedback [4]. Atlidakis et al. proposed RESTler to\ninfer dependencies among request types declared in Swagger\nusing a test-generation grammar [6]. Tree-based approaches,\nlike linkage model trees [36] and API trees [19], were also\nconsidered. In contrast, we focus on constructing the ODG,\nleveraging natural language descriptions in specifications to\nguide LLMs for support and revision.\nInter-parameter dependency. Martin-Lopez et al. pro-\nposed Inter-parameter Dependency Language (IDL), a domain-\nspecific language for analyzing dependencies among input\nparameters [37]. RESTest added inter-parameter dependencies\nto OpenAPI specifications using the IDL [37], implementing\nConstraint-Based Testing mode [2]. Wu et al. applied NLP\nfor extracting inter-parameter constraints using pattern-based\nmethods [16]. These approaches formalize inter-parameter de-\npendencies, often requiring manual addition of IDL language\nor pre-defined NLP patterns, which is time-consuming and\npotentially error-prone. In contrast, Kim et al. used Reinforce-\nment Learning to explore dependencies [14], Mirabella et al.\ntrained a deep-learning model [38], while we leverage LLMs\nto extract inter-parameter dependencies from OAS files.\nNatural language processing in API testing. Kim et al.\nintroduced NLPtoREST, using NLP techniques to add supple-\nmentary OpenAPI rules from the human-readable specifica-\ntion [15]. Wanwarang et al. applied NLP to extract concepts\nassociated with labels, used to query knowledge bases for input\nvalues [39]. ARTE explored various API elements, generating\nrealistic test inputs by querying knowledge bases [5]. Liu\net al. introduced RESTInfer, a two-phase approach inferring\nparameter constraints from RESTful API descriptions [40].\nX. C ONCLUSION\nKAT is an AI-driven method for automated black-box test-\ning of RESTful APIs. It uses a large language model and ad-\nvanced prompting techniques to identify dependencies among\noperations, input parameters, and between operations and their\nparameters. KAT incorporates ODG construction in test script\ncreation, detects inter-parameter dependencies, and considers\nparameter constraints to generate test data on target APIs.\nWe evaluated KAT using 12 real-world RESTfull API\nservices. The results show that it can automatically generate\ntest cases and data to improve status code coverage, detect\nmore undocumented status codes, and reduces more false\npositives in these services compared to RTG. It improves an\noverall status code coverage by 15.7% over RTG, increasing\nthe coverage from 59.2% status codes by RTG to 74.9%.\nACKNOWLEDGMENTS\nVu Nguyen is partially supported by Vietnam’s NAFOSTED\ngrant NCUD.02-2019.72. Tien N. Nguyen is supported in\npart by the US NSF grant CNS-2120386 and the NSA grant\nNCAE-C-002-2021.\n10\nREFERENCES\n[1] E. Viglianisi, M. Dallago, and M. Ceccato, “RestTestGen: automated\nblack-box testing of restful APIs,” in 2020 IEEE 13th International\nConference on Software Testing, Validation and Verification (ICST) .\nIEEE, 2020, pp. 142–152.\n[2] A. Martin-Lopez, S. Segura, and A. Ruiz-Cort ´es, “RESTest: Automated\nBlack-Box Testing of RESTful Web APIs,” in Proceedings of the\n30th ACM SIGSOFT International Symposium on Software Testing and\nAnalysis, ser. ISSTA ’21. Association for Computing Machinery, 2021.\n[3] A. Arcuri, “Restful api automated test case generation with evomaster,”\nACM Transactions on Software Engineering and Methodology (TOSEM),\nvol. 28, no. 1, pp. 1–37, 2019.\n[4] Y . Liu, Y . Li, G. Deng, Y . Liu, R. Wan, R. Wu, D. Ji, S. Xu,\nand M. Bao, “Morest: model-based restful api testing with execution\nfeedback,” in Proceedings of the 44th International Conference on\nSoftware Engineering, 2022, pp. 1406–1417.\n[5] J. C. Alonso, A. Martin-Lopez, S. Segura, J. M. Garcia, and A. Ruiz-\nCortes, “ARTE: Automated Generation of Realistic Test Inputs for Web\nAPIs,” IEEE Transactions on Software Engineering , vol. 49, no. 1, pp.\n348–363, 2022.\n[6] V . Atlidakis, P. Godefroid, and M. Polishchuk, “Restler: Stateful rest API\nfuzzing,” in 2019 IEEE/ACM 41st International Conference on Software\nEngineering (ICSE). IEEE, 2019, pp. 748–758.\n[7] N. Laranjeiro, J. Agnelo, and J. Bernardino, “A black box tool for\nrobustness testing of rest services,” IEEE Access , vol. 9, pp. 24 738–\n24 754, 2021.\n[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[9] D. Corradini, A. Zampieri, M. Pasqua, and M. Ceccato, “RestTestGen:\nAn Extensible Framework for Automated Black-box Testing of RESTful\nAPIs,” in 2022 IEEE International Conference on Software Maintenance\nand Evolution (ICSME) . IEEE, 2022, pp. 504–508.\n[10] “Apis.guru,” 2024. [Online]. Available: https://apis.guru/\n[11] D. Corradini, A. Zampieri, M. Pasqua, and M. Ceccato, “Restats: A\ntest coverage tool for RESTful APIs,” in 2021 IEEE International\nConference on Software Maintenance and Evolution (ICSME) . IEEE,\n2021, pp. 594–598.\n[12] T. Vassiliou-Gioles, “A simple, lightweight framework for testing restful\nservices with ttcn-3,” in 2020 IEEE 20th International Conference on\nSoftware Quality, Reliability and Security Companion (QRS-C) . IEEE,\n2020, pp. 498–505.\n[13] M. Kim, Q. Xin, S. Sinha, and A. Orso, “Automated test generation for\nrest apis: No time to rest yet,” in Proceedings of the 31st ACM SIGSOFT\nInternational Symposium on Software Testing and Analysis , 2022, pp.\n289–301.\n[14] M. Kim, S. Sinha, and A. Orso, “Adaptive REST API Testing with\nReinforcement Learning,” arXiv preprint arXiv:2309.04583 , 2023.\n[15] M. Kim, D. Corradini, S. Sinha, A. Orso, M. Pasqua, R. Tzoref-Brill,\nand M. Ceccato, “Enhancing REST API Testing with NLP Techniques,”\nin Proceedings of the 32nd ACM SIGSOFT International Symposium on\nSoftware Testing and Analysis , 2023, pp. 1232–1243.\n[16] H. Wu, L. Xu, X. Niu, and C. Nie, “Combinatorial testing of restful\nAPIs,” in Proceedings of the 44th International Conference on Software\nEngineering, 2022, pp. 426–437.\n[17] K. Yamamoto, “Efficient penetration of API sequences to test stateful\nRESTful services,” in 2021 IEEE International Conference on Web\nServices (ICWS). IEEE, 2021, pp. 734–740.\n[18] S. Karlsson, A. ˇCauˇsevi´c, and D. Sundmark, “QuickREST: Property-\nbased test generation of OpenAPI-described RESTful APIs,” in 2020\nIEEE 13th International Conference on Software Testing, Validation and\nVerification (ICST). IEEE, 2020, pp. 131–141.\n[19] J. Lin, T. Li, Y . Chen, G. Wei, J. Lin, S. Zhang, and H. Xu, “foREST:\nA Tree-based Approach for Fuzzing RESTful APIs,” arXiv preprint\narXiv:2203.02906, 2022.\n[20] “Proshop API,” 2024. [Online]. Available: https://anonymous.4open.\nscience/r/proshop-api-B648\n[21] A. Golmohammadi, M. Zhang, and A. Arcuri, “Testing RESTful APIs: A\nSurvey,” ACM Transactions on Software Engineering and Methodology ,\n2022.\n[22] A. Ehsan, M. A. M. Abuhaliqa, C. Catal, and D. Mishra, “RESTful API\ntesting methodologies: Rationale, challenges, and solution directions,”\nApplied Sciences, vol. 12, no. 9, p. 4369, 2022.\n[23] A. Martin-Lopez, S. Segura, and A. Ruiz-Cort ´es, “Online testing of\nRESTful APIs: Promises and challenges,” in Proceedings of the 30th\nACM Joint European Software Engineering Conference and Symposium\non the Foundations of Software Engineering , 2022, pp. 408–420.\n[24] A. Sharma, M. Revathi et al. , “Automated API testing,” in 2018\n3rd International Conference on Inventive Computation Technologies\n(ICICT). IEEE, 2018, pp. 788–791.\n[25] B. Marculescu, M. Zhang, and A. Arcuri, “On the faults found in rest\nAPIs by automated test generation,” ACM Transactions on Software\nEngineering and Methodology (TOSEM), vol. 31, no. 3, pp. 1–43, 2022.\n[26] A. Martin-Lopez, A. Arcuri, S. Segura, and A. Ruiz-Cort ´es, “Black-\nbox and white-box test case generation for RESTful APIs: Enemies\nor allies?” in 2021 IEEE 32nd International Symposium on Software\nReliability Engineering (ISSRE) . IEEE, 2021, pp. 231–241.\n[27] D. Corradini, A. Zampieri, M. Pasqua, E. Viglianisi, M. Dallago, and\nM. Ceccato, “Automated black-box testing of nominal and error sce-\nnarios in RESTful APIs,” Software Testing, Verification and Reliability,\nvol. 32, no. 5, p. e1808, 2022.\n[28] O. Sahin and B. Akay, “A discrete dynamic artificial bee colony with\nhyper-scout for RESTful web service API test suite generation,” Applied\nSoft Computing, vol. 104, p. 107246, 2021.\n[29] A. Arcuri, “Automated black-and white-box testing of restful APIs with\nevomaster,” IEEE Software, vol. 38, no. 3, pp. 72–78, 2020.\n[30] A. Arcuri and J. P. Galeotti, “Enhancing search-based testing with\ntestability transformations for existing APIs,” ACM Transactions on\nSoftware Engineering and Methodology (TOSEM) , vol. 31, no. 1, pp.\n1–34, 2021.\n[31] ——, “Handling sql databases in automated system test generation,”\nACM Transactions on Software Engineering and Methodology (TOSEM),\nvol. 29, no. 4, pp. 1–31, 2020.\n[32] M. Zhang and A. Arcuri, “Adaptive hypermutation for search-based\nsystem test generation: A study on rest APIs with EvoMaster,” ACM\nTransactions on Software Engineering and Methodology (TOSEM) ,\nvol. 31, no. 1, pp. 1–52, 2021.\n[33] ——, “Enhancing resource-based test case generation for RESTful APIs\nwith SQL handling,” in International Symposium on Search Based\nSoftware Engineering. Springer, 2021, pp. 103–117.\n[34] M. Zhang, B. Marculescu, and A. Arcuri, “Resource-based test case\ngeneration for RESTful web services,” in Proceedings of the genetic\nand evolutionary computation conference , 2019, pp. 1426–1434.\n[35] ——, “Resource and dependency based test case generation for restful\nweb services,” Empirical Software Engineering , vol. 26, no. 4, p. 76,\n2021.\n[36] D. Stallenberg, M. Olsthoorn, and A. Panichella, “Improving test case\ngeneration for rest APIs through hierarchical clustering,” in 2021 36th\nIEEE/ACM International Conference on Automated Software Engineer-\ning (ASE). IEEE, 2021, pp. 117–128.\n[37] A. Martin-Lopez, “Automated analysis of inter-parameter dependencies\nin web APIs,” in Proceedings of the ACM/IEEE 42nd International\nConference on Software Engineering: Companion Proceedings , 2020,\npp. 140–142.\n[38] A. G. Mirabella, A. Martin-Lopez, S. Segura, L. Valencia-Cabrera, and\nA. Ruiz-Cort ´es, “Deep learning-based prediction of test input validity\nfor restful APIs,” in 2021 IEEE/ACM Third International Workshop on\nDeep Learning for Testing and Testing for Deep Learning (DeepTest) .\nIEEE, 2021, pp. 9–16.\n[39] T. Wanwarang, N. P. Borges Jr, L. Bettscheider, and A. Zeller, “Testing\napps with real-world inputs,” in Proceedings of the IEEE/ACM 1st\nInternational Conference on Automation of Software Test , 2020, pp. 1–\n10.\n[40] Y . Liu, “RESTInfer: automated inferring parameter constraints from\nnatural language RESTful API descriptions,” in Proceedings of the 30th\nACM Joint European Software Engineering Conference and Symposium\non the Foundations of Software Engineering , 2022, pp. 1816–1818.\n11"
}