{
    "title": "Large Language Models As Annotators: A Preliminary Evaluation For Annotating Low-Resource Language Content",
    "url": "https://openalex.org/W4392669965",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2140998463",
            "name": "Savita Bhat",
            "affiliations": [
                "Indian Institute of Technology Hyderabad"
            ]
        },
        {
            "id": "https://openalex.org/A2146554912",
            "name": "Vasudeva Varma",
            "affiliations": [
                "Indian Institute of Technology Hyderabad"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4287758476",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4362655923",
        "https://openalex.org/W4362679631",
        "https://openalex.org/W4366733439",
        "https://openalex.org/W4321392130",
        "https://openalex.org/W3046357466",
        "https://openalex.org/W4385570009",
        "https://openalex.org/W3022569409",
        "https://openalex.org/W4327811957",
        "https://openalex.org/W4323697401",
        "https://openalex.org/W4389519239",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W4319793302",
        "https://openalex.org/W4285178342",
        "https://openalex.org/W3099919888",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4317553041",
        "https://openalex.org/W4389519817",
        "https://openalex.org/W2916132663",
        "https://openalex.org/W4385571124",
        "https://openalex.org/W4362515116"
    ],
    "abstract": "The process of collecting human-generated annotations is time-consuming and resource-hungry. In the case of low-resource (LR) languages such as Indic languages, these efforts are more expensive due to the dearth of data and human experts. Considering their importance in solving downstream applications, there have been concentrated efforts exploring alternatives for human-generated annotations. To that extent, we seek to evaluate multilingual large language models (LLMs) for their potential to substitute or aid human-generated annotation efforts. We use LLMs to re-label publicly available datasets in LR languages for the tasks of natural language inference, sentiment analysis, and news classification. We compare these annotations with existing ground truth labels to analyze the efficacy of using LLMs for annotation tasks. We observe that the performance of these LLMs varies substantially across different tasks and languages. The results show that off-the-shelf use of multilingual LLMs is not appropriate and results in poor performance in two of the three tasks.",
    "full_text": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 100–107\nNovember 1, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models As Annotators: A Preliminary Evaluation For\nAnnotating Low-Resource Language Content\nSavita Bhat†‡ and Vasudeva Varma‡\n†TCS Research, ‡IIIT Hyderabad\nsavita.bhat@tcs.com, vv@iiit.ac.in\nAbstract\nThe process of collecting human-generated\nannotations is time-consuming and resource-\nhungry. In the case of low-resource (LR) lan-\nguages such as Indic languages, these efforts\nare more expensive due to the dearth of data\nand human experts. Considering their impor-\ntance in solving downstream applications, there\nhave been concentrated efforts exploring alter-\nnatives for human-generated annotations. To\nthat extent, we seek to evaluate multilingual\nlarge language models (LLMs) for their poten-\ntial to substitute or aid human-generated anno-\ntation efforts. We use LLMs to re-label publicly\navailable datasets in LR languages for the tasks\nof natural language inference, sentiment analy-\nsis, and news classification. We compare these\nannotations with existing ground truth labels to\nanalyze the efficacy of using LLMs for annota-\ntion tasks. We observe that the performance of\nthese LLMs varies substantially across differ-\nent tasks and languages. The results show that\noff-the-shelf use of multilingual LLMs is not\nappropriate and results in poor performance in\ntwo of the three tasks.\n1 Introduction\nTraditionally, compiling annotations using human\nexperts has been the primary step in formulating\na supervised solution 1 for various tasks such as\nsentiment analysis (Rosenthal et al., 2017), bot de-\ntection (Fagni et al., 2021), and inference (Bowman\net al., 2015; Wang et al., 2018). The process of col-\nlecting human-generated annotations is often time-\nintensive and resource-hungry. Specifically, in the\ncase of LR languages, these efforts are more expen-\nsive due to a lack of quality data and human experts.\nTherefore, alternatives to human-generated labels\nare being actively explored (Cruz and Cheng, 2020;\nMagueresse et al., 2020).\nRecent LLMs2, such as ChatGPT, demonstrate\n1https://en.wikipedia.org/wiki/Supervised_learning\n2LLMs and generative models are used interchangeably.\nimpressive performance in various NLP applica-\ntions such as summarization, classification, and\ntext generation (Liu et al., 2023). Furthermore,\ninteresting use cases and applications using these\ngenerative models have been explored and reported\n(Zhao et al., 2023). The research community is\ncurious to know how close LLMs are to human\nexperts and annotators. Accordingly, (Guo et al.,\n2023) conduct extensive evaluations in a question-\nanswering setup. In (Zhu et al., 2023), ChatGPT\nis evaluated in the context of reproducing human-\ngenerated label annotations in social computing\ntasks. Similar studies for misinformation in (Bang\net al., 2023) and hate speech in (Huang et al., 2023)\nhave considered ChatGPT for annotations. Addi-\ntionally, several works (Kuzman et al., 2023; Gao\net al., 2023; Wang et al., 2023) compareChatGPT’s\nannotation and evaluation performance with human\nexperts.\nThe point to note is that most of these efforts fo-\ncus on high-resource (HR) languages like English.\nIn reality, these HR languages are not recognized\nas the native languages for most of the world’s\npopulation. For example, people in India prefer to\ninteract in one of the Indic languages despite of\nbeing literate in English. These Indic languages\nare generally categorized as low-resource (LR) lan-\nguages because of the unavailability of quality data\nsources (Lai et al., 2023). Considering India as the\nmost populated country3 in the world, it is essen-\ntial to evaluate current multilingual LLMs in the\ncontext of LR languages like Indic languages. Sec-\nondly, besides ChatGPT, other multilingual LLMs\nlike mT0 and BLOOMZ must also be evaluated for\nsuch use cases.\nTo this extent, we primarily explore the possibil-\nity of using multilingual LLMs as a substitute for\nhuman annotators. Specifically, we focus on low-\nresource languages such as Indic languages and\ncompare the LLM-generated annotations with the\n3https://tinyurl.com/2tz9d3u2; Last accessed: 09/06/2023\n100\nground truth human-generated labels. To the best\nof our knowledge, this is the first work to evaluate\nthe efficacy of LLMs as annotators for LR Indic\nlanguages. We examine three LLMs- ChatGPT,\nmT0, and BLOOMZ, for three tasks- document\nclassification, sentiment analysis, and natural lan-\nguage inference. The main observations from our\nexperiments are as follows:\n1. All three LLMs perform well in identifying\nsentiments. Surprisingly, ChatGPT shows\nslightly worse capability for simple classifi-\ncation, parsing, and inference tasks. It does\nremarkably well in a more complex task of\nnews category classification.\n2. The performance of these LLMs, in cor-\nrectly annotating the samples, is not uni-\nform and varies across different tasks and dif-\nferent LR languages. This observation de-\nmands more informative, clear, and better\nprompts/instructions while using generative\nmodels as annotators.\n3. Fine-tuned baseline models have superior per-\nformance in most of the languages and tasks,\nhighlighting the need for focused task-specific\ntraining.\n4. ChatGPT is the only LLM that often provides\na justification with the answer, which helps in\nunderstanding annotation choices.\n2 Methodology\nWe follow a comparative approach to study the\ndifferences between human-generated and LLM-\ngenerated annotations for Indic languages. Under\nthis premise, we consider three broad categories of\ntasks and relevant datasets: 1) WNLI - Winograd\ninference task involving inference based on a given\ncontext, 2) SA - identifying sentiment for a given\ntext, and 3) NewsCLS - categorizing given news\ntext. We consider appropriate prompting strategies\nto simulate the manual annotation process. In the\nfollowing subsections, we describe the multilingual\nLLMs used for annotations (Section 2.1), Datasets\nused for the three tasks (Section 2.2), and our ap-\nproach for the annotation process (Section 2.3).\n2.1 LLMs\nWe explore the following LLMs in the context of\nIndic languages for our annotation experiments.\nThe choice of LLMs was guided by the following\nconstraints: 1) LLM should be trained on multilin-\ngual data sources, including Indic languages, and 2)\nLLM training consists of multiple tasks converted\nto text-to-text format. This way, we make sure that\nthe strategies, i.e., the instructions to the selected\nLLMs, do not have large variations and are similar\nin nature.\nChatGPT (GPT-3.5) is known to be created by\nfinetuning the GPT-3.5 variant using reinforcement\nlearning from human feedback ( RLHF) (Chris-\ntiano et al., 2017). We evaluate this model using\ngpt-3.5-turbo API between 5th September to 6th\nSeptember 2023. Even though there is no definite\ninformation released by OpenAI on this model, it is\nassumed that‘CommonCrawl’ corpus, which con-\ntains some percentage of data in Indic languages,\nis a part of the training data for this model 4.\nBLOOMZ (Muennighoff et al., 2022) is an open-\nsource multilingual LLM. Multitask prompted fine-\ntuning (MTF) is applied to pretrained BLOOM\nLLM (Scao et al., 2022) to build the fine-tuned\nvariant, BLOOMZ. BLOOMZ family consists of\nmodels with 300M to 176B parameters and sup-\nports 59 languages.\nmT0 (Muennighoff et al., 2022) is the fine-tuned\nvariant of pretrained multilingual mT5 language\nmodel. Like BLOOMZ, MTF is applied to mT5\nto produce mT0 with model variants ranging from\n300M to 176B.\nBLOOMZ and mT0 families have been trained on\ndatasets, xP3 and xP3MT, consisting of 13 training\ntasks in 46 languages. xP3 uses English prompts,\nwhereas xP3MT uses prompts machine-translated\nto 20 languages. Indic languages constitute a small\npart of the training data for both of these model\nfamilies.\n2.2 Datasets\nWe consider 11 Indic languages as LR languages\nfor our experiments. It should be noted that not all\nof these languages have quality datasets identified\nand compiled for certain tasks. We choose the\ndatasets and tasks with maximum representation\nfrom Indic languages. Out of 11 Indic languages,\nHindi is the only medium-resource (MR) language,\nwhereas Punjabi, Oria, and Assamese are classified\nas extremely low-resource (XR) languages. The\nremaining Indic languages are categorized as LR\n4https://en.wikipedia.org/wiki/GPT-3. Last accessed on\n6th September 2023\n101\nFigure 1: Prompt templates for annotation. ChatGPT responses show the justifications provided for annotation\nchoices. mT0 and BLOOMZ do not provide any justification.\nlanguages (Lai et al., 2023). This categorization is\nbased on their representation in the CommonCrawl\ncorpus. IndicNLPSuite (Kakwani et al., 2020)\nintroduced NLP resources for Indic languages. We\nchoose the following datasets from its IndicGLUE\nevaluation benchmark. The datasets are selected\nbased on two criteria, 1) the datasets are annotated\nby humans, and 2) the dataset covers as many of\nIndic languages as possible.\nSentiment Analysis (SA) We use IndicSenti-\nment5 dataset from Huggging Facedatasets. Each\nexample contains a review text and corresponding\nsentiment. As per the dataset card, the annota-\ntions are expert-generated. The input records in the\ndataset are translated into various Indic languages\n(Doddapaneni et al., 2023). The task is to identify\nthe sentiment of a given text.\nNews Category Classification (NewsCLS) The\ntask is to categorize a news article into a given\nset of topics. This dataset is compiled by crawl-\ning regional news websites. We assume that the\ncategories are manually assigned to the news arti-\ncles based on the URLs while publishing on the\nwebsite.\nWinograd NLI (WNLI) We use the Indic ver-\nsion of WNLI dataset (Kakwani et al., 2020). The\ndataset is created and verified by experts by trans-\nlating the original dataset into 3 Indic languages\n(mr, hi, gu). Each example consists of a pair of\nsentences where the second sentence is constructed\n5https://huggingface.co/datasets/ai4bharat/IndicSentiment\nfrom the first sentence by replacing an ambiguous\npronoun with a possible referent within the sen-\ntence. The task is to predict if the original sentence\nentails the second sentence.\n2.3 Annotation\nWe attempt to re-annotate the data samples for each\ntask and dataset using ChatGPT, BLOOMZ, and\nmT0. We use PromptSource toolkit (Bach et al.,\n2022) to identify candidate prompts for our tasks.\nWe experiment with relevant prompts and choose\nthe ones appropriate for chosen LLMs and tasks.\nAlthough the context is given in Indic languages,\nthe prompts are in English. Example prompts are\npresented in Figure 1.\nSA For the SA task, we ask the LLMs to identify\nthe for a given context as follows:\nContent: {text content}\nWhat is the sentiment expressed in the\ngiven text?\nwhere {text content} is the review text in a LR\nlanguage.\nNewsCLS This task consists of categorizing\ngiven news content in one of the categories. It\nis observed that the news records in every language\nhave a certain closed set of categories. We use\nthese sets to modify the prompt template as below:\nContent: {news content} Is this news\narticle regarding {categories}?\nwhere {news content} is the news text and\n{categories} is the set of candidate categories.\n102\nTask Language as bn gu hi kn ml mr or pa ta te\nmT0 0.910 0.915 0.911 0.931 0.911 0.898 0.929 0.763 0.856 0.947 0.890\nBLOOMZ 0.927 0.955 0.944 0.971 0.899 0.939 0.942 0.938 0.927 0.940 0.891\nSA ChatGPT 0.856 0.8761 0.845 0.909 0.839 0.843 0.836 0.772 0.846 0.822 0.768\nmBERT 0.57 0.68 0.66 0.73 0.68 0.68 0.69 0.49 0.75 0.71 0.66\nindicBERTplus0.931 0.93 0.933 0.933 0.928 0.932 0.938 0.931 0.933 0.936 0.937\nTable 1: Sentiment Analysis: Language-wise weighted F1-score for mT0, BLOOMZ, and ChatGPT. The bold\nnumber indicates the highest value per language, whereas the red colour denotes the highest performance amongst\nmultilingual LLMs for every language.\nWNLI Since the task is to identify entailment\ngiven a context and secondary sentence, we con-\nsider the prompt where the entailment is explored\nthrough a true/false question. The prompt used is\nas follows:\nContext: {sentence1}\nQuestion: {sentence2}\nTrue or False?\nwhere {sentence1} and {sentence2} are the\ncontext and secondary sentence respectively.\n3 Experimental Setup\nAs mentioned earlier, we primarily use three tasks\nand corresponding datasets to evaluate if LLMs\ncan replace or to some extent, aid the manual an-\nnotation efforts. We formulate the annotation task\nas a zero-shot inference task. We compare LLM\nannotations with the ground truth labels. We con-\nsider ‘test’ split from all the datasets to ensure no\ndata leakage. The ‘ gpt-3.5-turbo’ API for Chat-\nGPT is paid and under a constrained usage policy.\nHence, we use a subset of samples for the Chat-\nGPT experiments. For mT0 and BLOOMZ, we use\nthe entire split whenever possible. The dataset dis-\ntributions are as follows: We use the entire ‘ test’\nsplit distributed across various Indic languages for\nWNLI and NewsCLS tasks, totaling to 284 and\n5986 data samples, respectively. For sentiment\nanalysis, we randomly select a total of 2862 sam-\nples spread across 11 languages with approximately\n250 samples each, considering the budget for the\npaid experiments with the ‘gpt-3.5-turbo’ API. We\nuse the following abbreviations for languages: as\n(Assamese), bn (Bengali), gu (Gujarati), hi (Hindi),\nkn (Kannada), ml (Malayalam), mr (Marathi), or\n(Odia), pa (Punjabi), ta (Tamil), and te (Telugu).\nFor ChatGPT, we use the official OpenAI API\n(gpt-3.5-turbo) with default settings to annotate the\nsamples. Similarly, we use Hugging Face mod-\nels and tokenizers for mT0 and BLOOMZ LLMs\nfor annotations. Due to infrastructure constraints,\nwe use the ‘ mT0-large’ model for mT0 and the\n‘BLOOMZ-1b1’ model for BLOOMZ experiments.\nNo training is involved since we consider zero-\nshot inferencing with the off-the-shelf model, i.e.,\na zero-shot setting. For comparison, we consider\nstate-of-the-art baselines finetuned for these spe-\ncific tasks. For Sentiment Analysis, we use results\nreported in (Doddapaneni et al., 2023), while re-\nsults from (Kakwani et al., 2020) are considered as\nbaseline for WNLI and NewsCLS tasks.\nWe use weighted-precision, weighted-recall, and\nweighted-F1 metrics from sklearn library for eval-\nuation. We also report macro-average calculated\nacross all languages to indicate the correctness of\nlabels for a specific task.\n4 Results & Analysis\nThis section presents the overview of the annotation\nexperiments for three tasks and three LLMs. Rep-\nresentative detailed language-wise performance re-\nsults (F1 measure) for each task are listed in Table\n1,2, and 3. Table 4 describes correctly labeled in-\nstances across different tasks and LLMs.\nSA - Superior performance in zero-shot infer-\nence All three LLMs perform well in identifying\nsentiment for a given textual content. It is inter-\nesting to see that ChatGPT is ranked last amongst\nLLMs in most cases. In 9 out of 11 languages,\nBLOOMZ shows superior or at-par performance as\ncompared to baseline models. It is encouraging to\nsee good zero-shot inference with just a single in-\nstruction. We expect even better results with more\ninformed and aligned prompting strategies.\nSA - Additional information and justification\nIt should be noted that mT0 and BLOOMZ con-\nsider two sentiments ( Positive and Negative) as\ncandidates for the assignment. In contrast, Chat-\nGPT considers three sentiments by default (Neutral\nas additional sentiment). After manual validation,\nwe observe that the records are indeed of neutral\n103\nsentiment. Secondly, ChatGPT also provides rea-\nsonable justification for the suggested annotations.\nThese justifications are useful in providing clear\ninstructions for training the crowd-workers for an-\nnotations. We believe that this additional informa-\ntion and justifications will help in aligning expert-\ngenerated and machine-generated annotations.\nNewsCLS - Complex tasks need focused train-\ning and instructions To introduce more com-\nplexity in the classification task, we consider News\ncategory classification task. This is a multi-class\nproblem with a very fuzzy class separation. Out of\nthree LLMs, ChatGPT performs better in 6 out of 7\nlanguages. The othe two LLMs demonstrate varied\nperformance ranging from low to high accuracy.\nAdditionally, all these LLMs lags behind the base-\nlines and fail to reproduce the human-generated\nannotations. As can be seen the task-specific fine-\ntuning boosts the model performance. We believe\nthat the prompts/instructions given to the LLMs\nwere simple and unable to fully specify the com-\nplexity and requirements of the task. Accordingly,\nwe conclude that complex tasks need more focused\nand aligned instructions to help the LLM in anno-\ntations.\nNewsCLS - Appropriate corrections for noisy\ndata samples We note that the annotations for\nthis dataset are noisy, and a few records can be\nassigned to multiple categories instead of just one\ncategory. We believe that this may have affected\nthe evaluation using automatic metrics. It is also\nobserved that only ChatGPT looks beyond the can-\ndidate categories and suggests appropriate alternate\ncategories that are valuable in annotation efforts.\nOn manual validation, we observe that these sug-\ngestions are indeed relevant and useful.\nWNLI - Reasoning and inference tasks are\nharder All models, including the three LLMs\nand baselines, show average performance in recre-\nating the annotations for the inferencing task. It\nis interesting to note that the zero-shot inferencing\nwith multilingual LLMs comes close to the perfor-\nmance of finetuned baseline models. In general,\nthe reasoning and inferencing tasks require natural\nlanguage understanding and hence are more com-\nplex to train for. With LR languages, the problem\nbecomes harder, considering the unavailability of\ntraining and annotation resources. We believe that\nclear prompts and supplementary explanations will\nhelp in improving the performance.\nWNLI - Justification may help in language un-\nderstanding It is observed that only ChatGPT\nprovides relevant justification for the inference in\nmost cases. These justifications often explain the\ndecision and the logical reasoning behind that deci-\nsion. These justifications are useful in understand-\ning the annotation choices and, hence, can serve as\na guiding tool for better annotation alignment.\nAnnotation Correctness Percentages of cor-\nrectly labeled samples for the three tasks and three\nLLMs are listed in Table 4. This is the macro av-\nerage across all relevant languages for a particular\ntask. It is interesting to see that ChatGPT performs\nfar worse than mT0 and BLOOMZ in the relatively\nsimpler task of sentiment analysis. In NewsCLS,\nall three LLMs have poor showing, whereas in\nWNLI, only ChatGPT seems to have more than\na chance performance. In the case of mT0 and\nBLOOMZ, it is difficult to conclude that the per-\nformance is not random. The performance in indi-\nvidual languages documented in Table 3 does not\nseem to be a by-chance result. However, further in-\nvestigation with more samples and varied prompts\nis required to understand this result.\nLLMs for LR languages As mentioned ear-\nlier, LR languages occupy a small portion of the\nCommonCrawl campus. Consequently, the LLMs\ntrained on this corpus also have a similar small rep-\nresentation in their embeddings, often demonstrat-\ning a limited linguistic understanding of these lan-\nguages. It is reiterated by the F1 score and the cor-\nrectly labeled portion in NewsCLS and WNLI tasks.\nThese tasks require a certain degree of language un-\nderstanding and reasoning capability, which none\nof the three LLMs demonstrate in any Indic lan-\nguage except Hindi.\nLanguage families such as Dravidian (Kannada,\nTamil, Telugu, and Malayalam) and Indo-Aryan\n(Hindi and Marathi) share a lot of commonalities\namong themselves. Despite that, the significant\ndifference in the scores supports the dependence on\nlanguage exposure during training. As can be seen\nfrom the results, the LLMs have different levels of\nunderstanding of these languages, and there seems\nto be no clear winner.\nAnnotations & Justifications We observe anno-\ntations provided by three LLMs, mT0, BLOOMZ,\nand ChatGPT. Only ChatGPT offers a justification\nwhile providing an answer/annotation. These jus-\ntifications often explain the reasoning behind var-\n104\nTask Language bn gu hi ml mr ta te\nmT0 0.20 0.69 0.076 0.739 0.257 0.27 0.292\nBLOOMZ 0.26 0.69 0.18 0.6250 0.32 0.488 0.426\nNewsCLS ChatGPT 0.472 0.757 0.53 0.68 0.522 0.49 0.10\nmBERT 0.80 0.89 0.60 0.82 0.87 0.92 -\nIndicBERT 0.78 0.92 0.74 0.94 0.94 0.96 -\nTable 2: NewsCLS Task: Language-wise weighted F1-score for mT0, BLOOMZ, and ChatGPT. The bold number\nindicates the highest value per language and the red colour denotes the highest performance value amongst the\nmultilingual LLMs for every language.\nTask Language gu hi mr\nmT0 0.400 0.415 0.344\nBLOOMZ 0.3751 0.508 0.539\nWNLI ChatGPT 0.406 0.406 0.406\nmBERT 0.56 0.56 0.56\nIndicBERT 0.56 0.56 0.56\nTable 3: WNLI Task: Language-wise weighted F1-score\nfor mT0, BLOOMZ, and ChatGPT. The bold number in-\ndicates the highest value for every language and the red\ncolor denotes the highest performance value amongst\nthe multilingual LLMs for every language.\nTask mT0 BLOOMZ ChatGPT\nSA 89.8% 93.4% 83.8%\nNewsCLS 32.1% 38.9% 51.4%\nWNLI 38.8% 47.7% 40.6%\nTable 4: Correctly labelled records by mT0, BLOOMZ,\nand ChatGPT. The number in bold indicates the average\nhighest performance for the corresponding task. We\nconsider the macro-average calculated across all the\nrelevant languages for a task.\nious annotation choices. We concur with (Huang\net al., 2023) that these justifications reinforce hu-\nman annotators’ perception and understanding of a\ngiven task. We believe that this kind of response is\nhelpful to non-expert annotators in improving their\nannotation performance.\n5 Concluding Remarks\nRemarkable progress in LLMs has opened up in-\nteresting possibilities in diverse domains. Accord-\ningly, we evaluate a novel way of using LLMs as\nannotators. We explore the efficacy of these LLMs\nas a substitute or as an aid for human annotators in\nthe context of low-resource languages, specifically\nIndic languages. Despite the presence of multilin-\ngual training data, including data from Indic lan-\nguages data, the LLMs struggle to provide correct\nresponses in Indic languages. We report that anno-\ntations for simpler tasks, such as sentiment analysis,\ncan be readily recreated by the current set of LLMs.\nWe observe that these LLMs still have a long way\nto go before they can be used as annotators in LR\nlanguage tasks where linguistic understanding and\nreasoning are essential, e.g., natural language in-\nferencing and news classification. Even though\nrecent works have documented the feasibility of en-\nabling annotations using these models in a positive\nlight, these works are focused on high-resource lan-\nguages. With this work, we wanted to highlight that\nadditional efforts are needed for similar undertak-\ning in low-resource Indic languages. In the future,\nwe intend to employ advanced prompting strategies\nto aid annotations, such as using linguistic markers\nas knowledge prompts and in-context learning to\nguide the evaluations. We also hope to use back-\ntranslation to aid LLMs’ understanding. We intend\nto experiment with these LLM annotations as weak\nlabels to assist improvements to data collection ex-\nercises for low-resource languages. We also plan to\nexplore the possibility of using LLMs as evaluators\nfor quality metrics such as relevance, coherence,\nand fluency in the future. Furthermore, we note that\nthe justifications provided by ChatGPT, along with\nanswers, are helpful and can be further exploited\nfor annotators’ training. We plan to use these justifi-\ncations to improve the prompt guidelines for LLM\nannotations.\nLimitations We evaluate the performance of\nLLMs as annotators for certain tasks. There are\na few limitations to note: 1) LLM performance\nheavily depends on the prompts. Currently, we use\nheuristically identified prompts, but exploring bet-\nter prompts may give even better annotations in the\nfuture. 2) We agree that the experiments need more\nrigor. Due to restrictions on API usage, we use\nonly a subset of available datasets. 3) We believe\nthat quality data is also an area of concern. We use\ntranslated data in some cases, which may adversely\naffect the performance.\n105\nEthics-Impact Statement\nAll the datasets and pre-trained models used in\nthis work are publicly available for research pur-\nposes. The authors foresee no ethical concerns or\ncopyright violations with the work presented in this\npaper.\nReferences\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert\nWebson, Colin Raffel, Nihal V . Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, Zaid Alyafeai, Manan Dey, Andrea San-\ntilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu,\nGunjan Chhablani, Han Wang, Jason Alan Fries,\nMaged S. Al-shaibani, Shanya Sharma, Urmish\nThakker, Khalid Almubarak, Xiangru Tang, Xian-\ngru Tang, Mike Tian-Jian Jiang, and Alexander M.\nRush. 2022. Promptsource: An integrated develop-\nment environment and repository for natural language\nprompts.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nJC Blaise Cruz and Charibeth Cheng. 2020. Establish-\ning baselines for text classification in low-resource\nlanguages. arXiv preprint arXiv:2005.02068.\nSumanth Doddapaneni, Rahul Aralikatte, Gowtham\nRamesh, Shreya Goyal, Mitesh M Khapra, Anoop\nKunchukuttan, and Pratyush Kumar. 2023. Towards\nleaving no indic language behind: Building mono-\nlingual corpora, benchmark and models for indic\nlanguages. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 12402–12426.\nTiziano Fagni, Fabrizio Falchi, Margherita Gambini, An-\ntonio Martella, and Maurizio Tesconi. 2021. Tweep-\nfake: About detecting deepfake tweets. Plos one,\n16(5):e0251415.\nMingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship-\ning Yang, and Xiaojun Wan. 2023. Human-like sum-\nmarization evaluation with chatgpt. arXiv preprint\narXiv:2304.02554.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. 2023. How close is chatgpt to human experts?\ncomparison corpus, evaluation, and detection. arXiv\npreprint arXiv:2301.07597.\nFan Huang, Haewoon Kwak, and Jisun An. 2023. Is\nchatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate\nspeech. In Companion Proceedings of the ACM Web\nConference 2023, pages 294–297.\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, NC Gokul, Avik Bhattacharyya, Mitesh M\nKhapra, and Pratyush Kumar. 2020. IndicNLPSuite:\nMonolingual corpora, evaluation benchmarks and\npre-trained multilingual language models for indian\nlanguages. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 4948–\n4961.\nTaja Kuzman, Igor Mozetic, and Nikola Ljubešic. 2023.\nChatgpt: Beginning of an end of manual linguistic\ndata annotation? use case of automatic genre identifi-\ncation. ArXiv, abs/2303.03953.\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben\nVeyseh, Hieu Man, Franck Dernoncourt, Trung Bui,\nand Thien Huu Nguyen. 2023. Chatgpt beyond en-\nglish: Towards a comprehensive evaluation of large\nlanguage models in multilingual learning. arXiv\npreprint arXiv:2304.05613.\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,\nYuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, et al. 2023. Summary\nof chatgpt/gpt-4 research and perspective towards\nthe future of large language models. arXiv preprint\narXiv:2304.01852.\nAlexandre Magueresse, Vincent Carles, and Evan Heet-\nderks. 2020. Low-resource languages: A review\nof past work and future challenges. arXiv preprint\narXiv:2006.07264.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 task 4: Sentiment analysis in Twitter.\nIn Proceedings of the 11th International Workshop\non Semantic Evaluation (SemEval-2017), pages 502–\n518, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\n106\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nYiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui,\nand Gareth Tyson. 2023. Can chatgpt reproduce\nhuman-generated labels? a study of social computing\ntasks. arXiv preprint arXiv:2304.10145.\n107"
}