{
    "title": "Adapting protein language models for rapid DTI prediction",
    "url": "https://openalex.org/W4308268814",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5020346838",
            "name": "Samuel Sledzieski",
            "affiliations": [
                "Tufts University"
            ]
        },
        {
            "id": "https://openalex.org/A5081688191",
            "name": "Rohit Singh",
            "affiliations": [
                "Tufts University"
            ]
        },
        {
            "id": "https://openalex.org/A5026411021",
            "name": "Lenore Cowen",
            "affiliations": [
                "Tufts University"
            ]
        },
        {
            "id": "https://openalex.org/A5044078921",
            "name": "Bonnie Berger",
            "affiliations": [
                "Tufts University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2730223999",
        "https://openalex.org/W3000043291",
        "https://openalex.org/W2909727437",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W2086286404",
        "https://openalex.org/W3040739508",
        "https://openalex.org/W3017961061",
        "https://openalex.org/W3169392527",
        "https://openalex.org/W3139654928",
        "https://openalex.org/W3109916301",
        "https://openalex.org/W3018980093",
        "https://openalex.org/W2786722833",
        "https://openalex.org/W3005111505",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W2899788782",
        "https://openalex.org/W2096864392",
        "https://openalex.org/W2044834685",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W1988037271",
        "https://openalex.org/W2605409611",
        "https://openalex.org/W3199468887",
        "https://openalex.org/W2860192827"
    ],
    "abstract": "Abstract We consider the problem of sequence-based drug-target interaction (DTI) prediction, showing that a straightforward deep learning architecture that leverages pre-trained protein language models (PLMs) for protein embedding outperforms state of the art approaches, achieving higher accuracy, expanded generalizability, and an order of magnitude faster training. PLM embeddings are found to contain general information that is especially useful in few-shot (small training data set) and zero-shot instances (unseen proteins or drugs). Additionally, the PLM embeddings can be augmented with features tuned by task-specific pre-training, and we find that these task-specific features are more informative than baseline PLM features. We anticipate such transfer learning approaches will facilitate rapid prototyping of DTI models, especially in low-N scenarios.",
    "full_text": null
}