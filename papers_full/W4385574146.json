{
  "title": "CN-AutoMIC: Distilling Chinese Commonsense Knowledge from Pretrained Language Models",
  "url": "https://openalex.org/W4385574146",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2111712397",
      "name": "Chen Hao Wang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence",
        "Shandong Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2160888529",
      "name": "Jiachun Li",
      "affiliations": [
        "Shandong Institute of Automation",
        "University of Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2099156817",
      "name": "Yu-Bo Chen",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Shandong Institute of Automation",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2072929885",
      "name": "Kang Liu",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Shandong Institute of Automation",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2028683064",
      "name": "Jun Zhao",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence",
        "Shandong Institute of Automation"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2083897630",
    "https://openalex.org/W3100683605",
    "https://openalex.org/W4205523161",
    "https://openalex.org/W2071718761",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W3174202502",
    "https://openalex.org/W3152665347",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W3098323839",
    "https://openalex.org/W3174464510",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2986213397",
    "https://openalex.org/W3138643305",
    "https://openalex.org/W3034918576",
    "https://openalex.org/W4285122753",
    "https://openalex.org/W3176793246",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3176617251",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W3152979241",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W3201663597",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W3034346995",
    "https://openalex.org/W4223974161",
    "https://openalex.org/W4226049437",
    "https://openalex.org/W2561529111"
  ],
  "abstract": "Commonsense knowledge graphs (CKGs) are increasingly applied in various natural language processing tasks. However, most existing CKGs are limited to English, which hinders related research in non-English languages. Meanwhile, directly generating commonsense knowledge from pretrained language models has recently received attention, yet it has not been explored in non-English languages. In this paper, we propose a large-scale Chinese CKG generated from multilingual PLMs, named as **CN-AutoMIC**, aiming to fill the research gap of non-English CKGs. To improve the efficiency, we propose generate-by-category strategy to reduce invalid generation. To ensure the filtering quality, we develop cascaded filters to discard low-quality results. To further increase the diversity and density, we introduce a bootstrapping iteration process to reuse generated results. Finally, we conduct detailed analyses on CN-AutoMIC from different aspects. Empirical results show the proposed CKG has high quality and diversity, surpassing the direct translation version of similar English CKGs. We also find some interesting deficiency patterns and differences between relations, which reveal pending problems in commonsense knowledge generation. We share the resources and related models for further study.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9253–9265\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nCN-AutoMIC: Distilling Chinese Commonsense Knowledge from\nPretrained Language Models\nChenhao Wang1,2, Jiachun Li1,2, Yubo Chen1,2, Kang Liu1,2,3 and Jun Zhao1,2\n1National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China\n2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n3Beijing Academy of Artificial Intelligence, Beijing, China\n{chenhao.wang,jiachun.li,yubo.chen,kliu,jzhao}@nlpr.ia.ac.cn\nAbstract\nCommonsense knowledge graphs (CKGs) are\nincreasingly applied in various natural lan-\nguage processing tasks. However, most exist-\ning CKGs are limited to English, which hin-\nders related research in non-English languages.\nMeanwhile, directly generating commonsense\nknowledge from pretrained language models\nhas recently received attention, yet it has not\nbeen explored in non-English languages. In this\npaper, we propose a large-scale Chinese CKG\ngenerated from multilingual PLMs, named as\nCN-AutoMIC, aiming to fill the research gap\nof non-English CKGs. To improve the effi-\nciency, we propose generate-by-category strat-\negy to reduce invalid generation. To ensure the\nfiltering quality, we develop cascaded filters to\ndiscard low-quality results. To further increase\nthe diversity and density, we introduce a boot-\nstrapping iteration process to reuse generated\nresults. Finally, we conduct detailed analyses\non CN-AutoMIC from different aspects. Em-\npirical results show the proposed CKG has high\nquality and diversity, surpassing the direct trans-\nlation version of similar English CKGs. We\nalso find some interesting deficiency patterns\nand differences between relations, which reveal\npending problems in commonsense knowledge\ngeneration. We share the resources and related\nmodels for further study1.\n1 Introduction\nCompiling large-scale commonsense knowledge\nresources is a long-term goal of the AI commu-\nnity. Recent efforts focus on constructing com-\nmonsense knowledge graphs (CKGs) via manually\ncompiling (Speer et al., 2017; Sap et al., 2019;\nMostafazadeh et al., 2020) or automatic extrac-\ntion (Tandon et al., 2014; Romero et al., 2019;\nZhang et al., 2020; Nguyen et al., 2021). These\nprojects have shown benefits for a wide range of\ndownstream tasks (Lin et al., 2019; Tian et al.,\n2020; Ammanabrolu et al., 2021).\n1http://github.com/wchrepo/cnautomic/\nHowever, most CKG projects are limited to En-\nglish, which hinders further research in other lan-\nguages. To go beyond such an English-centric\ntrend in commonsense knowledge research, there\nare some challenging issues. First, directly\ntranslating English CKGs is not enough. For\nexample, (PersonX is a little girl, xWant,\nto ask Christmas presents) is a triple from a\nCKG crowdsourced by English users (Sap et al.,\n2019), but it is not common in non-Christian cul-\ntures. In fact, such resources reflect only the com-\nmonsense perspectives of English-speaking com-\nmunities. The translation will omit the cultural\ndifferences in other languages, and even implicitly\nexacerbate the English-centric bias (Mehrabi et al.,\n2021). Therefore, when creating CKGs for new\nlanguages, it is better to rely on native speakers and\ncorpora. Second, current common practices in En-\nglish CKG construction, i.e. manually compiling\nand automatic extraction, are difficult to generalize.\nFor manually compiling, creating human-authored\nCKGs for each new language is very expensive.\nFor automatic extraction, current pipelines rely on\nEnglish-specific hand-craft patterns or language\nprocessing tools, which are not available to other\nlanguages. Therefore, when creating CKGs for new\nlanguages, the cost and availability of construction\nscheme should also be concerned.\nRecent work reveals pretrained language models\n(PLMs) can be a new source to generate common-\nsense knowledge (Bosselut et al., 2019; Nguyen\nand Razniewski, 2022). The Latest research indi-\ncates the CKG built from huge PLMs (e.g. GPT-3)\ncan surpass the crowdsourced ones in quantity and\nquality (West et al., 2021), and only a small amount\nof human-authored data are required for prompt-\ning and filtering. Interestingly, we find this way\ncould be the ideal start point to construct CKGs\nfor new languages, since PLMs can be trained on\nthe corpora of target languages, and the generation\nprocess is low-cost and independent of language-\n9253\nspecific tools. However, up to now, work in this\nthread has not extended to non-English languages.\nThe main challenge of this paradigm is that the gen-\neration quality and diversity are often conflicting\nand difficult to control. To sample diverse results,\nthe generation should be run extensive times, and a\nlarge number of generated results are invalid and\nneed to be filtered out. For new languages, as there\nis often no comparable PLM to GPT-3 (Brown\net al., 2020) in English, the generated results will\nbe even noisier. Therefore, we need to reduce un-\nnecessary generation to increase the efficiency and\ntake measures to ensure the quality.\nIn this paper, we propose a framework to distill\ncommonsense knowledge from multilingual pre-\ntrained language models. To increase the genera-\ntion efficiency, we propose a generate-by-category\nstrategy to reduce invalid generation. To ensure\nthe filtering quality, we propose cascaded filters\nto discard low-quality results. To further increase\nthe diversity and density, we introduce a bootstrap-\nping process. Based on the framework, we pro-\npose a large-scale Chinese commonsense knowl-\nedge graph, CN-AutoMIC (Automatically Ob-\ntained MachIne Commonsense). To the best of\nour knowledge, it is the first non-English CKG\nbuilt from pretrained language models. The em-\npirical results show the proposed CKG has high\nquality and diversity. We also discuss some inter-\nesting deficiencies that need further solutions. We\nsummarize the contribution as follows.\n• We propose a framework to distill common-\nsense knowledge from multilingual PLMs,\nincorporating several novel strategies to im-\nprove the generation efficiency and quality.\n• We propose the first large-scale Chinese com-\nmonsense knowledge graph built with large-\nsized PLMs, CN-AutoMIC. Its high-quality\nsubset contains 1.1M triples with an accuracy\nof 87.2%. Besides the CKG, we also release\nsmall-sized commonsense models trained on\nit, named as CN-COMET.\n• We conduct comprehensive evaluation and\nanalysis for CN-AutoMIC and CN-COMET.\nBesides verifying the quality and diversity, we\nalso get some valuable observations about the\ndeficiencies of generation. Considering gen-\nerating commonsense knowledge with PLMs\nis still not fully explored, our findings can\nprovide more insights into this topic.\n某人 X 受到攻击  \nPersonX is attacked \n某人 X 夸赞某人 Y\nPersonX praises\nPersonY\n某人 X 受到攻击  \nPersonX is attacked \n某人 X 夸赞某人 Y\nPersonX praises\nPersonY\n某人 X 收到补偿 \nPersonX receives\ncompensation \n某人 X 选择离开 \nPersonX choose to\nleave \n某人 X 被某人 Y 救下 \nPersonX is saved by\nPersonY某人 X 寻找证据 \nPersonX looks for\nevidence \nxEffect\nxWant\n某人 X 受到攻击  \nPersonX is attacked \n某人 X 夸赞某人 Y\nPersonX praises\nPersonY\nxNeed\n某人 X 收到补偿 \nPersonX receives\ncompensation \n某人 X 寻找证据 \nPersonX looks for\nevidence \n某人 X 被某人 Y 救下 \nPersonX is saved by\nPersonY\nxEffect\n某人 X 选择放弃 \nPersonX choose to\ngive up \n向法院起诉  \ngo to court \n开心  \nhappy \nxReact\nxIntent\nxReact\nxReact 承受损失 \nsustain losses \n逃走  \nflee \n1. Generating more  \nhead items of  \nthe same category\n2. Generating triples  \nand filter out  \nlow-quality results\n向法院起诉  \ngo to court \n某人 X 向法院起诉 \nPersonX go to court \n3. Converting tail items  \nand bootstrapping to  \nmore triples\n \n \n \n \nPLM \nPLM \nPLM \nFigure 1: The construction demonstration of CN-\nAutoMIC. The dashed nodes the relations are generated\nand filtered with pretrained language models.\n2 Related Work\n2.1 Commonsense Knowledge Graphs\nAfter some pioneers of strict logic formaliza-\ntion (Lenat et al., 1990), most recent commonsense\nknowledge resources, also known as commonsense\nknowledge graphs, represent commonsense knowl-\nedge in loosely-structured ( head, relation, tail)\ntriples, where the head and tail can be concepts or\nsituations described in free-form phrases, and the\nrelation can be various commonsense dimensions.\nSome of such resources are constructed by manu-\nally compiling or crowdsourcing (Speer et al., 2017;\nSap et al., 2019; Hwang et al., 2021; Mostafazadeh\net al., 2020). The others are mainly mined from\ncorpora with human-crafted patterns or language\nprocessing tools (Tandon et al., 2014; Romero\net al., 2019; Zhang et al., 2020; Fang et al., 2021;\nNguyen et al., 2021).\nUnfortunately, most of these projects are limited\nin English. Among the mainstream CKGs, Con-\nceptNet (Speer et al., 2017) is the only multilingual\none. It supports 10 core languages and more com-\nmon languages. However, most of its non-English\nparts are taxonomic or lexical knowledge. The rest\nparts are limited in quantity and coverage. A re-\nmarkable recent work of Chinese commonsense\nknowledge resources is C3KG (Li et al., 2022). It\nis based on the translation of ATOMIC (Sap et al.,\n2019; Hwang et al., 2021), which may fail to cap-\nture the cultural differences. Therefore, our work\ncan be a strong complement to them.\n9254\n2.2 Extracting Knowledge from PLMs\nSince PLMs have seen a great number of doc-\numents and latently learned associations among\nconcepts, there are extensive efforts to probe or\nextract relational knowledge from PLMs (Petroni\net al., 2019; Sung et al., 2021; AlKhamissi et al.,\n2022). As for commonsense knowledge, some ear-\nlier work has tried to automatically complete CKGs\nby fine-tuning PLMs (Bosselut et al., 2019; Guo\net al., 2020; Hwang et al., 2021), which still needs\na large number of existing triples as training data.\nRecent research demonstrates that through natural\nlanguage prompting, PLMs can adapt to generate\ncommonsense knowledge under the few-shot set-\nting (Da et al., 2021), or directly act as triple scorers\nwithout training (Davison et al., 2019). A signifi-\ncant progress is made by West et al. (2021). They\nuse GPT-3 to generate a CKG from scratch. During\nthe construction, only a small amount of human-\nauthored data are required for in-context prompting\ngeneration and filtering results. They show the re-\nsulting CKG surpasses human-authored ATOMIC\nin quantity, quality, and diversity. Compared with\nWest et al. (2021), our work proposes more im-\nprovement in generation and filtering and brings\nmore comprehensive analysis for the paradigm\nfrom a non-English perspective.\n3 Construction of CN-AutoMIC\nFor clarity, in this section, we first show the\noverview of the construction task, then describe\nthe construction process of CN-AutoMIC in detail.\n3.1 Overview\nWe expect to obtain commonsense knowledge rep-\nresented in (head, relation, tail) triples via prompt-\ning generation. The demonstration of construc-\ntion is shown in Figure 1. Similar to the construc-\ntion workflow of crowdsourced CKGs, we hope\nto first collect head items (Section 3.2) and then\ncollect tail items according to several pre-defined\nrelations (Section 3.3). We limit the head items\nto the description of eventualities (events, activi-\nties and states) (Bach, 1986), such as “ 某人Ｘ离\n开家(PersonX leaves home)”. Following West\net al. (2021), we consider seven relation types about\neventualities from ATOMIC, which are listed in Ta-\nble 2. Since the raw generated results are mixed\nwith noise and degeneration, we introduce human\nsupervision to train filter models to distinguish\nhigh-quality results (Section 3.4). To reuse the gen-\nerated tails and increase the density, we propose a\nbootstrapping iteration process (Section 3.5).\n3.2 Generating Head Items\nWe start from a minor size of head item seeds, using\nthem as examples to prompt PLMs to generate\nmore head items in the same format.\nNotably, although previous work (West et al.,\n2021) treats all head items without distinction and\ncollects knowledge about them for all predefined\nrelations, we argue that it is necessary to further\nsubdivide head items into different categories, be-\ncause some heads and relations are in conflict and\nthey cannot produce valid results. For example, we\ncannot infer the intent of X (xIntent) in “某人\nＸ受到攻击 (PersonX is attacked)”, because\nhe is passively involved in it rather than intention-\nally causes it. Therefore, we divide the head items\nin three categories (voluntary occurrences, involun-\ntary occurrences and states) and match them with\ndifferent relations, as illustrated in Table 1. Note\nthat these categories are not strict, but they can\nhopefully reduce invalid generation.\nThen, we collect head item seeds for the three\ncategories. We mainly sample the high-quality\nhead items from ATOMIC, manually categoriz-\ning and translating them. We intentionally discard\nthe instances that are English-specific or rare in\nChinese context, such as “ PersonX has a baby\nshower”. In total, we collect 100, 50, and 45\nseeds for voluntary occurrences, involuntary oc-\ncurrences, and states, respectively.\nThrough pilot studies, we empirically choose\nmT5-XXL (13B parameters) (Xue et al., 2021) for\ngeneration. It is one of the biggest publicly avail-\nable multilingual PLMs2, covering 101 languages,\nbut still 13x smaller than GPT-3 used in West et al.\n(2021). During the generation, we use the prompt\nshown in Figure 2. For each generation cycle, we\nsample 10 examples from the seeds to construct the\nprompt, and use nucleus sampling with p= 0.9 to\ndecode 100 results. The generation cycles for dif-\nferent head categories are independently conducted.\nAfter generating 600K raw results (2,000 cycles\nfor each category), we discard 30% results of high\nnegative log-likelihood and merge the duplicates in\nthe remaining part, resulting in 100K head items.\n2We note that mT5 has an additional multilingual advan-\ntage. It can conduct text-infilling generation for any position\nthe special token <extra_id_0> appears. This is helpful for\nsome languages (e.g. SOV languages) which cannot guarantee\nthat the content to be generate is at the end of the prompt.\n9255\nCategory Examples Valid Relations\nV oluntary\nOccurrences\n某人Ｘ租房子；某人Ｘ学开车；某人X夸赞某人Y xWant, xReact, xEffect, xAttr,\nxNeed, xIntent, HinderedByPersonX rents a house; PersonX learns to drive; PersonX praises PersonY\nInvoluntary\nOccurrences\n某人Ｘ受到攻击；某人Ｘ睡过头；某人X收到某人Y的来信 xWant, xReact, xEffect xAttr,\nxNeed, HinderedByPersonX is attacked; PersonX oversleeps; PersonX receives a letter from PersonY\nStates 某人Ｘ很疲惫；某人Ｘ头晕；某人X认识某人Y xWant, xAttr, xNeed, xEffect,\nHinderedByPersonX is tired; PersonX feels dizzy; PersonX knows PersonY\nTable 1: Three categories of the head items: voluntary occurrences (PersonX intentionally cause it), involuntary\noccurrences (PersonX is involuntarily involved in it), andstates (PersonX is in it for some time).\nRelation Description / Verbalizing Templates\nxWant {head}，在此之后，{X}想要{tail}\n{head}. As a result,{X}wants{tail}\nxReact {head}，对此，{X}感觉{tail}\n{head}. For that,{X}feels{tail}\nxEffect {head}。结果，{X}{tail}\n{head}. The outcome is that{X} {tail}\nxAttr {head}，据此，可以看出{X}是{tail}\nWhen{head}, people think{X}is{tail}\nxNeed {head}，在此之前，{X}需要{tail}\n{head}. Before that,{X}needed{tail}\nxIntent {head}，{X}的意图是{tail}\n{head}, {X}’s intent in it is{tail}\nHinderedBy{head}，这受到阻碍，因为{tail}\n{head}can be hindered by{tail}\nTable 2: The relations and their verbalizing templates.\nThe words in brackets are placeholders and will be re-\nplaced according to the head and tail.\n 1. 某人 X 买书；  \n(1. PersonX buys a book;) \n ...... \n 10. 某人 X 和某人 Y 一起打篮球；  \n(10. PersonX plays basketball with PersonY;) \n 11. <extra_id_0> \n(11. <extra_id_0>)\nFigure 2: The prompt for generating head items. The\ntranslation in parentheses is not actually in the prompt.\n<extra_id_0> is a special token to make the mT5 model\ndo text-infilling generation.\n 请填写人物的意图，例如：  \n(Please write down the intent of the person, Example: ) \n 1. 晓燕取回报纸，晓燕的意图是阅读报纸；  \n(1. Xiaoyan takes back the newspaper,  Xiaoyan's intent is to  \nread the newspaper;) \n ...... \n 9. 张三联系警察，张三的意图是  <extra_id_0>；  \n(9. Zhang calls the police, Zhang's intent is <extra_id_0>;)\nFigure 3: The prompt for generating tail items according\nto a pre-defined relation.\n3.3 Generating Triples\nTo obtain complete triples, we further generate tail\nitems according to different relations. Similar to\nSection 3.2, we use example triples for prompting\ngeneration. To make effective use of the capabili-\nties of PLMs, we need to convert triples into natural\nlanguage sentences. For this sake, we use the tem-\nplates in Table 2 to verbalize triples, and further\nreplace “ 某人Ｘ(PersonX)” placeholders with\nrandom Chinese names.\nWe continue to use mT5-XXL model for gen-\neration. With the verbalized example triples, we\nconstruct the prompt as shown in Figure 3. For\neach relation, we use 8 example triples, which are\nsampled from ATOMIC and manually translated\ninto Chinese. The prompt is used to generate 10\ntail items for each (head,relation) pair with nucleus\nsampling (p= 0.7). As said before, each head item\nis only paired with the valid relations according to\nits category3, so that the invalid generation results\nare reduced. After converting names back to place-\nholders and removing duplicated triples, this step\nproduces 5.2M raw triples in total.\n3.4 Filtering Results\nAnnotation To train filters that can distinguish\nhigh-quality triples, we randomly sample 4000 in-\nstances from the raw generated triples and ask three\nnative Chinese speakers to annotate them. We in-\ntentionally give three questions for each triple. The\nannotators should first rate the head and tail alone\nto indicate whether they are acceptable. If not,\nthe options of rejecting include syntax errors, ab-\nnormal or impossible situations (e.g. “ 某人X在\n天上游泳 (PersonX swims in the sky)”) and\nother faults (e.g. containing real names rather\nthan name placeholders). Then, if the annota-\n3We tag head items with the category of examples used to\ngenerate them. It could be wrong sometimes, but we empiri-\ncally find the overall Precision is moderate (Section 4.2).\n9256\nHeads Tails Triples\nAcceptance Rate 85.2% 94.4% 47.6%\nTable 3: Annotator’s acceptance for sampled raw triples.\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.6\n0.7\n0.8\n0.9\n1.0Precision Head\nT ail\nTriple\nTriple*\nFigure 4: The precision-recall curves of filter models.\n“Head”, “Tail”, “Triple” are the performances of head\nclassifier, tail classifier and triple classifier respectively.\n“Triple*” is the performance of triple classifier on a\nsubset, where the head and tail are already acceptable.\ntors have accepted the head and tail, they should\nrate the triple as a whole, with options for accept-\nability: “always/often”, “sometimes/likely”, “far-\nfetched/never”, or “invalid”. The former two are\nconsidered “accepted”. The latter two are “re-\njected”. Table 3 shows the acceptance rate. For the\noverall results, we find the fleiss’s κ(Fleiss, 1971)\nis 0.439, which indicates moderate agreement.\nTraining We use 80% of the annotated data for\ntraining, and the remaining parts for validating\nand testing. We train binary classifier models\nto predict whether the input is acceptable. Con-\nsidering the acceptance differences in the anno-\ntation results(Table 3), we train single-use classi-\nfiers for heads, tails, and triples, respectively. We\nempirically choose a Chinese version 4 of ELEC-\nTRA (Clark et al., 2020) as the underlying model\nand fine-tune it for the tasks. We set learning rate to\n5e-5 and batch size to 128 by grid search, and use\nearly stopping to maximize the average precision\n(AP) on the validation data. We also tried other\nmultilingual or Chinese PLMs of similar size, but\nthe ELECTRA-based models obtain the best AP.\nCascaded Filtering We report the precision-\nrecall curves of the trained models in Figure 4,\nincluding all three classifiers. We also report the\nperformance of triple classifier on “clean data”,\n4The hfl/chinese-electra-180g-large checkpoint.\nRelation Tail-to-Head Example ConvertedCategory\nxNeed, xWant,xIntent 去看医生→某人X去看医生 V oluntaryOccurrences go to the doctor→PersonX goes to the doctor\nxEffect,HinderedBy失去工作→某人X失去工作 InvoluntaryOccurrences lose the job→PersonX loses the job\nxReact 满意→某人X感觉满意 Statessatisfied→PersonX feels satisfied\nTable 4: Converting tails to heads for bootstrapping\niteration.\nwhere the head and tail in the triple are acceptable\n(Triple*). From the curves, we can find head and\ntail classifiers can reach very high precision at al-\nmost all recall value. And triple classifier perform\nbetter on “clean data”. It indicates that we can\nachieve better performance by cascaded filtering,\ni.e., applying the head and tail classifier first, and\nthen using the triple classifier. We also find it is\nuseful to set different thresholds for different rela-\ntion types. Finally, we set the thresholds for head\nand tail classifier to ensure precision >0.98 on\nthe test data. We empirically search three groups\nof thresholds for the triple classifier, based on\nprecision = 0.9,0.8,0.75 on each relation. We\nuse these thresholds to get three subsets with differ-\nent sizes and denote them as high/mid/low subsets.\n3.5 Bootstrapping Iteration\nWe note that although the generated tail items have\ndifferent formats from the head items, many of\nthem can be converted into head items with simple\ntemplates, as shown in Table 4. After the above\nsteps, many of the tail items have never appeared\nas head items. To further increase the diversity and\ndensity, we use the high-frequency tail items from\nthe mid subset to generate more triples. We re-\npeat the generating and filtering process described\nin Section 3.3 and Section 3.4, using the trained\nfilters. Such bootstrapping iteration can be done\nseveral times, though we only conduct it once in\nthis work. Finally, the resulting triples from differ-\nent iterations are merged. We denote the merged\nsets as CN-AutoMIChigh/mid/low respectively.\n4 Evaluation and Analysis\nIn this section, we evaluate and analyze the re-\nsources in three parts. First, we comprehensively\nevaluate CN-AutoMIC in size, quality and diver-\nsity. In this step, we also evaluate the common-\nsense model (CN-COMET) trained on it. Second,\nwe conduct specific analyses for different construc-\n9257\nKnowledge Graph Construction Unique\nHead Items\nUnique\nTail Items Triples Human\nAcceptance\nEnglish\nCKGs\nATOMIC2020 (Hwang et al., 2021) Human 25,807 354,777 760,034 86.8*\nATOMIC10X(Unfiltered) (West et al., 2021) Generation 165,783 874,417 6,456,300 78.5*\nATOMIC10X(High-Quality) Generation 164,553 357,761 2,512,720 96.4*\nChinese\nCKGs\nATOMIC-zh (Li et al., 2022) Translation 20,949 276,446 712,970 38.7\n(ours) CN-AutoMIC (Unfiltered) Generation 114,364 1,101,556 6,868,766 47.6\n(ours) CN-AutoMIClow Generation 99,817 385,333 2,764,465 75.2\n(ours) CN-AutoMICmid Generation 97,329 269,655 1,812,175 80.5\n(ours) CN-AutoMIChigh Generation 89,738 182,893 1,140,840 87.2\nTable 5: The statistics of CN-AutoMIC and related resources. The * results are from West et al. (2021)\ntion steps. Third, we inspect the cases of culture-\nspecific commonsense knowledge and generation\ndeficiencies.\n4.1 Evaluating the Graph\nSetup We count the size of triples, the unique\nheads, and the unique tails in the graph to inves-\ntigate the quantity and diversity. For comparison,\nwe refer to two English CKGs, including human-\nauthored ATOMIC20\n20 (Hwang et al., 2021) and au-\ntomatically generated ATOMIC10x (West et al.,\n2021). We also refer to a Chinese CKG (ATOMIC-\nzh) (Li et al., 2022) which is automatically trans-\nlated from ATOMIC20\n20. Then, to test the quality,\nwe sample 1000 triples from ATOMIC-zh and CN-\nAutoMIC and conduct a human evaluation. The\nannotation setting is similar to Section 3.4, but only\nthe triples need to be rated. We keep the annota-\ntion results by majority vote and report the average\nacceptance for the triples5.\nOverall Statistics The overall statistics of CN-\nAutoMIC are shown in Table 5. From the results,\nwe find: (1) Compared with the existing translated\nCKG, CN-AutoMIC contains a larger size of triples\nwith better quality, as well as more unique head\nitems. Interestingly, even the raw generated triples\nhave better average acceptance than the translated\nCKG. We speculate that is because the translated\nCKG has a lot of syntax and translation errors. (2)\nAfter filtering, the human acceptance reaches up to\n87.2 from 47.6, indicating the effect of the filtering\nprocess. As a trade-off, the diversity of tail items\nis decreased. (3) The English ATOMIC10x is gen-\nerated by GPT-3 and has better basic quality. After\nfiltering, it can reach very high acceptance, surpass-\ning human-authored ATOMIC20\n20 by 10 percent. By\ncontrast, CN-AutoMIC struggles on reaching high\n5The fleiss’s κ is 0.535, indicating moderate agreement.\nRelation Unique\nTail Items Triples Acceptance\nxWant 23,673 179,861 84.8\nxReact 1,985 145,431 95.4\nxEffect 91,808 463,298 86.5\nxAttr 1,660 28,973 88.9\nxNeed 45,221 209,525 81.2\nxIntent 20,575 79,012 88.1\nHinderedBy 8,868 34,740 85.0\nTable 6: Relation-level results of CN-AutoMIChigh.\nacceptance, which shows the difficulty of generat-\ning non-English commonsense knowledge.\nRelation-level Results We show the relation-\nlevel results of CN-AutoMIChigh in Table 6. We\ncan find the acceptance on most of the relations is\nbetween 80 to 90 percent. However, the number of\ntriples varies significantly. That is because we use\ndifferent filter thresholds for different relations to\nachieve the same quality level. To some extent, the\nchange of triple amounts reflects how much com-\nmonsense knowledge of a specific relation type\nexists in the PLM. According to the results, mT5\nseems to be better at xEffect than HinderedBy.\nCommonsense Model To examine the data qual-\nity from another perspective, we also train com-\nmonsense knowledge models (COMET) (Bosselut\net al., 2019; Hwang et al., 2021) on CN-AutoMIC\nor ATOMIC-zh triples. We denote these models as\nCN-COMET. The models are based on mT5-base\n(580M), which is 20x smaller than T5-XXL. Dur-\ning training, we set the learning rate to 1e-4 and\nbatch size to 128 by a small grid search. We linearly\ndecay the learning rate for all training steps, and fi-\nnally take the checkpoints with the lowest negative\nlog-likelihood on the validation set. For fair com-\n9258\nModel/Training Data Train Data\nAcc.\nGeneration\nAcc.\nCN-COMET (ATOMIC-zh) 38.7 29.7\nCN-COMET (CN-AutoMIClow) 75.2 54.3\nCN-COMET (CN-AutoMICmid) 80.5 60.2\nCN-COMET (CN-AutoMIChigh) 87.2 66.9\nTable 7: The performance of commonsense knowledge\nmodel trained on different CKGs.\nCategory V oluntary\nOccurences\nInvoluntary\nOccurences States\nPrecision 84% 71% 76%\nTable 8: The category precision of generated head items.\nparison, we evaluate all these models on a held-out\nset of ATOMIC-zh, and manually check the results.\nDuring the evaluation, we remove the instances that\nhas unreadable head items. The results are shown\nin Table 7. The model trained on CN-AutoMIChigh\nachieves the best performance, indicating the high-\nquality CKG can make the small-sized model infer\nbetter commonsense knowledge. Nevertheless, the\nbest performance is still not satisfactory. The qual-\nity of training data might be still not good enough.\nAnd the translation noise in test data could also\nexacerbate the difficulty.\n4.2 Analyzing the Construction Steps\nIn this section, we analyze the effect of some inter-\nmediate steps.\nCategory Precision As described in Section 3.2,\nhead items are generated with three categories of\nseeds, so we can reduce invalid generation accord-\ning to the categories. However, the categories of\ngenerated head items are not guaranteed to be what\nthey are generated from. Therefore, we manually\ncheck 100 generated head items for each category,\nand report the precision in Table 8. The results\nindicate that most of the head items belong to the\ncategory they are generated from. Based on this,\nwe avoid nearly 10% invalid generation according\nto the category.\nEffect of Cascaded Filtering We validate the\neffect of cascaded filtering by temporarily remov-\ning the head and tail classifiers during constructing\nCN-AutoMIChigh. That adds 47K new triples in\ntotal. We sample 500 triples from them and con-\nduct manually checking. About 43.2% of them are\nBefore Iteration After Iteration\nUnique Head Items 67,263 89,738\nUnique Tail Items 143,195 182,893\nTriples 768,124 1,140,840\nRetaining Rate 0.148 0.166\nTable 9: The changes of high subset after one iteration.\nRetaining rate means the proportion of high subset in\nall generated triples.\nbad triples. According to the estimation, remov-\ning head and tail classifiers can make the overall\nacceptance drops by 1.9%.\nEffect of Bootstrapping Iteration In Table 9,\nwe report the changes of high subset after a boot-\nstrapping iteration. From the results, we find the\nquantities of unique heads, tails, and triples have\nsubstantially increased. Also, we find the retaining\nrate (i.e. the proportion of triples that are retained\nby the filters) also increases. We conjecture that it\nis because the filtered triples before iteration have\nhigh-quality tail items. Converting them to head\nitems and conducting generation can get better per-\nformance.\n4.3 Case Study\nCulture-Specific Knowledge Since mT5 has\nbeen trained on Chinese corpora, it may gener-\nate commonsense knowledge specific to Chinese\ncontext. We find some explicitly culture-specific\nknowledge triples from the high subset and list\nthem in Table 10. These examples involve festivals,\ntraditional practices, games, and apps that are fa-\nmiliar to Chinese people but not popular in English\ncommunities. Therefore they cannot be found in\ncurrent English CKGs. In contrast, CN-AutoMIC\ncan capture such commonsense knowledge in Chi-\nnese perspectives to some extent.\nDeficiency Patterns We show some regular gen-\neration mistakes in Table 11. We note two interest-\ning error types: (1) Some head items cannot pair\nwith some relations. Taking them as input will al-\nways result in errors. For example, “PersonX kills\nhimself” cannot pair with xWant (after that, X\nwants), because he will lose consciousness and can-\nnot want to do anything. Though we have set three\nhead categories for such problems, there are still\nsome intractable cases. The fundamental reason\nis that PLMs are unable to “reject” inappropriate\ninput. Further research is needed to avoid such\n9259\nFestival 某人X和某人Y共度中秋→xNeed →做月饼\nPersonX spends the Mid-Autumn Festival with PersonY→xNeed →make moon cakes\nTraditional 某人X坐月子→xIntent →把身体照顾好\nPractice PersonX is in postpartum confinement →xIntent →take good care of (her) health\nGame 某人X歇一整天→xWant →打麻将\nPersonX takes a day off→xWant →play mahjong\nApp 某人X看视频→xNeed →打开优酷\nPersonX watches videos →xNeed →open the Youku app.\nTable 10: The cases of generated commonsense knowledge triples that are specific to Chinese context. Mid-Autumn\nFestival is a Chinese traditional festival. Postpartum confinement (or lying-in) is a traditional custom for new\nmothers. Mahjong is a game popular among Chinese. Youku is a website similar to Youtube/Netflix.\nConflict with the Relation\n某人X杀死自己→xWant→参加葬礼\nPersonX kills himself→xWant (after that x wants)→Attend the funeral\n某人X失去意识→xReact→激动\nPersonX is knocked unconscious→xReact (after that x feels)→excited\nNegative Expression or\nUnfavorable Situation\n某人X没有房屋→xNeed→赚钱\nPersonX has no house→xNeed (before that x needs)→earn money\n某人X手很疼痛→HinderedBy→某人X没有止疼药\nPersonX’s hand is aching→HinderedBy→PersonX doesn’t have painkillers\nTable 11: The error cases of generated commonsense knowledge triples.\nresults. (2) For negative expressions or unfavorable\nsituations, the model often performs badly genera-\ntion for some relations. For example, in “PersonX\nhas no house; before that, X needs”, the model is\ntrying to generate the methods to avoid the trouble\n(such as “earning money”), rather than the reason\nthat makes X get in the trouble. This might be due\nto the ambiguity in the natural language prompts.\n5 Discussion\nWhat is the upper-bound size of the generation?\nThe PLMs can conduct ever-lasting generation, but\nit seems there is a soft upper bound. The results\ngenerated later are easy to repeat previous results.\nTherefore, the cost of novel results would gradually\nincrease and eventually become unaffordable. In\nthis work, we have generated tens of millions of\ntriples. It has still not reached the limit, but similar\nor repeated content has appeared in large numbers.\nFor example, during generating head phrases, we\nobserve that the proportion of non-repetitive results\nkeeps descending (Figure 5).\nIf PLMs have already learned commonsense\nknowledge, why is it necessary to extract the\nknowledge from it First, according to the results\n0 50K 100K 150K 200K\nQuantity of Generated Results\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Proportion of Non-Repetitve Results\n0\n50K\n100K\n150K\n200K\nQuantity of Non-Repetitive Results\nFigure 5: The change of quantity and proportion of non-\nrepetitive results during generating head phrases.\nin this paper, the quality of direct generated results\nis still poor. It indicates that even for a model with\n13B+ parameters, the learned knowledge is still\nrough and full of mistakes. A distilling process\ncan distinguish the clean knowledge and benefit\nsmaller applicable models. Besides, recent work\nshows that even though explicitly training PLMs\nwith knowledge, there is no guarantee that they can\nactually use such knowledge in target tasks(Wang\net al., 2021). Therefore, we can exploit explicit\nsymbolic knowledge as auxiliary information.\n9260\n6 Conclusion\nConsidering the dilemma of lacking non-English\ncommonsense knowledge resources, in this paper,\nwe propose CN-AutoMIC, the first Chinese com-\nmonsense knowledge graph that is totally generated\nby pretrained language models. During the con-\nstruction, we use prompting generation to obtain\nhead and tail items, as well as introduce catego-\nrized generation, cascaded filtering and bootstrap-\nping iteration to improve the quantity, quality and\ndiversity. Through human evaluation, the resource\nis shown to have better quality than directly trans-\nlated resources from English language. We discuss\nthe culture-related phenomena and common defi-\nciency patterns in the generated knowledge graph.\nAlthough our work is limited to Chinese, the basic\nframework and methods can be used to populate\nCKGs in more languages.\nLimitations\nThe main limitations of this work include: (1) We\nrequire large-scale pretrained models. The genera-\ntion performance is strongly dependent on the size\nof models. We use 4 RTX-A6000 GPUs for run-\nning the T5-XXL model. (2) Due to the lack of\nlarge-sized PLMs, the quantity and quality of this\ngenerated Chinese CKG still fall behind similar\nEnglish resources. (3) We still cannot interpret the\nbehavior of large PLMs. The specific source of\nthe generated commonsense knowledge is hard to\nlocate, and there are potential ethical risks since\nthe results are not completely checked. (4) We\nstill require extra human labor when applying the\nmethod to each new language. Although basically\nour methods and underlying models (mT5) can gen-\neralize for other languages, we still need human-\ncrafted prompts and a minor size of annotations for\ntraining filter models.\nAcknowledgements\nThis work is supported by the National Key Re-\nsearch and Development Program of China (No.\n2020AAA0106400), and the National Natural\nScience Foundation of China (No. 61922085,\n61976211, 62176257). This work is also sup-\nported by the Strategic Priority Research Pro-\ngram of Chinese Academy of Sciences (Grant\nNo. XDA27020200), the Youth Innovation Promo-\ntion Association CAS, and Yunnan Provincial Ma-\njor Science and Technology Special Plan Projects\n(No.202202AD080004).\nReferences\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona\nDiab, and Marjan Ghazvininejad. 2022. A review on\nlanguage models as knowledge bases. arXiv preprint\narXiv:2204.06031.\nPrithviraj Ammanabrolu, Wesley Cheung, William\nBroniec, and Mark O Riedl. 2021. Automated sto-\nrytelling via causal, commonsense plot ordering. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 5859–5867.\nEmmon Bach. 1986. The algebra of events. Linguistics\nand philosophy, pages 5–16.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for auto-\nmatic knowledge graph construction. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4762–4779, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language Models are Few-Shot Learners.\narXiv:2005.14165 [cs]. ArXiv: 2005.14165.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn International Conference on Learning Representa-\ntions.\nJeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and\nAntoine Bosselut. 2021. Analyzing commonsense\nemergence in few-shot knowledge models. In 3rd\nConference on Automated Knowledge Base Construc-\ntion.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1173–1178, Hong Kong, China. Association\nfor Computational Linguistics.\nTianqing Fang, Hongming Zhang, Weiqi Wang,\nYangqiu Song, and Bin He. 2021. DISCOS: Bridg-\ning the gap between discourse knowledge and com-\nmonsense knowledge. In Proceedings of the Web\nConference 2021. ACM.\n9261\nJoseph L. Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological Bulletin,\n76(5):378–382.\nDaya Guo, Duyu Tang, Nan Duan, Jian Yin, Daxin\nJiang, and Ming Zhou. 2020. Evidence-aware in-\nferential text generation with vector quantised varia-\ntional AutoEncoder. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 6118–6129, Online. Association\nfor Computational Linguistics.\nJena D Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2021. (comet-) atomic 2020: On sym-\nbolic and neural commonsense knowledge graphs.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 6384–6392.\nDouglas B. Lenat, R. V . Guha, Karen Pittman, Dexter\nPratt, and Mary Shepherd. 1990. Cyc: toward pro-\ngrams with common sense. Communications of the\nACM, 33(8):30–49.\nDawei Li, Yanran Li, Jiayi Zhang, Ke Li, Chen Wei,\nJianwei Cui, and Bin Wang. 2022. C 3KG: A Chi-\nnese commonsense conversation knowledge graph.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022 , pages 1369–1383, Dublin,\nIreland. Association for Computational Linguistics.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. KagNet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2829–2839, Hong Kong,\nChina. Association for Computational Linguistics.\nNinareh Mehrabi, Pei Zhou, Fred Morstatter, Jay Pu-\njara, Xiang Ren, and Aram Galstyan. 2021. Lawyers\nare dishonest? quantifying representational harms in\ncommonsense knowledge resources. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5016–5033, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nNasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon,\nDavid Buchanan, Lauren Berkowitz, Or Biran, and\nJennifer Chu-Carroll. 2020. GLUCOSE: Gener-\naLized and COntextualized story explanations. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4569–4586, Online. Association for Computa-\ntional Linguistics.\nTuan-Phong Nguyen and Simon Razniewski. 2022. Ma-\nterialized knowledge bases from commonsense trans-\nformers. In Proceedings of the First Workshop on\nCommonsense Representation and Reasoning (CSRR\n2022), pages 36–42, Dublin, Ireland. Association for\nComputational Linguistics.\nTuan-Phong Nguyen, Simon Razniewski, and Gerhard\nWeikum. 2021. Advanced semantics for common-\nsense knowledge extraction. In Proceedings of the\nWeb Conference 2021. ACM.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nJulien Romero, Simon Razniewski, Koninika Pal, Jeff\nZ. Pan, Archit Sakhadeo, and Gerhard Weikum. 2019.\nCommonsense Properties from Query Logs and Ques-\ntion Answering Forums. In Proceedings of the 28th\nACM International Conference on Information and\nKnowledge Management, CIKM ’19, pages 1411–\n1420, New York, NY , USA. Association for Comput-\ning Machinery.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A. Smith, and Yejin Choi. 2019.\nATOMIC: An Atlas of Machine Commonsense for\nIf-Then Reasoning. Proceedings of the AAAI Confer-\nence on Artificial Intelligence, 33:3027–3035.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptNet 5.5: An Open Multilingual Graph of\nGeneral Knowledge. In Proceedings of the AAAI\nConference on Artificial Intelligence , pages 4444–\n4451.\nMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sung-\ndong Kim, and Jaewoo Kang. 2021. Can language\nmodels be biomedical knowledge bases? In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 4723–4734,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nNiket Tandon, Gerard de Melo, Fabian Suchanek, and\nGerhard Weikum. 2014. WebChild: harvesting and\norganizing commonsense knowledge from the web.\nIn Proceedings of the 7th ACM international confer-\nence on Web search and data mining , WSDM ’14,\npages 523–532, New York, NY , USA. Association\nfor Computing Machinery.\nZhixing Tian, Yuanzhe Zhang, Kang Liu, Jun Zhao,\nYantao Jia, and Zhicheng Sheng. 2020. Scene restor-\ning for narrative machine reading comprehension. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3063–3073, Online. Association for Computa-\ntional Linguistics.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can\ngenerative pre-trained language models serve as\nknowledge bases for closed-book QA? In Proceed-\nings of the 59th Annual Meeting of the Association for\n9262\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3241–3251, Online.\nAssociation for Computational Linguistics.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D\nHwang, Liwei Jiang, Ronan Le Bras, Ximing\nLu, Sean Welleck, and Yejin Choi. 2021. Sym-\nbolic knowledge distillation: from general language\nmodels to commonsense models. arXiv preprint\narXiv:2110.07178.\nGenta Indra Winata, Andrea Madotto, Zhaojiang Lin,\nRosanne Liu, Jason Yosinski, and Pascale Fung. 2021.\nLanguage models are few-shot multilingual learners.\nIn Proceedings of the 1st Workshop on Multilingual\nRepresentation Learning. Association for Computa-\ntional Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao,\nZhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\nWang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yi-\nfan Yao, Xinjing Huang, Jun Wang, Jianfeng Yu,\nQi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao\nTao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing\nJiang, Han Zhang, Lingfeng Deng, Yehong Zhang,\nZhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo,\nShanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng\nJin, Qun Liu, and Yonghong Tian. 2021. Pangu-\nα: Large-scale autoregressive pretrained chinese lan-\nguage models with auto-parallel computation.\nHongming Zhang, Daniel Khashabi, Yangqiu Song,\nand Dan Roth. 2020. TransOMCS: From linguistic\ngraphs to commonsense knowledge. In Proceedings\nof the Twenty-Ninth International Joint Conference\non Artificial Intelligence , pages 4004–4010. Inter-\nnational Joint Conferences on Artificial Intelligence\nOrganization.\nZhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen,\nChaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi,\nJian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng,\nZhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao\nHan, Yang Liu, Xiaoyan Zhu, and Maosong Sun.\n2021. CPM-2: Large-scale cost-effective pre-trained\nlanguage models. AI Open, 2:216–224.\nA Annotation Details\nIn this study, we mainly have two kinds of annota-\ntion tasks, annotating training data for filters and\nhuman evaluation for results. Both of them require\nthe workers to review a bunch of triples and an-\nswer questions. We show the annotation page in\nFigure 6. There are three questions for the workers:\n(1) Whether the head is acceptable. (2) Whether\nthe tail is acceptable. (3) Whether the triple is\nacceptable as a whole.\nFor question (1) and (2), we provide one op-\ntion for acceptance and four options for rejection:\nabnormal expressions (syntax errors), violation of\ncommonsense (impossible situations), unusable for-\nmat (incomplete generation or containing names of\nreal identities), and mismatch to the relation.\nFor question (3), there are four options\nabout whether the triple is acceptable (plausi-\nble): “always/often”, “sometimes/likely”, “far-\nfetched/never”, and “invalid”.\nBefore annotation, each worker are shown with\na instruction, which contains background knowl-\nedge of the annotation task and examples for each\nanswer options.\nB Alternative Models for Generation\nDuring the pilot studies, we also compared several\nalternative text generation backbones besides mT5,\nincluding a multilingual autoregressive model\nXGLM (Winata et al., 2021) and two Chinese-\nspecific models, Pangu-Alpha (Zeng et al., 2021)\nand CPM2 (Zhang et al., 2021). We construct\na small set of test prompts to make the models\ncomplete some commonsense knowledge triples.\nFor each model, We use its biggest publicly avail-\nable checkpoint to generate 100 results for each\nprompt (directly sampling without hyper-parameter\nsearch).\nWe show the results and samples in Table 12\nand Table 13. All these backbones can generate\nsome plausible results. However, the XGLM (7.5B)\nmodel often generate <unk> tokens and CPM2\n(11B) model sometimes give degenerate or irrele-\nvant long results. In general, Pangu-Alpha (13B)\nand mT5-XXL (13B) have better generation qual-\nity. Based on comprehensive consideration of effi-\nciency and feasibility, we conduct full experiments\nand analyses with mT5-XXL in this paper.\n9263\nFigure 6: The annotation page.\nmT5-XXL (13B) XGLM (7.5B) Pangu-Alpha (13B) CPM2 (11B)\nPilot Tested\nTriple Acceptance 0.58 0.42 0.61 0.46\nTable 12: The acceptance of generated triples for different backbone generation models.\n9264\nPrompt: . . . . . . X爱上Y，在此之后，X想要\n......X falls in love with Y . As a result, X wants to\nmT5-XXL (13B)[\"当Y的妻子\", \"娶Y\", \"看电视\", \"追求\", \"结婚\"]\n[\"become Y’s wife\", \"marry Y (as the husband)\", \"watch TV\", \"chase\", \"get married\"]\nXGLM (7.5B) [\"与李四保持联系\", \"追求李四\", \"去做手术\", \"和Y谈恋爱\", \"向Y表白\"]\n[\"keep in touch with Y\", \"chase Y\", \"go to surgery\", \"have a love affair with Y\", \"confess love to Y\"]\nPangu-Alpha (13B)[\"拆散二人\", \"拥有Y\", \"结婚\", \"追Y\", \"结识新朋友\"]\n[\"break up two\", \"own Y\", \"get married\", \"chase Y\", \"making new friends\"]\nCPM2 (11B) [\"追回Y\", \"Y嫁给Z\", \"求她\", \"将Y追回来\", \"嫁给Y\"]\n[\"chase back Y\", \"Y marry Z, \"beg her\", \"chase Y back\", \"marry Y (as the wife)\"]\nPrompt: . . . . . .在X买书之前，X需要\n......X buys book. Before that, X needs to\nmT5-XXL (13B)[\"借书\", \"钱\", \"书\", \"在书店\", \"看书\", \"有钱\"]\n[\"borrow books\", \"money\", \"book\", \"in the bookstore\", \"read books\", \"have money\"]\nXGLM (7.5B) [\"买书\", \"花钱\",\"拥有一本书\",\"看一本书\",\"将书带回家\"]\n[\"buy a book\", \"spend money\", \"own a book\", \"read a book\", \"take the book home\"]\nPangu-Alpha (13B)[\"拥有图书馆的任何一本书\",\"准备考试\",\"拥有书签\",\"去拿书\"]\n[\"own any book in the library\", \"prepare for the exam\", \"own a bookmark\", \"get a book\"]\nCPM2 (11B) [\"存钱10\",\"能说服某人拿一堆书看\",\"在11点完成手上的工作\",\"赚到钱\"]\n[\"save money 10\", \"able to persuade someone to read a pile of books\", \"finish the work at 11 o’clock\", \"make money\"]\nPrompt: . . . . . . X发动战争，据此，可以看出X是\n......When X start a war, people think X is\nmT5-XXL (13B)[\"凶悍的\", \"自私的\", \"暴戾的\", \"野蛮的\", \"有胆量的\"]\n[\"ferocious\", \"selfish\", \"violent\", \"barbaric\", \"courageous\"]\nXGLM (7.5B) [\"强<unk>的\", \"聪明的\", \"勇敢的\", \"胆小怕事的\"]\n[\"Str<unk>g\", \"smart\", \"brave\", \"timid\"]\nPangu-Alpha (13B)[\"粗暴的\", \"自私的\", \"有勇有谋的\", \"奋起抗争的\", \"莽撞的\"]\n[\"rude\", \"selfish\", \"Brave and resourceful\", \"arising to struggle\", \"reckless\"]\nCPM2 (11B) [\"失去\",\"凶残的\",\"残暴的\",\"作大仗的\",\"偏激的\"]\n[\" loss\", \"ferocious\", \"brutal\", (meaningless expression), \"extreme\"]\nPrompt: . . . . . . X复习，对此，X感觉\n. . . . . . X reviews (lessons). For that, X feels\nmT5-XXL (13B)[\"疲惫\", \"压力\", \"紧张\", \"无聊\"]\n[\"tired\", \"stress\", \"nervous\", \"boring\"]\nXGLM (7.5B) [\"疲<unk>\",\"轻松\",\"精力充<unk>\",\"紧张\"]\n[\"tir<unk>\", \"relaxed\", \"energe<unk>\", \"nervous\"]\nPangu-Alpha (13B)[\"考得不错\",\"没意义\",\"头疼\",\"学习压力太大\",\"轻松\"]\n[\"good grades\", \"meaningless\", \"headache\", \"too much study pressure\", \"relaxed\"]\nCPM2 (11B) [\"寂寞\", \"紧张\", \"与以往不同\", \"压力大\"]\n[\"lonely\", \"nervous\", \"different from the past\", \"too much pressure\"]\nTable 13: The sampled generation results of different backbone models. We omit the seed examples in prompts and\nonly show the part to be complete. For legibility, we also replace the person names with X, Y or Z.\n9265",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8449099063873291
    },
    {
      "name": "Bootstrapping (finance)",
      "score": 0.8082411289215088
    },
    {
      "name": "Natural language processing",
      "score": 0.6237627863883972
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.6114141345024109
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5927996635437012
    },
    {
      "name": "Reuse",
      "score": 0.5630427002906799
    },
    {
      "name": "Process (computing)",
      "score": 0.5489471554756165
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5302046537399292
    },
    {
      "name": "Parallel corpora",
      "score": 0.517220139503479
    },
    {
      "name": "Machine translation",
      "score": 0.44027677178382874
    },
    {
      "name": "Language model",
      "score": 0.4155692458152771
    },
    {
      "name": "Knowledge extraction",
      "score": 0.1638159453868866
    },
    {
      "name": "Programming language",
      "score": 0.11824175715446472
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Financial economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210094879",
      "name": "Shandong Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ]
}