{
    "title": "Colorization Transformer",
    "url": "https://openalex.org/W3127839344",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1991299761",
            "name": "Kumar, Manoj",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281193570",
            "name": "Weissenborn, Dirk",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221877697",
            "name": "Kalchbrenner, Nal",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2785714067",
        "https://openalex.org/W2804078698",
        "https://openalex.org/W2016922058",
        "https://openalex.org/W2169321544",
        "https://openalex.org/W2006957355",
        "https://openalex.org/W2295537950",
        "https://openalex.org/W2979841973",
        "https://openalex.org/W2103155998",
        "https://openalex.org/W3148140980",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2007607024",
        "https://openalex.org/W2603777577",
        "https://openalex.org/W2308529009",
        "https://openalex.org/W2963073614",
        "https://openalex.org/W1986416281",
        "https://openalex.org/W2584890299",
        "https://openalex.org/W2963245493",
        "https://openalex.org/W2211456655",
        "https://openalex.org/W2136154655",
        "https://openalex.org/W2160530465",
        "https://openalex.org/W3098736533",
        "https://openalex.org/W2005964300",
        "https://openalex.org/W2461158874",
        "https://openalex.org/W1834627138",
        "https://openalex.org/W2069345396",
        "https://openalex.org/W2129112648",
        "https://openalex.org/W2963426332",
        "https://openalex.org/W2120963736",
        "https://openalex.org/W2326925005",
        "https://openalex.org/W2931764303",
        "https://openalex.org/W2891401260",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2240798854"
    ],
    "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in more than 60% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available at https://github.com/google-research/google-research/tree/master/coltran",
    "full_text": "Published as a conference paper at ICLR 2021\nCOLORIZATION TRANSFORMER\nManoj Kumar, Dirk Weissenborn & Nal Kalchbrenner\nGoogle Research, Brain Team\n{mechcoder,diwe,nalk}@google.com\nABSTRACT\nWe present the Colorization Transformer, a novel approach for diverse high ﬁdelity\nimage colorization based on self-attention. Given a grayscale image, the coloriza-\ntion proceeds in three steps. We ﬁrst use a conditional autoregressive transformer to\nproduce a low resolution coarse coloring of the grayscale image. Our architecture\nadopts conditional transformer layers to effectively condition grayscale input. Two\nsubsequent fully parallel networks upsample the coarse colored low resolution\nimage into a ﬁnely colored high resolution image. Sampling from the Colorization\nTransformer produces diverse colorings whose ﬁdelity outperforms the previous\nstate-of-the-art on colorising ImageNet based on FID results and based on a human\nevaluation in a Mechanical Turk test. Remarkably, in more than 60% of cases\nhuman evaluators prefer the highest rated among three generated colorings over the\nground truth. The code and pre-trained checkpoints for Colorization Transformer\nare publicly available at this url.\n1 I NTRODUCTION\nFigure 1:Samples of our model showing diverse, high-ﬁdelity colorizations.\nImage colorization is a challenging, inherently stochastic task that requires a semantic understanding\nof the scene as well as knowledge of the world. Core immediate applications of the technique include\nproducing organic new colorizations of existing image and video content as well as giving life to\noriginally grayscale media, such as old archival images (Tsaftaris et al., 2014), videos (Geshwind,\n1986) and black-and-white cartoons (S`ykora et al., 2004; Qu et al., 2006; Cinarel & Zhang, 2017).\nColorization also has important technical uses as a way to learn meaningful representations without\nexplicit supervision (Zhang et al., 2016; Larsson et al., 2016; V ondrick et al., 2018) or as an unsuper-\nvised data augmentation technique, whereby diverse semantics-preserving colorizations of labelled\nimages are produced with a colorization model trained on a potentially much larger set of unlabelled\nimages.\nThe current state-of-the-art in automated colorization are neural generative approaches based on\nlog-likelihood estimation (Guadarrama et al., 2017; Royer et al., 2017; Ardizzone et al., 2019).\nProbabilistic models are a natural ﬁt for the one-to-many task of image colorization and obtain better\nresults than earlier determinisitic approaches avoiding some of the persistent pitfalls (Zhang et al.,\n2016). Probabilistic models also have the central advantage of producing multiple diverse colorings\nthat are sampled from the learnt distribution.\nIn this paper, we introduce the Colorization Transformer (ColTran), a probabilistic colorization\nmodel composed only of axial self-attention blocks (Ho et al., 2019b; Wang et al., 2020). The main\n1\narXiv:2102.04432v2  [cs.CV]  7 Mar 2021\nPublished as a conference paper at ICLR 2021\nadvantages of axial self-attention blocks are the ability to capture a global receptive ﬁeld with only\ntwo layers and O(D\n√\nD) instead of O(D2) complexity. They can be implemented efﬁciently using\nmatrix-multiplications on modern accelerators such as TPUs (Jouppi et al., 2017). In order to enable\ncolorization of high-resolution grayscale images, we decompose the task into three simpler sequential\nsubtasks: coarse low resolution autoregressive colorization, parallel color and spatial super-resolution.\nFor coarse low resolution colorization, we apply a conditional variant of Axial Transformer (Ho et al.,\n2019b), a state-of-the-art autoregressive image generation model that does not require custom kernels\n(Child et al., 2019). While Axial Transformers support conditioning by biasing the input, we ﬁnd that\ndirectly conditioning the transformer layers can improve results signiﬁcantly. Finally, by leveraging\nthe semi-parallel sampling mechanism of Axial Transformers we are able to colorize images faster\nat higher resolution than previous work (Guadarrama et al., 2017) and as an effect this results in\nimproved colorization ﬁdelity. Finally, we employ fast parallel deterministic upsampling models to\nsuper-resolve the coarsely colorized image into the ﬁnal high resolution output. In summary, our\nmain contributions are:\n• First application of transformers for high-resolution (256 ×256) image colorization.\n• We introduce conditional transformer layers for low-resolution coarse colorization in Section\n4.1. The conditional layers incorporate conditioning information via multiple learnable\ncomponents that are applied per-pixel and per-channel. We validate the contribution of each\ncomponent with extensive experimentation and ablation studies.\n• We propose training an auxiliary parallel prediction model jointly with the low resolution\ncoarse colorization model in Section 4.2. Improved FID scores demonstrate the usefulness\nof this auxiliary model.\n• We establish a new state-of-the-art on image colorization outperforming prior methods by a\nlarge margin on FID scores and a 2-Alternative Forced Choice (2AFC) Mechanical Turk test.\nRemarkably, in more than 60% of cases human evaluators prefer the highest rated among\nthree generated colorings over the ground truth.\n2 R ELATED WORK\nColorization methods have initially relied on human-in-the-loop approaches to provide hints in the\nform of scribbles (Levin et al., 2004; Ironi et al., 2005; Huang et al., 2005; Yatziv & Sapiro, 2006;\nQu et al., 2006; Luan et al., 2007; Tsaftaris et al., 2014; Zhang et al., 2017; Ci et al., 2018) and\nexemplar-based techniques that involve identifying a reference source image to copy colors from\n(Reinhard et al., 2001; Welsh et al., 2002; Tai et al., 2005; Ironi et al., 2005; Pitié et al., 2007;\nMorimoto et al., 2009; Gupta et al., 2012; Xiao et al., 2020). Exemplar based techniques have been\nrecently extended to video as well (Zhang et al., 2019a). In the past few years, the focus has moved\non to more automated, neural colorization methods. The deterministic colorization techniques such\nas CIC (Zhang et al., 2016), LRAC (Larsson et al., 2016), LTBC (Iizuka et al., 2016), Pix2Pix (Isola\net al., 2017) and DC (Cheng et al., 2015; Dahl, 2016) involve variations of CNNs to model per-pixel\ncolor information conditioned on the intensity.\nGenerative colorization models typically extend unconditional image generation models to incorporate\nconditioning information from a grayscale image. Speciﬁcally, cINN (Ardizzone et al., 2019) use\nconditional normalizing ﬂows (Dinh et al., 2014), V AE-MDN (Deshpande et al., 2017; 2015) and\nSCC-DC (Messaoud et al., 2018) use conditional V AEs (Kingma & Welling, 2013), and cGAN (Cao\net al., 2017) use GANs (Goodfellow et al., 2014) for generative colorization. Most closely related to\nColTran are other autoregressive approaches such as PixColor (Guadarrama et al., 2017) and PIC\n(Royer et al., 2017) with PixColor obtaining slightly better results than PIC due to its CNN-based\nupsampling strategy. ColTran is similar to PixColor in the usage of an autoregressive model for\nlow resolution colorization and parallel spatial upsampling. ColTran differs from PixColor in the\nfollowing ways. We train ColTran in a completely unsupervised fashion, while the conditioning\nnetwork in PixColor requires pre-training with an object detection network that provides substantial\nsemantic information. PixColor relies on PixelCNN (Oord et al., 2016) that requires a large depth to\nmodel interactions between all pixels. ColTran relies on Axial Transformer (Ho et al., 2019b) and\ncan model all interactions between pixels with just 2 layers. PixColor uses different architectures\nfor conditioning, colorization and super-resolution, while ColTran is conceptually simpler as we\nuse self-attention blocks everywhere for both colorization and superresolution. Finally, we train\n2\nPublished as a conference paper at ICLR 2021\nour autoregressive model on a single coarse channel and a separate color upsampling network that\nimproves ﬁdelity (See: 5.3). The multi-stage generation process in ColTran that upsamples in depth\nand in size is related to that used in Subscale Pixel Networks (Menick & Kalchbrenner, 2018) for\nimage generation, with differences in the order and representation of bits as well as in the use of fully\nparallel networks. The self-attention blocks that are the building blocks of ColTran were initially\ndeveloped for machine translation (Vaswani et al., 2017), but are now widely used in a number of\nother applications including density estimation (Parmar et al., 2018; Child et al., 2019; Ho et al.,\n2019a; Weissenborn et al., 2019) and GANs (Zhang et al., 2019b)\n3 B ACKGROUND : A XIAL TRANSFORMER\n3.1 R OW AND COLUMN SELF -ATTENTION\nSelf-attention (SA) has become a standard building block in many neural architectures. Although\nthe complexity of self-attention is quadratic with the number of input elements (here pixels), it has\nbecome quite popular for image modeling recently (Parmar et al., 2018; Weissenborn et al., 2019) due\nto modeling innovations that don’t require running global self-attention between all pixels. Following\nthe work of (Ho et al., 2019b) we employ standard qkv self-attention (Vaswani et al., 2017) within\nrows and columns of an image. By alternating row- and column self-attention we effectively allow\nglobal exchange of information between all pixel positions. For the sake of brevity we omit the exact\nequations for multihead self-attention and refer the interested reader to the Appendix H for more\ndetails. Row/column attention layers are the core components of our model. We use them in the\nautoregressive colorizer, the spatial upsampler and the color upsampler.\n3.2 A XIAL TRANSFORMER\nThs Axial Transformer (Ho et al., 2019b) is an autoregressive model that applies (masked) row- and\ncolumn self-attention operations in a way that efﬁciently summarizes all past information xi,<j and\nx<i,·to model a distribution over pixel xi,j at position i,j. Causal masking is employed by setting\nall Am,n = 0where n>m during self-attention (see Eq. 15).\nOuter decoder. The outer decoder computes a state so over all previous rows x≤i,·by applying N\nlayers of full row self-attention followed by masked column self-attention. (Eq 2). so is shifted down\nby a single row, such that the output context oi,j at position i,j only contains information about\npixels x<i,·from prior rows. (Eq 3)\ne = Embeddings(x) (1)\nso = MaskedColumn(Row(e)) ×N (2)\no = ShiftDown(so) (3)\nInner decoder. The embeddings to the inner decoder are shifted right by a single column to mask\nthe current pixel xi,j. The context o from the outer decoder conditions the inner decoder by biasing\nthe shifted embeddings. It then computes a ﬁnal state h, by applying N layers of masked row-wise\nself-attention to infuse additional information from prior pixels of the same row xi,<j (Eq 4). hi,j\ncomprises information about all past pixelsx<i and xi,<j. A dense layer projects h into a distribution\np(xij) over the pixel at position (i,j) conditioned on all previous pixels xi,<j and x<i,·.\nz = o + ShiftRight(e) (4)\nh = MaskedRow(z) ×N (5)\np(xij) =Dense(h) (6)\nEncoder. As shown above, the outer and inner decoder operate on 2-D inputs, such as a single\nchannel of an image. For multi-channel RGB images, when modeling the \"current channel\", the\nAxial Transformer incorporates information from prior channels of an image (as per raster order)\nwith an encoder. The encoder encodes each prior channel independently with a stack of unmasked\nrow/column attention layers. The encoder outputs across all prior channels are summed to output a\nconditioning context c for the \"current channel\". The context conditions the outer and inner decoder\nby biasing the inputs in Eq 1 and Eq 4 respectively.\n3\nPublished as a conference paper at ICLR 2021\nShift \ndown\nColTran \nCore\nGrayscale \nencoder\nN \nx\nConditional \nMasked \nRow \nAttention \nEmbeddings\n+\nArea \ninterpolation\nConditional \nRow \nAttention \nBlock\nConditional \nMasked \nColumn \nAttention \nEmbeddings\n+\nShift \nright\nLinear\nEmbeddings\nEmbeddings\n+\nSoftmax\nSample\nArgmax\nColTran \nUpsamplers\nOuter \nDecoder\n+\nN \nx\nLinear\nSoftmax\nColumn \nAttention\nRow \nAttention\nN \nx\nEmbeddings\nLinear\nSoftmax\nColumn \nAttention\nRow \nAttention\nN \nx\nN \nx\nLinear\nSoftmax\nColumn \nAttention\nRow \nAttention\nArgmax\nEmbeddings\nInner \nDecoder\nFigure 2:Depiction of ColTran. It consists of 3 individual models: an autoregressive colorizer (left), a color\nupsampler (middle) and a spatial upsampler (right). Each model is optimized independently. The autoregressive\ncolorizer (ColTran core) is an instantiation of Axial Transformer (Sec. 3.2, Ho et al. (2019b)) with conditional\ntransformer layers and an auxiliary parallel head proposed in this work (Sec. 4.1). During training, the ground-\ntruth coarse low resolution image is both the input to the decoder and the target. Masked layers ensure that\nthe conditional distributions for each pixel depends solely on previous ground-truth pixels. (See Appendix G\nfor a recap on autoregressive models). ColTran upsamplers are stacked row/column attention layers that\ndeterministically upsample color and space in parallel. Each attention block (in green) is residual and consists of\nthe following operations: layer-norm →multihead self-attention →MLP.\nSampling. The Axial Transformer natively supports semi-parallel sampling that avoids re-\nevaluation of the entire network to generate each pixel of a RGB image. The encoder is run\nonce per-channel, the outer decoder is run once per-row and the inner decoder is run once per-pixel.\nThe context from the outer decoder and the encoder is initially zero. The encoder conditions the\nouter decoder (Eq 1) and the encoder + outer decoder condition the inner decoder (Eq 4). The inner\ndecoder then generates a row, one pixel at a time via Eqs. (4) to (6). After generating all pixels in a\nrow, the outer decoder recomputes context via Eqs. (1) to (3) and the inner decoder generates the\nnext row. This proceeds till all the pixels in a channel are generated. The encoder, then recomputes\ncontext to generate the next channel.\n4 P ROPOSED ARCHITECTURE\nImage colorization is the task of transforming a grayscale image xg ∈RH×W×1 into a colored\nimage x∈RH×W×3. The task is inherently stochastic; for a given grayscale image xg, there exists\na conditional distribution over x, p(x|xg). Instead of predicting x directly from xg, we instead\nsequentially predict two intermediate low resolution images xs↓ and xs↓c↓ with different color depth\nﬁrst. Besides simplifying the task of high-resolution image colorization into simpler tasks, the smaller\nresolution allows for training larger models.\nWe obtain xs↓, a spatially downsampled representation of x, by standard area interpolation. xs↓c↓ is a\n3 bit per-channel representation of xs↓, that is, each color channel has only 8 intensities. Thus, there\nare 83 = 512coarse colors per pixel which are predicted directly as a single “color” channel. We\nrewrite the conditional likelihood p(x|xg) to incorporate the intermediate representations as follows:\np(x|xg) =p(x|xg) ·1 =p(x|xg) ·p(xs↓c↓\n,xs↓\n|x,xg) =p(xs↓c↓\n,xs↓\n,x|xg) (7)\n= p(x|xs↓\n,xg) ·p(xs↓\n|xs↓c↓\n,xg) ·p(xs↓c↓\n|xg) (8)\nColTran core (Section 4.1), a parallel color upsampler and a parallel spatial upsampler (Section 4.3)\nmodel p(xs↓c↓|xg),p(xs↓|xs↓c↓,xg) and p(x|xs↓) respectively. In the subsections below, we describe\n4\nPublished as a conference paper at ICLR 2021\nComponent Unconditional Conditional\nSelf-Attention y = Softmax(qk⊤\n√\nD )v\ny = Softmax(\nqck⊤\nc√\nD )vc\nwhere ∀z = k,q,v\nzc = (cUz\ns) ⊙z + (cUz\nb)\nMLP y = ReLU(xU1 + b1)U2 + b2\nh = ReLU(xU1 + b1)U2 + b2\ny = (cUf\ns) ⊙h + (cUf\nb )\nLayer Norm y = βNorm(x) +γ\ny = βcNorm(x) +γc\nwhere ∀µ= βc,γc\nc ∈RH×W×D →ˆc ∈RHW×D\nµ= (u ·ˆc)Uµ\nd u ∈RHW\nTable 1:We contrast the different components of unconditional self-attention with self-attention conditioned on\ncontext c ∈RM×N×D. Learnable parameters speciﬁc to conditioning are denoted by u and U·∈RD×D.\nthese individual components in detail. From now on we will refer to all low resolutions as M ×N\nand high resolution as H×W. An illustration of the overall architecture is shown in Figure 2.\n4.1 C OLTRAN CORE\nIn this section, we describe ColTran core, a conditional variant of the Axial Transformer (Ho et al.,\n2019b) for low resolution coarse colorization. ColTran Core models a distribution pc(xs↓c↓|xg) over\n512 coarse colors for every pixel, conditioned on a low resolution grayscale image in addition to the\ncolors from previously predicted pixels as per raster order (Eq. 9).\npc(xs↓c↓\n|xg) =\nM∏\ni=1\nN∏\nj=1\npc(xs↓c↓\nij |xg,xs↓c↓\n<i ,xs↓c↓\ni,<j) (9)\nGiven a context representation c ∈RM×N×D we propose conditional transformer layers in Table 1.\nConditional transformer layers have conditional versions of all components within the standard\nattention block (see Appendix H, Eqs. 14-18).\nConditional Self-Attention.For every layer in the decoder, we apply six 1×1 convolutions to c to\nobtain three scale and shift vectors which we apply element-wise to q, k and v of the self-attention\noperation (Appendix 3.1), respectively.\nConditional MLP.A standard component of the transformer architecture is a two layer pointwise\nfeed-forward network after the self-attention layer. We scale and shift to the output of each MLP\nconditioned on c as for self-attention.\nConditional Layer Norm.Layer normalization (Ba et al., 2016) globally scales and shifts a given\nnormalized input using learnable vectors β, γ. Instead, we predict βc and γc as a function of c. We\nﬁrst aggregate c into a global 1-D representation c ∈RL via a learnable, spatial pooling layer. Spatial\npooling is initialized as a mean pooling layer. Similar to 1-D conditional normalization layers (Perez\net al., 2017; De Vries et al., 2017; Dumoulin et al., 2016; Huang & Belongie, 2017), we then apply a\nlinear projection on c to predict βc and γc, respectively.\nA grayscale encoder consisting of multiple, alternating row and column self-attention layers encodes\nthe grayscale image into the initial conditioning contextcg. It serves as both context for the conditional\nlayers and as additional input to the embeddings of the outer decoder. The sum of the outer decoder’s\noutput and cg condition the inner decoder. Figure 2 illustrates how conditioning is applied in the\nautoregressive core of the ColTran architecture.\nConditioning every layer via multiple components allows stronger gradient signals through the\nencoder and as an effect the encoder can learn better contextual representations. We validate this\nempirically by outperforming the native Axial Transformer that conditions context states by biasing\n(See Section 5.2 and Section 5.4).\n5\nPublished as a conference paper at ICLR 2021\n4.2 A UXILIARY PARALLEL MODEL\nWe additionally found it beneﬁcial to train an auxiliary parallel prediction model that models˜pc(xs↓c↓)\ndirectly on top of representations learned by the grayscale encoder which we found beneﬁcial for\nregularization (Eq. 10)\n˜pc(xs↓c↓\n|xg) =\nM∏\ni=1\nN∏\nj=1\n˜pc(xs↓c↓\nij |xg) (10)\nIntuitively, this forces the model to compute richer representations and global color structure already\nat the output of the encoder which can help conditioning and therefore has a beneﬁcial, regularizing\neffect on learning. We apply a linear projection, Uparallel ∈RL×512 on top of cg (the output of the\ngrayscale encoder) into a per-pixel distribution over 512 coarse colors. It was crucial to tune the\nrelative contribution of the autoregressive and parallel predictions to improve performance which we\nstudy in Section 5.3\n4.3 C OLOR & SPATIAL UPSAMPLING\nIn order to produce high-ﬁdelity colorized images from low resolution, coarse color images and a\ngiven high resolution grayscale image, we train color and spatial upsampling models. They share\nthe same architecture while differing in their respective inputs and resolution at which they operate.\nSimilar to the grayscale encoder, the upsamplers comprise of multiple alternating layers of row and\ncolumn self-attention. The output of the encoder is projected to compute the logits underlying the per\npixel color probabilities of the respective upsampler. Figure 2 illustrates the architectures\nColor Upsampler.We convert the coarse image xs↓c↓ ∈RM×N×1 of 512 colors back into a 3 bit\nRGB image with 8 symbols per channel. The channels are embedded using separate embedding\nmatrices to xs↓c↓\nk ∈RM×N×D, where k ∈{R,G,B }indicates the channel. We upsample each\nchannel individually conditioning only on the respective channel’s embedding. The channel em-\nbedding is summed with the respective grayscale embedding for each pixel and serve as input to\nthe subsequent self-attention layers (encoder). The output of the encoder is further projected to per\npixel-channel probability distributions ˜pc↑(xs↓\nk |xs↓c↓,xg) ∈RM×N×256 over 256 color intensities\nfor all k∈{R,G,B }(Eq. 11).\n˜pc↑(xs↓\n|xg) =\nM∏\ni=1\nN∏\nj=1\n˜pc↑(xs↓\nij |xg,xs↓c↓\n) (11)\nSpatial Upsampler.We ﬁrst naively upsample xs↓ ∈RM×N×3 into a blurry, high-resolution RGB\nimage using area interpolation. As above, we then embed each channel of the blurry RGB image and\nrun a per-channel encoder exactly the same way as with the color upsampler. The output of the encoder\nis ﬁnally projected to per pixel-channel probability distributions ˜ps↑(xk|xs↓,xg) ∈RH×W×256 over\n256 color intensities for all k∈{R,G,B }. (Eq. 12)\n˜ps↑(x|xg) =\nH∏\ni=1\nW∏\nj=1\n˜ps↑(xij|xg,xs↓\n) (12)\nIn our experiments, similar to (Guadarrama et al., 2017), we found parallel upsampling to be sufﬁcient\nfor high quality colorizations. Parallel upsampling has the huge advantage of fast generation which\nwould be notoriously slow for full autoregressive models on high resolution. To avoid plausible minor\ncolor inconsistencies between pixels, instead of sampling each pixel from the predicted distribution\nin (Eq. 12 and Eq. 11), we just use the argmax. Even though this slightly limits the potential diversity\nof colorizations, in practice we observe that sampling only coarse colors via ColTran core is enough\nto produce a great variety of colorizations.\nObjective. We train our architecture to minimize the negative log-likelihood (Eq. 13) of the data.\npc/˜pc, ˜ps↑, ˜pc↑ are maximized independently and λis a hyperparameter that controls the relative\ncontribution of pc and ˜pc\nL= (1−λ) logpc + λlog ˜pc + log˜pc↑ + log˜ps↑ (13)\n6\nPublished as a conference paper at ICLR 2021\nFigure 3: Per pixel log-likelihood of coarse colored 64 ×64 images over the validation set as a function\nof training steps. We ablate the various components of the ColTran core in each plot. Left: ColTran with\nConditional Transformer Layers vs a baseline Axial Transformer which conditions via addition (ColTran-B).\nColTran-B 2xand ColTran-B 4xrefer to wider baselines with increased model capacity. Center: Removing\neach conditional sub-component one at a time (no cLN, no cMLPand no cAtt). Right: Conditional shifts only\n(Shift), Conditional scales only (Scale), removal of kq conditioning in cAtt (cAtt, only v) and ﬁxed mean pooling\nin cLN (cLN, mean pool). See Section 5.2 for more details.\n5 E XPERIMENTS\n5.1 T RAINING AND EVALUATION\nWe evaluate ColTran on colorizing 256×256 grayscale images from the ImageNet dataset (Rus-\nsakovsky et al., 2015). We train the ColTran core, color and spatial upsamplers independently on\n16 TPUv2 chips with a batch-size of 224, 768 and 32 for 600K, 450K and 300K steps respectively.\nWe use 4 axial attention blocks in each component of our architecture, with a hidden size of 512\nand 4 heads. We use RMSprop (Tieleman & Hinton, 2012) with a ﬁxed learning rate of 3e−4. We\nset apart 10000 images from the training set as a holdout set to tune hyperparameters and perform\nablations. To compute FID, we generate 5000 samples conditioned on the grayscale images from this\nholdout set. We use the public validation set to display qualitative results and report ﬁnal numbers.\n5.2 A BLATIONS OF COLTRAN CORE\nThe autoregressive core of ColTran models downsampled, coarse-colored images of resolution64×64\nwith 512 coarse colots, conditioned on the respective grayscale image. In a series of experiments\nwe ablate the different components of the architecture (Figure 3). In the section below, we refer to\nthe conditional self-attention, conditional layer norm and conditional MLP subcomponents as cAtt,\ncLN and cMLP respectively. We report the per-pixel log-likelihood over 512 coarse colors on the\nvalidation set as a function of training steps.\nImpact of conditional transformer layers.The left side of Figure 3 illustrates the signiﬁcant\nimprovement in loss that ColTran core (with conditional transformer layers) achieves over the original\nAxial Transformer (marked ColTran-B). This demonstrates the usefulness of our proposed conditional\nlayers. Because conditional layers introduce a higher number of parameters we additionally compare\nto and outperform the original Axial Transformer baselines with 2x and 4x wider MLP dimensions\n(labeled as ColTran-B 2xand ColTran-B 4x). Both ColTran-B 2xand ColTran-B 4xhave an increased\nparameter count which makes for a fair comparison. Our results show that the increased performance\ncannot be explained solely by the fact that our model has more parameters.\nImportance of each conditional component.We perform a leave-one-out study to determine the\nimportance of each conditional component. We remove each conditional component one at a time and\nretrain the new ablated model. The curves no cLN, no cMLPand no cAttin the middle of Figure 3\nquantiﬁes our results. While each conditional component improves ﬁnal performance, cAtt plays the\nmost important role.\nMultiplicative vs Additive Interactions.Conditional transformer layers employ both conditional\nshifts and scales consisting of additive and multiplicative interactions, respectively. The curves Scale\nand Shift on the right hand side of Figure 3 demonstrate the impact of these interactions via ablated\narchitectures that use conditional shifts and conditional scales only. While both types of interactions\nare important, multiplicative interactions have a much stronger impact.\n7\nPublished as a conference paper at ICLR 2021\nFigure 4: Left:FID of generated 64 ×64 coarse samples as a function of training steps for λ = 0.01 and\nλ= 0.0. Center: Final FID scores as a function of λ. Right: FID as a function of log-likelihood.\nContext-aware dot product attention.Self-attention computes the similarity between pixel repre-\nsentations using a dot product between q and k (See: Eq 15). cAtt applies conditional shifts and\nscales on q, k and allow modifying this similarity based on contextual information. The curve cAtt,\nonly von the right of Figure 3 shows that removing this property, by conditioning only on v leads to\nworse results.\nFixed vs adaptive global representation:cLN aggregates global information with a ﬂexible learn-\nable spatial pooling layer. We experimented with a ﬁxed mean pooling layer forcing all the cLN\nlayers to use the same global representation with the same per-pixel weight. The curve cLN, mean\npool on the right of Figure 3 shows that enforcing this constraint causes inferior performance as\ncompared to even having no cLN. This indicates that different aggregations of global representations\nare important for different cLN layers.\n5.3 O THER ABLATIONS\nAuxiliary Parallel Model. We study the effect of the hyperparameter λ, which controls the con-\ntribution of the auxiliary parallel prediction model described in Section 4.2. For a given λ, we now\noptimize ˆpc(λ) = (1−λ) logpc(.) +λlog ˜pc(.) instead of just log pc(.). Note that ˜pc(.), models\neach pixel independently, which is more difﬁcult than modelling each pixel conditioned on previous\npixels given by pc(.). Hence, employing ˆpc(λ) as a holdout metric, would just lead to a trivial soluion\nat λ = 0. Instead, the FID of the generated coarse 64x64 samples provides a reliable way to ﬁnd\nan optimal value of λ. In Figure 4, at λ= 0.01, our model converges to a better FID faster with a\nmarginal but consistent ﬁnal improvement. At higher values the performance deteriorates quickly.\nUpsamplers. Upsampling coarse colored, low-resolution images to a higher resolution is much\nsimpler. Given ground truth 64 ×64 coarse images, the ColTran upsamplers map these to ﬁne grained\n256 ×256 images without any visible artifacts and FID of 16.4. For comparison, the FID between\ntwo random sets of 5000 samples from our holdout set is 15.5. It is further extremely important\nto provide the grayscale image as input to each of the individual upsamplers, without which the\ngenerated images appear highly smoothed out and the FID drops to 27.0. We also trained a single\nupsampler for both color and resolution. The FID in this case drops marginally to 16.6.\n5.4 F RECHET INCEPTION DISTANCE\nWe compute FID using colorizations of 5000 grayscale images of resolution 256 ×256 from the\nImageNet validation set as done in (Ardizzone et al., 2019). To compute the FID, we ensure that\nthere is no overlap between the grayscale images that condition ColTran and those in the ground-truth\ndistribution. In addition to ColTran, we report two additional results ColTran-S and ColTran-B.\nColTran-B refers to the baseline Axial Transformer that conditions via addition at the input. PixColor\nsamples smaller 28 ×28 colored images autoregressively as compared to ColTran’s 64×64. As a\ncontrol experiment, we train an autoregressive model on resolution 28×28 (ColTran-S) to disentangle\narchitectural choices and the inherent stochasticity of modelling higher resolution images. ColTran-S\nand ColTran-B obtains FID scores of 22.06 and 19.98 that signiﬁcantly improve over the previous\nbest FID of 24.32. Finally, ColTran achieves the best FID score of 19.37. All results are presented in\nTable 2 left.\n8\nPublished as a conference paper at ICLR 2021\nModels FID\nColTran 19.37 ±0.09\nColTran-B 19.98 ±0.20\nColTran-S 22.06 ±0.13\nPixColor [16] 24.32 ±0.21\ncGAN [3] 24.41 ±0.27\ncINN [1] 25.13 ±0.3\nV AE-MDN [11] 25.98 ±0.28\nGround truth 14.68 ±0.15\nGrayscale 30.19 ±0.1\nModels AMT Fooling rate\nColTran (Oracle) 62.0 % ±0.99\nColTran (Seed 1) 40.5 % ±0.81\nColTran (Seed 2) 42.3 %±0.76\nColTran (Seed 3) 41.7 % ±0.83\nPixColor [16] (Oracle) 38.3 % ±0.98\nPixColor (Seed 1) 33.3 % ±1.04\nPixColor (Seed 2) 35.4 % ±1.01\nPixColor (Seed 3) 33.2 % ±1.03\nCIC [56] 29.2 % ±0.98\nLRAC [27] 30.9 % ±1.02\nLTBC [22] 25.8 % ±0.97\nTable 2:We outperform various state-of-the-art colorization models both on FID (left) and human evaluation\n(right). We obtain the FID scores from (Ardizzone et al., 2019) and the human evaluation results from\n(Guadarrama et al., 2017). ColTran-B is a baseline Axial Transformer that conditions via addition and ColTran-S\nis a control experiment where we train ColTran core (See: 4.1) on smaller 28 ×28 colored images.\nFigure 5:We display the per-pixel, maximum predicted probability over 512 colors as a proxy for uncertainty.\nCorrelation between FID and Log-likelihood. For each architectural variant, Figure 4 right\nillustrates the correlation between the log-likelihood and FID after 150K training steps. There is a\nmoderately positive correlation of 0.57 between the log-likelihood and FID. Importantly, even an\nabsolute improvement on the order of 0.01 - 0.02 can improve FID signiﬁcantly. This suggests that\ndesigning architectures that achieve better log-likelihood values is likely to lead to improved FID\nscores and colorization ﬁdelity.\n5.5 Q UALITATIVE EVALUATION\nHuman Evaluation. For our qualitative assessment, we follow the protocol used in PixColor\n(Guadarrama et al., 2017). ColTran colorizes 500 grayscale images, with 3 different colorizations\nper image, denoted as seeds. Human raters assess the quality of these colorizations with a two\nalternative-forced choice (2AFC) test. We display both the ground-truth and recolorized image\nsequentially for one second in random order. The raters are then asked to identify the image with fake\ncolors. For each seed, we report the mean fooling rate over 500 colorizations and 5 different raters.\nFor the oracle methods, we use the human rating to pick the best-of-three colorizations. ColTran’s\nbest seed achieves a fooling rate of 42.3 % compared to the 35.4 % of PixColor’s best seed. ColTran\nOracle achieves a fooling rate of 62 %, indicating that human raters prefer ColTran’s best-of-three\ncolorizations over the ground truth image itself.\nVisualizing uncertainty. The autoregressive core model of ColTran should be highly uncertain\nat object boundaries when colors change. Figure 5 illustrates the per-pixel, maximum predicted\nprobability over 512 colors as a proxy for uncertainty. We observe that the model is indeed highly\nuncertain at edges and within more complicated textures.\n6 C ONCLUSION\nWe presented the Colorization Transformer (ColTran), an architecture that entirely relies on self-\nattention for image colorization. We introduce conditional transformer layers, a novel building block\nfor conditional, generative models based on self-attention. Our ablations show the superiority of\nemploying this mechanism over a number of different baselines. Finally, we demonstrate that ColTran\ncan generate diverse, high-ﬁdelity colorizations on ImageNet, which are largely indistinguishable\nfrom the ground-truth even for human raters.\n9\nPublished as a conference paper at ICLR 2021\nREFERENCES\nLynton Ardizzone, Carsten Lüth, Jakob Kruse, Carsten Rother, and Ullrich Köthe. Guided image\ngeneration with conditional invertible neural networks. arXiv preprint arXiv:1907.02392, 2019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nYun Cao, Zhiming Zhou, Weinan Zhang, and Yong Yu. Unsupervised diverse colorization via\ngenerative adversarial networks, 2017.\nZezhou Cheng, Qingxiong Yang, and Bin Sheng. Deep colorization. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 415–423, 2015.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\nYuanzheng Ci, Xinzhu Ma, Zhihui Wang, Haojie Li, and Zhongxuan Luo. User-guided deep anime\nline art colorization with conditional adversarial networks. In Proceedings of the 26th ACM\ninternational conference on Multimedia, pp. 1536–1544, 2018.\nCeyda Cinarel and Byoung-Tak Zhang. Into the colorful world of webtoons: Through the lens\nof neural networks. In 2017 14th IAPR International Conference on Document Analysis and\nRecognition (ICDAR), volume 3, pp. 35–40. IEEE, 2017.\nRyan Dahl. Automatic colorization, 2016.\nHarm De Vries, Florian Strub, Jérémie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C\nCourville. Modulating early visual processing by language. In Advances in Neural Information\nProcessing Systems, pp. 6594–6604, 2017.\nAditya Deshpande, Jason Rock, and David Forsyth. Learning large-scale automatic image colorization.\nIn Proceedings of the IEEE International Conference on Computer Vision, pp. 567–575, 2015.\nAditya Deshpande, Jiajun Lu, Mao-Chuang Yeh, Min Jin Chong, and David Forsyth. Learning\ndiverse image colorization. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 6837–6845, 2017.\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components\nestimation. arXiv preprint arXiv:1410.8516, 2014.\nVincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic\nstyle. arXiv preprint arXiv:1610.07629, 2016.\nDavid M Geshwind. Method for colorizing black and white footage, August 19 1986. US Patent\n4,606,625.\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint\narXiv:1406.2661, 2014.\nSergio Guadarrama, Ryan Dahl, David Bieber, Mohammad Norouzi, Jonathon Shlens, and Kevin\nMurphy. Pixcolor: Pixel recursive colorization. arXiv preprint arXiv:1705.07208, 2017.\nRaj Kumar Gupta, Alex Yong-Sang Chia, Deepu Rajan, Ee Sin Ng, and Huang Zhiyong. Image\ncolorization using similar images. In Proceedings of the 20th ACM international conference on\nMultimedia, pp. 369–378, 2012.\nJonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving ﬂow-\nbased generative models with variational dequantization and architecture design. arXiv preprint\narXiv:1902.00275, 2019a.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv preprint arXiv:1912.12180, 2019b.\n10\nPublished as a conference paper at ICLR 2021\nXun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normal-\nization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1501–1510,\n2017.\nYi-Chin Huang, Yi-Shin Tung, Jun-Cheng Chen, Sung-Wen Wang, and Ja-Ling Wu. An adaptive\nedge detection based colorization algorithm and its applications. In Proceedings of the 13th annual\nACM international conference on Multimedia, pp. 351–354, 2005.\nSatoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Let there be color! joint end-to-end learning\nof global and local image priors for automatic image colorization with simultaneous classiﬁcation.\nACM Transactions on Graphics (ToG), 35(4):1–11, 2016.\nRevital Ironi, Daniel Cohen-Or, and Dani Lischinski. Colorization by example. In Rendering\nTechniques, pp. 201–210. Citeseer, 2005.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\nconditional adversarial networks. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 1125–1134, 2017.\nNorman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa,\nSarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre luc Cantin, Clifford\nChao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir\nGhaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug\nHogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander\nKaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law,\nDiemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana\nMaggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy\nNix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt\nRoss, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter,\nDan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle,\nVijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter\nperformance analysis of a tensor processing unit, 2017.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic\ncolorization. In European conference on computer vision, pp. 577–593. Springer, 2016.\nAnat Levin, Dani Lischinski, and Yair Weiss. Colorization using optimization. In ACM SIGGRAPH\n2004 Papers, pp. 689–694. 2004.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\nProceedings of the IEEE international conference on computer vision, pp. 3730–3738, 2015.\nQing Luan, Fang Wen, Daniel Cohen-Or, Lin Liang, Ying-Qing Xu, and Heung-Yeung Shum. Natural\nimage colorization. In Proceedings of the 18th Eurographics conference on Rendering Techniques,\npp. 309–320, 2007.\nJacob Menick and Nal Kalchbrenner. Generating high ﬁdelity images with subscale pixel networks\nand multidimensional upscaling. arXiv preprint arXiv:1812.01608, 2018.\nSafa Messaoud, David Forsyth, and Alexander G. Schwing. Structural consistency and controllability\nfor diverse colorization. In Proceedings of the European Conference on Computer Vision (ECCV),\nSeptember 2018.\nYuji Morimoto, Yuichi Taguchi, and Takeshi Naemura. Automatic colorization of grayscale images\nusing multiple images on the web. In SIGGRAPH 2009: Talks, pp. 1–1. 2009.\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\narXiv preprint arXiv:1601.06759, 2016.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. arXiv preprint arXiv:1802.05751, 2018.\n11\nPublished as a conference paper at ICLR 2021\nEthan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual\nreasoning with a general conditioning layer. arXiv preprint arXiv:1709.07871, 2017.\nFrançois Pitié, Anil C Kokaram, and Rozenn Dahyot. Automated colour grading using colour\ndistribution transfer. Computer Vision and Image Understanding, 107(1-2):123–137, 2007.\nYingge Qu, Tien-Tsin Wong, and Pheng-Ann Heng. Manga colorization. ACM Transactions on\nGraphics (TOG), 25(3):1214–1220, 2006.\nErik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. Color transfer between images.\nIEEE Computer graphics and applications, 21(5):34–41, 2001.\nAmelie Royer, Alexander Kolesnikov, and Christoph H Lampert. Probabilistic image colorization.\narXiv preprint arXiv:1705.04258, 2017.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition\nchallenge. International journal of computer vision, 115(3):211–252, 2015.\nDaniel S`ykora, Jan Buriánek, and Jiˇrí Žára. Unsupervised colorization of black-and-white cartoons.\nIn Proceedings of the 3rd international symposium on Non-photorealistic animation and rendering,\npp. 121–127, 2004.\nYu-Wing Tai, Jiaya Jia, and Chi-Keung Tang. Local color transfer via probabilistic segmentation by\nexpectation-maximization. In 2005 IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’05), volume 1, pp. 747–754. IEEE, 2005.\nTijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31,\n2012.\nSotirios A Tsaftaris, Francesca Casadio, Jean-Louis Andral, and Aggelos K Katsaggelos. A novel\nvisualization tool for art history and conservation: Automated colorization of black and white\narchival photographs of works of art. Studies in conservation, 59(3):125–135, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nCarl V ondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Tracking\nemerges by colorizing videos. In Proceedings of the European conference on computer vision\n(ECCV), pp. 391–408, 2018.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-\ndeeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint arXiv:2003.07853,\n2020.\nDirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video models.\narXiv preprint arXiv:1906.02634, 2019.\nTomihisa Welsh, Michael Ashikhmin, and Klaus Mueller. Transferring color to greyscale images. In\nProceedings of the 29th annual conference on Computer graphics and interactive techniques, pp.\n277–280, 2002.\nChufeng Xiao, Chu Han, Zhuming Zhang, Jing Qin, Tien-Tsin Wong, Guoqiang Han, and Shengfeng\nHe. Example-based colourization via dense encoding pyramids. In Computer Graphics Forum,\nvolume 39, pp. 20–33. Wiley Online Library, 2020.\nLiron Yatziv and Guillermo Sapiro. Fast image and video colorization using chrominance blending.\nIEEE transactions on image processing, 15(5):1120–1129, 2006.\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:\nConstruction of a large-scale image dataset using deep learning with humans in the loop. arXiv\npreprint arXiv:1506.03365, 2015.\n12\nPublished as a conference paper at ICLR 2021\nBo Zhang, Mingming He, Jing Liao, Pedro V Sander, Lu Yuan, Amine Bermak, and Dong Chen.\nDeep exemplar-based video colorization. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 8052–8061, 2019a.\nHan Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative\nadversarial networks. In International Conference on Machine Learning, pp. 7354–7363. PMLR,\n2019b.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European\nconference on computer vision, pp. 649–666. Springer, 2016.\nRichard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S Lin, Tianhe Yu, and Alexei A\nEfros. Real-time user-guided image colorization with learned deep priors. arXiv preprint\narXiv:1705.02999, 2017.\n13\nPublished as a conference paper at ICLR 2021\nFigure 6: Left: FID vs training steps, with and without polyak averaging. Right: The effect of K in top-K\nsampling on FID. See Appendix B and E\nACKNOWLEDGEMENTS\nWe would like to thank Mohammad Norouzi, Rianne van den Berg, Mostafa Dehghani for their useful\ncomments on the draft and Avital Oliver for assistance in the Mechanical Turk setup.\nCHANGELOG\n• v2: Dataset Sharding ﬁx across multiple TPU workers. This changed the FID scores of\nColTran, ColTran-B and ColTran-S from their v1 values of 19.71, 21.6 and 21.9 to their v2\nvalues of 19.37, 19.98 and 22.06 respecitvely.\nA C ODE , CHECKPOINTS AND TENSORBOARD FILES\nOur implementation is open-sourced in the google-research framework at https://github.com/google-\nresearch/google-research/tree/master/coltran with a zip compressed version here. Our full set of\nhyperparameters are available here.\nWe provide pre-trained checkpoints of the colorizer and upsamplers on ImageNet at\nhttps://console.cloud.google.com/storage/browser/gresearch/coltran. Finally, reference tensorboard\nﬁles for our training runs are available at colorizer tensorboard, color upsampler tensorboard and\nspatial upsampler tensorboard.\nB E XPONENTIAL MOVING AVERAGE\nWe found using an exponential moving average (EMA) of our checkpoints, extremely crucial to\ngenerate high quality samples. In Figure 6, we display the FID as a function of training steps, with\nand without EMA. On applying EMA, our FID score improves steadily over time.\nC N UMBER OF PARAMETERS AND INFERENCE SPEED\nInference speed. ColTran core can sample a batch of 20 64x64 grayscale images in around 3.5 -5\nminutes on a P100 GPU vs PixColor that takes 10 minutes to colorize 28x28 grayscale images on a\nK40 GPU. Sampling 28x28 colorizations takes around 30 seconds. The upsampler networks take in\nthe order of milliseconds.\nFurther, in our naive implementation, we recompute the activations,cUz\ns ,cUz\nb ,cUf\ns ,cUf\nb in Table\n1 to generate every pixel in the inner decoder. Instead, we can compute these activations once\nper-grayscale image in the encoder and once per-row in the outer decoder and reuse them. This is\nlikely to speed up sampling even more and we leave this engineering optimization for future work.\nNumber of parameters. ColTran has a total of ColTran core (46M) + Color Upsampler (14M) +\nSpatial Upsampler (14M) = 74M parameters. In comparison, PixColor has Conditioning network\n(44M) + Colorizer network (11M) + Reﬁnement Network (28M) = 83M parameters.\n14\nPublished as a conference paper at ICLR 2021\nFigure 7:Ablated models. Gated: Gated conditioning layers as done in (Oord et al., 2016) and cAtt + cMLP ,\nglobal: Global conditioning instead of pointwise conditioning in cAtt and cLN.\nD L OWER COMPUTE REGIME\nWe retrained the autoregressive colorizer and color upsampler on 4 TPUv2 chips (the lowest con-\nﬁguration) with a reduced-batch size of 56 and 192 each. For the spatial upsampler, we found that\na batch-size of 8 was sub-optimal and lead to a large deterioration in loss. We thus used a smaller\nspatial upsampler with 2 axial attention blocks with a batch-size of 16 and trained it also on 4 TPUv2\nchips. The FID drops from 19.71 to 20.9 which is still signiﬁcantly better than the other models in 2.\nWe note that in this experiment, we use only 12 TPUv2 chips in total while PixColor (Guadarrama\net al., 2017) uses a total of 16 GPUs.\nE I MPROVED FID WITH TOP-K SAMPLING\nWe can improve colorization ﬁdelity and remove artifacts due to unnatural colors via Top-K sampling\nat the cost of reduced colorization diversity. In this setting, for a given pixel ColTran generates a\ncolor from the top-K colors (instead of 512 colors) as determined by the predicted probabilities. Our\nresults in Figure 6 K = 4and K = 8demonstrate a performance improvement over the baseline\nColTran model with K = 512\nF A DDITIONAL ABLATIONS :\nAdditional ablations of our conditional transformer layers are in Figure 7 which did not help.\n• Conditional transformer layers based on Gated layers (Oord et al., 2016) (Gated)\n• A global conditioning layer instead of pointwise conditioning in cAtt and cLN. cAtt + cMLP ,\nglobal\n.\nG A UTOREGRESSIVE MODELS\nAutoregressive models are a family of probabilistic methods that model joint distribution of data\nP(x) or a sequence of symbols(x1,x2,...x n) as a product of conditionals∏N\ni=1 P(xi|x<i). During\ntraining, the input to autoregressive models are the entire sequence of ground-truth symbols. Masking\nensures that the contribution of all \"future\" symbols in the sequence are zeroed out. The outputs of\nthe autoregressive model are the corresponding conditional distributions. P(xi|x<i). Optimizing the\nparameters of the autoregressive model proceeds by a standard log-likelihood objective.\nGeneration happens sequentially, symbol-by-symbol. Once a symbol xi is generated, the entire\nsequence (x1,x2,...x i) are fed to the autoregressive model to generate xi+1.\nIn the case of autoregressive image generation symbols typically correspond to the 3 RGB pixel-\nchannel. These are generated sequentially in raster-scan order, channel by channel and pixel by\npixel.\n15\nPublished as a conference paper at ICLR 2021\nFigure 8:We train our colorization model on ImageNet and display high resolution colorizations from LSUN\nH R OW/COLUMN SELF -ATTENTION\nIn the following we describe row self-attention, that is, we omit the height dimension as all operations\nare performed in parallel for each column. Given the representation of a single row within of an\nimage xi,·∈RW×D, row-wise self-attention block is applied as follows:\n[q,k,v] = LN(xi,·)Uqkv Uqkv ∈RD×3Dh (14)\nA= softmax\n(\nqk⊤/\n√\nDh\n)\nA∈RW×W (15)\nSA(xi,·) =Av (16)\nMSA(xi,·) = [SA1(xi,·),SA2(xi,·),··· ,SAk(xi,·)] Uout Uout ∈Rk·Dh×D (17)\nLN refers to the application of layer normalization (Ba et al., 2016). Finally, we apply residual\nconnections and a feed-forward neural network with a single hidden layer and ReLU activation\n(MLP) after each self-attention block as it is common practice in transformers.\nˆxi,·= MLP(LN(x′\ni,·)) +x′\ni,· x′\ni,·= MSA(xi,·) +xi,· (18)\nColumn-wise self-attention over x·,j ∈RH×D works analogously.\nI O UT OF DOMAIN COLORIZATIONS\nWe use our trained colorization model on ImageNet to colorize high-resolution grayscale images\nfrom LSUN 256 ×256 (Yu et al., 2015) and low-resolution grayscale images from Celeb-A (Liu\n16\nPublished as a conference paper at ICLR 2021\nFigure 9:We train our colorization model on ImageNet and display low resolution colorizations from Celeb-A\n17\nPublished as a conference paper at ICLR 2021\nFigure 10: Top: Colorizations Bottom: Ground truth. From left to right, our colorizations have a progressively\nhigher fooling rate.\net al., 2015) 64 ×64. Note that these models were trained only on ImageNet and not ﬁnetuned on\nCeleb-A or LSUN.\nJ N UMBER OF AXIAL ATTENTION BLOCKS\nWe did a very small hyperparameter sweep using the baseline axial transformer (no conditional layers)\nwith the following conﬁgurations:\n• hidden size = 512, number of blocks = 4\n• hidden size = 1024, number of blocks = 2\n• hidden size = 512, number of blocks = 2\nOnce we found the optimal conﬁguration, we ﬁxed this for all future architecture design.\nK A NALYSIS OF MTURK RATINGS\nFigure 11:In each column, we display the ground truth followed by 3 samples. Left: Diverse and real. Center:\nRealism improves from left to right. Right: Failure cases\nFigure 12:We display the per-pixel, maximum predicted probability over 512 colors as a proxy for uncertainty.\nWe analyzed our samples on the basis of the MTurk ratings in Figure 11. To the left, we show images,\nwhere all the samples have a fool rate > 60 %. Our model is able to show diversity in color for both\nhigh-level structure and low-level details. In the center, we display samples that have a high variance\nin MTurk ratings, with a difference of 80 % between the best and the worst sample. All of these are\ncomplex objects, that our model is able to colorize reasonably well given multiple attempts. To the\nright of Figure 11, we show failure cases where all samples have a fool rate of 0 %, For these cases,\nour model is unable to colorize highly complex structure, that would arguably be difﬁcult even for a\nhuman.\n18\nPublished as a conference paper at ICLR 2021\nL M ORE PROBABILITY MAPS\nWe display additional probability maps to visualize uncertainty as done in 5.5.\nM M ORE SAMPLES\nWe display a wide-diversity of colorizations from ColTran that were not cherry-picked.\n19\nPublished as a conference paper at ICLR 2021\n20\nPublished as a conference paper at ICLR 2021\n21\nPublished as a conference paper at ICLR 2021\n22\nPublished as a conference paper at ICLR 2021\n23\nPublished as a conference paper at ICLR 2021\n24"
}