{
    "title": "Bayesian Transformer Language Models for Speech Recognition",
    "url": "https://openalex.org/W3128159137",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2353717108",
            "name": "Xue, Boyang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2232452073",
            "name": "Yu Jianwei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2495477728",
            "name": "Xu, Junhao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3162642130",
            "name": "Liu, Shansong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3182179045",
            "name": "Hu, Shoukang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2285746836",
            "name": "Ye Zi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222904452",
            "name": "Geng, Mengzhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2745215271",
            "name": "Liu, Xunying",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2743877034",
            "name": "Meng, Helen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W71499226",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W582134693",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W1524333225",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2413794162",
        "https://openalex.org/W2888867175",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W3161873458",
        "https://openalex.org/W2514741789",
        "https://openalex.org/W2597655663",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W2951004968",
        "https://openalex.org/W2267186426",
        "https://openalex.org/W3015484572",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2936202865",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2963386218",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2972942899",
        "https://openalex.org/W2904530326",
        "https://openalex.org/W2108677974",
        "https://openalex.org/W2970971315",
        "https://openalex.org/W2171928131",
        "https://openalex.org/W2094147890",
        "https://openalex.org/W2943845043",
        "https://openalex.org/W2242047860",
        "https://openalex.org/W2951595529",
        "https://openalex.org/W2964059111",
        "https://openalex.org/W2963403868"
    ],
    "abstract": "State-of-the-art neural language models (LMs) represented by Transformers are highly complex. Their use of fixed, deterministic parameter estimates fail to account for model uncertainty and lead to over-fitting and poor generalization when given limited training data. In order to address these issues, this paper proposes a full Bayesian learning framework for Transformer LM estimation. Efficient variational inference based approaches are used to estimate the latent parameter posterior distributions associated with different parts of the Transformer model architecture including multi-head self-attention, feed forward and embedding layers. Statistically significant word error rate (WER) reductions up to 0.5\\% absolute (3.18\\% relative) and consistent perplexity gains were obtained over the baseline Transformer LMs on state-of-the-art Switchboard corpus trained LF-MMI factored TDNN systems with i-Vector speaker adaptation. Performance improvements were also obtained on a cross domain LM adaptation task requiring porting a Transformer LM trained on the Switchboard and Fisher data to a low-resource DementiaBank elderly speech corpus.",
    "full_text": "arXiv:2102.04754v1  [cs.CL]  9 Feb 2021\nBA YESIAN TRANSFORMER LANGU AGE MODELS FOR SPEECH RECOGNITION\nBoyang Xue∗, Jianwei Y u∗ , Junhao Xu, Shansong Liu, Shoukang Hu, Zi Y e,\nMengzhe Geng, Xunying Liu, Helen Meng\n1The Chinese University of Hong Kong\n{byxue,jwyu,jhxu,ssliu,skhu,zye,mzgeng,xyliu,hmmeng}@se.cuhk.edu.hk\nABSTRACT\nState-of-the-art neural language models (LMs) represente d by Trans-\nformers are highly complex. Their use of ﬁxed, deterministi c pa-\nrameter estimates fail to account for model uncertainty and lead to\nover-ﬁtting and poor generalization when given limited tra ining data.\nIn order to address these issues, this paper proposes a full B ayesian\nlearning framework for Transformer LM estimation. Efﬁcien t varia-\ntional inference based approaches are used to estimate the l atent pa-\nrameter posterior distributions associated with differen t parts of the\nTransformer model architecture including multi-head self -attention,\nfeed forward and embedding layers. Statistically signiﬁca nt word er-\nror rate (WER) reductions up to 0.5% absolute (3.18% relativ e) and\nconsistent perplexity gains were obtained over the baselin e Trans-\nformer LMs on state-of-the-art Switchboard corpus trained LF-MMI\nfactored TDNN systems with i-V ector speaker adaptation. Pe rfor-\nmance improvements were also obtained on a cross domain LM\nadaptation task requiring porting a Transformer LM trained on the\nSwitchboard and Fisher data to a low-resource DementiaBank el-\nderly speech corpus.\nIndex T erms— neural language models, Transformer, Bayesian\nlearning, model uncertainty , speech recognition\n1. INTRODUCTION\nLanguage models (LMs) play an important role in automatic sp eech\nrecognition (ASR) systems and many other applications. Lan guage\nmodels compute the joint probability of a given sentence W =\n(w1, w2, ..., wT ) as:\np(W) =p(w1, w2, ..., wn) =\nn∏\nt=1\nP (wt|wt− 1, ..., w1) (1)\nwhich can be expressed using the multiplication of word leve l prob-\nabilities. The key part of the statistical language modelli ng prob-\nlem is to learn long-range context dependencies. Directly m odelling\nlong-span contexts lead to a severe data sparsity problem fo r n-gram\nlanguage models [1]. T o this end, neural language models tha t can\nrepresent longer span preceding history contexts in a conti nuous vec-\ntor space, for example, based on long-short term memory recu rrent\nneural networks (LSTM-RNNs) [2, 3] can be used.\nIn recent years deep Transformer models [4] have deﬁned stat e-\nof-the-art language modelling performance across a range o f speech\nrecognition tasks [5]. The Transformer model architecture features a\ndeep stacking of multiple self-attention layers [6, 7, 8] wi th residual\nconnections [9] and layer normalization [10] to learn long- range con-\ntexts. Positional encoding layers [4, 11] are used to furthe r augment\nEqual contribution\nthe self-attention layers with sequence order information . Perfor-\nmance improvements over the conventional LSTM-RNN languag e\nmodels have been widely reported [5, 12].\nThe highly complex neural architecture design of Transform ers\noften leads to a large increase in the overall system complex ity , for\nexample, up to hundreds of millions of parameters [5]. In com mon\nwith other deep learning based language modelling approach es [2, 3],\nthe use of ﬁxed, deterministic parameter estimates in conve ntional\nTransformer models fails to account for model uncertainty . When\ngiven limited training data, standard Transformer models a re prone\nto over-ﬁtting and poor generalization. This issue can be fu rther ag-\ngregated when rapidly adapting a well-trained Transformer model\nto small size dataset associated with a new style, genre or do main\n[12]. The current solution to this problem is largely based o n dropout\n[13], a simple and effective regularization approach used i n many\ndeep learning systems including neural network language mo dels\n[14, 15, 16]. However, it lacks of a mathematically well-deﬁ ned\nframework [17, 18]. The underlying dropout distribution al so re-\nquires hyper-parameter setting on an empirical basis for di fferent\ntasks.\nIn order to address these issues, this paper proposes a full\nBayesian learning framework to account for model uncertain ty in\nTransformer language model estimation. An efﬁcient variat ional\ninference based approach is adopted to estimate the latent p arameter\nposterior distribution. A systematic investigation on the effects of\nperforming Bayesian estimation in different parts of the Tr ansformer\nmodel architecture including the self-attention, feed for ward and em-\nbedding layers is performed. Statistically signiﬁcant wor d error rate\n(WER) reductions up to 0.5% absolute (3.18% relative) were o b-\ntained over the baseline Transformer LM on a state-of-the-a rt 900\nhour speed perturbed Switchboard corpus trained LF-MMI fac tored\nTDNN system with i-V ector speaker adaptation [19]. Consist ent per-\nformance improvements were also obtained on a cross domain L M\nadaptation task requiring rapidly porting a Transformer LM trained\non Switchboard and Fisher data to a small size DementiaBank\nelderly speech corpus.\nThe main contributions of this paper are summarized as follo ws.\nFirst, to the best of our knowledge, this paper is the ﬁrst wor k to\napply Bayesian learning methods to Transformer language mo dels\nfor speech recognition tasks. In contrast, the only previou s research\non Bayesian Transformer [18, 20] was conducted on machine tr ans-\nlation and probabilistic programming tasks. Prior works on uncer-\ntainty modelling under the Bayesian framework for neural ne twork\nlanguage modelling approaches were limited to RNNs [14] and their\nLSTM or GRU based variants [16].\nThe rest of this paper is organized as follows. Section 2 revi ews\nthe conventional Transformer based language models. Secti on 3\npresents Bayesian Transformer language models. Implement ation\nissues are discussed in Section 4. Experiments and results a re shown\nin section 5. Finally , conclusions and future work are discu ssed in\nsection 6.\n2. TRANSFORMER LANGUAGE MODELS\nThe original Transformer architecture proposed in [4] for n eural ma-\nchine translation contains an encoder and a decoder. In this work,\nfollowing [5, 12, 21], the decoder component inside the Tran sformer\narchitecture was adopted for language modelling.\nAs shown in Figure 1, the Transformer language model used in\nthis work is a stack of 6 Transformer decoder blocks. Each blo ck\nconsists of a multi-head self-attention [7, 8] module and a f eed for-\nward module. Residual connections [9] and layer normalizat ion [10]\nare also inserted between these two modules. Let xl− 1\nt denotes the\noutput of the (l − 1)-th Transformer block at time t. The multi-head\nself-attention module in the l-th block transforms xl− 1\nt to zl\nt is given\nas follows:\nql\nt, kl\nt, vl\nt = Qxl− 1\nt , Kxl− 1\nt , Vxl− 1\nt (2)\nhl\nt = (hl\nt− 1, (kl\nt, vl\nt)) (3)\nyl\nt = Wh\nlSelfAttention(hl\nt, ql\nt) +xl− 1\nt (4)\nzl\nt = LayerNorm(yl\nt) (5)\nwhere Q, K, V are projection matrices which map the input xl− 1\nt\ninto query ql\nt, key kl\nt and value vl\nt respectively . hl\nt is the sequence\nof of cached key-value pairs up to time t, which only contains the\nhistory context information and can prevent the model from u sing\nany future context. SelfAttention denotes the scaled multi -head dot\nproduct self-attention [4]. LayerNorm represents the laye r normal-\nization operation [10]. Wh denotes the projection matrix applied to\nthe outputs of the SelfAttention operation for residual con nection [9].\nThe normalized output zl\nt is then fed into the feed forward module:\nsl\nt = W2\nlGELU(Wl\n1zl\nt) +zl\nt (6)\nxl\nt = LayerNorm(sl\nt) (7)\nIn this work, the Gaussian error linear unit (GELU) activati on func-\ntion [22] is adopted as the activation function in the feed fo rward\nmodule.\nFig. 1 : An illustration of the proposed Bayesian Transformer lan-\nguage model architecture. θF\ni and θA\ni denotes the Bayesian model\nparameters in the feed forward and multi-head self-attenti on module\nrespectively .\n3. BA YESIAN TRANSFORMER LMS\nIn this section, we ﬁrst propose the formulation of the Bayes ian\nTransformer LM and then present an efﬁcient training scheme based\non vairational inference [23, 24] for the proposed model.\n3.1. Bayesian Neural Language Model\nAlthough Transformer LMs have demonstrated state-of-the- art per-\nformance on many speech recognition tasks, the use of ﬁxed-p oint\nparameter estimates in these models fails to account for the model\nuncertainty associated with the words prediction. When giv en lim-\nited training data, standard Transformer models are prone t o over-\nﬁtting and poor generalization. T o model the parameter unce rtainty\nin Transformer LMs, Bayesian neural networks can be adopted to\ntreat the model parameters Θ as a posterior probability distribution\np(Θ |D). Given the word history context, the word prediction at\nframe t is computed as follows:\np(wt|w1, ...,wt− 1)\n=\n∫\np(wt|w1, ..., wt− 1, Θ )p(Θ |D)dΘ (8)\nwhere D represents the whole training set for model development\nand p(Θ |D) denotes the posterior distribution of the model parame-\nters learned from the training data.\n3.2. V ariational T raining for Bayesian T ransformer LMs\nT o estimate the posterior distribution of the model paramet ers\np(Θ |D), the usual approach in Bayesian learning is to maximize the\nmarginal probability . However, computing this marginal di stribution\nL is intractable under the Transfomer LM framework. Thus, the fol-\nlowing variational lower bound is often adopted as an approx imation\n[25]:\nlog p(D) = log\n∫\np(D|Θ )pr(Θ )dΘ (9)\n≥\nN∑\nn=1\nlog\n∫\np(Wn|Θ )q(Θ )dΘ\n\n \nL1\n− KL(q(Θ )||pr(Θ ))  \nL2\n= L\n(10)\nwhere Wn denotes the n-th sentence in the training set D and N\nis the total number of sentence in the training set. q(Θ ) is the\nvariational approximation of the parameter posterior dist ribution\np(Θ |D), pr(Θ ) is the prior distribution of Θ and KL (·||·) denotes\nthe Kullback-Leiber (KL) divergence. As shown in Equation ( 10),\nthe variational lower bound can be decomposed into two parts : 1)\nthe expectation of the log likelihood of the word sequence W over\nthe approximated posterior distribution q(Θ ); 2) the KL divergence\nbetween q(Θ ) and the prior distribution pr(Θ ). Equation (10) is\nused as the objective function during the model training pro cess.\nAs commonly adopted in [18], both q(Θ ) and pr(Θ ) are as-\nsumed to be diagonal Gaussian distributions in this work:\nq(Θ ) =N (Θ ;µµµ,σσσ), p r(Θ ) =N (Θ ;µµµr,σσσr) (11)\nThe expectation log likelihood term in Equation (10) can be e fﬁ-\nciently approximated by the Monte Carlo sampling method:\nL1 ≈ 1\nK\nK∑\nk=1\np(D|Θ k) (12)\nwhere K is the number of samples and Θ k is the k-th sample from\ndistribution q(Θ ). It has been reported that directly using the mean µµµ\nand variance σσσ to sample Θ k can make the training process unstable.\nT o address this issue, the reparameterization trick [26] is adopted to\nsample Θ k as follows:\nΘ = µµµ + σσσ ⊙ ǫǫǫk, ǫ ǫǫk ∼ N (0, I). (13)\nUnder the Gaussian assumption, the second term of Equation ( 10)\ncan be computed as:\nKL(q(Θ )||pr(Θ )) =\n∑\ni\n{\nlog σr,i\nσi\n+ σ2\ni + (µi − µr,i)2\n2σ2\nr,i\n− 1\n2\n}\n(14)\nwhere µi and σi are the i-th component of µµµ and σσσ respectively . The\ngradient of the Bayesian model parameters µµµ and σσσ can be computed\nusing the standard back-propagation algorithm as follows:\n∂L\n∂µi\n= 1\nK\nK∑\nk=1\n∂L1\n∂µi\n− µi − µr,i\nσ2\nj\n(15)\n∂L\n∂σi\n= 1\nK\nK∑\nk=1\n∂L1\n∂σi\n− σ2\ni − σ2\nr,i\nσ2\nj\n(16)\n3.3. Implementation details of the Bayesian T ransformer LM\nThe performance and efﬁciency of the proposed Bayesian Tran s-\nformer LMs are affected by the follow details:\nPosition of uncertainty modelling: Although applying Bayesian\nestimation to all model parameters in the Transformer LM is t he-\noretically feasible, it is practically highly expensive in both model\ntraining and evaluation. T o solve this issue, the Bayesian e stimation\nis only applied on part of the model parameters to narrow down the\nscope of uncertainty modelling. Equation (9) can be re-writ ten as:\nlog p(D) = log\n∫\np(D|Θ )pr(θθθ)dθθθ (17)\nwhere θθθ ∈ Θ is the part of parameters associated with Bayesian\nestimation. W e applied Bayesian estimation to the feed forw ard\nand multi-head self-attention modules in the Transformer b lock and\nthe embedding layer respectively . Speciﬁcally , when Bayes ian infer-\nence is applied on the multi-head attention layers, the quer y , key and\nvalue weight matrices in Equation (2) are assumed to be indep endent\namong themselves, thus separate variational distribution s are used to\nmodel the uncertainty associated with them.\nParameter sampling strategy: As shown in Equation (12), the\nBayesian Transformer LM requires Monte Carlo sampling to ap prox-\nimate the log likelihood. The computation cost of the model i s lin-\nearly increased respect to the number of samples K. T o maintain\nthe Bayesian Transformer LM’s computation cost comparable to the\nstandard model, we set K = 1during the training stage. As for eval-\nuation, we use the mean of the Bayesian parameters to approxi mate\nEquation (8) as follows:\np(wt|w1, ...,wt− 1) ≈ p(wt|w1, ..., wt− 1, Θ mean) (18)\nChoice of prior distribution: When training the Bayesian Trans-\nformer LM, a suitable choice of the prior is required. In our e xperi-\nments we used the parameters obtained from a standard Transf ormer\nLM as the prior’s mean. The prior’s variance is set to be 1. All the\nTransformer and Bayesian Transformer LMs are interpolated with\nthe 4gram LM.\n4. EXPERIMENT AL SETUP\nIn this section, we present the details of the datasets used i n the exper-\niments before introducing the baseline speech recognition systems.\n4.1. Datasets\nSwitchboard and Fisher : The combined Switchboard and Fisher\ntranscriptions adopted in our experiments contain 34M word s with a\n30k vocabulary lexicon for language modelling.\nDementiaBank Pitt : The small DementiaBank Pitt transcription\n[27] adopted in our domain adaptation experiments contains 167k.\nA 3.6k words recognition vocabulary was used.\n4.2. Baseline T ransformer LMs\nThe standard and Bayesian Transfomer LMs used in our experim ents\ncontain 6 Transformer blocks with 4096 hidden nodes in the fe ed\nforward module and 512 dimension for the residual connectio n. The\noutput dimensionality of the word embedding layer is set to b e 512\nand the input dimensionality is set to be equal to the vocabul ary size\nof the dataset. Pytorch was used to implement the Transforme r LMs.\nThe model parameters are optimized using stochastic gradie nt de-\nscent (SGD) optimizer with initial learning rate 0.1. All Tr ansformer\nLMs in our experiments are on word level. W e use 1 Nvidia V100\nGPUs to train the LMs.\n4.3. Baseline Speech Recognition Systems\nSwitchboard system: Following the Kaldi [28] recipe 1 , in the\nSwitchboard experiments, the speech recognition system us ed to\ngenerate the N-best list for rescoring was based on factoriz ed time-\ndelay neural networks (TDNN-Fs) [29] featuring speech pert urba-\ntion, i-V ector, LHUC speaker adaptation and Lattice-free m aximum\nmutual information (LF-MMI) [30] sequence training.\nDementiaBank Pitt system: The speech recognition system used\nthe DementiaBank Pitt experiments was similar to the TDNN-F\nbased Switchboard system with additional domain adaptatio n. De-\ntails of this system can be found in [27].\n5. EXPERIMENTS\nIn this section, we present our experimental results in term s of per-\nplexity (PPL) and word error rate (WER) for the proposed Baye sian\nTransformer LMs on the Switchboard and DementiaBank corpor a.\n5.1. Experiments on the Switchboard Corpus\nT able 1 presents the experimental results of the proposed Tr ans-\nformer language model on the Switchboard corpus. Several tr ends\ncan be observed from T able 1: 1) The proposed Bayesian Trans-\nformer LMs (line 3-5) outperform the baseline Transformer l an-\nguage model (line 2) in terms of both perplexity and word erro r\nrate. 2) Applying the Bayesian estimation on the feed forwar d (FF)\nmodule outperforms using Bayesian estimation on the multi- head\nself-attention (MAH) module and the embedding (EMB) layer i n\nterms of the PPL and WER; 3) Compared with applying Bayesian\nestimation to multiple Transformer blocks (line 6 - 10), ado pting\nBayesian estimation only on the lowest Transformer block (l ine 5)\nproduced the best PPL and WER performance. One possible expl a-\nnation of this observation is that the parameters associate d with the\nhigher Transformer blocks are expected to be more determini stic\n1 Kaldi: egs/swbd/s5c/local/chain/tuning/run tdnn 7q.sh\nT able 1: Perplexity and WER(%) of the baseline 4-gram (4g) LM, Trans former LM and various Bayesian Transformer LMs before and af ter\nfurther interpolation with the baseline Transformer on NIS T Switchboard English eval2000, rt02 and rt03 test sets. FF , MHA and EMB\nrepresent the feed forward, multi-head self-attention and the embedding layer respectively . ”+4g” denotes interpola tion with 4gram. ” †”\ndenotes statistically signiﬁcant results were obtained ov er the Transformer baseline (line 2).\nID LM Bayesian PPL eval2000 rt02 rt03\nBlock Position (swbd) swbd callhm swbd1 swbd2 swbd3 fsh swbd\n1 4gram Not Applied - 9.7 18.0 11.5 15.3 20.0 12.6 19.5\n2 Transformer(+4g) Not Applied 41.50 7.9 15.7 9.5 12.8 17.4 10.4 17.3\n3\nBayes Transformer(+4g)\n- EMB 41.01 7.7 15.6 9.5 12.6 17.1 † 10.2 17.1 †\n4 1 MHA 40.95 7.7 15.5 9.5 12.5† 17.1† 10.2 17.1 †\n5 1 FF 40.65 7.7 15.4† 9.4 12.6† 17.0† 10.2† 17.0†\n6 1-2 FF 41.11 7.7 15.6 9.5 12.6 17.2 10.3 17.1\n7 1-3 FF 42.45 7.8 15.8 9.5 12.7 17.2 10.3 17.2\n8 1-4 FF 47.54 8.0 16.0 9.9 13.0 17.6 10.7 17.5\n9 1-5 FF 54.19 8.3 16.2 10.2 13.5 18.0 11.1 18.0\n10 1-6 FF 74.50 8.9 17.3 10.8 14.3 18.7 12.0 18.8\n11\n+Transformer(+4g)\n- EMB 40.03 7.7 15.5 9.4 12.6 † 17.1† 10.1† 17.0†\n12 1 MHA 39.70 7.6† 15.4† 9.3 12.5† 17.0† 10.1† 16.9†\n13 1 FF 39.42 7.6† 15.2† 9.3 12.5 † 17.0† 10.1† 16.9†\nthan those in the lower blocks, while the larger part of the un der-\nlying data variability is expected at the lowest block immed iately\nafter the embedding layer. 4) Further performance improvem ents\ncan be obtained by interpolating the Bayesian Transform LM w ith\nthe standard Transformer LM (line 11-14).\nThe Bayesian Transformer LM with parameter uncertainty mod -\nelled at the lowest feed forward layer produced the best perf ormance\nafter interpolation with both the 4-gram and baseline Trans former\n(line 13). Statistically signiﬁcant WER reductions of 0.3- 0.5%\nwere obtained across all the data sets except the swbd1 portion\nof rt02 over the baseline Transformer LM (line 2). The statisti-\ncal signiﬁcance test was conducted at level 0.5 based on matc hed\npairs sentence-segment word error (MAPSSWE) for recogniti on\nperformance analysis.\nT o further analyse the proposed Bayesian Transformer LM’s\nability in reducing the risk of overﬁtting and improving gen eral-\nization, Figure 2 compares the performance between the prop osed\nBayesian Transformer LM (line 5 in T able 1) and the standard T rans-\nformer with and without the dropout operation. As shown in Fi gure\n2, the proposed Bayesian LM consistently outperforms the ot her two\nLMs with varying feed forward module dimensionality from 51 2 to\n16384.\nFig. 2 : Perplexity on SWBD test data obtained using the baseline\nand the Bayesian Transformer LMs with varying feed forward l ayer\ndimensionality .\n5.2. Experiments on the DementiaBank Pitt Corpus\nThe PPL and WER results on the DementiaBank Corpus are pre-\nsented in T able 2. The 4gram, Transformer and Bayesian Trans-\nformer LMs were ﬁrst trained using the combined 2.4M words De -\nmentiaBank Pitt, Switchboard and Fisher transcriptions. T o reduce\nthe domain mismatch between the three corpora, the baseline and\nBayesian Transformer LMs were further adapted to the Pitt tr an-\nscripts only by using either ﬁne-tuning or Bayesian adaptat ion. This\ntwo adapted Transformer LMs are shown in line 3 and 5 in T able\n2 respectively . The ﬁne-tuning adapted model parameters in line 3\nserved as the prior of the Bayesian Transformer adaptation i n line\n5. It can be observed from T able 2 that the Bayesian adapted Tr ans-\nformer LM outperforms the ﬁne-tuned Transformer LM by 0.37%\nabsolute WER reduction.\nT able 2: PPL and WER(%) results on the DemntiaBank Pitt Corpus.\n”ﬁnetune” means ﬁne-tuning the model parameters using only the\nDemntiaBank Pitt Corpus. ”bayes-adapt” mean adapt the Baye sian\nTransformer using only the DemntiaBank Pitt Corpus LM with t he\nparameters in system 4 as prior. ”+4g” denotes interpolatio n with\n4gram. ” †” denotes statistically signiﬁcant results were obtained\nover the system 3.\nID LMs Adapt PPL WER(%)\n1 4gram ✗ 17.07 30.67\n2 Transformer(+4g) ✗ 21.83 30.65\n3 ﬁne-tuning 14.56 30.25\n4 Bayes Transformer(+4g) ✗ 19.88 30.49\n5 bayes-adapt 13.99 29.88†\n6. CONCLUSION\nThis paper presents a Bayesian learning framework for Trans former\nlanguage model estimation to improve their generalization perfor-\nmance. Consistent performance improvements in terms of bot h per-\nplexity and WER were obtained on the Swithboard and Dementia -\nBank Pitt datasets, thus demonstrating the advantages of th e pro-\nposed Bayesian Transformer LMs for speech recognition.\n7. ACKNOWLEDGEMENT\nThis research is supported by Hong Kong RGC GRF grant No.\n14200218, 14200220, Theme-based Research Scheme T45-407/ 19N,\nInnovation & T echnology Fund grant No. ITS/254/19, and Shun\nHing Institute of Advanced Engineering grant No. MMT -p1-19 .\n8. REFERENCES\n[1] Reinhard Kneser and Hermann Ney , “Improved backing-off\nfor m-gram language modeling, ” in ICASSP. IEEE, 1995,\nvol. 1, pp. 181–184.\n[2] T omas Mikolov , Martin Karaﬁ ´ at, Lukas Burget, Jan Cerno ck,\nand Sanjeev Khudanpur, “Recurrent neural network based lan -\nguage model, ” in Interspeech, 2015.\n[3] T om´ aˇ s Mikolov , Stefan Kombrink, Luk´ aˇ s Burget, Jan\nˇCernock ` y, and Sanjeev Khudanpur, “Extensions of recurren t\nneural network language model, ” in ICASSP. IEEE, 2011, pp.\n5528–5531.\n[4] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkor eit,\nLlion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polo-\nsukhin, “ Attention is all you need, ” in Advances in neural\ninformation processing systems, 2017, pp. 5998–6008.\n[5] Kazuki Irie, Albert Zeyer, Ralf Schl ¨ uter, and Hermann N ey ,\n“Language Modeling with Deep Transformers, ” in Interspeech,\n2019, pp. 3905–3909.\n[6] Jianpeng Cheng, Li Dong, and Mirella Lapata, “Long short -\nterm memory-networks for machine reading, ” in Proceedings\nof the 2016 Conference on Empirical Methods in Natural Lan-\nguage Processing. 2016, pp. 551–561, Association for Compu-\ntational Linguistics.\n[7] Zhouhan Lin, Minwei Feng, C´ ıcero Nogueira dos Santos,\nMo Y u, Bing Xiang, Bowen Zhou, and Y oshua Bengio, “ A\nstructured self-attentive sentence embedding, ” CoRR, vol.\nabs/1703.03130, 2017.\n[8] Ankur Parikh, Oscar T¨ ackstr ¨ om, Dipanjan Das, and Jako b\nUszkoreit, “ A decomposable attention model for natural lan -\nguage inference, ” in Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing , Austin,\nT exas, Nov . 2016, pp. 2249–2255.\n[9] Kaiming He, X. Zhang, Shaoqing Ren, and J. Sun, “Deep\nresidual learning for image recognition, ” in CVPR, June 2016,\npp. 770–778.\n[10] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton, “Layer normalization, ” arXiv preprint arXiv:1607.06450,\n2016.\n[11] Jonas Gehring, Michael Auli, David Grangier, Denis Y ar ats,\nand Y ann N. Dauphin, “Convolutional sequence to sequence\nlearning, ” in ICML. 2017, p. 1243–1252, JMLR.org.\n[12] Ke Li, Zhe Liu, Tianxing He, Hongzhao Huang, and Sanjeev\nKhudanpur, “ An empirical study of transformer-based neura l\nlanguage model adaptation, ” in ICASSP, 2020.\n[13] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky , Ilya\nSutskever, and Ruslan Salakhutdinov , “Dropout: A simple wa y\nto prevent neural networks from overﬁtting, ” J. Mach. Learn.\nRes., vol. 15, no. 1, pp. 1929–1958, Jan. 2014.\n[14] Jen-Tzung Chien and Y uan Chu Ku, “Bayesian recurrent ne u-\nral network for language modeling, ” IEEE Transactions on\nNeural Networks and Learning Systems, vol. 27, no. 2, pp. 361–\n374, Feb. 2016.\n[15] Max W . Y . Lam, Xie Chen, Shoukang Hu, Jianwei Y u, and He-\nlen M. Meng, “Gaussian process lstm recurrent neural networ k\nlanguage models for speech recognition, ” in IEEE ICASSP ,\n2019.\n[16] Jianwei Y u, Max W . Y . Lam, Shoukang Hu, Xixin Wu, and\nHelen M. Meng, “Comparative study of parametric and rep-\nresentation uncertainty modeling for recurrent neural net work\nlanguage models, ” in Interspeech, 2019.\n[17] Y arin Gal and Zoubin Ghahramani, “Dropout as a bayesian\napproximation: Representing model uncertainty in deep lea rn-\ning, ” ICML, 2015.\n[18] Dustin Tran, Mike Dusenberry , Mark van der Wilk, and Dan i-\njar Hafner, “Bayesian layers: A module for neural network\nuncertainty , ” in Advances in Neural Information Processing\nSystems 32, pp. 14660–14672. Curran Associates, Inc., 2019.\n[19] Pawel Swietojanski and Steve Renals, “Learning hidden unit\ncontributions for unsupervised speaker adaptation of neur al net-\nwork acoustic models, ” in 2014 IEEE Spoken Language T ech-\nnology W orkshop (SLT). IEEE, 2014, pp. 171–176.\n[20] Charles Y uan and Jan Hoffmann, “Blt: Exact bayesian inf er-\nence with distribution transformers, ” .\n[21] Alec Radford, Karthik Narasimhan, Tim Salimans, and Il ya\nSutskever, “Improving language understanding by generati ve\npre-training, ” 2018.\n[22] Dan Hendrycks and Kevin Gimpel, “Bridging nonlinearit ies\nand stochastic regularizers with gaussian error linear uni ts, ”\nCoRR, vol. abs/1606.08415, 2016.\n[23] David Barber and Christopher M Bishop, “Ensemble learn ing\nin bayesian neural networks, ” Nato ASI Series F Computer and\nSystems Sciences, vol. 168, pp. 215–238, 1998.\n[24] Alex Graves, “Practical variational inference for neu ral net-\nworks, ” in Advances in neural information processing systems,\n2011, pp. 2348–2356.\n[25] Diederik P Kingma and Max W elling, “ Auto-encoding vari a-\ntional bayes, ” arXiv preprint arXiv:1312.6114, 2013.\n[26] Diederik P Kingma, Tim Salimans, and Max W elling, “V ari -\national dropout and the local reparameterization trick, ” Com-\nputer Science, 2015.\n[27] Zi YE, Shoukang Hu, Jinchao Li, Xurong Xie, Mengzhe Geng ,\nJianwei Y u, Junhao Xu, Boyang Xue, Shansong Liu, Xuny-\ning Liu, and Helen Meng, “Development of the cuhk elderly\nspeech recognition system for neurocognitive disorder det ec-\ntion using the dementiabank corpus, ” in ICASSP. IEEE, 2021.\n[28] Daniel Povey , Arnab Ghoshal, Gilles Boulianne, Lukas B ur-\nget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr\nMotlicek, Y anmin Qian, Petr Schwarz, Jan Silovsky , Georg\nStemmer, and Karel V esely , “The kaldi speech recognition\ntoolkit, ” 2011, IEEE Catalog.\n[29] Daniel Povey , Gaofeng Cheng, Yiming W ang, Ke Li, Hainan\nXu, Mahsa Y armohammadi, and Sanjeev Khudanpur, “Semi-\northogonal low-rank matrix factorization for deep neural n et-\nworks., ” in Interspeech, 2018, pp. 3743–3747.\n[30] Daniel Povey , Vijayaditya Peddinti, Daniel Galvez, Pe gah\nGhahremani, Vimal Manohar, Xingyu Na, Yiming W ang, and\nSanjeev Khudanpur, “Purely sequence-trained neural netwo rks\nfor asr based on lattice-free mmi, ” Interspeech, pp. 2751–2755,\n2016."
}