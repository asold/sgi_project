{
    "title": "Towards Deployment of Robust AI Agents for Human-Machine Partnerships",
    "url": "https://openalex.org/W2978648282",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287217598",
            "name": "Ghosh, Ahana",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282815486",
            "name": "Tschiatschek, Sebastian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288957392",
            "name": "Mahdavi, Hamed",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224911517",
            "name": "Singla, Adish",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2964182728",
        "https://openalex.org/W2912793366",
        "https://openalex.org/W2769019609",
        "https://openalex.org/W2751659792",
        "https://openalex.org/W1999874108",
        "https://openalex.org/W2963289505",
        "https://openalex.org/W2145339207",
        "https://openalex.org/W2594794854",
        "https://openalex.org/W2963231791",
        "https://openalex.org/W2134824646",
        "https://openalex.org/W2410842990",
        "https://openalex.org/W2031305127",
        "https://openalex.org/W2964267629",
        "https://openalex.org/W2966120739",
        "https://openalex.org/W2952765942",
        "https://openalex.org/W2964251366",
        "https://openalex.org/W2891287243",
        "https://openalex.org/W2970502043",
        "https://openalex.org/W2916904544",
        "https://openalex.org/W1571120101",
        "https://openalex.org/W2563829177",
        "https://openalex.org/W3105729849",
        "https://openalex.org/W2082555149",
        "https://openalex.org/W2890752237",
        "https://openalex.org/W1992643004",
        "https://openalex.org/W2735388838",
        "https://openalex.org/W2103104707",
        "https://openalex.org/W2963627051",
        "https://openalex.org/W2091891040",
        "https://openalex.org/W2891076394",
        "https://openalex.org/W2171392812",
        "https://openalex.org/W2789066082",
        "https://openalex.org/W2914115734"
    ],
    "abstract": "We study the problem of designing AI agents that can robustly cooperate with people in human-machine partnerships. Our work is inspired by real-life scenarios in which an AI agent, e.g., a virtual assistant, has to cooperate with new users after its deployment. We model this problem via a parametric MDP framework where the parameters correspond to a user's type and characterize her behavior. In the test phase, the AI agent has to interact with a user of unknown type. Our approach to designing a robust AI agent relies on observing the user's actions to make inferences about the user's type and adapting its policy to facilitate efficient cooperation. We show that without being adaptive, an AI agent can end up performing arbitrarily bad in the test phase. We develop two algorithms for computing policies that automatically adapt to the user in the test phase. We demonstrate the effectiveness of our approach in solving a two-agent collaborative task.",
    "full_text": "Towards Deployment of Robust AI Agents\nfor Human-Machine Partnerships\nAhana Ghosh\nMPI-SWS\ngahana@mpi-sws.org\nSebastian Tschiatschek\nMicrosoft Research\nsetschia@microsoft.com\nHamed Mahdavi\nMPI-SWS\nhmahdavi@mpi-sws.org\nAdish Singla\nMPI-SWS\nadishs@mpi-sws.org\nAbstract\nWe study the problem of designing AI agents that can robustly cooperate with\npeople in human-machine partnerships. Our work is inspired by real-life scenarios\nin which an AI agent, e.g., a virtual assistant, has to cooperate with new users after\nits deployment. We model this problem via a parametric MDP framework where\nthe parameters correspond to a userâ€™s type and characterize her behavior. In the test\nphase, the AI agent has to interact with a user of unknown type. Our approach to\ndesigning a robust AI agent relies on observing the userâ€™s actions to make inferences\nabout the userâ€™s type and adapting its policy to facilitate efï¬cient cooperation. We\nshow that without being adaptive, an AI agent can end up performing arbitrarily\nbad in the test phase. We develop two algorithms for computing policies that\nautomatically adapt to the user in the test phase. We demonstrate the effectiveness\nof our approach in solving a two-agent collaborative task.\n1 Introduction\nAn increasing number of AI systems are deployed in human-facing applications like autonomous\ndriving, medicine, and education [ Yu et al., 2017]. In these applications, the human-user\nand the AI system (agent) form a partnership, necessitating mutual awareness for achieving\noptimal results [ Hadï¬eld-Menell et al., 2016, Wilson and Daugherty, 2018, Amershi et al., 2019].\nFor instance, to provide high utility to a human-user, it is important that an AI agent can\naccount for a userâ€™s preferences deï¬ning her behavior and act accordingly, thereby being\nadaptive to the userâ€™s type [ Nikolaidis et al., 2015, Nikolaidis et al., 2017a, Amershi et al., 2019,\nTschiatschek et al., 2019, Haug et al., 2018]. As a concrete example, an AI agent for autonomous\ndriving applications should account for a userâ€™s preference to take scenic routes instead of the fastest\nroute and account for the userâ€™s need for more AI support when driving manually in confusing\nsituations.\nAI agents that do not account for the userâ€™s preferences and behavior typically degrade the utility\nfor their human users. However, this is challenging because the AI agent needs to (a) infer in-\nformation about the interacting user and (b) be able to interact efï¬ciently with a large number of\ndifferent human users, each possibly showing different behaviors. In particular, during development\nof an AI agent, it is often only possible to interact with a limited number of human users and the\nAI agent needs to generalize to new users after deployment (or acquire information needed there-\nfore quickly). This resembles multi-agent reinforcement learning settings in which an AI agent\nfaces unknown agents at test time [Grover et al., 2018] and the cold-start problem in recommender\nsystems [Bobadilla et al., 2012].\nSafety and Robustness in Decision Making (SRDM) Workshop, NeurIPS 2019.\narXiv:1910.02330v2  [cs.LG]  15 Jun 2020\nIn this paper, we study the problem of designing AI agents that can robustly cooperate with new\nunknown users for human-machine partnerships in reinforcement learning (RL) settings after de-\nployment. In these problems, the AI agent often only has access to the reward information during\nits development while no (explicit) reward information is available once the agent is deployed. As\nshown in this paper, an AI agent can only achieve high utility in this setting if it is adaptive to its\nuser while a non-adaptive AI agent can perform arbitrarily bad. We propose two adaptive policies\nfor our considered setting, one of which comes with strong theoretical robustness guarantees at test\ntime, while the other is inspired by recent deep-learning approaches for RL and is easier to scale to\nlarger problems. Both policies build upon inferring the human userâ€™s properties and leverage these\ninferences to act robustly.\nOur approach is related to ideas of multi-task, meta-learning, and generalization in reinforcement\nlearning. However, most of these approaches require access to reward information at test time and\nrarely offer theoretical guarantees for robustness (see discussion on related work in Section 7). Below,\nwe highlight our main contributions:\nâ€¢ We provide an algorithmic framework for designing robust policies for interacting with\nagents of unknown behavior. Furthermore, we prove robustness guarantees for approaches\nbuilding on our framework.\nâ€¢ We propose two policies according to our framework: ADAPT POOL which pre-computes a\nset of best-response policies and executes them adaptively based on inferences of the type\nof human-user; and ADAPT DQN which implements adaptive policies by a neural network\nin combination with an inference module.\nâ€¢ We empirically demonstrate the excellent performance of our proposed policies when facing\nan unknown user.\n2 The Problem Setup\nWe formalize the problem through a reinforcement learning (RL) framework. The agents are hereafter\nreferred to as agent Ax and agent Ay: here, agent Ay represents the AI agent whereas agentAx could\nbe a person, i.e., human user. Our goal is to develop a learning algorithm for agent Ay that leads to\nhigh utility even in cases when the behavior of agent Ax and its committed policy is unknown.\n2.1 The model\nWe model the preferences and induced behavior of agent Ax via a parametric space Î˜. From\nagent Ayâ€™s perspective, eachÎ¸âˆˆÎ˜ leads to a parameterized MDP M(Î¸) := (S,A,T Î¸,RÎ¸,Î³, D0)\nconsisting of the following:\nâ€¢ a set of states S, with sâˆˆSdenoting a generic state.\nâ€¢ a set of actions A, with aâˆˆAdenoting a generic action of agent Ay.\nâ€¢ a transition kernel parameterized by Î¸ as TÎ¸(sâ€² |s,a), which is a tensor with indices\ndeï¬ned by the current state s, the agent Ayâ€™s actiona, and the next state sâ€². In particular,\nTÎ¸(sâ€²|s,a) = Eax[Tx,y(sâ€²|s,a,a x)], where ax âˆ¼Ï€x\nÎ¸(Â·| s) is sampled from agent Axâ€™s\npolicy in state s. That is, TÎ¸(sâ€²|s,a) corresponds to the transition dynamics derived from a\ntwo-agent MDP with transition dynamics Tx,y and agent Axâ€™s policyÏ€x\nÎ¸.\nâ€¢ a reward function parameterized by Î¸as RÎ¸: SÃ—Aâ†’[0,rmax] for rmax >0. This captures\nthe preferences of agent Ax that agent Ay should account for.\nâ€¢ a discount factor Î³ âˆˆ[0,1) weighing short-term rewards against long-term rewards.\nâ€¢ an initial state distribution D0.\nOur goal is to develop a learning algorithm that achieves high utility even in cases whenÎ¸is unknown.\nIn line with the motivating applications discussed above, we consider the following two phases:\nâ€¢ Training (development) phase.During development, our learning algorithm can iteratively\ninteract with a limited number of different MDPs M(Î¸) for Î¸ âˆˆÎ˜train âŠ†Î˜: here, agent\nAy can observe rewards as well as agent Axâ€™s actions needed for learning purposes.\n2\nâ€¢ Test (deployment) phase.After deployment, our learning algorithm interacts with a pa-\nrameterized MDP as described above for unknown Î¸test âˆˆÎ˜: here, agent Ay only observes\nagent Axâ€™s actions but not rewards.\n2.2 Utility of agentAy\nFor a ï¬xed policy Ï€of agent Ay, we deï¬ne its total expected reward in the MDP MÎ¸ as follows:\nJÎ¸(Ï€) = E\n[âˆâˆ‘\nÏ„=1\nÎ³Ï„âˆ’1RÎ¸(sÏ„,aÏ„) |D0,TÎ¸,Ï€\n]\n, (1)\nwhere the expectation is over the stochasticity of policy Ï€and the transition dynamics TÎ¸. Here sÏ„ is\nthe state at time Ï„. For Ï„ = 1, this comes from the distribution D0.\nFor knownÎ¸. When the underlying parameter Î¸is known, the task of ï¬nding the best response\npolicy of agent Ay reduces to the following:\nÏ€âˆ—\nÎ¸ = arg max\nÏ€âˆˆÎ \nJÎ¸(Ï€) (2)\nwhere Î  = {Ï€|Ï€: SÃ—Aâ†’[0,1]}deï¬nes the set of stationary Markov policies.\nFor unknownÎ¸. However, when the underlying parameter Î¸âˆˆÎ˜ is unknown, we deï¬ne the best\nresponse (in a minmax sense) policy Ï€âˆˆÎ  of agent Ay as:\nÏ€âˆ—\nÎ˜ = arg min\nÏ€âˆˆÎ \nmax\nÎ¸âˆˆÎ˜\n(\nJÎ¸(Ï€âˆ—\nÎ¸) âˆ’JÎ¸(Ï€)\n)\n(3)\nClearly, JÎ¸(Ï€âˆ—\nÎ¸) âˆ’JÎ¸(Ï€âˆ—\nÎ˜) â‰¥0 âˆ€Î¸ âˆˆÎ˜. In general, this gap can be arbitrarily large, as formally\nstated in the following theorem.\nTheorem 1. There exists a problem instance where the performance of agent Ay can be arbitrarily\nworse when agent Axâ€™s typeÎ¸test is unknown. In other words, the gap maxÎ¸âˆˆÎ˜\n(\nJÎ¸(Ï€âˆ—\nÎ¸) âˆ’JÎ¸(Ï€âˆ—\nÎ˜)\n)\nis arbitrarily high.\nThe proof is presented in the supplementary material. Theorem 1 shows that the performance of agent\nAy can be arbitrarily bad when it doesnâ€™t know Î¸test and is restricted to execute a ï¬xed stationary\nMarkov policy. In the next section, we present an algorithmic framework for designing robust policies\nfor agent Ay for unknown Î¸test.\n3 Designing Robust Policies\nIn this section, we introduce our algorithmic framework for designing robust policies for the AI agent\nAy.\n3.1 Algorithmic framework\nOur approach relies on observing the behavior (i.e., actions taken) to make inferences about the\nagent Axâ€™s typeÎ¸and adapting agent Ayâ€™s policy accordingly to facilitate efï¬cient cooperation. This\nis inspired by how people make decisions in uncertain situations (e.g., ability to safely drive a car\neven if the other driver on the road is driving aggressively). The key intuition is that at test time,\nthe agent Ay can observe agent Axâ€™s actions which are taken asax âˆ¼Ï€x\nÎ¸(Â·| s) when in state sto\ninfer Î¸, and in turn use this additional information to make an improved decision on which actions to\ntake. More formally, we deï¬ne the observation history available at the beginning of timestep tas\nOtâˆ’1 = (sÏ„,ax\nÏ„)Ï„=1,...,tâˆ’1 and use it to infer the type of agent Ax and act appropriately.\nIn particular, we will make use of an INFERENCE procedure (details provided in Section 5). Given\nOtâˆ’1, this procedure returns an estimate of the type of agent Ax at time tgiven by Î¸t âˆˆÎ˜. Then, we\nconsider stochastic policies of the form Ïˆ: SÃ—AÃ—Î˜ â†’[0,1]. The space of these policies is given\n3\nAlgorithm 1Algorithmic framework for robust policies\nTraining phase\n1: Input: parameter space Î˜train\n2: adaptive policy Ïˆâ†TRAINING (Î˜train)\nTest phase\n1: Input: adaptive policy Ïˆ\n2: O0 â†()\n3: for t= 1,2,... do\n4: Observe current state st\n5: Estimate Axâ€™s type asÎ¸t â†INFERENCE (Otâˆ’1)\n6: Take action at â†Ïˆ(st,Î¸t)\n7: Observe Axâ€™s actionax\nt; Ot â†Otâˆ’1 âŠ•(st,ax\nt)\n8: end for\nby Î¨ = {Ïˆ|Ïˆ: SÃ—AÃ—Î˜ â†’[0,1]}. For a ï¬xed policy Ïˆof agent Ay and ï¬xed, unknown Î¸, we\ndeï¬ne its total expected reward in the MDP M(Î¸) as follows:\nJÎ¸(Ïˆ) = E\n[âˆâˆ‘\nÏ„=1\nÎ³Ï„âˆ’1RÎ¸(sÏ„,aÏ„) |D0,TÎ¸,Ïˆ\n]\n. (4)\nNote that at any timet, we haveat âˆ¼Ïˆ(st,Î¸t) and Otâˆ’1 = (sÏ„,ax\nÏ„)Ï„=1,...,tâˆ’1 is generated according\nto ax\nÏ„ âˆ¼Ï€x\nÎ¸(sÏ„).\nWe seek to ï¬nd the policy for agent Ay given by the following optimization problem:\nmin\nÏˆâˆˆÎ¨\nmax\nÎ¸âˆˆÎ˜\n(\nJÎ¸(Ï€âˆ—\nÎ¸) âˆ’JÎ¸(Ïˆ)\n)\n(5)\nIn the next two sections, we will design algorithms to optimize the objective in Equation(5) following\nthe framework outlined in Algorithm 1. In particular, we will discuss two possible architectures for\npolicy Ïˆand corresponding TRAINING procedures in Section 4. Then, in Section 5, we describe\nways to implement the INFERENCE procedure for inferring agent Axâ€™s type using observed actions.\nBelow, we provide theoretical insights into the robustness of the proposed algorithmic framework.\n3.2 Performance analysis\nWe begin by specifying three technical questions that are important to gain theoretical insights into\nthe robustness of the proposed framework, see below:\nQ.1 Independent of the speciï¬c procedures used for TRAINING and INFERENCE , the ï¬rst question\nto tackle is the following: When agent Axâ€™s true type isÎ¸test and agent Ay uses a best response\npolicy for Ï€âˆ—\nË†Î¸ such that ||Î¸test âˆ’Ë†Î¸||â‰¤ Ïµ, what are the performance guarantees on the total utility\nachieved by agent Ay? (see Theorem 2).\nQ.2 Regarding TRAINING procedure: When agent Axâ€™s type isÎ¸test and the inference procedure\noutputs Ë†Î¸such that ||Î¸test âˆ’Ë†Î¸||â‰¤ Ïµ, what is the performance of policy Ïˆ? (see Section 4).\nQ.3 Regarding INFERENCE procedure: When agent Axâ€™s type isÎ¸test, can we infer Ë†Î¸such that either\n||Î¸test âˆ’Ë†Î¸||is small, or agent Axâ€™s policiesÏ€x\nË†Î¸ and Ï€x\nÎ¸test are approximately equivalent? (see\nSection 5)\n3.2.1 Smoothness properties\nFor addressing Q.1, we introduce a number of properties characterizing our problem setting. These\nproperties are essentially smoothness conditions on MDPs that enable us to make statements about\nthe following intermediate issue: For two types Î¸,Î¸â€², how â€œsimilar\" are the corresponding MDPs\nM(Î¸),M(Î¸â€²) from agent Ayâ€™s point of view?\n4\nThe ï¬rst property characterizes the smoothness of rewards for agent Ay w.r.t. parameter Î¸. Formally,\nthe parametric MDP M(Î¸) is Î±-smooth with respect to the rewards if for any Î¸and Î¸â€²we have\nmax\nsâˆˆS,aâˆˆA\n|RÎ¸(s,a) âˆ’RÎ¸â€²(s,a)|â‰¤ Î±Â·rmax Â·||Î¸âˆ’Î¸â€²||2 (6)\nThe second property characterizes the smoothness of policies for agent Ax w.r.t. parameter Î¸; this in\nturn implies that the MDPâ€™s transition dynamics as perceived by agentAy are smooth. Formally, the\nparametric MDP M(Î¸) is Î²-smooth in the behavior of agent Ax if for any Î¸and Î¸â€²we have\nmax\nsâˆˆS\nKL\n(\nÏ€x\nÎ¸(.|s); Ï€x\nÎ¸â€²(.|s)\n)\nâ‰¤Î²Â·||Î¸âˆ’Î¸â€²||2. (7)\nFor instance, one setting where this property holds naturally is when Ï€x\nÎ¸ is a soft Bellman\npolicy computed w.r.t. a reward function for agent Ax which is smooth in Î¸ [Ziebart, 2010,\nKamalaruban et al., 2019].\nThe third property is a notion of inï¬‚uence as introduced by [Dimitrakakis et al., 2017]: This notion\ncaptures how much one agent can affect the probability distribution of the next state with her actions\nas perceived by the second agent. Formally, we capture the inï¬‚uence of agent Ax on agent Ay as\nfollows:\nIx:=max\nsâˆˆS\n(\nmax\na,b,bâ€²\nâˆ¥Tx,y(.|s,a,b ) âˆ’Tx,y(.|s,a,b â€²)âˆ¥1\n)\n, (8)\nwhere arepresents the action of agent Ay , b,bâ€²represents two distinct actions of agent Ax, and Tx,y\nis the transition dynamics of the two-agent MDP (see Section 2.1). Note that Ix âˆˆ[0,1] and allows\nus to do ï¬ne-grained performance analysis: for instance, when Ix = 0, then agent Ax doesnâ€™t affect\nthe transition dynamics as perceived by agent Ay and we can expect to have better performance for\nagent Ay.\n3.2.2 Guarantees\nPutting this together, we can provide the following guarantees as an answer for Q.1:\nTheorem 2. Let Î¸test âˆˆÎ˜ be the type of agent Ax at test time and agent Ay uses a policy Ï€âˆ—\nË†Î¸ such\nthat ||Î¸test âˆ’Ë†Î¸||â‰¤ Ïµ. The parameters (Î±,Î², Ix) characterize the smoothness as deï¬ned above. Then,\nthe total reward achieved by agent Ay satisï¬es the following guarantee\nJÎ¸test (Ï€âˆ—\nË†Î¸) â‰¥JÎ¸test (Ï€âˆ—\nÎ¸test ) âˆ’ÏµÂ·Î±Â·rmax\n1 âˆ’Î³ âˆ’Ix Â·âˆš2 Â·Î²Â·ÏµÂ·rmax\n(1 âˆ’Î³)2\nThe proof of the theorem is provided in the supplementary material and builds up on the theory of\napproximate equivalence of MDPs by [Even-Dar and Mansour, 2003]. In the next two sections, we\nprovide speciï¬c instantiations of TRAINING and INFERENCE procedures.\n4 T RAINING Procedures\nIn this section, we present two procedures to train adaptive policiesÏˆ(see TRAINING in Algorithm 1).\n4.1 T RAINING procedure ADAPT POOL\nThe basic idea of ADAPT POOL is to maintain a pool POOL of best response policies for Ay and, in\nthe test phase, switch between these policies based on inference of the type Î¸test.\n4.1.1 Architecture of the policyÏˆ\nThe adaptive pool based policy Ïˆ(ADAPT POOL ) consists of a pool (POOL ) of best response policies\ncorresponding to different possible agent Axâ€™s types Î¸, and a nearest-neighbor policy selection\nmechanism. In particular, when invokingADAPT POOL for state stand inferred agentAxâ€™s typeÎ¸t, the\npolicy Ïˆ(st,Î¸t) ï¬rst identiï¬es the most similar agent Ax in POOL , i.e., Ë†Î¸t = arg minÎ¸âˆˆÎ˜train âˆ¥Î¸âˆ’Î¸tâˆ¥,\nand then executes an action at âˆ¼Ï€âˆ—\nË†Î¸t\n(Â·| st) using the best response policy Ï€âˆ—\nË†Î¸t\n.\n5\nINFERENCE\nğ‘‚\"#$\nğœƒ\"\nğœƒ&\" = argmin.\tâˆˆ\t123456 \t ğœƒ âˆ’ ğœƒ\"\nğ‘\"~ğœ‹.;<\nâˆ— (? |ğ‘ \")\nğœ‹.;<\nâˆ—\nğ‘ \"\nğ‘\"\nNearest-neighbor selection from POOL\n(a) Test phase in Algorithm 1 with policy Ïˆ\ntrained using ADAPT POOL procedure.\nINFERENCE\nğ‘‚\"#$\nğœƒ\" ğ‘ \"\nğ‘\"\n(b) Test phase in Algorithm 1 with policy Ïˆ\ntrained using ADAPT DQN procedure.\nFigure 1: Two different instantiations of Algorithm 1 with the adaptive policy Ïˆ trained using\nprocedures ADAPT POOL and ADAPT DQN . (a) ADAPT POOL trains a set of best response policies\n{Ï€âˆ—\nÎ¸ |Î¸âˆˆÎ˜train}. In the test phase at time step twith Î¸t as the output of INFERENCE , the action at\nis sampled from a distribution Ï€âˆ—\nË†Î¸t\n(Â·| st) where Ë†Î¸t is the nearest match for Î¸t in the set Î˜train. (b)\nADAPT DQN trains one deep Q-Network (DQN) with an augmented state space given by (s,Î¸). At\ntime t, with Î¸t as the output of INFERENCE , the DQN network is given as input a tuple (st,Î¸t) and\nthe network outputs an action at.\n4.1.2 Training process\nDuring training we compute a pool of best response policies POOL for a set of possible agent Axâ€™s\ntypes Î˜train, see Algorithm 2.\nAlgorithm 2ADAPT POOL : Training process\n1: Input: Parameter space Î˜train\n2: POOL â†{}\n3: for each Î¸iter âˆˆÎ˜train do\n4: Ï€âˆ—\nÎ¸iter â†best response policy for MDP M(Î¸iter)\n5: POOL â†POOL âˆª{(Î¸iter,Ï€âˆ—\nÎ¸iter )}\n6: end for\n7: return POOL\n4.1.3 Guarantees\nIt turns out that if the set of possible agent Axâ€™s typesÎ˜train is chosen appropriately, Algorithm 1\ninstantiated with ADAPT POOL enjoys strong performance guarantees. In particular, choosing Î˜train\nas a sufï¬ciently ï¬ne Ïµâ€²-cover of the parameter space Î˜, ensures that for any Î¸test âˆˆÎ˜, that we might\nencounter at test time, we have considered a sufï¬ciently similar agent Ax during training and hence\ncan execute a best response policy which achieves good performance, see corollary below.\nCorollary 3. Let Î˜train be an Ïµâ€²-cover for Î˜, i.e., for all Î¸âˆˆÎ˜,âˆƒÎ¸â€²âˆˆÎ˜train s.t. ||Î¸âˆ’Î¸â€²||â‰¤ Ïµâ€². Let\nÎ¸test âˆˆÎ˜ be the type of agentAxand the INFERENCE procedure outputs Î¸tsuch that ||Î¸tâˆ’Î¸test||â‰¤ Ïµâ€²â€².\nLet Ïµ:= Ïµâ€²+ Ïµâ€²â€². Then, at time t, the policy Ï€âˆ—\nË†Î¸t\nused by agent Ay has the following guarantees:\nJÎ¸test (Ï€âˆ—\nË†Î¸t\n) â‰¥JÎ¸test (Ï€âˆ—\nÎ¸test ) âˆ’ÏµÂ·Î±Â·rmax\n1 âˆ’Î³ âˆ’Ix Â·âˆš2 Â·Î²Â·ÏµÂ·rmax\n(1 âˆ’Î³)2\nCorollary 3 follows from the result of Theorem 2 given that the pool POOL of policies trained by\nADAPT POOL is sufï¬ciently rich. Note that the accuracy Ïµâ€²â€²of INFERENCE would typically improve\nover time and hence the performance of the algorithm is expected to improve over time in practice, see\nSection 6.2. Building on the idea of ADAPT POOL , next we provide a more practical implementation\nof training procedure which does not require to maintain an explicit pool of best response policies\nand therefore is easier to scale to larger problems.\n4.2 T RAINING procedure ADAPT DQN\nADAPT DQN builds on the ideas of ADAPT POOL : Here, instead of explicitly maintaining a pool of\nbest response policies for agent Ay, we have a policy network trained on an augmented state space\n6\nSÃ—Î˜. This policy network resembles Deep Q-Network (DQN) architecture [Mnih et al., 2015], but\noperates on an augmented state space and takes as input a tuple (s,Î¸). Similar architecture was used\nby [Hessel et al., 2019], where one policy network was trained to play 57 Atari games, and the state\nspace was augmented with the index of the game. In the test phase, agent Ay selects actions given by\nthis policy network.\n4.2.1 Architecture of the policyÏˆ\nThe adaptive policy Ïˆ(ADAPT DQN ) consists of a neural network trained on an augmented state\nspace SÃ—Î˜. In particular, when invoking ADAPT DQN for state st and inferred agent Axâ€™s type\nÎ¸t, we use the augmented state space (st,Î¸t) as input to the neural network. The output layer of the\nnetwork computes the Q-values of all possible actions corresponding to the augmented input state.\nAgent Ay selects the action with the maximum Q-value.\n4.2.2 Training process\nHere, we provide a description of how we train the policy network using augmented state space,\nsee Algorithm 3. During one iteration of training the policy network, we ï¬rst sample a parameter\nÎ¸iter âˆ¼Î˜train. We then obtain the optimal best response policyÏ€âˆ—\nÎ¸iter of agent Ay for the MDP M(Î¸iter).\nWe compute the vector of all Q-values corresponding to this policy, i.e,Q(s,a) âˆ€sâˆˆS,a âˆˆA(repre-\nsented by QÏ€âˆ—\nÎ¸iter in Algorithm 3), using the standard Bellman equations [Sutton and Barto, 1998]. In\nour setting, we use these pre-computed Q-values to serve as the target values for the associated param-\neter Î¸iter for training the policy network. The loss function used for training is the standard squared\nerror loss between the target Q-values computed using the procedure described above and those given\nby the network under training. The gradient of this loss function is used for back-propagation through\nthe network. Multiple such iterations are carried out during training, until a convergence criteria is\nmet. For more details on Deep Q-Networks, we refer the reader to see [Mnih et al., 2015].\nAlgorithm 3ADAPT DQN: Training process\n1: Input: Parameter space Î˜train\n2: Ïˆâ†Init. policy network on augmented state space\n3: while convergence criteria is metdo\n4: sample Î¸iter âˆ¼Uniform(Î˜train)\n5: Ï€âˆ—\nÎ¸iter â†best response policy for MDP M(Î¸iter)\n6: QÏ€âˆ—\nÎ¸iter â†Q-values for policy Ï€âˆ—\nÎ¸iter in MDP M(Î¸iter)\n7: Train Ïˆfor one episode:\n(i) by augmenting the state space with Î¸iter\n(ii) by using target Q-values QÏ€âˆ—\nÎ¸iter\n8: end while\n9: return Ïˆ\n5 Inference Procedure\nIn the test phase, the inference of agent Axâ€™s type Î¸test from an observation history Otâˆ’1 is a\nkey component of our framework, and crucial for facilitating efï¬cient collaboration. Concretely,\nTheorem 2 implies that a best response policy Ï€âˆ—\nË†Î¸ also achieves good performance for agent Ax with\ntrue parameter Î¸test if ||Ë†Î¸âˆ’Î¸test||is small and MDP M(Î¸) is smooth w.r.t. parameter Î¸as described\nin Section 3.2.\nThere are several different approaches that one can consider for inference, depending on appli-\ncation setting. For instance, we can use probabilistic approaches as proposed in the work of\n[Everett and Roberts, 2018] where a pool of agent Axâ€™s policiesÏ€x\nÎ¸ âˆ€Î¸âˆˆÎ˜ is maintained and infer-\nence is done at run time via simple probabilistic methods. Based on the work by [Grover et al., 2018],\nwe can also maintain a more compact representation of agent Axâ€™s policies and then apply probabilis-\ntic methods on this representation.\nWe can also do inference based on ideas of inverse reinforcement learning (IRL) where observation\nhistory Otâˆ’1 serves the purpose of demonstrations [ Abbeel and Ng, 2004, Ziebart, 2010]. This is\n7\nğ’œğ‘¥\nğ’œğ‘¦\n+1\n+0.5\n(a) Î¸= [+1,+0.5]\nğ’œğ‘¥\nğ’œğ‘¦\n-1\n+0.8 (b) Î¸= [âˆ’1,+0.8]\nğ’œğ‘¥\nğ’œğ‘¦\n+1\n-1 (c) Î¸= [+1,âˆ’1]\nğ’œğ‘¥\nğ’œğ‘¦\n-0.5\n-0.5 (d) Î¸= [âˆ’0.5,âˆ’0.5]\nFigure 2: We evaluate the performance on a gathering game environment, a variant of the environ-\nments considered by [Leibo et al., 2017] and [Raileanu et al., 2018]. The objective is to maximize\nthe total reward by collecting fruits while avoiding collisions and agent Ax is assisted by agent Ay in\nachieving this objective. The environment has a 5x5 grid space resulting in 25 grid cells and thestate\nspace is determined by the joint location of agent Ax and agent Ay (i.e., |S|= 25 Ã—25). Actions\nare given by A ={â€˜step upâ€™, â€˜step leftâ€™, â€˜step downâ€™, â€˜step rightâ€™, â€˜stayâ€™}. Each action is executed\nsuccessfully with 0.8 probability; with random move probabilityof 0.2, the agent is randomly placed\nin one of the four neighboring cells located in vertical or horizontal positions. Two types of fruit\nobjects are placed in two ï¬xed grid cells (shown by â€˜shaded blueâ€™ and â€˜blueâ€™ cells). The rewards\nassociated with these two fruit types are given by the parameter vector Î¸âˆˆÎ˜ where Î˜ := [âˆ’1,+1]2.\nIn our environment, the location of these two fruit types is ï¬xed and fruits do not disappear (i.e., there\nis an unlimited supply of each fruit type in their respective locations). For any ï¬xed Î¸, agent Axâ€™s\npolicy Ï€x\nÎ¸ is computed ï¬rst by ignoring the presence of agent Ay. From agent Ayâ€™s point of view,\neach Î¸gives rise to a parametric MDPM(Î¸). Transition dynamicsTÎ¸ in M(Î¸) are obtained by\nmarginalizing out the effect of agent Axâ€™s policyÏ€x\nÎ¸. Reward functionRÎ¸ in M(Î¸) corresponds to\nthe reward associated with fruits which depends onÎ¸; in addition to collecting fruits, agent Ay should\navoid collision or close proximity to agent Ax. This is enforced by a collision cost of âˆ’5 when agent\nAy is in the same cell as agent Ax, and a proximity cost of âˆ’2 when agent Ay is in one of the four\nneighboring cells located in vertical or horizontal positions. The discount factor Î³is set to 0.99, and\nthe initial state distribution D0 corresponds to both agents starting in two corners. The above four\nillustrations correspond to four different Î¸parameters, highlighting agent Axâ€™s policyÏ€x\nÎ¸ and the best\nresponse policy Ï€âˆ—\nÎ¸ for agent Ay.\nparticularly suitable when the parameter Î¸exactly corresponds to the rewards used by agent Ax when\ncomputing its policy Ï€x\nÎ¸. In fact, this is the approach that we follow for our inference module, and\nin particular, we employ the popular IRL algorithm, namely Maximum Causal Entropy (MCE) IRL\nalgorithm [Ziebart, 2010]. We refer the reader to Section 6.1 for more details.\n6 Experiments\nWe evaluate the performance of our algorithms using a gathering game environment, see Figure 2.\nBelow, we provide details of the experimental setup and then discuss results.\n6.1 Experimental setup\n6.1.1 Environment details\nFor our experiments, we consider an episodic setting where two agents play the game repeatedly for\nmultiple episodes enumerated as e= 1,2,... . Each episode of the game lasts for 500 steps. Now, to\ntranslate the episode count to time steps tas used in Algorithm 1 (line 3), we have t= 500 Ã—eat the\nend of eth episode.\nFor any ï¬xed Î¸, agent Axâ€™s policyÏ€x\nÎ¸ is computed ï¬rst by ignoring the presence of agent Ay as\ndescribed belowâ€”this is in line with our motivating applications where agent Ax is the human-\nagent with a pre-speciï¬ed policy. In order to compute agent Axâ€™s policy Ï€x\nÎ¸, we consider agent\nAx operating in a single-agent MDP denoted as Mx(Î¸) = (Sx,A,R x\nÎ¸,Tx,Î³,D x\n0 ) where (i) sâˆˆSx\ncorresponds to the location of agent Ax in the grid-space, (ii) the action space is as described in\nFigure 2, (iii) the reward function Rx\nÎ¸ corresponds to reward associated with two fruit types given\nby Î¸, (iv) Tx corresponds to transition dynamics of agent Ax alone in the environment, (v) discount\n8\nfactor Î³ = 0.99, and (vi) Dx\n0 corresponds to agent Ax starting in the upper-left corner (see Figure 2).\nGiven Mx(Î¸), we compute Ï€x\nÎ¸ as a soft Bellman policy â€“ suitable to capture sub-optimal human\nbehaviour in applications [Ziebart, 2010].\nFrom agent Ayâ€™s point of view, eachÎ¸gives rise to a parametric MDP M(Î¸) in which agent Ay is\noperating in the game along with the corresponding agent Ax, see Figure 2.\n6.1.2 Baselines and implementation details.\nWe use three baselines to compare the performance of our algorithms: (i) RAND corresponds to\npicking a random Î¸âˆˆÎ˜ and using best response policy Ï€âˆ—\nÎ¸, (ii) FIXED MM corresponds to the ï¬xed\nbest response (in a minmax sense) policy in Eq. 3, and (iii) FIXED BEST is a variant of FIXED MM\nand corresponds to the ï¬xed best response (in a average sense) policy.\nWe implemented two variants of ADAPT POOL which store policies corresponding to Ïµâ€² = 1 and\nÏµâ€²= 0.25 covers of Î˜ (see Corollary 3), denoted as ADAPT POOL 1 and ADAPT POOL 0.25 in Figure 3.\nNext, we give speciï¬cations of the trained policy network used in ADAPT DQN. We used Î˜train to be\na 0.25 level discretization of Î˜. The trained network Ïˆhas 3 hidden layers with leaky RELU-units\n(with Î±= 0.1) having 64, 32, and 16 hidden units respectively, and a linear output layer with 5 units\n(corresponding to the size of action set |A|) (see [Mnih et al., 2015] for more details on training Deep\nQ-Network). The input to the neural network is a concatenation of the location of the 2 agents, and\nthe parameter vector Î¸t, where |Î¸t|= 2 (this corresponds to the augmented state space described in\nSection 4.2). The location of each agent is represented as a one-hot encoding of a vector of length 25\ncorresponding to the number of grid cells Hence the length of the input vector to the neural network\nis 25 Ã—2 + 2 (= 52). During training, agent Ay implemented epsilon-greedy exploratory policies\n(with exploration rate decaying linearly over training iterations from 1.0 to 0.01). Training lasted\nfor about 50 million iterations.\nOur inference module is based on the MCE-IRL approach [Ziebart, 2010] to infer Î¸test by observing\nactions taken by agent Axâ€™s policy. Note that, we are using MCE-IRL to infer the reward function\nparameters Î¸test used by agent Ax for computing its policy in the MDP Mx(Î¸test) (see â€œEnvironment\ndetails\" above). At the beginning, the inference module is initialized with Î¸0 = [0,0], and its output\nat time tgiven by Î¸t is based on history Otâˆ’1. In particular, we implemented a sequential variant of\nMCE-IRL algorithm which updates the estimate Î¸t only at the end of every episode eusing stochastic\ngradient descent with learning rate Î·= 0.001. We refer the reader to [Ziebart, 2010] for details on\nthe original MCE-IRL algorithm and to [Kamalaruban et al., 2019] for the sequential variant.\n6.2 Results: Worst-case and average performance\nWe evaluate the performance of algorithms on441 different Î¸test obtained by a 0.1 level discretization\nof the 2-D parametric space Î˜ := [ âˆ’1,+1]2. For a given Î¸test, the results were averaged over 10\nruns. Results are shown in Figure 3. As can be seen in Figure 3a, the worst-case performance of both\nADAPT DQN and ADAPT POOL is signiï¬cantly better than that of the three baselines (FIXED BEST ,\nRAND and FIXED MM), indicating robustness of our algorithmic framework. In our experiments, the\nFIXED MM and FIXED BEST baselines correspond to best response policies Ï€âˆ—\nÎ¸ for Î¸= [0.1,âˆ’1] and\nÎ¸= [0,âˆ’0.1] respectively. Under both these policies, agent Ayâ€™s behavior is qualitatively similar to\nthe one shown in Figure 2c. As can be seen, under these policies, agent Ay avoids both fruits and\navoids any collision; however, this does not allow agent Ay to assist agent Ax in collecting fruits\neven in scenarios where fruits have positive rewards.\nIn Figure 3c, we show the convergence behavior of the inference module. Here, WORST shows the\nworst case performance: As can be seen in the WORST line, there are cases where the performance\nof the inference procedure is bad, i.e., âˆ¥Î¸t âˆ’Î¸testâˆ¥is large. This usually happens when different\nparameter values of Î¸results in agent Ax having equivalent policies. In these cases, estimating the\nexact Î¸test without any additional information is difï¬cult. In our experiments, we noted that even\nif âˆ¥Î¸t âˆ’Î¸testâˆ¥is large, it is often the case that agent Axâ€™s policiesÏ€x\nÎ¸t and Ï€x\nÎ¸test are approximately\nequivalent which is important for getting a good approximation of the transition dynamics TÎ¸test .\nDespite the poor performance of the inference module in such cases, the performance of our algorithms\nis signiï¬cantly better than the baselines (as is evident in Figure 3a). In the supplementary material,\nwe provide additional experimental results corresponding to the algorithmsâ€™ performance for each\nindividual Î¸test to gain further insights.\n9\n0 200 400 600 800 1000\nEpisode e (time t = 500 Ã— e)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nJÎ¸test(Ï€âˆ—\nÎ¸test) âˆ’ JÎ¸test(Alg)\nRand\nFixedBest\nFixedMM\nAdaptPool1\nAdaptPool0.25\nAdaptDQN\n(a) Total reward: Worst-case\n0 200 400 600 800 1000\nEpisode e (time t = 500 Ã— e)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nJÎ¸test(Ï€âˆ—\nÎ¸test) âˆ’ JÎ¸test(Alg)\nRand\nFixedBest\nFixedMM\nAdaptPool1\nAdaptPool0.25\nAdaptDQN (b) Total reward: Average-case\n0 200 400 600 800 1000\nEpisode e (time t = 500 Ã— e)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n||Î¸t âˆ’ Î¸test||2\nWorst Avg (c) Inference module\nFigure 3: (a) Worst-case performance of both ADAPT DQN and ADAPT POOL is signiï¬cantly bet-\nter than that of the baselines, indicating robustness of our algorithmic framework. (a, b) Two\nvariants of ADAPT POOL are shown corresponding to 1-cover and 0.25-cover. As expected, the\nalgorithm ADAPT POOL 0.25 with larger pool size has better performance compared to the algorithm\nADAPT POOL 1. (c) Plot shows the convergence behavior of the inference module as more observa-\ntional data is gathered: AVG shows the average performance (averaged âˆ¥Î¸t âˆ’Î¸testâˆ¥w.r.t. different\nÎ¸test) and WORST shows the worst case performance (maximum âˆ¥Î¸t âˆ’Î¸testâˆ¥w.r.t. different Î¸test).\n-1 0 1\nÎ¸test[0]\n-1\n0\n1 Î¸test[1]\n0 15 30 45\n(a) Ï€âˆ—\nÎ¸test\n-1 0 1\nÎ¸test[0]\n-1\n0\n1 Î¸test[1]\n0 15 30 45 (b) FIXED BEST\n-1 0 1\nÎ¸test[0]\n-1\n0\n1 Î¸test[1]\n0 15 30 45 (c) ADAPT POOL 0.25\n-1 0 1\nÎ¸test[0]\n-1\n0\n1 Î¸test[1]\n0 15 30 45 (d) ADAPT DQN\n-1 0 1\nÎ¸test[0]\n-1\n0\n1 Î¸test[1]\n0.2 0.4 0.6 0.8 (e) Inference Module\nFigure 4: (a, b, c, d) Heat map of the total rewards obtained by different algorithms when measured\nin the episode e= 1000. (e) Heat map of the norm âˆ¥Î¸t âˆ’Î¸testâˆ¥, i.e., the gap between the estimated\nand true parameter Î¸test at the end of episode e= 1000. The performance of the inference procedure\nis poor in cases when different parameter values of Î¸test results in agent Ax having equivalent\npolicies. However, in these cases as well, the performance of our algorithms (ADAPT POOL 0.25 and\nADAPT DQN are shown in the ï¬gure) is signiï¬cantly better than the baselines ( FIXED BEST is shown\nin the ï¬gure).\n6.3 Results: Performance heatmaps for eachÎ¸test\nHere, we provide additional experimental results to gain further insights into the performance of our\nalgorithms. These results are presented in Figure 4 in the form of heat maps for each individual Î¸test:\nHeat maps either represent performance of algorithms (in terms of the total rewardJÎ¸test (ALG)) or the\nperformance of inference procedure (in terms of the norm âˆ¥Î¸t âˆ’Î¸testâˆ¥). These results are plotted in\nthe episode e= 1000 (cf., Figure 3 where the performance was plotted over time with increasing e).\nIt is important to note that there are cases where the performance of inference procedure is bad,\ni.e., âˆ¥Î¸t âˆ’Î¸testâˆ¥is large. This usually happens when different parameter values of Î¸test results in\nagent Ax having equivalent policies. In these cases, estimating the exact Î¸test without any additional\ninformation is difï¬cult. In our experiments, we noted that even if âˆ¥Î¸t âˆ’Î¸testâˆ¥is large, it is often the\ncase that agent Axâ€™s policiesÏ€x\nÎ¸t and Ï€x\nÎ¸test are approximately equivalent which is important for getting\na good approximation of the transition dynamics TÎ¸test . Despite the poor performance of the inference\nmodule in such cases, the performance of our algorithms (see ADAPT POOL 0.25 and ADAPT DQN in\nthe ï¬gure) is signiï¬cantly better than the baselines (see F IXED BEST in the ï¬gure).\n7 Related Work\nModeling and inferring about other agents.The inference problem has been considered in the\nliterature in various forms. For instance, [ Grover et al., 2018] consider the problem of learning\npolicy representations that can be used for interacting with unseen agents when using representation-\n10\nconditional policies. They also consider the case of inferring another agentâ€™s representation (pa-\nrameters) during test time. [ Macindoe et al., 2012] consider planners for collaborative domains\nthat can take actions to learn about the intent of another agent or hedge against its uncertainty.\n[Nikolaidis et al., 2015] cluster human users into types and aim to infer the type of new user online,\nwith the goal of executing the policy for that type. They test their approach in robot-human interaction\nbut do not provide any theoretical analysis of their approach. Beyond reinforcement learning, the\nproblem of modeling and inferring about other agents has been studied in other applications such as\npersonalization of web search ranking results by inferring userâ€™s preferences based on their online\nactivity [White et al., 2013, White et al., 2014, Singla et al., 2014].\nMulti-task and meta-learning.Our problem setting can be interpreted as a multi-task RL problem\nin which each possible agent Ax corresponds to a different task, or as a meta-learning RL problem in\nwhich the goal is to learn a policy that can quickly adapt to new partners. [Hessel et al., 2019] study\nthe problem of multi-task learning in the RL setting in which a single agent has to solve multiple tasks,\ne.g., solve all Atari games. However, they do not consider a separate test set to measure generalization\nof trained agents but rather train and evaluate on the same tasks. [SÃ¦mundsson et al., 2018] consider\nthe problem of meta learning for RL in the context of changing dynamics of the environment and\napproach it using a Gaussian processes and a hierarchical latent variable model approach.\nRobust RL.The idea of robust RL is to learn policies that are robust to certain types of errors\nor mismatches. In the context of our paper, mismatch occurs in the sense of encountering human\nagents that have not been encountered at training time and the learned policies should be robust\nin this situation. [ Pinto et al., 2017] consider training of policies in the context of a destabilizing\nadversary with the goal of coping with model mismatch and data scarcity. [Roy et al., 2017] study\nthe problem of RL under model mismatch such that the learning agent cannot interact with the actual\ntest environment but only a reasonably close approximation. The authors develop robust model-free\nlearning algorithms for this setting.\nMore complex interactions, teaching, and steering.In our paper, the type of interaction between\ntwo agents is limited as agent Ay does not affect agent Axâ€™s behaviour, allowing us to gain a\ndeeper theoretical understanding of this setting. There is also a related literature on â€œsteeringâ€ the\nbehavior of other agent. For example, (i) the environment designframework of [Zhang et al., 2009],\nwhere one agent tries to steer the behavior of another agent by modifying its reward function, (ii)\nthe cooperative inverse reinforcement learningof [Hadï¬eld-Menell et al., 2016], where the human\nuses demonstrations to reveal a proper reward function to the AI agent, and (iii) the advice-based\ninteraction model [Amir et al., 2016], where the goal is to communicate advice to a sub-optimal\nagent on how to act.\nDealing with non-stationary agents.The work of [ Everett and Roberts, 2018] is closely related\nto ours: they design a Switching Agent Model(SAM) that combines deep reinforcement learning with\nopponent modelling to robustly switch between multiple policies. [Zheng et al., 2018] also consider\na similar setting of detecting non-stationarity and reusing policies on the ï¬‚y, and introduce distilled\npolicy networkthat serves as the policy library. Our algorithmic framework is similar in spirit to\nthese two papers, however, in our setting, the focus is on acting optimally against an unknown\nagent whose behavior is stationary and we provide theoretical guarantees on the performance of\nour algorithms. [ Singla et al., 2018] have considered the problem of learning with experts advice\nwhere experts are not stationary and are learning agents themselves. However, their focus is\non designing a meta-algorithm on how to coordinate with these experts and is technically very\ndifferent from ours. A few other recent papers have also considered repeated human-AI interaction\nwhere the human agent is non-stationary and is evolving its behavior in response to AI agent (see\n[Radanovic et al., 2019, Nikolaidis et al., 2017b]. Prior work also considers a learner that is aware\nof the presence of other actors (see [Foerster et al., 2018, Raileanu et al., 2018]).\n8 Conclusions\nInspired by real-world applications like virtual personal assistants, we studied the problem of\ndesigning AI agents that can robustly cooperate with new people in human-machine partnerships.\nInspired by our motivating applications, we focused on an important practical aspect that there is often\na clear distinction between the training and test phase: the explicit reward information is only available\nduring training but adaptation is also needed during testing. We provided a framework for designing\nadaptive policies and gave theoretical insights into its robustness. In experiments, we demonstrated\nthat these policies can achieve good performance when interacting with previously unseen agents.\n11\nAcknowledgements\nThis work was supported by Microsoft Research through its PhD Scholarship Programme.\nReferences\n[Abbeel and Ng, 2004] Abbeel, P. and Ng, A. Y . (2004). Apprenticeship learning via inverse rein-\nforcement learning. In ICML.\n[Amershi et al., 2019] Amershi, S., Weld, D., V orvoreanu, M., Fourney, A., Nushi, B., Collisson, P.,\nSuh, J., Iqbal, S., Bennett, P. N., Inkpen, K., Teevan, J., Kikin-Gil, R., and Horvitz, E. (2019).\nGuidelines for human-AI interaction. In CHI, pages 3:1â€“3:13. ACM.\n[Amir et al., 2016] Amir, O., Kamar, E., Kolobov, A., and Grosz, B. (2016). Interactive teaching\nstrategies for agent training. In IJCAI.\n[Bobadilla et al., 2012] Bobadilla, J., Ortega, F., Hernando, A., and Bernal, J. (2012). A collaborative\nï¬ltering approach to mitigate the new user cold start problem. Knowledge-Based Systems, 26:225\nâ€“ 238.\n[Dimitrakakis et al., 2017] Dimitrakakis, C., Parkes, D. C., Radanovic, G., and Tylkin, P. (2017).\nMulti-view decision processes: The helper-AI problem. In Advances in Neural Information\nProcessing Systems.\n[Even-Dar and Mansour, 2003] Even-Dar, E. and Mansour, Y . (2003). Approximate equivalence of\nMarkov decision processes. In SchÃ¶lkopf, B. and Warmuth, M. K., editors, Learning Theory and\nKernel Machines, pages 581â€“594, Berlin, Heidelberg. Springer.\n[Everett and Roberts, 2018] Everett, R. and Roberts, S. J. (2018). Learning against non-stationary\nagents with opponent modelling and deep reinforcement learning. In AAAI Spring Symposia 2018.\n[Foerster et al., 2018] Foerster, J. N., Chen, R. Y ., Al-Shedivat, M., Whiteson, S., Abbeel, P., and\nMordatch, I. (2018). Learning with opponent-learning awareness. In AAMAS, pages 122â€“130.\n[Grover et al., 2018] Grover, A., Al-Shedivat, M., Gupta, J. K., Burda, Y ., and Edwards, H. (2018).\nLearning policy representations in multiagent systems. In ICML, pages 1797â€“1806.\n[Hadï¬eld-Menell et al., 2016] Hadï¬eld-Menell, D., Russell, S. J., Abbeel, P., and Dragan, A. D.\n(2016). Cooperative inverse reinforcement learning. InAdvances in Neural Information Processing\nSystems.\n[Haug et al., 2018] Haug, L., Tschiatschek, S., and Singla, A. (2018). Teaching inverse reinforcement\nlearners via features and demonstrations. In Advances in Neural Information Processing Systems,\npages 8464â€“8473.\n[Hessel et al., 2019] Hessel, M., Soyer, H., Espeholt, L., Czarnecki, W., Schmitt, S., and van Hasselt,\nH. (2019). Multi-task deep reinforcement learning with popart. In AAAI, pages 3796â€“3803.\n[Kamalaruban et al., 2019] Kamalaruban, P., Devidze, R., Cevher, V ., and Singla, A. (2019). Inter-\nactive teaching algorithms for inverse reinforcement learning. In IJCAI.\n[Leibo et al., 2017] Leibo, J. Z., Zambaldi, V . F., Lanctot, M., Marecki, J., and Graepel, T. (2017).\nMulti-agent reinforcement learning in sequential social dilemmas. In AAMAS, pages 464â€“473.\n[Macindoe et al., 2012] Macindoe, O., Kaelbling, L. P., and Lozano-PÃ©rez, T. (2012). Pomcop:\nBelief space planning for sidekicks in cooperative games. In AIIDE.\n[Mnih et al., 2015] Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G.,\nGraves, A., Riedmiller, M. A., Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C., Sadik,\nA., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015).\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529â€“533.\n[Nikolaidis et al., 2017a] Nikolaidis, S., Forlizzi, J., Hsu, D., Shah, J. A., and Srinivasa, S. S. (2017a).\nMathematical models of adaptation in human-robot collaboration. CoRR, abs/1707.02586.\n[Nikolaidis et al., 2017b] Nikolaidis, S., Nath, S., Procaccia, A. D., and Srinivasa, S. (2017b). Game-\ntheoretic modeling of human adaptation in human-robot collaboration. In Proceedings of the\nInternational conference on human-robot interaction, pages 323â€“331.\n12\n[Nikolaidis et al., 2015] Nikolaidis, S., Ramakrishnan, R., Gu, K., and Shah, J. A. (2015). Efï¬cient\nmodel learning from joint-action demonstrations for human-robot collaborative tasks. In HRI,\npages 189â€“196.\n[Pinto et al., 2017] Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A. (2017). Robust adversarial\nreinforcement learning. In ICML.\n[Radanovic et al., 2019] Radanovic, G., Devidze, R., Parkes, D., and Singla, A. (2019). Learning to\ncollaborate in Markov decision processes. In ICML.\n[Raileanu et al., 2018] Raileanu, R., Denton, E., Szlam, A., and Fergus, R. (2018). Modeling others\nusing oneself in multi-agent reinforcement learning. In ICML, pages 4254â€“4263.\n[Roy et al., 2017] Roy, A., Xu, H., and Pokutta, S. (2017). Reinforcement learning under model\nmismatch. In Advances in Neural Information Processing Systems, pages 3043â€“3052.\n[SÃ¦mundsson et al., 2018] SÃ¦mundsson, S., Hofmann, K., and Deisenroth, M. P. (2018). Meta\nreinforcement learning with latent variable Gaussian processes. In UAI.\n[Singla et al., 2018] Singla, A., Hassani, S. H., and Krause, A. (2018). Learning to interact with\nlearning agents. In AAAI, pages 4083â€“4090.\n[Singla et al., 2014] Singla, A., White, R. W., Hassan, A., and Horvitz, E. (2014). Enhancing\npersonalization via search activity attribution. In SIGIR, pages 1063â€“1066.\n[Sutton and Barto, 1998] Sutton, R. S. and Barto, A. G. (1998). Reinforcement learning - an intro-\nduction. Adaptive computation and machine learning. MIT Press.\n[Tschiatschek et al., 2019] Tschiatschek, S., Ghosh, A., Haug, L., Devidze, R., and Singla, A.\n(2019). Learner-aware teaching: Inverse reinforcement learning with preferences and constraints.\nIn Advances in Neural Information Processing Systems.\n[White et al., 2013] White, R. W., Chu, W., Hassan, A., He, X., Song, Y ., and Wang, H. (2013).\nEnhancing personalized search by mining and modeling task behavior. InWWW, pages 1411â€“1420.\n[White et al., 2014] White, R. W., Hassan, A., Singla, A., and Horvitz, E. (2014). From devices to\npeople: Attribution of search activity in multi-user settings. In WWW, pages 431â€“442.\n[Wilson and Daugherty, 2018] Wilson, H. J. and Daugherty, P. R. (2018). Collaborative intelligence:\nHumans and AI are joining forces. Harvard Business Review, 96(4):114â€“123.\n[Yu et al., 2017] Yu, H., Miao, C., Leung, C., and White, T. J. (2017). Towards AI-powered\npersonalization in MOOC learning. npj Science of Learning, 2(1):15.\n[Zhang et al., 2009] Zhang, H., Parkes, D. C., and Chen, Y . (2009). Policy teaching through reward\nfunction learning. In EC, pages 295â€“304.\n[Zheng et al., 2018] Zheng, Y ., Meng, Z., Hao, J., Zhang, Z., Yang, T., and Fan, C. (2018). A deep\nbayesian policy reuse approach against non-stationary agents. In Advances in Neural Information\nProcessing Systems, pages 962â€“972.\n[Ziebart, 2010] Ziebart, B. D. (2010). Modeling Purposeful Adaptive Behavior with the Principle of\nMaximum Causal Entropy. PhD thesis.\n13"
}