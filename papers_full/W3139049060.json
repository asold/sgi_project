{
  "title": "Conditional Positional Encodings for Vision Transformers",
  "url": "https://openalex.org/W3139049060",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2357614981",
      "name": "Chu, Xiangxiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2079118816",
      "name": "Tian, Zhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1910347209",
      "name": "Zhang Bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A780421308",
      "name": "Wang Xinlong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2360656043",
      "name": "Shen, Chunhua",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2186615578",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2995426144",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3035206215",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2981413347"
  ],
  "abstract": "We propose a conditional positional encoding (CPE) scheme for vision Transformers. Unlike previous fixed or learnable positional encodings, which are pre-defined and independent of input tokens, CPE is dynamically generated and conditioned on the local neighborhood of the input tokens. As a result, CPE can easily generalize to the input sequences that are longer than what the model has ever seen during training. Besides, CPE can keep the desired translation-invariance in the image classification task, resulting in improved performance. We implement CPE with a simple Position Encoding Generator (PEG) to get seamlessly incorporated into the current Transformer framework. Built on PEG, we present Conditional Position encoding Vision Transformer (CPVT). We demonstrate that CPVT has visually similar attention maps compared to those with learned positional encodings and delivers outperforming results. Our code is available at https://github.com/Meituan-AutoML/CPVT .",
  "full_text": "Published as a conference paper at ICLR 2023\nCONDITIONAL POSITIONAL ENCODINGS FOR VISION\nTRANSFORMERS\nXiangxiang Chu1, Zhi Tian 1, Bo Zhang 1, Xinlong Wang 2, Chunhua Shen 3∗\n1 Meituan Inc. 2 Beijing Academy of AI 3 Zhejiang University, China\n{chuxiangxiang, tianzhi02, zhangbo97}@meituan.com,\nxinlong.wang96@gmail.com, chunhua@me.com\nABSTRACT\nWe propose a conditional positional encoding (CPE) scheme for vision Trans-\nformers (Dosovitskiy et al., 2021; Touvron et al., 2020). Unlike previous ﬁxed\nor learnable positional encodings that are predeﬁned and independent of input to-\nkens, CPE is dynamically generated and conditioned on the local neighborhood of\nthe input tokens. As a result, CPE can easily generalize to the input sequences that\nare longer than what the model has ever seen during the training. Besides, CPE can\nkeep the desired translation equivalence in vision tasks, resulting in improved per-\nformance. We implement CPE with a simple Position Encoding Generator (PEG)\nto get seamlessly incorporated into the current Transformer framework. Built on\nPEG, we present Conditional Position encoding Vision Transformer (CPVT). We\ndemonstrate that CPVT has visually similar attention maps compared to those\nwith learned positional encodings and delivers outperforming results. Our Code\nis available at: https://git.io/CPVT.\n1 I NTRODUCTION\nRecently, Transformers (Vaswani et al., 2017) have been viewed as a strong alternative to Convolu-\ntional Neural Networks (CNNs) in visual recognition tasks such as classiﬁcation (Dosovitskiy et al.,\n2021) and detection (Carion et al., 2020; Zhu et al., 2021). Unlike the convolution operation in\nCNNs, which has a limited receptive ﬁeld, the self-attention mechanism in the Transformers can\ncapture the long-distance information and dynamically adapt the receptive ﬁeld according to the\nimage content. Consequently, Transformers are considered more ﬂexible and powerful than CNNs,\nbeing promising to achieve more progress in visual recognition.\nHowever, the self-attention operation in Transformers is permutation-invariant, which discards the\norder of the tokens in an input sequence. To mitigate this issue, previous works (Vaswani et al.,\n2017; Dosovitskiy et al., 2021) add the absolute positional encodings to each input token (see Fig-\nure 1a), which enables order-awareness. The positional encoding can either be learnable or ﬁxed\nwith sinusoidal functions of different frequencies. Despite being effective, these positional encod-\nings seriously harm the ﬂexibility of the Transformers, hampering their broader applications. Taking\nthe learnable version as an example, the encodings are often a vector of equal length to the input\nsequence, which are jointly updated with the network weights during training. As a result, the length\nand the value of the positional encodings are ﬁxed once trained. During testing, it causes difﬁculties\nof handling the sequences longer than the ones in the training data.\nThe inability to adapt to longer input sequences during testing greatly limits the range of general-\nization. For instance, in vision tasks like object detection, we expect the model can be applied to the\nimages of any size during inference, which might be much larger than the training images. A possi-\nble remedy is to use bicubic interpolation to upsample the positional encodings to the target length,\nbut it degrades the performance without ﬁne-tuning as later shown in our experiments. For vision\nin general, we expect that the models be translation-equivariant. For example, the output feature\nmaps of CNNs shift accordingly as the target objects are moved in the input images. However, the\nabsolute positional encoding scheme might break the translation equivalence because it adds unique\n∗Corresponding author.\n1\narXiv:2102.10882v3  [cs.CV]  13 Feb 2023\nPublished as a conference paper at ICLR 2023\nEmbeddingcls\nPE\nL × Encoders\nMLP\nEncoder\n(L-1) × Encoders\nEmbedding\ncls\nEncoder\nPEG\n(L-1) × Encoders\nEmbedding\nGAP\nPEG\nMLP\nMLP\n(a) ViT\nEmbeddingcls\nPE\nL × Encoders\nMLP\nEncoder\n(L-1) × Encoders\nEmbedding\ncls\nEncoder\nPEG\n(L-1) × Encoders\nEmbedding\nGAP\nPEG\nMLP\nMLP (b) CPVT\ne\nEmbeddingcls\nPE\nL × Encoders\nMLP\nEncoder\n(L-1) × Encoders\nEmbedding\ncls\nEncoder\nPEG\n(L-1) × Encoders\nEmbedding\nGAP\nPEG\nMLP\nMLP\n(c) CPVT-GAP\nFigure 1. Vision Transformers: (a) ViT (Dosovitskiy et al., 2021) with explicit 1D learnable posi-\ntional encodings (PE) (b) CPVT with conditional positional encoding from the proposed Position\nEncoding Generator (PEG) plugin, which is the default choice. (c) CPVT-GAP without class token\n(cls), but with global average pooling (GAP) over all items in the sequence. Note that GAP is a\nbonus version which has boosted performance.\npositional encodings to each token (or each image patch). One may overcome the issue with rela-\ntive positional encodings as in (Shaw et al., 2018). However, relative positional encodings not only\ncome with extra computational costs, but also require modifying the implementation of the standard\nTransformers. Last but not least, the relative positional encodings cannot work equally well as the\nabsolute ones, because the image recognition task still requires absolute position information (Islam\net al., 2020), which the relative positional encodings fail to provide.\nIn this work, we advocate a novel positional encoding (PE) scheme to incorporate the position\ninformation into Transformers. Unlike the predeﬁned and input-agnostic positional encodings used\nin previous works (Dosovitskiy et al., 2021; Vaswani et al., 2017; Shaw et al., 2018), the proposed\nPE is dynamically generated and conditioned on the local neighborhood of input tokens. Thus, our\npositional encodings can change along with the input size and try to keep translation equivalence. We\ndemonstrate that the vision transformers (Dosovitskiy et al., 2021; Touvron et al., 2020) with our new\nPE (i.e. CPVT, see Figure 1c) achieve even better performance. We summarize our contributions as,\n• We propose a novel positional encoding (PE) scheme, termed conditional position encod-\nings (CPE). CPE is dynamically generated with Positional Encoding Generators (PEG) and\ncan be effortlessly implemented by the modern deep learning frameworks (Paszke et al.,\n2019; Abadi et al., 2016; Chen et al., 2015), requiring no changes to the current Trans-\nformer APIs. Through an in-depth analysis and thorough experimentations, we unveil that\nthis design affords both absolute and relative encoding yet it goes above and beyond.\n• As opposed to widely-used absolute positional encodings, CPE can provide a kind of\nstronger explicit bias towards the translation equivalence which is important to improve\nthe performance of Transformers.\n• Built on CPE, we propose Conditional Position encoding Vision Transformer (CPVT). It\nachieves better performance than previous vison transformers (Dosovitskiy et al., 2021;\nTouvron et al., 2020).\n• CPE can well generalize to arbitrary input resolutions, which are required in many impor-\ntant downstream tasks such as segmentation and detection. Through experiments we show\nthat CPE can boost the segmentation and detection performance for pyramid transformers\nlike (Wang et al., 2021) by a clear margin.\n2\nPublished as a conference paper at ICLR 2023\n2 R ELATED WORK\nSince self-attention itself is permutation-equivariant (see A), positional encodings are commonly\nemployed to incorporate the order of sequences (Vaswani et al., 2017). The positional encodings\ncan either be ﬁxed or learnable, while either being absolute or relative. Vision transformers follow\nthe same fashion to imbue the network with positional information.\nAbsolute Positional Encoding. The absolute positional encoding is the most widely used. In\nthe original transformer (Vaswani et al., 2017), the encodings are generated with the sinusoidal\nfunctions of different frequencies and then they are added to the inputs. Alternatively, the positional\nencodings can be learnable, where they are implemented with a ﬁxed-dimension matrix/tensor and\njointly updated with the model’s parameters with SGD.\nRelative Positional Encoding. The relative position encoding (Shaw et al., 2018) considers dis-\ntances between the tokens in the input sequence. Compared to the absolute ones, the relative posi-\ntional encodings can be translation-equivariant and can naturally handle the sequences longer than\nthe longest sequences during training ( i.e., being inductive). A 2-D relative position encoding is\nproposed for image classiﬁcation in (Bello et al., 2019), showing superiority to 2D sinusoidal em-\nbeddings. The relative positional encoding is further improved in XLNet (Yang et al., 2019b) and\nDeBERTa (He et al., 2020), showing better performance.\nOther forms. Complex-value embeddings (Wang et al., 2019) are an extension to model global\nabsolute encodings and show improvement. RoFormer (Su et al., 2021) utilizes a rotary position em-\nbedding to encode both absolute and relative position information for text classiﬁcation. FLOATER\n(Liu et al., 2020) proposes a novel continuous dynamical model to capture position encodings. It is\nnot limited by the maximum sequence length during training, meanwhile being parameter-efﬁcient.\nSimilar designs to CPE. Convolutions are used to model local relations in ASR and machine\ntranslation (Gulati et al., 2020; Mohamed et al., 2019; Yang et al., 2019a; Yu et al., 2018). However,\nthey are mainly limited to 1D signals. We instead process 2D vision images.\n3 V ISION TRANSFORMER WITH CONDITIONAL POSITION ENCODINGS\n3.1 M OTIVATION\nIn vision transformers, an input image of size H ×W is split into patches with size S ×S, the\nnumber of patches is N = HW\nS2\n1. The patches are added with the same number of learnable absolute\npositional encoding vectors. In this work, we argue that the positional encodings used here have\ntwo issues. First, it prevents the model from handling the sequences longer than the learnable PE.\nSecond, it makes the model not translation-equivariant because a unique positional encoding vector\nis added to every one patch. The translation equivalence plays an important role in classiﬁcation\nbecause we hope the networks’ responses changes accordingly as the object moves in the image.\nOne may note that the ﬁrst issue can be remedied by removing the positional encodings since except\nfor the positional encodings, all other components (e.g., MHSA and FFN) of the vision transformer\ncan directly be applied to longer sequences. However, this solution severely deteriorates the perfor-\nmance. This is understandable because the order of the input sequence is an important clue and the\nmodel has no way to extract the order without the positional encodings. The experiment results on\nImageNet are shown in Table 1. By removing the positional encodings, DeiT-tiny’s performance on\nImageNet dramatically degrades from 72.2% to 68.2%.\nSecond, in DeiT (Touvron et al., 2020), they show that we can interpolate the position encodings\nto make them have the same length of the longer sequences. However, this method requires ﬁne-\ntuning the model a few more epochs, otherwise the performance will remarkably drop, as shown\nin Table 1. This goes contrary to what we would expect. With the higher-resolution inputs, we\noften expect a remarkable performance improvement without any ﬁne-tuning. Finally, the relative\nposition encodings (Shaw et al., 2018; Bello et al., 2019) can cope with both the aforementioned\n1H and W shall be divisible by S, respectively.\n3\nPublished as a conference paper at ICLR 2023\nTable 1. Comparison of various positional encoding (PE) strategies tested on ImageNet valida-\ntion set in terms of the top-1 accuracy. Removing the positional encodings greatly damages the\nperformance. The relative positional encodings have inferior performance to the absolute ones\nModel Encoding Top-1@224(%) Top-1@384(%)\nDeiT-tiny (Touvron et al., 2020) \u0017 68.2 68.6\nDeiT-tiny (Touvron et al., 2020) learnable 72.2 71.2\nDeiT-tiny (Touvron et al., 2020) sin-cos 72.3 70.8\nDeiT-tiny 2D RPE (Shaw et al., 2018) 70.5 69.8\nissues. However, the relative positional encoding cannot provide absolute position information ,\nwhich is also important to the classiﬁcation performance (Islam et al., 2020). As shown in Table 1,\nthe model with relative position encodings has inferior performance (70.5% vs. 72.2%).\n3.2 C ONDITIONAL POSITIONAL ENCODINGS\nWe argue that a successful positional encoding for vision tasks should meet these requirements,\n(1) Making the input sequence permutation-variant and providing stronger explicit bias to-\nwards translation-equivariance.\n(2) Being inductive and able to handle the sequences longer than the ones during training.\n(3) Having the ability to provide the absolute position to a certain degree. This is important to\nthe performance as shown in (Islam et al., 2020).\nIn this work, we ﬁnd that characterizing the local relationship by positional encodings is sufﬁcient\nto meet all of the above. First, it is permutation-variant because the permutation of input sequences\nalso affects the order in some local neighborhoods. However, translation of an object in an input im-\nage does not change the order in its local neighborhood,i.e., translation-equivariant (see Section A).\nSecond, the model can easily generalize to longer sequences since only the local neighborhoods of a\ntoken are involved. Besides, if the absolute position of any input token is known, the absolute posi-\ntion of all the other tokens can be inferred by the mutual relation between input tokens. We will show\nthat the tokens on the borders can be aware of their absolute positions due to the commonly-used\nzero paddings.\nH\nW\n...\nd\nN\n...\nclass token\nfeature tokens position encodings\nreshape F reshape\nPEG\nFigure 2. Schematic illustration of Positional Encoding Generator\n(PEG). Note d is the embedding size, N is the number of tokens.\nTherefore, we propose\npositional encoding gen-\nerators (PEG) to dynam-\nically produce the posi-\ntional encodings condi-\ntioned on the local neigh-\nborhood of an input to-\nken.\nPositional Encoding\nGenerator. PEG is\nillustrated in Figure 2.\nTo condition on the\nlocal neighbors, we ﬁrst\nreshape the ﬂattened input sequence X ∈RB×N×C of DeiT back to X′ ∈RB×H×W×C in the\n2-D image space. Then, a function (denoted by Fin Figure 2) is repeatedly applied to the local\npatch in X′to produce the conditional positional encodings EB×H×W×C. PEG can be efﬁciently\nimplemented with a 2-D convolution with kernel k (k ≥3) and k−1\n2 zero paddings. Note that the\nzero paddings here are important to make the model be aware of the absolute positions, and Fcan\nbe of various forms such as various types of convolutions and many others.\n3.3 C ONDITIONAL POSITIONAL ENCODING VISION TRANSFORMERS\nBuilt on the conditional positional encodings, we propose our Conditional Positional Encoding Vi-\nsion Transformers (CPVT). Except that our positional encodings are conditional, we exactly follow\n4\nPublished as a conference paper at ICLR 2023\nViT and DeiT to design our vision transformers and we also have three sizes CPVT-Ti, CPVT-S and\nCPVT-B. Similar to the original positional encodings in DeiT, the conditional positional encodings\nare also added to the input sequence, as shown in Figure 1 (b). In CPVT, the position where PEG is\napplied is also important to the performance, which will be studied in the experiments.\nIn addition, both DeiT and ViT utilize an extra learnable class token to perform classiﬁcation ( i.e.,\ncls token shown in Figure 1 (a) and (b)). By design, the class token is not translation-invariant,\nalthough it can learn to be so. A simple alternative is to directly replace it with a global average\npooling (GAP), which is inherently translation-invariant, resulting in our CVPT-GAP. Together with\nCPE, CVPT-GAP achieves much better image classiﬁcation performance.\n4 E XPERIMENTS\n4.1 S ETUP\nDatasets. Following DeiT (Touvron et al., 2020), we use ILSVRC-2012 ImageNet dataset (Deng\net al., 2009) with 1K classes and 1.3M images to train all our models. We report the results on the\nvalidation set with 50K images. Unlike ViT (Dosovitskiy et al., 2021), we do not use the much\nlarger undisclosed JFT-300M dataset (Sun et al., 2017).\nModel variants. We have three models with various sizes to adapt to various computing scenarios.\nThe detailed settings are shown in Table 9 (see B.1). All experiments in this paper are performed on\nTesla V100 machines. Training the tiny model for 300 epochs takes about 1.3 days on a single node\nwith 8 V100 GPU cards. CPVT-S and CPVT-B take about 1.6 and 2.5 days, respectively.\nTraining details All the models (except for CPVT-B) are trained for 300 epochs with a global batch\nsize of 2048 on Tesla V100 machines using AdamW optimizer (Loshchilov & Hutter, 2019). We\ndo not tune the hyper-parameters and strictly comply with the settings in DeiT (Touvron et al.,\n2020). The learning rate is scaled with this formula lrscale = 0.0005·BatchSizeglobal/512. The detailed\nhyperparameters are in the B.2.\n4.2 G ENERALIZATION TO HIGHER RESOLUTIONS\nAs mentioned before, our proposed PEG can directly generalize to larger image sizes without any\nﬁne-tuning. We conﬁrm this here by evaluating the models trained with 224 ×224 images on the\n384 ×384, 448 ×448, 512 ×512 images, respectively. The results are shown in Table 2. With the\n384 ×384 input images, the DeiT-tiny with learnable positional encodings degrades from 72.2%\nto 71.2%. When equipped with sine encoding, the tiny model degrades from 72.2% to 70.8%. In\nconstrat, our CPVT model with the proposed PEG can directly process the larger input images, and\nCPVT-Ti’s performance is boosted from 73.4% to 74.2% when applied to 384 ×384 images. Our\nCPVT-Ti outperforms DeiT-tiny by 3.0%. This gap continues to increase as the input resolution\nenlarges.\nTable 2. Direct evaluation on other resolutions without ﬁne-tuning. The models are trained on\n224×224. A simple PEG of a single layer of 3×3 depth-wise convolution is used here\nModel Params 160(%) 224(%) 384(%) 448(%) 512(%)\nDeiT-tiny 6M 65.6 72.2 71.2 68.8 65.9\nDeiT-tiny (sin) 6M 65.2 72.3 70.8 68.2 65.1\nDeiT-tiny (no pos) 6M 62.1 68.2 68.6 68.4 65.0\nCPVT-Ti 6M 66.8(+1.2) 72.4(+0.2) 73.2(+2.0) 71.8(+3.0) 70.3(+4.4)\nCPVT-Ti‡ 6M 67.7 (+2.1) 73.4(+1.2) 74.2(+3.0) 72.6(+3.8) 70.8(+4.9)\nDeiT-small 22M 75.6 79.9 78.1 75.9 72.6\nCPVT-S 22M 76.1(+0.5) 79.9 80.4(+1.5) 78.6(+2.7) 76.8(+4.2)\nDeiT-base 86M 79.1 81.8 79.7 79.8 78.2\nCPVT-B 86M 80.5(+1.4) 81.9(+0.1) 82.3(+2.6) 82.4(+2.6) 81.0(+2.8)\n‡: Insert one PEG each after the ﬁrst encoder till the ﬁfth encoder\n5\nPublished as a conference paper at ICLR 2023\n4.3 CPVT WITH GLOBAL AVERAGE POOLING\nBy design, the proposed PEG is translation-equivariant (ignore paddings). Thus, if we further use\nthe translation-invariant global average pooling (GAP) instead of the cls token before the ﬁnal\nclassiﬁcation layer of CPVT. CPVT can be translation-invariant, which should be beneﬁcial to the\nImageNet classiﬁcation task. Note the using GAP here results in even less computation complexity\nbecause we do not need to compute the attention interaction between the class token and the image\npatches. As shown in Table 3, using GAP here can boost CPVT by more than 1%. For example,\nequipping CPVT-Ti with GAP obtains 74.9% top-1 accuracy on the ImageNet validation dataset,\nwhich outperforms DeiT-tiny by a large margin (+2.7%). Moreover, it even exceeds DeiT-tiny model\nwith distillation (74.5%). In contrast, DeiT with GAP cannot gain so much improvement (only 0.4%\nas shown in Table 3) because the original learnable absolute PE is not translation-equivariant and\nthus GAP with the PE is not translation-invariant. Given the superior performance, we hope our\nmodel can be a strong PE alternative in vision transformers.\nTable 3. Performance comparison of Class Token (CLT) and global average pooling (GAP) on\nImageNet. CPVT’s can be further boosted with GAP\nModel Head Params Top-1 Acc Top-5 Acc\n(%) (%)\nDeiT-tiny (Touvron et al., 2020) CLT 6M 72.2 91.0\nDeiT-tiny GAP 6M 72.6 91.2\nCPVT-Ti‡ CLT 6M 73.4 91.8\nCPVT-Ti‡ GAP 6M 74.9 92.6\nDeiT-small (Touvron et al., 2020) CLT 22M 79.9 95.0\nDeiT-small GAP 22M 80.2 95.2\nCPVT-S ‡ CLT 23M 80.5 95.2\nCPVT-S ‡ GAP 23M 81.5 95.7\n‡: Insert one PEG each after the ﬁrst encoder till the ﬁfth encoder\n4.4 C OMPLEXITY OF PEG\nFew Parameters. Given the model dimensiond, the extra number of parameters introduced by PEG\nis d×l×k2 if we choosel depth-wise convolutions with kernelk. If we use l separable convolutions,\nthis value becomes l(d2 + k2d). When k = 3 and l = 1, CPVT-Ti (d = 192) brings about 1, 728\nparameters. Note that DeiT-tiny utilizes learnable position encodings with 192 ×14 ×14 = 37632\nparameters. Therefore, CPVT-Ti has 35, 904 fewer number of parameters than DeiT-tiny. Even\nusing 4 layers of separable convolutions, CPVT-Ti introduces only 38952 −37632 = 960 more\nparameters, which is negelectable compared to the 5.7M model parameters of DeiT-tiny.\nFLOPs. As for FLOPs, l layers of k ×k depth-wise convolutions possesses 14 ×14 ×d ×l ×k2\nFLOPS. Taking the tiny model for example, it involves 196 ×192 ×9 = 0.34M FLOPS for the\nsimple case k = 3and l = 1, which is neglectable because the model has 2.1G FLOPs in total.\n4.5 P ERFORMANCE COMPARISON\n20 40 60 80\nParams (M)\n72\n74\n76\n78\n80\n82Top-1 Accuracy (%)\nDeiT\nDeiT @384\nCPVT (Single PEG)\nCPVT (0-5)\nCPVT @384\nCPVT-GAP\nFigure 3. Comparison of CPVT and DeiT models\nunder various conﬁgurations. Note CPVT@384\nhas improved performance. More PEGs can result\nin better performance. CPVT-GAP is the best.\nWe evaluate the performance of CPVT models\non the ImageNet validation dataset and report\nthe results in Table 4. Compared with DeiT,\nCPVT models have much better top-1 accuracy\nwith similar throughputs. Our models can en-\njoy performance improvement when inputs are\nupscaled without ﬁne-tuning, while DeiT de-\ngrades as discussed in Table 2, see also Fig-\nure 3 for a clear comparison. Noticeably, Our\nmodel with GAP marked a new state-of-the-art\nfor vision Transformers.\n6\nPublished as a conference paper at ICLR 2023\nTable 4. Comparison with ConvNets and Transformers on ImageNet and ImageNet Real (Beyer\net al., 2020). CPVT have much better performance compared with prior Transformers\nModels Params(M) Input throughput⋆ ImNet Real\ntop-1 % top-1 %\nResNet-50 (He et al., 2016) 25 2242 1226.1 76.2 82.5\nResNet-101 (He et al., 2016) 45 2242 753.6 77.4 83.7\nResNet-152 (He et al., 2016) 60 2242 526.4 78.3 84.1\nRegNetY-4GF (Radosavovic et al., 2020) 21 2242 1156.7 80.0 86.4\nEfﬁcientNet-B0 (Tan & Le, 2019) 5 2242 2694.3 77.1 83.5\nEfﬁcientNet-B1 (Tan & Le, 2019) 8 2402 1662.5 79.1 84.9\nEfﬁcientNet-B2 (Tan & Le, 2019) 9 2602 1255.7 80.1 85.9\nEfﬁcientNet-B3 (Tan & Le, 2019) 12 3002 732.1 81.6 86.8\nEfﬁcientNet-B4 (Tan & Le, 2019) 19 3802 349.4 82.9 88.0\nViT-B/16 (Dosovitskiy et al., 2021) 86 3842 85.9 77.9 -\nViT-L/16 307 3842 27.3 76.5 -\nDeiT-tiny w/o PE (Touvron et al., 2020) 6 2242 2536.5 68.2 -\nDeiT-tiny (Touvron et al., 2020) 6 2242 2536.5 72.2 80.1\nDeiT-tiny (sine) 6 2242 2536.5 72.3 80.3\nCPVT-Ti‡ 6 2242 2500.7 73.4 81.3\nCPVT-Ti-GAP‡ 6 2242 2520.1 74.9 82.5\nDeiT-tiny (Touvron et al., 2020)\n⚗ 6 2242 2536.5 74.5 82.1\nCPVT-Ti\n⚗ 6 2242 2500.7 75.9 83.0\nDeiT-small (Touvron et al., 2020) 22 2242 940.4 79.9 85.7\nCPVT-S ‡ 23 2242 930.5 80.5 86.0\nCPVT-S-GAP‡ 23 2242 942.3 81.5 86.6\nDeiT-base (Touvron et al., 2020) 86 2242 292.3 81.8 86.7\nCPVT-B ‡ 88 2242 285.5 82.3 87.0\nCPVT-B-GAP‡ 88 2242 290.2 82.7 87.7\n⋆: Measured in img/s on a 16GB V100 GPU as in (Touvron et al., 2020).\n‡: Insert one PEG each after the ﬁrst encoder till the ﬁfth encoder\n⚗\n: trained with hard distillation using RegNetY-160 as the teacher.\nWe further train CPVT-Ti and DeiT-tiny using the aforementioned training settings plus the hard\ndistillation proposed in (Touvron et al., 2020). Speciﬁcally, we use RegNetY-160 (Radosavovic\net al., 2020) as the teacher. CPVT obtains 75.9%, exceeding DeiT-tiny by 1.4%.\n4.6 PEG ON PYRAMID TRANSFORMER ARCHITECTURES\nPVT (Wang et al., 2021) is a vision transformer with the multi-stage design like ResNet (He et al.,\n2016). Swin (Liu et al., 2021) is a follow-up work and comes with higher performance. We apply\nour method on both to demonstrate its generalization ability.\nImageNet classiﬁcation. Speciﬁcally, we remove its learnable PE and apply our PEG in position\n0 of each stage with a GAP head. We use the same training settings to make a fair comparison and\nshow the results in Table 13. Our method can signiﬁcantly boost PVT-tiny by 3.1% and Swin-tiny\nby 1.15% on ImageNet (c.f. B.5). We also evaluate the performance of PEG on some downstream\nsemantic segmentation and object detection tasks (see B.6). Note these tasks usually handle the\nvarious input resolutions as the training because multi-scale data augmentation is extensively used.\n5 A BLATION STUDY\n5.1 P OSITIONAL ENCODING OR MERELY A HYBRID ?\nOne might suspect that the PEG’s improvement comes from the extra learnable parameters intro-\nduced by the convolutional layers in PEG, instead of the local relationship retained by PEG. One\n7\nPublished as a conference paper at ICLR 2023\nway to test the function of PEG is only adding it when calculating Q and K in the attention layer,\nso that only the positional information of PEG is passed through. We can achieve 71.3% top-1 ac-\ncuracy on ImageNet with DeiT-tiny. This is signiﬁcantly better than DeiT-tiny w/o PE (68.2%) and\nis similar to the one with PEG on Q, K and V (72.4%), which suggests that PEG mainly serves as a\npositional encoding scheme.\nTable 5. Positional encoding rather than added pa-\nrameters gives the most improvement\nKernel Style Params Top-1 Acc\n(M) (%)\nnone - 5.68 68.2\n3 ﬁxed (random init) 5.68 71.3\n3 ﬁxed (learned init) 5.68 72.3\n1 (12 ×) learnable 6.13 68.6\n3 learnable 5.68 72.4\nWe also design another experiment to remove\nthis concern. By randomly-initializing a 3 ×3\nPEG and ﬁxing its weights during the train-\ning, we can obtain 71.3% accuracy (Table 5),\nwhich is much higher (3.1%↑) than DeiT with-\nout any PE (68.2%). Since the weights of PEG\nare ﬁxed and the performance improvement can\nonly be due to the introduced position informa-\ntion. On the contrary, when we exhaustively\nuse 12 convolutional layers (kernel size being\n1, i.e., not producing local relationship) to re-\nplace the PEG, these layers have much more\nlearnable parameters than PEG. However, it only boosts the performance by 0.4% to 68.6%.\nAnother interesting ﬁnding is that ﬁxing a learned PEG also helps training. When we initialize with\na learned PEG instead of the random values and train the tiny version of the model from scratch\nwhile keeping the PEG ﬁxed, the model can also achieve 72.3% top-1 accuracy on ImageNet. This\nis very close to the learnable PEG (72.4%).\n5.2 PEG P OSITION IN CPVT\nWe also experiment by varying the position of the PEG in the model. Table 6 (left) presents the\nablations for variable positions (denoted as PosIdx) based on the tiny model. We consider the input\nof the ﬁrst encoder by index -1. Therefore, position 0 is the output of the ﬁrst encoder block. PEG\nshows strong performance (∼72.4%) when it is placed at [0, 3].\nNote that positioning the PEG at 0 can have much better performance than positioning it at -1 ( i.e.,\nbefore the ﬁrst encoder), as shown in Table 6 (left). We observe that the difference between the two\nsituations is they have different receptive ﬁelds. Speciﬁcally, the former has a global ﬁeld while the\nlatter can only see a local area. Hence, they are supposed to work similarly well if we enlarge the\nconvolution’s kernel size. To verify our hypothesis, we use a quite large kernel size 27 with a padding\nsize 13 at position -1, whose result is reported in Table 6 (right). It achieves similar performance to\nthe one positioning the PEG at 0 (72.5%), which veriﬁes our assumption.\nTable 6. Comparison of different plugin positions (left) and kernels (right) using DeiT-tiny\nPosIdx Top-1 (%) Top-5 (%)\nnone 68.2 88.7\n−1 70.6 90.2\n0 72.4 91.2\n3 72.3 91.1\n6 71.7 90.8\n10 69.0 89.1\nPosIdx kernel Params Top-1 (%) Top-5 (%)\n-1 3×3 5.7M 70.6 90.2\n-1 27×27 5.8M 72.5 91.3\n5.3 C OMPARISONS WITH OTHER POSITIONAL ENCODINGS\nWe compare PEG with other commonly used encodings: absolute positional encoding (e.g. sinu-\nsoidal (Vaswani et al., 2017)), relative positional encoding (RPE) (Shaw et al., 2018) and learnable\nencoding (LE) (Devlin et al., 2019; Radford et al., 2018), as shown in Table 7.\nDeiT-tiny obtains 72.2% with the learnable absolute PE. We experiment with the 2-D sinusoidal\nencodings and it achieves on-par performance. For RPE, we follow (Shaw et al., 2018) and set the\n8\nPublished as a conference paper at ICLR 2023\nTable 7. Comparison of various positional encoding strategies. LE: learnable positional encoding.\nRPE: relative positional encoding\nModel PEG Pos Encoding Top-1 Top-5\n(%) (%)\nDeiT-tiny (2020) - LE 72.2 91.0\nDeiT-tiny - 2D sin-cos 72.3 91.0\nDeiT-tiny - 2D RPE 70.5 90.0\nCPVT-Ti 0-1 PEG 72.4 91.2\nCPVT-Ti 0-1 PEG + LE 72.9 91.4\nCPVT-Ti 0-1 4×PEG + LE 72.9 91.4\nCPVT-Ti 0-5 PEG 73.4 91.8\nlocal range hyper-parameter K as 8, with which we obtain 70.5%. RPE here does not encode any\nabsolute position information, see discussion in D.1 and B.3.\nMoreover, we combine the learnable absolute PE with a single-layer PEG. This boosts the baseline\nCPVT-Ti (0-1) by 0.5%. If we use 4-layer PEG, it can achieve 72.9%. If we add a PEG to each of\nthe ﬁrst ﬁve blocks, we can obtain 73.4%, which is better than stacking them within one block.\nCPE is not a simple combination of APE and RPE. We further compare our method with a\nbaseline with combination of APE and RPE. Speciﬁcally, we use learnable positional encoding\n(LE) as DeiT at the beginning of the model and supply 2D RPE for every transformer block. This\nsetting achieves 72.4% top-1 accuracy on ImageNet, which is comparable to a single PEG (72.4%).\nNevertheless, this experiment does not necessarily indicate that our CPE is a simple combination\nof APE and RPE. When tested on different resolutions, this baseline cannot scale well compared to\nours (Table 8). RPE is not able to adequately mitigate the performance degradation on top of LE.\nThis shall be seen as a major difference.\nTable 8. Direct evaluation on other resolutions without ﬁne-tuning. The models are trained on\n224×224. CPE outperforms LE+RPE combination on untrained resolutions.\nModel Positional Params 160(%) 224(%) 384(%) 448(%) 512(%)\nDeiT-tiny (LE+RPE) 40011 65.6 72.4 70.8 68.4 65.6\nDeiT-tiny (PEG at Pos 0) 1920 66.8 72.4 73.2 71.8 70.3\nPEG can continuously improve the performance if stacked more. We use LE not only at the\nbeginning but also in the next 5 layers to have a similar thing as 0-5 PEG conﬁguration.This setting\nachieves 72.7% top-1 accuracy on ImageNet, which is 0.7% lower than PEG (0-5). This setting\nsuggests that it is also beneﬁcial to have more of LEs, but not as good as ours. It is expected since\nwe exploit relative information via PEGs at the same time.\n6 C ONCLUSION\nWe introduced CPVT, a novel method to provide the position information in vision transformers,\nwhich dynamically generates the position encodings based on the local neighbors of each input\ntoken. Through extensive experimental studies, we demonstrate that our proposed positional en-\ncodings can achieve stronger performance than the previous positional encodings. The transformer\nmodels with our positional encodings can naturally process longer input sequences and keep the\ndesired translation equivalence in vision tasks. Moreover, our positional encodings are easy to im-\nplement and come with negligible cost. We look forward to a broader application of our method in\ntransformer-driven vision tasks like segmentation and video processing.\nREFERENCES\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu\nDevin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-\n9\nPublished as a conference paper at ICLR 2023\nscale machine learning. In 12th {USENIX}symposium on operating systems design and imple-\nmentation ({OSDI}16), pp. 265–283, 2016.\nIrwan Bello. Lambdanetworks: Modeling long-range interactions without attention. InInternational\nConference on Learning Representations, 2021. URL https://openreview.net/forum?\nid=xTJEN-ggl1b.\nIrwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented\nconvolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 3286–3295, 2019.\nLucas Beyer, Olivier J H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨aron van den Oord. Are\nwe done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European Conference\non Computer Vision, pp. 213–229. Springer, 2020.\nHanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chun-\njing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv preprint\narXiv:2012.00364, 2020.\nTianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,\nChiyuan Zhang, and Zheng Zhang. Mxnet: A ﬂexible and efﬁcient machine learning library for\nheterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.\nXiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In\nNeurIPS 2021, 2021.\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In International Conference on Learning Representations ,\n2020.\nEkin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data\naugmentation with a reduced search space. Advances in Neural Information Processing Systems,\n33, 2020.\nZihang Dai, Hanxiao Liu, Quoc Le, and Mingxing Tan. Coatnet: Marrying convolution and attention\nfor all data sizes. Advances in Neural Information Processing Systems, 34, 2021.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni-\ntion at scale. In International Conference on Learning Representations , 2021. URL https:\n//openreview.net/forum?id=YicbFdNTTy.\nSt´ephane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent\nSagun. Convit: Improving vision transformers with soft convolutional inductive biases. In Inter-\nnational Conference on Machine Learning, pp. 2286–2296. PMLR, 2021.\n10\nPublished as a conference paper at ICLR 2023\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer\nfor speech recognition. arXiv preprint arXiv:2005.08100, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n770–778, 2016.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention. arXiv preprint arXiv:2006.03654, 2020.\nElad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Aug-\nment your batch: Improving generalization through instance repetition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8129–8138, 2020.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis\nHawthorne, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu, and Douglas Eck. Music\ntransformer. In Advances in Neural Processing Systems, 2018.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\nstochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016.\nMd Amirul Islam, Sen Jia, and Neil DB Bruce. How much position information do convolutional\nneural networks encode? In International Conference on Learning Representations, 2020.\nAlexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Panoptic feature pyramid net-\nworks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 6399–6408, 2019.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Focal loss for dense\nobject detection. In Proceedings of the IEEE international conference on computer vision , pp.\n2980–2988, 2017.\nXuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. Learning to encode position for\ntransformer with continuous dynamical model. In International Conference on Machine Learn-\ning, pp. 6327–6335. PMLR, 2020.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations , 2019. URL https://openreview.net/forum?id=\nBkg6RiCqY7.\nAbdelrahman Mohamed, Dmytro Okhonko, and Luke Zettlemoyer. Transformers with convolutional\ncontext for asr. arXiv preprint arXiv:1904.11660, 2019.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In Jennifer Dy and Andreas Krause (eds.), Proceedings of\nthe 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine\nLearning Research, pp. 4055–4064, Stockholmsm ¨assan, Stockholm Sweden, 10–15 Jul 2018.\nPMLR.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in Neural Information Processing Systems, 32:\n8026–8037, 2019.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n11\nPublished as a conference paper at ICLR 2023\nIlija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Designing\nnetwork design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 10428–10436, 2020.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya,\nand Jon Shlens. Stand-alone self-attention in vision models. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett (eds.), Ad-\nvances in Neural Information Processing Systems , volume 32, pp. 68–80. Curran Asso-\nciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/\n3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-\ntions. In Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 2, pp. 464–468, 2018.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overﬁtting. The journal of machine\nlearning research, 15(1):1929–1958, 2014.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\nfectiveness of data in deep learning era. In Proceedings of the IEEE international conference on\ncomputer vision, pp. 843–852, 2017.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-\ning the inception architecture for computer vision. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 2818–2826, 2016.\nMingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural net-\nworks. In International Conference on Machine Learning, pp. 6105–6114. PMLR, 2019.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv´e J´egou. Training data-efﬁcient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st In-\nternational Conference on Neural Information Processing Systems, pp. 6000–6010, 2017.\nBenyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen.\nEncoding word order in complex embeddings. arXiv preprint arXiv:1912.12333, 2019.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122, 2021.\nYuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and\nHuaxia Xia. End-to-end video instance segmentation with transformers. arXiv preprint\narXiv:2011.14503, 2020.\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\nBaosong Yang, Longyue Wang, Derek Wong, Lidia S Chao, and Zhaopeng Tu. Convolutional self-\nattention networks. arXiv preprint arXiv:1904.03107, 2019a.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019b.\n12\nPublished as a conference paper at ICLR 2023\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and\nQuoc V Le. Qanet: Combining local convolution with global self-attention for reading compre-\nhension. arXiv preprint arXiv:1804.09541, 2018.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceed-\nings of the IEEE/CVF International Conference on Computer Vision, pp. 6023–6032, 2019.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond em-\npirical risk minimization. In International Conference on Learning Representations, 2018. URL\nhttps://openreview.net/forum?id=r1Ddp1-Rb.\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from\na sequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data aug-\nmentation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 34, pp.\n13001–13008, 2020.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene\nparsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 633–641, 2017.\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: De-\nformable transformers for end-to-end object detection. In International Conference on Learning\nRepresentations, 2021. URL https://openreview.net/forum?id=gZ9hCDWe6ke.\n13\nPublished as a conference paper at ICLR 2023\nA T RANSLATION EQUIVARIANCE\nThe term translation-equivariance means the output feature maps can be equally translated with\nthe input signal. Imagine there is a person in the left-top of an image, if the person is moved to\nthe right-bottom, the output feature maps will change accordingly. This property is very important\nto the success of convolution network. Convolution (ignoring paddings), RPE, and self-attention\nare all translation-equivariant operations (regardless of their receptive ﬁeld). It’s nontrivial to make\nabsolute positional encodings like DeiT (using learnable positional encoding) translation-equivariant\nsince different absolute positions will be added if the input signal is translated. Note that our method\nis not strictly translation-equivariant because of the zero padding. Instead, it provides a kind of\nstronger explicit bias towards the translation-equivariant property.\nB E XPERIMENT DETAILS\nB.1 A RCHITECTURE VARIANTS OF CPVT\nTable 9. CPVT architecture variants. The larger model, CPVT-B, has the same architecture as ViT-\nB (Dosovitskiy et al., 2021) and DeiT-B (Touvron et al., 2020). CPVT-S and CPVT-Ti have the\nsame architecture as DeiT-small and DeiT-tiny respectively\nModel #channels #heads #layers #params\nCPVT-Ti 192 3 12 6M\nCPVT-S 384 6 12 22M\nCPVT-B 768 12 12 86M\nB.2 T HE HYPERPARAMETERS OF CPVT\nAs for the ImageNet classiﬁcation task, we use exactly the same hyperparameters as DeiT except for\nthe base model because it is not always stably trained using AdamW. The detailed setting is shown\nin Table 10.\nTable 10. Hyper-parameters for ViT, DeiT and CPVT\nMethods ViT DeiT CPVT\nEpochs 300 300 300\nBatch size 4096 1024 1024\nOptimizer AdamW AdamW LAMB\nLearning rate decay cosine cosine cosine\nWeight decay 0.3 0.05 0.05\nWarmup epochs 3.4 5 5\nLabel smoothingε (Szegedy et al., 2016) \u0017 0.1 0.1\nDropout (Srivastava et al., 2014) 0.1 \u0017 \u0017\nStoch. Depth (Huang et al., 2016) \u0017 0.1 0.1\nRepeated Aug (Hoffer et al., 2020) \u0017 \u0013 \u0013\nGradient Clip. \u0013 \u0017 \u0017\nRand Augment (Cubuk et al., 2020) \u0017 9/0.5 9/0.5\nMixup prob. (Zhang et al., 2018) \u0017 0.8 0.8\nCutmix prob. (Yun et al., 2019) \u0017 1.0 1.0\nErasing prob. (Zhong et al., 2020) \u0017 0.25 0.25\nB.3 I MPORTANCE OF ZERO PADDINGS\nWe design an experiment to verify the importance of the zero paddings, which can help the model\ninfer the absolute positional information. Speciﬁcally, we use CPVT-S and simply remove the zero\npaddings from CPVT while keeping all other settings unchanged. Table 11 shows that this can\nonly obtain 70.5%, which indicates that the zero paddings and absolute positional information play\nimportant roles in classifying objects.\n14\nPublished as a conference paper at ICLR 2023\nTable 11. Ablation study on ImageNet performance w/ or w/o zero paddings\nModel Padding Top-1 Acc(%) Top-5 Acc(%)\nCPVT-Ti ✓ 72.4 91.2\n\u0017 70.5 89.8\nB.4 S INGLE PEG VS. M ULTIPLE PEG S\nWe further evaluate whether or not using multi-position encodings can beneﬁt the performance in\nTable 12. Notice we denote by i-j the inserted positions of PEG which start from the i-th encoder\nand end at the j −1-th one (inclusion). By inserting PEGs to ﬁve positions, the top-1 accuracy of the\ntiny model can achieve 73.4%, which surpasses DeiT-tiny by 1.2%. Similarly, CPVT-S can achieve\n80.5%. It turns out more PEGs do help, but up to a level where more PEGs become incremental (0-5\nvs. 0-11).\nTable 12. CPVT’s sensitivity to number of plugin positions\nPositions Model Params Top-1 Acc Top-5 Acc\n(M) (%) (%)\n0-1 tiny 5.7 72.4 91.2\n0-5 tiny 5.9 73.4 91.8\n0-11 tiny 6.1 73.4 91.8\n0-1 small 22.0 79.9 95.0\n0-5 small 22.9 80.5 95.2\n0-11 small 23.8 80.6 95.2\nB.5 C LASSFICATION EVALUATION OF SWIN WITH PEG\nWe show the validation curves when training Swin (Liu et al., 2021) equipped with PEG in Figure 4.\nIt can boost Swin-tiny from 81.10% to 82.25% (+1.15%↑) on ImageNet.\n0 50 100 150 200 250 300\nEpochs\n70\n75\n80\n85Top-1 Val Accuracy (%)\nSwin Tiny\nSwin Tiny + CPE\nFigure 4. CPE boosts Swin Tiny on ImageNet by 1.15% top-1 Acc.\nB.6 E VALUATION ON SEGMENTATION AND DETECTION\nSemantic segmentation on ADE20K. We evaluate the performance of PEG on the ADE20K\n(Zhou et al., 2017) segmentation task. Based on the Semantic FPN framework (Kirillov et al.,\n2019), PVT achieves much better results than ResNet (He et al., 2016) baselines. Under carefully\ncontrolled settings, PEG further boosts PVT-tiny by 3.1% mIoU.\nObject detection on COCO. We also perform controlled experiments with the RetinaNet (Lin\net al., 2017) framework on the COCO detection task. The results are shown in Table 13. In the\nstandard 1×schedule, PEG improves PVT-tiny by 2.0% mAP. PEG brings 2.4% higher mAP under\nthe 3 ×schedule.\n15\nPublished as a conference paper at ICLR 2023\nTable 13. Our method boosts the performance of PVT on ImageNet classiﬁcation, ADE20K seg-\nmentation and COCO detection\nBackbone ImageNet Semantic FPN on ADE20K RetinaNet on COCO\nParams Top-1 Params mIoU Params mAP mAP\n(M) (%) (M) (%) (M) (%, 1×) (%, 3×, +MS)\nResNet-18 (He et al., 2016) 12 69.8 16 32.9 21 31.8 35.4\nPVT-tiny (Wang et al., 2021) 13 75.0 17 35.7 23 36.7 39.4\nPVT-tiny+PEG 13 77.3 17 38.0 23 38.0 41.8\nPVT-tiny+GAP 13 75.9 17 36.0 23 36.9 39.7\nPVT-tiny+PEG+GAP 13 78.1 17 38.8 23 38.7 41.8\nPVT-small (Wang et al., 2021) 25 79.8 28 39.8 34 40.4 42.2\nPVT-small+PEG+GAP 25 81.2 28 44.3 34 43.0 45.2\nPVT-Medium (Wang et al., 2021) 44 81.2 48 41.6 54 41.9 43.2\nPVT-Medium+PEG+GAP 44 82.7 48 44.9 54 44.3 46.4\nB.7 A BLATION ON OTHER FORMS OF PEG\nWe explore several forms of PEG based on the tiny model, which change the type of convolution,\nkernel size and layers. The inserted position is 0. The result is shown in Table 14. When we use large\nkernel of 7×7 or dense convolution, the performance improvement is limited. Stacking more layers\nof depth-wise convolution doesn’t bring signiﬁcant improvement. Therefore, we use the simplest\nform as our default implementation. It indicates that this design is enough to provide good position\ninformation.\nTable 14. Other forms of PEG. The simple form of a single depth-wise 3×3 is good enough.\nVariants Model Top-1 Acc (%)\n1 Depthwise Conv 3×3 tiny 72.4\n1 Depthwise Conv 7×7 tiny 72.5\n4 * (Depthwise Conv 3×3 +BN+ReLU) tiny 72.4\n1 Dense Conv 3×3 tiny 72.3\n4 * (Dense Conv 3×3+BN+ReLU) tiny 72.5\nC E XAMPLE CODE\nC.1 PEG\nIn the simplest form, we use a single depth-wise convolution and show its usage in Transformer by\nthe following PyTorch snippet. Through experiments, we ﬁnd that such a simple design (i.e., depth-\nwise 3×3) readily achieves on par or even better performance than the recent SOTAs. We give the\ntorch implementation example in Alg. 1.\nD M ORE DISCUSSIONS\nD.1 W HY RPE WORKS LESS WELL THAN ABSOLUTE PE?\nAs mentioned in Section 5.3 (main text), RPE is inferior to the absolute positional encoding. It\nis because RPE does not encode any absolute position information. Also discussed in Section B.3\n(main text), absolute position information is also important even for ImageNet classiﬁcation as it is\nneeded to determine which object is at the center of the image. Note that there might be multiple\nobjects in an image, and the label of an image is the category of the object at the center.\nAdditionally, although RPE becomes popular recently, it is often jointly used with absolute posi-\ntional encodings (e.g., in ConViT (d’Ascoli et al., 2021)), or the absolute position information is\nleaked in other ways (e.g., convolution paddings in CoAtNet (Dai et al., 2021)). This further sug-\ngests absolute position information is crucial.\n16\nPublished as a conference paper at ICLR 2023\nAlgorithm 1 PyTorch snippet of PEG.\nimport torch\nimport torch.nn as nn\nclass VisionTransformer:\ndef __init__(layers=12, dim=192, nhead=3, img_size=224, patch_size=16):\nself.pos_block = PEG(dim)\nself.blocks = nn.ModuleList([TransformerEncoderLayer(dim, nhead, dim*4) for _ in range(\nlayers)])\nself.patch_embed = PatchEmbed(img_size, patch_size, dim*4)\ndef forward_features(self, x):\nB, C, H, W = x.shape\nx, patch_size = self.patch_embed(x)\n_H, _W = H // patch_size, W // patch_size\nx = torch.cat((self.cls_tokens, x), dim=1)\nfor i, blk in enumerate(self.blocks):\nx = blk(x)\nif i == 0:\nx = self.pos_block(x, _H, _W)\nreturn x[:, 0]\nclass PEG(nn.Module):\ndef __init__(self, dim=2\\textsc{56}, k=3):\nself.pos = nn.Conv2d(dim, dim, k, 1, k//2, groups=dim) # Only for demo use, more\ncomplicated functions are effective too.\ndef forward(self, x, H, W):\nB, N, C = x.shape\ncls_token, feat_tokens = x[:, 0], x[:, 1:]\nfeat_tokens = feat_tokens.transpose(1, 2).view(B, C, H, W)\nx = self.pos(feat_tokens) + feat_tokens\nx = x.flatten(2).transpose(1, 2)\nx = torch.cat((cls_token.unsqueeze(1), x), dim=1)\nreturn x\nD.2 C OMPARISON TO LAMBDA NETWORKS\nOur work is also related to Lambda Networks (Bello, 2021) which uses 2D relative positional encod-\nings. We evaluate its lambda module with an embedding size of 128, where we denote its encoding\nscheme as RPE2D-d128. Noticeably, this conﬁguration has about 5.9M parameters (comparable to\nDeiT-tiny) but only obtains 68.7%. We attribute its failure to the limited ability in capturing the\ncorrect positional information. After all, lambda layers are designed with the help of many CNN\nbackbones components such as down-sampling to form various stages, to replace ordinary convolu-\ntions in ResNet (He et al., 2016). In contrast, CPVT is transformer-based.\nD.3 Q UALITATIVE ANALYSIS OF CPVT\nThus far, we have shown that PEG can have better performance than the original positional encod-\nings. However, because PEG provides the position in an implicit way, it is interesting to see if PEG\ncan indeed provide the position information as the original positional encodings. Here we inves-\ntigate this by visualizing the attention weights of the transformers. Speciﬁcally, given a 224 ×224\nimage (i.e. 14 ×14 patches), the score matrix within a single head is 196 ×196. We visualize the\nnormalized self-attention score matrix of the second encoder block.\nWe ﬁrst visualize the attention weights of DeiT with the original positional encodings. As shown in\nFigure 5 (middle), the diagonal element interacts strongly with its local neighbors but weakly with\nthose far-away elements, which suggests that DeiT with the original positional encodings learn to\nattend the local neighbors of each patch. After the positional encodings are removed (denoted by\nDeiT w/o PE), all the patches produce similar attention weights and fail to attend to the patches near\nthemselves, see Figure 5 (left).\nFinally, we show the attention weights of our CPVT model with PEG. As shown in Figure 5 (right),\nlike the original positional encodings, the model with PEG can also learn a similar attention pattern,\nwhich indicates that the proposed PEG can provide the position information as well.\nWe illustrate the attention scores in several encoder blocks of DeiT (Touvron et al., 2020) and CPVT\nin the Fig. 6. It shows both methods learn similar locality patterns. As attention scores are computed\nover the tokens projected in different subspaces (Q and K), they do not necessarily show a strict\ndiagonal pattern, where some may have slight shift, see DeiT in Fig. 6c and CPVT of Fig. 5 right.\n17\nPublished as a conference paper at ICLR 2023\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nDeiT w/o PE\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nDeiT\n0.1\n0.2\n0.3\n0.4\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nCPVT\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFigure 5. Normalized attention scores (ﬁrst head) of the second encoder block of DeiT without po-\nsition encoding (DeiT w/o PE), DeiT (Touvron et al., 2020), and CPVT on the same input sequence.\nPosition encodings are key to developing a schema of locality in lower layers of DeiT. Meantime,\nCPVT proﬁts from conditional encodings and follows a similar locality pattern.\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nDeiT\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nCPVT\n0.05\n0.10\n0.15\n0.20\n0.25\n(a) The second head (encoder 2)\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nDeiT\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nCPVT\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14 (b) The third head (encoder 2)\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nDeiT\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nCPVT\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n(c) The ﬁrst head (encoder 3)\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nDeiT\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0 50 100 150\n0\n25\n50\n75\n100\n125\n150\n175\nCPVT\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200 (d) The second head (encoder 3)\nFigure 6. Normalized attention scores (the second and third head) of the second and third encoder\nblock of DeiT (Touvron et al., 2020), and CPVT on the same input sequence. DeiT and CPVT share\nsimilar locality patterns that are aligned diagonally (some might shift).\nD.4 C OMPARISON WITH OTHER APPROACHES\nWe further compare our method with other approaches such as CvT (Wu et al., 2021), ConViT\n(d’Ascoli et al., 2021) and CoAtNet (Dai et al., 2021) on ImageNet validation set in Table 15. To\nmake fair comparisons, we categorize these methods into two groups: plain and pyramid models.\nSince our models are primarily for plain models, we adapt our methods on two popular pyramid\nframeworks PVT and Swin. Our CPVT-S-GAP slightly outperforms ConViT-S by 0.2% with 4M\nfewer parameters and 0.8G fewer FLOPs. When equipped with pyramid designs, our methods are\nstill comparable to CvT and CoAtNet.\nComparison with DeiT w/ Convolutional Projection. Note CvT uses a depth-wise convolution\nin q-k-v projection which they call it Convolutional Projection. Instead of using it in all layers, we\nput only one of such design into DeiT-tiny and train such a model from scratch under strictly con-\ntrolled settings. We insert it in the position 0 as in our method. The result is shown in Table 16. This\nCvT-ﬂavored DeiT achieves 70.6% top-1 accuracy on ImageNet validation set, which is lower than\nours (72.4%). Note that q-k-v projections in CvT utilize three depthwise convolutions, therefore,\nthis setting has more parameters than ours. This attests the difference of CvT and CPVT, verifying\nour advantage by learning better position encodings other than inserting them in all layers to have\nthe ability to capture local context and to remove ambiguity in attention.\n18\nPublished as a conference paper at ICLR 2023\nTable 15. Performance comparison with other approaches such as CvT (Wu et al., 2021), ConViT\n(d’Ascoli et al., 2021) and CoAtNet (Dai et al., 2021) on ImageNet validation set. All the models\nare trained on ImageNet-1k dataset and tested on the validation set using 224×224 resolution.\nModel Type Params FLOPs Top-1 Acc\n(%)\nDeiT-small (Touvron et al., 2020) Plain 22M 4.6G 79.9\nConViT-S (d’Ascoli et al., 2021) Plain 27M 5.4G 81.3\nCPVT-S-GAP (ours) Plain 23M 4.6G 81.5\nCoAtNet-0 (Dai et al., 2021) Pyramid 25M 4.2G 81.6\nCvT-13 (Wu et al., 2021) Pyramid 20M 4.5G 81.6\nPVT-small (Wang et al., 2021) Pyramid 25M 3.8G 79.8\nPVT-small+PEG+GAP Pyramid 25M 3.8G 81.2\nSwin-tiny (Liu et al., 2021) Pyramid 29M 4.5G 81.3\nSwin-tiny+PEG+GAP Pyramid 29M 4.5G 82.3\nTable 16. Comparison with positional encoding in CvT (Wu et al., 2021) on ImageNet validation set.\nAll the models are trained on ImageNet-1k dataset and tested on the validation set using 224 ×224\nresolution.\nModel Params Insert Position Top-1 Acc\n(%)\nCPVT-Ti 5681320 0 72.4\nDeiT+ Convolutional Projection 5685352 0 70.6\n19",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7757964134216309
    },
    {
      "name": "Computer science",
      "score": 0.6931594610214233
    },
    {
      "name": "Encoding (memory)",
      "score": 0.579391598701477
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5562041997909546
    },
    {
      "name": "Computer vision",
      "score": 0.40646710991859436
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3373851776123047
    },
    {
      "name": "Voltage",
      "score": 0.1330447494983673
    },
    {
      "name": "Engineering",
      "score": 0.11257386207580566
    },
    {
      "name": "Electrical engineering",
      "score": 0.07096078991889954
    }
  ],
  "topic": "Transformer"
}