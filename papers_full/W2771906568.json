{
    "title": "Character-Based Machine Learning vs. Language Modeling for Diacritics Restoration",
    "url": "https://openalex.org/W2771906568",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A5043042273",
            "name": "Jurgita Kapočiūtė-Dzikienė",
            "affiliations": [
                "Vytautas Magnus University"
            ]
        },
        {
            "id": "https://openalex.org/A5087926459",
            "name": "Andrius Davidsonas",
            "affiliations": [
                "Vytautas Magnus University"
            ]
        },
        {
            "id": "https://openalex.org/A5068635646",
            "name": "Aušra Vidugirienė",
            "affiliations": [
                "Vytautas Magnus University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2251114654",
        "https://openalex.org/W1524281572",
        "https://openalex.org/W2012804051",
        "https://openalex.org/W2790740913",
        "https://openalex.org/W2042547117",
        "https://openalex.org/W2149995043",
        "https://openalex.org/W2579409202",
        "https://openalex.org/W4381304672",
        "https://openalex.org/W2522453850",
        "https://openalex.org/W2560674852",
        "https://openalex.org/W2084413241",
        "https://openalex.org/W1514518395",
        "https://openalex.org/W2051349122",
        "https://openalex.org/W4251358250",
        "https://openalex.org/W2156202195",
        "https://openalex.org/W2251939381",
        "https://openalex.org/W2554098798",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W2009304130",
        "https://openalex.org/W2573128311",
        "https://openalex.org/W2741920900",
        "https://openalex.org/W566524735",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2148924495",
        "https://openalex.org/W2187307450",
        "https://openalex.org/W1574718645",
        "https://openalex.org/W3167840799",
        "https://openalex.org/W195358446"
    ],
    "abstract": "In this research we compare two approaches, in particular, character-based machine learning and language-modeling and offer the best solution for the diacritization problem solving. Parameters of tested approaches (i.e., a huge variety of feature types for the character-based method and a value n for the n-gram language-modeling method) were tuned to achieve the highest possible accuracy. Despite the main focus is on the Lithuanian language, we posit that obtained findings can also be applied to other, similar (Latvian or Slavic) languages. During experiments we measured the performance of approaches on 10 domains (including normative texts and non-normative Internet comments). The best results reaching ~99.5% and ~98.4% of the accuracy on characters and words, respectively, were achieved with the tri-gram language modeling method. It outperformed the character-based machine learning approach with an optimal composed feature set by ~1.4% and ~3.8% of the accuracy on characters and words, respectively.DOI: http://dx.doi.org/10.5755/j01.itc.46.4.18066",
    "full_text": "Information T echnology and Control 2017/4/46\n508\nCharacter-Based Machine \nLearning vs. Language Modeling \nfor Diacritics Restoration\nITC 4/46\nJournal of Information Technology  \nand Control\nVol. 46 / No. 4 / 2017\npp. 508-520\nDOI 10.5755/j01.itc.46.4.18066  \n© Kaunas University of Technology\nCharacter-Based Machine Learning vs. Language  \nModeling for Diacritics Restoration\nReceived  2017/04/27 Accepted after revision  2017/10/05\n    http://dx.doi.org/10.5755/j01.itc.46.4.18066 \nCorresponding author: jurgita.kapociute-dzikiene@vdu.lt\nJurgita Kapočiūtė-Dzikienė, Andrius Davidsonas, Aušra Vidugirienė \nFaculty of Informatics; Vytautas Magnus University; Vileikos Str. 8, LT-44404, Kaunas, Lithuania \nIn this research we compare two approaches (in particular, character-based machine learning and language \nmodeling) and according to their results offer the best solution for the diacritization problem solving. Parame-\nters of tested approaches (i.e., a huge variety of feature types for the character-based method and a value n for \nthe n-gram language modeling method) were tuned to achieve the highest accuracy. Despite the main focus is \non the Lithuanian language, we posit that obtained findings can also be applied to other, similar (Latvian or \nSlavic) languages.\nDuring experiments we measured the performance of used approaches on 10 domains (including normative \ntexts and non-normative Internet comments). The best results reaching ~99.5% and ~98.4% of the accuracy \non characters and words, respectively, were achieved with the tri-gram language modeling method. It outper -\nformed the character-based machine learning approach with the tuned feature set by ~1.4% and ~3.8% of the \naccuracy on characters and words, respectively.\nKEYWORDS: Diacritics restoration, character-based machine learning vs. language modeling, the Lithuanian \nlanguage.\n1. Introduction\nThe increasingly rapid pace of life causes people to \nsave every minute, but does not suppress their will to \nshare news and ideas on social networks, to express \nopinions on forums, to react to news by posting In -\nternet comments, to tweet or to chat. People want to \nbe quick and responsive, they often write from their \ntablets and smart-phones. However, all these habits \ndo not pass without a trace in an everyday written \n509\nInformation T echnology and Control 2017/4/46\nlanguage. The Internet slang phenomenon refers \nto a presence of invalid words (typing errors, abbre -\nviations, simplified versions of existing words and \nneologisms), influent and incomplete sentences. In \naddition, due to ergonomic reasons or writing habits \npeople tend to ignore diacritic marks, by replacing \ncharacters with diacritics with their ASCII equiva -\nlents. Missing diacritics reduce the readability of texts \nand cause various ambiguities. For instance, from a \nshort Lithuanian phrase with missing diacritics sune-\nlis susirgo (where sunelis is a grammatically incorrect \nword) is not clear if sūnelis susirgo (a sonny got sick) \nor šunelis susirgo (a doggy got sick). However, words \nwhich maintain grammatical correctness with miss -\ning diacritics cause even more difficulties: e.g., rastas \n(found) vs. rąstas (a balk) or raštas (a script); karstas \n(a coffin) vs. karštas (hot), etc. Most of similar ambi -\nguities are easily resolved by humans (who are able to \nretain contextual and meta-information in mind and/\nor to link it with the general knowledge) but it is not \nthe case for the computational processing. Due to this \nreason, the diacritics restoration research area is still \nimportant and active. \nThe contemporary Natural Language Processing \nresearch is more focused on the real-world applica -\ntions, mostly involving the non-normative language \ntexts with missing diacritics. However, the effective \nsolutions for, e.g., Corpora Acquisition, Information \nRetrieval, Machine Translation and other applica -\ntions often rely on the external resources (such as \nontologies or databases) and tools (morphological \nanalyzers or parsers), which are mostly adjusted for \nthe normative texts. Therefore, a low quality of text \ndegrades the performance of these tools and as a \nconsequence reduces the overall accuracy of all pre -\nviously-mentioned applications. Hence, an insertion \nof missing diacritics becomes an important pre-pro -\ncessing component.\nOf all the 36 European languages, only English does \nnot have the diacritization problem. Even though \nsome loanwords (e.g., café, pâté, etc.) exist in English, \nall of them do not have undiacritized equivalents. \nThe focus of this paper is on the Lithuanian language \nwhich orthography has nine letters with diacritics ą, \nč, ę, ė, į, š, ų, ū, ž often replaced with a, c, e, e, i, s, u, u, z, \nrespectively. Before offering an effective solution for \nsolving the diacritization problem for the Lithuanian \nlanguage, we will review the existing approaches.\n2. Related Work\nDespite the non-usage of diacritics is based on a de -\nliberate writer’s decision, this problem is often con -\nsidered as a special case of spelling correction. In gen-\neral, the diacritization problem does not seem very \ncomplicated: the previous research works done on \nvarious languages report over ~90% of the accuracy. \nEven though the main goal always remains to achieve \nas high accuracy as possible, the solutions tackling \nthis problem have evolved over time. \nThe pioneering attempts were based on the naïve dic-\ntionary lookup methods: if the matching word could \nnot be found in a dictionary, a system provided a list \nof possible word-alternatives. More sophisticated \nknowledge-based approaches, which already check \nthe context of the analyzed word, require syntax anal-\nysis and word-sense disambiguation. The seminal \nwork in this field was done by Yarowsky [31], who \nformulated the diacritization problem as the disam -\nbiguation problem and solved it for the Spanish and \nFrench languages. The offered decision approach \nanalyzed a context around the target word relying on \nn-gram part-of-speech tags, morphological analysis \nand word classes for Spanish, but not on linguistic or \nlexical resources for French (where a classifier was \ntrained on raw word associations solely). Even for \nthe most difficult ambiguities, the accuracy exceeded \n~90%. A very similar method based on the automatic \nconstruction of the lemmatizer with lemmatization \npatterns from the full form lemma dictionary was \napplied on the Czech language [14]. The achieved dia-\ncritics restoration results were similar to the lemma-\ntization results on the Prague Dependency Treebank.\nLater, the main interest of the diacritization problem \nsolving shifted towards statistical approaches, which \ncan be grouped into two main categories: i.e., charac-\nter-based and word-based. \nThe main advantage of the character-based ap -\nproaches is that they are rather fast, simple, and easy \nto implement. Moreover, they are considered as lan -\nguage-independent, because they do not require any \nadditional language resources apart from the pure \ntext. The ambiguous characters (letters with diacrit -\nics and their ASCII equivalents) are treated as the \ntraining instances: the original form with diacritics \n(the so-called base form) becomes a class and the con-\ntext around the target character (preceding or suc -\nInformation T echnology and Control 2017/4/46\n510\nceeding characters, words or their parts) plays a role \nof features. Afterwards, all these instances are used to \ntrain a classifier. The method of this type was applied \nand evaluated on the newspaper texts in four languag-\nes: Czech, Hungarian, Romanian, and Polish [20-21]. \nThe instance-based approach of the TiMBL (Tilburg \nMemory-Based Learner) implementation was used \nas a classifier. The best accuracy was achieved with a \nwindow size equal to ten (having five preceding and \nfive succeeding characters around the target one). A \nsimilar approach is offered for the Maori language \n[7]. It employs the naïve Bayes classifier and a rich set \nof features, containing character and word n-grams \n(window before/ after the target character) both sim-\nple and compound. The experiments carried out on \nthe corpus of short stories, Bible verses, dictionary \ndefinitions and conversational texts resulted in over \n~99% of accuracy. The paper in [9] describes experi -\nments based on (1) the machine learning (ML) and \n(2) the technique combining the ML with the lexi -\ncon lookup for (1)  unrecognized and (2)  recognized \nwords, respectively. This method was able to restore \ndiacritics on the basis of the local character context \n(using a sliding window of eleven characters, includ -\ning the target one). The authors trained the memo -\nry-based TiMBL classifier and applied their method \non 14 languages, including five Indo-European ones. \nAnother language-independent approach is based on \na sequence classification and uses a recurrent Neural \nNetwork with memory layers to restore diacritics for \nArabic [5]. During training, each text sequence is fed \ninto the network to create a prediction for each char -\nacter. The input layer stacks preceding and succeed -\ning character vectors, thus enabling the model to also \nlearn contextual information. The experiments per -\nformed on the Arabic Treebank prove that the offered \ntechnique is competitive to the other state-of-the-art \nmethods that have access to external resources.\nUnlike character-based, the word-based systems are \ntypically language-dependent and knowledge-inten -\nsive, i.e., they rely on dictionaries and probabilistic \nlanguage models, which indeed are an integral part of \nthe language. The ambiguities in the text are usually \nresolved by using the n-gram language model which as-\nsigns the probability to some word (or phrase) depend-\ning on the previous words, corrected with the language \nmodeling. However, the creation of such exhaustive \nand robust models containing probability distributions \nover sequences of words and reflecting the language it \nrepresents requires huge amounts of grammatically \ncorrect text and still remains a risk that some word may \nnot be encountered. The method of this type was suc-\ncessfully applied on the Romanian language [23]. The \nauthors describe the methodology which allows iden-\ntifying correct and reliable text sections (with regard \nof diacritics use) in the corpus. If some text produces \na ratio below the determined threshold, the text is con-\nsidered unreliable (i.e., having a lack of diacritics). This \nmethod was developed for the automatic speech rec -\nognition task and demonstrated the ~20% reduction of \nthe out-of-vocabulary words. Another method applied \non the Romanian language and Internet-extracted text \ncorpora is based on a Viterbi algorithm, which allows se-\nlecting an optimal state sequence from a variable num-\nber of possible options for each word in a sentence [10]. \nOne more statistical language modeling method offered \nfor the Romanian language builds two high-level struc-\ntures, an n-gram language model and a probabilistic \nmap linking non-diacritical words to their diacritical \nforms [8]. The ambiguities in the mapping are resolved \nby finding the sequence with the highest probability. \nThe authors also tested n of n-grams from the interval \n[2, 5] and achieved the best results with n=3. A solution \nfor the Spanish language combines bigrams of the target \nword with preceding or succeeding words, backed-off  \nwith unigrams of the target words [4]. Another meth-\nod described in [28] was applied on the French texts. It \nuses a stochastic Hidden Markov Model (HMM) as the \nlanguage model (in which non-diacritized words are \nconsidered as observations and their possible diacriti-\nzation alternatives are hidden states that produce these \nobservations) which assigns the scores to any sequence \nof words. Afterwards, each possible combination is ex-\namined to find the most probable one. The dictionary \nwith the bi-gram language model (using Witten-Bell \nsmoothing) method (out of two more tested methods, \nprecisely, (1) dictionary-based and (2) dictionary-based \nwith unsmoothed bi-gram language model) was the \nmost accurate on the Croatian newspaper articles and \nforum posts [25]. The dynamic programming method \naims to assign diacritics to the Arabic texts, collected \nfrom the Islamic religious heritage books [13]. The pos-\nsible sequences with diacritics are assigned with scores \nusing the n-gram language model and afterwards the \ndynamic algorithm searches the most likely sequence. \nThe researchers claim that the higher order n-grams (in \nparticular, trigrams, tetra-grams) can lead to a higher \nword accuracy rate.\n511\nInformation T echnology and Control 2017/4/46\nThere are some methods which do not completely fit \ninto the frame of previously described word-based \napproaches. A good example of such approach is a \nsystem proposed for the Algerian Arabic dialectal \ntexts [12]. The diacritization process is performed via \nthe statistical machine translation from the undia -\ncritized source text to the diacritized target text. The \nsystem is phrase-based, uses GIZA++ toolkit (devel -\noped for the Statistical Machine Translation) for the \nword alignment, and is trained on books. Even though \nthe experiments were performed on a small parallel \ncorpus, the authors claim that the system achieved \nacceptable results. Another similar approach, also \nbased on the statistical machine translation but im -\nplemented for the Hungarian texts, is described in \n[22]. It incorporates a morphological analyzer and is \nable to achieve the accuracy equal to ~99%. \nThere also are examples of hybrid approaches, com -\nbining the advantages of several techniques. The pro-\nposed hybrid method restoring diacritics in Turkish \nsocial media texts uses discriminative sequence clas-\nsifier (i.e., Conditional Random Fields – CRFs) in the \nfirst stage and a language validator to select the pos -\nsible options in the second [2]. Another statistical \napproach integrating multiple knowledge sources \n(stemming, part-of-speech tagging, pronunciation \nlexicons, and word bigrams) revealed that charac -\nter-level trigrams achieve the highest accuracy by ap-\nplying all these language sources on the Urdu corpus \n[3]. The system applied on the Romanian journalism \nand juridical texts (described in [29]) uses lexicon \nlookup and HMM language model for known words \nand character-based method (similar to described by \n[20]) for the out-of-vocabulary words.  \nThe diacritization problem solved by using only one \nmethod under very different experimental conditions \nfor the various languages does not give the answer \nwhich method is actually the top-notch. The compar-\native research on Wikipedia, general web and Twitter \ntexts using the lexicon approach (applying the most \nfrequent token-level translation) and the corpus-based \napproach (combining information about the proba -\nbility of a translation and the probability of observing \na translation in the given context via a simple log-lin -\near model) proves the superiority of the corpus-based \napproach for all three tested languages, in particular, \nCroatian, Serbian, and Slovene [17]. The research de-\nscribed in [27] compares two diacritization methods \non the Arabic Treebank and AppTek Data texts. The \nfirst method is based on the machine translation which \npost-edits rule-based diacritization system. The sec -\nond one explores the traditional view of sequence la -\nbeling problem using the CRFs classifier and a rich \nset of features (lexical, morphological and character). \nThe experiments claim that machine translation ap -\nproach performs better compared to the sequence la -\nbeling. The authors in [1] compare four techniques on \nthe Hungarian Web corpus and Facebook comments, \nand demonstrate that the character-based technique \nis robust enough to outperform the dictionary-lookup, \nthe dictionary-based, and the dictionary with charac-\nter bigram based methods. Moreover, it requires only a \nfew characters of context, thus can be applied to very \nshort text segments, as, e.g., tweets. The comprehen -\nsive comparative research described in [26] compares \nseven algorithms based on two lexicon lookup, four \ncharacter-level statistical models and the combination \nof both techniques (i.e., lexicon lookup and the best of \ncharacter-level statistical models for words that do or \ndo not appear in the lexicon, respectively). These al -\ngorithms were applied on the texts harvested from the \nInternet of ~100 African and other languages, including \nLithuanian. The offered tool uses the naïve Bayes clas-\nsifier for the word-level and the character-level mod -\neling; all models are trained on the lowercased letters \nand use smoothing. \nAs demonstrated by Scannell [26], the best results \non the Lithuanian texts are achieved with the lexi -\ncon lookup method. Scannell’s paper is influential \nbecause it reports the only diacritics restoration re -\nsults for the Lithuanian language. On the other hand, \nthe tested methods are not specifically adjusted to \nthe language characteristics; the testing set is small \nand unvaried. Consequently, the contribution of our \nwork is to test and compare different techniques and \naccording to the obtained results to offer the best pos-\nsible diacritics restoration technique for the Lithua -\nnian language, taking into account language-specific \ncharacteristics.\n3. A Character-Based ML Method\nA formal description of the task\nThe mathematical formulation of the diacritization \nproblem that we are attempting to solve with the \nInformation T echnology and Control 2017/4/46\n512\ncharacter-based ML method is presented below.\nLet di be any character in the lowercased text. Let set \n= {a, ą, c, č, e, ę, ė, i, į, s, š, u, ų, ū, z, ž} denote a set of am-\nbiguous characters in the Lithuanian language.\nThe (undiacritized) target character di has its correct \nbase form (so-called class ci), where class ci  ∈  set or \nci ∉ set (i.e., ci = *).  Thus, with | set| + 1 = 16 classes in \ntotal we get the multi-class classification task. \nLet f be a fixed set of features, describing each di later \nused in the ML task. \nLet γ : ∀di → ci denote a function mapping each di to its \nclass (correct character base form) ci. The aim of this \nwork is to offer a method, which would return as close \napproximation of γ as possible.\nFeatures\nLet d0 and w0 denote a target character and a tar -\nget word (such that d0 ∈ w0), respectively. Thus, the \ntraining instance of that target character (within a se-\nquence of other characters) is described with a set of \nthe following features: \n _ dn – a single character without diacritics. In our \nexperiments we used n from the interval [-1,1]. \nThus, d0, d-1 and d1 points to the target character, \nto the character preceding d0, to the character \nsucceeding d0, respectively.\n _ wm – a single word without diacritics. In our \nexperiments we tested m ∈ [-3,3].\n _ d[n1, n2] – a sequence of characters without diacritics, \nwhich precede (n1 ∈ [-6,-1]) or succeed (n2 ∈ [1,6]) \nthe target character d0 in the word w0. \n _ d(wm)[n1, n2] – a sequence of characters without \ndiacritics extracted from the end ( n1  ∈  [-3,-1] \nand n2  =  0) of w-1 or the beginning ( n1  =  0 and \nn2 ∈ [3,4]) of w1.\nClassification\nDuring the classification stage, the labeled (with \nknown classes) training instances are given to a clas -\nsifier. The classifier generates a model, which can af -\nterwards be used for the diacritics restoration. \nIn our experiments, we used the CRFs classifier, \nintroduced by Lafferty et al. [16]. 1 This discrimina -\ntive-based approach was selected due to a number \n1  The implementation of the CRFs method was downloaded \nfrom https:/ /taku910.github.io/crfpp/.\nof reasons. Firstly, the created model outputs confi -\ndence measures indicating how certain it is about the \npredicted labels. Moreover, CRFs are not based on an \nindependent assumption (stating that features do not \ndepend, therefore do not affect each other) and allow \nproviding neighboring characters as features into \nthe system. Furthermore, this method has been suc -\ncessfully tested in similar diacritization tasks, e.g., on \nTurkish [2]. \nIn our experiments, we tuned only the feature set (by \nadding new features and testing their influence on the \noverall accuracy) and for each instance selected the \npredicted label with the highest probability. \n4. The Language Modeling-Based \nMethod\nThe formal description of the task\nThe mathematical formulation of the diacritization \nproblem that we are attempting to solve with the \nn-gram language modeling-based method is present -\ned below. \nLet wi denote a word and m denote a length of a word \nsequence. \nThe language model assigns a probability P to any se-\nquence of words appearing in the text P(w1, w2,… , wm).\nHaving a unigram model with n=1, the probability \nP(wi) of some wi depends on the word itself and is cal-\nculated as P(wi) = count(wi) / count(all). Thus, the sum \nof all probabilities in the corpus must be equal to 1.\nHaving the higher order n-gram model with n > 1 the \nprobability is calculated as:\n∏\n=\n−−−≈\nm\ni\niniim w,...,w|wPw,...,w,wP\n1\n1)1(21 )()( ,\nin which the conditional probability is calculated as: \n)(\n)(\n)(\n1)1(\n1)1(\n1)1(\n−−−\n−−−\n−−− =\nini\niini\ninii w,...,wcount\nw,w,...,wcount\nw,...,w|wP ,\nLanguage models\nDuring the diacritics restoration process, a group of \npossible word-candidates with diacritics are gener -\n513\nInformation T echnology and Control 2017/4/46\nated for each undiacritized target word. For instance, \nthe group of candidates for kasti would consist of two \ngrammatically correct words: kasti (to dig), kąsti (to \nbite) and six grammatically incorrect words: kašti, \nkąšti, kastį, kąstį, kaštį, kąštį. \nWhen multiple grammatically correct word-candi -\ndates are possible (e.g., kasti (to dig), kąsti (to bite)), \nthe ambiguity is resolved considering their probabil -\nities obtained from the training data. Under the uni -\ngram language model, the word-candidate with the \nhigher probability is considered as the right choice \nand replaces the undiacritized target word. When \nusing the higher order n-gram language model, the \nreplacement of the target word depends on the pre -\nviously replaced words. The overall probability of the \nwhole n-gram has to be the highest [6].\nThe search is performed by using the back-off search \nstrategy, i.e., if some n-gram language model fails, the \nmethod continues with the lower order of n-gram, i.e., \nwith n-1, n-2, etc., and if necessary, n=1. In our exper-\niments, we used our own language modeling method \nimplementation.\n5. The Data\nIn our experiments, we used two datasets: (1)  Devel-\nopmentSet (see in Table  1) for tuning the feature set \nof the character-based ML method; (2)  TestSet (see \nin Table  2) for testing both character-based ML and \nlanguage modeling methods. The captions Numb. of \nwords, Numb. (%) of amb. words , Numb. of characters, \nNumb. (%) of amb. char.  in both tables denote a num -\nber of words, a number (percentage) of words having \nambiguous characters ( a, which can remain a or be \nreplaced to ą; c which can remain c or be replaced to č, \netc.), a number of characters, a number of ambiguous \ncharacters (percentage), respectively.\nDevelopmentSet (used for training and parameter \ntuning) was composed of the normative texts taken \nfrom Vytautas Magnus University corpus [18]. It con-\nsists of 50 thousand words from each of nine domains \n(republican newspapers, local newspapers, popular \nperiodicals, specialized periodicals, fiction, non-fic -\ntion, state documents, philosophical literature trans -\nlations, and memoirs). \nTestSet was also composed of the normative texts tak-\nen from Vytautas Magnus University corpus (none of \nthe texts in DevelopmentSet and TestSet overlapped). \nHowever, in addition to the normative texts (which \ncan be considered as artificial for the diacritization \nproblem solving) we added one more domain, i.e., the \nnon-normative Internet comments. These comments \nwere harvested from under the articles of two sub -\njects “In Lithuania” and “Abroad” in the news portal \nwww.delfi.lt. These texts were manually corrected by \na human expert to become suitable for the diacritics \nrestoration testing. It is hard to distinguish problems \nrelated to some specific diacritization marks (either \nall or none of the diacritization marks were used in \nthe text). It is important to note that the correction \nprocess involved only diacritization problems (not \ntouching any other mistakes) (see Internet comments \ndomain in Table 2). Moreover, diacritized characters \nand words found in the Internet comments were not \nused for testing and evaluation, therefore, they could \nnot affect the overall diacritics restoration results. \nIn the language modeling experiments, we used four \ntext corpora, containing only normative language texts: \n _ parliamentary transcripts. The corpus \nSTENOGRAMOS_INDV of 23,908,302 running \ntokens. \n _ fiction texts. The corpus GROŽINĖ_INDV of \n9,762,077 running tokens.2\n _ news articles. The corpus, described in [15] of \n2,251,725 running tokens.\n _ texts from nine different domains. The corpus, \npresented in Table 2 of 450,000 running tokens.\nAll these corpora were used to create frequency word \n(or their n-gram) lists (ignoring punctuation and dig -\nits) (see Table 3) afterwards used in language model -\ning tasks.\nT able 1\nStatistics about the dataset DevelopmentSet\nNumb. of \nwords\nNumb. (%) of \namb. words\nNumb. of\ncharacters\nNumb. (%) of \namb. char.\n450,000 423,870 \n(94.2%) 2,747,451 1,427,260 \n(51.9%)\n2 Both corpora, i.e., STENOGRAMOS_INDV and \nGROŽINĖ_INDV were downloaded from http:/ /dangus.vdu.\nlt/~jkd/eng/?page_id=16.\nInformation T echnology and Control 2017/4/46\n514\n6. Experiments and Results\nFor diacritization, we lowercased all the words in \nthe texts and performed the following sets of experi -\nments:\nExperiment set No. 1:  the feature set tuning for the \ncharacter-based ML method (presented in the Char-\nacter-based ML method  section). The tuning of the \nfeature set for the CRFs model was performed iter -\natively in the greedy manner, only the feature which \ngave the highest accuracy during particular iteration \nwas added to the feature template for the later itera -\ntions. The development process was initiated with \nthe empty feature template and continued until any \nadded feature (described in the Features subsection \nof the Character-based ML method  section) still al -\nlowed improving the accuracy. During each iteration, \nwe performed an exhaustive search in the set of the \nremaining features (not yet added to the feature tem -\nT able 2\nStatistics about the dataset TestSet\nDomain Numb. of words Numb. (%) of amb. words Numb. of characters Numb. (%) of amb. char.\nrepublican newspapers 20,000 18,806 (94.03%) 122,886 63,108 (51.4%) \nlocal newspapers 20,000 18,969 (94.8%) 124,001 64,928 (52.4%) \npopular periodicals 20,000 17,966 (89%) 116,437 60,485 (51.9%) \nspecialized periodicals 3,355 3,225 (96.1%) 20,053 10,492 (52.3%) \nfiction 20,000 19,365 (96.8%) 108,473 58,142 (53.6%) \nnon-fiction 20,000 19,404 (97.0%) 124,982 64,876 (51.9%) \nstate documents 16,307 14,198 (87.1%) 105,858 51,712 (48.9%)  \nphilosophical lit. transl. 5,938 5,749 (96.8%) 33,067 17,371 (52.5%) \nmemoirs 20,000 18,691 (93.5%) 116,850 61,292 (52.5%) \nInternet comments 20,000 19,120 (95.6%) 113,203 59,708 (52.7%) \nOverall: 165,600 155,493 (93.9%) 985,810 512,114 (51.9%)\nT able 3 \nThe created frequency lists\nOf n  words Numb. of elements The highest frequency\nn=1 705,185 1,285,817\nn=2 12,173,949 60,864\nn=3 25,255,584 18,059\nplate) excluding those features which gave degrada -\ntion in performance in any previous iteration. The \nexperiments were carried out on DevelopmentSet (de-\nscribed in The data  section) using stratified tenfold \ncross-validation. \nExperiment set No. 2:  the evaluation of n for the \nn-gram language modeling method (presented in \nthe Language modeling-based method  section). If the \nmethod with the back-off strategy failed even with \nn=1 (in case of, e.g., the out-of-vocabulary word), it re-\nturned the word without diacritics. The method was \ntested on each of domains in TestSet and on the entire \ndataset. \nExperiment set No. 3:  the evaluation of the charac -\nter-based ML method. After the most accurate feature \ntemplate was determined (during the experiment set \nNo.  1), we trained CRFs classifier on the entire De-\nvelopmentSet to generate the final model. This model \nwas evaluated on the separate domains in TestSet and \non the entire TestSet. The obtained results were com-\npared with the language modeling method.\nDuring all the previously described experiments, we \nevaluated Character Accuracy Rates  ( CAR) Eq.  (1) \nand Word Accuracy Rates (WAR) Eq. (2):\ntersall_charac\ncharacters_ddiacritize_correctlyCAR = (1)\n515\nInformation T echnology and Control 2017/4/46\nall_words\nwords_ddiacritize_correctlyWAR = (2)\nIt is important to mention that while evaluating CAR, \nwe encountered only ambiguous characters ( a, ą, c, č, \ne, ę, ė, i, į, s, š, u, ų, ū, z, ž). \nWe also calculated the baselines to ensure that the \nachieved results are effective and reasonable (i.e., ex -\nceed the determined baselines). The baselines CARB \nand WARB denote CAR and WAR values obtained on \nthe undiacritized TestSet, respectively.\nThe results, obtained during the experiment set No. 1 \nand No. 2 are presented in Figure 1 and Table 4, respec-\ntively. It is important to note that despite the features \npresented in Figure 1, four more (in particular, d[1,3], d[-\n2,-1], d[-5,-1], d[5,1]) gave the marginal increase in the accu-\nracy. As this increase was not statistically significant, \nwe have not added these features in the figure, but still \nhave preserved in the feature template for the experi-\nment set No. 3. In addition, the statistical significance \nwas evaluated by using the McNemar [19] test (with \nthe significance level of 95%), meaning that differences \nwere considered statistically significant if calculated \nprobability density function p was lower than α = 0.05. \nThe language modeling method achieved marginally \nthe best results with n=3. Despite the increase of n=3 \nFigure 1 \nThe accuracy (y axis) achieved with different features (x axis). The gray and black curves represent CAR and WAR values, \nrespectively. The gray and black straight lines – CARB and WARB equal to 0.868 and 0.621, respectively\non n=2 being statistically insignificant on most of the \ndomains, it demonstrated the statistically significant \nincrease on the entire dataset TestSet. The aim of this \nresearch was to compare the effectiveness of charac -\nter-based ML and language modeling methods (with \ntheir best parameters, determined feature set and \nn=3). Figures 2 and 3 report CAR and WAR values ob-\ntained with the character-based ML method (in the \nexperiment set No. 2 and language modeling method \n(in the experiment set No. 3), respectively.3\n7. Discussion\nThe Lithuanian texts consist of ~6.9% diacritized let -\nters [11], moreover, ~39.1% of Lithuanian word forms \ncontain at least one diacritical letter [30]. However, \nthe problem is more complicated, diacritized letters \nhave their ASCII equivalents, therefore, the total \nnumber of ambiguous characters and words in undi -\nacritized texts jumps up to ~52% and ~94%, respec -\ntively (see Tables 1 and 2). Despite it, zooming into \nTable 4 and Figures 1-3 allows us to make the most \n3  All real numbers (presented on each column in Figu-  \nre 2 and Figure 3) represent exact CAR and WAR values (not \ntheir increase over CARB and WARB, respectively).\n \n \nInformation T echnology and Control 2017/4/46\n516\nimportant statement – the offered methods are robust \nenough, because all the obtained results exceed their \nbaselines (character and word accuracy rates calcu -\nlated on the undiacritized texts).\nAs can be seen from Figure 1, the best features (mak -\ning the largest impact on the results) for the charac -\nter-based ML method are the following: a character \nwithout diacritics, a word without diacritics, a context \nof four characters around the target character, etc. \nMoreover, all these options in the feature template \nare logically explained. A character without diacritics \nallows determining the particular set of alternatives \n(e.g., c might remain as c or be replaced with č, but \nnever with ą). The word (with or without diacritics) \nitself helps the method to “memorize” incoming dia -\ncritics. The close context around the target character \nT able 4 \nThe accuracy values using different n with the language \nmodeling method. The upper/lower values in each cell \nrepresent CAR/WAR values, respectively. The underlined \nvalues indicate statistically insignificant increase on the \nresults in the previous column\nDomain CAR & WAR \nwith n=1\nwith n=2 \n(incr. on n=1)\nwith n=3 \n(incr. on n=2)\nrepublican \nnewspapers\n0.9860\n0.9565\n0.0104\n0.0321\n0.0003\n0.0010\nlocal \nnewspapers\n0.9851\n0.9523\n0.0113\n0.0359\n0.0003\n0.0009\npopular \nperiodicals\n0.9860\n0.9583\n0.0111\n0.0330\n0.0002\n0.0007\nspecialized \nperiodicals\n0.9857\n0.9553\n0.0099\n0.0310\n0.0010\n0.0033\nfiction 0.9858\n0.9597\n0.0113\n0.0320\n0.0024\n0.0070\nnon-fiction 0.9839\n0.9480\n0.0123\n0.0396\n0.0002\n0.0007\nstate documents 0.9899\n0.9679\n0.0060\n0.0189\n0.0007\n0.0022\nphilosophical lite- \nrature translations\n0.9777\n0.9360\n0.0166\n0.0477\n0.0010\n0.0030\nmemoirs 0.9903\n0.9705\n0.0054\n0.0165\n0.0015\n0.0044\nInternet \ncomments\n0.9723\n0.9238\n0.0026\n0.0077\n0.0001\n0.0002\nOverall: 0.9846\n0.9537\n0.0092\n0.0280\n0.0007\n0.0022\nis more important compared to the information in the \npreceding or succeeding words around the target one. \nEven in diacritized texts, the rate of ambiguous mor -\nphological forms reaches ~47% for the Lithuanian \nlanguage [24], therefore, in the undiacritized texts \nthis problem is even more evident. Due to this rea -\nson, the unigram language modeling method is not \nthe best option, because it naïvely chooses the most \ncommon word-candidate not considering any context \naround the target word. Intuitively, it seems that the \nlarger n should assure the higher accuracy (especially \ntaking into account that the language modeling meth-\nod uses the back-off strategy). In fact, it assures (see \nTable 4), but the increase of n=3 over n=2 on most \nof the domains is not statistically significant. We as -\nsume that the higher order n-grams become not very \nhelpful due to the relatively free word order in Lith -\nuanian sentences: the variety becomes huge (e.g., out \nof ~36.4 million running words there were generated \nmore than ~25.2 million trigrams (see Table 3)), but \nsome occurrences are very rare.\nAs can be seen from Figures 2 and 3, the language \nmodeling method outperforms character-based ML \non all domains and on the entire testing set; more -\nover, the differences in their accuracies are statisti -\ncally significant. It allows us to state that the language \nmodeling method is more suitable for the Lithuanian \nlanguage. The explanation of this phenomenon lies \nin the nature of each method. The language model -\ning method has less opportunity to make mistakes, \nbecause it predicts the whole word in its context. The \ncharacter-based ML approach considers one charac -\nter at a time in the analyzed word, which usually has \nmore than one ambiguous character.\nThe character-based ML method with CRFs achieved \nthe best results on the memoirs and state documents; \nthe language modeling on fiction and memoirs. How -\never, normative texts are artificial data, where dia -\ncritics were simply removed for the testing/training \nreasons. The weak spot of both methods is rather low \naccuracy on the real data (i.e., Internet comments), \nwhere diacritics were absent (i.e., not inserted by their \nauthors). The detailed error analysis revealed that the \ncharacter-based ML method fails on the foreign lan -\nguage insertions (which are the most often in the In -\nternet comments, but rare/ absent in the memoirs and \nstate documents), because it still tries to “restore” \ndiacritics. Since most of the foreign words (found in \nthe Internet comments, but absent in frequency lists) \n517\nInformation T echnology and Control 2017/4/46\nFigure 2 \nCAR values (y axis) achieved on the different domains (x axis). The gray and white columns (including the black part of \nCARB values) represent values for the character-based ML  and language modeling  methods, respectively\n \n \nFigure 3\nWAR values (y axis) achieved on the different domains (x axis). The gray and white columns (including the black part of \nWARB values) represent values for the character-based ML  and language modeling methods, respectively\n \n \n \nInformation T echnology and Control 2017/4/46\n518\nare in English, the language modeling method returns \nthem in the untouched/undiacritized and at the same \ntime corrects form. One more difficulty for both of the \nmethods was unusual abbreviations occurring in the \ndataset of Internet comments. The character-based \nML method was trying to restore diacritics, where -\nas the language modeling method treated them as \nout-of-vocabulary words leaving in the undiacritized \nform, which was not always correct.\nResuming, the drawback of the character-based ML \nmethod is the fact that it does not consider the whole \nwords and their context at the same time. The language \nmodeling method suffers from the out-of-vocabulary \nwords problem. Despite the huge number of words or \ntheir n-grams in the frequency lists, something still \nmight be missing. The Lithuanian language is fusional \n(meaning that morphemes in a combination with dif-\nferent affixes denote multiple grammatical, syntactic or \nsemantic word forms), therefore, during the creation of \nfrequency lists it is important not only to find a partic-\nular word, but to find it in all its possible forms, which \nalready requires much more diverse training data. \nWe anticipate that our findings about the diacritization \nproblem solving might be useful not only for the Lithua-\nnian language, but for other languages (as, e.g., Latvian, \nSlavic languages) sharing similar characteristics as well.\n8. Conclusions and Future Work\nIn this research, we are solving the important diacri -\ntization problem for the Lithuanian language, which \nhas never been solved before considering the Lithua -\nnian language characteristics. \nIn this paper we experimentally compared two ap -\nproaches, in particular, character-based ML and \nlanguage modeling, and proved the superiority of the \nlanguage modeling method. The language modeling \nmethod outperformed character-based ML by ~1.4% \nand ~3.8%, achieving ~99.5% and ~98.4% accuracy on \ncharacters and words, respectively. \nThe worst results were obtained on the real data (i.e., \nInternet comments); therefore, in the future research \nwe are planning to focus on similar types of non-nor -\nmative texts. Probably the best solution could be the \ntwo-stage hybrid approach – the out-of-vocabulary \nwords (not recognized with the language modeling) \ncould be processed at the character level and correct-\ned by using the machine learning approach.\nReferences\n1. Ács, J., Halmi, J. Hunaccent: Small Footprint Diacritic \nRestoration for Social Media. Normalisation and Analy-\nsis of Social Media Texts (NormSoMe) Workshop, 2016.\n2. Adalı, K., Eryiğit, G. Vowel and Diacritic Restoration \nfor Social Media Texts. The 5th Workshop on Language \nAnalysis for Social Media (LASM) at EACL, 2014, 53–\n61. https:/ /doi.org/10.3115/v1/W14-1307\n3. Ali, A. R., Hussain, S. Automatic Diacritization for \nUrdu. Proceedings of the Conference on Language and \nTechnology, 2010, 105–111.\n4. Atserias, J., Fuentes, M., Nazar, R., Renau, I. Spell \nChecking in Spanish: the Case of Diacritic Accents. \nProceedings of the 8th International Conference on \nLanguage Resources and Evaluation (LREC), 2012, \n737–742.\n5. Belinkov, Y., Glass, J. Arabic Diacritization with Recur-\nrent Neural Networks. Proceedings of the 2015 Con -\nference on Empirical Methods in Natural Language \nProcessing, 2015, 2281–2285. https:/ /doi.org/10.1007/\ns10032-015-0242-2\n6. Brown, P . F ., deSouza, P . V ., Mercer, R. L., Pietra, V . J. D., \nLai, J. C. Class-Based n-gram Models of Natural Lan -\nguage. Computational Linguistics, 1992, 18, 467–479.\n7. Cocks, J., Keegan, T. T. A Word-Based Approach for Di-\nacritic Restoration in Maori. Proceedings of the Aus -\ntralasian Language Technology Association Workshop \n2011, 2011, 126–130.\n8. Cucu, H., Buzo, A., Besacier, L., Burileanu, C. SMT-\nBased ASR Domain Adaptation Methods for Un -\nder-Resourced Languages: Application to Romanian. \nSpeech Communication, 2014, 56, 195–212. https:/ /doi.\norg/10.1016/j.specom.2013.05.003\n9. De Pauw, G., Wagacha, P . W ., de Schryver, G. M. Auto -\nmatic Diacritic Restoration for Resource-Scarce Lan -\nguages. Proceedings of Text, Speech and Dialogue, the \n10th International Conference, 2007, 4629, 170–179. \nhttps:/ /doi.org/10.1007/978-3-540-74628-7_24\n10. Dumitrescu, S. D., Boroş, T. A Unified Corpora-Based \nApproach to Diacritic Restoration and Word Casing. \nThe 7th Language & Technology Conference: Human \n519\nInformation T echnology and Control 2017/4/46\nLanguage Technologies as a Challenge for Computer \nScience and Linguistics, 2013, 299–303.\n11. Grigas, G., Juškevičienė, A. Raidžių dažnių lietuvių \nir kitose kalbose, vartojančiose lotyniškus rašmenis, \nanalizė [Letter Frequency Analysis of Lithuanian and \nOther Languages Using the Latin Alphabet]. Santalka. \nFilologija, Edukologija, 2015, 23, 81–91.\n12. Harrat, S., Abbas, M., Meftouh, K., Smaïli, K. Diacritics \nRestoration for Arabic Dialect Texts. The 14th Annual \nConference of the International Speech Communica -\ntion Association (INTERSPEECH), 2013, 1429–1433. \nhttps:/ /doi.org/10.21700/ijcis.2016.119\n13. Hifny, Y. Higher Order n-gram Language Models for Ara-\nbic Diacritics Restoration. The 12th ESOLE Conference \non Language Engineering (ESOLEC’12), 2012, 1–5.\n14. Kanis, J., Müller, L. Using Lemmatization Technique for \nAutomatic Diacritics Restoration. SPECOM 2005 pro -\nceedings, 2005, 255–258. https:/ /doi.org/10.1007/978-\n3-540-30120-2_45\n15. Kapočiūtė-Dzikienė, J. Krilavičius, T. Topic Classifica-\ntion Problem Solving for Morphologically Complex Lan-\nguages. The 22nd international conference on Informa-\ntion and Software Technologies (ICIST), 2016, 511–524. \nhttps:/ /doi.org/10.1007/978-3-319-46254-7_41\n16. Lafferty, J. D., McCallum, A., Pereira, F . C. N. Condition-\nal Random Fields: Probabilistic Models for Segment -\ning and Labeling Sequence Data. Proceedings of the \n18th International Conference on Machine Learning \n(ICML’01), 2001, 282–289.\n17. Ljubešić, N., Erjavec, T., Fišer, D. Corpus-Based Diacrit-\nic Restoration for South Slavic Languages. Proceedings \nof the 10th International Conference on Language Re -\nsources and Evaluation (LREC 2016), 2016, 3612–3616.\n18. Marcinkevičienė, R. Tekstynų lingvistika (teorija ir pa-\nktika) [Corpus Linguistics (Theory and Practice)]. Dar-\nbai ir dienos, 2000, 24, 7–63.\n19. McNemar, Q. M. Note on the Sampling Error of the Dif-\nference Between Correlated Proportions or Percent -\nages. Psychometrika, 1947, 12(2), 153–157. https:/ /doi.\norg/10.1007/BF02295996\n20. Mihalcea, R. F . Diacritics Restoration: Learning from \nLetters versus Learning from Words. Computational \nLinguistics and Intelligent Text Processing, the 3rd \nInternational Conference (CICLing 2002), 2002, 339–\n348. https:/ /doi.org/10.1007/3-540-45715-1_35\n21. Mihalcea, R. F ., Nastase, V . A. Letter Level Learning for \nLanguage Independent Diacritics Restoration. Pr o-\nceedings of CoNLL-2002, 2002, 105–111. https:/ /doi.\norg/10.3115/1118853.1118874\n22. Novák, A., Siklósi, B. Automatic Diacritics Restoration \nfor Hungarian. Proceedings of the 2015 Conference on \nEmpirical Methods in Natural Language Processing \n(EMNLP), 2015, 2286–2291. http:/ /www.aclweb.org/\nanthology/D15-1275\n23. Petrica, L., Cucu, H., Buzo, A., Burileanu, C. A Robust Di-\nacritics Restoration System using Unreliable Raw Text \nData. The 4th Workshop on Spoken Language Technol-\nogies for Under-resourced Languages (SLTU), 2014, \n215–220.\n24. Rimkutė, E. Morfologinio daugiareikšmiškumo ribo -\njimas kompiuteriniame tekstyne [The Limitation of the \nMorphological Disambiguation in the Digitalized Cor -\npus]. Ph.D. thesis, Vytautas Magnus University, 2006.\n25. Šantić, N., Šnajder, J., Bašić, B. D. Automatic Diacritics \nRestoration in Croatian Texts. The Future of Informa -\ntion Sciences, Digital Resources and Knowledge Shar -\ning, 2009, 309–318.\n26. Scannell, K. P . Statistical Unicodification of African \nLanguages. Language Resources and Evaluation, 2011, \n45 (3), 375–386. https:/ /doi.org/10.1007/s10579-011-\n9150-3\n27. Schlippe, T., Nguyen, T., Vogel, S. Diacritization as a \nTranslation Problem and as a Sequence Labeling Prob -\nlem. The 8th Conference of the Association for Machine \nTranslation in the Americas, 2008.\n28. Simard, M. Automatic Restoration of Accents in French \nText. Technical Report Co28-1/129-1996E, Industry \nCanada, Centre for Information Technology Innovation \n(CITI), 1996.\n29. Tufiş, D. Ceauşu, A. DIAC+: a Professional Diacritics \nRecovering System. Proceedings of the 6th Internation-\nal Conference on Language Resources and Evaluation \n(LREC’08), 2008, 167–174. http:/ /www.lrec-conf.org/\nproceedings/lrec2008/pdf/54_paper.pdf\n30. Utka, A. Dažninis rašytinės lietuvių kalbos žodynas. \n[Frequency Dictionary of the Lithuanian Language], \n2009. http:/ /donelaitis.vdu.lt/publikacijos/Dazninis_\nzodynas.pdf\n31. Yarowsky, D. Decision Lists for Lexical Ambiguity Res-\nolution: Application to Accent Restoration in Spanish \nand French. Proceedings of the 32nd Annual Meeting of \nthe Association for Computational Linguistics (ACL), \n1994, 88–95. https:/ /doi.org/10.3115/981732.981745\nInformation T echnology and Control 2017/4/46\n520\nIn this research we compare two approaches (in particular, character-based machine learning and language \nmodeling) and according to their results offer the best solution for the diacritization problem solving. Param -\neters of tested approaches (i.e., a huge variety of feature types for the character-based method and a value n for \nthe n-gram language modeling method) were tuned to achieve the highest accuracy. Despite the main focus is \non the Lithuanian language, we posit that obtained findings can also be applied to other, similar (Latvian or \nSlavic) languages.\nDuring experiments we measured the performance of used approaches on 10 domains (including normative \ntexts and non-normative Internet comments). The best results reaching ~99.5% and ~98.4% of the accuracy \non characters and words, respectively, were achieved with the tri-gram language modeling method. It outper -\nformed the character-based machine learning approach with the tuned feature set by ~1.4% and ~3.8% of the \naccuracy on characters and words, respectively.\nLietuviškus nenorminius tekstus (interneto komentarus, elektroninius laiškus, forumų tekstus, socialinių tinklų \nžinutes) įprasta rašyti nenaudojant diakritinių ženklų. Deja, tokius tekstus sudėtinga analizuoti automatiškai: \nmorfologiniai ar sintaksiniai analizatoriai, automatinio vertimo įrankiai, paieškos sistemos ir kt. yra pritaikyti \nveikti su norminės kalbos tekstais. Įvairūs klaidų taisymo įrankiai (lietuvių kalbai) dažniausiai tik siūlo galimas \ntaisymo alternatyvas, tačiau neanalizuoja konteksto ir nėra pritaikyti automatiškai atstatyti diakritinius ženklus \nvisame tekste.\nIš didelės apimties tekstynų (daugiau nei 36 mln. žodžių) konstruojame kalbos modelius, įvertiname įvairių para-\nmetrų, požymių modifikacijų poveikį rezultatams, bei siūlome efektyviausią sprendimą lietuvių kalbai. Metodus \nįvertiname su 10 skirtingų sričių tekstais (interneto komentarais, populiariąja periodika, memuarais ir kt.). Ge-\nriausi rezultatai buvo pasiekti su trigraminiu kalbos modeliu: ~99,5% tikslumas atstatytiems simboliams, ~98,5% – \natstatytiems žodžiams. Suformuluotos išvados gali būti naudingos ir kitoms panašioms kalboms (latvių, slavų).\nSummary / Santrauka"
}