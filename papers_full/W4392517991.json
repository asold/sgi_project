{
    "title": "Spatial deformable transformer for 3D point cloud registration",
    "url": "https://openalex.org/W4392517991",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5034256540",
            "name": "Fengguang Xiong",
            "affiliations": [
                null,
                "North University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5100568642",
            "name": "Yu Kong",
            "affiliations": [
                "North University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5104253814",
            "name": "Shuaikang Xie",
            "affiliations": [
                "North University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5009855115",
            "name": "Liqun Kuang",
            "affiliations": [
                null,
                "North University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5101923580",
            "name": "Xie Han",
            "affiliations": [
                null,
                "North University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4379537143",
        "https://openalex.org/W4388034611",
        "https://openalex.org/W4385551746",
        "https://openalex.org/W4386920567",
        "https://openalex.org/W4382874089",
        "https://openalex.org/W4387415195",
        "https://openalex.org/W2097832352",
        "https://openalex.org/W2981995220",
        "https://openalex.org/W2774320778",
        "https://openalex.org/W4376144045",
        "https://openalex.org/W3167684723",
        "https://openalex.org/W3034675048",
        "https://openalex.org/W2990613095",
        "https://openalex.org/W3177280664",
        "https://openalex.org/W3216353509",
        "https://openalex.org/W2085261163",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W4312960790",
        "https://openalex.org/W2271206385",
        "https://openalex.org/W2098764590",
        "https://openalex.org/W2160821342",
        "https://openalex.org/W1972485825",
        "https://openalex.org/W2574367920",
        "https://openalex.org/W4214707182",
        "https://openalex.org/W4386065804",
        "https://openalex.org/W3196228261",
        "https://openalex.org/W4206998227",
        "https://openalex.org/W4322102230",
        "https://openalex.org/W4324137756",
        "https://openalex.org/W4312516208",
        "https://openalex.org/W3096538374",
        "https://openalex.org/W3035030518",
        "https://openalex.org/W2003447360",
        "https://openalex.org/W2222512263",
        "https://openalex.org/W3171433839",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4226373316",
        "https://openalex.org/W4312349930",
        "https://openalex.org/W2981556384",
        "https://openalex.org/W2963719584",
        "https://openalex.org/W2796426482",
        "https://openalex.org/W3034275286",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W2986382673",
        "https://openalex.org/W3013243617",
        "https://openalex.org/W3035338950"
    ],
    "abstract": "Abstract Deformable attention only focuses on a small group of key sample-points around the reference point and make itself be able to capture dynamically the local features of input feature map without considering the size of the feature map. Its introduction into point cloud registration will be quicker and easier to extract local geometric features from point cloud than attention. Therefore, we propose a point cloud registration method based on Spatial Deformable Transformer (SDT). SDT consists of a deformable self-attention module and a cross-attention module where the deformable self-attention module is used to enhance local geometric feature representation and the cross-attention module is employed to enhance feature discriminative capability of spatial correspondences. The experimental results show that compared to state-of-the-art registration methods, SDT has a better matching recall, inlier ratio, and registration recall on 3DMatch and 3DLoMatch scene, and has a better generalization ability and time efficiency on ModelNet40 and ModelLoNet40 scene.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports\nSpatial deformable transformer \nfor 3D point cloud registration\nFengguang Xiong 1,2,3*, Yu Kong 2, Shuaikang Xie 2, Liqun Kuang 1,2,3 & Xie Han 1,2,3\nDeformable attention only focuses on a small group of key sample-points around the reference \npoint and make itself be able to capture dynamically the local features of input feature map without \nconsidering the size of the feature map. Its introduction into point cloud registration will be quicker \nand easier to extract local geometric features from point cloud than attention. Therefore, we propose \na point cloud registration method based on Spatial Deformable Transformer (SDT). SDT consists of a \ndeformable self-attention module and a cross-attention module where the deformable self-attention \nmodule is used to enhance local geometric feature representation and the cross-attention module is \nemployed to enhance feature discriminative capability of spatial correspondences. The experimental \nresults show that compared to state-of-the-art registration methods, SDT has a better matching \nrecall, inlier ratio, and registration recall on 3DMatch and 3DLoMatch scene, and has a better \ngeneralization ability and time efficiency on ModelNet40 and ModelLoNet40 scene.\nPoint cloud registration is a significant task in the field of computer vision and plays a crucial role in the fields \nof 3D  reconstruction1,2,  SLAM3,4 and autonomous  driving5,6 and so on. The process of 3D point cloud registra-\ntion techniques is to align multiple point clouds from different viewpoints or sensors into a same coordinate \nsystem. Due to the effects of noise, outliers, low overlap rate, etc., point cloud registration becomes a challenging \nproblem. Therefore, it is of great theoretical and practical significance to implement a high-precision and robust \npoint cloud registration algorithm.\nThe traditional Iterative Closest Point (ICP)7 is the most widely used rigid point cloud registration algorithm, \nwhich minimizes point-to-point or point-to-plane distances in the overlapping areas between point clouds, \nand alternately updates the corresponding relationship and transformation matrix between source point cloud \nand target point cloud. However, the main drawback of the ICP algorithm is that it easily converges to local \noptimums. To address this problem, J. Y ang et al.8 proposed global optimal iterative nearest-point algorithm \nGo-ICP , which uses a branch-and-bound approach to search for the globally optimal registration at the cost of \nlonger computing time.\nWith the continuous improvement of computer performance, deep learning-based methods have transformed \ntraditional feature extraction methods. Choy et al.9 proposed FCGF , which used a  ResUNet10 architecture built on \n3D sparse convolution to extract features. However, FCGF is computationally expensive and implicitly decreases \n resolution11. Ao et al.12 proposed SpinNet to extract point cloud rotation invariance features. It consists of two \nmodules, a spatial point transformer and a feature extractor, which make the network be able to learn local spatial \nfeatures with strong robustness to finely register point cloud. X. Bai et al.13 proposed D3Feat including a  KPConv14 \nfeature extraction network which can be extended to deformable convolutions that learn to adapt kernel points \nto local geometry. Meanwhile, D3Feat proposed a novel keypoint selection method and a self-supervised detec-\ntor loss to eliminate the impact of point cloud density on keypoints. On the basis of D3Feat, combined with the \nattention  mechanism15, S.Huang et al.16 proposed PREDATOR which alternately uses self-attention and cross-\nattention mechanisms, and aggregates local and global information of point cloud. PREDATOR showing higher \nregistration accuracy on 3DMatch scene dataset. Li et al. 17 proposed Lepard which can register point cloud in \ndeformable scenes. Lepard builds network using Transformer with self and cross attention, and ideas with dif -\nferentiable matching. In rigid cases, Lepard combined with  RANSAC18 and ICP demonstrates state-of-the-art \nregistration recall. In deformable cases, Lepard also achieves higher non-rigid feature matching recall than the \nstate-of-the-art method.\nWith the development of Transformer with self/cross attention in point cloud registration, the accuracy and \nprecision of point cloud registration have a certain improvement. However, self/cross attention in Transformer \nis a global mechanism that focuses on all positions in the input features, which makes it difficult for the model \nOPEN\n1Shanxi Provincial Key Laboratory of Machine Vision and Virtual Reality, Taiyuan 030051, China. 2School of \nComputer Science and Technology, North University of China, Taiyuan 030051, China. 3Shanxi Province’s Vision \nInformation Processing and Intelligent Robot Engineering Research Center, Taiyuan 030051, China.  *email: \nhopenxfg@nuc.edu.cn\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nto capture local features of the point cloud, and thereby reduces the robustness of registration to noise under \nlow overlap scene. Meanwhile, self/cross attention calculates the weight of each position in the input features, \nwhich increases the computational complexity and affects registration efficiency. In recent years, some research-\ners proposed deformable  attention19,20 on 2D image detection/recognition/classification to break through the \nlimitation of self/cross attention. Deformable self/cross attention is a more flexible attention mechanism, whose \ncore idea is to introduce deformability into the traditional self/cross attention mechanism, and allows the model \nto adaptively adjust the attention focus based on the specific situation of the input feature map. Deformable \nself/cross attention only focuses on a small group of key sampling points around the reference point, without \nconsidering the size of the feature map, and dynamically adjusts the weights of different positions in the input \nfeature. In this way, it can obtain local features and improve the efficiency of feature extraction.\nIn this paper, we introduce deformable self/cross attention into point cloud registration and use spatial local \npositional relationships as the local position embeddings for deformable self-attention. Based on these, we pro-\npose Spatial Deformable Transformer (SDT) for point cloud registration. This approach enhances the ability to \nlearn local geometric features through the SDT module, and reduces effectively the mismatching impact on the \nrobustness of registration by constructing correspondence matrix based on Sinkhorn and Hungarian algorithm. \nOur main contributions are shown as follows.\n• We propose a novel 3D point cloud registration network based on SDT to address point cloud registration \nunder low overlap scenes.\n• We construct a deformable self-attention module to interact local geometric spatial information within the \npoint cloud to enhance the representation of features and make them easier to match.\n• We construct a deformable cross-attention module to transfer features between point clouds to enhance \nfeature discriminative capability of spatial correspondences.\n• We design a balanced weighted loss function which uses focal loss between soft correspondence confidence \nmatrix and the ground truth correspondence matrix as supervision to obtain more accurate hard matching \ncorrespondences between pairs of point clouds.\nRelated Work\nTraditional point cloud registration\nICP7 is a classical traditional point cloud registration method, which finds the closest target points for each point \nin source point to generate 3D-3D correspondences and performs a least-squares optimization to compute rigid \ntransformation between a pair of point clouds. The two steps are iteratively performed until a termination condi-\ntion is satisfied. Many variants, such as Go-ICP 8, Generalized-ICP 21 and Sparse  ICP22, have been proposed to \nincrease its efficiency or o improve robustness to noise and mismatches. However, the main drawback of these \nmethods is that they require proper initialization to converge to a good  solution23. Another issue of ICP and \nits variants is poor robustness to outliers and partial overlaps that often occur in real-world data. Therefore, \nsome traditional methods register point cloud by matching local shape descriptor and RANSAC algorithm. The \nrepresentative shape descriptor includes  PFH24,  FPFH25,  SHOT26,  RoPs27,  GASD28 etc. Nevertheless, the quality \nof such hand-craft descriptors can be affected by the point density and  outliers29, and heavily rely on low-level \ngeometrical attributes to compute  orientations30.\nLearning-based point cloud registration\nRecently, various deep  learning31–34 approaches have been proposed for registration, such as  PREDATOR16, \nREGTR 35,  PCRNet36, and so on. Learning-based Registration can be summarized into two categories: Feature \nlearning-based methods and End-to-end learning-based methods. Unlike the traditional point cloud registration \nmethods, Feature learning-based methods use deep neural network to learn a robust feature correspondence \nsearch, and then, the transformation matrix is ultimately determined through one-step estimation (e.g. RANSAC) \nwithout any iteration. PREDATOR employs an attention mechanism to extract contextual information for learn-\ning more distinctive feature descriptors and find soft-correspondences from overlap between a pair of point \nclouds. REGTR utilizes self-attention and cross-attention to directly predict a consistent set of final point cor -\nrespondences. All these methods are using deep learning as a feature extraction tool and aim to estimate robust \ncorrespondences by the learned distinctive feature. The End-to-end learning-based methods solve registration \nproblem with an end-to-end neural network. The input of the network is a pair of point clouds, and the output \nis the transformation matrix to align the pair of point clouds. The network not only can extract feature of point \ncloud, but also can estimate transformation. Different from the network of End-to-end learning-based method, \nthe network of feature learning-based method is separate from the transformation estimation and focuses on \nfeature learning. PCRNet uses PointNet to extract global features, and then connects these features together and \nprovides them as input to the MLP network for regression transformation parameters.  DeepGMR37 leverages \na neural network to learn pose-invariant point-to-distribution parameter correspondences. Then, these cor -\nrespondences are fed into the GMM optimization module to estimate the transformation matrix.  DGR38 puts \nforward a 6-dimensional convolutional network architecture for internal likelihood prediction, and estimates \nthe transformation through a weighted Procrustes module.\nProblem definition\nConsider a pair of point clouds P ∈ RNp×3 and Q ∈ RNQ×3 , we denote as source point cloud and target point \ncloud, respectively.NP and NQ denote the number of points in source point cloud P  and target point cloud Q , \nrespectively. The objective of point cloud registration is to estimate an unknown rigid transformation consisting \nof a rotation R ∈ SO(3) and a translation t ∈ R3 , which aligns P to Q.\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nMethodology\nFigure 1 illustrates our overall framework which consists of three main modules: feature extraction and embed-\nding module, SDT module and overlapping correspondence prediction. In feature extraction and embedding \nmodule, it extracts feature of a pair of point cloud by a feature extraction network with shared weight, and \nwe also compute local spatial relationships as local position embeddings between points of point cloud after \ndownsampling. In SDT module, it first receives extracted feature and local position embeddings from feature \nextraction and embedding module, and then iteratively performs deformable self-attention and cross-attention \nwhose purpose is to simulate the process of human browsing back and forth during matching. Deformable self-\nattention aims to make features more expressive for matching by enhancing local geometric feature representation \nof a point cloud, and deformable cross-attention aims to compare the similarity between a pair of point clouds \nby enhancing feature discriminative capability of spatial correspondences. In overlap correspondence predic-\ntion module, we first obtain a similarity matrix by matrix operations on the high- high-dimensional feature \nmap from the previous module, and then we add edge slack block for the similar matrix and use  Sinkhorn39 \nalgorithm to obtain a soft correspondence confidence matrix, and we transform the soft feature correspondence \ninto a one-to-one point correspondence through the utilization of the Hungarian  algorithm40. Finally, RANSAC \nalgorithm is employed to estimate the final transformation relationship between the source point cloud P and \ntarget point cloud Q.\nFeature extraction and local position embedding\nFeature extraction\nTo effectively utilize the input information of the original point cloud, the feature extraction adopts position \nadaptive convolution (PAConv)41 and residual network  ResNet42 for multilevel resolution feature extraction and \nfusion. Unlike general convolutional networks, PAConv builds convolution kernels by dynamically assembling \nbasic weight matrices stored in a weight bank, which can better handle irregular and disordered point cloud data \nand thus improve model registration performance. The backbone network architecture is illustrated in Fig.  2. \nInput point clouds can be expressed as ( NP , 3) and ( NQ , 3), where NP and NQ are the number of points in source \npoint cloud P and target point cloud Q, respectively, and 3 represents the coordinate dimension of each point. \nAn original source/ target point cloud is input to the feature extraction network and passes through multi-layer \nResBlockA and ResBlockB. In ResBlockA consists of a Conv1D convolution layer, a PAConv convolution layer, \na Layer-Norm normalized layer, and a Leaky-ReLU activation layer and a shortcut Conv1D convolution layer. \nIn ResBlockB, if parameter strided is set to true, PAConv will downsample the number of points to 1/4 of the \nnumber of points in upper-level structure, and the maxpool operation must be executed on the shortcut to ensure \nthem be same dimension. We combine residual connections in the feature extraction backbone network and add \nmultilevel resolved feature maps and convolutional results to achieve multilevel feature fusion. The correlation \nbetween FP and FQ of the point cloud P and Q is finally obtained. The dimension of feature map is (  N ′\nP , 1024) \nand ( N′\nQ , 1024) respectively, where N ′\nP is 1/64 of the size of NP and N′\nQ is 1/64 of the size of NQ.\n/tildenosp\n/tildenosp\nFigure 1.  Main framework of our proposed point cloud registration. Feature extraction and embedding \ndownsample the source point cloud P and the target point clouds Q, and learn features in multiple resolution \nlevels and extract local position relationships from these point clouds as their local position embeddings, \nrespectively. Spatial deformable transformer can enhance feature representation by deformable self-attention \nand can compare the similarity between two features by deformable cross-attention. Overlap correspondences \nprediction can estimate correspondences between these point clouds in the overlapping region by Sinkhorn and \nHungarian algorithm.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nLocal position embedding\nThe input of local position embedding (LPE) is the result by downsampling the source point cloud P  and the \ntarget point cloud Q. Based  on43, the spatial position relation within the single point cloud is explicitly calcu-\nlated and is taken as LPE of deformable self-attention in SDT. The spatial position relation of the point clouds \nis shown in Fig. 3.\nIn Fig. 3, p i and p j represent two points within a single point cloud, pkn\ni  representing the n-th nearest neighbor \nof p i and pkn\nj  representing the n-th nearest neighbor of p j . We define a function gD,i,j to describe the distance \nrelationship between two points, and define ρi,j to represent the Euclidean distance between p i and p j , and d i,kt\ni  \nto represent the distance between p i and its n-th.\nnearest neighbor point, and d j,kt\nj  to represent the distance between p j and its n-th nearest neighbor point, and \n1\nn\nn∑\nt=1\nd i,kt\ni  to represent the average distance of p i and its n neighbor points (n is defined as 3 in this paper) , and \n1\nn\nn∑\nt=1\nd j,kt\nj  to represent the average distance of p j and its n neighbor points, and σd to represent a constant used to \ncontrol the sensitivity of distance change, and dt to represent the dimension of the embedding vector. The func-\ntion gD,i,j is defined as follow\nConv1D(64,32)\nPAConv(32,32)\nConv1D(32,128)\nConv1D(64,128)\nPAConv(3,64)\n× 3 × 3\nResBlockA(in, out)\nLeaky-ReLU\nConv1D(128,32)\nPAConv(32,32)\nConv1D(32,128)\nResBlockB(in,out,[strided])\nLeaky-ReLU\nResBlockA(128,256)\nResBlockB(256,256)\nResBlockB(256,256,strided)\nResBlockA(256,512)\nResBlockB(512,512)\nResBlockB(512,512,strided)\nResBlockA(512,1024)\nResBlockB(1024,1024)\n× 1024× 1024\nHFUXR6WHJUD7\n6RXUFH\u0003\n)HDWXUH\n7DUJHW\u0003\n)HDWXUH\nP NQ N\nP NQ N\n’’\nFigure 2.  Feature extraction network structure. The dimension of original source point cloud is (NP,3 ) , and \nthe dimension will turn to (NP, 64) after executing PAConv(3,64), and will turn to (NP/4, 128 ) after executing a \ngroup of ResBlock marked with green color, and will turn to (NP/16, 256 ) after executing a group of ResBlock \nmarked purple color, and turn to (NP/64, 512 ) after executing a group of ResBlock marked pink color, and turn \nto (NP/64, 512 ) after executing a group of ResBlock marked red color which does not downsample the number \nof points since parameter strided in ResBlockB does not set to true.\nFigure 3.  Local Position Relation.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nWe define a function gA,i,j,ik to describe the angle relation between three points, where αit\ni,j denotes the angle \nbetween vector \n⇀\np ip j and \n⇀\np ip kt\ni  , and αjt\ni,j denotes the angle between vector \n⇀\np ip j and \n⇀\np jp kt\nj  , and σa is a constant that \ncontrols the sensitivity to angle change, dt is the dimension of the embedding vector. The function gA,i,j,ik is \ndefined as follow\nFinally, the spatial position relation ˜F between the p i and p j is defined as follow\nwhere gD,i,j is the distance relation between two points, gA,i,j,ik is the angle relation between three points, WD \nand WA are the projection matrices of the distance and angle relations, respectively, and the dimension of ˜F is \n( N′,N′ , 255).\nSpatial deformable transformer\nSDT consists of a deformable self-attention module for enhancing local geometric feature expression capability \nand a deformable cross-attention module for transferring point cloud features whose aim is to compare the simi-\nlarity between a pair of point clouds. It explicitly receives the local position embeddings and the high-dimensional \nfeatures, and performs weighted aggregation of the features. In order to improve the computational efficiency, \nwe change the dimension of feature map extracted from feature extract module from 1024 to 256d by linear \nprojection. In the SDT module, these two deformable attention modules are executed iteratively for n times. We \nconduct extensive experiments and find that setting n to 4 can better and faster aggregate local features of point \nclouds. The outputs of SDT is FP and FQ according to ( FP, ˜FP ) and ( FQ, ˜FQ ) respectively, and their dimensions \nare ( NP′ , 256) and ( NQ′,256) respectively.\n(A) Deformable self-attention module\nThe original attention is used to describe the degree of autocorrelation of input information, and is represented \nby the attention weight matrix which is calculated by the query vector (Query , Q), key vector (Key, K ), value \nvector (Value, V). Usually, V is weighted based on the relative importance of Q and K to obtain the attention \nmatrix which can be expressed as follow\nwhere dk is the dimension of the key vector. Attention is also called self-attention if Q , K and V comes from a \nsame feature ˜F.\nDifferent from self-attention, deformable self-attention 20,44 predicts k  position offsets according to query \nvector Q, and calculates attention score according to Q  and those k  position of K and V. In this paper, we use \ncontinuous position bias (CPB) method proposed in Swin Transformer  V245 to generate spatial deformation \noffset Bi,j which improves the model’s ability to capture local geometric information. Bi,j is defined by the fol-\nlowing formula\nwhere Gi,j is by default a narrow network with one layer of ReLU activation function between two layers of MLP , \nand Bi,j is the relative position offset between the query vector Q at p i and the key vector K at p j.\nIn the following, we describe the computation for ( FP, ˜FP ) and the same goes for ( FQ, ˜FQ ). Deformable self-\nattention performs a grouping  strategy46,47 on the high-dimensional features X ∈ R|Fp |×dt to obtain Q , K and \nV, and perform groups grid  sample48 on local position embeddings to obtain G . By performing respectively \ndot product between Q and K, Q and G, and then adding it to the spatial deformation offset Bi,j , we obtain the \nattention score ei,j in Deformable Attention\nwhere gi,j ∈ R|X |×d t denotes local position embedding between p i and p j,WQ,WK,WG are the projection matrix \nof the Q, K and G respectively, and dt is the dimension of the input vector. Based on the obtained attention scores, \nthe output feature matrix zi of deformable self-attention is the weighted sum of all projected input features\n(1)\n\n\n\ngD,i,j,2k = sin(\n(ρi,j+ 1\nn\n�n\nt=1 d j,kt\nj + 1\nn\n�n\nt=1 d i,kt\ni )/σd\n10000 2k /d t )\ngD,i,j,2k+1 = cos(\n(ρi,j+ 1\nn\n�n\nt=1 d j,kt\nj + 1\nn\n�n\nt=1 d i,kt\ni )/σd\n10000 2k /d t ).\n(2)\n\n\n\ngA,i,j,ik,2x = sin(\n(1\nn\nn�\nt=1\nα it\ni,j+ 1\nn\nn�\nt=1\nα jt\ni,j)/σa\n10000 2x /d t )\ngA,i,j,ik,2x +1 = cos(\n(1\nn\nn�\nt=1\nα it\ni,j+ 1\nn\nn�\nt=1\nα jt\ni,j)/σa\n10000 2x /d t ).\n(3)˜F = gD,i,jW D + gA,i,j,ik W A ,\n(4)Attention(Q,K,V) = softmax( QKT\n√dk\n)V,\n(5)Bi,j(�x , �y ) = Gi,j(�x , �y ),\n(6)ei,j =\n(xiW Q )(xjW K + gi,jW G )T\n√dt\n+ Bi,j,\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nwhere ai,j denotes the weight coefficients computed by a row-wise softmax on the attention score ei,j , and WV \ndenotes the projection matrix of V . Figure 4 shows deformable self-attention module, in which left part is the \nconstruction of deformable self-attention and right part is the computation graph of deformable self-attention.\nThe deformable self-attention module transforms the global feature interactions in traditional self-attention \ninto local feature interactions which adapt to different geometric constructure of point cloud. By the deformable \nself-attention module, the model can adaptively learn the local geometric spatial information within the point \ncloud to enhance the representation of features and hence improve the accuracy of point-to-point matching.\n(B) Deformable cross-attention module\nA typical step in the point cloud registration task is deformable cross-attention module, which is used to exchange \nglobal features between points and then obtain the similarity between a pair of point clouds. Given the deform-\nable self-attention feature matrix fP  and fQ  of the source point cloud P  and target point cloud Q , the feature \ncorrelation of fP  relative to fQ  can be expressed by ei,j\nwhere WQ,WK are the projection matrices of the query vector Q and the key vector K respectively, and dt is the \ndimension of the input vector. Then, deformable cross-attention feature matrix zP ,i of fP  relative to fQ  can be \ndenoted as follow\nin which ai,j is computed by a row-wise softmax on the attention score ei,j , and WV denotes the projection matrix \nof V. Figure 5 shows deformable self-attention module, in which left part is the construction of deformable cross-\nattention and right part is the computation graph of deformable cross-attention. The deformable cross-attention \nfeature matrix of fQ  relative to fP  are computed in the same way, resulting in a more robust and discriminative \nfeature representation after feature interactions.\nOverlap correspondence prediction\nOverlap correspondence prediction module receives the output of SDT FP and FQ , and unifies their dimensions \nas (max(N ′\nP,N′\nQ ), 256) by bilinear  interpolation49. So, cosine similarity matrix S can be defined as follow\n(7)zi =\n|X |∑\nj=1\nai,j(xjW V ),\n(8)ei,j =\n(fP,iW Q )(fQ ,jW K )T\n√dt\n,\n(9)zP,i =\n|Q |∑\nj=1\nai,j(fQ, jW V ),\n(10)Si,j =\nfP\ni · fQ\nj\n⏐⏐fP\ni\n⏐⏐\n⏐⏐⏐fQ\nj\n⏐⏐⏐\n,\nFigure 4.  Deformable Self-Attention Module.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nwhere f P\ni  and fQ\nj  denote respectively a feature in FP and FT\nQ . Before generating cosine similarity matrix S, we can \nnormalize respectively each feature in FP and FT\nQ , so the value of \n⏐⏐f P\ni\n⏐⏐ and \n⏐⏐⏐fQ\nj\n⏐⏐⏐ are both 1, and S also can be defined \nas follow\nWe can obtain initially correspondences between source point cloud P and target point cloud Q from cosine \nsimilarity matrix S following a certain principle, for example f P\ni  and fQ\nj  are a pair of points if the value of S i,j is \ngreater than a certain threshold. However, this approach will make a feature point in FP correspond to multiple \nfeature points in FQ , and will raise a lot of mismatching pairs which can decrease accuracy and robustness of \nregistration. In response to the above issues, based on Dustbin mechanism of  SuperGlue50, we add Edge Slack \nBlock to normalized cosine similarity matrix S , and utilize the Sinkhorn algorithm on S  to compute a soft  \ncorrespondence confident matrix.  Finally, we use Hungarian algorithm on the soft correspondence confident \nmatrix to obtain a hard one-to-one correspondence confident matrix MC . The process of overlap correspond -\nence prediction is shown as Fig. 6.\nLoss\nInspired by  UTOPIC51, we construct a supervised loss function via real correspondences based on the α-balance \ncross-entropy  loss20. The formula of the α-equilibrium cross-entropy loss is defined as follow\nwhere M c is a confidence matrix and denotes point-to-point correspondences, α is the balancing factor that \nresolves the imbalance of correspondences, α ∈[ 0, 1][0, 1] is used on the correct correspondences, and 1 − α is \nused on the incorrect correspondences. Based on the α-balance cross-entropy loss, we add modulation factor \n(1 − MC)γ to obtain the correct corresponding loss Lα\nSimilarly, the incorrect corresponding loss Lα is defined as follow\n(11)Si,j = fP\ni · fQ\nj .\n(12)L(M C) =− α log (M C),\n(13)Lα (M c) =− α(1 − M c)γM gtlog(M c)\nFigure 5.  Deformable Cross-Attention Module.\nSimilar Matrix\nZ N + 1\nEdge\nSlack Block\nM + 1\nSoft correspondence \nconfidence matrix\n1sum\n1sum\nSinkhorn Hungarian\nAlgorithm\nPoint correspondence matrix\n=\n=\nFigure 6.  Process of overlap correspondence prediction.\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nFinally, we obtain the total loss L(M c) of the model as follow\nwhere NP′ and NQ′ represent the number of points after downsampling of source point cloud P and target point \ncloud Q, respectively,Lα (pt) indicating the correct corresponding loss function and Lα (pt) is the incorrect cor-\nresponding loss function. For registration data in 3DMatch and 3DLoMatch scenarios, we set α as 0.25 and set \nγ as 2 according  to52. For registration data in Modelnet40 scenarios, we set α as 0.45 and set γ as 2.5.\nFigure 7 shows curve of loss function for 40 epochs on Modelnet40 and 3DMatch under learning rate 0.0001 \nand decay 0.005. It is clearly note that the loss function continues to decrease for 3DMath and ModelNet40 as \nthe number of epochs increases. Loss function on ModelNet40 converge to 0.5 after 20 epochs training and Loss \nfunction on 3DMatch converges to 0.9 after 16 epochs training.\nExperiments\nExperimental dataset and parameter setting\nWe evaluate SDT via the publicly available 3DMatch and ModelNet40 datasets. 3DMatch dataset contains 62 \nscenarios, in which 46 scenarios are used for training set, 8 scenarios are used for validation set, and 8 scenarios \nare used for testing. ModelNet40 dataset contains 40 CAD models from different classes, with the top 20 classes \nused for training and validating, rest 20 classes for testing. In our experiment, the data with overlap between 10 \nand 30% on 3DMatch are used as 3DLoMatch, and the average overlap is below 53.6% on ModelNet40 are used \nas ModelLoNet40, and these data are used to test the effect of our proposed method in the low overlap scenario. \nDuring training, the AdamW optimizer is used with the initial learning rate set to 0.0001 and learning rate decay \nis used to reduce learning rate to ensure better model convergence after 40 epochs. The model was trained and \ntested via PyTorch framework on a server equipped with an NVIDIA GeForce RTX 3090 GPU.\nModel evaluation metrics\nWe evaluate our method on 3DMatch and ModelNet40 datasets, and generalize directly the training model to \nlow-overlap 3DLoMatch and ModelNet40 datasets, and compare the indicators of our proposed method with \nstate-of-the-art registration methods. Three metrics  from12 are used to evaluate the performance of our proposed \nmethod on 3DMatch dataset: (1) FMR (Features Matching Recall), the fraction of point cloud pairs whose inlier \nratio exceeds a certain threshold; (2) IR (Inlier Ratio), the fraction of estimated correspondences whose residuals \nare below a certain threshold under the ground-truth transformation; (3) RR (Registration Recall), the fraction \nof point cloud pairs whose transformation error is smaller than a certain threshold. We evaluate the performance \nof our proposed method on ModelNet40 dataset by (1) RRE (Relative Rotation Error), the geodesic distance \nbetween estimated and ground-truth rotation matrices; (2) RRE (Relative Translation Error), the geodesic dis -\ntance between estimated and ground-truth translation vectors; (3) CD (Chamfer Distance), a sum of positive \ndistances between a pair of aligned point clouds.\nComparison of the experiments\nTable 1 estimates the performance of SDT and state-of-the-art registration methods, such as FCGF , D3Feat Preda-\ntor, CoFiNet 53. It is obvious from Table 1 that (1) On 3DMatch dataset, the FMR of our SDT is only slightly lower \n(14)Lα(M c) =− (1 − α)M γ\nc (1 − M gt) log(1 − M c).\n(15)L(M c) =\nN P ′∑\ni=1\nM Q ′∑\nj=1\n(Lα (M c) + Lα (M c)),\nFigure 7.  Curve of loss function.\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nthan CoFiNet, the IR of our SDT is lower than Predator, and the RR of our SDF outperforms all other methods; \n(2) On 3DLoMatch dataset, all metrics of our SDT outperform other methods, FMR of our SDT has a 3.7% higher \nthan that of CoFiNet, IR of our SDT has a 2.2% higher than that of Predator, and RR of our SDT has a 3.9% higher \nthan that of CoFiNet. All these experimental results show that our SDT can effectively register point clouds and \nis more robust and accurate to register point clouds with low-overlap. The registration graphs of our SDT and \nPredator on the 3DMatch and 3DLoMatch datasets are shown in Fig.  8, where diagrams in 1st and 2nd rows \ndisplay some raw data from 3DMatch with 48.3% and 73.4% overlap and their registration results, and diagrams \nTable 1.  Comparison of FMR, IR and RR (%) of different methods. Significant values are in bold.\nMethod\nFMR IR RR\n3DMatch 3DLoMatch 3DMatch 3DLoMatch 3DMatch 3DLoMatch\nFCGF 97.4 75.9 56.9 22.0 87.3 41.7\nD3Feat 95.8 67.4 40.7 15.5 84.9 46.9\nPredator 96.6 77.9 73.8 37.8 90.6 62.4\nCoFiNet 98.1 83.1 52.2 26.9 89.3 67.5\nSDT (ours) 97.5 86.8 67.3 40.0 91.0 71.4\nFigure 8.  Registration Results on 3DMathc and 3DLoMatch.\nTable 2.  Comparison of RR (%) of different methods on 3DMatch. Significant values are in bold.\nMethod\n3DMatch(RR)\nKitchen Home_1 Home_2 Hotel_1 Hotel_2 Hotel_3 Study Lab Mean\nFCGF 98.0 94.3 68.6 96.7 91.0 84.6 76.1 71.1 85.1\nD3Feat 96.0 86.8 67.3 90.7 88.5 80.8 78.2 64.4 81.6\nPredator 97.6 97.2 74.8 98.9 96.2 88.5 85.9 73.3 89.0\nCoFiNet 96.2 99.1 73.2 95.8 91.2 84.6 89.9 84.4 89.3\nSDT (ours) 97.6 96.8 81.2 98.4 89.1 89.1 90.7 87.3 91.3\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nin 3rd and 4th rows display some raw data from 3DLoMatch with 29.0% and 21.5% and their registration results. \nIt is obvious that our SDT can distinguish similar objects at different positions (see the comparison of Predator \nand SDT in the 3rd and 4th columns), and recognize small overlapping regions in complex environment thanks \nto local significant features obtained from the deformable self-attention and cross-attention.\nDue to structural differences for 8 test scenarios of 3DMatch and 3DLoMatch, the features obtained by differ-\nent method are also extremely different for these different scenarios. Tables 2 and 3 show the comparison results \nof RR between different methods on 8 test scenarios of 3DMatch and 3DLoMatch, respectively. Experimental \nresults show that our SDT outperforms other methods. In detail, on 3DMatch, RR of our SDT outperforms \nmost scenarios, especially the hard scenarios such as Home_2 and Lab, and our SDT has a most mean RR; on \n3DLoMatch, RR of our SDT is only lower that of CoFinet on Home_1 and Study, and has a most mean RR. These \nexperimental results further show that our SDT together with overlap correspondences prediction is not only \nrobust, but also accurate registration.\nIn order to verify the robustness of our SDT at different sample points, the number of sampling points pro-\nvided to network is gradually reduced in our experiment and RR of different methods are shown on Table 4. In \nall other cases, only when sample points on 3DModel are 1000, RR of Predator is slightly higher than our SDT, \nand whatever the number of sampling points on 3DLoMatch is, RR of our SDT outperforms all other methods. \nAt the same time, the experimental results also show that our SDT is relatively robust to the number of different \nsampling points, even when the number of sample points is only 250.\nTo further verify the generalization ability of our proposed method, we use first 20 categories of ModelNet40 \ndataset to train model and perform model test on left 20 unseen categories of ModelNet40 via trained model. \nTable 5 shows RRE, RTE, CD of our SDT and other methods on unseen categories. It is clearly shown that the \nperformance of our SDT is as good as that of REGTR, and is better than that of DCP-v2 54, RPM-Net 55, and \nPredator. Experimental results also show that our SDT has strong generalization ability and better registration in \nlow overlap scenarios. The registration graphs of our SDT and Predator on the ModelNet40 and ModelLoNet40 \nare shown in Fig.  9, where diagrams in 1st and 2nd rows display some raw data from ModelNet40 and their \nregistration results, and diagrams in 3rd and 4th rows display some raw data from ModelLoNet40 and their \nTable 3.  Comparison of RR (%) of different method on 3DLoMatch. Significant values are in bold.\nMethod\n3DLoMatch (RR)\nKitchen Home_1 Home_2 Hotel_1 Hotel_2 Hotel_3 Study Lab Mean\nFCGF 60.8 42.2 53.6 53.1 38.0 26.8 16.1 30.4 40.1\nD3Feat 49.7 37.2 47.3 47.8 36.5 31.7 15.7 31.9 37.2\nPredator 71.5 58.2 60.8 77.5 64.2 61.0 45.8 39.1 59.8\nCoFiNet 74.1 67.5 64.4 81.7 65.5 63.1 54.8 68.1 67.4\nSDT (ours) 85.5 64.0 71.6 87.6 71.7 66.7 54.0 70.0 71.4\nTable 4.  Comparison of RR (%) of different algorithms at different corresponding sampling points. Significant \nvalues are in bold.\nMethod\n3DMatch (RR) 3DLoMatch(RR)\n5000 2500 1000 500 250 Mean 5000 2500 1000 500 250 Mean\nFCGF 85.1 84.7 83.3 81.6 71.4 81.2 40.1 41.7 38.2 35.4 26.8 36.4\nD3Feat 81.6 84.5 83.4 82.4 77.9 82.0 37.2 42.7 46.9 43.8 39.1 42.0\nPredator 89.0 89.9 90.6 88.5 86.6 88.9 59.8 61.2 62.4 60.8 58.1 60.5\nCoFiNet 89.3 88.8 88.7 87.8 87.0 88.3 67.3 66.9 64.5 63.1 62.0 64.8\nSDT (ours) 91.0 90.4 90.3 90.5 90.1 90.5 71.3 71.0 70.7 71.4 70.1 70.9\nTable 5.  Point cloud registration experiment with unknown object category. Significant values are in bold.\nMethod\nModelNet40 ModelLoNet40\nRRE RTE CD RRE RTE CD\nDCP-v2 11.975 0.171 0.0117 16.501 0.300 0.0268\nRPM-Net 1.712 0.018 0.00085 7.342 0.124 0.0050\nPredator 1.739 0.019 0.00089 5.235 0.132 0.0083\nREGTR 1.476 0.014 0.00079 3.934 0.088 0.0038\nSDT (ours) 1.614 0.013 0.00085 3.915 0.078 0.0041\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nregistration results. It is obvious that our SDT outperform Predator (see the comparison of Predator and SDT in \nthe 3rd and 4th row) on ModelLoNet40, which thanks to local significant features obtained from the deformable \nself-attention and cross-attention.\nAblations experiments\nWe perform ablation experiments on 3DMatch dataset to explore the impact of different components of our SDT \non the registration results. Specifically, we test the effect of our network at three different modules.\n(1) DGCNN / no Sinkhorn. We use DGCNN to replace our FE in the process of feature extract and embedding, \nand Sinkhorn algorithm is removed from the overlap correspondences predict module.\n(2) DGCNN / Sinkhorn. We use DGCNN to replace our FE in the process of feature extract and embedding, \nand Sinkhorn algorithm is added to the overlap correspondences predict module.\n(3) Graph neural network / no Sinkhorn. In SDT module, graph neural network is used to replace SDT module, \nand Sinkhorn algorithm is removed from the overlap correspondences predict module.\n(4) Graph neural network / Sinkhorn. In SDT module, graph neural network is used to replace SDT module, \nand Sinkhorn algorithm is added to the overlap correspondences predict module.\n(5) Self/cross attention based/no-Sinkhorn. In SDT module, the original self-attention is used to replace the \ndeformable self-attention in the SDT module, and the original cross-attention is used to replace the deform-\nable cross-attention in the SDT module, and Sinkhorn algorithm is removed from the overlap correspond-\nences predict module.\n(6) Self/cross attention-Based/ Sinkhorn. In SDT module, the original self-attention is used to replace the \ndeformable self-attention in the SDT module, and the original cross-attention is used to replace the deform-\nable cross-attention in the SDT module, and Sinkhorn algorithm is added to the overlap correspondences \npredict module.\n(7) Deform-self-Attention-Base/no-Sinkhorn. Sinkhorn algorithm is added to the overlap correspondences \npredict module.\n(8) Ours model. Deformable self-Attention is added to the SDT module, and Sinkhorn algorithm is added to \nthe overlap correspondences predict module.\nFigure 9.  Registration Result on ModelNet40 and ModelLoNet40.\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nThe results in Table 6 demonstrate that our SDT is essential for solving rigid transformations in low overlap \nscenarios. Among them, our proposed SDT has the best registration performance, whose FMR and RR has \nraised more 6% on 3DLoMatch. Meanwhile, the results in Table  6 also demonstrate that deformable attention \nand attention will be greater to improve FMR, IR and RR in point cloud registration comparing to graph neural \nnetwork and DGCNN. Finally, it is worth noting that the addition of Sinkhorn algorithm in the DGCNN, graph \nneural network, self/cross attention and deformable self/cross attention can improve the registration effect.\nFollowing that, we investigate the design of geometric structure embedding from the following aspects.\n1) Number of neighbor points.We change the number of nearest neighbors to compute the triplet-wise distance/\nangular embedding of p i or p j in Fig. 3.\n2) σd . It represent a constant used to control the sensitivity of distance change in formula (1 ) and we change \nthe hyper-parameter to test its impact on registration performance.\n3) σa . It represent a constant used to control the sensitivity of angular change in formula (2) and we change the \nhyper-parameter to test its impact on registration performance.\nThe results in Table 7 demonstrate that the impact of hyper-parameters on registration performance. Accord-\ning to Table 7, it is clear that the model with both distance and angular embeddings outperforms the model \nwith simply distance embedding by a significant margin, which aligns with our goal. Increasing the number of \nneighbors will increases registration performance by providing exact structural information when the number \nof neighbor points is less than or equal to 3 , but registration performance whill decrease when the number of \nneighbor points is greater than 3, which shows the geometric structure formed by a reference point and its closest \nthree neighbor points is the most robust to noise and the highest invariant to rigid transformation. Meanwhile, \nit is noted from Table 7 that the best results are obtained around 0.2 for σd and 10°for σa . A too small (where the \nembedding is too sensitive to distance changes) or too large (where the embedding neglects small distance vari-\nations) σd could harm the performance, but the differences are not significant. And similar observations can be \nTable 6.  Comparison of FMR, IR and RR on network model. Significant values are in bold.\nModel\nFMR IR RR\n3DMatch 3DLoMatch 3DMatch 3DLoMatch 3DMatch 3DLoMatch\nDGCNN/no Sinkhorn 93.2 80.7 47.5 25.2 82.1 64.2\nDGCNN/Sinkhorn 94.0 81.6 54.7 30.3 84.3 67.1\nGraph neural network/no Sinkhorn 95.1 82.1 48.2 26.6 84.3 64.7\nGraph neural network/Sinkhorn 95.9 82.9 54.5 29.2 87.5 67.8\nSelf/Cross attention based/no Sinkhorn 96.1 83.3 48.8 30.5 86.7 66.9\nSelf/Cross attention based/Sinkhorn 97.0 83.8 64.3 35.7 88.7 68.2\nDeformable self/cross attention Based/no Sinkhorn 97.2 85.9 65.1 37.1 90.4 70.1\nSDT (ours) 97.5 86.8 67.3 40.0 91.0 71.4\nTable 7.  Comparison of FMR, IR and RR on hyper-parameters. Significant values are in bold.\nModel FMR IR RR\nName Value 3DMatch 3DLoMatch 3DMatch 3DLoMatch 3DMatch 3DLoMatch\nNumber of neighbor points\n0 92.0 82.1 63.1 35.3 84.7 66.1\n1 95.9 84.1 62.6 33.7 87.0 68.5\n3 97.0 83.8 64.3 35.7 88.7 68.2\n5 95.9 84.2 63.4 33.5 88.3 67.5\n7 96.1 83.8 60.0 33.8 85.8 66.5\nσd\n0.1 95.6 83.7 61.4 33.8 90.7 69.2\n0.2 96.4 83.8 63.0 37.4 91.8 69.5\n0.3 97.5 86.8 67.3 40.0 91.0 71.4\n0.4 96.1 87.0 65.3 36.0 87.8 70.0\n0.5 95.8 86.3 63.3 34.1 85.0 70.2\nσa\n5° 95.9 80.4 66.9 39.4 90.3 70.6\n10° 97.5 86.8 67.3 40.0 91.0 71.4\n15° 96.0 86.4 68.7 39.6 91.8 73.2\n20° 95.9 86.0 67.3 39.0 91.4 70.6\n25° 96.1 85.7 66.6 39.3 91.2 70.1\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\nobtained for the angular changes σa . Nevertheless, all of these models outperforms pervious methods by a large \nmargin, indicating that our proposed SDT is still robust to the distance/angular hyper-parameters.\nEfficiency\nWe compare the inference time of several methods on a desktop computer equipped with an Intel I7-12700 \nCPU, an Nvidia GTX 3060 GPU, and 32G memory. Computational time is measured in seconds and calculated \nby averaging 100 results. As shown in Table 8, FCGF is the fastest method among these methods, and RPM-net, \nD3Feat are also faster than our proposed SDT, which is because their network is relatively simple and none of \nthem adopt Transformer structure. Our proposed SDT is faster than DCP-v2, Predator, REGTR and CoFiNet, in \nwhich the former leverages deformable self/cross attention and the latter utilize Transformer structure with self/\ncross attention. From the results of the Table 8, it is clearly showed that Transformer with deformable self/cross \nattention (such as our proposed SDT) has a higher time efficiency than Transformer with self/cross attention.\nConclusion\nWe propose a 3D point cloud registration method based on SDT. First, we propose a feature extraction and \nembedding module to extract basic features of point cloud and compute local spatial relationships between \npoints in the point cloud as local positional embedding of basic feature, and formulate a SDT module to fuse \nand enhance above two kinds of information into new feature of point cloud by Self-Attention and Cross-\nAttention mechanisms. Second, we develop an overlap correspondence predict module to obtain correspond -\nences between the pairwise point clouds by a series of handle for above new features of a pair of point clouds. \nFinally, we construct an α-balance cross-entropy loss based on real correspondences of pairs of point clouds \nto train our unsupervised network, and we use outputs of this network to generate transformation matrices of \npairs of point clouds via RANSAC algorithm. Extensive experimental results on the 3DMatch/3DLoMatch and \nModelNet40/ModelLoNet40 demonstrate that our proposed method has high accuracy and strong robustness \nin solving point cloud registration problems in low overlap scenarios. Unfortunately, this leads to longer train-\ning and calculation times because the model uses the SDT module and the RANSAC approach simultaneously. \nImproving model performance, developing more efficient feature extraction and aggregation techniques, and \nextending our approach to more complicated scenarios need continue to study in the future.\nEthical and informed consent\nData used in our study are publicly available, and ethical approval and informed consent were obtained in each \noriginal study.\nData availability\nThe datasets generated during and/or analyzed during our study are available from the corresponding author \non reasonable request.\nReceived: 16 December 2023; Accepted: 4 March 2024\nReferences\n 1. Rodriguez-Lozano, F . J. et al. 3D reconstruction system and multiobject local tracking algorithm designed for billiards. Appl. Intell. \n53, 21543–21575. https:// doi. org/ 10. 1007/ s10489- 023- 04542-3 (2023).\n 2. Sun, C. et al. Research on point cloud hole filling and 3D reconstruction in reflective area. Sci. Rep. 13, 18524 (2023).\n 3. Liu, J. et al. Online object-level SLAM with dual bundle adjustment. Appl. Intell. 53, 25092–25105 (2023).\n 4. Zhu, J., Li, H. & Zhang, T. Camera, LiDAR, and IMU based multi-sensor fusion SLAM: A survey. Tsinghua Sci. Technol. 29(2), \n415–429 (2024).\n 5. Tao, C. et al. 3D object detection algorithm based on multi-sensor segmental fusion of frustum association for autonomous driving  \n(Springer, 2023).\n 6. He, X. et al. Fear-neuro-inspired reinforcement learning for safe autonomous driving. IEEE Trans. Pattern Anal. Mach. Intell. 46(1), \n267–279 (2024).\n 7. Besl, P . J. & McKay, N. D. Method for registration of 3-D shapes[C]//Sensor fusion IV: control paradigms and data structures. Spie \n1611, 586–606 (1992).\nTable 8.  Inference time (in seconds). Significant values are in bold.\nModel ModelNet 3DMatch\nFCGF 0.16 0.17\nRPM-Net 0.19 0.22\nD3Feat 0.21 0.28\nDCP-v2 0.74 0.85\nPredator 0.30 0.38\nREGTR 0.42 0.51\nCoFiNet 0.83 0.94\nSDT (ours) 0.24 0.29\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\n 8. Y ang J, Li H, Jia Y . Go-ICP: Solving 3d registration efficiently and globally optimally[C]//Proc. of the IEEE International Confer-\nence on Computer Vision 1457–1464 (2013).\n 9. Choy C, Park J, Koltun V . Fully convolutional geometric features[C]//Proc. of the IEEE/CVF International Conference on Computer \nVision 8958–8966 (2019).\n 10. Zhang, Z., Liu, Q. & Wang, Y . Road extraction by deep residual u-net. IEEE Geosci. Remote Sens. Lett. 15(5), 749–753 (2018).\n 11. Mateus, A. et al. Fast and accurate 3D registration from line intersection constraints. Int. J. Comput. Vis. 131, 2044–2069 (2023).\n 12. Ao S, Hu Q, Y ang B, et al. Spinnet: Learning a general surface descriptor for 3d point cloud registration[C]// Proc. of the IEEE/\nCVF Conference on Computer Vision and Pattern Recognition 11753–11762 (2021).\n 13. X. Bai, Z. Luo, L. Zhou, et al. D3feat: Joint learning of dense detection and description of 3d local features[C]// Proc. CVPR, 2020: \n6359–6367.\n 14. Thomas H, Qi C R, Deschaud J E, et al. Kpconv: Flexible and deformable convolution for point clouds[C]//Proc. of the IEEE/CVF \nInternational Conference on Computer Vision 6411–6420 (2019).\n 15. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need [J]. Advances in neural information processing systems, 2017, 30.\n 16. Huang S, Gojcic Z, Usvyatsov M, et al. Predator: Registration of 3d point clouds with low overlap[C]//Proceedings of the IEEE/\nCVF Conference on Computer Vision and Pattern Recognition. 2021: 4267–4276.\n 17. Li Y , Harada T. Lepard: Learning partial point cloud matching in rigid and deformable scenes[C]//Proc. of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition 5554–5564 (2022).\n 18. Fischler, M. A. & Bolles, R. C. Random sample consensus: a paradigm for model fitting with applications to image analysis and \nautomated cartography [J]. Commun. ACM 24(6), 381–395 (1981).\n 19. Zhu, X., Su, W ., Lu, L., Li, B., Wang, X., & Dai, J. (2021). DEFORMABLE DETR: DEFORMABLE TRANSFORMERS FOR END-\nTO-END OBJECT DETECTION.ICLR 2021 - 9th International Conference on Learning Representations.\n 20. Xia Z, Pan X, Song S, et al. Vision transformer with deformable attention[C]//Proc. of the IEEE/CVF conference on computer \nvision and pattern recognition 4794–4803 (2022).\n 21. Segal, A., Haehnel, D. & Thrun, S. Generalized-icp[C]//Robotics: Science and Systems (Seattle, 2009).\n 22. Bouaziz, S., Tagliasacchi, A. & Pauly, M. Sparse iterative closest point[C]//computer graphics forum. Oxford UK Blackwell Publ. \nLtd 32(5), 113–123 (2013).\n 23. Pomerleau, F ., Colas, F . & Siegwart, R. A review of point cloud registration algorithms for mobile robotics. Found. Trends® Robot. \n4(1), 1–104 (2015).\n 24. Rusu, R B, Blodow, N, Marton, Z C, et al. Aligning point cloud views using persistent feature histograms[C]// IEEE/RSJ interna-\ntional conference on intelligent robots and systems IEEE 3384–3391 (2008).\n 25. Rusu R B, Blodow N, Beetz M. Fast point feature histograms (FPFH) for 3D registration[C]//2009 IEEE international conference \non robotics and automation. IEEE 3212–3217 (2009).\n 26. Salti, S., Tombari, F . & Di Stefano, L. SHOT: Unique signatures of histograms for surface and texture description [J]. Comput. \nVision Imag. Underst. 125, 251–264 (2014).\n 27. Guo Y , Sohel F A, Bennamoun M, et al. RoPS: A local feature descriptor for 3D rigid objects based on rotational projection \nstatistics[C]//2013 1st International Conference on Communications, Signal Processing, and their Applications (ICCSPA) IEEE \n1–6 (2013).\n 28. do Monte Lima J P S, Teichrieb V . An efficient global point cloud descriptor for object recognition and pose estimation[C]//2016 \n29th SIBGRAPI conference on graphics, patterns and images (SIBGRAPI). IEEE 56–63 (2016).\n 29. Deng Z, Y ao Y , Deng B, et al. A robust loss for point cloud registration[C]//Proceedings of the IEEE/CVF International Conference \non Computer Vision 6138–6147 (2021).\n 30. Ao S, Hu Q, Wang H, et al. BUFFER: Balancing Accuracy, Efficiency, and Generalizability in Point Cloud Registration[C]//Pro -\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 1255–1264 (2023).\n 31. MR-DCAE: Manifold regularization-based deep convolutional autoencoder for unauthorized broadcasting identification, Inter -\nnational Journal of Intelligent Systems vol. 36, no. 12, pp. 7204–7238 (2021).\n 32. Fine-grained modulation classification using multi-scale radio transformer with dual-channel representation, IEEE Communica-\ntions Letters vol 26 no. 6, pp 1298–1302 (2022).\n 33. Application of wavelet-packet transform driven deep learning method in PM2. 5 concentration prediction: A case study of Qingdao, \nChina Sustainable Cities and Society 92 104486 (2023).\n 34. DL-PR: Generalized automatic modulation classification method based on deep learning with priori regularization, Engineering \nApplications of Artificial Intelligence 122 106082 (2023).\n 35. Y ew Z J, Lee G H. Regtr: End-to-end point cloud correspondences with transformers[C]//Proc. of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition 6677–6686 (2022).\n 36. Sarode V , Li X, Goforth H, et al. Pcrnet: Point cloud registration network using pointnet encoding[J]. arXiv preprint arXiv: 1908. \n07906, 2019.\n 37. Yuan, W . et al. Deepgmr: Learning latent gaussian mixture models for registration[C]//. In Computer Vision–ECCV 2020: 16th \nEuropean Conference, Glasgow, UK, August 23–28, 2020, Part V 16 (eds Yuan, W . et al.) 733–750 (Springer International Publish-\ning, 2020).\n 38. Choy C, Dong W , Koltun V . Deep global registration[C]//Proc. of the IEEE/CVF conference on computer vision and pattern \nrecognition 2514–2523 (2020)\n 39. Sinkhorn, R. & Knopp, P . Concerning nonnegative matrices and doubly stochastic matrices [J]. Pac. J. Math. 21(2), 343–348 (1967).\n 40. Kuhn, H. W . The Hungarian method for the assignment problem[J]. Nav. Res. Logist. Q. 2(1–2), 83–97 (1955).\n 41. Xu M, Ding R, Zhao H, et al. Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds[C]//Proc. \nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition 3173–3182 (2021).\n 42. He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proc. of the IEEE conference on Computer Vision \nand Pattern Recognition 770–778 (2016).\n 43. Qin Z, Yu H, Wang C, et al. Geometric transformer for fast and robust point cloud registration[C]//Proc. of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition 11143–11152 (2022).\n 44. Zhu X, Su W , Lu L, et al. Deformable detr: Deformable transformers for end-to-end object detection [J]. Preprint @ http:// arXiv. \norg/ 2010. 04159 (2020).\n 45. Liu Z, Hu H, Lin Y , et al. Swin transformer v2: Scaling up capacity and resolution[C]//Proc. of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition 12009–12019 (2022).\n 46. Li J, Lee G H. Usip: Unsupervised stable interest point detection from 3d point clouds[C]//Proc. of the IEEE/CVF International \nConference on Computer Vision 361–370 (2019).\n 47. Li J, Chen B M, Lee G H. So-net: Self-organizing network for point cloud analysis[C]//Proc. of the IEEE Conference on Computer \nVision and Pattern Recognition 9397–9406 (2018).\n 48. Y ang Y , Feng C, Shen Y , et al. Foldingnet: Point cloud auto-encoder via deep grid deformation[C]//Proc. of the IEEE Conference \non Computer Vision and Pattern Recognition 206–215 (2018).\n 49. Wang, N. The use of bilinear interpolation filter to remove image noise[C]//journal of physics: Conference series. IOP Publ. 2303(1), \n012089 (2022).\n15\nVol.:(0123456789)Scientific Reports |         (2024) 14:5560  | https://doi.org/10.1038/s41598-024-56217-9\nwww.nature.com/scientificreports/\n 50. Sarlin P E, DeTone D, Malisiewicz T, et al. Superglue: Learning feature matching with graph neural networks[C]//Proc. of the \nIEEE/CVF conference on computer vision and pattern recognition 4938–4947 (2020).\n 51. Zhilei, C. et al. UTOPIC: Uncertainty-aware overlap prediction network for partial point cloud registration [J]. Comput. Gr. Forum \n41(7), 87–98 (2023).\n 52. Lin T Y , Goyal P , Girshick R, et al. Focal loss for dense object detection[C]//Proc. of the IEEE International Conference on Com-\nputer Vision 2980–2988 (2017).\n 53. Yu, H. et al. Cofinet: Reliable coarse-to-fine correspondences for robust pointcloud registration [J]. Adv. Neural Inf. Process. Syst. \n34, 23872–23884 (2021).\n 54. Wang Y , Solomon J M. Deep closest point: Learning representations for point cloud registration[C]//Proc. of the IEEE/CVF \ninternational conference on computer vision 3523–3532 (2019).\n 55. Y ew, Z J, Lee, G H. Rpm-net: Robust point matching using learned features[C]//Proc. of the IEEE/CVF Conference onComputer \nVision and Pattern Recognition 11824–11833 (2020).\nAcknowledgements\nThe authors thank the anonymous reviewers for their detailed comments and suggestions, which resulted in the \nimprovement of this paper. This work was supported in part by Supported by National Natural Science Founda-\ntion of China under Grant 62272426, and in part by Shanxi Province Science and Technology Major Special \nPlan \"Unveiling and Leading\" Project under Grant 202201150401021, and in part by Shanxi Provincial Natural \nScience Foundation under Grant 202203021212138.\nAuthor contributions\nXiong Fengguang: Methodology Providing and Implementing, Original Draft Writing. Kong Yu: Manuscript \nReviewing and Reediting, Xie Shuaikang: Experimentation Testing. Liqun Kuang: Manuscript Reviewing and \nFunding acquisition. Xie Han: Funding acquisition and Resources.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to F .X.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}