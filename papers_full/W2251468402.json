{
  "title": "Investigating Continuous Space Language Models for Machine Translation Quality Estimation",
  "url": "https://openalex.org/W2251468402",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A5113272498",
      "name": "Kashif Shah",
      "affiliations": [
        null,
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A5071411952",
      "name": "Raymond W. M. Ng",
      "affiliations": [
        null,
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A5079465565",
      "name": "Fethi Bougares",
      "affiliations": [
        null,
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A5053217291",
      "name": "Lucia Specia",
      "affiliations": [
        null,
        "University of Sheffield"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2270190199",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2164984707",
    "https://openalex.org/W2093790824",
    "https://openalex.org/W2087735403",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2257408573",
    "https://openalex.org/W2251222643",
    "https://openalex.org/W2437096199",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2887933277",
    "https://openalex.org/W2104511424",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W3044695148",
    "https://openalex.org/W4298302210",
    "https://openalex.org/W2115990601",
    "https://openalex.org/W1975742580",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W2164479068",
    "https://openalex.org/W2254518567"
  ],
  "abstract": "We present novel features designed with a deep neural network for Machine Trans-lation (MT) Quality Estimation (QE). The features are learned with a Continuous Space Language Model to estimate the probabilities of the source and target seg-ments. These new features, along with standard MT system-independent features, are benchmarked on a series of datasets with various quality labels, including post-editing effort, human translation edit rate, post-editing time and METEOR. Results show significant improvements in predic-tion over the baseline, as well as over sys-tems trained on state of the art feature sets for all datasets. More notably, the addition of the newly proposed features improves over the best QE systems in WMT12 and WMT14 by a significant margin. 1",
  "full_text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1073–1078,\nLisbon, Portugal, 17-21 September 2015.c⃝2015 Association for Computational Linguistics.\nInvestigating Continuous Space Language Models for Machine\nTranslation Quality Estimation\nKashif Shah§, Raymond W. M. Ng§, Fethi Bougares†, Lucia Specia§\n§Department of Computer Science, University of Shefﬁeld, UK\n{kashif.shah, wm.ng, l.specia}@sheffield.ac.uk\n†LIUM, University of Le Mans, France\nfethi.bougares@lium.univ-lemans.fr\nAbstract\nWe present novel features designed with a\ndeep neural network for Machine Trans-\nlation (MT) Quality Estimation (QE). The\nfeatures are learned with a Continuous\nSpace Language Model to estimate the\nprobabilities of the source and target seg-\nments. These new features, along with\nstandard MT system-independent features,\nare benchmarked on a series of datasets\nwith various quality labels, including post-\nediting effort, human translation edit rate,\npost-editing time and METEOR. Results\nshow signiﬁcant improvements in predic-\ntion over the baseline, as well as over sys-\ntems trained on state of the art feature sets\nfor all datasets. More notably, the addition\nof the newly proposed features improves\nover the best QE systems in WMT12 and\nWMT14 by a signiﬁcant margin.\n1 Introduction\nQuality Estimation (QE) is concerned with pre-\ndicting the quality of Machine Translation (MT)\noutput without reference translations. QE is ad-\ndressed with various features indicating ﬂuency,\nadequacy and complexity of the translation pair.\nThese features are used by a machine learning al-\ngorithm along with quality labels given by humans\nto learn models to predict the quality of unseen\ntranslations.\nA variety of features play a key role in QE.\nA wide range of features from source segments\nand their translated segments, extracted with the\nhelp of external resources and tools, have been\nproposed. These go from simple, language-\nindependent features, to advanced, linguistically\nmotivated features. They include features that\nsummarise how the MT systems generate transla-\ntions, as well as features that are oblivious to the\nsystems. The majority of the features in the lit-\nerature are extracted from each sentence pair in\nisolation, ignoring the context of the text. QE\nperformance usually differs depending on the lan-\nguage pair, the speciﬁc quality score being opti-\nmised (e.g., post-editing time vs translation ad-\nequacy) and the feature set. Features based on\nn-gram language models, despite their simplicity,\nare among those with the best performance in most\nQE tasks (Shah et al., 2013b). However, they may\nnot generalise well due to the underlying discrete\nnature of words in n-gram modelling.\nContinuous Space Language Models (CSLM),\non the other hand, have shown their potential\nto capture long distance dependencies among\nwords (Schwenk, 2012; Mikolov et al., 2013). The\nassumption of these models is that semantically or\ngrammatically related words are mapped to simi-\nlar geometric locations in a high-dimensional con-\ntinuous space. The probability distribution is thus\nmuch smoother and therefore the model has a bet-\nter generalisation power on unseen events. The\nrepresentations are learned in a continuous space\nto estimate the probabilities using neural networks\nwith single (called shallow networks) or multi-\nple (called deep networks) hidden layers. Deep\nneural networks have been shown to perform bet-\nter than shallow ones due to their capability to\nlearn higher-level, abstract representations of the\ninput (Arisoy et al., 2012). In this paper, we ex-\nplore the potential of these models in context of\nQE for MT. We obtain more robust features with\nCSLM and improve the overall prediction power\nfor translation quality.\nThe paper is organised as follows: In Section\n2 we brieﬂy present the related work. Section 3\ndescribes the CSLM model training and its vari-\nous settings. In Section 4 we propose the use of\nCSLM features for QE. In Section 5 we present\nour experiments along with their results.\n2 Related Work\nFor a detailed overview of various features and\nalgorithms for QE, we refer the reader to the\n1073\nWMT12-14 shared tasks on QE (Callison-Burch\net al., 2012; Bojar et al., 2013; Ling et al., 2014).\nMost of the research work lies on deciding which\naspects of quality are more relevant for a given\ntask and designing feature extractors for them.\nWhile simple features such as counts of tokens\nand language model scores can be easily extracted,\nfeature engineering for more advanced and useful\ninformation can be quite labour-intensive.\nSince their introduction in (Bengio et al.,\n2003), neural network language models have\nbeen successfully exploited in many speech and\nlanguage processing problems, including auto-\nmatic speech recognition (Schwenk and Gau-\nvain, 2005; Schwenk, 2007) and machine trans-\nlation (Schwenk, 2012).\nRecently, (Banchs et al., 2015) used a Latent\nSemantic Indexing approach to model sentences\nas bag-of-words in a continuous space to measure\ncross language adequacy. (Tan et al., 2015) pro-\nposed to train models with deep regression for ma-\nchine translation evaluation in a task to measure\nsemantic similarity between sentences. They re-\nported positive results on simple features; larger\nfeature sets did not improve these results.\nIn this paper, we propose to estimate the prob-\nabilities of source and target segments with con-\ntinuous space language models based on a deep\narchitecture and to use these estimated probabili-\nties as features along with standard feature sets in\na supervised learning framework. To the best of\nour knowledge, such approach has not been stud-\nied before in the context of QE for MT. The result\nshows signiﬁcant improvements in many predic-\ntion tasks, despite its simplicity. Monolingual data\nfor source and target language is the only resource\nrequired to extract these features.\n3 Continuous Space Language Models\nA key factor for quality inference of a translated\ntext is to determine the ﬂuency of such a text and\nhow well it conforms to the linguistic regularities\nof the target language. It involves grammatical\ncorrectness, idiomatic and stylistic word choices\nthat can be derived by using n-gram language\nmodels. However, in high-order n-grams, the pa-\nrameter space is sparse and conventional mod-\nelling is inefﬁcient. Neural networks model the\nnon-linear relationship between the input features\nand target outputs. They often outperform con-\nventional techniques in difﬁcult machine learning\ntasks. Neural network language models (CSLM)\nalleviate the curse of dimensionality by projecting\nwords into a continuous space, and modelling and\nestimating probabilities in this space.\nThe architecture of a deep CSLM is illus-\ntrated in Figure 1. The inputs to a CSLM\nmodel are the (K − 1) left-context words\n(wi−K+1, . . . , wi−2, wi−1) to predict wi. A one-\nhot vector encoding scheme is used to repre-\nsent the input wi−k with an N-dimensional vec-\ntor. The output of CSLM is a vector of pos-\nterior probabilities for all words in vocabulary,\nP(wi|wi−1, wi−2, . . . , wi−K+1). Due to the large\noutput layer (vocabulary size), the complexity of a\nbasic neural network language model is very high.\nSchwenk (2007) proposed efﬁcient training strate-\ngies in order to reduce the computational complex-\nity and speed up the training time. They process\nseveral examples at once and use a short-list vo-\ncabulary V with only the most frequent words.\nFigure 1: Deep CSLM architecture.\nFollowing the settings mentioned in (Schwenk\net al., 2014), all CSLM experiments described\nin this paper are performed using deep networks\nwith four hidden layers: ﬁrst layer for the projec-\ntion (320 units for each context word) and three\nhidden layers of 1024 units with tanh activation.\nAt the output layer, we use a softmax activation\nfunction applied to a short-list of the 32k most\nfrequent words. The probabilities of the out-of-\nvocabulary words are obtained from a standard\nback-off n-gram language model. The projection\nof the words onto the continuous space and the\ntraining of the neural network is done by the stan-\ndard back-propagation algorithm and outputs are\nthe converged posterior probabilities. The model\nparameters are optimised on a development set.\n4 CSLM and Quality Estimation\nIn the context of MT, CSLMs are generally trained\non the target side of a given language pair to ex-\n1074\npress the probability that the generated sentence\nis “correct” or “likely”, without looking at the\nsource sentence. However, QE is also concerned\nwith how well the source segments can be trans-\nlated. Therefore, we trained two models, one for\neach side of a given language pair. We extracted\nthe probabilities for QE training and test sets for\nboth source and its translation with their respec-\ntive models and used them as features, along with\nother features, in a supervised learning setting.\nFinally, we also used CSLM in a spoken lan-\nguage translation (SLT) task. In SLT, an auto-\nmatic speech recogniser (ASR) is used to decode\nthe source language text from audio. This creates\nan extra source of variability, where different ASR\nmodels and conﬁgurations give different outputs.\nIn this paper, we use QE to exploit different ASR\noutputs (i.e. MT inputs) which in turn can lead to\ndifferent MT outputs.\n5 Experiments\nWe focus on experiments with sentence level QE\ntasks. Our English-Spanish experiments are based\non the WMT QE shared task data from 2012 to\n2015.1 These tasks are diverse in nature, with dif-\nferent sizes and labels such as post-editing effort\n(PEE), post-editing time (PET) and human trans-\nlation error rate (HTER). The results reported in\nSection 5.5 are directly comparable with the of-\nﬁcial systems submitted for each of the respec-\ntive tasks. We also performed experiments on the\nIWSLT 2014 English-French SLT task 2 to study\nthe applicability of our models on n-best ASR\n(MT inputs) comparison.\n5.1 QE Datasets\nIn Table 1 we summarise the data and tasks for our\nexperiments. We refer readers to the WMT and\nIWSLT websites for detailed descriptions of these\ndatasets. All datasets are publicly available.\nWMT12: English-Spanish news sentence trans-\nlations produced by a Moses “baseline” statisti-\ncal MT (SMT) system, and judged for perceived\npost-editing effort in 1–5 (highest-lowest), taking\na weighted average of three annotators (Callison-\nBurch et al., 2012).\nWMT13 (Task-1): English-Spanish sentence\ntranslations of news texts produced by a Moses\n1http://www.statmt.org/wmt[12,13,14,\n15]/quality-estimation-task.html\n2https://sites.google.com/site/\niwsltevaluation2014/slt-track\n“baseline” SMT system. These were then post-\nedited by a professional translator and labelled\nusing HTER. This is a superset of the WMT12\ndataset, with 500 additional sentences for test, and\na different quality label (Bojar et al., 2013).\nWMT14 (Task-1.1): English-Spanish news\nsentence translations. The dataset contains source\nsentences and their human translations, as well\nas three versions of machine translations: by an\nSMT system, a rule-based system system and a\nhybrid system. Each translation was labelled by\nprofessional translators with 1-3 (lowest-highest)\nscores for perceived post-editing effort.\nWMT14 (Task-1.3): English-Spanish news\nsentence translations post-edited by a professional\ntranslator, with the post-editing time collected on a\nsentence-basis and used as label (in milliseconds).\nWMT15 (Task-1): Large English-Spanish news\ndataset containing source sentences, their machine\ntranslations by an online SMT system, and the\npost-editions of the translation by crowdsourced\ntranslators, with HTER used as label.\nIWSLT14: English-French dataset containing\nsource language data from the 10-best (sentences)\nASR system output. On the target side, the 1-\nbest MT translation is used. The ASR system\nleads to different source segments, which in turn\nlead to different translations. METEOR (Banerjee\nand Lavie, 2005) is used to label these alternative\ntranslations against a reference (human) transla-\ntion. Both ASR and MT outputs come from a sys-\ntem submission in IWSLT 2014 (Ng et al., 2014).\nThe ASR system is a multi-pass deep neural net-\nwork tandem system with feature and model adap-\ntation and rescoring. The MT system is a phrase-\nbased SMT system produced using Moses.\nDataset Lang. Train Test Label\nWMT12 en-es 1, 832 422 PEE 1-5\nWMT13 en-es 2, 254 500 HTER 0-1\nWMT14task1.1 en-es 3, 816 600 PEE 1-3\nWMT14task1.3 en-es 650 208 PET (ms)\nWMT15 en-es 11, 271 1 , 817 HTER 0-1\nIWSLT14 en-fr 8, 180 11 , 240 MET. 0-1\nTable 1: QE datasets: # sentences and labels.\n5.2 CSLM Dataset\nThe dataset used for CSLM training consists of\nEuroparl, News-commentary and News-crawl cor-\npus. We used a data selection method (Moore\n1075\nand Lewis, 2010) to select the most relevant train-\ning data with respect to a development set. For\nEnglish-Spanish, the development data is the con-\ncatenation of newstest2012 and newstest2013 of\nthe WMT translation track. For English-French,\nthe development set is the concatenation of the\nIWSLT dev2010 and eval2010. In Table 2 we\nshow statistics on the selected monolingual data\nused to train back-off LM and CSLM.\nLang. Train Dev LM ppl CSLM ppl\nen 4.3G 137.7k 164.63 116.58 (29.18%)\nfr 464.7M 54K 99.34 64.88 (34.68%)\nes 21.2M 149.4k 145.49 87.14 (40.10%)\nTable 2: Training data size (number of tokens) and\nlanguage models perplexity (ppl). The values in\nparentheses in last column shows percentage de-\ncrease in perplexity.\n5.3 Feature Sets\nWe use the QuEst 3 toolkit (Specia et al., 2013;\nShah et al., 2013a) to extract two feature sets for\neach dataset:\n•BL: 17 features used as baseline in the WMT\nshared tasks on QE.\n•AF: 80 a ugmented MT system-independent\nfeatures4 (superset of BL). For the En-Fr SLT\ntask, we have additional 36 features (21 ASR\n+ 15 MT-dependent features)\nThe resources used to extract these features (cor-\npora, etc.) are also available as part of the WMT\nshared tasks on QE. The CSLM features for each\nof the source and target segments are extracted us-\ning the procedure described in Section 3 with the\nCSLM toolkit. 5\nWe trained QE models with following combina-\ntion of features:\n•BL + CSLM src,tgt: CSLM features for\nsource and target segments, plus the baseline\nfeatures.\n•AF + CSLM src,tgt: CSLM features for\nsource and target segments, plus all available\nfeatures.\nFor the WMT12 task, we performed further exper-\niments to analyse the improvements with CSLM:\n•CSLMsrc: Source side CSLM feature only.\n•CSLMtgt: Target side CSLM feature only.\n•CSLMsrc,tgt: Source and target CSLM fea-\ntures by themselves.\n3http://www.quest.dcs.shef.ac.uk/\n480 features http://www.quest.dcs.shef.ac.\nuk/quest_files/features_blackbox\n5http://www-lium.univ-lemans.fr/cslm/\n•FS(AF) + CSLMsrc,tgt: CSLM features in\naddition to the best performing feature set\n(FS(AF)) selected as described in (Shah et\nal., 2013b; Shah et al., 2015).\n5.4 Learning algorithms\nWe use the Support Vector Machines implementa-\ntion of the scikit-learn toolkit to perform re-\ngression (SVR) with either Radial Basis Function\n(RBF) or linear kernel and parameters optimised\nvia grid search. To evaluate the prediction models\nwe use Mean Absolute Error (MAE), its squared\nversion – Root Mean Squared Error (RMSE), and\nPearson’s correlation (r) score.\nTask System #feats MAE RMSE r\nWMT12\nBL 17 0.6821 0.8117 0.5595\nAF 80 0.6717 0.8103 0.5645\nBL + CSLMsrc,tgt 19 0.6463 0.7977 0.5805\nAF + CSLMsrc,tgt 82 0.6462 0.7946 0.5825\nWMT13\nBL 17 0.1411 0.1812 0.4612\nAF 80 0.1399 0.1789 0.4751\nBL + CSLMsrc,tgt 19 0.1401 0.1791 0.4771\nAF + CSLMsrc,tgt 82 0.1371 0.1750 0.4820\nWMT14\nTask 1.1\nBL 17 0.5241 0.6591 0.2502\nAF 80 0.4896 0.6349 0.3310\nBL + CSLMsrc,tgt 19 0.4931 0.6351 0.3545\nAF + CSLMsrc,tgt 82 0.4628∗ 0.6165∗ 0.3824∗\nWMT14\nTask 1.3\nBL 17 0.1798 0.2865 0.5661\nAF 80 0.1753 0.2815 0.5871\nBL + CSLMsrc,tgt 19 0.1740 0.2758 0.6243\nAF + CSLMsrc,tgt 82 0.1701∗∗ 0.2734 0.6201\nWMT15\nBL 17 0.1562 0.2036 0.1382\nAF 80 0.1541 0.1995 0.2205\nBL + CSLMsrc,tgt 19 0.1501 0.1971 0.2611\nAF + CSLMsrc,tgt 82 0.1471 0.1934 0.2862\nIWSLT14\nBL 17 0.1390 0.1791 0.5012\nAF 116 0.1361 0.1775 0.5211\nBL + CSLMsrc,tgt 19 0.1358 0.1750 0.5321\nAF + CSLMsrc,tgt 118 0.1337 0.1728 0.5445\nTable 3: Results for datasets with various feature\nsets. Figures with ∗beat the ofﬁcial best systems,\nand with ∗∗are second best. Results with CSLM\nfeatures are signiﬁcantly better than BL and AF on\nall tasks (paired t-test with p ≤0.05).\nTaskSystem #feats MAE RMSE r\nWMT12\nBL + CSLMsrc 18 0.6751 0.8125 0.5626\nBL + CSLMtgt 18 0.6694 0.8023 0.5815\nCSLMsrc,tgt 2 0.6882 0.8430 0.5314\nFS(AF) 19 0.6131 0.7598 0.6296\nFS(AF) + CSLMsrc,tgt 21 0.5950∗ 0.7442∗ 0.6482∗\nTable 4: Impact of different combinations of\nCSLM features on the WMT12 task. Figures with\n∗beat the ofﬁcial best system. Results with CSLM\nfeatures are signiﬁcantly better than BL and AF on\nall tasks (paired t-test with p ≤0.05).\n1076\n5.5 Results\nTable 3 presents the results with different feature\nsets for data from various shared tasks. It can be\nnoted that CSLM features always bring signiﬁcant\nimprovements whenever added to either baseline\nor augmented feature set. A reduction in both error\nscores (MAE and RMSE) as well as an increase\nin Pearson’s correlation with human labels can be\nobserved on all tasks. It is also worth noticing\nthat the CSLM features bring improvements over\nall tasks with different labels, evidencing that dif-\nferent optimisation objectives and language pairs\ncan beneﬁt from these features. However, the im-\nprovements are more visible when predicting post-\nediting effort for WMT12 and WMT14’s Task 1.1.\nFor these two tasks, we are able to achieve state-\nof-the-art performance by adding the two CSLM\nfeatures to all available or selected feature sets.\nFor WMT12, we performed another set of ex-\nperiments to study the effect of CSLM features\nby themselves and in combination. The results\nin Table 4 show that the target side CSLM fea-\nture bring larger improvements than its source side\ncounterpart. We believe that it is because the tar-\nget side feature directly reﬂects the ﬂuency of the\ntranslation, whereas the source side feature (re-\ngarded as a translation complexity feature) only\nhas indirect effect on quality. Interestingly, the\ntwo CSLM features alone give comparable re-\nsults (slightly worse) than the BL feature set 6 de-\nspite the fact that these 17 features cover many\ncomplexity, adequacy and ﬂuency quality aspects.\nCSLM features bring further improvements on\npre-selected feature sets, as shown in Table 3. We\nalso performed feature selection over the full fea-\nture set along with CSLM features, following the\nprocedure in (Shah et al., 2013b). Interestingly,\nboth CSLM features were selected among the top\nranked features, conﬁrming their relevance.\nIn order to investigate whether our CSLM fea-\ntures results hold for other feature sets, we ex-\nperimented with the feature sets provided by most\nteams participating in the WMT12 QE shared task.\nThese feature sets are very diverse in terms of the\ntypes of features, resources used, and their sizes.\nTable 5 shows the ofﬁcial results from the shared\ntask (Off.) (Callison-Burch et al., 2012), those\nfrom training an SVR on these features with and\nwithout CSLM features. Note that the ofﬁcial\nscores are often different from the results obtained\nwith our SVR models because of differences in\n6We compare results in terms of MAE scores only.\nthe learning algorithms. As shown in Table 5,\nwe observed similar improvements with additional\nCSLM features over all of these feature sets.\nSystem #feats Off. SVR SVR\nwithout CSLM with CSLM\nSDL 15 0.61 0.6115 0.5993\nUU 82 0.64 0.6513 0.6371\nLoria 49 0.68 0.6978 0.6729\nUEdin 56 0.68 0.6879 0.6724\nTCD 43 0.68 0.6972 0.6715\nWL-SH 147 0.69 0.6791 0.6678\nUPC 57 0.84 0.8419 0.8310\nDCU 308 0.75 0.6825 0.6812\nPRHLT 497 0.70 0.6699 0.6649\nTable 5: MAE score on ofﬁcial WMT12 feature\nsets using SVR with and without CSLM features.\n6 Conclusions\nWe proposed novel features for machine transla-\ntion quality estimation obtained using a deep con-\ntinuous space language models. The proposed fea-\ntures led to signiﬁcant improvements over stan-\ndard feature sets for a variety of datasets, outper-\nforming the state-of-art on two ofﬁcial WMT QE\ntasks. These results showed that different opti-\nmisation objectives and language pairs can bene-\nﬁt from the proposed features. The proposed fea-\ntures have been shown to also perform well on QE\nwithin a spoken language translation task.\nBoth source and target CSLM features improve\nprediction quality, either when used separately\nor in combination. They proved complementary\nwhen used together with other feature sets and\nproduce comparable results to high performing\nbaseline features when used alone for prediction.\nFinally, results comparing all ofﬁcial WMT12 QE\nfeature sets showed signiﬁcant improvements in\nthe predictions when CSLM features were added\nto those submitted by participating teams. These\nﬁndings provide evidence that the proposed fea-\ntures bring valuable information into prediction\nmodels, despite their simplicity and the fact that\nthey require only monolingual data as resource,\nwhich is available in abundance for many lan-\nguages.\nAs future work, it would be interesting to ex-\nplore various distributed word representations for\nquality estimation and joint models that look at\nboth the source and the target sentences simulta-\nneously.\nAcknowledgements\nThis work was supported by the QT21 (H2020\nNo. 645452), Cracker (H2020 No. 645357) and\nDARPA Bolt projects.\n1077\nReferences\nEbru Arisoy, Tara N. Sainath, Brian Kingsbury, and\nBhuvana Ramabhadran. 2012. Deep neural network\nlanguage models. In NAACL-HLT 2012 Workshop:\nWill We Ever Really Replace the N-gram Model? On\nthe Future of Language Modeling for HLT, pages\n20–28, Montreal, Canada.\nRafael E Banchs, Luis F D’Haro, and Haizhou Li.\n2015. Adequacy–ﬂuency metrics: Evaluating mt\nin the continuous space model framework. Au-\ndio, Speech, and Language Processing, IEEE/ACM\nTransactions on, 23(3):472–482.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In ACL work-\nshop on intrinsic and extrinsic evaluation measures\nfor machine translation and/or summarization, vol-\nume 29, pages 65–72.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. The Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nOndrej Bojar, Christian Buck, Chris Callison-Burch,\nChristian Federmann, Barry Haddow, Philipp\nKoehn, Christof Monz, Matt Post, Radu Soricut, and\nLucia Specia. 2013. Findings of the 2013 Work-\nshop on Statistical Machine Translation. In Eighth\nWorkshop on Statistical Machine Translation, pages\n1–44, Soﬁa, Bulgaria.\nChris Callison-Burch, Philipp Koehn, Christof Monz,\nMatt Post, Radu Soricut, and Lucia Specia. 2012.\nFindings of the 2012 WMT. In Seventh Workshop\non Statistical Machine Translation, pages 10–51,\nMontr´eal, Canada.\nWang Ling, Luis Marujo, Chris Dyer, Alan Black, and\nIsabel Trancoso. 2014. Crowdsourcing high-quality\nparallel data extraction from twitter. In Ninth Work-\nshop on Statistical Machine Translation, WMT14,\npages 426–436, Baltimore, USA.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013. Linguistic regularities in continuous space\nword representations. In HLT-NAACL, pages 746–\n751.\nRobert C. Moore and William Lewis. 2010. Intelli-\ngent selection of language model training data. In\nProceedings of the ACL 2010 Conference Short Pa-\npers, ACLShort ’10, pages 220–224, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nRaymond W. N. Ng, Mortaza Doulaty, Rama Dod-\ndipatla, Oscar Saz, Madina Hasan, Thomas Hain,\nWilker Aziz, Kashif Shaf, and Lucia Specia. 2014.\nThe USFD spoken language translation system for\nIWSLT 2014. Proc. IWSLT, pages 86–91.\nHolger Schwenk and Jean-Luc Gauvain. 2005. Train-\ning neural network language models on very large\ncorpora. In Conference on Human Language Tech-\nnology and Empirical Methods in Natural Language\nProcessing, pages 201–208.\nHolger Schwenk, Fethi Bougares, and Loic Barrault.\n2014. Efﬁcient training strategies for deep neural\nnetwork language models. In NIPS workshop on\nDeep Learning and Representation Learning.\nHolger Schwenk. 2007. Continuous space language\nmodels. Computer Speech & Language, 21(3):492–\n518.\nHolger Schwenk. 2012. Continuous space translation\nmodels for phrase-based statistical machine transla-\ntion. In COLING (Posters), pages 1071–1080.\nKashif Shah, Eleftherios Avramidis, Ergun Bic ¸icic, and\nLucia Specia. 2013a. Quest - design, implemen-\ntation and extensions of a framework for machine\ntranslation quality estimation. The Prague Bulletin\nof Mathematical Linguistics, 100:19–30.\nKashif Shah, Trevor Cohn, and Lucia Specia. 2013b.\nAn investigation on the effectiveness of features for\ntranslation quality estimation. In Machine Transla-\ntion Summit, volume 14, pages 167–174.\nKashif Shah, Trevor Cohn, and Lucia Specia. 2015.\nA bayesian non-linear method for feature selection\nin machine translation quality estimation. Machine\nTranslation, 29(2):101–125.\nLucia Specia, Kashif Shah, Jos ´e G. C. de Souza, and\nTrevor Cohn. 2013. QuEst - A translation qual-\nity estimation framework. In 51st Annual Meeting\nof the Association for Computational Linguistics:\nDemo Session, pages 79–84, Soﬁa, Bulgaria.\nLiling Tan, Carolina Scarton, Lucia Specia, and Josef\nvan Genabith. 2015. Usaar-shefﬁeld: Semantic\ntextual similarity with deep regression and machine\ntranslation evaluation metrics. In Proceedings of the\n9th International Workshop on Semantic Evaluation,\npages 85–89, Denver, Colorado.\n1078",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8282874822616577
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.8234465718269348
    },
    {
      "name": "Machine translation",
      "score": 0.817243218421936
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6526278853416443
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5786060690879822
    },
    {
      "name": "Translation (biology)",
      "score": 0.5759693384170532
    },
    {
      "name": "Baseline (sea)",
      "score": 0.52140212059021
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.49153169989585876
    },
    {
      "name": "Feature vector",
      "score": 0.4676263928413391
    },
    {
      "name": "Language model",
      "score": 0.46570736169815063
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.4194561243057251
    },
    {
      "name": "Machine learning",
      "score": 0.4092062711715698
    },
    {
      "name": "Speech recognition",
      "score": 0.39123284816741943
    },
    {
      "name": "Natural language processing",
      "score": 0.37568679451942444
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34751421213150024
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}