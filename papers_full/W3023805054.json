{
    "title": "Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA",
    "url": "https://openalex.org/W3023805054",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2798637181",
            "name": "Nina Poerner",
            "affiliations": [
                "Ludwig-Maximilians-Universität München",
                "Siemens (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A61766753",
            "name": "Ulli Waltinger",
            "affiliations": [
                "Siemens (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2035156685",
            "name": "Hinrich Schütze",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2983577274",
        "https://openalex.org/W2154142897",
        "https://openalex.org/W2126725946",
        "https://openalex.org/W2768282280",
        "https://openalex.org/W2071879021",
        "https://openalex.org/W2970283086",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W2149369282",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2047782770",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W2798908575",
        "https://openalex.org/W2100627415",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2997419538",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2952190837",
        "https://openalex.org/W2169099542",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2971277088"
    ],
    "abstract": "Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO_2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60% of the BioBERT-BERT F1 delta, at 5% of BioBERT's CO_2 footprint and 2% of its cloud compute cost. We also show how to quickly adapt an existing general-domain Question Answering (QA) model to an emerging domain: the Covid-19 pandemic.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1482–1490\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n1482\nInexpensive Domain Adaptation of Pretrained Language Models:\nCase Studies on Biomedical NER and Covid-19 QA\nNina Poerner∗†and Ulli Waltinger†and Hinrich Sch¨utze∗\n∗Center for Information and Language Processing, LMU Munich, Germany\n†Corporate Technology Machine Intelligence (MIC-DE), Siemens AG Munich, Germany\npoerner@cis.uni-muenchen.de | inquiries@cislmu.org\nAbstract\nDomain adaptation of Pretrained Language\nModels (PTLMs) is typically achieved by un-\nsupervised pretraining on target-domain text.\nWhile successful, this approach is expensive\nin terms of hardware, runtime and CO 2 emis-\nsions. Here, we propose a cheaper alternative:\nWe train Word2Vec on target-domain text and\nalign the resulting word vectors with the word-\npiece vectors of a general-domain PTLM. We\nevaluate on eight English biomedical Named\nEntity Recognition (NER) tasks and compare\nagainst the recently proposed BioBERT model.\nWe cover over 60% of the BioBERT – BERT\nF1 delta, at 5% of BioBERT’s CO 2 footprint\nand 2% of its cloud compute cost. We also\nshow how to quickly adapt an existing general-\ndomain Question Answering (QA) model to an\nemerging domain: the Covid-19 pandemic.1\n1 Introduction\nPretrained Language Models (PTLMs) such as\nBERT (Devlin et al., 2019) have spearheaded ad-\nvances on many NLP tasks. Usually, PTLMs\nare pretrained on unlabeled general-domain and/or\nmixed-domain text, such as Wikipedia, digital\nbooks or the Common Crawl corpus.\nWhen applying PTLMs to speciﬁc domains, it\ncan be useful to domain-adapt them. Domain adap-\ntation of PTLMs has typically been achieved by pre-\ntraining on target-domain text. One such model is\nBioBERT (Lee et al., 2020), which was initialized\nfrom general-domain BERT and then pretrained\non biomedical scientiﬁc publications. The domain\nadaptation is shown to be helpful for target-domain\ntasks such as biomedical Named Entity Recogni-\ntion (NER) or Question Answering (QA). On the\ndownside, the computational cost of pretraining can\nbe considerable: BioBERTv1.0 was adapted for ten\n1www.github.com/npoe/covid-qa\ndays on eight large GPUs (see Table 1), which is\nexpensive, environmentally unfriendly, prohibitive\nfor small research labs and students, and may delay\nprototyping on emerging domains.\nWe therefore propose afast, CPU-only domain-\nadaptation method for PTLMs : We train\nWord2Vec (Mikolov et al., 2013a) on target-domain\ntext and align the resulting word vectors with the\nwordpiece vectors of an existing general-domain\nPTLM. The PTLM thus gains domain-speciﬁc lexi-\ncal knowledge in the form of additional word vec-\ntors, but its deeper layers remain unchanged. Since\nWord2Vec and the vector space alignment are efﬁ-\ncient models, the process requires a fraction of the\nresources associated with pretraining the PTLM\nitself, and it can be done on CPU.\nIn Section 4, we use the proposed method to\ndomain-adapt BERT on PubMed+PMC (the data\nused for BioBERTv1.0) and/or CORD-19 (Covid-\n19 Open Research Dataset). We improve over\ngeneral-domain BERT on eight out of eight biomed-\nical NER tasks, using a fraction of the compute cost\nassociated with BioBERT. In Section 5, we show\nhow to quickly adapt an existing Question Answer-\ning model to text about the Covid-19 pandemic,\nwithout any target-domain Language Model pre-\ntraining or ﬁnetuning.\n2 Related work\n2.1 The BERT PTLM\nFor our purpose, a PTLM consists of three parts:\nA tokenizer TLM : L+ →L+\nLM, a wordpiece em-\nbedding lookup function ELM : LLM → RdLM\nand an encoder function FLM. LLM is a lim-\nited vocabulary of wordpieces. All words from\nthe natural language L+ that are not in LLM\nare tokenized into sequences of shorter word-\npieces, e.g., dementia becomes dem ##ent ##ia.\nGiven a sentence S = [ w1, . . . , wT ], tokenized\n1483\nsize Domain adaptation hardware Power(W) Time(h) CO 2(lbs) Google Cloud $\nBioBERTv1.0 base 8 NVIDIA v100 GPUs (32GB) 1505 240 544 1421 – 4762\nBioBERTv1.1 base 8 NVIDIA v100 GPUs (32GB) 1505 552 1252 3268 – 10952\nGreenBioBERT (Section 4) base 12 Intel Xeon E7-8857 CPUs, 30GB RAM 1560 12 28 16 – 76\nGreenCovidSQuADBERT (Section 5) large 12 Intel Xeon E7-8857 CPUs, 40GB RAM 1560 24 56 32 – 152\nTable 1: Domain adaptation cost. CO 2 emissions are calculated according to Strubell et al. (2019). Since our\nhardware conﬁguration is not available on Google Cloud, we take anm1-ultramem-40 instance (40 vCPUs, 961GB\nRAM) to estimate an upper bound on our Google Cloud cost.\nas TLM(S) = [TLM(w1); . . .; TLM(wT )], ELM em-\nbeds every wordpiece inTLM(S) into a real-valued,\ntrainable wordpiece vector. The wordpiece vec-\ntors of the entire sequence are stacked and fed into\nFLM. Note that we consider position and segment\nembeddings to be a part of FLM rather than ELM.\nIn the case of BERT, FLM is a Transformer\n(Vaswani et al., 2017), followed by a ﬁnal Feed-\nForward Net. During pretraining, the Feed-\nForward Net predicts the identity of masked word-\npieces. When ﬁnetuning on a supervised task, it is\nusually replaced with a randomly initialized layer.\n2.2 Domain-adapted PTLMs\nDomain adaptation of PTLMs is typically achieved\nby pretraining on unlabeled target-domain text.\nSome examples of such models are BioBERT\n(Lee et al., 2020), which was pretrained on the\nPubMed and/or PubMed Central (PMC) corpora,\nSciBERT (Beltagy et al., 2019), which was pre-\ntrained on papers from SemanticScholar, Clinical-\nBERT (Alsentzer et al., 2019; Huang et al., 2019a)\nand ClinicalXLNet (Huang et al., 2019b), which\nwere pretrained on clinical patient notes, and Adapt-\naBERT (Han and Eisenstein, 2019), which was\npretrained on Early Modern English text. In most\ncases, a domain-adapted PTLM is initialized from\na general-domain PTLM (e.g., standard BERT),\nthough Beltagy et al. (2019) report better results\nwith a model that was pretrained from scratch with\na custom wordpiece vocabulary. In this paper, we\nfocus on BioBERT, as its domain adaptation cor-\npora are publicly available.\nAcc@1 Acc@5 Acc@10\ntrain (19.8K words) 53.6 63.5 65.7\nheldout (2.2K words) 39.4 51.6 54.3\nTable 2: LW2V →LLM alignment accuracy (%), i.e.,\nhow often the identical string is in the top-K nearest\nneighbors.\n2.3 Word vectors\nWord vectors are distributed representations of\nwords that are trained on unlabeled text. Con-\ntrary to PTLMs, word vectors are non-contextual,\ni.e., a word type is always assigned the same vec-\ntor, regardless of context. In this paper, we use\nWord2Vec (Mikolov et al., 2013a) to train word\nvectors. We will denote the Word2Vec lookup func-\ntion as EW2V : LW2V →RdW2V .\n2.4 Word vector space alignment\nWord vector space alignment has most frequently\nbeen explored in the context of cross-lingual word\nembeddings. For instance, Mikolov et al. (2013b)\nalign English and Spanish Word2Vec spaces by a\nsimple linear transformation. Wang et al. (2019)\nuse a related method to align cross-lingual word\nvectors and multilingual BERT wordpiece vectors.\nIn this paper, we apply the method to the problem\nof domain adaptation within the same language.\n3 Method\nIn the following, we assume access to a general-\ndomain PTLM, as described in Section 2.1, and a\ncorpus of unlabeled target-domain text.\n3.1 Creating new input vectors\nIn a ﬁrst step, we train Word2Vec on the target-\ndomain corpus. In a second step, we take the in-\ntersection of LLM and LW2V. In practice, the in-\ntersection mostly contains wordpieces from LLM\nthat correspond to standalone words. It also con-\ntains single characters and other noise, however, we\nfound that ﬁltering them does not improve align-\nment quality. In a third step, we use the intersec-\ntion to ﬁt an unconstrained linear transformation\nW ∈RdLM×dW2V via least squares:\nargmin\nW\n∑\nx∈LLM∩LW2V\n||WEW2V(x) −ELM(x)||2\n2\nIntuitively, W makes Word2Vec vectors “look\nlike” the PTLM’s native wordpiece vectors, just\n1484\nQuery NNs of query in ELM[LLM] NNs of query in WEW2V[LW2V]\nquery ∈LW2V ∩LLM\nBoldface: Training vector pairs\nsurgeon physician, psychiatrist, surgery surgeon, urologist, neurosurgeon\nsurgeon surgeon , physician, researcher neurosurgeon, urologist, radiologist\ndepression Depression, recession, depressed depression, Depression, hopelessness\ndepression depression , anxiety, anxiousness depressive, insomnia, Depression\nfatal lethal, deadly, disastrous fatal, lethal, deadly\nfatal fatal , catastrophic, disastrous lethal, devastating, disastrous\nquery ∈LW2V −LLM\nventricular cardiac, pulmonary, mitochondrial atrial, ventricle, RV\ndementia diabetes, Alzheimer, autism VaD, MCI, AD\nsuppressants medications, medicines, medication suppressant, prokinetics, painkillers\nanesthesiologist surgeon, technician, psychiatrist anesthetist, anaesthesiologist, anaesthetist\nnephrotoxicity toxicity, inﬂammation, contamination hepatotoxicity, ototoxicity, cardiotoxicity\nimpairment inability, disruption, disorders impairments, deﬁcits, deterioration\nTable 3: Examples of within-space and cross-space nearest neighbors (NNs) by cosine similarity in Green-\nBioBERT’s wordpiece embedding layer. Blue: Original wordpiece space. Green: Aligned Word2Vec space.\nlike cross-lingual alignment makes word vectors\nfrom one language “look like” word vectors from\nanother language. In Table 2, we report word align-\nment accuracy when we split LLM ∩LW2V into a\ntraining and development set.2 In Table 3, we show\nexamples of within-space and cross-space nearest\nneighbors after alignment.\n3.2 Updating the wordpiece embedding layer\nNext, we redeﬁne the wordpiece embedding layer\nof the PTLM. The most radical strategy would be to\nreplace the entire layer with the aligned Word2Vec\nvectors:\nˆELM : LW2V →RdLM ; ˆELM(x) = WEW2V(x)\nIn initial experiments, this strategy led to a\ndrop in performance, presumably because func-\ntion words are not well represented by Word2Vec,\nand replacing them disrupts BERT’s syntactic abil-\nities. To prevent this problem, we leave existing\nwordpiece vectors intact and only add new ones:\nˆELM : LLM ∪LW2V →RdLM;\nˆELM(x) =\n{\nELM(x) if x ∈LLM\nWEW2V(x) otherwise (1)\n3.3 Updating the tokenizer\nIn a ﬁnal step, we update the tokenizer to account\nfor the added words. Let TLM be the standard\nBERT tokenizer, and let ˆTLM be the tokenizer that\ntreats all words in LLM ∪LW2V as one-wordpiece\ntokens, while tokenizing any other words as usual.\nIn practice, a given word may or may not beneﬁt\nfrom being tokenized by ˆTLM instead of TLM. To\n2Since we are not primarily interested in word alignment\naccuracy, we use the entire intersection as a training set in all\nother experiments.\ngive a concrete example, 82% of the words in the\nBC5CDR NER dataset that end in the sufﬁx -ia are\npart of a disease entity (e.g., dementia). TLM tok-\nenizes this word as dem ##ent ##ia, thereby expos-\ning this strong orthographic cue to the model. As\na result, TLM improves recall on -ia diseases. But\nthere are many cases where wordpiece tokeniza-\ntion is meaningless or misleading. For instance\neuthymia (not a disease) is tokenized by TLM as e\n##uth ##ym ##ia, making it likely to be classiﬁed\nas a disease. By contrast, ˆTLM gives euthymia a\none-wordpiece representation that depends only on\ndistributional semantics. We ﬁnd that using ˆTLM\nimproves precision on -ia diseases.\nTo combine these complementary strengths, we\nuse a 50/50 mixture of TLM-tokenization and ˆTLM-\ntokenization when ﬁnetuning the PTLM on a task.\nAt test time, we use both tokenizers and mean-pool\nthe outputs. Let o(S; T) be some output of interest\n(e.g., a logit), given sentence S tokenized by T.\nWe predict:\nˆo(S) = o(S; TLM) + o(S; ˆTLM)\n2\n4 Experiment 1: Biomedical NER\nIn this section, we use the proposed method to\ncreate GreenBioBERT, an inexpensive and envi-\nronmentally friendly alternative to BioBERT. Re-\ncall that BioBERTv1.0 (biobert v1.0 pubmed pmc)\nwas initialized from general-domain BERT (bert-\nbase-cased) and then pretrained on PubMed+PMC.\n4.1 Domain adaptation\nWe train Word2Vec with vector size dW2V =\ndLM = 768 on PubMed+PMC (see Appendix for\ndetails). Then, we update the wordpiece embed-\nding layer and tokenizer of general-domain BERT\n(bert-base-cased) as described in Section 3.\n1485\nBERT (ref) BioBERTv1.0 (ref) BioBERTv1.1 (ref) GreenBioBERT\nBiomedical NER task (NER task ID) (Lee et al., 2020) (Lee et al., 2020) (Lee et al., 2020) (with standard error of the mean)\nBC5CDR-disease (Li et al., 2016) (1) 81.97 / 82.48 / 82.41 85.86 / 87.27 / 86.56 86.47 / 87.84 / 87.15 84.88 (.07) / 85.29 (.12) / 85.08 (.08)\nNCBI-disease (Do˘gan et al., 2014) (2) 84.12 / 87.19 / 85.63 89.04 / 89.69 / 89.36 88.22 / 91.25 / 89.71 85.49 (.23) / 86.41 (.15) / 85.94 (.16)\nBC5CDR-chem (Li et al., 2016) (3) 90.94 / 91.38 / 91.16 93.27 / 93.61 / 93.44 93.68 / 93.26 / 93.47 93.82 (.11) / 92.35 (.17) / 93.08 (.07)\nBC4CHEMD (Krallinger et al., 2015) (4) 91.19 / 88.92 / 90.04 92.23 / 90.61 / 91.41 92.80 / 91.92 / 92.36 92.80 (.04) / 89.78 (.07) / 91.26 (.04)\nBC2GM (Smith et al., 2008) (5) 81.17 / 82.42 / 81.79 85.16 / 83.65 / 84.40 84.32 / 85.12 / 84.72 83.34 (.15) / 83.58 (.09) / 83.45 (.10)\nJNLPBA (Kim et al., 2004) (6) 69.57 / 81.20 / 74.94 72.68 / 83.21 / 77.59 72.24 / 83.56 / 77.49 71.93 (.12) / 82.58 (.12) / 76.89 (.10)\nLINNAEUS (Gerner et al., 2010) (7) 91.17 / 84.30 / 87.60 93.84 / 86.11 / 89.81 90.77 / 85.83 / 88.24 92.50 (.17) / 84.54 (.26) / 88.34 (.18)\nSpecies-800 (Paﬁlis et al., 2013) (8) 69.35 / 74.05 / 71.63 72.84 / 77.97 / 75.31 72.80 / 75.36 / 74.06 73.19 (.26) / 75.47 (.33) / 74.31 (.24)\nTable 4: Biomedical NER test set precision / recall / F1 (%). “(ref)”: Reference scores from Lee et al. (2020).\nBoldface: Best model in row. Underlined: Best model without target-domain LM pretraining.\n0.00 0.25 0.50 0.75 1.00 1.25 1.50\nTest set F1 shifted and scaled\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n(8)NER task ID\nBERT (ref)\nBERT (repro)\nBioBERTv1.0 (ref)\nBioBERTv1.0 (repro)\nBioBERTv1.1 (ref)\nGreenBioBERT\nFigure 1: NER test set F1, transformed as (x −\nBERT(ref))/(BioBERTv1.0(ref) −BERT(ref)). This\nplot shows what portion of the reported BioBERT –\nBERT F1 delta is covered. “(ref)”: Reference scores\nfrom Lee et al. (2020). “(repro)”: Results of our repro-\nduction experiments. Error bars: Standard error of the\nmean.\nNER task ID (1) (2) (3) (4) (5) (6) (7) (8)\nnon-aligned -4.88 -3.50 -4.13 -3.34 -2.34 -0.56 -0.84 -4.63\nrandom init -4.33 -3.60 -3.19 -3.19 -1.92 -0.50 -0.84 -3.58\nTable 5: Absolute drop in dev set F1 when using non-\naligned word vectors or randomly initialized word vec-\ntors, instead of aligned word vectors.\n4.2 Finetuning\nWe ﬁnetune GreenBioBERT on the eight publicly\navailable NER tasks used in Lee et al. (2020). We\nalso do reproduction experiments with general-\ndomain BERT and BioBERTv1.0, using the same\nsetup as our model. See Appendix for details on\npreprocessing and hyperparameters. Since some of\nthe datasets are sensitive to the random seed, we\nreport mean and standard error over eight runs.\n4.3 Results and discussion\nTable 4 shows entity-level precision, recall and F1,\nas measured by the CoNLL NER scorer. For ease\nof visualization, Figure 1 shows test set F1 shifted\nand scaled as\nf(x) = x −BERT(ref)\nBioBERTv1.0(ref) −BERT(ref)\nwhere BERT(ref) and BioBERTv1.0(ref) are re-\nported scores from Lee et al. (2020). In other\nwords, the ﬁgure shows what portion of the re-\nported BioBERT – BERT F1 delta is covered by\nour less expensive GreenBioBERT model. On av-\nerage, we cover between 61% and 70% of the delta\n(61% for BioBERTv1.0, 70% for BioBERTv1.1,\nand 61% if we take our reproduction experiments\nas reference points).\n4.3.1 Ablation study\nTo test whether the improvements over general-\ndomain BERT are due to the aligned Word2Vec\nvectors, or just to the availability of additional word\nvectors in general, we perform an ablation study\nwhere we replace the aligned vectors with their\nnon-aligned counterparts (by setting W = 1 in Eq.\n1) or with randomly initialized vectors. Table 5\nshows that dev set F1 drops on all datasets under\nthese circumstances, i.e., vector space alignment\nseems to be important.\n5 Experiment 2: Covid-19 QA\nIn this section, we use the proposed method to\nquickly adapt an existing general-domain QA\nmodel to an emerging target domain: the Covid-19\npandemic. Our baseline model is SQuADBERT,3\nan existing BERT model that was ﬁnetuned on the\ngeneral-domain SQuAD dataset (Rajpurkar et al.,\n2016). We evaluate on Deepset-AI Covid-QA\n(M¨oller et al., 2020), a SQuAD-style dataset with\n2019 annotated span-selection questions about 147\npapers from CORD-19 (Covid-19 Open Research\nDataset).4 We assume that there is no labeled target-\ndomain data for ﬁnetuning on the task, and instead\nuse the entire Covid-QA dataset as a test set. This\nis a realistic setup for an emerging domain without\nannotated training data.\n3www.huggingface.co/bert-large-uncased-\nwhole-word-masking-finetuned-squad\n4https://pages.semanticscholar.org/\ncoronavirus-research\n1486\ndomain adaptation corpus size EM F1 substr\nSQuADBERT ——– 33.04 58.24 65.87\nGreenCovid- CORD-19 only 2GB 34.62 60.09 68.20\nSQuADBERT CORD-19+PubMed+PMC 94GB 34.32 60.23 68.00\nTable 6: Results (%) on Deepset-AI Covid-QA. EM\n(exact answer match) and F1 (token-level F1 score) are\nevaluated with the SQuAD scorer. “substr”: Predic-\ntions that are a substring of the gold answer. Much\nhigher than EM, because many gold answers are not\nminimal answer spans (see Appendix, “Notes on Covid-\nQA”, for an example).\n5.1 Domain adaptation\nWe train Word2Vec with vector size dW2V =\ndLM = 1024 on CORD-19 and/or PubMed+PMC.\nThe process takes less than an hour on CORD-\n19 and about one day on the combined corpus,\nagain without the need for a GPU. Then, we update\nSQuADBERT’s wordpiece embedding layer and\ntokenizer, as described in Section 3. We refer to\nthe resulting model as GreenCovidSQuADBERT.\n5.2 Results and discussion\nTable 6 shows that GreenCovidSQuADBERT out-\nperforms general-domain SQuADBERT on all mea-\nsures. Interestingly, the small CORD-19 corpus is\nenough to achieve this result (compare “CORD-19\nonly” and “CORD-19+PubMed+PMC”), presum-\nably because it is speciﬁc to the target domain and\ncontains the Covid-QA context papers.\n6 Conclusion\nAs a reaction to the trend towards high-resource\nmodels, we have proposed an inexpensive, CPU-\nonly method for domain-adapting Pretrained Lan-\nguage Models: We train Word2Vec vectors on\ntarget-domain data and align them with the word-\npiece vector space of a general-domain PTLM.\nOn eight biomedical NER tasks, we cover over\n60% of the BioBERT – BERT F1 delta, at 5%\nof BioBERT’s domain adaptation CO2 footprint\nand 2% of its cloud compute cost. We have also\nshown how to rapidly adapt an existing BERT QA\nmodel to an emerging domain – the Covid-19 pan-\ndemic – without the need for target-domain Lan-\nguage Model pretraining or ﬁnetuning.\nWe hope that our approach will beneﬁt practi-\ntioners with limited time or resources, and that it\nwill encourage environmentally friendlier NLP.\nAcknowledgements\nThis research was funded by Siemens AG. We\nthank our anonymous reviewers for their helpful\ncomments.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In 2nd Clinical Natural\nLanguage Processing Workshop, pages 72–78, Min-\nneapolis, USA.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn EMNLP-IJCNLP, pages 3606–3611, Hong Kong,\nChina.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT, pages 4171–4186, Min-\nneapolis, USA.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy\nSchwartz, and Noah A Smith. 2019. Show your\nwork: Improved reporting of experimental results.\nIn EMNLP-IJCNLP, pages 2185–2194, Hong Kong,\nChina.\nRezarta Islamaj Do ˘gan, Robert Leaman, and Zhiyong\nLu. 2014. NCBI disease corpus: a resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nMartin Gerner, Goran Nenadic, and Casey M Bergman.\n2010. LINNAEUS: a species name identiﬁcation\nsystem for biomedical literature. BMC bioinformat-\nics, 11(1):85.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsuper-\nvised domain adaptation of contextualized embed-\ndings for sequence labeling. In EMNLP-IJCNLP,\npages 4229–4239, Hong Kong, China.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019a. ClinicalBERT: Modeling clinical notes and\npredicting hospital readmission. arXiv preprint\narXiv:1904.05342.\nKexin Huang, Abhishek Singh, Sitong Chen, Ed-\nward T Moseley, Chih-ying Deng, Naomi George,\nand Charlotta Lindvall. 2019b. Clinical XLNet:\nModeling sequential clinical notes and predicting\nprolonged mechanical ventilation. arXiv preprint\narXiv:1912.11975.\nJin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,\nYuka Tateisi, and Nigel Collier. 2004. Introduc-\ntion to the bio-entity recognition task at JNLPBA.\nIn International Joint Workshop on Natural Lan-\nguage Processing in Biomedicine and its Applica-\ntions, pages 70–75.\n1487\nMartin Krallinger, Obdulia Rabal, Florian Leitner,\nMiguel Vazquez, David Salgado, Zhiyong Lu,\nRobert Leaman, Yanan Lu, Donghong Ji, Daniel M\nLowe, et al. 2015. The CHEMDNER corpus of\nchemicals and drugs and its annotation principles.\nJournal of cheminformatics, 7(1):1–17.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. BioBERT: A pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. BioCreative V CDR task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nIlya Loshchilov and Frank Hutter. 2018. Fixing weight\ndecay regularization in Adam.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b.\nExploiting similarities among languages for ma-\nchine translation. arXiv preprint arXiv:1309.4168.\nTimo M ¨oller, Anthony Reina, Raghavan Jayakumar,\nand Malte Pietsch. 2020. Covid-qa: A question &\nanswer dataset for covid-19.\nEvangelos Paﬁlis, Sune P Frankild, Lucia Fanini,\nSarah Faulwetter, Christina Pavloudi, Aikaterini\nVasileiadou, Christos Arvanitidis, and Lars Juhl\nJensen. 2013. The SPECIES and ORGANISMS re-\nsources for fast and accurate identiﬁcation of taxo-\nnomic names in text. PloS one, 8(6).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In EMNLP, pages\n2383–2392, Austin, USA.\nLarry Smith, Lorraine K Tanabe, Rie Johnson nee\nAndo, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan\nHsu, Yu-Shi Lin, Roman Klinger, Christoph M\nFriedrich, Kuzman Ganchev, et al. 2008. Overview\nof BioCreative II gene mention recognition.\nGenome biology, 9(2):S2.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In ACL, pages 3645–3650,\nFlorence, Italy.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In NeurIPS, pages 5998–6008, Long\nBeach, USA.\nHai Wang, Dian Yu, Kai Sun, Janshu Chen, and Dong\nYu. 2019. Improving pre-trained multilingual mod-\nels with vocabulary expansion. In CoNLL, pages\n316–327, Hong Kong, China.\n1488\nInexpensive Domain Adaptation of\nPretrained Language Models (Appendix)\nWord2Vec training\nWe downloaded the PubMed, PMC and CORD-19\ncorpora from:\n• https://ftp.ncbi.nlm.nih.gov/pub/\npmc/oa_bulk/ [20 January 2020, 68GB raw text]\n• https://ftp.ncbi.nlm.nih.gov/pubmed/\nbaseline/ [20 January 2020, 24GB raw text]\n• https://pages.semanticscholar.org/\ncoronavirus-research [17 April 2020, 2GB\nraw text]\nWe extract all abstracts and text bodies and apply\nthe BERT basic tokenizer (a rule-based word tok-\nenizer that standard BERT uses before wordpiece\ntokenization). Then, we train CBOW Word2Vec5\nwith negative sampling. We use default parame-\nters except for the vector size (which we set to\ndW2V = dLM).\nExperiment 1: Biomedical NER\nPretrained models\nGeneral-domain BERT and BioBERTv1.0 were\ndownloaded from:\n• www.storage.googleapis.com/bert_\nmodels/2018_10_18/cased_L-12_H-\n768_A-12.zip\n• www.github.com/naver/biobert-\npretrained\nData\nWe downloaded the NER datasets by follow-\ning instructions on www.github.com/dmis-lab/\nbiobert#Datasets. For detailed dataset statistics,\nsee Lee et al. (2020).\nPreprocessing\nWe use Lee et al. (2020)’s preprocessing strategy:\nWe cut all sentences into chunks of 30 or fewer\nwhitespace-tokenized words (without splitting in-\nside labeled spans). Then, we tokenize every chunk\nS with T = TLM or T = ˆTLM and add special\ntokens:\nX = [CLS] T(S) [SEP]\nWord-initial wordpieces in T(S) are labeled as\nB(egin), I(nside) or O(utside), while non-word-\ninitial wordpieces are labeled as X(ignore).\n5www.github.com/tmikolov/word2vec\nModeling, training and inference\nWe follow Lee et al. (2020)’s implementation\n(www.github.com/dmis-lab/biobert): We add\na randomly initialized softmax classiﬁer on top\nof the last BERT layer to predict the labels. We\nﬁnetune the entire model to minimize negative log\nlikelihood, with the AdamW optimizer (Loshchilov\nand Hutter, 2018) and a linear learning rate sched-\nuler (10% warmup). All ﬁnetuning runs were done\non a GeForce Titan X GPU (12GB).\nAt inference time, we gather the output logits\nof word-initial wordpieces only. Since the number\nof word-initial wordpieces is the same for TLM(S)\nand ˆTLM(S), this makes mean-pooling the logits\nstraightforward.\nHyperparameters\nWe tune the batch size and peak learning rate on\nthe development set (metric: F1), using the same\nhyperparameter space as Lee et al. (2020):\nBatch size: [10, 16, 32, 64]6\nLearning rate: [1 ·10−5, 3 ·10−5, 5 ·10−5]\nWe train for 100 epochs, which is the upper end\nof the 50–100 range recommended by the original\nauthors. After selecting the best conﬁguration for\nevery task and model (see Table 7), we train the\nﬁnal model on the concatenation of training and\ndevelopment set, as was done by Lee et al. (2020).\nSee Figure 2 for expected maximum development\nset F1 as a function of the number of evaluated hy-\nperparameter conﬁgurations (Dodge et al., 2019).\nExperiment 2: Covid-19 QA\nPretrained model\nWe downloaded the SQuADBERT baseline from:\n• www.huggingface.co/bert-large-\nuncased-whole-word-masking-\nfinetuned-squad\nData\nWe downloaded the Deepset-AI Covid-QA dataset\nfrom:\n• www.github.com/deepset-ai/COVID-\nQA/blob/master/data/question-\nanswering/COVID-QA.json [24 June 2020]\n6Since LINNAEUS and BC4CHEM have longer maximum\ntokenized chunk lengths than the other datasets, our hardware\nwas insufﬁcient to evaluate batch size 64 on them.\n1489\nAt the time of writing, the dataset contains 2019\nquestions and gold answer spans. Every question\nis associated with one of 147 research papers (con-\ntexts) from CORD-19.7 Since we do not do target-\ndomain ﬁnetuning, we treat the entire dataset as a\ntest set.\nPreprocessing\nWe tokenize every question-context pair (Q, C)\nwith T = TLM or T = ˆTLM, which yields\n(T(Q), T(C)). Since T(C) is usually too long\nto be digested in a single forward pass, we de-\nﬁne a sliding window with width and stride N =\nﬂoor(509−|T(Q)|\n2 ). At step n, the “active” win-\ndow is between a(l)\nn = (n −1)N + 1 and a(r)\nn =\nmin(|C|, nN). The input is deﬁned as:\nX(n) = [CLS] T(Q) [SEP]\nT(C)a(l)\nn −p(l)\nn :a(r)\nn +p(r)\nn\n[SEP]\np(l)\nn and p(r)\nn are chosen such that |X(n)|= 512 ,\nand such that the active window is in the center of\nthe input (if possible).\nModeling and inference\nFeeding X(n) into the QA model yields start log-\nits h′(start,n) ∈R|X(n)|and end logits h′(end,n) ∈\nR|X(n)|. We extract and concatenate the slices that\ncorrespond to the active windows of all steps:\nh(∗) ∈R|T(C)|\nh(∗) = [h′(∗,1)\na(l)\n1 :a(r)\n1\n; . . .; h′(∗,n)\na(l)\nn :a(r)\nn\n; . . .]\nNext, we map the logits from the wordpiece level\nto the word level. This allows us to mean-pool the\noutputs of TLM and ˆTLM even when |TLM(C)|̸=\n|ˆTLM(C)|.\nLet ci be a word in C and let T(C)j:j+|T(ci)|be\nthe corresponding wordpieces. The start and end\nlogits of ci are:\no(∗)\ni = maxj≤j′≤j+|T(ci)|[h(∗)\nj′ ]\nFinally, we return the answer span Ck:k′ that\nmaximizes o(start)\nk + o(end)\nk′ , subject to the con-\nstraints that k′does not precede k and the answer\ncontains no more than 500 characters.\n7www.github.com/deepset-ai/COVID-\nQA/issues/103\nNotes on Covid-QA\nThere are some important differences between\nCovid-QA and SQuAD, which make the task chal-\nlenging:\n•The Covid-QA contexts are full documents\nrather than single paragraphs. Thus, the cor-\nrect answer may appear several times, often\nwith slightly different wordings. But only a\nsingle occurrence is annotated as correct, e.g.:\nQuestion: What was the prevalence of Coro-\nnavirus OC43 in community samples in\nIlorin, Nigeria?\nCorrect: 13.3% (95% CI 6.9-23.6%) # from\nmain text\nPredicted: 13.3%, 10/75 # from abstract\n•SQuAD gold answers are deﬁned as the\n“shortest span in the paragraph that answered\nthe question” (Rajpurkar et al., 2016, p. 4),\nbut many Covid-QA gold answers are longer\nand contain non-essential context, e.g.:\nQuestion: When was the Middle East Res-\npiratory Syndrome Coronavirus isolated\nﬁrst?\nCorrect: (MERS-CoV) was ﬁrst isolated in\n2012, in a 60-year-old man who died in\nJeddah, KSA due to severe acute pneu-\nmonia and multiple organ failure\nPredicted: 2012\nThese differences are part of the reason why the\nexact match score is lower than the word-level F1\nscore and the substring score (see Table 6, bottom,\nmain paper).\n1490\nBERT (repro) BioBERTv1.0 (repro) GreenBioBERT\nBiomedical NER task (ID) hyperparams dev set F1 hyperparams dev set F1 hyperparams dev set F1\nBC5CDR-disease (1) 32, 3 · 10−5 82.12 10, 1 · 10−5 85.15 32, 1 · 10−5 83.90\nNCBI-disease (2) 32, 3 · 10−5 87.52 32, 1 · 10−5 87.99 10, 3 · 10−5 88.43\nBC5CDR-chem (3) 64, 3 · 10−5 91.00 32, 1 · 10−5 93.36 10, 1 · 10−5 92.59\nBC4CHEMD (4) 16, 1 · 10−5 88.02 32, 1 · 10−5 89.35 16, 1 · 10−5 88.53\nBC2GM (5) 32, 1 · 10−5 83.91 64, 3 · 10−5 85.54 64, 3 · 10−5 84.25\nJNLPBA (6) 32, 5 · 10−5 85.18 32, 5 · 10−5 85.30 10, 3 · 10−5 85.10\nLINNAEUS (7) 16, 1 · 10−5 96.67 32, 1 · 10−5 97.22 10, 1 · 10−5 96.49\nSpecies-800 (8) 32, 1 · 10−5 72.70 32, 1 · 10−5 77.34 16, 1 · 10−5 75.93\nTable 7: Best hyperparameters (batch size, peak learning rate) and best dev set F1 per NER task and model. BERT\n(repro) and BioBERTv1.0 (repro) refer to our reproduction experiments.\n3 6 9 12\n82\n84 (1)\n3 6 9 12\n87\n88\n(2)\n3 6 9 12\n91\n92\n93\n(3)\n3 6 9\n87\n88\n89\n(4)\n3 6 9 12\n84\n85\n(5)\n3 6 9 12\n84\n85\n(6)\n3 6 9\n95\n96\n97\n(7)\n3 6 9 12\n72\n74\n76\n(8)\n0.0 0.2 0.4 0.6 0.8 1.0\nNumber of hyperparameter configurations evaluated\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nExpected max dev set F1\nBERT (repro) BioBERTv1.0 (repro) GreenBioBERT\nFigure 2: Expected maximum F1 on NER development sets as a function of the number of evaluated hyperparam-\neter conﬁgurations. Numbers in brackets are NER task IDs (see Table 7)."
}