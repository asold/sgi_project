{
  "title": "TheanoLM — An Extensible Toolkit for Neural Network Language Modeling",
  "url": "https://openalex.org/W2345764277",
  "year": 2016,
  "authors": [
    {
      "id": null,
      "name": "Enarvi, Seppo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221772979",
      "name": "Kurimo, Mikko",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W136130055",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1606347560",
    "https://openalex.org/W2936995161",
    "https://openalex.org/W2399981156",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2964076070",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2142937603",
    "https://openalex.org/W1637614433",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2299662909",
    "https://openalex.org/W6908809",
    "https://openalex.org/W2293185259",
    "https://openalex.org/W3140710042",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2056590938",
    "https://openalex.org/W2408752871",
    "https://openalex.org/W2474824677",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2146502635"
  ],
  "abstract": "We present a new tool for training neural network language models (NNLMs),\\nscoring sentences, and generating text. The tool has been written using Python\\nlibrary Theano, which allows researcher to easily extend it and tune any aspect\\nof the training process. Regardless of the flexibility, Theano is able to\\ngenerate extremely fast native code that can utilize a GPU or multiple CPU\\ncores in order to parallelize the heavy numerical computations. The tool has\\nbeen evaluated in difficult Finnish and English conversational speech\\nrecognition tasks, and significant improvement was obtained over our best\\nback-off n-gram models. The results that we obtained in the Finnish task were\\ncompared to those from existing RNNLM and RWTHLM toolkits, and found to be as\\ngood or better, while training times were an order of magnitude shorter.\\n",
  "full_text": "arXiv:1605.00942v2  [cs.CL]  8 Aug 2016\nTheanoLM — An Extensible Toolkit for Neural Network Language Modeling\nSeppo Enarvi, Mikko Kurimo\nAalto University, Finland\nfirstname.lastname@aalto.fi\nAbstract\nWe present a new tool for training neural network language mod-\nels (NNLMs), scoring sentences, and generating text. The tool\nhas been written using Python library Theano, which allows re-\nsearcher to easily extend it and tune any aspect of the training\nprocess. Regardless of the ﬂexibility, Theano is able to gener-\nate extremely fast native code that can utilize a GPU or multi-\nple CPU cores in order to parallelize the heavy numerical com-\nputations. The tool has been evaluated in difﬁcult Finnish and\nEnglish conversational speech recognition tasks, and signiﬁcant\nimprovement was obtained over our best back-off n-gram mod-\nels. The results that we obtained in the Finnish task were com-\npared to those from existing RNNLM and RWTHLM toolkits,\nand found to be as good or better, while training times were an\norder of magnitude shorter.\nIndex Terms: language modeling, artiﬁcial neural networks,\nautomatic speech recognition, conversational language\n1. Introduction\nNeural network language models (NNLM) are known to outper-\nform traditional n-gram language models in speech recognition\naccuracy [1, 2]. For modeling word sequences with temporal de-\npendencies, the recurrent neural network (RNN) is an attractive\nmodel as it is not limited to a ﬁxed window size. Perhaps the\nsimplest variation of RNN that has been used for language mod-\neling contains just one hidden layer [3]. The ability of an RNN\nto model temporal dependencies is limited by the vanishing gra-\ndient problem. Various modiﬁcations have been proposed to\nthe standard RNN structure that reduce this problem, such as\nthe popular long short-term memory (LSTM) [4].\nFigure 1 shows a typical LSTM network. Following the ar-\nchitecture by Bengio et al [5], the ﬁrst layer projects the words\nwt into a continuous lower-dimensional vector space, which\nis followed by a hidden layer. In recurrent networks the hid-\nden layer stateht is passed to the next time step. LSTM is a\nspecial case of a recurrent layer that also passescell stateCt.\nSigmoid gates control what information will be added to or re-\nmoved from the cell state, making it easy to maintain important\ninformation in the cell state over extended periods of time.The\nﬁnal neural network layer normalizes the output probabilities\nusingsoftmax .\nAnother inﬂuential design choice concerns performance\nwhen the vocabulary is large. The computational cost of train-\ning and evaluating a network with softmax output layer is highly\ndependent on the number of output neurons, i.e. the size of the\nvocabulary. Feedforward networks are basically n-gram models,\nso it is straightforward to use the neural network to predictonly\nwords in a short-list and use a back-off model for the infrequent\nwords [6]. Replacing a single softmax layer with a hierarchy\nof softmax layers [7] improves performance, although the num-\nber of model parameters is not reduced. Using word classes in\nP (w1|w0) P (w2|w1, w0) P (w3|w2, w1, w0)\nsoftmax softmax softmax\ntanh tanh tanh\nLSTM LSTM LSTM\nprojection projection projection\nw0 w1 w2\nC0 C1 C2\nh0 h1 h2\nFigure 1:Recurrent NNLM with LSTM andtanh hidden layers.\n1-of-N encoded wordswt are projected into lower-dimensional\nvectors. An LSTM layer passes the hidden stateht to the next\ntime step, like a standard recurrent layer, but also the cellstate\nCt, which is designed to convey information over extended time\nintervals.\nthe input and output of the network [8] has the beneﬁt that the\nmodel becomes smaller, which may be necessary for using a\nGPU.\nWhile several toolkits have been made available for lan-\nguage modeling with neural networks [6, 8, 9], they do not\nsupport research and prototyping work well. The most impor-\ntant limitation is the difﬁculty in extending the toolkits with\nrecently proposed methods and different network architectures.\nAlso, training large networks is very slow without GPU support.\nTheanoLM is a new NNLM tool that we have developed, mo-\ntivated by our ongoing research on improving conversational\nFinnish automatic speech recognition (ASR). Our goal is to\nmake it versatile and fast, easy to use, and write code that is\neasily extensible.\nIn the next section we will give an introduction on how\nTheanoLM works. Then we will evaluate it on two conversa-\ntional ASR tasks with large data sets. In order to verify that\nit works correctly, the results will be compared to those from\nother toolkits.\ninput type=class name=class_input\nlayer type=projection name=projection_layer input=class_input size=5/zero.noslash/zero.noslash\nlayer type=dropout name=dropout_layer_1 input=projection_layer dropout_rate=/zero.noslash.25\nlayer type=lstm name=hidden_layer_1 input=dropout_layer_1 size=15/zero.noslash/zero.noslash\nlayer type=dropout name=dropout_layer_2 input=hidden_layer_1 dropout_rate=/zero.noslash.25\nlayer type=tanh name=hidden_layer_2 input=dropout_layer_2 size=15/zero.noslash/zero.noslash\nlayer type=dropout name=dropout_layer_3 input=hidden_layer_2 dropout_rate=/zero.noslash.25\nlayer type=softmax name=output_layer input=dropout_layer_3\nFigure 2:An example of a network architecture description.\n2. TheanoLM\nTheano is a Python library for high-performance mathematical\ncomputation [10]. It provides a versatile interface for build-\ning neural network applications, and has been used in many\nneural network tasks. For language modeling, toolkits suchas\nRNNLM are more popular, because they are easier to approach.\nWe have developed TheanoLM as a complete package for train-\ning and applying recurrent neural network language models in\nspeech recognition and machine translation, as well as generat-\ning text by sampling from an NNLM. It is freely available on\nGitHub1.\nThe neural network is represented in Python objects as a\nsymbolic graph of mathematical expressions. Theano performs\nsymbolic differentiation, making it easy to implement gradient-\nbased optimization methods. We have already implemented\nNesterov’s Accelerated Gradient [11], Adagrad [12], Adadelta\n[13], Adam [14], and RMSProp optimizers, in addition to plain\nStochastic Gradient Descent (SGD).\nEvaluation of the expressions is performed using native\nCPU and GPU code transparently. The compilation does intro-\nduce a short delay before the program can start to train or usea\nmodel, but in a typical application the delay is negligible com-\npared to the actual execution. On the other hand, the execution\ncan be highly parallelized using a GPU, speeding up trainingof\nlarge networks to a fraction of CPU training times.\nStandard SGD training is very sensitive to the learning rate\nhyperparameter. The initial value should be as high as possi-\nble, given that the optimization still converges. Gradually de-\ncreasing (annealing) the learning rate enables ﬁner adjustments\nlater in the optimization process. TheanoLM can perform cross-\nvalidation on development data at regular intervals, in order to\ndecide how quickly annealing should occur. However, adap-\ntive learning rate methods such as Adagrad and Adadelta do not\nrequire manual tuning of the learning rate—cross-validation is\nneeded only for determining when to stop training.\nEspecially in Finnish and other highly agglutinative lan-\nguages the vocabulary is too large for the ﬁnal layer to predict\nwords directly. In this work we use class-based models, where\neach wordwt belongs to exactly one word classc(wt):\nP (wt|wt− 1 . . . ) =P (c(wt)|c(wt− 1) . . . )P (wt|c(wt)) (1)\nWhen the classes are chosen carefully, this model will not\nnecessarily perform worse than a word-based model in Finnish\ndata, because there are not enough examples of the rarer words\nto give robust estimates of word probabilities. The advantage\n1https://github.com/senarvi/theanolm\nof this solution is that the model size depends on the number of\nclasses instead of the number of words.\nAn arbitrary network architecture can be provided in a text\nﬁle as a list of layer descriptions. The layers have to speciﬁed\nin the order that the network is constructed, meaning that the\nnetwork has to be acyclic. The network may contain word and\nclass inputs, which have to be followed by a projection layer.\nA projection layer may be followed by any number of LSTM\nand GRU [15] layers, as well as non-recurrent layers. The ﬁ-\nnal layer has to be a softmax or a hierarchical softmax layer.\nMultiple layers may have the same element in their input, anda\nlayer may have multiple inputs, in which case the inputs willbe\nconcatenated.\nThe example in Figure 2 would create an LSTM network\nwith a projection layer of 500 neurons and two hidden layers\nof 1500 neurons. After each of those layers is a dropout [16]\nlayer, which contains no neurons, but only sets some activations\nrandomly to zero at train time to prevent overﬁtting. This isthe\nconﬁguration that we have used in this paper to train the larger\nTheanoLM models.\nLSTM and GRU architectures help avoid the vanishing gra-\ndient problem. In order to prevent gradients from exploding, we\nlimit the norm of the gradients, as suggested by Mikolov in his\nthesis [2].\n3. Conversational Finnish ASR Experiment\n3.1. Data\nIn order to develop models for conversational Finnish ASR,\nwe have recorded and transcribed conversations held by stu-\ndents in pairs, in the basic course in digital signal processing\nat Aalto University. The collected corpus, calledAalto Univer-\nsity DSP Course Conversation Corpus (DSPCON)is available\nfor research use in the Language Bank of Finland2. The cor-\npus is updated annually; currently it includes recordings from\nyears 2013–2015. In addition we have used the spontaneous\nsentences of SPEECON corpus, FinDialogue subcorpus of Fin-\nINTAS 3, and some transcribed radio conversations, totaling 34\nhours of acoustic training data. This is practically all thecon-\nversational Finnish data that is available for research.\nWe have collected language modeling data from the Inter-\nnet, and ﬁltered it to match transcribed conversations [18]. Simi-\nlar data is available in the Language Bank of Finland4. This was\naugmented with 43,000 words of transcribed spoken conversa-\ntions from DSPCON, totaling 76 million words. A 8,900-word\ndevelopment set was used in language modeling.\n2http://urn.fi/urn:nbn:fi:lb-2/zero.noslash151/zero.noslash19/zero.noslash1\n3http://urn.fi/urn:nbn:fi:lb-2/zero.noslash14/zero.noslash73/zero.noslash194\n4http://urn.fi/urn:nbn:fi:lb-2/zero.noslash1412171\nTable 1:Language model training times and word error rates\n(%) given by the model alone and interpolated with the best\nKneser-Ney model. Finnish results are from the evaluation\nset, but the same set was used for optimizing language model\nweights.\nTraining Dev Eval\nModel time Alone Int. Alone Int.\nFinnish\nKN word 4-gram 7 min 52.1\nKN word 4-gram weighted 19 min 51.0\nKN class 4-gram weighted 55 min 51.2 50.5\nRNNLM 300 361 h 50.4 49.8\nRWTHLM 100+300+300 480 h † 49.4 48.6\nTheanoLM 100+300+300 20 h 49.1 49.0\nTheanoLM 500+1500+1500 139 h 48.7 48.4\nEnglish\nKN word 4-gram 8 min 42.1 41.9\nKN word 4-gram weighted 15 min 41.0 41.2\nTheanoLM 100+300+300 62 h 40.1 39.6 41.2 40.5\nTheanoLM 500+1500+1500 290 h 39.6 39.5 40.5 40.0\n†Using 12 CPUs.\nA set of 541 sentences from unseen speakers, totaling 44\nminutes and representing the more natural spontaneous speech\nof various topics, was used for evaluation. Because of the nu-\nmerous ways in which conversational Finnish can be written\ndown, ASR output should be evaluated on transcripts that con-\ntain such alternative word forms. As we did not have another\nsuitable evaluation set, the same data was used for optimizing\nlanguage model weights. The lattices were generated using\nAalto ASR [17] and a triphone HMM acoustic model.\n3.2. Models\nIn this task we have obtained results also from other freely avail-\nable NNLM toolkits, RWTHLM [8] and RNNLM [9], for com-\nparison. With RWTHLM and TheanoLM we have used one\nLSTM layer and onetanh layer on top of the projection layer\n(100+300+300 neurons), as in Figure 1. RNNLM supports only\na simple recurrent network with one hidden layer. Because of\nthe faster training time with TheanoLM, we were also able to\ntry a larger model with ﬁve times the number of neurons in each\nlayer. This model includes dropout after each layer. The archi-\ntecture description is given in Figure 2. We trained also models\nwith third existing toolkit, CSLM [6], but were unable to get\nmeaningful sentence scores. CSLM supports only feedforward\nnetworks, but is very fast because of GPU support.\nThe toolkits also differ in how they handle the vocabulary.\nWith RWTHLM and TheanoLM we used word classes. 2000\nword classes were created using the exchange algorithm [19],\nwhich tries to optimize the log probability of a bigram modelon\nthe training data, by moving words between classes. RNNLM\ncreates classes by frequency binning, but uses words in the input\nand output of the neural network. Classes are used for decom-\nposition of the output layer, which speeds up training and eval-\nuation [20], but with millions of words in the vocabulary, the\nnumber of parameters in the RNNLM model with 300 neurons\nis larger than in the RWTHLM and TheanoLM models with\n100+300+300 neurons.\nTable 2:Development and evaluation set perplexities from the\nfull-vocabulary models.\nPerplexity\nModel Dev Eval\nFinnish\nKN class 4-gram weighted 755 763\nRWTHLM 100+300+300 687 743\nRNNLM 300 881 872\nTheanoLM 100+300+300 677 701\nTheanoLM 500+1500+1500 609 642\nEnglish\nKN class 4-gram weighted 98 91\nTheanoLM 100+300+300 102 99\nTheanoLM 500+1500+1500 90 88\nWith TheanoLM we have used Adagrad optimizer without\nannealing. While we could not evaluate different optimizers\nextensively, Adagrad seemed to be among the best in terms of\nboth speed of convergence and performance of the ﬁnal model.\nNesterov’s Accelerated Gradient with manual annealing gave\na slightly better model with considerably longer training time.\nThe other toolkits use standard SGD.\nKneser-Ney smoothed 4-grams were used in the back-off\nmodels. The data sets collected from different sources varied\nin size and quality. Instead of pooling all the data together, the\nbaseline back-off models were weighted mixtures of models es-\ntimated from each subset. The weights were optimized using\ndevelopment data. The back-off model vocabulary was limited\nto 200,000 of the 2,400,000 different word forms that occurred\nin the training data, selected with the EM algorithm to maximize\nthe likelihood of the development data [21]. Out-of-vocabulary\nrate in the evaluation data was 5.06 %.\n3.3. Results\nEvaluation of neural network probabilities is too slow to beus-\nable in the ﬁrst decoding pass of a large-vocabulary continuous\nspeech recognizer. Another pass is performed by decoding and\nrescoring word lattices created using a traditional n-grammodel.\nRWTHLM is able to rescore word lattices directly, the other\ntoolkits can only rescore n-best lists created from word lattices.\nWord error rates (WER) after rescoring are shown in Ta-\nble 1. The table also includes word error rates given by inter-\npolation of NNLM scores with theKN word 4-gram weighted\nmodel. We interpolated the sentence scores (log probabilities)\nas\nlog P = (1− λ)sbo log Pbo(w1 . . . w n)\n+ λsnn log Pnn(w1 . . . w n). (2)\nsbo is the LM scale factor that was optimal for the back-off\nmodel. The same value was used for generating the n-best lists.\nsnn and λ, the NNLM scale factor and interpolation weight,\nwere optimized from a range of values.\nV ocabulary size affects perplexity computation, so we have\nomitted the limited-vocabulary models from the perplexityre-\nsults in Table 2. Finnish vocabulary was ﬁve times larger, so\nthe perplexities are considerably higher than in the English lan-\nguage experiment, as expected. The values are as reported by\neach tool; there might be differences in e.g. how unknown\nwords and sentence starts and ends are handled. Perplexi-\nties of the Kneser-Ney models were computed using SRILM,\nwhich excludes unknown words from the computation. With\nTheanoLM we used the same behaviour (--unk-penalty 0).\nWe have also recorded the training times in Table 1, al-\nthough it has to be noted that the jobs were run in a compute\ncluster that assigns them to different hardware, so the reported\ndurations are only indicative. RWTHLM supports paralleliza-\ntion through various math libraries. RNNLM is able to use only\none CPU core, which means that the computation is inevitably\nslow. TheanoLM was used with an Nvidia Tesla GPU.\nWhen rescoring ASR output, all neural network models out-\nperformed the back-off models, even without interpolationwith\nback-off scores. Since we get the back-off model scores from\nthe ﬁrst decoding pass without additional cost, it is reasonable to\nlook at the performance when both models are combined with\ninterpolation. This further improves the results. The back-off\nmodel is clearly improved by weighting individual corpora,be-\ncause the different corpora are not homogeneous. At the time\nof writing we have implemented training set weighting schemes\nin TheanoLM as well, but so far the improvements have been\nsmaller than with the well-established back-off model weight-\ning.\nRWTHLM and RNNLM were stopped after 8 training\nepochs. RNNLM did not improve the baseline as much as ex-\npected, but training further iterations could have improved its\nperformance.\n4. English Meeting Recognition Experiment\nFor the English task we used more advanced acoustic mod-\nels trained using Kaldi [22] with discriminative training using\nthe maximum mutual information (MMI) criterion. The lat-\ntices were generated using maximum likelihood linear regres-\nsion (MLLR) speaker adaptation. The acoustic training data\nwas 73 hours from ICSI Meeting Corpus5. For language model\ntraining we used also part 2 of the Fisher corpus6 and match-\ning data from the Internet. In total the text data contained 159\nmillion words, of which 11 million words were transcribed con-\nversations. The development and evaluation data were from\nthe NIST Rich Transcription 2004 Spring Meeting Recognition\nEvaluation. The development set contained 18,000 words and\nthe evaluation set was 104 minutes and consisted of 2450 utter-\nances.\nIn the back-off models, vocabulary was limited to 50,000\nof the 470,000 different words that occurred in training data.\nOnly 0.30 % of the evaluation set tokens were left outside the\nvocabulary. TheanoLM was used to train models of the same ar-\nchitecture using the same parameters as in the Finnish task.In\nthis task we had roughly twice the amount of data. Training was\nmore than two times slower, but still manageable. The results\nin Table 1 show that the beneﬁt from the larger network is pro-\nnounced in this task. The smaller network is clearly incapable\nof modeling the larger data set as well.\n5. Conclusions\nSeveral different NNLM toolkits, implemented in C++, are\nfreely available. However, the ﬁeld is rapidly changing, and\na toolkit should be easily extensible with the latest algorithms.\n5https://catalog.ldc.upenn.edu/LDC2/zero.noslash/zero.noslash4S/zero.noslash2\n6https://catalog.ldc.upenn.edu/LDC2/zero.noslash/zero.noslash5T19\nAlso, different languages and data sets work best with differ-\nent algorithms and architectures, so a good solution needs to\nbe versatile. We offer a new toolkit that has been implemented\nusing Python and Theano, provides implementations of the lat-\nest training methods for artiﬁcial neural networks, and is easily\nextensible. Another important advantage is speed—Theano pro-\nvides highly optimized C++ implementations of the expensive\nmatrix operations, and can automatically utilize a GPU.\nWhile back-off n-gram models are still two orders of mag-\nnitude faster to train, training a model using TheanoLM was an\norder of magnitude faster than training a similar model withthe\nother toolkits. The speed advantage of TheanoLM was mainly\ndue to GPU support, but the Adagrad optimizer that we used\nwith TheanoLM was also faster to converge. 4 epochs were\nrequired for convergence using Adagrad, while the other toolk-\nits continued to improve perplexity for at least 8 epochs. The\nfaster training time makes it practical to train larger networks\nwith TheanoLM. When more data is available, the beneﬁt of a\nlarger network becomes pronounced, and training without GPU\nsupport becomes impractical.\nIn a conversational Finnish speech recognition task we have\nused practically all the data that is available for research. A\n4-gram Kneser-Ney model, trained simply by concatenating\nthe training data, gave us 52.1 % WER. As a baseline we\ntook a mixture model that was combined from smaller mod-\nels with optimized interpolation weights. The baseline model\ngave 51.0 % WER. 48.4 % WER was reached when interpolat-\ning TheanoLM and baseline LM scores, a relative improvement\nof 5.1 %. RWTHLM gives similar results with a similar neu-\nral network architecture, which increases our conﬁdence inthe\ncorrectness of the implementation.\nEvaluating the progress we have made in the conversational\nFinnish task, 48.4 % WER is 10.5 % better than our previous\nrecord, 54.1 % [18]. However, we have collected new acoustic\ndata, which explains the 51.0 % baseline in this paper. The\nacoustic models used in these experiments are still not state of\nthe art. In order to ﬁnd out the absolute performance of these\nmodels, we will also need to obtain a proper development set\nfor optimizing language model scale and interpolation weight.\nIt appears that some types of NNLMs work better with one\nlanguage than another. RNNLM did not perform as well as we\nexpected in the Finnish task, probably because the network ar-\nchitecture was not optimal. RNNLM offers only a simple net-\nwork architecture that takes words as input and contains one\nhidden layer. The number of input connections is huge with the\nFinnish vocabulary, and the size of the hidden layer is limited if\nwe want training time to be reasonable. Previously interpolating\nRNNLM with a Kneser-Ney model has been shown to give 3 to\n8 % relative improvement in conversational English ASR [23].\nIn the English meeting recognition task more data was avail-\nable, and the smaller neural network was not better than the\nweighted mixture back-off model. Still the neural network\nbrings new information so much that interpolation of the smaller\nNNLM scores with back-off scores improves from the baseline\nof 41.2 % WER to 40.5 %. The larger network brings 2.9 %\nimprovement to 40.0 % WER. When more data is available,\na larger network is needed, which makes training speed even\nmore important.\nSo far our research focus has been on conversational\nFinnish and other highly agglutinative languages. Considering\nthat as much effort has not gone into the English task, the re-\nsults are satisfactory, and show that TheanoLM works also ina\nrelatively standard conversational English task.\n6. Acknowledgements\nThis work was ﬁnancially supported by the Academy of Finland\nunder the grant number 251170 and made possible by the com-\nputational resources provided by the Aalto Science-IT project.\n7. References\n[1] H. Schwenk and J. L. Gauvain, “Connectionist language\nmodeling for large vocabulary continuous speech recog-\nnition,” inProceedings of the 2002 IEEE International\nConference on Acoustics, Speech, and Signal Processing\n(ICASSP), vol. 1, May 2002, pp. I–765–I–768.\n[2] T. Mikolov, “Statistical language models based on\nneural networks,” Ph.D. dissertation, Brno Uni-\nversity Of Technology, 2012. [Online]. Available:\nhttp://www.ﬁt.vutbr.cz/research/view_pub.php?id=10158\n[3] T. Mikolov, M. Karaﬁát, L. Burget, J. Cernocký,\nand S. Khudanpur, “Recurrent neural network\nbased language model,” in Proceedings of the\n11th Annual Conference of the International\nSpeech Communication Association (INTERSPEECH),\nSep. 2010, pp. 1045–1048. [Online]. Available:\nhttp://www.isca-speech.org/archive/interspeech_2010/i10_1045.html\n[4] S. Hochreiter and J. Schmidhuber, “Long short-\nterm memory,” Neural Computation, vol. 9, no. 8,\npp. 1735–1780, Nov. 1997. [Online]. Available:\nhttp://dx.doi.org/10.1162/neco.1997.9.8.1735\n[5] Y . Bengio, R. Ducharme, P. Vincent, and C. Jan-\nvin, “A neural probabilistic language model,”The\nJournal of Machine Learning Research , vol. 3,\npp. 1137–1155, Mar. 2003. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=944919.944966\n[6] H. Schwenk, “CSLM - a modular open-source continuous\nspace language modeling toolkit,” inProceedings\nof the 14th Annual Conference of the International\nSpeech Communication Association (INTERSPEECH),\nAug. 2013, pp. 1198–1202. [Online]. Available:\nhttp://www.isca-speech.org/archive/interspeech_2013/i13_1198.html\n[7] F. Morin and Y . Bengio, “Hierarchical probabilistic\nneural network language model,” inProceedings of the\n10th International Workshop on Artiﬁcial Intelligence and\nStatistics (AISTATS). Society for Artiﬁcial Intelligence\nand Statistics, 2005, pp. 246–252. [Online]. Available:\nhttp://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf\n[8] M. Sundermeyer, R. Schlüter, and H. Ney, “rwthlm — the\nRWTH Aachen University neural network language mod-\neling toolkit,” inProceedings of the 15th Annual Confer-\nence of the International Speech Communication Associa-\ntion (INTERSPEECH), Sep. 2014, pp. 2093–2097.\n[9] T. Mikolov, S. Kombrink, A. Deoras, L. Burget, and J.ˇCer-\nnocký, “RNNLM - recurrent neural network language\nmodeling toolkit,” inProceedings of the 2011 IEEE Work-\nshop on Automatic Speech Recognition and Understand-\ning (ASRU). IEEE Signal Processing Society, Dec. 2011,\niEEE Catalog No.: CFP11SRW-USB. [Online]. Available:\nhttp://www.ﬁt.vutbr.cz/research/view_pub.php?id=10087\n[10] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J.\nGoodfellow, A. Bergeron, N. Bouchard, and Y . Bengio,\n“Theano: new features and speed improvements,” NIPS\n2012 Workshop on Deep Learning and Unsupervised Fea-\nture Learning, 2012.\n[11] Y . Nesterov, “A method of solving a convex programming\nproblem with convergence rateO(1/k 2),”Soviet Mathe-\nmatics Doklady, vol. 27, no. 2, pp. 372–376, 1983.\n[12] J. Duchi, E. Hazan, and Y . Singer, “Adaptive subgradient\nmethods for online learning and stochastic optimization,”\nThe Journal of Machine Learning Research, vol. 12, pp.\n2121–2159, Jul. 2011.\n[13] M. D. Zeiler, “ADADELTA: an adaptive learn-\ning rate method,” Computing Research Reposi-\ntory, vol. abs/1212.5701, 2012. [Online]. Available:\nhttp://arxiv.org/abs/1212.5701\n[14] D. P. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” inProceedings of the International\nConference on Learning Representations (ICLR) 2015,\n2015. [Online]. Available: http://arxiv.org/abs/1412.6980\n[15] K. Cho, B. van Merrienboer, Ç. Gülçehre, F. Bougares,\nH. Schwenk, and Y . Bengio, “Learning phrase rep-\nresentations using RNN encoder-decoder for statistical\nmachine translation,” inProceedings of the 2014\nConference on Empiricial Methods in Natural Lan-\nguage Processing (EMNLP), 2014. [Online]. Available:\nhttp://arxiv.org/abs/1406.1078\n[16] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,\nand R. Salakhutdinov, “Dropout: A simple way\nto prevent neural networks from overﬁtting,”The\nJournal of Machine Learning Research , vol. 15,\nno. 1, pp. 1929–1958, Jan. 2014. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=2627435.2670313\n[17] T. Hirsimäki, J. Pylkkönen, and M. Kurimo, “Impor-\ntance of high-order n-gram models in morph-based speech\nrecognition,”IEEE Transactions on Audio, Speech, and\nLanguage Processing, vol. 17, no. 4, pp. 724–732, 2009.\n[18] M. Kurimo, S. Enarvi, O. Tilk, M. Varjokallio, A. Man-\nsikkaniemi, and T. Alumäe, “Modeling under-resourced\nlanguages for speech recognition,”Language Resources\nand Evaluation (LRE), 2016.\n[19] R. Kneser and H. Ney, “Forming word classes\nby statistical clustering for statistical language mod-\nelling,” in Contributions to Quantitative Linguis-\ntics, R. Köhler and B. Rieger, Eds. Springer\nNetherlands, 1993, pp. 221–226. [Online]. Available:\nhttp://dx.doi.org/10.1007/978-94-011-1769-2_15\n[20] T. Mikolov, S. Kombrink, L. Burget, J.ˇCernocký, and\nS. Khudanpur, “Extensions of recurrent neural network\nlanguage model,” inProceedings of the 2011 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), May 2011, pp. 5528–5531.\n[21] A. Venkataraman and W. Wang, “Techniques for effective\nvocabulary selection,” inProceedings of the 8th European\nConference on Speech Communication and Technol-\nogy (EUROSPEECH) , Sep. 2003. [Online]. Available:\nhttp://www.isca-speech.org/archive/eurospeech_2003/e03_0245.html\n[22] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glem-\nbek, N. Goel, M. Hannemann, P. Motlicek, Y . Qian,\nP. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, “The\nKaldi speech recognition toolkit,” inProceedings of the\n2011 IEEE Workshop on Automatic Speech Recognition\nand Understanding (ASRU). IEEE Signal Processing So-\nciety, Dec. 2011, iEEE Catalog No.: CFP11SRW-USB.\n[23] S. Kombrink, T. Mikolov, M. Karaﬁát, and L. Bur-\nget, “Recurrent neural network based language\nmodeling in meeting recognition,” inProceedings\nof the 12th Annual Conference of the International\nSpeech Communication Association (INTERSPEECH),\nAug. 2011, pp. 2877–2880. [Online]. Available:\nhttp://www.isca-speech.org/archive/interspeech_2011/i11_2877.html",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8369816541671753
    },
    {
      "name": "Extensibility",
      "score": 0.6979734897613525
    },
    {
      "name": "Artificial neural network",
      "score": 0.49750855565071106
    },
    {
      "name": "Programming language",
      "score": 0.46482163667678833
    },
    {
      "name": "Software engineering",
      "score": 0.36315423250198364
    },
    {
      "name": "Natural language processing",
      "score": 0.33854466676712036
    },
    {
      "name": "Artificial intelligence",
      "score": 0.330649733543396
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9927081",
      "name": "Aalto University",
      "country": "FI"
    }
  ],
  "cited_by": 30
}