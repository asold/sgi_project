{
  "title": "PlanCollabNL: Leveraging Large Language Models for Adaptive Plan Generation in Human-Robot Collaboration",
  "url": "https://openalex.org/W4401413669",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Izquierdo-Badiola, Silvia",
      "affiliations": [
        "Centre Tecnologic de Telecomunicacions de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A2751723044",
      "name": "Canal Gerard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751638440",
      "name": "Rizzo Carlos",
      "affiliations": [
        "Centre Tecnologic de Telecomunicacions de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A3177483345",
      "name": "Alenyà, Guillem",
      "affiliations": [
        "Universitat Politècnica de Catalunya"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285102170",
    "https://openalex.org/W2767074019",
    "https://openalex.org/W6810640255",
    "https://openalex.org/W6809509765",
    "https://openalex.org/W6754154301",
    "https://openalex.org/W6802963923",
    "https://openalex.org/W4231966580",
    "https://openalex.org/W4240985012",
    "https://openalex.org/W2222794653",
    "https://openalex.org/W2763497381",
    "https://openalex.org/W2945696949",
    "https://openalex.org/W4389665836",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4388720459",
    "https://openalex.org/W6854555012",
    "https://openalex.org/W6850503672",
    "https://openalex.org/W4386215566",
    "https://openalex.org/W6839928859",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W6853035495",
    "https://openalex.org/W6850072970",
    "https://openalex.org/W6845648307",
    "https://openalex.org/W6852136651",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2119709400",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W3174464510",
    "https://openalex.org/W6777615688",
    "https://openalex.org/W4388624208",
    "https://openalex.org/W4366999541",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W2595229763",
    "https://openalex.org/W4302305808",
    "https://openalex.org/W2912448913",
    "https://openalex.org/W2502094132",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4320559489"
  ],
  "abstract": "<i>\"Hey, robot. Let's tidy up the kitchen. By the way, I have back pain today\"</i>. How can a robotic system devise a shared plan with an appropriate task allocation from this abstract goal and agent condition? Classical AI task planning has been explored for this purpose, but it involves a tedious definition of an inflexible planning problem. Large Language Models (LLMs) have shown promising generalisation capabilities in robotics decision-making through knowledge extraction from Natural Language (NL). However, the translation of NL information into constrained robotics domains remains a challenge. In this paper, we use LLMs as translators between NL information and a structured AI task planning problem, targeting human-robot collaborative plans. The LLM generates information that is encoded in the planning problem, including specific subgoals derived from an NL abstract goal, as well as recommendations for subgoal allocation based on NL agent conditions. The framework, PlanCollabNL, is evaluated for a number of goals and agent conditions, and the results show that correct and executable plans are found in most cases. With this framework, we intend to add flexibility and generalisation to HRC plan generation, eliminating the need for a manual and laborious definition of restricted planning problems and agent models.",
  "full_text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nKing’s Research Portal \n \nDOI:\n10.1109/ICRA57147.2024.10610055\nDocument Version\nPeer reviewed version\nLink to publication record in King's Research Portal\nCitation for published version (APA):\nIzquierdo-Badiola, S., Canal, G., Rizzo, C., & Alenyà, G. (2024). PlanCollabNL: Leveraging Large Language\nModels for Adaptive Plan Generation in Human-Robot Collaboration. In 2024 IEEE International Conference on\nRobotics and Automation (ICRA) (pp. 17344-17350). (Proceedings - IEEE International Conference on Robotics\nand Automation). IEEE. https://doi.org/10.1109/ICRA57147.2024.10610055\nCiting this paper\nPlease note that where the full-text provided on King's Research Portal is the Author Accepted Manuscript or Post-Print version this may\ndiffer from the final Published version. If citing, it is advised that you check and use the publisher's definitive version for pagination,\nvolume/issue, and date of publication details. And where the final published version is provided on the Research Portal, if citing you are\nagain advised to check the publisher's website for any subsequent corrections.\nGeneral rights\nCopyright and moral rights for the publications made accessible in the Research Portal are retained by the authors and/or other copyright\nowners and it is a condition of accessing publications that users recognize and abide by the legal requirements associated with these rights.\n•Users may download and print one copy of any publication from the Research Portal for the purpose of private study or research.\n•You may not further distribute the material or use it for any profit-making activity or commercial gain\n•You may freely distribute the URL identifying the publication in the Research Portal\nTake down policy\nIf you believe that this document breaches copyright please contact librarypure@kcl.ac.uk providing details, and we will remove access to\nthe work immediately and investigate your claim.\nDownload date: 05. Nov. 2025\nPlanCollabNL: Leveraging Large Language Models for Adaptive Plan\nGeneration in Human-Robot Collaboration\nSilvia Izquierdo-Badiola1,3, Gerard Canal 2, Carlos Rizzo 1 and Guillem Aleny `a3\nAbstract— “Hey, robot. Let’s tidy up the kitchen. By the way, I\nhave back pain today”. How can a robotic system devise a shared\nplan with an appropriate task allocation from this abstract goal\nand agent condition? Classical AI task planning has been ex-\nplored for this purpose, but it involves a tedious definition of an\ninflexible planning problem. Large Language Models (LLMs)\nhave shown promising generalisation capabilities in robotics\ndecision-making through knowledge extraction from Natural\nLanguage (NL). However, the translation of NL information\ninto constrained robotics domains remains a challenge. In this\npaper, we use LLMs as translators between NL information\nand a structured AI task planning problem, targeting human-\nrobot collaborative plans. The LLM generates information that\nis encoded in the planning problem, including specific subgoals\nderived from an NL abstract goal, as well as recommendations\nfor subgoal allocation based on NL agent conditions. The\nframework, PlanCollabNL, is evaluated for a number of goals\nand agent conditions, and the results show that correct and\nexecutable plans are found in most cases. With this framework,\nwe intend to add flexibility and generalisation to HRC plan\ngeneration, eliminating the need for a manual and laborious\ndefinition of restricted planning problems and agent models.\nI. I NTRODUCTION\nLet us consider a scenario in which a human initiates a\ncollaboration by saying “Let’s tidy up the kitchen. By the\nway, I have back pain today”. In this situation, human agents\nwould be able to use common sense to devise a plan with\nspecific subgoals, such as storing a spoon in a drawer, and\nto allocate these subgoals appropriately, for example, not\nassigning the task of tidying up a heavy casserole to the\nhuman with back pain. Now, let us consider a Human-Robot\nCollaboration (HRC) where a similar goal is shared, and a\nnumber of specific and executable tasks must be planned and\nassigned to each agent. In this case, how can a robotic system\nleverage the available abstract information in a natural and\nflexible manner to generate an executable HRC plan with\nspecific subgoals and an appropriate task allocation ?\nAI task planning may be used [1], [2], but has a main\ndrawback: the planning problem, including a world model\n1Eurecat, Centre Tecnol `ogic de Catalunya, Robotics and Automation\nUnit, Barcelona, Spain\n2Department of Informatics, King’s College London, United Kingdom\n3Institut de Rob`otica i Inform`atica Industrial, CSIC-UPC Barcelona Spain\n*S. Izquierdo is a fellow of Eurecat’s Vicente L´opez PhD grant program.\nG. Canal has been supported by the Royal Academy of Engineering and\nthe Office of the Chief Science Adviser for National Security under the UK\nIntelligence Community Postdoctoral Research Fellowship programme. This\nwork was partially financed by MCIN/ AEI /10.13039/501100011033 and by\nthe “European Union NextGenerationEU/PRTR” under the project ROB-IN\n(PLEC2021-007859); and by the project COHERENT funded by MCIN/\nAEI /10.13039/501100011033, Spain, the “European Union NextGenera-\ntionEU/PRTR”, Spain, (PCI2020-120718-2), and by UKRI (EP/V062506/1).\nCorrespondence: silvia.izquierdo@eurecat.org\n(domain) with an initial state and goal (problem), needs\nto be carefully designed. This is a time-consuming task,\nwhich often results in a rigid problem definition, difficult\nto generalise and adapt to a range of different situations.\nAlternatively, Large Language Models (LLMs) can gen-\nerate plans with no need for careful design [3], [4], by\nextracting common-sense knowledge from Natural Language\n(NL), such as an abstract goal. However, this domain-\nindependent method suffers from grounding problems, often\nresulting in plans that are not executable by the robot.\nAn added challenge arises when planning for HRC, con-\nsisting of allocating tasks to different agents based on their\nstrengths, limitations, or preferences. This remains an open\nquestion, as agent models are often limited and unrealistic,\nunable to cover all elements representing the agent states.\nIn this paper, we propose to combine the strengths of AI\ntask planning and LLMs for effortless and adaptive HRC plan\ngeneration. We present PlanCollabNL (see Fig. 1), a frame-\nwork to generate grounded collaborative plans from three\npieces of information: an abstract NL goal, the current scene\nstate specified in NL (objects, locations, agent conditions),\nand the available tasks defined in the planning domain. The\nkey contributions of this work are:\n1) A method leveraging LLMs to derive grounded plan-\nning subgoals from an NL abstract goal and the current\nenvironment state. This eliminates the need for a\nmanual definition of a specific planning problem goal.\n2) A method that uses LLMs to reason about NL agent\nconditions and influence the task allocation in the\nHRC plan. This eliminates the need for restricted and\nunrealistic agent models, and enables a high degree of\nadaptability to the collaborating agents’ conditions.\n3) A framework that integrates these methods and trans-\nlates their acquired common-sense knowledge into a\nstructured task planning problem implemented using\nPlanning Domain Definition Language (PDDL) to ef-\nficiently generate grounded plans.\nThe framework is evaluated for a number of NL abstract\ngoals and agent conditions, with results showing that exe-\ncutable plans with a coherent task allocation between the\nagents are obtained in most cases.\nII. R ELATED WORK\nAI task planning for HRC. AI (or automated) task plan-\nning frameworks have been used to efficiently generate and\ndistribute the sequence of actions required to achieve a shared\ngoal [1], [2], [5]–[8]. Defining the planning problem consti-\ntutes a laborious task, and is usually based on restricted,\nNL goal\nLet's tidy up the kitchen and prepare a meal.\nSubgoal generation\nSubgoal allocation\nrecommendation\nNL agent conditions\nThe human loves to cook.\nThe human has back pain.\nThe robot can't pick small objects.\nsalmon, cooked, oven\nmop, used to clean, floor\nspoon, stored, drawer\nsalmon, cooked, oven\nmop, used to clean, floor\nspoon, stored, drawer\nPlanning - Collaborative PDDL plan\n(move human init table)\n(pick human spoon table) \n(move human table drawer) \n(store human spoon drawer)\n(move human init counter)\n...\n(cook human salmon oven)\n(move robot init mop_loc)\n(pick human mop mop_loc)\n(move robot mop floor)\n(clean robot mop floor)\n1\n2\n3\n   Planning Action Costs (in PDDL)\n(cook human salmon oven) --\n(clean human mop floor) ++\n(store robot spoon drawer) ++\nPlanning Goals (in PDDL)\n(cooked salmon oven)\n(used_to_clean mop floor)\n(stored spoon drawer)\nSCENE DESCRIPTION LARGE LANGUAGE MODEL (LLM) AI TASK PLANNING\nNL locations: fridge, sink, oven, drawer \nNL objects: salmon, spoon, broom\nPDDL: Planning Domain Definition Language\npositive / negative \n favoured / disfavoured \ndecreased cost / increased cost \nLLM\nLLM\nsalmon, cooked, oven\nmop, used to clean, floor\nspoon, stored, drawer\nFiltering\nFig. 1. PlanCollabNL: an LLM is used to generate and filter subgoals from an abstract goal and the current environment (1), and later to reason about\nthe subgoal allocation among the agents, favouring or disfavouring subgoals for each agent based on their conditions (2). This information is encoded into\nthe PDDL planning problem, translating the LLM output subgoals into PDDL goals, and modifying the action costs based on the LLM recommendations.\nThe complete planning problem is given to a planner to produce grounded and executable collaborative plans that consider the agent states (3).\nnot generalisable and closed-world models. This limits the\nability of the systems to deal with complex and abstract\ntasks, and to adapt the plan to both the environment and the\ncontributing agents. We are thus motivated to take advantage\nof LLMs’ reasoning abilities, whilst maintaining the use of\nAI planning to guarantee sound, complete and executable\nsolutions, something which LLMs still fail to do [9]. In this\nwork, the planning problem is defined using PDDL [10],\na standardised planning encoding consisting of two files: a\ndomain and a problem. The domain includes the definition of\nthe world and underlying rules, whilst the problem contains\nthe initial state and goal conditions. Planners are then able to\nfind a plan based on a search guided by heuristics extracted\nautomatically from the problem representation.\nAgent modelling for plan adaptability. For successful\nHRC, the planner should adapt the plan and assign the\nappropriate complementary actions to the team members\nbased on their state and capabilities. In previous work [1], we\ntargeted the gap between human modelling and its effective\nintegration in the planning framework. This is something\nother works have attempted to do [2], [11], with some of\nthe elements modelled including factors such as knowledge\n[7], [8], capacity, distraction and fatigue [12]–[14]. In these\nworks, the models are fixed and unable to encompass the full\ncomplexity of the agents’ states. LLMs have been adopted to\nmodel and simulate human behaviour [15], [16], or to learn\nhuman preferences from past experiences [17]. However,\nnone of the works exploit LLMs to reason about other agents’\nconditions for collaborative plan adaptation. In our work, we\nremove the need for a limited agent model, by leveraging\nLLMs reasoning capacities to influence the plan based on\nany agent condition expressed in NL form.\nLLMs in robotics planning. Recent studies have show-\ncased LLMs’ reasoning and few-shot generalisation abili-\nties, motivating their integration into embodied systems for\ndecision-making [18]–[20]. Nevertheless, a major challenge\nremains in grounding language to the robot capabilities, and\ndealing with LLMs’ hallucinations. We identify two lines of\nresearch, where LLMs are used directly as a planner, or as an\nauxiliary helper bringing knowledge to the planning system.\nWhen using LLMs as planners, several methods have been\nproposed to mitigate grounding issues, including affordance\nfunctions for action feasibility [3], or semantic translation of\nplan steps to admissible actions through an LLM [4]. Other\nworks [21]–[24] add sources of NL feedback such as scene\ninformation or precondition errors during plan execution\nto enable dynamic replanning. In [25], the plan is nested\nwithin the LLM-generated policy code for direct execution\nby the robot. Although these approaches are promising,\nexecutability issues are still present, raising the question of\nwhether LLMs are ready to plan and to fully take control of\nrobot behaviours [9]. The second line of research investigates\nLLMs as an auxiliary tool for planning, noting in [26] and\n[27] that while LLMs fail to directly solve many problems,\nthey can be useful in guiding the search. They call for further\nresearch on the topic and highlight real-time implementation\nchallenges. Other works use LLMs to translate natural lan-\nguage to structured PDDL format, focusing on goals [28], ac-\ntion definitions [29], or full planning problems [30]. Despite\nachieving promising results, directly generating structured\nplanning language increases the likelihood of syntax errors\nand unsolvable plans. We build on the idea of combining the\nstrengths of grounded AI task planning with flexible LLM\ncapabilities, bringing adaptability and facilitating the tedious\ntask of defining a rigid planning problem. In this work,\nthe planning domain capabilities are not modified. LLMs\nreason about planning subgoals, which are then filtered and\ngrounded before being integrated into the planning problem,\navoiding executability issues and guaranteeing sound plans.\nWe also notice that none of the works integrating LLMs with\ntask planning address HRC plans, using LLMs to reason\nabout the environment’s and agents’ states. The uses of\nLLMs to generate grounded and filtered planning subgoals\nfrom an NL goal and to reason about NL agent abilities in\nan HRC planning scenario are both novel contributions to\nthe existing literature.\nIII. M ETHODOLOGY\nA. Framework\nThe implemented framework (as seen in Fig. 1) generates\nan adapted HRC plan to reach a given abstract NL goal,\ntaking into account the current scene (objects, locations,\nand agent conditions) and the available actions defined\nin the planning domain. Regarding the LLM, we utilise\nthe pretrained GPT-3 [31] without further fine-tuning, and\napply n-shot learning, where the prompt includes n previous\nexamples. The process consists of three main phases:\n1) Subgoal generation and filtering (Sec. III-B): The\nLLM initially divides the abstract goal of the plan\n(e.g. prepare a meal) into a number of subgoals, in the\nform of: “object”, “task (predicate)”, “location” (e.g.,\n“salmon, cooked, hob”), considering the current scene\nand available actions. These subgoals are generated,\ngrounded and filtered in three stages.\n2) Subgoal allocation recommendation (Sec. III-C):\nThe LLM reasons about the preferred subgoals’ allo-\ncation among the agents based on their conditions.\n3) PDDL task planning (Sec. III-D): The outputs from 1)\nand 2) are integrated into the PDDL planning problem.\nThe subgoals are parsed and formatted into PDDL\ngoals, and the subgoal allocation recommendation is\ntranslated into action costs associated to the relevant\nsubgoals. The final HRC plan can then be generated by\na planner from the complete domain and the problem.\nB. Generation of Grounded Planning Subgoals from an\nAbstract NL Goal\nThe first component of the framework receives an NL\nabstract goal from the user and generates grounded planning\nsubgoals in the form of “object, predicate, location” (e.g.,\n“spoon, stored, drawer”). The process is done in three stages,\ndepicted in Fig. 2. In the first stage, the system generates a\nprompt based on a template, incorporating the user’s goal\nand the environment information, and queries the LLM\nto output a list of subgoals (Fig. 2-Stage 1). The prompt\nincludes five previous examples, made of the goal, available\npredicates, objects, locations, and the subgoals expected as\noutput. After prompting the LLM with a new goal, the\nreturned list of subgoals is grounded to the available objects,\nlocations and predicates in the planning domain (Fig. 2-Stage\n2). Finally, the subgoals are filtered with further templated\nLLM prompts, generated by the system based on the current\nsubgoals. Incorrect subgoals are filtered out by the LLM\nbased on common sense and on their contribution to the\nhigh-level goal (Fig. 2-Stage 3). The full prompts containing\nthe examples are made available 1. This method provides\na robust, natural and effortless way of defining specific\nplanning subgoals for an NL abstract goal, where all stages\nare essential to ensure the executability of the final plan.\n1See http://www.iri.upc.edu/groups/perception/#HRC_\nTaskPlanningLLM\nSubgoal generation and filtering\nStage 1 - Subgoal generation through LLM\nStage 3 - Subgoal filtering through LLM\n...n previous examples for n-shot learning...\nGoal: I want to tidy up the cuttlery and clean the fridge. \nPredicates: stored, used to clean, served, placed, cooked\nObjects: fork, banana, cloth, mop\nLocations: sink, drawer, floor, fridge\nSubgoals:\nfork, stored, drawer\ncloth, used to clean, fridge\nmop, used to clean, fridge\nfork, washed, sink\nCommon-sense check for each subgoal e.g. mop, used to clean, fridge\nUse common sense. Answer yes or no.\nA mop is used to clean the fridge. Does it make sense?\nNo\nGoal contribution check for each subgoal e.g. fork, stored, drawer\nThe goal is: I want to tidy up the cuttlery and clean the fridge. \nWould the following subgoal contribute to the goal? Answer yes or no.\nA fork is stored in a drawer.\nYes Final Subgoals:\nfork, stored, drawer\ncloth, used to clean, fridge\nSubgoals generated:\nfork, stored, drawer\ncloth, used to clean, fridge\nmop, used to clean, fridge\nfork, washed, sink\nStage 2 - Ground subgoals to planning domain\nSubgoals remaining:\nfork, stored, drawer\ncloth, used to clean, fridge\nmop, used to clean, fridge\nfork, washed, sink\nSubgoals remaining:\nfork, stored, drawer\ncloth, used to clean, fridge\nmop, used to clean, fridge\n1\nLLM output\nSystem Prompt\nLLM output\nLLM output\nSystem Prompt\nSystem Prompt\nfor subgoal in subgoals:\n if [predicate] not in PDDL domain\n or [object] not in PDDL domain\n or [location] not in PDDL domain:\n  discard subgoal\nFig. 2. Subgoal generation stages: in the first stage, an LLM generates a\nset of subgoals from a prompt constructed by the framework from an NL\nabstract goal. These subgoals are then grounded to the planning domain\ncapabilities. The third stage filters the remaining subgoals by querying an\nLLM on whether they make sense and contribute to the high-level goal.\nC. Reasoning About Subgoal Allocation Based on NL Agent\nConditions\nIn a collaboration, actions should be assigned to different\nagents based on their capabilities and preferences. The\nsecond component of the framework takes in the filtered\nsubgoals along with the given NL agent conditions, and\nqueries the LLM to identify which subgoals should be\nfavoured or disfavoured for each agent (see Fig. 3). Note\nthat the LLM decision does not force the planner to allocate\nthe actions, but influences the planner allocation, penalising\nthe corresponding actions with a cost. A detailed explanation\nof this process is provided in the next Section III-D. This\napproach offers a flexible and efficient means of adapting\nthe plan to any agent condition expressed in NL form, elim-\ninating the need for complex and restrictive agent models.\nD. Generalisable PDDL Task Planning Problem Definition\nThe planning problem is defined in PDDL [32] and\nconsists of a domain, including the object types, predicates,\nSubgoal allocation recommendation\nSubgoal allocation recommendation through LLM\n...n previous examples for n-shot learning...\nA human and a robot are collaborating together to\nachieve some goals. The tasks are distributed between the agents.\nI have the following tasks:\nmop, used to clean, floor\nspoon, stored, drawer\nThe human doesn't want to get wet.\nThe agent should disfavour these tasks:\nmop, used to clean, floor\nThe agent should favour the tasks:\nNone\n2\nLLM output\nSystem Prompt\nFig. 3. Subgoal allocation recommendation: The LLM suggests to favour\nor disfavour certain subgoals for an agent based on their conditions.\nfunctions and actions that can exist within the model, and a\nproblem, collecting what objects exist, what the states of the\npredicates and functions are, and what the end goal is. The\nPDDL files are provided at the link in footnote 1.\nPDDL domain. In our framework, the domain is fixed and\nencompasses all agents’ capabilities and scenario constraints.\nIt is important to note that the LLM only modifies the\nPDDL problem and not the domain, ensuring the generation\nof grounded plans and reducing executability issues. The\nenvironment is defined in terms of agents, objects and\nlocations, represented as object types in the domain: agent,\nobj, loc . With the aim of creating a planning definition\nthat is easily extendable to different tasks, we have defined\ntwo types of actions and predicates: standard and subgoal\n(see Fig. 4). A subgoal action (e.g., store) has a subgoal\npredicate (e.g., stored) as an effect, directly achieving a\nsubgoal which might be part of the goal. It has a linked\ncost (e.g., stored cost) as an additional effect, associated to\nthe action parameters ( agent, obj, loc ), which increases the\ntotal cost of the plan. The standard predicates and actions\nare needed as intermediate steps in the plan to reach the\nsubgoal elements, and remain internal to the planning sys-\ntem. A standard action (e.g., move) has standard predicates\n(e.g. at loc) as effects, and has no cost. This provides a\ngeneralisable framework, extendable to a great number of\nscenarios where an LLM can reason to provide external\nplanning knowledge, as described in the paragraph below.\nTo define a new scenario with new capabilities, the possible\ntasks must be identified (e.g., painting) and associated to\na subgoal action (e.g., paint), predicate (e.g., painted), and\ncost (e.g., painted cost). This will enable the LLM system\nto automatically specify subgoals involving the task, and to\ninfluence their allocation, as detailed next.\nPDDL problem. For each new situation, the initial PDDL\nproblem only contains the existing objects, locations and\nagents, and the initial state of the standard predicates. The\nsubgoal predicates composing the goal and the subgoal\naction costs are determined by the LLM based on the\ncurrent situation, as described in Sec. III-B and Sec. III-\nC, respectively. The outputs of the LLMs are translated into\nPDDL DOMAIN\n:types\n:predicates\n:actions\n:functions\nobj loc agent\nstandard predicates\nat_loc ?agent ?loc\nsubgoal predicates\nstored ?obj ?loc\nsubgoal actions\nstore ?agent ?obj ?loc\neffects stored_cost ?agent ?obj ?loc\nplan_cost\nstandard actions\nmove ?agent ?loc\npreconditions\neffects\nPDDL PROBLEM\n:objects\nobj - apple, fork...\nloc - drawer, table...\nagent - human, robot\n:init\nat_loc human table\nat_loc robot  table\n...\n*stored_cost robot fork drawer = 100\n...\n:goal\n*stored fork drawer\n*placed apple table\n...\n...\n...\n*determined by LLM\npreconditions\n...\n...\nFig. 4. PDDL planning definition: two types of actions have been defined\nin the planning domain (subgoal and standard). The subgoal actions directly\nachieve a subgoal, which is defined using a subgoal predicate. These actions\nhave an associated cost. The standard actions are intermediate steps to reach\nthe subgoals and have no cost. The planning problem contains the planning\ngoal (made of subgoals) and the action costs, determined by the LLM.\nPDDL elements in the following way:\nPDDL goals. The subgoals returned by the LLM in the\nform of “object, predicate, location” are converted to PDDL\ngoals, defined as subgoal predicates such as “predicate ?obj\n?loc” (e.g., “used to clean mop floor” ). The LLM is only\nallowed to reason about the subgoal predicates that constitute\nthe planning goal, and cannot modify standard predicates.\nPDDL action costs. The planner is responsible for dis-\ntributing the actions in the plan to the appropriate agents. As\nit attempts to minimise the plan cost, it will avoid actions\ninvolving a high cost for an agent. The agent conditions\nshould therefore be reflected in the action costs. Manually\nencoding the agent conditions in PDDL would be a com-\nplex and tedious task. We therefore take advantage of the\nLLM and map its recommendations to PDDL action costs.\nAs an example, if the subgoal “used to clean mop floor”\nshall be disfavoured for the human, the corresponding cost\n“used to clean cost human mop floor” is increased, and the\nplanner will tend to avoid assigning the action “clean mop\nfloor” to the human, as its effects increase the total cost by\nthis value. Oppositely, for a subgoal that is favoured for one\nagent, the cost of its corresponding action will be increased\nfor the other agents in the collaboration, thereby disfavouring\nthe assignment of the favoured subgoal to these other agents.\nDecreasing the cost instead would lead to negative plan\ncosts that the planner would be unable to deal with. With\nthis method, the automatic plan generation is now able to\nconsider any agent condition in the action allocation.\nIV. E VALUATION\nThe system is evaluated in two stages. In Sec. IV-A,\nthe results of generating planning subgoals from a list of\nNL abstract goals are presented. In Sec. IV-B, we evaluate\nthe subgoal allocation for a list of NL agent conditions,\non a number of scenarios with subgoals generated in the\nfirst evaluation. This second evaluation, therefore, assesses\nthe full framework from the inputs (NL abstract goal and\nagent conditions) to the output (executable collaborative\nplan). The time taken for the process was evaluated on 320\nruns, taking an average of 4.719 ± 4.208 seconds: where\n3.143 seconds are for subgoal generation, 1.076 seconds\nfor subgoal allocation recommendation, and 0.5 seconds\nfor planning (based on a planning timeout, where a valid\nplan was returned in all cases). This result encourages the\nimplementation of the system on real applications.\nScenario. The scenario evaluated consists in a kitchen\nsetting, comprising 27 objects (cup, sponge, salmon...) and\n17 locations (hob, fridge, table...). The available tasks are\ncooking, cleaning, storing, serving and placing.\nGround Truth. The datasets 1 created for the evaluation\nof the system contain a number of defined test cases, along\nwith the expected ground truth (GT) output.\nMetrics. The following metrics are used for evaluation:\n1) Completeness: Number of generated subgoals corre-\nsponding to a GT subgoal, over the number of GT\nsubgoals: n correct subgoals\nn GT subgoals . If no subgoals should be\ngenerated and some are generated, completeness is 0.\n2) Correctness: Number of generated subgoals corre-\nsponding to a GT subgoal, over the number of gen-\nerated subgoals: n correct subgoals\nn generated subgoals . For no subgoals\ngenerated, correctness is 1 if GT matches, 0 otherwise.\n3) Planning: Whether a plan can be generated from the\noutput subgoals. If the subgoals are not represented in\nthe planning domain, a plan cannot be generated.\n4) Executability: Whether the resulting plan is executable.\nA subgoal might be translated into a legal action such\nas cleaning the floor with an object. If this object is\nnot valid for the goal (i.e., an apple), the plan is not\nconsidered executable.\nA. Evaluation of Planning Subgoals Generation from an NL\nAbstract Goal\nDataset. A dataset containing 114 different NL goals has\nbeen created by adapting the data used in [3]. Their dataset\nvaries in terms of time horizon and language complexity\nand was created via crowd-sourcing on Amazon Mechanical\nTurk, in-person kitchen user interviews, and benchmarks for\neveryday activities [33], [34]. Our work differs from theirs in\ntwo main aspects: we deal with a goal-focused collaboration\nrather than an instruction-driven assistive scenario, and we\ngenerate subgoals for posterior planning instead of using\nLLMs to generate a final plan. The instructions have been\nselected from the dataset and adapted into the form of a goal.\nFig. 5. Subgoal generation results: The correctness of the subgoals, plan\nsuccess and executability are increased throughout the stages, whilst the\ncompleteness of the subgoals is decreased in stage 3. This represents a trade-\noff between completeness in the plan subgoals and final plan executability.\nFurthermore, two new goal types have been added to deal\nwith a wider range of abstract tasks: multi-task and high-\nlevel abstraction . Table I provides, for each goal type, an\nexplanation, example, and expected output subgoals.\nResults. Figure 5 shows the results of the subgoal genera-\ntion at the three stages described in Sec. III-B. In stage 1, the\nLLM’s raw response is considered without any grounding\nto the planning domain (happening in stage 2) or further\nfiltering by the LLM (occurring in stage 3). As expected,\nsubgoal correctness, planning success and executability are\nincreased throughout the stages, whilst completeness of the\nsubgoals decreases in stage 3. At stage 2, all the subgoals\n(with 89% correctness) are grounded to the domain and\nplanning reaches 100%, whilst only 1% of the plans are not\nexecutable due to legal subgoals not making sense (e.g. bowl\nused to clean sink). At stage 3, after filtering the subgoals\nbased on common sense and goal contribution through an\nLLM, correctness increases to 94% and all plans are exe-\ncutable, at the cost of a decrease in completeness (84%). The\nfiltering stage greatly increases correctness, though a good\ncompromise between completeness and executability needs\nto be found by tuning the filtering prompts. In most cases, our\nsystem proves to generate correct and executable HRC plans\nfrom an NL abstract goal. The subgoals are automatically\ndefined by taking into account the current environment,\neliminating the laborious step of manually defining a specific\nplanning problem for each scenario.\nB. Evaluation of Subgoal Allocation for Different NL Agent\nConditions\nDataset. A dataset including 28 agent conditions has been\ncreated to evaluate the subgoal allocation adaptation to the\ncollaborating agents. Conditions are divided into positive and\nnegative, intended to result in some subgoals being favoured\nor disfavoured for the involved agent respectively. Table II\nprovides, for each condition type, an example condition and\nits GT LLM output. Each condition is evaluated in four\ndifferent scenarios (generated in the evaluation Sec. IV-A),\nresulting in a total of 112 tests evaluating the full system.\nGoal Type (n examples) Explanation Example Goal Ground Truth Output Subgoals\nNL Single Primitive (4) NL queries for a single primitive. I want to let go of the coke can. coke, placed, trash can\nNL Nouns (25) NL queries focused on abstract\nnoun synonyms.\nI want to serve something with\ncaffeine at the table. coffee, served, table\nNL Verbs (25) NL queries focused on abstract\nverb synonyms. I want to prepare a chicken meal. chicken, cooked, grill\nvegetables, cooked, hob\nStructured Language (15) Structured language queries. I want to pick up the apple and\nmove it to the trash. apple, placed, trash can\nContext/Disturbances (15) Queries in unstructured formats. My favorite drink is redbull, I want\none. I am at the counter. redbull, placed, counter\nLong-Horizon (15) Long-horizon queries that\nrequire many steps of reasoning.\nI spilled my coke on the table, I want\nto throw it away and bring\nsomething to clean.\ncoke, placed, trash can\ncleaning cloth, used to clean, table\nMulti-task (10) Multi-task queries. I want to clean the floor and cook a\nsalmon.\nmop, used to clean, floor\nsalmon, cooked, hob\nHigh-level abstraction (5) No mention of specific objects\nor locations. I want to tidy up the room.\nspoon, stored, drawer\ncup, stored, cupboard\nbanana, stored, fridge, etc.\nTABLE I\nGOAL TYPES USED FOR EVALUATION\nAgent\nCondition Type Example Condition GT Output Subgoals to\nFavour/Disfavour\n+ve Robot The robot is fast at\ntidying up objects.\nFavour robot:\ncup, stored, cupboard\n+ve Human The human loves\nto cook.\nFavour human:\nsalmon, cooked, hob\n-ve Robot The robot can’t\nget wet.\nDisfavour robot:\nmop, used to clean, floor\n-ve Human The human has back\npain.\nDisfavour human:\ncloth, used to clean, floor\nTABLE II\nEXAMPLE AGENT CONDITIONS USED TO EVALUATE THE SYSTEM\nSubgoals to favour Subgoals to disfavourAgent\nCondition Type Compl. Corr. Compl. Corr.\n+ve Robot 74% 75% 96% 96%\n+ve Human 71% 71% 100% 98%\n-ve Robot 96% 96% 72% 77%\n-ve Human 96% 96% 75% 77%\nAll 84% 84% 86% 88%\nTABLE III\nRESULTS FOR SUBGOAL ALLOCATION REASONING\n(COMPLETENESS AND CORRECTNESS )\nResults. The results in Table III show how the system finds\nthe right subgoals to favour in 84% of cases, and to disfavour\nin 86% of cases. Although there is no significant difference\nbetween the human and robot conditions, there exists a dif-\nference in performance for positive and negative conditions.\nPositive conditions involve some subgoals to favour and\nnone to disfavour, whilst negative conditions involve some\nsubgoals to disfavour and none to favour. The system demon-\nstrates greater performance in determining that no subgoals\nshould be favoured or disfavoured, achieving success in more\nthan 96% of such cases, as opposed to the 71-77% success\nrate observed when it needs to identify particular subgoals\nto either favour or disfavour. In addition to further tuning of\nthe LLM prompts, common-sense knowledge covering social\ninteractions [35] could be injected into the LLM to increase\nthe performance. Nevertheless, we demonstrate how LLMs\nhave the potential to help robots reason about NL agent\nconditions during HRC plans, bringing in a large level of\nadaptability and naturalness in such scenarios.\nV. L IMITATIONS AND FUTURE WORK DIRECTIONS\nWe identify a number of limitations of the system, present-\ning potential opportunities for future work. Firstly, the system\ninherits the limitations of LLMs, the performance of which\nsignificantly depends on the prompt given. Grounding and\nfiltering have proven to get rid of incorrect LLM responses,\nbut the trade-off between executability and flexibility in the\nsystem still remains. Retrieval Augmented Generation [36]\ncould be implemented, integrating common-sense social,\nphysical and eventive knowledge [35] into pretrained LLMs.\nA second limitation lies in the fact that the system reasons\nand plans based on subgoals that have no temporal dependen-\ncies between them. This, although limiting the complexity\nof the application, allows for a framework generalisable to\nother applications with any type of independent goals. Lastly,\nfor an optimal workload allocation among the agents to be\nobtained, further research such as the one presented in [37]\nshould be conducted to find “optimal” cost values for the\nfavoured or disfavoured subgoals.\nVI. C ONCLUSIONS\nIn this paper, we have implemented a reasoning and\nplanning framework, PlanCollabNL, that combines LLMs\nand AI task planning to improve HRC plan generation in\ntwo principal ways. Firstly, the planning problem definition\nis simplified and accelerated by leveraging LLM common-\nsense reasoning capabilities to automatically generate plan-\nning subgoals from an NL abstract goal. Secondly, a high de-\ngree of adaptability to the collaborating agents is achieved by\ninfluencing the allocation of the planning subgoals based on\nan LLM reasoning about NL agent conditions. The planning\nproblem definition has been structured in a generalisable way,\nso that the system can be easily extended to a wide range\nof tasks. The framework has been evaluated for a number\nof goals and agent conditions, and is now ready for imple-\nmentation in a real robot. Results show that executable and\nagent-adapted plans are successfully generated in most cases,\neliminating the need for a manual and tedious definition of\nrestricted and inflexible planning problems and agent models.\nREFERENCES\n[1] S. Izquierdo-Badiola, G. Canal, C. Rizzo, and G. Aleny `a;, “Improved\ntask planning through failure anticipation in human-robot collab-\noration,” in International Conference on Robotics and Automation\n(ICRA), 2022, pp. 7875–7880.\n[2] S. Devin, A. Clodic, and R. Alami, “About decisions during human-\nrobot shared plan achievement: Who should act and how?” in Social\nRobotics. Springer International Publishing, 2017, pp. 453–463.\n[3] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David,\nC. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu,\nJ. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jes-\nmonth, N. Joshi, R. C. Julian, D. Kalashnikov, Y . Kuang, K.-H. Lee,\nS. Levine, Y . Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao,\nJ. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev,\nV . Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, and M. Yan, “Do As I\nCan, Not As I Say: Grounding Language in Robotic Affordances,” in\nConference on Robot Learning , Apr. 2022.\n[4] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language Models\nas Zero-Shot Planners: Extracting Actionable Knowledge for Embod-\nied Agents,” ArXiv, vol. abs/2201.07207, Jan. 2022.\n[5] R. Lallement, L. De Silva, and R. Alami, “HATP: Hierarchical Agent-\nbased Task Planner,” in International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS 2018) , 2018.\n[6] S. Bezrucav and B. Corves, “Improved AI planning for cooperating\nteams of humans and robots,” in Proceedings of the 8th ICAPS\nWorkshop on Planning and Robotics (PlanRob) , 2020.\n[7] S. Devin and R. Alami, “An implemented theory of mind to improve\nhuman-robot shared plans execution,” ACM/IEEE International Con-\nference on Human-Robot Interaction (HRI) , pp. 319–326, 2016.\n[8] G. Milliez, R. Lallement, M. Fiore, and R. Alami, “Using human\nknowledge awareness to adapt collaborative plan generation, explana-\ntion and monitoring,” in 11th ACM/IEEE International Conference on\nHuman-Robot Interaction (HRI) , 2016, pp. 43–50.\n[9] K. Valmeekam, A. Olmo, S. Sreedharan, and S. Kambhampati, “Large\nLanguage Models Still Can’t Plan (A Benchmark for LLMs on\nPlanning and Reasoning about Change),” in NeurIPS 2022 Foundation\nModels for Decision Making Workshop , Nov. 2022.\n[10] M. Ghallab, A. Howe, C. Knoblock, D. Mcdermott, A. Ram,\nM. Veloso, D. Weld, and D. Wilkins, “PDDL—The Planning Domain\nDefinition Language,” 1998.\n[11] M. Fiore, A. Clodic, and R. Alami, “On Planning and Task achieve-\nment Modalities for Human-Robot Collaboration,” in International\nSymposium on Experimental Robotics (ISER 2014) , 2014, p. 15p.\n[12] O. G ¨or¨ur, B. Rosman, G. Hoffman, and S. Albayrak, “Toward inte-\ngrating theory of mind into adaptive decision- making of social robots\nto understand human intention,” in HRI Workshop on the Role of\nIntentions, 2017.\n[13] O. C. G ¨or¨ur, B. Rosman, F. Sivrikaya, and S. Albayrak, “Social cobots:\nAnticipatory decision-making for collaborative robots incorporating\nunexpected human behaviors,” in ACM/IEEE International Conference\non Human-Robot Interaction , 2018, p. 398–406.\n[14] O. C. G ¨or¨ur, B. Rosman, and S. Albayrak, “Anticipatory bayesian pol-\nicy selection for online adaptation of collaborative robots to unknown\nhuman types,” in International Conference on Autonomous Agents and\nMultiAgent Systems, 2019, p. 77–85.\n[15] B. Zhang and H. Soh, “Large Language Models as Zero-Shot Human\nModels for Human-Robot Interaction,” 2023 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pp. 7961–7968,\nOct. 2023, conference Name: 2023 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS) ISBN: 9781665491907\nPlace: Detroit, MI, USA Publisher: IEEE.\n[16] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.\nBernstein, “Generative Agents: Interactive Simulacra of Human Be-\nhavior,” in 36th Annual ACM Symposium on User Interface Software\nand Technology, Oct. 2023, pp. 1–22.\n[17] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\nS. Rusinkiewicz, and T. Funkhouser, “TidyBot: personalized robot\nassistance with large language models,” Autonomous Robots, vol. 47,\npp. 1–16, Nov. 2023.\n[18] I. Dasgupta, A. K. Lampinen, S. C. Y . Chan, A. Creswell, D. Kumaran,\nJ. L. McClelland, and F. Hill, “Language models show human-like\ncontent effects on reasoning,” ArXiv, vol. arXiv:2207.07051, Jul. 2022.\n[19] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, “Chatgpt for\nrobotics: Design principles and model abilities,” Microsoft, Tech. Rep.\nMSR-TR-2023-8, February 2023.\n[20] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\nA. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y . Chebotar,\nP. Sermanet, D. Duckworth, S. Levine, V . Vanhoucke, K. Hausman,\nM. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence, “PaLM-\nE: an embodied multimodal language model,” in 40th International\nConference on Machine Learning , vol. 202, 2023, pp. 8469–8488.\n[21] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay,\nD. Fox, J. Thomason, and A. Garg, “ProgPrompt: program genera-\ntion for situated robot task planning using large language models,”\nAutonomous Robots, vol. 47, no. 8, pp. 999–1012, Aug. 2023.\n[22] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\nJ. Tompson, I. Mordatch, Y . Chebotar, P. Sermanet, T. Jackson,\nN. Brown, L. Luu, S. Levine, K. Hausman, and B. Ichter, “Inner\nMonologue: Embodied Reasoning through Planning with Language\nModels,” in 6th Conference on Robot Learning , 2023, pp. 1769–1782.\n[23] S. S. Raman, V . Cohen, E. Rosen, I. Idrees, D. Paulius, and\nS. Tellex, “Planning With Large Language Models Via Corrective Re-\nPrompting,” in NeurIPS 2022 Foundation Models for Decision Making\nWorkshop, Nov. 2022.\n[24] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and\nY . Su, “LLM-Planner: Few-Shot Grounded Planning for Embodied\nAgents with Large Language Models,” in IEEE/CVF International\nConference on Computer Vision , 2023, pp. 2998–3009.\n[25] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,\nand A. Zeng, “Code as Policies: Language Model Programs for Em-\nbodied Control,” in 2023 IEEE International Conference on Robotics\nand Automation (ICRA) , May 2023, pp. 9493–9500.\n[26] T. Silver, V . Hariprasad, R. S. Shuttleworth, N. Kumar, T. Lozano-\nP´erez, and L. P. Kaelbling, “PDDL Planning with Pretrained Large\nLanguage Models,” in NeurIPS 2022 Foundation Models for Decision\nMaking, Nov. 2022.\n[27] Z. Zhao, W. S. Lee, and D. Hsu, “Large Language Models as\nCommonsense Knowledge for Large-Scale Task Planning,” in RSS\n2023 Workshop on Learning for Task and Motion Planning, Jun. 2023.\n[28] Y . Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh, “Translating\nNatural Language to Planning Goals with Large-Language Models,”\nArXiv, vol. arXiv:2302.05128, Feb. 2023.\n[29] Y . Ding, X. Zhang, S. Amiri, N. Cao, H. Yang, C. Esselink, and\nS. Zhang, “Robot Task Planning and Situation Handling in Open\nWorlds,” ArXiv, vol. arXiv:2210.01287, Oct. 2022.\n[30] B. Liu, Y . Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone,\n“LLM+P: Empowering Large Language Models with Optimal Plan-\nning Proficiency,” ArXiv, vol. arXiv:2304.11477, May 2023.\n[31] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhari-\nwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M.\nZiegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,\nI. Sutskever, and D. Amodei, “Language models are few-shot learners,”\nComputing Research Repository (CoRR) , vol. abs/2005.14165, 2020.\n[32] M. Fox and D. Long, “PDDL2.1: An extension to PDDL for ex-\npressing temporal planning domains,” Journal of artificial intelligence\nresearch, vol. 20, pp. 61–124, 2003.\n[33] M. Shridhar, J. Thomason, D. Gordon, Y . Bisk, W. Han, R. Mottaghi,\nL. Zettlemoyer, and D. Fox, “ALFRED: A Benchmark for Interpreting\nGrounded Instructions for Everyday Tasks,” in IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2020, pp. 10 737–10 746.\n[34] S. Srivastava, C. Li, M. Lingelbach, R. Mart ´ın-Mart´ın, F. Xia,\nK. Vainio, Z. Lian, C. Gokmen, S. Buch, C. K. Liu, S. Savarese,\nH. Gweon, J. Wu, and L. Fei-Fei, “BEHA VIOR:benchmark for ev-\neryday household activities in virtual, interactive, ecological environ-\nments,” Computing Research Repository , vol. abs/2108.03332, 2021.\n[35] J. D. Hwang, C. Bhagavatula, R. Le Bras, J. Da, K. Sakaguchi,\nA. Bosselut, and Y . Choi, “Comet-Atomic 2020: On Symbolic and\nNeural Commonsense Knowledge Graphs,” in AAAI Conference on\nArtificial Intelligence, vol. 35, 2021, pp. 6384–6392.\n[36] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel, S. Riedel, and\nD. Kiela, “Retrieval-augmented generation for knowledge-intensive\nNLP tasks,” in 34th International Conference on Neural Information\nProcessing Systems, Dec. 2020, pp. 9459–9474.\n[37] S. Izquierdo-Badiola, G. Aleny `a, and C. Rizzo, “Adaptive human-robot\ncollaboration: Evolutionary learning of action costs using an action\noutcome simulator,” in 31st IEEE International Conference on Robot\nand Human Interactive Communication (RO-MAN) , 2023.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8014043569564819
    },
    {
      "name": "Plan (archaeology)",
      "score": 0.6655744314193726
    },
    {
      "name": "Human–robot interaction",
      "score": 0.501960039138794
    },
    {
      "name": "Robot",
      "score": 0.4650403559207916
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4142329692840576
    },
    {
      "name": "Human–computer interaction",
      "score": 0.334713876247406
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210087295",
      "name": "Centre Tecnologic de Telecomunicacions de Catalunya",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I9617848",
      "name": "Universitat Politècnica de Catalunya",
      "country": "ES"
    }
  ],
  "cited_by": 11
}