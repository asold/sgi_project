{
  "title": "Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network",
  "url": "https://openalex.org/W3113377113",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5084510895",
      "name": "Jiayi Ji",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5020074537",
      "name": "Yunpeng Luo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5059926864",
      "name": "Xiaoshuai Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5008400844",
      "name": "Fuhai Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102997988",
      "name": "Gen Luo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100716576",
      "name": "Yongjian Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101410329",
      "name": "Yue Gao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5016080094",
      "name": "Rongrong Ji",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998135987",
    "https://openalex.org/W2949474740",
    "https://openalex.org/W2998988444",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2971183306",
    "https://openalex.org/W2968660381",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W2463955103",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2996766022",
    "https://openalex.org/W2817535134",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2800782462",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2971310675",
    "https://openalex.org/W2962931466",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W3034513523",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W3035284526",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W3042227549",
    "https://openalex.org/W2984138079",
    "https://openalex.org/W3035323998",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2552161745",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035160838",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2965359408"
  ],
  "abstract": "Transformer-based architectures have shown great success in image captioning, where object regions are encoded and then attended into the vectorial representations to guide the caption decoding. However, such vectorial representations only contain region-level information without considering the global information reflecting the entire image, which fails to expand the capability of complex multi-modal reasoning in image captioning. In this paper, we introduce a Global Enhanced Transformer (termed GET) to enable the extraction of a more comprehensive global representation, and then adaptively guide the decoder to generate high-quality captions. In GET, a Global Enhanced Encoder is designed for the embedding of the global feature, and a Global Adaptive Decoder are designed for the guidance of the caption generation. The former models intra- and inter-layer global representation by taking advantage of the proposed Global Enhanced Attention and a layer-wise fusion module. The latter contains a Global Adaptive Controller that can adaptively fuse the global information into the decoder to guide the caption generation. Extensive experiments on MS COCO dataset demonstrate the superiority of our GET over many state-of-the-arts.",
  "full_text": "Improving Image Captioning by Leveraging Intra- and Inter-layer Global\nRepresentation in Transformer Network\nJiayi Ji1, Yunpeng Luo1, Xiaoshuai Sun1,2*, Fuhai Chen1,\nGen Luo1, Yongjian Wu3, Yue Gao4, Rongrong Ji1,2\n1 Media Analytics and Computing Lab, Department of Artiï¬cial Intelligence,\nSchool of Informatics, Xiamen University, 361005, China\n2 Institute of Artiï¬cial Intelligence, Xiamen University\n3 Tencent Youtu Lab 4 Tsinghua University\njjyxmu@gmail.com,lyricpoem1997@gmail.com,xssun@xmu.edu.cn,chenfuhai3c@163.com,\nluogen@stu.xmu.edu.cn, littlekenwu@tencent.com, gaoyue@tsinghua.edu.cn, rrji@xmu.edu.cn\nAbstract\nTransformer-based architectures have shown great success\nin image captioning, where object regions are encoded and\nthen attended into the vectorial representations to guide the\ncaption decoding. However, such vectorial representations\nonly contain region-level information without considering the\nglobal information reï¬‚ecting the entire image, which fails to\nexpand the capability of complex multi-modal reasoning in\nimage captioning. In this paper, we introduce a Global En-\nhanced Transformer (termed GET) to enable the extraction of\na more comprehensive global representation, and then adap-\ntively guide the decoder to generate high-quality captions. In\nGET, a Global Enhanced Encoder is designed for the embed-\nding of the global feature, and a Global Adaptive Decoder\nare designed for the guidance of the caption generation. The\nformer models intra- and inter-layer global representation by\ntaking advantage of the proposed Global Enhanced Attention\nand a layer-wise fusion module. The latter contains a Global\nAdaptive Controller that can adaptively fuse the global infor-\nmation into the decoder to guide the caption generation. Ex-\ntensive experiments on MS COCO dataset demonstrate the\nsuperiority of our GET over many state-of-the-arts.\nINTRODUCTION\nImage captioning aims to describe the semantic content of\nan image via neural language, which has recently attracted\nextensive research attention. Inspired by the sequence-to-\nsequence model for machine translation, most captioning\nmodels (Vinyals et al. 2016; Xu et al. 2015; Anderson et al.\n2018; Huang et al. 2019) mainly adopt a encoder-decoder\nframework, where an encoder network encodes the input im-\nage into a vectorial feature, and a decoder network takes the\nvectorial feature as input and generates the output caption.\nSuch an encoder-decoder framework is recently well pro-\nmoted with the development of the Transformer (Vaswani\net al. 2017), where the self-attention is efï¬ciently utilized to\ncapture the correlations among the regions and words (Liu\net al. 2019; Huang et al. 2019; Li et al. 2019a; Herdade et al.\n2019; Cornia et al. 2020).\n*Corresponding Author\nCopyright Â© 2021, Association for the Advancement of Artiï¬cial\nIntelligence (www.aaai.org). All rights reserved.\n(a)\n(b)\nFigure 1: (a) The self-attention mechanism in the l-th layer\nof a standard Transformer. The vectorial representation vl\ni\nis region-biased, which only focuses on the region-level in-\nformation (Devlin et al. 2018; Song et al. 2020; Weng et al.\n2020). (b) Two key issues of the traditional Transformer-\nbased captioning model that we try to address: object miss-\ning (top: missing â€œsnowâ€) and false prediction (bottom: pre-\ndicting â€œplaying with a boyâ€ as â€œwaking downâ€).\nIn the Transformer architecture, a set of image regions\nare encoded and attended into vectorial representations, as\nshown in Fig. 1 (a). These representations are then fused into\nthe decoder to generate the corresponding captions. How-\never, as demonstrated by earlier works (Devlin et al. 2018;\nSong et al. 2020; Weng et al. 2020), even though the vecto-\nrial representations of these regions are hierarchically calcu-\nlated by being attended to all regions in the image, they still\nignore the image-level characteristics and are thereby less\neffective for the decoder (Weng et al. 2020; Anderson et al.\n2018). It causes the problem of object missing when gener-\narXiv:2012.07061v1  [cs.CV]  13 Dec 2020\nating descriptions, which is attributed to the limit number of\ncategories in object detectors. As shown in the top of Fig. 1\n(b), an important concept, i.e. â€œsnowâ€, is not presented. Be-\nsides, it is more error-prone by focusing on local information\nwhile ignoring global guidance, as shown in the bottom of\nFig. 1 (b), which is attributed to treating each object in iso-\nlation, to lead to a relationship bias.\nTo improve the caption quality, a natural way is to capture\nand leverage global representation to guide the selection of\nattractive objects and their relationships, which is however\nnontrivial due to two challenges. First, directly extracting a\nglobal representation from an image by techniques like pool-\ning might introduce strong contextual noises, which severely\ncause semantic ambiguity and damage the representation ac-\ncuracy. Such damage can be even accumulated for multi-step\nself-attention in Transformers. Second, the extracted global\nrepresentation can not be directly used by the Transformer\ndecoder since the need for global guidance varies during the\ngeneration of captions.\nTo solve the above problems, we propose a new Trans-\nformer architecture, i.e., Global Enhanced Transformer\n(termed GET) as shown in Fig. 2. GET captures the global\nfeature via Global Enhanced Attention and utilizes the\nglobal feature to guide the caption generation via Gated\nAdaptive Controller. In GET, we ï¬rst design a Global En-\nhanced Encoder to extract intra- and inter-layer global rep-\nresentations. Speciï¬cally, we adopt Global Enhanced Atten-\ntion to aggregate local information from each layer to form\nintra-layer global representation. After that, the global fea-\ntures are sequentially aggregated among layers via recurrent\nneural networks, which discard useless information from the\nprevious layers. Then we adaptively fuse the distilled global\nrepresentation into the decoder via a Global Adaptive Con-\ntroller module, which can be implemented by two alterna-\ntive gating modules to control the fusion,i.e., Gate Adaptive\nController and Multi-Head Adaptive Controller. As the lo-\ncal vectorial representations may be insufï¬ciently compre-\nhensive in detail, GET explores the global parts of images to\nsupplement the local vectorial representation, which could\nbe more comprehensive and instructive for caption genera-\ntion.\nTo sum up, our major contributions are itemized below:\nâ€¢ We address the issue of object missing and relationship\nbias by leveraging global represention to provide more\ncomprehensive visual information and play the role of\nconnecting various local parts, which is fundamental in\nimage captioning task.\nâ€¢ We devise a unique encoder, termed Global Enhanced\nEncoder, which enables the Transformer framework to\nmodel intra- and inter-layer global information simulta-\nneously, and propose a novel gating mechanism named\nGated Adaptive Controller to provide an adaptive and so-\nphisticated control for the fusion of global information.\nâ€¢ Through extensive experiments, we demonstrate that our\nGlobal Enhanced Transformer (GET) model can achieve\nnew state-of-the-art performance on MS COCO dataset.\nRELATED WORK\nImage Captioning. Inspired by the encoder-decoder archi-\ntectures in machine translation (Bahdanau, Cho, and Bengio\n2014; Sutskever, Vinyals, and Le 2014), most existing im-\nage captioning approaches typically adopt the CNN-RNN\nframework (Vinyals et al. 2016; Karpathy and Fei-Fei 2015),\nwhere a convolution neural network (CNN) (He et al. 2016;\nLin et al. 2020) is used to encode a given image, which is\nfollowed by a recurrent neural network (RNN) (Hochreiter\nand Schmidhuber 1997) to decode the CNN output into a\nsentence. Recently, a variety of advanced models (Yao et al.\n2018; Yang et al. 2019; Anderson et al. 2018; Lu et al. 2017)\nhave been proposed with attention (Xu et al. 2015) and RL-\nbased training objectives (Rennie et al. 2017).\nTransformer-based Image Captioning. Some recent ap-\nproaches have explored the use of the Transformer model\n(Vaswani et al. 2017) in Vision-Language tasks. (Huang\net al. 2019) introduced a Transformer-like encoder to en-\ncode the regions into the hidden states, which was paired\nwith an LSTM decoder. Recently, (Zhu et al. 2018; Herdade\net al. 2019; Pan et al. 2020; Guo et al. 2020; Li et al. 2019b;\nCornia et al. 2020) proposed to replace conventional RNN\nwith the Transformer architecture, achieving new state-of-\nthe-art performance. On the same line, (Li et al. 2019a; Liu\net al. 2019, 2020) used the Transformer to integrates both vi-\nsual information and additional semantic concepts given by\nan external tagger. However, leveraging global information\nin the Transformer for the image captioning task has never\nbeen explicitly explored, which motivates our work in this\npaper.\nPRELIMINARIES\nThe Transformer-based models formulate the calculation of\nthe t-th hidden state of decoder as\nht = Decoder(Encoder(I),w1,Â·Â·Â· ,wtâˆ’1), (1)\nwhere wi represents the feature embedding of the i-th word.\nThe Transformer contains an encoder which consists of a\nstack of self-attention and feed-forward layers, and a de-\ncoder which uses self-attention on textual words and cross-\nattention over the vectorial representations from the encoder\nto generate the caption word by word.\nWe ï¬rst present a basic form of attention, called â€œScaled\nDot-Product Attentionâ€ , which is ï¬rst proposed as a core\ncomponent in Transformer (Vaswani et al. 2017). All intra-\nmodality and cross-modality interactions between word and\nimage-level features are modeled via this basic form of at-\ntention. The attention module operates on some queries Q,\nkeys K and values V and generates weighted average vec-\ntors Ë†V, which can be formulated as:\nË†V = Attention (Q,K,V ) = softmax\n(QKT\nâˆš\nd\n)\nV, (2)\nwhere Qis a matrix of nq query vectors, Kand V both con-\ntain nk keys and values, all with the same dimensionality,\nand d is a scaling factor.\na young man hitting a tennis ball with a tennis racket\nFaster \nRCNN\nEncoder Layer 1\nEncoder Layer \nğ‘™ğ‘™ + 1\nÃ— (L-1)\nLSTM\nLSTM\nLSTM\nğ‘£ğ‘£1\n0~ğ‘£ğ‘£ğ‘ğ‘\n0\nğ‘£ğ‘£1\nğ¿ğ¿~ğ‘£ğ‘£ğ‘ğ‘\nğ¿ğ¿\nğ‘”ğ‘”0\nğ‘”ğ‘”ğ¿ğ¿\nğ‘Šğ‘Šğ‘¡ğ‘¡âˆ’1\nSelf-Attention\nCross-Attention\nFeedforward\nLinear+Softmax\nman\nğ»ğ»ğ‘¡ğ‘¡\nğ‘™ğ‘™\nğ‘ğ‘ğ‘¡ğ‘¡\nğ‘™ğ‘™+1\nğ‘’ğ‘’ğ‘¡ğ‘¡\nğ‘™ğ‘™+1\nâ„ğ‘¡ğ‘¡\nğ‘™ğ‘™+1\nâ„ğ‘¡ğ‘¡\nğ¿ğ¿\nÃ— L\nğ‘”ğ‘”ğ¹ğ¹\nFFN\nGEA\nğ‘£ğ‘£ğ‘ğ‘\nğ‘™ğ‘™\nğ‘£ğ‘£ğ‘ğ‘\nğ‘™ğ‘™+1\nFFN\nGEA\nğ‘”ğ‘”ğ‘ğ‘\nğ‘™ğ‘™\nğ‘”ğ‘”ğ‘ğ‘\nğ‘™ğ‘™+1\nâ‹¯\nFFN\nGEA\nğ‘£ğ‘£1\nl\nğ‘£ğ‘£ğ‘ğ‘\nğ‘™ğ‘™+1\nğ‘£ğ‘£ğ‘ğ‘\nL ğ‘£ğ‘£ğ‘ğ‘\nL ğ‘£ğ‘£ğ‘ğ‘\nLâ‹¯\nAttention\nğ‘£ğ‘£ğ‘ğ‘\nL ğ‘£ğ‘£ğ‘ğ‘\nL ğ‘£ğ‘£ğ‘ğ‘\nLâ‹¯value\nkey\net\nl+1\nÌ… ğ‘’ğ‘’ğ‘¡ğ‘¡\nğ‘™ğ‘™+1\nğ‘’ğ‘’ğ‘¡ğ‘¡\nğ‘™ğ‘™+1\nğ‘”ğ‘”ğ¹ğ¹query\nGlobal Enhanced Encoder Global Adaptive Decoder\nGlobal Adaptive Controller\nCross-attention\nGlobal Enhanced Attention\nFigure 2: Overview of our Global Enhanced Transformer Networks (GET) for image captioning. A set of regions are ï¬rst fed\ninto a global enhanced encoder to extract intra- and inter-layer global information and region-level representation, which are\nthen adaptively fused into the decoder to generate captions. Notice that the Residual Connections, Layer Normalizations, and\nEmbedding Layers are omitted.\nTo extend the capacity of exploring subspaces, Trans-\nformer employs an effective module called multi-head at-\ntention, which is deï¬ned as\nMultiHead(Q,K,V ) =Concat(H1,...,H h) WO, (3)\nHi = Attention\n(\nQWQ\ni ,KW K\ni ,VW V\ni\n)\n, (4)\nwhere WQ\ni ,WK\ni ,WV\ni âˆˆ R\nd\nh Ã—d are the independent head\nprojection matrices, i = 1,2,Â·Â·Â· ,h, and WO denotes the\nlinear transformation.\nOUR METHOD\nIn this section, we devise our Global Enhanced Trans-\nformer (GET) for image captioning. As shown in Fig. 2, the\noverall architecture follows the encoder-decoder paradigm.\nFirst, a global-enhanced encoder maps the original inputs\ninto highly abstract local representations and extracts the\nintra- and inter-layer global representation. Then the de-\ncoder adaptively incorporates the multimodal information\nsimultaneously through the proposed global adaptive con-\ntroller to generate the caption word by word.\nGlobal-enhanced Encoder\nThe image is represented as a group of visual features V =\n{v1,v2,Â·Â·Â· ,vN}extracted from a pre-trained object detec-\ntor as (Ren et al. 2015), where N is the number of visual\nregions. Speciï¬cally, the detector is a Faster-RCNN model\npre-trained on the Visual Genome dataset (Krishna et al.\n2016). We can represent the images as:\ng= 1\nN\nNâˆ‘\ni=1\nvi. (5)\nEach encoder is a stack of L identical layers, of which\neach one contains a novel structure,i.e., the global-enhanced\nself-attention (GEA). To adapt the feature dimensionality\nto the encoder, the visual features V is ï¬rst fed into a\nfully-connected layer, then we get projected features V0 =\n{v0\n1,v0\n2,Â·Â·Â· ,v0\nN}and g0.\nGlobal-enhanced attention . The early methods only\nfeed regions to the encoder to extract the vectorial repre-\nsentation. As shown in (Devlin et al. 2018; Song et al. 2020;\nWeng et al. 2020), even though the vectorial representation\nof each region is hierarchically calculated by attending to\nall regions in the image, these vectorial representations only\ncontain local features which focus on the region-level in-\nformation. To capture a comprehensive global representa-\ntion, both region features V and global feature g are fed\ninto the multi-head self-attention module in each layer. By\nthis way, the local information can be aggregated to form\nthe global representation, through which we can capture\nthe intra-layer global information. Speciï¬cally, the output\nof the l-th (0 â‰¤ l<L) layer Ol âˆˆ RdÃ—(n+1) is fed into\nthe multi-head self-attention module in the (l+ 1)-th layer,\nwhich is then followed by a residual connection and a layer-\nnormalization:\nV\nl+1\n=GEA(Ol)\n=MultiHead(Ol,Ol,Ol),\n(6)\nVl+1 = LayerNorm(Ol + V\nl+1\n), (7)\nwhere O0 = (V0; g0), and the residual connections help\navoid the vanishing gradient problem the training phase.\nThen a ï¬nal feed-forward neural network is adopted for ad-\nditional processing of the outputs, which is also followed by\na residual connection and a layer normalization step:\nOl+1 = LayerNorm(Vl+1 + FFN (Vl+1)), (8)\nAs illustrated in (Dou et al. 2018; Wang et al. 2020c), the\nrepresentations in different layers have different meanings.\nThus we integrate the global representation from different\nlayers to fuse all the low- and high-level information. Note\nthat such a fusion can also help ease the information ï¬‚ow in\nthe stack (Wang et al. 2020c). A straightforward way is pool-\ning (e.g., average pooling), which however loses layer infor-\nmation. In contrast, we adopt LSTM network (Hochreiter\nand Schmidhuber 1997) for layer-wise fusion and achieve\nthe ï¬nal global representation gF:\nhi = LSTM(gi,hiâˆ’1),gF = hL, (9)\nwhere the LSTM control the model to forget useless infor-\nmation from previous layers via the forgetting gate, which\naggregates the global representation from the ï¬rst layer to\nL-th layer to obtain inter-layer information.\nGlobal Adaptive Decoder\nIn the decoding phase, the global representation was adap-\ntively fused into the decoder to guide caption generation.\nSimilar to the encoder, the decoder consists of N identi-\ncal layers. We start with the basic layer of the global adap-\ntive decoder, which contains a global adaptive controller\n(GAC) to decide how much the global contextual informa-\ntion should be considered.\nBased on the local representation VL and global repre-\nsentation gF, the decoder generates captions for the image\nword-by-word. Suppose the decoder is generating the t-th\nword in the target sentence. We denote wt âˆˆ RdÃ—1as the\nvector representation of the t-th word, which is the sum of\nword embedding and positional encoding. Therefore, the in-\nput matrix representation for time step tis:\nWtâˆ’1 = (w0,w1,Â·Â·Â· ,wtâˆ’1), (10)\nwhere w0 represents the start of sentence.\nFor the (l + 1) -th layer, the inputs Hl\nt =\n{hl\n1,hl\n2,Â·Â·Â· ,hl\nt} âˆˆ RdÃ—t are fed into a multi-head\nself-attention module:\nal+1\nt = MutiHead(hl\nt,Hl\nt,Hl\nt). (11)\nNote that Wtâˆ’1 are the inputs of the ï¬rst layer and h0\nt =\nwtâˆ’1. Then there is a residual connection around them,\nwhich is followed by a layer-normalization step:\nal+1\nt = LayerNorm(hl\nt + al+1\nt ). (12)\nSubsequently, the output al+1\nt is passed into the other multi-\nhead cross-attention, e.g., GAC to incorporate with features\nV and gF, which is followed by a residual connection and a\nlayer-normalization:\nel+1\nt = GAC(al+1\nt ,V L,gF) (13)\nel+1\nt = LayerNorm(al+1 + el+1\nt ), (14)\nwhere el+1\nt contains multi-model information, which is\nadaptively reï¬ned by the global representation to model a\nmore comprehensive and suitable representation. The detail\nof GAC is described in the next subsection. Then we feed\nit into a feed-forward neural network (FFN), which is fol-\nlowed by a residual connection and a layer-normalization to\nobtain the output:\nhl+1\nt = LayerNorm(el+1\nt + FFN (el+1\nt )). (15)\nFinally, the output of layer N is fed into the classiï¬er over\nvocabulary to predict the next word. Let the predicted cap-\ntion be Yt = {y0,y1,Â·Â·Â· ,yt}, where yi âˆˆV, and V is the\nvocabulary of the captions. Then the conditional probability\ndistribution of words at time t is p(yt|Ytâˆ’1), which can be\ncalculated by:\np(yt|Ytâˆ’1) =softmax(WyhL\nt ), (16)\nwhere Wy âˆˆR|V|Ã—d, and |V|is the number of words in the\nvocabulary.\nGlobal Adaptive Controller Cross-attention\nIn the generation process, we design two alternative func-\ntions for the global adaptive controller to fuse the global in-\nformation into decoder according to the contextual signals,\ni.e., Gate Adaptive Controller (GAC) and Multi-Head Adap-\ntive Controller (MAC).\nGate Adaptive Controller Self-Attention. The demand\nfor global information for each target word is different. Mo-\ntivated by (Lu et al. 2017), we propose a context gating\nmechanism to control the importance of global information.\nThe context gate is determined by the query al+1\nt and the\nglobal representation gL:\nÎ±= sigmoid\n(\n(al+1\nt )TgL\n)\n. (17)\nWe then adaptively fuse the global representation to reï¬ne\nthe output from multi-head self-attention as below:\nË†el+1\nt = MultiHead(al+1\nt ,V L,V L), (18)\nel+1\nt = Ë†el+1\nt + Î±âˆ—gL. (19)\nMulti-Head Adaptive Controller Self-Attention. A\nmore sophisticated method is to use the multi-head attention\nfor fusion, which naturally fuses the local represention and\nthe global representation by taking a weighted sum of region\nvectors VL and global vector gL. We set Vg = (VL; gF) âˆˆ\nR(N+1)Ã—d\nel+1\nt = MultiHead(al+1\nt ,Vg,Vg) (20)\nNoticeably, attentive weights depend solely on the pairwise\nsimilarities between visual vectors ( e.g., region vectors and\nglobal vectors) and the query vector. In such a way, the\noutput can capture suitable global information to reï¬ne the\noriginal local representation. Besides, the multi-head mech-\nanism allows the model to jointly attend to information from\ndifferent representation subspaces.\nTraining\nFor a given caption YT = {y0,Â·Â·Â· ,yT}, the distribution is\ncalculated as the product of the conditional distributions at\nall time steps:\npl(Y) =\nTâˆ\nt=0\np(yt|Ytâˆ’1). (21)\nThe training process consists of two phases: pre-training\nby supervised learning and ï¬ne-tuning by reinforcement\nB-1 B-4 M R C S\nSCST - 34.2 26.7 55.7 114.0 -\nUp-Down 79.8 36.3 27.7 56.9 120.1 21.4\nRFNet 79.1 36.5 27.7 57.3 121.9 21.2\nGCN-LSTM 80.5 38.2 28.5 58.3 127.6 22.0\nUp-Down+HIP - 38.2 28.4 58.3 127.6 22.0\nSGAE 80.8 38.4 28.4 58.6 127.8 22.1\nETA 81.5 39.3 28.8 58.9 126.6 22.7\nSRT 80.3 38.5 28.7 58.4 129.1 22.4\nAoANet 80.2 38.9 29.2 58.8 129.8 22.4\nORT 80.5 38.6 28.7 58.4 128.3 22.6\nMMT 80.8 39.1 29.2 58.6 131.2 22.6\nPOS-SCAN 80.2 38.0 28.5 - 125.9 22.2\nCBT - 39.0 29.1 59.2 128.1 22.9\nOurs(w/ GAC) 80.8 38.8 29.0 58.6 130.5 22.4\nOurs(w/ MAC) 81.5 39.5 29.3 58.9 131.6 22.8\nTable 1: Comparison with the state of the art on the â€œKarpa-\nthyâ€ test split, in single-model setting. All values are re-\nported as percentage (%).\nlearning. Let Î¸ be the parameters of the model. In pre-\ntraining, given a target ground truth sequence Yâˆ— =\n{yâˆ—\n0,Â·Â·Â· ,yâˆ—\nT}, the objective is to minimize the cross-entropy\nloss (XE):\nL(Î¸) =âˆ’\nTâˆ‘\nt=0\nlog\n(\np(yâˆ—\nt|Yâˆ—\ntâˆ’1)\n)\n. (22)\nAt the ï¬ne-tuning stage, we employ a variant of the self-\ncritical sequence training approach (Rennie et al. 2017) on\nsequences sampled using beam search to directly optimize\nthe metric, following previous works (Rennie et al. 2017;\nAnderson et al. 2018). The ï¬nal gradient for one sample is\ncalculated as:\nâˆ‡Î¸L(Î¸) =âˆ’1\nk\nkâˆ‘\ni=1\n((\nr\n(\nY i)\nâˆ’b\n)\nâˆ‡Î¸log p\n(\nY i))\n(23)\nwhere r(Â·) can be any evaluation score metric, and we use\nthe CIDEr-D score as a reward. Yi = {yi\n0,Â·Â·Â· ,yi\nT}is the\ni-th sentence in the beam, and b =\n(âˆ‘\nir\n(\nYi))\n/k is the\nbaseline, computed as the mean of the rewards obtained by\nthe sampled sequences.\nEXPERIMENTS\nDataset and Implementation Details\nAll the experiments are conducted on the most popular\nbenchmark dataset of image captioning, i.e., MS COCO (Lin\net al. 2014). The whole MSCOCO dataset contains 123,287\nimages, which includes 82,783 training images, 40,504 val-\nidation images, and 40,775 testing images. Each image is\nequipped with ï¬ve ground-truth sentences. The online eval-\nuation is done on the MS COCO test split, for which ground-\ntruth annotations are not publicly available. In ofï¬‚ine test-\ning, we use the Karpathy splits (Karpathy and Fei-Fei 2015)\nthat have been used extensively for reporting results in previ-\nous works. This split contains 113,287 training images, and\n5K images respectively for validation and testing.\nModel B-1 B-4 M R C S\nEnsemble/Fusion of 2 models\nGCN-LSTM 80.9 38.3 28.6 58.5 128.7 22.1\nSGAE 81.0 39.0 28.4 58.9 129.1 22.2\nETA 81.5 39.9 28.9 59.0 127.6 22.6\nGCN-LSTM+HIP - 39.1 28.9 59.2 130.6 22.3\nMMT 81.6 39.8 29.5 59.2 133.2 23.1\nOurs 81.9 40.3 29.6 59.4 133.5 23.3\nEnsemble/Fusion of 4 models\nSCST - 35.4 27.1 56.6 117.5 -\nRFNet 80.4 37.9 28.3 58.3 125.7 21.7\nAoANet 81.6 40.2 29.3 59.4 132.0 22.8\nMMT 82.0 40.5 29.7 59.5 134.5 23.5\nOurs 82.1 40.6 29.8 59.6 135.1 23.8\nTable 2: Comparison with the state of the art on the â€œKarpa-\nthyâ€ test split, using ensemble technique, where B-N, M,\nR, C and S are short for BLEU-N, METEOR, ROUGE-L,\nCIDEr and SPICE scores. All values are reported as per-\ncentage (%).\nWe use Faster R-CNN (Ren et al. 2015) with ResNet-101\n(He et al. 2016) ï¬netuned on the Visual Genome dataset (Kr-\nishna et al. 2016) to represent image regions. In our model,\nwe set the dimensionality d of each layer to 512, and the\nnumber of heads to 8. We employ dropout with a keep prob-\nability of 0.9 after each attention and feed-forward layer.\nPre-training with XE is done following the learning rate\nscheduling strategy with a warmup equal to 10, 000 itera-\ntions. Then, during CIDEr-D optimization, we use a ï¬xed\nlearning rate of 5 Ã—10âˆ’6. We train all models using the\nAdam optimizer (Kingma and Ba 2014), a batch size of 50,\nand a beam size equal to 5. At the inference stage, we adopt\nthe beam search strategy and set the beam size as 3. Five\nevaluation metrics, i.e., BLEU (Papineni et al. 2002), ME-\nTEOR (Banerjee and Lavie 2005), ROUGE-L (Lin 2004),\nCIDEr (Vedantam, Lawrence Zitnick, and Parikh 2015), and\nSPICE (Anderson et al. 2016), are simultaneously utilized\nto evaluate our model.\nPerformance Comparison\nOfï¬‚ine Evaluation. Tab. 1 and Tab. 2 show the perfor-\nmance comparisons between the state-of-the-art models and\nour proposed approach on the ofï¬‚ine COCO Karpathy test\nsplit. We show the performances for both the single model\nversion and the ensemble version. The baseline models we\ncompared include SCST (Rennie et al. 2017), LSTM-A (Yao\net al. 2017), Up-Down (Anderson et al. 2018), RFNet (Ke\net al. 2019), GCN-LSTM (Yao et al. 2018), SGAE (Yang\net al. 2019), AoANet (Huang et al. 2019) ORT (Herdade\net al. 2019), ETA (Li et al. 2019a), MMT (Cornia et al.\n2020), SRT (Wang et al. 2020b), POS-SCAN (Zhou et al.\n2020) and CBT (Wang et al. 2020a). We present the results\nof the proposed GET with two different global adaptive con-\ntrollers (e.g., GAC and MAC). For clarity, the symbol â€œoursâ€\nonly represents the latter one in the following section.\nSingle model. In Tab. 1, we report the performance of\nour method in comparison with the aforementioned state-\nof-the-art methods, using captions predicted from a single\nmodel and optimization on the CIDEr score. Our method\nTransformer: a woman holding a \nbasketball in a room.\nGEA: a young man in a green \nuniform is holding a basketball.\nTransformer: an airplane parked on \nthe runway at an airport.\nGEA: a large airplane parked at the \nairport with a man.\nTransformer: a group of traffic lights \non a pole.\nGEA: a traffic light with a blue sky in \nthe background.\nTransformer: a small plane sitting \non top of a field.\nGEA: a small plane is painted with \nblue and white stripes.\nTransformer: two birds sitting on \ntop of a wooden post.\nGEA: a bird perched on a telephone \npole with power lines.\nGEA: a woman is cooking in a \nkitchen with a stove.\nTransformer: a woman standing in \na kitchen standing next to a stove.\nTransformer: a woman sitting in \nfront of a box.\nGEA: a woman is holding a cake with \na picture on it.\nTransformer: a piece of cake on a \nplate with a fork.\nGEA: a piece of cake on a plate with \na spoon.\nFigure 3: Examples of captions generated by our approach and the standard Transformer model. Some detailed and accurate\nwords are marked in green, the wrong words are marked in red, and the inaccurate words are marked in yellow. Our method\nyields more detailed and accurate descriptions.\nModel BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr-D\nMetric c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nSCST 78.1 93.7 61.9 86.0 47.0 75.9 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.0\nLSTM-A 78.7 93.7 62.7 86.7 47.6 76.5 35.6 65.2 27.0 35.4 56.4 70.5 116.0 118.0\nUp-Down 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5\nRF-Net 80.4 95.0 64.9 89.3 50.1 80.1 38.0 69.2 28.2 37.2 58.2 73.1 122.9 125.1\nGCN-LSTM - - 65.5 89.3 50.8 80.3 38.7 69.7 28.5 37.6 58.5 73.4 125.3 126.5\nSGAE 81.0 95.3 65.6 89.5 50.7 80.4 38.5 69.7 28.2 37.2 58.6 73.6 123.8 126.5\nAoANet 81.0 95.0 65.8 89.6 51.4 81.3 39.4 71.2 29.1 38.5 58.9 74.5 126.9 129.6\nETA 81.2 95.0 65.5 89.0 50.9 80.4 38.9 70.2 28.6 38.0 58.6 73.9 122.1 124.4\nMMT 81.6 96.0 66.4 90.8 51.8 82.7 39.7 72.8 29.4 39.0 59.2 74.8 129.3 132.1\nOurs 81.6 96.1 66.5 90.9 51.9 82.8 39.7 72.9 29.4 38.8 59.1 74.4 130.3 132.5\nTable 3: MS COCO Online Evaluation. All values are reported as percentage (%), with the highest value of each entry high-\nlighted in boldface.\nsurpasses all other approaches in terms of BLEU-4, ME-\nTEOR and CIDEr, and achieves competitive performance on\nSPICE and ROUGE-L compared to the SOTA. In particular,\nit advances the current state of the art on CIDEr by 0.4%.\nEnsemble model. Following the common practice (Ren-\nnie et al. 2017; Huang et al. 2019) of building an ensemble\nof models, we also report the performances of our approach\nwhen averaging the output probability distributions of mul-\ntiple and independently trained instances of our model. In\nTab. 2, we use ensembles of two and four models, trained\nfrom different random seeds. Noticeably, when using four\nmodels, our approach achieves the best performance accord-\ning to all metrics, with an increase of 0.6 CIDEr points with\nrespect to the current state of the art (Cornia et al. 2020).\nOnline Evaluation. Finally, we also report the perfor-\nmance of our method on the online COCO test server. In\nthis case, we use the ensemble of four models previously de-\nscribed, trained on the â€œKarpathyâ€ training split. Results are\nreported in Tab. 3, in comparison with the top-performing\napproaches on the leaderboard. For fairness of comparison,\nthey also used an ensemble conï¬guration. As can be seen,\nour method surpasses the current state of the art on most of\nthe metrics, achieving an improvement of 1.0 CIDEr points\nwith respect to the best performer.\nQualitative Analysis. Fig. 2 shows several image cap-\ntioning results of the plain Transformer and our GET. Gen-\nerally, compared with the captions of the plain Transformer\nwhich are somewhat relevant to image content and logically\ncorrect, our GET produces more accurate and descriptive\nsentences by exploiting intra- and inter-modal interactions.\nFor example, our GET generates the phrase of â€œa green\nuniformâ€ and â€œa manâ€, while they are missing from the\nplain Transformer. Besides, our GET generates more pre-\ncise phrases, such as â€œholding a cake with a picture on itâ€\nand â€œcookingâ€. These also conï¬rm the advantage of captur-\ning and leveraging the intra- and inter-layer global represen-\nFigure 4: The visualization of attended image regions along with the caption generation process for plain Transformer and the\nproposed GET. At the decoding step for each word, we outline the image region with the maximum output attribution in red.\nLayer BLUE-4 METEOR ROUGE-L CIDEr\n2 38.2 28.9 58.3 129.7\n3 39.5 29.2 58.9 131.6\n4 39.2 29.2 58.6 130.7\n5 39.0 28.9 58.5 130.3\n6 39.0 29.0 58.5 130.3\nTable 4: Ablation on the number of encoding and decoding\nlayers. All values are reported as percentage (%).\ntation in the Transformer architectures.\nExperimental Analysis\nAblation Study. To validate the effectiveness of our pro-\nposed modules, we conduct ablation studies by comparing\ndifferent variants of the GET.\nFirstly, we investigate the impact of the number of the en-\ncoding and decoding layers on captioning performance for\nthe GET. As shown in Tab. 4, varying the number of layers,\nwe observe a slight decrease in performance when increas-\ning the number of layers. Following this ï¬nding, all subse-\nquent experiments uses three layers.\nThen, we investigate the impact of all the proposed mod-\nules in both encoder and decoder. We choose the plain Trans-\nformer as the baseline, which is shown in the third line\nin Tab. 5. Then we extend the baseline model by adopting\nthe GEA module, which slightly improves the performance.\nThe results indicate that the GEA module can also improve\nthe region level presentation via aggregate information from\nglobal representation. Then we investigate the impact of dif-\nferent global representations. As shown in the 5-th line and\n6-th line, the performance improvements validate the effec-\ntiveness of GEA to obtain better presentation than the orig-\ninal presentation g0 via aggregating the intra-layer informa-\ntion. Then we exploit different strategies to fuse the inter-\nlayer information, and the LSTM network obtains the best\nperformance, which basically validates the effectiveness of\nsuch layer-wise global representation. Both the GAC and\nMAC gain expected performance, which further indicates\nthe effectiveness of our intra- and inter-layer global repre-\nsentation. And MAC is the better one, which shows that the\nMulti-Head mechanism works better at feature fusion for its\nencoder decoder B-4 M R Cintra-layer inter-layer\n- - - 37.9 28.0 57.9 128.1\nGEA - - 38.1 28.1 58.2 128.3\ng0 - MAC 38.2 28.3 58.0 128.6\nGEA - MAC 38.4 28.3 58.2 128.9\nGEA average MAC 38.5 28.7 58.1 129.4\nGEA attention MAC 38.7 29.0 58.2 129.8\nGEA LSTM MAC 39.5 29.2 58.9 131.6\nGEA LSTM GAC 38.8 29.0 58.6 130.5\nTable 5: Ablation on different variants of the Transformer.\nAll values are reported as percentage (%).\nability of complex relationship modeling.\nAttention Visualization. In order to better qualitatively\nevaluate the generated results with GET, we visualize the\nevolutions of the contribution of detected regions to the\nmodel output along with the caption generation processes\nfor plain Transformer and the proposed GET in Fig. 4. The\ncontribution of one region with respect to the output is given\nby complex non-linear dependencies, which cannot be ex-\ntracted easily. Therefore, we employ the Integrated Gradi-\nents approach (Sundararajan, Taly, and Yan 2017), which ap-\nproximates the integral of gradients with respect to the given\ninput via a summation. Results presented in Fig. 4 show that\nour approach can help to ground the correct image regions\nto words by exploring the proposed global representation.\nCONCLUSION\nIn this paper, we present Global Enhanced Transformer\n(GET) for image captioning. GET addresses the problem of\ntraditional Transformer-based architectures on the ignorance\nof global contextual information that limits the capability of\nreasoning in image captioning. Our model incorporates the\nGlobal Enhanced Encoder which captures both intra- and\ninter-layer global representation to provide more compre-\nhensive visual information and play the role of connecting\nvarious local parts, and the Global Adaptive Decoder which\nadaptively fuses the global information into the decoder to\nguide caption generation. We show the superior performance\nof the proposed GET both quantitatively and qualitatively on\nthe MS COCO datasets.\nAcknowledgments\nThis work is supported by the National Science Fund\nfor Distinguished Young (No.62025603), the National Nat-\nural Science Foundation of China (No.U1705262, No.\n62072386, No. 62072387, No. 62072389, No. 62002305,\nNo.61772443, No.61802324 and No.61702136) and and\nGuangdong Basic and Applied Basic Research Foundation\n(No.2019B1515120049).\nReferences\nAnderson, P.; Fernando, B.; Johnson, M.; and Gould, S.\n2016. Spice: Semantic propositional image caption evalu-\nation. In ECCV.\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-up and top-down at-\ntention for image captioning and visual question answering.\nIn CVPR.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural ma-\nchine translation by jointly learning to align and translate.\narXiv .\nBanerjee, S.; and Lavie, A. 2005. METEOR: An automatic\nmetric for MT evaluation with improved correlation with hu-\nman judgments. In ACL (Workshops).\nCornia, M.; Stefanini, M.; Baraldi, L.; and Cucchiara, R.\n2020. Meshed-Memory Transformer for Image Captioning.\nIn CVPR.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv .\nDou, Z.-Y .; Tu, Z.; Wang, X.; Shi, S.; and Zhang, T. 2018.\nExploiting deep representations for neural machine transla-\ntion. arXiv .\nGuo, L.; Liu, J.; Zhu, X.; Yao, P.; Lu, S.; and Lu, H. 2020.\nNormalized and Geometry-Aware Self-Attention Network\nfor Image Captioning. In CVPR.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR.\nHerdade, S.; Kappeler, A.; Boakye, K.; and Soares, J. 2019.\nImage Captioning: Transforming Objects into Words. In\nNeurIPS.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation.\nHuang, L.; Wang, W.; Chen, J.; and Wei, X.-Y . 2019. Atten-\ntion on Attention for Image Captioning. In ICCV.\nKarpathy, A.; and Fei-Fei, L. 2015. Deep visual-semantic\nalignments for generating image descriptions. In CVPR.\nKe, L.; Pei, W.; Li, R.; Shen, X.; and Tai, Y .-W. 2019. Re-\nï¬‚ective Decoding Network for Image Captioning. In ICCV.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint.\nKrishna, R.; Zhu, Y .; Groth, O.; Johnson, J.; Hata, K.;\nKravitz, J.; Chen, S.; Kalantidis, Y .; Li, L.-J.; Shamma,\nD. A.; Bernstein, M.; and Fei-Fei, L. 2016. Visual Genome:\nConnecting Language and Vision Using Crowdsourced\nDense Image Annotations.\nLi, G.; Zhu, L.; Liu, P.; and Yang, Y . 2019a. Entangled\nTransformer for Image Captioning. In ICCV.\nLi, J.; Yao, P.; Guo, L.; and Zhang, W. 2019b. Boosted trans-\nformer for image captioning. Applied Sciences.\nLin, C.-Y . 2004. Rouge: A package for automatic evaluation\nof summaries. In ACL (Workshops).\nLin, M.; Ji, R.; Wang, Y .; Zhang, Y .; Zhang, B.; Tian, Y .;\nand Shao, L. 2020. HRank: Filter Pruning Using High-Rank\nFeature Map. In CVPR.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll Â´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In ECCV.\nLiu, F.; Liu, Y .; Ren, X.; He, X.; and Sun, X. 2019. Aligning\nvisual regions and textual concepts for semantic-grounded\nimage representations. In NeurIPS.\nLiu, F.; Ren, X.; Liu, Y .; Lei, K.; and Sun, X. 2020. Explor-\ning and distilling cross-modal information for image cap-\ntioning. arXiv .\nLu, J.; Xiong, C.; Parikh, D.; and Socher, R. 2017. Knowing\nwhen to look: Adaptive attention via a visual sentinel for\nimage captioning. In CVPR.\nPan, Y .; Yao, T.; Li, Y .; and Mei, T. 2020. X-Linear Attention\nNetworks for Image Captioning. In CVPR.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBLEU: a method for automatic evaluation of machine trans-\nlation. In ACL.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-\ncnn: Towards real-time object detection with region proposal\nnetworks. In NeurIPS.\nRennie, S. J.; Marcheret, E.; Mroueh, Y .; Ross, J.; and Goel,\nV . 2017. Self-critical sequence training for image caption-\ning. In CVPR.\nSong, K.; Wang, K.; Yu, H.; Zhang, Y .; Huang, Z.; Luo, W.;\nDuan, X.; and Zhang, M. 2020. Alignment-Enhanced Trans-\nformer for Constraining NMT with Pre-Speciï¬ed Transla-\ntions. AAAI.\nSundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic\nattribution for deep networks. In ICML.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence to\nsequence learning with neural networks. In NeurIPS.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS.\nVedantam, R.; Lawrence Zitnick, C.; and Parikh, D. 2015.\nCider: Consensus-based image description evaluation. In\nCVPR.\nVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2016.\nShow and tell: Lessons learned from the 2015 mscoco image\ncaptioning challenge. IEEE PAMI.\nWang, J.; Xu, W.; Wang, Q.; and Chan, A. B. 2020a. Com-\npare and Reweight: Distinctive Image Captioning Using\nSimilar Images Sets. arXiv preprint arXiv:2007.06877.\nWang, L.; Bai, Z.; Zhang, Y .; and Lu, H. 2020b. Show, Re-\ncall, and Tell: Image Captioning with Recall Mechanism. In\nAAAI.\nWang, Q.; Li, F.; Xiao, T.; Li, Y .; Li, Y .; and Zhu, J. 2020c.\nMulti-layer representation fusion for neural machine trans-\nlation. arXiv .\nWeng, R.; Wei, H.; Huang, S.; Yu, H.; Bing, L.; Luo, W.;\nand Chen, J. 2020. GRET: Global Representation Enhanced\nTransformer. arXiv .\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-\nnov, R.; Zemel, R.; and Bengio, Y . 2015. Show, attend and\ntell: Neural image caption generation with visual attention.\nIn ICML.\nYang, X.; Tang, K.; Zhang, H.; and Cai, J. 2019. Auto-\nencoding scene graphs for image captioning. In CVPR.\nYao, T.; Pan, Y .; Li, Y .; and Mei, T. 2018. Exploring visual\nrelationship for image captioning. In ECCV.\nYao, T.; Pan, Y .; Li, Y .; Qiu, Z.; and Mei, T. 2017. Boosting\nimage captioning with attributes. In ICCV.\nZhou, Y .; Wang, M.; Liu, D.; Hu, Z.; and Zhang, H. 2020.\nMore Grounded Image Captioning by Distilling Image-Text\nMatching Model. In CVPR.\nZhu, X.; Li, L.; Liu, J.; Peng, H.; and Niu, X. 2018. Cap-\ntioning transformer with stacked attention modules. Applied\nSciences .",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.8323317170143127
    },
    {
      "name": "Computer science",
      "score": 0.7560675144195557
    },
    {
      "name": "Transformer",
      "score": 0.5418323874473572
    },
    {
      "name": "Encoder",
      "score": 0.4977908134460449
    },
    {
      "name": "Decoding methods",
      "score": 0.4936912953853607
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4454711079597473
    },
    {
      "name": "Autoencoder",
      "score": 0.44038066267967224
    },
    {
      "name": "Feature learning",
      "score": 0.4170609414577484
    },
    {
      "name": "Computer vision",
      "score": 0.3496691584587097
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3324132561683655
    },
    {
      "name": "Deep learning",
      "score": 0.22608304023742676
    },
    {
      "name": "Algorithm",
      "score": 0.15825524926185608
    },
    {
      "name": "Engineering",
      "score": 0.082712322473526
    },
    {
      "name": "Voltage",
      "score": 0.0820392370223999
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}