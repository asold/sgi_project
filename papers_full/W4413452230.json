{
  "title": "Impact of retrieval augmented generation and large language model complexity on undergraduate exams created and taken by AI agents",
  "url": "https://openalex.org/W4413452230",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Erick Tyndall",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5116175284",
      "name": "Colleen Gayheart",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5095543678",
      "name": "Alexandre Some",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2076351909",
      "name": "Joseph Genz",
      "affiliations": [
        "University of Hawaii at Hilo"
      ]
    },
    {
      "id": "https://openalex.org/A2951195431",
      "name": "Torrey J. Wagner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4273467788",
      "name": "Brent Langhals",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Erick Tyndall",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5116175284",
      "name": "Colleen Gayheart",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5095543678",
      "name": "Alexandre Some",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2076351909",
      "name": "Joseph Genz",
      "affiliations": [
        "University of Hawaii at Hilo"
      ]
    },
    {
      "id": "https://openalex.org/A2951195431",
      "name": "Torrey J. Wagner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4273467788",
      "name": "Brent Langhals",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4395065483",
    "https://openalex.org/W4404570405",
    "https://openalex.org/W6948396718",
    "https://openalex.org/W6891802577",
    "https://openalex.org/W4394947904",
    "https://openalex.org/W6948426764",
    "https://openalex.org/W4405765801",
    "https://openalex.org/W6600210674",
    "https://openalex.org/W6949549905",
    "https://openalex.org/W4407631942",
    "https://openalex.org/W6801175739",
    "https://openalex.org/W4403579445",
    "https://openalex.org/W6930768765",
    "https://openalex.org/W6966545553",
    "https://openalex.org/W4398192278",
    "https://openalex.org/W4387703131",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W4407423003",
    "https://openalex.org/W6604786921",
    "https://openalex.org/W4398795761",
    "https://openalex.org/W4405173318",
    "https://openalex.org/W3203052795",
    "https://openalex.org/W3199189488",
    "https://openalex.org/W4401857670",
    "https://openalex.org/W4410529474"
  ],
  "abstract": "Abstract The capabilities of large language models (LLMs) have advanced to the point where entire textbooks can be queried using retrieval-augmented generation (RAG), enabling AI to integrate external, up-to-date information into its responses. This study evaluates the ability of two OpenAI models, GPT-3.5 Turbo and GPT-4 Turbo, to create and answer exam questions based on an undergraduate textbook. 14 exams were created with four true-false, four multiple-choice, and two short-answer questions derived from an open-source Pacific Studies textbook. Model performance was evaluated with and without access to the source material using text-similarity metrics such as ROUGE-1, cosine similarity, and word embeddings. Fifty-six exam scores were analyzed, revealing that RAG-assisted models significantly outperformed those relying solely on pre-trained knowledge. GPT-4 Turbo also consistently outperformed GPT-3.5 Turbo in accuracy and coherence, especially in short-answer responses. These findings demonstrate the potential of LLMs in automating exam generation while maintaining assessment quality. However, they also underscore the need for policy frameworks that promote fairness, transparency, and accessibility. Given regulatory considerations outlined in the European Union AI Act and the NIST AI Risk Management Framework, institutions using AI in education must establish governance protocols, bias mitigation strategies, and human oversight measures. The results of this study contribute to ongoing discussions on responsibly integrating AI in education, advocating for institutional policies that support AI-assisted assessment while preserving academic integrity. The empirical results suggest not only performance benefits but also actionable governance mechanisms, such as verifiable retrieval pipelines and oversight protocols, that can guide institutional policies.",
  "full_text": "RESEARCH ARTICLE\nRESEARCH ARTICLE\nImpact of retrieval augmented generation and large language\nmodel complexity on undergraduate exams created and taken\nby AI agents\nErick Tyndall1 , Colleen Gayheart1, Alexandre Some1, Joseph Genz2, Torrey Wagner1 and\nBrent Langhals1\n1Department of Systems Engineering and Management, Air Force Institute of Technology, Wright-Patterson Air Force Base, Ohio,\nUSA\n2Department of Anthropology, University of Hawaiʻi at Hilo, Hilo, Hawaii, USA\nCorresponding author:Erick Tyndall; Email:erick.tyndall@us.af.mil\nReceived: 05 April 2025;Revised: 01 July 2025;Accepted: 21 July 2025\nKeywords: academic examinations; artificial intelligence; generative pre-training transformer; large language models; retrieval\naugmented generation\nAbstract\nThe capabilities of large language models (LLMs) have advanced to the point where entire textbooks can be queried\nusing retrieval-augmented generation (RAG), enabling AI to integrate external, up-to-date information into its\nresponses. This study evaluates the ability of two OpenAI models, GPT-3.5 Turbo and GPT-4 Turbo, to create and\nanswer exam questions based on an undergraduate textbook. 14 exams were created with four true-false, four\nmultiple-choice, and two short-answer questions derived from an open-source Pacific Studies textbook. Model\nperformance was evaluated with and without access to the source material using text-similarity metrics such as\nROUGE-1, cosine similarity, and word embeddings. Fifty-six exam scores were analyzed, revealing that RAG-\nassisted models significantly outperformed those relying solely on pre-trained knowledge. GPT-4 Turbo also\nconsistently outperformed GPT-3.5 Turbo in accuracy and coherence, especially in short-answer responses. These\nfindings demonstrate the potential of LLMs in automating exam generation while maintaining assessment quality.\nHowever, they also underscore the need for policy frameworks that promote fairness, transparency, and accessibility.\nGiven regulatory considerations outlined in the European Union AI Act and the NIST AI Risk Management\nFramework, institutions using AI in education must establish governance protocols, bias mitigation strategies, and\nhuman oversight measures. The results of this study contribute to ongoing discussions on responsibly integrating AI\nin education, advocating for institutional policies that support AI-assisted assessment while preserving academic\nintegrity. The empirical results suggest not only performance benefits but also actionable governance mechanisms,\nsuch as verifiable retrieval pipelines and oversight protocols, that can guide institutional policies.\nPolicy Significance Statement\nRapid progress in large language models means that today’s best-performing system is often overtaken within\nmonths. Yet two design choices appear to deliver durable benefits across architectures and domains: (1) pairing the\nmodel with retrieval-augmented generation so that it grounds its output in verifiable sources, and (2) maintaining\n© The Author(s), 2025. Published by Cambridge University Press. This is an Open Access article, distributed under the terms of the Creative Commons\nAttribution licence (http://creativecommons.org/licenses/by/4.0), which permits unrestricted re-use, distribution and reproduction, provided the\noriginal article is properly cited.\nThis research article was awarded Open Data and Open Materials badges for transparent practices. See the Data Availability\nStatement for details.\nData & Policy(2025), 7: e57\ndoi:10.1017/dap.2025.10024\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\nhuman-in-the-loop oversight for critical applications such as graded assessments. Although limited to GPT-3.5\nTurbo and GPT-4 Turbo models on a single undergraduate-level textbook, the experiment demonstrates these\nprinciples in practice. Adding retrieval-augmented generation cut hallucinations and boosted answer accuracy more\neffectively than upgrading to a more advanced base model alone. For policy makers, this suggests that procurement\nguidelines and accreditation standards should emphasize verifiable retrieval pipelines rather than mandating\nspecific proprietary models. Because retrieval-augmented generation can elevate the performance of lower-cost\nmodels to near parity, equity-minded policies can require that any AI-assisted assessment workflow be reproducible\non an open-weight or low-cost model to reduce the risk of widening resource gaps. These recommendations align\nwith the governance obligations for high-risk educational systems in the EU AI Act and the risk-management steps\nin the NIST AI RMF, including bias mitigation, documentation, and human oversight.\n1. Introduction\nAI is becoming an increasingly prominent and policy-relevant topic, with growing implications for govern-\nance, education, and workforce regulation. With the rapid development of large language models (LLMs),\nsuch as OpenAI’s Generative Pre-training Transformer (GPT) models, new opportunities and challenges\nemerge in the automation of information and content generation. These advances raisecritical policy questions\nsurrounding fairness, transparency, and compliance with national and international education standards.\nThe application in this work uses GPT LLMs for exam generation and answering, specifically\nanalyzing how retrieval-augmented generation (RAG) enhances accuracy and reliability in automated\nacademic assessments. As institutions explore AI-driven testing, policymakers and accreditation bodies\nmust evaluate the implications for educational integrity, bias mitigation, and regulatory compliance,\nensuring that AI tools align with ethical and legal standards in academia. This study contributes to that\nevaluation by testing performance differences between GPT models with and without RAG and\ntranslating those empirical findings into design principles that support responsible, policy-aligned\nimplementation of AI-assisted academic assessments.\n1.1. Overview\nAI has continually been a popular and controversial topic, particularly in the context of assisting with or\nreplacing everyday human tasks, automating processes, shaping workforce dynamics, and influencing\neducation policy. Although AI technologies have existed for decades, they have remained largely\nconfined to specialized domains. This changed with the introduction of consumer-friendly LLMs, which\nmade the technology broadly accessible to the public. This all changed with the creation of OpenAI,\nwhose stated goal is to be“an organization focused on developing artificial generative intelligence to\nbenefit humanity” (Ray, 2023). OpenAI released models such as GPT, GPT-2, and GPT-3, before finally\nreleasing the transformative ChatGPT. Optimized for conversation-based tasks, including contextual\nunderstanding and coherence (Ray,2023), ChatGPT has become increasingly available, with both free\naccess and paid subscription tiers that offer more advanced models and faster response times.\nThe academic domain is one space where ChatGPT use has been especially prevalent, creating both\nopportunities and regulatory concerns. Initially, students used the software to generate answers or even\npapers, leading to ethical concerns about cheating if such practices were prohibited by educational\ninstitutions. However, the GPT space is still relatively unexplored from educators’ and administrators’\nperspectives, such as using a specific resource (i.e., textbook) to generate an exam, create matching solution\nsets, and scoring answers. One example in the literature reviewed was the creation of multiple-choice exams\nfor medical students. According to the study,“GPT-4 can be used as an adjunctive tool in creating multi-\nchoice question medical examinations, yet rigorous inspection by specialist physicians remains pivotal”\n(Klang et al.,2023). However, regulatory bodies and institutions must establish policies to ensure that\nAI-generated assessments align with academic standards, maintain fairness, and mitigate potential biases.\ne57-2 Erick Tyndall et al.\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\nAlthough the creation of LLMs has created a beneficial effect on the scope of generative AI, these\nmodels by themselves often suffer from hallucinations, where the AI generates information that seems\nplausible but is wholly fabricated or factually inaccurate. This can be based on training data limitations,\nmisunderstanding the context of the prompt, or a model architecture that prioritizes coherence over\naccuracy (Turing,2023). RAG was created to assist with this problem (Lewis et al.,2021). This process is\noften effective for small datasets or niche areas of information that an organization may use. For example,\nalthough an LLM might have some general knowledge in medical technology, the diverse amount of data\nit’s trained on would not allow it to produce accurate answers about specific medical technology or\ncapture relevant advancements. RAG systems“effectively reduces the problem of generating factually\nincorrect content” (Gao et al.,2024). This is because an organization can tailor an LLM by including\nrelevant files as a basis for the model to answer questions. As the accessible knowledge base grows,\nefficiently retrieving relevant information from documents becomes crucial (Gao et al.,2024). This\ntechnology is essential in the academic landscape, where educational topics are often specific to key areas\nor narrow domains that generalized LLMs typically lack without RAG capabilities.\nAs AI becomes increasingly integrated into academia, from content generation to assessment design,\nthere is a growing need for clear regulatory frameworks. Policymakers and educational institutions must\nwork together to promote transparency, fairness, and accountability in AI-assisted assessments. These\npolicies should address concerns such as data privacy, model bias, and the appropriate boundaries for AI\nuse in student evaluation and curriculum development. Without such guidelines, the rapid adoption of AI\ntools risks undermining academic standards and introducing fairness and equity challenges that this study\nseeks to anticipate and address through a focused evaluation of RAG-enhanced GPT models.\n1.2. Objectives and scope\nThe objective of this paper is to study the variations within different forms of GPT by creating exam\nquestions from a specified resource and measuring each model’s accuracy in answering those questions. The\nwork creates a diverse set of exam questions using OpenAI’s GPT, focusing on RAG to enhance accuracy\nand reliability, and explores the methods it would take to enable AI models to create and answer the\nquestions. Furthermore, the paper will analyze the differences between GPT versions to determine which\nproduces higher-quality assessments and provides the most accurate answers. This will be done using a\nrange of text-similarity and scoring metrics discussed later in the Methodology section. The primary\nprompts used can be found in theSupplementary Appendicesto allow replication and validation of this\nwork, which is crucial for developing policies that ensure transparency and fairness when using AI tools.\nA limitation of this study is that while there are thousands of LLMs, only two variations of OpenAI’s\nGPT models will be utilized. These models were selected as ChatGPT remains one of the most widely\naccessible AI tools for educators. A second boundary is that this study evaluates AI performance using a\nsingle undergraduate textbook, providing a controlled test case while acknowledging that broader\napplications may require additional resources to ensure assessment validity across diverse educational\nsettings. Although limited in scope, the controlled setup enables clear attribution of performance\ndifferences to RAG and model complexity, offering insight into durable design choices for AI-assisted\nassessment workflows. These findings can inform procurement, oversight, and fairness considerations\nacross diverse institutional settings.\n2. Background\nThere are multiple types of LLMs, including cloud-based and locally deployed models, each with\ndiffering advantages and disadvantages. LLMs have various applications and capabilities that can be\nenhanced with methods like RAG. As these models are increasingly integrated into education and\nassessment, policymakers must evaluate their implications for academic integrity, data security, and\nregulatory compliance. LLMs have rapidly grown in use and availability in recent years, and with this\nrecent growth, there are expanding concerns for ethical considerations and the need for governance\nData & Policy e57-3\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\nframeworks within the academic domain. This study identifies which design features, such as retrieval\ngrounding or model complexity, most impact performance and trustworthiness in educational settings.\n2.1. Infrastructure and applications of large language models\nLLMs are typically either locally deployed or cloud-based. Cloud LLMs such as ChatGPT are accessed\nvia the internet through a web interface or an application programming interface (API), as is used in this\nwork. These cloud-based models free users from the responsibility of managing and updating the required\ninfrastructure, which can be quite extensive. They also reduce the initial costs related to purchasing\nhardware and software, allowing users to access the model as needed (Dilmegani,2024). However, with\nthis reduced burden on the user there are increased security risks due to the nature of accessing a cloud\nenvironment. This includes the opportunity for data breaches and illegitimate access to data (Dilmegani,\n2024). Cloud LLMs are easily scalable and offer advantages for research institutions requiring extensive\nor high-performance computing resources such as multiple instances of high-end GPUs and large\namounts of data storage (Awan,2023). If the user does not have the necessary hardware, the time to\ntrain or use a model can be immense without the help from cloud computing. While cloud LLMs can have\nlower startup costs, they suffer from higher total costs reflected in subscription fees or pay-as-you-go\npayment plans (Dilmegani,2024).\nLocal LLMs, such as BERT or T5, are run on individual devices or servers without needing a\nconnection to the internet or cloud services. This provides users with greater control over the LLM and\nits environment and reduces data security and privacy concerns. However, this requires greater familiarity\nwith the technology and maintenance support (Dilmegani,2024). Since the information input into the\nLLM is stored locally, rather than online, it is inherently more secure and difficult to access without\nphysical access to the computer. However, authorized access and usage policies must still be considered\nduring deployment. Depending on the user’s needs, the cost of hardware can be substantial. While this\nstudy did not evaluate locally deployed models, understanding their characteristics is useful for context-\nualizing policy considerations across different deployment environments.\nLLMs can be applied to a wide range of tasks, many of which are depicted inFigure 1. The x-axis\nrepresents different applications, including text generation and text classification, and the y-axis repre-\nsents the number of AI models available for each category. The figure shows the number of artifacts (such\nas models) hosted on huggingface.com in each category as of March 2024, totaling 583,326. Hugging-\nface.com started with about 30,000 models in 2022, and in March 2024 alone, 50,000 artifacts were\nadded, including 4000 text classification models. There has been a linear rise in the number of uploads per\nmonth, too many to keep track of or list individually. Currently, there are 54,131 text classification models,\nreflecting the rapid expansion of AI tools and the increasing demand for governance and policy\nframeworks to manage their integration into education and research.\nThe left side ofFigure 1highlights the most popular LLM applications, including:\n Text Generation: Producing coherent and contextually relevant text, such as a creative short story or\nan AI-generated news article from a brief headline.\n Text Classification: Sorting text into predefined categories based on its content, such as organizing\ncustomer reviews into positive, negative, or neutral sentiment categories.\n Reinforcement Learning: Fine-tuning AI agents to improve conversational dialogue quality through\nuser interactions.\n Text-to-Text Generation: Transforming input text into a different format or style, such as converting\na formal piece of text into a more casual version or translating text from one language to another.\n2.2. AI-assisted exam generation and scoring\nRAG is a powerful method that enhances the performance of LLMs by incorporating external knowledge\nsources into the text generation process. This technique involves retrieving relevant information from a\ne57-4 Erick Tyndall et al.\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\nlarge corpus of documents and using this information to generate more accurate and contextually\nappropriate responses. RAG has shown significant promise in various applications, including automated\nexam generation and scoring, where the accuracy, relevance, fairness, and reliability of AI-generated\ncontent are critical for educational policy considerations.\nKlang et al. (2023) explored the use of GPT-4 in creating medical examinations, relying solely on the\nmodel’s pretrained internal knowledge without the use of retrieval-augmented methods. While GPT-4\ndemonstrated the ability to rapidly generate a large volume of multiple-choice questions, the absence of\nexternal verification contributed to several factual, contextual, and methodological errors. For instance,\nthe model confused treatment protocols for different conditions and used outdated medical terminology.\nAdditionally, the study noted redundancy in question generation and a small number of questions that\nincluded flawed logic or oversimplified clinical reasoning. These findings underscore the limitations of\nrelying solely on an LLM’s internal knowledge base for high-stakes educational content. To ensure\nalignment with accreditation policies and institutional assessment standards, quality assurance mechan-\nisms involving subject-matter experts and educational authorities remain essential.\nBuilding on these concerns, Guinet et al. (2024) demonstrate an automated approach to evaluate\nRAG’s effectiveness in generating task-specific exams. Their method involves creating synthetic exams\ncomposed of multiple-choice questions based on a designated corpus. This approach utilizes Item\nResponse Theory to assess the quality of the generated exams, ensuring that the questions are informative\nand accurately reflect the model’s understanding of the content.\nIn the context of scoring, RAG’s ability to retrieve relevant information plays a pivotal role in\nenhancing the accuracy of generated answers. The study by Guinet et al. (2024) emphasizes the\nFigure 1.Number of artifacts (models & datasets) per application, based on huggingface.com tags.\nData & Policy e57-5\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\nimportance of selecting the appropriate retrieval algorithms, noting that the choice of retrieval mechanism\ncan significantly impact the model’s performance. Their findings suggest that optimizing retrieval\nstrategies often yields more substantial improvements than merely increasing the model size, underscor-\ning the need for standardized guidelines on AI-driven grading transparency and reliability.\nThese studies demonstrate both the promise and the limitations of AI-assisted exam generation and\nscoring. While techniques like RAG can enhance accuracy through targeted retrieval, even advanced\nmodels like GPT-4 require expert oversight to ensure content quality and alignment with academic\nstandards. As AI becomes more integrated into educational assessment, establishing clear protocols for\nvalidation, review, and bias mitigation will be essential to maintain fairness and credibility.\n2.3. Ethical considerations in using AI for exam purposes\nAlthough this paper has thus far explained the benefits of AI use in exam creation, it is important to\nhighlight ethical and regulatory considerations as well. While there is a paucity of research on the ethical\nuse of AI-generated exams, there is relevant research on the general topic of AI use in education. One\nreport focused on K-12 education identified the following ethical concerns: privacy, surveillance,\nautonomy, bias, and discrimination (Akgun and Greenhow,2021). While not all these issues will affect\nthe ethics of simply creating exams, some are highly relevant to AI-assisted assessment policies. Bias and\ndiscrimination are possible problems that need to be addressed, particularly as educators and institutions\nshould be aware that“automated assessment algorithms have the potential to reconstruct unfair and\ninconsistent results” (Akgun and Greenhow,2021). When creating these exams, especially when the topic\nis socio-cultural in nature, it is important to ensure that questions are properly and appropriately\nformulated and structured in a way that mitigates bias and adheres to academic fairness standards. This\nis especially pertinent in the context of this study, which used a textbook on Pacific Studies, a subject area\nwith inherent cultural dimensions that require sensitive treatment.\nFurthermore, autonomy could be a challenge when AI-generated exams become standard practice in\neducational environments. Professors may become overly reliant on AI, continuing to use it even if it is\nnot in the best interest of the students. Conversely, it could even become an institutional norm at\nuniversities to mitigate potential discrepancies and inconsistencies among professors. While this might\nseem like a good way to equalize issues and ensure fairness, it carries the risk of“[jeopardizing] students\nand teachers’ autonomy” (Akgun and Greenhow,2021). These concerns highlight the need for clear\ninstitutional policies that balance AI integration with human oversight. Regulatory frameworks should\nensure that AI remains a tool for academic enhancement rather than a substitute for pedagogical judgment.\nUltimately, while these concerns may not immediately materialize, they warrant consideration as AI-tools\ncontinue to develop and become ubiquitous.\n2.4. Policy considerations in using AI for exam purposes\nThe integration of AI in educational assessments has introduced both opportunities and regulatory\nchallenges, prompting universities, accreditation bodies, and policymakers to consider new frameworks\nfor AI-assisted exam generation. While AI-driven tools such as RAG can enhance efficiency, consistency,\nand accessibility in test creation, they also raise concerns about fairness, transparency, and academic\nintegrity. Given the rapid deployment of AI in educational settings, it is crucial to examine existing\npolicies and governance frameworks that guide the ethical and responsible use of AI-driven assessments.\nThe retrieval-augmented approach tested in this study aligns closely with these regulatory goals,\nparticularly those focused on transparency, human oversight, and data traceability (Gao et al.,2024;\nHolistic AI,2024).\nOne of the most significant regulatory developments in AI governance is the European Union’s AI Act,\nwhich classifies AI systems based on risk levels. Under this framework, universities operating within the\nEU are considered high-risk AI system providers, meaning they must comply with stringent requirements\nto ensure transparency, accountability, and bias mitigation in AI-driven processes (Digital Education\nCouncil, 2024). Universities using AI-powered assessment tools, such as automated exam generation and\ne57-6 Erick Tyndall et al.\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\nscoring, must establish risk management systems, conduct data governance to ensure fairness and\ncompleteness, and maintain technical documentation for compliance verification. Additionally, the Act\nmandates record-keeping of system modifications, human oversight mechanisms, and quality manage-\nment systems to ensure the reliability of AI applications in education. These requirements, while\npromoting responsible AI use, may pose administrative and financial burdens on universities, particularly\nsmaller institutions with limited resources to maintain compliance. This highlights the importance of\nscalable and cost-effective solutions, such as pairing lower-cost models with retrieval techniques, to\nensure compliance does not exacerbate institutional disparities.\nThe EU AI Act also has broader implications beyond Europe, particularly for universities engaged in\nmultinational research collaborations or institutions replicating the work of foreign academic bodies. As\nAI-driven assessments become standardized in global education, universities that collaborate on research,\nshare AI-generated exam datasets, or replicate findings from foreign institutions may face regulatory\nchallenges when aligning with different AI governance frameworks. Institutions outside the EU must also\nremain aware of these regulations, as international AI policies may shape future national regulations and\ninfluence best practices for AI adoption in education. The growing regulatory landscape suggests that\nuniversities engaging in cross-border AI research and assessments must prioritize compliance with\nevolving AI laws to avoid potential legal and ethical pitfalls.\nIn the United States, AI governance is influenced by the National Institute of Standards and\nTechnology (NIST) AI Risk Management Framework (AI RMF), which was established under the\nNational Artificial Intelligence Initiative Act of 2020 (Holistic AI,2024). Unlike the EU AI Act, which\nenforces strict compliance for high-risk AI applications, the NIST AI RMF is a voluntary framework,\ndesigned to help organizations assess and manage AI risks while promoting responsible development and\ndeployment. The framework outlines four core functions, Govern, Map, Measure, and Manage, that guide\ninstitutions in ensuring that AI systems are transparent, trustworthy, and aligned with ethical consider-\nations.\nWhile the NIST AI RMF does not impose mandatory regulations, its voluntary guidelines are\nincreasingly being adopted by universities and research institutions as a best-practice model for AI\ngovernance. This is particularly relevant for AI-generated assessments, as the framework encourages bias\nmitigation strategies, risk assessments, and human oversight mechanisms to enhance the reliability and\nfairness of AI-driven exams. Moreover, the NIST framework emphasizes alignment with international AI\nstandards, which is crucial for U.S. universities engaging in multinational research collaborations or\nimplementing AI-based assessment methods influenced by foreign institutions.\nAs AI-generated assessments become more prevalent in higher education, policymakers and institu-\ntions must consider several key policy areas. These include bias detection and fairness, ensuring\naccountability and transparency in AI-driven assessments, addressing academic integrity concerns, and\npromoting equitable access to AI tools. By establishing comprehensive policies, institutions can ensure\nthat AI remains a valuable tool for educators while upholding the principles of fairness, accuracy, and\nstudent autonomy.\n3. Method\nThis experiment’s purpose was to accurately generate and answer exam questions from an undergraduate\ntextbook using GPT models. The project was implemented in a Python Jupyter Notebook, beginning with\nthe import of essential libraries. These libraries were used for various purposes such as interacting with the\nOpenAI API, handling data in JSON and CSV formats, performing regular expression operations, and\nconducting evaluations, such as precision scoring, word embeddings similarity scoring, and cosine\nsimilarity scoring. The versions of the main libraries and frameworks are shown inTable 1.\nThe hardware and software specifications used in this work are located inTable 2.\nWithin the OpenAI API, RAG was implemented by creating two“Assistants” that had access to the\ntextbook, one utilized ChatGPT 3.5-Turbo while the other used ChatGPT 4-Turbo. These Assistants\nrelied on OpenAI’s file_search tool to retrieve content from a vectorized index of the textbook, enabling\nData & Policy e57-7\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\ngrounded responses based solely on the uploaded material. Each Assistant was tasked with generating\nthree types of examination questions: true-false, multiple choice, and short answer. For each question, the\nAssistant also produced a corresponding answer key and included supporting excerpts from the textbook.\nAfter the exams were created, both Assistants were prompted to take the exams as if they were students,\nwith and without access to the same source material. Their short-answer responses were then evaluated\nusing three text similarity metrics. The design ensured that all answers from RAG-enabled models could\nbe traced to specific source excerpts, supporting traceability and citation auditability in alignment with\nemerging policy frameworks, and is documented in an open-access Zenodo repository (Tyndall et al.,\n2025).\n3.1. Textbook selection\nThe textbook used in this study had to meet specific criteria to support controlled, reproducible testing. All\nquestions needed to be based solely on textual content, allowing the models to generate and answer\nquestions without interpreting illustrations. The text also had to reflect undergraduate-level academic\nquality to align with higher education use cases. While the selected textbook met these conditions, the\napproach is generalizable, and any similar resource could be used to replicate or extend this study.\nThe open-source textbook used in this work isIntroduction to Pacific Studies, which is Volume 6 of the\nTeaching Oceaniaseries of books written by the University of Hawaiʻi Center for Pacific Island Studies\n(Mawyer et al.,2020). The book is organized into seven sections, each addressing key political and\ncultural issues relevant to Oceania and its peoples.\nThe models were instructed to follow specific criteria when generating exam questions:\n Create a ten-question quiz.\n Compose the quiz of true-false, multiple choice, and short-answer questions.\n Ensure questions have answers easily found within the book; creatively generated questions were\nnot acceptable.\n Require multiple-sentence responses for short-answer questions.\n Do not ask questions that require reading charts, graphs, or other illustrations.\nTable 1. Key frameworks, libraries, modules\nPython 3.10.12\nOpenAI 1.30.1\nJSON 2.0.9\nCSV 1.0\nRegEx 2.21\nEvaluate 0.4.2\nROUGE 0.1.2\nspaCy 3.7.4\nscikit-learn 1.2.2\nTable 2. Hardware/Software\nRAM 12.67 GB\nCPU Dual-core Intel(R) Xeon(R) CPU @ 2.20GHz\nOS Ubuntu 22.04.3 LTS\nKernel Linux 6.1.85+ x86_64\ne57-8 Erick Tyndall et al.\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\n3.2. Metrics\nFor the true-false and multiple-choice questions, the answers were scored using accuracy. Short-answer\nquestions were scored using the three metrics shown inTable 3, to ensure a comprehensive evaluation.\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE): A token similarity metric that“meas-\nures the similarity reference text and generated text by focusing on recall” (Jain, 2023). ROUGE has been\n“employed to assess the quality and coherence of the generated text” (Jain, 2023). ROUGE-1 was\nimplemented via the rouge.compute() method, using the rouge-score library, to measure the overlap of\nindividual words between the generated answers and the reference answers (Hugging Face,n.d).\nCosine Similarity: This metric treats documents as vectors, with each unique word“as a dimension”\n(Supe, 2023). After converting two different documents into vectors, it measures the angle between their\nvectors and take the cosine of that angle (Supe,2023). This metric is widely used due to its flexibility in\ntext analytics. Because cosine similarity only measures direction and not magnitude, document length\ndoes not affect the calculation, allowing for proper comparison of documents of different lengths. Cosine\nsimilarity scores were computed using the scikit-learn library, employing CountVectorizer() and cosi-\nne_similarity() methods.\nWord embeddings: These are multi-dimensional representations of words used in language models and\ncan measure the similarity of two objects (Honnibal et al.,2020). Using algorithms like word2vec, these\nvectors are compared to determine the semantic similarity of different documents. Word embeddings\nsimilarity scores were calculated using the spaCy library with the en_core_web_md pipeline.\n3.3. Generative AI\nTo utilize an LLM to create an exam, the first step involved setting up two assistants. During initial runs,\nthis was achieved by creating new OpenAI assistants using the beta.assistants.create() method. For\nsubsequent runs, the same assistants were accessed using the beta.assistants.retrieve() method. Each\nassistant was then instantiated with specific key parameters to define functionality. The instruction given\nto each assistant was:“You are an expert Anthropologist in the area of Pacific Studies. Use uploaded files\nonly to answer questions about anthropology.” This directive defined the scope (anthropology) and\nbehavior (expert Anthropologist and reliance on vector-stored documents) expected from the assistant\n(How Assistants work, n.d.).\nEach assistant was created with the model set as either GPT-3.5 Turbo or GPT-4 Turbo. The tools\nparameter was set to [{“type”: “file_search”}], enabling the assistant to perform specialized tasks.\nSpecifically, the file search tool enables the assistant to perform text-based searches on documents\nuploaded to an attached vector store (File Search,n.d.). For data preparation, the textbook was uploaded as\na text file to a vector store. This was accomplished by beta.vector_stores.file_batches.upload_and_poll()\nand beta.assistants.update() methods, ensuring the assistant had access to the necessary academic content.\nThe next step was the core of the project: generating exam content. Each assistant was prompted to\ncreate sections of an examination through three independent prompts, which are shown in\nSupplementary Appendix A. First, they generated a specified number of true-false questions, along with\nTable 3. Metrics and brief description\nMetric Library Description\nROUGE–1 rouge-score Automatic summarization and machine translation software (Lin,\n2004)\nCosine Similarity scikit-learn Used in text analytics to compare documents and determine if they are\nsimilar and how much (Supe,2023)\nWord Embeddings spaCy Similarity through comparing multi-dimensional representations of a\nword (Honnibal et al.,2020)\nData & Policy e57-9\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\ntheir answers and excerpts. Next, they produced multiple-choice questions, also with answers and\nexcerpts. Finally, they created short-answer questions, complete with answers and excerpts. This\nstructured approach attempted to enforce consistency across the responses and address the brittleness\nof regular expression parsing when each model produced unpredictable outputs. By organizing the\nprompts in this manner, each assistant produced well-defined and distinct sections of the examination,\nfacilitating easier validation and processing. Sample exam questions, answers, and textbook excerpts are\nlocated inSupplementary Appendix B.\nAfter creating each portion of the examination, the responses were processed using a ChatGPT model\nand regular expressions to capture the exam information in a Python list, which simplified the storage and\nsubsequent scoring of results. This resulted in two complete exams, one produced by a GPT 3.5-Turbo\nAssistant, and one produced by a GPT 4-Turbo Assistant.\nOnce processed, both exams were then fed back to the original assistant and the competing assistant\nwith a new prompt, instructing them to answer the questions using only the uploaded textbook. Each\nassistant’s responses were validated through a simple text parsing function to ensure all questions were\nanswered. These new answers were then processed similarly by a ChatGPT model and regular expres-\nsions to capture the responses for storage and scoring. The prompts used to answer the exams are shown in\nSupplementary Appendix C.\nThis architecture mirrors policy recommendations that emphasize source-grounded outputs, citation\ntransparency, and model accountability, as seen in the EU AI Act and NIST AI RMF. Because the\ngeneration and answer formats were structured with modular prompts, this workflow can be adapted for\nother subjects, levels, or textbooks with minimal modification. It provides a reproducible template for\nregulated educational use that prioritizes auditability and fairness.\n4. Results and analysis\nIn evaluating the performance of GPT-3.5 Turbo and GPT-4 Turbo, the study utilized a combination of\nquantitative metrics, namely the number of correct true-false and multiple-choice answers, along with the\nsimilarity scores for short-answer questions. This choice of metrics was discussed in the methodology\nsection, where these specific measures were identified due to their ability to assess not only the factual\ncorrectness but also the linguistic and semantic quality of the responses generated by the models. The\nresults of the performance evaluation for one exam are presented inTable 4, comparing the performance of\nthe models across exams generated by GPT-3.5 Turbo Assistant and GPT-4 Turbo Assistant.\nEach model was subjected to identical test conditions with and without access to source texts, designed\nto test what retrieval-augmented generation capabilities provide when answering domain-specific ques-\ntions. This approach evaluated the robustness and adaptability of each model under controlled academic\nconditions. Notably, when textbook access was restricted, the performance gap between GPT-3.5 Turbo\nand GPT-4 Turbo narrowed. This suggests that while GPT-4 Turbo has stronger retrieval capabilities, both\nmodels are capable of generating logically consistent answers using their embedded knowledge alone.\n4.1. Detailed results\nAll models showed consistently high rankings in true-false questions. GPT-4 Turbo Assistant and\nGPT-3.5 Turbo Assistant typically received the highest rankings, indicating their effectiveness in\naccurately interpreting and responding to binary questions based on the text.\nIn multiple-choice rankings the GPT-4 Turbo Assistant consistently outperformed other models,\nsuggesting superior capabilities in understanding and selecting the correct answers from multiple options.\nThis is indicative of its robust comprehension and retrieval abilities.\nThe ROUGE-1 rankings reflect the models’ ability to produce text closely matching the reference\nmaterial. Here, the GPT-4 Turbo Assistant often ranked higher, particularly in generating answers that\nalign well with the expected responses, demonstrating a strong grasp of content accuracy and relevance.\nThe rankings in cosine similarity scores were varied, with GPT-4 Turbo generally achieving\nbetter results. Higher rankings in this metric indicate a closer match to the textual style and content\ne57-10 Erick Tyndall et al.\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\nof the reference material underlining GPT-4 Turbo’s adeptness at maintaining textual integrity and\ncontext.\nThe embeddings scores provide a perspective on semantic understanding. GPT-4 Turbo frequently\nreceived top rankings, illustrating its superior ability to grasp and replicate the underlying semantic\nproperties of the original text. This suggests a deep and nuanced understanding of the material, which is\ncrucial for generating contextually accurate responses.\nTable 4. Model performance for one exam\nModel Scoring\nGPT-3.5 Turbo\nassistant exam\nGPT-4 Turbo\nassistant exam\nGPT–3.5 Turbo Assistant with RAG True/False 1.0000 1.0000\nMult Choice 0.5000 1.0000\nROUGE–1 0.5103 0.4752 0.3368 0.3600\nCosine 0.6872 0.8077 0.5220 0.5231\nEmbeddings 0.9789 0.9896 0.9781 0.9797\nGPT–4 Turbo Assistant with RAG True/False 1.0000 1.0000\nMult Choice 0.7500 1.0000\nROUGE–1 0.5046 0.4250 0.4478 0.4396\nCosine 0.6786 0.7070 0.6437 0.6777\nEmbeddings 0.9874 0.9828 0.9808 0.9857\nChatGPT–3.5 Turbo without RAG True/False 1.0000 0.7500\nMult Choice 0.7500 1.0000\nROUGE–1 0.2609 0.4228 0.2927 0.3770\nCosine 0.4949 0.7467 0.5271 0.5930\nEmbeddings 0.9661 0.9755 0.9559 0.9803\nChatGPT–4 Turbo without RAG True/False 1.0000 1.0000\nMult Choice 1.0000 1.0000\nROUGE–1 0.2632 0.4031 0.4000 0.4088\nCosine 0.4800 0.7207 0.5821 0.6254\nEmbeddings 0.9716 0.9831 0.9686 0.9803\nTable 5. Model rankings per exam\nModel Exam Ranking\nGPT–3.5 Turbo Assistant with RAG GPT –3.5 Turbo 1\nGPT–4 Turbo Assistant with RAG GPT –4 Turbo 2\nGPT–4 Turbo Assistant with RAG GPT –3.5 Turbo 3\nChatGPT–4 Turbo without RAG GPT –4 Turbo 4\nChatGPT–4 Turbo without RAG GPT –3.5 Turbo 5\nGPT–3.5 Turbo Assistant with RAG GPT –4 Turbo 6\nChatGPT–3.5 Turbo without RAG GPT –4 Turbo 7\nChatGPT–3.5 Turbo without RAG GPT –3.5 Turbo 8\nData & Policy e57-11\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\n4.2. Ranked performance\nThe ranked performance of the models across various metrics is shown inTables 5and 6. These tables\nprovide insights into how each model performs relative to others in specific evaluation criteria.\nTable 5provides a breakdown of performance rankings per exam, where a lower rank indicates better\nperformance. While the base GPT-4 Turbo model generally outperformed its GPT-3.5 counterparts, the\nGPT-3.5 Turbo Assistant with RAG achieved the top ranking in one exam. This suggests that RAG can\nsignificantly enhance the performance of a less powerful model, allowing it to outperform more advanced\nmodels under certain conditions.\nWhen examining the ranked performance inTable 6the GPT-4 Turbo Assistant with RAG ranked\nhighest in overall performance. Notably, both assistant models that had access to the textbook through\nRAG (GPT-4 Turbo Assistant and GPT-3.5 Turbo Assistant) outperformed their non-RAG counterparts.\nThis highlights the substantial performance boost provided by retrieval-augmented generation, even\nwhen applied to a less powerful model. The consistent strength of the GPT-4 Turbo Assistant across all\nevaluation metrics demonstrates not only the robustness of the base model but also the importance of\nintegrating external knowledge sources in exam generation and answering tasks.\nThe ranked performance, as detailed inTables 5 and 6 illustrates a clear differentiation in the\ncapabilities of GPT-3.5 Turbo and GPT-4 Turbo. GPT-4 Turbo consistently outperformed GPT-3.5 Turbo\nparticularly in settings where text access was unrestricted suggesting a superior ability to leverage\navailable resources for answer generation. This is indicative of GPT-4 Turbo’s enhanced retrieval\nmechanisms and updated training algorithms which seem to allow it to better understand and utilize\ncontext from the provided source material.\nFurthermore, in multiple-choice settings where nuanced comprehension of the question context and\nsubtleties in phrasing can significantly influence the outcome GPT-4 Turbo demonstrated higher\naccuracy. This suggests advancements in its language processing capabilities likely attributable to its\nlarger training dataset and more sophisticated neural network architecture compared to GPT-3.5 Turbo\n(Emmanuel 2023).\nThe comparative analysis highlights the implications of evolving model architectures and training\nenvironments in the development of AI applications for academic testing. The observed performance\ndifferences, particularly the implementation of RAG, demonstrate how model enhancements can directly\ninfluence assessment quality. These findings suggest that future enhancements in model training and\ndevelopment could further exploit these capabilities, potentially leading to more sophisticated AI tools for\neducational assessment.\n4.3. Analysis of results\nThe results indicate that GPT-4 Turbo Assistant consistently outperforms other models, particularly in\nterms of ROUGE-1, cosine similarity, and word embeddings similarity scores. This suggests that GPT-4 is\nparticularly effective at both generating high-quality exam content and providing accurate answers. The\nstrength of GPT-4 in handling true-false, multiple-choice, and short-answer questions also highlights its\nutility in structured academic testing environments.\nTable 6. Model overall rankings\nModel Ranking\nGPT–4 Turbo Assistant with RAG 1\nGPT–3.5 Turbo Assistant with RAG 2\nChatGPT–4 Turbo without RAG 3\nChatGPT–3.5 Turbo without RAG 4\ne57-12 Erick Tyndall et al.\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\nIn the case of true-false and multiple-choice questions, all models performed well, often achieving\nperfect scores. However, the real differentiation among the models was observed in the short-answer\nquestions as evidenced by the ROUGE-1, cosine similarity, and word embeddings similarity scores.\nThe GPT-4 Turbo Assistant showed a balanced performance across all metrics, making it the most\nreliable model for generating high-quality exam content and providing accurate answers. The GPT-3.5\nTurbo Assistant also performed well but had slightly lower scores in some of the more nuanced metrics,\nlike cosine similarity and word embeddings similarity.\n5. Discussion and conclusions\nThe GPT-4 Turbo models performed the best compared to their GPT-3.5 Turbo counterparts, which was\nanticipated as their complexity (number of parameters) is an order of magnitude greater. What was less\nexpected was that, while GPT-3.5 Turbo did not outperform GPT-4 Turbo, it demonstrated a level of\nperformance that was sufficiently close in several metrics. GPT-3.5 Turbo performed particularly well\nwhen analyzing summarization metrics. Even though it was not better, GPT-3.5 Turbo performed\nrelatively close enough to GPT-4 Turbo to suggest that individuals without the resources to purchase a\nsubscription could use an equivalent process with the model effectively, provided there is additional\noversight. Even with true-false and multiple-choice questions, the GPT-3.5 Turbo model often fell only\none question behind. More testing is needed to determine exactly how far the GPT-3.5 Turbo model lags\nbehind the GPT-4 Turbo model in this context. However, these findings suggest that AI accessibility\nremains a key consideration for educational policymakers, since reliance on premium models may\nunintentionally increase resource disparities among students and institutions.\nWhat seemed far more important than which specific GPT model was being used was whether the\nmodel was utilized as an assistant, which allows it to leverage RAG and the textbook as a source. As\nshown in the analysis, the GPT-3.5 Turbo Assistant performed better overall than the ChatGPT-4 Turbo\nmodel, which did not have access to the college textbook. While this study did not directly test for\nhallucinations, citations were manually verified to ensure that model responses were grounded in the\ntextbook and accurately reflected its content. In doing so, it was found that RAG-supported models\nconsistently produced valid and contextually appropriate citations, whereas non-RAG models occasion-\nally generated responses that lacked clear textual grounding. This suggests a higher likelihood of\nhallucinations or outdated information when models rely solely on pre-trained knowledge. By contrast,\nmodels utilizing RAG consistently retrieved accurate and relevant information to generate their\nresponses. These results demonstrate that RAG is an effective way to create tests and answer said tests\nusing a specified scholarly source.\nThese findings can be applied to academic professionals looking for ways to create mass variations of\nexams that are both fair and accurate. Further research in this field can include expanding the number of\nLLMs tested, evaluating different metrics, and exploring textbooks that cover other topic areas. Although\nthis study focused on a narrow setup with two models and one textbook, the results highlight workflow\ndecisions that remain relevant across many platforms. Retrieval grounding and citation transparency, in\nparticular, offer practical solutions to issues of accuracy, cost, and fairness. Additionally, policymakers\nand accreditation bodies should consider establishing guidelines for the responsible use of AI-assisted\nexam generation to maintain transparency, academic integrity, and equitable access to AI-driven tools\nacross diverse educational institutions.\n5.1. Analytical framework\nThe rapid turnover of foundation models means that any label, such as OpenAI’s “GPT-4 Turbo” will date\nquickly, whereas certain workflow choices remain stable across model generations. Two such choices\nappear to determine most of the educational risk surface:\nData & Policy e57-13\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\n Knowledge-grounding strategy. A system can rely on internal weights (“closed-book”) or retrieve\npassages from an external, auditable corpus (“retrieval-grounded”).\n Oversight locus. Assessment can be scored entirely automatically or require a human decision point\nat one or more stages (“human-in-the-loop”).\nCrossing these axes yields a 2 by 2 grid that captures the design space for AI-assisted exams (seeTable 7).\nThe closed-book with automatic quadrant corresponds to public chatbot deployments that generate items\nand grades without citations. Retrieval-grounded with automatic systems resembles the pipeline evalu-\nated in this study, where the model must attach a source snippet to every answer. The quadrants that\nintroduce human review add formal guardrails. In the closed-book with human-in-the-loop scenario,\nfaculty must validate questions that originate only from the model’s internal knowledge before they are\npresented to students. Retrieval-grounded with human-in-the-loop pipelines require educators to approve\nboth the retrieved material and the generated answer at every step.\nThis framework helps decouple policy decisions from the pace of vendor-specific advancements.\nRegulators and institutions can ask which quadrant a proposed tool occupies and whether its safeguards\nmatch the residual risks. For example, requiring retrieval source traceability through cited references\naddresses many accuracy concerns even before considering the base model. Likewise, mandating faculty\nsign-off moves a system into the higher-trust human-in-the-loop category regardless of the architecture\nimplemented. By organizing oversight strategies around this grid, institutions can adopt a consistent\npolicy framework that scales across tools and academic contexts.\n5.2. Framework implications\nMapping exam-generation workflows onto the two design choices yields four archetypes:\nEmpirical results occupy the bottom-right cell that combines retrieval grounding with human-in-the-\nloop oversight. Relative performance advantages, including higher ROUGE-1 and BLEU scores and a\nlower hallucination rate, suggest that retrieval and oversight reinforce each other. Their combination\nimproves both accuracy and traceability, supporting workflows that are better aligned with policy goals\nrelated to quality assurance and transparency.\nQuality control of the knowledge base remains essential. Accuracy improves only when sources are\nversion-controlled and subject to peer review. A viable governance protocol should implement semantic\nversioning for each corpus release, maintain source traceability metadata for every paragraph, and require\nperiodic faculty audits focused on coverage gaps. These steps enable traceability, reproducibility, and\ninstitutional accountability.\nData privacy considerations also arise, since textbook excerpts may be under copyright or\nprotected by privacy regulations such as Family Educational Rights and Privacy Act (FERPA) and\nGeneral Data Protection Regulation (GDPR) (Azam et al.,2024; Kelso et al.,2024; Cooper et al.,\n2025). Retrieval APIs should enforce in-place querying so that student prompts are processed within a\nsecure institutional environment and responses are streamed without being stored externally (Zhao,\nTable 7. AI-assisted exam workflow archetypes\nClosed-book Retrieval-grounded\nAutomatic Base model writes and grades exams without\nexternal material.\nRAG model extracts textbook\npassages without human review.\nHuman-in-the-loop Base model to writes exams; professor\nmanually edits questions and grades\nresponses.\nRAG model drafts and grades exams;\nprofessor reviews flagged answers.\ne57-14 Erick Tyndall et al.\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\n2024; Mithun et al.,2025). This safeguards both legal compliance and student trust (Shrestha et al.,\n2024).\nEquitable access depends on cost as well as accuracy. According to OpenAI’s published pricing,\nGPT-3.5 Turbo is substantially less expensive per token than GPT-4 Turbo (Pricing, n.d.). When paired\nwith retrieval-augmented generation, GPT-3.5 delivered performance approaching that of GPT-4 across\nseveral metrics in this study. This makes it a viable option for institutions operating under budget\nconstraints, especially if procurement guidelines emphasize retrieval transparency and traceability over\nuse of the most advanced proprietary model.\nGrounding also reduces hallucinations tied to contextual bias by ensuring that all answers are traceable\nto a citable excerpt. Manual citation auditing confirmed this benefit. These safeguards promote fairness\nand consistency, especially in courses involving sensitive or contested subject matter. The remaining three\ncells in the matrix represent opportunities for further research and policy development rather than\nregulatory gaps. By centering workflow design choices rather than model branding, institutions can\napply this framework regardless of which LLM vendor or architecture is implemented.\n5.3. Implementation considerations\nThe adoption of retrieval-grounded language models for educational assessment raises several oper-\national considerations that institutions must address to ensure responsible and sustainable deployment\n(Lewis et al.,2021; Gan et al.,2025; Ni et al.,2025).\nFirst, knowledge-base curation and quality control must be formalized. Each retrievable passage\nshould include source traceability metadata, version information aligned with the associated course\nmaterials, and a defined review schedule approved by instructional staff (Gan et al.,2025; Ni et al.,2025).\nInstitutions should consider implementing automated content audits to flag obsolete information or\ninconsistencies, allowing faculty to revise the corpus before assessments are administered (Lewis\net al.,2021). These practices are essential to maintain alignment with current academic standards and\nto ensure transparent and traceable decision-making.\nSecond, data privacy and copyright compliance are central to system design. Legal constraints such as\nFERPA and GDPR restrict how educational data and copyrighted content may be used. A layered access\nmodel can help address these concerns (Koga et al.,2025; Ni et al.,2025). Students should only receive\nthe specific excerpt used to generate or justify an answer, while faculty maintain access to the full corpus\nfor auditing and accreditation. Retrieval tools should be configured to support in-place querying so that\nprompts and document content are processed within secure institutional environments rather than external\nservers (Koga et al.,2025).\nThird, equity and resource allocation should guide infrastructure decisions. Institutions with con-\nstrained budgets may not be able to afford high-end proprietary models or high-volume API usage.\nRetrieval-grounded workflows can help offset these limitations by boosting the effectiveness of lower-\ncost models (Li et al.,2024; An et al.,2025). Regardless of vendor, institutions should prioritize systems\nthat enable verifiable source citations and offer sufficient control over token or compute budgets to avoid\nunexpected cost spikes during exam generation and scoring (An et al.,2025).\nFinally, fairness and bias mitigation require ongoing oversight. Even when using a vetted corpus,\nretrieval-grounded systems may reflect cultural, linguistic, or ideological biases present in the source\nmaterial (Dai et al.,2024; Guo et al.,2024). Institutions should establish routine audits of both prompts\nand retrieved content, supported by faculty review and student feedback channels (Ni et al.,2025). An\nappeals process for AI-scored responses can further promote procedural fairness and transparency.\nThese implementation considerations are not tied to any specific model or domain and can be adapted\nacross disciplines and technical platforms. By embedding these safeguards into institutional workflows,\nuniversities and other educational organizations can support AI-assisted assessment in a way that aligns\nwith evolving regulatory expectations and ethical standards (Ni et al.,2025).\nData & Policy e57-15\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\n5.4. Strategic and policy implications\nAs educational institutions evaluate how to integrate LLMs into academic assessment, this study\nhighlights two strategic design choices that remain stable across rapid model iteration: (1) coupling the\nsystem with RAG to ground answers in auditable source material, and (2) enforcing human-in-the-loop\noversight at key decision points. These principles are architecture-agnostic and vendor-neutral, making\nthem a durable basis for institutional policy, procurement standards, and accreditation frameworks.\nRather than focus on proprietary model benchmarks, institutions should assess whether an AI-assisted\nexam system provides traceable citations, retrieval transparency, and override controls for educators. For\nexample, retrieval grounding improved answer accuracy more consistently than upgrading from GPT-3.5\nTurbo to GPT-4 Turbo in the absence of RAG. This suggests that transparent retrieval pipelines can raise\nthe performance floor for less expensive models and reduce reliance on high-cost commercial APIs. It also\nunderscores the importance of investing in retrieval infrastructure, versioned knowledge bases, and\ncitation audit trails rather than focusing exclusively on model specifications.\nThese findings carry important equity implications. Because RAG can elevate the quality of responses\nfrom lower-cost models, institutions with constrained budgets can still achieve acceptable validity and\nreliability targets. Policymakers can mitigate resource disparities by requiring that AI-assisted assessment\nworkflows be reproducible on at least one open-weight or low-cost model. This approach echoes the risk-\nbased logic of the EU AI Act and the documentation principles in the NIST AI RMF, both of which\nprioritize accountability over model branding.\nThe study also emphasizes that hallucination is not primarily a function of model size. Non-RAG\nmodels occasionally produced plausible-sounding but unsupported answers, while RAG-enabled models\nconsistently retrieved relevant and citable textbook content. Because hallucinations result from the\nprobabilistic nature of generative decoding, the presence of curated source material is a more effective\nsafeguard than simply using a larger model. Embedding retrieval into the workflow is, therefore, a general\nmitigation strategy applicable across future architectures.\nTo operationalize these insights, several oversight mechanisms can be incorporated into institutional\nand accreditation policy:\n1. Target oversight where it matters most. The largest accuracy gap was found in short-answer\nresponses, where RAG and non-RAG implementations differed by an average of approximately\n6.6 ROUGE-1 points. In contrast, true-false performance remained consistently high across\nimplementations. Institutions should therefore prioritize faculty review or automated similarity\nchecks for open-ended answers, rather than applying them uniformly across all question formats.\nThis targeted oversight reduces workload without compromising quality.\n2. Use retrieval confidence as a risk flag. Non-RAG models offered no basis for evaluating confi-\ndence, but RAG-enabled pipelines allow inspection of top-k retrieval scores. A practical protocol is\nto route answers with low retrieval confidence to faculty for manual grading. This threshold-based\napproach aligns with risk management principles and avoids blanket intervention.\n3. Define model-agnostic procurement baselines. A GPT-3.5 Turbo model with RAG outperformed\nGPT-4 Turbo without RAG on several exams. Institutions can use this configuration as a public\nbaseline when evaluating vendor proposals. Systems that fail to meaningfully outperform this setup\nshould not justify premium pricing. This benchmarking method encourages both competition and\ntransparency.\n4. Redirect budgets toward open, verifiable infrastructure. Because RAG offers a higher return on\ninvestment than simply upgrading model tiers, educational funding should support the develop-\nment of well-maintained retrieval frameworks and curated knowledge corpora. This also promotes\nthe use of openly licensed educational materials that benefit the wider academic community.\n5. Ensure auditability and due process. Traceable answers allow students and faculty to challenge or\nverify AI-generated outcomes. Accreditation bodies could require that every exam item include a\nstructured citation format that supports both automated processing, such as machine-readable\ne57-16 Erick Tyndall et al.\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\nmetadata, and manual review, such as human-readable citations. Assessment systems should also\nsupport comprehensive record-keeping and review, including the ability to log retrieved source\nexcerpts, track model responses over time, and generate audit reports that faculty or administrators\ncan inspect during grade disputes or accreditation reviews.\nTaken together, these strategic and operational recommendations support a governance model grounded\nin transparency, oversight, and adaptability. Rather than focusing on model size or vendor branding,\ninstitutions can prioritize traceability, cost-effectiveness, and instructional alignment. As large language\nmodel technologies continue to evolve, this framework offers a durable foundation for responsibly\nintegrating AI-assisted assessment systems that meet academic, legal, and equity standards.\n5.5. Limitations and future research\nThe experiment was deliberately scoped to two OpenAI models and a single undergraduate textbook to\ncontrol for confounding factors. While this limits direct generalization, the mechanisms highlighted\n(retrieval grounding and human oversight) are not bound to those specifics. Future work should stress-test\nthese principles in multilingual courses, across subject domains with higher conceptual complexity, and\nwith truly open-weight models. Such studies would provide further evidence that the policy guidance\narticulated here is robust to technological churn and disciplinary variation.\nSupplementary material. The supplementary material for this article can be found athttp://doi.org/10.1017/dap.2025.10024.\nData availability statement.The results were derived from interactions with GPT-3.5 Turbo and GPT-4 Turbo during the first\nquarter of 2024. Due to the evolving nature of large language models, the outputs may no longer be reproducible with newer model\niterations. However, the complete codebase and all exam-generation outputs have been deposited on Zenodo athttps://doi.org/\n10.5281/zenodo.15769736 (Tyndall et al.,2025) as of 29 June 2025. The repository contains the Jupyter notebook that implements\nthe assistants and evaluation pipeline, a combined exam-response CSV file with 14 exams, textbook excerpts, and model answers,\nand a CSV containing the summary of metrics.\nAuthor contribution. Conceptualization: C.G., A.S.; Validation: J.G.; Project administration: E.T., T.W., B.L.; Writing: original\ndraft: C.G., A.S., E.T.; Writing: review and editing: E.T., T.W., B.L.; All authors approved the final submitted draft.\nFunding statement. This work received no specific grant funding.\nCompeting interests. The authors declare no potential conflicts of interest with respect to the research, authorship and/or\npublication of this article. The views expressed are those of the authors and do not reflect the official guidance or position of the\nUnited States Government, the Department of Defense, the United States Air Force, the United States Space Force or any agency\nthereof. Reference to specific commercial products does not constitute or imply its endorsement, recommendation, or favoring by\nthe U.S. Government. The authors declare this is a work of the U.S. Government and is not subject to copyright protections in the\nUnited States. This article has been cleared with case number WPAFB-2024-0625.\nReferences\nAkgun S and Greenhow C(2021) Artificial intelligence in education: Addressing ethical challenges in K-12 settings.AI and Ethics\n2(3). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8455229/.\nAn Y, Cheng Y, Park SJ and Jiang J(2025) HyperRAG: Enhancing quality-efficiency tradeoffs in retrieval-augmented generation\nwith Reranker KV-Cache reuse.arXiv. https://doi.org/10.48550/arXiv.2504.02921.\nAwan AA(2023) The Pros and Cons of Using LLMs in the Cloud Versus Running LLMs Locally.Datacamp. https://www.datacamp.\ncom/blog/the-pros-and-cons-of-using-llm-in-the-cloud-versus-running-llm-locally.\nAzam N, Michala AL, Ansari S and Truong N(2024) Modelling Technique for GDPR-compliance: Toward a Comprehensive\nSolution. arXiv. https://doi.org/10.48550/arXiv.2404.13979.\nCooper AF, Gokaslan A, Cyphert AB, Sa CD, Lemley MA, Ho DE and Liang P(2025) Extracting memorized pieces of\n(copyrighted) books from open-weight language models.arXiv. https://doi.org/10.48550/arXiv.2505.12546.\nDai S, Xu C, Xu S, Pang L, Dong Z and Xu J(2024) Bias and unfairness in information retrieval systems: New challenges in the\nLLM Era. In:Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. pp. 6437–6447.\nhttps://doi.org/10.1145/3637528.3671458\nDigital Education Council(2024) EU AI Act: What It Means for Universities.The Digital Education Council.https://www.\ndigitaleducationcouncil.com/post/eu-ai-act-what-it-means-for-universities.\nData & Policy e57-17\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press\nDilmegani C(2024) In-Depth Guide to Cloud Large Language Models (LLMs) in 2024.AIMultiple: High Tech Use Cases & Tools\nto Grow Your Business.https://research.aimultiple.com/cloud-llm/.\nEmmanuel C(2023, August 3) GPT-3.5 and GPT-4 Comparison:Medium. https://medium.com/@chudeemmanuel3/gpt-3-5-and-\ngpt-4-comparison-47d837de2226.\nGan A, Yu H, Zhang K, Liu Q, Yan W, Huang Z, Tong S and Hu G(2025) Retrieval Augmented Generation Evaluation in the\nEra of Large Language Models: A Comprehensive Survey.arXiv. https://doi.org/10.48550/arXiv.2504.14891.\nGao Y, Xiong Y, Gao X, Jia K, Pan J, Bi Y, Dai Y, Sun J, Wang M and Wang H(2024) Retrieval-Augmented Generation for\nLarge Language Models: A Survey.https://arxiv.org/pdf/2312.10997.\nGuo Y, Guo M, Su J, Yang Z, Zhu M, Li H, Qiu M and Liu SS(2024) Bias in Large Language Models: Origin, Evaluation, and\nMitigation. arXiv. https://doi.org/10.48550/arXiv.2411.10915.\nGuinet G, Omidvar-Tehrani B, Deoras A and Callot L(2024) Automated Evaluation of Retrieval-Augmented Language Models\nwith Task-Specific Exam Generation. arXiv.https://doi.org/10.48550/arXiv.2405.13622.\nHolistic AI(2024) Elements of the NISTAI RMF: What You Need to Know.Holistic AI.https://www.holisticai.com/blog/nist-ai-rmf-\ncore-elements.\nHonnibal M, Montani I, Van Landeghem S and Boyd A(2020) spaCy: Industrial-strength Natural Language Processing in\nPython. Zenodo.https://doi.org/10.5281/zenodo.1212303.\nHugging Face(n.d.) Metric: Rouge. ROUGE– A Hugging Face Space by Evaluate-Metric.https://huggingface.co/spaces/evaluate-\nmetric/rouge.\nJain S (2023) Elevating LLMs with ROUGE Evaluation.UpTrain AI. https://blog.uptrain.ai/evaluating-llms-with-rouge-evalu\nation/.\nKelso E, Soneji A, Rahaman S, Soshitaishvili Y and Hasan R(2024) Trust, Because You Can’t Verify: Privacy and Security\nHurdles in Education Technology Acquisition Practices.arXiv. https://doi.org/10.48550/arXiv.2405.11712.\nKlang E, Portugez S, Gross R, Brenner A, Gilboa M, Ortal T and Segal G(2023) Advantages and pitfalls in utilizing artificial\nintelligence for crafting medical examinations: A medical education pilot study with GPT-4.BMC Medical Education 23. https://\ndoi.org/10.1186/s12909-023-04752-w.\nKoga T, Wu R and Chaudhuri K(2025) Privacy-Preserving Retrieval-Augmented Generation with Differential Privacy.arXiv.\nhttps://doi.org/10.48550/arXiv.2412.04697.\nLewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, Küttler H, Lewis M, Yih W-t, Rocktäschel T, Riedel S and Kiela\nD (2021) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv.https://arxiv.org/abs/2005.11401.\nLi Z, Li C, Zhang M, Mei Q and Bendersky M(2024) Retrieval Augmented Generation or Long-Context LLMs? A Compre-\nhensive Study and Hybrid Approach.arXiv. https://doi.org/10.48550/arXiv.2407.16833,\nLin CY(2004) ROUGE: A Package for Automatic Evaluation of Summaries.https://www.aclweb.org/anthology/W04-1013.\nMawyer A, Auelua R, Aikau H, Barcham M, Boeger Z, Dawrs S, Genz J and Kava L(2020) Introduction to Pacific Studies.\nHonolulu: Center for Pacific Islands Studies, University of Hawaiʻi-Mānoa.\nMithun P, Noriega-Atala E, Merchant N and Skidmore E(2025) AI-VERDE: A Gateway for Egalitarian Access to Large\nLanguage Model-Based Resources For Educational Institutions.arXiv. https://doi.org/10.48550/arXiv.2502.09651.\nNi B, Liu Z, Wang L, Lei Y, Zhao Y, Cheng X, Zeng Q, Dong L, Xia Y, Kenthapadi K, Rossi R, Dernoncourt F, Tanjim MM,\nAhmed N, Liu X, Fan W, Blasch E, Wang Y, Jiang M and Derr T(2025) Towards Trustworthy Retrieval Augmented\nGeneration for Large Language Models: A Survey (No. arXiv:2502.06872; Version 1). arXiv. https://doi.org/10.48550/\narXiv.2502.06872.\nOpenAI (n.d.) File Search.Tools. https://platform.openai.com/docs/assistants/tools.\nOpenAI (n.d.) How Assistants Work.How It Works.https://platform.openai.com/docs/assistants/how-it-works.\nOpenAI (n.d.) Pricing. Other Models.https://platform.openai.com/docs/pricing#other-models.\nRay PP(2023) ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future\nscope. Internet of Things and Cyber-Physical Systems 3(1), 121–154. https://doi.org/10.1016/j.iotcps.2023.04.003.\nShrestha AK, Barthwal A, Campbell M, Shouli A, Syed S, Joshi S and Vassileva J(2024) Navigating AI to Unpack Youth\nPrivacy Concerns: An In-Depth Exploration and Systematic Review.arXiv. https://doi.org/10.48550/arXiv.2412.16369.\nSupe K (2023) Understanding Cosine Similarity in Python with Scikit-Learn[Review of Understanding Cosine Similarity in\nPython with Scikit-Learn]. Memgraph.https://memgraph.com/blog/cosine-similarity-python-scikit-learn.\nTuring.com (2023) Best Strategies to Minimize Hallucinations in LLMs: A Comprehensive Guide.https://www.turing.com/\nresources/minimize-llm-hallucinations-strategy.\nTyndall E, Gayheart C, Some A, Wagner T, Langhals B and Genz J(2025) Undergraduate Pacific Studies Exam Generation\nand Answering Using Retrieval Augmented Generation and Large Language Models. Zenodo. https://doi.org/10.5281/\nzenodo.15769737.\nZhao D (2024) FRAG: Toward Federated Vector Database Management for Collaborative and Secure Retrieval-Augmented\nGeneration. arXiv. https://doi.org/10.48550/arXiv.2410.13272.\nCite this article:Tyndall E, Gayheart C, Some A, Genz J, Wagner T and Langhals B (2025). Impact of retrieval augmented\ngeneration and large language model complexity on undergraduate exams created and taken by AI agents.Data & Policy, 7: e57.\ndoi:10.1017/dap.2025.10024\ne57-18 Erick Tyndall et al.\nhttps://doi.org/10.1017/dap.2025.10024 Published online by Cambridge University Press",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5391682982444763
    },
    {
      "name": "Natural language processing",
      "score": 0.38711118698120117
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34372758865356445
    },
    {
      "name": "Information retrieval",
      "score": 0.32540225982666016
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35722693",
      "name": "University of Hawaii at Hilo",
      "country": "US"
    }
  ],
  "cited_by": 0
}