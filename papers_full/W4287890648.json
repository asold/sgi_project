{
  "title": "Language Model Augmented Monotonic Attention for Simultaneous Translation",
  "url": "https://openalex.org/W4287890648",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5052409554",
      "name": "Sathish Reddy Indurthi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5060189444",
      "name": "Mohd Abbas Zaidi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101981867",
      "name": "Beomseok Lee",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A5007209790",
      "name": "Nikhil Kumar Lakumarapu",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A5100761049",
      "name": "Sang‐Ha Kim",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2605141709",
    "https://openalex.org/W2963545917",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2951456627",
    "https://openalex.org/W2975711469",
    "https://openalex.org/W4287649493",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2964078338",
    "https://openalex.org/W3096871117",
    "https://openalex.org/W2963174344",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2597891111",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2952992734",
    "https://openalex.org/W2945700568"
  ],
  "abstract": "Sathish Reddy Indurthi, Mohd Abbas Zaidi, Beomseok Lee, Nikhil Kumar Lakumarapu, Sangha Kim. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 38 - 45\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nLanguage Model Augmented Monotonic Attention for Simultaneous\nTranslation\nSathish Indurthi§∗ Mohd Abbas Zaidi‡\nBeomseok Lee‡† Nikhil Kumar Lakumarapu‡† Sangha Kim‡\n‡Samsung Research, South Korea §Zoom AI Lab, Singapore\nsathishreddy.indurthi@zoom.us, {abbas.zaidi, bsgunn.lee, n07.kumar, sangha01.kim}@samsung.com\nAbstract\nThe state-of-the-art adaptive policies for simul-\ntaneous neural machine translation (SNMT)\nuse monotonic attention to perform read/write\ndecisions based on the partial source and target\nsequences. The lack of sufficient information\nmight cause the monotonic attention to take\npoor read/write decisions, which in turn neg-\natively affects the performance of the SNMT\nmodel. On the other hand, human translators\nmake better read/write decisions since they can\nanticipate the immediate future words using\nlinguistic information and domain knowledge.\nIn this work, we propose a framework to aid\nmonotonic attention with an external language\nmodel to improve its decisions. Experiments on\nMuST-C English-German and English-French\nspeech-to-text translation tasks show the future\ninformation from language model improves the\nstate-of-the-art monotonic multi-head attention\nmodel further.\n1 Introduction\nA typical application of simultaneous neural ma-\nchine translation (SNMT) is conversational speech\nor live video caption translation. In order to achieve\nlive translation, an SNMT model alternates be-\ntween performing read from source sequence and\nwrite to target sequence. For a model to decide\nwhether to read or write at certain moment, either a\nfixed or an adaptive read/write policy can be used.\nEarlier approaches in simultaneous translation\nsuch as Ma et al. (2019a) and Dalvi et al. (2018)\nemploy a fixed policy that alternate between read\nand write after the waiting period of ktokens. To\nalleviate possible long delay of fixed polices, re-\ncent works such as monotonic infinite lookback\nattention (MILk) (Arivazhagan et al., 2019), and\nmonotonic multihead attention (MMA) (Ma et al.,\n2019c) developed flexible policies using monotonic\nattention (Raffel et al., 2017).\n∗⋆Work done while at Samsung Research\n†Equal contribution\nFigure 1: The finetuned XLM-RoBERTa language\nmodel predicts German words using the prefix as in-\nput.(Green: Correct, Red: Incorrect, Black: Neutral).\nWhile these monotonic attention anticipates tar-\nget words using only available prefix source and\ntarget sequence, human translators anticipate the\ntarget words using their language expertise (linguis-\ntic anticipation) as well as contextual information\n(extra-linguistic anticipation) (Vandepitte, 2001).\nInspired by human translation experts, we aim to\naugment monotonic attention with future informa-\ntion using language models (LM) (Devlin et al.,\n2019; Conneau et al., 2019).\nIntegrating the external information effectively\ninto text-to-text machine translation (MT) systems\nhas been explored by several works (Khandelwal\net al., 2020; Gulcehre et al., 2015, 2017; Stahlberg\net al., 2018). Also, integrating future information\nimplicitly into SNMT system during training is ex-\nplored in Wu et al. (2020) by simultaneously train-\ning different wait-kSNMT systems. However, no\nprevious works make use of explicit future informa-\ntion both during training and inference. To utilize\nexplicit future information, we explored to inte-\ngrate future information from LM directly into the\noutput layer of the MMA model. However, it did\nnot provide any improvements (refer to Appendix\nA), thus motivating us to explore a tighter integra-\ntion of the LM information into SNMT model.\nIn this work, we explicitly use plausible future\n38\nFigure 2: Overview of the proposed language model augmented monotonic attention for SNMT.\ninformation from LM during training by transform-\ning the monotonic attention mechanism. As shown\nin Figure 1, at each step, the LM takes the prefix\ntarget (and source, for cross-lingual LM) sequence\nand predicts the probable future information. We\nhypothesize that aiding the monotonic attention\nwith this future information can improve MMA\nmodel’s read/write policy, eventually leading to\nbetter translation with less delay. Several experi-\nments on MuST-C (Di Gangi et al., 2019) English-\nGerman and English-French speech-to-text transla-\ntion tasks with our proposed approach show clear\nimprovements of latency-quality trade-offs over the\nstate-of-the-art MMA models.\n2 Monotonic Attention with Future\nInformation Model\n2.1 Monotonic Attention\nIn simultaneous machine translation (SNMT) mod-\nels, the probability of predicting the target token\nyi ∈y depends on the partial source and target\nsequences (x≤j ∈x,y<i ∈y). In sequence-to-\nsequence based SNMT model, each target token yi\nis generated as follows:\nhj = E(x≤j) (1)\nsi = D(y<i,ci = A(si−1,h≤j)) (2)\nyi = Output(si) (3)\nwhere E(.) and D(.) are the encoder and decoder\nlayers, and ci is a context vector. In monotonic\nattention based SNMT, the context vector is com-\nputed as follows:\nei,j = MonotonicEnergy(si−1,hj) (4)\npi,j = Sigmoid(ei,j) (5)\nzi,j ∼Bernoulli(pi,j) (6)\nWhen generating a target token yi, the decoder\nchooses whether to read/write based on Bernoulli\nselection probability pi,j. When zi,j = 1 (write),\nmodel sets ti = j, ci = hj and generates the target\ntoken yi. For zi,j = 0 (read), it sets ti = j+ 1 and\nrepeats Eq. 4 to 6. Here ti refers to the index of\nthe encoder when decoder needs to produce the ith\ntarget token. Instead of hard alignment of ci = hj,\nRaffel et al. (2017) compute an expected alignment\nin a recurrent manner and propose a closed-form\nparallel solution. Arivazhagan et al. (2019) adopt\nmonotonic attention into SNMT and later, Ma et al.\n(2019c) extend it to MMA to integrate it into the\nTransformer model (Vaswani et al., 2017).\n2.2 Monotonic Attention with Future\nInformation\nThe monotonic attention described in Section 2.1\nperforms anticipation based only on the currently\navailable source and target information. To aug-\nment this anticipation process using future informa-\ntion extracted using LMs, we propose the following\nmodifications to the monotonic attention.\nFuture Representation Layer: At every de-\ncoding step i, the previous target token yi−1 is\nequipped with a plausible future token ˆyi as shown\nin the Figure 2. Since the token ˆyi comes from\nan LM possibly with a different tokenizer and vo-\ncabulary set, applying the model’s tokenizer and\nvocabulary might split the tokenˆyi further into mul-\ntiple sub-tokens {ˆy1\ni ,ˆy2\ni ,··· ,ˆym\ni }. To get a single\nfuture token representation ˜si ∈Rd from all the\nsub-tokens, we apply a sub-token summary layer:\n˜si = Γ({ˆy1\ni ,ˆy2\ni ,··· ,ˆym\ni }) (7)\nThe Γ represents a general sequence representation\nlayer such as a Transformer encoder layer or a sim-\nple normalized sum of sub-token representations.\n39\nWe enrich˜si at every layer lof the decoder block\nby applying a residual feed-forward network.\n˜sl\ni = FFN (˜yl−1\ni ) (8)\nMonotonic Energy Layer with Future Informa-\ntion: Despite the fact that we can add the plau-\nsible future information to the output layer (Ap-\npendix A) or append it to the target token represen-\ntation yi−1, the MMA read/write decisions happen\nin Eq. 4. Therefore, we integrate ˜si into the Eq. 4\ninstead.\nThe integration is carried out by modifying Eq.\n4 - Eq. 5. We compute the monotonic energy for\nfuture information using the enriched future token\nrepresentation ˜si available at each layer:\n˜ei,j = MonotonicEnergy(˜si,hj) (9)\nWe integrate the future monotonic energy function\ninto Eq. 5 as follows:\n˜pi,j = Sigmoid(ei,j + ˜ei,j) (10)\nAfter computing ˜pi,j, we compute ci similar to\nMMA model.\nThis way of integration of future information\nallows the model to condition the LM output us-\nage on the input sequence. The model can control\nthe relative weightage given to the LM output by\nvarying the ˜ei,j. In case of insufficient source in-\nformation in the low latency regime, we expect the\nmodel’s decision policy to rely more on ˜ei,j.\nInference: During inference, the start token does\nnot contain any plausible information. After pre-\ndicting the first target token, for every subsequent\nprediction of target token yi, we invoke the LM to\npredict the next plausible future token and integrate\nthis new information into Eq. 10.\n3 Experiments and Results\n3.1 Experimental Settings\nDatasets and Metrics: We conduct our experi-\nments on the MuST-C English(En)-German(De)\nand English(En)-French(Fr) speech-to-text (ST)\ntranslation task. The speech sequence is repre-\nsented using 80-dimensional log-mel filter bank\nfeatures. The target sequence is represented as sub-\nwords using a SentencePiece (Kudo and Richard-\nson, 2018) model with a unigram vocabulary of\nsize 10,000. We evaluate the performance of the\nmodels on both the latency and quality aspects. We\nuse Average Lagging(AL) as our latency metric\nand case-sensitive detokenized SacreBLEU (Post,\n2018) to measure the translation quality, similar\nto (Ma et al., 2020). The best models are chosen\nbased on the dev set results and reported results are\nfrom the MuST-C test (tst-COMMON) sets.\nLanguage Models We use two language mod-\nels to train our proposed modified MMA model.\nFirstly, we use the pretrained XLM-RoBERTa\n(Conneau et al., 2019) model from Huggingface\nTransformers1 model repository. Since the LM out-\nput can be very open-ended and might not directly\nsuit/cater to our task and dataset, we finetune the\nhead of the model using the MuST-C target text\ndata for each task.\nWe also train a smaller language model (SLM),\nwhich contains 6 Transformer decoder layers, 512\nhidden-states and 24M parameters. We use the\nMuST-C data along with additional data augmen-\ntation to reduce overfitting. The SLM helps to\nremove the issues related to vocabulary mismatch\nas discussed in the Section 2.2.\nImplementation Details: Our base model is\nadopted from Ma et al. (2020). We use a pre-\ndecision ratio of 7 , which means that the simultane-\nous read/write decisions are made after every seven\nencoder states. We use λ or λlatency to refer to\nthe hyperparameter corresponding to the weighted\naverage(λavg) in MMA. The values of this hyperpa-\nrameter λare chosen from the set{0.01,0.05,0.1}.\nThe Γ layer in Eq. 7 computes the normalized sum\nof the sub-token representations. For SLM, it sim-\nply finds the embedding since it shares the same\nvocabulary set. All the models are trained on a\nNVIDIA v100 GPU with update_freq set to 8.\nSimultaneous Translation Models: Even\nthough future information can be integrated\nexplicitly into the fixed policy approaches such as\nWait-K (Ma et al., 2019b), we choose monotonic\nattention as our baseline due to its superior\nperformance (Arivazhagan et al., 2019; Ma et al.,\n2019c). We train a baseline based on Ma et al.\n(2020) work, called as MMA model. The MMA\nmodel encoder and decoder embedding dimensions\nare set to 392, whereas our proposed model’s\nencoder and decoder embeddings are set to 256\nto have similar parameters ( ≈ 39M) for a fair\ncomparison. We train two models using the\n1https://huggingface.co/transformers/\n40\nFigure 3: LM prediction weight vs λ\nmodified MMA based on two LMs (XLM, SLM),\nreferred as MMA-XLM and MMA-SLM.\n3.2 Results\nWe first analyze how the LM predictions are being\nutilized by the our model. In order to measure the\nrelative weight given to model’s internal states ver-\nsus the predictions from the LM, we compare the\nnorm of the monotonic energies corresponding to\nthe LM predictions epred (Eq. 9) and the previous\noutput tokens eoutput (Eq. 4). Let us define LM\nprediction weight as follows:\nLMpw =\n( ∥epred∥\n∥eoutput∥\n)\n(11)\nIn Figure 3, we plot the variation of LMpw (av-\neraged) vs. λ. We use two additional values of\nλ ∈{0.005,0.001}to obtain this plot. We can\nobserve that as the latency requirements become\nmore and more strict, the model starts to give more\nweightage to the predictions coming from the LM.\nThis shows that the model learns to utilize the in-\nformation coming from LM predictions based on\nlatency requirements.\nNext, we discuss the performance improvements\nobtained from our proposed approach. By vary-\ning the λ, we train separate models for different\nlatency regimes. Moreover, the quality and latency\nfor a particular model can also be varied by control-\nling the speech segment size during the inference.\nSpeech segment size or step size refers to the du-\nration of speech (in ms) processed corresponding\nto each read decision. We vary these hyperparame-\nters for all the three models, namely MMA, MMA-\nXLM and MMA-SLM.\nThe BLEU-AL curves for all the models have\nbeen provided in Figure 4 and BLEU-AL num-\nbers for all models are included in Appendix F\n500 1000 1500 2000 2500 3000\nLatency (AL)\n4\n6\n8\n10\n12\n14\n16\n18\nBLEU\nstep size 120\nstep size 200\nstep size 280\nstep size 360\nstep size 440\nstep size 520\nMMA MMA- XLM MMA-SLM\n(a) EnDe Task\n500 1000 1500 2000 2500\nLatency (AL)\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\nBLEU\nstep size 120\nstep size 200\nstep size 280\nstep size 360\nstep size 440\nstep size 520\nMMA MMA- XLM MMA-SLM\n(b) EnFr Task\nFigure 4: BLEU vs Average Lagging results for MMA,\nMMA-XLM and MMA-SLM models.\nfor reference. We vary the step sizes in intervals\nof 80ms from 120 ms to 520 ms in order to get\nperformances corresponding to different latency\nregimes. We can observe that the LM-based mod-\nels using both XLM and SLM provide a significant\nperformance improvement over the baseline MMA\nmodel. We observe improvements in the range of\n1-2 BLEU scores consistently across all the latency\nregimes (λ = 0 .1,0.05,0.01). The MMA using\nSLM language model performs slightly better than\nMMA using XLM language model. This is due to\nSLM’s higher accuracy on the next token predic-\ntion task as compared to XLM, 30.15% vs. 21.5%\nfor German & 31.65% vs. 18.45% for French. The\nhigh accuracy of SLM is attributed to its training\non in-domain data.\n4 Conclusion\nIn this work, we provide a generic framework to\nintegrate the linguistic and extra-linguistic infor-\nmation into simultaneous models. We rely on lan-\n41\nguage models to extract this plausible future in-\nformation and propose a new monotonic attention\nmechanism to infuse this information. Several ex-\nperiments on speech-to-text translation tasks show\nthe effectiveness of proposed approach on obtain-\ning superior quality-latency trade-offs, compared to\nthe state-of-the-art monotonic multihead attention.\nReferences\nNaveen Arivazhagan, Colin Cherry, Wolfgang\nMacherey, Chung-Cheng Chiu, Semih Yavuz, Ruom-\ning Pang, Wei Li, and Colin Raffel. 2019. Monotonic\ninfinite lookback attention for simultaneous machine\ntranslation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics, pages 1313–1323, Florence, Italy.\nAssociation for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, and Stephan\nV ogel. 2018. Incremental decoding and training\nmethods for simultaneous translation in neural ma-\nchine translation. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 2 (Short Papers), pages\n493–499, New Orleans, Louisiana. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli,\nMatteo Negri, and Marco Turchi. 2019. MuST-C: a\nMultilingual Speech Translation Corpus. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2012–2017, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On\nusing monolingual corpora in neural machine trans-\nlation.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, and Yoshua Bengio. 2017. On integrating a lan-\nguage model into neural machine translation. Com-\nputer Speech and Language, 45:137–148.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Nearest\nneighbor machine translation. arXiv preprint\narXiv:2010.00710.\nSosuke Kobayashi. 2018. Contextual augmentation:\nData augmentation by words with paradigmatic re-\nlations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 452–457,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nMingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,\nKaibo Liu, Baigong Zheng, Chuanqiang Zhang,\nZhongjun He, Hairong Liu, Xing Li, Hua Wu, and\nHaifeng Wang. 2019a. Stacl: Simultaneous trans-\nlation with implicit anticipation and controllable la-\ntency using prefix-to-prefix framework.\nMingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,\nKaibo Liu, Baigong Zheng, Chuanqiang Zhang,\nZhongjun He, Hairong Liu, Xing Li, Hua Wu, and\nHaifeng Wang. 2019b. STACL: Simultaneous trans-\nlation with implicit anticipation and controllable la-\ntency using prefix-to-prefix framework. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3025–3036, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nXutai Ma, Juan Pino, James Cross, Liezl Puzon, and\nJiatao Gu. 2019c. Monotonic multihead attention.\nXutai Ma, Juan Pino, and Philipp Koehn. 2020.\nSimulmt to simulst: Adapting simultaneous text\ntranslation to end-to-end simultaneous speech trans-\nlation. arXiv preprint arXiv:2011.02048.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. arXiv preprint arXiv:1804.08771.\nColin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J.\nWeiss, and Douglas Eck. 2017. Online and linear-\ntime attention by enforcing monotonic alignments.\nIn Proceedings of the 34th International Conference\non Machine Learning, volume 70 of Proceedings\nof Machine Learning Research, pages 2837–2846.\nPMLR.\nFelix Stahlberg, James Cross, and Veselin Stoyanov.\n2018. Simple fusion: Return of the language model.\nIn Proceedings of the Third Conference on Machine\nTranslation: Research Papers, pages 204–211, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\n42\nSonia Vandepitte. 2001. Anticipation in conference\ninterpreting: a cognitive process. Alicante Journal\nof English Studies / Revista Alicantina de Estudios\nIngleses, 0(14):323–335.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nXueqing Wu, Yingce Xia, Lijun Wu, Shufang Xie,\nWeiqing Liu, Jiang Bian, Tao Qin, and Tie-Yan Liu.\n2020. Learn to use future information in simultane-\nous translation.\nA LM at MMA Output Layer\nWe explored a naive approach of integrating LM\ninformation into the MMA. In this approach, we in-\ntegrate the future information obtained from the\nLM directly into the output layer of the MMA\nmodel. We refer to this experiment as ‘LM Rescor-\ning(LMR)’, and the corresponding model is called\nMMA-LMR.\nAs observed in Figure 5, MMA-LMR has infe-\nrior performance compared to the MMA model.\nSince the LM information integration is only done\nat the output layer of the model, the MMA model\ncannot easily discard the incorrect information\nfrom LM. This motivates us to tightly integrate\nthe LM information into the simultaneous model.\nB Language Models\nAs mentioned earlier, we train two different lan-\nguage models (LMs) and use them to improve the\nanticipation in monotonic attention based Simulta-\nneous models.\nB.1 XLM-Roberta(XLM-R)\nXLM-R Large model 2 was trained on the 100 lan-\nguages CommonCrawl corpora total size of 2.5TB\nwith 550M parameters from 24 layers, 1024 hid-\nden states, 4096 feed-forward hidden-states, and\n16 heads. Total number of parameters is 558M. We\nfinetune the head of the XLM-R LM model using\nthe Masked Language Modeling objective which\naccounts for 0.23% of the total model parameters,\ni.e., 1.3M parameters.\nB.2 Smaller Language Model\nSince the LM predictions are computed serially\nduring inference, the time taken to compute the\n2https://huggingface.co/xlm-roberta-large\nLM token serves as a bottleneck to the latency re-\nquirements. To reduce the LM computation time,\nwe train a smaller Language Model (SLM) from\nscratch using the Causal Language Modeling ob-\njective. SLM is composed of 6 Transformer de-\ncoder blocks, 512 hidden-states, 2048 feed-forward\nhidden-states & 8 attention heads. It alleviates\nthe need for the sub-token summary layer since\nit shares the vocabulary and tokenization with the\nMMA models. The train examples are at the sen-\ntence level, rather than forming a block out of multi-\nple sentences(which is the usual case for Language\nModels).\nSince the target texts contain lesser than 250k\nexamples, we use additional data augmentation\ntechniques to upsample the target data. We also use\nadditional data to avoid overfitting on the MuST-C\ntarget text. Details have been provided in B.2.1.\nB.2.1 Data Augmentation\nUp-Sampling: To boost the LM performance\nand mitigate overfitting, we use contextual data\naugmentation (Kobayashi, 2018) to upsample the\nMuST-C target text data by substituting and insert-\ning words based on LM predictions. We use the\nNLPAUG3 package to get similar words based on\ncontextual embeddings. From the Hugging Face\nRepository, we use two different pretrained BERT\n(Devlin et al., 2019) models for German bert-base-\ngerman-dbmdz-cased & bert-base-german-dbmdz-\nuncased and bert-base-fr-cased for French. We\nupsample German to 1.13M examples and French\nto 1.38M examples.\nAdditional Data: We also use additional data\nto avoid overfitting. For German we use the\nNewscrawl(WMT 19) data which includes 58M\nexamples. For French, we use Common Crawl and\nEuroparl to augment 4M extra training examples.\nWe observe that both upsampling and data aug-\nmentation help us to reduce the overfitting on the\nMuST-C dev set.\nB.3 Token Prediction\nFor each output token, the LM prediction is ob-\ntained by feeding the prefix upto that token to the\nLM model. These predictions are pre-computed\nfor training and validation sets. This ensures par-\nallelization and avoids the overhead to run the LM\nsimultaneously during the training process. During\n3https://pypi.org/project/nlpaug/\n43\n(a) EnDe Task\n (b) EnFr Task\nFigure 5: BLEU vs Average Lagging results for MMA and MMA-LMR models. Each model is trained with\ndifferent λ= 0.1,0.05,0.01 values. Each BLUE-AL point obtained by varying step size and λ.\ninference, the LM model is called every time a new\noutput token is written.\nC Dataset\nThe MuST-C dataset comprises of English TED\ntalks, the translations and transcriptions have been\naligned with the speech at sentence level. Dataset\nstatistics have been provided in the Table 1.\nD Effect of LM Size on Latency-Quality\nWe train several SLM models with varying sizes in\nour experiments and choose the best model based\non the top-1 accuracy. As we increase the number\nof layers in the LM model from 2 to 4 to 6 layers,\nthe SLM and the proposed MMA with future infor-\nmation models have shown performance improve-\nments. However, increasing the number of layers\ngreater than 6 does not yield any performance im-\nprovements. We also notice this degradation of\nperformance with the XLM model while varying\nthe number of hidden layers in the LM head.\nE Training Details\nWe follow the training process similar to Ma et al.\n(2020) training process. We train an English ASR\nmodel using the source speech data. Next, we\ntrain a simultaneous model without the latency loss\n(setting λlatency = 0) after initializing the encoder\nfrom the English ASR model. After this step, we\nfinetune the simultaneous model for different λs.\nThis training process is repeated for all the reported\nmodels and for each task. The details regarding the\nhyperparameters for the model have been provided\nin Table 2.\nF BLEU-AL Numbers\nAs mentioned in the results section of the main pa-\nper, we vary the latency weight hyperparameter (λ)\nto train different models to obtain different latency\nregimes. We also vary the step-size/speech seg-\nment size during inference. In total, we obtain 18\ndifferent data points corresponding to each model.\nIn Table 3, we compare the results obtained using\nMMA, MMA-XLM and MMA-SLM under similar\nhyperparameter settings. It will help the reader to\nquantify the benefits obtained from our proposed\napproach.\n44\nTask # Hours # Sentences # Talks # Words\nTrain Dev Test Source Target\nEnglish-German 408 225k 1,423 2,641 2,093 4.3M 4M\nEnglish-French 492 269k 1,412 2,632 2,510 5.2M 5.4M\nTable 1: Dataset Statistics(# - Number of)\nMMA MMA-XLM/CLMHyperparameter\nencoder layers 12 12\nencoder embed dim 292 256\nencoder ffn embed dim 2048 2048\nencoder attention heads 4 4\ndecoder layers 6 6\ndecoder embed dim 292 256\ndecoder ffn embed dim 2048 2048\nmonotonic ffn embed dim – 2048\ndecoder attention heads 4 4\ndropout 0.1 0.1\noptimizer adam adam\nadam-β (0.9, 0.999) (0.9, 0.999)\nclip-norm 10.0 10.0\nlr scheduler inverse sqrt inverse sqrt\nlearning rate 0.0001 0.0001\nwarmup-updates 4000 4000\nlabel-smoothing 0.0 0.0\nmax tokens 40000 40000\nconv layers 2 2\nconv stride (2,2) (2,2)\n#params ≈39M ≈39M\nTable 2: Model Hyperparameters\nλ Model step size (AL(msec) / BLEU)\n120 200 280 360 440 520\nEnglish-German Task\n0.1\nMMA 378 / 3.54 887 / 7.17 1317 / 9.72 1671 / 11.54 1935 / 12.95 2376 / 13.98\nMMA-XLM 348 / 5.03 848 / 8.77 1269 / 10.4 1631 / 12.78 1961 / 14.22 2272 / 15.34\nMMA-SLM 748 / 8.83 1192 / 10.43 1566 / 12.43 1857 / 13.82 2156 / 14.29 2421 / 15.44\n0.05\nMMA 775 / 6.5 1220 / 10.08 1683 / 11.72 1891 / 12.92 2484 / 13.85 2441 / 14.51\nMMA-XLM 766 / 7.76 1200 / 10.84 1654 / 13.2 1873 / 13.72 2655 / 16.36 2456 / 15.90\nMMA-SLM 1250 / 12.12 1588 / 13.53 1899 / 14.68 2171 / 15.18 2424 / 15.72 2665 / 15.99\n0.01\nMMA 1841 / 13.33 2183 / 14.24 2455 / 14.58 2683 / 15.11 2839 / 16.05 3079 / 16.18\nMMA-XLM 1846 / 14.83 2167 / 15.57 2439 / 16.13 2662 / 16.21 2855 / 16.87 3085 / 17.24\nMMA-SLM 2047 / 14.43 2039 / 15.33 2420 / 16.87 2503 / 16.87 2786 / 17.01 2871 / 17.35\nEnglish-French Task\n0.1\nMMA 471 / 6.77 924 / 13.2 1299 / 17.37 1667 / 19.78 1975 / 20.77 2269 / 22.28\nMMA-XLM 478 / 7.54 902 / 14.48 1284 / 17.74 1607 / 21.02 1972 / 22.17 2251 / 22.79\nMMA-SLM 460 / 7.89 701 / 14.86 1313 / 18.40 1604 / 21.23 1971 / 22.50 2211 / 22.91\n0.05\nMMA 806 / 12.9 1209 / 18.13 1533 / 20.57 1825 / 22.30 2137 / 22.95 2390 / 23.98\nMMA-XLM 796 / 14.06 1184 / 19.19 1512 / 21.25 1807 / 23.17 2117 / 23.75 2363 / 24.78\nMMA-SLM 794 / 14.20 1197 / 19.79 1504 / 21.44 1878 / 23.5 2122 / 24.27 2341 / 24.96\n0.01\nMMA 1728 / 22.84 1725 / 23.6 2204 / 25.07 2416 / 25.44 2632 / 25.56 2824 / 25.81\nMMA-XLM 1713 / 23.11 1701 / 24.89 2116 / 26.24 2420 / 26.19 2631 / 26.07 2796 / 26.17\nMMA-SLM 1725 / 23.33 1704 / 25.16 2217 / 26.54 2412 / 26.63 2631 / 26.57 2812 / 26.55\nTable 3: BLEU vs Average Lagging results for MMA, MMA-XLM and MMA-SLM models on English-German\nand English-French tasks. The models are trained using different latency loss weights (λ= 0.1,0.05,0.01).\n45",
  "topic": "Monotonic function",
  "concepts": [
    {
      "name": "Monotonic function",
      "score": 0.7088268399238586
    },
    {
      "name": "Machine translation",
      "score": 0.6224501132965088
    },
    {
      "name": "Translation (biology)",
      "score": 0.588929295539856
    },
    {
      "name": "Computer science",
      "score": 0.5622069835662842
    },
    {
      "name": "Linguistics",
      "score": 0.5450466275215149
    },
    {
      "name": "Association (psychology)",
      "score": 0.4707513451576233
    },
    {
      "name": "Natural language processing",
      "score": 0.42676645517349243
    },
    {
      "name": "Computational linguistics",
      "score": 0.42076972126960754
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4046252965927124
    },
    {
      "name": "Cognitive science",
      "score": 0.35530292987823486
    },
    {
      "name": "Speech recognition",
      "score": 0.32459449768066406
    },
    {
      "name": "Psychology",
      "score": 0.2985716760158539
    },
    {
      "name": "Philosophy",
      "score": 0.20532092452049255
    },
    {
      "name": "Mathematics",
      "score": 0.19470074772834778
    },
    {
      "name": "Epistemology",
      "score": 0.09388342499732971
    },
    {
      "name": "Chemistry",
      "score": 0.08065655827522278
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}