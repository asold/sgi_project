{
  "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies",
  "url": "https://openalex.org/W2175585630",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225813013",
      "name": "Ji, Shihao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288917512",
      "name": "Vishwanathan, S. V. N.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748145643",
      "name": "Satish Nadathur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4302361629",
      "name": "Anderson, Michael J",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749491488",
      "name": "Dubey, Pradeep",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2138204974",
    "https://openalex.org/W2437096199",
    "https://openalex.org/W1723811852",
    "https://openalex.org/W2058695628",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2271177914",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2295800168",
    "https://openalex.org/W2152808281",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W2251682575",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W1800356822",
    "https://openalex.org/W1917432393",
    "https://openalex.org/W2950797609",
    "https://openalex.org/W2143719855",
    "https://openalex.org/W2169054943",
    "https://openalex.org/W101286142",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W1520465330",
    "https://openalex.org/W2963603213",
    "https://openalex.org/W2108563286"
  ],
  "abstract": "We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers.",
  "full_text": "Published as a conference paper at ICLR 2016\nBLACK OUT: S PEEDING UP RECURRENT NEURAL NET-\nWORK LANGUAGE MODELS WITH VERY LARGE VO-\nCABULARIES\nShihao Ji\nParallel Computing Lab, Intel\nshihao.ji@intel.com\nS. V . N. Vishwanathan\nUniv. of California, Santa Cruz\nvishy@ucsc.edu\nNadathur Satish, Michael J. Anderson & Pradeep Dubey\nParallel Computing Lab, Intel\n{nadathur.rajagopalan.satish,michael.j.anderson,pradeep.dubey}@intel.com\nABSTRACT\nWe propose BlackOut, an approximation algorithm to efï¬ciently train massive\nrecurrent neural network language models (RNNLMs) with million word vocab-\nularies. BlackOut is motivated by using a discriminative loss, and we describe\na weighted sampling strategy which signiï¬cantly reduces computation while im-\nproving stability, sample efï¬ciency, and rate of convergence. One way to under-\nstand BlackOut is to view it as an extension of the DropOut strategy to the out-\nput layer, wherein we use a discriminative training loss and a weighted sampling\nscheme. We also establish close connections between BlackOut, importance sam-\npling, and noise contrastive estimation (NCE). Our experiments, on the recently\nreleased one billion word language modeling benchmark, demonstrate scalabil-\nity and accuracy of BlackOut; we outperform the state-of-the art, and achieve\nthe lowest perplexity scores on this dataset. Moreover, unlike other established\nmethods which typically require GPUs or CPU clusters, we show that a carefully\nimplemented version of BlackOut requires only 1-10 days on a single machine to\ntrain a RNNLM with a million word vocabulary and billions of parameters on one\nbillion words. Although we describe BlackOut in the context of RNNLM training,\nit can be used to any networks with large softmax output layers.\n1 I NTRODUCTION\nStatistical language models are a crucial component of speech recognition, machine translation and\ninformation retrieval systems. In order to handle the data sparsity problem associated with tradi-\ntional n-gram language models (LMs), neural network language models (NNLMs) (Bengio et al.,\n2001) represent the history context in a continuous vector space that can be learned towards error\nrate reduction by sharing data among similar contexts. Instead of using ï¬xed number of words to\nrepresent context, recurrent neural network language models (RNNLMs) (Mikolov et al., 2010) use\na recurrent hidden layer to represent longer and variable length histories. RNNLMs signiï¬cantly\noutperform traditional n-gram LMs, and are therefore becoming an increasingly popular choice for\npractitioners (Mikolov et al., 2010; Sundermeyer et al., 2013; Devlin et al., 2014).\nConsider a standard RNNLM, depicted in Figure 1. The network has an input layer x, a hidden\nlayer s(also called context layer or state) with a recurrent connection to itself, and an output layer\ny. Typically, at time step tthe network is fed as input xt âˆˆRV, where V denotes the vocabulary\nsize, and stâˆ’1 âˆˆRh, the previous state. It produces a hidden state st âˆˆRh, where his the size\nof the hidden layer, which in turn is transformed to the output yt âˆˆRV. Different layers are fully\nconnected, with the weight matrices denoted by â„¦ = {WVÃ—h\nin ,WhÃ—h\nr ,WVÃ—h\nout }.\nFor language modeling applications, the inputxtis a sparse vector of a 1-of-V (or one-hot) encoding\nwith the element corresponding to the input word wtâˆ’1 being 1 and the rest of components of xt set\nto 0; the state of the network st is a dense vector, summarizing the history context {wtâˆ’1,Â·Â·Â· ,w0}\n1\narXiv:1511.06909v7  [cs.LG]  31 Mar 2016\nPublished as a conference paper at ICLR 2016\nğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡\nğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡\nâ„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›\nğ‘(ğ‘¤ğ‘¡|ğ‘¤ğ‘¡âˆ’1,â‹¯,ğ‘¤0)\nğ‘Šğ‘–ğ‘›\nğ‘Šğ‘œğ‘¢ğ‘¡\nğ‘Šğ‘Ÿ\n<s>                   A                  cat          â€¦         sofa\nA                    cat                  is           â€¦        </s>\nâ‹¯\nğ‘¥0\nğ‘¦0\nğ‘ 0\nğ‘¥1\nğ‘¦1\nğ‘ 1\nğ‘¥2\nğ‘¦2\nğ‘ 2\nğ‘¥ğ‘›\nğ‘¦ğ‘›\nğ‘ ğ‘›\nğ‘¤ğ‘¡âˆ’1\n=\nğ‘Šğ‘–ğ‘› ğ‘Šğ‘–ğ‘› ğ‘Šğ‘–ğ‘› ğ‘Šğ‘–ğ‘›\nğ‘Šğ‘œğ‘¢ğ‘¡ ğ‘Šğ‘œğ‘¢ğ‘¡ ğ‘Šğ‘œğ‘¢ğ‘¡ ğ‘Šğ‘œğ‘¢ğ‘¡\nğ‘Šğ‘Ÿ ğ‘Šğ‘Ÿ ğ‘Šğ‘Ÿ\nFigure 1: The network architecture of a standard RNNLM and its unrolled version for an example\ninput sentence: <s> A cat is sitting on a sofa </s>.\npreceding the output word wt; and the output yt is a dense vector, with the i-th element denoting\nthe probability of the next word being wi, that is, p(wi|wtâˆ’1,Â·Â·Â· ,w0), or more concisely, p(wi|st).\nThe input to output transformation occurs via:\nst = Ïƒ(WT\ninxt + Wrstâˆ’1) (1)\nyt = f(Woutst), (2)\nwhere Ïƒ(v) = 1/(1 + exp(âˆ’v)) is the sigmoid activation function, and f(Â·) is the softmax function\nf(ui) := exp(ui)/âˆ‘V\nj=1 exp(uj).\nOne can immediately see that if xt uses a 1-of-V encoding, then the computations in equation (1)\nare relatively inexpensive (typically his of the order of a few thousand, and the computations are\nO(h2)), while the computations in equation (2) are expensive (typicallyV is of the order of a million,\nand the computations are O(Vh)). Similarly, back propagating the gradients from the output layer\nto the hidden layer is expensive. Consequently, the training times for some of the largest models\nreported in literature are of the order of weeks (Mikolov et al., 2011; Williams et al., 2015).\nIn this paper, we ask the following question: Can we design an approximate training scheme for\nRNNLM which will improve on the state of the art models, while using signiï¬cantly less compu-\ntational resources? Towards this end, we propose BlackOut an approximation algorithm to efï¬-\nciently train massive RNNLMs with million word vocabularies. BlackOut is motivated by using a\ndiscriminative loss, and we describe a weighted sampling strategy which signiï¬cantly reduces com-\nputation while improving stability, sample efï¬ciency, and rate of convergence. We also establish\nclose connections between BlackOut, importance sampling, and noise contrastive estimation (NCE)\n(Gutmann & HyvÂ¨arinen, 2012; Mnih & Teh, 2012), and demonstrate that BlackOut mitigates some\nof the limitations of both previous methods. Our experiments, on the recently released one billion\nword language modeling benchmark (Chelba et al., 2014), demonstrate scalability and accuracy of\nBlackOut; we outperform the state-of-the art, achieving the lowest perplexity scores on this dataset.\nMoreover, unlike other established methods which typically require GPUs or CPU clusters, we show\nthat a carefully implemented version of BlackOut requires only 1-10 days on a single CPU machine\nto train a RNNLM with a million word vocabulary and billions of parameters on one billion words.\nOne way to understand BlackOut is to view it as an extension of the DropOut strategy (Srivastava\net al., 2014) to the output layer, wherein we use a discriminative training loss and a weighted sam-\npling scheme. The connection to DropOut is mainly from the way they operate in model training\nand model evaluation. Similar to DropOut, in BlackOut training a subset of output layer is sampled\nand trained at each training batch and when evaluating, the full network participates. Also, like\nDropOut, a regularization technique, our experiments show that the models trained by BlackOut are\nless prone to overï¬tting. A primary difference between them is that DropOut is routinely used at\ninput and/or hidden layers of deep neural networks, while BlackOut only operates at output layer.\nWe chose the name BlackOut in light of the similarities between our method and DropOut, and the\ncomplementary they offer to train deep neural networks.\n2\nPublished as a conference paper at ICLR 2016\n2 B LACK OUT: A SAMPLING -BASED APPROXIMATION\nWe will primarily focus on estimation of the matrix Wout. To simplify notation, in the sequel we\nwill use Î¸to denote Wout and Î¸j to denote the j-th row of Wout. Moreover, let âŸ¨Â·,Â·âŸ©denote the dot\nproduct between two vectors. Given these notations, one can rewrite equation (2) as\npÎ¸(wi|s) = exp (âŸ¨Î¸i,sâŸ©)âˆ‘V\nj=1 exp (âŸ¨Î¸j,sâŸ©)\nâˆ€iâˆˆ{1,Â·Â·Â· ,V }. (3)\nRNNLMs with a softmax output layer are typically trained using cross-entropy as the loss function,\nwhich is equivalent to maximum likelihood (ML) estimation, that is, to ï¬nd the model parameter Î¸\nwhich maximizes the log-likelihood of target word wi, given a history context s:\nJs\nml(Î¸) = log pÎ¸(wi|s), (4)\nwhose gradient is given by\nâˆ‚Js\nml(Î¸)\nâˆ‚Î¸ = âˆ‚\nâˆ‚Î¸ âŸ¨Î¸i,sâŸ©âˆ’\nVâˆ‘\nj=1\npÎ¸(wj|s) âˆ‚\nâˆ‚Î¸ âŸ¨Î¸j,sâŸ©,\n= âˆ‚\nâˆ‚Î¸ âŸ¨Î¸i,sâŸ©âˆ’EpÎ¸(w|s)\n[âˆ‚\nâˆ‚Î¸ âŸ¨Î¸w,sâŸ©\n]\n. (5)\nThe gradient of log-likelihood is expensive to evaluate because (1) the cost of computing pÎ¸(wj|s)\nis O(Vh) and (2) the summation above takes time linear in the vocabulary size O(V).\nTo alleviate the computational bottleneck of computing the gradient (5), we propose to use the\nfollowing discriminative objective function for training RNNLM:\nJs\ndisc(Î¸) = log ËœpÎ¸(wi|s) +\nâˆ‘\njâˆˆSK\nlog(1 âˆ’ËœpÎ¸(wj|s)), (6)\nwhere SK is a set of indices of K words drawn from the vocabulary, and i /âˆˆSK. Typically, K is a\ntiny fraction of V, and in our experiments we use K â‰ˆV/200. To generate SK we will sample K\nwords from the vocabulary using an easy to sample distribution Q(w), and set qj := 1\nQ(wj) in order\nto compute\nËœpÎ¸(wi|s) = qiexp (âŸ¨Î¸i,sâŸ©)\nqiexp (âŸ¨Î¸i,sâŸ©) + âˆ‘\njâˆˆSK\nqjexp (âŸ¨Î¸j,sâŸ©). (7)\nEquation 6 is the cost function of a standard logistic regression classiï¬er that discriminates one\npositive sample wi from K negative samples wj,âˆ€j âˆˆSK. The ï¬rst term in (6) corresponds to the\ntraditional maximum likelihood training, and the second term explicitly pushes down the probability\nof negative samples in addition to the implicit shrinkage enforced by the denominator of (7). In our\nexperiments, we found the discriminative training (6) outperforms the maximum likelihood training\n(the ï¬rst term of Eq. 6) in all the cases, with varying degree of accuracy improvement depending on\nK.\nThe weighted softmax function (7) can be considered as a stochastic version of the standard soft-\nmax (3) on a different base measure. While the standard softmax (3) uses a base measure which\ngives equal weights to all words, and has support over the entire vocabulary, the base measure used\nin (7) has support only on K+ 1 words: the target word wi and K samples from Q(w). The noise\nportion of (7) has the motivation from the sampling scheme, and the qi term for target word wi is\nintroduced mainly to balance the contributions from target word and noisy sample words. 1 Other\njustiï¬cations are discussed in Sec. 2.1 and Sec. 2.2, where we establish close connections between\nBlackOut, importance sampling, and noise contrastive estimation.\nDue to the weighted sampling property of BlackOut, some words might be sampled multiple times\naccording to the proposal distribution Q(w), and thus their indices may appear multiple times in\nSK. As wi is the target word, which is assumed to be included in computing (7), we therefore set\ni /âˆˆSK explicitly.\n1Itâ€™s shown empirically in our experiments that settingqi = 1in (7) hurts the accuracy signiï¬cantly.\n3\nPublished as a conference paper at ICLR 2016\nSubstituting (7) into (6) and letting uj = âŸ¨Î¸j,sâŸ©and Ëœpj = ËœpÎ¸(wj|s), we have\nJs\ndisc(Î¸) âˆui âˆ’(K+ 1) log\nâˆ‘\nkâˆˆ{i}âˆªSK\nqkexp(uk) +\nâˆ‘\njâˆˆSK\nlog\nï£«\nï£­ âˆ‘\nkâˆˆ{i}âˆªSK\nqkexp(uk) âˆ’qjexp(uj)\nï£¶\nï£¸.\n(8)\nThen taking derivatives with respect to uj,âˆ€j âˆˆ{i}âˆªSK, yields\nâˆ‚Js\ndisc(Î¸)\nâˆ‚ui\n= 1 âˆ’\nï£«\nï£­K+ 1 âˆ’\nâˆ‘\njâˆˆSK\n1\n1 âˆ’Ëœpj\nï£¶\nï£¸Ëœpi (9)\nâˆ‚Js\ndisc(Î¸)\nâˆ‚uj\n= âˆ’\nï£«\nï£­K+ 1 âˆ’\nâˆ‘\nkâˆˆSK\\{j}\n1\n1 âˆ’Ëœpk\nï£¶\nï£¸Ëœpj, for j âˆˆSK. (10)\nBy the chain rule of derivatives, we can propagate the errors backward to previous layers and com-\npute the gradients with respect to the full model parameters â„¦. In contrast to Eq. 5, Eqs. 9 and 10\nare much cheaper to evaluate as (1) the cost of computingËœpj is O(Kh) and (2) the summation takes\nO(K), hence roughly a V/K times of speed-up.\nNext we turn our attention to the proposal distribution Q(w). In the past, a uniform distribution\nor the unigram distribution have been advocated as promising candidates for sampling distributions\n(Bengio & Sen Â´ecal, 2003; Jean et al., 2015; Bengio & Sen Â´ecal, 2008; Mnih & Teh, 2012). As we\nwill see in the experiments, neither one is suitable for a wide range of datasets, and we ï¬nd that the\npower-raised unigram distribution of Mikolov et al. (2013) is very important in this context:\nQÎ±(w) âˆpÎ±\nuni(w), Î± âˆˆ[0,1]. (11)\nNote that QÎ±(w) is a generalization of uniform distribution (when Î±= 0) and unigram distribution\n(when Î± = 1). The rationale behind our choice is that by tuning Î±, one can interpolate smoothly\nbetween sampling popular words, as advocated by the unigram distribution, and sampling all words\nequally. The best Î± is typically dataset and/or problem dependent; in our experiments, we use a\nholdout set to ï¬nd the best value of Î±. Itâ€™s worth noting that this sampling strategy has been used by\nMikolov et al. (2013) in a similar context of word embedding, while here we explore its effect in the\nlanguage modeling applications.\nAfter BlackOut training, we evaluate the predictive performance of RNNLM by perplexity. To cal-\nculate perplexity, we explicitly normalize the output distribution by using the exact softmax func-\ntion (3). This is similar to DropOut (Srivastava et al., 2014), wherein a subset of network is sampled\nand trained at each training batch and when evaluating, the full network participates.\n2.1 C ONNECTION TO IMPORTANCE SAMPLING\nBlackOut has a close connection to importance sampling (IS). To see this, differentiating the loga-\nrithm of Eq. 7 with respect to model parameter Î¸, we have\nâˆ‚\nâˆ‚Î¸ log ËœpÎ¸(wi|s) = âˆ‚\nâˆ‚Î¸ âŸ¨Î¸i,sâŸ©âˆ’ 1âˆ‘\nkâˆˆ{i}âˆªSK\nqkexp(âŸ¨Î¸k,sâŸ©)\nâˆ‘\njâˆˆ{i}âˆªSK\nqjexp(âŸ¨Î¸j,sâŸ©) âˆ‚\nâˆ‚Î¸ âŸ¨Î¸j,sâŸ©\n= âˆ‚\nâˆ‚Î¸ âŸ¨Î¸i,sâŸ©âˆ’EËœpÎ¸(w|s)\n[âˆ‚\nâˆ‚Î¸ âŸ¨Î¸w,sâŸ©\n]\n. (12)\nIn contrast with Eq. 5, it shows that the weighted softmax function (7) corresponds to an IS-based\nestimator of the standard softmax (3) with a proposal distribution Q(w).\nImportance sampling has been applied to NNLMs with large output layers in previous works (Bengio\n& SenÂ´ecal, 2003; 2008; Jean et al., 2015). However, either uniform distribution or unigram distri-\nbution is used for sampling and all aforementioned works exploit the maximum likelihood learning\nof model parameter Î¸. By contrast, BlackOut uses a discriminative training (6) and a power-raised\nunigram distribution QÎ±(w) for sampling; these two changes are important to mitigate some of lim-\nitations of IS-based approaches. While an IS-based approach with a uniform proposal distribution\n4\nPublished as a conference paper at ICLR 2016\nis very stable for training, it suffers from large bias due to the apparent divergence of the uniform\ndistribution from the true data distribution pÎ¸(w|s). On the other hand, a unigram-based IS esti-\nmate can make learning unstable due to the high variance (Bengio & SenÂ´ecal, 2003; 2008). Using a\npower-raised unigram distribution QÎ±(w) entails a better trade-off between bias and variance, and\nthus strikes a better balance between these two extremes. In addition, as we will see from the experi-\nments, the discriminative training of BlackOut speeds up the rate of convergence over the traditional\nmaximum likelihood learning.\n2.2 C ONNECTION TO NOISE CONTRASTIVE ESTIMATION\nThe basic idea of NCE is to transform the density estimation problem to the problem of learning\nby comparison, e.g., estimating the parameters of a binary classiï¬er that distinguishes samples from\nthe data distribution pd from samples generated by a known noise distribution pn (Gutmann &\nHyvÂ¨arinen, 2012). In the language modeling setting, the data distribution pd will be the distribution\npÎ¸(w|s) of interest, and the noise distribution pn is often chosen from the ones that are easy to\nsample from and possibly close to the true data distribution (so that the classiï¬cation problem isnâ€™t\ntrivial). While Mnih & Teh (2012) uses a context-independent (unigram) noise distribution pn(w),\nBlackOut can be formulated into the NCE framework by considering a context-dependent noise\ndistribution pn(w|s), estimated from Ksamples drawn from Q(w), by\npn(wi|s) = 1\nK\nâˆ‘\njâˆˆSK\nqj\nqi\npÎ¸(wj|s), (13)\nwhich is a probability distribution function under the expectation that K samples are drawn from\nQ(w): SK âˆ¼Q(w) since ESKâˆ¼Q(w)(pn(wi|s)) = Q(wi) and ESKâˆ¼Q(w)(âˆ‘V\ni=1 pn(wi|s)) = 1\n(See the proof in Appendix A).\nSimilar to Gutmann & Hyv Â¨arinen (2012), noise samples are assumed K times more frequent than\ndata samples so that data points are generated from a mixture of two distributions: 1\nK+1 pÎ¸(w|s)\nand K\nK+1 pn(w|s). Then the conditional probability of sample wi being generated from the data\ndistribution is\npÎ¸(D= 1|wi,s) = pÎ¸(wi|s)\npÎ¸(wi|s) + Kpn(wi|s). (14)\nInserting Eq. 13 into Eq. 14, we have\npÎ¸(D= 1|wi,s) = qiexp(âŸ¨Î¸i,sâŸ©)\nqiexp(âŸ¨Î¸i,sâŸ©) + âˆ‘\njâˆˆSK\nqjexp(âŸ¨Î¸j,sâŸ©), (15)\nwhich is exactly the weighted softmax function deï¬ned in (7). Note that due to the noise distribution\nproposed in Eq. 13, the expensive denominator (or the partition function Z) of pÎ¸(wj|s) is canceled\nout, while in Mnih & Teh (2012) the partition function Z is either treated as a free parameter to be\nlearned or approximated by a constant. Mnih & Teh (2012) recommended to setZ = 1.0 in the NCE\ntraining. However, from our experiments, setting Z = 1.0 often leads to sub-optimal solutions2 and\ndifferent settings of Z sometimes incur numerical instability since the log-sum-exp trick 3 can not\nbe used there to shift the scores of the output layer to a range that is amenable to the exponential\nfunction. BlackOut does not have this hyper-parameter to tune and the log-sum-exp trick still works\nfor the weighted softmax function (7). Due to the discriminative training of NCE and BlackOut,\nthey share the same objective function (6).\nWe shall emphasize that according to the theory of NCE, theKsamples should be sampled from the\nnoise distribution pn(w|s). But in order to calculate pn(w|s), we need the K samples drawn from\nQ(w) beforehand. As an approximation, we use the same K samples drawn from Q(w) as the K\nsamples from pn(w|s), and only use the expression of pn(w|s) in (13) to evaluate the noise density\nvalue required by Eq. 14. This approximation is accurate since ESKâˆ¼Q(w)(pn(wi|s)) = Q(wi) as\nproved in Appendix A, and we ï¬nd empirically that it performs much better (with improved stability)\nthan using a unigram noise distribution as in Mnih & Teh (2012).\n2Similarly, Chen et al. (2015) reported that setting ln(Z) = 9gave them the best results.\n3https://en.wikipedia.org/wiki/LogSumExp\n5\nPublished as a conference paper at ICLR 2016\n2.3 R ELATED WORK\nMany approaches have been proposed to address the difï¬culty of training deep neural networks with\nlarge output spaces. In general, they can be categorized into four categories:\nâ€¢ Hierarchical softmax (Morin & Bengio, 2005; Mnih & Hinton, 2008) uses a hierarchical\nbinary tree representation of the output layer with the V words as its leaves. It allows ex-\nponentially faster computation of word probabilities and their gradients, but the predictive\nperformance of the resulting model is heavily dependent on the tree used, which is of-\nten constructed heuristically. Moreover, by relaxing the constraint of a binary structure, Le\net al. (2011) introduces a structured output layer with an arbitrary tree structure constructed\nfrom word clustering. All these methods speed up both the model training and evaluation\nconsiderably.\nâ€¢ Sampling-based approximations select at random or heuristically a small subset of the out-\nput layer and estimate gradient only from those samples. The use of importance sampling\nin Bengio & Sen Â´ecal (2003; 2008); Jean et al. (2015), and the use of NCE (Gutmann &\nHyvÂ¨arinen, 2012) in Mnih & Teh (2012) all fall under this category, so does the more\nrecent use of Locality Sensitive Hashing (LSH) techniques (Shrivastava & Li, 2014; Vi-\njayanarasimhan et al., 2014) to select a subset of good samples. BlackOut, with close con-\nnections to importance sampling and NCE, also falls in this category. All these approaches\nonly speed up the model training, while the model evaluation still remains computationally\nchallenging.\nâ€¢ Self normalization (Devlin et al., 2014) extends the cross-entropy loss function by explicitly\nencouraging the partition function of softmax to be as close to 1.0 as possible. Initially, this\napproach only speeds up the model evaluation and more recently itâ€™s extended to facilitate\nthe training as well with some theoretical guarantees (Andreas & Klein, 2014; Andreas\net al., 2015).\nâ€¢ Exact gradient on limited loss functions (Vincent et al., 2015) introduces an algorithmic\napproach to efï¬ciently compute the exact loss, gradient update for the output weights in\nO(h2) per training example instead of O(Vh). Unfortunately, it only applies to a lim-\nited family of loss functions that includes squared error and spherical softmax, while the\nstandard softmax isnâ€™t included.\nAs discussed in the introduction, BlackOut also shares some similarity to DropOut (Srivastava et al.,\n2014). While DropOut is often applied to input and/or hidden layers of deep neural networks to\navoid feature co-adaptation and overï¬tting by uniform sampling, BlackOut applies to a softmax\noutput layer, uses a weighted sampling, and employs a discriminative training loss. We chose the\nname BlackOut in light of the similarities between our method and DropOut, and the complementary\nthey offer to train deep neural networks.\n3 I MPLEMENTATION AND FURTHER SPEED -UP\nWe implemented BlackOut on a standard machine with a dual-socket 28-core Intel RâƒXeon Râƒ4\nHaswell CPU. To achieve high throughput, we train RNNLM with Back-Propagation Through Time\n(BPTT) (Rumelhart et al., 1988) with mini-batches (Chen et al., 2014). We use RMSProp (Hinton,\n2012) for learning rate scheduling and gradient clipping (Bengio et al., 2013) to avoid the gradi-\nent explosion issue of recurrent networks. We use the latest Intel MKL library (version 11.3.0) for\nSGEMM calls, which has improved support for tall-skinny matrix-matrix multiplications, which\nconsume about 80% of the run-time of RNNLMs.\nIt is expensive to access and update large models with billions of parameters. Fortunately, due to\nthe 1-of-V encoding at input layer and the BlackOut sampling at output layer, the model update on\nWin and Wout is sparse, i.e., only the model parameters corresponding to input/output words and\nthe samples in SK are updated at each training batch. However, subnet updates have to be done\ncarefully due to the dependency within RMSProp updating procedure. We therefore propose an\napproximated RMSProp that enables an efï¬cient subnet update and thus speeds up the algorithm\neven further. Details can be found in Appendix C.\n4Intel and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries.\n6\nPublished as a conference paper at ICLR 2016\n4 E XPERIMENTS\nIn our experiments, we ï¬rst compare BlackOut, NCE and exact softmax (without any approxima-\ntion) using a small dataset. We then evaluate the performance of BlackOut on the recently released\none billion word language modeling benchmark (Chelba et al., 2014) with a vocabulary size of up to\none million. We compare the performance of BlackOut on a standard CPU machine versus the state-\nof-the-arts reported in the literature that are achieved on GPUs or on clusters of CPU nodes. Our\nimplementation and scripts are open sourced at https://github.com/IntelLabs/rnnlm.\nCorpus Models are trained and evaluated on two different corpora: a small dataset provided by\nthe RNNLM Toolkit5, and the recently released one billion word language modeling benchmark 6,\nwhich is perhaps the largest public dataset in language modeling. The small dataset has 10,000\ntraining sentences, with 71,350 words in total and 3,720 unique words; and the test perplexity is\nevaluated on 1,000 test sentences. The one billion word benchmark was constructed from a mono-\nlingual/English corpora; after all necessary preprocessing including de-duplication, normalization\nand tokenization, 30,301,028 sentences (about 0.8 billion words) are randomly selected for training,\n6,075 sentences are randomly selected for test and the remaining 300,613 sentences are reserved for\nfuture development and can be used as holdout set.\n4.1 R ESULTS ON SMALL DATASET\nWe evaluate BlackOut, NCE and exact softmax (without any approximation) on the small dataset\ndescribed above. This small dataset is used so that we can train the standard RNNLM algorithm\nwith exact softmax within a reasonable time frame and hence to provide a baseline of expected\nperplexity. There are many other techniques involved in the training, such as RMSProp for learning\nrate scheduling (Hinton, 2012), subnet update (Appendix C), and mini-batch splicing (Chen et al.,\n2014), etc., which can affect the perplexity signiï¬cantly. For a fair comparison, we use the same\ntricks and settings for all the algorithms, and only evaluate the impact of the different approximations\n(or no approximation) on the softmax output layer. Moreover, there are a few hyper-parameters that\nhave strong impact on the predictive performance, including Î±of the proposal distribution QÎ±(w)\nfor BlackOut and NCE, and additionally for NCE, the partition functionZ. We pay an equal amount\nof effort to tune these hyper-parameters for BlackOut and NCE on the validation set as number of\nsamples increases.\nFigure 2 shows the perplexity reduction as a function of number of samples K under two different\nvocabulary settings: (a) a full vocabulary of 3,720 words, and (b) using the most frequent 2,065\nwords as vocabulary. The latter is a common approach used in practice to accelerate RNNLM\ncomputation by using RNNLM to predict only the most frequent words and handling the rest using\nan n-gram model (Schwenk & Gauvain, 2005). We will see similar vocabulary settings when we\nevaluate BlackOut on the large scale one billion word benchmark.\nAs can be seen, when the size of the samples increases, in general both BlackOut and NCE improve\ntheir prediction accuracy under the two vocabulary settings, and even with only 2 samples both al-\ngorithms still converge to reasonable solutions. BlackOut can utilize samples much more effectively\nthan NCE as manifested by the signiï¬cantly lower perplexities achieved by BlackOut, especially\nwhen number of samples is small; Given about 20-50 samples, BlackOut and NCE reach similar\nperplexities as the exact softmax, which is expensive to train as it requires to evaluate all the words\nin the vocabularies. When the vocabulary size is 2,065, BlackOut achieves even better perplexity\nthan that of the exact softmax. This is possible since BlackOut does stochastic sampling at each\ntraining example and uses the full softmax output layer in prediction; this is similar to DropOut that\nis routinely used in input layer and/or hidden layers of deep neural networks (Srivastava et al., 2014).\nAs in DropOut, BlackOut has the beneï¬t of regularization and avoids feature co-adaption and is pos-\nsibly less prone to overï¬tting. To verify this hypothesis, we evaluate the perplexities achieved on the\ntraining set for different algorithms and provide the results in Figure 5 at Appendix B. As can been\nseen, the exact softmax indeed overï¬ts to the training set and reaches lower training perplexities\nthan NCE and BlackOut.\n5http://www.rnnlm.org/\n6https://code.google.com/p/1-billion-word-language-modeling-benchmark/\n7\nPublished as a conference paper at ICLR 2016\n0 50 100 150 200\n60\n80\n100\n120\n140\n160\n180\n200\n220\n240\n260\nK\ntest perplexity\n \n \nNCE\nBlackOut\nExact Softmax\n0 50 100 150 200\n65\n70\n75\n80\n85\nK\ntest perplexity\n \n \nNCE\nBlackOut\nExact Softmax\nFigure 2: Test perplexity evolution as a function of number of samples K(a) with a full vocabulary\nof 3,720 words, and (b) with the most frequent 2,065 words in vocabulary. The experiments are\nexecuted on the RNNLMs with 16 hidden units.\nNext we compare the convergence rates of BlackOut and NCE when training the RNNLMs with 16\nhidden units for a full vocabulary of 3,720 words. Figures 3(a) and 3(b) plot the learning curves of\nBlackOut and NCE when 10 samples or 50 samples are used in training, respectively. The ï¬gure\nshows that BlackOut enjoys a much faster convergence rate than NCE, especially when number of\nsamples is small (Figure 3(a)); but this advantage gets smaller when number of samples increases\n(Figure 3(b)). We also observed similar behavior when we evaluated BlackOut and NCE on the\nlarge scale one billion word benchmark.\n0 10 20 30 40 50\n0\n2000\n4000\n6000\n8000\n10000\n12000\niteration\ntest perplexity\n \n \nNCE\nBlackOut\n0 10 20 30 40 50\n60\n80\n100\n120\n140\n160\n180\n200\n220\niteration\ntest perplexity\n \n \nNCE\nBlackOut\nFigure 3: The learning curves of BlackOut and NCE when training the RNNLMs with 16 hidden\nunits with (a) 10 samples, and (b) 50 samples.\n4.2 R ESULTS ON ONE BILLION WORD BENCHMARK\nWe follow the experiments from Williams et al. (2015) and Le et al. (2015) and compare the perfor-\nmance of BlackOut with the state-of-the-art results provided by them. While we evaluated Black-\nOut on a dual-socket 28-core Intel RâƒXeon RâƒHaswell machine, Williams et al. (2015) implemented\nRNNLM with the NCE approximation on NVIDIA GTX Titan GPUs, and Le et al. (2015) executed\nan array of recurrent networks, including deep RNN and LSTM, without approximation on a CPU\ncluster. Besides the time-to-solution comparison, these published results enable us to cross-check\nthe predictive performance of BlackOut with another implementation of NCE or with other compet-\nitive network architectures.\n4.2.1 W HEN VOCABULARY SIZE IS 64K\nFollowing the experiments in Williams et al. (2015), we evaluate the performance of BlackOut on\na vocabulary of 64K most frequent words. This is similar to the scenario in Figure 2(b) where the\n8\nPublished as a conference paper at ICLR 2016\nmost frequent words are kept in vocabulary and the rest rare words are mapped to a special <unk>\ntoken. We ï¬rst study the importance of Î±of the proposal distribution QÎ±(w) and the discrimina-\ntive training (6) as proposed in BlackOut. As we discussed in Sec. 2, when Î± = 0 , the proposal\ndistribution QÎ±(w) degenerates to a uniform distribution over all the words in the vocabulary, and\nwhen Î± = 1, we recover the unigram distribution. Thus, we evaluate the impact of Î±in the range\nof [0,1]. Figure 4(a) shows the evolution of test perplexity as a function of Î±for the RNNLMs with\n256 hidden units. As can be seen, Î±has a signiï¬cant impact on the prediction accuracy. The com-\nmonly used uniform distribution (when Î± = 0) and unigram distribution (when Î± = 1) often yield\nsub-optimal solutions. For the dataset and experiment considered, Î±= 0.4 gives the best perplexity\n(consistent on holdout set and test set). We therefore use Î± = 0.4 in the experiments that follow.\nThe number of samples used is 500, which is about 0.8% of the vocabulary size.\n0 0.2 0.4 0.6 0.8 1\n70\n80\n90\n100\n110\n120\n130\n140\nÎ±\ntest perplexity\n0 5 10 15 20\n65\n70\n75\n80\n85\n90\n95\n100\n105\niterations\ntest perplexity\n \n \nmaximum likelihood (Î± =0)\ndiscriminative (Î± =0)\nmaximum likelihood (Î± =0.4)\ndiscriminative (Î± =0.4)\nFigure 4: (a) The impact of Î±evaluated when 256 hidden units are used; (b) The learning curves of\nmaximum likelihood and discriminative training when 512 hidden units are used.\nFigure 4(b) demonstrates the impact of discriminative training (6) over the maximum likelihood\ntraining (the ï¬rst term of Eq. 6) on the RNNLMs with 512 hidden units using two different Î±â€™s. In\ngeneral, we observe 1-3 points of perplexity reduction due to discriminative training over traditional\nmaximum likelihood training.\nFinally, we evaluate the scalability of BlackOut when number of hidden units increases. As the\ndataset is large, we observed that the performance of RNNLM depends on the size of the hidden\nlayer: they perform better as the size of the hidden layer gets larger. As a truncated 64K word\nvocabulary is used, we interpolate the RNNLM scores with a full size 5-gram to ï¬ll in rare word\nprobabilities (Schwenk & Gauvain, 2005; Park et al., 2010). We report the interpolated perplexities\nBlackOut achieved and compare them with the results from Williams et al. (2015) in Table 1. As can\nbe seen, BlackOut reaches lower perplexities than those reported in Williams et al. (2015) within\ncomparable time frames (often 10%-40% faster). We achieved a perplexity of 42.0 when the hidden\nlayer size is 4096. To the best of our knowledge, this is the lowest perplexity reported on this\nbenchmark.\nTable 1: Performance on the one billion word benchmark by interpolating RNNLM on a 64K word\nvocabulary with a full-size KN 5-gram LM.\n#Params Test Perplexity Time to Solution\nModel [millions] Published1 BlackOut Published1 BlackOut\nKN 5-gram 1,748 66.95 45m\nRNN-128 + KN 5-gram 1,764 60.8 59.0 6h 9h\nRNN-256 + KN 5-gram 1,781 57.3 55.1 16h 14h\nRNN-512 + KN 5-gram 1,814 53.2 51.5 1d2h 1d\nRNN-1024 + KN 5-gram 1,880 48.9 47.6 2d2h 1d14h\nRNN-2048 + KN 5-gram 2,014 45.2 43.9 4d7h 2d15h\nRNN-4096 + KN 5-gram 2,289 42.4 42.0 14d5h 10d\n1Data from Table 1 of Williams et al. (2015).\n9\nPublished as a conference paper at ICLR 2016\n4.2.2 W HEN VOCABULARY SIZE IS 1M\nIn the ï¬nal set of experiments, we evaluate the performance of BlackOut with a very large vocabulary\nof 1,000,000 words, and the results are provided in Table 2. This is the largest vocabulary used on\nthis benchmark that we could ï¬nd in existing literature. We consider the RNNLM with 1,024 hidden\nunits (about 2 billion parameters) and 2,048 hidden units (about 4.1 billion parameters) and compare\ntheir test perplexities with the results from Le et al. (2015). We use 2,000 samples, 0.2% of the\nvocabulary size, for BlackOut training with Î± = 0.1. Comparing to the experiments with the 64K\nword vocabulary, a much smaller Î±is used here since the sampling rate (0.2%) is much lower than\nthat is used (0.8%) when the vocabulary size is 64K, and a smallerÎ±strikes a better balance between\nsample coverage per training example and convergence rate. In contrast, NCE with the same setting\nconverges very slowly (similar to Figure 3(a)) and couldnâ€™t reach a competitive perplexity within\nthe time frame considered, and its results are not reported here.\nAs the standard RNN/LSTM algorithms (without approximation) are used in Le et al. (2015), a\ncluster of 32 CPU machines (at least 20 cores each) are used to train the models for about 60\nhours. BlackOut enables us to train this large model using a single CPU machine for 175 hours.\nSince different model architectures are used in the experiments (deep RNN/LSTM vs. standard\nRNNLM), the direct comparison of test perplexity isnâ€™t very meaningful. However, this experiment\ndemonstrates that even though our largest model is about 2-3 times larger than the models evaluated\nin Le et al. (2015), BlackOut, along with a few other optimization techniques, make this large scale\nlearning problem still feasible on a single box machine without using GPUs or CPU clusters.\nTable 2: Performance on the one billion word benchmark with a vocabulary of 1,000,000 words.\nSingle model (RNN/LSTM-only) perplexities are reported; no interpolation is applied to any models.\nModel Perplexity\nResults from LSTM (512 units) 68.8\nLe et al. (2015) IRNN (4 layers, 512 units) 69.4\n60 hours IRNN (1 layer, 1024 units + 512 linear units) 70.2\n32 machines RNN (4 layers, 512 tanh units) 71.8\nRNN (1 layer, 1024 tanh units + 512 linear units) 72.5\nOur Results RNN (1 layer, 1024 sigmoid units) 78.4\n175 hours, 1 machine RNN (1 layer, 2048 sigmoid units) 68.3\nLast, we collect all the state of the art results we are aware of on this benchmark and summarize\nthem in Table 3. Since all the models are the interpolated ones, we interpolate our best RNN model7\nfrom Table 2 with the KN 5-gram model and achieve a perplexity score of 47.3. Again, different\npapers provide their best models trained with different architectures and vocabulary settings. Hence,\nan absolutely fair comparison isnâ€™t possible. Regardless of these discrepancies, our models, within\ndifferent groups of vocabulary settings, are very competitive in terms of prediction accuracy and\nmodel size.\nTable 3: Comparison with the state of the art results reported on the one billion word benchmark.\nModel #Params [billions] Test Perplexity\nRNN-1024 (full vocab) + MaxEnt1 20 51.3\nRNN-2048 (full vocab) + KN 5-gram2 5.0 47.3\nRNN-1024 (full vocab) + MaxEnt + 3 models1 42.9 43.8\nRNN-4096 (64K vocab) + KN 5-gram3 2.3 42.4\nRNN-4096 (64K vocab) + KN 5-gram2 2.3 42.0\n1Data from Chelba et al. (2014); 2Our results; 3Data from Williams et al. (2015).\n7To be consistent with the benchmark in Chelba et al. (2014), we retrained it with the full-size vocabulary\nof about 0.8M words.\n10\nPublished as a conference paper at ICLR 2016\n5 C ONCLUSION\nWe proposedBlackOut, a sampling-based approximation, to train RNNLMs with very large vocabu-\nlaries (e.g., 1 million). We established its connections to importance sampling and noise contrastive\nestimation (NCE), and demonstrated its stability, sample efï¬ciency and rate of convergence on the\nrecently released one billion word language modeling benchmark. We achieved the lowest reported\nperplexity on this benchmark without using GPUs or CPU clusters.\nAs for future extensions, our plans include exploring other proposal distributions Q(w), and theo-\nretical properties of the generalization property and sample complexity bounds for BlackOut. We\nwill also investigate a multi-machine distributed implementation.\nACKNOWLEDGMENTS\nWe would like to thank Oriol Vinyals, Andriy Mnih and the anonymous reviewers for their excellent\ncomments and suggestions, which helped improve the quality of this paper.\nREFERENCES\nAndreas, Jacob and Klein, Dan. When and why are log-linear models self-normalizing? InProceed-\nings of the Annual Meeting of the North American Chapter of the Association for Computational\nLinguistics, 2014.\nAndreas, Jacob, Rabinovich, Maxim, Klein, Dan, and Jordan, Michael I. On the accuracy of self-\nnormalized log-linear models. In NIPS, 2015.\nBengio, Yoshua and SenÂ´ecal, Jean-SÂ´ebastien. Quick training of probabilistic neural nets by impor-\ntance sampling. In AISTATS, 2003.\nBengio, Yoshua and SenÂ´ecal, Jean-SÂ´ebastien. Adaptive importance sampling to accelerate training\nof a neural probabilistic language model. In IEEE Transactions on Neural Networks, volume 19,\npp. 713â€“722, 2008.\nBengio, Yoshua, Ducharme, Rjean, and Vincent, Pascal. A neural probabilistic language model. In\nNIPS, pp. 932â€“938, 2001.\nBengio, Yoshua, Boulanger-Lewandowski, Nicolas, and Pascanu, Razvan. Advances in optimizing\nrecurrent networks. In ICASSP, pp. 8624â€“8628, 2013.\nChelba, Ciprian, Mikolov, Tomas, Schuster, Mike, Ge, Qi, Brants, Thorsten, Koehn, Phillipp, and\nRobinson, Tony. One billion word benchmark for measuring progress in statistical language\nmodeling. In INTERSPEECH, pp. 2635â€“2639, 2014.\nChen, Xie, Wang, Yongqiang, Liu, Xunying, Gales, Mark JF, and Woodland, Philip C. Efï¬cient\ngpu-based training of recurrent neural network language models using spliced sentence bunch. In\nINTERSPEECH, 2014.\nChen, Xie, Liu, Xunying, Gales, Mark JF, and Woodland, Philip C. Recurrent neural network\nlanguage model training with noise contrastive estimation for speech recognition. In ICASSP,\n2015.\nDevlin, Jacob, Zbib, Rabih, Huang, Zhongqiang, Lamar, Thomas, Schwartz, Richard, and Makhoul,\nJohn. Fast and robust neural network joint models for statistical machine translation. In ACL,\n2014.\nGutmann, Michael U. and HyvÂ¨arinen, Aapo. Noise-contrastive estimation of unnormalized statisti-\ncal models, with applications to natural image statistics. JMLR, 13:307â€“361, 2012.\nHinton, Geoffrey. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent\nmagnitude. COURSERA: Neural Networks for Machine Learning, 2012.\n11\nPublished as a conference paper at ICLR 2016\nJean, Sbastien, Cho, Kyunghyun, Memisevic, Roland, and Bengio, Yoshua. On using very large\ntarget vocabulary for neural machine translation. In ACL, 2015.\nLe, Hai-Son, Oparin, Ilya, Allauzen, Alexandre, Gauvain, Jean-Luc, and Yvon, Franc Â¸ois. Structured\noutput layer neural network language model. In ICASSP, pp. 5524â€“5527, 2011.\nLe, Quoc V ., Jaitly, Navdeep, and Hinton, Geoffrey. A simple way to initialize recurrent networks\nof rectiï¬ed linear units. arXiv preprint arxiv:1504.00941, 2015.\nMikolov, Tomas, Karaï¬Â´at, Martin, Burget, Luk Â´as, CernockÂ´y, Jan, and Khudanpur, Sanjeev. Recur-\nrent neural network based language model. In INTERSPEECH, pp. 1045â€“1048, 2010.\nMikolov, Tomas, Deoras, Anoop, Povey, Dan, Burget, Lukar, and Cernocky, Jan Honza. Strategies\nfor training large scale neural network language models. IEEE Automatic Speech Recognition\nand Understanding Workshop, 2011.\nMikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Distributed rep-\nresentations of words and phrases and their compositionality. In Burges, Chris, Bottou, Leon,\nWelling, Max, Ghahramani, Zoubin, and Weinberger, Kilian (eds.),Advances in Neural Informa-\ntion Processing Systems 26, 2013.\nMnih, Andriy and Hinton, Geoffrey E. A scalable hierarchical distributed language model. InNIPS,\nvolume 21, pp. 1081â€“1088, 2008.\nMnih, Andriy and Teh, Yee Whye. A fast and simple algorithm for training neural probabilistic\nlanguage models. In Proceedings of the International Conference on Machine Learning, 2012.\nMorin, Frederic and Bengio, Yoshua. Hierarchical probabilistic neural network language model. In\nProceedings of the international workshop on artiï¬cial intelligence and statistics , pp. 246â€“252.\nCiteseer, 2005.\nPark, Junho, Liu, Xunying, Gales, Mark J. F., and Woodland, P. C. Improved neural network based\nlanguage modelling and adaptation. In Proc. ISCA Interspeech, pp. 10411044, 2010.\nRumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. Neurocomputing: Foundations\nof research. chapter Learning Representations by Back-propagating Errors, pp. 696â€“699. MIT\nPress, Cambridge, MA, USA, 1988.\nSchwenk, Holger and Gauvain, Jean-Luc. Training neural network language models on very large\ncorpora. In Proceedings of Human Language Technology Conference and Conference on Empir-\nical Methods in Natural Language Processing, pp. 201â€“208, 2005.\nShrivastava, Anshumali and Li, Ping. Asymmetric lsh (alsh) for sublinear time maximum inner\nproduct search (mips). In NIPS, volume 27, pp. 2321â€“2329, 2014.\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan.\nDropout: A simple way to prevent neural networks from overï¬tting. JMLR, 15:1929â€“1958, 2014.\nSundermeyer, Martin, Oparin, Ilya, Gauvain, Jean-Luc, Freiberg, Ben, Schl\nâ€uter, Ralf, and Ney, Hermann. Comparison of feedforward and recurrent neural network lan-\nguage models. In ICASSP, pp. 8430â€“8434, 2013.\nVijayanarasimhan, Sudheendra, Shlens, Jonathon, Monga, Rajat, and Yagnik, Jay. Deep networks\nwith large output spaces. arXiv preprint arxiv:1412.7479, 2014.\nVincent, Pascal, de Brbisson, Alexandre, and Bouthillier, Xavier. Efï¬cient exact gradient update for\ntraining deep networks with very large sparse targets. In NIPS, 2015.\nWilliams, Will, Prasad, Niranjani, Mrva, David, Ash, Tom, and Robinson, Tony. Scaling recurrent\nneural network language models. In ICASSP, 2015.\n12\nPublished as a conference paper at ICLR 2016\nBlackOut: Speeding up Recurrent Neural Network Language\nModels with Very Large V ocabularies\n(Supplementary Material)\nA N OISE DISTRIBUTION pn(wi|s)\nTheorem 1 The noise distribution function pn(wi|s) deï¬ned in Eq. 13 is a probability distribution\nfunction under the expectation thatKsamples in SK are drawn fromQ(w) randomly, SK âˆ¼Q(w),\nsuch that ESKâˆ¼Q(w)(pn(wi|s)) = Q(wi) and ESKâˆ¼Q(w)(âˆ‘V\ni=1 pn(wi|s)) = 1.\nProof\nESKâˆ¼Q(w)(pn(wi|s)) = ESKâˆ¼Q(w)\nï£«\nï£­1\nK\nâˆ‘\njâˆˆSK\nqj\nqi\npÎ¸(wj|s)\nï£¶\nï£¸\n= Q(wi)\nK ESKâˆ¼Q(w)\nï£«\nï£­âˆ‘\njâˆˆSK\npÎ¸(wj|s)\nQ(wj)\nï£¶\nï£¸\n= Q(wi)\nK\nâˆ‘\nwk,âˆ€kâˆˆSK\nï£«\nï£­ âˆ\nkâˆˆSK\nQ(wk) Â·\nâˆ‘\njâˆˆSK\npÎ¸(wj|s)\nQ(wj)\nï£¶\nï£¸\n= Q(wi)\nK K\n= Q(wi)\nESKâˆ¼Q(w)\n( Vâˆ‘\ni=1\npn(wi|s)\n)\n=\nVâˆ‘\ni=1\nESKâˆ¼Q(w) (pn(wi|s)) =\nVâˆ‘\ni=1\n(Q(wi)) = 1\nB P ERPLEXITIES ON TRAINING SET\n0 50 100 150 200\n40\n60\n80\n100\n120\n140\n160\n180\n200\n220\n240\nK\ntrain perplexity\n \n \nNCE\nBlackOut\nExact Softmax\n0 50 100 150 200\n45\n50\n55\n60\n65\n70\n75\n80\nK\ntrain perplexity\n \n \nNCE\nBlackOut\nExact Softmax\nFigure 5: Training perplexity evolution as a function of number of samples K(a) with a full vocab-\nulary of 3,720 words, and (b) with the most frequent 2,065 words in vocabulary. The experiments\nare executed on the RNNLMs with 16 hidden units.\n13\nPublished as a conference paper at ICLR 2016\nC S UBNET UPDATE WITH APPROXIMATED RMSP ROP\nRMSProp (Hinton, 2012) is an adaptive learning rate method that has found much success in prac-\ntice. Instead of using a single learning rate to all the model parameters in â„¦, RMSProp dedicates\na learning rate for each model parameter and normalizes the gradient by an exponential moving\naverage of the magnitude of the gradient:\nvt = Î²vtâˆ’1 + (1 âˆ’Î²)(âˆ‡J)2 (16)\nwhere Î² âˆˆ(0,1) denotes the decay rate. The model update at time step tis then given by\nÎ¸t = Î¸tâˆ’1 + Ïµâˆ‡J(Î¸tâˆ’1)âˆšvt + Î» (17)\nwhere Ïµis the learning rate and Î»is a damping factor, e.g., Î» = 10âˆ’6. While RMSProp is one of\nthe most effective learning rate scheduling techniques, it requires a large amount of memory to store\nper-parameter vt in addition to model parameter â„¦ and their gradients.\nIt is expensive to access and update large models with billions of parameters. Fortunately, due to\nthe 1-of-V encoding at input layer and the BlackOut sampling at output layer, the model update on\nWin and Wout is sparse, e.g., only the model parameters corresponding to input/output words and\nthe samples in SK are to be updated.8 For Eq. 16, however, even a model parameter is not involved\nin the current training, its vt value still needs to be updated by vt = Î²vtâˆ’1 since its (âˆ‡J)2 = 0.\nIgnoring this update has detrimental effect on the predictive performance; in our experiments, we\nobserved 5 âˆ’10 point perplexity loss if we ignore this update completely.\nWe resort to an approximation to vt = Î²vtâˆ’1. Given pu(w) is the probability of a word w being\nselected for update, the number of time steps elapsed when it is successfully selected follows a\ngeometric distribution with a success rate pu(w), whose mean value is 1/pu(w). Assume that an\ninput/output word is selected according to the unigram distribution puni(w) and the samples in SK\nare drawn from QÎ±(w), Eq. 16 can be approximated by\nvt â‰ˆÎ²1/puvtâˆ’n + (1 âˆ’Î²)(âˆ‡J)2 (18)\nwith\npu(w) =\n{puni(w) Ã—BÃ—T for word wat input layer\npuni(w) Ã—BÃ—T + QÎ±(w) Ã—KÃ—T for word wat output layer, (19)\nwhere B is the mini-batch size and T is the BPTT block size. Now we can only update the model\nparameters, typically a tiny fraction of â„¦, that are really involved in the current training, and thus\nspeed up the RNNLM training further.\n8The parameter update on Wr is still dense, but its size is several orders of magnitude smaller than those of\nWin and Wout.\n14",
  "topic": "Blackout",
  "concepts": [
    {
      "name": "Blackout",
      "score": 0.9483349919319153
    },
    {
      "name": "Softmax function",
      "score": 0.8712934851646423
    },
    {
      "name": "Computer science",
      "score": 0.8104742169380188
    },
    {
      "name": "Perplexity",
      "score": 0.7826212644577026
    },
    {
      "name": "Discriminative model",
      "score": 0.577465832233429
    },
    {
      "name": "Scalability",
      "score": 0.5489159822463989
    },
    {
      "name": "Vocabulary",
      "score": 0.5289530754089355
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5089561939239502
    },
    {
      "name": "Word (group theory)",
      "score": 0.48709988594055176
    },
    {
      "name": "Language model",
      "score": 0.48291417956352234
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.4472443163394928
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4380345344543457
    },
    {
      "name": "Artificial neural network",
      "score": 0.43800824880599976
    },
    {
      "name": "Machine learning",
      "score": 0.4248034358024597
    },
    {
      "name": "Mathematics",
      "score": 0.06852275133132935
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Electric power system",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}