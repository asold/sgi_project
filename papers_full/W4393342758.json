{
    "title": "PLMSearch: Protein language model powers accurate and fast sequence search for remote homology",
    "url": "https://openalex.org/W4393342758",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A1973321923",
            "name": "Wei Liu",
            "affiliations": [
                "Shanghai Institute for Science of Science",
                "Fudan University",
                "Shanghai Center for Brain Science and Brain-Inspired Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2198031327",
            "name": "Wang Ziye",
            "affiliations": [
                "Fudan University",
                "Shanghai Center for Brain Science and Brain-Inspired Technology",
                "Shanghai Institute for Science of Science"
            ]
        },
        {
            "id": "https://openalex.org/A2227862674",
            "name": "Ronghui You",
            "affiliations": [
                "Shanghai Institute for Science of Science",
                "Shanghai Center for Brain Science and Brain-Inspired Technology",
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2512551316",
            "name": "Chenghan Xie",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2010640023",
            "name": "Hong Wei",
            "affiliations": [
                "Nankai University"
            ]
        },
        {
            "id": "https://openalex.org/A2102660201",
            "name": "Yi Xiong",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2111635996",
            "name": "Jianyi Yang",
            "affiliations": [
                "Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A2099759720",
            "name": "Shanfeng Zhu",
            "affiliations": [
                "Shanghai Institute for Science of Science",
                "Shanghai Innovative Research Center of Traditional Chinese Medicine",
                "ShangHai JiAi Genetics & IVF Institute",
                "Shanghai Center for Brain Science and Brain-Inspired Technology",
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A1973321923",
            "name": "Wei Liu",
            "affiliations": [
                "Shanghai Center for Brain Science and Brain-Inspired Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2198031327",
            "name": "Wang Ziye",
            "affiliations": [
                "Shanghai Center for Brain Science and Brain-Inspired Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2227862674",
            "name": "Ronghui You",
            "affiliations": [
                "Shanghai Center for Brain Science and Brain-Inspired Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2512551316",
            "name": "Chenghan Xie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2010640023",
            "name": "Hong Wei",
            "affiliations": [
                "Nankai University"
            ]
        },
        {
            "id": "https://openalex.org/A2102660201",
            "name": "Yi Xiong",
            "affiliations": [
                "Shanghai Center For Bioinformation Technology",
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2111635996",
            "name": "Jianyi Yang",
            "affiliations": [
                "Shandong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2099759720",
            "name": "Shanfeng Zhu",
            "affiliations": [
                "Fudan University",
                "Shanghai Innovative Research Center of Traditional Chinese Medicine",
                "Shanghai Center for Brain Science and Brain-Inspired Technology",
                "ShangHai JiAi Genetics & IVF Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3165795318",
        "https://openalex.org/W2951282333",
        "https://openalex.org/W3179436811",
        "https://openalex.org/W4200166788",
        "https://openalex.org/W4220991280",
        "https://openalex.org/W4366163632",
        "https://openalex.org/W3134169372",
        "https://openalex.org/W3022179362",
        "https://openalex.org/W2950954328",
        "https://openalex.org/W2055043387",
        "https://openalex.org/W3143063265",
        "https://openalex.org/W2808291890",
        "https://openalex.org/W2138122982",
        "https://openalex.org/W2145268834",
        "https://openalex.org/W2051210555",
        "https://openalex.org/W2972411752",
        "https://openalex.org/W2097270746",
        "https://openalex.org/W2574496196",
        "https://openalex.org/W2605650084",
        "https://openalex.org/W3202243296",
        "https://openalex.org/W2049540991",
        "https://openalex.org/W2108361377",
        "https://openalex.org/W4375858802",
        "https://openalex.org/W2152326664",
        "https://openalex.org/W3003287532",
        "https://openalex.org/W2161151688",
        "https://openalex.org/W2102245393",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W3186179742",
        "https://openalex.org/W3211795435",
        "https://openalex.org/W6769989246",
        "https://openalex.org/W4214886481",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3179485843",
        "https://openalex.org/W4327550249",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W6763868836",
        "https://openalex.org/W4213112325",
        "https://openalex.org/W3095583226",
        "https://openalex.org/W4316926589",
        "https://openalex.org/W3106745904",
        "https://openalex.org/W4282984452",
        "https://openalex.org/W4311661986",
        "https://openalex.org/W4386496033",
        "https://openalex.org/W4386860638",
        "https://openalex.org/W2087064593",
        "https://openalex.org/W2074231493",
        "https://openalex.org/W2101220662",
        "https://openalex.org/W2115540209",
        "https://openalex.org/W2121125726",
        "https://openalex.org/W2044363505",
        "https://openalex.org/W4310184849",
        "https://openalex.org/W2161072217",
        "https://openalex.org/W3215918380",
        "https://openalex.org/W2130479394",
        "https://openalex.org/W3104537585",
        "https://openalex.org/W3159719254",
        "https://openalex.org/W2040233577",
        "https://openalex.org/W2107758049",
        "https://openalex.org/W2081771568",
        "https://openalex.org/W6907496296",
        "https://openalex.org/W6902228163",
        "https://openalex.org/W2984894304",
        "https://openalex.org/W4393342758",
        "https://openalex.org/W4309506674",
        "https://openalex.org/W2971227267",
        "https://openalex.org/W4388024559"
    ],
    "abstract": "Abstract Homologous protein search is one of the most commonly used methods for protein annotation and analysis. Compared to structure search, detecting distant evolutionary relationships from sequences alone remains challenging. Here we propose PLMSearch ( P rotein L anguage M odel), a homologous protein search method with only sequences as input. PLMSearch uses deep representations from a pre-trained protein language model and trains the similarity prediction model with a large number of real structure similarity. This enables PLMSearch to capture the remote homology information concealed behind the sequences. Extensive experimental results show that PLMSearch can search millions of query-target protein pairs in seconds like MMseqs2 while increasing the sensitivity by more than threefold, and is comparable to state-of-the-art structure search methods. In particular, unlike traditional sequence search methods, PLMSearch can recall most remote homology pairs with dissimilar sequences but similar structures. PLMSearch is freely available at https://dmiip.sjtu.edu.cn/PLMSearch .",
    "full_text": "Article https://doi.org/10.1038/s41467-024-46808-5\nPLMSearch: Protein language model powers\naccurate and fast sequence search for remote\nhomology\nWei Liu 1,Z i y eW a n g1, Ronghui You1, Chenghan Xie2, Hong Wei3, Yi Xiong 4,\nJianyi Yang 5 & Shanfeng Zhu 1,6,7,8,9\nHomologous protein search is one of the most commonly used methods for\nprotein annotation and analysis. Compared to structure search, detecting\ndistant evolutionary relationships fromsequences alone remains challenging.\nHere we propose PLMSearch (Protein Language Model), a homologous pro-\ntein search method with only sequences as input. PLMSearch uses deep\nrepresentations from a pre-trained protein language model and trains the\nsimilarity prediction model with a large number of real structure similarity.\nThis enables PLMSearch to capture theremote homology information con-\ncealed behind the sequences. Extensive experimental results show that\nPLMSearch can search millions of query-target protein pairs in seconds like\nMMseqs2 while increasing the sensitivity by more than threefold, and is\ncomparable to state-of-the-art structure search methods. In particular, unlike\ntraditional sequence search methods, PLMSearch can recall most remote\nhomology pairs with dissimilar sequences but similar structures. PLMSearch is\nfreely available athttps://dmiip.sjtu.edu.cn/PLMSearch.\nHomologous protein search is a key component of bioinformatics\nmethods used in protein function prediction1– 6,p r o t e i n– protein inter-\naction prediction7, and protein-phenotype association prediction8.T h e\ngoal of homologous protein search is, for each query protein, homo-\nlogous proteins from the target dataset (generally a large-scale standard\ndataset like Swiss-Prot9) are needed to be found. The target protein with\na higher probability of homology should be ranked higher. According to\nthe type of input data, homologous protein search can be divided into\nsequence search and structure search.\nD u et ot h el o wc o s ta n dl a r g es c a l eo fs e q u e n c ed a t a ,t h em o s t\nwidely used homologous protein search methods are based on\nsequence similarity, such as MMseqs2\n10,B L A S T p11,a n dD i a m o n d12.\nDespite the success of homology inference based on sequence simi-\nlarity, it remains challenging to detect distant evolutionary relationships\nfrom sequences only\n13. Sequence proﬁles and proﬁle hidden Markov\nmodels (HMMs) are condensed representations of multiple sequence\nalignment (MSAs), which specify for each position the probability of\nobserving each of the 20 amino acids in evolutionarily related proteins.\nWhen the sequence identity is lower than 0.3, methods based on proﬁle\nHMMs such as HMMER\n14, HHsearch15,a n dH H b l i t s16,17 are better tools for\nhomologous protein search.\nIn scenarios involving highly distant evolutionary relationships,\nsequences may have diverged to such an extent that detecting their\nrelatedness becomes challenging. Since structures diverge much more\nReceived: 28 May 2023\nAccepted: 8 March 2024\nCheck for updates\n1Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science, Fudan University, 200433 Shanghai,C h i n a .\n2School of Mathematical Sciences, Fudan University, 200433 Shanghai, China.3School of Mathematical Sciences, Nankai University, 300071 Tianjin, China.\n4Department of Bioinformatics and Biostatistics, Shanghai Jiao Tong University, 200240 Shanghai, China.5Ministry of Education Frontiers Science Center for\nNonlinear Expectations, Research Center for Mathematics and Interdisciplinary Science, Shandong University, 266237 Qingdao, China.6Shanghai Qi Zhi\nInstitute, Shanghai, China.7Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence (Fudan University), Ministry of Education,\nShanghai, China.8Shanghai Key Lab of Intelligent Information Processing and Shanghai Institute of Artiﬁcial Intelligence Algorithm, Fudan University,\nShanghai, China.9Zhangjiang Fudan International Innovation Center, Shanghai, China.e-mail: yangjy@sdu.edu.cn; zhusf@fudan.edu.cn\nNature Communications|         (2024) 15:2775 1\n1234567890():,;\n1234567890():,;\nslowly than sequences, detecting similarity between protein structures\nby 3D superposition provides higher sensitivity18. Protein structure\nsearch methods can be divided into (1) contact/distance map-based,\nsuch as Map_align\n19,E i g e n T H R E A D E R20, and DiscoVER21; (2) structural\nalphabet-based, such as 3D-BLAST-SW22, CLE-SW23,F o l d s e e k ,a n d\nFoldseek-TM24; (3) structural alignment-based, such as CE25,D a l i26,a n d\nTM-align27,28. Protein structure prediction methods (like AlphaFold2)\nand AlphaFold Protein Structure Database (AFDB) have greatly\nreduced the cost of obtaining protein structures29– 31, which expands\nthe usage scenarios of the structure search methods. However, in the\nvast majority of cases, the sequence search method is still faster and\nmore convenient. This is notably evident in scenarios involving a large\nnumber of new sequences, such as metagenomic sequences\n32,\nsequences generated by protein engineering33, and antibody variant\nsequences34.\nAt the same time, protein language models (PLMs) such as\nESMs35– 37 and ProtTrans38 only take protein sequences as input, trained\non hundreds of millions of unlabeled protein sequences using self-\nsupervised tasks such as masked amino acid prediction. PLMs perform\nwell in various downstream tasks\n39, especially in structure-related tasks\nlike secondary structure prediction and contact prediction40.M o r e\nrecently, ProtENN41 uses an ensemble deep learning framework that\ngenerated protein sequence embeddings to classify protein domains\ninto Pfam families42;C A T H e43 trains an ANN on embeddings from the\nPLM ProtT538 to detect remote homologs for CATH44 superfamilies;\nEmbedding-based annotation transfer (EAT)45 uses Euclidean distance\nbetween vector representations (initialized from ProtT5 embeddings)\nof proteins to transfer annotations from a set of labeled lookup protein\nembeddings to query protein embeddings; DEDAL\n46,D e e p B L A S T47,\nand latest pLM-BLAST48 obtain a continuous representation of protein\nsequences that, combined with the Smith-Waterman (SW) 49 or\nNeedleman-Wunsch (NW)50 algorithm, leads to a more accurate pair-\nwise sequence alignment and homology detection method. These\nmethods apply representations generated by deep learning models to\nprotein domain classiﬁcation, protein annotation, and pairwise\nsequence alignment, fully demonstrating the advantage of deep\nlearning models in identifying remote homology. However, protein\nlanguage models are not fully utilized for the large-scale protein\nsequence search.\nTo improve the sensitivity while maintaining the universality and\nefﬁciency of sequence search, we propose PLMSearch (Fig.1a-c).\nPLMSearch mainly consists of the following three steps: (1) PfamClan\nﬁlters out protein pairs that share the same Pfam clan domain\n42.\n(2) SS-predictor (StructuralSimilarity predictor) predicts the similarity\nM amino acids\nQuery\nProtein Language Model\nNW\nTarget\nN amino acids\nd PLMAlign\nPer-residue embeddings\nSubs/g415tu/g415on matrix\nGet pairs\na PfamClan b Similarity predic/g415on\nQueries TargetsQuery 1\nQuery 2\nQuery 3\nPfamScan\nQuery 1 Query 3 Query 2 Similarity\n0.5\n2\n0.9\n1\n0.7\n1\n0.4\n2\n0.6\n1\n-0.2\n4\n0.3\n2\n-0.1\n3\n-0.3\n5\nProtein pairs\nOutput\nQueries\nMSMSQPTETVSDAP…\nFor\nall pairs\nTargets\nMEEEEDVNFKPGSN…\nProtein Language Model\nAverage Pooling Layer\nSS-predictor\nQuery Embeddings\n Target Embeddings\nSimilarity\nc Search result\n0.60.9 0.5 -0.2 0.7 0.3 0.4 -0.1 -0.3\nFig. 1 | Overview of the PLMSearch pipeline. aPfamClan. Initially, PfamScan54\nidentiﬁes the Pfam clan domains of the query protein sequences, which are\ndepicted in different color blocks. Subsequently, PfamClan searches the target\ndataset for proteins sharing the same Pfam clan domain with the query proteins.\nNotably, the last query protein lacks any Pfam clan domain, and therefore, its all\npairs with target proteins are retained.b Similarity prediction. The protein lan-\nguage model generates deep sequence embeddings for query and target proteins.\nSubsequently, SS-predictor predicts the similarity of all query-target pairs.c Search\nresult. Finally, PLMSearch selects the similarity of the protein pairs pre-ﬁltered by\nPfamClan, sorts these protein pairs based on their predicted similarity, and outputs\nthe search results for each query protein separately.d PLMAlign. PLMAlign utilizes\nper-residue embeddings as input to compute a substitution matrix. This substitu-\ntion matrix is then employed to replace the static substitution matrix in the Smith-\nWaterman (SW)\n49 or Needleman-Wunsch (NW)50 algorithm, enabling the local or\nglobal sequence alignment. The global alignment is illustrated in theﬁgure, where\nthe length of the query protein is 105, the length of the target protein is 123, and the\nembedding dimension of ProtT5-XL-UniRef50 used by PLMAlign is 1024.\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 2\nbetween all query-target pairs with embeddings generated by the pro-\ntein language model. PLMSearch willnot lose much sensitivity without\nstructures as input, because it uses the protein language model to\ncapture remote homology information from deep sequence embed-\ndings. In addition, the SS-predictorused in this step uses the structural\nsimilarity (TM-score) as the ground truth for training. This allows\nPLMSearch to acquire reliable similarity even without structures as\ninput. (3) PLMSearch sorts the pairs pre-ﬁltered by PfamClan based on\ntheir predicted similarity and outputs the search results for each query\nprotein accordingly. Subsequently, PLMAlign provides sequence align-\nments and alignment scores for top-ranked protein pairs retrieved by\nPLMSearch (Fig.1d). Search tests on SCOPe40-test and Swiss-Prot reveal\nthat PLMSearch is always one of the best methods and provides the best\ntradeoff between accuracy and speed. Speciﬁcally, PLMSearch can\nsearch millions of query-target protein pairs in seconds like MMseqs2,\nbut increases the sensitivity by morethan threefold, and approaches the\nstate-of-the-art structure search methods. The improvement in sensi-\ntivity is particularly apparent in remote homology pairs.\nResults\nPLMsearch reaches similar sensitivity as structure search\nmethods\nWe benchmarked the sensitivity of SS-predictor, PLMSearch,\nPLMSearch + PLMAlign, ﬁve other sequence search methods\n(MMseqs2, Blastp, HHblits, EAT, and pLM-BLAST), four structural\nalphabet-based search methods (3D-BLAST-SW, CLE-SW, Foldseek,\nand Foldseek-TM), and three structural alignment-based search\nmethods (CE, Dali, and TM-align). We performed search tests on\nSCOPe40-test and Swiss-Prot after ﬁltering homologs from the\ntraining dataset (see“Datasets”, “Metrics\", and“Baselines\" Sections).\nIn the SCOPe40-test dataset (2207 proteins), we performed an all-\nversus-all search test. Therefore, a total of 4,870,849 query-target\npairs were tested for all the methods. Figure2a– c shows the results of\nthe 11 most competitive methods in sensitivity and speed. Supple-\nmentary Fig. 1a– c shows the results of the other two structural\nalphabet-based and two structural alignment-based search methods.\nIn the Swiss-Prot search test, we randomly selected 50 queries from\nFig. 2 | PLMsearch reaches similar sensitivity as structure search methods.\na– c The all-versus-all search test on SCOPe40-test. For family-level, superfamily-\nlevel, and fold-level recognition, TPs were deﬁned as same family, same superfamily\nbut different family, and same fold but different superfamily, respectively. Hits\nfrom different folds are FPs. After sorting the search result of each query according\nto similarity, we calculated the sensitivity as the fraction of TPs in the sorted list up\nto theﬁrst FP.a We took the mean sensitivity over all queries as AUROC. In addition,\nthe total search time for the all-versus-all search test with a 56-core Intel(R) Xeon(R)\nCPU E5-2680 v4 @ 2.40 GHz and 256 GB RAM server is shown on the legend.\nb Precision-recall curve.c MAP, P@1, and P@10.d Evaluation on new proteins (see\n“New protein search test\" Section). Supplementary Table 2 and Supplementary\nTable 4 record the speciﬁc values of each metric. Source data are provided as a\nSource Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 3\nSwiss-Prot and 50 queries from SCOPe40-test (a total of 100 query\nproteins) and searched for 430,140 target proteins in Swiss-Prot.\nTherefore, a total of 43,014,000 query-target pairs were tested for\nthe six most efﬁcient methods on searching large-scale datasets\n(Supplementary Fig. 2, Supplementary Table 1).\nSupplementary Table 2 records the speciﬁc values of each metric\nfor the all-versus-all search test on SCOPe40-test. PLMSearch performs\nwell on most metrics, especially at the superfamily-level and fold-level,\nwhich are shallower and have less signiﬁcant similarity between pro-\ntein sequences. PLMSearch is 3, 16, and 219 times exceeding MMseqs2\nin AUROC of the family-level (from 0.318 to 0.928), superfamily-level\n(from 0.050 to 0.826), and fold-level (from 0.002 to 0.438), respec-\ntively. Supplementary Table 3 indicates that the primary reason for the\nimprovement is that PLMSearch is more robust and makes the rank of\nthe ﬁrst FP (False Positive) lower, increasing the number of total TPs\n(True Positives) up to theﬁrst FP (38 times exceeding MMseqs2, from\n2.74 to 104.78). PLMSearch + PLMAlign uses PLMAlign to align protein\npairs with a similarity exceeding 0.3 from PLMSearch. The alignment\nscore is then used to rerank, which improves the precision rate,\nespecially on the search test for new proteins that fail to scan out any\nPfam domain (Fig.2d, Supplementary Fig. 1d, Supplementary Table 4).\nAccordingly, the exclusion of pairs with a similarity below 0.3 leads to a\nmarginal decrease in the recall rate.\nPLMSearch searches millions of query-target pairs in seconds\nWe ﬁrst compared the total search time of different methods for the\nall-versus-all search test on SCOPe40-test (2,207 proteins, 4,870,849\nquery-target pairs). To ensure fairness, we used the same computing\nresources (a 56-core Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40 GHz and\n256 GB RAM server) when implementing different methods in the\nevaluation. For HHblits, pLM-BLAST, TM-align, and various other par-\nallelizable methods, we utilized all 56 cores by default. Moreover,\nalmost all methods need to preprocess the target dataset in advance.\nThis part of the time is not required while searching, so we did not\ninclude the preprocessing time in the search time statistics. As shown\nin the legend of Fig.2a, by using SS-predictor to predict the similarity\ninstead of calculating the structural similarity (TM-score) of all protein\npairs, SS-predictor (10 s) and PLMSearch (4 s) are among the fastest\nmethods, and more than four orders of magnitude faster than TM-\nalign (11,303 s).\nPLMSearch can achieve similar efﬁciency on our publicly available\nweb server with CPU only (64 * Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50\nGHz and 512 GB RAM). Searching a query against Swiss-Prot (568K\nproteins) and UniRef50 (53.6M proteins)\n9 and employing PLMAlign to\nalign the query with the Top-10 targets requires approximately\n0.15 min and 1.1 min, respectively (Supplementary Table 5). In fact,\nwhen searching a query against Swiss-Prot (568K proteins), PLMAlign\ntakes up 0.12 min (more than 80% of the total time), and PLMSearch\nonly takes about 0.03 minutes (Supplementary Table 6). This is\nbecause PLMSearch generates and preloads the embeddings of all\ntarget proteins in advance. This strategy helps to save much time by\navoiding repeated forward propagations of the protein language\nmodel with a large number of parameters and saves the time for\nloading embeddings from disk to RAM. As a result, just one forward\npass through the SS-predictor network is required to predict the\nsimilarity of millions of query-target pairs.\nPLMSearch accurately detects remote homology pairs\nRemote homology pairs generally refer to homologous protein pairs\nwith dissimilar sequences but similar structures\n51. Such protein pairs\nhave low sequence similarity, so their homology is dif ﬁcult to\nbe detected by sequence alignment-based methods (MMseqs2,\nBlastp), but can be detected by structure-based search methods\n(Foldseek, Foldseek-TM, TM-align) (Fig.3a). In this study, pairs with\nsimilar sequences and similar structures are deﬁned as sequence\nidentity > 0.3\n51 and TM-score > 0.552,53 and are called“easy pairs\"; pairs\nwith dissimilar sequences but similar structures are deﬁned as\nsequence identity < 0.3 but TM-score > 0.5 and are called“remote\nhomology pairs\". We conducted a speciﬁc analysis of recalled pairs and\nmissed pairs (deﬁned in Fig.3b). We calculated the TM-score and the\nsequence identity (see“Sequence alignment\" Supplementary Section)\nof the recalled/missed pairs and projected them onto a 2D scatter plot\n(Fig. 3c– h).\nCompared with easy pairs (theﬁr s tq u a d r a n ti nF i g .3c– h), remote\nhomology pairs (the fourth quadrant in Fig.3c– h) in the“twilight zone\"\nof protein sequence homology are more difﬁcult to detect\n51.A m o n gt h e\nsix methods (Supplementary Table 7), even the least sensitive methods\nMMseqs2 and Blastp recall all the easy pairs (574/574), but perform\npoorly on remote homology pairs (MMseqs2: 183/1105, Blastp: 203/\n1105). In contrast, powered by the protein language model, SS-predictor\nand PLMSearch search out most of the remote homology pairs (SS-\npredictor: 1022/1105, PLMSearch: 1087/1105, six times exceeding\nMMseqs2), and the recall rate exceeds Foldseek, which directly uses\nstructural data as input (Foldseek: 934/1105, Foldseek-TM: 940/1105).\nAblation experiments: PfamClan, SS-predictor, and PLMAlign\nmake PLMSearch more robust\nTo evaluate PLMSearch without the PfamClan component, we screened\na total of 110 queries from the 2207 queries in SCOPe40-test, which\nfailed to scan any Pfam domain (see“New protein search test\" Section).\nAs expected, the performance of PLMSearch is exactly the same as that\nof SS-predictor, because PfamClan does notﬁlter out any protein pair,\nwhereas PLMSearch still achieves relatively sensitive search results (MAP\n= 0.612, P@1 = 0.845, P@10 = 0.712, see Supplementary Fig. 3e, Sup-\nplementary Table 4). Using PLMAlign to align and rank based on align-\nment scores signiﬁcantly enhances precision. This improvement stems\nfrom the fact that, unlike SS-predictor, PLMAlign employs per-residue\nembeddings rather than per-protein embeddings as input and uses\npairwise alignment instead of large-scale similarity prediction. Besides, it\nis noteworthy that both SS-predictor + PLMAlign and PLMSearch +\nPLMAlign only align pairs from SS-predictor and PLMSearch pre-ﬁlter\nresults with a similarity exceeding 0.3 (totaling 1,591,492 and 379,707\npairs, respectively), in contrast to aligning all pairs like PLMAlign/pLM-\nBLAST (4,870,849 pairs). This streamlined approach signiﬁcantly redu-\nces the alignment time (nearly 16 times) while maintaining comparable\nprecision, underscoring the beneﬁts of leveraging SS-predictor and\nPLMSearch to pre-ﬁlter (Supplementary Fig. 3b).\nTo evaluate PLMSearch without the SS-predictor component, we\nﬁrst clustered the SCOPe40-test and Swiss-Prot datasets based on\nPfamClan (Supplementary Fig. 4, Supplementary Table 8). Speciﬁ-\ncally, proteins belonging to the same Pfam clan are clustered. The\nclustering results show a signiﬁcant long-tailed distribution. After\npre-ﬁltering with PfamClan, more than 50% of the pre-ﬁltered protein\npairs (orange rectangles in the picture) come from the largest 1– 2\nclusters (big clusters), accounting for only a minor fraction of the\noverall clusters (SCOPe40-test: 0.231%; Swiss-Prot: 0.032%). There-\nfore, big clusters will result in a signiﬁcant number of irrelevant\nprotein pairs in the pre-ﬁltering results, reducing accuracy, and must\nbe further sorted andﬁltered based on similarity, which is what the\nSS-predictor does.\nFurthermore, among all similarity-based search methods (See\n“Baselines”), we further compared the correlation between the pre-\ndicted similarity and TM-score (Supplementary Fig. 3a). The correla-\ntion between the similarity predicted by Euclidean (COS) and TM-score\nis not high, resulting in a large number of actually dissimilar protein\npairs rankingﬁrst. The similarity predicted by the SS-predictor is more\ncorrelated with TM-score (with a higher Spearman correlation coefﬁ-\ncient). This is why SS-predictor outperforms other similarity-based\nsearch methods with the same embeddings as input (Supplementary\nFig. 3b-e, Supplementary Table 2, Supplementary Table 4).\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 4\nSequence iden/g415ty 0.216\nBit score (MMseqs2) No hit\nBit score (Blastp) No hit\nProbability (Foldseek) 1.000\nProbability (Foldseek-TM) 0.984\nTM-score (TM-align) 0.836\nSimilarity (PLMSearch) 0.846 \nSearch result\n    \nWM\na\nb\nc\nd\ne\nf\nM\nW\nab\nfe\ncd\nhg\nFig. 3 | PLMSearch accurately detects remote homology pairs. aCase study. The\nsequence identity between the blue structure and the green structure is low\n(0.216 < 0.3) but they share similar structures. Foldseek, Foldseek-TM, TM-align,\nand PLMSearch recall the remote homology pair that is missed by MMseqs2 and\nBlastp. b Deﬁnition diagram.Legend amarks three pairs with a TM-score > 0.5,\nusually assumed to have the same fold\n52,53. Legend bmarks six pairs with a TM-\nscore between 0.2 and 0.5.Legend cmarks six pairs with a TM-score < 0.2, usually\nassumed as randomly selected irrelevant pairs52,53. Legend dmarks sixﬁltered pairs.\nLegend emarks the pair at (3,3) with a TM-score > 0.5 but is notﬁltered out, which\nis a“Missed\" pair. Correspondingly, protein pairs in (1,4) and (2,2) are“Recalled\"\npairs.Legend fmarks the pair at (3,5) with a TM-score < 0.2 but isﬁltered out, which\nis a“Wrong\" pair.c– h From the search results ofﬁve randomly selected queries to\navoid oversampling (with Swiss-Prot as the target dataset, a total of 2,150,700\nquery-target pairs), we selected the 5000 pairs with the highest similarity for dif-\nferent search methods and counted the recalled and missed pairs:c MMseqs2.\nd Blastp.e Foldseek.f Foldseek-TM.g SS-predictor.h PLMSearch. For recalled pairs\n(left) and missed pairs (right) in each subplot, the TM-score (x-axis) and sequence\nidentity (y-axis) are shown on the 2D scatter plot. The thresholds, sequence\nidentity = 0.3\n51 and TM-score = 0.552,53, are shown by dashed lines. All methods\nsuccessfully recall the easy pairs in theﬁrst quadrant. But for remote homology\npairs in the fourth quadrant, SS-predictor & PLMSearch did the best, followed by\nFoldseek & Foldseek-TM, and MMseqs2 & Blastp were the worst. Supplementary\nTable 7 records the speciﬁc values of each metric. Source data are provided as a\nSource Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 5\nDiscussion\nWe study the use of protein language models for large-scale homo-\nlogous protein search in this work. We propose PLMSearch, which\ntakes only sequences as input and searches for homologous proteins\nusing the protein language model and Pfam sequence analysis, allow-\ning PLMSearch to extract remote homology information buried\nbehind sequences. Subsequently, PLMAlign is used to align protein\npairs retrieved by PLMSearch and obtain the alignment scores.\nExperiments reveal that PLMSearch outperforms MMseqs2 in terms of\nsensitivity and is comparable to the state-of-the-art structural search\napproaches. The improvement is especially noticeable in remote\nhomology pairs. PLMSearch, on the other hand, is one of the fastest\nsearch methods in comparison to other baselines, capable of searching\nmillions of query-target protein pairs in seconds. We also summarized\nthe 11 most competitive approaches based on their input and perfor-\nm a n c ei nS u p p l e m e n t a r yT a b l e9 .\nWe discuss the differences between search methods (like\nPLMSearch) and alignment methods (like pLM-BLAST and PLMAlign) in\ndetail in Supplementary Table 10. It is noteworthy that residue\nembedding-based alignment methods, such as PLMAlign and pLM-\nBLAST\n48, achieve respectable sensitivity. However, a primary limitation\nlies in the maximum size of the target dataset. This is particularly evi-\ndent in two key aspects: (1) Residue embedding-based alignment\nnecessitates retaining the embeddings of all residues for each protein in\nthe target dataset, denoted asN*L\ni*D (where N is the number of pro-\nteins,Li i st h el e n g t ho ft h ep r o t e i n ,a n dD is the embedding dimension).\nIn contrast, PLMSearch only requires retaining per-protein embeddings,\nexpressed asN*1*D. This results in a size difference exceeding three\norders of magnitude, posing a signiﬁcant challenge when implementing\na dataset with the size of UniRef50, which contains 53.6 million\nproteins\n9. (2) Residue embedding-based alignment determines the\nsimilarity between protein pairs through pairwise global (local) align-\nments. In contrast, PLMSearch only needs a single forward pass through\nthe SS-predictor network to predict the similarity of millions of query-\ntarget pairs. However, it is important to note that PLMSearch can solely\npredict the similarity of protein pairs without any alignment sugges-\ntions. To this end, PLMSearch + PLMAlign provides alignment for pro-\ntein pairsﬁltered by PLMSearch with a similarity higher than 0.3. This\napproach not only compensates for PLMSearch’s limitations but also\navoids numerous low similarity and meaningless alignments, thereby\nmaintaining high efﬁciency. In the future, we intend to explore the\nmutual attention between query and target per-residue embeddings to\nprovide better global and local sequence alignment results.\nIn summary, we believe that PLMSearch has removed the low\nsensitivity limitations of sequence search methods. Since the sequence\nis more applicable and easier to obtain than structure, PLMSearch is\nexpected to become a more convenient large-scale homologous pro-\ntein search method.\nMethods\nPLMSearch pipeline\nPLMSearch consists of three steps (Fig.1a-c). (1) PfamClan. Initially,\nPfamScan54 identiﬁes the Pfam clan domains of the query protein\nsequences. Subsequently, PfamClan searches the target dataset for\nproteins sharing the same Pfam clan domain with the query proteins.\nIn addition, a limited number of query proteins lack any Pfam clan\ndomain, or their Pfam clans differ from any target protein. To prevent\nsuch queries from yielding no results, all pairs between such query\nproteins and target proteins will be retained. (2) Similarity prediction.\nThe protein language model generates deep sequence embeddings for\nquery and target proteins. Subsequently, SS-predictor predicts the\nsimilarity of all query-target pairs. (3) Search result. Finally, PLMSearch\nselects the similarity of the protein pairs pre-ﬁltered by PfamClan, sorts\nthe protein pairs based on their similarity, and outputs the search\nresults for each query protein separately.\nFor the top-ranked query-target pairs, PLMAlign is used to generate\nlocal or global alignments and alignment scores. In addition, we also\nadded parallel sequence-based Needleman-Wunsch alignment and\nstructure-based TM-align at the end of our pipeline for users to choose.\nPfamClan\nPfamClan ﬁlters out protein pairs that share the same Pfam clan\ndomain (Fig.1a). It is worth noting that the recall rate is more impor-\ntant in the initial pre-ﬁltering. PfamClan is based on a more relaxed\nstandard of sharing the same Pfam clan domain, instead of sharing the\nsame Pfam family domain (what PfamFamily does). This feature allows\nPfamClan to outperform PfamFamily in recall rate (Fig.4)a n ds u c -\ncessfully recalls high TM-score protein pairs that PfamFamily misses\n(Supplementary Table 11).\nSimilarity prediction\nBased on the protein language model and SS-predictor, PLMSearch\nperforms further similarity prediction based on the pre-ﬁltering results\nof PfamClan (Fig.1b). The motivation is that the clustering results\nbased on PfamClan show a signiﬁcant long-tailed distribution (Sup-\nplementary Fig. 4). As the size of the dataset increases, the number of\nproteins contained in big clusters will greatly expand, further leading\nto a rapid increase in the number of pre-ﬁltered protein pairs\nFig. 4 | The pre-ﬁltering results of PfamFamily & PfamClan. aWe evaluated the\npre-ﬁltering results of PfamFamily & PfamClan on the SCOPe40-test, Swiss-Prot to\nSwiss-Prot, and SCOPe40-test to Swiss-Prot search tests (see“Datasets\"). PfamClan\nachieves a higher recall rate.b Same(1) or Different(0) fold on SCOPe40-test.\nc– d TM-score distributions using kernel density estimation (smoothed histogram\nusing a Gaussian kernel with the width automatically determined).c Swiss-Prot to\nSwiss-Prot;d SCOPe40-test to Swiss-Prot. The distribution of PfamFamily is overall\nto the right, because the requirements of PfamFamily are stricter than PfamClan, so\nthe protein pair it recalls has a higher probability of being in the same fold and\nsharing a higher TM-score. However, this also leads to PfamFamily having a lower\nrecall rate and missing some homologous protein pairs as shown in Supplementary\nTable 11. It is worth noting that the recall rate is more important in the initial pre-\nﬁltering. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 6\n(Supplementary Table 8). The required computing resources are\nexcessive with TM-align used for all the ﬁltered pairs. Instead,\nPLMSearch employs SS-predictor to predict similarity, which increases\nspeed and eliminates reliance on structures.\nAs shown in Fig.5a, the input protein sequences areﬁr s ts e n tt ot h e\nprotein language model (ESM-1b here) to generate per-residue\nembeddings (m*d,w h e r em is the sequence length and d is the\ndimension of the vector), and the per-protein embedding (1*d)i s\nobtained through the average pooling layer. Subsequently, SS-predictor\npredicts the structural similarity (TM-score) between proteins through a\nbilinear projection network (Fig.5b). However, it is difﬁcult for the\npredicted TM-score to directly rank protein pairs with extremely high\nsequence identity (often differing by only a few residues). This is\nbecause SS-predictor was trained on SCOPe40-train and CATHS40,\nwhere protein pairs share sequence identity < 0.4, so cases with extre-\nmely high sequence identity were not included. At the same time, COS\nsimilarity performs well in cases with extremely high sequence identity\n(see P@1 in Supplementary Fig. 3d-e) but becomes increasingly insen-\nsitive to targets afterTop-10. Therefore, theﬁnal similarity predicted by\nSS-predictor is composed of the predicted TM-scores and the COS\nsimilarity to complement each other. We studied the reference simi-\nlarity of COS similarity in SCOPe40-train (Supplementary Fig. 5a-b,\nSupplementary Table 12). We assemble the predicted TM-score with the\ntop COS similarity as follows. When COS similarity > 0.995, SS-predictor\nsimilarity equals COS similarity. Otherwise, SS-predictor similarity\nequals the predicted TM-score multiplied by COS similarity.\nWe also studied the reference similarity of SS-predictor similarity\n(Fig. 6) in SCOPe40-train. There is a clear phase transition occurring\naround the similarity of 0.3– 0.7. Supplementary Table 12 shows that\nfor SS-predictor, protein pairs with a similarity lower than 0.3 are\nusually assumed as randomly selected irrelevant protein pairs. In the\nridge plot Fig.6b, as expected, the protein pairs in the same fold and\ndifferent folds are well grouped in two different similarity ranges, i.e.\nthe protein pairs in the same fold have a higher similarity and the\nprotein pairs in different folds have a lower one. However, since\nsimilarity and SCOP fold do not have a one-to-one correspondence,\nthere is a small overlap. Furthermore, unlike Foldseek, which focuses\non local similarity, the similarity of SS-predictor, like TM-score, focuses\non global similarity (Supplementary Table 13).\nPLMAlign\nFor the retrieved protein pairs, PLMAlign takes per-residue embeddings\nas input to obtain speciﬁc alignments and alignment scores (Fig.1d).\nPLMAlign subsequently uses alignment scores to rerank, which improves\nthe ranking resultsf u r t h e r .S p e c iﬁcally, inspired by pLM-BLAST\n48,55,\nPLMAlign also uses per-residue embeddings of the query-target protein\npair to calculate the substitution matrix. The substitution matrix is then\nused in the SW/NW algorithm to perform local/global alignment. On the\none hand, Compared with traditional SW/NW using aﬁxed substitution\nmatrix, the substitution matrix calculated by PLMAlign uses protein\nembedding generated from the sequence context, thus containing deep\nevolutionary information. On the other hand, compared with pLM-\nBLAST, which uses cosine similarity, PLMAlign employs the dot product\nsimilarity and linear gap penalty. This enables PLMAlign to better align\nremote homology pairs while reducing the algorithm’sc o m p l e x i t yt o\nO(mn)to ensure high efﬁciency. The other differences between the SW/\nNW algorithm, pLM-BLAST, and PLMAlign are discussed in further detail\nin (Supplementary Table 14). Therefore, PLMAlign performs better on\nremote homology alignment (using“Malisam and Malidup\" datasets as\nbenchmarks, see Supplementary Fig. 6, Supplementary Table 15). Also,\nsee “Remote homology alignment\" Supplementary Section for detailed\nsettings of PLMAlign and the evaluation of alignment results. The\nreference score of PLMAlign is provided in Supplementary Fig. 5c, d and\nSupplementary Table 12. PLMAlign in the main text uses global align-\nment to generate alignment scores.\nDatasets\nThe data volumes and uses of each dataset are summarized in Sup-\nplementary Table 16.\nSCOPe40. The SCOPe40 dataset consists of single domains with real\nstructures. Clustering of SCOPe 2.01\n56,57 at 0.4 sequence identity yielded\n11,211 non-redundant protein domains t r u c t u r e s( S C O P e 4 0 ) .A sd o n ei n\nFoldseek, domains from SCOPe40 were split 8:2 by fold into SCOPe40-\nt r a i na n dS C O P e 4 0 - t e s t ,a n dt h e nd o m a i n sw i t has i n g l ec h a i nw e r e\nreserved. It is worth mentioning that each domain in SCOPe40-test\nbelongs to a different fold from all domains in SCOPe40-train, so the\nd i f f e r e n c eb e t w e e nt r a i n i n ga n dt e s t i n gd a t ai sm u c hl a r g e rt h a nt h a to f\npure random division. We also studied the max sequence identity of\nMSMSQPTETVSDAP… \nm \nProtein Language Model \nd \nm \nAverage Pooling Layer \n1 \nd \nm \nd \nn \nd \nd \nd \nSigmoid Layer \nn*m \nPredicted \nTM-scores \nn Queries \nm Targets \nSimilarity Search result \nOutput \n0.4 \n3 \n0.5 \n2 \n-0.1 \n5 \n0.9 \n1 \n0.3 \n4 \n0.7 \n1 \n-0.8 \n5 \n0.1 \n3 \n-0.2 \n4 \n0.4 \n2 \n0.6 \n1 \n-0.2 \n4 \n0.3 \n2 \n-0.1 \n3 \n-0.3 \n5 \nSimilarity \npredic/g415on \nAll pairs \n1 2 3 4 5 \n0.9 \n0.7 \n0.6 \n0.5 \n0.4 \n0.3 \n0.4 \n0.1 \n-0.1 \n0.3 \n-0.2 \n-0.2 \n-0.1 \n-0.8 \n-0.3  \na b \nc \nFig. 5 | SS-predictor. aProtein embedding generation. The protein language model\nconverts the protein sequence composed ofm residues intom*d per-residue\nembeddings, whered is the dimension of the embedding. Then the average pooling\nlayer converts them into the corresponding 1*d per-protein embeddingz. b SS-\npredictor. A bilinear projection network predicts all the TM-scores betweenn query\nembeddingsz1a n dm target embeddingsz2b yz1*W*z2, where the matricesW are\nthe learned parameters.c Similarity-based search methods. The similarity of all\nprotein pairs is predicted, sorted, and then outputted as a search result. According\nto different similarity, three corresponding search methods are Euclidean, COS,\nand SS-predictor.\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 7\neach protein in SCOPe40-test relative to the training dataset (Supple-\nmentary Fig. 7). From theﬁgure, we can draw a similar conclusion that\nthe sequences in SCOPe40-test and the training dataset are quite dif-\nferent, and most of the max sequenceidentity is between 0.2 and 0.3.\nPLMSearch performs well in SCOPe40-test (Fig.2a– c, Supplementary\nFig. 1a– c, Supplementary Table 2), implying that PLMSearch learns\nuniversal biological properties that are not easily captured by other\nsequence search methods\n13.\nNew protein search test.I nr e a l - w o r l ds c e n a r i o s ,n e w l yd i s c o v e r e d\nand unclassiﬁed proteins often play a crucial role in innovative\nresearch. To assess the effectiveness of various methods in searching\nthese proteins, we introduced an additional search test exclusively\ncomprising query proteins that failed to scan any Pfam domain. Spe-\nciﬁcally, we screened a total of 110 queries from the 2207 queries in the\nSCOPe40-test, which failed to scan any Pfam domain. In the all-versus-\nall search test on the SCOPe40-test dataset, we counted the MAP, P@1,\nand P@10 metrics with only new proteins as queries (110 proteins) and\nSCOPe40-test as targets (2207 proteins).\nSwiss-Prot. Unlike SCOPe, the Swiss-Prot dataset consists of full-length,\nmulti-domain proteins with predicted structures, which are closer to\nreal-world scenarios. Because the throughput of experimentally\nobserved structures is very low and requires a lot of human andﬁnancial\nresources, the number of real structures in datasets like PDB\n58– 60 tends\nto be low. AlphaFold protein structure database (AFDB) obtains protein\nstructure through deep learning prediction, so it contains the entire\nprotein universe and gradually becomes the mainstream protein\nstructure dataset. Therefore, in this set of tests, we used Swiss-Prot with\npredicted structures from AFDB as the target dataset.\nSpeciﬁcally, we downloaded the protein sequence from UniProt\n9\nand the predicted structure from the AlphaFold Protein Structure\nDatabase31. A total of 542,317 proteins withboth sequences and predicted\nstructures were obtained. For these proteins, we dropped low-quality\nproteins with an avg. pLDDT lower than 70, and left 498,659 proteins. In\norder to avoid possible data leakage issues, like SCOPe40, we used\n0.4 sequence identity as the threshold toﬁlter homologs in Swiss-Prot\nfrom the training dataset. Speciﬁcally, we used the previously screened\n498,659 proteins as query proteinsand SCOPe40-train as the target\ndataset. Weﬁrst pre-ﬁltered potential homologous protein pairs with\nMMseqs2 and calculated the sequence identity between all these pairs.\nThe query protein from Swiss-Protwill be discarded if the sequence\nidentity between the query protein and any target protein is greater\nthan or equal to 0.4. Finally, 68,519 proteins were deleted via homology\nﬁltering, leaving 430,140 proteins in Swiss-Prot, which we employed\nin our experiments. We also studied the max sequence identity of\neach protein in Swiss-Prot relative to the training dataset (Supplementary\nFig. 7) and found that the vast majority of them were below 0.3.\nSubsequently, we randomly selected 50 queries from Swiss-Prot\nand 50 queries from SCOPe40-test as query proteins (a total of 100\nquery proteins) and searched for 430,140 target proteins in Swiss-Prot.\nTherefore, a total of 43,014,000 query-target pairs were tested. The\nsearch test for 50 query proteins from Swiss-Prot and SCOPe40-test\nare called“Swiss-Prot to Swiss-Prot\" and“SCOPe40-test to Swiss-Prot\",\nrespectively.\nCATHS40. The SCOPe40-train dataset includes 8953 proteins and TM-\nscores for all protein pairs were calculated for training. As the majority\nof these pairs had TM-scores below 0.5, only 504,553 pairs (among\n80,156,209 in total) had a TM-score above 0.5 for model training. To\nenhance the model’s generality, we supplemented it with high-quality\nprotein pairs extracted from the curated CATH domain dataset\n44,47.\nWe began with the CATHS40 non-redundant dataset of protein\ndomains, which exhibits no more than 0.4 sequence similarity.\nDomains exceeding 300 residues wereﬁltered out, leaving 27,270\ndomains. To prevent potential data leakage issues, akin to SCOPe40,\nwe applied a 0.4 sequence identity threshold toﬁlter homologs in\nCATHS40 from the testing dataset (SCOPe40-test and Swiss-Prot).\nFinally, 21,474 proteins in CATHS40 were left for training, and the max\nsequence identity of the test dataset to the new training dataset is\nstill less than 0.4 (Supplementary Fig. 7). We then undersampled\nCATHS40 domain pairs from different folds to acquire a substantial\namount of training pairs with TM-scores above 0.5. Speciﬁcally, we\nsampled the TM-scores of 28,440,312 protein pairs for training, of\nwhich 7,813,946 pairs had a TM-score above 0.5 (Supplementary\nTable 16).\nTarget datasets on the web server. We currently have the following\nfour target datasets on the web server for users to search: (1) Swiss-Port\n(568K proteins)\n9, the original dataset withoutﬁltering; (2) PDB (680K\nproteins)58– 60; (3) UniRef50 (53.6M proteins)9.U n i R e f 5 0i sb u i l tb y\nclustering UniRef90 seed sequences that have at least 50% sequence\nidentity to and 80% overlap with the longest sequence in the cluster;\n(4) Self (the query dataset itself).\nFig. 6 | Reference similarity. aThe posterior probability of proteins with a given\nsimilarity being in the same fold or different folds in SCOPe40-train.b Similarity\ndistribution of the same and different folds protein pairs using kernel density\nestimation (smoothed histogram using a Gaussian kernel with the width auto-\nmatically determined). The similarity of protein pairs belonging to the same protein\npair is signiﬁcantly higher than that of protein pairs belonging to different folds.\nThe posterior probability corresponding to the similarity is shown in Supplemen-\ntary Table 12. See“Reference similarity\" Supplement Section for more details.\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 8\nMalisam and Malidup. We employed two gold-standard benchmark\ndatasets, Malisam61 and Malidup62, to establish a robust reference\nfor alignment. These sets rigorously select structural alignments,\nemphasizing dif ﬁcult-to-detect, low-sequence-identity remote\nhomology. Malidup contains pairwise structure alignments, speciﬁ-\ncally targeting homologous domains within the same chain, thereby\nexemplifying structurally similar remote homologs. Malisam con-\ntains pairs of analogous motifs.\nMetrics\nWe evaluated different homologous protein search methods using the\nfollowing four metrics: AUROC, AUPR, MAP, and P@K.\n AUROC\n24: The mean sensitivity over all queries, where the sensi-\ntivity is the fraction of TPs in the sorted list up to theﬁrst FP, all\nexcluding self-hits.\n AUPR24: Area under the precision-recall curve.\n MAP63: Mean average precision (MAP) for a set of queries is the\nmean of the average precision scores for each query.\nMAP =\nPQ\nq =1 AvePðqÞ\nQ\nð1Þ\nwhere Q is the number of queries.\n P@K20: For homologous protein search, as many queries have\nthousands of relevant targets, and few users will be interested in\ngetting all of them. Precision at k (P@k) is then a useful metric (e.g.,\nP@10 corresponds to the number of relevant results among the top\n10 retrieved targets). P@K here is the mean value for each query.\nOn SCOPe40-test, we performed an all-versus-all search test,\nwhich means both the query and the target dataset were SCOPe40-\ntest. To make a more objective comparison, the settings used in the all-\nversus-all search test are exactly the same as those used in Foldseek\n24.\nSpeciﬁcally, for family-level, superfamily-level, and fold-level recogni-\ntion, TPs were deﬁned as the same family, same superfamily but dif-\nferent family, and same fold but different superfamily, respectively.\nHits from different folds are FPs. After sorting the search result of each\nquery according to similarity (described in Supplementary Table 17),\nwe calculated the sensitivity as the fraction of TPs in the sorted list up\nto theﬁrst FP to better reﬂect the requirements for low false discovery\nrates in automatic searches. We then took the mean sensitivity over all\nqueries as AUROC. Additionally, we plotted weighted precision-recall\ncurves (precision = TP/(TP+FP) and recall = TP/(TP+FN)). All counts\n(TP, FP, FN) were weighted by the reciprocal of their family, super-\nfamily, or fold size. In this way, families, superfamilies, and folds con-\ntribute linearly with their size instead of quadratically\n14.M A Pa n dP @ K\nwere calculated according to the TPs and FPs deﬁned by fold-level.\nOn search tests against Swiss-Prot, without the human manual\nclassiﬁcation on SCOPe as the gold standard, proteins require a\nreference annotation method. Therefore, TPs were deﬁned as pairs\nwith a TM-score > 0.5, otherwise FPs. MAP and P@K are then calculated\naccordingly.\nBaselines\nPreviously proposed methods. (1) Sequence search: MMseqs210,\nBLASTp11, HHblits16,17,E A T45,a n dp L M - B L A S T48. (2) Structure search—\nstructural alphabet: 3D-BLAST-SW22, CLE-SW23, Foldseek, and Foldseek-\nTM24. (3) Structure search— structural alignment64:C E25,D a l i26,a n dT M -\nalign27,28.F o rt h es p e c iﬁc introduction and settings of these proposed\nmethods, see“Baseline details\" Supplementary Section.\nSimilarity-based search methods. These methods predict and sort\nthe similarity between all query-target pairs (Fig.5c). Different search\nmethods are distinguished according to the way they predict\nsimilarity.\n Euclidean: Use the reciprocal of the Euclidean distance between\nembeddings.\nsimilarity ðp,qÞ = 1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn\ni =1 pi /C0 qi\n/C0/C1 2\nq\n+1\nð2Þ\n COS: Use the COS distance between embeddings.ϵ is a small value\nto avoid division by zero.\nsimilarity ðp,qÞ = p /C1 q\nmax p\n/C13/C13 /C13/C13\n2 /C1 q\n/C13/C13 /C13/C13\n2,ϵ\n/C16/C17 ð3Þ\n SS-predictor: Combine the predicted TM-score with the top COS\nsimilarity. Unlike PLMSearch, which predicts pre-ﬁltered pairs\nfrom PfamClan, SS-predictor extends its prediction to all\nprotein pairs.\nExperiment settings\nPfam result generation. We obtained the Pfam family domains of\nproteins by PfamScan (version 1.6)54 and Pfam dataset (Pfam36.0,\n2023-09-12)42. For PfamClan, we query the comparison table Pfam-\nA.clans.tsv and replace the family domain with the clan domain it\nbelongs to. For the family domain that has no corresponding clan\ndomain, we treat it as a clan domain itself.\nProtein language model. ESMs are a set of protein language models\nthat have been widely used in recent years. We used ESM-1b (650M\nparameters)\n35, a SOTA general-purpose protein language model, to\nefﬁciently generate per-protein embeddings for PLMSearch.\nFor PLMAlign, the more extensive ProtT5-XL-UniRef50 (3B para-\nmeters) is used to generate per-residue embeddings. We conducted a\ndetailed evaluation and analysis of the results from ESM-1b and ProtT5-\nXL-UniRef50 embeddings, as elaborated in“Remote homology align-\nment” Supplementary Section (Supplementary Fig. 8, Supplementary\nTable 15). Based on the analysis, for PLMAlign, ProtT5-XL-UniRef50 was\nselected.\nSS-predictor training. We used the deep learning framework PyTorch\n(version 1.7.1), ADAM optimizer, with MSE as the loss function to train\nSS-predictor. The batch size was 100, and the learning rate was 1e-6 on\n200 epochs. The training ground truth was the TM-score calculated by\nTM-align. The datasets used for training (Supplementary Table 16)\ninclude (1) All protein pairs from SCOPe40-train (8953 proteins;\n80,156,209 query-target pairs). (2) Undersampled protein pairs from\nCATHS40 (21,474 proteins; 28,440,312 query-target pairs). The para-\nmeters in ESM-1b were frozen and only the parameters in the bilinear\nprojection network were trained.\nExperimental environment. We conducted the experiments on a\nserver with a 56-core Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40 GHz and\n256 GB RAM. The environment of our publicly available web server is\n64 * Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50 GHz and 512 GB RAM.\nStatistics and reproducibility\nThe experiments were not randomized.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 9\nData availability\nThe sequences and structures of the SCOPe40 dataset are available at\nhttps://scop.berkeley.edu. The sequences of the Swiss-Prot and Uni-\nRef50 dataset are freely available under the Creative Commons Attri-\nbution (CC BY 4.0) License athttps://www.uniprot.org. The predicted\nstructures are freely available from the AlphaFold Protein Structure\nDatabase at https://alphafold.ebi.ac.uk/download.C A T Hd o m a i n\nsequences and structures are publicly available athttp://www.cathdb.\ninfo. Malidup can be found athttp://prodata.swmed.edu/malidup;\nMalisam can be found athttp://prodata.swmed.edu/malisam.P f a mi s\nfreely available under the Creative Commons Zero (‘CC0’) license at\nhttps://pfam.xfam.org. For PfamClan, we query the comparison table\nPfam-A.clans.tsv athttps://ftp.ebi.ac.uk/pub/databases/Pfam/current_\nrelease/Pfam-A.clans.tsv.gz. ESM-1b protein language model is avail-\nable at https://github.com/facebookresearch/esm.P r o t T 5 - X L -\nUniRef50 protein language model is available athttps://github.com/\nagemagician/ProtTrans. The sequences of the PDB dataset are avail-\nable athttps://www.rcsb.org. Source data of our work is provided at: (1)\nPLMSearch: https://dmiip.sjtu.edu.cn/PLMSearch/static/download/\nplmsearch_data.tar.gz. (2) PLMAlign: https://dmiip.sjtu.edu.cn/\nPLMAlign/static/download/plmalign_data.tar.gz. Source data are pro-\nvided with this paper.\nCode availability\nPLMSearch is freely available at https://dmiip.sjtu.edu.cn/\nPLMSearch. PLMAlign is freely available athttps://dmiip.sjtu.edu.\ncn/PLMAlign. PLMSearch and related tutorials are freely available to\nthe public at GitHub https://github.com/maovshao/PLMSearch/\nblob/main/pipeline.ipynb. Reproducing our results and regenerat-\ning the main and Supplementary Figs. requires only one ﬁle at\nGitHub https://github.com/maovshao/PLMSearch/blob/main/main.\nipynb. The results of PLMSearch can also be reproduced through\nthe capsule published on Code Oceanhttps://doi.org/10.24433/CO.\n8325548.v165 or source code on Figsharehttps://doi.org/10.6084/\nm9.ﬁgshare.2325463766. Run PLMAlign and reproduce the align-\nment experiment in“Remote homology alignment\" Supplementary\nSection at GitHubhttps://github.com/maovshao/PLMAlign ,w h o s e\npipeline is modiﬁed from pLMBLAST48,55. pLM-BLAST is available\nfrom https://github.com/labstructbioinf/pLM-BLAST under an MIT\nLicense that allows the use, copying, modiﬁcation, merging and\npublishing of the software, with copyright notice and permission\nnotice included in all copies or substantial portions of the software.\nStructure visualizations were created in Pymol v.2.4.0 ( https://\ngithub.com/schrodinger/pymol-open-source ). For our PLMSearch\ndata visualizations, we used Python version 3.8.16, Seaborn Version\n0.12.2, and matplotlib-base Version 3.6.2.\nReferences\n1. Yao, S. et al. NetGO 2.0: improving large-scale protein function\nprediction with massive sequence, text, domain, family and net-\nwork information.Nucleic Acids Res.49, W469–W475\n(2021).\n2. You, R. et al. NetGO: improving large-scale protein function pre-\ndiction with massive network information.Nucleic Acids Res.47,\nW379–W387 (2019).\n3. You, R., Yao, S., Mamitsuka, H. & Zhu, S. DeepGraphGO: graph\nneural network for large-scale, multispecies protein function pre-\ndiction.Bioinformatics37,i 2 6 2–i271 (2021).\n4. Torres, M., Yang, H., Romero, A. E. & Paccanaro, A. Protein function\nprediction for newly sequenced organisms.Nat. Mach. Intell.3,\n1050–1060 (2021).\n5. Unsal, S. et al. Learning functional properties of proteins with lan-\nguage models.Nat. Mach. Intell.4, 227–245 (2022).\n6. Wang, S., You, R., Liu, Y., Xiong, Y. & Zhu, S. Netgo 3.0: Protein\nlanguage model improves large-scale functional annotations.\nGenomics Proteom. Bioinform.21,3 4 9–358 (2023).\n7. Hu, L., Wang, X., Huang, Y.-A., Hu, P. & You, Z.-H. A survey on\ncomputational models for predicting protein-protein interactions.\nBrief. Bioinform.22, bbab036 (2021).\n8. Liu, L., Huang, X., Mamitsuka, H. & Zhu, S. HPOLabeler: improving\nprediction of human protein-phenotype associations by learning to\nrank. Bioinformatics36,4 1 8 0–4188 (2020).\n9. The UniProt Consortium. UniProt: the Universal Protein Knowl-\nedgebase in 2023.Nucleic Acids Res.51,D 5 2 3–D531 (2022).\n10. Steinegger, M. & Söding, J. MMseqs2 enables sensitive protein\nsequence searching for the analysis of massive data sets.Nat.\nBiotechnol.35,1 0 2 6–1028 (2017).\n11. Altschul, S. F., Gish, W., Miller, W., Myers, E. W. & Lipman, D. J. Basic\nlocal alignment search tool.J. Mol. Biol.215,4 0 3–410 (1990).\n12. Buch ﬁnk, B., Reuter, K. & Drost, H.-G. Sensitive protein alignments\nat tree-of-life scale using DIAMOND.Nat. Methods18,\n366–368 (2021).\n13. Mahlich, Y., Steinegger, M., Rost, B. & Bromberg, Y. HFSP: high\nspeed homology-driven function annotation of proteins.Bioinfor-\nmatics 34,i 3 0 4–i312 (2018).\n14. Eddy, S. R. Accelerated proﬁ\nle HMM searches.PLoS Comput. Biol.\n7, e1002195 (2011).\n15. Söding, J. Protein homology detection by HMM-HMM comparison.\nBioinformatics21,9 5 1–960 (2004).\n16. Remmert, M., Biegert, A., Hauser,A. & Söding, J. HHblits: lightning-\nfast iterative protein sequence searching by HMM-HMM alignment.\nNat. Methods9,1 7 3–175 (2011).\n17. Steinegger, M. et al. HH-suite3for fast remote homology detection\nand deep protein annotation.BMC Bioinform.20,4 7 3( 2 0 1 9 ) .\n18. Illergård, K., Ardell, D. H. & Elofsson, A. Structure is three to ten\ntimes more conserved than sequence–a study of structural\nresponse in protein cores.Proteins 77, 499–508 (2009).\n19. Ovchinnikov, S. et al. Protein structure determination using meta-\ngenome sequence data.Science 355,2 9 4–298 (2017).\n20. Buchan, D. W. A. & Jones, D. T. EigenTHREADER: analogous protein\nfold recognition by efﬁcient contact map threading.Bioinformatics\n33,2 6 8 4–2690 (2017).\n21. Bhattacharya, S., Roche, R., Moussad, B. & Bhattacharya, D. Dis-\nCovER: distance- and orientation-based covariational threading for\nweakly homologous proteins.Proteins90,5 7 9–588 (2022).\n22. Yang, J.-M. & Tung, C.-H. Protein structure database search and\nevolutionary classiﬁcation. Nucleic Acids Res.34,\n3646–3659 (2006).\n23. Wang, S. & Zheng, W.-M. CLePAPS: fast pair alignment of protein\nstructures based on conformational letters.J. Bioinform. Comput.\nBiol. 6,3 4 7–366 (2008).\n24. van Kempen, M. et al. Fast and accurate protein structure search\nwith Foldseek.Nat. Biotechnol. 42,2 4 3–246 (2023).\n25. Shindyalov, I. N. & Bourne, P. E. Protein structure alignment by\nincremental combinatorial extension (CE) of the optimal path.Pro-\ntein Eng.11,7 3 9–747 (1998).\n26. Holm, L. Using Dali for protein structure comparison.Methods Mol.\nBiol. 2112,2 9–42 (2020).\n27. Zhang, Y. & Skolnick, J. Scoring function for automated assessment\nof protein structure template quality.\nProteins 57,7 0 2–710\n(2004).\n28. Zhang, Y. & Skolnick, J. TM-align: a protein structure alignment\nalgorithm based on the TM-score.Nucleic Acids Res.33,\n2302–2309 (2005).\n29. Jumper, J. et al. Highly accurate protein structure prediction with\nAlphaFold.Nature 596,5 8 3–589 (2021).\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 10\n30. Baek, M. et al. Accurate prediction of protein structures and inter-\nactions using a three-track neural network.Science 373,\n871–876 (2021).\n31. Varadi, M. et al. AlphaFold Protein Structure Database:\nmassively expanding the structural coverage of protein-sequence\nspace with high-accuracy models.Nucleic Acids Res.50,\nD439–D444 (2021).\n32. Mitchell, A. L. et al. MGnify: the microbiome analysis resource in\n2020. Nucleic Acids Res.48,D 5 7 0–D578 (2019).\n33. Nijkamp, E., Ruffolo, J. A., Weinstein, E. N., Naik, N. & Madani, A.\nProGen2: Exploring the boundaries of protein language models.\nCell Syst.14, 968–978.e3 (2023).\n34. Shan, S. et al. Deep learning guided optimization of human anti-\nbody against SARS-CoV-2 variants with broad neutralization.Proc.\nNatl Acad. Sci.119, e2122954119 (2022).\n35. Rives, A. et al. Biological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences.\nP r o c .N a t lA c a d .S c i .118, e2016239118 (2021).\n36. Meier, J. et al. Language models enable zero-shot prediction of the\neffects of mutations on protein function.Adv. Neural Inf. Process.\nSyst. 34,2 9 2 8 7–29303 (2021).\n37. Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein\nstructure with a language model.Science 379, 1123–1130 (2023).\n38. Elnaggar, A. et al. ProtTrans: toward understanding the language of\nlife through self-supervised learning.IEEE Trans. Pattern Anal. Mach.\nIntell. 44, 7112–7127 (2022).\n39. Hu, M. et al. Exploring evolution-aware &-free protein language\nmodels as protein function predictors.Adv. Neural Inf. Process.\nSyst. 35,3 8 8 7 3–38884 (2022).\n40. Rao, R. et al. Evaluating protein transfer learning with TAPE.Adv.\nNeural Inf. Process. Syst.32,9 6 8 9–9701 (2019).\n41. Bileschi, M. L. et al. Using deep learning to annotate the protein\nuniverse.Nat. Biotechnol.40,9 3 2–937 (2022).\n42. Mistry, J. et al. Pfam: The protein families database in 2021.Nucleic\nAcids Res.49,D 4 1 2–D419 (2020).\n43. Nallapareddy, V. et al. CATHe: detection of remote homologues for\nCATH superfamilies using embeddings from protein language\nmodels. Bioinformatics39\n, btad029 (2023).\n44. Sillitoe, I. et al. CATH: increased structural coverage of functional\nspace. Nucleic Acids Res.49,D 2 6 6–D273 (2020).\n45. Heinzinger, M. et al. Contrastive learning on protein embeddings\nenlightens midnight zone.NAR Genomics Bioinform.4,l q a c 0 4 3\n(2022).\n46. Llinares-López, F., Berthet, Q., Blondel, M., Teboul, O. & Vert, J.-P.\nDeep embedding and alignment of protein sequences.Nat. Meth-\nods 20,1 0 4–111 (2023).\n47. Hamamsy, T. et al. Protein remote homology detection and struc-\ntural alignment using deep learning.Nat. Biotechnol.( 2 0 2 3 ) .\n48. Kaminski, K., Ludwiczak, J., Pawlicki, K., Alva, V. & Dunin-Horkawicz,\nS. pLM-BLAST: distant homology detection based on direct com-\nparison of sequence representations from protein language mod-\nels. Bioinformatics39, btad579 (2023).\n49. Smith, T. & Waterman, M. Identiﬁcation of common molecular\nsubsequences.J. Mol. Biol.147,1 9 5–197 (1981).\n5 0 . N e e d l e m a n ,S .B .&W u n s c h ,C .D .Ag e n e r a lm e t h o da p p l i c a b l et o\nthe search for similarities in the amino acid sequence of two pro-\nteins. J. Mol. Biol.48, 443–453 (1970).\n51. Rost, B. Twilight zone of protein sequence alignments.Protein Eng.,\nDes. Selection12,8 5–94 (1999).\n52. Xu, J. & Zhang, Y. How signiﬁcant is a protein structure similarity\nwith TM-score = 0.5?Bioinformatics26,8 8 9–895 (2010).\n53. Zhang, Y., Hubner, I. A., Arakaki, A. K., Shakhnovich, E. & Skolnick, J.\nOn the origin and highly likely completeness of single-domain\nprotein structures.Proc. Natl Acad. Sci.103,2 6 0 5–2610 (2006).\n5 4 . M i s t r y ,J . ,B a t e m a n ,A .&F i n n ,R .D .P r e d i c t i n ga c t i v es i t er e s i d u e\nannotations in the Pfam database.BMC Bioinform.8,2 9 8\n(2007).\n5 5 . K a m i n s k i ,K . ,L u d w i c z a k ,J . ,P a w l i c k i ,K . ,A l v a ,V .&D u n i n - H o r k a w i c z ,\nS. Source code: pLM-BLAST– distant homology detection based on\ndirect comparison of sequence representations from protein lan-\nguage models.https://github.com/labstructbioinf/pLM-\nBLAST (2023).\n56. Fox, N. K., Brenner, S. E. & Chandonia, J.-M. SCOPe: structural\nclassiﬁcation of proteins–extended, integrating SCOP and ASTRAL\ndata and classiﬁcation of new structures.Nucleic Acids Res.42,\nD304–9( 2 0 1 4 ) .\n57. Chandonia, J.-M. et al. SCOPe: improvements to the structural\nclassi\nﬁcation of proteins - extended database to facilitate variant\ninterpretation and machine learning.Nucleic Acids Res.50,\nD553–D559 (2021).\n58. Berman, H. M. et al. The Protein Data Bank.Nucleic Acids Res.28,\n235–242 (2000).\n59. Burley, S. K. et al. RCSB ProteinData Bank: powerful new tools for\nexploring 3D structures of biological macromolecules for basic and\napplied research and education in fundamental biology, biomedi-\ncine, biotechnology, bioengineering and energy sciences.Nucleic\nAcids Res.49,D 4 3 7–D451 (2020).\n60. Sehnal, D. et al. Mol* Viewer: modern web app for 3D visualization\nand analysis of large biomolecular structures.Nucleic Acids Res.\n49, W431–W437 (2021).\n61. Cheng, H., Kim, B.-H. & Grishin, N. V. MALISAM: a database of\nstructurally analogous motifs in proteins.Nucleic Acids Res.36,\nD211–D217 (2007).\n6 2 . v a nH e e l ,A .J . ,d eJ o n g ,A . ,M o n t a l b á n - L ó p e z ,M . ,K o k ,J .&K u i p e r s ,\nO. P. BAGEL3: Automated identiﬁcation of genes encoding bac-\nteriocins and (non-)bactericidal posttranslationally modiﬁed pep-\ntides. Nucleic Acids Res.41, W448–53 (2013).\n63. Wikipedia contributors. Evaluation measures (information retrieval)—\nWikipedia, the free encyclopedia (2023).\n64. Hasegawa, H. & Holm, L. Advancesand pitfalls of protein structural\nalignment.Curr. Opin. Struct. Biol.19,3 4 1–348 (2009).\n65. Liu, W. Protein language model powers accurate and fast sequence\nsearch for remote homology.https://doi.org/10.24433/CO.\n8325548.v1(2024).\n66. Liu, W. & Zhu, S. Source code: build PLMSearch and PLMAlign\nlocally and reproduce experiments.https://doi.org/10.6084/m9.\nﬁgshare.23254637(2024).\nAcknowledgements\nThis work has been supported by the National Natural Science Foun-\ndation of China (Grant No. 62272105, T2225007, 62172274), the Shang-\nhai Municipal Science and Technology Major Project (Grant No.\n2018SHZDZX01), 111 Project (Grant No. B18015), the ZJ Lab, the Shanghai\nResearch Center for Brain Science and Brain-inspired Intelligence\nTechnology, Beijing Academy of Artiﬁcial Intelligence (BAAI) and the\nGHfund C (202302039294).\nAuthor contributions\nS.Z. conceived the project. S.Z. and J.Y. supervised the project. W.L., S.Z.\nand J.Y. designed the research and the methodological framework. W.L.\nwrote the software. W.L. and Y.X. deployed web servers. H.W. sorted out\nthe structure data. W.L., Z.W. and R.Y. analyzed the data. W.L. drafted the\npaper. Z.W., C.X., S.Z. and J.Y. modiﬁed the paper. All authors agree to\nthe content of theﬁnal paper.\nCompeting interests\nThe authors declare no competing interests.\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 11\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-024-46808-5.\nCorrespondenceand requests for materials should be addressed to\nJianyi Yang or Shanfeng Zhu.\nPeer review informationNature Communicationsthanks the anon-\nymous reviewer(s) for their contribution to the peer review of this work. A\npeer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2024, corrected publication 2024\nArticle https://doi.org/10.1038/s41467-024-46808-5\nNature Communications|         (2024) 15:2775 12"
}