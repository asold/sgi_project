{
  "title": "Chemical language modeling with structured state space sequence models",
  "url": "https://openalex.org/W4400876473",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2905503439",
      "name": "Rıza Özçelik",
      "affiliations": [
        "University Medical Center Utrecht",
        "Eindhoven University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5092966426",
      "name": "Sarah de Ruiter",
      "affiliations": [
        "Eindhoven University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3015973233",
      "name": "Emanuele Criscuolo",
      "affiliations": [
        "Eindhoven University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1854816946",
      "name": "Francesca Grisoni",
      "affiliations": [
        "Eindhoven University of Technology",
        "University Medical Center Utrecht"
      ]
    },
    {
      "id": "https://openalex.org/A2905503439",
      "name": "Rıza Özçelik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092966426",
      "name": "Sarah de Ruiter",
      "affiliations": [
        "Eindhoven University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3015973233",
      "name": "Emanuele Criscuolo",
      "affiliations": [
        "Eindhoven University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1854816946",
      "name": "Francesca Grisoni",
      "affiliations": [
        "University Medical Center Utrecht",
        "Eindhoven University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2017254234",
    "https://openalex.org/W3185391990",
    "https://openalex.org/W2594328795",
    "https://openalex.org/W2784270883",
    "https://openalex.org/W3127493072",
    "https://openalex.org/W4378802816",
    "https://openalex.org/W4318952054",
    "https://openalex.org/W4281619372",
    "https://openalex.org/W4318609278",
    "https://openalex.org/W4380225176",
    "https://openalex.org/W4392903016",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4394581738",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W4306179830",
    "https://openalex.org/W4229590462",
    "https://openalex.org/W4206367183",
    "https://openalex.org/W4315767347",
    "https://openalex.org/W4388730075",
    "https://openalex.org/W3217546525",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W4366462849",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W4200200013",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W2986232138",
    "https://openalex.org/W3157775920",
    "https://openalex.org/W4308988941",
    "https://openalex.org/W3208827570",
    "https://openalex.org/W569478347",
    "https://openalex.org/W4313442864",
    "https://openalex.org/W4323651242",
    "https://openalex.org/W4382603228",
    "https://openalex.org/W4389326242",
    "https://openalex.org/W4390784781",
    "https://openalex.org/W2963910749",
    "https://openalex.org/W2558999090",
    "https://openalex.org/W3094553402",
    "https://openalex.org/W2066273100",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W2070072705",
    "https://openalex.org/W2160592148",
    "https://openalex.org/W2783658781",
    "https://openalex.org/W2900694120",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W2395579298",
    "https://openalex.org/W3015572666",
    "https://openalex.org/W4310603653",
    "https://openalex.org/W4390604940",
    "https://openalex.org/W4385572850",
    "https://openalex.org/W3174777205",
    "https://openalex.org/W2039609876",
    "https://openalex.org/W1881549354",
    "https://openalex.org/W3011286504",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2952181316",
    "https://openalex.org/W2060531713",
    "https://openalex.org/W2088226047",
    "https://openalex.org/W3119952098",
    "https://openalex.org/W1969921890",
    "https://openalex.org/W2093265335",
    "https://openalex.org/W4283014919",
    "https://openalex.org/W2897337442",
    "https://openalex.org/W3118695441",
    "https://openalex.org/W2064963922",
    "https://openalex.org/W2982051641",
    "https://openalex.org/W2792143480",
    "https://openalex.org/W1971561764",
    "https://openalex.org/W2078902726",
    "https://openalex.org/W2022321294",
    "https://openalex.org/W2621769828",
    "https://openalex.org/W2604296437",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W4362522288",
    "https://openalex.org/W4221054065",
    "https://openalex.org/W3168436232",
    "https://openalex.org/W4232686413",
    "https://openalex.org/W1031578623",
    "https://openalex.org/W2141273392",
    "https://openalex.org/W6892968576",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W2206954826",
    "https://openalex.org/W2953128081",
    "https://openalex.org/W4400876473",
    "https://openalex.org/W3098269892"
  ],
  "abstract": "Abstract Generative deep learning is reshaping drug design. Chemical language models (CLMs) – which generate molecules in the form of molecular strings – bear particular promise for this endeavor. Here, we introduce a recent deep learning architecture, termed Structured State Space Sequence (S4) model, into de novo drug design. In addition to its unprecedented performance in various fields, S4 has shown remarkable capabilities to learn the global properties of sequences. This aspect is intriguing in chemical language modeling, where complex molecular properties like bioactivity can ‘emerge’ from separated portions in the molecular string. This observation gives rise to the following question: Can S4 advance chemical language modeling for de novo design? To provide an answer, we systematically benchmark S4 with state-of-the-art CLMs on an array of drug discovery tasks, such as the identification of bioactive compounds, and the design of drug-like molecules and natural products. S4 shows a superior capacity to learn complex molecular properties, while at the same time exploring diverse scaffolds. Finally, when applied prospectively to kinase inhibition, S4 designs eight of out ten molecules that are predicted as highly active by molecular dynamics simulations. Taken together, these findings advocate for the introduction of S4 into chemical language modeling – uncovering its untapped potential in the molecular sciences.",
  "full_text": "Article https://doi.org/10.1038/s41467-024-50469-9\nChemical language modeling with structured\nstate space sequence models\nRıza Özçelik 1,2, Sarah de Ruiter1,E m a n u e l eC r i s c u o l o1 &\nFrancesca Grisoni 1,2\nGenerative deep learning is reshaping drug design. Chemical language models\n(CLMs) – which generate molecules in the form of molecular strings– bear\nparticular promise for this endeavor. Here, we introduce a recent deep\nlearning architecture, termed Structured State Space Sequence (S4) model,\ninto de novo drug design. In addition to its unprecedented performance in\nvarious ﬁelds, S4 has shown remarkable capabilities to learn the global prop-\nerties of sequences. This aspect is intriguing in chemical language modeling,\nwhere complex molecular properties like bioactivity can‘emerge’ from sepa-\nrated portions in the molecular string.This observation gives rise to the fol-\nlowing question: Can S4 advance chemical language modeling for de novo\ndesign? To provide an answer, we systematically benchmark S4 with state-of-\nthe-art CLMs on an array of drug discovery tasks, such as the identiﬁcation of\nbioactive compounds, and the design of drug-like molecules and natural\nproducts. S4 shows a superior capacity tolearn complex molecular properties,\nwhile at the same time exploring diverse scaffolds. Finally, when applied\nprospectively to kinase inhibition, S4 designs eight of out ten molecules that\nare predicted as highly active by molecular dynamics simulations. Taken\ntogether, theseﬁndings advocate for the introduction of S4 into chemical\nlanguage modeling– uncovering its untapped potential in the molecular\nsciences.\nDesigning molecules with desired properties from scratch is a“needle\nin the haystack” problem. The chemical universe– estimated to com-\nprise up to 1060 small molecules1 – remains largely uncharted. Gen-\nerative deep learning offers unprecedented opportunities to explore\nthe chemical universe in a time- and cost-efﬁcient manner\n2, by enabling\nthe production of desirable molecules without the need for hand-\ncrafted design rules. In particular, chemical language models (CLMs)\nhave yielded experimentally-validated bioactive designs\n3– 7 and stood\nout as powerful molecular generators2,8– 13.\nCLMs adapt algorithms developed for sequence processing to\nlearn the“chemical language”, that is, how to generate molecules that\nare chemically valid (syntax) and possess desired properties\n(semantics)7. This is achieved by representing molecular structures as\nstring notations, such as the Simpliﬁed Molecular Input Line Entry\nSystems (SMILES14, Fig.1a), among others15,16.T h e s em o l e c u l a rs t r i n g s\nare then used for model training and subsequent generation of\nmolecules in textual form. Compared to generative methods based on\nmolecular graphs\n17, CLMs can learn more complex molecular proper-\nties better8, and generate increasingly larger molecules more\nefﬁciently18,19. These aspects have made CLMs become one of the de\nfacto approaches for de novo drug design.\nSeveral CLM architectures have been proposed for de novo\ndesign20, the most popular of which are long short-term memory\n(LSTM)3– 5,21,22 models. LSTMs are trained to produce molecular strings\nelement-by-element and have fast generation capabilities. However,\nthe iterative structure forces those models to compress the sequence\nReceived: 22 September 2023\nAccepted: 5 July 2024\nCheck for updates\n1Institute for Complex Molecular Systems and Department of Biomedical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands.\n2Centre for Living Technologies, Alliance TU/e, WUR, UU, UMC Utrecht, Utrecht, The Netherlands.e-mail: f.grisoni@tue.nl\nNature Communications|         (2024) 15:6176 1\n1234567890():,;\n1234567890():,;\ninto an information bottleneck and challenges the learning of global\nsequence properties23– 25. Transformers26, a more recent architecture,\novercome this bottleneck by processing the entire input molecular\nstring at once\n27,28. LSTMs and GPTs present different– and somewhat\ncomplementary– strengths and weaknesses when it comes to de novo\nmolecule design25,29– 32. The recurrent nature of LSTMs allows learning\nlocal properties better than GPTs, while GPTs capture global proper-\nties better thanks to their ‘holistic’ processing\n25. Moreover, while\nLSTMs remain efﬁcient, Transformers become increasingly compute-\nintensive when generating progressively longer SMILES strings, which\nmight limit their broad applicability in the chemical sciences. These\naspects make it necessary to stretch the boundaries of current CLM\napproaches further, to chart the chemical space more effectively in\nsearch for bioactive molecules\n25.\nStructured state space sequence models (S4s) are a recent\nmember of the fast-growing family of state space architectures33– 36,\nwhich are gathering increasing attention in the deep learning\ncommunity\n37– 40. S4s showed outstanding performance in audio, image,\nand text generation35 and have a“dual nature”:t h e y( 1 )a r et r a i n e do v e r\nthe entire input sequences to learn complex global properties and (2)\ngenerate one string element at a time– thereby combining some\nrespective strengths of Transformers and LSTMs. Motivated by such\n“best of two worlds” behavior, here we ask the following question: Can\nS4 advance the current state-of-the-art in chemical language model-\ning? Weﬁnd evidence that it can.\nHere, we apply S4 to chemical language modeling on SMILES\nstrings and benchmark it on various tasks relevant to drug\ndesign – from learning bioactivity to chemical space exploration and\nnatural-product design. Moreover, we further corroborate the promise\nof S4 via the prospective de novo design of kinase inhibitors, validated\nusing molecular dynamics simulations. Our results show the potential\nof S4 for chemical language modeling, especially in capturing\nbioactivity and complex molecular properties. To the best of our\nknowledge, this is theﬁrst time that state space models have been\napplied to molecular tasks, and we expect their relevance for chemical\nlanguage modeling to increase in the future.\nResults and discussion\nStructured state space sequence model (S4)\nS4s are an extension of discrete state space models, which are widely\nadopted in control engineering\n41. Discrete state space models map an\ninput sequence u to an output sequencey,t h r o u g ht h el e a r n a b l e\nparametersA 2 RN × N , B 2 RN ×1 , C 2 R1× N ,a n dD 2 R1×1 , as follows:\nxk = Axk/C0 1 + Buk\nyk = Cxk + Duk :\nð1Þ\nIn other words, discrete state space models deﬁne a “linear\nrecurrence”:a ta n ys t e pk,t h ek-th element of the input sequenceuk is\nfed into the model and used to update the hidden statexk and to\ngenerate an output,yk. The matricesA,B,C,a n dD control how the\ninput and the hidden state are combined to provide an out-\nput (Fig.1b).\nBesides their recurrent formulation, discrete state space models\ncan be formulated as a convolution with the same set of parameters. It\ncan be demonstrated that, by“unrolling” the linear recurrence (Eq. (1)),\nthe output sequencey can be obtained via a learnable convolution\nover the input sequenceu:\ny = u /C3\nK, ð2Þ\nwhere K is the convolutionﬁlter, parameterized viaA, B,a n dC (see\nSupplementary Eqs. (1)– (4) for a detailed derivation). This convolu-\ntional representation reveals a key aspect of state space models: they\nFig. 1 | Key concepts of structured state space sequence (S4) models for che-\nmical language modeling. aSimpliﬁed Molecular Input Line Entry System\n(SMILES) strings14, used as the chemical language. SMILES strings are obtained by\ntraversing the molecular graph and annotating atom types, rings, and bond types in\nthe form of text.b S4 for de novo SMILES design. During training, S4 is formulated\nas a global convolution and processes the whole molecular string simultaneously.\nThe global convolutionﬁlter K is parameterized via the matricesA, B,a n dC (Eq.\n(2)). During the generation, S4 switches to the recurrent formulation (via the same\nparameters, (Eq. (1)) and produces SMILES strings element-by-element for more\nefﬁcient and effective chemical space exploration.c Computational pipeline, where\nS4 was used to learn from known SMILES strings and generate new molecules\nde novo.\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 2\nlearn explicitly from the entire sequence (via global convolution) while\npreserving recurrent generation capabilities (Fig.1b).\nLearning the optimal parameters of a discrete state space system,\nhowever, introduces vanishing gradients and numerical instabilities in\nrecurrent and convolutional formulations, respectively. Structured\nstate space sequence models, (S4s)35, tackle those issues by introdu-\ncing additional structure to the model parameters (via the so-called\nhigh-order polynomial projection operators\n33) and reducing the\nunstable computations to the stable Cauchy kernel42 computation (see\nref. 35 for more detail). Ablation studies35 have shown the relevance of\nthe added structure to achieve computational feasibility and perfor-\nmance on long sequences. Moreover, such reduction allows S4 to\naddress numerical instabilities encountered in model training and\nmade S4 state-of-the-art in several generative tasks that require\nlearning long-distance relationships\n33– 35. Motivated by its performance\nin other domains and the potential beneﬁts of its dual structure, here\nwe introduce S4 to the molecular sciences for theﬁrst time.\nWe evaluated S4 for its ability to learn from and generate drug-like\nmolecules and natural products in an array of tasks, and in terms of\nmultiple molecular properties. LSTMs and Generative Pretrained\nTransformers (GPTs) were used as benchmarks, since they are the de\nfacto approaches in chemical language modeling for de novo\ndesign\n2,7,8,25. Furthermore, LSTM (recurrent training and generation)\nand GPT (holistic training and generation) constitute the ideal\nbenchmarks for S4, due to S4’s dual formulation (convolution during\ntraining and recurrence during generation), which allows inspecting\nthe effect of each of these aspects on the overall performance. Finally,\nthe prospective de novo design of putative mitogen-activated protein\nkinase 1 (MAPK1) inhibitors, corroborated by molecular dynamics\nsimulations, was performed to test the potential of S4 in real-world\ndrug discovery scenarios.\nDesigning drug-like molecules\nS4 was analyzed for its ability to design drug-like small molecules\n(SMILES length lower than 100 tokens) extracted from ChEMBL\ndatabase\n43, by focusing on its ability to (1) learn the chemical syntax, (2)\ncapture structural features relevant for bioactivity, and (3) designing\nstructurally diverse molecules.\nLearning the SMILES syntax. All investigated CLMs were trained on\n1.9M canonical SMILES strings extracted from ChEMBL v3143.T h e\ngenerated strings were evaluated according to their (1) validity, i.e., the\nnumber (and frequency) of SMILES corresponding to chemically valid\nmolecules; (2) uniqueness, which captures the number (and fre-\nquency) of structurally-unique molecules among the designs; and (3)\nnovelty, corresponding to the number (and frequency) of unique and\nvalid designs that are not included in the training set. A high number of\n“chemically-valid” designs suggests that the model has learned how to\ngenerate plausible molecules, while high uniqueness and novelty\nvalues indicate little redundancy among the designs and with the\ntraining set, respectively. Although these metrics are vulnerable to\ntrivial baselines\n44, they provide insights into a model’sc a p a c i t yt ol e a r n\nthe SMILES“syntax”.\nAll CLMs generated more than 91% valid, 91% unique and 81%\nnovel molecules (Table1). Moreover, their designs approximated the\ntraining and test sets in terms of selected molecular properties (i.e.,\noctanol-water partition coefﬁcient\n45, quantitative estimate of drug-\nlikeness46,B e r t zc o m p l e x i t y47, and synthetic accessibility48,49)w i t hn o\nnotable differences among architectures (Supplementary Fig. 1 and\nSupplementary Table 1). These results agree with the literature on\nCLMs (e.g., refs.2,50) and demonstrate the robustness of the model\ntraining procedure. S4 designs the most valid, unique, and novel\nmolecules, by generating more novel molecules than the benchmarks\n(from approximately 4000– 12,000 more), and displays a good ability\nto learn the“chemical syntax” of SMILES strings. The potential of S4 in\ncomparison with existing de novo design approaches was further\ncorroborated on the MOSES benchmark\n51, where S4 consistently\nscored among the top-performing deep learning approaches (Sup-\nplementary Table 2).\nTo shed additional light on the strengths and limitations of S4 in\ncomparison with the benchmarks, we analyzed the sources of invalid\nmolecule generation for all methods in terms of branching and ring\nerrors, erroneous bond assignment, and other (miscellaneous) syntax\nissues (Fig. 2). Interestingly, each method seems to show different\ntypes of errors leading to SMILES invalidity. LSTM struggles the most\nTable 1 | Designing drug-like molecules de novo with S4\nModel Valid Unique Novel\nS4 99,268 (97%) 98,712 (96%) 95,552 (93%)\nLSTM 97,151 (95%) 96,618 (94%) 82,988 (81%)\nGPT 93,580 (91%) 93,263 (91%) 91,590 (89%)\nThe results of LSTM and GPT models on the same tasks are reported for comparison. Each model\nwas trained on 1.9M SMILES strings from ChEMBL and used to generate 102,400 SMILES strings\nde novo. The number and percentage of valid, unique, and novel molecular designs are\nreported. The best value per metric is highlighted in boldface.\nFig. 2 | SMILES design errors, grouped by category and CLM architecture.Each CLM was trained on ChEMBL and used to design 102,400 SMILES strings. The invalid\ndesigns were categorized per error, and the reported values indicate the number of errors in each category.\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 3\nwith branching, and performs the best with bond assignment, while\nGPT struggles the most with rings and bond assignment, and has\nintermediate performance otherwise. S4 struggles more than LSTM\nwith bond assignment, and generates remarkably fewer errors than\nboth benchmarks in branching and ring design. Our hypothesis is that\nbond assignment indicates good learning of“short-range” dependen-\ncies, while branching and ring opening and closure require better\ncapturing of the “long-range” relationships. This suggests that S4\ncaptures relatively “long-distance” relationships well, in agreement\nwith existing evidence in other domains\n33– 35.\nCapturing bioactivity. We evaluated S4 for its ability to learn elements\nof bioactivity. With CLMs this is often achieved with transfer learning52,\nwhich allows transferring knowledge acquired from one task to\nanother task with fewer available data. Via transfer learning, after pre-\ntraining a CLM on a large corpus of SMILES strings, the model can be\nthen “ﬁne-tuned” on a smaller, and task-focused set (e.g., bioactive\nmolecules) by additional training\n22. Here, we performed ﬁve ﬁne-\ntuning campaigns, focusing on distinct macromolecular targets from\nthe LIT-PCBA\n53 dataset: (1) pyruvate kinase muscle isoform 2 (PKM2),\n(2) mitogen-activated protein kinase 1 (MAPK1), (3) glucocer-\nebrosidase (GBA), (4) mechanistic target of rapamycin (mTORC1), and\n(5) cellular tumor antigen p53 (TP53).\nEvaluating the bioactivity of de novo designs (besides synthesis\nand wet-lab testing) is non-trivial, since this property cannot be fully\ncaptured by traditional molecular descriptors, and might not be\naccurately predicted by quantitative structure-activity relationship\nmodels\n54,55. Hence, we used experimentally-tested molecules to eval-\nuate the capacity of a CLM to learn elements of bioactivity retro-\nspectively. Several studies have shown that the likelihoods learned by a\nCLM duringﬁne-tuning can be used to prioritize designs with high\nchances of being bioactive\n6,56,57. Based on the same principle, here we\nused the likelihoods learned by the CLMs to rank existing molecules\nand evaluate their capacity to prioritize bioactive compounds over\ninactive ones.\nFor each of the selected targets, bioactive molecules (Supple-\nmentary Table 3) were used forﬁne-tuning, with ten random training-\nvalidation-test splits. Afterﬁne-tuning the CLMs on each target, for\neach training-test split, we proceeded as follows:\n(1) With each ﬁne-tuned model and per each target, we predicted\nthe likelihoods (Eq. (4)) of the SMILES strings in the respective test\nset. The considered test sets resemble a real-world scenario in\nterms of hit-rate, and they comprise 11 (mTORC) to 56\n(PKM2) active molecules and 10,240 inactive molecules (except\nfor TP53, containing 3301 inactive molecules, Supplementary\nTable 3);\n(2) We ranked the molecules of the test set according to the pre-\ndicted likelihoods (Eq. (5));\n(3) For each target and each test set, we computed the fraction of\nactives ranked among the top 10, top 50, and top 100 molecules.\nThe higher the number of active molecules ranked in early por-\ntions of the test set by a CLM, the better the model has learned\nwhat is relevant for bioactivity on the investigated target after\nﬁne-tuning.\nOur results show variable performance depending on the target\n(Fig. 3). The most challenging target is TP53, on which no model could\nconsistently retrieve actives among the top 10 scoring molecules.\nNotably, this target has the most challenging test set, where inactive\nmolecules are similar to the actives of both the training and the test\nsets (Supplementary Fig. 2), potentially indicating the presence of\nactivity cliffs\n58. MAPK1 and mTORC1 also challenge the CLMs; here, S4\nretrieved more active molecules than the benchmarks, especially in the\nearly portions of the test set. PKM2 and GBA are the easiest datasets;\nhere, all CLMs identiﬁed bioactive molecules in their top 10, with S4\nachieving the highest median across the board. A Wilcoxon signed-\nranked test\n59 on the pooled scores across datasets supports the\nsuperior performance of S4 compared to the benchmarks (p [top\n10] = 8.41e−6, p [top 50]= 2.93e−7, p [top 100] = 1.45e−7c o m p a r e dt o\nLSTM, andp [top 10] = 2.33e−3, p [top 50] = 3.72e−3, p [top 100] = 2.61e\n−2 compared to GPT), and of GPT compared to LSTM ( p [top\n10] = 5.22e−3, p [top 50] = 3.75e−5, p [top 100] = 2.02e−6).\nUnder the constraints of the study design, these results indicate\nthat processing the input SMILES“holistically” (as GPT and S4 do) leads\nto capturing complex properties like bioactivity better, with a better\nperformance obtained by S4.\nChemical space exploration. We analyzed the ability of S4 to explore\nthe chemical space, in terms of generating structurally diverse and\nbioactive molecules. To this end, we employed a commonly-used\nstrategy with CLMs, that is, varying the sampling temperature (T)t o\ncontrol chemical diversity\n60. T affects which elements of a string are\ngenerated by a weighted random sampling (Eq. (3)). WhenT → 0t h e\nmost likely element (based on the CLM prediction) is selected as the\nnext element of the sequence, while the higher theT, the more random\nthe selections.T = 1 corresponds to using the CLM predictions as the\nsampling probability of each element at each generation step.\nWe experimented with an increasing sampling temperatures\n(from T =1 . 0t oT = 2.0 with a step of 0.25). EachT value was used to\ngenerate 10,240 SMILES strings per model across theﬁve chosen tar-\ngets and all training-test splits. Then, we evaluated the designs based\non three metrics (Fig.4):\nFig. 3 | Retrospective enrichment analysis for all models acrossﬁve selected\nmacromolecular targets.The ﬁne-tuned models were used to rank the held-out\nactives and inactives of the respective protein targets. The percentage of known\nactives ranked per considered number of test set molecules (10, 50, 100) was\ncomputed across ten runs. Bar heights report the median across runs and error bars\nreport theﬁrst and third quartiles (n = 10). Source data are provided as a Source\nData ﬁle.\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 4\n The validity of the generated strings, which captures how robust\nthe model is to increasing degrees of randomness in preserving a\ncorrect syntax. The higher the validity, the better.\n Rediscovery rate: de novo design models are often evaluated for\ntheir capacity to reproduce existing molecules with experimen-\ntally veriﬁed biological activities\n50,55. For this purpose, we used the\nheld-out actives previously described for each target. Moreover,\nto “relax” the criterion of rediscovery, we considered held-out\nactives with substructure similarity higher than 60% to a de novo\ndesign (as computed via Tanimoto similarity on extended\nconnectivity ﬁngerprints\n61) to compute rediscovery. Higher\nrediscovery rates at increased temperature values indicate that\nthe model can explore regions related to bioactivity despite\nincreased randomness.\n Scaffold diversity: designing molecules with novel scaffolds bears\nrelevance in lead identiﬁcation\n62, and can be used as a proxy to\nevaluate CLMs51. Here, to have a better evaluation of what con-\nstitutes a novel scaffold, the novel designs were grouped in clus-\nters based on their scaffold similarity. This was achieved via\nhierarchical clustering, to group designs with similar Bemis-\nMurcko scaffolds\n63 (as computed via the Tanimoto similarity on\nthe corresponding extended connectivityﬁngerprints61 higher\nthan 60%). Only novel and unique scaffolds were considered. We\nthen counted the number of obtained scaffold clusters, the\nhigher, the better.\nThe models display similar trends with increasingT values for all\nthe analyzed factors across datasets, with varying magnitude (Fig.4).\nIn general, the validity decreases with increasing temperature (as\npreviously observed\n60), with the highest effect observed for GPT\n(median validity across training setups getting lower than 40%, Fig.4a).\nBoth S4 and LSTM show higher robustness than GPT to increasing\ntemperature values (with LSTM performing slightly better forT ≥ 1.75),\nsuggesting that sequential generation can boost chemical space\nexploration. S4 outperforms LSTM in terms of rediscovery rate\n(Fig. 4b), in agreement with our previous results on bioactivity (Fig.3).\nWe also compute the exact rediscovery rate (identical molecular\nstructure) and observe that no model can consistently generate held-\nout actives. When it comes to the diversity of the designs (Fig.4c),\nLSTM can generate the highest number of structurally unique scaffolds\n(median across datasets and setups: 6602 clusters,T = 1.75) and S4 is\nthe close second-best model (6520 clusters, T = 1.75). While GPT\nobtains a suboptimal performance across the board, LSTM seems\nbetter for chemical space exploration when bioactivity is not the main\nobjective, while S4 can better capture bioactivity and preserve a good\nchemical space exploration at the same time, combining the strengths\nof the two benchmarks with its dual structure. These results conﬁrm\nthe promise of S4 when it comes to generating structurally diverse and\nbioactive drug-like molecules.\nDesigning natural products\nS4 was further tested on more challenging molecular entities than\ndrug-like molecules. To this end, we evaluated its capacity to design\nnatural products (NPs), which are invaluable sources of inspiration for\nmedicinal chemistry\n64,65. Compared to synthetic small molecules, NPs\ntend to possess more intricate molecular structures and ring systems,\nas well as a larger fraction of sp\n3-hybridized carbon atoms and chiral\ncenters66– 68. These characteristics correspond to longer SMILES\nsequences on average, with more long-range dependencies, and make\nnatural products a challenging test case for CLMs\n19,69.\nWe trained the CLMs on large natural products (32,360 SMILES\nstrings with length > 100, chosen to complement the previous analy-\nsis) from the COlleCtion of Open Natural ProdUcTs (COCONUT)\ndatabase\n70. We then used the CLMs to design 102,400 SMILES strings\nde novo and computed the fraction of valid, unique, and novel designs\n(Table 2). All CLMs can design natural products, with lower perfor-\nmance compared to drug-like molecules. S4 designs the highest\nnumber of valid molecules by approximately 6000 to 12,000 mole-\ncules (7– 13% better), and LSTM achieves the highest novelty by\napproximately 2000 molecules (2%) over S4.\nTo further investigate the characteristics of the designs, we\ncomputed the natural-product likeness\n71, which captures how similar a\nmolecule is to the chemical space covered by natural products in terms\nof its substructures (the higher the NP-likeness, the more similar). The\nnovel designs of S4 have signiﬁcantly higher values of NP-likeness than\nthe benchmarks (Mann – Whitney U test, p =1 . 4 1 e−53 compared\nto LSTM, andp =1 . 0 2 e−82 compared to GPT), closer to the values of\nthe training and test sets on average (Table2). Moreover, the NP-\nlikeness values better match the distribution of the COCONUT mole-\ncules in terms of Kolmogorov– Smirnov (KS) distance\n72,w h i c hq u a n t i -\nﬁes how much the cumulative distributions of two observations differ\n(between 0% and 100%; the lower, the closer the distributions).\nIn addition to NP-likeness, we evaluated the novel designs in terms\nof several structural properties important for natural products66– 68,\nnamely: the number of sp3-hybridized carbon atoms, aliphatic rings,\nFig. 4 | Model performance when varying the temperature value.Each model\nwas analyzed for its performance when varying the temperature values from 1.0 to\n2.0, with a step of 0.25, and by sampling 10,240 molecules.a Analysis of the SMILES\nvalidity across temperature.b Variation of rediscovery rate. The models were\nevaluated for their capability to rediscover bioactive molecules not used for model\ntraining or design molecules similar in structure (i.e., with a Tanimoto similarity on\nextended connectivityﬁngerprints higher than 60%).c Analysis of the number of\ndiverse groups of scaffolds generated per method. Scaffolds were clustered toge-\nther if they had a Tanimoto similarity (computed on extended connectivityﬁn-\ngerprints) larger than 60%. For each plot, the solid line indicates the median\nobtained across theﬁve analyzed protein targets (PKM2, MAPK1, mTORC1, and\nTP53) with ten runs each (n = 50), and the shaded area indicates the inter-quartile\nrange. The statistics per individual target can be found in Supplementary Fig. 3.\nSource data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 5\nspiro atoms and heavy atoms, as well as the molecular weight and the\nsize of the largest fused ring system. These properties provide addi-\ntional evidence on the molecular characteristics of the designs, and\nt h e i rs t r u c t u r a lc o m p l e x i t yi nc o m p a r i s o nw i t ht h et r a i n i n gn a t u r a l\nproducts. Here, S4 achieved the lowest KS distance to the training and\ntest sets across properties, indicating that its designs match the\ntraining natural products best. These results conﬁrm the ability of S4\nto learn complex molecular properties for de novo design.\nFinally, we analyzed the training and generation speed of the CLM\narchitectures when increasing the SMILES length, to test their practical\napplicability when designing bigger molecules, like natural products.\nOur analysis highlighted that S4 is as fast as GPT during training (both\nare approximately 1.3 times faster than LSTM), and the fastest in terms\nof generation (Supplementary Fig. 4), thanks to its dual formulation.\nThis further advocates for the introduction of S4 as an efﬁcient\napproach for molecule design, that“makes the best of both worlds”\ncompared to GPT and LSTM.\nProspective de novo design\nWe conducted a prospective in silico study with S4, focused on\ndesigning inhibitors of mitogen-activated protein kinase 1 (MAPK1), a\nrelevant target for oncological therapies\n73. The putative bioactivity of\nthe designs was then evaluated via molecular dynamics (MD).\nThe S4 model previously pre-trained on ChEMBL wasﬁne-tuned\nwith the SMILES strings of 68 manually-curated inhibitors from\nChEMBL, having an experimental constant of inhibition (Ki)l o w e rt h a n\n1 μMo nM A P K 1 .T h el a s tﬁve epochs of theﬁne-tuned model were then\nused to generate 256K molecules (51,200 designs per eachT value,\nranging from 1.0 to 2.0 with a step of 0.25).\nThe designs were ranked and ﬁltered via log-likelihood\nscore (Eq. (5)) and scaffold similarity to the training set (see“Materi-\nals and methods” for further details). The ten top-scoring molecules\n(1– 10,F i g .5aa n dT a b l e3) were considered for further characterization\nusing MD simulations. As a reference for evaluation, we performed\nMD simulations also for the closest ﬁne-tuning neighbor of the\nconsidered designs (compounds 11– 16, selected based on\nscaffold similarity; Fig.5aa n dT a b l e3) .T h ea b s o l u t ep r o t e i n - l i g a n d\nbinding free energy (expressed asΔG – t h el o w e rt h es t r o n g e rt h e\npredicted binding) for molecules1– 16 was computed via Umbrella\nSampling\n74 (Table 3). The computedΔG values for known bioactive\nmolecules (11– 16) have a good correspondence with experimentalKi\nvalues from ChEMBL (Table3), conﬁrming the validity of the chosen\nMD protocol.\nEight out of ten designs (except1 and 5) showed a high predicted\nafﬁnity (Table3), withΔG values ranging fromΔG = −10.3 ± 0.6 kcal mol−1\n(7)t oΔG = −23 ± 4 kcal mol−1 (2). Interestingly, these afﬁnities are com-\np a r a b l eo re v e ns u r p a s st h o s eo ft h ec l o s e s ta c t i v en e i g h b o r\n(ΔG = −9.1 ± 0.8 kcal mol\n−1 to ΔG = −13 ± 2 kcal mol−1). The global sub-\nstructure similarity (measured on extended connectivityﬁngerprints) of\nthe designs to their closest neighbor ranges from 31% (10)t o8 7 %(4,\nTable3).\nThe most potent design according to MD predictions is molecule\n2 (ΔG = −23 ± 4 kcal mol−1,T a b l e3). This molecule– which is the largest\no n ea m o n gt h ed e s i g n s( F i g .5a) – engages extensively with the binding\npocket of MAPK1 (Fig.5b), which explains the remarkably favorable\npredicted afﬁnity. Design2 has a limited substructure similarity to its\nclosest bioactive neighbor (molecule12, similarity equal to 57%);\nhowever, its synthetic accessibility may be limited. Design 3 is\nTable 2 | Natural product design with CLMs\nMetric S4 LSTM GPT Training Test\nSyntax Valid 82,633 (81%) 76,264 (74%) 70,117 (68%) n.a. n.a.\nUnique 53,293 (52%) 51,326 (50%) 50,487 (49%) n.a. n.a.\nNovel 40,897 (40%) 43,245 (42%) 43,168 (42%) n.a. n.a.\nNP likeness Value 1.6 ± 0.7 1.5 ± 0.7 1.5 ± 0.7 1.6 ± 0.7 1.6 ± 0.7\nKStrain 4.03% 5.89% 9.44% 0.00% 0.81%\nKStest 4.51% 6.60% 10.13% 0.81% 0.00%\nNo. sp3 carbons Value 42 ± 16 44 ± 17 43 ± 16 38 ± 16 37 ± 15\nKStrain 13.96% 17.31% 14.51% 0.00% 1.02%\nKStest 14.08% 17.45% 14.34% 1.02% 0.00%\nNo. aliphatic rings Value 6 ± 4 6 ± 4 6 ± 4 7 ± 4 6 ± 4\nKStrain 5.65% 6.91% 8.12% 0.00% 1.08%\nKStest 5.41% 6.25% 7.56% 1.08% 0.00%\nNo. spiro atoms Value 0.3 ± 0.9 0.3 ± 0.8 0.3 ± 0.7 0.6 ± 1.2 0.6 ± 1.2\nKStrain 10.81% 12.88% 12.71% 0.00% 0.21%\nKStest 10.87% 12.93% 12.77% 0.21% 0.00%\nMolecular weight Value 1114 ± 315 1180 ± 360 1119 ± 307 1061 ± 295 1063 ± 290\nKStrain 9.23% 16.97% 11.02% 0.00% 1.40%\nKStest 9.04% 16.67% 10.75% 1.40% 0.00%\nSize of the fused ring system Value 5 ± 2 5 ± 2 5 ± 2 5 ± 2 5 ± 2\nKStrain 8.05% 9.42% 11.19% 0.00% 0.60%\nKStest 7.93% 9.44% 11.21% 0.60% 0.00%\nNo. heavy atoms Value 78 ± 22 83 ± 25 79 ± 21 75 ± 20 75 ± 20\nKStrain 7.76% 15.81% 9.73 0.00% 1.24%\nKStest 7.30% 15.34% 9.31 1.24% 0.00%\nThe models were trained on 32,360 natural product SMILES strings from the COCONUT database70 and used to generate 102,400 SMILES strings de novo. The number and fraction of valid, unique,\nand novel molecular designs are calculated for each model. For the designs, the mean and standard deviation of (1) the natural-product-likeness values71, (2) the number of sp3 carbons, (3) the\nnumber of aliphatic rings, (4) the number of spiro atoms, (5) molecular weight, (6) size of the largest fused ring system, (7) the number of heavy atoms and the corresponding Kolmogorov– Smirnov\ndistance to the training and test sets (KStrain and KStest, respectively) are reported. The same statistics from train and test sets (32,360 and 5000 natural products, respectively) are reported for\ncomparison. For each CLM and each metric, the best value is highlighted in boldface. All descriptors were computed on valid, unique, and novel SMILES.\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 6\npredicted with the second highest afﬁnity (ΔG = −19.6 ± 0.9 kcal mol−1),\nand it shares the same scaffold of compound13. Design3 differs from\n13 by the replacement of the ether and hydroxy moieties with two\nﬂuorine atoms, and the addition of a methoxy group (Fig.5a, global\nsimilarity equal to 65%). Interestingly, this structural modiﬁcation\nleads to an improvement of the predictedΔG value (of approximately\n−10 kcal mol−1), possibly due to the ability to penetrate deeply into the\nbinding pocket thanks to theﬂuorine atoms (Fig.5c). Halogens are, in\nfact, favorable for MAPK1, as evident from theﬁne-tuning molecules\n(91% of them containing at least one halogen) and existing literature\n(e.g., refs.75– 78). Evidence of a favorable positioning of halogens is\ns h o w no nb o t ht h e“top”\n75,76 and “bottom”77,78 of the binding pocket,\nfurther supporting the predicted afﬁnity of compound3.\nDesign9 (ΔG = −17 ± 2 kcal mol−1) features halogens on both sides,\nunlike its closest neighbor, molecule13 (ΔG = −10.5 ± 0.7 kcal mol−1,\nglobal similarity equal to 33%), from which it also differs in the moiety\nattached to the pyridonic ring (Fig.5a). When inspecting the predicted\nbinding pose, it can be observed that the aromatic ring with halogen\nsubstituents, hydroxyl, and carbonyl of pyridone are situated in the\nsame region of the binding groove (Fig.5d). The difference inΔG\nvalues (approximately 6.5 kcal mol\n−1 in favor of design9)c o u l db e\nascribed, like with molecule3, to the presence of halogens in the lower\nbinding pocket region. This might also explain the high predicted\nafﬁnity of design10 (ΔG = −15 ± 2 kcal mol\n−1) – which differs from9 by a\ncarbonyl and a methyl group.\nTable 3 | In silico prospectivestudy on designing mitogen-\nactivated protein kinase (MAPK1) inhibitors with S4\nS4 design Most similar training\nactive\nScaffold\nSimilarity\nGlobal\nSimilarity\nID ΔG\n[kcal mol−1]\nID ΔG\n[kcal mol−1]\nKi\n[nM]\n1 −5.6 ± 0.9 11 −9.1 ± 0.8 0.1 79% 65%\n2 −23 ± 41 2 −12 ± 2 0.4 63% 57%\n3 −19.6± 0.9 13 −10.5 ± 0.7 3.0 100% 65%\n4 −13 ± 21 4 −11 ± 3 2.5 90% 87%\n5 −7±2 15 −13 ± 2 0.6 73% 85%\n6 −11 ± 14 −11 ± 3 2.5 56% 56%\n7 −10.3 ± 0.6 11 −9.1 ± 0.8 0.1 50% 52%\n8 −11.2 ± 0.4 15 −13 ± 2 0.6 58% 72%\n9 −17 ± 21 3 −10.5 ± 0.7 3.0 41% 42%\n10 −15 ± 21 6 −9.1 ± 0.2 63.0 30% 31%\nThe absolute binding free energy of interaction (ΔG, the lower, the better) was determined via\nUmbrella Sampling. Values are reported as the average over three repeats, along with the\ncorresponding standard deviation. De novo designs1– 10 are compared with the closest inhibitor\nfrom theﬁne-tuning set, selected based on the scaffold similarity (Tanimoto coefﬁcient on\nextended connectivityﬁngerprints [ECFPs], using Bemis-Murcko scaffolds63). The\nexperimentally-determined inhibition of MAPK1 for compounds11– 16 (constant of inhibition,Ki)\nare also reported. Designs whose predictedΔG is remarkably better than that of the closest\nbioactive neighbor are highlighted in boldface. Global substructure similarity to the closest\nneighbor (as measured by the Tanimoto coefﬁcient on ECFPs) is reported.\nO\nN\nH\nN\nN\nN\nO\nN\nO\nOH\nO\n1\nCl\nF\nN\nHO\nO\nH\nN\nOH\nN\nN\nN\nO OH\nF\nCl\nO\nO O\n2\nN\nN\nNH\nF\nF\nN\nO\nOH\nO F\nCl\n3\nN\nN\nN\nO\nOH\nN\nO\nH\nN\nSO\nO\nOH\nCl\nF\n4\nS\nO O\nHN\nN\nO N\nO O\nOH\nCl\nFN\nOHO\nN\nN\nN\nO\nCl\nF\nN\nH\nN\nO OH\nF\nCl\nHO\nCl\nN\nO\nN\nO\nN\nN\nH\nN\nO\nOH\nF\nCl\nN\nOHO\nN\nH O\nHO\nCl\nF\nF\nO\nH\nN N\nN\nN\nO OH\nCl\nO\nH\nN N\nN\nN\nO OH\nCl\nF\nO\nNH\nHO\nN\nN N\nO\nOH\nF\nCl\nS\nO\nO\nH\nN\nN\nN\nN\nO OH\nCl\nF\nO NH\nN\nN\nN\nO\nOH\nHN\nF\nCl\nCl\nN\nNH\nH\nN\nO\nN\nH F\nOH\nCl\nN\nOHO\nN\nH OH\nO\nN\nH\nN\nO F\na\n5 6 7 8\n9 10 11 12\n13 14 15 16\nbcd\n9\n13\n2\n12\n3\n13\nFig. 5 | Prospective de novo design of putative MAPK1 inhibitors with S4.\na Selected de novo designs (molecules1– 10) for further characterization. For each\nde novo design, its most similar training MAPK1 inhibitor (as reported in Table3)i s\ndepicted (compounds11– 16, gray box). The ligand binding pose (obtained via\nUmbrella Sampling) of selected designs interacting with MAPK1 (PDB-ID = 2Y9Q), in\ncomparison with their most similar bioactive molecule from theﬁne-tuning set is\nalso depicted:b Design 2 (green) compared with compound12 (gray). c Design 3\n(yellow) compared with compound13 (gray). d Design 9 (blue) compared with\ncompound 13 (gray).\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 7\nWith 8 out of 10 designs predicted as bioactive on the intended\ntarget by MD, with comparable or higher predicted afﬁnities than their\nclosest ﬁne-tuning molecules, these results further corroborate the\npotential of S4 for de novo drug design.\nOpportunities for molecular S4\nIn conclusion, this study pioneered the introduction of state space\nmodels into chemical language modeling, with a focus on structured\nstate spaces (S4s). The unique dual nature of S4s, involving convolu-\ntion during training and recurrent generation, makes them particularly\nintriguing for de novo design starting from SMILES strings.\nOur systematic comparison with GPT and LSTM on a variety of\ndrug discovery tasks revealed S4’s strengths: while recurrent genera-\ntion (LSTM and S4) is superior in learning the chemical syntax and\nexploring diverse scaffolds, learning holistically on the entire SMILES\nsequence (GPT and S4) excels in capturing certain complex properties,\nlike bioactivity. S4 with its dual nature, makes the best of both worlds”:\nit demonstrated comparable or better performance than LSTM in\ndesigning valid and diverse molecules, and systematically out-\nperformed both benchmarks in capturing complex molecular prop-\nerties – all while maintaining computational efﬁciency.\nThe application of S4 to MAPK1 inhibition, validated by MD\nsimulations, further showcases its potential to design potent bioactive\nmolecules. In the future, we will apply S4 prospectively in combination\nwith wet-lab experiments to enhance its impact in theﬁeld. Strategies\nto increase the structural diversity of the considered designs, such as\nSMILES augmentation\n79 and improved ranking protocols, could fur-\nther boost its potential in medicinal chemistry.\nSeveral aspects of S4 await to be explored in the molecular sci-\nences, such as its potential with longer sequences (e.g., macrocyclic\npeptides and protein sequences) and on additional molecular tasks\n(e.g., organic reaction planning\n80 and structure-based drug design81).\nIn the future, we envision the relevance of S4 for molecule dis-\ncovery to increase, and to potentially replace widely established che-\nmical language models like LSTM and GPT. We believe that the\nprovided open-access code will contribute to the adoption and\nexpansion of S4, to further stretch the boundaries of chemical lan-\nguage modeling.\nMethods\nDesigning drug-like molecules\nData curation. The pre-training set was generated starting from\nChEMBL v3143. Fine-tuning datasets were extracted from LIT-PCBA53.\nAll sets were generated by (1) retaining molecules containing selected\natoms (C, H, O, N, S, P, F, Cl, Br, and I), (2) removing salts and dis-\nconnected structures, as well as stereochemistry annotations and\ncharge, (3) retaining molecules whose canonicalized SMILES strings\ncontained 100 tokens or fewer. After sanitization, canonicalization,\nlabel encoding, and padding (to 100), molecules were randomly split\ninto training, validation, and test sets. For ChEMBL, this led to a\ntraining set of 1,900,000, and a validation and a test set of 100,000\nand 23,680 molecules, respectively. The number of compounds for\neach ﬁn e - t u n i n gc a m p a i g ni sr e p o r t e di nS u p p l e m e n t a r yT a b l e3 .\nTraining\nPretraining. The hyper-parameters of the LSTM and GPT were tuned\nwith random search for 5 days on a single NVIDIA A100 40GB GPU. The\ndeﬁned hyper-parameter space is based on previous work\n27,57,60,82\n(Supplementary Table 4). 40 LSTM and 35 GPT models were optimized\nwithin a 5-day limit. Hyper-parameter search was conducted to max-\nimize the validity during pre-training.\nTo account for the lack of previous information on optimal hyper-\nparameters for molecule generation with S4, we implemented a two-\nstep procedure for hyper-parameter tuning. First, 242 models were\ntrained to prioritize hyper-parameters (see Supplementary Table 4).\nHigh-performing hyper-parameter values in terms of validation accu-\nracy were advanced to the second phase, where 108 experiments were\nconducted. Hyper-parameter search was conducted for 10 days on\nmultiple NVIDIA A100 40GB GPUs to maximize the validity during pre-\ntraining.\nFine-tuning.F i v eﬁne-tuning campaigns were conducted onﬁve tar-\ngets: PKM2, MAPK1, GBA, mTORC1, and TP53. For each target, ten runs\nwith different training (80%), validation (10%), and test (10%) splits\nwere performed (except for PKM2 where we used 70%– 15%– 15% due to\nlimited data). Early stopping on the validation cross-entropy was\nadopted with a patience ofﬁve epochs and a tolerance of 10\n−5.\nTemperature sampling. The sampling probability (p) of each i-th\nelement at any step of the sequence was computed as follows:\npi = eð yi=TÞ\nP\njeð yj =TÞ ð3Þ\nwhere yi is the predicted probability of theith element,T is the sam-\npling temperature, andj runs over all tokens in the vocabulary.\nMolecule ranking with log-likelihoods. The molecules were ranked\nbased on the joint likelihood of the tokens (i.e., SMILES characters)\nthey contain\n82. For each test molecule, the joint log-likelihood (L)b ya\nmodel (M) was computed as:\nLðMÞ =\nX\ni\nlog pðtiÞ ð4Þ\nwhere ti is theith token of the SMILES string of a given test molecule\nand p(ti) is the probability of that token as predicted by the modelM; i\nruns over all the elements in the molecular string.\nTo only consider theﬁne-tuning information and remove poten-\ntial pre-training bias (as previously observed82), the pre-training log-\nlikelihood was subtracted from theﬁne-tuning likelihood, to obtain a\nﬁnal score:\nLscoreðMÞ = LðMft Þ/C0 LðMptÞ ð5Þ\nwhere Mft is theﬁne-tuned model andMpt is the pre-trained model.\nThe obtainedLscore was used to rank each test molecule, the higher the\nLscore, the better the rank.\nNatural product design\nThe COlleCtion of Open Natural ProdUcTs (COCONUT)70 database was\nused for model training. Salts, disconnected structures, stereo-\nchemistry, and charge annotations were removed. Molecules with\ncanonical SMILES strings longer than 100 characters were used to train\nthe models. A random search strategy was adopted to tune the hyper-\nparameters of all models, as previously explained. The models were\ngiven a 5-day limit on a cloud NVIDIA A100 GPU and 1024 strings were\ngenerated by each model. The models with the highest SMILES validity\nwere selected for further evaluation.\nProspective de novo design\nData curation. Fine-tuning data were collected from ChEMBL v3343.A l l\nannotations for MAPK1 were retained (target ID: CHEMBL4040).\nAvailable assay descriptions were manually inspected and analyzed.\nMolecules whose inhibitory constant (K\ni) was lower than 1μMo n\nreliable inhibition assays (CHEMBL3412886, CHEMBL917079) were\nretained. SMILES canonicalization and removal of stereochemistry and\nduplicates led to a set of 68 unique SMILES strings forﬁne-tuning\n(SMILES strings available in the dedicated GitHub repository).\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 8\nModel ﬁne-tuning and de novo design.T h eﬁne-tuning dataset was\nsplit into ten train and validation splits (80– 20%) toﬁnd the optimal\nnumber ofﬁne-tuning epochs. Early stopping on validation loss was\nused, with patience ofﬁve epochs and tolerance of 10−5. The experi-\nments suggested 45 epochs to be optimal; the pre-trained model was\nﬁne-tuned on the whole dataset accordingly.\nThe models of the lastﬁve ﬁne-tuning epochs were used to design\nmolecules. A total of 10,240 designs for temperature values ranging\nfrom 1.0 to 2.0 (step size 0.25) were generated per temperature and\nmodel, totaling 5 × 5 × 10, 240 = 256K designs. The novel and unique\nmolecules among those designs were ranked by theirﬁne-tuning log-\nlikelihood (Eq. (4) )a n dt h et o p5 0 0 0m o l e c u l e sw e r es e l e c t e df o r\nfurther analysis.\nThe 5000 top-scoring molecules were divided into two groups,\nbased on their similarity to theﬁne-tuning set. The similarity was\nmeasured via Tanimoto similarity on the extended connectivity\nﬁngerprints\n61 of the Bemis-Murcko scaffolds63 (using a radius of 3\nbonds and 2048 bits), and a threshold of 60% similarity. The designs in\nthe lists were grouped by their most similar training molecule and\nranked by the log-likelihood score (Eq. (5)). The highest-scoring\nmolecule in each group was picked. The topﬁve molecules of the\ndesign lists (i.e.,1– 5 for higher similarity, and6– 10 for lower similarity)\nand their most similar actives (11– 16, based on scaffold similarity,\nTable 3) were selected for molecular dynamics simulations.\nMolecular dynamics simulation. The protein structure of MAPK1 was\nsourced from the Protein Data Bank under the accession code 2Y9Q,\ncharacterized by a Resolution of 1.55Å and an R-Value Free of 0.177.\nInitial complex structures resulted from Docking simulations using\nVina\n83, establishing the binding pose for subsequent investigation via\nUmbrella Sampling. The setup of the simulation system was facilitated\nby the CHARMM-GUI web-based graphical interface\n84. A cubic water\nbox with an edge distance of 13Å encapsulated the system, supple-\nmented by a 0.15 M ionic NaCl solution for solvation neutralization.\nGromacs software version 2021\n85, operationalized on the Dutch\nsupercomputer Snellius, facilitated all simulations. The energy mini-\nmization of solvated systems involved a sequence of steps utilizing the\nsteepest descent method and the conjugate gradient algorithm. Sub-\nsequently, equilibration occurred through 5 ns NPT (constant Number\nof atoms, Pressure, Temperature) ensemble after theﬁrst 1 ns NVT\n(constant Number of atoms, Volume, Temperature) ensemble.\nBinding free energy calculation. The last conformations of the\nequilibration phase were used as the starting structures of ligand\nunbinding simulation. The distance-based Steered MD simulation\n(center-of-mass-pulling method) was used to pull the ligand away from\nthe protein by approximately 30Å over the course of 4 ns by using a\n1000 kJ/(mol nm\n2) force along the reaction coordinate (ξ), with a\npulling speed (ν) set at 0.001 nm/ps. Snapshot intervals of 10 ps gen-\nerated 400 conﬁgurations from these pulling simulations. Different\nligands prompted the extraction of varying conformations, ranging\nfrom 22 to 28, along the reaction coordinate (ξ)a ta p p r o x i m a t e l y\n0.1 nm intervals. These distinct conﬁgurations were then employed as\nthe initial points for individual Umbrella Sampling simulations, dif-\nfering in quantity depending on the speciﬁc ligand under study. Each\nconformation underwent independent NPT equilibration for 5 ns, fol-\nlowed by a 20 ns MD run in triplicate for each ligand. The potential\nmean force (PMF) was determined via the weighted histogram analysis\nmethod (WHAM)\n86, a component of Gromacs. The resultant PMF\ngraphs depicted force in kcal mol−1, representing the force required to\ndissociate the ligand from the binding pocket, against the corre-\nsponding distance. The computation of the binding free energy (ΔG)\nfor each ligand involved comparing the plateau region of the PMF\ncurve to the energy minimum obtained from each simulation. In total,\nthe Umbrella Sampling simulations spanned 400 to 560 ns,\ncomprising 3 replicates, thereby accumulating simulation times ran-\nging from 1.2μst o1 . 6μs for each ligand.\nSoftware and code\nData preprocessing, scaffold determination, and molecularﬁngerprint\nand descriptor calculation were performed with default settings\n(unless otherwise noted), using RDKit v2020.09.01 in a Python envir-\nonment. LSTM and GPT were implemented in Keras v2.7.0 (Tensorﬂow\nv2.7.1). The S4 code was extracted from the existing Pytorch-lightning\nv1.15.0 implementation\n35 and simpli ﬁed to rely solely on\nPytorch v1.13.1.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nThe data used in our study are available on GitHub at the following\nURL: https://github.com/molML/s4-for-de-novo-drug-design.S o u r c e\ndata are provided with this paper. The molecule designs and source\ndata are also available at:10.5281/zenodo.12666371.\nCode availability\nThe Python code to replicate and extend our study is available on\nGitHub at the following URL: https://github.com/molML/s4-for-de-\nnovo-drug-design. The code at the time of publishing is available at:10.\n5281/zenodo.1266637187.\nReferences\n1. Bohacek, R. S., McMartin, C. & Guida, W. C. The art and practice of\nstructure-based drug design: a molecular modeling perspective.\nMed. Res. Rev.16,3 – 50 (1996).\n2 . S k i n n i d e r ,M .A . ,S t a c e y ,R .G . ,W i s h a r t ,D .S .&F o s t e r ,L .J .C h e m i c a l\nlanguage models enable navigation in sparsely populated chemical\nspace. Nat. Mach. Intell.3,7 5 9– 770 (2021).\n3. Yuan, W. et al. Chemical space mimicry for drug discovery.J. Chem.\nInf. Model.57,8 7 5– 882 (2017).\n4. Merk, D., Friedrich, L., Grisoni, F. & Schneider, G. De novo design of\nbioactive small molecules by artiﬁcial intelligence.Mol. Inform.37,\n1700153 (2018).\n5. Grisoni, F. et al. Combining generative artiﬁcial intelligence and on-\nchip synthesis for de novo drug design.Sci. Adv.7, eabg3338\n(2021).\n6. Ballarotto, M. et al. De novo design of Nurr1 agonists via fragment-\naugmented generative deep learning in low-data regime.J. Med.\nChem. 66,8 1 7 0– 8177 (2023).\n7. Grisoni, F. Chemical language models for de novo drug design:\nchallenges and opportunities.Curr. Opin. Struct. Biol.79,\n102527 (2023).\n8. Flam-Shepherd, D., Zhu, K. & Aspuru-Guzik, A. Language models\ncan learn complex molecular distributions.Nat. Commun.13,\n3293 (2022).\n9. Hong, Y.-B., Lee, K.-J., Heo, D. & Choi, H. Molecule generation for\ndrug discovery with new transformer architecture.https://ssrn.\ncom/abstract=4195528(2022).\n10. Wang, Y., Zhao, H., Sciabola, S. & Wang, W. cMolGPT: a conditional\ngenerative pre-trained transformer for target-speciﬁcd en o v o\nmolecular generation.Molecules28, 4430 (2023).\n1 1 . H e ,Z .e ta l .T D - G P T :t a r g e tp r o t e i n - s p e c iﬁc drug molecule gen-\neration gpt. InICASSP 2024-2024 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP),2 3 5 5– 2359\n(IEEE, 2024).\n12. Hu, X., Liu, G., Zhao, Y. & Zhang, H. De novo drug design using\nreinforcement learning with multiple gpt agents.Advances in\nNeural Information Processing Systems36 (2024).\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 9\n13. Gummesson Svensson, H., Tyrchan, C., Engkvist, O. & Haghir\nChehreghani, M. Utilizing reinforcement learning for de novo drug\ndesign. Mach. Learn.113,1 – 33 (2024).\n14. Weininger, D. SMILES, a chemical language and information sys-\ntem. 1. Introduction to methodology and encoding rules.J. Chem.\nInf. Comput. Sci.28,3 1– 36 (1988).\n15. Krenn, M. et al. SELFIES and the future of molecular string repre-\nsentations.Patterns3, 100588 (2022).\n16. O ’Boyle, N. & Dalke, A. DeepSMILES: an adaptation of smiles for use\nin machine-learning of chemical structures.ChemRxiv(2018).\n1 7 . A t z ,K . ,G r i s o n i ,F .&S c h n e i d e r ,G .G e o m e t r i cd e e pl e a r n i n go n\nmolecular representations.Nat. Mach. Intell.3,1 0 2 3– 1032 (2021).\n18. Abate, C., Decherchi, S. & Cavalli, A. Graph neural networks for\nconditional de novo drug design.W i l e yI n t e r d i s c i p .R e v .C o m p u t .\nMol. Sci.13, e1651 (2023).\n19. Ochiai, T. et al. Variational autoencoder-based chemical latent\nspace for large molecular structures with 3d complexity.Commun.\nChem. 6, 249 (2023).\n20. Wang, M. et al. Deep learning approaches for de novo drug design:\nan overview.Curr. Opin. Struct. Biol.72,1 3 5– 144 (2022).\n21. Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neural\nComput. 9,1 7 3 5– 1780 (1997).\n2 2 . S e g l e r ,M .H . ,K o g e j ,T . ,T y r c h a n ,C .&W a l l e r ,M .P .G e n e r a t i n g\nfocused molecule libraries for drugd i s c o v e r yw i t hrecurrent neural\nnetworks.ACS Cent. Sci.4,1 2 0– 131 (2018).\n23. Bahdanau, D., Cho, K. H. & Bengio, Y. Neural machine translation by\njointly learning to align and translate. In3rd International Con-\nference on Learning Representations, ICLR 2015(2015).\n24. Gómez-Bombarelli, R. et al. Automatic chemical design using a\ndata-driven continuous representation of molecules.ACS Cent. Sci.\n4,2 6 8– 276 (2018).\n25. Chen, Y. et al. Molecular language models: RNNs or transformer?\nBrief. Funct. Genomics22,3 9 2– 400 (2023).\n26. Vaswani, A. et al. Attention is all you need.Advances in Neural\nInformation Processing Systems30 (NIPS, 2017).\n2 7 . B a g a l ,V . ,A g g a r w a l ,R . ,V i n o d ,P .&P r i y a k u m a r ,U .D .M o l G P T :\nmolecular generation using a transformer-decoder model.J. Chem.\nInf. Model.62,2 0 6 4–\n2076 (2021).\n28. Yang, L. et al. Transformer-based generative model accelerating\nthe development of novel braf inhibitors.ACS Omega6,\n33864– 33873 (2021).\n29. Wang, S., Guo, Y., Wang, Y., Sun, H. & Huang, J. SMILES-BERT: large\nscale unsupervised pre-training for molecular property prediction.\nIn Proceedings of the 10th ACM International Conference on Bioin-\nformatics, Computational Biology and Health Informatics,\n429– 436 (2019).\n30. Honda, S., Shi, S. & Ueda, H. R. SMILES transformer: pre-trained\nmolecularﬁngerprint for low data drug discovery. Preprint atarXiv\nhttps://doi.org/10.48550/arXiv.1911.04738(2019).\n31. Lim, S. & Lee, Y. O. Predicting chemical properties using self-\nattention multi-task learning based on SMILES representation. In\n2020 25th International Conference on Pattern Recognition (ICPR),\n3146– 3153 (IEEE, 2021).\n32. Jiang, J. et al. TranGRU: focusing on both the local and global\ninformation of molecules for molecular property prediction.Appl.\nIntell. 53,1 5 2 4 6– 15260 (2023).\n33. Gu, A., Dao, T., Ermon, S., Rudra, A. & Ré, C. Hippo: recurrent\nmemory with optimal polynomial projections.Adv. Neural Inf. Pro-\ncess. Syst.33,1 4 7 4– 1487 (2020).\n34. Gu, A. et al. Combining recurrent, convolutional, and continuous-\ntime models with linear state space layers.Adv. Neural Inf. Process.\nSyst. 34,5 7 2– 585 (2021).\n35. Gu, A., Goel, K. & Ré, C. Efﬁciently modeling long sequences with\nstructured state spaces. InThe International Conference on Learn-\ning Representations (ICLR)(2022).\n36. Fu, D. Y. et al. Hungry hungry hippos: towards language modeling\nwith state space models. Preprint atarXivhttps://doi.org/10.48550/\narXiv.2212.14052(2022).\n37. Lu, C. et al. Structured state space models for in-context reinfor-\ncement learning.Adv. Neural Inf. Process. Syst.36 (2024).\n38. Nguyen, E. et al. Hyenadna: Long-range genomic sequence mod-\neling at single nucleotide resolution.Adv. Neural Inf. Process. Syst.\n36 (2024).\n39. Gu, A. & Dao, T. MAMBA: linear-time sequence modeling with\nselective state spaces. Preprint atarXiv https://doi.org/10.48550/\narXiv.2312.00752(2023).\n40. Ma, J., Li, F. & Wang, B. U-MAMBA: enhancing long-range depen-\ndency for biomedical image segmentation. Preprint atarXivhttps://\ndoi.org/10.48550/arXiv.2401.04722(2024).\n41. Hamilton, J. D. State-space models.Handb. Econ.4,3 0 3 9– 3080\n(1994).\n42. Pan, V. Fast approximate computations with cauchy matrices and\npolynomials.Math. Comput.86,2 7 9 9– 2826 (2017).\n43. Gaulton, A. et al. The ChEMBL database in 2017.Nucleic Acids Res.\n45, D945– D954 (2017).\n44. Renz, P., Van Rompaey, D., Wegner, J. K., Hochreiter, S. & Klam-\nbauer, G. On failure modes in molecule generation and optimiza-\ntion. Drug Discov. Today Technol.32,5 5– 63 (2019).\n45. Wildman, S. A. & Crippen, G. M. Prediction of physicochemical\nparameters by atomic contributions.J. Chem. Inf. Comput. Sci.39,\n868– 873 (1999).\n46. Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S. & Hopkins, A.\nL. Quantifying the chemical beauty of drugs.Nat. Chem.4,\n90– 98 (2012).\n4 7 . B e r t z ,S .H .T h eﬁrst general index of molecular complexity.J. Am.\nChem. Soc.103,3 5 9 9– 3601 (1981).\n48. Ertl, P. & Schuffenhauer, A. Estimation of synthetic accessibility\nscore of drug-like molecules based on molecular complexity and\nfragment contributions.J. Cheminformatics1,1 – 11 (2009).\n49. Coley, C. W., Rogers, L., Green, W. H. & Jensen, K. F. SCScore:\nsynthetic complexity learned from a reaction corpus.J. Chem. Inf.\nModel. 58,2 5 2– 261 (2018).\n5 0 . B r o w n ,N . ,F i s c a t o ,M . ,S e g l e r ,M .H .&V a u c h e r ,A .C .G u a c a M o l :\nbenchmarking models for de novo molecular design.J. Chem. Inf.\nModel. 59,1 0 9 6– 1108 (2019).\n51. Polykovskiy, D. et al. Molecular sets (moses): a benchmarking\nplatform for molecular generation models.Front. Pharmacol.11,\n565644 (2020).\n52. Weiss, K., Khoshgoftaar, T. M. & Wang, D. A survey of transfer\nlearning.J. Big Data3,1 – 40 (2016).\n53. Tran-Nguyen, V.-K., Jacquemard, C. & Rognan, D. Lit-pcba: an\nunbiased data set for machine learning and virtual screening.J.\nChem. Inf. Model.60,4 2 6 3– 4273 (2020).\n54. van Tilborg, D., Alenicheva, A. & Grisoni, F. Exposing the limitations\nof molecular machine learning with activity cliffs.J. Chem. Inf.\nModel. 62,5 9 3 8– 5951 (2022).\n55. Weng, G. et al. Rediscmol: benchmarking molecular generation\nmodels in biological properties.J. Med. Chem.67\n,1 5 3 3– 1543\n(2024).\n56. Laban, P., Wu, C.-S., Liu, W. & Xiong, C. Near-negative distinction:\ngiving a second life to human evaluation datasets. InProceedings of\nt h e2 0 2 2C o n f e r e n c eo nE m p i r i c a lM e t h o d si nN a t u r a lL a n g u a g e\nProcessing,2 0 9 4– 2108 (2022).\n5 7 . M o r e t ,M . ,H e l m s t ä d t e r ,M . ,G r i s o n i ,F . ,S c h n e i d e r ,G .&M e r k ,D .\nBeam search for automated design and scoring of novel ror ligands\nwith machine intelligence.Angew. Chem. Int. Ed.60, 19477– 19482\n(2021).\n58. Maggiora, G. M. On outliers and activity cliffs why qsar often dis-\nappoints.J. Chem. Inf. Model.46,1 5 3 5– 1535 (2006).\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 10\n59. Woolson, R. F. Wilcoxon signed-rank test. InWiley Encyclopedia of\nClinical Trials(eds D’Agostino, R. B., Sullivan, L. & Massaro, J.) 1– 3\n(John Wiley & Sons, Ltd., 2007).\n60. Moret, M., Friedrich, L., Grisoni, F., Merk, D. & Schneider, G. Gen-\nerative molecular design in low data regimes.Nat. Mach. Intell.2,\n171– 180 (2020).\n61. Rogers, D. & Hahn, M. Extended-connectivityﬁngerprints.J. Chem.\nInf. Model.50,7 4 2– 754 (2010).\n62. Schneider, G., Schneider, P. & Renner, S. Scaffold-hopping: how far\ncan you jump?QSAR Comb. Sci.25, 1162– 1171 (2006).\n6 3 . B e m i s ,G .W .&M u r c k o ,M .A .T h ep r o p e r t i e so fk n o w nd r u g s .1 .\nMolecular frameworks.J. Med. Chem.39, 2887– 2893 (1996).\n64. Harvey, A. L., Edrada-Ebel, R. & Quinn, R. J. The re-emergence of\nnatural products for drug discovery in the genomics era.Nat. Rev.\nDrug Discov.14, 111– 129 (2015).\n6 5 . A t a n a s o v ,A .G . ,Z o t c h e v ,S .B . ,D i r s c h ,V .M .&S u p u r a n ,C .T .N a t u r a l\nproducts in drug discovery: advances and opportunities.Nat. Rev.\nDrug Discov.20,2 0 0– 216 (2021).\n66. Lee, M.-L. & Schneider, G. Scaffold architecture and pharmaco-\nphoric properties of natural products and trade drugs: application\nin the design of natural product-based combinatorial libraries.J.\nComb. Chem.3,2 8 4– 289 (2001).\n67. Henkel, T., Brunne, R. M., Müller, H. & Reichel, F. Statistical inves-\ntigation into the structural complementarity of natural products and\nsynthetic compounds.Angew. Chem. Int. Ed.38, 643– 647 (1999).\n6 8 . C h e n ,Y . ,R o s e n k r a n z ,C . ,H i r t e ,S .&K i r c h m a i r ,J .R i n gs y s t e m si n\nnatural products: structural diversity, physicochemical properties,\nand coverage by synthetic compounds.Nat. Prod. Rep.39,\n1544– 1556 (2022).\n69. Merk, D., Grisoni, F., Friedrich, L. & Schneider, G. Tuning artiﬁcial\nintelligence on the de novo design of natural-product-inspired\nretinoid x receptor modulators.Commun. Chem.1,6 8( 2 0 1 8 ) .\n7 0 . S o r o k i n a ,M . ,M e r s e b u r g e r ,P . ,R a j a n ,K . ,Y i r i k ,M .A .&S t e i n b e c k ,C .\nCOCONUT online: collection of open natural products database.J.\nCheminformatics13,1 – 13 (2021).\n71. Ertl, P., Roggo, S. & Schuffenhauer, A. Natural product-likeness\nscore and its application for prioritization of compound libraries.J.\nChem. Inf. Model.\n48,6 8– 74 (2008).\n72. Smirnov, N. On the estimation of the discrepancy between\nempirical distribution for two independent samples.Bull. Math.\nUniv. Mosc.2, 2 (1939).\n73. Braicu, C. et al. A comprehensive review on MAPK: a promising\ntherapeutic target in cancer.Cancers 11,1 6 1 8( 2 0 1 9 ) .\n74. Kästner, J. Umbrella sampling.W i l e yI n t e r d i s c i p .R e v .C o m p u t .M o l .\nSci. 1,9 3 2– 942 (2011).\n75. Aronov, A. M. et al. Flipped out: structure-guided design of selec-\ntive pyrazolylpyrrole erk inhibitors.J. Med. Chem.50,1 2 8 0– 1287\n(2007).\n76. Chaikuad, A. et al. A unique inhibitor binding site in ERK1/2 is\nassociated with slow binding kinetics.Nat. Chem. Biol.10,\n853– 860 (2014).\n7 7 . B l a k e ,J .F .e ta l .D i s c o v e r yo f5 ,6 ,7 ,8 - t e t r a h y d r o p y r i d o[ 3 ,4 - d ]\npyrimidine inhibitors of ERK2.Bioorg. Med. Chem. Lett.24,\n2635– 2639 (2014).\n78. Liu, F. et al. Structure-based optimization of pyridoxal 5’-phosphate-\ndependent transaminase enzyme (bioa) inhibitors that target biotin\nbiosynthesis in mycobacterium tuberculosis.J. Med. Chem.60,\n5507– 5520 (2017).\n79. Bjerrum, E. J. SMILES enumeration as data augmentation for neural\nnetwork modeling of molecules. Preprint atarXiv https://doi.org/\n10.48550/arXiv.1703.07076(2017).\n80. Schwaller, P. et al. Molecular transformer: a model for uncertainty-\ncalibrated chemical reaction prediction.ACS Cent. Sci.5,1 5 7 2– 1583\n(2019).\n81. Özçelik, R., van Tilborg, D., Jiménez-Luna, J. & Grisoni, F. Structure-\nb a s e dd r u gd i s c o v e r yw i t hd e e pl e a r n i n g .ChemBioChem23,\ne202200776 (2023).\n82. Moret, M., Grisoni, F., Katzberger, P. & Schneider, G. Perplexity-\nbased molecule ranking and bias estimation of chemical language\nmodels. J. Chem. Inf. Model.62, 1199– 1206 (2022).\n83. Eberhardt, J., Santos-Martins, D., Tillack, A. F. & Forli, S. Autodock\nvina 1.2. 0: new docking methods, expanded forceﬁeld, and python\nbindings.J. Chem. Inf. Model.61,3 8 9 1– 3898 (2021).\n84. Lee, J. et al. CHARMM-GUI input generator for NAMD, GROMACS,\nAMBER, OpenMM, and CHARMM/OpenMM simulations using the\nCHARMM36 additive forceﬁ\neld. Biophys. J.110,6 4 1 a( 2 0 1 6 ) .\n8 5 . A b r a h a m ,M .J .e ta l .G R O M A CS: high performance molecular\nsimulations through multi-level par a l l e l i s mf r o ml a p t o p st os u p e r -\ncomputers.SoftwareX1,1 9– 25 (2015).\n8 6 . H u b ,J .S . ,D eG r o o t ,B .L .&v a nd e rS p o e l ,D .g _ w h a m :af r e e\nweighted histogram analysis implementation including robust error\nand autocorrelation estimates.J. Chem. Theory Comput.6,\n3713– 3720 (2010).\n87. Özçelik, R., de Ruiter, S., Criscuolo, E. & Grisoni, F. Chemical lan-\nguage modeling with structured state space sequence models.\nhttps://github.com/molML/s4-for-de-novo-drug-design, https://\ndoi.org/10.5281/zenodo.12666371(2024).\nAcknowledgements\nThis research was co-funded by the European Union (ERC, ReMINDER,\n101077879). Views and opinions expressed are however those of the\nauthor(s) only and do not necessarily reﬂect those of the European Union\nor the European Research Council. Neither the European Union nor the\ngranting authority can be held responsible for them. The authors also\nacknowledge support from the Irene Curie Fellowship, the Centre for\nLiving Technologies, and SURF (NWO grant EINF-5406). The authors\nthank Selen Parlar and the Molecular Machine Learning team (H. Brink-\nmann, C. Izquierdo-Lozano, M. Reksoprodjo, L. Rossen, Y.G. Nana Teu-\nkam, D. van Tilborg, L. van Weesep) for their feedback on the\nmanuscript.\nAuthor contributions\nConceptualization: R.Ö. and F.G. Data curation: R.Ö. Formal analysis: all\nauthors. Investigation: all authors.Methodology: all authors. Software:\nR.Ö. and S.d.R. Visualization: R.Ö., F.G. and E.C. Writing– original draft:\nR.Ö. and F.G. Writing– review and editing: all authors.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-024-50469-9.\nCorrespondenceand requests for materials should be addressed to\nFrancesca Grisoni.\nPeer review informationNature Communicationsthanks Chiranjib\nChakraborty and the other anonymous reviewer(s) for their contribution\nto the peer review of this work. A peer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 11\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2024\nArticle https://doi.org/10.1038/s41467-024-50469-9\nNature Communications|         (2024) 15:6176 12",
  "topic": "Chemical space",
  "concepts": [
    {
      "name": "Chemical space",
      "score": 0.8665270805358887
    },
    {
      "name": "Computer science",
      "score": 0.6341896057128906
    },
    {
      "name": "Drug discovery",
      "score": 0.5112590789794922
    },
    {
      "name": "String (physics)",
      "score": 0.4933091700077057
    },
    {
      "name": "Generative model",
      "score": 0.48192501068115234
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.45817941427230835
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4574197828769684
    },
    {
      "name": "Molecular model",
      "score": 0.4551818072795868
    },
    {
      "name": "Language model",
      "score": 0.44370776414871216
    },
    {
      "name": "Identification (biology)",
      "score": 0.43167030811309814
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40895384550094604
    },
    {
      "name": "Computational biology",
      "score": 0.39131873846054077
    },
    {
      "name": "Generative grammar",
      "score": 0.3785371780395508
    },
    {
      "name": "Chemistry",
      "score": 0.26595979928970337
    },
    {
      "name": "Bioinformatics",
      "score": 0.2029181718826294
    },
    {
      "name": "Biology",
      "score": 0.19269433617591858
    },
    {
      "name": "Physics",
      "score": 0.12504345178604126
    },
    {
      "name": "Stereochemistry",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I83019370",
      "name": "Eindhoven University of Technology",
      "country": "NL"
    }
  ]
}