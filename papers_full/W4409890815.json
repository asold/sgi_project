{
  "title": "Evaluation of the performance of large language models in clinical decision-making in endodontics",
  "url": "https://openalex.org/W4409890815",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4292248955",
      "name": "Yağız Özbay",
      "affiliations": [
        "Karabük University"
      ]
    },
    {
      "id": "https://openalex.org/A2127801659",
      "name": "Deniz Erdoğan",
      "affiliations": [
        "Psychiatric Association of Turkey"
      ]
    },
    {
      "id": "https://openalex.org/A3081831097",
      "name": "Gözde Akbal Dinçer",
      "affiliations": [
        "Okan University"
      ]
    },
    {
      "id": "https://openalex.org/A4292248955",
      "name": "Yağız Özbay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127801659",
      "name": "Deniz Erdoğan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3081831097",
      "name": "Gözde Akbal Dinçer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3016417837",
    "https://openalex.org/W2969715170",
    "https://openalex.org/W2784246882",
    "https://openalex.org/W4376866715",
    "https://openalex.org/W3166540310",
    "https://openalex.org/W4366822220",
    "https://openalex.org/W4366603014",
    "https://openalex.org/W4367186868",
    "https://openalex.org/W4387389166",
    "https://openalex.org/W4387472364",
    "https://openalex.org/W4388220857",
    "https://openalex.org/W4385637002",
    "https://openalex.org/W4388823522",
    "https://openalex.org/W4389396369",
    "https://openalex.org/W4365451987",
    "https://openalex.org/W4389618630",
    "https://openalex.org/W4401028500",
    "https://openalex.org/W4399264184",
    "https://openalex.org/W4388678005",
    "https://openalex.org/W2608532111",
    "https://openalex.org/W2975592441",
    "https://openalex.org/W2300771567",
    "https://openalex.org/W2914669715",
    "https://openalex.org/W3158352812",
    "https://openalex.org/W2100698432",
    "https://openalex.org/W4403214194",
    "https://openalex.org/W6852317843",
    "https://openalex.org/W4387242094",
    "https://openalex.org/W4403087472",
    "https://openalex.org/W4390940237",
    "https://openalex.org/W4387166123",
    "https://openalex.org/W4389286021",
    "https://openalex.org/W4385997381",
    "https://openalex.org/W4367556998",
    "https://openalex.org/W4384824783"
  ],
  "abstract": null,
  "full_text": "RESEARCH Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the \nlicensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  \nv e c  o m m  o n s .  o r  g / l  i c e  n s e s  / b  y - n c - n d / 4 . 0 /.\nÖzbay et al. BMC Oral Health          (2025) 25:648 \nhttps://doi.org/10.1186/s12903-025-06050-x\nBMC Oral Health\n*Correspondence:\nYağız Özbay\nyagiz_ozbay@hotmail.com\n1Department of Endodontics, Faculty of Dentistry, Karabük University, \nKarabük, Türkiye\n2Private Dentist, Ankara, Türkiye\n3Department of Endodontics, Faculty of Dentistry, Okan University, \nİstanbul , Türkiye\nAbstract\nBackground Artificial intelligence (AI) chatbots are excellent at generating language. The growing use of generative \nAI large language models (LLMs) in healthcare and dentistry, including endodontics, raises questions about their \naccuracy. The potential of LLMs to assist clinicians’ decision-making processes in endodontics is worth evaluating. This \nstudy aims to comparatively evaluate the answers provided by Google Bard, ChatGPT-3.5, and ChatGPT-4 to clinically \nrelevant questions from the field of Endodontics.\nMethods 40 open-ended questions covering different areas of endodontics were prepared and were introduced to \nGoogle Bard, ChatGPT-3.5, and ChatGPT-4. Validity of the questions was evaluated using the Lawshe Content Validity \nIndex. Two experienced endodontists, blinded to the chatbots, evaluated the answers using a 3-point Likert scale. All \nresponses deemed to contain factually wrong information were noted and a misinformation rate for each LLM was \ncalculated (number of answers containing wrong information/total number of questions). The One-way analysis of \nvariance and Post Hoc Tukey test were used to analyze the data and significance was considered to be p < 0.05.\nResults ChatGPT-4 demonstrated the highest score and the lowest misinformation rate (P = 0.008) followed by \nChatGPT-3.5 and Google Bard respectively. The difference between ChatGPT-4 and Google Bard was statistically \nsignificant (P = 0.004).\nConclusion ChatGPT-4 provided more accurate and informative information in endodontics. However, all LLMs \nproduced varying levels of incomplete or incorrect answers.\nKeywords Chat GPT, Chatbot, Large Language model, Endodontics, Endodontology\nEvaluation of the performance of large \nlanguage models in clinical decision-making \nin endodontics\nYağız Özbay1*, Deniz Erdoğan2 and Gözde Akbal Dinçer3\nPage 2 of 9\nÖzbay et al. BMC Oral Health           (2025) 25:648 \nIntroduction\nThe term “artificial intelligence” (AI) was introduced in \nthe 1950s and refers to the concept of creating machines \nthat can perform tasks typically carried out by humans \n[1]. AI employs algorithms to simulate intelligent actions \nwith little human input. Predominantly reliant on \nmachine learning, AI’s scope includes data retrieval and \nanalysis of datasets and images [ 2, 3]. AI has been grow -\ning rapidly in dental practices over the past few years, \nenabling computers to handle tasks previously requiring \nhuman involvement and enabling healthcare profession -\nals to provide better oral health care.\nLarge language models (LLMs) are AI-based software \nthat simulates human language processing abilities such \nas understanding the meaning of a phrase or responding \nand creating new content after being trained with mas -\nsive datasets. Thus, an LLM can generate an article on \nany subject, answer a question, or translate a text after \nbeing accordingly instructed [ 4]. In dentistry, LLMs can \ncontribute to research, clinical decision-making, and per-\nsonalized patient care, enhancing efficiency and reducing \nerrors [1]. They can assist dentists in diagnosing oral dis -\neases by analyzing patient records, medical histories, and \nimaging reports. They also support treatment planning \nby synthesizing vast amounts of scientific literature and \nproviding evidence-based recommendations [5]. As these \nmodels continue to evolve, they hold significant poten -\ntial to transform healthcare delivery and improve patient \noutcomes.\nReleased in November 2022, Chatbot Generative Pre -\ntrained Transformer (ChatGPT) (OpenAI, San Francisco, \nCA, USA), is an LLM that can answer questions quickly \nand fluently and interact with the user in a way that \nmimics human communication [ 6]. ChatGPT can write \ncode or articles, summarise text, remember the previous \nuser’s input and response in the thread, and elaborate its \nanswers with further queries [ 7, 8]. ChatGPT is shown to \nhave a basic understanding of oral potentially malignant \ndisorders with potential limitations such as inaccurate \ncontent and references [ 9]. Currently, there is an older \nversion, ChatGPT-3.5 available for free and the latest ver-\nsion ChatGPT-4 which is subject to a paid subscription. \nA recent study by Suárez et al. showed that ChatGPT-3.5 \ncan consistently answer dichotomous questions, but \nis not able to give correct answers to all questions [ 10]. \nUnlike its previous model, ChatGPT-4 can also retrieve \ninformation from the internet. On the other hand, The \nGoogle Bard chatbot (Alphabet, Mountain View, Califor -\nnia), was launched in March 2023 as a potential competi -\ntor, can also complete language-related tasks and answer \nquestions with detailed information [ 11]. Another study \ndemonstrated that Google Bard holds the potential \nto aid evidence-based dentistry; nevertheless, it gives \noccasional wrong answers or no answers despite its live \naccess to the internet [12].\nThere are studies examining the ability of LLMs to \nanswer multiple choice questions and the ability to \nanswer open-ended questions in various clinical scenar -\nios [10, 13–16]. It has been observed that success varies \ndepending on the LLM used, field and type of question. \nUnlike previous ones, the present study focuses on \nthe ability of LLMs to provide scientific knowledge in \nendodontics.\nEndodontics is a branch of dentistry that requires clini-\ncians to have up-to-date knowledge in many areas such \nas pathologies, stages of root canal treatment, vital pulp \ntreatments, dental trauma, etc. The application of LLMs \nin endodontics has the potential to enhance diagnos -\ntic accuracy, treatment planning, and patient manage -\nment. These AI-driven models can analyze vast amounts \nof endodontic literature, clinical guidelines, and patient \nrecords to assist clinicians in identifying complex root \ncanal pathologies and suggesting evidence-based treat -\nment approaches [ 17]. Moreover, LLMs can aid in the \ninterpretation of radiographic images by integrating nat -\nural language processing (NLP) with deep learning tech -\nniques, improving the detection of periapical lesions and \nroot fractures [18].\nIn one study, periapical X-rays were analyzed by Chat -\nGPT and its performance was evaluated based on its abil-\nity to accurately identify a variety of dental conditions, \nincluding tooth decay, endodontic treatments, dental \nrestorations, and other oral health problems. Endo-oral \nlesions were missed in 56% of cases. The AI’s overall cor -\nrect interpretation rate was 11%, indicating limited clini -\ncal utility in its current form. Misinterpretations included \nincorrect tooth identification and failure to recognize \ncertain dental lesions. Although ChatGPT is promising \nin endodontics, it has been reported that it has signifi -\ncant gaps in its diagnostic accuracy [19].\nThe potential of LLMs to assist clinicians in decision-\nmaking in the endodontic clinic is worth evaluating so \nthat, if adequate, they can be utilized and, if not, so that \nclinicians are aware of the limitations that exist and can \ncontribute to improving the deficiencies of these lan -\nguage models. To the best of the authors’ knowledge, \nthere are no prior studies that compare LLMs such as \nChatGPT and Google Bard as potential sources of infor -\nmation in Endodontics. Therefore, this study aimed to \ncompare the accuracy and completeness of the answers \ngenerated by ChatGPT-3.5, ChatGPT-4, and Google Bard \nto open-ended questions related to Endodontics.\nMaterials and methods\nThe study was conducted under the Declaration of Hel -\nsinki. Since our study had no human subjects involved, \nethical approval was not required. 57 open-ended \nPage 3 of 9\nÖzbay et al. BMC Oral Health           (2025) 25:648 \nquestions covering different areas of endodontics within \nthe content of the Guidelines and Position Statements of \nthe European Society of Endodontology, and the Ameri -\ncan Association of Endodontists were prepared (Table  1) \n[20–24]. These articles were chosen by two endodontists \n(Y.Ö. and G.A.D.) since articles represent a consensus \nachieved by expert committees in the field. The validity \nof the questions was evaluated by 8 volunteer endodon -\ntists using the Lawshe’s Content Validity Index, a widely \naccepted method [25].\nThe experts rated each question as “essential, ” “useful \nbut not essential, ” or “not essential. ” The critical content \nvalidity ratio value obtained according to Wilson et al., is \n0.69 for 8 volunteers at a significance level of 0.05 [ 25]. \nHence, questions with a content validity ratio value of \n≥ 0.69 were selected for inclusion. As a result, 40 open-\nended, clinically relevant questions that require text-\nbased responses were included. The sample size was \ncalculated at the significance level of 0.05, effect size of \n0.3 and power of 0.85 using G* Power v3.1 (Heinrich \nHeine, Universität Düsseldorf, Germany). Questions \naimed to measure broad endodontic knowledge, such as \nendodontic indications, antibiotic use, root canal treat -\nment procedures, symptoms of acute apical abscesses, \nTable 1 Questions directed to the LLMs\nQuestions\n1. What are the indications for root canal treatment?\n2. What are the contra-indications for root canal treatment?\n3. What are the indications for nonsurgical root canal retreatment?\n4. What are the considerations in the preparation of the access cavity?\n5. Which pulp tests are applied to assess the health of dental pulp?\n6. Why is rubber dam used in Endodontics?\n7. How is working length determined during root canal treatment?\n8. What are the considerations in the obturation of root canals?\n9. What are the considerations in the apical preparation of root canals?\n10. What are the considerations in the delivery of irrigants in root canals?\n11. How is an acute apical abscess with systemic involvement endodontically managed?\n12. How is the glide path in the root canal created?\n13. What are the preoperative and intraoperative considerations of intentional replantation of the tooth?\n14. What are the clinical symptoms of reversible pulpitis?\n15. What are the clinical symptoms of irreversible pulpitis?\n16. When and how is direct pulp capping performed?\n17. When and how is full pulpotomy performed?\n18. Which materials are used in vital pulp therapies?\n19. How is lateral luxation of permanent tooth diagnosed and treated?\n20. How is avulsion of permanent tooth diagnosed and treated?\n21. How is a complicated crown fracture of permanent teeth diagnosed and treated?\n22. How is a horizontal root fracture of a permanent tooth diagnosed and treated?\n23. What are the clinical features of external cervical resorption?\n24. Which intracanal medicaments are currently used in Endodontics?\n25. What are the applications of calcium hydroxide in Endodontics?\n26. How is internal resorption of the tooth treated?\n27. What is a cracked tooth?\n28. What are the indications for periradicular surgery?\n29. What are the criteria for the use of CBCT in Endodontics?\n30. What are the indications for systemic antibiotics in Endodontics?\n31. What are the contra-indications for systemic antibiotics in Endodontics?\n32. What are the clinical symptoms of chronic apical abscesses?\n33. What are the clinical steps of the revitalization of teeth?\n34. What are the clinical symptoms of acute apical abscesses?\n35. What are the radiographic features of condensing osteitis?\n36. When using engine-driven NiTi files, which principles should be followed?\n37. How is inadvertent extrusion of sodium hypochlorite beyond the end of the root into the periradicular tissues managed?\n38. Which clinical and radiological findings indicate a favorable outcome after root canal treatment?\n39. Which local anesthetics are commonly used for pain control before root canal treatment?\n40. Which irrigants are currently used in Endodontics?\nPage 4 of 9\nÖzbay et al. BMC Oral Health           (2025) 25:648 \nand common conditions that may cause concern during \nendodontic treatment, were presented. All questions and \nresponses were in English.\nThe questions were introduced into Google Bard, Chat-\nGPT-3.5, and ChatGPT-4 using the “new chat” option \non 20th December 2023. Each question was presented \nonly once to each LLM, and there was no follow-up, \nrephrasing, or annotation reflecting real-world situations \nfor general dentists. Examples of answers to the ques -\ntions are presented in Figs.  1, 2 and 3. All answers were \nrecorded for subsequent analysis. Two endodontists, \n(Y.Ö. and D.E.) with ten years of experience [ 26], blindly \nevaluated the answers using a 3-point (incorrect, partially \ncorrect/incorrect or correct). The Likert scale, which is \nvery popular in social research and has a simple measure-\nment process, was used [ 27]. Likert scale adapted from \nSuárez et al. (Table  2) [ 14]. When disagreements arose \nin some evaluations, these were resolved by the inter -\nvention of a third researcher. Additionally, all responses \ndeemed to contain factually wrong information were \nnoted and a misinformation rate for each LLM was cal -\nculated (number of answers containing wrong informa -\ntion/total number of questions). Scores were saved in an \nExcel© spreadsheet (Microsoft, Redmond, Washington, \nUSA).\nStatistical analysis\nThe statistical analysis was performed via MiniTab 17 \n(Minitab Inc., USA). The normality of the data was \nassessed with Ryan-Joiner test and the normal distribu -\ntion of the data was confirmed. One-way analysis of vari -\nance (ANOVA) and Post Hoc Tukey test were performed. \nA significance level of P < 0.05 was used to determine sta-\ntistical significance. \nResults\nThe mean values and standard deviations of each group \nare presented in Table 3; Fig. 4. ChatGPT-4 demonstrated \nthe highest score and the lowest misinformation rate fol -\nlowed by ChatGPT-3.5 and Google Bard respectively. \nThe difference between ChatGPT-4 and Google Bard was \nstatistically significant (P = 0.004). However, there was no \nstatistically significant difference between ChatGPT-3.5 \nand ChatGPT-4, and ChatGPT-3.5 and Google Bard.\nDiscussion\nInterest in LLMs has increased in studies in the field of \ndentistry as in other fields. Some of these studies have \nexamined the ability of LLMs to answer multiple-choice \nquestions, and some studies have examined the ability \nto answer open-ended questions in various clinical sce -\nnarios [12, 14–16, 28]. It has been observed that success \nvaries according to the LLM used, the field and the type \nof question. This study rather focuses on ability of LLMs \nto provide scientific knowledge in endodontics.\nOne of the important issues to consider when using \nchatbots as a source of medical information is the for -\nmulation of questions, which significantly influences \nchatbot responses [ 29]. Open-ended questions better \ncapture the nuances of the medical decision-making pro -\ncess [30]. The limitation of this study is that the questions \nare narrow in scope and only related to the field of end -\nodontics. Conversational LLMs perform well in summa -\nrizing health-related texts, answering general questions \nin health care, and collecting information from patients \n[31].\nIt was reported that ChatGPT-3 was generally effec -\ntive and could be used as an aid when an oral radiologist \nneeded additional information about pathologies. How -\never, it was also emphasized that it could not be used as \na main reference source [ 32]. In a diagnostic accuracy \nstudy, ChatGPT had the most difficulty recognizing den -\ntal lesions. In some cases, images incorrectly identified \nby ChatGPT as tooth decay were reported to be actually \ncaused by overlapping tooth ridges, and were found to be \ninadequate for detecting bone loss associated with peri -\nodontal disease. It was also shown that when interpreting \nsome radiographs, it did not properly recognise ceramic \ncrowns placed on teeth that were slightly radiopaque [19]. \nOn the other hand, in a study comparing the answers \ngiven by AI and humans in the exam, ChatGPT-3 and \nChatGPT 4 were found to outperform humans, and were \nreported that the newer version performed better [ 33]. \nAI has the potential to provide significant practicality in \nclinical applications such as treatment recommendations \nin the field of endodontics, and more scientific studies are \nneeded to evaluate its possible benefits and risks in a bal -\nanced manner.\nSimilar to a study evaluating the ability of ChatGPT to \nbe a potential source of information for patients’ ques -\ntions about periodontal diseases [34], it was also observed \nthat all LLMs suggested to seek professional help from a \ndentist or medical professional especially when it comes \nto questions related to diagnosis, diseases, and treat -\nment indications. A comprehensive review reported that \nconversational LLMs showed promising results in sum -\nmarizing and providing general medical information to \npatients with relatively high accuracy. However, it was \nnoted that conversational LLMs such as ChatGPT can -\nnot always provide reliable answers to complex health-\nrelated tasks that require expertise (e.g., diagnosis). \nAlthough bias or confidentiality issues are often cited as \nconcerns, no research has yet been found in articles that \ncarefully examine how conversational LLMs lead to these \nproblems in health services research. Furthermore, vari -\nous ethical concerns have been reported regarding the \napplication of LLMs in human health care, including \nPage 5 of 9\nÖzbay et al. BMC Oral Health           (2025) 25:648 \nFig. 1 Sample answer from Google Bard\n \nPage 6 of 9\nÖzbay et al. BMC Oral Health           (2025) 25:648 \nreliability, bias, confidentiality and public acceptance \n[31]. Clear policies should be established to manage the \nuse of sensitive data in the use of AI, ensure transparency \nin decision-making, and protect against biased results, \nand to mitigate security risks that may arise from the use \nof LLMs [ 35]. It is also critical to develop guidelines on \nethical issues that balance the need to protect individual \nprivacy and autonomy, especially in cases where errone -\nous or harmful advice is provided.\nTo simplify the evaluation process, the answers given \nby the language models were scored only in terms of \naccuracy and completeness and no other evaluation cri -\nteria were used. The predominance of ChatGPT-4 over \nother LLMs in providing accurate information has also \nbeen shown in studies using general clinical dentistry \nand Japanese National Dentist Examination questions in \nagreement with our study [ 12, 16]. However, a drawback \nof ChatGPT-4 is not being free of charge and the pricing \nis the same for all countries, which lessens the accessibil -\nity to low-income countries. Suárez et al. reported that \nthe correct answer rate was 57.33% when they directed \ndichotomous questions in endodontics to ChatGPt-3.5 \nsoftware [10]. The relatively low correct answer rate may \nbe because this language model does not have internet \naccess and the trained data set does not include scientific \npublications in the field of endodontics.\nThe questions were asked only once without rephras -\ning to simulate a real clinical consultation. An important \nFig. 2 Sample answer from ChatGPT-4\n \nPage 7 of 9\nÖzbay et al. BMC Oral Health           (2025) 25:648 \nfinding of this is, the capability of all LLMs to perfectly \nunderstand the questions and generating an answer in \nthe context. The answers of LLMs were found to be well-\nstructured and written convincingly while containing \nmisleading or wrong information. This finding, a phe -\nnomenon so-called “hallucination” , has been also \nreported in previous studies [ 12, 36]. For instance, in \na study on head and neck surgery, it was observed that \n46.4% of the references provided by ChatGPT4 did not \nactually exist [37]. Such made-up or misleading informa -\ntion could mislead dentists and potentially harm patients.\nA limitation of the study may be that the prepared \nquestions may not cover all clinical scenarios in end -\nodontics. Since the questions and answers were limited \nto guidelines and position statements issued by scientific \nsocieties, points not covered in these documents were \nnot included in the questions. In the study, questions \nwere asked with specific prompts as much as possible to \nobtain the response and to evaluate the usefulness of the \ninformation obtained in clinical decision-making. Fur -\nther studies that will evaluate LLMs as a source of infor -\nmation for patients may need to use different question \npatterns using less scientific terms.\nIt can be presumed that LLMs will continue to improve \nin terms of usability, precision, and ability to provide \nreliable information. Therefore, evaluation of LLMs as \npotential sources of information in the future might be \nvaluable. Alternatively, and as suggested earlier, LLMs \ncan be integrated into databases such as Web of Science \nTable 2 Likert scale description\nExperts’ \ngrading\nDescription\nIncorrect (0) The answer provided is completely incorrect or unre-\nlated to the question. It does not show an adequate \nunderstanding or knowledge of the topic.\nPartially correct \nor Incomplete \n(1)\nThe answer shows some understanding or knowl-\nedge of the topic, but there are significant errors or \nmissing elements. Although not entirely incorrect, the \nanswer is not sufficiently accurate or complete to be \nconsidered confident or appropriate\nCorrect (2) The answer is completely correct and shows a sound \nand accurate understanding of the topic. All key ele-\nments are addressed accurately and comprehensively.\nTable 3 Mean, standard deviation of the likert scales cores\nLanguage Model N Mean StDev Misinformation Rate\nGoogle BardA 40 1,3000 0,5164 25%\nChatGPT-3.5A, B 40 1,4500 0,5524 15%\nChatGPT-4B 40 1,7000 0,5164 10%\nDifferent letters indicate statistically significant difference (P <.05)\nFig. 3 Sample answer from ChatGPT-3.5\n \nPage 8 of 9\nÖzbay et al. BMC Oral Health           (2025) 25:648 \nand PubMed and made available for academic use. A \nchatbot that can provide information with real scientific \nreferences can thus provide convenience and save time \nfor clinicians and academics [15]. In addition to the exist-\ning LLMs, software to be developed with the input of \nscientific publications for use in the field of dentistry can \nhelp clinicians and researchers. With proper training and \ndevelopment of LLMs, both diagnostic accuracy can be \nincreased, and treatment planning can be facilitated. For \nexample, AI can help quickly gain insight into adverse \nevents encountered during endodontic procedures or \ndecide on appropriate root canal filling material instead \nof detecting endodontic lesions on radiographs. Scientific \nguidance from dental associations will undoubtedly play \na major role in the development of such software. Once \nvalidated, such software can serve both as a chairside \nassistant to dentists and be used in the training of dental \nstudents.\nConclusion\nThis study represents the evaluation of the accuracy and \ncompleteness of different LLMs as potential sources in \nendodontics. Compared to other LLMs, ChatGPT-4 \nseems to be ahead in providing accurate and informa -\ntive information in the field of endodontics. While this \nstudy, in agreement with the literature, proves that LLMs \nhave utilizable features in dentistry, they are not free of \nflaws. Although LLMs are not designed to impart knowl -\nedge in dentistry and do not claim to do so, with further \nimprovements in place, LLMs are promising tools for \nassisting dentists in endodontics.\nAbbreviations\nAI  Artificial intelligence\nLLMs  Large language models\nUSMLE  United States Medical Licensing Exam\nANOVA  One-way analysis of variance\nSupplementary Information\nThe online version contains supplementary material available at  h t t p s :   /  / d o  i .  o r  \ng  /  1 0  . 1 1   8 6  / s 1 2  9 0 3 -  0 2 5 - 0  6 0 5 0 - x.\nSupplementary Material 1\nAcknowledgements\nNot applicable.\nAuthor contributions\nY.Ö., designed the study. D.E, and G.A.D acquired data. Y.Ö, D.E, and G.A.D \ncompleted the analysis and writing of the manuscript. All authors reviewed \nthe manuscript and approved the final submission.\nFunding\nNot applicable.\nData availability\nData is provided within the manuscript or supplementary information files.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 20 January 2025 / Accepted: 23 April 2025\nReferences\n1. Schwendicke F, Samek W, Krois J. Artificial intelligence in dentistry: chances \nand challenges. J Dent Res. 2020;99(7):769–74.\n2. Howard J. Artificial intelligence: implications for the future of work. Am J Ind \nMed. 2019;62(11):917–26.\n3. Deng L. Artificial intelligence in the rising wave of deep learning: the \nhistorical path and future outlook [perspectives]. IEEE Signal Process Mag. \n2018;35(1):180–177.\n4. Abd-Alrazaq A, AlSaad R, Alhuwail D, Ahmed A, Healy PM, Latifi S, Aziz S, \nDamseh R, Alabed Alrazak S, Sheikh J. Large Language models in medical \neducation: opportunities, challenges, and future directions. JMIR Med Educ. \n2023;9:e48291.\n5. Revilla-Leon M, Gomez-Polo M, Vyas S, Barmak BA, Galluci GO, Att W, \nKrishnamurthy VR. Artificial intelligence applications in implant dentistry: A \nsystematic review. J Prosthet Dent. 2023;129(2):293–300.\n6. Plebani M. ChatGPT: Angel or Demond? Critical thinking is still needed. In., \nvol. 61: De Gruyter; 2023: 1131–1132.\n7. Cadamuro J, Cabitza F, Debeljak Z, De Bruyne S, Frans G, Perez SM, Ozdemir \nH, Tolios A, Carobene A, Padoan A. Potentials and pitfalls of ChatGPT and \nFig. 4 Mean, standard deviation and misinformation rates of the Likert Scales cores. Different letters indicate statistically significant difference ( P < 0.05)\n(Mean ± SD, n = 40). Misinformation rates of each language model were given inside columns with % symbol\n \nPage 9 of 9\nÖzbay et al. BMC Oral Health           (2025) 25:648 \nnatural-language artificial intelligence models for the Understanding of labo-\nratory medicine test results. An assessment by the European federation of \nclinical chemistry and laboratory medicine (EFLM) working group on artificial \nintelligence (WG-AI). Clin Chem Lab Med. 2023;61(7):1158–66.\n8. Li H, Moon JT, Purkayastha S, Celi LA, Trivedi H, Gichoya JW. Ethics of large \nLanguage models in medicine and medical research. Lancet Digit Health. \n2023;5(6):e333–5.\n9. Diniz-Freitas M, Rivas-Mundina B, Garcia-Iglesias JR, Garcia-Mato E, Diz-Dios P . \nHow ChatGPT performs in oral medicine: the case of oral potentially malig-\nnant disorders. Oral Dis 2023.\n10. Suarez A, Diaz-Flores Garcia V, Algar J, Gomez Sanchez M, Llorente de \nPedro M, Freire Y. Unveiling the ChatGPT phenomenon: evaluating the \nconsistency and accuracy of endodontic question answers. Int Endod J. \n2024;57(1):108–13.\n11. Cheong RCT, Unadkat S, McNeillis V, Williamson A, Joseph J, Randhawa P , \nAndrews P , Paleri V. Artificial intelligence chatbots as sources of patient edu-\ncation material for obstructive sleep apnoea: ChatGPT versus Google bard. \nEur Arch Otorhinolaryngol 2023.\n12. Giannakopoulos K, Kavadella A, Aaqel Salim A, Stamatopoulos V, Kaklamanos \nEG. Evaluation of generative artificial intelligence large Language models \nchatGPT, Google bard, and Microsoft Bing chat in supporting Evidence-based \ndentistry: A comparative Mixed-Methods study. J Med Internet Res 2023.\n13. Giannakopoulos K, Kavadella A, Stamatopoulos V, Kaklamanos E. Evaluation \nof generative artificial intelligence large Language models chatGPT, Google \nbard, and Microsoft Bing chat in supporting Evidence-based dentistry: A \ncomparative Mixed-Methods study. J Med Internet Res. 2023;25(1):e51580.\n14. Suárez A, Jiménez J, de Pedro ML, Andreu-Vázquez C, García VD-F, Sánchez \nMG, Freire Y. Beyond the scalpel: assessing ChatGPT’s potential as an auxiliary \nintelligent virtual assistant in oral surgery. Comput Struct Biotechnol J. \n2024;24:46–52.\n15. Balel Y. Can ChatGPT be used in oral and maxillofacial surgery? J Stomatology \nOral Maxillofacial Surg. 2023;124(5):101471.\n16. Ohta K, Ohta S. The performance of GPT-3.5, GPT-4, and bard on the Japanese \nNational dentist examination: A comparison study. Cureus. 2023; 15(12).\n17. Ourang SA, Sohrabniya F, Mohammad-Rahimi H, Dianat O, Aminoshariae A, \nNagendrababu V, Dummer PMH, Duncan HF, Nosrat A. Artificial intelligence \nin endodontics: fundamental principles, workflow, and tasks. Int Endod J. \n2024;57(11):1546–65.\n18. Setzer FC, Li J, Khan AA. The use of artificial intelligence in endodontics. J \nDent Res. 2024;103(9):853–62.\n19. Bragazzi NL, Szarpak Ł, Piccotti F. Assessing ChatGPT’s Potential in Endodon-\ntics: Preliminary Findings from A Diagnostic Accuracy Study. Available SSRN \n4631017 2023.\n20. Segura-Egea JJ, Gould K, Sen BH, Jonasson P , Cotti E, Mazzoni A, Sunay H, \nTjaderhane L, Dummer PMH. European society of endodontology position \nstatement: the use of antibiotics in endodontics. Int Endod J. 2018;51(1):20–5.\n21. Patel S, Brown J, Semper M, Abella F, Mannocci F. European society of \nendodontology position statement: use of cone beam computed tomogra-\nphy in endodontics: European society of endodontology (ESE) developed by. \nInt Endod J. 2019;52(12):1675–8.\n22. Galler K, Krastl G, Simon S, Van Gorp G, Meschi N, Vahedi B, Lambrechts P . \nEuropean society of endodontology position statement: revitalization proce-\ndures. Int Endod J. 2016;49(8):717–23.\n23. by, Duncan ESE, Galler H, Tomson K, Simon P , El-Karim S, Kundzina I, Krastl R, \nDammaschke G, Fransson T. European society of endodontology position \nstatement: management of deep caries and the exposed pulp. Int Endod J. \n2019;52(7):923–34.\n24. by, Krastl ESE, Weiger G, Filippi R, Van Waes A, Ebeleseder H, Ree K, Connert M, \nWidbiller T, Tjäderhane M. European society of endodontology position state-\nment: endodontic management of traumatized permanent teeth. Int Endod \nJ. 2021;54(9):1473–81.\n25. Wilson FR, Pan W, Schumsky DA. Recalculation of the critical values for Law-\nshe’s content validity ratio. Meas Evaluation Couns Dev. 2012;45(3):197–210.\n26. Snigdha NT, Batul R, Karobari MI, Adil AH, Dawasaz AA, Hameed MS, Mehta \nV, Noorani TY. Assessing the performance of ChatGPT 3.5 and ChatGPT 4 \nin operative dentistry and endodontics: an exploratory study. Hum Behav \nEmerg Technol. 2024;2024(1):1119816.\n27. Tanujaya B, Prahmana RCI, Mumu J. Likert scale in social sciences research: \nproblems and difficulties. FWU J Social Sci. 2022;16(4):89–101.\n28. Suarez A, Diaz-Flores Garcia V, Algar J, Sanchez MG, de Pedro ML, Freire Y. \nUnveiling the ChatGPT phenomenon: evaluating the consistency and accu-\nracy of endodontic question answers. Int Endod J. 2023.\n29. Alhaidry HM, Fatani B, Alrayes JO, Almana AM, Alfhaed NK. ChatGPT in den-\ntistry: A comprehensive review. Cureus. 2023;15(4):e38317.\n30. Goodman RS, Patrinely JR, Stone CA Jr., Zimmerman E, Donald RR, Chang \nSS, Berkowitz ST, Finn AP , Jahangir E, Scoville EA, et al. Accuracy and reli-\nability of chatbot responses to physician questions. JAMA Netw Open. \n2023;6(10):e2336483.\n31. Wang L, Wan Z, Ni C, Song Q, Li Y, Clayton E, Malin B, Yin Z. Applications and \nconcerns of ChatGPT and other conversational large Language models in \nhealth care: systematic review. J Med Internet Res. 2024;26:e22769.\n32. Mago J, Sharma M. The potential usefulness of ChatGPT in oral and maxil-\nlofacial radiology. Cureus. 2023;15(7):e42133.\n33. Newton P , Xiromeriti M. ChatGPT performance on multiple choice question \nexaminations in higher education. A pragmatic scoping review. Assess Evalu-\nation High Educ. 2024;49(6):781–98.\n34. Alan R, Alan BM. Utilizing ChatGPT-4 for providing information on periodontal \ndisease to patients: A DISCERN quality analysis. Cureus. 2023;15(9):e46213.\n35. Villena F, Véliz C, García-Huidobro R, Aguayo S. Generative artificial intel-\nligence in dentistry: current approaches and future challenges. ArXiv Preprint \narXiv:240717532 2024.\n36. Acar AH. Can natural Language processing serve as a consultant in oral \nsurgery? natural Language processing in oral surgery?. J Stomatology Oral \nMaxillofacial Surg 2023:101724.\n37. Vaira LA, Lechien JR, Abbate V, Allevi F, Audino G, Beltramini GA, Bergonzani \nM, Bolzoni A, Committeri U, Crimi S et al. Accuracy of ChatGPT-Generated \ninformation on head and neck and oromaxillofacial surgery: A multicenter \ncollaborative analysis. Otolaryngol Head Neck Surg 2023.\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.8998423218727112
    },
    {
      "name": "Endodontics",
      "score": 0.8213084936141968
    },
    {
      "name": "Oral and maxillofacial surgery",
      "score": 0.7971878051757812
    },
    {
      "name": "Clinical decision making",
      "score": 0.5506842732429504
    },
    {
      "name": "Dentistry",
      "score": 0.44732001423835754
    },
    {
      "name": "Medical physics",
      "score": 0.4167934060096741
    },
    {
      "name": "Orthodontics",
      "score": 0.3638969659805298
    },
    {
      "name": "Family medicine",
      "score": 0.31449592113494873
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I173761726",
      "name": "Karabük University",
      "country": "TR"
    },
    {
      "id": "https://openalex.org/I4210101192",
      "name": "Psychiatric Association of Turkey",
      "country": "TR"
    },
    {
      "id": "https://openalex.org/I96268669",
      "name": "Okan University",
      "country": "TR"
    }
  ]
}