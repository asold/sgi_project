{
    "title": "Myers-Briggs Personality Classification and Personality-Specific Language Generation Using Pre-trained Language Models",
    "url": "https://openalex.org/W2960748659",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287957989",
            "name": "Keh, Sedrick Scott",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227929432",
            "name": "Cheng, I-Tsun",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1979453298",
        "https://openalex.org/W2787278694",
        "https://openalex.org/W1523841156",
        "https://openalex.org/W2730785892",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2575864632",
        "https://openalex.org/W2807032982",
        "https://openalex.org/W2963128932"
    ],
    "abstract": "The Myers-Briggs Type Indicator (MBTI) is a popular personality metric that uses four dichotomies as indicators of personality traits. This paper examines the use of pre-trained language models to predict MBTI personality types based on scraped labeled texts. The proposed model reaches an accuracy of $0.47$ for correctly predicting all 4 types and $0.86$ for correctly predicting at least 2 types. Furthermore, we investigate the possible uses of a fine-tuned BERT model for personality-specific language generation. This is a task essential for both modern psychology and for intelligent empathetic systems.",
    "full_text": "arXiv:1907.06333v1  [cs.LG]  15 Jul 2019\nMyers-Briggs Personality Classiﬁcation and Personality- Speciﬁc\nLanguage Generation Using Pre-trained Language Models\nSedrick Scott Keh\nThe Hong Kong University of\nScience and T echnology\nsskeh@connect.ust.hk\nI-T sun Cheng\nThe Hong Kong University of\nScience and T echnology\nichengaa@connect.ust.hk\nAbstract\nThe Myers-Briggs T ype Indicator (MBTI)\nis a popular personality metric that uses\nfour dichotomies as indicators of per-\nsonality traits. This paper examines the\nuse of pre-trained language models to\npredict MBTI personality types based on\nscraped labeled texts. The proposed\nmodel reaches an accuracy of 0.47 for\ncorrectly predicting all 4 types and 0.86\nfor correctly predicting at least 2 types.\nFurthermore, we investigate the possi-\nble uses of a ﬁne-tuned BERT model\nfor personality-speciﬁc language gener-\nation. This is a task essential for both\nmodern psychology and for intelligent\nempathetic systems.\n1 Introduction\nProposed by psychoanalyst Carl Jung, the Myers-\nBriggs T ype Indicator (MBTI) is one of the most\ncommonly-used personality tests of the 21st\ncentury . Its prevalence ranges from casual Inter-\nnet users to large corporations who use it as part\nof their recruitment process (Michael, 2003).\nThis test uses four metrics to capture abstract\nideas related to one ’ s personality . These four di-\nchotomies are as follows:\n• introversion vs extroversion (I vs E)\n• sensing vs intuition (S vs N)\n• thinking vs feeling ( T vs F)\n• perception vs judging (P vs J)\nThe MBTI combines four letters to come up\nwith an overall personality type. In total, there\nare 2 4 = 16 personality types. Each of these types\ncorresponds to a unique personality class.\nOverall, personality classiﬁcation has many\ncomplexities because there are countless factors\ninvolved. Furthermore, even a human being\nmay not be able to accurately classify a person-\nality given a text. However , we hypothesize that\nusing pre-trained language models might allow\nus to pick up on various subtleties in how differ-\nent personality types use language. The objec-\ntive of this paper is thus two-fold: given a set of\nlabeled texts, ﬁrst, we want to train a model that\ncan predict the Myers-Briggs type, and second,\nuse this model to generate a stream of text given\na certain type.\n2 Related Work\n2.1 Personality Classiﬁcation Systems\nThe earliest forms of personality recognition\nsystems employed a combination of SVM and\nfeature engineering (Rangel et al., 2016). Other\nmethods also utilized part-of-speech frequen-\ncies (Litvinova et al., 2015). More recently , newer\nsystems have begun to use deep learning tech-\nniques such as convolutional neural networks\nand recurrent neural networks (Shi et al., 2017).\nThese have the advantage of being able to eas-\nily extract meaningful features, as well as cap-\nture temporal relationships between neighbor-\ning context words.\nT oday , state-of-the-art systems for personal-\nity classiﬁcation employ a wide combination\nof deep learning techniques. C2W2S4PT is a\nmodel that combines word bidirectional RNN\nand character bidirectional RNN to create hi-\nerarchical word and sentence representations\nfor personality modeling (Liu et al., 2016). Other\nmodels also incorporate audio and video inputs\nin CNNs to output more accurate predictions\n(Kampman et al., 2018).\n2.2 Pre-trained Language Models\nMore recently , pre-trained language mod-\nels such as BERT have begun to emerge\n(Devlin et al., 2018). BERT involves bidirectional\ntraining of transformers on two tasks simul-\ntaneously . These tasks are masked language\nmodelling and next sentence prediction. BERT\nhas led to numerous improvements in various\nareas of natural language processing, such\nas question answering and natural language\ninference (Zhang and Xu, 2018).\nHowever , to this day , very few research has\nbeen done on applying pre-trained language\nmodels on personality classiﬁcation. As such,\nthis paper introduces the use of BERT on clas-\nsifying personality . Furthermore, we also extend\nthis by training the models to be able to gener-\nate sentences given a type of personality . Such a\nfeature will be helpful especially for empathetic\ndialogue systems.\n3 Data Scraping and Preprocessing\n3.1 Scraping\nFor MBTI personality types, there is no standard\ndataset. Previous studies on the topic mostly\nscraped datasets from various social media plat-\nforms, including T witter (Plank and Hovy , 2015)\nand Reddit (Gjurkovic and Šnajder , 2018).\nFor this paper , we scraped\nposts from the forums in\nhttps://www.personalitycafe.com.\nPersonalityCafe ’ s MBTI forums are divided into\n16 distinct sections (one for each MBTI type).\nW e also analyzed and discovered that over 95%\nof users who post in a particular section identify\nas a member of that type, so these posts serve as\na good general representation of how people of\nthese personality types communicate.\nWhen scraping from this forum, we only con-\nsidered posts that were over 50 characters long,\nsince posts that are too short likely do not con-\ntain any meaningful information. Overall, we\nscraped the 5,000 most recent posts from each\nforum, resulting in a sample size of 68,733 posts\nand 3,827,558 words. (Some forums had less\nthan 5,000 posts). This data was divided in an\n85-15 train-test ratio.\n3.2 Cleaning and Preprocessing\nFor data cleaning, we ﬁrst removed all the sym-\nbols that were not letters, numbers, or punctu-\nations, and then put spaces after punctuations\nto separate them as new tokens. W e separated\nthings like ’ re or ’ll to be new tokens (ex. you’ re =\nyou + ’ re). Lastly , we converted all tokens to low-\nercase.\nAnother important preprocessing step was to\nget rid of the instances where MBTI types were\nexplicitly mentioned, and replace them with\nplaceholder \" < type> \", as these may distort the\ntask or make the task too easy for the classiﬁer .\n4 Classiﬁcation Methodology\n4.1 T okenization\nFor tokenization, we used BERT’ s custom to-\nkenizer , which includes masking and padding\nthe sentences. Sentences that were too long\nwere truncated by the indicated maximum se-\nquence length, and sentences that were too\nshort were padded with zeros. For classiﬁcation\ntasks such as this, the start of sentence was indi-\ncated with special token \"[CLS]\" and end of sen-\ntence with special token \"[SEP]\". This works be-\ncause the model has been pre-trained and we\nare only ﬁne-tuning over the pre-trained model\n(Devlin et al., 2018).\n4.2 BERT Model Fine-T uning and T raining\nThe ﬁne-tuning model was created using Py-\ntorch ’ s custom \"BertForSequenceClassiﬁcation\"\nmodel. Since the model was already pre-trained\non 110 million parameters, the ﬁne-tuning part\nbasically involves training the ﬁnal set of layers\non our own corpus of inputs and outputs. The\nmain purpose of ﬁne-tuning is to allow the BERT\nbidirectional transformer model to adapt to our\ncorpus and classiﬁcation task, since the model\nitself was pre-trained on different tasks. Here,\nthe main architecture for the BERT model’ s ﬁne-\ntuning for the sequence classiﬁcation task con-\nsists of 12 hidden layers with hidden size 768 and\na dropout probability of 0.1, as well as incorpo-\nrated attention functions with attention dropout\nof 0.1.\nIn the training part, we generated a dataloader\nand iterated over it. T raining was done by batch\n(batch size 32). For the loss function, a cross en-\ntropy loss was used, and for the optimizer , we\nused the BertAdam optimizer with warmup pro-\nportion 0.1 and weight decay 0.01.\n5 Classiﬁcation Results and Analysis\n5.1 Hyperparameter Optimization\nThere were 3 main parameters that we exam-\nined, namely learning rate, maximum sequence\nlength, and number of epochs. Other parame-\nters were mostly kept constant: we used bert-\nbase-uncased model, training batch size of 32,\nevaluation batch size of 8, and warmup propor-\ntion of 0.1. The results are highlighted in the ta-\nble in the following page.\nLearn. Rate Max Seq. Epochs Acc.\n0.001 128 5 0.0972\n0.0001 128 5 0.4701\n0.00001 128 5 0.4135\n0.000001 128 5 0.1739\n0.0000001 128 5 0.0901\n0.00001 128 30 0.4797\n0.00001 64 5 0.4138\n0.00001 256 5 0.4146\nT able 1: Classiﬁcation accuracy of different pa-\nrameter combinations.\n5.2 Discussion\nThe best accuracy of the model was 0.4797,\nwhich was achieved with a learning rate of 10 − 5 ,\nmaximum sequence length 128, and 30 epochs.\nThis model vastly outperforms other models and\nprevious benchmarks. Below is a table of how\nthis BERT model performs compares to previous\nstudies done on MBTI personality classiﬁcation:\nAdditionally , from the results of T able 1 (differ-\nent parameter combinations), we can also ana-\nlyze the effects of various parameters to the over-\nall accuracy . First, we see that the learning rate\nplays a very signiﬁcant role in the accuracy of the\nmodel. As we lower learning rate, the accuracy\nincreases then begins to decrease. At learning\nrate levels of 10 − 3 and 10 − 7 , accuracy was around\n0.09, which was basically close to random guess-\ning.\nFor the number of epochs, there is also a rela-\ntively signiﬁcant increase in accuracy: when the\nnumber of epochs were increased from 5 to 30\nfor the 10 − 5 learning rate model, the accuracy in-\ncreased from 0.4135 to 0.4797. Meanwhile, for\nthe maximum sequence length, the difference is\nnegligible, as changing from 128 to 64 or 256 only\nresulted in accuracy changes of less than 0.01.\nMethod Dataset Acc.\nLogistic Reg\n(Plank & Hovy ,\n2015)\nT witter (2.1\nmillion tweets) 0.190\nSVM\n(Gjurkovic &\nSnajder , 2018)\nReddit\n(22.9 million\ncomments)\n0.370\nLSTM\n(Cui & Qi, 2018)\nKaggle dataset\n(8675 sentences) 0.380\nBERT PersonalityCafe\nforums (68k posts) 0.479\nT able 2: Classiﬁcation accuracy of different pa-\nrameter combinations.\n5.3 Further Results: Other W ays to Measure\nAccuracy\nAside from the simple measure of accuracy (ex-\nact match of all 4 letters), since the MBTI per-\nsonality consists of 4 letters, we can also con-\nsider other the number of correctly classiﬁed let-\nters (personality categories) per prediction. This\nmakes sense because for instance, if the true\npersonality is INTJ, then a prediction of INTP\n(3 matches) would be a much better prediction\nas compared to a prediction of ESTP (1 match).\nThus, for our personality classiﬁer , we also con-\nsider the number of correctly classiﬁed letters as\na measure of accuracy:\nAt least\n1 match\nAt least\n2 matches\nAt least\n3 matches\nAt least\n4 matches\n0.9813 0.8573 0.6606 0.4797\nT able 3: Number of correctly predicted letters\nFrom the table above, we can see that almost\nall the time, there is at least 1 match. Proba-\nbilistically , the expected number of matches is\n2.9789.\nIn addition, we can also consider the accuracy\nof the predictions each of the letter categories,\nto see if the BERT language model works well as\na binary classiﬁer:\nFrom the table above, we notice that \"E/I\" and\n\"F/T\" have the highest distinctions, which indi-\ncates that these two are the classes that the BERT\nmodel ﬁnds easiest to discern. Comparatively ,\nE/I N/S F/T P/J\n0.7583 0.7441 0.7575 0.7190\nT able 4: Individual category accuracies\nthe model has a relatively harder time differenti-\nating between \"P/J\", although by not that large of\na difference. These observations are consistent\nacross other studies (Cui and Qi, 2017) and gen-\nerally reﬂect observations about the MBTI met-\nric.\nOverall, the performance as a binary classiﬁer\nis actually not that high, as some other models\nsuch as SVM were able to achieve an accuracy of\naround 0.80 (Gjurkovic and Šnajder , 2018). W e\nbelieve that with more training data and using\na larger BERT model (bert-large), we can fur-\nther increase the accuracy of our binary classiﬁer\nmodel.\n6 Language Generation from Given\nPersonality T ype\n6.1 Methodology\nIn this task, we want to be able to generate\na stream of text from a given MBTI person-\nality type. The language generation is built\nusing the Pytorch â ˘AIJBertForMaskedLanguage-\nModellingâ ˘A˙I custom model, which consists of\nthe BERT transformer with a fully pre-trained\nmasked language model. Similar to the case in\nthe sequence classiﬁcation task, the main archi-\ntecture of the pre-trained BERT model contains\n12 layers, a hidden size of 768, and around 110\nmillion parameters. However , while the BERT\nmodel used in classiﬁcation task focuses only on\nlower case words, the BERT model here accounts\nfor upper case words too, since we want the lan-\nguage generation model to train and learn when\nto generate upper-case and lower-case words.\nThe tokenizer implemented in this language\nmodel uses the same method as that in classiﬁ-\ncation.\nFor each personality type, we train the lan-\nguage model on the corresponding texts we\nscraped. W e compute the losses after each epoch\nand gather the ﬁnal losses of the experiment run\nfor each personality type as the results.\n6.2 Language Generation Results\nT ype Loss T ype Loss\nENFJ 0.01591 INFJ 0.032599\nENFP 0.021193 INFP 0.028531\nENTJ 0.02907 INTJ 0.028092\nENTP 0.030716 INTP 0.028124\nESFJ 0.017829 ISFJ 0.027062\nESFP 0.016334 ISFP 0.025123\nESTJ 0.016708 ISTJ 0.02662\nESTP 0.025886 ISTP 0.0239\nT able 5: Language Generation Results\nFor the hyperparameter settings, we used a\ntraining batch size of 16, learning rate of 3 · 10− 5 ,\nnumber of training epochs of 10, max sequence\nlength of 128, and a warmup proportion of 0.1.\nW e found this to be quite optimal in training the\nlanguage model of different personality types as\nthe loss decreases consistently . The results after\n10 epochs for each personality type are shown\nabove.\n6.3 Discussion\nThe results show that ENFJ, ESFJ, ESFP , ESTJ all\nhave the lowest loss among all the personality\ntypes as their losses are all under 0.02 after train-\ning for 10 epochs. All of these types contain\nâ ˘AIJEâ ˘A˙I as their value of the ﬁrst dichotomy . In-\nterestingly , this means BERT is better at generat-\ning language for extroverted personalities as op-\nposed to introverted personalities. This might be\ndue to the fact that there are more data for extro-\nverted personalities as they tend to be more ac-\ntive on forums and thus BERT is more capable of\nmimicking them.\nHere is an example of generated sentence\ntrained on ENFJ that has the lowest loss of all.\nThe red sentence is input and the highlighted\nsentence is generated.\n“\nI have no idea if he feels the same way ,\nand I am too afraid to press it. Our rela-\ntionship is very special to too lose them\nnot much reasons if for anyone if or\n[UNK] or so Kahn else anything which\nis public or now not is or or ...”\nComparing between the pairs that differ only\nby the second dichotomy \"N/S\" (for example,\nENFP and ESFP) indicates that S usually results\nin lower loss than its counterpart. Intuition per-\nsonalities are generally more less dependent on\nthe senses and logic, while sensing personalities\nusually rely on more facts, suggesting that BERT\ncan more easily generate more sensical and log-\nical language as opposed to abstract language.\nFor the last two dichotomies, \"F/T\" and \"J/P\",\nthey do not impact the performance of the lan-\nguage model that much. The ﬁrst two di-\nchotomies are more dominant in affecting the\nresults of language generation.\n7 Conclusion and Future Work\nIn this paper , we explored the use of pre-trained\nlanguage models (BERT ) in personality classiﬁ-\ncation. For the classiﬁcation task, the proposed\nmodel achieved an accuracy of 0.479. Further-\nmore, for the language generation, we achieved\nlosses of around 0.02. Possible improvements\nto this model would include using larger and\ncleaner datasets. If computation and memory\nresources permit, we can also use a larger bert\nmodel (bert-large trained on 340 million param-\neters) instead of bert-base.\nReferences\n[Cui and Qi2017] Brandon Cui and Calvin Qi. 2017.\nSurvey analysis of machine learning methods for\nnatural language processing for mbti personality\ntype prediction.\n[Devlin et al.2018] Jacob Devlin, Ming-W ei Chang,\nKenton Lee, and Kristina T outanova. 2018. BERT :\npre-training of deep bidirectional transformers for\nlanguage understanding. CoRR, abs/1810.04805.\n[Gjurkovic and Šnajder2018] Matej Gjurkovic and Jan\nŠnajder . 2018. Reddit: A gold mine for personality\nprediction. NAACL HL T 2018, page 87.\n[Kampman et al.2018] Onno Kampman, Elham\nJ. Barezi, Dario Bertero, and Pascale Fung. 2018.\nInvestigating audio, video, and text fusion meth-\nods for end-to-end automatic personality predic-\ntion. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics ( V ol-\nume 2: Short Papers) , pages 606–611, Melbourne,\nAustralia, July . Association for Computational\nLinguistics.\n[Litvinova et al.2015] T A Litvinova, PV Seredin, and\nOA Litvinova. 2015. Using part-of-speech se-\nquences frequencies in a text to predict author per-\nsonality: a corpus study . Indian Journal of Science\nand T echnology, 8:93.\n[Liu et al.2016] Fei Liu, Julien Perez, and Scott Now-\nson. 2016. A recurrent and compositional model\nfor personality trait recognition from short texts.\nIn Proceedings of the W orkshop on Computational\nModeling of People’s Opinions, Personality , and\nEmotions in Social Media (PEOPLES), pages 20–29,\nOsaka, Japan, December . The COLING 2016 Orga-\nnizing Committee.\n[Michael2003] James Michael. 2003. Using the\nmyers-briggs type indicator as a tool for leader-\nship development? apply with caution. Journal of\nLeadership & Organizational Studies, 10(1):68–81.\n[Plank and Hovy2015] Barbara Plank and Dirk Hovy .\n2015. Personality traits on twitterâ ˘AˇT orâ˘AˇThow to\nget 1,500 personality tests in a week. pages 92–98,\n01.\n[Rangel et al.2016] Francisco Rangel, Paolo Rosso,\nBen V erhoeven, W alter Daelemans, Martin Pot-\nthast, and Benno Stein. 2016. Overview of the\n4th author proﬁling task at pan 2016: cross-genre\nevaluations. In W orking Notes Papers of the CLEF\n2016 Evaluation Labs. CEUR W orkshop Proceed-\nings/Balog, Krisztian [edit.]; et al., pages 750–784.\n[Shi et al.2017] Z. Shi, M. Shi, and C. Li. 2017. The\nprediction of character based on recurrent neural\nnetwork language model. In 2017 IEEE/ACIS 16th\nInternational Conference on Computer and Infor-\nmation Science (ICIS), pages 613–616, May .\n[Zhang and Xu2018] Yuwen Zhang and Zhaozhuo Xu.\n2018. Bert for question answering on squad 2.0."
}