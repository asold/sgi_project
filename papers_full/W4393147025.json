{
  "title": "Graph Neural Prompting with Large Language Models",
  "url": "https://openalex.org/W4393147025",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2103307121",
      "name": "Yijun Tian",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A2152163733",
      "name": "Huan Song",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2164288045",
      "name": "zichen wang",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2415779747",
      "name": "Haozhu Wang",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2971532587",
      "name": "Ziqing Hu",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2076714287",
      "name": "Fang Wang",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A1979796846",
      "name": "Nitesh V. Chawla",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A2113036423",
      "name": "Panpan Xu",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2415779747",
      "name": "Haozhu Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979796846",
      "name": "Nitesh V. Chawla",
      "affiliations": [
        "Amazon (Germany)",
        "University of Notre Dame"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4379933424",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3021649351",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W3176175717",
    "https://openalex.org/W4309618995",
    "https://openalex.org/W6793601707",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W2971986145",
    "https://openalex.org/W3175270222",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4296605665",
    "https://openalex.org/W2972851234",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W2803457824",
    "https://openalex.org/W3167967639",
    "https://openalex.org/W4307309259",
    "https://openalex.org/W4281561077",
    "https://openalex.org/W6841633465",
    "https://openalex.org/W6805178984",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W4388275974",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W4381125108",
    "https://openalex.org/W4307003748",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W3152801999",
    "https://openalex.org/W3090656107",
    "https://openalex.org/W4226281578",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3172335055",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4386148087",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4387559676",
    "https://openalex.org/W4389217124",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4382317738",
    "https://openalex.org/W4205342726",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W4380993239",
    "https://openalex.org/W2963829073",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3118999024",
    "https://openalex.org/W4320858112",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W3182414949",
    "https://openalex.org/W4385734218",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4389500732",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3097986428",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2998374885",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4387210421",
    "https://openalex.org/W2892167328"
  ],
  "abstract": "Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP.",
  "full_text": "Graph Neural Prompting with Large Language Models\nYijun Tian1, Huan Song2, Zichen Wang2, Haozhu Wang2,\nZiqing Hu2, Fang Wang2, Nitesh V . Chawla1, Panpan Xu2\n1University of Notre Dame\n2Amazon\n{yijun.tian, nchawla}@nd.edu, {huanso, zichewan, haozhuw, ziqinghu, fwfang, xupanpan}@amazon.com\nAbstract\nLarge language models (LLMs) have shown remarkable\ngeneralization capability with exceptional performance\nin various language modeling tasks. However, they still\nexhibit inherent limitations in precisely capturing and\nreturning grounded knowledge. While existing work has\nexplored utilizing knowledge graphs (KGs) to enhance\nlanguage modeling via joint training and customized model\narchitectures, applying this to LLMs is problematic owing\nto their large number of parameters and high computational\ncost. Therefore, how to enhance pre-trained LLMs using\ngrounded knowledge, e.g., retrieval-augmented generation,\nremains an open question. In this work, we propose\nGraph Neural Prompting (GNP), a novel plug-and-play\nmethod to assist pre-trained LLMs in learning beneficial\nknowledge from KGs. GNP encompasses various designs,\nincluding a standard graph neural network encoder, a cross-\nmodality pooling module, a domain projector, and a self-\nsupervised link prediction objective. Extensive experiments\non multiple datasets demonstrate the superiority of GNP on\nboth commonsense and biomedical reasoning tasks across\ndifferent LLM sizes and settings. Code is available at\nhttps://github.com/meettyj/GNP.\nIntroduction\nLarge Language Models (LLMs) have demonstrated\nexceptional performance and general capability in\nvarious NLP tasks and use cases such as question\nanswering (Robinson, Rytting, and Wingate 2023) and\ntext summarization (Zhang et al. 2023). Moreover, the\nsignificant growth in model size has further endowed LLMs\nwith emergent capabilities (Wei et al. 2022b), laying the\ngroundwork for exploring artificial general intelligence\n(Bubeck et al. 2023). Accordingly, LLMs have attracted\ntremendous interest from academia (Wei et al. 2022a; Zhao\net al. 2023) and industry (Anil et al. 2023; OpenAI 2023).\nGiven the broad success of LLMs, many techniques\nhave emerged to adapt these general-purpose models to\ndownstream tasks. Beyond the conventional approach of\nmodel fine-tuning where all model parameters are adjusted\n(Howard and Ruder 2018), prompt-based adaptation methods\nare proposed to modulate a frozen LLM’s behavior through\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Result comparison across LLM Frozen (parameters\nunchanged) and LLM Tuned (parameters updated) settings.\nThe proposed Graph Neural Prompting significantly\nimproves the performance. Reported results are averaged\nacross six datasets on two tasks for an 11B FLAN-T5 model.\nprompts (Brown et al. 2020; Lester, Al-Rfou, and Constant\n2021; Li and Liang 2021). Rather than adapt the parameters\nin LLMs, these methods freeze the LLMs and typically\nintroduce additional trainable parameters. The idea of\nfreezing LLMs is appealing, especially as the model size\ngrows and the training resource dependency intensifies.\nOn the other hand, despite the success of LLMs in\nhandling different real-world applications and the feasibility\nof adapting to specific downstream tasks, they still exhibit\nthe inherent limitations of language modeling in accurately\ncapturing and returning grounded knowledge (Lewis et al.\n2020; Pan et al. 2023). Knowledge graphs (KGs), storing\nenormous facts, serve as a systematic way of representing\nknowledge (Ji et al. 2021). Consequently, existing methods\nhave incorporated KGs to assist language modeling, often by\ndesigning customized model architectures to accommodate\nboth KGs and textual data, followed by joint training sessions\n(Yasunaga et al. 2022; Zhang et al. 2022). Nonetheless,\njoint training KGs and text for LLMs is challenging due\nto the extensive parameters LLMs contain and the substantial\ncomputation resources they require. In addition, numerous\npre-trained LLMs with exceptional capabilities are released.\nIt becomes advantageous to employ these pre-existing LLMs,\nparticularly beneficial if we can sidestep the need to craft a\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19080\nspecialized model and train it from scratch. A direct approach\nto employing KGs for retrieval-augmented generation (Lewis\net al. 2020) is to feed the KG triples into LLMs directly\n(Baek, Aji, and Saffari 2023). However, this method can\nintroduce substantial noise, given that KGs might contain\nvarious extraneous contexts. Therefore, we ask:\nCan we learn beneficial knowledge from KGs\nand integrate them into pre-trained LLMs?\nTo answer the question, we propose Graph Neural Prompting\n(GNP), a novel plug-and-play method to assist pre-trained\nLLMs in learning beneficial knowledge from KGs. GNP\nretrieves and encodes the pertinent grounded knowledge\nto derive Graph Neural Prompt, an embedding vector\nthat can be sent into LLMs to provide guidance and\ninstructions. In particular, GNP first utilizes a graph neural\nnetwork (GNN) to capture and encode the intricate graph\nknowledge into entity/node embeddings. Then, a cross-\nmodality pooling module is present to determine the most\nrelevant node embeddings in relation to the text input, and\nconsolidate these node embeddings into a holistic graph-\nlevel embedding. After that, GNP encompasses a domain\nprojector to bridge the inherent disparities between the\ngraph and text domains. Finally, a self-supervised link\nprediction objective is introduced to enhance the model\ncomprehension of relationships between entities and capture\ngraph knowledge in a self-supervised manner.\nTo fully evaluate our model, we conduct extensive\nexperiments on multiple public benchmark datasets in the\ntasks of commonsense reasoning and biomedical reasoning.\nWe further report the results across different LLM sizes\nand settings. We conclude that GNP can effectively encode\nintricate knowledge in KGs and significantly improve\nperformance. Figure 1 shows the averaged performance\nimprovement using our method across six datasets.\nSpecifically, GNP improves the baseline by +13.5% when\nLLM is frozen, validating the superiority of our method in\nlearning effective prompts. In addition, by using our method,\nfine-tuning LLMs with parameter-efficient approach LoRA\n(Hu et al. 2022) shows an improvement of +1.8%. More\npromisingly, compared to model full fine-tuning without\nleveraging any efficient tuning approaches, our method can\nachieve competitive or superior performance in 10 out of\n12 evaluations, as shown in the experiment section. To\nsummarize, our main contributions are:\n• To the best of our knowledge, this is the first attempt to\nstudy the learning of beneficial knowledge from KGs for\npre-trained LLMs.\n• We propose GNP, a novel plug-and-play method for pre-\ntrained LLMs to extract valuable knowledge from KGs.\nThe proposed method contains various tailored designs,\nincluding a standard GNN, a cross-modality pooling\nmodule, a domain projector, and a self-supervised graph\nlearning objective.\n• Extensive experiments demonstrate the superiority of\nGNP on multiple datasets across different settings.\nWe also present the ablation study, model design\ncomparison, parameter sensitivity analysis, case study\nand visualization to validate the effectiveness of GNP.\nRelated Work\nLarge Language Models and Question Answering.\nRecently, various LLMs have been proposed (Chung et al.\n2022; Touvron et al. 2023; Brown et al. 2020) and have\ndemonstrated remarkable performance across different tasks\n(Shi et al. 2023; Chen et al. 2023b; Wei et al. 2024; Hong et al.\n2023). Question answering, as a fundamental task, demands\nintricate reasoning and understanding comprehension skills\nto interpret the text and provide appropriate responses to\nthe posed questions (Lu et al. 2022; Zhu et al. 2021; Wang\net al. 2023; Chen et al. 2023a). Although LLMs have strong\nlearning capabilities, they have the limitation of precisely\ncapturing accurate factual knowledge and are susceptible\nto generating unfounded responses (Zhao et al. 2023; Ji\net al. 2023; Bang et al. 2023). In addition, the enormous\nnumber of parameters in LLMs poses difficulties in adapting\nLLMs for downstream tasks (Scao et al. 2022; Smith et al.\n2022). Correspondingly, various approaches are presented to\nalleviate the intensive training dependency and reduce the\ncomputational expenses (Lester, Al-Rfou, and Constant 2021;\nLi and Liang 2021; Hu et al. 2022). For instance, Prompt\nTuning (Lester, Al-Rfou, and Constant 2021) introduces soft\nprompts to condition the pre-trained LLMs for downstream\ntasks. In our work, we propose to retrieve the factual\nknowledge from KGs to enhance LLMs, while still benefiting\nfrom circumventing the burdensome training expenses by\nusing pre-trained LLMs.\nKnowledge Graphs for Language Modeling. Many graph\nlearning methods are proposed to encode graphs and KGs\n(Ji et al. 2021; Tian et al. 2023a,b; Tang et al. 2022; Wang,\nJin, and Derr 2022; Xu et al. 2023; Kou et al. 2022). Recent\nstudies indicate that KGs can enhance language modeling by\nproviding background knowledge (Ren et al. 2021; Wang\net al. 2019). One approach to achieve this is integrating\nKGs into the pre-training stage of language modeling. For\ninstance, ERNIE (Sun et al. 2021), JAKET (Yu et al. 2022),\nand JointGT (Ke et al. 2021) develop pre-training objectives\ntailored for KG triples and the paired sentences. DRAGON\n(Yasunaga et al. 2022) introduces a customized fusion\nframework to jointly pre-train the model for KGs and text.\nMoreover, KGs are leveraged to assist language modeling\nfor question answering (Lin et al. 2019; Lv et al. 2020;\nFeng et al. 2020; Mihaylov and Frank 2018). Specifically,\nGreaseLM (Zhang et al. 2022) and QAGNN (Yasunaga\net al. 2021) suggest that KGs can scaffold reasoning about\nentities with the graph structure such as negation and multi-\nhop reasoning to facilitate complex question answering. To\nencode KGs, many works study methods to learn KG entity\nand relation embeddings, such as TransE (Bordes et al. 2013)\nand DistMult (Yang et al. 2015). Recently, with the aim of\nintegrating KGs into the emerging domain of LLMs, given\nexisting studies pose difficulties when applying, KAPING\n(Baek, Aji, and Saffari 2023) employs knowledge graphs to\nextract relevant triples. These triples correspond to the input\nquestion, with the expectation that directly feeding them into\nLLMs is beneficial, despite the presence of noise. In our\nwork, we present a learning method for identifying beneficial\nknowledge from KGs, offering substantial benefits to LLMs.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19081\nFigure 2: The overall framework. Given a multiple choice question, we first retrieve subgraphs from the knowledge graph based\non the entities in the question and options. We then develop Graph Neural Prompting (GNP) to encode the pertinent factual\nknowledge and structural information to obtain the Graph Neural Prompt. GNP contains various designs including a GNN, a\ncross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Later, the obtained Graph\nNeural Prompt is sent into LLM for inference along with the input text embedding. We utilize the standard maximum likelihood\nobjective for downstream task adaptation, while LLM is kept frozen or tuned depending on different experimental settings.\nPreliminary\nIn this section, we describe the knowledge graph and formally\ndefine the problem of multiple choice question answering.\nDefinition 1. Knowledge Graph. A knowledge graph is\ndefined as G = (E, R, T ), where E is the set of entities and\nR is the set of relations. T is the collection of fact triples\n{(eh, r, et)} ∈ E × R × E, where eh denotes the head entity,\nr is the relation, and et indicates the tail entity.\nProblem 1. Multiple Choice Question Answering. Given\na question Q, a set of answer options A = {ak}K\nk=1, and\nan optional context C depending on open-book or close-\nbook, the task is to design a machine learning model FΘ\nwith parameters Θ that selects the best option to answer the\nquestion. Here K denotes the total number of answer options\nand ak indicates the k-th answer option. The ground truth\nlabel y ∈ A is the correct answer for Q. In addition, we use\nknowledge graph G to provide rich knowledge and assist the\nmodel to answer the question.\nMethodology\nIn this section, we introduce the techniques of prompting\nLLMs for question answering as well as subgraph retrieval.\nAdditionally, we present Graph Neural Prompting and\nelaborate on its components and designs. Figure 2 illustrates\nthe framework of our method.\nPrompting LLMs for Question Answering\nPrompting is the de facto approach to elicit responses from\nLLMs (Liu et al. 2023). The typical approach of prompting\nLLMs for multi-choice question answering is simple. Given a\nquestion Q, the optional contextC, and the answer optionsA,\nwe first tokenize the concatenation ofC, Q, Ainto a sequence\nof input text tokens X. We then design a series of prompt\ntokens, P, and prepend it to the input text tokens X, which\nis later considered as input for the LLM model to generate\nprediction y′ = f([P, X]). The LLM model can be trained\nfor downstream task adaptation using a standard maximum\nlikelihood loss using teacher forcing (Williams and Zipser\n1989) and a cross-entropy loss:\nLllm = −log p(y|X, Θ), (1)\nwhere p is the probability distribution parameterized by the\nmodel. The prompt P can be either a hard prompt in the form\nof textual input, or a soft prompt in the form of learnable\nembedding vectors.\nUnlike existing methods that solely use a text string as the\nhard prompt, our Graph Neural Prompting approach encodes\nstructural and factual information contained in the knowledge\ngraph\nG into a soft promptP, which is a sequence of trainable\nvectors that can be concatenated with the token embedding of\nX. The learning of P is encouraged to provide rich structural\ninformation and knowledge fromG as well as task instruction\nfor each data instance.\nSubgraph Retrieval\nTo semantically align the input text tokens X with the\nmassive knowledge graph G with millions of nodes, we\nretrieve subgraphs of G that contain the relevant entities to\nthe tokens in X. In particular, for each answer option ak and\nits corresponding context C and question Q, we first obtain a\nset of matched entities Ematch via entity linking to match the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19082\ntokens in X to the entities in G. We then retrieve a subgraph\nG′ based on the entities in Ematch by including their two-hop\nneighbors and the relations that connect them (Yasunaga et al.\n2022). The retrieved subgraph contains the necessary content\nand knowledge to assist the model in answering Q.\nGraph Neural Prompting\nGraph Neural Prompting contains various designs, including\na GNN encoder that embeds the knowledge graph, a cross-\nmodality pooling module that determines the pertinent\nnode embeddings, a domain projector that bridges the\ndiscrepancies between graph and text, and a self-supervised\nlink prediction objective that encourages the model to\nrecognize structural information.\nGNN Encoder. Although the retrieved subgraph G′ contains\nrich contextual information regarding the question and\nanswer choices, some entities and relations are not relevant\nto the actual question. Directly feeding every fact triples in\nG′ can introduce noise and prevent the LLM model from\nconcentrating on the critical information. Therefore, we\nintroduce a GNN to encode the most relevant knowledge and\nfurther integrate the complex relationships among the entities.\nIn particular, we first initialize the node embeddings using\npre-trained entity embeddings (Feng et al. 2020; Yasunaga,\nLeskovec, and Liang 2022). Next, we employ a standard\ngraph attention network (Veliˇckovi´c et al. 2018) as our GNN\nencoder for the retrieved subgraph G′.\nH1 = fGNN (G′), (2)\nwhere H1 ∈ Rdg represents the node embeddings learned\nby GNN for every node in G′, and dg denotes the output\ndimension of the GNN encoder.\nCross-modality Pooling. With the aim of identifying the\nmost pertinent nodes in relation to the question, and\nconsolidating the node embeddings into a holistic graph-\nlevel representation for subsequent use, we design the cross-\nmodality pooling module. In particular, we first introduce a\nself-attention layer to dynamically identify node significance\nusing the internal graph characteristics and the implicit\ninteractions among nodes:\nH2 = Self-Attn(H1), (3)\nwhere H2 is the obtained node embeddings and Self-Attn\nindicates the self-attention component. Then, we leverage the\ntextual prompt to calculate the importance of nodes within\nthe graph. To ensure uniformity, we utilize the dictionary in\nthe LLM to obtain the text embeddings T ∈Rdt for every\ntoken in the input text, where dt denotes the dimension of the\nLLM dictionary. Next, we apply a transformation to the text\nembeddings T and obtain the transformed text embedding\nT ′, ensuring that the dimension of T ′ matches the dimension\ndg of node embeddings H2. After that, we calculate the cross-\nmodality attention using H2 and T ′. We use H2 as the query\nand the T ′ as the key and value. The procedure is as follows:\nT ′ = FFN1(σ(FFN2(T ))),\nH3 = softmax[H2 · (T ′)T /\np\ndg] · T′,\n(4)\nwhere σ is the GELU activation function, FFN1 and FFN2\nare feed-forward neural networks, and H3 is the final\nnode embeddings obtained with cross-modality attention\nconsidered. Next, we generate the graph-level embedding\nby average pooling the node embeddings H3 in G′:\nH4 = POOL(H3), (5)\nwhere H4 represents the graph-level embedding that takes\ninto account the node significance in G′.\nDomain Projector. In order to create a mapping between\nthe graph-level embeddings and the text domain to facilitate\ncomprehension by the LLM, we design a domain projector\nto align them. This projector aims to bridge the inherent\ndisparities between the graph and text, allowing for more\nseamless integration. In addition, the projector maps the\ngraph-level embeddings to the same dimension\ndt of\nLLM, which ensures compatibility and consistency when\ninterfacing with the LLM’s inherent structures. We design\nthe projector as follows:\nZ = FFN3(σ(FFN4(H4))), (6)\nwhere Z denotes Graph Neural Prompt, the final output of\nGNP, and FFN3, FFN4 are feed-forward neural networks.\nSelf-supervised Link Prediction. While the downstream\ncross-entropy objective enables the model to learn and adapt\nto the target dataset, we design a link prediction task to further\nrefine its understanding of relationships between entities\nand capture graph knowledge in a self-supervised manner.\nSpecifically, we mask out some edges in G′ and enforce the\nmodel to predict them. This encourages the model to learn to\nuse the partial graph content and structure to reason about the\nmissing links. Concretely, we denote the set of masked-out\nedges as Emask ⊆ E. Given the learned node embeddings of\nthe head entity and tail entity in a triplet {h3, t3} ∈H3, we\nadopt a widely-used knowledge graph embedding method\nDistMult (Yang et al. 2015) to map the entity embeddings\nand relation in the KG to vectors, h, r, t. We then define the\nscoring function ϕ(eh, et) = ⟨h, r, t⟩ to generate the scores\nfor each triple, where ⟨·, ·, ·⟩ denotes the trilinear dot product,\nand r represents the relations in KGs. A higher ϕ indicates\na higher chance of (eh, r, et) being a correct positive triple\ninstead of an incorrect negative triple. We enforce the model\nto predict the masked edges in Emask as positive and other\nrandom edges as negative. The link prediction loss Llp is\ndefined as follows:\nLlp =\nX\n(eh,r,et)∈Emask\n(Spos + Sneg), (7)\nwhere Spos = −log σs(ϕ(eh, et) + γ) indicates the\nscore for correct positive triples, γ is the margin, σs is\nthe sigmoid function, {(e′\nh, r, e′\nt)} are n negative triples\ncorresponding to the positive triplet (eh, r, et), and Sneg =\n1\nn\nP\n(e′\nh,r,e′\nt) log σs(ϕ(e′\nh, e′\nt) + γ) is the score for incorrect\nnegative triples. The final objective function L is defined as\nthe weighted combination of Lllm and Llp:\nL = Lllm + λLlp, (8)\nwhere λ is a trade-off weight for balancing two losses.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19083\nCommonsense Reasoning Biomedical Reasoning\nLLM Setting Method OBQA ARC PIQA Riddle PQA BioASQ Total\nFLAN-T5\nxlarge\n(3B)\nLLM Frozen\nLLM-only 69.20 68.24 58.43 53.73 71.50 65.85 64.49\nPrompt Designs* 72.20 70.99 60.94 52.75 70.50 67.48 65.33\nKG Flattening REL 61.80 64.12 57.56 43.33 69.25 65.04 60.18\nKG Flattening BFS 62.80 63.86 56.69 44.12 69.25 65.04 60.29\nKAPING TH 58.80 63.52 52.34 40.78 70.00 65.04 58.41\nKAPING OH 60.00 63.09 51.69 41.37 70.00 65.04 58.53\nPrompt Tuning 72.20 70.64 60.83 53.33 72.00 66.67 65.95\nGNP 79.80 71.85 61.48 66.86 76.75 89.43 74.36\n∆PT ↑ 10.53% ↑ 1.71% ↑ 1.07% ↑ 25.37% ↑ 6.60% ↑ 34.14% ↑ 12.76%\nLLM T\nuned\nFull Fine-tuning 82.80 73.30 63.55 74.12 76.25 91.06 76.85\nLoRA 80.40 71.33 63.76 72.94 76.25 92.68 76.23\nLoRA + GNP 83.40 72.45 64.31 75.49 76.25 92.68 77.43\n∆LoRA ↑ 3.73% ↑ 1.57% ↑ 0.86% ↑ 3.50% ↑ 0.00% ↑ 0.00% ↑ 1.58%\nFLAN-T5\nxxlarge\n(11B)\nLLM Frozen\nLLM-only 76.80 68.93 56.58 61.37 71.75 65.85 66.88\nPrompt Designs* 79.60 74.16 58.00 60.59 71.25 66.67 68.38\nKG Flattening REL 72.80 66.78 56.80 53.53 69.50 66.67 64.35\nKG Flattening BFS 72.40 66.95 56.37 54.90 68.75 65.85 64.20\nKAPING TH 60.60 57.25 53.21 48.43 68.75 66.67 59.15\nKAPING OH 60.00 56.65 52.99 47.65 69.25 66.67 58.87\nPrompt Tuning 78.80 74.85 61.26 61.37 70.00 65.04 68.55\nGNP 87.20 78.20 63.66 70.98 76.75 90.24 77.84\n∆PT ↑ 10.66% ↑ 4.48% ↑ 3.92% ↑ 15.66% ↑ 9.64% ↑ 38.75% ↑ 13.54%\nLLM T\nuned\nFull Fine-tuning 89.40 76.82 65.61 80.78 78.00 92.68 80.55\nLoRA 88.60 78.54 65.61 74.90 77.75 91.06 79.41\nLoRA + GNP 89.60 78.71 65.94 76.67 79.75 94.31 80.83\n∆LoRA ↑ 1.13% ↑ 0.22% ↑ 0.50% ↑ 2.36% ↑ 2.57% ↑ 3.57% ↑ 1.79%\nTable 1: Overall experimental results on commonsense reasoning and biomedical reasoning tasks. The best results across different\nLLM sizes and settings are highlighted in bold. ∆PT and ∆LoRA represent the relative performance improvement of our method\nto Prompt Tuning and LoRA, respectively. We also include the full fine-tuning result in gray color for further reference. * means\nmultiple prompt design methods are evaluated while only the best result is reported. Accuracy is used as the evaluation metric.\nExperiments\nIn this section, we conduct extensive experiments to compare\nthe performances of different models. We also show ablation\nstudy, model design comparison, and parameter sensitivity\nanalysis to demonstrate the effectiveness of GNP. Moreover,\nwe present case study and visualization to provide an intuitive\nunderstanding and illustrate how KGs benefit.\nExperiment Setup\nKnowledge Graphs and Datasets. We conduct experiments\non both the general domain (commonsense reasoning) and\nthe biomedical domain (biomedical reasoning). For the\nused knowledge graphs, we consider ConceptNet (Speer,\nChin, and Havasi 2017) that contains rich commonsense\nknowledge regarding the daily concepts, and Unified Medical\nLanguage System (UMLS) (Bodenreider 2004) that involves\nwell-structured health and biomedical information. For\ndatasets, we use four commonsense reasoning datasets,\nincluding OpenBookQA (OBQA) (Mihaylov et al. 2018),\nAI2 Reasoning Challenge (ARC) (Clark et al. 2018), Physical\nInteraction Question Answering (PIQA) (Bisk et al. 2020),\nand RiddleSense (Riddle) (Lin et al. 2021). In addition, we\nconsider PubMedQA (PQA) (Jin et al. 2019) and BioASQ\n(Tsatsaronis et al. 2015) for biomedical reasoning.\nTwo Settings: LLM Frozen vs. LLM Tuned. To fully\nevaluate the model, we employ two settings: LLM Frozen\nand LLM Tuned. For LLM Frozen, we keep the parameters in\nLLM unchanged and only adapt the prompt. For LLM Tuned,\nthe original LLM parameters are updated for downstream\ntasks by utilizing LoRA or full fine-tuning.\nBaselines. In the setting of LLM Frozen, we compare with\nnine baselines, including LLM-only that uses no prompt,\nthree prompt design methods that use different instructions\nas hard prompts, KG Flattening that flattens the nodes\nin the graph into a sequence via relevance score (REL)\nranking (Yasunaga et al. 2022) or breadth-first search (BFS),\nKAPING (Baek, Aji, and Saffari 2023) that injects the\nimportant KG triples within one-hop (OH) and two-hop (TH)\nneighborhoods, and Prompt Tuning (Lester, Al-Rfou, and\nConstant 2021) that introduces soft prompts. In the setting\nof LLM Tuned, we compare with LoRA that updates partial\nLLM parameters. In addition, we include full model fine-\ntuning results as the referencing benchmark.\nImplementation Details. For the proposed model, we set the\nlearning rate to 1e-4, batch size to 8, hidden dimension of\nGNN to 1024, and training epochs to 50. In order to adapt\nthe model effectively to each dataset, we search the GNN\nlayers from 2 to 5, cross-modality pooling layers from 1 to 3,\ntrade-off weight λ from {0.1, 0.5}, and link drop rate from\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19084\n{0.1, 0.3, 0.7}. We choose FLAN-T5 xlarge (3B parameters)\nand xxlarge (11B parameters) as the LLMs used in this paper.\nWe adjust the maximum sequence length of LLMs to best fit\nthe question length for each dataset. We run all experiments\non four NVIDIA Tesla V100 GPUs with 24GB RAM.\nPerformance Comparison\nTo comprehensively evaluate our model, we conduct rigorous\nexperiments using various LLMs across different tasks and\nsettings in Table 1. According to the table, in the setting of\nLLM Frozen, we observe that the utilization of the prompt\ndesign instructions often yields performance improvement,\ncompared to LLM-only that uses no instructions, though\nthe enhancement is mostly marginal. Interestingly, the\nbaseline methods that inject KG information directly (KG\nFlattening and KAPING) can significantly hurt the model\nperformance. This aligns with our motivation that KGs\ncontain irrelevant contexts for the downstream tasks that\ncould introduce noises or even alter the semantics if not\nhandled carefully. While Prompt Tuning shows improved\noutcomes using the trainable soft prompts, their improvement\nis trivial. In contrast, our GNP exhibits significant and notable\nperformance improvements across various datasets, settings,\nand LLMs. For example, for the commonsense reasoning task,\nGNP provides +25.37% improvement on Riddle for 3B LLM,\nand +15.66% improvement for 11B LLM. In addition, for the\nbiomedical reasoning task, GNP improves the performance\nby +34.14% on BioASQ for 3B LLM and +38.75% for 11B\nLLM. In general, GNP achieves an improvement of+12.76%\nand +13.54% for 3B and 11B LLM, respectively.\nIn the setting of LLM Tuned, we first study the\nperformance in comparison with LoRA and then report the\nmodel full fine-tuning for additional reference. As shown in\nthe table, LoRA is a significantly more powerful approach\nthan Prompt Tuning due to the direct update of the LLM\ninternal parameters. Combining with the proposed GNP, the\nperformance can be further improved. For example, GNP\nachieves 3.73% improvement on OBQA for 3B LLM, and\n3.57% improvement on BioAQS for 11B LLM. Moreover,\nmodel full fine-tuning is an important reference to study the\nperformance gap since LoRA only updates a small fraction\nof the model parameters. Surprisingly, we find that the\nincorporation of GNP can surpass the results of full fine-\ntuning. In contrast, relying solely on LoRA shows difficulties\nin achieving a comparable performance of full fine-tuning. In\ntotal, our final performance matches or surpasses model full\nfine-tuning in 10 out of 12 evaluations across different LLM\nsizes and datasets, as shown in Table 1.\nAblation Study\nSince GNP contains various model components (i.e., cross-\nmodality pooling (CMP), self-supervised link prediction\n(SLP), and domain projector (DP)), we conduct ablation\nstudies to analyze the contributions of different components\nby removing each of them independently (see Table\n2). Specifically, removing DP significantly affects the\nperformance, showing that DP has a large contribution to the\nproposed method. In addition, the decreasing performances\nof removing CMP and SLP demonstrate the effectiveness\nCommonsense Biomedical\nLLM V ariant OBQA ARC PQA BioASQ\nFLAN-T5\nxlarge\n(3B)\nw/o CMP 78.00 69.44 76.00 86.18\nw/o SLP 78.80 69.18 75.75 88.62\nw/o DP 73.00 70.30 76.25 83.74\nGNP 79.80 71.85 76.75 89.43\nFLAN-T5\nxxlarge\n(11B)\nw/o CMP 85.20 76.91 75.75 87.80\nw/o SLP 83.60 76.74 73.25 89.43\nw/o DP 79.40 74.59 71.75 85.37\nGNP 87.20 78.20 76.25 90.24\nTable 2: Results of ablation study.\nCommonsense Biomedical\nLLM Design OBQA ARC PQA BioASQ\nFLAN-T5\nxlarge\n(3B)\nGNP 79.80 71.85 76.75 89.43\n+ DLP 79.80 70.30 75.50 89.43\n+ RGNN 79.00 71.49 75.50 89.43\nFLAN-T5\nxxlarge\n(11B)\nGNP 87.20 78.20 76.25 90.24\n+ DLP 86.20 76.05 75.00 88.62\n+ RGNN 85.20 76.48 75.25 89.43\nTable 3: Results of integrating different model designs.\nof CMP and SLP in enhancing the model. In most cases,\nSLP yields greater significance compared to CMP, while\nin BioASQ, CMP plays a more important role. Finally, the\nproposed GNP achieves the best results in all cases, indicating\nthe strong capability of different components in our model.\nModel Design Comparison\nA salient property of GNP is the learning of Graph Neural\nPrompt for each data instance, i.e., various questions yield\ndifferent retrieved subgraphs, resulting in unique prompts.\nGiven its distinction to the dataset-level prompt (DLP) from\nPrompt Tuning that learns prompt for each dataset, we present\nthe outcomes of integrating DLP for further investigation.\nAs shown in Table 3, incorporating DLP cannot further\nboost the performance and might even diminish it in certain\ncases. This indicates that our instance-level prompt provides\nadequate guidance for LLM to perform well. In addition, we\nvalidate the importance of explicitly modeling relations using\na widely-used Relational GNN (RGNN) (Zhang et al. 2022).\nThe observed decline in performance suggests that a standard\nGNN is sufficient to capture the graph information, and\nexplicitly modeling the relations might increase the difficulty\nof generating suitable guidance for the task.\nParameter Sensitivity\nNext, we perform sensitivity analysis focusing on the\nfollowing parameters: the number of GNN layers and the\nnumber of layers in the cross-modality pooling component.\nImpact of GNN Layers. We evaluate the influence of GNN\nlayers for both 3B and 11B models in Figure 3. According\nto the figure, we have the following observations. First,\nvarious datasets have different optimal numbers of GNN\nlayers. To illustrate, for ARC, 3 layers can achieve the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19085\nFigure 3: Impact of GNN layers.\nFigure 4: Impact of cross-modality pooling layers.\noptimal performance while 4 layers perform the best for\nPQA. Second, the optimal number of GNN layers for 3B\nand 11B LLMs differs. For example, for OBQA, 3 layers\nwork best for 3B LLM, while 11B LLM reaches its top\nperformance when using 5 layers. Third, choosing different\nGNN layers can have a weak impact on some datasets while\ncan also drastically affect the performance on other datasets.\nTo demonstrate, increasing from 3 layers to 5 layers for\n11B LLM can decrease the performance on ARC by a large\nmargin (from 78.1 to 74.3), while adjusting the layers for\nBioASQ may not lead to a big change in the performance.\nImpact of Cross-modality Pooling Layers. We report the\nperformance of different cross-modality pooling layers in\nFigure 4. As shown in the figure, we observe that the\ncommonsense reasoning dataset OBQA and biomedical\nreasoning dataset BioASQ demonstrate different reactions\nto layer numbers. Specifically, for OBQA, the performance\nof the larger 11B LLM increases with more layers, while\nthe performance of the smaller 3B LLM decreases. On the\nother hand, for BioASQ, the larger 11B LLM tends to show\na degraded performance when adding more layers, while the\nsmaller 3B model presents an improved performance. This\nindicates that suitable cross-modality pooling layers can lead\nto the best model performance.\nCase Study and Visualization\nFor a more intuitive understanding and comparison, we\nrandomly select two examples from the OBQA dataset\nand visualize the retrieved subgraphs in Figure 5. For\nvisualization clarity, we only show question entities and a\nlimited number of their neighbors. We remarkably notice that\nthe retrieved subgraphs encompass certain entities for the\ncorrect answer, and there exist edges connecting the question\nand answer entities, which makes the task of question\nFigure 5: Case study on two QA examples from OBQA\ndataset. Question entities are marked in orange and their\nsubsampled neighbors in the KG are marked in blue. The\nentities appearing in the correct answer are marked in green.\nanswering easier by leveraging this information.\nTo answer the question “What is the best way to guess\na babies eye color?”, Prompt Tuning makes the wrong\ngeneration “Just take a random guess”. On the other hand,\nour retrieved subgraph offers the links that directly relate\nthe entity “babies” to “family”, “record”, and further to\n“genealogy”, which all appear in the correct option (d). This\nimportant context provides valuable insights for the model.\nNote that the subgraph also contains irrelevant entities such\nas “round” and “nursery”. This explains why directly using\nthe knowledge graph can introduce noise. However, our GNP\nmethod possesses the capability to collect the most critical\ninformation in the graph to determine the correct answer.\nThe second question “were there fossil fuels in the ground\nwhen humans evolved?” requires correctly identifying the\nhistorical sequencing order between the entity “humans” and\n“fossil fuels”. The retrieved subgraph contains the critical\nrelation, i.e., “humans”, “evolve”, “prior”, “fossil fuel”.\nNevertheless, the subgraph also contains the entity “created”\nthat could confuse the model into selecting option (a). GNP\nis able to capture the structural proximity among the key\nentities and select the correct answer (c).\nConclusion\nIn this paper, we address the limitations of LLMs in precisely\ncapturing and returning grounded knowledge. In particular,\nwe propose Graph Neural Prompting (GNP), a novel plug-\nand-play method to assist pre-trained LLMs in learning\nbeneficial knowledge from KGs. Extensive experiments on\ncommonsense and biomedical reasoning tasks demonstrate\nthat GNP can improve the performance by +13.5% when\nLLM is frozen, and +1.8% when LLM is tuned. In addition,\nwe present ablation studies, model design comparison,\nparameter sensitivity, case study and visualization to validate\nthe effectiveness of the proposed method.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19086\nReferences\nAnil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin,\nD.; Passos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen,\nZ.; et al. 2023. Palm 2 technical report. arXiv preprint\narXiv:2305.10403.\nBaek, J.; Aji, A. F.; and Saffari, A. 2023. Knowledge-\nAugmented Language Model Prompting for Zero-Shot\nKnowledge Graph Question Answering. In ACL Workshop\non Matching Entities.\nBang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; et al. 2023. A\nmultitask, multilingual, multimodal evaluation of chatgpt on\nreasoning, hallucination, and interactivity. arXiv preprint\narXiv:2302.04023.\nBisk, Y .; Zellers, R.; Gao, J.; Choi, Y .; et al. 2020. Piqa:\nReasoning about physical commonsense in natural language.\nIn AAAI.\nBodenreider, O. 2004. The unified medical language system\n(UMLS): integrating biomedical terminology. Nucleic acids\nresearch.\nBordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and\nYakhnenko, O. 2013. Translating embeddings for modeling\nmulti-relational data. In NeurIPS.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. In\nNeurIPS.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; et al. 2023. Sparks of artificial general intelligence: Early\nexperiments with gpt-4. arXiv preprint arXiv:2303.12712.\nChen, X.; Jiang, J.-Y .; Chang, W.-C.; Hsieh, C.-J.; Yu,\nH.-F.; and Wang, W. 2023a. MinPrompt: Graph-based\nMinimal Prompt Data Augmentation for Few-shot Question\nAnswering. arXiv preprint arXiv:2310.05007.\nChen, X.; Liu, Y .; Yang, Y .; Yuan, J.; You, Q.; Liu, L.-P.;\nand Yang, H. 2023b. Reason out Your Layout: Evoking the\nLayout Master from Large Language Models for Text-to-\nImage Synthesis. arXiv preprint arXiv:2311.17126.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .;\nFedus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.;\net al. 2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nClark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.;\nSchoenick, C.; and Tafjord, O. 2018. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457.\nFeng, Y .; Chen, X.; Lin, B. Y .; Wang, P.; Yan, J.; and Ren, X.\n2020. Scalable multi-hop relational reasoning for knowledge-\naware question answering. In EMNLP.\nHong, J.; Wang, J. T.; Zhang, C.; Li, Z.; Li, B.; and\nWang, Z. 2023. DP-OPT: Make Large Language Model\nYour Privacy-Preserving Prompt Engineer. arXiv preprint\narXiv:2312.03724.\nHoward, J.; and Ruder, S. 2018. Universal language model\nfine-tuning for text classification. In ACL.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang, S.;\nWang, L.; and Chen, W. 2022. LoRA: Low-Rank Adaptation\nof Large Language Models. In ICLR.\nJi, S.; Pan, S.; Cambria, E.; Marttinen, P.; and Philip, S. Y .\n2021. A survey on knowledge graphs: Representation,\nacquisition, and applications. IEEE transactions on neural\nnetworks and learning systems.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii,\nE.; Bang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey\nof hallucination in natural language generation. ACM\nComputing Surveys.\nJin, Q.; Dhingra, B.; Liu, Z.; Cohen, W. W.; and Lu, X.\n2019. Pubmedqa: A dataset for biomedical research question\nanswering. In EMNLP.\nKe, P.; Ji, H.; Ran, Y .; Cui, X.; Wang, L.; Song, L.; Zhu, X.;\nand Huang, M. 2021. Jointgt: Graph-text joint representation\nlearning for text generation from knowledge graphs. In ACL-\nIJCNLP.\nKou, Z.; Zhang, Y .; Zhang, D.; and Wang, D. 2022.\nCrowdGraph: A Crowdsourcing Multi-Modal Knowledge\nGraph Approach to Explainable Fauxtography Detection.\nProceedings of the ACM on Human-Computer Interaction.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The power\nof scale for parameter-efficient prompt tuning. In EMNLP.\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V .;\nGoyal, N.; K¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt¨aschel, T.;\net al. 2020. Retrieval-augmented generation for knowledge-\nintensive nlp tasks. In NeurIPS.\nLi, X. L.; and Liang, P. 2021. Prefix-tuning: Optimizing\ncontinuous prompts for generation. In ACL-IJCNLP.\nLin, B. Y .; Chen, X.; Chen, J.; and Ren, X. 2019.\nKagnet: Knowledge-aware graph networks for commonsense\nreasoning. In EMNLP.\nLin, B. Y .; Wu, Z.; Yang, Y .; Lee, D.-H.; and Ren, X. 2021.\nRiddleSense: Reasoning about Riddle Questions Featuring\nLinguistic Creativity and Commonsense Knowledge. In ACL-\nIJCNLP.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2023. Pre-train, prompt, and predict: A systematic survey\nof prompting methods in natural language processing. ACM\nComputing Surveys.\nLu, P.; Mishra, S.; Xia, T.; Qiu, L.; Chang, K.-W.; Zhu, S.-C.;\nTafjord, O.; Clark, P.; and Kalyan, A. 2022. Learn to explain:\nMultimodal reasoning via thought chains for science question\nanswering. In NeurIPS.\nLv, S.; Guo, D.; Xu, J.; Tang, D.; Duan, N.; Gong, M.;\nShou, L.; Jiang, D.; Cao, G.; and Hu, S. 2020. Graph-\nbased reasoning over heterogeneous external knowledge for\ncommonsense question answering. In AAAI.\nMihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A. 2018.\nCan a suit of armor conduct electricity? a new dataset for\nopen book question answering. In EMNLP.\nMihaylov, T.; and Frank, A. 2018. Knowledgeable reader:\nEnhancing cloze-style reading comprehension with external\ncommonsense knowledge. In ACL.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19087\nOpenAI. 2023. GPT-4 Technical Report. arXiv preprint\narXiv:2303.08774.\nPan, S.; Luo, L.; Wang, Y .; Chen, C.; Wang, J.; and Wu, X.\n2023. Unifying Large Language Models and Knowledge\nGraphs: A Roadmap. arXiv preprint arXiv:2306.08302.\nRen, H.; Dai, H.; Dai, B.; Chen, X.; Yasunaga, M.; Sun, H.;\nSchuurmans, D.; Leskovec, J.; and Zhou, D. 2021. Lego:\nLatent execution-guided reasoning for multi-hop question\nanswering on knowledge graphs. In ICML.\nRobinson, J.; Rytting, C. M.; and Wingate, D. 2023.\nLeveraging large language models for multiple choice\nquestion answering. In ICLR.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili´c, S.; Hesslow,\nD.; Castagn´e, R.; Luccioni, A. S.; Yvon, F.; Gall´e, M.; et al.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100.\nShi, Y .; Xu, S.; Liu, Z.; Liu, T.; Li, X.; and Liu, N. 2023.\nMededit: Model editing for medical question answering with\nexternal knowledge bases. arXiv preprint arXiv:2309.16035.\nSmith, S.; Patwary, M.; Norick, B.; LeGresley, P.;\nRajbhandari, S.; Casper, J.; Liu, Z.; Prabhumoye, S.;\nZerveas, G.; Korthikanti, V .; et al. 2022. Using deepspeed\nand megatron to train megatron-turing nlg 530b, a\nlarge-scale generative language model. arXiv preprint\narXiv:2201.11990.\nSpeer, R.; Chin, J.; and Havasi, C. 2017. Conceptnet 5.5: An\nopen multilingual graph of general knowledge. In AAAI.\nSun, Y .; Wang, S.; Feng, S.; Ding, S.; Pang, C.; Shang,\nJ.; Liu, J.; Chen, X.; Zhao, Y .; Lu, Y .; et al. 2021.\nErnie 3.0: Large-scale knowledge enhanced pre-training\nfor language understanding and generation. arXiv preprint\narXiv:2107.02137.\nTang, Z.; Pei, S.; Zhang, Z.; Zhu, Y .; Zhuang, F.; Hoehndorf,\nR.; and Zhang, X. 2022. Positive-unlabeled learning\nwith adversarial data augmentation for knowledge graph\ncompletion. In IJCAI.\nTian, Y .; Dong, K.; Zhang, C.; Zhang, C.; and Chawla, N. V .\n2023a. Heterogeneous Graph Masked Autoencoders. In\nAAAI.\nTian, Y .; Zhang, C.; Guo, Z.; Zhang, X.; and Chawla, N.\n2023b. Learning MLPs on Graphs: A Unified View of\nEffectiveness, Robustness, and Efficiency. In ICLR.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971.\nTsatsaronis, G.; Balikas, G.; Malakasiotis, P.; Partalas, I.;\nZschunke, M.; Alvers, M. R.; Weissenborn, D.; Krithara, A.;\nPetridis, S.; Polychronopoulos, D.; et al. 2015. An overview\nof the BIOASQ large-scale biomedical semantic indexing\nand question answering competition. BMC bioinformatics.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\nP.; and Bengio, Y . 2018. Graph attention networks. InICLR.\nWang, X.; Kapanipathi, P.; Musa, R.; Yu, M.; Talamadupula,\nK.; Abdelaziz, I.; Chang, M.; Fokoue, A.; Makni, B.; Mattei,\nN.; et al. 2019. Improving natural language inference using\nexternal knowledge in the science questions domain. InAAAI.\nWang, Y .; Jin, W.; and Derr, T. 2022. Graph neural\nnetworks: Self-supervised learning. Graph Neural Networks:\nFoundations, Frontiers, and Applications.\nWang, Y .; Lipka, N.; Rossi, R. A.; Siu, A.; Zhang, R.;\nand Derr, T. 2023. Knowledge Graph Prompting for\nMulti-Document Question Answering. arXiv preprint\narXiv:2308.11730.\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2022a. Finetuned\nlanguage models are zero-shot learners. In ICLR.\nWei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;\nBorgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler,\nD.; et al. 2022b. Emergent abilities of large language models.\nTransactions on Machine Learning Research.\nWei, W.; Ren, X.; Tang, J.; Wang, Q.; Su, L.; Cheng, S.; Wang,\nJ.; Yin, D.; and Huang, C. 2024. LLMRec: Large Language\nModels with Graph Augmentation for Recommendation. In\nWSDM.\nWilliams, R. J.; and Zipser, D. 1989. A learning algorithm for\ncontinually running fully recurrent neural networks. Neural\ncomputation.\nXu, Z.; Zeng, H.; Tan, J.; Fu, Z.; Zhang, Y .; and Ai, Q.\n2023. A Reusable Model-agnostic Framework for Faithfully\nExplainable Recommendation and System Scrutability. ACM\nTransactions on Information Systems.\nYang, B.; Yih, W.-t.; He, X.; Gao, J.; and Deng, L. 2015.\nEmbedding entities and relations for learning and inference\nin knowledge bases. In ICLR.\nYasunaga, M.; Bosselut, A.; Ren, H.; Zhang, X.; Manning,\nC. D.; Liang, P. S.; and Leskovec, J. 2022. Deep bidirectional\nlanguage-knowledge graph pretraining. In NeurIPS.\nYasunaga, M.; Leskovec, J.; and Liang, P. 2022. Linkbert:\nPretraining language models with document links. In ACL.\nYasunaga, M.; Ren, H.; Bosselut, A.; Liang, P.; and Leskovec,\nJ. 2021. QA-GNN: Reasoning with language models and\nknowledge graphs for question answering. In NAACL.\nYu, D.; Zhu, C.; Yang, Y .; and Zeng, M. 2022. Jaket: Joint\npre-training of knowledge graph and language understanding.\nIn AAAI.\nZhang, T.; Ladhak, F.; Durmus, E.; Liang, P.; McKeown,\nK.; and Hashimoto, T. B. 2023. Benchmarking large\nlanguage models for news summarization. arXiv preprint\narXiv:2301.13848.\nZhang, X.; Bosselut, A.; Yasunaga, M.; Ren, H.; Liang,\nP.; Manning, C. D.; and Leskovec, J. 2022. GreaseLM:\nGraph REASoning Enhanced Language Models for Question\nAnswering. In ICLR.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .;\nMin, Y .; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. A survey\nof large language models. arXiv preprint arXiv:2303.18223.\nZhu, F.; Lei, W.; Wang, C.; Zheng, J.; Poria, S.; and Chua,\nT.-S. 2021. Retrieving and reading: A comprehensive\nsurvey on open-domain question answering. arXiv preprint\narXiv:2101.00774.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19088",
  "topic": "Graph",
  "concepts": [
    {
      "name": "Graph",
      "score": 0.4669042229652405
    },
    {
      "name": "Computer science",
      "score": 0.46486932039260864
    },
    {
      "name": "Theoretical computer science",
      "score": 0.20679861307144165
    }
  ]
}