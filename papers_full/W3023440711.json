{
  "title": "Image Captioning Through Image Transformer",
  "url": "https://openalex.org/W3023440711",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5035606153",
      "name": "Sen He",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A5101803082",
      "name": "Wentong Liao",
      "affiliations": [
        "Leibniz University of Applied Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5002546998",
      "name": "Hamed R. Tavakoli",
      "affiliations": [
        "Nokia (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A5014321482",
      "name": "Michael Ying Yang",
      "affiliations": [
        "University of Twente"
      ]
    },
    {
      "id": "https://openalex.org/A5040412734",
      "name": "Bodo Rosenhahn",
      "affiliations": [
        "Leibniz University of Applied Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5087140377",
      "name": "Nicolas Pugeault",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2550553598",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2795151422",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W2965597639",
    "https://openalex.org/W2983141445",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2136848157",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2970682957",
    "https://openalex.org/W2970550739",
    "https://openalex.org/W2971351900",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2903538854",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2971614030",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2973039175",
    "https://openalex.org/W2971546166",
    "https://openalex.org/W2556967412",
    "https://openalex.org/W3103651098",
    "https://openalex.org/W2971310675",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W2963420686"
  ],
  "abstract": null,
  "full_text": "Image Captioning through Image Transformer\nSen He‚ãÜ1, Wentong Liao‚ãÜ2, Hamed R. Tavakoli3, Michael Yang4\nBodo Rosenhahn2, and Nicolas Pugeault 5\n1 CVSSP, University of Surrey, UK\n2 Leibniz University of Hanover, Germany\n3 Nokia Technologies, Finland\n4 University of Twente, Netherlands\n5 School of Computing Science, University of Glasgow\nsenhe752@gmail.com\nAbstract. Automatic captioning of images is a task that combines the\nchallenges of image analysis and text generation. One important as-\npect of captioning is the notion of attention: how to decide what to\ndescribe and in which order. Inspired by the successes in text analysis\nand translation, previous works have proposed the transformer archi-\ntecture for image captioning. However, the structure between the se-\nmantic units in images (usually the detected regions from object de-\ntection model) and sentences (each single word) is diÔ¨Äerent. Limited\nwork has been done to adapt the transformer‚Äôs internal architecture to\nimages. In this work, we introduce the image transformer, which con-\nsists of a modiÔ¨Åed encoding transformer and an implicit decoding trans-\nformer, motivated by the relative spatial relationship between image re-\ngions. Our design widens the original transformer layer‚Äôs inner archi-\ntecture to adapt to the structure of images. With only regions feature\nas inputs, our model achieves new state-of-the-art performance on both\nMSCOCO oÔ¨Ñine and online testing benchmarks. The code is available\nat https://github.com/wtliao/ImageTransformer.\n1 Introduction\nImage captioning is the task of describing the content of an image in words.\nThe problem of automatic image captioning by AI systems has received a lot\nof attention in the recent years, due to the success of deep learning models for\nboth language and image processing. Most image captioning approaches in the\nliterature are based on a translational approach, with a visual encoder and a\nlinguistic decoder. One challenge in automatic translation is that it cannot be\ndone word by word, but that other words inÔ¨Çuence then meaning, and there-\nfore the translation, of a word; this is even more true when translating across\nmodalities, from images to text, where the system must decide what must be\ndescribed in the image. A common solution to this challenge relies on attention\nmechanisms. For example, previous image captioning models try to solve where\n‚ãÜ Equal contribution\narXiv:2004.14231v2  [cs.CV]  2 Oct 2020\n2 Sen He et al.\nto look in the image [1,2,3,4] (now partly solved by the Faster-RCNN object\ndetection model [5]) in the encoding stage and use a recurrent neural network\nwith attention mechanism in the decoding stage to generate the caption. But\nmore than just to decide what to describe in the image, recent image caption-\ning models propose to use attention to learn how regions of the image relate to\neach other, eÔ¨Äectively encoding their context in the image. Graph convolutional\nneural networks [6] were Ô¨Årst introduced to relate regions in the image; however,\nthose approaches [7,8,9,10] usually require auxiliary models (e.g. visual relation-\nship detection and/or attribute detection models) to build the visual scene graph\nin the image in the Ô¨Årst place. In contrast, in the natural language processing\nÔ¨Åeld, the transformer architecture [11] was developed to relate embedded words\nin sentences, and can be trained end to end without auxiliary models explicitly\ndetecting such relations. Recent image captioning models [12,13,14] adopted the\ntransformer architectures to implicitly relate informative regions in the image\nthrough dot-product attention achieving state-of-the-art performance.\nHowever, the transformer architecture was designed for machine translation\nof text. In a text, a word is either to the left or to the right of another word,\nwith diÔ¨Äerent distances. In contrast, images are two-dimensional (indeed, rep-\nresent three-dimensional scenes), so that a region may not only be on the left\nor right of another region, it may also contain or be contained in another re-\ngion. The relative spatial relationship between the semantic units in images has\na larger degree of freedom than that in sentences. Furthermore, in the decoding\nstage of machine translation, a word is usually translated into another word in\nother languages (one to one decoding), whereas for an image region, we may\ndescribe its context, its attribute and/or its relationship with other regions (one\nto more decoding). One limitation of previous transformer-based image caption-\ning models [12,13,14] is that they adopt the transformer‚Äôs internal architecture\ndesigned for the machine translation, where each transformer layer contains a\nsingle (multi-head) dot-product attention module. In this paper, we introduce\nthe image transformer for image captioning, where each transformer layer\nimplements multiple sub-transformers, to encode spatial relationships between\nimage regions and decode the diverse information in image regions.\nThe diÔ¨Äerence between our method and previous transformer based models\n[12,14,13] is that our method focuses on the inner architectures of the trans-\nformer layer, in which we widen the transformer module. Yao et al. [10] used a\nhierarchical concept in the encoding part of their model, our model focuses on\nthe local spatial relationships for each query region whereas their method is a\nglobal tree hierarchy. Furthermore, our model does not require auxiliary models\n(ie, for visual relation detection and instance segmentation) to build the visual\nscene graph. Our encoding method can be viewed as the combination of a visual\nsemantic graph and a spatial graph which use a transformer layer to implicitly\ncombine them without auxiliary relationship and attribute detectors.\nThe contributions of this paper can be summarised as follows:\nImage Captioning through Image Transformer 3\nImage \nCaptioningA little girl is blowing\ncandles on the birthday \ncake\nMachine \nTranslationEin kleines M√§dchen \nbl√§st Kerzen auf die \nGeburtstagstorte\nFig. 1: Image captioning vs machine translation.\n‚Äì We propose a novel internal architecture for the transformer layer adapted\nto the image captioning task, with a modiÔ¨Åed attention module suited to the\ncomplex natural structure of image regions.\n‚Äì We report thorough experiments and ablation study were done in the work to\nvalidate our proposed architecture, state-of-the-art performance was achieved\non the MSCOCO image captioning oÔ¨Ñine and online testing dataset with\nonly region features as input.\nThe rest of the paper is organized as follows: Sec. 2 reviews the related\nattention-based image captioning models; Sec. 3 introduces the standard trans-\nformer model and our proposed image transformer; followed by the experiment\nresults and analysis in Sec. 4; Ô¨Ånally, we will conclude this paper in Sec. 5.\n2 Related Work\nWe characterize current attention-based image captioning models into single-\nstage attention models, two-stages attention models, visual scene graph based\nmodels, and transformer-based models. We will review them one by one in this\nsection.\n2.1 Single-Stage Attention Based Image Captioning\nSingle-stage attention-based image captioning models are the models where at-\ntention is applied at the decoding stage, where the decoder attends to the most\ninformative region [15] in the image when generating a corresponding word.\nThe availability of large-scale annotated datasets [16,17] enabled the train-\ning of deep models for image captioning. Vinyals et al. [18] proposed the Ô¨Årst\ndeep model for image captioning. Their model uses a CNN pre-trained on Ima-\ngeNet [16] to encode the image, then a LSTM [19] based language model is used\nto decode the image features into a sequence of words. Xuet al.[1] introduced an\nattention mechanism into image captioning during the generation of each word,\nbased on the hidden state of their language model and the previous generated\nword. Their attention module generates a matrix to weight each receptive Ô¨Åeld\nin the encoded feature map, and then feed the weighted feature map and the\nprevious generated word to the language model to generate the next word. In-\nstead of only attending to the receptive Ô¨Åeld in the encoded feature map, Chenet\nal. [2] added a feature channel attention module, their channel attention module\n4 Sen He et al.\nre-weight each feature channel during the generation of each word. Not all the\nwords in the sentence have a correspondence in the image, so Lu et al.[20] pro-\nposed an adaptive attention approach, where their model has a visual sentinel\nwhich adaptively decides when and where to rely on the visual information.\nThe single-stage attention model is computational eÔ¨Écient, but lacks accu-\nrate positioning of informative regions in the original image.\n2.2 Two-Stages Attention Based Image Captioning\nTwo stage attention models consists of bottom-up attention and top-down at-\ntention, where bottom-up attention Ô¨Årst uses object detection models to detect\nmultiple informative regions in the image, then top-down attention attends to\nthe most relevant detected regions when generating a word.\nInstead of relying on the coarse receptive Ô¨Åelds as informative regions in the\nimage, as single-stage attention models do, Andersonet al.[3] train the detection\nmodels on the Visual Genome dataset [21]. The trained detection models can\ndetect 10 ‚àí100 informative regions in the image. They then use a two-layers\nLSTM network as decoder, where the Ô¨Årst layer generates a state vector based\non the embedded word vector and the mean feature of the detected regions and\nthe second layer uses the state vector from the previous layer to generate a\nweight for each detected region. The weighted sum of detected regions feature\nis used as a context vector for predicting the next word. Lu et al. [4] developed\na similar network, but with a detection model trained on MSCOCO [22], which\nis a smaller dataset than Visual Genome, and therefore less informative regions\nare detected.\nThe performance of two-stage attention based image captioning models is im-\nproved a lot against single-stage attention based models. However, each detected\nregion is isolated from others, lacking the relationship with other regions.\n2.3 Visual Scene Graph Based Image Captioning\nVisual scene graph based image captioning models extend two-stage attention\nmodels by injecting a graph convolutional neural network to relate detected\ninformative regions, and therefore reÔ¨Åne their features before feeding into the\ndecoder.\nYao et al.[7] developed a model which consists of a semantic scene graph and\na spatial scene graph. In the semantic scene graph, each region is connected with\nother semantically related regions, those relationships are usually determined by\na visual relationship detector among a union box. In the spatial scene graph, the\nrelationship between two regions is deÔ¨Åned by their relative positions. Then the\nfeature of each node in the scene graph is reÔ¨Åned with their related nodes through\ngraph neural networks [6]. Yang et al. [8] use an auto-encoder, where they Ô¨Årst\nencode the graph structure in the sentence based on the SPICE [23] evaluation\nmetric to learn a dictionary, then the semantic scene graph is encoded using the\nlearnt dictionary. The previous two works treat the semantic relationships as\nedges in the scene graph, while Guo et al. [9] treat them as nodes in the scene\nImage Captioning through Image Transformer 5\ngraph. Also, their decoder focuses on diÔ¨Äerent aspects of a region. Yao et al.[10]\nfurther introduces the tree hierarchy and instance level feature into the scene\ngraph.\nIntroducing the graph neural network to relate informative regions yields\na sizeable performance improvement for image captioning models, compared\nto two-stage attention models. However, it requires auxiliary models to detect\nand build the scene graph at Ô¨Årst. Also those models usually have two parallel\nstreams, one responsible for the semantic scene graph and another for spatial\nscene graph, which is computationally ineÔ¨Écient.\n2.4 Transformer Based Image Captioning\nTransformer based image captioning models use the dot-product attention mech-\nanism to relate informative regions implicitly.\nSince the introduction of original transformer model [11], more advanced\narchitectures were proposed for machine translation based on the structure or the\nnatural characteristic of sentences [24,25,26]. In image captioning, AoANet [12]\nuses the original internal transformer layer architecture, with the addition of a\ngated linear layer [27] on top of the multi-head attention. The object relation\nnetwork [14] injects the relative spatial attention into the dot-product attention.\nAnother interesting result described by Herdade et al. [14] is that the simple\nposition encoding (as proposed in the original transformer) did not improve\nimage captioning performance. The entangled transformer model [13] features a\ndual parallel transformer to encode and reÔ¨Åne visual and semantic information\nin the image, which is fused through gated bilateral controller.\nCompared to scene graph based image captioning models, transformer based\nmodels do not require auxiliary models to detect and build the scene graph at\nÔ¨Årst, which is more computational eÔ¨Écient. However current transformer based\nmodels still use the inner architecture of the original transformer, designed for\ntext, where each transformer layer has a single multi-head dot-product attention\nreÔ¨Åning module. This structure does not allow to model the full complexity\nof relations between image regions, therefore we propose to change the inner\narchitecture of the transformer layer to adapt it to image data. We widen the\ntransformer layer, such that each transformer layer has multiple reÔ¨Åning modules\nfor diÔ¨Äerent aspects of regions both in the encoding and decoding stages.\n3 Image Transformer\nIn this section, we Ô¨Årst review the original transformer layer [11], we then elab-\norate the encoding and decoding part for the proposed image transformer\narchitecture.\n3.1 Transformer Layer\nA transformer consists of a stack of multi-head dot-product attention based\ntransformer reÔ¨Åning layer.\n6 Sen He et al.\nFaster-RCNN\n l1l2l3\nRefinement through spatialgraph transformerùë®\nLSTM\nRegiondetection\nùê¥! l1\nDecoding through implicit decoding transformerùê¥!\nùëä\"ùúã\nA girl is blowing candles on birthday cakeùíâùíÑ\nFig. 2: The overall architecture of our model, the reÔ¨Ånement part consists of 3\nstacks of spatial graph transformer layer, and the decoding part has a LSTM\nlayer with a implicit decoding transformer layer.\nIn each layer, for a given input A ‚ààRN√óD, consisting of N entries of D di-\nmensions. In natural language processing, the input entry can be the embedded\nfeature of a word in a sentence, and in computer vision or image captioning, the\ninput entry can be the feature describing a region in an image. The key function\nof transformer is to reÔ¨Åne each entry with other entries through multi-head dot-\nproduct attention. Each layer of a transformer Ô¨Årst transforms the input into\nqueries (Q= AWQ, WQ ‚ààRD√óDk), keys (K = AWK, WK ‚ààRD√óDk) and val-\nues (V = AWV, WA ‚ààRD√óDv) though linear transformations, then the scaled\ndot-product attention is deÔ¨Åned by:\nAttention(Q,K,V ) = Softmax\n(QKT\n‚àöDk\n)\nV, (1)\nwhere Dk is the dimension of the key vector and Dv the dimension of the value\nvector (D = Dk = Dv in the implementation). To improve the performance of\nthe attention layer, multi-head attention is applied:\nMultiHead(Q,K,V ) = Concat(head1,..., headh)WO,\nheadi = Attention(AWQi,AWKi,AWVi). (2)\nThe output from the multi-head attention is then added with the input and\nnormalised:\nAm = Norm(A+ MultiHead(Q,K,V )), (3)\nwhere Norm(¬∑) denote layer normalisation.\nThe transformer implements residual connections in each module, such that\nthe Ô¨Ånal output of a transformer layer is:\nA\n‚Ä≤\n= Norm(Am + œÜ(AmWf)), (4)\nwhere œÜ is a feed-forward network with non-linearity.\nEach reÔ¨Åning layer takes the output of its previous layer as input (the Ô¨Årst\nlayer takes the original input). The decoding part is also a stack of transformer\nreÔ¨Åning layers, which take the output of encoding part as well as the embedded\nfeatures of previous predicted word.\nImage Captioning through Image Transformer 7\n3.2 Spatial Graph Encoding Transformer Layer\n(a)\n (b)\nFig. 3: (a) Image with detected regions; (b) An example of query region in the\nimage (man in the red bounding box), and its neighbor regions (regions in blue\nbounding boxes, bull, umbrella, etc), child regions (regions in the yellow bound-\ning boxes, hair,cloth).\nIn contrast to the original transformer, which only considers spatial relation-\nships between query and key pairs asneighborhood, we propose to use a spatial\ngraph transformer in the encoding part, where we consider three common cate-\ngories of spatial relationship for each query region in a graph structure: parent,\nneighbor, and child (an example shown in Fig. 3). Thus we widen each trans-\nformer layer by adding three sub-transformer layers in parallel in each layer, each\nsub-transformer responsible for a category of spatial relationship, all sharing the\nsame query. In the encoding stage, we deÔ¨Åne the relative spatial relationship be-\ntween two regions based on their overlap. We Ô¨Årst compute the graph adjacent\nmatrices ‚Ñ¶p ‚ààRN√óN (parent node adjacent matrix), ‚Ñ¶n ‚ààR‚ààN√óN (neighbor\nnode adjacent matrix), and ‚Ñ¶c ‚ààR‚ààN√óN (child node adjacent matrix) for all\nregions in the image:\n‚Ñ¶p[l,m] =\nÔ£±\nÔ£≤\nÔ£≥\n1, if Area(l‚à©m)\nArea(l) ‚©æ œµ and Area(l‚à©m)\nArea(l) > Area(l‚à©m)\nArea(m)\n0, otherwise.\n‚Ñ¶c[l,m] = ‚Ñ¶p[m,l]\nwith\n‚àë\ni‚àà{p,n,c}\n‚Ñ¶i[l,m] = 1\n(5)\nwhere œµ= 0.9 in our experiment. The spatial graph adjacent matrices are used\nas the spatial hard attention embedded into each sub-transformer to combine\nthe output of each sub-transformer in the encoder. More speciÔ¨Åcally, the original\n8 Sen He et al.\nQ K V\nInput\nOutput\nQKp Kn Kc Vc Vn vp\ndot product \nattention\ndot product \nattention\n  \n  \n  \nOutput\nInput\nQK1 K2 K3 V1 V2 v3\ndot product \nattention\nOutput glu\nInput1\nInput2\nOriginal transformer layer Our encoding transformer layer Our decoding transformer layer\nFig. 4: The diÔ¨Äerence between the original transformer layer and the proposed\nencoding and decoding transformer layers.\nencoding transformer deÔ¨Åned in Eqs. (1) and (2) are reformulated as:\nAttention(Q,Ki,Vi) = ‚Ñ¶i ‚ó¶Softmax\n(QKT\ni‚àö\nd\n)\nVi, (6)\n‚ó¶is the Hadamard product, and\nAm = Norm\nÔ£´\nÔ£≠A+\n‚àë\ni‚àà{p,n,c}\nMultiHead(Q,Ki,Vi)\nÔ£∂\nÔ£∏. (7)\nAs we widen the transformer, we halve the number of stacks in the encoder to\nachieve similar complexity as the original one (3 stacks, while the original trans-\nformer features 6 stacks). With our formulation, we combined the spatial graph\nand semantic graph (the scene graph based methods [7,9] require two branches\nto encode them) into a transformer layer. Note that the original transformer\narchitecture is a special case of the proposed architecture, when no region in the\nimage either contains or is contained by another.\n3.3 Implicit Decoding Transformer Layer\nOur decoder consists of a LSTM [28] layer and an implicit transformer decoding\nlayer, which we proposed to decode the diverse information in a region in the\nimage. The LSTM layer is a common memory module and the transformer layer\ninfers the most relevant region in the image through dot product attention.\nAt Ô¨Årst, the LSTM layer receives the mean of the output ( A= 1\nN\n‚àëN\ni=1 A\n‚Ä≤\ni)\nfrom the encoding transformer, a context vector (ct‚àí1) at last time step and the\nembedded feature vector of current word in the ground truth sentence:\nxt = [WeœÄt,A+ ct‚àí1]\nht,mt = LSTM(xt,ht‚àí1,mt‚àí1) (8)\nImage Captioning through Image Transformer 9\nWhere, We is the word embedding matrix, œÄt is the tth word in the ground truth.\nThe output state ht is then transformed linearly and treated as the query for\nthe input of the implicit decoding transformer layer. The diÔ¨Äerence between the\noriginal transformer layer and our implicit decoding transformer layer is that we\nalso widen the decoding transformer layer by adding several sub-transformers\nin parallel in one layer, such that each sub-transformer can implicitly decode\ndiÔ¨Äerent aspects of a region. It is formalised as follows:\nAD\nt,i = MultiHead(WDQht,WDKiA\n‚Ä≤\n,WDViA\n‚Ä≤\n) (9)\nThen, the mean of the sub-transformers‚Äô output is passed through a gated linear\nlayer (GLU) [27] to extract the new context vector ( ct) at the current step by\nchannel:\nct = GLU\n(\nht, 1\nM\nM‚àë\ni=1\nAD\nt,i\n)\n(10)\nThe context vector is then used to predict the probability of word at time step\nt:\np(yt|y1:t‚àí1) = Softmax(wpct + bp) (11)\nThe overall architecture of our model is illustrated in Fig. 2, and the diÔ¨Äerence\nbetween the original transformer layer and our proposed encoding and decoding\ntransformer layer is showed in Fig. 4.\n3.4 Training Objectives\nGiven a target ground truth as a sequence of words y‚àó\n1:T, for training the model\nparameters Œ∏, we follow the previous method, such that we Ô¨Årst train the model\nwith cross-entropy loss:\nLXE(Œ∏) = ‚àí\nT‚àë\nt=1\nlog(pŒ∏(y‚àó\nt|y‚àó\n1:t‚àí1)) (12)\nthen followed by self-critical reinforced training [29] optimizing the CIDEr score\n[30]:\nLR(Œ∏) = ‚àíE(y1:T‚àºpŒ∏)[r(y1:T)] (13)\nwhere r is the score function and the gradient is approximated by:\n‚ñΩŒ∏ ‚âà‚àí(r(ys\n1:T) ‚àí(ÀÜy1:T)) ‚ñΩŒ∏ log pŒ∏(ys\n1:T) (14)\n4 Experiment\n4.1 Datasets and Evaluation Metrics\nOur model is trained on the MSCOCO image captioning dataset [17]. We follow\nKarpathy‚Äôs splits [32], with 11,3287 images in the training set, 5,000 images in\n10 Sen He et al.\nmodel Bleu1 Bleu4 METEOR ROUGE-L CIDEr SPICE\nsingle-stage model\nAtt2all[29] - 34.2 26.7 55.7 114.0 -\ntwo-stages model\nn-babytalk[4] 75.5 34.7 27.1 - 107.2 20.1\nup-down[3] 79.8 36.3 27.7 56.9 120.1 21.4\nscene graph based model\nGCN-LSTM‚àó[7] 80.9 38.3 28.6 58.5 128.7 22.1\nAUTO-ENC[8] 80.8 38.4 28.4 58.6 127.8 22.1\nALV‚àó[9] - 38.4 28.5 58.4 128.6 22.0\nGCN-LSTM-HIP‚àó‚Ä†[10] - 39.1 28.9 59.2 130.6 22.3\ntransformer based model\nEntangle-T‚àó[13] 81.5 39.9 28.9 59.0 127.6 22.6\nAoA[12] 80.2 38.9 29.2 58.8 129.8 22.4\nVORN[14] 80.5 38.6 28.7 58.4 128.3 22.6\nOurs 80.8 39.5 29.1 59.0 130.8 22.8\nTable 1: Comparison on MSCOCO Karpathy oÔ¨Ñine test split. ‚àómeans fusion of\ntwo models. ‚Ä†means SENet [31] as feature extraction backbone\nthe validation set and 5,000 images in the test set. Each image has 5 captions\nas ground truth. We discard the words which occur less than 4 times, and the\nÔ¨Ånal vocabulary size is 10,369. We test our model on both Karpathy‚Äôs oÔ¨Ñine\ntest set (5,000 images) and MSCOCO online testing datasets (40,775 images).\nWe use Bleu [33], METEOR [34], ROUGE-L [35], CIDEr [30], and SPICE [23]\nas evaluation metrics.\n4.2 Implementation Details\nFollowing previous work, we Ô¨Årst train Faster R-CNN on Visual Genome [21],\nuse resnet-101 [36] as backbone, pretrained on ImageNet [16]. For each image,\nwe can detect 10 ‚àí100 informative regions, the boundaries of each are Ô¨Årst nor-\nmalised and then used to compute the spatial graph matrices. We then train\nour proposed model for image captioning using the computed spatial graph ma-\ntrices and extracted features for each image region. We Ô¨Årst train our model\nwith cross-entropy loss for 25 epochs, the initial learning rate is set to 2 √ó10‚àí3,\nand we decay the learning rate by 0 .8 every 3 epochs. Our model is optimized\nthrough Adam [37] with a batch size of 10. We then further optimize our model\nby reinforced learning for another 35 epochs. The size of the decoder‚Äôs LSTM\nlayer is set to 1024, and beam search of size 3 is used in the inference stage.\n4.3 Experiment Results\nWe compare our model‚Äôs performance with published image captioning models.\nThe compared models include the top performing single-stage attention model,\nAtt2all [29]; two-stages attention based models, n-babytalk [4] and up-down\n[3]; visual scene graph based models, GCN-LSTM [7], AUTO-ENC [8], ALV\n[9], GCN-LSTM-HIP [10]; and transformer based models Entangle-T [13], AoA\nImage Captioning through Image Transformer 11\nmodel B1 B4 M R C\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nscene graph based model\nGCN-LSTM‚àó[7] 80.8 95.9 38.7 69.7 28.5 37.6 58.5 73.4 125.3 126.5\nAUTO-ENC‚àó[8] - - 38.5 69.7 28.2 37.2 58.6 73.6 123.8 126.5\nALV‚àó[9] 79.9 94.7 37.4 68.3 28.2 37.1 57.9 72.8 123.1 125.5\nGCN-LSTM-HIP‚àó‚Ä†[10] 81.6 95.9 39.3 71.0 28.8 38.1 59.0 74.1 127.9 130.2\ntransformer based model\nEntangle-T‚àó[13] 81.2 95.0 38.9 70.2 28.6 38.0 58.6 73.9 122.1 124.4\nAoA[12] 81.0 95.0 39.4 71.2 29.1 38.5 58.9 74.5 126.9 129.6\nOurs 81.2 95.4 39.6 71.5 29.1 38.4 59.2 74.5 127.4 129.6\nTable 2: Leaderboard of recent published models on the MSCOCO online testing\nserver. ‚àó means fusion of two models. ‚Ä† means SENet [31] as feature extraction\nbackbone\n[12], VORN [14]. The comparison on the MSCOCO Karpathy oÔ¨Ñine test set is\nillustrated in Table 1. Our model achieves new state-of-the-art on the CIDEr\nand SPICE score, while other evaluation scores are comparable to the previous\ntop performing models. Note that because most visual scene graph based models\nfused semantic and spatial scene graph, and require the auxiliary models to build\nthe scene graph at Ô¨Årst, our model is more computationally eÔ¨Écient. VORN [14]\nalso integrated spatial attention in their model, and our model performs better\nthan them among all kinds of evaluation metrics, which shows the superiority\nof our spatial graph transformer layer. The MSCOCO online testing results are\nlisted in Tab. 2, our model outperforms previous transformer based model on\nseveral evaluation metrics.\n4.4 Ablation Study and Analysis\nIn the ablation study, we use AoA [12] as a strong baseline6 (with a single multi-\nhead dot-product attention module per layer), which add the gated linear layer\n[27] on top of the multi-head attention. In the encoder part, we study the spatial\nrelationship‚Äôs eÔ¨Äect in the encoder, where we ablate the spatial relationship\nby simply taking the mean output of three sub-transformers in each layer by\nreformulating Eqs. 6 and 7 as: Attention(Q,Ki,Vi) = Softmax\n(\nQKT\ni‚àö\nd\n)\nVi,Am =\nNorm\n(\nA+ 1\n3\n‚àë\ni‚àà{p,n,c}MultiHead(Q,Ki,Vi)\n)\n. We also study where to use our\nproposed spatial graph encoding transformer layer in the encoding part: in the\nÔ¨Årst layer, second layer, third layer or three of them? In the decoding part, we\nstudy the eÔ¨Äect of the number of sub-transformers ( M in Eq. 10) in the implicit\ndecoding transformer layer.\nAs we can see from Tab. 3, by widening the encoding transformer layer, there\nis a signiÔ¨Åcant improvement on the model‚Äôs performance. While not every layers\nin the encoding transformer are equal, when we use our proposed transformer\nlayer at the top layer of the encoding part, the improvement was reduced. This\n6 Our experiments are based on the code released at:https://github.com/husthuaan/AoANet\n12 Sen He et al.\nmodel Bleu1 Bleu4 METEOR ROUGE-L CIDEr SPICE\nbaseline(AoA) 77.0 36.5 28.1 57.1 116.6 21.3\npositions to embed our spatial graph encoding transformer layer\nbaseline+layer1 77.8 36.8 28.3 57.3 118.1 21.3\nbaseline+layer2 77.2 36.8 28.3 57.3 118.2 21.3\nbaseline+layer3 77.0 37.0 28.2 57.1 117.3 21.2\nbaseline+layer1,2,3 77.5 37.0 28.3 57.2 118.2 21.4\neÔ¨Äect of spatial relationships in the encoder\nbaseline+layer1,2,3 w/o spatial rela 77.5 36.8 28.2 57.1 117.8 21.4\nnumber of sub-transformers in the implicit decoding transformer layer\nbaseline+layer1,2,3 (M=2) 77.5 37.6 28.4 57.4 118.8 21.3\nbaseline+layer1,2,3 (M=3) 78.0 37.4 28.4 57.6 119.1 21.6\nbaseline+layer1,2,3 (M=4) 77.5 37.8 28.4 57.5 118.6 21.4\nTable 3: Ablation study, results reported without RL training. baseline+layer1\nmeans only the Ô¨Årst layer of encoding transformer uses our proposed spatial\ntransformer layer, other layers use the original one. M is the number of sub-\ntransformers in the decoding transformer layer.\nmay be because spatial relationships at the top layer of the transformer are\nnot as informative, we use our spatial transformer layer at all layers in the\nencoding part. When we reduce the spatial relationship in our proposed wider\ntransformer layer, there is also some performance reduction, which shows the\nimportance of the spatial relationship in our design. After widening the decoding\ntransformer, the improvement was further increased (the CIDEr score increased\nfrom 118.2 to 119.1 after widening the decoding transformer layer with 3 sub-\ntransformers), while not more wider gives better result, with 4 sub-transformers\nin the decoding transformer layer, there is some performance decrease, therefore\nthe Ô¨Ånal design of our decoding transformer layer has 3 sub-transformers in\nparallel. The qualitative example of our models results is illustrated in Fig. 5.\nAs we can see, the baseline model without spatial relationships wrongly described\nthe police oÔ¨Écers on a red bus (top right), and people on a train (bottom left).\nEncoding implicit graph visualisation:the transformer layer can be seen as an\nimplicit graph, which relates the informative regions through dot-product atten-\ntion. Here we visualise how our proposed spatial graph transformer layer learn\nto connect the informative regions through attention in Fig. 6. In the top ex-\nample, the original transformer layer strongly relates the train with the people\non the mountain, yields wrong description, while our proposed transformer layer\nrelates the train with the tracks and mountain; in the bottom example, the orig-\ninal transformer relates the bear with its reÔ¨Çection in water and treats them as\n‚Äòtwo bears‚Äô, while our transformer can distinguish the bear from its reÔ¨Çection\nand relate it to the snow area.\nDecoding feature space visualisation:We also visualised the output of our decod-\ning transformer layer (Fig. 7). Compared to the original decoding transformer\nlayer, which only has one sub-transformer inside it. The output of our proposed\nimplicit decoding transformer layer covers a larger area in the reduced feature\nImage Captioning through Image Transformer 13\nGT:\n‚Ä¢ A traffic light over a street surrounded \nby tall buildings.\n‚Ä¢ A black and white shot of a city with a \ntall skyscraper in the .\n‚Ä¢ Some buildings a traffic light and a \ncloudy sky.\n‚Ä¢ A black and white photograph of a \nstop light from the street.\n‚Ä¢ A traffic light and street sign \nsurrounded by buildings.\nGT:\n‚Ä¢ A man is holding a cell phone in \nfront of a mountain.\n‚Ä¢ An older man standing on top of a \nsnow covered slops.\n‚Ä¢ A man looking at a vast mountain \nlandscape.\n‚Ä¢ A man takes a picture of snowy \nmountains with his cell phone.\nBaseline: A couple of traffic lights on a city street.\nOurs: A  traffic light on a street with a building. \nBaseline: A man is taking a picture of a mountain range.\nOurs: A man taking a picture of a mountain with a cell phone. \nGT:\n‚Ä¢ A group of police officers standing \nin front of a red bus.\n‚Ä¢ Three bikers by a red bus in the \nstreet.\n‚Ä¢ A big red bus by some people on \nmotorcycles.\n‚Ä¢ Some men on bikes are passing a \nred bus.\n‚Ä¢ Parking officials are riding beside \na red bus.\nBaseline: A group of police officers on a red bus.\nOurs: A group of police officers on motorcycles in front of a red bus. \nGT:\n‚Ä¢ An image of a train that is going \ndown the tracks.\n‚Ä¢ Some people are standing on rocks \nwith a railroad.\n‚Ä¢ A train moving along a track on a \nhill during the day.\n‚Ä¢ A single train car passing tracks on \na hill.\nBaseline: A group of people on a train on the tracks.\nOurs: A  train is traveling down the tracks on a mountain. \nFig. 5: Qualitative examples from our method on the MSCOCO image captioning\ndataset [17], compared against the ground truth annotation and a strong baseline\nmethod (AoA [12]).\nspace than the original one, which means that our decoding transformer layer\ndecoding more information in the image regions. In the original feature space\n(1,024 dimensions) from the output of decoding transformer layer, we compute\nthe trace of the feature maps‚Äô co-variance matrix from 1,000 examples, the trace\nfor original transformer layer is 30.40 compared to 454.57 for our wider decoding\ntransformer layer, which indicates that our design enables the decoder‚Äôs output\nto cover a larger area in the feature space. However, it looks like individual sub-\ntransformers in the decoding transformer layer still do not learn to disentangle\ndiÔ¨Äerent factors in the feature space (as there is no distinct cluster from the\noutput of each sub-transformer), we speculate this is because we have no direct\nsupervision to their output, which may not able to learn the disentangled feature\nautomatically [38].\n5 Discussion and Conclusion\nIn this work, we introduced the image transformer architecture. The core\nidea behind the proposed architecture is to widen the original transformer layer,\ndesigned for machine translation, to adapt it to the structure of images. In the\nencoder, we widen the transformer layer by exploiting the spatial relationships\nbetween image regions, and in the decoder, the wider transformer layer can\ndecode more information in the image regions. Extensive experiments were done\nto show the superiority of the proposed model, the qualitative and quantitative\nanalyses were illustrated in the experiments to validate the proposed encoding\nand decoding transformer layer. Compared to the previous top models in image\ncaptioning, our model achieves a new state-of-the-art SPICE score, while in the\n14 Sen He et al.\nA train is traveling down the tracks on a mountain\nBaseline Ours\nA group of people on the train on the tracks\nA polar bear walking in the snowTwo polar bears are playing in the water\nFig. 6: A visualization of how the query region relates to its other key regions\nthrough attention, the region in the red bounding box is the query region and\nother regions are key regions. The transparency of each key region shows its\ndot-product attention weight with the query region. Higher transparency means\nlarger dot-product attention weight, vice versa.\n(a) original\n (b) ours\nFig. 7: t-SNE [39] visualisation of the output from decoding transformer layer\n(1,000 examples), diÔ¨Äerent color represent the output from diÔ¨Äerent sub-\ntransformers in the decoder in our model.\nother evaluation metrics, our model is either comparable or outperforms the\nprevious best models, with a better computational eÔ¨Éciency.\nWe hope our work can inspire the community to develop more advanced\ntransformer based architectures that can not only beneÔ¨Åt image captioning but\nalso other computer vision tasks which need relational attention inside it. Our\ncode will be shared with the community to support future research.\nImage Captioning through Image Transformer 15\nReferences\n1. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R.,\nBengio, Y.: Show, attend and tell: Neural image caption generation with visual\nattention. In: International conference on machine learning. (2015) 2048‚Äì2057\n2. Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua, T.S.: Sca-cnn:\nSpatial and channel-wise attention in convolutional networks for image captioning.\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition.\n(2017) 5659‚Äì5667\n3. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.:\nBottom-up and top-down attention for image captioning and visual question an-\nswering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. (2018) 6077‚Äì6086\n4. Lu, J., Yang, J., Batra, D., Parikh, D.: Neural baby talk. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition. (2018) 7219‚Äì7228\n5. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detec-\ntion with region proposal networks. In: Advances in neural information processing\nsystems. (2015) 91‚Äì99\n6. Kipf, T.N., Welling, M.: Semi-supervised classiÔ¨Åcation with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907 (2016)\n7. Yao, T., Pan, Y., Li, Y., Mei, T.: Exploring visual relationship for image captioning.\nIn: Proceedings of the European Conference on Computer Vision (ECCV). (2018)\n684‚Äì699\n8. Yang, X., Tang, K., Zhang, H., Cai, J.: Auto-encoding scene graphs for image\ncaptioning. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. (2019) 10685‚Äì10694\n9. Guo, L., Liu, J., Tang, J., Li, J., Luo, W., Lu, H.: Aligning linguistic words and\nvisual semantic units for image captioning. In: Proceedings of the 27th ACM\nInternational Conference on Multimedia. (2019) 765‚Äì773\n10. Yao, T., Pan, Y., Li, Y., Mei, T.: Hierarchy parsing for image captioning. In:\nProceedings of the IEEE International Conference on Computer Vision. (2019)\n2621‚Äì2629\n11. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems. (2017) 5998‚Äì6008\n12. Huang, L., Wang, W., Chen, J., Wei, X.Y.: Attention on attention for image\ncaptioning. In: Proceedings of the IEEE International Conference on Computer\nVision. (2019) 4634‚Äì4643\n13. Li, G., Zhu, L., Liu, P., Yang, Y.: Entangled transformer for image captioning.\nIn: Proceedings of the IEEE International Conference on Computer Vision. (2019)\n8928‚Äì8937\n14. Herdade, S., Kappeler, A., Boakye, K., Soares, J.: Image captioning: Transforming\nobjects into words. In: Advances in Neural Information Processing Systems. (2019)\n11135‚Äì11145\n15. Luo, W., Li, Y., Urtasun, R., Zemel, R.: Understanding the eÔ¨Äective receptive\nÔ¨Åeld in deep convolutional neural networks. In: Advances in neural information\nprocessing systems. (2016) 4898‚Äì4906\n16. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-\nscale hierarchical image database. In: 2009 IEEE conference on computer vision\nand pattern recognition, Ieee (2009) 248‚Äì255\n16 Sen He et al.\n17. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll¬¥ ar, P., Zitnick, C.L.:\nMicrosoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325 (2015)\n18. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image\ncaption generator. In: Proceedings of the IEEE conference on computer vision and\npattern recognition. (2015) 3156‚Äì3164\n19. Gers, F.A., Schmidhuber, J., Cummins, F.: Learning to forget: Continual predic-\ntion with lstm. Neural Computation 12 (2000) 2451‚Äì2471\n20. Lu, J., Xiong, C., Parikh, D., Socher, R.: Knowing when to look: Adaptive attention\nvia a visual sentinel for image captioning. In: Proceedings of the IEEE conference\non computer vision and pattern recognition. (2017) 375‚Äì383\n21. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,\nKalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language\nand vision using crowdsourced dense image annotations. International Journal of\nComputer Vision 123 (2017) 32‚Äì73\n22. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll¬¥ ar, P.,\nZitnick, C.L.: Microsoft coco: Common objects in context. In: European conference\non computer vision, Springer (2014) 740‚Äì755\n23. Anderson, P., Fernando, B., Johnson, M., Gould, S.: Spice: Semantic propositional\nimage caption evaluation. In: European Conference on Computer Vision, Springer\n(2016) 382‚Äì398\n24. Hao, J., Wang, X., Shi, S., Zhang, J., Tu, Z.: Multi-granularity self-attention for\nneural machine translation. arXiv preprint arXiv:1909.02222 (2019)\n25. Wang, X., Tu, Z., Wang, L., Shi, S.: Self-attention with structural position repre-\nsentations. arXiv preprint arXiv:1909.00383 (2019)\n26. Wang, Y.S., Lee, H.Y., Chen, Y.N.: Tree transformer: Integrating tree structures\ninto self-attention. arXiv preprint arXiv:1909.06639 (2019)\n27. Dauphin, Y.N., Fan, A., Auli, M., Grangier, D.: Language modeling with gated\nconvolutional networks. In: Proceedings of the 34th International Conference on\nMachine Learning-Volume 70, JMLR (2017) 933‚Äì941\n28. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9\n(1997) 1735‚Äì1780\n29. Rennie, S.J., Marcheret, E., Mroueh, Y., Ross, J., Goel, V.: Self-critical sequence\ntraining for image captioning. In: Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition. (2017) 7008‚Äì7024\n30. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based image\ndescription evaluation. In: Proceedings of the IEEE conference on computer vision\nand pattern recognition. (2015) 4566‚Äì4575\n31. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the\nIEEE conference on computer vision and pattern recognition. (2018) 7132‚Äì7141\n32. Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image\ndescriptions. In: Proceedings of the IEEE conference on computer vision and\npattern recognition. (2015) 3128‚Äì3137\n33. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic\nevaluation of machine translation. In: Proceedings of the 40th annual meeting on\nassociation for computational linguistics, Association for Computational Linguis-\ntics (2002) 311‚Äì318\n34. Banerjee, S., Lavie, A.: Meteor: An automatic metric for mt evaluation with im-\nproved correlation with human judgments. In: Proceedings of the acl workshop on\nintrinsic and extrinsic evaluation measures for machine translation and/or summa-\nrization. (2005) 65‚Äì72\nImage Captioning through Image Transformer 17\n35. Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: Proc.\nACL workshop on Text Summarization Branches Out. (2004) 10\n36. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition.\n(2016) 770‚Äì778\n37. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n38. Locatello, F., Bauer, S., Lucic, M., R¬® atsch, G., Gelly, S., Sch¬® olkopf, B., Bachem,\nO.: Challenging common assumptions in the unsupervised learning of disentangled\nrepresentations. In: Proceedings of the 36th International Conference on Machine\nLearning-Volume 97, JMLR (2019) 4114‚Äì4124\n39. Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne. Journal of machine\nlearning research 9 (2008) 2579‚Äì2605",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9389292001724243
    },
    {
      "name": "Transformer",
      "score": 0.7727041244506836
    },
    {
      "name": "Computer science",
      "score": 0.7421954870223999
    },
    {
      "name": "Architecture",
      "score": 0.5021247863769531
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4775862693786621
    },
    {
      "name": "Decoding methods",
      "score": 0.434650182723999
    },
    {
      "name": "Image (mathematics)",
      "score": 0.396605521440506
    },
    {
      "name": "Computer vision",
      "score": 0.3501659333705902
    },
    {
      "name": "Speech recognition",
      "score": 0.34650349617004395
    },
    {
      "name": "Natural language processing",
      "score": 0.3259524703025818
    },
    {
      "name": "Algorithm",
      "score": 0.14970886707305908
    },
    {
      "name": "Voltage",
      "score": 0.12265715003013611
    },
    {
      "name": "Electrical engineering",
      "score": 0.11224114894866943
    },
    {
      "name": "Engineering",
      "score": 0.09595933556556702
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}