{
    "title": "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
    "url": "https://openalex.org/W3034510440",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2161984462",
            "name": "Jennifer Hu",
            "affiliations": [
                "Institute of Cognitive and Brain Sciences",
                "Massachusetts Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2530850308",
            "name": "Jon Gauthier",
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Institute of Cognitive and Brain Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A1975658276",
            "name": "Peng Qian",
            "affiliations": [
                "Institute of Cognitive and Brain Sciences",
                "Massachusetts Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2201425052",
            "name": "Ethan Wilcox",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A1982032445",
            "name": "Roger Lévy",
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Institute of Cognitive and Brain Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2921890305",
        "https://openalex.org/W2130924704",
        "https://openalex.org/W2043146406",
        "https://openalex.org/W3093963706",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4288631803",
        "https://openalex.org/W2061311021",
        "https://openalex.org/W2962961857",
        "https://openalex.org/W3044103552",
        "https://openalex.org/W2054518132",
        "https://openalex.org/W2994665957",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2118276816",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4256303026",
        "https://openalex.org/W2953320089",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W1497614198",
        "https://openalex.org/W2864832950",
        "https://openalex.org/W2963073938",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2139996985",
        "https://openalex.org/W2038536973",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2963751529",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2054125330",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W2888922637",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2994726368",
        "https://openalex.org/W108437174",
        "https://openalex.org/W2506931122",
        "https://openalex.org/W2002654918",
        "https://openalex.org/W4289373464",
        "https://openalex.org/W1522920269",
        "https://openalex.org/W2990018842",
        "https://openalex.org/W2964222268",
        "https://openalex.org/W2151073408",
        "https://openalex.org/W2795342569",
        "https://openalex.org/W61686895",
        "https://openalex.org/W2891399254",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2034407119",
        "https://openalex.org/W2143017621",
        "https://openalex.org/W2972896975",
        "https://openalex.org/W2963411763",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2942054564",
        "https://openalex.org/W1592072150",
        "https://openalex.org/W1595256356",
        "https://openalex.org/W2605717780",
        "https://openalex.org/W2108010971",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W4289552613",
        "https://openalex.org/W2963084773",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W3039224521",
        "https://openalex.org/W1585040575",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2971016963",
        "https://openalex.org/W2962733492",
        "https://openalex.org/W2531882892",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2910243263",
        "https://openalex.org/W2918996109"
    ],
    "abstract": "While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1725–1744\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n1725\nA Systematic Assessment of Syntactic Generalization\nin Neural Language Models\nJennifer Hu1, Jon Gauthier1, Peng Qian1, Ethan Wilcox2, and Roger P. Levy1\n1Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology\n2Department of Linguistics, Harvard University\n{jennhu,pqian,rplevy}@mit.edu\njon@gauthiers.net, wilcoxeg@g.harvard.edu\nAbstract\nWhile state-of-the-art neural network models\ncontinue to achieve lower perplexity scores on\nlanguage modeling benchmarks, it remains un-\nknown whether optimizing for broad-coverage\npredictive performance leads to human-like\nsyntactic knowledge. Furthermore, existing\nwork has not provided a clear picture about the\nmodel properties required to produce proper\nsyntactic generalizations. We present a sys-\ntematic evaluation of the syntactic knowledge\nof neural language models, testing 20 com-\nbinations of model types and data sizes on\na set of 34 English-language syntactic test\nsuites. We ﬁnd substantial differences in syn-\ntactic generalization performance by model ar-\nchitecture, with sequential models underper-\nforming other architectures. Factorially manip-\nulating model architecture and training dataset\nsize (1M–40M words), we ﬁnd that variabil-\nity in syntactic generalization performance is\nsubstantially greater by architecture than by\ndataset size for the corpora tested in our ex-\nperiments. Our results also reveal a dissocia-\ntion between perplexity and syntactic general-\nization performance.\n1 Introduction\nA growing body of work advocates that assess-\nment of neural language models should include\nboth information-theoretic metrics, such as per-\nplexity, as well as targeted linguistic evaluation.\nBenchmarks such as GLUE (Wang et al., 2019a,b)\nhave demonstrated that neural language models\ntrained on naturalistic corpora for next-word predic-\ntion learn representations that can yield remarkable\nperformance on many semantic tasks. Targeted\nsyntactic evaluations have shown that these mod-\nels also implicitly capture many syntactic gener-\nalizations, ranging from subject–verb agreement\nMaterials and code can be found at https://github.\ncom/cpllab/syntactic-generalization.\nto long-distance ﬁller–gap dependencies (Linzen\net al., 2016; Marvin and Linzen, 2018; Futrell et al.,\n2018; Wilcox et al., 2019b). This paper aims to\nbring targeted evaluations of syntactic performance\nto scale, complementing similar developments in\nsemantic evaluation (McCoy et al., 2019).\nBecause the most widespread currency of evalu-\nation for language models is perplexity—how well,\non average, a model predicts a word in its context—\na primary focus of this paper is the relationship\nbetween a model’s perplexity and its performance\non targeted syntactic evaluations. As perplexity im-\nproves, can we expect more human-like syntactic\ngeneralization? How do training dataset size and\nmodel architecture jointly affect syntactic gener-\nalization? And what picture of models’ syntactic\ngeneralization emerges when evaluation is brought\nto scale, across dozens of controlled syntactic tests?\nIn this paper we offer initial answers to these\nquestions, systematically assessing the syntactic\ngeneralization abilities of neural language models\non 34 targeted test suites (33 adapted from pre-\nviously published work, and 1 novel) covering a\nwide range of syntactic phenomena. Test suites\nare written using a standard format that allows for\nﬂexible predictions which more closely resemble\nthose used in psycholinguistic studies, speciﬁcally\nallowing for predictions about interactions among\nmultiple testing conditions. Performance on each\ntest suite is reported as a Syntactic Generalization\n(SG) score. We group test suites into six syntac-\ntic circuits based on the linguistic representations\nneeded to achieve high performance on each suite.\nWe train four classes of neural models and one\nbaseline n-gram model on four datasets derived\nfrom a newswire corpus, consisting of 1, 5, 14,\nand 42 million tokens. While previous work has\ncompared model architectures for a ﬁxed dataset\nsize (e.g. Wilcox et al., 2019b) and network sizes\nfor a ﬁxed architecture (e.g. van Schijndel et al.,\n1726\n2019), our controlled regime allows us to make an\napples-to-apples comparison across model architec-\ntures on a range of sizes. In addition, we evaluate\nseveral off-the-shelf models which were trained on\ndatasets ranging up to 2 billion tokens.\nOur results address the three questions posed\nabove: First, for the range of model architectures\nand dataset sizes tested, we ﬁnd a substantial disso-\nciation between perplexity and SG score. Second,\nwe ﬁnd a larger effect of model inductive bias than\ntraining data size on SG score, a result that accords\nwith van Schijndel et al. (2019). Models afforded\nexplicit structural supervision during training out-\nperform other models: One structurally supervised\nmodel is able to achieve the same SG scores as\na purely sequence-based model trained on ∼100\ntimes the number of tokens. Furthermore, several\nTransformer models achieve the same SG score as\na Transformer trained on ∼200 times the amount\nof data. Third, we ﬁnd that architectures have dif-\nferent relative advantages across types of syntactic\ntests, suggesting that the tested syntactic phenom-\nena tap into different underlying processing capaci-\nties in the models.\n2 Background\n2.1 Perplexity\nStandard language models are trained to predict\nthe next token given a context of previous tokens.\nLanguage models are typically assessed by their\nperplexity, the inverse geometric mean of the joint\nprobability of words w1,...,w N in a held-out test\ncorpus C:\nPPL(C) =p(w1,w2,...w N)− 1\nN (1)\nModels with improved perplexity have also been\nshown to better match various human behavioral\nmeasures, such as gaze duration during reading\n(Frank and Bod, 2011; Fossum and Levy, 2012;\nGoodkind and Bicknell, 2018; Wilcox et al., 2020).\nHowever, a broad-coverage metric such as per-\nplexity may not be ideal for assessing human-like\nsyntactic knowledge for a variety of reasons. In\nprinciple, a sentence can appear with vanishingly\nlow probability but still be grammatically well-\nformed, such as Colorless green ideas sleep fu-\nriously (Chomsky, 1957). While perplexity re-\nmains an integral part of language model evalua-\ntion, ﬁne-grained linguistic assessment can provide\nboth more challenging and more interpretable tests\nto evaluate neural models.\n2.2 Targeted tests for syntactic generalization\nAlternatively, a language model can be evaluated\non its ability to make human-like generalizations\nfor speciﬁc syntactic phenomena (Linzen et al.,\n2016; Lau et al., 2017; Gulordava et al., 2018).\nThe targeted syntactic evaluation paradigm (Mar-\nvin and Linzen, 2018; Futrell et al., 2019) incorpo-\nrates methods from psycholinguistic experiments,\ndesigning sentences which hold most lexical and\nsyntactic features of each sentence constant while\nminimally varying features that determine gram-\nmaticality or surprise characteristics of the sen-\ntence. For example, given the two strings The keys\nto the cabinet are on the table and *The keys to the\ncabinet is on the table, a model that has learned the\nproper subject–verb number agreement rules for\nEnglish should assign a higher probability to the\ngrammatical plural verb in the ﬁrst sentence than\nto the ungrammatical singular verb in the second\n(Linzen et al., 2016).\nAlthough some targeted syntactic evaluations,\nsuch as the example discussed above, involve sim-\nple comparisons of conditional probabilities of a\nword in its context, other evaluations are more\ncomplex. We can demonstrate this with an evalua-\ntion of models’ “garden-pathing” behavior (Futrell\net al., 2019). For example, the sentence The child\nkicked in the chaos found her way back home\nyields processing disruption for humans at the word\nfound. This is because, up to right before that word,\nthe part-of-speech ambiguous kicked is preferen-\ntially interpreted as the main verb of the sentence,\nwhereas it turns out to be a passive participle in\na reduced relative clause modifying child. This\ngarden-path disambiguation effect is ameliorated\nby replacing kicked with forgotten, which is not\npart-of-speech ambiguous (B below; Trueswell\net al., 1994) or by using an unreduced relative\nclause (C below; Ferreira and Clifton, 1986). In\nprobabilistic language models, these garden-path\ndisambiguation effects are well captured by word\nnegative log probabilities, or SURPRISALS (Hale,\n2001): S(w|C) =−log2 p(w|C), which are inde-\npendently well-established to predict human incre-\nmental processing difﬁculty over several orders of\nmagnitude in word probability (Smith and Levy,\n2013). A targeted syntactic evaluation for garden-\npathing is provided by comparing surprisals at the\ndisambiguating word found in the set of four exam-\nples below (Futrell et al., 2019):\n(A) The child kicked in the chaos found . . .\n1727\n(B) The child forgotten in the chaos found . . .\n(C) The child who was kicked in the chaos found . . .\n(D) The child who was forgotten in the chaos found . . .\nSuccessful human-like generalization involves\nthree criteria: (i) found should be less surprising\n(i.e., more probable) in B than A; (ii) found should\nbe more probable in C than A; (iii) the C–D sur-\nprisal difference should be smaller than the A–B\nsurprisal difference—a 2 ×2 interaction effect on\nsurprisal—because the syntactic disambiguation ef-\nfect of not reducing the relative clause was achieved\nby using a part-of-speech unambiguous verb.\nWe will use these controlled tests to help us de-\nscribe and test for human-like syntactic knowledge\nin language models.\n2.3 Related work\nThe testing paradigm presented here differs in sev-\neral crucial ways from recent, related syntactic as-\nsessments and provides complementary insights.\nUnlike Warstadt et al. (2019a), our approach does\nnot involve ﬁne-tuning, but rather assesses what\nsyntactic knowledge is induced from the language\nmodeling objective alone. The most closely related\nwork is the Benchmark of Linguistic Minimal Pairs\n(Warstadt et al., 2020), which is a challenge set\nof automatically-generated sentence pairs also de-\nsigned to test language models on a large set of\nsyntactic phenomena. Our approach differs in im-\nportant ways: we compare critical sentence regions\ninstead of full-sentence probabilities, and employ a\n2 ×2 paradigm with a strict, multi-fold success cri-\nterion inspired by psycholinguistics methodology.\nThis allows us to factor out as many confounds as\npossible, such as the lexical frequency of individual\ntokens and low-level n-gram statistics.\n3 Methods\nWe designed a controlled paradigm for systemati-\ncally testing the relationship between two design\nchoices — model class and dataset size — and two\nperformance metrics — perplexity and syntactic\ngeneralization capacity. Section 3.1 describes the\ntest suites collected for our evaluation, and Sec-\ntions 3.2 and 3.3 describe the datasets and model\nclasses investigated.\n3.1 Test suites\nWe assemble a large number of test suites inspired\nby the methodology of experimental sentence-\nprocessing and psycholinguistic research. Each\ntest suite contains a number of ITEMS (typically be-\ntween 20 and 30), and each item appears in several\nCONDITIONS : across conditions, a given item will\ndiffer only according to a controlled manipulation\ndesigned to target a particular feature of grammati-\ncal knowledge. Each test suite contains at least one\nPREDICTION , which speciﬁes inequalities between\nsurprisal values at pairs of regions/conditions that\nshould hold if a model has learned the appropriate\nsyntactic generalization.\nWe expect language models which have learned\nthe appropriate syntactic generalizations from their\ninput to satisfy these inequalities without further\nﬁne-tuning. We compute accuracy on a test suite as\nthe proportion of items for which the model’s be-\nhavior conforms to the prediction. Most of our test\nsuites involve 2×2 designs and a success criterion\nconsisting of a conjunction of inequalities across\nconditions, as in the garden-pathing example de-\nscribed in Section 2.2.1 Random baseline accuracy\nvaries by test suite and is ∼25% overall. Most of\nthese test suites and criteria are designed so that\nn-gram models cannot perform above chance for\nn= 5(sometimes greater).\nSyntactic coverage In order to assess the cover-\nage of our test suites, we manually inspected the\nphenomena covered in Carnie (2012), a standard\nintroductory syntax textbook. Of the 47 empirical\nphenomena reviewed in the summary sections at\nthe end of each chapter, our tests target 16 (∼34%).\nThese are evenly distributed across the whole range\nof subject matter, with tests targeting phenomena\nin 11 of the 15 chapters (∼73%).2\nModiﬁers Five test suites include paired modiﬁer\nversions, where extra syntactically irrelevant (but\nsemantically plausible) content, such as a preposi-\ntional phrase or relative clause, is inserted before\nthe critical region being measured. We use these\npaired test suites to evaluate models’ stability to in-\ntervening content within individual syntactic tests.\nCircuits The test suites are divided into 6 syntac-\ntic circuits, based on the type of algorithm required\nto successfully process each construction. We give\na brief overview of each circuit below.3\n•Agreement is a constraint on the feature val-\nues of two co-varying tokens. For example,\n1The exception is Center Embedding, which features a 2-\ncondition design with a single-inequality criterion.\n2For more details on this analysis, see Appendix A.\n3A full overview of our test suites is given in Appendix B.\n1728\nthe number feature of a verb must agree with\nthe number feature of its upstream subject.\nWe include 3Subject-Verb Number Agreement\nsuites from Marvin and Linzen (2018).\n•Licensing occurs when a particular token\nmust exist within the scope of an upstream\nlicensor token. Scope is determined by the\ntree-structural properties of the sentence. Test\nsuites include Negative Polarity Item Licens-\ning (NPI) (4 suites) and Reﬂexive Pronoun\nLicensing (6 suites), both from Marvin and\nLinzen (2018).\n•Garden-Path Effects are well-studied syn-\ntactic phenomena that result from tree-\nstructural ambiguities that give rise to locally-\ncoherent but globally implausible syntactic\nparses. Garden-path test suites include Main\nVerb / Reduced Relative Clause (MVRR) (2\nsuites) and NP/Z Garden-paths (NPZ) (4\nsuites), both from Futrell et al. (2018).\n•Gross Syntactic Expectationis a processor’s\nexpectation for large syntactic chunks such as\nverb phrases or sentences, and are often set up\nby subordinating conjunctions such as while,\nalthough and despite. Our tests for gross syn-\ntactic expectation include Subordination (4\nsuites) from Futrell et al. (2018).\n•Center Embedding sentences are sentences\nrecursively nested within each other. Subject\nand verbs must match in a ﬁrst-in-last-out\norder, meaning models must approximate a\nstack-like data-structure in order to success-\nfully process them. Our 2 suites of Center\nEmbedding sentences come from the items\npresented in Wilcox et al. (2019a).\n•Long-Distance Dependencies are co-\nvariations between two tokens that span long\ndistances in tree depth. Test suites include\nFiller-Gap Dependencies (FGD) (6 suites)\nfrom Wilcox et al. (2018) and Wilcox et al.\n(2019b), and 2 novel Cleft suites, described in\ndetail below.\nNovel test suite: Cleft We introduce one novel\ntest suite that assesses models’ ability to process\npseudo-cleft constructions, which are used to put a\nparticular syntactic constituent into focus via pas-\nsive transformation. Consider Example (1):\nBLLIP sizes: XS SM MD LG\n# sentences 40K 200K 600K 1.8M\n# tokens 1M 4.8M 14M 42M\n# non-UNK types 24K 57K 100K 170K\n# UNK types 68 70 71 74\nTable 1: Statistics of training set for each corpus size.\n(1) a. What he did after coming in from the rain\nwas eat a hot meal. [DO/VP]\nb.*What he devoured after coming in from the\nrain was eat a hot meal. [LEX/VP]\nc.*What he did after coming in from the rain\nwas a hot meal. [DO/NP]\nd. What he devoured after coming in from the\nrain was a hot meal. [LEX/NP]\nWhen this constituent is a verb, it must be replaced\nin the wh-clause that heads the sentence with the\nDO verb, as in (1a), below. However, when it is\na noun, the lexical verb for which it serves as an\nobject must be preserved, as in(1d). If models have\nproperly learned the pseudo-cleft construction, then\nDO verbs should set up expectations for VPs (the\nregion in bold should have a lower surprisal in (1a)\nthan in (1b)) and lexicalized verbs should set up\nexpectations for NPs (the region in bold should\nhave a lower surprisal in (1d) than in (1c)).\n3.2 Model training data\nCorpora We train and evaluate models on En-\nglish newswire corpora of four different sizes, ob-\ntained by randomly sampling sections from the\nBrown Laboratory for Linguistic Information Pro-\ncessing 1987-89 Corpus Release 1 (BLLIP; Char-\nniak et al., 2000). The corpora are sampled such\nthat the training set of each corpus is a proper\nsubset of each larger corpus. We call these four\ncorpora BLLIP- XS (40K sentences, 1M tokens);\nBLLIP- SM (200K sentences, 5M tokens); BLLIP-\nMD (600K sentences, 14M tokens); andBLLIP- LG\n(2M sentences, 42M tokens). Table 1 summarizes\nstatistics of the training set for each corpus.\nTo ensure consistency in perplexity evalua-\ntion across datasets, we report perplexity scores\nachieved by the models on a shared held-out test\nset. We additionally use a shared held-out valida-\ntion for tuning and early stopping.\nWe use the NLTK implementation of the Penn\nTreebank tokenizer to process all datasets (Bird and\nLoper, 2004; Marcus et al., 1993).\n1729\n# layers # hidden units Embedding size\nLSTM 2 256 256\nON-LSTM 3 1150 400\nRNNG 2 256 256\nGPT-2 12 768 768\nTable 2: Size of neural models in our controlled exper-\niments.\nBLLIP sizes: XS SM MD LG\nLSTM 13.4M 30.5M 52.2M 88.1M\nON-LSTM 30.8M 44.2M 61.2M 89.2M\nRNNG 22.8M 48.4M 81.1M 134.9M\nGPT-2 124.4M 124.4M 124.4M 124.4M\nTable 3: Parameter counts for neural models in our con-\ntrolled experiments.\nOut-of-vocabulary tokens For each corpus, we\ndesignate a token as OOV if the token appears\nfewer than two times in the training set. Our larger\ntraining datasets thus contain larger vocabularies\nthan our smaller training datasets. This allows\nlarger-training-set models to learn richer word-\nspeciﬁc information, but may also harm perplexity\nevaluation because they have vocabulary items that\nare guaranteed to not appear in the BLLIP- XS test\nset. This means that perplexity scores across train-\ning dataset sizes will not be strictly comparable:\nif a larger-training-set model does better than a\nsmaller-training-set model, we can be conﬁdent\nthat it has meaningfully lower perplexity, but the\nreverse is not necessarily the case. The exception\nto the above is GPT-2, which uses sub-words from\nbyte-pair encoding and has no OOVs (see also Foot-\nnote 6).\nUnkiﬁcation We follow the convention used by\nthe Berkeley parser (Petrov and Klein, 2007),\nwhich maps OOVs to UNK classes which pre-\nserve ﬁne-grained information such as orthographic\ncase distinctions and morphological sufﬁxes (e.g.\nUNK-ed, UNK-ly). Before training, we veriﬁed\nthat the UNK classes in the test and validation sets\nwere all present in the training set.\n3.3 Model classes\nIn order to study the effects of model inductive\nbias and dataset size, we trained a ﬂeet of models\nwith varying inductive biases on each corpus. Be-\ncause many of our test suites exploit ambiguities\nthat arise from incremental processing, we restrict\nevaluation to left-to-right language models; future\nBLLIP sizes: XS SM MD LG\nLSTM 98.19 65.52 59.05 57.09\nON-LSTM 71.76 54.00 56.37 56.38\nRNNG 122.46 86.72 71.12 69.57\nGPT-2 529.90 183.10 37.04 32.14\nn-gram 240.21 158.60 125.58 106.09\nTable 4: Perplexity averages achieved by each con-\ntrolled model on each corpus. Perplexity scores across\ntraining dataset sizes are not always strictly comparable\n(see Section 3.2).\nwork could involve evaluation of bidirectional mod-\nels (Devlin et al., 2018; Yang et al., 2019) on an\nappropriate subset of our test suites, and/or adapta-\ntion of our suites for use with bidirectional models\n(Goldberg, 2019). Training ran until convergence\nof perplexity on a held-out validation set. Wher-\never possible, we trained multiple seeds of each\nmodel class and corpus size. We use the model\nsizes and training hyperparameters reported in the\npapers introducing each model (Table 2).4 The full\nparameter counts and perplexity scores for each\nmodel ×corpus combination are given in Tables 3\nand 4, respectively.\nLSTM Our baseline neural model is a vanilla\nlong short-term memory network (LSTM; Hochre-\niter and Schmidhuber, 1997) based on the boiler-\nplate PyTorch implementation (Paszke et al., 2017).\nOrdered-Neurons We consider the Ordered-\nNeurons LSTM architecture (ON-LSTM; Shen\net al., 2019), which encodes an explicit bias to-\nwards modeling hierarchical structure.\nRNNG Recurrent neural network grammars\n(RNNG; Dyer et al., 2016) model the joint prob-\nability of a sequence of words and its syntactic\nstructure. RNNG requires labeled trees that con-\ntain complete constituency parses, which we pro-\nduce for BLLIP sentences with an off-the-shelf\nconstituency parser (Kitaev and Klein, 2018).5 To\ncompute surprisals from RNNG, we use word-\nsynchronous beam search (Stern et al., 2017) to\napproximate the conditional probability of the cur-\nrent word given the context.\n4Due to computational constraints, we performed only mini-\nmal tuning past these recommended hyperparameters.\n5While the BLLIP corpus already contains Treebank-style\nparses, we strip the terminals and re-parse in order to obtain\nmore accurate, up-to-date syntactic parses.\n1730\nGPT/uni00AD2/uni00ADXL *\nGPT/uni00AD2 *\nTransformer/uni00ADXL *\nJRNN *\nGPT/uni00AD2GRNN *\nRNNGON/uni00ADLSTMLSTM n/uni00ADgram\nModel\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSG/uni00A0score\nFigure 1: Average SG score by model class. Asterisks\ndenote off-the-shelf models. Error bars denote boot-\nstrapped 95% conﬁdence intervals of the mean.\nTransformer Transformer models (Vaswani\net al., 2017) have recently gained popularity in lan-\nguage processing tasks. We use GPT-2 (Radford\net al., 2019) as a representative Transformer model\nand train it from scratch on our BLLIP corpora.6\nn-gram As a baseline, we consider a 5-gram\nmodel with modiﬁed Kneser-Ney smoothing.\n3.4 Off-the-shelf models\nWe also test ﬁve off-the-shelf models: GRNN,\ntrained on 90M tokens from Wikipedia (Gulordava\net al., 2018); JRNN, trained on 800M tokens from\nthe 1 Billion Word Benchmark (Jozefowicz et al.,\n2016); Transformer-XL, trained on 103M tokens\nfrom WikiText-103 (Dai et al., 2019); and the pre-\ntrained GPT-2 and GPT-2-XL, trained on 40GB of\nweb text (Radford et al., 2019). These models are\norders of magnitude larger than our controlled ones\nin parameter count and/or training set size.\n4 Results\nFigure 1 shows the average accuracy of all mod-\nels on the complete set of SG test suites. Aster-\nisks denote off-the-shelf models. All neural mod-\nels achieve a SG score signiﬁcantly greater than\na random baseline (dashed line). However, the\nrange within neural models is notable, with the best-\nperforming model (GPT-2-XL) scoring over twice\nas high as the worst-performing model (LSTM).\nAlso notable are the controlled GPT-2 and RNNG\nmodels, which achieve comparable performance to\nTransformer-XL and JRNN, despite being trained\non signiﬁcantly smaller data sizes.\n6Our GPT-2 code is based on nshepperd/gpt-2. The\nmodel vocabulary consists of byte-pair encoded sub-words\nextracted from the GPT-2 pre-trained model, not from the\nBLLIP training corpora. To calculate GPT-2 perplexities, we\ndivide the sum of all sub-word conditional log-probabilities\nby the total number of words in the corpus.\n0 50 100 150 200 250\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8SG/uni00A0score\n520 540\nTest/uni00A0perplexity\nGPT/uni00AD2 *\nGPT/uni00AD2/uni00ADXL *\nGRNN *\nJRNN *\nRandom\nTransformer/uni00ADXL *\nGPT/uni00AD2 *\nGPT/uni00AD2/uni00ADXL *\nGRNN *\nJRNN *\nRandom\nTransformer/uni00ADXL *\nBLLIP/uni00ADLG\nLSTM\nBLLIP/uni00ADMD\nON/uni00ADLSTM\nBLLIP/uni00ADSM\nRNNG\nBLLIP/uni00ADXS\nGPT/uni00AD2 n/uni00ADgram\nFigure 2: Relationship between SG score and perplex-\nity on our held-out BLLIP test set for each model.\nWe now return to the three major issues pre-\nsented in Section 1. In 4.1 we present evidence that\nSG score is dissociated from perplexity. In 4.2 we\nargue that model architecture accounts for larger\ngains in SG score than amount of training data.\nAnd in 4.3 we show that this cross-architecture dif-\nference is due largely to variance on a handful of\nkey test suites.\n4.1 Syntactic generalization and perplexity\nFigure 2 shows the relationship between SG score\nand perplexity on the BLLIP test set across mod-\nels and training set sizes. As expected, n-gram\nmodels never rise appreciably above chance in SG\nscore. Among neural models, GPT-2 achieves both\nthe worst (BLLIP- XS and BLLIP- SM) and best\n(BLLIP- MD and BLLIP- LG) performance; the im-\npressive performance of these latter models comes\nwith the caveat that the sub-words come from the\npre-trained GPT-2 model, tacitly importing infor-\nmation from a larger training dataset (see further\ndiscussion in Section 4.5). For the remaining neu-\nral models, there is no simple relationship between\nperplexity and SG score, especially once training\ndataset size is controlled for (comparing points in\nFigure 2 of the same color). For example, there is\na remarkable amount of variance in the SG score\nof models trained on BLLIP- LG not explained by\nperplexity. This suggests that targeted syntactic\nevaluation can reveal information that may be or-\nthogonal to perplexity.\n1731\nLSTM ON/uni00ADLSTMRNNG GPT/uni00AD2n/uni00ADgram\nModel\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nSG/uni00A0score/uni00A0delta\nBLLIP/uni00ADLGBLLIP/uni00ADMDBLLIP/uni00ADSMBLLIP/uni00ADXS\nCorpus\nLSTM ON/uni00ADLSTMRNNG GPT/uni00AD2n/uni00ADgram\nFigure 3: Main results of our controlled evaluation of model class and dataset size. SG score varies more by model\nclass (left) than by training dataset size (right).\n4.2 Inductive bias and data scale\nIn order to decouple the effects of model class and\ndata scale from test suite difﬁculty, we represent a\nparticular trained model’s performance on each test\nsuite as a delta relative to the average performance\nof all models on this test suite. Unless noted oth-\nerwise, the remainder of the ﬁgures in this section\nplot a score delta, aggregating these deltas within\nmodel classes or corpus types.\nFigure 3 tracks the inﬂuence of model class and\ndata scale across the model types tested in our ex-\nperiments, with SG score deltas on the y-axis. The\nleft-hand panel shows the difference in SG score by\nmodel class. We ﬁnd that model class clearly inﬂu-\nences SG score: for example, the error bars (boot-\nstrapped 95% conﬁdence intervals of the mean) for\nRNNG and LSTM do not overlap. The right-hand\npanel shows the difference in SG score delta by\ntraining dataset, and shows a much more minor in-\ncrease in mean SG score as training data increases.\nWe tested the inﬂuence of these factors quan-\ntitatively using a linear mixed-effects regression\nmodel, predicting suite-level performance as a fea-\nture of model architecture and training dataset size\n(represented as log-number of words). Both fea-\ntures made statistically signiﬁcant contributions to\nSG score (both p< 0.001). However, predictor ab-\nlation indicates that architecture affects regression\nmodel ﬁt more (AIC=–581 when dataset size is\nablated; AIC=–574 when architecture is ablated).7\nBeyond the above analysis, our GPT-2 results\noffer another striking example of the inﬂuence of\n7n-grams and/or GPT-2 could arguably be expected to have\nqualitatively different sensitivity to training dataset size (the\nlatter due to byte-pair encoding), so we repeated the anal-\nyses here and in Section 4.3 excluding both architectures\nindividually as well as simultaneously. In all cases the same\nqualitative patterns described in the main text hold.\nmodel architecture relative to data scale. Figure 2\nshows that our controlled BLLIP- MD and BLLIP-\nLG GPT-2 models achieve roughly the same SG\nscore as the pre-trained GPT-2 model, despite being\ntrained on less than 1% of the data used by the pre-\ntrained model. This suggests diminishing returns\nto training data scale for syntactic generalization\nperformance.\n4.3 Circuit-level effects on SG score\nFigure 4 shows the breakdown at the circuit level by\nmodel architecture (left) and training dataset size\n(right). The right panel demonstrates little effect\nof dataset size on SG score delta within most cir-\ncuits, except for Agreement, on which the models\ntrained on our smallest dataset fare poorly. In the\nleft panel we ﬁnd substantial between-circuit dif-\nferences across architectures. Linear mixed-effects\nanalyses support this ﬁnding: interactions with cir-\ncuit are signiﬁcant for both training dataset size\nand model architecture, but stronger for the latter\n(AIC=–654 and AIC=–623 when size and architec-\nture are respectively ablated).\nWhile model inductive biases separate clearly in\nperformance on some circuits, they have little ef-\nfect on performance on Licensing. This minimally\nsuggests that Licensing taps into a distinct syntac-\ntic process within language models. One potential\nexplanation for this is that the interactions tested by\nLicensing involve tracking two co-varying tokens\nwhere the downstream token is optional (see e.g.\nHu et al., 2020).\nWe show the circuit-level breakdown of absolute\nSG scores for all models (including off-the-shelf)\nin Figure 5. In general, the models that obtain high\nSG scores on average (as in Figure 1) also perform\nwell across circuits: pre-trained GPT-2 and GPT-\n1732\nAgreement\nCenter/uni00A0EmbeddingGarden/uni00ADPath/uni00A0EffectsGross/uni00A0Syntactic/uni00A0State\nLicensing\nLong/uni00ADDistance/uni00A0Dependencies\nCircuit\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\nSG/uni00A0score/uni00A0delta\nLSTM ON/uni00ADLSTMRNNG GPT/uni00AD2n/uni00ADgram\nAgreement\nCenter/uni00A0EmbeddingGarden/uni00ADPath/uni00A0EffectsGross/uni00A0Syntactic/uni00A0State\nLicensing\nLong/uni00ADDistance/uni00A0Dependencies\nCircuit\nBLLIP/uni00ADLGBLLIP/uni00ADMDBLLIP/uni00ADSMBLLIP/uni00ADXS\nFigure 4: Controlled evaluation results, split across test suite circuits. Circuit-level differences in SG score vary\nmore by model class (left) than by training dataset size (right).\nAgreement\nCenter/uni00A0EmbeddingGarden/uni00ADPath/uni00A0EffectsGross/uni00A0Syntactic/uni00A0State\nLicensing\nLong/uni00ADDistance/uni00A0Dependencies\nCircuit\n0.0\n0.5\n1.0SG/uni00A0score\nGPT/uni00AD2/uni00ADXL *\nGPT/uni00AD2 *\nTransformer/uni00ADXL *\nJRNN *\nGPT/uni00AD2\nGRNN *\nRNNG\nON/uni00ADLSTM\nLSTM\nn/uni00ADgram\nFigure 5: Evaluation results on all models, split across test suite circuits.\n2-XL outperform all other models on each circuit,\nincluding Licensing, on which JRNN, GRNN, and\nmost of our custom-trained models perform partic-\nularly poorly. Again, we highlight the impressive\nperformance of RNNG: it achieves comparable av-\nerage performance to GRNN on all circuits, despite\nbeing trained on a fraction of the data size.\n4.4 Stability to modiﬁers\nWe separately investigate the degree to which mod-\nels’ syntactic generalizations are robustly stored in\nmemory. For ﬁve test suites (Center Embedding,\nCleft, MVRR, NPZ-Ambiguous, NPZ-Object), we\ndesigned minimally edited versions where syntac-\ntically irrelevant intervening content was inserted\nbefore the critical region. An ideal model should\nrobustly represent syntactic features of its input\nacross these modiﬁer insertions.\nIn Figure 6 we plot models’ average scores on\nthese ﬁve test suites (dark bars) and their minimally\nedited versions (light bars), evaluating how robust\neach model is to intervening content. Among mod-\nels in our controlled experiments, we see that model\nclass clearly inﬂuences the degree to which predic-\ntions are affected by intervening content (compare\ne.g. the stability of RNNG to that of ON-LSTM).\nSome off-the-shelf models, such as GPT-2-XL, per-\nform near ceiling on the original ﬁve test suites and\nare not affected at all by intervening content.\nGPT/uni00AD2/uni00ADXL *\nGPT/uni00AD2 *\nTransformer/uni00ADXL *\nJRNN *\nGPT/uni00AD2GRNN *\nRNNGON/uni00ADLSTMLSTM n/uni00ADgram\nModel\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0SG/uni00A0score\nNo/uni00A0modifier\nWith/uni00A0modifier\nFigure 6: SG score on the pairs of test suites with\nand without intervening modiﬁers: Center Embedding,\nCleft, MVRR, NPZ-Ambiguous, and NPZ-Object.\n1733\n4.5 Effects of model pre-processing\nThe GPT-2 models trained and evaluated in this pa-\nper use a sub-word vocabulary learned by byte-pair\nencoding (BPE; Sennrich et al., 2016) to represent\ntheir inputs, while all other models represent and\ncompute over word-level inputs. This byte-pair\nencoding was taken from the pre-trained GPT-2\nmodel trained on a much larger corpus. The results\nreported for these models thus conﬂate a choice\nof model class (a deep Transformer architecture)\nand preprocessing standard (sub-word tokenization\ncomputed on a larger corpus). Some preliminary\nwork suggests that sub-word tokenization is indeed\nresponsible for much of the larger GPT-2 mod-\nels’ success: we ﬁnd that GPT-2 models trained\non word-level representations of BLLIP- LG and\nBLLIP- MD achieve good perplexity measures, but\ndegrade sharply in SG score.\nPeculiarities of the GPT-2 training regime may\nbe responsible for its particularly bad performance\non the smaller corpora. Its sub-word vocabulary\nwas held constant across training corpora, meaning\nthat the model vocabulary size also remained con-\nstant across corpora, unlike the other models tested.\nThe poor performance of GPT-2 models trained on\nsmaller corpora may thus be due to overparame-\nterization, and not due to fundamental problems\nwith the model architecture at small data scales.\nWe leave a thorough investigation of the role of\nsub-word tokenization to future work.\n5 Discussion\nThis work addresses multiple open questions about\nsyntactic evaluations and their relationship to other\nlanguage model assessments. Our results dissoci-\nate model perplexity and performance in syntactic\ngeneralization tests, suggesting that the two metrics\ncapture complementary features of language model\nknowledge. In a controlled evaluation of different\nmodel classes and datasets, we ﬁnd model architec-\nture plays a more important role than training data\nscale in yielding correct syntactic generalizations.\nOur circuit-level analysis reveals consistent failure\non Licensing but inconsistent behavior on other\ncircuits, suggesting that different syntactic circuits\nmake use of different underlying processing capac-\nities. In addition to the insight these results provide\nabout neural NLP systems, they also bear on ques-\ntions central to cognitive science and linguistics,\nputting lower bounds on what syntactic knowledge\ncan be acquired from string input alone.\nTargeted syntactic evaluation is just one in a se-\nries of complementary methods being developed\nto assess the learning outcomes of neural language\nprocessing models. Other methods include classi-\nfying sentences as grammatical or ungrammatical\n(Warstadt et al., 2019b), decoding syntactic fea-\ntures from a model’s internal state (Belinkov et al.,\n2017; Giulianelli et al., 2018), or transfer learning\nto a strictly syntactic task such as parsing or POS\ntagging (Hewitt and Manning, 2019). As each task\nbrings an explicit set of assumptions, complemen-\ntary assessment methods can collectively provide\ngreater insight into models’ learning outcomes.\nAlthough this paper, together with Warstadt et al.\n(2020), report what is to our knowledge the largest-\nscale targeted syntactic evaluations to date, we\nemphasize that they are only ﬁrst steps toward a\ncomprehensive understanding of the syntactic capa-\nbilities of contemporary language models. This\nunderstanding will be further advanced by new\ntargeted-evaluation test suites covering a still wider\nvariety of syntactic phenomena, additional trained\nmodels with more varied hyperparameters and ran-\ndomization seeds, and new architectural innova-\ntions. Humans develop extraordinary grammatical\ncapabilities through exposure to natural linguistic\ninput. It remains to be seen to just what extent\ncontemporary artiﬁcial systems do the same.\nAcknowledgments\nThe authors would like to thank the anonymous\nreviewers and Samuel R. Bowman for their feed-\nback, Miguel Ballesteros for advice and technical\nguidance, and Tristan Thrush for technical assis-\ntance. J.H. is supported by the NIH under award\nnumber T32NS105587 and an NSF Graduate Re-\nsearch Fellowship. J.G. is supported by an Open\nPhilanthropy AI Fellowship. R.P.L. gratefully ac-\nknowledges support from the MIT-IBM Watson\nAI Lab, a Google Faculty Research Award, and a\nNewton Brain Science Award.\nReferences\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan\nSajjad, and James Glass. 2017. What do neural ma-\nchine translation models learn about morphology?\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 861–872.\nTom Bever. 1970. The cognitive basis for linguistic\nstructures. In J.R. Hayes, editor, Cognition and\n1734\nthe Development of Language, pages 279–362. New\nYork: John Wiley & Sons.\nSteven Bird and Edward Loper. 2004. NLTK: The nat-\nural language toolkit. In Proceedings of the ACL In-\nteractive Poster and Demonstration Sessions, pages\n214–217, Barcelona, Spain. Association for Compu-\ntational Linguistics.\nKathryn Bock and Carol A. Miller. 1991. Broken\nagreement. Cognitive Psychology, 23:45–93.\nAndrew Carnie. 2012. Syntax: A generative introduc-\ntion, volume 18. John Wiley & Sons.\nEugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,\nJohn Hale, and Mark Johnson. 2000. BLLIP 1987-\n89 WSJ Corpus Release 1 LDC2000T43. Linguistic\nData Consortium.\nRui P. Chaves. 2020. What don’t RNN language mod-\nels learn about ﬁller-gap dependencies? In Proceed-\nings of the Society for Computation in Linguistics.\nNoam Chomsky. 1957. Syntactic structures. Walter de\nGruyter.\nShammur Absar Chowdhury and Roberto Zamparelli.\n2018. RNN simulations of grammaticality judg-\nments on long-distance dependencies. In Proceed-\nings of the 27th International Conference on Compu-\ntational Linguistics, pages 133–144, Santa Fe, New\nMexico, USA.\nStephen Crain and Janet Dean Fodor. 1985. How can\ngrammars help parsers? In David Dowty, Lauri\nKartunnen, and Arnold M. Zwicky, editors, Natu-\nral Language Parsing: Psycholinguistic, Computa-\ntional, and Theoretical Perspectives, pages 940–128.\nCambridge: Cambridge University Press.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 4171–\n4186, Minneapolis, Minnesota.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers).\nFernanda Ferreira and Charles Clifton, Jr. 1986. The\nindependence of syntactic processing. Journal of\nMemory and Language, 25:348–368.\nVictoria Fossum and Roger P. Levy. 2012. Sequen-\ntial vs. hierarchical syntactic models of human in-\ncremental sentence processing. In Proceedings of\nthe 3rd Workshop on Cognitive Modeling and Com-\nputational Linguistics, pages 61–69.\nStefan L Frank and Rens Bod. 2011. Insensitivity\nof the human sentence-processing system to hierar-\nchical structure. Psychological Science, 22(6):829–\n834.\nLyn Frazier and Keith Rayner. 1982. Making and cor-\nrecting errors during sentence comprehension: Eye\nmovements in the analysis of structurally ambiguous\nsentences. Cognitive Psychology, 14:178–210.\nRichard Futrell, Ethan Wilcox, Takashi Morita, and\nRoger Levy. 2018. RNNs as psycholinguistic sub-\njects: Syntactic state and grammatical dependency.\narXiv preprint arXiv:1809.01329.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic sub-\njects: Representations of syntactic state. In Pro-\nceedings of the 18th Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 32–42.\nAnastasia Giannakidou. 2011. Negative and positive\npolarity items: Variation, licensing, and composi-\ntionality. In Semantics: An international hand-\nbook of natural language meaning, volume 3, pages\n1660–1712. Berlin: Mouton de Gruyter.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the hood: Using diagnostic classiﬁers to in-\nvestigate and improve how language models track\nagreement information. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP , pages 240–\n248.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. arXiv preprint arXiv:1901.05287.\nAdam Goodkind and Klinton Bicknell. 2018. Predic-\ntive power of word surprisal for reading times is a\nlinear function of language model quality. In Pro-\nceedings of the 8th Workshop on Cognitive Modeling\nand Computational Linguistics, pages 10–18.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1195–1205, New Or-\nleans, Louisiana.\nJohn Hale. 2001. A probabilistic Earley parser as a psy-\ncholinguistic model. In Proceedings of the second\n1735\nmeeting of the North American Chapter of the Asso-\nciation for Computational Linguistics on Language\ntechnologies, pages 1–8.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nFrancis Roger Higgins. 1973. The Pseudo-Cleft Con-\nstruction in English. Ph.D. thesis, MIT.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nJennifer Hu, Sherry Yong Chen, and Roger P. Levy.\n2020. A closer look at the performance of neural\nlanguage models on reﬂexive anaphor licensing. In\nProceedings of the Meeting of the Society for Com-\nputation in Linguistics.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2676–2686, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nWilliam Ladusaw. 1979. Polarity Sensitivity as Inher-\nent Scope Relations . Ph.D. thesis, University of\nTexas at Austin.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probabil-\nity: A probabilistic view of linguistic knowledge.\nCognitive Science, 5:1202–1247.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. In Transactions of\nthe Association for Computational Linguistics , vol-\nume 4, pages 521–535.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19:313–330.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 3428–\n3448, Florence, Italy.\nGeorge A. Miller and Noam Chomsky. 1963. Fini-\ntary models of language users. In R. Duncan Luce,\nRobert R. Bush, and Eugene Galanter, editors,Hand-\nbook of Mathematical Psychology, volume II, pages\n419–491. New York: John Wiley & Sons, Inc.\nDon C. Mitchell. 1987. Lexical guidance in human\nparsing: Locus and processing characteristics. In\nMax Coltheart, editor, Attention and Performance\nXII: The psychology of reading. London: Erlbaum.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nIn Neural Information Processing Systems Autodiff\nWorkshop.\nSlav Petrov and Dan Klein. 2007. Improved inference\nfor unlexicalized parsing. In Human Language Tech-\nnologies 2007: The Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics; Proceedings of the Main Conference ,\npages 404–411, Rochester, New York. Association\nfor Computational Linguistics.\nMartin J. Pickering and Matthew J. Traxler. 1998. Plau-\nsibility and recovery from garden paths: An eye-\ntracking study. Journal of Experimental Psychology:\nLearning, Memory, & Cognition, 24(4):940–961.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report.\nTanya Reinhart. 1981. Deﬁnite NP anaphora and c-\ncommand domains. Linguistic Inquiry, 12(4):605–\n635.\nJohn Robert Ross. 1967. Constraints on Variables in\nSyntax. Ph.D. thesis, MIT.\nMarten van Schijndel and Tal Linzen. 2018. Modeling\ngarden path effects without explicit hierarchical syn-\ntax. In Proceedings of the 40th Annual Meeting of\nthe Cognitive Science Society.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with\nneural language models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5835–5841.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\n1736\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2019. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks. In\nInternational Conference on Learning Representa-\ntions.\nNathaniel J. Smith and Roger P. Levy. 2013. The effect\nof word predictability on reading time is logarithmic.\nCognition, 128:302–319.\nAdrian Staub. 2007. The parser doesn’t ignore intransi-\ntivity, after all. Journal of Experimental Psychology:\nLearning, Memory, & Cognition, 33(3):550–569.\nMitchell Stern, Daniel Fried, and Dan Klein. 2017. Ef-\nfective inference for generative neural parsing. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n1695–1700.\nLaurie A Stowe. 1986. Parsing wh-constructions: Evi-\ndence for on-line gap location. Language & Cogni-\ntive Processes, 1(3):227–245.\nPatrick Sturt, Martin J. Pickering, and Matthew W.\nCrocker. 1999. Structural change and reanalysis dif-\nﬁculty in language comprehension. Journal of Mem-\nory and Language, 40:136–150.\nJohn C. Trueswell, Michael K. Tanenhaus, and Su-\nsan M. Garnsey. 1994. Semantic inﬂuences on pars-\ning: Use of thematic role information in syntactic\nambiguity resolution. Journal of Memory and Lan-\nguage, 33:285–318.\nShravan Vasishth, Sven Br ¨ussow, Richard L Lewis,\nand Heiner Drenhaus. 2008. Processing polarity:\nHow the ungrammatical intrudes on the grammati-\ncal. Cognitive Science, 32(4):685–712.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019a. SuperGLUE: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, pages 3266–3280.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R\nBowman. 2020. BLiMP: A Benchmark of Linguis-\ntic Minimal Pairs for English. In Proceedings of the\nSociety for Computation in Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019a. CoLA: The Corpus of Linguistic Ac-\nceptability (with added annotations).\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019b. Neural network acceptability judg-\nments. Transactions of the Association for Compu-\ntational Linguistics, 7:625–641.\nEthan Wilcox, Jon Gauthier, Jennifer Hu, Peng Qian,\nand Roger P. Levy. 2020. Evaluating neural net-\nworks as models of human online language process-\ning. In Proceedings of the 42nd Meeting of the Cog-\nnitive Science Society (CogSci 2020). To appear.\nEthan Wilcox, Roger P. Levy, and Richard Futrell.\n2019a. Hierarchical representation in neural lan-\nguage models: Suppression and recovery of expec-\ntations. In Proceedings of the 2019 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP.\nEthan Wilcox, Roger P. Levy, Takashi Morita, and\nRichard Futrell. 2018. What do RNN language\nmodels learn about ﬁller–gap dependencies? In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallestros, and Roger P. Levy. 2019b. Structural su-\npervision improves learning of non-local grammati-\ncal dependencies. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 3302–3312, Minneapo-\nlis, Minnesota.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems.\nA Syntactic coverage of test suites\nIn order to assess the coverage of our syntactic\ntests, we manually inspected the “Ideas, Rules and\nConstraints introduced in this Chapter” section for\neach chapter in Carnie (2012), a standard introduc-\ntory syntax textbook. We included entries from\nthese sections which are theory-neutral and refer to\nobservable linguistic data. For example, we do not\ninclude afﬁx lowering (Chapter 7) or theta criterion\n(Chapter 8) because these phenomena presuppose\na commitment to one particular syntactic analysis.\nWe found that our tests covered 16 of the 47\nphenomena presented ( ∼34%). Of the 15 chap-\nters surveyed, our tests assessed phenomena in 11\n1737\nCHAPTER1: GENERATIVEGRAMMAR Lexical gender\nNumber ✓\nPerson\nCase\nCHAPTER2: PARTS OFSPEECH Parts of Speech ✓\nPlurality ✓\nCount vs. Mass Nouns\nArgument Structure of Verbs ✓\nCHAPTER3: CONSTITUENCY, TREES, RULES Constituency Tests\nHierarchical Structure ✓\nCHAPTER4: STRUCTURALRELATIONS c-command ✓\nGovernment\nCHAPTER5: BINDINGTHEORY R-expression vs. Pronominals\nAnaphoric expressions and their antecedents ✓\nCo-reference and co-indexation\nBinding Principles (A,B,C) ✓\nLocality Constraints ✓\nCHAPTER6: X-BARTHEORY One Replacement\nDo-so Replacement\nCHAPTER7: EXTENDINGX-BARTHEORY Fundamental Phrase Types of DP/CP/TP\nTOFUNCTIONALCATEGORIES Genitives: of-genitives and ’s genitives\nSubjects and Predicates\nClausal Embedding ✓\nClausal\nTense/Finiteness and its restrictions\nYes/No Questions\nSubject-Auxilliary Inversion\nCHAPTER8: CONSTRAININGX-BARTHEORY: Thematic Relations ✓\nTHELEXICON Internal Theta role vs. External Theta Roles\nExpletive Pronouns and Expletive Insertion\nExtended Projection Principle\nCHAPTER9: HEAD-TO-HEADMOVEMENT V→T Movement\nT→C movement ✓\nDo-Support\nCHAPTER10: DP MOVEMENT Passive Constructions ✓\nDP-Raising\nCHAPTER11: WH-MOVEMENT Wh-Movement ✓\nStructural Constraints on Wh-Movement (Island Constraints)✓\nWh in-Situ and Echo Questions\nCHAPTER12: A UNIFIEDTHEORY Universal Quantiﬁers vs. Existential Quantiﬁers\nOF MOVEMENT Quantiﬁcational Scope and Quantiﬁer Raising\nCHAPTER13: EXTENDEDVPS Light Verbs\nObject Shift (and end weight)\nEllipsis\nPseudogapping\nCHAPTER14: RAISINGCONTROL AND Control, Subject-to-Subject and Subject-to-Object Raising (ECM)\nEMPTYCATEGORIES\nCHAPTER15: ADVANCEDTOPICS IN Binding PrincipleAandB ✓\nBINDINGTHEORY\nTable 5: Test suite coverage of syntactic phenomena presented in Carnie (2012).\n1738\n(∼73%). We did not assess coverage from the last\ntwo chapters of the book, which explore alternative\nsyntactic formalisms. The outcome of our manual\ninspection is given in Table 5.\nA ✓indicates that some aspect of that phenom-\nena was tested in one or more of our suites. ✓does\nnot necessarily mean that the test suite was de-\nsigned explicitly for the purpose of testing that\nphenomena, but merely that the phenomena was\nimplicated in model success. For example, we\nplace a ✓next to Parts of Speech because differen-\ntiation between verbs and nouns is necessary for\nmodels to succeed in the Cleft Structure tests.\nB Description of test suites\nIn this work we have assembled a large number of\ntest suites inspired by the methodology of experi-\nmental sentence-processing and psycholinguistic\nresearch. Each test suite contains a number of\nITEMS , and each item appears in several CONDI -\nTIONS : across conditions, a given item will differ\nonly according to a controlled manipulation de-\nsigned to target a particular feature of grammatical\nknowledge. For each suite we deﬁne a SUCCESS\nCRITERION , which stipulates inequalities among\nconditional probabilities of sentence substrings.\nIn the main paper, a model’s accuracy for a test\nsuite is computed as the percentage of the test\nsuite’s items for which it satisﬁes the criterion. In\nthis appendix, we brieﬂy describe each test suite\nand the criterion used to determine whether a given\nmodel succeeds on each item of the test suite.\nB.1 Notation\nB.1.1 Sentence status\nFollowing and building on linguistic traditions, we\nannotate examples as follows. Examples marked\nwith a * violate a well-established grammatical con-\nstraint, and are ungrammatical. Examples marked\nwith ? or ?? are not necessarily ungrammatical, but\nare marginal: for example, they may require an\nunusual interpretation of a word in order for the\nsentence to be grammatical. (More ?’s is roughly\nintended to indicate more severe marginality). Ex-\namples marked with ! are not ungrammatical, but\ninduce severe processing difﬁculty that is mea-\nsurable in real-time human sentence processing.\nFor all test suites, we include references to estab-\nlished literature on the relevant grammatical and/or\nsentence-processing phenomena.\nB.1.2 Success criteria\nCriteria involve inequalities among conditional\nprobabilities of sentence substrings given the com-\nplete sentence context preceding the substring.\nIn describing criteria, we use P(·) for raw prob-\nabilities and S(·) for surprisals (negative log-\nprobabilities), and leave the conditioning on pre-\nceding context implicit. For concision, we use\nsubscripts on P and S to indicate the variant of\nthe sentence within the test suite that we are refer-\nring to. In the ﬁrst described test suite, CENTER\nEMBEDDING B.2, we show the criterion in both\nconcise and fully spelled-out forms, to help clarify\nthe conventions we are using in the concise form.\nAll items within a given test suite share the same\ncriterion for success.\nWe provide chance accuracy on the assumption\nthat the order of probabilities among conditions\nfor a given item is random. In some cases, exactly\ndetermining chance accuracy may require further\nassumptions about the distribution of these proba-\nbilities; in this case we provide an upper bound on\nchance accuracy.\nB.2 Center embedding\nCenter embedding, the ability to embed a phrase\nin the middle of another phrase of the same type,\nis a hallmark feature of natural language syntax.\nCenter-embedding creates NESTED SYNTACTIC\nDEPENDENCIES , which could pose a challenge for\nsome language models. To succeed in generating\nexpectations about how sentences will continue in\nthe context of multiple center embedding, a model\nmust maintain a representation not only of what\nwords appear in the preceding context but also of\nthe order of those words, and must predict that up-\ncoming words occur in the appropriate order. In\nthis test suite we use verb transitivity and subject–\nverb plausibility to test model capabilities in this\nrespect. For example, A below is a correct center-\nembedding, but B is not:\n(A) The paintingN1 that the artist N2 paintedV2\ndeterioratedV1 . [correct]\n(B) ?? The painting N1 that the artist N2\ndeterioratedV1 paintedV2 . [incorrect]\nHere, Ni and Vi correspond to matched subject–\nverb pairs.\nIn the WITH -MODIFIER version of the test suite,\nwe postmodify N2 with a relative clause to increase\nthe linear distance over which the nested dependen-\n1739\ncies must be tracked, potentially leading to a harder\ntest suite:\n(A) The paintingN1 that the artist N2 who lived\nlong ago paintedV2 deterioratedV1 . [correct]\n(B) # The paintingN1 that the artist N2 who lived\nlong ago deterioratedV1 paintedV2 . [incor-\nrect]\nCriterion The probability of the verb sequence\nin the correct variant should be higher than the\nprobability of the verb sequence in the incorrect\nvariant:\nPA(V2V1) >PB(V1V2)\nIn full form, this criterion for the example item in\nthe no-modiﬁer version of this test suite would be:\nP(painted deteriorated|The painting that the artist) >\nP(deteriorated painted|The painting that the artist)\nChance performance on these center-embedding\ntest suites would be 50%.\nReferences Miller and Chomsky (1963);Wilcox\net al. (2019a)\nB.3 Pseudo-clefting\nThe pseudo-cleft construction involves (i) an ex-\ntraction of a TARGETED CONSTITUENT from a\nsentence and (ii) a constituent that provides the\nsemantic contents of the targeted constituent and\nmust match it in syntactic category, where (i) and\n(ii) are linked by the copula. The pseudo-cleft con-\nstruction can target both NPs and VPs; in the latter\ncase, the VP of the free relative becomes an in-\nﬂected form of do. This means that a free relative\nsubject plus the copula can set up a requirement\nfor the syntactic category that comes next. If the\nfree relative clause has a do VP without a direct\nobject, then the main-clause postcopular predicate\ncan be a VP (A below). Otherwise, the postcopular\npredicate must be an NP (C below):\n(A) What the worker did was\nVP\n  \nboard the plane.\n(B) ?What the worker did was\nNP\n  \nthe plane.\n(C) What the worker repaired was\nNP\n  \nthe plane.\n(D) * What the worker repaired was\nVP\n  \nboard the plane.\nCriterion The postcopular predicate should be\nmore surprising when its syntactic category mis-\nmatches the cleft, averaging across VP and NP\npostcopular predicates:\nSD(VP) +SB(NP) >SC(NP) +SA(VP)\nChance is 50%. A more stringent criterion would\nbe to apply this requirement separately for each of\nNP and VP postcopular predicates:\nSD(VP) >SA(VP) ∧SB(NP) >SC(NP)\nHowever, it is often possible to use an NP post-\ncopular predicate with a do cleft through semantic\ncoercion (e.g., in B “did” can be interpreted as\n“ﬁxed” or “was responsible for”), so we felt that\nthis latter criterion might be too stringent.\nReferences Higgins (1973)\nB.4 Filler–gap dependencies\nConsider the following sentence, in which all argu-\nments and adjuncts appear “in situ” (in the syntac-\ntic position at which they are normally interpreted\nsemantically):\nI know that our uncle grabbed the food\nin front of the guests at the holiday party.\nA FILLER –GAP DEPENDENCY can be created by\nEXTRACTING any of a number of elements from\nthe subordinate clause, including our uncle (sub-\nject extraction), the food (object extraction) or the\nguests (extraction from a prepositional phrase).\nThese possibilities serve as the basis for several\ntest suites on ﬁller–gap dependencies.\nReferences Ross (1967); Crain and Fodor\n(1985); Stowe (1986); Wilcox et al. (2018); Chowd-\nhury and Zamparelli (2018); Chaves (2020)\nB.4.1 Subject extractions\n(A) I know that\nα\n  \nour uncle grabbed the food in\nfront of the guests at the holiday party.\n[THAT, NO GAP ]\n(B) * I know who\nα\n  \nour uncle grabbed the food in\nfront of the guests at the holiday party. [WH,\nNO GAP ]\n(C) * I know that\nβ\n  \ngrabbed the food in front of the\nguests at the holiday party. [THAT, GAP ]\n1740\n(D) I know who\nβ\n  \ngrabbed the food in front of the\nguests at the holiday party. [WH, GAP ]\nCriterion We require that a model successfully\npass a two-part criterion for each item: the wh-\nﬁller should make the unextracted subject αmore\nsurprising in the NO-GAP conditions and should\nmake the post-gap material βless surprising in the\nGAP conditions:\nSB(α) >SA(α) ∧SC(β) >SD(β)\nChance is 25%.\nB.4.2 Object extractions\nThe logic of this test suite is the same as that for\nsubject extraction above. Note that we use obliga-\ntorily transitive embedded verbs, so that omitting\na direct object should be highly surprising when\nthere is no ﬁller, as in C.\n(A) I know that our uncle grabbed\nα\n  \nthe food in\nfront of the guests at the holiday party.\n[THAT, NO GAP ]\n(B) * I know what our uncle grabbed\nα\n  \nthe food in\nfront of the guests at the holiday party. [WH,\nNO GAP ]\n(C) ?? I know that our uncle grabbed\nβ\n  \nin front of the\nguests at the holiday party. [THAT, GAP ]\n(D) I know what our uncle grabbed\nβ\n  \nin front of in\nfront of the guests at the holiday party. [WH,\nGAP ]\nCriterion\nSB(α) >SA(α) ∧SC(β) >SD(β)\nB.4.3 Extraction from prepositional phrases\nThe logic of this test suite is the same as that for\nsubject and object extractions above.\n(A) I know that our uncle grabbed the food\nin front of\nα\n  \nthe guests at the holiday party.\n[THAT, NO GAP ]\n(B) * I know who our uncle grabbed the food in\nfront of\nα\n  \nthe guests at the holiday party. [WH,\nNO GAP ]\n(C) * I know that our uncle grabbed the food in\nfront of\nβ\n  \nat the holiday party. [THAT, GAP ]\n(D) I know who our uncle grabbed the food in\nfront of\nβ\n  \nat the holiday party. [WH, GAP ]\nCriterion\nSB(α) >SA(α) ∧SC(β) >SD(β)\nB.4.4 Tests for unboundedness\nFiller–gap dependencies are “unbounded” in the\nsense that there is no limit to how many clausal\nlevels above the gap the ﬁller can be extracted.\nThis serves as the basis for harder versions of the\nobject-extracted test suites, involving three or four\nlevels of clausal embedding. Example [THAT, NO\nGAP ] sentences are given below:\nI know that our mother said her friend\nremarked that the park attendant reported\nyour friend threw the plastic into the\ntrash can. [3 levels of embedding]\nI know that our mother said her friend\nremarked that the park attendant reported\nthe cop thinks your friend threw the plas-\ntic into the trash can. [4 levels of embed-\nding]\nThese base sentences give rise to 4-condition test\nsuites using the same manipulations as for the basic\nobject-extraction test suite (Section B.4.2), and the\ncriterion for success is the same.\nB.5 Main-verb/reduced-relative garden-path\ndisambiguation\nThis is one of the best-studied instances of syntactic\ngarden-pathing in the psycholinguistics literature.\nAn example 4-condition item is given below:\n(A) ! The child kicked in the chaos\nV∗\n  \nfound her way\nback home. [REDUCED , AMBIG ]\n(B) The child who was kicked in the chaos\nV∗\n  \nfound\nher way back home.\n(C) The child forgotten in the chaos\nV∗\n  \nfound her\nway back home.\n(D) The child who was forgotten in the chaos\nV∗\n  \nfound her way back home.\n1741\nCriterion Relative to the [ REDUCED , AMBIG ]\ncondition, not reducing the relative clause should\nmake V∗ less surprising, as should changing the\nparticipial verb to one that is the same form as\na simple past-tense verb. Additionally, the ef-\nfect of not reducing the relative clause on V∗ sur-\nprisal should be smaller for unambiguous particip-\nial verbs than for participial verbs:\nSA(V∗) >SB(V∗) ∧SA(V∗) >SC(V∗)∧\nSA(V∗) −SB(V∗) >SC(V∗) −SD(V∗)\nChance is somewhere below 25%.\nReferences Bever (1970); Ferreira and Clifton\n(1986); Trueswell et al. (1994); van Schijndel and\nLinzen (2018); Futrell et al. (2019)\nB.6 Negative Polarity Licensing\nThe words any and ever, in their most common\nuses, are “negative polarity items” (NPIs): they can\nonly be used in an appropriate syntactic-semantic\nenvironment—to a ﬁrst approximation, in the scope\nof negation. For example, the determiner no can li-\ncense NPIs, but its NP has to structurally command\nthe NPI. Below, A and D are acceptable, because\nno is the determiner for the subject noun managers.\nThere is no negation in C so the NPI is unlicensed\nand the sentence is unacceptable; crucially, how-\never, B is unacceptable despite the presence of no\nearlier in the sentence, because no is embedded\ninside a modiﬁer of the main-clause subject and\nthus does not command the NPI.\n(A) No managers that respected the guard have\nhad\nNPI\nany luck. [+ NEG ,–DISTRACTOR ]\n(B) * The managers that respected no guard have\nhad\nNPI\nany luck. [– NEG ,+DISTRACTOR ]\n(C) * The managers that respected the guard have\nhad\nNPI\nany luck. [– NEG ,–DISTRACTOR ]\n(D) No managers that respected no guard have\nhad\nNPI\nany luck. [+ NEG ,+DISTRACTOR ]\nIn the above test suite, the “distractor” position\nfor no is inside a subject-extracted relative clause\nmodifying the main-clause subject. We also used a\nvariant test suite in which these relative clauses are\nobject-extracted:\n(A) No managers that the guard respected have\nhad\nNPI\nany luck. [+ NEG ,–DISTRACTOR ]\n(B) * The managers that no guard respected have\nhad\nNPI\nany luck. [– NEG ,+DISTRACTOR ]\n(C) * The managers that the guard respected have\nhad\nNPI\nany luck. [– NEG ,–DISTRACTOR ]\n(D) No managers that no guard respected have\nhad\nNPI\nany luck. [+ NEG ,+DISTRACTOR ]\nThe above two test suites use any as the NPI; we\nalso use test suites with ever as the NPI. Subject-\nextracted relative clause example:\n(A) No managers that respected the guard have\nNPI\never gotten old. [+ NEG ,–DISTRACTOR ]\n(B) * The managers that respected no guard have\nNPI\never gotten old. [– NEG ,+DISTRACTOR ]\n(C) * The managers that respected the guard have\nNPI\never gotten old. [– NEG ,–DISTRACTOR ]\n(D) No managers that respected no guard have\nNPI\never gotten old. [+ NEG ,+DISTRACTOR ]\nObject-extracted relative clause example:\n(A) No managers that the guard respected have\nNPI\never gotten old. [+ NEG ,–DISTRACTOR ]\n(B) * The managers that no guard respected have\nNPI\never gotten old. [– NEG ,+DISTRACTOR ]\n(C) * The managers that the guard respected have\nNPI\never gotten old. [– NEG ,–DISTRACTOR ]\n(D) No managers that no guard respected have\nNPI\never gotten old. [+ NEG ,+DISTRACTOR ]\nCriterion Changing the main-clause subject’s\ndeterminer from The to No should increase the\nprobability of the NPI where it appears, regardless\nof whether there is a distractor no in the subject-\nmodifying relative clause. Furthermore, when there\nis exactly one no in the sentence, the NPI should be\nhigher-probability when it is in a licensing position\nrather than in a distractor position:\nPA(NPI) >PC(NPI) ∧PD(NPI) >PB(NPI)∧\nPA(NPI) >PB(NPI)\nChance is 5\n32 .\n1742\nReferences Ladusaw (1979); Vasishth et al.\n(2008); Giannakidou (2011); Marvin and Linzen\n(2018); Futrell et al. (2018)\nB.7 NP/Z garden-path ambiguity\nThis is another well-studied syntactic garden-\npathing conﬁguration. In A below, the NP the\nwaters introduces a local syntactic ambiguity: it\ncould be (1) the direct object of crossed, in which\ncase the sentence-initial subordinate clause has not\nyet ended, or (2) the subject of the main clause, in\nwhich case crossed is used intransitively and is the\nlast word of the sentence-initial subordinate clause.\n(This was dubbed “NP/Z” by Sturt et al. (1999) be-\ncause the subordinate-clause verb might have either\nan NP object or a Z(ero), i.e. null, object.) The next\nword, remained, is only compatible with (2); the\nruling out of (1) generally yields increased process-\ning difﬁculty for human comprehenders. Marking\nthe end of the subordinate clause with a comma, as\nin B, makes the sentence easier at V∗, as does an\nobligatorily intransitive subordinate-clause verb, as\nin C.\n(A) ! As the ship crossed the waters\nV∗\n  \nremained blue\nand calm. [TRANS ,NO COMMA ]\n(B) As the ship crossed, the waters\nV∗\n  \nremained\nblue and calm. [TRANS ,COMMA ]\n(C) As the ship drifted the waters\nV∗\n  \nremained blue\nand calm. [INTRANS ,NO COMMA ]\n(D) As the ship drifted, the waters\nV∗\n  \nremained blue\nand calm. [INTRANS ,COMMA ]\nCriterion Similar to the main-verb/reduced-\nrelative garden-pathing ambiguity, a model must\npass a three-part criterion. Relative to A, either\nmarking the subordinate-clause end with a comma\nor using an obligatorily intransitive verb in the sub-\nordinate clause should reduce the surprisal of V∗.\nFurthermore, the surprisal-reduction effect of the\ncomma should be smaller when the subordinate-\nclause verb is intransitive than when it is transitive:\nSA(V∗) >SB(V∗) ∧SA(V∗) >SC(V∗)∧\nSA(V∗) −SB(V∗) >SC(V∗) −SD(V∗)\nWe also use an NP/Z test suite where the sec-\nond means of disambiguation is not changing\nthe subordinate-clause verb to an intransitive, but\nrather giving the transitive subordinate-clause verb\nan overt direct object. For the above example item,\nthe ﬁrst two conditions are the same and the other\ntwo conditions would be:\n(C) As the ship crossed the sea the waters\nV∗\n  \nremained blue and calm.\n(D) As the ship crossed the sea, the waters\nV∗\n  \nremained blue and calm.\nThe success criterion remains the same.\nFinally, we create harder versions of both the\nabove test suites by adding a postmodiﬁer to the\nmain-clause subject (in the above example, the wa-\nters becomes the waters of the Atlantic Ocean).\nReferences Frazier and Rayner (1982); Mitchell\n(1987); Pickering and Traxler (1998); Sturt et al.\n(1999); Staub (2007)\nB.8 Subject–verb number agreement\nThis task tests a language model for how well it pre-\ndicts the number marking on English ﬁnite present-\ntense verbs (whether it should be the third-person\nsingular form, or the non-third-person-singular\nform, generally referred to as the plural form for\nsimplicity, although technically this is the form\nfor ﬁrst- and second-person singular as well). In\ncontrolled, targeted versions of this test, multiple\nNP precede the verb: the verb’s actual subject, as\nwell as a DISTRACTOR NP with number that is\ndifferent from that of the subject. A successful\nlanguage model should place higher probability on\nthe verbform matching that of the subject, not the\ndistractor. We have three versions of this test suite:\none where the distractor is in a prepositional phrase\npostmodiﬁer of the subject:\n(A) The farmer near the clerks knowsVsg many\npeople.\n(B) * The farmer near the clerks know Vpl many\npeople.\n(C) The farmers near the clerk know Vpl many\npeople.\n(D) * The farmers near the clerk knowsVsg many\npeople.\none in which the distractor is in a subject-extracted\nrelative clause postmodiﬁer of the subject:\n(A) The farmer that embarrassed the clerks\nknowsVsg many people.\n1743\n(B) * The farmer that embarrassed the clerks\nknowVpl many people.\n(C) The farmers that embarrassed the clerk\nknowVpl many people.\n(D) * The farmers that embarrassed the clerk\nknowsVsg many people.\nand one in which the distractor is in an object-\nextracted relative clause postmodiﬁer of the sub-\nject:\n(A) The farmer that the clerks embarrassed\nknowsVsg many people.\n(B) * The farmer that the clerks embarrassed\nknowVpl many people.\n(C) The farmers that the clerk embarrassed\nknowVpl many people.\n(D) * The farmers that the clerk embarrassed\nknowsVsg many people.\nCriterion Following Linzen et al. (2016) and\nMarvin and Linzen (2018), we require successful\ndiscrimination of the preferred upcoming verbform\nof the given lemma (rather than, for example, suc-\ncessful discrimination of the better context given a\nparticular verbform). For success we require that a\nmodel successfully predicts the preferred verbform\nfor both the singular- and plural-subject versions\nof an item:\nPA(Vsg) >PB(Vpl) ∧PC(Vpl) >PD(Vsg)\nChance performance is thus 25%, though a\ncontext-insensitive baseline that places different\nprobabilities on Vsg and Vpl would score 50%.\nReferences Bock and Miller (1991); Linzen et al.\n(2016); Marvin and Linzen (2018)\nB.9 Reﬂexive pronoun licensing\nThe noun phrase with which a reﬂexive pronoun\n(herself, himself, themselves) corefers must com-\nmand it in a sense similar to that relevant for\nnegative-polarity items (Section B.6). In the be-\nlow example, the reﬂexive pronoun ending the sen-\ntence can only corefer to the subject of the sentence,\nauthor, with which it must agree in number: a sin-\ngular subject requires a singular reﬂexive Rsg, and\na plural subject requires a plural reﬂexive Rpl.\n(A) The author next to the senators hurt\nherselfRsg.fem .\n(B) * The authors next to the senator hurt\nherselfRsg.fem .\n(C) The authors next to the senator hurt\nthemselvesRpl .\n(D) * The authors next to the senator hurt\nthemselvesRpl .\nWe generated a pair of test suites—one in which\nthe singular reﬂexive is herself, and another where\nthe singular reﬂexive is himself, on the template of\nthe above example, where the distractor NP is in\na prepositional-phrase postmodiﬁer of the subject\nNP. We also generated a similar pair of test suites\nwhere the distractor NP is inside a subject-extracted\nrelative clause modifying the subject:\n(A) The author that liked the senators hurt\nherselfRsg.fem .\n(B) * The authors that liked the senator hurt\nherselfRsg.fem .\n(C) The authors that liked the senator hurt\nthemselvesRpl .\n(D) * The authors that liked the senator hurt\nthemselvesRpl .\nand a pair of test suites where the distractor NP is\ninside an object-extracted relative clause modifying\nthe subject:\n(A) The author that the senators liked hurt\nherselfRsg.fem .\n(B) * The authors that the senator liked hurt\nherselfRsg.fem .\n(C) The authors that the senator liked hurt\nthemselvesRpl .\n(D) * The authors that the senator liked hurt\nthemselvesRpl .\nCriterion For each item in each test suite, we\nrequire that for both the singular and the plural\nversions of the reﬂexive pronoun the model assign\nhigher conditional probability in the correct licens-\ning context than in the incorrect licensing context:\nPA(Rsg) >PB(Rsg) ∧PC(Rpl) >PD(Rpl)\nChance is 25%.\nReferences Reinhart (1981); Marvin and Linzen\n(2018)\nB.10 Subordination\nBeginning a sentence with As, When, Before, After,\nor Because, implies that an immediately following\nclause is not the main clause of the sentence, as\nwould have otherwise been the case, but instead is\n1744\na SUBORDINATE CLAUSE that must be followed\nby the main clause. Ending the sentence without a\nmain clause, as in B, is problematic. Conversely,\nfollowing an initial clause with a second clauseMC\n(without linking it to the initial clause withand, but,\ndespite, or a similar coordinator or subordinator),\nas in C below, is unexpected and odd.\n(A) The minister praised the building\nEND\n\n.\n(B) *After the minister praised the building\nEND\n\n.\n(C) ?? The minister praised the\nbuilding\nMC\n  \n, it started to rain.\n(D) After the minster praised the\nbuilding\nMC\n  \n, it started to rain.\nIn addition to the base test suite exempliﬁed by the\nitem above, we include three versions with longer\nand more complex initial clauses, which may make\nthe test suite more difﬁcult. In the ﬁrst of these\nversions, we postmodify both the subject and object\nof the initial clauses with prepositional phrases:\nthe minister praised the building\n↓\nthe minister in the dark suit and white tie praised\nthe new building on the town’s main square\nIn the second of these versions, the postmodiﬁers\nare subject-extracted relative clauses:\nthe minister praised the building\n↓\nthe minister who wore a black suit praised the\nnew building that was built by the square\nIn the third of these versions, the postmodiﬁers are\nobject-extracted relative clauses:\nthe minister praised the building\n↓\nthe minister who the mayor had invited praised\nthe new building that the businessman had built\ndowntown\nCriterion Introducing a subordinator at the be-\nginning of the sentence should make an ending\nwithout a second clause less probable, and should\nmake a second clause more probable:\nPA(END) >PB(END) ∧PD(MC) <PC(MC)\nReferences Futrell et al. (2018)"
}