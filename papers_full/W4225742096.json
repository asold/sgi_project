{
  "title": "RNGDet: Road Network Graph Detection by Transformer in Aerial Images",
  "url": "https://openalex.org/W4225742096",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2165268189",
      "name": "Xu ZhenHua",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2358168312",
      "name": "Liu, YuXuan",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2108058676",
      "name": "Gan Lu",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2225931016",
      "name": "Sun Yuxiang",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2222143088",
      "name": "Wu Xinyu",
      "affiliations": [
        "Shenzhen Institutes of Advanced Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1991239782",
      "name": "Liu Ming",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2347953154",
      "name": "Wang, Lujia",
      "affiliations": [
        "Shenzhen Bay Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1486673100",
    "https://openalex.org/W73112891",
    "https://openalex.org/W1973653666",
    "https://openalex.org/W2163923520",
    "https://openalex.org/W1974097572",
    "https://openalex.org/W2304676573",
    "https://openalex.org/W2975194617",
    "https://openalex.org/W2780861787",
    "https://openalex.org/W2969786836",
    "https://openalex.org/W6801786959",
    "https://openalex.org/W2593886839",
    "https://openalex.org/W3214248441",
    "https://openalex.org/W2798925380",
    "https://openalex.org/W2981989409",
    "https://openalex.org/W3110395491",
    "https://openalex.org/W3035012694",
    "https://openalex.org/W6769091439",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6759496466",
    "https://openalex.org/W3002903653",
    "https://openalex.org/W2944451185",
    "https://openalex.org/W2907660292",
    "https://openalex.org/W2620833909",
    "https://openalex.org/W6685322675",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W2956134899",
    "https://openalex.org/W3187359482",
    "https://openalex.org/W6804134274",
    "https://openalex.org/W2798725844",
    "https://openalex.org/W2984281342",
    "https://openalex.org/W6797962455",
    "https://openalex.org/W2945957599",
    "https://openalex.org/W4213374683",
    "https://openalex.org/W3184784650",
    "https://openalex.org/W3126772615",
    "https://openalex.org/W3186475895",
    "https://openalex.org/W6640174482",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3186564193",
    "https://openalex.org/W4214549494",
    "https://openalex.org/W3101780148",
    "https://openalex.org/W6680724558",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2811199523",
    "https://openalex.org/W2928165649",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3210679603",
    "https://openalex.org/W3200935312",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2142641780",
    "https://openalex.org/W3179351458",
    "https://openalex.org/W3211873831",
    "https://openalex.org/W3100582685",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W4289029597",
    "https://openalex.org/W3009268227",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W4298175679",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W4293319870",
    "https://openalex.org/W2982362409",
    "https://openalex.org/W1931877416",
    "https://openalex.org/W4288581795"
  ],
  "abstract": "Road network graphs provide critical information for autonomous-vehicle applications, such as drivable areas that can be used for motion planning algorithms. To find road network graphs, manually annotation is usually inefficient and labor-intensive. Automatically detecting road network graphs could alleviate this issue, but existing works still have some limitations. For example, segmentation-based approaches could not ensure satisfactory topology correctness, and graph-based approaches could not present precise enough detection results. To provide a solution to these problems, we propose a novel approach based on transformer and imitation learning in this paper. In view of that high-resolution aerial images could be easily accessed all over the world nowadays, we make use of aerial images in our approach. Taken as input an aerial image, our approach iteratively generates road network graphs vertex-by-vertex. Our approach can handle complicated intersection points with various numbers of incident road segments. We evaluate our approach on a publicly available dataset. The superiority of our approach is demonstrated through the comparative experiments. Our work is accompanied with a demonstration video which is available at https://tonyxuqaq.github.io/projects/RNGDet/. IEEE",
  "full_text": "1\nRNGDet: Road Network Graph Detection by\nTransformer in Aerial Images\nZhenhua Xu, Student Member, IEEE, Yuxuan Liu, Student Member, IEEE,\nLu Gan, Student Member, IEEE, Yuxiang Sun, Member, IEEE, Xinyu Wu, Member, IEEE,\nMing Liu, Senior Member, IEEE, and Lujia Wang, Member, IEEE\nAbstractâ€”Road network graphs provide critical information for\nautonomous-vehicle applications, such as drivable areas that can\nbe used for motion planning algorithms. To ï¬nd road network\ngraphs, manually annotation is usually inefï¬cient and labor-\nintensive. Automatically detecting road network graphs could\nalleviate this issue, but existing works still have some limitations.\nFor example, segmentation-based approaches could not ensure\nsatisfactory topology correctness, and graph-based approaches\ncould not present precise enough detection results. To provide a\nsolution to these problems, we propose a novel approach based\non transformer and imitation learning in this paper. In view\nof that high-resolution aerial images could be easily accessed\nall over the world nowadays, we make use of aerial images in\nour approach. Taken as input an aerial image, our approach\niteratively generates road network graphs vertex-by-vertex. Our\napproach can handle complicated intersection points with various\nnumbers of incident road segments. We evaluate our approach\non a publicly available dataset. The superiority of our approach\nis demonstrated through the comparative experiments. Our work\nis accompanied with a demonstration video which is available at\nhttps://tonyxuqaq.github.io/projects/RNGDet/.\nIndex Termsâ€”Road Network Graph Detection, Transformer,\nImitation Learning, Aerial Images, Remote Sensing, Autonomous\nDriving.\nI. I NTRODUCTION\nI\nN recent years, road networks have attracted considerable at-\ntention in the ï¬eld of autonomous driving. The graph of road\nnetworks can provide fundamental information for autonomous-\nvehicle applications. The graph of road networks is a kind of\nvectorized data representation, which consists of vertices and\nedges [1]. Each road segment could be seen as a graph edge, and\nthe intersection points of road segments are vertices. Manually\nannotating the road network graph is time-consuming and labor-\nintensive, especially when road networks cover a large area\nThis work was supported by Zhongshan Science and Technology Bu-\nreau Fund, under project 2020AG002, Foshan-HKUST Project no. FSUST20-\nSHCIRI06C, and Guangdong Basic and Applied Basic Research Foundation\nproject no. 2020A0505090008, awarded to Prof. Ming Liu.\nZhenhua Xu, Yuxuan Liu, Lu Gan are with The Hong Kong University of\nScience and Technology (email: {zxubg,yliuhb,lganaa}@connect.ust.hk).\nYuxiang Sun is with the Department of Mechanical Engineering, The Hong\nKong Polytechnic University, Hung Hom, Kowloon, Hong Kong (e-mail:\nyx.sun@polyu.edu.hk, sun.yuxiang@outlook.com).\nXinyu Wu is with Shenzhen Institutes of Advanced Technology, CAS,\nShenzhen, China (email: xy.wu@siat.ac.cn)\nMing Liu is with The Hong Kong University of Science and Technology\n(Guangzhou), Nansha, Guangzhou, 511400, Guangdong, China, and also with\nThe Hong Kong University of Science and Technology, Hong Kong SAR, China,\nand also with HKUST Shenzhen-Hong Kong Collaborative Innovation Research\nInstitute, Futian, Shenzhen. (email: eelium@ust.hk)\nLujia Wang is with The Hong Kong University of Science and Technology,\nand also with Clear Water Bay Insitute of Autonomous Driving (Shenzhen)\n(email: eewang@ust.hk).\nCorresponding author: Lujia Wang.\n(a) Ground-truth\n (b) RNGDet\nFig. 1: A sample result of RNGDet. (a) The ground-truth road\nnetwork graph (cyan lines). (b) The graph predicted by RNGDet\n(orange lines for edges and yellow points for vertices). We can\nsee that RNGDet effectively detects the road network graph with\nhigh quality. For better visualization, the lines are drawn with\na thicker width while it is actually one-pixel width. The ï¬gure\nis best viewed in color. Please zoom in for details.\n(e.g., a whole city). Therefore, how to automatically detect road\nnetwork graphs using automatic algorithms in large areas is of\ngreat interest to the research community.\nTo address this issue, past approaches on road network graph\ndetection usually use aerial images obtained from unmanned\naerial vehicles (UA Vs) or satellites [2]. As aerial imaging tech-\nnology evolves, high-resolution and high-quality aerial images\ncan be easily accessed world-widely nowadays. Moreover, some\naerial imaging datasets also provide extra channels besides Red-\nGreen-Blue (RGB), such as the infrared channel [3], making\nthem more informative for detection purpose. So, this work also\nuses aerial images for road network graph detection.\nExisting works on road network graph detection can be\ngenerally classiï¬ed into two categories: segmentation-based\napproaches [2], [4]â€“[13] and graph-based approaches [14]â€“[18].\nThe segmentation-based approaches ï¬rst predict the probabilis-\ntic segmentation map of the road network graph, and then\nconduct a series of processing to obtain the graph structure\nof the road network, such as skeletonization and ï¬ltering.\nMost of the early works on road network graph detection\nin this ï¬eld fall into this category. The segmentation-based\napproaches could present good results in the pixel-level eval-\nuation (e.g., by F1 score) due to the use of existing powerful\nsemantic segmentation networks, but they usually suffer from\nunsatisfactory topology correctness such as incorrect crossroad\nconnectivity and false disconnection on the road. To address\narXiv:2202.07824v2  [cs.CV]  26 Jun 2022\n2\nthis issue, recent graph-based approaches resort to detecting\nthe graph of road networks directly [14], [15], [17], [18]. They\nusually ï¬rst predict candidate initial vertices, then, starting from\neach candidate initial vertex, train a decision-making agent to\npredict adjacent vertices of the current vertex. In this way, road\nnetwork graphs can be generated vertex-by-vertex in an iterative\nmanner. Although these graph-based approaches could enhance\nthe topology correctness, they are usually composed of two\nseparate stages, making them hard to be optimized in an end-\nto-end way. The separate stages might accumulate errors and\nhence degrade their effectiveness and efï¬ciency.\nTo provide a solution to these issues, in this paper, we propose\na graph-based end-to-end approach named Road Network Graph\nDetection by Transformer (RNGDet). Similar to previous graph-\nbased approaches, RNGDet starts from predicted candidate ini-\ntial vertices to extract local visual features using a convolutional\nneural network (CNN) backbone, and then sends the features\nto a transformer network inspired by the DETR structure [19].\nDue to the use of deep vertex queries, RNGDet can directly\npredict any number of adjacent vertices of the current vertex\nat one time, so that it can handle any road networks, even\nthose with complicated topology (e.g., road intersections of\narbitrary numbers of road segments). Different from previous\ngraph-based approaches, RNGDet can be optimized as a whole\nand trained end-to-end. We train RNGDet through imitation\nlearning to enable it to take the most appropriate action under\ndifferent circumstances. To generate the training data (i.e.,\nexpert demonstration from the imitation learning perspective),\nwe propose a sampling algorithm to supervise the agent to\nexplore the whole road network. The proposed RNGDet is\ntrained and evaluated on a publicly available dataset released\nby RoadTracer [14]. With this dataset, we compare RNGDet\nwith state-of-the-art works based on multiple evaluation metric\nscores. An example of RNGDet is visualized in Fig. 1. The\ncontributions of our work are summarized below:\nâ€¢ We propose an end-to-end trainable approach named\nRNGDet based on transformer and imitation learning to\nautomatically detect the road network graph.\nâ€¢ We propose an algorithm to automatically generate training\nsamples for RNGDet.\nâ€¢ We evaluate RNGDet and compare it with state-of-the-art\nworks on a publicly available dataset to demonstrate the\nsuperiority of RNGDet.\nThe remainder of this article is organized as follows. Section\nII introduces related works. Section III describes the structure\nand working pipeline of RNGDet. Section IV presents exper-\nimental results, discussions, and limitations. Conclusions and\nfuture work are drawn in the last section.\nII. R ELATED WORKS\nA. Segmentation-based approaches\nSegmentation-based road network graph detection approaches\n[2], [4]â€“[13], [20]â€“[22] mainly have two stages: (1) predict\nthe segmentation map (i.e., probabilistic map of the road\nnetwork) and (2) process the segmentation map and obtain\ngraph structures by post-processing, such as skeletonization and\nheuristic-based algorithms [23], [24]. It is believed that [2]\nproposed by Mnih et al. is the ï¬rst work that implemented\nneural networks to detect the road network in aerial images.\nThey ï¬rst split the large aerial image into small patches, then\npredicted the road network within each patch and ï¬nally merged\npatches into the ï¬nal predicted road network segmentation\nmap. Most later segmentation-based works followed a similar\npipeline, but with more powerful segmentation networks, such\nas U-Net [25], DeepLab V3+ [26], FPN [27] and HRNet\n[28]. Batra et al. [8] extended the aforementioned works by\nadding another reï¬nement network to ï¬x incorrect pixels in\nthe predicted segmentation map, which effectively improved the\nï¬nal performance. In [11], Gedara et al. proposed a Spatial\nand Interaction Space Graph Reasoning (SPIN) module, which\nperformed reasoning over graphs constructed on spatial and\ninteraction spaces projected from the feature maps. In this way,\nspatial and topology information are better utilized to improve\nroad detection performance. Generative adversarial network\n(GAN) is also a popular tool for map generation tasks [20]â€“\n[22], but usually, these works output maps in raster format,\nsuch as styled map tiles. Thus, post-processing is still needed\nto vectorize maps.\nAfter obtaining the probabilistic map of a road network, post-\nprocessing should be conducted to extract the road centerline,\nï¬lter out outlier road segments and ï¬x incorrect disconnections\nof the road network. To better extract the road-network graph,\nin [24], Bulatov et al. proposed a similarity criteria to fuse raw\nroad segments into chains and designed a two-stage algorithm\nto optimize the obtained road chains. This work can handle\ncurve roads and sharp circle arcs well. Wenzel et al. [23]\nfurther enhanced [24] to generate longer and more accurate road\nnetwork chains by proposing an iterative greedy optimization\nprocedure.\nSince semantic segmentation only works on pixel-level pre-\ndictions, the topology information is not effectively considered.\nThus, the road network graph obtained by segmentation-based\napproaches tends to have poor topology correctness. Moreover,\nhandcrafted or heuristic post-processing algorithms cannot ef-\nfectively correct errors in the obtained road network graph.\nB. Graph-based approaches\nDifferent from segmentation-based approaches, graph-based\napproaches for road network graph detection can directly output\nthe graph structure. Most of the graph-based works detect the\nroad network graph by iterative graph generation [14]â€“[18].\nRoadTracer [14] is believed to be the ï¬rst work that iteratively\ngenerates the road network graph. In this work, ground-truth\ninitial vertices were used to initialize the iteration. From each\ninitial vertex, RoadTracer predicted the direction of the adjacent\nvertices of the current vertex as a multi-class classiï¬cation\nproblem, then moved the agent in the predicted direction by\na ï¬xed length. RoadTracer presented much better topology\nperformance than past segmentation-based approaches, but it\nfailed to detect road intersections with high quality due to the\nï¬xed step length. Tan et al. [17] solved this problem by re-\nplacing the direction prediction with heatmap prediction, where\nheatmap demonstrated the probabilistic distribution of adjacent\nvertices of the current vertex. After obtaining the heatmap, the\nauthors extracted local peaks as the predicted adjacent vertices.\nAlthough this approach had dynamic step length and presented\n3\nsuperior performance to past works, it can not be optimized in\nan end-to-end way due to the post-processing of the heatmap,\nwhich degrades the ï¬nal performance. In addition, this work\ncannot distinguish vertices that are close to each other.\nDifferent from aforementioned iterative graph-based ap-\nproaches, Song et al. [16] proposed Sat2Graph to directly\npredict the road network graph. Taken as input an aerial image,\nSat2Graph predicted a 19 dimension tensor. This high dimension\ntensor contained all the information of the road network graph\nso that the graph could be decoded from the feature tensor by\nalgorithms proposed in Sat2Graph [16]. However, Sat2Graph\nhad the isomorphic encoding issue analyzed in [16], which\nmade it difï¬cult to supervise during training. Moreover, it cannot\ndistinguish road segments whose intersection angle is small.\nC. Graph detection of objects similar to road networks\nThere are some tasks that are similar to road network de-\ntection, such as the detection of road boundaries [29]â€“[31],\nroad lane lines [32]â€“[35], road lane [36], [37] and road curbs\n[38], [39]. Even though these works do not work on road\nnetwork detection, their tasks are similar to ours and some\nideas or techniques are also inspiring to us. Xu et al. [38] ï¬rst\nproposed to analyze the graph detection problem using imitation\nlearning, and designed a DAgger-based system for road curb\ndetection following the DAgger algorithm [40]. Homayounfar et\nal. proposed DagMapper [33] to detect road lane line graph in\nthe point cloud map on the highway. DagMapper can predict the\ndirection of adjacent vertices of the current vertex, and whether\nthe agent should create a new lane line branch when lane line\nintersections were encountered. Although these works could be\ninspiring for our task, they cannot handle the road network graph\ndetection task since road networks have much more complicated\ntopology (e.g., road split, road merge and crossroads).\nD. Transformer-based detection\nIn recent years, transformer [41] has been receiving more\nand more attention since its powerful parallelization capacity\nand great ability to handle sequential tasks. Considering these\nproperties, Carion et al. [19] proposed Detection by Transformer\n(DETR) for one-shot 2D object detection, which is anchor free\nand can be trained in an end-to-end way. After extracting image\nfeatures by a CNN backbone, DETR sent the obtained features\nas well as multiple object queries to a transformer, and then ob-\ntained the bounding box coordinates of objects. Each bounding\nbox was encoded by a 4D embedding. Therefore, taken as input\nan image, DETR can directly output the coordinates of object\nbounding boxes. Xu et al. [42] adapted DETR to line segment\ndetection task and named the new model as Line Segment\nDetection by Transformer (LETR). In this paper, the authors\nencoded each line segment as an embedding and predicted the\nencoding embedding by a DETR-based network. Similarly, Can\net al. [43] modiï¬ed the DETR network and pursued to detect\nlane centerlines. Each lane centerline was ï¬tted by a B-spline\nand each B-spline was encoded by an embedding. In this way,\nthe authors could directly detect all lane centerlines at one time\nby predicting the embedding that encoded B-spline information.\nOur work RNGDet is also inspired by DETR, while the output\nembedding encodes the information of adjacent vertices of the\ncurrent vertex.\nE. Imitation learning\nImitation learning aims to train a decision-making agent\nnetwork to mimic an expert. The most commonly used imitation\nlearning algorithm is behavior cloning [44]. In behavior cloning,\nan expert generates a set of demonstrations and the agent tries\nto learn the policy of the expert. This algorithm is efï¬cient but\nit suffers from the drifting problem of imitation learning [45].\nRoss et al. proposed a meta-algorithm called Dataset Aggrega-\ntion (DAgger) [40] which could cover a much larger state space\nto relieve the drifting problem. However, the DAgger algorithm\npresents quite a low sampling efï¬ciency. Based on DAgger, Xu\net al. designed approaches to detect road elements by imitation\nlearning in [30], [38]. In this paper, since behavior cloning can\nalready achieve satisfactory performance, considering the low-\nefï¬ciency performance of DAgger, we adopt behavior cloning\nin our experiments.\nIII. T HE PROPOSED APPROACH\nA. Approach overview\nThis work aims to detect the road network graph from aerial\nimages, and the road network graph can be used for real-world\napplications (e.g., autonomous vehicle navigation). Suppose the\ninput is a large aerial image IA, then the ï¬nal output should be\na graph G= (V,E). E = {ei}is a set of graph edges and each\nedge ei represents a road segment. V = {vj}is a set of graph\nvertices, and each vertex vj is either one intersection point of\nsome road segments or the endpoint of a broken road.\nBased on the DETR structure, our proposed RNGDet detects\nthe road network graph by iterations. Starting from a pre-\npredicted candidate initial vertex in C = {ck}K\nk=1, RNGDet\niteratively generates the road network graph by controlling an\nagent exploring the road network. During iterations, the history\ntrajectory of the agent is recorded by the graph G. At each\nstep, centering at the current position of the agent vt, RNGDet\ncrops an ROI âˆˆR3Ã—LÃ—L on IA and rasterizes G within the\nROI as Hâˆˆ R1Ã—LÃ—L. A CNN backbone network is utilized\nto extract the deep visual feature of the ROI as FI which is\nsent to the segmentation heads to predict the road segment\nsegmentation map Sand the road intersection segmentation map\nI. The candidate initial vertices in C can be obtained by ï¬nding\nlocal peaks of I. After concatenating Hwith Sand I, another\nCNN backbone network is used to extract the deep feature FH.\nThen, FH and FI are fused as the input feature tensor of the\ntransformer.\nTaking N vertex queries Q = {qi}N\ni=1 as input, the trans-\nformer decoder directly predicts N vertex embeddings encoding\nthe valid probability and coordinates of vertices. Each vertex\nquery is a learned embedding, which could be treated as a slot\nused by RNGDet to make the prediction of one vertex. Thus\nthe number of vertex queries N must be larger than the largest\nnumber of vertices needed to be predicted. After ï¬ltering out\nvertices with low probability, we obtain M(M â‰¤ N) valid\nvertices that are adjacent to vt as a set V = {vi\nt+1}M\ni=1. If\nM = 0, RNGDet pops a new candidate initial vertex from C\nand repeats the above process; if M = 1, the agent moves to the\npredicted coordinate and repeats the above process; if M >1,\nthe agent pushes all the predicted vertices into set C, pops one\nvertex from set C and then repeats the above process. When\n4\nRasterized past \ntrajectory     \nCNN backbones\nPositional encoding\nTransformer\nEncoder\nInput aerial image ğ¼ğ´\n4096Ã—4096Ã—3\nTransformer\nDecoder\nVertex \nqueries â€¦\nFFN\nFFN\nFFN\nFFN\nğ‘£ğ‘¡+1\n1\nğ‘£ğ‘¡+1\n2\nğ‘£ğ‘¡+1\n3\nğ‘£ğ‘¡+1\nğ‘\nâ€¦\nğ‘£ğ‘¡+1\n1\nğ‘£ğ‘¡+1\n2\nğ‘£ğ‘¡+1\nğ‘€\nâ€¦\nValid\nvertices\nFilter\nCropped ROI \nğ¿Ã—ğ¿Ã—3\nğ‘£ğ‘¡\n: segmentation of road segments\n: segmentation of road intersections\n: concatenated deep features\nGenerated graph G\n: new vertices\n: new edges\n: past edges\n ğ’—ğ’•+ğŸ\nğŸ\nğ’—ğ’•+ğŸ\nğŸ\nğ’—ğ’•+ğŸ\nğŸ‘\n: current vertex\nUpdated graph\nâ€¦\nFig. 2: The system overview of RNGDet. The ï¬gure shows a single step in the graph detection process of RNGDet. In this\nexample, RNGDet processes a road intersection. Suppose the agent is at vt at the current time step (denoted by a yellow node\nin H), we crop an ROI on the input aerial image IA and rasterize the graph generated so far G within the ROI as H. A CNN\nbackbone network predicts the feature FI of the ROI. Then, FI is sent to the segmentation heads to predict Sand I. Taken as\ninput S, Iand H, another CNN network predicts the deep feature tensor FH and concatenates it with FI as the ï¬nal feature\ntensor F. The transformer predicts the adjacent vertices V= {vi\nt+1}M\ni=1 of vt. In the updated graph of this example, there are\nthree predicted adjacent vertices {vi\nt+1}3\ni=1 (yellow nodes), and they are connected with the current vertex vt (blue node) by three\nnew edges (orange lines). The graph G is generated in this way iteratively. This ï¬gure is best viewed in color. Please zoom in\nfor details.\nC is empty, RNGDet stops and outputs the ï¬nal road network\ngraph. The overall pseudocode of our system is shown in Alg.\n1. The system diagram of RNGDet is displayed in Fig. 2.\nB. CNN backbone and segmentation\nThe input of RNGDet is a large RGB aerial image IA âˆˆ\nR3Ã—4096Ã—4096. Since RNGDet requires the history information\nfor iterative graph generation, we maintain the graph Grecord-\ning the past trajectory of the agent. Due to the size of input\nimages, RNGDet crops a ROI âˆˆ R3Ã—LÃ—L (L is 256 in our\nexperiment) on IA. A multi-layer convolutional neural network\n(CNN) is utilized to extract the deep feature of the ROI as FI,\nand the CNN backbone in this paper is ResNet [46]. Based on\nthe extracted deep feature FI, two feature paradigm network\n(FPN) [27] segmentation heads predict the segmentation map S\nand I, where Sdemonstrates the distribution of road segments\nand Ishows the distribution of road intersection points as well\nas endpoints of broken roads. Both segmentation tasks are binary\nsegmentation.\nTo obtain the information of past trajectories, we crop G in\nthe same way as cropping the ROI and rasterize the cropped G\nas Hâˆˆ R1Ã—LÃ—L for afterward concatenation. With S, Iand\nHas input, another CNN network outputs the feature tensor\nFH. Two feature tensors are concatenated together as the ï¬nal\nfeature tensor F, containing all the information required by the\ntransformer.\nC. Transformer architecture\nAfter obtaining the deep feature tensor F, the transformer\npredicts the adjacent vertices {vi\nt+1}M\ni=1 of the current vertex vt.\nAlgorithm 1: RNGDet\nInput: An aerial image IA\nOutput: The road network graph G= (V,E)\n1 begin\n2 C â†find initial vertex(IA)\n3 Initialize G as an empty graph\n4 while C not empty do\n5 tâ†0\n6 vt â†C.pop()\n7 while true do\n8 Fâ† CNN(IA,G,v t)\n9 {Ë†vi\nt+1}N\ni=1 â†Transformer (F|Q)\n10 Vâ† filter({Ë†vi\nt+1}N\ni=1)\n11 if |V|= 0 then\n12 break\n13 else if |V|= 1 then\n14 vt â†Ë†v1\nt+1\n15 tâ†t+ 1\n16 update G\n17 else if |V|>1 then\n18 C â†CâˆªV\n19 update G\n20 break\n21 end\n22 end\n23 return G\n24 end\n5\nThe feature tensor Fis reduced to a sequence of smaller tensors,\nand then ï¬xed positional encoding [47], [48] is fused due\nto the permutation-invariant characteristics of the transformer\narchitecture [19]. The input and output of the transformer\nencoder have the same length.\nThe decoder of the transformer takes in the output sequence\nof the encoder as well as a set of vertex queries Q= {qi}N\ni=1,\nand predicts N embeddings of the adjacent vertices. Each vertex\nquery qi is a learned embedding and produces one predicted\nadjacent vertex. In fact, the transformer outputs the Maximum-\na-Posterior (MAP) estimation of vertices at the next time step\n{argmax\nvi\nt+1\np[vi\nt+1|F,vt,qi]}N\ni=1, (1)\nEach output embedding can be decoded into a valid proba-\nbility pi\nt+1 and a 2D vertex coordinate vi\nt+1 by feed-forward\nnetworks (FFN). pi\nt+1 demonstrates the probability that vi\nt+1\nis valid and should be added into the road network graph G.\nSuppose we have M(M â‰¤N) valid predicted vertices as a set\nV= {vi\nt+1}M\ni=1, then for each valid predicted vertex vi\nt+1, we\nupdate the road network graph G by adding vi\nt+1 into V and\na new edge connecting vi\nt+1 with vt into E. The transformer\narchitecture is visualized in Fig. 3.\n(CNN Feature Tensor)\nBÃ—256Ã—8Ã—8\nâ€¦\nEncoder\nMulti-Head Self-\nattention\nAdd & Norm\nFeed Forward \nNetwork (FFN)\nAdd & Norm\nâ€¦\n6Ã—\nOutput\n64Ã—BÃ—256\nâ€¦\nVertex queries (Learned Embeddings)\nNÃ—256\nMulti-Head Self-\nattention\nAdd & Norm\nMulti-Head \nAttention\nAdd & Norm\n6Ã—\nFeed Forward \nNetwork (FFN)\nAdd & Norm\nFFN FFN\nValid \nProbability ( ğ‘ğ‘¡+1\nğ‘– )\nNÃ—2\nVertex \nCoordinates ( ğ‘£ğ‘¡+1\nğ‘– )\nNÃ—2\nEncoder Input Sequence\n64Ã—BÃ—256\nPositional \nEncoding\nBÃ—256Ã—8Ã—8\nDecoder\nFig. 3: The architecture of the transformer in RNGDet. In this\nï¬gure, B represents batch size, and N represents the number\nof vertex queries. Taken as input the predicted CNN feature\ntensor F, the transformer ï¬rst splits F into a sequence of\nsmaller tensors. In our experiment, the BÃ—256 Ã—8 Ã—8-sized\nfeature tensor is split into a 64-length sequence, which contains\nBÃ—256-sized tensors. Then, the transformer encoder processes\nthe input tensors and outputs a sequence of feature tensors\nwhose shape is the same as that of the input. Finally, based on\nthe encoder output and vertex queries, the transformer decoder\npredicts N vertices {vi\nt+1}N\ni=1, and the valid probability pi\nt+1 of\neach vertex vi\nt+1. Vertices with high enough pi\nt+1 are regarded\nas valid vertices and used for graph updates.\nRNGDet\nğ‘£ğ‘¡+1\n1\nğ‘£ğ‘¡+1\n2\nğ‘£ğ‘¡+1\nğ‘€\nâ€¦\nValid vertices\nğ’—ğ’•\nM?\nM=0\nM=1\nM>1\nUpdate ğ‘£ğ‘¡\nby ğ‘£ğ‘¡+1\n1\nPush all \nğ‘£ğ‘¡+1\nğ‘– into C\nOutput road \nnetwork graph\nCandidate initial \nvertex set C|C|=0? Find local peaks \nof  I  as C\nPop ğ‘£ğ‘¡\nfrom C\nY\nN\nFig. 4: The pipeline of road network graph generation by\nRNGDet. RNGDet iteratively generates the road network graph\nvertex-by-vertex. According to the number of valid vertices at\neach step, RNGDet takes different actions to update the graph.\nPlease zoom in for details.\n(a) Step t\n (b) S\n (c) I\n (d) Step t + 1\nFig. 5: Visualization of graph updating. Each row represents an\nexample that shows how RNGDet iteratively generates the road\nnetwork graph. (a) ROI of the current step t. The cyan lines\nare the ground-truth road network, the pink lines demonstrate\nhistory graph and the blue node is vt. (b) The segmentation map\nof road segments (i.e., S). (c) The segmentation map of road\nintersections (i.e., I). (d) Updated graph at the next time step\nt+ 1. Yellow nodes are valid predicted adjacent vertices vi\nt+1\nwhile red nodes are invalid ones. Valid vertices will be used to\nupdate the road network graph. Orange lines are the predicted\nnew edges connecting vt with vi\nt+1. This ï¬gure is best viewed\nin color. Please zoom in for details.\nD. Policy for graph generation during inference period\nDuring the inference period, RNGDet generates the road\nnetwork graph by iterations. First, we initialize the candidate\ninitial vertex set Cby ï¬nding local peaks of the road intersection\nsegmentation map I. Then, RNGDet pops one vertex vt from\nthe set C. Centering at vt, RNGDet crops input images and\npredicts valid vertices at the next time step as V. Based on\nthe number of valid vertices |V|= M, RNGDet takes different\nactions to update the road network graph: (1) M = 0. It means\n6\nGraph\nsimplification\nFig. 6: Visualization of the simpliï¬cation of the ground-truth\nroad network graph. In the raw ground-truth graph, there are\nvarious kinds of vertices with different degrees (yellow nodes\nin the left subï¬gure). For training label calculation, we remove\nvertices whose degrees are two. After ï¬ltering, only intersection\nvertices (degrees larger than 2) and endpoint vertices (degrees\nare 1) remain (pink nodes in the right subï¬gure). The road\nconnecting two adjacent vertices is deï¬ned as a road segment.\nThe agent must ï¬nish exploring a road segment before switching\nto other road segments. In this way, we can guarantee the\ncorrectness of automatically generated labels. This ï¬gure is best\nviewed in color.\nthere is no road ahead, thus RNGDet should stop processing the\ncurrent road and turns to work on other roads if C is not empty.\n(2) M = 1. This happens when RNGDet travels along a single\nroad. RNGDet adds v1\nt+1 into V and edge (vt,v1\nt+1) into E,\nand then moves to v1\nt+1. RNGDet keeps updating the graph in\nthis way until intersections or broken roads are met. (3) M >1.\nThis indicates that RNGDet encounters road intersections and\nneeds to generate new vertices in multiple directions. RNGDet\nwill update the graph, push all vertices in Vto C, and pop one\ncandidate initial vertex from C.\nRNGDet keeps running the above iterations to generate the\nroad network graph vertex-by-vertex. If and only if the candidate\ninitial vertex set C is empty, RNGDet stops and outputs the\ngenerated road network graph G= (V,E). The pipeline of the\nroad network graph generation process of RNGDet is visualized\nin Fig. 4. Some example visualizations are shown in Fig. 5.\nE. Training label calculation\nBased on the ground-truth road network graph and the current\nlocation of the agent vt, we can automatically generate the\ntraining label for RNGDet (i.e., Sâˆ—,Iâˆ—,Vâˆ—= {(vi\nt+1)âˆ—}M\ni=1). The\nsegmentation labels can be simply cropped from the ground-\ntruth segmentation masks centering at vt. For Vâˆ—, we need to\nobtain coordinates of the ground-truth vertices at the next time\nstep. To achieve that, we (1) simplify the ground-truth road\nnetwork graph by removing vertices whose degree is 2 and (2)\ngenerate the label vertices at the next time step based on the\nground-truth road network graph.\nThe raw ground-truth graph consists of various kinds of\nvertices, such as endpoint vertices (degrees are 1), vertices in\nthe middle of roads (degrees are 2) and intersection vertices\n(degrees are larger than 2). Among them, vertices whose degrees\nare 2 are not uniquely deï¬ned and could be removed without\nharming the road network topology. Therefore, for simplicity,\n(a)\n (b)\n (c)\n (d)\nFig. 7: Examples of training label generation. Cyan lines are\nground-truth road network, pink lines are history road network,\nblue nodes represents vt and (vi\nt+1)âˆ— is denoted by yellow\nnodes. The radiant of the orange circle in (a)-(c) is Ï„ while\nthe radiant of the orange circle in (d) is Ï„â€². Ï„â€² is smaller\nthan Ï„ in our experiment. (a) Road-segment-mode. The road\nahead is straight. (b) Road-segment-mode. The road ahead has\nturning points with large enough curvature. (c) Road-segment-\nmode. The agent should connect vt with a previously generated\ncandidate initial vertex. (d) Intersection-mode. There are three\nnew road segments that are incident to the current intersection\npoint. This ï¬gure is best viewed in color.\nwe remove these vertices from the ground-truth graph. This pro-\ncess is visualized in Fig. 6. After the graph simpliï¬cation, there\nare only endpoint vertices and intersection vertices remaining.\nTo facilitate the calculation of labels, we deï¬ne every road\nconnecting adjacent vertices as a road segment. The agent will\nbe either in road-segment-mode or intersection-mode. The road-\nsegment-mode indicates that the agent is currently traveling\nalong a road segment, and there will be only one ground-truth\nvalid vertex at each step (i.e., Vâˆ—= {(v1\nt+1)âˆ—}). Three examples\nare visualized in subï¬gure (a), (b) and (c) of Fig. 7. To prevent\nthe agent from being trapped in an inï¬nite loop, the training\nlabel should encourage the agent to move forward (i.e., (v1\nt+1)âˆ—\nshould be far enough from vt and move to the unexplored\npart of the current road segment). If the road ahead is straight\n(i.e., no turning points with large curvature within Ï„ distance\nto vt), we select the point whose distance is Ï„ away from vt\nas (v1\nt+1)âˆ— (subï¬gure (a) of Fig. 7). If there are some turning\npoints ahead, we ï¬nd the turning point that is closest to vt as\n(v1\nt+1)âˆ— (subï¬gure (b) of Fig. 7). If there is some candidate\ninitial vertices within Ï„ distance to vt, the closest candidate\ninitial vertex is treated as (v1\nt+1)âˆ—(subï¬gure (c) of Fig. 7). Note\nthat the above algorithm is only utilized to generate the training\nlabel and does not affect the graph updating process during\ninference.\nWhen the agent ï¬nishes exploring the current road segment,\nit switches to the intersection-mode and ï¬nds incident road seg-\nments. For each incident road segment, the point whose distance\nis Ï„â€²away from vt is deï¬ned as the label vertex (subï¬gure (d) of\nFig. 7). Usually there will be multiple (vi\nt+1)âˆ—when RNGDet is\nin the intersection-mode (i.e., Vâˆ—= {(vi\nt+1)âˆ—}M\ni=1 and M >1).\nF . Loss functions\nAt each step, RNGDet predicts two segmentation maps as\nwell as coordinates and valid probability of vertices at the next\nstep. Thus, in our experiment three loss functions are utilized\nto train RNGDet. Suppose the predicted segmentation maps are\n7\nË†Sand Ë†Iwhile the ground-truth segmentation masks are Sâˆ—and\nIâˆ—, then we have\nLseg = L( Ë†S,Sâˆ—) + L(Ë†I,Iâˆ—), (2)\nwhere L is focal loss [49]. Similarly, suppose the ground-truth\nvertices at the next step t+ 1 are {vâˆ—\nj}M\nj=1 and the predictions\nare {Ë†vi}N\ni=1, where M â‰¤N. They can be matched by solving\na bipartite matching problem through minimizing the following\nfunction:\nË†Ïƒ= argmin\nÏƒ\nNâˆ‘\ni\nLmatch(Ë†vi,vâˆ—\nÏƒ), (3)\nwhere Ïƒ is the index of vâˆ— matched with Ë†vi, and Lmatch cal-\nculates pair wise Euclidean distance. After the vertex matching,\nwe have the L1 loss as\nLcoord = 1\nN\nNâˆ‘\ni\n|Ë†vi âˆ’vâˆ—\nÏƒ|. (4)\nRNGDet also predicts the valid probability Ë†pi of each Ë†vi, and\nonly vertices with high enough Ë†pi will be used to update the\nroad network graph. The ground-truth value of Ë†pi is 1 if Ë†vi is\nmatched with a vâˆ—\nÏƒ, otherwise the ground-truth value of Ë†pi is 0\n(i.e., Ë†vi does not matched with any vâˆ—). Binary cross entropy\nloss is utilized to optimize Ë†pi:\nLvalid = BCELoss( Ë†P,Pâˆ—), (5)\nwhere Ë†P = {Ë†pi}M\ni=1 and Pâˆ— = {pâˆ—\ni}M\ni=1. The ï¬nal loss\nfunction training RNGDet is the weighted summation of the\naforementioned loss functions:\nL= Lseg + Î±Lcoord + Î²Lvalid, (6)\nwhere, Î± and Î² balance the loss function. We have Î±= 5 and\nÎ² = 1 in our experiments.\nG. Training data sampling\nTo obtain the training dataset for RNGDet, we propose a\nbehavior-cloning-based algorithm to explore the ground-truth\nroad network graph to generate training samples. Suppose the\nagent is now at vt, we then crop the ROI on IA, crop Sâˆ— on\nthe ground-truth road segment segmentation mask, crop Iâˆ— on\nthe ground-truth road intersection segmentation mask, crop and\nrasterize G for H, and calculate the ground-truth vertices as\nwell as valid probability at the next time step (i.e., Vâˆ— and\nPâˆ—) based on aforementioned approaches. Then, one training\nsample (ROI, H,Sâˆ—,Iâˆ—,Vâˆ—,Pâˆ—) is obtained. If the agent is in\nthe road-segment-mode and the ground-truth vertex at the next\ntime step (v1\nt+1)âˆ— is an intersection point, we use (v1\nt+1)âˆ— to\nupdate the graph (i.e., the agent moves to (v1\nt+1)âˆ—); otherwise\nwe add (vi\nt+1)âˆ— with Gaussian noises âˆ† to update the graph\n(i.e., the agent moves to (vi\nt+1)âˆ—+ âˆ†). The agent is driven to\nexplore road networks by this behavior-cloning-based sampling\nalgorithm, and the generated samples are used to train RNGDet.\nIV. E XPERIMENTAL RESULTS AND DISCUSSIONS\nA. Dataset\nIn this paper, all the experiments are conducted on the Road-\nTracer dataset [14]. The dataset contains 300 high-resolution\naerial images (60cm/pixel) obtained from Google map, and the\nground-truth road network graphs are from the OpenStreetMap\n(OSM). All the data has been converted to the image coordinate\nsystem. This dataset covers 40 cities (e.g., Los Angeles and\nBoston). Each aerial image has 3 channels and is 4096 Ã—4096-\nsized. A city may be composed of multiple aerial images.\nB. Implementation\nTo obtain the dataset to train RNGDet, we run the pro-\nposed data sampling algorithm to explore the ground-truth road\nnetwork graph of the training aerial images and generate the\ntraining samples. At each step, Gaussian noise is added for\ngraph updating in order to make RNGDet more robust. Finally,\nthe training set contains around 300K samples from different\naerial images. We split 10K samples from the training set as\nthe validation data.\nIn our experiment, we set the crop size of ROI as 256 (i.e.,\nL = 256 ) to trade-off between effectiveness and efï¬ciency.\nWhen we generate the training labels, Ï„ is set as 40 pixels\nand Ï„â€² is set as 20 pixels. For the transformer, the number of\ninput vertex queries is 10 (i.e., |Q|= N = 10 ). RNGDet is\ntrained with a learning rate as 10âˆ’4 and a decay rate as 10âˆ’5\nfor 50 epochs. We evaluate the performance of RNGDet on the\nvalidation set at the end of each epoch. All the experiments are\nconducted on 4 RTX-3090 GPUs.\nC. Baselines\nWe compare our proposed RNGDet with two segmentation-\nbased approaches and three graph-based approaches.\nâ€¢ ImprovedRoad [8] (CVPR 2019): ImprovedRoad is one of\nthe state-of-the-art segmentation-based approaches in the\npast. Orientation information is used to enhance the road\nsegmentation, and it trains an extra reï¬ne network to ï¬x\nincorrect road segmentation predictions.\nâ€¢ SPIN RoadMapper [11] (ICRA 2022): Based on Improve-\ndRoad, SPIN RoadMapper proposes a graph reasoning\nscheme to further capture spatial information of the aerial\nimage. ImprovedRoad and SPIN RoadMapper are trained\nfor 120 epochs. These two approaches usually suffer from\npoor topology correctness.\nâ€¢ RoadTracer [14] (CVPR 2018): RoadTracer is believed to\nbe the ï¬rst graph-based approach. It predicts the directions\nof the vertices at the next step as a multi-class classiï¬cation\nproblem. However, it has a ï¬xed step size and the train-\ning label generation algorithm may produce inappropriate\nlabels.\nâ€¢ VecRoad [17] (CVPR 2020): VecRoad is an improved\nversion of RoadTracer, and is the state-of-the-art graph-\nbased approach. It predicts the distribution of the vertices\nat the next step, which allows ï¬‚exible step size. But it is\nstill two-stage and cannot be optimized in an end-to-end\nmanner. Moreover, it may not be able to distinguish vertices\nthat are close to each other.\n8\nâ€¢ Sat2Graph [16] (ECCV 2020): Sat2Graph proposes a new\ngraph encoding scheme and designs deep neural networks\nto predict the graph encoding of the input image. The\npredicted graph encoding can be decoded into road network\ngraphs with satisfactory accuracy. However, it suffers from\nthe isomorphic encoding issue which is analyzed in [16],\nand it has relatively inferior performance when arc roads\nare encountered please refer to the fourth row of Fig. 8 as\nan example).\nD. Evaluation metrics\nIn our experiments, we use three metrics pixel-precision\n(P-P), pixel-recall (P-R) and pixel-F1-score (P-F) for pixel-\nlevel evaluation, three metrics intersection-precision (I-P),\nintersection-recall (I-R) and intersection-F1-score (I-F) for in-\ntersection point evaluation and one metric average path length\nsimilarity (APLS) [50] for topology correctness evaluation.\nTo calculate the pixel-level metric scores, we rasterize the\nground-truth graph and the predicted graph as binary images\nBâˆ— and Ë†B, respectively. For a pixel in Bâˆ—, if there exists a\npixel in Ë†B and the Euclidean distance between them is smaller\nthan Î´, then this pixel is treated as correctly retrieved. Similarly,\nif a pixel in Ë†B can ï¬nd a pixel in Bâˆ— within Î´ distance, then\nwe say this pixel is correctly detected. In this way, P-P, P-R and\nP-F can be obtained by the following equations:\nP-P = |{p|âˆ¥p,qâˆ¥<Î´, âˆƒqâˆˆBâˆ—,âˆ€pâˆˆ Ë†B}|\n|Ë†B|\n,\nP-R = |{p|âˆ¥p,qâˆ¥<Î´, âˆƒqâˆˆ Ë†B,âˆ€pâˆˆBâˆ—}|\n|Bâˆ—| ,\nP-F = 2P-P Â·P-R\nP-P + P-R,\n(7)\nwhere âˆ¥Â·âˆ¥ calculates the Euclidean distance and |Â·| is the\ncardinality of a set. Threshold Î´ measures the level of error\ntolerance, and we show the metric scores with Î´ as 2, 5 and 10\npixels for more comprehensive evaluation.\nI-P, I-R and I-F are calculated in a similar way as the\naforementioned pixels-level metrics. The only difference is that\ninstead of rasterizing the whole graph into binary images Bâˆ—\nand Ë†B, these three metrics only care about intersection points\n(i.e., rasterized binary images only contain intersection points).\nThese three metrics evaluate the ability of approaches to detect\nroad intersections. We also show these metric scores in our\nexperimental results with different Î´.\nAPLS measures the similarity of the ground-truth graph Gâˆ—\nand the predicted graph Ë†G. It samples multiple vertex pairs on\nboth graphs and compares the difference between the shortest\ndistance of vertex pairs. APLS is calculated by the following\nequation:\nAPLS = 1 âˆ’ 1\nNs\nNsâˆ‘\nmin\n(\n1,|L(a,b) âˆ’L(aâ€²,bâ€²)|\nL(a,b)\n)\n, (8)\nwhere (a,b) demonstrates a vertex pair sampled from Gâˆ— and\n(aâ€²,bâ€²) is the corresponding vertex pair sampled from Ë†G. The\nnumber of sampled vertex pairs is denoted by Ns, and L(Â·,Â·)\ncalculates the length of the shortest path between two vertices.\nHigher APLS indicates better graph similarity and topology\ncorrectness.\nE. Comparative results\nRNGDet is compared with ï¬ve baseline approaches, includ-\ning two segmentation-based baselines and three graph-based\nbaselines. The quantitative comparison results are shown in\nTab. I. In Tab. I, besides baseline approaches, we also evaluate\nRNGDet with different backbones (i.e., ResNet-34, ResNet-50\nand ResNet-101) for more fair and comprehensive comparison.\nQualitative visualizations are provided in Fig. 8.\nFor segmentation results, we ï¬rst predict the segmentation\nmap, and then binarize it by thresholding. Finally, we run\nskeletonization algorithms to extract the graph of predicted seg-\nmentation maps. From the comparison results, we ï¬nd that these\napproaches have good pixel-level results since they directly opti-\nmize pixel-level segmentation. However, since they cannot fully\nutilize spatial and geometric information, they have poor per-\nformance on topology correctness. Thus segmentation-based ap-\nproaches have relatively inferior intersection-level and topology-\nlevel scores. Therefore, segmentation-based approaches are not\nsufï¬cient for our road network graph detection task.\nGraph-based approaches directly optimize the graph, thus\nthey present much better results from the topology perspective.\nRoadTracer has a ï¬xed step length, which makes it unable to\nhandle some scenarios very well, especially when the agent\nis near road intersections. VecRoad is more powerful due to\nthe use of Res2Net [51] backbone network and the ï¬‚exible\nstep size. However, VecRoad predicts vertices by ï¬nding local\npeaks on heatmaps, so when vertices are very close to each\nother, it may not be able to correctly detect them. Therefore,\nit may fail to correctly detect some precise graph structures.\nSat2Graph presents good results on straight road detection, but\nit cannot handle curve roads very well, which degrades its\nï¬nal performance (please refer to the fourth row of Fig. 8 as\nan example). Our proposed RNGDet can directly output the\ncoordinates of vertices, therefore RNGDet can handle more\ncomplicated situations and has more powerful performance.\nRNGDet with ResNet-101 backbone has the best performance\nduring evaluation, gaining 2-5% improvement on almost all\nmetrics scores compared with past state-of-the-art approaches.\nRNGDet with ResNet-50 backbone has a little bit inferior\nperformance, but it still outperforms VecRoad except APLS.\nRNGDet with ResNet-34 backbone presents unsatisfactory re-\nsults, which may be caused by the shallow backbone network\nand FPN segmentation heads.\nF . Ablation studies\nIn this section, we study the signiï¬cance of some components\nof our network design, including the road segment segmenta-\ntion S, road intersection segmentation I and the transformer\nstructure of RNGDet. All ablation studies are conducted on\nRNGDet with ResNet-101 backbone. The quantitative results\nof our ablation studies are shown in Tab. II.\nFirst, we completely remove the segmentation heads from\nRNGDet. From the evaluation results, we notice that the perfor-\nmance of RNGDet drops a lot because the segmentation heads\ncan provide strong supervision to assist the CNN backbone to\nextract features of the input. Then, we evaluate RNGDet by\nonly removing the road segment segmentation S. All metrics\nscores degrade, especially pixel-level metrics scores. After this,\n9\n(a) Ground truth\n (b) ImprovedRoad [8]\n (c) SPIN Road [11]\n (d) RoadTracer [14]\n (e) VecRoad [17]\n (f) Sat2Graph [16]\n (g) RNGDet\nFig. 8: Qualitative demonstrations. We visualize the road network detection results on aerial images. The size of each image is\n512 Ã—512. (a) Ground-truth road network graph (cyan lines). (b)-(c) The road network graph predicted by segmentation-based\napproaches (orange lines). These two approaches have poor topology performance such as incorrect disconnections. (d)-(g) The\nroad network graph predicted by graph-based approaches (orange lines as edges and yellow points as vertices). Compared with\nRoadTracer, VecRoad and Sat2Graph, RNGDet presents more precise graph structures. For better visualization, lines are widened\nbut they are actually of one-pixel width. This ï¬gure is best viewed in color. Please zoom in for details.\nwe remove the road intersection segmentation I only, and\nthe intersection-level metrics scores and APLS are severely\nharmed. Because Iis critical to make RNGDet aware of road\nintersections, removing I will lead to incorrect connections\nnear road intersections and make the topology correctness much\nworse. In this way, the necessity of the segmentation maps\nincluding Sand Iis veriï¬ed.\nFinally, to examine the importance of the transformer struc-\nture in RNGDet, we replace the transformer with a modiï¬ed\nMask-RCNN [52]. In our task, similar to VecRoad [17], Mask-\nRCNN ï¬rst predicts the heatmap of vertices at the next time\nstep, and then extracts the coordinate of vertices by handcrafted\n10\nTABLE I: The quantitative comparison results. The best results are highlighted in bold font. For all the metrics, larger values\nindicate better performance.\nApproaches\nP-P â†‘ P-R â†‘ P-F â†‘ I-P â†‘ I-R â†‘ I-F â†‘\nAPLS â†‘2.0 5.0 10.0 2.0 5.0 10.0 2.0 5.0 10.0 2.0 5.0 10.0 2.0 5.0 10.0 2.0 5.0 10.0\nImprovedRoad [8] 68.22 75.70 78.72 47.50 54.09 57.80 56.01 63.10 66.66 29.00 40.62 43.10 23.25 32.60 34.52 25.81 36.17 38.34 41.71\nSPIN RoadMapper [11] 78.53 85.82 89.02 54.02 60.28 63.71 64.01 70.82 74.27 45.27 59.66 62.08 28.87 37.56 38.93 33.67 35.26 46.10 47.85\nRoadTracer [14] 57.49 68.26 74.53 35.09 41.81 46.28 43.58 51.86 57.10 22.79 55.50 78.93 13.18 32.11 44.16 16.59 16.70 40.68 56.63\nVecRoad [17] 60.87 69.33 73.97 64.91 74.00 78.88 62.83 71.59 76.35 37.49 63.87 68.77 37.51 63.70 68.43 37.50 63.78 68.60 65.69\nSat2Graph [16] 57.87 65.33 69.43 63.02 71.33 76.15 60.33 68.20 72.64 29.61 57.02 60.33 34.20 65.90 69.64 31.74 61.14 64.65 63.21\nRNGDet (ResNet-34) 58.33 69.07 73.81 63.80 75.15 79.43 60.94 71.98 76.52 32.83 57.02 69.22 34.19 55.96 72.16 33.50 56.49 70.66 55.64\nRNGDet (ResNet-50) 62.11 69.54 74.39 66.07 74.92 80.05 64.03 72.12 77.13 36.51 63.97 75.20 39.98 65.48 62.84 38.17 64.72 68.47 63.76\nRNGDet (ResNet-101) 65.63 72.31 77.08 66.42 75.08 82.13 66.02 73.67 79.52 42.37 65.30 72.18 40.40 66.50 73.23 41.36 65.89 72.70 67.88\nTABLE II: The quantitative results for the ablation study. The best results are highlighted in bold font. For all the metrics,\nlarger values indicate better performance. We assess the road segment segmentation ( S), the road intersection segmentation ( I)\nand the transformer structure (T).\nP-P â†‘ P-R â†‘ P-F â†‘ I-P â†‘ I-R â†‘ I-F â†‘\nAPLS â†‘S I T 2.0 5.0 10.0 2.0 5.0 10.0 2.0 5.0 10.0 2.0 5.0 10.0 2.0 5.0 10.0 2.0 5.0 10.0\nâœ“ 37.63 56.80 61.79 55.24 60.12 64.77 44.77 58.41 63.24 25.40 47.03 69.62 20.70 40.93 53.26 22.81 43.77 60.35 50.32\nâœ“ âœ“ 40.09 55.83 65.97 57.41 63.28 66.21 47.21 59.32 66.09 36.95 56.38 73.72 33.39 58.64 64.30 35.08 57.49 68.69 61.22\nâœ“ âœ“ 59.80 65.34 72.79 62.19 73.83 79.02 60.97 69.33 75.78 32.14 50.86 68.44 32.62 55.04 62.47 32.38 52.87 65.32 59.90\nâœ“ âœ“ 56.04 69.91 74.52 62.88 71.64 74.56 59.26 70.76 74.54 31.65 57.21 63.90 32.10 59.33 62.42 31.87 58.25 63.15 60.79\nâœ“ âœ“ âœ“ 65.63 72.31 77.08 66.42 75.08 82.13 66.02 73.67 79.52 42.37 65.30 72.18 40.40 66.50 73.23 41.36 65.89 72.70 67.88\nor heuristic post-processing. Thus, RNGDet with Mask-RCNN\ncannot be optimized in an end-to-end way either, which affects\nits ï¬nal performance. From the experimental results, RNGDet\nwith Mask-RCNN presents inferior results compared with the\noriginal RNGDet. Therefore, the importance of the transformer\nstructure is proved.\nG. Number of vertex queries\nUnder normal circumstances, the number of queries |Q|\nshould be obviously larger than the maximum number of\nvertices at the next time step. Usually, the road intersections\nhave at most 5 roads incident with each other. Thus, we try\nto train RNGDet with 5, 10 and 20 queries and observe the\nobtained performance. We use RNGDet with ResNet-101 for\nthe experiments. During the experiments, we use metrics P-F\n(Î´= 5), I-F ( Î´= 5) and APLS to evaluate models.\nBased on the results shown in Tab. III, RNGDet with 10\ninput queries presents the best performance, thus the number of\nqueries is set to 10.\nTABLE III: The quantitative results of RNGDet with different\nnumbers of queries ( |Q|).\nP-F â†‘ I-F â†‘ APLS â†‘\nRNGDet with 5 queries 73.10 61.74 62.78\nRNGDet with 10 queries 73.67 65.89 67.88\nRNGDet with 20 queries 73.90 64.27 65.31\nH. Failure cases\nAlthough RNGDet presents superiority against past ap-\nproaches, it still cannot handle some very complicated cases,\nsuch as occluded overpasses, very well yet. Moreover, RNGDet\nalso suffers from the drifting issue of imitation learning (i.e.,\nwhen the agent is away from the right track, it may not be\nable to get back to the correct state) similar to other graph-\nbased approaches, even though it can relieve this problem to\nsome extent. Some example visualizations of failure cases of\nRNGDet are shown in Fig. 9. These cases could be handled in\nthe future with more powerful backbone networks or training\nstrategies.\nV. C ONCLUSIONS AND FUTURE WORK\nWe proposed here RNGDet, a novel iterative approach to\nautomatically detect the road network graph from aerial images.\nTaken as input an aerial image, RNGDet could directly output\nthe road network graph with vertices and edges. First, RNGDet\npredicted a set of candidate initial vertices, and then iteratively\ngenerated the road network graph vertex-by-vertex starting from\neach candidate initial vertex. Due to the use of transformer and\ndeep queries, RNGDet could handle complicated intersection\npoints with an arbitrary number of incident road segments.\nRNGDet was evaluated on a publicly available dataset and\npresented the state-of-the-art performance in terms of all eval-\nuation metrics, including pixel-level metrics, intersection-level\nmetrics and topology-level metric APLS. The experimental\nresults demonstrated the superiority of our work. In the future,\nwe would like to further improve RNGDet by using more\npowerful backbone networks and training strategies. Besides,\nwe will adapt RNGDet to other graph detection tasks, such as\nroad laneline detection and road lane centerline detection.\nREFERENCES\n[1] Y .-Y . Chiang and C. A. Knoblock, â€œExtracting road vector data from raster\nmaps,â€ in International Workshop on Graphics Recognition . Springer,\n2009, pp. 93â€“105.\n11\n(a) Ground truth\n (b) VecRoad\n (c) RNGDet\nFig. 9: Qualitative demonstrations of the failure cases. (a) The\nground-truth (cyan lines); (b) The result of VecRoad (orange\nlines for edges and yellow points for vertices); (c) The result of\nRNGDet (orange lines for edges and yellow points for vertices).\nThe ï¬rst and second rows show complicated road intersections\nand overpasses with occlusion. The last row is an example\ncontaining severe tree occlusion. RNGDet at this stage cannot\nhandle these cases very well yet. This problem could be relieved\nin the future by using more powerful backbone networks or\ntraining strategies for RNGDet. The ï¬gure is best viewed in\ncolor. Please zoom in for details.\n[2] V . Mnih and G. E. Hinton, â€œLearning to detect roads in high-resolution\naerial images,â€ in European Conference on Computer Vision . Springer,\n2010, pp. 210â€“223.\n[3] N. O. Department of Information Technology & Telecommuni-\ncations (DoITT), â€œNYC-Planimetrics Database,â€ https://github.com/\nCityOfNewYork/nyc-planimetrics, 2019.\n[4] X. Hu, Y . Li, J. Shan, J. Zhang, and Y . Zhang, â€œRoad centerline extraction\nin complex urban scenes from lidar data based on multiple features,â€ IEEE\nTransactions on Geoscience and Remote Sensing , vol. 52, no. 11, pp.\n7448â€“7456, 2014.\n[5] W. Shi, Z. Miao, Q. Wang, and H. Zhang, â€œSpectralâ€“spatial classiï¬cation\nand shape features for urban road centerline extraction,â€ IEEE Geoscience\nand Remote Sensing Letters , vol. 11, no. 4, pp. 788â€“792, 2013.\n[6] C. Unsalan and B. Sirmacek, â€œRoad network detection using probabilistic\nand graph theoretical methods,â€ IEEE Transactions on Geoscience and\nRemote Sensing, vol. 50, no. 11, pp. 4441â€“4453, 2012.\n[7] G. Cheng, F. Zhu, S. Xiang, and C. Pan, â€œRoad centerline extraction via\nsemisupervised segmentation and multidirection nonmaximum suppres-\nsion,â€ IEEE Geoscience and Remote Sensing Letters , vol. 13, no. 4, pp.\n545â€“549, 2016.\n[8] A. Batra, S. Singh, G. Pang, S. Basu, C. Jawahar, and M. Paluri, â€œImproved\nroad connectivity by joint learning of orientation and segmentation,â€ in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 10 385â€“10 393.\n[9] G. M Â´attyus, W. Luo, and R. Urtasun, â€œDeeproadmapper: Extracting road\ntopology from aerial images,â€ in Proceedings of the IEEE International\nConference on Computer Vision , 2017, pp. 3438â€“3446.\n[10] A. V . Etten, â€œCity-scale road extraction from satellite imagery v2: Road\nspeeds and travel times,â€ in Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision , 2020, pp. 1786â€“1795.\n[11] W. Gedara Chaminda Bandara, J. M. J. Valanarasu, and V . M. Patel,\nâ€œSpin road mapper: Extracting roads from aerial images via spatial and\ninteraction space graph reasoning for autonomous driving,â€ arXiv e-prints,\npp. arXivâ€“2109, 2021.\n[12] G. Cheng, Y . Wang, S. Xu, H. Wang, S. Xiang, and C. Pan, â€œAutomatic\nroad detection and centerline extraction via cascaded end-to-end convo-\nlutional neural network,â€ IEEE Transactions on Geoscience and Remote\nSensing, vol. 55, no. 6, pp. 3322â€“3337, 2017.\n[13] G. Zhou, W. Chen, Q. Gui, X. Li, and L. Wang, â€œSplit depth-wise separable\ngraph-convolution network for road extraction in complex environments\nfrom high-resolution remote-sensing images,â€ IEEE Transactions on Geo-\nscience and Remote Sensing , 2021.\n[14] F. Bastani, S. He, S. Abbar, M. Alizadeh, H. Balakrishnan, S. Chawla,\nS. Madden, and D. DeWitt, â€œRoadtracer: Automatic extraction of road\nnetworks from aerial images,â€ in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2018, pp. 4720â€“4728.\n[15] Z. Li, J. D. Wegner, and A. Lucchi, â€œTopological map extraction from\noverhead images,â€ in Proceedings of the IEEE International Conference\non Computer Vision , 2019, pp. 1715â€“1724.\n[16] S. He, F. Bastani, S. Jagwani, M. Alizadeh, H. Balakrishnan, S. Chawla,\nM. M. Elshrif, S. Madden, and M. A. Sadeghi, â€œSat2graph: road graph\nextraction through graph-tensor encoding,â€ in Computer Visionâ€“ECCV\n2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020,\nProceedings, Part XXIV 16 . Springer, 2020, pp. 51â€“67.\n[17] Y .-Q. Tan, S.-H. Gao, X.-Y . Li, M.-M. Cheng, and B. Ren, â€œVecroad:\nPoint-based iterative graph exploration for road graphs extraction,â€ in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2020, pp. 8910â€“8918.\n[18] D. Belli and T. Kipf, â€œImage-conditioned graph generation for road\nnetwork extraction,â€ NeurIPS 2019 workshop on Graph Representation\nLearning, 2019.\n[19] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, â€œEnd-to-end object detection with transformers,â€ in Eu-\nropean Conference on Computer Vision . Springer, 2020, pp. 213â€“229.\n[20] S. Ganguli, P. Garzon, and N. Glaser, â€œGeogan: A conditional gan with\nreconstruction and style loss to generate standard layer of maps from\nsatellite images,â€ arXiv preprint arXiv:1902.05611 , 2019.\n[21] R. He, X. Li, G. Chen, G. Chen, and Y . Liu, â€œGenerative adversarial\nnetwork-based semi-supervised learning for real-time risk warning of\nprocess industries,â€ Expert Systems with Applications, vol. 150, p. 113244,\n2020.\n[22] Y . Kang, S. Gao, and R. E. Roth, â€œTransferring multiscale map styles using\ngenerative adversarial networks,â€ International Journal of Cartography ,\nvol. 5, no. 2-3, pp. 115â€“141, 2019.\n[23] S. Wenzel and D. Bulatov, â€œSimultaneous chain-forming and generaliza-\ntion of road networks,â€ Photogrammetric Engineering & Remote Sensing ,\nvol. 85, no. 1, pp. 19â€“28, 2019.\n[24] D. Bulatov, S. Wenzel, G. H Â¨aufel, and J. Meidow, â€œChain-wise gener-\nalization of road networks using model selection,â€ ISPRS Annals of the\nPhotogrammetry, Remote Sensing and Spatial Information Sciences, vol. 4,\np. 59, 2017.\n[25] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, â€œSequence level training\nwith recurrent neural networks,â€ arXiv preprint arXiv:1511.06732 , 2015.\n[26] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-\ndecoder with atrous separable convolution for semantic image segmen-\ntation,â€ in Proceedings of the European conference on computer vision\n(ECCV), 2018, pp. 801â€“818.\n[27] T.-Y . Lin, P. Doll Â´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,\nâ€œFeature pyramid networks for object detection,â€ in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2017, pp.\n2117â€“2125.\n[28] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y . Zhao, D. Liu, Y . Mu,\nM. Tan, X. Wang et al., â€œDeep high-resolution representation learning for\nvisual recognition,â€ IEEE transactions on pattern analysis and machine\nintelligence, vol. 43, no. 10, pp. 3349â€“3364, 2020.\n[29] J. Liang, N. Homayounfar, W.-C. Ma, S. Wang, and R. Urtasun, â€œConvolu-\ntional recurrent network for road boundary extraction,â€ in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition , 2019,\npp. 9512â€“9521.\n[30] Z. Xu, Y . Sun, and M. Liu, â€œTopo-boundary: A benchmark dataset on\ntopological road-boundary detection using aerial images for autonomous\ndriving,â€ IEEE Robotics and Automation Letters , vol. 6, no. 4, pp. 7248â€“\n7255, 2021.\n[31] Z. Xu, Y . Liu, L. Gan, X. Hu, Y . Sun, L. Wang, and M. Liu, â€œcsboundary:\nCity-scale road-boundary detection in aerial images for high-deï¬nition\nmaps,â€ arXiv preprint arXiv:2111.06020 , 2021.\n[32] N. Homayounfar, W.-C. Ma, S. Kowshika Lakshmikanth, and R. Urtasun,\nâ€œHierarchical recurrent attention networks for structured online maps,â€ in\n12\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 3417â€“3426.\n[33] N. Homayounfar, W.-C. Ma, J. Liang, X. Wu, J. Fan, and R. Urtasun,\nâ€œDagmapper: Learning to map by discovering lane topology,â€ in Proceed-\nings of the IEEE International Conference on Computer Vision , 2019, pp.\n2911â€“2920.\n[34] Q. Li, Y . Wang, Y . Wang, and H. Zhao, â€œHdmapnet: A local semantic\nmap learning and evaluation framework,â€ 2021.\n[35] X. Yang, X. Li, Y . Ye, R. Y . Lau, X. Zhang, and X. Huang, â€œRoad detection\nand centerline extraction via deep recurrent convolutional neural network\nu-net,â€ IEEE Transactions on Geoscience and Remote Sensing , vol. 57,\nno. 9, pp. 7209â€“7220, 2019.\n[36] S. He and H. Balakrishnan, â€œLane-level street map extraction from\naerial imagery,â€ in Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision , 2022, pp. 2080â€“2089.\n[37] Y . Zhou, Y . Takeda, M. Tomizuka, and W. Zhan, â€œAutomatic construction\nof lane-level hd maps for urban scenes,â€ in 2021 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2021, pp.\n6649â€“6656.\n[38] Z. Xu, Y . Sun, and M. Liu, â€œicurb: Imitation learning-based detection of\nroad curbs using aerial images for autonomous driving,â€ IEEE Robotics\nand Automation Letters , vol. 6, no. 2, pp. 1097â€“1104, 2021.\n[39] Z. Xu, Y . Sun, L. Wang, and M. Liu, â€œCp-loss: Connectivity-preserving\nloss for road curb detection in autonomous driving with aerial images,â€\nin 2021 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS). IEEE, 2021, pp. 1117â€“1123.\n[40] S. Ross, G. Gordon, and D. Bagnell, â€œA reduction of imitation learning and\nstructured prediction to no-regret online learning,â€ in Proceedings of the\nfourteenth international conference on artiï¬cial intelligence and statistics .\nJMLR Workshop and Conference Proceedings, 2011, pp. 627â€“635.\n[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nÅ. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€ in Advances in\nneural information processing systems , 2017, pp. 5998â€“6008.\n[42] Y . Xu, W. Xu, D. Cheung, and Z. Tu, â€œLine segment detection using\ntransformers without edges,â€ in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2021, pp. 4257â€“4266.\n[43] Y . B. Can, A. Liniger, D. P. Paudel, and L. Van Gool, â€œStructured\nbirdâ€™s-eye-view trafï¬c scene understanding from onboard images,â€ in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 15 661â€“15 670.\n[44] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and J. Pe-\nters, â€œAn algorithmic perspective on imitation learning,â€ arXiv preprint\narXiv:1811.06711, 2018.\n[45] S. Ross and D. Bagnell, â€œEfï¬cient reductions for imitation learning,â€\nin Proceedings of the thirteenth international conference on artiï¬cial\nintelligence and statistics. JMLR Workshop and Conference Proceedings,\n2010, pp. 661â€“668.\n[46] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for image\nrecognition,â€ in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770â€“778.\n[47] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, â€œAttention\naugmented convolutional networks,â€ in Proceedings of the IEEE/CVF\ninternational conference on computer vision , 2019, pp. 3286â€“3295.\n[48] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and\nD. Tran, â€œImage transformer,â€ in International Conference on Machine\nLearning. PMLR, 2018, pp. 4055â€“4064.\n[49] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. DollÂ´ar, â€œFocal loss for dense\nobject detection,â€ in Proceedings of the IEEE international conference on\ncomputer vision, 2017, pp. 2980â€“2988.\n[50] A. Van Etten, D. Lindenbaum, and T. M. Bacastow, â€œSpacenet: A remote\nsensing dataset and challenge series,â€ arXiv preprint arXiv:1807.01232 ,\n2018.\n[51] S. Gao, M.-M. Cheng, K. Zhao, X.-Y . Zhang, M.-H. Yang, and P. H. Torr,\nâ€œRes2net: A new multi-scale backbone architecture,â€ IEEE transactions\non pattern analysis and machine intelligence , 2019.\n[52] K. He, G. Gkioxari, P. Doll Â´ar, and R. Girshick, â€œMask r-cnn,â€ in Proceed-\nings of the IEEE international conference on computer vision , 2017, pp.\n2961â€“2969.\nZhenhua Xu (Student Member 2022) received the\nbachelorâ€™s degree from Harbin Institute of Technology,\nHarbin, China, in 2018. He is now a PhD candidate\nsupervised by Prof. Ming Liu and Prof. Huamin Qu at\nthe Department of Computer Science and Engineering,\nThe Hong Kong University of Science and Technol-\nogy, HKSAR, China. His current research interests\ninclude HD map automatic annotation, line-shaped ob-\nject detection, imitation learning, autonomous driving,\netc.\nYuxuan Liu (Student Member 2022) Yuxuan Liu\nreceived his Bachelorâ€™s degree from Zhejiang Uni-\nversity, Zhejiang, China in 2019, majoring in Mecha-\ntronic. He is now a Ph.D candidate at the Department\nof Electronic and Computer Engineering, The Hong\nKong University of Science and Technology, Hong\nKong, China. His current research interests include\nautonomous driving, deep learning, robotics, visual 3D\nobject detection, visual depth prediction, etc.\nLu Gan (Student Member 2022) received both her\nbachelor and master degreee from Nanjing Univer-\nsity of Aeronautics and Astronautics. After that, she\nworked in Nanyang Technological University as a\nresearch associate. Now she is a PhD student in Hong\nKong University od Science and Technology (GZ). Her\nresearch interests include autonomous driving, deep\nlearning and uncertainty-aware motion prediction and\nplanning.\nYuxiang Sun (Member 2022) received the bache-\nlorâ€™s degree from the Hefei University of Technology,\nHefei, China, in 2009, the masterâ€™s degree from the\nUniversity of Science and Technology of China, Hefei,\nin 2012, and the Ph.D. degree from The Chinese\nUniversity of Hong Kong, Hong Kong, in 2017.\nHe is now a research associate at the Department\nof Electronic and Computer Engineering, The Hong\nKong University of Science and Technology, Hong\nKong, China. His current research interests include\nautonomous driving, deep learning, robotics and au-\ntonomous systems, semantic scene understanding, etc.\nHe is a recipient of the Best Paper in Robotics Award at IEEE-ROBIO 2019,\nand the Best Student Paper Finalist Award at IEEE-ROBIO 2015.\nXinyu Wu (Member 2022) is now a professor at\nShenzhen Institutes of Advanced Technology, and di-\nrector of the Center for Intelligent Bionic. He received\nhis B.E. and M.E. degrees from the Department of\nAutomation, University of Science and Technology\nof China in 2001 and 2004, respectively. His Ph.D.\ndegree was awarded from the Chinese University of\nHong Kong in 2008. He has published over 180 papers\nand two monographs. His research interests include\ncomputer vision, robotics, and intelligent systems.\n13\nMing Liu (Senior Member 2022) received the B.A.\ndegree at Tongji University in 2005. He stayed one\nyear in Erlangen-N Â¨unberg University and Fraunhofer\nInstitute IISB, Germany, as visiting scholar. He grad-\nuated as a PhD student from ETH Z Â¨urich in 2013.\nHe is currently an Assoicate Professor at the Depart-\nment of Electronic and Computer Engineering, The\nHong Kong University of Science and Technology,\nHong Kong. He has been involved in several NSF\nprojects, and National 863-Hi-Tech-Plan projects in\nChina. He is PI of 20+ projects including projects\nfunded by RGC, NSFC, ITC, SZSTI, etc. He was the general chair of ICVS-\n2017, the program chair of IEEE-RCAR 2016, and the program chair of\nInternational Robotic Alliance Conference 2017.\nHis current research interests include dynamic environment modeling, 3D\nmapping, machine learning and visual control, etc.\nLujia Wang (Member 2022) received the Ph.D. degree\nfrom the Department of Electronic Engineering, The\nChinese University of Hong Kong, Hong Kong, in\n2015. She was a Research Fellow with the School\nof Electrical Electronic Engineering, Nanyang Tech-\nnological University, Singapore, from 2015 to 2016.\nShe was an associate professor with the Shenzhen\nInstitutes of Advanced Technology, Chinese Academy\nof Sciences, Shenzhen, Guangdong, from 2016-2021.\nHer current research interests include Cloud Robotics,\nLifelong Federated Robotic Learning, Resource/Task\nAllocation for Robotic Systems, and Applications on Autonomous Driving.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8330352306365967
    },
    {
      "name": "Correctness",
      "score": 0.5906228423118591
    },
    {
      "name": "Aerial image",
      "score": 0.4935206174850464
    },
    {
      "name": "Network topology",
      "score": 0.4825965166091919
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4821717143058777
    },
    {
      "name": "Segmentation",
      "score": 0.4753182530403137
    },
    {
      "name": "Image segmentation",
      "score": 0.4610942602157593
    },
    {
      "name": "Computer vision",
      "score": 0.42685604095458984
    },
    {
      "name": "Graph",
      "score": 0.4261436462402344
    },
    {
      "name": "Data mining",
      "score": 0.3466467261314392
    },
    {
      "name": "Image (mathematics)",
      "score": 0.2388826310634613
    },
    {
      "name": "Algorithm",
      "score": 0.23378834128379822
    },
    {
      "name": "Theoretical computer science",
      "score": 0.23108476400375366
    },
    {
      "name": "Computer network",
      "score": 0.15401631593704224
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210145761",
      "name": "Shenzhen Institutes of Advanced Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210096887",
      "name": "HKUST Shenzhen Research Institute",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210102541",
      "name": "Shenzhen Bay Laboratory",
      "country": "CN"
    }
  ]
}