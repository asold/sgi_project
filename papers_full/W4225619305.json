{
    "title": "Towards End-to-End Image Compression and Analysis with Transformers",
    "url": "https://openalex.org/W4225619305",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2150409112",
            "name": "Yuanchao Bai",
            "affiliations": [
                "Harbin Institute of Technology",
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2055575727",
            "name": "Xu Yang",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2113346361",
            "name": "Xianming Liu",
            "affiliations": [
                "Harbin Institute of Technology",
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2108913913",
            "name": "Junjun Jiang",
            "affiliations": [
                "Harbin Institute of Technology",
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2109965745",
            "name": "Yaowei Wang",
            "affiliations": [
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2189497957",
            "name": "Xiangyang Ji",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2108823143",
            "name": "Wen Gao",
            "affiliations": [
                "Peking University",
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2150409112",
            "name": "Yuanchao Bai",
            "affiliations": [
                "Harbin Institute of Technology",
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2055575727",
            "name": "Xu Yang",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2113346361",
            "name": "Xianming Liu",
            "affiliations": [
                "Harbin Institute of Technology",
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2108913913",
            "name": "Junjun Jiang",
            "affiliations": [
                "Harbin Institute of Technology",
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2109965745",
            "name": "Yaowei Wang",
            "affiliations": [
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2189497957",
            "name": "Xiangyang Ji",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2108823143",
            "name": "Wen Gao",
            "affiliations": [
                "Peking University",
                "Peng Cheng Laboratory"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6729906282",
        "https://openalex.org/W2178928294",
        "https://openalex.org/W2785562966",
        "https://openalex.org/W3105128802",
        "https://openalex.org/W6786585107",
        "https://openalex.org/W2998183800",
        "https://openalex.org/W3102451820",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3000726408",
        "https://openalex.org/W3116647679",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2997572967",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6794391090",
        "https://openalex.org/W2604392022",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W6781810459",
        "https://openalex.org/W2892278106",
        "https://openalex.org/W2086161653",
        "https://openalex.org/W3092877102",
        "https://openalex.org/W6776188000",
        "https://openalex.org/W6648982606",
        "https://openalex.org/W6674699539",
        "https://openalex.org/W2593493485",
        "https://openalex.org/W2276024283",
        "https://openalex.org/W2786977213",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W6660586467",
        "https://openalex.org/W2946942188",
        "https://openalex.org/W6679216473",
        "https://openalex.org/W2944223741",
        "https://openalex.org/W6600281463",
        "https://openalex.org/W3120857301",
        "https://openalex.org/W6743440100",
        "https://openalex.org/W2963447011",
        "https://openalex.org/W3132890542",
        "https://openalex.org/W4294567867",
        "https://openalex.org/W2993383518",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2965248168",
        "https://openalex.org/W3160345508",
        "https://openalex.org/W4297659253",
        "https://openalex.org/W2140196014",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W3034175346",
        "https://openalex.org/W4293469690",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3082548248",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W2964098744",
        "https://openalex.org/W3171125843",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W1686946872",
        "https://openalex.org/W3133277045",
        "https://openalex.org/W3034429256",
        "https://openalex.org/W1933799648",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2992308087",
        "https://openalex.org/W3136416617",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2765407302",
        "https://openalex.org/W2129652681",
        "https://openalex.org/W4214669216",
        "https://openalex.org/W2099563019",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3175544090",
        "https://openalex.org/W3170778815",
        "https://openalex.org/W3160555381",
        "https://openalex.org/W3160673571",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2949361041",
        "https://openalex.org/W4226151124"
    ],
    "abstract": "We propose an end-to-end image compression and analysis model with Transformers, targeting to the cloud-based image classification application. Instead of placing an existing Transformer-based image classification model directly after an image codec, we aim to redesign the Vision Transformer (ViT) model to perform image classification from the compressed features and facilitate image compression with the long-term information from the Transformer. Specifically, we first replace the patchify stem (i.e., image splitting and embedding) of the ViT model with a lightweight image encoder modelled by a convolutional neural network. The compressed features generated by the image encoder are injected convolutional inductive bias and are fed to the Transformer for image classification bypassing image reconstruction. Meanwhile, we propose a feature aggregation module to fuse the compressed features with the selected intermediate features of the Transformer, and feed the aggregated features to a deconvolutional neural network for image reconstruction. The aggregated features can obtain the long-term information from the self-attention mechanism of the Transformer and improve the compression performance. The rate-distortion-accuracy optimization problem is finally solved by a two-step training strategy. Experimental results demonstrate the effectiveness of the proposed model in both the image compression and the classification tasks.",
    "full_text": "Towards End-to-End Image Compression and Analysis with Transformers\nYuanchao Bai1;2\u0003, Xu Yang1\u0003, Xianming Liu1;2y, Junjun Jiang1;2,\nYaowei Wang2, Xiangyang Ji3, Wen Gao2;4\n1Harbin Institute of Technology,2Peng Cheng Laboratory, 3Tsinghua University, 4Peking University\nyuanchao.bai@gmail.com, 20s103274@stu.hit.edu.cn, {csxm,jiangjunjun}@hit.edu.cn\nwangyw@pcl.ac.cn, xyji@tsinghua.edu.cn, wgao@pku.edu.cn\nAbstract\nWe propose an end-to-end image compression and analysis\nmodel with Transformers, targeting to the cloud-based im-\nage classiﬁcation application. Instead of placing an existing\nTransformer-based image classiﬁcation model directly after\nan image codec, we aim to redesign the Vision Transformer\n(ViT) model to perform image classiﬁcation from the com-\npressed features and facilitate image compression with the\nlong-term information from the Transformer. Speciﬁcally, we\nﬁrst replace the patchify stem (i.e., image splitting and em-\nbedding) of the ViT model with a lightweight image encoder\nmodelled by a convolutional neural network. The compressed\nfeatures generated by the image encoder are injected convolu-\ntional inductive bias and are fed to the Transformer for image\nclassiﬁcation bypassing image reconstruction. Meanwhile,\nwe propose a feature aggregation module to fuse the com-\npressed features with the selected intermediate features of the\nTransformer, and feed the aggregated features to a deconvo-\nlutional neural network for image reconstruction. The aggre-\ngated features can obtain the long-term information from the\nself-attention mechanism of the Transformer and improve the\ncompression performance. The rate-distortion-accuracy op-\ntimization problem is ﬁnally solved by a two-step training\nstrategy. Experimental results demonstrate the effectiveness\nof the proposed model in both the image compression and the\nclassiﬁcation tasks.\nIntroduction\nVision Transformer (ViT) (Dosovitskiy et al. 2021) and its\nvariations (Touvron et al. 2021; Wu et al. 2021; Yuan et al.\n2021; Chen et al. 2021b; Liu et al. 2021), inherited from\nTransformer architecture (Vaswani et al. 2017) in natural\nlanguage processing (NLP), have recently demonstrated out-\nstanding performance on a board range of image analysis\ntasks, such as image classiﬁcation (Dosovitskiy et al. 2021),\nsegmentation (Zheng et al. 2021) and object detection (Fang\net al. 2021). With the self-attention mechanism, these mod-\nels are capable of capturing long-range dependencies in the\nimage data, but inevitably result in high computational cost.\nIn practice, Transformer-based models are usually deployed\nin the cloud-based paradigm and executed remotely. For ex-\nample, massive image data is acquired by the frontend de-\n\u0003Equal contribution. †Corresponding author.\nCopyright c⃝ 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nvices, such as mobile phones or surveillance cameras, and\ntransmitted to the cloud (i.e., data center) for further analy-\nsis, sharing and storage. Image compression serves as a fun-\ndamental infrastructure for data communication between the\nfrontend and the cloud.\nIn the traditional paradigm of cloud-based applications,\nimage compression is considered independent of image\nanalysis, and adopts lossy image compression standards de-\nsigned for human vision, such as JPEG (Wallace 1992). In\nparticular, the raw images are ﬁrst transformed to the fre-\nquency domain with Discrete Cosine Transform (DCT). The\nfrequency coefﬁcients are then quantized to discard high fre-\nquencies that are less sensitive to human eyes. The quantized\ncoefﬁcients are encoded to bitstreams with entropy encod-\ning and are transmitted to the cloud. On the cloud side, the\nquantized coefﬁcients are recovered from the received bit-\nstreams, which are then inversely transformed to reconstruct\nimages. The reconstruction distortions are minimized with\nrespect to Peak Signal-to-Noise Ratio (PSNR). However, if\nthe reconstructed images optimized by PSNR are fed into\nthe downstream image analysis tasks, which are tailored to\nmachine vision instead, the corresponding results may be in-\naccurate, because the principle of machine vision is different\nfrom human vision (Wang et al. 2020). Besides, the tradi-\ntional image codecs are comprised of hand-crafted modules\nwith complex dependencies. It is difﬁcult to optimize the\nsophisticated compression frameworks together with subse-\nquent machine analysis tasks.\nRecently, learning-based image compression emerges as\nan active research area in computer vision community. A\nnumber of learning-based image codecs, such as (Toderici\net al. 2016; Theis et al. 2017; Li et al. 2018; Ball ´e et al.\n2018; Minnen, Ball´e, and Toderici 2018; Cheng et al. 2020;\nMa et al. 2020; Hu et al. 2021), have achieved compara-\nble or even better perceptual performance than traditional\nimage codecs for human vision. Besides, by replacing the\nhand-crafted modules with deep neural networks (DNNs),\nlearning-based image compression can be integrated with\nhigh-level tasks and end-to-end optimized for machine vi-\nsion (Torfason et al. 2018; Chamain et al. 2021; Le et al.\n2021). However, compared with image compression for hu-\nman vision, image compression for machine vision is still in\nits infancy, because it is challenging to achieve the best of\nboth worlds for low-level and high-level tasks.\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n104\nIn this paper, we propose a novel paradigm that is friendly\nfor both human vision and machine vision, which integrates\nlearning-based image compression with Transformer-based\nimage analysis. The derived end-to-end image compression\nand analysis model leads to the synergy effect of these two\ntasks. Instead of placing an existing Transformer-based im-\nage classiﬁcation model directly after an image codec, we\nredesign the ViT model to perform image classiﬁcation from\nthe compressed features (Torfason et al. 2018) and facilitate\nimage compression with the long-term information from the\nTransformer. Speciﬁcally, we replace the the patchify stem\n(i.e., image splitting and embedding) of the ViT model with\na lightweight image encoder modelled by a convolutional\nneural network (CNN). The compressed features generated\nby the image encoder are injected convolutional inductive\nbias and are more expressive than the features extracted by\nthe patchify stem from the decoded images. When trans-\nmitted to the cloud, the compressed features are fed to the\nTransformer for image classiﬁcation bypassing image recon-\nstruction. We further propose a feature aggregation module\nto fuse the compressed features with the selected interme-\ndiate features of the Transformer, and feed the aggregated\nfeatures to a deconvolutional neural network for image re-\nconstruction. The aggregated features obtain the long-term\ninformation from the Transformer and effectively improve\nthe compression performance. We interpret the correspond-\ning rate-distortion-accuracy optimization problem based on\nvariational auto-encoder (V AE) (Theis et al. 2017; Ball ´e\net al. 2018) and information bottleneck (IB) (Tishby, Pereira,\nand Bialek 2000; Alemi et al. 2017), and ﬁnally solve it with\na two-step training strategy.\nThe main contributions are summarized as follows:\n•We propose an end-to-end image compression and anal-\nysis model, which performs image classiﬁcation from\nthe compressed features. We interpret the rate-distortion-\naccuracy optimization problem based on V AE and IB.\n•We design the network by integrating learning-based im-\nage compression with ViT-based image analysis, which\nleads to the synergy between the two tasks.\n•In terms of rate-distortion, the proposed model achieves\nPSNR performance close to BPG (Bellard 2014). In\nterms of rate-accuracy, the proposed model outperforms\nResNet50 (He et al. 2016), DeiT-S (Touvron et al. 2021)\nand Swin-T (Liu et al. 2021) classiﬁcation from the de-\ncoded images, while signiﬁcantly reduces the computa-\ntional cost under equivalent number of parameters.\nRelated Work\nImage Compression for Machine Vision. With the fast\nprogress of artiﬁcial intelligence, an increasing amount of\nvisual data is now not only viewed by humans but also ana-\nlyzed by machines. Recently, image/video compression for\nmachine vision has drawn signiﬁcant interests in the com-\nputer vision community (Duan et al. 2020).\nIn order to optimize image compression with analysis,\n(Choi and Han 2020; Luo et al. 2021) and (Chamain, Che-\nung, and Ding 2019) proposed to optimize the quantization\nof the traditional codecs JPEG and JPEG2000 to improve\nthe performance of the following image classiﬁcation. How-\never, since the frameworks of traditional codecs are different\nfrom fully optimizable DNN and only the quantization is in-\nvolved in the optimization, the improvement is limited. In\ncontrast, learning-based image compression is more suitable\nto be jointly optimized with DNN-based image analysis. The\nrelated works can be divided into two categories: 1)RGB in-\nference, such as (Chamain et al. 2021) and (Le et al. 2021),\nperforms image analysis from RGB reconstructed images\nby placing image analysis methods directly after existing\nimage codecs. 2) Compressed inference, such as (Torfason\net al. 2018), performs image analysis directly from the com-\npressed features bypassing image reconstruction.\nIn this paper, we propose an end-to-end image compres-\nsion and analysis model with Transformers, inspired by\n(Torfason et al. 2018). Beyond (Torfason et al. 2018), we\ninterpret the rate-distortion-accuracy optimization problem\nbased on V AE and IB, and design the Transformer-based\nmodel leading to the synergy between the two tasks.\nTransformers in Computer Vision. Nowadays, Trans-\nformers have shown their potential to be a viable alterna-\ntive to CNNs in computer vision tasks. However, the ViT\nmodel (Dosovitskiy et al. 2021) without any human-deﬁned\ninductive bias suffers from over-ﬁtting when the training\ndata is limited, and thus needs sophisticated data augmen-\ntation schemes (Touvron et al. 2021). In order to improve\nthe performance and the robustness of Transformers, several\nworks (Wu et al. 2021; Yuan et al. 2021; Chen et al. 2021b)\nincorporated CNNs into Transformers.\nIn this paper, we propose to replace the patchify stem of\nthe ViT model with a CNN-based image encoder, which\ncan enable image analysis from the compressed features\nand effectively improve the performance of image classi-\nﬁcation. The concurrent work (Xiao et al. 2021) also ob-\nserves that early convolutions in Transformers can increase\nthe optimization stability and improve the Top-1 accuracy.\nOur experimental results are consistent with the observation\nof (Xiao et al. 2021).\nProposed Method\nProblem Formulation\nWe aim to perform image analysis from the compressed fea-\ntures. Given a raw image x and its label y, our goal is to\nlearn a compressed representation ^z that facilitates both im-\nage decoding (reconstruction) and analysis, as sketched in\nFig. 1. Since the compressed representation ^z is extracted\nfrom the image x while not accessing the label y, we as-\nsume that x, y, ^z form a Markov chain y ↔ x ↔ ^z, leading\nto p(^z|x;y) =p(^z|x).\nImage Compression. We ﬁrst formulate the lossy image\ncompression model without taking image analysis into con-\nsideration. Following the standard framework of variational\nauto-encoder based image compression (Theis et al. 2017;\nBall´e et al. 2018), the latent representation z is transformed\nfrom the raw image x by an encoder and is quantized to the\ndiscrete-valued ^z. Then, ^z is losslessly compressed with en-\ntropy encoding techniques (Witten, Neal, and Cleary 1987;\n105\nFigure 1: The theoretical framework of the proposed end-to-\nend image compression and analysis model.\nDuda 2009) to form a bitstream. On the decoder side, ^z is\nrecovered from the bitstream and inversely transformed to\na reconstructed image ^x. To optimize the performance of\nthe compression model, it can be approximated by the min-\nimization of the expectation of Kullback-Leibler (KL) di-\nvergence between the intractable true posterior p(^z|x) and a\nparametric inference model q(^z|x) over the data distribution\np(x) (Ball´e et al. 2018):\nEp(x)Dkl[q(^z|x)||p(^z|x)] =Ep(x)Eq(^z|x)[\u0018\u0018\u0018\u0018\u0018 :0\nlog q(^z|x)\n−log p(x|^z) −log p(^z)] + const (1)\nwhere Dkl[·||·] denotes KL divergence. Because the trans-\nform from x to z is deterministic and the quantization\nof z is relaxed by adding noise from uniform distribution\nU(−1\n2 ;1\n2 ), we have q(^z|x) =Q\niU(zi−1\n2 ;zi+ 1\n2 ) and thus\nthe ﬁrst term log q(^z|x) = 0. The second term of (1) is inter-\npreted as the expected distortion between x and ^x, and the\nthird term is interpreted as the cost of encoding^z, leading to\nthe rate-distortion trade-off (Shannon 1948).\nImage Analysis. We then turn to consider image analy-\nsis. We propose to maximize the mutual informationI(^z;y)\nbetween the compressed representation ^z and the label y,\ninspired by information bottleneck (Tishby, Pereira, and\nBialek 2000; Alemi et al. 2017). The mutual information\nI(^z;y) is the reduction in the uncertainty of y due to the\nknowledge of ^z:\nI(^z;y) =H(y) −H(y|^z)\n= H(y) +\nX\ny;^z\np(y;^z) logp(y|^z) (2)\nwhere H(·) denotes the entropy. Because the true poste-\nrior p(y|^z) is also intractable, we propose a variational ap-\nproximation q(y|^z), which is the decoder for image analy-\nsis apart from the decoder for image reconstruction. Since\nDkl[p(y|^z)||q(y|^z)] ≥0, we have P\ny p(y|^z) logp(y|^z) ≥P\ny p(y|^z) logq(y|^z) and thus\nI(^z;y) ≥H(y) +\nX\ny;^z\np(y;^z) logq(y|^z) (3)\nBecause the entropy H(y) is independent of ^z, we can\nmaximize P\ny;^z p(y;^z) logq(y|^z) as a proxy for I(^z;y).\nBased on the Markov chain assumption, we replace p(y;^z)\nwith P\nx p(x;y;^z) = P\nx p(x;y)p(^z|x), and can rewriteP\ny;^z p(y;^z) logq(y|^z) as\nEp(x;y)Ep(^z|x) log q(y|^z) (4)\nFigure 2: The network architecture of the encoder. We set\nN = 128 and M = 192 same as the low-rate setting of\n(Ball´e et al. 2018). The encoder replaces the patchify stem\nof ViT and is relatively lightweight, which can be deployed\nat the frontend. Q: Quantization. AE: Arithmetic Encoder.\nWith q(y|^z), we can generate the estimated label ^y from ^z.\nJoint Optimization. With (1) and (4), we further formu-\nlate the joint optimization of both image compression and\nanalysis. Since p(^z|x) in (4) is intractable, we share the in-\nference model q(^z|x) in (1) as the approximation, and min-\nimize the approximated negative (4) together with (1)1:\nEp(x;y){−\u000bEq(^z|x) log q(y|^z) +Dkl[q(^z|x)||p(^z|x)]}(5)\nwhere \u000b is a trade-off parameter. Suppose that p(x|^z) is\ngiven by N(x|^x;(2\f)−11), we can ﬁnally rewrite (5) to the\nobjective function:\nEp(x;y)Eq(^z|x)[−\u000blog q(y|^z) +\f∥x−^x∥2\n2 −log p(^z)] (6)\nThe ﬁrst term of (6) weighted by \u000bcan be interpreted as the\ncross-entropy loss for image analysis, such as image classiﬁ-\ncation or segmentation, based on the types of labely. In this\nwork, we choose image classiﬁcation as the target task. The\nsecond term weighted by \fis the mean square error (MSE)\ndistortion loss. The third term is the rate loss.\nIn contrast to the image compression models (Theis et al.\n2017; Ball´e et al. 2018), the compressed representation ^z in\n(6) is also optimized for image analysis tasks. The complex-\nity of ^z in (6) is controlled by minimizing the cost of encod-\ning ^z, rather than controlled by minimizing I(^z;x) in the\ninformation bottleneck models (Tishby, Pereira, and Bialek\n2000; Alemi et al. 2017).\nTransformer-based Network Architecture\nWe realize the theoretical framework in Fig. 1 by proposing\nan end-to-end image compression and analysis model with\nTransformers. The proposed model can promote the synergy\nbetween the two tasks.\nEncoder. The network architecture of the proposed en-\ncoder is illustrated in Fig. 2. Similar to the setting of (Ball ´e\net al. 2018), we employ four stride-2 5 ×5 convolutional\nlayers to extract features with gradually reduced spatial\nresolution from the input image x ∈ RH×W×3. We use\nLeakyReLU as the activation function instead of using the\nGeneralized Divisive Normalization (GDN) (Ball´e, Laparra,\nand Simoncelli 2016), because GDN results in convergence\n1Ep(x)Dkl[q(^z|x)||p(^z|x)] =Ep(x;y)Dkl[q(^z|x)||p(^z|x)]\n106\nproblem when training with the Transformer blocks in the\nproposed model.\nThe resulting feature z ∈R\nH\n16 ×W\n16 ×M is then quantized\nto the discrete-valued ^z. We employ the hyper-prior mod-\nule (Ball´e et al. 2018; Minnen, Ball ´e, and Toderici 2018) to\nestimate p(^z|h^z) for the entropy encoding of ^z, where h^z\ndenotes the hyper-prior of ^z. We do not use the serial au-\ntoregressive module (Minnen, Ball´e, and Toderici 2018), be-\ncause its corresponding decoding time is too long on large-\nscale image classiﬁcation datasets.\nFrom the perspective of Transformer architecture design,\nthe proposed encoder can be considered as the replacement\nof the patchify stem, i.e., a stride-16 16×16 convolutional\nlayer, applied to the input image in the ViT model (Dosovit-\nskiy et al. 2021). Several concurrent works (Wu et al. 2021;\nYuan et al. 2021; Chen et al. 2021b; Xiao et al. 2021) also\nreplace the patchify stem with a stack of convolutional lay-\ners, in order to improve the performance of image classi-\nﬁcation. In the experiments, we observe that the proposed\nencoder is capable of extracting compressed features suit-\nable for both image decoding reconstruction and classiﬁca-\ntion through joint optimization. Note that the proposed en-\ncoder is relatively lightweight, thus can be deployed at the\nfrontend, such as mobile phones or surveillance cameras.\nDecoder-Classiﬁer. The decoder receives the bitstream\nfrom the encoder and adopts the shared hyper-prior mod-\nule p(^z|h^z) to recover ^z. For image classiﬁcation, we di-\nrectly feed ^z to an inference network, instead of the common\nsubsequent approach—ﬁrst reconstruct the decoded image ^x\nand then conduct inference on ^x.\nSpeciﬁcally, we adopt the standard Transformer blocks\nin the ViT model (Dosovitskiy et al. 2021) with the num-\nber of parameters equivalent to ResNet50 (He et al. 2016),\nas shown in Fig. 3. We expand the channel dimension of\n^z to C with a 1 ×1 convolutional layer, and reshape the\nresulting feature ^z0 ∈ R\nH\n16 ×W\n16 ×C to a sequence ^z0 ∈\nR\nHW\n162 ×C. To maintain the spatial information of the fea-\nture ^z0, we add learnable position embeddings p to ^z0 lead-\ning to ~z0 = ^z0 + p. Following (Dosovitskiy et al. 2021),\nwe prepend a learnable class embedding ~c0, and feed the\nsequence [~c0; ~z0] ∈R( HW\n162 +1)×C to the Transformer con-\nsisting of L Transformer blocks. The architecture of each\nTransformer block is illustrated in Fig. 3. The computation\nprocess can be formulated as\n[~c′\ni; ~z′\ni] = MSA(LN([~ci−1; ~zi−1])) + [~ci−1; ~zi−1] (7)\n[~ci; ~zi] = FFN(LN([~c′\ni; ~z′\ni])) + [~c′\ni; ~z′\ni] i= 1;:::;L\nwhere MSA(·) denotes the multi-head self-attention mod-\nule, FFN(·) denotes the feed forward network andLN(·) de-\nnotes the layer normalization (Ba, Kiros, and Hinton 2016),\nrespectively.\nWith the self-attention mechanism, the class embedding\n~ci interacts with the image feature ~zi, and the ﬁnal output\n~cL is used to compute q(y|^z) for image classiﬁcation:\nq(y|^z) = Softmax(FFN(LN(~cL))) (8)\nwhere Softmax(·) denotes softmax operation. The FFN(·)\nis the classiﬁer head mapping the embedding dimension\nfrom Cto the number of classes.\nDecoder-Reconstructor. Reconstructing image ^x directly\nfrom ^z (or ^z0) ignores the global spatial correlations among\nthe latent features. Recent image compression works (Qian\net al. 2021; Guo et al. 2021) demonstrate that leveraging\nglobal context information during entropy coding can im-\nprove the compression performance. Transformers naturally\ncapture the global spatial information among the latent fea-\ntures, which can also beneﬁts low-level tasks, such as image\nprocessing (Chen et al. 2021a) and image generation (Jiang,\nChang, and Wang 2021). Motivated by these works, we aim\nto extract the intermediate features ~zi’s of the Transformer\nand incorporate them into image reconstruction.\nSpeciﬁcally, we select ^z0 and [~z1;~z2;~z3], and propose a\nfeature aggregation module to fuse these features, similar\nto (Zheng et al. 2021). Selecting [~z1;~z2;~z3] means that the\nﬁrst three Transformer blocks are also involved in the im-\nage reconstruction process. Since image reconstruction may\nwork independently of image classiﬁcation, we avoid using\n{~zi(i> 3)}that involve too many Transformer blocks in the\nimage reconstruction, in order to reduce the computational\ncomplexity. The feature aggregation module is illustrated in\nFig. 3. The computational process can be formulated as\n^z′′\n0 = Conv0\n1(^z0); ~z′′\n1 = Conv1\n1(~z1)\n~z′′\n2 = Conv2\n1(~z2); ~z′′\n3 = Conv3\n1(~z3) (9)\n^zf = Conv2([^z′′\n0; ~z′′\n1; ~z′′\n2; ~z′′\n3])\nwhere Convi\n1(·) denotes a 1×1 convolutional layer reducing\nthe channel dimension of the input toC\n4 . Conv2(·) is another\n1 ×1 convolutional layer with C channels fusing the four\nconcatenated input features. ^zf is the fused feature.\nFinally, we input the fused feature^zf to four stride-2 5×5\ndeconvolutional layers gradually increasing the spatial reso-\nlution, leading to the reconstructed RGB image ^x.\nTraining Strategy\nWe observe that the one-step training strategy,i.e., minimiz-\ning (6) to train the encoder and decoder from scratch, leads\nto convergence problem in the experiments. Instead, we em-\nploy a two-step training strategy:\n1) We pretrain the proposed model without considering the\nquantization of z and the hyper-prior module of ^z. We\nremove the rate loss in (6) temporarily, and minimize the\ncross-entropy loss together with the MSE loss. Because\nthe value of the cross-entropy loss is much smaller than\nthat of the MSE loss, we set \u000b= 1and \f = 0:001 in (6)\nto balance the contributions of the two losses.\n2) We load the pretrained parameters and minimize (6) to\ntrain the entire network including the quantization of z\nand the hyper-prior module of ^z. The \u000band \f in (6) are\ntuned with ﬁxed \u000b\n\f to achieve different bit rates.\nExperiments\nExperimental Settings\nDatasets. We perform extensive experiments on the Ima-\ngeNet dataset (Deng et al. 2009) and iNaturalist19 (INat19)\n107\nFigure 3: The network architecture of the decoder. We set C = 384, L = 12 and N = 128. The number of parameters\nis equivalent to ResNet50 (He et al. 2016). The decoder is deployed on the cloud to classify or reconstruct images from the\nreceived bitstreams. AD: Arithmetic Decoder.\ndataset (Horn and Aodha 2019). ImageNet is well-known\nimage classiﬁcation dataset containing 1000 object classes\nwith 1;281;167 training images and 50;000 validation im-\nages. INat19 is a ﬁne-grained classiﬁcation dataset contain-\ning 1010 species of plants and animals with 265;213 train-\ning images and 3030 validation images.\nPretraining w/o Compression. As aforementioned, we\npretrain the proposed model without the quantization of z\nand the hyper-prior module of ^z. We remove the rate loss,\nand minimize the cross entropy loss together with the MSE\nloss. We set \u000b = 1and \f = 0:001, respectively. We set the\ninput size to224×224 and adopt the same data augmentation\nas DeiT (Touvron et al. 2021), except for the Exponential\nMoving Average (EMA) (Polyak and Juditsky 1992), which\ndo not enhance the performance of the proposed model. The\ninput images are normalized with ImageNet default mean\nand standard deviation, and are denormalized during im-\nage reconstruction. We observe that random erasing (Zhong\net al. 2020), mixup (Zhang et al. 2017) and cutmix (Yun et al.\n2019) designed for the training of image classiﬁcation are\nalso compatible with the training of image reconstruction in\nour experiments.\nOn the ImageNet dataset, we train the proposed network\nfrom scratch. We use AdamW optimizer (Loshchilov and\nHutter 2019) for 300 epochs with minibatches of size 1024.\nWe set the initial learning rate to 0:001 and use a cosine\ndecay learning rate scheduler with 5 epochs warm-up.\nOn the INat19 dataset, we initialize the network with the\npretrained parameters on the ImageNet dataset. The classi-\nﬁer head is adjusted to the class number of INat19. We use\nAdamW optimizer for 100 epochs with minibatches of size\n512. We set the initial learning rate to 0:0005 and use a co-\nsine decay learning rate scheduler with 2 epochs warm-up.\nTraining w/ Compression. We load the pretrained param-\neters on the ImageNet and INat19 datasets, respectively. We\nrecover the quantization of z and the hyper-prior module of\n^z. We ﬁx \u000b\n\f = 100and set \u000b ∈{0:1;0:3;0:6}. We observe\nthat the hyper-prior module of^z is sensitive to data augmen-\ntation, and thus we only employ RandomResizedCropAnd-\nInterpolation and RandomHorizontalFlip during training.\nOn the ImageNet dataset, we load the corresponding pre-\ntrained parameters, and use Adam optimizer (Kingma and\nBa 2015) with a initial learning rate of 0:0001, following\n(Ball´e et al. 2018). We train the proposed network for 300\nepochs with minibatches of size 1024, and use a cosine de-\ncay learning rate scheduler with 5 epochs warm-up.\nOn the INat19 dataset, we load the corresponding pre-\ntrained parameters, and also use Adam optimizer with a ini-\ntial learning rate of 0:0001. We train the proposed network\nfor 300 epochs with minibatches of size 512, and use a co-\nsine decay learning rate scheduler with 2 epochs warm-up.\nExperimental Results\nPretrained Model. Table 1(a) reports the experimental re-\nsults of our pretrained model without compression on Ima-\ngeNet. We compare with the existing image classiﬁcation\nmodels including CNN-based models, such as ResNet50\n(He et al. 2016) and RegNetY-4G (Radosavovic et al. 2020),\nand Transformer-based models, such as ViT-B (Dosovitskiy\net al. 2021), DeiT-S (Touvron et al. 2021), CvT-13 (Wu et al.\n2021), CeiT-S (Yuan et al. 2021), Visformer-S (Chen et al.\n2021b), Swin-T (Liu et al. 2021) and ViTC-4GF (Xiao et al.\n2021). We select the speciﬁc settings of the models with the\nnumber of parameters closest to ResNet50.\nTable 1(b) reports the experimental results of our pre-\ntrained model on INat19 dataset, compared with ResNet50,\n108\n(a) Results on ImageNet Dataset\nModel input\nsize\nParams\n(M)\nTop-1\n(%)\nPSNR\n(dB)\nResNet50 224 25.6 75.9 −\nRegNetY-4G 224 20.6 80.0 −\nViT-B 224 86.5 77.9 −\nDeiT-S 224 22.1 79.9 −\nCvT-13 224 20.0 81.6 −\nCeiT-S 224 24.2 82.0 −\nVisformer-S 224 40.2 82.3 −\nSwin-T 224 28.3 81.2 −\nViTC-4GF 224 17.8 81.4 −\nOurs 224 25.6 81.7 31.7\n(b) Results on INat19 Dataset\nModel input\nsize\nParams\n(M)\nTop-1\n(%)\nPSNR\n(dB)\nResNet50 224 25.6 71.8 −\nDeiT-S 224 22.1 76.2 −\nSwin-T 224 28.3 77.9 −\nOurs 224 25.6 78.0 31.5\nTable 1: Results of our pretrained model without compres-\nsion on the ImageNet/INat19 datasets, compared with the\nexisting image classiﬁcation models.\nDeiT-S and Swin-T. ResNet50 is ﬁnetuned based on (He\net al. 2016). DeiT-S and Swin-T are ﬁnetuned using the same\nsetting as our pretrained model.\nFrom Table 1, our pretrained model can achieve compara-\nble or better Top-1 accuracies than the existing image clas-\nsiﬁcation models, which demonstrates the efﬁcacy of the\nimage encoder for the Transformer-based image classiﬁca-\ntion. In terms of image reconstruction evaluated by PSNR,\nour pretrained model achieves 31:7 dB and 31:5 dB, re-\nspectively. All these results demonstrate that the pretrained\nmodel can provide a satisfactory initialization for the follow-\ning training with compression.\nFull Model. In Fig. 4, we report the rate-distortion and\nrate-accuracy results of our full model on ImageNet and\nINat19. We compare with the existing image codecs and the\nimage classiﬁcation models applied to the decoded images.\nWe select the traditional image codecs including JPEG\n(Wallace 1992), JPEG2000 (Skodras, Christopoulos, and\nEbrahimi 2001), BPG (Bellard 2014), and the learning-\nbased image codecs including bmshj (Ball´e et al. 2018) and\nmbt-m (Minnen, Ball ´e, and Toderici 2018). The mbt-m re-\nmoves the serial autoregressive module, avoiding long de-\ncoding time on the large-scale datasets. The sophisticated\nlearning-based image codecs with complex entropy models\nand network architectures, such as (Hu, Yang, and Liu 2020;\nCheng et al. 2020; Qian et al. 2021; Guo et al. 2021), are too\ntime-consuming to be evaluated on the large-scale datasets,\ndespite their good compression performance.\nWe select the decoded images of the best performed tradi-\ntional and learning-based image codecs in our experiments,\ni.e., BPG and mbt-m, and adopt the image classiﬁcation\nmodels ResNet50, DeiT-S and Swin-T to compute the Top-1\naccuracies in comparison with the proposed model. More-\n(a) Rate-distortion and rate-accuracy results on ImageNet.\n(b) Rate-distortion and rate-accuracy results on INat19.\nFigure 4: Rate-distortion and rate-accuracy results of the\nproposed model, compared with the existing image codecs\nand the image classiﬁcation methods applied to the recon-\nstructed RGB images. JFT means joint ﬁnetune.\nover, we jointly ﬁnetune mbt-m together with ResNet50,\nDeit-S and Swin-T by minimizing (6) with \u000b\n\f = 100, same\nas the proposed model.\nIn terms of the rate-distortion performance, the pro-\nposed model signiﬁcantly outperforms the traditional image\ncodecs JPEG and JPEG2000. It is comparable to BPG at\nrelatively low bit-rates but is surpassed by BPG at high bit-\nrates. The proposed model outperforms the learning-based\nimage codecs bmshj and mbt-m. The rate-distortion perfor-\nmance of the jointly ﬁnetuned mbt-m is similar to the origi-\nnal mbt-m. The proposed model also outperforms the jointly\nﬁnetuned mbt-m.\nIn terms of the rate-accuracy performance, the proposed\nmodel outperforms ResNet50, DeiT-S and Swin-T applied\nto the decoded images of BPG and mbt-m, because the im-\nage classiﬁcation models trained on the original datasets\nare not robust to the decoded images at low bit-rates. Al-\nthough Swin-T outperforms DeiT-S on the original datasets\n(Table. 1), DeiT-S outperforms Swin-T on the decoded im-\nages at low bit-rates. After jointly ﬁnetuned, Swin-T sur-\npasses DeiT-S on the decoded images at the corresponding\nbit-rates. The proposed model also outperforms the jointly\n109\n(a) JPEG\n (b) JPEG2000\n (c) BPG\n(d) mbt-m\n (e) mbt-m (JFT)\n (f) Ours\nFigure 5: (a) JPEG+DeiT-S, 29.3dB, 0.60bpp, maraca (×).\n(b) JPEG2000+DeiT-S, 29.17dB, 0.41bpp, can opener (×).\n(c) BPG+DeiT-S, 29.41dB, 0.18bpp, can opener (×). (d)\nmbt-m+DeiT-S, 29.79dB, 0.18bpp, can opener (×). (e)\nmbt-m+Swin-T(JFT), 29.75dB, 0.19bpp, reel (×). (f) Ours,\n29.52dB, 0.19bpp, teddy bear (X).\nﬁnetuned ResNet50, Swin-T and DeiT-S, because the image\nreconstruction constrained by MSE loss damages the high-\nfrequency information effective for the image classiﬁcation.\nThe proposed model bypasses the image reconstruction pro-\ncess and directly do inference from the compressed features,\nwhich can utilize these high-frequency information.\nIn Fig. 5, we show an illustrative example of the exper-\nimental results. Under similar PSNRs, our model achieves\ncomparable or less bit-per-pixel (bpp) and the correct classi-\nﬁcation result, compared with the other methods.\nAblation Studies\nRate-Distortion-Accuracy Trade-off. When the bit-rates\nof ^z is constrained, minimizing (6) with different \u000b\n\f leads to\nbit allocation between image classiﬁcation and reconstruc-\ntion. We empirically set \u000b\n\f ∈ {50;100;200}and test the\nrate-distortion-accuracy trade-off as shown in Fig. 6. We can\nobserve that larger \u000b\n\f leads to better Top-1 accuracy but sac-\nriﬁces PSNR. In contrast, smaller \u000b\n\f leads to better PSNR\nbut lower Top-1 accuracy.\nFeature Aggregation. In Fig. 6, we compare with the pro-\nposed model without the feature aggregation module, i.e.,\ndirectly feeding ^z0 to the deconvolutional neural network\nfor image reconstruction. Although the feature aggregation\nmodule is only applied to the image reconstruction process,\nit can beneﬁt both the image classiﬁcation and reconstruc-\ntion through rate-distortion-accuracy optimization. The re-\nduced bit-rates from the image reconstruction are potentially\nallocated to the image classiﬁcation, leading to the improve-\nment of both tasks. Although the improvement of Top-1 ac-\ncuracy is more obvious than PSNR in Fig. 6, the decrease\nFigure 6: Ablation studies of rate-distortion-accuracy trade-\noff and the feature aggregation (FA) module.\nFigure 7: Ablation study of the computational cost.\nof the cross entropy loss −\u000blog q(y|^z) is actually similar to\nthat of the MSE loss \f∥x−^x∥2\n2 in (6) in the experiments.\nComputational Cost. In Fig. 7, we compare the computa-\ntional cost of the proposed model with the concatenation of\nthe learning-based image codec mbt-m and the image clas-\nsiﬁcation methods including ResNet50, DeiT-S and Swin-T.\nThe architecture of the proposed encoder is similar to the\nlow-rate setting of mbt-m, thus their computational costs of\nimage encoding are similar. In terms of image reconstruc-\ntion on the decoder side, our image reconstructor needs more\nFLOPs than mbt-m due to the feature aggregation module. In\nterms of image classiﬁcation on the decoder side, our image\nclassiﬁer directly performs inference from the compressed\nfeatures without the image reconstruction process, and thus\nneeds far less computational cost compared with the infer-\nence from reconstructed RGB images.\nConclusion\nIn this paper, we learn an end-to-end image compression and\nanalysis model with Transformers, targeting to the cloud-\nbased image classiﬁcation application. At the frontend, a\nCNN-based image encoder extracts compressed features\nfrom raw images and transmits them to the cloud. On the\ncloud, the compressed features injected convolutional induc-\ntive bias are directly fed to the Transformer for image classi-\nﬁcation bypassing image reconstruction. Meanwhile, the in-\ntermediate features of the Transformer capturing global in-\nformation are aggregated with the compressed features for\nimage reconstruction. Experimental results demonstrate the\neffectiveness of the proposed model in both rate-distortion\nand rate-accuracy performance.\n110\nAcknowledgements\nThis work was supported by National Key Research and\nDevelopment Project under Grant 2019YFE0109600, Na-\ntional Natural Science Foundation of China under Grants\n61922027, 61827804, 6207115, 61971165 and U20B2052,\nPCNL Key Project under Grant PCL2021A07, China Post-\ndoctoral Science Foundation under Grant 2020M682826.\nReferences\nAlemi, A. A.; Fischer, I.; Dillon, J. V .; and Murphy, K. 2017.\nDeep variational information bottleneck. In International\nConference on Learning Representations.\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer Nor-\nmalization. arXiv preprint arXiv:1607.06450.\nBall´e, J.; Laparra, V .; and Simoncelli, E. P. 2016. Density\nmodeling of images using a generalized normalization trans-\nformation. In International Conference on Learning Repre-\nsentations.\nBall´e, J.; Minnen, D.; Singh, S.; Hwang, S. J.; and John-\nston, N. 2018. Variational image compression with a scale\nhyperprior. In International Conference on Learning Repre-\nsentations.\nBellard, F. 2014. BPG Image format. https://bellard.org/\nbpg/. Accessed: 2020-09-30.\nChamain, L. D.; Cheung, S.-c. S.; and Ding, Z. 2019. Quan-\nnet: Joint Image Compression and Classiﬁcation Over Chan-\nnels with Limited Bandwidth. In IEEE International Con-\nference on Multimedia and Expo (ICME), 338–343.\nChamain, L. D.; Racap´e, F.; Begaint, J.; Pushparaja, A.; and\nFeltman, S. 2021. End-to-End optimized image compres-\nsion for machines, a study. InData Compression Conference\n(DCC), 163–172.\nChen, H.; Wang, Y .; Guo, T.; Xu, C.; Deng, Y .; Liu, Z.; Ma,\nS.; Xu, C.; Xu, C.; and Gao, W. 2021a. Pre-trained image\nprocessing transformer. In IEEE Conference on Computer\nVision and Pattern Recognition.\nChen, Z.; Xie, L.; Niu, J.; Liu, X.; Wei, L.; and Tian, Q.\n2021b. Visformer: The vision-friendly transformer. arXiv\npreprint arXiv:2104.12533.\nCheng, Z.; Sun, H.; Takeuchi, M.; and Katto, J. 2020.\nLearned image compression with discretized gaussian mix-\nture likelihoods and attention modules. In IEEE Conference\non Computer Vision and Pattern Recognition, 7939–7948.\nChoi, J.; and Han, B. 2020. Task-Aware Quantization Net-\nwork for JPEG Image Compression. In European Confer-\nence on Computer Vision, 309–324. Cham: Springer Inter-\nnational Publishing. ISBN 978-3-030-58565-5.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, 248–255. Ieee.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; and Gelly, S. 2021. An image is worth 16x16\nwords: Transformers for image recognition at scale. In In-\nternational Conference on Learning Representations.\nDuan, L.-Y .; Liu, J.; Yang, W.; Huang, T.; and Gao, W. 2020.\nVideo Coding for Machines: A Paradigm of Collaborative\nCompression and Intelligent Analytics. IEEE Transactions\non Image Processing, 29: 8680–8695.\nDuda, J. 2009. Asymmetric numeral systems.arXiv preprint\narXiv:0902.0271.\nFang, Y .; Liao, B.; Wang, X.; Fang, J.; Qi, J.; Wu, R.; Niu,\nJ.; and Liu, W. 2021. You Only Look at One Sequence:\nRethinking Transformer in Vision through Object Detection.\narXiv preprint arXiv:2106.00666.\nGuo, Z.; Zhang, Z.; Feng, R.; and Chen, Z. 2021. Causal\nContextual Prediction for Learned Image Compression.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In IEEE Conference on\nComputer Vision and Pattern Recognition, 770–778.\nHorn, G. V .; and Aodha, O. M. 2019. iNatural-\nist2019. https://www.kaggle.com/c/inaturalist-2019-fgvc6.\nAccessed: 2021-05-30.\nHu, Y .; Yang, W.; and Liu, J. 2020. Coarse-to-ﬁne hyper-\nprior modeling for learned image compression. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence, vol-\nume 34, 11013–11020.\nHu, Y .; Yang, W.; Ma, Z.; and Liu, J. 2021. Learning end-to-\nend lossy image compression: A benchmark. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence.\nJiang, Y .; Chang, S.; and Wang, Z. 2021. TransGAN: Two\nTransformers Can Make One Strong GAN. arXiv preprint\narXiv:2102.07074.\nKingma, D. P.; and Ba, J. 2015. Adam: A method for\nstochastic optimization. In International Conference on\nLearning Representations.\nLe, N.; Zhang, H.; Cricri, F.; Youvalari, R. G.; and Rahtu, E.\n2021. Image Coding For Machines: an End-To-End Learned\nApproach. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 1590–1594.\nLi, M.; Zuo, W.; Gu, S.; Zhao, D.; and Zhang, D. 2018.\nLearning Convolutional Networks for Content-Weighted\nImage Compression. In IEEE Conference on Computer\nVision and Pattern Recognition, 3214–3223. ISBN 2575-\n7075.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin Transformer: Hierarchical Vi-\nsion Transformer using Shifted Windows. arXiv preprint\narXiv:2103.14030.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In International Conference on Learn-\ning Representations.\nLuo, X.; Talebi, H.; Yang, F.; Elad, M.; and Milanfar, P.\n2021. The Rate-Distortion-Accuracy Tradeoff: JPEG Case\nStudy. In Data Compression Conference (DCC), 354–354.\nMa, H.; Liu, D.; Yan, N.; Li, H.; and Wu, F. 2020. End-to-\nEnd Optimized Versatile Image Compression With Wavelet-\nLike Transform. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 1–1.\n111\nMinnen, D.; Ball´e, J.; and Toderici, G. D. 2018. Joint autore-\ngressive and hierarchical priors for learned image compres-\nsion. In Advances in Neural Information Processing Sys-\ntems, 10771–10780.\nPolyak, B. T.; and Juditsky, A. B. 1992. Acceleration of\nstochastic approximation by averaging. SIAM journal on\ncontrol and optimization, 30(4): 838–855.\nQian, Y .; Tan, Z.; Sun, X.; Lin, M.; Li, D.; Sun, Z.; Li, H.;\nand Jin, R. 2021. Learning Accurate Entropy Model with\nGlobal Reference for Image Compression. In International\nConference on Learning Representations.\nRadosavovic, I.; Kosaraju, R. P.; Girshick, R. B.; He, K.; and\nDoll´ar, P. 2020. Designing Network Design Spaces. IEEE\nConference on Computer Vision and Pattern Recognition ,\n10425–10433.\nShannon, C. E. 1948. A mathematical theory of communi-\ncation. The Bell system technical journal, 27(3): 379–423.\nSkodras, A.; Christopoulos, C.; and Ebrahimi, T. 2001. The\njpeg 2000 still image compression standard. IEEE Signal\nProcessing Magazine, 18(5): 36–58.\nTheis, L.; Shi, W.; Cunningham, A.; and Husz ´ar, F. 2017.\nLossy image compression with compressive autoencoders.\nIn International Conference on Learning Representations.\nTishby, N.; Pereira, F. C.; and Bialek, W. 2000. The infor-\nmation bottleneck method. arXiv preprint physics/0004057.\nToderici, G.; O’Malley, S. M.; Hwang, S. J.; Vincent, D.;\nMinnen, D.; Baluja, S.; Covell, M.; and Sukthankar, R.\n2016. Variable Rate Image Compression with Recurrent\nNeural Networks. In International Conference on Learning\nRepresentations.\nTorfason, R.; Mentzer, F.; Agustsson, E.; Tschannen, M.;\nTimofte, R.; and Gool, L. 2018. Towards Image Understand-\ning from Deep Compression without Decoding. In Interna-\ntional Conference on Learning Representations.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efﬁcient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWallace, G. K. 1992. The JPEG still picture compres-\nsion standard. IEEE Transactions on Consumer Electronics,\n38(1): xviii–xxxiv.\nWang, H.; Wu, X.; Huang, Z.; and Xing, E. P. 2020. High-\nfrequency component helps explain the generalization of\nconvolutional neural networks. In IEEE Conference on\nComputer Vision and Pattern Recognition, 8684–8694.\nWitten, I. H.; Neal, R. M.; and Cleary, J. G. 1987. Arithmetic\ncoding for data compression. Communications of the ACM,\n30(6): 520–540.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. arXiv preprint arXiv:2103.15808.\nXiao, T.; Singh, M.; Mintun, E.; Darrell, T.; Doll ´ar, P.; and\nGirshick, R. 2021. Early Convolutions Help Transformers\nSee Better. arXiv preprint arXiv:2106.14881.\nYuan, K.; Guo, S.; Liu, Z.; Zhou, A.; Yu, F.; and Wu, W.\n2021. Incorporating Convolution Designs into Visual Trans-\nformers. arXiv preprint arXiv:2103.11816.\nYun, S.; Han, D.; Oh, S. J.; Chun, S.; Choe, J.; and Yoo, Y .\n2019. CutMix: Regularization Strategy to Train Strong Clas-\nsiﬁers With Localizable Features. International Conference\non Computer Vision, 6022–6031.\nZhang, H.; Ciss ´e, M.; Dauphin, Y .; and Lopez-Paz, D.\n2017. mixup: Beyond Empirical Risk Minimization. arXiv\npreprint arXiv:1710.09412.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; and Torr, P. H. 2021. Rethinking\nSemantic Segmentation from a Sequence-to-Sequence Per-\nspective with Transformers. In IEEE Conference on Com-\nputer Vision and Pattern Recognition.\nZhong, Z.; Zheng, L.; Kang, G.; Li, S.; and Yang, Y . 2020.\nRandom Erasing Data Augmentation. In AAAI Conference\non Artiﬁcial Intelligence.\n112"
}