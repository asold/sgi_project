{
  "title": "Multi-Task Bidirectional Transformer Representations for Irony Detection",
  "url": "https://openalex.org/W2972070042",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A842301804",
      "name": "Zhang Chiyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222167785",
      "name": "Abdul-Mageed, Muhammad",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3209042722",
    "https://openalex.org/W2805351602",
    "https://openalex.org/W2798990753",
    "https://openalex.org/W2970485137",
    "https://openalex.org/W2027232045",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2914220664",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2890269216",
    "https://openalex.org/W2806962830",
    "https://openalex.org/W2907186736",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2609130030",
    "https://openalex.org/W2101217916",
    "https://openalex.org/W2970377225",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2806962183",
    "https://openalex.org/W3004218475",
    "https://openalex.org/W2970513828",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2945824677",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1614862348",
    "https://openalex.org/W2516255829",
    "https://openalex.org/W2767481019",
    "https://openalex.org/W2805535038",
    "https://openalex.org/W2740340151",
    "https://openalex.org/W2744881172",
    "https://openalex.org/W2906697481",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W2963842982",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W3000128329",
    "https://openalex.org/W2407776548",
    "https://openalex.org/W2806834924",
    "https://openalex.org/W2805089673",
    "https://openalex.org/W2971092004"
  ],
  "abstract": "Supervised deep learning requires large amounts of training data. In the context of the FIRE2019 Arabic irony detection shared task (IDAT@FIRE2019), we show how we mitigate this need by fine-tuning the pre-trained bidirectional encoders from transformers (BERT) on gold data in a multi-task setting. We further improve our models by by further pre-training BERT on `in-domain' data, thus alleviating an issue of dialect mismatch in the Google-released BERT model. Our best model acquires 82.4 macro F1 score, and has the unique advantage of being feature-engineering free (i.e., based exclusively on deep learning).",
  "full_text": "arXiv:1909.03526v3  [cs.CL]  31 Oct 2019\nMulti-Task Bidirectional Transformer\nRepresentations for Irony Detection\nChiyu Zhang and Muhammad Abdul-Mageed\nNatural Language Processing Lab\nThe University of British Columbia\nchiyuzh@mail.ubc.ca, muhammad.mageeed@ubc.ca\nAbstract. Supervised deep learning requires large amounts of trainin g\ndata. In the context of the FIRE2019 Arabic irony detection shared task\n(IDAT@FIRE2019), we show how we mitigate this need by ﬁne-tu ning\nthe pre-trained bidirectional encoders from transformers (BERT) on gold\ndata in a multi-task setting. We further improve our models b y further\npre-training BERT on ‘in-domain’ data, thus alleviating an issue of di-\nalect mismatch in the Google-released BERT model. Our best m odel\nacquires 82.4 macro F1 score, and has the unique advantage of being\nfeature-engineering free (i.e., based exclusively on deep learning).\nKeywords: irony detection, Arabic, social media, BERT, multi-task\nlearning\n1 Introduction\nThe proliferation of social media has provided a locus for use, and th ereby col-\nlection, of ﬁgurative and creative language data, including irony [18]. According\nto the Merriam-Webster online dictionary, 1 irony refers to “the use of word to\nexpress something other than and especially the opposite of the lite ral meaning.”\nA complex, controversial, and intriguing linguistic phenomenon, irony has been\nstudied in disciplines such as linguistics, philosophy, and rhetoric. Iro ny detec-\ntion also has implications for several NLP tasks such as sentiment an alysis, hate\nspeech detection, fake news detection, etc [18]. Hence, automat ic irony detection\ncan potentially improve systems designed for each of these tasks. In this paper,\nwe focus on learning irony. More speciﬁcally, we report our work sub mitted to\nthe FIRE 2019 Arabic irony detection task (IDAT@FIRE2019). 2 We focus our\nenergy on an important angle of the problem–the small size of trainin g data.\nDeep learning is the most successful under supervised conditions w ith large\namounts of training data (tens-to-hundreds of thousands of ex amples). For most\nreal-world tasks, we hard to obtain labeled data. Hence, it is highly de sir-\nable to eliminate, or at least reduce, dependence on supervision. In NLP, pre-\ntraining language models on unlabeled data has emerged as a success ful approach\n1 https://www.merriam-webster.com/dictionary/irony.\n2 https://www.irit.fr/IDAT2019/\n2 Zhang and Abdul-Mageed\nfor improving model performance. In particular, the pre-trained multilingual\nBidirectional Encoder Representations from Transformers (BERT) [15] was in-\ntroduced to learn language regularities from unlabeled data. Multi-t ask learning\n(MTL) is another approach that helps achieve inductive transfer b etween vari-\nous tasks. More speciﬁcally, MTL leverages information from one or more source\ntasks to improve a target task [10,11]. In this work, we introduce T ransformer\nrepresentations (BERT) in an MTL setting to address the data bot tleneck in\nIDAT@FIRE2019. To show the utility of BERT, we compare to a simpler model\nwith gated recurrent units (GRU) in a single task setting. To identify the utility,\nor lack thereof, of MTL BERT, we compare to a single task BERT mode l. For\nMTL BERT, we train on a number of tasks simultaneously. Tasks we tr ain on\nare sentiment analysis , gender detection, age detection, dialect identiﬁcation, and\nemotion detection .\nAnother problem we face is that the BERT model released by Google is\ntrained only on Arabic Wikipedia, which is almost exclusively Modern Stan dard\nArabic (MSA). This introduces a language variety mismatch due to th e irony\ndata involving a number of dialects that come from the Twitter domain . To\nmitigate this issue, we further pre-train BERT on an in-house dialect al Twitter\ndataset, showing the utility of this measure. To summarize, we make the following\ncontributions:\n– In the context of the Arabic irony task, we show how a small-sized lab eled\ndata setting can be mitigated by training models in a multi-task learning\nsetup.\n– We view diﬀerent varieties of Arabic as diﬀerent domains, and hence in tro-\nduce a simple, yet eﬀective, ‘in-domain’ training measure where we fu rther\npre-train BERT on a dataset closer to task domain (in that it involves di-\nalectal tweet data).\n2 Methods\n2.1 GRU\nFor our baseline, we use gated recurrent units (GRU) [12], a simpliﬁca tion of\nlong-short term memory (LSTM) [21], which in turn is a variation of re current\nneural networks (RNNs). A GRU learns based on the following:\nh(t) =\n(\n1 − z (t)\n)\nh(t− 1) + z (t)˜h\n(t)\n(1)\nwhere the update state z (t) decides how much the unit updates its content:\nz (t) = σ\n(\nWz x (t) + Uzh(t− 1)\n)\n(2)\nMulti-Task Bidirectional Transformer Representations for Irony Detection 3\nwhere W and U are weight matrices. The candidate activation makes u se of\na reset gate r(t):\n˜h\n(t)\n= tanh\n(\nW x (t) + r(t) ⊙\n(\nUh(t− 1)\n))\n(3)\nwhere ⊙ is a Hadamard product (element-wise multiplication). When its\nvalue is close to zero, the reset gate allows the unit to forget the previously\ncomputed state. The reset gate r(t) is computed as follows:\n˜r(t) = σ\n(\nWrx (t) + Urh(t− 1)\n)\n(4)\n2.2 BERT\nBERT [15] is based on the Transformer [36], a network architecture that de-\npends solely on encoder-decoder attention. The Transformer at tention employs\na function operating on queries, keys, and values. This attention function maps\na query and a set of key-value pairs to an output, where the outpu t is a weighted\nsum of the values. Encoder of the Transformer in [36] has 6 attention layers, each\nof which is composed of two sub-layers: (1) multi-head attention where queries,\nkeys, and values are projected h times into linear, learned projections and ulti-\nmately concatenated; and (2) fully-connected feed-forward network (FFN) that\nis applied to each position separately and identically. Decoder of the Trans-\nformer also employs 6 identical layers, yet with an extra sub-layer t hat performs\nmulti-head attention over the encoder stack. The architecture o f BERT [15] is\na multi-layer bidirectional Transformer encoder [36]. It uses maske d language\nmodels to enable pre-trained deep bidirectional representations, in addition to\na binary next sentence prediction task captures context (i.e., sentence relation-\nships). More information about BERT can be found in [15].\n2.3 Multi-task Learning\nIn multi-task learning (MTL), a learner uses a number of (usually rele vant) tasks\nto improve performance on a target task [10,11]. The MTL setup ena bles the\nlearner to use cues from various tasks to improve the performanc e on the target\ntask. MTL also usually helps regularize the model since the learner ne eds to\nﬁnd representations that are not speciﬁc to a single task, but rat her more gen-\neral. Supervised learning with deep neural networks requires large amounts of\nlabeled data, which is not always available. By employing data from addit ional\ntasks, MTL thus practically augments training data to alleviate need for large la-\nbeled data. Many researchers achieve state-of-the-art result s by employing MTL\nin supervised learning settings [20,25]. In speciﬁc, BERT was success fully used\nwith MTL. Hence, we employ multi-task BERT (following [25]). For our tr ain-\ning, we use the same pre-trained BERT-Base Multilingual Cased mode l as the\ninitial checkpoint. For this MTL pre-training of BERT, we use the sam e afore-\nmentioned single-task BERT parameters. We now describe our data .\n4 Zhang and Abdul-Mageed\n3 Data\nThe shared task dataset contains 5,030 tweets related to diﬀeren t political issues\nand events in the Middle East taking place between 2011 and 2018. Tw eets are\ncollected using pre-deﬁned keywords (i.e. targeted political ﬁgure s or events)\nand the positive class involves ironic hashtags such as #sokhria, #ta hakoum,\nand #maskhara (Arabic variants for “irony”). Duplicates, retwee ts, and non-\nintelligible tweets are removed by organizers. Tweets involve both MS A as well as\ndialects at various degrees of granularity such as Egyptian, Gulf, and Levantine.\nIDAT@FIRE2019 [17] is set up as a binary classiﬁcation task where tw eets\nare assigned labels from the set {ironic, non-ironic}. A total of 4,024 tweets were\nreleased by organizers as training data. In addition, 1,006 tweets w ere used by\norganizers as test data. Test labels were not release; and teams w ere expected\nto submit the predictions produced by their systems on the test sp lit. For our\nmodels, we split the 4,024 released training data into 90% TRAIN ( n=3,621\ntweets; ‘ironic’=1,882 and ‘non-ironic’=1,739) and 10% DEV ( n=403 tweets;\n‘ironic’=209 and ‘non-ironic’=194). We train our models on TRAIN, and e valu-\nate on DEV.\nOur multi-task BERT models involve six diﬀerent Arabic classiﬁcation ta sks.\nWe brieﬂy introduce the data for these tasks here:\n– Author proﬁling and deception detection in Arabic (APDA). [28] 3.\nFrom APDA, we only use the corpus of author proﬁling (which includes\nthe three proﬁling tasks of age, gender , and variety). The organizers of\nAPDA provide 225,000 tweets as training data. Each tweet is labelled w ith\nthree tags (one for each task). To develop our models, we split the train-\ning data into 90% training set ( n=202,500 tweets) and 10% development\nset ( n=22,500 tweets). With regard to age, authors consider tweets of three\nclasses: {Under 25 , Between 25 and 34 , and Above 35 }. For the Arabic\nvarieties, they consider the following ﬁfteen classes: {Algeria, Egypt, Iraq,\nKuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar,\nSaudi Arabia , Sudan, Tunisia, UAE, Yemen}. Gender is labeled as a binary\ntask with {male,female} tags.\n– LAMA+DINA Emotion detection. Alhuzali et al. [7] introduce LAMA,\na dataset for Arabic emotion detection. They use a ﬁrst-person s eed phrase\napproach and extend work by Abdul-Mageed et al. [4] for emotion da ta\ncollection from 6 to 8 emotion categories (i.e. anger, anticipation, disgust,\nfear, joy, sadness, surprise and trust). We use the combined LAMA+DINA\ncorpus. It is split by the authors as 189,902 tweets training set, 910 as de-\nvelopment, and 941 as test. In our experiment, we use only the training set\nfor out MTL experiments.\n– Sentiment analysis in Arabic tweets. This dataset is a shared task on\nKaggle by Motaz Saad 4. The corpus contains 58,751 Arabic tweets (46,940\n3 https://www.autoritas.net/APDA/\n4 https://www.kaggle.com/mksaad/arabic-sentiment-twitter-corpus\nMulti-Task Bidirectional Transformer Representations for Irony Detection 5\ntraining, and 11,811 test). The tweets are annotated with positive and neg-\native labels based on an emoji lexicon.\n4 Models\n4.1 GRU\nWe train a baseline GRU network with our irony TRAIN data. This netwo rk\nhas only one layer unidirectional GRU, with 500 unites and a linear, out put\nlayer. The input word tokens are embedded by the trainable word ve ctors which\nare initialized with a standard normal distribution, with µ = 0, and σ = 1,\ni.e., W ∼ N(0, 1). We use Adam [23] with a ﬁxed learning rate of 1 e − 3 for\noptimization. For regularization, we use dropout [33] with a rate of 0 .5 on the\nhidden layer. We set the maximum sequence sequence in our GRU mode l to 50\nwords, and use all 22,000 words of training set as vocabulary. We em ploy batch\ntraining with a batch size of 64 for this model. We run the network for 20 epochs\nand save the model at the end of each epoch, choosing the model t hat performs\nhighest on DEV as our best model. We report our best result on DEV in Table\n1. Our best result is acquired with 12 epochs. As Table 1 shows, the b aseline\nobtains accuracy = 73 . 70% and F1 = 73 . 47.\n4.2 Single-Task BERT\nWe use the BERT-Base Multilingual Cased model released by the auth ors [15] 5.\nThe model is trained on 104 languages (including Arabic) with 12 layers , 768\nhidden units each, 12 attention heads. The entire model has 110M p arameters.\nThe model has 119,547 shared WordPieces vocabulary, and was pre -trained on\nthe entire Wikipedia for each language. For ﬁne-tuning, we use a max imum\nsequence size of 50 tokens and a batch size of 32. We set the learnin g rate to\n2e − 5 and train for 20 epochs. For single-task learning, we ﬁne-tune BE RT on\nthe training set (i.e., TRAIN) of the irony task exclusively. We refer t o this\nmodel as BERT-ST, ST standing for ‘single task.’ As Table 1 shows, BERT-ST\nunsurprisingly acquires better performance than the baseline GRU model. On\naccuracy, BERT-ST is 7.94% better than the baseline. BERT-ST obt ains 81.62\nF1 which is 7.35 better than the baseline.\n4.3 Multi-Task BERT\nWe follow the work of Liu et al. [25] for training an MTL BERT in that we ﬁ ne-\ntune the afore-mentioned BERT-Base Multilingual Cased model with diﬀerent\ntasks jointly. First, we ﬁne-tune with the three tasks of author p roﬁling and the\nirony task simultaneously. We refer to this model trained on the 4 ta sks simply as\nBERT-MT4. BERT-MT5 refers to the model ﬁne-tuned on the 3 aut hor proﬁling\ntasks, the emotion task, and the irony task. We also refer to the m odel ﬁne-tuned\n5 https://github.com/google-research/bert/blob/master/multilingual.md.\n6 Zhang and Abdul-Mageed\non all six tasks (adding the sentiment task mentioned earlier) as BER T-MT6.\nFor MTL BERT, we use the same parameters as the single task BERT lis ted in\nthe previous sub-section (i.e., Single-Task BERT ). In Table 1, we present the\nperformance on the DEV set of only the irony detection task. 6 We note that\nall the results of multitask learning with BERT are better than those with the\nsingle task BERT. The model trained on all six tasks obtains the best result,\nwhich is 2.23% accuracy and 2.25% F1 higher than the single task BERT model.\nTable 1. Model Performance\nModel Acc F1\nGRU 0.7370 0.7347\nBERT-ST 0.8164 0.8162\nBERT-MT4 0.8189 0.8187\nBERT-MT5 0.8362 0.8359\nBERT-MT6 0.8387 0.8387\nBERT-1M-MT5 0.8437 0.8434\nBERT-1M-MT6 0.8362 0.8360\n4.4 In-Domain Pre-Training\nOur irony data involves dialects such as Egyptian, Gulf, and Levantin e, as we\nexplained earlier. The BERT-Base Multilingual Cased model we used, h owever,\nwas trained on Arabic Wikipedia, which is mostly MSA. We believe this dialec t\nmismatch is sub-optimal. As Sun et al. [34] show, further pre-trainin g with do-\nmain speciﬁc data can improve performance of a learner. Viewing diale cts as\nconstituting diﬀerent domains, we turn to dialectal data to furthe r pre-train\nBERT. Namely, we use 1M tweets randomly sampled from an in-house T witter\ndataset to resume pre-training BERT before we ﬁne-tune on the ir ony data. 7\nWe use BERT-Base Multilingual Cased model as an initial checkpoint an d pre-\ntrain on this 1M dataset with a learning rate of 2 e − 5, for 10 epochs. Then, we\nﬁne-tune on MT5 (and then on MT6) with the new further-pre-trained BERT\nmodel. We refer to the new models as BERT-1M-MT5 and BERT-1M-MT 6,\nrespectively. As Table 1 shows, BERT-1M-MT5 performs best: BER T-1M-MT5\nobtains 84.37% accuracy (0.5% less than BERT-MT6) and 83.34 F1 (0.47% less\nthan BERT-MT6).\n6 We do not list acquired results on other tasks, since the focu s of this paper is exclu-\nsively the IDAT@FIRE2019 shared task.\n7 A nuance is that we require each tweet in the 1M dataset to be > 20 words long,\nand so this process is not entirely random.\nMulti-Task Bidirectional Transformer Representations for Irony Detection 7\n4.5 IDAT@FIRE2019 Submission\nFor the shared task submission, we use the predictions of BERT-1M -MT5 as\nour ﬁrst submitted system. Then, we concatenate our DEV and TR AIN data to\ncompose a new training set (thus using all the training data released by orga-\nnizers) to re-train BERT-1M-MT5 and BERT-MT6 with the same para meters.\nWe use the predictions of these two models as our second and third s ubmissions.\nOur second submission obtains 82.4 F1 on the oﬃcial test set, and ranks 4 th on\nthis shared task.\n5 Related Work\nMulti-Task Learning. MTL has been eﬀectively used to model several NLP\nproblems. These include, for example, syntactic parsing [26], seque nce label-\ning [32,30], and text classiﬁcation [24].\nIrony in diﬀerent languages. Irony detection has been investigated in\nvarious languages. For example, Hee et al. [35] propose two irony de tection tasks\nin English tweets. Task A is a binary classiﬁcation task ( irony vs. non-irony), and\nTask B is multi-class identiﬁcation of a speciﬁc type of irony from the s et {verbal,\nsituational, other-irony, non-ironic }. They use hashtags to automatically collect\ntweets that they manually annotate using a ﬁne-grained annotatio n scheme.\nParticipants in this competition construct models based on logistic re gression\nand support vector machine (SVM) [31], XGBoost [29], convolutional neural\nnetworks (CNNs) [29], long short-term memory networks (LSTMs) [37], etc. For\nthe Italian language, Cignarella et al. propose the IronTA shared ta sk [13], and\nthe best system [14] is a combination of bi-directional LSTMs, word n-grams, and\naﬀective lexicons. For Spanish, Ortega-Bueno1 et al. [27] introduc e the IroSvA\nshared task, a binary classiﬁcation task for tweets and news comm ents. The best-\nperforming model on the task, [19], employs pre-trained Word2Vec , multi-head\nTransformer encoder and a global average pooling mechanism.\nIrony in Arabic. Arabic is a widely spoken collection of languages ( ∼ 300\nmillion native speakers) [3,38]. A large amount of works in Arabic are tho se\nfocusing on other text classiﬁcation tasks such as sentiment analy sis [5,6,2,1],\nemotion [7], and dialect identiﬁcation [38,16,8,9]. Karoui et al. [22] creat ed a\nArabic irony detection corpus of 5,479 tweets. They use pre-deﬁn ed hashtags\nto obtain irony tweets related to the US and Egyptian presidential e lections.\nIDAT@FIRE2019 [17] aims at augmenting the corpus and enriching th e topics,\ncollecting more tweets within a wider region (the Middle East) and over a longer\nperiod (between 2011 and 2018).\n6 Conclusion\nIn this paper, we described our submissions to the Irony Detection in Arabic\nshared task (IDAT@FIRE2019). We presented how we acquire eﬀe ctive models\nusing pre-trained BERT in a multi-task learning setting. We also showe d the\n8 Zhang and Abdul-Mageed\nutility of viewing diﬀerent varieties of Arabic as diﬀerent domains by re porting\nbetter performance with models pre-trained with dialectal data ra ther than ex-\nclusively on MSA. Our multi-task model with domain-speciﬁc BERT rank s 4th in\nthe oﬃcial IDAT@FIRE2019 evaluation. The model has the advanta ge of being\nexclusively based on deep learning. In the future, we will investigate other multi-\ntask learning architectures, and extend our work with semi-super vised methods.\n7 Acknowledgement\nWe acknowledge the support of the Natural Sciences and Engineer ing Research\nCouncil of Canada (NSERC), the Social Sciences Research Council of Canada\n(SSHRC), and Compute Canada ( www.computecanada.ca).\nReferences\n1. Abdul-Mageed, M.: Modeling arabic subjectivity and sent iment in lexical space.\nInformation Processing & Management (2017)\n2. Abdul-Mageed, M.: Not all segments are created equal: Syn tactically motivated\nsentiment analysis in lexical space. In: Proceedings of the third Arabic natural\nlanguage processing workshop. pp. 147–156 (2017)\n3. Abdul-Mageed, M., Alhuzali, H., Elaraby, M.: You tweet wh at you speak: A city-\nlevel dataset of arabic dialects. In: LREC. pp. 3653–3659 (2 018)\n4. Abdul-Mageed, M., AlHuzli, H., DuaaAbu Elhija, M.D.: Din a: A multidialect\ndataset for arabic emotion analysis. In: The 2nd Workshop on Arabic Corpora\nand Processing Tools. p. 29 (2016)\n5. Abdul-Mageed, M., Diab, M., K¨ ubler, S.: Samar: Subjectivity and sentiment anal-\nysis for arabic social media. Computer Speech & Language 28(1), 20–37 (2014)\n6. Al-Ayyoub, M., Khamaiseh, A.A., Jararweh, Y., Al-Kabi, M .N.: A comprehensive\nsurvey of arabic sentiment analysis. Information Processing & Management 56(2),\n320–342 (2019)\n7. Alhuzali, H., Abdul-Mageed, M., Ungar, L.: Enabling deep learning of emo-\ntion with ﬁrst-person seed expressions. In: Proceedings of the Second Work-\nshop on Computational Modeling of People’s Opinions, Perso nality, and Emo-\ntions in Social Media. pp. 25–35. Association for Computati onal Linguistics,\nNew Orleans, Louisiana, USA (Jun 2018). https://doi.org/10.18653/v1/W18-1104,\nhttps://www.aclweb.org/anthology/W18-1104\n8. Bouamor, H., Habash, N., Salameh, M., Zaghouani, W., Rambow, O., Abdulrahim,\nD., Obeid, O., Khalifa, S., Eryani, F., Erdmann, A., et al.: The madar arabic dialect\ncorpus and lexicon. In: Proceedings of the Eleventh Interna tional Conference on\nLanguage Resources and Evaluation (LREC-2018) (2018)\n9. Bouamor, H., Hassan, S., Habash, N.: The madar shared task on arabic ﬁne-grained\ndialect identiﬁcation. In: Proceedings of the Fourth Arabic Natural Language Pro-\ncessing Workshop. pp. 199–207 (2019)\n10. Caruana, R.: Multitask learning: A knowledge-based sou rce of inductive bias. In:\nProceedings of the 10th International Conference on Machin e Learning (1993)\n11. Caruana, R.: Multitask learning. Machine learning 28(1), 41–75 (1997)\nMulti-Task Bidirectional Transformer Representations for Irony Detection 9\n12. Cho, K., Van Merri¨ enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,\nH., Bengio, Y.: Learning phrase representations using rnn e ncoder-decoder for\nstatistical machine translation. arXiv preprint arXiv:14 06.1078 (2014)\n13. Cignarella, A.T., Frenda, S., Basile, V., Bosco, C., Pat ti, V., Rosso, P., et al.:\nOverview of the evalita 2018 task on irony detection in itali an tweets (ironita). In:\nSixth Evaluation Campaign of Natural Language Processing a nd Speech Tools for\nItalian (EVALITA 2018). vol. 2263, pp. 1–6. CEUR-WS (2018)\n14. Cimino, A., De Mattei, L., Dell’Orletta, F.: Multi-task learning in deep neural\nnetworks at evalita 2018. In: EVALITA@ CLiC-it (2018)\n15. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv pre print arXiv:1810.04805\n(2018)\n16. Elaraby, M., Abdul-Mageed, M.: Deep models for arabic di alect identiﬁcation on\nbenchmarked data. In: Proceedings of the Fifth Workshop on N LP for Similar\nLanguages, Varieties and Dialects (VarDial 2018). pp. 263– 274 (2018)\n17. Ghanem, B., Karoui, J., Benamara, F., Moriceau, V., Ross o, P.: Idat@ﬁre2019:\nOverview of the track on irony detection in arabic tweets. In : Mehta P., Rosso P.,\nMajumder P., Mitra M. (Eds.) Working Notes of the Forum for In formation Re-\ntrieval Evaluation (FIRE 2019). CEUR Workshop Proceedings. In: CEUR-WS.org,\nKolkata, India, December 12-15 (2019)\n18. Ghosh, A., Li, G., Veale, T., Rosso, P., Shutova, E., Barn den, J., Reyes, A.:\nSemeval-2015 task 11: Sentiment analysis of ﬁgurative lang uage in twitter. In:\nProceedings of the 9th International Workshop on Semantic E valuation (SemEval\n2015). pp. 470–478 (2015)\n19. Gonz´ alez, J., Hurtado, L.F., Pla, F.: Elirf-upv at iros va: Transformer encoders\nfor spanish irony detection. In: Proceedings of the Iberian Languages Evaluation\nForum (IberLEF 2019), co-located with 34th Conference of th e Spanish Society\nfor Natural Language Processing (SEPLN 2019). CEUR Worksho p Proceedings.\nCEUR-WS. org, Bilbao, Spain (2019)\n20. Guo, H., Pasunuru, R., Bansal, M.: Soft layer-speciﬁc mu lti-task summarization\nwith entailment and question generation. arXiv preprint ar Xiv:1805.11004 (2018)\n21. Hochreiter, S., Schmidhuber, J.: Long short-term memor y. Neural computation\n9(8), 1735–1780 (1997)\n22. Karoui, J., Zitoune, F.B., Moriceau, V.: Soukhria: Towa rds an irony detection\nsystem for arabic in social media. Procedia Computer Science 117, 161–168 (2017)\n23. Kingma, D., Ba, J.: Adam: A method for stochastic optimiz ation. arXiv preprint\narXiv:1412.6980 (2014)\n24. Liu, P., Qiu, X., Huang, X.: Recurrent neural network for text classiﬁcation with\nmulti-task learning. arXiv preprint arXiv:1605.05101 (20 16)\n25. Liu, X., He, P., Chen, W., Gao, J.: Multi-task deep neural networks for natural\nlanguage understanding. arXiv preprint arXiv:1901.11504 (2019)\n26. Luong, M.T., Le, Q.V., Sutskever, I., Vinyals, O., Kaise r, L.: Multi-task sequence\nto sequence learning. arXiv preprint arXiv:1511.06114 (20 15)\n27. Ortega-Bueno, R., Rangel, F., Hern´ andez Farıas, D., Rosso, P., Montes-y G´ omez,\nM., Medina Pagola, J.E.: Overview of the task on irony detect ion in spanish vari-\nants. In: Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2019),\nco-located with 34th Conference of the Spanish Society for N atural Language Pro-\ncessing (SEPLN 2019). CEUR-WS. org (2019)\n28. Rangel, F., Rosso, P., Charﬁ, A., Zaghouani, W., Ghanem, B., Snchez-Junquera,\nJ.: Overview of the track on author proﬁling and deception de tection in arabic. In:\n10 Zhang and Abdul-Mageed\nMehta P., Rosso P., Majumder P., Mitra M. (Eds.) Working Note s of the Forum\nfor Information Retrieval Evaluation (FIRE 2019). CEUR Workshop Proceedings.\nIn: CEUR-WS.org, Kolkata, India, December 12-15 (2019)\n29. Rangwani, H., Kulshreshtha, D., Singh, A.K.: Nlprl-iit bhu at semeval-2018 task\n3: Combining linguistic features and emoji pre-trained cnn for irony detection in\ntweets. In: Proceedings of The 12th International Workshop on Semantic Evalua-\ntion. pp. 638–642 (2018)\n30. Rei, M.: Semi-supervised multitask learning for sequen ce labeling. arXiv preprint\narXiv:1704.07156 (2017)\n31. Rohanian, O., Taslimipoor, S., Evans, R., Mitkov, R.: Wl v at semeval-2018 task\n3: Dissecting tweets in search of irony. In: Proceedings of T he 12th International\nWorkshop on Semantic Evaluation. pp. 553–559 (2018)\n32. Søgaard, A., Goldberg, Y.: Deep multi-task learning with low level tasks supervised\nat lower layers. In: Proceedings of the 54th Annual Meeting o f the Association for\nComputational Linguistics (Volume 2: Short Papers). vol. 2 , pp. 231–235 (2016)\n33. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:\nDropout: a simple way to prevent neural networks from overﬁt ting. The Journal\nof Machine Learning Research 15(1), 1929–1958 (2014)\n34. Sun, C., Qiu, X., Xu, Y., Huang, X.: How to ﬁne-tune bert fo r text classiﬁcation?\narXiv preprint arXiv:1905.05583 (2019)\n35. Van Hee, C., Lefever, E., Hoste, V.: Semeval-2018 task 3: Irony detection in en-\nglish tweets. In: Proceedings of The 12th International Wor kshop on Semantic\nEvaluation. pp. 39–50 (2018)\n36. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n/suppress L., Polosukhin, I.: Attention is all you need. In: Advancesin Neural Information\nProcessing Systems. pp. 6000–6010 (2017)\n37. Wu, C., Wu, F., Wu, S., Liu, J., Yuan, Z., Huang, Y.: Thu ngn at semeval-2018\ntask 3: Tweet irony detection with densely connected lstm and multi-task learning.\nIn: Proceedings of The 12th International Workshop on Seman tic Evaluation. pp.\n51–56 (2018)\n38. Zhang, C., Abdul-Mageed, M.: No army, no navy: Bert semi- supervised learning of\narabic dialects. In: Proceedings of the Fourth Arabic Natural Language Processing\nWorkshop. pp. 279–284 (2019)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7213479280471802
    },
    {
      "name": "Transformer",
      "score": 0.7173900604248047
    },
    {
      "name": "Encoder",
      "score": 0.6794801354408264
    },
    {
      "name": "Feature engineering",
      "score": 0.6135097742080688
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5896937251091003
    },
    {
      "name": "Arabic",
      "score": 0.5159002542495728
    },
    {
      "name": "Task (project management)",
      "score": 0.5043553113937378
    },
    {
      "name": "Natural language processing",
      "score": 0.49581941962242126
    },
    {
      "name": "Deep learning",
      "score": 0.45037922263145447
    },
    {
      "name": "Machine learning",
      "score": 0.4490165114402771
    },
    {
      "name": "Training set",
      "score": 0.4133252203464508
    },
    {
      "name": "Speech recognition",
      "score": 0.3543824553489685
    },
    {
      "name": "Linguistics",
      "score": 0.14025098085403442
    },
    {
      "name": "Voltage",
      "score": 0.12898066639900208
    },
    {
      "name": "Engineering",
      "score": 0.11349302530288696
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": []
}