{
  "title": "Identification of tools used to assess the external validity of randomized controlled trials in reviews: a systematic review of measurement properties",
  "url": "https://openalex.org/W4223569629",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2277209610",
      "name": "Andres Jung",
      "affiliations": [
        "University of Lübeck"
      ]
    },
    {
      "id": "https://openalex.org/A2156954821",
      "name": "Julia Balzer",
      "affiliations": [
        "European University of Applied Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2098545391",
      "name": "Tobias Braun",
      "affiliations": [
        "Hochschule für Gesundheit - University of Applied Sciences",
        "HSD Hochschule Döpfer"
      ]
    },
    {
      "id": "https://openalex.org/A2044110821",
      "name": "Kerstin Luedtke",
      "affiliations": [
        "University of Lübeck"
      ]
    },
    {
      "id": "https://openalex.org/A2277209610",
      "name": "Andres Jung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156954821",
      "name": "Julia Balzer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098545391",
      "name": "Tobias Braun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2044110821",
      "name": "Kerstin Luedtke",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2146668368",
    "https://openalex.org/W4238666307",
    "https://openalex.org/W2755459284",
    "https://openalex.org/W2033890227",
    "https://openalex.org/W2994704700",
    "https://openalex.org/W2974111596",
    "https://openalex.org/W2511605419",
    "https://openalex.org/W3086629998",
    "https://openalex.org/W2126629865",
    "https://openalex.org/W2022239173",
    "https://openalex.org/W2889456626",
    "https://openalex.org/W2145975717",
    "https://openalex.org/W2083474731",
    "https://openalex.org/W2052820909",
    "https://openalex.org/W1915515888",
    "https://openalex.org/W2909919470",
    "https://openalex.org/W2108737279",
    "https://openalex.org/W2809266094",
    "https://openalex.org/W2032026400",
    "https://openalex.org/W2015712245",
    "https://openalex.org/W2114488753",
    "https://openalex.org/W3123893780",
    "https://openalex.org/W2732703188",
    "https://openalex.org/W2779791612",
    "https://openalex.org/W2793320726",
    "https://openalex.org/W2790424199",
    "https://openalex.org/W3034318717",
    "https://openalex.org/W3178159557",
    "https://openalex.org/W3041971279",
    "https://openalex.org/W3117780856",
    "https://openalex.org/W2171762906",
    "https://openalex.org/W6943911177",
    "https://openalex.org/W2003397926",
    "https://openalex.org/W2966934336",
    "https://openalex.org/W1984212773",
    "https://openalex.org/W2762494750",
    "https://openalex.org/W2937633338",
    "https://openalex.org/W600840809",
    "https://openalex.org/W2560438049",
    "https://openalex.org/W4247903468",
    "https://openalex.org/W2157427387",
    "https://openalex.org/W2056785271",
    "https://openalex.org/W2117505138",
    "https://openalex.org/W2587436624",
    "https://openalex.org/W2142171035",
    "https://openalex.org/W2126009758",
    "https://openalex.org/W1685438967",
    "https://openalex.org/W2041828149",
    "https://openalex.org/W2134671054",
    "https://openalex.org/W2055767022",
    "https://openalex.org/W1965074153",
    "https://openalex.org/W2962535643",
    "https://openalex.org/W1981920930",
    "https://openalex.org/W2979728113",
    "https://openalex.org/W2588520113",
    "https://openalex.org/W2329828496",
    "https://openalex.org/W604279218",
    "https://openalex.org/W2167445077",
    "https://openalex.org/W2129950400",
    "https://openalex.org/W7073905444",
    "https://openalex.org/W2029671258",
    "https://openalex.org/W4211250500",
    "https://openalex.org/W2063339427",
    "https://openalex.org/W2123364617",
    "https://openalex.org/W1887931550",
    "https://openalex.org/W2110777313",
    "https://openalex.org/W2040242013",
    "https://openalex.org/W2991257326",
    "https://openalex.org/W1837422480",
    "https://openalex.org/W2168257212",
    "https://openalex.org/W1770229812",
    "https://openalex.org/W2005084763",
    "https://openalex.org/W2048504229",
    "https://openalex.org/W3127782791",
    "https://openalex.org/W2091267779",
    "https://openalex.org/W1840938196",
    "https://openalex.org/W2057212716",
    "https://openalex.org/W2062677551",
    "https://openalex.org/W2045910459",
    "https://openalex.org/W2167619051",
    "https://openalex.org/W2120079020",
    "https://openalex.org/W2607413575",
    "https://openalex.org/W2019981067",
    "https://openalex.org/W2895872414",
    "https://openalex.org/W2171111655",
    "https://openalex.org/W2201094377",
    "https://openalex.org/W3024080019",
    "https://openalex.org/W2147117340",
    "https://openalex.org/W2162163297",
    "https://openalex.org/W1987117090",
    "https://openalex.org/W2753047867",
    "https://openalex.org/W2017233206",
    "https://openalex.org/W1498899738",
    "https://openalex.org/W2005598996",
    "https://openalex.org/W1988897250",
    "https://openalex.org/W4240787498",
    "https://openalex.org/W2290237287",
    "https://openalex.org/W3087770431",
    "https://openalex.org/W3012473070",
    "https://openalex.org/W1660053632",
    "https://openalex.org/W2480133395",
    "https://openalex.org/W1168171428",
    "https://openalex.org/W1730782591",
    "https://openalex.org/W2035343397",
    "https://openalex.org/W2064296154",
    "https://openalex.org/W2032541700",
    "https://openalex.org/W2167728471"
  ],
  "abstract": "Abstract Background Internal and external validity are the most relevant components when critically appraising randomized controlled trials (RCTs) for systematic reviews. However, there is no gold standard to assess external validity. This might be related to the heterogeneity of the terminology as well as to unclear evidence of the measurement properties of available tools. The aim of this review was to identify tools to assess the external validity of RCTs. It was further, to evaluate the quality of identified tools and to recommend the use of individual tools to assess the external validity of RCTs in future systematic reviews. Methods A two-phase systematic literature search was performed in four databases: PubMed, Scopus, PsycINFO via OVID, and CINAHL via EBSCO. First, tools to assess the external validity of RCTs were identified. Second, studies investigating the measurement properties of these tools were selected. The measurement properties of each included tool were appraised using an adapted version of the COnsensus based Standards for the selection of health Measurement INstruments (COSMIN) guidelines. Results 38 publications reporting on the development or validation of 28 included tools were included. For 61% (17/28) of the included tools, there was no evidence for measurement properties. For the remaining tools, reliability was the most frequently assessed property. Reliability was judged as “ sufficient ” for three tools (very low certainty of evidence). Content validity was rated as “ sufficient ” for one tool (moderate certainty of evidence). Conclusions Based on these results, no available tool can be fully recommended to assess the external validity of RCTs in systematic reviews. Several steps are required to overcome the identified difficulties to either adapt and validate available tools or to develop a better suitable tool. Trial registration Prospective registration at Open Science Framework (OSF): 10.17605/OSF.IO/PTG4D .",
  "full_text": "Jung et al. \nBMC Medical Research Methodology          (2022) 22:100  \nhttps://doi.org/10.1186/s12874-022-01561-5\nRESEARCH\nIdentification of tools used to assess \nthe external validity of randomized controlled \ntrials in reviews: a systematic review \nof measurement properties\nAndres Jung1* , Julia Balzer2 , Tobias Braun3,4  and Kerstin Luedtke1  \nAbstract \nBackground: Internal and external validity are the most relevant components when critically appraising randomized \ncontrolled trials (RCTs) for systematic reviews. However, there is no gold standard to assess external validity. This might \nbe related to the heterogeneity of the terminology as well as to unclear evidence of the measurement properties \nof available tools. The aim of this review was to identify tools to assess the external validity of RCTs. It was further, to \nevaluate the quality of identified tools and to recommend the use of individual tools to assess the external validity of \nRCTs in future systematic reviews.\nMethods: A two-phase systematic literature search was performed in four databases: PubMed, Scopus, PsycINFO via \nOVID, and CINAHL via EBSCO. First, tools to assess the external validity of RCTs were identified. Second, studies investi-\ngating the measurement properties of these tools were selected. The measurement properties of each included tool \nwere appraised using an adapted version of the COnsensus based Standards for the selection of health Measurement \nINstruments (COSMIN) guidelines.\nResults: 38 publications reporting on the development or validation of 28 included tools were included. For 61% \n(17/28) of the included tools, there was no evidence for measurement properties. For the remaining tools, reliability \nwas the most frequently assessed property. Reliability was judged as “sufficient” for three tools (very low certainty of \nevidence). Content validity was rated as “sufficient” for one tool (moderate certainty of evidence).\nConclusions: Based on these results, no available tool can be fully recommended to assess the external validity of \nRCTs in systematic reviews. Several steps are required to overcome the identified difficulties to either adapt and vali-\ndate available tools or to develop a better suitable tool.\nTrial registration: Prospective registration at Open Science Framework (OSF): https:// doi. org/ 10. 17605/ OSF. IO/ \nPTG4D.\nKeywords: External validity, Generalizability, Applicability, Measurement properties, Tools, Randomized controlled \ntrial\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nBackground\nSystematic reviews are powerful research formats to \nsummarize and synthesize the evidence from primary \nresearch in health sciences [1, 2]. In clinical practice, their \nresults are often applied for the development of clinical \nOpen Access\n*Correspondence:  a.jung@uni-luebeck.de\n1  Institute of Health Sciences, Department of Physiotherapy, Pain \nand Exercise Research Luebeck (P .E.R.L), Universität zu Lübeck, \nRatzeburger Allee 160, 23562 Lübeck, Germany\nFull list of author information is available at the end of the article\nPage 2 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \nguidelines and treatment recommendations [3]. Conse -\nquently, the methodological quality of systematic reviews \nis of great importance. In turn, the informative value of \nsystematic reviews depends on the overall quality of the \nincluded controlled trials [3, 4]. Accordingly, the evalu -\nation of the internal and external validity is considered a \nkey step in systematic review methodology [4, 5].\nInternal validity relates to the systematic error or bias \nin clinical trials [6] and expresses how methodologically \nrobust the study was conducted. External validity is the \ninference about the extent to which “a causal relation -\nship holds over variations in persons, settings, treatments \nand outcomes” [7, 8]. There are plenty of definitions for \nexternal validity and a variety of different terms. Hence, \nexternal validity, generalizability, applicability, and trans -\nferability, among others, are used interchangeably in the \nliterature [9]. Schünemann et  al. [10] suggest that: (1) \ngeneralizability “may refer to whether or not the evidence \ncan be generalized from the population from which the \nactual research evidence is obtained to the population for \nwhich a healthcare answer is required”; (2) applicability \nmay be interpreted as “whether or not the research evi -\ndence answers the healthcare question asked by a clini -\ncian or public health practitioner” and (3) transferability \nis often interpreted as to “whether research evidence can \nbe transferred from one setting to another” . Four essential \ndimensions are proposed to evaluate the external validity \nof controlled clinical trials in systematic reviews: patients, \ntreatment (including comparator) variables, settings, and \noutcome modalities [4, 11]. Its evaluation depends on \nthe specificity of the reviewers´ research question, the \nreview´s inclusion and exclusion criteria compared to the \ntrial´s population, the setting of the study, as well as the \nquality of reporting these four dimensions.\nIn health research, however, external validity is often \nneglected when critically appraising clinical studies [12, \n13]. One possible explanation might be the lack of a gold \nstandard for assessing the external validity of clinical tri -\nals. Systematic and scoping reviews examined published \nframeworks and tools for assessing the external validity \nof clinical trials in health research [9, 12, 14–18]. A sub -\nstantial heterogeneity of terminology and criteria as well \nas a lack of guidance on how to assess the external valid -\nity of intervention studies was found [9, 12, 15–18]. The \nresults and conclusions of previous reviews were based \non descriptive as well as content analysis of frameworks \nand tools on external validity [9, 14–18]. Although the \nfeasibility of some frameworks and tools was assessed \n[12], none of the previous reviews evaluated the quality \nregarding the development and validation processes of \nthe used frameworks and tools.\nRCTs are considered the most suitable research \ndesign for investigating cause and effect mechanisms of \ninterventions [19]. However, the study design of RCTs is \nsusceptible to a lack of external validity due to the rand -\nomization, the use of exclusion criteria and poor willing -\nness of eligible participants to participate [20, 21]. There \nis evidence that the reliability of external validity evalua -\ntions with the same measurement tool differed between \nrandomized and non-randomized trials [22]. In addition, \ndue to differences in requested information from report -\ning guidelines (e.g. consolidated standards of reporting \ntrials (CONSORT) statement, strengthening the report -\ning of observational studies in Epidemiology (STROBE) \nstatement), respective items used for assessing the exter -\nnal validity vary between research designs. Acknowl -\nedging the importance of RCTs in the medical field, this \nreview focused only on tools developed to assess the \nexternal validity of RCTs. The aim was to identify tools to \nassess the external validity of RCTs in systematic reviews \nand to evaluate the quality of evidence regarding their \nmeasurement properties. Objectives: (1) to identify pub -\nlished measurement tools to assess the external validity \nof RCTs in systematic reviews; (2) to evaluate the quality \nof identified tools; (3) to recommend the use of tools to \nassess the external validity of RCTs in future systematic \nreviews.\nMethods\nThis systematic review was reported in accordance with \nthe Preferred Reporting Items for Systematic reviews and \nMeta-Analyses (PRISMA) 2020 Statement [23] and used \nan adapted version of the PRISMA flow diagram to illus -\ntrate the systematic search strategy used to identify clini -\nmetric papers [24]. This study was conducted according \nto an adapted version of the COnsensus-based Stand -\nards for the selection of health Measurement INstru -\nments (COSMIN) methodology for systematic reviews \nof measurement instruments in health sciences [25–27] \nand followed recommendations of the JBI manual for \nsystematic reviews of measurement properties [28]. The \nCOSMIN methodology was chosen since this method \nis comprehensive and validation processes do not differ \nsubstantially between patient-reported outcome meas -\nures (PROMs) and measurement instruments of other \nlatent constructs. According to the COSMIN authors, it \nis acceptable to use this methodology for non-PROMs \n[26]. Furthermore, because of its flexibility, it has already \nbeen used in systematic reviews assessing measurement \ntools which are not health measurement instruments \n[29–31]. However, adaptations or modifications may be \nnecessary [26]. The type of measurement instrument of \ninterest for the current study were reviewer-reported \nmeasurement tools. Pilot tests and adaptation-processes \nof the COSMIN methodology are described below (see \nsection “Quality assessment and evidence synthesis”). \nPage 3 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \nThe definition of each measurement property evaluated \nin the present review is based on COSMIN´s taxonomy, \nterminology and definition of measurement properties \n[32]. The review protocol was prospectively registered \non March 6, 2020 in the Open Science Framework (OSF) \nwith the registration DOI: https:// doi. org/ 10. 17605/ OSF. \nIO/ PTG4D [33].\nDeviations from the preregistered protocol\nOne of the aims listed in the review protocol was to eval -\nuate the characteristics and restrictions of measurement \ntools in terms of terminology and criteria for assessing \nexternal validity. This issue has been addressed in two \nrecent reviews with a similar scope [9, 17]. Although our \neligibility criteria differed, it was concluded that no novel \ndata was available for the present review to extract, since \nauthors of included tools did not describe the definition \nor construct of interest or cited the same reports. There -\nfore, this objective was omitted.\nLiterature search and screening\nA search of the literature was conducted in four data -\nbases: PubMed, Scopus, PsycINFO via OVID, and \nCINAHL via EBSCO. The eligibility criteria and search \nstrategy were predefined in collaboration with a research \nlibrarian and is detailed in Table  S1 (see Additional \nfile 1). The search strategy was designed according to the \nCOSMIN methodology and consists of the following four \nkey elements: (1) construct (external validity of RCTs \nfrom the review authors´perspective), (2) population(s) \n(RCTs), (3) type of instrument(s) (measurement tools, \nchecklists, surveys etc.), and (4) measurement properties \n(e.g. validity and reliability) [34]. The four key elements \nwere divided into two main searches (adapted from pre -\nvious reviews [24, 35, 36]): the phase 1 search contained \nthe first three key elements to identify measurement \ntools to assess the external validity of RCTs. The phase \n2 search aimed to identify studies evaluating the meas -\nurement properties of each tool, which was identified and \nincluded during phase 1. For this second search, a sensi -\ntive PubMed search filter developed by Terwee et al. [37] \nwas applied. Translations of this filter for the remaining \ndatabases were taken from the COSMIN website and \nfrom other published COSMIN reviews [38, 39] with per-\nmission from the authors. Both searches were conducted \nuntil March 2021 without restriction regarding the time \nof publication (databases were searched from inception). \nIn addition, forward citation tracking with Scopus (which \nis a specialized citation database) was conducted in phase \n2 using the ‘cited by’-function. The Scopus search filter \nwas then entered into the ‘search within results’-function. \nThe results from the forward citation tracking with Sco -\npus were added to the database search results into the \nRayyan app for screening. Reference lists of the retrieved \nfull-text articles and forward citations with PubMed \nwere scanned manually for any additional studies by one \nreviewer (AJ) and checked by a second reviewer (KL).\nTitle and abstract screening for both searches and the \nfull-text screening during phase 2 were performed inde -\npendently by at least two out of three involved research -\ners (AJ, KL & TB). For pragmatic reasons, full-text \nscreening and tool/data extraction in phase 1 was per -\nformed by one reviewer (AJ) and checked by a second \nreviewer (TB). This screening method is acceptable for \nfull-text screening as well as data extraction [40]. Data \nextraction for both searches was performed with a pre -\ndesigned extraction sheet based on the recommendations \nof the COSMIN user manual [34]. The Rayyan Qatar \nComputing Research Institute (QCRI) web app [41] was \nused to facilitate the screening process (both searches) \naccording to a priori defined eligibility criteria. A pilot \ntest was conducted for both searches in order to reach \nagreement between the reviewers during the screening \nprocess. For this purpose, the first 100 records in phase \n1 and the first 50 records in phase 2 (sorted by date) in \nthe Rayyan app were screened by two reviewers indepen -\ndently and subsequently, issues regarding the feasibility \nof screening methods were discussed in a meeting.\nEligibility criteria\nPhase 1 search (identification of tools)\nRecords were considered for inclusion based on their \ntitle and abstract according to the following criteria: (1) \nrecords that described the development and or imple -\nmentation (application), e.g. manual or handbook, of any \ntool to assess the external validity of RCTs; (2) systematic \nreviews that applied tools to assess the external validity of \nRCTs and which explicitly mentioned the tool in the title \nor abstract; (3) systematic reviews or any other publica -\ntion potentially using a tool for external validity assess -\nment, but the tool was not explicitly mentioned in the \ntitle or abstract; (4) records that gave other references to, \nor dealt with, tools for the assessment of external validity \nof RCTs, e.g. method papers, commentaries.\nThe full-text screening was performed to extract or \nto find references to potential tools. If a tool was cited, \nbut not presented or available in the full-text version, \nthe internet was searched for websites on which this \ntool was presented, to extract and review for inclusion. \nPotential tools were extracted and screened for eligibil -\nity as follows: measurement tools aiming to assess the \nexternal validity of RCTs and designed for implementa -\ntion in systematic reviews of intervention studies. Since \nthe terms external validity, applicability, generalizability, \nrelevance and transferability are used interchangeably \nin the literature [10, 11], tools aiming to assess one of \nPage 4 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \nthese constructs were eligible. Exclusion criteria: (1) The \nmultidimensional tool included at least one item related \nto external validity, but it was not possible to assess and \ninterpret external validity separately. (2) The tool was \ndeveloped exclusively for study designs other than RCTs. \n(3) The tool contained items assessing information not \nrequested in the CONSORT-Statement [42] (e.g. cost-\neffectiveness of the intervention, salary of health care \nprovider) and these items could not be separated from \nitems on external validity. (4) The tool was published in a \nlanguage other than English or German. (5) The tool was \nexplicitly designed for a specific medical profession or \nfield and cannot be used in other medical fields.\nPhase 2 search (identification of reports on the measurement \nproperties of included tools)\nFor the phase 2 search, records evaluating the measure -\nment properties of at least one of the included meas -\nurement tools were selected. Reports only using the \nmeasurement tool as an outcome measure without the \nevaluation of at least one measurement property were \nexcluded. If a report did not evaluate the measurement \nproperties of a tool, it was also excluded. Hence, reports \nproviding data on the validity or the reliability of sum-\nscores of multidimensional tools, only, were excluded \nif the dimension “external validity” was not evaluated \nseparately.\nIf there was missing data or information (phase 1 or \nphase 2), the corresponding authors were contacted.\nQuality assessment and evidence synthesis\nAll included reports were systematically evaluated: (1) \nfor their methodological quality by using the adapted \nCOSMIN Risk of Bias (RoB) checklist [25] and (2) against \nthe updated criteria for good measurement properties \n[26, 27]. Subsequently, all available evidence for each \nmeasurement property for the individual tool were sum -\nmarized and rated against the updated criteria for good \nmeasurement properties and graded for their certainty \nof evidence, according to COSMIN´s modified GRADE \napproach [26, 27]. The quality assessment was performed \nby two independent reviewers (AJ & JB). In case of irrec -\noncilable disagreement, a third reviewer (TB) was con -\nsulted to reach consensus.\nThe COSMIN RoB checklist is a tool [25, 27, 32, 43] \ndesigned for the systematic evaluation of the methodo -\nlogical quality of studies assessing the measurement \nproperties of health measurement instruments [25]. \nAlthough this checklist was specifically developed for \nsystematic reviews of PROMs, it can also be used for \nreviews of non-PROMs [26] or measurement tools of \nother latent constructs [28, 29]. As mentioned in the \nCOSMIN user manual, adaptations for some items in \nthe COSMIN RoB checklist might be necessary, in rela -\ntion to the construct being measured [34]. Therefore, \npilot tests were performed for the assessment of meas -\nurement properties of tools assessing the quality of RCTs \nbefore data extraction, aiming to ensure feasibility during \nthe planned evaluation of the included tools. The pilot \ntests were performed with a random sample of publica -\ntions on measurement instruments of potentially rel -\nevant tools. After each pilot test, results and problems \nregarding the comprehensibility, relevance and feasibility \nof the instructions, items, and response options in rela -\ntion to the construct of interest were discussed. Where \nnecessary, adaptations and/or supplements were added \nto the instructions of the evaluation with the COSMIN \nRoB checklist. Saturation was reached after two rounds \nof pilot testing. Substantial adaptations or supplements \nwere required for Box  1 (‘development process’) and \nBox  10 (‘responsiveness’) of the COSMIN RoB check -\nlist. Minor adaptations were necessary for the remaining \nboxes. The specification list, including the adaptations, \ncan be seen in Table S2 (see Additional file  2). The meth-\nodological quality of included studies was rated via the \nfour-point rating scale of the COSMIN RoB checklist as \n“inadequate” , “doubtful” , “adequate” , or “very good” [25]. \nThe lowest score of any item in a box is taken to deter -\nmine the overall rating of the methodological quality of \neach single study on a measurement property [25].\nAfter the RoB-assessment, the result of each single \nstudy on a measurement property was rated against the \nupdated criteria for good measurement properties for \ncontent validity [27] and for the remaining measure -\nment properties [26] as “sufficient” (+), “insufficient” (-), \nor “indeterminate” (?). These ratings were summarized \nand an overall rating for each measurement property \nwas given as “sufficient” (+), “insufficient” (-), “inconsist -\nent” (±), or “indeterminate” (?). However, the overall rat -\ning criteria for good content validity was adapted to the \nresearch topic of the present review. This method usually \nrequires an additional subjective judgement from review-\ners [44]. Since one of the biggest limitations within this \nfield of research is the lack of consensus on terminology \nand criteria as well as on how to assess the external valid-\nity [9, 12], a reviewers’ subjective judgement was consid -\nered inappropriate. After this issue was also discussed \nwith one leading member of the COSMIN steering com -\nmittee, the reviewers’ rating was omitted. A “sufficient” \n(+) overall rating was given if there was evidence of face \nor content validity of the final version of the measure -\nment tool assessed by a user or expert panel. Otherwise, \nthe rating “indeterminate” (?) or “insufficient” (-) was \nused for the content validity.\nThe summarized evidence for each measurement prop-\nerty for the individual tool was graded using COSMIN´s \nPage 5 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \nmodified GRADE approach [26, 27]. The certainty (qual -\nity) of evidence was graded as “high” , “moderate” , “low” , \nor “very low” according to the approach for content \nvalidity [27] and for the remaining measurement prop -\nerties [26]. COSMIN´s modified GRADE approach dis -\ntinguishes between four factors influencing the certainty \nof evidence: risk of bias, inconsistency, indirectness, \nand imprecision. The starting point for all measure -\nment properties is high certainty of evidence and is sub -\nsequently downgraded by one to three levels per factor \nwhen there is risk of bias, (unexplained) inconsistency, \nimprecision (not considered for content validity [27]), or \nindirect results [26, 27]. If there is no study on the content \nvalidity of a tool, the starting point for this measurement \nproperty is “moderate” and is subsequently downgraded \ndepending on the quality of the development process \n[27]. The grading process according to COSMIN [26, 27] \nis described in Table S4. Selective reporting bias or publi-\ncation bias is not taken into account in COSMIN´s modi-\nfied GRADE approach, because of a lack of registries for \nstudies on measurement properties [26].\nThe evidence synthesis was performed qualitatively \naccording to the COSMIN methodology [26]. If several \nreports revealed homogenous quantitative data (e.g. \nsame statistics, population) on internal consistency, reli -\nability, measurement error or hypotheses testing of a \nmeasurement tool, pooling the results was considered \nusing generic inverse variance (random effects) meth -\nodology and weighted means as well as 95% confidence \nintervals for each measurement property [34]. No sub -\ngroup analysis was planned. However, statistical pooling \nwas not possible in the present review.\nWe used three criteria for the recommendation of \na measurement tool in accordance with the COSMIN \nmanual: (A) “Evidence for sufficient content validity (any \nlevel) and at least low-quality evidence for sufficient \ninternal consistency” for a tool to be recommended; (B) \ntool “categorized not in A or C” and further research on \nthe quality of this tool is required to be recommended; \nand (C) tool with “high quality evidence for an insuffi -\ncient psychometric property” and this tool should not be \nrecommended [26].\nResults\nLiterature search and selection process\nFigure  1 shows the selection process. In the phase 1 \nsearch, from 5397 non-duplicate records, 5020 irrelevant \nrecords were excluded. 377 reports were screened, and \n74 potential tools were extracted. After reaching con -\nsensus, 46 tools were excluded (reasons for exclusion are \npresented in Table S3 (see Additional file  3)) and finally \n28 were included. Any disagreements during the screen -\ning process were resolved through discussion. There \nwas one case during the full-text screening process in \nFig. 1 Flow diagram “of systematic search strategy used to identify clinimetric papers”[24]\nPage 6 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \nthe phase 1 search, in which the whole review team was \ninvolved to reach consensus about the inclusion/exclu -\nsion of two tools (Agency for Healthcare Research and \nQuality (AHRQ) criteria for applicability and TRANS -\nFER approach, both listed in Table S3).\nIn the phase 2 search, 2191 non-duplicate records \nwere screened for title and abstract. 2146 records were \nexcluded as they did not assess any measurement prop -\nerty of the included tools. Of 45 reports, 8 reports were \nincluded. The most common reason for exclusion was \nthat reports evaluating the measurement properties of \nmultidimensional tools did not evaluate external validity \nas a separate dimension. For example, one study assess -\ning the interrater reliability of the GRADE method [45] \nwas identified during full-text screening, but had to be \nexcluded, since it did not provide separate data on the \nreliability of the indirectness domain (representing exter-\nnal validity). Two additional reports were included dur -\ning reference screening. Any disagreements during the \nscreening process were resolved through discussion.\nThirty-eight publications on the development or eval -\nuation of the measurement properties of 28 included \ntools were included for quality appraisal according to the \nadapted COSMIN guidelines.\nWe contacted the corresponding authors of three \nreports [46–48] for additional information. One corre -\nsponding author did reply [48].\nMethods to assess the external validity of RCTs\nDuring full-text screening in phase 1, several concepts to \nassess the external validity of RCTs were found (Table 1). \nTwo main concepts were identified: experimental/sta -\ntistical methods and non-experimental methods. The \nexperimental/statistical methods were summarized and \ncollated into five subcategories giving a descriptive over -\nview of the different approaches used to assess the exter -\nnal validity. However, according to our eligibility criteria, \nthese methods were excluded, since they were not devel -\noped for the use in systematic reviews of interventions. \nIn addition, a comparison of these methods as well as \nappraisal of risk of bias with the COSMIN RoB check -\nlist would not have been feasible. Therefore, the experi -\nmental/statistical methods described below were not \nincluded for further evaluation.\nCharacteristics of included measurement tools\nThe included tools and their characteristics are listed \nin Table  2. Overall, the tools were heterogenous with \nrespect to the number of items or dimensions, response \noptions and development processes. The number of \nitems varied between one and 26 items and the response \noptions varied between 2-point-scales to 5-point-scales. \nMost tools used a 3-point-scale (n = 20/28, 71%). For \n14/28 (50%) of the tools, the development was not \ndescribed in detail [63–76]. Seven review authors appear \nto have developed their own tool but did not provide any \ninformation on the development process [63–68, 71].\nThe constructs aimed to be measured by the tools \nor dimensions of interest are diverse. Two of the tools \nfocused on the characterization of RCTs on an efficacy-\neffectiveness continuum [47, 86], three tools focused \npredominantly on the report quality of factors essential \nto external validity [69, 75, 88] (rather than the external \nvalidity itself), 18 tools aimed to assess the representa -\ntiveness, generalizability or applicability of population, \nsetting, intervention, and/or outcome measure to usual \npractice [22, 63–65, 70, 71, 73, 74, 76–78, 81–83, 92, 94, \n100], and five tools seemed to measure a mixture of these \ndifferent constructs related to external validity [66, 68, \n72, 79, 98]. However, the construct of interest of most \ntools was not described adequately (see below).\nMeasurement properties\nThe results of the methodological quality assessment \naccording to the adapted COSMIN RoB checklist are \ndetailed in Table 3. If all data on hypotheses testing in an \narticle had the same methodological quality rating, they \nwere combined and summarized in Table 3 in accordance \nwith the COSMIN manual [34]. The results of the rat -\nings against the updated criteria for good measurement \nTable 1 Experimental/statistical methods to evaluate the EV of RCTs\nAbbreviations: EV external validity, NNT numbers needed to treat, RCT randomized controlled trial\nFor non-experimental methods, please refer to Table 2\n1. Comparing differences of characteristics and/or NNT analysis from not-enrolled eligible patients with enrolled patients [49–52]\n2. Conduction of observational studies to assess the “real world” applicability of RCTs [20, 53, 54]\n3. Meta-analysis of patient characteristics data from RCTs [55, 56]\n4. Comparison of data from RCTs with data from health record database and/or other epidemiological data:\na) retrospectively [55–59]\nb) simulation-based (a priori and retrospective) [60, 61]\n5. Review of exclusion criteria in RCTs which would limit the EV [62]\nPage 7 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \nTable 2 Characteristics of included tools\nDimension and/or tool Authors Construct(s), as \ndescribed by the authors\nTarget population Domains, nr. of items Response options Development and \nvalidation\n“Applicability”-dimension \nof LEGEND\nClark et al. [77] Applicability of results to \ntreating patients\nP1: RCTs and CCTs P2: \nreviewers and clinicians\n3 items 3-point-scale Deductive and inductive \nitem-generation. Tool was \npilot tested among an \ninterprofessional group of \nclinicians.\n“Applicability”-dimension \nof Carr´s evidence-grading \nscheme\nCarr et al. [63] Generalizability of study \npopulation\nP1: clinical trials\nP:2 authors of SRs\n1 item 3-point-classification-scale No specific information on \ntool development.\nBornhöft´s checklist Bornhöft et al. [78] External validity (EV) and \nModel validity (MV) of \nclinical trials\nP1: clinical trials\nP2: authors of SRs\n4 domains with 26 items \nfor EV and MV each\n4-point-scale Development with a \ncomprehensive, deductive \nitem-generation from the \nliterature. Pilot-tests were \nperformed, but not for the \nwhole scales.\nCleggs´s external validity \nassessment\nClegg et al. [64] Generalizability of clinical \ntrials to England and Wales\nP1: clinical trials\nP2: authors of SRs and \nHTAs\n5 items 3-point-scale No specific information on \ntool development\nClinical applicability Haraldsson et al. [66] Report quality and \napplicability of interven-\ntion, study population and \noutcomes\nP1: RCTs\nP2: reviewers\n6 items 3-point-scale and 4-point-\nscale\nNo specific information on \ntool development\nClinical Relevance Instru-\nment\nCho & Bero [79] Ethics and Generalizability \nof outcomes, subjects, \ntreatment and side effects\nP1: clinical trials\nP2: reviewers\n7 items 3-point-scale Tool was pilot tested on \n10 drug studies. Content \nvalidity was confirmed by \n7 reviewers with research \nexperience.\n- interrater reliability:\nICC = 0.56 (n = 127) [80]\n“Clinical Relevance” accord-\ning to the CCBRG\nVan Tulder et al. [81] Applicability of patients, \ninterventions and out-\ncomes\nP1: RCTs\nP2: authors of SRs\n5 items 3-point-scale (Staal et al., \n2008)\nDeductive item-generation \nfor Clinical Relevance. \nResults were discussed in a \nworkshop. After two rounds, \na final draft was circulated \nfor comments among edi-\ntors of the CCBRG.\nClinical Relevance Score Karjalainen et al. [68] Report quality and appli-\ncability of results\nP1: RCTs\nP2: reviewers\n3 items 3-point-scale No specific information on \ntool development.\nEstrada´s applicability \nassessment criteria\nEstrada et al. [82] Applicability of population, \nintervention, implementa-\ntion and environmental \ncontext to Latin America\nP1: RCTs\nP2: reviewers\n5 domains with 8 items 3-point-scale for each \ndomain\nDeductive item generation \nfrom the review by Munthe-\nKaas et al. [17]. Factors and \nitems were adapted, and \npilot tested by the review \nteam (n = 4) until consensus \nwas reached.\nPage 8 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \nTable 2 (continued)\nDimension and/or tool Authors Construct(s), as \ndescribed by the authors\nTarget population Domains, nr. of items Response options Development and \nvalidation\nEVAT (External Validity \nAssessment Tool)\nKhorsan & Crawford [83] External validity of par-\nticipants, intervention, and \nsetting\nP1: RCTs and non-rand-\nomized studies\nP2: reviewers\n3 items 3-point-scale Deductive item-generation. \nTool developed based on \nthe GAP-checklist [76] and \nthe Downs and Black-\nchecklist [22]. Feasibility \nwas tested and a rulebook \nwas developed but not \npublished.\n“External validity”-\ndimension of the Downs & \nBlack-Checklist\nDowns & Black [22] Representativeness \nof study participants, \ntreatments and settings \nto source population or \nsetting\nP1: RCTs and non-ran-\ndomised studies\nP2: reviewers\n3 items 3-point-scale Deductive item-generation, \npilot test and content vali-\ndation of pilot version. Final \nversion tested for:\n- internal consistency: \nKR-20 = 0.54 (n = 20),\n- reliability:\ntest-retest:\nk = -0.05-0.48 and 10–15% \ndisagreement (measure-\nment error) (n = 20), [22]\ninterrater reliability: k = \n-0.08-0.00 and 5–20% disa-\ngreement (measurement \nerror) (n = 20) [22];\nICC = 0.76 (n = 20) [84]\n“External validity”-\ndimension of Foy´s quality \nchecklist\nFoy et al. [65] External validity of \npatients, settings, interven-\ntion and outcomes\nP1: intervention studies\nP2: reviewers\n6 items not clearly described Deductive item-generation. \nNo further information on \ntool development.\n“External validity”-dimen-\nsion of Liberati´s quality \nassessment criterias\nLiberati et al. [69] Report quality and gener-\nalizability\nP1: RCTS\nP2: reviewers\n9 items dichotomous and 3-point-\nscale\nTool is a modified version \nof a previously developed \nchecklist [85] with additional \ninductive item-generation. \nNo further information on \ntool development.\n“External validity”-dimen-\nsion of Sorg´s checklist\nSorg et al. [71] External validity of popula-\ntion, interventions, and \nendpoints\nP1: RCTs\nP2: reviewers\n4 domains with 11 items not clearly described Developed based on \nBornhöft et al. [78] No \nfurther information on tool \ndevelopment.\n“external validity”-criteria \nof the USPSTF\nUSPSTF Procedure manual \n[73]\nGeneralizability of study \npopulation, setting and \nproviders for US primary \ncare\nP1: clinical studies\nP2: USPSTF reviewers\n3 items Sum-score- rating:\n3-point-scale\nTool developed for USPSTF \nreviews. No specific informa-\ntion on tool development.\n- interrater reliability:\nICC = 0.84 (n = 20) [84]\nPage 9 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \nTable 2 (continued)\nDimension and/or tool Authors Construct(s), as \ndescribed by the authors\nTarget population Domains, nr. of items Response options Development and \nvalidation\nFAME (Feasibility, Appropri-\nateness, Meaningfulness \nand Effectiveness) scale\nAveris et al. [70] Grading of recommenda-\ntion for applicability and \nethics of intervention\nP1: intervention studies\nP2: reviewers\n4 items 5-point-scale The FAME framework was \ncreated by a national group \nof nursing research experts. \nDeductive and inductive \nitem-generation. No further \ninformation on tool devel-\nopment.\nGAP (Generalizability, \nApplicability and\nPredictability) checklist\nFernandez-Hermida et al. \n[76]\nExternal validity of\npopulation, setting, inter-\nvention and endpoints\nP1: RCTs\nP2: Reviewers\n3 items 3-point-scale No specific information on \ntool development.\nGartlehner´s tool Gartlehner et al. [86] To distinguish between \neffectiveness and efficacy \ntrials\nP1: RCTs\nP2: reviewers\n7 items Dichotomous Deductive and inductive \nitem-generation.\n- criterion validity testing \nwith studies selected by 12 \nexperts as gold standard.:\nspecificity = 0.83, sensitiv-\nity = 0.72 (n = 24)\n- measurement error: 78.3% \nagreement (n = 24)\n- interrater reliability:\nk = 0.42 (n = 24) [86];\nk = 0.11–0.81 (n = 151) [87]\nGreen & Glasgow´s exter-\nnal validity quality rating \ncriteria\nGreen & Glasgow [88] Report quality for gener-\nalizability\nP1: trials (not explicitly \ndescribed) P2: reviewers\n4 Domains with 16 items Dichotomous Deductive item-generation. \nMainly based on the Re-Aim \nframework.[89]\n- interrater reliability:\nICC = 0.86 (n = 14) [90]\n- discriminative validity: \nTREND studies report on \n77% and non-TREND studies \nreport on 54% of scale items\n(n = 14) [90]\n- ratings across included \nstudies (n = 31) [91], no \nhypothesis was defined\n“Indirecntess”-dimension \nof the GRADE handbook\nSchünemann et al. [92] Differences of popula-\ntion, interventions, and \noutcome measures to \nresearch question\nP1: intervention studies\nP2: authors of SRs, clinical \nguidelines and HTAs\n4 items Overall:\n3-point-scale (downgrad-\ning options)\nDeductive and inductive \nitem-generation, pilot-\ntesting with 17 reviewers \n(n = 12) [48].\n- interrater reliability:\nICC = 0.00–0.13 (n > 100) [93]\nPage 10 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \nTable 2 (continued)\nDimension and/or tool Authors Construct(s), as \ndescribed by the authors\nTarget population Domains, nr. of items Response options Development and \nvalidation\nLoyka´s external validity \nframework\nLoyka et al. [75] Report quality for gener-\nalizability\nof research in psychologi-\ncal science\nP1: intervention studies\nP2: researchers\n4 domains\nwith 15 items\nDichotomous Deductive item generation \n(including Green & Glasgow \n[88]) and adaptation for \npsychological science. No \nfurther information on tool \ndevelopment.\n- measurement error:\n60-100% agreement \n(n = 143)\nModified “Indirectness” of \nthe Checklist for GRADE\nMeader et al. [94] Differences of popula-\ntion, interventions, and \noutcome measures to \nresearch question.\nP1: meta-analysis of RCTs\nP2: authors of SRs, clinical \nguidelines and HTAs\n5 items Item-level: 2-and 3-point-\nscale\nOverall:\n3-point-scale (grading \noptions)\nDeveloped based on GRADE \nmethod, two phase pilot-\ntests,\n- interrater reliability:\nkappa was poor to almost \nperfect on item-level [94] \nand\nk = 0.69 for overall rating of \nindirectness (n = 29) [95]\nexternal validity checklist \nof the NHMRC handbook\nNHMRC handbook [74] external validity of an \neconomic study\nP1: clinical studies\nP2: clinical guideline \ndevelopers, reviewers\n6 items 3-point-scale No specific information on \ntool development.\nrevised GATE in NICE \nmanual (2012)\nNICE manual [72] Generalizability of popula-\ntion, interventions and \noutcomes\nP1: intervention studies\nP2: reviewers\n2 domains with 4 items 3-point-scale and 5-point-\nscale\nBased on Jackson et al. [96] \nNo specific information on \ntool development.\nRITES (Rating of Included \nTrials on the Efficacy-Effec-\ntiveness Spectrum)\nWieland et al. [47] To characterize RCTs on \nan efficacy-effectiveness \ncontinuum.\nP1: RCTs\nP2: reviewers\n4 items 5-point-likert-scale Deductive and inductive \nitem-generation, modified \nDelphi procedure with \n69–72 experts, pilot testing \nin 4 Cochrane reviews, con-\ntent validation with Delphi \nprocedure and core expert \ngroup (n = 14) [47],\n- interrater reliability:\nICC = 0.54-1.0 (n = 22) [97]\n- convergent validity with \nPRECIS 2 tool:\nr = 0.55 correlation (n = 59) \n[97]\nPage 11 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \nTable 2 (continued)\nDimension and/or tool Authors Construct(s), as \ndescribed by the authors\nTarget population Domains, nr. of items Response options Development and \nvalidation\nSection A (Selection Bias) \nof EPHPP (Effective Public \nhealth Practice Project) \ntool\nThomas et al. [98] Representativeness of \npopulation and participa-\ntion rate.\nP1: clinical trials\nP2: reviewers\n2 items Item-level:\n4-point-scale and 5-point-\nscale\nOverall:\n3-point-scale\nDeductive item-generation, \npilot-tests, content valida-\ntion by 6 experts,\n- convergent validity with \nGuide to Community Ser-\nvices (GCPS) instrument:\n52.5–87.5% agreement \n(n = 70) [98]\n- test-retest reliability:\nk = 0.61–0.74 (n = 70) [98]\nk = 0.60 (n = 20) [99]\nSection D of the CASP \nchecklist for RCTs\nCASP Programme [100] Applicability to local popu-\nlation and outcomes\nP1: RCTs\nP2: participants of work-\nshops, reviewers\n2 items 3-point-scale Deductive item-generation, \ndevelopment and pilot-tests \nwith group of experts.\nWhole Systems research \nconsiderations´ checklist\nHawk et al. [67] Applicability of results to \nusual practice\nP1: RCTs P2: Reviewers \n(developed for review)\n7 domains with 13 items Item-level: dichotomous\nOverall: 3-point-scale\nDeductive item-generation. \nNo specific information on \ntool development.\nAbbreviations: CASP Critical Appraisal Skills Programme, CCBRG Cochrane Collaboration Back Review Group, CCT controlled clinical trial, GATE Graphical Appraisal Tool for Epidemiological Studies, GRADE Grading of \nRecommendations Assessment, Development and Evaluation, HTA Health Technology Assessment, ICC intraclass correlation, LEGEND Let Evidence Guide Every New Decision, NICE National Institute for Health and Care \nExcellence, PRECIS PRagmatic Explanatory Continuum Indicator Summary, RCT randomized controlled trial, TREND  Transparent Reporting of Evaluations with Nonrandomized Designs, USPSTF U.S. Preventive Services Task \nForce\nPage 12 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \nTable 3 Methodological quality of included studies based on COSMIN risk of bias (RoB) checklist\nTool or dimension Report Content validity Internal structure Remaining measurement properties\nDevelopment 2.1\nCB\n2.2\nRE\n2.3\nCH\nStructural \nvalidity\nInternal \nconsistency\nCross-\ncultural \nvalidity\nReliability Measurement \nerror\nCriterion \nvalidity\nConstruct \nvalidity\n“Applicability”-dimen-\nsion of LEGEND\nClark et al. \n[77]\ndoubtful\n“Applicability”-dimen-\nsion of Carr´s evidence-\ngrading scheme\nCarr et al. \n[63]\ninadequate\nBornhöft´s checklist Bornhöft \net al. [78]\ninadequate\nCleggs´s external valid-\nity assessment\nClegg et al. \n[64]\ninadequate\nClinical Applicability Haraldsson \net al. [66]\ninadequate\nClinical Relevance Instru-\nment\nCho & Bero \n[79]\ndoubtful doubtful doubtful doubtful\nCho & Bero \n[80]\nadequate\nClinical Relevance \naccording to the CCBRG\nVan Tulder \net al. [81]\ninadequate doubtful doubtful doubtful\nClinical relevance scores \n(Karjalainen´s)\nKarjalainen \net al. [68]\ninadequate\nEstrada´s applicability \nassessment criteria\nEstrada et al. \n[82]\ndoubtful\nEVAT Khorsan & \nCrawford \n[83]\ndoubtful\n“External validity”-\ndimension of the Downs \n& Black Checklist\nDowns & \nBlack [22]\ndoubtful doubtful doubtful doubtful doubtful very  gooda inadequatea adequate\nvery  gooda inadequatea\nO´Connor \net al. [84]\nvery good\n“External validity”-\ndimension of Foy´s qual-\nity checklist\nFoy et al. [65] inadequate\n“External validity”-\ndimension of Liberati´s \nquality assessment \ncriteria\nLiberati et al. \n[69]\ninadequate\n“External validity”-\ndimension of Sorg´s \nchecklist\nSorg et al. \n[71]\ninadequate\nPage 13 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \nTable 3 (continued)\nTool or dimension Report Content validity Internal structure Remaining measurement properties\nDevelopment 2.1\nCB\n2.2\nRE\n2.3\nCH\nStructural \nvalidity\nInternal \nconsistency\nCross-\ncultural \nvalidity\nReliability Measurement \nerror\nCriterion \nvalidity\nConstruct \nvalidity\n“External validity”-crite-\nria of the USPSTF\nUSPSTF \nmanual [73]\ninadequate\nO´Connor \net al. [84]\nvery good\nFAME scale Averis et al. \n[70]\ninadequate\nGAP checklist Fernandez-\nHermida \net al. [76]\ninadequate\nGartlehner´s tool Gartlehner \net al. [86]\ninadequate very good adequate adequate\nZettler et al. \n[87]\nvery good\nGreen & Glasgow´s \nexternal validity quality \nrating criteria\nGreen & \nGlasgow [88]\ninadequate\nLaws et al. \n[91]\ndoubtful\nMirza et al. \n[90]\nadequate doubtful\n“Indirecntess”-dimen-\nsion from the GRADE \nHandbook [92]\nAtkins et al. \n[48]\nadequate\nWu et al. [93] inadequate\nLoyka´s external validity \nframework\nLoyka et al.75 doubtful adequate\nmodified “Indirectness” \nof the Checklist for \nGRADE\nMeader et al. \n[94]\nadequate adequateb\nLlewellyn \net al. [95]\nExternal validity \nchecklist of the NHMRC \nHandbook\nNHMRMC \nHandbook \n[74]\ninadequate\nrevised GATE in the NICE \nmanual\nNICE Guide-\nline [72]\ninadequate\nPage 14 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \nTable 3 (continued)\nTool or dimension Report Content validity Internal structure Remaining measurement properties\nDevelopment 2.1\nCB\n2.2\nRE\n2.3\nCH\nStructural \nvalidity\nInternal \nconsistency\nCross-\ncultural \nvalidity\nReliability Measurement \nerror\nCriterion \nvalidity\nConstruct \nvalidity\nRITES tool Wieland et al. \n[47]\nadequate adequate very good very good\nAves et al. \n[97, 101]\ninadequate very good\n“Selection Bias”-dimen-\nsion (Section A) of the \nEPHPP tool\nThomas et al. \n[98]\ninadequate doubtful doubtful doubtful doubtful doubtful\nArmijo-Olivo \net al. [99]\ndoubtful\nSection D of the CASP \nchecklist for RCTs\nCritical \nAppraisal \nSkills \nProgramme \n[100]\ninadequate\nWhole Systems research \nconsiderations´checklist\nHawk et al. \n[67]\ninadequate\nFields left blank indicate that those measurement properties were not assessed by the study authors\nAbbreviations: CB comprehensibility, RE relevance, CV comprehensiveness, CCBRG Cochrane Collaboration Back Review Group, EPHPP Effective Public Health Practice Project, EVAT External Validity Assessment Tool, FAME \nFeasibility, Appropriateness, Meaningfulness and Effectiveness, GAP Generalizability, Applicability and Predictability; GATE Graphical Appraisal Tool for Epidemiological Studies, GRADE Grading of Recommendations \nAssessment, Development and Evaluation; LEGEND Let Evidence Guide Every New Decision, NHMRC National Health & Medical Research Council, NICE National Institute for Health and Care Excellence, RITES Rating of \nIncluded Trials on the Efficacy-Effectiveness Spectrum, USPSTF U.S. Preventive Services Task Force\na  two studies on reliability (test-retest & inter-rater reliability) in the same article\nb  results from the same study on reliability reported in two articles [94, 95]\nPage 15 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \nproperties and the overall certainty of evidence, accord -\ning to the modified GRADE approach, can be seen in \nTable 4. The detailed grading is described in Table  S4 \n(see Additional file  4). Disagreements between review -\ners during the quality assessment were resolved through \ndiscussion.\nContent validity\nThe methodological quality of the development process \nwas “inadequate” for 19/28 (68%) of the included tools \n[63–66, 68–74, 76, 78, 81, 88, 98, 100]. This was mainly \ndue to insufficient description of the construct to be \nmeasured, the target population, or missing pilot tests. \nSix development studies had a “doubtful” methodological \nquality [22, 75, 77, 79, 82, 83] and three had an “adequate” \nmethodological quality [47, 48, 94].\nThere was evidence for content validation of five tools \n[22, 47, 79, 81, 98]. However, the methodological qual -\nity of the content validity studies was “adequate” and \n“very good” only for the Rating of Included Trials on \nthe Efficacy-Effectiveness Spectrum (RITES) tool [47] \nand “doubtful” for Cho´s Clinical Relevance Instrument \n[79], the “external validity”-dimension of the Downs & \nBlack-checklist [22], the “Selection Bias”-dimension of \nthe Effective Public Health Practice Project (EPHPP) tool \n[98], and the “Clinical Relevance” tool [81]. The overall \ncertainty of evidence for content validity was “very low” \nfor 19 tools (mainly due to very serious risk of bias and \nserious indirectness) [63–76, 78, 82, 86, 88, 100], “low” \nfor three tools (mainly due to serious risk of bias or seri -\nous indirectness) [77, 83, 94] and “moderate” for six \ntools (mainly due to serious risk of bias or serious indi -\nrectness) [22, 47, 79, 81, 92, 98]. All but one tool had an \n“indeterminate” content validity. The RITES tool [47] had \n“moderate” certainty of evidence for “sufficient” content \nvalidity.\nInternal consistency\nOne study assessed the internal consistency for one tool \n(“external validity”-dimension of the Downs & Black-\nchecklist) [22]. The methodological quality of this study \nwas “doubtful” due to a lack of evidence on unidimen -\nsionality (or structural validity). Thus, this tool had a \n“very low” certainty of evidence for “indeterminate” \ninternal consistency. Reasons for downgrading were a \nvery serious risk of bias and imprecision.\nReliability\nOut of 13 studies assessing the reliability of 9 tools, \neleven evaluated the interrater reliability [80,  84, 86, 87, \n90, 93–95, 97, 99], one the test-retest reliability [98], \nand one evaluated both [22]. Two studies had an “inad -\nequate” [93, 101], two had a “doubtful” [98, 99], three had \nan “adequate” [80,  91, 94, 95], and six had a “very good” \nmethodological quality [22, 84, 86, 87]. The overall cer -\ntainty of evidence was “very low” for five tools (reasons \nfor downgrading please refer to Table S4) [47, 73, 88, 92, \n94]. The certainty of evidence was “low” for the “Selec -\ntion Bias”-dimension of the EPHPP tool (due to serious \nrisk of bias and imprecision) [98] and “moderate” for \nGartlehner´s tool [86], the “external validity”-dimension \nof the Downs & Black-checklist [22], as well as the clini -\ncal relevance instrument [79] (due to serious risk of bias \nand indirectness).\nOut of nine evaluated tools, the Downs & Black-check -\nlist [22] had “inconsistent” results on reliability. The \nClinical Relevance Instrument [79], Gartlehner´s tool \n[86], the “Selection Bias”-dimension of the EPHPP [98], \nthe indirectness-dimension of the GRADE handbook \n[92] and the modified indirectness-checklist [94] had an \n“insufficient” rating for reliability. Green & Glasgow´s \ntool [88], the external validity dimension of the U.S. Pre -\nventive Services Task Force (USPSTF) manual [73] and \nthe RITES tool [47] had a “very low” certainty of evidence \nfor “sufficient” reliability.\nMeasurement error\nMeasurement error was reported for three tools. Two \nstudies on measurement error of Gartlehner´s tool [86] \nand Loyka´s external validity framework [75], had an \n“adequate” methodological quality. Two studies on meas-\nurement error of the external validity dimension of the \nDowns & Black-checklist [22] had an “inadequate” meth -\nodological quality. However, all three tools had a “very \nlow” certainty of evidence for “indeterminate” measure -\nment error. Reasons for downgrading were risk of bias, \nindirectness, and imprecision due to small sample sizes.\nCriterion validity\nCriterion validity was reported only for Gartlehner´s \ntool [86]. Although there was no gold standard available \nto assess the criterion validity of this tool, the authors \nused expert opinion as the reference standard. The study \nassessing this measurement property had an “adequate” \nmethodological quality. The overall certainty of evidence \nwas “very low” for “sufficient” criterion validity due to \nrisk of bias, imprecision, and indirectness.\nConstruct validity (hypotheses testing)\nFive studies [22, 90, 91, 97, 98] reported on the con -\nstruct validity of four tools. Three studies had a “doubt -\nful” [90, 91, 98], one had an “adequate” [22] and one \nhad a “very good” [97] methodological quality. The \noverall certainty of evidence was “very low” for three \ntools (mainly due to serious risk of bias, imprecision \nand serious indirectness) [22, 88, 98] and “low” for one \nPage 16 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \nTable 4 Criteria for good measurement properties & certainty of evidence according to the modified GRADE method\nTool or dimension Content validity Internal consistency Reliability Measurement error Criterion validity Construct validity\n“Applicability”-dimension of LEGEND [77]\nCGMP (?)\nGRADE Low\n“Applicability”-dimension of Carr´s evidence-grading scheme [63]\nCGMP (?)\nGRADE Very Low\nBornhöft´s checklist [78]\nCGMP (?)\nGRADE Very Low\nCleggs´s external validity assessment [64]\nCGMP (?)\nGRADE Very Low\nClinical Applicability [66]\nCGMP (?)\nGRADE Very Low\nClinical Relevance Instrument [79, 80]\nCGMP (?) (-)\nGRADE Moderate Moderate\nClinical Relevance according to the CCBRG [81]\nCGMP (?)\nGRADE Moderate\nClinical relevance scores [68]\nCGMP (?)\nGRADE Very Low\nEstrada´s applicability assessment criteria [82]\nCGMP (?)\nGRADE Very Low\nExternal Validity Assessment Tool (EVAT) [83]\nCGMP (?)\nGRADE Low\n“External validity”-dimension of the Downs & Black Checklist [22, 84]\nCGMP (?) (?) (±)a (?) (-)\nGRADE Moderate Very Low Moderate Very Low Very Low\n“External validity”-dimension of Foy´s quality checklist [65]\nCGMP (?)\nGRADE Very Low\n“External validity”-dimension of Liberati´s quality assessment criteria [69]\nCGMP (?)\nGRADE Very Low\n“External validity”-dimension of Sorg´s checklist [71]\nCGMP (?)\nGRADE Very Low\n“External validity”-criteria of the USPSTF manual [73, 84]\nCGMP (?) (+)\nGRADE Very Low Very Low\nFeasibility, Appropriateness, Meaningfulness and Effectiveness (FAME) scale [70]\nCGMP (?)\nGRADE Very Low\nPage 17 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \ntool (due to imprecision and serious indirectness) [47]. \nThe “Selection-Bias”-dimension of the EPHPP tool [98] \nhad “very low” certainty of evidence for “sufficient” \nconstruct validity and the RITES tool [47] had “low” \ncertainty of evidence for “sufficient” construct validity. \nBoth, the Green & Glasgow´s tool [88] and the Downs \n& Black-checklist [22], had “very low” certainty of evi -\ndence for “insufficient” construct validity.\nStructural validity and cross-cultural validity were \nnot assessed in any of the included studies.\nAbbreviations: CCBRG Cochrane Collaboration Back Review Group, CGMP criteria for good measurement properties, EPHPP Effective Public Health Practice Project, \nGRADE Grading of Recommendations Assessment, Development and Evaluation, LEGEND Let Evidence Guide Every New Decision, NICE National Institute for Health \nand Care Excellence, USPSTF U.S. Preventive Services Task Force;\nCriteria for good measurement properties: (+) = sufficient; (?) = indeterminate; (-) = insufficient, (±) or inconsistent\nLevel of evidence according to the modified GRADE approach: high, moderate, low, or very low evidence.Note: the measurement properties “structural validity” and \n“cross-cultural validity” are not presented in this table, since they were not assessed in any of the included studies\nFields left blank indicate that those measurement properties were not assessed by the study authors\na  please refer to Table S4 for more information on reliability of the “external validity”-dimension of the Downs & Black checklist\nTable 4 (continued)\nTool or dimension Content validity Internal consistency Reliability Measurement error Criterion validity Construct validity\nGeneralizability, Applicability and Predictability (GAP) checklist [76]\nCGMP (?)\nGRADE Very Low\nGartlehner´s tool [86, 87]\nCGMP (?) (-) (?) (+)\nGRADE Very Low Moderate Very Low Very Low\nGreen & Glasgow´s external validity quality rating criteria [88, 90, 91]\nCGMP (?) (+) (-)\nGRADE Very Low Very Low Very Low\n“Indirecntess”-dimension from the GRADE Handbook [48, 92, 93]\nCGMP (?) (-)\nGRADE Moderate Very Low\nLoyka´s external validity framework [75]\nCGMP (?) (?)\nGRADE Very Low Low\nmodified “Indirectness” of the Checklist for GRADE [94, 95]\nCGMP (?) (-)\nGRADE Low Very Low\nExternal validity checklist of the National Health & Medical Research Council (NHMRC) Handbook [74]\nCGMP (?)\nGRADE Very Low\nrevised Graphical Appraisal Tool for Epidemiological Studies (GATE) [72]\nCGMP (?)\nGRADE Very Low\nRating of Included Trials on the Efficacy-Effectiveness Spectrum (RITES) [47, 97]\nCGMP (+) (+) (+)\nGRADE Moderate Very Low Low\n“Selection Bias”-dimension (Section A) of EPHPP [98, 99]\nCGMP (?) (-) (+)\nGRADE Moderate Low Very Low\nSection C of the CASP checklist for RCTs [100]\nCGMP (?)\nGRADE Very Low\nWhole Systems research considerations´checklist [67]\nCGMP (?)\nGRADE Very Low\nPage 18 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \nDiscussion\nSummary and interpretation of results\nTo our knowledge this is the first systematic review \nidentifying and evaluating the measurement proper -\nties of tools to assess the external validity of RCTs. A \ntotal of 28 tools were included. Overall, for more than \nhalf (n = 17/28, 61%) of the included tools the measure -\nment properties were not reported. Only five tools had \nat least one “sufficient” measurement property. Moreo -\nver, the development process was not described in 14/28 \n(50%) of the included tools. Reliability was assessed most \nfrequently (including inter-rater and/or test-retest reli -\nability). Only three of the included tools had “sufficient” \nreliability (“very low” certainty of evidence) [47, 73, 88]. \nHypotheses testing was evaluated in four tools, with half \nof them having “sufficient” construct validity (“low” and \n“very low” certainty of evidence) [47, 98]. Measurement \nerror was evaluated in three tools, all with an “indeter -\nminate” quality rating (“low” and “very low” certainty of \nevidence) [22, 75, 86]. Criterion validity was evaluated for \none tool, having “sufficient” with “very low” certainty of \nevidence [86]. The RITES tool [47] was the measurement \ntool with the strongest evidence for validity and reli -\nability. Its content validity, based on international expert-\nconsensus, was “sufficient” with “moderate” certainty of \nevidence, while reliability and construct validity were \nrated as “sufficient” with “very low” and “low” certainty of \nevidence, respectively.\nFollowing the three criteria for the recommendation of \na measurement tool, all included tools were categorized \nas ‘B’ . Hence, further research will be required for the \nrecommendation for or against any of the included tools \n[26]. Sufficient internal consistency may not be relevant \nfor the assessment of external validity, as the measure -\nment models might not be fully reflective. However, none \nof the authors/developers did specify the measurement \nmodel of their measurement tool.\nSpecification of the measurement model is consid -\nered a requirement of the appropriateness for the latent \nconstruct of interest during scale or tool development \n[102]. It could be argued that researchers automatically \nexpect their tool to be a reflective measurement model. \nE.g., Downs and Black [22] assessed internal consistency \nwithout prior testing for unidimensionality or structural \nvalidity of the tool. Structural validity or unidimensional -\nity is a prerequisite for internal consistency [26] and both \nmeasurement properties are only relevant for reflective \nmeasurement models [103, 104]. Misspecification as well \nas lack of specification of the measurement model can \nlead to potential limitations when developing and vali -\ndating a scale or tool [102, 105]. Hence, the specification \nof measurement models should be considered in future \nresearch.\nContent validity is the most important measurement \nproperty of health measurement instruments [27] and a \nlack of face validity is considered a strong argument for \nnot using or to stop further evaluation of a measurement \ninstrument [106]. Only the RITES tool [47] had evidence \nof “sufficient” content validity. Nevertheless, this tool \ndoes not directly measure the external validity of RCTs. \nThe RITES tool [47] was developed to classify RCTs on \nan efficacy-effectiveness continuum. An RCT catego -\nrized as highly pragmatic or as having a “strong emphasis \non effectiveness” [47] implies that the study design pro -\nvides rather applicable results, but it does not automati -\ncally imply high external validity or generalizability of a \ntrial´s characteristics to other specific contexts and set -\ntings [107]. Even a highly pragmatic/effectiveness study \nmight have little applicability or generalizability to a spe -\ncific research question of review authors. An individual \nassessment of external validity may still be needed by \nreview authors in accordance with the research question \nand other contextual factors.\nAnother tool which might have some degree of con -\ntent or face validity is the indirectness-dimension of the \nGRADE method [92]. This method is a widely used and \naccepted method in research synthesis in health science \n[108]. It has been evolved over the years based on work \nfrom the GRADE Working Group and on feedback from \nusers worldwide [108]. Thus, it might be assumed that \nthis method has a high degree of face validity, although \nit has not been systematically tested for content validity.\nIf all tools are categorized as ‘B’ in a review, the COS -\nMIN guidelines suggests that the measurement instru -\nment “with the best evidence for content validity could \nbe the one to be provisionally recommended for use, \nuntil further evidence is provided” [34]. In accordance \nwith this suggestions, the use of the RITES tool [47] as an \nprovisionally solution might therefore be justified until \nmore research on this topic is available. However, users \nshould be aware of its limitations, as described above.\nImplication for future research\nThis study affirms and supplements what is already \nknown from previous reviews [9, 12, 14–18]. The het -\nerogeneity of characteristics of tools included in those \nreviews was also observed in the present review. \nAlthough Dyrvig et  al. [16] did not assess the measure -\nment properties of available tools, they reported a lack \nof empirical support of items included in measurement \ntools. The authors of previous reviews could not recom -\nmend a measurement tool. Although their conclusions \nwere mainly based on descriptive analysis rather than the \nassessment of quality of the tools, the conclusion of the \npresent systematic review is consistent with them.\nPage 19 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \nOne major challenge on this topic is the serious het -\nerogeneity regarding the terminology, criteria and guid -\nance to assess the external validity of RCTs. Development \nof new tools and/or further revision (and validation) of \navailable tools may not be appropriate before consen -\nsus-based standards are developed. Generally, it may \nbe argued whether these methods to assess the external \nvalidity in systematic reviews of interventions are suit -\nable [9, 12]. The experimental/statistical methods pre -\nsented in Table  1 may offer a more objective approach \nto evaluate the external validity of RCTs. However, they \nare not feasible to implement in the conduction of sys -\ntematic reviews. Furthermore, they focus mainly on the \ncharacteristics and generalizability of the study popula -\ntions, which is insufficient to assess the external validity \nof clinical trials [109], since they do not consider other \nrelevant dimensions of external validity such as interven -\ntion settings or treatment variables etc. [4, 109].\nThe methodological possibilities in tool/scale develop -\nment and validation regarding this topic have not been \nexploited, yet. More than 20 years ago, there was no \nconsensus regarding the definition of quality of RCTs. \nIn 1998, Verhagen et al. [110] performed a Delphi study \nto achieve consensus regarding the definition of quality \nof RCTs and to create a quality criteria list. Until now, \nthese criteria list has been a guidance in tool develop -\nment and their criteria are still being implemented in \nmethodological quality or risk of bias assessment tools \n(e.g. the Cochrane Collaboration risk of bias tool 1 & 2.0, \nthe Physiotherapy Evidence Database (PEDro) scale etc.). \nConsequently, it seems necessary to seek consensus in \norder to overcome the issues regarding the external valid-\nity of RCTs in a similar way. After reaching consensus, \nfurther development and validation is needed following \nstandard guidelines for scale/tool development (e.g. de \nVet et al. [106]; Streiner et al. [111]; DeVellis [112]). Since \nthe assessment of external validity seems highly context-\ndependent [9, 12], this should be taken into account in \nfuture research. A conventional checklist approach seems \ninappropriate [9, 12, 109] and a more comprehensive but \nflexible approach might be necessary. The experimen -\ntal/statistical methods (Table  1) may offer a reference \nstandard for convergent validity testing of the dimension \n“patient population” in future research.\nThis review has highlighted the necessity for more \nresearch in this area. Published studies and evaluation \ntools are important sources of information and should \ninform the development of a new tool or approach.\nStrengths and limitations\nOne strength of the present review is the two-phase \nsearch method. With this method we believe that the \nlikelihood of missing relevant studies was addressed \nadequately. The forward citation tracking using Scopus \nis another strength of the present review. The quality of \nthe included measurement tools was assessed with an \nadapted and comprehensive methodology (COSMIN). \nNone of the previous reviews has attempted such an \nassessment.\nThere are some limitations of the present review. First, \na search for grey literature was not performed. Second, \nwe focused on RCTs only and did not include assessment \ntools for non-randomized or other observational study \ndesign. Third, due to heterogeneity in terminology, we \nmight have missed some tools with our electronic litera -\nture search strategy. Furthermore, it was challenging to \nfind studies on measurement properties of some included \ntools, that did not have a specific name or abbreviation \n(such as EVAT). We tried to address this potential limita-\ntion by performing a comprehensive reference screening \nand snowballing (including forward citation screening).\nConclusions\nBased on the results of this review, no available measure -\nment tool can be fully recommended for the use in sys -\ntematic reviews to assess the external validity of RCTs. \nSeveral steps are required to overcome the identified dif -\nficulties before a new tool is developed or available tools \nare further revised and validated.\nAbbreviations\nCASP:  Critical Appraisal Skills Programme; CCBRG:  Cochrane Collaboration \nBack Review Group; CCT : controlled clinical trial; COSMIN: COnsensus based \nStandards for the selection of health Measurement Instruments; EPHPP: \nEffective Public Health Practice Project; EVAT: External Validity Assessment Tool; \nFAME: Feasibility, Appropriateness, Meaningfulness and Effectiveness; GATE: \nGraphical Appraisal Tool for Epidemiological Studies; GAP: Generalizability, \nApplicability and Predictability; GRADE: Grading of Recommendations Assess-\nment, Development and Evaluation; HTA: Health Technology Assessment; \nICC: intraclass correlation; LEGEND: Let Evidence Guide Every New Decision; \nNICE: National Institute for Health and Care Excellence; PEDro: Physiotherapy \nEvidence Database; PRECIS: PRagmatic Explanatory Continuum Indicator Sum-\nmary; RCT : randomized controlled trial; RITES: Rating of Included Trials on the \nEfficacy-Effectiveness Spectrum; TREND: Transparent Reporting of Evaluations \nwith Nonrandomized Designs; USPSTF: U.S. Preventive Services Task Force.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s12874- 022- 01561-5.\nAdditional file 1. \nAdditional file 2. \nAdditional file 3. \nAdditional file 4. \nAcknowledgements\nWe would like to thank Sven Bossmann and Sarah Tiemann for their assistance \nwith the elaboration of the search strategy.\nPage 20 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \nAuthors’ contributions\nAll authors contributed to the design of the study. AJ designed the search \nstrategy and conducted the systematic search. AJ and TB screened titles and \nabstracts as well as full-text reports in phase (1) AJ and KL screened titles \nand abstracts as well as full-text reports in phase (2) Data extraction was \nperformed by AJ and checked by TB. Quality appraisal and data analysis was \nperformed by AJ and JB. AJ drafted the manuscript. JB, TB and KL critically \nrevised the manuscript for important intellectual content. All authors read and \napproved the final manuscript.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nAvailability of data and materials\nAll data generated or analyzed during this study are included in this published \narticle (and its supplementary information files).\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1  Institute of Health Sciences, Department of Physiotherapy, Pain and Exer-\ncise Research Luebeck (P .E.R.L), Universität zu Lübeck, Ratzeburger Allee \n160, 23562 Lübeck, Germany. 2 Faculty of Applied Public Health, European \nUniversity of Applied Sciences, Werftstr. 5, 18057 Rostock, Germany. 3 Divi-\nsion of Physiotherapy, Department of Applied Health Sciences, Hochschule \nfür Gesundheit (University of Applied Sciences), Gesundheitscampus 6-8, \n44801 Bochum, Germany. 4 Department of Health, HSD Hochschule Döpfer \n(University of Applied Sciences), Waidmarkt 9, 50676 Cologne, Germany. \nReceived: 20 August 2021   Accepted: 28 February 2022\nReferences\n 1. Bastian H, Glasziou P , Chalmers I. Seventy-five trials and eleven \nsystematic reviews a day: how will we ever keep up? PLoS Med. \n2010;7:e1000326.\n 2. Aromataris E, Munn Z (eds). JBI Manual for Evidence Synthesis. JBI \nMan Evid Synth. 2020. https:// doi. org/ 10. 46658/ jbimes- 20- 01\n 3. Knoll T, Omar MI, Maclennan S, et al. Key Steps in Conducting \nSystematic Reviews for Underpinning Clinical Practice Guidelines: \nMethodology of the European Association of Urology. Eur Urol. \n2018;73:290–300.\n 4. Jüni P , Altman DG, Egger M. Systematic reviews in health care: Assess-\ning the quality of controlled clinical trials. BMJ. 2001;323:42–6.\n 5. Büttner F, Winters M, Delahunt E, Elbers R, Lura CB, Khan KM, Weir \nA, Ardern CL. Identifying the ’incredible’! Part 1: assessing the risk of \nbias in outcomes included in systematic reviews. Br J Sports Med. \n2020;54:798–800.\n 6. Boutron I, Page MJ, Higgins JPT, Altman DG, Lundh A, Hróbjartsson \nA, Group CBM. Considering bias and conflicts of interest among the \nincluded studies. Cochrane Handb. Syst. Rev. Interv. 2021; version 6.2 \n(updated Febr. 2021)\n 7. Cook TD, Campbell DT, Shadish W. Experimental and quasi-experimental \ndesigns for generalized causal inference. Boston: Houghton Mifflin; 2002.\n 8. Avellar SA, Thomas J, Kleinman R, Sama-Miller E, Woodruff SE, Coughlin \nR, Westbrook TR. External Validity: The Next Step for Systematic \nReviews? Eval Rev. 2017;41:283–325.\n 9. Weise A, Büchter R, Pieper D, Mathes T. Assessing context suitability \n(generalizability, external validity, applicability or transferability) of \nfindings in evidence syntheses in healthcare-An integrative review of \nmethodological guidance. Res Synth Methods. 2020;11:760–79.\n 10. Schunemann HJ, Tugwell P , Reeves BC, Akl EA, Santesso N, Spencer \nFA, Shea B, Wells G, Helfand M. Non-randomized studies as a source of \ncomplementary, sequential or replacement evidence for randomized \ncontrolled trials in systematic reviews on the effects of interventions. \nRes Synth Methods. 2013;4:49–62.\n 11. Atkins D, Chang SM, Gartlehner G, Buckley DI, Whitlock EP , Berliner E, \nMatchar D. Assessing applicability when comparing medical interven-\ntions: AHRQ and the Effective Health Care Program. J Clin Epidemiol. \n2011;64:1198–207.\n 12. Burchett HED, Blanchard L, Kneale D, Thomas J. Assessing the appli-\ncability of public health intervention evaluations from one setting to \nanother: a methodological study of the usability and usefulness of \nassessment tools and frameworks. Heal Res policy Syst. 2018;16:88.\n 13. Dekkers OM, von Elm E, Algra A, Romijn JA, Vandenbroucke JP . How to \nassess the external validity of therapeutic trials: a conceptual approach. \nInt J Epidemiol. 2010;39:89–94.\n 14. Burchett H, Umoquit M, Dobrow M. How do we know when research \nfrom one setting can be useful in another? A review of external validity, \napplicability and transferability frameworks. J Health Serv Res Policy. \n2011;16:238–44.\n 15. Cambon L, Minary L, Ridde V, Alla F. Transferability of interventions in \nhealth education: a review. BMC Public Health. 2012;12:497.\n 16. Dyrvig A-K, Kidholm K, Gerke O, Vondeling H. Checklists for external \nvalidity: a systematic review. J Eval Clin Pract. 2014;20:857–64.\n 17. Munthe-Kaas H, Nøkleby H, Nguyen L. Systematic mapping of check-\nlists for assessing transferability. Syst Rev. 2019;8:22.\n 18. Nasser M, van Weel C, van Binsbergen JJ, van de Laar FA. Generalizabil-\nity of systematic reviews of the effectiveness of health care interven-\ntions to primary health care: concepts, methods and future research. \nFam Pract. 2012;29(Suppl 1):i94–103.\n 19. Hariton E, Locascio JJ. Randomised controlled trials - the gold standard \nfor effectiveness research: Study design: randomised controlled trials. \nBJOG. 2018;125:1716.\n 20. Pressler TR, Kaizar EE. The use of propensity scores and observational \ndata to estimate randomized controlled trial generalizability bias. Stat \nMed. 2013;32:3552–68.\n 21. Rothwell PM. External validity of randomised controlled trials: “to whom \ndo the results of this trial apply?” Lancet. 2005;365:82–93.\n 22. Downs SH, Black N. The feasibility of creating a checklist for the \nassessment of the methodological quality both of randomised and \nnon-randomised studies of health care interventions. J Epidemiol Com-\nmunity Health. 1998;52:377–84.\n 23. Page MJ, Moher D, Bossuyt PM, et al. PRISMA 2020 explanation and \nelaboration: updated guidance and exemplars for reporting systematic \nreviews. BMJ. 2021;372:n160.\n 24. Clark R, Locke M, Hill B, Wells C, Bialocerkowski A. Clinimetric properties \nof lower limb neurological impairment tests for children and young \npeople with a neurological condition: A systematic review. PLoS One. \n2017;12:e0180031.\n 25. Mokkink LB, de Vet HCW, Prinsen CAC, Patrick DL, Alonso J, Bouter LM, \nTerwee CB. COSMIN Risk of Bias checklist for systematic reviews of \nPatient-Reported Outcome Measures. Qual Life Res. 2018;27:1171–9.\n 26. Prinsen CAC, Mokkink LB, Bouter LM, Alonso J, Patrick DL, de Vet HCW, \nTerwee CB. COSMIN guideline for systematic reviews of patient-\nreported outcome measures. Qual Life Res. 2018;27:1147–57.\n 27. Terwee CB, Prinsen CAC, Chiarotto A, Westerman MJ, Patrick DL, Alonso \nJ, Bouter LM, de Vet HCW, Mokkink LB. COSMIN methodology for \nevaluating the content validity of patient-reported outcome measures: \na Delphi study. Qual Life Res. 2018;27:1159–70.\n 28. Stephenson M, Riitano D, Wilson S, Leonardi-Bee J, Mabire C, Cooper K, \nMonteiro da Cruz D, Moreno-Casbas MT, Lapkin S. Chap. 12: Systematic \nReviews of Measurement Properties. JBI Man Evid Synth. 2020 https:// \ndoi. org/ 10. 46658/ JBIMES- 20- 13\n 29. Glover PD, Gray H, Shanmugam S, McFadyen AK. Evaluating collabora-\ntive practice within community-based integrated health and social \ncare teams: a systematic review of outcome measurement instruments. \nJ Interprof Care. 2021;1–15. https:// doi. org/ 10. 1080/ 13561 820. 2021. \n19022 92. Epub ahead of print.\nPage 21 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \n 30. Maassen SM, Weggelaar Jansen AMJW, Brekelmans G, Vermeulen H, \nvan Oostveen CJ. Psychometric evaluation of instruments measuring \nthe work environment of healthcare professionals in hospitals: a sys-\ntematic literature review. Int J Qual Heal care J Int Soc Qual Heal Care. \n2020;32:545–57.\n 31. Jabri Yaqoob MohammedAl, Kvist F, Azimirad T, Turunen M. A system-\natic review of healthcare professionals’ core competency instruments. \nNurs Health Sci. 2021;23:87–102.\n 32. Mokkink LB, Terwee CB, Patrick DL, Alonso J, Stratford PW, Knol DL, \nBouter LM, de Vet HCW. The COSMIN study reached international \nconsensus on taxonomy, terminology, and definitions of measurement \nproperties for health-related patient-reported outcomes. J Clin Epide-\nmiol. 2010;63:737–45.\n 33. Jung A, Balzer J, Braun T, Luedtke K. Psychometric properties of tools to \nmeasure the external validity of randomized controlled trials: a system-\natic review protocol. 2020; https:// doi. org/ 10. 17605/ OSF. IO/ PTG4D\n 34. Mokkink LB, Prinsen CAC, Patrick DL, Alonso J, Bouter LM, de Vet HCW, \nTerwee CB COSMIN manual for systematic reviews of PROMs, user \nmanual. 2018;1–78. https:// www. cosmin. nl/ wp- conte nt/ uploa ds/ \nCOSMIN- syst- review- for- PROMs- manual_ versi on-1_ feb- 2018-1. pdf. \nAccessed 3 Feb 2020.\n 35. Bialocerkowski A, O’shea K, Pin TW. Psychometric properties of out-\ncome measures for children and adolescents with brachial plexus birth \npalsy: a systematic review. Dev Med Child Neurol. 2013;55:1075–88.\n 36. Matthews J, Bialocerkowski A, Molineux M. Professional identity \nmeasures for student health professionals - a systematic review of \npsychometric properties. BMC Med Educ. 2019;19:308.\n 37. Terwee CB, Jansma EP , Riphagen II, De Vet HCW. Development of a \nmethodological PubMed search filter for finding studies on meas-\nurement properties of measurement instruments. Qual Life Res. \n2009;18:1115–23.\n 38. Sierevelt IN, Zwiers R, Schats W, Haverkamp D, Terwee CB, Nolte PA, \nKerkhoffs GMMJ. Measurement properties of the most commonly used \nFoot- and Ankle-Specific Questionnaires: the FFI, FAOS and FAAM. A sys-\ntematic review. Knee Surg Sports Traumatol Arthrosc. 2018;26:2059–73.\n 39. van der Hout A, Neijenhuijs KI, Jansen F, et al. Measuring health-related \nquality of life in colorectal cancer patients: systematic review of meas-\nurement properties of the EORTC QLQ-CR29. Support Care Cancer. \n2019;27:2395–412.\n 40. Whiting P , Savović J, Higgins JPT, Caldwell DM, Reeves BC, Shea B, \nDavies P , Kleijnen J, Churchill R. ROBIS: A new tool to assess risk of bias \nin systematic reviews was developed. J Clin Epidemiol. 2016;69:225–34.\n 41. Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyan-a web \nand mobile app for systematic reviews. Syst Rev. 2016;5:210.\n 42. Moher D, Hopewell S, Schulz KF, Montori V, Gøtzsche PC, Devereaux PJ, \nElbourne D, Egger M, Altman DG. CONSORT 2010 explanation and elab-\noration: updated guidelines for reporting parallel group randomised \ntrials. Int J Surg. 2012;10:28–55.\n 43. Mokkink LB, Terwee CB. The COSMIN checklist for assessing the \nmethodological quality of studies on measurement properties of \nhealth status measurement instruments: an international Delphi study. \n2010;539–549\n 44. Terwee CB, Prinsen CA, Chiarotto A, De Vet H, Bouter LM, Alonso J, West-\nerman MJ, Patrick DL, Mokkink LB. COSMIN methodology for assessing \nthe content validity of PROMs–user manual. Amsterdam VU Univ. Med. \nCent. 2018; https:// cosmin. nl/ wp- conte nt/ uploa ds/ COSMIN- metho \ndology- for- conte nt- valid ity- user- manual- v1. pdf. Accessed 3 Feb 2020.\n 45. Mustafa RA, Santesso N, Brozek J, et al. The GRADE approach is repro-\nducible in assessing the quality of evidence of quantitative evidence \nsyntheses. J Clin Epidemiol. 2013;66:735–6.\n 46. Jennings H, Hennessy K, Hendry GJ. The clinical effectiveness of intra-\narticular corticosteroids for arthritis of the lower limb in juvenile idi-\nopathic arthritis: A systematic review. Pediatr Rheumatol. 2014. https:// \ndoi. org/ 10. 1186/ 1546- 0096- 12- 23.\n 47. Wieland LS, Berman BM, Altman DG, et al. Rating of Included Trials on \nthe Efficacy-Effectiveness Spectrum: development of a new tool for \nsystematic reviews. J Clin Epidemiol. 2017;84:95–104.\n 48. Atkins D, Briss PA, Eccles M, et al. Systems for grading the quality of \nevidence and the strength of recommendations II: pilot study of a new \nsystem. BMC Health Serv Res. 2005;5:25.\n 49. Abraham NS, Wieczorek P , Huang J, Mayrand S, Fallone CA, Barkun AN. \nAssessing clinical generalizability in sedation studies of upper GI endos-\ncopy. Gastrointest Endosc. 2004;60:28–33.\n 50. Arabi YM, Cook DJ, Zhou Q, et al. Characteristics and Outcomes of \nEligible Nonenrolled Patients in a Mechanical Ventilation Trial of \nAcute Respiratory Distress Syndrome. Am J Respir Crit Care Med. \n2015;192:1306–13.\n 51. Williams AC, de Nicholas C, Richardson MK, de Pither PH, FAC. General-\nizing from a controlled trial: The effects of patient preference versus \nrandomization on the outcome of inpatient versus outpatient chronic \npain management. Pain. 1999;83:57–65.\n 52. De Jong Z, Munneke M, Jansen LM, Ronday K, Van Schaardenburg \nDJ, Brand R, Van Den Ende CHM, Vliet Vlieland TPM, Zuijderduin WM, \nHazes JMW. Differences between participants and nonparticipants in \nan exercise trial for adults with rheumatoid arthritis. Arthritis Care Res. \n2004;51:593–600.\n 53. Hordijk-Trion M, Lenzen M, Wijns W, et al. Patients enrolled in coronary \nintervention trials are not representative of patients in clinical practice: \nResults from the Euro Heart Survey on Coronary Revascularization. Eur \nHeart J. 2006;27:671–8.\n 54. Wilson A, Parker H, Wynn A, Spiers N. Performance of hospital-at-home \nafter a randomised controlled trial. J Heal Serv Res Policy. 2003;8:160–4.\n 55. Smyth B, Haber A, Trongtrakul K, Hawley C, Perkovic V, Woodward M, \nJardine M. Representativeness of Randomized Clinical Trial Cohorts \nin End-stage Kidney Disease: A Meta-analysis. JAMA Intern Med. \n2019;179:1316–24.\n 56. Leinonen A, Koponen M, Hartikainen S. Systematic Review: Representa-\ntiveness of Participants in RCTs of Acetylcholinesterase Inhibitors. PLoS \nOne. 2015;10:e0124500–e0124500.\n 57. Chari A, Romanus D, Palumbo A, Blazer M, Farrelly E, Raju A, Huang H, \nRichardson P . Randomized Clinical Trial Representativeness and Out-\ncomes in Real-World Patients: Comparison of 6 Hallmark Randomized \nClinical Trials of Relapsed/Refractory Multiple Myeloma. Clin Lymphoma \nMyeloma Leuk. 2020;20:8.\n 58. Susukida R, Crum RM, Ebnesajjad C, Stuart EA, Mojtabai R. Generaliz-\nability of findings from randomized controlled trials: application to \nthe National Institute of Drug Abuse Clinical Trials Network. Addiction. \n2017;112:1210–9.\n 59. Zarin DA, Young JL, West JC. Challenges to evidence-based medicine: \na comparison of patients and treatments in randomized controlled \ntrials with patients and treatments in a practice research network. Soc \nPsychiatry Psychiatr Epidemiol. 2005;40:27–35.\n 60. Gheorghe A, Roberts T, Hemming K, Calvert M. Evaluating the General-\nisability of Trial Results: Introducing a Centre- and Trial-Level Generalis-\nability Index. Pharmacoeconomics. 2015;33:1195–214.\n 61. He Z, Wang S, Borhanian E, Weng C. Assessing the Collective Population \nRepresentativeness of Related Type 2 Diabetes Trials by Combining \nPublic Data from ClinicalTrials.gov and NHANES. Stud Health Technol \nInform. 2015;216:569–73.\n 62. Schmidt AF, Groenwold RHH, van Delden JJM, van der Does Y, Klungel \nOH, Roes KCB, Hoes AW, van der Graaf R. Justification of exclusion \ncriteria was underreported in a review of cardiovascular trials. J Clin \nEpidemiol. 2014;67:635–44.\n 63. Carr DB, Goudas LC, Balk EM, Bloch R, Ioannidis JP , Lau J. Evidence report \non the treatment of pain in cancer patients. J Natl Cancer Inst Monogr. \n2004;32:23–31.\n 64. Clegg A, Bryant J, Nicholson T, et al. Clinical and cost-effectiveness \nof donepezil, rivastigmine and galantamine for Alzheimer’s dis-\nease: a rapid and systematic review. Health Technol Assess (Rockv). \n2001;5:1–136.\n 65. Foy R, Hempel S, Rubenstein L, Suttorp M, Seelig M, Shanman R, Shek-\nelle PG. Meta-analysis: effect of interactive communication between \ncollaborating primary care physicians and specialists. Ann Intern Med. \n2010;152:247–58.\n 66. Haraldsson BG, Gross AR, Myers CD, Ezzo JM, Morien A, Goldsmith \nC, Peloso PM, Bronfort G. Massage for mechanical neck disorders. \nCochrane database Syst Rev. 2006. https:// doi. org/ 10. 1002/ 14651 858. \nCD004 871. pub3.\n 67. Hawk C, Khorsan R, AJ L, RJ F. Chiropractic care for nonmusculoskeletal \nconditions: a systematic review with implications for whole systems \nresearch. J Altern Complement Med. 2007;13:491–512.\nPage 22 of 23Jung et al. BMC Medical Research Methodology          (2022) 22:100 \n 68. Karjalainen K, Malmivaara A, van Tulder M, et al. Multidisciplinary \nrehabilitation for fibromyalgia and musculoskeletal pain in working \nage adults. Cochrane Database Syst Rev. 2000. https:// doi. org/ 10. 1002/ \n14651 858. CD001 984.\n 69. Liberati A, Himel HN, Chalmers TC. A quality assessment of randomized \ncontrol trials of primary treatment of breast cancer. J Clin Oncol. \n1986;4:942–51.\n 70. Averis A, Pearson A. Filling the gaps: identifying nursing research priori-\nties through the analysis of completed systematic reviews. Jbi Reports. \n2003;1:49–126.\n 71. Sorg C, Schmidt J, Büchler MW, Edler L, Märten A. Examination of \nexternal validity in randomized controlled trials for adjuvant treatment \nof pancreatic adenocarcinoma. Pancreas. 2009;38:542–50.\n 72. National Institute for Health and Care Excellence. Methods for the \ndevelopment of NICE public health guidance, Third edit. National \nInstitute for Health and Care Excellence. 2012; https:// www. nice. org. uk/ \nproce ss/ pmg4/ chapt er/ intro ducti on. Accessed 15 Apr 2020\n 73. U.S. Preventive Services Task Force. Criteria for Assessing External \nValidity (Generalizability) of Individual Studies. US Prev Serv Task Force \nAppendix VII. 2017; https:// uspre venti veser vices taskf orce. org/ uspstf/ \nabout- uspstf/ metho ds- and- proce sses/ proce dure- manual/ proce dure- \nmanual- appen dix- vii- crite ria- asses sing- exter nal- valid ity- gener aliza \nbility- indiv idual- studi es. Accessed 15 Apr 2020.\n 74. National Health and Medical Research Council NHMRC handbooks. \nhttps:// www. nhmrc. gov. au/ about- us/ publi catio ns/ how- prepa re- and- \nprese nt- evide nce- based- infor mation- consu mers- health- servi ces# block- \nviews- block- file- attac hments- conte nt- block-1. Accessed 15 Apr 2020.\n 75. Loyka CM, Ruscio J, Edelblum AB, Hatch L, Wetreich B, Zabel Caitlin M. \nWeighing people rather than food: A framework for examining external \nvalidity. Perspect Psychol Sci. 2020;15:483–96.\n 76. Fernandez-Hermida JR, Calafat A, Becoña E, Tsertsvadze A, Foxcroft DR. \nAssessment of generalizability, applicability and predictability (GAP) for \nevaluating external validity in studies of universal family-based preven-\ntion of alcohol misuse in young people: systematic methodological \nreview of randomized controlled trials. Addiction. 2012;107:1570–9.\n 77. Clark E, Burkett K, Stanko-Lopp D. Let Evidence Guide Every New \nDecision (LEGEND): an evidence evaluation system for point-of-\ncare clinicians and guideline development teams. J Eval Clin Pract. \n2009;15:1054–60.\n 78. Bornhöft G, Maxion-Bergemann S, Wolf U, Kienle GS, Michalsen A, \nVollmar HC, Gilbertson S, Matthiessen PF. Checklist for the qualitative \nevaluation of clinical studies with particular focus on external validity \nand model validity. BMC Med Res Methodol. 2006;6:56.\n 79. Cho MK, Bero LA. Instruments for assessing the quality of drug \nstudies published in the medical literature. JAMA J Am Med Assoc. \n1994;272:101–4.\n 80. Cho MK, Bero LA. The quality of drug studies published in symposium \nproceedings. Ann Intern Med 1996;124:485–489\n 81. van Tulder M, Furlan A, Bombardier C, Bouter L. Updated method \nguidelines for systematic reviews in the cochrane collaboration back \nreview group. Spine (Phila Pa 1976). 2003;28:1290–9.\n 82. Estrada F, Atienzo EE, Cruz-Jiménez L, Campero L. A Rapid Review \nof Interventions to Prevent First Pregnancy among Adolescents \nand Its Applicability to Latin America. J Pediatr Adolesc Gynecol. \n2021;34:491–503.\n 83. Khorsan R, Crawford C. How to assess the external validity and model \nvalidity of therapeutic trials: a conceptual approach to system-\natic review methodology. Evid Based Complement Alternat Med. \n2014;2014:694804.\n 84. O’Connor SR, Tully MA, Ryan B, Bradley JM, Baxter GD, McDonough SM. \nFailure of a numerical quality assessment scale to identify potential \nrisk of bias in a systematic review: a comparison study. BMC Res Notes. \n2015;8:224.\n 85. Chalmers TC, Smith H, Blackburn B, Silverman B, Schroeder B, Reitman \nD, Ambroz A. A method for assessing the quality of a randomized \ncontrol trial. Control Clin Trials. 1981;2:31–49.\n 86. Gartlehner G, Hansen RA, Nissman D, Lohr KN, Carey TS. A simple and \nvalid tool distinguished efficacy from effectiveness studies. J Clin Epide-\nmiol. 2006;59:1040–8.\n 87. Zettler LL, Speechley MR, Foley NC, Salter KL, Teasell RW. A scale for \ndistinguishing efficacy from effectiveness was adapted and applied to \nstroke rehabilitation studies. J Clin Epidemiol. 2010;63:11–8.\n 88. Green LW, Glasgow RE. Evaluating the relevance, generalization, and \napplicability of research: issues in external validation and translation \nmethodology. Eval Health Prof. 2006;29:126–53.\n 89. Glasgow RE, Vogt TM, Boles SM. Evaluating the public health impact of \nhealth promotion interventions: the RE-AIM framework. Am J Public \nHealth. 1999;89:1322–7.\n 90. Mirza NA, Akhtar-Danesh N, Staples E, Martin L, Noesgaard C. Compara-\ntive Analysis of External Validity Reporting in Non-randomized Interven-\ntion Studies. Can J Nurs Res. 2014;46:47–64.\n 91. Laws RA, St George AB, Rychetnik L, Bauman AE. Diabetes prevention \nresearch: a systematic review of external validity in lifestyle interven-\ntions. Am J Prev Med. 2012;43:205–14.\n 92. Schünemann H, Brożek J, Guyatt G, Oxman A. Handbook for grad-\ning the quality of evidence and the strength of recommendations \nusing the GRADE approach (updated October 2013). GRADE Work. \nGr. 2013; https:// gdt. grade pro. org/ app/ handb ook/ handb ook. html. \nAccessed 15 Apr 2020.\n 93. Wu XY, Chung VCH, Wong CHL, Yip BHK, Cheung WKW, Wu JCY. CHIME-\nRAS showed better inter-rater reliability and inter-consensus reliability \nthan GRADE in grading quality of evidence: A randomized controlled \ntrial. Eur J Integr Med. 2018;23:116–22.\n 94. Meader N, King K, Llewellyn A, Norman G, Brown J, Rodgers M, Moe-\nByrne T, Higgins JPT, Sowden A, Stewart G. A checklist designed to \naid consistency and reproducibility of GRADE assessments: Develop-\nment and pilot validation. Syst Rev. 2014. https:// doi. org/ 10. 1186/ \n2046- 4053-3- 82.\n 95. Llewellyn A, Whittington C, Stewart G, Higgins JP , Meader N. The Use \nof Bayesian Networks to Assess the Quality of Evidence from Research \nSynthesis: 2. Inter-Rater Reliability and Comparison with Standard \nGRADE Assessment. PLoS One. 2015;10:e0123511.\n 96. Jackson R, Ameratunga S, Broad J, Connor J, Lethaby A, Robb G, Wells S, \nGlasziou P , Heneghan C. The GATE frame: critical appraisal with pictures. \nEvid Based Med 2006;11:35 LP– 38\n 97. Aves T. The Role of Pragmatism in Explaining Heterogeneity in \nMeta-Analyses of Randomized Trials: A Methodological Review. 2017; \nMcMaster University. http:// hdl. handle. net/ 11375/ 22212. Accessed 12 \nJan 2021.\n 98. Thomas BH, Ciliska D, Dobbins M, Micucci S. A process for systemati-\ncally reviewing the literature: providing the research evidence for \npublic health nursing interventions. Worldviews Evidence-Based Nurs. \n2004;1:176–84.\n 99. Armijo-Olivo S, Stiles CR, Hagen NA, Biondo PD, Cummings GG. Assess-\nment of study quality for systematic reviews: a comparison of the \nCochrane Collaboration Risk of Bias Tool and the Effective Public Health \nPractice Project Quality Assessment Tool: methodological research. J \nEval Clin Pract. 2012;18:12–8.\n 100. Critical Appraisal Skills Programme. CASP Randomised Controlled Trial \nStandard Checklist. 2020; https:// casp- uk. net/ casp- tools- check lists/. \nAccessed 10 Dec 2020.\n 101. Aves T, Allan KS, Lawson D, Nieuwlaat R, Beyene J, Mbuagbaw L. The \nrole of pragmatism in explaining heterogeneity in meta-analyses of ran-\ndomised trials: a protocol for a cross-sectional methodological review. \nBMJ Open. 2017;7:e017887.\n 102. Diamantopoulos A, Riefler P , Roth KP . Advancing formative measure-\nment models. J Bus Res. 2008;61:1203–18.\n 103. Fayers PM, Hand DJ. Factor analysis, causal indicators and quality of life. \nQual Life Res. 1997. https:// doi. org/ 10. 1023/A: 10264 90117 121.\n 104. Streiner DL. Being Inconsistent About Consistency: When Coefficient \nAlpha Does and Doesn’t Matter. J Pers Assess. 2003;80:217–22.\n 105. MacKenzie SB, Podsakoff PM, Jarvis CB. The Problem of Measurement \nModel Misspecification in Behavioral and Organizational Research and \nSome Recommended Solutions. J Appl Psychol. 2005;90:710–30.\n 106. De Vet HCW, Terwee CB, Mokkink LB, Knol DL. Measurement in medi-\ncine: a practical guide. 2011; https:// doi. org/ 10. 1017/ CBO97 80511 \n996214\n 107. Dekkers OM, Bossuyt PM, Vandenbroucke JP . How trial results are \nintended to be used: is PRECIS-2 a step forward? J Clin Epidemiol. \n2017;84:25–6.\nPage 23 of 23\nJung et al. BMC Medical Research Methodology          (2022) 22:100 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 108. Brozek JL, Canelo-Aybar C, Akl EA, et al. GRADE Guidelines 30: the \nGRADE approach to assessing the certainty of modeled evidence-An \noverview in the context of health decision-making. J Clin Epidemiol. \n2021;129:138–50.\n 109. Burchett HED, Kneale D, Blanchard L, Thomas J. When assessing gen-\neralisability, focusing on differences in population or setting alone is \ninsufficient. Trials. 2020;21:286.\n 110. Verhagen AP , de Vet HCW, de Bie RA, Kessels AGH, Boers M, Bouter LM, \nKnipschild PG. The Delphi List: A Criteria List for Quality Assessment of \nRandomized Clinical Trials for Conducting Systematic Reviews Devel-\noped by Delphi Consensus. J Clin Epidemiol. 1998;51:1235–41.\n 111. Streiner DL, Norman GR, Cairney J. Health measurement scales: a \npractical guide to their development and use, Fifth edit. Oxford: Oxford \nUniversity Press; 2015.\n 112. DeVellis RF. Scale development: Theory and applications, Fourth edi. Los \nAngeles: Sage publications; 2017.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "External validity",
  "concepts": [
    {
      "name": "External validity",
      "score": 0.6403038501739502
    },
    {
      "name": "Randomized controlled trial",
      "score": 0.6105273962020874
    },
    {
      "name": "Identification (biology)",
      "score": 0.5965276956558228
    },
    {
      "name": "Systematic review",
      "score": 0.5747854709625244
    },
    {
      "name": "MEDLINE",
      "score": 0.4618019461631775
    },
    {
      "name": "Medicine",
      "score": 0.4617028832435608
    },
    {
      "name": "Internal validity",
      "score": 0.4551934003829956
    },
    {
      "name": "Research design",
      "score": 0.42067304253578186
    },
    {
      "name": "Psychology",
      "score": 0.35334211587905884
    },
    {
      "name": "Computer science",
      "score": 0.3435210883617401
    },
    {
      "name": "Statistics",
      "score": 0.22856906056404114
    },
    {
      "name": "Mathematics",
      "score": 0.134789377450943
    },
    {
      "name": "Pathology",
      "score": 0.12072297930717468
    },
    {
      "name": "Social psychology",
      "score": 0.10687211155891418
    },
    {
      "name": "Biology",
      "score": 0.093467116355896
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9341345",
      "name": "University of Lübeck",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210090743",
      "name": "European University of Applied Sciences",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210131861",
      "name": "Hochschule für Gesundheit - University of Applied Sciences",
      "country": "DE"
    }
  ]
}