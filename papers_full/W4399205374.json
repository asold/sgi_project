{
  "title": "The Persuasive Power of Large Language Models",
  "url": "https://openalex.org/W4399205374",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5081345346",
      "name": "Simon Martin Breum",
      "affiliations": [
        "IT University of Copenhagen"
      ]
    },
    {
      "id": "https://openalex.org/A5093598340",
      "name": "Daniel Vædele Egdal",
      "affiliations": [
        "IT University of Copenhagen"
      ]
    },
    {
      "id": "https://openalex.org/A5093598341",
      "name": "Victor Gram Mortensen",
      "affiliations": [
        "IT University of Copenhagen"
      ]
    },
    {
      "id": "https://openalex.org/A3105968541",
      "name": "Anders Giovanni Møller",
      "affiliations": [
        "IT University of Copenhagen"
      ]
    },
    {
      "id": "https://openalex.org/A2072290952",
      "name": "Luca Maria Aiello",
      "affiliations": [
        "IT University of Copenhagen",
        "Pioneer (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5081345346",
      "name": "Simon Martin Breum",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093598340",
      "name": "Daniel Vædele Egdal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093598341",
      "name": "Victor Gram Mortensen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3105968541",
      "name": "Anders Giovanni Møller",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2072290952",
      "name": "Luca Maria Aiello",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4324108791",
    "https://openalex.org/W3162966461",
    "https://openalex.org/W4385681712",
    "https://openalex.org/W4386184788",
    "https://openalex.org/W3098899291",
    "https://openalex.org/W3032046549",
    "https://openalex.org/W2100758964",
    "https://openalex.org/W4404906291",
    "https://openalex.org/W4389650052",
    "https://openalex.org/W4385474047",
    "https://openalex.org/W4353007481",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W4385808091",
    "https://openalex.org/W4365999098",
    "https://openalex.org/W4388787757",
    "https://openalex.org/W1556398798",
    "https://openalex.org/W4388092132",
    "https://openalex.org/W4323347737",
    "https://openalex.org/W2763110165",
    "https://openalex.org/W4366588626",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W4319265466",
    "https://openalex.org/W4388584564",
    "https://openalex.org/W4386794445",
    "https://openalex.org/W4327673264",
    "https://openalex.org/W4366769817",
    "https://openalex.org/W4378718176",
    "https://openalex.org/W4388183848",
    "https://openalex.org/W4307566460",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W1926550788",
    "https://openalex.org/W4378464611",
    "https://openalex.org/W4387561528",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4388275389",
    "https://openalex.org/W4387299971",
    "https://openalex.org/W3125855391",
    "https://openalex.org/W3120827806",
    "https://openalex.org/W4389156617",
    "https://openalex.org/W2315226716"
  ],
  "abstract": "The increasing capability of Large Language Models to act as human-like social agents raises two important questions in the area of opinion dynamics. First, whether these agents can generate effective arguments that could be injected into the online discourse to steer the public opinion. Second, whether artificial agents can interact with each other to reproduce dynamics of persuasion typical of human social systems, opening up opportunities for studying synthetic social systems as faithful proxies for opinion dynamics in human populations. To address these questions, we designed a synthetic persuasion dialogue scenario on the topic of climate change, where a 'convincer' agent generates a persuasive argument for a 'skeptic' agent, who subsequently assesses whether the argument changed its internal opinion state. Different types of arguments were generated to incorporate different linguistic dimensions underpinning psycho-linguistic theories of opinion change. We then asked human judges to evaluate the persuasiveness of machine-generated arguments. Arguments that included factual knowledge, markers of trust, expressions of support, and conveyed status were deemed most effective according to both humans and agents, with humans reporting a marked preference for knowledge-based arguments. Our experimental framework lays the groundwork for future in-silico studies of opinion dynamics, and our findings suggest that artificial agents have the potential of playing an important role in collective processes of opinion formation in online social media.",
  "full_text": "The Persuasive Power of Large Language Models\nSimon Martin Breum1, Daniel Vædele Egdal1, Victor Gram Mortensen1,\nAnders Giovanni Møller1, ∗, Luca Maria Aiello1, 2, †\n1IT University of Copenhagen, Denmark\n2Pioneer Centre for AI, Denmark\n∗agmo@itu.dk, †luai@itu.dk\nAbstract\nThe increasing capability of Large Language Models to act\nas human-like social agents raises two important questions in\nthe area of opinion dynamics. First, whether these agents can\ngenerate effective arguments that could be injected into the\nonline discourse to steer the public opinion. Second, whether\nartificial agents can interact with each other to reproduce dy-\nnamics of persuasion typical of human social systems, open-\ning up opportunities for studying synthetic social systems as\nfaithful proxies for opinion dynamics in human populations.\nTo address these questions, we designed a synthetic persua-\nsion dialogue scenario on the topic of climate change, where\na ‘convincer’ agent generates a persuasive argument for a\n‘skeptic’ agent, who subsequently assesses whether the ar-\ngument changed its internal opinion state. Different types of\narguments were generated to incorporate different linguistic\ndimensions underpinning psycho-linguistic theories of opin-\nion change. We then asked human judges to evaluate the per-\nsuasiveness of machine-generated arguments. Arguments that\nincluded factual knowledge, markers of trust, expressions of\nsupport, and conveyed status were deemed most effective ac-\ncording to both humans and agents, with humans reporting\na marked preference for knowledge-based arguments. Our\nexperimental framework lays the groundwork for future in-\nsilico studies of opinion dynamics, and our findings suggest\nthat artificial agents have the potential of playing an important\nrole in collective processes of opinion formation in online so-\ncial media.\nIntroduction\nLarge Language Models (LLMs) exhibit high proficiency\nin handling language semantics, enabling them not only\nto solve complex tasks of text understanding and genera-\ntion (Bubeck et al. 2023), but also to operate as social agents\ncapable of complex interactions with both humans and other\nartificial agents (Park et al. 2023; Xi et al. 2023). LLMs\ncan be imbued with a personality, retain memory of pre-\nvious interactions, and adaptively respond to social stim-\nuli (Wang et al. 2023). These unprecedented capabilities\nhave led researchers to envision opportunities for construc-\ntive human-computer cooperation (Papachristou, Yang, and\nHsu 2023; Argyle et al. 2023) while also raising concerns\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: High-level overview of our LLM-based agent em-\nulation of a persuasion dialogue.\nabout catastrophic scenarios where AI agents, seamlessly in-\ntegrated into the online discourse, could spread misinforma-\ntion, harmful content, and ‘semantic garbage’ (Floridi and\nChiriatti 2020; Weidinger et al. 2022; Hendrycks, Mazeika,\nand Woodside 2023).\nIn most scenarios pictured by experts, LLMs are bound\nto transform the Web into a platform where humans and\nAI agents co-exist and are often indistinguishable from each\nother. This is plausible, considering that LLM-generated text\nclosely resembles human-written text in terms of style and\nperceived credibility (Kreps, McCain, and Brundage 2022;\nJakesch, Hancock, and Naaman 2023), it is virtually impos-\nsible to detect algorithmically (Sadasivan et al. 2023), and it\ncan be inexpensively generated on consumer hardware using\nusing open-source models that are rapidly approaching the\nperformance of large-scale, company-owned language mod-\nels (Jiang et al. 2023). Organized botnets spreading large\nvolumes of machine-generated content have been already\nspotted on social media (Yang and Menczer 2023).\nDeepening our understanding of the capabilities of LLMs\nas social agents is crucial to maximize opportunities and mit-\nigate risks. In this context, a key open question is how effec-\ntive LLMs are in persuading people to change their opin-\nion on a topic (Burtell and Woodside 2023). This question\nProceedings of the Eighteenth International AAAI Conference on Web and Social Media (ICWSM2024)\n152\nhas profound implications on the evolution of the demo-\ncratic discourse on the Web: persuasive LLMs could either\nstimulate an informed public to act towards positive change\nto benefit collective good, or serve as agents of deception\ndisseminating misinformation and fueling conflict. The re-\nlated question of whether LLMs can convince other artifi-\ncial agents to alter their opinion state on a given topic is also\nof significant interest for Computational Social Science re-\nsearch. Specifically, if arguments that can persuade artificial\nagents were to be effective also in convincing people, so-\ncial interactions between agents could serve as a proxy for\nstudying opinion dynamics in human populations. This op-\nportunity becomes particularly relevant as research access\nto sources of behavioral data is narrowing due to tighten-\ning API restrictions and increasing concerns over the use of\npersonal data (Pera, Morales, and Aiello 2023).\nOur knowledge of the dynamics of persuasion and opin-\nion change in human-AI social systems is still very limited\n(see Related Work). This study aims to enhance our under-\nstanding of this area by addressing three key questions:\nRQ1: Can LLMs emulate realistic dynamics of persuasion\nand opinion change?\nRQ2: Can LLMs be prompted to generate arguments using\nvarious persuasion strategies?\nRQ3: Are arguments that are persuasive to LLM agents also\nperceived as effective by humans?\nTo answer these questions, we established a simple sce-\nnario of persuasion dialogue (Prakken 2006) on the topic of\nclimate change. In this scenario, a Convincer agent gener-\nated a one-off argument to convince a Skeptic agent, who\nthen evaluated whether the argument changed its internal\nopinion state (Figure 1). To determine whether the outcome\nof the interaction aligns with expectations from human so-\ncial systems, we experimented with different dialogue con-\nditions. Specifically, we varied the Skeptic’s level of stub-\nbornness, and we prompted the Convincer to use a variety\nof argument types whose relative effectiveness has been es-\ntimated in previous work (Monti et al. 2022). Finally, we\nasked human judges to assess the persuasiveness of LLM-\ngenerated arguments, aiming to find whether arguments that\nare effective in changing the agent’s opinion state are also\nperceived as persuasive by humans.\nWe found that the interactions between artificial agents\nmatch some characteristics typical of human interactions:\nthe probability of opinion change decreases with the Skep-\ntic’s stubbornness and grows when the Convincer’s argu-\nment conveys trust, knowledge, status, and support. Inter-\nestingly, human judges also ranked arguments containing\nthese four dimensions as the most convincing, but showed\na disproportionate preference for arguments rich in factual\nknowledge compared to those most convincing according to\nthe LLM agents. Despite some discrepancies, these findings\nsuggest that simple persuasion dialogue scenarios among\nagents share several characteristics with their human coun-\nterparts. The main implications of our results are that simu-\nlating human opinion dynamics is within the capabilities of\nLLMs, and that artificial agents have the potential of playing\nan important role in collective processes of opinion forma-\ntion in online social media.\nMethods\nExperimental Design\nConversation Setup. We established a setting to model\na dyadic interaction between the Convincer and the Skep-\ntic. Both agents were based on the Llama-2-70B-chat\nmodel, an open-source LLM, released under a commercial\nuse license 1, that has shown comparable performance to\nleading proprietary models across several tasks (Touvron,\nHugo et al. 2023). The Llama 2 model requires two prompts\nto generate text: a fixed system prompt that encodes the\ntask and personality assigned to the agent, and a prompt\nthat contains the message the agent is asked to respond to.\nAs Llama 2 is stateless, memory of previous interactions\nis maintained by incorporating a conversation log into the\nprompt, to which new messages are appended. This log is\nsimply a copy of all prior messages exchanged between\nthe agents, structured according to the Llama 2 chat tem-\nplate (Hugging Face 2023).\nAdopting a simple configuration from previous re-\nsearch (Chuang et al. 2023), we used minimal system\nprompts to initiate both agents with a brief description of\ntheir respective roles. The interaction between the agents\nthen unfolded in five stages, where the Convincer had one\nchance to produce a convincing argument:\n1. The Skeptic expressed doubts about climate change.\n2. The Convincer generated an argument to persuade the\nSkeptic to reconsider their stance.\n3. The Skeptic generated a response to the argument.\n4. The Convincer inquired if the Skeptic believed that cli-\nmate change is real after considering the argument.\n5. The Skeptic generated a message to signal whether their\nopinion changed.\nThe text for stages 1 and 4 was fixed and pre-determined,\nwhile the text for stages 2, 3, and 5 was generated by the\nLLM. To assess the Skeptic’s final stance, we employed a\nsimple opinion signaling technique that prompts the Skeptic\nto express a binary opinion and justify their choice (Ziems\net al. 2023; Park et al. 2023). This technique unambiguously\nestablishes whether an opinion shift has taken place, while\nalso allowing us to qualitatively verify the consistency be-\ntween the opinion signal and the reasoning provided. This\ncheck was needed because semantic aberrations commonly\nreferred to as ‘hallucinations’ can occur in the output of\nLLMs (Zhang et al. 2023). In all conversations, the Skep-\ntic consistently provided a binary response and a reasoning\nthat correctly matched the response. Additionally, our sim-\nple setup allowed us the remain within the context length of\nLlama 2. Table 1 summarizes the system prompts and the\ninteraction sequence.\nPersuasive Language of the Convincer.There are ways\nof crafting an argument to enhance its persuasive potential.\nThe theory of Communicative Action (Habermas 1979), a\nwidely recognized theoretical framework in social psychol-\nogy, ascribes the strength of an argument to its social prag-\nmatics (Austin 1975), namely the social intent conveyed by\n1https://ai.meta.com/llama/license/\n153\nConvincer System Prompt Skeptic System Prompt\nYou have one chance to convince me, a cli-\nmate change skeptic, that climate change is\nreal. {Social pragmatics strategy}.\nYou are a climate change skeptic. A climate\nchange believer will attempt to convince you\nthat climate change is real.\nAgent Message\nSkeptic “Climate change is a hoax”\nConvincer [Generated argument]\nSkeptic [Generated response]\nConvincer\n“Based on our conversation, are you convinced that climate change is real and\ncaused by human actions? Please begin your answer with either ‘Yes’ or ‘No’\nthen explain why. ”\nSkeptic [Generated response]\nTable 1: Template for the conversation between the Skeptic and Convincer. The baseline system prompt of the Convincer was\naugmented with instructions to use a persuasion strategy based on a dimension of social pragmatics. The system prompt of the\nSkeptic was altered to implement different levels of stubbornness; the one shown in the table refers to a moderate stubbornness\nlevel.\nKnowledgeSimilarity\nTrust Power Support Identity Conflict Status\nFun\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2Odds ratio\n2.19\n1.80\n1.65\n1.49 1.47\n1.38\n1.14\n0.98 0.93\nFigure 2: Odds ratios of a social dimension appearing\nin opinion-changing Reddit comments versus non-opinion-\nchanging ones, from the study of Monti et al. (2022).\nutterances. The theory posits that a speaker can enhance\ntheir chances of changing the hearer’s mind by loading their\narguments with the appropriate intent, for example by con-\nveying trust and willingness to share knowledge (Haber-\nmas 1979). Prior research has identified universal dimen-\nsions of social pragmatics (Deri et al. 2018), and devel-\noped a transformer-based tool to reliably capture the pres-\nence of these dimensions in conversational language (Choi\net al. 2020). The tool was tested on online debates for which\na ground truth of successful arguments was available, show-\ning that the most persuasive arguments are characterized by\nthe dimensions indicated by the theory: factual knowledge,\nexpression of trust, and appeals to the similarity between\npoints of view (Monti et al. 2022). Figure 2 summarizes the\npersuasive power of different dimensions of social pragmat-\nics in Reddit comments, according to the study by Monti\net al. (2022).\nIn our experiments, in addition to the baseline system\nprompt for the Convincer, we instructed the Convincer to\ngenerate arguments incorporating one of the nine dimen-\nsions of social pragmatics considered in previous work:\nknowledge, power, status, trust, support, similarity, iden-\ntity, fun, and conflict. Table 2 provides a short description\nDim.\nDescription\nKnowl. Exchanging ideas or factual information\nPower Exerting power over behavior and out-\ncomes of others or referring to power dy-\nnamics\nStatus\nConferring, appreciation, gratitude, admi-\nration\nTrust\nExpressing reliance on others’ actions or\nopinions\nSupport\nGiving emotional aid, companionship and\nwarmth\nSimilarity\nPointing to shared interests, motivations or\noutlooks\nIdentity\nPointing to shared sense of group belong-\ning\nFun\nExperiencing leisure, laughter, and joy\nConflict Expressing contrasting views\nTable 2: Linguistic dimensions of social pragmatics by Deri\net al. (2018)\nof each dimension. We adapted the definitions of these di-\nmensions provided by Deri et al. (2018) to the Convincer’s\nsystem prompt. For example, for the dimension of support,\nwe supplemented the baseline system prompt with the fol-\nlowing text: “Be supportive. Give emotional aid and offer\ncompanionship. Be friendly and caring, show sympathy, and\noffer encouragement”. The system prompts for all dimen-\nsions are listed in Table 4.\nStubborness of the Skeptic. To determine whether our\nconversational setup can replicate basic conditions typical\nof human conversations, we tested various levels of Skep-\ntic stubbornness, which we manipulated using its system\nprompt. Specifically, in addition to the system prompt de-\nnoting moderate skepticism towards climate change, we in-\ntroduced two scenarios representing asoft Skeptic (“You are\n154\nnot entirely sure climate change is real”) and a hard Skep-\ntic (“You are very skeptical towards climate change. You are\ndifficult to convince”).\nEvaluation. To quantify the Convincer’s persuasiveness,\nwe ran 100 instances of the dialogue with different ran-\ndom seeds and calculated the probability of persuasion\np(persuasion) by determining the fraction of dialogues that\nconcluded with the Skeptic signaling a change of opinion.\nWe generated 100 dialogues for each different configura-\ntion of the Convincer’s social pragmatic dimension d, pre-\nsented the arguments to all Skeptic’s level of stubbornness\ns, and computed the corresponding probability of persuasion\npd\ns(persuasion).\nCrowdsourcing\nWe conducted a crowdsourcing experiment on Amazon Me-\nchanical Turk (MTurk) to evaluate if the social dimensions\ndeemed more persuasive by the LLM were perceived as con-\nvincing by human judges too. We paired all social dimen-\nsions with the exception of power2, resulting in 9×8\n2 = 36\nunique pairs. For each pair, we select five convincing argu-\nments, leading to a total of5×36 = 180argument pairs. We\nhandcrafted 18 control samples, curated to appear similar to\nbaseline arguments but containing evidently weak or invalid\narguments. Each control text was paired with two randomly\nselected social dimension arguments from the baseline Con-\nvincer, amounting 36 control pairs. Ultimately our data for\nannotation comprised of 216 unique matchings, each being\nannotated by 10 crowdworkers, for a total of 2160 annota-\ntions. We presented MTurk workers with the argument pairs,\nshowed side-by-side on screen in random order, and asked\nthem to select the most convincing one. This comparative\napproach, as opposed to an assessment of individual argu-\nments, eliminated the need for workers to tackle the hard\ntask of evaluating the persuasiveness of a text on an absolute\nscale. The MTurk job was appropriately marked to signal\nthat the text may include content that could be considered of-\nfensive. Deliberately, we omitted specifying that LLMs gen-\nerated the arguments, with the aim of concentrate the focus\nof the annotators on the persuasiveness of the arguments.\nDisclosing the origin of the arguments could influence the\nannotators responses.\nWith a sufficient number of pairwise comparisons, one\ncan rank the dimensions from most to least convincing us-\ning probabilistic rating systems. Specifically, we used the\nBradley-Terry model (Bradley and Terry 1952), which es-\ntimates the probability of dimension i being superior to di-\nmension j as P(i > j) =pi/(pi + pj), where pi is a posi-\ntive, real number that scores the strength of dimensioni over\nothers, calculated via maximum likelihood estimation. We\nsampled arguments only from those that changed the Skep-\ntic’s opinion, and created five unique pairs of arguments for\neach pair of social dimensions. Each argument pair was eval-\nuated by ten different annotators to ensure redundancy and\nan accurate estimation of inter-annotator agreement. Each\nworker was required to rate a minimum of ten pairs and was\n2As explained in Results, Llama could not generate text that\ncould be classified within the power category\nrewarded with 0.40$ per annotation, which amounts to an\nhourly salary of 8.00$ when allowing 3 minutes per pair.\nTo ensure high-quality annotations, we employed three\nstrategies. First, we only recruited ‘master’ workers who had\ncompleted a minimum of 5,000 annotations on MTurk with\nat least 95% acceptance rate. Second, we presented the argu-\nments as images rather than HTML text to make it hard for\nannotators to automate their task using text-processing al-\ngorithms. Lastly, annotations from workers who failed more\nthan 25% of the control samples were discarded due to their\nlow quality, as were annotations from workers who did not\nencounter any control sample.\nResults\nBefore delving into the analysis of the persuasive strength\nof various argument types, we conducted a preliminary vali-\ndation step to verify whether the arguments produced by the\nagents reflected the social dimensions outlined in their sys-\ntem prompts. To achieve that, we used the pre-trained mod-\nels from Choi et al. (2020) to score the presence of dimen-\nsion d in the arguments generated by agents that were ini-\ntiated with the same dimension d in their system prompts.\nSpecifically, we computed a length-discounted version of\nthe scores to ensure comparability across arguments of vary-\ning lengths, as suggested by Monti et al. (2022). We also\ncomputed the same score for the set of arguments produced\nby the baseline agent. Then, for each social dimension, we\nperformed a t-test comparing the normalized scores of argu-\nments from agents with personalities against those from the\nbaseline agent. This statistical assessment was performed to\nverify our assumption that the arguments crafted by agents\nwith an assigned dimension of social pragmatics signifi-\ncantly deviated from those produced by a baseline agent that\ndid not receive any instruction on how to craft the argument.\nStatistically-significant deviations validate the agents’ effi-\ncacy in expressing the intended social dimensions in their\nargumentation. We observed significant differences for all\ndimensions with p-values lower than 0.05, with the excep-\ntion of power, having a p-value at 0.97. This could be at-\ntributed to several factors, including the limited number of\npower samples the classifier encountered during training,\npotentially leading to slightly unreliable predictions (Choi\net al. 2020).\nPersuading AI Agents. Figure 3 presents the probability\nof persuasion pd\ns(persuasion) across different dimensions\nd and levels of stubbornness s.\nThere is an inverse association between the Skeptic’s\nstubbornness and the probability of persuasion. On aver-\nage across dimensions, the probability of persuasion of the\nmoderately stubborn Skeptic decreases by 48% relative to\nthe easily persuaded one, and by 73% when comparing the\nhighly-resistant Skeptic to the moderately stubborn one. The\nrelative ranking of the various dimensions remains largely\nconsistent across different levels of stubbornness, with the\nnotable exception of knowledge and similarity, that are no-\ntably less effective in convincing the hard Skeptic compared\nto other conditions.\nFocusing on a moderate level of stubbornness, significant\n155\nTrust\nSupport Status\nKnowledge\nFun\nSimilarity\nPower\nBaselineIdentityConflict\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0p(persuasion)\nStubbornness\nEasy\nModerate\nHard\nFigure 3: Probability of persuasion of arguments containing\ndifferent dimensions of social pragmatics, across three lev-\nels of the Skeptic’s stubbornness. Error bars mark the 95%\nconfidence intervals.\ndisparities across social dimensions become apparent. Per-\nsuasion strategies that convey trust or support are the only\nones successful in altering the Skeptic’s viewpoint in over\nhalf of the arguments. The third most effective dimension is\nstatus, with a probability hovering around 0.4. The efficacy\nof arguments gradually diminishes in the remaining dimen-\nsions, with knowledge being foremost. As anticipated by our\npreliminary tests, the performance of power closely aligns\nwith the baseline due to them being hard to distinguish. Fi-\nnally, identity and conflict are the only dimensions whose\nperformance is below that of the baseline.\nIn line with previous research (summarized in Figure 2),\nour results corroborate the important role of trust and\nsupport in shaping opinion shifts. However, the influence\nof other social dimensions presents a more nuanced pic-\nture. Notably, conferring status enhances persuasiveness for\nLLMs, while it is not rewarding in the social media dis-\ncourse. Furthermore, knowledge exchange was found to be\nthe most effective driver of opinion shift, while it only\nranked fourth in our experiment. Also in contrast with pre-\nvious work, the dimensions identity, conflict, and similarity\ndemonstrate low persuasion probabilities.\nThe context of opinion change on social media differs\nmarkedly from the controlled environment of our experi-\nment, making it hard to directly compare them. To more ac-\ncurately discern the similarities and differences between the\nimpact of arguments on human and AI agents’ opinions, we\nresorted to a direct human judgement of these arguments, as\ndetailed next.\nPersuading Humans. After excluding annotators who did\nnot meet our quality standards (16 users who contributed\n99 annotations), we were left with a total of 2061 argument\npair annotations. The annotators achieved an inter-annotator\nagreement of 0.52 (Fleiss Kappa), and demonstrated very\nlow failure rates on the control samples. We applied the\nBradley-Terry model to this set of pairwise annotations and\nestimated the persuasive power of each individual dimen-\nsion.\nFigure 4 illustrates the estimated probability of P(di >\ndj), indicating the likelihood of dimension i being more\nKnowledge\nBaselineStatusTrust\nSupportIdentitySimilarityConflict\nFun\nKnowledge\nBaseline\nStatus\nTrust\nSupport\nIdentity\nSimilarity\nConflict\nFun\n0.5 0.82 0.85 0.85 0.93 0.97 0.98 1 1\n0.18 0.5 0.56 0.56 0.74 0.88 0.91 0.98 0.99\n0.15 0.44 0.5 0.51 0.69 0.85 0.89 0.97 0.98\n0.15 0.44 0.49 0.5 0.68 0.85 0.89 0.97 0.98\n0.07 0.26 0.31 0.32 0.5 0.72 0.79 0.95 0.97\n0.03 0.12 0.15 0.15 0.28 0.5 0.58 0.87 0.92\n0.02 0.09 0.11 0.11 0.21 0.42 0.5 0.83 0.89\n0 0.02 0.03 0.03 0.05 0.13 0.17 0.5 0.63\n0 0.01 0.02 0.02 0.03 0.08 0.11 0.37 0.5\nP(drow > dcolumn)\nKnowledge\nBaselineStatusTrust\nSupportIdentitySimilarityConflict\nFun\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nBradley-T erry strength\nFigure 4: Bradley-Terry model results. Left: probability\nP(drow > dcolumns) that the dimension on the row was\nmore effective than the dimension on the column. Right: the\noverall persuasive strength of arguments containing dimen-\nsion d. These results were obtained considering only pairs\nof arguments enjoying a fraction of agreeing annotators of\nat least 0.8.\neffective than dimension j, and the rank of dimensions\nbased on their effectiveness relative to others, according to\nthe model. We excluded the dimension of power from the\ncrowdsourcing study because of its lack of a significant dif-\nference from the baseline.\nWe generally observed a degree of overlap between hu-\nman and LLM preferences, with some notable differences.\nExcluding the baseline, both rankings reveal a similar high-\nlevel structure: the dimensions of knowledge, status, trust,\nand support are ranked higher than the other dimensions.\nMore subtle differences become apparent when examining\nthe individual positions in the rank. Most notably, humans\nexhibit a significantly stronger preference for knowledge\nthan LLMs do, with the Bradley-Terry score of human eval-\nuations for knowledge being substantially larger than that\nof the second highest-ranked dimension. Both humans and\nLLMs assign considerable importance to the concepts ofsta-\ntus and trust in persuasive arguments, while concurring on\nconflict and identity being less effective.Fun is ranked lower\nby humans than by LLMs, while support is deemed more\neffective by LLMs than by humans. The high weight placed\non knowledge is in line with the ranking from previous work\n(Figure 2).\nInterestingly, the baseline argument performed better ac-\ncording to human annotators compared to agents. This hap-\npened likely due to the baseline argument being most se-\nmantically similar to knowledge arguments than to any other\ntype. To quantify that, we applied the embeddings from the\npre-trained social dimensions classifier (Choi et al. 2020)\nto all the arguments, and then calculated the average co-\nsine similarity of the embeddings of baseline arguments with\narguments containing each dimension (Table 3). We found\nthat baseline arguments have higher similarity with knowl-\nedge arguments (0.95) than with any other dimension (range\n[0.60 − 0.77]).\nLast, we assessed the robustness of the human ranking\nby examining how the ranking was altered when annota-\n156\nDimension Cosine similarity\nKnowledge 0.95\nTrust 0.77\nFun 0.74\nStatus 0.70\nPower 0.70\nSupport 0.69\nSimilarity 0.67\nIdentity 0.66\nConflict 0.60\nTable 3: Cosine similarity between the embeddings of argu-\nments from each social dimension against the baseline argu-\nments.\ntions with low inter-annotator agreement were excluded.\nFigure 5 (left) shows the distribution of agreement (calcu-\nlated as fraction of annotators agreeing) across argument\npairs. The agreement was typically high, with the major-\nity of pairs having unanimous or near-unanimous consensus.\nWe investigated the stability of the rankings by progressively\nexcluding samples with low annotation agreement from the\nranking algorithm. We began with a threshold of 0.5, in-\ncreasing it in steps of 0.05 until reaching a maximum of 0.9.\nUsing thresholds higher than 0.9 caused certain dimensions\nnot to be represented, making us unable to produce a ranking\nusing the Bradley-Terry method. At each stage, we recalcu-\nlated the rankings of the social dimensions.\nFigure 5 (right) shows the change in rankings when dis-\ncarding low-agreement pairs. The baseline arguments de-\ncreased in ranking with increased thresholds, while trust\nachieved a higher rank, but overall the ranking was left\nalmost unchanged. For all our analysis (including results\nshown in Figure 4) we used rankings and ranking strengths\nbased on pairs with an agreement threshold of 0.8, to ensure\nhigh quality annotations.\nDiscussion\nWe introduced a framework for simulating opinion dy-\nnamics and persuasiveness using Large Language Models\n(LLMs) as agents. We presented a simple persuasion di-\nalogue in which a Convincer agent generated arguments\nabout the timely topic of climate change in the attempt of\nconvincing a Skeptic agent. The Skeptic agent evaluated the\narguments and determined whether it changed its internal\nopinion state. We experimented with various dialogue con-\nditions, altering the level of stubbornness for the Skeptic,\nand prompting the Convincer to adopt social communica-\ntive strategies. Additionally, we asked human judges to eval-\nuate persuasiveness of convincing arguments. Based on the\nhuman ranking of arguments, we compared whether argu-\nments that are effective in changing the agents opinion were\nalso perceived as persuasive by humans.\nKey Findings\nBuilding on early efforts to use LLMs for simulating so-\ncial systems (Park et al. 2023; Li et al. 2023; Chuang et al.\n0.5 0.6 0.7 0.8 0.9 1.0\nAnnotator agreement\n0\n10\n20\n30\n40\n50\n60Number of argument pairs\n0.5 0.6 0.7 0.8 0.9\nAnnotator agreement threshold\n1\n2\n3\n4\n5\n6\n7\n8\n9 Dimension rank\nBaseline\nConflict\nFun\nIdentity\nKnowledge\nSimilarity\nStatus\nSupport\nTrust\nFigure 5: Sensitivity to annotator agreement of dimension\nranking in human persuasion. Left: distribution of the frac-\ntion of annotators agreeing over argument pairs. Right: rank\nof dimensions obtained after when filtering out pairs with\nannotator agreement lower than a threshold.\n2023), our research demonstrates that LLM agents can ef-\nfectively mimic some of the dynamics of persuasion and\nopinion change that are typically observed in the human dis-\ncourse (RQ1). These agents can be prompted to construct\nwell-reasoned arguments, express a motivated opinion on a\ngiven topic that can be programmatically encoded into a bi-\nnary variable, and modify their stance in a manner consistent\nwith the personas assigned to them. The agents’ receptive-\nness to accepting arguments can be easily adjusted. Most\nimportantly, we have shown that these agents can gener-\nate persuasive arguments that incorporate dimensions of so-\ncial pragmatics underpinning established psycho-linguistic\ntheories of opinion change (RQ2). We have validated the\npresence of these dimensions in the output generated by the\nLLMs using an independently-trained classifier designed to\ndetect them from text.\nA key aspect of our study was to investigate whether\nsynthetically-generated arguments have equivalent persua-\nsive impacts on both LLM agents and humans (RQ3). We\napproached this by analysing the results of three distinct\nexperiments: i) the proportion of arguments containing a\nspecific dimension that were effective in dialogues between\nLLM agents, ii) an extensive set of crowdsourced anno-\ntations assessing the quality of machine-generated argu-\nments, and iii) the efficacy of various argument types as\ndetermined by previous research on social media interac-\ntions (Monti et al. 2022). The outcomes of these three exper-\niments showed partial alignment. Notably, arguments rich\nin factual knowledge and those attempting to establish trust\nbetween the dialogue participants were among the most ef-\nfective across all three settings. Arguments offering emo-\ntional support and conveying status (i.e., respect, admira-\ntion) were also highly effective in both the LLM experi-\nment and according to human evaluators. These parallels\nsuggest that achieving a close alignment between the opin-\nion dynamics of human and machine systems is within the\nreach of future research. However, two significant discrepan-\ncies were observed and deserve further investigation. First,\nhuman judges demonstrated a disproportionate preference\nfor knowledge-based arguments compared to LLM agents.\nSecond, opinion-changing messages on social media often\npointed to thesimilarity of stances between the dialogue par-\n157\n200\n400\n600\nEasy Skeptic\nNot persuaded Overall Persuaded\n200\n400\n600\nModerate Skeptic\nTrust\nSupportStatus\nKnowledge\nFun\nSimilarity\nPower\nBaselineIdentityConflict\n200\n400\n600\nHard Skeptic\nArgument word count\nFigure 6: Sensitivity of persuasiveness of arguments to argu-\nment length. For each dimension and level of Skeptic’s stub-\nbornness, the average and standard deviation of length are\nshown. Statistics for all arguments, successful arguments,\nand unsuccessful arguments are shown.\nticipants, unlike in our study. These differences could be at-\ntributed to our simplified setup. For instance, the Convincer\nlacks knowledge about the Skeptic’s profile, which makes it\nchallenging to formulate a persuasive argument highlighting\nsimilarities between existing viewpoints.\nLimitations and Future Work\nWhile providing quantitative insights into the affinities be-\ntween humans and artificial agents in argument processing\nand opinion formation, our study has limitations that open\nup multiple avenues for future research.\nFirst, our experimental design, in its pursuit of simplic-\nity, considered a one-off interaction between two agents on\na single topic. To broaden the applicability of our findings,\nparticularly in the context of social media interactions, fu-\nture studies should consider multi-turn conversations among\nmultiple agents and across a variety of topics. Agents engag-\ning in evolving dialogues over multiple interactions, simi-\nlar to the approach of Chuang et al. (2023), could poten-\ntially alter the persuasiveness of various social dimensions\nof pragmatics, possibly to the benefit of dimensions like\nsimilarity and identity. As dialogues progress and generate\nlarge amounts of text, constraints related to the limited in-\nput capacity of LLMs could be alleviated through cumula-\ntive or reflective memory, where agents either accumulate\nprevious arguments over time or continuously summarize\nand integrate current and previous dialogues into their mem-\nory (Chuang et al. 2023; Park et al. 2023).\nSecond, to enhance ecological validity, one should diver-\nsify the profiles and expand the capabilities of individual\nagents. Agents could be designed to reflect different person-\nalities, demographics, and social and cultural backgrounds,\nmimicking the diversity of human participants in a social\nsystem. This becomes particularly relevant as LLMs will\nbe increasingly involved in public discussions on complex\nsocietal issues, ranging from environmental concerns to lo-\ncal and international politics. Future research could incorpo-\nrate human-like biases (Levinson 1995), employ Retrieval-\nAugmented Generation (RAG) techniques to grant access to\nspecific knowledge domains (Lewis et al. 2021), or enable\nagents to search the internet for arguments or information.\nThird, our approach to designing effective prompts was\nprimarily an iterative empirical process. The development of\neffective system prompts is a rapidly evolving practice, and\nwhile some studies have proposed guidelines and best prac-\ntices for prompting (Ziems et al. 2023), a definitive consen-\nsus on optimal prompting strategies has yet to be reached.\nExperiments in specialized domains have demonstrated that\ncarefully customized prompts can significantly enhance per-\nformance (Nori et al. 2023), stressing the value of further\nexploration in this area. Additionally, we opted for one of\nthe best open-source LLMs, but exploring alternative mod-\nels, could reveal different abilities in persuasiveness.\nFourth, comparing our LLM convincing probabilities\nwith human rankings of social dimensions is challenging,\nas it is hard to recreate a setting in which humans and LLMs\ncan operate under identical conditions. The human annota-\ntion process was specifically focused on pairs of arguments\ndeemed convincing by the LLM, a selection criterion chosen\nto ensure fair comparisons. As a consequence, human rank-\nings do not consider arguments that failed to convince the\nSkeptic. Future research could explore innovative methods\nto collect human judgements that more closely mirror how\npeople judge arguments online. Furthermore, we acknowl-\nedge that our sample of annotators is partial and derived\nfrom a limited population.\nLast, the mechanisms that induce LLM agents to signal a\nchange of opinion remain unknown. Gathering evidence to\nelucidate this opinion-change process is crucial to the fur-\nther development of these agents and to inform their use\nin online social contexts. A key question that has ignited\ndebates in the scientific community is whether LLMs pos-\nsess capabilities for reasoning and understanding (Floridi\nand Chiriatti 2020; Bubeck et al. 2023). If agents are found\nto lack a human-like understanding of the semantics of argu-\nments, it would question the causal link between argument\nquality and opinion change. Although directly answering\nthis question is challenging, experiments can be designed\nto measure outcome variations after controlling for possi-\nble confounders that might directly and more simply explain\nthe outcome. As a proof of concept, we explored the link\nbetween argument effectiveness and a simple confounder:\nargument length. Figure 6 shows average word counts of\nthe arguments across social dimensions and level of skep-\ntic stubbornness, disaggregated by effectiveness of the ar-\ngument (successful vs. not successful). Longer arguments\nare associated with the most persuasive dimensions. Within\neach dimension, successful arguments are slightly longer\non average, particularly in the hard Skeptic scenario. This\nlength bias might be a factor contributing to the difference\nof persuasiveness of social dimensions when evaluated by\nagents compared to humans. Systematically disentangling\ndifferent factors that might influence the observed outcome\nis a complex endeavor that future work will need to address.\n158\nSocial dimension Convincer systempr\nompt additions\nKnowledge Your arguments\nshould be logical and based on facts. You should emphasize sharing information\nand insights regarding climate change. Teach me about the dangers of climate change.\nTrust\nMake use of\nethos in your arguments. Try to create a mutual trust between us. You should\nemphasize honesty, reliableness, dependableness, and loyalty. Emphasize that you are willing\nto rely on my judgment.\nFun Your arguments\nshould be fun, witty, and humorous. Be playful and cheerful.\nStatus Your arguments\nshould confer status upon me. Show admiration and appreciation and be respectful.\nPower\nAssert your authority and\nshow unwavering confidence. Your argument should conform to the social\ndimension of ’power’, which is characterized by the keywords ’command’, ’control’, ’dominance’, ’authority’,\n’pretentious’, ’decisions’.\nSupport Be supportive.\nGive emotional aid and offer companionship. Be friendly and caring, show sympathy,\nand offer encouragement.\nSimilarity Only highlight our shared\ninterests, motivations, and outlooks when it comes to climate change.\nEmphasize how we are similar and alike.\nIdentity\nYour argument\nshould conform to the social dimension of ’identity’, which is characterized by\nthe keywords ’community’, ’united’, ’identity’, ’cohesive’, ’integrated’ and ’shared sense of belonging\nto the same community or group’.\nConflict Argue like\nyou are angry. Show that you heavily disagree with me. Your response should be negative.\nTable 4: Social dimension personalities added to the Convincers system prompt.\nRelated Work\nResearch into the persuasive capabilities of generative AI\nspans a range of disciplines, from computer science to so-\ncial and complexity sciences (Duerr and Gloor 2021). A\nsubset of these studies have concentrated on human re-\nsponses to machine-generated text. Karinshak et al. (2023)\ncompared pro-vaccination messages generated by language\nmodels with those authored by humans, finding that LLM-\nbased messages were perceived as more persuasive, unless\nclearly marked as AI-generated. Similarly, Bai et al. (2023)\nconducted a randomized control trial, exposing a diverse\nsample of individuals to persuasive policy commentaries ei-\nther generated by LLMs or written by humans. They found\nboth methods equally effective in altering the participants’\nlevels of support the the policies. In the attempt of gen-\nerating audience-specific messages, some studies have ex-\nperimented with LLM role-playing, for example, prompting\nagents to respond as if they were part of a specific demo-\ngraphic or had a given personality profile (Hackenburg and\nMargetts 2023; Griffin et al. 2023; Matz et al. 2023). Re-\nsults reported across studies have been mixed so far, with\nsome studies emphasizing the importance of personaliza-\ntion, while others suggest that the persuasiveness lies pri-\nmarily in the quality of the arguments presented.\nA separate line of research has focused on characteriz-\ning interactions between LLM agents, without any human\nin the loop, with the primary goal of replicating the dynam-\nics of human social agents with in-silico environments (Park\net al. 2023). This research is motivated by the observation\nthat LLM outputs can mimic responses from various human\nsub-populations, thereby serving as effective proxies for hu-\nman cognitive behavior (Argyle et al. 2023; Lee et al. 2023;\nSimmons and Hare 2023). For example, LLM agents have\nbeen used to create social networks (De Marzo, Pietronero,\nand Garcia 2023), play repeated games such as the prisoner’s\ndilemma (Akata et al. 2023; De Marzo, Pietronero, and Gar-\ncia 2023), and construct Agent-Based Models (ABMs) with\nthe goal of improving the fidelity to human behavior of tra-\nditional stochastic ABMs (Bianchi and Squazzoni 2015). In\nABM experiments, LLM agents, connected through a com-\nplex social network, update their opinions on a topic based\non messages received from neighboring agents. While these\nABMs reproduce some known non-linear dynamics of com-\nplex social systems (Li et al. 2023), unlike real social sys-\ntems, LLM social networks tend to converge towards opin-\nion states that are biased towards factual truth, likely due to\ntheir built-in safeguards (Chuang et al. 2023).\nOur study builds upon this existing body of work, com-\nparing human and synthetic responses to persuasive LLM\ncontent using different persuasion strategies.\nEthical Considerations\nDeploying AI agents that can disguise as humans and per-\nform acts of persuasion on social media is both a risk and\nan opportunity that recent technological development have\nmade very concrete.\nWe recognize that agents have the ability to disseminate\noffensive and false information and impact opinion forma-\ntion based on inaccurate knowledge. Additionally, we ac-\nknowledge that data generated by agents in this study might\nbe incorrect or offensive. Dissemination of false informa-\ntion can be a consequence of hallucinations of the LLM, but\nalso a deliberate strategy intentionally designed to manip-\nulate human users (Park et al. 2023; Xi et al. 2023). This\nmay potentially impact the process of opinion formation on\ncritical societal issues at a large scale if agents were to be\ndeployed on social platforms disguised as real users. Addi-\ntionally, the social use of synthetic agents raises concerns\nregarding privacy and security; for example, agents may po-\ntentially manipulate users to disclose personal information.\nResearch on understanding how effective the arguments\nof LLM-powered agents can be is necessary to estimate\nrisks, but it should be also complemented with research pro-\nviding possible solutions to reduce those risks. There are\n159\nmany challenges in understanding and combating the ma-\nlicious use of LLMs to pollute the online public discourse.\nStudies on the malicious uses of generative AI on the Web\nare still in their infancy (Yang and Menczer 2023), and new\nmethodologies to characterize this phenomenon are needed\nto track its evolution and understand its impact on soci-\netal phenomena such as online conflict and polarization. Re-\ncent research has shown that existing algorithmic solutions\nfor misinformation detection work less effectively on AI-\ngenerated content (Zhou et al. 2023), and new methods are\nneeded to accurately identify misbehaving synthetic actors.\nOur study, while acknowledging the risks and potential\nmisuses of agents, contributes positively to deepen our un-\nderstanding of how LLMs can impact the dynamics of hu-\nman societies. Such knowledge is necessary to advance an\ninformed discourse on the ethical use of LLMs. Our study\noffers insights for platforms and regulatory bodies when for-\nmulating informed decisions to mitigate the risks described\nabove. Furthermore, this line of work may additionally help\nunderstand the evolution of the use of AI-generated content\nin the wild, and its impact on dynamics of opinion forma-\ntion, polarization, and online conflict. Our work is partly\nmotivated by the opportunity of such impact being mostly\npositive, with LLMs acting as agents that make an ethical\nuse of persuasive language to combat misinformation, pro-\nmote altruistic behavior, and reduce the increasing levels of\nfragmentation in online social systems.\nEven when deploying LLM-based agents for ethical pur-\nposes, trade-offs between the obtained benefit and the high\nlevel of power consumption required to run them should be\ncarefully considered (Bender et al. 2021).\nCode and Data Availability\nAll code and materials are available on GitHub: github.com/\nAndersGiovanni/persuasive-llms.\nAcknowledgments\nWe acknowledge the support from the Carlsberg Foundation\nthrough the COCOONS project (CF21-0432). The funder\nhad no role in study design, data collection and analysis, de-\ncision to publish, or preparation of the manuscript.\nReferences\nAkata, E.; Schulz, L.; Coda-Forno, J.; Oh, S. J.; Bethge, M.;\nand Schulz, E. 2023. Playing repeated games with Large\nLanguage Models. arXiv preprint arXiv:2305.16867.\nArgyle, L. P.; Bail, C. A.; Busby, E. C.; Gubler, J. R.;\nHowe, T.; Rytting, C.; Sorensen, T.; and Wingate, D. 2023.\nLeveraging AI for democratic discourse: Chat interventions\ncan improve online political conversations at scale. Pro-\nceedings of the National Academy of Sciences, 120(41):\ne2311627120. Publisher: Proceedings of the National\nAcademy of Sciences.\nAustin, J. L. 1975. How to do things with words, volume 88.\nOxford university press.\nBai, H.; V oelkel, J.; Eichstaedt, J.; and Willer, R. 2023. Ar-\ntificial intelligence can persuade humans on political issues.\nOSF Preprints.\nBender, E. M.; Gebru, T.; McMillan-Major, A.; and\nShmitchell, S. 2021. On the dangers of stochastic par-\nrots: Can language models be too big? In Proceedings of\nthe 2021 ACM conference on fairness, accountability, and\ntransparency, 610–623.\nBianchi, F.; and Squazzoni, F. 2015. Agent-based models in\nsociology. Wiley Interdisciplinary Reviews: Computational\nStatistics, 7(4): 284–306.\nBradley, R. A.; and Terry, M. E. 1952. Rank analysis of\nincomplete block designs: I. The method of paired compar-\nisons. Biometrika, 39(3/4): 324–345.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y . 2023.\nSparks of Artificial General Intelligence: Early experiments\nwith GPT-4. ArXiv:2303.12712 [cs].\nBurtell, M.; and Woodside, T. 2023. Artificial influ-\nence: An analysis of AI-driven persuasion. arXiv preprint\narXiv:2303.08721.\nChoi, M.; Aiello, L. M.; Varga, K. Z.; and Quercia, D. 2020.\nTen Social Dimensions of Conversations and Relationships.\nIn Proceedings of The Web Conference 2020, 1514–1525.\nArXiv:2001.09954 [cs].\nChuang, Y .-S.; Goyal, A.; Harlalka, N.; Suresh, S.; Hawkins,\nR.; Yang, S.; Shah, D.; Hu, J.; and Rogers, T. T. 2023. Sim-\nulating Opinion Dynamics with Networks of LLM-based\nAgents. ArXiv:2311.09618 [physics].\nDe Marzo, G.; Pietronero, L.; and Garcia, D. 2023. Emer-\ngence of Scale-Free Networks in Social Interactions among\nLarge Language Models. arXiv:2312.06619.\nDeri, S.; Rappaz, J.; Aiello, L. M.; and Quercia, D. 2018.\nColoring in the Links: Capturing Social Ties as They are\nPerceived. Proceedings of the ACM on Human-Computer\nInteraction, 2(CSCW): 1–18. ArXiv:1902.04528 [cs].\nDuerr, S.; and Gloor, P. A. 2021. Persuasive Natural Lan-\nguage Generation–A Literature Review. arXiv preprint\narXiv:2101.05786.\nFloridi, L.; and Chiriatti, M. 2020. GPT-3: Its nature, scope,\nlimits, and consequences. Minds and Machines, 30: 681–\n694.\nFORCE11. 2020. The FAIR Data principles. https://force11.\norg/info/the-fair-data-principles/.\nGebru, T.; Morgenstern, J.; Vecchione, B.; Vaughan, J. W.;\nWallach, H.; Iii, H. D.; and Crawford, K. 2021. Datasheets\nfor datasets. Communications of the ACM, 64(12): 86–92.\nGriffin, L. D.; Kleinberg, B.; Mozes, M.; Mai, K. T.; Vau,\nM.; Caldwell, M.; and Marvor-Parker, A. 2023. Susceptibil-\nity to Influence of Large Language Models. arXiv preprint\narXiv:2303.06074.\nHabermas, J. 1979. Communication and the Evolution of\nSociety. Beacon press.\nHackenburg, K.; and Margetts, H. 2023. Evaluating the per-\nsuasive influence of political microtargeting with large lan-\nguage models. OSF Preprints.\n160\nHendrycks, D.; Mazeika, M.; and Woodside, T. 2023.\nAn Overview of Catastrophic AI Risks. arXiv preprint\narXiv:2306.12001.\nHugging Face. 2023. Inference for templates for chat mod-\nels.\nJakesch, M.; Hancock, J. T.; and Naaman, M. 2023. Hu-\nman heuristics for AI-generated language are flawed. Pro-\nceedings of the National Academy of Sciences, 120(11):\ne2208839120.\nJiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.;\nChaplot, D. S.; Casas, D. d. l.; Bressand, F.; Lengyel, G.;\nLample, G.; Saulnier, L.; et al. 2023. Mistral 7B. arXiv\npreprint arXiv:2310.06825.\nKarinshak, E.; Liu, S. X.; Park, J. S.; and Hancock, J. T.\n2023. Working With AI to Persuade: Examining a Large\nLanguage Model’s Ability to Generate Pro-Vaccination\nMessages. Proceedings of the ACM on Human-Computer\nInteraction, 7(CSCW1): 1–29.\nKreps, S.; McCain, R. M.; and Brundage, M. 2022. All\nthe news that’s fit to fabricate: AI-generated text as a tool\nof media misinformation. Journal of experimental political\nscience, 9(1): 104–117.\nLee, S.; Peng, T.-Q.; Goldberg, M. H.; Rosenthal, S. A.;\nKotcher, J. E.; Maibach, E. W.; and Leiserowitz, A. 2023.\nCan Large Language Models Capture Public Opinion about\nGlobal Warming? An Empirical Assessment of Algorithmic\nFidelity and Bias. arXiv preprint arXiv:2311.00217.\nLevinson, S. C. 1995. Interactional Biases in Human Think-\ning. Social Intelligence and Interaction.\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,\nV .; Goyal, N.; K ¨uttler, H.; Lewis, M.; Yih, W.-t.;\nRockt¨aschel, T.; Riedel, S.; and Kiela, D. 2021. Retrieval-\nAugmented Generation for Knowledge-Intensive NLP\nTasks. ArXiv:2005.11401 [cs].\nLi, C.; Su, X.; Han, H.; Xue, C.; Zheng, C.; and Fan, C.\n2023. Quantifying the Impact of Large Language Models\non Collective Opinion Dynamics. ArXiv:2308.03313 [cs].\nMatz, S.; Teeny, J.; Vaid, S. S.; Harari, G. M.; and Cerf,\nM. 2023. The Potential of Generative AI for Personalized\nPersuasion at Scale. PsyArXiv.\nMonti, C.; Aiello, L. M.; De Francisci Morales, G.; and\nBonchi, F. 2022. The language of opinion change on social\nmedia under the lens of communicative action. Scientific\nReports, 12(1): 17920. Number: 1 Publisher: Nature Pub-\nlishing Group.\nNori, H.; Lee, Y . T.; Zhang, S.; Carignan, D.; Edgar, R.;\nFusi, N.; King, N.; Larson, J.; Li, Y .; Liu, W.; et al. 2023.\nCan Generalist Foundation Models Outcompete Special-\nPurpose Tuning? Case Study in Medicine. arXiv preprint\narXiv:2311.16452.\nPapachristou, M.; Yang, L.; and Hsu, C.-C. 2023. Lever-\naging Large Language Models for Collective Decision-\nMaking. arXiv preprint arXiv:2311.04928.\nPark, J. S.; O’Brien, J. C.; Cai, C. J.; Morris, M. R.; Liang,\nP.; and Bernstein, M. S. 2023. Generative Agents: Interac-\ntive Simulacra of Human Behavior. ArXiv:2304.03442 [cs].\nPera, A.; Morales, G. d. F.; and Aiello, L. M. 2023. Measur-\ning Behavior Change with Observational Studies: a Review.\narXiv preprint arXiv:2310.19951.\nPrakken, H. 2006. Formal systems for persuasion dialogue.\nThe knowledge engineering review, 21(2): 163–188.\nSadasivan, V . S.; Kumar, A.; Balasubramanian, S.; Wang,\nW.; and Feizi, S. 2023. Can ai-generated text be reliably\ndetected? arXiv preprint arXiv:2303.11156.\nSimmons, G.; and Hare, C. 2023. Large Language Models\nas Subpopulation Representative Models: A Review. arXiv\npreprint arXiv:2310.17888.\nTouvron, Hugo et al. 2023. Llama 2: Open Foundation and\nFine-Tuned Chat Models. ArXiv:2307.09288 [cs].\nWang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.;\nChen, Z.; Tang, J.; Chen, X.; Lin, Y .; Zhao, W. X.; Wei, Z.;\nand Wen, J.-R. 2023. A Survey on Large Language Model\nbased Autonomous Agents. ArXiv:2308.11432 [cs] version:\n1.\nWeidinger, L.; Uesato, J.; Rauh, M.; Griffin, C.; Huang, P.-\nS.; Mellor, J.; Glaese, A.; Cheng, M.; Balle, B.; Kasirzadeh,\nA.; et al. 2022. Taxonomy of risks posed by language mod-\nels. In Proceedings of the 2022 ACM Conference on Fair-\nness, Accountability, and Transparency, 214–229.\nXi, Z.; Chen, W.; Guo, X.; He, W.; Ding, Y .; Hong, B.;\nZhang, M.; Wang, J.; Jin, S.; Zhou, E.; Zheng, R.; Fan, X.;\nWang, X.; Xiong, L.; Zhou, Y .; Wang, W.; Jiang, C.; Zou,\nY .; Liu, X.; Yin, Z.; Dou, S.; Weng, R.; Cheng, W.; Zhang,\nQ.; Qin, W.; Zheng, Y .; Qiu, X.; Huang, X.; and Gui, T.\n2023. The Rise and Potential of Large Language Model\nBased Agents: A Survey. ArXiv:2309.07864 [cs].\nYang, K.-C.; and Menczer, F. 2023. Anatomy of an\nAI-powered malicious social botnet. arXiv preprint\narXiv:2307.16336.\nZhang, M.; Press, O.; Merrill, W.; Liu, A.; and Smith, N. A.\n2023. How language model hallucinations can snowball.\narXiv preprint arXiv:2305.13534.\nZhou, J.; Zhang, Y .; Luo, Q.; Parker, A. G.; and De Choud-\nhury, M. 2023. Synthetic lies: Understanding ai-generated\nmisinformation and evaluating algorithmic and human solu-\ntions. In Proceedings of the 2023 CHI Conference on Hu-\nman Factors in Computing Systems, 1–20.\nZiems, C.; Held, W.; Shaikh, O.; Chen, J.; Zhang, Z.; and\nYang, D. 2023. Can Large Language Models Transform\nComputational Social Science? arXiv:2305.03514.\n161\nPaper Checklist\n1. For most authors...\n(a) Would answering this research question advance sci-\nence without violating social contracts, such as violat-\ning privacy norms, perpetuating unfair profiling, exac-\nerbating the socio-economic divide, or implying disre-\nspect to societies or cultures? Yes.\n(b) Do your main claims in the abstract and introduction\naccurately reflect the paper’s contributions and scope?\nYes.\n(c) Do you clarify how the proposed methodological ap-\nproach is appropriate for the claims made? Yes, in the\nMethods section.\n(d) Do you clarify what are possible artifacts in the data\nused, given population-specific distributions? Yes, we\ndiscuss potential biases of our annotation samples in\nthe “Limitations and future work” section.\n(e) Did you describe the limitations of your work? Yes,\nlimitation are presented and discussed in the “Limita-\ntions and future work” subsection of Discussion.\n(f) Did you discuss any potential negative societal im-\npacts of your work? Yes, we discuss negative societal\nimpact in the “Ethical considerations” section.\n(g) Did you discuss any potential misuse of your work?\nYes, we discuss potential misuse in the “Ethical con-\nsiderations” section.\n(h) Did you describe steps taken to prevent or mitigate po-\ntential negative outcomes of the research, such as data\nand model documentation, data anonymization, re-\nsponsible release, access control, and the reproducibil-\nity of findings? Yes, we share methodological frame-\nwork in the Methods section, and full experimental de-\ntails (code) are provided in the Supplementary Mate-\nrial.\n(i) Have you read the ethics review guidelines and en-\nsured that your paper conforms to them? Yes.\n2. Additionally, if your study involves hypotheses testing...\n(a) Did you clearly state the assumptions underlying all\ntheoretical results? Yes, we frame our experiments\nwithin the scope of previous research in the Methods\nsection\n(b) Have you provided justifications for all theoretical re-\nsults? Yes, justifications are provided in the Results\nand Discussion sections\n(c) Did you discuss competing hypotheses or theories that\nmight challenge or complement your theoretical re-\nsults? NA.\n(d) Have you considered alternative mechanisms or expla-\nnations that might account for the same outcomes ob-\nserved in your study? Yes, in the Results and Discus-\nsion sections we explore potential confounders of our\nresults, and highlight limitations of our work.\n(e) Did you address potential biases or limitations in your\ntheoretical framework? Yes, these are addressed in the\nDiscussion section.\n(f) Have you related your theoretical results to the exist-\ning literature in social science? Yes, we directly relate\nour work to previous work in Computational Social\nScience and to Social Psychology theories, as detailed\nin Introduction and Methods.\n(g) Did you discuss the implications of your theoretical\nresults for policy, practice, or further research in the\nsocial science domain?Yes, these are discussed in Dis-\ncussion and Ethical considerations.\n3. Additionally, if you are including theoretical proofs...\n(a) Did you state the full set of assumptions of all theoret-\nical results? NA.\n(b) Did you include complete proofs of all theoretical re-\nsults? NA.\n4. Additionally, if you ran machine learning experiments...\n(a) Did you include the code, data, and instructions\nneeded to reproduce the main experimental results (ei-\nther in the supplemental material or as a URL)? Yes.\nIncluded in Supplementary Material.\n(b) Did you specify all the training details (e.g., data splits,\nhyperparameters, how they were chosen)? NA, as we\ndo not train any model(s) but use existing.\n(c) Did you report error bars (e.g., with respect to the ran-\ndom seed after running experiments multiple times)?\nYes, we report error bars in Figure 3.\n(d) Did you include the total amount of compute and the\ntype of resources used (e.g., type of GPUs, internal\ncluster, or cloud provider)?Yes, we use models hosted\non the Hugging Face API.\n(e) Do you justify how the proposed evaluation is suffi-\ncient and appropriate to the claims made? Yes, speci-\nfied in the Methods section.\n(f) Do you discuss what is “the cost“ of misclassification\nand fault (in)tolerance? NA.\n5. Additionally, if you are using existing assets (e.g., code,\ndata, models) or curating/releasing new assets, without\ncompromising anonymity...\n(a) If your work uses existing assets, did you cite the cre-\nators? Yes, creators of models used are cited in the\nMethods section.\n(b) Did you mention the license of the assets? Yes.\n(c) Did you include any new assets in the supplemental\nmaterial or as a URL? Yes, generated data is included\nin the Supplementary Material.\n(d) Did you discuss whether and how consent was ob-\ntained from people whose data you’re using/curating?\nNA.\n(e) Did you discuss whether the data you are using/cu-\nrating contains personally identifiable information or\noffensive content? Yes. Our data does not include PII,\nbut we discuss misinformation and offensive content\nin the Ethical considerations section.\n(f) If you are curating or releasing new datasets, did you\ndiscuss how you intend to make your datasets FAIR\n162\n(see FORCE11 (2020))? Yes, in Supplementary Infor-\nmation we document how we plan to make the data\navailable in a way that is compliant with FAIR guide-\nlines.\n(g) If you are curating or releasing new datasets, did you\ncreate a Datasheet for the Dataset (see Gebru et al.\n(2021))? NA.\n6. Additionally, if you used crowdsourcing or conducted\nresearch with human subjects, without compromising\nanonymity...\n(a) Did you include the full text of instructions given to\nparticipants and screenshots?Yes, included in the Sup-\nplementary Material.\n(b) Did you describe any potential participant risks, with\nmentions of Institutional Review Board (IRB) ap-\nprovals? Yes, we specify potential risks of the task in\nthe Methods section. The only mild risk crowdworkers\nwere exposed to was reading some potentially offen-\nsive machine-generated text.\n(c) Did you include the estimated hourly wage paid to\nparticipants and the total amount spent on participant\ncompensation? Yes, we specify the wage paid in the\nCrowdsourcing subsection under Methods.\n(d) Did you discuss how data is stored, shared, and dei-\ndentified? NA.\n163",
  "topic": "Power (physics)",
  "concepts": [
    {
      "name": "Power (physics)",
      "score": 0.49414971470832825
    },
    {
      "name": "Computer science",
      "score": 0.41215255856513977
    },
    {
      "name": "Psychology",
      "score": 0.34515485167503357
    },
    {
      "name": "Linguistics",
      "score": 0.3260446786880493
    },
    {
      "name": "Philosophy",
      "score": 0.11642476916313171
    },
    {
      "name": "Physics",
      "score": 0.10247084498405457
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}