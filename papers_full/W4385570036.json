{
  "title": "Evaluating Reading Comprehension Exercises Generated by LLMs: A Showcase of ChatGPT in Education Applications",
  "url": "https://openalex.org/W4385570036",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4282319581",
      "name": "Changrong Xiao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2306018239",
      "name": "Sean Xin Xu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2109111729",
      "name": "Kunpeng Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2103024338",
      "name": "Yufang Wang",
      "affiliations": [
        "Educational Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2101121212",
      "name": "Lei Xia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3011574394",
    "https://openalex.org/W4318257512",
    "https://openalex.org/W2996614149",
    "https://openalex.org/W2567552181",
    "https://openalex.org/W2147192413",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2785896739",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W4282824651",
    "https://openalex.org/W4306177347",
    "https://openalex.org/W1763968285",
    "https://openalex.org/W2564406167",
    "https://openalex.org/W574645793",
    "https://openalex.org/W4316041216",
    "https://openalex.org/W4206232983",
    "https://openalex.org/W2117373735",
    "https://openalex.org/W1481128830",
    "https://openalex.org/W4313384121",
    "https://openalex.org/W2251762574",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W4313564799",
    "https://openalex.org/W2963878748",
    "https://openalex.org/W4365601249",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W3187134297",
    "https://openalex.org/W3101652466",
    "https://openalex.org/W3110131742",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W1483044931",
    "https://openalex.org/W2989613245",
    "https://openalex.org/W3037013468",
    "https://openalex.org/W3115731421",
    "https://openalex.org/W2250864115"
  ],
  "abstract": "The recent advancement of pre-trained Large Language Models (LLMs), such as OpenAI's ChatGPT, has led to transformative changes across fields. For example, developing intelligent systems in the educational sector that leverage the linguistic capabilities of LLMs demonstrates a visible potential. Though researchers have recently explored how ChatGPT could possibly assist in student learning, few studies have applied these techniques to real-world classroom settings involving teachers and students. In this study, we implement a reading comprehension exercise generation system that provides high-quality and personalized reading materials for middle school English learners in China. Extensive evaluations of the generated reading passages and corresponding exercise questions, conducted both automatically and manually, demonstrate that the system-generated materials are suitable for students and even surpass the quality of existing human-written ones. By incorporating first-hand feedback and suggestions from experienced educators, this study serves as a meaningful pioneering application of ChatGPT, shedding light on the future design and implementation of LLM-based systems in the educational context.",
  "full_text": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 610–625\nJuly 13, 2023c⃝2023 Association for Computational Linguistics\nEvaluating Reading Comprehension Exercises Generated by LLMs:\nA Showcase of ChatGPT in Education Applications\nChangrong Xiao1, Sean Xin Xu 1, Kunpeng Zhang 2, Yufang Wang 3, Lei Xia 4\n1School of Economics and Management, Tsinghua University\n2Department of Decision, Operations & Information Technologies, University of Maryland\n3Beijing Xicheng Educational Research Institute\n4Shawn Tech\nxcr21@mails.tsinghua.edu.cn, xuxin@sem.tsinghua.edu.cn,\nkpzhang@umd.edu, wangwang7587@163.com, xialei@shawntech.com.cn\nAbstract\nThe recent advancement of pre-trained Large\nLanguage Models (LLMs), such as OpenAI’s\nChatGPT, has led to transformative changes\nacross fields. For example, developing intel-\nligent systems in the educational sector that\nleverage the linguistic capabilities of LLMs\ndemonstrates a visible potential. Though re-\nsearchers have recently explored how Chat-\nGPT could possibly assist in student learning,\nfew studies have applied these techniques to\nreal-world classroom settings involving teach-\ners and students. In this study, we implement\na reading comprehension exercise generation\nsystem that provides high-quality and person-\nalized reading materials for middle school En-\nglish learners in China. Extensive evaluations\nof the generated reading passages and corre-\nsponding exercise questions, conducted both\nautomatically and manually, demonstrate that\nthe system-generated materials are suitable for\nstudents and even surpass the quality of ex-\nisting human-written ones. By incorporating\nfirst-hand feedback and suggestions from expe-\nrienced educators, this study serves as a mean-\ningful pioneering application of ChatGPT, shed-\nding light on the future design and implementa-\ntion of LLM-based systems in the educational\ncontext.\n1 Introduction\nReading comprehension is a vital skill that English\nlearners need to develop and master. Chinese mid-\ndle school students, for instance, are required to do\nnumerous English practices, including reading at\nleast 150,000 words of supplemental materials to\nenhance their reading abilities, as mandated by the\nEnglish Curriculum Standards.\nThrough interviews with experienced English\nteachers in Beijing, we discovered a challenge\nfaced by both educators and students: the repeated\nuse of outdated reading materials, with only minor\nmodifications made, if any. For instance, Grade 8\nstudents are likely to practice the same exercises\nused by their predecessors in the previous academic\nyear (currently Grade 9 students). English teachers\nbelieve that offering up-to-date, engaging reading\nexercises tailored to each student’s capabilities and\ninterests can spark their enthusiasm for learning\nand ultimately boost their English proficiency.\nHowever, obtaining a large collection of diverse,\ncustomized, high-quality English reading exercises\nproves to be a non-trivial task. There is an abun-\ndance of articles in newspapers, magazines, text-\nbooks, and children’s books from English-speaking\ncountries that could serve as potential sources\nof reading materials for middle school students.\nNonetheless, adjustments and rewrites are typically\nnecessary due to variations in topic, length, and\ndifficulty level. Moreover, even for veteran teach-\ners, crafting appropriate exercise questions based\non textual materials is still not easy.\nPre-trained Large Language Models (LLMs)\nhave been proposed by researchers as a means to\naddress this labor-intensive and unscalable issue\n(Zhai, 2022; Dwivedi et al., 2023). Reading com-\nprehension exercises typically consist of two com-\nponents: a lengthy, coherent passage and several\nmultiple-choice questions that align with its con-\ntent. To generate such exercises, it is essential for\nLLMs to possess an advanced understanding and\ninference ability of human language. While the\ngeneration of long texts (such as stories, news arti-\ncles, and poems) (Li et al., 2021) and question-and-\nanswer (Q&A) pairs (Kurdi et al., 2020) have been\nextensively studied, existing task-specific models\nfall short of meeting our needs. For instance,\nthe generated content still remains distinguishable\nfrom human-written text, and the level of personal-\nization for different learners is inadequate (Kurdi\net al., 2020), making these models unsuitable for\ndirect application in educational settings.\nRecently, OpenAI released ChatGPT1, a versa-\ntile and interactive chatbot that outperforms state-\n1https://openai.com/blog/chatgpt\n610\nof-the-art models in various NLP tasks, even in\nzero-shot or few-shot scenarios. This powerful\nLLM presents numerous opportunities for educa-\ntion, including the creation of reading materials and\ncustomized practice questions. In this study, we at-\ntempt to develop a system for middle school teach-\ners and students that leverages ChatGPT to gener-\nate reading comprehension exercises. Guided by\ncarefully crafted prompts, ChatGPT can produce\npersonalized reading passages and multiple-choice\nquestions of high quality. To assess the generated\nexercises and the overall system, human evaluators\n(comprising students, teachers, and native speak-\ners) conducted an extensive analysis, determining\nthat the system holds promise for implementation\nin middle schools and has the potential to make a\nsignificant educational impact. In summary, this\nstudy makes threefold contributions:\n• We fully leverage the capabilities of the state-\nof-the-art LLMs to tackle complex and com-\npound tasks, integrating them within a care-\nfully designed education system2. The read-\ning passages and exercise questions generated\nby our system significantly surpass the quality\nof those produced by previous models, with\nsome even exceeding the standard of human-\nwritten textbook exercises.\n• To the best of our knowledge, our reading ex-\nercise generation system is among the first\napplications of ChatGPT in the education con-\ntext. The system has been utilized by middle\nschool English teachers, making real impacts\nin schools.\n• We gather feedback from both experts and gen-\neral users regarding the efficacy of our system.\nWe believe this is valuable, as there are few\ninstances of ChatGPT applications being em-\nployed in real-world educational settings. Our\nfindings offer insights for future researchers\nand practitioners to develop more effective\nAI-driven educational systems.\n2 Related Work\nLLM and Controllable Text Generation With\nthe emergence of Transformers (Vaswani et al.,\n2017), LLMs have been performing remarkably\nwell and showing considerable progress across a\n2The codes for our system is avail-\nable at https://github.com/Xiaochr/\nReading-Exercise-Generation-System .\nvariety of NLP tasks (Qiu et al., 2020). For ex-\nample, OpenAI’s GPT series models are power-\nful LLMs that perform well in long open-ended\ntext generation. While they are able to generate\ntexts of high fluency, researchers have found that\nas the generated text gets longer, it starts to wander,\nswitch to unrelated topics, and become incoherent\n(Rashkin et al., 2020). By fine-tuning with specific\ndomain data or applying some plug-and-play ap-\nproaches like PPLM (Dathathri et al., 2020), LLMs\nwill obtain some controllability and generate more\ncoherent text, though the quality is still limited.\nChatGPT is developed on the foundation of GPT-\n3.5 or GPT-4 architectures, with the inclusion of ad-\nditional human-directed instructions for enhanced\nperformance. It possesses robust in-context learn-\ning capabilities, enabling it to interpret require-\nments specified in input prompts without the need\nfor additional information (zero-shot learning), or\nby utilizing a minimal number of provided exam-\nples (few-shot learning). Even without massive\ndomain knowledge, ChatGPT is able to follow hu-\nman instructions and generate text of higher quality.\nFor instance, to generate a 200-word reading pas-\nsage on the topic of school life, one simply needs\nto specify the subject and length requirements in\nthe prompt to ChatGPT.\nChatGPT in Education With the thriving of\nAI technology, its applications in education have\nbeen increasing, transforming ways of teaching\nand learning (Zhang and Aslan, 2021). Recog-\nnizing the surprising capacity of LLMs, such as\nChatGPT, researchers have been discussing their\nenormous potential impacts in various educational\nscenarios (Zhai, 2022). Some studies (Dwivedi\net al., 2023; Pettinato Oltz, 2023) suggested that\nChatGPT can provide students with basic educa-\ntional materials. LLMs are trained on vast corpora\ncreated by humans to “learn” the language, and now\nthey can “teach” human learners what they have\nalready learned. Moreover, inherent to its chatbot\ncharacteristics, ChatGPT can function as a personal\ntutor, providing real-time feedback (Zentner, 2022),\npersonalized evaluations and suggestions (Baidoo-\nAnu and Owusu Ansah, 2023; Zhang, 2023), and\nother learning supports (Dwivedi et al., 2023), such\nas improving the engagement and autonomy of stu-\ndents (Firat, 2023) and addressing the low teacher-\nstudent ratio problem (Chen et al., 2023).\nOn the other hand, the misuse of ChatGPT has\nexisted since its release (Zhang et al., 2023). A poll\n611\n3 done by Study.com (an online course provider)\nreveals that 89% of the participating students uti-\nlized ChatGPT for homework and 48% of them\nconfessed to using ChatGPT for at-home tests. It\nis important and still under exploration to design\nsuitable learning tasks and systems that can guide\nstudents to use ChatGPT properly as a helpful learn-\ning assistant.\nEvaluation of Long Text Generation To eval-\nuate the quality of the generated long text, re-\nsearchers have developed several metrics, including\nSelf-BLEU (Zhu et al., 2018) andn-gram repetition\nscore (Welleck et al., 2020). They are often unreli-\nable and inconsistent with human judgment (Belz\net al., 2020). Therefore, human evaluation remains\nthe gold standard for most long text generation\ntasks, even if it is expensive and time-consuming\n(Celikyilmaz et al., 2020).\nBelz and Reiter (2006) grouped the common\nhuman evaluation approaches into intrinsic and ex-\ntrinsic ones. Most current text generation tasks are\nmeasured with intrinsic human evaluations, where\nparticipants are asked to rate the quality of the\ngenerated text, either overall or along with some\ndesigned dimensions (e.g., fluency, coherence, and\ncorrectness) (Celikyilmaz et al., 2020). Likert and\nsliding scale are commonly used scoring methods,\ndespite the many limitations (e.g., inconsistency,\nnot straightforward) (Celikyilmaz et al., 2020). To\naddress this, comparative approaches, such as rank-\ning, have been proposed and found to achieve high\ninter-annotator agreement (Callison-Burch et al.,\n2007). On the other hand, the extrinsic evaluation\nmeasures how successful a system is in downstream\ntasks, from both a user’s success in a task and the\nsystem’s success in fulfilling its purpose (Celikyil-\nmaz et al., 2020; Hastie and Belz, 2014).\n3 Methods\n3.1 Reading passage Generation Baseline\nWe use a fine-tuned GPT-2 (Radford et al., 2019)\nwith PPLM (Dathathri et al., 2020) control as the\nbaseline method to generate reading passages. The\ntwo-stage development of the baseline model is\nshown in Figure 1.\nIn the first step, we fine-tune our base LLM,\nGPT-2 medium, using two reading datasets ob-\ntained from middle school teachers: supplemental\n3https://futurism.com/the-byte/\nstudents-admit-chatgpt-homework\nFigure 1: The fine-tuned GPT-2 + PPLM baseline\nreading materials (Dataset 1) and textbook exercise\npassages that are currently used in middle schools\n(Dataset 2). We adopt a two-step fine-tuning strat-\negy with varying learning rates to accommodate\nthe distinct characteristics of each dataset. In the\nsecond step, we employ PPLM, a plug-and-play\ncontrollable text generation approach, to guide the\nfine-tuned language model in generating more co-\nherent passages based on specified topic keywords.\nFor more details, please refer to the Appendix A.\n3.2 ChatGPT for Reading Exercise\nGeneration\nUtilizing the impressive capabilities of ChatGPT,\nwe manually design input prompts to generate high-\nquality reading comprehension passages without\nthe need for fine-tuning or additional control meth-\nods. In this study, we produce textual content in\ntwo settings: zero-shot and one-shot, which allow\nus to control the output to varied degrees.\nIn the zero-shot setting, we instructed ChatGPT\nto be a helpful learning assistant capable of gener-\nating high-quality reading passages in the system\nprompt. We provided customized requirements\nwithin the conversation prompt, including length,\ngenre, difficulty level, and topics. In addition to cre-\nating reading passages from scratch, teachers often\nsource content from the web or other materials and\nseek to adapt them into suitable reading passages\nfor students. Thus we added an extra requirement,\na referenced passage, in the one-shot setting.\nWe also generate questions and correspond-\ning answers for given passages using appropriate\nprompts. We set the number of questions, the num-\nber of options per question, and the question type\nfor customization in the input prompt. ChatGPT\ncan generate exercise questions based on either\na passage it previously created or a passage pro-\nvided by users. Moreover, an extra toxicity check\nis applied before the generated exercises are made\navailable to teachers and students.\nWe will describe the process of reading exer-\ncise generation using ChatGPT and the design of\nappropriate prompts in Appendix B.\n612\nFigure 2: The screenshot of the system interface.\n3.3 System Design\nCatering to non-technical users such as middle\nschool teachers and students, we integrate the fea-\ntures discussed in previous sections into a uni-\nfied system with a graphical user interface. The\nprompts and API calls are managed at the system\nbackend, while a user-friendly and straightforward\ninterface (Figure 2) is designed for ease of use4.\nOn the left side of the interface, users can easily\nset their requirements, with each previously men-\ntioned feature incorporated. The output reading\npassages and exercise questions are displayed on\nthe right. These text areas are editable, allowing\nteachers to further modify the generated content\nto create a final version of exercises suitable for\nstudent practice.\n4 Evaluation\nIn this section, we conduct extensive evaluations\nof our reading exercise generation system, which\nare visually depicted in Figure 3.\nFor reading passage quality evaluation, we ran-\ndomly select 30 human-written reading passages\nfrom Dataset 2 (the reading exercises from text-\nbooks), which are paired with an additional 60\npassages: 30 produced by ChatGPT and 30 by\nthe baseline model. This mixture of passages is\nshuffled and compiled into what we refer to as the\n4To try the system demo online, please refer to our GitHub\nrepository. We will keep the link to the demo up-to-date.\nReading Passages Example Set 1. We utilize both\nautomatic evaluation metrics and human assess-\nments (Section 4.1) in order to comprehensively\nevaluate these passages.\nTo further verify the high quality of ChatGPT\npassages, a series of one-to-one comparisons is\nconducted between passages produced by language\nmodels and their human-written counterparts. We\nselect 10 human-written reading comprehension\npassages, distinct from the passages in the Read-\ning Passages Example Set 1, and summarize the\ntopic of each one. We then use these topics as guid-\ning constraints to direct conditional text generation\nwith both the GPT-2 + PPLM baseline and Chat-\nGPT (zero-shot), resulting in passages mirroring\nthe topics of the original human-written examples.\nAdditionally, a one-shot variant of ChatGPT, using\nthe human-written passage as a reference, is uti-\nlized to generate a third group of passages. To sum\nup, the Reading Passages Example Set 2 encom-\npasses 10 original human-written passages, aug-\nmented with 30 generated passages that align with\nthe same topics.\nMoving to the evaluation of exercise question\nquality, we select 10 exercises containing reading\npassages and their associated questions from the\ntextbook to serve as benchmarks. A new set of\nmultiple-choice questions is generated based on the\nhuman-written passages using our system. Thus,\nthese 10 reading passages and their corresponding\n613\nFigure 3: The illustration for each evaluation section.\n20 sets of questions form the Exercise Questions\nExample Set, which is thoroughly evaluated in Sec-\ntion 4.2.\nFor the overall evaluation of our system (Sec-\ntion 4.3), we invite middle school educators, the\nintended users of our system, to utilize it first-hand.\nWe request their insightful feedback and sugges-\ntions, furthering our goal of consistent improve-\nment and customization to user needs.\n4.1 Reading Passage Quality Assessment\nAutomatic Metrics First, we apply automatic\nmetrics commonly used in the literature on the\nReading Passages Example Set 1. Table 1\npresents the quantitative performance comparison\nof ChatGPT-generated reading passages with those\nproduced by the baseline model and those written\nby human educators in textbooks. In general, the\nresults indicate that the passages generated by the\nfine-tuned GPT-2 baseline are the easiest to read,\nand their average negative log-likelihood (NLL)\nis the lowest. However, this does not necessar-\nily imply that the fine-tuned GPT-2 is the best\nmodel (Wang et al., 2022), as it may be overfitted\nin terms of NLL and generate text with high repeti-\ntion. Moreover, high readability does not guarantee\nthat the passages are logical and coherent, which\nare important dimensions for evaluating the quality\nof generated long text. The ChatGPT-generated\npassages receive the lowest readability scores, and\nalso exhibit greater diversity.\nIn addition to automatic metrics, scores evalu-\nated by experienced and trained human annotators\nserve as more reliable benchmarks (Clark et al.,\n2021). Next, we will introduce two designs for\nhuman evaluation in this study.\nReadability Diversity\nNLL SMOG Flesch TTR Rep.\nHuman 21.89 8.46 81.46 53.84 3.06\nGPT-2 18.60 6.59 92.50 44.76 4.05\nChatGPT 24.90 9.81 73.29 56.51 2.28\nTable 1: Results of automatic evaluation metrics on the\nReading Passages Example Set 1. NLL (Alihosseini\net al., 2019): the average negative log-likelihood loss;\nSMOG (McLaughlin, 1969): SMOG grade index esti-\nmates the years of education needed to understand the\nwriting; Flesch (Flesch, 1979): Flesch reading-ease test,\nhigher scores indicate material that is easier to read;\nTTR (%) (Richards, 1987; Celikyilmaz et al., 2020):\nthe number of unique words (types) divided by the total\nnumber of words (tokens); Rep. (%) (Welleck et al.,\n2020; Pascual et al., 2021): the proportion of repeated\n4-grams.\nHuman Evaluation 1: Multi-dimension Quality\nScoring We invite two groups of participants to\nassess the quality of the Reading Passages Example\nSet 1: 9 Chinese college students and 364 native\nEnglish speakers. Chinese college students have\nyears of English exercise training experience from\nmiddle school, and are familiar with reading com-\nprehension exercises. Meanwhile, native English\nspeakers possess a higher level of English profi-\nciency than Chinese students, and their evaluation\nof the language may be more professional, but they\nhave no idea what the reading passages in Chinese\nmiddle schools look like.\nBefore scoring each passage, the 9 student evalu-\nators are given detailed guidelines about the evalu-\nation rules, including the meanings of each quality\ndimension and two examples of middle school read-\ning comprehension passages. To prevent fatigue,\neach evaluator is assigned only 30 passages. We\n614\nReadability Correctness Coherence Engagement Overall Quality\nChinese Students\nHuman-Written 4.52 4.32 4.39 4.07 4.18\nFine-tuned GPT-2 3.57 3.73 2.69 2.78 2.84\nChatGPT (zero-shot) 4.61 4.60 4.65 4.37 4.46\nNative Speakers\nHuman-Written 3.79 3.67 3.77 3.77 3.89\nFine-tuned GPT-2 3.52 3.51 3.53 3.62 3.75\nChatGPT (zero-shot) 3.78 3.69 3.77 3.93 4.06\nTable 2: Quality scores of the three groups of passages in five dimensions evaluated by experienced Chinese students\nand English native speakers.\ncollect 270 individual evaluations in total, with 3\nevaluations for each passage. For native English\nspeakers, we recruit them from Amazon Mechani-\ncal Turk and collect 5 evaluations for each passage.\nEach evaluation consists of 5 scores measuring\ndifferent dimensions of text quality. These dimen-\nsions are widely used in human evaluations of text-\ngeneration studies and have been carefully selected\nbased on their importance to the reading compre-\nhension scenario. The explanations of quality di-\nmensions are as follows:\n• Readability: The extent to which texts are\neasy to read (Forrest et al., 2018; Di Fabbrizio\net al., 2014) and fluent (Mahapatra et al., 2016;\nBelz and Kow, 2010).\n• Correctness: The extent to which texts ac-\ncurately reflect facts and commonsense, how\nlogical they are (Celikyilmaz et al., 2020), and\nwhether they are proper in grammar (Wubben\net al., 2016).\n• Coherence: The extent to which texts are con-\nsistent with certain topics or storylines (San-\nthanam and Shaikh, 2019).\n• Engagement: The extent to which texts are\ninteresting and engaging.\n• Overall Quality: The overall text quality of\nthe reading passages.\nThe evaluation results are shown in Table 2. Sur-\nprisingly, as rated by experienced students, the qual-\nity scores of ChatGPT passages are higher than\nthe scores of human-written passages across all se-\nlected dimensions. The passages generated by the\nfine-tuned GPT-2 baseline are generally of lower\nquality, and not comparable to the other two groups\nof passages. For the evaluations of native speakers,\nthe scores of the passages are generally lower than\nthose marked by Chinese students, since the read-\ning materials used by middle school students may\nbe too simple for native speakers. Nonetheless, the\nconclusion does not change: ChatGPT passages\nhave the highest overall quality.\nWe also conduct inter-annotator reliability tests\nto make sure the evaluation results are reliable.\nAmong the student evaluators, we observe an aver-\nage Pearson’s Correlation of 0.64, and the average\nCronbach’s Alpha of the rating scores is 0.82, in-\ndicating a high internal consistency and a reliable\nmeasurement. Similar tests were conducted in the\nfollowing human evaluations, all of which showed\nreliable results, so we will not elaborate on further.\nHuman Evaluation 2: Pairwise Comparison\nThe three groups of generated passages (GPT-2 +\nPPLM, ChatGPT zero-shot, and ChatGPT one-shot\ngenerated) in the Reading Passages Example Set\n2 are displayed side-by-side with human-written\npassages for evaluators to compare. In other words,\neach evaluator is presented with two passages at\na time, one generated by the model and the other\nwritten by humans, with the order randomized.\nWe did not recruit native speakers for this evalu-\nation but relied entirely on college students. Since\nwe believe that native speakers who are not familiar\nwith reading comprehension exercises in China are\nnot suitable for the comparison evaluation. Another\n9 students were recruited for Human Evaluation 2\nto avoid the learning effect. Similar to Human Eval-\nuation 1, we collect 3 evaluations for each set of\npassages. The evaluation questions are as follows.\n• Relative quality score . Since the previous\nevaluation has already assessed multiple dimen-\nsions, here we only focus on the overall quality for\nsimple verification. For the two passages displayed\nsimultaneously, we ask the evaluators to mark the\npassage of better quality with a score of 1, and the\nother one with a score of 0. By taking the average\nat the level of passages and evaluators, we obtain\nthree average quality scores for the three groups of\ngenerated passages and three for the human-written\n615\nones, respectively. The following evaluation ques-\ntions are analyzed in a similar way.\nTable 3 shows that the ChatGPT scores are much\nhigher than the baseline score. Moreover, evalua-\ntors believe that the quality of ChatGPT passages\nis even better than human-written ones (0.87 vs.\n0.13 in the zero-shot setting and 0.80 vs. 0.20 in\nthe one-shot setting), which is consistent with our\nfindings in Human Evaluation 1. For the ChatGPT\npassages, the one-shot score is slightly lower than\nthe zero-shot score (0.80 vs. 0.87), which may be\ndue to more restrictions leading to a slight decrease\nin quality. Nonetheless, ChatGPT performs quite\nwell in the reading passage generation task with\nour designed prompts.\nHuman Generated\nFine-tuned GPT-2 + PPLM 0.70 0.30\nChatGPT (zero-shot) 0.13 0.87\nChatGPT (one-shot) 0.20 0.80\nTable 3: The comparison of relative quality score be-\ntween human-written passages and generated ones. A\nhigher score indicates better quality.\n• Model-Generated Score. We also investi-\ngate whether evaluators can distinguish between\npassages written by humans and those generated\nby models. To do so, we design a simple Turing\ntest by asking evaluators to assign a score of 1 if\nthey believe the passage is generated by language\nmodels, and 0 otherwise. Therefore, the lower the\nscore, the more likely the passage is perceived to\nbe written by humans. From Table 4, we find that\nthe passages generated by ChatGPT scored lower\nthan the human-written passages displayed side-by-\nside, meaning that evaluators believe the ChatGPT\npassages are more likely to be human-written than\nthe true ones, which is an interesting finding.\nAnother finding is that both generated and\nhuman-written passages in the one-shot setting\nscored the lowest. One plausible reason is that\nChatGPT imitated the styles and structures of the\nreferenced passage very well. When two similar\npassages of high quality appeared at the same time,\nevaluators tended to think that they were unlikely\nto be generated by models.\nNote that if native speakers were asked to eval-\nuate this dimension, the results might be different.\nBecause they have a higher language proficiency\nand are more likely to notice characteristics that\nnon-native speakers did not pay attention to.\n• Topic Coherence Score. We examine whether\nHuman Generated\nFine-tuned GPT-2 + PPLM 0.40 0.57\nChatGPT (zeor-shot) 0.53 0.30\nChatGPT (one-shot) 0.33 0.23\nTable 4: The comparison of model-generated score\nbetween human-written passages and generated ones. A\nhigher score indicates that the passage is more likely to\nbe perceived from language models, instead of written\nby humans.\nthe passages are consistent with the given topics,\nthat is, the control and personalization ability of\nthe models. A score of 1 is given for consistency\nwhile 0 means inconsistency. Table 5 shows that\neven after fine-tuning with domain knowledge and\nwith the extra control of PPLM, the GPT-2 baseline\nstill did not generate passages that follow the given\nrequirements well. In contrast, ChatGPT scored\nparticularly high even in zero-shot (with a score of\n0.97), indicating that it understands and follows the\ninstructs specified in the prompts quite well.\nHuman Generated\nFine-tuned GPT-2 + PPLM 0.87 0.40\nChatGPT (zero-shot) 0.77 0.97\nChatGPT (one-shot) 0.77 0.97\nTable 5: The comparison of topic coherence score\nbetween human-written passages and generated ones. A\nhigher topic coherence score indicates that the passage\nis more consistent with the given topics.\n• Suitability Score. This evaluation dimension\nrequires the evaluator to have extensive experience\nwith reading comprehension exercises and is not\nsuitable for native English speakers who are unfa-\nmiliar with Chinese English education. If deemed\nsuitable, the passage should receive a score of 1,\n0 otherwise. Our findings in Table 6 show that\nevaluators generally believe that the passages gen-\nerated by ChatGPT are largely suitable as reading\ncomprehension materials and are even better than\npassages currently used as exercises.\nHuman Generated\nFine-tuned GPT-2 + PPLM 0.53 0.37\nChatGPT (zero-shot) 0.40 0.77\nChatGPT (one-shot) 0.53 0.77\nTable 6: The comparison of suitability score between\nhuman-written passages and generated ones. A higher\nsuitability score indicates that the passage is more suit-\nable for middle school students in China.\n616\nIn summary, the human evaluation results sug-\ngest that the ChatGPT passages generated by our\nsystem are of high quality across various dimen-\nsions, and even better than the human-written read-\ning passages in many cases. The experienced eval-\nuators believe that it is suitable to apply these ma-\nterials in real educational contexts.\n4.2 Exercise Question Quality Assessment\nNext, we will evaluate the quality of the generated\nreading exercise questions. Currently, there is no\nreliable metric for evaluating the quality of gener-\nated multiple-choice questions, so we entirely rely\non human evaluation.\nSimilar to how we evaluate passages in Human\nEvaluation 2, each evaluator is presented with two\nsets of questions, one generated by the system and\none written by humans, along with the base passage\nin the Exercise Questions Example Set. The evalua-\ntors are asked to assess the quality of the questions\naccording to various aspects, using scores ranging\nfrom 1 to 5. The following aspects are considered:\n• The extent to which the questions match\nthe passage content. We want to check whether\nthe questions generated by our system align with\nthe content of the passages and whether we can\nfind correct answers within the passages. This is a\nbasic requirement for the generated questions to be\nsuitable for student practice.\n• The extent to which the questions are useful\nfor the training of students. Moreover, we ensure\nthat the questions are not meaningless and that they\ncan serve as effective exercises that contribute to\nstudents’ English training.\n• The extent to which the questions are suit-\nable for middle school English learners. This\ndimension is similar to the previous one. Based\non their extensive experience with English read-\ning exercises, evaluators rate whether the generated\nquestions are too difficult or too simple for students\nin Chinese middle schools.\n• The extent to which the questions appear to\nbe written by language models. If the generated\nquestions exhibit certain patterns, they will be eas-\nily distinguished from the exercise questions in the\ntextbook, indicating that the generated questions\nare too rigid and not flexible enough.\nFrom Table 7, we observe that human-written\nquestions outperform generated questions across all\nfour dimensions. Although the generated questions\nare highly relevant to the passage content (with a\nHuman Generated\nMatch 4.58 4.38\nUseful 3.93 3.25\nSuitable 3.92 3.48\nGenerated or not 0.27 0.67\nTable 7: The comparison of exercise quality in four\ndimensions between human-written and generated ones.\nMatch score of 4.38 out of 5), some of them exhibit\nobvious patterns, are too straightforward, and lack\nvariation. Teachers may need to select suitable\nexercise questions from the various generated ones\nbefore assigning them to students.\n4.3 System Quality Assessment\nOur system, which integrates the features described\nabove, is primarily designed for middle school\nteachers. To gather feedback on the system, we\ninvited three experienced teachers in Beijing, who\nhave many years of teaching experience, to person-\nally use the system for a week and provide their\nfeedback through interviews. Their feedback and\nsuggestions are summarized in Table 12 in Ap-\npendix C.\nAlthough there is still room for improvement,\nsuch as further optimizing the generation of\nmultiple-choice questions, the quality of reading\nexercises generated by our system has greatly ex-\nceeded teachers’ expectations. Teachers view this\nsystem as a valuable tool that can significantly re-\nduce cost and time while providing students with\nmore diverse and personalized learning materials.\n5 Conclusion\nIn this study, we attempted to develop an educa-\ntional system for teachers and English learners in\nChinese middle schools that leverages the capabili-\nties of LLMs to generate reading comprehension\nexercises. Extensive evaluations were conducted\namong various groups of representative human\nevaluators, and the high quality of the generated\nreading exercises was widely acknowledged. Expe-\nrienced English teachers also provided extremely\npositive feedback on the system, indicating its po-\ntential for widespread use in real-world education.\nOur system is among the first applications of Chat-\nGPT in educational contexts, and the valuable feed-\nback and findings are likely to inspire future re-\nsearchers and educators in integrating AI technol-\nogy into education.\n617\nLimitations\nAs noted in the evaluation section, our system does\nnot perform perfectly in multiple-choice question\ngeneration, particularly when it comes to gener-\nating distracting options, even with the powerful\nChatGPT. In the next step, we can adopt an open-\nsource framework of LLMs and fine-tune a domain-\nspecific model using the extensive educational ma-\nterials provided by middle school teachers. This\nway, the question generation ability may be im-\nproved, and we will not need to rely on the OpenAI\nAPI.\nOn the other hand, although extensive evalua-\ntions have been conducted, they only involve a\nsmall fraction of teachers and students in a pre-\ninterview setting. Once our system is widely de-\nployed, a larger amount of user feedback will be\ncollected and analyzed to monitor its effectiveness.\nReferences\nDanial Alihosseini, Ehsan Montahaei, and Mahdieh So-\nleymani Baghshah. 2019. Jointly measuring diversity\nand quality in text generation models. InProceedings\nof the Workshop on Methods for Optimizing and Eval-\nuating Neural Language Generation, pages 90–98,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nDavid Baidoo-Anu and Leticia Owusu Ansah. 2023. Ed-\nucation in the era of generative artificial intelligence\n(ai): Understanding the potential benefits of chatgpt\nin promoting teaching and learning. Available at\nSSRN 4337484.\nAnja Belz and Eric Kow. 2010. Comparing rating scales\nand preference judgements in language evaluation.\nIn Proceedings of the 6th International Natural Lan-\nguage Generation Conference. Association for Com-\nputational Linguistics.\nAnja Belz and Ehud Reiter. 2006. Comparing auto-\nmatic and human evaluation of NLG systems. In\n11th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 313–\n320, Trento, Italy. Association for Computational\nLinguistics.\nAnya Belz, Simon Mille, and David M. Howcroft. 2020.\nDisentangling the properties of human evaluation\nmethods: A classification system to support compa-\nrability, meta-evaluation and reproducibility testing.\nIn Proceedings of the 13th International Conference\non Natural Language Generation , pages 183–194,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nChris Callison-Burch, Cameron Fordyce, Philipp Koehn,\nChristof Monz, and Josh Schroeder. 2007. (meta-)\nevaluation of machine translation. In Proceedings of\nthe Second Workshop on Statistical Machine Transla-\ntion, pages 136–158, Prague, Czech Republic. Asso-\nciation for Computational Linguistics.\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n2020. Evaluation of text generation: A survey. arXiv\npreprint arXiv:2006.14799.\nYu Chen, Scott Jensen, Leslie J Albert, Sambhav Gupta,\nand Terri Lee. 2023. Artificial intelligence (ai) stu-\ndent assistants in the classroom: Designing chatbots\nto support student success. Information Systems\nFrontiers, 25(1):161–182.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that’s ‘human’ is not gold: Evaluating\nhuman evaluation of generated text. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7282–7296, Online.\nAssociation for Computational Linguistics.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\nInternational Conference on Learning Representa-\ntions.\nGiuseppe Di Fabbrizio, Amanda Stent, and Robert\nGaizauskas. 2014. A hybrid approach to multi-\ndocument summarization of opinions in reviews. In\nProceedings of the 8th International Natural Lan-\nguage Generation Conference (INLG), pages 54–63,\nPhiladelphia, Pennsylvania, U.S.A. Association for\nComputational Linguistics.\nYogesh K Dwivedi, Nir Kshetri, Laurie Hughes,\nEmma Louise Slade, Anand Jeyaraj, Arpan Kumar\nKar, Abdullah M Baabdullah, Alex Koohang, Vish-\nnupriya Raghavan, Manju Ahuja, et al. 2023. “so\nwhat if chatgpt wrote it?” multidisciplinary perspec-\ntives on opportunities, challenges and implications\nof generative conversational ai for research, prac-\ntice and policy. International Journal of Information\nManagement, 71:102642.\nMehmet Firat. 2023. How chat gpt can transform au-\ntodidactic experiences and open education. Depart-\nment of Distance Education, Open Education Faculty,\nAnadolu Unive.\nRudolf Flesch. 1979. How to write plain english. Uni-\nversity of Canterbury.\nJames Forrest, Somayajulu Sripada, Wei Pang, and\nGeorge Coghill. 2018. Towards making NLG a voice\nfor interpretable machine learning. In Proceedings\nof the 11th International Conference on Natural Lan-\nguage Generation, pages 177–182, Tilburg Univer-\nsity, The Netherlands. Association for Computational\nLinguistics.\n618\nHelen Hastie and Anja Belz. 2014. A comparative eval-\nuation methodology for NLG in interactive systems.\nIn Proceedings of the Ninth International Conference\non Language Resources and Evaluation (LREC’14),\npages 4004–4011, Reykjavik, Iceland. European Lan-\nguage Resources Association (ELRA).\nMuhammad Khalifa, Hady Elsahar, and Marc Dymet-\nman. 2021. A distributional approach to controlled\ntext generation. In International Conference on\nLearning Representations.\nGhader Kurdi, Jared Leo, Bijan Parsia, Uli Sattler, and\nSalam Al-Emari. 2020. A systematic review of auto-\nmatic question generation for educational purposes.\nInternational Journal of Artificial Intelligence in Ed-\nucation, 30:121–204.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong\nWen. 2021. Pretrained language model for text gener-\nation: A survey. In Proceedings of the Thirtieth Inter-\nnational Joint Conference on Artificial Intelligence,\nIJCAI-21, pages 4492–4499. International Joint Con-\nferences on Artificial Intelligence Organization. Sur-\nvey Track.\nJoy Mahapatra, Sudip Kumar Naskar, and Sivaji Bandy-\nopadhyay. 2016. Statistical natural language genera-\ntion from tabular non-textual data. In Proceedings of\nthe 9th International Natural Language Generation\nconference, pages 143–152.\nGH McLaughlin. 1969. Smog grading – a new readabil-\nity formula. Journal of Reading, 12(8):639–646.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nDamian Pascual, Beni Egressy, Clara Meister, Ryan\nCotterell, and Roger Wattenhofer. 2021. A plug-and-\nplay method for controlled text generation. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 3973–3997, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nTammy Pettinato Oltz. 2023. Chatgpt, professor of law.\nProfessor of Law (February 4, 2023).\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, 63(10):1872–\n1897.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nHannah Rashkin, Asli Celikyilmaz, Yejin Choi, and\nJianfeng Gao. 2020. PlotMachines: Outline-\nconditioned generation with dynamic plot state track-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 4274–4295, Online. Association\nfor Computational Linguistics.\nBrian Richards. 1987. Type/token ratios: What do they\nreally tell us? Journal of child language, 14(2):201–\n209.\nSashank Santhanam and Samira Shaikh. 2019. Towards\nbest experiment design for evaluating dialogue sys-\ntem output. In International Conference on Natural\nLanguage Generation.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nYequan Wang, Jiawen Deng, Aixin Sun, and Xuy-\ning Meng. 2022. Perplexity from plm is unreli-\nable for evaluating text quality. arXiv preprint\narXiv:2210.05892.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\nInternational Conference on Learning Representa-\ntions.\nSander Wubben, Emiel Krahmer, Antal van den Bosch,\nand Suzan Verberne. 2016. Abstractive compression\nof captions with attentive recurrent neural networks.\nIn Proceedings of the 9th International Natural Lan-\nguage Generation conference, pages 41–50, Edin-\nburgh, UK. Association for Computational Linguis-\ntics.\nAeron Zentner. 2022. Applied innovation: Artificial\nintelligence in higher education. Available at SSRN\n4314180.\nXiaoming Zhai. 2022. Chatgpt user experience: Impli-\ncations for education. Available at SSRN 4312418.\nBo Zhang. 2023. Preparing educators and students for\nchatgpt and ai technology in higher education.\nChaoning Zhang, Chenshuang Zhang, Chenghao\nLi, Yu Qiao, Sheng Zheng, Sumit Kumar Dam,\nMengchun Zhang, Jung Uk Kim, Seong Tae Kim,\nJinwoo Choi, et al. 2023. One small step for genera-\ntive ai, one giant leap for agi: A complete survey on\nchatgpt in aigc era. arXiv preprint arXiv:2304.06488.\nKe Zhang and Ayse Begum Aslan. 2021. Ai technolo-\ngies for education: Recent research & future direc-\ntions. Computers and Education: Artificial Intelli-\ngence, 2:100025.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\nbenchmarking platform for text generation models.\nIn The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval,\npages 1097–1100.\n619\nA GPT-2 + PPLM Baseline\nA.1 Data\nWe collaborate with the Municipal Education Com-\nmission and 8 local middle schools in Beijing. We\nare provided 8,650 reading passages in total, includ-\ning 5,066 supplemental reading materials (Dataset\n1) and 3,584 currently used textbook exercise pas-\nsages (Dataset 2), covering different difficulty lev-\nels from Grade 7 to Grade 9.\nThe descriptive statistics of our manually col-\nlected two datasets are shown in Table 8.\nDataset 1 Dataset 2\n# passages 5,066 3,584\nmin. length 32 30\navg. length 967.34 251.07\nmax. length 15,242 780\nTable 8: Descriptive statistics of the two datasets.\nDue to the confidentiality of educational re-\nsources, we are not able to publicly offer access to\nDataset 1. Nonetheless, the trained model (with the\nfine-tuning process) using our datasets is provided\nin our GitHub repository.\nA.2 GPT-2 Fine-tuning\nWhen fine-tuning, we adopt a two-step fine-tuning\nstrategy to account for the different characteristics\nof the two datasets. In the first step, the model\nlearns the general language features with a larger\nlearning rate from Dataset 1. In the second step,\nfine-tuning on Dataset 2 with a lower learning rate\nand longer training epochs, the model is able to\nlearn fine-grained characteristics of textbook read-\ning passages, including formats, topics, and writing\nstyles.\nAll the training processes of the GPT-2 baseline\nare implemented on a 16 GB NVIDIA Tesla P100\nPCIe GPU provided by Google Colab.\nWe use the OpenAI GPT-2 medium model with\n24-layer, 1,024-hidden layers, 16-heads, and 345M\nparameters, implemented by the Huggingface trans-\nformer library.\nThe textual materials in the dataset are tokenized\nby GPT-2 tokenizer. Since the max input length of\nthe GPT-2 medium model is 1,024, we truncate all\nthe passages that are longer than 1,024 tokens, and\npad all passages that are shorter than 1,024 tokens\nto the same length of 1,024.\nWe randomly split the dataset into 80% as the\ntraining set and the remaining 20% as the test set.\nThe batch size is 2 and the random seed is 42.\nThe AdamW optimizer with ϵ= 10−8 is applied,\nand we adopt a linear learning schedule with 100\nwarm-up steps. The detailed training setting of our\nproposed two-step fine-tuning and other baseline\nstrategies are shown in Table 9. The entire fine-\ntuning process using our two-step strategy takes\napproximately 6 hours.\nLearning rate # epochs\nDataset 1 1 × 10−5 5\nDataset 2 1 × 10−5 3\nSingle-step 1 × 10−5 5\nTwo-step (1) 5 × 10−4 3\nTwo-step (2) 1 × 10−5 5\nTable 9: Hyper-parameter setting for fine-tuning.\nBy manually examining the generated passages\nfrom all baseline strategies, we summarize and\nconclude that our two-step fine-tuning strategy\nachieves the best performance.\n• Fine-tuning with only dataset 1: The lengths\nof generated passages are often too short or\ntoo long, and the word repetition problem of-\nten occurs.\n• Fine-tuning with only dataset 2: The lengths\nof generated passages are often too short or\ntoo long. The format and word repetition prob-\nlems exist.\n• Single-step fine-tuning with combined\ndatasets: The overall quality of the generated\npassages is higher than fine-tuning with only\none dataset, but their length is still unstable.\n• Proposed two-step fine-tuning: It performs\nthe best, and the problems mentioned above\nare significantly alleviated.\nA.3 PPLM\nTo generate more coherent texts on a given topic,\nwe apply a plug-and-play controllable text genera-\ntion approach with topic keywords provided. It is\nexpected that providing more keywords will lead to\nmore coherent generated passages. We first provide\na few (e.g., 3 to 5) initial topic words. This list can\nthen be expanded to include more similar words\n(e.g., 30 words) by finding similar words based on\nword embeddings from a Word2Vec (Mikolov et al.,\n2013) model trained on our two reading datasets.\nPrevious studies (Khalifa et al., 2021) showed that\n620\nPPLM tends to produce texts with frequent repeti-\ntions due to inappropriate hyper-parameters. There-\nfore, before applying PPLM to guide text genera-\ntion, we use a simple grid search strategy to find\nthe best hyper-parameters for each topic.\nWe adopt the Word2Vec model implemented\nby the gensim library 5 and train it from scratch\nwith our reading passage datasets. The hyper-\nparameters of Word2Vec are as follows: vec-\ntor_size=512, window=5, min_count=5, work-\ners=4.\nAs mentioned above, a simple grid search is ap-\nplied to seek the best hyper-parameters for each set\nof keywords, respectively. According to Dathathri\net al. (2020), we tune the hyper-parameters that are\nrelevant to the topic control intensity. The ranges\nof these parameters are listed in Table 10. The\ncriterion to select hyper-parameters is based on\nmanual examinations of the quality of generated\npassages. The set of hyper-parameters that guide\nthe fine-tuned GPT-2 to generate passages with the\nhighest overall quality will be regarded as the best\none.\nParameter Range\nstep_size [0.02, 0.025, 0.03, 0.035, 0.04]\ngm_scale [0.7, 0.75, 0.8, 0.85, 0.9]\nkl_scale [0.01, 0.02, 0.03, 0.04, 0.05]\ngrad_length [100, 1000, 10000]\nTable 10: Grid search hyper-parameter bounds of\nPPLM.\nB Design of Reading Exercise Generation\nSystem\nB.1 Reading Passage Generation\nZero-Shot setting In the zero-shot setting, we\ninstructed ChatGPT to be a helpful learning as-\nsistant capable of generating high-quality read-\ning passages in the system prompt. We provided\npersonalized requirements within the conversation\nprompt, including length, genre, difficulty, and top-\nics. Reading passages for middle school students\ntypically consist of around 200 words. Their dif-\nficulty level ranges from A1 to B2 according to\nthe widely recognized CEFR standard, as middle\nschool students are generally beginners. As for\ntopics, teachers or students can freely select any\n5https://radimrehurek.com/gensim/\nmodels/word2vec.html\nsubject of interest using keywords, phrases, or sen-\ntences. ChatGPT’s remarkable ability enables it\nto comprehend these requirements and adhere to\nthem throughout the text-generation process.\nOne-Shot setting In addition to creating reading\npassages from scratch, teachers often source con-\ntent from the web or other materials and seek to\nadapt them into suitable reading passages for stu-\ndents. In the one-shot setting, we added an extra\nrequirement: a referenced passage. Teachers can\nsupply a referenced passage for ChatGPT, allowing\nthe model to learn language styles and structural\nfeatures. This setting facilitates more practical use\nof our system, though the added constraint may\nlimit the model’s flexibility and creativity.\nB.2 Exercise Question Generation\nWe also generate questions and corresponding an-\nswer options for middle school reading comprehen-\nsion exercises using appropriate prompts. Unlike\nthe Q&A generation task in the NLP field, Chi-\nnese middle school students are mostly practicing\nmultiple-choice selection questions. Few existing\nmodels focus on this task, and we have not identi-\nfied a comparable method as a baseline for multiple-\nchoice question generation. Given the high qual-\nity of ChatGPT-generated questions, we compare\nthem directly to human-written exercise questions.\nFor the prompt design, we input the number of\nquestions, the number of options per question, and\nthe question type for personalized customization.\nChatGPT can generate exercise questions based on\neither a passage it previously created or a passage\ninput by users. We did not set a difficulty level\nfor the questions, as there is no reliable measure-\nment standard. Nonetheless, question types can\nindirectly reflect difficulty. For example, logical\ninference questions are generally more challenging\nthan word interpretation questions.\nB.3 Toxicity Check\nTo ensure the safety of middle school students and\navoid ethical issues, we have implemented mea-\nsures to prevent the generation of toxic text. In\nour prompts, we explicitly specify that the gener-\nated content must not contain violence, racism, or\nother harmful elements for young language learn-\ners. While OpenAI has devoted considerable atten-\ntion to addressing toxicity concerns, and such texts\nare unlikely to appear in ChatGPT’s responses, we\nhave implemented an additional layer of security\n621\nby using Google’s toxicity score tool 6 to screen\nthe generated text. Exercises are made available to\nteachers and students only after passing the toxicity\ncheck.\nB.4 ChatGPT Prompts\nAn example of the manually crafted prompts for\nthe above tasks is presented in Table 11.\nC Subjective Feedback from Users\nThe evaluation and feedback from system users,\nthat is, experienced middle school teachers, are\nsummarized in Table 12.\nD Examples of Generated Exercises\nHere we present several examples of human-\nwritten, GPT-2-generated, and ChatGPT-generated\npassages in Table 13. An example of a comparison\nbetween human-designed exercise questions and\nsystem-generated questions is shown in Table 14.\nYou can also test our demo system to generate more\nreading comprehension exercises.\n6https://developers.\nperspectiveapi.com/s/\nabout-the-api-attributes-and-languages\n622\nPrompt\nPassage System You are a helpful assistant to generate reading comprehension materials for\nChinese middle school English learners. Your responses should not include\nany toxic content.\nConversation\n(zero-shot)\nPlease generate a passage (without a title) that is similar to the given example\nand satisfies the following requirements: Topics: {basketball competition};\nLength: no more than {200} words; Genre: {narrative}; CEFR level: {B1}\nConversation\n(one-shot)\nPlease generate a passage (without a title) that is similar to the given example\nand satisfies the following requirements: Topics: {basketball competition};\nLength: no more than {200} words; Genre: {narrative}; CEFR level: {B1};\nExample: {a referenced passage}\nQuestion System You are a helpful assistant to generate reading comprehension exercise ques-\ntions for Chinese middle school English learners. Your responses should not\ninclude any toxic content.\nConversation Please generate {5} multiple choice questions (each question with {4}\nchoices), the corresponding answers and explanations for the following read-\ning comprehension exercise. The type of questions should be {inference}\nquestions. Exercise: {input reading passage}\nTable 11: An example of the prompts for ChatGPT to generate high-quality reading comprehension exercises.\nEvaluations and Suggestions\nPassages Content /enc-33The generated passages are coherent in language.\n/enc-33The language characteristics are obvious and the quality of the generated\npassages is good.\nTopic /enc-33The function of \"generating based on the referenced passage\" can present\npassages of different genres on the same topic effectively.\n/enc-33The system can perfectly follow the requirements of the topic, difficulty\nlevel, and passage genre.\nExercises Questions /enc-33The generated questions are of good quality and are based on the main idea\nand details of the passages.\n/enc-33Before using the system, I thought the AI can only generate exercise\nquestions that are very simple and straightforward. Actually, the system can\ndo more than that. The generated questions are usually good enough to help\nstudents understand the passages and examine their language ability.\n/enc-37The types of generated questions are not rich enough. It is easy to find\ntheir patterns, such as many of them are \"What is something?\", \"What did\nsomeone do something?\", \"Why did someone do something?\", etc.\nOptions /enc-37The quality of the questions is good, but the options are not so perfect.\nSome answer options are inaccurate or repetitive.\n/enc-37The correct answers are always accurate, but the wrong answers are of low\nquality. Sometimes they are too easy for students and cannot play a role as\ndistractors.\nSystem\nUsefulness /enc-33The system is like a personalized resource library. Rich information can be\nprovided for teachers in daily teaching, which can further enhance teachers’\nability to optimize resources while organizing them, thus providing diverse\nand personalized educational resources to improve students’ English reading\nability.\nEase of Use /enc-33The system interface is simple and the features are easy to understand.\n/enc-33It is easy to use the system even for teachers who know nothing about AI.\nOverall Quality /enc-33I will rate the system 80/100. I am very satisfied with it.\n/enc-33This system is totally out of my expectation. I am happy such a powerful\nsystem will be applied in real-world education soon.\nTable 12: The evaluation of the system quality and the summarized feedback from experienced middle school\nteachers.\n623\nExample Passages\nHuman-Written Deep in the forest lives David, who loves books. As soon as he reads one book, he brings\nhome another. Soon the house is filled with books. His wife, Alice, is angry. You must do\nsomething! \" David thinks hard and has an idea. \"I can bring my books into the faraway\nhills and share what I have with the children. \" Every week, David sets off across the\ncountry-side to faraway villages with his two donkeys. Early on a sunny morning, David\nand his donkeys stop at a river to drink. Then, a bandit jumps out! \"Please let us pass, \"\nDavid says. \"The children are waiting. The bandit takes one book and shouts, \"Next time\nI want some money! \" They move on until at last, David sees some houses below. The\nchildren run to meet him. David reads them a story first. When the story ends, it’s time for\neveryone to choose a book. The children hold their books close as they say good-bye and\nrun home cheerfully. David and his donkeys head back, over and around the hills, and into\nthe sunset. At home, Alice feeds her hungry husband and the donkeys. But then, instead of\nsleeping, David picks up his book, and reads deep into the night. And far away in the hills,\ncandles and lanterns burn as the children read borrowed books deep into their night, too.\nFine-tuned GPT-2 One sunny afternoon, an old man sat down with his 25-year-old son on a bus. They were\ntalking about some interesting things. The old man began to show his son the things he\nsaw. The son asked, \"Dad, why does your father not allow me to take off his bus? \" The\nold man said, \"Well, when our bus got full, I asked my son to sit on the floor and I sat\ndown with him. He was very surprised. I asked him the other old man, \"What do you\nmean, son? \" The old man said, \"I am just going to the park to play a game. The game is\ncalled ’Musical Chairs’. As we were playing, my son asked me, ’Dad, why does your son\nallow me to sit on the bus? \"\"He doesn’t want to get off the bus! \" the son said with tears\nin his eyes. \"Well, it’s simple, but it’s a difficult game for me. Every time I get off the bus,\nI have to sit down. If I don’t get off, I will have to wait for the other driver to take me off. \"\nThe old man continued, \"Now my son is very calm. We are going to the park to play. Can\nyou play this game with me? \" With tears in his eyes, the young man said, \"Yes, I know!\nLook at my son. He was almost killed in a car accident two years ago. When my son got\nout of hospital, he had to wait for the other driver’s help to get him back on the bus. \"\nChatGPT (zero-shot) Sarah was feeling a bit anxious as she entered the gym for her first dance class. She had\nalways loved dancing, but had never taken a formal class before. As she looked around at\nthe other students stretching and chatting, Sarah wondered if she would fit in. The class\nbegan with some warm-up exercises, and Sarah started to relax as she found that she could\nkeep up with the others. They moved on to some basic steps, and Sarah felt her confidence\ngrowing with each new move. Before she knew it, the class was over, and Sarah couldn’t\nbelieve how quickly the time had flown by. As she left the gym, she couldn’t wait to\ncome back for the next class. Reflecting on her experience, Sarah realized that trying\nnew things could be scary, but it was important to take that first step. She also realized\nthat she was capable of more than she had given herself credit for, and that the feeling of\naccomplishment was worth the initial discomfort. From that day forward, Sarah made a\ncommitment to herself to try new things more often, and to approach each new experience\nwith an open mind and a willingness to learn.\nTable 13: Examples of human-written, GPT-2-generated, and ChatGPT-generated passages.\n624\nExample\nPassage Blue jackets, white shirts, and black pants... The school uniform looks so out of\nfashion. Like many of you, I don’t like it. However, if I were given a chance, I\nwouldn’t throw my school uniform away. Uniforms show that we are a part of\nan organization (our school). Wearing them shows we are all in this organization\ntogether. Some people argue that uniforms can improve learning. I completely agree\nwith them. Uniforms make the classroom a serious environment. We will find it\neasier to focus on our schoolwork. Most importantly, a uniform means we don’t\nhave to worry about peer pressure. When everyone is dressed the same, we stop\nworrying about our appearance. There is no competition for being dressed in the\nlatest fashion. This can save our parents a lot of money. In America, many schools\ndo not have uniforms. Meanwhile,160,000 children miss school every day, because\nof a fear of school bullying. This might not be directly connected with what the\nstudents are wearing. However, having a uniform can be safe for many students. It’s\nhard to make fun of what someone is wearing when everyone is dressed the same.\nNo one will feel left out because of what he is wearing. The school uniform gives\nus so much. They may not be fashionable, but that’s just why I think they should be\nhere to stay.\nHuman-Designed Ex-\nercise\n(1) The write doesn’t like the school uniform because\nA. it’s white.\nB. it’s blue.\nC. it’s out of fashion.\nD. it’s old.\n(2) What does wearing uniforms show for students?\nA. They’re too poor to buy their own clothes.\nB. They are a part of their school.\nC. They’re good children of their parents.\nD. They like to look the same.\n(3) Which of the following is NOT true according to the third paragraph?\nA. The writer thinks wearing uniforms can make him/her nicer.\nB. The writer thinks wearing uniforms can make his/her classroom more serious.\nC. The writer thinks wearing uniforms can improve his/her study.\nD. The writer thinks wearing uniforms means we don’t have to worry about peer\npressure.\n(4) The passage mainly tells us\nA. wearing uniforms is important\nB. students must wear uniforms\nC. some schools don’t have uniforms in America\nD. the uniforms are out of fashion\nChatGPT-Generated\nExercise\n(1) Why does the author wear their school uniform even though they do not like it?\nA. They do not have any other clothes to wear\nB. They believe it shows that they are part of their school community\nC. They think it is fashionable\nD. They want to fit in with their peers\n(2) What is one argument the author makes in favor of school uniforms?\nA. They can help students keep up with the latest fashion trends\nB. They can improve student learning\nC. They can be a source of peer pressure\nD. They can be expensive for parents to purchase\n(3) Why does the author suggest that school uniforms can be a good way to prevent\nbullying?\nA. Uniforms are fashionable and will make students popular\nB. Uniforms make it difficult to make fun of someone’s clothing\nC. Uniforms are expensive and will make students feel more important\nD. Uniforms create a sense of competition among students\n(4) How do schools in America compare to schools in other countries with regards\nto school uniforms?\nA. American schools do not have uniforms, unlike many other countries\nB. American schools have uniforms, but they are not required\nC. American schools have uniforms, and they are required\nD. American schools have uniforms, but only for certain grades or classes\nTable 14: An example of a comparison between human-designed exercise questions and system-generated questions.\n625",
  "topic": "Transformative learning",
  "concepts": [
    {
      "name": "Transformative learning",
      "score": 0.6707876920700073
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6704028248786926
    },
    {
      "name": "Computer science",
      "score": 0.6269333362579346
    },
    {
      "name": "Reading comprehension",
      "score": 0.5757340788841248
    },
    {
      "name": "Reading (process)",
      "score": 0.5542506575584412
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5425271987915039
    },
    {
      "name": "Comprehension",
      "score": 0.4868849813938141
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4356241226196289
    },
    {
      "name": "Mathematics education",
      "score": 0.43146276473999023
    },
    {
      "name": "Multimedia",
      "score": 0.41063258051872253
    },
    {
      "name": "Pedagogy",
      "score": 0.2792421579360962
    },
    {
      "name": "Psychology",
      "score": 0.26133865118026733
    },
    {
      "name": "Artificial intelligence",
      "score": 0.25186240673065186
    },
    {
      "name": "Political science",
      "score": 0.1206769347190857
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3019523068",
      "name": "Educational Research Institute",
      "country": "PL"
    }
  ],
  "cited_by": 85
}