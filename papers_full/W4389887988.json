{
  "title": "Long-Term Trajectory Prediction Model Based on Transformer",
  "url": "https://openalex.org/W4389887988",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097524791",
      "name": "Qiang Tong",
      "affiliations": [
        "Beijing Information Science & Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2105195222",
      "name": "Jinqing Hu",
      "affiliations": [
        "Beijing Information Science & Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2115301589",
      "name": "Yu-Li Chen",
      "affiliations": [
        "Beijing Information Science & Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2105215522",
      "name": "Dongdong Guo",
      "affiliations": [
        "Beijing Information Science & Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2308979159",
      "name": "Xiulei Liu",
      "affiliations": [
        "Beijing Information Science & Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2808204277",
    "https://openalex.org/W1965073335",
    "https://openalex.org/W4367609966",
    "https://openalex.org/W2885195348",
    "https://openalex.org/W2897997731",
    "https://openalex.org/W2963608065",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2894495610",
    "https://openalex.org/W3003760843",
    "https://openalex.org/W3000241400",
    "https://openalex.org/W2960590043",
    "https://openalex.org/W2977065288",
    "https://openalex.org/W6757825368",
    "https://openalex.org/W6810703010",
    "https://openalex.org/W3146366485",
    "https://openalex.org/W4220982586",
    "https://openalex.org/W4295925559",
    "https://openalex.org/W3197177318",
    "https://openalex.org/W6737778391",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6797155008",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W4243683928",
    "https://openalex.org/W6757116499",
    "https://openalex.org/W2901045356",
    "https://openalex.org/W3202586194",
    "https://openalex.org/W4289085255",
    "https://openalex.org/W3212890323",
    "https://openalex.org/W4308947955"
  ],
  "abstract": "Recurrent neural network models have problems such as memory loss and gradient disappearance when dealing with long time series data. This paper proposes a long-term trajectory prediction model based on Transformer to process long-term sequence information. Firstly, the position encoding is used to preserve the relative positional relationship between trajectory points. Secondly, the multi-head attention mechanism is used to fully learn the feature information between different trajectories, and the trajectory data can be encoded at one time. Finally, the encoder and decoder mechanism is used to predict future trajectory data. Compared with the long-term trajectory prediction benchmark method TrajAirNet, the average displacement error, absolute displacement error of the proposed model on the long-term trajectory dataset are reduced by about 8.2&#x0025; and 51.4&#x0025;, respectively. The experimental results show that the proposed model has higher accuracy and robustness on long-term trajectory prediction dataset.",
  "full_text": null,
  "topic": "Term (time)",
  "concepts": [
    {
      "name": "Term (time)",
      "score": 0.576328456401825
    },
    {
      "name": "Computer science",
      "score": 0.5629246830940247
    },
    {
      "name": "Trajectory",
      "score": 0.5341479182243347
    },
    {
      "name": "Transformer",
      "score": 0.4103674292564392
    },
    {
      "name": "Voltage",
      "score": 0.10075190663337708
    },
    {
      "name": "Engineering",
      "score": 0.09580233693122864
    },
    {
      "name": "Electrical engineering",
      "score": 0.07669660449028015
    },
    {
      "name": "Physics",
      "score": 0.07324284315109253
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Astronomy",
      "score": 0.0
    }
  ]
}