{
  "title": "Large Language Models and Medical Knowledge Grounding for Diagnosis Prediction",
  "url": "https://openalex.org/W4389040134",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2144316405",
      "name": "Yanjun Gao",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2095671731",
      "name": "Ruizhe Li",
      "affiliations": [
        "University of Aberdeen"
      ]
    },
    {
      "id": "https://openalex.org/A5093345669",
      "name": "Emma Croxford",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A5003762551",
      "name": "Samuel Tesch",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2099568306",
      "name": "Daniel To",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2132341066",
      "name": "John Caskey",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2429196096",
      "name": "Brian W. Patterson",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A896309055",
      "name": "Matthew M. Churpek",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2096515443",
      "name": "Timothy Miller",
      "affiliations": [
        "Boston Children's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2625526828",
      "name": "Dmitriy Dligach",
      "affiliations": [
        "Loyola University Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2102230835",
      "name": "Majid Afshar",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2144316405",
      "name": "Yanjun Gao",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2095671731",
      "name": "Ruizhe Li",
      "affiliations": [
        "University of Aberdeen"
      ]
    },
    {
      "id": "https://openalex.org/A5093345669",
      "name": "Emma Croxford",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A5003762551",
      "name": "Samuel Tesch",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2099568306",
      "name": "Daniel To",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2132341066",
      "name": "John Caskey",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2429196096",
      "name": "Brian W. Patterson",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A896309055",
      "name": "Matthew M. Churpek",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2096515443",
      "name": "Timothy Miller",
      "affiliations": [
        "Boston Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2625526828",
      "name": "Dmitriy Dligach",
      "affiliations": [
        "Loyola University Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2102230835",
      "name": "Majid Afshar",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4378770578",
    "https://openalex.org/W4323651065",
    "https://openalex.org/W4385567033",
    "https://openalex.org/W4387800173",
    "https://openalex.org/W2168490582",
    "https://openalex.org/W4322618218",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2079609562",
    "https://openalex.org/W4386002582",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2158506930",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W2998856543",
    "https://openalex.org/W4380136714",
    "https://openalex.org/W4310997991",
    "https://openalex.org/W3098266846",
    "https://openalex.org/W3104725668",
    "https://openalex.org/W2951103577",
    "https://openalex.org/W4386757283",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4379539983",
    "https://openalex.org/W4286567174",
    "https://openalex.org/W4380136742",
    "https://openalex.org/W3155578984",
    "https://openalex.org/W4225845990",
    "https://openalex.org/W4285249816",
    "https://openalex.org/W4380993239",
    "https://openalex.org/W1970417391",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2735585131",
    "https://openalex.org/W3186100284",
    "https://openalex.org/W2146089916",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W2958894272",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4385436582",
    "https://openalex.org/W3175944005",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W3177314227",
    "https://openalex.org/W4307003748",
    "https://openalex.org/W4387560733",
    "https://openalex.org/W2302501749",
    "https://openalex.org/W4385572365",
    "https://openalex.org/W3170450419",
    "https://openalex.org/W4389520255",
    "https://openalex.org/W4385571070",
    "https://openalex.org/W4385571510"
  ],
  "abstract": "Abstract While Large Language Models (LLMs) have showcased their potential in diverse language tasks, their application in the healthcare arena needs to ensure the minimization of diagnostic errors and the prevention of patient harm. A Medical Knowledge Graph (KG) houses a wealth of structured medical concept relations sourced from authoritative references, such as UMLS, making it a valuable resource to ground LLMs’ diagnostic process in knowledge. In this paper, we examine the synergistic potential of LLMs and medical KG in predicting diagnoses given electronic health records (EHR), under the framework of Retrieval-augmented generation (RAG). We proposed a novel graph model: D r .K nows , that selects the most relevant pathology knowledge paths based on the medical problem descriptions. In order to evaluate D r .K nows , we developed the first comprehensive human evaluation approach to assess the performance of LLMs for diagnosis prediction and examine the rationale behind their decision-making processes, aimed at improving diagnostic safety. Using real-world hospital datasets, our study serves to enrich the discourse on the role of medical KGs in grounding medical knowledge into LLMs, revealing both challenges and opportunities in harnessing external knowledge for explainable diagnostic pathway and the realization of AI-augmented diagnostic decision support systems.",
  "full_text": "Large Language Models and Medical Knowledge Grounding for\nDiagnosis Prediction\nYanjun Gao1⇤, Ruizhe Li2, Emma Croxford1, Samuel Tesch1, Daniel To1,\nJohn Caskey1, Brian W. Patterson1, Matthew M. Churpek1,\nTimothy Miller4, Dmitriy Dligach3, and Majid Afshar1\n1 Department of Medicine, School of Medicine and Public Health,\nUniversity of Wisconsin Madison\n2 Department of Computing Science, University of Aberdeen\n3 Department of Computer Science, Loyola University Chicago\n4 Boston Children’s Hospital and Harvard Medical School\n1{ygao, jcaskey, bpatter, mchurpek, mafshar}@medicine.wisc.edu\n1{croxford, sgtesch, dcto}@wisc.edu\n2 ruizhe.li@abdn.ac.uk\n3 ddligach@luc.edu\n4 Timothy.Miller@childrens.harvard.edu\nAbstract\nWhile Large Language Models (LLMs) have showcased their potential in diverse language tasks, their\napplication in the healthcare arena needs to ensure the minimization of diagnostic errors and the prevention\nof patient harm. A Medical Knowledge Graph (KG) houses a wealth of structured medical concept\nrelations sourced from authoritative references, such as UMLS, making it a valuable resource to ground\nLLMs’ diagnostic process in knowledge. In this paper, we examine the synergistic potential of LLMs\nand medical KG in predicting diagnoses given electronic health records (EHR), under the framework\nof Retrieval-augmented generation (RAG). We proposed a novel graph model:DR.KNOWS , that selects\nthe most relevant pathology knowledge paths based on the medical problem descriptions. In order to\nevaluate DR.KNOWS , we developed the ﬁrst comprehensive human evaluation approach to assess the\nperformance of LLMs for diagnosis prediction and examine the rationale behind their decision-making\nprocesses, aimed at improving diagnostic safety. Using real-world hospital datasets, our study serves to\nenrich the discourse on the role of medical KGs in grounding medical knowledge into LLMs, revealing\nboth challenges and opportunities in harnessing external knowledge for explainable diagnostic pathway\nand the realization of AI-augmented diagnostic decision support systems.\n1 Introduction\n1 The ubiquitous use of Electronic Health Records (EHRs) and the standard documentation practice of\ndaily care notes are integral to the continuity of patient care by providing a comprehensive account of\nthe patient’s health trajectory, inclusive of condition status, diagnoses, and treatment plans (Brown et al.,\n2014). Yet, the ever-increasing complexity and verbosity of EHR narratives, often laden with redundant\ninformation, presents the risk of cognitive overload for healthcare providers, potentially culminating in\ndiagnostic inaccuracies (Rule et al., 2021; Liu et al., 2022; Nijor et al., 2022; Furlow, 2020). Physicians\n1*Corresponding author: ygao@medicine.wisc.edu\n1\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nGao et. al, 2023, Under Review\noften skip sections of lengthy and repetitive notes and rely on decisional shortcuts (i.e. decisional\nheuristics) that contribute to diagnostic errors (Croskerry, 2005).\nCurrent efforts at automating diagnosis generation from daily progress notes leverage language models.\nGao et al. (2022) introduced a summarization task that takes progress notes as input and generates a\nsummary of active diagnoses. They annotated a set of progress notes from the publicly available EHR\ndataset called Medical Information Mart for Intensive Care III (MIMIC -III ) (Johnson et al., 2016). The\nBioNLP 2023 Shared Task, known asPROB SUM, built upon this work by providing additional annotated\nnotes and attracted multiple efforts focused on developing solutions (Gao et al., 2023; Manakul et al.,\n2023; Li et al., 2023). These prior studies utilize language models like T5 (Raffel et al., 2020) and\nGPT (Floridi and Chiriatti, 2020), demonstrating a growing interest in applying generative large language\nmodels (LLMs) to serve as solutions. Unlike the conventional language tasks where LLMs have shown\npromising abilities, automated diagnosis generation is a critical task that requires high accuracy and\nreliability to ensure patient safety and optimize healthcare outcomes. Concerns regarding the potential\nmisleading and hallucinated information that could result in life-threatening events prevent them from\nbeing utilized for diagnosis prediction (Baumgartner, 2023).\nOne of the solutions to improve factual accuracy is to utilize a knowledge graph to retrieve relevant\nknowledge to guide the LLMs with better instruction (Pan et al., 2023). In the biomedical domain, the\nUniﬁed Medical Language System (UMLS) (Bodenreider, 2004), a comprehensive resource developed\nby the National Library of Medicine in the United States, has been extensively used in NLP research. It\nserves as the leading medical knowledge source, facilitating the integration and retrieval of biomedical\ninformation. The UMLS offers concept vocabulary and semantic relationships, enabling the construction\nof medical knowledge graphs. Prior studies have leveraged UMLS knowledge graphs for tasks such as\ninformation extraction (Huang et al., 2020; Lu et al., 2021; Aracena et al., 2022; He et al., 2020), and\nquestion-answering (Lu et al., 2021). However, UMLS knowledge graphs have not been applied to the\ntask of diagnosis prediction.\nMining relevant knowledge for diagnosis is particularly challenging for two reasons: the highly\nspeciﬁc factors related to the patient’s complaints, histories, and symptoms in EHR, and the vast search\nspace within a knowledge graph containing 4.5 million concepts and 15 million relations for diagnosis\ndetermination. While utilizing a multi-hop reasoning mechanism for disease pathology discovery via\nthe UMLS knowledge graph aligns with the need for extensive medical knowledge in diagnostics,\nimplementing this approach is hampered by its computational complexity. Speciﬁcally, the number of\nconcepts in the UMLS knowledge graph reachable within one hop ranges from 2 to 33k, with a median of\n368. The number of two-hop paths may exhibit exponential growth due to the UMLS knowledge graph’s\nhigh connectivity. Therefore, addressing the computational complexity of multi-hop reasoning within the\nextensive UMLS knowledge graphs is crucial for effective knowledge mining in medical diagnostics.\nIn this study, we explore using knowledge graphs as an external module to ground LLM’s diagnostic\nprocess in medical knowledge and take the initial step of building a graph model to discover relevant paths\nusing the UMLS. We proposeDR.KNOWS (Diagnostic Reasoning Knowledge Graphs), that retrieves\ntop N case-speciﬁc knowledge paths about the pathology of diseases through a multi-hop mechanism,\novercoming the difﬁculties of retrieving and selecting paths from the entire knowledge graph. We then\nadapt the predicted paths into a graph-prompting method for LLMs. We utilized ChatGPT-3.5-turbo for\nour experiments on knowledge grounding since it represents the cutting-edge in language models and has\nbeen frequently examined as a diagnostic instrument in earlier research (Kuroiwa et al., 2023; Caruccio\net al., 2024).\nGoing beyond the technical aspects of constructing knowledge graphs, our work also focuses on the\nprecise evaluation of LLMs, motivated by the need of improving diagnostic performance and ensuring\ndiagnostic safety (Balogh et al., 2015; Donaldson et al., 2000). Existing evaluation metrics for LLM\noutput are insufﬁcient for evaluating diagnostic accuracies, where precise performance is necessary to\nensure diagnostic safety. We focus on an evaluation framework that can identify the diagnostic errors\nand the root cause, and assess the self-explanatory aspects of LLMs’ diagnostic processes. We designed\nthe ﬁrst human evaluation survey, following the SaferDX instrument, an organizational self-assessment\n2\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nFigure 1: Study overview: we focused on generating diagnoses (red font text in the Plan section) using the\nSOAP-Format Progress Note with the aid of large language models (LLM). The input consists of Subjective,\nObjective and Assessment sections (the dotted line box on the example progress note), and diagnoses in the Plan\nsections are the ground truth. We introduced an innovative knowledge graph model, namelyDR.KNOWS , that\nidentiﬁes and extracts the most relevant knowledge trajectories from the UMLS Knowledge Graph. The nodes for\nthe UMLS knowledge graph are Concept Unique Identiﬁers (CUIs) and edges are the semantic relations among\nCUIs. We experimented with prompting ChatGPT for diagnosis generation, with and withoutDR.KNOWS predicted\nknowledge paths. Furthermore, we investigated how this knowledge grounding inﬂuences the diagnostic output of\nLLMs using human evaluation. Text with underlines are the UMLS concepts identiﬁed through a concept extractor.\nFigure 2:Inferring possible diagnoses within 2-hops from a UMLS knowledge graph given a patient’s medical\ndescription. We highlight the UMLS medical concept in the color boxes (“female\", “sepsis\", etc). Each concept has\nits own subgraph, where concepts are the vertices, and semantic relations are the edges (for space constraint, we\nneglect the subgraph for “female\" in this graph presentation). On the ﬁrst hop, we could identify the most relevant\nneighbor concepts to the input description. The darker color the vertices are, the more relevant they are to the\ninput description. A second hop could be further performed based on the most relevant nodes, and reach the ﬁnal\ndiagnoses “Pneumonia and inﬂuenza\" and “Respiratory Distress Syndrome\". Note that we use the preferred text of\nConcept Unique Identiﬁers (CUIs) for presentation purposes. The actual UMLS KG is built on CUIs rather than\npreferred text.\n3\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\ntool with recommended practices aimed at improving diagnostic safety (Singh et al., 2019), for LLMs\ndiagnosis prediction. The survey also incorporates the latest evaluation criteria for LLM, including factual\naccuracy, hallucination, quality of evidence, and other relevant aspects, identiﬁed from previous work\nin the ﬁeld of biomedical NLP (Otmakhova et al., 2022; Singhal et al., 2023; Moramarco et al., 2021;\nAdams et al., 2021). Our aim is to bridge the gap between comprehensive diagnostic evaluation for safety\nand the capabilities of advanced language models, facilitating a deeper understanding of their diagnostic\nperformance, and paving the way for safe LLM-augmented diagnostic decision support.\nOur work and contribution are structured into four primary components:\n1. designing and evaluatingDR.KNOWS , a graph-based model that selects the top N probable diagnoses\nwith explainable paths (§4);\n2. designing and implementing the ﬁrst human evaluation framework for LLMs diagnosis generation\nand reasoning (§4.3),\n3. revealing the usefulness ofDR.KNOWS as an additional module to augment LLMs in generating\nrelevant diagnoses, a ﬁrst iteration of integrating knowledge graphs for graph prompting (§2.3),\n4. demonstrating the utilities of our proposed human evaluation framework that reveals LLM’s\ndiagnostic performance with critical aspects of ensuring diagnostic safety (§2.3).\nOur research poses a new problem that has not been addressed in the realm of NLP for diagnosis\ngeneration - harnessing the power of knowledge graphs for the controllability and explainability of LLMs.\nThe following key ﬁndings will inform future work on developing knowledge graph-based methods for\nLLMs for diagnostic prediction:\nStrong Diagnostic Performance: Using the proposed human evaluation framework, ChatGPT\ndemonstrated robust diagnostic accuracy with a median score of 66%, supported by exceptional self-\nexplanation capabilities (“Reasoning” median score > 94%), underscoring its potential as a clinical\ndiagnostic decision-support tool.\nKnowledge Graph’s Impact on Abstraction and Correct Reasoning: Integrating knowledge graphs\ninto ChatGPT had a notable impact on ﬁnding the correct medical concepts, enhancing the model’s\nability to generate abstractive diagnoses and improving reasoning (RATIONALE sub-category in human\nevaluation) by guiding the diagnostic process with relevant knowledge paths.\nFuture Knowledge Graph Model Enhancements: Analysis ofDR.KNOWS highlighted limitations\nin cases with unrelated pathways. Addressing these challenges through improving clinical narrative\nembedding, as well as improving the design ofDR.KNOWS with other components like Bayesian network,\nmight enhance the diagnostic potential of KG-based models in the future.\nUtility of Our Proposed Human Evaluation for LLM:While the overall diagnostic accuracy and\nreasoning scores show the output with and without knowledge paths in the input has no differences, the\nbroken-down scores present the strengths and weaknesses of different models. The granular approach of\nevaluation enables a more informed analysis of LLMs for particular applications and contributes to the\niterative process of model reﬁnement. The scoring aspects ofPLAUSIBILITY , OMISSION , SPECIFICITY ,\nRATIONALE address various critical facets of AI interpretability and decision-making quality, aiming\nat mitigating the risks and enhancing the reliability and safety of diagnostics provided by AI systems.\nWe provide the full guidelines of human evaluation in Supplementary Material and hope to contribute to\nfacilitating the development of safe AI diagnostic tools.\nFigure 1 presents the study overview of this work. We studied summarizing diagnoses from daily\nprogress notes written in the SOAP-format. We developed a novel graph model,DR.KNOWS , that\nidentiﬁes and retrieves relevant knowledge paths from UMLS KG.DR.KNOWS is available in two\nversions: TriAttnw, which employs trilinear attention to determine the relevance scores for each knowledge\npath, andMultiAttnw, which utilizes a multi-head attention mechanism to score and select knowledge\npaths. Our initial evaluation ofDR.KNOWS focuses on its capability to identify and predict Concept\nUnique Identiﬁers (CUIs) for diagnoses, speciﬁcally addressing theCUI prediction task. Subsequently,\n4\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nwe explored how these additional knowledge pathways could be harnessed to enhance ChatGPT’s ability\nto summarize diagnoses derived from daily progress notes. To achieve this, we integrated the knowledge\npathways predicted byDR.KNOWS into a prompting framework for ChatGPT. Additionally, we presented\nthe performance difference between the zero-shot and few-shot settings.\nWe summarized the deployment of evaluation metrics in Table 2. On CUI prediction task, we\nreported CUI-based Recall, Precision and F-score. The metrics helped us understand if theDR.KNOWS\ncould accurately identify CUIs that are the ﬁnal diagnoses. On the results obtained through ChatGPT,\nwe ﬁrst applied automated metrics including CUI-based Recall, Precision, F-score, and two ROUGE\nvariants (ROUGE-2 and ROUGE-L Lin (2004)). Then we asked two medical professionals to conduct\na human evaluation using our proposed framework under the supervision of two senior physicians. By\nexamining the effects of graph prompting on LLMs with real-world EHR data, we strive to contribute to\nan explainable AI diagnostic pathway.\n2 Results\n2.1 Data overview\nWe used two sets of progress notes from different clinical settings in this study:MIMIC -III and IN-HOUSE\nEHR datasets. MIMIC -III is one of the largest publicly available databases that contains de-identiﬁed\nhealth data from patients admitted to intensive care units (ICUs), developed by the Massachusetts Institute\nof Technology and Beth Israel Deaconess Medical Center (BIDMC).MIMIC -III includes data from\nover 38,000 patients admitted to ICUs at the BIDMC between 2001 and 2012. The second set, namely\nthe IN-HOUSE EHR data, was a subset of EHRs including adult patients (ages > 18) admitted to the\nUnivesity of Wisconsin Health System between 2008 to 2021. In contrast to theMIMIC subset, theIN-\nHOUSE set covered progress notes from all hospital settings, including Emergency Department, General\nMedicine Wards, Subspecialty Wards, etc. While the two datasets originated from separate hospitals\nand departmental settings and might reﬂect distinct note-taking practices, they both followed the SOAP\ndocumentation format for progress notes.\nGao et al. (2022, 2023) introduced a subset of 1005 progress notes fromMIMIC -III with active\ndiagnoses annotated from the Plan sections. Therefore, we applied this dataset for training and evaluation\nfor both graph model intrinsic evaluation (§2.2) and diagnosis summarization (§2.3). TheIN-HOUSE\ndataset did not contain human annotation. Still, by parsing the text with a medical concept extractor that\nwas based on UMLS SNOMED-CT vocabulary, we were able to pull out concepts that belonged to the\nsemantic type ofT047 DISEASE AND SYNDROMES . We deployed this set of concepts as the ground\ntruth data to train and evaluate the graph model in §2.2. The ﬁnal set ofIN-HOUSE data contained 4815\nprogress notes. We presented the descriptive statistics in Table 1. When contrasting withMIMIC -III , the\nIN-HOUSE dataset exhibited a greater number of CUIs in its input, leading to an extended CUI output.\nAdditionally, MIMIC -III encompassed a wider range of abstractive concepts compared to the progress\nnotes ofIN-HOUSE . Example Plan sections from the two datasets are in the Appendix A.\nDataset Dept. #Input #Output % Abstractive\nCUIs CUIs Concepts\nMIMIC -III ICU 15.95 3.51 48.92%\nIN-HOUSE All 41.43 5.81 <1%\nTable 1:Average number of unique Concept Unique Identiﬁers (CUI) in the input and output on the two EHR\ndataset: MIMIC -III and IN-HOUSE . Abstractive concepts are those not found in the input, but present in the gold\nstandard diagnoses.\nGiven that our work encompasses a public EHR dataset (MIMIC -III ) and a private EHR dataset\nwith protected health information (IN-HOUSE ), we conducted training using three distinct computing\nenvironments. Speciﬁcally, most of the experiments onMIMIC -III were done on Google Cloud Computing\n(GCP), utilizing 1-2 NVIDIA A100 40GB GPUs, and a conventional server equipped with 1 RTX 3090 Ti\n5\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nEvaluation metrics Description Tasks\nCUI-based Recall, An automated metric measuring the set overlap of CUI prediction (§2.2);\nPrecision, F-score predicted CUIs and gold CUIs. Formulas are described Diagnosis prediction(§2.3)\nin §4.\nROUGE-2, ROUGE-L Automated metrics measuring the overlap of the Diagnosis prediction(§2.3)\nLin (2004) bigrams (ROUGE-2) or longest common sub-sequences\n(ROUGE-L) between predicted text and reference text.\nHuman evaluation Medical professionals evaluating ChatGPT’s diagnosis Diagnosis prediction(§2.3)\noutput and reasoning output. Sub-categories include\naccuracy, omission, abstraction, plausibility, speciﬁcity\nand others. Details can be found at §4.3.\nTable 2:Overview of the evaluation for the tasks. Note that for CUI-based evaluation, the text is ﬁrst converted to a\nset of UMLS CUIs using a concept extractor.\n24GB GPU. TheIN-HOUSE EHR dataset is stored on a workstation located within a hospital research\nlab. The workstation operates within a HIPAA-compliant network, ensuring the conﬁdentiality, integrity,\nand availability of electronic protected health information (ePHI), and is equipped with a single NVIDIA\nV100 32GB GPU. To use ChatGPT, we utilized an in-house ChatGPT-3.5-turbo version hosted on our\nlocal cloud infrastructure. This setup ensures that no data is transmitted to OpenAI or external websites,\nand we are in strict compliance with the MIMIC data usage agreement.\n2.2 Evaluation of DR.KNOWS on Predicting Diagnoses\nModel MIMIC IN-HOUSE\nTop N Recall Precision F-Score Top N Recall Precision F-Score\nConcept Ex. - 56.91 13.59 21.13 - 90.11 12.38 20.09\n(95% CI) 55.62, 58.18 12.32, 14.88 19.85, 22.41 88.84, 91.37 11.09, 13.66 18.81, 21.37\nMultiAttnW 4 26.91 22.79 23.10 6 24.68 15.82 17.69\n(95% CI) 25.64, 28.19 21.51, 24.06 21.83, 24.39 23.35, 25.91 14.55, 17.10 16.40, 18.96\n6 29.14 16.73 19.94 8 28.69 15.82 17.33\n27.85, 30.41 15.46, 18.00 18.66, 21.22 27.43, 29.98 14.55, 17.11 16.06, 18.60\nTriAttnW 4 29.85 17.61 20.93 6 34.00 22.88 23.39\n(95% CI) 26.23, 33.45 16.33, 18.89 19.67, 22.21 31.04,36.97 20.92, 24.85 21.71, 25.06\n6 37.06 19.10 25.20 8 44.58 22.43 25.70\n35.80, 38.33 17.82, 20.37 23.93, 26.48 41.38, 47.78 20.62, 24.23 24.06, 27.37\nTable 3:Performance comparison between concept extraction (Concept Ex.) and twoDR.KNOWS variants on\ntarget CUI prediction usingMIMIC and IN-HOUSE dataset.\nWe comparedDR.KNOWS with QuickUMLS (Soldaini and Goharian, 2016), which is a concept\nextraction baseline that identiﬁed the medical concepts from raw text. We took input text, parsed it with\nthe QuickUMLS and outputted a list of concepts. Table 3 provided results on the two EHR datasetsMIMIC\nand IN-HOUSE . The selection of different topN values was determined by the disparity in length between\nthe two datasets (see App. A).DR.KNOWS demonstrated superior precision and F-score across both\ndatasets compared to the baseline, with precision scores of 19.10 (95% CI: 17.82 - 20.37) versus 13.59\n(95% CI: 12.32 - 14.88) onMIMIC , and 22.88 (95% CI: 20.92 - 24.85) versus 12.38 (95% CI: 11.09 -\n13.66) on the in-house dataset. Additionally, its F-scores of 25.20 on MIMIC and 25.70 on the in-house\ndataset exceeded the comparison scores of 21.13 (95% CI: 19.85 - 22.41) and 20.09 (95% CI: 18.81 -\n21.37), respectively, underscoring its effectiveness in accurately predicting diagnostic CUIs. The TriAttnw\nvariant of Dr. Knows consistently outperformed the MultiAttnw variant on both datasets, with F-scores\nof 25.20 (95% CI: 23.93 - 26.48) versus 23.10 (95% CI: 21.83 - 24.39) onMIMIC and 25.70 (95% CI:\n6\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\n24.06 - 27.37) versus 17.69 (95% CI: 16.40 - 18.96) onIN-HOUSE . The concept extractor baseline reached\nthe highest recall, with 56.91 onMIMIC and 90.11 onIN-HOUSE , as it found all the input concepts that\noverlapped with the reference CUIs, in particular on theIN-HOUSE dataset that was largely an extractive\ndataset (App. A).\n2.3 Prompting Large Language Models for Diagnosis Generation\nModel ROUGE 2 ROUGE L CUI Recall CUI Precision CUI F-Score\nPrompt-based Zero-shot\nChatGPT 7.05 19.77 23.68 15.52 16.04\n(95% CI) 6.54, 7.56 19.26, 20.28 23.18, 24.19 15.00, 16.02 15.53, 16.55\n+Path 5.70 15.49 25.33 17.05 18.21\n(95% CI) 5.19, 6.21 14.98, 15.99 24.82, 25.84 16.29, 17.81 17.46, 18.98\nPrompt-based Few-shot\nChatGPT 3-shot 9.63 21.84 22.71 19.57 21.02\n(95% CI) 8.32,10.06 19.99,22.09 20.99,23.96 17.23, 19.78 20.26, 21.79\n5-shot 9.73 21.23 22.45 19.67 20.96\n(95% CI) 8.52, 10.18 19.58, 21.72 20.93, 23.80 17.66, 20.33 20.19, 21.73\n3-shot+Path 10.66 24.32 26.48 24.22 25.30\n(95% CI) 9.17, 10.72 22.44, 24.25 25.33, 28.36 21.44, 24.21 24.52, 26.06\n5-shot+Path 11.73 25.43 27.76 24.56 26.02\n(95% CI) 10.51, 12.25 23.53, 25.35 26.56, 29.39 22.47, 25.12 25.25, 26.78\nTable 4:Best performance onMIMIC test set (with annotated active diagnoses) from ChatGPT across all prompt\nstyles with (+Path) and withoutDR.KNOWS path prompting. We report ROUGE-2, ROUGE-L, CUI Recall,\nPrecision and F-score to illustrate the performance difference better. We useteal color to highlight the 95%\nconﬁdence interval (CIs) when there is a distinct CIs for the +Path compared to no path scenarios.\nResults reported in automated metricsShifting from a zero-shot to a few-shot learning scenario\nresulted in a clear boost in performance. The few-shot’s minimum ROUGE-2 score of 9.63 (95% CI: 8.32\n- 10.06), surpassed the zero-shot’s maximum of 7.05 (95% CI: 6.54 - 7.56), and the few-shot’s minimum\nCUI-F score of 20.96 (95% CI: 20.19 - 21.73) outperformed zero-shot’s score of 18.21 (95% CI: 17.46. -\n18.98).\nThe performance comparison between ChatGPT withDR.KNOWS in the predicted paths scenario\nversus the no paths scenario provided additional improvement in the few-shot setting. Notably, in the\n3-shot scenario, the +Path yielded a ROUGE-L score of 24.32 (95% CI: 22.44 - 24.25) versus 21.84 (95%\nCI: 19.99 - 22.09), and a CUI-F score of 25.30 (95% CI: 24.52 - 26.06) versus 21.02 (95% CI: 20.26 -\n21.79) from the no path scenarios. In the 5-shot setting, the +Path conﬁguration outperformed the no\npath setting across all metrics (Table 4). The ROUGE-2 score was 11.73 (95% CI: 10.51 - 12.25), and\nexceeded the no path score of 9.73 (95% CI: 8.52 - 10.18). ROUGE-L scores were also higher at 25.43\n(95% CI: 23.53 - 25.35) compared to 21.23 (95% CI: 19.58 - 21.71).\nResults from human evaluationHuman evaluation was performed on few-shot ChatGPT with and\nwithout KG, using 38.88% samples of the test set (n=92). Figure 3 shows the diagnosis scores and\nreasoning scores from ChatGPT with and withoutDR.KNOWS . Both models achieved diagnostic accuracy\nwith a median score surpassing 0.66 (IQR: 0.57-0.74 for with knowledge graph (KG); IQR 0.54-0.75 for\nno KG), and their reasoning scores exhibited a median exceeding 0.90 (IQR: 0.86-0.97 for KG; 0.90-0.97\nfor no KG). In contrast to the automated metrics, human evaluation indicated that the presence or absence\nof DR.KNOWS did not yield an overall difference in performance (p=0.63); however, several subgroup\ncomponents were different.\nFigure 4 describes all components of the diagnosis scores, considering six distinct scoring aspects.\nChatGPT models with and without KG paths exhibited similar performance inACCURACY , OMISSION ,\n7\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nFigure 3: Overall performance for ChatGPT models with the absence (“No KG\") and the presence of\nDR.KNOWS (“KG”).\nFigure 4: Diagnosis scores for ChatGPT models with the absence (“No KG\") and the presence of\nDR.KNOWS (“KG”).\n8\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nFigure 5: Reasoning scores for ChatGPT models with the absence (“No KG\") and the presence of\nDr.Knows (“KG”).\nUNCERTAINTY , PLAUSIBILITY , andSPECIFICITY . Notably, both models excelled in terms ofACCURACY ,\nconsistently providing about 80% afﬁrmative answers (\"Yes\") to the question of whether the output meets\nthe criteria for an ofﬁcial diagnosis. In contrast, their performance inABSTRACTION ranged from 13%\n(\"KG\") to 18% (\"No KG\"). On omitted diagnoses, approximately 14% to 15% stemmed from aleatoric\nuncertainty. This uncertainty contributed to about 18% of cases for “majority aleatoric\" and 33% for “all\naleatoric\" scenarios for both models. Lastly, concerning the level ofABSTRACTION , ChatGPT with KG\ndid not favor more extractive diagnoses than ChatGPT without KG, scoring 87% compared to 81% for\n\"No\" answers (p=0.09).\nIn Figure 5, when examining the reasoning scores, there was no signiﬁcant increase in omission, with\n16% observed with KG, as opposed to 10% without KG (p=0.16). When it comes toRATIONALE (correct\nreasoning), ChatGPT with KG exhibits a 55% strong agreement with humans, while ChatGPT “No KG”\ndemonstrates 50% strong agreement (p<0.01). On theABSTRACTION category asking about the presence\nof abstraction in model output, there was a notable drop from 88% (\"No KG\") to 78% (\"KG\") in the\nafﬁrmative responses (p=0.03), indicating less abstraction required with KG paths. Differences were also\nnoted inEFFECTIVE ABSTRACTION in favor of the KG paths (p<0.01).\nError analysis We discovered two primary types of error inDR.KNOWS output that could result in\nmissed opportunities for improving knowledge grounding. Figure 6 presents an example where the\nChatGPT did not ﬁnd the provided knowledge paths useful. In this case, the majority of the provided\nknowledge paths were highly extractive (“leukocytosis” “reticular dysgenesis”“paraplegia” are target\nconcepts the knowledge paths led to and all have “self” relationship). On the abstraction paths the target\nconcepts “abdomen hernia scrotal” and “chronic neutrophilia” were found, which were not relevant to the\ninput patient condition.\nAnother error observed occurred whenDR.KNOWS selected the source CUIs that were less likely\nto generate pertinent pathways for clinical diagnoses, resulting in ineffective knowledge paths. Figure 7\nshows a retrieved path from “Consulting with (procedure)” to “Consultation-action (qualiﬁer value)”.\n9\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nFigure 6: An error example ofDR.KNOWS retrieved knowledge pathways.DR.KNOWS ﬁnds two paths\nleading to irrelevant and misleading diagnosis, marked as red fonts. The symbol represents a self-loop.\nFigure 7: An example from ChatGPT withDR.KNOWS extracted knowledge pathways. Two paths had\nsource CUIs (“Consulting with (procedure), Drug Allergy”) that were less likely to generate pertinent\npaths for clinical diagnoses. Note that the path of “Drug allergy” led to a path contradicting to the “No\nKnown Drug Allergies” description in the input. The path of “cirrhosis of liver” was a correct diagnosis,\nbut ChatGPT failed to include it.\n10\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nFigure 8: Illustration of ChatGPT with KG paths selected byDR.KNOWS , introducing abstraction\nconcepts and enhancing predicted diagnosis plausibility. Human annotators ﬁnd the predicted diagnosis\nfrom ChatGPT, with knowledge paths, more plausible compared to the one without.\nFigure 9: The “Reasoning” output of ChatGPT with and without knowledge paths on the same input\nexample as in Fig 8. The highlighted green text demonstrates the utilization ofDR.KNOWS knowledge\npaths to enhance ChatGPT’s reasoning capabilities. Human evaluators ﬁnd that the ChatGPT with KG\nproduces correct reasoning.\n11\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nAlthough some procedure-related concepts like endoscopy or blood testing were valuable for clinical\ndiagnosis, this speciﬁc path of consulting did not contribute meaningfully to the input case. Similarly,\nanother erroneous pathway began with “Drug Allergies\" and led to \"Allergy to dimetindene (ﬁnding)”,\nwhich is contradictory given that the input note explicitly states “No Known Drug Allergies”. While\nthe consulting path’s issue was its lack of utility, the “Drug Allergies” path could introduce the risk of\nhallucination (misleading or fabricated content) within ChatGPT.\nIn addition toDR.KNOWS ’errors, there were instances where ChatGPT failed to leverage accurate\nknowledge paths presented. Figure 7 includes a knowledge path about “Cirrhosis of liver”, which was a\ncorrect diagnosis. However, ChatGPT did not contain this diagnosis.\nFinally, whenDR.KNOWS retrieved the correct knowledge paths and ChatGPT utilized it well, there\nwas an improvement in the output quality. Figure 8 presents an example where all the paths retrieved by\nDR.KNOWS were relevant to the input, and successfully led to ChatGPT outputting plausible diagnoses.\nThis led to higherPLAUSIBILITY scores from human evaluators.\n3 Discussion\nOn the few-shot setting, with and withoutDR.KNOWS retrieved paths, ChatGPT demonstrated a median\ndiagnostic accuracy of 66% and exhibited a remarkable median score exceeding 94% in reasoning, as per\nhuman evaluation. The incorporation ofDR.KNOWS retrieved paths proved to be beneﬁcial, enhancing\nChatGPT’s performance, as evidenced by higher scores from automated metrics and improvements\nnoted inABSTRACTION and RATIONALE aspects during human evaluation. A primary source of errors\nstemmed fromDR.KNOWS incorrectly identifying irrelevant target concepts and initiating retrievals with\nless effective CUIs. This issue, along with ChatGPT’s struggle to incorporate the correct paths, was\nhighlighted as key areas for improvements.\nImpact of KG on LLM knowledge groundingBased on human evaluation of overall diagnostic\naccuracy and reasoning, integrating a knowledge graph appeared to make no noticeable impact on the per-\nformance of ChatGPT. However, closer examination of the scoring sub-category revealed thatDR.KNOWS\nenhances ChatGPT’s ability to identify abstractive diagnoses and accurately deduce connections between\ninput and possible diagnoses. Half diagnoses within MIMIC dataset are not abstracted (< 50%), which\nmay have limited the ability of the knowledge graph approach to demonstrate beneﬁts over the native\nLLM, as the knowledge graph approach would be expected to speciﬁcally augment the abstraction task.\nThe 10% decrease in ChatGPT’s abstraction with KG can be attributed to the more abstract information\nprovided in the input when using KG. Human evaluation also favored ChatGPT with KG’s rationale rather\nthan without KG (p<0.01), indicating that the inclusion of KG enhances the medical grounding of the\nresponses, leading to more clinically relevant and factually supported reasoning. Results evaluated by\nautomated metrics, ROUGE and Concept F-score also illustrated the improved precision and F-score\nin identifying the correct diagnostic concepts. Such knowledge grounding highlighted the potential\nfor strengthening LLM’s medical decision-making and reducing hallucinations, which is critical in an\nAI-augmented diagnostic decision-support system.\nThrough these results, our work presented the potential beneﬁts of knowledge grounding through\na retrieval-augmented generation framework utilizing the most important concepts and relations for\nknowledge-intensive tasks. Expanding or modifying the memory and knowledge of large language models\nis not a straightforward task, potentially resulting in factual inaccuracies and hallucinations. The use of a\nretrieve-and-augment framework, leveraging external knowledge sources, has demonstrated its ability to\nmitigate these issues, as evidenced by previous research (Lewis et al., 2020; Shuster et al., 2021).\nOverall performance and insights drawn from human evaluation scoresThe median diagnostic\naccuracy of 66%, achieved by both few-shot prompting ChatGPT models, revealed ChatGPT’s robust\nperformance in generating diagnoses from daily hospital progress notes. The exceptionally strong\nperformance in reasoning, with a median score surpassing 94%, highlights ChatGPT’s capacity for\n12\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nweighing and integrating various pieces of evidence when arriving at a diagnosis, a promising indication\nfor clinical diagnostic reasoning. This evidence-based approach is crucial for LLMs for clinical diagnostic\ndecision-support, ensuring that the model’s recommendations are rooted in the provided input and that\nsuch evidence-based grounding is accessible to healthcare providers.\nThe detailed scoring in human evaluation not only highlighted ChatGPT’s performance but also\npointed towards areas for future enhancement. One signiﬁcant issue to address was the omission of\ndiagnoses. Currently, ChatGPT exhibited no omitted diagnoses in only 15% of cases, with the majority\nof omitted diagnoses attributed to aleatoric uncertainty. This uncertainty arises when the evidence for\ndiagnoses is present in the input, but the model fails to accurately capture and incorporate this information.\nAddressing and minimizing this type of uncertainty is pivotal for enhancing the precision and reliability\nof the diagnostic process using ChatGPT.\nDiscrepancy between automated metrics and human evaluationOur experiments revealed intriguing\ndifferences between the results obtained from automated metrics and human evaluation. While the\nautomated metrics suggest a performance difference between the two models, with the KG-augmented\nmodel demonstrating a performance gain over its non-KG counterpart, human evaluation results show\nthat both models are consistently rated as equally proﬁcient. We attributed this divergence to the speciﬁc\ndimension assessed by automated metrics, as opposed to human evaluation scores that aggregate multiple\ndistinct scoring criteria. ROUGE assesses content quality through string overlap analysis, while the\nconcept-based F-score gauges the precision of identiﬁed concepts in the generated text. These metrics\noffer distinct perspectives on model performance. Nevertheless, it is important to recognize that these\nmetrics may not entirely capture the nuanced aspects of human evaluation. Further investigation on the\ncorrelation between automated metrics and human scoring is concluded as future work. We also encourage\nfuture research to explore ways to bridge the gap between automated metrics and human judgment for a\nmore comprehensive assessment of model performance.\nInforming future knowledge graph model development fromDR.KNOWS error analysis Error\nanalysis showed thatDR.KNOWS still suffered from recognizing knowledge paths that were not related to\nthe input patient representation, and that the selection of starting medical concepts was pivotal in ﬁnding\nthe right paths. Currently,DR.KNOWS relied solely on semantic-based ranking on the candidate paths,\nthat is, the cosine similarity between candidate path embeddings and input text, with the quality of these\nembeddings being crucial for ranking performance. In addition to enhancing the representation method\nand these embeddings, other elements that are essential in modeling relations between symptoms and\ndiagnoses, for instance, probabilistic modeling (Rotmensch et al., 2017; Wan and Du, 2021), should be\nincorporated into the graph-based methods. We encourage future research to explore this integration and\nimprove DR.K NOWS ’ diagnostic potential.\nThe error analysis also presented instances where ChatGPT neglected to incorporate certain beneﬁcial\nknowledge paths. It’s important to acknowledge that ChatGPT operates as a black-box API model, with\nits internal weights and training processes being inaccessible. To enhance the efﬁcacy of the graph-based\nretrieve-and-augment framework, it would be advantageous to explore the potential of graph-prompting\nand instruction tuning on open-source language models. These methods could reﬁne the model’s ability\nto utilize relevant information effectively. Other relevant research also employs advanced prompting\ntechniques, such as self-retrieval-augmented generation (Asai et al., 2023) and step-back prompting (Zheng\net al., 2023), which merit further exploration in future investigations.\nIn conclusion, LLMs like ChatGPT are a promising direction for generating diagnoses for clinical\ndecision support; however, methods such as graph prompting are needed to guide the model down correct\nreasoning paths to avoid hallucinations and provide comprehensive diagnoses. While we show some\nprogress in a graph prompting approach withDR.KNOWS , more work is needed to improve methods that\nleverage the UMLS knowledge source for grounding to achieve more accurate outputs. Furthermore, our\nhuman evaluation framework carries strong face validity and reliability to evaluate a model’s strengths\nand weaknesses as a diagnostic decision support system.\n13\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nFigure 10:DR.KNOWS model architecture. The input concepts (“female”, “fever”, etc) are represented by concept\nunique identiﬁers (CUIs, represented as the combination of letters and numbers, e.g.“C0243026”, “C0015967”).\n4 Methods\n4.1 Grounding Medical Knowledge with Knowledge Graph\n4.1.1 Problem Formulation\nDiagnosis in progress notesDaily progress notes are formatted using the SOAP Format (Weed, 1969).\nThe Subjective section of a SOAP format daily progress note comprises the patient’s self-reported\nsymptoms, concerns, and medical history. TheObjective section consists of structural data collected by\nhealthcare providers during observation or examination, such as vital signs (e.g., blood pressure, heart\nrate), laboratory results, or physical exam ﬁndings. TheAssessment section summarizes the patient’s\noverall condition with a focus on the most active problems/diagnoses for that day. Finally, thePlan section\ncontains multiple subsections, each outlining a diagnosis/problem and its treatment plan. Our task is to\npredict the list of problems and diagnoses that are part of thePlan section.\nUsing UMLS KG to ﬁnd potential diagnoses given a patient’s medical narrativeThe UMLS concepts\nvocabulary comprises over 187 sources. For our study, we focused on the Systematized Nomenclature\nof Medicine-Clinical Terms (SNOMED CT). The UMLS vocabulary is a comprehensive, multilingual\nhealth terminology and the US national standard for EHRs and health information exchange. Each UMLS\nmedical concept is assigned a unique SNOMED concept identiﬁer (CUI) from the clinical terminology\nsystem. We utilize semantic types, networks, and semantic relations from UMLS knowledge sources to\ncategorize concepts based on shared attributes, enabling efﬁcient exploration and supporting semantic\nunderstanding and knowledge discovery across various medical vocabularies.\nGiven a medical knowledge graph where vertices are concepts and edges are semantic relations, and\nan input text describing a patient’s problems, we could perform multi-hop reasoning over the graphs and\ninfer the ﬁnal diagnoses. Figure 1 demonstrated how UMLS semantic relations and concepts can be used\nto identify potential diagnoses from the evidence provided in a daily care note. The example patient\npresents with medical conditions of fever, coughing, and sepsis, which are the concepts recognized by\nmedical concepts extractors (cTAKES (Savova et al., 2010) and QuickUMLS (Soldaini and Goharian,\n2016)) and the starting concepts for multi-hop reasoning. Initially, we extracted the direct neighbors for\nthese concepts. Relevant concepts that align with the patient’s descriptions were preferred. For precise\ndiagnoses, we chose the topN most relevant nodes at each hop.\nThis section introduces the architecture design forDR.KNOWS . As shown in Figure 10, all identiﬁed\nUMLS concepts with assigned CUI from the input patient text will be used to retrieve 1-hop subgraphs\nfrom the constructed large UMLS knowledge graph. These subgraphs are encoded as graph representations\nby a Stack Graph Isomorphism Network (SGIN) (Xu et al., 2019) and then fed to the Path Encoder,\nwhich generates path representations. The Path Ranker module assesses 1-hop paths by considering\ntheir semantic and logical association with the input text and concept, generating a score using the path\n14\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nrepresentation, input text, and concept representation. The top N scores among the set of 1-hop neighbor\nnodes, aggregated from all paths pointing to those nodes, guide the subsequent hop exploration. In case a\nsuitable diagnosis node is not found, termination is assigned to the self-loop pointing to the current node.\n4.1.2 Contextualized Node Representation\nWe deﬁned the deterministic UMLS knowledge graphG = VE based on SNOMED CUIs and semantic\nrelations, whereV is a set of CUIs, andE is a set of semantic relations. Given an input textx containing\na set of source CUIsVsrc ✓V , and their 1-hop relationsEsrc ✓E , we can construct relation paths for\neach hviiI\ni=1 ✓V src as P = {p1, p2,..., pJ } s.t. pj = {v1, e1, v2 ... et\u00001, vt}, j 2 J, where t is a\npre-deﬁned scalar andJ is non-deterministic. Relationset were encoded as one-hot embeddings. We\nconcatenated all concept names forvi with special token [SEP], s.t.li = [name 1 [SEP] name 2 [SEP] ...],\nand encodedli using SapBERT (Liu et al., 2021) to obtainhi. This allowed the CUI representation\nto serve as the contextualized representation of its corresponding concept names. We chose SapBERT\nfor its UMLS-trained biomedical concept representation. Thehi is further updated through topological\nrepresentation using SGIN:\nh(k)\ni = MLP(k)((1 +✏(k))h(k)\ni +\nX\ns2N(vi)\nRELU(hs, es,i)) ,\nhi =[ h(1)\ni ; h(2)\ni ; ... ; h(K)\ni ] .\n(1)\nwhere N (vi) represents the neighborhood of nodevi, h(k)\ni is the representation of nodevi at layerk, ✏(k)\nis a learnable parameter, andMLP(k) is a multilayer perceptron. GIN iteratively aggregates neighborhood\ninformation using graph convolution followed by nonlinearity, modeling interactions among different\nv ✓V . Furthermore, the stacking mechanism is introduced to combine multiple GIN layers. The\nﬁnal node representationvi at layer K is computed by stacking the GIN layers, where[· ; · ] denotes\nconcatenation.\nWe empirically observed that some types of CUIs are less likely to lead to useful paths for diseases,\ne.g., the concept “recent\" (CUI: C0332185) is a temporal concept and the neighbors associated with it are\nless useful. We designed a TF-IDF-based weighting scheme to assign higher weights to more relevant\nCUIs and semantic types, and multiply theseWCUI to its correspondinghi:\nWCUI = TFIDFconcept ⇤\nX\nTFIDFsemtypeconcept . (2)\n4.1.3 Path Reasoning and Ranking\nFor each node representationhi, we used its n-hoph(n)\nt,i of the set neighborhoodV(n)\nt for hi and the\nassociated relation edgee(n)\nt,i to generate the corresponding path embeddings:\npi =\n(\nhi if n =1\np(n\u00001)\nt,i otherwise , (3)\np(n)\nt,i = FFN(Wih(n)\ni + Wt([e(n)\nt,i , h(n)\nt,i ])) . (4)\nwhere FFN is feed-forward network, andn is the number of hop in the subgraphGsrc.\nFor each path embeddingpi, we proposed two attention mechanisms, i.e., MultiHead attention\n(MultiAttn) and Trilinear attention (TriAttn), to compute its logical relation leveraging the input narrative\nrepresentation hx and input list of CUIshv, both of which are encoded by SapBERT. We further deﬁned\nHi as context relevancy matrix, andZi as concept relevancy matrix:\nHi =[ hx; pi; hx \u0000 pi; hx \u0000 pi], Hi 2 R4D ,\nZi =[ hv; pi; hv \u0000 pi; hv \u0000 pi], Zi 2 R4D ,\n↵i = MultiAttn(Hi \u0000 Zi) ,\nSMulti\ni = \u0000(Relu(\u0000(↵i))) .\n(5)\n15\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nThese relevancy matrices were inspired by prior work on natural language inference (Conneau et al.,\n2017), specifying the logical relations as matrix concatenation, difference, and product. An alternative\ndesign is Trilinear attention which learns the intricate relations by three attention maps:\n↵i =( hx, hv, pi)=\nX\na,b,c\n(hx)a(hv)b(pi)cWabc ,\nSTri\ni = \u0000(Relu(\u0000(↵i))) .\n(6)\nwhere hx, pi and hv have same dimensionalityD, and\u0000 is a MLP. Finally, we aggregated the MultiAttn\nor TriAttn scores on all candidate nodes , and select the topN entity VN for next hop iteration based on\nthe aggregated scores:\n\u0000 = Softmax(⌃Vsrc\ni=1⌃T\nt=1STri\ni,t) ,\nVN = argmaxN (\u0000) .\n(7)\n4.1.4 Loss Function\nOur loss function consisted of two parts, i.e., a CUI prediction loss and a contrastive learning loss:\nL = LPred + LCL . (8)\nFor prediction lossLPred, we used Binary Cross Entropy (BCE) loss to calculate whether selectedVN\nis in the gold labelY:\nLPred =\nMX\nm\nNX\nn\n(ym,n ⇤ log(vm,n)+\n(1 \u0000 ym,n) ⇤ log(1 \u0000 vm,n)) . (9)\nwhere M is the number of gold labelY.\nFor contrastive learning lossLCL, we encouraged the model to learn meaningful and discriminative\nrepresentations by comparing with positive and negative samples:\nLCL =\nX\ni\nmax(cos(Ai,f i+) \u0000 cos(Ai,f i\u0000)+ margin, 0) . (10)\nwhere Ai is the anchor embedding, deﬁned ashx \u0000 hv, and\u0000 is Hadamard product.P\ni indicates a\nsummation over a set of indicesi, typically representing different training samples or pairs. Inspired\nfrom (Yasunaga et al., 2022), we constructcos(Ai,f i+) and cos(Ai,f i\u0000) to calculate cosine similarity\nbetween Ai and positive featurefi+ or negative featurefi\u0000, respectively. This equation measures the\nloss when the similarity between an anchor and its positive feature is not signiﬁcantly greater than the\nsimilarity between the same anchor and a negative feature, considering a margin for desired separation.\nAppendix C described the full DR.KNOWS model training process.\n4.1.5 Prompting for foundational models\nTo incorporate graph model predicted paths into a prompt, we applied a prompt engineering strategy\nutilizing domain-independent prompt patterns, as delineated in White et al. (2023). Our prompt was\nconstructed with two primary components: theoutput customizationprompt, which speciﬁes the require-\nment of exploiting knowledge paths, and thecontext control patterns, which are directly linked to the\nDR.KNOWS ’s output.\nGiven that our core objective was to assess the extent to which the prompt can bolster the model’s\nperformance, it became imperative to test an array of prompts. Gonen et al. (2022) presented a tech-\nnique, BETTER PROMPT , which relied onSELECTING PROMPTS BY ESTIMATING LANGUAGE MODEL\nLIKELIHOOD (SPELL ). Essentially, we initiated the process with a set of manual task-speciﬁc prompts,\n16\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nGroup Output Customization Prompts and No. Perplexity\nNon-Subj. A. <Explain> B.You may utilize these facts: C. You may ﬁnd these facts helpful: 3.86e-13\nSubj. D. /E.*Act as a medical doctor, and list the top three direct and indirect diagnoses from\nthe Assessment.*(You will be provided with some hints from a knowledge graph.)\n1.03e-03\nExplain the reasoning and assumptions behind your answer.\nNo. Context Control with Path Presentation\n1 Structural.e.g. “Infectious Diseases –> has pathological process –> Pneumonia\n\"“–>\" is added to tokenizers as a new token.\n2 Clause. e.g. “Infectious Diseases has pathological process Pneumonia\"\nTable 5:Five manually designed prompts (Output Customization) and two path representation styles (Context Con-\ntrol) we create for the path-prompting experiments. There are 10 prompt patterns in total (5 Output Customization x\n2 Context Control). For each Output Customization prompt, we generate 50 paraphrases using ChatGPT and run\nBETTER PROMPT to obtain the perplexity. This table also include the average perplexity for each prompt. Prompts\nwith *are also deployed for path-less T5 ﬁne-tuning (baseline).\nsubsequently expanding the prompt set via automatic paraphrasing facilitated by ChatGPT and backtrans-\nlation. We then ranked these prompts by their perplexity score (averaged over a representative sample of\ntask inputs), ultimately selecting those prompts exhibiting the lowest perplexity.\nGuided by this framework, we manually crafted ﬁve sets of prompts to integrate the path input, which\nare visually represented in Table 5. Speciﬁcally, the ﬁrst three prompts were designed by a non-medical\ndomain expert (computer scientist), whereas the ﬁnal two sets of prompts were developed by a medical\ndomain expert (a critical care physician and a medical informaticist). We designated the last two prompts\nas \"Subject-Matter Prompts,\" with the medical persona, and the ﬁrst three prompts as \"Non-Subject-Matter\nPrompts.\" A comprehensive outline elucidating our approach to generating the prompt with paths can be\nfound in Appendix E.\n4.2 Experiments and Automated Evaluation\nWe trained the proposedDR.KNOWS (TriAttnW and MultiAttnW ) onIN-HOUSE and MIMIC dataset. We\nobtained a data split of 600, 81, and 87 on theMIMIC dataset and 3885, 520, 447 on theIN-HOUSE dataset.\nThe main task is to assess how wellDR.KNOWS predicts diagnoses using CUIs. To achieve this, we\nanalyzed the text in the plan section using a concept extractor and extract the CUIs that fall under the\nsemantic typeT047 DISEASE AND SYNDROMES . Speciﬁcally, we included the CUIs that are guaranteed\nto have at least one path with a maximum length of 2 hops between the target CUIs and input CUIs.\nThese selected CUIs formed the \"gold\" CUI set, which was used for training and evaluating the model’s\nperformance. Appendix B and D described the preprocessing and training setup, respectively.\nSince DR.KNOWS predicts the topN CUIs, we measured the Recall@N and Precision@N as below.\nThe F-score is the harmonic mean between Recall and Precision, which will also be reported.\nRecall = |pred \\ gold|\n|gold| (11)\nPrecision = |pred \\ gold|\n|pred| (12)\nWhen evaluating the output diagnoses, we applied the above evaluation metric as well as ROUGE (Lin,\n2004). Speciﬁcally, ROUGE is a widely used set of metrics designed for evaluating the quality of machine-\ngenerated text by comparing it to reference texts. We utilized the ROUGE-L variant, which is based on\nthe longest common substring, and the ROUGE-2 variant, which focuses on bigram matching.\n4.3 Metrics Development for Human Evaluation\n4.3.1 Motivation\nExisting frameworks of human evaluation have been implemented for generative AI on certain tasks such\nas radiology report generation, but the ﬁeld of diagnosis generation remains underdeveloped. Robust\nevaluation methodologies like SaferDX (Singh et al., 2019) have paved the way for assessing missed\n17\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\ndiagnostic opportunities, but their potential integration with Language Model evaluations has yet to\nbe explored. Our reﬁned framework underscores the pressing need for a structured human evaluation\napproach, which remains the reference standard and overcomes the limitations of quantitative evaluations.\nOur rigor in modeling SaferDx, performing a thorough literature review, and iterative user-centered design\nby subject matter experts helped to design an evaluation framework that was further validated by strong\ninter-rater agreement by medical experts.\nWe identiﬁed seven broad aspects widely deployed in human evaluation for biomedical NLP tasks:\n(1) Factual Consistency (Guo et al., 2020; Yadav et al., 2021; Wallace et al., 2020; Abacha et al.,\n2023; Moramarco et al., 2021; Otmakhova et al., 2022; Dalla Serra et al., 2022; Cai et al., 2022), (2)\nHallucination (Guo et al., 2020; Umapathi et al., 2023), (3) Quality of Evidence (Otmakhova et al.,\n2022; Singhal et al., 2023), (4) Safety / Potential for Harm (Singhal et al., 2023; Dalla Serra et al., 2022;\nAdams et al., 2023), (5) Conﬁdence (Otmakhova et al., 2022), (6) Omission (Abacha et al., 2023), and\n(7) Linguistic Quality (Radev and Tam, 2003; Guo et al., 2020). These aspects were then broken down\nand more clearly deﬁned for inclusion in a human evaluation framework. The only factor not considered\nwas Linguistic Quality. This factor was tied to general domain tasks and those intent on the ﬂuency and\nreadability of generated text for the general population. However, in a clinical setting, this is not a key\nfocus so attention was given to aspects relating to content, instead.\n4.3.2 Survey Development\nEvaluation criteria The intent of evaluation of clinical diagnostic reasoning tasks is to verify that\ninclusion of generative LLMs in the clinical setting does not introduce additional potential for harm on\npatients. Therefore, the diagnostic evaluation portion was largely inﬂuenced by the revised SaferDx\ninstrument (Singh et al., 2019) because of its applications in identifying and deﬁning diagnostic errors and\ntheir potential for harm. Based on this instrument and our 6 identiﬁed aspects of manual evaluation from\nliterature searching, the diagnostic evaluation process was broken down into 4 sections: (1)ACCURACY ,\n(2) PLAUSIBILITY , (3)SPECIFICITY , and (4)OMISSION AND UNCERTAINTY . ACCURACY was intended\nto capture the factuality of the diagnostic output as well as penalize a model for hallucinating output that\ndoes not qualify as a diagnosis.PLAUSIBILITY , which is conditional onACCURACY , was intended to\ncapture the potential for harm present in an inaccurate diagnosis.SPECIFICITY , which is conditional\non PLAUSIBILITY , is deﬁned as the level of detail provided in the diagnosis. Finally,OMISSION AND\nUNCERTAINTY deﬁned cases when a diagnosis is not included in the list of outputted diagnoses but would\nbe considered by a clinician in the clinical setting based upon the input data. In the case of the omission,\nthe UNCERTAINTY further deﬁned the reasons asaleatoric uncertainty– when LLM has been provided\nwith the necessary information but has not utilized it;epistemic uncertainty– when the input to LLM\ndoes not contain the data needed to make a diagnosis.\nThe quality of evidence aspect of evaluation becomes a key factor in evaluating the reasoning output\nbecause clinical diagnostic reasoning is not a deﬁnitive process. Therefore, the reasoning evaluation\nportion was largely inﬂuenced by the framework established in Singhal et al. (2023), because of their\nrigorous validity measures compared to other established evaluation frameworks and focus on evidence\nquality as an aspect of evaluation. We utilized three of the aspects of their evaluation framework - (1)\nREADING COMPREHENSION , (2)RATIONALE , and (3)RECALL OF KNOWLEDGE - and incorporated an\naspect on (4)OMISSION of diagnostic reasoning.READING COMPREHENSION was intended to capture if\na model understood the information in a progress note.RATIONALE was intended to capture the inclusion\nof incorrect reasoning steps.RECALL OF KNOWLEDGE was intended to capture the hallucination of\nincorrect facts as well as the inclusion of irrelevant facts in the output. Finally,OMISSION served the same\npurpose as previously by capturing when the model failed to support conclusions or provide evidence for\na diagnostic choice.\nIn addition to the aspects outlined above, the evaluators were also asked to answer questions based on\nthe amount ofABSTRACTION present in each part of the output. This was to ascertain how the knowledge\npaths inﬂuenced the type of output produced and whether or not the model was able to use abstraction.\nSince abstraction does not directly equate to better text generation, these questions did not impact the\n18\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nFigure 11: The structure of the survey questions given an LLM output. The output consists of two\ncomponents: diagnoses (red-colored text) and reasoning (blue-colored text). For each component, there\nare corresponding questions evaluating certain aspects.\nscoring process, but served as an additional piece of information. For the reasoning output,EFFECTIVE\nABSTRACTION , conditional onABSTRACTION , was also utilized to determine if any of the abstracted\noutput aided or hindered the reasoning.\nImplementation Figure 11 presents the structure of the proposed human evaluation survey, and the\nquestions asked under each scoring aspect. Each model output consists of the model predicted diagnoses\n(“Diagnosis”) and reasoning (“<Reasoning>”). We scored diagnoses and reasoning both at the individual\ninstance level and their entirety. The scoring aspects of each component were highlighted in §4.3.2.\nThe evaluation framework was implemented utilizing the Research Electronic Data Capture (REDCap)\nweb application. The input, output, and gold standards were auto-populated into REDCap for the\nevaluators. Each evaluator was treated as a different arm in a longitudinal data collection framework\nthat had two deﬁned events: one for the model utilizing knowledge graph paths and one for the model\nwithout them. The guidelines given to each evaluator contain a step-by-step guide on how to complete\nan evaluation in the REDCap system. We attached the complete survey and REDCap interface to\nSupplementary Materials.\nValidation We employ two crucial methods, construct validity and content validity, to ensure the\nrobustness and effectiveness of our proposed human evaluation process. Construct validity and content\nvalidity are indispensable tools in the realm of research and assessment, playing pivotal roles in the\nveriﬁcation of usability and the quality of our evaluation framework. two senior physicians who are\nexperts with more than 10 years of experience in taking care of patients and also board-certiﬁed in clinical\ninformatics served as advisors and pilot test users, which met the requirements for content validity. The\nhelped design the user guide and train two medical professionals with medical school training to perform\nthe human evaluations.\nThe construct validity is supported by the inter-annotator agreement between the two senior physicians\nand two medical professionals. Utilizing approximately 20 output examples from each model, iterative\ncorrections were made to the human evaluation process to maximize usability, clarity, and applicability.\nUpon agreement between the clinicians, the two medical professionals were trained to complete the\nevaluations. They were trained on approximately 20 output examples from each model until they were\nin agreement with the senior clinicians (Kappa >0.7). The inter-annotator agreement between the two\nﬁnal evaluators was also veriﬁed (Kappa >0.7).\nThe construct validity of the proposed survey received further support from our literature search on\nprevious work that used the same criteria or standards for assessment. We examined over 50 manual\n19\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nevaluation framework for text summarization from publications in the Association for Computational\nLinguistics and PubMed, and identiﬁed the 7 broad aspects of manual evaluation (see §??). We also used\nthe SaferDx survey instrument to guide our survey development, ensuring the survey was designed with a\nfocus on diagnostic safety.\n4.3.3 Survey scoring\nOnce the resident and medical student were veriﬁed as in agreement with the senior clinicians, each was\ngiven a set of output records from each model to evaluate. In total, at least 92 records were evaluated for\neach model.\nProcessing Steps In the pre-processing phase, we handled missing values in the Plausibility and\nSpeciﬁcity category differently depending on the cause.\nDue to the inherent branching logic within some of the categories, missing values were substituted\nwith a value of0 during the score calculations. Additionally, we implemented a scoring transformation to\nthe COMPREHENSION ,R ECALL , AND RATIONALE questions: to address the reverse interpretation of\nthese questions, we employed a transformation formula: (6 \u0000 x).\nDiagnosis Scoring The diagnosis scoreDi given a recordi is computed as below:\nDi = ¯pi +¯ si + oi\n15 (13)\nwhere ¯pi is the mean of the plausibility scores for recordi, ¯si is the mean of the speciﬁcity scores for\nrecord i, oi is the mean of the omission and uncertainty scores for recordi, The denominator is15 because\neach component was scored on a 5-point Likert scale and this 15 normalizes the scores into a(0, 1) scale.\nReasoning Scoring The reasoning scoreRi given a recordi is computed as below:\nRi = ¯ci +¯ ei +¯ ai\n15 (14)\nwhere ¯ci is the mean of the comprehension scores,¯ei is the mean of the recall scores,¯ai is the mean of the\nrationale scores for recordi. The denominator is15 because each component was scored on a 5-point\nLikert scale and this 15 normalizes the scores into a(0, 1) scale.\n4.3.4 Signiﬁcance Testing\nStatistical signiﬁcance testing was performed utilizing a paired assumption. Since the KG and No KG\nscoring processes were done using the same progress notes, a pair was considered to be the score from\neach model for a particular progress note. Tests on statistical signiﬁcance between normalized diagnosis\nand reasoning scores used a two-sided paired t-test. This is because the diagnosis and reasoning scores\nwere quantitative values on a 0 to 1 scale. In cases where analysis was done on aspects of the scores (i.e.\nSPECIFICITY , OMISSION , PLAUSIBILITY ), a McNemar test was utilized. The Likert and binary scale\nvalues were considered nominal categories for this test. All statistical signiﬁcance testing was performed\nin R v4.3.1.\n5 Data Availability\nThe annotated MIMIC dataset used in this work could be downloaded fromhttps://www.physionet.\norg/content/bionlp-workshop-2023-task-1a/2.0.0/ . Note that Data Use Agreement is required.\nThe second dataset,IN-HOUSE EHR dataset, contains clinical notes from adult patients from University\nof Wisconsin, which are securely stored at the HIPPA-compliant server at the University of Wisconsin,\n20\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nMadison, USA. The raw notes contain protected health information which could compromise privacy\nor consent. The processed textual data with concept unique identiﬁers is however available from the\ncorresponding author on reasonable request.\n6 Code Availability\nThe codebase for this study will be made available from the corresponding authors upon reasonable\nrequest.\n7 Ethics\nPatient Privacy and Data Security:The use of real-world EHRs, even when de-identiﬁed, carries\ninherent risks to patient privacy. It’s crucial to ensure strict data protection and privacy measures\nthroughout the process, in compliance with relevant regulations like HIPAA. We adhere strictly to the\nData Use Agreement of the MIMIC dataset, refraining from sharing the data with any third parties. EHR\nclinical notes and structured data from adult hospitalizations at the University of Wisconsin (UWHealth)\nHospital (IN-HOUSE EHR), saved, accessed and experimented upon a HIPAA-Compliant and University\nIT-managed analytic server.\nAlgorithm Transparency and Accountability:The black-box nature of many AI models may raise\nquestions about transparency and accountability. While our model offers an explainable diagnostic\npathway, the complexity of AI decision-making may still be challenging to fully interpret and validate,\nraising potential concerns about trustworthiness.\nRisk of Overreliance on AI:The usage of AI for diagnostic predictions should not replace the critical\nthinking and decision-making abilities of healthcare professionals. It should serve as a tool to assist and\naugment their abilities, and it’s important to ensure that healthcare providers don’t become over-reliant on\nAI systems.\nPotential for Bias: AI models are prone to reproduce and even amplify the biases present in their\ntraining data. Unconscious biases in healthcare can lead to disparities in care quality and outcomes. It’s\ncritical to scrutinize the model for potential biases and continually monitor its performance in various\ndemographic groups to ensure fairness.\nReferences\nAsma Ben Abacha, Wen-wai Yim, George Michalopoulos, and Thomas Lin. 2023. An investigation of\nevaluation metrics for automated medical note generation.arXiv preprint arXiv:2305.17364.\nGrifﬁn Adams, Emily Alsentzer, Mert Ketenci, Jason Zucker, and Noémie Elhadad. 2021. What’s in a\nsummary? laying the groundwork for advances in hospital-course summarization. InProceedings of\nthe conference. Association for Computational Linguistics. North American Chapter. Meeting, volume\n2021, page 4794. NIH Public Access.\nGrifﬁn Adams, Jason Zucker, and Noémie Elhadad. 2023. A meta-evaluation of faithfulness metrics for\nlong-form hospital-course summarization.arXiv preprint arXiv:2303.03948.\nClaudio Aracena, Fabián Villena, Matías Rojas, and Jocelyn Dunstan. 2022. A knowledge-graph-based\nintrinsic test for benchmarking medical concept embeddings and pretrained language models. In\nProceedings of the 13th International Workshop on Health Text Mining and Information Analysis\n(LOUHI), pages 197–206.\n21\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to\nretrieve, generate, and critique through self-reﬂection.arXiv preprint arXiv:2310.11511.\nErin P Balogh, Bryan T Miller, and John R Ball. 2015. Improving diagnosis in health care.\nChristian Baumgartner. 2023. The potential impact of chatgpt in clinical and translational medicine.\nClinical and translational medicine, 13(3).\nOlivier Bodenreider. 2004. The uniﬁed medical language system (umls): integrating biomedical terminol-\nogy. Nucleic acids research, 32(suppl_1):D267–D270.\nPJ Brown, JL Marquard, B Amster, M Romoser, J Friderici, S Goff, and D Fisher. 2014. What do\nphysicians read (and ignore) in electronic progress notes?Applied clinical informatics, 5(02):430–444.\nPengshan Cai, Fei Liu, Adarsha Bajracharya, Joe Sills, Alok Kapoor, Weisong Liu, Dan Berlowitz, David\nLevy, Richeek Pradhan, and Hong Yu. 2022. Generation of patient after-visit summaries to support\nphysicians. InProceedings of the 29th International Conference on Computational Linguistics, pages\n6234–6247, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\nLoredana Caruccio, Stefano Cirillo, Giuseppe Polese, Giandomenico Solimando, Shanmugam Sundara-\nmurthy, and Genoveffa Tortora. 2024. Can chatgpt provide intelligent diagnoses? a comparative study\nbetween predictive models and chatgpt to deﬁne a new medical diagnostic bot.Expert Systems with\nApplications, 235:121186.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from natural language inference data. InProceedings\nof the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680,\nCopenhagen, Denmark. Association for Computational Linguistics.\nPat Croskerry. 2005. Diagnostic failure: A cognitive and affective approach.Advances in Patient Safety:\nFrom Research to Implementation (Volume 2: Concepts and Methodology).\nFrancesco Dalla Serra, William Clackett, Hamish MacKinnon, Chaoyang Wang, Fani Deligianni, Jeff\nDalton, and Alison Q. O’Neil. 2022. Multimodal generation of radiology reports using knowledge-\ngrounded extraction of entities and relations. InProceedings of the 2nd Conference of the Asia-Paciﬁc\nChapter of the Association for Computational Linguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 1: Long Papers), pages 615–624, Online only. Association\nfor Computational Linguistics.\nMolla S Donaldson, Janet M Corrigan, Linda T Kohn, et al. 2000. To err is human: building a safer health\nsystem.\nLuciano Floridi and Massimo Chiriatti. 2020. Gpt-3: Its nature, scope, limits, and consequences.Minds\nand Machines, 30:681–694.\nBryant Furlow. 2020. Information overload and unsustainable workloads in the era of electronic health\nrecords. The Lancet Respiratory Medicine, 8(3):243–244.\nYanjun Gao, Dmitriy Dligach, Timothy Miller, Matthew M Churpek, and Majid Afshar. 2023. Overview of\nthe problem list summarization (probsum) 2023 shared task on summarizing patients’ active diagnoses\nand problems from electronic health record progress notes.arXiv preprint arXiv:2306.05270.\nYanjun Gao, Dmitriy Dligach, Timothy Miller, Dongfang Xu, Matthew MM Churpek, and Majid Afshar.\n2022. Summarizing patients’ problems from hospital progress notes using pre-trained sequence-to-\nsequence models. InProceedings of the 29th International Conference on Computational Linguistics,\npages 2979–2991.\n22\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nHila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. 2022. Demystifying prompts\nin language models via perplexity estimation.arXiv preprint arXiv:2212.04037.\nYue Guo, Wei Qiu, Yizhong Wang, and Trevor Cohen. 2020. Automated lay language summarization of\nbiomedical scientiﬁc reviews.CoRR, abs/2012.12573.\nBin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu, Nicholas Jing Yuan, and Tong Xu. 2020. Bert-mk:\nIntegrating graph contextualized knowledge into pre-trained language models. InFindings of the\nAssociation for Computational Linguistics: EMNLP 2020, pages 2281–2290.\nKung-Hsiang Huang, Mu Yang, and Nanyun Peng. 2020. Biomedical event extraction with hierarchical\nknowledge graphs. InFindings of the Association for Computational Linguistics: EMNLP 2020, pages\n1277–1285, Online. Association for Computational Linguistics.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. Mimic-iii, a freely\naccessible critical care database.Scientiﬁc data, 3(1):1–9.\nTomoyuki Kuroiwa, Aida Sarcon, Takuya Ibara, Eriku Yamada, Akiko Yamamoto, Kazuya Tsukamoto,\nand Koji Fujita. 2023. The potential of chatgpt as a self-diagnostic tool in common orthopedic diseases:\nExploratory study.Journal of Medical Internet Research, 25:e47621.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks.Advances in Neural Information Processing Systems,\n33:9459–9474.\nHao Li, Yuping Wu, Viktor Schlegel, Riza Batista-Navarro, Thanh-Tung Nguyen, Abhinav Ramesh\nKashyap, Xiaojun Zeng, Daniel Beck, Stefan Winkler, and Goran Nenadic. 2023. Pulsar: Pre-training\nwith extracted healthcare terms for summarising patients’ problems and data augmentation with black-\nbox large language models.arXiv preprint arXiv:2306.02754.\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. InText Summarization\nBranches Out, pages 74–81.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier. 2021. Self-alignment\npretraining for biomedical entity representations. InProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\npages 4228–4238.\nJinghui Liu, Daniel Capurro, Anthony Nguyen, and Karin Verspoor. 2022. “note bloat” impacts deep\nlearning-based nlp models for clinical prediction tasks.Journal of biomedical informatics, 133:104149.\nQiuhao Lu, Dejing Dou, and Thien Huu Nguyen. 2021. Parameter-efﬁcient domain knowledge integration\nfrom multiple sources for biomedical pre-trained language models. InFindings of the Association for\nComputational Linguistics: EMNLP 2021, pages 3855–3865.\nPotsawee Manakul, Yassir Fathullah, Adian Liusie, Vyas Raina, Vatsal Raina, and Mark Gales. 2023. Cued\nat probsum 2023: Hierarchical ensemble of summarization models.arXiv preprint arXiv:2306.05317.\nFrancesco Moramarco, Damir Juric, Aleksandar Savkov, and Ehud Reiter. 2021. Towards objectively\nevaluating the quality of generated medical summaries.arXiv preprint arXiv:2104.04412.\nSohn Nijor, Gavin Rallis, Nimit Lad, and Eric Gokcen. 2022. Patient safety issues from information\noverload in electronic medical records.Journal of Patient Safety, 18(6):e999–e1003.\n23\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nYulia Otmakhova, Karin Verspoor, Timothy Baldwin, and Jey Han Lau. 2022. The patient is more dead\nthan alive: exploring the current state of the multi-document summarisation of the biomedical literature.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 5098–5111, Dublin, Ireland. Association for Computational Linguistics.\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2023. Unifying large\nlanguage models and knowledge graphs: A roadmap.arXiv preprint arXiv:2306.08302.\nDragomir R. Radev and Daniel Tam. 2003. Summarization evaluation using relative utility. InProceedings\nof the Twelfth International Conference on Information and Knowledge Management, CIKM ’03, page\n508–511, New York, NY , USA. Association for Computing Machinery.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485–5551.\nMaya Rotmensch, Yoni Halpern, Abdulhakim Tlimat, Steven Horng, and David Sontag. 2017. Learning a\nhealth knowledge graph from electronic medical records.Scientiﬁc reports, 7(1):5994.\nAdam Rule, Steven Bedrick, Michael F Chiang, and Michelle R Hribar. 2021. Length and redundancy\nof outpatient progress notes across a decade at an academic medical center.JAMA Network Open,\n4(7):e2115334–e2115334.\nGuergana K Savova, James J Masanz, Philip V Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C Kipper-\nSchuler, and Christopher G Chute. 2010. Mayo clinical text analysis and knowledge extraction system\n(ctakes): architecture, component evaluation and applications.Journal of the American Medical\nInformatics Association, 17(5):507–513.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. InFindings of the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803.\nHardeep Singh, Arushi Khanna, Christiane Spitzmueller, and Ashley ND Meyer. 2019. Recommendations\nfor using the revised safer dx instrument to help measure and improve diagnostic safety.Diagnosis,\n6(4):315–323.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical\nknowledge. Nature, 620(7972):172–180.\nLuca Soldaini and Nazli Goharian. 2016. Quickumls: a fast, unsupervised approach for medical concept\nextraction. InMedIR workshop, sigir, pages 1–4.\nLogesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. 2023. Med-halt: Medical domain\nhallucination test for large language models.arXiv preprint arXiv:2307.15343.\nByron C. Wallace, Sayantan Saha, Frank Soboczenski, and Iain James Marshall. 2020. Generating\n(factual?) narrative summaries of rcts: Experiments with neural multi-document summarization.CoRR,\nabs/2008.11293.\nGuojia Wan and Bo Du. 2021. Gaussianpath: A bayesian multi-hop reasoning framework for knowledge\ngraph reasoning. InProceedings of the AAAI conference on artiﬁcial intelligence, volume 35, pages\n4393–4401.\nLawrence L Weed. 1969. Medical records, medical education, and patient care: The problem-oriented\nmedical record as a basic tool.Cleveland, OH: Press of Case Western University.\n24\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint \nGao et. al, 2023, Under Review\nJules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar,\nJesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt pattern catalog to enhance prompt\nengineering with chatgpt.arXiv preprint arXiv:2302.11382.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful are graph neural\nnetworks? In International Conference on Learning Representations.\nShweta Yadav, Deepak Gupta, Asma Ben Abacha, and Dina Demner-Fushman. 2021. Reinforce-\nment learning for abstractive question summarization with question-aware semantic rewards.CoRR,\nabs/2107.00176.\nMichihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S\nLiang, and Jure Leskovec. 2022. Deep bidirectional language-knowledge graph pretraining.Advances\nin Neural Information Processing Systems, 35:37309–37323.\nHuaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and\nDenny Zhou. 2023. Take a step back: Evoking reasoning via abstraction in large language models.\narXiv preprint arXiv:2310.06117.\n25\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 27, 2023. ; https://doi.org/10.1101/2023.11.24.23298641doi: medRxiv preprint ",
  "topic": "Unified Medical Language System",
  "concepts": [
    {
      "name": "Unified Medical Language System",
      "score": 0.716722846031189
    },
    {
      "name": "Medical diagnosis",
      "score": 0.6882442235946655
    },
    {
      "name": "Harm",
      "score": 0.5927814841270447
    },
    {
      "name": "Computer science",
      "score": 0.5586099028587341
    },
    {
      "name": "Decision support system",
      "score": 0.4931173026561737
    },
    {
      "name": "Clinical decision support system",
      "score": 0.4806846082210541
    },
    {
      "name": "Health care",
      "score": 0.4319576323032379
    },
    {
      "name": "Process (computing)",
      "score": 0.4146648049354553
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3523843586444855
    },
    {
      "name": "Knowledge management",
      "score": 0.3469547629356384
    },
    {
      "name": "Data science",
      "score": 0.3377063274383545
    },
    {
      "name": "Medicine",
      "score": 0.2665099501609802
    },
    {
      "name": "Psychology",
      "score": 0.20995160937309265
    },
    {
      "name": "Pathology",
      "score": 0.14502733945846558
    },
    {
      "name": "Political science",
      "score": 0.10641035437583923
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}