{
  "title": "Predictive Maintenance for General Aviation Using Convolutional Transformers",
  "url": "https://openalex.org/W3207690495",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5109315885",
      "name": "Hong Yang",
      "affiliations": [
        "Rochester Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5057665534",
      "name": "Aidan LaBella",
      "affiliations": [
        "Rochester Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5065630093",
      "name": "Travis Desell",
      "affiliations": [
        "Rochester Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3031724869",
    "https://openalex.org/W7016021835",
    "https://openalex.org/W3004401633",
    "https://openalex.org/W2972137370",
    "https://openalex.org/W6754957482",
    "https://openalex.org/W2913556461",
    "https://openalex.org/W3046296398",
    "https://openalex.org/W2808117860",
    "https://openalex.org/W2754051771",
    "https://openalex.org/W2783323081",
    "https://openalex.org/W2280236323",
    "https://openalex.org/W2515503816",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W2052624719",
    "https://openalex.org/W2981830988",
    "https://openalex.org/W2235186757",
    "https://openalex.org/W2767395101",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2551393996",
    "https://openalex.org/W2944223741",
    "https://openalex.org/W6600281463",
    "https://openalex.org/W3111507638",
    "https://openalex.org/W2289400088",
    "https://openalex.org/W3191026187",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2963434542",
    "https://openalex.org/W2892035503",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W2963532813",
    "https://openalex.org/W2962785940",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2746314669",
    "https://openalex.org/W3025165719",
    "https://openalex.org/W2963467407",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3153872861",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W3116498222"
  ],
  "abstract": "Predictive maintenance systems have the potential to significantly reduce costs for maintaining aircraft fleets as well as provide improved safety by detecting maintenance issues before they come severe. However, the development of such systems has been limited due to a lack of publicly labeled multivariate time series (MTS) sensor data. MTS classification has advanced greatly over the past decade, but there is a lack of sufficiently challenging benchmarks for new methods. This work introduces the NGAFID Maintenance Classification (NGAFID-MC) dataset as a novel benchmark in terms of difficulty, number of samples, and sequence length. NGAFID-MC consists of over 7,500 labeled flights, representing over 11,500 hours of per second flight data recorder readings of 23 sensor parameters. Using this benchmark, we demonstrate that Recurrent Neural Network (RNN) methods are not well suited for capturing temporally distant relationships and propose a new architecture called Convolutional Multiheaded Self Attention (Conv-MHSA) that achieves greater classification performance at greater computational efficiency. We also demonstrate that image inspired augmentations of cutout, mixup, and cutmix, can be used to reduce overfitting and improve generalization in MTS classification. Our best trained models have been incorporated back into the NGAFID to allow users to potentially detect flights that require maintenance as well as provide feedback to further expand and refine the NGAFID-MC dataset.",
  "full_text": "Predictive Maintenance for General Aviation Using Convolutional Transformers\nHong Yang, Aidan LaBella, Travis Desell\nGolisano College of Computing and Information Sciences\nRochester Institute of Technology\n20 Lomb Memorial Dr.\nRochester, New York 14623\n{hy3134, apl1341, tjdvse}@rit.edu\nAbstract\nPredictive maintenance systems have the potential to signif-\nicantly reduce costs for maintaining aircraft fleets as well as\nprovide improved safety by detecting maintenance issues be-\nfore they come severe. However, the development of such\nsystems has been limited due to a lack of publicly labeled\nmultivariate time series (MTS) sensor data. MTS classifica-\ntion has advanced greatly over the past decade, but there is\na lack of sufficiently challenging benchmarks for new meth-\nods. This work introduces the NGAFID Maintenance Clas-\nsification (NGAFID-MC) dataset as a novel benchmark in\nterms of difficulty, number of samples, and sequence length.\nNGAFID-MC consists of over 7,500 labeled flights, repre-\nsenting over 11,500 hours of per second flight data recorder\nreadings of 23 sensor parameters. Using this benchmark, we\ndemonstrate that Recurrent Neural Network (RNN) meth-\nods are not well suited for capturing temporally distant\nrelationships and propose a new architecture called Con-\nvolutional Multiheaded Self Attention (Conv-MHSA) that\nachieves greater classification performance at greater compu-\ntational efficiency. We also demonstrate that image inspired\naugmentations of cutout, mixup, and cutmix, can be used to\nreduce overfitting and improve generalization in MTS classi-\nfication. Our best trained models have been incorporated back\ninto the NGAFID to allow users to potentially detect flights\nthat require maintenance as well as provide feedback to fur-\nther expand and refine the NGAFID-MC dataset.\nIntroduction\nIn the domain of aviation, especially for small scale gen-\neral aviation fleets, aircraft maintenance is performed with\nfixed schedules or after some maintenance issue is detected\nduring operation of an aircraft.Predictive maintenance tech-\nniques can be performed to reduce cost, improve machinery\nperformance and life, as well as mitigate risk and increase\nsafety. The majority of published literature covers non neu-\nral network methods (Carvalho et al. 2019). Machine learn-\ning presents the potential to predict maintenance issues by\nmeasuring anomalies or degradation of multivariate time se-\nries (MTS) sensor data; however this has been limited by the\nproprietary nature of most flight data, with the further issue\nof acquiring the data necessary to label flight data with and\nwithout specific maintenance issues.\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nWhile the abundance of multivariate temporal data has\nenabled significant advances in MTS analysis for a wide\nvariety of fields, current literature lacks an evaluation of\nMTS methods for non synthetic, extremely long sequences\n(greater than 1024 time steps) from large labeled datasets\n(more than 5000 datapoints) (Fawaz et al. 2019). This pa-\nper utilizes data from the National General Aviation Flight\nInformation Database (NGAFID) and the MaintNet project\nto create a new large scale labeled MTS benchmark, the\nNGAFID Maintenance Classification dataset (NGAFID-\nMC), with over 7,500 labeled flight sensor data files 1, for\ndevelopment of predictive maintenance systems for aviation.\nUsing this dataset, our results show that previous MTS\nclassification methods face great difficulty in classifying pre\nand post maintenance flights. We also demonstrate that a\nnew Convolutional Multiheaded Self Attention architecture\ncan better capture complex temporally distant relationships\nwithin NGAFID-MC and leverage them for better classifi-\ncation performance and computational efficiency. We also\ndemonstrate the need for robust augmentations and intro-\nduce a set of MTS augmentations that improve general-\nization. We provide a Google Colab Notebook for anyone\nto fully replicate the results of our experiments 2. Finally,\nour best trained models have been reincorporated into the\nNGAFID to inform users and collect their feedback to fur-\nther refine the models and expand the NGAFID-MC dataset.\nRelated Work\nSeveral methods have been developed for MTS classifica-\ntion, for a review see (Fawaz et al. 2019). Notable non-deep\nlearning methods include distance based k-nearest neighbors\nby (Orsenigo and Vercellis 2010) and Dynamic Time Warp-\ning KNN by (Seto, Zhang, and Zhou 2015). For deep learn-\ning methods, well performing MTS classifiers tend to utilize\nsome combination of Recurrent Neural Network (RNN) and\nConvolutional Neural Network (CNN) methods,e.g. (Karim\net al. 2017), or Temporal CNN (TCNN) methods (Wang,\nYan, and Oates 2017). However, RNN methods struggle\nwith long sequences due to the vanishing gradient prob-\nlem (Le and Zuidema 2016). TCNN methods perform well\nfor MTS classification (Assaf et al. 2019), but may struggle\n1https://www.kaggle.com/hooong/ngafid-mc-20210917\n2https://tinyurl.com/b35mxv98\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n12636\nwhen relevant features are temporally sparse and related.\nTransformer models have been used in sequence tasks,\nsuch as NLP by (Devlin et al. 2018) and MTS prediction\nby (Zhou et al. 2021). They do not suffer the vanishing gra-\ndient problem described by (Le and Zuidema 2016), allow-\ning them to learn more temporally distant relationships. Ap-\nplication of transformer models and their underlying Mul-\ntiheaded Self Attention (MHSA) mechanisms may lead to\nperformance gains compared to RNNs.\n(Fawaz et al. 2019) notes that time series augmentation\nlacks a thorough study, compared to NLP and Computer Vi-\nsion. Augmentations techniques in Computer Vision, such\nas cutmix (Yun et al. 2019), may be applicable to MTS data.\nThe datasets used by (Fawaz et al. 2019) do not exceed\n1024 timesteps, except the WalkVsRun dataset consisting\nof only 28 training and 16 test examples. To the authors’\nknowledge, there are no MTS datasets that are not simu-\nlated, have greater length than 1024, and have labeled exam-\nples greater than 5000. Datasets meeting this criteria provide\nmore realistic benchmarks for many real world applications,\nespecially those related to engineering systems and predic-\ntive maintenance.\nDataset and Data Collection\nThe NGAFID serves as a repository for general aviation\nflight data, with a web portal for viewing and tracking flight\nsafety events for individual pilots as well as for fleets of\naircraft (Karboviak et al. 2018). The NGAFID currently\ncontains over 900,000 hours of flight data generated by\nover 780,000 flights by 12 different types of aircraft, pro-\nvided by 65 fleets and individual users, resulting in over\n3.15 billion per second flight data records across 103 po-\ntential flight data recorder parameters. Five years of tex-\ntual maintenance records from a fleet which provided data\nto the NGAFID have been clustered by maintenance issue\ntype and then validated by domain experts for the Maint-\nNet project (Akhbardeh, Desell, and Zampieri 2020). Flights\nwere extracted from the NGAFID and labeled as before or\nafter the date of the maintenance action, creating a MTS\ndataset for training predictive maintenance models.\nMaintNet’s maintenance record logbook data was clus-\ntered into 39 different maintenance issue types. Because\nsome issues occurred very rarely, this work focused on the\ntwo largest clusters, representing the most common main-\ntenance issues: cluster 28 (C28): intake gasket leak/dam-\nage and cluster 37 (C37): rocker cover loose/leak/damage.\nThe C28 and C37 clusters contain 1674 and 1248 main-\ntenance records, respectively. Using the tail number from\nthese maintenance records, the five flights preceding any of\nthese maintenance records were exported from the NGAFID\nto represent flight data relating to those maintenance issues.\nTo provide a robust set of “good” flights without mainte-\nnance issues to compare these against, the five flights after\nthe maintenance issues were exported as well, unless they\nwere within 5 flights of any other maintenance issue. Flights\nshorter than 30 minutes were excluded as these are typically\ndo not involve any actual flight. Flights were then further fil-\ntered within 2 days of maintenance (before and after). As the\nmaintenance records only provided a day (and not a time)\nof action, flights occurring on the same day as maintenance\nwere excluded as it was not possible to determine if they\noccurred before or after maintenance.\nThis resulted in a benchmark dataset containing 7,505\nflight data files representing 11,500 hours of Cessna 172S\nflight data, with each flight data file in this dataset consisting\nof data from 23 sensors (internal, external and operational\nsensors, e.g., engine RPM, oil temperature, oil pressure, gas-\nket temperature, airspeed, pitch, roll, outside air tempera-\nture) recorded every second, with each flight labeled as pre\nor post maintenance. Flights are split for the two mainte-\nnance issues resulting in 1432 pre and 984 post examples\nfor C37 and 2814 pre and 2275 post examples for C28.\nBackground\nA major goal of this work is to be able to classify flights\nas problematic (leading to some maintenance issue), or in\ngood condition (post maintenance). Three factors make this\ndataset challenging. First is the sequence length, often ex-\nceeding 3600 time steps. Second is the nature of the predic-\ntion task, where the goal is to detect features relevant for\nclassification. Third is the significant impact of unobserv-\nable variables, such as pilot actions, on the engine outputs.\nTo formalize the problem, we seek to predict the proba-\nbility that a time series was generated by a pre or post main-\ntenance flight given the flight sensor data. This can be ex-\npressed as P(Yi|Xi). We have access to the variable Ximt\nas a matrix containing the flight sensor data, with imt rep-\nresenting the ith flight’s mth variable at timestep t. Yi rep-\nresents the ith flight’s pre or post maintenance state as 1 and\n0, respectively. Uit represents the pilot’s actions for the ith\nflight’s timestep t. This unknown variable U is significant\nbecause it changes our understanding of the function gen-\nerating X to f(Ui, Yi) = Xi. A pilot’s actions can impact\nXimt more than maintenance state of the aircraft.\nWe cannot construct a model to to predictXim(t+1) using\nonly the past timesteps of Xi due to the impact of Ui. Sim-\nilarly, a compressed representation c(X) may be useless for\nclassification because it must first explain variance caused\nby U. The authors believe that non-deep learning methods\nwill struggle to perform well in these conditions.\nThis dataset provides an exciting challenge compared to\nindustrial datasets, such as power plant data, because it mea-\nsures a dynamic system that changes arbitrarily in a largely\nuncontrolled and inconsistent environment. Routine flight\noperations, such as landing and takeoff, can vary signifi-\ncantly from flight to flight due to the experience of the pilot,\nthe weather, and wind conditions. We hope this dataset can\nserve as a challenging benchmark for MTS classification.\nModel Architecture and Training\nAugmentation To address the limitations of the size of\nthis dataset, we looked into augmentations for MTS data. We\nconsidered only basic domain augmentation methods based\non the taxonomy proposed by (Wen et al. 2020), as advanced\ndomain augmentations are too complex, requiring one to\ntrain generative models. Basic time domain methods, as de-\nscribed by (Le Guennec, Malinowski, and Tavenard 2016)\n12637\nand (Wen et al. 2020), include window slicing (training on\nslices of the MTS) and window warping (reducing or ex-\ntending the length of a segment of the MTS). These methods\nwere not seen to be applicable as window slicing should fail\nif features for classification are temporally distant and infre-\nquent, and window warping may not be applicable if the data\nis not sinusoidal in nature, which this data is not.\nThe authors of this paper decided to explore new basic do-\nmain augmentations, inspired by highly effective augmenta-\ntions for image classification. We consider the ideas cutout\nfrom (DeVries and Taylor 2017), mixup from (Zhang et al.\n2017), and cutmix from (Yun et al. 2019). To our knowledge,\nthis paper is the first to evaluate the these augmentations on\nMTS, due to their absence in the surveys on MTS augmen-\ntation by (Wen et al. 2020) and (Iwana and Uchida 2021).\nTemporal cutout selects a random time segment and set of\nchannels from a MTS and sets selected values to 0. Temporal\ncutmix selects a random time segment from the first MTS\nand a random time segment of a random second MTS of any\nlabel. It then selects a random set of channels and replaces\nthe first MTS’s segment’s channel values with those of the\nsecond. Temporal mixup multiplies a randomly selected set\nof channels in the first MTS by a value, m, and then adds\nall values from the same set of channels from a randomly\nselected second MTS of any label multiplied by 1 −m.\nConvolutional Multi-Headed Self Attention Multi-\nHeaded Self Attention (MHSA) modules were popularized\nby (Devlin et al. 2018) for usage in Natural Language\nProcessing and by (Dosovitskiy et al. 2020) for Computer\nVision, but published usage of MHSA in MTS data is\nless common than the previous two applications. Both\n(Song et al. 2018) and (Rußwurm and K ¨orner 2020) use\nMHSA for MTS classification, but neither implements a 1D\nconvolution followed by MHSA. (Karim et al. 2019) utilizes\nattention mechanisms in an Attention-LSTM network, but\nnot MHSA. The benefits of a low level convolution prior\nto MHSA is demonstrated by (Gulati et al. 2020), where\nconvolutions can capture basic local relationships with high\nefficiency while MHSA handles global relationships.\nAt the time of writing, the authors are not aware of any\npublication that evaluates a convolutional MHSA model for\nMTS classification. This model implements attention layers\nthat mimic the functionality of the encoder layers present\nin BERT (Devlin et al. 2018). Instead of token embeddings,\nthe model generates sequence embeddings with the use of\n1D convolutions along the temporal dimension. These learn-\nable sequence embeddings capture local relationships and to\ncompress the MTS to a shorter length.\nMHSA modules offer significant benefits over LSTMs\nwhen applied to MTS. In particular, (Zhou et al. 2021) has\nshown the capacity for MHSA to model long term rela-\ntionships in time series data. Furthermore, when applied to\nlonger sequences, MHSA avoids the problems associated\nwith a vanishing gradient as described by (Le and Zuidema\n2016). Not only does it better model long term relationships,\nthere is a significant computational efficiency over RNNs.\nThe Conv-MHSA model evaluated in this paper (see Fig-\nure 1) uses a series of 1D convolutions to reduce the tempo-\nFigure 1: Layers and output shapes of the Conv-MHSA\nmodel. The first dimension represents time and the second\nrepresents channels. Note that white the boxes do not per-\nform an operation, but mark significant states in the network.\nral resolution from 4096 to 512 and then employs 4 stacked\nMHSA encoder layers with 8 heads each and 64 dense units\nper head. The output is globally average pooled and fed to a\ndense layer for classification.\nConvolutional Long Short Term Memory Networks\n(Keren and Schuller 2016) present a 1 dimensional convo-\nlutional LSTM as an enhancement to the traditional RNN.\nBy using a 1 dimensional convolution, it is possible to ex-\ntract features from the sequence before the LSTM layers and\nreduce MTS temporal resolution.\nWe consider two Conv-LSTM models. The first, referred\nto as Conv-LSTM, utilizes the same series of 1D convo-\nlutions as the Conv-MHSA model, but instead employs 4\nstacked Bidirectional LSTMs with 512 units. The output is\nglobally average pooled and fed to a dense layer for classifi-\ncation. The second Conv-LSTM is referred to as EX-Conv-\nLSTM, which utilizes 2 additional 1D convolutions before\nthe stacked Bidirectional LSTMs to further reduce the tem-\nporal resolution to 128.\nConvolutional GRU Variational Auto Encoders Varia-\ntional Autoencoders (V AE) were popularized by (An and\nCho 2015) for anomaly detection. V AE’s assume that a sys-\ntem’s observable outputs X can be described via a vector\nor embedding E, generated by the encoder component of\nthe V AE model. WhenX cannot be described via a model\ngenerated embedding E, it indicates anomalous activity. The\nability to describe X via E is based on the ability of a de-\ncoder model to reconstruct X using only E and is measured\nas the reconstruction error. V AE learn their embeddings as\na Gaussian distribution, with Kullback–Leibler divergence\n(KLD) for regularization. (An and Cho 2015) shows better\nperformance for V AEs over standard Autoencoders.\nGRU based V AE models have been employed by (Guo\net al. 2018) for anomaly detection in MTS data. For clas-\nsification, this approach trains on within class data (post\nmaintenance) with the expectation that out of class data (pre\nmaintenance) will have greater reconstruction error.\nWe implemented a V AE-Conv-GRU that uses 1D convo-\nlutions to reduce the temporal resolution to 256, followed\nby a bidirectional GRU (BD-GRU) with 256 units, then a\n12638\nModel Step Time in ms Parameters\nC.MHSA 50 7.9M\nC.LSTM 800 24.7M\nEX C.LSTM 220 28.4M\nV AE-Conv-GRU 130 18.3M\nTable 1: Approximate Training Step Time in Miliseconds\n1D convolution to reduce the temporal resolution to 128, fol-\nlowed by another BD-GRU with 512 units. An embedding of\n512 mixture Gaussian distributions and 8 mixtures per distri-\nbution is generated from the BD-GRU outputs, regularized\nvia KLD. The decoder structure matches the encoder, with\n1D Transposed Convolutions for the purpose of expanding\nthe temporal resolution. Augmentations were not used.\nTraining Setup All results reported were generated us-\ning a Google Colab instance with a v2-8 TPU. All models\nwere trained for 30 epochs using a batch size of 32, with\n5-fold cross validation. Steps per epoch are 250 for MHSA\nand LSTM models, 1000 for V AE models, and 500 for ex-\ntended training LSTM models. Flights are truncated to the\nlast 4096 time steps and padded to be of the same size.\nTo ensure the validation data is a good measure of gener-\nalized performance, the validation data is only composed of\nflights from tail numbers (unique identifier for planes) not\npresent in the training data. Classification models used an\nAdam optimizer with a decaying learning rate starting at 1e-\n5 for MHSA and 2e-5 for LSTM and V AE models used an\nAdam optimizer with a decaying learning rate starting at 1e-\n4. Each augmentation (temporal cutout, cutmix, and mixup)\nwas performed on a MTS with a 40% chance. The time seg-\nment length for cutout and cutmix was selected uniformly\nat random between 64 and 512. Each channel had a 30%\nchance of being selected for cutout and cutmix. For tempo-\nral mixup, m was selected uniformly at random between 0.6\nand 0.9, and it was applied to all time steps, with channels\nbeing selected with a 40% chance.\nResults\nComputational Efficiency We observe significant com-\nputational advantages in the training of the Conv-MHSA\ncompared to all other models. When using a TPU, the train-\ning step time (time to train on 1 batch) of Conv-MSHA is at\nleast 4x faster than Extra-Conv-LSTM and at least 15x faster\nthan Conv-LSTM. The results are summarized in Table 1.\nSome of these advantages in step time could be caused by\nTPUs, which utilize matrix multiplication units (MXU’s).\nPerformance may differ on GPU systems.\nClassification Performance We evaluate each model’s\nArea Under the Curve score for Precision-Recall (PR) and\nReceiver Operating Characteristic (ROC). These threshold\nindependent metrics better measure generalized model per-\nformance than accuracy. Accuracy (ACC) is excluded from\nanalysis because it depends on defining a threshold for pre-\ndictions, which may be misleading due to class imbalance.\nBinary Cross Entropy loss in also considered as a metric\nModel Type A Loss ROC PR ACC\nC28\nC.LSTM Y 0.630 0.701 0.654 0.653\nN 0.617 0.742 0.697 0.685\nC.LSTM+ Y 0.623 0.730 0.644 0.691\nN 0.613 0.757 0.711 0.694\nC.MHSA Y 0.528 0.826 0.802 0.744\nN 0.557 0.819 0.792 0.751\nEX C.LSTM Y 0.612 0.725 0.678 0.667\nN 0.614 0.755 0.713 0.694\nEX C.LSTM+ Y 0.608 0.764 0.699 0.718\nN 0.612 0.785 0.737 0.733\nC37\nC.LSTM Y 0.643 0.674 0.567 0.655\nN 0.679 0.553 0.489 0.596\nC.LSTM+ Y 0.635 0.723 0.639 0.693\nN 0.644 0.711 0.618 0.683\nC.MHSA Y 0.601 0.775 0.711 0.723\nN 0.680 0.559 0.485 0.590\nEX C.LSTM Y 0.632 0.708 0.620 0.677\nN 0.640 0.709 0.608 0.680\nEX C.LSTM+ Y 0.639 0.731 0.643 0.699\nN 0.651 0.714 0.619 0.681\nTable 2: Mean of the best metrics for each configuration.\nLSTM + models are trained for 500 steps per epoch. C.\nstands for Conv. A stands for augmented.\nto evaluate model overconfidence in wrong predictions. Re-\nsults for V AE-Conv-GRU models are excluded from the ta-\nble due to poor performance. See Table 2.\nResults indicate that Conv-MHSA models consistently\nperform better than Conv-LSTM models by a wide margin.\nEven when Conv-LSTM models are given twice the num-\nber of training steps, they fail to reach the performance of\nMHSA models.\nClassification using V AE-Conv-GRU While the V AE-\nConv-GRU model is capable of achieving a validation Root\nMean Squared error of 0.0338, it cannot predict pre or post\nmaintenance. With mean squared error as the reconstruction\nloss for comparing within class and out of class examples,\nthe PR-AUC and ROC-AUC values never exceed 0.55.\nDiscussion\nTemporally Distant Attention To explore the question\nas to why MHSA can achieve better performance on this\ndataset compared to RNNs, it is important to observe how\nthe various heads attend to different positions of the se-\nquence. Figure 2 is a visualisation of the 4 MHSA layers\nwith multiple input datapoints. We can clearly observe in\nsample 0 that some layers are attending to time steps that\nare 300 units apart. An RNN model may have great diffi-\nculty in propagating information from time step 50 to time\nstep 400 due to memory degradation and vanishing gradi-\nents. MHSA allows any time step to attend to any other time\nstep and better capture temporally distant relationships.\nTo further show that the relationship between temporally\ndistant features is necessary for classification, we attempted\nto train a Short-LSTM network using randomly sampled\nslices 128 time steps long. This network uses 4 stacked 512\nunit bidirectional LSTMs followed by global average pool-\n12639\nFigure 2: Attention maps illustrate how each time step at-\ntends to other timesteps in MHSA. This shows 3 MHSA\nLayers for 2 different datapoints from validation data. Y\nAxis represents Query and X axis represent Key. Sample 2\nis positive and sample 1 is negative. Bright sections are im-\nportant timesteps that the model focuses on.\ning and a dense sigmoid layer for classification. This Short-\nLSTM network did not perform significantly better than a\nfully random predictor, outputting random floating point val-\nues between 0 and 1. This demonstrates that random sub\nsamples from the overall MTS is not sufficient.\nAugmentation The 3 augmentations of cutout, mixup, and\ncutmix, have similar functionality as dropout, described by\n(Srivastava et al. 2014). While it may seem counter intu-\nitive to generate unrealistic sequences, these augmentations\npenalize the model for memorizing a small subset of time\nsteps by removing or modifying them. Like their computer\nvision equivalents, these augmentations help models learn\nmore resilient representations and improve generalization.\nAugmentations are particularly important for Conv-\nMHSA networks, which are prone to overfitting on small\ndatasets. Conv-LSTM networks do not overfit and may not\nbenefit from augmentation. Results from experiments on\nConv-MHSA models show a small advantage in the mean of\nall metrics when training on the C28 dataset, but a significant\nadvantage when training on the C37 dataset. These differ-\nences are significant, such that Conv-MHSA models trained\non C37 without augmentation perform not significantly bet-\nter than random guessing. This is most likely caused by a\ndifference in the dataset size, where C37 is about half the\nsize of C28. This difference may also be caused by a differ-\nence in the nature of the data, where it is possible that C28 is\neasier to generalize on than C37. Figure 3 shows the Conv-\nMHSA overfitting when training without augmentations on\nthe C37 dataset.\nV AE and Reconstruction Loss Figure 4 shows that the\nreconstruction loss is the same for both classes. This likely\nbecause a significant portion of the variance in X is caused\nby an unobservable variable U. Any V AE model would first\nseek to learn how U impacts X. Based on the analysis of\nMHSA on this dataset, there may be only a few, short seg-\nFigure 3: Validation Loss by epoch for Conv-MHSA model\non the C37 dataset.\nFigure 4: The Y axis indicates percentage of validation data-\npoints having more MSE than the number in X axis. The or-\nange and blue lines represent pre and post maintenance, re-\nspectively. The distributions show no significant difference.\nments of the MTS that are actually useful for classification.\nThis suggests that V AE methods may struggle.\nFuture Research The NGAFID-MC dataset can be used\nto evaluate a wide variety of models and approaches, such as\nTCNN (Assaf et al. 2019) and dynamic time warping (Seto,\nZhang, and Zhou 2015). Further studies can be performed\non the full flight sequences, rather than only the last 4096\nseconds of the flight. Future work should also evaluate mut-\nliclass classification to identify which issue is present. We\nalso intend to expand the NGAFID-MC dataset with more\nmaintenance issue cluster types, as well as refinements based\non user annotations and as additional maintenance records\nare received.\nThe Conv-MHSA architecture performs much better than\nConv-LSTM models on this dataset and it would be interest-\ning to evaluate this on other datasets. It is also plausible that\nwe can improve Conv-MHSA architecture by incorporating\nmemory efficient methods described by (Kitaev, Kaiser, and\nLevskaya 2020). Additional work should be done on alterna-\ntive loss functions for MTS classification, such as focal loss\n12640\nFigure 5: Screenshots showing the integration of the maintenance prediction models into the NGAFID user interface.\n(Lin et al. 2017) and label smoothing (Szegedy et al. 2016).\nCutout, mixup, and cutmix augmentations should be eval-\nuated against other MTS augmentation methods and models.\nDue to the limited size of many MTS datasets and the cost\nto acquire data, further study into augmentation can increase\nthe viability of MTS classification methods for general use.\nLimitations There may be mislabeled datapoints due to\nthe nature of airplane maintenance. A reported maintenance\nissue may not be fully fixed or an issue was falsely identi-\nfied by the pilot. If given resources, the authors would like\nto construct a small and rigorously annotated test set of data\n(1000 examples) with the help of domain experts. Addition-\nally, the flights which occured on the day of maintenance\nwere not included, these will be included in the future as\nthey are annotated by domain experts.\nNGAFID Deployment and Integration\nThe NGAFID provides a set of utilities for Flight Data Mon-\nitoring (FDM), which allow users to access the per-second\ntime series data and perform various analytics. We added\nadditional functionality to calculate and display the proba-\nbility that a flight may require maintenance for the Cessna\n172S aircraft type for flights exceeding 30 minutes (see Fig-\nure 5). This includes a feedback system was created to give\nusers the ability to rate the accuracy of P(Yi|Xi) using a\nthree-point scale (accurate, inaccurate or unsure), based on\ntheir knowledge of aviation and aircraft maintenance. This\nallows users to provide valuable feedback and labeled data\nfor refining and improving future models.\nHowever, there are infrastructure challenges that need to\nbe addressed before NGAFID can provide real time predic-\ntive maintenance alerts to improve safety and reduce costs.\nThe main obstacle is the lack of wireless flight data trans-\nmission (WFDT), which is more common in commercial\naviation settings. The current data import process for the\nNGAFID occurs weekly and requires ground crews to man-\nually extract and upload the data. NGAFID partner fleets are\nin the process of deploying WFDT systems that will allow\nthe NGAFID to perform real time predictive maintenance,\nas the WFDT systems can upload data immediately after an\naircraft lands and returns to the hangar.\nConclusion\nWe demonstrate the challenging nature of the NGAFID-MC\ndataset and its value for assessing various MTS approaches.\nWhile some datasets exceed NGAFID-MC in terms of data-\npoints or sequence length, the authors are not aware of any\ndataset that has both greater datapoints and sequence length.\nThe authors are also not aware of any other MTS dataset\nthat tracks a dynamic system that changes arbitrarily in a\nlargely uncontrolled and inconsistent environment. Further-\nmore, we demonstrate that this dataset contains temporally\ndistant relationships that previous MTS classification meth-\nods struggle with. We hope that the difficulty of this dataset\nwill inspire new and better methods for MTS classification.\nWe also introduce a more computationally efficient and\nperformant architecture, the Conv-MHSA. This architecture\ncan better capture temporally distant relationships in long\nsequences and it does so with at much greater computational\nefficiency than RNN methods. We also show that cutmix,\ncutout, and mixup augmentations can significantly improve\ngeneralization.\nThe ability to differentiate between pre and post mainte-\nnance flights leads can provide a significant benefit to the do-\nmain of general aviation. Early detection of maintenance is-\nsues has the potential to reduce long term maintenance costs\nby catching issues before they cause more serious problems.\nBy detecting the need for maintenance one or two days prior\nto maintenance, we can minimize the amount of flight hours\nthat a pilot spends on compromised aircraft, leading to in-\ncreased safety. We have already incorporated preliminary\nmodels for maintenance classification for NGAFID, which\nwill allow us to gather feedback from users to further refine\nand improve the early maintenance issue detection system.\nWe hope that these tools will lead to increased safety and\nreduced costs for general aviation.\nReferences\nAkhbardeh, F.; Desell, T.; and Zampieri, M. 2020. Maint-\nNet: A Collaborative Open-Source Library for Predictive\nMaintenance Language Resources. In Proceedings of the\n28th International Conference on Computational Linguis-\ntics: System Demonstrations, 7–11. Barcelona, Spain (On-\nline): International Committee on Computational Linguis-\ntics (ICCL).\n12641\nAn, J.; and Cho, S. 2015. Variational autoencoder based\nanomaly detection using reconstruction probability. Special\nLecture on IE, 2(1): 1–18.\nAssaf, R.; Giurgiu, I.; Bagehorn, F.; and Schumann, A. 2019.\nMtex-cnn: Multivariate time series explanations for predic-\ntions with convolutional neural networks. In 2019 IEEE In-\nternational Conference on Data Mining (ICDM), 952–957.\nIEEE.\nCarvalho, T. P.; Soares, F. A. A. M. N.; Vita, R.; da P. Fran-\ncisco, R.; Basto, J. P.; and Alcal´a, S. G. S. 2019. A system-\natic literature review of machine learning methods applied\nto predictive maintenance. Computers and Industrial Engi-\nneering, 137: 106024.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDeVries, T.; and Taylor, G. W. 2017. Improved regulariza-\ntion of convolutional neural networks with cutout. arXiv\npreprint arXiv:1708.04552.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFawaz, H. I.; Forestier, G.; Weber, J.; Idoumghar, L.; and\nMuller, P.-A. 2019. Deep learning for time series classifica-\ntion: a review.Data mining and knowledge discovery, 33(4):\n917–963.\nGulati, A.; Qin, J.; Chiu, C.-C.; Parmar, N.; Zhang, Y .;\nYu, J.; Han, W.; Wang, S.; Zhang, Z.; Wu, Y .; et al. 2020.\nConformer: Convolution-augmented transformer for speech\nrecognition. arXiv preprint arXiv:2005.08100.\nGuo, Y .; Liao, W.; Wang, Q.; Yu, L.; Ji, T.; and Li, P. 2018.\nMultidimensional time series anomaly detection: A gru-\nbased gaussian mixture variational autoencoder approach. In\nAsian Conference on Machine Learning, 97–112. PMLR.\nIwana, B. K.; and Uchida, S. 2021. An empirical survey of\ndata augmentation for time series classification with neural\nnetworks. Plos one, 16(7): e0254841.\nKarboviak, K.; Clachar, S.; Desell, T.; Dusenbury, M.;\nHedrick, W.; Higgins, J.; Walberg, J.; and Wild, B. 2018.\nClassifying aircraft approach type in the national general\naviation flight information database. In International Con-\nference on Computational Science, 456–469. Springer.\nKarim, F.; Majumdar, S.; Darabi, H.; and Chen, S. 2017.\nLSTM fully convolutional networks for time series classifi-\ncation. IEEE access, 6: 1662–1669.\nKarim, F.; Majumdar, S.; Darabi, H.; and Harford, S.\n2019. Multivariate LSTM-FCNs for time series classifica-\ntion. Neural Networks, 116: 237–245.\nKeren, G.; and Schuller, B. 2016. Convolutional RNN: an\nenhanced model for extracting features from sequential data.\nIn 2016 International Joint Conference on Neural Networks\n(IJCNN), 3412–3419. IEEE.\nKitaev, N.; Kaiser, Ł.; and Levskaya, A. 2020. Reformer:\nThe efficient transformer. arXiv preprint arXiv:2001.04451.\nLe, P.; and Zuidema, W. 2016. Quantifying the vanishing\ngradient and long distance dependency problem in recur-\nsive neural networks and recursive LSTMs. arXiv preprint\narXiv:1603.00423.\nLe Guennec, A.; Malinowski, S.; and Tavenard, R. 2016.\nData augmentation for time series classification using con-\nvolutional neural networks. In ECML/PKDD workshop on\nadvanced analytics and learning on temporal data.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll ´ar, P.\n2017. Focal loss for dense object detection. In Proceedings\nof the IEEE international conference on computer vision ,\n2980–2988.\nOrsenigo, C.; and Vercellis, C. 2010. Combining discrete\nSVM and fixed cardinality warping distances for multivari-\nate time series classification. Pattern Recognition, 43(11):\n3787–3794.\nRußwurm, M.; and K ¨orner, M. 2020. Self-attention for raw\noptical satellite time series classification. ISPRS Journal of\nPhotogrammetry and Remote Sensing, 169: 421–435.\nSeto, S.; Zhang, W.; and Zhou, Y . 2015. Multivariate time\nseries classification using dynamic time warping template\nselection for human activity recognition. In2015 IEEE Sym-\nposium Series on Computational Intelligence, 1399–1406.\nIEEE.\nSong, H.; Rajan, D.; Thiagarajan, J. J.; and Spanias, A. 2018.\nAttend and diagnose: Clinical time series analysis using at-\ntention models. In Thirty-second AAAI conference on artifi-\ncial intelligence.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: a simple way to prevent\nneural networks from overfitting. The journal of machine\nlearning research, 15(1): 1929–1958.\nSzegedy, C.; Vanhoucke, V .; Ioffe, S.; Shlens, J.; and Wojna,\nZ. 2016. Rethinking the inception architecture for computer\nvision. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2818–2826.\nWang, Z.; Yan, W.; and Oates, T. 2017. Time series clas-\nsification from scratch with deep neural networks: A strong\nbaseline. In 2017 International joint conference on neural\nnetworks (IJCNN), 1578–1585. IEEE.\nWen, Q.; Sun, L.; Yang, F.; Song, X.; Gao, J.; Wang, X.;\nand Xu, H. 2020. Time series data augmentation for deep\nlearning: A survey. arXiv preprint arXiv:2002.12478.\nYun, S.; Han, D.; Oh, S. J.; Chun, S.; Choe, J.; and Yoo,\nY . 2019. Cutmix: Regularization strategy to train strong\nclassifiers with localizable features. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n6023–6032.\nZhang, H.; Cisse, M.; Dauphin, Y . N.; and Lopez-Paz, D.\n2017. mixup: Beyond empirical risk minimization. arXiv\npreprint arXiv:1710.09412.\nZhou, H.; Zhang, S.; Peng, J.; Zhang, S.; Li, J.; Xiong, H.;\nand Zhang, W. 2021. Informer: Beyond efficient transformer\nfor long sequence time-series forecasting. In Proceedings of\nAAAI.\n12642",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.8581035137176514
    },
    {
      "name": "Computer science",
      "score": 0.7431086301803589
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6255703568458557
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5772026181221008
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5272250175476074
    },
    {
      "name": "Generalization",
      "score": 0.523377537727356
    },
    {
      "name": "Machine learning",
      "score": 0.5207545757293701
    },
    {
      "name": "Predictive maintenance",
      "score": 0.47007280588150024
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4566539525985718
    },
    {
      "name": "Data mining",
      "score": 0.36908987164497375
    },
    {
      "name": "Artificial neural network",
      "score": 0.22796058654785156
    },
    {
      "name": "Reliability engineering",
      "score": 0.17373019456863403
    },
    {
      "name": "Engineering",
      "score": 0.11186128854751587
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I155173764",
      "name": "Rochester Institute of Technology",
      "country": "US"
    }
  ]
}