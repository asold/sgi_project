{
  "title": "PVT v2: Improved baselines with pyramid vision transformer",
  "url": "https://openalex.org/W3175515048",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101827340",
      "name": "Wenhai Wang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A5041031140",
      "name": "Enze Xie",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100693026",
      "name": "Xiang Li",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5056294284",
      "name": "Deng-Ping Fan",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5028035527",
      "name": "Kaitao Song",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100751872",
      "name": "Ding Liang",
      "affiliations": [
        "Group Sense (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5061696740",
      "name": "Tong Lü",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100752686",
      "name": "Ping Luo",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5082634513",
      "name": "Ling Shao",
      "affiliations": [
        "Inception Institute of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1583837637",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4214709605",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W6630441560",
    "https://openalex.org/W3176187859",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W6600662924",
    "https://openalex.org/W6635487051",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W6600175266",
    "https://openalex.org/W6604344240",
    "https://openalex.org/W2912083425",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W3035396860",
    "https://openalex.org/W3172752666",
    "https://openalex.org/W2604738573",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W4297779827",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W2995426144",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3102710196",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3110402800",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W4288325606",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3133696297"
  ],
  "abstract": "Transformer recently has presented encouraging progress in computer vision.\\nIn this work, we present new baselines by improving the original Pyramid Vision\\nTransformer (PVT v1) by adding three designs, including (1) linear complexity\\nattention layer, (2) overlapping patch embedding, and (3) convolutional\\nfeed-forward network. With these modifications, PVT v2 reduces the\\ncomputational complexity of PVT v1 to linear and achieves significant\\nimprovements on fundamental vision tasks such as classification, detection, and\\nsegmentation. Notably, the proposed PVT v2 achieves comparable or better\\nperformances than recent works such as Swin Transformer. We hope this work will\\nfacilitate state-of-the-art Transformer researches in computer vision. Code is\\navailable at https://github.com/whai362/PVT.\\n",
  "full_text": "Computational Visual Media\nhttps://doi.org/10.1007/s41095-022-0274-8 Vol. 8, No. 3, September 2022, 415–424\nResearch Article\nPVT v2: Improved baselines with Pyramid Vision Transformer\nWenhai Wang1,2 (\u0000 ), Enze Xie3, Xiang Li4, Deng-Ping Fan5, Kaitao Song4, Ding Liang6, Tong Lu2,\nPing Luo3, and Ling Shao7\nc⃝ The Author(s) 2022.\nAbstract Transformers have recently lead to encourag-\ning progress in computer vision. In this work, we present\nnew baselines by improving the original Pyramid Vision\nTransformer (PVT v1) by adding three designs: (i) a\nlinear complexity attention layer, (ii) an overlapping\npatch embedding, and (iii) a convolutional feed-forward\nnetwork. With these modifications, PVT v2 reduces\nthe computational complexity of PVT v1 to linearity\nand provides significant improvements on fundamental\nvision tasks such as classification, detection, and\nsegmentation. In particular, PVT v2 achieves comparable\nor better performance than recent work such as the Swin\ntransformer. We hope this work will facilitate state-of-\nthe-art transformer research in computer vision. Code is\navailable at https://github.com/whai362/PVT.\nKeywords transformers; dense prediction; image\nclassiﬁcation; object detection; semantic\nsegmentation\n1 Introduction\nRecent studies on transformers for computer vision\n1 Shanghai AI Laboratory, Shanghai 200232, China.\nE-mail: wangwenhai362@gmail.com ( \u0000 ).\n2 Department of Computer Science and Technology, Nanjing\nUniversity, Nanjing 210023, China. E-mail: lutong@nju.edu.cn.\n3 Department of Computer Science, the University of\nHong Kong, Hong Kong 999077, China. E-mail: E. Xie,\nxieenze@hku.hk; P. Luo, pluo@cs.hku.hk.\n4 School of Computer Science and Engineering, Nanjing\nUniversity of Science and Technology, Nanjing 210014,\nChina. E-mail: X. Li, xiang.li.implus@njust.edu.cn;\nK. Song, kt.song@njust.edu.cn.\n5 Computer Vision Lab, ETH Zurich, Zurich 8092,\nSwitzerland. E-mail: dengpfan@gmail.com.\n6 SenseTime, Beijing 100080, China. E-mail: liangding@\nsensetime.com.\n7 Inception Institute of Artificial Intelligence, Abu Dhabi,\nUnited Arab Emirates. E-mail: ling.shao@inceptioniai.org.\nManuscript received: 2021-12-22; accepted: 2022-02-08\nare converging on the backbone network [ 1–8]f o r\ndownstream vision tasks, such as image classifica-\ntion, object detection, and instance and semantic\nsegmentation. To date, there have been promising\nresults. For example, Vision Transformer (ViT) [\n1]\nfirst showed that a pure transformer can archive state-\nof-the-art performance in image classification. The\nPyramid Vision Transformer (PVT v1) [\n3]s h o w e d\nthat a pure transformer backbone can also surpass\nCNN counterparts for dense prediction tasks such\nas detection and segmentation [\n9–11]. Later, Swin\ntransformer [ 5], CoaT [ 6], LeViT [ 7], and Twins\n[8] further improved classification, detection, and\nsegmentation performance with transformer backbones.\nThis work aims to establish stronger and more\nfeasible baselines built on the PVT v1 framework.\nWe report three design improvements: (i) a linear\ncomplexity attention layer, (ii) an overlapping patch\nembedding, and (iii) a convolutional feed-forward\nnetwork, which are orthogonal to the PVT v1\nframework, and when used with it, can bring better\nimage classiﬁcation, object detection, and instance\nand semantic segmentation results. We call the\nimproved framework PVT v2; it has 6 diﬀerent size\nvariants, from B0 to B5 according to the number\nof parameters. In particular, PVT v2-B5 yields\nan 83.8% top-1 error on ImageNet, better than\nSwin-B [5] and Twins-SVT-L [8], while having fewer\nparameters and using fewer GFLOPs. Moreover,\nGFL [12] with PVT-B2 archives 50.2 AP on COCO\n2017 val, 2.6 AP higher when using Swin-T [ 5], and\n5.7 AP higher when using ResNet50 [ 13]. We hope\nthese improved baselines will provide a reference for\nfuture research on vision transformers.\n2 Related work\nWe now discuss transformer backbones related to\n415\n\n416 W. Wang, E. Xie, X. Li, et al.\nthis work. ViT [ 1] treats each image as a sequence\nof tokens (patches) with a ﬁxed length, and then\nfeeds them to multiple transformer layers to perform\nclassiﬁcation. It was the ﬁrst work to demonstrate\nthat a pure transformer can archive state-of-the-art\nimage classiﬁcation results given suﬃcient training\ndata (e.g., ImageNet-22k [ 14], JFT-300M). DeiT [ 2]\nfurther explores a data-eﬃcient training strategy and\na distillation approach for ViT.\nTo improve image classiﬁcation results, recent\nmethods make tailored changes to ViT. T2T\nViT [ 15] progressively concatenates tokens within\nan overlapping sliding window into a single token.\nTNT [16] utilizes inner and outer transformer blocks\nto generate pixel and patch embeddings. CPVT [ 17]\nreplaces the ﬁxed size position embedding in ViT with\nconditional position encodings, making it easier to\nprocess images of arbitrary resolution. CrossViT [ 18]\nprocesses image patches of diﬀerent sizes via a dual-\nbranch transformer. LocalViT [ 19] incorporates\ndepth-wise convolution into vision transformers to\nimprove the local continuity of features.\nTo adapt to dense prediction tasks such as\nobject detection, instance and semantic segmentation,\ncertain methods [ 3–8] introduce the pyramid\nstructure in CNNs to the design of transformer\nbackbones. PVT v1 was the ﬁrst pyramid structure\ntransformer, presenting a hierarchical transformer\nwith four stages, and showing that a pure transformer\nbackbone can be as versatile as CNN counterparts and\nprovide better results for detection and segmentation\ntasks. Later, various improvements [4–8] were made\nto enhance local continuity of features and to remove\nthe ﬁxed size position embedding. For example, the\nSwin transformer [5] replaces the ﬁxed size position\nembedding with relative position biases, and restricts\nself-attention within shifted windows. CvT [ 4],\nCoaT [ 6], and LeViT [ 7] introduce convolution-\nlike operations into vision transformers. Twins [ 8]\ncombines local and global attention mechanisms to\nobtain a stronger feature representation.\n3 Methodology\n3.1 Limitations of PVT v1\nP V Tv 1[ 3] has three main limitations: (i) like\nViT [1], when processing high-resolution input (with\nthe shorter side being 800+ pixels), the computational\nrequirements of PVT v1 are relatively large, (ii) PVT\nv1 treats an image as a sequence of non-overlapping\npatches, which loses local continuity in the image to\na certain extent, and (iii) the position encoding in\nPVT v1 is ﬁxed-size, which is inﬂexible when images\nof arbitrary size must be processed. These problems\nlimit the utility of PVT v1 for vision tasks.\nTo address these issues, we propose PVT v2, which\nimproves PVT v1 through three designs, given in\nSections 3.2–3.4.\n3.2 Linear spatial reduction attention\nFirst, to reduce the high computational cost caused\nby attention operations, we propose a linear spatial\nreduction attention (SRA) layer, illustrated in Fig. 1.\nUnlike SRA [ 3] which uses convolutions for spatial\nreduction, linear SRA uses average pooling to reduce\nthe spatial dimension ( h × w) to a ﬁxed size ( P × P )\nbefore the attention operation. In this way, linear\nSRA enjoys linear computational and memory costs\nlike a convolutional layer. Speciﬁcally, given an input\nof size h × w × c, the complexity of SRA and linear\nSRA are\nΩ(SRA) = 2h2w2c\nR2 + hwc2R2 (1)\nΩ(linear SRA) = 2hwP 2c (2)\nwhere R is the spatial reduction ratio of SRA [3], and\nP is the pooling size of linear SRA, which is set to 7.\n3.3 Overlapping patch embedding\nSecondly, to model the local continuity information,\nwe utilize an overlapping patch embedding to tokenize\nimages. As shown in Fig. 2(a), we enlarge the patch\nwindow, making adjacent windows overlap by half\nof their area, and pad the feature map with zeros to\nkeep the resolution. In this work, we use convolution\nwith zero padding to implement overlapping patch\nembedding. Speciﬁcally, given input of size h × w × c,\nwe feed it to a convolution with stride S,k e r n e ls i z e\nFig. 1 SRA in PVT v1 and linear SRA in PVT v2.\n\nPVT v2: Improved baselines with Pyramid Vision Transformer 417\nFig. 2 Two improvements in PVT v2: (a) overlapping patch\nembedding, (b) convolutional feed-forward network.\n2S − 1, padding size S − 1, and c′ kernels. The output\nsize is (h/S)(w/S )C′.\n3.4 Convolutional feed-forward\nThirdly, inspired by Refs. [17, 19, 20], we remove the\nﬁxed-size position encoding [ 1], and introduce zero\npadding position encoding into PVT. As shown in\nFig. 2(b), we add a 3 × 3 depth-wise convolution [21]\nwith padding size of 1 between the ﬁrst fully-\nconnected (FC) layer and GELU [ 22] in feed-forward\nnetworks.\n3.5 Details of PVT v2 series\nWe scale up PVT v2 from B0 to B5 By changing the\nhyper-parameters, which are as follows for Stage i:\nSi: stride of the overlapping patch embedding\nCi: number of channels of output\nLi: number of encoder layers\nRi: reduction ratio of the SRA\nPi: adaptive average pooling size of the linear SRA\nNi: number of heads of the eﬃcient self-attention\nEi: expansion ratio of the feed-forward layer [23]\nTable 1 gives detailed information for the PVT\nv2 series. Our design follows the principles of\nResNet [24]: (i) the channel dimension increases and\nthe spatial resolution shrinks as the layers get deeper,\nand (ii) Stage 3 has the greatest computational cost.\nT able 1 Detailed settings for PVT v2 series. “-Li” denotes PVT v2 with linear SRA\nOutput size Layer name Pyramid Vision Transformer v2\nB0 B1 B2 B2-Li B3 B4 B5\nStage 1 H\n4 × W\n4\nOverlapping\npatch embedding\nS1 =4\nC1 =3 2 C1 =6 4\nTransformer\nencoder\nR1 =8\nN1 =1\nE1 =8\nL1 =2\nR1 =8\nN1 =1\nE1 =8\nL1 =2\nR1 =8\nN1 =1\nE1 =8\nL1 =3\nP1 =7\nN1 =1\nE1 =8\nL1 =3\nR1 =8\nN1 =1\nE1 =8\nL1 =3\nR1 =8\nN1 =1\nE1 =8\nL1 =3\nR1 =8\nN1 =1\nE1 =4\nL1 =3\nStage 2 H\n8 × W\n8\nOverlapping\npatch embedding\nS2 =2\nC2 =6 4 C2 = 128\nTransformer\nencoder\nR2 =4\nN2 =2\nE2 =8\nL2 =2\nR2 =4\nN2 =2\nE2 =8\nL2 =2\nR2 =4\nN2 =2\nE2 =8\nL2 =3\nP2 =7\nN2 =2\nE2 =8\nL2 =3\nR2 =4\nN2 =2\nE2 =8\nL2 =3\nR2 =4\nN2 =2\nE2 =8\nL2 =8\nR2 =4\nN2 =2\nE2 =4\nL2 =6\nStage 3 H\n16 × W\n16\nOverlapping\npatch embedding\nS3 =2\nC3 = 160 C3 = 320\nTransformer\nencoder\nR3 =2\nN3 =5\nE3 =4\nL3 =2\nR3 =2\nN3 =5\nE3 =4\nL3 =2\nR3 =2\nN3 =5\nE3 =4\nL3 =6\nP3 =7\nN3 =5\nE3 =4\nL3 =6\nR3 =2\nN3 =5\nE3 =4\nL3 =1 8\nR3 =2\nN3 =5\nE3 =4\nL3 =2 7\nR3 =2\nN3 =5\nE3 =4\nL3 =4 0\nStage 4 H\n32 × W\n32\nOverlapping\npatch embedding\nS4 =2\nC4 = 256 C4 = 512\nTransformer\nencoder\nR4 =1\nN4 =8\nE4 =4\nL4 =2\nR4 =1\nN4 =8\nE4 =4\nL4 =2\nR4 =1\nN4 =8\nE4 =4\nL4 =3\nP4 =7\nN4 =8\nE4 =4\nL4 =3\nR4 =1\nN4 =8\nE4 =4\nL4 =3\nR4 =1\nN4 =8\nE4 =4\nL4 =3\nR4 =1\nN4 =8\nE4 =4\nL4 =3\n\n418 W. Wang, E. Xie, X. Li, et al.\n3.6 Advantages of PVT v2\nCombining these improvements, PVT v2 can (i)\nachieve more local continuity of images and feature\nmaps, (ii) process variable-resolution input more\nreadily, and (iii) enjoy the same linear complexity as\na CNN.\n4 Experiments\n4.1 Image classiﬁcation\n4.1.1 Setting\nImage classiﬁcation experiments were performed\non the ImageNet-1K dataset [ 27], which comprises\n1.28 million training images and 50k validation images\nin 1000 categories. All models were trained on the\ntraining set for fair comparison and we report the top-\n1 error on the validation set. We followed DeiT [ 2]\nand applied random cropping, random horizontal\nﬂipping [\n28], label-smoothing regularization [ 29],\nmixup [ 30], and random erasing [ 31] for data\naugmentation. During training, we employed\nAdamW [32] with a momentum of 0.9, a mini-batch\nsize of 128, and a weight decay of 5× 10−2 to optimize\nmodels. The initial learning rate was set to 10 −3\nand decreased following a cosine schedule [ 33]. All\nmodels were trained for 300 epochs from scratch on 8\nV100 GPUs. We applied a 224 × 224 center crop on\nthe validation set for benchmarking to evaluate the\nclassiﬁcation accuracy.\n4.1.2 Results\nIn Table 2, we see that PVT v2 provides best results\nfor ImageNet-1K classiﬁcation. Compared to PVT v1,\nPVT v2 uses similar ﬂops and number of parameters,\nbut the image classiﬁcation accuracy is improved.\nFor example, PVT v2-B1 is 3.6% higher than PVT\nv1-Tiny, and PVT v2-B4 is 1.9% higher than PVT-\nLarge.\nCompared to other recent counterparts, PVT v2\nseries also have large advantages in terms of accuracy\nand model size. For example, PVT v2-B5 achieves\n83.8% ImageNet top-1 accuracy, which is 0.5% higher\nthan Swin transformer [ 5]a n dT w i n s[8], while using\nfewer parameters and GFLOPS.\n4.2 Object detection\n4.2.1 Setting\nObject detection experiments were conducted on\nthe challenging COCO benchmark [ 9]. All models\nT able 2 Image classiﬁcation performance on the ImageNet validation\nset. #Param = millions of parameters. GFLOPs is calculated for\ninput of size 224 ×224. * = performance of the method trained under\nthe strategy of its original paper. Acc = top-1 accuracy. -Li = PVT\nv2 with linear SRA\nMethod #Param GFLOPs Acc (%)\nPVT v2-B0 (ours) 3.4 0.6 70.5\nResNet18∗ [24] 11.7 1.8 69.8\nDeiT-Tiny/16 [2] 5.7 1.3 72.2\nPVT v1-Tiny [3] 13.2 1.9 75.1\nPVT v2-B1 (ours) 13.1 2.1 78.7\nResNet50∗ [24] 25.6 4.1 76.1\nResNeXt50-32x4d∗ [25] 25.0 4.3 77.6\nRegNetY-4G [26] 21.0 4.0 80.0\nDeiT-Small/16 [2] 22.1 4.6 79.9\nT2T-ViTt -14 [15] 22.0 6.1 80.7\nPVT v1-Small [3] 24.5 3.8 79.8\nTNT-S [16] 23.8 5.2 81.3\nSwin-T [5] 29.0 4.5 81.3\nCvT-13 [4] 20.0 4.5 81.6\nCoaT-Lite Small [6] 20.0 4.0 81.9\nTwins-SVT-S [8] 24.0 2.8 81.7\nPVT v2-B2-Li (ours) 22.6 3.9 82.1\nPVT v2-B2 (ours) 25.4 4.0 82.0\nResNet101∗ [24] 44.7 7.9 77.4\nResNeXt101-32x4d∗ [25] 44.2 8.0 78.8\nRegNetY-8G [26] 39.0 8.0 81.7\nT2T-ViTt -19 [15] 39.0 9.8 81.4\nPVT v1-Medium [3] 44.2 6.7 81.2\nCvT-21 [4] 32.0 7.1 82.5\nPVT v2-B3 (ours) 45.2 6.9 83.2\nResNet152∗ [24] 60.2 11.6 78.3\nT2T-ViTt -24 [15] 64.0 15.0 82.2\nPVT v1-Large [3] 61.4 9.8 81.7\nTNT-B [16] 66.0 14.1 82.8\nSwin-S [5] 50.0 8.7 83.0\nTwins-SVT-B [8] 56.0 8.3 83.2\nPVT v2-B4 (ours) 62.6 10.1 83.6\nResNeXt101-64x4d∗ [25] 83.5 15.6 79.6\nRegNetY-16G [26] 84.0 16.0 82.9\nViT-Base/16 [1] 86.6 17.6 81.8\nDeiT-Base/16 [2] 86.6 17.6 81.8\nSwin-B [5] 88.0 15.4 83.3\nTwins-SVT-L [8] 99.2 14.8 83.7\nPVT v2-B5 (ours) 82.0 11.8 83.8\nwere trained on COCO 2017 train (118k images)\nand evaluated on COCO 2017 val (5k images). We\nveriﬁed the eﬀectiveness of PVT v2 backbones with\nmainstream detectors, including RetinaNet [34], Mask\nR-CNN [35], Cascade Mask R-CNN [ 36], ATSS [37],\nGFL [12], and Sparse R-CNN [ 38]. Before training,\nwe used weights pre-trained on ImageNet to initialize\nthe backbone and Xavier [ 39] to initialize the newly\nadded layers. We trained all models with batch\n\nPVT v2: Improved baselines with Pyramid Vision Transformer 419\nsize 16 on 8 V100 GPUs, and adopted AdamW [ 32]\nwith an initial learning rate of 10 −4 as optimizer.\nFollowing common practice [34, 35, 40], we adopted\na1 × or 3× training schedule (12 or 36 epochs) to\ntrain all detection models. Training images were\nresized to have a shorter side of 800 pixels, while the\nlonger side did not exceed 1333 pixels. When using\nthe 3× training schedule, we randomly resized the\nshorter side of the input image to lie within the range\n[640, 800]. In the testing phase, the shorter side of\nthe input image was ﬁxed to 800 pixels.\n4.2.2 Results\nAs Table 3 reports, PVT v2 signiﬁcantly outperforms\nPVT v1 on both one-stage and two-stage object\ndetectors with similar model size. For example, PVT\nv2-B4 achieves 46.1 AP with RetinaNet [ 34], and\n47.5 APb with Mask R-CNN [ 35], surpassing models\nwith PVT v1 by 3.5 AP and 4.6 AP b, respectively.\nWe present some qualitative object detection and\ninstance segmentation results on COCO 2017 val [ 9]\nin Fig. 3, which also shows the good results from our\nmodels.\nFor a fair comparison between PVT v2 and\nSwin transformer [5], we kept all settings the same,\nincluding ImageNet-1K pre-training and COCO ﬁne-\ntuning strategies. We evaluated Swin transformer and\nPVT v2 on four state-of-the-art detectors, including\nCascade R-CNN [ 36], ATSS [ 37], GFL [ 12], and\nSparse R-CNN [38] (Table 4). We see that PVT v2\nobtains much better AP than Swin transformer for all\ndetectors, showing its better feature representation\nability. For example, on ATSS, PVT v2 uses a similar\nnumber of parameters and ﬂops to Swin-T, but PVT\nv2 achieves 49.9 AP, 2.7 higher than Swin-T. Our\nPVT v2-Li reduces the computation from 258 to 194\nGFLOPs, while only sacriﬁcing a little performance.\n4.3 Semantic segmentation\n4.3.1 Settings\nFollowing PVT v1 [ 3], we chose ADE20K [ 10]\nto benchmark semantic segmentation. For a fair\ncomparison, we tested the PVT v2 backbones by\nusing them with Semantic FPN [ 41]. In the training\nphase, the backbone was initialized with weights pre-\ntrained on ImageNet [14], and the newly added layers\nwere initialized with Xavier [39]. We optimized our\nmodels using AdamW [ 32] with an initial learning\nrate of 10 −4. Following common practices [ 41, 42],\nwe trained our models for 40k iterations with a batch\nsize of 16 on 4 V100 GPUs. The learning rate decayed\nfollowing a polynomial decay schedule with a power\nof 0.9. We randomly resized and cropped images to\n512 × 512 for training, and rescaled the shorter side\nto 512 pixels during testing.\n4.3.2 Results\nAs Table 5 shows, when using Semantic FPN [ 41]\nfor semantic segmentation, PVT v2 consistently\noutperforms PVT v1 [ 3] and other counterparts.\nFor example, using almost the same number of\nparameters and GFLOPs, PVT v2-B1/B2/B3/B4\nT able 3 Object detection and instance segmentation on COCO 2017 val. #P = millions of parameters. AP b = bounding box AP. AP m =\nmask AP. -Li = PVT v2 with linear SRA\nBackbone RetinaNet 1× Mask R-CNN 1 ×\n#P AP AP 50 AP75 APS APM APL #P APb APb\n50 APb\n75\nAPm APm\n50\nAPm\n75\nPVT v2-B0 13.0 37.2 57.2 39.5 23.1 40.4 49.7 23.5 38.2 60.5 40.7 36.2 57.8 38.6\nResNet18 [24] 21.3 31.8 49.6 33.6 16.3 34.3 43.2 31.2 34.0 54.0 36.7 31.2 51.0 32.7\nPVT v1-Tiny [3] 23.0 36.7 56.9 38.9 22.6 38.8 50.0 32.9 36.7 59.2 39.3 35.1 56.7 37.3\nPVT v2-B1 (ours) 23.8 41.2 61.9 43.9 25.4 44.5 54.3 33.7 41.8 64.3 45.9 38.8 61.2 41.6\nResNet50 [24] 37.7 36.3 55.3 38.6 19.3 40.0 48.8 44.2 38.0 58.6 41.4 34.4 55.1 36.7\nPVT v1-Small [3] 34.2 40.4 61.3 43.0 25.0 42.9 55.7 44.1 40.4 62.9 43.8 37.8 60.1 40.3\nPVT v2-B2-Li (ours) 32.3 43.6 64.7 46.8 28.3 47.6 57.4 42.2 44.1 66.3 48.4 40.5 63.2 43.6\nPVT v2-B2 (ours) 35.1 44.6 65.6 47.6 27.4 48.8 58.6 45.0 45.3 67.1 49.6 41.2 64.2 44.4\nResNet101 [24] 56.7 38.5 57.8 41.2 21.4 42.6 51.1 63.2 40.4 61.1 44.2 36.4 57.7 38.8\nResNeXt101-32x4d [25] 56.4 39.9 59.6 42.7 22.3 44.2 52.5 62.8 41.9 62.5 45.9 37.5 59.4 40.2\nPVT v1-Medium [3] 53.9 41.9 63.1 44.3 25.0 44.9 57.6 63.9 42.0 64.4 45.6 39.0 61.6 42.1\nPVT v2-B3 (ours) 55.0 45.9 66.8 49.3 28.6 49.8 61.4 64.9 47.0 68.1 51.7 42.5 65.7 45.7\nPVT v1-Large [3] 71.1 42.6 63.7 45.4 25.8 46.0 58.4 81.0 42.9 65.0 46.6 39.5 61.9 42.5\nPVT v2-B4 (ours) 72.3 46.1 66.9 49.2 28.4 50.0 62.2 82.2 47.5 68.7 52.0 42.7 66.1 46.1\nResNeXt101-64x4d [25] 95.5 41.0 60.9 44.0 23.9 45.2 54.0 101.9 42.8 63.8 47.3 38.4 60.6 41.3\nPVT v2-B5 (ours) 91.7 46.2 67.1 49.5 28.5 50.0 62.5 101.6 47.4 68.6 51.9 42.5 65.7 46.0\n\n420 W. Wang, E. Xie, X. Li, et al.\nT able 4 Comparison with Swin transformer on object detection.\nAPb = bounding box AP. #P = millions of parameters. #G =\nGFLOPs calculated for an input size 1280 × 800. -Li = PVT v2 with\nlinear SRA\nBackbone Method APb APb\n50 APb\n75\n#P #G\nResNet50 [24]\nCascade\nMask\nR-CNN\n46.3 64.3 50.5 82 739\nSwin-T [5] 50.5 69.3 54.9 86 745\nPVT v2-B2-Li (ours) 50.9 69.5 55.2 80 725\nPVT v2-B2 (ours) 51.1 69.8 55.3 83 788\nResNet50 [24]\nATSS\n43.5 61.9 47.0 32 205\nSwin-T [5] 47.2 66.5 51.3 36 215\nPVT v2-B2-Li (ours) 48.9 68.1 53.4 30 194\nPVT v2-B2 (ours) 49.9 69.1 54.1 33 258\nResNet50 [24]\nGFL\n44.5 63.0 48.3 32 208\nSwin-T [5] 47.6 66.8 51.7 36 215\nPVT v2-B2-Li (ours) 49.2 68.2 53.7 30 197\nPVT v2-B2 (ours) 50.2 69.4 54.7 33 261\nResNet50 [24]\nSparse\nR-CNN\n44.5 63.4 48.2 106 166\nSwin-T [5] 47.9 67.3 52.3 110 172\nPVT v2-B2-Li (ours) 48.9 68.3 53.4 104 151\nPVT v2-B2 (ours) 50.1 69.5 54.9 107 215\nprovide at least 5.3% higher mIoU than PVT v1-\nTiny/Small/Medium/Large. Moreover, although\nPVT-Large uses 12% less GFLOPs than ResNeXt101-\nT able 5 Semantic segmentation results for diﬀerent backbones using\nthe ADE20K validation set. #P = millions of parameters. #G =\nGFLOPs with input size 512 × 512. -Li = PVT v2 with linear SRA\nBackbone Semantic FPN\n#P #G mIoU (%)\nPVT v2-B0 (ours) 7.6 25.0 37.2\nResNet18 [24] 15.5 32.2 32.9\nPVT v1-Tiny [3] 17.0 33.2 35.7\nPVT v2-B1 (ours) 17.8 34.2 42.5\nResNet50 [24] 28.5 45.6 36.7\nPVT v1-Small [3] 28.2 44.5 39.8\nPVT v2-B2-Li (ours) 26.3 41.0 45.1\nPVT v2-B2 (ours) 29.1 45.8 45.2\nResNet101 [24] 47.5 65.1 38.8\nResNeXt101-32x4d [25] 47.1 64.7 39.7\nPVT v1-Medium [3] 48.0 61.0 41.6\nPVT v2-B3 (ours) 49.0 62.4 47.3\nPVT v1-Large [3] 65.1 79.6 42.1\nPVT v2-B4 (ours) 66.3 81.3 47.9\nResNeXt101-64x4d [25] 86.4 103.9 40.2\nPVT v2-B5 (ours) 85.7 91.1 48.7\n64x4d, its mIoU is still 8.5% higher. Figure 3 shows\nsome qualitative semantic segmentation results on\nADE20K [10]. These results demonstrate that PVT v2\nFig. 3 Results for object detection and instance segmentation on COCO 2017 val [ 9], and semantic segmentation on ADE20K [ 10]. Left to\nright: results generated by PVT v2-B2-based RetinaNet [34], Mask R-CNN [35], and Semantic FPN [41].\n\nPVT v2: Improved baselines with Pyramid Vision Transformer 421\nbackbones can extract powerful features for semantic\nsegmentation, beneﬁting from the improved designs.\n4.4 Ablation study\n4.4.1 Model analysis\nAblation experiments on PVT v2 are reported in\nTable 6. We see that all three designs improve\nthe model in terms of result quality, number of\nparameters, or computational requirements.\n4.4.2 Overlapping patch embedding\nOverlapping patch embedding (OPE) is important.\nComparing #1 and #2 in Table 6, the model with\nOPE obtains better top-1 accuracy (81.1% vs. 79.8%)\non ImageNet and better AP (42.2 vs. 40.4) on\nCOCO than when using the original patch embedding\n(PE) [ 1]. OPE is eﬀective because it can model\nthe local continuity of images and feature maps via\noverlapping sliding windows.\n4.4.3 Convolutional feed-forward network\nThe convolutional feed-forward network (CFFN)\nmatters. Compared to the original feed-forward\nnetwork (FFN) [ 1], our CFFN contains a zero-\npadding convolutional layer, which can capture local\ncontinuity of the input tensor. In addition, due to the\npositional information introduced by zero-padding\nin OPE and CFFN, we can remove the ﬁxed-size\npositional embeddings used in PVT v1, giving the\nmodel the ﬂexibility to handle variable resolution\ninput. As reported in #2 and #3 in Table 6, CFFN\nbrings 0.9 points improvement on ImageNet (82.0%\nvs. 81.1%) and 2.4 points improvement on COCO,\ndemonstrating its eﬀectiveness.\n4.4.4 Linear SRA\nLinear SRA (LSRA) contributes to a better\nmodel. As reported in #3 and #4 in Table 6,\ncompared to SRA [\n3], LSRA signiﬁcantly reduces\nthe computational load (in GFLOPs) of the model by\n22%, while providing comparable top-1 accuracy on\nT able 6 Ablation experiments on PVT v2. OPE, CFFN, and LSRA\nrepresent overlapping patch embedding, convolutional feed-forward\nnetwork (PVT v2-B2), and linear SRA (PVT v2-B2-Li), respectively.\n#P = millions of parameters. #G = GFLOPs. Acc = top-1 accuracy\n# Setting Acc\n(%)\nRetinaNet 1×\n#P #G AP\n1 PVT v1-Small [3] 79.8 34.2 285.8 40.4\n2 + OPE 81.1 34.9 288.6 42.2\n3 ++ CFFN 82.0 35.1 290.7 44.6\n4 +++ LSRA 82.1 32.3 227.4 43.6\nImageNet (82.1% vs. 82.0%), and only 1 point lower\nAP on COCO (43.6 vs. 44.6). These results show the\nlower computational cost and good eﬀects of LSRA.\n4.4.5 Computational complexity\nAs Fig. 4 shows, with increasing input scale, the\ngrowth rate of the computational requirements in\nGFLOPS for the proposed PVT v2-B2-Li are much\nlower than for PVT v1-Small [ 3], and are similar to\nthose of ResNet-50 [13]. This demonstrates that PVT\nv2-Li successfully addresses the high computational\noverheads caused by the attention layer.\nFig. 4 GFLOPs required for diﬀerent input sizes.\n5 Conclusions\nWe studied the limitations of the Pyramid Vision\nTransformer (PVT v1) and improved it with three\ndesigns: an overlapping patch embedding, a convo-\nlutional feed-forward network, and a linear spatial\nreduction attention layer. Extensive experiments on\ndiﬀerent tasks, such as image classiﬁcation, object\ndetection, and semantic segmentation demonstrate\nthat the proposed PVT v2 is stronger than\nits predecessor PVT v1 and other state-of-the-\nart transformer-based backbones, with comparable\nnumbers of parameters. We hope these improved\nbaselines will provide a reference for future research\nin vision transformers.\nAcknowledgements\nThis work was supported by the National\nNatural Science Foundation of China under Grant\nNos. 61672273 and 61832008, the Science Foundation\nfor Distinguished Young Scholars of Jiangsu under\nGrant No. BK20160021, the Postdoctoral Innovative\n\n422 W. Wang, E. Xie, X. Li, et al.\nTalent Support Program of China under Grant Nos.\nBX20200168 and 2020M681608, and the General\nResearch Fund of Hong Kong under Grant No. 27208720.\nDeclaration of competing interest\nThe authors have no competing interests to declare\nthat are relevant to the content of this article.\nReferences\n[1] Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer,\nM.; Heigold, G.; S. Gelly, S.; et al. An image is worth\n16×16 words: Transformers for image recognition at\nscale. In: Proceedings of the International Conference\non Learning Representations, 2021.\n[2] Touvron, H.; Cord, M.; Douze, M.; Massa, F.;\nJ´egou, H. Training data-eﬃcient image transformers &\ndistillation through attention. In: Proceedings of the\n38th International Conference on Machine Learning,\n2021.\n[3] Wang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.;\nLiang, D.; Lu, T.; Luo, P.; Shao, L. Pyramid\nvision transformer: A versatile backbone for dense\nprediction without convolutions. In: Proceedings of\nthe IEEE/CVF International Conference on Computer\nVision, 568–578, 2021.\n[4] Wu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.;\nYuan, L.; Zhang, L. CvT: Introducing convolutions to\nvision transformers. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 22–31,\n2021.\n[5] Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.;\nLin, S.; Guo, B. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In: Proceedings of\nthe IEEE/CVF International Conference on Computer\nVision, 10012–10022, 2021.\n[6] Xu, W.; Xu, Y.; Chang, T.; Tu, Z. Co-scale conv-\nattentional image transformers. In: Proceedings of\nthe IEEE/CVF International Conference on Computer\nVision, 9981–9990, 2021.\n[7] Graham, B.; El-Nouby, A.; Touvron, H.; Stock, P.;\nJoulin, A.; J´ egou, H. LeViT: A vision transformer in\nConvNet’s clothing for faster inference. In: Proceedings\nof the IEEE/CVF International Conference on\nComputer Vision, 12259–12269, 2021.\n[8] Chu, X.; Tian, Z.; Wang, Y.; Zhang, B.; Ren, H.;\nWei, X.; Xia, H.; Shen, C. Twins: Revisiting the\ndesign of spatial attention in vision transformers.\nIn: Proceedings of the 35th Conference on Neural\nInformation Processing Systems, 2021.\n[9] Lin, T. Y.; Maire, M.; Belongie, S.; Hays, J.; Perona,\nP.; Ramanan, D.; Doll´ ar, P.; Zitnick, C. L. Microsoft\nCOCO: Common objects in context. In: Computer\nVision – ECCV 2014. Lecture Notes in Computer\nScience, Vol. 8693. Fleet, D.; Pajdla, T.; Schiele, B.;\nTuytelaars, T. Eds. Springer Cham, 740–755, 2014.\n[10] Zhou, B. L.; Zhao, H.; Puig, X.; Fidler, S.; Barriuso, A.;\nTorralba, A. Scene parsing through ADE20K dataset.\nIn: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 5122–5130, 2017.\n[11] Dong, B.; Wang, W.; Fan, D.-P.; Li, J.; Fu, H.; Shao, L.\nPolyp-PVT: Polyp segmentation with pyramid vision\ntransformers. arXiv preprint arXiv:2108.06932, 2021.\n[12] Li, X.; Wang, W.; Wu, L.; Chen, S.; Hu, X.;\nLi, J.; Tang, J.; Yang, J. Generalized focal loss:\nLearning qualiﬁed and distributed bounding boxes for\ndense object detection. In: Proceedings of the 34th\nConference on Neural Information Processing Systems,\n2020.\n[13] He, K. M.; Zhang, X. Y.; Ren, S. Q.; Sun, J.\nDelving deep into rectiﬁers: Surpassing human-\nlevel performance on ImageNet classiﬁcation. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 1026–1034, 2015.\n[14] Deng, J.; Dong, W.; Socher, R.; Li, L. J.; Kai, L.; Li, F.\nF. ImageNet: A large-scale hierarchical image database.\nIn: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 248–255, 2009.\n[15] Yuan, L.; Chen, Y.; Wang, T.; Yu, W.; Shi, Y.;\nJiang, Z.; Tay, F. E.; Feng, J.; Yan, S. Tokens-to-\ntoken ViT: Training vision transformers from scratch\non ImageNet. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 558–567,\n2021.\n[16] Han, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.;\nWang, Y. Transformer in transformer. arXiv preprint\narXiv:2103.00112, 2021.\n[17] Chu, X.; Tian, Z.; Zhang, B.; Wang, X.; Wei, X.; Xia,\nH.; Shen, C. Conditional positional encodings for vision\ntransformers. arXiv preprint arXiv:2102.10882, 2021.\n[18] Chen, C.-F.; Fan, Q.; Panda, R. CrossViT: Cross-\nattention multi-scale vision transformer for image\nclassiﬁcation. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 357–366,\n2021.\n[19] Li, Y.; Zhang, K.; Cao, J.; Timofte, R.; van Gool,\nL. LocalViT: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707, 2021.\n[20] Islam, M. A.; Jia, S.; Bruce, N. D. B. How\nmuch position information do convolutional neural\nnetworks encode? In: Proceedings of the International\nConference on Learning Representations, 2020.\n\nPVT v2: Improved baselines with Pyramid Vision Transformer 423\n[21] Howard, A. G.; Zhu, M.; Chen, B.; Kalenichenko,\nD.; Wang, W.; Weyand, T; Andreetto, M.;\nAdam, H. MobileNets: Eﬃcient convolutional neural\nnetworks for mobile vision applications. arXiv preprint\narXiv:1704.04861, 2017.\n[22] Hendrycks, D.; Gimpel, K. Gaussian error linear units\n(GELUs). arXiv preprint arXiv:1606.08415, 2016.\n[23] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.;\nJones, L.; Gomez, A. N.; Kaiser, L.; Polosukhin, I.\nAttention is all you need. In: Proceedings of the\n31st International Conference on Neural Information\nProcessing Systems, 6000–6010, 2017.\n[24] He, K. M.; Zhang, X. Y.; Ren, S. Q.; Sun, J. Deep\nresidual learning for image recognition. In: Proceedings\nof the IEEE Conference on Computer Vision and\nPattern Recognition, 770–778, 2016.\n[25] Xie, S. N.; Girshick, R.; Dollar, P.; Tu, Z. W.; He, K.\nM. Aggregated residual transformations for deep neural\nnetworks. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 5987–5995,\n2017.\n[26] Radosavovic, I.; Kosaraju, R. P.; Girshick, R.; He, K.\nM.; Doll´ ar, P. Designing network design spaces. In:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 10425–10433, 2020.\n[27] Russakovsky, O.; Deng, J.; Su, H.; Krause, J.;\nSatheesh, S.; Ma, S. A.; Huang, Z.; Karpathy, A.;\nKhosla, A.; Bernstein, M.; et al. ImageNet large scale\nvisual recognition challenge. International Journal of\nComputer Vision Vol. 115, No. 3, 211–252, 2015.\n[28] Szegedy, C.; Liu, W.; Jia, Y. Q.; Sermanet, P.; Reed, S.;\nAnguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich,\nA. Going deeper with convolutions. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 1–9, 2015.\n[29] Szegedy, C.; Vanhoucke, V.; Ioﬀe, S.; Shlens, J.; Wojna,\nZ. Rethinking the inception architecture for computer\nvision. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2818–2826,\n2016.\n[30] Zhang, H.; Cisse, M.; Dauphin, Y. N.; Lopez-\nPaz, D. mixup: Beyond empirical risk minimization.\nIn: Proceedings of the International Conference on\nLearning Representations, 2018.\n[31] Zhong, Z.; Zheng, L.; Kang, G. L.; Li, S. Z.; Yang, Y.\nRandom erasing data augmentation. Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence Vol. 34, No.\n7, 13001–13008, 2020.\n[32] Loshchilov, I.; Hutter, F. Decoupled weight decay\nregularization. In: Proceedings of the International\nConference on Learning Representations, 2019.\n[33] Loshchilov, I.; Hutter, F. SGDR: Stochastic gradient\ndescent with warm restarts. In: Proceedings of the\nInternational Conference on Learning Representations,\n2017.\n[34] Lin, T. Y.; Goyal, P.; Girshick, R.; He, K. M.; Doll´ ar, P.\nFocal loss for dense object detection. In: Proceedings\nof the IEEE International Conference on Computer\nVision, 2999–3007, 2017.\n[35] He, K. M.; Gkioxari, G.; Doll´ ar, P.; Girshick, R. Mask\nR-CNN. In: Proceedings of the IEEE International\nConference on Computer Vision, 2980–2988, 2017.\n[36] Cai, Z. W.; Vasconcelos, N. Cascade R-CNN: Delving\ninto high quality object detection. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 6154–6162, 2018.\n[37] Zhang, S. F.; Chi, C.; Yao, Y. Q.; Lei, Z.; Li, S. Z.\nBridging the gap between anchor-based and anchor-\nfree detection via adaptive training sample selection.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 9756–9765,\n2020.\n[38] Sun, P. Z.; Zhang, R. F.; Jiang, Y.; Kong, T.; Xu,\nC. F.; Zhan, W.; Tomizuka, M.; Li, L.; Yuan, Z.;\nWang, C.; et al. Sparse R-CNN: End-to-end object\ndetection with learnable proposals. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 14449–14458, 2021.\n[39] Glorot, X.; Bengio, Y. Understanding the diﬃculty\nof training deep feedforward neural networks. In:\nProceedings of the 13th International Conference on\nArtiﬁcial Intelligence and Statistics, 249–256, 2010.\n[40] Chen, K.; Wang, J. Q.; Pang, J. M.; Cao, Y. H.; Xiong,\nY.; Li, X.; Sun, S.; Feng, W.; Liu, Z.; Xu, J.; et al.\nMMDetection: Open MMLab detection toolbox and\nbenchmark. arXiv preprint arXiv:1906.07155, 2019.\n[41] Kirillov, A.; Girshick, R.; He, K. M.; Doll´ ar, P.\nPanoptic feature pyramid networks. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 6392–6401, 2019.\n[42] Chen, L. C.; Papandreou, G.; Kokkinos, I.; Murphy, K.;\nYuille, A. L. DeepLab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and\nfully connected CRFs. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence Vol. 40, No. 4, 834–\n848, 2018.\nW enhai Wangreceived his B.S. degree\nfrom Nanjing University of Science\nand Technology, China, in 2016. He\nis currently a Ph.D. student with\nthe Department of Computer Science,\nNanjing University. His main research\ninterests include scene text detection\nand recognition, deep neural network\nexploration, object detection, and instance segmentation.\n\n424 W. Wang, E. Xie, X. Li, et al.\nEnze Xie received his B.S. degree\nfrom Nanjing University of Aeronautics\nand Astronautics, China, in 2016, and\nhis M.S. degree from Tongji University,\nChina, in 2019. He is currently a\nPh.D. student with the Department of\nComputer Science, the University of\nHong Kong. His main research interests\ninclude object detection and instance segmentation.\nXiang Li received his B.S. degree in\ncomputer science from Nanjing University\nof Science and Technology in 2013, where\nhe is currently working towards a Ph.D.\ndegree in pattern recognition and intelligent\nsystems. His research interests include\ncomputer vision, pattern recognition, data\nmining, and deep learning.\nDeng-Ping F an is a postdoctoral\nresearcher at ETH Zurich, Switzerland.\nHe received his Ph.D. degree from\nNankai University in 2019. He joined\nthe Inception Institute of Artiﬁcial\nIntelligence (IIAI) in 2019. He has\npublished about 30+ top journal and\nconference papers. His research interests\ninclude computer vision, deep learning, and saliency detection.\nKaitao Song received his Ph.D. degree\nin computer science from Nanjing\nUniversity of Science and Technology\nin 2021. His research interests focus\non machine learning and deep learning\nalgorithms for natural language process-\ning and speech processing, including pre-\ntrained language models, neural machine\ntranslation, music generation, text summarization, neural\narchitecture search for NLP, audio speech recognition, text-\nto-speech synthesis, etc.\nDing Liang has been working for\nSenseTime Ltd., since he graduated\nfrom Tsinghua University. He is now\nan associate director and head of the\nOCR team. His main research interests\ninclude OCR, face recognition, and\nmodel compression.\nT ong Lu received his Ph.D. degree\nin computer science from Nanjing\nUniversity in 2005, where he also received\nhis M.Sc. and B.Sc. degrees in 2002 and\n1997, respectively. He served as associate\nprofessor and assistant professor in the\nDepartment of Computer Science and\nTechnology at Nanjing University from\n2007 and 2005, respectively, where he is now a full professor.\nHe is also a member of the National Key Laboratory of\nNovel Software Technology in China. He has published over\n130 papers and authored 2 books, and received more than\n30 international and Chinese patents. His current interests\nare in multimedia, computer vision, and pattern recognition\nalgorithms and systems.\nPing Luo is an assistant professor in the\nDepartment of Computer Science, The\nUniversity of Hong Kong. He received his\nPh.D. degree in 2014 from Information\nEngineering, the Chinese University of\nHong Kong, and was a postdoctoral\nfellow there from 2014 to 2016. He\njoined SenseTime Research as a principal\nresearch scientist from 2017 to 2018. His research interests\nare machine learning and computer vision. He has published\n100+ peer-reviewed articles in top-tier conferences and\njournals. He was named a young innovator under 35 by\nMIT Technology Review (TR35) Asia Paciﬁc.\nLing Shao is the CEO and Chief\nScientist of the Inception Institute of\nAI (IIAI), Abu Dhabi, United Arab\nEmirates (UAE). He was the initiator\nand Founding Provost and Executive\nVice President of the Mohamed bin\nZayed University of Artiﬁcial Intelligence\n(the world’s ﬁrst AI University), UAE.\nHis research interests include computer vision, machine\nlearning, and medical imaging. He is a fellow of the IEEE,\nthe IAPR, the IET, and the BCS.\nOpen Access This article is licensed under a Creative\nCommons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduc-\ntion in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link\nto the Creative Commons licence, and indicate if changes\nwere made.\nThe images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and\nyour intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission\ndirectly from the copyright holder.\nTo view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\nOther papers from this open access journal are available\nfree of charge from http://www.springer.com/journal/41095.\nTo submit a manuscript, please go to https://www.\neditorialmanager.com/cvmj.\n",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7208367586135864
    },
    {
      "name": "Computer science",
      "score": 0.7118923664093018
    },
    {
      "name": "Segmentation",
      "score": 0.53497314453125
    },
    {
      "name": "Embedding",
      "score": 0.5141149163246155
    },
    {
      "name": "Artificial intelligence",
      "score": 0.459147185087204
    },
    {
      "name": "Computation",
      "score": 0.42108750343322754
    },
    {
      "name": "Computer vision",
      "score": 0.38902029395103455
    },
    {
      "name": "Computer engineering",
      "score": 0.3799448609352112
    },
    {
      "name": "Algorithm",
      "score": 0.19465816020965576
    },
    {
      "name": "Engineering",
      "score": 0.14404615759849548
    },
    {
      "name": "Electrical engineering",
      "score": 0.10113146901130676
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210128910",
      "name": "Group Sense (China)",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210116052",
      "name": "Inception Institute of Artificial Intelligence",
      "country": "AE"
    }
  ]
}