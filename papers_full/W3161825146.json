{
  "title": "A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images",
  "url": "https://openalex.org/W3161825146",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2109953969",
      "name": "Libo Wang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2041764006",
      "name": "Rui Li",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2109119725",
      "name": "Chenxi Duan",
      "affiliations": [
        "State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2130065647",
      "name": "Ce Zhang",
      "affiliations": [
        "UK Centre for Ecology & Hydrology",
        "Lancaster University"
      ]
    },
    {
      "id": "https://openalex.org/A2126293579",
      "name": "Xiaoliang Meng",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2128391902",
      "name": "Shenghui Fang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2109953969",
      "name": "Libo Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2041764006",
      "name": "Rui Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109119725",
      "name": "Chenxi Duan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130065647",
      "name": "Ce Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126293579",
      "name": "Xiaoliang Meng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128391902",
      "name": "Shenghui Fang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2787091153",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3009297390",
    "https://openalex.org/W3046711384",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3040304705",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3093142463",
    "https://openalex.org/W2963995737",
    "https://openalex.org/W3109998321",
    "https://openalex.org/W2778539913",
    "https://openalex.org/W6722710385",
    "https://openalex.org/W3129042754",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2963659230",
    "https://openalex.org/W6795463671",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2598666589",
    "https://openalex.org/W6739696289",
    "https://openalex.org/W4200142823",
    "https://openalex.org/W3200075728",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3190334976",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3137572916",
    "https://openalex.org/W3103010481",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W4303959453",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2488187315",
    "https://openalex.org/W3183174367",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W3007268491",
    "https://openalex.org/W4300524495",
    "https://openalex.org/W3164208409"
  ],
  "abstract": "The fully convolutional network (FCN) with an encoder-decoder architecture\\nhas been the standard paradigm for semantic segmentation. The encoder-decoder\\narchitecture utilizes an encoder to capture multilevel feature maps, which are\\nincorporated into the final prediction by a decoder. As the context is crucial\\nfor precise segmentation, tremendous effort has been made to extract such\\ninformation in an intelligent fashion, including employing dilated/atrous\\nconvolutions or inserting attention modules. However, these endeavors are all\\nbased on the FCN architecture with ResNet or other backbones, which cannot\\nfully exploit the context from the theoretical concept. By contrast, we\\nintroduce the Swin Transformer as the backbone to extract the context\\ninformation and design a novel decoder of densely connected feature aggregation\\nmodule (DCFAM) to restore the resolution and produce the segmentation map. The\\nexperimental results on two remotely sensed semantic segmentation datasets\\ndemonstrate the effectiveness of the proposed scheme.Code is available at\\nhttps://github.com/WangLibo1995/GeoSeg\\n",
  "full_text": " 1 \n  \nAbstractâ€”The Fully Convolutional Network (FCN) with an \nencoder-decoder architecture has been the standard paradigm for \nsemantic segmentation. The encoder-decoder architecture utilizes \nan encoder to capture multi -level feature maps, which are \nincorporated into the final prediction by a decoder. As the context \nis crucial for precise segmentation, tremendous effort has been \nmade to extract such information in an intelligent fashion, \nincluding employing dilated/atrous convolutions or ins erting \nattention modules. However, these endeavours are all based on the \nFCN architecture with ResNet or other backbones, which cannot \nfully exploit the context from the theoretical concept. By contrast, \nwe introduce the Swin Transformer as the backbone to extract the \ncontext information and design a novel decoder of densely \nconnected feature aggregation module (DCFAM) to restore the \nresolution and produce the segmentation map. The experimental \nresults on two remotely sensed semantic segmentation datasets \ndemonstrate the effectiveness of the proposed scheme.  Code is \navailable at https://github.com/WangLibo1995/GeoSeg. \nIndex Terms â€”semantic segmentation , fine -resolution remote \nsensing images, transformer \nI. INTRODUCTION \nAs an effective method to extract features automatically and \nhierarchically from images, the convolution al neural network \n(CNN) has become the common framework for tasks related to \ncomputer vision (CV) [4]. For semantic segmentation, the Fully \nConvolutional Network (FCN) [7] is the first proven and \neffective end-to-end CNN structure. Specifically, there are two \nsymmetric paths in the FCN and its variants: a contracting path, \ni.e., the encoder, for extracting features, and an expanding path, \ni.e., the decoder, for exacting positions [10].  The contracting \npath, by definition, gradually downsamples the resolution of \nfeature maps to reduce the computational c onsumption, while \nthe expanding path can learn more semantic meaning via a \nprogressively increasing receptive field . Benefit from its \ntranslation equivariance and locality, the FCN enhances the \nsegmentation performance significantly and influences the \nentire field. Specifically, the translation equivariance underpins \nthe generalization capability of the model to unseen data, while \nthe locality reduces the complexity of the model  by sharing \n                                                           \nThis work was funded by National Natural Science Foundation of China \n(NSFC) under grant number 41971352. (Corresponding author: Shenghui \nFang.) \nL. Wang, R. Li, X. Meng and S. Fang are with School of Remote Sensing \nand Information Engineering, Wuhan University, Wuhan 430079, China  (e-\nmail: wanglibo@whu.edu.cn; lironui@whu.edu.cn; xmeng@whu.edu.cn; \nshfang@whu.edu.cn). \nparameters. \nThe outcome of FCN, although encouraging, appears to be \ncoarse due to the over -simplified design of the decoder. \nSubsequently, more elaborate encoder-decoder structures were \nproposed [17], thus increasing the accuracy further.  However, \nthe long-range dependency is limited by the locality property of \nFCN-based methods, which is critical for segmentation in \nunconstrained scene images. There are two types of methods to \naddress the issue, either modifying the convolution operation or \nutilizing the attention mechanism. The former aiming to enlarge \nthe receptive fields using large kernel sizes [18] , dilated \nconvolutions [19], or feature pyramids [2, 20], whereas the \nlatter focuses on integrating attention mechanisms with the \nFCN architecture to capture long -range dependencies of the \nfeature maps [5, 21]. Nevertheless, both methods fail to liberate \nthe network from the dependence of the FCN structure.  More \nrecently, several inspiring advances [22, 23]  attempt to avoid \nconvolution operations  completely by employing attention -\nalone models, thereby achieving feature maps with long -range \ndependencies effectively. \nFor natural language processing (NLP),  the dominant \narchitecture is the  Transformer [24], which adopts the multi -\nhead attention to model long-range dependencies for sequence \nmodelling and  transduction tasks.  The tremendous \nbreakthrough in the natural language domain inspires \nresearchers to explore the potential and feasibility of \nTransformer in the computer vision field.  Obviously, the \nsuccessful application of Transformer will become the first and \nforemost step to integrate computer vision and NLP, thereby \nproviding a universal and uniform artificial intelligence (AI) \nscheme. \nThe pioneering work of  Swin Transformer [22] presents a \nhierarchical feature representation scheme that demonstrates \nimpressive performances with linear computational complexity. \nIn this Letter, we first  introduce the Swin Transformer for \nsemantic segmentation of fine -resolution remote sensing \nimages. Most importantly, we propose a densely connected \nC. Duan is with the State Key Laboratory of Information Engineering in \nSurveying, Mapping, and Remote Sensing, Wuhan University, Wuhan 430079, \nChina; chenxiduan@whu.edu.cn (e-mail: chenxiduan@whu.edu.cn). \nC. Zhang is with Lancaster Environment Centre, Lancaster University, \nLancaster LA1 4YQ, United Kingdom; UK Centre for Ecology & Hydrology, \nLibrary Avenue, Lancaster, LA1 4AP, United Kingdom  (e-mail: \nc.zhang9@lancaster.ac.uk). \nA Novel Transformer based Semantic \nSegmentation Scheme for Fine-Resolution \nRemote Sensing Images \nLibo Wang, Rui Li, Chenxi Duan, Ce Zhang, Xiaoliang Meng and Shenghui Fang \n 2 \nfeature aggregation module (DCFAM) to extract multi -scale \nrelation-enhanced semantic features for precise segmentation. \nCombining Swin Transformer and DCFAM, a novel semantic \nsegmentation scheme of Densely Connected Swin Transformer \n(DC-Swin) is established. \nII. METHODOLOGY \nThe overall architecture of our DC-Swin is constructed based \non the encoder-decoder structure, where the Swin Transformer \nis intro duced as the encoder while the proposed DCFAM is \nselected as the decoder. \nA. Swin Transformer \nAs shown in Fig.1 (a), the Swin Transformer backbone [22] \nfirst utilizes a patch partition module to split the input RGB \nimage into non-overlapping patches as â€œtokensâ€. The feature of \neach patch is set as a concatenation of the raw pixel RGB values. \nSubsequently, this raw-valued feature is fed into the multistage \nfeature transformation. In stage 1, a linear embedding layer is \ndeployed to project features to an arbitrary dimension C. \nThereafter, pairs of Swin Transformer blocks (Fig.1 (b)), which \ncan maintain the number  of tokens (e.g., HW/16), are adopted \nto extract semantic features. In the remaining stages, the \nnumber of tokens is gradually reduced by patch merging layers \nalong with the increasing depth of the network to produce a \nhierarchical representation. The outputs of the four stages are \nprocessed by a standard 1 ïƒ1 convolution to generate four \nhierarchical Swin Transformer features ( ST1, ST 2, ST 3, and \nST4). \nBy choosing diverse hyper-parameters, i.e., the dimensions \nC and the number of Swin Transformer blocks in ea ch stage, \nfour Swin Transformer backbones with different complexities \ncan be obtained: \nâ€¢ Swin-T: C = 96, block numbers = {2, 2, 6, 2}  \nâ€¢ Swin-S: C = 96, block numbers = {2, 2, 18, 2}  \nâ€¢ Swin-B: C = 128, block numbers = {2, 2, 18, 2}   \nâ€¢ Swin-L: C = 192, block numbers = {2, 2, 18, 2} \nIn this letter, to balance the efficiency and effectiveness,  we \nchoose Swin-S pre-trained on the ImageNet as the backbone of \nthe encoder, with the number of parameters (50M) comparable \nto ResNet-101 (45M).  \nB. Densely Connected Feature Aggregation Module \nMulti-scale and confusing geospatial objects appear \nfrequently in fine -resolution remote sensing images, which \nseriously affects the quality of segmentation. To handle this \nissue, we propose a novel DCFAM method for feature \nrepresentation. To be specific, we design a Shared Spatial \nAttention (SSA) and a Shared Channel Attention (SCA) to \nenhance the spatial -wise and channel -wise relationship of the \nsemantic features based on our previous work of linear attention \nmechanism [25]. Besides, multi -level features are further \nintegrated using the Downsample Connection and the Large -\nfield Upsample  Connection for improving multi -scale \nrepresentation. As shown in Fig.1, the DCFAM connects the \nfour hierarchical transformer features with cross -scale \nconnections (i.e., Downsample Connection and Large Field \nUpsample Connection) and attention blocks (i.e. , Shared \nSpatial Attention and Shared Channel Attention), generating \nfour aggregation features (i.e., AF\n1, AF 2, AF 3, and AF 4). \nCapitalising on the benefits provided by the DCFAM, the final \nsegmentation feature AF 1 is abundant in multi -scale \ninformation and relation-enhanced context. \nDownsample Connection:  The D ownsample connection \naims to connect the low -level and high -level transformer \nfeatures for fusion, which can be defined as follow: \n \nFig. 1 (a) The overall architecture of DC-Swin, (b) Pair of Swin Transformer Blocks, (c) Downsample Connection, (d) Large Field Upsample Connection,  \n(e) Shared Spatial Attention, and (f) Shared Channel Attention. The values of H and W are both 1024. Please enlarge the PDF to >=200% to get a better view. \n\n 3 \nğ·ğ·ğ‘–ğ‘–\nğ‘—ğ‘—(ğ‘¿ğ‘¿) = ğ‘“ğ‘“ğœğœ(ğ‘“ğ‘“ğ›¿ğ›¿(ğ‘¿ğ‘¿) + ğ‘“ğ‘“ğœ‡ğœ‡(ğ‘“ğ‘“ğœƒğœƒ(ğ‘¿ğ‘¿))) (1) \nwhere ğ‘¿ğ‘¿ is the input vector. ğ‘“ğ‘“ğœğœ is a ReLU activation function. \nğ‘“ğ‘“ğ›¿ğ›¿ and ğ‘“ğ‘“ğœ‡ğœ‡ are a 3ïƒ3 convolution layer with a stride of 2, ğ‘“ğ‘“ğœƒğœƒ \nis a 3 ïƒ3 convolution layer with a stride of 1, and each \nconvolution layer involves a batch normalization operation. ğ‘–ğ‘– \nand ğ‘—ğ‘— denote the number of the input channels and output \nchannels, respectively. \nLarge field Upsample Connection:  To capture multi -scale \ncontext effectively, we embedded the dilated convolution into \nthe Large filed Upsample Connection formulated as: \nğ¿ğ¿ğ¿ğ¿ğ‘šğ‘š\nğ‘›ğ‘›(ğ‘¿ğ‘¿) = ğ‘“ğ‘“ğœ‘ğœ‘\n12(ğ‘“ğ‘“ğœğœ(ğ‘“ğ‘“ğœ‘ğœ‘\n6(ğ‘¿ğ‘¿))) (2) \nwhere ğ‘“ğ‘“ğœ‘ğœ‘\n12 is a com posite function that contains a standard \n1ïƒ1 convolution, a dilated convolution with a dilated rate of \n12, and a standard transpose convolution. Similarly, ğ‘“ğ‘“ğœ‘ğœ‘\n6 has a \ndilated rate of 6. ğ‘šğ‘š and ğ‘›ğ‘› represent the number of the input \nchannel and output channel, respectively. \nShared Spatial Attention:  Based on the linear attention \nmechanism [25], we utilize the Shared Spatial Attention  to \nmodel the long- range dependencies in the spatial dimension \ndefined as: \nğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†(ğ‘¿ğ‘¿)\n=\nâˆ‘ ğ‘‰ğ‘‰(ğ‘¿ğ‘¿)ğ‘ğ‘,ğ‘›ğ‘›ğ‘›ğ‘› + ï¿½ ğ‘„ğ‘„(ğ‘¿ğ‘¿)\nâ€–ğ‘„ğ‘„(ğ‘¿ğ‘¿)â€–2\nï¿½ï¿½ï¿½ ğ¾ğ¾(ğ‘¿ğ‘¿)\nâ€–ğ¾ğ¾(ğ‘¿ğ‘¿)â€–2\nï¿½\nğ‘‡ğ‘‡\nğ‘‰ğ‘‰(ğ‘¿ğ‘¿)ï¿½\nğ‘ğ‘ + ï¿½ ğ‘„ğ‘„(ğ‘¿ğ‘¿)\nâ€–ğ‘„ğ‘„(ğ‘¿ğ‘¿)â€–2\nï¿½âˆ‘ ï¿½ ğ¾ğ¾(ğ‘¿ğ‘¿)\nâ€–ğ¾ğ¾(ğ‘¿ğ‘¿)â€–2\nï¿½\nğ‘ğ‘,ğ‘›ğ‘›\nğ‘‡ğ‘‡\nğ‘›ğ‘›\n (3) \nwhere Q(ğ‘¿ğ‘¿), K(ğ‘¿ğ‘¿), and V(ğ‘¿ğ‘¿) represent the convolutional \noperation to generate the query matrix ğ‘¸ğ‘¸ âˆˆ â„ğ‘ğ‘Ã—ğ·ğ·ğ‘˜ğ‘˜, key matrix \nğ‘²ğ‘² âˆˆ â„ğ‘ğ‘Ã—ğ·ğ·ğ‘˜ğ‘˜, and value matrix ğ‘½ğ‘½ âˆˆ â„ğ‘ğ‘Ã—ğ·ğ·ğ‘£ğ‘£. N is the number of \npixels in the input feature maps. c  and n indicate the channel \ndimension and the flattened spatial dimension. \nShared Channel Attention:  Similarly, the Shared Channel \nAttention is designed to extract the long -range dependencies \namong the channel dimension: \nğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†(ğ‘¿ğ‘¿)\n=\nâˆ‘ ğ‘…ğ‘…(ğ‘¿ğ‘¿)ğ‘ğ‘,ğ‘›ğ‘›ğ‘ğ‘ + ï¿½ğ‘…ğ‘…(ğ‘¿ğ‘¿)ğ‘ğ‘,ğ‘›ğ‘› ï¿½ ğ‘…ğ‘…(ğ‘¿ğ‘¿)\nâ€–ğ‘…ğ‘…(ğ‘¿ğ‘¿)â€–2\nï¿½\nğ‘‡ğ‘‡\nï¿½ ğ‘…ğ‘…(ğ‘¿ğ‘¿)\nâ€–ğ‘…ğ‘…(ğ‘¿ğ‘¿)â€–2\nğ‘ğ‘+ ï¿½ ğ‘…ğ‘…(ğ‘¿ğ‘¿)\nâ€–ğ‘…ğ‘…(ğ‘¿ğ‘¿)â€–2\nï¿½\nğ‘‡ğ‘‡\nâˆ‘ ï¿½ ğ‘…ğ‘…(ğ‘¿ğ‘¿)\nâ€–ğ‘…ğ‘…(ğ‘¿ğ‘¿)â€–2\nï¿½\nğ‘ğ‘,ğ‘›ğ‘›\nğ‘‡ğ‘‡\nğ‘ğ‘\n (4) \nwhere ğ‘…ğ‘…(ğ‘¿ğ‘¿)  indicate the reshape operation to flatten the \nspatial dimension. The detailed information about our previous \nwork on the linear attention mechanism can be referred to [25]. \nFeature aggregation:  The four aggregation features  (AF\n1, \nAF2, AF 3, and AF 4) can eventually be computed by the \nfollowing equations: \nğ‘¨ğ‘¨ğ‘¨ğ‘¨ğŸ’ğŸ’ = ğ‘ºğ‘ºğ‘ºğ‘ºğŸ’ğŸ’ + ğ·ğ·384\n768(ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†(ğ·ğ·192\n384(ğ‘ºğ‘ºğ‘ºğ‘ºğŸğŸ)) (5) \nğ‘¨ğ‘¨ğ‘¨ğ‘¨ğŸ‘ğŸ‘ = ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†(ğ‘ºğ‘ºğ‘ºğ‘ºğŸ‘ğŸ‘) + ğ·ğ·192\n384(ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†(ğ·ğ·96\n192(ğ‘ºğ‘ºğ‘ºğ‘ºğŸğŸ)) (6) \nğ‘¨ğ‘¨ğ‘¨ğ‘¨ğŸğŸ = ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†(ğ‘ºğ‘ºğ‘ºğ‘ºğŸğŸ) + ğ¿ğ¿ğ¿ğ¿768\n192(ğ‘¨ğ‘¨ğ‘¨ğ‘¨ğŸ’ğŸ’) (7) \nğ‘¨ğ‘¨ğ‘¨ğ‘¨ğŸğŸ = ğ‘ºğ‘ºğ‘ºğ‘ºğŸğŸ + ğ¿ğ¿(ğ‘¨ğ‘¨ğ‘¨ğ‘¨ğŸğŸ) + ğ¿ğ¿ğ¿ğ¿384\n96 (ğ‘¨ğ‘¨ğ‘¨ğ‘¨ğŸ‘ğŸ‘) (8) \nHere, ğ¿ğ¿ is a bilinear interpolation upsample operation with a \nscale factor of 2.  \nIII. EXPERIMENTAL RESULTS \nA. Dataset \nWe test the effectiveness of the proposed scheme on the well-\nknown ISPRS Vaihingen and Potsdam semantic labelling \n \nFig. 2 Enlarged visualization of results on the Vaihingen dataset (Top) and Potsdam dataset (Bottom). \n\n 4 \ndatasets. There are 33 tiles extracted from true orthophoto and \nthe co -registered normalized DSMs in the Vaihingen dataset \nwith an average size of 2494 ïƒ2064 pixels. The Potsdam \ndataset contains 38 tiles and the size of each tile is 6000ïƒ6000. \nFollowing previous pieces of literature  [3, 9, 11] , in the \nVaihingen dataset, we use the benchmark organizer defined 16 \nimages for training and 17 for testing, while the setting in the \nPotsdam dataset is 24 tiles for training and 14 tiles for testing. \nThe image tiles are cropped into 1024ïƒ1024 px patches as the \ninput. We do not employ  DSMs in o ur experiments to reduce \ncomputation.  \nB. Experimental Setting \nAll of the experiments are implemented with PyTorch on a \nsingle RTX 3090, and the optimizer is set as AdamW with  a \n0.0003 learning rate. The soft cross -entropy is used as the loss \nfunction. For each method, the overall accuracy (OA), mean \nIntersection over Union (mIoU), and F1 -score (F1) are chosen \nas evaluation indices: \nğ‘‚ğ‘‚ğ‘†ğ‘† =\nâˆ‘ ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘˜ğ‘˜ğ‘ğ‘\nğ‘˜ğ‘˜=1\nâˆ‘ ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘˜ğ‘˜+ğ¹ğ¹ğ‘‡ğ‘‡ğ‘˜ğ‘˜+ğ‘‡ğ‘‡ğ‘ğ‘ğ‘˜ğ‘˜+ğ¹ğ¹ğ‘ğ‘ğ‘˜ğ‘˜ğ‘ğ‘\nğ‘˜ğ‘˜=1\n, (9) \nğ‘šğ‘šğ‘šğ‘šğ‘šğ‘šğ¿ğ¿ =\n1\nğ‘ğ‘ âˆ‘ ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘˜ğ‘˜\nğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘˜ğ‘˜+ğ¹ğ¹ğ‘‡ğ‘‡ğ‘˜ğ‘˜+ğ¹ğ¹ğ‘ğ‘ğ‘˜ğ‘˜\nğ‘ğ‘\nğ‘˜ğ‘˜=1 ,  (10) \nğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘–ğ‘–ğ‘šğ‘šğ‘›ğ‘› =\n1\nğ‘ğ‘ âˆ‘ ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘˜ğ‘˜\nğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘˜ğ‘˜+ğ¹ğ¹ğ‘‡ğ‘‡ğ‘˜ğ‘˜\nğ‘ğ‘\nğ‘˜ğ‘˜=1\n, (11) \nğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ =\n1\nğ‘ğ‘ âˆ‘ ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘˜ğ‘˜\nğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘˜ğ‘˜+ğ¹ğ¹ğ‘ğ‘ğ‘˜ğ‘˜\nğ‘ğ‘\nğ‘˜ğ‘˜=1\n, (12) \nğ¹ğ¹1 = 2 Ã—\nğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘›ğ‘›Ã—ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ\nğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘›ğ‘›+ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ, (13) \nwhere ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘˜ğ‘˜, ğ¹ğ¹ğ‘‡ğ‘‡ğ‘˜ğ‘˜, ğ‘‡ğ‘‡ğ‘ğ‘ğ‘˜ğ‘˜, and ğ¹ğ¹ğ‘ğ‘ğ‘˜ğ‘˜ indicate the true positive, \nfalse positive, true negative, and false negatives, respectively, \nfor the specific object indexed as class k . OA is computed for \nall categories including the background. \nC. Semantic Segmentation Results and Analysis \n1) Performance  Comparison: The experimental results on \nthe Vaihingen and Potsdam datasets among state -of-the-art \nmethods are listed in Table â…  and Table â…¡.  The quantitive \nindices demonstrate the effectiveness of the proposed \nsegmentation scheme. To be specific, our prop osed DC-Swin \nachieves 90.71% in mean F1-score, 91.63% in OA, and 83.22% \nin mIoU for the Vaihingen  dataset, with 93.25%, 92.00%, and \n87.56% for the Potsdam dataset, outperforming the majority of \nResNet-based methods with highly competitive accuracy . \nBenefit from the global context information modelled by the \nSwin-S and the DCFAM, the performance of our scheme not \nonly outperforms recent contextual information aggregation \nmethods designed initially for natural images, such as \nDeepLabV3+ and PSPNet,  but also prevails over the lastest \nmulti-scale feature aggregation models proposed  for remote \nsensing images, such as EaNet and DDCM -Net, as well as the \ntransformer networks BoTNet and ResT. \n2) Ablation Study : As we not only propose a novel feature \naggregation model but also introduce a brand-new backbone for \nsegmentation, it is valuable to conduct the ablation study and \ninvestigate the contribution of each part upon accuracy. For the \nablation study, we select Res Net-101 and Swin- S with the \ndirect upsample operation as the baseline. ResNet101+DC and \nSwin-S+DC, which remove the SCA and SSA from DCFAM, \nare developed for the ablation study of dense connections. \nDCFAM-NS denotes the modified DCFAM that adopts the no-\nshared form structure. As shown in Table â…¢, the substitution of \nthe backbone from ResNet-101 to Swin-S yields a 3% increase \nin the Vaihingen dataset and a 4.05% increase in the Potsdam \ndataset for the mIoU index, showing the superiority of Swin-S. \nResNet101+DC and Swin -S+DC improve the performance of \nthe corresponding baseline method dramatically, indicating the \neffectiveness of dense connections. Meanwhile, deploying the \nTABLE â…  \nTHE EXPERIMENTAL RESULTS ON THE VAIHINGEN DATASET. \nMethod Backbone Imp. surf. Building Low veg. Tree Car Mean F1 OA mIoU \nDeepLabV3+ [1] ResNet101 92.38  95.17  84.29  89.52  86.47  89.57  90.56  81.47  \nPSPNet [2] ResNet101 92.79  95.46  84.51  89.94  88.61  90.26  90.85  82.58  \nDANet [5] ResNet101 91.63  95.02  83.25  88.87  87.16  89.19  90.44  81.32  \nEaNet [8] ResNet101 93.40  96.20  85.60  90.50  88.30  90.80  91.20  - \nDDCM-Net [3] ResNet50 92.70  95.30  83.30  89.40  88.30  89.80  90.40  - \nCASIA2 [11] ResNet101 93.20  96.00  84.70  89.90  86.70  90.10  91.10  - \nV-FuseNet [9] FuseNet 91.00  94.40  84.50  89.90  86.30  89.20  90.00  - \nDLR_9 [15] - 92.40  95.20  83.90  89.90  81.20  88.50  90.30  - \nBoTNet [14] ResNet50 92.24 95.28 83.88 89.99 85.47 89.37 90.51 81.05 \nResT [16] ResT-Base 92.15 94.88 84.17 90.02 84.97 89.24 90.43 80.82 \nOurs Swin-S 93.60 96.18 85.75 90.36 87.64 90.71 91.63 83.22 \n \nTABLE â…¡ \nTHE EXPERIMENTAL RESULTS ON THE POTSDAM DATASET. \nMethod Backbone Imp. surf. Building Low veg. Tree Car Mean F1 OA mIoU \nDeepLabV3+ [1] ResNet101 92.95  95.88  87.62  88.15  96.02  92.12  90.88  84.32  \nPSPNet [2] ResNet101 93.36  96.97  87.75  88.50  95.42  92.40  91.08  84.88  \nDDCM-Net [3] ResNet50 92.90  96.90  87.70  89.40  94.90  92.30  90.80  - \nCCNet [6] ResNet101 93.58  96.77  86.87  88.59  96.24  92.41  91.47  85.65  \nAMA_1 - 93.40  96.80  87.70  88.80  96.00  92.54  91.20  - \nSWJ_2 ResNet101 94.40  97.40  87.80  87.60  94.70  92.38  91.70  - \nV-FuseNet [9] FuseNet 92.70  96.30  87.30  88.50  95.40  92.04  90.60  - \nDST_5 [12] FCN 92.50  96.40  86.70  88.00  94.70  91.66  90.30  - \nBoTNet [14] ResNet50 93.13 96.37 87.31 88.01 95.79 92.12 90.76 85.62 \nResT [16] ResT-Base 92.74 96.08 87.48 88.55 94.76 91.92 90.57 85.23 \nOurs Swin-S 94.19 97.57 88.57 89.62 96.31 93.25 92.00 87.56 \n \n 5 \nshared attention modules in DCFAM further increases the \naccuracy, demonstrating the effectiveness of the SCA and SSA. \nBesides, the employment of DCFAM-NS obtains lower scores \ncompared to the utilization of DCFAM, which demonstrates the \nadvantage of our shared form structure. Benefiting from the \nlong-range dependencies and shared multi -scale structure, \nSwin-S+DCFAM obtains the highest accuracy on  the two \ndatasets, whose performance can also be observed in Fig. 2. \nIV. C\nONCLUSION \nIn this Letter, for the first time, we introduce Transformer into \nsemantic segmentation of fine -resolution remote sensing \nimages. We develop a densely connected feature aggregation \nmodule to capture multi -scale relation -enhanced semantic \nfeatures, thereby i ncreasing the segmentation accuracy. \nNumerical experiments conducted on the ISPRS Vaihingen and \nPotsdam datasets demonstrate the effectiveness of our scheme \nin segmentation accuracy. We envisage this pioneering Letter \ncould inspire researchers and practiti oners in this field to \nexplore the potential and feasibility of the Transformer more \nwidely in the remote sensing and Earth observation domain. \nR\nEFERENCE \n[1]  L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, \n\"Encoder-decoder with atrous separable convolution for semantic \nimage segmentation,\" in Proceedings of the European conference \non computer vision (ECCV), 2018, pp. 801-818.  \n[2]  H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \"Pyramid scene parsing \nnetwork,\" in Proceedings of the IEEE conference on computer \nvision and pattern recognition, 2017, pp. 2881-2890.  \n[3] Q. Liu, M. Kampffmeyer, R. Jenssen, and A. B. Salberg, \"Dense \nDilated Convolutionsâ€™ Merging Network for Land Cover \nClassification,\" IEEE Transactions on Geoscience and Remote \nSensing, vol. 58, no. 9, pp. 6309- 6320, 2020, doi: \n10.1109/TGRS.2020.2976658. \n[4] R. Li, S. Zheng, C. Duan, L. Wang, and C. Zhang, \"Land cover \nclassification from remote sensing images based on multi-scale fully \nconvolutional network,\" Geo-spatial Information Science, pp. 1-17, \n2022. \n[5]  J. Fu et al., \"Dual attention network for scene segmentation,\" in \nProceedings of the IEEE/CVF Conference on Computer Vision and \nPattern Recognition, 2019, pp. 3146-3154.  \n[6] Z. Huang  et al. , \"CCNet: Criss -Cross Attention for Semantic \nSegmentation,\" IEEE Transactions on Pattern Analysis and \nMachine Intelligence, pp. 1- 1, 2020, doi: \n10.1109/TPAMI.2020.3007032. \n[7]  J. Long, E. Shelhamer, and T. Darrell, \"Fully convolutional \nnetworks for semantic segmentation,\" in Proceedings of the \nIEEE/CVF Conference on Computer Vision and Pattern \nRecognition, 2015 2015, pp. 3431-3440.  \n[8] X. Zheng, L. Huan, G.- S. Xia, and J. Gong , \"Parsing very high \nresolution urban scene images by learning deep ConvNets with \nedge-aware loss,\" ISPRS Journal of Photogrammetry and Remote \nSensing, vol. 170, pp. 15-28, 2020. \n[9] N. Audebert, B. Le Saux, and S. LefÃ¨vre, \"Beyond RGB: Very high \nresolution urban remote sensing with multimodal deep networks,\" \nISPRS Journal of Photogrammetry and Remote Sensing, vol. 140, \npp. 20-32, 2018. \n[10] R. Li et al., \"Multiattention network for semantic segmentation of \nfine-resolution remote sensing images,\" IEEE Trans actions on \nGeoscience and Remote Sensing, 2021. \n[11] Y. Liu, B. Fan, L. Wang, J. Bai, S. Xiang, and C. Pan, \"Semantic \nlabeling in very high resolution images via a self -cascaded \nconvolutional neural network,\" ISPRS journal of photogrammetry \nand remote sensing, vol. 145, pp. 78-95, 2018. \n[12] J. Sherrah, \"Fully convolutional networks for dense semantic \nlabelling of high -resolution aerial imagery,\" arXiv preprint \narXiv:1606.02585, 2016. \n[13] R. Li, C. Duan, S. Zheng, C. Zhang, and P. M. Atkinson, \"MACU -\nNet for semantic segmentation of fine- resolution remotely sensed \nimages,\" IEEE Geoscience and Remote Sensing Letters, 2021. \n[14]  A. Srinivas, T. -Y. Lin, N. Parmar, J. Shlens, P. Abbeel, and A. \nVaswani, \"Bottleneck transformers for visual recognition,\" in \nProceedings of the IEEE/CVF Conference on Computer Vision and \nPattern Recognition, 2021, pp. 16519-16529.  \n[15] D. Marmanis, K. Schindler, J. D. Wegner, S. Galliani, M. Datcu, \nand U. Stilla, \"Classification with an edge: Improving semantic \nimage segmentation with boundary detection,\" ISPRS Journal of \nPhotogrammetry and Remote Sensing, vol. 135, pp. 158- 172, \n2018/01/01/ 2018, doi: \nhttps://doi.org/10.1016/j.isprsjprs.2017.11.009. \n[16] Q. Zhang and Y. Yang, \"ResT: An Efficient Transformer for Visual \nRecognition,\" arXiv preprint arXiv:2105.13677, 2021. \n[17]  O. Ronneberger, P. Fischer, and T. Brox, \"U -net: Convolutional \nnetworks for biomedical image segmentation,\" in International \nConference on Medical image computing and computer -assisted \nintervention, 2015: Springer, pp. 234-241.  \n[18]  C. Peng, X. Zhang, G. Yu, G. Luo, and J. Sun, \"Large kernel \nmatters--improve semantic segmentation by global convolutional \nnetwork,\" in Proceedings of the IEEE conference on computer \nvision and pattern recognition, 2017, pp. 4353-4361.  \n[19] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, \"Rethinking \natrous convolution for semantic image segmentation,\" arXiv \npreprint arXiv:1706.05587, 2017. \n[20] L. Wang, C. Zhang, R. Li, C. Duan, X. Meng, and P. M. Atkinson, \n\"Scale-Aware Neural Network for Semantic Segmentation of Multi-\nResolution Remote Sensing Images,\" Remote Sensing, vol. 13, no. \n24, p. 5015, 2021. \n[21] R. Li, S. Zheng, C. Zhang, C. Duan, L. Wang, and P. M. Atkinson,  \n\"ABCNet: Attentive bilateral contextual network for efficient \nsemantic segmentation of Fine- Resolution remotely sensed \nimagery,\" ISPRS Journal of Photogrammetry and Remote Sensing, \nvol. 181, pp. 84- 98, 2021/11/01/ 2021, doi: \nhttps://doi.org/10.1016/j.isprsjprs.2021.09.005. \n[22] Z. Liu  et al. , \"Swin transformer: Hierarchical vision transformer \nusing shifted windows,\" arXiv preprint arXiv:2103.14030, 2021. \n[23] L. Wang, R. Li, D. Wang, C. Duan, T. Wang, and X. Meng, \n\"Transformer Meets Convolution: A Bilateral Awareness Network \nfor Semantic Segmentation of Very Fine Resolution Urban Scene \nImages,\" Remote Sensing, vol. 13, no. 16, p. 3065, 2021. \n[24] A. Vaswani et al. , \"Attention is all you need,\" arXiv preprint \narXiv:1706.03762, 2017. \n[25] R. Li, S. Zheng, C. Duan, J. Su, and C. Zhang, \"Multistage Attention \nResU-Net for Semantic Segmentation of Fine- Resolution Remote \nSensing Images,\" IEEE Geoscience and Remote Sensing Letters, pp. \n1-5, 2021, doi: 10.1109/LGRS.2021.3063381. \n \nTABLE â…¢ \nABLATION STUDY ON THE VAIHINGEN AND POTSDAM DATASETS. \nDataset Method Mean F1 OA mIoU \nVaihingen \nResNet101 85.31 89.59 75.48 \nResNet101+DC 88.96 90.73 80.48 \nResNet101+DCFAM-NS 89.48 90.87 81.26 \nResNet101+DCFAM 90.22 91.04 82.43 \nSwin-S 87.54 90.50 78.48 \nSwin-S+DC 89.91 91.11 81.94 \nSwin-S+DCFAM-NS 89.96 91.26 82.02 \nSwin-S+DCFAM 90.71 91.63 83.22 \nPotsdam \nResNet101 88.66 89.24 79.97 \nResNet101+DC 91.75 90.45 84.95 \nResNet101+DCFAM-NS 91.81 90.49 85.05 \nResNet101+DCFAM 92.28 90.81 85.87 \nSwin-S 91.20 90.54 84.02 \nSwin-S+DC 92.55 91.33 86.32 \nSwin-S+DCFAM-NS 92.82 91.47 86.80 \nSwin-S+DCFAM 93.25 92.00 87.56 \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8758106231689453
    },
    {
      "name": "Encoder",
      "score": 0.7695989608764648
    },
    {
      "name": "Segmentation",
      "score": 0.7126858234405518
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6386951208114624
    },
    {
      "name": "Transformer",
      "score": 0.5358852744102478
    },
    {
      "name": "Decoding methods",
      "score": 0.48180049657821655
    },
    {
      "name": "Computer vision",
      "score": 0.4771311283111572
    },
    {
      "name": "Image segmentation",
      "score": 0.47369837760925293
    },
    {
      "name": "Convolutional neural network",
      "score": 0.43872249126434326
    },
    {
      "name": "Image resolution",
      "score": 0.42658519744873047
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41577956080436707
    },
    {
      "name": "Algorithm",
      "score": 0.11268541216850281
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}