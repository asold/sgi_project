{
  "title": "Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language",
  "url": "https://openalex.org/W2946676565",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4229255782",
      "name": "Kuratov, Yuri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2762042765",
      "name": "Arkhipov, Mikhail",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2951815760",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2850661852",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2768116540",
    "https://openalex.org/W2798338181",
    "https://openalex.org/W2768697094",
    "https://openalex.org/W2803728898",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2798999921",
    "https://openalex.org/W2962739339"
  ],
  "abstract": "The paper introduces methods of adaptation of multilingual masked language models for a specific language. Pre-trained bidirectional language models show state-of-the-art performance on a wide range of tasks including reading comprehension, natural language inference, and sentiment analysis. At the moment there are two alternative approaches to train such models: monolingual and multilingual. While language specific models show superior performance, multilingual models allow to perform a transfer from one language to another and solve tasks for different languages simultaneously. This work shows that transfer learning from a multilingual model to monolingual model results in significant growth of performance on such tasks as reading comprehension, paraphrase detection, and sentiment analysis. Furthermore, multilingual initialization of monolingual model substantially reduces training time. Pre-trained models for the Russian language are open sourced.",
  "full_text": "Submitted to Dialogue 2019 conference\nAdaptation of Deep Bidirectional Multilingual\nTransformers for Russian Language\nYuri Kuratov1 Mikhail Arkhipov1\n1 Neural Networks and Deep Learning Lab,\nMoscow Institute of Physics and Technology\nyurii.kuratov@phystech.edu arkhipov.mu@mipt.ru\nAbstract\nThe paper introduces methods of adaptation of multilingual masked\nlanguage models for a speciﬁc language. Pre-trained bidirectional lan-\nguage models show state-of-the-art performance on a wide range of tasks\nincluding reading comprehension, natural language inference, and senti-\nment analysis. At the moment there are two alternative approaches to\ntrain such models: monolingual and multilingual. While language speciﬁc\nmodels show superior performance, multilingual models allow to perform\na transfer from one language to another and solve tasks for diﬀerent lan-\nguages simultaneously. This work shows that transfer learning from a\nmultilingual model to monolingual model results in signiﬁcant growth of\nperformance on such tasks as reading comprehension, paraphrase detec-\ntion, and sentiment analysis. Furthermore, multilingual initialization of\nmonolingual model substantially reduces training time. Pre-trained mod-\nels for the Russian language are open sourced.\n1 Introduction\nA large amount of work is devoted to unsupervised pre-training of neural net-\nworks on a variety of Natural Language Processing (NLP) tasks. Unsuper-\nvised pre-training shows signiﬁcant improvements in almost every NLP task\n[3, 4, 8, 10].\nAt the moment one of the best performing models for unsupervised pre-\ntraining is BERT [4]. This model is based on Transformer [16] architecture and\ntrained on a large number of unlabeled texts from Wikipedia to solve Masked\nLanguage Modelling task. It shows state-of-the-art results on a wide range of\nNLP tasks in English.\nCurrently, there are publicly available two monolingual English and Chi-\nnese models and a single multilingual model. It was previously reported that\nmonolingual models performance is signiﬁcantly better than multilingual ones1.\n1https://github.com/google-research/bert/blob/master/multilingual.md#results\narXiv:1905.07213v1  [cs.CL]  17 May 2019\nSubmitted to Dialogue 2019 conference\nIn the present work, we consider the possibility of multilingual to monolin-\ngual transfer. We use Russian as a target language for transfer. We show that\nit is possible to train the monolingual model using multilingual initialization.\nTo show this, we evaluated the multilingual model on a number of common\nNLP tasks from the target language. The model trained in a monolingual setting\nachieves substantially better performance compared to the multilingual model.\n2 Model Architecture\nIn the present work, we use BERT [4] model for all our experiments. The model\nis a Transformer [16] encoder. The basic building blocks of the model is Self-\nAttention. The model was trained on the Masked Language Modelling and next\nsentence prediction tasks. We refer readers to check BERT original paper for\ndetails about the model[4].\nWe used 12 layers (Transformer blocks) version of BERT with self-attention\nhidden size 768, feed-forward hidden size 3072, and 12 self-attention heads.\nThis setting corresponds to BERT BASE model from [4]. Task-speciﬁc layers\nwere trained according to the BERT paper.\n3 Language transfer\nIn this work, we consider the transfer of multilingual BERT model to monolin-\ngual. Authors of [4] showed that monolingual models show superior performance\ncompared to multilingual one. Furthermore, the BERT model uses the subword\nsegmentation algorithm[14] to cope with large vocabulary problem. Multilingual\nmodels use only a small part of the entire vocabulary for a single language. It\nresults in much longer sequences after tokenization compared to the monolingual\nmodel. Since the Transformer model has quadratic computational complexity\nin terms of input sequence length, it is highly undesirable.\nWe investigated the possibility of using the multilingual model as initial-\nization for the monolingual model. The basic idea is to use knowledge about\ntarget language that already captured during multilingual training. It also is\nknown that training model using data from multiple languages can signiﬁcantly\nimprove the performance of the model [9]. We used the multilingual model\nfrom BERT repository 2. This model was trained on one hundred languages\nwith largest Wikipedias. The target language is Russian. All parameters of the\nmodel except word embeddings were initialized from the multilingual model [9].\nThe new subword vocabulary was obtained using subword-nmt 3. Training\nof the subword vocabulary was performed on the Russian part of Wikipedia and\nnews data. The part of Wikipedia data was around 80%. The result of this step\nis a new monolingual Russian subword vocabulary. This vocabulary contains\nlonger Russian words and subwords compared to multilingual one.\n2https://github.com/google-research/bert\n3https://github.com/rsennrich/subword-nmt\nSubmitted to Dialogue 2019 conference\nNew word embeddings matrix was obtained by assembling monolingual em-\nbeddings from multilingual. Namely, embeddings of all tokens from the intersec-\ntion of multilingual and monolingual vocabulary were left without any changes.\nThe same for special tokens like [UNK] or [CLS]. We replaced all tokens from\noutside the intersection with tokens from the monolingual vocabulary. These\ntokens are mostly longer subword units which are combinations of shorter units\npresent in the intersection. New tokens are initialized with the mean value of\nembeddings from the intersection. For example, there are tokens ’bi’ and ’##rd’\nin the intersection of vocabularies, where ’##’ stands for the continuation of\nthe word. There is also a token ’bird’ present in the monolingual vocabulary\nand absent in the multilingual vocabulary. The embedding of ’bird’ is initialized\nas the mean value of the embeddings ’bi’ and ’##rd’.\nThe model with reassembled vocabulary and embeddings matrix was trained\non the same data that was used for building of the monolingual vocabulary. The\nfollowing hyperparameters were used for training:\n• batch size: 256\n• learning rate: 2 · 10−5\n• optimizer: Adam\n• L2 regularization: 10 −2\nThe monolingual Russian model 4 is available as a part of the DeepPavlov\nlibrary5.\n4 Tasks description\nWe have chosen three tasks to evaluate our approach: paraphrase identiﬁcation,\nsentiment analysis, and question answering. We brieﬂy describe them in this\nsection.\n4.1 Paraphrase Identiﬁcation with ParaPhraser\nParaPhraser [11] is a dataset for paraphrase detection in Russian language.\nTwo sentences are paraphrases if they have the same meaning. This dataset\nconsists of 7227/1924 train/test pairs of sentences which are labeled as precise\nparaphrases, near paraphrases or non-paraphrases. One approach for para-\nphrase identiﬁcation is a binary classiﬁcation: ﬁrst class is precise and near\nparaphrases, second class - non-paraphrases.\n4http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_\nv1.tar.gz\n5https://github.com/deepmipt/deeppavlov/\nSubmitted to Dialogue 2019 conference\n4.2 Sentiment Analysis with RuSentiment\nRuSentiment [13] is a dataset for sentiment analysis of posts from VKontakte\n(VK), the most popular social network in Russia. Realised in 2018, it became\none of the largest sentiment datasets for Russian language with 30521 posts.\nEach post is labeled with one of the ﬁve classes. The informal language presented\nin RuSentiment dataset makes it more challenging for our model, trained on\nWikipedia and news articles.\n4.3 Question answering with SDSJ Task B\nAs part of Sberbank Data Science Journey 6 2017 was held a competition with\ntwo tasks. Task B was inspired by Stanford Question Answering Dataset (SQuAD)\n[12]. Organizers collected about 50,000 (train and development set) questions\nand contexts, where the answer is always a span from corresponding context.\nSQuAD dataset encouraged community to develop sophisticated and eﬀective\nneural architectures such as BiDAF [15], R-NET [17], Mnemonic Reader [5].\nAll this models are based on attention mechanisms [1, 7] and building joint\ncontext-question representation.\n5 Results\nWe evaluated BERT multilingual model and BERT trained with our approach\n(RuBERT) on three tasks: paraphrase identiﬁcation, sentiment analysis, and\nquestion answering. All reported results were obtained by averaging across 5\nruns.\nmodel F-1 Accuracy\nNeural networks [11] 79.82 76.65\nClassiﬁer + linguistic features [11] 81.10 77.39\nMachine Translation + Semantic similarity [6] 78.51 81.41\nBERT multilingual 85.48 ± 0.19 81.66 ± 0.38\nRuBERT 87.73 ± 0.26 84.99 ± 0.35\nTable 1: ParaPhraser. We compare BERT based models with models in non-\nstandard run setting, when all resources were allowed.\nSDSJ Task B and ParaPhraser datasets share the same domain with data,\nwhich we used for training RuBERT. The RuSentiment dataset is based on\nposts from a social network and shows to be more challenging for RuBERT. As\nresult, we can see only 1 F-1 point improvement from previous state of the art\nfor RuSentiment in Table 2, comparing to 4-6 F-1 points improvement on SDSJ\nTask B and ParaPhraser (results in Table 1 and Table 3).\n6https://sdsj.sberbank.ai/\nSubmitted to Dialogue 2019 conference\nmodel F-1 Precision Recall\nLogistic Regression [13] 68.84 69.53 69.46\nLinear SVC [13] 68.56 69.46 69.25\nGradient Boosting [13] 68.48 69.63 69.19\nNN classiﬁer [13] 71.64 71.99 72.15\nBERT multilingual 70.82 ± 0.75 - -\nRuBERT 72.63 ± 0.55 - -\nTable 2: RuSentiment. We used only randomly selected posts (21,268) subset\nfor training.\nmodel F-1 (dev) EM (dev)\nR-Net from DeepPavlov [2] 80.04 60.62\nBERT multilingual 83.39 ± 0.08 64.35 ± 0.39\nRuBERT 84.60 ± 0.11 66.30 ± 0.24\nTable 3: Results on question answering with SDSJ Task B. Models performance\nwas evaluated on development set (public leaderboard subset).\n5.1 Vocabulary comparison\nBERT multilingual and RuBERT have the same size of vocabulary (about 120k\nsubtokens), but RuBERT vocabulary was built especially for Russian language.\nFigure 1 shows that RuBERT model allows to reduce mean sequence length in\n1.6 times in subtokens, what makes possible to increase batch size or feed longer\ntexts to the model, comparing to BERT multilingual.\n(a) BERT multilingual\n (b) RuBERT\nFigure 1: Distribution of lengths in subtokens of contexts with their questions\n(SDSJ Task B dataset). Red vertical lines represent mean values.\n5.2 Training dynamics\nIn this section we compare training BERT model for Russian language from\nscratch (random initialization) and initialized with BERT multilingual. Figure\nSubmitted to Dialogue 2019 conference\n2 shows that BERT multilingual initialization helps to converge faster: about\n800 thousand steps is required for random initialized model to get the same loss\nas at 250 thousand step of multilingual initialization. It takes about two days to\ntrain for 250 thousand steps (on Tesla P100 x 8), so it helped us to save six days\nof computational time. Proposed averaging of new subtokens in vocabulary also\nhas positive eﬀect on the rate of convergence (instead of averaging we could take\nrandom initialization for new subtokens).\nFigure 2: Models training dynamics to get to the same value of loss.\n6 Conclusion\nIn this work, we have shown that Transformer network pre-trained on the multi-\nlingual Masked Language Modelling task signiﬁcantly improves performance on\na number of Russian NLP tasks compared to existing solutions. Furthermore,\nlanguage-speciﬁc unsupervised training with multilingual initialization results\nin even better improvements. Pre-trained models for the Russian language are\nopen sourced, as well as code to reproduce our results as part of DeepPavlov\nlibrary.\n7 Acknowledgments\nThis work was supported by National Technology Initiative and PAO Sberbank\nproject ID 0000000007417F630002.\nSubmitted to Dialogue 2019 conference\nReferences\n[1] Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation\nby jointly learning to align and translate. arXiv preprint arXiv:1409.0473 .\n[2] Burtsev, M., Seliverstov, A., Airapetyan, R., Arkhipov, M., Baymurzina,\nD., Bushkov, N., Gureenkova, O., Khakhulin, T., Kuratov, Y., Kuznetsov,\nD., et al. (2018). Deeppavlov: Open-source library for dialogue systems.\nProceedings of ACL 2018, System Demonstrations , pages 122–127.\n[3] Dai, A. M. and Le, Q. V. (2015). Semi-supervised sequence learning. In\nAdvances in neural information processing systems , pages 3079–3087.\n[4] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805.\n[5] Hu, M., Peng, Y., Huang, Z., Qiu, X., Wei, F., and Zhou, M. (2017). Rein-\nforced mnemonic reader for machine reading comprehension. arXiv preprint\narXiv:1705.02798.\n[6] Kravchenko, D. (2017). Paraphrase detection using machine translation and\ntextual similarity algorithms. In Conference on Artiﬁcial Intelligence and\nNatural Language, pages 277–292. Springer.\n[7] Luong, M.-T., Pham, H., and Manning, C. D. (2015). Eﬀective approaches to\nattention-based neural machine translation.arXiv preprint arXiv:1508.04025.\n[8] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Eﬃcient estimation\nof word representations in vector space. arXiv preprint arXiv:1301.3781 .\n[9] Mulcaire, P., Swayamdipta, S., and Smith, N. (2018). Polyglot semantic role\nlabeling. arXiv preprint arXiv:1805.11598 .\n[10] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,\nand Zettlemoyer, L. (2018). Deep contextualized word representations. In\nProc. of NAACL.\n[11] Pivovarova, L., Pronoza, E., Yagunova, E., and Pronoza, A. (2017). Para-\nphraser: Russian paraphrase corpus and shared task. In Conference on Arti-\nﬁcial Intelligence and Natural Language , pages 211–225. Springer.\n[12] Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016). Squad:\n100,000+ questions for machine comprehension of text. arXiv preprint\narXiv:1606.05250.\n[13] Rogers, A., Romanov, A., Rumshisky, A., Volkova, S., Gronas, M., and\nGribov, A. (2018). Rusentiment: An enriched sentiment analysis dataset for\nsocial media in russian. In Proceedings of the 27th International Conference\non Computational Linguistics , pages 755–763.\nSubmitted to Dialogue 2019 conference\n[14] Sennrich, R., Haddow, B., and Birch, A. (2015). Neural machine translation\nof rare words with subword units. arXiv preprint arXiv:1508.07909 .\n[15] Seo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H. (2016). Bidirectional\nattention ﬂow for machine comprehension. arXiv preprint arXiv:1611.01603.\n[16] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA. N., Kaiser,  L., and Polosukhin, I. (2017). Attention is all you need. In\nAdvances in Neural Information Processing Systems , pages 5998–6008.\n[17] Wang, W., Yang, N., Wei, F., Chang, B., and Zhou, M. (2017). Gated self-\nmatching networks for reading comprehension and question answering. In\nProceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , volume 1, pages 189–198.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8602077960968018
    },
    {
      "name": "Natural language processing",
      "score": 0.6581516861915588
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6404542326927185
    },
    {
      "name": "Paraphrase",
      "score": 0.6139549016952515
    },
    {
      "name": "Language model",
      "score": 0.5586065649986267
    },
    {
      "name": "Machine translation",
      "score": 0.5201823711395264
    },
    {
      "name": "Inference",
      "score": 0.46381232142448425
    },
    {
      "name": "Transformer",
      "score": 0.4495665431022644
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4492664039134979
    },
    {
      "name": "Reading comprehension",
      "score": 0.4467761516571045
    },
    {
      "name": "Transfer of learning",
      "score": 0.4249917268753052
    },
    {
      "name": "Reading (process)",
      "score": 0.29332423210144043
    },
    {
      "name": "Linguistics",
      "score": 0.2902742624282837
    },
    {
      "name": "Psychology",
      "score": 0.07509151101112366
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ]
}