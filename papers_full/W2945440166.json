{
  "title": "Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation",
  "url": "https://openalex.org/W2945440166",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098653865",
      "name": "Dai Ning",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Liang, Jianze",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378878701",
      "name": "Qiu, Xipeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378608106",
      "name": "Huang, Xuanjing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W2788321882",
    "https://openalex.org/W2565378226",
    "https://openalex.org/W2125389028",
    "https://openalex.org/W2963206679",
    "https://openalex.org/W2962937198",
    "https://openalex.org/W2964008635",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2914442349",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963034998",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2892100238",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2888173624",
    "https://openalex.org/W2963667126",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963631950",
    "https://openalex.org/W2769134508",
    "https://openalex.org/W2993787675",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W2888161220",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2963626623"
  ],
  "abstract": "Disentangling the content and style in the latent space is prevalent in unpaired text style transfer. However, two major issues exist in most of the current neural models. 1) It is difficult to completely strip the style information from the semantics for a sentence. 2) The recurrent neural network (RNN) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation.",
  "full_text": "Style Transformer: Unpaired Text Style Transfer without\nDisentangled Latent Representation\nNing Dai, Jianze Liang, Xipeng Qiu∗, Xuanjing Huang\nShanghai Key Laboratory of Intelligent Information Processing, Fudan University\nSchool of Computer Science, Fudan University\n825 Zhangheng Road, Shanghai, China\n{ndai16,jzliang18,xpqiu,xjhuang}@fudan.edu.cn\nAbstract\nDisentangling the content and style in the la-\ntent space is prevalent in unpaired text style\ntransfer. However, two major issues exist in\nmost of the current neural models. 1) It is\ndifﬁcult to completely strip the style informa-\ntion from the semantics for a sentence. 2)\nThe recurrent neural network (RNN) based en-\ncoder and decoder, mediated by the latent rep-\nresentation, cannot well deal with the issue of\nthe long-term dependency, resulting in poor\npreservation of non-stylistic semantic content.\nIn this paper, we propose the Style Trans-\nformer, which makes no assumption about the\nlatent representation of source sentence and\nequips the power of attention mechanism in\nTransformer to achieve better style transfer\nand better content preservation. Source code\nwill be available on Github1.\n1 Introduction\nText style transfer is the task of changing the\nstylistic properties (e.g., sentiment) of the text\nwhile retaining the style-independent content\nwithin the context. Since the deﬁnition of the text\nstyle is vague, it is difﬁcult to construct paired sen-\ntences with the same content and differing styles.\nTherefore, the studies of text style transfer focus\non the unpaired transfer.\nRecently, neural networks have become the\ndominant methods in text style transfer. Most\nof the previous methods (Hu et al., 2017; Shen\net al., 2017; Fu et al., 2018; Carlson et al., 2017;\nZhang et al., 2018b,a; Prabhumoye et al., 2018;\nJin et al., 2019; Melnyk et al., 2017; dos Santos\net al., 2018) formulate the style transfer problem\ninto the “encoder-decoder” framework. The en-\ncoder maps the text into a style-independent latent\n∗Corresponding author\n1https://github.com/fastnlp/\nstyle-transformer\nrepresentation (vector representation), and the de-\ncoder generates a new text with the same content\nbut a different style from the disentangled latent\nrepresentation plus a style variable.\nThese methods focus on how to disentangle the\ncontent and style in the latent space. The la-\ntent representation needs better preserve the mean-\ning of the text while reducing its stylistic proper-\nties. Due to lacking paired sentence, an adversar-\nial loss (Goodfellow et al., 2014) is used in the\nlatent space to discourage encoding style informa-\ntion in the latent representation. Although the dis-\nentangled latent representation brings better inter-\npretability, in this paper, we address the following\nconcerns for these models.\n1) It is difﬁcult to judge the quality of disen-\ntanglement. As reported in (Elazar and Goldberg,\n2018; Lample et al., 2019), the style information\ncan be still recovered from the latent represen-\ntation even the model has trained adversarially.\nTherefore, it is not easy to disentangle the stylistic\nproperty from the semantics of a sentence.\n2) Disentanglement is also unnecessary. Lam-\nple et al. (2019) reported that a good decoder can\ngenerate the text with the desired style from an en-\ntangled latent representation by “overwriting” the\noriginal style.\n3) Due to the limited capacity of vector repre-\nsentation, the latent representation is hard to cap-\nture the rich semantic information, especially for\nthe long text. The recent progress of neural ma-\nchine translation also proves that it is hard to re-\ncover the target sentence from the latent represen-\ntation without referring to the original sentence.\n4) To disentangle the content and style infor-\nmation in the latent space, all of the existing ap-\nproaches have to assume the input sentence is en-\ncoded by a ﬁx-sized latent vector. As a result,\nthese approaches can not directly apply the atten-\ntion mechanism to enhance the ability to preserve\narXiv:1905.05621v3  [cs.CL]  20 Aug 2019\nthe information in the input sentence.\n5) Most of these models adopt recurrent neural\nnetworks (RNNs) as encoder and decoder, which\nhas a weak ability to capture the long-range de-\npendencies between words in a sentence. Besides,\nwithout referring the original text, RNN-based de-\ncoder is also hard to preserve the content. The gen-\neration quality for long text is also uncontrollable.\nIn this paper, we address the above concerns of\ndisentangled models for style transfer. Different\nfrom them, we propose Style Transformer, which\ntakes Transformer (Vaswani et al., 2017) as the ba-\nsic block. Transformer is a fully-connected self-\nattention neural architecture, which has achieved\nmany exciting results on natural language pro-\ncessing (NLP) tasks, such as machine translation\n(Vaswani et al., 2017), language modeling (Dai\net al., 2019), text classiﬁcation (Devlin et al.,\n2018). Different from RNNs, Transformer uses\nstacked self-attention and point-wise, fully con-\nnected layers for both the encoder and decoder.\nMoreover, Transformer decoder fetches the infor-\nmation from the encoder part via attention mech-\nanism, compared to a ﬁxed size vector used by\nRNNs. With the strong ability of Transformer, our\nmodel can transfer the style of a sentence while\nbetter preserving its meaning. The difference be-\ntween our model and the previous model is shown\nin Figure 1.\nOur contributions are summarized as follows:\n•We introduce a novel training algorithm\nwhich makes no assumptions about the dis-\nentangled latent representations of the input\nsentences, and thus the model can employ\nattention mechanisms to improve its perfor-\nmance further.\n•To the best of our knowledge, this is the ﬁrst\nwork that applies the Transformer architec-\nture to style transfer task.\n•Experimental results show that our proposed\napproach generally outperforms the other\napproaches on two style transfer datasets.\nSpeciﬁcally, to the content preservation,\nStyle Transformer achieves the best perfor-\nmance with a signiﬁcant improvement.\n2 Related Work\nRecently, many text style transfer approaches have\nbeen proposed. Among these approaches, there\nis a line of works aims to infer a latent repre-\nsentation for the input sentence, and manipulate\nthe style of the generated sentence based on this\nlearned latent representation. Shen et al. (2017)\npropose a cross-aligned auto-encoder with adver-\nsarial training to learn a shared latent content dis-\ntribution and a separated latent style distribution.\nHu et al. (2017) propose a new neural generative\nmodel which combines variational auto-encoders\nand holistic attribute discriminators for the effec-\ntive imposition of semantic structures. Following\ntheir work, many methods (Fu et al., 2018; John\net al., 2018; Zhang et al., 2018a,b) has been pro-\nposed based on standard encoder-decoder archi-\ntecture.\nAlthough, learning a latent representation will\nmake the model more interpretable and easy to\nmanipulate, the model which is assumed a ﬁxed\nsize latent representation cannot utilize the infor-\nmation from the source sentence anymore.\nOn the other hand, there are also some ap-\nproaches without manipulating latent representa-\ntion are proposed recently. Xu et al. (2018) pro-\npose a cycled reinforcement learning method for\nunpaired sentiment-to-sentiment translation task.\nLi et al. (2018) propose a three-stage method.\nTheir model ﬁrst extracts content words by delet-\ning phrases a strong attribute value, then retrieves\nnew phrases associated with the target attribute,\nand ﬁnally uses a neural model to combine these\ninto a ﬁnal output. Lample et al. (2019) reduce text\nstyle transfer to unsupervised machine translation\nproblem (Lample et al., 2018). They employ De-\nnoising Auto-encoders (Vincent et al., 2008) and\nback-translation (Sennrich et al., 2016) to build a\ntranslation style between different styles.\nHowever, both lines of the previous models\nmake few attempts to utilize the attention mech-\nanism to refer the long-term history or the source\nsentence, except Lample et al. (2019). In many\nNLP tasks, especially for text generation, atten-\ntion mechanism has been proved to be an essential\ntechnique to enable the model to capture the long-\nterm dependency (Bahdanau et al., 2014; Luong\net al., 2015; Vaswani et al., 2017).\nIn this paper, we follow the second line of work\nand propose a novel method which makes no as-\nsumption about the latent representation of source\nsentence and takes the proven self-attention net-\nwork, Transformer, as a basic module to train a\nstyle transfer system.\nx Encoder z Decoder y\ns\n(a) Disentangled Style Transfer\nx Transformer y\ns\n(b) Style Transformer\nFigure 1: General illustration of previous models and\nour model. z denotes style-independent content vector\nand s denotes the style variable.\n3 Style Transformer\nTo make our discussion more clearly, in this sec-\ntion, we will ﬁrst give a brief introduction to the\nstyle transfer task, and then start to discuss our\nproposed model based on our problem deﬁnition.\n3.1 Problem Formalization\nIn this paper, we deﬁne the style transfer prob-\nlem as follows: Considering a bunch of datasets\n{Di}K\ni=1, and each dataset Di is composed of\nmany natural language sentences. For all of the\nsentences in a single dataset Di , they share some\nspeciﬁc characteristic (e.g. they are all the posi-\ntive reviews for a speciﬁc product), and we refer\nthis shared characteristic as the style of these sen-\ntences. In other words, a style is deﬁned by the\ndistribution of a dataset. Suppose we have K dif-\nferent datasets Di, then we can deﬁne K differ-\nent styles, and we denote each style by the symbol\ns(i). The goal of style transfer is that: given a ar-\nbitrary natural language sentence x and a desired\nstyle ˆs ∈{s(i)}K\ni=1, rewrite this sentence to a new\none ˆx which has the style ˆs and preserve the infor-\nmation in original sentence x as much as possible.\n3.2 Model Overview\nTo tackle the style transfer problem we deﬁned\nabove, our goal is to learn a mapping function\nfθ(x,s) where x is a natural language sentence\nand s is a style control variable. The output of\nthis function is the transferred sentence ˆx for the\ninput sentence x.\nA big challenge in the text style transfer is that\nwe have no access to the parallel corpora. Thus\nwe can’t directly obtain supervision to train our\ntransfer model. In section 3.4, we employ two\ndiscriminator-based approaches to create supervi-\nsion from non-parallel corpora.\nFinally, we will combine the Style Transformer\nnetwork and discriminator network via an overall\nlearning algorithm in section 3.5 to train our style\ntransfer system.\n3.3 Style Transformer Network\nGenerally, Transformer follows the standard\nencoder-decoder architecture. Explicitly, for a in-\nput sentence x = (x1,x2,...,x n), the Transformer\nencoder Enc(x; θE) maps inputs to a sequence\nof continuous representations z = (z1,z2,...,z n).\nAnd the Transformer decoder Dec(z; θD) esti-\nmates the conditional probability for the output\nsentence y = (y1,y2,...,y n) by auto-regressively\nfactorized its as:\npθ(y|x) =\nm∏\nt=1\npθ(yt|z,y1,...,y t−1). (1)\nAt each time step t, the probability of the next\ntoken is computed by a softmax classiﬁer:\npθ(yt|z,y1,...,y t−1) =softmax(ot), (2)\nwhere ot is logit vector outputted by decoder net-\nwork.\nTo enable style control in the standard Trans-\nformer framework, we add a extra style em-\nbedding as input to the Transformer encoder\nEnc(x,s; θE). Therefore the network can com-\npute the probability of the output condition both\non the input sentence x and the style control vari-\nable s. Formally, this can be expressed as:\npθ(y|x,s) =\nm∏\nt=1\npθ(yt|z,y1,...,y t−1), (3)\nand we denote the predicted output sentence of\nthis network by fθ(x,s).\n3.4 Discriminator Network\nSuppose we use x and s to denote the sentence\nand its style from the dataset D. Because of the\nabsence of the parallel corpora, we can’t directly\nobtain the supervision for the case fθ(x,ˆs) where\ns ̸= ˆs. Therefore, we introduce a discriminator\nnetwork to learn this supervision from the non-\nparallel copora.\nThe intuition behind the training of discrimina-\ntor is based on the assumption below: As we men-\ntioned above, we only have the supervision for the\ncase fθ(x,s). In this case, because of the input\nsentence x and chosen style s are both come from\nthe same dataset D, one of the optimum solutions,\nin this case, is to reproduce the input sentence.\nThus, we can train our network to reconstruct the\ninput in this case. In the case of fθ(x,s) where\ns ̸= ˆs, we construct supervision from two ways.\n1) For the content preservation, we train the net-\nwork to reconstruct original input sentencex when\nwe feed transferred sentence ˆy = fθ(x,ˆs) to the\nStyle Transformer network with the original style\nlabel s. 2) For the style controlling, we train a dis-\ncriminator network to assist the Style Transformer\nnetwork to better control the style of the generated\nsentence.\nIn short, the discriminator network is another\nTransformer encoder, which learns to distinguish\nthe style of different sentences. And the Style\nTransformer network receives style supervision\nfrom this discriminator. To achieve this goal, we\nexperiment with two different discriminator archi-\ntectures.\nConditional Discriminator In a setting similar\nto Conditional GANs (Mirza and Osindero, 2014),\ndiscriminator makes decision condition on a in-\nput style. Explicitly, a sentence x and a proposal\nstyle s are feed into discriminator dφ(x,s), and\nthe discriminator is asked to answer whether the\ninput sentence has the corresponding style. In dis-\ncriminator training stage, the real sentence from\ndatasets x, and the reconstructed sentence y =\nfθ(x,s) are labeled as positive, and the transferred\nsentences ˆy = fθ(x,ˆs) where s ̸= ˆs, are labeled\nas negative. In Style Transformer network train-\ning stage, the network fθ is trained to maximize\nthe probability of positive when feed fθ(x,ˆs) and\nˆs to the discriminator.\nMulti-class Discriminator Different from the\nprevious one, in this case, only one sentence is\nfeed into discriminator dφ(x), and the discrimi-\nnator aims to answer the style of this sentence.\nMore concretely, the discriminator is a classiﬁer\nwith K+ 1classes. The ﬁrst K classes represent\nK different styles, and the last class is stand for\nthe generated data from fθ(x,ˆs) , which is also\noften referred as fake sample. In discriminator\ntraining stage, we label the real sentences x and\nreconstructed sentences y = fθ(x,s) to the label\nof the corresponding style. And for the transferred\nsentence ˆy = fθ(x,ˆs) where s ̸= ˆs, is labeled as\nthe class 0. In Style Transformer network learning\nstage, we train the network fθ(x,ˆs) to maximize\nx fθ(x,s) y\ns\nfθ(ˆy,s)y\nfθ(x,ˆs)x\nˆy\nˆs dφ(ˆy)\nLself\nLstyle\nLcycle\nFigure 2: The training process for Style Transformer\nnetwork. The input sentence x and input style s(ˆs) is\nfeed into Transformer network fθ. If the input style s\nis the same as the style of sentence x, generated sen-\ntence y will be trained to reconstruct x. Otherwise,\nthe generated sentence ˆy will be feed into Transformer\nfθ and discriminator dφ to reconstruct input sentence x\nand input style ˆs respectively.\nthe probability of the class which is stand for style\nˆs.\n3.5 Learning Algorithm\nIn this section, we will discuss how to train these\ntwo networks. And the training algorithm of our\nmodel can be divided into two parts: the dis-\ncriminator learning and Style Transformer net-\nwork learning. The brief illustration is shown in\nFigure 2.\n3.5.1 Discriminator Learning\nLoosely speaking, in the discriminator training\nstage, we train our discriminator to distinguish be-\ntween the real sentence x and reconstructed sen-\ntence y = fθ(x,s) from the transferred sentence\nˆy = fθ(x,ˆs). The loss function for the discrimi-\nnator is simply the cross-entropy loss of the clas-\nsiﬁcation problem.\nFor the conditional discriminator:\nLdiscriminator(φ) =−pφ(c|x,s). (4)\nAnd for the multi-class discriminator:\nLdiscriminator(φ) =−pφ(c|x). (5)\nAccording to the difference of discriminator ar-\nchitecture, there is a different protocol for how to\nlabel these sentences, and the details can be found\nin Algorithm 1.\nAlgorithm 1:Discriminator Learning\nInput: Style Transformer fθ, discriminator dφ, and a\ndataset Di with style s\n1 Sample a minibatch of m sentences {x1,x2,...xm}\nfrom Di. ;\n2 foreach x ∈ {x1,x2,...xm} do\n3 Randomly sample a style ˆs(s ̸= ˆs);\n4 Use fθ to generate two new sentence\n5 y = fθ(x,s)\n6 ˆy = fθ(x,ˆs) ;\n7 if dφ is conditional discriminator then\n8 Label {(x,s),(y,s)} as 1 ;\n9 Label {(x,ˆs),(ˆy,ˆs)} as 0 ;\n10 else\n11 Label {x,y} as i;\n12 Label {ˆy} as 0 ;\n13 end\n14 Compute loss for dφ by Eq. (4) or (5) .\n15 end\n3.5.2 Style Transformer Learning\nThe training of Style Transformer is developed ac-\ncording to the different cases of fθ(x,ˆs) where\ns = ˆs or s ̸= ˆs.\nSelf Reconstruction For the case s = ˆs , or\nequivalently, the case fθ(x,s). As we discussed\nbefore, the input sentence x and the input style s\ncomes from the same dataset , we can simply train\nour Style Transformer to reconstruct the input sen-\ntence by minimizing negative log-likelihood:\nLself(θ) =−pθ(y = x|x,s). (6)\nFor the case s ̸= ˆs, we can’t obtain direct su-\npervision from our training set. So, we introduce\ntwo different training loss to create supervision in-\ndirectly.\nCycle Reconstruction To encourage generated\nsentence preserving the information in the input\nsentence x, we feed the generated sentence ˆy =\nfθ(x,ˆs) to the Style Transformer with the style\nof x and training our network to reconstruct orig-\ninal input sentence by minimizing negative log-\nlikelihood:\nLcycle(θ) =−pθ(y = x|fθ(x,ˆs),s). (7)\nStyle Controlling If we only train our Style\nTransformer to reconstruct the input sentence x\nfrom transferred sentence ˆy = fθ(x,ˆs), the net-\nwork can only learn to copy the input to the out-\nput. To handle this degeneration problem, we fur-\nther add a style controlling loss for the generated\nsentence. Namely, the network generated sentence\nˆy is feed into discriminator to maximize the prob-\nability of style ˆs.\nFor the conditional discriminator, the Style\nTransformer aims to minimize the negative log-\nlikelihood of class 1 when feed to the discrimina-\ntor with the style label ˆs:\nLstyle(θ) =−pφ(c = 1|fθ(x,ˆs),ˆs). (8)\nAnd in the case of the multi-class discrimina-\ntor, the Style Transformer is trained to minimize\nthe the negative log-likelihood of the correspond-\ning class of style ˆs:\nLstyle(θ) =−pφ(c = ˆs|fθ(x,ˆs)). (9)\nCombining the loss function we discussed\nabove, the training procedure of the Style Trans-\nformer is summarized in Algorithm 2.\nAlgorithm 2:Style Transformer Learning\nInput: Style Transformer fθ, discriminator dφ, and a\ndataset Di with style s\n1 Sample a minibatch of m sentences {x1,x2,...xm}\nfrom Di. ;\n2 foreach x ∈ {x1,x2,...xm} do\n3 Randomly sample a style ˆs(s ̸= ˆs);\n4 Use fθ to generate two new sentence\n5 y = fθ(x,s)\n6 ˆy = fθ(x,ˆs) ;\n7 Compute Lself(θ) for y by Eq. (6) ;\n8 Compute Lcycle(θ) for ˆy by Eq. (7) ;\n9 Compute Lstyle(θ) for ˆy by Eq. (8) or (9) ;\n10 end\n3.5.3 Summarization and Discussion\nFinally, we can construct our ﬁnal training algo-\nrithm based on discriminator learning and Style\nTransformer learning steps. Similar to the train-\ning process of GANs (Goodfellow et al., 2014), in\neach training iteration, we ﬁrst perform nd steps\ndiscriminator learning to get a better discrimina-\ntor, and then train our Style Transformer nf steps\nto improve its performance. The training process\nis summarized in Algorithm 3.\nBefore ﬁnishing this section, we ﬁnally discuss\na problem which we will be faced with in the train-\ning process. Because of the discrete nature of the\nnatural language, for the generated sentence ˆy =\nfθ(x,ˆs), we can’t directly propagate gradients\nfrom the discriminator through the discrete sam-\nples. To handle this problem, one can use REIN-\nFORCE (Williams, 1992) or the Gumbel-Softmax\ntrick (Kusner and Hern´andez-Lobato, 2016) to es-\ntimates gradients from the discriminator. How-\never, these two approaches are faced with high\nAlgorithm 3:Training Algorithm\nInput: A bunch of datasets {Di}K\ni=1, and each\nrepresent a different style s(i)\n1 Initialize the Style Transformer network fθ, and the\ndiscriminator network dφ with random weights θ,φ ;\n2 repeat\n3 for nd step do\n4 foreach dataset Di do\n5 Accumulate loss by Algorithm 1\n6 end\n7 Perform gradient decent to update dφ.\n8 end\n9 for nf step do\n10 foreach dataset Di do\n11 Accumulate loss by Algorithm 2\n12 end\n13 Perform gradient decent to update fθ.\n14 end\n15 until network fθ(x,s) converges;\nvariance problem, which will make the model hard\nto converge. In our experiment, we also observed\nthat the Gumbel-Softmax trick would slow down\nthe model converging, and didn’t bring much per-\nformance improvement to the model. For the rea-\nsons above, empirically, we view the softmax dis-\ntribution generated byfθ as a “soft” generated sen-\ntence and feed this distribution to the downstream\nnetwork to keep the continuity of the whole train-\ning process. When this approximation is used, we\nalso switch our decoder network from greedy de-\ncoding to continuous decoding. Which is to say, at\nevery time step, instead of feed the token that has\nmaximum probability in previous prediction step\nto the network, we feed the whole softmax distri-\nbution (Eq. (2)) to the network. And the decoder\nuses this distribution to compute a weighted av-\nerage embedding from embedding matrix for the\ninput.\n4 Experiment\n4.1 Datasets\nWe evaluated and compared our approach with\nseveral state-of-the-art systems on two review\ndatasets, Yelp Review Dataset (Yelp) and IMDb\nMovie Review Dataset (IMDb). The statistics of\nthe two datasets are shown in Table 1.\nYelp Review Dataset (Yelp)The Yelp dataset is\nprovided by the Yelp Dataset Challenge, consist-\ning of restaurants and business reviews with senti-\nment labels (negative or positive). Following pre-\nvious work, we use the possessed dataset provided\nby Li et al. (2018). Additionally, it also provides\nhuman reference sentences for the test set.\nDataset Yelp IMDb\nPositive Negative Positive Negative\nTrain 266,041 177,218 178,869 187,597\nDev 2,000 2,000 2,000 2,000\nTest 500 500 1,000 1,000\nAvg. Len. 8.9 18.5\nTable 1: Datasets statistic.\nIMDb Movie Review Dataset (IMDb)The IMDb\ndataset2 consists of movie reviews written by on-\nline users. To get a high quality dataset, we use\nthe highly polar movie reviews provided by Maas\net al. (2011). Based on this dataset, we con-\nstruct a highly polar sentence-level style transfer\ndataset by the following steps: 1) ﬁne tune a BERT\n(Devlin et al., 2018) classiﬁer on original training\nset, which achieves 95% accuracy on test set; 2)\nsplit each review in the original dataset into sev-\neral sentences; 3) ﬁlter out sentences with conﬁ-\ndence threshold below 0.9 by our ﬁne-tuned BERT\nclassiﬁer; 4) remove sentences with uncommon\nwords. Finally, this dataset contains 366K, 4k, 2k\nsentences for training, validation, and testing, re-\nspectively.\n4.2 Evaluation\nA goal transferred sentence should be a ﬂuent,\ncontent-complete one with target style. To evalu-\nate the performance of the different model, follow-\ning previous works, we compared three different\ndimensions of generated samples: 1) Style con-\ntrol, 2) Content preservation and 3) Fluency.\n4.2.1 Automatic Evaluation\nStyle ControlWe measure style control automat-\nically by evaluating the target sentiment accuracy\nof transferred sentences. For an accurate evalu-\nation of style control, we trained two sentiment\nclassiﬁers on the training set of Yelp and IMDb\nusing fastText (Joulin et al., 2017).\nContent PreservationTo measure content preser-\nvation, we calculate the BLEU score (Papineni\net al., 2002) between the transferred sentence\nand its source input using NLTK. A higher\nBLEU score indicates the transferred sentence can\nachieve better content preservation by retaining\nmore words from the source sentence. If a human\nreference is available, we will calculate the BLEU\n2https://github.com/fastnlp/\nnlp-dataset\nModel Yelp IMDb\nACC ref-BLEU self -BLEU PPL ACC self -BLEU PPL\nInput Copy 3.8 23 100 41 5.1 100 58\nRetrieveOnly (Li et al., 2018) 92.6 0.4 0.7 7 N/A N/A N/A\nTemplateBased (Li et al., 2018) 84.3 13.7 44.1 117 N/A N/A N/A\nDeleteOnly (Li et al., 2018) 85.7 9.7 28.6 72 N/A N/A N/A\nDeleteAndRetrieve (Li et al., 2018) 87.7 10.4 29.1 60 58.8 55.4 57\nControlledGen (Hu et al., 2017) 88.8 14.3 45.7 219 94.1 62.1 143\nCrossAlignment (Shen et al., 2017) 76.3 4.3 13.2 53 N/A N/A N/A\nMultiDecoder (Fu et al., 2018) 49.8 9.2 37.9 90 N/A N/A N/A\nCycleRL(Xu et al., 2018) 88.0 2.8 7.2 107 97.8 4.9 177\nOurs (Conditional) 93.7 17.1 45.3 90 86.6 66.2 107\nOurs (Multi-Class) 87.7 20.3 54.9 73 80.3 70.5 105\nTable 2: Automatic evaluation results on Yelp and IMDb datset\nscore between the transferred sentence and corre-\nsponding reference as well. Two BLEU score met-\nrics are referred to as self -BLEU and ref-BLEU\nrespectively.\nFluency Fluency is measured by the perplexity of\nthe transferred sentence, and we trained a 5-gram\nlanguage model on the training set of two datasets\nusing KenLM (Heaﬁeld, 2011).\n4.2.2 Human Evaluation\nDue to the lack of parallel data in style transfer\narea, automatic metrics are insufﬁcient to evaluate\nthe quality of the transferred sentence. Therefore\nwe also conduct human evaluation experiments on\ntwo datasets.\nWe randomly select 100 source sentences (50\nfor each sentiment) from each test set for human\nevaluation. For each review, one source input and\nthree anonymous transferred samples are shown to\na reviewer. And the reviewer is asked to choose\nthe best sentence for style control, content preser-\nvation, and ﬂuency respectively.\n•Which sentence has the most opposite senti-\nment toward the source sentence?\n•Which sentence retains most content from the\nsource sentence?\n•Which sentence is the most ﬂuent one?\nTo avoid interference from similar or same gener-\nated sentences, ”no preference.” is also an option\nanswer to these questions.\n4.3 Training Details\nIn all of the experiment, for the encoder, decoder,\nand discriminator, we all use 4-layer Transformer\nwith four attention heads in each layer. The hidden\nsize, embedding size, and positional encoding size\nin Transformer are all 256 dimensions. Another\nembedding matrix with 256 hidden units is used\nto represent different style, which is feed into en-\ncoder as an extra token of the input sentence. And\nthe positional encoding isn’t used for the style to-\nken. For the discriminator, similar to Radford et al.\n(2018) and Devlin et al. (2018), we further add a\n<cls> token to the input, and the output vector of\nthe corresponding position is feed into a softmax\nclassiﬁer which represents the output of discrimi-\nnator.\nIn the experiment, we also found that preform-\ning random word dropout for the input sentence\nwhen computing the self reconstruction loss (Eq.\n(6)) can help model more easily to converge to a\nreasonable performance. On the other hand, by\nadding a temperature parameter to the softmax\nlayer (Eq. (2)) and using a sophisticated tempera-\nture decay schedule can also help the model to get\na better result in some case.\n4.4 Experimental Results\nResults using automatic metricsare presented in\nTable 2. Comparing to previous approaches, our\nmodels achieve competitive performance overall\nand get better content preservation at all of two\ndatasets. Our conditional model can achieve a bet-\nter style controlling compared to the multi-class\nmodel. Both our models are able to generate sen-\ntences with relatively low perplexity. For those\nprevious models performing the best on a single\nmetric, an obvious drawback can always be found\non another metric.\nFor the human evaluation, we choose two\nModel Yelp IMDb\nStyle Content Fluency Style Content Fluency\nCtrlGen 16.8 23.6 17.7 30.0 19.5 22.0\nDAR 13.6 15.5 21.4 21.0 27.0 25.0\nOurs 48.6 36.8 41.4 29.5 35.0 31.5\nNo Preference 20.9 24.1 19.5 19.5 18.5 21.5\nTable 3: Human evaluation results on two datasets.\nEach cell indicates the proportion of being preferred.\nof the most well-performed models according to\nthe automatic evaluation results as competitors:\nDeleteAndRetrieve (DAR) (Li et al., 2018) and\nControlled Generation (CtrlGen) (Hu et al., 2017).\nAnd the generated outputs from multi-class dis-\ncriminator model is used as our ﬁnal model. We\nhave performed over 400 human evaluation re-\nviews. Results are presented in Table 3. The\nhuman evaluation results are mainly conformed\nwith our automatic evaluation results. And it also\nshows that our models are better in content preser-\nvation, compared to two competitor model.\nFinally, to better understand the characteristic of\ndifferent models, we sampled several output sen-\ntences from the Yelp dataset, which are shown in\nTable 4.\n4.5 Ablation Study\nTo study the impact of different components on\noverall performance, we further did an ablation\nstudy for our model on Yelp dataset, and results\nare reported in Table 5.\nFor better understanding the role of different\nloss functions, we disable each loss function by\nturns and retrain our model with the same setting\nfor the rest of hyperparameters. After we disable\nself-reconstruction loss (Eq. (6)), our model failed\nto learn a meaningful output and only learned\nto generate a single word for any combination\nof input sentence and style. However, when we\ndon’t use cycle reconstruction loss (Eq. (7)), it’s\nalso possible to train the model successfully, and\nboth of two models converge to reasonable perfor-\nmance. And comparing to the full model, there\nis a small improvement in style accuracy, but a\nsigniﬁcant drop in BLEU score. As our expected,\nthe cycle reconstruction loss is able to encourage\nthe model to preserve the information from the in-\nput sentence. At last, when the discriminator loss\n(Eq. (8) and (9)) is not used, the model quickly\ndegenerates to a model which is only copying the\ninput sentence to output without any style modi-\nﬁcation. This behaviour also conforms with our\nintuition. If the model is only asked to minimize\nthe self-reconstruction loss and cycle reconstruc-\ntion loss, directly copying input is one of the op-\ntimum solutions which is the easiest to achieve.\nIn summary, each of these loss plays an important\nrole in the Style Transformer training stage: 1) the\nself-reconstruction loss guides the model to gen-\nerate readable natural language sentence. 2) the\ncycle reconstruction loss encourages the model to\npreserve the information in the source sentence.\n3) the discriminator provides style supervision to\nhelp the model control the style of generated sen-\ntences.\nAnother group of study is focused on the dif-\nferent type of samples used in the discriminator\ntraining step. In Algorithm 1, we used a mixture\nof real sentence x and generated sentence y as\nthe positive training samples for the discriminator.\nBy contrast, in the ablation study, we trained our\nmodel with only one of them. As the result shows,\nthe generated sentence is the key component in\ndiscriminator training. When we remove the real\nsentence from the training data of discriminator,\nour model can also achieve a competitive result\nas the full model with only a small performance\ndrop. However, if we only use the real sentence\nthe model will lose a signiﬁcant part of the abil-\nity to control the style of the generated sentence,\nand thus yields a bad performance in style accu-\nracy. However, the model can still perform a style\ncontrol far better than the input copy model dis-\ncussed in the previous part. For the reasons above,\nwe used a mixture of real sample and generated\nsample in our ﬁnal version.\n5 Conclusions and Future Work\nIn this paper, we proposed the Style Transformer\nwith a novel training algorithm for text style\ntransfer task. Experimental results on two text\nstyle transfer datasets have shown that our model\nachieved a competitive or better performance com-\npared to previous state-of-the-art approaches. Es-\npecially, because our proposed approach doesn’t\nassume a disentangled latent representation for\nmanipulating the sentence style, our model can get\nbetter content preservation on both of two datasets.\nIn the future, we are planning to adapt our Style\nTransformer to the multiple-attribute setting like\nLample et al. (2019). On the other hand, the back-\nnegative to positive\nInput the food ’s ok , the service is among the worst i have encountered .\nDAR the food ’s ok , the service is among great and service among .\nCtrlGen the food ’s ok , the service is among the randy i have encountered .\nOurs the food ’s delicious , the service is among the best i have encountered .\nHuman the food is good , and the service is one of the best i ’ve ever encountered .\nInput this is the worst walmart neighborhood market out of any of them .\nDAR walmart market is one of my favorite places in any neighborhood out of them .\nCtrlGen fantastic is the randy go neighborhood market out of any of them .\nOurs this is the best walmart neighborhood market out of any of them .\nHuman this is the best walmart out of all of them .\nInput always rude in their tone and always have shitty customer service !\nDAR i always enjoy going in always their kristen and always have shitty customer service !\nCtrlGen always good in their tone and always have shitty customer service !\nOurs always nice in their tone and always have provides customer service !\nHuman such nice customer service , they listen to anyones concerns and assist them with it .\npositive to negative\nInput everything is fresh and so delicious !\nDAR small impression was ok , but lacking i have piss stufﬁng night .\nCtrlGen everything is disgrace and so bland !\nOurs everything is overcooked and so cold !\nHuman everything was so stale .\nInput these two women are professionals .\nDAR these two scam women are professionals .\nCtrlGen shame two women are unimpressive .\nOurs these two women are amateur .\nHuman these two women are not professionals .\nInput fantastic place to see a show as every seat is a great seat !\nDAR there is no reason to see a show as every seat seat !\nCtrlGen unsafe place to embarrassing lazy run as every seat is lazy disappointment seat !\nOurs disgusting place to see a show as every seat is a terrible seat !\nHuman terrible place to see a show as every seat is a horrible seat !\nTable 4: Case study from Yelp dataset. The red words indicate good transfer; the blue words indicate bad transfer;\nthe brown words indicate grammar error.\nConditional Multi-class\nModel ACC BLEU PPL ACC BLEU PPL\nStyle Transformer 93.6 17.1 78 87.6 20.3 50\n- self reconstruction 50.0 0 N/A 20.7 0 N/A\n- cycle reconstruction 94.2 8.6 56 93.2 8.7 40\n- discriminator 3.3 22.9 11 3.3 22.9 11\n- real sample 89.7 17.4 75 83.8 19.4 55\n- generated sample 46.3 21.6 34 35.6 22.0 33\nTable 5: Model ablation study results on Yelp dataset\ntranslation technique developed in Lample et al.\n(2019) can also be adapted to the training process\nof Style Transformer. How to combine the back-\ntranslation with our training algorithm is also a\ngood research direction that is worth to explore.\nAcknowledgment\nWe would like to thank the anonymous reviewers\nfor their valuable comments. The research work is\nsupported by National Natural Science Foundation\nof China (No. 61751201 and 61672162), Shang-\nhai Municipal Science and Technology Commis-\nsion (16JC1420401 and 17JC1404100), Shang-\nhai Municipal Science and Technology Major\nProject(No.2018SHZDZX01)and ZJLab.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR,\nabs/1409.0473.\nKeith Carlson, Allen Riddell, and Daniel N. Rockmore.\n2017. Zero-shot style transfer in text using recurrent\nneural networks. CoRR, abs/1711.04731.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G.\nCarbonell, Quoc V . Le, and Ruslan Salakhutdi-\nnov. 2019. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. CoRR,\nabs/1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nYanai Elazar and Yoav Goldberg. 2018. Adversarial\nremoval of demographic attributes from text data. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, Brussels,\nBelgium, October 31 - November 4, 2018, pages 11–\n21.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan\nZhao, and Rui Yan. 2018. Style transfer in text:\nExploration and evaluation. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in Neural Information\nProcessing Systems, pages 2672–2680.\nKenneth Heaﬁeld. 2011. Kenlm: Faster and smaller\nlanguage model queries. In Proceedings of the sixth\nworkshop on statistical machine translation , pages\n187–197. Association for Computational Linguis-\ntics.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P Xing. 2017. Toward con-\ntrolled generation of text. In Proceedings of the 34th\nInternational Conference on Machine Learning-\nVolume 70, pages 1587–1596. JMLR. org.\nZhijing Jin, Di Jin, Jonas Mueller, Nicholas Matthews,\nand Enrico Santus. 2019. Unsupervised Text Style\nTransfer via Iterative Matching and Translation.\narXiv e-prints, page arXiv:1901.11333.\nVineet John, Lili Mou, Hareesh Bahuleyan, and Olga\nVechtomova. 2018. Disentangled representation\nlearning for text style transfer. arXiv preprint\narXiv:1808.04339.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of tricks for efﬁcient\ntext classiﬁcation. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 427–431. Association for Computational\nLinguistics.\nMatt J. Kusner and Jos ´e Miguel Hern ´andez-Lobato.\n2016. GANS for sequences of discrete elements\nwith the gumbel-softmax distribution. CoRR,\nabs/1611.04051.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, Brussels, Belgium, October 31 - November 4,\n2018, pages 5039–5049.\nGuillaume Lample, Sandeep Subramanian, Eric Smith,\nLudovic Denoyer, Marc’Aurelio Ranzato, and Y-\nLan Boureau. 2019. Multiple-attribute text rewrit-\ning. In International Conference on Learning Rep-\nresentations.\nJuncen Li, Robin Jia, He He, and Percy Liang.\n2018. Delete, retrieve, generate: A simple approach\nto sentiment and style transfer. arXiv preprint\narXiv:1804.06437.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2015, Lisbon, Portu-\ngal, September 17-21, 2015, pages 1412–1421.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nIgor Melnyk, C ´ıcero Nogueira dos Santos, Kahini\nWadhawan, Inkit Padhi, and Abhishek Kumar. 2017.\nImproved neural text attribute transfer with non-\nparallel data. CoRR, abs/1711.09395.\nMehdi Mirza and Simon Osindero. 2014. Condi-\ntional generative adversarial nets. arXiv preprint\narXiv:1411.1784.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan\nSalakhutdinov, and Alan W. Black. 2018. Style\ntransfer through back-translation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Papers,\npages 866–876.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\nC´ıcero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on so-\ncial media with unsupervised text style transfer. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 189–194.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2016, August 7-12, 2016,\nBerlin, Germany, Volume 1: Long Papers.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In Advances in neural informa-\ntion processing systems, pages 6830–6841.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 6000–6010.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio,\nand Pierre-Antoine Manzagol. 2008. Extracting\nand composing robust features with denoising au-\ntoencoders. In Machine Learning, Proceedings of\nthe Twenty-Fifth International Conference (ICML\n2008), Helsinki, Finland, June 5-9, 2008 , pages\n1096–1103.\nRonald J. Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine Learning, 8:229–256.\nJingjing Xu, SUN Xu, Qi Zeng, Xiaodong Zhang, Xu-\nancheng Ren, Houfeng Wang, and Wenjie Li. 2018.\nUnpaired sentiment-to-sentiment translation: A cy-\ncled reinforcement learning approach. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), volume 1, pages 979–988.\nYe Zhang, Nan Ding, and Radu Soricut. 2018a.\nSHAPED: shared-private encoder-decoder for text\nstyle adaptation. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 1528–1538.\nZhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang,\nPeng Chen, Mu Li, Ming Zhou, and Enhong Chen.\n2018b. Style transfer as unsupervised machine\ntranslation. CoRR, abs/1808.07894.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7973320484161377
    },
    {
      "name": "Sentence",
      "score": 0.6929726004600525
    },
    {
      "name": "Computer science",
      "score": 0.6698888540267944
    },
    {
      "name": "Encoder",
      "score": 0.6154686808586121
    },
    {
      "name": "Natural language processing",
      "score": 0.5156716108322144
    },
    {
      "name": "Style (visual arts)",
      "score": 0.504334568977356
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4803149104118347
    },
    {
      "name": "Representation (politics)",
      "score": 0.47594761848449707
    },
    {
      "name": "Artificial neural network",
      "score": 0.4642043709754944
    },
    {
      "name": "Engineering",
      "score": 0.09392857551574707
    },
    {
      "name": "History",
      "score": 0.06835076212882996
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    }
  ],
  "cited_by": 11
}