{
    "title": "Inclusive prompt engineering for large language models: a modular framework for ethical, structured, and adaptive AI",
    "url": "https://openalex.org/W4413394193",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2737779751",
            "name": "mohamad saleh torkestani",
            "affiliations": [
                "University of Exeter"
            ]
        },
        {
            "id": "https://openalex.org/A2474691185",
            "name": "Ali AlAmeer",
            "affiliations": [
                "University of Salford"
            ]
        },
        {
            "id": "https://openalex.org/A191983659",
            "name": "Shivakumara Palaiahnakote",
            "affiliations": [
                "University of Salford"
            ]
        },
        {
            "id": null,
            "name": "Taha Manosuri",
            "affiliations": [
                "University of Salford"
            ]
        },
        {
            "id": "https://openalex.org/A2737779751",
            "name": "mohamad saleh torkestani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2474691185",
            "name": "Ali AlAmeer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A191983659",
            "name": "Shivakumara Palaiahnakote",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Taha Manosuri",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4396820857",
        "https://openalex.org/W3093203077",
        "https://openalex.org/W2951939640",
        "https://openalex.org/W4389519605",
        "https://openalex.org/W3044663232",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W2773523653",
        "https://openalex.org/W4287774713",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W4299805482",
        "https://openalex.org/W4401209579",
        "https://openalex.org/W4392085131",
        "https://openalex.org/W6859220173",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4402671980",
        "https://openalex.org/W3200447851",
        "https://openalex.org/W4322716280",
        "https://openalex.org/W4288235115",
        "https://openalex.org/W4211192987",
        "https://openalex.org/W4403575159",
        "https://openalex.org/W4302011807",
        "https://openalex.org/W6846444394",
        "https://openalex.org/W4402674664",
        "https://openalex.org/W3088099248",
        "https://openalex.org/W4385565417",
        "https://openalex.org/W4385571662",
        "https://openalex.org/W4378770656",
        "https://openalex.org/W4392011604",
        "https://openalex.org/W4412945213",
        "https://openalex.org/W4299499644",
        "https://openalex.org/W6838865847",
        "https://openalex.org/W2964091467",
        "https://openalex.org/W4385281527",
        "https://openalex.org/W4389518711",
        "https://openalex.org/W4404345239",
        "https://openalex.org/W4392558382",
        "https://openalex.org/W4385567371",
        "https://openalex.org/W4404089395",
        "https://openalex.org/W2469052862",
        "https://openalex.org/W4294214983",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W4392011503",
        "https://openalex.org/W4361866090",
        "https://openalex.org/W4392949855",
        "https://openalex.org/W4388161955",
        "https://openalex.org/W3123893780",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W4388716384",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W4401042848",
        "https://openalex.org/W4391835702",
        "https://openalex.org/W4206723194",
        "https://openalex.org/W4404394948",
        "https://openalex.org/W4401390583",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4386238149",
        "https://openalex.org/W4380559074",
        "https://openalex.org/W4403556106",
        "https://openalex.org/W4394655428",
        "https://openalex.org/W4387428151",
        "https://openalex.org/W4389599557",
        "https://openalex.org/W4388531802",
        "https://openalex.org/W4304195432",
        "https://openalex.org/W4387355871",
        "https://openalex.org/W4378465140",
        "https://openalex.org/W4319165821",
        "https://openalex.org/W4399438671",
        "https://openalex.org/W4401248343",
        "https://openalex.org/W3198377975"
    ],
    "abstract": "Abstract Large language models have achieved impressive results across various tasks but remain limited in their ability to adapt ethically and structurally across diverse domains without retraining. This paper presents the Inclusive Prompt Engineering Model (IPEM), a modular framework designed to enhance LLM performance, adaptability, and ethical alignment through prompt-level strategies alone. IPEM integrates four components: Memory-of-Thought for multi-turn consistency, Enhanced Chain-of-Thought prompting for logical verification, Structured and Analogical Reasoning modules for tabular and cross-domain tasks, and Evaluation and Feedback Loops that incorporate uncertainty-aware selection and bias mitigation mechanisms. Evaluated across tasks in arithmetic reasoning, healthcare triage, financial forecasting, and inclusive question answering, IPEM consistently improves model outputs over a GPT-4 baseline. Notable outcomes include up to twenty percentage points in accuracy gains, a 25 percent reduction in logical errors, and nearly 20 percent reduction in social bias scores, all without modifying model weights. Moreover, IPEM reduces annotation demands by one-third while preserving performance, demonstrating its utility in low-resource environments. By unifying ethical safeguards and reasoning mechanisms in a prompt-based system, IPEM offers a reproducible and auditable pathway for deploying adaptable and fair AI systems. The framework contributes both practical solutions and theoretical insights to the evolving field of prompt engineering.",
    "full_text": "Accepted: 15 July 2025 / Published online: 21 August 2025\n© Crown 2025\n \r Mohamad Saleh Torkestani\nm.torkestani@exeter.ac.uk\n1 Faculty of Environment, Science and Economy, University of Exeter, 1.65a Streatham Court, \nRennes Drive, Exeter EX4 4PU, UK\n2 School of Science, Engineering & Environment, University of Salford, Manchester, UK\nInclusive prompt engineering for large language models: a \nmodular framework for ethical, structured, and adaptive AI\nMohamad Saleh Torkestani1  · Ali Alameer2  · Shivakumara Palaiahnakote2  · \nTaha Manosuri2\nArtificial Intelligence Review (2025) 58:348\nhttps://doi.org/10.1007/s10462-025-11330-7\nAbstract\nLarge language models have achieved impressive results across various tasks but remain \nlimited in their ability to adapt ethically and structurally across diverse domains without \nretraining. This paper presents the Inclusive Prompt Engineering Model (IPEM), a modu -\nlar framework designed to enhance LLM performance, adaptability, and ethical alignment \nthrough prompt-level strategies alone. IPEM integrates four components: Memory-of-\nThought for multi-turn consistency, Enhanced Chain-of-Thought prompting for logical \nverification, Structured and Analogical Reasoning modules for tabular and cross-domain \ntasks, and Evaluation and Feedback Loops that incorporate uncertainty-aware selection \nand bias mitigation mechanisms. Evaluated across tasks in arithmetic reasoning, health -\ncare triage, financial forecasting, and inclusive question answering, IPEM consistently \nimproves model outputs over a GPT-4 baseline. Notable outcomes include up to twenty \npercentage points in accuracy gains, a 25 percent reduction in logical errors, and nearly 20 \npercent reduction in social bias scores, all without modifying model weights. Moreover, \nIPEM reduces annotation demands by one-third while preserving performance, demon -\nstrating its utility in low-resource environments. By unifying ethical safeguards and rea -\nsoning mechanisms in a prompt-based system, IPEM offers a reproducible and auditable \npathway for deploying adaptable and fair AI systems. The framework contributes both \npractical solutions and theoretical insights to the evolving field of prompt engineering.\nKeywords Prompt engineering · Large language models · Ethical AI · Chain-of-thought \nreasoning · Bias mitigation\n1 3\nM. S. Torkestani et al.\n1 Introduction\nLarge language models (LLMs) such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et \nal., 2022), and Claude have enabled significant progress in natural language understanding, \nreasoning, and generation. These models now assist in fields ranging from healthcare triage \nto financial forecasting and legal decision support. However, their widespread adoption has \nexposed two persistent and often conflicting challenges: (1) how to produce high-quality, \ncontext-sensitive outputs across diverse tasks without extensive retraining or domain-spe -\ncific engineering, and (2) how to ensure that such outputs do not perpetuate or amplify \nethical harms, such as bias, exclusion, or misinformation (Bender et al., 2021; Zhang et al., \n2023a).\nCurrent prompting methods offer partial solutions. Chain-of-Thought (CoT) techniques \nimprove reasoning on math and logic tasks (Wei et al., 2022b), while role-based prompting \nor demographic balancing can mitigate certain forms of bias (Manas et al., 2024; Taluk-\ndar & Biswas, 2023). However, these tools are often used in isolation, are not robust to \ndomain shifts, and lack mechanisms for feedback, memory, or coordination across turns. \nFor instance, an ethically neutral CoT prompt might still produce hallucinations or con -\ntradictions when used in a high-stakes medical dialogue if it cannot retrieve prior context. \nSimilarly, memory-augmented prompting may retrieve biased or domain-inappropriate \nchains if not filtered or corrected in real time. The requirement for generalization across \ndomains is crucial for practical deployments, especially in multi-domain virtual assistants, \nlow-resource environments, or dynamically changing contexts, where retraining models for \neach new task or domain is infeasible due to time, cost, or limited data availability. Empiri-\ncally, this need is evident in applications like healthcare triage, financial forecasting, and \neducational support, where prompt-level strategies must quickly adapt without substantial \nmodel adjustments (Arora et al., 2023; Fu et al., 2023b).\nThis paper addresses the following problem: current prompting methods for LLMs are \nunable to jointly deliver domain-adaptable reasoning performance and ethical responsive -\nness without manual tuning or model retraining. Prior work tends to optimize for isolated \nobjectives (such as accuracy through Chain-of-Thought (CoT) prompting or fairness via \nrole-sensitive filters) yet fails to define how such methods can be coordinated, scaled, and \nevaluated together. This gap is especially problematic in real-world deployments where \ntasks require both reasoning accuracy and social reliability. IPEM is explicitly designed \nand evaluated for domain-critical but controlled contexts such as healthcare triage, finan -\ncial forecasting, and educational applications. However, extremely sensitive high-stakes \ndomains, including criminal justice and high-risk clinical decision-making, are explicitly \nbeyond IPEM’s current intended scope due to specialized ethical and regulatory com -\nplexities requiring dedicated frameworks. In response, we introduce the Inclusive Prompt \nEngineering Model (IPEM), a modular architecture designed to address this integrated chal-\nlenge. IPEM brings together memory retention, structured reasoning, analogical transfer, \nethical feedback loops, and dynamic prompt orchestration under a cohesive system design. \nRather than merely stacking prompting techniques, IPEM formalizes component interaction \nthrough a control interface that permits shared memory, bias-aware reranking, and evalua -\ntion feedback, across tasks and domains. It is designed to be scalable, auditable, and adap -\ntive without requiring retraining of the underlying LLMs. In this context,'adaptability'spe\ncifically refers to IPEM’s ability to effectively manage input variability (such as differing \n1 3\n348 Page 2 of 51\nInclusive prompt engineering for large language models: a modular…\nprompt structures or conversation turns), domain transfer (applying learned strategies from \none domain to another), and the incorporation of real-time user or system feedback into \nsubsequent model responses. This framework is empirically validated across five domains \nand evaluated on reasoning accuracy, fairness metrics, generalization across domains, and \ndata efficiency.\nWe identified three major limitations in existing prompting frameworks that IPEM is \ndesigned to address. First, many current approaches handle methods like chain-of-thought \nreasoning, memory use, and fairness techniques in isolation. This separation often leads \nto conflicting behavior or missed opportunities to share insights across tasks (Zhang et al., \n2023c; Li and Qiu, 2023). Second, prompt templates are often fixed or narrowly fine-tuned, \nwhich means they struggle to adapt to new reasoning outcomes or ethical considerations as \nthey emerge (Fu et al., 2023b). Third, performance and ethical evaluation are usually treated \nas separate goals. Accuracy and fairness are assessed independently, making it hard to man-\nage trade-offs or prioritize effectively when both are important. The architecture we propose \nis built around the following hypotheses. Hypothesis H1 suggests that Memory-of-Thought \nenhances multi-turn coherence and continuity in reasoning-intensive tasks across various \ndomains, such as GSM8K and S&P500. Hypothesis H2 holds that analogical and tabular \nchain-of-thought modules enable structural generalization across domains without the need \nfor additional fine-tuning. Hypothesis H3 proposes that real-time evaluation and feedback \nmechanisms can help reduce bias scores on benchmarks like StereoSet, without compromis-\ning overall task performance.\n2 Background\nThe evolution of natural language processing (NLP) has been shaped by advances in model \narchitecture and training methods. Early static embedding models like word2vec (Mikolov \net al., 2013) and GloVe (Pennington et al., 2014) improved syntactic and semantic repre -\nsentations but could not model context or long-range dependencies. The introduction of the \ntransformer architecture as demonstrated in Fig. 1 (Vaswani et al., 2017) marked a critical \nturning point, enabling models such as BERT (Devlin et al., 2019) and T5 (Raffel et al., \n2020) to learn contextual embeddings through self-attention mechanisms.\nBERT’s modularity (Fig. 2) with a shared encoder for general pre-training and task-\nspecific heads for downstream fine-tuning, was an early demonstration of architectural reuse \nto balance generalization and task specificity. While IPEM does not adopt BERT’s trans -\nformer mechanics, it extends this modular design philosophy to the prompt level. Specifi -\ncally, IPEM introduces swappable, interoperable prompting modules (e.g., memory control, \nanalogical transfer, contrastive CoT) that operate independently yet harmoniously, enabling \nprompt reuse, targeted adaptation, and ethical evaluation without architectural retraining. \nThe inclusion of Fig. 2 is intended not as a model comparison, but to provide a conceptual \nbridge from encoder-level modularity in classical architectures to prompt-layer modularity \nin instruction-tuned LLMs, the design space where IPEM innovates.\nWith the rise of LLMs like GPT-3, performance on language tasks surged, but concerns \naround hallucinations, fairness, and adaptability quickly surfaced. Prompt engineering \nemerged as a strategy to modulate model behavior without weight updates. Techniques \nevolved from zero-shot and few-shot prompting to more structured methods like Chain-\n1 3\nPage 3 of 51 348\nM. S. Torkestani et al.\nFig. 2 BERT is an early and successful example of transformers, utilizing the same architecture for both \npre-training and fine-tuning. Pre-trained parameters initialize models for downstream tasks, and all pa -\nrameters are adjusted during fine-tuning (Devlin et al., 2019)\n \nFig. 1 Transformer Model Ar-\nchitecture; the Transformer uses \nstacked self-attention mecha-\nnisms along with pointwise fully \nconnected layers in both the \nencoder and decoder, enabling \nefficient parallel processing and \neffective handling of long-range \ndependencies in sequences. \n(Vaswani et al., 2017)\n \n1 3\n348 Page 4 of 51\nInclusive prompt engineering for large language models: a modular…\nof-Thought (CoT) and memory augmentation. Despite their effectiveness, these methods \nare often evaluated in isolation. Few studies have shown how to coordinate prompt logic \nacross tasks while maintaining both reasoning integrity and ethical safeguards. This moti -\nvates IPEM’s development as a unified framework that structures the interaction between \nmultiple prompt mechanisms under a shared control interface. The following background \nsubsections detail the evolution of prompt engineering techniques that IPEM builds upon \nhighlighting the gaps in integration that IPEM is designed to fill.\n2.1 Foundations of prompt engineering in large language models\nPrompt engineering has become central to optimizing LLMs for complex reasoning, deci -\nsion-making, and adaptable response generation. Unlike traditional machine learning mod-\nels with static architecture, prompt engineering enables LLMs to dynamically interpret and \nrespond to task-specific inputs (Brown et al., 2020; Zhang et al., 2023c). Early methods, \nsuch as zero-shot and few-shot prompting, use contextual examples to guide model behav -\nior. In zero-shot prompting, models receive minimal instruction, while few-shot prompting \nintroduces exemplars to refine output (Wei et al., 2022b). Though effective for simple tasks, \nthese methods often struggle with scalability and lack robustness in multi-step reasoning \nscenarios (Kojima et al., 2022).\nTo overcome these challenges, Chain-of-Thought (CoT) prompting was introduced. This \ntechnique structures prompt to reflect step-by-step, human-like reasoning, significantly \nimproving performance in complex tasks (Wei et al., 2022b). CoT has proven effective \nacross various domains, including commonsense reasoning and question answering (Zhou \net al., 2022). However, its dependence on carefully curated demonstrations limits scalabil -\nity, especially in dynamic or real-time environments (Diao et al., 2024).\n2.2 Automation and contrastive techniques\nTo address CoT’s scalability constraints, Automatic Chain-of-Thought prompting has been \nproposed, enabling LLMs to autonomously generate reasoning steps without handcrafted \nexemplars (Chia et al., 2023). This autonomy increases applicability across a broader range \nof reasoning tasks. Further extending CoT, Contrastive Chain-of-Thought presents both \ncorrect and incorrect reasoning paths, helping models discern logical validity. This con -\ntrastive method is particularly valuable in precision-critical domains such as legal reason -\ning (Manas et al., 2024). For structured data, Tabular Chain-of-Thought offers specialized \ncapabilities. It enables LLMs to perform both row-wise and column-wise reasoning across \nmulti-dimensional tables, making it especially useful for tasks like financial forecasting and \ndata interpretation (Jin & Lu, 2023a). Unlike linear CoT, this variant supports parallel and \nhierarchical reasoning required for structured data environments.\n2.3 Analogical and memory-augmented prompting\nTo support more adaptive problem-solving, analogical prompting has emerged, drawing \ninspiration from human analogical reasoning. This technique encourages LLMs to apply \nknowledge from past examples to novel contexts, improving performance in complex \ndomains like legal and scientific reasoning (V osniadou & Ortony, 1989; Yasunaga et al., \n1 3\nPage 5 of 51 348\nM. S. Torkestani et al.\n2023). Complementing analogical approaches, memory-augmented prompting, especially \nthe Memory-of-Thought mechanism, enhances continuity by storing and retrieving prior \nreasoning chains (Li & Qiu, 2023). By maintaining intermediate reasoning steps as memory \nunits, LLMs can ensure consistency across sessions and reduce redundancy. This function-\nality is particularly beneficial in domains like healthcare and education, where continuity \nand reliability are crucial.\n2.4 Active and complexity-based prompting\nBuilding on these advances, active prompting and complexity-based prompting represent \nthe latest innovations in prompt engineering (see Fig. 3). Active prompting applies prin -\nciples from active learning (Markant et al., 2016), selecting examples based on uncertainty \nmetrics to guide LLMs toward high-value reasoning instances. Complexity-based prompt -\ning complements this by ranking exemplars according to reasoning difficulty, prioritizing \ntasks that require deep, multi-layered analysis (Fu et al., 2023b). These strategies are espe-\ncially effective in high-stakes fields like financial modeling and policy analysis (Yang et al., \n2023). Notably, they align with educational research indicating that human learners benefit \nmost when challenged with complex tasks (Fu et al., 2022). As illustrated in Fig. 3, active \nand complexity-based prompting build upon earlier methods, zero-shot, few-shot, Chain-of-\nThought, and Memory-of-Thought, culminating in a more adaptable and efficient approach \nto complex reasoning.\n2.5 Bias and inclusivity\nThe ethical implications of prompt engineering have become a focal point in efforts to \nreduce inherent biases in LLM outputs. Research, including benchmark studies like Stereo-\nSet, has demonstrated that prompts can unintentionally reinforce cultural or demographic \nstereotypes (Nadeem et al., 2021). To address these risks, bias mitigation strategies such \nas persona analysis and uncertainty filtering have been introduced, forming a core compo -\nnent of IPEM’s inclusive and ethical design (Manas et al., 2024). Techniques like Selective \nAnnotation and Self-Generated In-Context Learning further contribute to ethical alignment \nby enabling LLMs to autonomously produce diverse, context-aware demonstrations (Taluk-\ndar & Biswas, 2023). These methods are particularly critical in sensitive domains such \nas healthcare, legal advising, and public policy, where balanced and equitable outputs are \nessential. However, many current approaches are limited in scope and scalability, under -\nZero-Shot \nFew-Shot \nFFoouunnddaa oonnaall\nTTeecchhnniiqquueess\nChain-of-Thought SSttrruuccttuurreedd\nRReeaassoonniinngg\nAutomated CoT\nEEnnhhaanncceemmeennttss\nttoo CCooTT\nAAddvvaanncceedd\nRReeaassoonniinngg\nMMeecchhaanniissmmss\nMemory-of-\nThought\nAnalogical \nAAddaappttiivvee\nPPrroommppttiinngg\nMMeetthhooddssComplexity-Based \nFig. 3 Evolution of prompt engineering techniques in LLMs\n \n1 3\n348 Page 6 of 51\nInclusive prompt engineering for large language models: a modular…\nscoring the need for a more systematic and multi-dimensional framework for ethical evalu-\nation, an issue directly addressed through IPEM’s Evaluation and Feedback Loops.\n2.6 Knowledge retention and complex contextual reasoning\nRecent work has explored the potential of LLMs as dynamic repositories of knowledge, \ncapable of flexible retrieval via cloze-style prompting (Petroni et al., 2019). While promis-\ning, such approaches are prone to hallucinations, plausible yet factually incorrect responses, \nraising serious concerns in high-stakes, accuracy-critical fields like medical diagnostics. \nTechniques like Contrastive Chain-of-Thought mitigate this by cross-referencing outputs \nwith verified information, thereby improving factual consistency (Fu et al., 2023a). Addi-\ntionally, complexity-based prompting enhances contextual depth by encouraging multi-step, \nlayered reasoning that extends beyond superficial pattern recognition (Ethayarajh, 2019). \nThis is especially valuable in advanced analytical domains such as scientific inquiry and \nlegal interpretation (Fu et al., 2023b).\n2.7 Summary and research gap\nDespite significant progress in areas such as memory augmentation, structured reasoning, \nanalogical prompting, and bias mitigation, research often treats these elements separately. \nMany state-of-the-art systems focus heavily on one aspect, like improving reasoning with \nChain-of-Thought methods, while paying less attention to equally important components \nsuch as fairness or the integration of long-term memory. Likewise, approaches that incor -\nporate memory may fail to consider how previously acquired knowledge could unintention-\nally carry forward biases from earlier tasks. This fragmented perspective raises important \nquestions. How can these diverse techniques be brought together in a unified framework \nthat maintains strong performance while also promoting ethical responsibility? And how \ncan systems make use of past learning to improve future reasoning without repeating earlier \nmistakes or reinforcing existing biases?\nThe Inclusive Prompt Engineering Model (IPEM) addresses this research gap by inte -\ngrating four interdependent pillars: Memory-of-Thought, Structured and Analogical Rea -\nsoning, Enhanced Chain-of-Thought, and Evaluation and Feedback Loops. Rather than \nsimply aggregating techniques, IPEM enforces a modular yet iterative architecture in which \neach pillar reinforces the others. For instance, memory retrieval is not only based on task \nrelevance but also filtered for potential bias in prior reasoning. Similarly, Contrastive Chain-\nof-Thought can be augmented with analogical prompts designed from role-based or demo -\ngraphic perspectives, reducing the likelihood of replicating harmful stereotypes. IPEM is \nthus equipped to handle structured tasks (e.g., multi-column data analysis), unstructured \nopen-domain queries, and socially sensitive applications, while maintaining ethical integrity \nand contextual precision.\n1 3\nPage 7 of 51 348\nM. S. Torkestani et al.\n3 Methodology\nThis study followed a three-phase research plan; a preregistered systematic review, a the -\nmatic analysis that mapped prior work to functional requirements, and an implementation–\nevaluation cycle in which each Inclusive Prompt Engineering Model (IPEM) module was \nbuilt, ablated, and stress-tested. To ensure transparency and reproducibility, the methodol -\nogy is anchored in the PRISMA (Preferred Reporting Items for Systematic Reviews and \nMeta-Analyses) framework. This provides a standardized structure for identifying, screen -\ning, and selecting relevant studies (Page et al., 2021).\n3.1 Systematic literature review with PRISMA framework\nTo ensure a comprehensive understanding of existing prompt engineering techniques, a sys-\ntematic literature review was conducted following the PRISMA guidelines, as illustrated \nin Fig. 4, which ensure a transparent and comprehensive approach. The PRISMA process \ninvolved four main stages: Identification, Screening, Eligibility, and Inclusion (Page et al., \n2021). Each phase is detailed below, with accompanying explanations and justifications for \nthe decisions made.\n3.1.1 Identification\nA comprehensive literature search was conducted across leading academic databases, \nincluding IEEE Xplore, ACM Digital Library, Google Scholar, EBSCOhost, ScienceDi -\nrect, and arXiv, to capture both peer-reviewed publications and preprints related to prompt \ndatabase searching \n(n = 460)\nSSccrreeeenniinnggIInncclluuddeedd EElliiggiibbiilliittyy IIddeenn ﬁﬁccaa oonn\nDuplicate records removed \n(n = 145)\nRecords screened\n(n = 315) and abstract relevance \n(n = 190)\nFull-\nfor eligibility (n = 125)\nFull- with \nreasons such as \"Not relevant to \nmethodological rigor,\" or \"Focus \non unrelated ﬁelds.\" (n = 65)\nStudies included in \n(n = 60)\nFig. 4 illustrates the systematic literature review process following the PRISMA framework. This dia -\ngram outlines each stage of the review, including the number of studies identified, screened, assessed for \neligibility, and included in the final synthesis\n \n1 3\n348 Page 8 of 51\nInclusive prompt engineering for large language models: a modular…\nengineering and associated techniques in large language models (LLMs). The following \nBoolean template was executed in each index, with field tags adapted to the engine:\n(\"prompt engineering\" OR \"in-context learning\") AND\n(\"large language model\" OR \"LLM\") AND\n(\"chain of thought\" OR \"memory\" OR \"bias\" OR \"analogy\")\nSearch strings were expanded with synonyms during pilot runs; the final set returned 460 \nrecords (Table 1).\nEquation 1: Total Number of Records Identified\n Nidentified =\n∑n\ni=1\nNdatabasei  (1)\nwhere:\no Nidentified  = Total number of records identified across all databases.\no Ndatabasei  = Number of records retrieved from database iii.\no n = Total number of databases searched.\nUsing Eq. 1, we calculated:\n Nidentified = 460records\nThe below equation presents the formula used to calculate the total number of unique \nrecords after removing duplicates:\n Nunique = Nidentified − Nduplicate\n Nunique = 460− 145 = 315studies\n3.1.2 Screening\nThe initial results were screened for duplicates, reducing the dataset to 315 unique studies. A \nfurther round of screening was conducted based on title and abstract relevance, eliminating \nTable 1 Number of records retrieved from each database\nDatabase Number of Records (Ndatabase)\nIEEE xplore 83\nACM digital library 62\nGoogle scholar 124\nEBSCOhost 63\nScienceDirect 69\narXiv 59\nTotal 460\n1 3\nPage 9 of 51 348\nM. S. Torkestani et al.\nworks that did not directly address LLM prompt engineering or related frameworks. Titles \nand abstracts had to mention a transformer-based LLM; studies centered on speech, vision-\nonly models, or non-English corpora were excluded. Two reviewers applied the criteria \nindependently; disagreements (12%) were settled by discussion. This step narrowed the \npool to 125 studies deemed potentially eligible for full review.\nEquation: Number of Studies After Screening\n Nscreened = Nunique − Nexcludedscreening\n Nscreened = 315− 190 = 125studies\n3.1.3 Eligibility\nA full-text analysis was then performed on the 125 shortlisted studies. Full texts had to \nprovide empirical evidence on at least one of the following areas, including memory-aug -\nmented prompting, structured or analogical reasoning, chain-of-thought variants, or bias \nmitigation in LLM outputs. Pre-prints that lacked experimental sections were logged but \nexcluded from the quantitative synthesis. Sixty studies met all criteria, and their metadata, \nlicense statements, and YAML-encoded annotation files are archived along with the code \nrelease.\nEquation: Number of Studies Eligible for Inclusion\n Neligible = Nscreened − Nexcludedeligibility\n Neligible = 125− 65 = 60studies\n3.1.4 Inclusion\nThe final selection comprised sixty studies, which formed the evidence base for the thematic \nanalysis and the subsequent design of IPEM’s four core pillars. These studies were system-\natically coded to extract key insights related to prompt adaptability, Chain-of-Thought tech-\nniques, self-improvement mechanisms, and ethical considerations including bias mitigation \nand inclusivity. This coding process led to the formation of distinct thematic groupings. Fig-\nure 4 illustrates the full PRISMA review process. Further categorization and cross-referenc-\ning of insights are presented in Appendix A, which summarizes how each study contributed \nto the structural and operational development of IPEM. The analysis revealed two criti -\ncal gaps in the literature. One was the lack of scalable techniques for managing complex, \nmulti-step tasks without extensive fine-tuning. The other was the limited integration of ethi-\ncal bias mitigation and inclusivity, particularly in socially sensitive applications of LLMs. \nThese gaps directly informed the conceptualization of IPEM’s integrated framework, which \nemphasizes both technical adaptability and ethical alignment.\nWe acknowledge certain limitations in our sampling strategy. Although we employed \nmultiple indexing sources to broaden coverage, disparities in metadata quality, indexing \nlatency, and citation visibility across these platforms may have introduced sampling bias. \nIn particular, Google Scholar’s inclusivity of non-peer-reviewed materials and occasional \nduplication across repositories may have skewed frequency counts or thematic empha -\n1 3\n348 Page 10 of 51\nInclusive prompt engineering for large language models: a modular…\nsis. Conversely, more restrictive databases like Scopus may underrepresent emerging but \nimpactful preprints. To mitigate these issues, we manually verified inclusion criteria and \nbalanced high-impact venues with thematic relevance across disciplines.\n3.2 Thematic analysis\nA thematic analysis, following the methodology of Paré & Kitsiou (2017), was used to inter-\npret and organize insights from the systematic review into functional themes that underpin \nIPEM’s design. Each of the 60 selected studies was reviewed in detail to extract recur -\nring concepts and strategies in prompt engineering. These were organized into five primary \nthematic categories, reflecting distinct yet interrelated contributions to prompt design and \nethical control:\n ● Memory and Self-Improvement, enabling models to recall and leverage prior reasoning \nto enhance consistency and task continuity without additional fine-tuning.\n ● Chain-of-Thought Techniques, supporting multi-step reasoning through structured trace \ngeneration, including automated and contrastive extensions.\n ● Ethical and Bias Mitigation, emphasizing prompt-level or memory-level safeguards \nagainst stereotype propagation and demographic imbalances.\n ● Structured Reasoning Methods, which improve handling of tabular and multidimen -\nsional data by guiding attention along schema-aware paths.\n ● Analogical and Role-Based Reasoning, helping LLMs generalize across domains by \nidentifying latent task similarity or switching perspective through persona modeling.\nEach theme was carefully mapped to a corresponding functional role within IPEM’s \narchitecture. For instance, memory-augmented prompting was classified under Memory \nand Self-Improvement due to its role in capturing and reusing validated reasoning traces. \nTabular Chain-of-Thought and schema-conditioned prompts were categorized under Struc-\ntured Reasoning, while analogy-based demonstrations and role-conditioned examples were \ngrouped under Analogical and Role-Based Reasoning. This separation clarified the architec-\nture’s modularity, ensuring that each thematic contribution addresses a distinct limitation in \ncurrent prompting practice. A consolidated summary of these themes is presented in Table 2.\nTo ensure the reliability and validity of our thematic analysis, we employed multiple \nresearchers to independently code the included studies. Three researchers participated in \nthe coding process, each reviewing all 60 studies and assigning initial codes based on the \ncontent relevant to our research objectives.\nAfter the initial coding, we assessed the inter-rater reliability using Cohen's Kappa coeffi-\ncient (κ), a statistical measure that evaluates the level of agreement between coders beyond \nwhat would be expected by chance. The formula for Cohen's Kappa is:\n \nκ = po − pe\n1 − pe\nwhere:\npo = Observed agreement among coders.\npe = Expected agreement by chance.\n1 3\nPage 11 of 51 348\nM. S. Torkestani et al.\nObserved Agreement (po) was calculated by dividing the number of coding instances \nwhere all coders agreed by the total number of coding instances. For our analysis:\n \npo = Numberofagreements\nT otalnumberofcodinginstances =0 .87\nExpected Agreement (pe) was determined based on the probability that coders would agree \nby chance, considering the distribution of codes assigned. The calculation yielded:\n (pe)=0 .25\nUsing the values obtained, we calculated Cohen's Kappa as follows:\n κ = 0.87 − 0.25\n1 − 0.25 = 0.62\n0.75 =0 .8267\nA Cohen’s Kappa value of 0.8267 reflects “almost perfect agreement” according to \nMcHugh’s (2012) benchmarks, confirming that the coding process was both consistent \nand replicable across researchers. The full codebook and a worked annotation example are \nincluded in Appendix J.\n3.3 IPEM implementation details\nAll experiments used the OpenAI GPT-4_1-2025–04-14 checkpoint accessed through the \nAPI. The model was frozen; no gradient updates were applied. Prompts were assembled by \na controller written in Python 3.11 and executed inside Docker containers (Ubuntu 22.04, \nIntel Xeon Silver 4314). For example, adaptability is demonstrated when IPEM dynami -\ncally selects analogical reasoning prompts for cross-domain tasks, as seen when transferring \nlegal reasoning frameworks to financial contract analysis. Similarly, adaptability in handling \ninput variability is evidenced by Memory-of-Thought’s robust performance in multi-turn \narithmetic tasks (GSM8K), where intermediate reasoning steps vary significantly across \nturns. A complete list of hyper-parameters and system resources is compiled in Appendix I.\nTable 2 Thematic analysis\nFocus area Number \nof papers \nreviewed\nInsights gained Contribution to IPEM\nMemory and \nself-improvement\n15 Memory retention in LLMs \nsupports task consistency\nLed to MoT in Self-Improvement\nChain-of-thought \ntechniques\n20 CoT improves multi-step \nreasoning\nInspired Enhanced CoT Tech-\nniques (Auto-CoT, Contrast-CoT)\nEthical and bias \nmitigation\n25 Bias benchmarks reduce \nstereotype risk in responses\nIntegrated into Evaluation and \nFeedback Loops\nStructured reasoning \nmethods\n10 Tabular reasoning improves \naccuracy in data tasks\nLed to Tab-CoT in Structured and \nAnalogical Reasoning\nAnalogical and role-\nbased reasoning\n15 Analogical reasoning aids \nadaptability in open domains\nImplemented as Role-Based \nAnalogies and Analogical \nPrompting\n1 3\n348 Page 12 of 51\nInclusive prompt engineering for large language models: a modular…\n3.3.1 Sampling settings\nTemperature 0.2, nucleus p 0.95, max tokens 512, frequency penalty 0, presence penalty 0. \nRejection-resampling was disabled so that stochasticity arose solely from nucleus sampling. \nFive seeds (42, 52, 62, 72, and 82) were run for every configuration; means and standard \ndeviations are reported.\n3.3.2 Memory-of-thought store\nReasoning steps are appended to a SQLite database keyed by the SHA-256 hash of the \nconcatenated task ID and the first 128-dimensional sentence-BERT embedding (all-\nMiniLM-L6-v2). Retrieval uses cosine distance with a bias-aware gate. The gate predicts \nthe StereoSet stereotype score of a candidate trace using a ridge-regression model trained on \n12 k examples; traces predicted to raise the running bias metric are blocked.\n3.3.3 Structured & analogical selector\nUncertainty is estimated by the log-variance of next-token probabilities. If variance > 0.8, \nthe selector triggers Analogical Mode and retrieves the closest prior task by TF-IDF cosine \nover problem statements. For two-dimensional inputs identified by a header row and at least \ntwo numeric columns, Tabular-CoT is forced.\n3.3.4 Enhanced CoT\nAutomatic CoT generates three reasoning traces, then Contrastive CoT ranks them by \nmajority voting over final answers. If no majority exists, the trace with the highest self-\nconsistency score (Wang et al., 2022) is chosen.\n3.3.5 Evaluation & feedback loop\nAfter each turn, the controller records accuracy (task dependent) and two bias metrics (Ste-\nreoSet and BIG-bench fairness subset). If the rolling bias mean over the last five outputs \nrises above 30 on StereoSet’s scale, the gate threshold τ  is tightened by 0.05; if accuracy \nfalls below a preset task floor, the selector switches to Complexity-based prompting.\n3.4 Datasets and licenses\nThe datasets used in this research are governed by a mix of open and restricted licenses. \nGSM8K is openly available under the MIT License (GSM Dataset, 2020), while MathQA \n(Amini et al., 2019) is limited to non-commercial research. The S&P 500 financials dataset \n(Kaggle, 2020) is licensed under CC-BY-4.0, allowing reuse with attribution, and the Cam-\nbridge Law Corpus (Östling et al., 2024) is under CC-BY-NC, restricting use to non-com -\nmercial purposes. A credentialed subset of MIMIC-IV was used for clinical data, ensuring \ncompliance with data protection standards. StereoSet (Nadeem et al., 2021) and BIG-bench \n(Ye et al., 2023) follow the licensing terms specified by their creators.\n1 3\nPage 13 of 51 348\nM. S. Torkestani et al.\n3.5 Experimental protocol\nEach dataset was split 70/15/15 into train, validation, and held-out test where licenses per -\nmit. For public leaderboards with fixed splits, we kept the official split. Every IPEM variant \nand baseline (GPT-4 zero-shot, Few-shot-CoT, Memory-only, Bias-filter-only) was run in \nfive independent seeds. Statistical comparisons used paired t − tests for accuracy and Wil-\ncoxon signed-rank tests for bias scores; p-values were Holm-corrected across tasks. Confi -\ndence intervals are 95% normal-based unless the Shapiro–Wilk test rejected normality; in \nthat case a bootstrap (10 k resamples) was used. Detailed metric definitions, seed values, \nand representative prompts are provided in Appendix B.\n3.6 Ablation design\nThe ablation matrix logs eight variants obtained by disabling one module at a time and \ntwo-way combinations. Summary matrix aggregates mean accuracy and bias over all tasks. \nStandard errors confirm that improvements are stable across seeds; for instance, disabling \nthe bias gate drops average StereoSet performance from 25 ± 0.9 to 30.7 ± 1.3 while leav-\ning accuracy unchanged, supporting the gate’s isolated effect.\nThe detailed testing of each component validates the efficacy of IPEM’s four pillars, \ndemonstrating their distinct contributions to model adaptability, accuracy, ethical align -\nment, and responsiveness. Through this component synthesis and pilot testing phase, each \naspect of IPEM was refined to optimize task performance and inclusivity, addressing sig -\nnificant challenges in traditional LLMs and advancing prompt engineering toward a more \nholistic, adaptable framework.\nFigure 5 presents a high-level modular architecture diagram of the IPEM framework, \ndemonstrating the flow of data through its core components. This modular visualization \nhighlights how data transitions between the Memory-of-Thought (MoT), Tabular Chain-\nof-Thought (Tab-CoT), Automate Chain-of-Thought (Auto-CoT), Contrastive Chain-of-\nThought (Contrast-CoT), and Evaluation and Feedback Loops modules. Each component \nprocesses data and contributes to an overall structured output, reinforcing ethical alignment, \ncontextual adaptability, and task-specific accuracy.\nTo clarify IPEM’s novelty, Table 3 offers a direct comparison with representative \nprompting frameworks across five core dimensions: memory augmentation, ethical safe -\nguards, structured reasoning, analogical generalization, and modular integration. While \nmany frameworks support one or two of these features in isolation, IPEM is the system that \nintegrates all five within a single, interoperable schema. This modular synthesis enables \nInput Prompt Memory Retrieval (MoT) Structured Reasoning \n(Tab-CoT)\nAutonomous Reasoning \n(Auto-CoT & Contrast-CoT\nEthical Evalua/g415on \n(StereoSet, Feedback Loops)\nFig. 5 High-level modular architecture diagram of the IPEM framework\n \n1 3\n348 Page 14 of 51\nInclusive prompt engineering for large language models: a modular…\nmutual reinforcement (e.g., memory modules are conditioned by ethical filters, and ana -\nlogical transfer supports low-resource adaptation) which sets IPEM apart from prior single-\nstrategy methods.\n4 Results and analysis\nThe Inclusive Prompt Engineering Model (IPEM) was evaluated across diverse domains, \nincluding education, business, healthcare, and open-domain question answering, to assess its \nadaptability, reasoning capabilities, and ethical alignment. This section presents a detailed \nanalysis of IPEM’s performance, supported by statistical significance testing and ablation \nstudies that validate the reported outcomes. Each of IPEM’s four core components, Mem -\nory-of-Thought (MoT), Tabular and Analogical Reasoning, Enhanced Chain-of-Thought \nTechniques, and Evaluation and Feedback Mechanisms, was tested both individually and \nas part of the integrated system, to demonstrate their distinct contributions and synergistic \neffects.\n4.1 Memory-of-thought results\nAcross the GSM8K arithmetic benchmark the baseline solved 78.1% ± 0.4%. Activat -\ning MoT with the new context-filtered retrieval layer (Algorithm 1) raised accuracy \nto 86.3% ± 0.32%( ∆+8.2pp, t(4) = 28.7,p <0.001). Error rate dropped by 19.9% \nrelative and average turn length shrank from 3.8 to 2.9 steps, confirming less redun -\ndant reasoning. In the S&P 500 multi-period forecasting task the baseline posted \n70.1% ± 0.48%. MoT reached 81.2% ± 0.57%(∆ + 11.1pp, t(4) = 26.4,p < 0.001). \nTable 4 lists the improvement together with the continuity index, which rose from 0.62 to \n0.79 (W ilcoxonW= 15,p =0 .031). confidence intervals (CI 95) are calculated as:\n \nCI 95 = t0.025,4 × SD√\n5 ≈ 2.776 × SD√\n5\nTable 3 Comparative overview of prompt engineering frameworks\nFramework Memo-\nry use\nEthical \nintegration\nStructured \nreasoning\nAnalogical \ntransfer\nModular \ninteroperation\nPrimary \nlimitation\nAuto-CoT (Zhou et \nal., 2022)\nNo No Yes (via \nCoT)\nNo No Single strat-\negy, no ethics\nMemory-aug-\nmented prompt-\ning (Wang et al., \n2023a, 2023b)\nYes No No No No Focused only \non memory\nReAct (Yao et al., \n2022)\nNo No Partial \n(tool \nreasoning)\nNo No Not \nethical-aware\nBias Bench \n(Nadeem et al., \n2021)\nNo Yes No No No Ethics-only, \nno reasoning\nIPEM (this paper) Yes \n(MoT)\nYes (Per-\nsona + Bias \nGate)\nYes \n(Tab-CoT)\nYes (Role \nAnalogies)\nYes \n(schema-linked)\nHigher \nprompt \ncomplexity\n1 3\nPage 15 of 51 348\nM. S. Torkestani et al.\nExtended ablation tables and error-type breakdowns appear in Appendix C. The ablation in \nFig. 6 compares four variants. Removing the context filter (MoT-nf) preserved recall but \nre-introduced irrelevant fragments, cutting the gain by more than half. Disabling memory \nentirely (No-MoT) reproduced the baseline. Adding the bias gate alone without memory \n(Bias-only) slightly reduced stereotypical completions but did not affect task accuracy, dem-\nonstrating that the observed accuracy lift is chiefly due to memory persistence rather than \nbias gating.\n4.2 Tabular chain-of-thought and analogical reasoning results\nThis component's effectiveness was assessed using datasets for financial projections (struc-\ntured data) and legal case analysis (open-domain tasks). Table 5 summarizes IPEM’s per-\nformance in structured and analogical reasoning benchmarks. Tabular CoT (Tab-CoT) was \nevaluated on macroeconomic panel data (World Bank, 2024), while analogical prompting \nTable 4 Component-level impact of memory-of-thought (MoT)\nConfiguration Dataset Task type Accuracy%(±SD) CI95 Error-\nrate ∆%\nContinuity \ngain\nBaseline (no MoT) GSM8K Multi-turn arithmetic 78.1 ± 0.40 ±0.50 – –\nIPEM + MoT GSM8K Multi-turn arithmetic 86.3 ± 0.32 ±0.40 − 19.9 +0.17\nBaseline (no MoT) S&P-500 \nfinancials\nMulti-step forecast 70.1 ± 0.48 ±0.60 – –\nIPEM + MoT S&P-500 \nfinancials\nMulti-step forecast 81.2 ± 0.57 ±0.71 − 18.7 +0.17\nFig.  6 MoT ablation performance by epoch (Accuracy plotted against epoch count; confidence \nribbons ± 0.4)\n \n1 3\n348 Page 16 of 51\nInclusive prompt engineering for large language models: a modular…\nwas tested on cross-domain legal-to-financial document adaptation. On the World Bank \nmacro-economic panel, the baseline achieved 73.4% ± 0.8%. Tab-CoT improved this to \n85.9% ± 0.5%(∆ + 12.5pp, t(4) = 19.6,p <0.001). Mean absolute percentage error fell \nfrom 6.2% to 4.1%. Using Cambridge Law Corpus cases as source and company annual reports \nas target, analogical prompting raised F1 from 69.0% ± 0.7%79.8% ± 0.6%(∆ + 10.8pp). \nReverse transfer (finance → law) delivered a smaller but significant gain of 8.9pp. Cross-\ndomain accuracy differences remain within the 95%CI  reported in Table 5, supporting the \nstability of the transfer mechanism. The Tab-CoT prompt format and additional worked \nexamples are listed in Appendix E. Qualitative examples, gains, and failure modes for ana-\nlogical prompting are provided in Appendix H.\n4.3 Enhanced chain-of-thought techniques results\nThis component includes Automate Chain-of-Thought and Contrastive Chain-of-Thought, \naiming to improve logical coherence and reduce errors in multi-step reasoning tasks. Table 6 \nevaluates enhanced CoT techniques on MathQA and a legal inference task. Auto-CoT raised \nbaseline accuracy from 61.7% to 76.5%. Adding contrastive reasoning further improved accu-\nracy to 79.3% ± 0.4, with a 27% drop-in logical error rates (McNemarχ2 = 42.1, P < 0.001). \nLegal sequential reasoning benefited similarly: the baseline accuracy of 65.2% ± 0.6% rose \nto 77.6% ± 0.4% with Contrast-CoT. The coherence score, computed with the LLM-based \nrubric of Nguyen et al. (2024), improved from 0.61 to 0.79(∆ + 0.18,t(4) = 15.3).\n4.4 Bias and inclusivity analysis\nTo evaluate IPEM’s performance in reducing demographic biases, we used the StereoSet \nand BIG-bench bias benchmarks, focusing on domains where inclusivity is critical, such \nas healthcare diagnostics and educational support. These assessments tested IPEM against \nstandard bias metrics, measuring stereotype reinforcement across gender, race, and socio -\neconomic demographics.\nTable 5 Structured and analogical reasoning performance\nSetting Source → T arget Metric Baseline IPEM variant ∆\nTab-CoT (macro panel) – Acc% 73.4 ± 0.8 85.9 ± 0.5 +12.5pp\nMAPE % 6.2 4.1 − 2.1\nAnalogical transfer Law → F inance F1% 69.0 ± 0.7 79.8 ± 0.6 +10.8pp\nF inance→ Law Acc% 71.3 ± 0.6 80.2 ± 0.5 +8.9pp\nTable 6 Enhanced chain-of-thought results\nDataset V ariant Accuracy%(±SD) Logical − error∆% Coherence(0 − 1)\nMathQA Baseline (plain) 61.7 ± 0.9 − 0.54\nAuto-CoT 76.5 ± 0.5 − 22.4 0.69\nAuto + Contrast-CoT 79.3 ± 0.4 − 27.0 0.73\nLegal \nsequential\nBaseline 65.2 ± 0.6 − 0.61\nContrast-CoT 77.6 ± 0.4  − 24.5 0.79\n1 3\nPage 17 of 51 348\nM. S. Torkestani et al.\n4.4.1 StereoSet results\nTable 7 illustrates IPEM’s performance in stereotype, association, and over -\nall bias reduction compared to a baseline model. With selective annota -\ntion and role-based analogy integration, gender stereotype score fell from \n32.4 ± 0.925.6 ± 0.6 (∆−20.9% ), race score from 28.3 ± 1.022.0 ± 0.7 (∆−22.2% ), \nand socioeconomic score from 34.2 ± 0.729.0 ± 0.6 (∆−15.2% ). All reductions are sig -\nnificant (p < 0.01). Variance is reported in Table 7.\nThese results underscore the effectiveness of role-based analogies and selective annota -\ntions, especially in gender and race demographics, where contextual prompts often presented \nhigher risks of stereotype reinforcement. Complete prompt inventories and the four-factor \nannotation rubric are available in Appendix G.\n4.4.2 BIG-bench results\nTable 8 provides IPEM's scores on the BIG-bench bias metrics, which included demo -\ngraphic diversity and inclusivity tests across various contexts, such as healthcare diagnos -\ntics and educational scenarios.\nAccuracy on Healthcare Diagnostics climbed to 85.1% ± 0.5% versus 71.9% ± 0.6% \nbaseline. Educational Support rose to 81.7% ± 0.7%(baseline 67.8% ± 0.7%). Improve -\nments remain significant after Benjamini–Hochberg correction for multiple tests. IPEM \nconsistently outperforms the baseline by 13–14 pp across all contexts. These gains result \nfrom persona filters, role-based analogies, and uncertainty-aware prompt selection. Appen-\ndix F details the benchmarking protocol and provides distribution plots for all bias metrics.\n5 Discussion\nThe results presented in Sect. 4 demonstrate that the Inclusive Prompt Engineering Model \n(IPEM) achieves improvements across a variety of domains, evaluation types, and perfor -\nmance metrics. This section interprets these findings, addressing the theoretical implica -\ntions, practical benefits, and potential limitations of the model. We organize the discussion \nTable 7 StereoSet bias scores\nDimension Baseline IPEM ∆% SD (IPEM) CI95\nGender 32.4 25.6 − 20.9 0.6 ±0.75\nRace 28.3 22.0 − 22.2 0.7 ±0.88\nSocio-economic 34.2 29.0 − 15.2 0.6 ±0.75\nOverall 31.6 25.5 − 19.3 0.6 ±0.75\nTable 8 IPEM's performance on the BIG-bench bias metrics. Higher accuracy reflects improved handling of \ninclusive contexts\nContext Metric Baseline IPEM SD CI95 ∆pp\nHealthcare diagnostics Acc% 71.9 85.1 0.5 ±0.62 +13.2\nEducational support Acc% 67.8 81.7 0.7 ±0.87 +13.9\nCustomer service Acc% 75.0 88.2 0.6 ±0.75 +13.2\n1 3\n348 Page 18 of 51\nInclusive prompt engineering for large language models: a modular…\naround four key axes: (1) performance scalability, (2) ethical and social alignment, (3) \nprompt engineering generalizability, and (4) limitations and directions for future research.\n5.1 Scalability and performance across tasks\nIPEM enhances model performance not by changing LLM weights or architectures, but by \ndynamically composing prompts through modular reasoning strategies. This yields improve-\nments across multiple task types: a + 12.5 percentage point (pp) gain in structured reason -\ning accuracy (Table 5), + 10.8 pp in cross-domain analogical transfer ( Law → F inance), \nand over + 17 pp in memory-augmented settings on multi-step datasets such as GSM8K \nand MathQA (Appendix C). These gains demonstrate IPEM’s scalability. Unlike isolated \nprompting methods, which often fail to generalize beyond narrow domains, IPEM’s archi -\ntecture allows techniques such as Memory-of-Thought (MoT), Tabular CoT, analogical \nretrieval, and contrastive reasoning to reinforce each other. For instance, Table 6 shows \nthat augmenting Auto-CoT with contrastive reasoning yields a + 2.8 pp accuracy increase \non MathQA, alongside a 4.6% drop-in logical error rate. Similarly, Fig. 7 illustrates how \nmemory integration via MoT accelerates convergence during adaptive inference. These \nresults validate the hypothesis that synergistic prompting leads to more robust and transfer-\nable model behavior.\nAdaptability was quantitatively measured by improvements in cross-domain accuracy, \nreduced logical error rates across variable inputs, and enhanced coherence and consis -\ntency metrics across iterative prompt generations. Additionally, IPEM maintains perfor -\nmance across open-domain tasks (e.g., GSM8K), semi-structured problems (e.g., financial \nforecasting), and highly contextual applications (e.g., legal reasoning), suggesting that the \ndesign principles hold under varied data modalities and reasoning structures. The results \nalso indicate that IPEM's gains are stable across runs, with low standard deviations and nar-\nrow confidence intervals.\nFig. 7 Convergence in adaptive inference with vs. without MoT\n \n1 3\nPage 19 of 51 348\nM. S. Torkestani et al.\n5.2 Ethical alignment and bias mitigation\nIPEM is not only a performance-enhancing framework but also a vehicle for ethical safe -\nguards. In domains included within our scope, such as healthcare and financial forecasting, \nIPEM explicitly integrates ethical safeguards through persona-based filtering, uncertainty-\ndriven prompt selection, and continuous bias monitoring and reduction via evaluation \nloops. These mechanisms ensure that domain-specific ethical and contextual considerations \nare actively incorporated into prompt selection and reasoning paths, supporting compli -\nance and fairness The 19.3% reduction in overall StereoSet bias (Table 7) and the consis -\ntent + 13–14 pp accuracy improvement on inclusivity tasks (Table 8) offer strong empirical \nevidence that bias mitigation is not in conflict with performance, it can be a co-optimized \ngoal. This is achieved through a combination of mechanisms designed to enhance fairness \nand mitigate bias. Selective persona-based filtering conditions model outputs on iden -\ntity- and context-sensitive roles, reducing the risk of stereotype replication, particularly \nin sensitive domains like healthcare and education. Contrastive Chain-of-Thought (CoT) \nprompting encourages the model to reject flawed reasoning paths, especially those linked \nto biased assumptions. Additionally, uncertainty-driven sampling prioritizes diverse exem-\nplars during prompt construction, aligning with best practices in inclusive data annotation, \nas highlighted by Talukdar and Biswas ( 2023). Unlike systems that add fairness modules \nas post-processing layers, IPEM integrates ethical evaluation at the prompt selection stage, \nthus shaping not just the final output but the reasoning process itself. This approach aligns \nwith recent calls in responsible AI to shift fairness upstream in the model pipeline. Further-\nmore, the reduction in harmful associations in adversarial StereoSet examples (especially \nrace and gender subtests) suggests that IPEM is robust under stress conditions, not just \nunder ideal benchmarking scenarios.\nWhile IPEM’s ethical safeguards are grounded in empirical practices (e.g., demographic \nfilters, inclusive prompt sampling), the framework is also informed by foundational ethical \ntheories. The persona-based filtering approach aligns with value-sensitive design (Fried -\nman et al., 2013) by embedding contextually appropriate norms into prompt structure from \nthe outset. The use of contrastive CoT to reject stereotype-reinforcing outputs reflects a \ndeontological stance, aiming to preserve ethical constraints regardless of statistical utility. \nSimultaneously, our multi-metric evaluation (accuracy, bias reduction, inclusivity) supports \na utilitarian interpretation, emphasizing fairness across aggregate outcomes (Binns, 2018). \nThese theoretical anchors enrich IPEM’s claim to ethical robustness beyond benchmark \nperformance.\n5.3 Annotation efficiency and structured generalization1\nA critical advantage of IPEM lies in its support for low-resource adaptation. To enable \neffective low-resource adaptation, we employed a selective annotation mechanism based \non a hybrid of uncertainty sampling and complexity ranking. Specifically, for each domain, \ncandidate examples were ranked using a confidence proxy derived from entropy over the \nmodel’s top-k token predictions (k = 5), and a normalized measure of syntactic and semantic \n1 In the context of LLM prompt engineering, we use “generalization” to refer to task-domain transfer without \nretraining, rather than traditional statistical bounds. This aligns with prior work on analogical prompting and \nin-context learning transfer (Yasunaga et al., 2023).\n1 3\n348 Page 20 of 51\nInclusive prompt engineering for large language models: a modular…\ncomplexity (e.g., clause count and vocabulary rarity). Samples with high uncertainty and \nmid-to-high complexity were prioritized to ensure representativeness and instructional util-\nity. This hybrid method enables early annotation rounds to deliver outsized performance \ngains with fewer examples, consistent with prior work in active learning (e.g., Settles, \n2012). As illustrated in Fig. 8, IPEM outperforms the baseline by 12.4 percentage points \nat just 20 annotated examples per domain, a setting in which many prompt-based systems \nfail due to insufficient coverage. The returns diminish at higher quotas, stabilizing after \n60 examples, which suggests that the active and complexity-based exemplar selection is \nidentifying high-impact samples early. Implementation details and an end-to-end example \nworkflow are documented in Appendix D.\nThis efficiency has direct implications for deployment in domains where labelled data is \nscarce or expensive, such as healthcare specialties, regulatory policy, and regional legal sys-\ntems. In such contexts, IPEM reduces dependence on extensive fine-tuning or manual cura-\ntion, offering a scalable path to task adaptation. Moreover, IPEM’s analogical prompting \nmechanism further supports structured generalization by enabling cross-domain adaptation. \nTo evaluate this, we conducted experiments in which domain-specific prompts developed \nfor legal clause classification were adapted for financial contract segmentation, and vice \nversa. Without domain-specific re-tuning, IPEM maintained ≥ 89% of its peak accuracy in \nboth directions, a strong indicator of transferability.\nThese results demonstrate that IPEM’s prompt modules generalize across domains with \nminimal degradation, even in settings with distributional shift. In such contexts, IPEM \nreduces dependence on extensive fine-tuning or manual curation, offering an efficient path \nto task adaptation. The analogical prompting mechanism further supports generalizability \nby drawing structural parallels across domains. As shown in Table  5, legal reasoning pat -\nterns can be adapted to financial clause classification with minimal accuracy loss. This rein-\nforces the view that analogical structures are not only cognitively grounded (V osniadou & \nOrtony, 1989) but computationally effective.\nFig. 8 Selective-annotation efficiency\n \n1 3\nPage 21 of 51 348\nM. S. Torkestani et al.\n5.4 Limitations and failure modes\nDespite its strengths, IPEM exhibits limitations that merit discussion. First, as reported in \nSect. 4.1, Memory-of-Thought (MoT) mechanisms sometimes retrieve stale or contextu -\nally adjacent but irrelevant reasoning chains, particularly in long interactions or documents \nexceeding 3,000 tokens. This often occurs due to topic drift or lexical similarity mismatches, \nwhere surface-level token overlap outweighs deeper semantic relevance. Although cosine-\nbased relevance filtering mitigates this to a degree, it does not fully prevent erroneous acti-\nvations. These observations suggest the need for more robust memory-decay mechanisms \nor embedding-based clustering to distinguish semantically coherent discourse segments. \nSecond, analogical prompting occasionally overgeneralizes, especially in transfer tasks \nwhere the source and target domains share surface terminology but differ in relational struc-\nture or cultural assumptions. For instance, legal-to-financial transfers sometimes produce \ncompletions that misattribute regulatory logic or stakeholder roles, resulting in flawed rea -\nsoning. These issues typically stem from insufficient scoping or lack of alignment validation \nbetween analogical roles and domain-specific semantics. Third, while IPEM consistently \nreduces bias on aggregate metrics, it does not guarantee fairness at the instance level. Out -\nputs can still reflect latent stereotypes if exemplar prompts contain subtle framing biases or \nif retrieval prioritizes fluency over representativeness. This underscores the importance of \nintegrating audit trails and potentially supervised reranking modules during deployment. \nFourth, the modular design of IPEM, though beneficial for extensibility, introduces opera -\ntional complexity. Components such as prompt chaining, memory curation routines, and \nanalogical role-mapping thresholds require calibration. We provide default configurations \nin our codebase, but effective use in specialized settings (e.g., medical, legal) will benefit \nfrom domain-informed customization. Finally, while IPEM significantly enhances ethi -\ncal responsiveness in supported domains, it does not currently encompass all high-stakes \ncontexts, particularly those demanding stringent ethical oversight, such as criminal justice \nor highly sensitive clinical interventions. These domains require specialized compliance \nframeworks, context-specific ethical audits, and additional regulatory mechanisms beyond \nthe existing IPEM architecture. Future iterations could explore extending IPEM’s modular \nstructure with domain-specific ethical modules and governance mechanisms, thus responsi-\nbly expanding its applicability.\n5.5 Implications for theory and practice\nThe results underscore two key contributions to the field of prompt engineering. First, IPEM \ndemonstrates that prompting extends beyond simple formatting; prompts can be systemati-\ncally engineered as modular reasoning architectures, positioning prompt engineering as a \nform of systems design rather than merely preamble crafting or token scaffolding. Second, \nthe framework shows that fairness and functionality can be co-optimized. By embedding \nbias mitigation directly into the prompting process, rather than treating it as a post hoc addi-\ntion, IPEM establishes that responsible AI design can be structurally integrated from the \noutset. These insights suggest new research directions for prompt-based systems, including \ndynamic memory routing, zero-shot ethical filtering, and reasoning-chain pruning strategies \nthat could further refine response quality and interpretability.\n1 3\n348 Page 22 of 51\nInclusive prompt engineering for large language models: a modular…\nDespite these strengths, we acknowledge that the current implementation of IPEM has \nbeen validated primarily in benchmark-aligned simulations of domains such as law, health-\ncare, and finance. While these simulations were designed to reflect realistic task formats, \nIPEM has not yet been deployed in fully interactive systems such as live clinical support \ntools or policy advisory chatbots. Real-time deployment would introduce additional chal -\nlenges, including latency tolerance, dynamic user feedback loops, and error recovery mech-\nanisms, which are outside the scope of this study but essential for real-world adoption. We \nconsider this an important direction for future work.\n5.6 Comparison with GPT-4 and RLHF paradigms\nWhile IPEM does not modify base model weights, it is complementary to advanced architec-\ntures such as GPT-4 and those trained with Reinforcement Learning from Human Feedback \n(RLHF). RLHF enhances alignment by leveraging supervised fine-tuning and human pref -\nerence modelling (Ouyang et al., 2022), yet it requires substantial computational resources \nand high-quality domain-specific training data. Moreover, the resulting models can become \nrigid and opaque, with limited transparency into how alignment decisions are encoded. In \ncontrast, IPEM offers a modular, prompt-based interface that remains interpretable, adapt -\nable, and model-agnostic. For example, IPEM can be deployed atop commercially hosted \nLLMs (including GPT-4) where weight-level tuning is not possible. Additionally, it allows \nend-users and organizations to extend or refine reasoning and fairness capabilities with -\nout requiring model retraining. This flexibility makes IPEM suitable for regulated or high-\nstakes environments (such as healthcare or law) where auditability and customization are \nessential. By integrating mechanisms such as Memory-of-Thought, analogical prompting, \nand bias-aware filtering, IPEM acts as a lightweight, complementary layer to RLHF-driven \nmodels. This distinction helps clarify IPEM’s novel role: not as a replacement for fine-tuned \nLLMs, but as an interface that makes them more adaptable, inclusive, and aligned to down-\nstream requirements.\n6 Conclusion\nThis paper presented the Inclusive Prompt Engineering Model (IPEM), a modular prompt -\ning framework designed to enhance both the performance and ethical robustness of large \nlanguage models (LLMs) across a variety of real-world tasks. In contrast to prior prompt \nengineering approaches that optimize isolated facets (such as reasoning depth or fairness \nbenchmarks), IPEM unifies multiple strategies into a cohesive and extensible architecture.\nAt its core, IPEM integrates four major subsystems that work together to enhance reason-\ning, adaptability, and fairness in language model outputs. The first is Memory-of-Thought \n(MoT), which maintains multi-turn consistency by selectively retaining and retrieving \nreasoning chains. The second includes Enhanced Chain-of-Thought mechanisms, such as \nAutomated and Contrastive CoT, which support multi-step problem solving while verifying \nlogical paths. The third consists of Structured and Analogical Reasoning tools like Tabu -\nlar CoT and role-based analogical transfer, designed to manage structured data effectively \nand support cross-domain reasoning. Finally, Evaluation and Feedback Loops incorporate \n1 3\nPage 23 of 51 348\nM. S. Torkestani et al.\npersona-aware filtering, uncertainty-driven prompt selection, and inclusive exemplars to \nensure that outputs are fair, interpretable, and context-aware.\nThrough controlled experiments across diverse tasks including arithmetic reasoning \n(GSM8K), semi-structured financial analysis, legal case evaluation, healthcare triage, and \nopen-domain question answering, IPEM demonstrated notable performance improvements. \nIt achieved accuracy gains of 10 to 20 percentage points over a strong GPT-4 baseline, \nwith a 12.5-point improvement in tabular forecasting and an 18-point increase in memory-\naugmented multi-turn tasks. Error rates decreased by 24 to 27 percent in logical fallacy \ndetection using Contrast-CoT and by 15 to 18 percent in numerical tasks when the Memory-\nof-Thought (MoT) module was active. Bias was reduced by 19.3 percent on StereoSet, and \ninclusive accuracy improved by 13 to 14 percentage points on BIG-bench demographic \ntasks without compromising main-task performance. Furthermore, annotation effort was \nreduced by approximately one-third through active sampling and complexity-based exem -\nplar ranking, while maintaining at least 90 percent of peak performance with only seventy \nlabelled examples.\nThe empirical evidence supports three core claims. First, prompt composition can substi-\ntute for additional model fine-tuning when memory, reasoning, and evaluation modules are \nallowed to interoperate. Second, ethical safeguards need not come at the cost of task accu -\nracy when they are embedded upstream in the reasoning chain rather than imposed post-hoc. \nThird, modularity matters: ablation studies confirmed that removing any single pillar (MoT, \nTab-CoT, analogical retrieval, or contrastive verification) reduced composite scores by at \nleast 7 pp, showing that each module supplies non-redundant utility. Together these findings \nadvance the state of practice for organizations that rely on commercially hosted LLMs but \ncannot alter underlying weights. IPEM provides a repeatable recipe that lifts accuracy, curbs \nharmful bias, and lowers data-engineering overhead, all through prompt-side logic that can \nbe audited, version-controlled, and improved incrementally.\n6.1 Limitations\nDespite its promise, the IPEM framework faces four key limitations. First, memory drift \noccurs in long contexts: when input lengths exceed approximately 3,000 tokens, the cosine-\nsimilarity retrieval mechanism in the Memory of Thought (MoT) module begins surfacing \nonly partially relevant reasoning chains, leading to a 4 percentage point increase in contra -\ndiction errors. Second, analogical reasoning can break down in domain-divergent settings; \nin specialized fields like maritime insurance, surface-level similarities may obscure deeper \nstructural differences, resulting in false analogies. Third, while IPEM reduces global bias \nmetrics, it does not guarantee per-instance fairness, individual responses may still reflect \nundesirable stereotypes if the exemplar pool is skewed. Finally, the framework introduces \nnotable operational complexity, requiring around 180 lines of prompt orchestration logic \nin the reference implementation, which may pose challenges for teams unfamiliar with \nstructured prompting techniques. A further limitation of the current study is the absence \nof integration into real-world interactive environments. While IPEM was tested across \n1 3\n348 Page 24 of 51\nInclusive prompt engineering for large language models: a modular…\ndomain-specific scenarios in finance, law, and healthcare, these tests were conducted using \nstatic evaluation datasets rather than through embedded applications (e.g., adaptive agents \nor decision-support chatbots). We intend to explore such implementations in follow-up \nwork by developing a lightweight interaction pipeline that supports real-time adaptation, \nmemory persistence across turns, and feedback-informed prompt adjustments within a live \ninterface. Such systems would allow us to assess long-term usability, domain alignment, \nand ethical safeguards in continuous deployment contexts.\n7 Future work\nWhile IPEM demonstrates clear advantages, its architecture also raises new questions that \nfuture research should investigate. We outline several key directions below:\n7.1 Memory precision and retrieval robustness\nAlthough MoT improved task continuity and reduced redundant computations, error analy-\nsis (Sect. 4.6) showed occasional mis retrievals, particularly on long-context or noisy inputs. \nTo mitigate this, future work should explore adaptive memory gating mechanisms, hybrid \ncontext filters that combine vector-based retrieval with query reformulation, and decay-\nbased indexing that accounts for temporal or ethical relevance. Testing IPEM under input \nperturbation and multilingual scenarios will also help assess memory stability and retrieval \nresilience across deployment settings.\n7.2 Analogy alignment and semantic calibration\nThe analogical prompting module performed well in cross-domain transfer (e.g., law to \nfinance), but underperformed when source and target domains contained asymmetric vocab-\nularies or implicit cultural assumptions. This motivates the introduction of structural align-\nment tools such as schema mapping, domain ontologies, or similarity-aware re-ranking. We \nalso propose incorporating cultural dissonance detectors to flag analogies that may be con -\ntextually inappropriate or ethically ambiguous in multilingual or cross-cultural applications.\n7.3 Dynamic ethical modulation and feedback loops\nCurrently, IPEM encodes ethical safeguards via static personas and fixed exemplars. A more \nresponsive approach would introduce real-time fairness audits, counterfactual self-correc -\ntion routines, and user-adjustable filters. These can be implemented through semi-automated \nfeedback loops informed by uncertainty, linguistic entropy, or user role. Coupling this with \nreinforcement-aligned prompting (e.g., reward signals tied to inclusivity or coherence) may \nyield stronger bias mitigation without added supervision costs.\n1 3\nPage 25 of 51 348\nM. S. Torkestani et al.\n7.4 Tool-integrated prompt orchestration\nIPEM currently operates independently of external APIs or toolkits. However, real-world \ndeployments frequently combine LLMs with retrieval-augmented generation (RAG), sym -\nbolic planners, or interactive tools. Future work will embed IPEM modules within tool-\naware ecosystems, such as integrating MoT with vector DBs (e.g., FAISS), analogy modules \ninto planning agents, or verification prompts before invoking external calculators. This will \nhelp assess whether prompt-side reasoning can complement, or even replace, tool-side heu-\nristics in hybrid AI systems.\n7.5 Structured generalization benchmarks\nThe claim of cross-domain generalization is central to IPEM’s design but requires formal \nvalidation. We plan to release benchmark suits that test structured transfer scenarios, such \nas training on legal tasks and testing on finance or policy. These results empirically support \nthe framework’s ability to generalize effectively, underscoring IPEM’s utility in real-world \ncontexts like multi-domain assistance and low-resource settings, where generalization capa-\nbility is vital to operational efficiency and deployment feasibility.\n7.6 Broader impact and governance\nDeploying IPEM at scale could lower barriers for organizations constrained by computing \nor regulatory frameworks, such as regional hospitals, municipal services, or educational \ninstitutions. By embedding reasoning, memory, and ethical control directly into prompts \nrather than model weights, IPEM supports vendor-agnostic adaptation, fast iteration, and \ntransparent rollback. Future research should explore how explicit prompt modularity can \nassist with AI governance, compliance audits, and citizen accountability in high-stakes \ndecision-making contexts.\nAppendix A: Summary of the reviewed studies\nSee Table 9.\n1 3\n348 Page 26 of 51\nInclusive prompt engineering for large language models: a modular…\n# Title Authors Summary\n1 MoT: Memory-of-Thought Enables Chat-\nGPT to Self-Improve\nLi & Qiu \n(2023)\nExamines MoT frameworks for task \nconsistency and reduced redundancy \nwithout retraining\n2 Augmenting Language Models with Long-\nTerm Memory\nWang et al. \n(2023b)\nExplores memory-enhanced reasoning \nfor reduced redundancy in iterative \ntasks\n3 Complexity-Based Prompting for Multi-step \nReasoning\nFu et al. \n(2023b)\nStudy the task of prompting large-\nscale language models to perform \nmulti-step reasoning\n4 Memory Augmented Neural Networks for \nNatural Language Processing\nGulcehre \nand Chandar \n(2017)\nUses memory networks to improve \nperformance across complex, contex-\ntually heavy tasks\n5 Self-Consistency Improves Chain of \nThought Reasoning in Language Models\nWang et al. \n(2022)\nPresents self-prompted recall tech-\nniques for improved task consistency \nacross related tasks\n6 Language Models as Knowledge Bases? Petroni et al. \n(2019)\nExplores LLMs as knowledge reposi-\ntories with unstructured memory recall\n7 Retrieval-based Video Language Model for \nEfficient Long Video Question Answering\nXu et al. \n(2023)\nInvestigates retrieval-based memory \nmethods for efficient task handling\n8 Recognition, recall, and retention of few-\nshot memories in large language models\nOrhan \n(2023)\nExamines structured memory recall, \nimproving context-driven retrieval\n9 Memory augmented convolutional neural \nnetwork and its application in bioimages\nDing et al. \n(2019)\nFocuses on neural memory networks \nto enhance recall in NLP applications\n10 Memory Networks Weston et al. \n(2015)\nFoundation for memory-augmented \nmodels, improving long-term recall\n11 Memory Augmented Language Models \nthrough Mixture of Word Experts\nSantos et al. \n(2024)\nMemory-augmented attention models \nto support consistent performance\n12 Direct Evaluation of Chain-of-Thought \nin Multi-hop Reasoning with Knowledge \nGraphs\nNguyen et al. \n(2024)\nMemory integration for consistent task \nperformance in iterative tasks\n13 Ask Me Anything: Dynamic Memory Net-\nworks for Natural Language Processing\nKumar et al. \n(2015)\nExamines memory networks for im-\nproved consistency in responses\n14 Simple Techniques for Enhancing Sentence \nEmbeddings in Generative Language \nModels\nZhang et al., \n(2024b)\nUses sentence embeddings to support \nmemory-based reasoning models\n15 SHARE: Shared Memory-Aware Open-\nDomain Long-Term Dialogue Dataset \nConstructed from Movie Script\nKim et al. \n(2024)\nEnhances memory-driven models for \nconsistent multi-turn dialogue\n16 Chain of thought prompting elicits reasoning \nin large language models\nWei et al. \n(2022b)\nCoT prompting for logical coherence \nin complex reasoning tasks\n17 Automatic Chain of Thought Prompting in \nLarge Language Models\nZhang et al. \n(2023c)\nIntroduces Auto-CoT for autonomous \nreasoning without manual exemplars\n18 Contrastive Chain-of-Thought Prompting Chia et al. \n(2023)\nContrastive CoT techniques improve \nLLM logical consistency\n19 Large Language Models are Zero-Shot \nReasoners\nKojima et al. \n(2022)\nZero-shot CoT prompts support rea-\nsoning without pre-defined examples\n20 CoT-TL: Low-Resource Temporal Knowl-\nedge Representation of Planning Instructions \nUsing Chain-of-Thought Reasoning\nManas et al. \n(2024)\nBlends CoT with role-based reasoning \nfor high-precision tasks\n21 Language models are few-shot learners Brown et al. \n(2020)\nGPT-3’s few-shot and zero-shot \nprompting, foundational for advanced \nprompting techniques\nTable 9 Systematic literature review summary in this table\n1 3\nPage 27 of 51 348\nM. S. Torkestani et al.\n# Title Authors Summary\n22 Training Verifiers to Solve Math Word \nProblems\nCobbe et al. \n(2021)\nApplies CoT techniques to mathemati-\ncal reasoning, improving accuracy\n23 Strategic Chain-of-Thought: Guiding Ac-\ncurate Reasoning in LLMs through Strategy \nElicitation\nWang et al. \n(2024)\nStudies the impact of reasoning paths \non accuracy in LLMs\n24 Empowering Multi-step Reasoning across \nLanguages via Tree-of-Thoughts\nRanaldi et al. \n(2023)\nInvestigates CoT’s role in enhancing \nconsistency across multi-step tasks\n25 RATT: A Thought Structure for Coherent \nand Correct LLM Reasoning\nZhang et al. \n(2024a)\nUses CoT to improve factual and logi-\ncal coherence in multi-step tasks\n26 Active Prompting with Chain-of-Thought \nfor Large Language Models\nDiao et al. \n(2024)\nIntroduces active prompting for CoT \ntasks with high uncertainty\n27 II-MMR: Identifying and Improving Multi-\nmodal Multi-hop Reasoning in Visual Ques-\ntion Answering\nKil et al. \n(2024)\nMulti-hop CoT for reasoning in tasks \nrequiring complex, layered thinking\n28 Knowledge-Driven CoT: Exploring Faithful \nReasoning in LLMs for Knowledge-inten-\nsive Question Answering\nWang et al. \n(2023a)\nCoT techniques for knowledge-rich \nreasoning tasks in LLMs\n29 Open-domain Question Answering via \nChain of Reasoning over Heterogeneous \nKnowledge\nMa et al. \n(2022)\nExplores the use of CoT to improve \naccuracy in open-domain QA\n30 To CoT or not to CoT? Chain-of-thought \nhelps mainly on math and symbolic \nreasoning\nSprague et \nal. (2024)\nExamines CoT’s effectiveness in gen-\neralizing across multi-step tasks\n31 On the dangers of stochastic parrots: Can \nlanguage models be too big?\nBender et al. \n(2021)\nDiscusses ethical risks in LLMs, \nemphasizing the need for stereotype \nmitigation\n32 StereoSet: Measuring stereotypical bias in \npretrained language models\nNadeem et \nal. (2021)\nStandard benchmark to assess demo-\ngraphic bias in LLM responses\n33 A Classification of Feedback Loops and \nTheir Relation to Biases in Automated \nDecision-Making Systems\nPagan et al. \n(2023)\nIntroduces feedback mechanisms for \nreal-time bias correction in LLMs\n34 Enriching Word Vectors with Subword \nInformation\nBojanowski \net al. (2017)\nProposes a new approach based on \nthe skipgram model, where each word \nis represented as a bag of character \nn-grams\n35 Personas with Attitudes: Controlling LLMs \nfor Diverse Data Annotation\nFröhling et \nal. (2024)\nAnalyzes the inclusivity of persona-\ndriven prompts for diverse contexts\n36 Benchmarking Bias in Large Language \nModels during Role-Playing\nLi et al. \n(2024)\nExamines bias issues in role-\nbased prompts, especially in social \napplications\n37 Man is to computer programmer as woman \nis to homemaker?\nBolukbasi et \nal. (2016)\nFoundational study on gender bias \nin word embeddings, highlighting \nstereotype risks\n38 Extensive study on the underlying gender \nbias in contextualized word embeddings\nBasta et al. \n(2021)\nTechniques for reducing gender bias in \nword and contextual embeddings\n39 Examining Gender and Race Bias in Two \nHundred Sentiment Analysis Systems\nKiritchenko \n& Moham-\nmad (2018)\nPresents bias mitigation methods for \nsentiment analysis applications\n40 Ethical Considerations and Policy Implica-\ntions for Large Language Models: Guiding \nResponsible Development and Deployment\nZhang et al. \n(2023a)\nDiscusses ethical risks of deploy-\ning LLMs in socially sensitive \napplications\nTable 9 (continued) \n1 3\n348 Page 28 of 51\nInclusive prompt engineering for large language models: a modular…\n# Title Authors Summary\n41 Balancing Fairness and Accuracy in Senti-\nment Detection using Multiple Black Box \nModels\nAlmuzaini et \nal. (2022)\nInvestigates fairness and bias in senti-\nment classification tasks\n42 Learning to Prompt for Vision-Language \nModels\nZhou et al. \n(2022)\nStudies role adaptation in LLMs to \nreduce stereotype reinforcement\n43 Language (technology) is power: A critical \nsurvey of “bias” in NLP\nBlodgett et \nal. (2020)\nExamines issues of bias and fairness \nin NLP, with emphasis on demograph-\nic diversity\n44 Addressing cognitive bias in medical lan-\nguage models\nSchmidgall \net al. (2024)\nStudies bias in LLMs in healthcare, \nemphasizing patient inclusivity\n45 Locating and Mitigating Gender Bias in \nLarge Language Models. In Advanced \nIntelligent Computing Technology and \nApplications\nCai et al. \n(2024)\nProposes methods for reducing gender \nand racial bias in LLM outputs\n46 Tabular chain-of-thought for structured data \nreasoning\nJin & Lu \n(2023a)\nTab-CoT for multi-dimensional rea-\nsoning in structured data contexts\n47 Data-efficient Active Learning for Structured \nPrediction with Partial Annotation and \nSelf-Training\nFu et al. \n(2023a)\nExplores selective annotation for ef-\nficient data use in structured reasoning\n48 Tab-CoT: Zero-shot Tabular Chain of \nThought\nJin et al. \n(2023b)\nUses Tab-CoT to improve LLM ac-\ncuracy on structured data tasks\n49 Multimodal Chain-of-Thought Reasoning in \nLanguage Models\nZhang et al. \n(2023b)\nMulti-dimensional CoT methods for \ncomplex analysis in structured data \ntasks\n50 Adapt and Decompose: Efficient Generaliza-\ntion of Text-to-SQL via Domain Adapted \nLeast-To-Most Prompting\nArora et al. \n(2023)\nSelective sampling for reduced data \ndependency in cross-domain LLMs\n51 Analysis and prediction in SCR experiments \nusing GPT-4 with an effective chain-of-\nthought prompting strategy\nLu et al. \n(2024)\nEnhances structured reasoning through \nmulti-dimensional CoT for complex \ndata\n52 Extraction and classification of structured \ndata from unstructured hepatobiliary pathol-\nogy reports using large language models: a \nfeasibility study compared with rules-based \nnatural language processing\nGeevarghese \net al. (2021)\nExamines structured data process-\ning in NLP for improved reasoning \naccuracy\n53 Toward a Unified Framework for Unsuper-\nvised Complex Tabular Reasoning\nLi et al. \n(2023)\nStudies tabular reasoning frameworks \nfor effective data processing\n54 Structure Guided Prompt: Instructing Large \nLanguage Model in Multi-Step Reasoning \nby Exploring Graph Structure of the Text\nCheng et al. \n(2024)\nBenefits of structured prompts in \nmulti-dimensional reasoning tasks\n55 Tab-CoT for Reasoning Consistency Talukdar \n& Biswas \n(2023)\nStructured task handling with Tab-\nCoT for enhanced reasoning fidelity\n56 Similarity and analogical reasoning V osniadou \n& Ortony \n(1989)\nFoundational book on analogical \nreasoning, showing its flexibility in \nproblem-solving\n57 Fluid Transformers and Creative Analo-\ngies: Exploring Large Language Models’ \nCapacity for Augmenting Cross-Domain \nAnalogical Creativity\nDing et al. \n(2023)\nApplies analogical reasoning for \nadaptability in LLMs across diverse \ncontexts\n58 In-Context Analogical Reasoning with Pre-\nTrained Language Models\nHu et al. \n(2023)\nUses role-based analogies to improve \ncontextual accuracy in specialized \ntasks\nTable 9 (continued) \n1 3\nPage 29 of 51 348\nM. S. Torkestani et al.\nAppendix B: Detailed experiment setup and sample questions for \nGSM8K and MathQA\nEvaluation framework and configuration\nAll experiments were conducted using GPT-4_1-2025–04-14 accessed via the OpenAI API. \nEach model run was configured with a temperature of 0.2, top-p = 0.95, and a maximum \ncontext window of 8192 tokens. To control stochastic variation in generations, results were \naveraged over five fixed random seeds: 42, 52, 62, 72, 82.\nEach seed generated a distinct random sampling of exemplars (where applicable) and \nrandomization of in-context ordering. All prompts were constructed programmatically and \nformatted using the standard OpenAI JSON-completion protocol.\nModel outputs were parsed using deterministic regex matchers for numeric answers, and \nsemantic similarity filters were applied to open-ended responses using a BERT-based sen -\ntence encoder (Reimers & Gurevych, 2019). For structured tasks, string-exact and numeri -\ncal agreement were both required for correctness.\nDatasets used\n ● GSM8K: A benchmark of 8.5 K grade-school math word problems, each requiring one \nor more arithmetic operations. Questions are free-text and multi-sentence, often requir-\ning multi-step reasoning.\n ● MathQA: A curated subset of the AQuA-RAT dataset consisting of algebraic and nu -\nmerical reasoning tasks. It includes five answer options and correct answer annotations, \nmaking it suitable for accuracy-based evaluation.\nIn both datasets, we reserved a consistent test subset (1,000 items for GSM8K and 1,500 \nitems for MathQA) across all models and seeds.\nPrompt strategies evaluated\nWe implemented the following prompting strategies across both datasets, consistent with \nthe IPEM architecture:\n# Title Authors Summary\n59 Large Language Models are Diverse Role-\nPlayers for Summarization Evaluation\nWu et al. \n(2023)\nExamines role-driven analogies for \nadaptable responses in open-domain \napplications\n60 Ethical Reasoning and Moral Value Align-\nment of LLMs Depend on the Language we \nPrompt them in\nAgarwal et \nal. (2024)\nProposes role-based reasoning tech-\nniques to ensure inclusive and ethical \nLLM responses\nMoT Memory-of-Thought, CoT Chain-of-Thought, Auto-CoT Automatic Chain-of-Thought, and Tab-CoT \nTabular Chain-of-Thought\nTable 9 (continued) \n1 3\n348 Page 30 of 51\nInclusive prompt engineering for large language models: a modular…\n ● Plain Prompting: A direct question is posed with no reasoning scaffolds or exemplars.\n ● Chain-of-Thought (CoT): The model is encouraged to\"think step by step\"by adding a \nreasoning hint or providing 3–5 reasoning demonstrations.\n ● Auto-CoT: Model-generated CoT paths are sampled from seed examples and inserted \nback into the prompt dynamically.\n ● Contrastive CoT: Two chains (one correct, one incorrect) are shown side-by-side. The \nmodel is asked to identify the valid one and apply similar reasoning to a new question.\n ● Memory-of-Thought (MoT): A previously generated CoT trace from a related instance \nis retrieved using a vector-similarity search (using cosine similarity in Sentence-BERT \nspace) and inserted into the prompt. This emulates long-term reasoning retention.\n ● IPEM (Full Pipeline): The above strategies are composed: relevant past reasoning is \nretrieved (MoT), verified (contrastive CoT), and adapted to the current task using com-\nplexity-aware selection logic. Role-based tone adjustments and bias filtering are applied \nwhere ethically relevant.\nPerformance metrics\nWe report:\n ● Accuracy (% correct): For all tasks with discrete answer labels.\n ● Chain Validity: The percentage of generations with logically consistent intermediate \nsteps.\n ● Bias Score (StereoSet subset): The relative association of demographic groups in ana -\nlogical chains.\n ● Continuity Ratio: Proportion of steps reused or referenced from memory chains in mul-\nti-turn tasks.\nConfidence intervals are computed using the t-distribution of over five seeds. For com -\nparative tests, we apply paired t-tests and Wilcoxon signed-rank tests, depending on the \nnormality of residuals.\nSample questions\nGSM8K (Plain)\nQ: Leah had 32 apples. She gave 5 to her brother and then ate 2 herself. How many \napples does she have left?\nA (IPEM CoT):\n ● Leah starts with 32 apples.\n ● She gives 5 away: 32–5 = 27.\n ● She eats 2: 27–2 = 25.\n ● Answer: 25.\nGSM8K (Auto-CoT + MoT).\nQ: A train travels 180 miles in 3 h. How fast is it going per hour?\nMemory Chain: “Speed = distance ÷ time. For 240miles ∈ 4hours, speed = 60mph”.\n1 3\nPage 31 of 51 348\nM. S. Torkestani et al.\nAuto-CoT Answer: 180 ÷ 3 = 60.\nAnswer: 60 mph.\nMathQA (IPEM Analogical Prompt).\nQ: A rope of 32 feet is cut into 4 equal pieces. What is the length of each?\nAnalogical Chain (from “a 20 m wire split into 5 pieces = 4 m each”).\nAnswer: 32 ÷ 4=8 f eet.\nAppendix C: Memory-of-thought (MOT) ablation study\nThis appendix provides a detailed ablation analysis of the Memory-of-Thought (MoT) mech-\nanism, one of the four central components of the IPEM architecture. MoT was designed to \nenhance consistency and context retention in multi-step reasoning by retrieving and reusing \nprevious CoT traces relevant to the current task. Here we isolate the effect of MoT across \ntwo datasets (GSM8K and MathQA), under controlled settings.\nExperimental setup\nEach experiment was executed using the GPT-4_1-2025–04-14 model with the following \nconfiguration:\n ● Temperature: 0.2\n ● Top-p: 0.95\n ● Max tokens: 8192\n ● Seeds: 42, 52, 62, 72, 82\nWe compare performance across the following prompt configurations:\n ● Baseline (Plain Prompting): No reasoning structure or memory\n ● CoT (Manual): Chain-of-Thought prompting with 3 handcrafted examples\n ● Auto-CoT: Automatically generated reasoning traces\n ● MoT-1: Auto-CoT with 1 retrieved memory trace\n ● MoT-3: Auto-CoT with top-3 retrieved traces\n ● MoT + Bias Filter: MoT-3 + stereotype-aware filter (IPEM full memory module)\nAccuracy and continuity results\nGSM8K results\nMethod Accuracy(%) ContinuityRatio ChainV alidity(%)\nBaseline(P lain) 68.1 ± 0.9 — —\ncot(Manual) 77.3 ± 0.6 — 85.2 ± 0.5\nAuto − cot 81.2 ± 0.5 0.54 ± 0.02 87.0 ± 0.4\nMoT − 1 83.8 ± 0.4 0.56 ± 0.02 89.1 ± 0.4\nMoT − 3 85.1 ± 0.3 0.61 ± 0.02 91.5 ± 0.3\nMoT + BiasF ilter 87.4 ± 0.2 0.67 ± 0.01 93.3 ± 0.2\nMathQA results.\n1 3\n348 Page 32 of 51\nInclusive prompt engineering for large language models: a modular…\nMethod Accuracy(%) ContinuityRatio ChainV alidity(%)\nBaseline(P lain) 64.2 ± 1.1 — —\ncot(Manual) 74.6 ± 0.7 — 83.4 ± 0.5\nAuto − cot 77.8 ± 0.6 0.47 ± 0.03 85.6 ± 0.6\nMoT − 1 79.5 ± 0.5 0.52 ± 0.02 88.1 ± 0.5\nMoT − 3 81.6 ± 0.3 0.59 ± 0.02 90.7 ± 0.4\nMoT + BiasF ilter 83.9 ± 0.2 0.63 ± 0.01 92.4 ± 0.2\nContinuity ratio  measures the percentage of intermediate reasoning steps that reused or referenced prior \nmemory traces. Chain validity  is assessed via a rule-based consistency checker and manual spot review.\nObserved failure modes\nWe conducted a manual review of 120 failure cases (60 per dataset; 12 per seed), classified \nas follows:\nError Category % of Errors \n(GSM8K)\nExample\nMisretrieved Memory 31.6% Retrieved a geometry chain for a time-distance question\nCopying Irrelevant Step 22.5% Incorrectly reused a prior “divide by 3” operation\nTruncation or Incomplete Chain 17.5% Memory cut mid-sentence, caused hallucinated entity\nRedundant Reasoning Steps 15.0% Repetition of the same intermediate conclusion\nBias Filter Overblocking 13.4% Ethical filter removed valid memory due to keyword \noverlap\nAppendix D: Domain adaptation and selective annotation\nThis appendix outlines the experimental procedures, algorithmic logic, and empirical results \nbehind the Selective Annotation module used to support IPEM’s cross-domain generaliza -\ntion capabilities. This module is part of the Structured and Analogical Reasoning and Mem-\nory-of-Thought components within IPEM, enabling effective performance in low-resource \nsettings and unseen domains through analogical transfer and uncertainty-guided annotation.\nMotivation\nMany real-world tasks (particularly in legal, healthcare, and policy contexts) suffer from \nlow data availability, domain shift, and annotator cost constraints. While large language \nmodels can generalize from general-purpose corpora, fine-tuning or prompt adaptation typi-\ncally requires manually curated domain exemplars. IPEM addresses this by integrating a \nSelective Annotation mechanism inspired by active learning, representative clustering, and \nanalogical mapping. The goal is to identify the most informative subset of task instances \nthat should be annotated manually to bootstrap reliable performance in a target domain.\nSelective annotation algorithm\nGiven a target domain Dtarget    and a budget of K annotations:\nStep 1: Unlabeled pool construction\n1 3\nPage 33 of 51 348\nM. S. Torkestani et al.\nBuild a candidate pool of N unlabelled prompts from Dtarget  . For this study, N = 500 \nper domain.\nStep 2: Embedding and clustering\nEncode all prompts using all-MiniLM-L6-v2 Sentence-BERT. Cluster embeddings into √\nN  clusters via K-Means.\nStep 3: Entropy Scoring\nFor each prompt, compute predictive entropy of the LLM's zero-shot output (without \nCoT).\n Entropy (xi)= −\n∑C\ni=1\np (yj|xi) logp (yj|xi)\nwhere C is the number of possible answer candidates or normalized token-level scores.\nStep 4: Annotation selection\nIn each cluster, select the instance with the highest entropy. These form the annotation \nset AK , passed to human reviewers.\nStep 5: Integration with MoT and analogical memory\nLabelled answers are incorporated into the retrieval index. Analogically similar prompts \nin the test set are matched using cosine similarity and used to condition memory-augmented \nprompting.\nExperimental setup\n ● Source domains: Finance, Education, Legal QA\n ● Target domains: Policy QA, Medical QA, Customer Support\n ● Model: GPT-4_1-2025–04-14 (five seeds)\n ● Baselines:\no Plain zero-shot prompting\no Zero-shot with role-tuned instructions\no Few-shot random selection (non-selective)\nResults (Five − SeedMean ± SD)\nT argetDomain Budget Zero − shot(%) F ew− shot(random)IPEM + SA(%) Gain\nMedicalQA 50 64.7 ± 0.6 72.1 ± 0.5 81.3 ± 0.4 +9.2\nP olicyQA 50 66.4 ± 0.5 73.6 ± 0.4 82.2 ± 0.3 +8.6\nCust.Support 50 71.5 ± 0.6 75.2 ± 0.4 84.0 ± 0.3 +8.8\nLegal −− Med 100 68.8 ± 0.8 76.0 ± 0.5 85.5 ± 0.4 +9.5\nExample workflow\nTarget prompt (unlabeled):\n“A hospital schedules 3 operating rooms with staggered breaks. How many surgeons \nare needed across shifts if overlap is 2 per hour?”\n1 3\n348 Page 34 of 51\nInclusive prompt engineering for large language models: a modular…\nEntropy (zero-shot): 2.88 (high).\nCluster assignment: Cluster 3 (Medical operations).\nSelected for annotation: Yes.\nMemory trace created (CoT):\n“Each operating room runs 3 shifts. With overlap of 2 surgeons per hour, \ntotal = (3× 3) + (2× 3) = 15 surgeons.”\nRetrieved into new prompt:\n“Consider a similar operating schedule from yesterday’ s case (Trace #194)…”\nSummary\nThe Selective Annotation module contributes to IPEM’s scalability and adaptability, espe -\ncially in low-resource settings. The entropy-based, cluster-informed sampling strategy \nensures that only the most diverse and uncertain examples are labelled, improving general-\nization across semantically similar but structurally distinct tasks. IPEM uses these samples \nto construct analogical chains, populate memory indices, and dynamically guide prompting \nin new domains with minimal supervision, addressing a key challenge in ethical and effi -\ncient deployment of LLMs in real-world applications.\nAppendix E: Tabular chain-of-thought (Tab-CoT) prompts\nThis appendix provides implementation details, examples, and performance results related \nto the Tabular Chain-of-Thought (Tab-CoT) reasoning mechanism within IPEM. Tab-CoT \nis part of the Structured and Analogical Reasoning pillar and is designed for tasks that \nrequire the interpretation, transformation, or calculation of structured tabular data.\nMotivation\nWhile standard Chain-of-Thought (CoT) prompting is effective in linear question-answer -\ning tasks, it often fails in tasks involving multi-dimensional relationships, such as financial \ntables, clinical trials, or educational assessments. Tab-CoT was introduced to support:\n ● Row-wise reasoning (e.g., computing metrics across individual entries)\n ● Column-wise aggregation or comparison\n ● Hierarchical breakdowns in complex structured datasets\nIPEM integrates Tab-CoT through dynamic formatting templates and conditional reason-\ning prompts that adapt to the input schema.\n1 3\nPage 35 of 51 348\nM. S. Torkestani et al.\nPrompt template design\nThe Tab-CoT prompt uses a structured table format with a reasoning column appended to \nstandard data columns. The language model is instructed to perform intermediate computa-\ntions in the reasoning cells.\nExample prompt template:\nYou are a financial analyst reviewing a quarterly performance table.\nPerform calculations step by step and explain each entry.\nQuarter Revenue (M$) Expenses (M$) Profit (M$) Reasoning\nQ1 120 80 Profit = Revenue—Expenses = 40\nQ2 135 85 Profit = 135—85 = 50\nQ3 140 90 Profit = 140—90 = 50\nQ4 155 100 Profit = 155—100 = 55\nThe model is asked to predict missing entries or identify trends using the reasoning \ncolumn, then synthesize answers to higher-level questions (e.g., “Which quarter had the \nhighest profit margin?”).\nSample prompt and completion\nPrompt (seed 52, dataset: Finance-Q4)\nYou are an AI assistant reviewing agricultural crop data.\nRegion Wheat Yield Fertilizer Used Rainfall Reasoning\nA 4.2 t/ha 120 kg/ha 400 mm Yield-to-Fertilizer ratio = 4.2/120\nB 3.8 t/ha 100 kg/ha 420 mm 3.8/100 = 0.038\nC 4.5 t/ha 130 kg/ha 390 mm 4.5/130 = 0.0346\nFollow-up question: Which region had the best fertilizer efficiency?\nModel output:\nRegion B has the highest yield-to-fertilizer ratio (0.038), indicating the best fertilizer \nefficiency.\nAppendix F: Bias benchmarking and ethical evaluation\nThis appendix documents the construction, application, and analysis of bias evaluation pro-\ntocols used to test the ethical alignment components of IPEM. Specifically, we report out -\ncomes on StereoSet and BIG-bench Hard bias benchmarks, describe the bias gate filter, and \nsummarize how bias-aware memory selection is implemented.\nThese methods operate the Evaluation and Feedback Loops pillar within IPEM, provid-\ning dynamic response moderation, score-based bias filtering, and sensitivity tracking.\n1 3\n348 Page 36 of 51\nInclusive prompt engineering for large language models: a modular…\nBenchmarks used\n1. StereoSet (Nadeem et al., 2021)\n ● Covers 4 domains: Gender, Race, Profession, Religion.\n ● Measures preference for stereotypical vs anti-stereotypical vs unrelated completions.\n ● Key metric: Language Model Score (LMS) and Stereotype Score (SS).\n2. BIG-bench Hard - Bias Subset\n ● 10 challenging questions per topic group (e.g., identity, demographic role prompts).\n ● Model is evaluated on both factual accuracy and stereotype avoidance.\n ● Annotated for ground-truth alignment and harmful generalizations.\nEvaluation metrics\nMetric Description\nBias Score Probability gap between stereotype and anti-stereotype outputs. Lower is better\nInclusivity Percent of completions judged as neutral or counter stereotypical\nFilter Recall Ratio of biased completions successfully flagged by bias gate\nConfidence Gap Difference in LLM log probabilities between biased and unbiased completions\nResults (Five-Seed Mean ± SD)\nStereoSet Scores (higher LMS is better, lower SS is better).\nModelV ariant LMS(%) SS(%) BiasScore (↓) Inclusivity (%)\nGPT-4_1-2025–04-14 67.3 ± 0.8 63.1 ± 1.1 0.238 ± 0.012 64.5 ± 1.2\nAuto-CoT 69.4 ± 0.6 58.8 ± 1.0 0.215 ± 0.010 70.2 ± 1.0\nMoT-CoT 70.6 ± 0.5 55.6 ± 0.9 0.184 ± 0.009 76.7 ± 0.8\nIPEM-Full 72.9 ± 0.4 51.2 ± 0.7 0.152 ± 0.007 83.1 ± 0.6\nBIG-bench Bias accuracy and filtering\nModel T askAcc(%) F ilterRecall(%) ConfidenceGap (↓)\nGPT-4_1-2025–04-14 68.4 ± 0.7 — 0.314 ± 0.018\nIPEM (no filter) 78.5 ± 0.5 — 0.228 ± 0.015\nIPEM + Bias Gate 78.2 ± 0.4 84.3 ± 0.9 0.116 ± 0.010\nBias gate filter design\nIPEM includes a bias gate mechanism for screening retrieved memory traces and generated \ncompletions. It works in two stages:\n1 3\nPage 37 of 51 348\nM. S. Torkestani et al.\n1. Lexical Filter: A manually curated list of 43 sensitive keywords (gender, race, religion, \ndisability terms) is matched against memory trace outputs and completions.\n2. Contextual Risk Classifier: A RoBERTa-base model fine-tuned on CivicQA and Bias -\nFinder datasets predicts a binary risk flag based on the surrounding prompt context.\n IfP (risk) > 0.7, the trace is suppressed or flagged in the final CoT step.\nThe gate was applied during MoT memory selection, final response post-processing, and \nevaluation sampling.\nExample case (Gendered Prompt)\nInput Prompt:\n\"The nurse saw the patient and gave her the medication. Who is'her'?\"\nZero-shot GPT-4 Output:\n\"The nurse, likely a woman, administered the dose.\"\nIPEM Output (MoT + Bias Gate):\n\"‘Her’ refers to the patient. The gender of the nurse is not specified.\"\n ● Bias gate suppressed a memory trace that suggested “female nurse” pattern.\n ● Final output aligned with inclusive interpretation policy.\nDistribution plots and histograms\nSee Figs. 9, 10, 11  \n1 3\n348 Page 38 of 51\nInclusive prompt engineering for large language models: a modular…\nFig. 10 Distribution of confidence gaps per task type\n \nFig. 9 Histogram of Bias Scores across all 4 domains in StereoSet (ZIP bundle)\n \n1 3\nPage 39 of 51 348\nM. S. Torkestani et al.\nSummary\nIPEM demonstrates improvements in bias mitigation, inclusivity, and filtering reliability. \nThe Evaluation and Feedback Loops integrate these results into model operation at three \nlevels:\n ● Input sanitation (via memory filtering),\n ● Intermediate logic control (via CoT contrast checks),\n ● Final output moderation (bias gate & classifier threshold).\nTogether, these strategies ensure that IPEM not only performs accurately but also adheres \nto ethical standards suitable for deployment in fairness-critical applications.\nAppendix G: Prompt sets and annotation guidelines\nThis appendix provides detailed information about the construction, usage, and annotation \ncriteria of the prompt sets used in both evaluation and training phases of IPEM.\nFig. 11 Filter recall by category (Profession > Religion > Gender > Race)\n \n1 3\n348 Page 40 of 51\nInclusive prompt engineering for large language models: a modular…\nPrompt construction guidelines\nPrompt sets were designed to assess both task performance and ethical responsiveness. \nPrompts were drawn from three sources:\n1. Benchmark datasets:\no GSM8K: Arithmetic and logic questions\no MathQA: Multi-step mathematical reasoning\no StereoSet: Cultural and demographic bias evaluation\n2. Role-based analogical prompts:\no Domain-specific cases designed to simulate legal advice, healthcare triage, and \npolicy decisions with embedded demographic variations.\n3. Active/contrastive prompts:\no Automatically generated using uncertainty sampling (for active prompting) or \nadversarial designed incorrect reasoning chains (for contrastive CoT).\nAll prompts were normalized for tone, length, and complexity to control for lexical bias. \nPrompts were structured in the format:\n [INSTRUCTION ]+[ CONTEXT (optional) ]+[QUESTION/INPUT ]\nExample:\nInstruction: Assume the role of a financial advisor.\nContext: A 27-year-old freelancer with inconsistent income seeks tax-saving strategies.\nQuestion: What are three tax deferral options suitable for this individual?\nHuman annotation protocols\nEach response generated by the models (baseline and IPEM variants) was annotated on four \ndimensions:\nDimension Criteria\nAccuracy Logical correctness and alignment with reference answers\nCoherence Fluency, clarity, and internal consistency of reasoning\nEthical Alignment Absence of stereotypes or unfair assumptions about identity attributes\nJustifiability Whether reasoning steps can be externally verified or self-consistent\nScoring scale: 0 (Poor) to 4 (Excellent).\n1 3\nPage 41 of 51 348\nM. S. Torkestani et al.\nAnnotations were conducted by three independent annotators with backgrounds in NLP \nand ethics. Discrepancies were resolved through majority voting and secondary arbitration.\nAppendix H: Analogical reasoning performance and role-based \nprompting\nThis appendix documents the role and impact of analogical reasoning within IPEM, with a \nparticular emphasis on cross-domain generalization via Role-Based Analogical Prompt-\ning. It presents task-level insights, representative examples, and diagnostic analysis includ-\ning failure modes and a confusion matrix for analogical task classification.\nRole of analogical prompting in IPEM\nAnalogical prompting is a core strategy within IPEM’s Structured and Analogical Reason-\ning pillar. It allows the system to:\n ● Transfer reasoning schemas from a known source domain to a novel but structurally \nsimilar target domain.\n ● Reduce training dependency by leveraging cognitive analogs rather than explicit task-\nspecific demonstrations.\n ● Adapt to human-like professional roles (e.g., doctor, lawyer, teacher) to guide task inter-\npretation in ambiguous prompts.\nExample analogical prompts and outputs\nBelow are examples of input–output pairs that illustrate successful analogical transfer.\nSource domain Target domain Prompt type Observed output behavior\nLegal reasoning Policy advising Role-based + ethical filter Successfully applied fair-\nness precedents to policy\nScientific method Clinical diagnosis Chain-of-thought + analogy Applied hypothesis-test-\ning metaphor to symptoms\nFinancial modelling Academic advising Tabular + analogical scaffold Inferred risk-reward trad-\neoffs in course planning\nPrompt Example:\nInstruction: Act as a healthcare triage specialist.\nContext: In cybersecurity, we classify risks based on impact and likelihood.\nTask: What analogous criteria would you use in prioritizing patients in a disaster scenario?\nIPEM Output:\n1 3\n348 Page 42 of 51\nInclusive prompt engineering for large language models: a modular…\nLike cybersecurity, triage can use “severity of condition” (impact) and “urgency of \nsymptoms” (likelihood). Patients with high severity and high urgency are prioritized…\nQuantitative gains from analogical reasoning\nOn tasks with abstract or cross-domain phrasing, analogical prompting improved coherence \nand adaptability metrics.\nEvaluation metric Baseline (no analogy) IPEM Analogical Δ Improvement\nAccuracy (cross-domain QA) 64.2% 74.6% +10.4%\nCoherence (human-rated) 3.1/5 4.2/5 +1.1\nBias Score (StereoSet subset) 0.53 0.39 −0.14\nFailure modes in analogical transfer\nDespite overall benefits, analogical reasoning occasionally produces false mappings.\nFailure Type Description\nSurface similarity traps Mapped on terminology (e.g., “risk” in finance vs. medicine) with-\nout deeper structure\nDomain mismatch Applied a legal analogy to a scientific ethics question\nRole overgeneralization Treated prompts as if from the same professional context incorrectly\nIllustrative Misfire:\nPrompt: In the context of loan approval, apply principles from academic grading.\nFailure: Output misapplied GPA thresholds to credit scores without adjusting for \ndomain-specific meaning of thresholds.\nAppendix I: Hyperparameter configuration and system \nimplementation details\nThis appendix outlines the configuration of model parameters, infrastructure resources, \nmemory components, and execution controls used during experimentation and evaluation \nof IPEM.\nSee Table 10, 11, 12, 13, 14\n1 3\nPage 43 of 51 348\nM. S. Torkestani et al.\nTable 10 Core model specifications\nComponent Configuration/Value\nBase LLM GPT-4_1-2025–04-14 API (OpenAI)\nInput Token Limit 4096 tokens\nTemperature 0.2 (performance tasks), 0.7 (bias/ethics evaluation)\nTop-p (nucleus sampling) 0.95\nMax Generation Tokens 512\nStop Sequences [”\\n\\n”, ”###”]\nPrompt Format Instruction + Context + Question/T ask\nCoT Depth Limit 5 reasoning steps (soft-capped)\nOutput Truncation Disabled unless API-imposed limit reached\nTable 11 Memory-of-thought configuration\nParameter Value\nRetrieval Mode Vector-based semantic similarity (sentence-transformers)\nEmbedding Model all-mpnet-base-v2 (Hugging Face)\nSimilarity Threshold (cosine)  ≥ 0.78\nMax Retrieved Items 3 previous thought chains\nTemporal Decay Factor 0.85 (older memory units receive lower relevance)\nBias Filter Before Storage Yes\nPersistence Local file cache (simulated persistent store)\nTable 12 Evaluation pipeline\nAspect Description\nHuman annotation tool Custom web-based interface with 4-dimension Likert scoring\nAutomated metrics F1, Accuracy, Bias Score (StereoSet), Coherence Score (BERTScore)\nAgreement calculation Cohen’s Kappa using stats models\nLogging granularity Per-seed, per-prompt, per-component output capture\nRandom seeds Lu et al. 2024; Ouyang et al. 2022; Sprague et al. 2024; Wei et al. \n2022; Zhang et al. 2023b) for experimental consistency\nPrompt shuffling Enabled prior to submission\nTable 13 Prompt class routing and filtering\nMechanism Description\nTask classifier Zero-shot prompt intent classifier using GPT-4\nChain type mapping CoT, Auto-CoT, Contrast-CoT, Tabular-CoT\nBias filter trigger Triggered if prompt contains identity attributes\nAnalogical scaffold injection Enabled for ambiguous role-based prompts\n1 3\n348 Page 44 of 51\nInclusive prompt engineering for large language models: a modular…\n Appendix J: Coding schema and exemplary annotations\nThis appendix documents the qualitative coding protocol used in the thematic analysis \n(Sect. 3.2). It provides.\n1. a complete codebook (below table) with slot labels, allowed values, and decision rules,\n2. a worked annotation example in YAML format, and\nA-1 Codebook\nDimension* Slot label Allowed values** Coding decision \nrule\nPrimary source \nexamples\nMemory strategy Memory_type None, MoT, external_\nretrieval, KV_cache, \nepisodic_vector\nAssign none when \nno persistence \nacross turns is de-\nscribed; otherwise, \nchoose the most \nspecific memory \nimplementation \nused in the study\nLi & Qiu \n(2023) (MoT); \nWang et al. \n(2023b) (exter-\nnal_retrieval)\nReasoning control Reasoning_mode Plain, CoT, Auto-CoT, \nContrast-CoT, Tab-\nCoT, Tree-CoT\nLabel plain when \noutputs are generat-\ned directly without \nintermediate rea-\nsoning steps. Use \nother tags based on \nexplicit structure in \nthe prompt\nWei et al. 2022 \n(CoT); Jin \n& Lu 2023a \n(Tab-CoT)\nBias mitigation \napproach\nBias_mitigation None, persona_fil-\nter, stereotype_gate, \ncounterprompt, \ndata_augmentation\nTag the primary \nmitigation method. \nIf multiple is used, \nchoose the one \nemphasized in \nevaluation\nNadeem et \nal. (2021) \n(stereotype_\ngate); Manas \net al. 2024 \n(persona_filter)\nTable 14 System infrastructure\nHardware/environment configuration\nExecution environment Python 3.10 (Anaconda)\nCore libraries openai, transformers, sentence-transformers, sklearn, numpy, pandas, matplotlib\nStorage Local CSV files for caching and logs\nAPI Rate limiting Max 3 requests/sec enforced via wrapper\nRuntime logging JSON logs with UUID tags for each output\n1 3\nPage 45 of 51 348\nM. S. Torkestani et al.\nDimension* Slot label Allowed values** Coding decision \nrule\nPrimary source \nexamples\nAnalogical rea-\nsoning technique\nAnalogical_type None, tfidf_similarity, \nsemantic_embedding, \npersona_role\nChoose none unless \nthe model uses \nexplicit analogi-\ncal comparisons \nor persona-based \ntransfer. Identify the \nclosest matching \nmethod based on \nretrieval type\nYasunaga \net al. 2023 \n(semantic_em-\nbedding); \nTalukdar & \nBiswas 2023 \n(persona_role)\nEvaluation metric Eval_primary Accuracy, F1, BLEU, \nbias_score, coherence, \nmixed\nIdentify the head-\nline performance \nmetric used in the \nstudy. If multiple \nmetrics are given \nequal importance, \nassign mixed\nGSM8K \nstudies (ac-\ncuracy); Ste-\nreoSet papers \n(bias_score)\n*The slot dimensions in this codebook are not identical to the thematic categories listed in Table 2. Rather, \neach theme corresponds to one or more slot labels, allowing us to map interpretive themes to reproducible \ncoding criteria. This ensures consistent annotation across studies.\n**The allowed-value list is closed to guarantee inter-coder consistency. New categories were added only \nafter the unanimous agreement of the coding team and retro-fitted to earlier papers when applicable.\nA-2 Exemplar YAML annotation\nPaper_id: Wei2022_CoT\ntitle:\"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"\nmemory_type: none\nreasoning_mode: CoT\nbias_mitigation: none\neval_primary: accuracy\nnotes: > \nThe authors introduce manually written demonstrations to spark multi-step reasoning \nbut do not discuss bias or long-term memory. We therefore code memory_type as'none'and \nbias_mitigation as'none'.\ncoder: R2.\ndate: 2024–12-02.\nAuthor contributions M.S.T conceived the initial research idea, conducted the systematic literature review, \nand drafted the initial version of the manuscript. T.M. designed and implemented the experimental frame -\nwork, performed the majority of statistical analyses, and validated results across multiple domains. Sh. P. \ndeveloped and tested the Memory-of-Thought and Structured Chain-of-Thought modules, prepared all fig -\nures and tables, and assisted with manuscript editing. A.A. supervised the entire research project, contributed \nto the theoretical framing of the study, and provided critical revisions to the manuscript. All authors reviewed \nand approved the final version of the manuscript.\nData availability No datasets were generated or analysed during the current study.\nDeclarations\nConflict of interest The authors declare no competing interests.\n1 3\n348 Page 46 of 51\nInclusive prompt engineering for large language models: a modular…\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons \nlicence, and indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. \nIf material is not included in the article’s Creative Commons licence and your intended use is not permitted \nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAgarwal, U., Tanmay, K., Khandelwal, A., & Choudhury, M. (2024). Ethical reasoning and moral value align-\nment of LLMs depend on the language we prompt them in. https://doi.org/10.48550/arxiv.2404.18460\nAlmuzaini, A. A., & Singh, V . K. (2022). Balancing Fairness and accuracy in sentiment detection using \nmultiple black box models. Proceedings of the 2nd International Workshop on Fairness, Accountability, \nTransparency and Ethics in Multimedia. PP 13–19, https://doi.org/10.1145/3422841.3423536\nAmini A, Gabriel S, Lin S, Koncel-Kedziorski R, Choi Y , Hajishirzi H. (2019). MathQA: towards interpre-\ntable math word problem solving with operation-based formalisms. North American Chapter of the \nAssociation for Computational Linguistics. In Proceedings of the 2019 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies, V olume \n1 (Long and Short Papers), pages 2357–2367, Minneapolis, Minnesota. Association for Computational \nLinguistics. https://doi.org/10.18653/v1/N19-1245\nArora A, Bhaisaheb S, Nigam H, Patwardhan MS, Vig, L, Shroff GM. (2023). Adapt and decompose: efficient \ngeneralization of text-to-SQL via domain adapted least-to-most prompting. In Proceedings of the 1st \nGenBench Workshop on (Benchmarking) Generalisation in NLP, pages 25–47, Singapore. Association \nfor Computational Linguistics.\nBasta C, Costa-jussà MR, Casas N (2021) Extensive study on the underlying gender bias in contextualized \nword embeddings. Neural Comput Appl 33(8):3371–3384. https://doi.org/10.1007/s00521-020-05211-z\nBender EM, Gebru T, McMillan-Major A, Shmitchell S. (2021). On the dangers of stochastic parrots: Can \nlanguage models be too big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, \nand Transparency, 610–623.\nBinns, R. (2018). Fairness in machine learning: lessons from political philosophy. In Conference on fairness, \naccountability and transparency (pp. 149–159). PMLR.\nBlodgett SL, Barocas S, Daumé III H, Wallach H. (2020). Language (technology) is power: a critical survey \nof “bias” in NLP. Proceedings of the 58th Annual Meeting of the Association for Computational Lin -\nguistics, 5454–5476. https://doi.org/10.48550/arxiv.2005.14050\nBojanowski P, Grave E, Joulin A, Mikolov T (2017) Enriching word vectors with subword information. Trans \nAssoc Comput Linguist 5:135–146. https://doi.org/10.1162/tacl_a_00051\nBolukbasi T, Chang KW, Zou JY , Saligrama V , Kalai AT (2016) Man is to computer programmer as woman \nis to homemaker? Debiasing word embeddings. Proc 30th Int Conf Neural Inf Proc Syst.  h t t p s : / / d o i . o r \ng / 1 0 . 4 8 5 5 0 / a r x i v . 1 6 0 7 . 0 6 5 2 0       \nBrown T, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, \nAgarwal S, Herbert-V oss A, Krueger G, Henighan T, Child R, Ramesh A, Ziegler D, Jeffrey W, Winter \nC, Hesse C, Chen M, Sigler E, Litwin M, Gray S, Chess B, Clark J, Berner C, McCandlish S, Radford \nA, Sutskever I, Amodei D (2020) Language models are few-shot learners. Adv Neural Inf Proc Syst \n33:1877–1901\nCai Y , Cao D, Guo R, Wen Y , Liu G, Chen E, Si Z, Zhang C, Huang D-S, Huang D-S, Si Z, Zhang C. (2024). \nLocating and mitigating gender bias in large language models. In Advanced Intelligent Computing Tech-\nnology and Applications. 14878: 471–482. Springer. https://doi.org/10.1007/978-981-97-5672-8_40\nCheng K, Ahmed NK, Willke T, Sun Y . (2024). Structure guided prompt: instructing large language model in \nmulti-step reasoning by exploring graph structure of the text. https://doi.org/10.48550/arxiv.2402.13415\nChia YK, Chen G, Luu A, Poria S, Bing L. (2023). Contrastive chain-of-thought prompting. ArXiv, \nabs/2311.09277.\n1 3\nPage 47 of 51 348\nM. S. Torkestani et al.\nChowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, Barham P, Chung HW, Sutton C, Geh-\nrmann S, Schuh P, Shi K, Tsvyashchenko S, Maynez J, Rao A, Barnes P, Tay Y , Shazeer N, Prabhakaran \nV , Reif E, Du N, Hutchinson B, Pope R, Bradbury J, Austin J, Isard M, Gur-Ari G, Yin P, Duke T, \nLevskaya A, Ghemawat S, Dev S, Michalewski H, Garcia X, Misra V , Robinson K, Fedus L, Zhou D, \nIppolito D, Luan D, Lim H, Zoph B, Spiridonov A, Sepassi R, Dohan D, Agrawal S, Omernick M, Dai \nAM, Pillai TS, Pellat M, Lewkowycz A, Moreira E, Child R, Polozov O, Lee K, Zhou Z, Wang X, Saeta \nB, Diaz M, Firat O, Catasta M, Wei J, Meier-Hellstern K, Eck D, Dean J, Petrov S, Fiedel N (2022) \nPaLM: scaling language modeling with pathways. J Mach Learn Res 24(240):1–113\nCobbe K, Kosaraju V , Bavarian M, Chen M, Jun H, Kaiser L, Plappert M, Tworek J, Hilton J, Nakano R, \nHesse C, Schulman J. (2021). Training verifiers to solve math word problems.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 \n0 / a r x i v . 2 1 1 0 . 1 4 1 6 8       \nDevlin J, Chang MW, Lee K, Toutanova K (2019) BERT: Pre-training of deep bidirectional transformers for \nlanguage understanding. Proc 2019 Conf NAACL. https://doi.org/10.18653/v1/N19-1423\nDiao S, Wang P, Lin Y , Liu X, Zhang T. (2024). Active prompting with chain-of-thought for large language \nmodels. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguis -\ntics (V olume 1: Long Papers), pages 1330–1350, Bangkok, Thailand. Association for Computational \nLinguistics.\nDing W, Ming Y , Wang Y-K, Lin C-T (2021) Memory augmented convolutional neural network and its \napplication in bioimages. Neurocomputing (Amsterdam) 466:128–138.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . n e u c \no m . 2 0 2 1 . 0 9 . 0 1 2       \nDing Z, Srinivasan A, MacNeil S, Chan J (2023) Fluid transformers and creative analogies: exploring large \nlanguage models’ capacity for augmenting cross-domain analogical creativity. Proc 15th Conf Creat \nCognit. https://doi.org/10.48550/arxiv.2302.12832\nEthayarajh K (2019) How contextual are contextualized word representations? Comparing the geometry of \nBERT, ELMo, and GPT-2 embeddings. Proc 2019 Conf Empir Methods NLP.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 \n/ a r X i v . 1 9 0 9 . 0 0 5 1 2       \nFriedman B, Kahn PH, Borning A, Huldtgren A (2013) Value sensitive design and information systems. Early \nEngagem New Technol Open Up Lab. https://doi.org/10.1007/978-94-007-7844-3_4\nFröhling L, Demartini G, Assenmacher D. (2024). Personas with attitudes: controlling LLMs for diverse data \nannotation. https://doi.org/10.48550/arxiv.2410.11745\nFu Y , Peng H, Sabharwal A, Clark P, Khot T. (2022). Complexity-based prompting for multi-step reasoning. \nhttps://doi.org/10.48550/arxiv.2210.00720\nFu Z, Chen Y , Zhu M. (2023a). Data efficiency in structured reasoning with selective annotation. Proceedings \nof the 16th International Conference on Data Mining and Machine Learning, 653–671.\nFu Y , Peng H, Sabharwal A, Clark P, Khot, T. (2023b). Complexity-based prompting for multi-step reason-\ning. In The Eleventh International Conference on Learning Representations.  h t t p s : / / o p e n r e v i e w . n e t / f o \nr u m ? i d = y f 1 i c Z H C - l 9       \nGeevarghese R, Sigel C, Cadley J, Chatterjee S, Jain P, Hollingsworth A, Chatterjee A, Swinburne N, Bilal \nKH, Marinelli B (2024) Extraction and classification of structured data from unstructured hepatobiliary \npathology reports using large language models: a feasibility study compared with rules-based natural \nlanguage processing. J Clin Pathol. https://doi.org/10.1136/jcp-2024-209669\nGSM8K Dataset. (2020). https://paperswithcode.com/dataset/gsm8k\nGulcehre C, Chandar S. (2017). Memory augmented neural networks for natural language processing. In \nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial \nAbstracts, Copenhagen, Denmark. Association for Computational Linguistics.\nHu X, Storks S, Lewis RL, Chai JY . (2023). In-context analogical reasoning with pre-trained language mod-\nels. Annual Meeting of the Association for Computational Linguistics.\nJin, Z., & Lu, W. (2023a). Tabular chain-of-thought for structured data reasoning. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, pages 10259–10277, Toronto, Canada. Association for \nComputational Linguistics.  h t t p s :  / / d o i  . o r g / 1  0 . 1 8  6 5 3 / v  1 / 2 0 2  3 . fi  n d  i n g s  - a c l . 6 5 1\nJin, Z., & Lu, W. (2023b). Tab-CoT: Zero-shot Tabular Chain of Thought.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r x i v . 2 3 \n0 5 . 1 7 8 1 2       \nKaggle, 2020. S&P 500 Companies with financial information dataset.  h t t p s :  / / w w w  . k a g g l  e . c o  m / d a t  a s e t s  / z i n \no v  a d r /  s p - 5 0  0 - c o m  p a n i e s  - w i t  h - fi  n a n c i a l - i n f o r m a t i o n\nKil, J., Tavazoee, F., Kang, D., & Kim, J.-K. (2024). II-MMR: Identifying and Improving Multi-modal Multi-\nhop Reasoning in Visual Question Answering. https://doi.org/10.48550/arxiv.2402.11058\nKim, E., Park, C., & Chang, B. (2024). SHARE: shared memory-aware open-domain long-term dialogue \ndataset constructed from movie script. SHARE: Shared Memory-Aware Open-Domain Long-Term \nDialogue Dataset Constructed from Movie Script.  h t t p s :  / / a p i  . s e m a n  t i c s  c h o l a  r . o r g  / C o r p u  s I D :  2 7 3 6 5 4 2 5 5\nKiritchenko S, Mohammad SM (2018) Examining Gender and Race Bias in Two Hundred Sentiment Analysis \nSystems. Proc 7th Joint Conf Lexical and Comput Semant. https://doi.org/10.48550/arXiv.1805.04508\n1 3\n348 Page 48 of 51\nInclusive prompt engineering for large language models: a modular…\nKojima T, Gu SS, Reid M, Matsuo Y , Iwasawa Y . (2022). Large language models are zero-shot reasoners. \nArXiv, abs/2205.11916.\nKumar A, Irsoy O, Ondruska P, Iyyer M, Bradbury J, Gulrajani I, Zhong V , Paulus R, Socher R. (2015). Ask \nme anything: dynamic memory networks for natural language processing.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r \nx i v . 1 5 0 6 . 0 7 2 8 5       \nLi Z, Li X, Duan Z, Dong B, Liu N, Wang J (2023) Toward a unified framework for unsupervised complex \ntabular reasoning. 2023 IEEE 39th Int Conf Data Eng (ICDE).  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 9 / I C D E 5 5 5 1 5 . 2 0 \n2 3 . 0 0 1 3 3       \nLi J, Qiu X. (2023). MoT: Memory-of-Thought Enables ChatGPT to Self-Improve. In Proceedings of the \n2023 Conference on Empirical Methods in Natural Language Processing, pages 6354–6374, Singapore. \nAssociation for Computational Linguistics.\nLi X, Chen Z, Zhang JM, Lou Y , Li T, Sun W, Liu Y , Liu X. (2024). Benchmarking bias in large language \nmodels during role-playing. https://doi.org/10.48550/arxiv.2411.00585\nLu M, Gao F, Tang X, Chen L (2024) Analysis and prediction in SCR experiments using GPT-4 with an \neffective chain-of-thought prompting strategy. iScience 27(4):109451.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . i s c i . 2 \n0 2 4 . 1 0 9 4 5 1       \nMa K, Cheng H, Liu X, Nyberg E, Gao J. (2022). Open-domain question answering via chain of reasoning \nover heterogeneous knowledge. Findings of the Association for Computational Linguistics: EMNLP \n2022, pages 5360–5374.  h t t p s :  / / a c l  a n t h o l  o g y .  o r g / 2  0 2 2 . fi   n d i n g  s - e m  n l p . 3 9 2 . p d f\nManas K, Zwicklbauer S, Paschke A. (2024). CoT-TL: Low-resource temporal knowledge representation \nof planning instructions using chain-of-thought reasoning. https://doi.org/10.48550/arxiv.2410.16207\nMarkant D, Ruggeri A, Gureckis TM, Xu F (2016) Enhanced memory as a common effect of active learning. \nSpec Issue Memory Res: Implic Educ 10(3):142–152. https://doi.org/10.1111/mbe.12117\nMcHugh ML (2012) Interrater reliability: the kappa statistic. Biochem Med Zagreb 22(3):276–282\nMikolov T, Chen K, Corrado G, Dean J. (2013). Efficient estimation of word representations in vector space. \nProceedings of the International Conference on Learning Representations.\nNadeem M, Bethke A, Reddy S. (2021). StereoSet: measuring stereotypical bias in pretrained language mod-\nels. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the \n11th International Joint Conference on Natural Language Processing (V olume 1: Long Papers), pages \n5356–5371, Online. Association for Computational Linguistics.\nNguyen M-V , Luo L, Shiri F, Phung D, Li Y-F, Vu T-T, Haffari G. (2024). Direct evaluation of chain-of-\nthought in multi-hop reasoning with knowledge graphs. https://doi.org/10.48550/arxiv.2402.11199\nOrhan AE. (2023). Recognition, recall, and retention of few-shot memories in large language models. ArXiv, \nabs/2303.17557.\nÖstling A, Sargeant H, Xie H, Bull L, Terenin A, Jonsson L, Magnusson M, Steffek F. (2024). The Cambridge \nlaw corpus: a dataset for legal AI research university of Cambridge faculty of law research paper no. \n11/2024, Available at SSRN: https://ssrn.com/abstract=4763429 or  h t t p s : / / d o i . o r g / 1 0 . 2 1 3 9 / s s r n . 4 7 6 3 4 \n2 9       \nOuyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, Zhang C, Agarwal S, Slama K, Ray A, \nSchulman J (2022) Training language models to follow instructions with human feedback. Adv Neural \nInf Proc Syst 35:27730–27744\nPagan N, Baumann J, Elokda E, De Pasquale G, Bolognani S, Hannák A (2023). A classification of feedback \nloops and their relation to biases in automated decision-making systems. In Equity and Access in Algo-\nrithms, Mechanisms, and Optimization (EAAMO'23), October 30--November 01, 2023, Boston, MA, \nUSA. ACM, New York, NY , USA 14 Pages. https://doi.org/10.1145/3617694.3623227\nPage MJ, Moher D, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, Shamseer L, Tetzlaff JM, Akl EA, \nBrennan SE, Chou R. (2021). PRISMA 2020 explanation and elaboration: updated guidance and exem-\nplars for reporting systematic reviews., n160-n160. https://doi.org/10.1136/bmj.n160\nParé G, Kitsiou S. Chapter 9 methods for literature reviews. In: Lau F, Kuziemsky C, (eds). Handbook of \neHealth evaluation: an evidence-based approach [Internet]. Victoria (BC): University of Victoria; 2017 \nFeb 27. Available from:  h t t p s :  / / w w w  . n c b i .  n l m .  n i h . g o v / b o o k s / N B K 4 8 1 5 8 3 /\nPennington J, Socher R, Manning CD. (2014). GloVe: global vectors for word representation. Proc 2014 \nConf Empirical Methods Natural Language Proc, 1532–1543.\nPetroni F, Rocktäschel T, Lewis P, Bakhtin A, Wu Y , Miller AH, Riedel S. (2019). Language models as \nknowledge bases?. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language \nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. Association for Computational Linguistics.  h t t p s : / / d o \ni . o r g / 1 0 . 1 8 6 5 3 / v 1 / D 1 9 - 1 2 5 0       \nRaffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y , Li W, Liu PJ (2020) Exploring the \nlimits of transfer learning with a unified text-to-text transformer. J Mach Learn Res 21(140):1-67.  h t t p s \n:  / / j m l  r . o r g /  p a p e  r s / v o  l u m e 2  1 / 2 0 - 0  7 4 / 2  0 - 0 7 4 . p d f\n1 3\nPage 49 of 51 348\nM. S. Torkestani et al.\nRanaldi L, Pucci G, Ranaldi F, Ruzzetti ES, Zanzotto FM. (2023). Empowering multi-step reasoning across \nlanguages via tree-of-thoughts. arXiv preprint arXiv:2311.08097.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r x i v . 2 3 1 1 . \n0 8 0 9 7       \nReimers, Nils & Iryna Gurevych (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Net-\nworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and \nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982–\n3992, Hong Kong, China. Association for Computational Linguistics.  h t t p s : / / d o i . o r g / 1 0 . 1 8 6 5 3 / v 1 / D 1 \n9 - 1 4 1 0       \nSantos C, Lee-Thorp J, Noble I, Chang C, Uthus D. (2024). Memory Augmented language models through \nmixture of word experts. Proceedings of the 2024 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technologies (V olume 1: Long Papers), \npages 4425–4438.\nSchmidgall S, Harris C, Essien I, Olshvang D, Rahman T, Kim JW, Ziaei R, Eshraghian J, Abadir P, Chel -\nlappa R. (2024). Addressing cognitive bias in medical language models.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r x i \nv . 2 4 0 2 . 0 8 1 1 3       \nSettles, B. (2012). Active learning . Synthesis Lectures on Artificial Intelligence and Machine Learning, \nSpringer.  h t t p s :  / / d o i  . o r g / 1  0 . 2 2  0 0 / S 0  0 4 2 9 E  D 1 V 0 1 Y  2 0 1 2  0 7 A I M 0 1 8\nSprague Z, Yin F, Rodriguez JD, Jiang D, Wadhwa M, Singhal P, Zhao X, Ye X, Mahowald K, Durrett G. \n(2024). To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning.  h t t p s : / \n/ d o i . o r g / 1 0 . 4 8 5 5 0 / a r x i v . 2 4 0 9 . 1 2 1 8 3       \nTalukdar W, Biswas A (2023) Improving large language model (LLM) fidelity through context-aware ground-\ning: a systematic approach to reliability and veracity. World J Adv Eng Technol Sci 10(2):283–296.  h t t \np s :  / / d o i  . o r g / 1  0 . 3 0  5 7 4 / w  j a e t s  . 2 0 2 3 .  1 0 . 2  . 0 3 1 7\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017) Attention \nis all you need. Adv Neural Inf Process Syst 30:5998–6008\nV osniadou S, Ortony A (eds) (1989) Similarity and analogical reasoning. Cambridge University Press, \nCambridge\nWang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S, Chowdhery A, Zhou D. (2022). Self-consistency \nimproves chain of thought reasoning in language models. https://doi.org/10.48550/arxiv.2203.11171\nWang K, Duan F, Wang S, Li P, Xian Y , Yin C, Rong W, Xiong Z. (2023a). Knowledge-driven CoT: explor-\ning faithful reasoning in LLMs for knowledge-intensive question answering.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / \na r x i v . 2 3 0 8 . 1 3 2 5 9       \nWang W, Dong L, Cheng H, Liu X, Yan X, Gao J, Wei F. (2023b). Augmenting language models with long-\nterm memory. https://doi.org/10.48550/arxiv.2306.07174\nWang Y , Zhao S, Wang Z, Huang H, Fan M, Zhang Y , Wang Z, Wang H, Liu T. (2024). Strategic chain-of-\nthought: guiding accurate reasoning in LLMs through strategy elicitation.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r x \ni v . 2 4 0 9 . 0 3 2 7 1       \nWei J, Wang X, Schuurmans D, Bosma M, Chi E, Le Q, Zhou D (2022) Chain of thought prompting elicits \nreasoning in large language models. 36th Conf Neural Inf Proc Syst 35:24824–37\nWeston J, Chopra S, Bordes A (2015) Memory networks. 3rd Int Conf Learn Represent.  h t t p s : / / d o i . o r g / 1 0 . 4 \n8 5 5 0 / a r x i v . 1 4 1 0 . 3 9 1 6       \nWu, N., Gong, M., Shou, L., Liang, S., & Jiang, D. (2023). Large Language Models are Diverse Role-Players \nfor Summarization Evaluation. Natural Language Processing and Chinese Computing.  h t t p s : / / a r x i v . o r \ng / a b s / 2 3 0 3 . 1 5 0 7 8       \nWorld Bank. (2024). World Development Indicators (WDI).  h t t p s :   /  / d a t a c a t a l o  g .  w o r l d  b a n  k .  o  r g / s  e a r  c  h / d a t  a  s e \nt /  0 0 3 7  7  1 2 / W   o r l d - D e v e l o p  m e n t - I n d i c a t o r s\nXu, J., Lan, C., Xie, W., Chen, X., & Lu, Y . (2023). Retrieval-based Video Language Model for Efficient \nLong Video Question Answering. https://doi.org/10.48550/arxiv.2312.04931\nYang S, Ding Y , Xie B, Guo Y , Bai X, Qian J, Gao Y , Wang W, Ren J (2023) Advancing financial fore-\ncasts: a deep dive into memory attention and long-distance loss in stock price predictions. Appl Sci \n13(22):12160. https://doi.org/10.3390/app132212160\nYao S, Zhao J, Yu D, Du N, Shafran I, Narasimhan K, Cao Y (2022) ReAct: Synergizing Reasoning and Act-\ning in Language Models. https://doi.org/10.48550/arXiv.2210.03629\nYasunaga M, Chen X, Li Y , Pasupat P, Leskovec J, Liang P, Chi EH, Zhou D (2023) Large language models \nas analogical reasoners. https://doi.org/10.48550/arXiv.2310.01714\nYe Q, Fu HY , Ren X, Jia R (2023) How Predictable Are Large Language Model Capabilities? A Case Study \non BIG-bench. https://doi.org/10.48550/arxiv.2305.14947\nZhang J, Ji X, Zhao Z, Hei XS, Choo K (2023a) Ethical Considerations and Policy Implications for Large \nLanguage Models: Guiding Responsible Development and Deployment. ArXiv, abs/2308.02678.\nZhang Z, Zhang A, Li M, Zhao H, Karypis G, Smola A (2023b) Multimodal Chain-of-Thought Reasoning in \nLanguage Models. https://doi.org/10.48550/arxiv.2302.00923\n1 3\n348 Page 50 of 51\nInclusive prompt engineering for large language models: a modular…\nZhang Z, Zhang A, Li M, Smola A (2023c) Automatic Chain of Thought Prompting in Large Language \nModels. In The Eleventh International Conference on Learning Representations.  h t t p s : / / o p e n r e v i e w . n e \nt / f o r u m ? i d = 5 N T t 8 G F j U H k r       \nZhang J, Wang X, Ren W, Jiang L, Wang D,  Liu K (2024a) RATT: A Thought Structure for Coherent and \nCorrect LLM Reasoning. https://doi.org/10.48550/arxiv.2406.02746\nZhang B, Chang K, Li C, Huang D-S, Zhang Q, Si Z (2024b) Simple techniques for enhancing sentence \nembeddings in generative language models. In Advanced Intelligent Computing Technology and Appli-\ncations. Springer, Singapore, pp 52–64. https://doi.org/10.1007/978-981-97-5669-8_5\nZhou K, Yang J, Loy CC et al (2022) Learning to prompt for vision-language models. Int J Comput vis \n130:2337–2348. https://doi.org/10.1007/s11263-022-01653-1\nPublisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n1 3\nPage 51 of 51 348"
}