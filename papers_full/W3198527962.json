{
    "title": "Sentence Bottleneck Autoencoders from Transformer Language Models",
    "url": "https://openalex.org/W3198527962",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3048621856",
            "name": "Ivan Montero",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2125829533",
            "name": "Nikolaos Pappas",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2183947846",
            "name": "Noah A. Smith",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2753738274",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W3038022805",
        "https://openalex.org/W3098929340",
        "https://openalex.org/W2997195635",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3105816068",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2963223306",
        "https://openalex.org/W3085190015",
        "https://openalex.org/W4313908941",
        "https://openalex.org/W4289306372",
        "https://openalex.org/W2963918774",
        "https://openalex.org/W3125507956",
        "https://openalex.org/W2962820504",
        "https://openalex.org/W2964222296",
        "https://openalex.org/W2891348164",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2973088264",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W3005116366",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2799054028",
        "https://openalex.org/W3034782902",
        "https://openalex.org/W2947314843",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2933374552",
        "https://openalex.org/W4293568373",
        "https://openalex.org/W2994610548",
        "https://openalex.org/W2963366196",
        "https://openalex.org/W2927928207",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3135335819",
        "https://openalex.org/W2949433733",
        "https://openalex.org/W2617566453",
        "https://openalex.org/W2952468927",
        "https://openalex.org/W3216852152",
        "https://openalex.org/W3102287766",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963341956"
    ],
    "abstract": "Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems. This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each input as a vector that allows full reconstruction. Autoencoders are attractive because of their latent space structure and generative properties. We therefore explore the construction of a sentence-level autoencoder from a pretrained, frozen transformer language model. We adapt the masked language modeling objective as a generative, denoising one, while only training a sentence bottleneck and a single-layer modified transformer decoder. We demonstrate that the sentence representations discovered by our model achieve better quality than previous methods that extract representations from pretrained transformers on text similarity tasks, style transfer (an example of controlled generation), and single-sentence classification tasks in the GLUE benchmark, while using fewer parameters than large pretrained models.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1822–1831\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1822\nSentence Bottleneck Autoencoders from Transformer Language Models\nIvan Montero♣ Nikolaos Pappas♣ Noah A. Smith♣♦\n♣Paul G. Allen School of Computer Science & Engineering, University of Washington\n♦Allen Institute for Artiﬁcial Intelligence\n{ivamon,npappas,nasmith}@cs.washington.edu\nAbstract\nRepresentation learning for text via pretrain-\ning a language model on a large corpus has\nbecome a standard starting point for building\nNLP systems. This approach stands in contrast\nto autoencoders, also trained on raw text, but\nwith the objective of learning to encode each\ninput as a vector that allows full reconstruction.\nAutoencoders are attractive because of their\nlatent space structure and generative proper-\nties. We therefore explore the construction of a\nsentence-level autoencoder from a pretrained,\nfrozen transformer language model. We adapt\nthe masked language modeling objective as a\ngenerative, denoising one, while only training\na sentence bottleneck and a single-layer mod-\niﬁed transformer decoder. We demonstrate\nthat the sentence representations discovered by\nour model achieve better quality than previous\nmethods that extract representations from pre-\ntrained transformers on text similarity tasks,\nstyle transfer (an example of controlled gener-\nation), and single-sentence classiﬁcation tasks\nin the GLUE benchmark, while using fewer pa-\nrameters than large pretrained models.1\n1 Introduction\nRecent research has focused on devising new unsu-\npervised pretraining methods from unlabeled data\nthat involves some form of language modeling, pri-\nmarily autoregressive (Peters et al., 2018; Radford\net al., 2019), masked (Devlin et al., 2019; Liu et al.,\n2019; Conneau et al., 2020) and generalized (Rad-\nford et al., 2019; Brown et al., 2020; Song et al.,\n2019), with much success on downstream tasks.\nUnder the hood, most of these methods use trans-\nformers (Vaswani et al., 2017) for encoding text se-\nquences, which allows them to learn powerful con-\ntextual word representations that have been used\nwidely for building models in NLP. However, this\ndoes not hold for sentence representations derived\n1Our code is available at: https://github.com/\nivanmontero/autobot\nfrom pretrained transformer language models based\non a special token or basic pooling operations. To\nthis end, representation learning methods have been\ndesigned to better capture semantic information\nfrom pretrained transformer language models, e.g.,\nusing Siamese networks trained with a triplet loss\n(Reimers and Gurevych, 2019) or transforming the\ndesired sentence distribution to a Gaussian distri-\nbution through normalizing ﬂows (Li et al., 2020).\nExisting sentence representations directly de-\nrived from pretrained language models or learned\nby specialized methods cannot guarantee perfect\nreconstruction of the input, a property that can en-\nhance the structure of their semantic space and en-\nable their use for controlled generation tasks. For\nthe latter, a few recent studies have looked into\nways to steer generation of pretrained language\nmodels towards a particular style (Dathathri et al.,\n2020; Krause et al., 2021), although they require\nfollowing the gradient during the sampling process\nand rely on style text classiﬁers which might not\nbe always available. The latent space of a text au-\ntoencoder allows one to perform controlled text\ngeneration by directly manipulating sentence repre-\nsentations using basic numerical operations (Shen\net al., 2020a). Yet, how to convert pretrained trans-\nformer language models to autoencoders with such\nproperties still remains unexplored.\nTo ﬁll in this gap, we introduce AUTOBOT , a\nnew autoencoder model for learning sentence “bot-\ntleneck” (i.e., ﬁxed-size) representations from pre-\ntrained transformers that is useful for similarity,\ngeneration, and classiﬁcation, displayed in Fig-\nure 1. Our model has two unique components:\n(i) a transformation that uses dot product attention\nto dynamically pool semantic information from the\npretrained model’s hidden states into a sentence\nbottleneck representation, and (ii) a shallow trans-\nformer decoder that is modiﬁed to operate based on\nthe bottleneck representation. Instead of training\nour autoencoder from scratch, we directly ﬁnetune\n1823\nit using an input reconstruction objective on the\nunlabeled data on which the original pretrained\ntransformer was trained. We keep the underlying\npretrained transformer encoder ﬁxed, which makes\nit more efﬁcient than training from scratch and\nproves beneﬁcial even if we compare to pretrained\ntransformers trained for an equal number of steps.\nOur evaluation on representative sentence simi-\nlarity, classiﬁcation, and generation tasks demon-\nstrates that the resulting sentence representations\nare compact, better capture semantic similarity at\nthe sentence-level than strong sentence representa-\ntion methods (Reimers and Gurevych, 2019), and\ncan be used for controlled generation tasks. Lastly,\nour model performs almost on par with the large\nRoBERTa model (Liu et al., 2019) even though it\nonly introduces 1.6% additional parameters relative\nto the base RoBERTa model.\n2 Model: A UTOBOT\nTaking inspiration from recent research on text\nautoencoders (Bowman et al., 2016b; Shen et al.,\n2020b; Mai et al., 2020), we extend standard au-\ntoregressive text autoencoders, which have been\npredominantly based on recurrent networks, to a\ntransformer-based architecture and integrate them\nwith pretrained language models; here we focus on\nRoBERTa (Liu et al., 2019).\nAutoencoders generally follow the encoder-\ndecoder model structure to reconstruct their input\nwith the constraint that the encoder produces a sin-\ngle, ﬁxed-length hidden representationenc(x) =z:\nAE(x) =dec(enc(x)) =x′. (1)\nHere, we focus on denoising autoencoders that aim\nto reconstruct a perturbed version of the input (Vin-\ncent et al., 2010; Shen et al., 2020b), which is com-\npatible with many of the pretrained language mod-\nels that are based on masked language modeling.\nIn our experiments, we use the same masking pro-\ncedure as Devlin et al. (2019) to perturb the input.\n2.1 Encoder\nStandard approaches use encoders that reduce the\ninput to a single representation z. To use a pre-\ntrained transformer for this purpose we need to\nreduce its output hidden representations H after\nprocessing the input to a single vector. Since us-\ning the special token representation or basic pool-\ning methods have been shown sub-optimal in prior\nwork (Reimers and Gurevych, 2019), here we opt\nPretrained\ntransformer\nH\n<latexit sha1_base64=\"/Jn9KJlq8K+ZhiygKEaIcKTg9Uo=\">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclZkq6LLopssK9oFtKZk004ZmMkNyRyhD/8KNC0Xc+jfu/Bsz7Sy09UDgcM695Nzjx1IYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJRoxpsskpHu+NRwKRRvokDJO7HmNPQlb/uTu8xvP3FtRKQecBrzfkhHSgSCUbTSYy+kOPaDtD4blMpuxZ2DrBIvJ2XI0RiUvnrDiCUhV8gkNabruTH2U6pRMMlnxV5ieEzZhI5411JFQ2766TzxjJxbZUiCSNunkMzV3xspDY2Zhr6dzBKaZS8T//O6CQY3/VSoOEGu2OKjIJEEI5KdT4ZCc4ZyagllWtishI2ppgxtSUVbgrd88ippVSveZaV6f1Wu3eZ1FOAUzuACPLiGGtShAU1goOAZXuHNMc6L8+58LEbXnHznBP7A+fwBtZeQ8A==</latexit>\nz\n<latexit sha1_base64=\"2jCnVSWDUqAN7QLjcE58ioy8lcA=\">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclZkq6LLoxmUF+8C2lEx6pw3NZIYkI9Shf+HGhSJu/Rt3/o2ZdhbaeiBwOOdecu7xY8G1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZQohg0WiUi1fapRcIkNw43AdqyQhr7Alj++yfzWIyrNI3lvJjH2QjqUPOCMGis9dENqRn6QPk37pbJbcWcgy8TLSRly1Pulr+4gYkmI0jBBte54bmx6KVWGM4HTYjfRGFM2pkPsWCppiLqXzhJPyalVBiSIlH3SkJn6eyOlodaT0LeTWUK96GXif14nMcFVL+UyTgxKNv8oSAQxEcnOJwOukBkxsYQyxW1WwkZUUWZsSUVbgrd48jJpViveeaV6d1GuXed1FOAYTuAMPLiEGtxCHRrAQMIzvMKbo50X5935mI+uOPnOEfyB8/kDAaCRIg==</latexit>\nenc\n<latexit sha1_base64=\"2OohaKWID2acyijLRLiyCYFYkIM=\">AAAB8XicbVDLSsNAFJ3UV62vqks3wSK4KkkVdFl047KCfWAbymR60w6dTMLMjVhC/8KNC0Xc+jfu/BsnMQttPTBwOOde5p7jx4JrdJwvq7Syura+Ud6sbG3v7O5V9w86OkoUgzaLRKR6PtUguIQ2chTQixXQ0BfQ9afXmd99AKV5JO9wFoMX0rHkAWcUjXQ/QHjEFCSbD6s1p+7ksJeJW5AaKdAaVj8Ho4glIUhkgmrdd50YvZQq5EzAvDJINMSUTekY+oZKGoL20vziuX1ilJEdRMo8iXau/t5Iaaj1LPTNZEhxohe9TPzP6ycYXHopl3GCWar8oyARNkZ2Ft8ecQUMxcwQyhQ3t9psQhVlaEqqmBLcxcjLpNOou2f1xu15rXlV1FEmR+SYnBKXXJAmuSEt0iaMSPJEXsirpa1n6816/xktWcXOIfkD6+MbGTSRMQ==</latexit>\ndec\n<latexit sha1_base64=\"/Waep+lgmais5+kRuUenOhmbiqY=\">AAAB8XicbVBNS8NAEN34WetX1aOXxSJ4KkkV9Fj04rGC/cA2lM1m2i7dbMLuRCyh/8KLB0W8+m+8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk41hwaPZazbATMghYIGCpTQTjSwKJDQCkY3U7/1CNqIWN3jOAE/YgMl+oIztNJDF+EJsxD4pFcquxV3BrpMvJyUSY56r/TVDWOeRqCQS2ZMx3MT9DOmUXAJk2I3NZAwPmID6FiqWATGz2YXT+ipVULaj7UthXSm/p7IWGTMOApsZ8RwaBa9qfif10mxf+VnQiUpguLzRf1UUozp9H0aCg0c5dgSxrWwt1I+ZJpxtCEVbQje4svLpFmteOeV6t1FuXadx1Egx+SEnBGPXJIauSV10iCcKPJMXsmbY5wX5935mLeuOPnMEfkD5/MHCfeRJw==</latexit>\n\u0000\n<latexit sha1_base64=\"EpwadKl79nuFFVBzWqBryCC0A38=\">AAAB7HicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGCaQttKJvtpl262YTdiVBCf4MXD4p49Qd589+4bXPQ1gcDj/dmmJkXplIYdN1vZ219Y3Nru7RT3t3bPzisHB23TJJpxn2WyER3Qmq4FIr7KFDyTqo5jUPJ2+H4bua3n7g2IlGPOEl5ENOhEpFgFK3k90KOtF+pujV3DrJKvIJUoUCzX/nqDRKWxVwhk9SYruemGORUo2CST8u9zPCUsjEd8q6lisbcBPn82Ck5t8qARIm2pZDM1d8TOY2NmcSh7YwpjsyyNxP/87oZRjdBLlSaIVdssSjKJMGEzD4nA6E5QzmxhDIt7K2EjaimDG0+ZRuCt/zyKmnVa95lrf5wVW3cFnGU4BTO4AI8uIYG3EMTfGAg4Ble4c1Rzovz7nwsWtecYuYE/sD5/AHFJI6o</latexit>\nx\n<latexit sha1_base64=\"butjOCTCphwFmdoCmokPtPNKqoI=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsxUQZdFNy4r2Ae0Y8lk0jY0kwxJRi1D/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJYs60cd1vp7Cyura+UdwsbW3v7O6V9w9aWiaK0CaRXKpOgDXlTNCmYYbTTqwojgJO28H4OvPbD1RpJsWdmcTUj/BQsAEj2FjpvhdIHupJZK/0adovV9yqOwNaJl5OKpCj0S9/9UJJkogKQzjWuuu5sfFTrAwjnE5LvUTTGJMxHtKupQJHVPvpLPUUnVglRAOp7BEGzdTfGymOdBbNTkbYjPSil4n/ed3EDC79lIk4MVSQ+UODhCMjUVYBCpmixPCJJZgoZrMiMsIKE2OLKtkSvMUvL5NWreqdVWu355X6VV5HEY7gGE7Bgwuoww00oAkEFDzDK7w5j86L8+58zEcLTr5zCH/gfP4AVJqTDQ==</latexit>\nˆx\n<latexit sha1_base64=\"zspXWC3ge2hJKsz4MMo7/0Hs4k4=\">AAAB/XicbVA7T8MwGHR4lvIKj43FokJiqpKCBGMFC2OR6ENqospxnNaqY0e2gyhRxF9hYQAhVv4HG/8Gp80ALSdZPt19n3y+IGFUacf5tpaWV1bX1isb1c2t7Z1de2+/o0QqMWljwYTsBUgRRjlpa6oZ6SWSoDhgpBuMrwu/e0+kooLf6UlC/BgNOY0oRtpIA/vQGyGdeYFgoZrE5soe8nxg15y6MwVcJG5JaqBEa2B/eaHAaUy4xgwp1XedRPsZkppiRvKqlyqSIDxGQ9I3lKOYKD+bps/hiVFCGAlpDtdwqv7eyFCsimxmMkZ6pOa9QvzP66c6uvQzypNUE45nD0Upg1rAogoYUkmwZhNDEJbUZIV4hCTC2hRWNSW4819eJJ1G3T2rN27Pa82rso4KOALH4BS44AI0wQ1ogTbA4BE8g1fwZj1ZL9a79TEbXbLKnQPwB9bnD7gYlgs=</latexit>\nFigure 1: Our autoencoder consists of a pretrained\ntransformer encoder enc, a function β that compresses\nthe encoder’s ﬁnal representations H of size T ×dto\na sentence bottleneck representation z of size d, and a\ntransformer decoder dec that is trained to fully recon-\nstruct the training sentence x.\nto keep the original encoder ﬁxed and train a trans-\nformation β that will learn to compress H into a\nsingle representation z = β(H; θ), with θbeing an\nadditional set of parameters to be learned during\nﬁnetuning. We choose βto be a multi-head atten-\ntion mechanism that takes as input the keys K and\nvalues V corresponding to the ﬁnal representations\nH from the pretrained model and a query vector q\ncorresponding to a context vector u that we choose\nto be the CLS vector from the pretrained model:\nβ(H; θ) =MultiHead(q,K,V) (2)\nwhere the parameters to be learned, θ, include the\nweights that are used to transform the query, keys,\nand values which amount to 3d2 with dbeing the\ndimensionality of each head (d= 64in our experi-\nments).\n2.2 Decoder\nThe cross-attention layer in the Transformer de-\ncoder architecture by Vaswani et al. (2017) expects\nhidden representations for every token input from\nthe encoder in order for each output candidate to\nattend to each input token. In the situation where\nonly a single representation comes from the en-\ncoder, we have\nAttention(Q,z⊤WK,z⊤WV ) =z⊤WV (3)\nNote that the queries Q, which come from the pre-\nvious masked self-attention layer, are not taken into\naccount, and each step in the decoder will receive\nthe exact same z⊤WV as a result. In order to miti-\ngate this, we propose a gating method inspired by\nHochreiter and Schmidhuber (1997). Concretely,\nlet Qt be the tth query representation. Then, the tth\noutput ot of the cross-attention layer is computed\nas follows\ngt = σ(GQt + G′z); ot = gt ⊙z⊤WV (4)\n1824\nwhere σ(·) is the sigmoid activation function and\nG and G′ are the parameters of the transformation\nfor the gate. One can view the role of the gate as\ndetermining the amount of per-element information\nfrom the linear transformation of the latent repre-\nsentation to keep for the current layer and timestep.\nPreliminary experiments found this method beneﬁ-\ncial for generation.\nTraining considerations To avoid training our\nmodel from scratch, we ﬁnetune it for 100K opti-\nmization steps on a pretraining dataset using the\nbase RoBERTa model (Liu et al., 2019) on the\nencoder side and a single layer decoder side for\nefﬁciency purposes (Kasai et al., 2021). The model\nis trained using an input reconstruction loss by min-\nimizing the negative log-likelihood computed over\nthe reconstructed inputs. Note that only the param-\neters of the sentence bottleneck and the decoder are\nlearned; the encoder parameters are kept ﬁxed.\n3 Experiments\nTo assess the quality of the sentence representa-\ntions learned by our model we evaluate on sentence\nsimilarity (Section 3.2), classiﬁcation (Section 3.3),\nand generation tasks (Section 3.4).\n3.1 Settings\nDatasets Since the RoBERTa dataset is not pub-\nlicly available, we use for pretraining the exact\nsame dataset as BERT (Devlin et al., 2019), which\nis composed of BooksCorpus (Zhu et al., 2015)\nand English Wikipedia. For sentence similarity, we\nuse the Natural Language Inference (NLI) dataset\n(Bowman et al., 2015) for ﬁnetuning and evaluate\non the Semantic Textual Similarity (STS) dataset\n(Cer et al., 2017), following Conneau et al. (2017).\nFor classiﬁcation, we use mainly single-sentence\ndatasets from the GLUE benchmark (Wang et al.,\n2018), namely Stanford Sentiment Treebank (SST)\nand Corpus of Linguistic Acceptability (CoLA)\ndatasets, but we also report the average perfor-\nmance on the remaining datasets. For generation,\nwe use the Yelp reviews dataset (Shen et al., 2017).\nBaselines For sentence similarity, we compare to\nSBERT which is a competitive method for deriv-\ning informative sentence representations from pre-\ntrained language models (Reimers and Gurevych,\n2019). They obtain sentence representations by us-\ning simple pooling methods over BERT represen-\ntations such as mean and max (instead of the CLS\ntoken representation) then ﬁnetuning the whole pre-\ntrained model using Siamese networks on a com-\nbination of natural language inference data. To\ncompare with them on sentence similarity, we in-\ncorporate our model within their framework and\nfollow their settings and training/evaluation proto-\ncol (details in Appendix A.2).\nFor sentence classiﬁcation, we compare our\nmodel to RoBERTa-base and RoBERTa-large mod-\nels (Liu et al., 2019). Note that BART (Lewis et al.,\n2019) achieves similar results to RoBERTa, so a\nsimilar comparison can be made.\nFor sentence generation tasks, we compare to a\nstrong and efﬁcient style transfer method by Shen\net al. (2020b), which is a recurrent network-based\ndenoising text autoencoder on in domain data. The\nstyle transfer is achieved through vector arithmetic,\nnamely computing a “sentiment vector”v by tak-\ning the vector difference between 100 negative and\npositive sentences, then evaluating by taking an\ninput sentence, encoding it, adding a multiple of\nthe sentiment vector to the sentence representation,\nthen decoding the resulting representation. In addi-\ntion to the denoising auto encoder (DAE) of Shen\net al. (2020b), we include more sophisticated meth-\nods for style transfer that are more computationally\nexpensive such as fast gradient iterative modiﬁca-\ntion (FGIM) of Wang et al. (2019) and Emb2Emb\nof Mai et al. (2020) for reference.\n3.2 Sentence Similarity\nThe results on the sentence similarity task are dis-\nplayed in Table 1. Due to resource constraints\nand unreported results by prior work, we report\nour model only with RoBERTa-base. We can ob-\nserve that AUTOBOT applied to RoBERTa-base\nsigniﬁcantly outperforms other supervised base\ntransformer methods. Additionally, AUTOBOT ap-\nproaches the performance of large transformers\nwhile having a minimal parameter overhead of\n1.6%.\nWe also ﬁnd that AUTOBOT without any super-\nvision (AUTOBOT -base unsup.) outperforms all of\nthe unsupervised methods, and most notably im-\nproves upon average BERT embeddings by 26.1%.\nThis demonstrates that our approach is effective in\nboth supervised and unsupervised settings.\nWe ﬁnd in Table 2 that using the proposed sen-\ntence bottleneck based on learned context provides\nnoticeable gains over using simpler pooling meth-\nods from prior work. We suspect this is due to the\n1825\nModel Spearman Parameters\nUnsupervised\nAvg. GloVe embeddings 58.02 -\nAvg. BERT embeddings 46.35 -\nAUTOBOT -base unsup. 58.49 -\nSupervised\nInferSent - GloVe 68.03 -\nUniversal Sentence Encoder 74.92 -\nRoBERTa-base 75.37 125M\nSRoBERTa-base 76.89 125M\nAUTOBOT -base (ours) 78.59 127M\nRoBERTa-large 80.16 355M\nTable 1: On semantic textual similarity (STS), A U-\nTOBOT outperforms previous sentence representation\nmethods and reaches a score similar to RoBERTa-large\nwhile having fewer parameters. We report Spearman’s\nrank correlation on the test set and the model sizes are\nreported in terms of trained parameter size.\nPooling Spearman\nMEAN 80.78\nMAX 78.76\nCLS 79.67\nβ (ours) 81.88\nTable 2: Performance of sentence representations from\nRoBERTa trained with different pooling methods on\nNLI data and then evaluated on STS benchmark’s de-\nvelopment set in terms of Spearman’s rank correlation.\nadditional ﬂexibility provided by our bottleneck\nacting as “weighted pooling” by attending over all\ntokens to compute the ﬁnal representation, as op-\nposed to equal contribution of all tokens regardless\nof the input.\n3.3 Sentence Classiﬁcation\nThe results on single-sentence classiﬁcation tasks\nand other tasks from the GLUE benchmark are\ndisplayed in Table 3. We ﬁnd that AUTOBOT pro-\nvides a noticable performance increase on single-\nsentence tasks, speciﬁcally on the CoLA datasets\nwhen using both the RoBERTa-base and RoBERTa-\nlarge models. Additionally, we also ﬁnd that AU-\nTOBOT , when fed both sentences concatenated for\ndual sentence GLUE tasks, maintains the original\nperformance of the underlying pretrained encoder.\nHence, our model improves the quality of the\nsentence representations from pretrained trans-\nformer models without hurting their performance.\nModel SST CoLA Others (avg)\nRoBERTa-base 94.8 63.6 88.7\nAUTOBOT -base 95.0 66.0 88.7\nRoBERTa-large 96.4 68.0 91.1\nAUTOBOT -large 96.9 70.2 91.1\nTable 3: Single-sentence GLUE classiﬁcation dev. re-\nsults. Median accuracy is reported over over three\nrandom seeds. Our model improves performance on\nsingle-sentence classiﬁcation tasks over both base and\nlarge RoBERTa models while maintaining their perfor-\nmance on the remaining multi-sentence tasks.\n3.4 Sentence Generation\nFor sentence generation, we focus on the senti-\nment transfer task proposed by Shen et al. (2020b)\nboth with and without further training on in-domain\ndata from Yelp. When ﬁnetuning, we perform an\nadditional 10K optimization steps using the Yelp\ndataset. Note that all the baselines require train-\ning on in-domain data, while this is optional for\nour model. In Figure 2 we ﬁnd that the AUTO -\nBOT model not exposed to the Yelp dataset during\nﬁnetuning performed on par with the DAE that\nwas trained speciﬁcally on Yelp. Additionally, AU-\nTOBOT outperforms the DAE in the above-40 per-\ncent accuracy range when ﬁnetuned on in-domain\ndata. We include AUTOBOT results with partial ﬁne-\ntuning of the encoder in the appendix, which we\nﬁnd considerably improves the Self-BLEU metric.\nSince AUTOBOT uses vector arithmetic, infer-\nence is as fast as the DAE and over twice that of\nother methods.\nFigure 2: Automatic evaluations of vector arithmetic\nfor sentiment transfer, plotted as accuracy vs. self-\nBLEU. Accuracy (ACC) is measured by a sentiment\nclassiﬁer, and values for varying multiples of the senti-\nment vector are plotted. Upper right is better.\n1826\n4 Related Work\nReconstructing text with autoencoders is an ac-\ntive area of research that has lead to several ad-\nvancements such as denoising (Vincent et al.,\n2010), variational (Kingma and Welling, 2014;\nHiggins et al., 2017; Dai and Wipf, 2019), adver-\nsarial (Makhzani et al., 2016; Zhao et al., 2018),\nand regularized (Ghosh et al., 2020) autoencoders.\nThey have been found especially useful in con-\ntrolled text generation (Hu et al., 2017; Logeswaran\net al., 2018; Bowman et al., 2016a), especially in\nsentiment style transfer (Mai et al., 2020; Shen\net al., 2017).\nThe encoder-decoder structure for obtaining rep-\nresentations has been used in pretraining (Lewis\net al., 2019), sentence inﬁlling (Huang et al., 2020),\nand multilingual (Artetxe and Schwenk, 2019) sce-\nnarios. In particular, Lewis et al. (2019) treat de-\nnoising as translation task to perform pretraining\nfrom scratch, but their approach does not induce\na sentence representation space with generative\nproperties. In contrast, our method makes use of\na frozen pretrained transformer to learn a shallow,\nsentence bottleneck autoencoder on top.\n5 Conclusion\nWe proposed an approach that converts a pretrained\ntransformer language model into a sentence-level\nautoencoder that is able to reconstruct its pretrain-\ning data. The resulting model improves the perfor-\nmance of the pretrained model on sentence-level\ntasks while maintaining its performance on multi-\nsentence tasks. In addition, the new sentence repre-\nsentations are suitable for efﬁcient conditional text\ngeneration such as sentiment transfer without the\nneed for training on in-domain data.\nAcknowledgments\nThe authors thank Jungo Kasai and the anonymous\nreviewers for their helpful feedback. Nikolaos Pap-\npas was supported by the Swiss National Science\nFoundation grant P400P2_183911.\nReferences\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nSamuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew\nDai, Rafal Jozefowicz, and Samy Bengio. 2016a.\nGenerating sentences from a continuous space. In\nProceedings of The 20th SIGNLL Conference on\nComputational Natural Language Learning, pages\n10–21.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics.\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016b. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Con-\nference on Computational Natural Language Learn-\ning, pages 10–21, Berlin, Germany. Association for\nComputational Linguistics.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Édouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In EMNLP.\nBin Dai and David Wipf. 2019. Diagnosing and en-\nhancing V AE models. In International Conference\non Learning Representations.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language mod-\nels: A simple approach to controlled text generation.\nIn International Conference on Learning Represen-\ntations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\n1827\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nPartha Ghosh, Mehdi S. M. Sajjadi, Antonio Ver-\ngari, Michael Black, and Bernhard Scholkopf. 2020.\nFrom variational to deterministic autoencoders. In\nInternational Conference on Learning Representa-\ntions.\nIrina Higgins, Loïc Matthey, Arka Pal, Christopher\nBurgess, Xavier Glorot, Matthew M Botvinick,\nShakir Mohamed, and Alexander Lerchner. 2017.\nbeta-vae: Learning basic visual concepts with a con-\nstrained variational framework. In International\nConference on Learning Representations.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P Xing. 2017. Toward con-\ntrolled generation of text. In ICML.\nYichen Huang, Yizhe Zhang, Oussama Elachqar, and\nYu Cheng. 2020. Inset: Sentence inﬁlling with\ninter-sentential transformer. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 2502–2515.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James Cross,\nand Noah Smith. 2021. Deep encoder, shallow\ndecoder: Reevaluating non-autoregressive machine\ntranslation. In International Conference on Learn-\ning Representations.\nDiederik P Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In International Confer-\nence on Learning Representations.\nBen Krause, Akhilesh Deepak Gotmare, Bryan Mc-\nCann, Nitish Shirish Keskar, Shaﬁq Joty, richard\nsocher, and Nazneen Rajani. 2021. Gedi: Genera-\ntive discriminator guided sequence generation.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nY . Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach.\nArXiv, abs/1907.11692.\nLajanugen Logeswaran, Honglak Lee, and Samy Ben-\ngio. 2018. Content preserving text generation with\nattribute controls. Advances in Neural Information\nProcessing Systems, 31.\nFlorian Mai, Nikolaos Pappas, Ivan Montero, Noah A\nSmith, and James Henderson. 2020. Plug and play\nautoencoders for conditional text generation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6076–6092.\nAlireza Makhzani, Jonathon Shlens, Navdeep Jaitly,\nand Ian Goodfellow. 2016. Adversarial autoen-\ncoders. In International Conference on Learning\nRepresentations.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. In EMNLP/IJCNLP.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In Proceedings of the 31st In-\nternational Conference on Neural Information Pro-\ncessing Systems, pages 6833–6844.\nTianxiao Shen, Jonas Mueller, Dr.Regina Barzilay, and\nTommi Jaakkola. 2020a. Educating text autoen-\ncoders: Latent representation guidance via denois-\ning. In Proceedings of the 37th International Confer-\nence on Machine Learning, volume 119 of Proceed-\nings of Machine Learning Research, pages 8719–\n8729. PMLR.\nTianxiao Shen, Jonas Mueller, Regina Barzilay, and\nTommi Jaakkola. 2020b. Educating text autoen-\ncoders: Latent representation guidance via denois-\ning. In International Conference on Machine Learn-\ning, pages 8719–8729. PMLR.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: Masked sequence to se-\nquence pre-training for language generation. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, volume 97 ofProceedings of Ma-\nchine Learning Research, pages 5926–5936. PMLR.\n1828\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie,\nYoshua Bengio, Pierre-Antoine Manzagol, and Léon\nBottou. 2010. Stacked denoising autoencoders:\nLearning useful representations in a deep network\nwith a local denoising criterion. Journal of machine\nlearning research, 11(12).\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP,\npages 353–355.\nKe Wang, Hang Hua, and Xiaojun Wan. 2019. Control-\nlable unsupervised text attribute transfer via editing\nentangled latent representation. In Advances in Neu-\nral Information Processing Systems, pages 11034–\n11044.\nJunbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexan-\nder M Rush, Yann LeCun, et al. 2018. Adversari-\nally regularized autoencoders. In Proceedings of the\n35th International Conference on Machine Learning,\nProceedings of Machine Learning Research. PMLR.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 19–\n27.\n1829\nA Reproducibility\nA.1 Experimental Setup\nComputing Infrastructure For all of our experi-\nments involving base models, we use a computation\ncluster with 5 NVIDIA RTX 2080 TI GPU, 11GB\nGPU memory, and 128GB RAM. For large models,\nwe use a computation cluster with 4 NVIDIA TI-\nTAN RTX GPUs, 24GB GPU memory and 256GB\nRAM.\nImplementation We will make our implementa-\ntion available on Github.2 We used Python 3.7, Py-\nTorch 1.6.0, and Sentence Transformers 0.3.7. We\nuse modiﬁed versions of Fairseq 0.9.0 and Trans-\nformers 3.3.1. We obtain our datasets from the\ncitations speciﬁed in the main paper.\nAUTOBOT Training We extract the sentences\nfrom the BooksCorpus and English Wikipedia\ndatasets to recreate the BERT dataset, and use\nRoBERTa-base’s pretrained tokenizer for tokeniza-\ntion. We report our hyperparameters for AUTO -\nBOT-base in Table 4. Our decoder only has one\nsingle layer, and RoBERTa-base remains ﬁxed dur-\ning ﬁnetuning.\nMODEL PARAMETERS VALUE\nFixed Parameters\nTransformer Encoder RoBERTa-base\nTransformer Encoder Fixed True\nWarmup Steps 4000\nDropout 0.1\nOptimizer Adam\nLearning Rate Schedule Linear Decay\nMax Sequence Length 128\nMax Tokens 24576\nBottleneck Heads 12\nHidden Size 768\nDecoder Layers 1\nTuned Parameters\nNum Steps {1k,10k,100k}\nLearning Rate {1e-3, 1e-4, 1e-5}\nOptimal Parameters\nNum Steps 100k\nLearning Rate 1e−3\nExtra Info\nModel Size (# params) 127M\nTable 4: Hyperparamters for AUTOBOT-base\n2https://github.com/ivanmontero/\nautobot\nA.2 Sentence Representations\nWe use the Sentence Transformers framework for\ntraining and evaluation of AUTOBOT . We use the\ndefault settings in their framework to train on NLI,\nand evaluate using the Spearman correlation of the\ncosine similarity. During NLI ﬁnetuning, we only\nuse the encoder and bottlneck, with the bottleneck\nrepresentation used as the sentence representation,\nand allow for all parameters to be ﬁnetuned.\nA.3 Sentence Generation\nWe use a modiﬁed version of Fairseq’s generation\ncode for encoder-decoder models to perform vec-\ntor arithmetic for sentiment transfer. We follow\nthe instructions of Mai et al. (2020) to ﬁnetune\na sentiment classiﬁer using DistilBERT from the\nHuggingface transformers library.\nFor the AUTOBOT models ﬁnetuned to the Yelp\ndataset, we follow the exact same steps as Ap-\npendix A.1 except beginning with the AUTOBOT -\nbase model, using the Yelp training set, and per-\nforming 10k optimization steps.\nA.4 Sentence Classiﬁcation\nWe use the Huggingface library to perform sen-\ntence classiﬁcation using AUTOBOT . During ﬁne-\ntuning, we only use the encoder and bottleneck,\nwith the bottleneck representation used as a CLS\nrepresentation, and allow for all parameters to be\nﬁnetuned. We perform a hyperparameter search\nsimilar to that of RoBERTa by comparing develop-\nment performances when using {1e-5, 2e-5, 3e-5}\nfor the learning rate.\nB Additional Results\nWe provide additional results in addition to our\nexperiments below.\nB.1 Autoencoding Steps\nWe perform an ablation study on the effect of au-\ntoencoding ﬁnetuning steps of the underlying pre-\ntrained encoder during autoencoding on the down-\nstream sentence representation performance. We\nprovide the detailed performances of performing\nTable 4 when using a learning rate of 1e-3 in Ta-\nble 5.\nB.2 Finetunable Encoder Layers\nWe perform an ablation study on the effect of ﬁne-\ntuning the underlying pretrained encoder during\n1830\nTraining Steps Spearman\n1 74.38\n1k 75.45\n10k 78.01\n100k 78.59\nbaseline 77.03\nTable 5: AUTOBOT pretraining steps vs. sentence repre-\nsentation performance when training on NLI and eval-\nuating on STS\nautoencoding on downstream sentence represen-\ntation performance. We provide the detailed per-\nformances of performing Table 4 with the optimal\nparameters, but varying how many of the last lay-\ners of RoBERTa-base to ﬁnetune. Results are in\nTable 6\nFinetunable Layers Spearman\nNone 78.59\n1 77.24\n2 76.17\n3 76.20\nbaseline 77.03\nTable 6: A UTOBOT ﬁnetunable layers vs. sentence\nrepresentation performance when training on NLI and\nevaluating on STS\nB.3 Finetunable Encoder Generation\nWe provide an appended generation table from Sec-\ntion 3.4 to include the generation results we ob-\ntained by allowing the top three layers of RoBERTa-\nbase to be ﬁnetuned during autoencoding on the\nstyle generation task. The results are shown in Fig-\nure 3. The same model as used in Appendix B.2 is\nused.\nB.4 Style Transfer Results\nWe provide Table 7 that reports results on the Yelp\nsentiment transfer test set from the generation table\nin Section 3.4, appending to the table (Mai et al.,\n2020). We outline the relative time differences dur-\ning inference. We can observe that our model not\nonly provides competitive speed-quality tradeoff.\nB.5 Detailed Sentence Classiﬁcation Results\nSection 3.3 provides a summary of the GLUE re-\nsults, while outlining the speciﬁc single-sentence\nclassiﬁcation performances. We provide the results\nfor each task in Table 8\nFigure 3: Automatic evaluations of vector arithmetic\nfor sentiment transfer, plotted as accuracy vs. self-\nBLEU. Accuracy is measured by a sentiment classiﬁer,\nand values for varying multiples of the sentiment vector\nare plotted. Upper right is better.\nModel Acc. BLEU +Time\nFGIM 94.9 10.8 70.0×\nMai et al. 2020 + FGIM 93.1 18.1 2820.0×\nMai et al. 2020 87.1 22.1 1.0×\nShen et al. (2019) 96.8 6.5 0.5×\nAUTOBOT-base (ours) 95.6 11.90 0.5×\nTable 7: Self BLEU on the Yelp sentiment transfer test\nset with highest transfer accuracy (“Acc.”). “+Time”\nreports the inference-time slowdown factor due to each\nmethod’s additional computation relative to the method\nby Mai et al. (2020).\n1831\nMNLI QNLI QQP RTE SST MRPC CoLA STS\nRoBERTa-base 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2\nAUTOBOT-base 88.0 92.7 91.9 79.5 95.0 88.4 66.0 91.4\nRoBERTa-large 90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4\nAUTOBOT-large 90.5 94.6 92.2 87.6 96.9 89.0 70.2 92.4\nTable 8: Dev. results on GLUE. For RTE, STS and MRPC we ﬁnetune starting from the MNLI model instead of\nthe baseline pretrained model."
}