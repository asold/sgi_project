{
    "title": "Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction",
    "url": "https://openalex.org/W2949568611",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2557386450",
            "name": "Christoph Alt",
            "affiliations": [
                "German Research Centre for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2288272137",
            "name": "Marc Hübner",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098634961",
            "name": "Leonhard Hennig",
            "affiliations": [
                "German Research Centre for Artificial Intelligence"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6813275036",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2609569121",
        "https://openalex.org/W2158139315",
        "https://openalex.org/W2964217331",
        "https://openalex.org/W2892316911",
        "https://openalex.org/W2138627627",
        "https://openalex.org/W2964317478",
        "https://openalex.org/W1838058638",
        "https://openalex.org/W1604644367",
        "https://openalex.org/W2606901057",
        "https://openalex.org/W2120143966",
        "https://openalex.org/W2760600531",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W174427690",
        "https://openalex.org/W2759996146",
        "https://openalex.org/W2251135946",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2963021258",
        "https://openalex.org/W2119465010",
        "https://openalex.org/W2963907426",
        "https://openalex.org/W2107598941",
        "https://openalex.org/W2963653592",
        "https://openalex.org/W2964173876",
        "https://openalex.org/W2891417293",
        "https://openalex.org/W2963561757",
        "https://openalex.org/W2515462165",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2949433733",
        "https://openalex.org/W2251960799",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2149557440",
        "https://openalex.org/W2963045354",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2132679783",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W1889268436",
        "https://openalex.org/W2890021306",
        "https://openalex.org/W1551842868"
    ],
    "abstract": "Distantly supervised relation extraction is widely used to extract relational facts from text, but suffers from noisy labels. Current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification. While achieving state-of-the-art results, we observed these models to be biased towards recognizing a limited set of relations with high precision, while ignoring those in the long tail. To address this gap, we utilize a pre-trained language model, the OpenAI Generative Pre-trained Transformer (GPT) [Radford et al., 2018]. The GPT and similar models have been shown to capture semantic and syntactic features, and also a notable amount of \"common-sense\" knowledge, which we hypothesize are important features for recognizing a more diverse set of relations. By extending the GPT to the distantly supervised setting, and fine-tuning it on the NYT10 dataset, we show that it predicts a larger set of distinct relation types with high confidence. Manual and automated evaluation of our model shows that it achieves a state-of-the-art AUC score of 0.422 on the NYT10 dataset, and performs especially well at higher recall levels.",
    "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1388–1398\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n1388\nFine-tuning Pre-Trained Transformer Language Models to Distantly\nSupervised Relation Extraction\nChristoph Alt Marc H ¨ubner Leonhard Hennig\nGerman Research Center for Artiﬁcial Intelligence (DFKI)\nSpeech and Language Technology Lab\n{christoph.alt, marc.huebner, leonhard.hennig}@dfki.de\nAbstract\nDistantly supervised relation extraction is\nwidely used to extract relational facts from\ntext, but suffers from noisy labels. Current re-\nlation extraction methods try to alleviate the\nnoise by multi-instance learning and by pro-\nviding supporting linguistic and contextual in-\nformation to more efﬁciently guide the relation\nclassiﬁcation. While achieving state-of-the-art\nresults, we observed these models to be biased\ntowards recognizing a limited set of relations\nwith high precision, while ignoring those in\nthe long tail. To address this gap, we utilize a\npre-trained language model, the OpenAI Gen-\nerative Pre-trained Transformer (GPT) (Rad-\nford et al., 2018). The GPT and similar mod-\nels have been shown to capture semantic and\nsyntactic features, and also a notable amount\nof “common-sense” knowledge, which we hy-\npothesize are important features for recogniz-\ning a more diverse set of relations. By extend-\ning the GPT to the distantly supervised set-\nting, and ﬁne-tuning it on the NYT10 dataset,\nwe show that it predicts a larger set of distinct\nrelation types with high conﬁdence. Manual\nand automated evaluation of our model shows\nthat it achieves a state-of-the-art AUC score of\n0.422 on the NYT10 dataset, and performs es-\npecially well at higher recall levels.\n1 Introduction\nRelation extraction (RE), deﬁned as the task\nof identifying the relationship between concepts\nmentioned in text, is a key component of many\nnatural language processing applications, such\nas knowledge base population (Ji and Grishman,\n2011) and question answering (Yu et al., 2017).\nDistant supervision (Mintz et al., 2009; Hoffmann\net al., 2011) is a popular approach to heuristically\ngenerate labeled data for training RE systems by\naligning entity tuples in text with known relation\ninstances from a knowledge base, but suffers from\nFigure 1: Distant supervision generates noisily labeled\nrelation mentions by aligning entity tuples in a text cor-\npus with relation instances from a knowledge base.\nnoisy labels and incomplete knowledge base in-\nformation (Min et al., 2013; Fan et al., 2014). Fig-\nure 1 shows an example of three sentences labeled\nwith an existing KB relation, two of which are\nfalse positives and do not actually express the re-\nlation.\nCurrent state-of-the-art RE methods try to ad-\ndress these challenges by applying multi-instance\nlearning methods (Mintz et al., 2009; Surdeanu\net al., 2012; Lin et al., 2016) and guiding the\nmodel by explicitly provided semantic and syn-\ntactic knowledge, e.g. part-of-speech tags (Zeng\net al., 2014) and dependency parse informa-\ntion (Surdeanu et al., 2012; Zhang et al., 2018b).\nRecent methods also utilize side information,\ne.g. paraphrases, relation aliases, and entity\ntypes (Vashishth et al., 2018). However, we ob-\nserve that these models are often biased towards\nrecognizing a limited set of relations with high\nprecision, while ignoring those in the long tail (see\nSection 5.2).\nDeep language representations, e.g. those\nlearned by the Transformer (Vaswani et al., 2017)\nvia language modeling (Radford et al., 2018),\nhave been shown to implicitly capture useful se-\nmantic and syntactic properties of text solely by\n1389\nunsupervised pre-training (Peters et al., 2018),\nas demonstrated by state-of-the-art performance\non a wide range of natural language processing\ntasks (Vaswani et al., 2017; Peters et al., 2018;\nRadford et al., 2018; Devlin et al., 2018), in-\ncluding supervised relation extraction (Alt et al.,\n2019). Radford et al. (2019) even found lan-\nguage models to perform fairly well on answer-\ning open-domain questions without being trained\non the actual task, suggesting they capture a lim-\nited amount of “common-sense” knowledge. We\nhypothesize that pre-trained language models pro-\nvide a stronger signal for distant supervision, bet-\nter guiding relation extraction based on the knowl-\nedge acquired during unsupervised pre-training.\nReplacing explicit linguistic and side-information\nwith implicit features improves domain and lan-\nguage independence and could increase the diver-\nsity of the recognized relations.\nIn this paper, we introduce a Distantly Super-\nvised Transformer for Relation Extraction (DIS-\nTRE). We extend the standard Transformer archi-\ntecture by a selective attention mechanism to han-\ndle multi-instance learning and prediction, which\nallows us to ﬁne-tune the pre-trained Transformer\nlanguage model directly on the distantly super-\nvised RE task. This minimizes explicit feature\nextraction and reduces the risk of error accumu-\nlation. In addition, the self-attentive architec-\nture allows the model to efﬁciently capture long-\nrange dependencies and the language model to\nutilize knowledge about the relation between en-\ntities and concepts acquired during unsupervised\npre-training. Our model achieves a state-of-the-art\nAUC score of 0.422 on the NYT10 dataset, and\nperforms especially well at higher recall levels,\nwhen compared to competitive baseline models.\nWe selected the GPT as our language model be-\ncause of its ﬁne-tuning efﬁciency and reasonable\nhardware requirements, compared to e.g. LSTM-\nbased language models (Ruder and Howard, 2018;\nPeters et al., 2018) or BERT (Devlin et al., 2018).\nThe contributions of this paper can be summarized\nas follows:\n•We extend the GPT to handle bag-level,\nmulti-instance training and prediction for dis-\ntantly supervised datasets, by aggregating\nsentence-level information with selective at-\ntention to produce bag-level predictions (§3).\n•We evaluate our ﬁne-tuned language model\non the NYT10 dataset and show that it\nachieves a state-of-the-art AUC compared\nto RESIDE (Vashishth et al., 2018) and\nPCNN+ATT (Lin et al., 2016) in held-out\nevaluation (§4, §5.1).\n•We follow up on these results with a manual\nevaluation of ranked predictions, demonstrat-\ning that our model predicts a more diverse set\nof relations and performs especially well at\nhigher recall levels (§5.2).\n•We make our code publicly available\nat https://github.com/DFKI-NLP/\nDISTRE.\n2 Transformer Language Model\nThis section reviews the Transformer language\nmodel as introduced by Radford et al. (2018). We\nﬁrst deﬁne the Transformer-Decoder (Section 2.1),\nfollowed by an introduction on how contextual-\nized representations are learned with a language\nmodeling objective (Section 2.2).\n2.1 Transformer-Decoder\nThe Transformer-Decoder (Liu et al., 2018a),\nshown in Figure 2, is a decoder-only variant of the\noriginal Transformer (Vaswani et al., 2017). Like\nthe original Transformer, the model repeatedly en-\ncodes the given input representations over mul-\ntiple layers (i.e., Transformer blocks), consisting\nof masked multi-head self-attention followed by\na position-wise feedforward operation. In contrast\nto the original decoder blocks this version contains\nno form of unmasked self-attention since there are\nno encoder blocks. This is formalized as follows:\nh0 = TWe + Wp\nhl = tf block(hl−1) ∀l ∈ [1,L] (1)\nWhere T is a matrix of one-hot row vectors of the\ntoken indices in the sentence, We is the token em-\nbedding matrix, Wp is the positional embedding\nmatrix, L is the number of Transformer blocks,\nand hl is the state at layer l. Since the Trans-\nformer has no implicit notion of token positions,\nthe ﬁrst layer adds a learned positional embedding\nep ∈Rd to each token embedding ep\nt ∈Rd at po-\nsition p in the input sequence. The self-attentive\narchitecture allows an output state hp\nl of a block to\nbe informed by all input states hl−1, which is key\nto efﬁciently model long-range dependencies. For\nlanguage modeling, however, self-attention must\nbe constrained (masked) not to attend to positions\n1390\nNext Token \nPrediction\nRelation \nClassifier\nLayer-Norm\nFeed Forward\nInput Embeddings \n(h0)\nLayer-Norm\nMasked Multi\nSelf-Attention\nL x\nTransformer \nBlock\n...\nSelective \nAttention\nsnsis1\nSentence\n Represent.\nBag...\n0.1 0.3 0.2... ... α\nFigure 2: Transformer-Block architecture and training\nobjectives. A Transformer-Block is applied at each of\nthe Llayers to produce states h1 to hL. After encoding\neach sentence in a bag into its representation si, selec-\ntive attention informs the relation classiﬁer with a rep-\nresentation aggregated over all sentences [s1,...,s n].\nahead of the current token. For a more exhaus-\ntive description of the architecture, we refer read-\ners to Vaswani et al. (2017) and the excellent guide\n“The Annotated Transformer”.1\n2.2 Unsupervised Pre-training of Language\nRepresentations\nGiven a corpus C= {c1,...,c n}of tokens ci, the\nlanguage modeling objective maximizes the like-\nlihood\nL1(C) =\n∑\ni\nlog P(ci|ci−1,...,c i−k; θ), (2)\nwhere k is the context window considered for pre-\ndicting the next tokenci via the conditional proba-\nbility P. The distribution over the target tokens is\nmodeled using the previously deﬁned Transformer\nmodel as follows:\nP(c) =softmax(hLWT\ne ), (3)\nwhere hL is the sequence of states after the ﬁnal\nlayer L, We is the embedding matrix, andθare the\nmodel parameters that are optimized by stochastic\ngradient descent. This results in a probability dis-\ntribution for each token in the input sequence.\n3 Multi-Instance Learning with the\nTransformer\nThis section introduces our extension to the orig-\ninal transformer architecture, enabling bag-level\n1http://nlp.seas.harvard.edu/2018/04/\n03/attention.html\nmulti-instance learning on distantly supervised\ndatasets (Section 3.1), followed by a description\nof our task-speciﬁc input representation for rela-\ntion extraction (Section 3.2).\n3.1 Distantly Supervised Fine-tuning on\nRelation Extraction\nAfter pre-training with the objective in Eq. 2,\nthe language model is ﬁne-tuned on the relation\nextraction task. We assume a labeled dataset\nD = {(xi,headi,taili,ri)}N\ni=1, where each ex-\nample consists of an input sequence of tokens\nxi = [x1,...,x m], the positions headi and taili\nof the relation’s head and tail entity in the se-\nquence of tokens, and the corresponding relation\nlabel ri, assigned by distant supervision. Due to\nits noisy annotation, label ri is an unreliable tar-\nget for training. Instead, the relation classiﬁcation\nis applied on a bag level, representing each entity\npair (head,tail) as a set S = {x1,...,x n}con-\nsisting of all sentences that contain the entity pair.\nA set representationsis then derived as a weighted\nsum over the individual sentence representations:\ns=\n∑\ni\nαisi, (4)\nwhere αi is the weight assigned to the correspond-\ning sentence representation si. A sentence rep-\nresentation is obtained by feeding the token se-\nquence xi of a sentence to the pre-trained model\nand using the last state hm\nL of the ﬁnal state repre-\nsentation hL as its representation si. The set rep-\nresentation s is then used to inform the relation\nclassiﬁer.\nWe use selective attention (Lin et al., 2016),\nshown in Figure 2, as our approach for aggregat-\ning a bag-level representation sbased on the indi-\nvidual sentence representations si. Compared to\naverage selection, where each sentence represen-\ntation contributes equally to the bag-level repre-\nsentation, selective attention learns to identify the\nsentences with features most clearly expressing a\nrelation, while de-emphasizing those that contain\nnoise. The weight αi is obtained for each sentence\nby comparing its representation against a learned\nrelation representation r:\nαi = exp(sir)∑n\nj=1 exp(sjr) (5)\nTo compute the output distributionP(l) over re-\nlation labels, a linear layer followed by a softmax\n1391\nis applied to s:\nP(l|S,θ) =softmax(Wrs+ b), (6)\nwhere Wr is the representation matrix of relations\nrand b∈Rdr is a bias vector. During ﬁne-tuning\nwe want to optimize the following objective:\nL2(D) =\n|S|∑\ni=1\nlog P(li|Si,θ) (7)\nAccording to Radford et al. (2018), introducing\nlanguage modeling as an auxiliary objective dur-\ning ﬁne-tuning improves generalization and leads\nto faster convergence. Therefore, our ﬁnal objec-\ntive combines Eq. 2 and Eq. 7:\nL(D) =λ∗L1(D) +L2(D), (8)\nwhere the scalar value λis the weight of the lan-\nguage model objective during ﬁne-tuning.\n3.2 Input Representation\nOur input representation (see Figure 3) encodes\neach sentence as a sequence of tokens. To make\nuse of sub-word information, we tokenize the in-\nput text using byte pair encoding (BPE) (Sennrich\net al., 2016). The BPE algorithm creates a vocabu-\nlary of sub-word tokens, starting with single char-\nacters. Then, the algorithm iteratively merges the\nmost frequently co-occurring tokens into a new to-\nken until a predeﬁned vocabulary size is reached.\nFor each token, we obtain its input representation\nby summing over the corresponding token embed-\nding and positional embedding.\nWhile the model is pre-trained on plain text sen-\ntences, relation extraction requires a structured in-\nput, namely a sentence and relation arguments. To\navoid task-speciﬁc changes to the architecture, we\nadopt a traversal-style approach similar to Radford\net al. (2018). The structured, task-speciﬁc input is\nconverted to an ordered sequence to be directly fed\nto the model without architectural changes. Fig-\nure 3 provides a visual illustration of the input for-\nmat. It starts with the tokens of the head and tail\nentity, separated by delimiters, followed by the to-\nken sequence of the sentence containing the en-\ntity pair, and ends with a special classiﬁcation to-\nken. The classiﬁcation token signals the model\nto generate a sentence representation for relation\nclassiﬁcation. Since our model processes the in-\nput left-to-right, we add the relation arguments to\nthe beginning, to bias the attention mechanism to-\nwards their token representation while processing\nthe sentence’s token sequence.\nest\ne5\neche\ne4\ne[sep]\ne3\nekey\ne2\ne[strt]\ne1\n+\nh0\nh1\ne[sep]\ne6\nbyte pair emb.\npositional \nemb.\n+ + + + +\n...\n...\n[strt] key [sep] chest [sep] The key ... chest [clf]\nFigure 3: Relation extraction requires a structured input\nfor ﬁne-tuning, with special delimiters to assign differ-\nent meanings to parts of the input. The input embed-\nding h0 is created by summing over the positional em-\nbedding and the byte pair embedding for each token.\nStates hl are obtained by self-attending over the states\nof the previous layer hl−1.\n4 Experiment Setup\nIn the following section we describe our ex-\nperimental setup. We run our experiments\non the distantly supervised NYT10 dataset and\nuse PCNN+ATTN (Lin et al., 2016) and RE-\nSIDE (Vashishth et al., 2018) as the state-of-the-\nart baselines.\nThe piecewise convolutional neural network\n(PCNN) segments each input sentence into parts\nto the left, middle, and right of the entity pair, fol-\nlowed by convolutional encoding and selective at-\ntention to inform the relation classiﬁer with a bag-\nlevel representation. RESIDE, on the other hand,\nuses a bidirectional gated recurrent unit (GRU) to\nencode the input sentence, followed by a graph\nconvolutional neural network (GCN) to encode the\nexplicitly provided dependency parse tree infor-\nmation. This is then combined with named entity\ntype information to obtain a sentence representa-\ntion that can be aggregated via selective attention\nand forwarded to the relation classiﬁer.\n4.1 NYT10 Dataset\nThe NYT10 dataset by Riedel et al. (2010) is a\nstandard benchmark for distantly supervised rela-\ntion extraction. It was generated by aligning Free-\nbase relations with the New York Times corpus,\nwith the years 2005–2006 reserved for training\nand 2007 for testing. We use the version of the\ndataset pre-processed by Lin et al. (2016), which\n1392\nis openly accessible online. 2 The training data\ncontains 522,611 sentences, 281,270 entity pairs\nand 18,252 relational facts. The test data contains\n172,448 sentences, 96,678 entity pairs and 1,950\nrelational facts. There are 53 relation types, in-\ncluding NA if no relation holds for a given sen-\ntence and entity pair. Per convention we report\nPrecision@N (precision scores for the top 100, top\n200, and top 300 extracted relation instances) and\na plot of the precision-recall curves. Since the test\ndata is also generated via distant supervision, and\ncan only provide an approximate measure of the\nperformance, we also report P@100, P@200, and\nP@300 based on a manual evaluation.\n4.2 Pre-training\nSince pre-training is computationally expensive,\nand our main goal is to show its effectiveness by\nﬁne-tuning on the distantly supervised relation ex-\ntraction task, we reuse the language model 3 pub-\nlished by Radford et al. (2018) for our experi-\nments. The model was trained on the BooksCor-\npus (Zhu et al., 2015), which contains around\n7,000 unpublished books with a total of more than\n800M words of different genres. The model con-\nsists of L = 12 decoder blocks with 12 atten-\ntion heads and 768 dimensional states, and a feed-\nforward layer of 3072 dimensional states. We\nreuse the byte-pair encoding vocabulary of this\nmodel, but extend it with task-speciﬁc tokens (e.g.,\nstart, end, delimiter).\n4.3 Hyperparameters\nDuring our experiments we found the hyperpa-\nrameters for ﬁne-tuning, reported in Radford et al.\n(2018), to be very effective. We used the Adam\noptimization scheme (Kingma and Ba, 2015) with\nβ1 = 0.9, β2 = 0.999, a batch size of 8, a learn-\ning rate of 6.25e-5, and a linear learning rate de-\ncay schedule with warm-up over 0.2% of training\nupdates. We trained the model for 3 epochs and\napplied residual and attention dropout with a rate\nof 0.1, and classiﬁer dropout with a rate of 0.2.\n5 Results\nThis section presents our experimental results. We\ncompare DISTRE to other works on the NYT10\ndataset, and show that it recognizes a more diverse\n2https://drive.google.com/file/d/\n1eSGYObt-SRLccvYCsWaHx1ldurp9eDN_\n3https://github.com/openai/\nfinetune-transformer-lm\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nRecall\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Precision\nPrecision-Recall\nDISTRE | AUC: 0.422\nRESIDE  | AUC: 0.415\nPCNN+ATT  | AUC: 0.342\nMintz  | AUC: 0.106\nFigure 4: Precision-Recall curve on the NYT dataset.\nOur method (DISTRE) shows a more balanced perfor-\nmance across relations, especially in the long tail. †\nmarks results reported by Vashishth et al. (2018). ‡in-\ndicates results we obtained with the OpenNRE4 imple-\nmentation.\nset of relations, while still achieving state-of-the-\nart AUC. Even without explicitly provided side in-\nformation and linguistic features.\n5.1 Held-out Evaluation\nTable 1 shows the results of our model on the\nheld-out dataset. DISTRE with selective atten-\ntion achieves a new state-of-the-art AUC value\nof 0.422. The precision-recall curve in Fig-\nure 4 shows that it outperforms RESIDE and\nPCNN+ATT at higher recall levels, while preci-\nsion is lower for top predicted relation instances.\nThe results of the PCNN+ATT model indicate that\nits performance is only better in the very beginning\nof the curve, but its precision drops early and only\nachieves an AUC value of 0.341. Similar, RE-\nSIDE performs better in the beginning but drops\nin precision after a recall-level of approximately\n0.25. This suggests that our method yields a more\nbalanced overall performance, which we believe is\nimportant in many real-world applications.\nTable 1 also shows detailed precision values\nmeasured at different points along the P-R curve.\nWe again can observe that while DISTRE has\nlower precision for the top 500 predicted relation\ninstances, it shows a state-of-the-art precision of\n60.2% for the top 1000 and continues to perform\nhigher for the remaining, much larger part of the\npredictions.\n4https://github.com/thunlp/OpenNRE\n1393\nSystem AUC P@100 P@200 P@300 P@500 P@1000 P@2000\nMintz† 0.107 52.3 50.2 45.0 39.7 33.6 23.4\nPCNN+ATT‡ 0.341 73.0 68.0 67.3 63.6 53.3 40.0\nRESIDE† 0.415 81.8 75.4 74.3 69.7 59.3 45.0\nDISTRE 0.422 68.0 67.0 65.3 65.0 60.2 47.9\nTable 1: Precision evaluated automatically for the top rated relation instances. †marks results reported in the\noriginal paper. ‡marks our results using the OpenNRE implementation.\n5.2 Manual Evaluation and Analysis\nSince automated evaluation on a distantly super-\nvised, held-out dataset does not reﬂect the actual\nperformance of the models given false positive la-\nbels and incomplete knowledge base information,\nwe also evaluate all models manually. This also\nallows us to gain a better understanding of the\ndifference of the models in terms of their predic-\ntions. To this end, three human annotators manu-\nally rated the top 300 predicted relation instances\nfor each model. Annotators were asked to label a\npredicted relation as correct only if it expressed a\ntrue fact at some point in time (e.g., for a /busi-\nness/person/company relationship, a person may\nhave worked for a company in the past, but not\ncurrently), and if at least one sentence clearly ex-\npressed this relation, either via a syntactic pattern\nor via an indicator phrase.\nTable 2 shows theP@100, P@200, P@300 and\naverage precision scores, averaged over all anno-\ntators. PCNN+ATT has the highest average preci-\nsion at 94.3%, 3% higher than the 91.2% of RE-\nSIDE and 5% higher than our model. However,\nwe see that this is mainly due to PCNN+ATT’s\nvery high P@100 and P@200 scores. For P@300,\nall models have very similar precision scores.\nPCNN+ATT’s scores decrease considerably, re-\nﬂecting the overall trend of its PR curve, whereas\nRESIDE’s and DISTRE’s manual precision scores\nremain at approximately the same level. Our\nmodel’s precision scores for the top rated predic-\ntions are around 2% lower than those of RESIDE,\nconﬁrming the results of the held-out evaluation.\nManual inspection of DISTRE’s output shows\nthat most errors among the top predictions arise\nfrom wrongly labeled/location/country/capital in-\nstances, which the other models do not predict\namong the top 300 relations.\nTable 3 shows the distribution over rela-\ntion types for the top 300 predictions of the\ndifferent models. We see that DISTRE’s\ntop predictions encompass 10 distinct rela-\ntion types, more than the other two mod-\nels, with /location/location/contains and /peo-\nple/person/nationality contributing 67% of the\npredictions. Compared to PCNN+ATT and RE-\nSIDE, DISTRE predicts additional relation types,\nsuch as e.g./people/person/place lived (e.g., ”Sen.\nPER, Republican/Democrat of LOC”) and /lo-\ncation/neighborhood/neighborhood of (e.g., ”the\nLOC neighborhood/area of LOC”), with high con-\nﬁdence.\nRESIDE’s top 300 predictions cover a\nsmaller range of 7 distinct relation types,\nbut also focus on /location/location/contains\nand /people/person/nationality (82% of the\nmodel’s predictions). RESIDE’s top predictions\ninclude e.g. the additional relation types /busi-\nness/company/founders (e.g., ”PER, the founder\nof ORG”) and /people/person/children (e.g.,\n”PER, the daughter/son of PER”).\nPCNN+ATT’s high-conﬁdence predic-\ntions are strongly biased towards a very\nsmall set of only four relation types. Of\nthese, /location/location/contains and /peo-\nple/person/nationality together make up 91%\nof the top 300 predictions. Manual inspection\nshows that for these relations, the PCNN+ATT\nmodel picks up on entity type signals and ba-\nsic syntactic patterns, such as ”LOC, LOC”\n(e.g., ”Berlin, Germany”) and ”LOC in LOC”\n(”Green Mountain College in Vermont”) for /lo-\ncation/location/contains, and ”PER of LOC”\n(”Stephen Harper of Canada”) for /peo-\nple/person/nationality. This suggests that the\nPCNN model ranks short and simple patterns\nhigher than more complex patterns where the\ndistance between the arguments is larger. The two\nother models, RESIDE and DISTRE, also identify\nand utilize these syntactic patterns.\nTable 4 lists some of the more challenging\nsentence-level predictions that our system cor-\n1394\nSystem P@100 P@200 P@300 Avg Prec\nPCNN+ATT 97.3 94.7 90.8 94.3\nRESIDE 91.3 91.2 91.0 91.2\nDISTRE 88.0 89.8 89.2 89.0\nTable 2: Precision evaluated manually for the top 300 relation instances, averaged across 3 human annotators.\nrelation DIS RES PCNN\nlocation/contains 168 182 214\nperson/nationality 32 65 59\nperson/company 31 26 19\nperson/place lived 22 – –\ncountry/capital 17 – –\nadmin div/country 13 12 6\nneighborhood/nbhd of 10 3 2\nlocation/team 3 – –\ncompany/founders 2 6 –\nteam/location 2 – –\nperson/children – 6 –\nTable 3: Distribution over the top 300 predicted\nrelations for each method. DISTRE achieves per-\nformance comparable to RESIDE, while predict-\ning a more diverse set of relations with high con-\nﬁdence. PCNN+ATT shows a strong focus on\ntwo relations: /location/location/contains and /peo-\nple/person/nationality.\nrectly classiﬁed.\n6 Related Work\nRelation Extraction Initial work in RE uses\nstatistical classiﬁers or kernel based methods\nin combination with discrete syntactic features,\nsuch as part-of-speech and named entities tags,\nmorphological features, and WordNet hyper-\nnyms (Mintz et al., 2009; Hendrickx et al., 2010).\nThese methods have been superseded by sequence\nbased methods, including recurrent (Socher et al.,\n2012; Zhang and Wang, 2015) and convolutional\nneural networks (Zeng et al., 2014, 2015). Conse-\nquently, discrete features have been replaced by\ndistributed representations of words and syntac-\ntic features (Turian et al., 2010; Pennington et al.,\n2014). Xu et al. (2015a,b) integrated shortest de-\npendency path (SDP) information into a LSTM-\nbased relation classiﬁcation model. Considering\nthe SDP is useful for relation classiﬁcation, be-\ncause it focuses on the action and agents in a sen-\ntence (Bunescu and Mooney, 2005; Socher et al.,\n2014). Zhang et al. (2018b) established a new\nstate-of-the-art for relation extraction on the TA-\nCRED dataset by applying a combination of prun-\ning and graph convolutions to the dependency\ntree. Recently, Verga et al. (2018) extended the\nTransformer architecture by a custom architecture\nfor supervised biomedical named entity and rela-\ntion extraction. In comparison, we ﬁne-tune pre-\ntrained language representations and only require\ndistantly supervised annotation labels.\nDistantly Supervised Relation Extraction\nEarly distantly supervised approaches (Mintz\net al., 2009) use multi-instance learning (Riedel\net al., 2010) and multi-instance multi-label\nlearning (Surdeanu et al., 2012; Hoffmann et al.,\n2011) to model the assumption that at least one\nsentence per relation instance correctly expresses\nthe relation. With the increasing popularity\nof neural networks, PCNN (Zeng et al., 2014)\nbecame the most widely used architecture, with\nextensions for multi-instance learning (Zeng\net al., 2015), selective attention (Lin et al., 2016;\nHan et al., 2018), adversarial training (Wu et al.,\n2017; Qin et al., 2018), noise models (Luo\net al., 2017), and soft labeling (Liu et al., 2017;\nWang et al., 2018). Recent work showed graph\nconvolutions (Vashishth et al., 2018) and capsule\nnetworks (Zhang et al., 2018a), previously applied\nto the supervised setting (Zhang et al., 2018b),\nto be also applicable in a distantly supervised\nsetting. In addition, linguistic and semantic\nbackground knowledge is helpful for the task, but\nthe proposed systems typically rely on explicit\nfeatures, such as dependency trees, named entity\ntypes, and relation aliases (Vashishth et al.,\n2018; Yaghoobzadeh et al., 2017), or task- and\ndomain-speciﬁc pre-training (Liu et al., 2018b;\nHe et al., 2018), whereas our method only relies\non features captured by a language model during\nunsupervised pre-training.\nLanguage Representations and Transfer\nLearning Deep language representations have\nshown to be an effective form of unsupervised\n1395\nSentence Relation\nMr. Snow asked, referring to Ayatollah Ali Khamenei, Iran’s supreme\nleader, and Mahmoud Ahmadinejad, Iran’s president.\n/people/person/nationality\nIn Oklahoma, the Democratic governor, Brad Henry, vetoed legisla-\ntion Wednesday that would ban state facilities and workers from per-\nforming abortions except to save the life of the pregnant woman.\n/people/person/place lived\nJakarta also boasts of having one of the oldest golf courses in Asia,\nRawamangun , also known as the Jakarta Golf Club.\n/location/location/contains\nCities like New York grow in their unbuilding: demolition tends to pre-\ncede development, most urgently and particularly in Lower Manhat-\ntan, where New York Citybegan.\n/location/location/contains\nTable 4: Examples of challenging relation mentions. These examples beneﬁt from the ability to capture more\ncomplex features. Relation arguments are marked in bold.\npre-training. Peters et al. (2018) introduced\nembeddings from language models (ELMo), an\napproach to learn contextualized word representa-\ntions by training a bidirectional LSTM to optimize\na disjoint bidirectional language model objective.\nTheir results show that replacing static pre-trained\nword vectors (Mikolov et al., 2013; Pennington\net al., 2014) with contextualized word represen-\ntations signiﬁcantly improves performance on\nvarious natural language processing tasks, such\nas semantic similarity, coreference resolution,\nand semantic role labeling. Ruder and Howard\n(2018) found language representations learned\nby unsupervised language modeling to signiﬁ-\ncantly improve text classiﬁcation performance,\nto prevent overﬁtting, and to increase sample\nefﬁciency. Radford et al. (2018) demonstrated\nthat general-domain pre-training and task-speciﬁc\nﬁne-tuning, which our model is based on, achieves\nstate-of-the-art results on several question an-\nswering, text classiﬁcation, textual entailment,\nand semantic similarity tasks. Devlin et al. (2018)\nfurther extended language model pre-training by\nintroducing a slot-ﬁlling objective to jointly train\na bidirectional language model. Most recently\n(Radford et al., 2019) found that considerably\nincreasing the size of language models results in\neven better generalization to downstream tasks,\nwhile still underﬁtting large text corpora.\n7 Conclusion\nWe proposed DISTRE, a Transformer which we\nextended with an attentive selection mechanism\nfor the multi-instance learning scenario, common\nin distantly supervised relation extraction. While\nDISTRE achieves a lower precision for the 300 top\nranked predictions, we observe a state-of-the-art\nAUC and an overall more balanced performance,\nespecially for higher recall values. Similarly, our\napproach predicts a larger set of distinct relation\ntypes with high conﬁdence among the top predic-\ntions. In contrast to RESIDE, which uses explic-\nitly provided side information and linguistic fea-\ntures, our approach only utilizes features implic-\nitly captured in pre-trained language representa-\ntions. This allows for an increased domain and\nlanguage independence, and an additional error re-\nduction because pre-processing can be omitted.\nIn future work, we want to further investigate\nthe extent of syntactic structure captured in deep\nlanguage language representations. Because of\nits generic architecture, DISTRE allows for inte-\ngration of additional contextual information, e.g.\nbackground knowledge about entities and rela-\ntions, which could also prove useful to further im-\nprove performance.\nAcknowledgments\nWe would like to thank the anonymous review-\ners for their comments. This research was par-\ntially supported by the German Federal Min-\nistry of Education and Research through the\nprojects DEEPLEE (01IW17001) and BBDC2\n(01IS18025E), and by the German Federal Min-\nistry of Transport and Digital Infrastructure\nthrough the project DAYSTREAM (19F2031A).\n1396\nReferences\nChristoph Alt, Marc H ¨ubner, and Leonhard Hennig.\n2019. Improving relation extraction by pre-trained\nlanguage representations. In Proceedings of the\n2019 Conference on Automated Knowledge Base\nConstruction, Amherst, Massachusetts.\nRazvan C. Bunescu and Raymond J. Mooney. 2005. A\nShortest Path Dependency Kernel for Relation Ex-\ntraction. In Proceedings of the Conference on Hu-\nman Language Technology and Empirical Methods\nin Natural Language Processing, HLT ’05, pages\n724–731. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. Computing Research Repository (CoRR),\nabs/1810.04805.\nMiao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu,\nThomas Fang Zheng, and Edward Y . Chang. 2014.\nDistant Supervision for Relation Extraction with\nMatrix Completion. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 839–\n849, Baltimore, Maryland. Association for Compu-\ntational Linguistics.\nXu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, and\nPeng Li. 2018. Hierarchical relation extraction with\ncoarse-to-ﬁne grained attention. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2236–2245. Asso-\nciation for Computational Linguistics.\nZhengqiu He, Wenliang Chen, Zhenghua Li, Meishan\nZhang, Wei Zhang, and Min Zhang. 2018. See:\nSyntax-aware entity embedding for neural relation\nextraction. In AAAI.\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva,\nPreslav Nakov, Diarmuid ´O S ´eaghdha, Sebastian\nPad´o, Marco Pennacchiotti, Lorenza Romano, and\nStan Szpakowicz. 2010. Semeval-2010 Task 8:\nMulti-Way Classiﬁcation of Semantic Relations be-\ntween Pairs of Nominals. In SemEval@ACL.\nRaphael Hoffmann, Congle Zhang, Xiao Ling,\nLuke Zettlemoyer, and Daniel S. Weld. 2011.\nKnowledge-Based Weak Supervision for Informa-\ntion Extraction of Overlapping Relations. In Pro-\nceedings of the 49th Annual Meeting of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 541–550, Portland, Ore-\ngon, USA. Association for Computational Linguis-\ntics.\nHeng Ji and Ralph Grishman. 2011. Knowledge base\npopulation: Successful approaches and challenges.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies - Volume 1, HLT ’11, pages\n1148–1158, Stroudsburg, PA, USA. Association for\nComputational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. Interna-\ntional Conference on Learning Representations ,\nabs/1412.6980.\nYankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,\nand Maosong Sun. 2016. Neural Relation Extrac-\ntion with Selective Attention over Instances. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 2124–2133, Berlin, Germany. Asso-\nciation for Computational Linguistics.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018a. Generating wikipedia by summa-\nrizing long sequences. ICLR.\nTianyi Liu, Xinsong Zhang, Wanhao Zhou, and Wei-\njia Jia. 2018b. Neural relation extraction via inner-\nsentence noise reduction and transfer learning. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages\n2195–2204. Association for Computational Linguis-\ntics.\nTianyu Liu, Kexiang Wang, Baobao Chang, and Zhi-\nfang Sui. 2017. A soft-label method for noise-\ntolerant distantly supervised relation extraction. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages\n1790–1795. Association for Computational Linguis-\ntics.\nBingfeng Luo, Yansong Feng, Zheng Wang, Zhanxing\nZhu, Songfang Huang, Rui Yan, and Dongyan Zhao.\n2017. Learning with noise: Enhance distantly su-\npervised relation extraction with dynamic transition\nmatrix. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 430–439. Associa-\ntion for Computational Linguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nBonan Min, Ralph Grishman, Li Wan, Chang Wang,\nand David Gondek. 2013. Distant Supervision for\nRelation Extraction with an Incomplete Knowledge\nBase. In Proceedings of the 2013 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 777–782, Atlanta, Georgia. Associ-\nation for Computational Linguistics.\nMike Mintz, Steven Bills, Rion Snow, and Daniel Ju-\nrafsky. 2009. Distant supervision for relation extrac-\ntion without labeled data. In ACL/IJCNLP.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In EMNLP.\n1397\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee, and\nLuke S. Zettlemoyer. 2018. Deep contextualized\nword representations. In NAACL-HLT.\nPengda Qin, Weiran XU, and William Yang Wang.\n2018. Dsgan: Generative adversarial training for\ndistant supervision relation extraction. In Proceed-\nings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 496–505. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. available as a\npreprint.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nSebastian Riedel, Limin Yao, and Andrew McCallum.\n2010. Modeling Relations and Their Mentions with-\nout Labeled Text. In Proceedings of the European\nConference on Machine Learning and Knowledge\nDiscovery in Databases (ECML PKDD ’10).\nSebastian Ruder and Jeremy Howard. 2018. Univer-\nsal language model ﬁne-tuning for text classiﬁca-\ntion. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), volume 1, pages\n1715–1725.\nRichard Socher, Brody Huval, Christopher D. Man-\nning, and Andrew Y . Ng. 2012. Semantic composi-\ntionality through recursive matrix-vector spaces. In\nEMNLP-CoNLL.\nRichard Socher, Andrej Karpathy, Quoc V . Le, Christo-\npher D. Manning, and Andrew Y . Ng. 2014.\nGrounded compositional semantics for ﬁnding and\ndescribing images with sentences. TACL, 2:207–\n218.\nMihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,\nand Christopher D. Manning. 2012. Multi-instance\nMulti-label Learning for Relation Extraction. In\nProceedings of the 2012 Joint Conference on Em-\npirical Methods in Natural Language Processing\nand Computational Natural Language Learning ,\nEMNLP-CoNLL ’12, pages 455–465, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nJoseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-\ngio. 2010. Word representations: A simple and gen-\neral method for semi-supervised learning. In ACL.\nShikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga,\nChiranjib Bhattacharyya, and Partha Talukdar. 2018.\nReside: Improving distantly-supervised neural rela-\ntion extraction using side information. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1257–1266.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nPatrick Verga, Emma Strubell, and Andrew McCallum.\n2018. Simultaneously self-attending to all mentions\nfor full-abstract biological relation extraction. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 872–884. Associa-\ntion for Computational Linguistics.\nGuanying Wang, Wen Zhang, Ruoxu Wang, Yalin\nZhou, Xi Chen, Wei Zhang, Hai Zhu, and Huajun\nChen. 2018. Label-free distant supervision for re-\nlation extraction via knowledge graph embedding.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2246–2255. Association for Computational Linguis-\ntics.\nYi Wu, David Bamman, and Stuart Russell. 2017. Ad-\nversarial training for relation extraction. InProceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 1778–1783.\nAssociation for Computational Linguistics.\nKun Xu, Yansong Feng, Songfang Huang, and\nDongyan Zhao. 2015a. Semantic relation classiﬁca-\ntion via convolutional neural networks with simple\nnegative sampling. In EMNLP.\nYan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,\nand Zhi Jin. 2015b. Classifying relations via long\nshort term memory networks along shortest depen-\ndency paths. In Proceedings of the 2015 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 1785–1794. Association for Com-\nputational Linguistics.\nYadollah Yaghoobzadeh, Heike Adel, and Hinrich\nSch¨utze. 2017. Noise mitigation for neural entity\ntyping and relation extraction. In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n1, Long Papers, pages 1183–1194. Association for\nComputational Linguistics.\nMo Yu, Wenpeng Yin, Kazi Saidul Hasan,\nC´ıcero Nogueira dos Santos, Bing Xiang, and\nBowen Zhou. 2017. Improved neural relation\ndetection for knowledge base question answering.\nIn ACL.\n1398\nDaojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.\n2015. Distant Supervision for Relation Extraction\nvia Piecewise Convolutional Neural Networks. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages\n1753–1762, Lisbon, Portugal. Association for Com-\nputational Linguistics.\nDaojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\nand Jun Zhao. 2014. Relation classiﬁcation via con-\nvolutional deep neural network. In Proceedings of\nCOLING 2014, the 25th International Conference\non Computational Linguistics: Technical Papers,\npages 2335–2344. Dublin City University and As-\nsociation for Computational Linguistics.\nDongxu Zhang and Dong Wang. 2015. Relation classi-\nﬁcation via recurrent neural network. arXiv preprint\narXiv:1508.01006.\nNingyu Zhang, Shumin Deng, Zhanling Sun, Xi Chen,\nWei Zhang, and Huajun Chen. 2018a. Attention-\nbased capsule networks with dynamic routing for re-\nlation extraction. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 986–992. Association for Com-\nputational Linguistics.\nYuhao Zhang, Peng Qi, and Christopher D. Manning.\n2018b. Graph Convolution over Pruned Depen-\ndency Trees Improves Relation Extraction. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2205–\n2215, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. 2015 IEEE International\nConference on Computer Vision (ICCV), pages 19–\n27."
}