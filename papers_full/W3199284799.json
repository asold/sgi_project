{
  "title": "GraFormer: Graph Convolution Transformer for 3D Pose Estimation",
  "url": "https://openalex.org/W3199284799",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5065037098",
      "name": "Weixi Zhao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5069782593",
      "name": "Yunjie Tian",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015317495",
      "name": "Qixiang Ye",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5110088325",
      "name": "Jianbin Jiao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5109203493",
      "name": "Weiqiang Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963601560",
    "https://openalex.org/W2522527348",
    "https://openalex.org/W2190691619",
    "https://openalex.org/W2920064193",
    "https://openalex.org/W2583372902",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2950762923",
    "https://openalex.org/W3138811691",
    "https://openalex.org/W2940457086",
    "https://openalex.org/W2468907370",
    "https://openalex.org/W2897765997",
    "https://openalex.org/W3175199633",
    "https://openalex.org/W2605973302",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3014316979",
    "https://openalex.org/W3097623574",
    "https://openalex.org/W2756050327",
    "https://openalex.org/W2736995747",
    "https://openalex.org/W2612706635",
    "https://openalex.org/W3098473649",
    "https://openalex.org/W3098612954",
    "https://openalex.org/W2766453196",
    "https://openalex.org/W2768683308",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W2963592930",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2935836316",
    "https://openalex.org/W2886970679",
    "https://openalex.org/W2953258117",
    "https://openalex.org/W3177188930",
    "https://openalex.org/W2609211631",
    "https://openalex.org/W2766703716",
    "https://openalex.org/W3151072205",
    "https://openalex.org/W2784435047",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2797184202"
  ],
  "abstract": "Exploiting relations among 2D joints plays a crucial role yet remains semi-developed in 2D-to-3D pose estimation. To alleviate this issue, we propose GraFormer, a novel transformer architecture combined with graph convolution for 3D pose estimation. The proposed GraFormer comprises two repeatedly stacked core modules, GraAttention and ChebGConv block. GraAttention enables all 2D joints to interact in global receptive field without weakening the graph structure information of joints, which introduces vital features for later modules. Unlike vanilla graph convolutions that only model the apparent relationship of joints, ChebGConv block enables 2D joints to interact in the high-order sphere, which formulates their hidden implicit relations. We empirically show the superiority of GraFormer through conducting extensive experiments across popular benchmarks. Specifically, GraFormer outperforms state of the art on Human3.6M dataset while using 18$\\%$ parameters. The code is available at https://github.com/Graformer/GraFormer .",
  "full_text": "GraFormer: Graph Convolution Transformer for 3D Pose Estimation\nWeixi Zhao∗, Yunjie Tian*, Qixiang Ye, Jianbin Jiao and Weiqiang Wang\nUniversity of Chinese Academy of Sciences, Beijing, China\n{zhaoweixi19, tianyunjie19}@mails.ucas.ac.cn, {qxye, jiaojb, wqwang}@ucas.ac.cn\nAbstract\nExploiting relations among 2D joints plays a crucial role\nyet remains semi-developed in 2D-to-3D pose estimation. To\nalleviate this issue, we propose GraFormer, a novel trans-\nformer architecture combined with graph convolution for 3D\npose estimation. The proposed GraFormer comprises two re-\npeatedly stacked core modules, GraAttention and ChebG-\nConv block. GraAttention enables all 2D joints to interact\nin global receptive ﬁeld without weakening the graph struc-\nture information of joints, which introduces vital features\nfor later modules. Unlike vanilla graph convolutions that\nonly model the apparent relationship of joints, ChebGConv\nblock enables 2D joints to interact in the high-order sphere,\nwhich formulates their hidden implicit relations. We empiri-\ncally show the superiority of GraFormer through conducting\nextensive experiments across popular benchmarks. Speciﬁ-\ncally, GraFormer outperforms state of the art on Human3.6M\ndataset while using 18% parameters. The code is available at\nhttps://github.com/Graformer/GraFormer.\nIntroduction\n3D human pose estimation has attracted much attention re-\ncent years in computer vision as its numerous practical ap-\nplications such as action recognition (Weng, Weng, and\nYuan 2017; Yan, Xiong, and Lin 2018; Li et al. 2019; Jiang\net al. 2021), virtual reality (Gan and Wang 2019; Lu and Luo\n2020), etc. The 2D-to-3D human pose estimation task takes\n2D joint coordinates as inputs and outputs the 3D pose tar-\nget directly, which remains a challenging problem because\nof the very limited knowledge contained in 2D joint coordi-\nnates. Prior works (Martinez et al. 2017; Mehta et al. 2017)\nhave shown that the 2D kinematic structure in the coordi-\nnates is vital to learn feature representations for 3D pose es-\ntimation. However, CNN-based methods are difﬁcult to di-\nrectly process these graph-structured data.\nTo conquer this difﬁculty, recent literature (Doosti et al.\n2020; Zhao et al. 2019; Liu et al. 2020; Xu and Takano 2021)\ntry to employ graph convolution networks (GCNs) to learn\nthe representation of these graph-structured data because of\ntheir capabilities of learning structure information. These\n*Contribute equally.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nGCN self-attention ChebGConv\n(a) (b) (c)\nFigure 1: (a): the normalized adjacency matrix. The adja-\ncency matrix activates the structurally close 2D joints and\nﬁnd few implicit relations. (b): the weight matrix of V in\nself-attention. The weight matrix is able to ﬁnd the implicit\nrelationship of 2D joints, however, which is based on the\ncoordinate values instead of the graph structure. As a com-\nplementary, the graph Laplacian (c) from ChebGConv block\ncan not only ﬁnd many implicit relations but also remain the\ngraph characteristics of 2D joints.\ntechniques achieve good performances but suffer from lim-\nited receptive ﬁeld when learn better representations. This\nis because they restrict the graph convolution ﬁlter to oper-\nate only on the ﬁrst-order neighbor nodes in the manner of\n(Kipf and Welling 2016) as illustrated Fig. 1 (a). This issue\ncan be alleviated by stacking multiple GCN layers, while the\nperformance may degrade due to the over-smoothing prob-\nlem. Other attempts (Zhao et al. 2019) utilize non-local\nmodules to increase the network receptive ﬁeld and (Lin,\nWang, and Liu 2021) utilize the recent popular transformer\nmodel to capture the global vision information over the en-\ntire RGB image alleviate the receptive ﬁeld problem and\nreach the state of the art.The self-attention modules of trans-\nformers facilitate the interaction among all 2D joints, and\nthus relations of them are exploited. However, as illustrated\nin Fig. 1 (b), the self-attention mechanism builds upon calcu-\nlating the similarities of these joints, which actually ignores\nor weakens the graph structure information among these 2D\njoints coordinates.\nTo address the aforementioned problems, we propose the\nGraAttention module to ﬁll in these imperfections by com-\nbining the transformer and a graph convolution ﬁlter with\na learnable adjacency matrix (referred to as LAM-Gconv in\nshort) (Kipf and Welling 2016; Doosti et al. 2020). In par-\nticular, we replace the multiple layer perceptron (MLP) of\ntransformers with LAM-Gconv, which facilitates the train-\ning process and boosts the performance. The self-attention\nmodule facilitates the interaction among 2D joints based on\nthe coordinate values. As a complementary, the LAM-Gconv\nutilizes graph structure to further boost interaction.With both\nsuperior properties of self-attention and LAM-Gconv, our\nGraAttention is able to learn robust features through captur-\ning global vision information without losing the graph struc-\nture details.\nFurthermore, we claim that graph-structured data contains\nricher joint relations than the apparent connections. For ex-\nample, the two knee joints of humans indeed exist some im-\nplicit connections, though they are not visually connected.\nWe note that self-attention is able to model this kind of re-\nlationship, yet based on the similarity calculations as noted\nabove. Therefore, we propose to adopt the second module\nof GraFormer, ChebGConv block (Defferrard, Bresson, and\nVandergheynst 2016) to conquer this difﬁculty. The pro-\nposed ChebGConv block can exchange information accord-\ning to the structural relations of nodes thus attain a larger re-\nceptive ﬁeld than the vanilla graph convolutions (Kipf and\nWelling 2016). As a result, ChebGConv enables 2D joints to\nexploit these implicit connections via formulating the inter-\nactions in a high-order sphere. As shown in Fig. 1 (c), we\nverify the effectiveness of ChebGConv with the visualiza-\ntion of implicit connections and ablation studies.\nWe demonstrate the effectiveness of our method by con-\nducting comprehensive evaluation and ablation studies on\nstandard 3D benchmarks. Experimental results show that\nour GraFormer outperforms the state of the arts on Hu-\nman3.6M (Ionescu et al. 2013) with only 0.65M parame-\nters. In particular, we achieve 13 ﬁrst-place results out of\n15 categories on Human3.6M. In addition, the superiority of\nGraFormer is also veriﬁed on ObMan (Hasson et al. 2019),\nFHAD (Garcia-Hernando et al. 2018), and GHD (Mueller\net al. 2018). The proposed method is task-independent and\nthus can be easily extended to other graph regression tasks.\nIn brief, the core contributions of our work are:\n• We claim that the relations of 2D joints are poorly ex-\nploited for 3D pose estimation up to now, and thus in-\ndepth study of the relationship between 2D joints will\nfurther improve its performance.\n• We propose the novel GraFormer which comprises two\nmodules, GraAttention and ChebGConv block. Com-\npared to prior methods, both modules aim at further ex-\nploiting relations among 2D joints for 3D pose estima-\ntion.\n• We have conducted extensive experiments on public\nbenchmarks, which show that our GraFormer outper-\nforms the state-of-the-art techniques for 3D pose estima-\ntion task with much fewer parameters.\nRelated works\n3D Pose Estimation\nOne-stage 3D pose estimation methods usually directly esti-\nmate 3D pose using image features. (Tekin et al. 2016) re-\ngresses 3D pose from a spatio-temporal volume of bounding\nboxes in the central frame. (Pavlakos et al. 2017) ﬁrst pre-\ndicts the 3D heat map and then yields the 3D pose. (Mehta\net al. 2017) utilizes transfer learning to produce multi-modal\ndata, which is fused to predict 3D pose. (Tekin, Bogo, and\nPollefeys 2019) utilize 3D YOLO (Redmon and Farhadi\n2017) model combined with the temporal information to\npredict the 3D pose of hand and object simultaneously. (Li,\nGao, and Sang 2021) groups and predicts the joint points in\na multi-tasks manner. (Lin, Wang, and Liu 2021) takes the\nfeatures extracted from the images as inputs of a transformer\nto predict 3D pose.\nDifferently, multi-stage methods ﬁrst adopt CNN net-\nworks to detect 2D joint coordinates, which then are used\nas inputs for 3D pose estimation. To yield 3D pose, (Chen\nand Ramanan 2017) proposes to match 2D coordinates with\na 3D pose database. Based on 2D coordinates, (Martinez\net al. 2017) proposes a simple and effective network archi-\ntecture using linear layers, batch normalization, dropout and\nReLU activation function to regress the 3D pose. (Simon\net al. 2017) proposes a multi-view method that estimates 3D\nHand pose by triangulating multiple 2D joints coordinates.\n(Hossain and Little 2018) considers 2D coordinate informa-\ntion as a sequence and utilizes temporal information to pre-\ndict the 3D coordinates in a sequence manner.\nTransformer-based Methods\nInstead of aggregating neighbor information in equal pro-\nportions according to the adjacency matrix in other works\n(Kipf and Welling 2016; Ge et al. 2019), GAT (Veli ˇckovi´c\net al. 2017) proposes to use self-attention to learn the\nweight of each node to aggregate neighbor information.\naGCN (Yang et al. 2018) works in the same way as GAT\nand learns the weights of neighbor joints through the self-\nattention mechanism. The difference lies in that aGCN uses\nthe different activation functions and transformation matrix.\nAlthough GAT and aGCN improve the effectiveness by ag-\ngregating neighbor joints, the limited receptive ﬁeld remains\na challenging problem. To attain the global receptive ﬁeld,\n(Zhao et al. 2019) utilizes the non-local layer (Wang et al.\n2018) to learn the relationships between 2D joints. (Lin,\nWang, and Liu 2021) modiﬁes the standard transformer en-\ncoder and adjusts the dimension of encoder layers. How-\never, such interaction ignores the graph structure as noted\nabove. Our method increases the receptive ﬁeld through\nself-attention mechanism and rediscovers graph structure by\nGCNs to effectively improve the performance for 3D pose\nestimation.\nGCN-based methods\nRecently, some works in the 3D pose estimation task have\nachieved state-of-the-art results by using graph convolu-\ntional networks, such as (Ge et al. 2019; Zhao et al. 2019;\nDoosti et al. 2020; Liu et al. 2020; Xu and Takano 2021).\nChebGConv\nLayer\nChebGConv\nLayer\n×N\n（16，2） (16，dim) (16，dim) （16，3）\nGraAttention\nChebGConv\nBlock\nMulti-head \nattention\nLAM-Gconv\nLayer\nReLU\n×2\nChebGConv\nLayer\nReLU\nChebGConv\nLayer\nReLU +\n×N\n(16，dim)\n+ +\n(16，dim)\nLayer Norm\nDropout\nLayer Norm\nDropout\nFigure 2: Framework of GraFormer. The core part is the stack of GraAttention and ChebGConv block, which boosts perfor-\nmance for 2D-to-3D pose estimation tasks by exploiting relations among 2D joints.\n(Ge et al. 2019) uses a stacked hourglass network (Newell,\nYang, and Deng 2016) to extract features from images,\nwhich are later reshaped to the graph structure. The 3D mesh\nis predicted using the extracted features by graph convolu-\ntion, which then is used to predict the 3D pose. (Zhao et al.\n2019) proposes semantic graph convolution, which learns\nthe weights among neighbor joints. Non-local modules are\nused to enhance interaction among 2D joints. (Doosti et al.\n2020) proposes modiﬁed graph pooling and unpooling op-\nerations, which make the up-sampling and down-sampling\nprocedures trainable for graph-structured data. (Xu and\nTakano 2021) proposes graph hourglass network and adopts\nSE Block (Hu, Shen, and Sun 2018) to fuse features ex-\ntracted from different layers of graph hourglass network. In\nthis paper, we use graph convolution operations combined\nwith transformer to solve the 3D pose estimation problem.\nMethod\nThe proposed framework is shown in Fig. 2. Our method\ntakes 2D joint coordinates as inputs and predicts the target\n3D poses. GraFormer is a combination of transformer and\ngraph convolution, which much further boosts the interac-\ntion of 2D joints to exploit the relations among them com-\npared to prior works (Zhao et al. 2019; Doosti et al. 2020;\nLiu et al. 2020; Xu and Takano 2021). GraFormer is formed\nby stacking GraAttentions and ChebGConv blocks, which\nare described in detail as below.\nPreliminaries\nGraph convolution operation and multi-head self-attention\nare the fundamental building elements of GraFormer. GCNs\nhave the ability to handle graph-structured data because of\nthe speciﬁc design described below. Assume that Xl ∈\nRj×Dl is the input of the l-th layer of GCN, which contains\njjoints and Xl\ni ∈RDl . Dl denotes the input dimension. The\ninitial input of GraFormer,X0 ∈Rj×2, is the 2D joint coor-\ndinates of human body or hand. The output of a GCN layer\ncan be formulated as:\nXl+1 = σ\n(\n˜D−1\n2 ˜A˜D−1\n2 XlΘ\n)\n(1)\nwhere σis the activation function,Θ ∈RDl×Dl+1 denotes a\nlearnable weight matrix, A∈Rj×j is the adjacency matrix,\n˜A = A+ I and ˜D is the diagonal node degree matrix. We\nmake ˜Alearnable for LAM-Gconv in GraAttention.\nThe gains achieved by transformer-based methods (Lin,\nWang, and Liu 2021) mainly come from the global vision\nview, which beneﬁts from the self-attention mechanism as\ndescribed below. We use the same notation of Xl ∈Rj×Dl\nas input, which is put into a MLP layer to produce a feature\nYl with the dimension of j×D, and then be cut into 3 por-\ntions, referred to as Ql, Kl and Vl respectively. Thereafter,\nXl+1 is attained by:\nXl+1 = σ(Ql ·KlT\n√\nD\n)Vl (2)\nGraAttention\nFrom this section, we start to introduce two GraFormer\ncore modules, GraAttention and ChebGConv block. As il-\nlustrated in Fig. 2, the 16 2D joint coordinates are ﬁrst pre-\nprocessed by a ChebGConv layer (which will be introduced\nbelow) followed by the stack of GraAttention and ChebG-\nConv block. Thereafter, 16 feature vectors with the dimen-\nsion of dim(referred to as feature dimension) are yielded,\nwhich then are put into GraAttention. Before the multi-head\nself-attention block in GraAttention, the input feature vec-\ntors are ﬁrst normalized by layer norm (LN), which is com-\nmonly used in transformer models (Vaswani et al. 2017).\nThe multi-head self-attention block is inherited from the\ntransformer encoder layers (Vaswani et al. 2017). Different\nfrom other applications of transformer on 3D pose estima-\ntion like (Lin, Wang, and Liu 2021), we remove the MLP\nlayer from the standard transformer. Because we observe\nthat the MLP impedes the learning procedure as shown in\nexperiments. Then, the self-attention output is regularized\nby a dropout layer (Srivastava et al. 2014). Each element\nof the output of the multi-head attention block contains all\n2D joint information because the global interaction on the\ngraph consists of all the 2D joints, such that the non-local\nrelationships can be exploited.\nNext, the output is normalized by the LN layer fol-\nlowed by GCN layers and a ReLU activation. Different from\nvanilla graph convolution operation, we make the adjacency\nmatrix to be learnable thus that the GCN layer could be more\nﬂexible to learn graph-structured data. The GCN layer is re-\nferred to as LAM-Gconv as noted above. Next, a dropout\nlayer is followed. GraAttention is a combination of multi-\nhead self-attention block and GCN layer, and both blocks\ninclude a shortcut connection as shown in Fig. 2.\nChebGConv block\nThe second graph convolution operation we used is Cheby-\nshev graph convolution (Defferrard, Bresson, and Van-\ndergheynst 2016), referred to as ChebGConv in short. Com-\npared with traditional GCN layers, ChebGConv is more\npowerful to handle with graph-structured data, which can be\nformulated as below. We ﬁrst introduce its calculation for-\nmula. The normalized graph Laplacian is computed as:\nL= I−˜D−1\n2 A˜D−1\n2 (3)\nThe formula of Chebyshev graph convolution is:\nXl+1 =\nK−1∑\nk=0\nTk\n(\n˜L\n)\nXlθk (4)\nwhere Tk (x) = 2xTk−1 (x) −Tk−2 (x) denotes the Cheby-\nshev polynomial of degree k, T0 = 1 ,T1 = x, and ˜L ∈\nRj ×j denotes the rescaled Laplacian, ˜L = 2L/λmax −I,\nλmax is the maximum eigenvalue of L. θk ∈ RDl×Dl+1\ndenotes the trainable parameters in the graph convolutional\nlayer. Since the convolution kernel is a K-order polynomial\ngraph Laplacian, ChebGConv block is able to fuse informa-\ntion among the K top neighbors of a joint, which brings\na larger receptive ﬁeld. Coupled with the characteristic of\ngraph convolution ﬁlter, ChebGConv block boosts the per-\nformance as veriﬁed in experiments.\nEven though the more expensive computation than the tra-\nditional GCN layer, ChebGConv block complexity increases\nslightly because the graph is simple with only 16 joints.\nTraining\nTo train GraFormer, we apply a loss function to the ﬁnal\noutput to minimize the error between the 3D predictions\nand ground truth. Given dataset S =\n{\nJ2d\ni ,J3d\ni\n}N\ni=1, where\nJ2d\ni ∈Rj×2 is 2D joints of the human body or hand,jis set\nto 16 for the human datasets and 21 for the hand datasets.\nJ3d\ni ∈Rj×3 is the 3D ground truth coordinates. N denotes\nthe total number of training samples.\nWe use the mean squared errors (MSE) to minimize the\nerror between the 3D predictions and ground truth coordi-\nnates.\nL= 1\nN\nN∑\ni=1\n(˜J3d\ni −J3d\ni\n\n2)\n(5)\nwhere ˜J3d\ni ∈Rj×3 denotes the predicted 3D coordinates.\nWe calculate all 3D coordinate errors in millimeters.\nExperiments\nIn this section, we ﬁrst introduce the experimental details\nand training settings. Next, we compare GraFormer with\nother state-of-the-art methods and analyze the results. Fi-\nnally, we conduct ablation studies to verify the effectiveness\nof GraFormer.\nExperimental Details\nDataset We use 3 popular hand datasets, including Ob-\nMan (Hasson et al. 2019), FHAD (Garcia-Hernando et al.\n2018), GHD (Mueller et al. 2018), and 1 human pose\ndataset Human3.6M (Ionescu et al. 2013) to evaluate our\nGraFormer. ObMan is a large synthetic dataset of hand-\nobject interaction scenarios. The hands are generated from\nMANO (Romero, Tzionas, and Black 2017) and the objects\nare selected from the Shapenet (Chang et al. 2015) dataset.\nThe ObMan dataset contains 141K training frames and 6K\nevaluation frames. Each frame contains an RGB image, a\ndepth image, a 3D mesh of the hand and object, and 3D co-\nordinates for the hand.\nFHAD (Garcia-Hernando et al. 2018) contains videos of\nmanipulating different objects from the ﬁrst-person perspec-\ntive. There are a total of 21,501 frames of images, where\n11019 frames are used for training and 10482 frames for\ntesting.\nThere are 143,449 frames without objects, and 188,050\nframes with objects in the GHD (Mueller et al. 2018)\ndataset. The dataset is divided into groups and each group\nconsists of 1024 frames. There are a total of 141 groups\nwithout objects and we use the ﬁrst 130 groups for training\nand the last 11 groups for testing.\nHuman3.6M (Ionescu et al. 2013) is the most widely used\ndataset in the 3D human pose estimation task. It provides\n3.6M accurate 3D poses captured by the MoCap system in\nthe indoor environment. It contains 15 actions performed\nby seven actors from four cameras. There are two common\nevaluation protocols by splitting training and testing set in\nprior methods (Martinez et al. 2017; Zhao et al. 2019; Liu\net al. 2020; Xu and Takano 2021). The ﬁrst protocol uses\nsubjects S1, S5, S6, S7 and S8 for training, and S9 and S11\nfor testing. Errors are calculated after the ground truth and\npredictions are aligned with the root joints. The second pro-\ntocol uses subjects S1, S5, S6, S7, S8 and S9 for training,\nand S11 for testing. We conduct experiments using the ﬁrst\nprotocol. In this way, there are 1,559,752 frames for training,\nand 543,344 frames for testing.\nEvaluation Metric We follow the same evaluation metric\nas in (Zhao et al. 2019). The evaluation metric is the Mean\nPer Joint Position Error (MPJPE) in millimeters, which is\ncalculated between the ground truth and the predicted 3D\ncoordinates across all cameras and joints after aligning the\npredeﬁned root joints (the pelvis joint).\nTraining Settings For the three hand datasets ObMan,\nFHAD and GHD, we directly take 2D image coordinates and\nMethods Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT.Avg.\n(Ionescu et al. 2013) PAMI132.7 183.6 132.3 164.4 162.1 205.9 150.6 171.3 151.6 243.0 162.1 170.7 177.1 96.6 127.9 162.1(Tekin et al. 2016) CVPR102.4 147.2 88.8 125.3 118.0 182.7 112.4 129.2 138.9 224.9 118.4 138.8 126.3 55.1 65.8 125.0(Zhou et al. 2016) CVPR87.4 109.3 87.1 103.2 116.2 143.3 106.9 99.8 124.5 199.2 107.4 118.1 114.2 79.4 97.7 113.0(Du et al. 2016) ECCV 85.1 112.7 104.9 122.1 139.1 135.9 105.9 166.2 117.5 226.9 120.0 117.7 137.4 99.3 106.5 126.5(Chen and Ramanan 2017) CVPR89.9 97.6 89.9 107.9 107.3 139.2 93.6 136.0 133.1 240.1 106.6 106.2 87.0 114.0 90.5 114.1(Pavlakos et al. 2017) CVPR67.4 71.9 66.7 69.1 72.0 77.0 65.0 68.3 83.7 96.5 71.7 65.8 74.9 59.1 63.2 71.9(Mehta et al. 2017) 3DV 52.6 64.1 55.2 62.2 71.6 79.5 52.8 68.6 91.8 118.4 65.7 63.5 49.4 76.4 53.5 68.6(Zhou et al. 2017) ICCV 54.8 60.7 58.2 71.4 62.0 65.5 53.8 55.6 75.2 111.6 64.1 66.0 51.4 63.2 55.3 64.9(Martinez et al. 2017) ICCV51.8 56.2 58.1 59.0 69.5 78.4 55.2 58.1 74.0 94.6 62.3 59.1 65.1 49.5 52.4 62.9(Sun et al. 2017) ICCV 52.8 54.8 54.2 54.3 61.8 53.1 53.6 71.7 86.7 61.5 67.2 53.4 47.1 61.6 53.4 59.1(Fang et al. 2018) AAAI 50.1 54.3 57.0 57.1 66.6 73.3 53.4 55.7 72.8 88.6 60.3 57.7 62.7 47.5 50.6 60.4(Zhao et al. 2019) CVPR48.2 60.8 51.8 64.0 64.6 53.6 51.1 67.4 88.7 57.7 73.2 65.6 48.9 64.8 51.9 60.8\nGraFormer (HG) 49.3 53.9 54.1 55.0 63.0 69.8 51.1 53.3 69.4 90.0 58.0 55.2 60.3 47.4 50.6 58.7\n(Martinez et al. 2017) (GT)37.7 44.4 40.3 42.1 48.2 54.9 44.4 42.1 54.6 58.0 45.1 46.4 47.6 36.4 40.4 45.5(Hossain and Little 2018) (GT)35.2 40.8 37.2 37.4 43.2 44.0 38.9 35.6 42.3 44.6 39.7 39.7 40.2 32.8 35.5 39.2(Zhao et al. 2019) (GT) 37.8 49.4 37.6 40.9 45.1 41.4 40.1 48.3 50.1 42.2 53.5 44.3 40.5 47.3 39.0 43.8(Liu et al. 2020) (GT) 36.8 40.3 33.0 36.3 37.5 45.0 39.7 34.9 40.3 47.7 37.4 38.5 38.6 29.6 32.0 37.8(Xu and Takano 2021) (GT)35.8 38.1 31.0 35.3 35.8 43.2 37.3 31.7 38.4 45.5 35.4 36.7 36.8 27.9 30.7 35.8\nGraFormer (GT) 32.0 38.0 30.4 34.4 34.7 43.3 35.2 31.4 38.0 46.2 34.2 35.7 36.1 27.4 30.6 35.2\nTable 1: MPJPE (mm) results on Human3.6M. This table is split into 2 groups. The inputs for the top group methods are images\nand the inputs for the bottom group are ground truth of 2D joints. We note that GraFormer achieves the state-of-the-art results\nwith only 0.65M parameters compared to (Xu and Takano 2021) with 3.70M parameters.\nMethods ObMan FHAD GHD\nLinear (Martinez et al. 2017) 23.64 26.15 39.25\nGraph U-Net(Doosti et al. 2020) 7.63 13.82 8.45\nGraFormer(ours) 3.29 11.68 4.25\nTable 2: MPJPE (mm) results compared with Linear model\nand Graph U-Net on three hand datasets, ObMan, FHAD\nand GHD.\n3D camera coordinates as the inputs and ground truth. The\n2D coordinates and 3D ground truth provided by ObMan can\nbe used by simply converting the ground truth from meter\nto millimeter. The 3D ground truth provided by FHAD are\nworld coordinates, and we use the extrinsic matrix to calcu-\nlate the corresponding camera coordinates. The 3D ground\ntruth of GHD are already the camera coordinate system, and\nthe 2D coordinates need to be cropped, scaled and restored.\nWe use the hand coordinates as input but object coordinates\nfor all hand datasets. For Human3.6M, because of the mul-\ntiple camera views, it needs to be normalized according to\n(Zhao et al. 2019) before training and evaluation.\nIn our experiment, we set the number of N in Fig. 2 to\n5 and adopt 4 heads for self-attention. Different from the\nfeature dimension value of 64 or 128 in prior works (Zhao\net al. 2019; Xu and Takano 2021), we set the middle feature\ndimension of the model to 96 for GraFormer with a dropout\nrate of 0.25. We adopt Adam (Kingma and Ba 2014) opti-\nmizer for optimization with an initial learning rate of 0.001\nand mini-batches of 64. For Human3.6M, we multiply the\nlearning rate by 0.9 every 75000 steps. For hand datasets,\nthe learning rate decays by 0.9 every 30 epochs. We train\nGraFormer for 50 epochs on Human3.6M, 900 epochs on\nObman and GHD and 3000 epochs on FHAD.\nPerformance and Comparison\nIn this section, we evaluate our GraFormer on several pop-\nular datasets and analyze the performances compared with\nother state-of-the-art methods.\nPerformance on Hand Datasets We ﬁrst verify\nGraFormer on hand datasets via comparing with Graph\nU-Net (Doosti et al. 2020) and Linear model (Martinez\net al. 2017) since all of these methods regress 3D pose\nresults by taking 2D ground truth coordinates as inputs.\nThe results on 3 hand datasets are shown in Tab. 2. One\ncan see that GraFormer achieves the best performance\nacross all these 3 hand datasets. In particular, GraFormer\nsurpasses Graph U-Net by a large margin of 4.34, 2.14 and\n4.2 on ObMan, FHAD and GHD respectively. We note that\nsmall datasets are not friendly to self-attention, even so,\nGraFormer still beats Graph U-Net on the severely small\ndataset FHAD.\nPerformance on Human Pose DatasetPrior work results\non Human3.6M can be divided into two groups as shown in\nTab. 1. The top group methods take the images as inputs and\nthen yield the 3D poses using the learned features. For our\nmethod, we adopt 2D coordinate detection results provided\nby (Zhao et al. 2019), which are detected by stacked hour-\nglass network (Newell, Yang, and Deng 2016). In the top\ngroup, we only compare our method with the methods be-\nfore (Zhao et al. 2019). Because the new methods use more\nadvanced 2D detectors, which can provide more accurate 2D\ncoordinates but their codes are not public available. In the\nbottom group, we compare with the most advanced methods\nusing the same 2D inputs. The results show that GraFormer\nsurpasses all the methods, which evaluates the superiority of\nour method.\nThe bottom group methods take 2D ground truth as in-\nputs to predict 3D pose coordinates directly. Compared with\nthe previous methods, GraFormer achieves the best perfor-\nmance which evaluates the effectiveness of our method. In\nparticular, GraFormer obviously improves scores in direc-\ntion, eating, greet, phone, pose, smoke, and wait with only\n0.65M parameters (18% of (Xu and Takano 2021)). Inter-\nestingly, we ﬁnd that these actions have large magnitude mo-\n1\n4\n8\n12\n16\n1 4 8 12 16\n(1) (5)(3) (4)(2)\n1\n4\n8\n12\n16\n1\n4\n8\n12\n16\n1\n4\n8\n12\n16\n1\n4\n8\n12\n16\n1 4 8 12 16 1 4 8 12 16 1 4 8 12 16 1 4 8 12 16\nFigure 3: Visualization of the learned adjacency matrixes of different LAM-Gconv layers in the GraAttention module. It can be\nobserved that the adjacency matrixes of the ﬁrst and last layers mainly aggregate information among their near neighbor joints.\nWhile the middle 3 layers aggregate long-range information.\ntions, which implies longer distances among 2D joints than\nthe actions of Photo and SittingD. This means GraFormer\nhas a more powerful capability to capture information of 2D\ngraph joints with larger magnitude action motions.\nMethods params MPJPE(mm)\nGAT(Veliˇckovi´c et al. 2017) 0.16M 82.9\nST-GCN(Yan, Xiong, and Lin 2018) 0.27M 57.4\nFC(Martinez et al. 2017) 4.29M 45.5\nSemGCN(Zhao et al. 2019) 0.43M 43.8\nPre-agg(Liu et al. 2020) 4.22M 37.8\nGraphSH(Xu and Takano 2021) 3.70M 35.8\nGraFormer-small (ours) 0.12M 38.9\nGraFormer (ours) 0.65M 35.2\nTable 3: Results on Human3.6M dataset under different pa-\nrameter conﬁgurations.\nAblation Study\nDiscussion on model parameters We start our ablation\nexperiments by comparing GraFormer on different parame-\nter conﬁgurations with other methods on the Human3.6M\ndataset, Tab. 3. We report the results of our models of\ntwo conﬁgurations to prove that our method can achieve\nbetter results with fewer parameters than other methods.\nGraFormer-small has only 2 layers, and the feature dimen-\nsion is 64 with a dropout rate of 0.1. GraFormer sets the N to\n5, and the feature dimension is set to 96 with a dropout rate\nof 0.25. Our GraFormer achieves better results with even\nmuch fewer parameters than GraphSH, etc. Our lightweight\nversion, GraFormer-small, with only 72% fewer parameters\nthan SemGCN, beats SemGCN by 4.9.\nEffects of GraFormer ModulesNext, we test GraFormer\nmodules on Human3.6M (Ionescu et al. 2013) and Ob-\nMan (Hasson et al. 2019). We design 5 models by remov-\ning or replacing GraFormer’s modules to test the effects\nof our method. We keep all parameters the same for the\n5 models if not particularly indicated. Speciﬁcally, Model-\nT is formed through replacing the stack of GraAttention\nand ChebGConv block by transformer encoder. Model-C\nremoves GraAttention from GraFormer. Model-M replaces\nGraAttention with self-attention. Model-AT removes the\nModels Human3.6M ObMan FHAD\nmodel-T 51.76 15.54 20.14\nmodel-C 47.81 8.51 16.30\nmodel-M 42.19 5.02 13.52\nmodel-AT 37.78 7.29 14.39\nmodel-AM 42.44 3.46 13.49\nGraFormer 35.17 3.29 11.68\nTable 4: MPJPE (mm) results on Human3.6M, ObMan and\nFHAD by removing or replacing GraFormer’s modules.\nChebGconv block from GraFormer. And model-AM re-\nserves MLP compared to GraFormer. The results are shown\nin Table 4.\nEpochs\nMPJPE(mm)\nmodel-AM\nGraFormer\n12.5\n15.0\n17.5\n20.0\n22.5\n0 500 1000 1500 2000\nmodel-AM\nGraFormer\nFigure 4: Test errors of model-AM and GraFormer on\nFHAD dataset.\nFrom the results of model-T, we can ﬁnd that the trans-\nformer is poor for 2D-to-3D pose estimation. This is because\ntransformer ignores graph structure information. The results\nof model-C and model-M are worse than GraFromer, which\nprove that GraAttention is necessary and more effective than\nself-attention. The loss of ChebGConv block in model-AT\nbrings worse performance, which illustrates the effective-\nness of ChebGConv block. The performance also degrades\nwhen plugging the MLP layer after self-attention in GraAt-\ntention, which veriﬁes that the MLP layer impedes the learn-\ning of 3D poses actually. In Fig. 4, we show the test errors\nof model-AM and GraFormer. We ﬁnd that the test error of\nmodel-AM is hindered at about 250 epochs while test error\nof GraFomer still decreases until below 12. This reconﬁrms\nthat the MLP layer can be a deterrent on 2D-to-3D pose es-\ntimation task.\nFigure 5: Skeleton results predicted by GraFormer on ObMan (Hasson et al. 2019) (top row), FHAD (Garcia-Hernando et al.\n2018) (middle row) and GHD (Mueller et al. 2018) (bottom row).\n(a) (b) (c)\nFigure 6: Visualization of graph Laplacian of ChebGConv.\nVisualization Fig. 3 shows the visualization results of\nlearned adjacency matrixes of LAM-Gconv layers of GraAt-\ntention. In the ﬁrst sub-ﬁgure (1), the color of some 3 ×3\nregions is obviously brighter than other regions, which in-\ndicates that interaction among these joints takes greater\nweights and these joints are closely connected. Interestingly,\nwe note that joints 2-4, 5-7, 11-13 and 14-16 are four limbs\nof the human, which are activated in 3 ×3 regions. Which\nimplies that the relations of joints on a limb are strongly con-\nnected, and our GraFormer is able to ﬁnd these relations ef-\nfectively. The regions of sub-ﬁgures from (2) to (4) become\nlarger, which shows that GraFormer ﬁnds long-range rela-\ntionships in these layers. The sub-ﬁgure (5) illustrates that\nthe interaction regions become much smaller, which implies\nthat joints mainly retain their own information, and a little\ninformation interacts among joints.\nIn Figure 5, we show the predicted 3D hand results on\nObMan (Hasson et al. 2019) (top row), FHAD (Garcia-\nHernando et al. 2018) (middle row) and GHD (Mueller et al.\n2018) (bottom row). The images in columns 1, 3 and 5 are\nskeleton ﬁgures drawn using 2D ground truth. In columns\n2, 4 and 6, the colored skeletons are drawn using the 3D\npredictions, and the black skeletons are drawn using the 3D\nground truth. We can ﬁnd that our method is able to estimate\nthe 3D poses accurately using 2D coordinates. It shows that\nour method could effectively learns 3D poses by exploiting\nthe relationship among 2D joints.\nFigure 6 is a visualization of graph Laplacian of different\norders of Chebyshev graph convolution. The top row shows\nthe schematic diagrams of joint information aggregation ac-\ncording to the bottom row. The width of the line implies the\nweights between 2D joints. The bottom row shows the visu-\nalization of the corresponding Laplacian matrix with 0-order\n(a), 1-order (b) and 2-order (c) respectively. It is easy to ﬁnd\nthat bigger orders matrix activates more 2D joints, which\nimplies that bigger orders of Laplacian matrix have the ca-\npability to ﬁnd more implicit relations.\nConclusions\n2D-to-3D pose estimation task takes graph-structured 2D\njoint coordinates as inputs. We claim that 2D joints relations\nremain semi-developed and the performance can be further\nboosted by exploiting these relations. We propose a novel\nmodel combined graph convolution and transformer, called\nGraFormer, for 3D pose estimation, which aims at better\nexploiting relations among graph-structured 2D joints. Ex-\ntensive experiment results across popular benchmarks show\nthat our method achieves state-of-the-art performances com-\npared with previous works using much fewer parameters.\nReferences\nChang, A. X.; Funkhouser, T.; Guibas, L.; Hanrahan, P.;\nHuang, Q.; Li, Z.; Savarese, S.; Savva, M.; Song, S.; Su,\nH.; et al. 2015. Shapenet: An information-rich 3d model\nrepository. arXiv preprint arXiv:1512.03012.\nChen, C.-H.; and Ramanan, D. 2017. 3d human pose es-\ntimation= 2d pose estimation+ matching. In IEEE CVPR,\n7035–7043.\nDefferrard, M.; Bresson, X.; and Vandergheynst, P. 2016.\nConvolutional neural networks on graphs with fast localized\nspectral ﬁltering. arXiv preprint arXiv:1606.09375.\nDoosti, B.; Naha, S.; Mirbagheri, M.; and Crandall, D. J.\n2020. Hope-net: A graph-based model for hand-object pose\nestimation. In IEEE CVPR, 6608–6617.\nDu, Y .; Wong, Y .; Liu, Y .; Han, F.; Gui, Y .; Wang, Z.;\nKankanhalli, M.; and Geng, W. 2016. Marker-less 3d human\nmotion capture with monocular image sequence and height-\nmaps. In ECCV, 20–36.\nFang, H.-S.; Xu, Y .; Wang, W.; Liu, X.; and Zhu, S.-C. 2018.\nLearning pose grammar to encode human body conﬁgura-\ntion for 3d pose estimation. In AAAI.\nGan, J.; and Wang, W. 2019. In-air handwritten English\nword recognition using attention recurrent translator.Neural\nComputing and Applications, 31(7): 3155–3172.\nGarcia-Hernando, G.; Yuan, S.; Baek, S.; and Kim, T.-K.\n2018. First-person hand action benchmark with rgb-d videos\nand 3d hand pose annotations. In IEEE CVPR, 409–419.\nGe, L.; Ren, Z.; Li, Y .; Xue, Z.; Wang, Y .; Cai, J.; and Yuan,\nJ. 2019. 3d hand shape and pose estimation from a single\nrgb image. In IEEE CVPR, 10833–10842.\nHasson, Y .; Varol, G.; Tzionas, D.; Kalevatykh, I.; Black,\nM. J.; Laptev, I.; and Schmid, C. 2019. Learning joint recon-\nstruction of hands and manipulated objects. In IEEE CVPR,\n11807–11816.\nHossain, M. R. I.; and Little, J. J. 2018. Exploiting temporal\ninformation for 3d human pose estimation. InECCV, 68–84.\nHu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-excitation\nnetworks. In IEEE CVPR, 7132–7141.\nIonescu, C.; Papava, D.; Olaru, V .; and Sminchisescu, C.\n2013. Human3. 6m: Large scale datasets and predictive\nmethods for 3d human sensing in natural environments.\nIEEE TPAMI, 36(7): 1325–1339.\nJiang, S.; Sun, B.; Wang, L.; Bai, Y .; Li, K.; and Fu, Y . 2021.\nSkeleton aware multi-modal sign language recognition. In\nIEEE CVPR, 3413–3423.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKipf, T. N.; and Welling, M. 2016. Semi-supervised classi-\nﬁcation with graph convolutional networks. arXiv preprint\narXiv:1609.02907.\nLi, M.; Chen, S.; Chen, X.; Zhang, Y .; Wang, Y .; and Tian,\nQ. 2019. Actional-structural graph convolutional networks\nfor skeleton-based action recognition. In IEEE CVPR,\n3595–3603.\nLi, M.; Gao, Y .; and Sang, N. 2021. Exploiting Learnable\nJoint Groups for Hand Pose Estimation. In AAAI.\nLin, K.; Wang, L.; and Liu, Z. 2021. End-to-end human pose\nand mesh reconstruction with transformers. In IEEE CVPR,\n1954–1963.\nLiu, K.; Ding, R.; Zou, Z.; Wang, L.; and Tang, W. 2020.\nA comprehensive study of weight sharing in graph networks\nfor 3d human pose estimation. In ECCV, 318–334.\nLu, D.; and Luo, L. 2020. FMKit: An In-Air-Handwriting\nAnalysis Library and Data Repository. In CVPR Workshop,\n2020.\nMartinez, J.; Hossain, R.; Romero, J.; and Little, J. J. 2017.\nA simple yet effective baseline for 3d human pose estima-\ntion. In IEEE ICCV, 2640–2649.\nMehta, D.; Rhodin, H.; Casas, D.; Fua, P.; Sotnychenko, O.;\nXu, W.; and Theobalt, C. 2017. Monocular 3d human pose\nestimation in the wild using improved cnn supervision. In\nIEEE 3DV, 506–516.\nMueller, F.; Bernard, F.; Sotnychenko, O.; Mehta, D.; Srid-\nhar, S.; Casas, D.; and Theobalt, C. 2018. Ganerated hands\nfor real-time 3d hand tracking from monocular rgb. In IEEE\nCVPR, 49–59.\nNewell, A.; Yang, K.; and Deng, J. 2016. Stacked hourglass\nnetworks for human pose estimation. In ECCV, 483–499.\nPavlakos, G.; Zhou, X.; Derpanis, K. G.; and Daniilidis, K.\n2017. Coarse-to-ﬁne volumetric prediction for single-image\n3D human pose. In IEEE CVPR, 7025–7034.\nRedmon, J.; and Farhadi, A. 2017. YOLO9000: better,\nfaster, stronger. In IEEE CVPR, 7263–7271.\nRomero, J.; Tzionas, D.; and Black, M. J. 2017. Embodied\nhands: Modeling and capturing hands and bodies together.\nACM Transactions on Graphics, 36(6): 1–17.\nSimon, T.; Joo, H.; Matthews, I.; and Sheikh, Y . 2017. Hand\nkeypoint detection in single images using multiview boot-\nstrapping. In IEEE CVPR, 1145–1153.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: a simple way to prevent\nneural networks from overﬁtting. The journal of machine\nlearning research, 15(1): 1929–1958.\nSun, X.; Shang, J.; Liang, S.; and Wei, Y . 2017. Composi-\ntional human pose regression. In IEEE ICCV, 2602–2611.\nTekin, B.; Bogo, F.; and Pollefeys, M. 2019. H+ o: Uniﬁed\negocentric recognition of 3d hand-object poses and interac-\ntions. In IEEE CVPR, 4511–4520.\nTekin, B.; Rozantsev, A.; Lepetit, V .; and Fua, P. 2016. Di-\nrect prediction of 3d body poses from motion compensated\nsequences. In IEEE CVPR, 991–1000.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\nP.; and Bengio, Y . 2017. Graph attention networks. arXiv\npreprint arXiv:1710.10903.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018. Non-\nlocal neural networks. In IEEE CVPR, 7794–7803.\nWeng, J.; Weng, C.; and Yuan, J. 2017. Spatio-temporal\nnaive-bayes nearest-neighbor (st-nbnn) for skeleton-based\naction recognition. In IEEE CVPR, 4171–4180.\nXu, T.; and Takano, W. 2021. Graph Stacked Hourglass\nNetworks for 3D Human Pose Estimation. In IEEE CVPR,\n16105–16114.\nYan, S.; Xiong, Y .; and Lin, D. 2018. Spatial temporal graph\nconvolutional networks for skeleton-based action recogni-\ntion. In AAAI.\nYang, J.; Lu, J.; Lee, S.; Batra, D.; and Parikh, D. 2018.\nGraph r-cnn for scene graph generation. InECCV, 670–685.\nZhao, L.; Peng, X.; Tian, Y .; Kapadia, M.; and Metaxas,\nD. N. 2019. Semantic graph convolutional networks for 3d\nhuman pose regression. In IEEE CVPR, 3425–3435.\nZhou, X.; Huang, Q.; Sun, X.; Xue, X.; and Wei, Y . 2017.\nTowards 3d human pose estimation in the wild: a weakly-\nsupervised approach. In IEEE ICCV, 398–407.\nZhou, X.; Sun, X.; Zhang, W.; Liang, S.; and Wei, Y . 2016.\nDeep kinematic pose regression. In ECCV, 186–201.",
  "topic": "Pose",
  "concepts": [
    {
      "name": "Pose",
      "score": 0.6465426087379456
    },
    {
      "name": "Transformer",
      "score": 0.6390436291694641
    },
    {
      "name": "Graph",
      "score": 0.5320732593536377
    },
    {
      "name": "Computer science",
      "score": 0.5134530663490295
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.45233866572380066
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37298083305358887
    },
    {
      "name": "Mathematics",
      "score": 0.3487631678581238
    },
    {
      "name": "Algorithm",
      "score": 0.3222026824951172
    },
    {
      "name": "Theoretical computer science",
      "score": 0.23741012811660767
    },
    {
      "name": "Engineering",
      "score": 0.16756302118301392
    },
    {
      "name": "Voltage",
      "score": 0.12293094396591187
    },
    {
      "name": "Electrical engineering",
      "score": 0.11814817786216736
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    }
  ],
  "institutions": []
}