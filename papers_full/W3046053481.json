{
    "title": "A Transformer Approach to Contextual Sarcasm Detection in Twitter",
    "url": "https://openalex.org/W3046053481",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A3045690963",
            "name": "Hunter, Gregory",
            "affiliations": [
                "Duke University"
            ]
        },
        {
            "id": "https://openalex.org/A2110734498",
            "name": "Steven Li",
            "affiliations": [
                "Duke University"
            ]
        },
        {
            "id": "https://openalex.org/A2140858088",
            "name": "Pouya Mohammadi",
            "affiliations": [
                "Duke University"
            ]
        },
        {
            "id": "https://openalex.org/A3045976334",
            "name": "Natalie Tarn",
            "affiliations": [
                "Duke University"
            ]
        },
        {
            "id": "https://openalex.org/A3131724768",
            "name": "Rachel Draelos",
            "affiliations": [
                "Duke University"
            ]
        },
        {
            "id": "https://openalex.org/A2141705163",
            "name": "Cynthia Rudin",
            "affiliations": [
                "Duke University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2758985501",
        "https://openalex.org/W2963667932",
        "https://openalex.org/W2251210340",
        "https://openalex.org/W2888643222",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W3003370095",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2750747353",
        "https://openalex.org/W2122522916",
        "https://openalex.org/W1689711448",
        "https://openalex.org/W2051840895",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963635943",
        "https://openalex.org/W4297801177",
        "https://openalex.org/W2607623312",
        "https://openalex.org/W1980931745",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2991568321"
    ],
    "abstract": "Understanding tone in Twitter posts will be increasingly important as more and more communication moves online. One of the most difficult, yet important tones to detect is sarcasm. In the past, LSTM and transformer architecture models have been used to tackle this problem. We attempt to expand upon this research, implementing LSTM, GRU, and transformer models, and exploring new methods to classify sarcasm in Twitter posts. Among these, the most successful were transformer models, most notably BERT. While we attempted a few other models described in this paper, our most successful model was an ensemble of transformer models including BERT, RoBERTa, XLNet, RoBERTa-large, and ALBERT. This research was performed in conjunction with the sarcasm detection shared task section in the Second Workshop on Figurative Language Processing, co-located with ACL 2020.",
    "full_text": "Proceedings of the Second Workshop on Figurative Language Processing, pages 270–275\nJuly 9, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n270\nA Transformer Approach to Contextual Sarcasm Detection in Twitter\nHunter Gregory, Steven Li, Pouya Mohammadi, Natalie Tarn, Rachel Draelos, Cynthia Rudin\nDepartment of Computer Science, Duke University\n{hlg16, sl561, pm201, nt94, rlb61}@duke.edu, cynthia@cs.duke.edu\nAbstract\nUnderstanding tone in Twitter posts will be in-\ncreasingly important as more and more com-\nmunication moves online. One of the most dif-\nﬁcult, yet important tones to detect is sarcasm.\nIn the past, LSTM and transformer architec-\nture models have been used to tackle this prob-\nlem. We attempt to expand upon this research,\nimplementing LSTM, GRU, and transformer\nmodels, and exploring new methods to clas-\nsify sarcasm in Twitter posts. Among these,\nthe most successful were transformer models,\nmost notably BERT. While we attempted a few\nother models described in this paper, our most\nsuccessful model was an ensemble of trans-\nformer models including BERT, RoBERTa,\nXLNet, RoBERTa-large, and ALBERT. This\nresearch was performed in conjunction with\nthe sarcasm detection shared task section in\nthe Second Workshop on Figurative Language\nProcessing, co-located with ACL 2020.\n1 Introduction\nSarcasm detection is an important step towards\ncomplete natural language comprehension since\na sarcastic phrase typically expresses a sentiment\ncontradictory to its literal meaning. Humans usu-\nally detect sarcasm with contextual clues, espe-\ncially intonation, which is not available in text-\nbased social media data.\nOne challenge of sarcasm is the frequent neces-\nsity of prior knowledge. Consider an example:\nContext: driver tailgating a cyclist gets\ninstant justice\nResponse: maybe he tried to save gas like you\ndo when you tailgate a transport truck\nAs humans, we can discern this response is sar-\ncastic since we know a large car behind a small\nbike would not improve aerodynamics and hence\ngas mileage. This is a Herculean inference for an\nalgorithm. In other situations, sarcastic inference\nrelies on knowledge about science, sports, politics,\nor movies.\nA broader challenge in classifying sarcasm is\nthat a model trained on one dataset does not neces-\nsarily generalize to another one.\nUsing a Twitter dataset from the Second Work-\nshop on Figurative Language Preprocessing, we\ntackle this difﬁcult challenge of sarcasm detection\nand its speciﬁc issues as discussed above.\n2 Datasets\nThe workshop provided two balanced sarcasm\ndatasets from both Twitter and Reddit with 5,000\nand 4,400 observations respectively. The work-\nshop collected and labeled the Twitter data using\nthe hashtags #sarcastic and #sarcasm, and prepro-\ncessed the data by replacing URLs and user men-\ntions with placeholders. The Reddit dataset is a\nsubset of that from Khodak et al. (2017). Both\ndatasets consist of a sarcasm label, response, and\nconversation context. Both the test datasets con-\ntained 1800 observations.\nUnfortunately, there were notable limitations in\nthe Twitter data. We removed 300 observations\nthat were duplicates (further discussed in Prepro-\ncessing). Nuances in twitter data such as acronyms,\nhashtags, and emojis needed to be processed. Fur-\nthermore, the Twitter dataset had missing hashtags,\naffecting the meaning of the response. Some tweets\nalso contained images in the response, but there\nwere no images in the data. For example, one tweet\ncontained “they gave me the most (#beautiful, re-\nmoved in the dataset) eggs for breakfast” with an\nimage of chickens, but without the image it would\nbe hard to determine its label.\nThere was also a considerable domain shift be-\ntween training and test data for the workshop. For\nall models, training and validation scores were sig-\nniﬁcantly higher than test scores.\n271\n3 Related Works and How We Use Them\nA wide variety of models have been created for sar-\ncasm detection. Our baseline model stems from the\nwork of Ghosh et al. (2018) about sarcasm analysis\nusing conversation context. This model involves\none LSTM reading the context and another reading\nthe response. While Ghosh and Veale (2017) pro-\nposed a similar architecture based on bidirectional\nLSTMs to detect sarcasm in Twitter, we found that\nbidirectional LSTMs performed similarly to uni-\ndirectional LSTMs. We used two different word\nembedding architectures, one from a previous pa-\nper trained on a separate Twitter data set and one\nusing CBOW with position-weights, with character\nn-grams of length 5, a window of size 5 and 10 neg-\natives (Ghosh et al., 2018; Grave et al., 2018). Sen-\ntence embeddings were obtained from averaging\nthe word embeddings. We experimented with both\nword-level and sentence-level attention models, but\nwe found that attention-based models performed\nsimilarly to those without (Yang et al., 2016).\nThe GRU architecture seemed to be a promising\nalternative option to the LSTM. Exploring tasks\non audio, handwriting, and musical classiﬁcation,\nGreff et al. (2016) discovered that GRUs performed\nthe same as LSTMs. Their ﬁnding across multiple\ndomains suggested that GRUs would perform simi-\nlarly to LSTMs in the Twitter domain. Given the\nsmall size of our dataset and the reduced parame-\nter size and complexity in GRUs, we believed this\narchitecture could generalize better than LSTMs.\nWe also experimented with transformer models,\nwhich have been very successful for other appli-\ncations such as sentiment analysis, question an-\nswering, and recently even for sarcasm detection\n(Devlin et al., 2019; Peters et al., 2018; Kayalvizhi\net al., 2019; Potamias et al., 2019). We experi-\nmented with using pre-trained representations from\nBERT as well as RoBERTa, obtained from a bidi-\nrectional approach of a masked language model\nand ELMo which uses a concatenation of repre-\nsentations obtained from a left-to-right and a right-\nto-left language model (Peters et al., 2018; Liu\net al., 2019). We applied ensemble learning us-\ning various pretrained transformer models (BERT,\nRoBERTa, XLNet, RoBERTa-large, ALBERT with\nno ﬁne-tuning on the transformer weights), where\neach model has a learned weighted sum for the\nhidden layers that is concatenated with hand-made\nfeatures, that is then fed into a dense layer for clas-\nsiﬁcation similar to the work of Wang et al. (2015).\n4 Methodology\n4.1 Preprocessing\nFor the Twitter dataset, we had to perform pre-\nliminary preprocessing. In the data, we removed\nseveral exact duplicates of observations and sev-\neral almost-exact duplicates, where the context or\nresponses would only have minor differences in\npunctuation (e.g. including or excluding a period)\nor a few characters. Text in this dataset also had\nan issue where a space was always placed on both\nsides of an apostrophe (e.g. ”ol’ Pete isn’t happy”\nwould become ”ol ’ Pete isn ’ t happy”). To ﬁx\nthis, we created a list of English contractions and\nslang terms paired with their expanded form (e.g.\n”ol’” with ”old” and ”isn’t” with ”is not”) and then\ncleaned all the text using the list. We also removed\nall occurrences of ” <URL>” and ”@USER” in\nthe text. Lastly, we expanded hashtags using the\npython package ekphrasis (Baziotis et al., 2017).\nFor both this cleaned Twitter data and the Red-\ndit data, we proceeded with the following prepro-\ncessing and feature creation. We lower-cased text,\nremoved punctuation (including hashtag symbols),\nand expanded contractions and slang terms using\nour list mentioned above. For additional features,\nWe created most of the extra features that Ghosh\net al. (2018) used when they were feasible (e.g. did\nnot require paying for a dataset). These included\nbinary features encoding whether any word in the\nresponse was entirely capitalized and whether the\nresponse contained quotation marks. We also cre-\nated a ternary feature for whether zero, one, or\nmultiple exclamation points were used in the re-\nsponse. Additionally, we developed features that\nwere duplicated for both the context and response.\nThese included a binary feature indicating if any\nword in the text had unnecessary repeating letters\nas well as a ternary feature for the sentiment of the\ncontext or response (positive, negative, or neutral)\nusing TextBlob (Loria, 2018). In addition, we had\na binary feature for whether a positive sentiment\nemoji was in the text, and similar features for nega-\ntive and neutral sentiment emojis based on a table\nfrom Kralj Novak et al. (2015).\nWe also created a political feature after noticing\nthat a large number of the sarcastic tweets were\npolitical in nature. More speciﬁcally, from a sample\nof 200 tweets that we took, we saw that 67.7% of\nthe political tweets were sarcastic while 31.8% of\nthe non-political tweets were sarcastic. We created\nthis feature by conducting a boolean search on the\n272\nWord Train setExtra Set\njoy 6.5% 76.9%\nwonderful6.7% 69.7%\nvoted 67.6% 6.0%\nlove 30.4% 72.7%\nTable 1: Sarcasm frequencies in different corpora\nresponse and context for key words and phrases\nthat we deemed to be political. These included the\nnames of famous politicians and words or phrases\nassociated with relevant political issues such as\n”elections”, ”capitalists”, ”planned parenthood”,\nor ”sanctuary city”. In order to reduce our false\npositive rate for political classiﬁcation, we were\ncareful to exclude words that are often political but\ncould easily be used in a non-political context such\nas ”president” (e.g. the president of a basketball\nteam) or ”immigrant” (e.g. a person referencing\ntheir life story).\nAfter running an XGBoost model with sentence-\nlevel response embeddings and our features, all\nseemed to have relatively equal feature impor-\ntances. Therefore, we proceeded with including\nall of the features above in our ﬁnal models.\nWe also thought certain words or certain top-\nics other than politics might be good predictors\nof sarcasm. We calculated the frequency of sar-\ncasm for all k-grams and discovered certain words\n(unigrams) appeared almost strictly in sarcastic or\nnon-sarcastic responses in the Twitter training data.\nAs a sanity check, we calculated the same word\nfrequencies for an extra Twitter dataset with over\n65,000 responses gathered in the same way as the\ntraining set (Pt ´aˇcek et al., 2014). Many compar-\nisons between frequencies, some displayed in Table\n1, proved that the patterns in the training set were\nnot representative of Twitter posts as a whole.\nWe also found that the LDA topic models with\nthe best coherence were not predictive of sarcasm.\nPredicting a tweet based on its top three topics with\na SVM or Logistic Regression Classiﬁer yielded\n53% training accuracy. Such low accuracy scores\nsuggested these features would not be likely to\ngeneralize. Thus, we did not use topic models or\nfrequency-based features in our ﬁnal models.\n4.2 Final Models and Performance\nFor all of our models, we focused on classifying\nthe Twitter dataset and left classifying the Reddit\ndataset to future works.\nFor our ﬁnal baseline model, we used Twit-\nter embeddings (Ghosh et al., 2018), and we em-\nployed a bidirectional LSTM with sentence-level\nembeddings for context and a bidirectional LSTM\nwith word-level embeddings for response. These\nLSTMs outputted sequences, which were modi-\nﬁed by the dimension-reducing attention scheme\nproposed by Yang et al. (2016). Finally, these\noutputs were concatenated with our extra features\nand passed into a dense layer. Switching to unidi-\nrectional LSTMs, changing the number of LSTM\nunits, or adding an extra dense layer yielded no im-\nprovement. Despite our hopes, replacing LSTMs\nwith GRUs also did not impact training/validation\nperformance.\nAs our two datasets have different distributions,\nmainly due to differences in text structure, format-\nting, and content, training by combining the two\ndatasets together reduced our testing performance\non the Twitter data. This was due to some fea-\ntures that are characteristic of the Reddit data, but\nmay not generalize to the Twitter data. In order\nto fully leverage both of the given datasets for pre-\ndicting sarcasm on the Twitter datasets, we utilized\na weighted binary cross entropy loss, where we\nweighed each batch of Reddit data less than our\nTwitter data. By doing so, the model picked up on\nmore universal sarcasm characteristics inherent to\nboth datasets, while still being tailored primarily to\nthe Twitter dataset. While we only tested this for\nour LSTM based models, this approach could be\ngeneralized to our other models.\nWe also utilized pre-trained transformer models\nto create sentence-level embeddings for classiﬁca-\ntion and word-level embeddings to feed into our\nLSTM model. For our testing, we chose to not ﬁne-\ntune the weights of the transformer models them-\nselves, however future works may consider doing\nso. Instead, we use a feature-based approach as\nsuggested in Devlin et al. (2019), where we extract\nfeatures from the hidden states of the pre-trained\ntransformer model for prediction.\nTwo different approaches were used to incor-\nporate context into the transformer inputs. The\nﬁrst approach was to concatenate the full context\nand response sentences into a single input. A po-\ntential downside to this method is that the con-\ncatenated sentences exceeded the transformer’s se-\nquence length limit for certain observations, and\nwe were forced to remove words from the front of\nthe context until the string was compatible with the\ntransformer. The second approach was to feed the\nfull context and responses into the transformer sep-\n273\narately and concatenate the outputs. The second ap-\nproach has the beneﬁt of allowing for sentences that\nare longer than the transformer’s sequence length\nlimit, allowing for more context to be utilized. We\nalso create a baseline model trained solely on the\nresponses. From our validation results (Table 3),\nwe decided to use the second approach for our ﬁnal\nsubmissions.\nWe obtained token representations for each of\nthe hidden states in the transformer model, which\nwere then averaged across the whole sentence to ob-\ntain a sentence representation for each of the hidden\nstates. To determine the best way to combine these\nhidden representations into a single sentence em-\nbedding, we experimented with using a weighted\nsum of all layers, a weighted average of the last\nfour layers, using only the last layer, summing all\nof the layers, and concatenating the last four layers.\nWe obtained the best validation results from using\na weighted sum of all hidden states (Table 2), and\nused this for our ﬁnal model. From here, we con-\ncatenated our additional features to this embedding\nand pass it through two dense layers for prediction.\nFor our ﬁnal submission, we applied this method\nto ﬁve separate pretrained transformer models and\nensembled their results, which boosted our F1-\nmacro score by a signiﬁcant margin on the test\nset (0.733 for a single model to 0.756). The models\nwe chose to ensemble were BERT-base-uncased,\nRoBERTa-base, XLNet-base-cased, RoBERTa-\nlarge, and ALBERT-base-v2 from the Huggingface\ntransformers library (Wolf et al., 2019).\nWe also developed an LSTM on a weighted sum\nof the BERT hidden layer outputs. This model was\nsimilar to the baseline LSTM. BERT outputs from\ncontext were passed into one bidirectional LSTM,\nand BERT outputs from response were passed into\nanother; however, these LSTMs outputted ﬁnal\nstates instead of sequences. These results were\nconcatenated with the extra features and passed\ninto two dense layers. On the test set, this model\nhad an approximate 2.2% increase in F1-macro\nscore compared to the other LSTM baseline.\nAdditionally, we implemented an ensemble\nlearning approach in the hopes of highlighting\nthe strengths of the many models that we devel-\noped. We were hopeful after seeing the improve-\nment from the multiple transformers model that\nthis model could achieve similar success. The as-\nsumption was that if we allowed models that were\nextremely conﬁdent in certain observations or per-\nRoBERTa BERTMethod PrecisionRecallF1 PrecisionRecallF1Weighted sum of lastfour hidden layers0.770 0.7690.7680.782 0.7760.776\nSum all layers 0.786 0.7670.7710.767 0.7500.751Last layer only 0.761 0.7600.7600.772 0.7670.768Concatenate last four layers0.773 0.7720.7730.769 0.7510.751Weighted sum of all layers0.764 0.7740.7730.785 0.7760.778\nTable 2: Macro-averaged validation scores using var-\nious methods of combining the hidden states. Best\nscores for each transformer are in bold\nRoBERTa BERTMethod PrecisionRecallF1 PrecisionRecallF1Response only0.757 0.7540.7550.752 0.7540.752Concatenated contextand response 0.791 0.7620.7650.776 0.7610.762\nSeparate contextand response 0.764 0.7740.7730.785 0.7760.778\nTable 3: Macro-averaged validation scores for our\ntransformer architecture with varying levels of contexts\nformed best on observations with certain charac-\nteristics to classify on those observations while\nallowing other models to classify observations in\nwhich this was not the case. In order to imple-\nment this, we ﬁrst trained and validated a multitude\nof models on only half of the training data given,\nreturning classiﬁcation probabilities on the other\nﬁfty percent of our training data and all of the test\ndata. These models included the LSTM model and\nBERT model embeddings passed through an SVM,\nboth discussed earlier, in addition to an LDA topic\nmodel, an XGBoost model, and a Gaussian Process\nClassiﬁer, all of which seemed to perform decently\non their own (besides LDA) and implement varying\nlogic to the same classiﬁcation problem. We chose\na 50/50 split for our ﬁrst training since we wanted\nto supply these initial models with enough train-\ning data to develop accurate classiﬁcations while\nleaving enough data for our ensemble classiﬁer to\ntrain on. Once obtaining these outputs, we trained a\nLogistic Regression model on the prediction prob-\nabilities generated by the many models for the re-\nmaining 50 percent of the training set as well as the\nsarcasm features that we had developed. We chose\nLogistic Regression because it seemed to overﬁt\nless than polynomial kernel SVM or decision forest,\nand there has been research on the beneﬁts of Logis-\ntic Regression on accuracy for ensemble learning\n(Wang et al., 2015). We then used this Logistic\nRegression to predict whether the test data was\nsarcastic or not. This model achieved a validation\nscore of 0.779 and a test score of 0.686, indicating\nthat the model did not generalize well. This ensem-\nble model also worsened scores of our best models,\nwhich would achieve a higher F1-macro validation\nscore when trained on the entirety of our training\n274\ndataset. However, the best transformer models and\nRNN models would drop from a validation score\nof above 0.703 and 0.682 to 0.667 and 0.504 re-\nspectively, when trained on half of the data as done\nin the ensemble model. Therefore, we believe that\nthe ensemble model would have achieved better\nscores if it were trained on a larger data set than\nthe one we worked with. However, it appears that\nthe weighted transformers model above performed\nbest given our dataset.\nAs seen in Table 5, all our ﬁnal models outper-\nformed the LSTM with attention (our implemen-\ntation of the baseline from Ghosh et al., 2018).\nTransformer representations seemed to capture the\nmost relevant information for sarcasm detection,\nand having context tends to improve results.\n5 Error Analysis\nThese results, however, are far below the F1-macro\nscores that were achieved in validation, which led\nus to believe that our training data may have come\nfrom a different distribution than our test data. In\norder to investigate this further, we calculated dis-\ntributions of our features for both the train and test\nset. We found a few differences between the dis-\ntributions indicating that there may be a covariate\nshift between the training and test sets. We provide\na few results from this analysis in Table 4. Notably,\nwe see that there was a difference of .0816 in the\npercentage of tweets that were political between\nthe test set and the training set. We also see far\nless observations with quotes or fully capitalized\nwords in the test set than the training set. While\nthis may be due to a small dataset and a high vari-\nance in the distribution of tweets, it still provides\nus with enough information to believe that there is\na covariate shift between the training and test sets.\nAs mentioned above, we also saw a large differ-\nence between our validation and test scores. For\nexample, we achieved a validation F1-macro score\nof 0.767 for our LSTM model with attention. How-\never, this model achieved an F1-macro score of\n0.669 on the test set. Additionally, our ensemble\nmodel went from a validation F1-macro score of\n0.779 to a test F1-Macro score of 0.686. In con-\njunction with the covariate shift, we believe that\nour training and test set come from different distri-\nbutions which would greatly increase our error.\nFeatures TrainTest Difference\nContains a Capitalized Word0.0340.002-0.032Contains a Quote 0.078.056 -0.022Positive Emoji in Context0.1920.2230.031Political 0.3970.315-0.082\nTable 4: Feature Distribution in Train and Test Sets\nFinal Models PrecisionRecallF1-macro\nTransformer Ensemble0.758 0.7670.756Solo RoBERTa Transformer0.733 0.7340.733BERT embeddings + LSTM0.695 0.7040.692Ensemble model 0.687 0.6890.686LSTM w/ attention 0.669 0.6690.669\nTable 5: Test Scores of Final Models\n6 Discussion and Conclusions\nThis research can hopefully guide future work on\nthe topic of detecting sarcasm in social media. On\nthat note, we would like to provide a few sugges-\ntions that others may ﬁnd helpful in tackling the\nproblem and a few of the ﬁndings that resulted in\nthe largest improvements for our results.\nFirst, utilizing multiple transformers and weight-\ning them by performance seems to perform far bet-\nter than a single transformer approach. Presumably,\nthis allows the model to contain the information\nprovided in each of the embeddings as opposed\nto a single form of embeddings. Next, we believe\nthat this task requires a large amount of training\ndata. We believe the reason the transformers per-\nformed so well was that they were pre-trained on\nlarge datasets. The models that we trained from\nscratch did not have as much training data, and we\nbelieve that they would have performed better with\nmore training data. We believe that the weighted\nbinary cross-entropy loss function to incorporate\nboth datasets is a potential approach to help with\nthis, and a future step would be to incorporate this\ninto our ﬁnal transformer models. Additionally, as\nemojis are an incredibly prevalent form of commu-\nnication on social media, as a future step, we would\nlike to incorporate emojis into the embedding space\nof our models. Finally, sarcasm detection is a difﬁ-\ncult task. Even for humans, it is difﬁcult to deter-\nmine whether individuals are being sarcastic online.\nThere are many complex variables that are difﬁcult\nto quantify when determining if a short post is sar-\ncastic. Therefore, capturing this notion of sarcasm\nwithin a model is difﬁcult as well. We hope our\ntechniques may be improved and expanded upon\nto solve other challenging natural language tasks.\n275\nReferences\nChristos Baziotis, Nikos Pelekis, and Christos Doulk-\neridis. 2017. Datastories at semeval-2017 task 4:\nDeep LSTM with attention for message-level and\ntopic-based sentiment analysis. In Proceedings of\nthe 11th International Workshop on Semantic Eval-\nuation (SemEval-2017), pages 747–754, Vancouver,\nCanada. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT 2019,\npages 4171–4186.\nAniruddha Ghosh and Tony Veale. 2017. Magnets for\nsarcasm: Making sarcasm detection timely, contex-\ntual and very personal. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 482–491.\nDebanjan Ghosh, Alexander R. Fabbri, and Smaranda\nMuresan. 2018. Sarcasm analysis using conversa-\ntion context. Computational Linguistics, 44(4):755–\n792.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings\nof the International Conference on Language Re-\nsources and Evaluation (LREC 2018).\nKlaus Greff, Rupesh K Srivastava, Jan Koutn ´ık,\nBas R Steunebrink, and J ¨urgen Schmidhuber. 2016.\nLSTM: A search space odyssey. IEEE transac-\ntions on neural networks and learning systems ,\n28(10):2222–2232.\nS. Kayalvizhi, D Thenmozhi, B Senthil Kumar, and\nChandrabose Aravindan. 2019. SSN NLP@IDAT-\nFIRE-2019: Irony detection in arabic tweets using\ndeep learning and features-based approaches. In\nFIRE.\nMikhail Khodak, Nikunj Saunshi, and Kiran V odrahalli.\n2017. A large self-annotated corpus for sarcasm.\narXiv preprint arXiv:1704.05579.\nPetra Kralj Novak, Jasmina Smailovi ´c, Borut Sluban,\nand Igor Mozetiˇc. 2015. Sentiment of emojis. PLoS\nONE, 10(12):e0144296.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nSteven Loria. 2018. textblob documentation. Release\n0.15, 2.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL.\nRolandos Alexandros Potamias, Georgios Siolas,\nand Andreas Georgios Stafylopatis. 2019. A\ntransformer-based approach to irony and sarcasm de-\ntection.\nTom´aˇs Pt ´aˇcek, Ivan Habernal, and Jun Hong. 2014.\nSarcasm detection on czech and english twitter.\npages 213–223.\nHong Wang, Qingsong Xu, and Lifeng Zhou. 2015.\nLarge unbalanced credit scoring using lasso-logistic\nregression ensemble. PloS one, 10(2).\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchi-\ncal attention networks for document classiﬁcation.\nIn Proceedings of the 2016 conference of the North\nAmerican chapter of the association for computa-\ntional linguistics: human language technologies ,\npages 1480–1489."
}