{
    "title": "Incorporating Source Syntax into Transformer-Based Neural Machine Translation",
    "url": "https://openalex.org/W2970247882",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2009816366",
            "name": "Anna Currey",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": "https://openalex.org/A1996490661",
            "name": "Kenneth Heafield",
            "affiliations": [
                "University of Edinburgh"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2962911926",
        "https://openalex.org/W2778814079",
        "https://openalex.org/W2123442489",
        "https://openalex.org/W2760656271",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W22168010",
        "https://openalex.org/W2884083742",
        "https://openalex.org/W2595715041",
        "https://openalex.org/W2903193068",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2124807415",
        "https://openalex.org/W2963648186",
        "https://openalex.org/W2963913268",
        "https://openalex.org/W2564486991",
        "https://openalex.org/W2963661253",
        "https://openalex.org/W2803214681",
        "https://openalex.org/W2945059185",
        "https://openalex.org/W2888539709",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W1869752048",
        "https://openalex.org/W2962982474",
        "https://openalex.org/W2912351236",
        "https://openalex.org/W2512924740",
        "https://openalex.org/W2889411721",
        "https://openalex.org/W2963069010",
        "https://openalex.org/W2737638662",
        "https://openalex.org/W2550821151",
        "https://openalex.org/W2963842982",
        "https://openalex.org/W2963355447",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2963653811",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W3082674894"
    ],
    "abstract": "Transformer-based neural machine translation (NMT) has recently achieved state-of-the-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018; Tang et al., 2018; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases. In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject syntax into the Transformer architecture without modifying it. We introduce two methods: a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences. We evaluate our methods on low-resource translation from English into twenty target languages, showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low- and medium-resource NMT but degenerates high-resource English-German translation.",
    "full_text": "Proceedings of the Fourth Conference on Machine Translation (WMT), V olume 1: Research Papers, pages 24–33\nFlorence, Italy, August 1-2, 2019.c⃝2019 Association for Computational Linguistics\n24\nIncorporating Source Syntax into Transformer-Based Neural Machine\nTranslation\nAnna Currey\nUniversity of Edinburgh\na.currey@sms.ed.ac.uk\nKenneth Heaﬁeld\nUniversity of Edinburgh\nkheafiel@ed.ac.uk\nAbstract\nTransformer-based neural machine transla-\ntion (NMT) has recently achieved state-of-\nthe-art performance on many machine trans-\nlation tasks. However, recent work (Ra-\nganato and Tiedemann, 2018; Tang et al.,\n2018; Tran et al., 2018) has indicated that\nTransformer models may not learn syntac-\ntic structures as well as their recurrent neu-\nral network-based counterparts, particularly in\nlow-resource cases. In this paper, we incor-\nporate constituency parse information into a\nTransformer NMT model. We leverage lin-\nearized parses of the source training sentences\nin order to inject syntax into the Transformer\narchitecture without modifying it.\nWe introduce two methods: a multi-task ma-\nchine translation and parsing model with a sin-\ngle encoder and decoder, and a mixed encoder\nmodel that learns to translate directly from\nparsed and unparsed source sentences. We\nevaluate our methods on low-resource trans-\nlation from English into twenty target lan-\nguages, showing consistent improvements of\n1.3 BLEU on average across diverse target lan-\nguages for the multi-task technique. We fur-\nther evaluate the models on full-scale WMT\ntasks, ﬁnding that the multi-task model aids\nlow- and medium-resource NMT but degener-\nates high-resource English →German transla-\ntion.\n1 Introduction\nTransformer-based neural machine translation\n(NMT) (Vaswani et al., 2017) has recently out-\nperformed recurrent neural network (RNN)-based\nmodels (Bahdanau et al., 2015; Cho et al., 2014)\nin many tasks (Bojar et al., 2018). However, there\nis still room for improvement for NMT, partic-\nularly for low- and moderate-resource language\npairs. Enriching NMT with syntactic informa-\ntion has the potential to improve generalization\nin low-resource scenarios, and adding syntax to\nTransformer-based NMT is currently an underex-\nplored research area.\nTransformer-based NMT may in fact stand to\nbeneﬁt even more from explicit syntactic annota-\ntions than RNN-based NMT, particularly in low-\nresource settings. On the one hand, the Trans-\nformer model already learns some syntax with-\nout explicit supervision in high-resource cases.\nVaswani et al. (2017) visualized a few encoder\nself-attentions in a trained NMT model and found\nthat they seemed to capture syntactic structure.\nThis was formalized by Raganato and Tiedemann\n(2018), who found that Transformer encoders\ntrained on high-resource NMT tasks were able to\nperform reasonably well at part-of-speech tagging,\nchunking, and other tasks. However, for Trans-\nformers trained on low-resource NMT, the results\non these tasks were not as strong. Additionally,\nTran et al. (2018) found that an RNN language\nmodel did better at predicting subject-verb agree-\nment than a Transformer language model; Tang\net al. (2018) saw similar results for Transformer\nvs. RNN NMT models.\nThus, the goal of this paper is to improve\nTransformer-based NMT using source-side syn-\ntactic supervision. We propose two methods that\nincorporate source-side linearized constituency\nparses into Transformer-based NMT. The ﬁrst,\nmulti-task, uses the Transformer to learn to parse\nand translate the source sentence simultaneously.\nThe second, mixed encoder, learns to translate di-\nrectly from both parsed and unparsed source sen-\ntences. This paper makes the following contribu-\ntions:\n•This is one of the ﬁrst attempts at using syn-\ntax to improve Transformer-based NMT\n•We introduce two methods for adding syntax\n25\nTransformer\n<TR> you have not been elected . <TR>\n<TR> let me make a comparison . <TR>\n<PA> you have not been elected . <PA>\n. . .\nno ha sido elegido .\nperm´ıtanme utilizar una comparaci´on .\n(ROOT (S (NP ) (VP (VP (VP ) ) ) ) )\n. . .\n(a) Multi-task syntactic NMT model. The system\nis trained to translate ( <TR>) and parse ( <PA>)\nsource sentences using the same architecture.\nTransformer\n(ROOT (S (NP you ) (VP have not (VP been (VP elected ) ) ) . ) )\n(ROOT (S (NP they ) (VP are (NP important issues ) ) . ) )\nyou have not been elected .\n. . .\nno ha sido elegido .\nestas cuestiones son importantes .\nno ha sido elegido .\n. . .\n(b) Mixed encoder syntactic NMT model. The system learns to trans-\nlate directly from both parsed and unparsed source sentences into un-\nparsed target sentences.\nFigure 1: Illustrations of the two proposed syntactic NMT methods.\nto NMT that are straightforward to incorpo-\nrate in practice\n•We empirically evaluate both methods on\ntranslation from English into 21 diverse tar-\nget languages, ﬁnding that the multi-task\nmethod improves consistently over a non-\nsyntactic baseline\n2 Transformer-Based NMT with\nLinearized Parses\nWe propose two models for incorporating lin-\nearized parses into Transformer-based NMT: a\nmulti-task model and a mixed encodermodel. Fig-\nure 1 summarizes the two proposed methods; they\nare discussed in detail in sections 2.2 and 2.3, re-\nspectively.\n2.1 Linearized Constituency Parses\nBoth of our proposed methods make use of lin-\nearized parses of the source sentences to inject\nsource syntax into Transformer-based NMT. Lin-\nearizing the parses allows us to add syntactic in-\nformation without modifying the Transformer ar-\nchitecture. Here, we describe how these parses are\ncreated. We generate and format the parsed data\nas follows:\n1. In order to generate syntactically parsed\ntraining data, we use the Stanford CoreNLP\nconstituency parser (Manning et al., 2014)\nto parse the source side of the parallel cor-\npus. This technique of parsing the parallel\ndata instead of using gold parses is common\nin syntactic NMT (Eriguchi et al., 2016) and\nin neural parsing (Vinyals et al., 2015). For\nthe multi-task model, it would be possible to\nincorporate gold parses into training as well,\nbut we leave this for future work.\n2. We linearize the resulting parses similarly\nto Vinyals et al. (2015) by using a depth-\nﬁrst tree traversal. We tokenize the opening\nparenthesis of each phrase with its phrase la-\nbel.\n3. Since neural machine translation already\nstruggles with long sentences (Bahdanau\net al., 2015), and adding the phrase nodes\nhas the potential to make the sentences much\nlonger, we remove part-of-speech tags from\nthe parses (as was done by Aharoni and Gold-\nberg, 2017).\n4. For our multi-task model (section 2.2), we re-\nmove words from the linearized parses. We\ndo this in order to further shorten the length\nof the target sequences. We do not expect that\nthis will make the parsing task too difﬁcult, as\na similar technique was used for neural pars-\n26\ntranslation <TR> you have not been elected . <TR> →no ha sido elegido .\nparsing <PA> you have not been elected . <PA> →(ROOT (S (NP ) (VP (VP (VP ) ) ) ) )\nTable 1: Example of English →Spanish training data for parsing and translation tasks in the multi-task system.\n(ROOT (S (NP you ) (VP have not (VP been (VP elected ) ) ) . ) ) →no ha sido elegido .\nyou have not been elected . →no ha sido elegido .\nTable 2: Example of English →Spanish training data for the mixed encoder system.\ning by Vinyals et al. (2015).\n5. For our mixed encoder model (section 2.3),\nwe convert the words in the parses into sub-\nwords using byte pair encoding (Sennrich\net al., 2016). We do not allow the parse la-\nbels to be broken into subwords.\nTables 1 and 2 give examples of the resulting\nparse formats.\n2.2 Multi-Task NMT and Parsing with\nShared Decoder\nOur ﬁrst method for incorporating source-side\nsyntax into Transformer-based NMT adopts a\nmulti-task framework. The main task is translat-\ning the source sentence into the target language;\nthe secondary task is parsing the source sentence.\nFor the parsing task, we employ the same encoder-\ndecoder framework as for NMT, with the sequen-\ntial source sentence as input and the linearized,\nunlexicalized parsed source sentence as output.\nThus, both tasks are trained using a single model\nwith a shared encoder and decoder. This is similar\nto the multi-task framework proposed by Luong\net al. (2016), with three main differences: 1) we do\nnot use separate decoders for each task, 2) we use\nthe same source data for both parsing and transla-\ntion, and 3) we use a Transformer rather than re-\ncurrent neural network-based architecture.\nWe do not directly use gold parses to train the\nparsing task, nor do we split the training data be-\ntween the two tasks. The reason for using the same\nsource data for both tasks is that we expect it to be\ndifﬁcult to ﬁnd a sufﬁciently large amount of in-\ndomain gold parses for training; additionally, our\nmain goal is to improve NMT, so we do not expect\nthe lower quality of the synthetic parses to matter.\nIn order to generate the training data for this\nmodel, we ﬁrst create linearized parses of the\nsource side of the training corpus as described\nabove. Next, we add a tag at the beginning and\nend of each source sentence indicating the desired\ntask, similar to what was done by Johnson et al.\n(2017) for multilingual NMT. Table 1 gives an ex-\nample of the data format. Finally, we shufﬂe the\nparsing and translation training data together and\ntrain the shared encoder and decoder on both tasks,\nmaking no further distinction between the tasks\nduring training. Since we parse all of the train-\ning data, each source sentence appears twice: once\nwith a target language sentence and once with a\nparse of the source sentence. These copies are\nshufﬂed separately.\n2.3 Mixed Encoder Transformer\nOur second method for augmenting the NMT\nTransformer with syntax is the mixed encoder\nmodel. This model learns to translate both from\nunparsed and parsed source sentences into un-\nparsed target sentences.\nIn order to train the mixed encoder model, we\ncreate two copies of the training data, one with\nparsed source sentences and the other with un-\nparsed source sentences. We then shufﬂe these\ntraining corpora together into a single corpus and\ntrain a standard Transformer NMT model on the\nﬁnal data, with a single encoder for both parsed\nand unparsed source sentences. The training data\ncontains (parsed source, unparsed target) and (un-\nparsed source, unparsed target) sentence pairs; Ta-\nble 2 gives an example of the two types of train-\ning sentence pairs for the mixed encoder method.\nSince the data is shufﬂed, these two sentence pairs\n(with identical target sentences) will not necessar-\nily be seen together during training.\nSince the mixed encoder model is trained on\nboth parsed and unparsed source sentences, during\ninference it is able to translate from either source\nsentence format. Inference on unparsed source\nsentences is slightly faster (since it does not re-\nquire parsing of the source sentence) and achieves\nslightly higher BLEU scores, so we show results\nusing unparsed source sentences for our experi-\nments (sections 4.2 and 5.2).\n27\n3 Experimental Setup\nWe evaluate our multi-task and mixed encoder\nmodels compared to a standard (non-syntactic)\nTransformer baseline on translation from English\ninto 21 target languages. Sections 4.1 and 5.1 con-\ntain detailed information on the target languages\nand data used. All models are implemented in\nSockeye (Hieber et al., 2017). For hyperparam-\neter settings, we follow the recommendations of\nVaswani et al. (2017).\nWe preprocess our data for all experiments as\nfollows. First, we tokenize and truecase the data\nusing the Moses scripts (Koehn et al., 2007). We\nthen train separate subword vocabularies (Sen-\nnrich et al., 2016) for the source and target lan-\nguages, with 30k merge operations per language.\nWe use the Stanford CoreNLP parser (Manning\net al., 2014) to generate constituency parses of\nthe source (English) sentences, and linearize and\nformat the parses as described in section 2.1.\nWe do not use any monolingual training data;\nhowever, our proposed models are amenable to\nadding monolingual data, and we expect that\nBLEU scores would strongly increase if monolin-\ngual training data were used.\n4 Small-Scale Cross-Lingual\nExperiments\n4.1 Data\nWe use the Europarl Parallel Corpus (Koehn,\n2005) as the basis for our small-scale cross-lingual\nexperiments. We consider translation from En-\nglish (EN) into each of the twenty remaining target\nlanguages; Table 3 contains a full list of the tar-\nget languages, as well as their language families\nor branches. By using this data set, we are able\nto evaluate the usefulness of syntactic information\nfor several relatively diverse target languages, un-\nlike most previous work on syntactic NMT (re-\nviewed in section 7). However, all the languages in\nour experiments are Indo-European or Uralic due\nto using Europarl.\nIn order to facilitate comparison between the\ntarget languages, we follow Cotterell et al. (2018)\nby taking only the intersections of the Europarl\ntraining data. This means that the source (EN) data\nis identical for all experiments, and the targets are\nall translations of each other in the different tar-\nget languages. This results in 170k parallel train-\ning sentences for each language pair. We reserve a\nFamily Language Abbrev.\nBaltic Latvian LV\nLithuanian LT\nGermanic Danish DA\nDutch NL\nGerman DE\nSwedish SV\nHellenic Greek EL\nRomance French FR\nItalian IT\nPortuguese PT\nRomanian RO\nSpanish ES\nSlavic Bulgarian BG\nCzech CS\nPolish PL\nSlovak SK\nSlovene SL\nUralic Estonian ET\nFinnish FI\nHungarian HU\nTable 3: Target languages used in our experiments,\nalong with their language families or branches and their\nabbreviations (abbrev.).\nrandom subset of 10k sentences from the original\ndata to use as development data and an additional\n10k sentences as test data; these development and\ntest sets are not included in the training data.\n4.2 Results\nTable 4 displays BLEU scores on the test data\nfor each target language for the proposed systems.\nThe multi-task system outperforms the baseline\nfor all target languages. In addition, for all but four\ntarget languages (SV , EL, SK, and ET), the multi-\ntask system is at least 1 BLEU point better than the\nbaseline. Thus, our proposed multi-task method\nconsistently improves over a non-syntactic base-\nline across several diverse target languages in low-\nresource scenarios. Additionally, in all cases but\ntwo (EN→LT and EN→ET), multi-task achieves\nthe highest BLEU score of all models.\nThe performance of the mixed encoder system\nin relation to the baseline is less consistent than\nthat of the multi-task system. In most cases, the\nmixed encoder improves only slightly (less than 1\nBLEU) over the baseline, although for LV , LT, RO,\nES, PL, and FI, the improvements are stronger.\nHowever, for four target languages (NL, EL, BG,\n28\nEN→* base mixed enc. multi-task\nLV 26.5 28.1 (+1.6) 28.2 (+1.7)\nLT 23.5 24.6 (+1.1) 24.8 (+1.3)\nDA 39.5 40.1 (+0.6) 40.7 (+1.2)\nNL 28.8 28.7 (-0.1) 30.6 (+1.8)\nDE 30.5 30.6 (+0.1) 32.1 (+1.6)\nSV 35.9 36.4 (+0.5) 36.4 (+0.5)\nEL 38.9 38.8 (-0.1) 39.7 (+0.8)\nFR 38.3 38.5 (+0.2) 40.4 (+2.1)\nIT 31.3 31.3 (==) 32.5 (+1.2)\nPT 39.2 39.3 (+0.1) 40.5 (+1.3)\nRO 36.3 37.8 (+1.5) 37.8 (+1.5)\nES 41.6 43.0 (+1.4) 43.1 (+1.5)\nBG 39.0 38.6 (-0.4) 40.5 (+1.5)\nCS 27.5 28.3 (+0.8) 28.8 (+1.3)\nPL 23.7 24.8 (+1.1) 25.1 (+1.4)\nSK 32.8 32.5 (-0.3) 32.9 (+0.1)\nSL 33.3 34.2 (+0.9) 34.9 (+1.6)\nET 20.2 20.9 (+0.7) 20.8 (+0.6)\nFI 21.5 22.8 (+1.3) 23.3 (+1.8)\nHU 22.3 22.6 (+0.3) 23.4 (+1.1)\nTable 4: BLEU scores on the test set for small-\nscale cross-lingual experiments for the baseline (base),\nmixed encoder (mixed enc.), and multi-task models.\nDifference with the baseline is shown in parentheses.\nand SK), the mixed encoder system does worse\nthan the non-syntactic baseline.\nTarget language family does not seem to have\na noticeable effect on the performance of either\nthe mixed encoder or the multi-task method; this\ncould be due to the fact that the syntactic anno-\ntations were on the source sentence only. It re-\nmains to be seen whether certain source languages\nare particularly amenable to incorporating source\nsyntax in NMT.\n5 Full-Scale WMT Experiments\n5.1 Data\nThe main goal of the previous section was to\nevaluate our proposed syntactic NMT methods\non a wide range of target languages and com-\npare the effect of target language on performance.\nIn this section, we run additional experiments\nin order to evaluate the proposed methods on a\nstandard benchmark. We train our models on\nthe following tasks: English →Turkish (TR) from\nthe WMT18 news translation shared task (Bojar\net al., 2018), English→Romanian WMT16 (Bojar\net al., 2016), and English→German WMT17 (Bo-\nSystem newstest2017 newstest2018\nbaseline 9.6 8.8\nmixed enc. 9.6 (==) 9.3 (+0.5)\nmulti-task 10.6 (+1.0) 10.4 (+1.6)\nTable 5: BLEU scores (and improvement over the\nbaseline) for EN →TR on the test (newstest2017) and\nheld-out (newstest2018) datasets.\njar et al., 2017).\nFor each experiment, we use all available par-\nallel training data from the task, but no monolin-\ngual data. This gives us 200k parallel training sen-\ntences for EN→TR, 600k for EN→RO, and 5.9M\nfor EN→DE. Note that the EN→RO and EN→DE\ntraining corpora contain some overlaps with the\ntraining data in section 4.1, although the experi-\nments in this section use signiﬁcantly more train-\ning data. We validate EN →TR on newstest2016,\nEN→RO on newsdev2016, and EN→DE on new-\nstest2015.\n5.2 Results\nThe results for the EN →TR experiments are dis-\nplayed in Table 5. These results mirror what was\nseen in the previous experiments: the mixed en-\ncoder method gives modest improvements over\nthe non-syntactic baseline (0–0.5 BLEU), while\nthe multi-task method yields the strongest results,\nwith an improvement of 1.0–1.6 BLEU points over\nthe baseline. Although Turkish is not related to\nany of the target languages studied in section 4,\nthe amount of training data for EN→TR is similar\nto what was used in the previous section, which\nmight be one explanation for the similar results.\nTable 6 shows performance of each model on\nthe WMT EN →RO experiments. Here, we see\nmore modest improvements from adding the syn-\ntactic data: only 0.5 BLEU over the baseline for\nboth the mixed encoder and multi-task methods.\nIt is interesting to compare this with the results for\nthe Europarl EN →RO experiments (section 4.2);\nthere, we saw a much larger improvement over the\nbaseline for both multi-task models (1.5 BLEU).\nThis indicates that the effectiveness of these mod-\nels may depend on amount of data (the WMT\nmodels were trained on about three times as much\ntraining data) rather than on target language fam-\nily.\nFinally, we display our WMT EN →DE re-\nsults in Table 7. Here, we see that for very\nhigh-resource EN→DE translation, the multi-task\n29\nSystem newstest2016\nbaseline 21.5\nmixed enc. 22.0 (+0.5)\nmulti-task 22.0 (+0.5)\nTable 6: BLEU scores (and improvement over the\nbaseline) for EN→RO on the test set (newstest2016).\nSystem newstest2016 newstest2017\nbaseline 31.7 25.5\nmixed enc. 31.9 (+0.2) 26.0 (+0.5)\nmulti-task 29.6 (-2.1) 23.4 (-2.1)\nTable 7: BLEU scores (and difference with the base-\nline) for EN→DE on the test (newstest2016) and held-\nout (newstest2017) datasets.\nmethod does much worse than the baseline (by\n2.1 BLEU points). In addition, the mixed encoder\nmethod achieves comparable BLEU scores to the\nbaseline (only 0.2–0.5 BLEU higher). Thus, nei-\nther proposed technique is particularly successful\nfor high-resource EN →DE NMT. Again, we can\ncontrast this with the Europarl EN →DE experi-\nments, where we saw strong improvements from\nthe multi-task model (1.6 BLEU). This lends fur-\nther credence to the hypothesis that these NMT\nmodels with linearized source parses are helpful\ncross-linguistically in low-resource scenarios, but\nnot in high-resource setups.\nWe further investigated the WMT EN →DE\nmulti-task model to ﬁnd reasons for the large drop\nin performance compared to the baseline. We\nfound that while the multi-task model was able to\ngenerate reasonable (albeit lower-quality) transla-\ntions, it did not successfully learn to parse. Dur-\ning parsing inference, the model always output the\nsame parse regardless of the input sentence: (ROOT\n(S (NP ) (VP (NP (NP ) (PP (NP (NP ) (PP (NP ) ) ) )\n) ) ) ). This was a common parse in the training\ndata (it occurred 12k times in the data). This issue\nis partially due to the fact that validation is only\ndone on the translation task, not on the parsing\ntask. However, we do not see this issue with the\nother language pairs and experiments. This failure\nto learn to parse indicates that the WMT EN→DE\nmulti-task model is not able to take advantage of\nthe syntactic annotations.\n6 Validity of Parses\nThe multi-task syntactic NMT models are trained\nboth to translate and to parse the input sentences.\nEN→* % Valid Parses\nLV 96.8%\nLT 99.2%\nDA 70.8%\nNL 93.3%\nDE 87.2%\nSV 95.4%\nEL 85.2%\nFR 92.3%\nIT 78.8%\nPT 89.4%\nRO 96.3%\nES 86.5%\nBG 97.5%\nCS 95.9%\nPL 98.1%\nSK 98.5%\nSL 97.3%\nET 98.2%\nFI 95.1%\nHU 93.6%\nTable 8: Percent of valid parses of the parses generated\nby the Europarl multi-task systems.\nThe main goal of these models has been to im-\nprove translation; those results were reported in\nsections 4.2 and 5.2. In this section, we analyze\nthe validity of the parses produced by the multi-\ntask systems. We use a standard parsing bench-\nmark, WSJ section 23 of the Penn Treebank (Mar-\ncus et al., 1993), as the evaluation dataset in this\nsection. We preprocess this dataset as described in\nsection 3 before using it as the source data for the\nmulti-task systems.\nThe multi-task models were trained to generate\nunlexicalized parses. Since we removed part-of-\nspeech tags from the parses during preprocessing,\nit is not possible to automatically relexicalize the\nparses. This is because there is no one-to-one cor-\nrespondence between the leaves of the parse tree\nand the number of words in the sentence. Thus,\nrather than evaluating the parses directly, we count\nthe number of valid parses (i.e. parses with bal-\nanced parentheses) per target language.\nTable 8 shows the percent of generated parses\nthat were valid for the Europarl multi-task mod-\nels. For most target languages, over 90% of the\ngenerated parses are valid.\nUnlike for the translation results, target lan-\nguage family does seem to have an effect on the\n30\nEN→* % Valid Parses\nTR 86.3%\nRO 99.8%\nDE 100%\nTable 9: Percent of valid parses of the parses generated\nby the WMT multi-task systems.\nparsing results. Overall, Romance, Germanic,\nand Hellenic target language systems generate the\nfewest valid parses. This indicates that Baltic,\nSlavic, and Uralic target languages are most help-\nful in learning to parse English in a multi-task sys-\ntem. Thus, from our cross-lingual experiments, it\nseems that the parsing performance of a multi-task\nsystem depends on the target language, whereas\nwe saw in the previous sections that the transla-\ntion success depends more on the amount of train-\ning data. Note, however, some caveats: 1) we did\nnot perform validation on the parsing task (only\non the translation task), and 2) we are measuring\nonly parsing validity here, rather than parsing per-\nformance.\nTable 9 shows the percent of valid parses for the\nthree WMT multi-task experiments. For EN→DE,\nall of the generated parses are valid because they\nare all identical (as dicussed in section 5.2). For\nEN→RO, nearly all the parses are valid as well.\nHowever, this language pair did not have the same\nissue as EN →DE: the parses generated for each\nsentence were different, and a manual analysis in-\ndicated that the generated EN →RO parses were\nreasonable. The EN →TR system generated a\nlarge amount of valid parses, but fewer than the\nEN→RO system; it is possible that the EN →TR\nsystem would have done better with more training\ndata.\n7 Related Work\nThe performance of many RNN-based NMT\nparadigms has been improved by adding explicit\nsyntactic annotations, particularly on the source\nside; we review some syntactic NMT models here.\nThis paper is, along with Wu et al. (2018) and\nZhang et al. (2019), among the ﬁrst to add explicit\nsyntax to Transformer-based NMT.\n7.1 Linearized Parses in Neural Networks\nIn this work, we use linearized parse trees to add\nsyntax into the Transformer. Vinyals et al. (2015)\nand Choe and Charniak (2016) introduced the idea\nof linearizing parse trees for neural parsing. Lin-\nearized parses are advantageous because they can\nbe used anywhere that standard sequences can be\nused; in fact, Vaswani et al. (2017) showed that\nthey can also be used by the Transformer to learn\nconstituency parsing. Here, we leverage this idea\nby using linearized parses as an additional signal\nfor the Transformer during NMT training.\n7.2 Syntactic NMT with Modiﬁed Encoder\nThere have been several recent proposals to incor-\nporate source-side syntax into RNN-based NMT\nby modifying the encoder architecture; we re-\nview some such models here. Eriguchi et al.\n(2016) augmented the RNN encoder with a tree-\nLSTM (Tai et al., 2015) to read in source-side\nHPSG parses, and combined this with a standard\nRNN decoder. Similarly, Bastings et al. (2017)\nused a graph convolutional encoder in combina-\ntion with an RNN decoder to translate from de-\npendency parsed source sentences. Although these\nmodels improved over non-syntactic RNN-based\nNMT systems, they relied heavily on parsed data\nduring both training and inference, whereas our\nmodels are able to translate unparsed data. In ad-\ndition, it is not clear how to incorporate such im-\nprovements into the state-of-the-art Transformer\narchitecture.\n7.3 Linearized Parses in NMT\nThis work ﬁts with another line of research that\nuses linearized parses to incorporate syntax into\nneural machine translation without requiring a\nspeciﬁc NMT architecture. Luong et al. (2016)\nused a single encoder and different decoders to\ntrain two tasks: parsing the source sentence and\ntranslating from source to target. Kiperwasser\nand Ballesteros (2018) also applied multi-task\nlearning to syntactic NMT; they used a shared\nRNN decoder for translation, dependency parsing,\nand part-of-speech tagging and evaluated differ-\nent scheduling techniques to combine the tasks.\nOur multi-task system builds off these two papers\nby training a joint NMT and parsing model us-\ning a single encoder and decoder in a Transformer\nframework, and further evaluates the multi-task\nframework on several language pairs.\nCurrey and Heaﬁeld (2018) leveraged a multi-\nsource NMT system to learn to translate from both\nunparsed and parsed source sentences. Wu et al.\n(2018) similarly combined the standard bidirec-\ntional encoder with two additional encoders, one\n31\nthat encoded the pre-order traversal of the de-\npendency parse of the sentence and one that en-\ncoded the post-order traversal. Unlike Currey and\nHeaﬁeld (2018), they joined the encoders on the\nword level and used a Transformer architecture.\nOur mixed encoder model is similar to these but\ninstead uses a single Transformer encoder for both\nparsed and unparsed source sentences.\nThe mixed RNN encoder model of Li et al.\n(2017) is also similar to our mixed encoder model;\ntheir model used an RNN to encode a linearized\nparse of a source sentence, but attended only to\nthe words of the parse. Our mixed encoder model\nis trained on both linearized parses and unparsed\nsentences, but for the linearized parses we attend\nto words and to parse labels. Zhang et al. (2019)\nused syntax to augment the word representations\nin both RNN-based and Transformer-based NMT;\nthis was done by concatenating the hidden states of\na dependency parser with the NMT word embed-\ndings. Their method is complementary to ours and\ncould be used along with our multi-task or mixed\nencoder models to enhance any NMT architecture.\nIn this work, we have concentrated on source-\nside syntax, but linearized parses have also been\npopular for incorporating target syntax into neu-\nral machine translation. Aharoni and Goldberg\n(2017) and Nadejde et al. (2017) both trained\nRNN-based neural machine translation systems to\ntranslate from sequential source sentences into lin-\nearized parses of target sentences; this could also\nbe done using a Transformer.\n8 Conclusions\nIn this paper, we proposed two methods for in-\ncorporating source-side syntactic annotations into\na Transformer-based neural machine translation\nsystem. The ﬁrst, multi-task, used a shared en-\ncoder and decoder to train two tasks: transla-\ntion and constituency parsing. The second, mixed\nencoder, learned to translate linearized parses of\nthe source sentences as well as unparsed source\nsentences directly into the target language. We\nperformed experiments from English into twenty\ntarget languages in a low-resource setup; the\nmulti-task system improved over the non-syntactic\nbaseline for all target languages. We further\ndemonstrated the success of this method on the\nEN→TR and EN →RO WMT datasets; however,\nfor the very high-resource EN →DE WMT setup,\nthe multi-task model performed poorly, while the\nmixed encoder model did only marginally better\nthan the non-syntactic baseline.\nIn the future, we plan on extending these\ntechniques to incorporate target-side syntax into\nTransformer-based NMT. In addition, we would\nlike to experiment with different source languages\nin order to ﬁnd out whether adding source-side\nsyntax has a greater effect on some source lan-\nguages than others. It would also be interesting to\nexperiment with a multi-task, multilingual NMT\nframework with multiple target languages.\nReferences\nRoee Aharoni and Yoav Goldberg. 2017. Towards\nstring-to-tree neural machine translation. In Pro-\nceedings of the 55th Annual Meeting of the ACL,\npages 132–140. Association for Computational Lin-\nguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations.\nJoost Bastings, Ivan Titov, Wilker Aziz, Diego\nMarcheggiani, and Khalil Sima’an. 2017. Graph\nconvolutional encoders for syntax-aware neural ma-\nchine translation. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1957–1967. Association for Com-\nputational Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara\nLogacheva, Christof Monz, Matteo Negri, Matt\nPost, Raphael Rubino, Lucia Specia, and Marco\nTurchi. 2017. Findings of the 2017 Conference\non Machine Translation (WMT17). In Proceedings\nof the Second Conference on Machine Translation,\npages 169–214. Association for Computational Lin-\nguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Aur ´elie\nN´ev´eol, Mariana Neves, Martin Popel, Matt Post,\nRaphael Rubino, Carolina Scarton, Lucia Spe-\ncia, Marco Turchi, Karin Verspoor, and Marcos\nZampieri. 2016. Findings of the 2016 Conference\non Machine Translation. In Proceedings of the First\nConference on Machine Translation, pages 131–\n198. Association for Computational Linguistics.\nOndˇrej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Philipp Koehn, and\nChristof Monz. 2018. Findings of the 2018 Con-\nference on Machine Translation (WMT18). In Pro-\nceedings of the Third Conference on Machine Trans-\n32\nlation, pages 272–303. Association for Computa-\ntional Linguistics.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1724–1734. Asso-\nciation for Computational Linguistics.\nDo Kook Choe and Eugene Charniak. 2016. Pars-\ning as language modeling. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2331–2336. Associa-\ntion for Computational Linguistics.\nRyan Cotterell, Sebastian J. Mielke, Jason Eisner, and\nBrian Roark. 2018. Are all languages equally hard\nto language-model? In Proceedings of NAACL-\nHLT, pages 536–541. Association for Computa-\ntional Linguistics.\nAnna Currey and Kenneth Heaﬁeld. 2018. Multi-\nsource syntactic neural machine translation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2961–\n2966. Association for Computational Linguistics.\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa\nTsuruoka. 2016. Tree-to-sequence attentional neu-\nral machine translation. In Proceedings of the 54th\nAnnual Meeting of the ACL, pages 823–833. Asso-\nciation for Computational Linguistics.\nFelix Hieber, Tobias Domhan, Michael Denkowski,\nDavid Vilar, Artem Sokolov, Ann Clifton, and Matt\nPost. 2017. Sockeye: A toolkit for neural machine\ntranslation. arXiv preprint arXiv:1712.05690.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi´egas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339–351.\nEliyahu Kiperwasser and Miguel Ballesteros. 2018.\nScheduled multi-task learning: From syntax to\ntranslation. Transactions of the Association for\nComputational Linguistics, 6:225–240.\nPhilipp Koehn. 2005. Europarl: A parallel corpus\nfor statistical machine translation. In 10th Machine\nTranslation Summit, volume 5, pages 79–86.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the ACL,\npages 177–180. Association for Computational Lin-\nguistics.\nJunhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min\nZhang, and Guodong Zhou. 2017. Modeling source\nsyntax for neural machine translation. In Proceed-\nings of the 55th Annual Meeting of the ACL, pages\n688–697. Association for Computational Linguis-\ntics.\nMinh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol\nVinyals, and Lukasz Kaiser. 2016. Multi-task se-\nquence to sequence learning. In 4th International\nConference on Learning Representations.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Rose Finkel, Steven Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP natural lan-\nguage processing toolkit. In Proceedings of the\n52nd Annual Meeting of the ACL, pages 55–60. As-\nsociation for Computational Linguistics.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19(2):313–330.\nMaria Nadejde, Siva Reddy, Rico Sennrich, Tomasz\nDwojak, Marcin Junczys-Dowmunt, Philipp Koehn,\nand Alexandra Birch. 2017. Predicting target lan-\nguage CCG supertags improves neural machine\ntranslation. In Proceedings of the Second Confer-\nence on Machine Translation, pages 68–79. Associ-\nation for Computational Linguistics.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An\nanalysis of encoder representations in Transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n287–297. Association for Computational Linguis-\ntics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the ACL, pages 1715–1725. Association\nfor Computational Linguistics.\nKai Sheng Tai, Richard Socher, and Christopher Man-\nning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. In Proceedings of the 53rd Annual Meeting\nof the ACL and the 7th International Joint Confer-\nence on Natural Language Processing, pages 1556–\n1566. Association for Computational Linguistics.\nGongbo Tang, Mathias M¨uller, Annette Rios, and Rico\nSennrich. 2018. Why self-attention? A targeted\nevaluation of neural machine translation architec-\ntures. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 4263–4272. Association for Computa-\ntional Linguistics.\nKe Tran, Arianna Bisazza, and Christof Monz. 2018.\nThe importance of being recurrent for modeling hi-\nerarchical structure. In Proceedings of the 2018\n33\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 4731–4736. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008.\nOriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,\nIlya Sutskever, and Geoffrey Hinton. 2015. Gram-\nmar as a foreign language. In Advances in Neu-\nral Information Processing Systems 28, pages 2773–\n2781.\nShuangzhi Wu, Dongdong Zhang, Zhirui Zhang,\nNan Yang, Mu Li, and Ming Zhou. 2018.\nDependency-to-dependency neural machine transla-\ntion. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 26(11):2132–2141.\nMeishan Zhang, Zhenghua Li, Guohong Fu, and Min\nZhang. 2019. Syntax-enhanced neural machine\ntranslation with syntax-aware word representations.\nIn Proceedings of NAACL, pages 1151–1161. Asso-\nciation for Computational Linguistics."
}