{
  "title": "Detail-Preserving Transformer for Light Field Image Super-resolution",
  "url": "https://openalex.org/W4221145747",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2309621673",
      "name": "Shunzhou Wang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2163744986",
      "name": "Tianfei Zhou",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2103997928",
      "name": "Yao Lu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2157295507",
      "name": "Huijun Di",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2309621673",
      "name": "Shunzhou Wang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2163744986",
      "name": "Tianfei Zhou",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2103997928",
      "name": "Yao Lu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2157295507",
      "name": "Huijun Di",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2892310177",
    "https://openalex.org/W6786585107",
    "https://openalex.org/W6736170873",
    "https://openalex.org/W54257720",
    "https://openalex.org/W2783741789",
    "https://openalex.org/W2591697814",
    "https://openalex.org/W3014252997",
    "https://openalex.org/W2242218935",
    "https://openalex.org/W2783178946",
    "https://openalex.org/W2735224642",
    "https://openalex.org/W3013686555",
    "https://openalex.org/W3193659426",
    "https://openalex.org/W6704184719",
    "https://openalex.org/W2572840108",
    "https://openalex.org/W4206244656",
    "https://openalex.org/W3121709906",
    "https://openalex.org/W3122412340",
    "https://openalex.org/W2800554784",
    "https://openalex.org/W2996053156",
    "https://openalex.org/W3111051040",
    "https://openalex.org/W1585668122",
    "https://openalex.org/W3033492948",
    "https://openalex.org/W2903622424",
    "https://openalex.org/W2588196171",
    "https://openalex.org/W2239879258",
    "https://openalex.org/W2884462366",
    "https://openalex.org/W2947690922",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W3049318984",
    "https://openalex.org/W4200017248",
    "https://openalex.org/W3119381934",
    "https://openalex.org/W3169612303",
    "https://openalex.org/W3194042166",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W3110151525",
    "https://openalex.org/W2345834824",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W3035271082",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3035557850"
  ],
  "abstract": "Recently, numerous algorithms have been developed to tackle the problem of light field super-resolution (LFSR), i.e., super-resolving low-resolution light fields to gain high-resolution views. Despite delivering encouraging results, these approaches are all convolution-based, and are naturally weak in global relation modeling of sub-aperture images necessarily to characterize the inherent structure of light fields. In this paper, we put forth a novel formulation built upon Transformers, by treating LFSR as a sequence-to-sequence reconstruction task. In particular, our model regards sub-aperture images of each vertical or horizontal angular view as a sequence, and establishes long-range geometric dependencies within each sequence via a spatial-angular locally-enhanced self-attention layer, which maintains the locality of each sub-aperture image as well. Additionally, to better recover image details, we propose a detail-preserving Transformer (termed as DPT), by leveraging gradient maps of light field to guide the sequence learning. DPT consists of two branches, with each associated with a Transformer for learning from an original or gradient image sequence. The two branches are finally fused to obtain comprehensive feature representations for reconstruction. Evaluations are conducted on a number of light field datasets, including real-world scenes and synthetic data. The proposed method achieves superior performance comparing with other state-of-the-art schemes. Our code is publicly available at: https://github.com/BITszwang/DPT.",
  "full_text": "Detail-Preserving Transformer for Light Field Image Super-resolution\nShunzhou Wang1\u0003, Tianfei Zhou2\u0003, Yao Lu1y, Huijun Di1\n1 Beijing Key Laboratory of Intelligent Information Technology,\nSchool of Computer Science and Technology, Beijing Institute of Technology, China\n2 Computer Vision Laboratory, ETH Zurich, Switzerland\nfshunzhouwang, vis yl, ajong@bit.edu.cn, tianfei.zhou@vision.ee.ethz.ch\nAbstract\nRecently, numerous algorithms have been developed to tackle\nthe problem of light ﬁeld super-resolution (LFSR),i.e., super-\nresolving low-resolution light ﬁelds to gain high-resolution\nviews. Despite delivering encouraging results, these ap-\nproaches are all convolution-based, and are naturally weak in\nglobal relation modeling of sub-aperture images necessarily\nto characterize the inherent structure of light ﬁelds. In this\npaper, we put forth a novel formulation built upon Trans-\nformers, by treating LFSR as a sequence-to-sequence recon-\nstruction task. In particular, our model regards sub-aperture\nimages of each vertical or horizontal angular view as a se-\nquence, and establishes long-range geometric dependencies\nwithin each sequence via a spatial-angular locally-enhanced\nself-attention layer, which maintains the locality of each sub-\naperture image as well. Additionally, to better recover image\ndetails, we propose a detail-preserving Transformer (termed\nas DPT), by leveraging gradient maps of light ﬁeld to guide\nthe sequence learning. DPT consists of two branches, with\neach associated with a Transformer for learning from an orig-\ninal or gradient image sequence. The two branches are ﬁnally\nfused to obtain comprehensive feature representations for re-\nconstruction. Evaluations are conducted on a number of light\nﬁeld datasets, including real-world scenes and synthetic data.\nThe proposed method achieves superior performance com-\nparing with other state-of-the-art schemes. Our code is pub-\nlicly available at: https://github.com/BITszwang/DPT.\nIntroduction\nLight ﬁeld (LF) imaging systems offer powerful capabilities\nto capture the 3D information of a scene, and thus enable\na variety of applications going from photo-realistic image-\nbased rendering to vision applications such as depth sens-\ning, refocusing, or saliency detection. However, current light\nﬁeld cameras naturally face a trade-off between the angular\nand spatial resolution, that is, a camera capturing views with\na high angular sampling typically at the expense of a limited\nspatial resolution, and vice versa. This limits the practical\napplications of LF, and also motivates many efforts to study\nsuper-resolution along the angular dimension (i.e., to syn-\n\u0003The ﬁrst two authors contribute equally to this work.\nyCorresponding author\nCopyright c\r2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nthesize new views) or the spatial dimension (i.e., to increase\nthe spatial resolution). Our work focuses on the latter one.\nIn comparison with the traditional 2D photograph that\nonly records the spatial intensity of light rays, a light ﬁeld\nadditionally collects the radiance of light rays along with\ndifferent directions, offering a multi-view description of\nthe scene. A naive solution of light ﬁeld super-resolution\n(LFSR) is to super-resolve each view independently us-\ning single image super-resolution (SISR) techniques. How-\never, despite the recent progress of SISR, the solution is\nsub-optimal mainly because it neglects the intrinsic re-\nlations (i.e., angular redundancy) of different light ﬁeld\nviews, possibly resulting in angularly inconsistent recon-\nstructions. To address this, many studies exploit comple-\nmentary information captured by different sub-aperture im-\nages for high-quality reconstruction. The seminal learning-\nbased method, i.e., (Yoon et al. 2015), directly stacks 4-\ntuples of sub-aperture images together as an input of a SISR\nmodel. Subsequent efforts develop more advanced tech-\nniques, e.g., to explore the geometric property of a light ﬁeld\nin multiple network branches (Zhang, Lin, and Sheng 2019),\nto align the features of the center view and its surrounding\nviews with deformable convolutions (Wang et al. 2021c), to\nencourage interactions between spatial and angular features\nfor more informative feature extraction (Wang et al. 2020),\nto combinatorially learn correlations between an arbitrary\npair of views for super-resolution (Jin et al. 2020), or to gain\nan efﬁcient low-rank light ﬁeld representation for restora-\ntion (Farrugia and Guillemot 2019).\nDespite the encouraging results of these approaches, they\nare all based on convolutional network architectures, thus\ninherently lack strong capabilities to model global relations\namong different views of light ﬁeld images. In light of the\nill-posed nature of the super-resolution problem, we believe\nthat an ideal solution should take into account as much in-\nformative knowledge in the LR input as possible.\nMotivated by the above analysis, we propose, to the best\nof our knowledge, the ﬁrst Transformer-based model to ad-\ndress LFSR from a holistic perspective. In stark contrast to\nexisting approaches, our model treats each light ﬁeld as a\ncollection of sub-aperture image (SAI) sequences (captured\nalong horizontal or vertical directions), and exploits self-\nattention to reveal the intrinsic geometric structure of each\nsequence. Despite the advantages of Transformers in long-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2522\nrange sequence modeling, it is non-trivial to apply vanilla\nTransformers (e.g., ViT (Dosovitskiy et al. 2020)) for super-\nresolution tasks, mainly because 1) they represent each in-\nput image with small-size (e.g., 16\u000216 or 32\u000232) patches\n(i.e., tokens), which may damage fundamental structures\n(e.g., edges, corners or lines) in images, and 2) the vanilla\nfully connected self-attention design focus on establishing\nlong-range dependencies among tokens, however, ignoring\nthe locality in the spatial dimension. To address these prob-\nlems, we introduce spatial-angular locally-enhanced self-\nattention (SA-LSA), which strengths locality using convolu-\ntions ﬁrst, then promotes non-local spatial-angular depen-\ndencies of each SAI sequence.\nBased on SA-LSA, we design a novel Transformer-based\nLFSR model, i.e., DPT, which simultaneously captures lo-\ncal structures within each SAI and global structures of all\nSAIs in the light ﬁeld. Concretely, DPT consists of a con-\ntent Transformer and a gradient Transformer to learn spatial-\nangular dependencies within each light ﬁeld and the corre-\nsponding gradient images, respectively. It is also equipped\nwith a cross-attention fusion Transformer to aggregate fea-\nture representations of the two branches, from which a high-\nresolution light ﬁeld is reconstructed.\nIn a nutshell, our contributions are three-fold:1) We refor-\nmulate the problem of LFSR from a sequence-to-sequence\nlearning perspective, which is differentiated to prior works\nin a sense that it fully explores non-local contextual in-\nformation among all sub-aperture images, better character-\nizing geometric structures of light ﬁelds; 2) We design a\nspatial-angular locally-enhanced self-attention layer, which,\nin comparison with its vanilla fully-connected counterparts,\noffers our Transformer a strong ability to maintain crucial lo-\ncal context within light ﬁelds; 3) We ﬁnally introduce DPT\nas a novel Transformer-based architecture, which not only\nmines non-local contexts from multiple views, but also pre-\nserving image details for each single view. Our DPT demon-\nstrates the promising performance on multiple benchmarks,\nwhile maintaining similar network parameters and compu-\ntational cost as existing convolution-based networks.\nRelated Work\nLight Field Super-Resolution. Early deep learning-based\nmethods usually respectively learn the spatial and angular\ninformation with two independent subnetworks: one sub-\nnetwork captures the spatial information and another one\nlearns the angular information. For example, (Yoon et al.\n2017) adopted the SRCNN (Dong et al. 2014) to separately\nprocess each SAI for spatial super-resolution and interpo-\nlated novel views for angular super-resolution with the an-\ngular super-resolution network. (Yuan, Cao, and Su 2018)\nutilized a single image super-resolution network to enlarge\nthe spatial resolution of each SAI, and applied the proposed\nEPI enhancement network to restore the geometric consis-\ntency of different SAIs. Recently, many researchers seek\nto simultaneously capture the spatial and angular informa-\ntion with a uniﬁed framework. (Wang et al. 2018) built a\nhorizontal and a vertical recurrent network to respectively\nsuper-resolve 3D LF data. (Zhang, Lin, and Sheng 2019)\nstacked the SAIs from different angular directions as inputs\nand sent them to a multi-branch network to capture the spa-\ntial and angular information. (Wang et al. 2021c) used the\ndeformable convolution (Dai et al. 2017) to align and aggre-\ngate the center-view and surrounding-view features to con-\nduct LFSR. (Wang et al. 2020) developed a spatial-angular\ninteraction network to learn the spatial-angular information\nfrom the macro-pixel image constructed with different SAIs.\nHowever, a major drawback of these approaches is that they\nfail to consider the long-range dependency among multi-\nple SAIs in learning rich spatial-angular representations. To\naddress this issue, we propose a Transformer based LFSR\nmodel, in which three proposed Transformers are leveraged\nto establish the non-local relationship of different SAIs for\nmore effective representation learning.\nTransformer for Image Super-Resolution. Attention-\nbased models have demonstrated great successes in diverse\nvision tasks (Wang et al. 2022; Zhou et al. 2020; Mou et al.\n2021; Zhou et al. 2022; Wang et al. 2021b,a; Zhou et al.\n2021) due to their powerful representative abilities. Some\nattention-based methods have been recently proposed to ad-\ndress the super-resolution tasks. These works can be roughly\ngrouped into two categories: Transformer for single image\nsuper-resolution and Transformer for multiple image super-\nresolution. The former one mainly uses the Transformer\nto mine the intra-frame long-range dependency for high\nquality image reconstruction, i.e., IPT (Chen et al. 2021),\nSwinIR (Liang et al. 2021), and ESRT (Lu et al. 2021).\nThe latter one adopts the Transformer to explore the inter-\nframe context information for accurate image reconstruc-\ntion. i.e.,TTSR (Yang et al. 2020) for reference-based im-\nage super-resolution and VSR (Cao et al. 2021) for video\nsuper-resolution. Motivated by these approaches, we devise\nthe ﬁrst Transformer based architecture for LFSR.\nOur Approach\nProblem Formulation. We treat the task of LFSR as a high-\ndimensional reconstruction problem, in which each LF is\nrepresented as a 2D angular collection of sub-aperture im-\nages (SAIs). Formally, considering an input low-resolution\nLF as LLR 2RU×V×H×W with angular resolution ofU\u0002V\nand spatial resolution of H\u0002W, LFSR aims at reconstruct-\ning a super-resolved LF LHR 2RU×V×\u000bH×\u000bW with \u000bthe\nupsampling factor. Following (Yeung et al. 2018; Zhang,\nLin, and Sheng 2019; Wang et al. 2020, 2021c), we con-\nsider the case that SAIs distribute in a square array, i.e.,\nU = V = A, where Aindicates the angular resolution along\nthe horizontal or vertical direction.\nNetwork Overview. The overall architecture of DPT is\nshown in Fig. 1. Given LLR as the network input, we com-\npute a gradient map for each 2D SAI, and organize them to-\ngether as a gradient ﬁeld GLR 2RA×A×H×W. LLR and GLR\nare separately fed into two small CNNs for convolutional\nfeature extraction, following by two unimodal Transform-\ners (i.e., a content Transformer and a gradient Transformer)\nto learn richer feature representations. To better learn rele-\nvant visual patterns among different SAIs, we treat LLR (or\nGLR) as a collection of A horizontal and A vertical angu-\nlar sequences, and each sequence includes A consecutive\n2523\nC\nC\nCConcatenationHorizontal SA-LSAVerticalSA-LSAContent Transformer\nGradient Transformer\nFusionTransformer\nECont\nEGrad\nLLR\nGLR\nFCont\nFGrad\nHCont\nHGrad\nDReco\nLHR\nFigure 1: Detailed architecture of DPT for light ﬁeld image super-resolution. Given an input light ﬁeld LLR and its gradient\nﬁeld GLR, our DPT leverages two separate convolutional networks ( i.e., ECont and EGrad) for low-level feature extraction. The\nfeatures are subsequently fed into a content TransformerTCont and a gradient TransformerTGrad, respectively, to explore global\ncontextual information across multiple SAIs. Next, their outputs are aggregated via a cross-attention fusion Transformer TFuse\nto learn detail-preserved feature representations. Finally, the image reconstruction module DReco is utilized to generate the\nsuper-resolve results.\nSAIs collected along one direction. Our content (or gradi-\nent) Transformer processes these sequences inLLR (or GLR)\none by one to avoid expensive computations as well as re-\ndundant interactions among irrelevant SAIs. Next, DPT ag-\ngregates the output features of the two Transformers via a\ncross-attention fusion Transformer, yielding a more compre-\nhensive representation that is able to well preserve image\ndetails, which in ﬁnal leads to better reconstruction.\nIn the following, we ﬁrst provide a detailed description\nof DPT. Then, we elaborate on the proposed spatial-angular\nlocally-enhanced self-attention layer, which is an essential\ncomponent of our Transformer.\nDetail-Preserving Transformer (DPT)\nConvolutional Feature Extraction.Given the light ﬁeld in-\nput LLR as well as its gradient ﬁeld GLR, two CNNs ECont\nand EGrad are leveraged to separately extract\nFCont = ECont(LLR) 2RA\u0002A\u0002C\u0002H\u0002W;\nFGrad = EGrad(GLR) 2RA\u0002A\u0002C\u0002H\u0002W;\n(1)\nper-SAI embeddings FCont and FGrad at spatial resolution\nH\u0002W and with embedding channel C.\nContent and Gradient Transformer. The convolutional\nfeature embeddings (i.e., FCont and FGrad) capture local con-\ntext within each SAI independently but lack global context\nacross different SAIs. We use Transformers (Dosovitskiy\net al. 2020) to enrich the embeddings with sequence-level\ncontext. We start by learning unimodal contextualized rep-\nresentations TCont and TGrad using a content Transformer\nTCont and a gradient Transformer TGrad:\nTCont = TCont(FCont) 2RA\u0002A\u0002C\u0002H\u0002W;\nTGrad = TGrad(FGrad) 2RA\u0002A\u0002C\u0002H\u0002W:\n(2)\nNote that the two transformers share the same network struc-\nture. For simplicity, we only describeTCont in the following.\nThe TCont is comprised of K spatial-angular attention\nblocks. Each block includes two consecutive SA-LSA layers\n(detailed in the next section), which exploit global relations\nwithin each horizontal or vertical sequence of the input LF\nimage, respectively.\nIn particular, in the k-th block, we treat its input as a\nset of horizontal (or row-wise) sequences, i.e., R= fRi 2\nRA×C×H×WgA\ni=1, where Ri indicates a sequence of convo-\nlutional features, corresponding to the i-th row in the input.\nThe ﬁrst horizontal SA-LSA layer H1\nk aims to explore the\ndependencies within each sequence independently. Speciﬁ-\ncally, for each horizontal sequenceRi, we obtain a non-local\nrepresentation ^Ri as follows:\n^Ri = H1\nk(Ri) 2RA\u0002C\u0002H\u0002W: (3)\nAfter processing all horizontal sequences in R, we obtain\na horizontal-enhanced content representation for the light\nﬁeld:\nRCont\nk = [^R1; ^R2; \u0001\u0001\u0001 ; ^RA] 2RA\u0002A\u0002C\u0002H\u0002W; (4)\nwhere ‘[ ]’ denotes the concatenation operation.\nNext, the second vertical SA-LSA layerH2\nk accepts RCont\nk\nas input, and explores the long-range relations of vertical (or\ncolumn-wise) sequences, i.e., C= fTi2RA×C×H×WgA\ni=1,\nwhere Ti indicates sequences of convolutional features, cor-\nresponding to the i-th column in RCont\nk . Similarly, the ver-\ntical sequences in Care transformed via H2\nk to produce a\nvertical-enhanced content representation TCont\nk :\nTCont\nk = [^T1; ^T2; \u0001\u0001\u0001 ; ^TA] 2RA\u0002A\u0002C\u0002H\u0002W; (5)\nwhere ^Ti = H2\nk(Ti) is the non-local representation of\nTi. Our content Transformer uses K spatial-angular atten-\ntion blocks to aggregate informative contextual knowledge\namong SAIs, eventually yielding a more comprehensive\ncontent representation TCont\nK .\nOur gradient TransformerTGrad performs in a similar way\nas the content Transformer. It acceptsFGrad as its input, and\nutilizes K spatial-angular attention blocks to deliver a gra-\ndient representation TGrad\nK .\nCross-Attention Fusion Transformer. While the content\nand gradient Transformers process each modality separately,\nwe design a fusion Transformer to aggregate together their\n2524\nrepresentations. To obtain more comprehensive unimodal\nrepresentations, we obtain the inputs of fusion Transformer\nby combining all intermediate features in the content or gra-\ndient Transformers:\nHCont =[FCont; TCont\n1 ; \u0001\u0001\u0001; TCont\nK ] 2RA\u0002A\u0002(K+1)C\u0002H\u0002W;\nHGrad =[FGrad; TGrad\n1 ; \u0001\u0001\u0001; TGrad\nK ] 2RA\u0002A\u0002(K+1)C\u0002H\u0002W;\n(6)\nDifferent from TCont and TGrad which capture non-local\ndependencies of tokens within the same sequence, our fu-\nsion Transformer aims to explore the relations between to-\nkens of two sequences. In particular, for the i-th horizon-\ntal (or vertical) sequences in HCont and HGrad, i.e., Ui 2\nRA×(K+1)C×H×W and Vi 2RA×(K+1)C×H×W, the TFuse\nachieves a detail-preserved representation Zi as:\nZi = TFuse(Ui; Vi) 2RA\u0002(K+1)C\u0002H\u0002W: (7)\nThe TFuse performs cross-attention between its inputs,\nwith query generated from Ui, key and value from Vi,\nto gather high-frequency information as a compensation\nof the content representation. We concatenate the out-\nputs fZigi together to obtain the fusion output Z 2\nRA×A×(K+1)C×H×W. Note that our fusion Transformer\nonly includes one spatial-angular attention block, and we see\nminor performance improvement when adding additional\nblocks.\nSAI Reconstruction. Finally, we leverage a reconstruction\nmodule DReco over Z to obtain a high-resolved LF LHR:\nLHR = DReco(Z) 2RA\u0002A\u0002\u000bH\u0002\u000bW: (8)\nHere, DReco is separately applied to each SAI.\nRemark. Our DPT employs Transformers to enrich convo-\nlutional features FCont and FGrad (Eq. 2) into richer repre-\nsentations HCont and HGrad, respectively, which are further\naggregated together via a fusion Transformer. In this man-\nner, our network is able to collect informative non-local con-\ntexts within each SAI and across different SAIs, allowing for\nhigher-quality reconstruction.\nIn addition, our Transformers process each sequence (or\nsequence fusion) independently rather than directly process\nall sequences together. This enables our model to meet hard-\nware resource constraints, and more importantly, avoid re-\ndundant interactions among irrelevant sub-aperture images.\nIn Transformer architectures (e.g.,ViT (Dosovitskiy et al.\n2020)), fully-connected self-attention is extensively em-\nployed to explore non-local interactions among input tokens.\nHowever, the vanilla self-attention layer neglects local spa-\ntial context within each input token, which is crucial for im-\nage reconstruction. To remedy this limitation, we introduce a\nspatial-angular locally-enhanced self-attention layerto of-\nfer our Transformer a strong capability in modeling locality.\nSpatial-Angular Locally-Enhanced Self-Attention\nInspired by VSR (Cao et al. 2021), our SA-LSA layer in-\ncludes three sequential operations: spatial-angular convolu-\ntional tokenization, spatial-angular self-attention, as well as\nspatial-angular convolutional de-tokenization. Its structure\nis illustrated in Fig. 2 (a).\n(a)\n(b)\nF\nfQ\nfK\nfV\nU\nU\nU\nQ\nK\nV\nP fP ^F\nUi\nVi\nfQ\nfK\nfV\nU\nU\nU\nQ\nK\nV\nP fP ^Ui\nFigure 2: Illustration of (a) the proposed SA-LSA layer (for\ncontent / gradient Transformer) as well as (b) cross-attention\nSA-LSA layer (for fusion Transformer).\nand \bdenote the\nelement-wise multiplication and summation operations, re-\nspectively.\nSpatial-Angular Convolutional Tokenization.ViT-like ar-\nchitectures (Dosovitskiy et al. 2020) leverage a linear pro-\njection layer to achieve input tokens at a very early stage.\nIn contrast, our proposed convolutional tokenization mech-\nanism obtains the tokens in the self-attention layer, yielding\na multi-stage hierarchy like CNNs. This allows our Trans-\nformer to capture local contexts from low-level features to\nhigh-level semantic representations.\nIn particular, denote F 2 RA×C×H×W as the spatial-\nangular representation of an angular sequence, where A is\nthe length of the sequence,Cdenotes feature dimension, and\nH\u0002W is the spatial dimension. The convolutional tokeniza-\ntion module generates query Q, key K and value V at each\nself-attention layer as follows:\nQ = U(fQ(F)); K = U(fK(F)); V = U(fV(F)): (9)\nIt produces the inputs of a self-attention layer via two steps.\nFirst, the input F is fed into three independent convolutional\nlayers (i.e., fQ, fK and fV). We implement each layer with\nkernel size 1 \u00021.\nNext, a function Uis designed to obtain spatial-angular\ntokens. In particular, it extracts a collection of overlapping\npatches with size Hp \u0002Wp. Thus, we are able to obtain a\nsequence X 2Rn×d of n tokens, and each token with a\nfeature dimension d= C\u0002Hp \u0002Wp.\nSpatial-Angular Self-Attention. The fully-connected self-\nattention layer is applied on X to explore the non-local\nspatial-angular relations among the tokens as follows:\nX0=Attention(Q; K; V )= softmax(QK>)V ; (10)\nwhere Attention denotes a standard self-attention layer\nas in (Vaswani et al. 2017; Dosovitskiy et al. 2020).\n2525\nSpatial-Angular Convolutional De-Tokenization. To en-\nable the application of convolutional tokenization in each\nself-attention layer, we further de-tokenize the attended fea-\nture sequence X′to fold the patches into a large feature map\n^F with the same dimension as F:\n^F = fP(P(X0)) +F 2RA\u0002C\u0002H\u0002W; (11)\nwhere Pdenotes the de-tokenization operation, and fP is a\nconvolutional layer as Eq. 9. A residual layer is also used\nto avoid the loss of important features. Here, ^F well en-\ncodes the local (in the spatial dimension) and global (in both\nspatial and angular dimensions) context of the input angular\nsequence, which could be expected to help produce better\nreconstruction results.\nNote that in the fusion Transformer, we improve SA-LSA\ninto a cross-attention SA-LSA layer to support the compu-\ntation of cross-attention between the two modalities. Fig. 2\nshows the detailed structure.\nDetailed Network Architecture\nConvolutional Feature Extraction. The convolutional\nmodules ECont and EGrad (Eq. 2) share a similar network\nstructure. Following (Wang et al. 2021c), each of them con-\nsists of two residual blocks and two residual atrous spatial\npyramid pooling blocks organized in an intertwine manner,\nfollowing by an angular alignment module without using de-\nformable convolutions.\nTransformer Networks. In the content and gradient Trans-\nformers, we set the number of attention blocks K to 2 by\ndefault. For TFuse, Eq. 9 is slightly modiﬁed to enable the\nexploration of cross-attention by generating query from the\ncontent representation, while key and value from the gradi-\nent representation.\nReconstruction Module. The reconstruction module DReco\n(Eq. 8) is implemented as a cascaded of ﬁve information\nmulti-distillation blocks, following with a upsampling layer,\nwhich consists of a pixel shufﬂe operation combined with\ntwo 1 \u00021 convolution layers, to generate the super-resolved\nSAIs (Wang et al. 2021c).\nExperiment\nDatasets and Evaluation Metrics\nWe conduct extensive experiments on ﬁve popular LFSR\nbenchmarks, i.e., EPFL (Rerabek and Ebrahimi 2016),\nHCInew (Honauer et al. 2016), HCIold (Wanner, Meister,\nand Goldluecke 2013), INRIA (Le Pendu, Jiang, and Guille-\nmot 2018), and STFgantry (Vaish and Adams 2008). All of\nthe light ﬁeld images from the above benchmarks have a\n9\u00029 angular resolution (i.e., U = V = A = 9). PSNR\nand SSIM are chosen as the evaluation metrics. For a test-\ning dataset with T scenes, we ﬁrst obtain the metric values\nof U\u0002V sub-aperture images, and average the summation\nresults of T\u0002U\u0002V to obtain the ﬁnal metric score.\nImplementation Details\nFollowing (Wang et al. 2021c), we convert the light ﬁeld\nimages from the RGB space to the YCbCr space. Our model\nonly super-resolves Y channel images, and uses the bicubic\ninterpolation to super-resolve Cb and Cr channels images,\nrespectively. The gradient maps are extracted from Y chan-\nnel of each SAI along the spatial dimension with the func-\ntion provided by (Ma et al. 2020). We perform \u00022 and \u00024\nSR with 5 \u00025 angular resolution on all benchmarks. In the\ntraining stage, the 64 \u000264 patches are cropped from each\nsub-aperture image, and we apply the bicubic interpolation\nto generate \u00022 and \u00024 patches. We use random horizontal\nrotation, vertical rotation and 90◦rotation to augment the\ntraining data. Spatial and angular resolution are processed\nsimultaneously for preserving the LF structure. The `1 loss\nis used to optimize our network. We use the Adam optimizer\nto train our network, with a batch size of 8. The initial learn-\ning rate is set to 2 \u000210−4 and it will be halved every 15\nepochs. We train the network for 75 epochs in total. All ex-\nperiments are carried out on a single Tesla V100 GPU card.\nComparisons with State-of-the-Art\nTo evaluate the effectiveness of DPT, we compare\nit with the several state-of-the-art LFSR methods\n( i.e., LFBM5D (Alain and Smolic 2018), GB (Rossi\nand Frossard 2018), resLF (Zhang, Lin, and Sheng 2019),\nLFSSR (Yeung et al. 2018), LF-InterNet (Wang et al. 2020),\nand LF-DFNet (Wang et al. 2021c)) and single image\nsuper-resolution methods ( i.e., VDSR (Kim, Lee, and Lee\n2016), EDSR (Lim et al. 2017), and RCAN (Zhang et al.\n2018)) on ﬁve LFSR benchmarks. Following (Wang et al.\n2021c), we treat Bicubic upsampling as the baseline.\nQuantitative Results. Table 1 lists the quantitative com-\nparisons. As seen, DPT achieves a promising performance\nin comparison with other methods. Single image super-\nresolution methods (i.e., VDSR (Kim, Lee, and Lee 2016),\nEDSR (Lim et al. 2017) and RCAN (Zhang et al. 2018))\nsuper-resolve each view separately. DPT outperforms all of\nthem, which can be attributed to its outstanding capabil-\nity to capture the complementary information of different\nviews. Furthermore, compared with other CNN-based meth-\nods, e.g., resLF (Zhang, Lin, and Sheng 2019), LFSSR (Ye-\nung et al. 2018), LF-InterNet (Wang et al. 2020), our DPT\noutperforms all of them for \u00024 SR. Speciﬁcally, compared\nwith the current leading approach LF-DFNet (Wang et al.\n2021c), DPT obtains superior results on EPFL (Rerabek and\nEbrahimi 2016), HCIold (Wanner, Meister, and Goldluecke\n2013) and INRIA (Le Pendu, Jiang, and Guillemot 2018)\nfor \u00024 SR. The reason is that, LF-DFNet only models the\nshort-range dependencies between each side-view SAI and\nthe center-view SAI, while our DPT explores long-range re-\nlations among all SAIs.\nQualitative Results. Fig. 3 depicts some representative vi-\nsual results of different approaches for \u00024 SR. As seen,\nLF-InterNet (Wang et al. 2020) and LF-DFNet (Wang et al.\n2021c) produce distorted results for the structures of the tele-\nscope stand in the Bicycle scene and the ﬂowerbed edge in\nthe Sculpture scene. In contrast, our DPT yields much better\nresults, with all above regions being well preserved.\nComputational Analysis. Table 2 provides a detailed anal-\nysis of the LFSR models in terms of parameters, FLOPs,\nand reconstruct accuracy on EPFL. Following (Wang et al.\n2021c), we set the size of the input LF image as5\u00025\u000232\u0002\n2526\nMethod Scale EPFL HCIne\nw HCIold INRIA STFgantry\nBicubic \u00022 29.50 /\n0.9350 31.69 / 0.9335 37.46 / 0.9776 31.10 / 0.9563 30.82 / 0.9473\nVDSR (Kim, Lee, and Lee 2016) \u00022 32.50 /\n0.9599 34.37 / 0.9563 40.61 / 0.9867 34.43 / 0.9742 35.54 / 0.9790\nEDSR (Lim et al. 2017) \u00022 33.09 /\n0.9631 34.83 / 0.9594 41.01 / 0.9875 34.97 / 0.9765 36.29 / 0.9819\nRCAN (Zhang et al. 2018) \u00022 33.16 /\n0.9635 34.98 / 0.9602 41.05 / 0.9875 35.01 / 0.9769 36.33 / 0.9825\nLFBM5D (Alain and Smolic 2018) \u00022 31.15 /\n0.9545 33.72 / 0.9548 39.62 / 0.9854 32.85 / 0.9695 33.55 / 0.9718\nGB (Rossi and Frossard 2018) \u00022 31.22 /\n0.9591 35.25 / 0.9692 40.21 / 0.9879 32.76 / 0.9724 35.44 / 0.9835\nresLF (Zhang, Lin, and Sheng 2019) \u00022 32.75 /\n0.9672 36.07 / 0.9715 42.61 / 0.9922 34.57 / 0.9784 36.89 / 0.9873\nLFSSR (Yeung et al. 2018) \u00022 33.69 /\n0.9748 36.86 / 0.9753 43.75 / 0.9939 35.27 / 0.9834 38.07 / 0.9902\nLF-InterNet (Wang et al. 2020) \u00022 34.14 /\n0.9761 37.28 / 0.9769 44.45 / 0.9945 35.80 / 0.9846 38.72 / 0.9916\nLF-DFnet (Wang et al. 2021c) \u00022 34.44 / 0.9766\n37.44 / 0.9786 44.23 / 0.9943 36.36 / 0.9841 39.61 / 0.9935\nDPT (Ours) \u00022 34.48 / 0.9759\n37.35 / 0.9770 44.31 / 0.9943 36.40 / 0.9843 39.52 / 0.9928\nBicubic \u00024 25.14 /\n0.8311 27.61 / 0.8507 32.42 / 0.9335 26.82 / 0.8860 25.93 / 0.8431\nVDSR (Kim, Lee, and Lee 2016) \u00024 27.25 /\n0.8782 29.31 / 0.8828 34.81 / 0.9518 29.19 / 0.9208 28.51 / 0.9012\nEDSR (Lim et al. 2017) \u00024 27.84 /\n0.8858 29.60 / 0.8874 35.18 / 0.9538 29.66 / 0.9259 28.70 / 0.9075\nRCAN (Zhang et al. 2018) \u00024 27.88 /\n0.8863 29.63 / 0.8880 35.20 / 0.9540 29.76 / 0.9273 28.90 / 0.9110\nLFBM5D (Alain and Smolic 2018) \u00024 26.61 /\n0.8689 29.13 / 0.8823 34.23 / 0.9510 28.49 / 0.9137 28.30 / 0.9002\nGB (Rossi and Frossard 2018) \u00024 26.02 /\n0.8628 28.92 / 0.8842 33.74 / 0.9497 27.73 / 0.9085 28.11 / 0.9014\nresLF (Zhang, Lin, and Sheng 2019) \u00024 27.46 /\n0.8899 29.92 / 0.9011 36.12 / 0.9651 29.64 / 0.9339 28.99 / 0.9214\nLFSSR (Yeung et al. 2018) \u00024 28.27 /\n0.9080 30.72 / 0.9124 36.70 / 0.9690 30.31 / 0.9446 30.15 / 0.9385\nLF-InterNet (Wang et al. 2020) \u00024 28.67 /\n0.9143 30.98 / 0.9165 37.11 / 0.9715 30.64 / 0.9486 30.53 / 0.9426\nLF-DFnet (Wang et al. 2021c) \u00024 28.77 /\n0.9165 31.23 / 0.9196 37.32 / 0.9718 30.83 / 0.9503 31.15 / 0.9494\nDPT (Ours) \u00024 28.93 / 0.9167 31.19 /\n0.9186 37.39 / 0.9720 30.96 / 0.9502 31.14 / 0.9487\nTable 1: Performance comparison of different methods for \u00022 and \u00024 SR. The best results are marked as bold.\nMethod Scale # P\naram (M) / FLOPs (G) PSNR / SSIM\nresLF \u00022 6.35 /\n37.06 32.75 / 0.9672\nLFSSR \u00022 0.81 /\n25.70 33.69 / 0.9748\nLF-InterNet \u00022 4.80 /\n47.46 34.14 / 0.9761\nLF-DFNet \u00022 3.94 /\n57.22 34.44 / 0.9766\nDPT (Ours) \u00022 3.73 /\n57.44 34.48 / 0.9759\nresLF \u00024 6.79 /\n39.70 27.46 / 0.8899\nLFSSR \u00024 1.61 /\n128.44 28.27 / 0.9080\nLF-InterNet \u00024 5.23 /\n50.10 28.67 / 0.9143\nLF-DFNet \u00024 3.99 /\n57.31 28.77 / 0.9165\nDPT (Ours) \u00024 3.78 /\n58.64 28.93 / 0.9167\nTable 2: Detailed comparisons of LFSR methods in terms of\nnumber of parameters, FLOPs, and reconstruction accuracy\non the EPFL dataset for \u00022 and \u00024 SR.\n32 for the computation of the FLOPs. As can be seen, DPT\nshows the best trade-off between reconstruction accuracy\nand model efﬁciency. For\u00024 SR, DPT has fewer parameters\nwhile better accuracy, in comparison with LF-DFNet (Wang\net al. 2021c). This result further conﬁrms the effectiveness of\nDPT, not only in better performance but also its efﬁciency.\nAblation Study\nTo gain more insights into our model, we conduct a set of\nablative experiments on the EPFL dataset for \u00024 SR. The\nresults are reported in Table 3. We show in the 1st row the\nperformance of baseline model (i.e., LF-DFNet (Wang et al.\n2021c)), and the 7th row the results of our full model.\nContent Transformer.We ﬁrst investigate the effect of the\ncontent Transformer by constructing a network which is im-\nplemented with the convolution feature extraction module\nECont, content Transformer TCont and the image reconstruc-\ntion module DReco. Its results are given in the 2nd row. We\ncan see that the content Transformer itself can lead to a sim-\nilar performance as the baseline model, however, our con-\ntent Transformer has fewer parameters. This conﬁrms that\nglobal relations among different views brought by the con-\ntent Transformer are beneﬁcial to LFSR.\nGradient Transformer. Furthermore, we combine the gra-\ndient Transformer into the content Transformer, with the re-\nsults being shown in the3rd row. As we can see, by introduc-\ning the gradient Transformer, the performance of the content\nTransformer (the 2nd row) improves by 0.09dB in terms of\nPSNR and 0.0011 improvement in terms of SSIM, respec-\ntively. Moreover, the models with dual transformers (the 4th,\nthe 6th and the 7th rows) outperform the model with only\na content Transformer (the 2nd row), which further con-\nﬁrms the effectiveness of gradient Transformer. Finally, we\nreplace the content Transformer and gradient Transformer\nof DPT with residual blocks while maintaining the network\nparameters almost unchanged. Its results are given in the 5th\nrow. The dual branches in DPT are equivalent to two convo-\nlutional neural networks for feature extraction. As reported\nin Table 3, DPT has a 0.21 dB PSNR drop, demonstrating\nthe effectiveness of the proposed Transformers.\nImpact of Fusion Mechanism. To explore the effect of\nfusion mechanism for content and gradient feature ag-\ngregation, we construct two model variants, which gener-\nates the detailed-preserved representation Zi in Eq. 7 by\nthe element-wise summation (the 3rd row) and the Trans-\nformer for a single view non-local dependencies exploration\n(the 4th row), respectively. As can be observed, our cross-\nattention fusion Transformer (the 7th row) brings a promis-\ning performance improvement over the results of 3rd row\nand the 4th row in PSNR and SSIM, which is attributed to\nits effectiveness for the complementary non-local informa-\ntion exploration of the content and gradient features.\n2527\nPSNR/SSIM\n27.48/0.8734\n29.93/0.9353\n29.94/0.9376\n30.09/0.9368\nPSNR/SSIM\n25.34/0.8025\n28.50/0.8941\n28.62/0.8961\n28.66/0.8953\nGround Truth\nBicubic\nLF\n-\nInterNet\nLF\n-\nDFNet\nProposed\nBicycle\n×\n4\nSculpture\n×\n4\nFigure 3: Visual comparisons of Bicycle scene from HCInew and Sculpture scene from INRIA for \u00024 SR. We see that our\napproach achieves compelling reconstruction results, with the details well preserved.\n# Content\nTransformer\nGradient\nT\nransformer\nFusion Mechanism # P\naram (M) PSNR /\nSSIMSum T\nransformer (image) Transformer (sequence)\n1 3.99 28.77 /\n0.9165\n2 X 2.62 28.77 /\n0.9142\n3 X X X 3.72 28.86 /\n0.9153\n4 X X X 3.75 28.81 /\n0.9149\n5 X 3.83 28.72 /\n0.9139\n6 X X X 3.91 28.89 /\n0.9157\n7 X X X 3.78 28.93 / 0.9167\nTable 3: Ablation study of DPT on the EPFL dataset for \u00024 SR.\nEfﬁcacy of SA-LSA. We study the effect of the SA-LSA\nlayers by replacing them with the vanilla fully-connected\nself-attention layers in DPT (the 6th row). As seen, the\nmodel with SA-LSA layers (the 7th row) obtains a better\nperformance with fewer parameters, proving the effective-\nness of local spatial context for SAI reconstruction.\nNumber of Attention Blocks K. At last, we study the per-\nformance of DPT with respect to the number of spatial-\nangular attention blocks in the content Transformer (or the\ngraidient Transformer). Table 4 reports the comparison re-\nsults. As seen, DPT achieves the best results atK=2. Thus,\nwe choose K = 2as the default number of spatial-angular\nattention blocks in DPT.\nConclusion\nThis work proposes a Detail Preserving Transformer (DPT),\nas the ﬁrst application of Transformer for LFSR. Instead\nof leveraging a vanilla fully-connected self-attention layer,\nwe develop a spatial-angular locally-enhanced self-attention\nlayer (SA-LSA) to promote non-local spatial-angular de-\nK 1 2\n3 4\nPSNR (dB) 28.69 28.93 28.78 28.74\n#\nParam (M) 2.89 3.78\n5.00 6.56\nTable 4: Performance and model size comparisons with dif-\nferent numbers of spatial-angular attention block on the\nEPFL dataset for \u00024 SR.\npendencies of each sub-aperture image sequence. Based on\nSA-LSA, we leverage a content Transformer and a gradi-\nent Transformer to learn spatial-angular content and gradient\nrepresentations, respectively. The comprehensive spatial-\nangular representations are further processed by a cross-\nattention fusion Transformer to aggregate the output of the\ntwo Transformers, from which a high-resolution light ﬁeld is\nreconstructed. We compare the proposed network with other\nstate-of-the-art methods over ﬁve commonly-used bench-\nmarks, and the experimental results demonstrate that it\nachieves favorable performance against other competitors.\n2528\nAcknowledgments\nThis work was supported in part by CCF-Baidu Open Fund,\nin part by National Key Research and Development Plan\n(No.2017YFC0112001), and in part by China Central Tele-\nvision (JG2018-0247).\nReferences\nAlain, M.; and Smolic, A. 2018. Light ﬁeld super-resolution\nvia LFBM5D sparse coding. In ICIP, 2501–2505.\nCao, J.; Li, Y .; Zhang, K.; and Van Gool, L. 2021.\nVideo Super-Resolution Transformer. arXiv preprint\narXiv:2106.06847.\nChen, H.; Wang, Y .; Guo, T.; Xu, C.; Deng, Y .; Liu, Z.; Ma,\nS.; Xu, C.; Xu, C.; and Gao, W. 2021. Pre-trained image\nprocessing transformer. In CVPR, 12299–12310.\nDai, J.; Qi, H.; Xiong, Y .; Li, Y .; Zhang, G.; Hu, H.; and\nWei, Y . 2017. Deformable convolutional networks. InICCV,\n764–773.\nDong, C.; Loy, C. C.; He, K.; and Tang, X. 2014. Learning\na deep convolutional network for image super-resolution. In\nECCV, 184–199.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. In\nICLR.\nFarrugia, R. A.; and Guillemot, C. 2019. Light ﬁeld super-\nresolution using a low-rank prior and deep convolutional\nneural networks. IEEE TPAMI, 42(5): 1162–1175.\nHonauer, K.; Johannsen, O.; Kondermann, D.; and Gold-\nluecke, B. 2016. A dataset and evaluation methodology for\ndepth estimation on 4d light ﬁelds. In ACCV, 19–34.\nJin, J.; Hou, J.; Chen, J.; and Kwong, S. 2020. Light\nﬁeld spatial super-resolution via deep combinatorial geome-\ntry embedding and structural consistency regularization. In\nCVPR, 2260–2269.\nKim, J.; Lee, J. K.; and Lee, K. M. 2016. Accurate image\nsuper-resolution using very deep convolutional networks. In\nCVPR, 1646–1654.\nLe Pendu, M.; Jiang, X.; and Guillemot, C. 2018. Light\nﬁeld inpainting propagation via low rank matrix completion.\nIEEE TIP, 27(4): 1981–1993.\nLiang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; and\nTimofte, R. 2021. SwinIR: Image restoration using swin\ntransformer. arXiv preprint arXiv:2108.10257.\nLim, B.; Son, S.; Kim, H.; Nah, S.; and Mu Lee, K. 2017.\nEnhanced deep residual networks for single image super-\nresolution. In CVPRW, 136–144.\nLu, Z.; Liu, H.; Li, J.; and Zhang, L. 2021. Efﬁcient Trans-\nformer for Single Image Super-Resolution. arXiv preprint\narXiv:2108.11084.\nMa, C.; Rao, Y .; Cheng, Y .; Chen, C.; Lu, J.; and Zhou, J.\n2020. Structure-preserving super resolution with gradient\nguidance. In CVPR, 7769–7778.\nMou, C.; Zhang, J.; Fan, X.; Liu, H.; and Wang, R. 2021.\nCOLA-Net: Collaborative Attention Network for Image\nRestoration. IEEE TMM.\nRerabek, M.; and Ebrahimi, T. 2016. New light ﬁeld im-\nage dataset. In 8th International Conference on Quality of\nMultimedia Experience, CONF.\nRossi, M.; and Frossard, P. 2018. Geometry-consistent light\nﬁeld super-resolution via graph-based regularization. IEEE\nTIP, 27(9): 4207–4218.\nVaish, V .; and Adams, A. 2008. The (new) stanford light\nﬁeld archive. Computer Graphics Laboratory, Stanford Uni-\nversity, 6(7).\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS, 5998–6008.\nWang, S.; Zhou, T.; Lu, Y .; and Di, H. 2022. Contextual\nTransformation Network for Lightweight Remote Sensing\nImage Super-Resolution. IEEE TGRS, 60: 1–13.\nWang, W.; Zhou, T.; Qi, S.; Shen, J.; and Zhu, S.-C. 2021a.\nHierarchical human semantic parsing with comprehensive\npart-relation modeling. IEEE TPAMI.\nWang, W.; Zhou, T.; Yu, F.; Dai, J.; Konukoglu, E.; and\nVan Gool, L. 2021b. Exploring Cross-Image Pixel Contrast\nfor Semantic Segmentation. In ICCV, 7303–7313.\nWang, Y .; Liu, F.; Zhang, K.; Hou, G.; Sun, Z.; and Tan, T.\n2018. LFNet: A novel bidirectional recurrent convolutional\nneural network for light-ﬁeld image super-resolution. IEEE\nTIP, 27(9): 4274–4286.\nWang, Y .; Wang, L.; Yang, J.; An, W.; Yu, J.; and Guo, Y .\n2020. Spatial-angular interaction for light ﬁeld image super-\nresolution. In ECCV, 290–308.\nWang, Y .; Yang, J.; Wang, L.; Ying, X.; Wu, T.; An, W.;\nand Guo, Y . 2021c. Light ﬁeld image super-resolution using\ndeformable convolution. IEEE TIP, 30: 1057–1071.\nWanner, S.; Meister, S.; and Goldluecke, B. 2013. Datasets\nand benchmarks for densely sampled 4D light ﬁelds. In Vi-\nsion, Modelling and Visualization, volume 13, 225–226.\nYang, F.; Yang, H.; Fu, J.; Lu, H.; and Guo, B. 2020. Learn-\ning texture transformer network for image super-resolution.\nIn CVPR, 5791–5800.\nYeung, H. W. F.; Hou, J.; Chen, X.; Chen, J.; Chen, Z.; and\nChung, Y . Y . 2018. Light ﬁeld spatial super-resolution using\ndeep efﬁcient spatial-angular separable convolution. IEEE\nTIP, 28(5): 2319–2330.\nYoon, Y .; Jeon, H.-G.; Yoo, D.; Lee, J.-Y .; and Kweon, I. S.\n2017. Light-ﬁeld image super-resolution using convolu-\ntional neural network. IEEE SPL, 24(6): 848–852.\nYoon, Y .; Jeon, H.-G.; Yoo, D.; Lee, J.-Y .; and So Kweon, I.\n2015. Learning a deep convolutional network for light-ﬁeld\nimage super-resolution. In ICCVW.\nYuan, Y .; Cao, Z.; and Su, L. 2018. Light-ﬁeld image super-\nresolution using a combined deep CNN based on EPI. IEEE\nSPL, 25(9): 1359–1363.\nZhang, S.; Lin, Y .; and Sheng, H. 2019. Residual networks\nfor light ﬁeld image super-resolution. In CVPR, 11046–\n11055.\n2529\nZhang, Y .; Li, K.; Li, K.; Wang, L.; Zhong, B.; and Fu, Y .\n2018. Image super-resolution using very deep residual chan-\nnel attention networks. In ECCV, 286–301.\nZhou, T.; Li, J.; Wang, S.; Tao, R.; and Shen, J. 2020. Mat-\nnet: Motion-attentive transition network for zero-shot video\nobject segmentation. IEEE TIP, 29: 8326–8338.\nZhou, T.; Li, L.; Li, X.; Feng, C.-M.; Li, J.; and Shao, L.\n2022. Group-Wise Learning for Weakly Supervised Seman-\ntic Segmentation. IEEE TIP, 31: 799–811.\nZhou, T.; Qi, S.; Wang, W.; Shen, J.; and Zhu, S.-C. 2021.\nCascaded parsing of human-object interaction recognition.\nIEEE TPAMI.\n2530",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6584402322769165
    },
    {
      "name": "Locality",
      "score": 0.6396355628967285
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5553431510925293
    },
    {
      "name": "Light field",
      "score": 0.5510497689247131
    },
    {
      "name": "Transformer",
      "score": 0.5158911943435669
    },
    {
      "name": "Computer vision",
      "score": 0.49400073289871216
    },
    {
      "name": "Image resolution",
      "score": 0.47787371277809143
    },
    {
      "name": "Sequence (biology)",
      "score": 0.470263808965683
    },
    {
      "name": "Code (set theory)",
      "score": 0.4391745924949646
    },
    {
      "name": "Superresolution",
      "score": 0.43869855999946594
    },
    {
      "name": "Algorithm",
      "score": 0.3622288703918457
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3067280054092407
    },
    {
      "name": "Physics",
      "score": 0.1139630377292633
    },
    {
      "name": "Voltage",
      "score": 0.07890322804450989
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}