{
  "title": "Practical Takes on Federated Learning with Pretrained Language Models",
  "url": "https://openalex.org/W4386576864",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108894082",
      "name": "Ankur Agarwal",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2258670661",
      "name": "Mehdi Rezagholizadeh",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2562464502",
      "name": "Prasanna Parthasarathi",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2784621220",
    "https://openalex.org/W4318619660",
    "https://openalex.org/W3171654528",
    "https://openalex.org/W3035000591",
    "https://openalex.org/W2994684563",
    "https://openalex.org/W4312950667",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W2120354757",
    "https://openalex.org/W3176617324",
    "https://openalex.org/W2104094955",
    "https://openalex.org/W4297687186",
    "https://openalex.org/W3193756050",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2907380995",
    "https://openalex.org/W3037913917",
    "https://openalex.org/W2889965839",
    "https://openalex.org/W3200809495",
    "https://openalex.org/W2804580284",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W4292084264",
    "https://openalex.org/W3114610051",
    "https://openalex.org/W2982756875",
    "https://openalex.org/W2158108973",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W113837456",
    "https://openalex.org/W3213291156",
    "https://openalex.org/W1593532658",
    "https://openalex.org/W3017949972",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3154565472",
    "https://openalex.org/W3013721857",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2073459066",
    "https://openalex.org/W1966784040",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4225262055",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4294106961",
    "https://openalex.org/W3038022836",
    "https://openalex.org/W2995409942",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4287332481",
    "https://openalex.org/W2980216952",
    "https://openalex.org/W3153137927",
    "https://openalex.org/W3120121071",
    "https://openalex.org/W3127057363",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4322588812",
    "https://openalex.org/W3037063616",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2902456977",
    "https://openalex.org/W4287121309"
  ],
  "abstract": "Real-world applications of language models entail data privacy constraints when learning from diverse data domains. Federated learning with pretrained language models for language tasks has been gaining attention lately but there are definite confounders that warrants a careful study. Specifically, understanding the limits of federated NLP applications through varying the effects of different aspects (such as data heterogeneity, the trade-off between training time and performance, the effect of different data, and client distributions and sensitivity of the shared model to learning local distributions) is necessary to evaluate whether language models indeed learn to generalize by adapting to the different domains. Towards that, we elaborate different hypotheses over the components in federated NLP architectures and study them in detail with relevant experiments over three tasks: Stanford Sentiment Treebank-2, OntoNotes-5.0 and GigaWord. The experiments with different Transformer inductive biases on the variety of tasks provide a glimpse at the understanding of federated learning at NLP tasks. Specifically, the analysis suggests that regularization due to the ensembling effect may be masquerading as domain adaptation of federated learning in NLP with pre-trained language models.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 454–471\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nPractical Takes on Federated Learning with Pretrained Language Models\nAnkur Agarwal Mehdi Rezagholizadeh Prasanna Parthasarathi\nHuawei Noah’s Ark Lab, Montréal\n{ankur.agarwal1,mehdi.rezagholizadeh,prasanna.parthasarathi}@huawei.com\nAbstract\nReal-world applications of language models\nentail data privacy constraints when learning\nfrom diverse data domains. Federated learn-\ning with pretrained language models for lan-\nguage tasks has been gaining attention lately\nbut there are definite confounders that warrants\na careful study. Specifically, understanding the\nlimits of federated NLP applications through\nvarying the effects of different aspects (such\nas data heterogeneity, the trade-off between\ntraining time and performance, the effect of\ndifferent data, and client distributions and sen-\nsitivity of the shared model to learning local\ndistributions) is necessary to evaluate whether\nlanguage models indeed learn to generalize by\nadapting to the different domains. Towards\nthat, we elaborate different hypotheses over the\ncomponents in federated NLP architectures and\nstudy them in detail with relevant experiments\nover three tasks: Stanford Sentiment Treebank-\n2, OntoNotes-5.0 and GigaWord. The experi-\nments with different Transformer inductive bi-\nases on the variety of tasks provide a glimpse at\nthe understanding of federated learning at NLP\ntasks. Specifically, the analysis suggests that\nregularization due to the ensembling effect may\nbe masquerading as domain adaptation of feder-\nated learning in NLP with pre-trained language\nmodels.\n1 Introduction\nThe success of large pretrained language models\n(Devlin et al., 2019; Liu et al., 2019; Radford et al.,\n2019; Lewis et al., 2019) have demonstrated their\napplicability in consumer-based natural language\nprocessing (NLP) applications (Otter et al., 2020).\nWhile there are such massive datasets(Kiela et al.,\n2021; Wang et al., 2021), making models trained on\nthese datasets to reflect the data diversity is an im-\nportant challenge towards building equitable NLP\nsystems. Hence, treating the distribution of data\nover the users as non-IID (McMahan and Ramage,\n2017; Xu et al., 2018; Liu and Mazumder, 2021) to\nbetter emphasize the preferences of users as person-\nalization gets naturally extended to the consumer\nNLP applications.\nBut, recent studies highlight that pretrained lan-\nguage models (PLMs) (Devlin et al., 2019) tend\nto get their predictions skewed by the frequency\neffects of tokens in the data distribution (Wei et al.,\n2021), this is concerning from a privacy and person-\nalization standpoint. Han and Eisenstein (2019);\nRamponi and Plank (2020); Carlini et al. (2022)\nshow that neural language models (even the large\npretrained architectures) have challenges in adapt-\ning to different data distributions on generative and\nclassification tasks alike.\nFederated learning (FL) (McMahan and Ram-\nage, 2017; Koneˇcn`y et al., 2016) has been gaining\npopularity in machine learning as a practical way\nto mitigate domain adaptation with the promise of\ndata privacy. FL as a learning paradigm focuses\non learning a shared model through training data\ndistributed over several clients. Such approaches\nhave only recently begun to focus on NLP applica-\ntions (Mammen, 2021; Lin et al., 2021). Lin et al.\n(2021) suggest that the success of federated algo-\nrithms can be improved through adapting over the\nclient distribution that improves the generalization\nperformance across the client distributions.\nHowever, the opaqueness of the pretraining rou-\ntine — primarily, quantifying what, and how much\nof that a language model has learnt from the pre-\ntraining corpora (Zhu et al., 2015; Devlin et al.,\n2019; Gao et al., 2020) cast a shadow on evaluating\nthe effectiveness of these architectures in learning\nfrom diverse domains. Understanding the roles of\ndifferent inductive biases not limited to the architec-\nture, loss functions and data distribution becomes\nimperative to carefully look at claims of “domain\nadaptation” (Kouw and Loog, 2018). The repre-\nsentation of texts guided by syntax and semantic\nelements makes generalization to non-IID distribu-\ntions in NLP more challenging if not the same when\n454\ncompared to domains such as computer vision (Liu\net al., 2020; Luo et al., 2021; Zhuang et al., 2021;\nYang et al., 2021). Then, it becomes imperative\nto understand the role of different constituents in\nthe federated learning in NLP setup to take steps\nin the right direction. In that regard, we investigate\nfour major hypotheses detailing the confounding\nvariables in federated NLP systems: ( 1) role of\npretrained weights in FL’s domain adaptation, (2)\ndistribution of clients, (3) Data homogeneity, and\n(4) robustness of personalizing to local distribu-\ntion. Although the primary focus of the paper is\nto generate discussions along these questions, we\nsupport the discussions with relevant experiments\non 3 different NLP tasks on 2 large Transformer\narchitectures.\n2 Background\nFederated Learning Federated learning assumes\nthe set up of Kclients {Ck}K\nk=1 with different data\ndistributions and a single server model S. Each\nclient model, θCk, is initialized by the server model\nθS a dedicated copy of the server model and then\nupdated locally on the kth client data distribution\nusing an optimizer OptCk. This distributed learn-\ning is repeated iteratively over Rrounds. At each\nround r, the client models, θCr\nk, are initialized with\nthe aggregated weights of all the client models from\nthe previous round (r−1), referred to as the central\nserver model, θr\nS. The rounds end with accumu-\nlation and aggregation of gradients from all the\nclient models to update the server model with op-\ntimizer OptS and continued until convergence on\nan unseen set (Dtest). FedOpt, a popular federated\nlearning algorithm is shown in Algorithm 1.\nAlgorithm 1FedOpt Algorithm (Asad et al., 2020)\nInput: θ0\nS,OptCk,OptS\nfor r= 1 to Rdo\nfor k= 1 to Kin parallel do\nθr\nCk ←θr−1\nS\nfor e= 1 to Edo\ngCr\nk ←∇θ(θCr\nk |Dk)\nθCr\nk ←OptCk(θCr\nk,gCr\nk)\nend for\n∆r\nk ←θCr\nk −θr−1\nS\nend for\n∆r ← 1\nKΣK\nk=0∆r\nk\nθr\nS ←OptS(θr−1\nS ,∆r)\nend for\nThe Performance Gap Like in (Lin et al., 2021),\nwe compare the performance of federated server,\nθS, with θcentral over a common unseen set, Dtest\nas in Equation 1,\n∆Perf = PerfS −Perfcentral (1)\nwhere θcentral is trained over{Dk}K\nk=1 until con-\nvergence. Accuracy, F1, or Rouge score (Lin, 2004)\ncan be used for measuring Perf.\nGeneralization Personalization trade-off Let\nthe best server generalization performance is mea-\nsured over Dtest be P∗\nserver. The generalization\nperformance of the client model trained in r∗is\nmeasured on Dtest be P∗\nclientk. The performance\nof the client model on Dk in round r∗measured be\n˜Pclientk, which acts as a proxy to the personaliza-\ntion on Dk. Then, we measure the difference in the\ntest loss for every client, k, between θC∗\nk and θ∗−1\nS\nas ∆P.\n∆Pk(xi) = P∗\nclientk (xi) −P∗−1\nserver (xi) (2)\nFor every xi ∈ Dtest, the correlation between\n∆Pk(xi) to that of ˜Pclientk measures the mutual\ncost of personalization to Dk on the generalization\nperformance on Dtest. Also, we measure the aver-\nage the empirical risk of θC∗\nk over Dk as ˜Pclientk:\n˜Pclientk = 1\n|Dk |\n|Dk|∑\nj=0\nPclientk (xj ) (3)\nWe now define the trade-off metric as the slope\nbetween ∆Pk(xi) and ˜Pclientk over all x ∼Dtest\nand K. m∆P measures the unit increase in gener-\nalization performance for adapting to Dk. Or m∆P\nestimates the cost of personalizing over the client\ndistribution. Consequently, we make the interpreta-\ntions for the metric m∆P — (a) positive slope (↗)\nindicates that learning on local distribution aids in\nbetter generalization, (b) negative slope (↘) indi-\ncates that generalization inhibits the learning from\nlocal distributions, or (c) neutral (−→) shows that\nthe model is unaffected by learning.\n3 Related Work\nMulti-Domain Learning Realtime applications\nof most tasks have shown diverse distribution of\ndatapoints requiring domain adaptation strategies\n(Daumé III, 2009; Dredze and Crammer, 2008).\nThe effect of such domain shift in NLP has been\n455\na topic of study for a while (Blitzer et al., 2006;\nQuiñonero-Candela et al., 2008; Blitzer, 2008; Ben-\nDavid et al., 2010; Cui and Bollegala, 2019). The\ngeneral topic of domain adaptation in NLP shares\nsimilarity with the topics of continual learning (Sun\net al., 2019), transfer learning (Devlin et al., 2019;\nRadford et al., 2019), multi-task learning (Col-\nlobert and Weston, 2008), and federated learning\n(Lin et al., 2021). Federated learning however, is\ndifferent from the other paradigms since it empha-\nsizes on the notion of preserving privacy of differ-\nent local data distributions. To that, sophisticated\napproaches to aggregate the gradients to transfer\nlearning from clients to the shared model (e.g. Fed-\nProx (Li et al., 2020), FedAvg (McMahan et al.,\n2017a), and FedOpt (Asad et al., 2020)) have show-\ncased improvements in the generalization of the\nshared parameters. On the other hand, due to the\nmany interactions of the clients with the server,\ncommunication overhead is an important aspect,\nand FedOpt (Asad et al., 2020) has been shown to\nbetter address it over existing federated algorithms.\nOverview of Federated Learning Federated\nlearning (McMahan and Ramage, 2017; Mammen,\n2021) addresses the challenge of learning from pri-\nvate data spanning over multiple clients. Although\nthe evaluation of such architectures prioritizes the\ngeneralization of the shared model, Mendieta et al.\n(2022) highlight that learning from the local distri-\nbutions is critical towards that. The key to such ef-\nficient learning in federated architectures has been\nshaped by homogeneous and heterogeneous data or\nmodel (Li and Wang, 2019) distribution in clients\n(model architectures across clients have similar or\ndifferent parameters). Further, the emphasis on pri-\nvacy of client data has also been mitigated through\nthe recent progress in knowledge distillation. How-\never, systematic studies (Kairouz et al., 2021; Li\net al., 2021) over federated architectures have iden-\ntified potential biases due to unbalanced data of\nclients or diversity in the label distribution among\nothers. Chen et al. (2018) propose a meta learning\napproach for federated learning that improves per-\nsonlaizing to the non-IID client distributions. Also,\nconstraints on data privacy makes it difficult to im-\nport approaches (Kirkpatrick et al., 2017; Rolnick\net al., 2019) that avoid catastrophic forgetting of\ndistributions in continual learning tasks.\nFederated Learning for language tasks Dis-\ntributed training on language tasks with federated\nlearning has been gaining some attention. McMa-\nhan et al. (2017b) trained a differentially private lan-\nguage model over non-IID data distributions while\nGe et al. (2020) trained a recurrent + convolutional\narchitecture for medical named entity recognition\ntask. Recently, Lin et al. (2021) proposed a frame-\nwork that enables using modern pretrained lan-\nguage models on different language understanding\ntasks. Lin et al. (2021) discuss and hypothesizes\na gap between the performance of Transformer ar-\nchitectures between the federated and centralized\nsetting with data heterogeneity. Dupuy et al. (2022)\nanalyze the effect of having clients with different\namounts of data gathered from Alexa devices and\nsuggest that non-uniform selection of devices im-\nproves the performance of the shared model.\nIn this work, we attempt to investigate different\npossible confounders for domain adaptation claims\nof federated systems in NLP and elaborately ana-\nlyze them in the language tasks of classification,\nsequence tagging and sequence generation tasks\nthat are popularly used with Transformer architec-\ntures.\n4 Experiments\nModels and Tasks We experiment with a focus\non the pretrain-finetune setup that is popular with\nTransformer architectures on many language tasks.\nOf the many, we pick three tasks— Stanford Sen-\ntiment Treebank 2 (SST-2) (Socher et al., 2013),\nOntoNotes (v5.0) (Weischedel et al., 2013) and Gi-\ngaword (Graff et al., 2003) that fall into the broad\ncategories of text classification, sequence tagging,\ntext-generation respectively. The data splits are\nas used in (Lin et al., 2021), please refer §B for\ndetails. As for the models 1, we use BART-Base\n(Lewis et al., 2019) for text generation and Distil-\nBERT (Sanh et al., 2019) for the other two tasks.\nIn the experiments we use the models with ( )\nand without ( ) pretrained weights23.\nCentralized Training We use batch-wise gradi-\nent descent with AdamW (Loshchilov and Hutter,\n2017) as the optimizer along with a linear learning\nrate scheduler. We use cross-entropy for model\nselection across the tasks. Also, in our analysis, we\n1We use the transformer weights shared in huggingface:\n‘distilbert-base-uncased’ and ‘facebook/bart-base’.\n2Across the experiments the line style and the colour is\nused to denote the corresponding model performances.\n3The without pretrained weights setting trains the models\nfrom scratch.\n456\nDataset Model Metric Pretrained Cent. Fed. ∆(Perf) ∆ (Rel.%)\nSST2 DistilBERT Accuracy ✓ 89.0±0.8 87.8±0.4 1.3 −\n✗ 69.2±3.2 67.8±1.0 1.4 7 .7 ▲\nOntoNotes DistilBERT F1 ✓ 85.9±0.1 84.4±0.1 1.5 −\n✗ 65.1±0.3 55.3±0.3 9.8 550 ▲\nGigaword BART Rouge1 ✓ 34.6±0.7 32.5±0.2 2.1 −\n✗ 6.1±0.5 2.9±0.6 3.2 50 ▲\nTable 1: Comparison between the ∆(Perf) of federated and corresponding centralized set up when using (✓) and\nnot using (✗) pretrained transformer weights. Across the 3 tasks it can be seen that the gap increases when not using\npretrained weights (▲) suggesting that the pretrained weights of transformer are possibly doing the heavy lifting in\ndomain adaptation of federated learning in language tasks.\nuse cross-entropy of samples in the test set to eval-\nuate the relative performance of models compared.\nThe complete results of the experiments are in §F.\nFederated Training For the federated experi-\nments, we partition the training dataset for the\nclients and train them using the FedOpt algorithm\n(Asad et al., 2020) to estimate the server parame-\nter updates. Further, we use AdamW with a linear\nlearning rate scheduler to estimate the gradients in\nour experiments. The round with the best server\ntest loss is selected as the best round (For the com-\nplete results please refer to §F; for their run-time\nrefer to §E).\nEvaluation Metrics The metric of evaluation\n(Perf) is accuracy for sentence classification, F1-\nspan for sequence tagging and ROUGE1 score for\ntext summarization.\n4.1 Motivation\nFederated NLP considers two powerful learning\nparadigms— Federated algorithms aggregate the\ngradient updates over clients trained with non-\nidentical data distributions while maintaining pri-\nvacy, and PLMs trained over large corpora with a\ngeneric objective that gives a better downstream\nperformance. Stickland and Murray (2019) and\nPeng et al. (2020) show that PLMs are success-\nful in tasks that require domain adaptation. The\nmotivation primarily relies on verifying if feder-\nated algorithms and PLMs share a synergy in the\nextreme domain adaptation scenario.\nTo that, we first study the role of pretrained\nweights as a confounding variable in the feder-\nated setup. The null hypothesis being the feder-\nated learning not affected much by the pretrained\nweights should be supported with ∆(Perf) remain-\ning similar in both cases. But, in Table 1 across\ndifferent tasks we see that ∆(Perf) increases when\nnot using pretrained weights. This suggests that the\npretrained weights may be supporting the perfor-\nmance of federated learning in NLP applications.\nThe corollary to this observation could be that the\nlearning in a federated setting may not be happen-\ning from adapting to the client distributions. This\nraises concerns on personalization that if at all the\nfederated NLP setup with PLM learns anything\nfrom the client distribution.\n4.2 Estimating the Confounding Variables\nPretraining and Federated LearningTowards\nunderstanding the essence of the pretrained\nweight’s semantic prior as a confounding role in the\nsuccess of FL for NLP, we continue to control for\nit in the remainder of the experiments. Disentan-\ngling such observations is necessary to objectively\nanalyze the federated algorithm for language tasks.\nContribution by Client size One major chal-\nlenge in the realistic setting of federated learning\nfor data-driven tasks is the data imbalances that\nnaturally occur among the clients (Lin et al., 2021;\nDupuy et al., 2022). The distribution of number of\ndata samples that each client has creates two dis-\ntinct classes of clients—major and minor players—\nwhose updates may affect the parameters of the\nshared server model differently. Generalization\naside, ensuring personalization to the local distribu-\ntion of data in the clients also becomes necessary\nin different scenarios arisen from the diverse data\ndistributions. While extreme distributions may pro-\nvide a regularization effect due to the ensemble\nlearning (Balaji et al., 2018; Kumar et al., 2020;\nStanton et al., 2021), the objective being able to\nbetter generalize through adapting to different lo-\ncal distributions require careful consideration of\nthe distribution of clients. Towards understanding\nthe limits of learning under the influence of clients\n457\nof different sizes, we evaluate the role of minor\nclients by ablating clients smaller than a threshold\n(τ) number of samples.\nClient Personalization and Server General-\nization Personalization emphasizes the shared\nmodel’s capacity to be representative of all clients’\ndistributions alike. But, training on the local distri-\nbution may affect the generalized representation of\nthe shared model similar to the catastrophic forget-\nting in continual learning (Kirkpatrick et al., 2017)\nor mode collapse in generative modeling (Salimans\net al., 2016). Using PLMs due to their robust\nsemantic representations could alleviate some of\nthese challenges. More specifically, we formulate\nour questions as ablation experiments: (a) How are\npersonalization and generalization related across\ndifferent tasks and client distributions (b) Does\nremoving updates from minor players affect this\nrelation? We study these questions in experiments\nwith the help of m∆Pmetric.\nClient data partitioning distributionGeneral-\nization to unseen distribution, and how well the\nlocal client distributions are personalized over the\nfederated learning rounds is also affected by the\ndistribution of samples over the clients. Towards\nunderstanding the effect of the sample distribution\non the personalization-generalization relation we\ncompare the learning of the server model between\nnon-uniform distribution that is closer to real world\nscenario, and a more controlled uniform distribu-\ntion of samples among the clients to (a) evaluate\nthe effect of generalization by the shared model and\nthe personalization to local distribution by varying\nthe number of samples per client uniformly over\nall the clients, and (b) we perform ablations on\nthe updates by thresholding the clients on hetero-\ngeneous data distribution set up to understand the\nideal scenarios in different language tasks.\n4.3 Additional Setup\nDataset Partitioning Strategies For the study,\nwe use data selection for the clients using two dif-\nferent strategies. Please refer to §C for the choice\nof hyper-parameters for the two methods.\nRandom partitioning samples data over {C}K\nk=1\nby sampling from a Dirichlet distribution over\nthe Kclients with α= 0.1 (Lin et al., 2021).\nFor ablation on random distribution, we use\nhyperparameter τ that denotes the minimum\nnumber of samples in Ci for its parameters to\nbe aggregated.\nUniform partitioningdistributes the data uni-\nformly over {C}K\ni=1, which is controlled by\na hyperparameter γ. Specifically, uniform dis-\ntribution is constructed by sampling at least γ\nsamples from each class for each client in the\nclassification tasks. For the text summariza-\ntion dataset, we cluster the SentenceBERT\nembedding (Reimers and Gurevych, 2019)\nand use KMeans++ 4(Arthur and Vassilvit-\nskii, 2006) with 8 as the number of clusters,\nand sample uniformly over them.\nDistributional Similarity We use MAUVE score\n(Pillutla et al., 2021) to measure the similarity be-\ntween different text distributions in the experiments.\nThe score uses GPT-2 (Radford et al., 2019) esti-\nmating the distributional similarity. Also, we use\nmauve_scaling_parameter set to 20. The score\nranges between 0 and 1, where 1 indicates signifi-\ncant overlap.\n5 Results\nFollowing our motivation in §4.1 and questions\nraised for the confounders to the federated system\nfor NLP tasks in §4.2, we structure the results to\nour investigation in this section.\n(a) SST2\n (b) OntoNotes\n (c) Gigaword\nFigure 1: We observe the effect of domain shift by\nmeasuring the δchange in the loss over every sample in\nthe test set (Pclient−P∗−1\nserver) and drawing a correlation\nwith the average loss over the train distribution in the\nclients ( ˜Pclient).\n5.1 Effectiveness of Pretrained weights in\nadapting to client data distributions\nAn ideal model is expected to not discount the\nlearning on the client distributions for better gener-\n4We use the implementation in www.scikit-learn.org.\n458\n(a) SST2-Random\n (b) OntoNotes-Random\n (c) Gigaword-Random\n(d) SST2-Uniform\n (e) OntoNotes-Uniform\n (f) Gigaword-Uniform\nFigure 2: By varying the number of trainable clients in the random setting across the tasks, we measure the sensitivity\nof the shared model’s performance (Cent. −Fed.: ▽ indicates federated model performing better) to data removal\nas high as ∼55% of training data (SST-2: ∼55%, OntoNotes: ∼33% and Gigaword: ∼20% with the smallest\nnumber of clients).\nalization performance. To verify whether such dis-\ncounting happens, we analyze the personalization-\ngeneralization trade-off with the m∆P metric. In\nFigure 1, across the datasets we observe that the\ncorrelation stayed more neutral than positive, sug-\ngesting that the pretrained model may not be learn-\ning much from the local distributions. We observe a\nrelatively positivem∆Pvalue when trained without\nthe pretrained weights. This could be an anticipated\nbehaviour, as the model relies on the information\nin the client distribution for generalization, unlike\nthe pretrained weights that come with a semantic\nprior.\n5.2 Client contributions to the metrics\nWe set up additional experiments to understand the\ncontribution of clients in more detail. Particularly,\nwe begin by studying our experiment of ablating up-\ndates from the clients that are below threshold (τ)\non the random experiments. We hypothesize that as\nthe threshold value is increased, the server model is\nrestricted to learning from fewer client local distri-\nbutions and the generalization performance should\nalso decline as a result. On the contrary, we note in\nFigure 2 (top row) that the performance of the with\npretrained model remained relatively unchanged\nacross the datasets. However, the MAUVE scores\nestimated over the Ablated, Unablated and Test\n(Unseen) distributions of data in Table 2 suggest\nthat the test distribution are not close to the ablated\nDataset U —A U —T A —T\nSST2 0.52±0.06 0.48±0.07 0.47±0.07\nOntoNotes 0.64±0.06 0.53±0.07 0.51±0.06\nGigaword 0.58±0.07 0.45±0.07 0.45±0.07\nTable 2: The MAUVE score between the distribution of\ndata in ablated (A), unablated (U) and unseen (T) splits\nwith maximum value of τ in the random distribution set-\nting. The values indicate the maximal distance between\nthe distributions across the tasks.\nor unablated clients’ distributions 5. The perfor-\nmance of the pretrained model in the federated\nsetup, still being little affected only suggests that\npretrained models may not be learning from the\nlocal distributions that hurt the claims of personal-\nization to these distributions.\nIf not adapting to the local distributions, we fur-\nther investigate whether the pretrained models use\nthe updates from client distributions as regulariza-\ntion. To that, we repeat the same experiments on\nthe same datasets with the more controlled uniform\ndistribution setting.\nDo client sizes affect the gapTo have a clearer\npicture of the client size affecting the learning con-\ntribution, we use the uniform distribution with γ\ncontrolling the data partitioning size uniformly over\nall the clients. In Figure 2 (bottom row), we ob-\nserve a trend showing that the models' performance\n5Scores closer to 1 indicate significant overlap\n459\n(a) SST2\n(b) OntoNotes\n(c) Gigaword\nFigure 3: Impact of personalization of clients on gen-\neralization of server —m∆P values depict the impact\nin Random distribution strategy when ablating minor\nclients over different τ values.\n(pretrained or not) decreases drastically as the num-\nber of clients increases and the client partitioning\nsizes progressively become smaller. Similar obser-\nvation across the different tasks suggests that the\nmodel requires a quorum of samples to minimize\nthe gap across (Equation 1) the different tasks in a\nfederated setup.\nDo client sizes affect the trade-offAgain here,\nwe adjust for τ in random setting, where the clients\nwith less than τ number of samples are restricted\nfrom updating the server parameters. We measure\nm∆P values for the varying τ values in Figure 3.\nWe observe that with updates primarily from more\nminor clients (lower τ value), the generalization is\nless affected by personalization. But, the gap being\nlower as shown in Figure 2 suggests that the noisy\nupdates with fewer clients be acting as a regularizer\nfor the server updates. Further, as the value of τ\nincreases (Figure 3), the trade-off remains healthier\nuntil a certain value of τ and then it drops. This\ntrend could be attributed to the fact that with a\nhigher τ value the number of clients updating the\nserver becomes lesser with more data points, which\nprovides better generalization but personalization\ndue to variance in the client distribution gets chal-\nlenging.\nTo understand the impact of varying client dis-\ntributions on the trade-off, we perform the same\nanalysis with uniform distribution shown in Fig-\nure 4. The pretrained model’s generalization re-\nmains unaffected with more minor clients across\nSST-2 and GigaWord tasks supporting the alternate\nthat updates from minor clients provide a regular-\nization effect as we also see the gap to remain the\nsame in Figure 2. On OntoNotes, while the m∆P\nvalue stays the same the gap widens as observed in\nFigure 2. The varying results do not provide con-\nclusive evidence on whether the pretrained models\ncan learn to adapt to different domains in such ex-\ntreme settings. Answering this is non-trivial which\nrequires a careful consideration of the pretraining\ndatasets and characterization of domains based on—\ntask, topic, syntax, style etc.\n(a) SST2\n(b) OntoNotes\n(c) Gigaword\nFigure 4: Impact of personalization of clients on gener-\nalization of server—m∆P values depicting the impact\nin Uniform distribution strategy with varying sizes of\nclients.\nTime-Performance trade-off with varying client\nsizes The dropping of updates from the minor\nclients could also provide acceleration in the num-\nber of rounds, R, as federated learning has a com-\nmunication overhead. We measure the performance\nof server with pretrained weights, PerfS, over the\ndifferent tasks and the number of rounds (R) taken\n460\n(a) SST2-Random-Accuracy\n (b) OntoNotes-Random-F1\n (c) Gigaword-Random-Rouge1\n(d) SST2-Uniform-Accuracy\n (e) OntoNotes-Uniform-F1\n (f) Gigaword-Uniform-Rouge1\nFigure 5: We compare Rtaken to converge when varying the number of clients in the uniform and in the random\ndistribution settings with pretrained weights (✓). We also measure the corresponding PerfS of the server model in\nthe task.\nby the set up to converge.\nIn Figure 5, we compare across the tasks with\nrandom distribution and the uniform distribution of\nsamples with pretrained weights by varying the τ\nand γrespectively. In the random distribution ex-\nperiments, as we are discarding parameter updates\nfrom clients we observe a not-so-steep drop in the\nPerfS of the server model in SST-2 and OntoNotes.\nOn the other hand, in Gigaword dataset experi-\nments the impact of dropping the clients did not\naffect PerfS. With respect to R, τ value being in-\nversely proportional to the number of clients, we\ndid not see a drastic acceleration to the number of\nrounds as lesser clients also increased the difficulty\nof the tasks.\nIn uniform setting, by varying the number of\nclients without the data loss, we make two ob-\nservations: ( 1) The PerfS is always better than\nwhen compared with random setting, (2) the per-\nformance saturates after a certainγacross the tasks,\nand (3) the number of rounds taken by the models\nto converge shows drastic decrease as the num-\nber of clients decreases. We hypothesize that with\nonly major clients the gradient updates are stable\nto enable faster convergence. This contradicts with\nthe observation in (Lin et al., 2021) that shows a\nwider gap in the performance when training pre-\ntrained transformer models in a federated set up,\nwhich we observe only when not using pretrained\nweights. Collectively, the results hint that the fed-\nerated set up with PLMs suffer from personalizing\nto the client distributions, and the generalization\non tasks may be a regularization of the distributed\nset-up.\n6 Conclusion\nThis work explores pertinent questions that require\na closer look at evaluating PLMs in the federated\nsetting. Through empirical observations, we find\nthat in federated learning, where the emphasis is\nmore on personalization while ensuring privacy\nthere could be a risk of pretrained models over-\nlooking the client distributions. We also evalu-\nated the effects of varying the client distributions\nwhich suggested that the gap between centralized\nand federated performance to be reduced when the\nsamples are uniformly distributed over the clients.\nWhile that is ideal, the random distribution too\ndoes not suffer significant performance loss with\npretrained weights. However, the critical aspect of\nthe questions stems from the need to investigate the\npretraining routine in identifying the right domain\nadaptation challenges for pretrained models. The\ngap being minimized while the personalization tak-\ning a toll calls for a deeper inspection to explore\nthe limits of domain adaptation in PLMs with an\nappropriate evaluation framework (datasets, and\nmetrics) that controls for the leak in the pretraining\ncorpus.\n461\nAcknowledgements\nWe thank Guojun Zhang, and Xi Chen from the\nfederated learning team at Huawei Noah’s Ark Lab,\nMontréal for the many interesting discussions. We\nthank the anonymous reviewers for their insightful\ncomments to our work. We also want to reproduce\nour results on Mindspore 6 in future, which is a\nnew deep learning computing framework.\nLimitations\nThe study, though, considers sample tasks from\nthe different language tasks the downstream tasks\ngenerally are smaller in the size, and not much di-\nversity with respect to the task complexity is consid-\nered. Though there is motivation for using FedOpt\nfor training, the claims could have been further\nsupported by exploring other possible federated al-\ngorithms. The scale of the experiments however\ndo not play in favour of such an exhaustive study.\nAlthough the distributional similarity is measured\nwith MAUVE, other aspects of texts n-gram, topic\nmodelling could be explored to understand the do-\nmain shifts. Further, the study does not consider\nlanguage models with different other inductive bi-\nases. The different transformer models and the\neffect of their respective pretraining datasets and\ntask remain unexplored for future work. In addition\nto the above, the behaviour of different federated\nalgorithms in the hypotheses we frame would also\nbecome interesting cases to scale our work.\nBroader Impact\nThe trend of fine tuning transformer models for\ndownstream tasks as both time and cost-effective\nsolution for improving performance in downstream\ntasks has been gaining enough popularity. With fed-\nerated algorithms giving access to learning from\nmore public data while tackling the privacy con-\ncerns, it becomes worthwhile to use pretrained lan-\nguage models for language applications. Thus, un-\nderstanding the adjustments to this federated lan-\nguage task learning with pretrained transformers on\nthe claims of personalization-generalization trade-\noff becomes necessary. Knowledge and role of\nvariables like client sizes and their distribution on\nthe federated performance help identifying better\ndecisions on setting up an appropriate domain for\nlearning in downstream NLP tasks.\n6https://www.mindspore.cn/\nReferences\nDavid Arthur and Sergei Vassilvitskii. 2006. k-\nmeans++: The advantages of careful seeding. Tech-\nnical report, Stanford.\nMuhammad Asad, Ahmed Moustafa, and Takayuki Ito.\n2020. Fedopt: Towards communication efficiency\nand privacy preservation in federated learning. Ap-\nplied Sciences, 10(8):2864.\nYogesh Balaji, Swami Sankaranarayanan, and Rama\nChellappa. 2018. Metareg: Towards domain gen-\neralization using meta-regularization. Advances in\nneural information processing systems, 31.\nShai Ben-David, John Blitzer, Koby Crammer, Alex\nKulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. 2010. A theory of learning from different\ndomains. Machine learning, 79(1):151–175.\nJohn Blitzer. 2008. Domain adaptation of natural lan-\nguage processing systems. Ph.D. thesis, University\nof Pennsylvania.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 con-\nference on empirical methods in natural language\nprocessing, pages 120–128.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models. arXiv preprint arXiv:2202.07646.\nFei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, and\nXiuqiang He. 2018. Federated meta-learning with\nfast convergence and efficient communication. arXiv\npreprint arXiv:1802.07876.\nRonan Collobert and Jason Weston. 2008. A unified\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Proceed-\nings of the 25th international conference on Machine\nlearning, pages 160–167.\nXia Cui and Danushka Bollegala. 2019. Self-adaptation\nfor unsupervised domain adaptation. Proceedings-\nNatural Language Processing in a Deep Learning\nWorld.\nHal Daumé III. 2009. Frustratingly easy domain adap-\ntation. arXiv preprint arXiv:0907.1815.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\n462\nMark Dredze and Koby Crammer. 2008. Online meth-\nods for multi-domain learning and adaptation. In\nProceedings of the 2008 Conference on Empirical\nMethods in Natural Language Processing, pages 689–\n697.\nChristophe Dupuy, Tanya G Roosta, Leo Long, Clement\nChung, Rahul Gupta, and Salman Avestimehr. 2022.\nLearnings from federated learning in the real world.\nIn ICASSP 2022-2022 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 8767–8771. IEEE.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nSuyu Ge, Fangzhao Wu, Chuhan Wu, Tao Qi, Yongfeng\nHuang, and Xing Xie. 2020. Fedner: Privacy-\npreserving medical named entity recognition with\nfederated learning. arXiv preprint arXiv:2003.09288.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.\n2003. English gigaword. Linguistic Data Consor-\ntium, Philadelphia, 4(1):34.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsu-\npervised domain adaptation of contextualized em-\nbeddings for sequence labeling. arXiv preprint\narXiv:1904.02817.\nPeter Kairouz, H Brendan McMahan, Brendan Avent,\nAurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,\nKallista Bonawitz, Zachary Charles, Graham Cor-\nmode, Rachel Cummings, et al. 2021. Advances and\nopen problems in federated learning. Foundations\nand Trends® in Machine Learning, 14(1–2):1–210.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh\nKaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-\ngen, Grusha Prasad, Amanpreet Singh, Pratik Ring-\nshia, et al. 2021. Dynabench: Rethinking benchmark-\ning in nlp. arXiv preprint arXiv:2104.14337.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nJakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Pe-\nter Richtárik, Ananda Theertha Suresh, and Dave\nBacon. 2016. Federated learning: Strategies for im-\nproving communication efficiency. arXiv preprint\narXiv:1610.05492.\nWouter M Kouw and Marco Loog. 2018. An intro-\nduction to domain adaptation and transfer learning.\narXiv preprint arXiv:1812.11806.\nAnanya Kumar, Tengyu Ma, and Percy Liang. 2020. Un-\nderstanding self-training for gradual domain adapta-\ntion. In International Conference on Machine Learn-\ning, pages 5468–5479. PMLR.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nDaliang Li and Junpu Wang. 2019. Fedmd: Heteroge-\nnous federated learning via model distillation. arXiv\npreprint arXiv:1910.03581.\nQinbin Li, Yiqun Diao, Quan Chen, and Bingsheng\nHe. 2021. Federated learning on non-iid data\nsilos: An experimental study. arXiv preprint\narXiv:2102.02079.\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar San-\njabi, Ameet Talwalkar, and Virginia Smith. 2020.\nFederated optimization in heterogeneous networks.\nProceedings of Machine Learning and Systems ,\n2:429–450.\nBill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin\nWang, Yufen Huang, Mahdi Soltanolkotabi, Xiang\nRen, and Salman Avestimehr. 2021. Fednlp: A re-\nsearch platform for federated learning in natural lan-\nguage processing. arXiv preprint arXiv:2104.08815.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nBing Liu and Sahisnu Mazumder. 2021. Lifelong and\ncontinual learning dialogue systems: learning during\nconversation. Proceedings of AAAI-2021.\nYang Liu, Anbu Huang, Yun Luo, He Huang, Youzhi\nLiu, Yuanyuan Chen, Lican Feng, Tianjian Chen,\nHan Yu, and Qiang Yang. 2020. Fedvision: An\nonline visual object detection platform powered by\nfederated learning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nMi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang,\nand Jiashi Feng. 2021. No fear of heterogeneity:\nClassifier calibration for federated learning with non-\niid data. Advances in Neural Information Processing\nSystems, 34:5972–5984.\nPriyanka Mary Mammen. 2021. Federated learn-\ning: Opportunities and challenges. arXiv preprint\narXiv:2101.05428.\n463\nBrendan McMahan, Eider Moore, Daniel Ramage,\nSeth Hampson, and Blaise Aguera y Arcas. 2017a.\nCommunication-efficient learning of deep networks\nfrom decentralized data. In Artificial intelligence and\nstatistics, pages 1273–1282. PMLR.\nBrendan McMahan and Daniel Ramage. 2017. Feder-\nated learning: Collaborative machine learning with-\nout centralized training data. Google Blog.\nH Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2017b. Learning differentially pri-\nvate recurrent language models. arXiv preprint\narXiv:1710.06963.\nMatias Mendieta, Taojiannan Yang, Pu Wang, Minwoo\nLee, Zhengming Ding, and Chen Chen. 2022. Local\nlearning matters: Rethinking data heterogeneity in\nfederated learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 8397–8406.\nDaniel W Otter, Julian R Medina, and Jugal K Kalita.\n2020. A survey of the usages of deep learning for\nnatural language processing. IEEE transactions on\nneural networks and learning systems , 32(2):604–\n624.\nYifan Peng, Qingyu Chen, and Zhiyong Lu. 2020.\nAn empirical study of multi-task learning on\nbert for biomedical text mining. arXiv preprint\narXiv:2005.02799.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. Advances in Neural Information Process-\ning Systems, 34:4816–4828.\nJoaquin Quiñonero-Candela, Masashi Sugiyama, Anton\nSchwaighofer, and Neil D Lawrence. 2008. Dataset\nshift in machine learning. Mit Press.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog.\nAlan Ramponi and Barbara Plank. 2020. Neural unsu-\npervised domain adaptation in nlp—a survey. arXiv\npreprint arXiv:2006.00632.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timo-\nthy Lillicrap, and Gregory Wayne. 2019. Experience\nreplay for continual learning. Advances in Neural\nInformation Processing Systems, 32.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba,\nVicki Cheung, Alec Radford, and Xi Chen. 2016.\nImproved techniques for training gans. Advances in\nneural information processing systems, 29.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nSamuel Stanton, Pavel Izmailov, Polina Kirichenko,\nAlexander A Alemi, and Andrew G Wilson. 2021.\nDoes knowledge distillation really work? Advances\nin Neural Information Processing Systems, 34:6906–\n6919.\nAsa Cooper Stickland and Iain Murray. 2019. Bert and\npals: Projected attention layers for efficient adapta-\ntion in multi-task learning. In International Con-\nference on Machine Learning , pages 5986–5995.\nPMLR.\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. 2019.\nLamol: Language modeling for lifelong language\nlearning. arXiv preprint arXiv:1909.03329.\nZijie J Wang, Dongjin Choi, Shenyu Xu, and Diyi\nYang. 2021. Putting humans in the natural lan-\nguage processing loop: A survey. arXiv preprint\narXiv:2103.04044.\nJason Wei, Dan Garrette, Tal Linzen, and Ellie Pavlick.\n2021. Frequency effects on syntactic rule learning in\ntransformers. arXiv preprint arXiv:2109.07020.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, Mohammed El-Bachouti, Robert Belvin,\nand Ann Houston. 2013. OntoNotes Release 5.0 .\nAbacus Data Network.\nHu Xu, Bing Liu, Lei Shu, and Philip S Yu. 2018.\nLifelong domain word embedding via meta-learning.\narXiv preprint arXiv:1805.09991.\nDong Yang, Ziyue Xu, Wenqi Li, Andriy Myronenko,\nHolger R Roth, Stephanie Harmon, Sheng Xu, Baris\nTurkbey, Evrim Turkbey, Xiaosong Wang, et al. 2021.\nFederated semi-supervised learning for covid region\nsegmentation in chest ct using multi-national data\nfrom china, italy, japan. Medical image analysis ,\n70:101992.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\n464\nWeiming Zhuang, Xin Gan, Yonggang Wen, Shuai\nZhang, and Shuai Yi. 2021. Collaborative unsuper-\nvised visual representation learning from decentral-\nized data. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 4912–\n4921.\n465\nA Reproducibility Checklist\nA.1 For all reported experimental results:\n1. A clear description of the mathematical set-\nting, algorithm, and/or model: We define the\ndetails of our experimental setup in §4.\n2. Description of computing infrastructure used:\nWe use multiple servers equipped with 8\nNVIDIA V100 (32 GB) GPUs and 72 cores\nCPU (754 GB) for running our experiments.\n3. The average runtime for each model or algo-\nrithm (e.g., training, inference, etc.), or es-\ntimated energy cost and number of parame-\nters in each model: The details of the run-\ntime costs per experiment and the model have\nbeen reported in Table 8. The experiment runs\nhave been tabulated in Table 9, Table 10 &\nTable 11.\n4. Corresponding validation performance for\neach reported test result: Not applicable.\n5. Explanation of evaluation metrics used, with\nlinks to code: This is specified with references\nin paragraph titled ‘Evaluation’ in §2.\nA.2 For all experiments with hyperparameter\nsearch:\n1. The exact number of training and evaluation\nruns: We run all centralized and random distri-\nbution on 3 seeds while the uniform distribu-\ntion experiments are run on a single seed. The\nrandom distribution leads to different client\nsize distributions while the uniform distribu-\ntion has all clients of similar size.\n2. Bounds for each hyperparameter:The tunable\nhyperparameters were batch size and learning\nrate in both centralized and federated training.\nFor each dataset in both cases of fine-tuning\nand training from scratch, we find the best\nlearning rate in the range [0.01,0.000001] for\ncentralized and federated training by tuning\non the exponent scale. For federated training\nwe find the best hyperparameters in random\ndistribution setting which we continue to use\nin other variants of our experiments. For the\nbatch size, we explore in the set 8,16,32,64.\n3. Hyperparameter configurations for best-\nperforming models: The Table 4, Table 5,\nTable 6 & Table 7 records the best hyperpa-\nrameters in use.\n4. Number of hyperparameter search trials: The\nbest hyperparameters are chosen over 3 seeds.\n5. The method of choosing hyperparameter val-\nues (e.g., uniform sampling, manual tuning,\netc.) and the criterion used to select among\nthem (e.g., accuracy) The best test loss re-\nsulting combination of hyperparameters is se-\nlected. The grid search method is used.\n6. Summary statistics of the results (e.g., mean,\nvariance, error bars, etc.) The tabulated re-\nsults show mean and standard deviation re-\nsults while the line plots are created using me-\ndian as an estimator. The plots involving test\nor train loss account the evaluations done on a\nsample level. The plots using Perf evaluations\nuse set of experiment level values.\nA.3 For all datasets used:\n1. Relevant details such as languages, and num-\nber of examples and label distributions: The\ndatasets used are SST2, OntoNotes and Giga-\nword which are all in the English language.\n2. Details of train/validation/test splits: This can\nbe found tabulated in Table 3.\n3. Explanation of any data that were excluded,\nand all pre-processing steps: For the task of\ntext classification we use the complete sen-\ntences as samples instead the parsed phrases.\n4. A zip file containing data or link to a down-\nloadable version of the data: The references\nto the datasets are provided in §4.\n5. For new data collected, a complete descrip-\ntion of the data collection process, such as\ninstructions to annotators and methods for\nquality control. Not applicable.\n466\nB Dataset Splits\nDataset Train set Test set Labels\nSST2 6,920 1,821 2\nOntoNotes 59,924 8,262 37\nGigaword 10,000 2000 N.A.\nTable 3: Statistics for the 3 different dataset used.\nC Client Data Distribution\nDuring the gradient accumulation, we normally use uniform weightage. For sanity check if uniform\nweighting is the best choice, we made comparison of the random distribution with SST2 dataset using\nweighted aggregation where the client gradients are weighted to their size proportions. We did not see any\nadvantage and hence continued using the uniform weightage. The comparison in performance can be seen\nin Table 9.\nC.1 Random Distribution\nDataset Clients τ\nSST2 3-100 {0, 100, 300, 500}\nOntoNotes 4-54 {0, 40, 120, 700, 4000}\nGigaword 7-63 {0, 18, 34, 126, 750, 1200}\nTable 4: Statistics for random distribution strategy experiments. The number of clients are effected by the ablation\nthreshold for minor clients (τ).\nC.2 Uniform Distribution\nDataset Clients γ\nSST2 5-173 {600, 500, 485, 460,\n320, 190, 80, 40, 20}\nOntoNotes 6-54 {1300, 630, 420, 321,\n252, 209, 180, 159, 139}\nGigaword 7-63 {180, 90, 60, 45,\n36, 30, 26, 22, 20}\nTable 5: Statistics for uniform distribution strategy experiments. The client count increases as the number of samples\nper label in a client (γ) decreases.\n467\nD Hyperparameters\nThe experiments on random distribution for all the datasets were carried out with 3 different seeds.\nHowever, for the uniform distribution we use only a single seed for OntoNotes and Gigaword datasets.\nUnlike the random distribution where the sampled client sizes keeps varying dramatically, the uniform\ndistribution has all clients with almost the same number of data samples. Thus, we relax the need for\nrepeating experiments with multiple seeds in the uniform distribution.\nDataset Pretraining Epochs Batch size L.R.\nSST2 ✓ 10 8 1.00E-05\n✗ 10 8 1.00E-05\nOntoNotes ✓ 5 8 2.00E-05\n✗ 5 32 2.00E-04\nGigaword ✓ 5 8 3.00E-05\n✗ 5 8 3.00E-05\nTable 6: The hyperparameters used for the centralized training experiments.\nDataset Pretraining Rounds Batch size L.R.\nSST2 ✓ 30 64 1.00E-05\n✗ 50 64 1.00E-05\nSST2 (Weighted Aggregation) ✓ 30 64 1.00E-04\n✗ 50 64 1.00E-04\nOntoNotes ✓ 30 64 2.00E-05\n✗ 50 64 2.00E-05\nGigaword ✓ 10 8 3.00E-05\n✗ 15 8 3.00E-05\nTable 7: The hyperparameters used for the federated training experiments.\nE Runtime of the experiments\nDataset Model (parameters) Experiment Pretrained GPUs Runtime (Hrs)\nSST2 distilbert-base-uncased (66.9M)\nCentralized Training ✓ 1 ∼0.5\n✗ ∼0.5\nRandom Distribution ✓ 8 5-6\n✗ 8-10\nUniform Distribution ✓ 8 9-12\n✗ 15-20\nOntoNotes distilbert-base-uncased (66.4M)\nCentralized Training ✓ 1 ∼1\n✗ ∼1\nRandom Distribution ✓ 8 4-5\n✗ 6.5-7.5\nUniform Distribution ✓ 8 6-27.5\n✗ 10-46\nGigaword facebook/bart-base (139.4M)\nCentralized Training ✓ 1 ∼1\n✗ ∼1\nRandom Distribution ✓ 8 2-15\n✗ 3-22.5\nUniform Distribution ✓ 8 3.5-17.5\n✗ 5-26.5\nTable 8: Time and resource costs per experiment run for the different datasets. The GPU refers to the NVIDIA\nV100 (32 GB) in a server having 8 of them.\n468\nF Master Results Tables\nF.1 SST 2\nCentralized learning\nPretraining Test Accuracy (%) Epochs\n✓ 89.02 ± 0.83 4.33 ± 2.87\n✗ 69.19 ± 3.2 5.33 ± 2.62\nRandom distribution (τ=0, 100 clients) (Weighted Aggregation)\nPretraining Test Accuracy (%) Rounds\n✓ 86.84 ± 0.1 30.0 ± 0.0\n✗ 52.99 ± 2.39 49.67 ± 0.47\nRandom distribution (τ=0, 100 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 87.77 ± 0.4 28.67 ± 1.89\n✗ 67.78 ± 1.04 50.0 ± 0.0\nRandom distribution (τ=100, 100 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 85.89 ± 1.08 30.0 ± 0.0\n✗ 64.34 ± 1.05 49.67 ± 0.47\nRandom distribution (τ=300, 100 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 86.44 ± 0.39 30.0 ± 0.0\n✗ 62.84 ± 0.5 50.0 ± 0.0\nRandom distribution (τ=500, 100 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 84.79 ± 1.42 30.0 ± 0.0\n✗ 61.14 ± 1.46 50.0 ± 0.0\nUniform distribution (γ=20, 173 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 88.05 ± 0.32 29.67 ± 0.47\n✗ 66.68 ± 1.17 50.0 ± 0.0\nUniform distribution (γ=40, 87 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 88.61 ± 0.23 30.0 ± 0.0\n✗ 72.29 ± 0.4 49.33 ± 0.47\nUniform distribution (γ=80, 44 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 89.07 ± 0.13 29.33 ± 0.94\n✗ 74.32 ± 0.64 48.67 ± 0.94\nUniform distribution (γ=190, 19 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 89.2 ± 0.2 23.67 ± 1.7\n✗ 78.0 ± 0.4 47.67 ± 1.7\nUniform distribution (γ=320, 11 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 89.11 ± 0.32 18.0 ± 1.41\n✗ 79.06 ± 0.42 41.0 ± 2.94\nUniform distribution (γ=460, 8 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 89.38 ± 0.09 16.33 ± 0.47\n✗ 79.7 ± 0.14 34.33 ± 1.7\nUniform distribution (γ=485, 8 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 89.35 ± 0.68 18.0 ± 2.94\n✗ 79.64 ± 1.05 32.33 ± 2.87\nUniform distribution (γ=500, 7 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 89.64 ± 0.13 16.0 ± 0.82\n✗ 79.75 ± 0.32 31.33 ± 1.25\nUniform distribution (γ=600, 6 clients)\nPretraining Test Accuracy (%) Rounds\n✓ 88.96 ± 0.12 13.67 ± 0.94\n✗ 79.33 ± 0.39 28.33 ± 2.05\nTable 9: Results of all experiments on SST2 dataset after model selection on the best server test loss.\n469\nF.2 OntoNotes\nCentralized learning\nPretraining Test F1 (%) Epochs\n✓ 85.93 ± 0.13 3.33 ± 1.7\n✗ 65.1 ± 0.29 1.0 ± 0.0\nRandom distribution (τ=0, 54 clients)\nPretraining Test F1 (%) Rounds\n✓ 84.44 ± 0.05 29.67 ± 0.47\n✗ 55.31 ± 0.27 49.67 ± 0.47\nRandom distribution (τ=40, 54 clients)\nPretraining Test F1 (%) Rounds\n✓ 84.49 ± 0.05 30.0 ± 0.0\n✗ 55.41 ± 0.26 50.0 ± 0.0\nRandom distribution (τ=120, 54 clients)\nPretraining Test F1 (%) Rounds\n✓ 84.48 ± 0.1 29.67 ± 0.47\n✗ 55.4 ± 0.16 50.0 ± 0.0\nRandom distribution (τ=700, 54 clients)\nPretraining Test F1 (%) Rounds\n✓ 84.27 ± 0.25 29.67 ± 0.47\n✗ 55.08 ± 0.18 49.67 ± 0.47\nRandom distribution (τ=4000, 54 clients)\nPretraining Test F1 (%) Rounds\n✓ 83.98 ± 0.32 30.0 ± 0.0\n✗ 53.07 ± 1.23 50.0 ± 0.0\nUniform distribution (γ=139, 54 clients)\nPretraining Test F1 (%) Rounds\n✓ 77.7 30.0\n✗ 52.67 50.0\nUniform distribution (γ=159, 48 clients)\nPretraining Test F1 (%) Rounds\n✓ 79.86 30.0\n✗ 54.0 50.0\nUniform distribution (γ=180, 42 clients)\nPretraining Test F1 (%) Rounds\n✓ 82.05 30.0\n✗ 55.72 50.0\nUniform distribution (γ=209, 36 clients)\nPretraining Test F1 (%) Rounds\n✓ 83.43 29.0\n✗ 57.38 50.0\nUniform distribution (γ=252, 30 clients)\nPretraining Test F1 (%) Rounds\n✓ 84.61 30.0\n✗ 60.26 50.0\nUniform distribution (γ=321, 24 clients)\nPretraining Test F1 (%) Rounds\n✓ 83.99 29.0\n✗ 63.2 50.0\nUniform distribution (γ=420, 18 clients)\nPretraining Test F1 (%) Rounds\n✓ 85.16 29.0\n✗ 65.22 48.0\nUniform distribution (γ=630, 12 clients)\nPretraining Test F1 (%) Rounds\n✓ 84.79 23.0\n✗ 67.61 45.0\nUniform distribution (γ=1300, 6 clients)\nPretraining Test F1 (%) Rounds\n✓ 84.49 16.0\n✗ 66.14 28.0\nTable 10: Results of all experiments on OntoNotes dataset after model selection on the best server test loss.\n470\nF.3 Gigaword\nCentralized learning\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Epochs\n✓ 34.57 ± 0.66 15.92 ± 0.36 32.35 ± 0.59 3.0 ± 1.41\n✗ 6.07 ± 0.51 0.36 ± 0.08 5.86 ± 0.5 3.0 ± 1.63\nRandom distribution (τ=0, 63 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 32.48 ± 0.22 14.28 ± 0.02 30.61 ± 0.19 10.0 ± 0.0\n✗ 2.89 ± 0.56 0.05 ± 0.02 2.89 ± 0.54 15.0 ± 0.0\nRandom distribution (τ=18, 63 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 34.72 ± 0.25 15.57 ± 0.08 32.25 ± 0.16 10.0 ± 0.0\n✗ 1.87 ± 0.3 0.01 ± 0.01 1.83 ± 0.33 14.33 ± 0.94\nRandom distribution (τ=34, 63 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 34.46 ± 0.54 15.48 ± 0.22 32.07 ± 0.36 10.0 ± 0.0\n✗ 2.0 ± 0.3 0.03 ± 0.01 1.96 ± 0.32 14.33 ± 0.94\nRandom distribution (τ=126, 63 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 32.86 ± 0.97 14.45 ± 0.61 30.88 ± 0.82 10.0 ± 0.0\n✗ 1.54 ± 0.24 0.02 ± 0.01 1.51 ± 0.26 14.33 ± 0.47\nRandom distribution (τ=750, 63 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 32.8 ± 1.73 14.54 ± 0.91 30.9 ± 1.39 8.33 ± 1.7\n✗ 3.29 ± 1.74 0.01 ± 0.01 3.22 ± 1.68 13.67 ± 1.89\nRandom distribution (τ=1200, 63 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 34.17 ± 0.38 15.41 ± 0.23 32.09 ± 0.26 9.33 ± 0.47\n✗ 4.71 ± 0.98 0.0 ± 0.0 4.67 ± 0.97 11.33 ± 1.25\nUniform distribution (γ=20, 63 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 33.05 14.56 31.04 10.0\n✗ 2.41 0.0 2.4 15.0\nUniform distribution (γ=22, 57 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 33.36 14.89 31.35 10.0\n✗ 1.58 0.04 1.56 15.0\nUniform distribution (γ=26, 49 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 33.68 15.03 31.66 10.0\n✗ 2.07 0.0 2.09 15.0\nUniform distribution (γ=30, 42 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 34.44 15.43 32.32 10.0\n✗ 2.06 0.0 2.06 15.0\nUniform distribution (γ=36, 35 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 35.09 15.88 32.68 10.0\n✗ 2.18 0.0 2.17 15.0\nUniform distribution (γ=45, 28 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 35.48 16.19 33.13 10.0\n✗ 5.59 0.11 5.38 15.0\nUniform distribution (γ=60, 21 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 35.39 16.38 33.11 10.0\n✗ 6.44 0.17 6.23 15.0\nUniform distribution (γ=90, 14 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 35.43 16.32 33.15 10.0\n✗ 2.73 0.1 2.69 15.0\nUniform distribution (γ=180, 7 clients)\nPretraining Test Rouge1 (%) Test Rouge2 (%) Test RougeL (%) Rounds\n✓ 35.8 15.99 33.2 10.0\n✗ 10.68 1.65 10.2 15.0\nTable 11: Results of all experiments on Gigaword dataset after model selection on the best server test loss.\n471",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8643438816070557
    },
    {
      "name": "Treebank",
      "score": 0.8533840179443359
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6671358346939087
    },
    {
      "name": "Language model",
      "score": 0.6609985828399658
    },
    {
      "name": "Domain adaptation",
      "score": 0.6335114240646362
    },
    {
      "name": "Transformer",
      "score": 0.5818767547607422
    },
    {
      "name": "Natural language processing",
      "score": 0.5346917510032654
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.49941539764404297
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4488983154296875
    },
    {
      "name": "Machine learning",
      "score": 0.44718703627586365
    },
    {
      "name": "Federated learning",
      "score": 0.44509491324424744
    },
    {
      "name": "Inductive bias",
      "score": 0.42056775093078613
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.41204535961151123
    },
    {
      "name": "Multi-task learning",
      "score": 0.3152413070201874
    },
    {
      "name": "Parsing",
      "score": 0.19545593857765198
    },
    {
      "name": "Task (project management)",
      "score": 0.07814446091651917
    },
    {
      "name": "Classifier (UML)",
      "score": 0.07801052927970886
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}