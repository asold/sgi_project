{
  "title": "Leveraging transformers and large language models with antimicrobial prescribing data to predict sources of infection for electronic health record studies",
  "url": "https://openalex.org/W4394948763",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2482481020",
      "name": "Kevin Yuan",
      "affiliations": [
        "University of Oxford",
        "Open Data Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2582721644",
      "name": "Chang Ho Yoon",
      "affiliations": [
        "University of Oxford",
        "Open Data Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2808482189",
      "name": "Qing-Ze Gu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2603508690",
      "name": "Henry Munby",
      "affiliations": [
        "University Hospitals Bristol NHS Foundation Trust",
        "University Hospitals Bristol and Weston NHS Foundation Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2153281715",
      "name": "A Sarah Walker",
      "affiliations": [
        "University of Oxford",
        "Oxford BioMedica (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2109825558",
      "name": "Tingting Zhu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2126843395",
      "name": "David W Eyre",
      "affiliations": [
        "Oxford University Hospitals NHS Trust",
        "Oxford BioMedica (United Kingdom)",
        "University of Oxford",
        "Open Data Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2482481020",
      "name": "Kevin Yuan",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2582721644",
      "name": "Chang Ho Yoon",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2808482189",
      "name": "Qing-Ze Gu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2603508690",
      "name": "Henry Munby",
      "affiliations": [
        "University Hospitals Bristol and Weston NHS Foundation Trust",
        "University Hospitals Bristol NHS Foundation Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2153281715",
      "name": "A Sarah Walker",
      "affiliations": [
        "Oxford BioMedica (United Kingdom)",
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2109825558",
      "name": "Tingting Zhu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2126843395",
      "name": "David W Eyre",
      "affiliations": [
        "University of Oxford",
        "Oxford BioMedica (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2317648909",
    "https://openalex.org/W2997101676",
    "https://openalex.org/W3152962621",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W2991490108",
    "https://openalex.org/W2805881292",
    "https://openalex.org/W2755089631",
    "https://openalex.org/W3083892815",
    "https://openalex.org/W4283156650",
    "https://openalex.org/W2101082552",
    "https://openalex.org/W2755626276",
    "https://openalex.org/W2809024468",
    "https://openalex.org/W2095754613",
    "https://openalex.org/W4280622721",
    "https://openalex.org/W1965092590",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W4394580037"
  ],
  "abstract": "Abstract Background Electronic health records frequently contain extensive unstructured free-text data, but extracting information accurately from these data and at scale is challenging. Using free-text from antibiotic prescribing data as an example, we investigate the performance of modern natural language processing methods (NLP) and large language models (LLMs) as tools for extracting features from medical records. Methods We used 938,150 hospital antibiotic prescriptions from Oxfordshire, UK. The 4000 most frequently used free-text indications justifying antibiotic use were labelled by clinical researchers into 11 categories describing the infection source/clinical syndrome being treated and used for model training. Traditional classification methods, fuzzy regex matching and n-grams with XGBoost, were compared against modern transformer models: we fine-tuned generic and domain-specific BERT models, fine-tuned GPT3.5, and investigated few-shot learning with GPT4. Models were evaluated on internal and external test datasets (2000 prescriptions each). Infection sources determined from ICD10 codes were also used for comparisons. Results In internal and external test datasets, the fine-tuned domain-specific Bio+Clinical BERT model averaged an F1 score of 0.97 and 0.98 respectively across the classes and outperformed the traditional regex (F1=0.71 and 0.74) and n-grams/XGBoost (F1=0.86 and 0.84). OpenAI’s GPT4 model achieved F1 scores of 0.71 and 0.86 without using labelled training data and a fine-tuned GPT3.5 model F1 scores of 0.95 and 0.97. Comparing infection sources extracted from ICD10 codes to those parsed from free-text indications, free-text indications revealed 31% more specific infection sources. Conclusion Modern transformer-based models can efficiently and accurately categorise semi-structured free-text in medical records, such as prescription free-text. Finetuned local transformer models outperform LLMs currently for structured tasks. Few shot LLMs match the performance of traditional NLP without the need for labelling. Transformer-based models have the potential to be used widely throughout medicine to analyse medical records more accurately, facilitating beter research and patient care.",
  "full_text": "Leveraging transformers and large language models with an�microbial \nprescribing data to predict sources of infec�on for electronic health \nrecord studies \n \nKevin Yuan1*, Chang Ho Yoon1*, Qingze Gu2*, Henry Munby3, A Sarah Walker2,4,5, Ting�ng Zhu6, David \nW Eyre1,4,5,7 \n \n*Contribu�on considered equal. \n \n1Big Data Ins�tute, Nuﬃeld Department of Popula�on Health, University of Oxford, Oxford, UK \n2Nuﬃeld Department of Medicine, University of Oxford, Oxford, UK \n3University Hospitals Bristol & Weston NHS Trust, Bristol, UK \n4NIHR Health Protec�on Research Unit in Healthcare Associated Infec�ons and An�microbial \nResistance, University of Oxford, Oxford, UK \n5NIHR Oxford Biomedical Research Centre, Oxford, UK \n6Ins�tute of Biomedical Engineering, University of Oxford, Oxford, UK \n7Oxford University Hospitals NHS Founda�on Trust, Oxford, UK \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nAbstract \nBackground. Electronic health records frequently contain extensive unstructured free-text data, but \nextrac�ng informa�on accurately from these data and at scale is challenging. Using free-text from \nan�bio�c prescribing data as an example, we inves�gate the performance of modern natural \nlanguage processing methods (NLP) and large language models (LLMs) as tools for extrac�ng features \nfrom medical records. \nMethods. We used 938,150 hospital an�bio�c prescrip�ons from Oxfordshire, UK. The 4000 most \nfrequently used free-text indica�ons jus�fying an�bio�c use were labelled by clinical researchers into \n11 categories describing the infec�on source/clinical syndrome being treated and used for model \ntraining. Tradi�onal classiﬁca�on methods, fuzzy regex matching and n-grams with XGBoost, were \ncompared against modern transformer models: we ﬁne-tuned generic and domain-speciﬁc BERT \nmodels, ﬁne-tuned GPT3.5, and inves�gated few-shot learning with GPT4. Models were evaluated on \ninternal and external test datasets (2000 prescrip�ons each). Infec�on sources determined from \nICD10 codes were also used for comparisons. \nResults. In internal and external test datasets, the ﬁne-tuned domain-speciﬁc Bio+Clinical BERT \nmodel averaged an F1 score of 0.97 and 0.98 respec�vely across the classes and outperformed the \ntradi�onal regex (F1=0.71 and 0.74) and n-grams/XGBoost (F1=0.86 and 0.84). OpenAI’s GPT4 model \nachieved F1 scores of 0.71 and 0.86 without using labelled training data and a ﬁne-tuned GPT3.5 \nmodel F1 scores of 0.95 and 0.97. Comparing infec�on sources extracted from ICD10 codes to those \nparsed from free-text indica�ons, free-text indica�ons revealed 31% more speciﬁc infec�on sources. \nConclusion. Modern transformer-based models can eﬃciently and accurately categorise semi-\nstructured free-text in medical records, such as prescrip�on free-text. Finetuned local transformer \nmodels outperform LLMs currently for structured tasks. Few shot LLMs match the performance of \ntradi�onal NLP without the need for labelling. Transformer-based models have the poten�al to be \nused widely throughout medicine to analyse medical records more accurately, facilita�ng beter \nresearch and pa�ent care.  \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nIntroduc�on  \nElectronic health records (EHRs) oﬀer unprecedented quan��es of structured and unstructured data \nfor driving research and improving care delivery. Manually extrac�ng relevant informa�on from \nunstructured free-text EHRs is costly and laborious. Recent developments in natural language \nprocessing (NLP) and the advent of large language models (LLMs) oﬀer promising and poten�ally \ntransforma�onal alterna�ves that can accurately acquire relevant informa�on from unstructured text \nfor pa�ent diagnosis, 1–3 as well as perform several rou�ne tasks from medical records.4,5   \nAs in other medical domains, studies of an�bio�c resistance, use, and stewardship have tradi�onally \nrelied on manual review of clinical notes and prescrip�ons6–8 or mapping of Interna�onal \nClassiﬁca�on of Diseases (ICD) diagnos�c codes to iden�fy acute infec�on diagnoses and chronic \ncomorbidi�es.9,10 However, in studies of sepsis, ICD codes were less sensi�ve than clinical data for \ndetec�ng cases,11,12 par�cularly for less common infec�ons like meningi�s,13 and had variable \nvalidity.14 Addi�onally, since codes are recorded only a�er pa�ent discharge, assigned infec�on \nsources may not align with individual an�bio�c prescrip�ons, par�cularly during the more uncertain \nini�al phase of inpa�ent care. Conversely, manual chart review has higher sensi�vity and can detect \nindica�ons evolving over �me, but �me and cost constraints mean that case numbers are o�en \nlimited. \nFor research, applying LLMs to en�re medical records can eﬀec�vely make predic�ons and generate \nnew medical content.4,5 However, there is also a clear need for research and service applica�ons to be \nable to extract speciﬁc individual features from EHRs reliably and eﬃciently whilst also mee�ng \ninforma�on governance requirements. As an example of this targeted approach, we inves�gated \nseveral methods using electronic prescribing data to classify the source of an infec�on/infec�ous \nsyndrome being treated, analysing free-text indica�ons documented by clinicians15 jus�fying the \nreason for an�bio�c prescrip�ons. We compared state-of-the-art NLP models, Bidirec�onal Encoder \nRepresenta�ons from Transformers (BERT),16 LLMs, and Genera�ve Pretrained Transformer (GPT),17 to \ntradi�onal ICD code-based approaches, classical NLP methods and regular expression-based text \nsearches. We show modern approaches have poten�al to accurately extract key informa�on from \nmedical records at scale, poten�ally opening opportuni�es for new epidemiological and interven�on \nstudies across all of medicine, as well as possibili�es for improving care delivery. \nMethods \nStudy design and popula�on \nWe used EHRs from two dis�nct hospital sites, Oxford and Banbury, with Oxford serving as our training \nand internal test set, and Banbury as our external test set. These sites collec�vely provide 1100 beds, \nserving 750,000 residents in Oxfordshire, ~1% of the UK popula�on. Deiden�ﬁed data were obtained \nfrom Infec�ons in Oxfordshire Research Database (IORD), which has approvals from the Na�onal \nResearch Ethics Service South Central – Oxford C Research Ethics Commitee (19/SC/0403), the Health \nResearch Authority and the Conﬁden�ality Advisory Group (19/CAG/0144)  as a deiden�ﬁed database \nwithout individual consent. All pa�ents aged ≥16 years who had an�bio�c prescrip�ons were included. \n‘Ground Truth’ Labelling  \nTwo clinical researchers reviewed each an�bio�c indica�on text string used for training and tes�ng \n(described below) to establish a reference or ‘ground truth’ label for the clinical syndrome being \ntreated. Any discrepancies were resolved by a third researcher, a clinical infec�on specialist. \nAn�bio�c indica�ons were labelled using 11 categories represen�ng infec�on source: Urinary; \nRespiratory; Abdominal; Neurological; Skin and So� Tissue; Ear, Nose and Throat (ENT); Orthopaedic; \nOther speciﬁc (i.e. another body site); Non-speciﬁc (i.e. no body site provided, e.g. “sepsis”), \nProphylaxis, Not informa�ve (i.e. text unrelated to the source of infec�on, e.g. “as instructed by Dr \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nX”). Each category was recorded as a binary variable, such that more than one poten�al source could \nbe recorded, e.g. the input string “urinary/chest” would be labelled as both urinary and respiratory. \nAn addi�onal variable was used to document the presence of uncertainty expressed by the \nprescriber, e.g.  “urinary/chest” or “? UTI”.  \nComparator Classiﬁca�on by ICD10 Codes \nAs a comparator, we mapped primary and secondary ICD10 diagnosis codes from the same admission \nas the an�bio�c prescrip�on to the 11 infec�on sources using CCSR classiﬁca�ons18 as an \nintermediate step (see Supplement and Appendix S1).  \nTradi�onal Classiﬁca�on Methods \nRegex Rules \nThe most intui�ve and determinis�c method for classifying free-text is searching for speciﬁc keywords \nfrom a list of predeﬁned words for a given category. We employed fuzzy regular expression (regex) \nmatching paterns with term-speciﬁc word boundaries and variable fuzziness to allow for misspellings \nand varia�ons (see Supplement and Appendix S2). \nn-grams & XGBoost \nA second approach used a separate tokeniser, embedding and classiﬁer structure; speciﬁcally, Scikit-\nlearn’s n-grams & count vectorisa�on and the gradient boos�ng model architecture XGBoost.19,20 \nEach free-text indica�on term was broken up into overlapping subwords of length $n$ and then \ncount-vectorised, with the count represen�ng the frequency of each subword’s occurrence. The \nvectors of dimension $vocabulary size$ were then fed as input features to the classiﬁca�on model. \nWe determined the op�mal n-gram size ($n$) and hyperparameters for XGBoost during model \ntraining by maximising the receiver operator curve area under the curve (ROC-AUC) (details below). \nBERT Classiﬁer \nCurrent state-of-the-art NLP tasks employ models built on Transformer architectures, with the \nBidirec�onal Encoder Representa�ons from Transformers (BERT) model family well suited for many \ntasks requiring seman�c understanding. We ﬁnetuned21 pre-trained BERT models on a single GPU \ninstance and used BERT for both encoding and classiﬁca�on. We evaluated the original generic \n“uncased base BERT” model, pre-trained on the BooksCorpus and English Wikipedia and a domain-\nspeciﬁc “Bio+Clinical BERT”, pre-trained on biomedical and clinical text.22,23  \nFew-Shot and ﬁnetuned LLM Classiﬁer \nCompared to BERT, the GPT family enables zero- or few-shot learning, i.e. there is poten�ally minimal \nneed for labelled data for task-speciﬁc training. We developed prompts for GPT4, comprised of \ninstruc�ons and the target categories, asking the model to complete the categories (see Appendix S3 \nfor speciﬁc prompts, and Appendix S4 for details of batch sizes).24  \nSimilarly, a GPT3.5 model was ﬁnetuned on the training dataset and instructed to classify the free-\ntext. Finetuning was achieved by presen�ng the desired output alongside the training input data. We \nused the same system prompt as for the few-shot model, while providing the training examples which \nwere fed in batches of ten. Addi�onal model hyperparameters, such as learning rate, and epochs, \nwere chosen through grid search. \nTraining, Test and External Evalua�on \nWe divided the Oxford data with a 90/10 train/test split, resul�ng in a raw training set and internal \ntest set. From the training data, we labelled and used the 4000 most frequently occurring unique \nindica�ons. To make labelling tractable we discarded the remaining unlabelled data from the training \nset. From the internal test data, we randomly selected and exhaus�vely labelled indica�ons present \nin 2000 prescrip�ons. For the external test set from Banbury, we also labelled 2000 randomly \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nselected entries. All models were trained on the training dataset with grid-search based \nhyperparameter tuning based on cross valida�on and tested on both the internal and external test \nsets. \nThe mul�-label performance of each method was evaluated using weighted F1 scores, precision-recall \n(PR-AUC) and ROC-AUC. Weighted averages take into account the varied distribu�ons of the classes, \nsuch that more common classes contribute more to the overall average, producing es�mates that \nreﬂect the original data source and represent real-world performance. \nResults \nWe obtained an�bio�c prescribing indica�on data from 826,533 prescrip�ons from 171,460 adult \ninpa�ents, ≥16 years, between 01-October-2014 and 30-June-2021 from three hospitals in Oxford, \nUK. The most commonly prescribed an�bio�cs were co-amoxiclav (n=269,945, 33%), gentamicin \n(n=70,002, 8%), and metronidazole (n=65,094, 8%) (Appendix S5), and the most common speciali�es \nwere General Surgery (n=146,719, 18%), Acute General Medicine (n=98,687, 12%), and Trauma and \nOrthopaedics (n=90,719, 11%) (Appendix S6). Pa�ents were a median 56 years old (IQR 36-73), and \n94,721 (55%) were female.  \nWe also used an independent external test dataset to assess classiﬁer performance further, from the \nHorton Hospital, Banbury (~30 miles from Oxford). This dataset comprised 111,617 prescrip�ons \nfrom 25,924 pa�ents between 01-December-2014 and 30-June-2021, with 13,650 unique free-text \nindica�ons. An�bio�cs prescribed (Appendix S5) and speciali�es (Appendix S6) were broadly similar \nto the Oxford training set. Pa�ents were a median 67 years old (IQR 47-80), and 13,853 (53%) were \nfemale. \nPrescrip�on indica�ons \nFrom the 826,533 Oxford prescrip�ons, 86,611 unique free-text indica�ons were recorded. The top \n10 accounted for 41% of all prescrip�ons; these included “Periopera�ve Prophylaxis” (20%), “UTI” \n(4%), “LRTI” (3%), “Sepsis” (3%), and “CAP” (3%). The most commonly occurring 4000 unique \nindica�ons, used for model training, accounted for 84% (692,310) of prescrip�ons (Appendix S7). \nAs expected, diﬀerent wording was used to reﬂect similar concepts, e.g. “CAP [community acquired \npneumonia]”, “LRTI [lower respiratory tract infec�on]”, “chest infec�on”, and “pneumonia”. \nAddi�onally, misspellings were common, e.g. “infc�on”, “c. diﬁfcile”. Mul�ple examples expressed \nuncertainty, or mul�ple poten�al sources of infec�on, e.g. “sepsis ?source”, “UTI/Chest”, etc. \nReﬂec�ng the complexity of prescribing, there were mul�ple poten�ally informa�ve, but rarely \noccurring indica�ons, e.g., “transplant pyelonephri�s”, “Ludwig’s angina”, and “deep neck infec�on”, \nwhich were only seen 51 (<1%), 27 (<1%), and 13 (<1%) �mes respec�vely (Appendix S8). \n‘Ground truth’ labels \nFollowing labelling by clinical experts, the 4000 most commonly occurring free-text indica�ons were \nclassiﬁed into 11 categories, with a separate variable capturing the presence of uncertainty. The most \ncommonly assigned sources were “Prophylaxis” (267,788/692,310 prescrip�ons, 39%), “Respiratory” \n(125,744, 18%) and “Abdominal” (61,670, 9%). 50% (n=344,773) prescrip�ons had “No Speciﬁc \nSource”. The most uncertainty was expressed in “Neurological” and ENT cases at 38% and 33%, \nrespec�vely (Figure 1A). Although “Respiratory” was the most common category overall a�er \n“Prophylaxis”, there were more dis�nct text strings associated with “Abdominal” infec�ons, with \n“Skin and So� Tissue” infec�on also having a dispropor�onately larger number of unique text strings \n(Figure 1B). Most ‘mul�-source’ prescrip�ons were a combina�on of “Prophylaxis” and a source \n(>90%). Excluding prophylaxis, the most common combina�ons of sources were “No Speciﬁc source” \nand “Not Informa�ve”, “Urinary” and “Respiratory”, and “Skin & So� Tissue” and EN T, in 1.6%, 0.58%, \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \n0.41% prescrip�ons, respec�vely (Figure 1C-D). The former two reﬂected diagnos�c uncertainty and \nthe later reﬂected infec�ons of the face, head and neck frequently involving skin/so� �ssue. \nClassiﬁer performance \nWe trained classiﬁers using the labelled training data from three Oxford hospitals (Figure 2). \nCompared to clinician-assigned labels, within the internal Oxford test dataset (n=2000), the weight-\naveraged F1 score across classes was highest using Bio+Clinical BERT (Average F1=0.97 [worst \nperforming class F1=0.84, best performing F1=0.98]) followed by ﬁnetuned GPT3.5 (F1=0.95 [0.77-\n0.99]), base-BERT (F1=0.93 [0.23-0.98]) and tokenisa�on+XGBoost (F1=0.86 [0.64-0.96]). Nearly all \napproaches exceeded tradi�onal regular expression-based matching (F1=0.71 [0.00-0.93]). The few-\nshot GPT4 model which did not require labelled data performed similarly to this baseline (F1= 0.71 \n[0.30-0.98]). Similar performance characteris�cs were achieved on the external valida�on dataset \nfrom Banbury (n=2000; weight-averaged F1 scores: Bio+Clinical BERT 0.98 [0.87-1.00], ﬁnetuned \nGPT3.5 0.97 [0.70-1.00],  Base BERT 0.97 [0.63-0.99], XGBoost 0.84 [0.63-1.00], Regex 0.74 [0.00-\n0.96], GTP4 0.86 [0.25-1.00]) (Table 1, also shows classiﬁca�on run �mes). \nClassiﬁer Performance by Class \nUsing the best performing classiﬁer, Bio+Clinical BERT, we assessed performance within each \ncategory. The best-performing categories within our internal test set were “Respiratory”, “No Speciﬁc \nSource” and “Prophylaxis” (F1 score=0.98), followed by “Urinary” (0.97), “Abdominal” (0.96), \n“Orthopaedic” (0.90), “Not Informa�ve” (0.89) and “Neurological” (0.88). The worst performing \ncategory was Orthopaedic (0.84), likely due to the high variety of terms used and low number of \ntraining samples (n=14, Appendix S9). Uncertainty was also well detected (0.96) (Figure 3A, Appendix \nS10). \nIn the external test data, scores varied slightly, with all source categories except for “Not Informa�ve” \nhaving F1 scores on average 0.02 higher compared to the internal test set. These small diﬀerences \nlikely arose from diﬀerent composi�ons of categories and the amount of shared vocabulary between \nthe training and test datasets. \nMisclassiﬁca�ons \nMost misclassiﬁca�ons were spread evenly across classes for single indica�ons. The two most \ncommon misclassiﬁca�ons occurred for “Orthopaedic” and “Other Speciﬁc” cases, with 12% being \nmisclassiﬁed as “Prophylaxis” and 8% as “Skin and So� Tissue”, respec�vely on the internal test set. \nOn the external test set, most misclassiﬁca�ons were predicted to be “Other Speciﬁc” or \n“Prophylaxis” (Figure 3C).  \nTraining Dataset Size \nWe examined the eﬀect of training size on model performance using randomly selected training \ndataset subsets of 250, 500, 750, 1000, 1500, 2000, 3000, and 4000 unique indica�ons, tested using \nboth internal/external test sets. There was a notable increase in performance (AUC-ROC and F1 \nscores) when the training size increased from 250 to 1000 samples, sugges�ng a minimum of 1000 \ntraining samples for adequate performance. However, we saw limited improvement as training \ndataset size rose to 4000, indica�ng there may be only marginal gains to expanding the training data \nbeyond 4000 samples (Appendix S11).  \nComparing free-text indica�ons to ICD10 codes \nWe also compared infec�on sources from manually labelled ‘ground truth’ free-text indica�ons to \nsources inferred from ICD10 diagnos�c codes. 31% of sources classiﬁed as “unspeciﬁc” using \ndiagnos�c codes could be resolved into speciﬁc sources using free-text. Rarer infec�on sources such \nas “CNS” and “ENT” (<1% and no occurrence in diagnos�c codes) were represented beter by sources \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nextracted from free-text (4% and 4% respec�vely). Overall, where deﬁned, sources listed in clinical \ncodes generally concurred with the ‘ground truth’ free-text sources (Figure 4).  \nDiscussion \nWe show that modern NLP methods can extract clinically relevant details from semi-structured free-\ntext ﬁelds. In our example applica�on, a ﬁnetuned Bio+Clincal BERT transformer model classiﬁed the \ninfec�on source being treated using clinician-writen an�bio�c indica�on text with an F1 score of \n0.97 (harmonic mean of sensi�vity and posi�ve predic�ve value). Although this required manual \nlabelling of several thousand text strings, exhaus�ve labelling of all possible prior strings was not \nrequired to achieve this performance. The accuracy of the Bio+Clinical BERT model was substan�ally \nbeter than a sophis�cated regular expression-based approach, despite the later being the solu�on \nthat many healthcare ins�tu�ons and researchers might choose at present. \nWe also explored what performance might be possible using LLMs without having to label data, e.g. \nwhere this is not possible or too resource intensive. However, few-shot learning with GPT4 only \nachieved modest performance, but it was s�ll similar to the baseline regex method and more \nconsistent across classes. Using LLMs with labelled training data, i.e. a ﬁne-tuned GPT3.5 variant \nachieved results comparable to the Bio+Clinical BERT approach when correctly speciﬁed and tuned \nbut showed more limited poten�al for deployment as responses can vary greatly in forma�ng, \nmaking it diﬃcult to parse correctly into a rigid format required for most downstream tasks or EHR \nsystems. In environments with limited compu�ng resources where the deployment of deep-learning \nmodels is not feasible, regex and XGBoost-based models provide possible alterna�ves with a reduced \nrun�me of 6.4 and 1.2 seconds/10k indica�ons vs 82.2 seconds/10k for Bio+Clinial BERT. \nCurrently, research or clinical use of free-text may be limited by concerns that personal data may be \nincluded. Here, by homogenising and categorising sensi�ve free-text data, we present a privacy-\naware solu�on that enables researchers to u�lise the depth of free-text data without direct access or \nthe possibility of iden�fying speciﬁc pa�ents.  \nOur study has several limita�ons, including that we only used a subset of the available training data, \nthrough the non-exhaus�ve labelling of a subset of an�bio�c indica�on text strings. However, \nlabelling the 4000 most common unique terms, accoun�ng for 84% of the data, achieved very high \nperformance, with sensi�vity analyses sugges�ng that labelling more examples would not have \nimproved performance substan�ally. This is likely possible because the underlying Bio+Clinical BERT \nmodel is already pre-trained on medical terms and capable of inferring similar words. Of note, many \nof the remaining 17% of text strings were diﬀerent combina�ons of already labelled words, \nsugges�ng fewer “new” or unseen keywords than might be expected. Not fully labelling the training \ndata also makes it more diﬃcult to compare class distribu�ons with the test data sets. We also only \nused a subset of the test data to evaluate performance; however the 2000 randomly selected \nsamples are likely representa�ve. Although the labelling process was somewhat subjec�ve, \nindependent labelling by two clinical researchers with a third adjudica�ng any discrepancies was \ndesigned to minimise this.  \nFuture enhancements could op�mise computa�onal eﬃciency while maintaining comparable \nperformance. While XGBoost oﬀers faster training and inference �mes, its performance was poorer \nthan Bio+Clinical BERT. Therefore, smaller, more eﬃcient NLP models might balance computa�onal \ndemands and performance. Techniques such as model pruning, quan�sa�on, and knowledge \ndis�lla�on could reduce model size and computa�onal requirements while preserving \nperformance.\n25–27 While GPT4 deployments can comply with data governance requirements, its use \npresents challenges in some se�ngs, as it is usually accessed via third-party cloud compute providers \nrather than healthcare ins�tu�ons. Where data need to remain on site, open-source, locally-deployed \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nlanguage models, such as LLAMA, ALPACA or Mistral 7B, may be alterna�ves that could be further \ninves�gated.28–30 \nOur approach has several possible applica�ons; for example, it could be used to monitor and evaluate \nprescribing prac�ce across diﬀerent condi�ons, it provides classiﬁca�on of possible infec�on sources \nfor epidemiological research,31 and is also a mechanism for extrac�ng standardised features from \nmedical records for use in predic�ve algorithms being developed to improve pa�ent care. Although \nwe demonstrate excellent performance for an�bio�c indica�ons, it could also be applied to other \nshort strings of free-text, for example descrip�ons of surgical procedures, pa�ent func�onal states, or \npresen�ng complaints in emergency department and hospital admission data. Across all these \ndomains, reﬁned pa�ent stra�ﬁca�on could improve both research and care delivery. \nIn summary, we show that state-of-the-art NLP can be used to eﬃciently and accurately categorise \nsemi-structured free-text in medical records. This has the poten�al to be applied widely to analyse \nmedical records more accurately. \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAcknowledgements and funding \nThis work was supported by the Na�onal Ins�tute for Health Research Health Protec�on Research \nUnit (NIHR HPRU) in Healthcare Associated Infec�ons and An�microbial Resistance at Oxford \nUniversity in partnership with the UK Health Security Agency (NIHR200915), and the NIHR Biomedical \nResearch Centre, Oxford. DWE is a Big Data Ins�tute Robertson Fellow. ASW is an NIHR Senior \nInves�gator. The views expressed are those of the authors and not necessarily those of the NHS, the \nNIHR, the Department of Health or the UK Health Security Agency. KY is supported by the EPSRC \nCentre for Doctoral Training in Health Data Science (EP/S02428X/1). The funders had no role in study \ndesign, data collec�on and analysis, decision to publish, or prepara�on of the manuscript. \nDeclara�on of interests \nNo other author has a conﬂict of interest to declare. \nData sharing \nThe data analysed are available from the Infec�ons in Oxfordshire Research Database \n(htps://oxfordbrc.nihr.ac.uk/research-themes/modernising-medical-microbiology-and-big-infec�on-\ndiagnos�cs/infec�ons-in-oxfordshire-research-database-iord/), subject to an applica�on and research \nproposal mee�ng on the ethical and governance requirements of the Database. Labelled training and \ntest datasets and the pre-trained BERT model are also available via an applica�on to the Database. \nCode availability \nAll tools developed for this study (Regex builder, Bio+Clinical BERT pipeline, GPT3.5 ﬁnetuning and \nGPT4 few-shot request tools), pre-trained models, as well as the evalua�on framework, are available \non GitHub: \nhtps://github.com/kevihiiin/EHR-Indica�on-Processing/  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nReferences (max reference: 50) \n1.  Kirby JC, Speltz P , Rasmussen LV, et al. PheKB: a catalog and workﬂow for crea�ng electronic \nphenotype algorithms for transportability. J Am Med Inform Assoc 2016;23(6):1046–52.  \n2.  Jamian L, Wheless L, Croﬀord LJ, Barnado A. Rule-based and machine learning algorithms \niden�fy pa�ents with systemic sclerosis accurately in the electronic health record. Arthri�s Res \nTher 2019;21(1):305.  \n3.  Maarseveen TD, Maurits MP , Niemantsverdriet E, van der Helm-van Mil AHM, Huizinga TWJ, \nKnevel R. Handwork vs machine: a comparison of rheumatoid arthri�s pa�ent popula�ons as \niden�ﬁed from EHR free-text by diagnosis extrac�on through machine-learning or tradi�onal \ncriteria-based chart review. Arthri�s Res Ther 2021;23(1):174.  \n4.  Singhal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge. Nature \n2023;620(7972):172–80.  \n5.  Jiang LY , Liu XC, Neja�an NP , et al. Health system-scale language models are all-purpose \npredic�on engines. Nature 2023;619(7969):357–62.  \n6.  McCoy D, Seb� R, Kuyumjian AG. An evalua�on of selected indica�ons and appropriateness of \nAmpicillin/Sulbactam, an unrestricted an�microbial, at a single center. Pharm Ther \n2017;42(3):189.  \n7.  Timmons V, Townsend J, McKenzie R, Burdalski C, Adams-Sommer V. An evalua�on of provider-\nchosen an�bio�c indica�ons as a targeted an�microbial stewardship interven�on. Am J Infect \nControl 2018;46(10):1174–9.  \n8.  Javaid W, Cavanaugh S, Lupone C, Stewart T, Fazili T, White B. Fixed vs. Free-text Documenta�on \nof Indica�on for An�bio�c Orders. Open Forum Infect Dis 2017;4(Suppl 1):S325–6.  \n9.  Kadri SS, Lai YL, Warner S, et al. Inappropriate empirical an�bio�c therapy for bloodstream \ninfec�ons based on discordant in-vitro suscep�bili�es: a retrospec�ve cohort analysis of \nprevalence, predictors, and mortality risk in US hospitals. Lancet Infect Dis 2021;21(2):241–51.  \n10.  Yoon CH, Bartlet S, Stoesser N, et al. Mortality risks associated with empirical an�bio�c ac�vity \nin Escherichia coli bacteraemia: an analysis of electronic health records. J An�microb \nChemother 2022;77(9):2536–45.  \n11.  Gaieski DF, Edwards JM, Kallan MJ, Carr BG. Benchmarking the Incidence and Mortality of \nSevere Sepsis in the United States*. Crit Care Med [Internet] 2013;41(5). Available from: \nhtps://journals.lww.com/ccmjournal/fulltext/2013/05000/benchmarking_the_incidence_and_\nmortality_of_severe.2.aspx \n12.  Rhee C, Dantes R, Epstein L, et al. Incidence and Trends of Sepsis in US Hospitals Using Clinical \nvs Claims Data, 2009-2014. JAMA 2017;318(13):1241–9.  \n13.  Wiese AD, Griﬃn MR, Stein CM, et al. Valida�on of discharge diagnosis codes to iden�fy serious \ninfec�ons among middle age and older adults. BMJ Open 2018;8(6):e020857.  \n14.  Jolley RJ, Sawka KJ, Yergens DW, Quan H, Jeté N, Doig CJ. Validity of administra�ve data in \nrecording sepsis: a systema�c review. Crit Care 2015;19(1):139.  \n15.  Saini S, Leung V, Si E, et al. Documen�ng the indica�on for an�microbial prescribing: a scoping \nreview. BMJ Qual Saf 2022;31(11):787–99.  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \n16.  Devlin J, Chang M-W, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirec�onal Transformers \nfor Language Understanding [Internet]. 2019 [cited 2024 Mar 14];Available from: \nhtp://arxiv.org/abs/1810.04805 \n17.  Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are unsupervised \nmul�task learners. OpenAI Blog 2019;1(8):9.  \n18.  Clinical Classiﬁca�ons So�ware Reﬁned (CCSR) [Internet]. [cited 2022 May 12];Available from: \nhtps://hcup-us.ahrq.gov/toolsso�ware/ccsr/ccs_reﬁned.jsp \n19.  Pedregosa F, Varoquaux G, Gramfort A, et al. Scikit-learn: Machine learning in Python. J Mach \nLearn Res 2011;12:2825–30.  \n20.  Chen T, Guestrin C. XGBoost: A Scalable Tree Boos�ng System [Internet]. In: Proceedings of the \n22nd ACM SIGKDD Interna�onal Conference on Knowledge Discovery and Data Mining. New \nYork, NY , USA: Associa�on for Compu�ng Machinery; 2016 [cited 2024 Mar 14]. p. 785–\n94.Available from: htps://dl.acm.org/doi/10.1145/2939672.2939785 \n21.  Wolf T, Debut L, Sanh V, et al. HuggingFace’s Transformers: State-of-the-art Natural Language \nProcessing [Internet]. 2020 [cited 2024 Mar 14];Available from: htp://arxiv.org/abs/1910.03771 \n22.  Alsentzer E, Murphy J, Boag W, et al. Publicly Available Clinical BERT Embeddings [Internet]. In: \nProceedings of the 2nd Clinical Natural Language Processing Workshop. Minneapolis, \nMinnesota, USA: Associa�on for Computa�onal Linguis�cs; 2019. p. 72–8.Available from: \nhtps://www.aclweb.org/anthology/W19-1909 \n23.  Johnson AEW, Pollard TJ, Shen L, et al. MIMIC-III, a freely accessible cri�cal care database. Sci \nData 2016;3(1):160035.  \n24.  OpenAI (2023). Gpt-4 technical report. ArXiv Prepr ArXiv230308774 2023; \n25.  Michel P , Levy O, Neubig G. Are Sixteen Heads Really Beter than One? [Internet]. In: Advances \nin Neural Informa�on Processing Systems. Curran Associates, Inc.; 2019 [cited 2024 Mar 18]. \nAvailable from: \nhtps://proceedings.neurips.cc/paper_ﬁles/paper/2019/hash/2c601ad9d2ﬀ9bc8b282670cdd54\nf69f-Abstract.html \n26.  Zafrir O, Boudoukh G, Izsak P , Wasserblat M. Q8BERT: Quan�zed 8Bit BERT [Internet]. In: 2019 \nFi�h Workshop on Energy Eﬃcient Machine Learning and Cogni�ve Compu�ng - NeurIPS \nEdi�on (EMC2-NIPS). 2019 [cited 2024 Mar 18]. p. 36–9.Available from: \nhtps://ieeexplore.ieee.org/abstract/document/9463531 \n27.  Sanh V, Debut L, Chaumond J, Wolf T. Dis�lBERT, a dis�lled version of BERT: smaller, faster, \ncheaper and lighter. ArXiv Prepr ArXiv191001108 2019; \n28.  Touvron H, Lavril T, Izacard G, et al. LLaMA: Open and Eﬃcient Founda�on Language Models \n[Internet]. 2023 [cited 2024 Mar 14];Available from: htp://arxiv.org/abs/2302.13971 \n29.  Taori R, Gulrajani I, Zhang T, et al. Stanford Alpaca: An Instruc�on-following LLaMA model \n[Internet]. GitHub Repos. 2023;Available from: htps://github.com/tatsu-lab/stanford_alpaca \n30.  Jiang AQ, Sablayrolles A, Mensch A, et al. Mistral 7B [Internet]. 2023 [cited 2024 Mar \n14];Available from: htp://arxiv.org/abs/2310.06825 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \n31.  Gu Q, Wei J, Yoon CH, et al. Dis�nct paterns of vital sign and inﬂammatory marker responses in \nadults with suspected bloodstream infec�on. J Infect 2024;106156.  \n32.  OpenAI Chat API Reference [Internet]. [cited 2024 Jan 23];Available from: \nhtps://pla�orm.openai.com/docs/api-reference/chat/create \n \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nFigures and Tables \n \n \nFigure 1: Infection source distributions within labelled training data from three Oxford hospitals. Bar charts A) and B) \nshow the distribution of the sources and the uncertainty relative to the infection source. The up-set plots C) and D) show the \noccurrence of multiple sources within the same prescription. Panels A) & C) show distributions across the entire labelled \nindications training set, panels B) & D) across a distinct set of 4000 most common indications. *Indications falling into ENT \nsuch as “neck abscess” were often also labelled with Skin & Soft Tissue. \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \n \nFigure 2: Data processing ﬂow chart for training and internal and external test data sets. Prescribing data was fetched \nfrom EHR databases and ﬁltered for complete data. The 4000 most frequent indications within the training split were \nlabelled, all remaining training data was discarded. 2000 entries were randomly sampled from both the internal and \nexternal test datasets and exhaustively labelled, resulting in a total of three datasets (training set, internal test set, external \ntest set).  \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \n \nFigure 3: Performance metrics for Bio+Clinical BERT on both internal and external test sets. Bar charts A) and B) show the \nper-class prediction performance. Confusion matrices C) and D) are single indication test cases and show model prediction \nerrors across the sources for given ground truths (clinician assigned sources). Panels A) and C) show evaluations performed \non the internal test set from three Oxford hospitals, B) and D) on the external test set from the Banbury hospital. \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \n \nFigure 4: Comparing infection sources between clinician assigned free-text indications (left) and diagnostic codes (right) in \nthe training and internal and external test sets. Clinician assigned categories were extracted from prescribing data and \nmanually labelled, diagnostic codes sources calculated from procedure and discharge codes.  \nOther\nCNS\nENT\nOrthopedic\nSkin, soft tissue\nMultiple sources\nAbdominal\nUrinary\nRespiratory\nUnspecific\nOther\nCNS\nSkin, soft tissue, ort hopedic\nMultiple sources\nAbdominal\nUrinary\nRespiratory\nUnspecific\nSources of Infection\nGround-truth labels Diagnostic Codes\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nTable 1: Model performance metrics for the internal (Oxford) and external (Banbury) test sets. Each score is listed with the weighted average across the classes (sources), with the lowest and \nhighest performing class. Overall Accuracy refers to the score calculated for a sample treated as a whole. The ﬁnetuned Bio+Clinical BERT outperforms all other methods on both internal and \nexternal test sets. \nInternal Oxford test set \nModel F1 Score ROC AUC  PR AUC  Per Class Accuracy Accuracy Training \nRuntime \nTest \nRuntime \nAggregation Average Lowest Highest Average Lowest Highest Average Lowest Highest Average Lowest Highest Overall Per 4k Per 10k  \nRegex 0.71 0.00 0.93 - - - - - - 0.82 0.32 0.99 0.14 - 6.4s \nXGBoost 0.86 0.64 0.96 0.96 0.87 0.99 0.9 0.62 0.99 0.95 0.92 1.00 0.72 6s 1.2s \nBase BERT 0.93 0.23 0.98 0.99 0.91 1.00 0.97 0.69 0.99 0.98 0.97 0.99 0.88 282s0F\n1 82.2s \nBio+Clinical BERT 0.97 0.84 0.98 0.99 0.96 1.00 0.98 0.88 1.00 0.99 0.98 1.00 0.94 \n279sError! \nBookmark not \ndefined. \n83.1s \nFine-Tuned OpenAI GPT3.5 0.95 0.77 0.99 - - - - - - 0.98 0.97 1.00 0.91 ~3500s2 ~3000s1F\n2 \nFew-Shot OpenAI GPT4 0.71 0.30 0.98 - - - - - - 0.87 0.64 1.00 0.50 - ~3000s2 \n \nExternal Banbury test set \nModel F1 Score ROC AUC  PR AUC  Per Class Accuracy Accuracy \nAggregation Average Lowest Highest Average Lowest Highest Average Lowest Highest Average Lowest Highest Overall \nRegex 0.74 0.00 0.96 - - - - - - 0.82 0.41 0.99 0.24 \nXGBoost 0.84 0.63 1.00 0.94 0.86 1.00 0.87 0.57 1.00 0.94 0.88 1.00 0.68 \nBase BERT 0.97 0.63 0.99 0.99 0.95 1.00 0.98 0.75 1.00 0.99 0.99 1.00 0.95 \nBio+Clinical BERT 0.98 0.87 1.00 0.99 0.97 1.00 0.98 0.87 1.00 0.99 0.99 1.00 0.97 \nFine-Tuned OpenAI GPT3.5 0.97 0.70 1.00 - - - - - - 0.99 0.98 1.00 0.95 \nFew-Shot OpenAI GPT4 0.86 0.25 1.00 - - - - - - 0.95 0.81 1.00 0.73 \n \n \n \n1 Using one Nvidia V100 GPU \n2 OpenAI’s cloud service \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nSupplementary appendix \n \nSupplementary appendix ...................................................................................................................... 18 \nSupplementary methods ................................................................................................................... 19 \nComparator Classiﬁca�on by ICD10 Codes ................................................................................... 19 \nTradi�onal Classiﬁca�on Methods ................................................................................................ 19 \nBERT Classiﬁer ............................................................................................................................... 19 \nFew-Shot and ﬁnetuned LLM Classiﬁer ......................................................................................... 19 \nAppendix S1 Code and LUTs Repository ............................................................................................ 20 \nAppendix S2 Regex Rule Builder ........................................................................................................ 21 \nAppendix S3 GPT4 Predic�on Prompt (API) ...................................................................................... 22 \nAppendix S4 GPT4 Token Limits ......................................................................................................... 23 \nAppendix S5 Prescribed Drugs ........................................................................................................... 24 \nAppendix S6 Prescribing Speciali�es ................................................................................................. 25 \nAppendix S7 Labelling Coverage ........................................................................................................ 26 \nAppendix S8 Uncommon Indica�ons ................................................................................................ 27 \nAppendix S9 Class Distribu�ons ........................................................................................................ 28 \nAppendix S 10 Per Class Predic�on Scores ........................................................................................ 29 \nAppendix S11 Training Set Size Eﬀects .............................................................................................. 31 \n \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nSupplementary methods \nComparator Classiﬁca�on by ICD10 Codes \nWe ﬁrst summarised all ICD10 codes into broader concepts using the CCSR classiﬁca�on tool, \ngrouping >70,000 ICD10 codes into 544 categories. Two independent clinical researchers then \ndeveloped a custom lookup table mapping CCSR categories to our 11 infec�on source groups \n(Appendix S1), with a third clinician resolving discrepancies. \nTradi�onal Classiﬁca�on Methods \nRegex Rules \nThe regex paterns for each category were built using the 50 most common indica�ons for that \ninfec�on source in the training data, with individually assigned error-rate thresholds for inexact \nmatching and speciﬁed word boundaries for each string. This allowed for strict exact matching on \nabbrevia�ons while permi�ng spelling mistakes for longer words. Word boundaries ensured that \nabbrevia�ons were not matched when part of a longer word (e. g. avoiding ﬁnding UTI in roUTIne \npost-op, cUTIbacterium). Our tool, designed to automate the crea�on of complex regex queries based \non a given reference set and user speciﬁca�ons, can be found in Appendix S2. The assisted regex \nbuilder simpliﬁed crea�ng complex matching rules for infec�on sources, extrac�ng the most common \nindica�ons for each category and expor�ng them into a pre-populated table with parsing op�ons for \naddi�onal user input. Users can then modify these rules: adding word boundaries for precise \nmatches, se�ng error rates (e.g., zero for abbrevia�ons), and excluding redundant words. The edited \ntable is then read back and converted into complex regex-matching strings for each category. This \nallows for medical experts to build and modify complex matching rules without needing to \nunderstand and debug error-prone regular expressions. \nBERT Classiﬁer \nWe evaluated the performance of the original generic “uncased base BERT” model, pre-trained on \nthe BooksCorpus and English Wikipedia and a domain-speciﬁc “Bio+Clinical BERT”, pre-trained on \nbiomedical and clinical text sourced from PubMed, PubMed Central and MIMIC-III v1.4 notes.22,23 \nBoth pre-trained models were fetched from the HuggingFace model hub (uploaded on the 18-June-\n2019 and 28-February-2020), and ﬁnetuned using the HuggingFace “transformers” library with \nindica�ons as input and the source categories as output.21 \nFew-Shot and ﬁnetuned LLM Classiﬁer \nWe developed prompts for GPT4, comprised of instruc�ons and the target categories, asking the \nmodel to complete the categories (speciﬁc prompt in Appendix S3).24 We made several itera�ons to \nthe prompt on a subset of the training data, aiming to increase the model’s understanding of the task. \nGiven the model’s genera�ve nature, we accessed GPT4 through the API and supplied inputs to \ncreate more structured, determinis�c and less crea�ve answers. Specifying a rigid output format is \ncrucial for a mul�-label task. We therefore instructed the model to present its predic�on output in \nJSON format, using the original indica�on as the key and the categories as a list of values. To prevent \nthe model from crea�ng new categories, we penalised it for returning new tokens not seen in the text \n(i.e. the prompt) by se�ng a higher `presence penalty’. A ﬁxed `seed’ and lower `temperature’ were \nchosen to coerce the model into returning more determinis�c and reproducible answers.\n32 The same \nhyperparameters were used for predic�on with the ﬁne-tuned GPT3.5 model, aiming to increase \ndeterminism and reproducibility. \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \n \nAppendix S1 Code and LUTs Repository \nICD10 to Infec�on Source Mapping Table \nhttps://github.com/kevihiiin/EHR-Indication-\nProcessing/blob/main/00_Data/LUTs/icd10_ccsr_mapping.csv  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAppendix S2 Regex Rule Builder \n \nFigure S2: Regex Rule Builder user speciﬁcations. Specify a non-default error rate for abbreviations, set left and right word \nboundaries and exclude duplicated words. One sheet per category. \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAppendix S3 GPT4 Predic�on Prompt (API) \nSystem prompt: \nYou are a helpful and precise UK medical expert; you have been given a list of indications describing why antibiotics were \nprescribed to patients in a hospital. You have been asked to **label** these indications into categories. \nYou can only choose from these categories which are: Urinary, Respiratory, Abdominal, Neurological, Skin Soft Tissue, Ent, \nOrthopaedic, Other Speciﬁc, No Speciﬁc Source, Prophylaxis, Uncertainty, Not Informative \nMultiple categories are allowed. \nWhen returning your answer, please return a json \nUser prompt: \nThis is the list of indications, return a json with the categories (multiple allowed) for each indication. \n“abdo pathology”, \n“sepsis ?hap”, \n“artholin abscess”, \n[…] \nThe results are then parsed from the JSON-formated response. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAppendix S4 GPT4 Token Limits \nThe number of supplied indica�ons per request is limited by the number of output tokens. We \ndetermined that 300 input indica�ons result in ~3800 return tokens, leaving a 10% safety margin for \nthe 4k return limit. \n \nFigure S4: Token consumption (prompt and model output) depending on input size (number of indications.) Providing a \nhigher number of supplied indications (input to classify) per round is more cost-eﬀective, as the system and user instruction \nprompts are only submitted once. However, the number of output tokens is limited to 4k or 16k (depending on the model).  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAppendix S5 Prescribed Drugs \nTable S5: Ten most commonly prescribed drugs.  \n \n \n  \nDrug [Oxford] Number Percentage \nCo-amoxiclav 269945 33% \nGentamicin 70002 8% \nMetronidazole 65094 8% \nCe�riaxone 58763 7% \nFlucloxacillin 47537 6% \nAmoxicillin 32151 4% \nCiproﬂoxacin 25145 3% \nVancomycin 23250 3% \nPiperacillin + Tazobactam (Tazocin equivalent) 21666 3% \nClarithromycin 18607 2% \nDrug [Banbury] Number Percentage \nCo-amoxiclav 39114 35% \nCe�riaxone 11369 10% \nGentamicin 10251 9% \nAmoxicillin 7297 7% \nClarithromycin 6525 6% \nFlucloxacillin 5630 5% \nMetronidazole 4851 4% \nDoxycycline 4416 4% \nNitrofurantoin 3204 3% \nPiperacillin + Tazobactam (Tazocin equivalent) 2862 3% \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAppendix S6 Prescribing Speciali�es \nTable S6: Specialties of the prescribing clinicians, ordered to show the ten most active specialties.  \n \n \n  \nClinician Main Specialty [Oxford] Number Percentage \nGeneral Surgery 146719 18% \nAcute General Medicine 98687 12% \nTrauma and Orthopaedics 90719 11% \nAcute Geratology 72371 9% \nClinical Haematology 46928 6% \nObstetrics 36431 4% \nNeurosurgery 34666 4% \nInfec�ous Diseases 30584 4% \nPlas�c Surgery 29334 4% \nUrology 29258 4% \nClinician Main Specialty [Banbury] Number Percentage \nAcute General Medicine 41312 37% \nAcute Geratology 14912 13% \nGastroenterology 9128 8% \nCardiology 8682 8% \nInfec�ous Diseases 7870 7% \nTrauma and Orthopaedics 7642 7% \nGeneral Surgery 6803 6% \nEmergency Medicine 4924 4% \nUrology 2885 3% \nGynaecology 2287 2% \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAppendix S7 Labelling Coverage \n \nFigure S7: Total coverage of data given a set of labelled data. Labelling the most common 4000 samples covers 84% of the \nentire data set. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAppendix S8 Uncommon Indica�ons \nTable S8: Example list of uncommon indications. Randomly sampled from all indications occurring more than 5 and less \nthan 100 times. Occurrences less than 10 are truncated to <10 for statistical disclosure control. \nUncommon Indication Occurrence Percentage \nsep�c le� knee <10 <1% \nabdo contamina�on <10 <1% \ninfected femur 10 <1% \nﬂu prophylaxis 63 <1% \nforearm celluli�s <10 <1% \nmesenteric panniculi�s <10 <1% \nintra-abdo inf 14 <1% \n?dental infec�on 14 <1% \ncap curb1 41 <1% \nsep�c unknown source <10 <1% \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAppendix S9 Class Distribu�ons \nTable S9: Distributions of the classes within the diﬀerent data sets. \n Urinary Respiratory Abdominal Neurological Skin & Soft \nTissue ENT Orthopaedic Other \nSpecific \nNo Specific \nSource Prophylaxis Not \nInformative \nTraining 6.26% \n(n=54882) \n12.91% \n(n=113211) \n6.33% \n(n=55486) \n0.76% \n(n=6641) \n4.43% \n(n=38843) \n1.47% \n(n=12902) \n1.66% \n(n=14522) \n2.30% \n(n=20177) \n35.40% \n(n=310479) \n27.50% \n(n=241201) \n0.98% \n(n=8598) \nOxford \nTest \n5.79% \n(n=160) \n13.53% \n(n=374) \n8.64% \n(n=239) 0.83% (n=23) 4.74% \n(n=131) 1.84% (n=51) 1.99% (n=55) 3.18% (n=88) 32.77% \n(n=906) \n25.46% \n(n=704) 1.23% (n=34) \nBanbury \nTest \n12.51% \n(n=305) \n28.95% \n(n=706) \n4.84% \n(n=118) 0.74% (n=18) 7.59% \n(n=185) 0.57% (n=14) 0.98% (n=24) 1.03% (n=25) 26.65% \n(n=650) \n14.72% \n(n=359) 1.44% (n=35) \n \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAppendix S 4 Per Class Predic�on Scores \nTable S10.1: Per class prediction scores on the internal Oxford test set. Values are reported in F1 score, Precision Recall AUC and ROC AUC where applicable.  \nModel Metric Urinary Respiratory Abdominal Neurological \nSkin & Soft \nTissue ENT Orthopaedic \nOther \nSpecific \nNo Specific \nSource Prophylaxis Uncertainty \nNot \nInformative \nRegex F1 0.87 0.63 0.63 0.68 0.76 0.54 0.3 0.11 0.78 0.93 0.20 0.00 \nXGBoost F1 0.64 0.81 0.9 0.76 0.83 0.83 0.7 0.71 0.91 0.96 0.66 0.9 \nBase_BERT F1 0.97 0.98 0.94 0.23 0.94 0.27 0.59 0.63 0.98 0.98 0.95 0.67 \nBio_ClinicalBERT F1 0.98 0.98 0.96 0.88 0.93 0.91 0.93 0.84 0.98 0.98 0.96 0.85 \nFine-Tuned GPT3.5 F1 0.98 0.97 0.95 0.93 0.83 0.8 0.95 0.77 0.97 0.97 0.92 0.99 \nFew-Shot GPT4 F1 0.98 0.96 0.83 0.88 0.87 0.79 0.87 0.30 0.34 0.94 0.78 0.30 \nRegex PR  - - - - - - - - - - - - \nXGBoost PR  0.62 0.87 0.91 0.68 0.89 0.82 0.65 0.72 0.97 0.99 0.69 0.83 \nBase_BERT PR  0.98 0.99 0.97 0.69 0.95 0.85 0.94 0.77 0.99 0.99 0.97 0.92 \nBio_ClinicalBERT PR  0.99 1.00 0.97 0.91 0.98 0.93 0.96 0.88 0.99 1.00 0.99 0.94 \nFine-Tuned GPT3.5 PR  - - - - - - - - - - - - \nFew-Shot GPT4 PR  - - - - - - - - - - - - \nRegex ROC  - - - - - - - - - - - - \nXGBoost ROC 0.9 0.96 0.97 0.92 0.98 0.96 0.91 0.92 0.97 0.99 0.87 0.94 \nBase_BERT ROC 0.98 1.00 0.98 0.97 0.99 0.96 0.99 0.91 0.99 0.99 1.00 0.98 \nBio_ClinicalBERT ROC 0.99 1.00 0.98 1.00 0.99 0.98 0.99 0.96 0.99 1.00 1.00 0.99 \nFine-Tuned GPT3.5 ROC - - - - - - - - - - - - \nFew-Shot GPT4 ROC - - - - - - - - - - - - \nRegex Accuracy 0.98 0.79 0.89 0.99 0.97 0.96 0.89 0.36 0.76 0.96 0.32 0.97 \nXGBoost Accuracy 0.96 0.94 0.98 1.00 0.98 0.99 0.99 0.98 0.92 0.97 0.95 1.00 \nBase_BERT Accuracy 0.99 0.99 0.99 0.99 0.99 0.98 0.98 0.97 0.98 0.98 0.99 0.99 \nBio_ClinicalBERT Accuracy 1.00 0.99 0.99 1.00 0.99 1.00 1.00 0.99 0.98 0.98 0.99 1.00 \nFine-Tuned GPT3.5 Accuracy 1.00 0.99 0.99 1.00 0.97 0.99 1.00 0.98 0.97 0.98 0.99 1.00 \nFew-Shot GPT4 Accuracy 1.00 0.98 0.96 1.00 0.98 0.99 0.99 0.94 0.64 0.96 0.97 0.96 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nTable S10.2: Per class prediction scores on the external Banbury test set. Values are reported in F1 score, Precision Recall AUC and ROC AUC where applicable.  \nModel Metric Urinary Respiratory Abdominal Neurological \nSkin & Soft \nTissue ENT Orthopaedic \nOther \nSpecific \nNo Specific \nSource Prophylaxis Uncertainty \nNot \nInformative \nRegex F1 0.96 0.73 0.35 0.78 0.90 0.30 0.25 0.03 0.81 0.95 0.30 0.00 \nXGBoost F1 0.67 0.8 0.86 1.00 0.91 0.87 0.76 0.63 0.92 0.98 0.72 0.87 \nBase_BERT F1 0.99 0.99 0.92 0.76 0.96 0.73 0.63 0.65 0.98 0.98 0.99 0.79 \nBio_ClinicalBERT F1 0.99 0.99 0.95 1.00 0.96 0.92 0.91 0.92 0.99 0.97 0.99 0.87 \nFine-Tuned GPT3.5 F1 0.98 0.98 0.95 1.00 0.91 0.81 0.90 0.70 0.98 0.96 0.95 1.00 \nFew-Shot GPT4 F1 0.99 1.00 0.85 1.00 0.96 0.88 0.91 0.25 0.6 0.96 0.88 0.59 \nRegex PR  - - - - - - - - - - - - \nXGBoost PR  0.67 0.87 0.83 1.00 0.91 0.91 0.77 0.57 0.95 0.98 0.73 0.83 \nBase_BERT PR  1.00 1.00 0.96 1.00 0.98 0.9 0.88 0.75 0.98 0.99 0.99 0.92 \nBio_ClinicalBERT PR  1.00 1.00 0.95 1.00 0.98 1.00 0.96 0.87 0.98 0.98 0.99 0.94 \nFine-Tuned GPT3.5 PR  - - - - - - - - - - - - \nFew-Shot GPT4 PR  - - - - - - - - - - - - \nRegex ROC  - - - - - - - - - - - - \nXGBoost ROC 0.86 0.93 0.94 1.00 0.96 0.96 0.90 0.93 0.97 0.99 0.87 0.94 \nBase_BERT ROC 1.00 1.00 0.98 1.00 0.99 0.99 1.00 0.95 1.00 0.99 1.00 0.98 \nBio_ClinicalBERT ROC 1.00 1.00 0.98 1.00 0.99 1.00 0.98 0.97 0.99 0.99 1.00 0.99 \nFine-Tuned GPT3.5 ROC - - - - - - - - - - - - \nFew-Shot GPT4 ROC - - - - - - - - - - - - \nRegex Accuracy 0.99 0.75 0.84 0.99 0.98 0.97 0.95 0.41 0.85 0.98 0.41 0.97 \nXGBoost Accuracy 0.92 0.88 0.98 1.00 0.98 1.00 0.99 0.99 0.95 0.99 0.94 1.00 \nBase_BERT Accuracy 1.00 0.99 0.99 1.00 0.99 1.00 0.99 0.99 0.99 0.99 1.00 0.99 \nBio_ClinicalBERT Accuracy 1.00 0.99 0.99 1.00 0.99 1.00 1.00 1.00 0.99 0.99 1.00 1.00 \nFine-Tuned GPT3.5 Accuracy 0.99 0.99 0.99 1.00 0.98 1.00 1.00 0.99 0.99 0.99 0.99 1.00 \nFew-Shot GPT4 Accuracy 1.00 0.98 0.96 1.00 0.98 0.99 0.99 0.94 0.64 0.96 0.97 0.96 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint \nAppendix S11 Training Set Size Eﬀects \n \nFigure S11: Training set size eﬀect on the performance of Bio+Clinical BERT, evaluated on both F1-Score and ROC AUC. The \ntraining was run on randomly sampled subsets of the training dataset of size [500, 1000, 1500, 2000, 3000, 4000] and \nevaluated on the same internal and external test sets (2000 samples each). \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 19, 2024. ; https://doi.org/10.1101/2024.04.17.24305966doi: medRxiv preprint ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6031579971313477
    },
    {
      "name": "Artificial intelligence",
      "score": 0.576457679271698
    },
    {
      "name": "Natural language processing",
      "score": 0.5610429048538208
    },
    {
      "name": "Transformer",
      "score": 0.5502576231956482
    },
    {
      "name": "F1 score",
      "score": 0.5379872918128967
    },
    {
      "name": "Health records",
      "score": 0.5341448187828064
    },
    {
      "name": "Medical prescription",
      "score": 0.5064936876296997
    },
    {
      "name": "Test set",
      "score": 0.45508673787117004
    },
    {
      "name": "Language model",
      "score": 0.44405075907707214
    },
    {
      "name": "Machine learning",
      "score": 0.4332556426525116
    },
    {
      "name": "Electronic health record",
      "score": 0.42290329933166504
    },
    {
      "name": "Medicine",
      "score": 0.20855757594108582
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Pharmacology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Health care",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ]
}