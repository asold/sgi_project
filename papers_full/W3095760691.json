{
  "title": "A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models",
  "url": "https://openalex.org/W3095760691",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5077006200",
      "name": "Usman Naseem",
      "affiliations": [
        "The University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A5033585021",
      "name": "Imran Razzak",
      "affiliations": [
        "Deakin University"
      ]
    },
    {
      "id": "https://openalex.org/A5091525827",
      "name": "Shah Khalid Khan",
      "affiliations": [
        "RMIT University"
      ]
    },
    {
      "id": "https://openalex.org/A5006355592",
      "name": "Mukesh Prasad",
      "affiliations": [
        "Association for Computing Machinery"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2220660951",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2043870592",
    "https://openalex.org/W2517456239",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2997617958",
    "https://openalex.org/W2801227764",
    "https://openalex.org/W3001645704",
    "https://openalex.org/W2593134789",
    "https://openalex.org/W2166183437",
    "https://openalex.org/W2604581755",
    "https://openalex.org/W2774853190",
    "https://openalex.org/W2061941130",
    "https://openalex.org/W2156741031",
    "https://openalex.org/W2137763598",
    "https://openalex.org/W2166053118",
    "https://openalex.org/W2113242816",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W40563694",
    "https://openalex.org/W2250418535",
    "https://openalex.org/W1922126009",
    "https://openalex.org/W2387314750",
    "https://openalex.org/W2590061102",
    "https://openalex.org/W2781487490",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2057308560",
    "https://openalex.org/W2749353156",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2776249353",
    "https://openalex.org/W2108420397",
    "https://openalex.org/W2091190355",
    "https://openalex.org/W2767608965",
    "https://openalex.org/W2153439141",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2507974895",
    "https://openalex.org/W1998257453",
    "https://openalex.org/W1990351156",
    "https://openalex.org/W2751418808",
    "https://openalex.org/W2106095291",
    "https://openalex.org/W3029929833",
    "https://openalex.org/W3095278059",
    "https://openalex.org/W3003958081",
    "https://openalex.org/W3127056209",
    "https://openalex.org/W3091311542",
    "https://openalex.org/W3090245549",
    "https://openalex.org/W3039554467",
    "https://openalex.org/W2166706824",
    "https://openalex.org/W3138979178",
    "https://openalex.org/W2299421909",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2234895705",
    "https://openalex.org/W2807432889",
    "https://openalex.org/W2128420091",
    "https://openalex.org/W2149706766",
    "https://openalex.org/W3021742956",
    "https://openalex.org/W4250071748",
    "https://openalex.org/W3132725388",
    "https://openalex.org/W1887786044",
    "https://openalex.org/W2049434052",
    "https://openalex.org/W2512317583",
    "https://openalex.org/W2807707859",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W17944974",
    "https://openalex.org/W2808079449",
    "https://openalex.org/W1990482343",
    "https://openalex.org/W2242874043",
    "https://openalex.org/W2250879510",
    "https://openalex.org/W2014545475",
    "https://openalex.org/W1971222444",
    "https://openalex.org/W3122364763",
    "https://openalex.org/W2250999864",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2788967885",
    "https://openalex.org/W2953246132",
    "https://openalex.org/W2990453928",
    "https://openalex.org/W144217556",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2199612470",
    "https://openalex.org/W80056832",
    "https://openalex.org/W2157765050",
    "https://openalex.org/W3022029496",
    "https://openalex.org/W40549020",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3099880460",
    "https://openalex.org/W2964101952",
    "https://openalex.org/W2473593971",
    "https://openalex.org/W2755957574",
    "https://openalex.org/W1994129711",
    "https://openalex.org/W2751607718",
    "https://openalex.org/W1589554437",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2737092125",
    "https://openalex.org/W1802825888",
    "https://openalex.org/W2738321088",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2262907013",
    "https://openalex.org/W2267835966",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2620504166",
    "https://openalex.org/W1841724727",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W3102624995",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2951737564",
    "https://openalex.org/W2468328197",
    "https://openalex.org/W1531910981",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2605058246",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W148415288",
    "https://openalex.org/W3087623576",
    "https://openalex.org/W2405154930",
    "https://openalex.org/W2149233311",
    "https://openalex.org/W1853454069",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3024622987",
    "https://openalex.org/W2567326027",
    "https://openalex.org/W2963083845",
    "https://openalex.org/W2966892770",
    "https://openalex.org/W2952566282",
    "https://openalex.org/W2963188749",
    "https://openalex.org/W1663587272",
    "https://openalex.org/W112196149",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2952250911",
    "https://openalex.org/W2952828476",
    "https://openalex.org/W2064794832",
    "https://openalex.org/W1523493493",
    "https://openalex.org/W2101461018",
    "https://openalex.org/W1743243001",
    "https://openalex.org/W2950455042",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W1826790618",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W15334911",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W3027440908",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2620558438",
    "https://openalex.org/W2574521949",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W1504008138",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W632291594",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W137217113",
    "https://openalex.org/W2118585731",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3193573834",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2949364118",
    "https://openalex.org/W2252215182",
    "https://openalex.org/W2770260627",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3105625590",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2613579016",
    "https://openalex.org/W2562607067",
    "https://openalex.org/W2949547296"
  ],
  "abstract": "Word representation has always been an important research area in the history of natural language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP-related tasks. In the end, this survey briefly discusses the commonly used ML- and DL-based classifiers, evaluation metrics, and the applications of these word embeddings in different NLP tasks.",
  "full_text": "A Comprehensive Survey on Word Representation\nModels: From Classical to State-Of-The-Art Word\nRepresentation Language Models\nUSMAN NASEEM‚àó, School of Computer Science, The University of Sydney, Australia\nIMRAN RAZZAK, School of Information Technology, Deakin University, Australia\nSHAH KHALID KHAN, School of Engineering, RMIT University, Australia\nMUKESH PRASAD,School of Computer Science, University of Technology Sydney, Australia\nWord representation has always been an important research area in the history of natural language\nprocessing (NLP). Understanding such complex text data is imperative, given that it is rich in information\nand can be used widely across various applications. In this survey, we explore different word representation\nmodels and its power of expression, from the classical to modern-day state-of-the-art word representation\nlanguage models (LMS). We describe a variety of text representation methods, and model designs have\nblossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text\ninto effective vector representations capturing the same semantic information. Further, such representations\ncan be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end,\nthis survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the\napplications of these word embeddings in different NLP tasks.\nAdditional Key Words and Phrases: Text Mining, Natural Language Processing, Word representation,\nLanguage Models\nACM Reference Format:\nUsman Naseem, Imran Razzak, Shah Khalid Khan, and Mukesh Prasad. 2018. A Comprehensive Survey on\nWord Representation Models: From Classical to State-Of-The-Art Word Representation Language Models.\nIn Woodstock ‚Äô18: ACM Symposium on Neural Gaze Detection, June 03‚Äì05, 2018, Woodstock, NY. ACM, New\nYork, NY, USA, 46 pages. https://doi.org/10.1145/1122445.1122456\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without\nfee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice\nand the full citation on the first page. Copyrights for components of this work owned by others than ACM must be\nhonored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to\nlists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n¬© 2018 Association for Computing Machinery.\nManuscript submitted to ACM\n1\narXiv:2010.15036v1  [cs.CL]  28 Oct 2020\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\n1 INTRODUCTION\nText-based data is increasing at a rapid rate, where the low quality of the unstructured text is\ngrowing rapidly than structured text. Textual data is very common in many different domains\nwhether social media, online forums, published articles or clinical notes for patients and online\nreviews given online where people express their opinions and sentiments to some products or\nbusinesses [53].\nText data is a rich source of getting information and gives more opportunity to explore valuable\ninsights which can not be achieved from quantitative data [54]. The main aim of different NLP\nmethods is to get a human-like understanding of the text. It helps to examine the vast amount of\nunstructured and low-quality text and discover appropriate insights. Couple with ML, it can formu-\nlate different models for the classification of low-quality text to give labels or obtain information\nbased on prior training. For instance; researchers in the past focused on mining the opinion and\nsentiments of users about a product, restaurant and movie reviews etc. to predict the sentiment\nof users. Over the years text has been used in various applications such as email filtering [ 20],\nIrony and sarcasm detection [113] document organization [46], sentiment and opinion mining\nprediction [107, 115], hate speech detection [105, 106, 110, 114], question answering [44], content\nmining [2], biomedical text mining [109, 112] and many more.\nFig. 1. Text Classification Pipeline\nHowever, being unstructured content, it adds complexity to the model, deciphers automatically\nor uses in conjunction with traditional features for a ML framework [57]. Moreover, even though\nlarge of volumes of text information is widely available and can be leveraged for interesting\napplications, it is rife with problems. Like most data, it suffers from traditional problems such\nas class imbalance and lack of class labels, but besides, there are some inherent issues with text\ninformation. Apart from being unstructured, text mining and representation learning become\nmore challenging due to the following discussed factors.\nThe language on social media is unstructured and informal. Social media users express their\nemotions and write in different ways, use abbreviations, punctuations, emoticons, slangs and often\nuse URLs. These language imperfections may cause noise and is a challenging task to handle by\n2\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\napplying appropriate pre-processing techniques. Besides, understanding semantics, syntactical\ninformation and context is important for text analysis [104, 111].\nMuch research has been dedicated to addressing each of these concerns individually. However,\nin this survey, we focus on how text can be represented as numeric\\continuous vectors for easier\nrepresentation, understanding and applicability to traditional machine-learning frameworks. Text\nmay be seen as a collection of entities such as documents, sentences, words or characters and most\nalgorithms leverage the implicit relationship between these entities to infer them as vectors.\nOver the years, many methods and algorithms have been used to infer vectors from text be at\ncharacter, word, sentence or document level. All the methods are aimed at better quantifying the\nrichness in the information and making them more suitable for machine learning frameworks such\nas to perform clustering, dimensionality reduction or text classification. In this survey, we study\nhow text representation methods have evolved from manually selecting the features called feature\nengineering to more SOTA representational learning methods which leverage neural networks to\ndiscover relevant embeddings.\nIn any NLP task, first, we should have data which we are interested in analyzing. The next\nstep is to represent the raw unstructured data in a form that ML classification algorithms can\nunderstand. Text representation is divided into main two parts: i) Text pre-processing and, ii)\nFeatures Extraction and then classify the learned representations using an appropriate classifier\n[3, 69].\nContribution and Organization In this paper, we present a comprehensive study of various\ntext representation methods starting from the bag of words approach to more SOTA representa-\ntional learning methods. We describe various commonly used text representation methods and\ntheir variations and discuss various text mining applications they have been used in. We conclude\nwith a discussion about the future of text representation based on our findings. We would like to\nnote that this paper, strictly focuses on the representation of text for low-quality text Classification\nand therefore uses content, data and text interchangeably.\nBelow, first, we briefly discuss different steps in text classification pipeline illustrated in Fig. 1,\nfollowed by the details of each step in next sections.\n(1) Unstructured (Low Quality) Text: Unstructured (Low Quality) text is a form of written\ntext which requires metadata and cannot easily be listed or classified. Usually, it is the\ninformation generated by users on social media postings, documents, email or messages.\nRaw text is scattered and sparse with less number of features and does not give sufficient\nword co-occurrence information. It is an important origin of information for businesses,\nresearch institutes and monitoring agencies. Often companies mine it for getting the data to\nimprove their marketing strategies and achieve an edge in the marketplace. It plays a big part\n3\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\nin predictive analytics and in analysing sentiments of users to find-out the overall opinion of\ncustomers. It helps to discover unique insights by revealing hidden information, discovering\ntrends and recognising relationships between irrelevant bits of the data [45, 161].\n(2) Text Representation: For text classification, the text should be converted into a form which\nthe computer can understand. First, we need to improve the quality of raw and unstructured\ntext and then extract features from it before classification. Both of these steps are briefly\ndiscussed below.\n‚Ä¢Text pre-processing: pre-processing is the crucial step, especially in the classification\nof short text. pre-processing techniques are valuable techniques for decreasing the data\nadequacy, sparsity and helps to improve the low quality of text especially in the case of\nshort text where everyone writes in their style, and use emoticons, abbreviations, make\nspelling mistakes and use URLs etc. A proper combination of common and advance pre-\nprocessing techniques can help to learn good text representation [8, 146]. pre-processing\ntechniques analysed in our study are briefly discussed in section 2.\n‚Ä¢ Features Extraction: Features extractions of the data is the critical step for machines to\nclassify and understand the data like humans. It is the process of transforming raw data\ninto numeric data which machines can understand. Usually, this feature extraction step of\ntransforming a raw data is called a features vector. Extracting robust word representations\nis not so easy without having a considerable amount of corpus due to diversity of expressing\nsentiments, emotions and intentions in the English language. However, due to social media\nplatforms, researchers now have access to get an enormous amount of data. However,\nassigning labels to this massive amount of data collected from social media platforms is not\nan easy job. To make this annotation process easy, researchers initially worked on finding\na sign of sentiment and emotion within the content of the text like emoticons and hashtags\n[69, 155, 165]. Some of the famous classical and current feature extraction algorithms are\nbriefly discussed in section 3.\n(3) Classification: Selecting the best classifier is the essential part of text classification pipeline.\nIt is hard to find out the most effective and adequate classifier for text classification task\nwithout understanding theoretically and conceptually each algorithm. Since the scope of\nthis paper is only restricted to present different text representation methods so we will not\ndiscuss different text classification algorithms in detail. These classifiers include famous\ntraditional ML algorithms of text classification such as Logistic Regression which is used in\nmany data mining areas [23, 123], Naive Bayes which is computationally not expensive and\nworks well with less amount of memory [73], K-nearest Neighbour which is non-parametric\nmethods and Support Vector Machine is a famous classifier which has been widely used in\nmany different areas earlier. Then tree-based algorithms like random forest and decision\n4\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\ntree are discussed followed by deep learning (DL)-based classifiers which are a collection of\nmethods and approaches motivated by the working mechanism of the human brain. These\nmethods utilise the extensive amounts of training input data to achieve the high quality of\nsemantically rich text representations which can be given as input to different ML methods\nwhich can make better predictions [67, 69].\n2 TEXT PRE-PROCESSING\nText datasets contain a lot of unwanted words such as stop-words, punctuation, incorrect spellings,\nslangs, etc. This unwanted noise and words may have an negative effect on the performance of the\nclassification task. Below first, we present the preliminaries where we discuss different methods\nand techniques related to text pre-processing and cleaning, followed by some literature review\nwhere researchers analyzed the effects of text pre-processing techniques.\n2.1 Preliminaries related to Text Pre-processing\n‚Ä¢Tokenization A process of transforming a text (sentence) into tokens or words is known\nas tokenization. Documents can be tokenized into sentences, whereas sentences can be\nconverted into tokens. In tokenization, a sequence to text is divided into the words, symbols,\nphrases or tokens [ 6]. The prime objective of tokenization is to find out the words in a\nsentence. Usually, tokenization is applied as a first and standard pre-processing step in any\nNLP task.[40]\n‚Ä¢Removal of Noise, URLs, Hashtag and User-mentions Unwanted strings and Unicode\nare considered as leftover during the crawling process, which is not useful for the machines\nand creates noise in the data. Also, almost all of tweets messages posted by users, contains\nURLs to provide extra information, User-mention/tags (ùõº) and use hashtag symbol ‚Äù#‚Äù to\nassociate their tweet message with some particular topic and can also express their sentiments\nin tweets by using hashtags. These give extra information which is useful for human beings,\nbut it does not provide any information to machines and considered as noise which needs to\nbe handled. Researchers have presented different techniques to handle this extra information\nprovided by users such as in the case of URLs; it is replaced with tags [ 1] whereas User-\nmentions (ùõº) are removed [13, 65]\n‚Ä¢Word Segmentation Word segmentation is the process of separating the phrases, content\nand keywords used in the hashtag. Moreover, this step can help in understanding and\nclassifying the content of tweets easily for machines without any human intervention. As\nmentioned earlier, Twitter users use # (hashtags) in almost all tweets to associate their tweets\nwith some particular topic. The phrase or keyword starting with # is known as hashtags.\nVarious techniques are presented in the literature for word segmentation in [22, 136].\n5\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\n‚Ä¢Replacing Emoticons and Emojis Twitter users use many different emoticons and emojis\nsuch as:), :(, etc. to express their sentiments and opinions. So it is important to capture this\nuseful information to classify the tweets correctly. There are few tokenizers available which\ncan capture few expressions and emotions and replace them with their associated meanings\n[41].\n‚Ä¢Replacement of abbreviation and slang Character limitations of Twitter enforce online\nusers to use abbreviations, short words and slangs in their posts online. An abbreviation is\na short or acronym of a word such as MIA which stands for missing in action. In contrast,\nslang is an informal way of expressing thoughts or meanings which is sometimes restricted\nto some particular group of people, context and considered as informal. So it is crucial to\nhandle such kind of informal nature of text by replacing them to their actual meaning to\nget better performance without losing information. Researchers have proposed different\nmethods to handle this kind of issue in a text, but the most useful technique is to convert\nthem to an actual word which is easy for a machine to understand [68, 100].\n‚Ä¢Replacing elongated characters Social media users, sometimes intentionally use elongated\nwords in which they purposely write or add more characters repeatedly more times, such as\nloooovvveee, greeeeat. Thus, it is important to deal with these words and change them to\ntheir base word so that classifier does not treat them different words. In our experiments,\nwe replaced elongated words to their original base words. Detection and Replacement of\nelongated words have been studied by [97] and [5].\n‚Ä¢Correction of Spelling mistakes Incorrect spellings and grammatical mistakes are very\ncommonly present in the text, especially in the case of social media platforms, especially on\nTwitter and Facebook. Correction of spelling and grammatical mistakes helps in reducing\nthe same words written indifferently. Textblob is one the library which can be used for this\npurpose. Norvig‚Äôs spell correction1 method is also widely used to correct spelling mistakes.\n‚Ä¢Expanding Contractions A contraction is a shortened form of the words which is widely\nbeing used by online users. An apostrophe is used in the place of the missing letter(s).\nBecause we want to standardize the text for machines to process easily so, in the removal of\ncontractions, shortened words are expanded to their original root /base words. For example,\nwords like how is, I‚Äôm, can‚Äôt and don‚Äôt are the contractions for words how is, I am, cannot\nand do not respectively. In the study conducted by [14], contractions were replaced with their\noriginal words or by the relevant word. If contractions are not replaced, then the tokenization\nstep will create tokens of the word \"can‚Äôt\" into \"can\" \"t\".\n1http://norvig.com/spell-correct.html\n6\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\n‚Ä¢Removing Punctuations Social media users use different punctuations to express their\nsentiments and emotions, which may are useful for humans but not all much useful for\nmachines for the classification of short texts. So removal of punctuation is common practice\nin classification tasks such as sentiment analysis. However, sometimes some punctuation\nsymbols like \"!\" and \"?\" shows/denotes the sentiments. Its common practice to remove\npunctuation. [82]. whereas, replacing question mark or sign of exclamation with tags has\nalso been studied by [5].\n‚Ä¢Removing Numbers Text corpus usually contains unwanted numbers which are useful for\nhuman beings to understand but not much use for machines which makes lowers the results of\nthe classification task. The simple and standard method is to remove them [47, 58]. However,\nwe could lose some useful information if we remove them before transforming slang and\nabbreviation into their actual words. For example, words like \"2maro\", \"4 u\", \"gr8\", etc. should\nbe first converted to actual words, and then we can proceed with this pre-processing step.\n‚Ä¢Lower-casing all words A sentence in a corpus has many different words with capitalization.\nThis step of pre-processing helps to avoid different copies of the same words. This diversity\nof capitalization within the corpus can cause a problem during the classification task and\nlower the performance. Changing each capital letters into a lower case is the most common\nmethod to handle this issue in text data. Although, this pre-processing technique projects\nall tokens in a corpus under the one feature space also causes a bunch of problems in the\ninterpretation of some words like \"US\" in the raw corpus. The word \"US \"could be pronoun\nand a country name as well, so converting it to a lower case in all cases can be problematic.\nThe study conducted by [33] has lower-cased words in corpus to get clean words.\n‚Ä¢Removing Stop-words In-text classification task, there are many words which do not have\ncritical significance and are present in high frequency in a text. It means the words which\ndoes not help to improve the performance because they do not have much information for\nthe sentiment classification task, so it is recommended to remove stop words before feature\nselection step. Words like (a, the, is, and, am, are, etc.). A popular and straightforward method\nto handle with such words is to remove them. There are different stop-word libraries available\nsuch as NLTK, scikit-learn and spaCy.\n‚Ä¢Stemming One word can turn up in many different forms, whereas the semantic meaning of\nthose words is still the same. Stemming is the techniques to replace and remove the suffixes\nand affixes to get the root, base or stem word. The importance of stemming was studied by\n[92]. There are several types of stemming algorithms which helps to consolidate different\nforms of words into the same feature space such as Porter Stemmer, Lancaster stemmer\nand Snowball stemmers etc. Feature reduction can be achieved by utilizing the stemming\ntechnique.\n7\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\n‚Ä¢Lemmatization The purpose of the lemmatization is the same as stemming, which is to cut\ndown the words to it‚Äôs base or root words. However, in lemmatization inflection of words\nare not just chopped off, but it uses lexical knowledge to transform words into its base forms.\nThere are many libraries available which help to do this lemmatization technique. Few of\nthe famous ones are NLTK (Wordnet lemmatizer), genism, Stanford CoreNLP, spaCy and\nTextBlob etc.\n‚Ä¢Part of Speech (POS) Tagging The purpose of Pat of speech (POS) tagging is to assign part\nof speech to text. It clubs together with the words which have the same grammatical with\nwords together.\n‚Ä¢Handling Negations For humans, it is simple to get the context if there is any negation\npresent in the sentence, but for machines sometimes it does not help to capture and classify\naccurately so handling a negation can be a challenging task in the case of word-level text\nanalysis. Replacing negation words with the prefix ‚ÄôNEG_ ‚Äô has been studied by [103]. Similarly,\nhandling negations with antonym has been studied by [124].\n2.2 Related work on text pre-processing methods\nText pre-processing plays a significant role in text classification. Many researchers in the past\nhave made efforts to understand the effectiveness of different pre-processing techniques and their\ncontribution to text classification tasks. Below we present some studies conducted on the effects\nof pre-processing techniques on text classification tasks.\nBao et al. [8] study showed the effect of pre-processing techniques on Twitter analysis task.\nUni-gram and bi-grams features were fed to Liblinear classifier for the classification. They showed\nin their study that reservation of URL features, the transformation of negation (negated words)\nand normalization of repeated tokens have a positive effect on classification results whereas\nlemmatization and stemming have a negative effect on classification results. Singh and Kumari [146]\nshowed the impact of pre-processing on Twitter dataset full of abbreviations, slangs, acronyms\nfor the sentiment classification task. In their study, they showed the importance and significance\nof slang and correction of spelling mistakes and used Support Vector Machine (SVM) classifier\nto study the role of pre-processing for sentiment classification. Haddi et al. [ 45] also explored\nthe effect of text pre-processing on movie review dataset. The experimental shows that pre-\nprocessing methods like the transformation of text such as changing abbreviations to actual\nwords and removal of stop word, special characters and handling of negation with the prefix\n‚ÄòNOT‚Äô and stemming can significantly improve the classification performance. The SVM classifier\nwas used in their experiments ‚Äî the study conducted by Uysal and Gunal. [161] to analyze the\nrole of pre-processing on two different languages for sentiment classification was presented.\nThey employed SVM classifier in their studies and showed that performance is improved by\n8\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nselecting an appropriate combination of different techniques such as removal of stop words,\nthe lower casing of text, tokenization and stemming. They concluded that researchers should\nchoose all possible combinations carefully because the inappropriate combination may result in\ndegradation of performance. Similarly, Jianqiang and Xiaolin [59] studied the role of six different\npre-processing techniques on five datasets in their study, where they used four different classifiers.\nTheir experimental results show that replacing acronyms (abbreviations) with actual words and\nnegations improved the sentiment classification, whereas removing stop-words, special characters,\nand URLs have an adverse influence on the results of sentiment classification. Role of text pre-\nprocessing to reduce the sparsity issue in Twitter sentiment classification is studied by Said\net al. [139]. Experimental results demonstrate that choosing a combination of appropriate pre-\nprocessing methods can decrease the sparsity and enhance the classification results. Agarwal\net al. [1] proposed novel tweets pre-processing approaches in their studies. They replaced URL,\nuser mentions, repeated characters and negated words with different tags and removed hashtags.\nClassification results were improved by their proposed pre-processing methods. In other studies\npresented by Saloot et al. [140] and Takeda and Takefuiji [168] in the natural language workshop\nwhich focuses on noise user-generated text2. Noisy nature of Twitter messages is reduced/decreased\nby normalizing tweets using a maximum entropy model and entity linking. Recently, Symeonidis\net al. [156] presented the comparative analysis of different techniques on two datasets for Twitter\nsentiment analysis. In their study, they studied the effect of each technique on four traditional\nML-based classifiers and one neural network-based classifier with only TFIDF (unigram) for words\nrepresentation method. Their study showed that pre-processing technique such as removing\nnumbers, lemmatization and expanding contractions to base words performs better, whereas\nremoving punctuation does not perform well in the classification task. Their study also presented\nthe interactions of the limited number of different techniques with others and showed that\ntechniques which perform well when interacted with others. However, no work has been done the\nrecommendation of pre-processing techniques to improve the quality of the text.\n3 FEATURE EXTRACTION METHODS\nIn this section, we discuss various popularly used feature extraction models. Different researchers\nin the past have proposed different features of extraction models to address the problem of loosing\nsyntactic and semantic relationships between words. These methods, along with the literature\nreview where different methods have been adopted for different NLP related tasks. First, we present\nsome classical models, followed by some famous representation learning models.\n2http://noisy-text.github.io/\n9\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\n3.1 Classical Models\nThis section presents some of the classical models which were commonly used in earlier days for\nthe text classification task. Frequency of words is the basis of this kind of words representation\nmethods. In these methods, a text is transformed into a vector form which contains the number\nof the words appearing in a document. First, we give a short description of categorical word\nrepresentation methods and then weighted word representation methods.\n(1) Categorical word representation: is the simplest way to represent text. In this method,\nwords are represented by a symbolic representation either \"1\" or \"0\". One-hot encoding and\nBag-of-words (BoW) are the two models which come under categorical word representation\nmethods. Both are briefly discussed below.\n‚Ä¢One hot encoding: The most straightforward method of text representation is one hot\nencoding. In one hot encoding, the dimension is the same amount of terms present in the\nvocabulary. Every term in vocabulary is represented as a binary variable such as 0 or 1,\nwhich means each word is made up of zeros and ones. Index of the corresponding word is\nmarked with 1, whereas all others are marked as zero (0). Each unique word has a unique\ndimension and will be represented by a 1 in that dimension with 0s everywhere else.\n‚Ä¢Bag-of-Words (BoW): BoW is simply an extension of one-hot encoding. It adds up the\none-hot representations of words in the sentence. The BOW method is used in many\ndifferent areas such as NLP, computer vision (CV), and information retrieval (IR) etc. The\nmatrix of words built using BOW ignore the semantic relationship between words and\norder of word is also ignored along with the grammar.\nFig. 2. An illustration of one-hot encoding and BoW models\nAs stated, BOW is an extension of one-hot encoding, e.g., encodes token in the vocabulary\nas a 1-hot-encoded vector. As vocabulary may increase to huge numbers, then vocabulary\nsize would increase and thereby, the length of the vectors would increase too. Besides, a\n10\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nlarge number of \"0s\" which may result in a sparse matrix, containing no order of text as\nwell as information of the grammar used in the sentences.\nAn example of \"Hello\" and \"World\" as one-hot encoding and \"Hello World \" as BoW is given\nin Fig. 2.\n(2) Weighted Word representation: Here, we present the common methods for weighted\nword representations such as Term Frequency (TF) and Term Frequency-Inverse Document\nFrequency (TF-IDF). These are associated with categorical word representation methods\nbut rather than only counting; weighted models feature numerical representations based on\nwords frequency. Both of them are briefly discussed below.\n‚Ä¢Term Frequency (TF) : Term frequency (TF), is the straightforward method of text feature\nextraction. TF calculates how often a word occurs in a document. A word can probably\nappear many times in large documents as compared to small ones. Hence, TF is computed\nby dividing the length of the document. In other words, TF of a word is computed by\ndividing it with the total number of words in the document.\n‚Ä¢Term Frequency-Inverse Document Frequency (TF-IDF) : To cut down the impact of\ncommon words such as ‚Äôthe‚Äô, ‚Äôand‚Äô etc. in the corpus, TF-IDF was presented by [150] for text\nrepresentation. TF here stands for Term frequency which is defined in the above section,\nand IDF denotes inverse document frequency which is a technique presented to be used\nwith TF to reduce the effect of common words. IDF assigns a more weight to words with\neither higher or lower frequencies. This combination of TF and IDF method is known as\nTF-IDF and is represented mathematically by the below equation.\nùëáùêπ ‚àíùêºùê∑ùêπ (ùë°,ùëë,ùê∑ )= ùëáùêπ(ùë°,ùëë)√ólog(ùê∑\nùëëùëìùë°\n)\nWhere ùë° denotes the terms; ùëë denotes each document; ùê∑ represents the collection of\ndocuments and ùëëùëìùë° denotes sum of documents with term ùë° in it. TF-IDF is built on the\nconcept of BOW model; therefore, it can not capture the order of words in a document,\nsemantics and syntactical information of words. Hence, TF-IDF is good to use as a lexical\nlevel feature.\n3.2 Representation Learning\nSince categorical word representations, models fail to capture syntactic and semantic meaning of\nthe words, and these models suffer the curse of high dimensionality. The shortcomings of these\nmodels led the researchers to learn the distributed word representation in low dimension space [17].\nThe limitations of classical feature extraction methods make it use a limited for building a suitable\nmodel in ML. Due to this, different models have been presented in the past, which discovers the\n11\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\nrepresentations automatically for downstream tasks such as classification. Such methods which\ndiscover features itself are called as feature learning or representation learning.\nIt is very important because the performance of ML models heavily depends on the represen-\ntations of the input [10]. DL-based model, which are good at learning important features itself,\nis changing traditional feature learning methods. Proper representation can be learned either by\nutilizing supervised learning methods or unsupervised methods.\nIn the area of NLP, unsupervised text representation methods like word embeddings have\nreplaced categorical text representation methods. These word embeddings turned into very efficient\nrepresentation methods to improve the performance of various downstream tasks due to having a\nprevious knowledge for different ML models. Classical feature learning methods have been replaced\nby these neural network-based methods due to their good representation learning capacity. Word\nembedding is a feature learning method where a word from the vocabulary is mapped to ùëÅ\ndimensional vector. Many different words embedding algorithms have been presented, and the\nfamous algorithms, for instance, Word2Vec, Glove and FastText, are discussed in this study.\nFirst, we briefly present different pre-training methods for learning the word representation of\nthe document. These pre-training methods are classified into three different groups: (i) Supervised\nlearning (SL), (ii) Unsupervised learning (UL), and (iii) Self-supervised learning (SSL). Below we\ndiscuss each of these briefly:\n(1) Supervised learning (SL) is to learn a feature that translates an input to an output on a\nbasis of input-output pair training data.\n(2) Unsupervised learning (UL) is to discover some intrinsic information, such as clusters,\ndensities, latent representations, from unlabeled data.\n(3) Self-Supervised learning (SSL) is a hybrid of SL and UL. SSL‚Äôs learning model is mostly\nthe same as SL, except the training data labels are automated. SSL‚Äôs main concept is to predict\nsome aspect of the input in some form from other parts. The Masked Language Model (MLM),\nfor instance, is a self-supervised task that tries to predict the masked words in a sentence\nprovided the remaining words.\n3.2.1 Distributed Representations. As previously mentioned, hand-crafted features were primarily\nused to model tasks in natural language before approaches based on neural networks came\naround and addressed some of the challenges faced by conventional Ml algorithms, such as the\ndimensionality curse.\n(1) Continuous Words Representation (Non-Contextual Embeddings):\nWord Embedding is NLP technique in which text from the corpus is mapped as the vectors.\nIn other words, it is a type of learned representation which allows same meaning words\nto have the same representation. It is the distributed representation of a text (words and\n12\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\ndocuments) which is a significant breakthrough for better performance for NLP related\nproblems. The most significant benefit of word embedding is this that it provides more\nefficient and expressive representation by keeping the word similarity of context and by low\ndimensional vectors. Nowadays, word embedding is being used in many different applications\nlike semantic analysis, philology, psychiatry, cognitive science, social science and psychology\n[34].\nAn automatic feature learning technique in which every token in a vocabulary is indexed\ninto an N dimension vector is known as distributed vectors or Word embedding. Which\nfollows the distributional hypothesis. According to this, words which are used and appear\nin the similar contexts tend to assure the same meanings. So these vectors tend to have the\nattributes of word‚Äôs neighbours, and they capture the similarity between words. During 1990,\nseveral researchers made attempts to lay down the foundation of distributional semantic\nlearning.\nBengio et al. [11] presented a model which learned word representations using distributed\nrepresentation. Authors presented NNLM model which obtains word representations as to\nthe product while training language model (LM). Just like traditional LM, NNLM also uses\nprevious ùëõ‚àí1 words/tokens to predict theùëõùë°‚Ñéword/token. Different word embedding models\nhave been proposed, which makes uni-grams useful and understandable to ML algorithms\nand usually, these models are used in the first layer in a deep neural network-based model.\nThese word embedding are pre-trained by predicting a word based on its context without\nlosing semantic and syntactical information. Thus, using these embedding techniques have\ndemonstrated to be helpful in many NLP tasks because It does not lose the order of words\nand captures the meaning of words (syntactic and semantic information of words). However,\nthe popularity of word representation methods are due to two famous models, Word2Vec\n[96] and GloVe [89]. These famous, along with others, are briefly discussed below.\n‚Ä¢Word2Vec\nWord2vec is words representation model developed by [96]. This model uses two hidden\nlayers which are used in a shallow neural network to create a vector of each word. The\nword vectors captured by Continuous Bag of words (CBOW) and Skip-gram models of\nword2vec are supposed to have semantic and syntactic information of words. To have\na better representation of words, it is recommended to train the corpus with the large\ncorpus. Word2Vec have proved to be useful in many NLP related tasks [ 28]. Word2Vec\nwas developed to build training of embedding more significant, and since then, it has been\nused as a standard for developing pre-trained word representation. Based on the context,\nWord2Vec predicts by using one of the two neural network models such as Continuous bag\nof words (CBOW) and Skip-gram. A predefined length of the window is moved together\n13\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\nwith the corpus in both models, and the training is done with words inside the window\nin each step [ 4]. This feature presentation algorithm gives a robust tool for unfolding\nrelationships in the corpus and the similarity between token. For instance, this method\nwould regard the two words such as ‚Äú small‚Äù and ‚Äú smaller‚Äù near to each other in the\nvector space. Fig.3 shows the working principle of both Word2Vec algorithms,CBOW and\nSkip-Gram.\nFig. 3. Working principle of Word2Vec\n(Image taken from [96])\n‚Äì Continuous Bag of words (CBOW): Continuous Bag of words (CBOW) gives words\nprediction of current work based on its context. CBOW communicates with the neighbour-\ning words in the window. Three layers are used in CBOW process. Context is considered\nas the first layer whereas the layer which is hidden matches with the estimation of every\nword from the input to the weight matrix which later on is estimated to the output which\nis considered as the third layer. The last phase of this method is to correlate the output\nand the work itself to improve the representation based on the backpropagation of the\nerror gradient. In a Fig.3, CBOW method predicts the middle word based on its context\nin skip-gram predicts the context word based on centre word [102].\n‚Äì Skip-Gram:\nSkip-Gram is the reverse of CBOW model; prediction is given based on the central\nword after the training of context in skip-gram. Input layer correlates with the targeted\nword, and the output layer corresponds with the context. This model looks for the\nestimation of the context given the word, unlike CBOW. The last phase of this model is\nthe correlation between output and every word of the context to adjust representation\nbased on back-propagation [34, 102].\nSkip-gram is efficient when we have less training data, and not frequent words are well\npresented. In comparison, CBOW is quicker and performs better with repeated words.\n14\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nTo address the issues of learning the final vectors, two algorithms are proposed. First\none is negative sampling in which we restrict the sum of output vectors which needs\nto be updated, so only a sample of the vectors is updated based on a noise distribution\nwhich is a probabilistic distribution used in the sample step. Moreover, the other method\nis Hierarchical softmax which is developed based on the Huffman tree. It is a binary tree\nwhich gives all words depending on their counts. Then normalization is done for each\nstep from the root to the target. Negative sampling is efficient when the dimension of\nvectors is less and works well with repeated words. In comparison, hierarchical softmax\nworks well when we have less frequent words [102].\n‚Ä¢Global Vectors (GloVe):\nWord2vec-trained word embedding will better capture the semantics of words and manip-\nulate the connectivity of words. However, Word2vec mainly focuses on the local context\nwindow knowledge, whereas the global statistical information is not used well. So the\nGlove [89] is presented, which is a famous algorithm based on the global co-occurrence\nmatrix, each element ùëãùëñùëó in the matrix depicts the frequency of the word ùë§ùëñ and the word\nùë§ùëó co-occur in a appropriate context window and is widely used for the text classification\ntask.\nGloVe is an expansion of the word2Vec for learning word vectors efficiently where the\nwords prediction is made based on surrounding words. Glove is based on the appearances\nof a word in the corpus, which is based on two steps. Creation of the co-occurrence\nmatrix from the corpus is the first step, followed by the factorization to get vectors. Like\nword2Vec, GloVe also provided pre-trained embeddings in a different dimension (100, 200,\n300 dimensions) which are trained over the vast corpus. The objective function of GloVe is\ngiven below:\nùêΩ = √çùëâ\nùëò,ùëó=1 ùëì(ùëãùëòùëó)(ùë§ùëá\nùëòùë§\n‚Ä≤\nùëó +ùëèùëò +ùëèùëó ‚àílog ùëãùëòùëó)\nwhere;\nV : is size of vocabulary,\nX : is co-occurrence matrix,\nùëãùëòùëó is frequency of word k co-occurring with word j,\nùëãùëò total number of occurrences of word k in the corpus,\nùëÉùëòùëó is the probability of word j occurring within the context of word k,\nw is a word embedding of dimension d,\nùë§\n‚Ä≤\nis the the context word embedding of dimension d\nWord representation methods such as Word2vec and GloVe are simple, accurate, and on\nlarge data sets, they can learn semantic representations of words. They do not, however,\nlearn embedded words from out-of-vocabulary(OOV) words. Such words can be defined\n15\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\nin two ways: words that are not included in the current vocabulary and words that do\nnot appear in the current training corpus. To solve these various models are proposed to\naddress this challenge. We briefly describe one of the most famous models below.\n‚Ä¢FastText\nBojanowski et al. [15] proposed FastText and is based on CBOW. When compared with\nother algorithms, FastText decreases the training time and maintains the performance.\nPreviously mentioned algorithms assign a distinct representation to every word which\nintroduces a limitation, especially in case of languages with sub-word level information/\nOOV.\nFastText model solved the issues mentioned above. FastText breaks a word in n-grams\ninstead of full word for feeding into a neural network, which can acquire the relationship\nbetween characters and pick up the semantics of words. FastText gives better results\nby having better word representation primarily in the case of rare words. Facebook has\npresented pre-trained word embeddings for 294 different languages, trained on Wikipedia\nusing FastText embedding on 300 dimensions and utilized word2Vec skip-gram model with\nits default parameters [63].\nAlthough these models are used to retain syntactic and semantic information of a document,\nthere remains the issue of how to keep the full context-specific representation of a document.\nUnderstanding the actual context is required for the most downstream tasks in NLP. Some\nwork recently tried to incorporate the word embedding with the LM to solve the problem of\nmeaning. Below, some of the common context-based models are briefly presented.\n(2) Contextual word representations:\n‚Ä¢Generic Context word representation (Context2Vec): Generic Context word repre-\nsentation (Context2Vec) was proposed by Melamud et al. [94] in 2016 to generate context-\ndependent word representations. Their model is based on word2Vec‚Äôs CBOW model but\nreplaces its average word representation within a fixed window with better and powerful\nBi-directional LSTM neural network. A large text corpus was used to learn neural model\nwhich embeds context from a sentence and target words in the same low dimension which\nlater is optimized to reflect the inter-dependencies between target and their entire sentential\ncontext as a whole as shown in Fig. 4.\n‚Ä¢Contextualized word representations Vectors (CoVe):\nMcCann et al. [ 90] presented their model contextualized word representations vectors\n(CoVe) which is based on context2Vec. They used machine translation to build CoVe instead\nof the approach used in Word2Vec (skip-gram or CBOW) or Glove (Matric factorization).\nTheir basic approach was to pre-train two-layer BiLSTM for attention sequence to sequence\ntranslation, starting from GloVe word vectors and then they took the output of sequence\n16\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nFig. 4. Working principle of Context2Vec\n(Image taken from[93])\nencoder and called it a CoVe, Combine it with GloVe vectors and use in a downstream\ntask-specific mode using transfer learning.\n‚Ä¢Embedding from language Models (ELMo)\nPeters et al. [125] proposed Embedding from Language Models (ELMo), which gives deep\ncontextual word representations. Researchers concur that two problems should be taken\ninto account in a successful word representation model: the dynamic nature of word use in\nsemantics and grammar, and as the language environment evolves, these uses should alter.\nThey therefore introduce a method of deep contextualised word representation to address\nthe two problems above, as seen in Fig. 5.\nThe final word vectors are learned from bi-directional language model (forward and back-\nward LMs). ELMo uses the linear concatenation of representations learnt from bidirectional\nlanguage model instead of only just the final layer representations like other contextual\nword representations. In different sentences, ELMo provides different word representations\nfor the same word. Word representations learned by ELMo are based on the representation\nlearned from Bi-language model (BiLM). The log-likelihood of sentences is used in the\ntraining phase of BiLMs both in forward and backward LMs. The final vector is computed\n17\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\nFig. 5. Working principle of ELMo\n(Image taken from[30])\nafter the concatenation of hidden representations from forwarding LM ‚àí ‚Üí‚ÑéùêøùëÄ\nùëõ,ùëó and backward\nLM ‚Üê ‚àí‚ÑéùêøùëÄ\nùëõ,ùëó , where ùëó = 1,....,ùêø and is given below:\nùêµùëñùêøùëÄ =\nùëò‚àëÔ∏Å\nùëõ=1\n(log ùëù(ùë°ùëõ|ùë°1,.....,ùë° ùëõ‚àí1; Œòùë•,‚àí ‚ÜíŒòùêøùëÜùëáùëÄ,Œòùë†)\n+log ùëù(ùë°ùëõ|ùë°ùëõ+1,....,ùë° ùëõ; Œòùë•,‚Üê ‚àíŒòùêøùëÜùëáùëÄ,Œòùë†)\nWhere the token representation parameters and softmax parameters ùúÉùë• and ùúÉùë† are shared\nbetween the forward and backward directions, respectively. And ‚àí ‚ÜíŒòùêøùëÜùëáùëÄ and ‚Üê ‚àíŒòùêøùëÜùëáùëÄ are\nthen forward and backward LSTM parameters respectively.In a downstream task, ELMo\nextracts the representations learned from BiLM from an intermediate layer and executes a\nlinear combination for each token. BiLM contains 2L+1 set representations as given below.\nùëÖùëõ = (ùëãùêøùëÄ\nùë• ,‚àí ‚Üí‚ÑéùêøùëÄ\nùëõ,ùëó ,‚Üê ‚àí‚ÑéùêøùëÄ\nùëõ,ùëó | ùëó = 1,....,ùêø )\n= (‚ÑéùêøùëÄ\nùëõ,ùëó | ùëó = 0,...,ùêø )\nwhere ‚ÑéùêøùëÄ\nùëõ,0 = ùë•ùêøùëÄ\nùëõ is the layer of token and ‚ÑéùêøùëÄ\nùëõ,ùëó = [‚àí ‚Üí‚ÑéùêøùëÄ\nùëõ,ùëó ,‚Üê ‚àí‚ÑéùêøùëÄ\nùëõ,ùëó ]for each bilstm layer.\nELMo is a combination of these characteristics unique to the task where all layers in M are\nflattened to a single vector and are given below:\nùê∏ùêøùëÄùëúùë°ùëéùë†ùëò\nùëõ = ùê∏(ùëÄùëõ; Œòùë°ùëéùë†ùëò)= ùõæùë°ùëéùë†ùëò\nùêø‚àëÔ∏Å\nùëó=0\nùë†ùë°ùëéùë†ùëò\nùëó ‚ÑéùêøùëÄ\n‚Ñé,ùëó (1)\nWhere ùë†ùë°ùëéùë†ùëò are weights which are softmax normalized for the combination of representa-\ntions from different layers and ùõæùë°ùëéùë†ùëò is a hyper-parameter for optimization and scaling of\nrepresentations.\n18\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nTable 1. Comparison of Classical, non-contextual and contextual (Context2Vec, CoVe, ELMo) Word Repre-\nsentation Models\nModel Architecture Type Pros Cons\nOne Hot Encoding\nand\nBoW\n- Count\nbased\ni) Easy to compute\nii) Works with the unknown word\niii) Fundamental metric to extract terms\ni) It does not capture the semantics syntactic info.\nii) Common words effect on the results\niii) Can not capture sentiment of words\nTF\nand\nTF-IDF\n-\ni) Easy to compute\nii) Fundamental metric to extract the descriptive terms\niii) Because of IDF, common terms do not impact results\ni) It does not capture the semantics syntactic info.\nii) Can not capture the sentiment of words\nWord2Vec Log Bilinear Prediction based i) It captures the text semantics syntactic\nii) Trained on huge corpus ( Pre-trained)\ni) Fails to capture contextual information.\nii) It fails to capture OOV words\niii) Need huge corpus to learn\nGloVe Log Bilinear Count based\ni) Enforce vectors in the vector space to identify\nsub-linear relationships\nii) Smaller weight will not affect the training progress\nfor common words pairs such as stop words\ni) It fails to capture contextual information\nii) Memory utilization for storage\niii) It fails to capture OOV words\niv) Need huge corpus to learn (Pre-trained)\nFastText Log Bilinear Prediction based i) Works for rare words\nii) Address OOV words issue.\ni) It fails to capture contextual information\nii) Memory consumption for storage\niii) Compared to GloVe and Word2Vec, it is more\ncostly computationally.\nContext2Vec\nCoVe ELMo BiLSTM Prediction based i) It solves the contextual information issue\ni) Improves performance\nii) Computationally is more expensive\niii) Require another word embedding for all\nLSTM and feed-forward layer\nTable 1 presents the comparison of Classical, non-contextual and contextual (Context2Vec,\nCoVe and ELMo) LMs with their Pros and cons.\nSummary: Text representation embeds textual data into a vector space, which significantly\naffects the performance of downstream learning tasks. Better representation of text is\nmore likely to facilitate better performance if it can efficiently capture intrinsic data\nattributes. Below we briefly highlight the limitations of categorical and continuous word\nrepresentation models.\nClassical word representation methods like categorical and weighted word representations\nare the most naive and most straightforward representation of textual data. These legacy\nword representation models have been used widely in early days for different classification\ntasks like document classification, Natural language processing (NLP), information retrieval\nand computer vision (CV). The categorical word representation models are simple and\nnot difficult to implement but their limitations such as they do not consider capture\nsemantics and syntactic information because they do not consider the order of words and\ndo not consider any relationship between words. Further, the size of the input vector is\nproportional to vocabulary size, which makes them computationally expensive, which\nresults in poor performance.\n19\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\nRepresentation learning methods have helped the research community to build powerful\nmodels. However, its drawback is that the features need to be selected manually so to\nsolve this shortcoming there was a need to present some methods which can discover\nand learn these representations automatically for any downstream task. This automatic\nextraction of features without human intervention is known as representation learning\nwhich has improved results drastically over the past few years in many areas such as image\ndetection, speech recognition, NLP etc. [ 76]. Continuous word representation models\nlike Word2Vec , GloVe and FastText [16, 63, 89, 96] etc. have drastically improved the\nclassification results and overcome shortcomings of categorical representations. It is found\nthat having these continuous word representation of words is more affected as compared\nto traditional linguistic features because of their ability to capture more semantic and\nsyntactic information of the textual data without losing much information. Despite their\nsuccess, there are still some limitations which they are not capable of addressing such as\nthey are unable to handle polysemy issues because they assign the same vector to word and\nignores its context. Also, models like Word2Vec and GloVe assigns a random vector to a\nword which they did not encounter during training which means they are unable to handle\nout of vocabulary (OOV) words which were solved by FastText which breaks words into\nn-grams. All of these limitations degrades the performance of text classification. Moreover,\nall of the current SOTA methods do not perform well in the case of the low-quality text.\nTable 2. Gap Analysis of Classic, Non-contextual, Contextual (Context2Vec, Cove and ELMo)LMs\nLanguage\nModels Semantics Syntactical Context Out of\nVocabulary\n1-Hot encoding [√ó] [√ó] [√ó] [√ó]\nBoW [√ó] [√ó] [√ó] [√ó]\nTF [√ó] [√ó] [√ó] [√ó]\nTF-IDF [√ó] [√ó] [√ó] [√ó]\nWord2Vec [‚úì] [‚úì] [√ó] [√ó]\nGloVe [‚úì] [‚úì] [√ó] [√ó]\nFastText [‚úì] [‚úì] [√ó] [‚úì]\nContext2Vec [‚úì] [‚úì] [‚úì] [‚úì]\nCoVe [‚úì] [‚úì] [‚úì] [√ó]\nELMo [‚úì] [‚úì] [‚úì] [‚úì]\n‚Ä¢Universal Language Model Fine-Tuning (ULMFiT)\nPresented by Jeremy Howard of fast.ai and Sebastian Ruder of the NUI Galway Insight\nCenter, Universal Language Model Fine-tuning (ULMFiT) [ 52] is basically a method to\nallow transfer learning and achieve excellent performance for any NLP task, without\n20\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\ntraining models from scratch. ULMFiT proposed two new methods within the network\nlayers, Discriminative Fine-tuning (Discr) and Slanted Triangular Learning Rates (STLR) to\nenhance the Transfer Learning process. This approach includes fine-tuning a pre-trained\nLM, trained on the dataset of Wikitext 103, to a new dataset in such a way that it does not\nneglect what it has learned before. UMFiT was based on the SOTA LM at that time which\nis LSTM-based model. The architecture and training method, ULMFiT, builds on similar\napproaches of CoVE and ELMo. In CoVe and ELMo, the encoder layers are frozen. ULMFiT\ninstead describes a way to train all layers, and does so without over-fitting or running\ninto ‚Äúcatastrophic forgetting‚Äù, which has been more of a problem for NLP (vs Computer\nvision) transfer learning in part because NLP models tend to be relatively shallow. Table 2\npresents the gap analysis of Classic, Non-contextual, Contextual (Context2Vec, Cove and\nELMo)LMs.\nFig. 6. Working principle of ULMFiT\n(Image taken from[52])\nULMFiT follows three-step to get the good results on downstream tasks, i.e., (i) General\nLM pre-training, (ii) Target task LM fine-tuning, and (iii) Target task classifier fine-tuning.\nThree training stages of ULMFiT is shown in Fig. 6.\nThe LM pre-training is unsupervised, as the unlabeled text datasets are numerous, the\npre-training can be expanded up as much as possible. It still has, however, a reliance on\ntask-customized models. Therefore, the enhancement is only gradual as looking for a better\nmodel architecture for each role remains non-trivial until the transformer-based models\nthat are discussed below come into being.\n‚Ä¢Transformer-based Pre-trained Language Models\nTransformer [162] has been proven to be more efficient and faster than LSTM or CNN\nfor language modelling, and thus the following advances in this domain will rely on this\narchitecture.\n‚Ä¢GPT (OpenAI Transformer): Generative Pre-Training, GPT [132], is the first Transformer-\nbased pre-trained LM that can effectively manipulate the semantics of words in terms of\n21\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\ncontext. By learning on a massive set of free text corporas, GPT extends the unsupervised\nLM to a much larger scale. Unlike ELMo, GPT uses the decoder of the transformer to model\nthe language as it is an auto-regressive model where the model predicts the next word\naccording to its previous context. GPT has shown good performance on many downstream\ntasks. One drawback of GPT is it‚Äôs uni-directional, i.e., the model is only trained to predict\nthe future left-to-right context. The overall model of GPT is shown in Fig.7.\nFig. 7. Working principle of GPT\n(Image taken from[132])\n‚Ä¢Bidirectional Encoder Representations from Transformers (BERT)\nAs seen in Fig. 8, Bidirectional Encoder Representations from Transformers (BERT) is\na direct descendant of GPT: train a huge LM on free text and then fine-tune individual\ntasks without custom network architectures. BERT [30] is another contextualised word\nrepresentation LM, where the transformer NN uses parallel attention layers rather than\nsequential recurrence [162].\nInstead of the basic language task, BERT is trained with two tasks to encourage bi-\ndirectional prediction and sentence-level understanding. BERT is trained on two unsuper-\nvised tasks: (1) a\" masked language model (MLM), where 15% of the tokens are arbitrarily\nmasked (i.e. replaced with the \"[MASK]\" token), and the model is trained to predict the\nmasked tokens, (2) a \"next sentence prediction\" (NSP) task, where a pair of sentences are\n22\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nFig. 8. Working principle of BERT\n(Image taken from[30])\nprovided to the model and trained to identify when the second one follows the first. This\nsecond task is intended to collect additional information that is long-term or pragmatic.\nBERT is trained in the dataset of Books Corpus [170] and English Wikipedia text passages.\nThere are two BERT pre-trained model available: BERT-Base and BERT-Large. BERT can\nbe used on un-annotated data or fine-tuned on one‚Äôs task-specific data straight from the\npre-trained model. The publicly accessible pre-trained model and fine-tuning code are\navailable online 3.\n‚Ä¢BERT Variants:\nRecent research also explores and strengthens the goal and architecture of BERT. Some of\nthem are briefly discussed below:\n‚Ä¢GPT2: The OpenAI team released a scaled-up variant of GPT in 2019 with GPT2 [132]. It\nincorporates some slight improvements compared to the previous concerning the position\nof layer normalisation and residual relations. Overall, there are four distinct GPT2 variants\nwith the smallest being identical to GPT, the medium one being similar in size to BERT-\nLARGE and the xlarge one being released with 1.5B parameters as the actual GPT2 standard.\n3https://github.com/google-research/bert\n23\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\n‚Ä¢XLNet: XLNet, also known as Generalized Auto-regressive Pre-training for Language\nUnderstanding [169] which proposes a new task to predict the bidirectional context instead\nof the masked Language task in BERT, it is a permutation language in which we make some\npermutations of each sentence so the two contexts will be taken into consideration. In\norder to maintain the position information of the token to be expected, authors employed\ntwo-stream self-attention. XLNET was presented to overcome the issue of pre-training\nfine-tune discrepancy and to include bidirectional contexts simultaneously.\n‚Ä¢RoBERTa: RoBERTa: A Robustly Optimized BERT Pre-training Approach was imple-\nmented in July 2019 [86], it is like a lite version of BERT, but it has fewer parameters and\nbetter performance as it removes the training on the sentence classification task. RoBERTa\nmade following changes to the BERT model: (1) Longer training of the model with larger\nbatches and more data; (2) Eliminating the NSP goal; (3) Longer sequence training; (4)\nDynamically during pre-training, the masked roles will change. All these changes boost\nthe model‚Äôs efficiency and make it efficient with XLNet ‚Äôs previous SOTA results.\n‚Ä¢ALBERT: Despite this success, BERT has some limitations such as BERT has a huge\nnumber of parameters which is the cause for problems like degraded pre-training time,\nmemory management issues and model degradation etc [72]. These issues are very well\naddressed in ALBERT, which is modified based on the architecture of BERT and proposed\nby Lan et al. [ 72]. In scaling pre-trained models, ALBERT implements two-parameter\nreduction methods that lift the essential barriers. (i) factorized embedding parameterization\n- decomposes the big vocabulary embedding matrix to two small matrices, (ii) replaces\nthe NSP loss by SOP loss; and (iii) cross-layer parameter sharing- stops the parameter\nfrom prospering with the network depth. These methods significantly lower the number of\nparameters used when compared with BERT without significantly affecting the performance\nof the model, thus increasing parameter-efficiency. An ALBERT configuration is the same\nas BERT (large) has 18 times less parameters and can be trained about 1.7 times faster.\nALBERT establishes new SOTA results while having fewer parameters compared to BERT.\n‚Ä¢Other Models: Some of the other recently proposed LMs are a cross-lingual LM Pre-\ntraining (XLM) [71] from Facebook enhanced BERT for Cross-lingual LM. Two unsuper-\nvised training objectives that only include monolingual corporations were introduced by\nXLM authors: Causal Language Modeling (CLM) and Masked Language Modeling (MLM)\nand demonstrated that both the CLM and MLM approaches have powerful cross-lingual fea-\ntures that can be used for pre-training models. Similarly, StructBERT [164] implemented\na structural objective word that randomly permits the reconstruction order of 3-grams and\na structural objective sentence that predicts the ordering of two consecutive segments.\n24\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nDistilBERT: [141], a distilled version of BERT, reduces the size of a BERT LM by 40% while\nretaining 97% of its language understanding proficiency and being 60% quicker. Mega-\ntronLM [145], a scaled-up transform-based model, 24 times larger than BERT, training\nmulti-billion parameter LMs using model parallelism. CRTL [64], A Conditional Trans-\nformer Language Model for Controllable Generation, is a 1.63 billion-parameter conditional\ntransformer LM, it is a conditional generative model. Another recently proposed model,\nERNIE [152], Enhanced representation through knowledge integration, used knowledge\nmasking techniques including entity-level masking and phrase-level masking instead of\nrandomly masking tokens. Authors of ERNIE extended their work and presented ERNIE\n2.0 [153] further incorporated more pre-training tasks, such as semantic closeness and\ndiscourse relations. SpanBERT [62] generalized ERNIE to mask random spans, without\nindicating to external knowledge.\nOther prominent LM includes UniLM: [32], which uses three objective functions : (i)\nlanguage modelling (LM), (ii) masked language modelling (MLM), and (iii) sequence-to-\nsequence language modelling (seq2seq LM), for pre-training a transformer model. In order\nto monitor what context the prediction conditions are in, UniLM uses special self-attention\nmasks. ELECTRA [26] proposed more better pre-training techniques as compared to BERT.\nAuthors of ELECTRA replaced some of the input tokens with their plausible substitute\nsamples from small generator network rather than corrupting some positions of the inputs\nwith [MASK]. ELECTRA trains a discriminator to determine whether or not each token\nhas been substituted by a generator in the corrupted input that can be used for fine-\ntuning in downstream tasks. MASS [ 149] is another recently proposed LM. In order to\npre-train sequence-to - sequence models, MASS uses masked sequences and adopts an\nencoder-decoder system and expands the MLM objective. Without pre-training or with\nother pre-training approaches, MASS makes substantial improvements over baselines\non a range of zero / low-resource language generation tasks, including neural machine\ntranslation (MT), text summarization and conversational response generation.\n‚Ä¢Text-to Text Transfer Transformer (T5): [133], unified natural language understand\nand generation by transforming the data to the format of text-to-text and apply the frame-\nwork of an encoder-decoder. In terms of pre-training objectives, architectures, pre-training\ndatasets and transfer techniques, T5 has implemented a novel pre-training corpus and also\nsystematically contrasts previously proposed methods. T5 adopts a text infilling objective,\nmore extended training and multi-task pre-training. For fine-tuning T5 uses the token\nvocabulary of the decoder as the prediction labels.\n‚Ä¢BART:[80]: For pre-training sequence-to-sequence models, BART added additional noise\nfunctions beyond MLM. First, using an arbitrary noise function, the input sequence is\n25\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\ndistorted. Then, a transformer network reconstructs the corrupted input. BART explores a\nbroad range of noise functionality, including token functions. masking, deletion of tokens,\ntext infilling, rotation of documents and shuffling of words. The best performance is attained\nby using both sentence shuffling and text infilling. BART matches RoBERTa‚Äôs performance\non GLUE and SQuAD and attain SOTA results on a number of tasks for generating text.\nTable 3. A comparison of popular Language models.\nLMs Release Date Architecture Pre-Training Task Corpus Used\nWord2Vec Jan-13 FCNN - Google News\nGloVe Oct-14 FCNN - Common Crawl corpus\nFastText Jul-16 FCNN - Wikipedia\nELMo Feb-18 BiLSTM BiLM Wiki-Text-103\nGPT Jun-18 Transformer\nDecoder LM Book-Corpus\nGPT-2 Jun-18 Transformer\nDecoder LM Web-Text\nBERT Oct-18 Transformer\nEncoder MLM & NSP WikiEn+Book-Corpus\nRoBERTa Jul-19 Transformer\nEncoder MLM & NSP Book-Corpus + CC-News\n+Open-Web-Text+ STORIES\nALBERT Sep-19 Transformer\nEncoder MLM+SOP same as BERT\nXLNet Jun-19 Auto-regressive Transformer\nEncoder PLM WikiEn+ Book-Corpus+Giga5\n+ Clue-Web + Common Crawl\nELECTRA 2020 Transformer\nEncoder RTD+MLM same as XLNet\nUniLM 2020 Transformer\nEncoder MLM+NSP WikiEn + Book-Corpus\nMASS 2020 Transformer Seq2Seq MLM *Task-dependent\nBART 2020 Transformer DAE same as RoBERTa\nT5 2020 Transformer Seq2Seq MLM Colossal Clean Crawled Corpus (C4)\nAlthough these models were able to solve context issues but are trained on general domain\ncorpora such as Wikipedia, which limits their applications to specific domains or tasks. To enhance\nthe performance in sub-domains, domain-specific transformer-based models have been proposed.\nSome of the most famous in the biomedical domain are Sci-BERT [9], BioBERT [79] and BioAL-\nBERT [109]. Recently, other domain-specific models such as BERTTweet [117], COVID Twitter\nBERT (CT-BERT) [101] have been trained on datasets from Twitter. Domain-specific models were\nshown to be useful replacements for LMs trained on general corpus for various downstream tasks.\nIn Table3, we present the architecture, Objective function and dataset used for training in LMs.\n3.3 Related work on Word representation methods\nBelow we present some relevant studies where different word representation models have been\nemployed for various text classification tasks.\n26\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nPang et al. [ 120] performed that binary classification task on IMDb dataset and employed\nunigrams, bigram and POS tags as features. For classification, they used SVM, Maximum entropy\nand NB classifiers in their study and found out that best results were achieved with unigrams\nas feature and SVM as classifier. Kwok and Wang [ 70] used n-grams features along with NB\nclassifier tweets classification. These legacy based word representation methods such as n-grams,\nBoW, TF, and TF-IDF have been widely used in different studies for various text classification\ntasks [29, 43, 68, 84]. These traditional methods for text classification are simple, computationally\neconomical. However, their limitations such as ignoring word order, unable to capture semantic\ninformation and high dimensionality etc. restrict their use for efficient text classification tasks.\nLater, representation learning methods of learning text representation directly using neural\nnetwork [27] was adopted, which improved classification results. Word embeddings from continu-\nous word representation models such as Word2Vec and GloVe are the most famous and widely\nused ones among these methods because of low dimensionality of vectors and capture semantic\nrelationships. Word representation models have also been used for sentence-level classification\ntask by averaging word vectors as feature representation which is utilized later on as input for\nsentence-level classification [21].\nWord embeddings which are created based on unigrams and by averaging embeddings are not\nable to capture the issue of syntactic dependencies like \"but\" and \"negations\" can change the\ncomplete meaning of a sentence and long dependencies within a sentence [21]. Sochet et al. [147]\nproposed a recursive neural network which can capture and model long semantic and sentiment\ndependencies of words and sentence at different stages. The disadvantage of this method is that it\ndepends on parsing, which makes it challenging to use on Twitter related text [37]. A paragraph\nrepresentation model solved this issue learns word vectors and does not reply on parsing [75]. Both\nof these, recursive neural and paragraph representation models have assessed on IMDb dataset\nused by Pang et al. [121], and both models improved the classification results obtained by using\nBoW features.\nDeep neural network-based methods have also been used for Text classification tasks. Tang\net al. [158] proposed sentiment specific word representation model, which are achieved from\nemoticons labelled tweet messages with the help of the neural network. Severyn and Moschitti [144]\npresented another neural network-based model where they used Word2Vec to learn embedding.\nTweets are presented as a matrix wherein which columns compare with words, thus retaining the\nposition they appear in a tweet. Emoticons annotated data was utilized to pre-train the weights\nand then trained by hand-annotated from SemEval contestant. The experimental results tell that\npretraining step enables for a better initialization of the networks‚Äô weights and therefore, has a\npositive role in classification results. In another study conducted by Fu et al. [38], Word2Vec was\nemployed to get word representation which was forwarded to the recursive encoder as an input\n27\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\nfor text classification. Ren et al. [135] also used Word2Vec to generate word representations and\nproposed a new model for the Twitter classification task. Lauren et al. [74] presented a different\ndocument representation model where they used the skip-gram algorithm for generating word\nrepresentations for the classification of clinical texts.\nDue to the limitations and restrictions in a few corpora, pre-trained word embeddings are\npreferred by researchers as an input of ML models. Qin et al. [128] used pre-trained Word2Vec\nembeddings and forwarded these word embeddings to CNN. Similarly, Kim [66] utilized pre-trained\nembeddings of Word2Vec and forwarded to CNN neural network, which increased the classification\nresults. Camacho et al. [18] for concept representation in their work. Jianqiang and Xiolin [60]\nhave initialized word embeddings using pre-trained GloVe embeddings in their DCNN model.\nSimilarly, Wallace [163] applied GloVe, and Word2Vec pre-trained word embedding in deep neural\nnetwork-based algorithms and enhanced the classification results. Similarly, a study conducted by\nWang et al. [166], used pre-trained GloVe embeddings as an input to LSTM with attention model\nfor aspect based classification and Liu et al. [85] employed pre-trained word embeddings Word2Vec\nfor recommending idioms in essay writing. Recently, Ilic et al. [56] have used contextual word\nembeddings (ELMo) for word representation for the detection of sarcasm and irony and shown\nthat using ELMo word representations have improved the classification results. The research\ncommunity has made limited efforts for solving the above mention limitations of continuous word\nrepresentation models by proposing different models. For example, for handing OOV words which\nare not seen in the training and they are assigned UNK token and same vector for all words and\ndegrades results if the number of OOV is large. Different methods to handle OOV words have been\nproposed in different studies [31, 48, 127] But still these models does not capture the polysemy\nissues. This issue of words with different meanings (polysemy) is addressed in different models\npresented by the [55, 116]. In recent days, researchers has presented more robust models to handle\nOOV words and polysemy issues [83, 91, 93, 126].\nTo handle domain-specific problems different studies have been conducted where researchers\nused existing knowledge encoded in semantic lexicons to these word embedding to improve the\ndownsides of using the pre-trained embedding which is trained on news data which is usually\ndifferent from the data we use in our tasks. Some of the models presented are proposed in the\nfollowing studies which inject external knowledge in existing word embedding and improves the\nresults [36, 99, 118, 143, 151]. Word embeddings are beneficial in different areas beyond NLP like\nlink prediction, information retrieval and recommendation systems. Ledell et al. [78] proposed a\nmodel which is suitable for many of the applications mentioned above and acted as a baseline.\nNone of the above mentioned is robust enough and fails to integrate sentiment of words in the\nrepresentations and does not work well in domain-specific tasks such as sentiment analysis etc.\n28\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nStudies show that adding sentiment information into conventional word representation models\nimproves performance. To integrate the sentiment information into word embeddings, researchers\nhave proposed different hybrid word representations by changing existing skip-gram model [160].\nTang et al. [159] proposed several hybrid ranking models (HyRank) and developed sentiment\nembeddings based on C&W, which considers context and sentiment polarity of tweets. Similarly,\nseveral other models are presented, which considers context and sentiment polarity of words\nfor sentiment analysis [81, 137, 157]. Yu et al. [81] proposed sentiment embeddings by refining\npre-trained embeddings Re(*) using the intensity score of external knowledge resource. Rezaeinia\net al. [138] proposed improved word vectors (IWV) by combining word embeddings, part of speech\n(POS) and combination of lexicons for sentiment analysis. Recently, Cambria et al. [19] proposed\ncontext embeddings for sentiment analysis by conceptual primitives from text and linked with\ncommonsense concepts and named entities.\nRecent studies have used these contextual and transformer-based LMs in their model in various\nNLP tasks. Furthermore, various studies have been presented which use the domain-specific LMs\nfor different NLP tasks. These hybrid and domain-specific LMs have improved the performance\nand ability to capture complex word attributes, such as semantics, OOV, context, and syntax, into\naccount in various NLP task.\n4 CLASSIFICATION TECHNIQUES\nChoosing an appropriate classifier is one of the main steps in the text classification task. Without\nhaving a comprehensive knowledge of every algorithm, we cannot find out the most effective\nmodel for the text classification task. Out of many ML algorithms used in text classification,\nwe will present some famous and commonly used classification algorithms. These are used for\nsentiment classification tasks such as Na√Øve Bayes (NB), Support vector machine (SVM), logistic\nregression (LR), Tree-based classifiers like decision tree (DT) and random forest (RF) and neural\nnetwork-based (DL) algorithms. Table 4 presents the pros and cons of classification algorithms.\n4.1 ML based classifiers\n‚Ä¢Naive Bayes(NB) classifiers : The Naive Bayes (NB) classifiers are a group of different\nclassification algorithms which are based on Bayes theorem, presented by Thomas Bayes\n[49]. All Naive Bayes algorithms have the same assumption, i.e., each pair of features being\nclassified is independent of others. The NB classification algorithms are widely used for\ninformation retrieval [129] and many text classification tasks [95, 119]. Naive Bayes classifiers\nare called \"Naive\" because it considers that every feature is independent of other features\nin the input. Whereas in reality, words and phrases in the sentences are highly interrelated.\n29\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\nThe meaning and sentiment depend on the position of words in the sentence, which can\nchange if the position is changed.\nNB classifiers are derived from Bayes theorem which states that given the number of doc-\numents (n) to be classified into z classes where ùëß ‚àà{ùë•1,ùë•2,....ùë•ùëß}the predicted label out is\nùë• ‚ààùëã. The Naive Bayes theorem is given as follows:\nùëÉ(ùë•|ùë¶)= ùëÉ(ùë¶|ùë•)ùëÉ(ùë•)\nùëÉ(ùë¶)\nWhere ùë¶denotes a document and ùë• refers to the classes. In simple words, the NB algorithm\nwill take each word in the training data and will calculate the probability of that word being\nclassified. Once the probabilities of every word are calculated, then classifier is read to classify\nnew data by utilizing the prior calculated probabilities during the training phase. Advantages\nof NB classifiers are; they are scalable, more suitable when the dimension of input is high, its\nimplementation is simple, less computationally expensive, works well when less training\ndata is available and can often outperform other classification algorithms. Whereas the\ndisadvantages are; NB classifiers make a solid makes a reliable hypothesis on the shape of\ndata distribution, i.e. any two features are independent given the output class, which gives\nbad results [148, 167]. Another limitation of NB classifiers is due to data scarcity. For any\nvalue of the feature, we have to approximate the likelihood value by a frequentist\n‚Ä¢Support vector machine (SVM) : The support vector machine (SVM) classifiers are one\nof the famous and common used algorithms used for text classification due to its good\nperformance. SVM is a non-probabilistic binary linear classification algorithm which performs\nby plotting the training data in multi-dimensional space. Then SVM categories the classes\nwith a hyper-plane. The algorithm will add a new dimension if the classes can not be separated\nlinearly in multi-dimensional space to separate the classes. This process will continue until a\ntraining data can be categorized into two different classes.\nThe advantage of SVM classifiers is that results are obtained by using SVM are usually better.\nThe disadvantage of SVM algorithms is that it is not easy to choose a suitable kernel, long\ntraining time in case of extensive data and more computational resources are required etc.\n‚Ä¢Logistic Regression (LR) classifier : Logistic regression (LR) is a statistical model and is\none of the earliest techniques used for classification. LR predicts probabilities rather than\nclasses [35, 39] or existence of an event like a win/lose or healthy/sick etc. This can be\nexpanded to model many classes of events like deciding whether an image consists of a\ncat, duck, cow, etc. Every object being identified in the image would be given a probability\nbetween 0 and 1 and the sum adding to one. LR predicts the results based on the set of\nindependent values. However, if the wrong independent values are added, then the model\n30\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nTable 4. Comparison of Classification Algorithms\nClassifiers Pros Cons\nNB\ni) Less computational time.\nii) Easy to understand & implement.\niii) Can easily be trained less data.\ni) Relies strongly on the class features independence\nand does not perform well if the condition is not met.\nii) Issue of zero conditional probability for zero\nfrequency features which makes total probability zero\nSVM\ni) Effective in higher dimension\nii) Can model non linear decision boundary\niii) Robust to the issue of over-fitting\ni) More computational time for large datasets\nii) Kernal selection is difficult\niii) Does not perform well in case of overlapped\nclasses\nLR\ni) Easy and Simple to implement.\nii) Less computationally expensive\niii) Does not need tuning and features\nto be uniformly distributed\ni) Fails in case of non-linear problems\nii) Need large datasets\niii) Predict results on the basis of independent\nvariables\nDT\ni) Interpretable and easy to understand\nii) Less pre-processing required\niii) Fast and almost zero hyper-parameters\nto tuned\ni) High chances of over fitting\nii) Less prediction accuracy as compared to others\niii) Complex calculation in large number of classes\nRF\ni) Fast to train, flexible and gives high\nresults ii) Less variance than single DT\niii) Less pre-processing required\ni) Not easy and simple to interpret\nii) Require more computational resources\niii) Require more time to predict as compared to\nothers\nDL\ni) Fast predictions once training is complete\nii) Works well in case of huge data\niii) Flexible architecture, can be utilized for\nclassification and regression tasks\ni) Require a large amount of data\nii) Computationally expensive and time-consuming\niii) DL based classifiers are like black-box (issue of\nmodel interpretability exists)\nwill not predict good results. It works well in the case of categorical results but fails in the\ncase of continuous results. Also, LR wants that every data point to be independent of all\nothers, but if the findings are interlinked to one another, then classifier will not predict good\nresults.\n‚Ä¢Decision Tree (DT) classifier : Decision tree (DT) was presented by Magerman [87] and\ndeveloped by Quinlan [131]. It is one of the earliest classification models for text and data\nmining and is employed successfully in different areas for classification task [ 98]. The\nmain intuition behind this idea was to create tree-based attributes for data points, but the\nmajor question is which feature could be a parent and which will be a child‚Äôs level. DT\nclassifier design contains a root, decision and leaf nodes which denote dataset, carry out\nthe computation and performs classification respectively. During the training phase, the\nclassifier learns the decision need to be executed to separate labelled categories. To classify\nthe unknown instance, the data is passed through the tree. A particular feature from the input\ntext is matched with the fixed which was known during the training stage. The calculation at\neach decision node compares the chosen features with this fixed feature earlier; the decision\nrelies on whether the feature is more prominent than or less than the fixed which creates\n31\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\ntwo-way division in the tree. The text will eventually go over these decision nodes until it\nreaches the leaf node that describes it assigned class.\nThe advantages of DT classifier are; the amount of hyper-parameters which require tuning\nis nearly zero, easy to describe, can be understood easily by its visualizations whereas the\nsignificant disadvantages of DT classifier are; it is sensitive to a minor change in the data [42]\nand have a probability of overfitting [130], complex computations in case of a large number\nof class labels and have difficulties with-out-of sample prediction.\n‚Ä¢Random Forest (RF) Classifier : Random forest which is also called an ensembles learning\ntechnique for text classification which concentrates on methods to compare the results of\nseveral trained models in line to give better classifier and performance than a single model.\nHo [50] proposed RF classifier, which is simple to understand and also gives better results in\nclassification. RF classifier is composed of the number of DT classifiers where every tree is\ntrained by a bootstrapped subset of the training text. An arbitrary subset of the characteristics\nis selected at every decision node, and the model will only examine part of these features.\nThe primary issue with utilizing the single tree is that it has massive variation so that the\narrangement of the training data and features can impact its results.\nThis classifier is quick to train for textual data but slow in giving predictions when trained\n[7]. Performs good with both categorical and continuous variables, can automatically handle\nmissing values, robust to outliers and less affected by noise whereas training a vast number\nof trees can be computationally expensive, require more training time and utilize much\nmemory.\n4.2 Deep learning based classifiers\nDL based models are motivated by the working of the human brain. It has attained SOTA\nresults in many different areas [108, 134] including NLP. It requires a large number of training\ndata to achieve a semantically good representation of textual data. DL models have attained\nexcellent results compared to models on different classification tasks. Main architectures of\nDL which are commonly used in any text classification task, are briefly discussed below.\n‚Ä¢Recurrent Neural Network (RNN) : RNN is one of the popular neural network-based\nmodel which is widely used for different text classification tasks [ 88, 154]. Previous data\npoints of a sequence are assigned more weights in an RNN model which makes it more useful\nand better for any text, string or sequential data classification. RNN models deal with data\nfrom previous layers/nodes in such a good way which makes them superiors for semantic\nanalysis of a corpus. Gated recurrent unit (GRU) and long short term memory (LSTM) are\nthe most common types of RNNs which are used of text classification.\n32\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nThe one of the drawback of RNN is that they are sensitive to gradient vanishing problem\nand exploding gradient when gradient descent‚Äôs error is back propagated [12].\n‚Ä¢Long Short-Term Memory (LSTM) : LSTM was presented by Hochreiter and Schmidhuber\n[51]. LSTM was presented to address the gradient descent issues of RNN by keeping the long\nterm dependency in a better way as compared to RNNs. It is more effective to overcome the\nissues of vanishing gradient [122]. Even though LSTMs have an architecture like a chain\nwhich is same as RNNs but it uses different gates which handles the volume of information\ncarefully, which is allowed from each node state. The role of each gate and node in a basic\nLSTM cell is explained below.\nùëñùë° = ùúé(ùëäùëñ[ùë•ùë°,‚Ñéùë° ‚àí1]+ùëèùëñ)\nbùê∂ùë° = ùë°ùëéùëõ‚Ñé(ùëäùëê[ùë•ùë°,‚Ñéùë° ‚àí1]+ùëèùëê),\nùëìùë° = ùúé(ùëäùëì [ùë•ùë°,‚Ñéùë°‚àí1]+ùëèùëì),\nùê∂ùë° = ùëñùë° ‚àóbùê∂ùë° +ùëìùë°ùê∂ùë°‚àí1,\nùëúùë° = ùúé(ùëäùëú)[ùë•ùë°,‚Ñéùë°‚àí1]+ùëèùëú,\n‚Ñéùë° = ùëúùë°ùë°ùëéùëõ‚Ñé(ùê∂ùë°),\nWhere ùëñùë°, bùê∂ùë° and ùëìùë° denotes input gate, candid memory cell and forget gate activation\nrespectively. whereas ùê∂ùë° computes new memory cell value and ùëúùë° and‚Ñéùë° represents the final\noutput gate. ùëèis bias vector,ùëä denotes weight matrix and ùë•ùë° denotes input to the memory\ncell at time ùë°.\n‚Ä¢Gated Recurrent Unit (GRU) : GRU is another type of RNNs which are presented by Chung\net al. [25] and Cho et al. [24]. GRU is the simplest form of LSTM architecture. However, it\nincludes two gates and does not contain internal memory which makes it different from\nLSTM. Also, in GRU, a second non-linearity (tanh) is not applied on a network. The working\nof a GRU cell is given below:\nùëßùë° = ùúéùëî(ùëäùëßùë•ùë° +ùëàùëß‚Ñéùë°‚àí1 +ùëèùëß)\nbùëüùë° = ùúéùëî(ùëäùëüùë•ùë° +ùëàùëü‚Ñéùë°‚àí1 +ùëèùëü)\n‚Ñéùë° = ùëßùë°‚Ñéùë°‚àí1 +(1 +ùëßùë°)\nùúé‚Ñé(ùëä‚Ñéùë•ùë° +ùëà‚Ñé(ùëüùë°‚Ñéùë°‚àí1)+ùëè‚Ñé)\nWhere ùëßùë° denotes to the update gate of t, ùë•ùë° represents input vector, W,U and b denotes\nparameter matrices. ùúéùëî which is a activation function can be ReLU or sigmoid, bùëüùë° represents\nreset gate of t,‚Ñéùë° is the output gate of vector t, andùúé‚Ñé denotes the hyperbolic tangent function.\n‚Ä¢Convolutional Neural Networks (CNN) : Another famous architecture of DL is CNN\nwhich is mostly used for hierarchical classification in a DL [57]. CNN was built and used for\nimage classification in early days, but over the period, it has shown excellent results for text\nclassification as well [77]. In image classification, an image tensor is convolved with a set of\nkernels of size ùëëxùëë. The convolution layers in the CNN are known as feature maps which\n33\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\ncan be stacked to have multiple filters. To overcome the computational issue due to the size\nof dimensionality, CNN uses pooling layer to reduce the size from one layer to the other\none. Different pooling methods have been proposed by researchers to decrease the output\nwithout losing features [142].\nMax pooling is the most common pooling technique where maximum elements in the pooling\nwindow are selected. To feed the pooled output from stacked features map to the next one,\nfeatures are flattened into one column. Usually, the last layer of CNNs is fully connected.\nWeights and feature filters are adjusted during the backpropagation step of CNN. The number\nof channels is the major issue with CNN‚Äôs for text classification, which is very few in case\nof image classification. Three channels form RGB. For text, it can be a vast number which\nmakes dimensions very high for text classification [61].\n5 EVALUATION METRICS\nIn terms of evaluating text classification models, accuracy, precision, recall, and F1 score are the\nmost used to assess the text classification methods. Below we briefly discuss each of these.\nConfusion matrix: Confusion matrix is a unique table or a method which is used to present\nthe efficiency of the classification algorithm. In Table 5, we present the confusion matrix. Details\nare given below:\nTable 5. Confusion Matrix\nActual Class\nPredicted Class\nPositive Negative\nPositive True Positive\n(TP)\nFalse Negative\n(FN)\nNegative False Positive\n(FP)\nTrue Negative\n(TN)\n‚Ä¢True Positives (TP) : TP are the accurately predicted positive instances.\n‚Ä¢True Negatives (TN) : TN are the accurately predicted negative instances.\n‚Ä¢False Positives (FP) : FP are wrongly predicted positive instances.\n‚Ä¢False Negatives (FN) : FN are wrongly predicted negative instances.\nOnce we understand the confusion matrix and its parameters, then we can define and understand\nevaluation metrics easily, briefly explained below:\n‚Ä¢Accuracy: Accuracy is the simple ratio of observations predicted correctly to the total\nobservations and is given by\n34\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\nùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ = ùëáùëÉ +ùëáùëÅ\nùëáùëÉ +ùêπùëÉ +ùêπùëÅ +ùëáùëÅ\n‚Ä¢Precision: Precision is the ratio of true positive (TP) observations to the overall positive\npredicted values (TP+FP) and is given by\nùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ = ùëáùëÉ\nùëáùëÉ +ùêπùëÉ\n‚Ä¢Recall: Recall is the ration of true positive (TP) observations to the overall observations\n(TP+FN) and is given by\nùëÖùëíùëêùëéùëôùëô = ùëáùëÉ\nùëáùëÉ +ùêπùëÅ\n‚Ä¢F1 score - Weighted average of Recall and Precision is knowns as F1 score which means\nF1-score consists of both FPs and FNs and is given by\nùêπ1 = 2 ‚àóùëÖùëíùëêùëéùëôùëô ‚àóùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ\nùëÖùëíùëêùëéùëôùëô +ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ\n6 APPLICATIONS\nIn the earliest history of ML and AI, these LMs have been widely used to extract features for text\nclassification tasks, especially in the area of information retrieval systems. However, as techno-\nlogical advances have emerged over time, these have been globally used in many domains such\nas medicine, social sciences, healthcare, psychology, law, engineering, etc. These LMs have been\nused in many different areas of text classification tasks such as Information Retrieval, Sentiment\nAnalysis, Recommender Systems, Summarization, Question Answering, Machine Translation,\nNamed Entity Recognition, and Adversarial Attacks and Defenses etc. in different areas.\n7 CONCLUSION\nIn this survey, we have introduced various algorithms that enable us to capture rich information in\ntext data and represent them as vectors for traditional frameworks. We firstly discussed classical\nmethods of text representation which mostly involved feature engineering followed by DL-based\nmodel. DL techniques have been attracting much attention in these years, which are well known\nespecially for their capability of addressing problems in computer vision and speech recognition\nareas. The great success DL achieved stems from its use of multiple layers of nonlinear processing\nunits for learning multiple layers of feature representations of data; different layers correspond to\ndifferent abstraction levels. DL methods not only shows powerful capability in semantic analysis\napplications on text data but can be successfully used in a number of tasks of text classification and\nnatural language processing. We discussed different word embedding methods such as Word2Vec,\n35\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\nGloVe, FastText and contextual words vectors like Context2Vec, CoVe and ELMO. Finally, in the\nend, we presented current different SOTA models based on the transformer trained on general\ncorpus as well as domain-specific transformer-based LMs. These LMs are still in their developing\nphase, but we expect in-depth learning-based NLP research to be driven in the direction of making\nbetter use of unlabeled data. We expect such a trend to continue with more and better model\ndesigns. We expect to see more NLP applications that employ reinforcement learning methods,\ne.g., dialogue systems. We also expect to see more research on multi-modal learning as, in the real\nworld, language is often grounded on (or correlated with) other signals.\nREFERENCES\n[1] Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca J. Passonneau. 2011. Sentiment Analysis of\nTwitter Data.\n[2] Charu C Aggarwal and Chandan K Reddy. 2013. Data clustering: algorithms and applications . CRC Press.\n[3] Charu C. Aggarwal and ChengXiang Zhai. 2012. A Survey of Text Classification Algorithms. InMining Text Data .\n[4] Edgar Altszyler, Mariano Sigman, and Diego Fern√°ndez Slezak. 2016. Comparative study of LSA vs Word2vec\nembeddings in small corpora: a case study in dreams database. ArXiv abs/1610.01520 (2016).\n[5] Alexandra Balahur. 2013. Sentiment Analysis in Social Media Texts. In WASSA@NAACL-HLT.\n[6] Jorge A. Balazs and Juan D. Vel√°squez. 2016. Opinion Mining and Information Fusion: A survey. Information\nFusion 27 (2016), 95‚Äì110.\n[7] Himani Bansal, Gulshan Shrivastava, Nguyen Nhu, and Loredana STANCIU. 2018. Social Network Analytics for\nContemporary Business Organizations . https://doi.org/10.4018/978-1-5225-5097-6\n[8] Yanwei Bao, Changqin Quan, Lijuan Wang, and Fuji Ren. 2014. The Role of Pre-processing in Twitter Sentiment\nAnalysis. In Intelligent Computing Methodologies , De-Shuang Huang, Kang-Hyun Jo, and Ling Wang (Eds.).\nSpringer International Publishing, Cham, 615‚Äì624.\n[9] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific Text.\narXiv:1903.10676 [cs.CL]\n[10] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives.\nIEEE transactions on pattern analysis and machine intelligence 35, 8 (2013), 1798‚Äì1828.\n[11] Yoshua Bengio, R√©jean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A Neural Probabilistic Language\nModel. J. Mach. Learn. Res. 3 (March 2003), 1137‚Äì1155. http://dl.acm.org/citation.cfm?id=944919.944966\n[12] Y. Bengio, P. Simard, and P. Frasconi. 1994. Learning Long-term Dependencies with Gradient Descent is Difficult.\nTrans. Neur. Netw. 5, 2 (March 1994), 157‚Äì166. https://doi.org/10.1109/72.279181\n[13] Adam Bermingham and Alan Smeaton. 2011. On Using Twitter to Monitor Political Sentiment and Predict Election\nResults. In Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology (SAAIP 2011) . Asian\nFederation of Natural Language Processing, Chiang Mai, Thailand, 2‚Äì10. https://www.aclweb.org/anthology/W11-\n3702\n[14] Marina Boia, Boi Faltings, Claudiu Cristian Musat, and Pearl Pu. 2013. A :) Is Worth a Thousand Words: How\nPeople Attach Sentiment to Emoticons and Words in Tweets. 2013 International Conference on Social Computing\n(2013), 345‚Äì350.\n36\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\n[15] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching Word Vectors with\nSubword Information. arXiv preprint arXiv:1607.04606 (2016).\n[16] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching Word Vectors with\nSubword Information. CoRR abs/1607.04606 (2016). arXiv:1607.04606 http://arxiv.org/abs/1607.04606\n[17] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to Computer\nProgrammer as Woman is to Homemaker? Debiasing Word Embeddings. In Advances in Neural Information\nProcessing Systems 29, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Eds.). Curran Associates, Inc.,\n4349‚Äì4357. http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-\ndebiasing-word-embeddings.pdf\n[18] Jos√É¬© Camacho-Collados, Mohammad Taher Pilehvar, and Roberto Navigli. 2016. Nasari: Integrating explicit\nknowledge and corpus statistics for a multilingual representation of concepts and entities. Artificial Intelligence\n240 (2016), 36 ‚Äì 64. https://doi.org/10.1016/j.artint.2016.07.005\n[19] Erik Cambria, Soujanya Poria, Devamanyu Hazarika, and Kenneth Kwok. 2018. SenticNet 5: Discovering\nConceptual Primitives for Sentiment Analysis by Means of Context Embeddings. In AAAI.\n[20] Xavier Carreras and Llu√≠s M√†rquez. 2001. Boosting Trees for Anti-Spam Email Filtering. CoRR cs.CL/0109015\n(2001). http://arxiv.org/abs/cs.CL/0109015\n[21] Giuseppe Castellucci, Danilo Croce, and Roberto Basili. 2015. Acquiring a Large Scale Polarity Lexicon Through\nUnsupervised Distributional Methods. In Natural Language Processing and Information Systems , Chris Biemann,\nSiegfried Handschuh, Andr√© Freitas, Farid Meziane, and Elisabeth M√©tais (Eds.). Springer International Publishing,\nCham, 73‚Äì86.\n[22] Arda Celebi and Arzucan Ozgur. 2016. Segmenting Hashtags using Automatically Created Training Data.\n[23] Wei James Chen, Xiaoshen Xie, Jiale Wang, Biswajeet Pradhan, Haoyuan Hong, Dieu Tien Bui, Zhao Duan, and\nJianquan Ma. 2017. A comparative study of logistic model tree, random forest, and classification and regression\ntree models for spatial prediction of landslide susceptibility.\n[24] Kyunghyun Cho, Bart van Merri√´nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder‚ÄìDecoder for Statistical Machine\nTranslation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Doha, Qatar, 1724‚Äì1734. https://doi.org/10.3115/v1/D14-1179\n[25] Junyoung Chung, √áaglar G√ºl√ßehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical Evaluation of Gated\nRecurrent Neural Networks on Sequence Modeling. CoRR abs/1412.3555 (2014). arXiv:1412.3555 http://arxiv.org/\nabs/1412.3555\n[26] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-training text\nencoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555 (2020).\n[27] Ronan Collobert and Jason Weston. 2008. A Unified Architecture for Natural Language Processing: Deep Neural\nNetworks with Multitask Learning. In Proceedings of the 25th International Conference on Machine Learning\n(Helsinki, Finland) (ICML ‚Äô08) . ACM, New York, NY, USA, 160‚Äì167. https://doi.org/10.1145/1390156.1390177\n[28] Ronan Collobert, Jason Weston, L√©on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011.\nNatural Language Processing (almost) from Scratch. CoRR abs/1103.0398 (2011). arXiv:1103.0398 http://arxiv.\norg/abs/1103.0398\n[29] Thomas Davidson, Dana Warmsley, Michael W. Macy, and Ingmar Weber. 2017. Automated Hate Speech Detection\nand the Problem of Offensive Language. CoRR abs/1703.04009 (2017). arXiv:1703.04009 http://arxiv.org/abs/1703.\n04009\n37\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\n[30] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. CoRR abs/1810.04805 (2018). arXiv:1810.04805\nhttp://arxiv.org/abs/1810.04805\n[31] Bhuwan Dhingra, Hanxiao Liu, Ruslan Salakhutdinov, and William W. Cohen. 2017. A Comparative Study of\nWord Embeddings for Reading Comprehension. CoRR abs/1703.00993 (2017). arXiv:1703.00993 http://arxiv.org/\nabs/1703.00993\n[32] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen\nHon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances\nin Neural Information Processing Systems . 13063‚Äì13075.\n[33] C√≠cero Nogueira dos Santos and Ma√≠ra A. de C. Gatti. 2014. Deep Convolutional Neural Networks for Sentiment\nAnalysis of Short Texts. In COLING.\n[34] √Ébel Elekes, Adrian Englhardt, Martin Sch√É¬§ler, and Klemens B√É‚ÅÑpilcrowhm. 2018. Toward meaningful notions of\nsimilarity in NLP embedding models. International Journal on Digital Libraries (04 2018). https://doi.org/10.1007/\ns00799-018-0237-y\n[35] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library\nfor Large Linear Classification. J. Mach. Learn. Res. 9 (June 2008), 1871‚Äì1874. http://dl.acm.org/citation.cfm?id=\n1390681.1442794\n[36] Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard H. Hovy, and Noah A. Smith. 2014.\nRetrofitting Word Vectors to Semantic Lexicons. CoRR abs/1411.4166 (2014). arXiv:1411.4166 http://arxiv.org/\nabs/1411.4166\n[37] Jennifer Foster, √ñzlem √áetinoƒülu, Joachim Wagner, Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and Josef van\nGenabith. 2011. From News to Comment: Resources and Benchmarks for Parsing the Language of Web 2.0. In\nProceedings of 5th International Joint Conference on Natural Language Processing . Asian Federation of Natural\nLanguage Processing, Chiang Mai, Thailand, 893‚Äì901. https://www.aclweb.org/anthology/I11-1100\n[38] Xianghua Fu, Wangwang Liu, Yingying Xu, and Laizhong Cui. 2017. Combine HowNet lexicon to train phrase\nrecursive autoencoder for sentence-level sentiment analysis. Neurocomputing 241 (2017), 18‚Äì27.\n[39] Alexander Genkin, David D Lewis, and David Madigan. 2007. Large-Scale Bayesian Logistic Regression\nfor Text Categorization. Technometrics 49, 3 (2007), 291‚Äì304. https://doi.org/10.1198/004017007000000245\narXiv:https://doi.org/10.1198/004017007000000245\n[40] Anastasia Giachanou, Julio Gonzalo, Ida Mele, and Fabio Crestani. 2017. Sentiment Propagation for Predicting\nReputation Polarity. https://doi.org/10.1007/978-3-319-56608-5_18\n[41] Kevin Gimpel, Nathan Schneider, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama,\nJeffrey Flanigan, and Noah A. Smith. [n.d.]. Part-of-Speech Tagging for Twitter: Annotation, Features, and\nExperiments.\n[42] Christian Giovanelli, Xin U. Liu, Seppo Antero Sierla, Valeriy Vyatkin, and Ryutaro Ichise. 2017. Towards an\naggregator that exploits big data to bid on frequency containment reserve market. IECON 2017 - 43rd Annual\nConference of the IEEE Industrial Electronics Society (2017), 7514‚Äì7519.\n[43] Edel Greevy. 2004. Automatic text categorisation of racist webpages.\n[44] Vishal Gupta and Gurpreet Lehal. 2009. A Survey of Text Mining Techniques and Applications. Journal of\nEmerging Technologies in Web Intelligence 1 (08 2009). https://doi.org/10.4304/jetwi.1.1.60-76\n[45] Emma Haddi, Xiaohui Liu, and Yong Shi. 2013. The Role of Text Pre-processing in Sentiment Analysis. In ITQM.\n38\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\n[46] Khaled M. Hammouda and Mohamed S. Kamel. 2004. Efficient Phrase-Based Document Indexing for Web\nDocument Clustering. IEEE Trans. on Knowl. and Data Eng. 16, 10 (Oct. 2004), 1279‚Äì1296. https://doi.org/10.\n1109/TKDE.2004.58\n[47] Yulan He, Chenghua Lin, and Harith Alani. 2011. Automatically Extracting Polarity-Bearing Topics for Cross-\nDomain Sentiment Classification. In Proceedings of the 49th Annual Meeting of the Association for Computational\nLinguistics: Human Language Technologies . Association for Computational Linguistics, Portland, Oregon, USA,\n123‚Äì131. https://www.aclweb.org/anthology/P11-1013\n[48] Aur√©lie Herbelot and Marco Baroni. 2017. High-risk learning: acquiring new word vectors from tiny data. CoRR\nabs/1707.06556 (2017). arXiv:1707.06556 http://arxiv.org/abs/1707.06556\n[49] Bruce M. Hill. 1968. Posterior Distribution of Percentiles: Bayes‚Äô Theorem for Sampling from a Population. J.\nAmer. Statist. Assoc. 63, 322 (1968), 677‚Äì691. http://www.jstor.org/stable/2284038\n[50] Tin Kam Ho. 1998. The Random Subspace Method for Constructing Decision Forests. IEEE Trans. Pattern Anal.\nMach. Intell. 20, 8 (Aug. 1998), 832‚Äì844. https://doi.org/10.1109/34.709601\n[51] Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. 9, 8 (Nov. 1997),\n1735‚Äì1780. https://doi.org/10.1162/neco.1997.9.8.1735\n[52] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\nAssociation for Computational Linguistics, Melbourne, Australia, 328‚Äì339. https://doi.org/10.18653/v1/P18-1031\n[53] Xia Hu and Huan Liu. 2012. Text Analytics in Social Media . Springer US, Boston, MA, 385‚Äì414. https://doi.org/\n10.1007/978-1-4614-3223-4_12\n[54] Ah hwee Tan. 1999. Text Mining: The state of the art and the challenges. In In Proceedings of the PAKDD 1999\nWorkshop on Knowledge Disocovery from Advanced Databases . 65‚Äì70.\n[55] Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2015. SensEmbed: Learning Sense Embeddings\nfor Word and Relational Similarity. InACL.\n[56] Suzana Ilic, Edison Marrese-Taylor, Jorge A. Balazs, and Yutaka Matsuo. 2018. Deep contextualized word\nrepresentations for detecting sarcasm and irony. CoRR abs/1809.09795 (2018). arXiv:1809.09795 http://arxiv.org/\nabs/1809.09795\n[57] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Reading Text in the Wild with\nConvolutional Neural Networks. CoRR abs/1412.1842 (2014). arXiv:1412.1842 http://arxiv.org/abs/1412.1842\n[58] Zhao Jianqiang. 2015. Pre-processing Boosting Twitter Sentiment Analysis? 748‚Äì753. https://doi.org/10.1109/\nSmartCity.2015.158\n[59] Zhao Jianqiang and Gui Xiaolin. 2017. Comparison Research on Text Pre-processing Methods on Twitter Sentiment\nAnalysis. IEEE Access 5 (2017), 2870‚Äì2879.\n[60] Zhao Jianqiang and Gui Xiaolin. 2018. Deep Convolution Neural Networks for Twitter Sentiment Analysis. IEEE\nAccess PP (01 2018), 1‚Äì1. https://doi.org/10.1109/ACCESS.2017.2776930\n[61] Rie Johnson and Tong Zhang. 2014. Effective Use of Word Order for Text Categorization with Convolutional\nNeural Networks. CoRR abs/1412.1058 (2014). arXiv:1412.1058 http://arxiv.org/abs/1412.1058\n[62] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predicting spans. Transactions of the Association for Computational\nLinguistics 8 (2020), 64‚Äì77.\n[63] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of Tricks for Efficient Text\nClassification. CoRR abs/1607.01759 (2016). arXiv:1607.01759 http://arxiv.org/abs/1607.01759\n39\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\n[64] Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. CTRL: A\nConditional Transformer Language Model for Controllable Generation. arXiv:1909.05858 [cs.CL]\n[65] Farhan Hassan Khan, Saba Bashir, and Usman Qamar. 2014. TOM: Twitter Opinion Mining Framework Using\nHybrid Classification Scheme. Decis. Support Syst. 57 (Jan. 2014), 245‚Äì257. https://doi.org/10.1016/j.dss.2013.09.004\n[66] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. arXiv preprint arXiv:1408.5882\n(2014).\n[67] Vandana Korde and C. Namrata Mahender. 2012. TEXT CLASSIFICATION AND CLASSIFIERS: A SURVEY.\n[68] Efthymios Kouloumpis, Theresa Wilson, and Johanna D. Moore. 2011. Twitter Sentiment Analysis: The Good the\nBad and the OMG!. In ICWSM.\n[69] Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura E. Barnes, and Donald E.\nBrown. 2019. Text Classification Algorithms: A Survey. CoRR abs/1904.08067 (2019). arXiv:1904.08067 http:\n//arxiv.org/abs/1904.08067\n[70] Irene Kwok and Yuzhou Wang. 2013. Locate the Hate: Detecting Tweets Against Blacks. In Proceedings of\nthe Twenty-Seventh AAAI Conference on Artificial Intelligence (Bellevue, Washington) (AAAI‚Äô13). AAAI Press,\n1621‚Äì1622. http://dl.acm.org/citation.cfm?id=2891460.2891697\n[71] Guillaume Lample and Alexis Conneau. 2019. Cross-lingual Language Model Pretraining. CoRR abs/1901.07291\n(2019). arXiv:1901.07291 http://arxiv.org/abs/1901.07291\n[72] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.\nALBERT: A Lite BERT for Self-supervised Learning of Language Representations. arXiv:1909.11942 [cs.CL]\n[73] Ray R. Larson. 2010. Introduction to Information Retrieval. J. Am. Soc. Inf. Sci. Technol. 61, 4 (April 2010), 852‚Äì853.\nhttps://doi.org/10.1002/asi.v61:4\n[74] Paula Lauren, Guangzhi Qu, Feng Zhang, and Amaury Lendasse. 2018. Discriminant document embeddings\nwith an extreme learning machine for classifying clinical narratives. Neurocomputing 277 (2018), 129‚Äì138.\nhttps://doi.org/10.1016/j.neucom.2017.01.117\n[75] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. CoRR\nabs/1405.4053 (2014). arXiv:1405.4053 http://arxiv.org/abs/1405.4053\n[76] Yann LeCun, Y Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature 521 (05 2015), 436‚Äì44. https:\n//doi.org/10.1038/nature14539\n[77] Yann Lecun, Leon Bottou, Y Bengio, and Patrick Haffner. 1998. Gradient-Based Learning Applied to Document\nRecognition. Proc. IEEE 86 (12 1998), 2278 ‚Äì 2324. https://doi.org/10.1109/5.726791\n[78] Ledell, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, and Jason Weston. 2017. StarSpace: Embed All\nThe Things! arXiv e-prints , Article arXiv:1709.03856 (Sep 2017), arXiv:1709.03856 pages. arXiv:1709.03856 [cs.CL]\n[79] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. BioBERT: a pre-trained biomedical language representation model for biomedical text mining.\narXiv:1901.08746 [cs.CL]\n[80] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov,\nand Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation,\ntranslation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).\n[81] Liang-Chih, Jin Wang, K. Robert Lai, and Xuejie Zhang. 2018. Refining Word Embeddings Using Intensity\nScores for Sentiment Analysis. IEEE/ACM Trans. Audio, Speech and Lang. Proc. 26, 3 (March 2018), 671‚Äì681.\nhttps://doi.org/10.1109/TASLP.2017.2788182\n40\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\n[82] Chenghua Lin and Yulan He. 2009. Joint Sentiment/Topic Model for Sentiment Analysis. In Proceedings of the\n18th ACM Conference on Information and Knowledge Management (Hong Kong, China) (CIKM ‚Äô09) . ACM, New\nYork, NY, USA, 375‚Äì384. https://doi.org/10.1145/1645953.1646003\n[83] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015. Learning Context-sensitive Word Embeddings with Neural\nTensor Skip-gram Model. In Proceedings of the 24th International Conference on Artificial Intelligence (Buenos\nAires, Argentina) (IJCAI‚Äô15). AAAI Press, 1284‚Äì1290. http://dl.acm.org/citation.cfm?id=2832415.2832428\n[84] Shuhua Liu and Thomas Forss. 2014. Combining N-gram Based Similarity Analysis with Sentiment Analysis\nin Web Content Classification. In Proceedings of the International Joint Conference on Knowledge Discovery,\nKnowledge Engineering and Knowledge Management - Volume 1 (Rome, Italy) (IC3K 2014) . SCITEPRESS - Science\nand Technology Publications, Lda, Portugal, 530‚Äì537. https://doi.org/10.5220/0005170305300537\n[85] Yuanchao Liu, Bingquan Liu, Lili Shan, and Xin Wang. 2018. Modelling context with neural networks for\nrecommending idioms in essay writing. Neurocomputing 275 (2018), 2287 ‚Äì 2293. https://doi.org/10.1016/j.\nneucom.2017.11.005\n[86] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR\nabs/1907.11692 (2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692\n[87] David M. Magerman. 1995. Statistical Decision-tree Models for Parsing. In Proceedings of the 33rd Annual Meeting\non Association for Computational Linguistics (Cambridge, Massachusetts) (ACL ‚Äô95). Association for Computational\nLinguistics, Stroudsburg, PA, USA, 276‚Äì283. https://doi.org/10.3115/981658.981695\n[88] Danilo P. Mandic and Jonathon Chambers. 2001. Recurrent Neural Networks for Prediction: Learning Algo-\nrithms,Architectures and Stability . John Wiley & Sons, Inc., New York, NY, USA.\n[89] Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford Corenlp Natural Language Processing Toolkit.. In ACL (System Demonstrations) . 55‚Äì60.\n[90] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in Translation: Contextualized\nWord Vectors. CoRR abs/1708.00107 (2017). arXiv:1708.00107 http://arxiv.org/abs/1708.00107\n[91] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in Translation: Contextualized\nWord Vectors. InNIPS.\n[92] Yelena Mejova and Padmini Srinivasan. 2011. Exploring Feature Definition and Selection for Sentiment Classifiers.\n[93] Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning Generic Context Embedding with\nBidirectional LSTM. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning .\nAssociation for Computational Linguistics, Berlin, Germany, 51‚Äì61. https://doi.org/10.18653/v1/K16-1006\n[94] Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning Generic Context Embedding with\nBidirectional LSTM. In CoNLL.\n[95] Prem Melville, Wojciech Gryc, and Richard D. Lawrence. 2009. Sentiment Analysis of Blogs by Combining\nLexical Knowledge with Text Classification. In Proceedings of the 15th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining (Paris, France) (KDD ‚Äô09) . ACM, New York, NY, USA, 1275‚Äì1284.\nhttps://doi.org/10.1145/1557019.1557156\n[96] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of\nWords and Phrases and Their Compositionality. InAdvances in neural information processing systems . 3111‚Äì9.\n[97] Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the State-of-the-Art in\nSentiment Analysis of Tweets. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume\n2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013) (Atlanta, Georgia,\n41\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\nUSA). Association for Computational Linguistics, 321‚Äì327. http://aclweb.org/anthology/S13-2053\n[98] James N. Morgan and John A. Sonquist. 1963. Problems in the Analysis of Survey Data, and a Proposal. J. Amer.\nStatist. Assoc. 58, 302 (1963), 415‚Äì434. http://www.jstor.org/stable/2283276\n[99] Nikola Mrksic, Ivan Vulic, Diarmuid √ì S√©aghdha, Ira Leviant, Roi Reichart, Milica Gasic, Anna Korhonen, and\nSteve J. Young. 2017. Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and\nCross-Lingual Constraints. CoRR abs/1706.00374 (2017). arXiv:1706.00374 http://arxiv.org/abs/1706.00374\n[100] T. Mullen and R. Malouf. 2006. A preliminary investigation into sentiment analysis of informal political discourse.\nAAAI Spring Symposium - Technical Report SS-06-03 (2006), 159‚Äì162. https://www.scopus.com/inward/record.\nuri?eid=2-s2.0-33747172751&partnerID=40&md5=6b12793b70eae006102989ed6d398fcb cited By 68.\n[101] Martin M√ºller, Marcel Salath√©, and Per E Kummervold. 2020. COVID-Twitter-BERT: A Natural Language\nProcessing Model to Analyse COVID-19 Content on Twitter. arXiv preprint arXiv:2005.07503 (2020).\n[102] Marwa Naili, Anja Habacha Chaibi, and Henda Hajjami Ben Ghezala. 2017. Comparative study of word embedding\nmethods in topic segmentation. Procedia computer science 112 (2017), 340‚Äì349.\n[103] Vivek Narayanan, Ishan Arora, and Arjun Bhatia. 2013. Fast and accurate sentiment classification using an\nenhanced Naive Bayes model. CoRR abs/1305.6143 (2013). arXiv:1305.6143 http://arxiv.org/abs/1305.6143\n[104] Usman Naseem. 2020. Hybrid Words Representation for the classification of low quality text . Ph.D. Dissertation.\n[105] U Naseem, SK Khan, M Farasat, and F Ali. 2019. Abusive Language Detection: A Comprehensive Review. Indian\nJournal of Science and Technology 12, 45 (2019), 1‚Äì13.\n[106] Usman Naseem, Shah Khalid Khan, Madiha Farasat, and Farasat Ali. 2019. Abusive Language Detection: A\nComprehensive Review. Indian Journal of Science and Technology 12 (2019). http://www.indjst.org/index.php/\nindjst/article/view/146538\n[107] Usman Naseem, Shah Khalid Khan, Imran Razzak, and Ibrahim A. Hameed. 2019. Hybrid Words Representation\nfor Airlines Sentiment Analysis. In AI 2019: Advances in Artificial Intelligence , Jixue Liu and James Bailey (Eds.).\nSpringer International Publishing, Cham, 381‚Äì392.\n[108] Usman Naseem, Matloob Khushi, Shah Khalid Khan, Nazar Waheed, Adnan Mir, Atika Qazi, Bandar Alshammari,\nand Simon K. Poon. 2020. Diabetic Retinopathy Detection Using Multi-layer Neural Networks and Split Attention\nwith Focal Loss. In International Conference on Neural Information Processing . Springer, 1‚Äì12.\n[109] Usman Naseem, Matloob Khushi, Vinay Reddy, Sakthivel Rajendran, Imran Razzak, and Jinman Kim. 2020.\nBioALBERT: A Simple and Effective Pre-trained Language Model for Biomedical Named Entity Recognition.\narXiv preprint arXiv:2009.09223 (2020).\n[110] Usman Naseem and Katarzyna Musial. 2019. Dice: Deep intelligent contextual embedding for twitter sentiment\nanalysis. In 2019 International Conference on Document Analysis and Recognition (ICDAR) . IEEE, 953‚Äì958.\n[111] Usman Naseem and Katarzyna Musial. 2019. DICE: Deep Intelligent Contextual Embedding for Twitter Sentiment\nAnalysis. 2019 15th International Conference on Document Analysis and Recognition (ICDAR) (2019), 1‚Äì5.\n[112] Usman Naseem, Katarzyna Musial, Peter Eklund, and Mukesh Prasad. 2020. Biomedical Named-Entity Recogni-\ntion by Hierarchically Fusing BioBERT Representations and Deep Contextual-Level Word-Embedding. In 2020\nInternational Joint Conference on Neural Networks (IJCNN) . IEEE, 1‚Äì8.\n[113] Usman Naseem, Imran Razzak, Peter Eklund, and Katarzyna Musial. 2020. Towards Improved Deep Contextual\nEmbedding for the identification of Irony and Sarcasm. In 2020 International Joint Conference on Neural Networks\n(IJCNN). IEEE, 1‚Äì7.\n[114] Usman Naseem, Imran Razzak, and Ibrahim A Hameed. 2019. Deep Context-Aware Embedding for Abusive and\nHate Speech detection on Twitter. Aust. J. Intell. Inf. Process. Syst. 15, 3 (2019), 69‚Äì76.\n42\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\n[115] Usman Naseem, Imran Razzak, Katarzyna Musial, and Muhammad Imran. 2020. Transformer based deep intelligent\ncontextual embedding for twitter sentiment analysis. Future Generation Computer Systems 113 (2020), 58‚Äì69.\n[116] Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2015. Efficient Non-parametric\nEstimation of Multiple Embeddings per Word in Vector Space. CoRR abs/1504.06654 (2015). arXiv:1504.06654\nhttp://arxiv.org/abs/1504.06654\n[117] Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. 2020. BERTweet: A pre-trained language model for English\nTweets. arXiv preprint arXiv:2005.10200 (2020).\n[118] Thomas Niebler, Martin Becker, Christian P√∂litz, and Andreas Hotho. 2017. Learning Semantic Relatedness From\nHuman Feedback Using Metric Learning. CoRR abs/1705.07425 (2017). arXiv:1705.07425 http://arxiv.org/abs/\n1705.07425\n[119] Alexander Pak and Patrick Paroubek. 2010. Twitter as a Corpus for Sentiment Analysis and Opinion Mining. In\nLREC.\n[120] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs Up?: Sentiment Classification Using\nMachine Learning Techniques. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language\nProcessing - Volume 10 (EMNLP ‚Äô02) . Association for Computational Linguistics, Stroudsburg, PA, USA, 79‚Äì86.\nhttps://doi.org/10.3115/1118693.1118704\n[121] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment Classification Using Machine\nLearning Techniques. In The Conference on Empirical Methods on Natural Language Processing . Association for\nComputational Linguistics, 79‚Äì86.\n[122] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the Difficulty of Training Recurrent Neural\nNetworks. In Proceedings of the 30th International Conference on International Conference on Machine Learning -\nVolume 28 (Atlanta, GA, USA) (ICML‚Äô13). JMLR.org, III‚Äì1310‚ÄìIII‚Äì1318. http://dl.acm.org/citation.cfm?id=3042817.\n3043083\n[123] Cristian Patriche, P√É¬Ærn√Ñ∆íu Gabriel, Adrian Grozavu, and Bogdan Ro√Ö≈∏ca. 2016. A Comparative Analysis of\nBinary Logistic Regression and Analytical Hierarchy Process for Landslide Susceptibility Assessment in the\nDobrov River Basin, Romania. Pedosphere 26 (06 2016), 335‚Äì350. https://doi.org/10.1016/S1002-0160(15)60047-9\n[124] Jacob Perkins. 2010. Python Text Processing with NLTK 2.0 Cookbook . Packt Publishing.\n[125] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n2018. Deep Contextualized Word Representations. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\n(New Orleans, Louisiana). Association for Computational Linguistics, 2227‚Äì2237. https://doi.org/10.18653/v1/N18-\n1202\n[126] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word representations. CoRR abs/1802.05365 (2018). arXiv:1802.05365\nhttp://arxiv.org/abs/1802.05365\n[127] Yuval Pinter, Robert Guthrie, and Jacob Eisenstein. 2017. Mimicking Word Embeddings using Subword RNNs.\nCoRR abs/1707.06961 (2017). arXiv:1707.06961 http://arxiv.org/abs/1707.06961\n[128] Pengda Qin, Weiran Xu, and Jun Guo. 2016. An Empirical Convolutional Neural Network Approach for Semantic\nRelation Classification. Neurocomput. 190, C (May 2016), 1‚Äì9. https://doi.org/10.1016/j.neucom.2015.12.091\n[129] Zhaowei Qu, Xiaomin Song, Shuqiang Zheng, Xiaoru Wang, Xiaohui Song, and Zuquan Li. 2018. Improved Bayes\nMethod Based on TF-IDF Feature and Grade Factor Feature for Chinese Information Classification. 2018 IEEE\nInternational Conference on Big Data and Smart Computing (BigComp) (2018), 677‚Äì680.\n43\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\n[130] J.R. Quinlan. 1987. Simplifying decision trees. International Journal of Man-Machine Studies 27, 3 (1987), 221 ‚Äì\n234. https://doi.org/10.1016/S0020-7373(87)80053-6\n[131] J. R. Quinlan. 1986. Induction of Decision Trees.Mach. Learn. 1, 1 (March 1986), 81‚Äì106. https://doi.org/10.1023/A:\n1022643204877\n[132] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2018. Language Models\nare Unsupervised Multitask Learners. (2018). https://d4mucfpksywv.cloudfront.net/better-language-models/\nlanguage-models.pdf\n[133] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv\npreprint arXiv:1910.10683 (2019).\n[134] Arshia Rehman, Saeeda Naz, Usman Naseem, Imran Razzak, and Ibrahim A Hameed. 2019. Deep AutoEncoder-\nDecoder Framework for Semantic Segmentation of Brain Tumor. Aust. J. Intell. Inf. Process. Syst. 15, 3 (2019),\n53‚Äì60.\n[135] Yafeng Ren, Yue Zhang, Meishan Zhang, and Donghong Ji. 2016. Context-sensitive Twitter Sentiment Classification\nUsing Neural Network. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (Phoenix, Arizona)\n(AAAI‚Äô16). AAAI Press, 215‚Äì221. http://dl.acm.org/citation.cfm?id=3015812.3015844\n[136] Jack Reuter, Jhonata Pereira-Martins, and Jugal Kalita. 2016. Segmenting Twitter Hashtags. International Journal\non Natural Language Computing 5 (08 2016), 23‚Äì36. https://doi.org/10.5121/ijnlc.2016.5402\n[137] Seyed Mahdi Rezaeinia, Ali Ghodsi, and Rouhollah Rahmani. 2017. Improving the Accuracy of Pre-trained Word\nEmbeddings for Sentiment Analysis.CoRR abs/1711.08609 (2017). arXiv:1711.08609 http://arxiv.org/abs/1711.08609\n[138] Seyed Mahdi Rezaeinia, Ali Ghodsi, and Rouhollah Rahmani. 2017. Improving the Accuracy of Pre-trained Word\nEmbeddings for Sentiment Analysis.CoRR abs/1711.08609 (2017). arXiv:1711.08609 http://arxiv.org/abs/1711.08609\n[139] Hassan Saif, Marta Fernandez Andres, Yulan He, and Harith Alani. 2013. Evaluation Datasets for Twitter Sentiment\nAnalysis: A survey and a new dataset, the STS-Gold. In ESSEM@AI*IA.\n[140] Mohammad Arshi Saloot, Norisma Idris, Nor Liyana Mohd Shuib, Ram Gopal Raj, and AiTi Aw. 2015. Toward\nTweets Normalization Using Maximum Entropy. In NUT@IJCNLP.\n[141] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT:\nsmaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).\n[142] Dominik Scherer, Andreas M√ºller, and Sven Behnke. 2010. Evaluation of Pooling Operations in Convolutional\nArchitectures for Object Recognition. In Artificial Neural Networks ‚Äì ICANN 2010 , Konstantinos Diamantaras,\nWlodek Duch, and Lazaros S. Iliadis (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 92‚Äì101.\n[143] Seungil, David Ding, Kevin Canini, Jan Pfeifer, and Maya Gupta. 2017. Deep Lattice Networks and Par-\ntial Monotonic Functions. arXiv e-prints , Article arXiv:1709.06680 (Sep 2017), arXiv:1709.06680 pages.\narXiv:1709.06680 [stat.ML]\n[144] Aliaksei Severyn and Alessandro Moschitti. 2015. Twitter Sentiment Analysis with Deep Convolutional\nNeural Networks. In Proceedings of the 38th International ACM SIGIR Conference on Research and Develop-\nment in Information Retrieval (Santiago, Chile) (SIGIR ‚Äô15) . ACM, New York, NY, USA, 959‚Äì962. https:\n//doi.org/10.1145/2766462.2767830\n[145] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.\narXiv:1909.08053 [cs.CL]\n[146] Tajinder Singh and Madhu Kumari. 2016. Role of Text Pre-processing in Twitter Sentiment Analysis.\n44\nA Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation\nLanguage Models Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY\n[147] R Socher, A Perelygin, J.Y. Wu, J Chuang, C.D. Manning, A.Y. Ng, and C Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank. EMNLP 1631 (01 2013), 1631‚Äì1642.\n[148] Saeid Soheily-Khah, Pierre-Fran√É¬ßois Marteau, and Nicolas B√©chet. 2017. Intrusion detection in network systems\nthrough hybrid supervised and unsupervised mining process- a detailed case study on the ISCX benchmark\ndataset -.\n[149] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. Mass: Masked sequence to sequence pre-training\nfor language generation. arXiv preprint arXiv:1905.02450 (2019).\n[150] Karen Sparck Jones. 1988. Document Retrieval Systems. Taylor Graham Publishing, London, UK, UK, Chapter A\nStatistical Interpretation of Term Specificity and Its Application in Retrieval, 132‚Äì142. http://dl.acm.org/citation.\ncfm?id=106765.106782\n[151] Robyn Speer, Joshua Chin, and Catherine Havasi. 2016. ConceptNet 5.5: An Open Multilingual Graph of General\nKnowledge. CoRR abs/1612.03975 (2016). arXiv:1612.03975 http://arxiv.org/abs/1612.03975\n[152] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and\nHua Wu. 2019. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223\n(2019).\n[153] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020. ERNIE 2.0: A\nContinual Pre-Training Framework for Language Understanding.. In AAAI. 8968‚Äì8975.\n[154] Ilya Sutskever, James Martens, and Geoffrey Hinton. 2011. Generating Text with Recurrent Neural Networks.\nIn Proceedings of the 28th International Conference on International Conference on Machine Learning (Bellevue,\nWashington, USA) (ICML‚Äô11). Omnipress, USA, 1017‚Äì1024. http://dl.acm.org/citation.cfm?id=3104482.3104610\n[155] Jared Suttles and Nancy Ide. 2013. Distant Supervision for Emotion Classification with Discrete Binary Values.\nIn Proceedings of the 14th International Conference on Computational Linguistics and Intelligent Text Processing -\nVolume 2 (Samos, Greece) (CICLing‚Äô13). Springer-Verlag, Berlin, Heidelberg, 121‚Äì136. https://doi.org/10.1007/978-\n3-642-37256-8_11\n[156] Symeon Symeonidis, Dimitrios Effrosynidis, and Avi Arampatzis. 2018. A comparative evaluation of pre-processing\ntechniques and their interactions for twitter sentiment analysis. Expert Systems with Applications 110 (2018), 298\n‚Äì 310. https://doi.org/10.1016/j.eswa.2018.06.022\n[157] Duyu Tang, Bing Qin, Furu Wei, Li Dong, Ting Liu, and Ming Zhou. 2015. A Joint Segmentation and Classification\nFramework for Sentence Level Sentiment Classification. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing 23, 11 (2015), 1750‚Äì61.\n[158] Duyu Tang, Furu Wei, Bing Qin, Nan Yang, Ting Liu, and Ming Zhou. 2016. Sentiment Embeddings with\nApplications to Sentiment Analysis. IEEE Transactions on Knowledge and Data Engineering 28, 2 (2016), 496‚Äì509.\n[159] Duyu Tang, Furu Wei, Bing Qin, Nan Yang, Ting Liu, and Ming Zhou. 2016. Sentiment Embeddings with\nApplications to Sentiment Analysis. IEEE Trans. on Knowl. and Data Eng. 28, 2 (Feb. 2016), 496‚Äì509. https:\n//doi.org/10.1109/TKDE.2015.2489653\n[160] Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning Sentiment-Specific Word\nEmbedding for Twitter Sentiment Classification. In Proceedings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Baltimore,\nMaryland, 1555‚Äì1565. https://doi.org/10.3115/v1/P14-1146\n[161] Alper Kursat Uysal and Serkan G√ºnal. 2014. The impact of preprocessing on text classification. Inf. Process.\nManage. 50 (2014), 104‚Äì112.\n45\nWoodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Naseem U, et al.\n[162] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is All You Need. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems (Long Beach, California, USA) (NIPS‚Äô17). Curran Associates Inc., USA, 6000‚Äì6010.\nhttp://dl.acm.org/citation.cfm?id=3295222.3295349\n[163] Byron Wallace. 2017. A Sensitivity Analysis of (and Practitioners‚Äô Guide to) Convolutional Neural Networks for\nSentence Classification. In Proceedings of the Eighth International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers) . Asian Federation of Natural Language Processing, Taipei, Taiwan, 253‚Äì263.\n[164] Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, and Luo Si. 2019. StructBERT:\nIncorporating Language Structures into Pre-training for Deep Language Understanding. arXiv:1908.04577 [cs.CL]\n[165] Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan, and Amit P. Sheth. 2012. Harnessing Twitter \"Big Data\"\nfor Automatic Emotion Identification. 2012 International Conference on Privacy, Security, Risk and Trust and 2012\nInternational Confernece on Social Computing (2012), 587‚Äì592.\n[166] Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016. Attention-based LSTM for Aspect-level Senti-\nment Classification. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Austin, Texas, 606‚Äì615. https://doi.org/10.18653/v1/D16-1058\n[167] Yuyang Wang, Roni Khardon, and Pavlos Protopapas. 2012. NONPARAMETRIC BAYESIAN ESTIMATION OF\nPERIODIC LIGHT CURVES. The Astrophysical Journal 756, 1 (aug 2012), 67. https://doi.org/10.1088/0004-\n637x/756/1/67\n[168] Ikuya Yamada, Hideaki Takeda, and Yoshiyasu Takefuji. 2015. Enhancing Named Entity Recognition in Twitter\nMessages Using Entity Linking. In NUT@IJCNLP.\n[169] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. XLNet: Gen-\neralized Autoregressive Pretraining for Language Understanding. CoRR abs/1906.08237 (2019). arXiv:1906.08237\nhttp://arxiv.org/abs/1906.08237\n[170] Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and\nReading Books. CoRR abs/1506.06724 (2015). arXiv:1506.06724 http://arxiv.org/abs/1506.06724\n46",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7821998000144958
    },
    {
      "name": "Natural language processing",
      "score": 0.7673234939575195
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.7590764760971069
    },
    {
      "name": "Representation (politics)",
      "score": 0.7300130724906921
    },
    {
      "name": "Word (group theory)",
      "score": 0.7019208073616028
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6811245083808899
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5600228309631348
    },
    {
      "name": "Language model",
      "score": 0.5209261775016785
    },
    {
      "name": "Linguistics",
      "score": 0.241524338722229
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I129604602",
      "name": "The University of Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I149704539",
      "name": "Deakin University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I82951845",
      "name": "RMIT University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I1321014770",
      "name": "Association for Computing Machinery",
      "country": "US"
    }
  ],
  "cited_by": 12
}