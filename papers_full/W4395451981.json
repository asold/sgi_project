{
  "title": "Improvements in viral gene annotation using large language models and soft alignments",
  "url": "https://openalex.org/W4395451981",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2224464268",
      "name": "William L. Harrigan",
      "affiliations": [
        "University of Hawaiʻi at Mānoa"
      ]
    },
    {
      "id": "https://openalex.org/A2328214636",
      "name": "Barbra D. Ferrell",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A1984994463",
      "name": "K. Eric Wommack",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A1990635859",
      "name": "Shawn W. Polson",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A5095888936",
      "name": "Zachary D. Schreiber",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A372410638",
      "name": "Mahdi Belcaid",
      "affiliations": [
        "University of Hawaiʻi at Mānoa"
      ]
    },
    {
      "id": "https://openalex.org/A2224464268",
      "name": "William L. Harrigan",
      "affiliations": [
        "University of Hawaiʻi at Mānoa",
        "University of Hawaii System"
      ]
    },
    {
      "id": "https://openalex.org/A2328214636",
      "name": "Barbra D. Ferrell",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A1984994463",
      "name": "K. Eric Wommack",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A1990635859",
      "name": "Shawn W. Polson",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A5095888936",
      "name": "Zachary D. Schreiber",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A372410638",
      "name": "Mahdi Belcaid",
      "affiliations": [
        "University of Hawaiʻi at Mānoa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2060797027",
    "https://openalex.org/W2117486996",
    "https://openalex.org/W1966912927",
    "https://openalex.org/W2148495628",
    "https://openalex.org/W2046892081",
    "https://openalex.org/W2055043387",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W3118936575",
    "https://openalex.org/W2945297971",
    "https://openalex.org/W3023746197",
    "https://openalex.org/W4280592348",
    "https://openalex.org/W1505893511",
    "https://openalex.org/W1486817521",
    "https://openalex.org/W3007807538",
    "https://openalex.org/W3045012301",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4313430582",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W2252523470",
    "https://openalex.org/W2001668098",
    "https://openalex.org/W2781086737",
    "https://openalex.org/W3175996595",
    "https://openalex.org/W2951765074",
    "https://openalex.org/W4290989027",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4315641887",
    "https://openalex.org/W3202237470",
    "https://openalex.org/W4306404289",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W6729835178",
    "https://openalex.org/W1999200033",
    "https://openalex.org/W4210400672",
    "https://openalex.org/W2076048958",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W1967320885",
    "https://openalex.org/W3155977246",
    "https://openalex.org/W6601298480",
    "https://openalex.org/W4392686654",
    "https://openalex.org/W2141885858",
    "https://openalex.org/W634418684"
  ],
  "abstract": "Abstract Background The annotation of protein sequences in public databases has long posed a challenge in molecular biology. This issue is particularly acute for viral proteins, which demonstrate limited homology to known proteins when using alignment, k-mer, or profile-based homology search approaches. A novel methodology employing Large Language Models (LLMs) addresses this methodological challenge by annotating protein sequences based on embeddings. Results Central to our contribution is the soft alignment algorithm, drawing from traditional protein alignment but leveraging embedding similarity at the amino acid level to bypass the need for conventional scoring matrices. This method not only surpasses pooled embedding-based models in efficiency but also in interpretability, enabling users to easily trace homologous amino acids and delve deeper into the alignments. Far from being a black box, our approach provides transparent, BLAST-like alignment visualizations, combining traditional biological research with AI advancements to elevate protein annotation through embedding-based analysis while ensuring interpretability. Tests using the Virus Orthologous Groups and ViralZone protein databases indicated that the novel soft alignment approach recognized and annotated sequences that both blastp and pooling-based methods, which are commonly used for sequence annotation, failed to detect. Conclusion The embeddings approach shows the great potential of LLMs for enhancing protein sequence annotation, especially in viral genomics. These findings present a promising avenue for more efficient and accurate protein function inference in molecular biology.",
  "full_text": "Improvements in viral gene annotation \nusing large language models and soft \nalignments\nWilliam L. Harrigan1†, Barbra D. Ferrell2†, K. Eric Wommack2, Shawn W. Polson3, Zachary D. Schreiber2 and \nMahdi Belcaid4* \nIntroduction\nThe association between amino acid sequences and corresponding protein function is a \nlong-standing problem in molecular biology [1, 2]. Despite substantial efforts towards \naddressing this issue, a significant number of public database sequences lack functional \nannotations [3]. The annotation gap is notably problematic for viruses due to their high \nAbstract \nBackground: The annotation of protein sequences in public databases has long \nposed a challenge in molecular biology. This issue is particularly acute for viral pro-\nteins, which demonstrate limited homology to known proteins when using alignment, \nk-mer, or profile-based homology search approaches. A novel methodology employing \nLarge Language Models (LLMs) addresses this methodological challenge by annotating \nprotein sequences based on embeddings.\nResults: Central to our contribution is the soft alignment algorithm, drawing from tra-\nditional protein alignment but leveraging embedding similarity at the amino acid level \nto bypass the need for conventional scoring matrices. This method not only surpasses \npooled embedding-based models in efficiency but also in interpretability, enabling \nusers to easily trace homologous amino acids and delve deeper into the alignments. \nFar from being a black box, our approach provides transparent, BLAST-like align-\nment visualizations, combining traditional biological research with AI advancements \nto elevate protein annotation through embedding-based analysis while ensuring \ninterpretability. Tests using the Virus Orthologous Groups and ViralZone protein data-\nbases indicated that the novel soft alignment approach recognized and annotated \nsequences that both blastp and pooling-based methods, which are commonly used \nfor sequence annotation, failed to detect.\nConclusion: The embeddings approach shows the great potential of LLMs \nfor enhancing protein sequence annotation, especially in viral genomics. These find-\nings present a promising avenue for more efficient and accurate protein function infer-\nence in molecular biology.\nKeywords: Large language models, Protein homology, Viruses, Alignments\nOpen Access\n© The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdo-\nmain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nHarrigan et al. BMC Bioinformatics          (2024) 25:165  \nhttps://doi.org/10.1186/s12859-024-05779-6\nBMC Bioinformatics\n†William L. Harrigan and Barbra \nD. Ferrell contributed equally to \nthis work.\n*Correspondence:   \nmahdi@hawaii.edu\n1 Hawai’i Institute of Marine \nBiology, University of Hawai’i \nat Mānoa, Honolulu, HI 96822, \nUSA\n2 Department of Plant & Soil \nSciences, University of Delaware, \nNewark, DE 19713, USA\n3 Department of Computer \nand Information Sciences, \nUniversity of Delaware, Newark, \nDE 19713, USA\n4 Department of Computer \nScience, University of Hawai’i \nat Mānoa, Honolulu, HI 96822, \nUSA\nPage 2 of 19Harrigan et al. BMC Bioinformatics          (2024) 25:165 \nmutation rates and vast sequence diversity [4]. In sequence-based protein annotation, \namino acid sequences are compared with known protein sequences from public data -\nbases using homology-based (e.g. BLAST) or profile-based (e.g. Hmmer) approaches.\nHomology-based predictions infer the function of a query sequence based on its best \nalignment with one or more known functional sequences, utilizing a predetermined sub-\nstitution matrix that measures amino acid similarity. The resulting alignments can high -\nlight conserved regions thus revealing structural and functional relationships between \nsequences. Substitution matrices used in judging alignment accuracy, are derived from \nempirical biochemical data and do not consider the context in which amino acids occur. \nFor instance, when employing the PAM250 substitution matrix, aligning the amino acids \nasparagine (N) and tryptophan (W) results in a negative alignment score of − 5, regard -\nless of the broader structural context where these amino acids occur. As such, much care \nand effort has gone into constructing substitution matrices, which assume unbiased \namino acid compositions. Currently, all protein database search methods use standard -\nized amino acid substitution matrices for scoring and assessing the statistical signifi -\ncance of sequence alignments [5]. BLAST remains the predominant homology-based \ntool that leverages substitution matrices for sequence annotation, with its two primary \npublications cited 102,958 and 82,934 times, as of July 2023 [6, 7], respectively.\nRecent studies have also explored the use of sophisticated neural networks to learn to \npredict protein function [8–10]. The advanced computational capabilities of these net -\nworks allow researchers to uncover complex patterns and relationships, enhancing the \naccuracy and depth of protein function predictions. For instance, the VPF-PLM model \nuses protein embeddings and a feedforward neural network to categorize input proteins \ninto one of nine predefined PHROG family categories. However, these and similar neural \nnetwork-based methods face limitations in their adaptability to predefined classes-such \nas the nine categories in this example. Adapting these methods to new classes neces -\nsitates sourcing new training data and the reengineering and retraining of the neural \nnetwork. Specifically, VPF-PLM employs pooled embeddings, which average data across \nsequences and may mask regions indicative of distinct characteristics. This averaging out \ncan lead to the oversight of localized sequence features. Furthermore, neural network-\nbased classification methods offer limited insight into their decision-making processes, \nmaking them less adaptable than more straightforward database search methods such \nas BLAST, which provides clear alignments for user interpretation and facilitates easy \ndatabase updates.\nProfile-based annotation prediction identifies protein function by creating a profile \nfrom multiple sequence alignments (MSAs). These methods offer higher sensitivity by \nconsidering multiple pieces of evidence rather than a single pairwise alignment, cap -\nturing expected amino acid variability in a sequence. Despite increased sensitivity, pro -\nfile-based sequence alignment is challenging because building a reliable MSA for rare \nsequences can be difficult or even impossible due to the limited availability of similar \nsequences in databases. Additionally, profile-based alignments are highly dependent \non MSA quality, thus errors or biases in the MSA impact the alignment results. Conse -\nquently, single-sequence homology-based methods, such as BLAST, remain the de-facto \nstandard in sequence-based protein functional annotation.\nPage 3 of 19\nHarrigan et al. BMC Bioinformatics          (2024) 25:165 \n \nProtein–protein interactions represent another class of solutions used in protein func-\ntion annotation and have been extensively studied [11]. Protein interaction networks \nprovide a framework for understanding the relationships between genes and phenotypes, \nas well as the mechanistic basis for cellular functions [12]. Early methods for assigning \nfunctions to unannotated proteins relied on the frequencies of interaction partners hav -\ning known functions [13]. More recently, techniques incorporating graph embedding \nand machine learning have been developed [14, 15]. These modern approaches utilize \nlow-dimensional representations of interaction networks for capturing essential features \nand patterns in protein interactions, identifying functionally related proteins even in \nthe absence of annotated homologs. However, protein–protein interaction networks are \npoorly suited for functional annotation of viral proteins because a substantial proportion \nof existing viral sequence space remains unannotated.\nThe large language model revolution\nRepresentation learning is the process through which machine learning algorithms \nacquire compact and meaningful representations of input text, referred to as embed -\ndings [16]. Proteins and text share a common characteristic as both can be represented \nas sequences of discrete elements, amino acids for proteins and words for text. Because \nof these structural similarities, the same representation-learning techniques can be \napplied to both proteins and text.\nLarge Language Models (LLMs) are a key foundational technology behind artificial \nintelligence (AI) and natural language processing (NLP) [17]. LLM-enabled AI systems \nprocess and generate text by utilizing deep learning techniques and training on vast \nquantities of textual data. LLMs have brought about significant advancements in natural \nlanguage processing, offering numerous advantages and a broad array of potential appli -\ncations. Development and widespread adoption of LLMs, which significantly improve \nperformance on various tasks such as machine translation and question answering, have \ndriven the revolution in natural language AI. The key innovation in the most recent NLP \nneural network architecture, transformers, is the self-attention mechanism. Through \nthis mechanism, the model weighs and focuses on different parts of a sequence when \nencoding a word. For instance, the word “bank” can refer to a financial institution, a strip \nof land along a river, or even an aviation maneuver, depending on the context. Encoding \nlinguistic features and nuances through context has been crucial to the success of LLMs.\nIn NLP , embeddings are typically computed at the word level, or token, meaning that \neach word chunk in the text is mapped to a unique continuous vector representation \nin a high-dimensional space. As a consequence, semantically similar words are close \nto each other in the vector space [18]. This word-level analysis captures word relation -\nships revealing the semantic and syntactic meaning of the text. Various language-based \napproaches have been successfully applied to protein sequences revealing a continuous \nrepresentation of amino acids in a high-dimensional space, where the amino acids show \nclose proximity in vector space if they have similar functions in the context of the input \nsequence [19–21]. Embeddings have been used as inputs to various machine learning \nmodels for tasks such as protein classification, protein–protein interaction prediction, \nand protein function prediction [22, 23].\nPage 4 of 19Harrigan et al. BMC Bioinformatics          (2024) 25:165 \nProtein sequence embeddings can be derived using methods utilizing biochemical \ndata (e.g., amino acid physicochemical properties, isoelectric point, hydrophobicity, and \npolarity) [24] or in an unsupervised manner, such as using bag-of-words [25], n-grams \n[26], term frequency-inverse document frequency (TF-IDF) [27] or deep neural net -\nworks [28, 29]. Building on the success of LLMs, recent developments in protein embed-\nding have utilized transformer-based architectures for deriving high-quality embeddings \nof protein sequences. These models use self-attention mechanisms that capture complex \nrelationships between amino acids and their 3D structures, thus creating embeddings \nthat represent nuanced information about the sequences [30, 31]. As a result, trans -\nformer-derived embeddings have demonstrated significant improvements in accuracy \nacross a range of applications from mutational effect and secondary structure to long-\nrange contact and protein structure prediction [19, 31–35].\nRelying on an aggregate representation for a sequence of words, such as sentences and \nparagraphs, rather than word-level representations is appropriate for many NLP tasks. \nA number of methods have been proposed for constructing a comprehensive represen -\ntation based on individual word-level embeddings. A commonly used method involves \npooling the individual word representations to generate a unified sequence representa -\ntion [36, 37]. The resulting pooled representations can then be compared using various \nsimilarity measures, such as cosine distance, which calculates the cosine of the angle \nbetween two vectors and is frequently used to assess the similarity between embeddings. \nHowever, in the context of proteins, we hypothesize that pooling amino acid embed -\ndings into a single protein-specific representation leads to information loss, similar to \nother compression methods. This loss of information could potentially result in incor -\nrect or incomplete annotations, thereby affecting the accuracy of the analysis.\nIt is also possible to infer distances between sequences of words without resorting to \npooling techniques. An example of this is the Word Mover’s Distance (WMD), which \nmeasures the dissimilarity between two paragraphs as the minimum amount of dis -\ntance that the embedded words of one document need to “travel” to reach the embedded \nwords of another document [38]. This is similar to global alignment methods, which try \nto identify matching amino acids with the smallest distance, according to the distance \nmatrix used. However, employing WMD in the context of aligning amino acid sequences \nfaces two primary challenges. First, the WMD may lead to word alignments that are not \nsuitable for aligning amino acid sequences, as it could include transversions. Second, \nWMD computational complexity, denoted by O (p3logp) , where p represents the num -\nber of unique words in the document, is computationally intensive. In addressing these \nchallenges, it is crucial to explore alternative methods or adaptations to WMD that can \naccommodate the specific requirements of amino acid sequence alignment while main -\ntaining computational efficiency.\nThe adaptation of LLM methods to protein sequence homology detection was tested \nby developing a computational pipeline incorporating a soft-alignment alignment scor -\ning approach analogous to pairwise alignment scoring in traditional homology search \nmethods. Functional annotation of viral proteins, which is a significant challenge for tra-\nditional homology search methods, was used as the test case for the pipeline. The soft \nalignment algorithm was both computationally tractable and interpretable, using statis -\ntics and an approach similar to that of the popular BLAST algorithm. Rigorous testing \nPage 5 of 19\nHarrigan et al. BMC Bioinformatics          (2024) 25:165 \n \nusing data from Virus Orthologous Groups (VOG) [ 39], and PFAM [40] databases dem-\nonstrated that embedding-based alignment scores were more complete and accurate \nthan blastp. These results indicate that the reported soft alignment approach substan -\ntially improves the functional annotation of viral protein sequences.\nMethods\nThe function of an unknown protein sequence was inferred using a three-step process. \nInstead of distance matrices, embeddings were used for identifying homologous protein \nsequences. First, embeddings were generated for a query sequence. Second, an embed -\ndings database of subject sequences was searched using a heuristic for the most similar \nsequences. Finally, soft alignments, an approach conceptually similar to pairwise global \nsequence alignments, was used to identify homologous residues, and, thus, estimate the \nsimilarity between the query sequence and its closest neighbors.\nGenerate protein embeddings\nProtein sequence embeddings are derived from a pre-trained (ESM2) transformer with \n36 layers and 3 billion parameters, and an embedding size of n = 2500 [ 31]. The ESM2 \nmodel was trained on 3.016 M clusters containing approximately 250 M sequences from \nthe UniRef90 dataset [41].\nFor any query sequence q ∈ S of length n, q’s embedding E(q) ∈ Rn×2500 is obtained \nusing the function E. In essence, the embedding function E maps every amino acid, \nor token, in each query sequence q ∈ S to a vector of real values in R2500 , denoted by \nE (q) = (e1 ,e2 ,... ,e2500 ) for each token in the sequence. In order to condense this infor -\nmation into a single vector, a pooled embedding, P(q) ∈ R2500 , is obtained by averaging \nthe embeddings at each position across all n tokens. In mathematical terms, each com -\nponent p i of the pooled embedding P(q) = (p1 ,p2 ,... ,p2500 ) is computed as:\nwhere e ji is the i-th component of the embedding of the j-th token in sequence q. In \nother words, p i is the average of the ith component of the embeddings of all n tokens in \nthe sequence.\nK‑nearest neighbors embeddings search\nFor a query q, let P(q) be the pooled-embedding vector representing the query q as \ndescribed in Step 2.1. An embedding database was built from a set of annotated subject \nsequences S, where each subject sequence s is associated with a unique vector P(s). The \nembedding database was implemented using FAISS [ 42], a specialized library designed \nfor efficient similarity search and clustering of dense vectors. The database is stored as \nan IndexFlatL2 index, which allows exact database search. Prior to insertion into the \ndatabase, embeddings are normalized using the faiss.normalize_L2 method. Similarly, \nquery sequences are normalized, allowing the conversion of a cosine distance to a maxi -\nmum inner product search, and ensuring effective retrieval of the most relevant anno -\ntated sequences for further analysis and comparison.\np i = 1\nn\neji\nj=1\nPage 6 of 19Harrigan et al. BMC Bioinformatics          (2024) 25:165 \nThe K-Nearest Neighbors (KNN) method [43] retrieves the k-nearest neighbors of \nthe query sequence q from the embedding database. Specifically, the cosine distance \nbetween P(q) and P(s) was computed, and the k embeddings with the maximum cosine \nsimilarity were selected as the k-nearest neighbors of P(q).\nIdentify homologous amino acids using soft alignment\nFor subject sequences identified through KNN search, homologous amino acids \nbetween the query and subject sequences were identified in a conceptually similar \nway to that used for local sequence alignments. Given two protein sequences, a query \nq = q1 ,q2 ,... ,qn and a subject s= s1 ,s2 ,... ,sm  , a matrix D is defined as the matrix \nof size n× m , where n and m are the lengths of protein q and s. Each value D i,j in the \nmatrix is the cosine similarity between the embeddings (E) for amino acids E (qi) and \nE (sj) at positions i and j in q and s.\nFor each amino acid q i ∈ q  , C (qi) = sj1 ,sj2 ,sj3 is defined as the top three matching \namino acids in s with the largest cosine similarity. Similarly, for each amino acid sj ∈ s , \nC (sj) = qi1 ,qi2 ,qi3 is defined as the top three matching amino acids in q with the larg -\nest cosine similarity. Mutual matches are denoted as pairs of amino acids (q i, sj) for \nwhich q i is the highest scoring match in C (sj) and sj is the highest scoring match in \nC (qi) . In other words, q i is sj best match and the other way around. Secondary matches \nare denoted as pairs of amino acids (q i, sj) that are not mutual matches but appear in \neach other’s top three best matches, i.e., qi ∈ C (sj) and sj ∈ C (qi) , with the addi -\ntional condition that neither q i nor sj is a mutual match for any other amino acid, i.e., \n∀k �= i, sj �= arg max d ∈C (qk ) cos(qk ,s) and ∀l �= j, q i �= arg max q ∈C (sl) cos(sl,q ) . This \nprocess is illustrated in Fig. 1.\nUsing a matrix to visualize the mutual and secondary matches for two sequences \nsharing homology over their embedding distance matrix D  reveals conserved regions \nFig. 1 A Similarity matrix D between a query sequence q in red and a subject sequence s in blue, both eight \namino acids in length. For each amino acid pair, D ij represents the cosine similarity between E (q i) and E (sj) , \nthe embeddings of the amino acids q i and s j respectively. Green cells represent mutual matches, whereas \nyellow cells are secondary matches. B Graph illustration depicting the mutual and secondary matches in q \nand s. Each vertex represents an amino acid. Solid arrows link a vertex to its top match, while dashed arrows \nconnect a vertex to its second-best match. The green outline surrounding amino acids indicates mutual \nmatches, where both edges are solid lines. This signifies that the enclosed amino acids are each other’s best \nmatch. The yellow outline denotes secondary matches, where at least one of the edges is a dashed line\nPage 7 of 19\nHarrigan et al. BMC Bioinformatics          (2024) 25:165 \n \nhighlighting the degree of sequence similarity. These regions are represented by \ndiagonals, or “alignment paths, ” in the matrix between the two sequences. A soft \nalignment is defined as the path traced through either mutual or secondary corre -\nspondences within the matrix D , such that the score computed by aggregating the \ncosine similarities of mutual or secondary matches along the path is optimal.\nOccasionally, mutual and secondary matches may be misidentified, resulting in \nmatches deviating from the main diagonal. In most cases, off-diagonal matches can \nbe correctly flagged as their inclusion in the alignment path would require insertions \nin one of the sequences or would require deviating from an otherwise high-quality \nalignment path to include the mismatch in the alignment. Consequently, prior to \ncomputing the embedding-based soft alignment between two sequences, the matrix \nD is first processed to exclude diagonals with fewer than five mutual matches unless \npositioned within less than five insertions or deletions away from a diagonal with \nmore than five mutual matches (Fig.  2A). Additionally, single gaps on valid diagonals \nare considered mutual matches if they contain the same amino acid (this is indicated \nby green cells in the matrix shown in Fig. 2 B).\nWe classify a soft alignment as significant and hence report the pair of involved \nsequences as homologous providing that the soft alignment score is at least 20. \nEmpirical observations indicated this score most consistently matched a BLAST \ne-value of 1e− 3 using GenBank NR.\nCode availability\nA Python implementation of the algorithm described above is available on \nGitHub  (https:// github. com/ hawaii- bioin forma tics/ prote in_ embed_ softa lign). The \naccompanying Jupyter notebook presents a comprehensive demonstration of the \nFig. 2 The similarity matrix is traversed removing spurious mutual matches. The soft alignment approach \nassumes that matches will exist near one another and on a diagonal close to the diagonal representing the \nalignment. A In the first step, the matrix is traversed to identify and discard top-left to bottom-right diagonals \nwith fewer than five mutual matches and which are positioned five insertions or deletions away from a \ndiagonal with more than five mutual matches (cells in red). B In the second step, Single gaps along the \nmain diagonal, and representing the same amino acid, are classified as reciprocal matches (green cells), an \nindication of a false negative\nPage 8 of 19Harrigan et al. BMC Bioinformatics          (2024) 25:165 \npackage. In the default configuration, the code runs on CPU, but it can be changed to \nuse GPU if one is available.\nExperiments\nExperiment 1: Statistical properties of soft alignments in unrelated sequence pairs\nThe pairwise similarity of one million randomly chosen protein sequence pairs from \nthe ViralZone protein database (https:// viral  zone. expasy. org/) was analyzed using the \nWATER tool from the EMBOSS suite [44]. The pairwise similarity was computed as \nthe sum of matches (identities plus positives), normalized by the length of the shorter \nsequence in each pair. Ten thousand pairs exhibiting the least similarity (maximum of \n6% pairwise similarity) were selected, indicating that these sequences might not have \nbeen related. These pairs were used as a baseline for determining the occurrence of false \nmutual matches in unrelated sequences.\nExperiment 2: Annotation into high‑level functional classes\nThe performance of soft alignments, BLAST, and pooled embeddings combined with \nKNN search, for classifying proteins into broad functional categories was compared. \nProtein sequences from the Virus Orthologous Groups (VOG) database (http:// vogdb. \norg), which groups NCBI RefSeq viral proteins into protein families using orthology \nand remote homology, were used as the test dataset. This dataset was annotated with \nfunctional categories using a process that combined language-based machine learning to \nproduce annotations from the protein description.\nVirus proteins are broadly grouped according to their functions into structural, non -\nstructural, and regulatory and accessory proteins [45]. In our analysis, we have opted to \nfurther categorize structural proteins into capsid and envelope proteins and subdivided \nnonstructural proteins into those involved in replication or assembly processes. The \nselected categories were, therefore, (1) Capsid Proteins, (2) Envelope Proteins (3) Rep -\nlication and Transcription Proteins, (4) Assembly and Release Proteins, and (5) Regula -\ntory and Accessory Proteins (see Table  1). These collective categories provide a detailed \nbreakdown and nuanced understanding of virus protein functions necessary to shed \nlight on the ecological and biological roles of viruses within ecosystems and understand \nthe high-level differences in viral composition across samples, and identify the potential \nimpact of viral infections on microbial host populations.\nThe VOG protein database was downloaded on February 20, 2023, from the VOGDB \nwebsite (http:// vogdb. org) and the most current descriptions for VOG proteins were \ndownloaded from the National Center for Biotechnology Information (NCBI). This was \nnecessary as several original VOGDB descriptions did not match NCBI descriptions. \nFor example, protein YP_004934080.1 had a VOGDB description of a helix-turn-helix \ntranscriptional regulator whereas the NCBI provided a more informative description of \nCI-like repressor. Given the substantial manual effort needed to validate functional cat -\negories, we chose to focus on the 200 most abundant descriptions, cautiously excluding \nthose with ambiguous or incomplete identifiers. This included protein descriptions with \nkeywords such as ‘incomplete’ , ‘hypothetical’ , or ‘domain-containing’ . The correspond-\ning descriptions and their relative abundances are accessible at the following DOI link: \nhttps:// doi. org/ 10. 5281/ zenodo. 82069 42.\nPage 9 of 19\nHarrigan et al. BMC Bioinformatics          (2024) 25:165 \n \nAn LLM prompt was engineered to assign protein descriptions to their most fitting \nfunctional categories, predicated initially on their roles as inferred solely from these \ndescriptions. This prompt was applied to each protein description for obtaining com -\nprehensive reasoning of the proteins’ roles, derived from the outputs of three LLMs—\nLLaMa (commit 53011c3 with 65 billion parameters) [46], GPT (v3.5 with 175 billion \nparameters) [47] (https:// platf orm. openai. com/ playg round) and Galactica (v1.0.0, large, \nwith 30 billion parameters) [48]. For example, given the input viral protein VP2, LLaMa \nreturned the following: “VP2 is a capsid protein that forms the outer shell, or capsid, of \nthe virus and protects its genetic material. Thus, VP2 is likely a capsid protein. ” Similarly, \nGPT3.5 returned the following: “VP2 is involved in the structure and assembly of the \nvirus, as well as in the interaction with host cells during infection. For example, in par -\nvoviruses, VP2 is a major structural protein that forms the capsid of the virus, while in \nrotaviruses, VP2 is involved in the attachment of the virus to host cells. ” In both cases, \nthe protein was assigned to the “Capsid proteins” category. Sequences not unanimously \nassigned by all three LLMs or had a probability of less than 0.9 were flagged for man -\nual curation. The probability was computed by normalizing the likelihoods of the top \nfive suggested word completions. The exact prompt used and tables assigning accession \nnumbers to categories are provided in the Additional file 1.\nA splitting strategy was used for mitigating potential biases resulting from splitting \nthe VOG dataset into training and test sets, ensuring no orthologous sequences were \npresent in both sets. Specifically, a VOG was randomly selected and its sequences were \nassigned to the test set. This process was repeated until roughly 20% of the data (13,811 \nsequences) were assigned to the test set (queries). Subsequently, the remaining 80% of \nthe data (54,886 sequences) were assigned to the training set (subject database). Both the \ntraining and test sets consisted of sequences from the same 2134 VOGs, with equal dis -\ntribution of all five categories in both. This method strengthened the analysis by reduc -\ning sequence similarity between the test and the training datasets based on membership \nin the same VOG. The annotation of each query was based on the functional category of \nTable 1 The five high-level categories used to classify VOG sequences\nCategory Description\nCapsid Proteins Proteins responsible for forming the outer shell, or capsid, of the virus. \nThe capsid provides protection for the viral genome and plays a critical \nrole in viral entry and infection\nEnvelope Proteins These proteins are present in the outer envelope of some viruses and are \ninvolved in the process of virus entry into host cells\nReplication and Transcription Proteins These proteins are involved in the replication and transcription of the \nviral genome within host cells, playing a critical role in viral gene expres-\nsion, replication, and the production of new virions\nAssembly and Release Proteins Proteins playing a variety of roles in the viral life cycle, including regula-\ntion of viral gene expression, modulation of host immune responses, \nand evasion of host defense mechanisms. They are often multifunctional \nand play critical roles in the survival and spread of the virus as well as \nmaintaining host metabolism during infection\nRegulatory and Accessory Proteins These proteins play a variety of roles in the life cycle of the virus, includ-\ning regulation of viral gene expression, modulation of host immune \nresponses, and evasion of host defense mechanisms. They are often \nmulti-functional and play critical roles in the survival and spread of the \nvirus within a host organism\nPage 10 of 19Harrigan et al. BMC Bioinformatics          (2024) 25:165 \nthe most similar sequence in the training set. The most similar sequence was identified \nby three methods: (1) soft alignment based on the highest soft alignment score (mini -\nmum score of 20); (2) blastp (v 2.13.0+), e-value ≤ 1e−3; and, (3) pooled embeddings \nbased on the maximum cosine similarity using the KNN search.\nExperiment 3: Granular functional annotation based on biological processes or molecular \nfunctions\nViral proteins from UniProt release 2022_05 corresponding to an annotation score of 3 \nor 4 (843,999 sequences) were selected for evaluating the efficacy of the soft alignment-\nbased approach for producing granular-level functional assignments. Every protein was \nassociated with at least one UniProt keyword, a standardized vocabulary designed for \nUniProtKB/Swiss-Prot entries that describes a molecular function or biological process. \nA total of 35,164 proteins longer than 1024 amino acids (the maximum context size sup-\nported by the transformer used to compute embeddings) were discarded. The remaining \nsequences were dereplicated using CD-HIT (version 4.8.1) with a similarity threshold of \n0.9, resulting in 24,070 protein clusters, each annotated with an arithmetic mean of 3.36 \nmolecular functions or biological processes (maximum 9). Annotation of each query \nwas based on the functional category of the most similar sequence in the training set \nas identified by two methods: (1) soft alignment based on the maximum soft alignment \nscore (minimum score of 20); and, (2) blastp (v 2.13.0+), e-value ≤ 1e−3.\nResults and discussion\nResults of Experiment 1: Exploring the statistical properties of soft alignments of unrelated \nsequence pairs\nThe soft alignments of the 10,000 dissimilar UniProtKB sequence pairs resulted in an \naverage of 1.9 mutual or secondary matches per pair using the criteria of an alignment \nscore of ≥ 20 and a minimum of k = 3 consecutive matches, i.e., a gap-free alignment of \nsize 3. The probability of detecting a correspondence between proteins assumed to be \nnon-homologous (less than 6% global similarity) was ≈ 1.1e− 5 for an average amino acid \nlength of 392 in the dataset.\nNext, the probability of encountering a gap-free soft-match alignment of size k = 3 \nthat originated spontaneously was calculated. This is equivalent to observing k = 3 \nsuccessive matches extending along the same top-left to bottom-right diagonal in the \nsimilarity matrix D. It is assumed that k <= n , where n represents the length of both \nsequences being aligned.\nA similarity matrix D of size n × n contains two diagonals of length 1 (top-right and \nbottom-left), two diagonals of length 2, two diagonals of length 3, etc ...  , and one diago-\nnal of length n, referred to as as the “main diagonal. ” Thus, a matrix of size D ∈ Rn×n has \n2 diagonals for each length i for 1 ≤ i < n , and one diagonal of length n.\nA gap-free alignment of size k can be interpreted as consecutive diagonal elements \nin the matrix D. For each diagonal of length i ≥ k  , there exist i − k + 1 sequences of k \nconsecutive alignments. Therefore, there are (n − k + 1) potential consecutive diagonal \nelements of length k in the main diagonal, and 2(i − k + 1) for i = k  to i = n − 1 con-\nsecutive diagonal elements of length k on the remaining diagonals. The total number of \nk-length consecutive diagonal elements, denoted as S, is:\nPage 11 of 19\nHarrigan et al. BMC Bioinformatics          (2024) 25:165 \n \nThe probability of picking any k consecutive alignments is:\nTherefore, the probability that k randomly selected positions form a consecutive \nsequence on the same diagonal is:\nwhere p is the probability of a single match between the two sequences.\nBased on the above, the probability of finding k = 3 consecutively aligned amino acids \nin two protein sequences of n = 400 amino acids and random single mismatch probabil-\nity of p =1.1e− 5 and C(400 × 400, 3) ≈ (160000 )3 /(27 × 4.341)) is:\nThus, assuming a single mismatch false positive rate of p =1.1e− 5, which was derived \nfrom the compared sequences that exhibit less than 6% similarity, 2.5e+28 pairwise \nalignments need to be computed to obtain a single alignment that passes the condition \nof having a gap-free alignment of size 3. Even with a mismatch rate 3 times that which \nwas initially estimated, the probability of having a gap-free alignment of size 3 is still \nextremely low, indicating the low likelihood of a valid soft alignment arising by chance \nalone.\nResults of Experiment 2: Annotation results on VOG data\nBroad functional annotation was assigned to 13,811 VOG proteins. The likelihood \nof a trivial function assignment was reduced by ensuring that no VOGs had member \nsequences in both the test (query) and training sets.\nThe soft alignment approach (minimum soft alignment score 20) annotated a total of \n6484 proteins, of which 6107 were correctly annotated (confusion matrix in Table  2a). \nThis outcome yielded a weighted average precision and recall of 0.942 and 0.985, respec-\ntively. Conversely, blastp annotated only 2181 proteins, of which 2060 were correctly \nannotated (Table 2b), resulting in a weighted average precision and recall of 0.944 and \n0.986, respectively. It is noteworthy that blastp e-values and the soft alignment score \nshow a statistically significant Spearman correlation of 0.563 (p value = 1.12e −224) \n(Fig. 3). This correlation underlines the similarity between the blastp methodology and \nthe soft alignment approach. More significantly, however, the soft alignment approach \nannotated three times more sequences than blastp using the standard blastp e-value \nthreshold of 1e−3 while yielding similar sensitivity and specificity values.\nThe pooling-based method (cosine distance threshold of 2.85) annotated 6171 pro -\nteins, of which 5176 were correctly annotated (Table 2c), resulting in a weighted average \nprecision and recall of 0.861 and 0.965, respectively.\nS = 2\n(n−1∑\ni=k\ni− k\nn−1∑\ni=k\n1 +\nn−1∑\ni=k\n1\n)\n+ (n − k + 1)\n(n × n\nk\n)\n= (n2)!\nk!(n2 − k)!\nP(k|p, n) = S/C (n ∗ n, k) × pk,\n(1)P(k = 3|p,n) = 158968/5.368e+18 ((1.1e-5)3 )\n= 3.94e − 29\nPage 12 of 19Harrigan et al. BMC Bioinformatics          (2024) 25:165 \nTable 2 Confusion matrices based on (a) soft alignment-based approach with a soft alignment \nscore threshold of 20, (b) blastp with an e-value cutoff of 1e−3, and (c) pooled embeddings \nsimilarity using KNN search for capsid proteins (CP), regulatory and accessory proteins (RAP), \nenvelope proteins (EP), replication and transcription protein (RTP), and assembly and release \nproteins (ARP)\nCP RAP EP RTP ARP Class sensitivity Class specificity\n(a) Soft alignment (minimum soft alignment score 20)\nCP 603 0 0 65 15 0.88 0.98\nRAP 1 414 0 42 4 0.9 0.99\nEP 4 1 83 2 0 0.92 1\nRTP 59 59 2 1758 68 0.9 0.98\nARP 13 21 2 19 3249 0.98 0.97\n(b) blastp (e-value cutoff 1e–3)\nCP 51 0 0 0 7 0.88 1\nRAP 0 187 0 53 2 0.77 0.93\nEP 1 0 5 0 0 0.83 1\nRTP 6 15 0 843 21 0.95 0.95\nARP 2 7 0 7 974 0.98 0.91\n(c) Best match using pooled embeddings\nCP 675 9 7 80 133 0.75 0.97\nRAP 7 435 3 87 76 0.72 0.97\nEP 46 6 92 17 8 0.55 0.97\nRTP 112 96 11 1680 108 0.84 0.94\nARP 34 66 48 41 3289 0.95 0.94\nFig. 3 Distribution of the soft alignment scores as a function of the BLAST score. The ranges were chosen to \nhave equal e-value intervals between 1e−3 and 1e−203\nPage 13 of 19\nHarrigan et al. BMC Bioinformatics          (2024) 25:165 \n \nAs illustrated in Fig.  4, the noticeably compact distribution of pairwise distances, pre -\ndominantly falling between 0 and 4.97 for a large portion of the sequences with only 13 \nsequences having a distance ≥ 5.0 posed a challenge for the pooling method. This nar -\nrow range failed to encapsulate the diversity inherent in sequence variability. This was \nparticularly problematic when considering scores at the edges of the similarity range. \nTo illustrate, consider pairs of sequences with low similarity (5–10%) in Fig.  4. For these \nsequences, pooling distances exhibited considerable variance, with normalized dis -\ntances ranging between 0.2 and 0.7. Likewise, the distributions of sequences with high \nsimilarity (94–99%) also demonstrated substantial variance, with normalized distances \nvarying between 0 and 0.2. Overlap occurred in the distribution of distances of pairs of \nsequences with low and high similarity, suggesting that pooling could not distinguish \nbetween low and high-similarity sequence pairs. In contrast, the distributions of the soft \nalignment scores between high or low-similarity sequence pairs showed less variance \nand were substantially separated, indicating a significant difference in similarity between \nthe two groups of sequence pairs (Fig. 4).\nFig. 4 Distribution of min–max normalized pooling distances and soft alignment scores for highly similar \n(94–99%, blue) and dissimilar (5–10%, orange) sequences. Distances from low- and high-similarity sequences \nwere combined prior to min–max normalization\nPage 14 of 19Harrigan et al. BMC Bioinformatics          (2024) 25:165 \nThe soft alignment method failed to detect a few similar sequences that were suc -\ncessfully detected by blastp (Fig.  5). While the KNN step enhances computational effi -\nciency by limiting soft alignment comparisons to five sequences (a parameter that can \nbe changed), this heuristic may yield false negatives due to the limitations of the pooling. \nIdentifying the precise cause of false negatives is challenging. One plausible explanation \ncould be that average pooling accounts for a uniform contribution of each amino acid \nwithin the protein. This could diminish the contributions of smaller regions of simi -\nlarity across proteins, such as short domains, a particular issue for longer amino acid \nsequences.\nFalse negative/positive outcomes are typical when using heuristics for reducing com -\nputational demands. For example, in blastp, an initial “Seed Step” or “Word Finding \nStep” is utilized for identifying sequences sharing identical matches of a certain length \nbetween a query and subject sequences. These matches, often referred to as “k-tuples” or \n“seeds” , reduce the number of sequences BLAST will consider. However, if the number of \nthese matches is set excessively high, false negatives can occur.\nNevertheless, the soft alignment false negative rate is relatively low, especially when \nconsidering the substantial computational savings resulting from using the KNN step. \nBecause the rate of false negatives is contingent on the number of neighbors under \nconsideration, increasing this number could potentially decrease the incidence of false \nnegatives. Alternatively, the process could start with a small number of neighbors, for \ninstance, k = 5, which would filter out sequences with significant matches, and then the \nunmatched sequences could be re-run using a larger k number, such as k = 20. Testing \nshowed that k = 15 proved sufficient in identifying all 48 hits missed with k = 5 but \nidentified (e-value of 1e−3) (Fig. 5).\nReliance on a predetermined distance matrix prevents blastp from accepting cer -\ntain amino acid substitutions in local alignments. For example, two 135 aa long protein \nsequences, YP_001468397.1 and YP_006990334.1, annotated as Minor head proteins, \nshow no detectable BLAST similarity (e-value of 0.015 and only 23.26% identity) but \nnearly matched over their complete length using soft alignments (130 out 135 amino \nacids) (Fig.  6). The reason for BLAST’s inability to identify similarity between the two \nsequences is detailed in the Additional file  1 (Section II: Analysis of BLAST Results of \nMinor Capsid Proteins).\nOverall, these experiments using non-overlapping test and training VOG pep -\ntide databases results show that the soft alignment score is significantly more accu -\nrate than the cosine similarity and yields more annotated sequences than blastp. The \nFig. 5 Venn diagrams representing the intersecting and exclusive regions of soft alignments and BLAST \nresults, based on e-value thresholds of 1e−3 (left) and 1e−4 (right)\nPage 15 of 19\nHarrigan et al. BMC Bioinformatics          (2024) 25:165 \n \nembeddings-based soft alignment approach correctly assigned function to VOG pep -\ntides, allowing rare or novel sequences to be accurately annotated.\nResults of Experiment 3: Assigning sequences into lower‑level functional categories\nSoft alignments and blastp were used for assigning granular functional annotations to \n24,070 dereplicated UniProtKB proteins with known biological processes or molecular \nfunction annotations. Soft alignments identified similar sequences (minimum soft align -\nment score 20) for 16,304 queries, with 16,293 (99%) correctly annotated. In compari -\nson, blastp annotated 16,313 queries (100% correctly annotated).\nAll 16,293 queries correctly annotated by soft alignments were also correctly anno -\ntated by blastp. For the 11 query sequences incorrectly annotated by soft alignments, the \ndiscrepancies in top hit ranking occurred predominantly when soft alignment scores for \nthe five KNN search results (step 2 of the algorithm) did not include the best hit iden -\ntified using blastp. However, false negatives were found to have significant soft align -\nment scores, i.e., greater than 20, against blastp’s best hits. For example, the KNN step \nwas unsuccessful in identifying SHUT_ADE02 as one of the top five closest neighbors \nof sequence A0A059XDH4_9ADEN, even though blastp identified it as the top match \nand despite the presence of 22 mutually matching amino acids spanning 69.5% of the \nquery. Furthermore, nine additional sequences were not annotated by the soft alignment \nmethod because alignment scores between each query and its KNN search hits were less \nthan 20, despite the database containing similar sequences. These discrepancies under -\nline the occasional limitations of the pooling method in fully recognizing the intrinsic \nsimilarity among sequences. Indeed, while the KNN approach can be a useful tool for \nreducing the number of sequences a query needs to be compared to, it is important to \nnote that the pooling method is, in essence, a compression method and can potentially \ndistort the true similarity between sequences.\nAs a solution to protein annotation challenges, our approach emphasizes reducing \nfalse positives, under the hypothesis that they can increase the likelihood of errors \nin downstream analyses. Users can, however, fine-tune the balance between false \nnegatives and false positives by increasing the number of k-nearest neighbors and \nFig. 6 Comparative analysis of sequences YP_001468397.1 and YP_006990334. blastp detected no significant \nsimilarity (top table), however a soft alignment found 130 aligned residues between the two proteins of the \nsame length (bottom matrix)\nPage 16 of 19Harrigan et al. BMC Bioinformatics          (2024) 25:165 \ndecreasing the soft alignment score. Such an adjustment can decrease the false nega -\ntive rate, albeit with the potential trade-off of increasing false positives.\nThe effectiveness of LLMs is fundamentally connected to the volume and quality of \ntheir training data. In domains where training data is scarce, LLMs may generate less \naccurate embeddings. This issue is particularly evident in sequences that are under -\nrepresented in training datasets, resulting in lower-quality embeddings due to insuf -\nficient training on their unique characteristics [49]. Two factors can exacerbate the \nunderrepresentation of proteins in a training swet: \n1. Evolutionary Diversity: The extensive evolutionary variation among proteins presents \na challenge, as the diversity of proteins from lesser-studied species, such as viruses, is \noften underrepresented in training data.\n2. Function Specificity: The high specificity of protein functions, which can be affected \nby minor sequence or structural changes, poses a challenge for LLMs in distinguish -\ning subtle differences, especially in proteins with novel or poorly understood func -\ntions.\nDespite these challenges, the results presented show that embeddings for viral sequences \nremain effective for annotating viral sequences. Moreover, it is common practice to fine-\ntune embeddings by incorporating additional data to cover unrepresented use cases. \nTherefore, the introduction of superior-quality embeddings would only enhance the \nalignment quality by enabling the identification of more cross-protein matches. This \nadaptability ensures that the proposed method remains relevant and effective as new \nmodels are introduced, further underlining the significance of our work in the field.\nConclusion\nThe protein sequence embeddings followed by soft alignment annotation significantly \nimproved functional annotation of viral proteins with no known homologs. For dec -\nades, protein function has been inferred based on the similarity between amino acid \nsequences. In a general context, viral proteins exhibit significant divergence, leading to \nchallenges in identifying homology. This divergence often hinders the determination of \nthe functional roles of such proteins. When given 13,811 unknown protein sequences, \nembedding-based soft alignments accurately assigned functions to 5810 more proteins \nthan blastp. In comparison, blastp annotated only 48 proteins that our method missed. \nSimilarly, the meaningful patterns in amino acids detected by our model allowed for the \naccurate classification of proteins into finer-grained functional categories.\nEmbeddings derived from specialized language models represent a novel approach \nto protein sequence annotation. The soft alignment approach was more accurate than \ncosine similarity, effectively annotating a greater number of previously unannotated \nsequences than BLAST while maintaining comparable accuracy to blastp for sequencing \nhaving database matches. These findings indicate that large language models have the \npotential to enhance protein sequence annotation significantly, making them a valuable \nresource for the scientific community studying viruses and particularly for those in viral \ngenomics, which currently lacks annotation for a large number of protein sequences.\nPage 17 of 19\nHarrigan et al. BMC Bioinformatics          (2024) 25:165 \n \nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 024- 05779-6.\nAdditional file 1. This file includes language model prompts used to annotate the protein sequences as well as a \nbrief illustration of the challenges alignment methods like BLAST face when aligning sequences with low amino \nacids conservation.\nAuthor contributions\nW.H. contributed to the conceptualization of the method, supported its implementation, participated in data analysis, \nand assisted in both writing and editing the manuscript. B.F. contributed to refining the method, designing the experi-\nments, analyzing the data, and editing the manuscript. E.W. contributed to to designing the experiments and interpret-\ning the results. E.W. also contributed to writing the paper. S.P . contributed to designing the experiments, analyzing \ninterpreting the results and edited the manuscript. Z.S. contributed to collecting the test data and participated in review-\ning the manuscript. M.B. conceived and implemented the method, helped implement the experiment and analyze the \nresults and was involved in manuscript preparation.\nFunding\nK.E.W. and S.W.P .’s work was funded by the NSF Grant OIA1736030. MB’s work was funded through a supplement to \nthe OIA1736030 and OIA2149133 awards. W.H. received support through the Hawaii EPSCoR fellowship program \n(OIA-2149133).\nAvailability of data and materials\nA snapshot of the VOG dataset (vog.faa.tar.gz) was acquired from the official website on February, 2023. The dataset \nwas obtained from the following URL: https:// vogdb. org/ downl oad. A snapshot of the PFAM Viral Sequences (manu-\nally reviewed proteins) was acquired from the ViralZone Website on February, 2023. The dataset was obtained from the \nfollowing URL: https:// viral zone. expasy. org/. The UniProt proteins associated with viral annotations scored higher than \n3 or 4 were downloaded from https:// viral zone. expasy. org/. The corresponding datasets were downloaded using the \nsubsequent URLs: For annotation score 3: https:// www. unipr ot. org/ unipr otkb? facets= revie wed: false ,annot ation_ score: \n3 & query= (taxon omy_ id: 10239), For annotation score 4: https:// www. unipr ot. org/ unipr otkb? facets= revie wed: false \n,annot ation_ score: 4 & query= (taxon omy_ id: 10239). For added convenience, this dataset is also accessible on the GitHub \nrepository’s data folder.\nCode availability\nProject name: Soft Alignment of Protein Embeddings Project home page: https:// github. com/ Hawaii- Bioin forma tics/ \nprote in_ embed_ softa lign Operating System: MacOS or Linux Programming Language: Python Any restrictions for non-\nacademics: None.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nNo, I declare that the authors have no conflict of interest as defined by BMC, or other interests that might be perceived \nto influence the results and/or discussion reported in this paper.\nReceived: 14 August 2023   Accepted: 12 April 2024\nReferences\n 1. Schnoes AM, Brown SD, Dodevski I, Babbitt PC. Annotation error in public databases: misannotation of molecular \nfunction in enzyme superfamilies. PLoS Comput Biol. 2009;5(12):1000605.\n 2. Radivojac P , Clark WT, Oron TR, Schnoes AM, Wittkop T, Sokolov A, Graim K, Funk C, Verspoor K, Ben-Hur A. A large-\nscale evaluation of computational protein function prediction. Nat Methods. 2013;10(3):221–7.\n 3. Dutilh BE, Cassman N, McNair K, Sanchez SE, Silva GG, Boling L, Barr JJ, Speth DR, Seguritan V, Aziz RK. A highly \nabundant bacteriophage discovered in the unknown sequences of human faecal metagenomes. Nat Commun. \n2014;5(1):4498.\n 4. Brochet X, Lefranc M-P , Giudicelli V. IMGT/V-QUEST: the highly customized and integrated system for IG and TR \nstandardized VJ and VDJ sequence analysis. Nucleic Acids Res. 2008;36(supp-l2):503–8.\n 5. Altschul SF, Wootton JC, Gertz EM, Agarwala R, Morgulis A, Schäffer AA, Yu Y-K. Protein database searches using \ncompositionally adjusted substitution matrices. FEBS J. 2005;272(20):5101–9.\n 6. Altschul SF, Gish W, Miller W, Myers EW, Lipman DJ. Basic local alignment search tool. J Mol Biol. 1990;215(3):403–10.\n 7. Altschul SF, Madden TL, Schäffer AA, Zhang J, Zhang Z, Miller W, Lipman DJ. Gapped blast and psi-blast: a new \ngeneration of protein database search programs. Nucleic Acids Res. 1997;25(17):3389–402.\nPage 18 of 19Harrigan et al. BMC Bioinformatics          (2024) 25:165 \n 8. Littmann M, Heinzinger M, Dallago C, Olenyi T, Rost B. Embeddings from deep learning transfer go annotations \nbeyond homology. Sci Rep. 2021;11(1):1160.\n 9. Sureyya Rifaioglu A, Doğan T, Jesus Martin M, Cetin-Atalay R, Atalay V. Deepred: automated protein function predic-\ntion with multi-task feed-forward deep neural networks. Sci Rep. 2019;9(1):7344.\n 10. Cai Y, Wang J, Deng L. Sdn2go: an integrated deep learning model for protein function prediction. Front Bioeng \nBiotechnol. 2020;8:391.\n 11. Devkota K, Schmidt H, Werenski M, Murphy JM, Erden M, Arsenescu V, Cowen LJ. Glider: function prediction from \nglide-based neighborhoods. Bioinformatics. 2022;38(13):3395–406.\n 12. Yeger-Lotem E, Sharan R. Human protein interaction networks across tissues and diseases. Front Genet. 2015;6:257.\n 13. Schwikowski B, Uetz P , Fields S. A network of protein–protein interactions in yeast. Nat Biotechnol. \n2000;18(12):1257–61.\n 14. Mohamed SK, Nounu A, Nováček V. Biological applications of knowledge graph embedding models. Brief Bioinform. \n2021;22(2):1679–93.\n 15. Yang F, Fan K, Song D, Lin H. Graph-based prediction of protein–protein interactions with attributed signed graph \nembedding. BMC Bioinform. 2020;21(1):1–16.\n 16. Bengio Y, Courville A, Vincent P . Representation learning: a review and new perspectives. IEEE Trans Pattern Anal \nMach Intell. 2013;35(8):1798–828.\n 17. Min B, Ross H, Sulem E, Veyseh APB, Nguyen TH, Sainz O, Agirre E, Heintz I, Roth D. Recent advances in natural \nlanguage processing via large pre-trained language models: a survey. ACM Comput Surv. 2021;56:1–40.\n 18. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their \ncompositionality. Adv Neural Inf Process Syst. 2013;26.\n 19. Wang W, Peng Z, Yang J. Single-sequence protein structure prediction using supervised transformer protein lan-\nguage models. Nat Comput Sci. 2022;2(12):804–14.\n 20. Elnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y, Jones L, Gibbs T, Feher T, Angerer C, Steinegger M. Prottrans: \ntoward understanding the language of life through self-supervised learning. IEEE Trans Pattern Anal Mach Intell. \n2021;44(10):7112–27.\n 21. Bepler T, Berger B. Learning the protein language: evolution, structure, and function. Cell Syst. 2021;12(6):654–69.\n 22. Madani A, Krause B, Greene ER, Subramanian S, Mohr BP , Holton JM, Olmos JL Jr, Xiong C, Sun ZZ, Socher R, et al. \nLarge language models generate functional protein sequences across diverse families. Nat Biotechnol. 2023;41:1–8.\n 23. Nijkamp E, Ruffolo J, Weinstein EN, Naik N, Madani A. Progen2: exploring the boundaries of protein language mod-\nels; 2022. arXiv: 2206. 13517.\n 24. Ofer D, Linial M. Profet: feature engineering captures high-level protein functions. Bioinformatics. \n2015;31(21):3429–36.\n 25. Lan M, Tan CL, Su J. Feature generation and representations for protein–protein interaction classification. J Biomed \nInform. 2009;42(5):866–72.\n 26. Islam SA, Heil BJ, Kearney CM, Baker EJ. Protein classification using modified n-grams and skip-grams. Bioinformatics. \n2018;34(9):1481–7.\n 27. Ranjan A, Fernández-Baca D, Tripathi S, Deepak A. An ensemble tf-idf based approach to protein function prediction \nvia sequence segmentation. IEEE/ACM Trans Comput Biol Bioinform. 2021;19(5):2685–96.\n 28. Hamid M-N, Friedberg I. Identifying antimicrobial peptides using word embedding with deep recurrent neural \nnetworks. Bioinformatics. 2019;35(12):2009–16.\n 29. Liu C-M, Ta V-D, Le NQK, Tadesse DA, Shi C. Deep neural network framework based on word embedding for protein \nglutarylation sites prediction. Life. 2022;12(8):1213.\n 30. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, Žídek A, Potapenko A. \nHighly accurate protein structure prediction with alphafold. Nature. 2021;596(7873):583–9.\n 31. Lin Z, Akin H, Rao R, Hie B, Zhu Z, Lu W, Smetanin N, Santos Costa A, Fazel-Zarandi M, Sercu T, Candido S, et al. Lan-\nguage models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv. 2022.\n 32. Zhang S, Fan R, Liu Y, Chen S, Liu Q, Zeng W. Applications of transformer-based language models in bioinformatics: a \nsurvey. Bioinform Adv. 2023;3(1):001.\n 33. Mullick B, Magar R, Jhunjhunwala A, Farimani AB. Understanding mutation hotspots for the SARS-CoV-2 spike \nprotein using Shannon Entropy and k-means clustering. Comput Biol Med. 2021;138: 104915.\n 34. Jin J, Yu Y, Wang R, Zeng X, Pang C, Jiang Y, Li Z, Dai Y, Su R, Zou Q. iDNA-ABF: multi-scale deep biological language \nlearning model for the interpretable prediction of DNA methylations. Genome Biol. 2022;23(1):1–23.\n 35. Rao R, Meier J, Sercu T, Ovchinnikov S, Rives A. Transformer protein language models are unsupervised structure \nlearners. Biorxiv. 2020;2020–12.\n 36. Devlin J, Chang M-W, Lee K, Toutanova K. Bert: pre-training of deep bidirectional transformers for language under-\nstanding. 2018. arXiv: 1810. 04805.\n 37. Le Q, Mikolov T. Distributed representations of sentences and documents. In: International conference on machine \nlearning; 2014;1188–1196. PMLR\n 38. Huang G, Guo C, Kusner MJ, Sun Y, Sha F, Weinberger KQ. Supervised word mover’s distance. Advances in neural \ninformation processing systems 2016;29.\n 39. Bao Y, Federhen S, Leipe D, Pham V, Resenchuk S, Rozanov M, Tatusov R, Tatusova T. National center for biotechnol-\nogy information viral genomes project. J Virol. 2004;78(14):7291–8.\n 40. Bateman A, Coin L, Durbin R, Finn RD, Hollich V, Griffiths-Jones S, Khanna A, Marshall M, Moxon S, Sonnhammer EL. \nThe Pfam protein families database. Nucleic Acids Res. 2004;32(suppl–1):138–41.\n 41. Suzek BE, Huang H, McGarvey P , Mazumder R, Wu CH. UniRef: comprehensive and non-redundant UniProt reference \nclusters. Bioinformatics. 2007;23(10):1282–8.\n 42. Johnson J, Douze M, Jégou H. Billion-scale similarity search with GPUs. IEEE Trans Big Data. 2019;7(3):535–47.\n 43. Fix E, Hodges JL. Discriminatory analysis: nonparametric discrimination, consistency properties. Int Stat Rev/Revue \nInternationale de Statistique. 1989;57(3):238–47.\nPage 19 of 19\nHarrigan et al. BMC Bioinformatics          (2024) 25:165 \n \n 44. Rice PM, Bleasby AJ, Ison JC, Mullan L, Bottu G. EMBOSS user’s guide: practical bioinformatics. Cambridge: Cam-\nbridge University Press; 2011.\n 45. Xue B, Blocquel D, Habchi J, Uversky AV, Kurgan L, Uversky VN, Longhi S. Structural disorder in viral proteins. Chem \nRev. 2014;114(13):6880–911.\n 46. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, Rozière B, Goyal N, Hambro E, Azhar F, et al. Llama: \nopen and efficient foundation language models; 2023. arXiv: 2302. 13971.\n 47. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are unsupervised multitask learners. \nOpenAI Blog. 2019;1(8):9.\n 48. Taylor R, Kardas M, Cucurull G, Scialom T, Hartshorn A, Saravia E, Poulton A, Kerkez V, Stojnic R. Galactica: a large \nlanguage model for science; 2022. arXiv: 2211. 09085.\n 49. Ding F, Steinhardt JN. Protein language models are biased by unequal sequence sampling across the tree of life. \nbioRxiv. 2024;2024–03.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Annotation",
  "concepts": [
    {
      "name": "Annotation",
      "score": 0.7472569346427917
    },
    {
      "name": "Computational biology",
      "score": 0.6436008214950562
    },
    {
      "name": "DNA microarray",
      "score": 0.5832805633544922
    },
    {
      "name": "Computer science",
      "score": 0.5167738199234009
    },
    {
      "name": "Gene",
      "score": 0.450982928276062
    },
    {
      "name": "Gene Annotation",
      "score": 0.4260697066783905
    },
    {
      "name": "Biology",
      "score": 0.4218555688858032
    },
    {
      "name": "Bioinformatics",
      "score": 0.3252643346786499
    },
    {
      "name": "Genetics",
      "score": 0.3249366581439972
    },
    {
      "name": "Genome",
      "score": 0.3006102442741394
    },
    {
      "name": "Gene expression",
      "score": 0.1637411117553711
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I117965899",
      "name": "University of Hawaiʻi at Mānoa",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I86501945",
      "name": "University of Delaware",
      "country": "US"
    }
  ],
  "cited_by": 8
}