{
  "title": "Investigating Transfer Learning in Multilingual Pre-trained Language Models through Chinese Natural Language Inference",
  "url": "https://openalex.org/W3174782183",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5069337524",
      "name": "Hai Hu",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Allen Institute",
        "Indiana University Bloomington",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A5101074708",
      "name": "He Zhou",
      "affiliations": [
        "Indiana University Bloomington"
      ]
    },
    {
      "id": "https://openalex.org/A5066345085",
      "name": "Zuoyu Tian",
      "affiliations": [
        "Indiana University Bloomington"
      ]
    },
    {
      "id": "https://openalex.org/A5100430650",
      "name": "Yiwen Zhang",
      "affiliations": [
        "Indiana University Bloomington"
      ]
    },
    {
      "id": "https://openalex.org/A5062237715",
      "name": "Yina Patterson",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100720241",
      "name": "Yanting Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066141653",
      "name": "Yixin Nie",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5039359682",
      "name": "Kyle Richardson",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2964068236",
    "https://openalex.org/W3037854022",
    "https://openalex.org/W3087094142",
    "https://openalex.org/W3153632605",
    "https://openalex.org/W3029801984",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2984978596",
    "https://openalex.org/W3099864716",
    "https://openalex.org/W3089006046",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2945067664",
    "https://openalex.org/W3035718362",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2056321066",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3099911888",
    "https://openalex.org/W2954194820",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3105721709",
    "https://openalex.org/W4383551279",
    "https://openalex.org/W4287554196",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W3089285634",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3009095382",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W3100511085",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2964222268",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W2798665661",
    "https://openalex.org/W3098987177",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3097977265",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3013749478",
    "https://openalex.org/W3105719633",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W3161110574",
    "https://openalex.org/W3017701505",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W2997789497",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W3098824823"
  ],
  "abstract": "Multilingual transformers (XLM, mT5) have been shown to have remarkable transfer skills in zero-shot settings.Most transfer studies, however, rely on automatically translated resources (XNLI, XQuAD), making it hard to discern the particular linguistic knowledge that is being transferred, and the role of expert annotated monolingual datasets when developing task-specific models.We investigate the cross-lingual transfer abilities of XLM-R for Chinese and English natural language inference (NLI), with a focus on the recent largescale Chinese dataset OCNLI.To better understand linguistic transfer, we created 4 categories of challenge and adversarial tasks (totaling 17 new datasets 1 ) for Chinese that build on several well-known resources for English (e.g., HANS, NLI stress-tests).We find that cross-lingual models trained on English NLI do transfer well across our Chinese tasks (e.g., in 3/4 of our challenge categories, they perform as well/better than the best monolingual models, even on 3/5 uniquely Chinese linguistic phenomena such as idioms, pro drop).These results, however, come with important caveats: cross-lingual models often perform best when trained on a mixture of English and high-quality monolingual NLI data (OCNLI), and are often hindered by automatically translated resources (XNLI-zh).For many phenomena, all models continue to struggle, highlighting the need for our new diagnostics to help benchmark Chinese and cross-lingual models.category n",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3770–3785\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3770\nInvestigating Transfer Learning in Multilingual Pre-trained Language\nModels through Chinese Natural Language Inference\nHai Hu†÷ ×He Zhou† Zuoyu Tian† Yiwen Zhang† Yina Ma‡\nYanting Li◁ Yixin Nie○ Kyle Richardson◇\n†Indiana University Bloomington ‡Brigham Young University ÷ ×Shanghai Jiaotong Univ.\n◁Northwestern University ○UNC Chapel Hill ◇Allen Institute for AI\nhu.hai@outlook.com; {hzh1,zuoytian,yiwezhan,yinama}@iu.edu;\nyanting.li@northwestern.edu; yixin1@cs.unc.edu; kyler@allenai.org\nAbstract\nMultilingual transformers (XLM, mT5) have\nbeen shown to have remarkable transfer skills\nin zero-shot settings. Most transfer studies,\nhowever, rely on automatically translated re-\nsources (XNLI, XQuAD), making it hard to\ndiscern the particular linguistic knowledge that\nis being transferred, and the role of expert an-\nnotated monolingual datasets when develop-\ning task-speciﬁc models. We investigate the\ncross-lingual transfer abilities of XLM-R for\nChinese and English natural language infer-\nence (NLI), with a focus on the recent large-\nscale Chinese dataset OCNLI. To better un-\nderstand linguistic transfer, we created 4 cat-\negories of challenge and adversarial tasks (to-\ntaling 17 new datasets1) for Chinese that build\non several well-known resources for English\n(e.g., HANS, NLI stress-tests). We ﬁnd that\ncross-lingual models trained on English NLI\ndo transfer well across our Chinese tasks (e.g.,\nin 3/4 of our challenge categories, they per-\nform as well/better than the best monolingual\nmodels, even on 3/5 uniquely Chinese lin-\nguistic phenomena such as idioms, pro drop).\nThese results, however, come with important\ncaveats: cross-lingual models often perform\nbest when trained on a mixture of English and\nhigh-quality monolingual NLI data (OCNLI),\nand are often hindered by automatically trans-\nlated resources (XNLI-zh). For many phenom-\nena, all models continue to struggle, highlight-\ning the need for our new diagnostics to help\nbenchmark Chinese and cross-lingual models.\n1 Introduction\nRecent pre-trained multilingual transformer mod-\nels, such as XLM(-R) (Conneau and Lample, 2019;\nConneau et al., 2020), mT5 (Xue et al., 2020) and\nothers (Liu et al., 2020; Lewis et al., 2020) have\n1All new datasets/code are released at https://github.com/\nhuhailinguist/ChineseNLIProbing.\nbeen shown to be successful in NLP tasks for sev-\neral non-English languages (Khashabi et al., 2020;\nChoi et al., 2021), as well as in multilingual bench-\nmarks (Devlin et al., 2019; Conneau et al., 2020;\nXue et al., 2020; Artetxe et al., 2020). A particular\nappeal is that they can be used forcross-lingual and\nzero-shot transfer. That is, after pre-training on a\nraw, unaligned corpus consisting of text from many\nlanguages, models can be subsequently ﬁne-tuned\non a particular task in a resource-rich language\n(e.g., English) and directly applied to the same task\nin other languages without requiring any additional\nlanguage-speciﬁc training.\nGiven this recent progress, a natural question\narises: does it make sense to invest in large-scale\ntask-speciﬁc dataset construction for low-resourced\nlanguages, or does cross-lingual transfer alone suf-\nﬁce for many languages and tasks? A closely re-\nlated question is: how well do multilingual mod-\nels transfer across speciﬁc linguistic and language-\nspeciﬁc phenomena? While there has been much\nrecent work on probing multilingual models (Wu\nand Dredze, 2019; Pires et al., 2019; Karthikeyan\net al., 2019), inter alia, a particular limitation is\nthat most studies rely on automatically translated\nresources such as XNLI (Conneau et al., 2018) and\nXQuAD (Artetxe et al., 2020), which makes it dif-\nﬁcult to discern the particular linguistic knowledge\nthat is being transferred and the role of large-scale,\nexpert annotated monolingual datasets when build-\ning task- and language-speciﬁc models.\nIn this paper, we investigate the cross-lingual\ntransfer abilities of XLM-R (Conneau et al., 2020)\nfor Chinese natural language inference (NLI). Our\nfocus on Chinese NLI is motivated by the recent\nrelease of the ﬁrst large-scale, human-annotated\nChinese NLI dataset OCNLI ( Original Chinese\nNLI) (Hu et al., 2020)2, which we use to directly in-\n2To our knowledge, OCNLI is currently the largest non-\n3771\ncategory n\nChinese\nHANS\nLexical overlap 1,428\nSubsequence 513\nstress tests\nDistraction: 2 categories 8,000\nAntonym 3,000\nSynonym 2,000\nSpelling 11,676\nNumerical reasoning 8,613\ndiagnostics\nCLUE (Xu et al., 2020) 514\nCLUE expansion (ours) 796\nWorld knowledge (ours) 38\nClassiﬁer (ours) 139\nChengyu/idioms (ours) 251\nPro-drop (ours) 198\nNon-core arguments (ours) 186\nsemantic\nprobing\nNegation 1,002\nBoolean 1,002\nQuantiﬁer 1,002\nCounting 1,002\nConditional 1,002\nComparative 1,002\nsum 43,364\nTable 1: Summary statistics of the four evaluation sets.\nvestigate the role of high-quality task-speciﬁc data\nvs. English-based cross-lingual transfer. To better\nunderstand linguistic transfer, and help benchmark\nrecent SOTA Chinese NLI models, we created 4 cat-\negories of challenge/adversarial tasks (totaling 17\nnew datasets) for Chinese that build on several well-\nestablished resources for English and the literature\non model probing (see Poliak (2020)). Our new re-\nsources, which are summarized in Table 1, include:\na new set of diagnostic tests in the style of the\nSuperGLUE (Wang et al., 2019) and CLUE (Xu\net al., 2020) diagnostics; Chinese versions of the\nHANS dataset (McCoy et al., 2019) and NLI stress-\ntests (Naik et al., 2018), as well as a collection of\nthe basic reasoning and logic semantic probes for\nChinese based on Richardson et al. (2020).\nOur results are largely positive: We ﬁnd that\ncross-lingual models trained exclusively on En-\nglish NLI do transfer relatively well across our\nnew Chinese tasks (e.g., in 3/4 of the challenge\ncategories shown in Table 1, they perform overall\nas well or better than the best monolingual Chinese\nmodels without additional specialized training on\nChinese data, and have competitive performance on\nOCNLI). A particularly striking result is that such\nmodels even perform well on 3/5 uniquely Chinese\nlinguistic phenomena such as idioms, pro drop ,\nproviding evidence that many language-speciﬁc\nphenomena do indeed transfer. These results, how-\nEnglish NLI dataset that was annotated in the style of English\nMNLI without any translation.\never, come with important caveats: on several phe-\nnomena we ﬁnd that models continue to struggle\nand are far outpaced by conservative estimates of\nhuman performance (e.g., our best model on Chi-\nnese HANS remains ∼19% behind human perfor-\nmance), highlighting the need for more language-\nspeciﬁc diagnostics tests. Also, ﬁne-tuning models\non mixtures of English NLI data and high-quality\nmonolingual data (OCNLI) consistently performs\nthe best, whereas mixing with automatically trans-\nlated datasets (XNLI-zh) can greatly hinder model\nperformance. This last result shows that high-\nquality monolingual datasets still play an important\nrole when building cross-lingual models, however,\nthe particular type of monolingual dataset that is\nneeded can vary and is best informed by targeted\nbehavioral testing of the type we pursue here.\n2 Related Work\nThere has been a lot of work on trying to understand\nmultilingual transformers (Wu and Dredze, 2019;\nPires et al., 2019), which has focused on either ex-\namining the representation of different layers in\nthe transformer architecture or the lexical overlap\nbetween languages. Karthikeyan et al. (2019) in-\nvestigate the role of network depth and number\nof attention heads, as well as syntactic/word-order\nsimilarity on the cross-lingual transfer performance.\nIn addition to studies cited at the outset, positive\nresults of cross-lingual transfer across a wide range\nof languages are reported in Wu and Dredze; Nozza\net al. (2020), with a focus on transfer across spe-\nciﬁc tasks such as POS tagging, NER; in contrast,\nwe focus on different categories of linguistic trans-\nfer, which has received less attention, as well as the\nrole of monolingual data for transfer in NLI.\nStudies into the linguistic abilities and robust-\nness of current NLI models have proliferated in\nrecent years, partly owing to the discovery of sys-\ntematic biases, or annotation artifacts (Gururangan\net al., 2018; Poliak et al., 2018), in benchmark NLI\ndatasets such as SNLI (Bowman et al., 2015) and\nMNLI (Williams et al., 2018). This has been cou-\npled with the development of new adversarial tests\nsuch as HANS (McCoy et al., 2019) and the NLI\nstress-tests (Naik et al., 2018), as well as several\nnew linguistic challenge datasets (Glockner et al.,\n2018; Richardson et al., 2020; Geiger et al., 2020;\nYanaka et al., 2019; Saha et al., 2020; Goodwin\net al., 2020), inter alia, that focus on a wide range\nof linguistic and reasoning phenomena. All of this\n3772\nwork focuses exclusively on English, whereas we\nfocus on constructing analogous probing datasets\ntailored to Chinese to help advance research on\nChinese NLI and cross-lingual transfer.\nThere has been a surge in the development of\nNLI resources for languages other than English.\nSuch resources are often created in the following\ntwo ways: (1) from scratch, in the style of MNLI\n(Williams et al., 2018), where annotators are used\nto produce hypotheses and inference labels based\non a provided set of premises, as pursued for Chi-\nnese OCNLI (Hu et al., 2020), or SciTail (Khot\net al., 2018), where sentences are paired automati-\ncally and labeled by annotators (Amirkhani et al.,\n2020; Hayashibe, 2020). (2) Through automatic\n(Conneau et al., 2018; Budur et al., 2020; Real et al.,\n2020) or manual (Wijnholds and Moortgat, 2021)\ntranslation from existing English datasets. Studies\non cross-lingual transfer for NLI have largely fo-\ncused on XNLI (Conneau et al., 2018), which we\nshow has limited utility for Chinese NLI transfer.\n3 Dataset creation\nIn this section, we describe the details of the 4 types\nof challenge datasets we constructed for Chinese to\nstudy cross-lingual transfer (see details in Table 1).\nThey ﬁt into two general categories: Adversarial\ndatasets (Section 3.1) built largely from patterns\nin OCNLI (Hu et al., 2020) and XNLI (Conneau\net al., 2018) and Probing/diagnostic datasets\n(Section 3.2), which are built from scratch in a\nparallel fashion to existing datasets in English.\nWhile we aim to mimic the annotation protocols\npursued in the original English studies, we place\nthe additional methodological constraint that each\nnew dataset is vetted, either through human anno-\ntation using a disjoint set of Chinese linguists, or\nthrough internal mediation among local Chinese\nexperts; details are provided below.\n3.1 Adversarial dataset\nExamples from the 7 adversarial tests we created\nare illustrated in Table 2.3 Chinese HANS is built\nfrom patterns extracted in the large-scale Chinese\nNLI dataset OCNLI (Hu et al., 2020), whereas the\nDistraction, Antonym, Synonym and Spelling\nsubsets are built from an equal mixture of OCNLI\nand XNLI-zh (Conneau et al., 2018) data; in the\nlatter case, such a difference allows us to fairly\n3A more detailed description of the data creation process\ncan be found in Appendix A.\ncompare the effect of training on expert-annotated\n(i.e., OCNLI) vs. automatically translated data (i.e.,\nXNLI-zh) as detailed in Section 4.\nChinese HANS McCoy et al. (2019) dis-\ncovered systematic biases/heuristics in the\nMNLI dataset, which they named “lexi-\ncal/subsequence/constituent” overlap. “Lexical\noverlap” is deﬁned to be the pairs where the\nvocabulary of the hypothesis is a subset of the\nvocabulary of the premise. For example, “The boss\nis meeting the client.” and “The client is meeting\nthe boss. ”, which has an entailment relation.\nHowever, lexical overlap does not necessarily\nmean the premise will entail the hypothesis, e.g.,\n“The judge was paid by the actor.” does not entail\n“The actor was paid by the judge.” (examples from\nMcCoy et al. (2019)). Thus a model relying on the\nheuristic will fail catastrophically in the second\ncase.\nInspired by the English HANS, we examine\nwhether OCNLI also possesses such biases, as it\nhas a similar annotation procedure as MNLI. We\nfollow the design of the original HANS experi-\nments, and adapt their scripts4 to extract examples\nin OCNLI that satisfy the two heuristics. We ﬁnd a\nheavy bias towards “entailment”, where 79.5% of\nsuch examples are “entailment”, similar to MNLI.\nTo construct a Chinese HANS, we ﬁrst look into\nsyntactic structures of the examples having the two\nheuristics. Then we write 29 templates for the\nlexical overlap heuristic and 11 templates for sub-\nsequence overlap.5 Using the templates and a vo-\ncabulary of 263 words, we generated 1,941 NLI\npairs. See Table 2 for examples and Appendix A\nfor details.\nDistraction We add distractions to the premise\nor hypothesis, similar to the “length mismatch”\nand “word overlap” conditions in the NLI stress\ntests of Naik et al. (2018). The distractions are\neither tautologies (“true is not false”) or a true\nstatement from our world knowledge (“Finnland\nis not a permanent member of the UN security\ncouncil”), which should not inﬂuence the infer-\nence label. We control whether the distraction con-\ntain a negation or not, and thus create four con-\nditions: premise-negation, premise-no-negation,\nhypothesis-negation, and hypothesis-no-negation.\nSee Table 2 for examples.\n4https://github.com/tommccoy1/hans\n5For details of the templates, see our Github repository.\n3773\ncategory n premise hypothesis label\nChinese\nHANS\nLexical\noverlap\n1428 我们把银行职员留在电影院了。We left the bank clerk in the cinema. 银行职员把我们留在电影院了。The bank\nclerk left us in the cinema.\nC\nSubsequence 513 谁说律师都是穿西装的。Who told you thatall lawyerswear suits. 律师都是穿西装的。All lawyerswear suits. C\nstress tests\nDistraction\n(add to\npremise)\n4000 国 有 企业 改 革 的 思路 和方 针 政 策 已 经 明确,而且\n刚做完手术出院的病人不应剧烈运动。The policy of the reform of\nstate-owned enterprises is now clear,and patients who just had surgery\nshouldn’thaveintense exercise.\n根本不存在国有企业。The state-owned en-\nterprises don’t exist.\nC\nDistraction\n(add to\nhypothesis)\n4000 这时李家院子挤满了参观的人。During this time, the Li family’s\nbackyard is full of people who came to visit.\n这 地 方 有 个 姓 李 的 人 家,\n而且真的不是假的。There is a Li fam-\nily here, andtrue is not false.\nE\nAntonym 3000 一些地方财政收支矛盾较大。The disagreement about local revenue\nis relatively big.\n一些地方财政收支矛盾较小。The disagree-\nment about local revenue is relatively small.\nC\nSynonym 2000 海部组阁困难说明了什么。What can you tell from the difﬁculties\nfrom Kaifu’s attempt to set up a cabinet?\n海部组阁艰难说明了什么。What can you\ntell from the hardships from Kaifu’s attempt\nto set up a cabinet?\nE\nSpelling 2980 身上裹一件工厂发的棉大衣,手插在袖筒里。(Someone is) wrapped\nup in a big cotton coat the factory gave with hands in the sleeves\n身上质少一件衣服。There’s at least[typo]\none coat on the body.\nE\nNumerical\nreasoning\n8613 小红每分钟打不到510个字。Xiaohong types fewer than 510 words\nper min.\n小红每分钟打110个字。Xiaohong types\n110 words per min.\nN\nTable 2: Example NLI pairs in Chinese HANS and stress tests with translations.\nAntonym We replace a word in the premise with\nits antonym to form a contradiction. To ensure the\nquality of the resulting NLI pairs, we manually\nexamine the initially generated data and decided to\nonly replace nouns and adjectives, as they are more\nlikely to produce real contradictions.\nSynonym We replace a word in the premise with\nits synonym to form an entailment.\nSpelling We replace one random character in the\nhypotheses with its homonym (character with the\nsame pinyin pronunciation ignoring tones) as this\nis one of the most common types of misspellings\nin Chinese.\nNumerical reasoning We create a probing set\nfor numerical reasoning, following simple heuris-\ntics such as the following. When the premise is\nMary types x words per minute , the entailed hy-\npothesis can be: Mary types less than y words per\nminute, where x < y. A contradictory hypothesis:\nMary types y words per minute, where x > y or x <\ny. Then a neutral pair can be produced by reversing\nthe premise and hypothesis of the above entailment\npair. 4 heuristic rules (with 6 words for quantiﬁca-\ntion) are used and the seed sentences are extracted\nfrom Ape210k (Zhao et al., 2020), a dataset of\nChinese elementary-school math problems. The\nresulting data contains 8,613 NLI pairs.\nFor quality control and to compute human per-\nformance, we randomly sampled 50 examples from\nall subsets and asked 5 Chinese speakers to verify.\nOur goal is to mimic the human annotation proto-\ncol from Nangia and Bowman (2019), which gives\nus a conservative estimate of human performance\ngiven that our annotators received very little in-\nstructions. Their majority vote agrees with the gold\nlabel 90.0% of the time, which suggests that our\ndata is of high quality and allows us to later com-\npare against model performance.6\n3.2 Probing/diagnostic datasets\nWhile the Chinese HANS and stress tests are de-\nsigned to adversarially test the models, we also cre-\nate probing or diagnostic datasets which are aimed\nat examining the models’ linguistic and reasoning\nabilities.\nHand-crafted diagnostics We expanded the di-\nagnostic dataset from the Chinese NLU Bench-\nmark (CLUE) (Xu et al., 2020) in the following\ntwo ways:\nFirst, 6 Chinese linguists (PhD students) created\ndiagnostics for 4 Chinese-speciﬁc linguistic phe-\nnomena. Here are two of the phenomena:7 (1) pro-\ndrop: subjects or objects in Chinese can be dropped\nwhen they can be recovered from the context (Li\net al., 1981). Thus the model needs to ﬁgure out the\nsubject/object from the context. (2) four-character\nidioms (i.e., 成语 Chengyu). They are a special\ntype of Chinese idioms that has exactly four char-\nacters, usually with a ﬁgurative meaning different\nfrom the literary meaning, e.g., 打草惊蛇 hit hay\nstartle snake (behaving carelessly and causing your\nenemy to become vigilant). We construct examples\nto test whether models understand the ﬁgurative\nmeaning in the idioms. Speciﬁcally, we ﬁrst create\na premise P which includes the idiom, where there\nis enough context so that a human is highly likely to\n6Speciﬁcally: 98.0% on Chinese HANS, 86.0% on the\nstress tests. For comparison, different subsets of the English\nstress tests receives 85% to 98% agreement (Naik et al., 2018).\n7For the other two, please refer to Appendix A.\n3774\ninterpret the idiom ﬁguratively. Then we create an\nentailed hypothesis that is based on the ﬁgurative\n(correct) interpretation, and a neutral/contradictory\nhypothesis that uses the literal (incorrect) meaning\n(see Table 11 in the Appendix for an example). For\neach P we write 3 hypothesis, one for each infer-\nence relation. We also added diagnostics involving\nworld knowledge.\nSecond, we double the number of diagnostic\npairs for all 9 existing linguistic phenomena in\nCLUE with pairs whose premises are selected from\na large news corpus 8 and hypotheses are hand-\nwritten by our linguists, to accompany the 514 ar-\ntiﬁcially created data in CLUE. The resulting new\ndiagnostics is 4 times as large as the original one,\nwith a total of 2,122 NLI pairs. For quality con-\ntrol, each pair is double-checked by local Chinese\nlinguists not involved in this study and the con-\ntroversial cases were discarded after a discussion\namong the 6 linguists. See Table 11 in Appendix A\nfor examples.\nSemantic fragments Following Richardson et al.\n(2020) and Salvatore et al. (2019), we design syn-\nthesized fragments to examine models’ understand-\ning ability of six types of linguistic and logic infer-\nence: boolean, comparative, conditional, count-\ning, negation and quantiﬁer, where each category\nhas 2-4 templates. See example templates and NLI\npairs in Table 3.\nThe data is generated using context-free gram-\nmar rules and a vocabulary of 80,000 person names\n(Chinese and transliterated), 8659 city names and\nexpanded predicates and comparative relations in\nRichardson et al. (2020) to make the data more\nchallenging. As a result, we generated 1,000 exam-\nples for each fragment. For quality control, each\ntemplate was checked by 3 linguists/logicians; also\n20 examples from each category were checked for\ncorrectness by local experts.\n4 Experimental setup\nOur main goal is to test whether cross-lingual trans-\nfer are robust against the adversarial and probing\ndata we created when evaluated without additional\ntraining. Thus we need to compare the best Chi-\nnese monolingual models with the best multilingual\nmodels trained either on English NLI data alone,\n8We use the BCC corpus (Xun et al., 2016): http://bcc.\nblcu.edu.cn/.\nor on combinations of Chinese and English data.9\nChinese monolingual models We experimented\nwith two current state-of-the-art transformer mod-\nels: RoBERTa-large (Liu et al., 2019) and Electra-\nlarge-discriminator (Clark et al., 2019). We use the\nChinese models released from (Cui et al., 2020)10\nimplemented the Huggingface Transformer library\n(Wolf et al., 2020).\nMultilingual model We use XLM-RoBERTa-\nlarge (Conneau et al., 2020). We choose XLM-R\nover mT5 (Xue et al., 2020) because XLM-R gen-\nerally performs better than mT5 under the same\nmodel size (see original paper for details). Also,\nXLM-R as a RoBERTa model is most related archi-\ntecturally to existing Chinese pre-trained models.\nFine-tuning data for Chinese models & XLM-\nR (1) XNLI: the full Chinese training set in the\nmachine-translated XNLI dataset, with 390k exam-\nples (Conneau et al., 2018). (2) XNLI-small: 50k\nexamples from XNLI, the same size as the train-\ning data of OCNLI. (3) OCNLI: Original Chinese\nNLI dataset (Hu et al., 2020). It is a Chinese NLI\ndataset collected from scratch, following the MNLI\nprocedure, with 50k training examples. We use this\nto measure the effect of the quality of training data;\nthat is, whether it is better to use small, high-quality\ntraining data (OCNLI), or large, low-quality MT\ndata (XNLI). (4) OCNLI + XNLI: a combination\nof the two training sets, 440k examples.\nFine-tuning data for XLM-R To examine cross-\nlingual transfer, we ﬁnetune XLM-R on English\nNLI data alone and English + Chinese NLI data: (1)\nMNLI: 390k examples from MNLI.train (Williams\net al., 2018). (2) English all NLI: we combine\nMNLI (Williams et al., 2018), SNLI (Bowman\net al., 2015), FeverNLI (Thorne et al., 2018; Nie\net al., 2019) with ANLI (Nie et al., 2020), a total\nof 1,313k examples. (3) OCNLI + English all NLI.\n(4) XNLI + English all NLI. These two are set to\nexamine whether combining Chinese and English\nﬁne-tuning data is helpful.\n9We also run the same experiments for Chinese-to-English\ntransfer, i.e., ﬁne-tuning XLM-R with OCNLI and evaluate\non the four English counterpart datasets. We ﬁnd that trans-\nferring from OCNLI to English does not perform as well as\nmonolingual English models, likely due to the small size of\nOCNLI. Detailed results are reported in Appendix C.\n10We usehfl/chinese-roberta-wwm-ext-large\nfrom https://github.com/ymcui/Chinese-BERT-wwm and\nhfl/chinese-electra-large-discriminator\nfrom https://github.com/ymcui/Chinese-ELECTRA.\n3775\ncategory premise hypothesis label\nNegation 库尔图尔只到过湛江市麻章区，丰隆格只到过大连市普兰店区. . . . . .\nperson1 only went tolocation1; person2 only went tolocation2; ....\n库尔图尔没到过大连市普兰店区。\nperson1 has not been tolocation2.\nE\nBoolean 何峥、管得宽、李国柱. . . . . .只到过临汾市襄汾县。\nperson1, person2 ... have only been tolocation1.\n何峥没到过遵义市红花岗区。\nperson1 has not been tolocation2.\nE\nQuantiﬁer 有人到过每一个地方，拥抱过每一个人。\nSomeone has been to every place and hugged every person.\n王艳没拥抱 过包一。person1 hasn’t hugged\nperson2.\nN\nCounting 韩声雄只拥抱过罗冬平、段秀芹. . . . . .赵常。\nperson1 only huggedperson2, person3 ... person8.\n韩声雄拥抱过超过10个人。\nperson1 hugged more than 10 people.\nC\nConditional . . . . . . ，穆肖贝夸到过赣州市定南县，如果穆肖贝夸没到过赣州市定南县，\n那么张本伟到过呼伦贝尔市阿荣旗。... personn has been to locationn. If\npersonn hasn’t been tolocationn, then personm has been tolocationm.\n张本伟没到过呼伦贝尔市阿荣旗。personm\nhasn’t been tolocationm.\nN\nComparative 龙银凤比武书瑾、卢耀辉. . . . . .奈德哈特都小，龙银凤和亚厄纳尔普一样\n大。person1 is younger thanperson2, ..., personn; person1 is as old aspersonm\n亚厄纳尔普比梁培娟大。personm is older than\npersonn−2.\nC\nTable 3: Example NLI pairs for semantic/logic probing with translations. Each label for each category has 2 to 4\ntemplates; we are only showing 1 template for 1 label. 1,000 examples are generated for each category.\nModel Fine-tuned on Acc Scenario\nRoBERTa zh MT: XNLI-small 67.44 monolingual\nRoBERTa zh MT: XNLI 70.29 monolingual\nRoBERTa zh ori: OCNLI 79.11 monolingual\nRoBERTa zh: OCNLI + XNLI 78.43 monolingual\nXLM-R zh MT: XNLI 72.55 monolingual\nXLM-R zh ori: OCNLI 79.24 monolingual\nXLM-R zh: OCNLI + XNLI 80.31 monolingual\nXLM-R en: MNLI 71.98 zero-shot\nXLM-R en: En-all-NLI 73.73 zero-shot\nXLM-R mix: OCNLI + En-all-NLI 82.18 mixed\nXLM-R mix: XNLI + En-all-NLI 74.12 mixed\nTable 4: Results on OCNLI dev. “ Scenario” indicates\nwhether the model is ﬁne-tuned on Chinese only data\n(monolingual), English data (zero-shot) or mixed En-\nglish and Chinese data; results in gray show best per-\nformance for each scenario. Best overall result in bold.\nSame below.\nWe ﬁne-tune the models on OCNLI-dev. Ac-\nknowledging that different training runs can pro-\nduce very different checkpoints for behavioral test-\ning (D’Amour et al., 2020), we run 5 models on\ndifferent seeds and report the mean accuracy of the\nmodels with the best hyper-parameter setting (for\ndetails see Appendix B).\n5 Results and discussion\n5.1 Results on OCNLI dev\nResults on the dev set of OCNLI are presented\nin Table 4. For monolingual RoBERTa, we see\na similar performance as reported in the OCNLI\npaper (Hu et al., 2020), with 79.11% accuracy. The\nmonolingual Electra achieves a very close accuracy\nof 79.02% (not shown in the Table). As we see the\nsame trend in the following experiments, we will\ntherefore only report results on RoBERTa.\nFor XLM-R, ﬁne-tuning on MNLI or En-all-NLI\ngives us reasonable results of around 72% to 74%,\nwhich is better than models ﬁne-tuned on XNLI, in-\ndicating that ﬁne-tuning on an English data (MNLI)\nalone can outperform monolingual models ﬁne-\ntuned on the same data but machine-translated into\nChinese (XNLI).11 This is consistent with previous\nresults on Korean (Choi et al., 2021) and Persian\n(Khashabi et al., 2020) for other NLU tasks.\nWhat is also interesting is that combining OC-\nNLI and En-all-NLI gives us a boost of 2% to\n82.18% (a result that is comparable to the current\npublished SOTA), showing the power of mixing\nhigh-quality English and Chinese training data.\n5.2 Chinese HANS\nTable 5 shows results of the Chinese HANS data\ntested on the aforementioned monolingual models\nand cross-lingual model.\nCross-lingual transfer achieves strong re-\nsults. We ﬁrst notice that when XLM-R is ﬁne-\ntuned solely on the English data (En-all-NLI), the\nperformance (∼69%) is only slightly worse than\nthe best monolingual model (∼71%). This suggests\nthat cross-lingual transfer from English to Chinese\nis quite successful for an adversarial dataset like\nHANS. Second, adding OCNLI to En-all-NLI in\nthe training data gives a big boost of about 9%,\nand achieves the overall best result. This is about\n12% higher than combining XNLI and the English\ndata, demonstrating the advantage of the expert-\nannotated OCNLI over machine translated XNLI,\neven though the latter is about 8 times the size of\nthe former. Despite these results, however, we note\nthat all models continue to perform below human\nperformance, suggesting more room for improve-\nment.\nOur results also suggest that examples involving\nthe sub-sequence heuristics are more difﬁcult than\n11For these experiments we also tested with another Chinese\nmachine-translated MNLI (CMNLI), translated by a different\nMT system, which was released by CLUE (https://github.\ncom/CLUEbenchmark/CLUE), and obtained similar results.\n3776\nModel Fine-tuned on Overall Lexical Overlap Sub-sequence Entailment Non-Entailment ∆\nRoBERTa zh MT: XNLI-small 49.48 58.12 25.42 99.22 30.26 37.18\nRoBERTa zh MT: XNLI 60.80 68.99 38.01 99.74 45.76 24.53\nRoBERTa zh ori: OCNLI 71.72 75.39 61.48 99.67 60.91 18.20\nRoBERTa zh: OCNLI+XNLI 69.33 74.73 54.27 99.89 57.51 20.92\nXLM-R zh ori: OCNLI 61.82 65.83 50.68 99.89 47.11 32.13\nXLM-R zh MT: XNLI 57.74 66.47 33.45 99.96 41.42 31.13\nXLM-R zh: OCNLI+XNLI 70.31 74.25 59.34 100.00 58.84 21.47\nXLM-R en: En-all-NLI 69.56 77.62 47.13 100.00 57.80 15.93\nXLM-R en: MNLI 66.74 73.12 48.97 100.00 53.89 18.09\nXLM-R mix: OCNLI+En-all-NLI 78.82 81.57 71.15 100.00 70.63 11.55\nXLM-R mix: XNLI+En-all-NLI 66.90 76.25 40.90 99.93 41.89 32.23\nHuman 98.00\nTable 5: Accuracy on Chinese HANS. ∆ =the difference of accuracy between OCNLI dev and Non-Entailment.\nthose targeting the lexical overlap heuristics for\nthe transformers models we tested (see the “sub-\nsequence” and “lexical overlap” columns in Ta-\nble 5). This is in line with the results reported in\nthe English HANS paper (speciﬁcally Table 15 in\nMcCoy et al. (2019) which also shows that the sub-\nsequence examples are more difﬁcult for the En-\nglish BERT model). Second, for the sub-sequence\nheuristics, results from monolingual model are 12%\nhigher than those from XLM-R under the zero-shot\ntransfer setting (61.48% versus 48.79% in “sub-\nsequence” column in Table 5). This stands in con-\ntrast with the lexical overlap heuristic, where the\nbest monolingual model performs similarly to the\nbest zero-shot cross-lingual transfer (75.39% ver-\nsus 77.62%). This is one of the few cases where\ncross-lingual transfer under-performs the monolin-\ngual setting by a large margin, suggesting that in\ncertain situations monolingual models may be pre-\nferred.\n5.3 Stress tests\nTable 6 presents the accuracies on all the stress\ntests. We ﬁrst see that cross-lingual zero-shot trans-\nfer using all English NLI data performs even better\nthan the best monolingual model (∼74% vs. ∼71%).\nThis demonstrates the power of the cross-lingual\ntransfer-learning. Adding OCNLI to all English\nNLI gives another increase of about 3 percentage\npoints (to 77%), while adding XNLI hurts the per-\nformance, again showing the importance of having\nexpert-annotated language-speciﬁc data.\nAntonyms and Synonyms All models except\nthose ﬁne-tuned on OCNLI achieved almost per-\nfect score on the synonym test. However, for\nantonyms, both monolingual and multilingual mod-\nels ﬁne-tuned with OCNLI perform better than\nXNLI. XLM-R ﬁne-tuned with English NLI data\nonly again outperforms the best of monolingual\nmodels (∼80% vs. ∼72%). Interestingly, adding\nXNLI to all English NLI data hurts the accuracy\nbadly (a 14% drop), while adding OCNLI to the\nsame English data improves the result slightly.\nAs antonyms are harder to learn (Glockner et al.,\n2018), we take our results to mean that either\nexpert-annotated data for Chinese or a huge English\nNLI dataset is needed for a model to learn decent\nrepresentations about antonyms, as indicated by\nthe high performance of RoBERTa ﬁne-tuned with\nOCNLI (71.81%), and XLM-R ﬁne-tuned with En-\nall-NLI (80.36%), on antonyms. That is, using\nmachine-translated XNLI will not work well for\nlearning antonyms (∼55% accuracy).\nDistraction Results in Table 6 show that adding\ndistractions to the hypotheses has a more nega-\ntive negative impact on models’ performance, com-\npared with appending distractions to the premises.\nThe difference is about 20% for all models (see\n“Distr H” columns and “Distr P” columns in Ta-\nble 6), which has not been reported in previous\nstudies, to the best of our knowledge. Including\na negation in the hypothesis makes it even more\nchallenging, as we see another one percent drop\nin the accuracy for all models. This is expected as\nprevious literature has demonstrated the key role\nnegation plays when hypotheses are produced by\nthe annotators (Poliak et al., 2018).\nSpelling This is another case where cross-lingual\ntransfer with English data alone falls behind mono-\nlingual Chinese models (by about 4%). Also the\nbest results are from ﬁne-tuning XLM-R with OC-\nNLI + XNLI, rather than a combination of En-\nglish and Chinese data. Considering the data is\ncreated by swapping Chinese characters with oth-\ners of the same pronunciation, we take it to suggest\n3777\nModel Fine-tuned on Overall Ant. Syn. Distr H Distr H-n Distr P Distr P-n Spell-\ning num.\nRoBERTa zh MT: XNLI-small 59.41 43.38 99.64 51.61 51.41 70.66 71.19 69.93 28.70\nRoBERTa zh MT: XNLI 66.22 52.28 99.79 54.83 53.8 74.55 74.57 72.22 53.53\nRoBERTa zh ori: OCNLI 64.49 71.81 73.66 52.95 51.8 73.43 73.86 71.79 54.16\nRoBERTa zh: OCNLI + XNLI 71.01 59.39 99.06 55.87 54.64 76.83 76.50 75.48 70.18\nXLM-R zh ori: OCNLI 69.08 71.29 88.63 55.93 55.05 76.84 77.00 71.42 65.51\nXLM-R zh MT: XNLI 66.87 55.53 99.96 56.11 55.29 77.69 77.9 74.37 46.81\nXLM-R zh: OCNLI + XNLI 71.49 61.85 99.45 58.15 57.92 79.16 79.28 77.93 61.88\nXLM-R en:MNLI 67.94 65.77 99.2 55.14 54.6 75.75 75.76 70.76 50.90\nXLM-R en: En-all-NLI 74.52 80.36 97.58 54.74 53.56 73.96 73.92 71.02 82.73\nXLM-R mix: OCNLI + En-all-NLI 77.36 81.93 95.09 59.23 58.00 79.88 79.92 74.53 87.77\nXLM-R mix: XNLI + En-all-NLI 73.57 66.15 99.68 57.02 55.51 78.38 78.53 75.15 80.33\nHuman 85.00 85.00 98.00 83.00 83.00 83.00 83.00 78.00 98.00\nTable 6: Accuracy on the stress test. Distr H/P(-n): distraction in Hypothesis/Premise (with negation).\nModel Fine-tuned on Overall\n∗Classiﬁer\n∗Idioms\n∗Non-core\narguement\n∗Pro-drop\n∗Time\nof event\nAnaphora\nArgument\nstructure\nCommon\nsense\nComparatives\nDouble\nnegation\nLexical\nsemantics\nMonotonicity\nNegation\nWorld\nknowledge\nRoBERTa zh MT: XNLI-small 62.9 65.8 64.7 55.2 80.5 60.0 59.6 67.4 54.3 61.4 48.3 60.9 59.7 66.2 39.0\nRoBERTa zh MT: XNLI 67.7 67.6 66.2 59.4 82.3 65.1 69.9 72.0 56.8 70.4 64.2 67.5 61.7 72.9 52.1\nRoBERTa zh ori: OCNLI 67.8 62.0 68.0 59.4 80.7 77.5 70.3 70.0 56.0 66.6 64.2 68.4 61.7 72.4 57.9\nRoBERTa zh: OCNLI + XNLI 69.3 66.3 67.1 58.6 83.0 74.0 70.1 73.5 54.9 74.1 67.5 69.1 62.5 76.0 60.0\nXLM-R zh ori: OCNLI 68.0 57.6 70.1 58.0 79.6 76.3 67.4 70.3 55.3 69.8 75.8 71.1 62.5 71.1 62.1\nXLM-R zh MT: XNLI 60.9 61.2 62.3 50.4 71.9 59.7 60.3 63.3 51.7 65.2 54.9 61.0 53.5 66.9 58.3\nXLM-R zh: OCNLI + XNLI 71.5 70.4 71.6 57.5 84.6 77.8 74.5 74.7 55.3 75.5 76.7 72.8 62.7 76.3 65.3\nXLM-R en: MNLI 70.2 70.1 73.9 57.5 86.4 70.8 69.3 72.9 48.9 76.0 62.5 67.8 62.6 77.0 62.1\nXLM-R en: En-all-NLI 71.9 71.8 74.3 56.2 87.4 75.7 74.9 74.8 49.1 80.5 70.8 69.1 63.8 77.8 64.2\nXLM-R mix: OCNLI + En-all-NLI74.9 72.7 74.3 60.1 88.5 84.5 77.3 78.1 56.6 81.3 79.2 77.2 65.6 78.0 67.9\nXLM-R mix: XNLI + En-all-NLI 71.4 70.2 58.5 85.5 71.3 75.2 75.5 55.1 79.2 70.0 69.1 62.4 76.2 72.1 71.3\nTable 7: Accuracy on the expanded diagnostics. Uniquely Chinese linguistic features are preﬁxed with ∗.\nthat monolingual models are still better at pick-\ning up the misspellings or learning the connections\nbetween characters at the phonological level.\nNumerical Reasoning Results in the last col-\numn of Table 6 suggest a similar pattern: using all\nEnglish NLI data for cross-lingual transfer outper-\nforms the best monolingual model. However, ﬁne-\ntuning a monolingual model with the small OCNLI\n(50k examples, accuracy: 54%) achieves better ac-\ncuracy than using a much larger MNLI (390k exam-\nples, accuracy: 51%) for cross-lingual transfer, al-\nthough both are worse than XLM-R ﬁne-tuned with\nall English NLI which has more than 1,000k exam-\nples (accuracy: 83%). This suggests that there are\ncases where a monolingual setting (RoBERTa with\nOCNLI) is competitive against zero-shot transfer\nwith a large English dataset (XLM-R with MNLI).\nHowever, that competitiveness may disappear when\nthe English dataset grows to an order of magnitude\nlarger in size or becomes more diverse (En-all-NLI\ncontains 4 different English NLI datasets).\n5.4 Hand-written diagnostics\nResults on the expanded diagnostics are presented\nin Table 7. We ﬁrst see that XLM-R ﬁne-tuned\nwith only English performs very well, at 70.2% and\n71.9%, even slightly higher than the best monolin-\ngual Chinese model (69.3%).\nMost surprisingly, in 3/5 categories with\nuniquely Chinese linguistic features, zero-\nshot transfer outperforms monolingual models.\nOnly in “non-core arguments” and “time of event”\ndo we see higher performance of OCNLI as the\nﬁne-tuning data. What is particularly striking is\nthat for “idioms ( Chengyu)”, XLM-R ﬁne-tuned\nonly on English data achieves the best result, sug-\ngesting that the cross-lingual transfer is capable of\nlearning meaning representation beyond the surface\nlexical information, at least for many of the idioms\nwe tested. The overall results (accuracy of 74.3%)\nindicate that cross-lingual transfer is very success-\nful in most cases. Manual inspection of the results\nshows that for many NLI pairs with idioms, XLM-\nR correctly predicts the ﬁgurative interpretation of\nthe idiom as entailment, and the literal interpreta-\ntion as non-entailment, as described in section 3.2.\nLooking at OCNLI and XNLI, we observe that they\nperform similarly when ﬁne-tuned on monolingual\nRoBERTa. However, when ﬁne-tuned with XLM-\nR, OCNLI has a clear advantage (68.0% versus\n60.9%), suggesting that OCNLI may produce more\n3778\nmodel ﬁnetune on overall boolean comparative conditional counting negation quantiﬁer\nRoBERTa zh MT: XNLI-small 46.57 32.81 34.41 61.48 81.82 33.27 35.63\nRoBERTa zh MT: XNLI 50.64 33.35 39.02 66.55 84.51 40.92 39.50\nRoBERTa zh ori: OCNLI 47.53 35.81 34.81 62.87 69.64 49.84 32.24\nRoBERTa zh: OCNLI + XNLI 51.13 38.16 37.98 66.19 75.73 53.31 35.43\nXLM-R zh ori: OCNLI 54.33 54.19 49.02 52.46 79.70 59.52 31.08\nXLM-R zh MT: XNLI 50.79 33.39 35.33 66.01 87.23 33.17 49.60\nXLM-R zh: OCNLI + XNLI 52.43 34.51 36.93 59.98 88.70 54.37 40.08\nXLM-R en: MNLI 49.09 33.27 37.98 66.25 89.70 34.69 32.65\nXLM-R en: En-all-NLI 55.37 33.43 39.70 66.65 92.34 64.11 35.99\nXLM-R mix: OCNLI + En-all-NLI 57.95 40.70 44.49 63.67 91.54 74.47 32.81\nXLM-R mix: XNLI + En-all-NLI 57.73 40.30 37.82 66.67 93.19 61.52 46.87\nTable 8: Accuracy on the Chinese semantic probing datasets, designed following Richardson et al. (2020).\nstable results than XNLI. Furthermore, when cou-\npled with English data to be used with XLM-R, we\nsee again that OCNLI + En-all-NLI results in an\naccuracy 3 percent higher than XNLI + En-all-NLI.\n5.5 Semantic fragments\nResults on the semantic probing datasets (shown in\nTable 8) are more mixed. First, the results are in\ngeneral much worse than the other evaluation data,\nbut overall, XLM-R ﬁne-tuned with OCNLI and\nall English data still performs the best. The over-\nall lower performance is likely due to the longer\nlength of premises and hypotheses in the semantic\nprobing datasets, compared with the other three\nevaluation sets. Second, zero-shot transfer is better\nor on par with monolingual Chinese RoBERTa in\n4/6 semantic fragments (except Boolean and quanti-\nﬁer). Third, for Boolean and comparative, XLM-R\nﬁne-tuned with OCNLI has a much better result\nthan all other monolingual models or XLM-R ﬁne-\ntuned with mixed data. We also observe that all\nmodels have highest performance on the “count-\ning” fragment. Note that none of the models have\nseen any data from the “counting” fragment during\nﬁne-tuning. That is, all the knowledge come from\nthe pre-training and ﬁne-tuning on general NLI\ndatasets. The surprisingly good performance of\nXLM-R model (w/ En-all-NLI, 92.34%) suggests\nthat it may have already acquired a mapping from\ncounting the words/names to numbers, and this\nknowledge can be transferred cross-linguistically.\n6 Conclusion and Future Work\nIn this paper, we examine the cross-lingual trans-\nfer ability of XLM-R in the context of Chinese\nNLI through four new sets of aversarial/probing\ntasks and a total of 17 new high quality and lin-\nguistically motivated challenge datasets. We ﬁnd\nthat multilingual transfer via ﬁne-tuning solely on\nbenchmark English data generally yields impres-\nsive performance. In 3/4 on our task categories,\nsuch zero-shot transfer outperforms our best mono-\nlingual models trained on benchmark Chinese NLI\ndata, including 3/5 of our hand-crafted challenge\ntasks that test uniquely Chinese linguistic phenom-\nena. These results suggest that multilingual models\nare indeed capable of considerable cross-lingual lin-\nguistic transfer and that zero-shot NLI may serve\nas a serious alternative to large-scale dataset devel-\nopment for new languages.\nThese results come with several important\ncaveats. Model performance is still outperformed\nby conservative estimates of human performance\nand our best models still have considerable room\nfor improvement; we hope that our new resources\nwill be useful for continuing to benchmark progress\non Chinese NLI. We also ﬁnd that high-quality Chi-\nnese NLI data (e.g., OCNLI) can help improve\nresults further, which suggests an important role\nfor certain types of expertly annotated monolingual\ndata in a training pipeline. In virtue of our study be-\ning limited to behavioral testing, the exact reason\nfor why cross-lingual zero-shot transfer generally\nperforms well, especially on some Chinese-speciﬁc\nphenomena, is an open question that requires fur-\nther investigation. In particular, we believe that\ntechniques that couple behavioral testing withinter-\nvention techniques (Geiger et al., 2020; Vig et al.,\n2020) and other analysis methods (Giulianelli et al.,\n2018; Belinkov and Glass, 2019) might provide in-\nsight, and that our new Chinese resources can play\nan important role in such future work.\nAcknowledgments\nThis research was supported in part by Lilly En-\ndowment, Inc., through its support for the Indi-\nana University Pervasive Technology Institute. He\nZhou is sponsored by China Scholarship Council.\n3779\nReferences\nHossein Amirkhani, Mohammad Azari Jafari, Azadeh\nAmirak, Zohreh Pourjafari, Soroush Faridan\nJahromi, and Zeinab Kouhkan. 2020. Farstail: A\nPersian Natural Language Inference Dataset. arXiv\npreprint arXiv:2009.08820.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of ACL.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of EMNLP.\nEmrah Budur, Rıza ¨Ozc ¸elik, Tunga Gungor, and\nChristopher Potts. 2020. Data and representation for\nturkish natural language inference. In Proceedings\nof EMNLP.\nWanxiang Che, Yunlong Feng, Libo Qin, and Ting Liu.\n2020. N-LTP: A open-source neural Chinese lan-\nguage technology platform with pretrained models.\narXiv preprint arXiv:2009.11616.\nHyunjin Choi, Judong Kim, Seongho Joe, Seungjai\nMin, and Youngjune Gwon. 2021. Analyzing zero-\nshot cross-lingual transfer in supervised nlp tasks.\nIn 2020 25th International Conference on Pattern\nRecognition (ICPR).\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. ELECTRA: Pre-\ntraining Text Encoders as Discriminators Rather\nThan Generators. In Proceedings of ICLR.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of ACL.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , pages\n7057–7067.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of EMNLP.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for Chinese natural language process-\ning. In Findings of EMNLP.\nAlexander D’Amour, Katherine Heller, Dan Moldovan,\nBen Adlam, Babak Alipanahi, Alex Beutel,\nChristina Chen, Jonathan Deaton, Jacob Eisen-\nstein, Matthew D Hoffman, et al. 2020. Un-\nderspeciﬁcation presents challenges for credibil-\nity in modern machine learning. arXiv preprint\narXiv:2011.03395.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of NAACL.\nAtticus Geiger, Kyle Richardson, and Christopher\nPotts. 2020. Neural natural language inference mod-\nels partially embed theories of lexical entailment and\nnegation. In Proceedings of BlackBoxNLP.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the hood: Using diagnostic classiﬁers to investi-\ngate and improve how language models track agree-\nment information. In Proceedings of BlackboxNLP,\npages 240–248.\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\n2018. Breaking NLI Systems with Sentences that\nRequire Simple Lexical Inferences. In Proceedings\nof ACL.\nEmily Goodwin, Koustuv Sinha, and Timothy\nO’Donnell. 2020. Probing linguistic systematicity.\nIn Proceedings of ACL, pages 1958–1969.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A\nSmith. 2018. Annotation artifacts in natural lan-\nguage inference data. In Proceedings of NAACL.\nYuta Hayashibe. 2020. Japanese realistic textual entail-\nment corpus. In Proceedings of LREC.\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra\nK¨ubler, and Lawrence Moss. 2020. OCNLI: Orig-\ninal Chinese Natural Language Inference. In Find-\nings of EMNLP.\nK Karthikeyan, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2019. Cross-lingual ability of multilin-\ngual bert: An empirical study. In Proceedings of\nICLR.\nDaniel Khashabi, Arman Cohan, Siamak Shakeri,\nPedram Hosseini, Pouya Pezeshkpour, Malihe\nAlikhani, Moin Aminnaseri, Marzieh Bitaab, Faeze\nBrahman, Sarik Ghazarian, et al. 2020. ParsiNLU:\nA suite of language understanding challenges for\nPersian. arXiv preprint arXiv:2012.06154.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nSciTaiL: A Textual Entailment Dataset from Science\nQuestion Answering. In Proceedings of AAAI.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020. Pre-training via paraphrasing. Advances in\nNeural Information Processing Systems, 33.\n3780\nCharles N Li, Sandra A Thompson, and Sandra A\nThompson. 1981. Mandarin Chinese: A Functional\nReference Grammar. Univ of California Press.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nR Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the Wrong Reasons: Diagnosing Syntactic\nHeuristics in Natural Language Inference. In Pro-\nceedings of ACL.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018.\nStress Test Evaluation for Natural Language Infer-\nence. In Proceedings of COLING.\nNikita Nangia and Samuel Bowman. 2019. Human vs.\nMuppet: A Conservative Estimate of Human Perfor-\nmance on the GLUE Benchmark. In Proceedings of\nACL.\nYixin Nie, Haonan Chen, and Mohit Bansal. 2019.\nCombining fact extraction and veriﬁcation with neu-\nral semantic matching networks. In Association for\nthe Advancement of Artiﬁcial Intelligence (AAAI).\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A New Benchmark for Natural Lan-\nguage Understanding. In Proceedings of ACL.\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2020.\nWhat the [mask]? making sense of language-speciﬁc\nBERT models. arXiv preprint arXiv:2003.02912.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of ACL.\nAdam Poliak. 2020. A survey on recognizing textual\nentailment as an NLP evaluation. In Proceedings of\nthe First Workshop on Evaluation and Comparison\nof NLP Systems.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis Only Baselines in Natural Language In-\nference. In Proceedings of *SEM.\nLivy Real, Erick Fonseca, and Hugo Gonc ¸alo Oliveira.\n2020. Organizing the ASSIN 2 shared task. In Pro-\nceedings of the ASSIN 2 Shared Task: Evaluating\nSemantic Textual Similarity and Textual Entailment\nin Portuguese.\nKyle Richardson, Hai Hu, Lawrence S Moss, and\nAshish Sabharwal. 2020. Probing Natural Language\nInference Models through Semantic Fragments. In\nProceedings of AAAI.\nSwarnadeep Saha, Yixin Nie, and Mohit Bansal. 2020.\nConjnli: Natural language inference over conjunc-\ntive sentences. In Proceedings of EMNLP.\nFelipe Salvatore, Marcelo Finger, and Roberto Hi-\nrata Jr. 2019. A logical-based corpus for cross-\nlingual evaluation. In Proceedings of the 2nd\nWorkshop on Deep Learning Approaches for Low-\nResource NLP (DeepLo 2019), pages 22–30.\nTianqi Sun. 2009. A Study on the License Pattern\nand Mechanism of Non-core Arguments in Man-\ndarin Chinese. Ph.D. thesis, Peking University.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nveriﬁcation. In Proceedings of NAACL.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Simas Sakenis, Jason\nHuang, Yaron Singer, and Stuart Shieber. 2020.\nCausal mediation analysis for interpreting neural\nnlp: The case of gender bias. arXiv preprint\narXiv:2004.12265.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. SuperGLUE: A\nStickier Benchmark for General-purpose Language\nUnderstanding Systems. In Advances in Neural In-\nformation Processing Systems.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of BlackboxNLP.\nGijs Wijnholds and Michael Moortgat. 2021. SICKNL:\nA Dataset for Dutch Natural Language Inference. In\nProceedings of EACL.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A Broad-Coverage Challenge Corpus for Sen-\ntence Understanding through Inference. In Proceed-\nings of NAACL.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of EMNLP.\nShijie Wu and Mark Dredze. Are all languages created\nequal in multilingual BERT? In Proceedings of the\n5th Workshop on Representation Learning for NLP.\n3781\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of EMNLP.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie\nCao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu,\nCong Yu, Yin Tian, Qianqian Dong, Weitang Liu,\nBo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao\nWang, Weijian Xie, Yanting Li, Yina Patterson,\nZuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua\nLiu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui\nZhang, Zhengliang Yang, Kyle Richardson, and\nZhenzhong Lan. 2020. CLUE: A Chinese language\nunderstanding evaluation benchmark. In Proceed-\nings of COLING.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nEndong Xun, Gaoqi Rao, Xiaoyue Xiao, and Jiaojiao\nZang. 2016. The construction of the BCC corpus in\nthe age of big data. Corpur Linguistics (in Chinese),\npages 93–109.\nHitomi Yanaka, Koji Mineshima, Daisuke Bekki, Ken-\ntaro Inui, Satoshi Sekine, Lasha Abzianidze, and Jo-\nhan Bos. 2019. Help: A dataset for identifying short-\ncomings of neural models in monotonicity reason-\ning. In Proceedings of *SEM, pages 250–255.\nWei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and\nJingming Liu. 2020. Ape210k: A large-scale and\ntemplate-rich dataset of math word problems. arXiv\npreprint arXiv:2009.11506.\n3782\nHeuristic entailment contradiction neutral\nlexical overlap 944 155 109\nsubsequence 190 10 18\nTable 9: Distribution of the two heuristics in OCNLI.\nHeuristic entailment contradiction neutral\nlexical overlap 441 647 340\nsubsequence 100 193 220\nTotal 541 840 560\nTable 10: Distributional statistics of our synthesized\nChinese HANS dataset.\nA Details for dataset creation\nIn this section, we list example NLI pairs and their\ntranslations. For examples of the Chinese HANS\nand stress tests, see Table 2. For the expanded\ndiagnostics, see Table 11. For the semantic/logic\nprobing dataset, see Table 3.\nChinese HANS Table 9 lists the number of ex-\namples in OCNLI for each inference label that sat-\nisfy the two heuristics we are examining. We ob-\nserve that entailment examples take the majority\nfor both heuristics. Therefore, we hypothesize that\nif the heuristics are learned, the entailment exam-\nples are likely to be correctly predicted while non-\nentailment (contradiction and neutral) examples\nare prone to receive wrong prediction.\nTo guarantee the generated sentences are syntac-\ntically and semantically sound, we add features for\nour vocabulary so that subject- predicate and verb-\nobject constraints are satisﬁed, e.g., some verbs\ncan only take animate subjects and objects. We\nthen generate 50 premise-hypothesis pairs for each\ntemplate described in our Github repository.12 Ex-\ncluding duplicated examples, our generated dataset\nhas 1,941 pairs and the distribution of the three\nlabels is shown in Table 10.\nAntonym After looking at the quality of initially\ngenerated data, we decided to replace only the\nnouns and adjectives with their antonyms since\nsuch replacements are most likely result in gram-\nmatical and contradictory hypotheses.13\n12https://github.com/huhailinguist/ChineseVariousNLI\n13We use the LTP toolkit (https://github.com/HIT-SCIR/ltp)\nto annotate the POS tags and our antonym list is from https:\n//github.com/liuhuanyong/ChineseAntiword.\nSynonym After inspecting the initially generated\ndata, we decided to perform replacements only to\nverbs and adjectives. To ensure the quality of syn-\nonyms, we rank the synonyms from a commonly\nused synonym dictionary by their vector similar-\nity to the original word, and pick the top ranking\nsynonym.14\nDistraction We created the distraction data simi-\nlar to the stress test setting (Naik et al., 2018) but ex-\nperimented with variations as to where “distractor\nstatement”—either a tautology or a true statement—\nwas added: the premise or the hypothesis. The\ndistractor statement also varied w.r.t. whether it\ncontains a negation:\n• Premise-no-negation: A distractor statement\nis added to the end of the premise and it con-\ntains no negation.\n• Premise-negation: A distractor statement\ncontaining a negation is added to the premise.\n• Hypothesis-no-negation: A distractor state-\nment is added to the end of the hypothesis.\n• Hypothesis-negation: Same as the previous\ncondition except that the distractor contains a\nnegation.\nOnly two tautologies are used in Naik et al.\n(2018). In this paper, to thoroughly examine the in-\nﬂuence of different true statements, we designed 50\ntautology/statements varied in three factors: length,\nout-of-vocabulary, and negation word. There are\n25 statements pairs in total (1 tautology and 24\ntrue statements); each pair includes a true state-\nment and its corresponding true statement with\nnegation form. All the statements range from 5\nto 16 characters. For the true statements in nega-\ntion form, two common Chinese negation words\n不 and 没 are used for negation. For the 24 true\nstatement pairs, half of them contains at least one\nOut-of-V ocabulary word in OCNLI.\nExperiments show that length, Out-of-\nV ocabulary words, and the choice of negator have\nlittle effects on the results.\nSpelling We generate a set of data containing\n“spelling errors” by replacing one random charac-\nter in the hypotheses with its homonym, which is\ndeﬁned as a character with the same pinyin pronun-\nciation ignoring tones. We also limit the frequency\n14We use the synonym list from https://github.com/\nKeson96/SynoCN and the similarity score from the\nPython package Synonyms at https://github.com/chatopera/\nSynonyms.\n3783\nof the homonym as within the range of 100 to 6000\nso that the character is neither too rare nor too fre-\nquent.\nNumerical reasoning We extracted sentences\nfrom Ape210k (Zhao et al., 2020), a large-scale\nmath word problem dataset containing 210K Chi-\nnese elementary school-level problems15. We gen-\nerate entailed, contradictory and neutral hypotheses\nfor each premise, with the rules below:\n1. Entailment: Randomly choose a number x\nand change it to y from the hypothesis. If the\ny > x, preﬁx it with one phrase that translate to\n“less than”; if y <x, preﬁx it with one phrase\nthat translate to “more than”.\nPremise: Mary types 110 words per minute.\nHypothesis: Mary types less than 510 words\nper minute.\n2. Contradiction: Perform either 1) randomly\nchoose a number x from the hypothesis and\nchange it; 2) randomly choose a number from\nthe hypothesis and preﬁx it with one phrase\nthat translate to “less than” or “more than”.\nPremise: Mary types 110 words per minute.\nHypothesis: Mary types 710 words per\nminute.\n3. Neutral: Reverse the corresponding entailed\npremise-hypothesis pairs.\nPremise: Mary types less than 510 words per\nminute. Hypothesis: Mary types 110 words\nper minute.\nThe result contains 2,871 unique premise sentences\nand 8,613 NLI pairs.\nDiagnostics The diagnstics for classiﬁers (or\nmeasure word) and non-core arguments are ex-\nplained in detail below (see examples in Table 11).\n1. classiﬁers (or measure word): in Chinese,\nwhen modiﬁed by a numeral, a noun must\nbe preceeded by a category of words called\nclassiﬁer. They can be semantically vacuous\nbut sometimes also carry semantic content:\n一匹狼 one pi wolf (one wolf); 一群狼 one\nqun wolf (one pack of wolves). Our examples\nrequire the model to understand the semantic\ncontent of the classiﬁers.\n15We split all problems into individual sentences and ﬁlter\nout sentences without numbers. Then we remove sentences\nwithout any named entities (“PERSON”, “LOCATION” and\n“ORGANIZATION”) using the NER tool provided by LTP\ntoolkit (Che et al., 2020).\n2. non-core arguments: in Chinese syntax, some-\ntimes a noun phrase at the argument position\n(e.g., object) is not serving as an object: 今\n天吃筷子，不吃叉子。today eat chopsticks,\nnot eat fork (We eat with chopsticks today,\nnot with fork). Sun (2009) shows that this\nstructure is very productive in Chinese and we\ntake example sentences from her dissertation.\nAdditionally, for the pro-drop examples, they\nare constructed such that the models return the cor-\nrect inference relation only when they successfully\nidentify what the dropped pro refers to. That is,\nour constructed premises involve several entities\nthe dropped pro could potentially refer to, and the\nentailed hypothesis identiﬁes the correct referent\nwhile the neutral/contradictory hypothesis does not\n(see Table 11 for an example).\nB Hyperparameters for experiments\nTable 12 presents the hyperparameters used for\nthe models. The learning-rate search space for\nRoBERTa is: 1e-5, 2e-5, 3e-5, 4e-5 and 5e-5, for\nXLM-R: 5e-6, 7e-6, 9e-6, 2e-5 and 5e-5.\nC Chinese-to-English transfer\nWe present Chinese-to-English transfer results in\nthis section. As mentioned in the main text, for\nmost of the cases, zero-shot transfer learning does\nnot work well mostly likely due to the small size of\nOCNLI. However, for 3 out of the 4 datasets, XLM-\nR ﬁne-tuned with the mix data outperforms the\nmonolingual setting, suggesting that even OCNLI\nis only 1/20 of En-all-NLI, XLM-R can still acquire\nsome useful information from OCNLI, in addition\nto what is present in En-all-NLI.\nSpeciﬁcally, (1) for English HANS, XLM-R ﬁne-\ntuned with OCNLI is about 13 percentage points\nbelow the best English monolingual model, shown\nin Table 13. (2) For stress tests shown in Table 14,\nthe gap is about 5 percent (XLM-R with OCNLI\n= 74%; RoBERTa with En-all-NLI = 79%). Inter-\nestingly, XLM-R with OCNLI performs the best\nfor Negation and Word overlap. It even outper-\nforms RoBERTa w/ MNLI on the Antonym, which\nseems to be consistent with the high performance of\nOCNLI-trained models on the Chinese Antonym\nin our constructed stress tests. (3) For semantic\nprobing data, as shown in Table 15, XLM-R with\nOCNLI is 5 percent behind monolingual model\nﬁne-tuned with all English NLI, but performs bet-\nter than the monolingual RoBERTa ﬁne-tuned with\n3784\ncategory n premise hypothesis label\nCLUE (Xu et al., 2020) 514 有些学生喜欢在公共澡堂里唱歌。\nSome students like to sing in public\nshowers.\n有些女生喜欢在公共澡堂里唱歌。\nSome female students like to sing in pub-\nlic showers.\nN\nCLUE expansion (ours) 800 雷克雅未克所有旅馆的床位加在一\n起才一千六百个。\nThere are only one thousand six hundred\nbeds in all hotels in Reykjavik.\n雷克雅未克有旅馆的床位超过一千\n个。\nSome hotel in Reykjavik has over a thou-\nsand beds.\nN\nWorld Knowledge\n(ours)\n37 上海在北京的南边。\nShanghai is to the south of Beijing.\n北京在上海的南边。\nBeijing is to the south of Shanghai.\nC\nClassiﬁer (ours) 138 这些孩子吃了一个苹果。\nThese children ate an apple.\n这些孩子吃了一筐苹果。\nThese children ate a basket of apples.\nN\nChengyu/idioms (ours) 250 这帮人可狡猾得很啊，你一个电\n话打过去，打草惊蛇，后果不堪设\n想。\nThese people are so cunning! If you call\nthem, it would alert them, and as we say\nin a Chinese idiom ”if you hit the grass,\nit would alert the snakes.” The conse-\nquences would be unimaginable.\n你打电话过去会让这帮人察觉，造\n成不好的结果。\nIf you call them, it will alert them, and\nbring negative consequences.\nE\nsame as above 这些狡猾的人养了很多蛇。\nThese cunning people have raised a lot\nof snakes.\nN\nPro-drop (ours) 197 见了很多学生，又给老师们开了两\n个小时会，校长和主任终于可以下\n班了。\nAfter (pro) meeting many students and\n(pro) having two hours of meeting with\nthe teachers, the principal and the direc-\ntor can ﬁnally get off work.\n校长见了很多学生。\nThe principal met many students.\nE\nsame as above 老师们见了很多学生。\nThe teachers met many students.\nN\nNon-core arguments\n(ours)\n185 平时范志毅都踢后卫的，今天却改\n当前锋了。\nZhiyi Fan usually plays full back in soc-\ncer, but today he switched to playing\nforward.\n范志毅经常用腿踢对方的后卫。\nZhiyi Fan usually uses his legs to kick\nthe other team’s full back.\nN\nTable 11: Example NLI pairs in expanded diagnostics with translations.\nModel Training Data LR\nRoBERTa zh MT: XNLI-small 3e-05\nRoBERTa zh MT: XNLI 2e-05\nRoBERTa zh ori: OCNLI 2e-05\nRoBERTa zh: OCNLI + XNLI 3e-05\nXLM-R zh ori: OCNLI 5e-06\nXLM-R zh MT: XNLI 7e-06\nXLM-R zh: OCNLI + XNLI 7e-06\nXLM-R en:MNLI 5e-06\nXLM-R en: En-all-NLI 7e-06\nXLM-R mix: OCNLI + En-all-NLI 7e-06\nXLM-R mix: XNLI + En-all-NLI 7e-06\nTable 12: Hyper-parameters used for ﬁne-tuning the\nmodels. All models are ﬁne-tuned for 3 epochs with\nmaximum length of 128.\nMNLI (53.6% vs. 51.3%). This is quite surprising\nsince the size of OCNLI is only 1/8 of MNLI. (4)\nFor the English diagnostics as shown in Table 16\nand Table 17, XLM-R with OCNLI is 7 percent\nbehind RoBERTa ﬁne-tuned with MNLI.\nWe leave it for future work to thoroughly ex-\namine transfer learning from a “low-resource” lan-\nguage such as Chinese to the high-resource one\nsuch as English.\n3785\nModel Fine-tuned on Overall Lexical overlap Subsequence Constituent Entailment Non-entailment\nRoBERTa en: En-all-NLI 76.54 96.79 67.77 65.06 99.81 53.27\nRoBERTa en: MNLI 77.63 95.60 68.08 69.21 99.74 55.52\nXLM-R en: En-all-NLI 75.72 95.52 62.99 68.63 99.91 51.52\nXLM-R en: MNLI 74.80 92.92 65.24 66.23 98.83 50.76\nXLM-R zh ori: OCNLI 64.37 71.28 54.42 67.41 98.39 30.35\nXLM-R zh MT: XNLI 68.83 81.67 62.07 62.74 99.13 38.53\nXLM-R zh mix: OCNLI+XNLI 71.30 82.52 61.72 69.66 99.08 43.52\nXLM-R mix: OCNLI+En-all-NLI78.56 96.92 64.91 73.84 99.92 57.20\nXLM-R mix: XNLI+En-all-NLI 74.65 93.93 60.97 69.04 99.96 49.34\nTable 13: Results of English HANS (McCoy et al., 2019).\nModel Fine-tuned on Overall Antonym\nContent\nword\nswap\nFunction\nword\nswap Keyboard SwapLength\nmismatchNegationNumerical\nreasoning\nWord\noverlap\nRoBERTa en: En-all-NLI 79.48 82.91 86.22 88.71 87.8 87.48 88.28 60.25 79.26 62.85\nRoBERTa en: MNLI 77.9 69.03 85.74 88.75 87.39 87.05 88.23 59.19 65.46 61.48\nXLM-R en: En-all-NLI 79.6 86.25 85.26 87.38 86.31 86.72 87.25 61.06 82.84 65.79\nXLM-R en: MNLI 77.6 74.65 85.09 87.33 86.08 86.42 86.96 60.95 54.66 65.13\nXLM-R zh ori: OCNLI 74.31 72.52 75.12 77.71 76.27 76.39 77.23 72.86 55.85 72.79\nXLM-R zh MT: XNLI 77.78 65.12 85.11 86.64 85.79 85.71 85.91 63.52 43.95 71.63\nXLM-R zh mix: OCNLI+XNLI 77.83 66.83 84.96 86.69 85.81 85.87 85.98 63.97 51.56 68.38\nXLM-R mix: OCNLI+En-all-NLI80.01 86.33 85.22 87.40 86.26 86.77 87.23 62.52 81.79 67.54\nXLM-R mix: XNLI+En-all-NLI 79.38 85.27 85.35 87.20 86.28 86.74 87.22 60.29 80.50 66.19\nTable 14: Results of English stress test (Naik et al., 2018).\nModel Fine-tuned on Overall Boolean Comparative Conditional CountingMonotonicity\nhard\nMonotonicity\nsimple Negation Quantiﬁer\nRoBERTa en: MNLI 51.31 43.58 39.60 66.24 63.34 61.28 60.10 37.26 39.08\nRoBERTa en: En-all-NLI 58.72 60.18 40.28 66.30 66.22 59.60 58.98 64.46 53.74\nXLM-R en: MNLI 53.54 59.16 41.62 66.30 61.72 63.26 62.82 33.52 39.92\nXLM-R en: En-all-NLI 59.85 71.58 45.18 66.30 60.40 63.86 62.02 65.68 43.78\nXLM-R zh ori: OCNLI 53.61 66.02 60.62 41.10 58.00 47.86 49.88 51.88 53.50\nXLM-R zh MT: XNLI 52.29 43.24 39.00 66.22 65.66 58.08 62.74 34.12 49.24\nXLM-R zh mix: OCNLI+XNLI 54.68 54.64 38.84 66.28 67.38 58.18 61.38 41.88 48.82\nXLM-R mix: OCNLI+En All NLI60.20 71.20 42.58 66.30 62.40 64.72 60.90 68.58 44.88\nXLM-R mix: XNLI+En-all-NLI 60.06 65.8 46.86 66.30 65.54 61.56 61.44 68.50 44.48\nTable 15: Results of English semantic probing datasets (Richardson et al., 2020).\nModel Fine-tuned on Overall\nactivepassive\nanaphoracoreference\ncommonsense\nconditionals\nanaphoracoreference\ncoordinationscope\ncore args\ndatives\ndisjunction\ndoublenegation\ndownwardmono-tone\nellipsisimplicits\nexistential\nfactivity\ngenitivespartitives\nintersectivityRoBERTa en: MNLI 66.87 62.35 67.59 69.4762.50 78.00 63.50 69.62 85.00 39.4792.86 19.3365.29 65.00 62.0695.0060.43RoBERTa en: En-all-NLI68.0361.76 70.00 69.3363.75 82.50 68.00 75.77 85.00 41.5892.14 18.6767.6565.0062.3594.00 59.57XLM-R en: MNLI 63.03 61.76 62.76 59.73 55.62 76.00 61.50 61.54 85.00 26.84 91.43 16.00 64.12 69.00 51.47 90.00 60.00XLM-R en: En-all-NLI 64.57 61.76 65.17 61.47 60.00 76.00 66.00 65.77 85.00 33.16 89.29 14.00 62.9471.0058.53 90.0060.87XLM-R zh ori: OCNLI 59.67 60.00 59.31 57.20 58.12 70.00 56.50 61.54 85.00 30.00 67.14 17.33 54.71 66.00 46.18 90.00 59.57XLM-R zh MT: XNLI 61.76 61.18 64.14 60.67 58.75 72.50 60.00 60.77 85.00 33.16 91.43 12.67 58.24 64.00 48.24 90.00 57.83XLM-R zh mix: OCNLI+XNLI 61.78 61.76 62.76 56.93 57.50 74.50 61.00 61.54 85.00 31.05 90.00 12.00 57.65 65.00 48.53 90.00 57.39XLM-R mix: OCNLI+En-all-NLI 64.51 61.76 63.45 61.60 58.75 76.00 66.00 67.31 85.00 35.26 90.71 15.33 60.59 68.00 60.59 91.00 60.87XLM-R mix: XNLI+En All LI 64.37 61.18 64.83 62.27 61.88 73.00 65.00 65.38 85.00 35.26 91.43 14.67 63.53 70.00 57.94 94.00 60.87\nTable 16: Results of English Diagnostics from GLUE-Part I (Wang et al., 2018).\nModel Fine-tuned on\nintervalsnumbers\nlexicalentailment\nmorphologicalnegation\nnamedentities\nnegation\nnominalization\nnon-monotone\nprepositionalphrases\nquantiﬁers\nredundancy\nrelativeclauses\nrestrictivity\nsymmetrycollectivity\ntemporal\nuniversal\nupwardmonotone\nworldknowledgeRoBERTa en: MNLI 54.74 66.71 89.23 57.22 66.59 82.14 56.00 86.18 78.46 79.23 63.75 55.38 67.86 56.2584.4476.47 48.51RoBERTa en: En-all-NLI63.16 71.57 89.2361.1169.0284.2957.3384.41 74.62 73.85 63.75 53.0870.00 69.3883.33 73.5349.55XLM-R en: MNLI 45.79 65.57 84.62 61.11 61.95 82.14 52.00 85.8882.6978.46 62.50 52.31 57.14 51.25 80.00 74.12 44.03XLM-R en: En-all-NLI 45.79 69.71 84.62 61.67 64.1585.7148.67 84.71 79.23 69.23 63.12 46.15 59.29 60.00 77.78 75.29 45.82XLM-R zh ori: OCNLI 39.47 56.29 75.38 41.11 53.41 73.57 51.33 85.59 63.08 83.08 62.5070.7764.29 54.38 62.2278.8247.31XLM-R zh MT: XNLI 42.11 60.14 84.62 61.11 61.95 74.29 53.3386.7673.46 84.62 60.62 60.00 57.86 40.6284.4468.24 47.31XLM-R zh mix: OCNLI+XNLI 43.68 59.29 83.0863.3362.20 74.29 52.0086.7676.9285.3862.50 60.77 59.29 43.75 82.22 67.65 47.61XLM-R mix: OCNLI+En-all-NLI 45.26 69.86 85.38 62.22 65.12 85.71 50.67 85.00 74.62 69.2368.1247.69 60.71 56.25 77.78 75.29 45.67XLM-R mix: XNLI+En All LI 44.21 67.29 86.15 62.22 63.90 83.57 49.33 84.71 75.00 70.77 66.88 46.92 58.57 57.50 74.44 74.12 45.82\nTable 17: Results of English Diagnostics from GLUE-Part II (Wang et al., 2018).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8065847158432007
    },
    {
      "name": "Natural language processing",
      "score": 0.6963992714881897
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6614345908164978
    },
    {
      "name": "Inference",
      "score": 0.5906155109405518
    },
    {
      "name": "Natural language",
      "score": 0.538666844367981
    },
    {
      "name": "Transfer of learning",
      "score": 0.49586257338523865
    },
    {
      "name": "Universal Networking Language",
      "score": 0.43830451369285583
    },
    {
      "name": "Language model",
      "score": 0.43124476075172424
    },
    {
      "name": "Comprehension approach",
      "score": 0.15138089656829834
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210119109",
      "name": "Indiana University Bloomington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I114027177",
      "name": "University of North Carolina at Chapel Hill",
      "country": "US"
    }
  ],
  "cited_by": 11
}