{
  "title": "Dependency-Based Bilingual Language Models for Reordering in Statistical Machine Translation",
  "url": "https://openalex.org/W2124322414",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2250830047",
      "name": "Ekaterina Garmash",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2109806231",
      "name": "Christof Monz",
      "affiliations": [
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2437005631",
    "https://openalex.org/W1969974515",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2796084947",
    "https://openalex.org/W122999227",
    "https://openalex.org/W2251843378",
    "https://openalex.org/W2116792345",
    "https://openalex.org/W126222424",
    "https://openalex.org/W1996430422",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2160382364",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2001064229",
    "https://openalex.org/W2989631226",
    "https://openalex.org/W2150378737",
    "https://openalex.org/W2016522586",
    "https://openalex.org/W2153653739",
    "https://openalex.org/W2097997328",
    "https://openalex.org/W2131367528",
    "https://openalex.org/W2144879357",
    "https://openalex.org/W2095755718",
    "https://openalex.org/W2158953777",
    "https://openalex.org/W2166905217",
    "https://openalex.org/W4241645538",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2100281225",
    "https://openalex.org/W2149709850",
    "https://openalex.org/W2142632103",
    "https://openalex.org/W2171421863",
    "https://openalex.org/W2168360976",
    "https://openalex.org/W2127318480",
    "https://openalex.org/W2112900913",
    "https://openalex.org/W2167302977",
    "https://openalex.org/W1828578481",
    "https://openalex.org/W2095650036",
    "https://openalex.org/W2092654472",
    "https://openalex.org/W4254408171",
    "https://openalex.org/W2013540053",
    "https://openalex.org/W2180952760",
    "https://openalex.org/W222053410",
    "https://openalex.org/W1510052640",
    "https://openalex.org/W2156985047"
  ],
  "abstract": "This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs).Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm.The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments.The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by Niehues et al. (2011).Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM.An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit.",
  "full_text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1689–1700,\nOctober 25-29, 2014, Doha, Qatar.c⃝2014 Association for Computational Linguistics\nDependency-Based Bilingual Language Models for\nReordering in Statistical Machine Translation\nEkaterina Garmash and Christof Monz\nInformatics Institute, University of Amsterdam\nScience Park 904, 1098 XH Amsterdam, The Netherlands\n{e.garmash,c.monz}@uva.nl\nAbstract\nThis paper presents a novel approach to\nimprove reordering in phrase-based ma-\nchine translation by using richer, syntac-\ntic representations of units of bilingual\nlanguage models (BiLMs). Our method\nto include syntactic information is simple\nin implementation and requires minimal\nchanges in the decoding algorithm. The\napproach is evaluated in a series of Arabic-\nEnglish and Chinese-English translation\nexperiments. The best models demon-\nstrate signiﬁcant improvements in BLEU\nand TER over the phrase-based baseline,\nas well as over the lexicalized BiLM by\nNiehues et al. (2011). Further improve-\nments of up to 0.45 BLEU for Arabic-\nEnglish and up to 0.59 BLEU for Chinese-\nEnglish are obtained by combining our de-\npendency BiLM with a lexicalized BiLM.\nAn improvement of 0.98 BLEU is ob-\ntained for Chinese-English in the setting of\nan increased distortion limit.\n1 Introduction\nIn statistical machine translation (SMT) reorder-\ning (also called distortion) refers to the order in\nwhich source words are translated to generate the\ntranslation in the target language. Word orders\ncan differ signiﬁcantly across languages. For in-\nstance, Arabic declarative sentences can be verb-\ninitial, while the corresponding English translation\nshould realize the verb after the subject, hence re-\nquiring a reordering. Determining the correct re-\nordering during decoding is a major challenge for\nSMT. This problem has received a lot of attention\nin the literature (see, e.g., Tillmann (2004), Zens\nand Ney (2003), Al-Onaizan and Papineni (2006)),\nas choosing the correct reordering improves read-\nability of the translation and can have a substan-\ntial impact on translation quality (Birch, 2011). In\nthis paper, we only consider those approaches that\ninclude a reordering feature function into the log-\nlinear interpolation used during decoding.\nThe simplest reordering model is linear distor-\ntion (Koehn et al., 2003) which scores the distance\nbetween phrases translated at steps tand t+ 1of\nthe derivation. This model ignores any contex-\ntual information, as the distance between trans-\nlated phrases is its only parameter. Lexical dis-\ntortion modeling (Tillmann, 2004) conditions re-\nordering probabilities on the phrase pairs trans-\nlated at the current and previous steps. Unlike\nlinear distortion, it characterizes reordering not in\nterms of distance but type: monotone, swap, or\ndiscontinuous.\nIn this paper, we base our approach to reorder-\ning on bilingual language models (Marino et al.,\n2006; Niehues et al., 2011). Instead of directly\ncharacterizing reordering, they model sequences\nof elementary translation events as a Markov pro-\ncess.1 Originally, Marino et al. (2006) used this\nkind of model as the translation model, while more\nrecently it has been used as an additional model\nin PBSMT systems (Niehues et al., 2011). We\nadopt and generalize the approach of Niehues et al.\n(2011) to investigate several variations of bilingual\nlanguage models. Our method consists of labeling\nelementary translation events (tokens of bilingual\nLMs) with their different contextual properties.\nWhat kind of contextual information should be\nincorporated in a reordering model? Lexical in-\nformation has been used by Tillmann (2004) but\nis known to suffer from data sparsity (Galley and\nManning, 2008). Also previous contributions to\nbilingual language modeling (Marino et al., 2006;\nNiehues et al., 2011) have mostly used lexical\ninformation, although Crego and Yvon (2010a)\nand Crego and Yvon (2010b) label bilingual to-\n1Note that the standard PBSMT translation model as-\nsumes that events of translating separate phrases in a sentence\nare independent.\n1689\nkens with a rich set of POS tags. But in gen-\neral, reordering is considered to be a syntactic phe-\nnomenon and thus the relevant features are syn-\ntactic (Fox, 2002; Cherry, 2008). Syntactic in-\nformation is incorporated in tree-based approaches\nin SMT, allowing one to provide a more detailed\ndeﬁnition of translation events and to redeﬁne de-\ncoding as parsing of a source string (Liu et al.,\n2006; Huang et al., 2006; Marton and Resnik,\n2008), of a target string (Shen et al., 2008), or\nboth (Chiang, 2007; Chiang, 2010). Reordering\nis a result of a given derivation, and CYK-based\ndecoding used in tree-based approaches is more\nsyntax-aware than the simple PBSMT decoding\nalgorithm. Although tree-based approaches poten-\ntially offer a more accurate model of translation,\nthey are also a lot more complex and requiring\nmore intricate optimization and estimation tech-\nniques (Huang and Mi, 2010).\nOur idea is to keep the simplicity of PBSMT but\nmove towards the expressiveness typical of tree-\nbased models. We incrementally build up the syn-\ntactic representation of a translation during decod-\ning by adding precomputed fragments from the\nsource parse tree. The idea to combine the mer-\nits of the two SMT paradigms has been proposed\nbefore, where Huang and Mi (2010) introduce in-\ncremental decoding for a tree-based model. On a\nvery general level, our approach is similar to theirs\nin that it keeps track of a sequence of source syn-\ntactic subtrees that are being translated at consec-\nutive decoding steps. An important difference is\nthat they keep track of whether the visited subtrees\nhave been fully translated, while in our approach,\nonce a syntactic structural unit has been added to\nthe history, it is not updated anymore.\nIn this paper, we focus on source syntactic in-\nformation. During decoding we have full access\nto the source sentence, which allows us to obtain\na better syntactic analysis (than for a partial sen-\ntence) and to precompute the units that the model\noperates with. We investigate the following re-\nsearch questions: How well can we capture re-\nordering regularities of a language pair by incor-\nporating source syntactic parameters into the units\nof a bilingual language model? What kind of\nsource syntactic parameters are necessary and suf-\nﬁcient?\nOur contributions can be summarized as fol-\nlows: We argue that the contextual information\nused in the original bilingual models (Niehues et\nal., 2011) is insufﬁcient and introduce a simple\nmodel that exploits source-side syntax to improve\nreordering (Sections 2 and 3). We perform a thor-\nough comparison between different variants of our\ngeneral model and compare them to the original\napproach. We carry out translation experiments\non multiple test sets, two language pairs (Arabic-\nEnglish and Chinese-English), and with respect to\ntwo metrics (BLEU and TER). Finally, we present\na preliminary analysis of the reorderings resulting\nfrom the proposed models (Section 4).\n2 Motivation\nIn this section, we elaborate on our research ques-\ntions and provide background for our approach.\nWe also discuss existing bilingual n-gram mod-\nels and argue that they are often not expressive\nenough to differentiate between alternative re-\norderings. We should ﬁrst note that the most com-\nmonly used n-gram model to distinguish between\nreorderings is a target language model, which does\nnot take translation correspondence into account\nand just models target-side ﬂuency. Al-Onaizan\nand Papineni (2006) show that target language\nmodels by themselves are not sufﬁcient to cor-\nrectly characterize reordering. In what follows we\nonly discuss bilingual models.\nThe word-aligned sentence pair in Figure 1.a 2\ndemonstrates a common Arabic-English reorder-\ning. As stated in the introduction, bilingual lan-\nguage models capture reordering regularities as a\nsequence of elementary translation events3. In the\ngiven example, one could decompose the sequen-\ntial process of translation as follows: First trans-\nlate the ﬁrst wordAlwzyr as the minister, then ArjE\nas attributed, then ArtfAE as the increase and so\non. The sequence of elementary translation events\nis modeled as an n-gram model (Equation 1, where\nti is a translation event). There are numerous ways\nin which ti can be deﬁned. Below we ﬁrst discuss\nhow they have been deﬁned within previous ap-\nproaches, and then introduce our deﬁnition.\np(t1,...,t m) =\nm∏\ni=1\np(ti|ti−n+1 ...t i−1) (1)\n2.1 Lexicalized bilingual LMs\nBy including both source and target information\ninto the representation of translation events we ob-\n2We used Buckwalter transliteration for Arabic words.\n3By an elementary translation event we mean a translation\nof some substructure of a sentence.\n1690\nthe minister attributed the increase of oil prices\nw ArjE Alwzyr ArtfAE AsEAr Albtrwl\n(a) The original word alignment.\nthe\nAlwzyr\nminister\nAlwzyr\nattributed\nArjE\nthe\nArtfAE\nincrease \nArtfAE\nof \nempty\noil\nAlbtrwl\nprices\nAsEAr\n(b) BiLM tokens extracted from sentence (a).\nempty\nw\nof oil\nAlbtrwl\nprices\nAsEAr\nthe minister\nAlwzyr ArjE\nthe the increase \nArtfAE (c) MTU tokens extracted from sentence (a).\nFigure 1: Arabic-English parallel sentence, automatically word-aligned. The bilingual token sequences\nare produced according to two alternative deﬁnitions (BiLM and MTU).\ntain a bilingual LM. The richer representation al-\nlows for a ﬁner distinction between reorderings.\nFor example, Arabic has a morphological marker\nof deﬁniteness on both nouns and adjectives. If\nwe ﬁrst translate a deﬁnite adjective and then an\nindeﬁnite noun, it will probably not be a likely se-\nquence according to the translation model. This\nkind of intuition underlies the model of Niehues et\nal. (2011), a bilingual LM(BiLM), which deﬁnes\nelementary translation events t1,...,t n as follows:\nti = ⟨ei,{f|f ∈A(ei)}⟩, (2)\nwhere ei is the i-th target word and A : E →\nP(F) is an alignment function, E and F refer-\nring to target and source sentences, andP(·) is the\npowerset function. In other words, the i-th trans-\nlation event consists of the i-th target word and all\nsource words aligned to it. Niehues et al. (2011)\nrefer to the deﬁned translation events ti as bilin-\ngual tokensand we adopt this terminology.\nThere are alternative deﬁnitions of bilingual\nlanguage models. Our choice of the above deﬁ-\nnition is supported by the fact that it produces an\nunambiguous segmentation of a parallel sentence\ninto tokens. Ambiguous segmentation is unde-\nsirable because it increases the token vocabulary,\nand thus the model sparsity. Another disadvan-\ntage comes from the fact that we want to compare\npermutations of the same set of elements. For ex-\nample, the two different segmentations of bainto\n[ba] and [b][a] still represent the same permuta-\ntion of the sequence ab. In Figure 1 one can pro-\nduce a segmentation of (AsEAr Albtrwl, oil prices)\ninto ( Albtrwl, oil) and ( AsEAr, prices) or leave\nit as is. If we allow for both segmentations, the\nlearnt probability parameters may be different for\nthe sum of ( Albtrwl, oil) and (AsEAr, prices) and\nfor the unsegmented phrase.\nDurrani et al. (2011) introduce an alternative\nmethod for unambiguous bilingual segmentation\nwhere tokens are deﬁned as minimal phrases,\ncalled minimal translation units (MTUs). Figure 1\ncompares the BiLM and MTU tokenization for a\nspeciﬁc example. Since Niehues et al. (2011) have\nshown their model to work successfully as an addi-\ntional feature in combination with commonly used\nstandard phrase-based features, we use their ap-\nproach as the main point of reference and base our\napproach on their segmentation method. In the\nrest of the text we refer to Niehues et al. (2011)\nas the original BiLM.4 At the same time, we do\nnot see any speciﬁc obstacles for combining our\nwork with MTUs.\n2.2 Suitability of lexicalized BiLM to model\nreordering\nAs mentioned in the introduction, lexical informa-\ntion is not very well-suited to capture reordering\nregularities. Consider Figure 2.a. The extracted\nsequence of bilingual tokens is produced by align-\ning source words with respect to target words (so\nthat they are in the same order), as demonstrated\nby the shaded part of the picture. If we substituted\nthe Arabic translation of Egyptian for the Arabic\ntranslation of Israeli, the reordering should remain\nthe same. What matters for reordering is the syn-\ntactic role or context of a word. By using unneces-\nsarily ﬁne-grained categories we risk running into\nsparsity issues.\nNiehues et al. (2011) also described an alterna-\ntive variant of the original BiLM, where words are\nsubstituted by their POS tags (Figure 2.a, shaded\npart). Also, however, POS information by itself\nmay be insufﬁciently expressive to separate cor-\n4Although, strictly speaking, it is not the original ap-\nproach (see the references in Section 1).\n1691\nEgyptian exports to\ntrAjEt SAdrAt mSr l Aldwl AlErbyp\nVBD NNS NNP IN DTNN DTJJ\nJJ NNS TO\nArabic countries declined …\n…\nJJ NNS VBD\ntrAjEtSAdrAtmSr l AldwlAlErbyp\nNNSNNP IN DTJJ\n…\nDTNN VBD\n(a)\ntrAjEt SAdrAt mSr l Aldwl AlErbyp\nVBD NNS NNP IN DTNN DTJJ\nArabic\nJJ\nAlErbyp\nDTJJ\ncountries\nNNS\nAldwl\nDTNN\ndeclined\nVBD\ntrAjEt\nVBD\nEgyptian exports to\nJJ NNS TO\nSAdrAtmSr l\nNNSNNP IN (b)\nFigure 2: Arabic-English parallel sentence, automatically parsed and word-aligned, with corresponding\nsequences of bilingual tokens (in the shaded part). Comparison between translations produced via correct\n(a) and incorrect (b) reorderings.\nJJ NNS TO JJ NNS VBD\nNNS ! NNP VBD ! NNS NNS ! IN DTNN ! DTJJ IN ! DTNN ROOT ! VBD\n(a)\nJJ NNS TOJJ NNS VBD\nNNS ! NNP VBD ! NNS NNS ! INDTNN ! DTJJ IN ! DTNN ROOT ! VBD\n(b)\nFigure 3: Sequences of bilingual tokens with\nsource words substituted with their and their par-\nents’ POS tags: correct (a) and incorrect (b) re-\norderings.\nrect and incorrect reorderings, see Figure 2.b. Al-\nthough the corresponding sequence of POS-tag-\nsubstituted bilingual tokens is different from the\ncorrect sequence (Figure 2.b, shaded part), it still\nis a likely sequence. Indeed, the log-probabilities\nof the two sequences with respect to a 4-gram\nBiLM model 5 result in a higher probability of\n−10.25 for the incorrect reordering than for the\ncorrect one (−10.39).\nSince fully lexicalized bilingual tokens suffer\nfrom data sparsity and POS-based bilingual tokens\nare insufﬁciently expressive, the question is which\nlevel of syntactic information strikes the right bal-\nance between expressiveness and generality.\n5Section 4 contains details about data and software setup.\n2.3 BiLM with dependency information\nDependency grammar is commonly used in NLP\nto formalize role-based relations between words.\nThe intuitive notion of syntactic modiﬁcation is\ncaptured by the primitive binary relation of depen-\ndence. Dependency relations do not change with\nthe linear order of words (Figure 2) and therefore\ncan provide a characterization of a word’s syntac-\ntic class that invariant under reordering.\nIf we incorporate dependency relations into the\nrepresentation of bilingual tokens, the incorrect re-\nordering in Figure 2.b will produce a highly un-\nlikely sequence. For example, we can substitute\neach source word with its POS tag and its par-\nent’s POS tag (Figure 3). Again, we computed\n4-gram log-probabilities for the corresponding se-\nquences: the correct reordering results in a sub-\nstantially higher probability of−10.58 than the in-\ncorrect one (−13.48). We may consider situations\nwhere more ﬁne-grained distinctions are required.\nIn the next section, we explore different represen-\ntations based on source dependency trees.\n3 Dependency-based BiLM\nIn this section, we introduce our model which\ncombines the BiLM from Niehues et al. (2011)\nwith source dependency information. We fur-\nther give details on how the proposed models are\ntrained and integrated into a phrase-based decoder.\n1692\n3.1 The general framework\nIn the previous section we outlined our framework\nas composed of two steps: First, a parallel sen-\ntence is tokenized according to the BiLM model\n(Niehues et al., 2011). Next, words in the bilingual\ntokens are substituted with their contextual prop-\nerties. It is thus convenient to use the following\ngeneralized deﬁnition for a token sequence t1...tn\nin our framework:\nti = ⟨ContE(ei),{ContF(f)|f ∈A(ei)}⟩, (3)\nwhere ei is the i-th target word, A : E →P(F)\nis an alignment function, F and E are source and\ntarget sentences, and ContE and ContF are tar-\nget and source contextual functions, respectively.\nA contextual function returns a word’s contextual\nproperty, based on its sentential context (source or\ntarget). See Figure 4 for an example of a sequence\nof BiLM tokens with a ContF deﬁned as return-\ning the POS tag of the source word combined with\nthe POS tags of its parent, grandparent and sib-\nlings, and ContE deﬁned as an identity function\n(see Section 3.2 for a detailed explanation of the\nfunctions and notation).\nIn this work we focus on source contextual\nfunctions ( ContF). We also exploit some very\nsimple target contextual functions, but do not go\ninto an in-depth exploration.\n3.2 Dependency-based contextual functions\nIn NLP approaches exploiting dependency struc-\nture, two kinds of relations are of special impor-\ntance: the parent-child relation and the sibling re-\nlation. Shen et al. (2008) work with two well-\nformed dependency structures, both of which are\ndeﬁned in such a way that there is one common\nparent and a set of siblings. Li et al. (2012) charac-\nterize rules in hierarchical SMT by labeling them\nwith the POS tags of the parents of the words in-\nside the rule. Lerner and Petrov (2013) model re-\nordering as a sequence of classiﬁcation steps based\non a dependency parse of a sentence. Their model\nﬁrst decides how a word is reordered with respect\nto its parent and then how it is reordered with re-\nspect to its siblings.\nBased on these previous approaches, we pro-\npose to characterize contextual syntactic roles of\na word in terms of POS tags of the words them-\nselves and their relatives in a dependency tree. It\nis straightforward to incorporate parent informa-\ntion since each node has a unique parent. As for\nsiblings information, we incorporate POS tags of\nthe closest sibling to the left and the closest to the\nright. We do not include all of the siblings to avoid\noverﬁtting. In addition to these basic syntactic re-\nlations, we consider the grandparent relation.\nThe following list is a summary of the source\ncontextual functions that we use. We describe\na function with respect to the kind of contextual\nproperty of a word it returns: (i) the word itself\n(Lex); (ii) POS label of the word ( Pos); (iii) POS\nlabel of the word’s parent; (iv) POS of the word’s\nclosest sibling to the left, concatenated with the\nPOS tag of the closest sibling to the right; (v)\nthe POS label of the word’s grandparent. We use\ntarget-side contextual functions returning: (i) an\nempty string, (ii) POS of the word, (iii) the word\nitself.\nNotation. We do not use the above functions\nseparately to deﬁne individual BiLM models, but\nuse combinations of these functions. We use the\nfollowing notation for function combinations: “•”\nhorizontally connects source (on the left) and tar-\nget (on the right) contextual functions for a given\nmodel. For example, Lex•Lex refers to the original\n(lexicalized) BiLM. We use arrows ( →) to des-\nignate parental information (the arrow goes from\nparent to child). Pos→Pos refers to a combination\nof a function returning the POS of a word and the\nPOS of its parent (as in Figure 3). Pos→Pos→Pos\nis a combination of the previous with the func-\ntion returning the grandparent’s POS. Finally, we\nuse +sibl to indicate the use of the sibling func-\ntion described above: For example, Pos→Pos+sibl\nis a source function that returns the word’s POS,\nits parent’s POS and the POS labels of the closest\nsiblings to left and right.6 Pos+sibl→Pos is a source\nfunction returning the word’s own POS, the POS\nof a word’s parent, and the POS tags of the par-\nent’s siblings (left- and right-adjacent).\nFigure 4 represents the sentence from Figure 2\nduring decoding in a system with an integrated\nPos→Pos→Pos+sibl•Lex feature. It shows the se-\nquence of produced bilingual tokens and corre-\nsponding labels in the introduced notation.\n3.3 Training\nTraining of dependency-based BiLMs consists of\na sequence of extraction steps: After having pro-\nduced word-alignments for a bitext (Section 4),\n6In case there is no sibling on one of the sides, ϵ (empty\nword) is returned.\n1693\nEgyptian exports\ntrAjEt SAdrAt mSr l Aldwl AlErbyp\nVBD NNS NNP IN DTNN DTJJ\nJJ NNS TO\nto\nEgyptian\nVBD NNS NNP IN\nto\nVBD NNS NNP IN\nexports\nVBD NNS\n…\nFigure 4: Sequence of bilingual tokens pro-\nduced by a Pos →Pos→Pos+sibl•Lex after\ntranslating three words of the source sentence:\nVBD→NNS→ϵ+NNS+IN•Egyptian, ROOT→VBD→\nϵ+NNS+ϵ•exports, VBD→NNS→NNP+IN+ϵ•to (if there\nis no sibling on either of the sides, ϵis returned).\nsentences are segmented according to Equation 3.\nWe produce a dependency parse of a source sen-\ntence and a POS-tag labeling of a target sen-\ntence. For Chinese, we use the Stanford depen-\ndency parser (Chang et al., 2009). For Arabic a\ndependency parser is not available for public use,\nso we produce a constituency parse with the Stan-\nford parser (Green and Manning, 2010) and ex-\ntract dependencies based on the rules in Collins\n(1999). For English POS-tagging, we use the\nStanford POS-tagger (Toutanova et al., 2003). Af-\nter having produced a labeled sequence of tokens,\nwe learn a 5-gram model using SRILM (Stolcke\net al., 2011). Kneyser-Ney smoothing is used\nfor all model variations except for Pos•Pos where\nWitten-Bell smoothing is used due to zero count-\nof-counts.\n3.4 Decoder integration\nDependency-based BiLMs are integrated into our\nphrase-based SMT decoder as follows: Before\ntranslating a sentence, we produce its dependency\nparse. Phrase-internal word-alignments, needed\nto segment the translation hypothesis into tokens,\nare stored in the phrase table, based on the most\nfrequent internal alignment observed during train-\ning. Likewise, we store the most likely target-side\nPOS-labeling for each phrase pair.\nThe decoding algorithm is augmented with one\nadditional feature function and one additional, cor-\nresponding feature weight. At each step of the\nderivation, as a new phrase pair is added to the\nTraining set N. of lines N. of tokens\nSource side of Ar-En set 4,376,320 148M\nTarget side of Ar-En set 4,376,320 146M\nSource side of Ch-En set 2,104,652 20M\nTarget side of Ch-En set 2,104,652 28M\nTable 1: Training data for Arabic-English and\nChinese-English experiments.\npartial translation hypothesis, this function seg-\nments the new phrase into bilingual tokens (given\nthe internal alignment information) and substitutes\nthe words in the phrase pair with syntactic labels\n(given the source parse and the target POS labeling\nassociated with the phrase). The new syntactiﬁed\nbilingual tokens are added to the stack of preced-\ning n−1 tokens, and the feature function computes\nthe weighted updated model probability. During\ndecoding, the probabilities of the BiLMs are com-\nputed in a stream-based fashion, with bilingual\ntokens as string tokens, and not in a class-based\nfashion, with syntactic source-side representations\nemitting the corresponding target words (Bisazza\nand Monz, 2014).\n4 Experiments\n4.1 Setup\nWe conduct translation experiments with a base-\nline PBSMT system with additionally one of the\ndependency-based BiLM feature functions speci-\nﬁed in Section 3. We compare the translation per-\nformance to a baseline PBSMT system and to a\nbaseline augmented with the original BiLMs from\n(Niehues et al., 2011).\nWord-alignment is produced with GIZA++\n(Och and Ney, 2003). We use an in-house imple-\nmentation of a PBSMT system similar to Moses\n(Koehn et al., 2007). Our baseline contains\nall standard PBSMT features including language\nmodel, lexical weighting, and lexicalized reorder-\ning. The distortion limit is set to 5. A 5-gram LM\nis trained on the English Gigaword corpus (1.6B\ntokens) using SRILM with modiﬁed Kneyser-Ney\nsmoothing and interpolation. The BiLMs were\ntrained as described in Section 3.3. Informa-\ntion about the parallel data used for training the\nArabic-English7 and Chinese-English systems8 is\n7The following Arabic-English parallel corpora were\nused: LDC2006E25, LDC2004T18, several gale corpora,\nLDC2004T17, LDC2005E46, LDC2007T08, LDC2004E13.\n8The following Chinese-English parallel corpora\nwere used: LDC2002E18, LDC2002L27, LDC2003E07,\nLDC2003E14, LDC2005T06, LDC2005T10, LDC2005T34,\n1694\nConﬁguration MT08 MT09 MT08+MT09\nBLEU TER BLEU TER BLEU TER\na PBSMT baseline 45.12 47.94 48.16 44.30 46.57 46.21\nb Lex•Lex 45.27 47.79 48.85▲ 43.96△ 46.98▲ 45.96△\nPos•Pos 44.80 47.84 48.22 44.14△,− 46.44 46.07\nc Pos→Pos•Pos 45.66▲,△ 47.17▲,▲ 49.00▲,− 43.45▲,▲ 47.25▲,△ 45.40▲,▲\nd Pos→Pos−sibl•Pos 45.46△,− 47.45▲,△ 48.69▲,− 43.64▲,△ 47.00▲,− 45.64▲,−\ne Pos→Pos→Pos•Pos 45.68▲,△ 47.42▲,△ 49.09▲,− 43.59▲,▲ 47.30▲,△ 45.60▲,▲\nf Lex•Lex + Pos→Pos→Pos•Pos 45.63▲,△ 47.48▲,△ 49.30▲,▲ 43.60▲,△ 47.38▲,▲ 45.63▲,▲\nTable 2: BLEU and TER scores for Arabic-English experiments. Statistically signiﬁcant improvements\nover the baseline (a) are marked ▲ at the p < .01 level and △ at the p < .05 level. Additionally, ·,▲ and\n·,△ indicate signiﬁcant improvements with respect to BiLMLex•Lex (b). Since TER is an error rate, lower\nscores are better.\nConﬁguration MT08 MT09 MT08+MT09\nBLEU TER BLEU TER BLEU TER\nPos→Pos•ϵ 45.66▲,△ 47.44▲,△ 48.78▲,− 43.94▲,− 47.15▲,− 45.77▲,△\nPos→Pos•Pos 45.66▲,△ 47.17▲,▲ 49.00▲,− 43.45▲,▲ 47.25▲,△ 45.40▲,▲\nPos→Pos•Lex 45.48△,− 47.34▲,▲ 48.90▲,− 43.87▲,△ 47.12▲,− 45.69▲,▲\nTable 3: Different combinations of a target contextual function with the Pos →Pos source contextual\nfunction for Arabic-English. See Table 2 for the notation regarding statistical signiﬁcance.\nshown in Table 1.\nThe feature weights were tuned by using pair-\nwise ranking optimization (Hopkins and May,\n2011) on the MT04 benchmark (for both language\npairs). During tuning, 14 PRO parameter estima-\ntion runs are performed in parallel on different\nsamples of the n-best list after each decoder itera-\ntion. The weights of the individual PRO runs are\nthen averaged and passed on to the next decoding\niteration. Performing weight estimation indepen-\ndently for a number of samples corrects for some\nof the instability that can be caused by individual\nsamples. For testing, we used MT08 and MT09 for\nArabic, and MT06 and MT08 for Chinese. We use\napproximate randomization (Noreen, 1989; Rie-\nzler and Maxwell, 2005) to test for statistically sig-\nniﬁcant differences.\nIn the next two subsections we discuss the gen-\neral results for Arabic and Chinese, where we use\ncase-insensitive BLEU (Papineni et al., 2002) and\nTER (Snover et al., 2006) as evaluation metrics.\nThis is followed by a preliminary analysis of ob-\nserved reorderings where we compare 4-gram pre-\ncision results and conduct experiments with an in-\ncreased distortion limit.\n4.2 Arabic-English translation experiments\nWe are interested in how a translation system\nwith an integrated dependency-based BiLM fea-\nand several gale corpora.\nture performs as compared to the standard PB-\nSMT baseline and, more importantly, to the orig-\ninal BiLM model. We consider two variants of\nBiLM discussed by Niehues et al. (2011): the stan-\ndard one, Lex•Lex, and the simplest syntactic one,\nPos•Pos. Results for the experiments can be found\nin Table 2. In the discussion below we mostly fo-\ncus on the experimental results for the large, com-\nbined test set MT08+MT09.\nTable 2.a–b compares the performance of the\nbaseline and original BiLM systems. Lex•Lex\nyields strongly signiﬁcant improvements over the\nbaseline for BLEU and weakly signiﬁcant im-\nprovements for TER. Therefore, for the rest of the\nexperiments we are interested in obtaining further\nimprovements over Lex•Lex.\nPos→Pos•Pos (Table 2.c) demonstrates the effect\nof adding minimal dependency information to a\nBiLM.9 It results in strongly signiﬁcant improve-\nments over the baseline and weak improvements\nover Lex•Lex in terms of BLEU. We additionally\nran experiments with the different target functions\n(Table 3). •Pos shows the highest results, and•ϵ the\nlowest ones: this implies that a rather expressive\nsource syntactic representation alone still beneﬁts\nfrom target-side syntactic information. Below, our\ndependency-based systems only use •Pos.\nNext, we tested the effect of adding more source\n9Additional signiﬁcance testing, which is not shown in\nTable 2, shows a strongly signiﬁcant improvement over the\noriginal syntactic BiLM Pos•Pos.\n1695\nConﬁguration MT06 MT08 MT06+MT08\nBLEU TER BLEU TER BLEU TER\na PBSMT baseline 31.89 57.79 25.53 60.71 28.99 59.14\nb Lex•Lex 32.84▲ 57.40▲ 25.91△ 60.23▲ 29.69▲ 58.72▲\nPos•Pos 32.31▲ 57.89 25.66 60.79 29.28 59.24\nc Pos→Pos•Pos 32.86▲,− 57.05▲,△ 26.09▲,− 59.87▲,△ 29.78▲,− 58.36▲,▲\nd Pos→Pos−sibl•Pos 32.27△,− 56.63▲,△ 25.75 59.47▲,▲ 29.30△,− 57.95▲,▲\ne Pos→Pos→Pos•Pos 33.09▲,− 57.54 26.35▲,△ 59.70▲,▲ 30.05▲,▲ 58.54▲,−\nf Lex•Lex + Pos→Pos→Pos•Pos 33.43▲,▲ 57.00▲,▲ 26.50▲,▲ 59.79▲,▲ 30.28▲,▲ 58.30▲,▲\nTable 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2\nfor the notation regarding statistical signiﬁcance.\nConﬁguration MT06 MT08 MT06+MT08\nBLEU TER BLEU TER BLEU TER\nPos→Pos•ϵ 32.43▲,− 57.42▲,− 25.84 60.51 29.43▲,− 58.86▲,−\nPos→Pos•Pos 32.86▲,− 57.05▲,△ 26.09▲,− 59.87▲,△ 29.78▲,− 58.36▲,▲\nPos→Pos•Lex 32.69▲,− 57.03▲,△ 25.72 60.17▲,− 29.52▲,− 58.49▲,△\nTable 5: Different combinations of a target contextual function with thePos→Pos source contextual func-\ntion for Chinese-English. See Table 2 for the notation regarding statistical signiﬁcance.\ndependency information. Pos→Pos+sibl•Pos (Ta-\nble 2.d) only improves over the PBSMT baseline\n(but also shows weak improvements over Lex•Lex\nfor TER). It signiﬁcantly degrades the perfor-\nmance with respect to thePos→Pos•Pos system (Ta-\nble 2.c). Pos→Pos→Pos•Pos (Table 2.e) shows the\nbest results overall for BLEU, although it must be\npointed out that the difference with Pos→Pos•Pos is\nvery small. With respect to TER, Pos→Pos•Pos out-\nperforms the grandparent variant.\nSo far, we can conclude that source par-\nent information helps improve translation perfor-\nmance. Increased speciﬁcity of a parent (par-\nent speciﬁed by a grandparent) tends to further\nimprove performance. Up to now, we have\nonly used syntactic information and obtained con-\nsiderable improvements over Pos•Pos, surpass-\ning the improvement provided by Lex•Lex. Can\nwe gain further improvements by also adding\nlexical information? To this end, we con-\nduct experiments combining the best performing\ndependency-based BiLM ( Pos→Pos→Pos•Pos) and\nthe lexicalized BiLM ( Lex•Lex). We hypothesize\nthat the two models improve different aspects of\ntranslation: Lex•Lex is biased towards improving\nlexical choice and Pos→Pos→Pos•Pos towards im-\nproving reordering. Combining these two models,\nwe may improve both aspects. The metric results\nfor the combined set indeed support this hypothe-\nsis (Table 2.f).\n4.3 Chinese-English translation experiments\nThe results of the Chinese-English experiments\nare shown in Table 4. In the discussion below\nwe mostly focus on the experimental results for\nthe large, combined test set MT06+MT08. We\nobserve the same general pattern for the Pos→Pos\nsource function (Table 4.c) as for Arabic-English:\nthe system with the •Pos target function has the\nhighest scores (Table 5). All of the Pos→Pos•con-\nﬁgurations show statistically signiﬁcant improve-\nments over the PBSMT baseline. For TER, two\nof the three Pos→Pos• variants signiﬁcantly out-\nperform Lex•Lex. The system with sibling in-\nformation (Table 4.d) obtains quite low BLEU\nresults, just as in the Arabic experiments. On\nthe other hand, its TER results are the highest\noverall. The system with the Pos→Pos→Pos•Pos\nfunction (Table 4.e) achieves the best results\namong dependency-based BiLMs for BLEU. Fi-\nnally, combining Pos→Pos→Pos•Pos and Lex•Lex re-\nsults in the largest and signiﬁcant improvements\nover all competing systems for BLEU.\n4.4 Preliminary analysis of reordering in\ntranslation experiments\nIn general, the experimental results show that us-\ning source dependency information yields consis-\ntent improvements for translating from Arabic and\nChinese into English. On the other hand, we have\npointed out some discrepancies between the two\nmetrics employed, suggesting that different sys-\ntem conﬁgurations may improve different aspects\n1696\nConﬁguration Ar-En Ch-En\nMT08 MT09 MT08+MT09 MT06 MT08 MT06+MT08\na PBSMT baseline 26.14 29.81 27.88 14.48 10.96 12.89\nb Lex•Lex 26.33 30.55 28.32 15.43 11.45 13.65\nPos•Pos 25.95 30.06 27.89 14.76 11.01 13.07\nc Pos→Pos•Pos 26.91 31.08 28.87 15.29 11.52 13.60\ne Pos→Pos−sibl•Pos 26.71 30.73 28.60 15.27 11.67 13.66\nd Pos→Pos→Pos•Pos 26.78 31.09 28.80 15.42 11.70 13.77\nf Lex•Lex + Pos→Pos→Pos•Pos 26.80 31.27 28.90 15.87 11.85 14.07\nTable 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.\nConﬁguration MT08 MT09 MT08+MT09\nBLEU TER 4gram BLEU TER 4gram BLEU TER 4gram\nLex•Lex 45.19 47.06 26.41 48.39 44.11 30.23 46.72 45.97 28.21\nPos→Pos→Pos•Pos 45.49 47.31△ 26.66 48.90▲ 43.57▲ 30.92 47.12▲ 45.52▲ 28.66\nTable 7: BLEU, TER and 4-gram precision scores for Arabic-English Lex•Lex and Pos→Pos→Pos•Pos\nwith a distortion limit of 10.\nConﬁguration MT06 MT08 MT06+MT08\nBLEU TER 4gram BLEU TER 4gram BLEU TER 4gram\nLex•Lex 33.26 56.81 16.06 25.67 60.19 11.42 29.79 58.38 13.96\nPos→Pos→Pos•Pos 33.92▲ 56.29▲ 16.26 27.00▲ 59.58▲ 12.26 30.77▲ 57.82▲ 14.46\nTable 8: BLEU, TER and 4-gram precision scores for Chinese-English Lex •Lex and\nPos→Pos→Pos•Pos with a distortion limit of 10.\nof translation. To this end, we conducted some ad-\nditional evaluations to understand how reordering\nis affected by the proposed features.\nWe use 4-gram precision as a metric of how\nmuch of the reference set word order is preserved.\nTable 6 shows the corresponding results for both\nlanguages. Just as in the previous two sections,\nconﬁgurations with parental information produce\nthe best results. For Arabic, all of the depen-\ndency conﬁgurations outperform Lex•Lex. But the\nsystem with two feature functions, one of which\nis Lex•Lex, still obtains the best results, which\nmay suggest that the lexicalized BiLM also helps\nto differentiate between word orders. For Chi-\nnese, Pos→Pos→Pos•Pos and the system combining\nthe latter and Lex•Lex also obtain the best results.\nHowever, other dependency-based conﬁgurations\ndo not outperform Lex•Lex.\nAll the experiments so far were run with a dis-\ntortion limit of 5. But both of the languages, es-\npecially Chinese, often require reorderings over a\nlonger distance. We performed additional experi-\nments with a distortion limit of 10 for the Lex•Lex\nand Pos→Pos→Pos•Pos systems (Tables 7 and 8). It\nis more difﬁcult to translate with a higher distor-\ntion limit (Green et al., 2010) as the set of permu-\ntations grows larger thereby making it more difﬁ-\ncult to differentiate between correct and incorrect\ncontinuations of the current hypothesis. It has also\nbeen noted that higher distortion limits are more\nlikely to result in improvements for Chinese rather\nthan Arabic to English translation (Chiang, 2007;\nGreen et al., 2010).\nWe compared performance of ﬁxed BiLM mod-\nels at distortion lengths of 5 and 10. Arabic-\nEnglish results did not reveal statistically signif-\nicant differences between the two distortion lim-\nits for Pos→Pos→Pos•Pos. On the other hand, for\nLex•Lex BLEU decreases when using a distor-\ntion limit of 10 compared to a limit of 5. This\nimplies that the dependency BiLM is more ro-\nbust in the more challenging reordering setting\nthan the lexicalized BiLM. Chinese-English re-\nsults for Pos→Pos→Pos•Pos do show signiﬁcant im-\nprovements over the distortion limit of 5 (up to\n0.49 BLEU higher than the best result in Table 4).\nThis indicates that the dependency-based BiLM is\nbetter capable to take advantage of the increased\ndistortion limit and discriminate between correct\nand incorrect reordering choices.\nComparing the results for Pos→Pos→Pos•Pos and\nLex•Lex at a distortion limit of 10, we obtain\nstrongly signiﬁcant improvements for all metrics.\nFor Chinese, a larger distortion limit helps for both\nconﬁgurations, but more so for our dependency\nBiLM, yielding an improvement of 0.98 BLEU\n1697\nover the original, lexicalized BiLM (Table 8).\n5 Conclusions\nIn this paper, we have introduced a simple, yet ef-\nfective way to include syntactic information into\nphrase-based SMT. Our method consists of en-\nriching the representation of units of a bilingual\nlanguage model (BiLM). We argued that the very\nlimited contextual information used in the original\nbilingual models (Niehues et al., 2011) can capture\nreorderings only to a limited degree and proposed\na method to incorporate information from a source\ndependency tree in bilingual units. In a series\nof translation experiments we performed a thor-\nough comparison between various syntactically-\nenriched BiLMs and competing models. The re-\nsults demonstrated that adding syntactic informa-\ntion from a source dependency tree to the repre-\nsentations of bilingual tokens in an n-gram model\ncan yield statistically signiﬁcant improvements\nover the competing systems.\nA number of additional evaluations provided an\nindication for better modeling of reordering phe-\nnomena. The proposed dependency-based BiLMs\nresulted in an increase in 4-gram precision and\nprovided further signiﬁcant improvements over\nall considered metrics in experiments with an in-\ncreased distortion limit.\nIn this paper, we have focused on rather elemen-\ntary dependency relations, which we are planning\nto expand on in future work. Our current approach\nis still strictly tied to the number of target tokens.\nIn particular, we are interested in exploring ways\nto better capture the notion of syntactic cohesion\nin translation (Fox, 2002; Cherry, 2008) within our\nframework.\nAcknowledgments\nWe thank Arianna Bisazza and the reviewers for\ntheir useful comments. This research was funded\nin part by the Netherlands Organization for Sci-\nentiﬁc Research (NWO) under project numbers\n639.022.213 and 612.001.218.\nReferences\nYaser Al-Onaizan and Kishore Papineni. 2006. Dis-\ntortion models for statistical machine translation. In\nProceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 529–536, Sydney, Australia, July. Asso-\nciation for Computational Linguistics.\nAlexandra Birch. 2011. Reordering Metrics for Statis-\ntical Machine Translation. Ph.D. thesis, University\nof Edinburgh.\nArianna Bisazza and Christof Monz. 2014. Class-\nbased language modeling for translating into mor-\nphologically rich languages. In Proceedings of\nthe 25th International Conference on Computa-\ntional Linguistics (COLING 2014), pages 1918–\n1927, Dublin, Ireland, August.\nPi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and\nChristopher D. Manning. 2009. Discriminative\nreordering with chinese grammatical relations fea-\ntures. In Proceedings of the Third Workshop on Syn-\ntax and Structure in Statistical Translation, pages\n51–59. Association for Computational Linguistics.\nColin Cherry. 2008. Cohesive phrase-based decoding\nfor statistical machine translation. In Proceedings\nof Association for Computational Linguistics, pages\n72–80.\nDavid Chiang. 2007. Hierarchical phrase-based trans-\nlation. Computational Linguistics, 33(2):201–228.\nDavid Chiang. 2010. Learning to translate with source\nand target syntax. In Proceedings of the 48th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1443–1452. Association for Com-\nputational Linguistics.\nMichael Collins. 1999. Head-Driven Statistical Mod-\nels for Natural Language Parsing. Ph.D. thesis,\nUniversity of Pennsylvania.\nJosep M. Crego and Franc ¸ois Yvon. 2010a. Factored\nbilingual n-gram language models for statistical ma-\nchine translation. Machine Translation, 24(2):159–\n175.\nJosep M. Crego and Franc ¸ois Yvon. 2010b. Improv-\ning reordering with linguistically informed bilin-\ngual n-grams. In Proceedings of the 23rd Inter-\nnational Conference on Computational Linguistics,\npages 197–205. Association for Computational Lin-\nguistics.\nNadir Durrani, Helmut Schmid, and Alexander Fraser.\n2011. A joint sequence translation model with in-\ntegrated reordering. In Proceedings of the 49th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1045–1054. Association for Com-\nputational Linguistics.\nHeidi J. Fox. 2002. Phrasal cohesion and statistical\nmachine translation. In Proceedings of the ACL-\n02 conference on Empirical methods in natural lan-\nguage processing, pages 304–3111. Association for\nComputational Linguistics.\n1698\nMichel Galley and Christopher D. Manning. 2008. A\nsimple and effective hierarchical phrase reordering\nmodel. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing,\npages 848–856. Association for Computational Lin-\nguistics.\nSpence Green and Christopher D. Manning. 2010.\nBetter arabic parsing: Baselines, evaluations, and\nanalysis. In Proceedings of the 23rd International\nConference on Computational Linguistics, pages\n394–402. Association for Computational Linguis-\ntics.\nSpence Green, Michel Galley, and Christopher D. Man-\nning. 2010. Improved models of distortion cost\nfor statistical machine translation. In Proceedings\nof the 2010 Annual Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics, pages 867–875. Association for Com-\nputational Linguistics.\nMark Hopkins and Jonathan May. 2011. Tuning as\nranking. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing,\npages 1352–1362. Association for Computational\nLinguistics.\nLiang Huang and Haitao Mi. 2010. Efﬁcient incre-\nmental decoding for tree-to-string translation. In\nProceedings of the 2010 Conference on Empirical\nMethods in Natural Language Processing, pages\n273–283. Association for Computational Linguis-\ntics.\nLiang Huang, Kevin Knight, and Aravind Joshi. 2006.\nStatistical syntax-directed translation with extended\ndomain of locality. In Proceedings of AMTA, pages\n223–226.\nPhilipp Koehn, Franz Josef Och, and Daniel Marcu.\n2003. Statistical phrase-based translation. In Pro-\nceedings of the 2003 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics on Human Language Technology, pages\n48–54. Association for Computational Linguistics.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, et al. 2007. Moses: Open source\ntoolkit for statistical machine translation. In Pro-\nceedings of the 45th Annual Meeting of the Associ-\nation for Computational Linguistics on Interactive\nPoster and Demonstration Sessions, pages 177–180.\nAssociation for Computational Linguistics.\nUri Lerner and Slav Petrov. 2013. Source-side classi-\nﬁer preordering for machine translation. In Proceed-\nings of the Empirical Methods in Natural Language\nProcessing.\nJunhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van\nGenabith. 2012. Head-driven hierarchical phrase-\nbased translation. In Proceedings of the 50th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 33–37. Association for Computa-\ntional Linguistics.\nYang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-\nto-string alignment template for statistical machine\ntranslation. In Proceedings of the 21st International\nConference on Computational Linguistics and the\n44th annual meeting of the Association for Compu-\ntational Linguistics, pages 609–616. Association for\nComputational Linguistics.\nJos´e B Marino, Rafael E Banchs, Josep M. Crego,\nAdria de Gispert, Patrik Lambert, Jos ´e A.R. Fonol-\nlosa, and Marta R. Costa-Juss `a. 2006. N-gram-\nbased machine translation. Computational Linguis-\ntics, 32(4):527–549.\nYuval Marton and Philip Resnik. 2008. Soft syntac-\ntic constraints for hierarchical phrased-based trans-\nlation. In Proceedings of the Association for Com-\nputational Linguistics, pages 1003–1011.\nJan Niehues, Teresa Herrmann, Stephan V ogel, and\nAlex Waibel. 2011. Wider context by using bilin-\ngual language models in machine translation. In\nProceedings of the Sixth Workshop on Statistical\nMachine Translation, pages 198–206. Association\nfor Computational Linguistics.\nEric W. Noreen. 1989. Computer Intensive Meth-\nods for Testing Hypotheses. An Introduction. Wiley-\nInterscience.\nFranz Josef Och and Hermann Ney. 2003. A sys-\ntematic comparison of various statistical alignment\nmodels. Computational Linguistics, 29(1):19–51.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Compu-\ntational Linguistics, pages 311–318. Association for\nComputational Linguistics.\nStefan Riezler and John T. Maxwell. 2005. On some\npitfalls in automatic evaluation and signiﬁcance test-\ning for MT. In Proceedings of the ACL Workshop on\nIntrinsic and Extrinsic Evaluation Measures for Ma-\nchine Translation and/or Summarization.\nLibin Shen, Jinxi Xu, and Ralph M. Weischedel. 2008.\nA new string-to-dependency machine translation al-\ngorithm with a target dependency language model.\nIn Proceedings of the Association for Computational\nLinguistics, pages 577–585.\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\nnea Micciulla, and John Makhoul. 2006. A study of\ntranslation edit rate with targeted human annotation.\nIn Proceedings of AMTA, pages 223–231.\nAndreas Stolcke, Jing Zheng, Wen Wang, and Victor\nAbrash. 2011. Srilm at sixteen: Update and out-\nlook. In Proceedings of IEEE Automatic Speech\nRecognition and Understanding Workshop, page 5.\n1699\nChristoph Tillmann. 2004. A unigram orientation\nmodel for statistical machine translation. In Pro-\nceedings of of the North American Chapter of the\nAssociation for Computational Linguistics, pages\n101–104. Association for Computational Linguis-\ntics.\nKristina Toutanova, Dan Klein, Christopher D. Man-\nning, and Yoram Singer. 2003. Feature-rich part-of-\nspeech tagging with a cyclic dependency network.\nIn Proceedings of the 2003 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics on Human Language Technology,\npages 173–180. Association for Computational Lin-\nguistics.\nRichard Zens and Hermann Ney. 2003. A comparative\nstudy on reordering constraints in statistical machine\ntranslation. In Proceedings of the 41st Annual Meet-\ning on Association for Computational Linguistics-\nVolume 1, pages 144–151. Association for Compu-\ntational Linguistics.\n1700",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8257220983505249
    },
    {
      "name": "Machine translation",
      "score": 0.822843074798584
    },
    {
      "name": "Dependency (UML)",
      "score": 0.8178490996360779
    },
    {
      "name": "Natural language processing",
      "score": 0.6638281345367432
    },
    {
      "name": "Translation (biology)",
      "score": 0.6017158031463623
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5997871160507202
    },
    {
      "name": "Example-based machine translation",
      "score": 0.46227604150772095
    },
    {
      "name": "Language model",
      "score": 0.4275178611278534
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ]
}