{
  "title": "Automated Disentangled Sequential Recommendation with Large Language Models",
  "url": "https://openalex.org/W4400148150",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2045332956",
      "name": "Xin Wang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2101238025",
      "name": "Hong Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A4304082858",
      "name": "Zirui Pan",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2121393930",
      "name": "Yuwei Zhou",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A3131990689",
      "name": "Chaoyu Guan",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2134465402",
      "name": "Li-Feng Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2111511002",
      "name": "Wenwu Zhu",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4386728933",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W6600688380",
    "https://openalex.org/W4392151693",
    "https://openalex.org/W2783944588",
    "https://openalex.org/W4224316819",
    "https://openalex.org/W2791176029",
    "https://openalex.org/W2512971201",
    "https://openalex.org/W2142144955",
    "https://openalex.org/W2808492412",
    "https://openalex.org/W4296591867",
    "https://openalex.org/W6601409588",
    "https://openalex.org/W6917172014",
    "https://openalex.org/W6823560597",
    "https://openalex.org/W2626454364",
    "https://openalex.org/W2730106296",
    "https://openalex.org/W6601574642",
    "https://openalex.org/W2054141820",
    "https://openalex.org/W6847131820",
    "https://openalex.org/W3119242082",
    "https://openalex.org/W4295934584",
    "https://openalex.org/W6601760687",
    "https://openalex.org/W6600669965",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W6601047702",
    "https://openalex.org/W6600146492",
    "https://openalex.org/W2973325524",
    "https://openalex.org/W6603143895",
    "https://openalex.org/W2171279286",
    "https://openalex.org/W2155106456",
    "https://openalex.org/W2042281163",
    "https://openalex.org/W2157881433",
    "https://openalex.org/W6600031136",
    "https://openalex.org/W4213448193",
    "https://openalex.org/W2767586661",
    "https://openalex.org/W6600577311",
    "https://openalex.org/W4388191850",
    "https://openalex.org/W4405643374",
    "https://openalex.org/W4385565184",
    "https://openalex.org/W6815619692",
    "https://openalex.org/W4304092657",
    "https://openalex.org/W4311118067",
    "https://openalex.org/W3035223629",
    "https://openalex.org/W2597601064",
    "https://openalex.org/W2560674852",
    "https://openalex.org/W569478347",
    "https://openalex.org/W4300382868",
    "https://openalex.org/W1602136775",
    "https://openalex.org/W3212687458",
    "https://openalex.org/W4226054951",
    "https://openalex.org/W4249142012",
    "https://openalex.org/W2998036008",
    "https://openalex.org/W2911840101",
    "https://openalex.org/W2913754224",
    "https://openalex.org/W4367046738",
    "https://openalex.org/W4250543126",
    "https://openalex.org/W1530404542",
    "https://openalex.org/W1502957213",
    "https://openalex.org/W3213068453",
    "https://openalex.org/W4393157099",
    "https://openalex.org/W2913668833",
    "https://openalex.org/W4206908526",
    "https://openalex.org/W3147409145",
    "https://openalex.org/W4300011764",
    "https://openalex.org/W2751936342",
    "https://openalex.org/W4301409532",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W2137245235",
    "https://openalex.org/W4239019441",
    "https://openalex.org/W4297794619",
    "https://openalex.org/W2531563875",
    "https://openalex.org/W2914911817",
    "https://openalex.org/W1847618513",
    "https://openalex.org/W4301074032",
    "https://openalex.org/W4385565193"
  ],
  "abstract": "Sequential recommendation aims to recommend the next items that a target user may have interest in based on the user’s sequence of past behaviors, which has become a hot research topic in both academia and industry. In the literature, sequential recommendation adopts a Sequence-to-Item or Sequence-to-Sequence training strategy, which supervises a sequential model with a user’s next one or more behaviors as the labels and the sequence of the past behaviors as the input. However, existing powerful sequential recommendation approaches employ more and more complex deep structures such as Transformer in order to accurately capture the sequential patterns, which heavily rely on hand-crafted designs on key attention mechanism to achieve state-of-the-art performance, thus failing to automatically obtain the optimal design of attention representation architectures in various scenarios with different data. Other works on classic automated deep recommender systems only focus on traditional settings, ignoring the problem of sequential scenarios. In this article, we study the problem of automated sequential recommendation, which faces two main challenges: (1) How can we design a proper search space tailored for attention automation in sequential recommendation, and (2) How can we accurately search effective attention representation architectures considering multiple user interests reflected in the sequential behavior. To tackle these challenges, we propose an automated disentangled sequential recommendation (AutoDisenSeq) model. In particular, we employ neural architecture search (NAS) and design a search space tailored for automated attention representation in attentive intention-disentangled sequential recommendation with an expressive and efficient space complexity of \\(O(n^{2})\\) given \\(n\\) as the number of layers. We further propose a context-aware parameter sharing mechanism taking characteristics of each sub-architecture into account to enable accurate architecture performance estimations and great flexibility for disentanglement of latent intention representation. Moreover, we propose AutoDisenSeq-large language model (LLM), which utilizes the textual understanding power of LLM as a guidance to refine the candidate list for recommendation from AutoDisenSeq. We conduct extensive experiments to show that our proposed AutoDisenSeq model and AutoDisenSeq-LLM model outperform existing baseline methods on four real-world datasets in both overall recommendation and cold-start recommendation scenarios.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6394150853157043
    },
    {
      "name": "Natural language processing",
      "score": 0.4704802334308624
    }
  ]
}