{
  "title": "Multilingual End-to-End Speech Recognition with A Single Transformer on Low-Resource Languages",
  "url": "https://openalex.org/W2808640845",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2357468579",
      "name": "Zhou Shiyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2067510015",
      "name": "Xu Shuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1937328933",
      "name": "Xu, Bo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2772270478",
    "https://openalex.org/W2798657914",
    "https://openalex.org/W2964107261",
    "https://openalex.org/W2627092829",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2767906378",
    "https://openalex.org/W2775766866",
    "https://openalex.org/W1565000507",
    "https://openalex.org/W2803399609",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2767634090",
    "https://openalex.org/W2106440210",
    "https://openalex.org/W2147768505",
    "https://openalex.org/W2025198378",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2775304348",
    "https://openalex.org/W2750545698",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963920996",
    "https://openalex.org/W2951184134",
    "https://openalex.org/W2408183654"
  ],
  "abstract": "Sequence-to-sequence attention-based models integrate an acoustic, pronunciation and language model into a single neural network, which make them very suitable for multilingual automatic speech recognition (ASR). In this paper, we are concerned with multilingual speech recognition on low-resource languages by a single Transformer, one of sequence-to-sequence attention-based models. Sub-words are employed as the multilingual modeling unit without using any pronunciation lexicon. First, we show that a single multilingual ASR Transformer performs well on low-resource languages despite of some language confusion. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence under the condition of language information being known during training. Experiments on CALLHOME datasets demonstrate that the multilingual ASR Transformer with the language symbol at the end performs better and can obtain relatively 10.5\\% average word error rate (WER) reduction compared to SHL-MLSTM with residual learning. We go on to show that, assuming the language information being known during training and testing, about relatively 12.4\\% average WER reduction can be observed compared to SHL-MLSTM with residual learning through giving the language symbol as the sentence start token.",
  "full_text": "Multilingual End-to-End Speech Recognition with A Single Transformer on\nLow-Resource Languages\nShiyu Zhou1,2, Shuang Xu1, Bo Xu1\n1Institute of Automation, Chinese Academy of Sciences\n2University of Chinese Academy of Sciences\n{zhoushiyu2013, shuang.xu, xubo}@ia.ac.cn\nAbstract\nSequence-to-sequence attention-based models integrate an\nacoustic, pronunciation and language model into a single neural\nnetwork, which make them very suitable for multilingual auto-\nmatic speech recognition (ASR). In this paper, we are concerned\nwith multilingual speech recognition on low-resource languages\nby a single Transformer, one of sequence-to-sequence attention-\nbased models. Sub-words are employed as the multilingual\nmodeling unit without using any pronunciation lexicon. First,\nwe show that a single multilingual ASR Transformer performs\nwell on low-resource languages despite of some language con-\nfusion. We then look at incorporating language information\ninto the model by inserting the language symbol at the begin-\nning or at the end of the original sub-words sequence under the\ncondition of language information being known during train-\ning. Experiments on CALLHOME datasets demonstrate that\nthe multilingual ASR Transformer with the language symbol at\nthe end performs better and can obtain relatively 10.5% average\nword error rate (WER) reduction compared to SHL-MLSTM\nwith residual learning. We go on to show that, assuming the\nlanguage information being known during training and testing,\nabout relatively 12.4% average WER reduction can be observed\ncompared to SHL-MLSTM with residual learning through giv-\ning the language symbol as the sentence start token.\nIndex Terms: ASR, speech recognition, multilingual, low-\nresource, sequence-to-sequence, Transformer\n1. Introduction\nMultilingual speech recognition has been investigated for many\nyears [1, 2, 3, 4, 5]. Conventional studies concentrate on\nthe area of multilingual acoustic modeling by the context-\ndependent deep neural network hidden Markov models (CD-\nDNN-HMM) [6]. The hidden layers of DNN in CD-DNN-\nHMM can be thought of complicated feature transformation\nthrough multiple layers of nonlinearity, which can be used to ex-\ntract universal feature transformation from multilingual datasets\n[1]. Among the CD-DNN-HMM based approaches, the ar-\nchitecture of SHL-MDNN [1], in which the hidden layers are\nshared across multiple languages while the softmax layers are\nlanguage dependent, is a signiﬁcant progress in the area of mul-\ntilingual ASR. These shared hidden layers and language depen-\ndent softmax layers of SHL-MDNN are optimized jointly by\nmultilingual datasets. SHL-MLSTM [5] further explores long\nshort-term memory (LSTM) [7] with residual learning as the\nshared hidden layer instead of DNN and achieves better results\nthan SHL-MDNN.\nAlthough these models achieve encouraging results on mul-\ntilingual ASR tasks, a hand-designed language-speciﬁc pronun-\nciation lexicon must be employed. This severely limits their\napplication on low-resource languages, which may have not\na well-designed pronunciation lexicon. Recent researches on\nsequence-to-sequence attention-based models try to remove this\ndependency on the pronunciation lexicon [8, 9, 10]. Chiu et\nal. shows that attention-based encoder-decoder architecture,\nnamely listen, attend, and spell (LAS), achieves a new state-\nof-the-art WER on a 12500 hour English voice search task us-\ning the word piece models (WPM) [10]. Our previous work\n[9] demonstrates that the lexicon independent models can out-\nperform lexicon dependent models on Mandarin Chinese ASR\ntasks by the ASR Transformer and the character based model\nestablishes a new state-of-the-art character error rate (CER) on\nHKUST datasets.\nSince the acoustic, pronunciation and language model are\nintegrated into a single neural network by sequence-to-sequence\nattention-based models, it makes them very suitable for mul-\ntilingual ASR. In this paper, we concentrate on multilingual\nASR on low-resource languages. Building on our work [9],\nwe employ sub-words generated by byte pair encoding (BPE)\n[11] as the multilingual modeling unit, which do not need any\npronunciation lexicon. The ASR Transformer is chosen to be\nthe basic architecture of sequence-to-sequence attention-based\nmodel [9, 12]. To alleviate the problem of few training data on\nlow-resource languages, a well-trained ASR Transformer from\na high-resource language is adopted as the initial model rather\nthan random initialization, whose softmax layer is replaced by\nthe language-speciﬁc softmax layer. We then look at incor-\nporating language information into the model by inserting the\nlanguage symbol at the beginning or at the end of the original\nsub-words sequence [13] under the condition of language infor-\nmation being known during training. A comparison with SHL-\nMLSTM [5] with residual learning is investigated on CALL-\nHOME datasets with 6 languages. Experimental results reveal\nthat the multilingual ASR Transformer with the language sym-\nbol at the end performs better and can obtain relatively 10.5%\naverage WER reduction compared to SHL-MLSTM with resid-\nual learning. We go on to show that, assuming the language\ninformation being known during training and testing, about rel-\natively 12.4% average WER reduction can be observed com-\npared to SHL-MLSTM with residual learning through giving\nthe language symbol as the sentence start token.\nThe rest of the paper is organized as follows. After an\noverview of the related work in Section 2, Section 3 describes\nthe proposed method in detail. We then show experimental re-\nsults in Section 4 and conclude this work in Section 5.\n2. Related work\nAlthough multilingual speech recognition has been studied\n[1, 2, 3, 4, 5] for a long time, these researches are commonly\nlimited to making acoustic model (AM) multilingual, which\narXiv:1806.05059v2  [eess.AS]  14 Jun 2018\nrequire language-speciﬁc pronunciation model (PM) and lan-\nguage model (LM). Recently, sequence-to-sequence attention-\nbased models, integrating the AM, PM and LM into a single\nnetwork, have attracted a lot of attention on multilingual ASR\n[13, 14, 15, 16]. [14, 15] have presented a single sequence-to-\nsequence attention-based model can be capable of recognizing\nany of the languages seen in training. [13] explored the possi-\nbility of training a single model serve different English dialects\nand compared different methods incorporating dialect-speciﬁc\ninformation into the model. However, multilingual ASR on\nlow-resource languages are few investigated by sequence-to-\nsequence attention-based models. Furthermore, we argue that\nthe modeling unit of sub-words allows for a much stronger de-\ncoder LM compared to graphemes [10], so sub-words encoded\nby BPE are employed as the multilingual modeling unit rather\nthan graphemes [13, 14].\n3. System overview\n3.1. ASR Transformer model architecture\nThe ASR Transformer architecture used in this work is the same\nas our work [9, 12] which is shown in Figure 1. It stacks multi-\nhead attention (MHA) [17] and position-wise, fully connected\nlayers for both the encode and decoder. The encoder is com-\nposed of a stack of N identical layers. Each layer has two sub-\nlayers. The ﬁrst is a MHA, and the second is a position-wise\nfully connected feed-forward network. Residual connections\nare employed around each of the two sub-layers, followed by\na layer normalization. The decoder is similar to the encoder\nexcept inserting a third sub-layer to perform a MHA over the\noutput of the encoder stack. To prevent leftward information\nﬂow and preserve the auto-regressive property in the decoder,\nthe self-attention sub-layers in the decoder mask out all values\ncorresponding to illegal connections. In addition, positional en-\ncodings [17] are added to the input at the bottoms of these en-\ncoder and decoder stacks, which inject some information about\nthe relative or absolute position of the tokens in the sequence.\nThe difference between the neural machine translation\n(NMT) Transformer [17] and the ASR Transformer is the input\nof the encoder. We add a linear transformation with a layer nor-\nmalization to convert the log-Mel ﬁlterbank feature to the model\ndimension dmodel for dimension matching, which is marked out\nby a dotted line in Figure 1.\n3.2. Multilingual modeling unit\nSub-words are employed as the multilingual modeling unit,\nwhich are generated by BPE 1 [11]. Firstly, the symbol vo-\ncabulary with the character vocabulary is initialized, and each\nword is represented as a sequence of characters plus a special\nend-of-word symbol ‘@@’, which allows to restore the original\ntokenization. Then, all symbol pairs are counted iteratively and\neach occurrence of the most frequent pair (‘A’, ‘B’) are replaced\nwith a new symbol ‘AB’. Each merge operation produces a new\nsymbol which represents a character n-gram. Frequent charac-\nter n-grams (or whole words) are eventually merged into a sin-\ngle symbol. Then the ﬁnal symbol vocabulary size is equal to\nthe size of the initial vocabulary, plus the number of merge op-\nerations α, which is the only hyper-parameter of this algorithm\n[11].\nIn our multilingual experiments, training transcripts in all\nlanguages are combined together to generate the multilingual\n1https://github.com/rsennrich/subword-nmt\nMulti-Head\nAttention\nK V Q\nPositional\nEncoding\nFeed\nForward\nAdd & Norm\nAdd & Norm\nMasked\nMulti-Head\nAttention\nK V Q\nPositional\nEncoding\nOutput\nEmbedding\nMulti-Head\nAttention\nAdd & Norm\nAdd & Norm\nOutputs\n(shifted right)\nFeed\nForward\nAdd & Norm\nK V Q\nLinear\nOutput\nProbabilities\nSoftmax\nN×\nN×\nFbank\nDim & Norm\nFigure 1: The architecture of the ASR Transformer.\nsymbol vocabulary, instead of directly merging each language\nsymbol vocabulary together. So same sub-words are shared\namong different languages automatically, which is very ben-\neﬁcial for languages belonging to the same language family.\nFor example, for a German word of “universit ¨atsgeb¨au”, it is\nencoded into “univer@@ sit@@ ¨a@@ ts@@ ge@@ b@@\n¨a@@ u”; for an English word of “university”, it is encoded\ninto “univer@@ sit@@ y”. Two sub-words “univer@@” and\n“sit@@” are shared in these two languages.\n3.3. Language information as output targets\nSimilar to [13, 18], we expand the symbol vocabulary of the\nmultilingual ASR Transformer to include a list of special sym-\nbols, each corresponding to a language. For example, we add\nthe symbol <S EN>into the symbol vocabulary when includ-\ning English. If the language information of training data can\nonly be known beforehand, two methods of adding the lan-\nguage symbol are explored, i.e. inserting at the beginning\n(Transformer-B) or at the end ( Transformer-E) of the original\nsub-words sequence [13, 18]. What’s more, if the language in-\nformation of both training and testing data can be known be-\nforehand, we directly take the language symbol <S Lang>as\nthe sentence start token ( Transformer-B2) rather than original\nsentence start token <S>. It can force the multilingual ASR\nTransformer to decode a speech utterance into the pointed lan-\nguage, which is able to alleviate the language confusion greatly\nduring testing.\nThe difference between Transformer-B and Transformer-\nB2 is whether to utilize the language information during testing.\nThe sentence start token is <S>in Transformer-B. It ﬁrst pre-\ndicts a language symbol by itself and then the following tokens\nare predicted as usual. Therefore, Transformer-B do not need\nto know the language information beforehand during testing. In\ncontrast, Transformer-B2 employs <S Lang> as its sentence\nstart token and predicts the following tokens as usual, which\nneed to know the language information beforehand during test-\ning. An example of adding the language symbol is shown in\nTable 1.\nTable 1: An example of adding the language symbol.\nModel Example\nSource amazing\nTransformer <S>ama@@ z@@ ing <\\S>\nTransformer-B <S><S EN>ama@@ z@@ ing <\\S>\nTransformer-E <S>ama@@ z@@ ing <S EN><\\S>\nTransformer-B2 <S EN>ama@@ z@@ ing <\\S>\n4. Experiment\n4.1. Data\nThe datasets in the paper come from CALLHOME corpora col-\nlected by Linguistic Data Consortium (LDC). The following six\nlanguages are used: Mandarin (MA), English (EN), Japanese\n(JA), Arabic (AR), German (GE) and Spanish (SP). We follow\nthe Kaldi [19] recipe to process CALLHOME datasets 2. The\ndetailed information is listed below in Table 2. We train the\nASR Transformer with a given number of epochs, so validation\nsets are not employed in this paper. All experiments are con-\nducted using 80-dimensional log-Mel ﬁlterbank features, com-\nputed with a 25ms window and shifted every 10ms. The fea-\ntures are normalized via mean subtraction and variance normal-\nization on the speaker basis. Similar to [20, 21], at the current\nframe t, these features are stacked with 3 frames to the left and\ndownsampled to a 30ms frame rate. We generate more train-\ning data by linearly scaling the audio lengths by factors of 0.9\nand 1.1 [22], since it is always beneﬁcial for training the ASR\nTransformer [9].\nTable 2: Multilingual dataset statistics.\nLanguage # training utts. # test utts.\nMandarin (MA) 23915 3021\nEnglish (EN) 21194 2840\nJapanese (JA) 27165 3381\nArabic (AR) 20828 2978\nGerman (GE) 20027 52363\nSpanish (SP) 17840 1982\nTotal 130969 19438\n4.2. Model and training details\nWe perform our experiments on the big model (D1024-H16)\n[9, 17] of the ASR Transformer. Table 3 lists our experimental\n2the scripts of ﬁsher callhome spanish in Kaldi are used to process\nall CALLHOME datasets with some tiny modiﬁcations.\n3We employ devtest as evaltest in German since there is no evaltest\nfrom CALLHOME corpora.\nparameters. The Adam algorithm [23] with gradient clipping\nand warmup is used for optimization. During training, label\nsmoothing of value ϵls = 0.1 is employed [24]. After trained,\nthe last 20 checkpoints are averaged to make the performance\nmore stable [17].\nAt the beginning we train the ASR Transformer on English\ndata with a random initialization, but the result is poor although\nthe CE loss looks good. We propose that one reason for the poor\nperformance could be the training data is too few but the param-\neters of the ASR Transformer are relatively large which is about\n230M in this work. To compensate the lack of training data on\nlow-resource languages, a well-trained ASR Transformer with a\nCER of 26.64% on HKUST dataset, a corpus of Mandarin Chi-\nnese conversational telephone speech, is adopted from our work\n[9]. Its softmax layer is replaced by the language-speciﬁc soft-\nmax layer which is initialized randomly. Through this initial-\nization method, the ASR Transformer can converge very well.\nAll experiments in this paper are conducted by this initialization\nmethod.\nTable 3: Experimental parameters conﬁguration.\nmodel N dmodel h dk dv warmup\nD1024-H16 6 1024 16 64 64 12000 steps\n4.3. Number of merge operations\nFirst, we evaluate how the number of merge operations α in\nBPE affects the performance of the ASR Transformer. When α\nis tiny, the number of sub-words is small. Otherwise the number\nof sub-words is large. Since the training data is quite few on\nlow-resource languages, it means that the number of sub-words\ncannot be too large in order to make sure each sub-word has\nenough training samples.\nFor each monolingual ASR Transformer, we ﬁrst experi-\nment on English dataset for choosing an appropriate α. As\nshown in Table 4, the performance reaches the best when α =\n500 and the number of sub-words is548 on English dataset. Ap-\npended with 4 extra tokens, (i.e. an unknown token (<UNK>),\na padding token ( <PAD>), and sentence start and end tokens\n(<S>/<\\S>)), the total number of sub-words is 552. In this\npaper, we choose α = 500 in monolingual ASR Transformer\nexperiments.\nTable 4: WERs(%) of different αon English dataset.\nα 50 100 500 1000 2000\n# output. 106 156 552 1047 1997\nWER 45.28 44.64 42.77 43.88 43.85\nFor the multilingual ASR Transformer, all languages train-\ning transcripts are combined together to generate the multi-\nlingual symbol vocabulary by BPE. Table 6 shows that α do\nnot affect the performance too much on average. We choose\nα = 3000 in all multilingual ASR Transformer experiments\nand the total number of sub-words is 8062.\n4.4. Results\nThe baseline systems come from our previous work [5] and all\nresults are summarized in Table 5.\nTable 5: Comparison of baseline systems and ASR Transformer on CALLHOME datasets in WER/CER (%). Relative WER/CER\nreduction is also shown between Multi-Transformer-B2 and SHL-MLSTM-RESIDUAL.\nModel # params. MA EN JA AR GE SP Average\nMono-DNN [5] ≈21.0M 53.05 50.45 57.52 61.52 59.11 59.77 56.90\nMono-LSTM [5] ≈17.8M 50.53 48.16 55.14 59.21 56.61 57.71 54.56\nSHL-MDNN [5] 38.0M 50.67 46.77 54.15 58.91 55.94 57.88 54.05\nSHL-MLSTM-RESIDUAL [5] 22.0M 45.85 43.93 50.13 56.47 51.75 53.38 50.25\nMono-Transformer ≈231M 39.62 42.77 39.55 50.78 48.94 54.42 46.01\nMulti\nTransformer 235M 40.28 42.35 39.29 50.87 47.82 53.26 45.65\nTransformer-B 235M 40.56 41.61 38.86 50.96 47.59 53.85 45.57\nTransformer-E 235M 40.49 40.63 38.67 50.16 47.24 52.58 44.96\nTransformer-B2 235M 37.62 40.36 38.13 48.82 46.22 53.07 44.03\nRelative WER/CER Reduction − 17.9% 8.1% 23.9% 13.5% 10.7% 0.6% 12.4%\nTable 6: Multilingual results with differentαin WER/CER (%).\nα 1000 3000 5000 7000 9000\n# output. 6084 8062 10025 11959 13883\nMA 41.14 40.28 40.66 40.14 40.72\nEN 42.76 42.35 42.49 42.73 42.76\nJA 40.04 39.29 38.63 38.68 39.76\nAR 51.04 50.87 51.32 51.15 51.80\nGE 48.92 47.82 48.85 48.21 48.11\nSP 54.34 53.26 53.07 53.37 53.73\nAverage 46.37 45.65 45.84 45.71 46.15\nTable 7: An English example of predictions from Multi-\nTransformer-B2 with different<S Lang>.\nCorrect Target by any means\n<S MA> 八月零零\n<S EN> by any means\n<S JA> バアエリミン\n<S AR> tayyib yacni min\n<S GE> war er nicht mit\n<S SP> vaya a dime mil\nFirst, we train six monolingual ASR Transformers ( Mono-\nTransformer) independently on each language data. As can be\nseen from Table 5, the monolingual ASR Transformer performs\nvery well on each low-resource language and can obtain about\nrelatively 15.7% WER reduction on average compared to mono-\nlingual LSTM (Mono-LSTM).\nFurthermore, we build a single multilingual ASR Trans-\nformer (Multi-Transformer) on all training data together with-\nout using any language information during training and testing.\nWe note that the Multi-Transformer can achieve slightly better\nperformance than Mono-Transformer on average, which repre-\nsents simply pooling the data together can give an acceptable\nrecognition performance by a single multilingual ASR Trans-\nformer.\nAfter analyzing recognition results from Multi-\nTransformer, we ﬁnd that some recognition results are\ncompletely wrong because of language confusion, especially\nwhen the speech utterance is short. For example, sometimes\nan English word “um” is decoded into a German word “ja”,\nbecause they have similar pronunciation.\nSince the language information of training data usually can\nbe known beforehand, we go on to build two multilingual ASR\nTransformers integrating language information as depicted in\nSection 3.3 to alleviate the problem of language confusion.\nHere, the language information is just used during training and\nthe model itself predicts the language symbol during testing.\nFrom Table 5, we can observe that inserting the language sym-\nbol at the end ( Multi-Transformer-E) is better than inserting\nit at the beginning ( Multi-Transformer-B). Compared to SHL-\nMLSTM-RESIDUAL, Multi-Transformer-B can obtain about\nrelatively 10.5% average WER reduction.\nIf the language information of both training and testing data\ncan be known beforehand, we directly take the language sym-\nbol <S Lang>as the sentence start token rather than original\nsentence start token<S>. It forces the multilingual ASR Trans-\nformer to decode a speech utterance into the pointed language,\nwhich greatly alleviate the language confusion during testing.\nAs can be seen from Table 5, Multi-Transformer-B2 performs\nbest and obtain relative 12.4% average WER reduction com-\npared to SHL-MLSTM-RESIDUAL although the improvement\non Spanish is very little. What’s more, an interesting observa-\ntion is that if we give a wrong language symbol <S Lang>\nas the sentence start token, Multi-Transformer-B2 is able to\ntransliterate speech into the pointed language. An English ex-\nample of predictions from Multi-Transformer-B2 with different\n<S Lang>is shown in Table 7. We can ﬁnd that the prediction\nfrom wrong <S Lang>is an approximate pronunciation of the\ncorrect target.\n5. Conclusions\nIn this paper we investigated multilingual speech recognition\non low-resource languages by a single multilingual ASR Trans-\nformer. Sub-words are chosen as the multilingual modeling\nunit to remove the dependency on the pronunciation lexicon. A\ncomparison with SHL-MLSTM with residual learning is inves-\ntigated on CALLHOME datasets with 6 languages. Experimen-\ntal results reveal that a single multilingual ASR Transformer by\ninserting the language symbol at the end can obtain relatively\n10.5% average WER reduction compared to SHL-MLSTM with\nresidual learning if the language information of training data\ncan be employed during training. We go on to show that about\nrelatively 12.4% average WER reduction can be observed com-\npared to SHL-MLSTM with residual learning by giving the lan-\nguage symbol as the sentence start token assuming the language\ninformation being known during training and testing.\n6. References\n[1] J.-T. Huang, J. Li, D. Yu, L. Deng, and Y . Gong, “Cross-language\nknowledge transfer using multilingual deep neural network with\nshared hidden layers,” inAcoustics, Speech and Signal Processing\n(ICASSP), 2013 IEEE International Conference on. IEEE, 2013,\npp. 7304–7308.\n[2] N. T. Vu, D. Imseng, D. Povey, P. Motlicek, T. Schultz, and\nH. Bourlard, “Multilingual deep neural network based acoustic\nmodeling for rapid language adaptation,” in Acoustics, Speech\nand Signal Processing (ICASSP), 2014 IEEE International Con-\nference on. IEEE, 2014, pp. 7639–7643.\n[3] A. Mohan and R. Rose, “Multi-lingual speech recognition with\nlow-rank multi-task deep neural networks,” in Acoustics, Speech\nand Signal Processing (ICASSP), 2015 IEEE International Con-\nference on. IEEE, 2015, pp. 4994–4998.\n[4] R. Sahraeian and D. Van Compernolle, “A study of rank-\nconstrained multilingual dnns for low-resource asr,” in Acous-\ntics, Speech and Signal Processing (ICASSP), 2016 IEEE Inter-\nnational Conference on. IEEE, 2016, pp. 5420–5424.\n[5] S. Zhou, Y . Zhao, S. Xu, and B. Xu, “Multilingual recurrent neural\nnetworks with residual learning for low-resource speech recogni-\ntion,” Proc. Interspeech 2017, pp. 704–708, 2017.\n[6] G. E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent\npre-trained deep neural networks for large-vocabulary speech\nrecognition,” IEEE Transactions on audio, speech, and language\nprocessing, vol. 20, no. 1, pp. 30–42, 2012.\n[7] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[8] T. N. Sainath, R. Prabhavalkar, S. Kumar, S. Lee, A. Kannan,\nD. Rybach, V . Schogol, P. Nguyen, B. Li, Y . Wuet al., “No need\nfor a lexicon? evaluating the value of the pronunciation lexica in\nend-to-end models,” arXiv preprint arXiv:1712.01864, 2017.\n[9] S. Zhou, L. Dong, S. Xu, and B. Xu, “A comparison of modeling\nunits in sequence-to-sequence speech recognition with the trans-\nformer on mandarin chinese,” arXiv preprint arXiv:1805.06239 ,\n2018.\n[10] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, K. Goninaet al., “State-\nof-the-art speech recognition with sequence-to-sequence models,”\narXiv preprint arXiv:1712.01769, 2017.\n[11] R. Sennrich, B. Haddow, and A. Birch, “Neural machine\ntranslation of rare words with subword units,” arXiv preprint\narXiv:1508.07909, 2015.\n[12] S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based sequence-\nto-sequence speech recognition with the transformer in mandarin\nchinese,” arXiv preprint arXiv:1804.10752, 2018.\n[13] B. Li, T. N. Sainath, K. C. Sim, M. Bacchiani, E. Weinstein,\nP. Nguyen, Z. Chen, Y . Wu, and K. Rao, “Multi-dialect speech\nrecognition with a single sequence-to-sequence model,” arXiv\npreprint arXiv:1712.01541, 2017.\n[14] S. Toshniwal, T. N. Sainath, R. J. Weiss, B. Li, P. Moreno, E. We-\ninstein, and K. Rao, “Multilingual speech recognition with a sin-\ngle end-to-end model,” arXiv preprint arXiv:1711.01694, 2017.\n[15] S. Kim and M. L. Seltzer, “Towards language-universal end-to-\nend speech recognition,” arXiv preprint arXiv:1711.02207, 2017.\n[16] S. Watanabe, T. Hori, and J. R. Hershey, “Language indepen-\ndent end-to-end architecture for joint language identiﬁcation and\nspeech recognition,” in Automatic Speech Recognition and Un-\nderstanding Workshop (ASRU), 2017 IEEE . IEEE, 2017, pp.\n265–271.\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems, 2017, pp.\n6000–6010.\n[18] M. Johnson, M. Schuster, Q. V . Le, M. Krikun, Y . Wu, Z. Chen,\nN. Thorat, F. Vi´egas, M. Wattenberg, G. Corradoet al., “Google’s\nmultilingual neural machine translation system: enabling zero-\nshot translation,” arXiv preprint arXiv:1611.04558, 2016.\n[19] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarzet al.,\n“The kaldi speech recognition toolkit,” in IEEE 2011 workshop\non automatic speech recognition and understanding , no. EPFL-\nCONF-192584. IEEE Signal Processing Society, 2011.\n[20] H. Sak, A. Senior, K. Rao, and F. Beaufays, “Fast and accurate\nrecurrent neural network acoustic models for speech recognition,”\narXiv preprint arXiv:1507.06947, 2015.\n[21] A. Kannan, Y . Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, “An analysis of incorporating an external lan-\nguage model into a sequence-to-sequence model,” arXiv preprint\narXiv:1712.01996, 2017.\n[22] T. Hori, S. Watanabe, Y . Zhang, and W. Chan, “Advances in joint\nctc-attention based end-to-end speech recognition with a deep cnn\nencoder and rnn-lm,” arXiv preprint arXiv:1706.02737, 2017.\n[23] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,” arXiv preprint arXiv:1412.6980, 2014.\n[24] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Re-\nthinking the inception architecture for computer vision,” in Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 2818–2826.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7842567563056946
    },
    {
      "name": "Pronunciation",
      "score": 0.648045539855957
    },
    {
      "name": "Transformer",
      "score": 0.6311479210853577
    },
    {
      "name": "Speech recognition",
      "score": 0.6126075387001038
    },
    {
      "name": "Language model",
      "score": 0.5706958770751953
    },
    {
      "name": "Natural language processing",
      "score": 0.537325918674469
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5091974139213562
    },
    {
      "name": "Cache language model",
      "score": 0.48165786266326904
    },
    {
      "name": "Lexicon",
      "score": 0.44624701142311096
    },
    {
      "name": "Natural language",
      "score": 0.31953972578048706
    },
    {
      "name": "Linguistics",
      "score": 0.19043806195259094
    },
    {
      "name": "Universal Networking Language",
      "score": 0.14534199237823486
    },
    {
      "name": "Comprehension approach",
      "score": 0.08706918358802795
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": []
}