{
  "title": "Language models are not naysayers: an analysis of language models on negation benchmarks",
  "url": "https://openalex.org/W4385718075",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3152431279",
      "name": "Thinh Hung Truong",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2096893938",
      "name": "Timothy Baldwin",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A717012417",
      "name": "Karin Verspoor",
      "affiliations": [
        "MIT University",
        "University of Melbourne",
        "RMIT University"
      ]
    },
    {
      "id": "https://openalex.org/A2188741563",
      "name": "Trevor Cohn",
      "affiliations": [
        "University of Melbourne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3002330681",
    "https://openalex.org/W2971044268",
    "https://openalex.org/W2139865360",
    "https://openalex.org/W4280550797",
    "https://openalex.org/W4287888537",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4389520255",
    "https://openalex.org/W4307309259",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W4285136463",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4385573170",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3033254023",
    "https://openalex.org/W3105661746",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3097977265",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3176245452",
    "https://openalex.org/W4385574034",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4285287265",
    "https://openalex.org/W3161374759",
    "https://openalex.org/W4385392860",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4297412056",
    "https://openalex.org/W4385573971",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4389519396",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3099843385",
    "https://openalex.org/W4389520124",
    "https://openalex.org/W4404783524",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4285309087",
    "https://openalex.org/W3034995113"
  ],
  "abstract": "Negation has been shown to be a major bottleneck for masked language models, such as BERT. However, whether this finding still holds for larger-sized auto-regressive language models (\"LLMs\") has not been studied comprehensively. With the ever-increasing volume of research and applications of LLMs, we take a step back to evaluate the ability of current-generation LLMs to handle negation, a fundamental linguistic phenomenon that is central to language understanding. We evaluate different LLMs - including the open-source GPT-neo, GPT-3, and InstructGPT - against a wide range of negation benchmarks. Through systematic experimentation with varying model sizes and prompts, we show that LLMs have several limitations including insensitivity to the presence of negation, an inability to capture the lexical semantics of negation, and a failure to reason under negation.",
  "full_text": "Proceedings of the The 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 101–114\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nLanguage models are not naysayers:\nAn analysis of language models on negation benchmarks\nThinh Hung Truong1 Timothy Baldwin1,3 Karin Verspoor2,1 Trevor Cohn1,∗\n1University of Melbourne 2RMIT University 3MBZUAI\nhungthinht@student.unimelb.edu.au tb@ldwin.net\nkarin.verspoor@rmit.edu.au trevor.cohn@unimelb.edu.au\nAbstract\nNegation has been shown to be a major bot-\ntleneck for masked language models, such as\nBERT. However, whether this finding still holds\nfor larger-sized auto-regressive language mod-\nels (“LLMs”) has not been studied compre-\nhensively. With the ever-increasing volume\nof research and applications of LLMs, we take\na step back to evaluate the ability of current-\ngeneration LLMs to handle negation, a funda-\nmental linguistic phenomenon that is central\nto language understanding. We evaluate differ-\nent LLMs — including the open-source GPT-\nneo, GPT-3, and InstructGPT — against a wide\nrange of negation benchmarks. Through sys-\ntematic experimentation with varying model\nsizes and prompts, we show that LLMs have\nseveral limitations including insensitivity to the\npresence of negation, an inability to capture the\nlexical semantics of negation, and a failure to\nreason under negation.\n1 Introduction\nDespite being a core linguistic phenomenon, nega-\ntion remains a major stumbling block for modern\nNLP architectures (Kassner and Schütze, 2020;\nHossain et al., 2022). A reason for this could be\nthat texts containing negation are underrepresented\nin training data of language models, as humans tend\nto express themselves using affirmative rather than\nnegative expressions (Ettinger, 2020). Regardless,\nnegation has been shown to be challenging even for\nhumans to correctly interpret due to the diversity of\nforms across domains (Truong et al., 2022a). For\ninstance, in clinical documents, many acronyms\nare used to denote negation such as NAD (no ab-\nnormality detected), and implicit negation abounds,\nsuch as normal chest x-ray scan, which implies the\nabsence of an abnormality. Even more complex is\nthe use of negation in combination with other lin-\nguistic phenomena such as quantifiers, gradable ad-\njectives (not unattractive does not imply attractive)\n∗Now at Google DeepMind.\n(Truong et al., 2022b); licensing context (negative\npolarity items, e.g. any, either, yet, normally appear\nin certain negative grammatical contexts Warstadt\net al. (2019)); downward entailment (A man owns a\ndog entails A man owns an animal but A man does\nnot own a dog does not entail A man does not own\nan animal) (Geiger et al., 2020).\nTraditionally, negation has been treated as a stan-\ndalone problem, e.g. as negation detection (Chap-\nman et al., 2001). The investigation of the im-\npact of negation in various downstream tasks (Hos-\nsain et al., 2022; Hossain and Blanco, 2022a), or\nthrough probing (Ettinger, 2020) has revealed sev-\neral limitations of modern large language models\n(“LLMs”) in handling negation. Given that LLMs\nare being adopted in an ever-growing range of tasks\nand have been shown to display emergent abilities\nfor high-level tasks that require complex reasoning\n(Wei et al., 2022a), we are interested in exploring\nhow the handling of negation has progressed.\nIn this work, we investigate the performance\nof auto-regressive language models on different\nnegation-focused benchmarks. Instead of just look-\ning at samples containing negation in common\nNLP datasets, we consider datasets in which nega-\ntion plays an important role in making the correct\njudgement. In particular, we classify the bench-\nmarks into three categories corresponding to the\nrequisite negation reasoning abilities: (1) sensitiv-\nity to negation through cloze completion (fill-in-\nthe-blank) queries of factual statements; (2) lexi-\ncal semantics of negation through classification of\nantonym/synonym relationships; and (3) ability to\nreason with negation through language inference\ntasks.\nWe conduct extensive experiments using prompt-\nbased learning to facilitate zero- and few-shot eval-\nuation of LLMs, and find the following:\n• larger LMs are more insensitive to negation\ncompared to smaller ones (Section 3);\n101\nBenchmark Task # Samples Example\nMKR-NQ Completion 3360 Query: Iburofen isn’t a kind of [MASK]. Wrong completions:\nNSAID, painkiller, drug, medicine.\nMWR Completion 27546 Query: Demand is an antonym of [MASK]. Wrong completions:\nnecessitate, demands, request, requirement, imposition, need,\ndemand.\nSAR NLI 2000 Word 1: Superiority / Word 2: Inferiority / Label: Antonym\nNegNLI NLI 4500 P: They watched me constantly for weeks. / H: They did not leave\nme on my own for weeks. / Label: Entailment\nNaN-NLI NLI 258 P: Not all people have had the opportunities you have had. /\nH: Some people have not had the opportunities you have had. /\nLabel: Entailment\nMoNLI NLI 200 P: The man does not own a dog. / H: The man does not own a\nmammal. / Label: Not Entailment\nTable 1: Summary of the negation-related benchmark datasets used in this paper.\n• LLMs lack lexical semantic knowledge about\nnegation, yielding almost random perfor-\nmance for synonym/antonym classification\n(Section 3);\n• LLMs have limited ability to reason un-\nder negation, performing worse than random\nacross most NLI datasets (Section 3). Only\nwith the latest instruction fine-tuned model\n(Ouyang et al., 2022; Chung et al., 2022) do\nwe observe above-chance performance (Sec-\ntion 3);\n• For each dataset, we also experiment with\nprompt variations and find that in most cases,\nproviding more information (context, instruc-\ntion, simple wording) leads to a degradation\nin performance.\n2 Experimental settings\nIn this section, we outline the settings that ,\nincluding benchmark datasets, models to eval-\nuate, and the prompts that were used. Our\ncode is available at https://github.com/\njoey234/llm-neg-bench.\n2.1 Benchmarks\nWe use a range of benchmark datasets that exhibit\nthe effects of negation across a wide range of tasks,\nin the form of either cloze completion or classifica-\ntion tasks. An overview of the datasets is presented\nin Table 1, categorized according to purpose and\nthe type of negation they contain. Specifically, we\nfocus on: (1) investigating whether LLMs are sen-\nsitive to the presence of negation in factual state-\nments; (2) testing whether LLMs capture negation\nin lexical semantics relations (synonym/antonym\nrelations); and (3) investigating whether LLMs are\nable to reason under negation through multiple nat-\nural language inference benchmarks. We discuss\nthe datasets in greater detail in Section 3.\n2.2 Models\nFor the LLMs, we primarily focus on open-source\nauto-regressive LLMs with up to 6.7B parame-\nters, including GPT-Neo (Black et al., 2021), and\nOPT (Zhang et al., 2022), which are claimed to\nbe comparable in performance to similar-sized\nGPT-3 class models. Architecture-wise, they are\nboth decoder-only PLMs pre-trained with a causal\nLM objective, with the main difference being in\ntheir pre-training corpora: GPT-neo was trained\nsolely on the Pile dataset (Gao et al., 2020) consist-\ning of 22 sub-datasets spanning different sources,\nwhereas OPT was trained on the combination of\ndatasets used in RoBERTa (Liu et al., 2019), Pile,\nand PushShift Reddit (Baumgartner et al., 2020).\nWe use the official model checkpoints from Hug-\ngingFace hub,1 as detailed in Appendix A. We ex-\nperiment with smaller-sized variants of these two\nclasses of models to observe the effect of scaling\non their performance over different benchmarks.\nWe also consider base GPT-3 (175B) (Brown\net al., 2020), and its instruction fine-tuned variant\nInstructGPT (Ouyang et al., 2022), as well as a\nstrong open-source instruction-tuned model FLAN-\nT5-XXL (11B) (Chung et al., 2022), to examine\nhow recent commercial LLMs perform on nega-\ntion.\n1https://huggingface.co/models\n102\nTask Prompt\nname\nExample\nMKR-\nNQ\nDefault An expectorant isn’t a type of\nContrasting An expectorant is a type of medicine. An expectorant isn’t a type of\nDiscourse An expectorant is a type of medicine. Therefore, an expectorant isn’t a type of\nMask An [MASK] is a type of medicine. An [MASK] isn’t a type of\nMWR Default Greed is an antonym of\nQuote The word “greed” is an antonym of the word “\nSAR Default Choose the correct answer: bad and good are antonyms or synonyms? Answer:\nSimple Choose the correct answer: bad and good are opposite or similar? Answer:\nNegation Antonyms are words with opposite meaning. Synonyms are words with similar\nmeaning. Choose the correct answer: bad and good are antonyms or synonyms?\nAnswer:\nNLI Default Not all people have had the opportunities you have had.\nQuestion: Some people have not had the opportunities you have had. True, False, or\nNeither?\nAnswer:\nNegation The question requires reasoning about negation.\nNot all people have had the opportunities you have had.\nQuestion: Some people have not had the opportunities you have had. True, False, or\nNeither?\nAnswer:\nTable 2: Prompts used for each task\n2.3 Prompts\nWe adopt prompt-based learning to enable zero-\nand few-shot evaluation of LLMs (Radford et al.,\n2019). Given that LLMs have been found to be\nsensitive to prompt variation (Wei et al., 2021), and\nthat more natural-sounding prompts correlate with\nmodel performance (Gonen et al., 2022), we also\nexperiment with different types of prompts (see\nTable 2).\nWe use GPT-3 style prompts (Brown et al., 2020)\nas the Default setting. As handling negation plays\nan important role in all tasks, we also design\nprompts to prime the LLMs to focus more on the\nnegation context, by introducing modifications spe-\ncific to each task. In detail, for the cloze com-\npletion task MKR-NQ, we investigate whether a\ngiven model can detect the difference between two\ncontrasting sentences (with/without negation). To\nachieve this, we prepend the prompt with the corre-\nsponding sentence without negation (Contrasting\nprompt). In addition, we also evaluate alternative\nprompts where we connect the two sentences with\na discourse marker ( Discourse prompt), or mask\nthe main subject to encourage the model to attend\nmore to negation cues (Mask prompt).\nFor antonym/synonym-related tasks (MWR,\nSAR), we also experiment with simplifying the\nprompt and use descriptive terms rather than the\nformal names of the relations (e.g. antonyms, syn-\nonyms →opposite of, similar to), based on the in-\ntuition that these terms will appear more frequently\nin the pre-training data.\nFinally, for classification tasks, we propose\nnegation-aware prompting (Negation prompt) by\nmodifying the prompts to explicitly state that the\ntask involves reasoning about negation. Note that\nwe explicitly include class options in the prompts\nto help reduce the effect of the surface form compe-\ntition causing LLMs to assign lower probabilities\nto the correct answers (Holtzman et al., 2021).\nFor datasets with an accompanying training set\n(SAR, MoNLI), we also experiment with few-shot\nevaluation formulated as in-context learning by\nprepending the input prompts with 10 random sam-\nples from the training set.\n2.4 Metrics\nTo evaluate cloze completion tasks, we employ\nWeighted Hit Rate (WHR) (Jang et al., 2022b),\nwhich measures the number of matches between\nthe top-k predicted words and a given set of target\nwrong predictions, taking into account the predic-\n103\ntion probabilities:\nWHRk(x, W) =\n∑k\ni=1 ci ×1(wi ∈Wx)\n∑k\ni=1 ci\n(1)\nwhere Wx is the wrong prediction set of the input\nquery x, and wi is the top i-th prediction with con-\nfidence score ci, obtained by taking the softmax of\nlog probabilities p(wi|x) from the LM. Note that\nthe model performance is better if there are fewer\nmatches between models’ predictions and wrong\ncompletions, WHR is an error metric (lower is bet-\nter). One problem with the WHR metric is that\nwe can only evaluate using a fixed set of wrong\npredictions. Regardless, we believe the relative\nperformance numbers are indicative of model per-\nformance.\nFor classification tasks, we evaluate using Ac-\ncuracy, noting that all datasets are reasonably bal-\nanced.\n3 Main findings\nWe summarize the main findings in this section.\nIn general, the performance of GPT-neo and OPT\nfollows a similar trend across all benchmarks (we\npresent GPT-neo results; results of OPT models are\nin Appendix B).\nFinding 1: Larger LMs are more insensitive to\nnegation\nMKR-NQ (Jang et al., 2022b) Masked Knowl-\nedge Retrieval – Negated Query (MKR-NQ) is a\nnegated version of the LAMA dataset (Petroni et al.,\n2019), which contains lexicalized statements of\ntriples in ConceptNet (Speer et al., 2017). This\ndataset contains factual statements with verbal\nnegations (i.e. negators not, don’t are associated\nwith the main verb of the sentence), e.g. Iburofen\nis a type of medicine. →Iburofen isn’t a type of\nmedicine.\nEach sample contains the query along with a set\nof wrong word completions, supporting the eval-\nuation of the sensitivity of the model to negation\nby measuring how likely a model will generate\nincorrect completions. Note that MKR-NQ only\nconsiders sample sentences that contain a single\nverb, making it trivial to negate the original sen-\ntences.\nFindings From Figure 1, which is based on\nLLMs with a negated factual statement ( Default\nprompt), we observe relatively low hit rates ( <\n0.15) across all model sizes, and a clear inverse\nscaling trend between model sizes and their perfor-\nmance. The smallest variant ( GPT-neo-125M)\nhas the best performance, which is comparable\nto that of masked language model of a similar\nsize (BERT-base, 110M parameters) (Jang et al.,\n2022b). This phenomenon reflects the finding that\nlarger models tend to memorize the training data\nmore (McKenzie et al., 2022; Jang et al., 2022a).\nMoreover, higher hit rates for top-1 predictions\nsuggest that models predict wrongly with high con-\nfidence.\nFor Contrasting prompts, in which we prepend\nthe negated statement with its non-negated ver-\nsion, we notice a drastic increase in WHR, show-\ning that models are prone to repeating what is pre-\nsented in the prior context, confirming the finding\nof Kassner and Schütze (2020). When a discourse\nterm is added to connect the two sentences ( Dis-\ncourse prompt), we do not observe any improve-\nment, and the performance of the largest model\nis even worse. To investigate whether this phe-\nnomenon is attributable to models not being able to\ndetect the presence/absence of negation, we experi-\nment with masking out the main noun/verb of the\nqueries (Mask prompt). We observed even higher\nWHR, especially for the top-1 prediction in this set-\nting. The results suggest that repetitions are caused\nmore by LLMs being easily primed by repeating\nwhat is present in the previous context, than by gen-\nerating words that are closely associated with the\nmain subject of interest. This again shows that the\nmodels cannot differentiate between identical con-\ntexts, differing only on whether negation is present\nor absent (i.e., outputs tend to be similar with or\nwithout negation).\nTo further analyze the outputs, we calculate the\nperplexity (PPL) of the generated predictions to\ndetermine their plausibility (Wilcox et al., 2020).\nHere, we choose the model with the best WHR5\nscore on the MKR-NQ benchmark, and calcu-\nlate the mean perplexity over all queries for each\nprompt type (5 completions for each query). PPL\nis calculated as the exponentiated average nega-\ntive log-likelihood of a sequence, with exponent\nbase e. As a point of reference, we calculated the\naverage perplexity of the provided completion of\nthe original non-negated dataset (denoted Corpus).\nFrom the reported perplexities (Table 3), we can\nsee that Default output are the most plausible (with\nPPL markedly lower than Corpus), while Contrast-\n104\nFigure 1: Zero-shot performance of GPT-neo on MKR-NQ using different prompts under the Weighted Hit Rate\n(WHR) metrics (lower scores are better). Note the different scale for the left-most plot.\nSetting Example Mean\nPPL↓\nCorpus [Baseball is a type of sport.] 434.42\nDefault [Baseball isn’t a type of sport.] 288.94\nContrasting Baseball is a type of sport.\n[Baseball isn’t a type of sport.]\n533.56\nDiscourse Baseball is a type of sport.\nTherefore, [baseball isn’t a type\nof sport.]\n477.44\nMask MASK is a type of sport.\n[MASK isn’t a type of sport.]\n448.23\nTable 3: Mean perplexity (PPL) calculated using the\nGPT-J-6B model. Only the strings enclosed in square\nbrackets are considered during calculation in order to\nprovide a fair comparison with similar token length.\nFor Corpus, PPL is calculated using the provided gold\ncompletion.\ning is the least natural. The remaining prompts\ntypes (Discourse, Mask) are comparable to Corpus.\nThese results show that LLMs can indeed generate\nplausible and human-like output for this task.\nFinding 2: LMs fail to capture\nsynonym/antonym lexical relations\nMWR (Jang et al., 2022b) To test the ability\nof LMs to capture negative lexical semantics, we\nuse MWR dataset, where models are asked to pre-\ndict the antonym/synonym of a target word. The\ndataset was constructed by using the most frequent\nnouns, adjectives, and adverbs that appear in SNLI\n(Bowman et al., 2015), then choosing their corre-\nsponding synonyms and antonyms from Concept-\nNet (Speer et al., 2017). The dataset also con-\ntains different wordings for antonym-asking and\nsynonym-asking queries (e.g. is the opposite of, is\ndifferent from and is similar to, is a rephrasing of)\nto test model sensitivity to prompt variations.\nFindings From Figure 2, we can observe the\nsame inverse scaling trend as for MKR-NQ using\nFigure 2: Zero-shot performance of GPT-neo on MWR\nusing different prompts (WHR metrics; lower is better)\nQuery Wrong comple-\ntions\nTop-5 predic-\ntions\nGreed is an\nantonym of\ngreed, avarice,\ndesire, greeds,\ngluttony\naltruism,\nself-sacrifice,\nself-denial,\nself-abnegation,\ngods\nFinale is an\nantonym of\nconclusion, fin-\nish, finales, fi-\nnale\nlast, epiphany,\nfinality, anti-\nclimax, anti-\nclimactic\nTable 4: Example output of GPT-J-6B on MWR.\nbolded words are related to target words, but are nei-\nther synonyms nor antonyms. underlined are wrong\nantonyms but are not in the given set of wrong comple-\ntions.\nthe Default prompt, where the hit rate of the small-\nest model is around 0.02, better than previously-\nreported SOTA results (Jang et al., 2022b). With a\nmore natural query with more focus on the target\nwords via quotation marks ( Quote prompt), sur-\nprisingly, we noticed a drastic jump in hit rates.\nHowever, MWR may not be a good indicator of\nmodel performance due to how the task is framed.\nOne problem is that models can generate words\nthat are not in the given wrong prediction set, but\nare also irrelevant, and are also neither antonyms\nnor synonyms of the given target words, as demon-\nstrated in Table 4.\n105\nFigure 3: Zero-shot performance of GPT-neo on SAR\ndataset using different prompts (accuracy metric; higher\nis better)\nSAR (Jang et al., 2022b) To further investigate\nthe ability of LLMs to capture negative lexical se-\nmantics, we consider the antonym/synonym rela-\ntion classification task (SAR). Different from the\nMWR cloze-style synonym/antonym prediction\ntask, this benchmark is framed as a binary clas-\nsification task of predicting the correct antonym\nor synonym relationship between two given words.\nData is once again taken from ConceptNet, where\ntriplets with synonym and antonym relations are\nextracted in equal numbers (1000 samples for each\nrelation).\nFindings In contrast to the high results for MWR,\nwe find that for this task, model performance is\nequivalent to random, with accuracy fluctuating\naround 0.5 (Figure 3). For prompt variants, we do\nnot observe any meaningful improvement, in that\nSimple follows a similar trend to Default and Nega-\ntion performs better for larger models (2.7B and\n6B). This is a huge degradation from previous fully\nfine-tuned results over encoder models. For in-\nstance, Jang et al. (2022b) reported that BERTlarge\nachieves 92.5% accuracy on SAR. We argue that\nthis is a specific task that is not captured in the next\ntoken prediction training objective of LLMs and\nthus, requires explicit supervision.\nFinding 3: LLMs are unable to reason under\nnegation\nNegNLI (Hossain et al., 2020) NegNLI con-\ntains 4500 premise–hypothesis pairs with impor-\ntant negation, where negation is essential in making\nthe correct judgement about the relationship be-\ntween the premise–hypothesis pairs. Samples are\nextracted from the commonly-used NLI datasets\n(RTE Dagan et al. (2005), SNLI Bowman et al.\n(2015), MNLI Williams et al. (2018)), then the\nnegator not is added to the main verb either in the\npremise, hypothesis, or both. Here, we consider\neach subset separately, as the number of classes are\nnot the same, and denote them SNLI-neg, MNLI-\nneg, RTE-neg.\nMoNLI (Geiger et al., 2020) MoNLI is an NLI\ndataset focused on lexical entailment and negation.\nSpecifically, the dataset investigates the downward\nmonotonicity property where negation reverses en-\ntailment relations (e.g. dance entails move, but not\nmove entails not dance). MoNLI was created by\nextending samples from SNLI by substituting the\nnouns by their hypernyms/hyponyms from Word-\nNet (Miller, 1998).\nNaN-NLI (Truong et al., 2022b) NaN-NLI is a\ntest suite which focuses on sub-clausal negation,\nin which only part of the sentence’s meaning is\nnegated, thus making it harder to correctly deter-\nmine the correct negation scope (e.g. inNot the first\ntime that they pulled that off the negation scope is\nonly Not the first time and the main clause of the\nsentence they pulled that off is not negated). Each\npremise–hypothesis pair is constructed so that the\ncorresponding hypotheses are constructed to reflect\ndifferent interpretations that the negated instance\nin the premise are likely to be misunderstood for.\nFindings Similar to the antonym/synonym clas-\nsification task, the performance for most negation-\nfocused NLI benchmarks is low. In particular,\nfor all NLI datasets, the performance is gener-\nally lower than baseline. As shown in Figure 4,\nscaling up model size has almost no effect, and\nthe largest model performs worse in many cases,\neven when the prompt explicitly states that the\ntask requires reasoning about negation (Negation\nprompt). For datasets which include a training set\n(SAR, MoNLI), we also experimented with few-\nshot learning but did not observe any noticable\nimprovement (Figure 5). One exception is that the\n2.7B model seems to pick up some signal from the\nprovided MoNLI training samples, but falls back\nagain when we increase the model size to 6B.\nEven with general NLI datasets, zero-shot ap-\nplications of LLMs were previously shown to be\nroughly equivalent to a random baseline (Wei et al.,\n2021). When negation is involved, the task be-\ncomes even more complex. As pointed out in\nBrown et al. (2020), one possible reason that LLMs\n106\nFigure 4: Zero-shot performance of GPT-neo on NLI datsets using different prompts (higher is better). The dashed\nline denotes a random baseline. Note that RTE-neg and MoNLI are 2-way classification tasks while the rest are\n3-way.\nBenchmark GPT-J-6B GPT-3 InstructGPT InstructGPT w/\nNeg. prompt\nFLAN-T5-XXL\nw/ Neg. prompt\nMKR-NQ\nWHR5\n↓ 0.083 0.172 0.195 NA NA\nMWR 0.125 0.488 0.504 NA NA\nSAR\nAccuracy\n↑\n0.490 0.501 0.687 0.780 0.507\nSNLI-neg 0.316 0.267 0.640 0.673 0.477\nMNLI-neg 0.275 0.359 0.548 0.625 0.354\nRTE-neg 0.211 0.525 0.767 0.807 0.770\nNaN-NLI 0.298 0.469 0.647 0.682 0.376\nMoNLI 0.500 0.540 0.470 0.400 0.500\nTable 5: Zero-shot results on the different benchmarks. “NA” denotes that Negation prompts are not applicable to\nMKR-NQ and MWR. The best results are bolded for each task (row).\nFigure 5: 10-shot performance of GPT-neo on SAR and\nMoNLI using Default prompt (higher is better)\nstruggle with NLI is that the samples consist of two\ndisjoint sentences, which are unlikely to appear\nnaturally in standard training corpora. We hypothe-\nsise that NLI is a generally hard task that requires\nsubstantially more supervision in order for models\nto detect meaningful patterns.\nFinding 4: Instruction fine-tuning improves\nreasoning under negation\nWe further evaluate with GPT-3 class models\nof significantly larger scale (175B), which have\nbeen shown to achieve strong results in zero- and\nfew-shot settings across a wide range of tasks\n(Brown et al., 2020). In detail, we benchmark\nthe largest GPT-3 model (text-davinci-001:\nBrown et al. (2020)) and its variant InstructGPT,\nwhich is trained to follow human instructions using\nreinforcement learning ( text-davinci-003:\nOuyang et al. (2022)). The results can be found in\nTable 5.\nFor the base GPT-3 model, the results over\nmost benchmarks are no better than much smaller\nlanguage models ( GPT-neo-125M). For cloze-\ncompletion tasks, consistent with the earlier-\n107\nFigure 6: A ChatGPT-generated output of a failed neg-\native monotonicity reasoning sample. The output was\ngenerated using ChatGPT Feb 13 Version\nobserved trend of larger models performing worse,\nwe observe higher (worse) WHR scores compared\nto that of smaller language models, confirming our\nfinding that larger models are more insensitive to\nthe presence of negation. Results get even worse\nwith using the instruction fine-tuned model.\nOn the other hand, for most classification tasks,\nInstructGPT achieves better zero-shot results than\nother models. In addition, using this model in com-\nbination with explicit instruction about negation\n(Negation prompt) further improves performance,\nwhich we did not observe for other LLMs. It is,\nhowever, unclear what data the instruction-tuning\nprocess was performed on. Thus, the huge gain in\nperformance could be attributed to the existence\nof similar patterns in the training set (i.e. explicit\nsupervision over similar tasks). Interestingly, In-\nstructGPT performance on MoNLI did not increase\n(it underperfomed other models). We hypothesize\nthat this is due to an inductive bias from model’s\nability to reason with hypernymy. For instance,\nthe model can understand that “dog is an animal”\n(and therefore own an animal entails own a dog),\nbut incorrectly generalizes this logic to a similar\nsample containing negation (not own a dog entails\nnot own an animal). This is indeed true when we\nlook at the explanation generated by ChatGPT, the\nsubsequent model to InstructGPT (Figure 6).\nWe also experiment with the instruction-tuned\nFLAN-T5-XXL model (Chung et al., 2022) and\nfind that the results are better than GPT-3 for most\nNLI tasks, despite being ∼16x smaller. These re-\nsults suggest that instruction fine-tuning has much\ngreater impact than model scaling in terms of mod-\nels developing the ability to perform reasoning\ntasks under negation.\n4 Related work\nOur work builds upon previous research on nega-\ntion. In particular, we were inspired by the pio-\nneering works of Kassner and Schütze (2020) and\nEttinger (2020), which reveal that pre-trained lan-\nguage models have a major issue in being insensi-\ntive to the presence of negation, based on evalua-\ntion over a set of cloze-style queries. Following this\nline of research, Jang et al. (2022b) also explored\nnegation in a cloze completion context by negating\nfactual statements extracted from ConceptNet and\ncome to a similar finding.\nIn a broader context, Hossain et al. (2020, 2022)\ninvestigated the performance of BERT-based meth-\nods on samples containing negation in the GLUE\n(Wang et al., 2018) and SuperGLUE (Wang et al.,\n2019) datasets. Their main finding is that the re-\nsults for the subsets containing only negation are\nlower than those without, as well as the whole\ntest set, showing that models struggle with nega-\ntion, even when fine-tuned on relevant training data.\nRavichander et al. (2022) proposed the challeng-\ning CONDAQA dataset to test the ability of mod-\nels to reason about the implications of negation.\nThe authors conducted comprehensive analysis of\ndifferent types of LLMs under different settings,\nand found that the best-performing models were\nstill well below human performance. Negation has\nalso been investigated as part of psycholinguistic\nprobing datasets (Lialin et al., 2022; Jumelet et al.,\n2021; Stali¯unait˙e and Iacobacci, 2020). Contrast-\ning previous finding, Gubelmann and Handschuh\n(2022) found that the ability to understand nega-\ntion of LMs is underestimated in previous studied.\nThrough designing a controlled dataset with min-\nimal pairs varying in syntactic structure, gender,\nprofession, and first name, they concluded that the\nmodels are indeed sensitive to negation and thus,\ntheir struggle comes more from the contextualiza-\ntion of the tasks.\nAs part of the analysis on emergent abilities of\nLMs, negation has been shown to be one of the\ntasks that displays a flat scaling curve (Wei et al.,\n2022a) or even inverse-scaling (McKenzie et al.,\n2022). This behaviour was later shown to be allevi-\nated by instruction fine-tuning (Wei et al., 2022b).\nThe effectiveness of instruction fine-tuning is fur-\nther supported in Jang and Lukasiewicz (2023).\nThe authors investigated the logical consistency\nof ChatGPT and found that ChatGPT understands\nnegation and antonyms much better than previous\n108\nmodels.\nBeside probing and evaluation, there have also\nbeen works on making language models more ro-\nbust to negation, including unlikelihood training\n(Hosseini et al., 2021), adaptive pre-training on\nrelevant data (Truong et al., 2022a), leveraging af-\nfirmative interpretations from negation (Hossain\nand Blanco, 2022b), and learning better representa-\ntion of negation through contrastive learning (Jiang\net al., 2022; Wang et al., 2022).\n5 Conclusion\nWe have shown that LLMs still struggle with dif-\nferent negation benchmarks through zero- and few-\nshot evaluations, implying that negation is not prop-\nerly captured through the current pre-training objec-\ntives. With the promising results from instruction-\ntuning, we can see that rather than just scaling up\nmodel size, new training paradigms are essential\nto achieve better linguistic competency. Through\nthis investigation, we also encourage the research\ncommunity to focus more on investigating other\nfundamental language phenomena, such as quan-\ntification, hedging, lexical relations, and downward\nentailment.\n6 Limitations\nFirst, regarding the experimental settings, theWHR\nmetrics used to evaluate cloze completion tasks are\nimperfect, as we discussed. Framing cloze com-\npletion tasks in the style of multiple-choice ques-\ntion answering to limit the options that models are\nevaluated on would be a good direction to follow\n(Robinson et al., 2022). In addition, the prompt en-\ngineering in this work is in no way exhaustive, and\ncould be extended using different prompt engineer-\ning strategies such as soft prompt tuning (Lester\net al., 2021), or mining- and paraphrasing-based\nmethods to generate high quality prompts (Jiang\net al., 2020).\nSecond, due to computational constraints, we\ncould not perform an extensive set of experiments\nfor larger models like PaLM (with up to 540B pa-\nrameters) (Chowdhery et al., 2022). Recent work\nby Wei et al. (2022b) has shown that the inverse\nscaling trend on several benchmarks can be alle-\nviated using the large instruction fine-tuned mod-\nels such as FLAN-PaLM-540B, which is largely\nin line with our findings regarding InstructGPT\nand FLAN-T5. With a small-scale experiment, we\nfound that ChatGPT displayed strong performance\non challenging samples in the investigated bench-\nmark, so the main findings of the paper may not\nhold true for newer LLMs.\nFinally, this work only considers negation in the\nEnglish language. There is every reason to believe\nthat negation is an equally challenging problem in\nother languages. As this is a linguistically-intensive\ntask, and requires native speakers to conduct thor-\nough analysis of the results, we leave this for future\nwork.\nAcknowledgement\nThe authors would like to thank the anonymous\nreviewers for their detailed, kind, and construc-\ntive reviews. This research was conducted by the\nAustralian Research Council Training Centre in\nCognitive Computing for Medical Technologies\n(project number ICI70200030) and funded by the\nAustralian Government. This research was un-\ndertaken using the LIEF HPC-GPGPU Facility\nhosted at the University of Melbourne. This Fa-\ncility was established with the assistance of LIEF\nGrant LE170100200.\nReferences\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\nPushShift Reddit dataset. In Proceedings of the Inter-\nnational AAAI Conference on Web and Social Media,\nvolume 14, pages 830–839.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nWendy W Chapman, Will Bridewell, Paul Hanbury, Gre-\ngory F Cooper, and Bruce G Buchanan. 2001. A\nsimple algorithm for identifying negated findings and\ndiseases in discharge summaries. Journal of Biomed-\nical Informatics, 34(5):301–310.\n109\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The Pascal Recognising Textual Entailment\nchallenge. In Machine learning challenges workshop,\npages 177–190. Springer.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe Pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nAtticus Geiger, Kyle Richardson, and Christopher Potts.\n2020. Neural natural language inference models\npartially embed theories of lexical entailment and\nnegation. In Proceedings of the Third BlackboxNLP\nWorkshop on Analyzing and Interpreting Neural Net-\nworks for NLP, pages 163–173, Online. Association\nfor Computational Linguistics.\nHila Gonen, Srini Iyer, Terra Blevins, Noah A Smith,\nand Luke Zettlemoyer. 2022. Demystifying prompts\nin language models via perplexity estimation. arXiv\npreprint arXiv:2212.04037.\nReto Gubelmann and Siegfried Handschuh. 2022. Con-\ntext matters: A pragmatic study of PLMs’ negation\nunderstanding. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 4602–4621,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form com-\npetition: Why the highest probability answer isn’t\nalways right. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7038–7051, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nMd Mosharaf Hossain and Eduardo Blanco. 2022a.\nLeveraging affirmative interpretations from negation\nimproves natural language understanding. arXiv\npreprint arXiv:2210.14486.\nMd Mosharaf Hossain and Eduardo Blanco. 2022b.\nLeveraging affirmative interpretations from negation\nimproves natural language understanding. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 5833–\n5847, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nMd Mosharaf Hossain, Dhivya Chinnappa, and Eduardo\nBlanco. 2022. An analysis of negation in natural lan-\nguage understanding corpora. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n716–723, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nMd Mosharaf Hossain, Venelin Kovatchev, Pranoy\nDutta, Tiffany Kao, Elizabeth Wei, and Eduardo\nBlanco. 2020. An analysis of natural language infer-\nence benchmarks through the lens of negation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9106–9118, Online. Association for Computa-\ntional Linguistics.\nArian Hosseini, Siva Reddy, Dzmitry Bahdanau, R De-\nvon Hjelm, Alessandro Sordoni, and Aaron Courville.\n2021. Understanding by understanding not: Model-\ning negation in language models. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1301–1312,\nOnline. Association for Computational Linguistics.\nJoel Jang, Seonghyeon Ye, and Minjoon Seo. 2022a.\nCan large language models truly understand prompts?\na case study with negated prompts. arXiv preprint\narXiv:2209.12711.\nMyeongjun Jang and Thomas Lukasiewicz. 2023.\nConsistency analysis of chatgpt. arXiv preprint\narXiv:2303.06273.\nMyeongjun Jang, Frank Mtumbuka, and Thomas\nLukasiewicz. 2022b. Beyond distributional hypoth-\nesis: Let language models learn meaning-text corre-\nspondence. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022 , pages 2030–\n2042, Seattle, United States. Association for Compu-\ntational Linguistics.\nTing Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang,\nDeqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen\nHuang, Denvy Deng, and Qi Zhang. 2022. Prompt-\nBERT: Improving BERT sentence embeddings with\nprompts. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 8826–8837, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\n110\nJaap Jumelet, Milica Denic, Jakub Szymanik, Dieuwke\nHupkes, and Shane Steinert-Threlkeld. 2021. Lan-\nguage models use monotonicity to assess NPI licens-\ning. In Findings of the Association for Computa-\ntional Linguistics: ACL-IJCNLP 2021, pages 4958–\n4969, Online. Association for Computational Lin-\nguistics.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot fly. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. Asso-\nciation for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nVladislav Lialin, Kevin Zhao, Namrata Shivagunde, and\nAnna Rumshisky. 2022. Life after BERT: What do\nother muppets understand about language? In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 3180–3193, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIan McKenzie, Alexander Lyzhov, Alicia Parrish,\nAmeya Prabhu, Aaron Mueller, Najoung Kim, Sam\nBowman, and Ethan Perez. 2022. Inverse scaling\nprize: Round 1 winners.\nGeorge A Miller. 1998. WordNet: An electronic lexical\ndatabase. MIT press.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nAbhilasha Ravichander, Matt Gardner, and Ana Maraso-\nvi´c. 2022. CONDAQA: A contrastive reading com-\nprehension dataset for reasoning about negation.\narXiv preprint arXiv:2211.00295.\nJoshua Robinson, Christopher Michael Rytting, and\nDavid Wingate. 2022. Leveraging large language\nmodels for multiple choice question answering.\narXiv preprint arXiv:2210.12353.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptNet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-first AAAI conference on\nartificial intelligence.\nIeva Stali¯unait˙e and Ignacio Iacobacci. 2020. Compo-\nsitional and lexical semantics in RoBERTa, BERT\nand DistilBERT: A case study on CoQA. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n7046–7056, Online. Association for Computational\nLinguistics.\nThinh Truong, Timothy Baldwin, Trevor Cohn, and\nKarin Verspoor. 2022a. Improving negation detection\nwith negation-focused pre-training. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4188–4193,\nSeattle, United States. Association for Computational\nLinguistics.\nThinh Hung Truong, Yulia Otmakhova, Timothy Bald-\nwin, Trevor Cohn, Jey Han Lau, and Karin Verspoor.\n2022b. Not another negation benchmark: The NaN-\nNLI test suite for sub-clausal negation. In Proceed-\nings of the 2nd Conference of the Asia-Pacific Chap-\nter of the Association for Computational Linguistics\nand the 12th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 883–894, Online only. Association for Compu-\ntational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. SuperGLUE: A stick-\nier benchmark for general-purpose language under-\nstanding systems. CoRR, abs/1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nHao Wang, Yangguang Li, Zhen Huang, Yong Dou,\nLingpeng Kong, and Jing Shao. 2022. Sncse: Con-\ntrastive learning for unsupervised sentence embed-\nding with soft negative samples.\n111\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019. Investigating\nBERT’s knowledge of language: Five analysis meth-\nods with NPIs. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2877–2887, Hong Kong, China. Association\nfor Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2021. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\nTransactions on Machine Learning Research.\nJason Wei, Yi Tay, and Quoc V Le. 2022b. In-\nverse scaling can become U-shaped. arXiv preprint\narXiv:2211.02011.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng\nQian, and Roger P. Levy. 2020. On the predictive\npower of neural language models for human real-\ntime comprehension behavior. In Proceedings of\nthe 42nd Annual Meeting of the Cognitive Science\nSociety, page 1707–1713.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOPT: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nA Model checkpoints\nFor open-sourced LMs, we consider the official\nreleased checkpoints on the HuggingFace hub at:\n• https://huggingface.co/EleutherAI/x\n• https://huggingface.co/facebook/y\nwhere x in {gpt-neo-125M,gpt-neo-1.3B,gpt-neo-\n2.7B,gpt-j-6B}, and y in {opt-125m,opt-350m,opt-\n1.3b,opt-2.7b,opt-6.7b}.\nFigure 7: Zero-shot performance of OPT on MWR\nusing different prompts\nFigure 8: Zero-shot performance of OPT on SAR using\ndifferent prompts\nFor GPT-3 models, we access them through the\nofficial API at https://openai.com/api/,\nusing the Text completionendpoint. The considered\nmodel identifiers along with their sizes are:\n• text-ada-001: 350M\n• text-babbage-001: 1.3B\n• text-curie-001: 6.7B\n• text-davinci-001: 175B\n• text-davinci-003: 175B\nB OPT results\nFor MWR, although we observe improvements\nwith increasing model sizes, the WHR scores are\nmuch higher than those of GPT-neo, showing that\nOPT is worse at predicting antonyms and syn-\nonyms of words. The gap in performance may\nlie in differences in training data between the two\ntypes of models.\nC Model outputs\n112\nFigure 9: Zero-shot performance of OPT on MKR using different prompts\nFigure 10: Zero-shot performance of OPT on NLI tasks using different prompts\n113\nPrompt Model Output\nParacetamol isn’t a kind of GPT-neo-125M muscle\nGPT-J-6B painkiller\nOPT-125M pain\nOPT-6.7B medicine\nGPT-3 medication\nInstructGPT NSAID\nEntrance is an antonym of GPT-neo-125M interest\nGPT-J-6B entrance\nOPT-125M entrance\nOPT-6.7B exit\nGPT-3 departure\nInstructGPT entrance\nChoose the correct answer: flimsy and sturdy are synonyms or\nantonyms?\nGPT-neo-125M Synonyms\nGPT-J-6B Synonyms\nOPT-125M Antonyms\nOPT-6.7B Synonyms\nGPT-3 Antonyms\nInstructGPT Antonyms\nI can not think of a few reasons for the allergy to substance. Question:\nThere are not reasons why there’s an allergy. True, False, or Neither?\nAnswer:\nGPT-neo-125M True\nGPT-J-6B True\nOPT-125M True\nOPT-6.7B Neither\nGPT-3 False\nInstructGPT Neither\nThe man does not own a dog. Question: the man does not own a\nmammal. True or Not true? Answer:\nGPT-neo-125M True\nGPT-J-6B True\nOPT-125M True\nOPT-6.7B True\nGPT-3 True\nInstructGPT Not True\nTable 6: Example outputs of models. Wrong answers are highlighted\n114",
  "topic": "Negation",
  "concepts": [
    {
      "name": "Negation",
      "score": 0.9548620581626892
    },
    {
      "name": "Bottleneck",
      "score": 0.6292874813079834
    },
    {
      "name": "Computer science",
      "score": 0.5857003927230835
    },
    {
      "name": "Linguistics",
      "score": 0.4429793357849121
    },
    {
      "name": "Natural language processing",
      "score": 0.4198063611984253
    },
    {
      "name": "Artificial intelligence",
      "score": 0.340483695268631
    },
    {
      "name": "Programming language",
      "score": 0.19334140419960022
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}