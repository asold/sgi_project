{
  "title": "Language Model Analysis for Ontology Subsumption Inference",
  "url": "https://openalex.org/W4385572570",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2102192046",
      "name": "Yuan He",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2120067049",
      "name": "Jiaoyan Chen",
      "affiliations": [
        "University of Manchester"
      ]
    },
    {
      "id": "https://openalex.org/A2729053939",
      "name": "Ernesto JimÃ©nez-Ruiz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1974730243",
      "name": "Hang Dong",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2010142017",
      "name": "Ian Horrocks",
      "affiliations": [
        "University of Oxford"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2150617113",
    "https://openalex.org/W4205537036",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2903298154",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W3099864716",
    "https://openalex.org/W2128608507",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4281690218",
    "https://openalex.org/W4367670019",
    "https://openalex.org/W1511252057",
    "https://openalex.org/W4283819412",
    "https://openalex.org/W2250770256",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W4223974161",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2139315672",
    "https://openalex.org/W4281262985",
    "https://openalex.org/W3216317364",
    "https://openalex.org/W3093526682",
    "https://openalex.org/W2159092541",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W3201663597",
    "https://openalex.org/W2103017472",
    "https://openalex.org/W3210129272",
    "https://openalex.org/W1577783107",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2507211235"
  ],
  "abstract": "Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 3439â€“3453\nJuly 9-14, 2023 Â©2023 Association for Computational Linguistics\nLanguage Model Analysis for Ontology Subsumption Inference\nYuan He1, Jiaoyan Chen2, Ernesto JimÃ©nez-Ruiz3,4,\nHang Dong1, Ian Horrocks1\n1 University of Oxford, 2 The University of Manchester,\n3 City, University of London, 4 University of Oslo\n{yuan.he,hang.dong,ian.horrocks}@cs.ox.ac.uk\njiaoyan.chen@manchester.ac.uk\nernesto.jimenez-ruiz@city.ac.uk\nAbstract\nInvestigating whether pre-trained language\nmodels (LMs) can function as knowledge\nbases (KBs) has raised wide research inter-\nests recently. However, existing works focus\non simple, triple-based, relational KBs, but\nomit more sophisticated, logic-based, concep-\ntualised KBs such as OWL ontologies. To\ninvestigate an LMâ€™s knowledge of ontologies,\nwe propose O NTO LAMA, a set of inference-\nbased probing tasks and datasets from on-\ntology subsumption axioms involving both\natomic and complex concepts 1. We conduct\nextensive experiments on ontologies of differ-\nent domains and scales, and our results demon-\nstrate that LMs encode relatively less back-\nground knowledge of Subsumption Inference\n(SI) than traditional Natural Language Infer-\nence (NLI) but can improve on SI signiï¬cantly\nwhen a small number of samples are given. We\nwill open-source our code and datasets.2\n1 Introduction\nThe advancements of large pre-trained language\nmodels (LMs) have sparked research interests in\ninvestigating how much explicit semantics LMs\ncan learn or infer from knowledge bases (KBs)\n(AlKhamissi et al., 2022). The LAMA (LAnguage\nModel Analysis) probe (Petroni et al., 2019) is\namong the ï¬rst works that adopt prompt-based\nmethods to simulate the process of querying re-\nlational knowledge from various KBs such as Con-\nceptNet (Speer and Havasi, 2012) and GoogleRE3.\nSome subsequent studies focus on probing speciï¬c\ntypes of knowledge from sources like common-\nsense KBs (Da et al., 2021), biomedical KBs (Sung\n1An ontology concept is also known as a class. To avoid\nconfusion with class in machine learning classiï¬cation, we\nstick to use the term concept.\n2Code and Instructions: https://krr-oxford.github.\nio/DeepOnto/ontolama; Dataset at HuggingFace: https://\nhuggingface.co/datasets/krr-oxford/OntoLAMA/ or at\nZenodo: https://doi.org/10.5281/zenodo.6480540\n3https://code.google.com/archive/p/\nrelation-extraction-corpus/\nentailed subsumption\nassumed disjointness\nPositiveNegativeğ¶â‰”ğµğ‘’ğ‘’ğ‘“ğ·â‰”ğ‘€ğ‘’ğ‘ğ‘¡âŠ“âˆƒğ‘‘ğ‘’ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘ ğ¹ğ‘Ÿğ‘œğ‘š.ğ¶ğ‘ğ‘¡ğ‘¡ğ‘™ğ‘’\nVerbaliserğ’±\nğ’±ğ¶âˆ¶=â€œbeefâ€ğ’±ğ·âˆ¶=\tâ€œmeat that derives from cattleâ€\nOntology ğ’ª\nLanguage Model\nTemplate(ğ’±ğ¶,ğ’±ğ·\t,\t<MASK>)\n EntailmentNot Entailment\nExtract Samples\nFigure 1: O NTO LAMA framework.\net al., 2021), temporal KBs (Dhingra et al., 2022),\nand cross-lingual KBs (Liu et al., 2021a).\nHowever, existing â€œLMs-as-KBsâ€ works focus\non simple, triple-based, relational KBs, but ne-\nglect more formalised, logic-based, conceptualised\nKBs. For example, a statement like â€œLondon is\nthe capital of the UKâ€ can be expressed in the\ntriple (London, capitalOf, UK); but a sentence\nlike â€œarthritis is a kind of arthropathy with an in-\nï¬‚ammatory morphologyâ€, which describes the con-\ncept â€œarthritisâ€, cannot be easily expressed using\njust triples. Conceptual knowledge like this re-\nquires a formal and expressive representation to be\ndeï¬ned precisely. A well-known model for con-\nceptual knowledge is the OWL4 ontology (Bech-\nhofer et al., 2004; Grau et al., 2008), which can\nbe seen as a description logic (DL) KB with rich\nbuilt-in vocabularies for knowledge representation\nand various reasoning tools supported. Taking\nthe example of â€œarthritisâ€, in DL the concept\ncan be described as Arthritis âŠ‘Arthropathy âŠ“\nâˆƒhasMorphology.Inflammatory .\nIn this work, we take a further step along the\nâ€œLMs-as-KBsâ€ research line towards more for-\nmalised semantics by targeting DL KBs and in par-\nticular the OWL ontologies. Current works on LMs\nconcerning ontologies are mostly driven by a tar-\nget application. Liu et al. (2020), He et al. (2022),\n4For simplicity, we refer to the second edition OWL 2 as\nOWL: https://www.w3.org/TR/owl2-overview/\n3439\nand Chen et al. (2022) apply language model ï¬ne-\ntuning to address ontology curation tasks such as\nconcept insertion and matching, while Ye et al.\n(2022) transform ontologies into graphs for data\naugmentation in few-shot learning. In contrast to\nthese application-driven approaches, we investigate\na more fundamental question: To what extent can\nLMs infer conceptual knowledge modelled by an\nontology? Particularly, we focus on the subsump-\ntion relationships between ontology concepts. As\nshown in Figure 1, we ï¬rst extract concept pairs\n(C, D) that are deemed as positive (C and D are in\na subsumption relationship) and negative (C and\nD are assumed to be disjoint) samples from an on-\ntology. Note that the sampling procedure is fully\nautomatic with the syntax and semantics of OWL\nontology carefully considered. To translate the con-\ncepts and especially the ones with complex logical\nexpressions into natural language texts, we develop\na recursive concept verbaliser. We formulate the\nSubsumption Inference (SI) task similarly to the\nNatural Language Inference (NLI) task and treat\nthe concept pairs as premise-hypothesis pairs (PadÃ³\nand Dagan, 2022), which will then be wrapped into\na template for generating inputs of LMs.\nWe have created SI datasets from ontologies of\nvarious domains and scales, and conducted exten-\nsive experiments. Our results demonstrate that LMs\nperform better on a typical NLI task than the con-\nstructed SI tasks under the zero-shot setting, indi-\ncating that LMs encode relatively less background\nknowledge of ontology subsumptions. However,\nby providing a small number of samples (K-shot\nsettings), the performance on SI is signiï¬cantly\nimproved. This observation is consistent with the\nthree LMs that are studied in this work.\n2 Background\n2.1 OWL Ontology\nAn OWL ontology is a description logic (DL)\nknowledge base that consists of the TBox (termi-\nnological), ABox (assertional), and RBox (rela-\ntional) axioms (KrÃ¶tzsch et al., 2012). In this work,\nwe focus on the TBox axioms which specify the\nsubsumption relationships between concepts of a\ndomain. A subsumption axiom has the form of\nC âŠ‘D where C and D are concept expressions in-\nvolving atomic concept, negation (Â¬), conjunction\n(âŠ“), disjunction (âŠ”), existential restriction (âˆƒr.C),\nuniversal restriction (âˆ€r.C), and so on (see com-\nplete deï¬nition in Appendix A). An atomic con-\ncept is a named concept, a top conceptâŠ¤(a concept\nwith every individual as an instance), or a bottom\nconcept âŠ¥(an empty concept); while a complex\nconcept consists of at least one of the available\nlogical operators. An equivalence axiom C â‰¡D is\nequivalent to C âŠ‘D and D âŠ‘C.\nRegarding the semantics, in DL we deï¬ne an\ninterpretation I= (âˆ†I, Â·I) that consists of an non-\nempty set âˆ†I and a function Â·I that maps each\nconcept C to CI âŠ†âˆ†I and each property r to\nrIâŠ†âˆ†IÃ—âˆ†I. We say Iis a model of C âŠ‘D if\nCIâŠ†DIholds, and Iis a model of an ontology\nOif Iis a model of all axioms in O. If CIâŠ†DI\nholds for every model Iof O, then we can say\nO|= C âŠ‘D. This deï¬nes logical entailment w.r.t.\nan ontology and it is more strictly deï¬ned than\ntextual entailment based on human beliefs.\nAn individual a is an instance of a concept C in\nOif O|= C(a) (aIâˆˆCIfor every modelIof O).\nC and D are disjoint in Oif O |= C âŠ“D âŠ‘âŠ¥ (or\nequivalently O |= C âŠ‘Â¬D) which means there\ncan be no common instance a of C and D.\nThe Open World Assumption (OW A) underpins\nOWL ontologies, according to which we cannot\nsay what is not entailed by the ontology is nec-\nessarily false. For example, if we have an on-\ntology that contains just one axiom Paella âŠ‘\nâˆƒhasIngredient.Chicken , in OWA we cannot\ndetermine if paella can have chorizo as an ingredi-\nent or not. To allow reuse and extension, ontologies\nare often (intentionally) underspeciï¬ed (Cimiano\nand Reyle, 2003); this characteristic motivates how\nwe deï¬ne the negative samples in Section 3.1.\n2.2 Related Work\nRecently, the rise of the prompt learning paradigm\nhas shed light on better usage of pre-trained LMs\nwithout, or with minor, supervision (Liu et al.,\n2022). However, LMs are typically pre-trained\nin a stochastic manner, making it challenging to\nstudy what knowledge LMs have implicitly en-\ncoded (Petroni et al., 2019) and how to access LMs\nin an optimal or controllable way (Gao et al., 2021;\nLi et al., 2022).\nOur work is informed by the â€œLMs-as-KBsâ€ lit-\nerature (AlKhamissi et al., 2022), where different\nprobes have been designed to test LMsâ€™ knowl-\nedge of relational data. In Petroni et al. (2019), the\nprobing task of world knowledge has been formu-\nlated as a cloze-style answering task where LMs\nare required to ï¬ll in the <MASK> token given in-\n3440\nput texts wrapped into a manually designed tem-\nplate. Sung et al. (2021) did a similar work but\nshift the focus to (biomedical) domain knowledge\nof domain-speciï¬c LMs. Liu et al. (2021a) pre-\ntrained LMs with multi-lingual knowledge graphs\n(KGs) and test on the cross-lingual tasks. Dhin-\ngra et al. (2022) proposed datasets with temporal\nsignals and probed LMs on them with templates\ngenerated by the text-to-text transformer T5 (Raffel\net al., 2022).\nHowever, existing â€œLMs-as-KBsâ€ works mostly\nfocus on relational facts, but omit logical seman-\ntics and conceptual knowledge. In contrast, our\nwork focuses on OWL ontologies which represent\nconceptual knowledge with an underlying logical\nformalism. Although there are some recent works\nconcerning both LMs and ontologies, they do not\ncompare them at the semantic level but rather em-\nphasise on downstream applications. For exam-\nple, He et al. (2022) adopted LMs as synonym\nclassiï¬ers to predict mappings between ontologies;\nwhereas Ye et al. (2022) used ontologies to provide\nextra contexts to help LMs to make predictions.\n3 Subsumption Inference\n3.1 Task Deï¬nition\nRecall the deï¬nitions in Section 2.1, a subsump-\ntion axiom C âŠ‘D can be interpreted as: â€œevery\ninstance of C is an instance of Dâ€. We can ac-\ncordingly form a premise-hypothesis pair where\nthe premise is â€œx is a Câ€ and the hypothesis is\nâ€œx is a Dâ€ for some individual x. Note that there\nare different ways to express the premise and hy-\npothesis, and we adopt a simple but effective one\n(see Section 5.1). Next, an ontology verbaliser is\nrequired for transforming the concept expressions\nC and D into natural language texts. Analogous to\nNatural Language Inference (NLI) or Recognising\nTextual Entailment (RTE) (Poliak, 2020; PadÃ³ and\nDagan, 2022), the task of Subsumption Inference\n(SI) is thus deï¬ned as classifying if the premise\nentails or does not entail the hypothesis. Note that\nSI is similar to a two-way RTE task5 where we do\nnot consider the neutral6 class.\nGiven an ontology O, we extract positive and\nnegative subsumptions to probe LMs. The positive\n5RTE guidelines: https://tac.nist.gov/2008/rte/\nrte.08.guidelines.html.\n6Neutral essentially means two terms are unrelated. On-\ntologies are invariably underspeciï¬ed, so even if two concepts\nhave not been entailed as a subsumption or non-subsumption,\nthey may still be implicitly related in the real world.\nsamples are concept pairs (C, D) with O|= C âŠ‘\nD. Due to OWA, we cannot determine if (C, D)\nwith O Ì¸|= C âŠ‘D really forms a negative sub-\nsumption (see Appendix F for more explanation);\nto generate plausible negative samples, we propose\nthe assumed disjointness7 deï¬ned as follows:\nDeï¬nition (Assumed Disjointness). If two con-\ncepts C and D are satisï¬able in Oâˆª{C âŠ“D âŠ‘âŠ¥}\nand there is no named atomic concept A in Osuch\nthat O|= A âŠ‘C and O|= A âŠ‘D, then C and\nD are assumed to be disjoint.\nThe ï¬rst condition ensures that C and D are still\nsatisï¬able after adding the disjointness axiom for\nthem into Owhereas the second condition ensures\nthat C and D have no common descendants be-\ncause otherwise the disjointness axiom will make\nany common descendant unsatisï¬able. If two con-\ncepts C and D satisfy these two conditions, we\ntreat (C, D) as a valid negative subsumption.\nHowever, in practice validating the satisï¬ability\nfor each concept pair (C, D) would be inefï¬cient\nespecially when the ontology is large and complex.\nThus, we propose a pragmatical alternative to the\nsatisï¬ability check in Appendix E.\nTo conduct reasoning to extract entailed posi-\ntive subsumptions and validate sampled negative\nsubsumptions, we need to adopt a proven sound\nand complete OWL reasoner, e.g., HermiT (Glimm\net al., 2014).\nIn the following sub-sections, we propose two\nspeciï¬c SI tasks and their respective subsumption\nsampling methods.\n3.2 Atomic Subsumption Inference\nThe ï¬rst task aims at subsumption axioms that in-\nvolve just named atomic concepts. Such axioms are\nusually the most prevalent in an ontology and can\nbe easily verbalised by using the concept names.\nIn this work, we use labels (in English) deï¬ned\nby the built-in annotation property rdfs:label as\nconcept names.\nThe positive samples are extracted from all en-\ntailed subsumption axioms of the target ontology.\nWe consider two types of negative samples: (i)\nsoft negative composed of two random concepts,\nand (ii) hard negative composed of two random\nsibling concepts. Two sibling concepts lead to a\nâ€œhardâ€ negative sample because they share a com-\nmon parent (thus having closer semantics) but are\n7Schlobach (2005) and Solimando et al. (2017) deï¬ned a\nsimilar assumption but in different contexts.\n3441\nPattern Verbalisation ( V)\nA (atomic) the name ( rdfs:label) of A\nr (property) the name ( rdfs:label) of r, subject to\nrules in Appendix C\nÂ¬C â€œnot V(C)â€\nâˆƒr.C â€œsomething that V(r) some V(C)â€\nâˆ€r.C â€œsomething that V(r) only V(C)â€\nC1 âŠ“ ... âŠ“ Cn if Ci = âˆƒ/âˆ€r.Di and Cj = âˆƒ/âˆ€r.Dj,\nthey will be re-written into âˆƒ/âˆ€r.(Di âŠ“\nDj) before verbalisation; suppose after re-\nwriting the new expression isC1âŠ“...âŠ“Cnâ€²\n(a) if all Cis (for i = 1, ..., nâ€²) are restric-\ntions, in the form of âˆƒ/âˆ€ri.Di:\nâ€œsomething that V(r1) some/only V (D1)\nand ... and V(rnâ€² ) some/only V (Dnâ€² )â€\n(b) if some Cis (for i = m + 1, ..., nâ€²)\nare restrictions, in the form of âˆƒ/âˆ€ri.Di:\nâ€œV(C1) and ... and V(Cm) that V(rm+1)\nsome/only V (Dm+1) and ... and V(rnâ€² )\nsome/only V (Dnâ€² )â€\n(c) if no Ci is a restriction:\nâ€œV(C1) and ... and V(Cnâ€² )â€\nC1 âŠ” ... âŠ” Cn similar to verbalising C1 âŠ“...âŠ“Cn except\nthat â€œandâ€ is replaced by â€œorâ€ and case\n(b) uses the same verbalisation as case (c)\nTable 1: Recursive rules for verbalising a complex con-\ncept expression C in OWL ontologies. Note that Ci in\nthe conjunction/disjunction pattern is also an arbitrary\ncomplex concept.\noften disjoint. The sampled pairs need to meet the\nassumed disjointness deï¬ned in Section 3.1 to be\naccepted as valid negatives. We ï¬rst sample equal\nnumbers of soft and hard negatives and then ran-\ndomly truncate the resulting set into the size of the\npositive sample set to keep class balance.\n3.3 Complex Subsumption Inference\nIn the second SI task, we consider subsumption\naxioms that involve complex concepts. Particularly,\nwe choose equivalence axioms of the formA â‰¡C8\n(where A and C are atomic and complex concepts,\nrespectively) as anchors, and equivalently trans-\nform them into subsumption axioms of the forms\nA âŠ‘C and C âŠ‘A, through which complex con-\ncepts can appear on both the premise and hypothe-\nsis sides.\nRecursive Concept Verbaliser To transform a\ncomplex C into a natural language text, we de-\nvelop the recursive concept verbaliser consisting\nof a syntax tree parser and a set of recursive rules\n(see Table 1). A concrete example is shown in\n8Equivalence axioms of this form are referred to as the\ndeï¬nition of the named concept, and are common in OWL.\nğ‘€ğ‘’ğ‘ğ‘¡âˆƒğ‘‘ğ‘’ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘ ğ¹ğ‘Ÿğ‘œğ‘š.ğ¶ğ‘ğ‘¡ğ‘¡ğ‘™ğ‘’âˆ€ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘‚ğ‘“.ğ¶ğ‘œğ‘›ğ‘¡ğ‘–ğ‘›ğ‘¢ğ‘ğ‘›ğ‘¡\nmeat\nmeatthatderivesfromsomecattleandispartofonlycontinuant\nğ‘‘ğ‘’ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘ ğ¹ğ‘Ÿğ‘œğ‘šğ¶ğ‘ğ‘¡ğ‘¡ğ‘™ğ‘’ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘‚ğ‘“ğ¶ğ‘œğ‘›ğ‘¡ğ‘–ğ‘›ğ‘¢ğ‘ğ‘›ğ‘¡âˆ€âˆƒâŠ“\nderives\tfromcattleis\tpart\tofcontinuantderives\tfrom\tsome\tcattleis\tpart\tof\tonly\tcontinuant\nğ‘€ğ‘’ğ‘ğ‘¡âŠ“âˆƒğ‘‘ğ‘’ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘ ğ¹ğ‘Ÿğ‘œğ‘š.ğ¶ğ‘ğ‘¡ğ‘¡ğ‘™ğ‘’âŠ“âˆ€ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘‚ğ‘“.ğ¶ğ‘œğ‘›ğ‘¡ğ‘–ğ‘›ğ‘¢ğ‘ğ‘›ğ‘¡\nsplit into sub-formulasmergeverbalisation\nFigure 2: Illustration of how the recursive concept ver-\nbaliser is applied to an example complex concept ex-\npression. The algorithm ï¬rst splits the complex concept\ninto a sub-formula tree, verbalising the leaf nodes, and\nthen merging the verbalised sub-formulas recursively.\nThe key word associated with the logical operator at\neach merging step is marked in red. See Appendix D\nfor more examples.\nFigure 2, where the complex concept Meat âŠ“\nâˆƒderivesFrom.Cattle âŠ“âˆ€partOf.Continuant\nis ï¬rst split into a sub-formula tree by the syntax\nparser, then verbalised according to the recursive\nrules in Table 1. The leaf nodes are either atomic\nconcepts or properties and they are verbalised by\ntheir names. At each recursive step, verbalised\nchild nodes are merged according to the logical\npattern in their parent node. Note that we enforce\nsome extent of redundancy removal for the conjunc-\ntion (âŠ“) and the disjunction (âŠ”) patterns. Taking the\nexample in Figure 2, the verbalised atomic concept\nâ€œmeatâ€ is placed before â€œthatâ€ as an antecedent,\nand the verbalised conjunction of two restrictions\nis placed after â€œthatâ€ as a relative clause. â€œmeatâ€\ncan be replaced by â€œsomethingâ€ if the concept\nMeat is not involved. Moreover, if two restric-\ntions with the same quantiï¬er and property are con-\nnected by âŠ“or âŠ”, they will be merged into one re-\nstriction. For example, âˆƒderivesFrom.Cattle âŠ“\nâˆƒderivesFrom.Sheep will be transformed into\nâˆƒderivesFrom. (Cattle âŠ“Sheep).\nWe extract equivalence axioms in the form of\nA â‰¡C from the target ontology. Taking each such\naxiom as an anchor, we can obtain positive com-\nplex subsumption axioms of the form Asub âŠ‘C\nor C âŠ‘Asuper where Asub and Asuper are a sub-\nclass and a super-class ofA, respectively. To derive\nchallenging negative samples, we ï¬rst randomly re-\n3442\nSource #Concepts #EquivAxioms #Dataset (Train/Dev/Test)\nSchema.org 894 - Atomic SI: 808/404/2, 830\nDOID 11, 157 - Atomic SI: 90, 500/11, 312/11, 314\nFoodOn 30, 995 2 , 383 Atomic SI: 768, 486/96, 060/96, 062\nComplex SI: 3, 754/1, 850/13, 080\nGO 43, 303 11 , 456 Atomic SI: 772, 870/96, 608/96, 610\nComplex SI: 72, 318/9, 040/9, 040\nMNLI - - biMNLI: 235, 622/26, 180/12, 906\nTable 2: Statistics for ontologies, SI datasets, and the biMNLI dataset.\nplace a named concept or a property in A â‰¡C\nto generate either (i) Aâ€²â‰¡C (if A is replaced by\nAâ€²) or (ii) A â‰¡Câ€²(if C is corrupted). Without\nloss of generality, we assume the random replace-\nment leads to case (ii). We then check if A and\nCâ€²satisfy the assumed disjointness as described\nin Section 3.1. In the afï¬rmative case, we can\nhave either A âŠ‘Câ€²or Câ€² âŠ‘A as the ï¬nal nega-\ntive subsumption; otherwise, we skip this sample.\nFor example, given SunflowerSeed â‰¡Seed âŠ“\nâˆƒDerivesFrom.HelianthusAnnuus , a possi-\nble negative subsumption is SunflowerSeed âŠ‘\nFruit âŠ“ âˆƒDerivesFrom.HelianthusAnnuus\nif Seed in C is replaced by Fruit to create Câ€².\n4 Datasets\nIn this work, we consider ontologies of different\ndomains and scales including:\nâ€¢ Schema.org9 (released on 2022-03-17): a\ngeneral-purpose ontology that maintains a ba-\nsic schema for structured data on the Web;\nâ€¢ DOID10 (released on 2022-09-29): an ontol-\nogy for human diseases (Schriml et al., 2012);\nâ€¢ FoodOn11 (released on 2022-08-12): an on-\ntology specialised in food-related knowledge\nincluding food products, food sources, food\nnutrition, and so on (Dooley et al., 2018).\nâ€¢ GO12 (released on 2022-11-03): a very ï¬ne-\ngrained and widely used biomedical ontology\nspecialised in genes and gene functions (Ash-\nburner et al., 2000).\nWe used the most updated versions at the time of\nexperiment. The details for pre-processing the on-\ntologies are illustrated in Appendix B.\n9https://schema.org/\n10https://disease-ontology.org/\n11https://foodon.org/\n12http://geneontology.org/\nWe construct an Atomic SI dataset for each on-\ntology, but Complex SI datasets are created for\nFoodOn and GO only, due to their abundance of\nequivalence axioms. To avoid too many repetitive\nconcept expressions brought by a particular equiv-\nalence axiom, we sample at most 4 positive and\n4 negative samples for each equivalence axiom in\nthe Complex SI setting. To attain class balance, we\npurposely keep the number of negative samples the\nsame as the positive samples in each data split. For\nmost of the resulting datasets, we divide each into\n8 : 1 : 1 for training, development, and testing;\nfor the Schema.orgâ€™s Atomic SI and theFoodOnâ€™s\nComplex SI datasets, which are relatively smaller,\nwe apply a 2 : 1 : 7 division instead. Note that\nwe mainly focus on K-shot settings in the probing\nstudy, thus the required training and development\nsample sets are small.\nTo compare with how LMs perform on tradi-\ntional NLI, we additionally create biMNLI, a sub-\nset of the Multi-Genre Natural Language Inference\n(MNLI) corpus (Williams et al., 2018) where (i) the\nneutral class and its samples are removed, (ii) the\nMatched and Mismatched testing sets are merged\ninto one testing set, (iii) 10% of the training data is\nused as the development set, and(iv) the entailment-\ncontradiction ratio is set to 1 : 1 (by discarding\nextra samples from the dominant class) for a bal-\nanced prior. The numbers of named concepts and\nequivalence axioms in ontologies, and the numbers\nof samples in (each split of) SI datasets and the\nbiMNLI dataset are reported in Table 2.\n5 Experiments\n5.1 Prompt-based Inference\nTo conduct the inference task under the prompt-\nbased settings, we wrap the verbalised subsump-\ntion axioms and the<MASK> token into a template to\n3443\nserve as inputs of LMs. We opt to use different com-\nbinations of manually designed templates13 (T1 and\nT2) and label words (L1 to L3) that have achieved\npromising results on the NLI tasks (Schick and\nSchÃ¼tze, 2021; Gao et al., 2021) as follows:\nT1 := It is <A> V(C)î´™ î´˜î´— î´š\npremise\n? <MASK>, it is <A> V(D)î´™ î´˜î´— î´š\nhypothesis\n.\nT2 := â€œ It is<A> V(C)î´™ î´˜î´— î´š\npremise\nâ€? <MASK>, â€œ it is<A> V(D)î´™ î´˜î´— î´š\nhypothesis\nâ€.\nL1 := {â€œpositiveâ€: [â€œYesâ€], â€œnegativeâ€: [â€œNoâ€]}\nL2 := {â€œpositiveâ€: [â€œRightâ€], â€œnegativeâ€: [â€œWrongâ€]}\nL3 := {â€œpositiveâ€: [â€œYesâ€, â€œRightâ€],\nâ€œnegativeâ€: [â€œNoâ€, â€œWrongâ€]}\nwhere <A> is â€œaâ€, â€œanâ€, or just blank depending on\nthe next word14, V(Â·) is the concept verbalisation\nfunction deï¬ned in Section 3, and <MASK> is the\ntoken that LMs need to predict. The probability of\npredicting class y (â€œpositiveâ€ or â€œnegativeâ€) for an\ninput sample x = (C, D) is deï¬ned as:\nP(y |x) = P(<MASK> âˆˆLj[y]) |Ti(C, D))\n=\nâˆ‘\nvâˆˆLj[y] exp(wv Â·h<MASK>)\nâˆ‘\nwâˆˆLj[Â·] exp(ww Â·h<MASK>)\nwhere Lj[Â·] and Lj[y] denote all the label words de-\nï¬ned in Lj and the label words of classy deï¬ned in\nLj, respectively; Ti(C, D) denotes the transformed\ntexts of concepts C and D through the template Ti;\nwv and ww are vectors for the label wordsv and w,\nrespectively; and h<MASK> denotes the hidden vector\nof the masked token. The prediction can be trained\nby minimising the cross-entropy loss.\nFor the biMNLI dataset, the premise and hypoth-\nesis are replaced by what were originally given in\nthe dataset â€“ except that we have removed trailing\npunctuations.\nIn the main experiments concerning language\nmodels, we consider all the combinations of Ti and\nLj and additionally consider 3 random seeds (thus\n18 experiments each) for K-shot settings where\nK > 0. The value of K refers to the number of\nsamples per classiï¬cation label (positive or nega-\ntive) we randomly extract from training and devel-\nopment sets, respectively. For K = 0 (zero-shot),\n13We make slight modiï¬cations by adding the preï¬x â€œIt/it\nis <A>â€ to make premise and hypothesis sentences complete.\n14â€œanâ€ is used when the next word starts with a vowel;\nleaving it blank when the next word is â€œsomethingâ€.\ndifferent random seeds do not affect the results.\nFor the fully supervised setting, we consider only\none random seed and one combination ( T1 and\nL1) because our pilot experiments demonstrate that\nï¬ne-tuning on large samples results in low variance\nbrought by different random seeds and different\ncombinations of templates and label words.\nOur code implementations mainly rely on\nThe OWL API15 for ontology processing and reason-\ning, and OpenPrompt16 for prompt learning (Ding\net al., 2022). Training of each K-shot (where\nK > 0) experiment takes 10 epochs, while for\nthe fully supervised setting involving very large\ntraining samples, we only train for 1 epoch.17 The\nbest-performing model on the development set (at\neach epoch) is selected for testing set inference. We\nuse the AdamW optimiser (Loshchilov and Hutter,\n2019) with the initial learning rate, weight decay,\nand the number of warm-up steps set to10âˆ’5, 10âˆ’2,\nand 50, respectively. All our experiments are con-\nducted on two Quadro RTX 8000 GPUs.\n5.2 Results and Analysis\nLMs and Settings We choose LMs from the\nRoBERTa family (Liu et al., 2019) as they are\nfrequently introduced in cloze-style probing tasks\n(Liu et al., 2021b; Sung et al., 2021; Kavumba\net al., 2022). In Table 3, we present key experi-\nment results for roberta-large and roberta-base;\nwe have a further ablation study for a biomedical\nvariant of roberta-large in the latter paragraph.\nFor both LMs in Table 3, we report results of\nK-shot settings with K âˆˆ{0, 4, 32, 128}. We ad-\nditionally present the results of the fully supervised\nsetting for roberta-large as the oracle. For each\nsetting, we report the averaged accuracy and stan-\ndard deviation (where applicable). To clearly ob-\nserve how the performance varies as K increases,\nwe present Figure 3 which visualises the K-shot\nresults for roberta-large with additional values\nof K ({8, 16, 64}). The complete result table for\nboth language models and the ï¬gure that visualises\nthe performance of roberta-base are available in\nAppendix G.\nBaselines As aforementioned, we purposely keep\nclass balance in each data split, thus the accuracy\nscores for majority voteare all 50.0%. Besides,\n15https://owlapi.sourceforge.net/\n16https://thunlp.github.io/OpenPrompt/\n17Since Schema.orgâ€™s Atomic SI and FoodOnâ€™s Complex\nSI datasets have a small training set, their fully supervised\nsettings still take 10 epochs.\n3444\nAtomic SI Complex SI\nSetting biMNLI Schema.org DOID FoodOn GO FoodOn GO\nmajority 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nword2vec\nK=4 51.5 (0.2) 54.9 (2.9) 64.6 (2.6) 63.5 (1.0) 60.1 (4.1) 56.8 (4.2) 56.9 (5.4)\nK=128 52.1 (0.4) 73.0 (0.4) 70.8 (1.7) 71.4 (1.0) 66.3 (0.9) 63.8 (0.6) 66.4 (1.3)\nroberta-base\nK=0 62.5 (6.5) 56.4 (3.6) 53.3 (4.0) 54.6 (4.4) 49.0 (2.4) 55.9 (3.6) 48.7 (3.1)\nK=4 67.6 (5.2) 62.9 (5.2) 61.8 (6.7) 62.1 (4.2) 65.2 (5.0) 62.4 (3.2) 52.2 (7.1)\nK=32 78.8 (1.1) 84.3 (2.0) 89.0 (1.4) 85.0 (1.1) 84.6 (2.5) 77.0 (1.5) 76.4 (2.5)\nK=128 85.1 (1.0) 91.1 (0.7) 92.4 (0.7) 90.0 (0.7) 89.0 (0.8) 85.5 (1.3) 86.9 (1.5)\nroberta-large\nK=0 68.7 (6.2) 61.7 (7.2) 59.8 (5.4) 60.1 (8.8) 54.6 (1.9) 56.1 (1.9) 50.4 (0.6)\nK=4 78.1 (6.6) 69.4 (5.4) 74.0 (5.5) 71.6 (4.4) 67.6 (3.4) 64.1 (5.1) 56.9 (5.7)\nK=32 89.9 (1.2) 87.3 (1.9) 92.3 (0.7) 88.9 (1.6) 87.7 (1.6) 80.8 (3.8) 81.6 (2.2)\nK=128 93.0 (0.8) 92.9 (0.8) 93.4 (0.5) 92.2 (0.5) 91.0 (0.7) 88.4 (1.1) 90.2 (1.0)\nfull 97.5 95.4 97.8 98.7 98.1 95.8 98.8\nTable 3: Results for the biMNLI, Atomic SI, and Complex SI tasks with each cell statingâ€œmean accuracy (standard\ndeviation)â€ except for majority voteand the fully supervised settings where standard deviation is not available.\n0 4 8 16 32 64 128\nK (samples per classification label)\n10\n30\n50\n70\n90Accuracy (%)\nbiMNLI DOID Schema.org FoodOn GO FoodOn (Comp) Go (Comp)\nFigure 3: Visualisation of K-shot results (for roberta-large) on the biMNLI, Atomic SI, and Complex SI tasks,\nwhere the dotted horizontal line indicates majority vote. The order of the bars is the same as in the legend.\nwe consider word2vec (Mikolov et al., 2013) pre-\ntrained on GoogleNews 18 with a logistic regres-\nsion classifer as a baseline model, which demon-\nstrates how a classic non-contextual word embed-\nding model performs on the SI tasks. For this base-\nline, we only report results for K âˆˆ{4, 128}as\nthe increase of K does not bring signiï¬cant change\nand results of K = 128 are roughly comparable to\nresults of K = 4 for roberta-large. This suggests\nthat the SI sample patterns are not easily captured\nwith word2vec.\nSI vs biMNLI From the results, we ï¬rst observe\nthat both roberta-large and roberta-base achieve\nbetter zero-shot results on biMNLI than on all the\nSI datasets by at least 7.0% and 6.1% respectively,\nshowing that under our prompt settings, both LMs\nencode better background knowledge on biMNLI\n18https://code.google.com/archive/p/word2vec/\nthan SI. However, as K grows, the performances\non both biMNLI and SI improve consistently and\nsigniï¬cantly (while the standard deviation gener-\nally reduces), and we can see at K = 32, the mean\naccuracy scores on the Atomic SI tasks have sur-\npassed biMNLI for roberta-base. At K = 64 (see\nFigure 3), the mean accuracy scores on biMNLI\nand all the Atomic SI tasks converge to around\n90.0%; the scores on the two Complex SI tasks\nare also above 80.0% for both LMs. Moreover,\nroberta-large consistently attains a better score\nthan roberta-base for every setting.\nComparison Among SI Tasks We observe that\nComplex SI is generally harder than Atomic SI. For\nexample, at K = 0, roberta-large attains 50.4%\nalmost as majority voteon the Complex SI dataset\nof GO; at K = 128, roberta-large attains 88.4% on\nthe Complex SI dataset of FoodOn while it attains\nmore than 90% for the others. We can also observe\n3445\nK DOID GO GO (Comp)\n0 49.7 (0.4) 50.1 (0.2) 50.0 (0.0)\n4 64.8 (7.9) 66.2 (6.5) 50.0 (0.7)\n32 94.7 (1.3) 93.5 (1.1) 73.5 (3.6)\n128 96.3 (0.4) 95.2 (0.5) 90.5 (1.8)\nTable 4: Results for roberta-large-pm-m3-voc on SI\ntasks of biomedical ontologies DOID and GO.\nfrom Figure 3 that the scores on Complex SI tasks\nare generally lower than those on the Atomic SI\ntasks. Among the Atomic SI tasks, we ï¬nd that\nGO is the most challenging which is as expected\nbecause GO is a ï¬ne-grained expert-level ontology.\nHowever, it surprises us that at K = 32 the score\n(92.3%) on DOID is better than all other tasks, con-\nsidering that DOID is a domain-speciï¬c ontology.\nDomain-speciï¬c SI We conduct further experi-\nments for domain-speciï¬c LMs on domain-speciï¬c\nSI tasks. Speciï¬cally, we consider the vari-\nant roberta-large-pm-m3-voc which has been\npre-trained on biomedical corpora PubMed ab-\nstracts, PMC full-text, and MIMIC-III clinical\nnotes with an updated sub-word vocabulary learnt\nfrom PubMed (Lewis et al., 2020). In Table 4, we\npresent the K-shot results of roberta-large-pm-\nm3-voc on three SI tasks related to biomedical\nontologies DOID and GO. The zero-shot scores are\nalmost equivalent to majority votebut the perfor-\nmance improves more prominently than roberta-\nlarge on the Atomic SI tasks of DOID and GO as K\nincreases. Surprisingly, the Complex SI setting of\nGO seems to be quite challenging to this biomedical\nvariant of RoBERTa. For example, at K = 4, the\nscore is not improved compared to K = 0.\nTemplate and Label Words The access to LMs\nis an inï¬‚uential factor of performance especially\nwhen there are no or fewer training samples. For ex-\nample, roberta-large attains a standard deviation\nof 8.8% for K = 0 on FoodOnâ€™s Atomic SI task,\nsuggesting that there is a signiï¬cant performance\nï¬‚uctuation brought by different combinations of\ntemplates and label words. Although the standard\ndeviation on GOâ€™s Complex SI is just0.6%, the cor-\nresponding accuracy score (50.4%) indicates that\nnone of these combinations work. Furthermore,\neffective template or label words are not transfer-\nable from one LM to another, as we can observe\nfrom the bad performance of roberta-large-pm-\nm3-voc for K = 0 on the SI tasks of biomedical\nontologies. These observations suggest that either\nwe did not ï¬nd a generalised template and label\nwords combination, or LMs require customised\naccess for different types of knowledge.\n6 Conclusion and Discussion\nAs a work that introduces ontologies to the â€œLMs-\nas-KBsâ€ collection, this paper emphasises on how\nto establish a meaningful adaptation from logical\nexpressions to natural language expressions, fol-\nlowing their formal semantics. To this end, we\nleverage the Natural Language Inference (NLI) set-\nting to deï¬ne the Subsumption Inference (SI) task\nwith careful considerations to address the differ-\nences between textual entailment and logical en-\ntailment. We also develop the recursive concept\nverbaliser for OWL ontologies as an auxiliary tool.\nOur results demonstrate that with our SI set-ups,\nLMs can successfully learn to infer both atomic\nand complex subsumptions when a small number\nof annotated samples are provided. This paves\nthe way for investigating more complex reasoning\ntasks with LMs or guiding LMs using ontology\nsemantics with limited training.\nIn fact, the current SI setting is not the only way\nfor probing subsumption knowledge of an ontol-\nogy; for example we can directly verbalise C âŠ‘D\nas â€œV(C) is a kind of V(D)â€ and formulate the\nprobing task similar to fact-checking or equiva-\nlently, an inference task with empty premises. How-\never, our pilot experiments demonstrate that such\nsetting is not as effective as the current SI setting.\nThe presented work brings opportunities for fu-\nture work as (i) the proposed ontology verbalisa-\ntion method has not covered all possible patterns\nof complex concepts (e.g., with cardinality restric-\ntions and nominals); (ii) we have not fully consid-\nered textual information such as synonyms, deï¬ni-\ntions, and comments, that are potentially available\nin an ontology; (iii) we have considered only TBox\n(terminological) axioms, but ABox (assertional)\naxioms can be involved in, e.g., the membership\nprediction task, where the objective is to classify\nwhich concept an individual belongs to. Therefore,\ndeveloping a robust tool for verbalising logical ex-\npressions and extending the ontology inference set-\ntings are potential next tasks. Another interesting\nline for the near future is to train an LM using on-\ntologies with their logical semantics considered.\nThe resulting LM is expected to be applicable to\ndifferent downstream ontology curation tasks such\nas ontology matching and entity linking, with fewer\nsamples necessary for ï¬ne-tuning.\n3446\nLimitations\nAs we mainly focus on conceptual knowledge cap-\ntured in so-called TBox (terminological) axioms,\nthe ABox (assertional) axioms are not considered.\nABox axioms can capture situations for speciï¬c\nindividuals (e.g., health status of a person) which\ncould cause privacy issue and we would not expect\nLMs to capture such knowledge. Hence, dealing\nwith ABox axioms could require additional engi-\nneering for data preprocessing.\nEthical Considerations\nIn this work, we construct new datasets for the\nproposed Subsumption Inference (SI) task from\npublicly available ontologies: Schema.org, DOID,\nFoodOn, and GO, with their download links spec-\niï¬ed in Section 4. The biMNLI dataset is con-\nstructed from the existing open-source MNLI\ndataset. We have conï¬rmed that there is no pri-\nvacy or license issue in all these datasets.\nAcknowledgements\nThis work was supported by Samsung Research\nUK (SRUK), and the EPSRC projects OASIS\n(EP/S032347/1), UK FIRES (EP/S019111/1) and\nConCur (EP/V050869/1).\nReferences\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz,\nMona T. Diab, and Marjan Ghazvininejad. 2022. A\nReview on Language Models as Knowledge Bases.\nArXiv, abs/2204.06031.\nMichael Ashburner, Catherine A. Ball, Judith A. Blake,\nDavid Botstein, Heather L. Butler, J. Michael\nCherry, Allan Peter Davis, Kara Dolinski, Selina S.\nDwight, Janan T. Eppig, Midori A. Harris, David P.\nHill, Laurie Issel-Tarver, Andrew Kasarskis,\nSuzanna E. Lewis, John C. Matese, Joel E. Richard-\nson, Martin Ringwald, Gerald M. Rubin, and Gavin\nSherlock. 2000. Gene Ontology: tool for the\nuniï¬cation of biology. Nature Genetics, 25:25â€“29.\nSean Bechhofer, Frank Van Harmelen, Jim Hendler,\nIan Horrocks, Deborah L McGuinness, Peter F Patel-\nSchneider, Lynn Andrea Stein, et al. 2004. OWL\nweb ontology language reference. W3C recommen-\ndation.\nJiaoyan Chen, Yuan He, Ernesto JimÃ©nez-Ruiz, Hang\nDong, and Ian Horrocks. 2022. Contextual Seman-\ntic Embeddings for Ontology Subsumption Predic-\ntion. ArXiv, abs/2202.09791.\nPhilipp Cimiano and Uwe Reyle. 2003. Ontology-\nbased semantic construction underspeciï¬cation and\ndisambiguation. In Proceedings of the Lor-\nraine/Saarland Workshop on Prospects and Recent\nAdvances in the Syntax-Semantics Interface, Octo-\nber 20-21, 2003, Nancy, France, pages 33â€“38.\nJeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and\nAntoine Bosselut. 2021. Analyzing Commonsense\nEmergence in Few-shot Knowledge Models. InCon-\nference on Automated Knowledge Base Construc-\ntion.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-aware language\nmodels as temporal knowledge bases. Transactions\nof the Association for Computational Linguistics ,\n10:257â€“273.\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\nZhiyuan Liu, Haitao Zheng, and Maosong Sun.\n2022. OpenPrompt: An open-source framework for\nprompt-learning. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations , pages 105â€“113,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nDamion M. Dooley, Emma J. Grifï¬ths, Gurinder\nPal Singh Gosal, Pier Luigi Buttigieg, R. Hoehn-\ndorf, Matthew Lange, Lynn M. Schriml, Fiona\nS. L. Brinkman, and William W. L. Hsiao. 2018.\nFoodOn: a harmonized food ontology to increase\nglobal food traceability, quality control and data in-\ntegration. NPJ Science of Food, 2.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 3816â€“3830, Online. Association for Computa-\ntional Linguistics.\nBirte Glimm, Ian Horrocks, Boris Motik, Giorgos Stoi-\nlos, and Zhe Wang. 2014. HermiT: an OWL 2 rea-\nsoner. Journal of Automated Reasoning, 53(3):245â€“\n269.\nBernardo Cuenca Grau, Ian Horrocks, Boris Motik, Bi-\njan Parsia, Peter Patel-Schneider, and Ulrike Sattler.\n2008. OWL 2: The next step for OWL. Web Seman-\ntics, 6(4):309â€“322.\nYuan He, Jiaoyan Chen, Denvar Antonyrajah, and Ian\nHorrocks. 2022. BERTMap: A BERT-Based Ontol-\nogy Alignment System. Proceedings of the AAAI\nConference on Artiï¬cial Intelligence , 36(5):5684â€“\n5691.\nPride Kavumba, Ryo Takahashi, and Yusuke Oda. 2022.\nAre prompt-based models clueless? In Proceedings\nof the 60th Annual Meeting of the Association for\n3447\nComputational Linguistics (Volume 1: Long Papers),\npages 2333â€“2352, Dublin, Ireland. Association for\nComputational Linguistics.\nMarkus KrÃ¶tzsch, FrantiÅ¡ek Siman Ë‡cÃ­k, and Ian Hor-\nrocks. 2012. A Description Logic Primer. ArXiv,\nabs/1201.4089.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomed-\nical and clinical tasks: Understanding and extend-\ning the state-of-the-art. In Proceedings of the 3rd\nClinical Natural Language Processing Workshop ,\npages 146â€“157, Online. Association for Computa-\ntional Linguistics.\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy\nLiang, and Tatsunori Hashimoto. 2022. Diffusion-\nLM Improves Controllable Text Generation. ArXiv,\nabs/2205.14217.\nHao Liu, Yehoshua Perl, and James Geller. 2020. Con-\ncept placement using BERT trained by transform-\ning and summarizing biomedical ontology structure.\nJournal of Biomedical Informatics, 112:103607.\nLinlin Liu, Xin Li, Ruidan He, Lidong Bing, Shaï¬q\nJoty, and Luo Si. 2021a. Knowledge Based\nMultilingual Language Model. arXiv preprint\narXiv:2111.10962.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2022. Pre-\ntrain, Prompt, and Predict: A Systematic Survey of\nPrompting Methods in Natural Language Processing.\nACM Computing Surveys (CSUR).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nZeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh\nHajishirzi, and Noah A. Smith. 2021b. Probing\nacross time: What does RoBERTa know and when?\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021 , pages 820â€“842, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nWeight Decay Regularization. In International Con-\nference on Learning Representations.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efï¬cient Estimation of Word\nRepresentations in Vector Space. arXiv preprint\narXiv:1301.3781.\nSebastian PadÃ³ and Ido Dagan. 2022. 679Textual En-\ntailment. In The Oxford Handbook of Computa-\ntional Linguistics. Oxford University Press.\nFabio Petroni, Tim RocktÃ¤schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463â€“2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAdam Poliak. 2020. A survey on recognizing textual\nentailment as an NLP evaluation. In Proceedings of\nthe First Workshop on Evaluation and Comparison\nof NLP Systems, pages 92â€“109, Online. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2022. Exploring the Lim-\nits of Transfer Learning with a Uniï¬ed Text-to-Text\nTransformer. J. Mach. Learn. Res., 21(1).\nTimo Schick and Hinrich SchÃ¼tze. 2021. Exploiting\ncloze-questions for few-shot text classiï¬cation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255â€“269, Online. Association for Com-\nputational Linguistics.\nStefan Schlobach. 2005. Debugging and Semantic\nClariï¬cation by Pinpointing. In The Semantic Web:\nResearch and Applications, Second European Se-\nmantic Web Conference (ESWC) , volume 3532 of\nLecture Notes in Computer Science, pages 226â€“240.\nSpringer.\nLynn M. Schriml, Cesar Arze, Suvarna Nadendla, Yu-\nWei Wayne Chang, Mark Mazaitis, Victor Felix,\nGang Feng, and W. Kibbe. 2012. Disease Ontol-\nogy: a backbone for disease semantic integration.\nNucleic Acids Research, 40:D940 â€“ D946.\nAlessandro Solimando, Ernesto JimÃ©nez-Ruiz, and\nGiovanna Guerrini. 2017. Minimizing conservativ-\nity violations in ontology alignments: algorithms\nand evaluation. Knowl. Inf. Syst., 51(3):775â€“819.\nRobyn Speer and Catherine Havasi. 2012. Represent-\ning General Relational Knowledge in ConceptNet 5.\nIn LREC.\nMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sung-\ndong Kim, and Jaewoo Kang. 2021. Can language\nmodels be biomedical knowledge bases? In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 4723â€“\n4734, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n3448\n1 (Long Papers) , pages 1112â€“1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nHongbin Ye, Ningyu Zhang, Shumin Deng, Xiang\nChen, Hui Chen, Feiyu Xiong, Xi Chen, and Huajun\nChen. 2022. Ontology-enhanced Prompt-tuning for\nFew-shot Learning. Proceedings of the ACM Web\nConference 2022.\nA OWL Ontology Concept Expression\nThe Description Logic SROIQunderlies the se-\nmantics of OWL 2 ontologies. Given the top con-\ncept âŠ¤, the bottom concept âŠ¥, the named concept\nA, an individual a, a role (or property) r and a non-\nnegative integer n, SROIQconcept expressions\nare constructed as:\nC, D::=âŠ¤|âŠ¥|A|(C âŠ“D)|(C âŠ”D)|Â¬C|âˆƒr.C|\nâˆ€r.C|â‰¥ n r.C|â‰¤ n r.C|âˆƒr.Self |{a}\nRecall the deï¬nition ofinterpretation I = (âˆ†I, Â·I),\nwhere âˆ†Iis a non-empty set (the domain) and Â·I\nmaps each concept C to CIâŠ†âˆ†I, a each property\nr to rI âŠ†âˆ†IÃ—âˆ†I and each individual a to an\nelement aIâˆˆâˆ†I. We present the semantics of the\nconcept constructors in Table 5.\nB Ontology Preprocessing\nIn case that some of the ontologies we use in this\nwork contain meaningless (e.g., obsolete) concepts\nregarding subsumption sampling and/or contain\nconcept names (or aliases) that are apparently un-\nnatural, we apply a general preprocessing proce-\ndure to all the ontologies, and then conduct indi-\nvidual preprocessing for each ontology.\nGeneral Preprocessing\nâ€¢ Remove obsolete concepts (which are in-\ndicated by the built-in annotation property\nowl:deprecated) and apparently redundant\nconcepts such as foodOn:stupidType.\nâ€¢ Use rdfs:label as the main annotation prop-\nerty to extract concept names except when its\nliteral value is not available. The extracted\nconcept names are lower-cased and any under-\nscores â€œ_â€ in them are removed.\nIndividual Preprocessing\nâ€¢ Schema.org: concept names (deï¬ned in this\nontology are in the Java-identiï¬er style; thus,\nthey are parsed into natural expressions, e.g.,\nâ€œAPIReferenceâ€ to â€œAPI Referenceâ€.\nConstructor Semantics\nA A I\nC âŠ“D C I âˆ©DI\nC âŠ”D C I âˆªDI\nÂ¬C âˆ†I \\CI\nâŠ¤ âˆ†I\nâŠ¥ âˆ…\nâˆƒr.C {x |some rI-successor of x is in CI}\nâˆ€r.C {x |all rI-successors of x are in CI}\nâ‰¥n r.C {x |at least n rI-successors of x are in CI}\nâ‰¤n r.C {x |at most n rI-successors of x are in CI}\nâˆƒr.Self {x |âŸ¨x, xâŸ©âˆˆ rI}\n{a} { aI}\nTable 5: Semantics of the OWL Ontology concept con-\nstructors.\nâ€¢ DOID: remove the concept doid:Disease be-\ncause it is a general concept just below the\nroot concept owl:Thing which will lead to\ntoo many simple subsumptions in the form of\nC âŠ‘Disease.\nâ€¢ FoodOn: reconstruct label strings containing\nnon-natural-language texts of three regular\nexpression patterns (note that (.*) captures\nwhat to be preserved):\n(a) [0-9]+ - (.*) \\(.+\\)\n(b) \\(/quotesingle.Var(.*)\\(gs1/quotesingle.Var, /quotesingle.Vargpc\\)/quotesingle.Var\\)\n(c) \\(/quotesingle.Var(.*)\\(efsa/quotesingle.Var, /quotesingle.Varfoodex2\\)/quotesingle.Var\\)\nfollowed by removal of leading and trail-\ning whitespaces. Note that concepts in this\nontology sometimes have an empty literal\ngiven by rdf:label; in these cases, the\nannotation properties obo:hasSynonym and\nobo:hasExactSynonym are used instead.\nâ€¢ GO: no individual processing.\nC Object Property Verbalisation\nDifferent from verbalising an atomic concept where\nwe simply use its name (or alias), we enforce some\nsimple rules to verbalise an object property for a\nbasic grammar ï¬x. If the property name starts with\na passive verb, adjective, or noun, we append â€œisâ€\nto the head. For example, â€œcharacteristic ofâ€ is\nchanged to â€œis characteristic ofâ€ ; â€œrealised inâ€ is\nchanged to â€œis realised inâ€. Note that the wordâ€™s\npart-of-speech tag is automatically determined us-\ning the Python library Spacy19.\n19https://spacy.io/\n3449\nComplex Concept C Verbalisation V(C)\nBioRegulation âŠ“âˆƒnegRegulate.ProlineBiosynProc â€œbiological regulation that negatively regulates some proline\nbiosynthetic processâ€\nApoptoticProc âŠ“âˆƒpartOf.Luteolysis â€œapoptotic process that is part of some lutelysisâ€\nConcnOf âŠ“âˆƒcharOf.(fucose âŠ“âˆƒpartOf.MaterialEnt ) â€œconcentration of something that is characteristic of some fucose\nthat is part of some material entityâ€\nâˆƒderivesFrom. (TimothyPlant âŠ”TrifoliumPratense ) âŠ“\nPlantFoodProd âŠ“Silage\nâ€œsilage and plant food product that derives from some timothy\nplant or trifolium pratenseâ€\nApple âŠ“Â¬âˆƒhasPart.ApplePeel â€œapple (whole or parts) and not something that has part some ap-\nple peelâ€\nTable 6: Examples of verbalised complex concepts from GOâ€™s andFoodOnâ€™s Complex SI datasets. Note that in the\nreal datasets, the named concepts and object properties are represented by their IRIs (unique identiï¬ers) instead of\nthe abbreviated names shown in the table.\nD Complex Concept Verbalisation\nExamples\nFor clearer understanding of how our verbalisation\napproach works, we present some typical exam-\nples of verbalised concepts from the constructed\nComplex SI datasets in Table 6.\nE Implementation Choices for Assumed\nDisjointness\nAs mentioned in Section 3.1, validating the dis-\njointness axiom for each concept pair (C, D) we\nhave sampled as a potential negative subsumption\nwould be time-consuming because we need to itera-\ntively add the disjointness axiom into the ontology\nO, conduct reasoning, and remove the axiom after-\nwards. Therefore, in practice we can use the follow-\ning conditions to replace the satisï¬ability check:\n(i) No subsumption relationship : OÌ¸|= C âŠ‘\nD and OÌ¸|= D âŠ‘C;\n(ii) No common instance : there is no named\ninstance a in Osuch that O |= C(a) and\nO|= D(a).\nIf C and D satisfy these two conditions, they are\nlikely to be satisï¬able after adding the disjoint-\nness axiom C âŠ“D âŠ‘âŠ¥ into O. Since these two\nconditions involve no extra reasoning for a new\naxiom, they are much more efï¬cient than iteratively\nconducting satisï¬ability check for candidates.\nIt is important to notice that we still need the no\ncommon descendant check to prevent foreseeable\nunsatisï¬ability.\n(iii) No common descendant: there is no named\natomic concept A in Osuch that O|= A âŠ‘C\nand O|= A âŠ‘D.\nThis is because if there is a named atomic concept\nA that is an inferred sub-class (i.e., descendant)\nDC\nDC\nCD\nDC(a) ğ¶âŠ‘ğ·\n(d) ğ¶âŠ“ğ·â‹¢\tâŠ¥but not (a) or (b)(c) ğ¶âŠ“ğ·âŠ‘âŠ¥\n(b) ğ·âŠ‘ğ¶\nFigure 4: Set-based semantics for relationships be-\ntween two ontology concepts.\nof C and D, then it is possible that C and D are\nsatisï¬able in Oâˆª{C âŠ“D âŠ‘âŠ¥}, but A is certainly\nunsatisï¬able (equivalent to âŠ¥).\nF Set-based Interpretations of\nSubsumption Samples\nIn this section, we provide more explanation for\nhow we deï¬ne positive and negative samples in the\nSubsumption Inference (SI) task.\nRecall the deï¬nitions in Section 2.1, an ontology\nOentails a subsumption axiom C âŠ‘D if it holds\nfor every interpretation Iof O. In terms of set-\nbased semantics, this refers to case (a) in Figure 4.\nIn the (b), (c), or (d) cases, there exists at least one\ninterpretation I, such that we can ï¬nd an individual\nx that xI âˆˆ CI and xI Ì¸âˆˆ DI; hence Odoes\nnot entail the subsumption axiom C âŠ‘D. Non-\nsubsumption is entailed only when (a) does not\nhold for every interpretation of O.\nDisjointness corresponds to(c) in Figure 4 where\nthe set of C and the set of D have no overlap for\nevery interpretation. Non-subsumptions an ontol-\nogy typically entails come from the disjointness\naxioms (but disjointness âˆ€x.C(x) â†’ Â¬D(x) is\n3450\nAtomic SI Complex SI\nSetting biMNLI Schema.org DOID FoodOn GO FoodOn GO\nroberta-base\nK=0 62.5 (6.5) 56.4 (3.6) 53.3 (4.0) 54.6 (4.4) 49.0 (2.4) 55.9 (3.6) 48.7 (3.1)\nK=4 67.6 (5.2) 62.9 (5.2) 61.8 (6.7) 62.1 (4.2) 65.2 (5.0) 62.4 (3.2) 52.2 (7.1)\nK=8 70.7 (4.5) 71.2 (4.5) 72.9 (5.7) 69.0 (5.2) 70.4 (5.1) 66.0 (4.4) 63.0 (5.0)\nK=16 74.3 (3.3) 79.7 (4.2) 83.4 (2.5) 79.8 (3.0) 78.3 (3.0) 70.2 (5.5) 73.8 (4.0)\nK=32 78.8 (1.1) 84.3 (2.0) 89.0 (1.4) 85.0 (1.1) 84.6 (2.5) 77.0 (1.5) 76.4 (2.5)\nK=64 80.9 (1.5) 88.3 (1.5) 91.2 (0.7) 88.2 (0.7) 87.3 (0.8) 80.0 (2.0) 81.7 (1.4)\nK=128 85.1 (1.0) 91.1 (0.7) 92.4 (0.7) 90.0 (0.7) 89.0 (0.8) 85.5 (1.3) 86.9 (1.5)\nroberta-large\nK=0 68.7 (6.2) 61.7 (7.2) 59.8 (5.4) 60.1 (8.8) 54.6 (1.9) 56.1 (1.9) 50.4 (0.6)\nK=4 78.1 (6.6) 69.4 (5.4) 74.0 (5.5) 71.6 (4.4) 67.6 (3.4) 64.1 (5.1) 56.9 (5.7)\nK=8 83.0 (5.2) 78.5 (3.0) 84.4 (3.8) 77.0 (6.0) 75.3 (3.2) 71.3 (3.1) 64.2 (7.6)\nK=16 87.5 (2.4) 84.4 (2.4) 87.6 (2.3) 83.4 (3.5) 82.8 (1.9) 76.2 (2.5) 76.4 (3.3)\nK=32 89.9 (1.2) 87.3 (1.9) 92.3 (0.7) 88.9 (1.6) 87.7 (1.6) 80.8 (3.8) 81.6 (2.2)\nK=64 90.8 (1.4) 90.4 (0.8) 92.6 (0.7) 90.9 (1.2) 90.1 (0.7) 84.1 (1.4) 86.2 (2.2)\nK=128 93.0 (0.8) 92.9 (0.8) 93.4 (0.5) 92.2 (0.5) 91.0 (0.7) 88.4 (1.1) 90.2 (1.0)\nfull 97.5 95.4 97.8 98.7 98.1 95.8 98.8\nTable 7: Full results of roberta-base and roberta-large on the biMNLI, Atomic SI, and Complex SI tasks with each cell\nstating â€œmean accuracy (standard deviation)â€ except for the majority vote and fully supervised settings where standard deviation\nis not available.\n0 4 8 16 32 64 128\nK (samples per classification label)\n10\n30\n50\n70\n90Accuracy (%)\nbiMNLI DOID Schema.org FoodOn GO FoodOn (Comp) Go (Comp)\nFigure 5: Visualisation of K-shot results (for roberta-base) on the biMNLI, Atomic SI, and Complex SI tasks where the dotted\nhorizontal line indicates majority vote. The order of the bars is the same as in the legend.\nstricter than non-subsumption âˆƒx.C(x)âˆ§Â¬D(x)).\nNevertheless, ontologies are typically underspec-\niï¬ed in terms of disjointness, and thus getting\nenough negative samples is unfeasible. To ï¬nd a\nmiddle ground, it is reasonable to adopt heuristics.\nThe assumed disjointness we follow in Section 3.1\nin the main body of the paper serves this purpose.\nIn the ideal setting where we check the satisï¬abil-\nity of C and D after adding the disjointness axiom\nand no common descendant of C and D, cases\n(a) and (b) in Figure 4 will be prevented and the\nchance of (d) reduced. Even in the practical alter-\nnative proposed in this Appendix E, the no sub-\nsumption relationship condition also ensures that\n(a) and (b) are not entailed and the no common\ndescendant and no common instance conditions\nreduce the chance of (d). Thus, the assumed dis-\njointness is a reasonable approach to approximate\nnon-subsumptions.\nG Complementary Results and Figures\nIn the main body of the paper, we report par-\ntial results (accuracy scores and standard devia-\ntions) of roberta-large and roberta-base for K âˆˆ\n{0, 4, 32, 128}. In Table 7, we present full results\nof both LMs for K âˆˆ{0, 4, 8, 16, 32, 64, 128}.\nBesides, we provide the visualisation of K-shot\nresults for roberta-base in Figure 5. The observa-\ntions are consistent with those for roberta-large\nin Figure 3.\n3451\nACL 2023 Responsible NLP Checklist\nA For every submission:\nâ–¡ A1. Did you describe the limitations of your work?\nLeft blank.\nâ–¡ A2. Did you discuss any potential risks of your work?\nLeft blank.\nâ–¡ A3. Do the abstract and introduction summarize the paperâ€™s main claims?\nLeft blank.\nâ–¡ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB â–¡ Did you use or create scientiï¬c artifacts?\nLeft blank.\nâ–¡ B1. Did you cite the creators of artifacts you used?\nLeft blank.\nâ–¡ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\nâ–¡ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciï¬ed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\nâ–¡ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiï¬es individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\nâ–¡ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\nâ–¡ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiï¬cant, while on small test sets they may not be.\nLeft blank.\nC â–¡ Did you run computational experiments?\nLeft blank.\nâ–¡ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n3452\nâ–¡ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\nâ–¡ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\nâ–¡ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD â–¡ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\nâ–¡ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\nâ–¡ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participantsâ€™ demographic\n(e.g., country of residence)?\nLeft blank.\nâ–¡ D3. Did you discuss whether and how consent was obtained from people whose data youâ€™re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\nâ–¡ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\nâ–¡ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n3453",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.83591628074646
    },
    {
      "name": "Ontology",
      "score": 0.7437894344329834
    },
    {
      "name": "Inference",
      "score": 0.7271028161048889
    },
    {
      "name": "Axiom",
      "score": 0.6103978753089905
    },
    {
      "name": "Natural language processing",
      "score": 0.5906832814216614
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5702109932899475
    },
    {
      "name": "Web Ontology Language",
      "score": 0.5665382742881775
    },
    {
      "name": "Rule of inference",
      "score": 0.5123640298843384
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5093939900398254
    },
    {
      "name": "Description logic",
      "score": 0.497771292924881
    },
    {
      "name": "Focus (optics)",
      "score": 0.4834950268268585
    },
    {
      "name": "Natural language",
      "score": 0.43803173303604126
    },
    {
      "name": "ENCODE",
      "score": 0.43267005681991577
    },
    {
      "name": "Programming language",
      "score": 0.21357029676437378
    },
    {
      "name": "Semantic Web",
      "score": 0.1622227430343628
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}