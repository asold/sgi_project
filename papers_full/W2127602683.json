{
    "title": "Style &amp; topic language model adaptation using HMM-LDA",
    "url": "https://openalex.org/W2127602683",
    "year": 2006,
    "authors": [
        {
            "id": "https://openalex.org/A4276821576",
            "name": "Bo-June (Paul) Hsu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2154846939",
            "name": "James Glass",
            "affiliations": [
                "Vassar College"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2001082470",
        "https://openalex.org/W2111732304",
        "https://openalex.org/W2112971401",
        "https://openalex.org/W2102439588",
        "https://openalex.org/W2127836646",
        "https://openalex.org/W2159748844",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2147152072",
        "https://openalex.org/W10704533",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W1552793991",
        "https://openalex.org/W1880262756",
        "https://openalex.org/W2118714763",
        "https://openalex.org/W2097927681",
        "https://openalex.org/W2148193186",
        "https://openalex.org/W2158266063",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W2570951272",
        "https://openalex.org/W2140124448",
        "https://openalex.org/W1996903695",
        "https://openalex.org/W2111305191",
        "https://openalex.org/W2159344129",
        "https://openalex.org/W4231510805",
        "https://openalex.org/W1644652583",
        "https://openalex.org/W2086891622",
        "https://openalex.org/W2112874453",
        "https://openalex.org/W1665921526"
    ],
    "abstract": "Adapting language models across styles and topics, such as for lecture transcription, involves combining generic style models with topic-specific content relevant to the target document.In this work, we investigate the use of the Hidden Markov Model with Latent Dirichlet Allocation (HMM-LDA) to obtain syntactic state and semantic topic assignments to word instances in the training corpus.From these context-dependent labels, we construct style and topic models that better model the target document, and extend the traditional bag-of-words topic models to n-grams.Experiments with static model interpolation yielded a perplexity and relative word error rate (WER) reduction of 7.1% and 2.1%, respectively, over an adapted trigram baseline.Adaptive interpolation of mixture components further reduced perplexity by 9.5% and WER by a modest 0.3%.",
    "full_text": null
}