{
  "title": "Style &amp; topic language model adaptation using HMM-LDA",
  "url": "https://openalex.org/W2127602683",
  "year": 2006,
  "authors": [
    {
      "id": "https://openalex.org/A4276821576",
      "name": "Bo-June (Paul) Hsu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154846939",
      "name": "James Glass",
      "affiliations": [
        "Vassar College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2001082470",
    "https://openalex.org/W2111732304",
    "https://openalex.org/W2112971401",
    "https://openalex.org/W2102439588",
    "https://openalex.org/W2127836646",
    "https://openalex.org/W2159748844",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W10704533",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1552793991",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2118714763",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W2148193186",
    "https://openalex.org/W2158266063",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2570951272",
    "https://openalex.org/W2140124448",
    "https://openalex.org/W1996903695",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W2159344129",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W1644652583",
    "https://openalex.org/W2086891622",
    "https://openalex.org/W2112874453",
    "https://openalex.org/W1665921526"
  ],
  "abstract": "Adapting language models across styles and topics, such as for lecture transcription, involves combining generic style models with topic-specific content relevant to the target document.In this work, we investigate the use of the Hidden Markov Model with Latent Dirichlet Allocation (HMM-LDA) to obtain syntactic state and semantic topic assignments to word instances in the training corpus.From these context-dependent labels, we construct style and topic models that better model the target document, and extend the traditional bag-of-words topic models to n-grams.Experiments with static model interpolation yielded a perplexity and relative word error rate (WER) reduction of 7.1% and 2.1%, respectively, over an adapted trigram baseline.Adaptive interpolation of mixture components further reduced perplexity by 9.5% and WER by a modest 0.3%.",
  "full_text": null,
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9714893102645874
    },
    {
      "name": "Hidden Markov model",
      "score": 0.8026502728462219
    },
    {
      "name": "Computer science",
      "score": 0.8008972406387329
    },
    {
      "name": "Language model",
      "score": 0.7435770034790039
    },
    {
      "name": "Latent Dirichlet allocation",
      "score": 0.6907014846801758
    },
    {
      "name": "Topic model",
      "score": 0.6684331893920898
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6238822937011719
    },
    {
      "name": "Natural language processing",
      "score": 0.5500081181526184
    },
    {
      "name": "Trigram",
      "score": 0.514980137348175
    },
    {
      "name": "Speech recognition",
      "score": 0.5024068355560303
    },
    {
      "name": "Context (archaeology)",
      "score": 0.451302170753479
    },
    {
      "name": "Word (group theory)",
      "score": 0.44710004329681396
    },
    {
      "name": "Linguistics",
      "score": 0.17427504062652588
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}