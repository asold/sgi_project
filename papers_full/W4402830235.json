{
    "title": "GOPT: Generalizable Online 3D Bin Packing via Transformer-Based Deep Reinforcement Learning",
    "url": "https://openalex.org/W4402830235",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2753214298",
            "name": "Xiong Heng",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2380082370",
            "name": "Guo Chang-rong",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2018307136",
            "name": "Peng Jian",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2009735302",
            "name": "Ding Kai",
            "affiliations": [
                "Robert Bosch (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1341808957",
            "name": "Chen Wenjie",
            "affiliations": [
                "Midea Group (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5094196572",
            "name": "Qiu Xuchong",
            "affiliations": [
                "Robert Bosch (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2185378682",
            "name": "Bai Long",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2004085934",
            "name": "Xu Jianfeng",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3188558100",
        "https://openalex.org/W3175494061",
        "https://openalex.org/W4380930001",
        "https://openalex.org/W3112629014",
        "https://openalex.org/W2598380049",
        "https://openalex.org/W6780123678",
        "https://openalex.org/W4200291756",
        "https://openalex.org/W4316660701",
        "https://openalex.org/W4226102240",
        "https://openalex.org/W4220778817",
        "https://openalex.org/W2247572377",
        "https://openalex.org/W14327270",
        "https://openalex.org/W1548434108",
        "https://openalex.org/W2101057470",
        "https://openalex.org/W2161926431",
        "https://openalex.org/W2134288639",
        "https://openalex.org/W3113053674",
        "https://openalex.org/W2539798475",
        "https://openalex.org/W2141119792",
        "https://openalex.org/W2967842325",
        "https://openalex.org/W4319950026",
        "https://openalex.org/W3082452785",
        "https://openalex.org/W6758687306",
        "https://openalex.org/W6748487558",
        "https://openalex.org/W4307726936",
        "https://openalex.org/W2999103220",
        "https://openalex.org/W6743802245",
        "https://openalex.org/W6692846177",
        "https://openalex.org/W4389335084",
        "https://openalex.org/W3182680257",
        "https://openalex.org/W6741002519",
        "https://openalex.org/W6627932998",
        "https://openalex.org/W6798240405",
        "https://openalex.org/W4297733535",
        "https://openalex.org/W1191599655",
        "https://openalex.org/W4287744247",
        "https://openalex.org/W3183892796",
        "https://openalex.org/W3107104084",
        "https://openalex.org/W2736601468"
    ],
    "abstract": "Robotic object packing has broad practical applications in the logistics and automation industry, often formulated by researchers as the online 3D Bin Packing Problem (3D-BPP). However, existing DRL-based methods primarily focus on enhancing performance in limited packing environments while neglecting the ability to generalize across multiple environments characterized by different bin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin Packing approach via Transformer-based deep reinforcement learning (DRL). First, we design a Placement Generator module to yield finite subspaces as placement candidates and the representation of the bin. Second, we propose a Packing Transformer, which fuses the features of the items and bin, to identify the spatial correlation between the item to be packed and available sub-spaces within the bin. Coupling these two components enables GOPT's ability to perform inference on bins of varying dimensions. We conduct extensive experiments and demonstrate that GOPT not only achieves superior performance against the baselines, but also exhibits excellent generalization capabilities. Furthermore, the deployment with a robot showcases the practical applicability of our method in the real world. The source code will be publicly available at https://github.com/Xiong5Heng/GOPT.",
    "full_text": "IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED SEPTEMBER, 2024 1\nGOPT: Generalizable Online 3D Bin Packing via\nTransformer-based Deep Reinforcement Learning\nHeng Xiong1, Changrong Guo 1, Jian Peng 1, Kai Ding 2, Wenjie Chen3,4, Xuchong Qiu 2, Long Bai 1, and Jianfeng\nXu1,5\nAbstract—Robotic object packing has broad practical applica-\ntions in the logistics and automation industry, often formulated\nby researchers as the online 3D Bin Packing Problem (3D-\nBPP). However, existing DRL-based methods primarily focus on\nenhancing performance in limited packing environments while\nneglecting the ability to generalize across multiple environments\ncharacterized by different bin dimensions. To this end, we\npropose GOPT, a g eneralizable online 3D Bin P acking approach\nvia Transformer-based deep reinforcement learning (DRL). First,\nwe design a Placement Generator module to yield finite sub-\nspaces as placement candidates and the representation of the\nbin. Second, we propose a Packing Transformer, which fuses the\nfeatures of the items and bin, to identify the spatial correlation\nbetween the item to be packed and available sub-spaces within the\nbin. Coupling these two components enables GOPT’s ability to\nperform inference on bins of varying dimensions. We conduct\nextensive experiments and demonstrate that GOPT not only\nachieves superior performance against the baselines, but also\nexhibits excellent generalization capabilities. Furthermore, the\ndeployment with a robot showcases the practical applicability of\nour method in the real world. The source code will be publicly\navailable at https://github.com/Xiong5Heng/GOPT.\nIndex Terms—Reinforcement learning, manipulation planning,\nrobotic packing.\nI. I NTRODUCTION\nW\nITH the prosperity of the global trade and e-commerce\nmarket, warehouse automation has developed rapidly\nin recent years. Efficient object placement in the warehouse\nthrough optimal packing strategies can bring numerous bene-\nfits, such as reduced labor requirements and cost savings [1].\nFig. 1 illustrates an example of item picking and packing\nusing a robotic arm. In this paper, it is assumed that the robot\nManuscript received: June 22, 2024; Accepted September 7, 2024.\nThis paper was recommended for publication by Editor Chao-Bo Yan upon\nevaluation of the Associate Editor and Reviewers’ comments. This work was\nsupported by the National Key R&D Program of China under Grant No.\n2022YFB4700300. (Corresponding author: Jianfeng Xu.)\n1Heng Xiong, Changrong Guo, Jian Peng, and Long Bai are with State Key\nLaboratory of Intelligent Manufacturing Equipment and Technology, School\nof Mechanical Science and Engineering, Huazhong University of Science\nand Technology, Wuhan 430074, China {xiongheng, guochangrong,\npeng_jian, bailong}@hust.edu.cn\n2Kai Ding and Xuchong Qiu are with BOSCH Corporate Research, China\n{firstname.lastname}@cn.bosch.com\n3,4Wenjie Chen is with State Key Laboratory of High-end Heavy-load\nRobots, Midea Group, Foshan 528300, China, and also with Midea Corporate\nResearch Center, Foshan 528311, China chenwj42@midea.com\n5Jianfeng Xu is with State Key Laboratory of Intelligent Manufactur-\ning Equipment and Technology, School of Mechanical Science and Engi-\nneering, Huazhong University of Science and Technology, Wuhan 430074,\nChina, and also with HUST-Wuxi Research Institute, Wuxi 214174, China\njfxu@hust.edu.cn\nDigital Object Identifier (DOI): see top of this page.\nPicking\nEstimation\nPacking\nTarget\nplacement\nPicking bin Packing bin\nFig. 1. Robot picking and packing pipeline. Left: A robot randomly picks an\nitem from a cluttered collection of boxes and packs it in a compact manner,\nand three RGB-D cameras are mounted. Right: Two overhead cameras observe\nthe status of the two bins, respectively, and one up-looking camera estimates\nthe dimension of the picked item.\npicking is well implemented. Researchers have commonly\naddressed the placement challenge in robot packing by formu-\nlating it as an online 3D Bin Packing Problem (3D-BPP) [2],\n[3]. As one of the classic combinatorial optimization problems,\n3D-BPP strives to place a set of known cuboid items in an\naxis-aligned fashion into a bin to maximize space utilization.\nHowever, observing all items and obtaining full knowledge\nabout them is challenging in many real-world scenarios. The\nonline 3D-BPP is a more practical variant of 3D-BPP that\nrefers to packing items one by one under the observation of\nonly the incoming item.\nDue to the limited knowledge, the online 3D-BPP cannot be\nsolved by exact algorithms [4]. Researchers have previously\nfocused on developing heuristics with the greedy objectives for\nthe problem, which are designed by abstracting the experience\nof human packers [5]. However, while intuitive, these heuris-\ntics typically yield sub-optimal solutions. In recent years,\nthere has been an emerging research interest in resolving\nonline 3D-BPP via deep reinforcement learning (DRL) [2],\n[3], [6], [7], and indeed, DRL-based methods demonstrate\nimpressive performance. Nevertheless, it is noteworthy that\nthe training process often encounters challenges in reaching\nconvergence [2], [8], and these methods struggle to generalize\neffectively across diverse packing scenarios, especially those\ncharacterized by different bin dimensions. These limitations\nsubstantially curtail the broader applicability of DRL in typical\nuse cases. More specifically, the current state-of-the-art DRL-\nbased methods can only perform inference on bins of the same\nsize as those they are trained on [3], [9]. Trained models are\narXiv:2409.05344v2  [cs.RO]  12 Sep 2024\n2 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED SEPTEMBER, 2024\nnot transferable to bins of different sizes. Additionally, the\ninherent dependence of the packing action space size on bin\ndimensions in these methods presents significant challenges\nfor model convergence, especially when dealing with larger\nbins [10].\nMotivated by the aforementioned limitations, this paper\nproposes GOPT, a generalizable online 3D Bin Packing ap-\nproach via Transformer-based DRL, as shown in Fig. 2. In\nGOPT, a Placement Generator (PG) module first adopts a\nheuristic to generate a fixed-length set of free sub-spaces\nwithin the current bin as placement candidates, which ensures\ncontrollability over the size of the packing action space.\nBoth the placement candidates and the item to be packed\nare collectively defined as the state of the Markov Decision\nProcess (MDP). Then, GOPT incorporates a novel packing\npolicy network that integrates a Packing Transformer module.\nThis module enhances GOPT’s generalizability by intrinsically\nidentifying the spatial correlation between the current item\nand the available sub-spaces, as well as the relations among\nthese sub-spaces, which are derived from the PG module.\nThe Packing Transformer employs self-attention layers and bi-\ndirectional cross-attention layers to extract features as inputs\nto the reinforcement learning policy.\nExperiments show that our method outperforms the state-\nof-the-art packing methods in terms of space utilization and\nthe number of packed objects. To the best of our knowledge,\nour work is the first to provide the generalization capability\nto infer across various bins with a trained model while\nmaintaining high performance. We also deploy our packing\nplanning method in a robotic manipulator to demonstrate its\npractical applicability in the real world.\nIn summary, our main contributions are: (1) GOPT, a\nnovel method for online 3D-BPP that enlarges the packing\nperformance and generalization; (2) A Placement Generator\nmodule to modulate the packing action space and represent\nthe state of the bin; (3) A network called Packing Transformer,\nwhich captures the relations between the current item and\nthe available sub-spaces, as well as interrelations among\nsub-spaces; (4) Extensive experimental evaluations comparing\nGOPT with baselines.\nII. R ELATED WORK\nThe 3D-BPP is a classical optimization problem and is\nknown to be strongly NP-hard [11]. We herein briefly review\nrelated heuristic and DRL-based methods.\nA. Heuristic Methods\nEarly works primarily focus on designing efficient heuris-\ntics for their simplicity. Researchers attempt to define some\npacking rules distilled from human workers’ experience, such\nas First Fit [12], Best Fit [13], and Deepest-Bottom-Left-\nFill [14]. Corner points (CP) [15], extreme points (EP) [16],\nempty maximal spaces (EMS) [17], and internal corners point\n(ICP) [18] endeavor to represent potential free spaces where\nitems can be packed for enhancing heuristic methods. For\ninstance, Ha et al. [5] propose OnlineBPH, which selects one\nEMS to minimize the margin between the faces of the item\nto be packed and the faces of the EMS. Yarimcam et al. [19]\nprovide heuristics expressed in terms of policy matrices by em-\nploying the Irace parameter tuning algorithm [20]. Wang et al.\n[21] propose Heightmap-Minimization (HM) which favors the\nplacement that minimizes occupied volume. To mitigate the\nuncertainties originating from the real world, Shuai et al. keep\ndeformed boxes stacked close to enhance stability [22]. Hu\net al. develop a Maximize-Accessible-Convex-Space (MACS)\nstrategy to optimize the available empty space for packing\npotential large future items [23]. These methods are intuitive\nand effective; however, they rely on hand-crafted rules and lack\nthe capacity to demonstrate superior performance consistently\nacross diverse problem settings. Our work draws on the\nrepresentation of empty spaces in heuristics, but uses DRL to\nlearn packing patterns without being limited by domain expert\nknowledge.\nB. DRL-based Methods\nDRL has shown promise in solving certain combinatorial\noptimization problems [24], [25]. Therefore, there is a trend\nto use DRL to solve the 3D-BPP recently. Que et al. [26]\ntackle the offline 3D-BPP with variable height by using DRL\nwith Transformer structure to sequentially address subtasks of\nposition, item selection, and orientation. Instead, we focus on\nthe online 3D-BPP and determine the position and orientation\nsimultaneously. To the best of our knowledge, Deep-Pack [27]\nis the first to use a DRL-based model to solve a 2D online\npacking problem, with potential extensions to the online 3D-\nBPP. It takes an image showing the current state of the bin as\ninput and outputs the pixel location for packing the incoming\nitem. Verma et al. [6] combine a search heuristic with DRL\nand propose a two-step strategy for solving the problem with\nany number and size of bins. Zhao et al. [2], [10] formulate\nthe problem as a constrained MDP and adopt ACKTR method\n[28] to train a CNN-based DRL agent. In [2], the DRL\nagent comprises an actor, a critic, and a predictor to estimate\naction probabilities, value, and feasibility mask, respectively.\nIt is then improved by decomposing the packing action into\nthe length and width dimensions and orientation to reduce\naction space [10]. They subsequently introduce the Packing\nConfiguration Tree (PCT) based on heuristic search rules and\nincorporate it into a DRL agent [8]. The agent employs Graph\nAttention Networks [29] as the policy and is also trained with\nACKTR. To investigate the synergies of heuristics and DRL,\nYang et al. [7] propose PackerBot, which utilizes heuristic\nreward to assist the DRL agent to perform better. Xiong et\nal. [3] introduce a candidate map mechanism to reduce the\ncomplexity of exploration and improve performance for the\nCNN-based DRL agent trained with A2C [30]. These methods\nusually concatenate features of the item and the bin directly to\nlearn policies. In contrast, GOPT first proposes free sub-spaces\nwithin a bin and utilizes a modified Transformer to discern\nthe relations among these spaces and the relations between\nthem and the current item. Our method ensures generalizability\nacross diverse packing environments.\nXIONG et al.: GOPT: GENERALIZABLE ONLINE 3D BIN PACKING VIA TRANSFORMER-BASED DEEP REINFORCEMENT LEARNING 3\nMLP\n(128, 128, 1)\nHeightmap\nPacking Transformer\nValue\nSoftmax\nMLP\n(32, 128)\nEMSs\nMask\nItem to be packed\nMLP\n(32, 128)\nMLP\n(128)\nMLP\n(128)MLP\n(128)\nMLP\n(128)\nElement-wise product Concatenation\nEMS\nFeatures\nItem\nFeatures\nActor\nCritic\nPlacement Generator\nFLB \nX \nY \nZ \nL W \nH \n55505552332223 002220000000\n2222222333\n00000000\n0\n000000000000000000\n2\n(a) The framework of GOPT\n Self-Attention\nEMS Embedding Item Embedding\nAdd & Norm\nMLP (256, 128)\nAdd & Norm\nAdd & Norm\nMLP (256, 128)\nAdd & Norm\n Self-Attention\nAdd & Norm\nMLP (256, 128)\nAdd & Norm\nAdd & Norm\nMLP (256, 128)\nAdd & Norm\n Cross-Attention\nEMS Features Item Features  \n Cross-Attention (b) Packing Transformer\nFig. 2. Overview of our method. (a) In the GOPT, the inputs comprise the item to be packed and the current heightmap of the bin, wherein each cell’s value\nrepresents the respective height. Utilizing the Placement Generator, a set of EMSs is produced, along with a pairwise action mask between each EMS and the\noptional orientation of the item. After that, we separately encode the EMSs and the item and then fuse the features using the Packing Transformer, of which\noutputs are fed into the actor and critic networks to generate logits of all actions and estimate the state-value function; (b) depicts the details of the proposed\nPacking Transformer. The transformer comprises three stacked blocks, each containing two self-attention and two cross-attention layers.\nIII. M ETHODOLOGY\nA. Problem Description\nAs shown in Fig. 1, a robot randomly picks an object\nfrom an unstructured pile with a set of box-shaped items\nof various dimensions. The complete knowledge about all\nitems is unavailable in advance. One camera measures the\ndimensions of the picked item, which is then placed into the\npacking bin. This specific scenario can be characterized as an\nonline 3D-BPP. The objective is to place as many items into\nthe bin as possible and maximize the bin’s space utilization.\nWe define the front-left-bottom (FLB) vertex of the bin\nwith dimensions (L, W, H) as the origin (0, 0, 0), and the\ndirections along the length, width, and height as X, Y , and\nZ directions, respectively, as shown in Fig. 2a. For items,\n(xt, yt, zt) denotes the FLB coordinate of the t-th item with\ndimensions (lt, wt, ht).\nIn the robot packing task, the following physical constraints\nmust be taken into consideration.\nOrthogonal placement: Items are placed orthogonally into\nthe bin, and their sides are aligned with the bin’s sides.\nOptional orientation: Items are placed in an upright manner;\nin conjunction with the first constraint, items have just two\ndistinct vertical in-plane orientations, either 0◦ or 90◦.\nStatic stability: During the process of packing, items must\nremain stable under gravity and inter-item forces. For compu-\ntational efficiency, an item is considered stable if the projection\nof its geometric center onto its bottom falls inside the support\npolygon which is formed by the convex hull of all horizontal\nsupport points of this item [23].\nB. Placement Generator\nFor the selected item to be packed, we predict the hori-\nzontal position (xt, yt) and the corresponding orientation of\nits placement in the bin. The vertical position zt is analyt-\nically determined by the lowest placement position due to\ngravity. As aforementioned, there are two possible orientations\nfor one item. Therefore, when placing an item into a bin\nwith dimensions (L, W, H), it results in a total number of\nL × W × 2 possible placements [2]. On the one hand, this\nquantity is unbearable for the packing problem with the\nsequential-decision nature because it will grow exponentially\nwith larger bin dimensions. On the other hand, some are\ninevitably unproductive for the item to be packed within this\nplacement set.\nWith the aim of constraining the potentially large place-\nment search space, we design a Placement Generator (PG)\nmodule to produce a finite and efficient placement subset\nbased on the incoming item and current bin configuration.\nWe first explicitly represent the real-time status of the bin by\nutilizing the heightmap. Other methods that leverage planned\nplacements for previous items as the representation [8] lack\nfeedback and closed-loop control. In contrast, the heightmap\ncan be derived from the visual observation captured by a\ncamera conveniently when deploying PG in a real-world robot\npacking task. Drawing from the empty maximal space (EMS)\nscheme for managing the empty spaces in a bin [17], [31],\ncandidate placements are computed based on the current state.\nSpecifically, we identify corner points by detecting height\nvariation along the heightmap’s X and Y directions. EMSs\nare then generated by extending unit rectangles from each\ncorner and halting when encountering higher elevation (Fig. 3).\nEach EMS can be defined by its FLB vertex and the corre-\nsponding opposite vertex as depicted in Fig. 3c. The resulting\n6-dimensional vector is normalized to [0, 1], regardless of\nthe dimensions of the bin. We obtain an EMS subset with\ncontrollable size and rank them by height value, denoted as\n{Ei}N\ni=1. Finally, given an item to be packed, we check the\nfeasibility of each EMS following Section III-A and produce\na pairwise mask between each EMS and orientation. When\npacking the item within the bin, we select an appropriate\n4 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED SEPTEMBER, 2024\nFig. 3. Illustration of the EMS generation procedure. (a) In an example scene\nwith two placed items, the heightmap indicates the current height of stacked\nitems in each grid cell; (b) Five corner points (black dots) are detected at\nthis heightmap; (c) Based on these points, the corresponding largest inscribed\nrectangles (blue) within the bin are generated, namely EMSs. Taking the first\nEMS as an example, it is defined by two red vertices of the blue rectangles.\nEMS and orientation and align the item’s and the EMS’s FLB\nvertices.\nC. Reinforcement Learning Formulation\nDRL problems are commonly modeled as a Markov Deci-\nsion Process (MDP). An MDP with parameters ⟨S, A, P, R, γ⟩\nis utilized to characterize the packing environment in this\npaper, where S denotes the state space, A denotes the action\nspace, P : S × A × S →[0, +∞) stands for the transition\nprobabilities, R : S × A →R is the scalar reward function,\nand γ ∈ (0, 1] is the discount factor for balancing the near-\nterm and long-term rewards in DRL. Reinforcement learning\nalgorithms aim to learn a policy π : S × A →R, which\ndetermines the probability of selecting an action a given a state\ns. The objective of the policy is to maximize the cumulative\ndiscounted reward over an episode, expressed as P\nt γtrt,\nwhere t denotes the time step, and rt, at, and st represent\nthe reward, action, and state at time step t, respectively. In the\nfollowing, we formulate the online 3D-BPP as an MDP for\nDRL training.\nState: At each time step t, the policy receives a state\nst, comprising the incoming item to be packed st,item and\nthe current bin configuration st,bin. For the first part, the\ndimension of the item (lt, wt, ht) is essential. Some studies\n[3], [7] employ this three-dimensional vector explicitly as the\nitem representation, while others prefer a three-channel map\nfor the convenience of neural network design [2], [9]. In the\nmap representation, each channel is assigned lt, wt, and ht,\nrespectively. To account for both the geometry and optional\norientations, we propose an item representation which is a\n2 ×3 matrix, st,item =\n\u0014\nlt wt ht\nwt lt ht\n\u0015\n, where (lt, wt, ht) and\n(wt, lt, ht) represent the dimensions of the item after rotating\nit by 0◦ and 90◦. For the second part, the existing methods\ninclude the heightmap [3], the list of packed items [8], and\nthe weighted 3D voxel grid [9]. We choose to leverage the\nproposed PG (Section III-B) to produce a sequence of EMSs\nsatisfying placement constraints as the bin’s configuration. The\nsequence is padded or clipped to a fixed length N with dummy\nEMSs, i.e. st,bin = {Ei}N\ni=1.\nAction: Given the packing state st = (st,item, st,bin), the\naction at involves selecting both an orientation and an EMS\nfor the current item from the sequence of available EMSs.\nThe size of the action space A depends solely on the length\nof the sequence and the number of optional orientations,\ni.e., ∥A∥ = 2N, irrespective of the bin dimensions. During\ntraining, we select the action at according to the probability\ndistribution over actions π(· |st), where · represents the set\nof all possible placements in st. During testing, we select the\naction in a deterministic manner by choosing the placement\nwith maximum probability in π(· |st). Note that the proba-\nbility distribution applying the pairwise action mask between\nEMSs and orientations ensures that the policy samples valid\nactions unless no EMS satisfies the constraints.\nState-Transition: In our setting, the transition model is\nassumed to be deterministic, implying that a specific pair\n(st, at) consistently leads to the same subsequent state st+1.\nReward: The target of the packing problem is to maximize\nthe space ratio of the bin. Hence, we formulate the reward as\nthe step-wise enhancement in space utilization, represented as\nrt = lt·wt·ht\nL·W·H . This dense reward encourages the DRL agent\nto perform more steps in an episode, thereby leading to more\npacked items and greater space utilization.\nD. Network Architecture\nThe design of a neural network architecture for the DRL\nagent is important because the chosen architecture affects the\nagent’s learning and generalization capabilities across varied\nenvironments. A simplistic network would be to concatenate\nthe bin and item representations [2] or embeddings [7]. How-\never, this method results in a model whose convolutional and\nlinear layer sizes are contingent upon the dimensions of the\nbin, rendering the trained model impractical for application\nacross different bins.\nTo overcome the challenge of generalization, we propose\nan attention-based network architecture that focuses on the\ncorrelation between the item and the bin’s partial spaces. As\nillustrated in Fig. 2a, this architecture comprises three primary\ncomponents: the Packing Transformer, the actor network, and\nthe critic network. Our network takes the bin representation\nst,bin ∈ RN×6 (i.e., a sequence of EMSs from PG) and the\nitem representation st,item ∈ R2×3 (i.e., item’s dimensions)\nas inputs. These inputs are then individually processed by\nMulti-Layer Perceptrons (MLP), which are two-layer linear\nnetworks with LeakyReLU activation function. The embed-\nding dimensions of both EMS and the item are set to 128.\nSubsequently, we then extract features from the embeddings\nusing the designed Packing Transformer, inspired by cross-\nmodality learning across language and vision [32]. The EMS\nand item features are then fed into the actor network to\ngenerate a probability distribution of potential actions, and\nfed into the critic network to estimate the expected cumulative\nreward based on the current state.\nPacking Transformer is depicted in detail in Fig. 2b.\nIt is constructed by stacking multiple (three in practice)\nidentical encoder blocks, each containing two self-attention\nlayers, one bi-directional cross-attention layer, and four MLP\nblocks of two layers comprising {128, 128} neurons. The bi-\ndirectional cross-attention layer consists of two unidirectional\ncross-attention layers, one from EMS to item and the other\nfrom item to EMS. Residual connections and layer normaliza-\ntion (Norm) are applied after each layer. The self-attention\nXIONG et al.: GOPT: GENERALIZABLE ONLINE 3D BIN PACKING VIA TRANSFORMER-BASED DEEP REINFORCEMENT LEARNING 5\nlayers play an important role in establishing the intrinsic\nconnections between EMSs or item dimensions, while the\nbi-directional cross-attention layer facilitates the discovery of\ninner-relationships from one to another.\nActor and critic networks are both implemented with the\nMLP layers shown in Fig. 2a. In the actor network, both the\nEMS and item features are processed through an MLP, and\nthe results are multiplied to compute a score map of actions.\nThis is followed by an element-wise multiplication with the\naction mask to eliminate infeasible actions.\nE. Training Method\nWe employ the Proximal Policy Optimization (PPO) algo-\nrithm [33] to train the proposed GOPT. PPO is a popular\non-policy reinforcement learning algorithm that alternates be-\ntween collecting data via interactions with the environment and\noptimizing the following objective, which is approximately\nmaximized in each iteration:\nL(θ) =ˆEt[LCLIP (θ) − c1LV F(θ) +c2S(πθ(· |st))] (1)\nwhere θ represents the network parameters, c1, c2 are coeffi-\ncients, LCLIP (θ) is the clipped surrogate objective, LV F(θ)\nis the squared-error loss for the value function, and S denotes\nthe entropy of the policy. Specifically, the surrogate objective\nis defined as:\nLCLIP = ˆEt[min(pt(θ) ˆAt, clip(pt(θ), 1 − ϵ, 1 +ϵ) ˆAt)] (2)\nwhere pt(θ) = πθ(at|st)\nπθold (at|st) is the action probability ratio\nbetween the current policy and the old policy, ˆAt is the\nestimation of the advantage function which we use Generalized\nAdvantage Estimator (GAE) [34] method to compute, and ϵ\nindicates the clipped ratio which is used to limit the volume\nof update and stabilize learning procedure.\nIV. E XPERIMENTS\nA. Implementation Details\nOur method is implemented utilizing PyTorch and adopts\nthe PPO algorithm within the Tianshou framework [35] for\npolicy training. The maximum number of EMS is set to 80\nduring each packing step. We train the policy for 1000 epochs\nand collect a total of 40,000 environment steps over 128\nparallel environments in every epoch. Policy updates occur\nafter every 640 environment steps (calculated as 5×128 steps),\nwith a batch size of 128. The Adam optimizer, coupled with\na linearly descending learning rate scheduler starting from\n7 × 10−5 is utilized for optimization. In terms of PPO loss\ncalculation, the coefficients for value and entropy loss c1,\nc2 are 0.5 and 0.001, respectively, and the clipped ratio ϵ is\n0.3. The discount factor γ is set to 1 to consider future and\nimmediate rewards equally important. For policy updates, we\nuse GAE with λGAE = 0.96. Our policy training is conducted\non a computer equipped with an NVIDIA GeForce RTX 3090\nand an Intel Core i7-14700K CPU, reaching convergence from\nscratch in about six hours.\nFor experimental validation, we utilize the RS dataset [2] for\ntraining and evaluating our DRL agent. The bin dimensions\nTABLE I\nPERFORMANCE COMPARISON ON A 10 × 10 × 10 BIN ALONG\nWITH THE RESULTS OF THE ABLATION STUDIES .\nMethod Uti Sta Num\nHeuristic\nOnlineBPH [5] 51.6% 0.142 20.5\nBest Fit [16] 57.9% 0.124 22.9\nMACS [23] 50.6% 0.171 19.6\nHM [21] 56.5% 0.105 22.1\nDRL-based\nZhao et al. [2] 70.9% 0.079 27.5\nPCT [8] 72.7% 0.073 28.1\nXiong et al. [3] 73.8% 0.068 28.3\nGOPT (ours) 76.1% 0.070 29.6\nAblation studies\nGOPT w/o PG 70.6% 0.086 27.5\nGOPT w/o IR 73.2% 0.078 28.5\nGOPT w/o PT 67.1% 0.085 26.2\nGOPT-MLP 67.8% 0.079 26.4\nGOPT-GRU 68.7% 0.082 26.9\nBold indicates the best and underline indicates the second best for\nthat metric.\n75.6%78.5%70.9% 81.3%\nZhao et al. PCT Xiong et al. GOPT\nOnlineBPH Best Fit MACS HM\n51.3% 57.1% 55.0% 61.9%\nFig. 4. Visualization results of different methods for an item sequence in a\n10 × 10 × 10 bin. The number beside each bin indicates the value of Uti.\nL × W × H are set to 10 × 10 × 10, and the dimensions\nof items follow min(L,W,H)\n10 ≤ lt, wt, ht ≤ min(L,W,H)\n2 .\nThe dataset comprises 125 types of heterogeneous items, and\nsequences are dynamically generated by bootstrap sampling\nduring training to reflect the variability in practical scenarios.\nAn additional set of 1000 instances is generated for evaluation\npurposes, and the average performance is recorded.\nB. Performance Evaluation\n1) Baselines: To illustrate the superiority of our method,\nwe select representative methods with publicly available im-\nplementations as baselines. We categorize these methods into\ntwo groups. The first group consists of four heuristic methods:\nOnlineBPH [5], Best Fit based on EP [16] that packs item\nin the lowest extreme point, MACS [23], and HM [21]. The\nsecond comprises three DRL-based methods: Zhao et al. [2],\nPCT [8], and Xiong et al. [3]. All methods are implemented\nand executed on the same desktop computer to ensure fair and\nrigorous comparisons. Furthermore, the DRL-based methods\nare trained from scratch with an equivalent number of steps,\nspecifically 40 million, to eliminate training disparity bias.\n6 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED SEPTEMBER, 2024\nTABLE II\nGENERALIZATION PERFORMANCE ON BINS OF DIFFERENT DIMENSIONS\nMethod Bin-10 Bin-30 Bin-50 Bin-100\nUti Num Uti Num Uti Num Uti Num\nZhao et al. [2] 70.9% 27.5 72.4% 27.9 51.7% 20.6 / /\nZhao et al. [10] 1 70.1% 27.1 71.7% 27.7 72.6% 28.1 71.3% 27.6\nPCT [8] 72.7% 28.1 73.1% 28.1 70.1% 27.2 72.7% 27.9\nXiong et al. [3] 73.8% 28.3 75.6% 28.9 75.3% 28.8 73.8% 28.2\nGOPT 76.1% 29.6 76.0% 29.5 75.7% 29.4 75.7% 29.4\nGOPT (Bin-10)2 76.1% 29.6 76.0% 29.2 75.8% 29.2 76.3% 29.6\n1Results are copied directly from [10] since the code is not available.\n2GOPT (Bin-10) refers to the GOPT policy trained in Bin-10, which we directly apply to four environments to obtain testing results. In contrast,\nthe other four methods, along with GOPT, require separate training and testing in these environments.\n2) Results: We evaluate the packing performance of these\nmethods using three metrics: average space utilization of\nthe bin ( Uti), average number of packed items ( Num), and\nstandard deviation of space utilization (Sta), the latter of which\nassesses the stability of the methods across all instances.\nQuantitative comparisons, presented in Table I, demonstrate\nthat our method outperforms all baselines in terms of Uti\nand Num. The findings indicate that our method achieves\nsuperior item packing and more efficient utilization of bin\nspace. It is noteworthy that our method achieves the second-\nhighest performance in terms of Sta, with DRL-based methods\nshowing comparable performance in this metric. Moreover, all\nDRL-based methods significantly outshine heuristic methods\nacross all evaluation metrics. This advantage is attributed to\nthe DRL-based method’s ability to extract packing patterns\nand regularities from extensive training samples. In contrast,\nheuristic methods may struggle to generalize beyond their\nspecific rules or strategies. The comparison with the baselines\nindicates our method’s effectiveness. Furthermore, we depict\nthe qualitative comparisons of visualized packing results from\ndifferent methods in Fig. 4. It is observed that our results are\nvisually superior to other competing methods.\nC. Generalization\nThe capacity of learning-based methods to generalize across\ndiverse datasets and unseen scenarios has consistently been\na subject of scrutiny and interest. This section evaluates the\ngeneralization performance of our method across various bins\nof different dimensions and unseen items.\nGeneralization on different bins. In addition to the initial\nbin dimensions for the aforementioned training, we introduce\nthree other environments where the bin dimensions are set to\n30×30×30, 50×50×50, and 100×100×100, respectively,\nand the item dimensions in the dataset are scaled up corre-\nspondingly. These environments are named Bin-10, Bin-30,\nBin-50, and Bin-100. The search space for actions increases as\nthe dimensions of bins grow, resulting in a higher complexity\nfor finding a solution. To assess our method’s generalization\nability regarding the bin dimensions, we directly transfer our\npolicy, trained solely in Bin-10, to the other three environments\nwithout fine-tuning. We additionally train and test our pro-\nposed GOPT, along with several DRL-based baseline methods\n[2], [3], [8], [10], separately in different environments for\ngreater persuasiveness. The results in terms of Uti and Num are\nTABLE III\nPERFORMANCE OF POLICIES TRAINED ON RSsub WHEN EVALUATED ON\nRSsub AND TWO DATASETS CONTAINING UNSEEN ITEMS\nMethod RSsub RS RS exc\nUti Num Uti Num Uti Num\nPCT [8] 73.9% 28.0 73.7% 28.2 73.7% 29.3\nXiong et al. [3] 73.8% 27.9 73.0% 27.8 72.9% 29.0\nGOPT 75.5% 28.7 76.1% 29.5 75.7% 30.2\nsummarized in Table II. It is noted that Zhao et al.’s method\n[2] fails to converge in Bin-100. According to Table II, GOPT\nnot only maintains consistent performance across different\nenvironments but also consistently outperforms other methods.\nSignificantly, the policy GOPT (Bin-10) without retraining\nshows stable performance in environments divergent from the\ntraining one. Other DRL-based methods do not possess such\nability as they need to be retrained when encountering varying\nbin dimensions. Intriguingly, some of them achieve relatively\nhigh performance in Bin-30. We surmise that this is due to\na balance between the increased number of model parameters\nand the moderate problem complexity for this size, allowing\nfor enhanced fitting capacity without the excessive difficulty\nobserved at larger bins.\nGeneralization on unseen items. Additionally, we con-\nduct experiments to assess the generalization performance\nof our method using unseen items in Bin-10. This test is\ncrucial and challenging as models often exhibit diminished\nperformance when confronted with testing data that possess\ndifferent characteristics. As previously mentioned, there are\n125 distinct types of items in the RS dataset. We randomly\nexclude 25 types of items (RS exc) from RS to train an agent\nwith the sub-dataset RS sub and test it with the complete RS\nand RS exc. We select two baselines that performed well in\nprevious experiments for comparison. As shown in Table III,\nour policy trained in the sub-dataset performs better than\nothers when tested on both the full dataset RS and the dataset\nRSexc consisting entirely of unseen items. This suggests the\ntrained policy exhibits adequate generalization ability even on\nunseen items. We also observe an increase in Num across all\nmethods on RS and RS exc, likely due to these datasets having\nmore small, easier-to-pack items.\nXIONG et al.: GOPT: GENERALIZABLE ONLINE 3D BIN PACKING VIA TRANSFORMER-BASED DEEP REINFORCEMENT LEARNING 7\n0 10 M 20 M 30 M 40 M\nTraining Step\n0.45\n0.55\n0.65\n0.75Episode Reward\nGOPT\nGOPT w/o PG\nGOPT w/o IR\nGOPT w/o PT\nGOPT-MLP\nGOPT-GRU\nFig. 5. Comparison of the training performance for the ablation studies. The\nresults are obtained with 128 different random seeds.\nD. Ablation Studies\nAdditional ablation studies are conducted to thoroughly\nanalyze the impact of various components in our method.\nThese components encompass the Placement Generator (PG),\nitem representation (IR), and Packing Transformer (PT). We\nexclude PG and provide the neural network with all the\nplacements and the corresponding masks to elucidate its effect.\nWe also present results obtained without transforming the item\nrepresentation from a three-dimensional vector to the proposed\nmode. Additionally, we conduct experiments by removing PT\n(GOPT w/o PT) and replacing PT with MLP (GOPT-MLP)\nand GRU (GOPT-GRU) to gain insights into its significance.\nThe results are depicted in Table I. We also present reward\ncurves versus training steps in Fig. 5.\nAs shown in Table I and Fig. 5, all three components\nintroduced in this study exhibit favorable outcomes in line\nwith our expectations. The comparative analyses indicate the\nperformance of GOPT w/o PT, GOPT-MLP, and GOPT-GRU\nis significantly degraded compared to GOPT. It highlights the\nadvantageous role of identifying spatial relations through the\nproposed PT in enhancing performance. This capability can be\nattributed to the superior efficacy of the attention mechanism\nin handling intricate sequential data and in learning long-\nrange dependencies compared to other networks. Additionally,\nfrom Fig. 5, the models incorporating PT (GOPT, GOPT w/o\nPG, GOPT w/o IR) require more training data to achieve\nconvergence than the models without PT (GOPT w/o PT,\nGOPT-MLP, GOPT-GRU), approximately 30 million versus\n10 million. Besides, GOPT achieves greater space utilization\nand packs more items than GOPT w/o IR, indicating that the\nproposed item representation facilitates the DRL agent’s learn-\ning and final performance. From Fig. 5, we note that GOPT\nw/o PG attains the least reward during the initial stages of\ntraining. This suggests that the PG module informed by human\nexperience can contribute to improving sampling efficiency\nwhen the DRL agent has yet to accumulate substantial packing\nknowledge.\nWe also investigate the impact of reward design for the\nproblem, encompassing the step-wise reward employed in this\nwork, the terminal reward [31] defined as the final space\nutilization in an episode, and the heuristic reward [9] which\nadds a penalty term to avoid wasted space due to unreasonable\nTABLE IV\nCOMPARISON OF DIFFERENT REWARD FUNCTIONS\nReward designs Uti Num\nStep-wise 76.1% 29.6\nTerminal [31] 70.9% 27.6\nHeuristic [9] 72.4% 28.0\nactions. According to Table IV, the agent trained with the\nterminal reward shows the poorest performance, while the\nstep-wise reward is more efficient despite its simpler and more\nintuitive nature than the heuristic reward.\nE. Real World Experiment\nWe establish a physical robot packing testbed to verify the\napplicability of our method in the real world, as depicted in\nFig. 6a. The dimensions of the bin for packing items are\n56cm × 36.5cm × 21cm, which is discretized into a bin of\n80 × 52 × 30, with each cell measuring 0.7cm in length.\nIn this task, a robot selects a box from a bin, moves it\nwithin the Lucid camera’s field of view to assess the box’s\ndimensions and in-hand pose, and subsequently places it into\nanother bin according to GOPT trained in the simulation.\nMeanwhile, two cameras are mounted to monitor these bins\nseparately. The heightmap of the packing bin is generated\nthrough the segmentation and projection of the point cloud\nand the detection of rectangles. The pick-and-pack process\nproceeds until no boxes remain for picking or there is not\nenough space for packing the next box. Experiments show that\na robot can utilize our method to complete the packing task\nin a real-world scenario. The demonstration video is provided\nin our supplementary materials.\nFrom experiments, we observe that camera-induced mea-\nsurement errors have the potential to cause collisions between\nboxes during placement (see Fig. 6b). To prevent this, an\nadditional 0.7cm buffer space is allocated around each placed\nbox, as shown in Fig. 6c, resulting in an average space\nutilization of 67.5% across 20 tests. Reducing the buffer to\nzero increases the risk of errors and leads to 2 out of 20\ntests failing, but achieves higher utilization ( 73.3% across 18\nsuccessful tests), as shown in Fig. 6d. These findings provide\nan impetus for future research aimed at enhancing both system\nreliability in real-world robotic packing scenarios and the\ncompactness of the packing outcomes.\nV. C ONCLUSIONS\nWe contribute a novel framework called GOPT for online\n3D bin packing. GOPT embraces the Placement Generator\nmodule to generate placement candidates and represent the\nstate of a bin with these candidates. Meanwhile, the Packing\nTransformer identifies spatial correlations for packing, which\nemploys a cross-attention mechanism to fuse information from\nitems and the bin effectively. Extensive experiments prove\nGOPT’s superiority over existing methods, demonstrating no-\ntable enhancements not only in packing performance but also\nin generalization capabilities. Specifically, trained GOPT pol-\nicy can generalize both across varying bins and unseen items.\n8 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED SEPTEMBER, 2024\n(b)\n(c)\n(d)\n(a)\nZivid Mech-Mind\nLucid \nPicking bin\nPacking bin\nFig. 6. The real-world experiments. (a) Our robot packing setup: A KUKA\nrobot is equipped with a suction cup and three 3D cameras; (b) Failure case:\nThe primary sources of failure in our experiments are measurement errors;\n(c) and (d) are snapshots of safe packing and tight packing.\nFinally, we successfully apply the trained packing policy in a\nrobotic system, demonstrating its practical applicability. In the\nfuture, we plan to extend our method’s application to include\npacking objects with irregular shapes, a common challenge in\nrobotic pick-and-place tasks. We also plan to explore how to\nimprove the reliability of the physical robot packing system.\nREFERENCES\n[1] F. Wang and K. Hauser, “Dense robotic packing of irregular and novel\n3d objects,” IEEE Transactions on Robotics , vol. 38, no. 2, pp. 1160–\n1173, 2021.\n[2] H. Zhao, Q. She, C. Zhu, Y . Yang, and K. Xu, “Online 3d bin packing\nwith constrained deep reinforcement learning,” in Proceedings of the\nAAAI Conference on Artificial Intelligence , vol. 35, no. 1, 2021, pp.\n741–749.\n[3] H. Xiong, K. Ding, W. Ding, J. Peng, and J. Xu, “Towards reliable\nrobot packing system based on deep reinforcement learning,” Advanced\nEngineering Informatics, vol. 57, p. 102028, 2023.\n[4] O. X. do Nascimento, T. A. de Queiroz, and L. Junqueira, “Practical\nconstraints in the container loading problem: Comprehensive formula-\ntions and exact algorithm,” Computers & Operations Research, vol. 128,\np. 105186, 2021.\n[5] C. T. Ha, T. T. Nguyen, L. T. Bui, and R. Wang, “An online packing\nheuristic for the three-dimensional container loading problem in dynamic\nenvironments and the physical internet,” in European Conference on the\nApplications of Evolutionary Computation . Springer, 2017, pp. 140–\n155.\n[6] R. Verma, A. Singhal, H. Khadilkar, A. Basumatary, S. Nayak, H. V .\nSingh, S. Kumar, and R. Sinha, “A generalized reinforcement learning\nalgorithm for online 3d bin-packing,” arXiv preprint arXiv:2007.00463,\n2020.\n[7] Z. Yang, S. Yang, S. Song, W. Zhang, R. Song, J. Cheng, and\nY . Li, “Packerbot: Variable-sized product packing with heuristic deep\nreinforcement learning,” in 2021 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS) . IEEE, 2021, pp. 5002–5008.\n[8] H. Zhao, Y . Yu, and K. Xu, “Learning Efficient Online 3D Bin\nPacking on Packing Configuration Trees,” in International Conference\non Learning Representations , 2022.\n[9] S. Yang, S. Song, S. Chu, R. Song, J. Cheng, Y . Li, and W. Zhang,\n“Heuristics integrated deep reinforcement learning for online 3d bin\npacking,” IEEE Transactions on Automation Science and Engineering ,\n2023.\n[10] H. Zhao, C. Zhu, X. Xu, H. Huang, and K. Xu, “Learning practically\nfeasible policies for online 3d bin packing,” Science China Information\nSciences, vol. 65, no. 1, pp. 1–17, 2022.\n[11] S. Ali, A. G. Ramos, M. A. Carravilla, and J. F. Oliveira, “On-line three-\ndimensional packing problems: a review of off-line and on-line solution\napproaches,” Computers & Industrial Engineering , p. 108122, 2022.\n[12] G. D ´osa and J. Sgall, “First fit bin packing: A tight analysis,” in 30th\nInternational Symposium on Theoretical Aspects of Computer Science\n(STACS 2013) . Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik,\n2013.\n[13] ——, “Optimal analysis of best fit bin packing,” in International\nColloquium on Automata, Languages, and Programming . Springer,\n2014, pp. 429–441.\n[14] L. Wang, S. Guo, S. Chen, W. Zhu, and A. Lim, “Two natural heuristics\nfor 3d packing with practical loading constraints,” in Pacific Rim\nInternational Conference on Artificial Intelligence . Springer, 2010,\npp. 256–267.\n[15] S. Martello, D. Pisinger, and D. Vigo, “The three-dimensional bin\npacking problem,” Operations research, vol. 48, no. 2, pp. 256–267,\n2000.\n[16] T. G. Crainic, G. Perboli, and R. Tadei, “Extreme point-based heuristics\nfor three-dimensional bin packing,” Informs Journal on computing ,\nvol. 20, no. 3, pp. 368–384, 2008.\n[17] F. Parre ˜no, R. Alvarez-Vald ´es, J. M. Tamarit, and J. F. Oliveira, “A\nmaximal-space algorithm for the container loading problem,” INFORMS\nJournal on Computing , vol. 20, no. 3, pp. 412–422, 2008.\n[18] M. Agarwal, S. Biswas, C. Sarkar, S. Paul, and H. S. Paul, “Jampacker:\nAn efficient and reliable robotic bin packing system for cuboid objects,”\nIEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 319–326, 2020.\n[19] A. Yarimcam, S. Asta, E. ¨Ozcan, and A. J. Parkes, “Heuristic generation\nvia parameter tuning for online bin packing,” in 2014 IEEE symposium\non evolving and autonomous learning systems (EALS) . IEEE, 2014,\npp. 102–108.\n[20] M. L ´opez-Ib´a˜nez, J. Dubois-Lacoste, L. P. C ´aceres, M. Birattari, and\nT. St ¨utzle, “The irace package: Iterated racing for automatic algorithm\nconfiguration,” Operations Research Perspectives , vol. 3, pp. 43–58,\n2016.\n[21] F. Wang and K. Hauser, “Stable bin packing of non-convex 3d objects\nwith a robot manipulator,” in 2019 International Conference on Robotics\nand Automation (ICRA) . IEEE, 2019, pp. 8698–8704.\n[22] W. Shuai, Y . Gao, P. Wu, G. Cui, Q. Zhuang, R. Chen, and X. Chen,\n“Compliant-based robotic 3d bin packing with unavoidable uncertain-\nties,” IET Control Theory & Applications , 2023.\n[23] R. Hu, J. Xu, B. Chen, M. Gong, H. Zhang, and H. Huang, “TAP-Net:\ntransport-and-pack using reinforcement learning,” ACM Transactions on\nGraphics (TOG), vol. 39, no. 6, pp. 1–15, 2020.\n[24] W. Kool, H. van Hoof, and M. Welling, “Attention, learn to\nsolve routing problems!” in International Conference on Learning\nRepresentations, 2019. [Online]. Available: https://openreview.net/\nforum?id=ByxBFsRqYm\n[25] M. Nazari, A. Oroojlooy, L. Snyder, and M. Tak ´ac, “Reinforcement\nlearning for solving the vehicle routing problem,” Advances in neural\ninformation processing systems , vol. 31, 2018.\n[26] Q. Que, F. Yang, and D. Zhang, “Solving 3d packing problem using\ntransformer network and reinforcement learning,” Expert Systems with\nApplications, vol. 214, p. 119153, 2023.\n[27] O. Kundu, S. Dutta, and S. Kumar, “Deep-pack: A vision-based 2d\nonline bin packing algorithm with deep reinforcement learning,” in 2019\n28th IEEE International Conference on Robot and Human Interactive\nCommunication (RO-MAN). IEEE, 2019, pp. 1–7.\n[28] Y . Wu, E. Mansimov, R. B. Grosse, S. Liao, and J. Ba, “Scalable trust-\nregion method for deep reinforcement learning using kronecker-factored\napproximation,” Advances in neural information processing systems ,\nvol. 30, 2017.\n[29] P. Veli ˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Ben-\ngio, “Graph attention networks,” arXiv preprint arXiv:1710.10903, 2017.\n[30] V . Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\nforcement learning,” in International conference on machine learning .\nPMLR, 2016, pp. 1928–1937.\n[31] J. Xu, M. Gong, H. Zhang, H. Huang, and R. Hu, “Neural packing:\nfrom visual sensing to reinforcement learning,” ACM Transactions on\nGraphics (TOG), vol. 42, no. 6, pp. 1–11, 2023.\n[32] P. Li, J. Gu, J. Kuen, V . I. Morariu, H. Zhao, R. Jain, V . Manjunatha,\nand H. Liu, “Selfdoc: Self-supervised document representation learning,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 5652–5660.\n[33] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[34] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-\ndimensional continuous control using generalized advantage estimation,”\narXiv preprint arXiv:1506.02438 , 2015.\n[35] J. Weng, H. Chen, D. Yan, K. You, A. Duburcq, M. Zhang, Y . Su,\nH. Su, and J. Zhu, “Tianshou: A highly modularized deep reinforcement\nlearning library,” The Journal of Machine Learning Research , vol. 23,\nno. 1, pp. 12 275–12 280, 2022."
}