{
  "title": "IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization",
  "url": "https://openalex.org/W3213207129",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223199574",
      "name": "Koto, Fajri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747998458",
      "name": "Lau, Jey Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750522270",
      "name": "Baldwin, Timothy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3110604908",
    "https://openalex.org/W2973089652",
    "https://openalex.org/W2964078775",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2897573126",
    "https://openalex.org/W2801887493",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W3116295307",
    "https://openalex.org/W2153848201",
    "https://openalex.org/W2757947833",
    "https://openalex.org/W3105601216",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3097707184",
    "https://openalex.org/W3004485777",
    "https://openalex.org/W2792017515",
    "https://openalex.org/W3101601200",
    "https://openalex.org/W3098466758",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W2783617855",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W3099008231",
    "https://openalex.org/W2914507741"
  ],
  "abstract": "We present IndoBERTweet, the first large-scale pretrained model for Indonesian Twitter that is trained by extending a monolingually-trained Indonesian BERT model with additive domain-specific vocabulary. We focus in particular on efficient model adaptation under vocabulary mismatch, and benchmark different ways of initializing the BERT embedding layer for new word types. We find that initializing with the average BERT subword embedding makes pretraining five times faster, and is more effective than proposed methods for vocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based datasets.",
  "full_text": "INDO BERT WEET : A Pretrained Language Model for Indonesian Twitter\nwith Effective Domain-Speciﬁc Vocabulary Initialization\nFajri Koto Jey Han Lau Timothy Baldwin\nSchool of Computing and Information Systems\nThe University of Melbourne\nffajri@student.unimelb.edu.au, jeyhan.lau@gmail.com, tb@ldwin.net\nAbstract\nWe present I NDO BERT WEET , the ﬁrst large-\nscale pretrained model for Indonesian Twitter\nthat is trained by extending a monolingually-\ntrained Indonesian BERT model with additive\ndomain-speciﬁc vocabulary. We focus in par-\nticular on efﬁcient model adaptation under vo-\ncabulary mismatch, and benchmark different\nways of initializing the BERT embedding layer\nfor new word types. We ﬁnd that initializing\nwith the average BERT subword embedding\nmakes pretraining ﬁve times faster, and is more\neffective than proposed methods for vocabu-\nlary adaptation in terms of extrinsic evaluation\nover seven Twitter-based datasets.1\n1 Introduction\nTransformer-based pretrained language models\n(Vaswani et al., 2017; Devlin et al., 2019; Liu et al.,\n2019; Radford et al., 2019) have become the back-\nbone of modern NLP systems, due to their success\nacross various languages and tasks. However, ob-\ntaining high-quality contextualized representations\nfor speciﬁc domains/data sources such as biomedi-\ncal, social media, and legal, remains a challenge.\nPrevious studies (Alsentzer et al., 2019;\nChalkidis et al., 2020; Nguyen et al., 2020) have\nshown that for domain-speciﬁc text, pretraining\nfrom scratch outperforms off-the-shelf BERT. As\nan alternative approach with lower cost, Gururan-\ngan et al. (2020) demonstrated that domain adap-\ntive pretraining (i.e. pretraining the model on target\ndomain text before task ﬁne-tuning) is effective,\nalthough still not as good as training from scratch.\nThe main drawback of domain-adaptive pretrain-\ning is that domain-speciﬁc words that are not in the\npretrained vocabulary are often tokenized poorly.\nFor instance, in BIOBERT (Lee et al., 2019), Im-\nmunoglobulin is tokenized into {I, ##mm, ##uno,\n##g, ##lo, ##bul, ##in}, despite being a common\n1Code and models can be accessed at https://\ngithub.com/indolem/IndoBERTweet\nterm in biology. To tackle this problem, Poerner\net al. (2020); Tai et al. (2020) proposed simple\nmethods to domain-extend the BERT vocabulary:\nPoerner et al. (2020) initialize new vocabulary us-\ning a learned projection from word2vec (Mikolov\net al., 2013), while Tai et al. (2020) use random ini-\ntialization with weight augmentation, substantially\nincreasing the number of model parameters.\nNew vocabulary augmentation has been also con-\nducted for language-adaptive pretraining, mainly\nbased on multilingual BERT ( MBERT). For in-\nstance, Chau et al. (2020) replace 99 “unused”\nWordPiece tokens of MBERT with new com-\nmon tokens in the target language, while Wang\net al. (2020) extend MBERT vocabulary with non-\noverlapping tokens (|VMBERT −Vnew|). These two\napproaches use random initialization for new Word-\nPiece token embeddings.\nIn this paper, we focus on the task of learning\nan Indonesian BERT model for Twitter, and show\nthat initializing domain-speciﬁc vocabulary with\naverage-pooling of BERT subword embeddings is\nmore efﬁcient than pretraining from scratch, and\nmore effective than initializing based on word2vec\nprojections (Poerner et al., 2020). We use IN-\nDOBERT (Koto et al., 2020b), a monolingual\nBERT for Indonesian as the domain-general model\nto develop a pretrained domain-speciﬁc model IN-\nDOBERT WEET for Indonesian Twitter.\nThere are two primary reasons to experiment\nwith Indonesian Twitter. First, despite being the of-\nﬁcial language of the 5th most populous nation, In-\ndonesian is underrepresented in NLP (notwithstand-\ning recent Indonesian benchmarks and datasets\n(Wilie et al., 2020; Koto et al., 2020a,b)). Sec-\nond, with a large user base, Twitter is often utilized\nto support policymakers, business (Fiarni et al.,\n2016), or to monitor elections (Suciati et al., 2019)\nor health issues (Prastyo et al., 2020). Note that\nmost previous studies that target Indonesian Twit-\nter tend to use traditional machine learning models\narXiv:2109.04607v1  [cs.CL]  10 Sep 2021\n(e.g. n-gram and recurrent models (Fiarni et al.,\n2016; Koto and Rahmaningtyas, 2017)).\nTo summarize our contributions: (1) we release\nINDO BERT WEET , the ﬁrst large-scale pretrained\nIndonesian language model for social media data;\nand (2) through extensive experimentation, we\ncompare a range of approaches to domain-speciﬁc\nvocabulary initialization over a domain-general\nBERT model, and ﬁnd that a simple average of sub-\nword embeddings is more effective than previously-\nproposed methods and reduces the overhead for\ndomain-adaptive pretraining by 80%.\n2 I NDO BERT WEET\n2.1 Twitter Dataset\nWe crawl Indonesian tweets over a 1-year period\nusing the ofﬁcial Twitter API, 2 from December\n2019 to December 2020, with 60 keywords cover-\ning 4 main topics: economy, health, education, and\ngovernment. We found that the Twitter language\nidentiﬁer is reasonably accurate for Indonesian, and\nso use it to ﬁlter out non-Indonesian tweets. From\n100 randomly-sampled tweets, we found a majority\nof them (87) to be Indonesian, with a small number\nbeing Malay (12) and Swahili (1).3\nAfter removing redundant tweets (with the same\nID), we obtain 26M tweets with 409M word tokens,\ntwo times larger than the training data used to pre-\ntrain INDO BERT (Koto et al., 2020b). We set aside\n230K tweets for development, and extract a vocabu-\nlary of 31,984 types based on WordPiece (Wu et al.,\n2016). We lower-case all words and follow the\nsame preprocessing steps as English BERT WEET\n(Nguyen et al., 2020): (1) converting user men-\ntions and URLs into @USER and HTTPURL, re-\nspectively; and (2) translating emoticons into text\nusing the emoji package.4\n2.2 I NDO BERT WEET Model\nINDO BERT WEET is trained based on a masked\nlanguage model objective (Devlin et al.,\n2019) following the same procedure as the\nindobert-base-uncased (INDO BERT) model.5\nIt is a transformer encoder with 12 hidden layers\n(dimension=768), 12 attention heads, and 3\n2https://developer.twitter.com/\n3Note that Indonesian and Malay are very closely related,\nbut also that we implicitly evaluate the impact of the language\nconﬂuence in our experiments over (pure) Indonesian datasets.\n4https://pypi.org/project/emoji/\n5https://huggingface.co/indolem/\nindobert-base-uncased\nfeed-forward hidden layers (dimension =3,072).\nThe only difference is the maximum sequence\nlength, which we set to 128 tokens based on the\naverage number of words per document in our\nTwitter corpus.\nIn this work, we train 5 INDO BERT WEET mod-\nels. The ﬁrst model is pretrained from scratch\nbased on the aforementioned conﬁguration. The re-\nmaining four models are based on domain-adaptive\npretraining with different vocabulary adaptation\nstrategies, as discussed in Section 2.3.\n2.3 Domain-Adaptive Pretraining with\nDomain-Speciﬁc Vocabulary\nInitialization\nWe apply domain-adaptive pretraining on the\ndomain-general INDO BERT (Koto et al., 2020b),\nwhich is trained over Indonesian Wikipedia, news\narticles, and an Indonesian web corpus (Medved\nand Suchomel, 2017). Our goal is to fully re-\nplace INDO BERT’s vocabulary (VIB) of 31,923\ntypes with INDO BERT WEET ’s vocabulary (VIBT)\n(31,984 types). In INDO BERT WEET , there are\n14,584 (46%) new types, and 17,400 (54%) Word-\nPiece types which are shared with INDO BERT.6\nTo initialize the domain-speciﬁc vocabulary, we\nuse INDO BERT embeddings for the 17,400 shared\ntypes, and explore four initialization strategies for\nnew word types: (1) random initialization from\nU(−1,1); (2) random initialization from N(µ,σ),\nwhere µand σare learned from INDO BERT em-\nbeddings; (3) linear projection via fastText em-\nbeddings (Poerner et al., 2020); and (4) averaging\nINDO BERT subword embeddings.\nFor the linear projection strategy (Method 3), we\ntrain 300d fastText embeddings (Bojanowski\net al., 2017) over the tokenized Indonesian Twit-\nter corpus. Following Poerner et al. (2020), we\nuse the shared types (VIB ∩VIBT) to train a linear\ntransformation from fastText embeddings EFT\nto INDO BERT embeddings EIB as follows:\nargmin\nW\n∑\nx∈VIB∩VIBT\n∥EFT(x) W −EIB(x)∥2\n2\nwhere W is a dim(EFT) ×dim(EIB) matrix.\nTo average subword embeddings of x ∈VIBT\n6In the implementation, we set the adaptive vocabulary\nto be the same size with INDO BERT by discarding some\n“[unused-x]” tokens of INDO BERT WEET .\nTask Data #labels #train #dev #test 5-Fold Evaluation\nSentiment Analysis IndoLEM (Koto et al., 2020b) 2 3,638 399 1,011 Yes F1 pos\nSmSA (Wilie et al., 2020) 3 11,000 1,260 500 No F1 macro\nEmotion Classiﬁcation EmoT (Wilie et al., 2020) 5 3,521 440 442 No F1 macro\nHate Speech Detection HS1 (Alﬁna et al., 2017) 2 499 72 142 Yes F1 pos\nHS2 (Ibrohim and Budi, 2019) 2 9,219 2,633 1,317 Yes F1 pos\nNamed Entity RecognitionFormal (Munarko et al., 2018) 3 6,500 657 1,122 No F1 entity\nInformal (Munarko et al., 2018) 3 6,500 657 1,227 No F1 entity\nTable 1: Summary of Indonesian Twitter datasets used in our experiments.\n(Method 4), we compute:\nEIBT(x) = 1\n|TIB(x)|\n∑\ny∈TIB(x)\nEIB(y)\nwhere TIB(x) is the set of WordPiece tokens for\nword xproduced by INDO BERT’s tokenizer.\n3 Experimental Setup\nWe accumulate gradients over 4 steps to simulate a\nbatch size of 2048. When pretraining from scratch,\nwe train the model for 1M steps, and use a learning\nrate of 1e−4 and the Adam optimizer with a linear\nscheduler. All pretraining experiments are done\nusing 4×V100 GPUs (32GB).\nFor domain-adaptive pretraining (using IN-\nDOBERT model), we consider three benchmarks:\n(1) domain-adaptive pretraining without domain-\nspeciﬁc vocabulary adaptation ( VIBT = VIB) for\n200K steps; (2) applying the new vocabulary adap-\ntation approaches from Section 2.3 without addi-\ntional domain-adaptive pretraining; and (3) apply-\ning the new vocabulary adaptation approaches from\nSection 2.3 with 200K domain-adaptive pretraining\nsteps.\nDownstream tasks. To evaluate the pretrained\nmodels, we use 7 Indonesian Twitter datasets, as\nsummarized in Table 1. This includes sentiment\nanalysis (Koto and Rahmaningtyas, 2017; Purwari-\nanti and Crisdayanti, 2019), emotion classiﬁcation\n(Saputri et al., 2018), hate speech detection (Al-\nﬁna et al., 2017; Ibrohim and Budi, 2019), and\nnamed entity recognition (Munarko et al., 2018).\nFor emotion classiﬁcation, the classes are fear,\nangry, sad, happy, and love. Named en-\ntity recognition (NER) is based on the PERSON,\nORGANIZATION, and LOCATION tags. NER has\ntwo test set partitions, where the ﬁrst is formal texts\n(e.g. news snippets on Twitter) and the second is\ninformal texts. The train and dev partitions are a\nmixture of formal and informal tweets, and shared\nacross the two test sets.\nFine-tuning. For sentiment, emotion, and hate\nspeech classiﬁcation, we add an MLP layer that\ntakes the average pooled output of INDO BER-\nTWEET as input, while for NER we use the ﬁrst\nsubword of each word token for tag prediction. We\npre-process the tweets as described in Section 2.1,\nand use a batch size of 30, maximum token length\nof 128, learning rate of5e−5, Adam optimizer with\nepsilon of 1e−8, and early stopping with patience\nof 5. We additionally introduce a canonical split\nfor both hate speech detection tasks with 5-fold\ncross validation, following Koto et al. (2020b). In\nTable 1, SmSA, EmoT, and NER use the original\nheld-out evaluation splits.\nBaselines. We use the two INDO BERT models\nfrom Koto et al. (2020b) and Wilie et al. (2020)\nas baselines, in addition to multilingual BERT\n(MBERT, which includes Indonesian) and a mono-\nlingual BERT for Malay (MALAY BERT).7 Our ra-\ntionale for including MALAY BERT is that we are\ninterested in testing its performance on Indonesian,\ngiven that the two languages are closely related\nand we know that the Twitter training data includes\nsome amount of Malay text.\n4 Experimental Results\nTable 2 shows the full results across the differ-\nent pretrained models for the 7 Indonesian Twit-\nter datasets. Note that the ﬁrst four models are\npretrained models without domain-adaptive pre-\ntraining (i.e. they are used as purely off-the-shelf\nmodels). In terms of baselines, MALAY BERT is\na better model for Indonesian than MBERT, con-\nsistent with Koto et al. (2020b), and better again\nare the two different INDO BERT models at al-\n7https://huggingface.co/huseinzol05/\nbert-base-bahasa-cased\nModel Sentiment Emotion Hate Speech NER Average\nIndoLEM SmSA EmoT HS1 HS2 Formal Informal\nMBERT 76.6 84.7 67.5 85.1 75.1 85.2 83.2 79.6\nMALAYBERT 82.0 84.1 74.2 85.0 81.9 81.9 81.3 81.5\nINDOBERT (Wilie et al., 2020) 84.1 88.7 73.3 86.8 80.4 86.3 84.3 83.4\nINDOBERT (Koto et al., 2020b) 84.1 87.9 71.0 86.4 79.3 88.0 86.9 83.4\nINDOBERTWEET(1M steps) 86.2 90.4 76.0 88.8 87.5 88.1 85.4 86.1\nINDOBERT (Koto et al., 2020b)+ 200K steps of domain-adaptive pretraining\nSame vocabulary (VIBT =VIB) 86.4 92.7 76.8 88.7 82.2 87.9 86.9 85.9\nINDOBERT (Koto et al., 2020b)+ vocabulary adaptation + 0 steps of domain-adaptive pretraining\nUniform distribution 82.9 84.6 73.2 84.9 78.2 84.3 84.4 81.8\nNormal distribution 83.5 86.7 71.1 85.2 77.4 85.0 86.3 82.2\nfastTextprojection 84.4 83.6 72.2 85.5 80.9 85.4 85.6 82.5\nAverage of subwords 84.2 88.1 71.6 86.2 78.3 86.4 87.4 83.2\nINDOBERT (Koto et al., 2020b)+ vocabulary adaptation + 200K steps of domain-adaptive pretraining\nUniform distribution 85.6 90.9 75.7 88.4 83.0 87.7 85.9 85.3\nNormal distribution 87.1 92.5 75.4 88.8 82.5 88.7 86.6 85.9\nfastTextprojection 86.4 89.7 78.5 88.7 84.4 88.0 86.6 86.0\nAverage of subwords 86.6 92.7 79.0 88.4 84.0 87.7 86.9 86.5\nTable 2: A comparison of pretrained models with different adaptive pretraining strategies for Indonesian tweets\n(%).\nmost identical performance. 8 INDO BERT WEET\n— trained from scratch for 1M steps — results in\na substantial improvement in terms of average per-\nformance (almost +3% absolute), consistent with\nprevious ﬁndings that off-the-shelf domain-general\npretrained models are sub-optimal for domain-\nspeciﬁc tasks (Alsentzer et al., 2019; Chalkidis\net al., 2020; Nguyen et al., 2020).\nFirst, we pretrain INDO BERT (Koto et al.,\n2020b) without vocabulary adaptation for 200K\nsteps, and ﬁnd that the results are slightly lower\nthan INDO BERT WEET . In the next set of exper-\niments, we take INDO BERT (Koto et al., 2020b)\nand replace the domain-general vocabulary with the\ndomain-speciﬁc vocabulary of INDO BERT WEET ,\nwithout any pretraining (“0 steps”). Results drop\noverall relative to the original model, with the\nembedding averaging method (“Average of Sub-\nwords”) yielding the smallest overall gap of−0.2%\nabsolute.\nFinally, we pretrain INDO BERT (Koto et al.,\n2020b) for 200K steps in the target domain, after\nperforming vocabulary adaptation. We see a strong\nimprovement for all initialization methods, with\nthe embedding averaging method once again per-\n8Noting that Wilie et al. (2020)’s version includes 100M\nwords of tweets for pretraining, but Koto et al. (2020b)’s\nversion does not.\nforming the best, in fact outperforming the domain-\nspeciﬁc INDO BERT WEET when trained for 1M\nsteps from scratch. These ﬁndings reveal that we\ncan adapt an off-the-shelf pretrained model very ef-\nﬁciently (5 times faster than training from scratch)\nwith better average performance.\n5 Discussion\nGiven these positive results on Indonesian, we con-\nducted a similar experiment in a second language,\nEnglish: we follow Nguyen et al. (2020) in adapt-\ning ROBERTA9 for Twitter using the embedding\naveraging method to initialize new vocabulary, and\ncompare ourselves against BERT WEET (trained\nfrom scratch on 845M English tweets).\nA caveat here is that BERT WEET (Nguyen et al.,\n2020) and ROBERTA (Liu et al., 2019) use differ-\nent tokenization methods: byte-level BPE vs.\nfastBPE (Sennrich et al., 2016). Because of this,\nrather than replacing ROBERTA’s vocabulary with\nBERT WEET ’s (like our Indonesian experiments),\nwe train ROBERTA’s BPE tokenizer on English\nTwitter data (described below) to create a domain-\nspeciﬁc vocabulary. This means that the two mod-\nels (BERT WEET and domain-adapted ROBERTA\n9The base version.\nModel Average\nROBERTA 72.9\nBERTWEET (Nguyen et al., 2020) 76.3\nROBERTA +vocabulary adaptation +\n200K steps of domain-adaptive pretraining74.1\nTable 3: English results (%). The presented perfor-\nmance is averaged over 7 downstream tasks (Nguyen\net al., 2020). Refer to the Appendix for details.\nwith modiﬁed vocabulary) will not be directly com-\nparable.\nFollowing Nguyen et al. (2020), we download\n42M tweets from the Internet Archive10 over the pe-\nriod July 2017 to October 2019 (the ﬁrst two days\nof each month), which we use for domain-adaptive\npretraining. Note that this pretraining data is an or-\nder of magnitude smaller than that of BERT WEET\n(42M vs. 845M). We use SpaCy11 to ﬁlter English\ntweets, and follow the same preprocessing steps\nand downstream tasks as Nguyen et al. (2020) (7\ntasks in total; see the Appendix for details). We\npretrain ROBERTA for 200K steps using the em-\nbedding averaging method.\nIn Table 3, we see that BERT WEET outper-\nforms ROBERTA (+3.4% absolute). With domain-\nadaptive pretraining using domain-speciﬁc vocabu-\nlary, the performance gap narrows to +2.2%, but\nare not as impressive as our Indonesian experi-\nments. There are two reasons for this: (1) our\ndomain-adaptive pretraining data is an order of\nmagnitude smaller than for BERT WEET ; and (2)\nthe difference in tokenization methods between\nBERT WEET and ROBERTA results in a very dif-\nferent vocabulary.\nLastly, we argue that the different tokeniza-\ntion settings between INDO BERT WEET and\nBERT WEET (ours) may also contribute to the dif-\nference in results. The differences include: (1)\nuncased vs. cased; (2) WordPiece vs. fastBPE\ntokenizer; and (3) vocabulary size (32K vs. 50K)\nbetween both models. In Figure 1, we present\nthe frequency distribution of #subword of new\ntypes in both models after tokenizing by each\ngeneral-domain tokenizer. Interestingly, we ﬁnd\nthat BERT WEET has more new types than IN-\nDOBERT WEET , with #subword after tokenization\nbeing more varied (average length of #subword of\n10https://archive.org/details/\ntwitterstream\n11https://spacy.io/\n#subword of a new type (vocabulary)\n0\n2500\n5000\n7500\n10000\n12500\n1 2 3 4 5 6 7 8 9 10 >10\nBERTweet (ours) IndoBERTweet\nFigure 1: Frequency of #subword of new types in\nBERT WEET (ours) and I NDO BERT WEET , tokenized\nby R OBERTA and I NDO BERT tokenizers, respec-\ntively. #subword = 1 means the new type is tokenized\nas “[UNK]”.\nnew types are 2.6 and 3.4 for INDO BERT WEET\nand BERT WEET , respectively).\n6 Conclusion\nWe present the ﬁrst large-scale pretrained model for\nIndonesian Twitter. We explored domain-adaptive\npretraining with domain-speciﬁc vocabulary adap-\ntation using several strategies, and found that the\nbest method — averaging of subword embeddings\nfrom the original model — achieved the best aver-\nage performance across 7 tasks, and is ﬁve times\nfaster than the dominant paradigm of pretraining\nfrom scratch.\nAcknowledgements\nWe are grateful to the anonymous reviewers for\ntheir helpful feedback and suggestions. The ﬁrst\nauthor is supported by the Australia Awards Schol-\narship (AAS), funded by the Department of Foreign\nAffairs and Trade (DFAT), Australia. This research\nwas undertaken using the LIEF HPC-GPGPU Fa-\ncility hosted at The University of Melbourne. This\nfacility was established with the assistance of LIEF\nGrant LE170100200.\nReferences\nIka Alﬁna, Rio Mulia, Mohamad Ivan Fanany, and\nYudo Ekanata. 2017. Hate speech detection in the\nindonesian language: A dataset and preliminary\nstudy. In 2017 International Conference on Ad-\nvanced Computer Science and Information Systems\n(ICACSIS), pages 233–238.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Asso-\nciation for Computational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904, Online. Association for Computational Lin-\nguistics.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual BERT, a small corpus, and\na small treebank. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1324–1334, Online. Association for Computational\nLinguistics.\nLeon Derczynski, Eric Nichols, Marieke van Erp, and\nNut Limsopatham. 2017. Results of the WNUT2017\nshared task on novel and emerging entity recogni-\ntion. In Proceedings of the 3rd Workshop on Noisy\nUser-generated Text, pages 140–147, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nCut Fiarni, Herastia Maharani, and Rino Pratama. 2016.\nSentiment analysis system for Indonesia online retail\nshop review using hierarchy naive Bayes technique.\nIn 2016 4th international conference on information\nand communication technology (ICoICT) , pages 1–\n6. IEEE.\nKevin Gimpel, Nathan Schneider, Brendan O’Connor,\nDipanjan Das, Daniel Mills, Jacob Eisenstein,\nMichael Heilman, Dani Yogatama, Jeffrey Flanigan,\nand Noah A. Smith. 2011. Part-of-speech tagging\nfor Twitter: Annotation, features, and experiments.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 42–47, Portland, Ore-\ngon, USA. Association for Computational Linguis-\ntics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nMuhammad Okky Ibrohim and Indra Budi. 2019.\nMulti-label hate speech and abusive language de-\ntection in Indonesian Twitter. In Proceedings of\nthe Third Workshop on Abusive Language Online ,\npages 46–57, Florence, Italy. Association for Com-\nputational Linguistics.\nFajri Koto, Jey Han Lau, and Timothy Baldwin. 2020a.\nLiputan6: A large-scale Indonesian dataset for text\nsummarization. In Proceedings of the 1st Confer-\nence of the Asia-Paciﬁc Chapter of the Association\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 598–608, Suzhou, China. Associa-\ntion for Computational Linguistics.\nFajri Koto, Afshin Rahimi, Jey Han Lau, and Timo-\nthy Baldwin. 2020b. IndoLEM and IndoBERT: A\nbenchmark dataset and pre-trained language model\nfor Indonesian NLP. In Proceedings of the 28th In-\nternational Conference on Computational Linguis-\ntics, pages 757–770, Barcelona, Spain (Online). In-\nternational Committee on Computational Linguis-\ntics.\nFajri Koto and Gemala Y Rahmaningtyas. 2017. InSet\nlexicon: Evaluation of a word list for Indonesian sen-\ntiment analysis in microblogs. In 2017 International\nConference on Asian Language Processing (IALP) ,\npages 391–394. IEEE.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nYijia Liu, Yi Zhu, Wanxiang Che, Bing Qin, Nathan\nSchneider, and Noah A. Smith. 2018. Parsing tweets\ninto Universal Dependencies. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 965–975, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nMarek Medved and Vít Suchomel. 2017. Indonesian\nweb corpus (idWac). In LINDAT/CLARIN digital li-\nbrary at the Institute of Formal and Applied Linguis-\ntics (ÚFAL), Faculty of Mathematics and Physics,\nCharles University.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nYuda Munarko, MS Sutrisno, W AI Mahardika, Ilyas\nNuryasin, and Yuﬁs Azhar. 2018. Named entity\nrecognition model for Indonesian tweet using CRF\nclassiﬁer. In IOP Conference Series: Materials Sci-\nence and Engineering, volume 403. IOP Publishing.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020. BERTweet: A pre-trained language model\nfor English tweets. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages 9–\n14, Online. Association for Computational Linguis-\ntics.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. Inexpensive domain adaptation of pretrained\nlanguage models: Case studies on biomedical NER\nand covid-19 QA. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1482–1490, Online. Association for Computational\nLinguistics.\nPulung Hendro Prastyo, Amin Siddiq Sumi, Ade Widy-\natama Dian, and Adhistya Erna Permanasari. 2020.\nTweets responding to the Indonesian government’s\nhandling of COVID-19: Sentiment analysis using\nSVM with normalized poly kernel. Journal of In-\nformation Systems Engineering and Business Intelli-\ngence, 6(2):112–122.\nAyu Purwarianti and Ida Ayu Putu Ari Crisdayanti.\n2019. Improving Bi-LSTM performance for In-\ndonesian sentiment analysis using paragraph vec-\ntor. In 2019 International Conference of Advanced\nInformatics: Concepts, Theory and Applications\n(ICAICTA), pages 1–5. IEEE.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAlan Ritter, Sam Clark, Mausam, and Oren Etzioni.\n2011. Named entity recognition in tweets: An ex-\nperimental study. In Proceedings of the 2011 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1524–1534, Edinburgh, Scotland,\nUK. Association for Computational Linguistics.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 task 4: Sentiment analysis in Twit-\nter. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017) ,\npages 502–518, Vancouver, Canada. Association for\nComputational Linguistics.\nMei Silviana Saputri, Rahmad Mahendra, and Mirna\nAdriani. 2018. Emotion classiﬁcation on Indonesian\nTwitter dataset. In 2018 International Conference\non Asian Language Processing (IALP), pages 90–95.\nIEEE.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nBenjamin Strauss, Bethany Toma, Alan Ritter, Marie-\nCatherine de Marneffe, and Wei Xu. 2016. Results\nof the WNUT16 named entity recognition shared\ntask. In Proceedings of the 2nd Workshop on Noisy\nUser-generated Text (WNUT) , pages 138–144, Os-\naka, Japan. The COLING 2016 Organizing Commit-\ntee.\nAndi Suciati, Ari Wibisono, and Petrus Mursanto.\n2019. Twitter buzzer detection for Indonesian pres-\nidential election. In 2019 3rd International Con-\nference on Informatics and Computational Sciences\n(ICICoS), pages 1–5. IEEE.\nWen Tai, H. T. Kung, Xin Dong, Marcus Comiter,\nand Chang-Fu Kuo. 2020. exBERT: Extending\npre-trained models with domain-speciﬁc vocabulary\nunder constrained training resources. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020 , pages 1433–1439, Online. Associa-\ntion for Computational Linguistics.\nCynthia Van Hee, Els Lefever, and Véronique Hoste.\n2018. SemEval-2018 task 3: Irony detection in En-\nglish tweets. In Proceedings of The 12th Interna-\ntional Workshop on Semantic Evaluation, pages 39–\n50, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, volume 30, pages 5998–6008.\nZihan Wang, Karthikeyan K, Stephen Mayhew, and\nDan Roth. 2020. Extending multilingual BERT to\nlow-resource languages. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 ,\npages 2649–2656, Online. Association for Computa-\ntional Linguistics.\nBryan Wilie, Karissa Vincentio, Genta Indra Winata,\nSamuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim,\nSidik Soleman, Rahmad Mahendra, Pascale Fung,\nSyafri Bahar, and Ayu Purwarianti. 2020. IndoNLU:\nBenchmark and resources for evaluating Indonesian\nnatural language understanding. In Proceedings of\nthe 1st Conference of the Asia-Paciﬁc Chapter of the\nAssociation for Computational Linguistics and the\n10th International Joint Conference on Natural Lan-\nguage Processing, pages 843–857, Suzhou, China.\nAssociation for Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin\nJohnson, Xiaobing Liu, Łukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto\nKazawa, Keith Stevens, George Kurian, Nishant\nPatil, Wei Wang, Cliff Young, Jason Smith, Jason\nRiesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2016. Google’s\nneural machine translation system: Bridging the\ngap between human and machine translation. arXiv\npreprint arXiv:1609.08144.\nA Results with English BERT WEET\nModel POS Tagging SemEval2017 SemEval2018 NER Avg.\nRitter11 ARK TB-v2 Sentiment an. Irony det. WNUT2016 WNUT2017\nROBERTA 88.6 91.0 93.4 70.8 71.6 45.5 49.9 72.9\nBERTWEET 90.5 93.2 94.8 72.6 75.5 51.9 55.9 76.3\nsteps = 0\nROBERTA w/\nBERTWEET\ntokenizer\n87.7 90.2 92.5 65.7 66.1 39.2 39.2 68.7\nROBERTA w/ new\ntokenizer\n87.4 89.6 92.8 66.5 70.7 42.5 42.4 70.3\nsteps = 200K\nROBERTA w/ new\ntokenizer\n90.1 90.9 94.9 72.1 73.3 47.2 50.0 74.1\nTable 4: English Results (%) over the test sets. All data, metrics, and splits are based off the experiments of Nguyen\net al. (2020). We re-ran all experiments and found slightly lower performance for some models as compared to\nBERT WEET . For evaluation, the POS tagging datasets (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2018)\nuse accuracy, SemEval2017 (Rosenthal et al., 2017) uses AvgRec, SemEval2018 (Van Hee et al., 2018) uses F1pos,\nand NER (Strauss et al., 2016; Derczynski et al., 2017) uses F1entity.",
  "topic": "Initialization",
  "concepts": [
    {
      "name": "Initialization",
      "score": 0.8882710933685303
    },
    {
      "name": "Vocabulary",
      "score": 0.8229086399078369
    },
    {
      "name": "Computer science",
      "score": 0.783893346786499
    },
    {
      "name": "Indonesian",
      "score": 0.767733097076416
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6527363657951355
    },
    {
      "name": "Natural language processing",
      "score": 0.605185329914093
    },
    {
      "name": "Focus (optics)",
      "score": 0.5893265604972839
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5860400795936584
    },
    {
      "name": "Domain adaptation",
      "score": 0.5723665952682495
    },
    {
      "name": "Language model",
      "score": 0.5245126485824585
    },
    {
      "name": "Embedding",
      "score": 0.517681360244751
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.49528446793556213
    },
    {
      "name": "Word (group theory)",
      "score": 0.49285459518432617
    },
    {
      "name": "Layer (electronics)",
      "score": 0.47698384523391724
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.46437588334083557
    },
    {
      "name": "Speech recognition",
      "score": 0.4260634183883667
    },
    {
      "name": "Linguistics",
      "score": 0.22972333431243896
    },
    {
      "name": "Psychology",
      "score": 0.09782996773719788
    },
    {
      "name": "Mathematics",
      "score": 0.07202470302581787
    },
    {
      "name": "Geography",
      "score": 0.054655998945236206
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}