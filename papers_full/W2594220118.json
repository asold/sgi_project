{
  "title": "Context-dependent factored language models",
  "url": "https://openalex.org/W2594220118",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A5088522711",
      "name": "Gregor Donaj",
      "affiliations": [
        "University of Maribor"
      ]
    },
    {
      "id": "https://openalex.org/A5072645144",
      "name": "Zdravko Kačič",
      "affiliations": [
        "University of Maribor"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2079885661",
    "https://openalex.org/W2090750282",
    "https://openalex.org/W2103589071",
    "https://openalex.org/W2130327745",
    "https://openalex.org/W2056250865",
    "https://openalex.org/W2067212632",
    "https://openalex.org/W2543953525",
    "https://openalex.org/W33263262",
    "https://openalex.org/W2155033295",
    "https://openalex.org/W1647108384",
    "https://openalex.org/W2156836370",
    "https://openalex.org/W2142937603",
    "https://openalex.org/W2182508908",
    "https://openalex.org/W2094655846",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2080213370",
    "https://openalex.org/W35767841",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W76063220",
    "https://openalex.org/W165885000",
    "https://openalex.org/W2130450156"
  ],
  "abstract": "The incorporation of grammatical information into speech recognition systems is often used to increase performance in morphologically rich languages. However, this introduces demands for sufficiently large training corpora and proper methods of using the additional information. In this paper, we present a method for building factored language models that use data obtained by morphosyntactic tagging. The models use only relevant factors that help to increase performance and ignore data from other factors, thus also reducing the need for large morphosyntactically tagged training corpora. Which data is relevant is determined at run-time, based on the current text segment being estimated, i.e., the context. We show that using a context-dependent model in a two-pass recognition algorithm, the overall speech recognition accuracy in a Broadcast News application improved by 1.73% relatively, while simpler models using the same data achieved only 0.07% improvement. We also present a more detailed error analysis based on lexical features, comparing first-pass and second-pass results.",
  "full_text": "DonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusic\nProcessing  (2017) 2017:6 \nDOI10.1186/s13636-017-0104-6\nRESEARCH OpenAccess\nContext-dependentfactoredlanguage\nmodels\nGregorDonaj* andZdravkoKa ˇciˇc\nAbstract\nTheincorporationofgrammaticalinformationintospeechrecognitionsystemsisoftenusedtoincreaseperformance\ninmorphologicallyrichlanguages.However,thisintroducesdemandsforsufficientlylargetrainingcorporaand\npropermethodsofusingtheadditionalinformation.Inthispaper,wepresentamethodforbuildingfactored\nlanguagemodelsthatusedataobtainedbymorphosyntactictagging.Themodelsuseonlyrelevantfactorsthathelp\ntoincreaseperformanceandignoredatafromotherfactors,thusalsoreducingtheneedforlarge\nmorphosyntacticallytaggedtrainingcorpora.Whichdataisrelevantisdeterminedatrun-time,basedonthecurrent\ntextsegmentbeingestimated,i.e.,thecontext.Weshowthatusingacontext-dependentmodelinatwo-pass\nrecognitionalgorithm,theoverallspeechrecognitionaccuracyinaBroadcastNewsapplicationimprovedby1.73%\nrelatively,whilesimplermodelsusingthesamedataachievedonly0.07%improvement.Wealsopresentamore\ndetailederroranalysisbasedonlexicalfeatures,comparingfirst-passandsecond-passresults.\nKeywords: Speechrecognition,Factoredlanguagemodel,Dynamicbackoffpath,Wordcontext,Inflectional\nlanguage,Morphosyntactictags\n1 Introduction\nSpeech recognition still performs poorly in inflec-\ntional languages compared to mainstream languages like\nEnglish.Thecausecanbefoundintherichmorphologyof\nsuchlanguages,whichincreasestheneedforlargervocab-\nularies. It is estimated that inflectional languages need up\ntotentimeslargervocabulariesthanEnglish[1,2].\nAlthough we can easily build larger vocabularies, thus\nreducingthenumberofrecognitionerrorscausedbyout-\nof-vocabularywords,theuseofsuchmodelsincreasesnot\nonlythesizeoflanguagemodelsbutalsotherequiredsize\nof training corpora. With larger vocabularies, words are\nalso substituted more easily, especially if they are acous-\ntically similar. Many errors occur only in the grammatical\nsense, when a recognition error results in a word with the\nsamelemmabutwithagrammaticalerror,e.g.,afalsecase\nornumber.\nAn often used approach in speech recognition of mor-\nphologically rich languages is the incorporation of gram-\nmatical information in some form. One practical imple-\nmentationtoachievethisistheuseofafactoredlanguage\n*Correspondence:gregor.donaj@um.si\nFacultyofElectricalEngineeringandComputerScience,UniversityofMaribor,\nSmetanovaul.17,SI-2000,Maribor,Slovenia\nmodel(FLM).Suchmodelswerefirstproposedforspeech\nrecognition in Arabic languages [3], but they have also\nbeen adopted in statistical machine translation [4] and,\nmorerecently,innaturallanguagegeneration[5].\nIn an FLM, each word is represented as a vector of fac-\ntors containing information about the word. One of the\nfactors is usually the word itself; other factors can be the\nlemma, the morphosyntactic description (MSD) tag, the\nword stem, the ending, the root, etc. While traditional\n(word-based) n-gram language models use only the basic\nword forms to estimate the probabilities of sentences,\nFLMs can use all defined factors and their information.\nSince these factors have fewer possible values (smaller\nvocabularies), we can build simple models on individual\nfactorsusingsmallertrainingcorpora.\nTheideaofFLMs,however,isto usedifferentfactors in\nthe same models. As each word can have several factors,\nsuch an FLM will have many more terms in its condi-\ntional probability than traditionaln-gram models or an\nFLM using only one factor. The potentially large num-\nber of selected factors again increases the need for larger\ntraining corpora, which may not be available for some\nlanguagesorapplications,causingdatasparsityproblems.\n©TheAuthor(s).2017 OpenAccess ThisarticleisdistributedunderthetermsoftheCreativeCommonsAttribution4.0\nInternationalLicense(http://creativecommons.org/licenses/by/4.0/),whichpermitsunrestricteduse,distribution,and\nreproductioninanymedium,providedyougiveappropriatecredittotheoriginalauthor(s)andthesource,providealinktothe\nCreativeCommonslicense,andindicateifchangesweremade.\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page2of16\nThe two important steps for defining an FLM are to\ndefine a set of relevant factors and to define the back-\noff path, which determines the order in which to discard\nfactorsinthecaseoftrainingdatasparsity.\nIn the present paper, we propose a novel approach to\nfactor definition and backoff path selection for FLMs, in\nwhich the model considers only specificfactors from spe-\ncific words that can improve recognition performance.\nThe selection of these factors and their order in the\nbackoff path is made dynamically, based on the parts of\nspeech of words in the current context. Thus, the name\nis context-dependent FLMs. With this approach, we can\nbuildFLMswithalimitednumberoffactorsineachprob-\nability estimation, which can improve recognition perfor-\nmance while avoiding new data sparsity problems. This\nmethod also makes use of grammatical properties of the\ntargetlanguage,astheprocessofdeterminingthe backoff\npath searches for specific correlations in a given sentence\nstructure.\n1.1 Previouswork\nThe first results on FLMs were perplexity calculations,\ne.g., on the Arabic Callhome corpus [6] or the Wall Street\nJournal corpus [7]. In both studies, the authors reported\nperplexity improvements. Later, the models were used in\nspeechrecognitionapplications[3].\nWhile a backoff path can be selected manually and\nmay remain fixed, the authors of FLMs proposed several\nmethodsfordeterminingthebackoffpathbasedontrain-\ning data, e.g., the (normalized) number of occurrences\nand the maximum probability of the vertex. One of these\nmethods—min/max counts—was used by the authors in\n[8].Theauthorsin[7]alsoproposedtousingseveralback-\noff paths simultaneously and determining the set of those\npathsinrun-time.\nFLMs are often used in Arabic and Slavic languages,\nprobablymostprominentlyinRussian[9,10],aswellasin\nother morphologically rich languages such as Romanian\n[8],Turkish[11],andAmharic[12].\nThere is previous work with N-best list rescoring\nusing FLMs. In [13], the authors presented an N-best\nlist rescoring approach in which the language model\nwas adapted based on the vocabulary in the hypothe-\nsis list. While the system used morpheme-based mod-\nels in the first pass, the performance improved in the\nsecond pass, in which the models were word based.\nPart of the difference between the first and second pass\ncan be explained with the use of trigram sub-word-\nbased models in the first pass and the trigram word-\nbased models in the second pass. It was shown that\nsub-word-based models require higher model orders to\nachieve performance comparable with word-based mod-\nels [14]. The authors in [15] implemented FLMs with\nthe use of morpheme units for language modeling. With\nN-best list rescoring, they achieved an improvement of\n0.3%absolutely.\nSak [11] showed for the Turkish language that the\nimprovements achieved by using FLMs rather than tra-\nditional n-gram models are greater while only limited\nsize corpora are available for training; the improvements\ndecreased as the corpus size increased. These results can\nbe considered relevant to speech recognition in specific\ndomains with limited training data, where data sparsity\nbecomesaproblemforword-basedmodels.\nIn[16],theauthorsdefinedtriggerpart-of-speech(POS)\ntags and used them as syntactic features, demonstrat-\ning that FLMs can outperform traditional trigram lan-\nguagemodelsregardingperplexityandmixederrorratein\nspeech recognition. While their approach included POS\ntags, the backoff paths were selected based on overall\nperplexity.\nIn previous research, backoff path selection is often\nmadewithfixedbackoffpaths,amethodbasedon n-gram\noccurrences in training data, or overall perplexity results\non development data. Our approach differs from previ-\nous work as we perform backoff path selection for each\nwordsequenceindividuallyinrun-time,selectingthepath\ndifferently for each possible sequence of POS tags in the\ncontext and basing the decision on the perplexity results\nrelevantonlytotheconsideredsequenceofPOStags.\n1.2 Structureofthepaper\nInSection2,wepresentthebasicsofFLMsrequiredforan\nunderstanding of the proposed context-dependent FLMs\nand their implementation. In Section 3, we present our\nproposed models along with all of the algorithms neces-\nsary to efficiently determine backoff path selection, and,\ninSection4,atwo-passrecognitionalgorithmsuitablefor\nusing the proposed models is introduced. In Section 5,\nwe describe our experimental system used for building\nand testing the models. The obtained results are then\npresented in Section 6, followed by the conclusion in\nSection7.\n2 Morphosyntacticdescriptiontags\nA basic process of text or speech annotation is the addi-\ntion of part-of-speech (POS) tags to each word in a text.\nThemostbasictagswouldbenoun,adjective,verb,adverb\n...residual. However, to be more useful, typically addi-\ntional properties are considered. Tags which hold more\ninformation,especiallyinmorphologicallyrichlanguages,\narecalledmorphosyntacticdescription(MDS)tags.\nFor example, Penn Treebank Project specifies 36 dif-\nferent POS tags for the English language. Some part of\nspeech have several possible tags, e.g., there are four\ndifferentnounslistedwithtagsinparentheses:\n• Noun,singularormass(NN)\n• Noun,plural (NNS)\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page3of16\n• Propernoun,singular(NP)\n• Propernoun,plural(NPS)\nThese tags are used by TreeTagger, a freely available MSD\ntaggerforseverallanguages.\nAs English is not a morphologically complex language,\nits POS tags give only sparse information on the gram-\nmaticalroleofawordinasentence.Typically,information\nthatisgivenbyPOStagsisthenumberfornouns(singular\nor plural) and some information about verbs (base form,\npasttense,presentparticiple,thirdperson,etc.).\nIn the MULTEXT-east project [17], tagsets for sev-\neral languages were specified and standardized includ-\ning a more complex tagset for English. Version 5 of the\nproject specifies 16 categories (parts of speech) with a\ntotal of 31 attributes and a set of 135 different tags.\nFor example, verbs have the following attributes: type,\nverb form, tense, person, and number, and the attribute\nperson has the following possible values: first, second,\nandthird.\nSlovene, being morphologically complex language, has\nmuch more different tags. The JOS specifications [18],\nwhich were derived from the MULTEXT-east system,\ndefine 12 parts of speech. Again, some parts of speech\nhave several attributes, e.g., verbs have the following\nattributes: type, aspect, form, person, number, gender,\nand negative. Some parts of speech have none additional\nattributes, e.g., particles and interjections. JOS specifies\n12partsofspeec hwit hat otalof37a t tribut esandasetof\n1902differenttags.\nConsidering all possible values for all attributes of a\ngiven part of speech and all parts of speech, JOS defines\n1902differentMSDtagsforSlovene.\nFigure 1 shows a Slovene sentence with part of speech\ntags. The set was annotated with Obeliks, a tagger for\nSlovene.ThetextisinXML-TEIformatandisalsotagged\nwith lemmas. A translation of the sentence into English is\ntaggedinFig.2.ThetranslationwastaggedwithTreeTag-\nger and result converted into the same XML-TEI format.\nThe sentence was taken from the BNSI test set, later used\ninthisresearch.\nFig.1 AnMSD-annotatedtextinSlovene\nFig.2 AnMSD-annotatedtextinEnglish\n3 Factoredlanguagemodels\nLanguage models are used to estimate the probability\nof a given word sequenceW = (w1,w2,... ,wn).M o s t\ncommonly used are traditional word-basedn-gram lan-\nguage models, which estimate the conditional probability\nofeachwordgiventheprevious n− 1wor ds.T hecurr en t\nand previous words in these models are often called the\ncontext.\nIn an FLM, each wordw is represented as a vector ofK\nfactors wi =\n(\nf1\ni ,f2\ni ,... ,fK\ni\n)\n,w h e r ei is the index number\nof the word. Much like in word-basedn-gram language\nmodels, the probability of a sentence is the product of the\nprobabilities of individual words. In the case of an FLM,\nwe use the conditional probabilities of all factors in the\nestimatedwordgiventhefactorsofpreviouswords:\nP\n(\nf1:K\ni |f1:K\ni−1,f1:K\ni−2,... ,f1:K\ni−n+1\n)\n.( 1 )\nUsing the chain rule for probabilities, we can express this\nprobabilitywith\nP\n(\nf1:K\ni\n)\n=\nK∏\nk=1\nP\n(\nfk\ni |f1:k−1\ni ,f1:K\ni−1,... ,f1:K\ni−n+1\n)\n.( 2 )\nThe probabilities of individual factors on the right-hand\nside are again conditional probabilities given the factors\nfrom previous words and previous factors from the cur-\nrent word. With several words of history and several\nfactors per word, the total number of given factors can\nquicklyexceedafeasibleamountforpracticalapplication.\nSimilarly, in word-based n-gram models, the above\nequation also makes use of data from sequences of n\nwordssothatwecanspeakofan n-gramFLM.Wesaythat\nnistheFLMorder.WealsousethisdefinitionoftheFLM\norder when only some of the possible factors are present\nintheconditionalprobability.\nIn the above equations, probabilities are estimated sta-\ntistically.Duetodatasparsity,wecannotalwaysefficiently\ndetermineprobabilityestimatesinlanguagemodels.Both\nword-based models and FLMs employ a technique called\nbackoff [19], in which we discard terms in the conditional\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page4of16\nprobability and back off to a model with fewer terms. If\nnecessary, we can repeat the process to reduce the num-\nber of terms further. The selection of a backoff path in a\nword-basedn-grammodelisratherclear:oneachstep,we\ndiscard the most distant word, based on the assumption\nthat more distant words have less impact on the current\nword. However, in an FLM, several factors can have the\nsamedistancefromthecurrentfactor.Furthermore,there\nis no guarantee that discarding one of the most distant\nfactors will result in better performance than discarding\nsomecloserfactor,whetheritisthesametypeoffactoror\nnot.\nOften,lemmatizersandMSDtaggersareusedastoolsto\nautomatically determine grammatical properties. In this\ncase, it is important that we try to identify all of the fac-\ntors with the potential to improve performance in the\napplication and include only those factors in the model.\nWhile other factors may not decrease performance, they\nwill increase the size of the model and make the final\napplicationmoretimeandspaceconsuming.\nIn the context of FLMs, the backoff path first became\nrelevant and the backoff graph was introduced. We can\ndefinethebackoffpathasasequenceoffactorsubsets.On\nevery step in the backoff procedure, we move to another\nsubset that has one factor less than the previous subset.\nThe selection of the factor to be removed at each step\ndetermines the backoff path. Assuming we havek factors\nas the conditional terms in the FLM probability, we start\nwith the set of all factors. On every step, we discard one\nfactorthatisstillinthepath.Thismethodgivesus k!pos-\nsible backoff paths. Figure 3 shows a backoff graph that\nillustrates this principle in the case of three factors. The\nvertices of the directed graph represent subsets of factors\nusedintheconditionalprobability.\nFig.3 BackoffgraphforanFLMwiththreefactors\n4 Thecontext-dependentbackoffpath\nIn the proposed models, we will use run-time determi-\nnation of the backoff path, although we will use only one\npath simultaneously. Backoff path determination will be\nachieved using POS information from the currently esti-\nmated word sequence. We have therefore called these\nbackoff paths and models context-dependent. We will\nlater define the backoff function, which will tell us which\npath to use for probability estimation, given the current\nword and its context.\nWebasedtheproposedmodelsonthepropertyofgram-\nmatical matching in inflectional languages, in which the\nproperties of some words correlate with the properties of\nother words. An example is the matching of case, num-\nber,andgenderbetweennounsandadjectives.Giventhat\nthese words might not be adjacent in the sentence, we\nmade the assumption that it is not always best to discard\nmoredistantfactors;examiningwhichpartsofspeechare\npresent in the sentence might suggest which factors are\nbettersuitedforprobabilityestimation.\nThefirststepinbuildingtheproposedlanguagemodels\nis to determine the set of different factors for each word.\nWe found that word-basedn-gram language models still\nhave the greatest impact on performance. Thus, one fac-\nt o rs h a l lr e m a i nt h ew o r di t s e l f .O t h e rf a c t o r sc a nb et h e\nlemma and grammatical properties, either as separated\nfactors or combined (e.g., MSD tag). We can also include\notherinformationifitisusefulandifappropriatetoolsare\navailabletodetermineitautomatically.\nOne necessary piece of information in the proposed\nmodels is the POS tag, which can be incorporated as a\nseparate factor or can be included within the MSD tag.\n4.1 Thebackofffunction\nLet Pi denote the POS tag of wordwi. Assuming we have\nan input word sequence(wi−n+1,... ,wi),w ec a nd e t e r -\nmine a corresponding POS sequence(Pi−n+1,... ,Pi).W e\nthendefinethebackofffunction\nBO :(Pi−n+1,... ,Pi) ↦→ (f1,... ,fl),( 3 )\nfor every possible POS sequence, where(f1,... ,fl) is an\nordered set of factors to be included in the conditional\nprobability (Eq. 2). While the set alone defines the start-\ning point in the backoff graph, its order determines the\nbackoffpath.Consideringtheaboveequation,wewillstart\nwithlfactorsandthenremovethemonebyonefromright\nto left.\nLet us assume that for each word, we have definedK\nfactors and we are considering a model of ordern.T h e\nconditional probability will have betweenK · (n− 1) and\nK · (n − 1) + K − 1 possible conditional factors depend-\ning on the modeled factor. Let|P| denote the number of\nallpossiblevaluesofPOStagsincludingstart-of-sentence\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page5of16\nandstop-of-sentencetags.Wecanderivethetotalnumber\nofvalidPOSsequencesoflength n ≥ 2as:\n(|P| − 1)2 · (|P| − 2)n−2 .( 4 )\nFor a sequence to be valid, the start-of-sentence tag can\nappear only in the first place and the end-of-sentence tag\ncan appear only in the last place, while all other tags are\nnotrestricted.\nFactor probabilities can be expressed conditionally\ngiven the factors of previous words and the previous fac-\ntors of the current word. If we are currently considering\nfactork,thenumberofpossibleconditionalfactors mis\nm = (n− 1) · K + k − 1. (5)\nIf wewereto lookfor backoffpathsthatstartwithasetof\nall possible factors, the number of backoff paths would be\nm!. For shorter paths of lengthl, the number of possible\nbackoffpathsis\nm!\n(m− l)!.( 6 )\nFinally,thenumberofpathsofallpossiblelengthsis\nm∑\nl=0\n( m!\n(m− l)!\n)\n,( 7 )\nwhere we also include a path of length 0, which is a\nmodel containing only the modeled factor itself and no\nconditionalfactors.\nDepending on the number of possible POS tags and the\nmodel order, the number of different POS sequences can\nincreasebeyondafeasibleamount.Thenumberofallpos-\nsiblepathsincreasesevenfaster,exceeding10 8 fortrigram\nmodels with three factors, for example. Therefore, it is\nnecessary to implement an efficient method to determine\nthebackofffunctionBO.\n4.2 Initialalgorithm\nLet us first assume that we have the necessary equipment\ntoconsiderthewholesearchspaceandatrainingsetlarge\nenough to train all models. A simple brute-force search\nto determine backoff paths is presented in Algorithm 1,\nwhere N is the maximal model order,K is the number of\nfactors in a word, andPn is the set of all possible POS\nsequencesoflength n.\nThe final output of the algorithm is one backoff pathˆM\nfor each possible POS sequence, given the model order\nand the factor to be estimated. The algorithm works\nby testing all possible pathsM and selecting the path\nthat maximizes a certain functionT. We need to repeat\nthe algorithm for different model orders and all defined\nfactors.\nLet us consider the computational complexity of the\ninitial algorithm. The maximum model orderN and the\nnumber of factors per wordK are given. The number of\nAlgorithm 1:Initial brute-force algorithm for backoff\npathdetermination.\nInput:N\nInput:K\nInput:Pn ∀n ∈ {1,... ,N}\nOutput:S\nbegin\nS =∅\nforn ∈ 1,... ,N do\nfork ∈ 1,... ,K do\nforP ∈ Pn do\nˆM ←−Nosolution\nforM ∈ Mdo\nifT(M)> T( ˆM) then\nˆM = M\nS ←−S ∪ (n,k,P, ˆM)\npossible POS sequencesPn is given in Eq. 4, as a function\nofthenumberofdifferentPOStags |P|.Thenumberofall\npathsofthemaximumgivenlength lisgiveninEq.7.Con-\nsideringthenestedloopsinAlgorithm1,thetotalnumber\nofmodelstobetrainedandtestedcanbeexpressedas\nN∑\nn=1\n( K∑\nk=1\n(\n(|P| − 1)2 (|P| − 2)n−2 m!\n(m− l)!\n) )\n,( 8 )\nwheremisdescribedinEq.5.\nT h en u m b e ro fp o s s i b l eP O Ss e q u e n c e sc a nb ei g n o r e d\nwhen we are interested in the umber of models to be\ntrained and tested, since models can be reused for any\nPOS sequence, and results can be filtered accordingly\nto the actual POS sequence. In any case, we can then\nderive the computational complexity of the brute-force\nalgorithmas\nO(|P|N(NK)(NK)).( 9 )\nClearly,thisalgorithmisnottractableandweshall,there-\nfore, derive a feasible system.\nHowever, the presented algorithm will be the starting\npointfromwhichwewillderiveafeasiblesystem.Inorder\ntoachievethis,wehavetoaddressseveralissues\n1. Determineacriterionfunction T thatwillbefastand\nwillhavetheabilitytoestimatetheperformanceof\nmodels.Thecriterionfunctionisneededtoestimate\nwhichmodelwillperformbetterinthefinal\napplication.\n2. Defineareasonablyorderedlist ofpathstobetested,\nfromwhichwecanquicklyobtainasatisfactoryresult\nalongwithcriteriadeterminingwhentodiscontinue\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page6of16\nsearching.Such alistisessentialasasearchthrough\nall possiblepathswouldnotbefeasible.\n3. DefineamethodtocombinedifferentPOS\nsequences,asthetrainingdataforsomeindividual\nsequences wouldbetoosmall forobtainingareliable\nprobabilityestimate.\n4.3 Criterionfunction\nInordertodeterminealanguagemodel’sperformance,we\ncould use it in the final application, in our case speech\nrecognition.Thiswillprovidethebestestimateofwhether\na particular model will outperform another model or not.\nThis method is only feasible if we test just a handful of\nmodels.However,thisdoesnotapplytoourcase.\nWe often estimate the performance of a model by cal-\nculating its perplexity on a selected development text.\nCompared to estimation within speech recognition, this\nmethod is much faster, as we only need to estimate each\nsentence once. Furthermore, it has been shown that there\nisacorrelationbetweenalanguagemodel’sperplexityand\nitsperformanceinspeechrecognition[20].\nIn the presented algorithm, we want to compare mod-\nels with different backoff paths. While the perplexity of a\nmodel is typically estimated using the whole text, we now\nwant this estimate only at words with the given context.\nThe backoff path selection for each word will be inde-\npendent of the backoff path selection at neighboring\nwords and so the contributions of individual words to\nthe perplexity of the whole text will also be independent.\nThus, we can use perplexity and slightly modify its calcu-\nlation for use as our criterion function. This modification\nwill be necessary as we need to compare models not on\nthe whole text but only at words with a given context.\nThe perplexity PP of a modelM on a textW is defined\nby\nPPM(W) = 2HM(W), (10)\nwhereHM(W) isthecrossentropy\nHP(W) = 1\n−T · log2 P(W)\n= 1\n−T · log2\nT∏\ni=1\nP(wi)\n, (11)\nwhereT isthetotallengthofthetext.\nWe want to modify the perplexity definition to our\nneeds.Let P∗ denotes a given POS sequence. For eachP∗,\nwedefineasetofnumbers\nI(P∗) ⊂ {1,... ,T} , (12)\ncontainingindexesofwordswiththePOScontext P∗:\ni ∈ I(P∗) ⇐⇒ (Pi−n,... ,Pi) = P∗. (13)\nWe can then define a modified cross entropy on the\nsubsetofcorrespondingwordsas\nH∗(W,P∗) = 1\n−|I(P∗)| log2\n∏\ni∈I(P∗)\nP(wi). (14)\nFor our criterion function, we can then use the modified\nperplexity\nPP∗(W,P∗) = 2H∗(W,P∗). (15)\nWe shall note that better models have a lower perplex-\nity, and we can decide either to minimise the criterion\nfunctionortousethenegativevalueoftheperplexityasa\ncriterionfunction T andmaximizeit.Inthepresentpaper,\nwewillusethelatterapproach.\n4.4 Backoffpathlists\nWederivedasearchalgorithmthatwillbuildalistofback-\noffpathstobetestedconsideringthefollowingguidelines:\n1. Uptoacertainlength,itis stillfeasibletotestall\npossible paths,andweshall doso.Wecanbuild\nlongerpathsby extendingthebestshorterpaths.\n2. Wewilluse abeamwidth,atechniqueusedby\nspeechrecognitionsearchalgorithmstoexcludeless\nlikely hypotheses.Asintheserecognitionsearch\nalgorithms,wewillkeepalistofpossible pathsand\nexclude candidatesthathaveresults (obtainedbythe\ncriterionfunction)thatfallsufficientlybelowthe\ncurrentbestpath.\n3. Ifacertainpathisgiven,wewillsearchforadditional\nfactorsconsideringthefactorsfromthenextword\nbacknotonlyinthecurrentword’shistorybutalso\ninseveralwordsofhistory.\nThe backoff path search Algorithm 2 takes the cur-\nrent list as input and gives a new list of backoff paths as\noutput.Foreachpathintheinputlist,thealgorithmdeter-\nmines all possible factorsF∗ with which the path can be\nexpanded. After we include these new paths in the list,\nthe algorithm tests them with the criterion function and\nfinds the best backoff pathM∗. Finally, a beam widthε\nis determined and all paths falling outside the beam are\ndiscarded.\nThe search algorithm must be repeatedm-times, where\nm is the maximum number of factors allowed in a path,\nas the algorithm always creates paths that are one factor\nlongerthantheinputpaths.However,thealgorithmisnot\nemployed for short paths, as the list is generated with all\npossible paths. The final result is a list of backoff paths of\nlengthmorshorter.\nThe beam widthε is empirically determined for each\nlength of the path. In our experimental system, we found\nthat for shorter paths,ε shall be greater. We saw that in\nshorter paths, the addition of another factor results in a\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page7of16\nAlgorithm 2:The search algorithm for determining a\nlist of backoff paths for a given POS sequence or POS\nclass.\nInput:M\nOutput:M∗\nbegin\nM∗ = M\nfor∀M ∈ Mdo\nDetermine F∗\nfor∀F ∈ F∗ do\nM∗ = M∗ ∪ {(M+ F)}\nˆM∗ = argmax\nM∗∈M∗\nT(M∗)\nDetermine ε\nfor∀M∗ ∈ M∗ do\nifT(M∗) ≤ T( ˆM) − ε then\nM∗ = M∗ \\ {(M)}\nlarger change of perplexity than in longer paths. If we\nexpand shorter paths with a certain difference in perfor-\nmancewithadditionalfactors,theworseperformingpath\nhas a greater chance of being expanded into a path that\nwill later outperform other paths.\nThe described search algorithm is executed for a given\nclass of POS sequences, and it must be repeated for every\nclass.Theseclasseswillbedefinedinthenextsection.\n4.5 POSsequenceclasses\nD u et ot h el a r g en u m b e ro fP O Ss e q u e n c e s ,d a t as p a r -\nsity will become inevitable. We decided to group POS\nsequencesinto asmallernumberofclassesbasedontheir\nsimilarity. Each class will consist of one or more POS\nsequences, and the final application will make use of a\nbackoff path assigned to the class containing a certain\nPOSsequence.Thus,thecriterionfunctionandthesearch\nalgorithmdescribedabovewillbeperformedusingclasses\nofPOSsequences.\nGiven that the primary problem leading to the need\nf o rP O Sc l a s s e si sd a t as p a r s i t yf o rs o m eP O Ss e q u e n c e s ,\nwe decided that the algorithm shall merge the POS class\nwith the smallest number of occurrences in the training\nset with the most similar POS class. The final result shall\nbe a set of POS classes that all have a sufficient number\nof occurrences and contain similar POS sequences. The\nalgorithm takes a set of classes as input and returns a\nsmallersetofclassesasoutput.\nWe define the first set of classes with one set for each\nPOS sequence that has occurred. The merging algorithm\nthen performs several iterations to reduce the number\nof classes until it reaches the outer loop threshold α.\nIn these iterations, we first determine a list of possible\nbackoff paths for each class using the search algorithm\ndescribed above. We then perform several iterations of\nclass merging, where the class with the smallest number\nofoccurrences ¯P′ ismergedwiththemostsimilarclass ¯P′′.\nForthispurpose,weneedtodefinethesimilarityfunction\nD.Werepeatthemerginguntilthenumberofclassesfalls\nbelowtheinnerloopthreshold β.\nThe similarity between two classes is based on the two\nobtained lists of backoff paths. We look for a backoff path\nthat gives good (but not necessarily the best) results in\nboth classes. When faced with a small number of occur-\nrences,wecanalsoincorporatethePOSsequencesthem-\nselves to determine the similarity of classes. Other infor-\nmation can also be included to determine the similarity\nbetweenclasses.\nThe algorithm was conceived with two nested loops.\nThe outer loop determines the final number of classes\nand incorporates the inner loop for a trade-off between\nspeed and reliability. We might assume that if two classes\nare similar, they will also be similar to the merged class.\nWe do not assume that after several iterations, merged\nclasseswillstillbehavesimilartotheoriginalclassesfrom\nwhich they were merged; we therefore incorporated an\ninner loop that determines how many merging iterations\nwe want to perform before determining new backoff path\nlistsforthemergedclasses.\nAlgorithm 3:The merging algorithm, used for merg-\ningdifferentPOSclassesbasedontheirsimilarity.\nInput:P\nOutput:P∗\nOutput:M(¯P) ∀¯P ∈ P∗\nbegin\nP∗ = P\nDetermine α\nrepeat\nfor∀¯P ∈ P∗ do\nDetermine M(¯P)\nDetermine β\nrepeat\n¯P′ = argmin\n¯P∈P\n|(¯P)|\n¯P′′ = argmin\n¯P∈P\\{¯P′}\nD(¯P′, ¯P′′)\nP∗ ← Merge ¯P′ and ¯P′′\nuntil No.classes <β\nuntilNo.classes <α\n4.6 Finalpathselection\nAfter reducing the number of POS classes toα using the\nmerging algorithm, we perform a final run of the search\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page8of16\nalgorithm.Theoutputwillagainbealistofdifferentback-\noff paths. For the final solution, one of them must be\nselected.\nOnesimplesolutionwouldbetochoosethemodelwith\nthesmallestperplexity,regardlessofotherconsiderations.\nHowever, we can compare the models by their perplexity\naswellastheirsize.Wearethereforeabletomakeatrade-\noff between the expected results and the expected speed\nofthemodels.\nWeproposeasimplealgorithmtoselectanothermodel,\nwhere we first consider smaller models that would esti-\nmate probabilities faster. We first choose the smallest\nmodel. We then test all other models in ascending order\nof model size. If a larger model has a lower result of the\ncriterion functionT (i.e., higher perplexity), we skip it. If\nalargermodelhasasignificantlyhigherresultofthecrite-\nrion function, we select this model as our current choice.\nWe need to determine a thresholdγ that defines how\nmuchbettertheresultofthecriterionfunctionmustbein\nordertocallitsignificant.\nW ed on o tw a n tt os e l e c tas i g n i f i c a n t l yl a r g e rm o d e l\nif the criterion function increases only by a minimal\namount; however, we still want to allow the possibility of\nchoosing a slightly larger model. Therefore, we allowed\nthe selection of a larger model if the criterion function\ngives a better result and the model’s size increases by an\namountsmallerthanthethreshold δ.\nTheprocessofselectingthefinalmodelisrepresentedin\nAlgorithm4.Weareagainforcedtodeterminetwoheuris-\ntic constants:γ and δ. We can start by settingγ = 0a n d\nδ =∞ so that the algorithm will give us the model with\nthebestcriterionfunctionresult.Wecanthenchangethe\nvaluesofbothconstantstoachieveacompromisebetween\nperformanceandspeed.\nAlgorithm 4:The selection algorithm, used to deter-\nminewhichmodelfromthesolutionsetwillbeusedin\ntrainingandrecognition.\nInput:M\nOutput: ˆM\nbegin\nSort Mbysize\nˆM ←−findsmallestmodelin M\nfor∀M ∈ Mdo\nDetermine γ inδ\nif(|M| > | ˆM| & T(M)< T( ˆM) then\nNext\nifT(M)> T( ˆM) + γ then\nˆM ←−M\nifT(M)> T( ˆM) & |M| < | ˆM|+ δ then\nˆM ←−M\n4.7 Modelusage\nOur final result obtained by the selection algorithm is\nthe backoff function. This function is determined for\neach model order and each defined factor. While using\na context-dependent FLM for a given word sequence, we\ndecide which model order we want to use. Usually, it\nwould be the largest order available, except at the begin-\nnings of speech segments, where there are fewer words\nin the context. We then determine the POS sequence and\nlook up the POS class in which this sequence has been\nmerged. For each factor, we find the selected path for this\nfunction and then calculate estimates using these backoff\npaths.\n5 Therecognitionalgorithm\nAn appropriate system for testing the performance of\ncontext-dependent models is a two-pass continuous\nspeechrecognitionalgorithm.\nIn the first recognition pass, we typically make use of\nacoustic models and word-based language models. The\nmain requirement of the speech recognition search algo-\nrithm is that it will produce a list of hypotheses for each\nspeech segment. In the first pass, each hypothesisW is\nscored with the equation\nP(W) = pAPA(W) + pLPL(W) + pIC(W), (16)\nwherePA andPL areprobabilitiesobtainedwithanacous-\ntic and a word-based language model, respectively. Func-\ntion C is a simple word count function. We call these\nresults the individual scores. CoefficientspA, pL,a n dpI\naretheacousticmodelweight,thelanguagemodelweight,\nandthewordinsertionpenalty,respectively.\nWe then use an MSD tagger to annotate the hypotheses\nwithallinformationusedinthecontext-dependentFLMs.\nInordertoreducethespacerequirementforthehypothe-\nses, we shall include only useful information, e.g., we can\ndiscardinformationproducedbyMSDtaggers,whichwill\nnotbeusedbythemodels.\nIn the second recognition pass, the hypotheses are\nrescored. We can use all three scores from the first pass\nand addscores from context-dependent FLMs. For exam-\nple, if we have three factors, i.e., three FLMs, the scoring\nfunctionbecomes\nP(W) = pAPA(W) + pLPL(W) + pIC(W)\n+ p1Pf1(W) + p2Pf2(W) + p3Pf3(W), (17)\nwherePf1(W),Pf2(W),and Pf3(W) areprobabilityscores\nobtained by the three context-dependent FLMs. Each of\nthese three scores also has an appropriate weight:p1, p2,\nandp3.\nIn both recognition passes, we have to optimize all\nweights using a development set. Usually, we set the\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page9of16\nacoustic model weight to 1 and we use an optimization\nalgorithmfortheremainingweights.\nWe perform several recognition run-throughs on the\ndevelopment set to find the best language model weight\nand word insertion penalty for the first recognition pass.\nWe then use the optimized values in a single recognition\nrun-throughontheevaluationset.\nI nt h es e c o n dp a s s ,w eu s el i s t so fN-best hypothe-\nses from both the development and the evaluation sets.\nWe optimized all model weights and the word insertion\npenalty again on the development set and used them in\nthe evaluation set.\nAbrute-forceexhaustivesearchofoptimalvalueswould\nagain not be feasible, as we have several real-valued val-\nues to optimize. Two weights are already present in the\nfirst pass but need to be optimized again. For each fac-\ntorinthecontext-dependentFLMs,wehaveanadditional\nweight to optimize. A simple algorithm was devised to\noptimize these weights. The algorithm works onN-best\nlistsofallhypothesesinthedevelopmentset.Itstartswith\nan initial set of values for all weights and then optimizes\nonly one weight at a time while all other weights remain\nconstant, before moving on to the next weight. The algo-\nrithmperformsseveraliterationsofoptimizingallweights\nfor a predefined number of times or until the optimized\nvaluesdonotdifferfromthevaluesobtainedintheprevi-\nous iteration. We can repeat the algorithm with different\nstartingvalues.\n6 Experimentalsystem\nWe evaluated the performance of the proposed language\nmodels on a large vocabulary continuous speech recogni-\ntion(LVCSR)applicationinaninflectivelanguage,namely\na Slovene Broadcast News transcription. The general\nstructureofoursystemispresentedinFig.4.\n6.1 Speechdatabases\nThe Slovene Broadcast News (BNSI) database [21] was\nthe only speech database used; it is also the only Slovene\nspeech database appropriate for LVCSR. It consists of\napproximately25hoftranscribedspeechdata.Thelargest\npart serves for acoustic model training, while the rest\nmakesupthedevelopmentsetandtheevaluationset,each\nconsistingofapproximately2.5hofspeechdata.Weused\nthe development set in all parameter optimization pro-\ncesses and the evaluation set for obtaining recognition\nresults.\nWetrainedword-based n-gramlanguagemodelsonthe\n620 million words FidaPLUS corpus of the Slovene lan-\nguage[22],whichconsistsmainlyofarticlesfromnewspa-\npersandjournals.Whileitisavailableinalemmatizedand\nMSD-taggedform,weusedonlybasicwordformsfromit.\nThe Slovene ssj500k corpus is a manually MSD-\ntagged and lemmatized corpus consisting of approxi-\nmately 500,000 words. We used it for the training of all\nFLMs. It is large enough to effectively train models with a\nlimitedvocabulary—inthiscasePOStagsandMSDtags.\nThe MSD tags in the ssj500k corpus follow the JOS\nspecifications [18], which themselves were derived from\ntheMULTEXT-EASTsystem[17].DependingonthePOS\ntags, the MSD tags can hold several grammatical cate-\ngories.However,wefoundthatonlyafewareusefulinthe\npresented application: POS, type, gender, case, number,\nandperson.Allothertagswereremoved.\n6.2 Firstpass\nThe speech database was manually segmented. The\ntrained acoustic models were HMM triphones with 16\nGaussianmixtures.Weused39featuresofmel-frequency\ncepstral coefficients (MFCC): energy features and 12 fea-\ntures with delta and delta-delta coefficients.\nFig.4 Theexperimentalsystem\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page10of16\nThe vocabularies in the first pass include the most\ncommonwordsintheFidaPLUScorpus.Webuiltvocabu-\nlariesrangingfrom60,000to300,000words,constructing\nbigram and trigram models on all vocabulary sizes. Two\nsmoothing techniques were used: Good-Turing and mod-\nifiedKnesser-Ney.\nIn the first pass, we used a Viterbi decoder to obtain\nan N-best list for the development and evaluation sets\n(N = 1000). We repeated the recognition on the devel-\nopment set in order to obtain optimal parameters for the\nlanguage model weight and the word insertion penalty\nand used the optimized values on the evaluation set.\nWe used the Obeliks tagger—an MSD tagger developed\nespeciallyfortheSlovenelanguage—onall N-bestlists.\n6.3 Context-dependentmodeltraining\nThe first step in building context-dependent models was\nto define factors. The Obeliks tagger generates tags\naccording to the JOS specifications, in which 12 different\nPOStagsaredefined,someofthemhavingdifferenttypes.\nCombiningthePOStagandtype,wedefinedanextended\nPOS tag, e.g., we distinguished the typescommon noun\nandpropername insteadofusing noun.Wedecidedtouse\nextended POS tags due to grammatical concerns, as dif-\nferent types of the same part of speech can have different\ngrammatical properties. The extended POS tags had 32\npossible values. According to the search space estimation\ni nt h ep r e v i o u ss e c t i o n ,w eh a v eat o t a ln u m b e ro fo v e r\nonebillionpossiblePOSsequenceswithlengthsuptosix.\nNext,webuiltMSDtagsthatincludedonlygender,case,\nnumber, and person. Preliminary experiments showed\nthat those grammatical categories could improve perfor-\nmance in speech recognition, while other grammatical\ncategories cannot. We therefore used a reduced MSD tag\nset.Differentpartsofspeechcanhavefrom1to255differ-\nentvaluesofthereducedMSDtag.Thelastchosenfactor\nwastheworditself.\nWe trained all MSD models on the ssj500k corpus. Not\nall possible POS sequences appeared in the corpus. For\nthe first determination of POS sequence classes, we used\nonly those sequences that did appear in the corpus. In\nTable 1, the number of classes depending on the model\norder is shown. The numbers are significantly lower than\nthe theoretical upper limit, as most theoretically possible\nsequences do not produce meaningful or grammatically\nsensiblesentences.\nWhile implementing all of the necessary algorithms to\ndetermine the backoff function, we had to determine sev-\neral heuristic constants. We did this experimentally to\nachievegoodperformancewithreasonabletimeandspace\ndemands. The beam widthε itself is newer fixed but is\nrather determined based on the currently best hypothe-\nses in the search algorithm and the number of factors\nalready in the backoff path, e.g., if the path has perplexity\nTable1 NumberofPOSclassesgiventhemodelorder n\nobservedintrainingandusedforthefirstdeterminationofPOS\nclasses\nn Numberofsequences\n13 0\n2 475\n3 3003\n4 8666\n5 13,682\n6 15,478\nPPmin and a certain number of factors, the beam width is\ndeterminedas\nε = PPmin · Fε, (18)\nwhere values for Fε are given in Table 2. Any backoff\npath with a perplexity higher than PPmin + ε will be\ndiscarded. Additionally, we implemented further criteria\nin the pruning procedure. If the set of possible backoff\npaths contains two paths that contain the same factors\nin a different order, we excluded the worse performing\nmodel.Wefoundthatthesearchalgorithmtimedemands\nchanged by about 10%, when using different values of the\nfactors in Table 2 ranging from 0.1 to 2 at all path lengths\nand, therefore, different values forε. Finally, the values in\nTable2werechosenbasedonpreliminaryresultsobtained\nduringtheimplementationofthesearchalgorithm.\nUnlike ε,t h et h r e s h o l d sα and β in the merging algo-\nrithm have almost no impact on the time demands in\ndeterminingthebackofffunction.\nWe determined α = 10 for unigram models and\nα = 50forhigherordermodelstobesuitablevalues.The\nparameter β is determined in each iteration at run-time.\nWeaddedtheconstraintthatthenumberofclassesinthe\ninnerloopmustnotbereducedbymorethan50%.\nLastly, we had to determine the parametersγ and δ in\nthe selection algorithm. Those parameters have no effect\non time demands for training. It was also found that they\ndo not have a significant impact on model performance.\nWe choseγ to be 5% of the perplexity of the currently\nTable2 Pruningwidthfactors kforthesearchalgorithmgiven\nthenumberoffactors\nNo.factors Fε\n11 . 0\n20 . 5\n30 . 3\n40 . 2\n≥50 . 1\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page11of16\nselectedmodeland δ tobe25%ofthesizeofthecurrently\nselectedmodelintermsofentriesinthelanguagemodel.\n6.4 Computationaldemands\nThe total time to perform the search algorithm was 56 h,\nof which the largest part (36 h) was spend using model\norder n = 5. It was performed on a dual-processor\nserver with 88 logical threads at 3.6 GHz and 128 GB\nof memory. We shall note that the exact time depends\nnot only on hardware configuration but also on software,\nother server load, development and training sets, and the\npossibility of parallel processing. We also reused already\nbuildmodels.\nRather than comparing real-time values, we shall state\nthe number of trained models. Table 3 shows the num-\nber of all possible backoff paths given the model ordern\nand the current factork. The number of maximum mod-\nels is based on Eq. 5; however, we also limited the backoff\npath length to eight factors. We also show the number of\nmodelsthatwereactuallytrainedandtestedwithourfinal\nsearch algorithm. We see that at higher model order, the\nnumberoftestedmodelsonlyslightlyincreases,whilethe\nnumber of maximum models reaches infeasible numbers.\nThefinalratiobetweenthesumofalltestedandsumofall\npossiblemodelsis1:1100.Thismeansthatwetested1100\ntimes fewer models than we would have to in brute force.\nEven, if we consider models with an order of up to 4, this\nratiowouldbe1:200.\nTable3 Comparisonofthenumberofallpossiblebackoffpaths\nwiththenumberoftrainedandtestedpathswithregardto\nmodelorder nandmodeledfactor k\nnk Maximum Tested\n111 0\n122 1\n135 4\n2 1 16 15\n2 2 65 55\n2 3 326 176\n3 1 1957 705\n3 2 13,700 1758\n3 3 109,601 5129\n4 1 623,530 15,320\n4 2 2.61 × 106 15,505\n4 3 8.71 × 106 21,552\n5 1 2.47 × 107 56,888\n5 2 6.19 × 107 45,085\n5 3 1.41 × 108 51,466\nTotal 2.40 × 108 213,659\nItshouldbenotedthatonepossibilityisamodelwithout\nany factors. This model was not tested as it always gives\nthelowestperformance.Therefore,thenumberofallpos-\nsiblebackoffpathsinthetableis1higherthanthenumber\noftestedmodelsatdatapointswhereotherwiseallmodels\nweretested.\n6.5 SimpleMSDmodels\nIn order to determine whether the context-dependent\nbackoff paths have any effect on the recognition perfor-\nmance that is not due to the inclusion of MSD data itself,\nwe also built simple language models based on MSD tags.\nThese models are based on the extended POS tag (P)a n d\nthe MSD tag (M).\nT hefirstsetofmode lsisdefinedby\nP(P0|P−1,... ,P−n),\nP(M0|M−1,... ,M−n). (19)\nThe backoff path in both models is to remove the most\ndistant factor. In the first set, we only use factors of the\nsametypeinthemodel.Thesemodelsare n-grammodels\nof extended POS tags andn-gram models of MSD tags.\nThesecondsetofmodelsisdefinedby\nP(P0|P−1,... ,P−n,M0,... ,M−n),\nP(M0|M−1,... ,M−n,P−1,... ,P−n). (20)\nHere,bothtypesoffactorsareusedinthesamemodel.We\nbuiltmodelswithamaximumofsevenconditionalfactors\nand considered at most five words of history. Therefore,\nwe obtained 6-gram FLMs with fixed backoff paths of no\nmore than eight vertices. The limits of the number of\nfactors and history length are due to preliminary results\nindicatingthatlongerpathsareofnofurtherbenefittothe\nperformance.Additionally,weaddedtheconstraintthatif\na particular factor appears in these models, then all fac-\ntors of the same type that are closer to the modeled word\nmustalsoappear.\nBackoff paths are built simply by adding one of two\npossible next factors to the path. A total of 244 possible\nmodelswerebuilt,allwithbackoffpathsthatcomplywith\nthepresentedconstraints.\nThese models were later used to obtain results\nregarding the degree to which performance can be\nimprovedifmorphologicalinformationisintroducedinto\nthe speech recognition algorithm without introducing\ncontext-dependent models.\n6.6 Tools\nWe used the HTK toolkit with the speech decoder HDe-\ncode in acoustic model training and the first recognition\npass. In order to build the language models (including\nFLMs), as well as for hypothesis rescoring, we used the\nSRILM toolkit, as it supports FLMs [23]. For backoff path\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page12of16\ndeterminationandthesecondrecognitionpass,toolswere\ndevelopedwithintheframeworkofthepresentresearch.\n7 Results\n7.1 Firstpassand N-bestlists\nTable 4 shows recognition results in word error rate\n(WER) for the first recognition pass with bigram and tri-\ngram language models with Good-Turing and modified\nKnesser-Ney smoothing and different vocabulary sizes.\nVocabularysizeandmodelorderhaveanexpectedimpact\non performance. However, while other research indicated\nthat modified Knesser-Ney smoothing performs better\nthan Good-Turing [24], our results show the opposite,\nalthough there are only slight, statistically non-significant\ndifferences. The best WER result of 22.64% was obtained\nwith a trigram Good-Turing smoothed model built on a\n300K vocabulary. Similar results can be obtained on the\nBNSIdevelopmentset.\nWe can also compare computational and memory con-\nsumption. Table 5 shows real-time factors (RTF) for all\nfirst-passscenarios.Wenoticeonlysmalldifferenceswith\nregard to the smoothing method. Results show that RTF\nincreasesbyafactorof2ifthevocabularysizeisincreased\nfrom 60K to 300K and by a factor of 3 if when using tri-\ngram models compared to bigram models. Considering\nalsorecognitionresults,weseethattheincreaseinvocab-\nularybroughtalargerperformanceincreasewithasmaller\nRTFincreasethantheincreaseinmodelorder.\nHowever, increasing the vocabulary size mainly elim-\ninates errors due to out-of-vocabulary words. With the\n300K vocabulary, the out-of-vocabulary rate is 1.02% and\nfurtherenlargementofthevocabularyresultsinonlysmall\nrecognitionimprovements.\nPreliminary research showed us that there are also only\nsmall differences in model perplexity if we increase the\nmodel order to 4-gram models. Also, the recognition\nresults we found increase only slightly if at all. This was\nfound using a two-pass rescoring algorithm as the recog-\nnition tool (HDecode) supports only bigram and trigram\nmodels.\nTable 6 shows us the the memory demands for all first-\npass scenarios. We can again observe that increase in\nTable4 RecognitionresultsinthefirstpassobtainedontheBNSI\nevaluationset\nVocabularysize\nWER[%]\nGood-Turing Knesser-Ney\nBigram Trigram Bigram Trigram\n60K 33.89 30.73 33.84 30.95\n100K 31.27 28.08 31.24 28.36\n200K 29.70 26.27 29.78 26.53\n300K 29.22 25.64 29.28 25.87\nTable5 Averagereal-timefactorsforrecognitioninthefirstpass\nobtainedontheBNSIevaluationset\nVocabularysize\nReal-timefactor\nGood-Turing Knesser-Ney\nBigram Trigram Bigram Trigram\n60K 6.29 18.46 6.14 18.62\n100K 7.82 23.42 7.52 23.37\n200K 10.44 31.55 10.48 31.71\n300K 12.66 37.09 12.88 37.70\nvocabulary size increases the memory demands slightly\nless than the increase in model order.\nWe have to add that the exact values for RTF as well as\nmemory consumption depended on the computer hard-\nwareandthesoftwaretoolsused.\nWhenrescoringthe N-bestlist,thereisalwaysanupper\nlimit of how much improvement can be achieved, as only\na limited number of hypotheses are available in the sec-\nond pass. Assuming we had a system in the second pass\nthat would always select the best hypothesis (an oracle),\nwe would still have an error rate—the so-called oracle\nerror rate (OER)—that is dependent on the number of\nhypotheses. The OER results on the BNSI evaluation set\nusing bigram and trigram language Good-Turing mod-\nels are shown in Figs. 5 and 6, respectively. Similar OER\nresults can also be obtained for other models and on the\nBNSIdevelopmentset.Inbothfigures,wecanseethatthe\noracle error rate at 1000 hypothesesis approximately 50%\nlowerthantheWER.\nWetaggedthehypothesesobtainedwiththebigramand\ntrigram Good-Turing models with the 300K vocabulary\nfor use in the second recognition pass. It was decided to\nuse theN-best lists with the best WER results, as well as\nthe list obtained with the corresponding bigram model.\nThe used tagger is rather slow and had real-time factors\nfrom26.07to26.67.\n7.2 Context-dependentmodels\nSeveral models were tested in the process of determining\nthe backoff functions for different maximal model orders\nTable6 Peakmemoryusageinthefirstpassobtainedonthe\nBNSIevaluationset\nVocabularysize\nMemoryusage[MB]\nGood-Turing Knesser-Ney\nBigram Trigram Bigram Trigram\n60K 743 1287 745 1299\n100K 917 1512 914 1530\n200K 973 1776 1110 1815\n300K 1252 1934 1273 1935\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page13of16\nFig.5 OracleerrorrateontheBNSIevaluationsetwithbigram\nmodelsanddifferentvocabularysizes\nand different factors. The total number of tested models\nwas 407,892, of which 322 were in the final selection for\nall backoff functions. In all cases, the number of selected\nmodels in a single backoff function lies below the thresh-\noldinthemergingalgorithm,whichdefinesthemaximum\nnumber of classes. The number is lower because some\nclasses use the same backoff path as others.\nTable 7 shows the perplexity results of context-\ndependentFLMsontheevaluationset.Theseperplexities\nwere obtained using the backoff function for probability\nestimationsinthetext.\nWe separately rescored theN-best lists obtained with a\nbigram and a trigram model in the first pass. The results\nFig.6 OracleerrorrateontheBNSIevaluationsetwithtrigram\nmodelsanddifferentvocabularysizes\nTable7 Perplexityresultsofcontext-dependentFLMonthe\nBNSIdevelopmentset\nModelorder Modeledfactor\nPM W\n1 / 6581 38,867\n2 8392 3490 33,536\n3 7275 3147 31,294\n4 6561 2970 29,447\n5 6231 2876 27,947\nare shown in Tables 8 and 9, respectively. We repeated\nrescoring using any one, any two, or all three FLMs. We\nalso indicate the model order with which the best results\nwereobtained.\nThe results show significant increases in performance\nwhen bigram language models were used in the first pass.\nWe also used a traditional trigram language model in the\nsecond pass instead of the bigram model from the first\npass. Thus, part of the increased performance is due to\nthismodelandparttotheuseofcontext-dependentFLM.\nWe decided to do so as the use of bigram models in the\nfirst pass is much faster and the results can be largely\nimproved with traditionaltrigramlanguagemodelsinthe\nsecondpass.\nStill, the results do now outperform the use of trigram\nlanguagemodelsinthefirstpass.Thebestimprovementis\nobservedusingonlytheFLMforMSDtags.Furthermore,\ncombinations with this model show greater improvement\nthanothercombinations.\nWhen rescoring the results obtained with a trigram\nmodel in the first pass, the differences are smaller as the\nsame traditional language model was used. However, in\nthis case, we can see the impact that context-dependent\nFLMs have onperformance. Whenonlyone FLM is used,\nthebestimprovementisobtainedwiththemodelforMSD\ntags.UsingthemodelforPOStagsevenlowerstheresults.\nThis can be due to either the nature of this model or\nthedifferencesintheevaluationanddevelopmentsonthe\nbasis of which the parameters were optimized. We also\nTable8 Recognitionresultsontheevaluationsetusingbigram\nmodelsinthefirstpassandcontext-dependentmodelsinthe\nsecondpass\nFLMs Optimalmodelorder WER[%] Relativeimprovement[%]\nP 5 26.51 9.26\nM 4 25.84 11.56\nW 5 26.07 10.78\nP+M 5 25.92 11.27\nP+W 5 26.38 9.71\nM+W 4 25.92 11.30\nAll 5 26.01 10.99\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page14of16\nTable9 Recognitionresultsontheevaluationsetusingtrigram\nmodelsinthefirstpassandcontext-dependentmodelsinthe\nsecondpass\nFLMs Optimalmodelorder WER[%] Relativeimprovement[%]\nP 3 25.90 −1.01\nM 5 25.39 0.99\nW 5 25.46 0.70\nP+M 5 25.61 0.14\nP+W 5 25.95 −1.20\nM+W 2 25.20 1.73\nAll 5 25.45 0.74\nseethatcombinationsusingthePOStagFLMshowworse\nresults.Thebestresultsareobtainedusingmodelsforthe\nMSDtagandforthebasicwordform.Weobservea1.73%\nrelative improvement over the first-pass result.\nTheresultsinbothtablessuggestthatPOStagFLMsare\nnot beneficial to recognition accuracy, while both of the\nother models are.\nThecomputationalandmemorydemandsinthesecond\npass are much smaller, as only language models are used\non a finite number of sentences. The real-time factor in\nthe second pass is about 0.50, while the real-time factor\nforsimplemodelsisabout0.01.\n7.3 ComparisonwithsimpleMSDmodels\nWe performed the same two-pass algorithm and param-\neter optimization using simple MSD models. The recog-\nnition results after the second pass on the evaluation set\nare shown in Table 10. Results are shown for using either\nbigram or trigram language models in the first pass and\neither only the model for MSD tags or both models for\nMS DandPOStagsint hesec ondpass.\nThe results using bigram models in the first pass are\ncomparable with the use of context-dependent models.\nThe results obtained while using trigram models in the\nfirst pass show that recognition accuracy was improved\nonly by 0.07% when using both models, for MSD tags and\nPOStags.Theaccuracywasevenreducedwhileusingonly\nthe model for MSD tags. Results using only the model for\nPOStagsweresignificantlylower.\nTable10 Recognitionresultsontheevaluationsetusingsimple\nMSDmodelsforPOStags( P)andMSDtags( M)\nFirstpass Secondpass WER[%] Relative\nmodel model improvement[%]\nBigram M 25.84 +11.56\nBigram M+P 26.01 +10.99\nTrigram M 25.86 −0.84\nTrigram M+P 25.62 +0.07\nThe comparison shows that the simple addition of\ngrammatical information does not improve the recogni-\ntion results significantly unless we use this information in\namoresophisticatedmodel.\n7.4 Erroranalysis\nWeanalysedthebestresultsinthefirstpassandtheirbest\nimprovementinthesecondpassinmoredetail.\nThe statistical significance of the improvement was\ntested with the approximate randomization test. This test\nwas selected because it does not require special assump-\ntions about the evaluation set. With 10,000 runs, we\nobtained a p value of 0.0055, indicating that the results\nwere statistically significant at a significance threshold\nof0.01.\nTable 11 shows the basic results of inserted, deleted,\nand substituted words. The largest difference can be seen\nin the number of deleted words—a reduction of 16%\nrelatively—whileothertypesoferrorincreasedslightly.\nTable 12 shows the WER results regarding different\nparts of speech. The results are shown for the first and\nsecondpass,displayingthedifferenceaswellasvaluesrel-\native to the number of occurrences in the reference tran-\nscription.Thegreatestimprovementsoccurwithpreposi-\ntions and conjunctions. Considering these results and the\ndata in Table 11, we can conclude that context-dependent\nFLMs have the most impact on correcting deletion errors\ninvolvingshortwords,mainlyminorpartsofspeech.\nThe results for inflected parts of speech are mixed.\nWhile the error rates for verbs, adjectives, and pro-\nnouns decreased, the error rates for nouns and numerals\nincreased.AlthoughtheincreaseWERfornounsisrather\nsmall,theincreasefornumeralsismoresignificant.\nFinally, Table 13 shows the results for inflectional parts\nof speech, where the correct POS and the correct lemma\nbut the false word form were recognized. These errors\nindicate falsely recognized word endings. The percentage\nvalues are given relative to the total number of substi-\ntution errors of the POS, e.g., 866 nouns were substi-\ntuted with other nouns, while 296 of these substitutions\n(34.58%) were with a noun with the same lemma. We see\nthat approximately 40% of all errors on inflectional parts\nof speech are due to falsely recognized word forms. In\nTable11 Comparisonofrecognitionerrorsbytypebetweenthe\nfirstandsecondpass\nErrortype 1stpass 2ndpass Difference\nDeletion 1446 1216 −230\nInsertion 401 498 97\nSubstitution 4013 4043 30\nTotal 5860 5757 −103\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page15of16\nTable12 ComparisonofrecognitionerrorsbyPOSbetweenthe\nfirstandsecondpass\nPOS Total 1stpass 2ndpass Difference\nNoun 6721 23.20% 23.26% +0.25%\nVerb 3901 28.22% 27.66% −2.00%\nAdjective 2646 20.67% 20.33% −1.65%\nAdverb 1667 24.84% 24.18% −2.66%\nPronoun 1536 34.18% 33.27% −2.67%\nNumeral 747 27.31% 27.71% +1.47%\nPreposition 2499 27.53% 26.65% −3.19%\nConjunction 1964 28.16% 26.78% −4.88%\nParticle 1038 24.76% 24.47% −1.17%\nInterjection 8 100.00% 50.00% −50.00%\nResidual 6 66.67% 100.00% +50.00%\nthe second pass, the total number of errors decreased by\n1.65%relatively.\n8C o n c l u s i o n s\nInthepresentpaper,weproposedanewmethodofdeter-\nmining the backoff path for factored language models.\nA st h eb a s i cm e t h o dl e a d st od a t as p a r s i t ya n dal a r g e\nsearch space, we also presented all of the necessary algo-\nrithms to build a feasible system. We demonstrated that\nmodels using these backoff paths outperformed factored\nlanguage models using more simply determined backoff\npaths. We also demonstrated that simpler models might\nn o th a v ea n yp o t e n t i a lt oi m p r o v er e c o g n i t i o nr e s u l t s ,\nwhile the use of models with context-dependent back-\noff paths resulted in an accuracy improvement that was\nshowntobestatisticallysignificant.\nThe methods presented still leave room for the further\noptimizationofseveralparametersusedinthealgorithms.\nTraining set and factor selection may also be considered\nin further research. While we performed the experiments\non Slovene, the proposed models can also be used in\nother inflectional or otherwise morphologically complex\nTable13 Falselyrecognizedwordformsofinflectionalpartsof\nspeechrelativetothetotalnumberofsubstitutionswithinthe\nsamepartofspeech\nPOS 1stpass 2ndpass\nNoun 296 34.58% 291 34.64%\nAdjective 135 57.94% 126 53.58%\nVerb 183 48.93% 177 47.58%\nPronoun 47 52.22% 46 51.11%\nNumeral 16 20.78% 16 21.33%\nTotal 667 41.53% 656 40.72%\nlanguages. Further research on the possibility of com-\nbining the presented statistics-based method with formal\nknowledge-based methods could also lead to improve-\nments in recognition of word relations in a spoken sen-\ntence and consequently to a reduction in recognition\nerrors.\nAcknowledgements\nThisworkwaspartiallyfinanciallysupportedbytheSlovenianResearch\nAgency(ARRS)undercontractnumber1000-10-310131.\nAuthors’contributions\nThispapershowsresultsfromthedoctoralresearchofGDunderthe\nsupervisionofZK.Bothauthorsreadandapprovedthefinalmanuscript.\nCompetinginterests\nTheauthorsdeclarethattheyhavenocompetinginterests.\nReceived:13October2016 Accepted:20February2017\nReferences\n1. SZablotskiy,KZablotskaya,WMinker,in IntelligentEnvironments(IE),2010\nSixthInternationalConferenceon .SomeapproachesforRussianspeech\nrecognition,(KualaLumpur,2010),pp.96–99\n2. TRotovnik,MSMau ˇcec,ZKa ˇciˇc,Largevocabularycontinuousspeech\nrecognitionofaninflectedlanguageusingstemsandendings.Speech\nCommun.49(6),437–452(2007)\n3. KKirchhoff,DVergyri,JBilmes,KDuh,AStolcke,Morphology-based\nlanguagemodelingforconversationalArabicspeechrecognition.\nComput.SpeechLang. 20(4),589–608(2006)\n4. AEAxelrod, Factoredlanguagemodelsforstatisticalmachinetranslation .\n(UniversityofEdinburgh,2006)\n5. EMdeNovais,Portuguesetextgenerationusingfactoredlanguage\nmodels.J.BrazilianComput.Soc. 19(2),135–146(2013)\n6. KKirchhoff,JBilmes,KDuh, Factoredlanguagemodelstutorial .(University\nofWashington,2016).http://ssli.ee.washington.edu/people/duh/papers/\nflm-manual.pdf.Accessed1October\n7. JABilmes,KKirchhoff,in NAACL-Short’03.Factoredlanguagemodelsand\ngeneralizedparallelbackoff,(Edmonton,2003),pp.4–6\n8. MLaz ˘ar,DMilitaru,in SpeechTechnologyandHuman-ComputerDialogue\n(SpeD),20137thConferenceon .ARomanianlanguagemodelingusing\nlinguisticfactors,(Cluj-Napoca,2013),pp.1–6\n9. DVazhenina,KMarkov,in AwarenessScienceandTechnologyand\nUbi-MediaComputing(iCAST-UMEDIA),2013InternationalJointConference\non.FactoredlanguagemodelingforRussianLVCSR,(Aizuwakamatsu,\n2013),pp.205-211\n10. IKipyatkova,AKarpov,in SPECOM2014.Studyofmorphologicalfactors\noffactoredlanguagemodelsforRussianASR,(NoviSad,2014),\npp.451–458\n11. HSak,MSaraçlar,TGüngör,in ICASSP.Morphology-basedandsub-word\nlanguagemodelingforTurkishspeechrecognition,(Dallas,2010),\npp.5402–5405\n12. MYTachbelie,STAbate,WMenzel,in LanguageandTechnology\nConference.Morpheme-basedandfactoredlanguagemodelingfor\namharicspeechrecognition,(Pozna ˇn,2011),pp.82–93\n13. ZAlumae,in ICASSP.Sentence-adaptedfactoredlanguagemodelfor\ntranscribingestonianspeech,(Toulouse,2006),pp.429–432\n14. THirsimaki,JPylkkonen,MKurimo,ImportanceofHigh-OrderN-Gram\nModelsinMorph-BasedSpeechRecognition.IEEETrans.Audio,Speech,\nLang.Process. 17(4),724–732(2009)\n15. AEDMousa,MABShaik,RSchlüter,HNey,in INTERSPEECH.Morpheme\nbasedfactoredlanguagemodelsforGermanLVCSR,(Florence,2011),\npp.1053–1056\n16. HAdel,NTVu,KKirchhoff,DTelaar,TSchultz,SyntacticandSemantic\nFeaturesForCode-SwitchingFactoredLanguageModels.IEEE/ACM\nTrans.Audio,Speech,Lang.Process. 23(3),431–440(2015)\n17. TErjavec,in LREC.Multext-eastversion4:multilingualmorphosyntactic\nspecifications,lexiconsandcorpora,(Valletta,2010),pp.2544–2547\nDonajandKa ˇciˇcEURASIPJournalonAudio,Speech,andMusicProcessing  (2017) 2017:6 Page16of16\n18. TErjavec,DFišer,SKrek,NLedinek,in LREC.TheJOSLinguisticallytagged\ncorpusofSlovene,(Valletta,2010),pp.1806–1809\n19. SKatz,Estimationofprobabilitiesfromsparsedataforthelanguage\nmodelcomponentofaspeechrecognizer.IEEETrans.Acoust.Speech,\nSignalProcess. 35(3),400–401(1987)\n20. DKlakow,JPeters,Testingthecorrelationofworderrorrateand\nperplexity.SpeechCommun. 38(1–2),19–28(2002)\n21. AŽgank,DVerdonik,AZMarkuš,ZKa ˇciˇc,in INTERSPEECH.BNSISlovenian\nbroadcastnewsdatabase—speechandtextcorpus,(Lisbon,2005),\npp.1537–1540\n22. ŠArhar,VGorjanc,SKrek,in ProceedingsoftheCorpusLinguistics\nConference.FidaPLUScorpusofSlovenian.Thenewgenerationofthe\nSlovenianreferencecorpus:itsdesignandtools,(Birmingham,2007)\n23. AStolcke,JWheng,WWang,VAbrash,in ProceedingsIEEEAutomatic\nSpeechRecognitionandUnderstandingWorkshop .SRILMatsixteen:update\nandoutlook,(Waikoloa,2011)\n24. SFChen,JGoodman,Anempiricalstudyofsmoothingtechniquesfor\nlanguagemodeling.Comput.SpeechLang. 13(4),359–394(1999)\nSubmit your manuscript to a \njournal and beneﬁ t from:\n7 Convenient online submission\n7 Rigorous peer review\n7 Immediate publication on acceptance\n7 Open access: articles freely available online\n7 High visibility within the ﬁ  eld\n7 Retaining the copyright to your article\n    Submit your next manuscript at 7 springeropen.com",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.5786669850349426
    },
    {
      "name": "Computer science",
      "score": 0.474235475063324
    },
    {
      "name": "Language model",
      "score": 0.4609350264072418
    },
    {
      "name": "Natural language processing",
      "score": 0.4057333767414093
    },
    {
      "name": "Geography",
      "score": 0.18194296956062317
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37696226",
      "name": "University of Maribor",
      "country": "SI"
    }
  ],
  "cited_by": 6
}