{
  "title": "Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models",
  "url": "https://openalex.org/W4385571855",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2224734793",
      "name": "Borui Wang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2146202550",
      "name": "Qiuyuan Huang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2182581315",
      "name": "Budhaditya Deb",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A1965735689",
      "name": "Aaron Halfaker",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2313410784",
      "name": "Li-Qun Shao",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2008753996",
      "name": "Daniel McDuff",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2094223786",
      "name": "Ahmed Hassan Awadallah",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2913816252",
      "name": "Dragomir Radev",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2104437897",
      "name": "Jian-Feng Gao",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2996164352",
    "https://openalex.org/W3175218683",
    "https://openalex.org/W2912904516",
    "https://openalex.org/W4238779987",
    "https://openalex.org/W4280638382",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034457116",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4225386295",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4238805501",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W4294753225",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W2963084773",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4231112046"
  ],
  "abstract": "Borui Wang, Qiuyuan Huang, Budhaditya Deb, Aaron Halfaker, Liqun Shao, Daniel McDuff, Ahmed Hassan Awadallah, Dragomir Radev, Jianfeng Gao. Findings of the Association for Computational Linguistics: ACL 2023. 2023.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 1762–1773\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLogical Transformers: Infusing Logical Structures into\nPre-Trained Language Models\nBorui Wang1∗ Qiuyuan Huang2 Budhaditya Deb2 Aaron Halfaker2\nLiqun Shao2 Daniel McDuff3† Ahmed Hassan Awadallah2 Dragomir Radev1\nJianfeng Gao2\n1Yale University 2Microsoft Research 3University of Washington\nborui.wang@yale.edu\nAbstract\nNatural language contains rich logical struc-\ntures and logical information, and correctly\ndetecting and accurately understanding these\nlogical structures and information underlying\nnatural language texts is very crucial for NLP\nmodels’ performance on many important NLU\nand NLG tasks. Existing pre-trained language\nmodels based on the transformer architecture\nmostly adopt a classical design for constructing\ntheir input embeddings that ignores the logical\nstructures underlying natural language texts,\nthus limiting their ability to better capture and\nencode key logical information in the input se-\nquences. To overcome such limitations, in this\npaper we first propose a novel approach to con-\nstruct logic-aware input embeddings for trans-\nformer language models through a combination\nof logic detection, logic mapping and hierar-\nchical logical projections, and then develop a\ncorresponding new modeling paradigm that can\nupgrade existing transformer language models\ninto logical transformers to boost their perfor-\nmance on different NLU and NLG tasks. Our\nempirical experiments on four important and\nchallenging NLU and NLG tasks demonstrate\nthat our proposed logical transformer language\nmodels can achieve superior performance over\ntheir baseline transformer models through a\ndeeper understanding of the logical structures\nof texts.\n1 Introduction\nNatural language contains rich logical structures\nand logical information (Lakoff, 1970; Van Ben-\nthem, 1986) that are crucial to a deep and accurate\nunderstanding of its meaning. Therefore, the abil-\nity to correctly detect and accurately understand\nthe logical structures and information within natu-\nral language texts is very crucial for NLP models’\n∗This work was done when Borui Wang was a research\nintern at Microsoft Research.\n†This work was done when Daniel McDuff was at Mi-\ncrosoft Research.\nperformance on many important Natural Language\nUnderstanding (NLU) and Natural Language Gen-\neration (NLG) tasks.\nThe types of logics contained in natural language\nare very diverse, including not only mathemat-\nically well-defined propositional logic and first-\norder logic (Lu et al., 2022; Han et al., 2022), but\nalso more general types of natural and structural\nlogical relationships that people frequently use in\nnatural language texts to convey and communi-\ncate their ideas and meanings more effectively and\nclearly.\nIn recent years we have witnessed huge progress\nand success in many fields of natural language pro-\ncessing brought about by the introduction of all dif-\nferent kinds of pre-trained language models (Devlin\net al., 2019; Liu et al., 2019; Lan et al., 2020; Yang\net al., 2019; Clark et al., 2020; Lewis et al., 2020;\nRaffel et al., 2020; Zhang et al., 2020) based on\nthe transformer architecture (Vaswani et al., 2017).\nMost existing pre-trained language models adopt\nthe classical approach for constructing the input\nembeddings that are fed into the encoder parts of\nthe language models, which can be summarized as\nthe summation of the following three key compo-\nnents (Devlin et al., 2019):\n(1) Token Embeddings - that are used to encode\nand represent the semantics and meaning of\neach token in the vocabulary;\n(2) Position Embeddings - that are used to encode\nthe positional information of each token in the\ninput sequence;\n(3) Segment Embeddings - that are used to indi-\ncate which segment of the input sequence each\ntoken belongs to.\nThis classical design of the input embeddings\nhas been proven to be very effective at capturing\nimportant semantic and positional features from\n1762\nnatural language texts and helping pre-trained lan-\nguage models to learn good contextualized repre-\nsentations of the input textual sequences (Devlin\net al., 2019). However, it also has a very important\nlimitation - it doesn’t consider or try to explicitly\nencode the logical structures underlying the text\ninputs, which are also very crucial for the deep and\naccurate understanding of the meaning of the text\ninputs.\nTherefore, in order to overcome this limitation\nand to enable pre-trained language models to better\ncapture and understand the important logical struc-\ntures underlying natural language texts, in this pa-\nper we propose a novel approach to constructlogic-\naware input embeddingsfor transformer-based\npre-trained language models and a corresponding\nnew modeling framework that can upgrade existing\ntransformer language models into logical trans-\nformers to boost their performance on different\nNLU and NLG tasks.\nOur new approach consists of two major mod-\nules: (1) logic detection and mapping, and (2)\nmulti-layer hierarchical logical projections. It has\nthe following key advantages:\n• Strong Generalizability: Our proposed new\napproach for constructing logic-aware input\nembeddings doesn’t alter the main architec-\nture of transformer language models and only\nmodifies the input embeddings at the front\nend before they are fed into the encoder part\nof the language models. Therefore, our new\napproach enjoys strong generalizability and\ncan be smoothly added to many different pre-\ntrained language models based on the trans-\nformer architecture.\n• Consistent Boost in Model Performance: Our\nproposed new approach is empirically shown\nto consistently boost the performance of differ-\nent transformer language models on different\nNLU and NLG tasks.\n• Negligible Increase in Model Size: Our pro-\nposed new approach will only increase the\nnumber of parameters of transformer language\nmodels by a negligible amount.\n• Low Overhead on Training Time : Our pro-\nposed new approach will not significantly in-\ncrease the training time of transformer lan-\nguage models by a large amount. The major-\nity of the overhead in training time will come\nfrom the initial text processing steps of logic\ndetection and logic mapping, which only need\nto be executed once before the actual training\nepochs start.\n2 Logical Relationships and Keywords\nIn this work, we consider logical relationships in\nnatural language texts as the underlying relation-\nships among different language constituents that\ncarry meaningful information regarding logical un-\nderstanding and reasoning of the texts. In natu-\nral language, such logical relationships are usually\nindicated by logically-connective keywords and\nphrases. In this paper, we define a taxonomy of 12\nmost commonly seen types of logical relationships\nand their corresponding sets1 of logical keywords\n(including phrases2) for natural language3:\n1. Conjunction: a conjunction logical relation-\nship indicates that the two language con-\nstituents involved are presented jointly in ad-\ndition to each other. Its logical keywords are:\nand, as well, as well as, also, at\nthe same time.\n2. Disjunction: a disjunction logical relation-\nship indicates that the two language con-\nstituents involved are presented alternatively\nnext to each other. Its logical keyword is:\nor.\n3. Negation: a negation logical relationship in-\ndicates that the meaning of the language con-\nstituent mapped by it is negated. Its logical\nkeywords are:\nnot, no, none, n’t, nothing.\n4. Conditional: a conditional logical relation-\nship indicates that the content of one language\nconstituent is the premise of the content of\nanother language constituent. Its logical key-\nwords are:\n1The sets of logical keywords listed here are not necessarily\nthe most exhaustive sets that contain all possible keywords in\neach category, but rather serve as the preliminary and exemplar\nsets that can already cover the majority of the most frequently\nappearing logical keywords in real-world texts. These sets are\nopen to extension.\n2For conciseness, in this paper we will use the term‘logical\nkeywords’ to refer to both logical keywords and logical key\nphrases.\n3Here the logical keywords are all defined in English, but\nsimilar categorization of logical relationships and sets of logi-\ncal keywords can also be defined in other languages as well.\n1763\nthemforeasytoo\nthem\nfor themtoo easy\nbe too easy for them\nbe\nwould be too easy for them\nwouldit\nit\nit would be too easy for thembecause\nbecause it would be too easy for them\nthat waygo\nway\ngo that way\nto go that way\nto\nnot to go that way\nnot\nthat\nvery hard\nhardvery\ntry\ntry very hard not to go that way because it would be too easy for them\nI\nIBut\nBut I try very hard not to go that way because it would be too easy for them.\nContrastive\nNegation Causal\n.\nFigure 1: The constituency parse tree for the example sentence ‘But I try very hard not to go that way because it\nwould be too easy for them. ’generated by the Berkeley Neural Parser (Kitaev and Klein, 2018).\nif, as long as.\n5. Negative Conditional: a negative conditional\nlogical relationship indicates that the negation\nof the content of one language constituent is\nthe premise of the content of another language\nconstituent. Its logical keywords are:\nunless, otherwise.\n6. Analogy: an analogy logical relationship in-\ndicates that the content of one language con-\nstituent is analogous to the content of another\nlanguage constituent. Its logical keywords\nare:\nas if, as though, just as, just\nlike, likewise, similarly.\n7. Comparative: a comparative logical relation-\nship indicates that the two language compo-\nnents involved are presented in comparison to\neach other. Its logical keywords are:\nbut, however, in comparison, while,\nyet, rather than, unlike, on the\nother hand, in contrast, contrary to,\non the contrary.\n8. Adversative: an adversative logical relation-\nship indicates that the content of one language\nconstituent is adversative to the content of\nanother language constituent. Its logical key-\nwords are:\nnevertheless, nonetheless,\nnotwithstanding, although, though,\ndespite, despite of, in spite of,\nregardless of, albeit.\n9. Temporal: a temporal logical relationship in-\ndicates that the content of one language con-\nstituent signifies the time when the content of\nanother language constituent takes place. Its\nlogical keywords are:\nduring, after, in, when, since,\nbefore, as, as soon as, while, then,\nuntil, meanwhile.\n10. Causal: a causal logical relationship indicates\nthat the content of one language constituent is\nthe cause or reason for the content of another\nlanguage constituent. Its logical keywords\nare:\nbecause, thanks to, since, as a\nresult, in order to, as, therefore,\nhence, so that, due to, thus,\nconsequently, thereby, now that.\n11. Progression: a progression logical relation-\nship indicates that the content of one language\nconstituent goes one step further on top of the\ncontent of another language constituent. Its\nlogical keywords are:\nmoreover, furthermore, in addition,\nbesides.\n12. Example: an example logical relationship in-\ndicates that the content of one language con-\nstituent exemplifies the content of another lan-\nguage constituent. Its logical keywords are:\nfor example, as an example, like,\nsuch as, for instance, including.\n1764\nFigure 2: 2-dimensional PCA (Hotelling, 1933) projec-\ntion of the contextualized last-layer hidden state vectors\nfor 20 randomly sampled different occurrences of the\nlogical keyword ‘since’ encoded by the ALBERT model\n(Lan et al., 2020). Occurrences with the causal logical\nmeaning ‘because’ is colored in pink, and occurrences\nwith the temporal logical meaning ‘from a time in the\npast’ is colored in yellow.\nAs an example, we sample a news article from\nthe training set of the CNN/Dailymail dataset (Nal-\nlapati et al., 2016) and manually annotate the ap-\npearances of the above defined types of logical\nrelationship in the article. See Figure 6 for the an-\nnotation of the logical relationships in this example\narticle, where the logical keywords associated with\ndifferent logical relationships are highlighted with\ndifferent colors.\n2.1 Categorization of Logical Relationships\nAccording to how many logical components (in the\nform of text spans) are associated with each logical\nkeywords and how different logical components\nare mapped by the logical keywords, we categorize\nthe set of all logical keywords into three different\ncategories:\n2.1.1 Unary Logical Relationships\nThe logical keywords indicating unary logical re-\nlationships are those that each only maps to one\nsingle logical component (text span). For exam-\nple, most keywords of negation relationship and\nexample relationship are indicating unary logical\nrelationships, such as not, for example, such as, etc.\n2.1.2 Intrinsically-Mapped Binary Logical\nRelationships\nThe logical keywords indicating intrinsically-\nmapped binary logical relationships are those that\neach maps to two separate logical components (text\nspans) that are both contained within the parent\nsentence constituent of the logical keyword itself.\nFor example, most keywords of conjunction rela-\nAlgorithm 1 Logic Detection and Mapping\nInput: Sentence s\nConstituency parser C: S→T\nSet of logical keywords K\nOutput: List of logic mapping dictionaries M\n1: Run Cover sto obtain its constituency parse tree T(s)\n2: Nkey(s) ←[ ]\n3: M← [ ]\n4: for each constituent node nin T(S) do\n5: if str(n) ∈K then\n6: Nkey(s) ←Nkey(s) +n\n7: for nk in Nkey(s) do\n8: Dk ←{}\n9: Dk[‘keyword’] = str(nk)\n10: if str(nk) ∈KU then\n11: Dk[‘α’] = str( pa(nk) \\nk )\n12: else ifstr(nk) ∈KBin then\n13: Use str(nk) to segment str( pa(nk) ) into 3 seg-\nments: str( pa(nk) )= A+ str(nk) + B\n14: Dk[‘α’] = A, Dk[‘β’] = B\n15: else ifstr(nk) ∈KBex then\n16: if ∃pa(pa(nk)) then\n17: Dk[‘α’] = str( pa(pa(nk)) \\pa(nk) )\n18: Dk[‘β’] = str( pa(nk) \\nk )\n19: else if∃another sentence s′right before sthen\n20: Dk[‘α’] = s′, Dk[‘β’] = str( pa(nk) \\nk )\n21: else\n22: Dk[‘α’] = ∅, Dk[‘β’] = str( pa(nk) \\nk )\n23: M←M + Dk\n24: return M\ntionship and disjunction relationship are indicating\nintrinsically-mapped binary logical relationships,\nsuch as and, as well as, or, etc.\n2.1.3 Extrinsically-Mapped Binary Logical\nRelationships\nThe logical keywords indicating extrinsically-\nmapped binary logical relationships are those that\neach maps to two separate logical components (text\nspans) where one is contained within the parent\nsentence constituent of the logical keyword itself\nwhile the other is outside (usually appears before)\nthe span of this parent sentence constituent. For ex-\nample, most keywords of conditional, comparative,\ntemporal and causal relationships are indicating\nextrinsically-mapped binary logical relationships,\nsuch as if, but, during, because, etc.\n3 Logic Detection and Mapping\nIn this section, we describe our logic detection\nand mapping module based on keyword detection\nand constituency parsing. For each sentence sin\nthe source text, we first perform constituency pars-\ning (Kitaev and Klein, 2018) over sto obtain its\nconstituency parsing tree T(s). In this paper, we\nuse the Berkeley Neural Parser (Kitaev and Klein,\n2018) to perform constituency parsing.\n1765\nBut\nBut I try …….  them.“People …… months.\nkeyword\nparent\n<latexit sha1_base64=\"c16HAYs8sCFzjk6BmE3oMl7QOLk=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyK+LgFvHiMaB6QrGF20psMmZ1dZmaFsOQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7gkRwbVz32ymsrK6tbxQ3S1vbO7t75f2Dpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGN1O/9YRK81g+mHGCfkQHkoecUWOle/k46pUrbtWdgSwTLycVyFHvlb+6/ZilEUrDBNW647mJ8TOqDGcCJ6VuqjGhbEQH2LFU0gi1n81OnZATq/RJGCtb0pCZ+nsio5HW4yiwnRE1Q73oTcX/vE5qwis/4zJJDUo2XxSmgpiYTP8mfa6QGTG2hDLF7a2EDamizNh0SjYEb/HlZdI8q3oX1fO780rtOo+jCEdwDKfgwSXU4Bbq0AAGA3iGV3hzhPPivDsf89aCk88cwh84nz9TEI3P</latexit>\nn\nk\n<latexit sha1_base64=\"yRo1ANztdtvHJ8g6hRyreBPT7qI=\">AAAB9XicbVBNS8NAEJ34WetX1aOXYBHqpSRS/LgVvHisYD+gTctmu2mXbjZhd6KW0P/hxYMiXv0v3vw3btsctPXBwOO9GWbm+bHgGh3n21pZXVvf2Mxt5bd3dvf2CweHDR0lirI6jUSkWj7RTHDJ6shRsFasGAl9wZr+6GbqNx+Y0jyS9ziOmReSgeQBpwSN1O0ge8I0JpOS7I7OeoWiU3ZmsJeJm5EiZKj1Cl+dfkSTkEmkgmjddp0YvZQo5FSwSb6TaBYTOiID1jZUkpBpL51dPbFPjdK3g0iZkmjP1N8TKQm1Hoe+6QwJDvWiNxX/89oJBldeymWcIJN0vihIhI2RPY3A7nPFKIqxIYQqbm616ZAoQtEElTchuIsvL5PGedm9KFfuKsXqdRZHDo7hBErgwiVU4RZqUAcKCp7hFd6sR+vFerc+5q0rVjZzBH9gff4AeAeSeA==</latexit>\npa( n\nk\n)\nthe previous sentence\n<latexit sha1_base64=\"3VILl0VJCn+UrVoMU98Lpger7F4=\">AAAB6XicbVDLSgNBEOz1GeMr6tHLYBA9hV0JPm4BLx6jmAckS5id9CZDZmeXmVkhLPkDLx4U8eofefNvnCR70MSChqKqm+6uIBFcG9f9dlZW19Y3Ngtbxe2d3b390sFhU8epYthgsYhVO6AaBZfYMNwIbCcKaRQIbAWj26nfekKleSwfzThBP6IDyUPOqLHSgz7rlcpuxZ2BLBMvJ2XIUe+Vvrr9mKURSsME1brjuYnxM6oMZwInxW6qMaFsRAfYsVTSCLWfzS6dkFOr9EkYK1vSkJn6eyKjkdbjKLCdETVDvehNxf+8TmrCaz/jMkkNSjZfFKaCmJhM3yZ9rpAZMbaEMsXtrYQNqaLM2HCKNgRv8eVl0ryoeJeV6n21XLvJ4yjAMZzAOXhwBTW4gzo0gEEIz/AKb87IeXHenY9564qTzxzBHzifPz9gjSg=</latexit>\ns\n0\n<latexit sha1_base64=\"6MmxRebE4Cm2FawSQk71fs/islo=\">AAACBXicdVDLSgNBEJz1bXxFPephMAh6CZsHbnITvHiMYFTIxjA76dUhs7PLTK8Ylly8+CtePCji1X/w5t84eQgqWtBQU9XNdFeQSGHQdT+cqemZ2bn5hcXc0vLK6lp+fePMxKnm0OSxjPVFwAxIoaCJAiVcJBpYFEg4D3pHQ//8BrQRsTrFfgLtiF0pEQrO0Eqd/LaPcItZwgZ76rK3T30DGAmVGmqfnXzBLXpevVz1qFt0R7CkUqtX6jVamigFMkGjk3/3uzFPI1DIJTOmVXITbGdMo+ASBjk/NZAw3mNX0LJUsQhMOxtdMaC7VunSMNa2FNKR+n0iY5Ex/SiwnRHDa/PbG4p/ea0Uw1o7EypJERQffxSmkmJMh5HQrtDAUfYtYVwLuyvl10wzjja4nA3h61L6PzkrF0sHxepJtXBYn8SxQLbIDtkjJeKRQ3JMGqRJOLkjD+SJPDv3zqPz4ryOW6ecycwm+QHn7RPZvpjM</latexit>\npa( n\nk\n) \\ n\nk\n<latexit sha1_base64=\"/FWAmq+z3GXrvBxyQ76jnWxzyzU=\">AAAB7HicdVBNS8NAEN3Ur1q/qh69LBbBU0jaYtpbwYvHCqYttKFsttt26WYTdidCCf0NXjwo4tUf5M1/47aNoKIPBh7vzTAzL0wE1+A4H1ZhY3Nre6e4W9rbPzg8Kh+fdHScKsp8GotY9UKimeCS+cBBsF6iGIlCwbrh7Hrpd++Z0jyWdzBPWBCRieRjTgkYyR+EDMiwXHFsz2tW6x52bGcFQ2qNZq3ZwG6uVFCO9rD8PhjFNI2YBCqI1n3XSSDIiAJOBVuUBqlmCaEzMmF9QyWJmA6y1bELfGGUER7HypQEvFK/T2Qk0noehaYzIjDVv72l+JfXT2HcCDIukxSYpOtF41RgiPHyczziilEQc0MIVdzciumUKELB5FMyIXx9iv8nnartXtn123ql1czjKKIzdI4ukYs81EI3qI18RBFHD+gJPVvSerRerNd1a8HKZ07RD1hvnzH1jvA=</latexit>\n\u0000 component\n<latexit sha1_base64=\"09EUrHkbMFZHIm6QLxcEqraGpd8=\">AAAB7XicdVBNS8NAEN3Ur1q/qh69LBbBU0jaYppbwYvHCrYW2lAm2027drMJuxuhlP4HLx4U8er/8ea/cdtGUNEHA4/3ZpiZF6acKe04H1ZhbX1jc6u4XdrZ3ds/KB8edVSSSULbJOGJ7IagKGeCtjXTnHZTSSEOOb0NJ5cL//aeSsUScaOnKQ1iGAkWMQLaSJ0+8HQMg3LFsT3Pr9Y97NjOEobUGn7Nb2A3VyooR2tQfu8PE5LFVGjCQame66Q6mIHUjHA6L/UzRVMgExjRnqECYqqC2fLaOT4zyhBHiTQlNF6q3ydmECs1jUPTGYMeq9/eQvzL62U6agQzJtJMU0FWi6KMY53gxet4yCQlmk8NASKZuRWTMUgg2gRUMiF8fYr/J52q7V7Y9et6penncRTRCTpF58hFHmqiK9RCbUTQHXpAT+jZSqxH68V6XbUWrHzmGP2A9fYJ+ZGPZA==</latexit>\n↵ component\n(a) Execution of Algorithm 1 over the logical keyword ‘But’.\nbecause\nbecause it would be too easy for them\ntry very hard not to go that way because it would be too easy for them\nparent\nparent\nkeyword\n<latexit sha1_base64=\"c16HAYs8sCFzjk6BmE3oMl7QOLk=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyK+LgFvHiMaB6QrGF20psMmZ1dZmaFsOQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7gkRwbVz32ymsrK6tbxQ3S1vbO7t75f2Dpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGN1O/9YRK81g+mHGCfkQHkoecUWOle/k46pUrbtWdgSwTLycVyFHvlb+6/ZilEUrDBNW647mJ8TOqDGcCJ6VuqjGhbEQH2LFU0gi1n81OnZATq/RJGCtb0pCZ+nsio5HW4yiwnRE1Q73oTcX/vE5qwis/4zJJDUo2XxSmgpiYTP8mfa6QGTG2hDLF7a2EDamizNh0SjYEb/HlZdI8q3oX1fO780rtOo+jCEdwDKfgwSXU4Bbq0AAGA3iGV3hzhPPivDsf89aCk88cwh84nz9TEI3P</latexit>\nn\nk\n<latexit sha1_base64=\"yRo1ANztdtvHJ8g6hRyreBPT7qI=\">AAAB9XicbVBNS8NAEJ34WetX1aOXYBHqpSRS/LgVvHisYD+gTctmu2mXbjZhd6KW0P/hxYMiXv0v3vw3btsctPXBwOO9GWbm+bHgGh3n21pZXVvf2Mxt5bd3dvf2CweHDR0lirI6jUSkWj7RTHDJ6shRsFasGAl9wZr+6GbqNx+Y0jyS9ziOmReSgeQBpwSN1O0ge8I0JpOS7I7OeoWiU3ZmsJeJm5EiZKj1Cl+dfkSTkEmkgmjddp0YvZQo5FSwSb6TaBYTOiID1jZUkpBpL51dPbFPjdK3g0iZkmjP1N8TKQm1Hoe+6QwJDvWiNxX/89oJBldeymWcIJN0vihIhI2RPY3A7nPFKIqxIYQqbm616ZAoQtEElTchuIsvL5PGedm9KFfuKsXqdRZHDo7hBErgwiVU4RZqUAcKCp7hFd6sR+vFerc+5q0rVjZzBH9gff4AeAeSeA==</latexit>\npa( n\nk\n)\n<latexit sha1_base64=\"nwBqW746ehyTlRPecCnOdhJUuVk=\">AAACAnicbZDLSgMxFIYz9VbrbdSVuAkWod2UGSledgU3LivYC7RjyaRpG5rJDMkZsQzFja/ixoUibn0Kd76NaTugtv4Q+PjPOZyc348E1+A4X1ZmaXlldS27ntvY3NresXf36jqMFWU1GopQNX2imeCS1YCDYM1IMRL4gjX84eWk3rhjSvNQ3sAoYl5A+pL3OCVgrI590AZ2D0lExoUfkrfDYrFj552SMxVeBDeFPEpV7dif7W5I44BJoIJo3XKdCLyEKOBUsHGuHWsWETokfdYyKEnAtJdMTxjjY+N0cS9U5knAU/f3REICrUeBbzoDAgM9X5uY/9VaMfTOvYTLKAYm6WxRLxYYQjzJA3e5YhTEyAChipu/YjogilAwqeVMCO78yYtQPym5p6XydTlfuUjjyKJDdIQKyEVnqIKuUBXVEEUP6Am9oFfr0Xq23qz3WWvGSmf20R9ZH99H65dS</latexit>\npa(pa( n\nk\n))\n<latexit sha1_base64=\"P+68BN5f6KEe2nuR729SPQreEq8=\">AAACG3icdVDLSgNBEJz1bXxFPXoZDIJell1NYvYmePEYwaiQxDA76eiQ2dllplcMS/7Di7/ixYMingQP/o2Th/hACxqKqm66u8JECoOe9+5MTE5Nz8zOzecWFpeWV/Kra6cmTjWHGo9lrM9DZkAKBTUUKOE80cCiUMJZ2D0c+GfXoI2I1Qn2EmhG7FKJjuAMrdTK7zYQbjBLWH/7i6mL7s4ObRjASKjU0J9OK1/wXK9cDkpF6rml0p5fDCwJgkrJ96jvekMUyBjVVv610Y55GoFCLpkxdd9LsJkxjYJL6OcaqYGE8S67hLqlikVgmtnwtz7dskqbdmJtSyEdqt8nMhYZ04tC2xkxvDK/vYH4l1dPsVNpZkIlKYLio0WdVFKM6SAo2hYaOMqeJYxrYW+l/IppxtHGmbMhfH5K/yenu65fdovHxcJBMI5jjmyQTbJNfLJPDsgRqZIa4eSW3JNH8uTcOQ/Os/Myap1wxjPr5Aectw9CMKIr</latexit>\npa(pa( n\nk\n)) \\ pa( n\nk\n)\n<latexit sha1_base64=\"BuT9ZmLmagCyRtDmRN68DIgOo3w=\">AAACBXicdVA9SwNBEN3z2/gVtdRiMQjaHHeaxFwn2FhGMCokMextJrpkb+/YnRPDkcbGv2JjoYit/8HOf+PmQ1DRBwNv35thZ16YSGHQ8z6cicmp6ZnZufncwuLS8kp+de3MxKnmUOOxjPVFyAxIoaCGAiVcJBpYFEo4D7tHA//8BrQRsTrFXgLNiF0p0RGcoZVa+c0Gwi1mCevvqMvuLm0YwEio1FD7bOULnuuVy0GpSD23VNr3i4ElQVAp+R71XW+IAhmj2sq/N9oxTyNQyCUzpu57CTYzplFwCf1cIzWQMN5lV1C3VLEITDMbXtGn21Zp006sbSmkQ/X7RMYiY3pRaDsjhtfmtzcQ//LqKXYqzUyoJEVQfPRRJ5UUYzqIhLaFBo6yZwnjWthdKb9mmnG0weVsCF+X0v/J2Z7rl93iSbFwGIzjmCMbZIvsEJ8ckENyTKqkRji5Iw/kiTw7986j8+K8jlonnPHMOvkB5+0T7LaY2Q==</latexit>\npa( n\nk\n) \\ n\nk\ncomponent\n<latexit sha1_base64=\"fGl0e7nb19o0zIPEejxqfbip4uQ=\">AAAB7XicdVDLSgMxFM34rPVVdekmWARXQ0ZnamdXcOOygn1AO5RMmraxmWRIMkIZ+g9uXCji1v9x59+YPgQVPXDhcM693HtPnHKmDUIfzsrq2vrGZmGruL2zu7dfOjhsapkpQhtEcqnaMdaUM0EbhhlO26miOIk5bcXjq5nfuqdKMyluzSSlUYKHgg0YwcZKzS7m6Qj3SmXkokolDHyI3CC48PzQkjCsBh6CnovmKIMl6r3Se7cvSZZQYQjHWnc8lJoox8owwum02M00TTEZ4yHtWCpwQnWUz6+dwlOr9OFAKlvCwLn6fSLHidaTJLadCTYj/dubiX95ncwMqlHORJoZKshi0SDj0Eg4ex32maLE8IklmChmb4VkhBUmxgZUtCF8fQr/J81z16u4/o1froXLOArgGJyAM+CBS1AD16AOGoCAO/AAnsCzI51H58V5XbSuOMuZI/ADztsnDJiPcQ==</latexit>\n↵\n<latexit sha1_base64=\"1t+eZUYFJcW/98Ub0gkUDmryg+o=\">AAAB7HicdVDLSsNAFJ3UV62vqks3g0VwFSaa1GZXcOOygmkLbSiT6aQdOpmEmYlQQr/BjQtF3PpB7vwbpw9BRQ9cOJxzL/feE2WcKY3Qh1VaW9/Y3CpvV3Z29/YPqodHbZXmktCApDyV3QgrypmggWaa024mKU4iTjvR5Hrud+6pVCwVd3qa0TDBI8FiRrA2UtCPqMaDag3ZqF73PRci2/MuHdc3xPcbnoOgY6MFamCF1qD63h+mJE+o0IRjpXoOynRYYKkZ4XRW6eeKZphM8Ij2DBU4oSosFsfO4JlRhjBOpSmh4UL9PlHgRKlpEpnOBOux+u3Nxb+8Xq7jRlgwkeWaCrJcFOcc6hTOP4dDJinRfGoIJpKZWyEZY4mJNvlUTAhfn8L/SfvCduq2e+vWmv4qjjI4AafgHDjgCjTBDWiBABDAwAN4As+WsB6tF+t12VqyVjPH4Aest09E7Y79</latexit>\n\u0000 component\n(b) Execution of Algorithm 1 over the logical keyword ‘be-\ncause’.\nnot to go that way\nnot\nkeyword\n<latexit sha1_base64=\"UFwEZuk2FaxYq1aloAXzOin9fss=\">AAAB83icbVDLSgNBEOz1GeMr6tHLYhCSS9iVoJ4k4MVjBPOAZA2zk9lkyOzsMNMrhJDf8OJBEa/+jDf/xkmyB00saCiquunuCpXgBj3v21lb39jc2s7t5Hf39g8OC0fHTZOkmrIGTUSi2yExTHDJGshRsLbSjMShYK1wdDvzW09MG57IBxwrFsRkIHnEKUErdQ3qkiIl+Tgql3uFolfx5nBXiZ+RImSo9wpf3X5C05hJpIIY0/E9hcGEaORUsGm+mxqmCB2RAetYKknMTDCZ3zx1z63Sd6NE25LoztXfExMSGzOOQ9sZExyaZW8m/ud1UoyugwmXKkUm6WJRlAoXE3cWgNvnmlEUY0sI1dze6tIh0YSijSlvQ/CXX14lzYuKf1mp3leLtZssjhycwhmUwIcrqMEd1KEBFBQ8wyu8Oanz4rw7H4vWNSebOYE/cD5/APXskPo=</latexit>\nstr ( pa ( n\nk\n))\n<latexit sha1_base64=\"6UTCCfj3P0ZZiqAfdHVTj0kw6TQ=\">AAAB9XicbVBNS8NAEJ34WetX1aOXYBHqpSRS1JMUvHisYD+gTctmu2mXbjZhd6KW0P/hxYMiXv0v3vw3btsctPXBwOO9GWbm+bHgGh3n21pZXVvf2Mxt5bd3dvf2CweHDR0lirI6jUSkWj7RTHDJ6shRsFasGAl9wZr+6GbqNx+Y0jyS9ziOmReSgeQBpwSN1O0ge8I0JpOS7I7OeoWiU3ZmsJeJm5EiZKj1Cl+dfkSTkEmkgmjddp0YvZQo5FSwSb6TaBYTOiID1jZUkpBpL51dPbFPjdK3g0iZkmjP1N8TKQm1Hoe+6QwJDvWiNxX/89oJBldeymWcIJN0vihIhI2RPY3A7nPFKIqxIYQqbm616ZAoQtEElTchuIsvL5PGedm9KFfuKsXqdRZHDo7hBErgwiVU4RZqUAcKCp7hFd6sR+vFerc+5q0rVjZzBH9gff4AeYiSfQ==</latexit>\npa( n\nk\n)\n<latexit sha1_base64=\"yRo1ANztdtvHJ8g6hRyreBPT7qI=\">AAAB9XicbVBNS8NAEJ34WetX1aOXYBHqpSRS/LgVvHisYD+gTctmu2mXbjZhd6KW0P/hxYMiXv0v3vw3btsctPXBwOO9GWbm+bHgGh3n21pZXVvf2Mxt5bd3dvf2CweHDR0lirI6jUSkWj7RTHDJ6shRsFasGAl9wZr+6GbqNx+Y0jyS9ziOmReSgeQBpwSN1O0ge8I0JpOS7I7OeoWiU3ZmsJeJm5EiZKj1Cl+dfkSTkEmkgmjddp0YvZQo5FSwSb6TaBYTOiID1jZUkpBpL51dPbFPjdK3g0iZkmjP1N8TKQm1Hoe+6QwJDvWiNxX/89oJBldeymWcIJN0vihIhI2RPY3A7nPFKIqxIYQqbm616ZAoQtEElTchuIsvL5PGedm9KFfuKsXqdRZHDo7hBErgwiVU4RZqUAcKCp7hFd6sR+vFerc+5q0rVjZzBH9gff4AeAeSeA==</latexit>\npa( n\nk\n)\n<latexit sha1_base64=\"c16HAYs8sCFzjk6BmE3oMl7QOLk=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyK+LgFvHiMaB6QrGF20psMmZ1dZmaFsOQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7gkRwbVz32ymsrK6tbxQ3S1vbO7t75f2Dpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGN1O/9YRK81g+mHGCfkQHkoecUWOle/k46pUrbtWdgSwTLycVyFHvlb+6/ZilEUrDBNW647mJ8TOqDGcCJ6VuqjGhbEQH2LFU0gi1n81OnZATq/RJGCtb0pCZ+nsio5HW4yiwnRE1Q73oTcX/vE5qwis/4zJJDUo2XxSmgpiYTP8mfa6QGTG2hDLF7a2EDamizNh0SjYEb/HlZdI8q3oX1fO780rtOo+jCEdwDKfgwSXU4Bbq0AAGA3iGV3hzhPPivDsf89aCk88cwh84nz9TEI3P</latexit>\nn\nk\nparent\n<latexit sha1_base64=\"NtTTl0iLeaz13zfZPKc9PgrG9jQ=\">AAACAnicbVDLTgIxFO3gC/GFujJuGsHEFZkhILAjceMSE3kkQEinFGjotJP2jpFMiBt/xY0LjXHrV7jzbyyPhaInaXJyzj25vccPBTfgul9OYm19Y3MruZ3a2d3bP0gfHjWMijRldaqE0i2fGCa4ZHXgIFgr1IwEvmBNf3w185t3TBuu5C1MQtYNyFDyAacErNRLn2Q7RIQjksUdYPcQUxWESjIJ01464+bKxVKxUMBuruDlK+WKJe4c2FuSDFqi1kt/dvqKRoENU0GMaXtuCN2YaOBUsGmqExkWEjomQ9a2VJKAmW48P2GKz63SxwOl7ZOA5+rPREwCYyaBbycDAiOz6s3E/7x2BINyN+YyjIBJulg0iAQGhWd94D7XjIKYWEKo5vavmI6IJhRsaylbgrd68l/SyOe8y1zhJp+pVpZ1JNEpOkMXyEMlVEXXqIbqiKIH9IRe0Kvz6Dw7b877YjThLDPH6Becj2+ltZeb</latexit>\n↵ component\n<latexit sha1_base64=\"eVkztoFe5zAyggC3GUB8oOuMoq4=\">AAAB7XicbVBNS8NAEN34WetX1aOXxSJ4CklJbXsrePFYwX5AG8pku2nXbjZhdyOU0P/gxYMiXv0/3vw3btsctPpg4PHeDDPzgoQzpR3ny9rY3Nre2S3sFfcPDo+OSyenHRWnktA2iXksewEoypmgbc00p71EUogCTrvB9Gbhdx+pVCwW93qWUD+CsWAhI6CN1BkATyYwLJUdu16tVT0PO7bnVhr1hiHOEtjNSRnlaA1Ln4NRTNKICk04KNV3nUT7GUjNCKfz4iBVNAEyhTHtGyogosrPltfO8aVRRjiMpSmh8VL9OZFBpNQsCkxnBHqi1r2F+J/XT3VY9zMmklRTQVaLwpRjHePF63jEJCWazwwBIpm5FZMJSCDaBFQ0IbjrL/8lnYrtXtvenVduNvI4CugcXaAr5KIaaqJb1EJtRNADekIv6NWKrWfrzXpftW5Y+cwZ+gXr4xvsWo9a</latexit>\n↵ component\n<latexit sha1_base64=\"Dsco2EY4KjZJ3ZkeSDD+Q8MR0XE=\">AAACBXicbVA9SwNBEN3z2/h1aqnFYhBiE+7k1KQL2FhGMFFIYtjbTHTJ3t6xOyeGI42Nf8XGQhFb/4Od/8bNR6HRBwNv35thZ16YSGHQ876cmdm5+YXFpeXcyura+oa7uVU3cao51HgsY30VMgNSKKihQAlXiQYWhRIuw97p0L+8A21ErC6wn0ArYjdKdAVnaKW2u9tEuMcsYYOCuu4d0KYBjIRKDbXPtpv3iqWjk6MgoF4x8A/LpbIl3gjUn5A8maDadj+bnZinESjkkhnT8L0EWxnTKLiEQa6ZGkgY77EbaFiqWASmlY2uGNB9q3RoN9a2FNKR+nMiY5Ex/Si0nRHDWzPtDcX/vEaK3VIrEypJERQff9RNJcWYDiOhHaGBo+xbwrgWdlfKb5lmHG1wORuCP33yX1I/LPrHxeA8yFfKkziWyA7ZIwXikxNSIWekSmqEkwfyRF7Iq/PoPDtvzvu4dcaZzGyTX3A+vgHMh5jC</latexit>\npa( n\nk\n) \\ n\nk\n(c) Execution of Algorithm 1 over the logical keyword ‘not’.\nFigure 3: An example execution of Algorithm 1 on the\nexample sentence ‘But I try very hard not to go that way\nbecause it would be too easy for them. ’over the three\ndetected logical keywords ‘But’, ‘because’, ‘not’.\nThen we search through all the constituent nodes\nin T(s) to detect the ones that exactly matches\nthe keyword strings of the logical keywords as de-\nfined in Section 2. Let Nkey(s) denote the set\nof constituent node in T(s) that matches logical\nkeywords. Then for each logical keyword node\nnk ∈Nkey(s), we fetch its parent constituent node\npa(nk). Now we have three different cases:\n1. If nk corresponds to a unary logical relation-\nship (i.e. negation and example), then the α\ncomponent of nk is detected as: pa(nk) \\nk.\nBut becausenotI try very hard to go that way it would be too easy for them  .\nnegation\n<latexit sha1_base64=\"/FWAmq+z3GXrvBxyQ76jnWxzyzU=\">AAAB7HicdVBNS8NAEN3Ur1q/qh69LBbBU0jaYtpbwYvHCqYttKFsttt26WYTdidCCf0NXjwo4tUf5M1/47aNoKIPBh7vzTAzL0wE1+A4H1ZhY3Nre6e4W9rbPzg8Kh+fdHScKsp8GotY9UKimeCS+cBBsF6iGIlCwbrh7Hrpd++Z0jyWdzBPWBCRieRjTgkYyR+EDMiwXHFsz2tW6x52bGcFQ2qNZq3ZwG6uVFCO9rD8PhjFNI2YBCqI1n3XSSDIiAJOBVuUBqlmCaEzMmF9QyWJmA6y1bELfGGUER7HypQEvFK/T2Qk0noehaYzIjDVv72l+JfXT2HcCDIukxSYpOtF41RgiPHyczziilEQc0MIVdzciumUKELB5FMyIXx9iv8nnartXtn123ql1czjKKIzdI4ukYs81EI3qI18RBFHD+gJPVvSerRerNd1a8HKZ07RD1hvnzH1jvA=</latexit>\n\u0000comparative\ncausal\n<latexit sha1_base64=\"fGl0e7nb19o0zIPEejxqfbip4uQ=\">AAAB7XicdVDLSgMxFM34rPVVdekmWARXQ0ZnamdXcOOygn1AO5RMmraxmWRIMkIZ+g9uXCji1v9x59+YPgQVPXDhcM693HtPnHKmDUIfzsrq2vrGZmGruL2zu7dfOjhsapkpQhtEcqnaMdaUM0EbhhlO26miOIk5bcXjq5nfuqdKMyluzSSlUYKHgg0YwcZKzS7m6Qj3SmXkokolDHyI3CC48PzQkjCsBh6CnovmKIMl6r3Se7cvSZZQYQjHWnc8lJoox8owwum02M00TTEZ4yHtWCpwQnWUz6+dwlOr9OFAKlvCwLn6fSLHidaTJLadCTYj/dubiX95ncwMqlHORJoZKshi0SDj0Eg4ex32maLE8IklmChmb4VkhBUmxgZUtCF8fQr/J81z16u4/o1froXLOArgGJyAM+CBS1AD16AOGoCAO/AAnsCzI51H58V5XbSuOMuZI/ADztsnDJiPcQ==</latexit>\n↵\n<latexit sha1_base64=\"1t+eZUYFJcW/98Ub0gkUDmryg+o=\">AAAB7HicdVDLSsNAFJ3UV62vqks3g0VwFSaa1GZXcOOygmkLbSiT6aQdOpmEmYlQQr/BjQtF3PpB7vwbpw9BRQ9cOJxzL/feE2WcKY3Qh1VaW9/Y3CpvV3Z29/YPqodHbZXmktCApDyV3QgrypmggWaa024mKU4iTjvR5Hrud+6pVCwVd3qa0TDBI8FiRrA2UtCPqMaDag3ZqF73PRci2/MuHdc3xPcbnoOgY6MFamCF1qD63h+mJE+o0IRjpXoOynRYYKkZ4XRW6eeKZphM8Ij2DBU4oSosFsfO4JlRhjBOpSmh4UL9PlHgRKlpEpnOBOux+u3Nxb+8Xq7jRlgwkeWaCrJcFOcc6hTOP4dDJinRfGoIJpKZWyEZY4mJNvlUTAhfn8L/SfvCduq2e+vWmv4qjjI4AafgHDjgCjTBDWiBABDAwAN4As+WsB6tF+t12VqyVjPH4Aest09E7Y79</latexit>\n\u0000causal\nFigure 4: Detected logical structure for an example\nsentence ‘But I try very hard not to go that way because\nit would be too easy for them. ’taken from the example\narticle in Figure 6.\n2. If nk corresponds to a binary logical re-\nlationship and the relationship is intrinsi-\ncally mapped, then str(pa(nk)) will be di-\nvided by str(nk) into three different segments:\nstr(pa(nk)) = A + str(nk) + B. Now the α\ncomponent of nk is detected as Aand the β\ncomponent of nk is detected as B.\n3. If nk corresponds to a binary logical rela-\ntionship and the relationship is extrinsically\nmapped, then the α component of nk is de-\ntected as: pa(pa(nk))\\pa(nk), and the βcom-\nponent of nk is detected as: pa(nk) \\nk.\nOur proposed methods for logic detection and\nmapping described above are summarized in Algo-\nrithm 1. See Figure 3 for an example of executing\nAlgorithm 1 on an example sentence taken from\nthe example article in Figure 6, based on the con-\nstituency parsing tree depicted in Figure 1.\n3.1 Sense Disambiguation of Logical\nKeywords\nIn English, certain logical keywords have multiple\nmeanings and can indicate different logical rela-\ntionships under different contexts. For example,\nthe logical keyword ‘since’ has two different mean-\nings: (1) ‘ from a time in the past ’, which indi-\ncates a temporal logical relationship; (2) ‘because’,\nwhich indicates a causal logical relationship. In\nour categorization of logical relationships and key-\nwords (described in Section 2), there are a total of\n3 keywords that can have multiple logical mean-\nings: since, as, and while. Therefore, in order to\nincrease accuracy of our proposed logic detection\nmodule, we need to first perform accurate logical\nsense disambiguation when we detect these logi-\ncally ambiguous keywords.\nIn our empirical experiments over a set of ran-\ndomly sampled sentences that contain ambiguous\nlogical keywords, each manually-labelled with its\nground-truth logical relationship under the context,\n1766\nwe found that different uses of ambiguous logical\nkeywords have very strong clustering tendency and\nare largely linearly-separable under the contextu-\nalized encoding of transformer language models.\nFor example, we use the ALBERT model (Lan\net al., 2020) to encode 20 different occurrences\nof the logical keyword ‘since’ randomly sampled\nfrom the CNN/Dailymail dataset (Nallapati et al.,\n2016), and project the last-layer hidden state vec-\ntors for these 20 ‘since’ onto their first two principal\ncomponents using Principal Component Analysis\n(PCA) (Hotelling, 1933), which is depicted in Fig-\nure 2. As we can see from Figure 2 the contextual-\nized embeddings of the logical keyword ‘since’ are\nlargely linearly separable between the two different\nlogical meanings.\nTherefore, in order to improve the accuracy of\nour logic detection module, we first manually col-\nlected logical relationship annotations for the set\nof ambiguous logical keywords in English. Then\nwe encode them using the ALBERT model (Lan\net al., 2020) and train individual support vector\nmachine (SVM) (Cortes and Vapnik, 1995) classi-\nfiers for each of the ambiguous logical keywords\nto accurately disambiguate their different logical\nmeanings.\n4 Logical Transformers\n4.1 Logical Embedding Vectors\nThe major new parameters that we introduce in\nour proposed modeling framework of logical trans-\nformers are a set of parametrized and trainable log-\nical embedding vectors. These logical embedding\nvectors share the same dimensionality, but their\ndimensionality doesn’t necessarily equal to the di-\nmensionality of the transformer language model’s\ntoken embedding vectors. Below we describe how\nto construct these logical embedding vectors in de-\ntail.\nFirst of all, the 12 types of logical relationships\nwe defined in Section 6 can be classified into two\ndifferent categories: (1) ‘unary logical relationship’\nthat maps to only one logical component; (2) ‘bi-\nnary logical relationship’ that maps to two logical\ncomponents. More specifically, negation and ex-\nample are unary logical relationships and all the\nother 10 types are binary logical relationships.\nFor each unary logical relationship U, we con-\nstruct two parametrized logical embedding vectors:\nvU\nkey and vU. In the logical embedding layer of U,\nwe assign vU\nkey to each token detected to be part of\nconcatenate\nlinear projection\nlinear projection\nTokeni’s \nToken Embedding\nlinear projection\nTokeni’s Logical \nEmbedding @ Level 3\n+\nTokeni’s Segment Embedding\n+\nTokeni’s Position Embedding\nTokeni+1\nTokeni\nTokeni-1\nTokenN\nToken1… …\nTokeni’s Logical \nEmbedding @ Level 2\nTokeni’s Logical \nEmbedding @ Level 1\nPre-Trained Language Model\nactivation\nactivation\nactivation\n+\nTokeni’s Token Embedding\nFigure 5: Illustration of our proposed multi-layer hier-\narchical logical projections for an example token with\nlogic depth K = 3.\nan appearance of some logical keyword in U, and\nassign vUto all the tokens that are within some text\nspan mapped by some logical keyword in U.\nFor each binary logical relationship B, we con-\nstruct three parametrized logical embedding vec-\ntors: vB\nkey, vB\nα and vB\nβ. In the logical embedding\nlayer of B, we assign vB\nkey to each token detected to\nbe part of an appearance of some logical keyword\nin B, assign vB\nα to all the tokens that are within\nsome left text span mapped by some logical key-\nword in B, and assign vB\nβ to all the tokens that\nare within some right text span mapped by some\nlogical keyword in B.\nAnd finally we construct another special\nparametrized logical embedding vector vEthat cor-\nresponds to empty logical association. For each to-\nken that doesn’t belong to any logical relationships\nin a logical embedding layer, it will be assigned vE\n1767\nLogical Sentence Tokens\nEmbeddingsBut I try very hard not to go that way bec. it wld be too easy for them .\nComparativekey β β β β β β β β β β β β β β β β β β\nCausal - - α α α α α α α α key β β β β β β β -\nNegation - - - - - key α α α α - - - - - - - - -\nTable 1: Illustration of our proposed multi-layer logical embeddings for an example sentence ‘But I try very hard not\nto go that way because it would be too easy for them. ’taken from the example article in Figure 6. The assignment\nof logical embedding vectors are based on the parsed logical structure depicted in Figure 4. In the second row the\ntoken ‘because’ is abbreviated into ‘bec.’ and the token ‘would’ is abbreviated into ‘wld’ due to space limit.\nfor this layer. See Table 1 for a concrete example\nof assigning multiple layers of logical embedding\nvectors to tokens in an input sequence based on the\nresults of logic detection and mapping.\nTherefore, based on the 12 different types of\nlogical relationships that we defined in Section 6,\nwe will construct a total of2 ×2 + 10×3 + 1 = 35\ndifferent logical embedding vectors for our logical\ntransformers.\n4.2 Multi-Layer Hierarchical Logical\nProjections\nNow we describe how to compute the logic-aware\ninput embeddings through multi-layer hierarchi-\ncal logical projections using the set of logical em-\nbedding vectors that we defined in Section 4.1.\nLet Nlogic denote the dimensionality of the logi-\ncal embedding vectors, and let N denote the di-\nmensionality of the token embedding vectors of\nthe transformer language model. We first define\na parametrized and trainable linear transformation\nlayer Lthat projects a (N + Nlogic)-dimensional\nvector into an N-dimensional vector.\nThen for each token t in the input token se-\nquence, we collect all the logical embedding vec-\ntors assigned to it during the logic detection and\nmapping process and sort them in order according\nto their associated logical keywords’ depth in the\nconstituency parse tree of the input sentence. Let’s\ndenote this sorted set of all the logical embedding\nvectors assigned to token tas: {v1\nt,...,v K\nt }, where\nKis the maximum number of logical layers to be\nconsidered and should be treated as a hyperparam-\neter.\nNow let’s denote the original token embedding\nvector for token tas wt, then to compute a logic-\naware token embedding vector wlogic\nt for t, we first\ninitialize u0\nt = wt, and then recursively apply the\nfollowing computation4:\n4This series of (linear projection + nonlinear activation)\ncan also be replaced by a series of multilayer perceptrons.\nui\nt = f(L(ui−1\nt ⊕vi\nt)),\nfor i = 1,...,K , where ⊕denotes vector con-\ncatenation and fis some non-linear activation func-\ntion, such as GELU (Hendrycks and Gimpel, 2016).\nThen we have:\nwlogic\nt = wt + uK\nt .\nNow let pt denote the position embedding vector\nof token tand st denote the segment embedding\nvector of token t, then the final logic-aware input\nembedding vector for each token tin the input se-\nquence would be computed as: wlogic\nt + pt + st.\nThen at the front end of our proposed logical trans-\nformers, we use these logic-aware input embed-\ndings to replace the traditional input embeddings\nand feed them into transformer encoders to help\nlanguage models better encode and learn logical\ninformation from the textual inputs. See Figure 5\nfor an illustration of multi-layer hierarchical logical\nprojections for an example token with logic depth\nK = 3.\n4.3 Model Training\nDuring the training of our proposed logical trans-\nformers, we set both the set of 35 logical embed-\nding vectors and the linear transformation layer L\nto be fully parametrized and trainable, and then ini-\ntialize them with random values. All these added\nnew parameters will be updated together with the\noriginal trainable parameters in the transformer lan-\nguage models during the model training process.\n4.4 Negligible Increase in Model Size\nThe only new parameters introduced in our pro-\nposed logical transformers, compared with their\ncorresponding baseline transformer language mod-\nels, are the set of 35 logical embedding vectors and\nthe linear transformation linear Lused in hierar-\nchical logical projections. Let Nlogic denote the di-\nmensionality of the logical embedding vectors, then\n1768\nReClor LogiQA DREAM\nModel Acc Acc Acc\nRoBERTa-large 62.6 35.3 82.1\nLogical-RoBERTa-large67.4 37.8 84.9\nTable 2: Our NLU experiment results on the ReClor\ndataset (Yu et al., 2020), the LogiQA dataset (Liu et al.,\n2020) and the DREAM dataset (Sun et al., 2019). Acc\ndenotes accuracy percentage. The higher value in each\npair of comparison is highlighted in bold.\nDialogSum\nModel R-1 R-2 R-L R-LSum\nBART-large 46.10 20.32 38.04 40.98\nLogical-BART-large46.97 20.69 38.33 41.30\nTable 3: Our NLG experiment results on the DialogSum\ndataset (Chen et al., 2021). The higher value in each\npair of comparison is highlighted in bold.\nthe total increase in model size can be calculated\nas: Nlogic×35 + (N+Nlogic)×Nlogic+Nlogic =\nN2\nlogic + N ·Nlogic + 36Nlogic.\nFor all the recently proposed transformer lan-\nguage models, this increase in model size is rather\nsmall and negligible compared with their very\nlarge number of parameters. For example, for\nthe RoBERTa-large model (Liu et al., 2019), its\ntotal number of parameters is 355M and the di-\nmensionality of its embedding vectors is 1024. If\nwe set Nlogic = 1024 as well, then after we use\nour proposed new modeling paradigm to upgrade\nRoBERTa-large into Logical-RoBERTa-large, the\npercentage of increase in model size is only:\n(10242 + 1024×1024 + 36×1024) ÷355M ≈\n0.601%, which is almost negligible. This efficiency\nin model size guarantees that the logical transform-\ners take roughly the same amount of computation\ntime during both training and inference as their\nbaseline transformer language models.\n5 Experiments\nIn order to evaluate our proposed logical trans-\nformer architecture’s performance boost on differ-\nent NLU and NLG tasks with different transformer\nlanguage models, in our experiments, we test it on\nthree NLU datasets and one NLG dataset.\n5.1 Natural Language Understanding Tasks\nIn the NLU part of our experiments, we test the\nRoBERTa model (Liu et al., 2019) and our Logical-\nRoBERTa model on three logically-challenging nat-\nural language understanding tasks over three corre-\nsponding datasets: (1) reading comprehension on\nthe ReClor dataset (Yu et al., 2020); (2) question\nanswering on the LogiQA dataset (Liu et al., 2020);\nand (3) dialogue-based reading comprehension on\nthe DREAM dataset (Sun et al., 2019). All of these\nthree datasets require logical reasoning.\n5.2 Natural Language Generation Task\nIn the NLG part of our experiments, we test the\nBART model (Lewis et al., 2020) and our Logical-\nBART model on the task of dialogue summariza-\ntion over the DialogSum (Chen et al., 2021) dataset.\n5.3 Results\nThe results of our three NLU experiments are\nshown in Table 2, and the results of NLG experi-\nment are shown in Table 3. As we can see from\nTable 2 and Table 3, the accuracy scores and the\nROUGE scores of our logical transformer language\nmodels are consistently higher than their corre-\nsponding baseline transformer language models\nacross all the different NLU and NLG tasks. This\nconsistent boost demonstrates that the important\nlogical structures and information extracted and\ncaptured by our proposed logical transformers are\nindeed very effective and useful in further improv-\ning transformer language models’ performance on\nlogically-challenging NLU and NLG tasks.\n6 Related Work\nRecently there has been increasing interest in im-\nproving pre-trained language models’ logical rea-\nsoning ability (Xu et al., 2022; Pi et al., 2022).\nFor example, Lu et al. (2022) proposed a new\nmethod for parsing natural language into the forms\nof propositional logic and first-order logic using\ndual reinforcement learning. Pi et al. (2022) pro-\nposed a new unsupervised adversarial pre-training\nmethod, called LogiGAN, in order to enhance lan-\nguage models’ abilities of logical reasoning. Xu\net al. (2022) proposed a new Logiformer archi-\ntecture based on a two-branch graph transformer\nnetwork to improve language models’ performance\non interpretable logical reasoning.\nIn contrast to these previous work that mostly\nfocus on introducing new training methods or con-\nstructing complex model architectures, our pro-\nposed method in this paper only modifies the in-\nput embeddings and is thus more straightforward\n1769\nLONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) \nfortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \n\"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young \nactor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those \npeople who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he \ntold an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are \nthings that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a \ndrink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box \noffice chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on \nhis plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" \nRadcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. \nDespite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always \nlooking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it \nwould be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is \nbreaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her \nreview of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy \nJack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" \nan Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured \nteenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \n\"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All \nrights reserved.This material may not be published, broadcast, rewritten, or redistributed.\nFigure 6: Detected logical keywords in an example article from the CNN/Dailymail dataset (Nallapati et al., 2016).\nIt contains 7 different types of logical relationships: conjunction, disjunction, negation, comparative, adversative,\ntemporal, and causal.\nand easily generalizable to different types of trans-\nformer language models.\n7 Conclusion\nIn this paper we introduced a new modeling\nparadigm for transformer language models that\ndetects and extracts important logical structures\nand information from input texts and then inte-\ngrates them into the input embeddings through\ncarefully designed multi-layer hierarchical logical\nprojections to infuse logical structures into pre-\ntrained language models. Our empirical experi-\nments on four important and challenging NLU and\nNLG tasks showed that our proposed logical trans-\nformer language models consistently perform bet-\nter than their corresponding baseline transformer\nlanguage models through a deeper understanding\nof the key logical structures underlying natural lan-\nguage texts.\n8 Limitations\nIn theory, the method proposed in this paper can\nbe applied to different types of transformer lan-\nguage models for both pre-training and fine-tuning.\nDue to limit of computational resource, we cur-\nrently haven’t had the chance to test our proposed\nmethod in the very promising setting of large-scale\nlanguage model pre-training yet. In future work,\nwe plan to further test our proposed logical trans-\nformer architecture on large-scale language model\npre-training to see how much performance boost it\ncan achieve.\nReferences\nYulong Chen, Yang Liu, Liang Chen, and Yue Zhang.\n2021. DialogSum: A real-life scenario dialogue sum-\nmarization dataset. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 5062–5074, Online. Association for Computa-\ntional Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn International Conference on Learning Representa-\ntions (ICLR).\nCorinna Cortes and Vladimir Naumovich Vapnik. 1995.\nSupport-vector networks. Machine Learning, 20:273–\n297.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n1770\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting\nQi, Martin Riddell, Luke Benson, Lucy Sun, Eka-\nterina Zubova, Yujie Qiao, Matthew Burtell, David\nPeng, Jonathan Fan, Yixin Liu, Brian Wong, Mal-\ncolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai,\nTao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fab-\nbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming\nXiong, and Dragomir R. Radev. 2022. Folio: Natu-\nral language reasoning with first-order logic. ArXiv,\nabs/2209.00840.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nHarold Hotelling. 1933. Analysis of a complex of sta-\ntistical variables into principal components. Journal\nof Educational Psychology, 24:498–520.\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2676–2686, Melbourne, Australia. Association\nfor Computational Linguistics.\nGeorge Lakoff. 1970. Linguistics and natural logic.\nSynthese.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Confer-\nence on Learning Representations (ICLR).\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang,\nYile Wang, and Yue Zhang. 2020. Logiqa: A chal-\nlenge dataset for machine reading comprehension\nwith logical reasoning. In Proceedings of the Twenty-\nNinth International Joint Conference on Artificial\nIntelligence, IJCAI-20, pages 3622–3628. Interna-\ntional Joint Conferences on Artificial Intelligence\nOrganization. Main track.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nXuantao Lu, Jingping Liu, Zhouhong Gu, Hanwen Tong,\nChenhao Xie, Junyang Huang, Yanghua Xiao, and\nWenguang Wang. 2022. Parsing natural language\ninto propositional and first-order logic with dual re-\ninforcement learning. In Proceedings of the 29th\nInternational Conference on Computational Linguis-\ntics, pages 5419–5431, Gyeongju, Republic of Korea.\nInternational Committee on Computational Linguis-\ntics.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nCaglar Gulcehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nXinyu Pi, Wanjun Zhong, Yan Gao, Nan Duan, and\nJian-Guang Lou. 2022. Logigan: Learning logi-\ncal reasoning via adversarial pre-training. ArXiv,\nabs/2205.08794.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,\nand Claire Cardie. 2019. DREAM: A challenge data\nset and models for dialogue-based reading compre-\nhension. Transactions of the Association for Compu-\ntational Linguistics, 7:217–231.\nJohan Van Benthem. 1986. Essays in logical semantics.\nSpringer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nFangzhi Xu, Jun Liu, Qika Lin, Yudai Pan, and Lin-\ngling Zhang. 2022. Logiformer: A two-branch graph\ntransformer network for interpretable logical reason-\ning. In Proceedings of the 45th International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR ’22, page 1055–1065,\nNew York, NY , USA. Association for Computing\nMachinery.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Neural Information Process-\ning Systems.\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng.\n2020. Reclor: A reading comprehension dataset re-\nquiring logical reasoning. In International Confer-\nence on Learning Representations.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2020. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML’20. JMLR.org.\n1771\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 8\n□\u0017 A2. Did you discuss any potential risks of your work?\nThere are no potential risks of our work.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nSection 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix A\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1772\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3 and Section 5\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1773",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5654177665710449
    },
    {
      "name": "Transformer",
      "score": 0.5044976472854614
    },
    {
      "name": "Programming language",
      "score": 0.46998536586761475
    },
    {
      "name": "Natural language processing",
      "score": 0.44644901156425476
    },
    {
      "name": "Computational linguistics",
      "score": 0.4296633005142212
    },
    {
      "name": "Linguistics",
      "score": 0.3901565372943878
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3753676414489746
    },
    {
      "name": "Engineering",
      "score": 0.1702197790145874
    },
    {
      "name": "Philosophy",
      "score": 0.12451532483100891
    },
    {
      "name": "Electrical engineering",
      "score": 0.11673533916473389
    },
    {
      "name": "Voltage",
      "score": 0.05586785078048706
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ],
  "cited_by": 4
}